<?xml version="1.0" encoding="UTF-8"?>
<algorithms version="110505">
<algorithm name="SectLabel" version="110505">
<variant no="0" confidence="0.000074">
<title confidence="0.964028">
Learning a Deep Hybrid Model for Semi-Supervised Text Classification
</title>
<author confidence="0.965941">
Alexander G. Ororbia II, C. Lee Giles, David Reitter
</author>
<affiliation confidence="0.981702">
College of Information Sciences and Technology
The Pennsylvania State University, University Park, PA
</affiliation>
<email confidence="0.948862">
{ago109, giles, reitter}@psu.edu
</email>
<sectionHeader confidence="0.997026" genericHeader="abstract">
Abstract
</sectionHeader>
<bodyText confidence="0.999923133333333">
We present a novel fine-tuning algorithm
in a deep hybrid architecture for semi-
supervised text classification. During
each increment of the online learning pro-
cess, the fine-tuning algorithm serves as
a top-down mechanism for pseudo-jointly
modifying model parameters following a
bottom-up generative learning pass. The
resulting model, trained under what we
call the Bottom-Up-Top-Down learning al-
gorithm, is shown to outperform a vari-
ety of competitive models and baselines
trained across a wide range of splits be-
tween supervised and unsupervised train-
ing data.
</bodyText>
<sectionHeader confidence="0.999514" genericHeader="introduction">
1 Introduction
</sectionHeader>
<bodyText confidence="0.999924131147541">
Recent breakthroughs in learning expressive neu-
ral architectures have addressed challenging prob-
lems in domains such as computer vision, speech
recognition, and natural language processing. This
success is owed to the representational power af-
forded by deeper architectures supported by long-
standing theoretical arguments (Hastad, 1987).
These architectures efficiently model complex,
highly varying functions via multiple layers of
non-linearities, which would otherwise require
very “wide” shallow models that need large quan-
tities of samples (Bengio, 2012). However, many
of these deeper models have relied on mini-batch
training on large-scale, labeled data-sets, either us-
ing unsupervised pre-training (Bengio et al., 2007)
or improved architectural components (such as ac-
tivation functions) (Schmidhuber, 2015).
In an online learning problem, samples are pre-
sented to the learning architecture at a given rate
(usually with one-time access to these data points),
and, as in the case of a web crawling agent, most
of these are unlabeled. Given this, batch training
and supervised learning frameworks are no longer
applicable. While incremental approaches such
as co-training have been employed to help these
models learn in a more update-able fashion (Blum
and Mitchell, 1998; Gollapalli et al., 2013), neural
architectures can naturally be trained in an online
manner through the use of stochastic gradient de-
scent (SGD).
Semi-supervised online learning does not only
address practical applications, but it also reflects
some challenges of human category acquisition
(Tomasello, 2001). Consider the case of a child
learning to discriminate between object categories
and mapping them to words, given only a small
amount of explicitly labeled data (the mother
pointing to the object), and a large portion of un-
supervised learning, where the child comprehends
an adult’s speech or experiences positive feedback
for his or her own utterances regardless of their
correctness. The original argument in this respect
applied to grammar (e.g., Chomsky, 1980; Pullum
&amp; Scholz, 2002). While neural networks are not
necessarily models of actual cognitive processes,
semi-supervised models can show learnability and
illustrate possible constraints inherent to the learn-
ing process.
The contribution of this paper is the develop-
ment of the Bottom-Up-Top-Down learning al-
gorithm for training a Stacked Boltzmann Ex-
perts Network (SBEN) (Ororbia II et al., 2015)
hybrid architecture. This procedure combines
our proposed top-down fine-tuning procedure for
jointly modifying the parameters of a SBEN with
a modified form of the model’s original layer-wise
bottom-up learning pass (Ororbia II et al., 2015).
We investigate the performance of the constructed
deep model when applied to semi-supervised text
classification problems and find that our hybrid ar-
chitecture outperforms all baselines.
</bodyText>
<page confidence="0.982939">
471
</page>
<note confidence="0.9904925">
Proceedings of the 2015 Conference on Empirical Methods in Natural Language Processing, pages 471–481,
Lisbon, Portugal, 17-21 September 2015. c�2015 Association for Computational Linguistics.
</note>
<sectionHeader confidence="0.999103" genericHeader="related work">
2 Related Work
</sectionHeader>
<bodyText confidence="0.99995303773585">
Recent successes in the domain of connection-
ist learning stem from the expressive power af-
forded by models, such as the Deep Belief Net-
work (DBN) (Hinton et al., 2006; Bengio et al.,
2007) or Stacked Denoising Autoencoder (Vincent
et al., 2010), that greedily learn layers of stacked
non-linear feature detectors, equivalent to levels of
abstraction of the original representation. In a va-
riety of language-based problems, deep architec-
tures have outperformed popular shallow models
and classifiers (Salakhutdinov and Hinton, 2009;
Liu, 2010; Socher et al., 2011; Glorot et al.,
2011b; Lu and Li, 2013; Lu et al., 2014). How-
ever, these architectures often operate in a multi-
stage learning process, where a generative archi-
tecture is pre-trained and then used to initialize pa-
rameters of a second architecture that can be dis-
criminatively fine-tuned (using back-propagation
of errors or drop-out: Hinton et al., 2012). Sev-
eral ideas have been proposed to help deep mod-
els deal with potentially uncooperative input dis-
tributions or encourage learning of discriminative
information earlier in the process, many leverag-
ing auxiliary models in various ways (Bengio et
al., 2007; Zhang et al., 2014; Lee et al., 2014). A
few methods for adapting deep architecture con-
struction to an incremental learning setting have
also been proposed (Calandra et al., 2012; Zhou
et al., 2012). Recently, it was shown in (Oror-
bia II et al., 2015) that deep hybrid architectures,
or multi-level models that integrate discriminative
and generative learning objectives, offer a strong
viable alternative to multi-stage learners and are
readily usable for categorization tasks.
For text-based classification, a dominating
model is the support vector machine (SVM)
(Cortes and Vapnik, 1995) with many useful in-
novations to yet further improve its discrimina-
tive performance (Subramanya and Bilmes, 2008).
When used in tandem with prior human knowl-
edge to hand-craft good features, this simple ar-
chitecture has proven effective in solving practical
text-based tasks, such as academic document clas-
sification (Caragea et al., 2014). However, while
model construction may be fast (especially when
using a linear kernel), this process is costly in
that it requires a great deal of human labor to an-
notate the training corpus. Our approach, which
builds on that of (Ororbia II et al., 2015), provides
a means for improving classification performance
when labeled data is in scarce supply, learning
structure and regularity within the text to reduce
classification error incrementally.
</bodyText>
<sectionHeader confidence="0.978478" genericHeader="method">
3 A Deep Hybrid Model for
</sectionHeader>
<subsectionHeader confidence="0.673389">
Semi-Supervised Learning
</subsectionHeader>
<bodyText confidence="0.999875111111111">
To directly handle the problem of discriminative
learning when labeled data is scarce, (Ororbia II
et al., 2015) proposed deep hybrid architectures
that could effectively leverage small amounts of
labeled and large amounts of unlabeled data. In
particular, the best-performing architecture was
the Stacked Boltzmann Experts Network (SBEN),
which is a variant of the DBN. In its construction
and training, the SBEN design borrows many re-
cent insights from efficiently learning good DBN
models (Hinton et al., 2006) and is essentially a
stack of building block models where each layer
of model parameters is greedily modified while
freezing the parameters of all others. In con-
trast to the DBN, which stacks restricted Boltz-
mann machines (RBM’s) and is often used to ini-
tialize a deep multi-layer perceptron (MLP), the
SBEN model is constructed by composing hybrid
restricted Boltzmann machines and can be directly
applied to the discriminative task in a single learn-
ing phase.
The hybrid restricted Boltzmann machine
(HRBM) (Schmah et al., 2008; Larochelle and
Bengio, 2008; Larochelle et al., 2012) building
block of the SBEN is itself an extension of the
RBM meant to ultimately perform classification.
The HRBM graphical model is defined via pa-
rameters Θ = (W, U, b, c, d) (where W is the
input-to-hidden weight matrix, U the hidden-to-
class weight matrix, b is the visible bias vector, c is
the hidden unit bias vector, and d is the class unit
bias vector), and is a model of the joint distribu-
tion of a binary feature vector x = (x1, · · · , xD)
and its label y E 11, · · · , C} that makes use of a
latent variable set h = (h1, · · · , hH). The model
assigns a probability to the triplet (y,x,h) using:
</bodyText>
<equation confidence="0.988960333333333">
p(y, x, h) =
e −E(y,x,h)
Z , (1)
p(y, x) = Z
1� e−E(y,x,h) (2)
h
</equation>
<bodyText confidence="0.986634">
where Z is known as the partition function. The
model’s energy function is defined as
</bodyText>
<page confidence="0.946974">
472
</page>
<equation confidence="0.7784335">
E(y, x, h) = −hTWx−bTx−cTh−dTey−hTUey.
(3)
</equation>
<bodyText confidence="0.999964111111111">
where ey = (1i=y)Ci=1 is the one-hot vector en-
coding of y. It is often not possible to compute
p(y, x, h) or the marginal p(y, x) due to the in-
tractable normalization constant. However, ex-
ploiting the model’s lack of intra-layer connec-
tions, block Gibbs sampling may be used to draw
samples of the HRBM’s latent variable layer given
the current state of the visible layer and vice versa.
This yields the following equations:
</bodyText>
<equation confidence="0.9978825">
p(h|y, x) = Y p(hj|y, x),
j
X
p(hj = 1|y, x) = σ(cj + Ujy +
i
edy+Ej Ujyhj
p(y|h) = (6)
Ey? edv*+Ej Ujy*h�
</equation>
<bodyText confidence="0.99988775">
where σ(v) = 1/(1 + e−v). Classification may
be performed directly with the HRBM by using its
free energy function F(y, x) to compute the con-
ditional distribution
</bodyText>
<equation confidence="0.996542333333333">
p(y|X) = e−F(y? X) (7)
P y?E{1,··· ,C}
e−F(y,X)
</equation>
<bodyText confidence="0.990822">
where the free energy is formally defined as
</bodyText>
<equation confidence="0.993863">
X X
−F(y, x) = (dy + ψ(cj + Ujy + Wjixi))
j
(g)
and ψ is the softplus activation function ψ(v)
log(1 + ev).
</equation>
<bodyText confidence="0.999214375">
To construct an N-layer SBEN (or N-SBEN), as
was shown in (Ororbia II et al., 2015), one may
learn a stack of HRBMs in one of two ways: (1)
in a strict greedy, layer-wise manner, where lay-
ers are each trained in isolation on all of the data
samples one at a time from the bottom-up; or (2)
in a more relaxed disjoint fashion, where all layers
are trained together on all of the data but still in a
</bodyText>
<figureCaption confidence="0.5956695">
Figure 1: Architecture of the SBEN model. The
model in feedforward mode can be viewed as a
directed model, however, during training, connec-
tions are bi-directional.
</figureCaption>
<bodyText confidence="0.999299375">
layer-wise bottom-up pass. To properly compute
intermediate data representations during training
and prediction in the SBEN, one must combine
Equations 4 and 7. (The specific procedure for do-
ing this can be found in the computeLayerwiseS-
tatistics sub-routine in Algorithm 1.) This gives
rise to the full SBEN architecture, which is de-
picted in Figure 1.
</bodyText>
<subsectionHeader confidence="0.999896">
3.1 Ensembling of Layer-Wise Experts
</subsectionHeader>
<bodyText confidence="0.999976153846154">
The SBEN may be viewed as a natural vertical en-
semble of layer-wise “experts”, where each layer
maps latent representations to predictions, which
differs from standard methods such as boosting
(Schapire, 1990). Traditional feedforward neural
models propagate data through the final network
to obtain an output prediction yt from a penulti-
mate layer for a given xt. In contrast, this hybrid
model is capable of a producing a label ynt at each
level n for xt.
To vertically aggregate layer-wise expert out-
puts, we compute a simple mean predictor,
p(y|x)ensemble, as follows:
</bodyText>
<equation confidence="0.988469333333333">
p(y|x)ensemble = N
1 XN p(y|x)n (9)
n=1
</equation>
<bodyText confidence="0.999897375">
This ensembling scheme provides a simple way to
incorporate acquired discriminative knowledge of
different levels of abstraction into the model’s fi-
nal prediction. We note that the SBEN’s inherent
layer-wise discriminative ability stands as an alter-
native to coupling helper classifiers (Bengio et al.,
2007) or the “companion objectives” (Lee et al.,
2014).
</bodyText>
<equation confidence="0.9842801">
Y
p(x|h) = p(xi|h),
i
X
p(xi = 1|h) = σ(bi +
j
(5)
Wjihj)
(4)
Wjixi)
</equation>
<page confidence="0.995811">
473
</page>
<subsectionHeader confidence="0.979624">
3.2 The Bottom-Up-Top-Down Learning
Algorithm
</subsectionHeader>
<bodyText confidence="0.999996">
With the SBEN architecture defined, we next
present its simple two-step training algorithm,
or the Bottom-Up-Top-Down procedure (BUTD),
which combines a greedy, bottom-up pass with a
subsequent top-down fine-tuning step. At every
iteration of training, the model makes use of a sin-
gle labeled sample (taken from an available, small
labeled data subset) and an example from either a
large unlabeled pool or a data-stream. We describe
each of the two phases in Sections 3.2.1 and 3.2.2.
</bodyText>
<subsectionHeader confidence="0.941906">
3.2.1 Bottom-Up Layer-wise Learning (BU)
</subsectionHeader>
<bodyText confidence="0.999973">
The first phase of N-SBEN learning consists of
a bottom-up pass where each layerwise HRBM
can be trained using a compound objective func-
tion. Data samples are propagated up the model
to the layer targeted for layer-wise training using
the feedforward schema described above. Each
HRBM layer of the SBEN is greedily trained us-
ing the frozen latent representations of the one be-
low, which are generated by using the lower level
expert’s input and prediction. The loss function
for each layer balances a discriminative objective
Ldisc, a supervised generative objective Lgen, and
an unsupervised generative objective Lunsup, fully
defined as follows:
</bodyText>
<equation confidence="0.998397">
Lsemi(Dtrain,Dunlab) = γLdisc(Dtrain)
+αLgen(Dtrain) (10)
+βLunsup(Dunlab)
</equation>
<bodyText confidence="0.9980945">
Unlike generative pre-training of neural architec-
tures (Bengio et al., 2007), the additional free pa-
rameters γ, α, and β offer explicit control over
the extent to which the final parameters discovered
are influenced by generative learning (Larochelle
et al., 2012; Ororbia II et al., 2015). More im-
portantly, the generative objectives may be viewed
as providing data-dependent regularization on the
discriminative learning gradient of each layer.
The objectives themselves are defined as:
</bodyText>
<equation confidence="0.977346">
Lunsup(Dunlab) = −
</equation>
<bodyText confidence="0.99996925">
where Dtrain = {(xt, y)} is the labeled training
data-set and Dunlab = {(ut)} is the unlabeled
training data-set. The gradient for Ldisc may be
computed directly, which follows the general form
</bodyText>
<equation confidence="0.995255">
� �
∂ log p(yt|x) ∂
∂θ = −Eh |yt,xt ∂θ (E(yt, xt, h))
� �
∂
+Ey,h|,x ∂θ (E(y, x, h))
</equation>
<bodyText confidence="0.9990322">
and can be calculated directly (see Larochelle et
al., 2012 , for details) or through a form of Drop-
ping, such as Drop-Out or Drop-Connect (Tom-
czak, 2013). The generative gradients themselves
follow the form
</bodyText>
<equation confidence="0.995884666666667">
� �
∂ log p(yt, x) ∂ ))
∂θ = −Eh|yt,xt ∂θ (E(yt, xt, h
� �
∂
+Ey,x,h ∂θ (E(y, x, h))
</equation>
<bodyText confidence="0.999992409090909">
and, despite being intractable for any sample
(xt, yt), may be approximated via the contrastive
divergence procedure (Hinton, 2002). The in-
tractable second expectation is replaced with a
point estimate using a single Gibbs sampling step.
To calculate the generative gradient for an unla-
beled sample u, a pseudo-label must be obtained
by using a layer-wise HRBM’s current estimate of
p(y|u), which can be viewed as a form of self-
training or Entropy Regularization (Lee, 2013).
The online procedure for computing the genera-
tive gradient (either labeled or unlabeled example)
for a single HRBM can be found in Ororbia et al.,
(2015).
Setting the coefficients that control learning ob-
jective influences can lead to different model con-
figurations (especially with respect to γ) as well as
impact the gradient-based training of each model
layer (i.e., α and β). In this paper, we shall ex-
plore two particular configurations, namely 1) by
setting γ = 0 and α = 1, which leads to con-
structing a purely generative model of Dtrain and
</bodyText>
<figure confidence="0.457902777777778">
Ldisc(Dtrain) = − |Dtrain |log p(y|xt), (11)
�
t=1
Lgen(Dtrain) = − |Dtrain |log p(yt, xt), and (12)
�
t=1
|Dunlab |logp(xt) (13)
�
t=1
</figure>
<page confidence="0.992413">
474
</page>
<bodyText confidence="0.975175666666667">
Algorithm 1 Top-down fine-tuning of an N-SBEN (ensemble back-propagation). Note that “·” indicates
a Hadamard product, ξ is an error signal vector, the prime superscript indicates a derivative (i.e., σ&apos; means
derivative function of the sigmoid), and bz is the symbol for linear pre-activation values.
</bodyText>
<equation confidence="0.8759843">
Input: (xt, yt) ∈ D, learning rate A and model parameters Θ = {Θ1, Θ2,..., ΘN}
function FINETUNEMODEL((xt, yt), A, Θ)
Ω ← ∅, xn ← xt, yn ← ∅ &gt; Initialize list of layer-wise model statistics &amp; variables
// Conduct feed-forward pass to collect layer-wise statistics
for Θn ∈ Θ do
(hn, bzn, yhn, xn) ← COMPUTELAYERWISESTATISTICS(xn, Θn)
Ωn ← (hn, bzn, yhn, xn), xn ← hn, yn ← yhn
// Conduct error back-propagation pass to adjust layer-wise parameters
ξl ← ∅
for l ← N,l−−, while l ≥ 1 do
(hl, bzl, yi , xl) ← Ω[l] &gt; Grab relevant statistics for layer l of model
if i = N then
(5disc, ξl) ← COMPUTEDISCIMINATIVEGRADIENT(yt, xl, ∅, hn,bz, Θl)
else
ξl ← ξl · σ&apos;(bzl)
(5disc, ξl) ← COMPUTEDISCIMINATIVEGRADIENT(yt, xl, ξl, hn,bz, Θl)
Θn ← Θn − A(5disc)
function COMPUTELAYERWISESTATISTICS(xt, Θn)
yht ← p(yt|xt, Θn) &gt; Equation 7 under the layerwise model
bz ← c + Wxt + Ueyt &gt; Can re-use bz to perform next step
ht ∼ p(h|yht , xt, Θn) &gt; Equation 4 under the layerwise model
return (ht,bz, yht , xt)
function COMPUTEDISCIMINATIVEGRADIENT(yt, xl, ξl, hn,bz, Θl)
o ← p(y|hn, Θl),t�tsoftmtaxt&apos;(o) · −(y(t/o)
5U ← ξhn+ Od ← S+ S US, S ξ · σ&apos;IZ)
if ξl =6 ∅ then
ξ ← ξ · ξl
5W ← ξxi, 5c ← ξ, 5b ← 0, 5U ← 5U + (ξeTyt), ξ ← WT ξ
return (5 ←
(5W, 5U, 5b, 5c, 5d), ξ)
</equation>
<bodyText confidence="0.9996204">
Dunsup, and 2) by setting -y = 1 with α freely
varying (which recovers the model of Ororbia et
al., 2015). In both scenarios, β is allowed to vary
as a user-defined hyper-parameter. The second set-
ting of -y allows for training the SBEN directly
with only the bottom-up phase defined in this sec-
tion. However, if the first setting is used, a sec-
ond phase may be used to incorporate a top-down
fine-tuning phase. A bottom-up pass simply en-
tails computing this compound gradient for each
layer of the model for 1 or 2 samples per training
iteration. Notice that the first scenario reduces the
number of hyper-parameters to explore in model
selection, requiring only an appropriate value for
β to be found.
</bodyText>
<subsubsectionHeader confidence="0.706435">
3.2.2 Top-Down Fine-tuning (TD)
</subsubsectionHeader>
<bodyText confidence="0.9999848">
Although efficient, the bottom-up procedure de-
scribed above is greedy, which means that the gra-
dients are computed for each layer-wise HRBM
independent of gradient information from other
layers of the model. One way we propose to
introduce a degree of joint training of param-
eters is to incorporate a second phase that ad-
justs the SBEN parameters via a modified form
of back-propagation. Such a routine can further
exploit the SBEN’s multiple predictors (or entry
points) where additional error signals may be com-
puted and aggregated while signals are reverse-
propagated down the network. We hypothesize
that holistic fine-tuning ensures that discrimina-
tive information is incorporated into the generative
</bodyText>
<page confidence="0.996953">
475
</page>
<construct confidence="0.6360525">
Algorithm 2 The Bottom-Up-Top-Down training procedure for learning an N-SBEN.
Input: (xt, yt) E Dtrain, (ut) E Dunlab, rates A &amp; Q, ¯p, &amp; parameters Θ = {Θ1, Θ2, ..., ΘN}
function BOTTOMUPTOPDOWN((yt, xt, ut, A, Q, Θ)
APPLYBOTTOMUPPASS(yt, xt, ut, A, γ = 0, α = 1, Q, Θ) &gt; See (Ororbia II et al., 2015)
// Up to two calls can be made to the top-down tuning routine
FINETUNEMODEL(xt, yt, A, Θ) D See Algorithm 1 for details
vt ← pensemble(y|x, Θn) &gt; Calculate pseudo-label probability using Equation 9
if max[vt] &gt; p¯ then
vt ← TOONEHOT(vt) &gt; Convert to 1-hot vector using argmax of model conditionals
FINETUNEMODEL(ut, vt, A, Θ)
</construct>
<bodyText confidence="0.999905705882353">
features being constructed in the bottom-up learn-
ing step. Furthermore, errors from experts above
are propagated down to lower layers, which were
initially frozen during the greedy, bottom-up train-
ing phase.
Fine-tuning in the context of training an SBEN
is different from using a pre-trained MLP that
is subsequently fine-tuned with back-propagation.
First, since the SBEN is a more complex architec-
ture than an MLP, pre-initializing an MLP would
be insufficient given that one would be tossing po-
tentially useful information stored in the SBEN’s
class filters (and corresponding class bias vectors)
of each layer-wise expert (i.e., U and d). Second,
merely using the SBEN as an intermediate model
ignores the fact the SBEN can already perform
classification directly. To avoid losing such infor-
mation and to fully exploit the model’s predictive
ability, we adapt the back-propagation algorithm
for training MLP’s to operate on the SBEN, which
we shall call ensemble back-propagation since
the fine-tuning method propagates error deriva-
tives down the network from many points of entry.
Ensemble back-propagation is described in Algo-
rithm 1.
With this second online training step, the
Bottom-Up-Top-Down (BUTD) training algorithm
for fully training an SBEN proceeds with a sin-
gle bottom-up modification step followed by a
single top-down joint fine-tuning step using the
ensemble back-propagation procedure defined in
Algorithm 1 for each training time step. A full
top-down phase can consist of up to two calls to
the ensemble back-propagation procedure. One
is used to jointly modify the SBEN’s parame-
ters with respect to the sample taken from Dtrain.
A second one is potentially needed to tune pa-
rameters with respect to the sample drawn from
Dunlab. For the unlabeled sample, if the high-
est class probability assigned by the SBEN (us-
ing Equation 9) is greater than a pre-set threshold
(i.e., max[pensemble(y|u)] &gt; ¯p), a pseudo-label is
created for that sample by converting the model’s
mean vector to a 1-hot encoding. The probability
threshold p¯ for the potential second call to the en-
semble back-propagation routine allows us to in-
corporate a tunable form of pseudo-labeling (Lee,
2013) into the Bottom-Up-Top-Down learning al-
gorithm.
The high-level view of the BUTD learning algo-
rithm is depicted in Algorithm 2.
</bodyText>
<sectionHeader confidence="0.997868" genericHeader="method">
4 Experimental Results
</sectionHeader>
<bodyText confidence="0.999890260869565">
We investigate the viability of our deep hybrid ar-
chitecture for semi-supervised text categorization.
Model performance was evaluated on the WebKB
data-set 1 and a small-scale version of the 20News-
Group data-set 2.
The original WebKB collection contains pages
from a variety of universities (Cornell, Texas,
Washington, and Wisconsin as well as miscella-
neous pages from others). The 4-class classifica-
tion problem we defined using this data-set was
to determine if a web-page could be identified as
one belonging to a Student, Faculty, Course, or
a Project, yielding a subset of usable 4,199 sam-
ples. We applied simple pre-processing to the text,
namely stop-word removal and stemming, chose
to leverage only the k most frequently occurring
terms (this varied across the two experiments), and
binarized the document low-level representation
(only 1 page vector was discarded due to pres-
ence of 0 terms). The 20NewsGroup data-set, on
the other hand, contained 16242 total samples and
was already pre-processed, containing 100 terms,
binary-occurrence low-level representation, with
</bodyText>
<footnote confidence="0.999958">
1The exact data-set we used can be found and downloaded
at http://www.cs.cmu.edu/afs/cs/project/theo-20/www/data/
2The exact data-set we used can be found and downloaded
at http://www.cs.nyu.edu/˜roweis/data.html.
</footnote>
<page confidence="0.997235">
476
</page>
<sectionHeader confidence="0.333171" genericHeader="method">
Mean CV Error
</sectionHeader>
<bodyText confidence="0.993707428571429">
tags for the four top-most highest level domains
or meta-topics in the newsgroups array.
For both data-sets, we evaluated model gen-
eralization performance using a stratified 5-fold
cross-validation (CV) scheme. For each possible
train/test split, we automatically partitioned the
training fold into separate labeled, unlabeled, and
validation subsets using stratified random sam-
pling without replacement. Generalization perfor-
mance was evaluated by estimating classification
error, average precision, average recall, and av-
erage F-Measure, where F-Measure was chosen
to be the harmonic mean of precision and recall,
F1 = 2(precision · recall)/(precision + recall).
</bodyText>
<subsectionHeader confidence="0.990776">
4.1 Model Designs
</subsectionHeader>
<bodyText confidence="0.9999604">
We evaluated the BUTD version of our model,
the 3-SBEN,BUTD, as described in Algorithm 2.
For simplicity, the number of latent variables at
each level of the SBEN was held equal to the di-
mensionality of the data (i.e., a complete repre-
sentation). We compared this model trained with
BUTD against a version utilizing only the bottom-
up phase (3-SBEN,BU) as in Ororbia et al. (2015).
Both SBEN models contained 3 layers of latent
variables.
We compared against an array of baseline clas-
sifiers. We used our implementation of an incre-
mental version of Maximum Entropy, or MaxEnt-
ST (which, as explained in Sarikaya et al., 2014,
is equivalent to a softmax classifier). Further-
more, we used our implementation of the Pega-
sos algorithm (SVM-ST) (Shalev-Shwartz et al.,
2011) which was extended to follow a proper
multi-class scheme (Crammer and Singer, 2002).
This is the online formulation of the SVM, trained
via sub-gradient descent on the primal objective
followed by a projection step (for simplicity, we
opted to using a linear-kernel). Additionally, we
implemented a semi-supervised Bernoulli Naive
Bayes classifier (NB-EM) trained via Expectation-
Maximization as in (Nigam et al., 1999). We
also compared our model against the HRBM
(Larochelle and Bengio, 2008) (effectively a sin-
gle layer SBEN), which serves as a powerful, non-
linear shallow classifier in of itself, as well as a
3-layer sparse deep Rectifier Network (Glorot et
al., 2011a), or Rect, composed of leaky rectifier
units.
All shallow classifiers (except NB-EM and the
HRBM) were extended to the semi-supervised set-
</bodyText>
<figure confidence="0.629653">
Number of Labeled Samples
</figure>
<figureCaption confidence="0.871732666666667">
Figure 2: Mean CV generalization performance as
a function of labeled sample subset size (using 200
features).
</figureCaption>
<bodyText confidence="0.999940857142857">
ting by leveraging a simple self-training scheme in
order to learn from unlabeled data samples. The
self-training scheme entailed using a classifier’s
estimate of p(y|u) for an unlabeled sample and,
if max[p(y|u)] &gt; ¯p, we created a 1-hot proxy
encoding using the argmax of model’s predictor,
where p¯ is a threshold meta-parameter. Since we
found this simple pseudo-labeling approach, sim-
ilar in spirit to (Lee, 2013), to improve the results
for all classifiers, and thus we report all results uti-
lizing this scheme. 3 All classes of models (SBEN,
HRBM, Rect, SVM-ST, MaxEnt-ST, NB-ST) were
subject to the same model selection procedure de-
scribed in the next section.
</bodyText>
<subsectionHeader confidence="0.997227">
4.2 Model Selection
</subsectionHeader>
<bodyText confidence="0.999979375">
Model selection was conducted using a paral-
lelized multi-setting scheme, where a configura-
tion file for each model was specified, describing
a set of hyper-parameter combinations to explore
(this is akin to a course-grained grid search, where
the points of model evaluation are set manually a
priori). For the SBEN’s, we varied the learning
rate ([0.01, 0.25]) and Q coefficient ([0.1, 1.0]) and
</bodyText>
<tableCaption confidence="0.730635333333333">
3All model implementations were computationally veri-
fied for correctness when applicable. Since most discrim-
inative objectives followed a gradient descent optimization
scheme and could be realized in an automatic differentiation
framework, we checked gradient validity via finite difference
approximation.
</tableCaption>
<figure confidence="0.840226833333333">
32 84 168 504 840 1512 2520
0.10 0.15 0.20 0.25 0.30
Model Types
SVM−ST
SBEN−BU
SBEN−BUTD
</figure>
<page confidence="0.99165">
477
</page>
<tableCaption confidence="0.991993">
Table 1: WEBKB categorization results on 1% of the training data labeled (8 examples per class), rest
unlabeled (i.e., 5-fold means with standard error of the mean, 250 features).
</tableCaption>
<table confidence="0.999960875">
Error Precision Recall F1-Score
NB-EM 0.369 ± 0.039 0.684 ± 0.022 0.680 ± 0.028 0.625 ± 0.043
MaxEnt-ST 0.402 ± 0.026 0.623 ± 0.025 0.593 ± 0.015 0.583 ± 0.020
SVM-ST 0.342 ± 0.020 0.663 ± 0.010 0.665 ± 0.014 0.644 ± 0.015
HRBM 0.252 ± 0.023 0.740 ± 0.019 0.765 ± 0.016 0.741 ± 0.021
3-Rect 0.328 ± 0.020 0.673 ± 0.017 0.680 ± 0.021 0.654 ± 0.023
3-SBEN,BU 0.239 ± 0.015 0.754 ± 0.014 0.780 ± 0.016 0.754 ± 0.015
3-SBEN,BUTD 0.210 ± 0.011 0.786 ± 0.009 0.784 ± 0.014 0.777 ± 0.012
</table>
<tableCaption confidence="0.992858">
Table 2: 20NewsGroup data-set categorization results on 1% of the training data labeled (8 examples per
class), rest unlabeled (i.e., 5-fold means with standard error of the mean).
</tableCaption>
<table confidence="0.9992805">
Error Precision Recall F1-Score
NB-EM 0.275 ± 0.006 0.7176 ± 0.010 0.6685 ± 0.010 0.6697 ± 0.010
MaxEnt-ST 0.335 ± 0.005 0.643 ± 0.007 0.643 ± 0.007 0.639 ± 0.007
SVM-ST 0.346 ± 0.008 0.669 ± 0.016 0.644 ± 0.012 0.634 ± 0.011
HRBM 0.284 ± 0.006 0.706 ± 0.012 0.699 ± 0.009 0.696 ± 0.008
3-Rect 0.318 ± 0.009 0.661 ± 0.011 0.661 ± 0.012 0.657 ± 0.011
3-SBEN,BU 0.270 ± 0.006 0.715 ± 0.009 0.714 ± 0.009 0.710 ± 0.007
3-SBEN,BUTD 0.256 ± 0.007 0.732 ± 0.005 0.727 ± 0.006 0.725 ± 0.006
</table>
<bodyText confidence="0.991055166666667">
experimented with stochastic and mean-field ver-
sions of the models 4 (we found that mean-field did
slightly better for this experiment and thus report
the performance of this model in this paper). The
HRBM’s meta-parameters were tuned using a sim-
ilar set-up to (Larochelle et al., 2012) with learn-
ing rate varied in ([0.01, 0.25]), α in ([0.1, 0.5]),
and Q in ({0.01, 0.1}). For the SVM-ST algo-
rithm, we tuned its slack variable A, searching in
the interval [0.0001, 0.5], for MaxEnt-ST its learn-
ing rate in [0.0001, 0.1], and for p¯ of all models
(shallow and deep) that used pseudo-labeling we
searched the interval [0.1, 1.0]. All models of all
configurations were trained for a 10,000 iteration
sweep incrementally on the data and the model
state with lowest validation error for that partic-
ular run was used. The SBEN, HRBM, and Rect
models were also set to use a momentum term of
0.9 (linearly increased from 0.1 in the first 1000
training iterations) and the Rect model made use
of a small L1 regularization penalty to encourage
additional hidden sparsity. For a data-set like the
20NewsGroup, which contained a number of unla-
beled samples greater than training iterations, we
view our schema as simulating access to a data-
4Mean-field simply means no sampling steps were taken
after computing probability vectors, or “means” in any stage
of the computation.
stream, since all models had access to any given
unlabeled example only once during a training run.
</bodyText>
<subsectionHeader confidence="0.999865">
4.3 Model Performance
</subsectionHeader>
<bodyText confidence="0.999941083333333">
We first conducted an experiment, using the We-
bKB data-set, exploring classification error as a
function of labeled data subset cardinality (Fig-
ure 2). In this setup, we repeated the strati-
fied cross-fold scheme for each possible labeled
data subset size, comparing the performance of
the SVM model against 3-SBEN,BU (blue dot-
ted curve) and 3-SBEN,BUTD (green dash-dotted
curve). We see that as the number of labeled ex-
amples increases (which entails greater human an-
notation effort) all models improve, nearly reach-
ing 90% accuracy. However, while the perfor-
mance difference between models becomes negli-
gible as the training set becomes more supervised,
as expected, it is in the less scarce regions of the
plot we are interested in. We see that for small
proportions, both variants of the SBEN outper-
form the SVM, and furthermore, the SBEN trained
via full BUTD can reach lower error, especially
for the most extreme scenario where only 8 la-
beled examples per class are available. We no-
tice a bump in the performance of BUTD as nearly
the whole training set becomes labeled and posit
that since the BUTD involves additional pseudo-
</bodyText>
<page confidence="0.999362">
478
</page>
<tableCaption confidence="0.999571">
Table 3: Top-most words that the SBEN (BUTD) model associates with the 4 NewsGroup meta-topics.
</tableCaption>
<subsectionHeader confidence="0.619328">
Meta-Topic Associated Terms
</subsectionHeader>
<bodyText confidence="0.937340884615385">
comp.* windows, graphics, card, driver, scsi, dos, files, display
rec.* players, hockey, season, nhl, team, league, baseball, games
sci.* orbit, shuttle, space, earth, mission, nasa, moon, doctor
talk.* jews, christian, religion, jesus, bible, war, israel, president
labeling steps (as in the top-down phase), there is
greater risk of reinforcing incorrect predictions in
the pseudo-joint 5 tuning of layerwise expert pa-
rameters. For text collections where most of the
data is labeled and unlabeled data is minimal, only
a simple bottom-up pass is needed to learn a good
hybrid model of the data.
The next set of experiments was conducted with
only 1% of the training sets labeled. We observe
(Tables 1 and 2) that our deep hybrid architec-
ture trained via BUTD outperforms all other mod-
els with respect to all performance metrics. While
the SBEN trained with simply an online bottom-up
performs significantly better than the SVM model,
we note a further reduction of error using our pro-
posed BUTD training procedure. The additional
top-down phase serves as a mechanism for uni-
fying the layer-wise experts, where error signals
for both labeled and pseudo-labeled examples in-
crease agreement among all model layer experts.
For the 20NewsGroup data-set, we conducted a
simple experiment to uncover some of the knowl-
edge acquired by our model with respect to the tar-
get categorization task. We applied the mechanism
from (Larochelle et al., 2012) to extract the vari-
ables that are most strongly associated with each
of the clamped target variables in the lowest layer
of a BUTD-trained SBEN. The top-scored terms
associated with each class variable are shown in
Table 3, using the 10 hidden nodes most highly
triggered by the clamped class node, in a model
trained on all of the 20NewsGroup data using a
model configuration determined from CV results
for the 20NewsGroup data-set reported in the pa-
per. Since the SBEN is a composition of layer-
wise experts each capable of classification, we
note that this procedure could be applied to each
level to uncover which unobserved variables are
most strongly associated with each class target.
We speculate that this could serve the basis for un-
5We use the phrase “pseudo-joint” to differentiate a model
that has all its parameters trained jointly from our own, where
only the top-down phase of BUTD introduces any form of
joint parameter modification.
covering the model’s underlying learnt hierarchy
of the data and be potentially used for knowledge
extraction, a subject for future work in analyzing
black box neural models such as our own.
</bodyText>
<sectionHeader confidence="0.999492" genericHeader="conclusions">
5 Conclusions
</sectionHeader>
<bodyText confidence="0.99993916">
We presented the Bottom-Up-Top-Down proce-
dure for training the Stacked Boltzmann Experts
Network, a hybrid architecture that balances both
discriminative and generative learning goals, in
the context of semi-supervised text categorization.
It combines a greedy, layer-wise bottom-up ap-
proach with a top-down fine-tuning method for
pseudo-joint modification of parameters.
Models were evaluated using two text corpora:
WebKB and 20NewsGroup. We compared re-
sults against several baseline models and found
that our hybrid architecture outperformed the oth-
ers in all settings investigated. We found that
the SBEN, especially when trained with the full
Bottom-Up-Top-Down learning procedure could
in some cases improve classification error by as
much 39% over the Pegasos SVM, and nearly 17%
over the HRBM, especially when data is in very
limited supply. While we were able to demon-
strate the viability of our hybrid model when using
only simple surface statistics of text, future work
shall include application of our models to more
semantic-oriented representations, such as those
leveraged in building log-linear language models
(Mikolov et al., 2013).
</bodyText>
<sectionHeader confidence="0.998139" genericHeader="acknowledgments">
Acknowledgments
</sectionHeader>
<bodyText confidence="0.9986796">
A.G.O. acknowledges support from The Penn-
sylvania State University and the National Sci-
ence Foundation (DGE-1144860). D.R. acknowl-
edges support from the National Science Founda-
tion (SES-1528409).
</bodyText>
<page confidence="0.998922">
479
</page>
<sectionHeader confidence="0.995642" genericHeader="references">
References
</sectionHeader>
<reference confidence="0.99954829245283">
Yoshua Bengio, Pascal Lamblin, Dan Popovici, Hugo
Larochelle, et al. 2007. Greedy layer-wise training
of deep networks. Advances in neural information
processing systems, 19:153.
Yoshua Bengio. 2012. Deep learning of representa-
tions for unsupervised and transfer learning. Jour-
nal of Machine Learning Research–Workshop and
Conference Proceedings, 27:17–37.
Avrim Blum and Tom Mitchell. 1998. Combin-
ing labeled and unlabeled data with co-training.
In Proceedings of the Eleventh Annual Conference
on Computational Learning Theory, pages 92–100.
ACM.
Roberto Calandra, Tapani Raiko, Marc Peter Deisen-
roth, and Federico Montesino Pouzols. 2012.
Learning Deep Belief Networks from Non-
stationary Streams. In Artificial Neural Networks
and Machine Learning - ICANN 2012, number
7553 in Lecture Notes in Computer Science, pages
379–386. Springer Berlin Heidelberg.
Cornelia Caragea, Jian Wu, Kyle Williams, Sujatha
Das, Madian Khabsa, Pradeep Teregowda, and
C. Lee Giles. 2014. Automatic identification of
research articles from crawled documents. In Pro-
ceedings of the Workshop: Web-Scale Classifica-
tion: Classifying Big Data from the Web, New York,
NY.
Noam Chomsky. 1980. Rules and representations. Be-
havioral and brain sciences, 3(01):1–15.
Corinna Cortes and Vladimir Vapnik. 1995. Support-
vector networks. 20(3):273–297.
Koby Crammer and Yoram Singer. 2002. On the algo-
rithmic implementation of multiclass kernel-based
vector machines. The Journal of Machine Learning
Research, 2:265–292.
Xavier Glorot, Antoine Bordes, and Yoshua Bengio.
2011a. Deep sparse rectifier networks. In Proc.
14th International Conference on Artificial Intelli-
gence and Statistics, volume 15, pages 315–323.
Xavier Glorot, Antoine Bordes, and Yoshua Bengio.
2011b. Domain Adaptation for Large-scale Senti-
ment Classification: A Deep Learning Approach. In
Proceedings of the 28th International Conference on
Machine Learning (ICML-11), pages 513–520.
Sujatha Das Gollapalli, Cornelia Caragea, Prasenjit
Mitra, and C. Lee Giles. 2013. Researcher Home-
page Classification Using Unlabeled Data. In Pro-
ceedings of the 22Nd International Conference on
World Wide Web, WWW ’13, pages 471–482. In-
ternational World Wide Web Conferences Steering
Committee.
Johan Hastad. 1987. Computational limitations of
small-depth circuits. MIT press.
Geoffrey E. Hinton, Simon Osindero, and Yee-Whye
Teh. 2006. A Fast Learning Algorithm for Deep
Belief Nets. Neural computation, 18(7):1527–1554.
Geoffrey E. Hinton, Nitish Srivastava, Alex
Krizhevsky, Ilya Sutskever, and Ruslan R. Salakhut-
dinov. 2012. Improving neural networks by
preventing co-adaptation of feature detectors. arXiv
preprint arXiv:1207.0580.
Geoffrey E. Hinton. 2002. Training Products of Ex-
perts by Minimizing Contrastive Divergence. Neu-
ral Computation, 14(8):1771–1800.
Hugo Larochelle and Yoshua Bengio. 2008. Classi-
fication using Discriminative Restricted Boltzmann
Machines. In Proceedings of the 25th International
Conference on Machine Learning, pages 536–543,
Helsinki, Finland.
Hugo Larochelle, Michael Mandel, Razvan Pascanu,
and Yoshua Bengio. 2012. Learning Algorithms
for the Classification Restricted Boltzmann Ma-
chine. The Journal of Machine Learning Research,
13:643–669.
Chen-Yu Lee, Saining Xie, Patrick Gallagher,
Zhengyou Zhang, and Zhuowen Tu. 2014. Deeply-
Supervised Nets. arXiv:1409.5185 [cs, stat].
Dong-Hyun Lee. 2013. Pseudo-label: The Simple
and Efficient Semi-supervised Learning Method for
Deep Neural Networks. In Workshop on Challenges
in Representation Learning, ICML, Atlanta, GA.
Tao Liu. 2010. A Novel Text Classification Approach
Based on Deep Belief Network. In Proceedings of
the 17th International Conference on Neural Infor-
mation Processing: Theory and Algorithms - Volume
Part I, ICONIP’10, pages 314–321. Springer-Verlag.
Zhengdong Lu and Hang Li. 2013. A Deep Architec-
ture for Matching Short Texts. In C. J. C. Burges,
L. Bottou, M. Welling, Z. Ghahramani, and K. Q.
Weinberger, editors, Advances in Neural Informa-
tion Processing Systems 26, pages 1367–1375. Cur-
ran Associates, Inc.
Shixiang Lu, Zhenbiao Chen, and Bo Xu. 2014.
Learning new semi-supervised deep auto-encoder
features for statistical machine translation. In Pro-
ceedings of the 52nd Annual Meeting of the Associa-
tion for Computational Linguistics, volume 1, pages
122–132, Baltimore, MD.
Tomas Mikolov, Ilya Sutskever, Kai Chen, Greg S Cor-
rado, and Jeff Dean. 2013. Distributed representa-
tions of words and phrases and their composition-
ality. In C. J. C. Burges, L. Bottou, M. Welling,
Z. Ghahramani, and K. Q. Weinberger, editors, Ad-
vances in Neural Information Processing Systems
26, pages 3111–3119. Curran Associates, Inc., Lake
Tahoe, NV.
</reference>
<page confidence="0.975753">
480
</page>
<reference confidence="0.999902098591549">
Kamal Nigam, John Lafferty, and Andrew McCallum.
1999. Using maximum entropy for text classifica-
tion. In IJCAI-99 workshop on Machine Learning
for Information Filtering, volume 1, pages 61–67.
Alexander G. Ororbia II, David Reitter, Jian Wu, and
C. Lee Giles. 2015. Online learning of deep hybrid
architectures for semi-supervised categorization. In
ECML PKDD, Porto, Portugal. Springer.
Geoffrey K Pullum and Barbara C Scholz. 2002. Em-
pirical assessment of stimulus poverty arguments.
The linguistic review, 18(1-2):9–50.
Ruslan Salakhutdinov and Geoffrey Hinton. 2009. Se-
mantic hashing. International Journal of Approxi-
mate Reasoning, 50(7):969–978, July.
R. Sarikaya, G.E. Hinton, and A. Deoras. 2014.
Application of Deep Belief Networks for Natu-
ral Language Understanding. IEEE/ACM Transac-
tions on Audio, Speech, and Language Processing,
22(4):778–784.
Robert E. Schapire. 1990. The Strength of Weak
Learnability. Machine learning, 5(2):197–227.
Tanya Schmah, Geoffrey E. Hinton, Steven L. Small,
Stephen Strother, and Richard S. Zemel. 2008.
Generative versus Discriminative Training of RBMs
for classification of fMRI images. In Advances in
neural information processing systems, pages 1409–
1416.
J¨urgen Schmidhuber. 2015. Deep learning in neural
networks: An overview. Neural Networks, 61:85–
117.
Shai Shalev-Shwartz, Yoram Singer, Nathan Srebro,
and Andrew Cotter. 2011. Pegasos: Primal Esti-
mated Sub-gradient Solver for SVM. Mathematical
programming, 127(1):3–30.
Richard Socher, Jeffrey Pennington, Eric H. Huang,
Andrew Y. Ng, and Christopher D. Manning. 2011.
Semi-supervised recursive autoencoders for predict-
ing sentiment distributions. In Proceedings of the
Conference on Empirical Methods in Natural Lan-
guage Processing, pages 151–161. Association for
Computational Linguistics.
Amarnag Subramanya and Jeff Bilmes. 2008. Soft-
supervised learning for text classification. In Em-
pirical Methods in Natural Language Processing,
pages 1090–1099.
Michael Tomasello. 2001. Perceiving intentions
and learning words in the second year of life. In
Melissa Bowerman and Stephen Levinson, editors,
Language acquisition and conceptual development,
pages 132–158.
Jakub M. Tomczak. 2013. Prediction of Breast
Cancer Recurrence using Classification Restricted
Boltzmann Machine with Dropping. arXiv preprint
arXiv:1308.6324.
Pascal Vincent, Hugo Larochelle, Isabelle Lajoie,
Yoshua Bengio, and Pierre-Antoine Manzagol.
2010. Stacked Denoising Autoencoders: Learning
Useful Representations in a Deep Network with a
Local Denoising Criterion. The Journal of Machine
Learning Research, 11:3371–3408.
Junbo Zhang, Guangjian Tian, Yadong Mu, and Wei
Fan. 2014. Supervised Deep Learning with Aux-
iliary Networks. In Proceedings of the 20th ACM
SIGKDD International Conference on Knowledge
Discovery and Data Mining, KDD ’14, pages 353–
361, New York City, New York. ACM.
Guanyu Zhou, Kihyuk Sohn, and Honglak Lee. 2012.
Online Incremental Feature Learning with Denois-
ing Autoencoders. In International Conference on
Artificial Intelligence and Statistics, pages 1453–
1461.
</reference>
<page confidence="0.998706">
481
</page>
</variant>
</algorithm>
<algorithm name="ParsHed" version="110505">
<variant no="0" confidence="0.711754">
<title confidence="0.999996">Learning a Deep Hybrid Model for Semi-Supervised Text Classification</title>
<author confidence="0.999695">Alexander G Ororbia C Lee Giles</author>
<author confidence="0.999695">David</author>
<affiliation confidence="0.98811">College of Information Sciences and The Pennsylvania State University, University Park,</affiliation>
<email confidence="0.750491">giles,</email>
<abstract confidence="0.9982285">We present a novel fine-tuning a deep hybrid architecture for semisupervised text classification. each increment of the online learning process, the fine-tuning algorithm serves as a top-down mechanism for pseudo-jointly modifying model parameters following a bottom-up generative learning pass. The resulting model, trained under what we the algorithm, is shown to outperform a variety of competitive models and baselines trained across a wide range of splits between supervised and unsupervised training data.</abstract>
</variant>
</algorithm>
<algorithm name="ParsCit" version="110505">
<citationList>
<citation valid="true">
<authors>
<author>Yoshua Bengio</author>
<author>Pascal Lamblin</author>
<author>Dan Popovici</author>
<author>Hugo Larochelle</author>
</authors>
<title>Greedy layer-wise training of deep networks. Advances in neural information processing systems,</title>
<date>2007</date>
<pages>19--153</pages>
<contexts>
<context position="1574" citStr="Bengio et al., 2007" startWordPosition="217" endWordPosition="220">ins such as computer vision, speech recognition, and natural language processing. This success is owed to the representational power afforded by deeper architectures supported by longstanding theoretical arguments (Hastad, 1987). These architectures efficiently model complex, highly varying functions via multiple layers of non-linearities, which would otherwise require very “wide” shallow models that need large quantities of samples (Bengio, 2012). However, many of these deeper models have relied on mini-batch training on large-scale, labeled data-sets, either using unsupervised pre-training (Bengio et al., 2007) or improved architectural components (such as activation functions) (Schmidhuber, 2015). In an online learning problem, samples are presented to the learning architecture at a given rate (usually with one-time access to these data points), and, as in the case of a web crawling agent, most of these are unlabeled. Given this, batch training and supervised learning frameworks are no longer applicable. While incremental approaches such as co-training have been employed to help these models learn in a more update-able fashion (Blum and Mitchell, 1998; Gollapalli et al., 2013), neural architectures</context>
<context position="4128" citStr="Bengio et al., 2007" startWordPosition="602" endWordPosition="605"> pass (Ororbia II et al., 2015). We investigate the performance of the constructed deep model when applied to semi-supervised text classification problems and find that our hybrid architecture outperforms all baselines. 471 Proceedings of the 2015 Conference on Empirical Methods in Natural Language Processing, pages 471–481, Lisbon, Portugal, 17-21 September 2015. c�2015 Association for Computational Linguistics. 2 Related Work Recent successes in the domain of connectionist learning stem from the expressive power afforded by models, such as the Deep Belief Network (DBN) (Hinton et al., 2006; Bengio et al., 2007) or Stacked Denoising Autoencoder (Vincent et al., 2010), that greedily learn layers of stacked non-linear feature detectors, equivalent to levels of abstraction of the original representation. In a variety of language-based problems, deep architectures have outperformed popular shallow models and classifiers (Salakhutdinov and Hinton, 2009; Liu, 2010; Socher et al., 2011; Glorot et al., 2011b; Lu and Li, 2013; Lu et al., 2014). However, these architectures often operate in a multistage learning process, where a generative architecture is pre-trained and then used to initialize parameters of a</context>
<context position="11220" citStr="Bengio et al., 2007" startWordPosition="1793" endWordPosition="1796">n output prediction yt from a penultimate layer for a given xt. In contrast, this hybrid model is capable of a producing a label ynt at each level n for xt. To vertically aggregate layer-wise expert outputs, we compute a simple mean predictor, p(y|x)ensemble, as follows: p(y|x)ensemble = N 1 XN p(y|x)n (9) n=1 This ensembling scheme provides a simple way to incorporate acquired discriminative knowledge of different levels of abstraction into the model’s final prediction. We note that the SBEN’s inherent layer-wise discriminative ability stands as an alternative to coupling helper classifiers (Bengio et al., 2007) or the “companion objectives” (Lee et al., 2014). Y p(x|h) = p(xi|h), i X p(xi = 1|h) = σ(bi + j (5) Wjihj) (4) Wjixi) 473 3.2 The Bottom-Up-Top-Down Learning Algorithm With the SBEN architecture defined, we next present its simple two-step training algorithm, or the Bottom-Up-Top-Down procedure (BUTD), which combines a greedy, bottom-up pass with a subsequent top-down fine-tuning step. At every iteration of training, the model makes use of a single labeled sample (taken from an available, small labeled data subset) and an example from either a large unlabeled pool or a data-stream. We descri</context>
<context position="12715" citStr="Bengio et al., 2007" startWordPosition="2025" endWordPosition="2028">del to the layer targeted for layer-wise training using the feedforward schema described above. Each HRBM layer of the SBEN is greedily trained using the frozen latent representations of the one below, which are generated by using the lower level expert’s input and prediction. The loss function for each layer balances a discriminative objective Ldisc, a supervised generative objective Lgen, and an unsupervised generative objective Lunsup, fully defined as follows: Lsemi(Dtrain,Dunlab) = γLdisc(Dtrain) +αLgen(Dtrain) (10) +βLunsup(Dunlab) Unlike generative pre-training of neural architectures (Bengio et al., 2007), the additional free parameters γ, α, and β offer explicit control over the extent to which the final parameters discovered are influenced by generative learning (Larochelle et al., 2012; Ororbia II et al., 2015). More importantly, the generative objectives may be viewed as providing data-dependent regularization on the discriminative learning gradient of each layer. The objectives themselves are defined as: Lunsup(Dunlab) = − where Dtrain = {(xt, y)} is the labeled training data-set and Dunlab = {(ut)} is the unlabeled training data-set. The gradient for Ldisc may be computed directly, which</context>
</contexts>
<marker>Bengio, Lamblin, Popovici, Larochelle, 2007</marker>
<rawString>Yoshua Bengio, Pascal Lamblin, Dan Popovici, Hugo Larochelle, et al. 2007. Greedy layer-wise training of deep networks. Advances in neural information processing systems, 19:153.</rawString>
</citation>
<citation valid="true">
<authors>
<author>Yoshua Bengio</author>
</authors>
<title>Deep learning of representations for unsupervised and transfer learning.</title>
<date>2012</date>
<booktitle>Journal of Machine Learning Research–Workshop and Conference Proceedings,</booktitle>
<pages>27--17</pages>
<contexts>
<context position="1405" citStr="Bengio, 2012" startWordPosition="195" endWordPosition="196">rvised and unsupervised training data. 1 Introduction Recent breakthroughs in learning expressive neural architectures have addressed challenging problems in domains such as computer vision, speech recognition, and natural language processing. This success is owed to the representational power afforded by deeper architectures supported by longstanding theoretical arguments (Hastad, 1987). These architectures efficiently model complex, highly varying functions via multiple layers of non-linearities, which would otherwise require very “wide” shallow models that need large quantities of samples (Bengio, 2012). However, many of these deeper models have relied on mini-batch training on large-scale, labeled data-sets, either using unsupervised pre-training (Bengio et al., 2007) or improved architectural components (such as activation functions) (Schmidhuber, 2015). In an online learning problem, samples are presented to the learning architecture at a given rate (usually with one-time access to these data points), and, as in the case of a web crawling agent, most of these are unlabeled. Given this, batch training and supervised learning frameworks are no longer applicable. While incremental approaches</context>
</contexts>
<marker>Bengio, 2012</marker>
<rawString>Yoshua Bengio. 2012. Deep learning of representations for unsupervised and transfer learning. Journal of Machine Learning Research–Workshop and Conference Proceedings, 27:17–37.</rawString>
</citation>
<citation valid="true">
<authors>
<author>Avrim Blum</author>
<author>Tom Mitchell</author>
</authors>
<title>Combining labeled and unlabeled data with co-training.</title>
<date>1998</date>
<booktitle>In Proceedings of the Eleventh Annual Conference on Computational Learning Theory,</booktitle>
<pages>92--100</pages>
<publisher>ACM.</publisher>
<contexts>
<context position="2126" citStr="Blum and Mitchell, 1998" startWordPosition="303" endWordPosition="306">data-sets, either using unsupervised pre-training (Bengio et al., 2007) or improved architectural components (such as activation functions) (Schmidhuber, 2015). In an online learning problem, samples are presented to the learning architecture at a given rate (usually with one-time access to these data points), and, as in the case of a web crawling agent, most of these are unlabeled. Given this, batch training and supervised learning frameworks are no longer applicable. While incremental approaches such as co-training have been employed to help these models learn in a more update-able fashion (Blum and Mitchell, 1998; Gollapalli et al., 2013), neural architectures can naturally be trained in an online manner through the use of stochastic gradient descent (SGD). Semi-supervised online learning does not only address practical applications, but it also reflects some challenges of human category acquisition (Tomasello, 2001). Consider the case of a child learning to discriminate between object categories and mapping them to words, given only a small amount of explicitly labeled data (the mother pointing to the object), and a large portion of unsupervised learning, where the child comprehends an adult’s speech</context>
</contexts>
<marker>Blum, Mitchell, 1998</marker>
<rawString>Avrim Blum and Tom Mitchell. 1998. Combining labeled and unlabeled data with co-training. In Proceedings of the Eleventh Annual Conference on Computational Learning Theory, pages 92–100. ACM.</rawString>
</citation>
<citation valid="true">
<authors>
<author>Roberto Calandra</author>
<author>Tapani Raiko</author>
<author>Marc Peter Deisenroth</author>
<author>Federico Montesino Pouzols</author>
</authors>
<title>Learning Deep Belief Networks from Nonstationary Streams.</title>
<date>2012</date>
<booktitle>In Artificial Neural Networks and Machine Learning - ICANN 2012, number 7553 in Lecture Notes in Computer Science,</booktitle>
<pages>379--386</pages>
<publisher>Springer</publisher>
<location>Berlin Heidelberg.</location>
<contexts>
<context position="5292" citStr="Calandra et al., 2012" startWordPosition="783" endWordPosition="786">s pre-trained and then used to initialize parameters of a second architecture that can be discriminatively fine-tuned (using back-propagation of errors or drop-out: Hinton et al., 2012). Several ideas have been proposed to help deep models deal with potentially uncooperative input distributions or encourage learning of discriminative information earlier in the process, many leveraging auxiliary models in various ways (Bengio et al., 2007; Zhang et al., 2014; Lee et al., 2014). A few methods for adapting deep architecture construction to an incremental learning setting have also been proposed (Calandra et al., 2012; Zhou et al., 2012). Recently, it was shown in (Ororbia II et al., 2015) that deep hybrid architectures, or multi-level models that integrate discriminative and generative learning objectives, offer a strong viable alternative to multi-stage learners and are readily usable for categorization tasks. For text-based classification, a dominating model is the support vector machine (SVM) (Cortes and Vapnik, 1995) with many useful innovations to yet further improve its discriminative performance (Subramanya and Bilmes, 2008). When used in tandem with prior human knowledge to hand-craft good feature</context>
</contexts>
<marker>Calandra, Raiko, Deisenroth, Pouzols, 2012</marker>
<rawString>Roberto Calandra, Tapani Raiko, Marc Peter Deisenroth, and Federico Montesino Pouzols. 2012. Learning Deep Belief Networks from Nonstationary Streams. In Artificial Neural Networks and Machine Learning - ICANN 2012, number 7553 in Lecture Notes in Computer Science, pages 379–386. Springer Berlin Heidelberg.</rawString>
</citation>
<citation valid="true">
<authors>
<author>Cornelia Caragea</author>
<author>Jian Wu</author>
<author>Kyle Williams</author>
<author>Sujatha Das</author>
<author>Madian Khabsa</author>
<author>Pradeep Teregowda</author>
<author>C Lee Giles</author>
</authors>
<title>Automatic identification of research articles from crawled documents.</title>
<date>2014</date>
<booktitle>In Proceedings of the Workshop: Web-Scale Classification: Classifying Big Data from the Web,</booktitle>
<location>New York, NY.</location>
<contexts>
<context position="6043" citStr="Caragea et al., 2014" startWordPosition="895" endWordPosition="898">at integrate discriminative and generative learning objectives, offer a strong viable alternative to multi-stage learners and are readily usable for categorization tasks. For text-based classification, a dominating model is the support vector machine (SVM) (Cortes and Vapnik, 1995) with many useful innovations to yet further improve its discriminative performance (Subramanya and Bilmes, 2008). When used in tandem with prior human knowledge to hand-craft good features, this simple architecture has proven effective in solving practical text-based tasks, such as academic document classification (Caragea et al., 2014). However, while model construction may be fast (especially when using a linear kernel), this process is costly in that it requires a great deal of human labor to annotate the training corpus. Our approach, which builds on that of (Ororbia II et al., 2015), provides a means for improving classification performance when labeled data is in scarce supply, learning structure and regularity within the text to reduce classification error incrementally. 3 A Deep Hybrid Model for Semi-Supervised Learning To directly handle the problem of discriminative learning when labeled data is scarce, (Ororbia II</context>
</contexts>
<marker>Caragea, Wu, Williams, Das, Khabsa, Teregowda, Giles, 2014</marker>
<rawString>Cornelia Caragea, Jian Wu, Kyle Williams, Sujatha Das, Madian Khabsa, Pradeep Teregowda, and C. Lee Giles. 2014. Automatic identification of research articles from crawled documents. In Proceedings of the Workshop: Web-Scale Classification: Classifying Big Data from the Web, New York, NY.</rawString>
</citation>
<citation valid="true">
<authors>
<author>Noam Chomsky</author>
</authors>
<title>Rules and representations. Behavioral and brain sciences,</title>
<date>1980</date>
<pages>3--01</pages>
<contexts>
<context position="2900" citStr="Chomsky, 1980" startWordPosition="422" endWordPosition="423">ed online learning does not only address practical applications, but it also reflects some challenges of human category acquisition (Tomasello, 2001). Consider the case of a child learning to discriminate between object categories and mapping them to words, given only a small amount of explicitly labeled data (the mother pointing to the object), and a large portion of unsupervised learning, where the child comprehends an adult’s speech or experiences positive feedback for his or her own utterances regardless of their correctness. The original argument in this respect applied to grammar (e.g., Chomsky, 1980; Pullum &amp; Scholz, 2002). While neural networks are not necessarily models of actual cognitive processes, semi-supervised models can show learnability and illustrate possible constraints inherent to the learning process. The contribution of this paper is the development of the Bottom-Up-Top-Down learning algorithm for training a Stacked Boltzmann Experts Network (SBEN) (Ororbia II et al., 2015) hybrid architecture. This procedure combines our proposed top-down fine-tuning procedure for jointly modifying the parameters of a SBEN with a modified form of the model’s original layer-wise bottom-up </context>
</contexts>
<marker>Chomsky, 1980</marker>
<rawString>Noam Chomsky. 1980. Rules and representations. Behavioral and brain sciences, 3(01):1–15.</rawString>
</citation>
<citation valid="true">
<authors>
<author>Corinna Cortes</author>
<author>Vladimir Vapnik</author>
</authors>
<date>1995</date>
<note>Supportvector networks. 20(3):273–297.</note>
<contexts>
<context position="5704" citStr="Cortes and Vapnik, 1995" startWordPosition="843" endWordPosition="846">arious ways (Bengio et al., 2007; Zhang et al., 2014; Lee et al., 2014). A few methods for adapting deep architecture construction to an incremental learning setting have also been proposed (Calandra et al., 2012; Zhou et al., 2012). Recently, it was shown in (Ororbia II et al., 2015) that deep hybrid architectures, or multi-level models that integrate discriminative and generative learning objectives, offer a strong viable alternative to multi-stage learners and are readily usable for categorization tasks. For text-based classification, a dominating model is the support vector machine (SVM) (Cortes and Vapnik, 1995) with many useful innovations to yet further improve its discriminative performance (Subramanya and Bilmes, 2008). When used in tandem with prior human knowledge to hand-craft good features, this simple architecture has proven effective in solving practical text-based tasks, such as academic document classification (Caragea et al., 2014). However, while model construction may be fast (especially when using a linear kernel), this process is costly in that it requires a great deal of human labor to annotate the training corpus. Our approach, which builds on that of (Ororbia II et al., 2015), pro</context>
</contexts>
<marker>Cortes, Vapnik, 1995</marker>
<rawString>Corinna Cortes and Vladimir Vapnik. 1995. Supportvector networks. 20(3):273–297.</rawString>
</citation>
<citation valid="true">
<authors>
<author>Koby Crammer</author>
<author>Yoram Singer</author>
</authors>
<title>On the algorithmic implementation of multiclass kernel-based vector machines.</title>
<date>2002</date>
<journal>The Journal of Machine Learning Research,</journal>
<pages>2--265</pages>
<contexts>
<context position="23701" citStr="Crammer and Singer, 2002" startWordPosition="3842" endWordPosition="3845">ete representation). We compared this model trained with BUTD against a version utilizing only the bottomup phase (3-SBEN,BU) as in Ororbia et al. (2015). Both SBEN models contained 3 layers of latent variables. We compared against an array of baseline classifiers. We used our implementation of an incremental version of Maximum Entropy, or MaxEntST (which, as explained in Sarikaya et al., 2014, is equivalent to a softmax classifier). Furthermore, we used our implementation of the Pegasos algorithm (SVM-ST) (Shalev-Shwartz et al., 2011) which was extended to follow a proper multi-class scheme (Crammer and Singer, 2002). This is the online formulation of the SVM, trained via sub-gradient descent on the primal objective followed by a projection step (for simplicity, we opted to using a linear-kernel). Additionally, we implemented a semi-supervised Bernoulli Naive Bayes classifier (NB-EM) trained via ExpectationMaximization as in (Nigam et al., 1999). We also compared our model against the HRBM (Larochelle and Bengio, 2008) (effectively a single layer SBEN), which serves as a powerful, nonlinear shallow classifier in of itself, as well as a 3-layer sparse deep Rectifier Network (Glorot et al., 2011a), or Rect,</context>
</contexts>
<marker>Crammer, Singer, 2002</marker>
<rawString>Koby Crammer and Yoram Singer. 2002. On the algorithmic implementation of multiclass kernel-based vector machines. The Journal of Machine Learning Research, 2:265–292.</rawString>
</citation>
<citation valid="true">
<authors>
<author>Xavier Glorot</author>
<author>Antoine Bordes</author>
<author>Yoshua Bengio</author>
</authors>
<title>Deep sparse rectifier networks.</title>
<date>2011</date>
<booktitle>In Proc. 14th International Conference on Artificial Intelligence and Statistics,</booktitle>
<volume>15</volume>
<pages>315--323</pages>
<contexts>
<context position="4523" citStr="Glorot et al., 2011" startWordPosition="659" endWordPosition="662">ational Linguistics. 2 Related Work Recent successes in the domain of connectionist learning stem from the expressive power afforded by models, such as the Deep Belief Network (DBN) (Hinton et al., 2006; Bengio et al., 2007) or Stacked Denoising Autoencoder (Vincent et al., 2010), that greedily learn layers of stacked non-linear feature detectors, equivalent to levels of abstraction of the original representation. In a variety of language-based problems, deep architectures have outperformed popular shallow models and classifiers (Salakhutdinov and Hinton, 2009; Liu, 2010; Socher et al., 2011; Glorot et al., 2011b; Lu and Li, 2013; Lu et al., 2014). However, these architectures often operate in a multistage learning process, where a generative architecture is pre-trained and then used to initialize parameters of a second architecture that can be discriminatively fine-tuned (using back-propagation of errors or drop-out: Hinton et al., 2012). Several ideas have been proposed to help deep models deal with potentially uncooperative input distributions or encourage learning of discriminative information earlier in the process, many leveraging auxiliary models in various ways (Bengio et al., 2007; Zhang et </context>
<context position="24289" citStr="Glorot et al., 2011" startWordPosition="3934" endWordPosition="3937">heme (Crammer and Singer, 2002). This is the online formulation of the SVM, trained via sub-gradient descent on the primal objective followed by a projection step (for simplicity, we opted to using a linear-kernel). Additionally, we implemented a semi-supervised Bernoulli Naive Bayes classifier (NB-EM) trained via ExpectationMaximization as in (Nigam et al., 1999). We also compared our model against the HRBM (Larochelle and Bengio, 2008) (effectively a single layer SBEN), which serves as a powerful, nonlinear shallow classifier in of itself, as well as a 3-layer sparse deep Rectifier Network (Glorot et al., 2011a), or Rect, composed of leaky rectifier units. All shallow classifiers (except NB-EM and the HRBM) were extended to the semi-supervised setNumber of Labeled Samples Figure 2: Mean CV generalization performance as a function of labeled sample subset size (using 200 features). ting by leveraging a simple self-training scheme in order to learn from unlabeled data samples. The self-training scheme entailed using a classifier’s estimate of p(y|u) for an unlabeled sample and, if max[p(y|u)] &gt; ¯p, we created a 1-hot proxy encoding using the argmax of model’s predictor, where p¯ is a threshold meta-p</context>
</contexts>
<marker>Glorot, Bordes, Bengio, 2011</marker>
<rawString>Xavier Glorot, Antoine Bordes, and Yoshua Bengio. 2011a. Deep sparse rectifier networks. In Proc. 14th International Conference on Artificial Intelligence and Statistics, volume 15, pages 315–323.</rawString>
</citation>
<citation valid="true">
<authors>
<author>Xavier Glorot</author>
<author>Antoine Bordes</author>
<author>Yoshua Bengio</author>
</authors>
<title>Domain Adaptation for Large-scale Sentiment Classification: A Deep Learning Approach.</title>
<date>2011</date>
<booktitle>In Proceedings of the 28th International Conference on Machine Learning (ICML-11),</booktitle>
<pages>513--520</pages>
<contexts>
<context position="4523" citStr="Glorot et al., 2011" startWordPosition="659" endWordPosition="662">ational Linguistics. 2 Related Work Recent successes in the domain of connectionist learning stem from the expressive power afforded by models, such as the Deep Belief Network (DBN) (Hinton et al., 2006; Bengio et al., 2007) or Stacked Denoising Autoencoder (Vincent et al., 2010), that greedily learn layers of stacked non-linear feature detectors, equivalent to levels of abstraction of the original representation. In a variety of language-based problems, deep architectures have outperformed popular shallow models and classifiers (Salakhutdinov and Hinton, 2009; Liu, 2010; Socher et al., 2011; Glorot et al., 2011b; Lu and Li, 2013; Lu et al., 2014). However, these architectures often operate in a multistage learning process, where a generative architecture is pre-trained and then used to initialize parameters of a second architecture that can be discriminatively fine-tuned (using back-propagation of errors or drop-out: Hinton et al., 2012). Several ideas have been proposed to help deep models deal with potentially uncooperative input distributions or encourage learning of discriminative information earlier in the process, many leveraging auxiliary models in various ways (Bengio et al., 2007; Zhang et </context>
<context position="24289" citStr="Glorot et al., 2011" startWordPosition="3934" endWordPosition="3937">heme (Crammer and Singer, 2002). This is the online formulation of the SVM, trained via sub-gradient descent on the primal objective followed by a projection step (for simplicity, we opted to using a linear-kernel). Additionally, we implemented a semi-supervised Bernoulli Naive Bayes classifier (NB-EM) trained via ExpectationMaximization as in (Nigam et al., 1999). We also compared our model against the HRBM (Larochelle and Bengio, 2008) (effectively a single layer SBEN), which serves as a powerful, nonlinear shallow classifier in of itself, as well as a 3-layer sparse deep Rectifier Network (Glorot et al., 2011a), or Rect, composed of leaky rectifier units. All shallow classifiers (except NB-EM and the HRBM) were extended to the semi-supervised setNumber of Labeled Samples Figure 2: Mean CV generalization performance as a function of labeled sample subset size (using 200 features). ting by leveraging a simple self-training scheme in order to learn from unlabeled data samples. The self-training scheme entailed using a classifier’s estimate of p(y|u) for an unlabeled sample and, if max[p(y|u)] &gt; ¯p, we created a 1-hot proxy encoding using the argmax of model’s predictor, where p¯ is a threshold meta-p</context>
</contexts>
<marker>Glorot, Bordes, Bengio, 2011</marker>
<rawString>Xavier Glorot, Antoine Bordes, and Yoshua Bengio. 2011b. Domain Adaptation for Large-scale Sentiment Classification: A Deep Learning Approach. In Proceedings of the 28th International Conference on Machine Learning (ICML-11), pages 513–520.</rawString>
</citation>
<citation valid="true">
<authors>
<author>Sujatha Das Gollapalli</author>
<author>Cornelia Caragea</author>
<author>Prasenjit Mitra</author>
<author>C Lee Giles</author>
</authors>
<title>Researcher Homepage Classification Using Unlabeled Data.</title>
<date>2013</date>
<booktitle>In Proceedings of the 22Nd International Conference on World Wide Web, WWW ’13,</booktitle>
<pages>471--482</pages>
<institution>International World Wide Web Conferences Steering Committee.</institution>
<contexts>
<context position="2152" citStr="Gollapalli et al., 2013" startWordPosition="307" endWordPosition="310">nsupervised pre-training (Bengio et al., 2007) or improved architectural components (such as activation functions) (Schmidhuber, 2015). In an online learning problem, samples are presented to the learning architecture at a given rate (usually with one-time access to these data points), and, as in the case of a web crawling agent, most of these are unlabeled. Given this, batch training and supervised learning frameworks are no longer applicable. While incremental approaches such as co-training have been employed to help these models learn in a more update-able fashion (Blum and Mitchell, 1998; Gollapalli et al., 2013), neural architectures can naturally be trained in an online manner through the use of stochastic gradient descent (SGD). Semi-supervised online learning does not only address practical applications, but it also reflects some challenges of human category acquisition (Tomasello, 2001). Consider the case of a child learning to discriminate between object categories and mapping them to words, given only a small amount of explicitly labeled data (the mother pointing to the object), and a large portion of unsupervised learning, where the child comprehends an adult’s speech or experiences positive f</context>
</contexts>
<marker>Gollapalli, Caragea, Mitra, Giles, 2013</marker>
<rawString>Sujatha Das Gollapalli, Cornelia Caragea, Prasenjit Mitra, and C. Lee Giles. 2013. Researcher Homepage Classification Using Unlabeled Data. In Proceedings of the 22Nd International Conference on World Wide Web, WWW ’13, pages 471–482. International World Wide Web Conferences Steering Committee.</rawString>
</citation>
<citation valid="true">
<authors>
<author>Johan Hastad</author>
</authors>
<title>Computational limitations of small-depth circuits.</title>
<date>1987</date>
<publisher>MIT press.</publisher>
<contexts>
<context position="1182" citStr="Hastad, 1987" startWordPosition="165" endWordPosition="166">arning pass. The resulting model, trained under what we call the Bottom-Up-Top-Down learning algorithm, is shown to outperform a variety of competitive models and baselines trained across a wide range of splits between supervised and unsupervised training data. 1 Introduction Recent breakthroughs in learning expressive neural architectures have addressed challenging problems in domains such as computer vision, speech recognition, and natural language processing. This success is owed to the representational power afforded by deeper architectures supported by longstanding theoretical arguments (Hastad, 1987). These architectures efficiently model complex, highly varying functions via multiple layers of non-linearities, which would otherwise require very “wide” shallow models that need large quantities of samples (Bengio, 2012). However, many of these deeper models have relied on mini-batch training on large-scale, labeled data-sets, either using unsupervised pre-training (Bengio et al., 2007) or improved architectural components (such as activation functions) (Schmidhuber, 2015). In an online learning problem, samples are presented to the learning architecture at a given rate (usually with one-ti</context>
</contexts>
<marker>Hastad, 1987</marker>
<rawString>Johan Hastad. 1987. Computational limitations of small-depth circuits. MIT press.</rawString>
</citation>
<citation valid="true">
<authors>
<author>Geoffrey E Hinton</author>
<author>Simon Osindero</author>
<author>Yee-Whye Teh</author>
</authors>
<title>A Fast Learning Algorithm for Deep Belief Nets. Neural computation,</title>
<date>2006</date>
<pages>18--7</pages>
<contexts>
<context position="4106" citStr="Hinton et al., 2006" startWordPosition="598" endWordPosition="601">se bottom-up learning pass (Ororbia II et al., 2015). We investigate the performance of the constructed deep model when applied to semi-supervised text classification problems and find that our hybrid architecture outperforms all baselines. 471 Proceedings of the 2015 Conference on Empirical Methods in Natural Language Processing, pages 471–481, Lisbon, Portugal, 17-21 September 2015. c�2015 Association for Computational Linguistics. 2 Related Work Recent successes in the domain of connectionist learning stem from the expressive power afforded by models, such as the Deep Belief Network (DBN) (Hinton et al., 2006; Bengio et al., 2007) or Stacked Denoising Autoencoder (Vincent et al., 2010), that greedily learn layers of stacked non-linear feature detectors, equivalent to levels of abstraction of the original representation. In a variety of language-based problems, deep architectures have outperformed popular shallow models and classifiers (Salakhutdinov and Hinton, 2009; Liu, 2010; Socher et al., 2011; Glorot et al., 2011b; Lu and Li, 2013; Lu et al., 2014). However, these architectures often operate in a multistage learning process, where a generative architecture is pre-trained and then used to init</context>
<context position="7058" citStr="Hinton et al., 2006" startWordPosition="1052" endWordPosition="1055"> the text to reduce classification error incrementally. 3 A Deep Hybrid Model for Semi-Supervised Learning To directly handle the problem of discriminative learning when labeled data is scarce, (Ororbia II et al., 2015) proposed deep hybrid architectures that could effectively leverage small amounts of labeled and large amounts of unlabeled data. In particular, the best-performing architecture was the Stacked Boltzmann Experts Network (SBEN), which is a variant of the DBN. In its construction and training, the SBEN design borrows many recent insights from efficiently learning good DBN models (Hinton et al., 2006) and is essentially a stack of building block models where each layer of model parameters is greedily modified while freezing the parameters of all others. In contrast to the DBN, which stacks restricted Boltzmann machines (RBM’s) and is often used to initialize a deep multi-layer perceptron (MLP), the SBEN model is constructed by composing hybrid restricted Boltzmann machines and can be directly applied to the discriminative task in a single learning phase. The hybrid restricted Boltzmann machine (HRBM) (Schmah et al., 2008; Larochelle and Bengio, 2008; Larochelle et al., 2012) building block</context>
</contexts>
<marker>Hinton, Osindero, Teh, 2006</marker>
<rawString>Geoffrey E. Hinton, Simon Osindero, and Yee-Whye Teh. 2006. A Fast Learning Algorithm for Deep Belief Nets. Neural computation, 18(7):1527–1554.</rawString>
</citation>
<citation valid="true">
<authors>
<author>Geoffrey E Hinton</author>
<author>Nitish Srivastava</author>
<author>Alex Krizhevsky</author>
<author>Ilya Sutskever</author>
<author>Ruslan R Salakhutdinov</author>
</authors>
<title>Improving neural networks by preventing co-adaptation of feature detectors. arXiv preprint arXiv:1207.0580.</title>
<date>2012</date>
<contexts>
<context position="4856" citStr="Hinton et al., 2012" startWordPosition="713" endWordPosition="716">eature detectors, equivalent to levels of abstraction of the original representation. In a variety of language-based problems, deep architectures have outperformed popular shallow models and classifiers (Salakhutdinov and Hinton, 2009; Liu, 2010; Socher et al., 2011; Glorot et al., 2011b; Lu and Li, 2013; Lu et al., 2014). However, these architectures often operate in a multistage learning process, where a generative architecture is pre-trained and then used to initialize parameters of a second architecture that can be discriminatively fine-tuned (using back-propagation of errors or drop-out: Hinton et al., 2012). Several ideas have been proposed to help deep models deal with potentially uncooperative input distributions or encourage learning of discriminative information earlier in the process, many leveraging auxiliary models in various ways (Bengio et al., 2007; Zhang et al., 2014; Lee et al., 2014). A few methods for adapting deep architecture construction to an incremental learning setting have also been proposed (Calandra et al., 2012; Zhou et al., 2012). Recently, it was shown in (Ororbia II et al., 2015) that deep hybrid architectures, or multi-level models that integrate discriminative and ge</context>
</contexts>
<marker>Hinton, Srivastava, Krizhevsky, Sutskever, Salakhutdinov, 2012</marker>
<rawString>Geoffrey E. Hinton, Nitish Srivastava, Alex Krizhevsky, Ilya Sutskever, and Ruslan R. Salakhutdinov. 2012. Improving neural networks by preventing co-adaptation of feature detectors. arXiv preprint arXiv:1207.0580.</rawString>
</citation>
<citation valid="true">
<authors>
<author>Geoffrey E Hinton</author>
</authors>
<title>Training Products of Experts by Minimizing Contrastive Divergence.</title>
<date>2002</date>
<journal>Neural Computation,</journal>
<volume>14</volume>
<issue>8</issue>
<contexts>
<context position="13850" citStr="Hinton, 2002" startWordPosition="2223" endWordPosition="2224">eled training data-set. The gradient for Ldisc may be computed directly, which follows the general form � � ∂ log p(yt|x) ∂ ∂θ = −Eh |yt,xt ∂θ (E(yt, xt, h)) � � ∂ +Ey,h|,x ∂θ (E(y, x, h)) and can be calculated directly (see Larochelle et al., 2012 , for details) or through a form of Dropping, such as Drop-Out or Drop-Connect (Tomczak, 2013). The generative gradients themselves follow the form � � ∂ log p(yt, x) ∂ )) ∂θ = −Eh|yt,xt ∂θ (E(yt, xt, h � � ∂ +Ey,x,h ∂θ (E(y, x, h)) and, despite being intractable for any sample (xt, yt), may be approximated via the contrastive divergence procedure (Hinton, 2002). The intractable second expectation is replaced with a point estimate using a single Gibbs sampling step. To calculate the generative gradient for an unlabeled sample u, a pseudo-label must be obtained by using a layer-wise HRBM’s current estimate of p(y|u), which can be viewed as a form of selftraining or Entropy Regularization (Lee, 2013). The online procedure for computing the generative gradient (either labeled or unlabeled example) for a single HRBM can be found in Ororbia et al., (2015). Setting the coefficients that control learning objective influences can lead to different model conf</context>
</contexts>
<marker>Hinton, 2002</marker>
<rawString>Geoffrey E. Hinton. 2002. Training Products of Experts by Minimizing Contrastive Divergence. Neural Computation, 14(8):1771–1800.</rawString>
</citation>
<citation valid="true">
<authors>
<author>Hugo Larochelle</author>
<author>Yoshua Bengio</author>
</authors>
<title>Classification using Discriminative Restricted Boltzmann Machines.</title>
<date>2008</date>
<booktitle>In Proceedings of the 25th International Conference on Machine Learning,</booktitle>
<pages>536--543</pages>
<location>Helsinki, Finland.</location>
<contexts>
<context position="7617" citStr="Larochelle and Bengio, 2008" startWordPosition="1142" endWordPosition="1145">ghts from efficiently learning good DBN models (Hinton et al., 2006) and is essentially a stack of building block models where each layer of model parameters is greedily modified while freezing the parameters of all others. In contrast to the DBN, which stacks restricted Boltzmann machines (RBM’s) and is often used to initialize a deep multi-layer perceptron (MLP), the SBEN model is constructed by composing hybrid restricted Boltzmann machines and can be directly applied to the discriminative task in a single learning phase. The hybrid restricted Boltzmann machine (HRBM) (Schmah et al., 2008; Larochelle and Bengio, 2008; Larochelle et al., 2012) building block of the SBEN is itself an extension of the RBM meant to ultimately perform classification. The HRBM graphical model is defined via parameters Θ = (W, U, b, c, d) (where W is the input-to-hidden weight matrix, U the hidden-toclass weight matrix, b is the visible bias vector, c is the hidden unit bias vector, and d is the class unit bias vector), and is a model of the joint distribution of a binary feature vector x = (x1, · · · , xD) and its label y E 11, · · · , C} that makes use of a latent variable set h = (h1, · · · , hH). The model assigns a probabil</context>
<context position="24111" citStr="Larochelle and Bengio, 2008" startWordPosition="3903" endWordPosition="3906">lent to a softmax classifier). Furthermore, we used our implementation of the Pegasos algorithm (SVM-ST) (Shalev-Shwartz et al., 2011) which was extended to follow a proper multi-class scheme (Crammer and Singer, 2002). This is the online formulation of the SVM, trained via sub-gradient descent on the primal objective followed by a projection step (for simplicity, we opted to using a linear-kernel). Additionally, we implemented a semi-supervised Bernoulli Naive Bayes classifier (NB-EM) trained via ExpectationMaximization as in (Nigam et al., 1999). We also compared our model against the HRBM (Larochelle and Bengio, 2008) (effectively a single layer SBEN), which serves as a powerful, nonlinear shallow classifier in of itself, as well as a 3-layer sparse deep Rectifier Network (Glorot et al., 2011a), or Rect, composed of leaky rectifier units. All shallow classifiers (except NB-EM and the HRBM) were extended to the semi-supervised setNumber of Labeled Samples Figure 2: Mean CV generalization performance as a function of labeled sample subset size (using 200 features). ting by leveraging a simple self-training scheme in order to learn from unlabeled data samples. The self-training scheme entailed using a classif</context>
</contexts>
<marker>Larochelle, Bengio, 2008</marker>
<rawString>Hugo Larochelle and Yoshua Bengio. 2008. Classification using Discriminative Restricted Boltzmann Machines. In Proceedings of the 25th International Conference on Machine Learning, pages 536–543, Helsinki, Finland.</rawString>
</citation>
<citation valid="true">
<authors>
<author>Hugo Larochelle</author>
<author>Michael Mandel</author>
<author>Razvan Pascanu</author>
<author>Yoshua Bengio</author>
</authors>
<title>Learning Algorithms for the Classification Restricted Boltzmann Machine.</title>
<date>2012</date>
<journal>The Journal of Machine Learning Research,</journal>
<pages>13--643</pages>
<contexts>
<context position="7643" citStr="Larochelle et al., 2012" startWordPosition="1146" endWordPosition="1149">g good DBN models (Hinton et al., 2006) and is essentially a stack of building block models where each layer of model parameters is greedily modified while freezing the parameters of all others. In contrast to the DBN, which stacks restricted Boltzmann machines (RBM’s) and is often used to initialize a deep multi-layer perceptron (MLP), the SBEN model is constructed by composing hybrid restricted Boltzmann machines and can be directly applied to the discriminative task in a single learning phase. The hybrid restricted Boltzmann machine (HRBM) (Schmah et al., 2008; Larochelle and Bengio, 2008; Larochelle et al., 2012) building block of the SBEN is itself an extension of the RBM meant to ultimately perform classification. The HRBM graphical model is defined via parameters Θ = (W, U, b, c, d) (where W is the input-to-hidden weight matrix, U the hidden-toclass weight matrix, b is the visible bias vector, c is the hidden unit bias vector, and d is the class unit bias vector), and is a model of the joint distribution of a binary feature vector x = (x1, · · · , xD) and its label y E 11, · · · , C} that makes use of a latent variable set h = (h1, · · · , hH). The model assigns a probability to the triplet (y,x,h)</context>
<context position="12902" citStr="Larochelle et al., 2012" startWordPosition="2055" endWordPosition="2058">of the one below, which are generated by using the lower level expert’s input and prediction. The loss function for each layer balances a discriminative objective Ldisc, a supervised generative objective Lgen, and an unsupervised generative objective Lunsup, fully defined as follows: Lsemi(Dtrain,Dunlab) = γLdisc(Dtrain) +αLgen(Dtrain) (10) +βLunsup(Dunlab) Unlike generative pre-training of neural architectures (Bengio et al., 2007), the additional free parameters γ, α, and β offer explicit control over the extent to which the final parameters discovered are influenced by generative learning (Larochelle et al., 2012; Ororbia II et al., 2015). More importantly, the generative objectives may be viewed as providing data-dependent regularization on the discriminative learning gradient of each layer. The objectives themselves are defined as: Lunsup(Dunlab) = − where Dtrain = {(xt, y)} is the labeled training data-set and Dunlab = {(ut)} is the unlabeled training data-set. The gradient for Ldisc may be computed directly, which follows the general form � � ∂ log p(yt|x) ∂ ∂θ = −Eh |yt,xt ∂θ (E(yt, xt, h)) � � ∂ +Ey,h|,x ∂θ (E(y, x, h)) and can be calculated directly (see Larochelle et al., 2012 , for details) o</context>
<context position="27663" citStr="Larochelle et al., 2012" startWordPosition="4501" endWordPosition="4504">07 0.639 ± 0.007 SVM-ST 0.346 ± 0.008 0.669 ± 0.016 0.644 ± 0.012 0.634 ± 0.011 HRBM 0.284 ± 0.006 0.706 ± 0.012 0.699 ± 0.009 0.696 ± 0.008 3-Rect 0.318 ± 0.009 0.661 ± 0.011 0.661 ± 0.012 0.657 ± 0.011 3-SBEN,BU 0.270 ± 0.006 0.715 ± 0.009 0.714 ± 0.009 0.710 ± 0.007 3-SBEN,BUTD 0.256 ± 0.007 0.732 ± 0.005 0.727 ± 0.006 0.725 ± 0.006 experimented with stochastic and mean-field versions of the models 4 (we found that mean-field did slightly better for this experiment and thus report the performance of this model in this paper). The HRBM’s meta-parameters were tuned using a similar set-up to (Larochelle et al., 2012) with learning rate varied in ([0.01, 0.25]), α in ([0.1, 0.5]), and Q in ({0.01, 0.1}). For the SVM-ST algorithm, we tuned its slack variable A, searching in the interval [0.0001, 0.5], for MaxEnt-ST its learning rate in [0.0001, 0.1], and for p¯ of all models (shallow and deep) that used pseudo-labeling we searched the interval [0.1, 1.0]. All models of all configurations were trained for a 10,000 iteration sweep incrementally on the data and the model state with lowest validation error for that particular run was used. The SBEN, HRBM, and Rect models were also set to use a momentum term of </context>
<context position="31566" citStr="Larochelle et al., 2012" startWordPosition="5143" endWordPosition="5146">nce metrics. While the SBEN trained with simply an online bottom-up performs significantly better than the SVM model, we note a further reduction of error using our proposed BUTD training procedure. The additional top-down phase serves as a mechanism for unifying the layer-wise experts, where error signals for both labeled and pseudo-labeled examples increase agreement among all model layer experts. For the 20NewsGroup data-set, we conducted a simple experiment to uncover some of the knowledge acquired by our model with respect to the target categorization task. We applied the mechanism from (Larochelle et al., 2012) to extract the variables that are most strongly associated with each of the clamped target variables in the lowest layer of a BUTD-trained SBEN. The top-scored terms associated with each class variable are shown in Table 3, using the 10 hidden nodes most highly triggered by the clamped class node, in a model trained on all of the 20NewsGroup data using a model configuration determined from CV results for the 20NewsGroup data-set reported in the paper. Since the SBEN is a composition of layerwise experts each capable of classification, we note that this procedure could be applied to each level</context>
</contexts>
<marker>Larochelle, Mandel, Pascanu, Bengio, 2012</marker>
<rawString>Hugo Larochelle, Michael Mandel, Razvan Pascanu, and Yoshua Bengio. 2012. Learning Algorithms for the Classification Restricted Boltzmann Machine. The Journal of Machine Learning Research, 13:643–669.</rawString>
</citation>
<citation valid="true">
<authors>
<author>Chen-Yu Lee</author>
<author>Saining Xie</author>
<author>Patrick Gallagher</author>
<author>Zhengyou Zhang</author>
<author>Zhuowen Tu</author>
</authors>
<date>2014</date>
<note>DeeplySupervised Nets. arXiv:1409.5185 [cs, stat].</note>
<contexts>
<context position="5151" citStr="Lee et al., 2014" startWordPosition="761" endWordPosition="764">i, 2013; Lu et al., 2014). However, these architectures often operate in a multistage learning process, where a generative architecture is pre-trained and then used to initialize parameters of a second architecture that can be discriminatively fine-tuned (using back-propagation of errors or drop-out: Hinton et al., 2012). Several ideas have been proposed to help deep models deal with potentially uncooperative input distributions or encourage learning of discriminative information earlier in the process, many leveraging auxiliary models in various ways (Bengio et al., 2007; Zhang et al., 2014; Lee et al., 2014). A few methods for adapting deep architecture construction to an incremental learning setting have also been proposed (Calandra et al., 2012; Zhou et al., 2012). Recently, it was shown in (Ororbia II et al., 2015) that deep hybrid architectures, or multi-level models that integrate discriminative and generative learning objectives, offer a strong viable alternative to multi-stage learners and are readily usable for categorization tasks. For text-based classification, a dominating model is the support vector machine (SVM) (Cortes and Vapnik, 1995) with many useful innovations to yet further im</context>
<context position="11269" citStr="Lee et al., 2014" startWordPosition="1801" endWordPosition="1804">a given xt. In contrast, this hybrid model is capable of a producing a label ynt at each level n for xt. To vertically aggregate layer-wise expert outputs, we compute a simple mean predictor, p(y|x)ensemble, as follows: p(y|x)ensemble = N 1 XN p(y|x)n (9) n=1 This ensembling scheme provides a simple way to incorporate acquired discriminative knowledge of different levels of abstraction into the model’s final prediction. We note that the SBEN’s inherent layer-wise discriminative ability stands as an alternative to coupling helper classifiers (Bengio et al., 2007) or the “companion objectives” (Lee et al., 2014). Y p(x|h) = p(xi|h), i X p(xi = 1|h) = σ(bi + j (5) Wjihj) (4) Wjixi) 473 3.2 The Bottom-Up-Top-Down Learning Algorithm With the SBEN architecture defined, we next present its simple two-step training algorithm, or the Bottom-Up-Top-Down procedure (BUTD), which combines a greedy, bottom-up pass with a subsequent top-down fine-tuning step. At every iteration of training, the model makes use of a single labeled sample (taken from an available, small labeled data subset) and an example from either a large unlabeled pool or a data-stream. We describe each of the two phases in Sections 3.2.1 and 3</context>
</contexts>
<marker>Lee, Xie, Gallagher, Zhang, Tu, 2014</marker>
<rawString>Chen-Yu Lee, Saining Xie, Patrick Gallagher, Zhengyou Zhang, and Zhuowen Tu. 2014. DeeplySupervised Nets. arXiv:1409.5185 [cs, stat].</rawString>
</citation>
<citation valid="true">
<authors>
<author>Dong-Hyun Lee</author>
</authors>
<title>Pseudo-label: The Simple and Efficient Semi-supervised Learning Method for Deep Neural Networks.</title>
<date>2013</date>
<booktitle>In Workshop on Challenges in Representation Learning, ICML,</booktitle>
<location>Atlanta, GA.</location>
<contexts>
<context position="14193" citStr="Lee, 2013" startWordPosition="2280" endWordPosition="2281">he generative gradients themselves follow the form � � ∂ log p(yt, x) ∂ )) ∂θ = −Eh|yt,xt ∂θ (E(yt, xt, h � � ∂ +Ey,x,h ∂θ (E(y, x, h)) and, despite being intractable for any sample (xt, yt), may be approximated via the contrastive divergence procedure (Hinton, 2002). The intractable second expectation is replaced with a point estimate using a single Gibbs sampling step. To calculate the generative gradient for an unlabeled sample u, a pseudo-label must be obtained by using a layer-wise HRBM’s current estimate of p(y|u), which can be viewed as a form of selftraining or Entropy Regularization (Lee, 2013). The online procedure for computing the generative gradient (either labeled or unlabeled example) for a single HRBM can be found in Ororbia et al., (2015). Setting the coefficients that control learning objective influences can lead to different model configurations (especially with respect to γ) as well as impact the gradient-based training of each model layer (i.e., α and β). In this paper, we shall explore two particular configurations, namely 1) by setting γ = 0 and α = 1, which leads to constructing a purely generative model of Dtrain and Ldisc(Dtrain) = − |Dtrain |log p(y|xt), (11) � t=</context>
<context position="20703" citStr="Lee, 2013" startWordPosition="3392" endWordPosition="3393">y the SBEN’s parameters with respect to the sample taken from Dtrain. A second one is potentially needed to tune parameters with respect to the sample drawn from Dunlab. For the unlabeled sample, if the highest class probability assigned by the SBEN (using Equation 9) is greater than a pre-set threshold (i.e., max[pensemble(y|u)] &gt; ¯p), a pseudo-label is created for that sample by converting the model’s mean vector to a 1-hot encoding. The probability threshold p¯ for the potential second call to the ensemble back-propagation routine allows us to incorporate a tunable form of pseudo-labeling (Lee, 2013) into the Bottom-Up-Top-Down learning algorithm. The high-level view of the BUTD learning algorithm is depicted in Algorithm 2. 4 Experimental Results We investigate the viability of our deep hybrid architecture for semi-supervised text categorization. Model performance was evaluated on the WebKB data-set 1 and a small-scale version of the 20NewsGroup data-set 2. The original WebKB collection contains pages from a variety of universities (Cornell, Texas, Washington, and Wisconsin as well as miscellaneous pages from others). The 4-class classification problem we defined using this data-set was </context>
<context position="24984" citStr="Lee, 2013" startWordPosition="4044" endWordPosition="4045"> and the HRBM) were extended to the semi-supervised setNumber of Labeled Samples Figure 2: Mean CV generalization performance as a function of labeled sample subset size (using 200 features). ting by leveraging a simple self-training scheme in order to learn from unlabeled data samples. The self-training scheme entailed using a classifier’s estimate of p(y|u) for an unlabeled sample and, if max[p(y|u)] &gt; ¯p, we created a 1-hot proxy encoding using the argmax of model’s predictor, where p¯ is a threshold meta-parameter. Since we found this simple pseudo-labeling approach, similar in spirit to (Lee, 2013), to improve the results for all classifiers, and thus we report all results utilizing this scheme. 3 All classes of models (SBEN, HRBM, Rect, SVM-ST, MaxEnt-ST, NB-ST) were subject to the same model selection procedure described in the next section. 4.2 Model Selection Model selection was conducted using a parallelized multi-setting scheme, where a configuration file for each model was specified, describing a set of hyper-parameter combinations to explore (this is akin to a course-grained grid search, where the points of model evaluation are set manually a priori). For the SBEN’s, we varied t</context>
</contexts>
<marker>Lee, 2013</marker>
<rawString>Dong-Hyun Lee. 2013. Pseudo-label: The Simple and Efficient Semi-supervised Learning Method for Deep Neural Networks. In Workshop on Challenges in Representation Learning, ICML, Atlanta, GA.</rawString>
</citation>
<citation valid="true">
<authors>
<author>Tao Liu</author>
</authors>
<title>A Novel Text Classification Approach Based on Deep Belief Network.</title>
<date>2010</date>
<booktitle>In Proceedings of the 17th International Conference on Neural Information Processing: Theory and Algorithms - Volume Part I, ICONIP’10,</booktitle>
<pages>314--321</pages>
<publisher>Springer-Verlag.</publisher>
<contexts>
<context position="4481" citStr="Liu, 2010" startWordPosition="653" endWordPosition="654">5. c�2015 Association for Computational Linguistics. 2 Related Work Recent successes in the domain of connectionist learning stem from the expressive power afforded by models, such as the Deep Belief Network (DBN) (Hinton et al., 2006; Bengio et al., 2007) or Stacked Denoising Autoencoder (Vincent et al., 2010), that greedily learn layers of stacked non-linear feature detectors, equivalent to levels of abstraction of the original representation. In a variety of language-based problems, deep architectures have outperformed popular shallow models and classifiers (Salakhutdinov and Hinton, 2009; Liu, 2010; Socher et al., 2011; Glorot et al., 2011b; Lu and Li, 2013; Lu et al., 2014). However, these architectures often operate in a multistage learning process, where a generative architecture is pre-trained and then used to initialize parameters of a second architecture that can be discriminatively fine-tuned (using back-propagation of errors or drop-out: Hinton et al., 2012). Several ideas have been proposed to help deep models deal with potentially uncooperative input distributions or encourage learning of discriminative information earlier in the process, many leveraging auxiliary models in va</context>
</contexts>
<marker>Liu, 2010</marker>
<rawString>Tao Liu. 2010. A Novel Text Classification Approach Based on Deep Belief Network. In Proceedings of the 17th International Conference on Neural Information Processing: Theory and Algorithms - Volume Part I, ICONIP’10, pages 314–321. Springer-Verlag.</rawString>
</citation>
<citation valid="true">
<authors>
<author>Zhengdong Lu</author>
<author>Hang Li</author>
</authors>
<title>A Deep Architecture for Matching Short Texts. In</title>
<date>2013</date>
<booktitle>Advances in Neural Information Processing Systems 26,</booktitle>
<pages>1367--1375</pages>
<editor>C. J. C. Burges, L. Bottou, M. Welling, Z. Ghahramani, and K. Q. Weinberger, editors,</editor>
<publisher>Curran Associates, Inc.</publisher>
<contexts>
<context position="4541" citStr="Lu and Li, 2013" startWordPosition="663" endWordPosition="666"> Related Work Recent successes in the domain of connectionist learning stem from the expressive power afforded by models, such as the Deep Belief Network (DBN) (Hinton et al., 2006; Bengio et al., 2007) or Stacked Denoising Autoencoder (Vincent et al., 2010), that greedily learn layers of stacked non-linear feature detectors, equivalent to levels of abstraction of the original representation. In a variety of language-based problems, deep architectures have outperformed popular shallow models and classifiers (Salakhutdinov and Hinton, 2009; Liu, 2010; Socher et al., 2011; Glorot et al., 2011b; Lu and Li, 2013; Lu et al., 2014). However, these architectures often operate in a multistage learning process, where a generative architecture is pre-trained and then used to initialize parameters of a second architecture that can be discriminatively fine-tuned (using back-propagation of errors or drop-out: Hinton et al., 2012). Several ideas have been proposed to help deep models deal with potentially uncooperative input distributions or encourage learning of discriminative information earlier in the process, many leveraging auxiliary models in various ways (Bengio et al., 2007; Zhang et al., 2014; Lee et </context>
</contexts>
<marker>Lu, Li, 2013</marker>
<rawString>Zhengdong Lu and Hang Li. 2013. A Deep Architecture for Matching Short Texts. In C. J. C. Burges, L. Bottou, M. Welling, Z. Ghahramani, and K. Q. Weinberger, editors, Advances in Neural Information Processing Systems 26, pages 1367–1375. Curran Associates, Inc.</rawString>
</citation>
<citation valid="true">
<authors>
<author>Shixiang Lu</author>
<author>Zhenbiao Chen</author>
<author>Bo Xu</author>
</authors>
<title>Learning new semi-supervised deep auto-encoder features for statistical machine translation.</title>
<date>2014</date>
<booktitle>In Proceedings of the 52nd Annual Meeting of the Association for Computational Linguistics,</booktitle>
<volume>1</volume>
<pages>122--132</pages>
<location>Baltimore, MD.</location>
<contexts>
<context position="4559" citStr="Lu et al., 2014" startWordPosition="667" endWordPosition="670">ent successes in the domain of connectionist learning stem from the expressive power afforded by models, such as the Deep Belief Network (DBN) (Hinton et al., 2006; Bengio et al., 2007) or Stacked Denoising Autoencoder (Vincent et al., 2010), that greedily learn layers of stacked non-linear feature detectors, equivalent to levels of abstraction of the original representation. In a variety of language-based problems, deep architectures have outperformed popular shallow models and classifiers (Salakhutdinov and Hinton, 2009; Liu, 2010; Socher et al., 2011; Glorot et al., 2011b; Lu and Li, 2013; Lu et al., 2014). However, these architectures often operate in a multistage learning process, where a generative architecture is pre-trained and then used to initialize parameters of a second architecture that can be discriminatively fine-tuned (using back-propagation of errors or drop-out: Hinton et al., 2012). Several ideas have been proposed to help deep models deal with potentially uncooperative input distributions or encourage learning of discriminative information earlier in the process, many leveraging auxiliary models in various ways (Bengio et al., 2007; Zhang et al., 2014; Lee et al., 2014). A few </context>
</contexts>
<marker>Lu, Chen, Xu, 2014</marker>
<rawString>Shixiang Lu, Zhenbiao Chen, and Bo Xu. 2014. Learning new semi-supervised deep auto-encoder features for statistical machine translation. In Proceedings of the 52nd Annual Meeting of the Association for Computational Linguistics, volume 1, pages 122–132, Baltimore, MD.</rawString>
</citation>
<citation valid="true">
<authors>
<author>Tomas Mikolov</author>
<author>Ilya Sutskever</author>
<author>Kai Chen</author>
<author>Greg S Corrado</author>
<author>Jeff Dean</author>
</authors>
<title>Distributed representations of words and phrases and their compositionality.</title>
<date>2013</date>
<booktitle>Advances in Neural Information Processing Systems 26,</booktitle>
<pages>3111--3119</pages>
<editor>In C. J. C. Burges, L. Bottou, M. Welling, Z. Ghahramani, and K. Q. Weinberger, editors,</editor>
<publisher>Curran Associates, Inc.,</publisher>
<location>Lake Tahoe, NV.</location>
<marker>Mikolov, Sutskever, Chen, Corrado, Dean, 2013</marker>
<rawString>Tomas Mikolov, Ilya Sutskever, Kai Chen, Greg S Corrado, and Jeff Dean. 2013. Distributed representations of words and phrases and their compositionality. In C. J. C. Burges, L. Bottou, M. Welling, Z. Ghahramani, and K. Q. Weinberger, editors, Advances in Neural Information Processing Systems 26, pages 3111–3119. Curran Associates, Inc., Lake Tahoe, NV.</rawString>
</citation>
<citation valid="true">
<authors>
<author>Kamal Nigam</author>
<author>John Lafferty</author>
<author>Andrew McCallum</author>
</authors>
<title>Using maximum entropy for text classification.</title>
<date>1999</date>
<booktitle>In IJCAI-99 workshop on Machine Learning for Information Filtering,</booktitle>
<volume>1</volume>
<pages>61--67</pages>
<contexts>
<context position="24036" citStr="Nigam et al., 1999" startWordPosition="3891" endWordPosition="3894"> MaxEntST (which, as explained in Sarikaya et al., 2014, is equivalent to a softmax classifier). Furthermore, we used our implementation of the Pegasos algorithm (SVM-ST) (Shalev-Shwartz et al., 2011) which was extended to follow a proper multi-class scheme (Crammer and Singer, 2002). This is the online formulation of the SVM, trained via sub-gradient descent on the primal objective followed by a projection step (for simplicity, we opted to using a linear-kernel). Additionally, we implemented a semi-supervised Bernoulli Naive Bayes classifier (NB-EM) trained via ExpectationMaximization as in (Nigam et al., 1999). We also compared our model against the HRBM (Larochelle and Bengio, 2008) (effectively a single layer SBEN), which serves as a powerful, nonlinear shallow classifier in of itself, as well as a 3-layer sparse deep Rectifier Network (Glorot et al., 2011a), or Rect, composed of leaky rectifier units. All shallow classifiers (except NB-EM and the HRBM) were extended to the semi-supervised setNumber of Labeled Samples Figure 2: Mean CV generalization performance as a function of labeled sample subset size (using 200 features). ting by leveraging a simple self-training scheme in order to learn fro</context>
</contexts>
<marker>Nigam, Lafferty, McCallum, 1999</marker>
<rawString>Kamal Nigam, John Lafferty, and Andrew McCallum. 1999. Using maximum entropy for text classification. In IJCAI-99 workshop on Machine Learning for Information Filtering, volume 1, pages 61–67.</rawString>
</citation>
<citation valid="true">
<authors>
<author>Alexander G Ororbia David Reitter</author>
<author>Jian Wu</author>
<author>C Lee Giles</author>
</authors>
<title>Online learning of deep hybrid architectures for semi-supervised categorization.</title>
<date>2015</date>
<booktitle>In ECML PKDD,</booktitle>
<publisher>Springer.</publisher>
<location>Porto, Portugal.</location>
<marker>Reitter, Wu, Giles, 2015</marker>
<rawString>Alexander G. Ororbia II, David Reitter, Jian Wu, and C. Lee Giles. 2015. Online learning of deep hybrid architectures for semi-supervised categorization. In ECML PKDD, Porto, Portugal. Springer.</rawString>
</citation>
<citation valid="true">
<authors>
<author>Geoffrey K Pullum</author>
<author>Barbara C Scholz</author>
</authors>
<title>Empirical assessment of stimulus poverty arguments. The linguistic review,</title>
<date>2002</date>
<pages>18--1</pages>
<contexts>
<context position="2924" citStr="Pullum &amp; Scholz, 2002" startWordPosition="424" endWordPosition="427">ing does not only address practical applications, but it also reflects some challenges of human category acquisition (Tomasello, 2001). Consider the case of a child learning to discriminate between object categories and mapping them to words, given only a small amount of explicitly labeled data (the mother pointing to the object), and a large portion of unsupervised learning, where the child comprehends an adult’s speech or experiences positive feedback for his or her own utterances regardless of their correctness. The original argument in this respect applied to grammar (e.g., Chomsky, 1980; Pullum &amp; Scholz, 2002). While neural networks are not necessarily models of actual cognitive processes, semi-supervised models can show learnability and illustrate possible constraints inherent to the learning process. The contribution of this paper is the development of the Bottom-Up-Top-Down learning algorithm for training a Stacked Boltzmann Experts Network (SBEN) (Ororbia II et al., 2015) hybrid architecture. This procedure combines our proposed top-down fine-tuning procedure for jointly modifying the parameters of a SBEN with a modified form of the model’s original layer-wise bottom-up learning pass (Ororbia I</context>
</contexts>
<marker>Pullum, Scholz, 2002</marker>
<rawString>Geoffrey K Pullum and Barbara C Scholz. 2002. Empirical assessment of stimulus poverty arguments. The linguistic review, 18(1-2):9–50.</rawString>
</citation>
<citation valid="true">
<authors>
<author>Ruslan Salakhutdinov</author>
<author>Geoffrey Hinton</author>
</authors>
<title>Semantic hashing.</title>
<date>2009</date>
<journal>International Journal of Approximate Reasoning,</journal>
<volume>50</volume>
<issue>7</issue>
<contexts>
<context position="4470" citStr="Salakhutdinov and Hinton, 2009" startWordPosition="649" endWordPosition="652">n, Portugal, 17-21 September 2015. c�2015 Association for Computational Linguistics. 2 Related Work Recent successes in the domain of connectionist learning stem from the expressive power afforded by models, such as the Deep Belief Network (DBN) (Hinton et al., 2006; Bengio et al., 2007) or Stacked Denoising Autoencoder (Vincent et al., 2010), that greedily learn layers of stacked non-linear feature detectors, equivalent to levels of abstraction of the original representation. In a variety of language-based problems, deep architectures have outperformed popular shallow models and classifiers (Salakhutdinov and Hinton, 2009; Liu, 2010; Socher et al., 2011; Glorot et al., 2011b; Lu and Li, 2013; Lu et al., 2014). However, these architectures often operate in a multistage learning process, where a generative architecture is pre-trained and then used to initialize parameters of a second architecture that can be discriminatively fine-tuned (using back-propagation of errors or drop-out: Hinton et al., 2012). Several ideas have been proposed to help deep models deal with potentially uncooperative input distributions or encourage learning of discriminative information earlier in the process, many leveraging auxiliary m</context>
</contexts>
<marker>Salakhutdinov, Hinton, 2009</marker>
<rawString>Ruslan Salakhutdinov and Geoffrey Hinton. 2009. Semantic hashing. International Journal of Approximate Reasoning, 50(7):969–978, July.</rawString>
</citation>
<citation valid="true">
<authors>
<author>R Sarikaya</author>
<author>G E Hinton</author>
<author>A Deoras</author>
</authors>
<title>Application of Deep Belief Networks for Natural Language Understanding.</title>
<date>2014</date>
<journal>IEEE/ACM Transactions on Audio, Speech, and Language Processing,</journal>
<volume>22</volume>
<issue>4</issue>
<contexts>
<context position="23472" citStr="Sarikaya et al., 2014" startWordPosition="3807" endWordPosition="3810">e evaluated the BUTD version of our model, the 3-SBEN,BUTD, as described in Algorithm 2. For simplicity, the number of latent variables at each level of the SBEN was held equal to the dimensionality of the data (i.e., a complete representation). We compared this model trained with BUTD against a version utilizing only the bottomup phase (3-SBEN,BU) as in Ororbia et al. (2015). Both SBEN models contained 3 layers of latent variables. We compared against an array of baseline classifiers. We used our implementation of an incremental version of Maximum Entropy, or MaxEntST (which, as explained in Sarikaya et al., 2014, is equivalent to a softmax classifier). Furthermore, we used our implementation of the Pegasos algorithm (SVM-ST) (Shalev-Shwartz et al., 2011) which was extended to follow a proper multi-class scheme (Crammer and Singer, 2002). This is the online formulation of the SVM, trained via sub-gradient descent on the primal objective followed by a projection step (for simplicity, we opted to using a linear-kernel). Additionally, we implemented a semi-supervised Bernoulli Naive Bayes classifier (NB-EM) trained via ExpectationMaximization as in (Nigam et al., 1999). We also compared our model against</context>
</contexts>
<marker>Sarikaya, Hinton, Deoras, 2014</marker>
<rawString>R. Sarikaya, G.E. Hinton, and A. Deoras. 2014. Application of Deep Belief Networks for Natural Language Understanding. IEEE/ACM Transactions on Audio, Speech, and Language Processing, 22(4):778–784.</rawString>
</citation>
<citation valid="true">
<authors>
<author>Robert E Schapire</author>
</authors>
<date>1990</date>
<booktitle>The Strength of Weak Learnability. Machine learning,</booktitle>
<pages>5--2</pages>
<contexts>
<context position="10508" citStr="Schapire, 1990" startWordPosition="1682" endWordPosition="1683">i-directional. layer-wise bottom-up pass. To properly compute intermediate data representations during training and prediction in the SBEN, one must combine Equations 4 and 7. (The specific procedure for doing this can be found in the computeLayerwiseStatistics sub-routine in Algorithm 1.) This gives rise to the full SBEN architecture, which is depicted in Figure 1. 3.1 Ensembling of Layer-Wise Experts The SBEN may be viewed as a natural vertical ensemble of layer-wise “experts”, where each layer maps latent representations to predictions, which differs from standard methods such as boosting (Schapire, 1990). Traditional feedforward neural models propagate data through the final network to obtain an output prediction yt from a penultimate layer for a given xt. In contrast, this hybrid model is capable of a producing a label ynt at each level n for xt. To vertically aggregate layer-wise expert outputs, we compute a simple mean predictor, p(y|x)ensemble, as follows: p(y|x)ensemble = N 1 XN p(y|x)n (9) n=1 This ensembling scheme provides a simple way to incorporate acquired discriminative knowledge of different levels of abstraction into the model’s final prediction. We note that the SBEN’s inherent</context>
</contexts>
<marker>Schapire, 1990</marker>
<rawString>Robert E. Schapire. 1990. The Strength of Weak Learnability. Machine learning, 5(2):197–227.</rawString>
</citation>
<citation valid="true">
<authors>
<author>Tanya Schmah</author>
<author>Geoffrey E Hinton</author>
<author>Steven L Small</author>
<author>Stephen Strother</author>
<author>Richard S Zemel</author>
</authors>
<title>Generative versus Discriminative Training of RBMs for classification of fMRI images.</title>
<date>2008</date>
<booktitle>In Advances in neural information processing systems,</booktitle>
<pages>1409--1416</pages>
<contexts>
<context position="7588" citStr="Schmah et al., 2008" startWordPosition="1138" endWordPosition="1141">rows many recent insights from efficiently learning good DBN models (Hinton et al., 2006) and is essentially a stack of building block models where each layer of model parameters is greedily modified while freezing the parameters of all others. In contrast to the DBN, which stacks restricted Boltzmann machines (RBM’s) and is often used to initialize a deep multi-layer perceptron (MLP), the SBEN model is constructed by composing hybrid restricted Boltzmann machines and can be directly applied to the discriminative task in a single learning phase. The hybrid restricted Boltzmann machine (HRBM) (Schmah et al., 2008; Larochelle and Bengio, 2008; Larochelle et al., 2012) building block of the SBEN is itself an extension of the RBM meant to ultimately perform classification. The HRBM graphical model is defined via parameters Θ = (W, U, b, c, d) (where W is the input-to-hidden weight matrix, U the hidden-toclass weight matrix, b is the visible bias vector, c is the hidden unit bias vector, and d is the class unit bias vector), and is a model of the joint distribution of a binary feature vector x = (x1, · · · , xD) and its label y E 11, · · · , C} that makes use of a latent variable set h = (h1, · · · , hH).</context>
</contexts>
<marker>Schmah, Hinton, Small, Strother, Zemel, 2008</marker>
<rawString>Tanya Schmah, Geoffrey E. Hinton, Steven L. Small, Stephen Strother, and Richard S. Zemel. 2008. Generative versus Discriminative Training of RBMs for classification of fMRI images. In Advances in neural information processing systems, pages 1409– 1416.</rawString>
</citation>
<citation valid="true">
<authors>
<author>J¨urgen Schmidhuber</author>
</authors>
<title>Deep learning in neural networks: An overview.</title>
<date>2015</date>
<journal>Neural Networks,</journal>
<volume>61</volume>
<pages>117</pages>
<contexts>
<context position="1662" citStr="Schmidhuber, 2015" startWordPosition="230" endWordPosition="231">cess is owed to the representational power afforded by deeper architectures supported by longstanding theoretical arguments (Hastad, 1987). These architectures efficiently model complex, highly varying functions via multiple layers of non-linearities, which would otherwise require very “wide” shallow models that need large quantities of samples (Bengio, 2012). However, many of these deeper models have relied on mini-batch training on large-scale, labeled data-sets, either using unsupervised pre-training (Bengio et al., 2007) or improved architectural components (such as activation functions) (Schmidhuber, 2015). In an online learning problem, samples are presented to the learning architecture at a given rate (usually with one-time access to these data points), and, as in the case of a web crawling agent, most of these are unlabeled. Given this, batch training and supervised learning frameworks are no longer applicable. While incremental approaches such as co-training have been employed to help these models learn in a more update-able fashion (Blum and Mitchell, 1998; Gollapalli et al., 2013), neural architectures can naturally be trained in an online manner through the use of stochastic gradient des</context>
</contexts>
<marker>Schmidhuber, 2015</marker>
<rawString>J¨urgen Schmidhuber. 2015. Deep learning in neural networks: An overview. Neural Networks, 61:85– 117.</rawString>
</citation>
<citation valid="true">
<authors>
<author>Shai Shalev-Shwartz</author>
<author>Yoram Singer</author>
<author>Nathan Srebro</author>
<author>Andrew Cotter</author>
</authors>
<title>Pegasos: Primal Estimated Sub-gradient Solver for SVM.</title>
<date>2011</date>
<booktitle>Mathematical programming,</booktitle>
<pages>127--1</pages>
<contexts>
<context position="23617" citStr="Shalev-Shwartz et al., 2011" startWordPosition="3829" endWordPosition="3832"> each level of the SBEN was held equal to the dimensionality of the data (i.e., a complete representation). We compared this model trained with BUTD against a version utilizing only the bottomup phase (3-SBEN,BU) as in Ororbia et al. (2015). Both SBEN models contained 3 layers of latent variables. We compared against an array of baseline classifiers. We used our implementation of an incremental version of Maximum Entropy, or MaxEntST (which, as explained in Sarikaya et al., 2014, is equivalent to a softmax classifier). Furthermore, we used our implementation of the Pegasos algorithm (SVM-ST) (Shalev-Shwartz et al., 2011) which was extended to follow a proper multi-class scheme (Crammer and Singer, 2002). This is the online formulation of the SVM, trained via sub-gradient descent on the primal objective followed by a projection step (for simplicity, we opted to using a linear-kernel). Additionally, we implemented a semi-supervised Bernoulli Naive Bayes classifier (NB-EM) trained via ExpectationMaximization as in (Nigam et al., 1999). We also compared our model against the HRBM (Larochelle and Bengio, 2008) (effectively a single layer SBEN), which serves as a powerful, nonlinear shallow classifier in of itself,</context>
</contexts>
<marker>Shalev-Shwartz, Singer, Srebro, Cotter, 2011</marker>
<rawString>Shai Shalev-Shwartz, Yoram Singer, Nathan Srebro, and Andrew Cotter. 2011. Pegasos: Primal Estimated Sub-gradient Solver for SVM. Mathematical programming, 127(1):3–30.</rawString>
</citation>
<citation valid="true">
<authors>
<author>Richard Socher</author>
<author>Jeffrey Pennington</author>
<author>Eric H Huang</author>
<author>Andrew Y Ng</author>
<author>Christopher D Manning</author>
</authors>
<title>Semi-supervised recursive autoencoders for predicting sentiment distributions.</title>
<date>2011</date>
<booktitle>In Proceedings of the Conference on Empirical Methods in Natural Language Processing,</booktitle>
<pages>151--161</pages>
<publisher>Association</publisher>
<institution>for Computational Linguistics.</institution>
<contexts>
<context position="4502" citStr="Socher et al., 2011" startWordPosition="655" endWordPosition="658">ssociation for Computational Linguistics. 2 Related Work Recent successes in the domain of connectionist learning stem from the expressive power afforded by models, such as the Deep Belief Network (DBN) (Hinton et al., 2006; Bengio et al., 2007) or Stacked Denoising Autoencoder (Vincent et al., 2010), that greedily learn layers of stacked non-linear feature detectors, equivalent to levels of abstraction of the original representation. In a variety of language-based problems, deep architectures have outperformed popular shallow models and classifiers (Salakhutdinov and Hinton, 2009; Liu, 2010; Socher et al., 2011; Glorot et al., 2011b; Lu and Li, 2013; Lu et al., 2014). However, these architectures often operate in a multistage learning process, where a generative architecture is pre-trained and then used to initialize parameters of a second architecture that can be discriminatively fine-tuned (using back-propagation of errors or drop-out: Hinton et al., 2012). Several ideas have been proposed to help deep models deal with potentially uncooperative input distributions or encourage learning of discriminative information earlier in the process, many leveraging auxiliary models in various ways (Bengio et</context>
</contexts>
<marker>Socher, Pennington, Huang, Ng, Manning, 2011</marker>
<rawString>Richard Socher, Jeffrey Pennington, Eric H. Huang, Andrew Y. Ng, and Christopher D. Manning. 2011. Semi-supervised recursive autoencoders for predicting sentiment distributions. In Proceedings of the Conference on Empirical Methods in Natural Language Processing, pages 151–161. Association for Computational Linguistics.</rawString>
</citation>
<citation valid="true">
<authors>
<author>Amarnag Subramanya</author>
<author>Jeff Bilmes</author>
</authors>
<title>Softsupervised learning for text classification.</title>
<date>2008</date>
<booktitle>In Empirical Methods in Natural Language Processing,</booktitle>
<pages>1090--1099</pages>
<contexts>
<context position="5817" citStr="Subramanya and Bilmes, 2008" startWordPosition="860" endWordPosition="863">itecture construction to an incremental learning setting have also been proposed (Calandra et al., 2012; Zhou et al., 2012). Recently, it was shown in (Ororbia II et al., 2015) that deep hybrid architectures, or multi-level models that integrate discriminative and generative learning objectives, offer a strong viable alternative to multi-stage learners and are readily usable for categorization tasks. For text-based classification, a dominating model is the support vector machine (SVM) (Cortes and Vapnik, 1995) with many useful innovations to yet further improve its discriminative performance (Subramanya and Bilmes, 2008). When used in tandem with prior human knowledge to hand-craft good features, this simple architecture has proven effective in solving practical text-based tasks, such as academic document classification (Caragea et al., 2014). However, while model construction may be fast (especially when using a linear kernel), this process is costly in that it requires a great deal of human labor to annotate the training corpus. Our approach, which builds on that of (Ororbia II et al., 2015), provides a means for improving classification performance when labeled data is in scarce supply, learning structure </context>
</contexts>
<marker>Subramanya, Bilmes, 2008</marker>
<rawString>Amarnag Subramanya and Jeff Bilmes. 2008. Softsupervised learning for text classification. In Empirical Methods in Natural Language Processing, pages 1090–1099.</rawString>
</citation>
<citation valid="true">
<authors>
<author>Michael Tomasello</author>
</authors>
<title>Perceiving intentions and learning words in the second year of life.</title>
<date>2001</date>
<pages>132--158</pages>
<editor>In Melissa Bowerman and Stephen Levinson, editors,</editor>
<contexts>
<context position="2436" citStr="Tomasello, 2001" startWordPosition="349" endWordPosition="350">as in the case of a web crawling agent, most of these are unlabeled. Given this, batch training and supervised learning frameworks are no longer applicable. While incremental approaches such as co-training have been employed to help these models learn in a more update-able fashion (Blum and Mitchell, 1998; Gollapalli et al., 2013), neural architectures can naturally be trained in an online manner through the use of stochastic gradient descent (SGD). Semi-supervised online learning does not only address practical applications, but it also reflects some challenges of human category acquisition (Tomasello, 2001). Consider the case of a child learning to discriminate between object categories and mapping them to words, given only a small amount of explicitly labeled data (the mother pointing to the object), and a large portion of unsupervised learning, where the child comprehends an adult’s speech or experiences positive feedback for his or her own utterances regardless of their correctness. The original argument in this respect applied to grammar (e.g., Chomsky, 1980; Pullum &amp; Scholz, 2002). While neural networks are not necessarily models of actual cognitive processes, semi-supervised models can sho</context>
</contexts>
<marker>Tomasello, 2001</marker>
<rawString>Michael Tomasello. 2001. Perceiving intentions and learning words in the second year of life. In Melissa Bowerman and Stephen Levinson, editors, Language acquisition and conceptual development, pages 132–158.</rawString>
</citation>
<citation valid="true">
<authors>
<author>Jakub M Tomczak</author>
</authors>
<title>Prediction of Breast Cancer Recurrence using Classification Restricted Boltzmann Machine with Dropping. arXiv preprint arXiv:1308.6324.</title>
<date>2013</date>
<contexts>
<context position="13580" citStr="Tomczak, 2013" startWordPosition="2173" endWordPosition="2175">ectives may be viewed as providing data-dependent regularization on the discriminative learning gradient of each layer. The objectives themselves are defined as: Lunsup(Dunlab) = − where Dtrain = {(xt, y)} is the labeled training data-set and Dunlab = {(ut)} is the unlabeled training data-set. The gradient for Ldisc may be computed directly, which follows the general form � � ∂ log p(yt|x) ∂ ∂θ = −Eh |yt,xt ∂θ (E(yt, xt, h)) � � ∂ +Ey,h|,x ∂θ (E(y, x, h)) and can be calculated directly (see Larochelle et al., 2012 , for details) or through a form of Dropping, such as Drop-Out or Drop-Connect (Tomczak, 2013). The generative gradients themselves follow the form � � ∂ log p(yt, x) ∂ )) ∂θ = −Eh|yt,xt ∂θ (E(yt, xt, h � � ∂ +Ey,x,h ∂θ (E(y, x, h)) and, despite being intractable for any sample (xt, yt), may be approximated via the contrastive divergence procedure (Hinton, 2002). The intractable second expectation is replaced with a point estimate using a single Gibbs sampling step. To calculate the generative gradient for an unlabeled sample u, a pseudo-label must be obtained by using a layer-wise HRBM’s current estimate of p(y|u), which can be viewed as a form of selftraining or Entropy Regularizatio</context>
</contexts>
<marker>Tomczak, 2013</marker>
<rawString>Jakub M. Tomczak. 2013. Prediction of Breast Cancer Recurrence using Classification Restricted Boltzmann Machine with Dropping. arXiv preprint arXiv:1308.6324.</rawString>
</citation>
<citation valid="true">
<authors>
<author>Pascal Vincent</author>
<author>Hugo Larochelle</author>
<author>Isabelle Lajoie</author>
<author>Yoshua Bengio</author>
<author>Pierre-Antoine Manzagol</author>
</authors>
<title>Stacked Denoising Autoencoders: Learning Useful Representations in a Deep Network with a Local Denoising Criterion.</title>
<date>2010</date>
<journal>The Journal of Machine Learning Research,</journal>
<pages>11--3371</pages>
<contexts>
<context position="4184" citStr="Vincent et al., 2010" startWordPosition="610" endWordPosition="613">formance of the constructed deep model when applied to semi-supervised text classification problems and find that our hybrid architecture outperforms all baselines. 471 Proceedings of the 2015 Conference on Empirical Methods in Natural Language Processing, pages 471–481, Lisbon, Portugal, 17-21 September 2015. c�2015 Association for Computational Linguistics. 2 Related Work Recent successes in the domain of connectionist learning stem from the expressive power afforded by models, such as the Deep Belief Network (DBN) (Hinton et al., 2006; Bengio et al., 2007) or Stacked Denoising Autoencoder (Vincent et al., 2010), that greedily learn layers of stacked non-linear feature detectors, equivalent to levels of abstraction of the original representation. In a variety of language-based problems, deep architectures have outperformed popular shallow models and classifiers (Salakhutdinov and Hinton, 2009; Liu, 2010; Socher et al., 2011; Glorot et al., 2011b; Lu and Li, 2013; Lu et al., 2014). However, these architectures often operate in a multistage learning process, where a generative architecture is pre-trained and then used to initialize parameters of a second architecture that can be discriminatively fine-t</context>
</contexts>
<marker>Vincent, Larochelle, Lajoie, Bengio, Manzagol, 2010</marker>
<rawString>Pascal Vincent, Hugo Larochelle, Isabelle Lajoie, Yoshua Bengio, and Pierre-Antoine Manzagol. 2010. Stacked Denoising Autoencoders: Learning Useful Representations in a Deep Network with a Local Denoising Criterion. The Journal of Machine Learning Research, 11:3371–3408.</rawString>
</citation>
<citation valid="true">
<authors>
<author>Junbo Zhang</author>
<author>Guangjian Tian</author>
<author>Yadong Mu</author>
<author>Wei Fan</author>
</authors>
<title>Supervised Deep Learning with Auxiliary Networks.</title>
<date>2014</date>
<booktitle>In Proceedings of the 20th ACM SIGKDD International Conference on Knowledge Discovery and Data Mining, KDD ’14,</booktitle>
<pages>353--361</pages>
<publisher>ACM.</publisher>
<location>New York City, New York.</location>
<contexts>
<context position="5132" citStr="Zhang et al., 2014" startWordPosition="757" endWordPosition="760">al., 2011b; Lu and Li, 2013; Lu et al., 2014). However, these architectures often operate in a multistage learning process, where a generative architecture is pre-trained and then used to initialize parameters of a second architecture that can be discriminatively fine-tuned (using back-propagation of errors or drop-out: Hinton et al., 2012). Several ideas have been proposed to help deep models deal with potentially uncooperative input distributions or encourage learning of discriminative information earlier in the process, many leveraging auxiliary models in various ways (Bengio et al., 2007; Zhang et al., 2014; Lee et al., 2014). A few methods for adapting deep architecture construction to an incremental learning setting have also been proposed (Calandra et al., 2012; Zhou et al., 2012). Recently, it was shown in (Ororbia II et al., 2015) that deep hybrid architectures, or multi-level models that integrate discriminative and generative learning objectives, offer a strong viable alternative to multi-stage learners and are readily usable for categorization tasks. For text-based classification, a dominating model is the support vector machine (SVM) (Cortes and Vapnik, 1995) with many useful innovation</context>
</contexts>
<marker>Zhang, Tian, Mu, Fan, 2014</marker>
<rawString>Junbo Zhang, Guangjian Tian, Yadong Mu, and Wei Fan. 2014. Supervised Deep Learning with Auxiliary Networks. In Proceedings of the 20th ACM SIGKDD International Conference on Knowledge Discovery and Data Mining, KDD ’14, pages 353– 361, New York City, New York. ACM.</rawString>
</citation>
<citation valid="true">
<authors>
<author>Guanyu Zhou</author>
<author>Kihyuk Sohn</author>
<author>Honglak Lee</author>
</authors>
<title>Online Incremental Feature Learning with Denoising Autoencoders.</title>
<date>2012</date>
<booktitle>In International Conference on Artificial Intelligence and Statistics,</booktitle>
<pages>1453--1461</pages>
<contexts>
<context position="5312" citStr="Zhou et al., 2012" startWordPosition="787" endWordPosition="790">used to initialize parameters of a second architecture that can be discriminatively fine-tuned (using back-propagation of errors or drop-out: Hinton et al., 2012). Several ideas have been proposed to help deep models deal with potentially uncooperative input distributions or encourage learning of discriminative information earlier in the process, many leveraging auxiliary models in various ways (Bengio et al., 2007; Zhang et al., 2014; Lee et al., 2014). A few methods for adapting deep architecture construction to an incremental learning setting have also been proposed (Calandra et al., 2012; Zhou et al., 2012). Recently, it was shown in (Ororbia II et al., 2015) that deep hybrid architectures, or multi-level models that integrate discriminative and generative learning objectives, offer a strong viable alternative to multi-stage learners and are readily usable for categorization tasks. For text-based classification, a dominating model is the support vector machine (SVM) (Cortes and Vapnik, 1995) with many useful innovations to yet further improve its discriminative performance (Subramanya and Bilmes, 2008). When used in tandem with prior human knowledge to hand-craft good features, this simple archi</context>
</contexts>
<marker>Zhou, Sohn, Lee, 2012</marker>
<rawString>Guanyu Zhou, Kihyuk Sohn, and Honglak Lee. 2012. Online Incremental Feature Learning with Denoising Autoencoders. In International Conference on Artificial Intelligence and Statistics, pages 1453– 1461.</rawString>
</citation>
</citationList>
</algorithm>
</algorithms>
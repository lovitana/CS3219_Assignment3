<?xml version="1.0" encoding="UTF-8"?>
<algorithms version="110505">
<algorithm name="SectLabel" version="110505">
<variant no="0" confidence="0.029971">
<title confidence="0.9959305">
Joint Event Trigger Identification and Event Coreference Resolution with
Structured Perceptron
</title>
<author confidence="0.990031">
Jun Araki and Teruko Mitamura
</author>
<affiliation confidence="0.897236666666667">
Language Technologies Institute
Carnegie Mellon University
Pittsburgh, PA 15213, USA
</affiliation>
<email confidence="0.9995">
{junaraki,teruko}@cs.cmu.edu
</email>
<sectionHeader confidence="0.997398" genericHeader="abstract">
Abstract
</sectionHeader>
<bodyText confidence="0.9997825625">
Events and their coreference offer use-
ful semantic and discourse resources.
We show that the semantic and dis-
course aspects of events interact with each
other. However, traditional approaches ad-
dressed event extraction and event coref-
erence resolution either separately or se-
quentially, which limits their interactions.
This paper proposes a document-level
structured learning model that simultane-
ously identifies event triggers and resolves
event coreference. We demonstrate that
the joint model outperforms a pipelined
model by 6.9 BLANC F1 and 1.8 CoNLL
F1 points in event coreference resolution
using a corpus in the biology domain.
</bodyText>
<sectionHeader confidence="0.999516" genericHeader="introduction">
1 Introduction
</sectionHeader>
<bodyText confidence="0.9999635">
Events convey semantic information such as who
did what to whom where and when. They also
corefer to each other, playing a role of discourse
connection points to form a coherent story. These
aspects of events have been already utilized in
a wide variety of natural language processing
(NLP) applications, such as automated population
of knowledge bases (Ji and Grishman, 2011), topic
detection and tracking (Allan, 2002), question an-
swering (Bikel and Castelli, 2008), text summa-
rization (Li et al., 2006), and contradiction detec-
tion (de Marneffe et al., 2008). This fact illustrates
the importance of event extraction and event coref-
erence resolution.
Those semantic and discourse aspects of events
are not independent from each other, and in fact
often work in interactive manners. We give two
examples of the interactions:
</bodyText>
<footnote confidence="0.9563516">
(1) British bank Barclays had agreed to buy(E1) Spanish
rival Banco Zaragozano for 1.14 billion euros. The
combination(E2) of the banking operations of
Barclays Spain and Zaragozano will bring together
two complementary businesses.
</footnote>
<bodyText confidence="0.980899035714286">
(2) The Palestinian Authority condemned the attack(E3),
saying it(E4) would divert international sympathy
away from the far higher Palestinian civilian death toll.
E1 corefers to E2, and E3 does to E4. E2 is more
abstract than E1, and has less evidence of being
an event. E4 is a pronoun, and thus may seem
to refer to an entity rather than an event. Thus,
E2 and E4 are relatively difficult to be recognized
as events by themselves. However, event coref-
erence E1-E2, which is supported primarily by
E2’s participants Barclays and Zaragozano shared
with E1, helps determine that E2 is an event. The
same logic applies to E3 and E4. On the other
hand, previous works typically rely on a pipelined
model that extracts events (e.g., E1 and E3) at
the first stage, and then resolves event corefer-
ence at the second stage. Although this modularity
is preferable from development perspectives, the
pipelined model limits the interactions. That is,
the first stage alone is unlikely to detect E2 and E4
as events due to the difficulties described above.
These missing events make it impossible for the
second stage to resolve event coreference E1-E2
and E3-E4.
In this work, we address the problem using the
ProcessBank corpus (Berant et al., 2014). Follow-
ing the terminology defined in the corpus, we in-
troduce several terms:
</bodyText>
<listItem confidence="0.9965153">
• Event: an abstract representation of a change of state,
independent from particular texts.
• Event trigger: main word(s) in text, typically a verb or
a noun that most clearly expresses an event.
• Event arguments: participants or attributes in text,
typically nouns, that are involved in an event.
• Event mention: a clause in text that describes an event,
and includes both a trigger and arguments.
• Event coreference: a linguistic phenomenon that two
event mentions refer to the same event.
</listItem>
<bodyText confidence="0.99807875">
We aim to explore the interactions between event
mentions and event coreference. As a first step to-
ward the goal, we focus on the task of identifying
event triggers and resolving event coreference, and
</bodyText>
<page confidence="0.949229">
2074
</page>
<note confidence="0.652612">
Proceedings of the 2015 Conference on Empirical Methods in Natural Language Processing, pages 2074–2080,
Lisbon, Portugal, 17-21 September 2015. c�2015 Association for Computational Linguistics.
</note>
<bodyText confidence="0.988864454545455">
propose a document-level joint learning model us-
ing structured perceptron (Collins, 2002) that si-
multaneously predicts them. Our assumption is
that the joint model is able to capture the interac-
tions between event triggers and event coreference
adequately, and such comprehensive decision im-
proves the system performance. For instance, the
joint model is likely to extract E2 as well as E1
successfully via their event coreference by simul-
taneously looking at coreference features.
Our contributions are as follows:
</bodyText>
<listItem confidence="0.998889777777778">
1. This is the first work that simultaneously pre-
dicts event triggers and event coreference us-
ing a single joint model. At the core of the
model is a document-level structured percep-
tron algorithm that learns event triggers and
event coreference jointly.
2. The incremental token-based prediction in
joint decoding poses a challenge of synchro-
nizing the assignments of event triggers and
coreference. To avoid this problem, we pro-
pose an incremental decoding algorithm that
combines the segment-based decoding and
best-first clustering algorithm.
3. Our experiments indicate that the joint model
achieves a substantial performance gain in
event coreference resolution with a corpus
in the biology domain, as compared to a
pipelined model.
</listItem>
<sectionHeader confidence="0.999583" genericHeader="related work">
2 Related Work
</sectionHeader>
<bodyText confidence="0.999959408163266">
No previous work deals with event extraction and
event coreference resolution simultaneously. We
thus describe how these two tasks have been ad-
dressed separately, and how joint structured learn-
ing has been studied in other NLP tasks.
Event extraction has been studied mainly in
the newswire domain and the biomedical domain
as the task of detecting event triggers and deter-
mining their event types and arguments. In the for-
mer domain, most work took a pipelined approach
where local classifiers identify triggers first, and
then detect arguments (Ji and Grishman, 2008;
Liao and Grishman, 2010; Hong et al., 2011). Li et
al. (2013) presented a structured perceptron model
to detect triggers and arguments jointly. Simi-
larly, joint dependencies in events were also ad-
dressed in the latter domain (Poon and Vander-
wende, 2010; McClosky et al., 2011; Riedel and
McCallum, 2011; Venugopal et al., 2014). How-
ever, none of them incorporated event coreference
into their model.
Event coreference resolution is more chal-
lenging and less explored. To set up event triggers
as a starting point of the task, some works use hu-
man annotation in a corpus (Bejan and Harabagiu,
2014; Liu et al., 2014), and others use the output
of a separate event extraction system (Lee et al.,
2012). Berant et al. (2014) presented a model that
jointly predicts event arguments and event coref-
erence (as well as other relations between event
triggers). However, none of them tries to predict
event triggers and event coreference jointly.
Joint structured learning has been applied
to several NLP tasks, such as word segmenta-
tion and part-of-speech (POS) tagging (Zhang and
Clark, 2008a), POS tagging and dependency pars-
ing (Bohnet and Nivre, 2012), dependency pars-
ing and semantic role labeling (Johansson and
Nugues, 2008), the extraction of event triggers and
arguments (Li et al., 2013), and the extraction of
entity mentions and relations (Li and Ji, 2014).
Their underlying ideas are similar to ours. That
is, one can train a structured learning model to
globally capture the interactions between two rel-
evant tasks via a certain kind of structure, while
making predictions specifically for these respec-
tive tasks. However, no prior work has studied
the interactions between event trigger identifica-
tion and event coreference resolution.
</bodyText>
<sectionHeader confidence="0.996296" genericHeader="method">
3 Approach
</sectionHeader>
<bodyText confidence="0.999995833333333">
We formalize the extraction of event triggers and
event coreference as a problem of structured pre-
diction. The output structure is a document-level
event graph where each node represents an event
trigger, and each edge represents an event corefer-
ence link between two event triggers.
</bodyText>
<subsectionHeader confidence="0.998289">
3.1 Corpus
</subsectionHeader>
<bodyText confidence="0.999987083333333">
The ProcessBank corpus consists of 200 para-
graphs from the textbook Biology (Campbell and
Reece, 2005). Table 1 shows statistics of our data
splits. The original corpus provides 150 para-
graphs as training data, and we split them into 120
and 30 for our training and development, respec-
tively. We chose ProcessBank instead of a larger
corpus such as the Automatic Content Extraction
(ACE) 2005 corpus for the following two reasons.
First, the human annotation of event coreference
links in ProcessBank enables us to apply the best-
first clustering directly; on the other hand, this is
</bodyText>
<page confidence="0.943374">
2075
</page>
<bodyText confidence="0.999520538461538">
not readily feasible in ACE 2005 since it anno-
tates event coreference as clusters, and gold stan-
dard event coreference links required for the best-
first clustering are not available. Second, event
coreference resolution using ProcessBank is novel
since almost no previous work on the task used
that corpus. The only exception could be (Berant
et al., 2014), where they extracted several types of
relations between event triggers, including event
coreference. However, they did not report any
performance scores of their system specifically on
event coreference, and thus their work is not com-
parable to ours.
</bodyText>
<table confidence="0.999322">
Train Dev Test Total
# of paragraphs 120 30 50 200
# of event triggers 823 224 356 1403
# of event coreferences 73 28 30 131
</table>
<tableCaption confidence="0.999883">
Table 1: Statistics of our dataset.
</tableCaption>
<bodyText confidence="0.999722428571429">
Unlike previous work (Berant et al., 2014; Li
et al., 2013), we explicitly allow an event trigger
to have multiple tokens, such as verb phrase ‘look
into’ and compound proper noun ‘World War II’.
This is a more realistic setting for event trigger
identification since in general there are a consid-
erable number of multi-token event triggers1.
</bodyText>
<subsectionHeader confidence="0.999802">
3.2 Event Graph Learning
</subsectionHeader>
<bodyText confidence="0.99998055">
Let x denote an input document with n to-
kens where xi is the i-th token in the docu-
ment. For event graph learning, we use structured
perceptron (Collins, 2002), and average weights
to reduce overfitting as suggested in (Collins,
2002). The algorithm involves decoding to gener-
ate the best event graph for each input document.
We elaborate on our decoding algorithm in Sec-
tion 3.3. Since an event graph has an exponen-
tially large search space, we use beam search to
approximate exact inference. We extract a range
of features by using Stanford CoreNLP (Manning
et al., 2014), MATE (Bj¨orkelund et al., 2009),
OpenNLP2, Nomlex (Macleod et al., 1998), and
Levin verb classes (Levin, 1993). For brevity, we
provide details of the structured perceptron algo-
rithm and features in the supplementary material.
We use the standard-update strategy in our
structured perceptron model. As variants of struc-
tured perceptron, one could employ the early up-
</bodyText>
<footnote confidence="0.999806666666667">
1For example, around 13.4% of the 1403 event triggers in
ProcessBank have multiple tokens.
2http://opennlp.apache.org/
</footnote>
<bodyText confidence="0.999897653846154">
date (Collins and Roark, 2004) and max-violation
update (Huang et al., 2012) to our model. Our
initial experiments indicated that early updates
happen too early to gain sufficient feedback on
weights from entire documents in training exam-
ples, ending up with a poorer performance than
the standard update. This contrasts with the fact
that the early-update strategy was successfully ap-
plied to other NLP tasks such as constituent pars-
ing (Collins and Roark, 2004) and dependency
parsing (Zhang and Clark, 2008b). The main rea-
son why the early update fell short of the stan-
dard update in our setting is that joint event trigger
identification and event coreference resolution is a
much more difficult task since they require more
complex knowledge and argument structures. Due
to the difficultly of the task, it is also very difficult
to develop such an effective feature set that beam
search can explore the search space of an entire
document thoroughly with early updates. This ob-
servation follows (Bj¨orkelund and Kuhn, 2014) on
entity coreference resolution. In contrast, the max-
violation update showed almost the same perfor-
mance as the standard update on the development
data. From these results, we chose the standard-
update strategy for simplicity.
</bodyText>
<subsectionHeader confidence="0.998203">
3.3 Joint Decoding
</subsectionHeader>
<bodyText confidence="0.999769260869565">
Given that an event trigger has one or more to-
kens, event trigger identification could be solved
as a token-level sequential labeling problem with
BIO or BILOU scheme in the same way as named
entity recognition (Ratinov and Roth, 2009). If
one uses this approach, a beam state may repre-
sent a partial assignment of an event trigger. How-
ever, event coreference can be explored only from
complete assignments of an event trigger. Thus,
one would need to synchronize the search process
of event coreference by comparing event corefer-
ences from the complete assignment at a certain
position with those from complete assignments at
following positions. This makes it complicated
to implement the formalization of token-level se-
quential labeling for joint decoding in our task.
One possible way to avoid this problem is to ex-
tract event trigger candidates with a preference on
high recall first, and then search event coreference
from those candidates, regarding them as com-
plete assignments of an event trigger. This recall-
oriented pre-filtering is often used in entity coref-
erence resolution (Lee et al., 2013; Bj¨orkelund
</bodyText>
<page confidence="0.972211">
2076
</page>
<bodyText confidence="0.7982296">
Algorithm 1 Joint decoding for event triggers and
coreference with beam search.
Input: input document x = (x1, x2, ... , xn)
Input: beam width k, max length of event trigger l„t x
Output: best event graph yˆ for x
</bodyText>
<listItem confidence="0.9922515">
1: initialize empty beam history B[1..n]
2: for i ← 1..n do
3: for l ← 1..l„t xdo
4: for y ∈ B[i − l] do
5: e ← CREATEEVENTTRIGGER(l, i).
6: APPENDEVENTTRIGGER(y, e)
7: B[i] ← k-BEST(B[i] ∪ y)
8: for j ← 1..i − 1 do
9: c ← CREATEEVENTCOREF(j, e).
10: ADDEVENTCOREF(y, c)
11: B[i] ← k-BEST(B[i] ∪ y)
12: return B[n][0]
</listItem>
<bodyText confidence="0.998033047619048">
and Farkas, 2012). In our initial experiments, we
observed that our rule-based filter gained around
97% recall, but extracted around 12,400 false posi-
tives against 823 true positives in the training data.
This made it difficult for our structured perceptron
to learn event triggers, which underperformed on
event coreference resolution.
We, therefore, employ segment-based decod-
ing with multiple-beam search (Zhang and Clark,
2008a; Li and Ji, 2014) for event trigger identi-
fication, and combine it with the best-first clus-
tering (Ng and Cardie, 2002) for event coref-
erence resolution in document-level joint decod-
ing. The key idea of segment-based decoding with
multiple-beam search is to keep previous beam
states available, and use them to form segments
from previous positions to the current position.
Let lmax denote the upper bound on the number
of tokens in one event trigger. The k-best partial
structures (event subgraphs) in beam B at the j-th
token is computed as follows:
</bodyText>
<equation confidence="0.99836">
B[j] = k-BEST
yE{y[1:j−l]EB[j−l], y[j−l+1,j]=s1
</equation>
<bodyText confidence="0.999861636363636">
where 1 &lt; l &lt; lmax, y[1:j] is an event subgraph
ending at the j-th token, and y[j−l+1,j] = s means
that partial structure y[j−l+1,j] is a segment, i.e.,
an event trigger candidate with a subsequence of
tokens x[j−l+1,j]. This approximates Viterbi de-
coding with beam search.
The best-first clustering incrementally makes
coreference decisions by selecting the most likely
antecedent for each trigger. Our joint decoding
algorithm makes use of the incremental process
to combine the segment-based decoding and best-
first clustering. Algorithm 1 shows the summary
of the joint decoding algorithm. Line 3 - 7 imple-
ments the segment-based decoding, and line 8 - 11
implements the best-first clustering. Once a new
event trigger is appended to an event subgraph at
line 6, the decoder uses it as a referring mention
regardless of whether the event subgraph is in the
beam, and seeks the best antecedent for it. This
enables the joint model to make a more global
decision on event trigger identification and event
coreference decision, as described in Section 1.
</bodyText>
<sectionHeader confidence="0.99788" genericHeader="method">
4 Experimental Settings
</sectionHeader>
<bodyText confidence="0.999986153846154">
When training our model, we observed that 20-
iteration training almost reached convergence, and
thus we set the number of iterations to 20. We
set lmax to 6 because we observed that the longest
event trigger in the entire ProcessBank corpus has
six tokens. When tuning beam width k on the de-
velopment set, large beam width did not give us a
significant performance difference. We attribute
this result to the small size of the development
data. In particular, the development data has only
28 event coreferences, which makes it difficult to
reveal the effect of beam width. We thus set k to 1
in our experiments.
</bodyText>
<subsectionHeader confidence="0.994236">
4.1 Baseline Systems
</subsectionHeader>
<bodyText confidence="0.999578066666667">
Our baseline is a pipelined model that divides the
event trigger decoding and event coreference de-
coding in Algorithm 1 into two separate stages.
It uses the same structured perceptron with the
same hyperparameters and feature templates. We
choose this baseline because it clearly reveals the
effectiveness of the joint model by focusing only
on the architectural difference. One could develop
other baseline systems. One of them is a determin-
istic sieve-based approach by Lee et al. (2013). A
natural extension to the approach for performing
event trigger identification as well as event coref-
erence resolution would be to develop additional
sieves to classify singletons into real event triggers
or spurious ones. We leave it for future work.
</bodyText>
<subsectionHeader confidence="0.945312">
4.2 Evaluation
</subsectionHeader>
<bodyText confidence="0.999936555555555">
We evaluate our system using a reference
implementation of coreference scoring algo-
rithms (Pradhan et al., 2014; Luo et al., 2014).
As for event trigger identification, this scorer
computes precision (P), recall (R), and the F1
score. With respect to event coreference reso-
lution, the scorer computes MUC (Vilain et al.,
1995), B3 (Bagga and Baldwin, 1998), two CEAF
metrics CEAFm and CEAFe (Luo, 2005), and
</bodyText>
<equation confidence="0.923">
Φ(x, y) · w
</equation>
<page confidence="0.94187">
2077
</page>
<table confidence="0.99892175">
MUC B3 CEAF. CEAF, BLANC CoNLL
System R P F1 R P F1 R P F1 R P F1 R P F1 F1
Baseline 26.66 19.51 22.53 55.47 58.64 57.01 53.08 60.38 56.50 52.68 63.14 57.44 30.13 25.10 25.05 45.66
Joint 20.00 37.50 26.08 53.37 63.36 57.93 53.93 62.95 58.09 55.06 62.11 58.38 27.51 38.43 31.91 47.45
</table>
<tableCaption confidence="0.999696">
Table 2: Results of event coreference resolution. ‘Baseline’ refers to the second stage of our baseline.
</tableCaption>
<bodyText confidence="0.994994">
BLANC (Recasens and Hovy, 2011) extended by
Luo et al. (2014). We also report the CoNLL av-
erage (Denis and Baldridge, 2009), which is the
average of MUC F1, B3 F1, and CEAF, F1.
</bodyText>
<sectionHeader confidence="0.999753" genericHeader="evaluation">
5 Results and Discussions
</sectionHeader>
<bodyText confidence="0.9999620625">
We first show the result of event coreference reso-
lution on the test data in Table 2. The joint model
outperforms the baseline by 6.9 BLANC F1 and
1.8 CoNLL F1 points. We observed that this over-
all performance gain comes largely from a preci-
sion gain, more specifically, substantially reduced
false positives. We explain the superiority of the
joint model as follows. In the baseline, the second
stage uses the output of the first stage. Since event
triggers are fixed at this point, the baseline ex-
plores coreference links only between these event
triggers. In contrast, the joint model seeks event
triggers and event coreference simultaneously, and
thus it explores a larger number of false positives
in the search process, thereby learning to penalize
false positives more adequately than the baseline.
</bodyText>
<table confidence="0.996094333333333">
System Recall Precision F1
Baseline 57.02 64.85 60.68
Joint 55.89 65.24 60.21
</table>
<tableCaption confidence="0.8218495">
Table 3: Results of event trigger identification.
‘Baseline’ refers to the first stage of our baseline.
</tableCaption>
<bodyText confidence="0.975202888888889">
Table 3 shows the results of event trigger iden-
tification on the test data. We observed that the
joint model also reduced false positives, similarly
in event coreference resolution. However, its im-
provement on precision is small, ending up with
almost the same F1 point as the baseline. We spec-
ulate that this is due to the small size of the corpus,
and the joint model was unable to show its advan-
tages in event trigger identification.
Below are two error cases in event coreference
resolution, where our model fails to resolve E5-
E6 and E7-E8. The model was unable to ade-
quately extract features for both event triggers and
event coreference, particularly because their sur-
face strings are not present in training data, they
are lexically and syntactically different, and they
do not share key semantic roles (e.g., agents and
patients) in a clear argument structure.
</bodyText>
<listItem confidence="0.997522">
(3) When the cell is stimulated, gated channels open that
facilitate Na+ diffusion(E5). Sodium ions then
”fall”(E6) down their electrochemical gradient, .. .
(4) The next seven steps decompose(E7) the citrate back
to oxaloacetate. It is this regeneration(E8) of
oxaloacetate that makes this process a cycle.
</listItem>
<sectionHeader confidence="0.959083" genericHeader="conclusions">
6 Conclusion and Future Work
</sectionHeader>
<bodyText confidence="0.99999">
We present a joint structured prediction model for
event trigger identification and event coreference
resolution. To our knowledge, this is the first work
that solves these two tasks simultaneously. Our
experiment shows that the proposed method ef-
fectively penalizes false positives in joint search,
thereby outperforming a pipelined model substan-
tially in event coreference resolution.
There are a number of avenues for future work.
One can further ensure the advantage of the joint
model using a larger corpus. Our preliminary ex-
periment on the ACE 2005 corpus shows that due
to its larger document size and event types, one
will need to reduce training time by a distributed
learning algorithm such as mini-batches (Zhao and
Huang, 2013). Another future work is to incorpo-
rate other components of events into the model.
These include event types, event arguments, and
other relations such as subevents. One could lever-
age them as other learning targets or constraints,
and investigate further benefits of joint modeling.
</bodyText>
<sectionHeader confidence="0.98692" genericHeader="acknowledgments">
Acknowledgments
</sectionHeader>
<bodyText confidence="0.999599083333333">
This research was supported in part by DARPA
grant FA8750-12-2-0342 funded under the Deep
Exploration and Filtering of Text (DEFT) Pro-
gram, and by U.S. Army Research Office (ARO)
grant W911NF-14-1-0436 under the Reading, Ex-
traction, and Assembly of Pathways for Eviden-
tiary Reading (REAPER) Program. Any opinion,
findings, and conclusions or recommendations ex-
pressed in this material are those of the authors
and do not necessarily reflect the view of DARPA,
ARO, or the U.S. government. Jun Araki is partly
supported by a Funai Overseas Scholarship.
</bodyText>
<page confidence="0.993929">
2078
</page>
<sectionHeader confidence="0.95945" genericHeader="references">
References
</sectionHeader>
<reference confidence="0.99565276635514">
James Allan. 2002. Topic Detection and Tracking:
Event-based Information Organization. Kluwer
Academic Publishers.
Amit Bagga and Breck Baldwin. 1998. Algorithms
for scoring coreference chains. In Proceedings of
LREC 1998 Workshop on Linguistics Coreference,
pages 563–566.
Cosmin Adrian Bejan and Sanda M. Harabagiu. 2014.
Unsupervised event coreference resolution. Compu-
tational Linguistics, 40(2):311–347.
Jonathan Berant, Vivek Srikumar, Pei-Chun Chen,
Abby Vander Linden, Brittany Harding, Brad
Huang, Peter Clark, and Christopher D. Manning.
2014. Modeling biological processes for reading
comprehension. In Proceedings of EMNLP 2014,
pages 1499–1510.
Daniel M. Bikel and Vittorio Castelli. 2008. Event
matching using the transitive closure of dependency
relations. In Proceedings of ACL 2008, pages 145–
148.
Anders Bj¨orkelund and Rich´ard Farkas. 2012. Data-
driven multilingual coreference resolution using re-
solver stacking. In Proceedings of EMNLP/CoNLL
2012, pages 49–55.
Anders Bj¨orkelund and Jonas Kuhn. 2014. Learn-
ing structured perceptrons for coreference resolution
with latent antecedents and non-local features. In
Proceedings ofACL 2014, pages 47–57.
Anders Bj¨orkelund, Love Hafdell, and Pierre Nugues.
2009. Multilingual semantic role labeling. In Pro-
ceedings of CoNLL 2009, pages 43–48.
Bernd Bohnet and Joakim Nivre. 2012. A transition-
based system for joint part-of-speech tagging and la-
beled non-projective dependency parsing. In Pro-
ceedings of EMNLP/CoNLL 2012, pages 1455–
1465.
Neil Campbell and Jane Reece. 2005. Biology. Ben-
jamin Cummings.
Michael Collins and Brian Roark. 2004. Incremental
parsing with the perceptron algorithm. In Proceed-
ings ofACL 2004, pages 111–118.
Michael Collins. 2002. Discriminative training meth-
ods for Hidden Markov Models: Theory and exper-
iments with perceptron algorithms. In Proceedings
of EMNLP 2002, pages 1–8.
Marie-Catherine de Marneffe, Anna N. Rafferty, and
Christopher D. Manning. 2008. Finding contra-
dictions in text. In Proceedings of ACL-HLT 2008,
pages 1039–1047.
Pascal Denis and Jason Baldridge. 2009. Global joint
models for coreference resolution and named entity
classification. Procesamiento del Lenguaje Natural,
42:87–96.
Yu Hong, Jianfeng Zhang, Bin Ma, Jianmin Yao,
Guodong Zhou, and Qiaoming Zhu. 2011. Us-
ing cross-entity inference to improve event extrac-
tion. In Proceedings of ACL-HLT 2011, pages
1127–1136.
Liang Huang, Suphan Fayong, and Yang Guo. 2012.
Structured perceptron with inexact search. In Pro-
ceedings of NAACL-HLT 2012, pages 142–151.
Heng Ji and Ralph Grishman. 2008. Refining event ex-
traction through cross-document inference. In Pro-
ceedings of ACL-HLT 2008, pages 254–262.
Heng Ji and Ralph Grishman. 2011. Knowledge
base population: Successful approaches and chal-
lenges. In Proceedings of ACL-HLT 2011, pages
1148–1158.
Richard Johansson and Pierre Nugues. 2008.
Dependency-based semantic role labeling of Prop-
Bank. In Proceedings of EMNLP 2008, pages 69–
78.
Heeyoung Lee, Marta Recasens, Angel Chang, Mihai
Surdeanu, and Dan Jurafsky. 2012. Joint entity and
event coreference resolution across documents. In
Proceedings of EMNLP/CoNLL 2012, pages 489–
500.
Heeyoung Lee, Angel Chang, Yves Peirsman,
Nathanael Chambers, Mihai Surdeanu, and Dan Ju-
rafsky. 2013. Deterministic coreference resolu-
tion based on entity-centric, precision-ranked rules.
Computational Linguistics, 39(4):885–916.
Beth Levin. 1993. English Verb Classes and Alterna-
tion: A Preliminary Investigation. The University of
Chicago Press.
Qi Li and Heng Ji. 2014. Incremental joint extraction
of entity mentions and relations. In Proceedings of
ACL 2014, pages 402–412.
Wenjie Li, Mingli Wu, Qin Lu, Wei Xu, and Chunfa
Yuan. 2006. Extractive summarization using
inter- and intra- event relevance. In Proceedings of
ACL/COLING 2006, pages 369–376.
Qi Li, Heng Ji, and Liang Huang. 2013. Joint event
extraction via structured prediction with global fea-
tures. In Proceedings ofACL 2013, pages 73–82.
Shasha Liao and Ralph Grishman. 2010. Using doc-
ument level cross-event inference to improve event
extraction. In Proceedings ofACL 2010, pages 789–
797.
Zhengzhong Liu, Jun Araki, Eduard Hovy, and Teruko
Mitamura. 2014. Supervised within-document
event coreference using information propagation. In
Proceedings of LREC 2014.
Xiaoqiang Luo, Sameer Pradhan, Marta Recasens, and
Eduard Hovy. 2014. An extension of BLANC
to system mentions. In Proceedings of ACL 2014,
pages 24–29.
</reference>
<page confidence="0.891523">
2079
</page>
<reference confidence="0.999926694915254">
Xiaoqiang Luo. 2005. On coreference resolution per-
formance metrics. In Proceedings of HLT/EMNLP
2005, pages 25–32.
Catherine Macleod, Ralph Grishman, Adam Meyers,
Leslie Barrett, and Ruth Reeves. 1998. Nomlex: A
lexicon of nominalizations. In Proceedings of EU-
RALEX 1998, pages 187–193.
Christopher Manning, Mihai Surdeanu, John Bauer,
Jenny Finkel, Steven Bethard, and David McClosky.
2014. The Stanford CoreNLP natural language pro-
cessing toolkit. In Proceedings ACL 2014: System
Demonstrations, pages 55–60.
David McClosky, Mihai Surdeanu, and Christopher
Manning. 2011. Event extraction as dependency
parsing. In Proceedings of ACL-HLT 2011, pages
1626–1635.
Vincent Ng and Claire Cardie. 2002. Improving ma-
chine learning approaches to coreference resolution.
In Proceedings of ACL 2002, pages 104–111.
Hoifung Poon and Lucy Vanderwende. 2010. Joint
inference for knowledge extraction from biomedi-
cal literature. In Proceedings of NAACL-HLT 2010,
pages 813–821.
Sameer Pradhan, Xiaoqiang Luo, Marta Recasens, Ed-
uard Hovy, Vincent Ng, and Michael Strube. 2014.
Scoring coreference partitions of predicted men-
tions: A reference implementation. In Proceedings
of ACL 2014, pages 30–35.
Lev Ratinov and Dan Roth. 2009. Design challenges
and misconceptions in named entity recognition. In
Proceedings of CoNLL 2009, pages 147–155.
Marta Recasens and Eduard Hovy. 2011. BLANC:
Implementing the Rand index for coreference eval-
uation. Natural Language Engineering, 17(4):485–
510.
Sebastian Riedel and Andrew McCallum. 2011. Fast
and robust joint models for biomedical event extrac-
tion. In Proceedings of EMNLP 2011, pages 1–12.
Deepak Venugopal, Chen Chen, Vibhav Gogate, and
Vincent Ng. 2014. Relieving the computational
bottleneck: Joint inference for event extraction
with high-dimensional features. In Proceedings of
EMNLP 2014, pages 831–843.
Marc Vilain, John Burger, John Aberdeen, Dennis Con-
nolly, and Lynette Hirschman. 1995. A model-
theoretic coreference scoring scheme. In Proceed-
ings of MUC-6, pages 45–52.
Yue Zhang and Stephen Clark. 2008a. Joint word seg-
mentation and POS tagging using a single percep-
tron. In Proceedings of ACL-HLT 2008, pages 888–
896.
Yue Zhang and Stephen Clark. 2008b. A tale of two
parsers: Investigating and combining graph-based
and transition-based dependency parsing. In Pro-
ceedings of EMNLP 2008, pages 562–571.
Kai Zhao and Liang Huang. 2013. Minibatch and par-
allelization for online large margin structured learn-
ing. In Proceedings of NAACL-HLT 2013, pages
370–379.
</reference>
<page confidence="0.991243">
2080
</page>
</variant>
</algorithm>
<algorithm name="ParsHed" version="110505">
<variant no="0" confidence="0.764852">
<title confidence="0.9961935">Joint Event Trigger Identification and Event Coreference Resolution with Structured Perceptron</title>
<author confidence="0.830165">Araki</author>
<affiliation confidence="0.897225">Language Technologies Carnegie Mellon</affiliation>
<address confidence="0.992686">Pittsburgh, PA 15213,</address>
<abstract confidence="0.998987">Events and their coreference offer useful semantic and discourse resources. We show that the semantic and discourse aspects of events interact with each other. However, traditional approaches addressed event extraction and event coreference resolution either separately or sequentially, which limits their interactions. This paper proposes a document-level structured learning model that simultaneously identifies event triggers and resolves event coreference. We demonstrate that the joint model outperforms a pipelined model by 6.9 BLANC F1 and 1.8 CoNLL F1 points in event coreference resolution using a corpus in the biology domain.</abstract>
</variant>
</algorithm>
<algorithm name="ParsCit" version="110505">
<citationList>
<citation valid="true">
<authors>
<author>James Allan</author>
</authors>
<title>Topic Detection and Tracking: Event-based Information Organization.</title>
<date>2002</date>
<publisher>Kluwer Academic Publishers.</publisher>
<contexts>
<context position="1320" citStr="Allan, 2002" startWordPosition="190" endWordPosition="191"> We demonstrate that the joint model outperforms a pipelined model by 6.9 BLANC F1 and 1.8 CoNLL F1 points in event coreference resolution using a corpus in the biology domain. 1 Introduction Events convey semantic information such as who did what to whom where and when. They also corefer to each other, playing a role of discourse connection points to form a coherent story. These aspects of events have been already utilized in a wide variety of natural language processing (NLP) applications, such as automated population of knowledge bases (Ji and Grishman, 2011), topic detection and tracking (Allan, 2002), question answering (Bikel and Castelli, 2008), text summarization (Li et al., 2006), and contradiction detection (de Marneffe et al., 2008). This fact illustrates the importance of event extraction and event coreference resolution. Those semantic and discourse aspects of events are not independent from each other, and in fact often work in interactive manners. We give two examples of the interactions: (1) British bank Barclays had agreed to buy(E1) Spanish rival Banco Zaragozano for 1.14 billion euros. The combination(E2) of the banking operations of Barclays Spain and Zaragozano will bring </context>
</contexts>
<marker>Allan, 2002</marker>
<rawString>James Allan. 2002. Topic Detection and Tracking: Event-based Information Organization. Kluwer Academic Publishers.</rawString>
</citation>
<citation valid="true">
<authors>
<author>Amit Bagga</author>
<author>Breck Baldwin</author>
</authors>
<title>Algorithms for scoring coreference chains.</title>
<date>1998</date>
<booktitle>In Proceedings of LREC 1998 Workshop on Linguistics Coreference,</booktitle>
<pages>563--566</pages>
<contexts>
<context position="17643" citStr="Bagga and Baldwin, 1998" startWordPosition="2835" endWordPosition="2838">A natural extension to the approach for performing event trigger identification as well as event coreference resolution would be to develop additional sieves to classify singletons into real event triggers or spurious ones. We leave it for future work. 4.2 Evaluation We evaluate our system using a reference implementation of coreference scoring algorithms (Pradhan et al., 2014; Luo et al., 2014). As for event trigger identification, this scorer computes precision (P), recall (R), and the F1 score. With respect to event coreference resolution, the scorer computes MUC (Vilain et al., 1995), B3 (Bagga and Baldwin, 1998), two CEAF metrics CEAFm and CEAFe (Luo, 2005), and Φ(x, y) · w 2077 MUC B3 CEAF. CEAF, BLANC CoNLL System R P F1 R P F1 R P F1 R P F1 R P F1 F1 Baseline 26.66 19.51 22.53 55.47 58.64 57.01 53.08 60.38 56.50 52.68 63.14 57.44 30.13 25.10 25.05 45.66 Joint 20.00 37.50 26.08 53.37 63.36 57.93 53.93 62.95 58.09 55.06 62.11 58.38 27.51 38.43 31.91 47.45 Table 2: Results of event coreference resolution. ‘Baseline’ refers to the second stage of our baseline. BLANC (Recasens and Hovy, 2011) extended by Luo et al. (2014). We also report the CoNLL average (Denis and Baldridge, 2009), which is the avera</context>
</contexts>
<marker>Bagga, Baldwin, 1998</marker>
<rawString>Amit Bagga and Breck Baldwin. 1998. Algorithms for scoring coreference chains. In Proceedings of LREC 1998 Workshop on Linguistics Coreference, pages 563–566.</rawString>
</citation>
<citation valid="true">
<authors>
<author>Cosmin Adrian Bejan</author>
<author>Sanda M Harabagiu</author>
</authors>
<title>Unsupervised event coreference resolution.</title>
<date>2014</date>
<journal>Computational Linguistics,</journal>
<volume>40</volume>
<issue>2</issue>
<contexts>
<context position="6613" citStr="Bejan and Harabagiu, 2014" startWordPosition="1033" endWordPosition="1036">ents (Ji and Grishman, 2008; Liao and Grishman, 2010; Hong et al., 2011). Li et al. (2013) presented a structured perceptron model to detect triggers and arguments jointly. Similarly, joint dependencies in events were also addressed in the latter domain (Poon and Vanderwende, 2010; McClosky et al., 2011; Riedel and McCallum, 2011; Venugopal et al., 2014). However, none of them incorporated event coreference into their model. Event coreference resolution is more challenging and less explored. To set up event triggers as a starting point of the task, some works use human annotation in a corpus (Bejan and Harabagiu, 2014; Liu et al., 2014), and others use the output of a separate event extraction system (Lee et al., 2012). Berant et al. (2014) presented a model that jointly predicts event arguments and event coreference (as well as other relations between event triggers). However, none of them tries to predict event triggers and event coreference jointly. Joint structured learning has been applied to several NLP tasks, such as word segmentation and part-of-speech (POS) tagging (Zhang and Clark, 2008a), POS tagging and dependency parsing (Bohnet and Nivre, 2012), dependency parsing and semantic role labeling (</context>
</contexts>
<marker>Bejan, Harabagiu, 2014</marker>
<rawString>Cosmin Adrian Bejan and Sanda M. Harabagiu. 2014. Unsupervised event coreference resolution. Computational Linguistics, 40(2):311–347.</rawString>
</citation>
<citation valid="true">
<authors>
<author>Jonathan Berant</author>
<author>Vivek Srikumar</author>
<author>Pei-Chun Chen</author>
<author>Abby Vander Linden</author>
<author>Brittany Harding</author>
<author>Brad Huang</author>
<author>Peter Clark</author>
<author>Christopher D Manning</author>
</authors>
<title>Modeling biological processes for reading comprehension.</title>
<date>2014</date>
<booktitle>In Proceedings of EMNLP 2014,</booktitle>
<pages>1499--1510</pages>
<contexts>
<context position="3200" citStr="Berant et al., 2014" startWordPosition="494" endWordPosition="497">E3 and E4. On the other hand, previous works typically rely on a pipelined model that extracts events (e.g., E1 and E3) at the first stage, and then resolves event coreference at the second stage. Although this modularity is preferable from development perspectives, the pipelined model limits the interactions. That is, the first stage alone is unlikely to detect E2 and E4 as events due to the difficulties described above. These missing events make it impossible for the second stage to resolve event coreference E1-E2 and E3-E4. In this work, we address the problem using the ProcessBank corpus (Berant et al., 2014). Following the terminology defined in the corpus, we introduce several terms: • Event: an abstract representation of a change of state, independent from particular texts. • Event trigger: main word(s) in text, typically a verb or a noun that most clearly expresses an event. • Event arguments: participants or attributes in text, typically nouns, that are involved in an event. • Event mention: a clause in text that describes an event, and includes both a trigger and arguments. • Event coreference: a linguistic phenomenon that two event mentions refer to the same event. We aim to explore the int</context>
<context position="6738" citStr="Berant et al. (2014)" startWordPosition="1056" endWordPosition="1059"> to detect triggers and arguments jointly. Similarly, joint dependencies in events were also addressed in the latter domain (Poon and Vanderwende, 2010; McClosky et al., 2011; Riedel and McCallum, 2011; Venugopal et al., 2014). However, none of them incorporated event coreference into their model. Event coreference resolution is more challenging and less explored. To set up event triggers as a starting point of the task, some works use human annotation in a corpus (Bejan and Harabagiu, 2014; Liu et al., 2014), and others use the output of a separate event extraction system (Lee et al., 2012). Berant et al. (2014) presented a model that jointly predicts event arguments and event coreference (as well as other relations between event triggers). However, none of them tries to predict event triggers and event coreference jointly. Joint structured learning has been applied to several NLP tasks, such as word segmentation and part-of-speech (POS) tagging (Zhang and Clark, 2008a), POS tagging and dependency parsing (Bohnet and Nivre, 2012), dependency parsing and semantic role labeling (Johansson and Nugues, 2008), the extraction of event triggers and arguments (Li et al., 2013), and the extraction of entity m</context>
<context position="9009" citStr="Berant et al., 2014" startWordPosition="1418" endWordPosition="1421">f a larger corpus such as the Automatic Content Extraction (ACE) 2005 corpus for the following two reasons. First, the human annotation of event coreference links in ProcessBank enables us to apply the bestfirst clustering directly; on the other hand, this is 2075 not readily feasible in ACE 2005 since it annotates event coreference as clusters, and gold standard event coreference links required for the bestfirst clustering are not available. Second, event coreference resolution using ProcessBank is novel since almost no previous work on the task used that corpus. The only exception could be (Berant et al., 2014), where they extracted several types of relations between event triggers, including event coreference. However, they did not report any performance scores of their system specifically on event coreference, and thus their work is not comparable to ours. Train Dev Test Total # of paragraphs 120 30 50 200 # of event triggers 823 224 356 1403 # of event coreferences 73 28 30 131 Table 1: Statistics of our dataset. Unlike previous work (Berant et al., 2014; Li et al., 2013), we explicitly allow an event trigger to have multiple tokens, such as verb phrase ‘look into’ and compound proper noun ‘World</context>
</contexts>
<marker>Berant, Srikumar, Chen, Linden, Harding, Huang, Clark, Manning, 2014</marker>
<rawString>Jonathan Berant, Vivek Srikumar, Pei-Chun Chen, Abby Vander Linden, Brittany Harding, Brad Huang, Peter Clark, and Christopher D. Manning. 2014. Modeling biological processes for reading comprehension. In Proceedings of EMNLP 2014, pages 1499–1510.</rawString>
</citation>
<citation valid="true">
<authors>
<author>Daniel M Bikel</author>
<author>Vittorio Castelli</author>
</authors>
<title>Event matching using the transitive closure of dependency relations.</title>
<date>2008</date>
<booktitle>In Proceedings of ACL</booktitle>
<pages>145--148</pages>
<contexts>
<context position="1367" citStr="Bikel and Castelli, 2008" startWordPosition="195" endWordPosition="198">el outperforms a pipelined model by 6.9 BLANC F1 and 1.8 CoNLL F1 points in event coreference resolution using a corpus in the biology domain. 1 Introduction Events convey semantic information such as who did what to whom where and when. They also corefer to each other, playing a role of discourse connection points to form a coherent story. These aspects of events have been already utilized in a wide variety of natural language processing (NLP) applications, such as automated population of knowledge bases (Ji and Grishman, 2011), topic detection and tracking (Allan, 2002), question answering (Bikel and Castelli, 2008), text summarization (Li et al., 2006), and contradiction detection (de Marneffe et al., 2008). This fact illustrates the importance of event extraction and event coreference resolution. Those semantic and discourse aspects of events are not independent from each other, and in fact often work in interactive manners. We give two examples of the interactions: (1) British bank Barclays had agreed to buy(E1) Spanish rival Banco Zaragozano for 1.14 billion euros. The combination(E2) of the banking operations of Barclays Spain and Zaragozano will bring together two complementary businesses. (2) The </context>
</contexts>
<marker>Bikel, Castelli, 2008</marker>
<rawString>Daniel M. Bikel and Vittorio Castelli. 2008. Event matching using the transitive closure of dependency relations. In Proceedings of ACL 2008, pages 145– 148.</rawString>
</citation>
<citation valid="true">
<authors>
<author>Anders Bj¨orkelund</author>
<author>Rich´ard Farkas</author>
</authors>
<title>Datadriven multilingual coreference resolution using resolver stacking.</title>
<date>2012</date>
<booktitle>In Proceedings of EMNLP/CoNLL 2012,</booktitle>
<pages>49--55</pages>
<marker>Bj¨orkelund, Farkas, 2012</marker>
<rawString>Anders Bj¨orkelund and Rich´ard Farkas. 2012. Datadriven multilingual coreference resolution using resolver stacking. In Proceedings of EMNLP/CoNLL 2012, pages 49–55.</rawString>
</citation>
<citation valid="true">
<authors>
<author>Anders Bj¨orkelund</author>
<author>Jonas Kuhn</author>
</authors>
<title>Learning structured perceptrons for coreference resolution with latent antecedents and non-local features.</title>
<date>2014</date>
<booktitle>In Proceedings ofACL 2014,</booktitle>
<pages>47--57</pages>
<marker>Bj¨orkelund, Kuhn, 2014</marker>
<rawString>Anders Bj¨orkelund and Jonas Kuhn. 2014. Learning structured perceptrons for coreference resolution with latent antecedents and non-local features. In Proceedings ofACL 2014, pages 47–57.</rawString>
</citation>
<citation valid="true">
<authors>
<author>Anders Bj¨orkelund</author>
<author>Love Hafdell</author>
<author>Pierre Nugues</author>
</authors>
<title>Multilingual semantic role labeling.</title>
<date>2009</date>
<booktitle>In Proceedings of CoNLL</booktitle>
<pages>43--48</pages>
<marker>Bj¨orkelund, Hafdell, Nugues, 2009</marker>
<rawString>Anders Bj¨orkelund, Love Hafdell, and Pierre Nugues. 2009. Multilingual semantic role labeling. In Proceedings of CoNLL 2009, pages 43–48.</rawString>
</citation>
<citation valid="true">
<authors>
<author>Bernd Bohnet</author>
<author>Joakim Nivre</author>
</authors>
<title>A transitionbased system for joint part-of-speech tagging and labeled non-projective dependency parsing.</title>
<date>2012</date>
<booktitle>In Proceedings of EMNLP/CoNLL 2012,</booktitle>
<pages>1455--1465</pages>
<contexts>
<context position="7164" citStr="Bohnet and Nivre, 2012" startWordPosition="1122" endWordPosition="1125">, some works use human annotation in a corpus (Bejan and Harabagiu, 2014; Liu et al., 2014), and others use the output of a separate event extraction system (Lee et al., 2012). Berant et al. (2014) presented a model that jointly predicts event arguments and event coreference (as well as other relations between event triggers). However, none of them tries to predict event triggers and event coreference jointly. Joint structured learning has been applied to several NLP tasks, such as word segmentation and part-of-speech (POS) tagging (Zhang and Clark, 2008a), POS tagging and dependency parsing (Bohnet and Nivre, 2012), dependency parsing and semantic role labeling (Johansson and Nugues, 2008), the extraction of event triggers and arguments (Li et al., 2013), and the extraction of entity mentions and relations (Li and Ji, 2014). Their underlying ideas are similar to ours. That is, one can train a structured learning model to globally capture the interactions between two relevant tasks via a certain kind of structure, while making predictions specifically for these respective tasks. However, no prior work has studied the interactions between event trigger identification and event coreference resolution. 3 Ap</context>
</contexts>
<marker>Bohnet, Nivre, 2012</marker>
<rawString>Bernd Bohnet and Joakim Nivre. 2012. A transitionbased system for joint part-of-speech tagging and labeled non-projective dependency parsing. In Proceedings of EMNLP/CoNLL 2012, pages 1455– 1465.</rawString>
</citation>
<citation valid="true">
<authors>
<author>Neil Campbell</author>
<author>Jane Reece</author>
</authors>
<date>2005</date>
<location>Biology. Benjamin Cummings.</location>
<contexts>
<context position="8168" citStr="Campbell and Reece, 2005" startWordPosition="1279" endWordPosition="1282"> certain kind of structure, while making predictions specifically for these respective tasks. However, no prior work has studied the interactions between event trigger identification and event coreference resolution. 3 Approach We formalize the extraction of event triggers and event coreference as a problem of structured prediction. The output structure is a document-level event graph where each node represents an event trigger, and each edge represents an event coreference link between two event triggers. 3.1 Corpus The ProcessBank corpus consists of 200 paragraphs from the textbook Biology (Campbell and Reece, 2005). Table 1 shows statistics of our data splits. The original corpus provides 150 paragraphs as training data, and we split them into 120 and 30 for our training and development, respectively. We chose ProcessBank instead of a larger corpus such as the Automatic Content Extraction (ACE) 2005 corpus for the following two reasons. First, the human annotation of event coreference links in ProcessBank enables us to apply the bestfirst clustering directly; on the other hand, this is 2075 not readily feasible in ACE 2005 since it annotates event coreference as clusters, and gold standard event corefer</context>
</contexts>
<marker>Campbell, Reece, 2005</marker>
<rawString>Neil Campbell and Jane Reece. 2005. Biology. Benjamin Cummings.</rawString>
</citation>
<citation valid="true">
<authors>
<author>Michael Collins</author>
<author>Brian Roark</author>
</authors>
<title>Incremental parsing with the perceptron algorithm.</title>
<date>2004</date>
<booktitle>In Proceedings ofACL 2004,</booktitle>
<pages>111--118</pages>
<contexts>
<context position="10881" citStr="Collins and Roark, 2004" startWordPosition="1725" endWordPosition="1728">pproximate exact inference. We extract a range of features by using Stanford CoreNLP (Manning et al., 2014), MATE (Bj¨orkelund et al., 2009), OpenNLP2, Nomlex (Macleod et al., 1998), and Levin verb classes (Levin, 1993). For brevity, we provide details of the structured perceptron algorithm and features in the supplementary material. We use the standard-update strategy in our structured perceptron model. As variants of structured perceptron, one could employ the early up1For example, around 13.4% of the 1403 event triggers in ProcessBank have multiple tokens. 2http://opennlp.apache.org/ date (Collins and Roark, 2004) and max-violation update (Huang et al., 2012) to our model. Our initial experiments indicated that early updates happen too early to gain sufficient feedback on weights from entire documents in training examples, ending up with a poorer performance than the standard update. This contrasts with the fact that the early-update strategy was successfully applied to other NLP tasks such as constituent parsing (Collins and Roark, 2004) and dependency parsing (Zhang and Clark, 2008b). The main reason why the early update fell short of the standard update in our setting is that joint event trigger ide</context>
</contexts>
<marker>Collins, Roark, 2004</marker>
<rawString>Michael Collins and Brian Roark. 2004. Incremental parsing with the perceptron algorithm. In Proceedings ofACL 2004, pages 111–118.</rawString>
</citation>
<citation valid="true">
<authors>
<author>Michael Collins</author>
</authors>
<title>Discriminative training methods for Hidden Markov Models: Theory and experiments with perceptron algorithms.</title>
<date>2002</date>
<booktitle>In Proceedings of EMNLP</booktitle>
<pages>1--8</pages>
<contexts>
<context position="4266" citStr="Collins, 2002" startWordPosition="662" endWordPosition="663">oth a trigger and arguments. • Event coreference: a linguistic phenomenon that two event mentions refer to the same event. We aim to explore the interactions between event mentions and event coreference. As a first step toward the goal, we focus on the task of identifying event triggers and resolving event coreference, and 2074 Proceedings of the 2015 Conference on Empirical Methods in Natural Language Processing, pages 2074–2080, Lisbon, Portugal, 17-21 September 2015. c�2015 Association for Computational Linguistics. propose a document-level joint learning model using structured perceptron (Collins, 2002) that simultaneously predicts them. Our assumption is that the joint model is able to capture the interactions between event triggers and event coreference adequately, and such comprehensive decision improves the system performance. For instance, the joint model is likely to extract E2 as well as E1 successfully via their event coreference by simultaneously looking at coreference features. Our contributions are as follows: 1. This is the first work that simultaneously predicts event triggers and event coreference using a single joint model. At the core of the model is a document-level structur</context>
<context position="9950" citStr="Collins, 2002" startWordPosition="1582" endWordPosition="1583"> 224 356 1403 # of event coreferences 73 28 30 131 Table 1: Statistics of our dataset. Unlike previous work (Berant et al., 2014; Li et al., 2013), we explicitly allow an event trigger to have multiple tokens, such as verb phrase ‘look into’ and compound proper noun ‘World War II’. This is a more realistic setting for event trigger identification since in general there are a considerable number of multi-token event triggers1. 3.2 Event Graph Learning Let x denote an input document with n tokens where xi is the i-th token in the document. For event graph learning, we use structured perceptron (Collins, 2002), and average weights to reduce overfitting as suggested in (Collins, 2002). The algorithm involves decoding to generate the best event graph for each input document. We elaborate on our decoding algorithm in Section 3.3. Since an event graph has an exponentially large search space, we use beam search to approximate exact inference. We extract a range of features by using Stanford CoreNLP (Manning et al., 2014), MATE (Bj¨orkelund et al., 2009), OpenNLP2, Nomlex (Macleod et al., 1998), and Levin verb classes (Levin, 1993). For brevity, we provide details of the structured perceptron algorithm a</context>
</contexts>
<marker>Collins, 2002</marker>
<rawString>Michael Collins. 2002. Discriminative training methods for Hidden Markov Models: Theory and experiments with perceptron algorithms. In Proceedings of EMNLP 2002, pages 1–8.</rawString>
</citation>
<citation valid="true">
<authors>
<author>Marie-Catherine de Marneffe</author>
<author>Anna N Rafferty</author>
<author>Christopher D Manning</author>
</authors>
<title>Finding contradictions in text.</title>
<date>2008</date>
<booktitle>In Proceedings of ACL-HLT</booktitle>
<pages>1039--1047</pages>
<marker>de Marneffe, Rafferty, Manning, 2008</marker>
<rawString>Marie-Catherine de Marneffe, Anna N. Rafferty, and Christopher D. Manning. 2008. Finding contradictions in text. In Proceedings of ACL-HLT 2008, pages 1039–1047.</rawString>
</citation>
<citation valid="true">
<authors>
<author>Pascal Denis</author>
<author>Jason Baldridge</author>
</authors>
<title>Global joint models for coreference resolution and named entity classification.</title>
<date>2009</date>
<booktitle>Procesamiento del Lenguaje Natural,</booktitle>
<pages>42--87</pages>
<contexts>
<context position="18223" citStr="Denis and Baldridge, 2009" startWordPosition="2944" endWordPosition="2947">in et al., 1995), B3 (Bagga and Baldwin, 1998), two CEAF metrics CEAFm and CEAFe (Luo, 2005), and Φ(x, y) · w 2077 MUC B3 CEAF. CEAF, BLANC CoNLL System R P F1 R P F1 R P F1 R P F1 R P F1 F1 Baseline 26.66 19.51 22.53 55.47 58.64 57.01 53.08 60.38 56.50 52.68 63.14 57.44 30.13 25.10 25.05 45.66 Joint 20.00 37.50 26.08 53.37 63.36 57.93 53.93 62.95 58.09 55.06 62.11 58.38 27.51 38.43 31.91 47.45 Table 2: Results of event coreference resolution. ‘Baseline’ refers to the second stage of our baseline. BLANC (Recasens and Hovy, 2011) extended by Luo et al. (2014). We also report the CoNLL average (Denis and Baldridge, 2009), which is the average of MUC F1, B3 F1, and CEAF, F1. 5 Results and Discussions We first show the result of event coreference resolution on the test data in Table 2. The joint model outperforms the baseline by 6.9 BLANC F1 and 1.8 CoNLL F1 points. We observed that this overall performance gain comes largely from a precision gain, more specifically, substantially reduced false positives. We explain the superiority of the joint model as follows. In the baseline, the second stage uses the output of the first stage. Since event triggers are fixed at this point, the baseline explores coreference l</context>
</contexts>
<marker>Denis, Baldridge, 2009</marker>
<rawString>Pascal Denis and Jason Baldridge. 2009. Global joint models for coreference resolution and named entity classification. Procesamiento del Lenguaje Natural, 42:87–96.</rawString>
</citation>
<citation valid="true">
<authors>
<author>Yu Hong</author>
<author>Jianfeng Zhang</author>
<author>Bin Ma</author>
<author>Jianmin Yao</author>
<author>Guodong Zhou</author>
<author>Qiaoming Zhu</author>
</authors>
<title>Using cross-entity inference to improve event extraction.</title>
<date>2011</date>
<booktitle>In Proceedings of ACL-HLT 2011,</booktitle>
<pages>1127--1136</pages>
<contexts>
<context position="6060" citStr="Hong et al., 2011" startWordPosition="941" endWordPosition="944">Work No previous work deals with event extraction and event coreference resolution simultaneously. We thus describe how these two tasks have been addressed separately, and how joint structured learning has been studied in other NLP tasks. Event extraction has been studied mainly in the newswire domain and the biomedical domain as the task of detecting event triggers and determining their event types and arguments. In the former domain, most work took a pipelined approach where local classifiers identify triggers first, and then detect arguments (Ji and Grishman, 2008; Liao and Grishman, 2010; Hong et al., 2011). Li et al. (2013) presented a structured perceptron model to detect triggers and arguments jointly. Similarly, joint dependencies in events were also addressed in the latter domain (Poon and Vanderwende, 2010; McClosky et al., 2011; Riedel and McCallum, 2011; Venugopal et al., 2014). However, none of them incorporated event coreference into their model. Event coreference resolution is more challenging and less explored. To set up event triggers as a starting point of the task, some works use human annotation in a corpus (Bejan and Harabagiu, 2014; Liu et al., 2014), and others use the output </context>
</contexts>
<marker>Hong, Zhang, Ma, Yao, Zhou, Zhu, 2011</marker>
<rawString>Yu Hong, Jianfeng Zhang, Bin Ma, Jianmin Yao, Guodong Zhou, and Qiaoming Zhu. 2011. Using cross-entity inference to improve event extraction. In Proceedings of ACL-HLT 2011, pages 1127–1136.</rawString>
</citation>
<citation valid="true">
<authors>
<author>Liang Huang</author>
<author>Suphan Fayong</author>
<author>Yang Guo</author>
</authors>
<title>Structured perceptron with inexact search.</title>
<date>2012</date>
<booktitle>In Proceedings of NAACL-HLT 2012,</booktitle>
<pages>142--151</pages>
<contexts>
<context position="10927" citStr="Huang et al., 2012" startWordPosition="1732" endWordPosition="1735">eatures by using Stanford CoreNLP (Manning et al., 2014), MATE (Bj¨orkelund et al., 2009), OpenNLP2, Nomlex (Macleod et al., 1998), and Levin verb classes (Levin, 1993). For brevity, we provide details of the structured perceptron algorithm and features in the supplementary material. We use the standard-update strategy in our structured perceptron model. As variants of structured perceptron, one could employ the early up1For example, around 13.4% of the 1403 event triggers in ProcessBank have multiple tokens. 2http://opennlp.apache.org/ date (Collins and Roark, 2004) and max-violation update (Huang et al., 2012) to our model. Our initial experiments indicated that early updates happen too early to gain sufficient feedback on weights from entire documents in training examples, ending up with a poorer performance than the standard update. This contrasts with the fact that the early-update strategy was successfully applied to other NLP tasks such as constituent parsing (Collins and Roark, 2004) and dependency parsing (Zhang and Clark, 2008b). The main reason why the early update fell short of the standard update in our setting is that joint event trigger identification and event coreference resolution i</context>
</contexts>
<marker>Huang, Fayong, Guo, 2012</marker>
<rawString>Liang Huang, Suphan Fayong, and Yang Guo. 2012. Structured perceptron with inexact search. In Proceedings of NAACL-HLT 2012, pages 142–151.</rawString>
</citation>
<citation valid="true">
<authors>
<author>Heng Ji</author>
<author>Ralph Grishman</author>
</authors>
<title>Refining event extraction through cross-document inference.</title>
<date>2008</date>
<booktitle>In Proceedings of ACL-HLT</booktitle>
<pages>254--262</pages>
<contexts>
<context position="6015" citStr="Ji and Grishman, 2008" startWordPosition="933" endWordPosition="936">in, as compared to a pipelined model. 2 Related Work No previous work deals with event extraction and event coreference resolution simultaneously. We thus describe how these two tasks have been addressed separately, and how joint structured learning has been studied in other NLP tasks. Event extraction has been studied mainly in the newswire domain and the biomedical domain as the task of detecting event triggers and determining their event types and arguments. In the former domain, most work took a pipelined approach where local classifiers identify triggers first, and then detect arguments (Ji and Grishman, 2008; Liao and Grishman, 2010; Hong et al., 2011). Li et al. (2013) presented a structured perceptron model to detect triggers and arguments jointly. Similarly, joint dependencies in events were also addressed in the latter domain (Poon and Vanderwende, 2010; McClosky et al., 2011; Riedel and McCallum, 2011; Venugopal et al., 2014). However, none of them incorporated event coreference into their model. Event coreference resolution is more challenging and less explored. To set up event triggers as a starting point of the task, some works use human annotation in a corpus (Bejan and Harabagiu, 2014; </context>
</contexts>
<marker>Ji, Grishman, 2008</marker>
<rawString>Heng Ji and Ralph Grishman. 2008. Refining event extraction through cross-document inference. In Proceedings of ACL-HLT 2008, pages 254–262.</rawString>
</citation>
<citation valid="true">
<authors>
<author>Heng Ji</author>
<author>Ralph Grishman</author>
</authors>
<title>Knowledge base population: Successful approaches and challenges.</title>
<date>2011</date>
<booktitle>In Proceedings of ACL-HLT 2011,</booktitle>
<pages>1148--1158</pages>
<contexts>
<context position="1276" citStr="Ji and Grishman, 2011" startWordPosition="182" endWordPosition="185">ntifies event triggers and resolves event coreference. We demonstrate that the joint model outperforms a pipelined model by 6.9 BLANC F1 and 1.8 CoNLL F1 points in event coreference resolution using a corpus in the biology domain. 1 Introduction Events convey semantic information such as who did what to whom where and when. They also corefer to each other, playing a role of discourse connection points to form a coherent story. These aspects of events have been already utilized in a wide variety of natural language processing (NLP) applications, such as automated population of knowledge bases (Ji and Grishman, 2011), topic detection and tracking (Allan, 2002), question answering (Bikel and Castelli, 2008), text summarization (Li et al., 2006), and contradiction detection (de Marneffe et al., 2008). This fact illustrates the importance of event extraction and event coreference resolution. Those semantic and discourse aspects of events are not independent from each other, and in fact often work in interactive manners. We give two examples of the interactions: (1) British bank Barclays had agreed to buy(E1) Spanish rival Banco Zaragozano for 1.14 billion euros. The combination(E2) of the banking operations </context>
</contexts>
<marker>Ji, Grishman, 2011</marker>
<rawString>Heng Ji and Ralph Grishman. 2011. Knowledge base population: Successful approaches and challenges. In Proceedings of ACL-HLT 2011, pages 1148–1158.</rawString>
</citation>
<citation valid="true">
<authors>
<author>Richard Johansson</author>
<author>Pierre Nugues</author>
</authors>
<title>Dependency-based semantic role labeling of PropBank.</title>
<date>2008</date>
<booktitle>In Proceedings of EMNLP</booktitle>
<pages>69--78</pages>
<contexts>
<context position="7240" citStr="Johansson and Nugues, 2008" startWordPosition="1133" endWordPosition="1136">; Liu et al., 2014), and others use the output of a separate event extraction system (Lee et al., 2012). Berant et al. (2014) presented a model that jointly predicts event arguments and event coreference (as well as other relations between event triggers). However, none of them tries to predict event triggers and event coreference jointly. Joint structured learning has been applied to several NLP tasks, such as word segmentation and part-of-speech (POS) tagging (Zhang and Clark, 2008a), POS tagging and dependency parsing (Bohnet and Nivre, 2012), dependency parsing and semantic role labeling (Johansson and Nugues, 2008), the extraction of event triggers and arguments (Li et al., 2013), and the extraction of entity mentions and relations (Li and Ji, 2014). Their underlying ideas are similar to ours. That is, one can train a structured learning model to globally capture the interactions between two relevant tasks via a certain kind of structure, while making predictions specifically for these respective tasks. However, no prior work has studied the interactions between event trigger identification and event coreference resolution. 3 Approach We formalize the extraction of event triggers and event coreference a</context>
</contexts>
<marker>Johansson, Nugues, 2008</marker>
<rawString>Richard Johansson and Pierre Nugues. 2008. Dependency-based semantic role labeling of PropBank. In Proceedings of EMNLP 2008, pages 69– 78.</rawString>
</citation>
<citation valid="true">
<authors>
<author>Heeyoung Lee</author>
<author>Marta Recasens</author>
<author>Angel Chang</author>
<author>Mihai Surdeanu</author>
<author>Dan Jurafsky</author>
</authors>
<title>Joint entity and event coreference resolution across documents.</title>
<date>2012</date>
<booktitle>In Proceedings of EMNLP/CoNLL 2012,</booktitle>
<pages>489--500</pages>
<contexts>
<context position="6716" citStr="Lee et al., 2012" startWordPosition="1052" endWordPosition="1055">ed perceptron model to detect triggers and arguments jointly. Similarly, joint dependencies in events were also addressed in the latter domain (Poon and Vanderwende, 2010; McClosky et al., 2011; Riedel and McCallum, 2011; Venugopal et al., 2014). However, none of them incorporated event coreference into their model. Event coreference resolution is more challenging and less explored. To set up event triggers as a starting point of the task, some works use human annotation in a corpus (Bejan and Harabagiu, 2014; Liu et al., 2014), and others use the output of a separate event extraction system (Lee et al., 2012). Berant et al. (2014) presented a model that jointly predicts event arguments and event coreference (as well as other relations between event triggers). However, none of them tries to predict event triggers and event coreference jointly. Joint structured learning has been applied to several NLP tasks, such as word segmentation and part-of-speech (POS) tagging (Zhang and Clark, 2008a), POS tagging and dependency parsing (Bohnet and Nivre, 2012), dependency parsing and semantic role labeling (Johansson and Nugues, 2008), the extraction of event triggers and arguments (Li et al., 2013), and the </context>
</contexts>
<marker>Lee, Recasens, Chang, Surdeanu, Jurafsky, 2012</marker>
<rawString>Heeyoung Lee, Marta Recasens, Angel Chang, Mihai Surdeanu, and Dan Jurafsky. 2012. Joint entity and event coreference resolution across documents. In Proceedings of EMNLP/CoNLL 2012, pages 489– 500.</rawString>
</citation>
<citation valid="true">
<authors>
<author>Heeyoung Lee</author>
<author>Angel Chang</author>
<author>Yves Peirsman</author>
<author>Nathanael Chambers</author>
<author>Mihai Surdeanu</author>
<author>Dan Jurafsky</author>
</authors>
<title>Deterministic coreference resolution based on entity-centric, precision-ranked rules.</title>
<date>2013</date>
<journal>Computational Linguistics,</journal>
<volume>39</volume>
<issue>4</issue>
<contexts>
<context position="13230" citStr="Lee et al., 2013" startWordPosition="2105" endWordPosition="2108">nt coreference by comparing event coreferences from the complete assignment at a certain position with those from complete assignments at following positions. This makes it complicated to implement the formalization of token-level sequential labeling for joint decoding in our task. One possible way to avoid this problem is to extract event trigger candidates with a preference on high recall first, and then search event coreference from those candidates, regarding them as complete assignments of an event trigger. This recalloriented pre-filtering is often used in entity coreference resolution (Lee et al., 2013; Bj¨orkelund 2076 Algorithm 1 Joint decoding for event triggers and coreference with beam search. Input: input document x = (x1, x2, ... , xn) Input: beam width k, max length of event trigger l„t x Output: best event graph yˆ for x 1: initialize empty beam history B[1..n] 2: for i ← 1..n do 3: for l ← 1..l„t xdo 4: for y ∈ B[i − l] do 5: e ← CREATEEVENTTRIGGER(l, i). 6: APPENDEVENTTRIGGER(y, e) 7: B[i] ← k-BEST(B[i] ∪ y) 8: for j ← 1..i − 1 do 9: c ← CREATEEVENTCOREF(j, e). 10: ADDEVENTCOREF(y, c) 11: B[i] ← k-BEST(B[i] ∪ y) 12: return B[n][0] and Farkas, 2012). In our initial experiments, we</context>
<context position="17017" citStr="Lee et al. (2013)" startWordPosition="2736" endWordPosition="2739">nces, which makes it difficult to reveal the effect of beam width. We thus set k to 1 in our experiments. 4.1 Baseline Systems Our baseline is a pipelined model that divides the event trigger decoding and event coreference decoding in Algorithm 1 into two separate stages. It uses the same structured perceptron with the same hyperparameters and feature templates. We choose this baseline because it clearly reveals the effectiveness of the joint model by focusing only on the architectural difference. One could develop other baseline systems. One of them is a deterministic sieve-based approach by Lee et al. (2013). A natural extension to the approach for performing event trigger identification as well as event coreference resolution would be to develop additional sieves to classify singletons into real event triggers or spurious ones. We leave it for future work. 4.2 Evaluation We evaluate our system using a reference implementation of coreference scoring algorithms (Pradhan et al., 2014; Luo et al., 2014). As for event trigger identification, this scorer computes precision (P), recall (R), and the F1 score. With respect to event coreference resolution, the scorer computes MUC (Vilain et al., 1995), B3</context>
</contexts>
<marker>Lee, Chang, Peirsman, Chambers, Surdeanu, Jurafsky, 2013</marker>
<rawString>Heeyoung Lee, Angel Chang, Yves Peirsman, Nathanael Chambers, Mihai Surdeanu, and Dan Jurafsky. 2013. Deterministic coreference resolution based on entity-centric, precision-ranked rules. Computational Linguistics, 39(4):885–916.</rawString>
</citation>
<citation valid="true">
<authors>
<author>Beth Levin</author>
</authors>
<title>English Verb Classes and Alternation: A Preliminary Investigation.</title>
<date>1993</date>
<publisher>The University of Chicago Press.</publisher>
<contexts>
<context position="10476" citStr="Levin, 1993" startWordPosition="1668" endWordPosition="1669">n the document. For event graph learning, we use structured perceptron (Collins, 2002), and average weights to reduce overfitting as suggested in (Collins, 2002). The algorithm involves decoding to generate the best event graph for each input document. We elaborate on our decoding algorithm in Section 3.3. Since an event graph has an exponentially large search space, we use beam search to approximate exact inference. We extract a range of features by using Stanford CoreNLP (Manning et al., 2014), MATE (Bj¨orkelund et al., 2009), OpenNLP2, Nomlex (Macleod et al., 1998), and Levin verb classes (Levin, 1993). For brevity, we provide details of the structured perceptron algorithm and features in the supplementary material. We use the standard-update strategy in our structured perceptron model. As variants of structured perceptron, one could employ the early up1For example, around 13.4% of the 1403 event triggers in ProcessBank have multiple tokens. 2http://opennlp.apache.org/ date (Collins and Roark, 2004) and max-violation update (Huang et al., 2012) to our model. Our initial experiments indicated that early updates happen too early to gain sufficient feedback on weights from entire documents in </context>
</contexts>
<marker>Levin, 1993</marker>
<rawString>Beth Levin. 1993. English Verb Classes and Alternation: A Preliminary Investigation. The University of Chicago Press.</rawString>
</citation>
<citation valid="true">
<authors>
<author>Qi Li</author>
<author>Heng Ji</author>
</authors>
<title>Incremental joint extraction of entity mentions and relations.</title>
<date>2014</date>
<booktitle>In Proceedings of ACL 2014,</booktitle>
<pages>402--412</pages>
<contexts>
<context position="7377" citStr="Li and Ji, 2014" startWordPosition="1156" endWordPosition="1159"> jointly predicts event arguments and event coreference (as well as other relations between event triggers). However, none of them tries to predict event triggers and event coreference jointly. Joint structured learning has been applied to several NLP tasks, such as word segmentation and part-of-speech (POS) tagging (Zhang and Clark, 2008a), POS tagging and dependency parsing (Bohnet and Nivre, 2012), dependency parsing and semantic role labeling (Johansson and Nugues, 2008), the extraction of event triggers and arguments (Li et al., 2013), and the extraction of entity mentions and relations (Li and Ji, 2014). Their underlying ideas are similar to ours. That is, one can train a structured learning model to globally capture the interactions between two relevant tasks via a certain kind of structure, while making predictions specifically for these respective tasks. However, no prior work has studied the interactions between event trigger identification and event coreference resolution. 3 Approach We formalize the extraction of event triggers and event coreference as a problem of structured prediction. The output structure is a document-level event graph where each node represents an event trigger, a</context>
<context position="14230" citStr="Li and Ji, 2014" startWordPosition="2279" endWordPosition="2282">TTRIGGER(y, e) 7: B[i] ← k-BEST(B[i] ∪ y) 8: for j ← 1..i − 1 do 9: c ← CREATEEVENTCOREF(j, e). 10: ADDEVENTCOREF(y, c) 11: B[i] ← k-BEST(B[i] ∪ y) 12: return B[n][0] and Farkas, 2012). In our initial experiments, we observed that our rule-based filter gained around 97% recall, but extracted around 12,400 false positives against 823 true positives in the training data. This made it difficult for our structured perceptron to learn event triggers, which underperformed on event coreference resolution. We, therefore, employ segment-based decoding with multiple-beam search (Zhang and Clark, 2008a; Li and Ji, 2014) for event trigger identification, and combine it with the best-first clustering (Ng and Cardie, 2002) for event coreference resolution in document-level joint decoding. The key idea of segment-based decoding with multiple-beam search is to keep previous beam states available, and use them to form segments from previous positions to the current position. Let lmax denote the upper bound on the number of tokens in one event trigger. The k-best partial structures (event subgraphs) in beam B at the j-th token is computed as follows: B[j] = k-BEST yE{y[1:j−l]EB[j−l], y[j−l+1,j]=s1 where 1 &lt; l &lt; lma</context>
</contexts>
<marker>Li, Ji, 2014</marker>
<rawString>Qi Li and Heng Ji. 2014. Incremental joint extraction of entity mentions and relations. In Proceedings of ACL 2014, pages 402–412.</rawString>
</citation>
<citation valid="true">
<authors>
<author>Wenjie Li</author>
<author>Mingli Wu</author>
<author>Qin Lu</author>
<author>Wei Xu</author>
<author>Chunfa Yuan</author>
</authors>
<title>Extractive summarization using inter- and intra- event relevance.</title>
<date>2006</date>
<booktitle>In Proceedings of ACL/COLING</booktitle>
<pages>369--376</pages>
<contexts>
<context position="1405" citStr="Li et al., 2006" startWordPosition="202" endWordPosition="205">1 and 1.8 CoNLL F1 points in event coreference resolution using a corpus in the biology domain. 1 Introduction Events convey semantic information such as who did what to whom where and when. They also corefer to each other, playing a role of discourse connection points to form a coherent story. These aspects of events have been already utilized in a wide variety of natural language processing (NLP) applications, such as automated population of knowledge bases (Ji and Grishman, 2011), topic detection and tracking (Allan, 2002), question answering (Bikel and Castelli, 2008), text summarization (Li et al., 2006), and contradiction detection (de Marneffe et al., 2008). This fact illustrates the importance of event extraction and event coreference resolution. Those semantic and discourse aspects of events are not independent from each other, and in fact often work in interactive manners. We give two examples of the interactions: (1) British bank Barclays had agreed to buy(E1) Spanish rival Banco Zaragozano for 1.14 billion euros. The combination(E2) of the banking operations of Barclays Spain and Zaragozano will bring together two complementary businesses. (2) The Palestinian Authority condemned the at</context>
</contexts>
<marker>Li, Wu, Lu, Xu, Yuan, 2006</marker>
<rawString>Wenjie Li, Mingli Wu, Qin Lu, Wei Xu, and Chunfa Yuan. 2006. Extractive summarization using inter- and intra- event relevance. In Proceedings of ACL/COLING 2006, pages 369–376.</rawString>
</citation>
<citation valid="true">
<authors>
<author>Qi Li</author>
<author>Heng Ji</author>
<author>Liang Huang</author>
</authors>
<title>Joint event extraction via structured prediction with global features.</title>
<date>2013</date>
<booktitle>In Proceedings ofACL 2013,</booktitle>
<pages>73--82</pages>
<contexts>
<context position="6078" citStr="Li et al. (2013)" startWordPosition="945" endWordPosition="948">k deals with event extraction and event coreference resolution simultaneously. We thus describe how these two tasks have been addressed separately, and how joint structured learning has been studied in other NLP tasks. Event extraction has been studied mainly in the newswire domain and the biomedical domain as the task of detecting event triggers and determining their event types and arguments. In the former domain, most work took a pipelined approach where local classifiers identify triggers first, and then detect arguments (Ji and Grishman, 2008; Liao and Grishman, 2010; Hong et al., 2011). Li et al. (2013) presented a structured perceptron model to detect triggers and arguments jointly. Similarly, joint dependencies in events were also addressed in the latter domain (Poon and Vanderwende, 2010; McClosky et al., 2011; Riedel and McCallum, 2011; Venugopal et al., 2014). However, none of them incorporated event coreference into their model. Event coreference resolution is more challenging and less explored. To set up event triggers as a starting point of the task, some works use human annotation in a corpus (Bejan and Harabagiu, 2014; Liu et al., 2014), and others use the output of a separate even</context>
<context position="7306" citStr="Li et al., 2013" startWordPosition="1144" endWordPosition="1147"> system (Lee et al., 2012). Berant et al. (2014) presented a model that jointly predicts event arguments and event coreference (as well as other relations between event triggers). However, none of them tries to predict event triggers and event coreference jointly. Joint structured learning has been applied to several NLP tasks, such as word segmentation and part-of-speech (POS) tagging (Zhang and Clark, 2008a), POS tagging and dependency parsing (Bohnet and Nivre, 2012), dependency parsing and semantic role labeling (Johansson and Nugues, 2008), the extraction of event triggers and arguments (Li et al., 2013), and the extraction of entity mentions and relations (Li and Ji, 2014). Their underlying ideas are similar to ours. That is, one can train a structured learning model to globally capture the interactions between two relevant tasks via a certain kind of structure, while making predictions specifically for these respective tasks. However, no prior work has studied the interactions between event trigger identification and event coreference resolution. 3 Approach We formalize the extraction of event triggers and event coreference as a problem of structured prediction. The output structure is a do</context>
<context position="9482" citStr="Li et al., 2013" startWordPosition="1500" endWordPosition="1503">lution using ProcessBank is novel since almost no previous work on the task used that corpus. The only exception could be (Berant et al., 2014), where they extracted several types of relations between event triggers, including event coreference. However, they did not report any performance scores of their system specifically on event coreference, and thus their work is not comparable to ours. Train Dev Test Total # of paragraphs 120 30 50 200 # of event triggers 823 224 356 1403 # of event coreferences 73 28 30 131 Table 1: Statistics of our dataset. Unlike previous work (Berant et al., 2014; Li et al., 2013), we explicitly allow an event trigger to have multiple tokens, such as verb phrase ‘look into’ and compound proper noun ‘World War II’. This is a more realistic setting for event trigger identification since in general there are a considerable number of multi-token event triggers1. 3.2 Event Graph Learning Let x denote an input document with n tokens where xi is the i-th token in the document. For event graph learning, we use structured perceptron (Collins, 2002), and average weights to reduce overfitting as suggested in (Collins, 2002). The algorithm involves decoding to generate the best ev</context>
</contexts>
<marker>Li, Ji, Huang, 2013</marker>
<rawString>Qi Li, Heng Ji, and Liang Huang. 2013. Joint event extraction via structured prediction with global features. In Proceedings ofACL 2013, pages 73–82.</rawString>
</citation>
<citation valid="true">
<authors>
<author>Shasha Liao</author>
<author>Ralph Grishman</author>
</authors>
<title>Using document level cross-event inference to improve event extraction.</title>
<date>2010</date>
<booktitle>In Proceedings ofACL 2010,</booktitle>
<pages>789--797</pages>
<contexts>
<context position="6040" citStr="Liao and Grishman, 2010" startWordPosition="937" endWordPosition="940">pelined model. 2 Related Work No previous work deals with event extraction and event coreference resolution simultaneously. We thus describe how these two tasks have been addressed separately, and how joint structured learning has been studied in other NLP tasks. Event extraction has been studied mainly in the newswire domain and the biomedical domain as the task of detecting event triggers and determining their event types and arguments. In the former domain, most work took a pipelined approach where local classifiers identify triggers first, and then detect arguments (Ji and Grishman, 2008; Liao and Grishman, 2010; Hong et al., 2011). Li et al. (2013) presented a structured perceptron model to detect triggers and arguments jointly. Similarly, joint dependencies in events were also addressed in the latter domain (Poon and Vanderwende, 2010; McClosky et al., 2011; Riedel and McCallum, 2011; Venugopal et al., 2014). However, none of them incorporated event coreference into their model. Event coreference resolution is more challenging and less explored. To set up event triggers as a starting point of the task, some works use human annotation in a corpus (Bejan and Harabagiu, 2014; Liu et al., 2014), and ot</context>
</contexts>
<marker>Liao, Grishman, 2010</marker>
<rawString>Shasha Liao and Ralph Grishman. 2010. Using document level cross-event inference to improve event extraction. In Proceedings ofACL 2010, pages 789– 797.</rawString>
</citation>
<citation valid="true">
<authors>
<author>Zhengzhong Liu</author>
<author>Jun Araki</author>
<author>Eduard Hovy</author>
<author>Teruko Mitamura</author>
</authors>
<title>Supervised within-document event coreference using information propagation.</title>
<date>2014</date>
<booktitle>In Proceedings of LREC</booktitle>
<contexts>
<context position="6632" citStr="Liu et al., 2014" startWordPosition="1037" endWordPosition="1040">; Liao and Grishman, 2010; Hong et al., 2011). Li et al. (2013) presented a structured perceptron model to detect triggers and arguments jointly. Similarly, joint dependencies in events were also addressed in the latter domain (Poon and Vanderwende, 2010; McClosky et al., 2011; Riedel and McCallum, 2011; Venugopal et al., 2014). However, none of them incorporated event coreference into their model. Event coreference resolution is more challenging and less explored. To set up event triggers as a starting point of the task, some works use human annotation in a corpus (Bejan and Harabagiu, 2014; Liu et al., 2014), and others use the output of a separate event extraction system (Lee et al., 2012). Berant et al. (2014) presented a model that jointly predicts event arguments and event coreference (as well as other relations between event triggers). However, none of them tries to predict event triggers and event coreference jointly. Joint structured learning has been applied to several NLP tasks, such as word segmentation and part-of-speech (POS) tagging (Zhang and Clark, 2008a), POS tagging and dependency parsing (Bohnet and Nivre, 2012), dependency parsing and semantic role labeling (Johansson and Nugue</context>
</contexts>
<marker>Liu, Araki, Hovy, Mitamura, 2014</marker>
<rawString>Zhengzhong Liu, Jun Araki, Eduard Hovy, and Teruko Mitamura. 2014. Supervised within-document event coreference using information propagation. In Proceedings of LREC 2014.</rawString>
</citation>
<citation valid="true">
<authors>
<author>Xiaoqiang Luo</author>
<author>Sameer Pradhan</author>
<author>Marta Recasens</author>
<author>Eduard Hovy</author>
</authors>
<title>An extension of BLANC to system mentions.</title>
<date>2014</date>
<booktitle>In Proceedings of ACL 2014,</booktitle>
<pages>24--29</pages>
<contexts>
<context position="17417" citStr="Luo et al., 2014" startWordPosition="2799" endWordPosition="2802">clearly reveals the effectiveness of the joint model by focusing only on the architectural difference. One could develop other baseline systems. One of them is a deterministic sieve-based approach by Lee et al. (2013). A natural extension to the approach for performing event trigger identification as well as event coreference resolution would be to develop additional sieves to classify singletons into real event triggers or spurious ones. We leave it for future work. 4.2 Evaluation We evaluate our system using a reference implementation of coreference scoring algorithms (Pradhan et al., 2014; Luo et al., 2014). As for event trigger identification, this scorer computes precision (P), recall (R), and the F1 score. With respect to event coreference resolution, the scorer computes MUC (Vilain et al., 1995), B3 (Bagga and Baldwin, 1998), two CEAF metrics CEAFm and CEAFe (Luo, 2005), and Φ(x, y) · w 2077 MUC B3 CEAF. CEAF, BLANC CoNLL System R P F1 R P F1 R P F1 R P F1 R P F1 F1 Baseline 26.66 19.51 22.53 55.47 58.64 57.01 53.08 60.38 56.50 52.68 63.14 57.44 30.13 25.10 25.05 45.66 Joint 20.00 37.50 26.08 53.37 63.36 57.93 53.93 62.95 58.09 55.06 62.11 58.38 27.51 38.43 31.91 47.45 Table 2: Results of ev</context>
</contexts>
<marker>Luo, Pradhan, Recasens, Hovy, 2014</marker>
<rawString>Xiaoqiang Luo, Sameer Pradhan, Marta Recasens, and Eduard Hovy. 2014. An extension of BLANC to system mentions. In Proceedings of ACL 2014, pages 24–29.</rawString>
</citation>
<citation valid="true">
<authors>
<author>Xiaoqiang Luo</author>
</authors>
<title>On coreference resolution performance metrics.</title>
<date>2005</date>
<booktitle>In Proceedings of HLT/EMNLP</booktitle>
<pages>25--32</pages>
<contexts>
<context position="17689" citStr="Luo, 2005" startWordPosition="2845" endWordPosition="2846">gger identification as well as event coreference resolution would be to develop additional sieves to classify singletons into real event triggers or spurious ones. We leave it for future work. 4.2 Evaluation We evaluate our system using a reference implementation of coreference scoring algorithms (Pradhan et al., 2014; Luo et al., 2014). As for event trigger identification, this scorer computes precision (P), recall (R), and the F1 score. With respect to event coreference resolution, the scorer computes MUC (Vilain et al., 1995), B3 (Bagga and Baldwin, 1998), two CEAF metrics CEAFm and CEAFe (Luo, 2005), and Φ(x, y) · w 2077 MUC B3 CEAF. CEAF, BLANC CoNLL System R P F1 R P F1 R P F1 R P F1 R P F1 F1 Baseline 26.66 19.51 22.53 55.47 58.64 57.01 53.08 60.38 56.50 52.68 63.14 57.44 30.13 25.10 25.05 45.66 Joint 20.00 37.50 26.08 53.37 63.36 57.93 53.93 62.95 58.09 55.06 62.11 58.38 27.51 38.43 31.91 47.45 Table 2: Results of event coreference resolution. ‘Baseline’ refers to the second stage of our baseline. BLANC (Recasens and Hovy, 2011) extended by Luo et al. (2014). We also report the CoNLL average (Denis and Baldridge, 2009), which is the average of MUC F1, B3 F1, and CEAF, F1. 5 Results a</context>
</contexts>
<marker>Luo, 2005</marker>
<rawString>Xiaoqiang Luo. 2005. On coreference resolution performance metrics. In Proceedings of HLT/EMNLP 2005, pages 25–32.</rawString>
</citation>
<citation valid="true">
<authors>
<author>Catherine Macleod</author>
<author>Ralph Grishman</author>
<author>Adam Meyers</author>
<author>Leslie Barrett</author>
<author>Ruth Reeves</author>
</authors>
<title>Nomlex: A lexicon of nominalizations.</title>
<date>1998</date>
<booktitle>In Proceedings of EURALEX</booktitle>
<pages>187--193</pages>
<contexts>
<context position="10438" citStr="Macleod et al., 1998" startWordPosition="1660" endWordPosition="1663">ment with n tokens where xi is the i-th token in the document. For event graph learning, we use structured perceptron (Collins, 2002), and average weights to reduce overfitting as suggested in (Collins, 2002). The algorithm involves decoding to generate the best event graph for each input document. We elaborate on our decoding algorithm in Section 3.3. Since an event graph has an exponentially large search space, we use beam search to approximate exact inference. We extract a range of features by using Stanford CoreNLP (Manning et al., 2014), MATE (Bj¨orkelund et al., 2009), OpenNLP2, Nomlex (Macleod et al., 1998), and Levin verb classes (Levin, 1993). For brevity, we provide details of the structured perceptron algorithm and features in the supplementary material. We use the standard-update strategy in our structured perceptron model. As variants of structured perceptron, one could employ the early up1For example, around 13.4% of the 1403 event triggers in ProcessBank have multiple tokens. 2http://opennlp.apache.org/ date (Collins and Roark, 2004) and max-violation update (Huang et al., 2012) to our model. Our initial experiments indicated that early updates happen too early to gain sufficient feedbac</context>
</contexts>
<marker>Macleod, Grishman, Meyers, Barrett, Reeves, 1998</marker>
<rawString>Catherine Macleod, Ralph Grishman, Adam Meyers, Leslie Barrett, and Ruth Reeves. 1998. Nomlex: A lexicon of nominalizations. In Proceedings of EURALEX 1998, pages 187–193.</rawString>
</citation>
<citation valid="true">
<authors>
<author>Christopher Manning</author>
<author>Mihai Surdeanu</author>
<author>John Bauer</author>
<author>Jenny Finkel</author>
<author>Steven Bethard</author>
<author>David McClosky</author>
</authors>
<title>The Stanford CoreNLP natural language processing toolkit.</title>
<date>2014</date>
<booktitle>In Proceedings ACL 2014: System Demonstrations,</booktitle>
<pages>55--60</pages>
<contexts>
<context position="10364" citStr="Manning et al., 2014" startWordPosition="1649" endWordPosition="1652">token event triggers1. 3.2 Event Graph Learning Let x denote an input document with n tokens where xi is the i-th token in the document. For event graph learning, we use structured perceptron (Collins, 2002), and average weights to reduce overfitting as suggested in (Collins, 2002). The algorithm involves decoding to generate the best event graph for each input document. We elaborate on our decoding algorithm in Section 3.3. Since an event graph has an exponentially large search space, we use beam search to approximate exact inference. We extract a range of features by using Stanford CoreNLP (Manning et al., 2014), MATE (Bj¨orkelund et al., 2009), OpenNLP2, Nomlex (Macleod et al., 1998), and Levin verb classes (Levin, 1993). For brevity, we provide details of the structured perceptron algorithm and features in the supplementary material. We use the standard-update strategy in our structured perceptron model. As variants of structured perceptron, one could employ the early up1For example, around 13.4% of the 1403 event triggers in ProcessBank have multiple tokens. 2http://opennlp.apache.org/ date (Collins and Roark, 2004) and max-violation update (Huang et al., 2012) to our model. Our initial experiment</context>
</contexts>
<marker>Manning, Surdeanu, Bauer, Finkel, Bethard, McClosky, 2014</marker>
<rawString>Christopher Manning, Mihai Surdeanu, John Bauer, Jenny Finkel, Steven Bethard, and David McClosky. 2014. The Stanford CoreNLP natural language processing toolkit. In Proceedings ACL 2014: System Demonstrations, pages 55–60.</rawString>
</citation>
<citation valid="true">
<authors>
<author>David McClosky</author>
<author>Mihai Surdeanu</author>
<author>Christopher Manning</author>
</authors>
<title>Event extraction as dependency parsing.</title>
<date>2011</date>
<booktitle>In Proceedings of ACL-HLT 2011,</booktitle>
<pages>1626--1635</pages>
<contexts>
<context position="6292" citStr="McClosky et al., 2011" startWordPosition="979" endWordPosition="982">LP tasks. Event extraction has been studied mainly in the newswire domain and the biomedical domain as the task of detecting event triggers and determining their event types and arguments. In the former domain, most work took a pipelined approach where local classifiers identify triggers first, and then detect arguments (Ji and Grishman, 2008; Liao and Grishman, 2010; Hong et al., 2011). Li et al. (2013) presented a structured perceptron model to detect triggers and arguments jointly. Similarly, joint dependencies in events were also addressed in the latter domain (Poon and Vanderwende, 2010; McClosky et al., 2011; Riedel and McCallum, 2011; Venugopal et al., 2014). However, none of them incorporated event coreference into their model. Event coreference resolution is more challenging and less explored. To set up event triggers as a starting point of the task, some works use human annotation in a corpus (Bejan and Harabagiu, 2014; Liu et al., 2014), and others use the output of a separate event extraction system (Lee et al., 2012). Berant et al. (2014) presented a model that jointly predicts event arguments and event coreference (as well as other relations between event triggers). However, none of them </context>
</contexts>
<marker>McClosky, Surdeanu, Manning, 2011</marker>
<rawString>David McClosky, Mihai Surdeanu, and Christopher Manning. 2011. Event extraction as dependency parsing. In Proceedings of ACL-HLT 2011, pages 1626–1635.</rawString>
</citation>
<citation valid="true">
<authors>
<author>Vincent Ng</author>
<author>Claire Cardie</author>
</authors>
<title>Improving machine learning approaches to coreference resolution.</title>
<date>2002</date>
<booktitle>In Proceedings of ACL</booktitle>
<pages>104--111</pages>
<contexts>
<context position="14332" citStr="Ng and Cardie, 2002" startWordPosition="2296" endWordPosition="2299">: ADDEVENTCOREF(y, c) 11: B[i] ← k-BEST(B[i] ∪ y) 12: return B[n][0] and Farkas, 2012). In our initial experiments, we observed that our rule-based filter gained around 97% recall, but extracted around 12,400 false positives against 823 true positives in the training data. This made it difficult for our structured perceptron to learn event triggers, which underperformed on event coreference resolution. We, therefore, employ segment-based decoding with multiple-beam search (Zhang and Clark, 2008a; Li and Ji, 2014) for event trigger identification, and combine it with the best-first clustering (Ng and Cardie, 2002) for event coreference resolution in document-level joint decoding. The key idea of segment-based decoding with multiple-beam search is to keep previous beam states available, and use them to form segments from previous positions to the current position. Let lmax denote the upper bound on the number of tokens in one event trigger. The k-best partial structures (event subgraphs) in beam B at the j-th token is computed as follows: B[j] = k-BEST yE{y[1:j−l]EB[j−l], y[j−l+1,j]=s1 where 1 &lt; l &lt; lmax, y[1:j] is an event subgraph ending at the j-th token, and y[j−l+1,j] = s means that partial structu</context>
</contexts>
<marker>Ng, Cardie, 2002</marker>
<rawString>Vincent Ng and Claire Cardie. 2002. Improving machine learning approaches to coreference resolution. In Proceedings of ACL 2002, pages 104–111.</rawString>
</citation>
<citation valid="true">
<authors>
<author>Hoifung Poon</author>
<author>Lucy Vanderwende</author>
</authors>
<title>Joint inference for knowledge extraction from biomedical literature.</title>
<date>2010</date>
<booktitle>In Proceedings of NAACL-HLT</booktitle>
<pages>813--821</pages>
<contexts>
<context position="6269" citStr="Poon and Vanderwende, 2010" startWordPosition="974" endWordPosition="978"> has been studied in other NLP tasks. Event extraction has been studied mainly in the newswire domain and the biomedical domain as the task of detecting event triggers and determining their event types and arguments. In the former domain, most work took a pipelined approach where local classifiers identify triggers first, and then detect arguments (Ji and Grishman, 2008; Liao and Grishman, 2010; Hong et al., 2011). Li et al. (2013) presented a structured perceptron model to detect triggers and arguments jointly. Similarly, joint dependencies in events were also addressed in the latter domain (Poon and Vanderwende, 2010; McClosky et al., 2011; Riedel and McCallum, 2011; Venugopal et al., 2014). However, none of them incorporated event coreference into their model. Event coreference resolution is more challenging and less explored. To set up event triggers as a starting point of the task, some works use human annotation in a corpus (Bejan and Harabagiu, 2014; Liu et al., 2014), and others use the output of a separate event extraction system (Lee et al., 2012). Berant et al. (2014) presented a model that jointly predicts event arguments and event coreference (as well as other relations between event triggers).</context>
</contexts>
<marker>Poon, Vanderwende, 2010</marker>
<rawString>Hoifung Poon and Lucy Vanderwende. 2010. Joint inference for knowledge extraction from biomedical literature. In Proceedings of NAACL-HLT 2010, pages 813–821.</rawString>
</citation>
<citation valid="true">
<authors>
<author>Sameer Pradhan</author>
<author>Xiaoqiang Luo</author>
<author>Marta Recasens</author>
<author>Eduard Hovy</author>
<author>Vincent Ng</author>
<author>Michael Strube</author>
</authors>
<title>Scoring coreference partitions of predicted mentions: A reference implementation.</title>
<date>2014</date>
<booktitle>In Proceedings of ACL 2014,</booktitle>
<pages>30--35</pages>
<contexts>
<context position="17398" citStr="Pradhan et al., 2014" startWordPosition="2795" endWordPosition="2798">s baseline because it clearly reveals the effectiveness of the joint model by focusing only on the architectural difference. One could develop other baseline systems. One of them is a deterministic sieve-based approach by Lee et al. (2013). A natural extension to the approach for performing event trigger identification as well as event coreference resolution would be to develop additional sieves to classify singletons into real event triggers or spurious ones. We leave it for future work. 4.2 Evaluation We evaluate our system using a reference implementation of coreference scoring algorithms (Pradhan et al., 2014; Luo et al., 2014). As for event trigger identification, this scorer computes precision (P), recall (R), and the F1 score. With respect to event coreference resolution, the scorer computes MUC (Vilain et al., 1995), B3 (Bagga and Baldwin, 1998), two CEAF metrics CEAFm and CEAFe (Luo, 2005), and Φ(x, y) · w 2077 MUC B3 CEAF. CEAF, BLANC CoNLL System R P F1 R P F1 R P F1 R P F1 R P F1 F1 Baseline 26.66 19.51 22.53 55.47 58.64 57.01 53.08 60.38 56.50 52.68 63.14 57.44 30.13 25.10 25.05 45.66 Joint 20.00 37.50 26.08 53.37 63.36 57.93 53.93 62.95 58.09 55.06 62.11 58.38 27.51 38.43 31.91 47.45 Tab</context>
</contexts>
<marker>Pradhan, Luo, Recasens, Hovy, Ng, Strube, 2014</marker>
<rawString>Sameer Pradhan, Xiaoqiang Luo, Marta Recasens, Eduard Hovy, Vincent Ng, and Michael Strube. 2014. Scoring coreference partitions of predicted mentions: A reference implementation. In Proceedings of ACL 2014, pages 30–35.</rawString>
</citation>
<citation valid="true">
<authors>
<author>Lev Ratinov</author>
<author>Dan Roth</author>
</authors>
<title>Design challenges and misconceptions in named entity recognition.</title>
<date>2009</date>
<booktitle>In Proceedings of CoNLL</booktitle>
<pages>147--155</pages>
<contexts>
<context position="12360" citStr="Ratinov and Roth, 2009" startWordPosition="1965" endWordPosition="1968">earch can explore the search space of an entire document thoroughly with early updates. This observation follows (Bj¨orkelund and Kuhn, 2014) on entity coreference resolution. In contrast, the maxviolation update showed almost the same performance as the standard update on the development data. From these results, we chose the standardupdate strategy for simplicity. 3.3 Joint Decoding Given that an event trigger has one or more tokens, event trigger identification could be solved as a token-level sequential labeling problem with BIO or BILOU scheme in the same way as named entity recognition (Ratinov and Roth, 2009). If one uses this approach, a beam state may represent a partial assignment of an event trigger. However, event coreference can be explored only from complete assignments of an event trigger. Thus, one would need to synchronize the search process of event coreference by comparing event coreferences from the complete assignment at a certain position with those from complete assignments at following positions. This makes it complicated to implement the formalization of token-level sequential labeling for joint decoding in our task. One possible way to avoid this problem is to extract event trig</context>
</contexts>
<marker>Ratinov, Roth, 2009</marker>
<rawString>Lev Ratinov and Dan Roth. 2009. Design challenges and misconceptions in named entity recognition. In Proceedings of CoNLL 2009, pages 147–155.</rawString>
</citation>
<citation valid="true">
<authors>
<author>Marta Recasens</author>
<author>Eduard Hovy</author>
</authors>
<title>BLANC: Implementing the Rand index for coreference evaluation.</title>
<date>2011</date>
<journal>Natural Language Engineering,</journal>
<volume>17</volume>
<issue>4</issue>
<pages>510</pages>
<contexts>
<context position="18131" citStr="Recasens and Hovy, 2011" startWordPosition="2927" endWordPosition="2930"> the F1 score. With respect to event coreference resolution, the scorer computes MUC (Vilain et al., 1995), B3 (Bagga and Baldwin, 1998), two CEAF metrics CEAFm and CEAFe (Luo, 2005), and Φ(x, y) · w 2077 MUC B3 CEAF. CEAF, BLANC CoNLL System R P F1 R P F1 R P F1 R P F1 R P F1 F1 Baseline 26.66 19.51 22.53 55.47 58.64 57.01 53.08 60.38 56.50 52.68 63.14 57.44 30.13 25.10 25.05 45.66 Joint 20.00 37.50 26.08 53.37 63.36 57.93 53.93 62.95 58.09 55.06 62.11 58.38 27.51 38.43 31.91 47.45 Table 2: Results of event coreference resolution. ‘Baseline’ refers to the second stage of our baseline. BLANC (Recasens and Hovy, 2011) extended by Luo et al. (2014). We also report the CoNLL average (Denis and Baldridge, 2009), which is the average of MUC F1, B3 F1, and CEAF, F1. 5 Results and Discussions We first show the result of event coreference resolution on the test data in Table 2. The joint model outperforms the baseline by 6.9 BLANC F1 and 1.8 CoNLL F1 points. We observed that this overall performance gain comes largely from a precision gain, more specifically, substantially reduced false positives. We explain the superiority of the joint model as follows. In the baseline, the second stage uses the output of the fi</context>
</contexts>
<marker>Recasens, Hovy, 2011</marker>
<rawString>Marta Recasens and Eduard Hovy. 2011. BLANC: Implementing the Rand index for coreference evaluation. Natural Language Engineering, 17(4):485– 510.</rawString>
</citation>
<citation valid="true">
<authors>
<author>Sebastian Riedel</author>
<author>Andrew McCallum</author>
</authors>
<title>Fast and robust joint models for biomedical event extraction.</title>
<date>2011</date>
<booktitle>In Proceedings of EMNLP 2011,</booktitle>
<pages>1--12</pages>
<contexts>
<context position="6319" citStr="Riedel and McCallum, 2011" startWordPosition="983" endWordPosition="986">ion has been studied mainly in the newswire domain and the biomedical domain as the task of detecting event triggers and determining their event types and arguments. In the former domain, most work took a pipelined approach where local classifiers identify triggers first, and then detect arguments (Ji and Grishman, 2008; Liao and Grishman, 2010; Hong et al., 2011). Li et al. (2013) presented a structured perceptron model to detect triggers and arguments jointly. Similarly, joint dependencies in events were also addressed in the latter domain (Poon and Vanderwende, 2010; McClosky et al., 2011; Riedel and McCallum, 2011; Venugopal et al., 2014). However, none of them incorporated event coreference into their model. Event coreference resolution is more challenging and less explored. To set up event triggers as a starting point of the task, some works use human annotation in a corpus (Bejan and Harabagiu, 2014; Liu et al., 2014), and others use the output of a separate event extraction system (Lee et al., 2012). Berant et al. (2014) presented a model that jointly predicts event arguments and event coreference (as well as other relations between event triggers). However, none of them tries to predict event trig</context>
</contexts>
<marker>Riedel, McCallum, 2011</marker>
<rawString>Sebastian Riedel and Andrew McCallum. 2011. Fast and robust joint models for biomedical event extraction. In Proceedings of EMNLP 2011, pages 1–12.</rawString>
</citation>
<citation valid="true">
<authors>
<author>Deepak Venugopal</author>
<author>Chen Chen</author>
<author>Vibhav Gogate</author>
<author>Vincent Ng</author>
</authors>
<title>Relieving the computational bottleneck: Joint inference for event extraction with high-dimensional features.</title>
<date>2014</date>
<booktitle>In Proceedings of EMNLP 2014,</booktitle>
<pages>831--843</pages>
<contexts>
<context position="6344" citStr="Venugopal et al., 2014" startWordPosition="987" endWordPosition="990"> in the newswire domain and the biomedical domain as the task of detecting event triggers and determining their event types and arguments. In the former domain, most work took a pipelined approach where local classifiers identify triggers first, and then detect arguments (Ji and Grishman, 2008; Liao and Grishman, 2010; Hong et al., 2011). Li et al. (2013) presented a structured perceptron model to detect triggers and arguments jointly. Similarly, joint dependencies in events were also addressed in the latter domain (Poon and Vanderwende, 2010; McClosky et al., 2011; Riedel and McCallum, 2011; Venugopal et al., 2014). However, none of them incorporated event coreference into their model. Event coreference resolution is more challenging and less explored. To set up event triggers as a starting point of the task, some works use human annotation in a corpus (Bejan and Harabagiu, 2014; Liu et al., 2014), and others use the output of a separate event extraction system (Lee et al., 2012). Berant et al. (2014) presented a model that jointly predicts event arguments and event coreference (as well as other relations between event triggers). However, none of them tries to predict event triggers and event coreferenc</context>
</contexts>
<marker>Venugopal, Chen, Gogate, Ng, 2014</marker>
<rawString>Deepak Venugopal, Chen Chen, Vibhav Gogate, and Vincent Ng. 2014. Relieving the computational bottleneck: Joint inference for event extraction with high-dimensional features. In Proceedings of EMNLP 2014, pages 831–843.</rawString>
</citation>
<citation valid="true">
<authors>
<author>Marc Vilain</author>
<author>John Burger</author>
<author>John Aberdeen</author>
<author>Dennis Connolly</author>
<author>Lynette Hirschman</author>
</authors>
<title>A modeltheoretic coreference scoring scheme.</title>
<date>1995</date>
<booktitle>In Proceedings of MUC-6,</booktitle>
<pages>45--52</pages>
<contexts>
<context position="17613" citStr="Vilain et al., 1995" startWordPosition="2830" endWordPosition="2833">ach by Lee et al. (2013). A natural extension to the approach for performing event trigger identification as well as event coreference resolution would be to develop additional sieves to classify singletons into real event triggers or spurious ones. We leave it for future work. 4.2 Evaluation We evaluate our system using a reference implementation of coreference scoring algorithms (Pradhan et al., 2014; Luo et al., 2014). As for event trigger identification, this scorer computes precision (P), recall (R), and the F1 score. With respect to event coreference resolution, the scorer computes MUC (Vilain et al., 1995), B3 (Bagga and Baldwin, 1998), two CEAF metrics CEAFm and CEAFe (Luo, 2005), and Φ(x, y) · w 2077 MUC B3 CEAF. CEAF, BLANC CoNLL System R P F1 R P F1 R P F1 R P F1 R P F1 F1 Baseline 26.66 19.51 22.53 55.47 58.64 57.01 53.08 60.38 56.50 52.68 63.14 57.44 30.13 25.10 25.05 45.66 Joint 20.00 37.50 26.08 53.37 63.36 57.93 53.93 62.95 58.09 55.06 62.11 58.38 27.51 38.43 31.91 47.45 Table 2: Results of event coreference resolution. ‘Baseline’ refers to the second stage of our baseline. BLANC (Recasens and Hovy, 2011) extended by Luo et al. (2014). We also report the CoNLL average (Denis and Baldri</context>
</contexts>
<marker>Vilain, Burger, Aberdeen, Connolly, Hirschman, 1995</marker>
<rawString>Marc Vilain, John Burger, John Aberdeen, Dennis Connolly, and Lynette Hirschman. 1995. A modeltheoretic coreference scoring scheme. In Proceedings of MUC-6, pages 45–52.</rawString>
</citation>
<citation valid="true">
<authors>
<author>Yue Zhang</author>
<author>Stephen Clark</author>
</authors>
<title>Joint word segmentation and POS tagging using a single perceptron.</title>
<date>2008</date>
<booktitle>In Proceedings of ACL-HLT</booktitle>
<pages>888--896</pages>
<contexts>
<context position="7101" citStr="Zhang and Clark, 2008" startWordPosition="1112" endWordPosition="1115">red. To set up event triggers as a starting point of the task, some works use human annotation in a corpus (Bejan and Harabagiu, 2014; Liu et al., 2014), and others use the output of a separate event extraction system (Lee et al., 2012). Berant et al. (2014) presented a model that jointly predicts event arguments and event coreference (as well as other relations between event triggers). However, none of them tries to predict event triggers and event coreference jointly. Joint structured learning has been applied to several NLP tasks, such as word segmentation and part-of-speech (POS) tagging (Zhang and Clark, 2008a), POS tagging and dependency parsing (Bohnet and Nivre, 2012), dependency parsing and semantic role labeling (Johansson and Nugues, 2008), the extraction of event triggers and arguments (Li et al., 2013), and the extraction of entity mentions and relations (Li and Ji, 2014). Their underlying ideas are similar to ours. That is, one can train a structured learning model to globally capture the interactions between two relevant tasks via a certain kind of structure, while making predictions specifically for these respective tasks. However, no prior work has studied the interactions between even</context>
<context position="11360" citStr="Zhang and Clark, 2008" startWordPosition="1801" endWordPosition="1804">ample, around 13.4% of the 1403 event triggers in ProcessBank have multiple tokens. 2http://opennlp.apache.org/ date (Collins and Roark, 2004) and max-violation update (Huang et al., 2012) to our model. Our initial experiments indicated that early updates happen too early to gain sufficient feedback on weights from entire documents in training examples, ending up with a poorer performance than the standard update. This contrasts with the fact that the early-update strategy was successfully applied to other NLP tasks such as constituent parsing (Collins and Roark, 2004) and dependency parsing (Zhang and Clark, 2008b). The main reason why the early update fell short of the standard update in our setting is that joint event trigger identification and event coreference resolution is a much more difficult task since they require more complex knowledge and argument structures. Due to the difficultly of the task, it is also very difficult to develop such an effective feature set that beam search can explore the search space of an entire document thoroughly with early updates. This observation follows (Bj¨orkelund and Kuhn, 2014) on entity coreference resolution. In contrast, the maxviolation update showed alm</context>
<context position="14211" citStr="Zhang and Clark, 2008" startWordPosition="2275" endWordPosition="2278">GER(l, i). 6: APPENDEVENTTRIGGER(y, e) 7: B[i] ← k-BEST(B[i] ∪ y) 8: for j ← 1..i − 1 do 9: c ← CREATEEVENTCOREF(j, e). 10: ADDEVENTCOREF(y, c) 11: B[i] ← k-BEST(B[i] ∪ y) 12: return B[n][0] and Farkas, 2012). In our initial experiments, we observed that our rule-based filter gained around 97% recall, but extracted around 12,400 false positives against 823 true positives in the training data. This made it difficult for our structured perceptron to learn event triggers, which underperformed on event coreference resolution. We, therefore, employ segment-based decoding with multiple-beam search (Zhang and Clark, 2008a; Li and Ji, 2014) for event trigger identification, and combine it with the best-first clustering (Ng and Cardie, 2002) for event coreference resolution in document-level joint decoding. The key idea of segment-based decoding with multiple-beam search is to keep previous beam states available, and use them to form segments from previous positions to the current position. Let lmax denote the upper bound on the number of tokens in one event trigger. The k-best partial structures (event subgraphs) in beam B at the j-th token is computed as follows: B[j] = k-BEST yE{y[1:j−l]EB[j−l], y[j−l+1,j]=s</context>
</contexts>
<marker>Zhang, Clark, 2008</marker>
<rawString>Yue Zhang and Stephen Clark. 2008a. Joint word segmentation and POS tagging using a single perceptron. In Proceedings of ACL-HLT 2008, pages 888– 896.</rawString>
</citation>
<citation valid="true">
<authors>
<author>Yue Zhang</author>
<author>Stephen Clark</author>
</authors>
<title>A tale of two parsers: Investigating and combining graph-based and transition-based dependency parsing.</title>
<date>2008</date>
<booktitle>In Proceedings of EMNLP</booktitle>
<pages>562--571</pages>
<contexts>
<context position="7101" citStr="Zhang and Clark, 2008" startWordPosition="1112" endWordPosition="1115">red. To set up event triggers as a starting point of the task, some works use human annotation in a corpus (Bejan and Harabagiu, 2014; Liu et al., 2014), and others use the output of a separate event extraction system (Lee et al., 2012). Berant et al. (2014) presented a model that jointly predicts event arguments and event coreference (as well as other relations between event triggers). However, none of them tries to predict event triggers and event coreference jointly. Joint structured learning has been applied to several NLP tasks, such as word segmentation and part-of-speech (POS) tagging (Zhang and Clark, 2008a), POS tagging and dependency parsing (Bohnet and Nivre, 2012), dependency parsing and semantic role labeling (Johansson and Nugues, 2008), the extraction of event triggers and arguments (Li et al., 2013), and the extraction of entity mentions and relations (Li and Ji, 2014). Their underlying ideas are similar to ours. That is, one can train a structured learning model to globally capture the interactions between two relevant tasks via a certain kind of structure, while making predictions specifically for these respective tasks. However, no prior work has studied the interactions between even</context>
<context position="11360" citStr="Zhang and Clark, 2008" startWordPosition="1801" endWordPosition="1804">ample, around 13.4% of the 1403 event triggers in ProcessBank have multiple tokens. 2http://opennlp.apache.org/ date (Collins and Roark, 2004) and max-violation update (Huang et al., 2012) to our model. Our initial experiments indicated that early updates happen too early to gain sufficient feedback on weights from entire documents in training examples, ending up with a poorer performance than the standard update. This contrasts with the fact that the early-update strategy was successfully applied to other NLP tasks such as constituent parsing (Collins and Roark, 2004) and dependency parsing (Zhang and Clark, 2008b). The main reason why the early update fell short of the standard update in our setting is that joint event trigger identification and event coreference resolution is a much more difficult task since they require more complex knowledge and argument structures. Due to the difficultly of the task, it is also very difficult to develop such an effective feature set that beam search can explore the search space of an entire document thoroughly with early updates. This observation follows (Bj¨orkelund and Kuhn, 2014) on entity coreference resolution. In contrast, the maxviolation update showed alm</context>
<context position="14211" citStr="Zhang and Clark, 2008" startWordPosition="2275" endWordPosition="2278">GER(l, i). 6: APPENDEVENTTRIGGER(y, e) 7: B[i] ← k-BEST(B[i] ∪ y) 8: for j ← 1..i − 1 do 9: c ← CREATEEVENTCOREF(j, e). 10: ADDEVENTCOREF(y, c) 11: B[i] ← k-BEST(B[i] ∪ y) 12: return B[n][0] and Farkas, 2012). In our initial experiments, we observed that our rule-based filter gained around 97% recall, but extracted around 12,400 false positives against 823 true positives in the training data. This made it difficult for our structured perceptron to learn event triggers, which underperformed on event coreference resolution. We, therefore, employ segment-based decoding with multiple-beam search (Zhang and Clark, 2008a; Li and Ji, 2014) for event trigger identification, and combine it with the best-first clustering (Ng and Cardie, 2002) for event coreference resolution in document-level joint decoding. The key idea of segment-based decoding with multiple-beam search is to keep previous beam states available, and use them to form segments from previous positions to the current position. Let lmax denote the upper bound on the number of tokens in one event trigger. The k-best partial structures (event subgraphs) in beam B at the j-th token is computed as follows: B[j] = k-BEST yE{y[1:j−l]EB[j−l], y[j−l+1,j]=s</context>
</contexts>
<marker>Zhang, Clark, 2008</marker>
<rawString>Yue Zhang and Stephen Clark. 2008b. A tale of two parsers: Investigating and combining graph-based and transition-based dependency parsing. In Proceedings of EMNLP 2008, pages 562–571.</rawString>
</citation>
<citation valid="true">
<authors>
<author>Kai Zhao</author>
<author>Liang Huang</author>
</authors>
<title>Minibatch and parallelization for online large margin structured learning.</title>
<date>2013</date>
<booktitle>In Proceedings of NAACL-HLT 2013,</booktitle>
<pages>370--379</pages>
<contexts>
<context position="21240" citStr="Zhao and Huang, 2013" startWordPosition="3435" endWordPosition="3438">r knowledge, this is the first work that solves these two tasks simultaneously. Our experiment shows that the proposed method effectively penalizes false positives in joint search, thereby outperforming a pipelined model substantially in event coreference resolution. There are a number of avenues for future work. One can further ensure the advantage of the joint model using a larger corpus. Our preliminary experiment on the ACE 2005 corpus shows that due to its larger document size and event types, one will need to reduce training time by a distributed learning algorithm such as mini-batches (Zhao and Huang, 2013). Another future work is to incorporate other components of events into the model. These include event types, event arguments, and other relations such as subevents. One could leverage them as other learning targets or constraints, and investigate further benefits of joint modeling. Acknowledgments This research was supported in part by DARPA grant FA8750-12-2-0342 funded under the Deep Exploration and Filtering of Text (DEFT) Program, and by U.S. Army Research Office (ARO) grant W911NF-14-1-0436 under the Reading, Extraction, and Assembly of Pathways for Evidentiary Reading (REAPER) Program. </context>
</contexts>
<marker>Zhao, Huang, 2013</marker>
<rawString>Kai Zhao and Liang Huang. 2013. Minibatch and parallelization for online large margin structured learning. In Proceedings of NAACL-HLT 2013, pages 370–379.</rawString>
</citation>
</citationList>
</algorithm>
</algorithms>
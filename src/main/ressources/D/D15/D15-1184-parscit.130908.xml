<?xml version="1.0" encoding="UTF-8"?>
<algorithms version="110505">
<algorithm name="SectLabel" version="110505">
<variant no="0" confidence="0.000002">
<title confidence="0.933644">
Reading Documents for Bayesian Online Change Point Detection
</title>
<author confidence="0.994764">
Taehoon Kim and Jaesik Choi
</author>
<affiliation confidence="0.998957">
School of Electrical and Computer Engineering
Ulsan National Institute of Science and Technology
</affiliation>
<address confidence="0.544346">
Ulsan, Korea
</address>
<email confidence="0.998899">
{carpedm20,jaesik}@unist.ac.kr
</email>
<sectionHeader confidence="0.997393" genericHeader="abstract">
Abstract
</sectionHeader>
<bodyText confidence="0.999333208333333">
Modeling non-stationary time-series data
for making predictions is a challenging
but important task. One of the key is-
sues is to identify long-term changes accu-
rately in time-varying data. Bayesian On-
line Change Point Detection (BO-CPD)
algorithms efficiently detect long-term
changes without assuming the Markov
property which is vulnerable to local sig-
nal noise. We propose a Document
based BO-CPD (DBO-CPD) model which
automatically detects long-term temporal
changes of continuous variables based on
a novel dynamic Bayesian analysis which
combines a non-parametric regression, the
Gaussian Process (GP), with generative
models of texts such as news articles and
posts on social networks. Since texts often
include important clues of signal changes,
DBO-CPD enables the accurate predic-
tion of long-term changes accurately. We
show that our algorithm outperforms exist-
ing BO-CPDs in two real-world datasets:
stock prices and movie revenues.
</bodyText>
<sectionHeader confidence="0.999471" genericHeader="keywords">
1 Introduction
</sectionHeader>
<bodyText confidence="0.999626870370371">
Time series data depends on the latent dependence
structure which changes over time. Thus, sta-
tionary parametric models are not appropriate to
represent such dynamic non-stationary processes.
Change point analysis (Smith, 1975; Stephens,
1994; Chib, 1998; Barry and Hartigan, 1993) fo-
cuses on formal frameworks to determine whether
a change has taken place without assuming the
Markov property which is vulnerable to local sig-
nal noise. When change points are identified, each
part of the time series is approximated by specified
parametric models under the stationary assump-
tions. Such change point detection models have
successfully been applied to a variety of data, such
as stock markets (Chen and Gupta, 1997; Hsu,
1977; Koop and Potter, 2007), analyzing bees’ be-
havior (Xuan and Murphy, 2007), forecasting cli-
mates (Chu and Zhao, 2004; Zhao and Chu, 2010),
and physics experiments (von Toussaint, 2011).
However, offline-based change point analysis suf-
fers from slow retrospective inference which pre-
vents real-time analysis.
Bayesian Online Change Point Detection (BO-
CPD) (Adams and MacKay, 2007; Steyvers and
Brown, 2005; Osborne, 2010; Gu et al., 2013)
overcomes this restriction by exploiting efficient
online inference algorithms. BO-CPD algorithms
efficiently detect long-term changes by analyzing
continuous target values with the Gaussian Pro-
cess (GP), a non-parametric regression method.
The GP-based CPD model is simple and flexible.
However, it is not straightforward to utilize rich
external data such as texts in news articles and
posts in social networks.
In this paper, we propose a novel BO-CPD
model that improves the detection of change
points in continuous signals by incorporating the
rich external information implicitly written in texts
on top of the long-term change analysis of the
GP. In particular, our model finds causes of sig-
nal changes in news articles which are influential
sources of markets of interests.
Given a set of news articles extracted from the
Google News service and a sequence of target,
continuous values, our new model, Document-
based Bayesian Online Change Point Detection
(DBO-CPD), learns a generative model which rep-
resents the probability of a news article given the
run length (a length of consecutive observations
without a change). By using the new prior, DBO-
CPD models a dynamic hazard rate (h) which de-
termines the rate at which change points occur.
In the literature, important information is ex-
tracted from news articles (Nothman et al., 2012;
</bodyText>
<page confidence="0.906481">
1610
</page>
<note confidence="0.983308">
Proceedings of the 2015 Conference on Empirical Methods in Natural Language Processing, pages 1610–1619,
Lisbon, Portugal, 17-21 September 2015. c�2015 Association for Computational Linguistics.
</note>
<figure confidence="0.73845">
(b) DBO-CPD (this work)
</figure>
<figureCaption confidence="0.979273">
Figure 1: This figures illustrates a graphical repre-
</figureCaption>
<bodyText confidence="0.821330590909091">
sentation of BO-CPD and our DBO-CPD model.
xt, rt, and Dt represent a continuous variable of
interest, the run length (hidden) variable, and doc-
uments, respectively. Our modeling contribution
is to add texts D1:t for the accurate prediction of
the run length rt+1.
Schumaker and Chen, 2009; Gid´ofalvi and Elkan,
2001; Fung et al., 2003; Fung et al., 2002; Schu-
maker and Chen, 2006), tweets on Twitter (Si et
al., 2013; Wang et al., 2012; Bollen et al., 2011;
St Louis et al., 2012), online chats (Kim et al.,
2010; Gruhl et al., 2005), and blog posts (Peng et
al., 2015; Mishne and Glance, 2006).
In experiments, we show that DBO-CPD can ef-
fectively distinguish whether an abrupt change is a
change point or not in real-world datasets (see Sec-
tion 3.1). Compared to previous BO-CPD models
which explain the changes by human manual map-
pings, our DBO-CPD automatically explains the
reasons why a change point has occurred by con-
necting the numerical sequence of data and textual
features of news articles.
</bodyText>
<sectionHeader confidence="0.979875" genericHeader="introduction">
2 Bayesian Online Change Point
Detection
</sectionHeader>
<bodyText confidence="0.984081352941176">
This section will review our research problem, the
change point detection (CPD) (Barry and Harti-
gan, 1993), and the Bayesian Online Change Point
Detection (BO-CPD) (Adams and MacKay, 2007)
and our model, Document Based Online Change
Point Detection (DBO-CPD).
Let xt∈R be a data observation at time t. We
assume that a sequence of data (x1, x2, ..., xt)
is composed of several non-overlapping produc-
tive partitions (Barry and Hartigan, 1992). The
boundaries that separate the partitions is called the
change points. Let r be the random variable that
denotes the run length, which is the number of
time steps since the last change point was detected.
rt is the current run at time t. xt (rt
) denotes the
most recent data corresponding to the run rt.
</bodyText>
<subsectionHeader confidence="0.940477">
2.1 Online Recursive Detection
</subsectionHeader>
<bodyText confidence="0.9996752">
To make an optimal prediction of the next data
xt+1, one may need to consider all possible run
lengths rt∈N and a probability distribution over
run length rt. Given a sequence of data up to time
t, x1:t = (x1,x2, ..., xt), the run length prediction
problem is formalized as computing the joint prob-
ability of random variables P(xt+1, x1:t). This
distribution can be calculated in terms of the poste-
rior distribution of run length at time t, P(rt|x1:t),
as follows:
</bodyText>
<equation confidence="0.999717">
P(xt+1|rt, x(rt)
t )P(rt|x1:t)
P(xt+1|x(rt)
t )P(rt|x1:t).(1)
</equation>
<bodyText confidence="0.952702">
The predictive distribution P(xt+1|rt, x(rt)
t ) de-
pends only on the most recent rt observations
xt . The posterior distribution of run length
</bodyText>
<equation confidence="0.989577">
(rt)
P(rt|x1:t) can be computed recursively:
P(rt |x1:t) = P(rt, x1)t) (2)
P(x1:t
where:
XP(x1:t) = P(rt, x1:t). (3)
rt
The joint distribution over run length rt and data
x1:t can be derived by summing P(rt, rt−1, x1:t)
over rt−1:
XP(rt, x1:t) = P(rt, rt−1, x1:t)
rt−1
X= P(rt, xt|rt−1, x1:t−1)P(rt−1, x1:t−1)
rt−1
X= P (rt|rt−1)P(xt|rt−1, x(rt)
t )P(rt−1, x1:t−1).
rt−1
</equation>
<bodyText confidence="0.999179">
This formulation updates the posterior distribution
of the run length given the prior over rt from rt−1
and the predictive distribution of new data.
</bodyText>
<equation confidence="0.954975">
(a) BO-CPD
XP(xt+1,x1:t) =
rt
X=
rt
</equation>
<page confidence="0.899201">
1611
</page>
<bodyText confidence="0.9353291875">
However, the existing BO-CPD model (Adams
and MacKay, 2007) specifies the conditional prior
on the change point P(rt|rt−1) in advance. This
approach may lead to model biased predictions be-
cause the update formula highly relies on the pre-
defined, fixed hazard rate (h). Furthermore, BO-
CPD is incapable of incorporating external infor-
mation that implicitly influences the observation
and explains the reasons for the current change of
the long-term trend.
Figure 2: This figure illustrates the recursive up-
dates of the posterior probability in the DBO-CPD
model. Even the BO-CPD model only uses current
and previous run length to calculate the posterior,
DBO-CPD can utilize the series of text documents
to compute the conditional probability accurately.
</bodyText>
<subsectionHeader confidence="0.999707">
2.2 Document-based Bayesian Online
Change Point Detection
</subsectionHeader>
<bodyText confidence="0.9565486">
This section explains our DBO-CPD model. To
represent the text documents, we add a variable D
which denotes a series of text documents related
to the observed data as shown in Figure 1. Let
Dt be a set of Nt text documents D1t , D2t , ..., DNt
</bodyText>
<equation confidence="0.72183">
t
</equation>
<bodyText confidence="0.9926568">
that are indexed at time of publication t, where Nt
is the number of documents observed at time t.
Then, we can rewrite the joint probability over the
run length as:
where D(rt)
</bodyText>
<equation confidence="0.965146">
t (= Dt−rt+1:t) is the set of the rt most
</equation>
<bodyText confidence="0.9995504">
recent documents. Figure 2 illustrates the recur-
sive updates of posterior probability where solid
lines indicate that the probability mass is passed
upwards and dotted lines indicate the probability
that the current run length rt is set to zero.
</bodyText>
<equation confidence="0.8759118">
Given documents D(rt)
t , the conditional proba-
bility is represented as follows:
P (rt = γ+1  |rt−1 = γ, Diγ))
P (rt−1 = γ, Dtγ)  |rt = γ+1) P(rt = γ+1)
=
γ+1�
¯γ=1 P (rt−1 = γ, Diγ)  |rt = γ¯) P(rt = ¯γ)
P (rt−1 = γ, Dtγ)  |rt = γ + 1) Pgap(γ+1)
P (rt−1 = γ, D(γ)t|rt = γ¯) Pgap(¯γ)
</equation>
<bodyText confidence="0.997863714285714">
where Pgap is the distribution of intervals be-
tween consecutive change-points. As the BO-CPD
model (Adams and MacKay, 2007), we assume the
simplest case where the probability of a change-
point at every step is constant if the length of a
segment is modeled by a discrete exponential (ge-
ometric) distribution as:
</bodyText>
<equation confidence="0.998938">
Pgap(rt|λ) = λexp−λrt (5)
</equation>
<bodyText confidence="0.999640857142857">
where λ &gt; 0, a rate parameter, is the parameter
of the distribution.
The update rule for the prior distribution on rt
makes the computation of the joint distribution
tractable, E+1 P(rt_1=, Dγ) I rt=Y) •Pg.p(Y)•ry i
Because rt can only be increased to γ + 1 or set to
0, the conditional probability is as follows:
</bodyText>
<equation confidence="0.99967225">
P(rt = γ + 1|rt−1 = γ, D(γ)
t )
TD(t, γ|γ+1)
TD(t,γ|γ+1) + TD(t,γ|0)
</equation>
<bodyText confidence="0.99568">
where the function TD(t, α|¯α) is an abbrevia-
tion of P (rt−1=α, Diα)  |rt=¯α) . In Equation (6),
</bodyText>
<equation confidence="0.774777">
TD(t,γ|γ+1)=P(rt−1=γ,D(γ)
t |rt=γ+1) is the
</equation>
<bodyText confidence="0.9985255">
joint probability of the run length rt−1 and a set
of documents D(γ)
t when no change has occurred
at time t and the run length becomes γ + 1. There-
fore, we can simplify the equation by removing
rt−1=γ from the condition as follows:
</bodyText>
<equation confidence="0.999395">
TD(t,γ|γ+1) = P(D(γ)
t |rt=γ + 1). (7)
</equation>
<bodyText confidence="0.968148666666667">
We represent the distribution of words by the bag-
of-words model. Let Dit be the set of M words
that is part of the ith document at time t, i.e.
</bodyText>
<equation confidence="0.911903333333333">
Dit = {di,1
t , di,2
t , ..., di,M
</equation>
<bodyText confidence="0.907183">
t I. In the model, we as-
sume that the probability of word di,j
t is indepen-
dent and identically distributed (iid) given a run
</bodyText>
<equation confidence="0.992828857142857">
�P(rt, x1:t) = E P (rt|rt−1,Dirt−1))
rt−1 D(rt−1)
t
P (xt |rt−1, xtrt−1)) P(rt−1, x1:t−1) (4)
γ+1�
¯γ=1
(6)
</equation>
<page confidence="0.834166">
1612
</page>
<bodyText confidence="0.9822485">
length parameter rt. In this setting, the conditional
probability of the words takes the following form:
</bodyText>
<equation confidence="0.997910833333333">
P (D1 fj
t7) I rt = γ+1) = Z
i,j
(8)
The conditional probability P(di,j
t |rt=γ+1) is
</equation>
<bodyText confidence="0.999376538461538">
represented by two generative models, φwf and
φwi which illustrates word frequency and word im-
pact, respectively. The key intuition of word fre-
quency is that a word tends to close to a change
point if a word has been frequently seen in arti-
cles, published when there was a rapid change.
The key intuition of word impact is that how
much does a word lose information in time which
will be discussed in next section. In our paper,
we use the unnormalized beta distribution of the
weights of words to represent the exponential de-
cays. The probability P( D(γ)  |rt=γ + 1) can be
represented recursively as:
</bodyText>
<equation confidence="0.992465">
P (Dtγ) |rt=7+1) =P (Dtγ)|7+1)
∝ Owi(D(γ)
t |7+1) · Owf(D(γ)
t |7+1)
= Owi(Dt|7+1) · Owf(Dt|7+1)
·Owi(D(γ−1)
t−1 |rt−1=7) · Owf(D(γ−1)
t−1 |rt−1=7)
Owi(di,j
t |7+1) · Owf(di,j
t |7+1) (9)
·Owi(D(γ−1)
t−1 |rt−1=7) · Owf(D(γ−1)
t−1 |rt−1=7)
where:
t , rt = γ)
φwf(dx,y
t |γ) = count(dx,y
Here, φwi(dx,y
t |γ) and φwf(dx,y
t |γ) are empirical
</equation>
<bodyText confidence="0.984731375">
potentials which contribute to represent P(di,j
t |γ).
φwi(·) is explained in Section 2.3. Here, count(E)
is the number of times event E appears in the
dataset. In Equation (9), τt is the time gap (dif-
ference) between t and the time when a document
is generated, and di,j represents a document with-
out considering the time domain.
</bodyText>
<equation confidence="0.998007428571429">
TD(t,γ|0) is represented as follows:
P(rt−1=γ, D(γ)
t |rt=0)
= P(rt−1=γ|rt = 0)P(D(γ)
t |rt=0)
= H(γ+1)P(D(γ)
t |rt=0)
</equation>
<bodyText confidence="0.935814">
where H(τ) is the hazard function (Forbes et al.,
2011),
</bodyText>
<equation confidence="0.946266">
— �gap (τ) (10)
H(τ) —
Et=τ Pgap(t).
</equation>
<figureCaption confidence="0.7678885">
Figure 3: This figure illustrates how our Equa-
tion (9) is calculated and how it determines
</figureCaption>
<bodyText confidence="0.979295642857143">
whether a change occurs or not. If the same data
is given, BO-CPD gives us the same answer to a
question whether an abrupt change at time t is a
change point or not. However, DBO-CPD uses
documents Dγt for its prediction to incorporate
the external information which cannot be inferred
only from the data.
When Pgap is the discrete exponential distribution,
the hazard function is constant at H(τ) = 1/λ
(Adams and MacKay, 2007).
As an illustrative example, suppose that we
found a rapid change in Google stock three days
ago. Today at t = 3, we want to know how the
articles are written and whether it will affect the
change tomorrow (t = 4). As shown in Figure 3,
we can calculate what degree a word, for example
rises or stays, is likely to appear in articles pub-
lished since today, which is P(D(γ)
t |rt = γ+1),
and this probability leads us to predict run lengths
from the texts. Documents for each τt = 0,1 and
2 are generated from the generative models with
a given predicted run length through recursive cal-
culation of the Bayesian models which enables on-
line prediction as shown in Equation (9). This
is the main contribution of this paper that enables
DBO-CPD to infer change points accurately with
information included in text documents.
</bodyText>
<equation confidence="0.99854875">
P (dti,j |rt = γ+ 1) .
�=
i,j
Ei,j count(dit,j, rt = γ).
</equation>
<page confidence="0.953384">
1613
</page>
<sectionHeader confidence="0.9763155" genericHeader="method">
2.3 Generative Models Trained from
Regression
</sectionHeader>
<bodyText confidence="0.9977802">
Let D E RT xNxM be N documents of news arti-
cles which consist of M vocabulary over time do-
main T. Dit E RM is the ith document of a set of
documents generated at time t, and define r E RN
as the corresponding set of the run length, which is
a time gap between the time when the document is
generated and the next change point occurs. Then,
given a text document Dit, we seek to predict the
value of run length r by learning a parameterized
function f:
</bodyText>
<equation confidence="0.9851294">
rˆ = f(Dit; w) (11)
where w E Rd are the weights of text features for
di,1
t , di,2
t , ..., di,M
</equation>
<bodyText confidence="0.9980115">
t which compose documents Dit.
From a collection of N documents, we use linear
regression which is trained by solving the follow-
ing optimization problem:
</bodyText>
<equation confidence="0.899078333333333">
minf(Dit; w) - C
w,Dit
(12)
</equation>
<bodyText confidence="0.999698571428571">
where r(w) is the regularization term and
ξ(w, Dit, rt) is the loss function. Parameter C &gt;
0 is a user-specified constant for balancing r(w)
and the sum of losses.
Let h be a function from a document into a
vector-space representation E Rd. In linear regres-
sion, the function f takes the form:
</bodyText>
<equation confidence="0.616319">
f(Dit; w) = h(Dit)Tw + E (13)
</equation>
<bodyText confidence="0.999699692307692">
where E is Gaussian noise.
Figure 4 illustrates how we trained a linear re-
gression model on a sample article. One issue
is that the run length can not be trained directly.
Suppose that we train r5 = 0 into regression, the
weight w of the model will become 0 even though
the set of words contained in Dj5, bj E 11, ..., T }
is composed of salient words which can incur a
possible future change point. To solve this inter-
pretability problem, we trained the weight in the
inverse exponential domain for the predicted vari-
able, predicting e−rt instead of rt. In this setting,
the predicted run-length takes the form:
</bodyText>
<equation confidence="0.905086">
e− ˆrt = h(Dt)Tw + E. (14)
</equation>
<bodyText confidence="0.9928134">
By this method, the regression model can give a
high weight to a word which often appears close
to change points. We can interpret that highly
Figure 4: This figure illustrates a graphical rep-
resentation of how we train a generative model
from a regression problem. We use a regression
model to predict time gap rt between the release
date of article and the nearest future change point.
The weights of regression model are changed into
the negative exponential scale to be considered as
word impact.
weighted words d are more closely related to an
outbreak of changes than lower weighted words.
With w, we can rewrite the probability of d, τt
given w as:
</bodyText>
<equation confidence="0.9612755">
φwi(d,τt) oc wd · (exp(-1/wd))τt
= wd · exp(—τt/wd). (15)
</equation>
<bodyText confidence="0.917021">
The potential, φwi, can also be represented recur-
sively as follows:
</bodyText>
<equation confidence="0.990936">
φwi(d,τt+1) = φwi(d,τt) · exp(-1/wd), (16)
</equation>
<bodyText confidence="0.906219">
since given a word d, τt+1 = τt+1 holds.
</bodyText>
<sectionHeader confidence="0.999743" genericHeader="method">
3 Experiments
</sectionHeader>
<bodyText confidence="0.999981">
Now we explain experiments of DBO-CPD in two
real-world datasets, stock prices and movie rev-
enues. The first case is the historical end-of-day
stock prices of five information technology corpo-
rations. In the second dataset, we examine daily
film revenues averaged by the number of theaters.
</bodyText>
<subsectionHeader confidence="0.973918">
3.1 Datasets
</subsectionHeader>
<bodyText confidence="0.999963888888889">
In the stock price dataset, we gather data for
five different companies: Apple (AAPL), Google
(GOOG), IBM (IBM), Microsoft (MSFT), and
Facebook (FB). These companies were selected
because they were the top 5 ranked in market value
in 2015.
We chose these technology companies because
the announcement of new IT products and features
and the interests of public media tend to be higher
</bodyText>
<equation confidence="0.985595666666667">
N
ξ(w, Dit, rt) + r(w)
i=1
</equation>
<page confidence="0.988923">
1614
</page>
<figureCaption confidence="0.893243">
Figure 5: (a) Two plots show the results of BO-CPD (top) and DBO-CPD (middle) on Apple stock
prices in January 2014. The stock price is plotted in light gray, with the predictive change points drawn
as small circles. The red line represents the most likely predicted run-lengths for each day. The bottom
figures are a set of visualizations of the top 15 strongly weighted words which are found at selected
change points which BO-CPD is unable to predict. The size of each word represents the weight of its
textual features learned during the training of the regression model.
</figureCaption>
<bodyText confidence="0.64658">
and lead to many news articles. We use the his-
torical stock price data from the Google Finance
service.1.
</bodyText>
<table confidence="0.9996511875">
category words documents words/doc
AAPL 15.0M 29,459 509
AAPL:N 11.0M 18,896 581
GOOG 15.0M 29,422 511
GOOG:N 8.2M 13,658 603
IBM 26.7M 45,741 583
IBM:N 3.4M 4,741 726
MSFT 20.5M 35,905 570
MSFT:N 3.5M 5,070 681
FB 18.9M 38,168 495
FB:N 4.3M 6,625 645
KNGHT 14.4M 16,874 852
INCPT 12.1M 17,155 705
AVGR 3.5M 6,476 537
FRZ 6.8M 15,021 454
INTRS 4.2M 7,846 538
</table>
<tableCaption confidence="0.995639">
Table 1: Dimensions of the datasets used in this
</tableCaption>
<bodyText confidence="0.968071">
paper, after tokenizing and filtering the news ar-
ticles. ‘:N’ means the articles are collected with
additional ‘NASDAQ:’ search query.
The second dataset is a set of movie revenues
averaged by the number of theaters for five months
from the release date of film. We target 5 different
</bodyText>
<footnote confidence="0.922332">
1https://www.google.com/finance
</footnote>
<bodyText confidence="0.9996174">
movies: The Dark Knight (KNGHT), Inception
(INCPT), The Avengers (AVGR), Frozen (FRZ)
and Interstellar (INTRS), because these movies
are on highest-grossing movie list and also are
screened recently. The cumulative daily revenue
per theater is collected from ‘Box Office Mojo’
(www.boxofficemojo.com).
News articles are collected from Google News
and we use Google search queries to extract spe-
cific articles related to each dataset in a specific
time period. During the online article crawling,
we store not only the titles of articles, HTML doc-
uments, and publication dates, but also the num-
ber of related articles. The number of articles is
used to differentiate the weight of news articles
during the training of regression. In the case of
stock price data, we use two different queries to
decrease noise. First, we search with the company
name such as ‘Google’. Then, we use queries spe-
cific to stock ‘NASDAQ:’ to make the content of
articles to be highly relevant to the stock market.
In case of movie data, we search with the movie
title with the additional word ‘movie’ to only col-
lect articles related to the target movie.
With these collected articles, we used two ar-
</bodyText>
<page confidence="0.97965">
1615
</page>
<bodyText confidence="0.999451">
ticle extractors, newspaper (Ou-Yang, 2013) and
python-goose (Grangier, 2013), to automate the
text extraction of 291,057 HTML documents. Af-
ter preprocessing, we could successfully extract
texts from 287,389 (98.74%) HTMLs.
</bodyText>
<subsectionHeader confidence="0.999409">
3.2 Textual Feature Representation
</subsectionHeader>
<bodyText confidence="0.999964529411765">
After extracting texts from HTMLs, we tokenize
the texts into words. We use three different tok-
enization methods which are downcasing the char-
acters, punctuation removal, and removing En-
glish stop words. Table 1 shows the statistics on
the corpora of collected news articles.
With these article corpora, we use a bag-of-
words (BoW) representation to change each word
into a vector representation where words from ar-
ticles are indexed and then weighted. Using these
vectors, we adopt three document representations,
TF, TFIDF, and LOG1P, which extend BoW rep-
resentation. TF and TFIDF (Sparck Jones, 1972)
calculate the importance of a word to a set of doc-
uments based on term frequency. LOG1P (Kogan
et al., 2009) calculates the logarithm of the word
frequencies.
</bodyText>
<subsectionHeader confidence="0.997472">
3.3 Training BO-CPD
</subsectionHeader>
<bodyText confidence="0.999997761904762">
As we noted earlier, we use BO-CPD to train the
regression model to learn high weight for words
which are more related to changes. When we
choose the parameters for the Gaussian Process of
BO-CPD, we try to find the value which makes
the distance of intervals between predicted change
points around 1-2 weeks. This is because we as-
sume that the information included in the articles
will have an immediate effect on the data right af-
ter it is published to the public, so the external
information in texts will indicate the short-term
causes for a future change.
For the reasonable comparison of BO-CPD and
DBO-CPD, we use the same parameter for the
Gaussian Process in both models. After several
experiments we found that a = 1 and b = 1 for
the Gaussian Process and Agap = 250 is appropri-
ate to train BO-CPD in the stock and film datasets.
We separate the training and testing examples for
cross-validation at a ratio of 2 : 1 for each year.
Then we train each model differently by year.
</bodyText>
<sectionHeader confidence="0.767302" genericHeader="method">
3.4 Learning the strength parameter w from
Regression
</sectionHeader>
<bodyText confidence="0.9996085">
The weight w of the regression model gives us an
intuition of how a word is important which affect
</bodyText>
<table confidence="0.999030625">
2010 2011 2012 2013 2014
AAPL BO-CPD 14.93 16.33 16.24 14.44 17.63
AAPL DBO-CPD I 14.81 16.22 16.20 14.21 17.12
AAPL DBO-CPD II 15.15 16.20 16.14 14.40 17.11
GOOG BO-CPD 15.03 15.65 15.49 19.43 19.04
GOOG DBO-CPD I 15.48 15.92 15.21 19.24 19.07
GOOG DBO-CPD II 15.31 15.62 15.36 19.20 19.02
IBM BO-CPD 17.10 17.83 17.42 16.25 16.30
IBM DBO-CPD I 17.66 17.81 17.40 16.20 16.04
IBM DBO-CPD II 17.04 17.82 17.38 16.14 16.39
MSFT BO-CPD 12.41 11.91 14.51 15.60 17.25
MSFT DBO-CPD I 12.33 12.60 14.48 14.92 16.43
MSFT DBO-CPD II 12.21 11.79 14.46 15.00 16.46
FB BO-CPD N/A N/A 12.32 13.07 16.68
FB DBO-CPD I N/A N/A 12.34 13.00 16.24
FB DBO-CPD II N/A N/A 12.43 12.98 16.25
</table>
<tableCaption confidence="0.993637">
Table 2: Negative log likelihood of five stocks
</tableCaption>
<bodyText confidence="0.996178315789474">
(Apple, Google, IBM, Microsoft, and Facebook)
without and with our model per year from 2010
to 2014. DBO-CPD I represents the experiments
without ‘NASDAQ:’ as a search query and DBO-
CPD II is the result of articles searched with
‘NASDAQ:’. Facebook data is not available be-
fore the year 2012.
to the length of the current run. With the predicted
run length calculated in Section 3.3, we change the
run length domain r E R into 0 &lt; r &lt; 1 by pre-
dicting ert rather than rt to solve the interpretabil-
ity problem. Therefore, we can think of a high
weight wz as a powerful word which changes the
current run length r to 0. To maintain the scala-
bility of w, we normalize the weight by rescaling
the range into w E [−1, 1]. With the word rep-
resentation calculated in Section 3.2, we train the
regression model by using the number of relevant
articles as the importance weight of training.
</bodyText>
<sectionHeader confidence="0.718512" genericHeader="evaluation">
3.5 Results
</sectionHeader>
<bodyText confidence="0.99900925">
We evaluate the performance of BO-CPD and
DBO-CPD by comparing the negative log likeli-
hood (NLL) (Turner et al., 2009) of two models at
time t as:
</bodyText>
<equation confidence="0.995674">
T
log p(x1:T|w) = log p(xt|x1:t−1, w).
t=1
</equation>
<bodyText confidence="0.9996855">
We calculate the marginal NLL by year and the re-
sults are described in Table 2 and Table 3. (Face-
book data is not available before the year 2012.)
The difference between DBO-CPD I and DBO-
CPD II is whether the search queries include
‘NASDAQ’. In stock data sets of 5 years, our
model outperforms BO-CPD in Apple, Google,
IBM, Microsoft dataset. The improvements of
</bodyText>
<page confidence="0.994143">
1616
</page>
<figureCaption confidence="0.6242642">
Figure 6: (b) The left plot illustrates daily stock prices of Google in 2013 from early January to late May.
The black line represents the stock price, black circles indicate the predicted change points, and the red
line shows the predicted run length calculated by DBO-CPD. The middle plot shows the negative log
likelihood (NLL) of BO-CPD and DBO-CPD on the same data. The overall marginal NLL of DBO-CPD
(19.1964) is smaller than BO-CPD (19.3438). The two zoomed intervals are the two longest intervals
</figureCaption>
<bodyText confidence="0.998603764705882">
where the negative log likelihood of DBO-CPD is smaller than BO-CPD. The right table shows the
sentences whose run length predicted by the regression model (described in Section 2.3) are the highest
at the two zoomed points, which means the sentences are likely to appear near feature change points.
The boldface words are the top 5 most strongly-weighted terms in the regression model.
DBO-CPD compared to the BO-CPD is statisti-
cally significant with 90% confidence in the four
stocks except for stock of Facebook. We also
found that most of the DBO-CPD II shows bet-
ter results than DBO-CPD I and BO-CPD in most
datasets due to noise reduction of texts through the
additional search query ‘NASDAQ:’. Out of 23
datasets, APPL in 2010 and FB in 2012 are the
only datasets where NLLs of BO-CPD is smaller
(better) than NLLs of DBO-CPD.
One of the advantages of using a linear model
is that we can investigate what the model discov-
ers about different terms. As shown in Figure 5,
we can find negative semantic words such as vi-
cious, whip, and desperately, and words represent-
ing the status of a company like propel, innova-
tions, and grateful are the most strongly-weighted
terms in the regression model. We analyze and vi-
sualize some change points where NLL of DBO-
CPD is lower than NLL of BO-CPD. The results
are shown in Figure 6 and three sentences are the
top 3 most weighted sentences in the regression
model for two changes with the boldface words
of top 5 strongly weighted terms like the terms
big, money, and steadily. A particularly interest-
ing case is the term earth which is found between
Jan. 25 and Feb. 13 in 2013. After we investigated
articles where the sentence is included, we found
that Google announced a new tour guide feature in
Google Earth on Jan. 31 and after this announce-
</bodyText>
<page confidence="0.975433">
1617
</page>
<table confidence="0.999262545454545">
NLL
KNGHT BO-CPD 39.76
KNGHT DBO-CPD I 39.54
INCPT BO-CPD 55.60
INCPT DBO-CPD I 55.54
AVGR BO-CPD 32.12
AVGR DBO-CPD I 32.10
FRZ BO-CPD 51.25
FRZ DBO-CPD I 51.04
INT BO-CPD 38.49
INT DBO-CPD I 38.31
</table>
<tableCaption confidence="0.8595704">
Table 3: Negative log likelihood (NLL) of five
movies (The Dark Knight, Inception, Avengers,
Frozen, and Interstellar) without and with our
model for 1 year from the release date of each
movie.
</tableCaption>
<bodyText confidence="0.997632333333333">
ment the stock price increased. We can also find
that the word million is also a positive term which
can predict a new change in the near feature.
</bodyText>
<sectionHeader confidence="0.999832" genericHeader="conclusions">
4 Conclusions
</sectionHeader>
<bodyText confidence="0.999973071428571">
In this paper, we propose a novel generative model
for online inference to find change points from
non-stationary time-series data. Unlike previ-
ous approaches, our model can incorporate exter-
nal information in texts which may includes the
causes of signal changes. The main contribution
of this paper is to combine the generative model
for online change points detection and a regres-
sion model learned from the weights of words in
documents. Thus, our model accurately infers the
conditional prior of the change points and auto-
matically explains the reasons of a change by con-
necting the numerical sequence of data and textual
features of news articles.
</bodyText>
<sectionHeader confidence="0.999901" genericHeader="discussions">
5 Future work
</sectionHeader>
<bodyText confidence="0.999712">
Our DBO-CPD can be improved further by incor-
porating more external information beyond docu-
ments. In principle, our DBO-CPD can incorpo-
rate other features if they are vectorized into a ma-
trix form. Our implementation currently only uses
the simple bag of words models (TF, TFIDF and
LOG1P) to improve the baseline GP-based CPD
models by bringing documents into change point
detection. One possible direction of future work
would explore ways to fully represent the rich in-
formation in texts by extending the text features
and language representations like continuous bag-
of-words (CBOW) models (Mikolov et al., 2013)
or Global vectors for word representation (GloVe)
(Pennington et al., 2014).
</bodyText>
<sectionHeader confidence="0.997712" genericHeader="acknowledgments">
Acknowledgments
</sectionHeader>
<bodyText confidence="0.999837857142857">
This work was supported by Basic Science
Research Program through the National Re-
search Foundation of Korea (NRF) grant funded
by the Ministry of Science, ICT &amp; Future
Planning (MSIP) (NRF- 2014R1A1A1002662),
the NRF grant funded by the MSIP (NRF-
2014M2A8A2074096).
</bodyText>
<sectionHeader confidence="0.998388" genericHeader="references">
References
</sectionHeader>
<reference confidence="0.974304682926829">
Ryan Prescott Adams and David JC MacKay. 2007.
Bayesian online changepoint detection. arXiv
preprint arXiv:0710.3742.
Daniel Barry and John A Hartigan. 1992. Product par-
tition models for change point problems. The An-
nals of Statistics, pages 260–279.
Daniel Barry and John A Hartigan. 1993. A bayesian
analysis for change point problems. Journal of the
American Statistical Association, 88(421):309–319.
Johan Bollen, Huina Mao, and Xiaojun Zeng. 2011.
Twitter mood predicts the stock market. Journal of
Computational Science, 2(1):1–8.
Jie Chen and AK Gupta. 1997. Testing and locat-
ing variance changepoints with application to stock
prices. Journal of the American Statistical Associa-
tion, 92(438):739–747.
Siddhartha Chib. 1998. Estimation and comparison
of multiple change-point models. Journal of econo-
metrics, 86(2):221–241.
Pao-Shin Chu and Xin Zhao. 2004. Bayesian change-
point analysis of tropical cyclone activity: The
central north pacific case. Journal of Climate,
17(24):4893–4901.
Catherine Forbes, Merran Evans, Nicholas Hastings,
and Brian Peacock. 2011. Statistical distributions.
John Wiley &amp; Sons.
Gabriel Pui Cheong Fung, Jeffrey Xu Yu, and Wai
Lam. 2002. News sensitive stock trend prediction.
In Advances in knowledge discovery and data min-
ing, pages 481–493. Springer.
Gabriel Pui Cheong Fung, Jeffrey Xu Yu, and Wai
Lam. 2003. Stock prediction: Integrating text min-
ing approach using real-time news. In IEEE Inter-
national Conference on Computational Intelligence
for Financial Engineering, pages 395–402.
Gyozo Gid´ofalvi and Charles Elkan. 2001. Us-
ing news articles to predict stock price movements.
Department of Computer Science and Engineering,
University of California, San Diego.
Xavier Grangier. 2013. Python-goose - article extrac-
tor.
</reference>
<page confidence="0.99555">
1618
</page>
<bodyText confidence="0.982599727272727">
Methods in Natural Language Processing (EMNLP
2014), 12:1532–1543.
Daniel Gruhl, Ramanathan Guha, Ravi Kumar, Jasmine
Novak, and Andrew Tomkins. 2005. The predic-
tive power of online chatter. In Proceedings of the
Eleventh ACM SIGKDD International Conference
on Knowledge Discovery and Data Mining, pages
78–87.
Robert Schumaker and Hsinchun Chen. 2006. Textual
analysis of stock market prediction using financial
news articles. AMCIS 2006 Proceedings, page 185.
</bodyText>
<reference confidence="0.9998315">
William Gu, Jaesik Choi, Ming Gu, Horst Simon, and
Kesheng Wu. 2013. Fast change point detection for
electricity market analysis. In IEEE International
Conference on Big Data, pages 50–57.
Der-Ann Hsu. 1977. Tests for variance shift at an un-
known time point. Applied Statistics, pages 279–
284.
Su Nam Kim, Lawrence Cavedon, and Timothy Bald-
win. 2010. Classifying dialogue acts in one-on-
one live chats. In Proceedings of the 2010 Con-
ference on Empirical Methods in Natural Language
Processing, pages 862–871.
Shimon Kogan, Dimitry Levin, Bryan R Routledge, Ja-
cob S Sagi, and Noah A Smith. 2009. Predicting
risk from financial reports with regression. In Pro-
ceedings of Human Language Technologies: Con-
ference of the North American Chapter of the Asso-
ciation for Computational Linguistics, pages 272–
280.
Gary Koop and Simon M. Potter. 2007. Estimation
and forecasting in models with multiple breaks. The
Review of Economic Studies, 74(3):pp. 763–789.
Tomas Mikolov, Kai Chen, Greg Corrado, and Jef-
frey Dean. 2013. Efficient estimation of word
representations in vector space. arXiv preprint
arXiv:1301.3781.
Gilad Mishne and Natalie S Glance. 2006. Predicting
movie sales from blogger sentiment. In AAAI Spring
Symposium: Computational Approaches to Analyz-
ing Weblogs, pages 155–158.
Joel Nothman, Matthew Honnibal, Ben Hachey, and
James R Curran. 2012. Event linking: Grounding
event reference in a news archive. In Proceedings
of the 50th Annual Meeting of the Association for
Computational Linguistics (ACL), pages 228–232.
Michael Osborne. 2010. Bayesian Gaussian processes
for sequential prediction, optimisation and quadra-
ture. Ph.D. thesis, Oxford University New College.
Lucas Ou-Yang. 2013. newspaper - news, full-text,
and article metadata extraction.
Baolin Peng, Jing Li, Junwen Chen, Xu Han, Ruifeng
Xu, and Kam-Fai Wong. 2015. Trending sentiment-
topic detection on twitter. In Computational Lin-
guistics and Intelligent Text Processing, pages 66–
77. Springer.
Jeffrey Pennington, Richard Socher, and Christopher D
Manning. 2014. Glove: Global vectors for
word representation. Proceedings of the Empiricial
Robert P Schumaker and Hsinchun Chen. 2009.
Textual analysis of stock market prediction using
breaking financial news: The azfin text system.
ACM Transactions on Information Systems (TOIS),
27(2):12.
Jianfeng Si, Arjun Mukherjee, Bing Liu, Qing Li,
Huayi Li, and Xiaotie Deng. 2013. Exploiting topic
based twitter sentiment for stock prediction.
AFM Smith. 1975. A bayesian approach to inference
about a change-point in a sequence of random vari-
ables. Biometrika, 62(2):407–416.
Karen Sparck Jones. 1972. A statistical interpretation
of term specificity and its application in retrieval.
Journal of documentation, 28(1):11–21.
Connie St Louis, Gozde Zorlu, et al. 2012. Can twitter
predict disease outbreaks? BMJ, 344.
DA Stephens. 1994. Bayesian retrospective multiple-
changepoint identification. Applied Statistics, pages
159–178.
Mark Steyvers and Scott Brown. 2005. Prediction and
change detection. In Advances in Neural Informa-
tion Processing Systems (NIPS), pages 1281–1288.
Ryan Turner, Yunus Saatci, and Carl Edward Ras-
mussen. 2009. Adaptive sequential bayesian
change point detection.
Udo von Toussaint. 2011. Bayesian inference in
physics. Reviews of Modern Physics, 83(3):943.
Hao Wang, Dogan Can, Abe Kazemzadeh, Franc¸ois
Bar, and Shrikanth Narayanan. 2012. A system for
real-time twitter sentiment analysis of 2012 us pres-
idential election cycle. In Proceedings of the ACL
2012 System Demonstrations, pages 115–120. As-
sociation for Computational Linguistics.
Xiang Xuan and Kevin Murphy. 2007. Modeling
changing dependency structure in multivariate time
series. In Proceedings of the 24th International
Conference on Machine Learning (ICML), pages
1055–1062.
Xin Zhao and Pao-Shin Chu. 2010. Bayesian change-
point analysis for extreme events (typhoons, heavy
rainfall, and heat waves): An rjmcmc approach.
Journal of Climate, 23(5):1034–1046.
</reference>
<page confidence="0.995698">
1619
</page>
</variant>
</algorithm>
<algorithm name="ParsHed" version="110505">
<variant no="0" confidence="0.554722">
<title confidence="0.999864">Reading Documents for Bayesian Online Change Point Detection</title>
<author confidence="0.998032">Kim</author>
<affiliation confidence="0.85355">School of Electrical and Computer Ulsan National Institute of Science and Ulsan,</affiliation>
<abstract confidence="0.999599">Modeling non-stationary time-series data for making predictions is a challenging but important task. One of the key issues is to identify long-term changes accuin time-varying data. Online Change Point Detection (BO-CPD) algorithms efficiently detect long-term changes without assuming the Markov property which is vulnerable to local signoise. We propose a BO-CPD (DBO-CPD) which automatically detects long-term temporal changes of continuous variables based on a novel dynamic Bayesian analysis which combines a non-parametric regression, the Gaussian Process (GP), with generative models of texts such as news articles and posts on social networks. Since texts often include important clues of signal changes, DBO-CPD enables the accurate prediction of long-term changes accurately. We show that our algorithm outperforms existing BO-CPDs in two real-world datasets: stock prices and movie revenues.</abstract>
</variant>
</algorithm>
<algorithm name="ParsCit" version="110505">
<citationList>
<citation valid="true">
<authors>
<author>Ryan Prescott Adams</author>
<author>David JC MacKay</author>
</authors>
<title>Bayesian online changepoint detection. arXiv preprint arXiv:0710.3742.</title>
<date>2007</date>
<contexts>
<context position="2300" citStr="Adams and MacKay, 2007" startWordPosition="334" endWordPosition="337">t of the time series is approximated by specified parametric models under the stationary assumptions. Such change point detection models have successfully been applied to a variety of data, such as stock markets (Chen and Gupta, 1997; Hsu, 1977; Koop and Potter, 2007), analyzing bees’ behavior (Xuan and Murphy, 2007), forecasting climates (Chu and Zhao, 2004; Zhao and Chu, 2010), and physics experiments (von Toussaint, 2011). However, offline-based change point analysis suffers from slow retrospective inference which prevents real-time analysis. Bayesian Online Change Point Detection (BOCPD) (Adams and MacKay, 2007; Steyvers and Brown, 2005; Osborne, 2010; Gu et al., 2013) overcomes this restriction by exploiting efficient online inference algorithms. BO-CPD algorithms efficiently detect long-term changes by analyzing continuous target values with the Gaussian Process (GP), a non-parametric regression method. The GP-based CPD model is simple and flexible. However, it is not straightforward to utilize rich external data such as texts in news articles and posts in social networks. In this paper, we propose a novel BO-CPD model that improves the detection of change points in continuous signals by incorpora</context>
<context position="5215" citStr="Adams and MacKay, 2007" startWordPosition="800" endWordPosition="803">s, we show that DBO-CPD can effectively distinguish whether an abrupt change is a change point or not in real-world datasets (see Section 3.1). Compared to previous BO-CPD models which explain the changes by human manual mappings, our DBO-CPD automatically explains the reasons why a change point has occurred by connecting the numerical sequence of data and textual features of news articles. 2 Bayesian Online Change Point Detection This section will review our research problem, the change point detection (CPD) (Barry and Hartigan, 1993), and the Bayesian Online Change Point Detection (BO-CPD) (Adams and MacKay, 2007) and our model, Document Based Online Change Point Detection (DBO-CPD). Let xt∈R be a data observation at time t. We assume that a sequence of data (x1, x2, ..., xt) is composed of several non-overlapping productive partitions (Barry and Hartigan, 1992). The boundaries that separate the partitions is called the change points. Let r be the random variable that denotes the run length, which is the number of time steps since the last change point was detected. rt is the current run at time t. xt (rt ) denotes the most recent data corresponding to the run rt. 2.1 Online Recursive Detection To make</context>
<context position="7098" citStr="Adams and MacKay, 2007" startWordPosition="1117" endWordPosition="1120">th (rt) P(rt|x1:t) can be computed recursively: P(rt |x1:t) = P(rt, x1)t) (2) P(x1:t where: XP(x1:t) = P(rt, x1:t). (3) rt The joint distribution over run length rt and data x1:t can be derived by summing P(rt, rt−1, x1:t) over rt−1: XP(rt, x1:t) = P(rt, rt−1, x1:t) rt−1 X= P(rt, xt|rt−1, x1:t−1)P(rt−1, x1:t−1) rt−1 X= P (rt|rt−1)P(xt|rt−1, x(rt) t )P(rt−1, x1:t−1). rt−1 This formulation updates the posterior distribution of the run length given the prior over rt from rt−1 and the predictive distribution of new data. (a) BO-CPD XP(xt+1,x1:t) = rt X= rt 1611 However, the existing BO-CPD model (Adams and MacKay, 2007) specifies the conditional prior on the change point P(rt|rt−1) in advance. This approach may lead to model biased predictions because the update formula highly relies on the predefined, fixed hazard rate (h). Furthermore, BOCPD is incapable of incorporating external information that implicitly influences the observation and explains the reasons for the current change of the long-term trend. Figure 2: This figure illustrates the recursive updates of the posterior probability in the DBO-CPD model. Even the BO-CPD model only uses current and previous run length to calculate the posterior, DBO-CP</context>
<context position="8965" citStr="Adams and MacKay, 2007" startWordPosition="1446" endWordPosition="1449"> illustrates the recursive updates of posterior probability where solid lines indicate that the probability mass is passed upwards and dotted lines indicate the probability that the current run length rt is set to zero. Given documents D(rt) t , the conditional probability is represented as follows: P (rt = γ+1 |rt−1 = γ, Diγ)) P (rt−1 = γ, Dtγ) |rt = γ+1) P(rt = γ+1) = γ+1� ¯γ=1 P (rt−1 = γ, Diγ) |rt = γ¯) P(rt = ¯γ) P (rt−1 = γ, Dtγ) |rt = γ + 1) Pgap(γ+1) P (rt−1 = γ, D(γ)t|rt = γ¯) Pgap(¯γ) where Pgap is the distribution of intervals between consecutive change-points. As the BO-CPD model (Adams and MacKay, 2007), we assume the simplest case where the probability of a changepoint at every step is constant if the length of a segment is modeled by a discrete exponential (geometric) distribution as: Pgap(rt|λ) = λexp−λrt (5) where λ &gt; 0, a rate parameter, is the parameter of the distribution. The update rule for the prior distribution on rt makes the computation of the joint distribution tractable, E+1 P(rt_1=, Dγ) I rt=Y) •Pg.p(Y)•ry i Because rt can only be increased to γ + 1 or set to 0, the conditional probability is as follows: P(rt = γ + 1|rt−1 = γ, D(γ) t ) TD(t, γ|γ+1) TD(t,γ|γ+1) + TD(t,γ|0) whe</context>
<context position="12588" citStr="Adams and MacKay, 2007" startWordPosition="2105" endWordPosition="2108"> t |rt=0) where H(τ) is the hazard function (Forbes et al., 2011), — �gap (τ) (10) H(τ) — Et=τ Pgap(t). Figure 3: This figure illustrates how our Equation (9) is calculated and how it determines whether a change occurs or not. If the same data is given, BO-CPD gives us the same answer to a question whether an abrupt change at time t is a change point or not. However, DBO-CPD uses documents Dγt for its prediction to incorporate the external information which cannot be inferred only from the data. When Pgap is the discrete exponential distribution, the hazard function is constant at H(τ) = 1/λ (Adams and MacKay, 2007). As an illustrative example, suppose that we found a rapid change in Google stock three days ago. Today at t = 3, we want to know how the articles are written and whether it will affect the change tomorrow (t = 4). As shown in Figure 3, we can calculate what degree a word, for example rises or stays, is likely to appear in articles published since today, which is P(D(γ) t |rt = γ+1), and this probability leads us to predict run lengths from the texts. Documents for each τt = 0,1 and 2 are generated from the generative models with a given predicted run length through recursive calculation of t</context>
</contexts>
<marker>Adams, MacKay, 2007</marker>
<rawString>Ryan Prescott Adams and David JC MacKay. 2007. Bayesian online changepoint detection. arXiv preprint arXiv:0710.3742.</rawString>
</citation>
<citation valid="true">
<authors>
<author>Daniel Barry</author>
<author>John A Hartigan</author>
</authors>
<title>Product partition models for change point problems. The Annals of Statistics,</title>
<date>1992</date>
<pages>260--279</pages>
<contexts>
<context position="5468" citStr="Barry and Hartigan, 1992" startWordPosition="842" endWordPosition="845">cally explains the reasons why a change point has occurred by connecting the numerical sequence of data and textual features of news articles. 2 Bayesian Online Change Point Detection This section will review our research problem, the change point detection (CPD) (Barry and Hartigan, 1993), and the Bayesian Online Change Point Detection (BO-CPD) (Adams and MacKay, 2007) and our model, Document Based Online Change Point Detection (DBO-CPD). Let xt∈R be a data observation at time t. We assume that a sequence of data (x1, x2, ..., xt) is composed of several non-overlapping productive partitions (Barry and Hartigan, 1992). The boundaries that separate the partitions is called the change points. Let r be the random variable that denotes the run length, which is the number of time steps since the last change point was detected. rt is the current run at time t. xt (rt ) denotes the most recent data corresponding to the run rt. 2.1 Online Recursive Detection To make an optimal prediction of the next data xt+1, one may need to consider all possible run lengths rt∈N and a probability distribution over run length rt. Given a sequence of data up to time t, x1:t = (x1,x2, ..., xt), the run length prediction problem is </context>
</contexts>
<marker>Barry, Hartigan, 1992</marker>
<rawString>Daniel Barry and John A Hartigan. 1992. Product partition models for change point problems. The Annals of Statistics, pages 260–279.</rawString>
</citation>
<citation valid="true">
<authors>
<author>Daniel Barry</author>
<author>John A Hartigan</author>
</authors>
<title>A bayesian analysis for change point problems.</title>
<date>1993</date>
<journal>Journal of the American Statistical Association,</journal>
<volume>88</volume>
<issue>421</issue>
<contexts>
<context position="1479" citStr="Barry and Hartigan, 1993" startWordPosition="206" endWordPosition="209">enerative models of texts such as news articles and posts on social networks. Since texts often include important clues of signal changes, DBO-CPD enables the accurate prediction of long-term changes accurately. We show that our algorithm outperforms existing BO-CPDs in two real-world datasets: stock prices and movie revenues. 1 Introduction Time series data depends on the latent dependence structure which changes over time. Thus, stationary parametric models are not appropriate to represent such dynamic non-stationary processes. Change point analysis (Smith, 1975; Stephens, 1994; Chib, 1998; Barry and Hartigan, 1993) focuses on formal frameworks to determine whether a change has taken place without assuming the Markov property which is vulnerable to local signal noise. When change points are identified, each part of the time series is approximated by specified parametric models under the stationary assumptions. Such change point detection models have successfully been applied to a variety of data, such as stock markets (Chen and Gupta, 1997; Hsu, 1977; Koop and Potter, 2007), analyzing bees’ behavior (Xuan and Murphy, 2007), forecasting climates (Chu and Zhao, 2004; Zhao and Chu, 2010), and physics experi</context>
<context position="5133" citStr="Barry and Hartigan, 1993" startWordPosition="787" endWordPosition="791">., 2005), and blog posts (Peng et al., 2015; Mishne and Glance, 2006). In experiments, we show that DBO-CPD can effectively distinguish whether an abrupt change is a change point or not in real-world datasets (see Section 3.1). Compared to previous BO-CPD models which explain the changes by human manual mappings, our DBO-CPD automatically explains the reasons why a change point has occurred by connecting the numerical sequence of data and textual features of news articles. 2 Bayesian Online Change Point Detection This section will review our research problem, the change point detection (CPD) (Barry and Hartigan, 1993), and the Bayesian Online Change Point Detection (BO-CPD) (Adams and MacKay, 2007) and our model, Document Based Online Change Point Detection (DBO-CPD). Let xt∈R be a data observation at time t. We assume that a sequence of data (x1, x2, ..., xt) is composed of several non-overlapping productive partitions (Barry and Hartigan, 1992). The boundaries that separate the partitions is called the change points. Let r be the random variable that denotes the run length, which is the number of time steps since the last change point was detected. rt is the current run at time t. xt (rt ) denotes the mo</context>
</contexts>
<marker>Barry, Hartigan, 1993</marker>
<rawString>Daniel Barry and John A Hartigan. 1993. A bayesian analysis for change point problems. Journal of the American Statistical Association, 88(421):309–319.</rawString>
</citation>
<citation valid="true">
<authors>
<author>Johan Bollen</author>
<author>Huina Mao</author>
<author>Xiaojun Zeng</author>
</authors>
<title>Twitter mood predicts the stock market.</title>
<date>2011</date>
<journal>Journal of Computational Science,</journal>
<volume>2</volume>
<issue>1</issue>
<contexts>
<context position="4439" citStr="Bollen et al., 2011" startWordPosition="672" endWordPosition="675">n, Portugal, 17-21 September 2015. c�2015 Association for Computational Linguistics. (b) DBO-CPD (this work) Figure 1: This figures illustrates a graphical representation of BO-CPD and our DBO-CPD model. xt, rt, and Dt represent a continuous variable of interest, the run length (hidden) variable, and documents, respectively. Our modeling contribution is to add texts D1:t for the accurate prediction of the run length rt+1. Schumaker and Chen, 2009; Gid´ofalvi and Elkan, 2001; Fung et al., 2003; Fung et al., 2002; Schumaker and Chen, 2006), tweets on Twitter (Si et al., 2013; Wang et al., 2012; Bollen et al., 2011; St Louis et al., 2012), online chats (Kim et al., 2010; Gruhl et al., 2005), and blog posts (Peng et al., 2015; Mishne and Glance, 2006). In experiments, we show that DBO-CPD can effectively distinguish whether an abrupt change is a change point or not in real-world datasets (see Section 3.1). Compared to previous BO-CPD models which explain the changes by human manual mappings, our DBO-CPD automatically explains the reasons why a change point has occurred by connecting the numerical sequence of data and textual features of news articles. 2 Bayesian Online Change Point Detection This section</context>
</contexts>
<marker>Bollen, Mao, Zeng, 2011</marker>
<rawString>Johan Bollen, Huina Mao, and Xiaojun Zeng. 2011. Twitter mood predicts the stock market. Journal of Computational Science, 2(1):1–8.</rawString>
</citation>
<citation valid="true">
<authors>
<author>Jie Chen</author>
<author>AK Gupta</author>
</authors>
<title>Testing and locating variance changepoints with application to stock prices.</title>
<date>1997</date>
<journal>Journal of the American Statistical Association,</journal>
<volume>92</volume>
<issue>438</issue>
<contexts>
<context position="1911" citStr="Chen and Gupta, 1997" startWordPosition="276" endWordPosition="279">ationary parametric models are not appropriate to represent such dynamic non-stationary processes. Change point analysis (Smith, 1975; Stephens, 1994; Chib, 1998; Barry and Hartigan, 1993) focuses on formal frameworks to determine whether a change has taken place without assuming the Markov property which is vulnerable to local signal noise. When change points are identified, each part of the time series is approximated by specified parametric models under the stationary assumptions. Such change point detection models have successfully been applied to a variety of data, such as stock markets (Chen and Gupta, 1997; Hsu, 1977; Koop and Potter, 2007), analyzing bees’ behavior (Xuan and Murphy, 2007), forecasting climates (Chu and Zhao, 2004; Zhao and Chu, 2010), and physics experiments (von Toussaint, 2011). However, offline-based change point analysis suffers from slow retrospective inference which prevents real-time analysis. Bayesian Online Change Point Detection (BOCPD) (Adams and MacKay, 2007; Steyvers and Brown, 2005; Osborne, 2010; Gu et al., 2013) overcomes this restriction by exploiting efficient online inference algorithms. BO-CPD algorithms efficiently detect long-term changes by analyzing con</context>
</contexts>
<marker>Chen, Gupta, 1997</marker>
<rawString>Jie Chen and AK Gupta. 1997. Testing and locating variance changepoints with application to stock prices. Journal of the American Statistical Association, 92(438):739–747.</rawString>
</citation>
<citation valid="true">
<authors>
<author>Siddhartha Chib</author>
</authors>
<title>Estimation and comparison of multiple change-point models.</title>
<date>1998</date>
<journal>Journal of econometrics,</journal>
<volume>86</volume>
<issue>2</issue>
<contexts>
<context position="1452" citStr="Chib, 1998" startWordPosition="204" endWordPosition="205">(GP), with generative models of texts such as news articles and posts on social networks. Since texts often include important clues of signal changes, DBO-CPD enables the accurate prediction of long-term changes accurately. We show that our algorithm outperforms existing BO-CPDs in two real-world datasets: stock prices and movie revenues. 1 Introduction Time series data depends on the latent dependence structure which changes over time. Thus, stationary parametric models are not appropriate to represent such dynamic non-stationary processes. Change point analysis (Smith, 1975; Stephens, 1994; Chib, 1998; Barry and Hartigan, 1993) focuses on formal frameworks to determine whether a change has taken place without assuming the Markov property which is vulnerable to local signal noise. When change points are identified, each part of the time series is approximated by specified parametric models under the stationary assumptions. Such change point detection models have successfully been applied to a variety of data, such as stock markets (Chen and Gupta, 1997; Hsu, 1977; Koop and Potter, 2007), analyzing bees’ behavior (Xuan and Murphy, 2007), forecasting climates (Chu and Zhao, 2004; Zhao and Chu</context>
</contexts>
<marker>Chib, 1998</marker>
<rawString>Siddhartha Chib. 1998. Estimation and comparison of multiple change-point models. Journal of econometrics, 86(2):221–241.</rawString>
</citation>
<citation valid="true">
<authors>
<author>Pao-Shin Chu</author>
<author>Xin Zhao</author>
</authors>
<title>Bayesian changepoint analysis of tropical cyclone activity: The central north pacific case.</title>
<date>2004</date>
<journal>Journal of Climate,</journal>
<volume>17</volume>
<issue>24</issue>
<contexts>
<context position="2038" citStr="Chu and Zhao, 2004" startWordPosition="297" endWordPosition="300">1975; Stephens, 1994; Chib, 1998; Barry and Hartigan, 1993) focuses on formal frameworks to determine whether a change has taken place without assuming the Markov property which is vulnerable to local signal noise. When change points are identified, each part of the time series is approximated by specified parametric models under the stationary assumptions. Such change point detection models have successfully been applied to a variety of data, such as stock markets (Chen and Gupta, 1997; Hsu, 1977; Koop and Potter, 2007), analyzing bees’ behavior (Xuan and Murphy, 2007), forecasting climates (Chu and Zhao, 2004; Zhao and Chu, 2010), and physics experiments (von Toussaint, 2011). However, offline-based change point analysis suffers from slow retrospective inference which prevents real-time analysis. Bayesian Online Change Point Detection (BOCPD) (Adams and MacKay, 2007; Steyvers and Brown, 2005; Osborne, 2010; Gu et al., 2013) overcomes this restriction by exploiting efficient online inference algorithms. BO-CPD algorithms efficiently detect long-term changes by analyzing continuous target values with the Gaussian Process (GP), a non-parametric regression method. The GP-based CPD model is simple and </context>
</contexts>
<marker>Chu, Zhao, 2004</marker>
<rawString>Pao-Shin Chu and Xin Zhao. 2004. Bayesian changepoint analysis of tropical cyclone activity: The central north pacific case. Journal of Climate, 17(24):4893–4901.</rawString>
</citation>
<citation valid="true">
<authors>
<author>Catherine Forbes</author>
<author>Merran Evans</author>
<author>Nicholas Hastings</author>
<author>Brian Peacock</author>
</authors>
<title>Statistical distributions.</title>
<date>2011</date>
<publisher>John Wiley &amp; Sons.</publisher>
<contexts>
<context position="12030" citStr="Forbes et al., 2011" startWordPosition="2006" endWordPosition="2009"> t−1 |rt−1=7) where: t , rt = γ) φwf(dx,y t |γ) = count(dx,y Here, φwi(dx,y t |γ) and φwf(dx,y t |γ) are empirical potentials which contribute to represent P(di,j t |γ). φwi(·) is explained in Section 2.3. Here, count(E) is the number of times event E appears in the dataset. In Equation (9), τt is the time gap (difference) between t and the time when a document is generated, and di,j represents a document without considering the time domain. TD(t,γ|0) is represented as follows: P(rt−1=γ, D(γ) t |rt=0) = P(rt−1=γ|rt = 0)P(D(γ) t |rt=0) = H(γ+1)P(D(γ) t |rt=0) where H(τ) is the hazard function (Forbes et al., 2011), — �gap (τ) (10) H(τ) — Et=τ Pgap(t). Figure 3: This figure illustrates how our Equation (9) is calculated and how it determines whether a change occurs or not. If the same data is given, BO-CPD gives us the same answer to a question whether an abrupt change at time t is a change point or not. However, DBO-CPD uses documents Dγt for its prediction to incorporate the external information which cannot be inferred only from the data. When Pgap is the discrete exponential distribution, the hazard function is constant at H(τ) = 1/λ (Adams and MacKay, 2007). As an illustrative example, suppose that</context>
</contexts>
<marker>Forbes, Evans, Hastings, Peacock, 2011</marker>
<rawString>Catherine Forbes, Merran Evans, Nicholas Hastings, and Brian Peacock. 2011. Statistical distributions. John Wiley &amp; Sons.</rawString>
</citation>
<citation valid="true">
<authors>
<author>Gabriel Pui Cheong Fung</author>
<author>Jeffrey Xu Yu</author>
<author>Wai Lam</author>
</authors>
<title>News sensitive stock trend prediction.</title>
<date>2002</date>
<booktitle>In Advances in knowledge discovery and data mining,</booktitle>
<pages>481--493</pages>
<publisher>Springer.</publisher>
<contexts>
<context position="4336" citStr="Fung et al., 2002" startWordPosition="652" endWordPosition="655">gs of the 2015 Conference on Empirical Methods in Natural Language Processing, pages 1610–1619, Lisbon, Portugal, 17-21 September 2015. c�2015 Association for Computational Linguistics. (b) DBO-CPD (this work) Figure 1: This figures illustrates a graphical representation of BO-CPD and our DBO-CPD model. xt, rt, and Dt represent a continuous variable of interest, the run length (hidden) variable, and documents, respectively. Our modeling contribution is to add texts D1:t for the accurate prediction of the run length rt+1. Schumaker and Chen, 2009; Gid´ofalvi and Elkan, 2001; Fung et al., 2003; Fung et al., 2002; Schumaker and Chen, 2006), tweets on Twitter (Si et al., 2013; Wang et al., 2012; Bollen et al., 2011; St Louis et al., 2012), online chats (Kim et al., 2010; Gruhl et al., 2005), and blog posts (Peng et al., 2015; Mishne and Glance, 2006). In experiments, we show that DBO-CPD can effectively distinguish whether an abrupt change is a change point or not in real-world datasets (see Section 3.1). Compared to previous BO-CPD models which explain the changes by human manual mappings, our DBO-CPD automatically explains the reasons why a change point has occurred by connecting the numerical sequen</context>
</contexts>
<marker>Fung, Yu, Lam, 2002</marker>
<rawString>Gabriel Pui Cheong Fung, Jeffrey Xu Yu, and Wai Lam. 2002. News sensitive stock trend prediction. In Advances in knowledge discovery and data mining, pages 481–493. Springer.</rawString>
</citation>
<citation valid="true">
<authors>
<author>Gabriel Pui Cheong Fung</author>
<author>Jeffrey Xu Yu</author>
<author>Wai Lam</author>
</authors>
<title>Stock prediction: Integrating text mining approach using real-time news.</title>
<date>2003</date>
<booktitle>In IEEE International Conference on Computational Intelligence for Financial Engineering,</booktitle>
<pages>395--402</pages>
<contexts>
<context position="4317" citStr="Fung et al., 2003" startWordPosition="648" endWordPosition="651">012; 1610 Proceedings of the 2015 Conference on Empirical Methods in Natural Language Processing, pages 1610–1619, Lisbon, Portugal, 17-21 September 2015. c�2015 Association for Computational Linguistics. (b) DBO-CPD (this work) Figure 1: This figures illustrates a graphical representation of BO-CPD and our DBO-CPD model. xt, rt, and Dt represent a continuous variable of interest, the run length (hidden) variable, and documents, respectively. Our modeling contribution is to add texts D1:t for the accurate prediction of the run length rt+1. Schumaker and Chen, 2009; Gid´ofalvi and Elkan, 2001; Fung et al., 2003; Fung et al., 2002; Schumaker and Chen, 2006), tweets on Twitter (Si et al., 2013; Wang et al., 2012; Bollen et al., 2011; St Louis et al., 2012), online chats (Kim et al., 2010; Gruhl et al., 2005), and blog posts (Peng et al., 2015; Mishne and Glance, 2006). In experiments, we show that DBO-CPD can effectively distinguish whether an abrupt change is a change point or not in real-world datasets (see Section 3.1). Compared to previous BO-CPD models which explain the changes by human manual mappings, our DBO-CPD automatically explains the reasons why a change point has occurred by connecting t</context>
</contexts>
<marker>Fung, Yu, Lam, 2003</marker>
<rawString>Gabriel Pui Cheong Fung, Jeffrey Xu Yu, and Wai Lam. 2003. Stock prediction: Integrating text mining approach using real-time news. In IEEE International Conference on Computational Intelligence for Financial Engineering, pages 395–402.</rawString>
</citation>
<citation valid="true">
<authors>
<author>Gyozo Gid´ofalvi</author>
<author>Charles Elkan</author>
</authors>
<title>Using news articles to predict stock price movements.</title>
<date>2001</date>
<institution>Department of Computer Science and Engineering, University of California,</institution>
<location>San Diego.</location>
<marker>Gid´ofalvi, Elkan, 2001</marker>
<rawString>Gyozo Gid´ofalvi and Charles Elkan. 2001. Using news articles to predict stock price movements. Department of Computer Science and Engineering, University of California, San Diego.</rawString>
</citation>
<citation valid="true">
<authors>
<author>Xavier Grangier</author>
</authors>
<title>Python-goose - article extractor.</title>
<date>2013</date>
<contexts>
<context position="19481" citStr="Grangier, 2013" startWordPosition="3319" endWordPosition="3320">used to differentiate the weight of news articles during the training of regression. In the case of stock price data, we use two different queries to decrease noise. First, we search with the company name such as ‘Google’. Then, we use queries specific to stock ‘NASDAQ:’ to make the content of articles to be highly relevant to the stock market. In case of movie data, we search with the movie title with the additional word ‘movie’ to only collect articles related to the target movie. With these collected articles, we used two ar1615 ticle extractors, newspaper (Ou-Yang, 2013) and python-goose (Grangier, 2013), to automate the text extraction of 291,057 HTML documents. After preprocessing, we could successfully extract texts from 287,389 (98.74%) HTMLs. 3.2 Textual Feature Representation After extracting texts from HTMLs, we tokenize the texts into words. We use three different tokenization methods which are downcasing the characters, punctuation removal, and removing English stop words. Table 1 shows the statistics on the corpora of collected news articles. With these article corpora, we use a bag-ofwords (BoW) representation to change each word into a vector representation where words from articl</context>
</contexts>
<marker>Grangier, 2013</marker>
<rawString>Xavier Grangier. 2013. Python-goose - article extractor.</rawString>
</citation>
<citation valid="true">
<authors>
<author>William Gu</author>
<author>Jaesik Choi</author>
<author>Ming Gu</author>
<author>Horst Simon</author>
<author>Kesheng Wu</author>
</authors>
<title>Fast change point detection for electricity market analysis.</title>
<date>2013</date>
<booktitle>In IEEE International Conference on Big Data,</booktitle>
<pages>50--57</pages>
<contexts>
<context position="2359" citStr="Gu et al., 2013" startWordPosition="344" endWordPosition="347">ls under the stationary assumptions. Such change point detection models have successfully been applied to a variety of data, such as stock markets (Chen and Gupta, 1997; Hsu, 1977; Koop and Potter, 2007), analyzing bees’ behavior (Xuan and Murphy, 2007), forecasting climates (Chu and Zhao, 2004; Zhao and Chu, 2010), and physics experiments (von Toussaint, 2011). However, offline-based change point analysis suffers from slow retrospective inference which prevents real-time analysis. Bayesian Online Change Point Detection (BOCPD) (Adams and MacKay, 2007; Steyvers and Brown, 2005; Osborne, 2010; Gu et al., 2013) overcomes this restriction by exploiting efficient online inference algorithms. BO-CPD algorithms efficiently detect long-term changes by analyzing continuous target values with the Gaussian Process (GP), a non-parametric regression method. The GP-based CPD model is simple and flexible. However, it is not straightforward to utilize rich external data such as texts in news articles and posts in social networks. In this paper, we propose a novel BO-CPD model that improves the detection of change points in continuous signals by incorporating the rich external information implicitly written in te</context>
</contexts>
<marker>Gu, Choi, Gu, Simon, Wu, 2013</marker>
<rawString>William Gu, Jaesik Choi, Ming Gu, Horst Simon, and Kesheng Wu. 2013. Fast change point detection for electricity market analysis. In IEEE International Conference on Big Data, pages 50–57.</rawString>
</citation>
<citation valid="true">
<authors>
<author>Der-Ann Hsu</author>
</authors>
<title>Tests for variance shift at an unknown time point. Applied Statistics,</title>
<date>1977</date>
<pages>279--284</pages>
<contexts>
<context position="1922" citStr="Hsu, 1977" startWordPosition="280" endWordPosition="281">dels are not appropriate to represent such dynamic non-stationary processes. Change point analysis (Smith, 1975; Stephens, 1994; Chib, 1998; Barry and Hartigan, 1993) focuses on formal frameworks to determine whether a change has taken place without assuming the Markov property which is vulnerable to local signal noise. When change points are identified, each part of the time series is approximated by specified parametric models under the stationary assumptions. Such change point detection models have successfully been applied to a variety of data, such as stock markets (Chen and Gupta, 1997; Hsu, 1977; Koop and Potter, 2007), analyzing bees’ behavior (Xuan and Murphy, 2007), forecasting climates (Chu and Zhao, 2004; Zhao and Chu, 2010), and physics experiments (von Toussaint, 2011). However, offline-based change point analysis suffers from slow retrospective inference which prevents real-time analysis. Bayesian Online Change Point Detection (BOCPD) (Adams and MacKay, 2007; Steyvers and Brown, 2005; Osborne, 2010; Gu et al., 2013) overcomes this restriction by exploiting efficient online inference algorithms. BO-CPD algorithms efficiently detect long-term changes by analyzing continuous tar</context>
</contexts>
<marker>Hsu, 1977</marker>
<rawString>Der-Ann Hsu. 1977. Tests for variance shift at an unknown time point. Applied Statistics, pages 279– 284.</rawString>
</citation>
<citation valid="true">
<authors>
<author>Su Nam Kim</author>
<author>Lawrence Cavedon</author>
<author>Timothy Baldwin</author>
</authors>
<title>Classifying dialogue acts in one-onone live chats.</title>
<date>2010</date>
<booktitle>In Proceedings of the 2010 Conference on Empirical Methods in Natural Language Processing,</booktitle>
<pages>862--871</pages>
<contexts>
<context position="4495" citStr="Kim et al., 2010" startWordPosition="683" endWordPosition="686">omputational Linguistics. (b) DBO-CPD (this work) Figure 1: This figures illustrates a graphical representation of BO-CPD and our DBO-CPD model. xt, rt, and Dt represent a continuous variable of interest, the run length (hidden) variable, and documents, respectively. Our modeling contribution is to add texts D1:t for the accurate prediction of the run length rt+1. Schumaker and Chen, 2009; Gid´ofalvi and Elkan, 2001; Fung et al., 2003; Fung et al., 2002; Schumaker and Chen, 2006), tweets on Twitter (Si et al., 2013; Wang et al., 2012; Bollen et al., 2011; St Louis et al., 2012), online chats (Kim et al., 2010; Gruhl et al., 2005), and blog posts (Peng et al., 2015; Mishne and Glance, 2006). In experiments, we show that DBO-CPD can effectively distinguish whether an abrupt change is a change point or not in real-world datasets (see Section 3.1). Compared to previous BO-CPD models which explain the changes by human manual mappings, our DBO-CPD automatically explains the reasons why a change point has occurred by connecting the numerical sequence of data and textual features of news articles. 2 Bayesian Online Change Point Detection This section will review our research problem, the change point dete</context>
</contexts>
<marker>Kim, Cavedon, Baldwin, 2010</marker>
<rawString>Su Nam Kim, Lawrence Cavedon, and Timothy Baldwin. 2010. Classifying dialogue acts in one-onone live chats. In Proceedings of the 2010 Conference on Empirical Methods in Natural Language Processing, pages 862–871.</rawString>
</citation>
<citation valid="true">
<authors>
<author>Shimon Kogan</author>
<author>Dimitry Levin</author>
<author>Bryan R Routledge</author>
<author>Jacob S Sagi</author>
<author>Noah A Smith</author>
</authors>
<title>Predicting risk from financial reports with regression.</title>
<date>2009</date>
<booktitle>In Proceedings of Human Language Technologies: Conference of the North American Chapter of the Association for Computational Linguistics,</booktitle>
<pages>272--280</pages>
<contexts>
<context position="20374" citStr="Kogan et al., 2009" startWordPosition="3459" endWordPosition="3462">tokenization methods which are downcasing the characters, punctuation removal, and removing English stop words. Table 1 shows the statistics on the corpora of collected news articles. With these article corpora, we use a bag-ofwords (BoW) representation to change each word into a vector representation where words from articles are indexed and then weighted. Using these vectors, we adopt three document representations, TF, TFIDF, and LOG1P, which extend BoW representation. TF and TFIDF (Sparck Jones, 1972) calculate the importance of a word to a set of documents based on term frequency. LOG1P (Kogan et al., 2009) calculates the logarithm of the word frequencies. 3.3 Training BO-CPD As we noted earlier, we use BO-CPD to train the regression model to learn high weight for words which are more related to changes. When we choose the parameters for the Gaussian Process of BO-CPD, we try to find the value which makes the distance of intervals between predicted change points around 1-2 weeks. This is because we assume that the information included in the articles will have an immediate effect on the data right after it is published to the public, so the external information in texts will indicate the short-t</context>
</contexts>
<marker>Kogan, Levin, Routledge, Sagi, Smith, 2009</marker>
<rawString>Shimon Kogan, Dimitry Levin, Bryan R Routledge, Jacob S Sagi, and Noah A Smith. 2009. Predicting risk from financial reports with regression. In Proceedings of Human Language Technologies: Conference of the North American Chapter of the Association for Computational Linguistics, pages 272– 280.</rawString>
</citation>
<citation valid="true">
<authors>
<author>Gary Koop</author>
<author>Simon M Potter</author>
</authors>
<title>Estimation and forecasting in models with multiple breaks.</title>
<date>2007</date>
<journal>The Review of Economic Studies,</journal>
<volume>74</volume>
<issue>3</issue>
<pages>763--789</pages>
<contexts>
<context position="1946" citStr="Koop and Potter, 2007" startWordPosition="282" endWordPosition="285">t appropriate to represent such dynamic non-stationary processes. Change point analysis (Smith, 1975; Stephens, 1994; Chib, 1998; Barry and Hartigan, 1993) focuses on formal frameworks to determine whether a change has taken place without assuming the Markov property which is vulnerable to local signal noise. When change points are identified, each part of the time series is approximated by specified parametric models under the stationary assumptions. Such change point detection models have successfully been applied to a variety of data, such as stock markets (Chen and Gupta, 1997; Hsu, 1977; Koop and Potter, 2007), analyzing bees’ behavior (Xuan and Murphy, 2007), forecasting climates (Chu and Zhao, 2004; Zhao and Chu, 2010), and physics experiments (von Toussaint, 2011). However, offline-based change point analysis suffers from slow retrospective inference which prevents real-time analysis. Bayesian Online Change Point Detection (BOCPD) (Adams and MacKay, 2007; Steyvers and Brown, 2005; Osborne, 2010; Gu et al., 2013) overcomes this restriction by exploiting efficient online inference algorithms. BO-CPD algorithms efficiently detect long-term changes by analyzing continuous target values with the Gaus</context>
</contexts>
<marker>Koop, Potter, 2007</marker>
<rawString>Gary Koop and Simon M. Potter. 2007. Estimation and forecasting in models with multiple breaks. The Review of Economic Studies, 74(3):pp. 763–789.</rawString>
</citation>
<citation valid="true">
<authors>
<author>Tomas Mikolov</author>
<author>Kai Chen</author>
<author>Greg Corrado</author>
<author>Jeffrey Dean</author>
</authors>
<title>Efficient estimation of word representations in vector space. arXiv preprint arXiv:1301.3781.</title>
<date>2013</date>
<marker>Mikolov, Chen, Corrado, Dean, 2013</marker>
<rawString>Tomas Mikolov, Kai Chen, Greg Corrado, and Jeffrey Dean. 2013. Efficient estimation of word representations in vector space. arXiv preprint arXiv:1301.3781.</rawString>
</citation>
<citation valid="true">
<authors>
<author>Gilad Mishne</author>
<author>Natalie S Glance</author>
</authors>
<title>Predicting movie sales from blogger sentiment. In AAAI Spring Symposium: Computational Approaches to Analyzing Weblogs,</title>
<date>2006</date>
<pages>155--158</pages>
<contexts>
<context position="4577" citStr="Mishne and Glance, 2006" startWordPosition="698" endWordPosition="701">llustrates a graphical representation of BO-CPD and our DBO-CPD model. xt, rt, and Dt represent a continuous variable of interest, the run length (hidden) variable, and documents, respectively. Our modeling contribution is to add texts D1:t for the accurate prediction of the run length rt+1. Schumaker and Chen, 2009; Gid´ofalvi and Elkan, 2001; Fung et al., 2003; Fung et al., 2002; Schumaker and Chen, 2006), tweets on Twitter (Si et al., 2013; Wang et al., 2012; Bollen et al., 2011; St Louis et al., 2012), online chats (Kim et al., 2010; Gruhl et al., 2005), and blog posts (Peng et al., 2015; Mishne and Glance, 2006). In experiments, we show that DBO-CPD can effectively distinguish whether an abrupt change is a change point or not in real-world datasets (see Section 3.1). Compared to previous BO-CPD models which explain the changes by human manual mappings, our DBO-CPD automatically explains the reasons why a change point has occurred by connecting the numerical sequence of data and textual features of news articles. 2 Bayesian Online Change Point Detection This section will review our research problem, the change point detection (CPD) (Barry and Hartigan, 1993), and the Bayesian Online Change Point Detec</context>
</contexts>
<marker>Mishne, Glance, 2006</marker>
<rawString>Gilad Mishne and Natalie S Glance. 2006. Predicting movie sales from blogger sentiment. In AAAI Spring Symposium: Computational Approaches to Analyzing Weblogs, pages 155–158.</rawString>
</citation>
<citation valid="true">
<authors>
<author>Joel Nothman</author>
<author>Matthew Honnibal</author>
<author>Ben Hachey</author>
<author>James R Curran</author>
</authors>
<title>Event linking: Grounding event reference in a news archive.</title>
<date>2012</date>
<booktitle>In Proceedings of the 50th Annual Meeting of the Association for Computational Linguistics (ACL),</booktitle>
<pages>228--232</pages>
<contexts>
<context position="3703" citStr="Nothman et al., 2012" startWordPosition="556" endWordPosition="559">les which are influential sources of markets of interests. Given a set of news articles extracted from the Google News service and a sequence of target, continuous values, our new model, Documentbased Bayesian Online Change Point Detection (DBO-CPD), learns a generative model which represents the probability of a news article given the run length (a length of consecutive observations without a change). By using the new prior, DBOCPD models a dynamic hazard rate (h) which determines the rate at which change points occur. In the literature, important information is extracted from news articles (Nothman et al., 2012; 1610 Proceedings of the 2015 Conference on Empirical Methods in Natural Language Processing, pages 1610–1619, Lisbon, Portugal, 17-21 September 2015. c�2015 Association for Computational Linguistics. (b) DBO-CPD (this work) Figure 1: This figures illustrates a graphical representation of BO-CPD and our DBO-CPD model. xt, rt, and Dt represent a continuous variable of interest, the run length (hidden) variable, and documents, respectively. Our modeling contribution is to add texts D1:t for the accurate prediction of the run length rt+1. Schumaker and Chen, 2009; Gid´ofalvi and Elkan, 2001; Fun</context>
</contexts>
<marker>Nothman, Honnibal, Hachey, Curran, 2012</marker>
<rawString>Joel Nothman, Matthew Honnibal, Ben Hachey, and James R Curran. 2012. Event linking: Grounding event reference in a news archive. In Proceedings of the 50th Annual Meeting of the Association for Computational Linguistics (ACL), pages 228–232.</rawString>
</citation>
<citation valid="true">
<authors>
<author>Michael Osborne</author>
</authors>
<title>Bayesian Gaussian processes for sequential prediction, optimisation and quadrature.</title>
<date>2010</date>
<tech>Ph.D. thesis,</tech>
<institution>Oxford University New College.</institution>
<contexts>
<context position="2341" citStr="Osborne, 2010" startWordPosition="342" endWordPosition="343">parametric models under the stationary assumptions. Such change point detection models have successfully been applied to a variety of data, such as stock markets (Chen and Gupta, 1997; Hsu, 1977; Koop and Potter, 2007), analyzing bees’ behavior (Xuan and Murphy, 2007), forecasting climates (Chu and Zhao, 2004; Zhao and Chu, 2010), and physics experiments (von Toussaint, 2011). However, offline-based change point analysis suffers from slow retrospective inference which prevents real-time analysis. Bayesian Online Change Point Detection (BOCPD) (Adams and MacKay, 2007; Steyvers and Brown, 2005; Osborne, 2010; Gu et al., 2013) overcomes this restriction by exploiting efficient online inference algorithms. BO-CPD algorithms efficiently detect long-term changes by analyzing continuous target values with the Gaussian Process (GP), a non-parametric regression method. The GP-based CPD model is simple and flexible. However, it is not straightforward to utilize rich external data such as texts in news articles and posts in social networks. In this paper, we propose a novel BO-CPD model that improves the detection of change points in continuous signals by incorporating the rich external information implic</context>
</contexts>
<marker>Osborne, 2010</marker>
<rawString>Michael Osborne. 2010. Bayesian Gaussian processes for sequential prediction, optimisation and quadrature. Ph.D. thesis, Oxford University New College.</rawString>
</citation>
<citation valid="true">
<authors>
<author>Lucas Ou-Yang</author>
</authors>
<title>newspaper - news, full-text, and article metadata extraction.</title>
<date>2013</date>
<contexts>
<context position="19447" citStr="Ou-Yang, 2013" startWordPosition="3315" endWordPosition="3316">icles. The number of articles is used to differentiate the weight of news articles during the training of regression. In the case of stock price data, we use two different queries to decrease noise. First, we search with the company name such as ‘Google’. Then, we use queries specific to stock ‘NASDAQ:’ to make the content of articles to be highly relevant to the stock market. In case of movie data, we search with the movie title with the additional word ‘movie’ to only collect articles related to the target movie. With these collected articles, we used two ar1615 ticle extractors, newspaper (Ou-Yang, 2013) and python-goose (Grangier, 2013), to automate the text extraction of 291,057 HTML documents. After preprocessing, we could successfully extract texts from 287,389 (98.74%) HTMLs. 3.2 Textual Feature Representation After extracting texts from HTMLs, we tokenize the texts into words. We use three different tokenization methods which are downcasing the characters, punctuation removal, and removing English stop words. Table 1 shows the statistics on the corpora of collected news articles. With these article corpora, we use a bag-ofwords (BoW) representation to change each word into a vector repr</context>
</contexts>
<marker>Ou-Yang, 2013</marker>
<rawString>Lucas Ou-Yang. 2013. newspaper - news, full-text, and article metadata extraction.</rawString>
</citation>
<citation valid="true">
<authors>
<author>Baolin Peng</author>
<author>Jing Li</author>
<author>Junwen Chen</author>
<author>Xu Han</author>
<author>Ruifeng Xu</author>
<author>Kam-Fai Wong</author>
</authors>
<title>Trending sentimenttopic detection on twitter.</title>
<date>2015</date>
<booktitle>In Computational Linguistics and Intelligent Text Processing,</booktitle>
<pages>66--77</pages>
<publisher>Springer.</publisher>
<contexts>
<context position="4551" citStr="Peng et al., 2015" startWordPosition="694" endWordPosition="697">e 1: This figures illustrates a graphical representation of BO-CPD and our DBO-CPD model. xt, rt, and Dt represent a continuous variable of interest, the run length (hidden) variable, and documents, respectively. Our modeling contribution is to add texts D1:t for the accurate prediction of the run length rt+1. Schumaker and Chen, 2009; Gid´ofalvi and Elkan, 2001; Fung et al., 2003; Fung et al., 2002; Schumaker and Chen, 2006), tweets on Twitter (Si et al., 2013; Wang et al., 2012; Bollen et al., 2011; St Louis et al., 2012), online chats (Kim et al., 2010; Gruhl et al., 2005), and blog posts (Peng et al., 2015; Mishne and Glance, 2006). In experiments, we show that DBO-CPD can effectively distinguish whether an abrupt change is a change point or not in real-world datasets (see Section 3.1). Compared to previous BO-CPD models which explain the changes by human manual mappings, our DBO-CPD automatically explains the reasons why a change point has occurred by connecting the numerical sequence of data and textual features of news articles. 2 Bayesian Online Change Point Detection This section will review our research problem, the change point detection (CPD) (Barry and Hartigan, 1993), and the Bayesian</context>
</contexts>
<marker>Peng, Li, Chen, Han, Xu, Wong, 2015</marker>
<rawString>Baolin Peng, Jing Li, Junwen Chen, Xu Han, Ruifeng Xu, and Kam-Fai Wong. 2015. Trending sentimenttopic detection on twitter. In Computational Linguistics and Intelligent Text Processing, pages 66– 77. Springer.</rawString>
</citation>
<citation valid="true">
<authors>
<author>Jeffrey Pennington</author>
<author>Richard Socher</author>
<author>Christopher D Manning</author>
</authors>
<title>Glove: Global vectors for word representation.</title>
<date>2014</date>
<booktitle>Proceedings of the Empiricial</booktitle>
<marker>Pennington, Socher, Manning, 2014</marker>
<rawString>Jeffrey Pennington, Richard Socher, and Christopher D Manning. 2014. Glove: Global vectors for word representation. Proceedings of the Empiricial</rawString>
</citation>
<citation valid="true">
<authors>
<author>Robert P Schumaker</author>
<author>Hsinchun Chen</author>
</authors>
<title>Textual analysis of stock market prediction using breaking financial news: The azfin text system.</title>
<date>2009</date>
<journal>ACM Transactions on Information Systems (TOIS),</journal>
<volume>27</volume>
<issue>2</issue>
<contexts>
<context position="4270" citStr="Schumaker and Chen, 2009" startWordPosition="640" endWordPosition="643">ion is extracted from news articles (Nothman et al., 2012; 1610 Proceedings of the 2015 Conference on Empirical Methods in Natural Language Processing, pages 1610–1619, Lisbon, Portugal, 17-21 September 2015. c�2015 Association for Computational Linguistics. (b) DBO-CPD (this work) Figure 1: This figures illustrates a graphical representation of BO-CPD and our DBO-CPD model. xt, rt, and Dt represent a continuous variable of interest, the run length (hidden) variable, and documents, respectively. Our modeling contribution is to add texts D1:t for the accurate prediction of the run length rt+1. Schumaker and Chen, 2009; Gid´ofalvi and Elkan, 2001; Fung et al., 2003; Fung et al., 2002; Schumaker and Chen, 2006), tweets on Twitter (Si et al., 2013; Wang et al., 2012; Bollen et al., 2011; St Louis et al., 2012), online chats (Kim et al., 2010; Gruhl et al., 2005), and blog posts (Peng et al., 2015; Mishne and Glance, 2006). In experiments, we show that DBO-CPD can effectively distinguish whether an abrupt change is a change point or not in real-world datasets (see Section 3.1). Compared to previous BO-CPD models which explain the changes by human manual mappings, our DBO-CPD automatically explains the reasons </context>
</contexts>
<marker>Schumaker, Chen, 2009</marker>
<rawString>Robert P Schumaker and Hsinchun Chen. 2009. Textual analysis of stock market prediction using breaking financial news: The azfin text system. ACM Transactions on Information Systems (TOIS), 27(2):12.</rawString>
</citation>
<citation valid="true">
<authors>
<author>Jianfeng Si</author>
<author>Arjun Mukherjee</author>
<author>Bing Liu</author>
<author>Qing Li</author>
<author>Huayi Li</author>
<author>Xiaotie Deng</author>
</authors>
<title>Exploiting topic based twitter sentiment for stock prediction.</title>
<date>2013</date>
<contexts>
<context position="4399" citStr="Si et al., 2013" startWordPosition="664" endWordPosition="667">e Processing, pages 1610–1619, Lisbon, Portugal, 17-21 September 2015. c�2015 Association for Computational Linguistics. (b) DBO-CPD (this work) Figure 1: This figures illustrates a graphical representation of BO-CPD and our DBO-CPD model. xt, rt, and Dt represent a continuous variable of interest, the run length (hidden) variable, and documents, respectively. Our modeling contribution is to add texts D1:t for the accurate prediction of the run length rt+1. Schumaker and Chen, 2009; Gid´ofalvi and Elkan, 2001; Fung et al., 2003; Fung et al., 2002; Schumaker and Chen, 2006), tweets on Twitter (Si et al., 2013; Wang et al., 2012; Bollen et al., 2011; St Louis et al., 2012), online chats (Kim et al., 2010; Gruhl et al., 2005), and blog posts (Peng et al., 2015; Mishne and Glance, 2006). In experiments, we show that DBO-CPD can effectively distinguish whether an abrupt change is a change point or not in real-world datasets (see Section 3.1). Compared to previous BO-CPD models which explain the changes by human manual mappings, our DBO-CPD automatically explains the reasons why a change point has occurred by connecting the numerical sequence of data and textual features of news articles. 2 Bayesian On</context>
</contexts>
<marker>Si, Mukherjee, Liu, Li, Li, Deng, 2013</marker>
<rawString>Jianfeng Si, Arjun Mukherjee, Bing Liu, Qing Li, Huayi Li, and Xiaotie Deng. 2013. Exploiting topic based twitter sentiment for stock prediction.</rawString>
</citation>
<citation valid="true">
<authors>
<author>AFM Smith</author>
</authors>
<title>A bayesian approach to inference about a change-point in a sequence of random variables.</title>
<date>1975</date>
<journal>Biometrika,</journal>
<volume>62</volume>
<issue>2</issue>
<contexts>
<context position="1424" citStr="Smith, 1975" startWordPosition="200" endWordPosition="201">ession, the Gaussian Process (GP), with generative models of texts such as news articles and posts on social networks. Since texts often include important clues of signal changes, DBO-CPD enables the accurate prediction of long-term changes accurately. We show that our algorithm outperforms existing BO-CPDs in two real-world datasets: stock prices and movie revenues. 1 Introduction Time series data depends on the latent dependence structure which changes over time. Thus, stationary parametric models are not appropriate to represent such dynamic non-stationary processes. Change point analysis (Smith, 1975; Stephens, 1994; Chib, 1998; Barry and Hartigan, 1993) focuses on formal frameworks to determine whether a change has taken place without assuming the Markov property which is vulnerable to local signal noise. When change points are identified, each part of the time series is approximated by specified parametric models under the stationary assumptions. Such change point detection models have successfully been applied to a variety of data, such as stock markets (Chen and Gupta, 1997; Hsu, 1977; Koop and Potter, 2007), analyzing bees’ behavior (Xuan and Murphy, 2007), forecasting climates (Chu </context>
</contexts>
<marker>Smith, 1975</marker>
<rawString>AFM Smith. 1975. A bayesian approach to inference about a change-point in a sequence of random variables. Biometrika, 62(2):407–416.</rawString>
</citation>
<citation valid="true">
<authors>
<author>Karen Sparck Jones</author>
</authors>
<title>A statistical interpretation of term specificity and its application in retrieval.</title>
<date>1972</date>
<journal>Journal of documentation,</journal>
<pages>28--1</pages>
<contexts>
<context position="20265" citStr="Jones, 1972" startWordPosition="3440" endWordPosition="3441">sentation After extracting texts from HTMLs, we tokenize the texts into words. We use three different tokenization methods which are downcasing the characters, punctuation removal, and removing English stop words. Table 1 shows the statistics on the corpora of collected news articles. With these article corpora, we use a bag-ofwords (BoW) representation to change each word into a vector representation where words from articles are indexed and then weighted. Using these vectors, we adopt three document representations, TF, TFIDF, and LOG1P, which extend BoW representation. TF and TFIDF (Sparck Jones, 1972) calculate the importance of a word to a set of documents based on term frequency. LOG1P (Kogan et al., 2009) calculates the logarithm of the word frequencies. 3.3 Training BO-CPD As we noted earlier, we use BO-CPD to train the regression model to learn high weight for words which are more related to changes. When we choose the parameters for the Gaussian Process of BO-CPD, we try to find the value which makes the distance of intervals between predicted change points around 1-2 weeks. This is because we assume that the information included in the articles will have an immediate effect on the d</context>
</contexts>
<marker>Jones, 1972</marker>
<rawString>Karen Sparck Jones. 1972. A statistical interpretation of term specificity and its application in retrieval. Journal of documentation, 28(1):11–21.</rawString>
</citation>
<citation valid="true">
<authors>
<author>Connie St Louis</author>
<author>Gozde Zorlu</author>
</authors>
<title>Can twitter predict disease outbreaks?</title>
<date>2012</date>
<journal>BMJ,</journal>
<pages>344</pages>
<marker>Louis, Zorlu, 2012</marker>
<rawString>Connie St Louis, Gozde Zorlu, et al. 2012. Can twitter predict disease outbreaks? BMJ, 344.</rawString>
</citation>
<citation valid="true">
<authors>
<author>DA Stephens</author>
</authors>
<title>Bayesian retrospective multiplechangepoint identification. Applied Statistics,</title>
<date>1994</date>
<pages>159--178</pages>
<contexts>
<context position="1440" citStr="Stephens, 1994" startWordPosition="202" endWordPosition="203">aussian Process (GP), with generative models of texts such as news articles and posts on social networks. Since texts often include important clues of signal changes, DBO-CPD enables the accurate prediction of long-term changes accurately. We show that our algorithm outperforms existing BO-CPDs in two real-world datasets: stock prices and movie revenues. 1 Introduction Time series data depends on the latent dependence structure which changes over time. Thus, stationary parametric models are not appropriate to represent such dynamic non-stationary processes. Change point analysis (Smith, 1975; Stephens, 1994; Chib, 1998; Barry and Hartigan, 1993) focuses on formal frameworks to determine whether a change has taken place without assuming the Markov property which is vulnerable to local signal noise. When change points are identified, each part of the time series is approximated by specified parametric models under the stationary assumptions. Such change point detection models have successfully been applied to a variety of data, such as stock markets (Chen and Gupta, 1997; Hsu, 1977; Koop and Potter, 2007), analyzing bees’ behavior (Xuan and Murphy, 2007), forecasting climates (Chu and Zhao, 2004; </context>
</contexts>
<marker>Stephens, 1994</marker>
<rawString>DA Stephens. 1994. Bayesian retrospective multiplechangepoint identification. Applied Statistics, pages 159–178.</rawString>
</citation>
<citation valid="true">
<authors>
<author>Mark Steyvers</author>
<author>Scott Brown</author>
</authors>
<title>Prediction and change detection.</title>
<date>2005</date>
<booktitle>In Advances in Neural Information Processing Systems (NIPS),</booktitle>
<pages>1281--1288</pages>
<contexts>
<context position="2326" citStr="Steyvers and Brown, 2005" startWordPosition="338" endWordPosition="341">approximated by specified parametric models under the stationary assumptions. Such change point detection models have successfully been applied to a variety of data, such as stock markets (Chen and Gupta, 1997; Hsu, 1977; Koop and Potter, 2007), analyzing bees’ behavior (Xuan and Murphy, 2007), forecasting climates (Chu and Zhao, 2004; Zhao and Chu, 2010), and physics experiments (von Toussaint, 2011). However, offline-based change point analysis suffers from slow retrospective inference which prevents real-time analysis. Bayesian Online Change Point Detection (BOCPD) (Adams and MacKay, 2007; Steyvers and Brown, 2005; Osborne, 2010; Gu et al., 2013) overcomes this restriction by exploiting efficient online inference algorithms. BO-CPD algorithms efficiently detect long-term changes by analyzing continuous target values with the Gaussian Process (GP), a non-parametric regression method. The GP-based CPD model is simple and flexible. However, it is not straightforward to utilize rich external data such as texts in news articles and posts in social networks. In this paper, we propose a novel BO-CPD model that improves the detection of change points in continuous signals by incorporating the rich external inf</context>
</contexts>
<marker>Steyvers, Brown, 2005</marker>
<rawString>Mark Steyvers and Scott Brown. 2005. Prediction and change detection. In Advances in Neural Information Processing Systems (NIPS), pages 1281–1288.</rawString>
</citation>
<citation valid="true">
<authors>
<author>Ryan Turner</author>
<author>Yunus Saatci</author>
<author>Carl Edward Rasmussen</author>
</authors>
<title>Adaptive sequential bayesian change point detection.</title>
<date>2009</date>
<contexts>
<context position="23313" citStr="Turner et al., 2009" startWordPosition="3988" endWordPosition="3991">ge the run length domain r E R into 0 &lt; r &lt; 1 by predicting ert rather than rt to solve the interpretability problem. Therefore, we can think of a high weight wz as a powerful word which changes the current run length r to 0. To maintain the scalability of w, we normalize the weight by rescaling the range into w E [−1, 1]. With the word representation calculated in Section 3.2, we train the regression model by using the number of relevant articles as the importance weight of training. 3.5 Results We evaluate the performance of BO-CPD and DBO-CPD by comparing the negative log likelihood (NLL) (Turner et al., 2009) of two models at time t as: T log p(x1:T|w) = log p(xt|x1:t−1, w). t=1 We calculate the marginal NLL by year and the results are described in Table 2 and Table 3. (Facebook data is not available before the year 2012.) The difference between DBO-CPD I and DBOCPD II is whether the search queries include ‘NASDAQ’. In stock data sets of 5 years, our model outperforms BO-CPD in Apple, Google, IBM, Microsoft dataset. The improvements of 1616 Figure 6: (b) The left plot illustrates daily stock prices of Google in 2013 from early January to late May. The black line represents the stock price, black c</context>
</contexts>
<marker>Turner, Saatci, Rasmussen, 2009</marker>
<rawString>Ryan Turner, Yunus Saatci, and Carl Edward Rasmussen. 2009. Adaptive sequential bayesian change point detection.</rawString>
</citation>
<citation valid="true">
<authors>
<author>Udo von Toussaint</author>
</authors>
<title>Bayesian inference in physics.</title>
<date>2011</date>
<journal>Reviews of Modern Physics,</journal>
<volume>83</volume>
<issue>3</issue>
<marker>von Toussaint, 2011</marker>
<rawString>Udo von Toussaint. 2011. Bayesian inference in physics. Reviews of Modern Physics, 83(3):943.</rawString>
</citation>
<citation valid="true">
<authors>
<author>Hao Wang</author>
<author>Dogan Can</author>
<author>Abe Kazemzadeh</author>
<author>Franc¸ois Bar</author>
<author>Shrikanth Narayanan</author>
</authors>
<title>A system for real-time twitter sentiment analysis of 2012 us presidential election cycle.</title>
<date>2012</date>
<booktitle>In Proceedings of the ACL 2012 System Demonstrations,</booktitle>
<pages>115--120</pages>
<publisher>Association</publisher>
<institution>for Computational Linguistics.</institution>
<contexts>
<context position="4418" citStr="Wang et al., 2012" startWordPosition="668" endWordPosition="671">es 1610–1619, Lisbon, Portugal, 17-21 September 2015. c�2015 Association for Computational Linguistics. (b) DBO-CPD (this work) Figure 1: This figures illustrates a graphical representation of BO-CPD and our DBO-CPD model. xt, rt, and Dt represent a continuous variable of interest, the run length (hidden) variable, and documents, respectively. Our modeling contribution is to add texts D1:t for the accurate prediction of the run length rt+1. Schumaker and Chen, 2009; Gid´ofalvi and Elkan, 2001; Fung et al., 2003; Fung et al., 2002; Schumaker and Chen, 2006), tweets on Twitter (Si et al., 2013; Wang et al., 2012; Bollen et al., 2011; St Louis et al., 2012), online chats (Kim et al., 2010; Gruhl et al., 2005), and blog posts (Peng et al., 2015; Mishne and Glance, 2006). In experiments, we show that DBO-CPD can effectively distinguish whether an abrupt change is a change point or not in real-world datasets (see Section 3.1). Compared to previous BO-CPD models which explain the changes by human manual mappings, our DBO-CPD automatically explains the reasons why a change point has occurred by connecting the numerical sequence of data and textual features of news articles. 2 Bayesian Online Change Point D</context>
</contexts>
<marker>Wang, Can, Kazemzadeh, Bar, Narayanan, 2012</marker>
<rawString>Hao Wang, Dogan Can, Abe Kazemzadeh, Franc¸ois Bar, and Shrikanth Narayanan. 2012. A system for real-time twitter sentiment analysis of 2012 us presidential election cycle. In Proceedings of the ACL 2012 System Demonstrations, pages 115–120. Association for Computational Linguistics.</rawString>
</citation>
<citation valid="true">
<authors>
<author>Xiang Xuan</author>
<author>Kevin Murphy</author>
</authors>
<title>Modeling changing dependency structure in multivariate time series.</title>
<date>2007</date>
<booktitle>In Proceedings of the 24th International Conference on Machine Learning (ICML),</booktitle>
<pages>1055--1062</pages>
<contexts>
<context position="1996" citStr="Xuan and Murphy, 2007" startWordPosition="290" endWordPosition="293">nary processes. Change point analysis (Smith, 1975; Stephens, 1994; Chib, 1998; Barry and Hartigan, 1993) focuses on formal frameworks to determine whether a change has taken place without assuming the Markov property which is vulnerable to local signal noise. When change points are identified, each part of the time series is approximated by specified parametric models under the stationary assumptions. Such change point detection models have successfully been applied to a variety of data, such as stock markets (Chen and Gupta, 1997; Hsu, 1977; Koop and Potter, 2007), analyzing bees’ behavior (Xuan and Murphy, 2007), forecasting climates (Chu and Zhao, 2004; Zhao and Chu, 2010), and physics experiments (von Toussaint, 2011). However, offline-based change point analysis suffers from slow retrospective inference which prevents real-time analysis. Bayesian Online Change Point Detection (BOCPD) (Adams and MacKay, 2007; Steyvers and Brown, 2005; Osborne, 2010; Gu et al., 2013) overcomes this restriction by exploiting efficient online inference algorithms. BO-CPD algorithms efficiently detect long-term changes by analyzing continuous target values with the Gaussian Process (GP), a non-parametric regression met</context>
</contexts>
<marker>Xuan, Murphy, 2007</marker>
<rawString>Xiang Xuan and Kevin Murphy. 2007. Modeling changing dependency structure in multivariate time series. In Proceedings of the 24th International Conference on Machine Learning (ICML), pages 1055–1062.</rawString>
</citation>
<citation valid="true">
<authors>
<author>Xin Zhao</author>
<author>Pao-Shin Chu</author>
</authors>
<title>Bayesian changepoint analysis for extreme events (typhoons, heavy rainfall, and heat waves): An rjmcmc approach.</title>
<date>2010</date>
<journal>Journal of Climate,</journal>
<volume>23</volume>
<issue>5</issue>
<contexts>
<context position="2059" citStr="Zhao and Chu, 2010" startWordPosition="301" endWordPosition="304">; Chib, 1998; Barry and Hartigan, 1993) focuses on formal frameworks to determine whether a change has taken place without assuming the Markov property which is vulnerable to local signal noise. When change points are identified, each part of the time series is approximated by specified parametric models under the stationary assumptions. Such change point detection models have successfully been applied to a variety of data, such as stock markets (Chen and Gupta, 1997; Hsu, 1977; Koop and Potter, 2007), analyzing bees’ behavior (Xuan and Murphy, 2007), forecasting climates (Chu and Zhao, 2004; Zhao and Chu, 2010), and physics experiments (von Toussaint, 2011). However, offline-based change point analysis suffers from slow retrospective inference which prevents real-time analysis. Bayesian Online Change Point Detection (BOCPD) (Adams and MacKay, 2007; Steyvers and Brown, 2005; Osborne, 2010; Gu et al., 2013) overcomes this restriction by exploiting efficient online inference algorithms. BO-CPD algorithms efficiently detect long-term changes by analyzing continuous target values with the Gaussian Process (GP), a non-parametric regression method. The GP-based CPD model is simple and flexible. However, it</context>
</contexts>
<marker>Zhao, Chu, 2010</marker>
<rawString>Xin Zhao and Pao-Shin Chu. 2010. Bayesian changepoint analysis for extreme events (typhoons, heavy rainfall, and heat waves): An rjmcmc approach. Journal of Climate, 23(5):1034–1046.</rawString>
</citation>
</citationList>
</algorithm>
</algorithms>
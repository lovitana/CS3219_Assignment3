<?xml version="1.0" encoding="UTF-8"?>
<algorithms version="110505">
<algorithm name="SectLabel" version="110505">
<variant no="0" confidence="0.027605">
<title confidence="0.999391">
“A Spousal Relation Begins with a Deletion of engage
and Ends with an Addition of divorce”:
Learning State Changing Verbs from Wikipedia Revision History
</title>
<author confidence="0.989557">
Derry Tanti Wijaya
</author>
<affiliation confidence="0.989877">
Carnegie Mellon University
</affiliation>
<address confidence="0.9102585">
5000 Forbes Avenue
Pittsburgh, PA, 15213
</address>
<email confidence="0.999305">
dwijaya@cs.cmu.edu
</email>
<author confidence="0.973419">
Ndapandula Nakashole
</author>
<affiliation confidence="0.98164">
Carnegie Mellon University
</affiliation>
<address confidence="0.909751">
5000 Forbes Avenue
Pittsburgh, PA, 15213
</address>
<email confidence="0.999365">
ndapa@cs.cmu.edu
</email>
<author confidence="0.991854">
Tom M. Mitchell
</author>
<affiliation confidence="0.989272">
Carnegie Mellon University
</affiliation>
<address confidence="0.91046">
5000 Forbes Avenue
Pittsburgh, PA, 15213
</address>
<email confidence="0.999433">
tom.mitchell@cs.cmu.edu
</email>
<sectionHeader confidence="0.993914" genericHeader="abstract">
Abstract
</sectionHeader>
<bodyText confidence="0.999900115384615">
Learning to determine when the time-
varying facts of a Knowledge Base (KB)
have to be updated is a challenging task.
We propose to learn state changing verbs
from Wikipedia edit history. When a
state-changing event, such as a marriage
or death, happens to an entity, the in-
fobox on the entity’s Wikipedia page usu-
ally gets updated. At the same time, the
article text may be updated with verbs ei-
ther being added or deleted to reflect the
changes made to the infobox. We use
Wikipedia edit history to distantly super-
vise a method for automatically learning
verbs and state changes. Additionally, our
method uses constraints to effectively map
verbs to infobox changes. We observe in
our experiments that when state-changing
verbs are added or deleted from an en-
tity’s Wikipedia page text, we can predict
the entity’s infobox updates with 88% pre-
cision and 76% recall. One compelling
application of our verbs is to incorporate
them as triggers in methods for updating
existing KBs, which are currently mostly
static.
</bodyText>
<sectionHeader confidence="0.999131" genericHeader="keywords">
1 Introduction
</sectionHeader>
<bodyText confidence="0.99994956">
Extracting relational facts between entities and
storing them in knowledge bases (KBs) has been a
topic of active research in recent years. The result-
ing KBs are generally static and are not updated
as the facts change (Suchanek et al., 2007; Carl-
son et al., 2010; Fader et al., 2011; Mitchell et al.,
2015). One possible approach to updating KBs is
to extract facts from dynamic Web content such
as news (Nakashole and Weikum, 2012). In this
paper, we propose to predict state changes caused
by verbs acting on entities in text. This is differ-
ent from simply applying the same text extraction
pipeline, that created the original KB, to dynamic
Web content.
In particular, our approach has the following ad-
vantages: (1) Consider for example the SPOUSE
relation, both marry and divorce are good patterns
for extracting this relation. In our work, we wish
to learn that they cause different state changes.
Thus, we can update the entity’s fact and its tem-
poral scope (Wijaya et al., 2014a). (2) Learning
state changing verbs can pave the way for learn-
ing the ordering of verbs in terms of their pre- and
post-conditions.
Our approach learns state changing verbs from
Wikipedia revision history. In particular, we seek
to establish a correspondence between infobox ed-
its and verbs edits in the same article. The infobox
of a Wikipedia article is a structured box that sum-
marizes an entity as a set of facts (attribute-value
pairs) . Our assumption is that when a state-
changing event happens to an entity e.g., a mar-
riage, its Wikipedia infobox is updated by adding
a new SPOUSE value. At approximately the same
time, the article text might be updated with verbs
that express the event, e.g., X is now married to Y.
Figure 1 is an example of an infobox of an entity
changing at the same time as the article’s main text
to reflect a marriage event.
Wikipedia revision history of many articles can
act as distant supervision data for learning the cor-
respondence between text and infobox changes.
However, these revisions are very noisy. Many in-
fobox slots can be updated when a single event
happens. For example, when a death happens,
slots regarding birth e.g., birthdate, birthplace,
may also be updated or added if they were miss-
ing before. Therefore, our method has to handle
these sources of noise. We leverage logical con-
straints to rule out meaningless mappings between
</bodyText>
<page confidence="0.911176">
518
</page>
<note confidence="0.8542655">
Proceedings of the 2015 Conference on Empirical Methods in Natural Language Processing, pages 518–523,
Lisbon, Portugal, 17-21 September 2015. c�2015 Association for Computational Linguistics.
</note>
<figure confidence="0.923536333333333">
marriage
time
05/23/2014 05/24/2014 05/25/2014
</figure>
<figureCaption confidence="0.982387333333333">
Figure 1: A snapshot of Kim Kardashian’s Wikipedia revision history, highlighting text and infobox
changes. In red (and green) are the differences between the page on 05/25/2014 and 05/23/2014: things
that are deleted from (and added to) the page.
</figureCaption>
<bodyText confidence="0.968363272727273">
infobox and text changes.
In summary, our contributions are as follows:
(1) we present an algorithm that uses Wikipedia
edit histories as distantly labeled data to learn
which verbs result in which state changes to en-
tities, and experimentally demonstrate its success,
(2) we make available this set of distantly la-
beled training data on our website1, and (3) we
also make available our learned mappings from
verbs to state changes, as a resource for other re-
searchers, on the same website.
</bodyText>
<sectionHeader confidence="0.991242" genericHeader="introduction">
2 Method
</sectionHeader>
<subsectionHeader confidence="0.998548">
2.1 Data Construction
</subsectionHeader>
<bodyText confidence="0.995668470588235">
We construct a dataset from Wikipedia edit histo-
ries of person entities whose facts change between
the year 2007 and 2012 (i.e., have at least one fact
in YAGO KB (Suchanek et al., 2007) with a start
or end time in this period). We obtain Wikipedia
URLs of this set of entities P from YAGO and
crawl their article’s revision history. Given a per-
son p, his/her Wikipedia revision history Rp has
a set of ordered dates Tp on which revisions are
made to his/her Wikipedia page (we consider date
granularity). Each revision rp,t E Rp is his/her
Wikipedia page at date t where t E Tp.
Each Wikipedia revision rp,t is a set of infobox
slots Sp,t and textual content Cp,t. Each infobox
slot s E Sp,t is a quadruple, (satt, svalue, sstart,
send) containing the attribute name (non-empty),
the attribute value, and the start and end time for
</bodyText>
<footnote confidence="0.98635">
1http://www.cs.cmu.edu/ dwijaya/postcondition.html
</footnote>
<bodyText confidence="0.998445225806452">
which this attribute-value pair holds in reality.
A document dp,t in our data set is the difference2
between any two consecutive revisions separated
by more than 24 hours i.e., dp,t = rp,t+2 − rp,t,
where rp,t+2 is the first revision on date t + 2 and
rp,t is the last revision on date t (as a page can be
revised many times in a day).
A document dp,t is therefore a set of infobox
changes ASp,t and textual changes ACp,t. Each
slot change δs E ASp,t = (satt, δsvalue, δsstart,
δsend) is prefixed with + or − to indicate whether
they are added or deleted in rp,t+2. Similarly, each
text change δc E ACp,t is prefixed with + or − to
indicate whether they are added or deleted.
For example, in Figure 1, a docu-
ment dkim,05/23/2014 = rkim,05/25/2014 −
rkim,05/23/2014 is a set of slot changes: (SPOUSE,
+“Kanye West”, +“2014”, “ ”), (PARTNER, −“Kanye
West”, −“2012-present; engaged”, “ ”) and a set of
text changes: +“Kardashian and West were married in
May 2014”, −“She began dating West”, −“they became
engaged in October 2013”.
For each dp,t, we use ASp,t to label the docu-
ment and ACp,t to extract features for the docu-
ment. We label dp,t that has a new value or start
time added to its infobox: (satt, +δsvalue, *, *) E
ASp,t or (satt, *, +δsstart, *) E ASp,t with the la-
bel begin-satt and label dp,t that has a new end
time added to its infobox: (satt, *, *, +δsend) E
ASp,t with the label end-satt.
The label represents the state change that
</bodyText>
<footnote confidence="0.5537325">
2a HTML document obtained by “compare selected revi-
sions” functionality in Wikipedia
</footnote>
<page confidence="0.994103">
519
</page>
<bodyText confidence="0.999937702702703">
happens in dp,t. For example, in Figure 1,
dkim, 05/23/2014 is labeled with begin-spouse.
The revision history dataset that we make avail-
able for future research consists of all documents
dp,t, labeled and unlabeled, bt E Tp, t E
[01/01/2007, 12/31/2012], and bp E P; a to-
tal of 288,184 documents from revision histories
of 16,909 Wikipedia entities. Using our labeling
process, we find that out of 288,184 documents,
only 41,139 have labels (i.e., have their infobox
updated with new values/start/end time). The dis-
tribution of labels in the dataset is skewed towards
birth and death events as these are life events that
happen to almost all person entities in Wikipedia.
The distribution of labels in the dataset that we re-
lease can be seen in Figure 2. We show only labels
that we evaluate in our task.
For our task of learning state changing verbs
from this revision history dataset, for each la-
beled dp,t, we extract as features, verbs (or
verbs+prepositions) v E OCp,t of which its sub-
ject (or object) matches the Wikipedia entity p
and its object (or subject resp.) matches an in-
fobox value, start or end time: (vsubject, vobject) =
(arg1, arg2) or (vsubject, vobject) = (arg2, arg1), where
arg1= p and (Satt,arg2, *, *) or (Satt, *,arg2, *)
or (Satt, *, *,arg2) E OSp,t. We use Stanford
CoreNLP (Manning et al., 2014) to dependency
parse sentences and extract the subjects and ob-
jects of verbs. We find that 27,044 out of the
41,139 labeled documents contain verb edits, but
only 4,735 contain verb edits with two arguments,
where one argument matches the entity and an-
other matches the value of the infobox change. We
use the latter for our task, to improve the chance
that the verb edits used as features are related to
the infobox change.
</bodyText>
<subsectionHeader confidence="0.998573">
2.2 Model
</subsectionHeader>
<bodyText confidence="0.998892444444444">
We use a Maximum Entropy (MAXENT) clas-
sifier3 given a set of training data = {(vd,, y)}
where vd,, = (v1, v2, ... v|V |) E R|V  |is the JVJ-
dimensional representation of a labeled document
dt where V is the set of all verbs in our training
data, and y is the label of dt as defined in 2.1.
These training documents are used to estimate a
set of weight vectors w = {w1, w2, ... w|Y |}, wy
E R|V |, one for each label y E Y , the set of all
</bodyText>
<footnote confidence="0.9527645">
3We use MALLET implementation of MAXENT:
http://mallet.cs.umass.edu/
</footnote>
<bodyText confidence="0.993497666666667">
labels in our training data. The classifier can then
be applied to classify an unlabeled document du
using:
</bodyText>
<equation confidence="0.9978385">
exp(wy &apos; vdu) (1)
p(yJvdu) = Ey, exp(wy, &apos; vdu)
</equation>
<subsectionHeader confidence="0.986275">
2.3 Feature Selection using Constraints
</subsectionHeader>
<bodyText confidence="0.9998345">
While feature weights from the MAXENT model
allow us to identify verbs that are good features
for predicting a particular state change label, our
distantly supervised training data is inherently
noisy. Changes to multiple infoboxes can hap-
pen within our revision. We therefore utilize con-
straints among state changes to select consistent
verb features for each type of state change.
We use two types of constraints: (1) mutual ex-
clusion (Mutex) which indicate that mutex state
changes do not happen at the same time e.g., up-
date on birthdate should not typically happen with
update on deathcause. Hence, their state changing
verbs should be different. (2) Simultaneous (Sim)
constraints which indicate that simultaneous state
changes should typically happen at the same time
e.g., update on birthdate should typically happen
with other birth-related updates such as birthplace,
birthname, etc. We manually specified these two
types of constraints to all pairs infoboxes where
they apply. We have 10 mutex constraints and 23
simultaneously updated constraints. The full list
of constraints can be found in our website.
Given a set of constraints, a set of labels Y ,
and a set of base verbs4 B in our training data,
we solve a Mixed-Integer Program (MIP) for each
base verb b E B to estimate whether b should be a
feature for state change y E Y .
We obtain label membership probabilities
{P(yJb) = count(y, b)/ Ey, count(y0, b)} from
our training data. The MIP takes the scores P(yJb)
and constraints as input and produces a bit vector
of labels ab as output, each bit ayb E {0, 1} repre-
sents whether or not b should be a feature for y.
The MIP formulation for a base verb b is pre-
sented by Equation 2. For each b, this method tries
to maximize the sum of scores of selected labels,
after penalizing for violation of label constraints.
Let ζy,y, be slack variables for Sim constraints, and
ξy,y, be slack variables for Mutex constraints.
</bodyText>
<footnote confidence="0.990025">
4The verb root or base form of a verb (after removing
preposition)
</footnote>
<page confidence="0.966633">
520
</page>
<figure confidence="0.966200413043478">
4500
4000
3500
3000
2500
2000
1500
1000
500
0
begin-deathplace
begin-birthplace
begin-deathdate
begin-birthdate
begin-name
begin-office
begin-occupation
begin-predecessor
begin-termstart
begin-successor
begin-termend
begin-currentclub
begin-birthname
begin-died
end-spouse
begin-location
end-yearsactive
begin-children
begin-spouse
begin-party
begin-yearsactive
begin-cityofbirth
begin-clubs
begin-almamater
begin-profession
begin-awards
begin-countryofdeath
begin-cityofdeath
begin-president
begin-alternativenames
begin-nationalteam
begin-youthclubs
begin-deathcause
begin-education
begin-club
begin-death
</figure>
<figureCaption confidence="0.999991">
Figure 2: Distribution of labels we evaluate in our task in the revision history dataset.
</figureCaption>
<bodyText confidence="0.9995086">
Solving MIP per base verb is fast; we reduce the
number of labels considered per base verb i.e., we
only consider a label y to be a candidate for b if
∃ vi ∈ V s.t. wiy &gt; 0 and b = base form of vi.
After we output ab for each b, we select features
for each label. We only select a verb vi to be a
feature for y if the learned weight wiy &gt; 0 and
ayb = 1, where b = the base form of vi. Essentially
for each label, we select verb features that have
positive weights and are consistent for the label.
</bodyText>
<figureCaption confidence="0.993345">
Figure 3: Results of predicting state change labels
(infobox types) using verb features.
</figureCaption>
<figure confidence="0.9721913125">
1
0.9
0.8
0.7
0.6
0.5
0.4
0.3
0.2
0.1
0
Precision Recall F1
Majority-­‐class MaxEnt MaxEnt+MIP
maximize C r r
ab, ζy,y0 , ξy,y0 ayb ∗ P(y|b)−ζy,y0 −
y hy,y0i∈Sim
</figure>
<equation confidence="0.7160255">
hy,y0i� utex �ξy,y0 (2)
(ay b − ay0 )&apos; ≤ ζy,y0 , ∀hy, y0i ∈ Sim
subject to b
ayb + ay0
b ≤ 1 + ξy,y0 , ∀hy, y0i ∈ Mutex
ζy,y0 , ξy,y0≥ 0, ayb ∈ {0, 1}, ∀y, y0
</equation>
<sectionHeader confidence="0.993934" genericHeader="background">
3 Experiments
</sectionHeader>
<bodyText confidence="0.999919538461538">
We use 90% of our labeled documents that have
verb edits as features (section 2.1) as training data
and test on the remaining 10%. Since revision his-
tory data is noisy, we manually go through our test
data to discard documents that have incorrect in-
fobox labels by looking the text that changed. The
task is to predict for each document (revision), the
label (infobox slot change) of the document given
its verbs features. We compute precision, recall,
and F1 values of our predictions and compare the
values before and after feature selection (Fig. 3).
To the best of our knowledge, the task to learn
state-changing verbs in terms of states defined
in existing knowledge bases and learning it from
Wikipedia edit histories is novel. There is no pre-
vious approach that can be used as baseline; there-
fore we have compared our structured prediction
using MIP and MAXENT with a majority class
baseline. Both our approaches (MAXENT and
MAXENT + MIP) perform better than the majority
class baseline (Figure 3).
We observe the value of doing feature selec-
tion by asserting constraints in an MIP formula-
tion. Feature selection improves precision; re-
sulting in a better F1. By asserting constraints,
some of the inconsistent verb features for the la-
</bodyText>
<page confidence="0.994971">
521
</page>
<table confidence="0.980201588235294">
Label Verb
begin- +(arg1) die on (arg2), +(arg1) die (arg2),
deathdate +(arg1) pass on (arg2)
begin- +(arg1) be born in (arg2), +(arg1) bear in (arg2),
birthplace +(arg1) be born at (arg2)
begin- +(arg1) succeed (arg2), +(arg1) replace (arg2),
predecessor +(arg1) join cabinet as (arg2), +(arg1) join as (arg2)
begin- +(arg1) lose seat to (arg2), +(arg1) resign on (arg2),
successor +(arg1) resign from post on (arg2)
begin- +(arg1) be appointed on (arg2), +(arg1) serve from (arg2),
termstart +(arg1) be elected on (arg2)
begin- +(arg1) marry on (arg2), +(arg1) marry (arg2),
spouse +(arg1) be married on (arg2), -(arg1) be engaged to (arg2)
end-spouse +(arg1) file for divorce in (arg2), +(arg1) die on (arg2),
+(arg1) divorce in (arg2)
begin- +(arg1) start career with (arg2),
youthclubs +(arg1) begin career with (arg2), +(arg1) start with (arg2)
</table>
<tableCaption confidence="0.99967">
Table 1: Comparison of verb phrases learned be-
</tableCaption>
<bodyText confidence="0.9891224375">
fore and after feature selection for various labels
(infobox types). The texts in bold are (preposi-
tion+) noun that occur most frequently with the
(verb phrase, label) pair in the training data.
bels were removed. For example, before feature
selection, the verbs: “marry”, and “be married
to” were high-weighted features for both begin-
spouse and end-spouse. After asserting constraints
that begin-spouse is mutex with end-spouse, these
verbs (whose base form is “marry”) are filtered out
from the features of end-spouse. We show some of
the learned verb features (after feature selection)
for some of the labels in (Table 1). In average,
we have about 18 verbs per infobox state change
in our state changing verb resource that we make
available for future research.
</bodyText>
<sectionHeader confidence="0.999915" genericHeader="related work">
4 Related Work
</sectionHeader>
<bodyText confidence="0.990895363636364">
Learning from Wikipedia Revision History.
Wikipedia edit history has been exploited in a
number of problems. A popular task in this re-
gard is that of Wikipedia edit history categoriza-
tion (Daxenberger and Gurevych, 2013). This task
involves characterizing a given edit instance as one
of many possible categories such as spelling error
correction, paraphrasing, vandalism, and textual
entailment (Nelken and Yamangil, 2008; Cahill et
al., 2013; Zanzotto and Pennacchiotti, 2010; Re-
casens et al., 2013). Prior methods target various
tasks different from ours.
Learning State Changing Verbs. Very few
works have studied the problem of learning state
changing verbs. (Hosseini et al., 2014) learned
state changing verbs in the context of solving
arithmetic word problems. They learned the effect
of words such as add, subtract on the current state.
The VerbOcean resource was automatically gener-
ated from the Web (Chklovski and Pantel, 2004).
The authors studied the problem of fine-grained
semantic relationships between verbs. They learn
relations such as if someone has bought an item,
they may sell it at a later time. This then involves
capturing empirical regularities such as “X buys
Y” happens before “X sells Y”. Unlike the work
we present here, the methods of (Chklovski and
Pantel, 2004; Hosseini et al., 2014) do not make a
connection to KB relations such as Wikipedia in-
foboxes. In a vision paper, (Wijaya et al., 2014b)
give high level descriptions of a number of possi-
ble methods for learning state changing methods.
They did not implement any of them.
</bodyText>
<sectionHeader confidence="0.999303" genericHeader="conclusions">
5 Conclusion
</sectionHeader>
<bodyText confidence="0.9997678">
In this paper we presented a method that learns
state changing verb phrases from Wikipedia revi-
sion history. We first constructed and curated a
novel dataset from Wikipedia revision history that
is tailored to our task. We showed that this dataset
is useful for learning verb phrase features that are
effective for predicting state changes in the knowl-
edge base (KB), where we considered the KB to
be infoboxes and their values. As future work we
wish to explore the usefulness of our verb resource
to other KBs to improve KB freshness. This is im-
portant because existing KBs are mostly static. We
wish to also explore the application of the learned
verb resource to domains other than Wikipedia in-
fobox and text e.g., for predicting state changes in
the knowledge base from news text.
In this paper, we learned post-conditions of
verbs: state changes that occur when an event ex-
pressed by a verb happens. As future work we
would also explore the feasibility of learning pre-
conditions of verbs from Wikipedia revisions. Ad-
ditionally, most Wikipedia revisions only have text
changes without the associated infobox change.
Therefore, another line of future work is to also
learn from these unlabeled documents.
</bodyText>
<sectionHeader confidence="0.998296" genericHeader="acknowledgments">
Acknowledgments
</sectionHeader>
<bodyText confidence="0.99579525">
We thank members of the NELL team at CMU
for their helpful comments. This research was
supported by DARPA under contract number
FA87501320005.
</bodyText>
<page confidence="0.994773">
522
</page>
<sectionHeader confidence="0.989599" genericHeader="references">
References
</sectionHeader>
<reference confidence="0.999693987341772">
Aoife Cahill, Nitin Madnani, Joel Tetreault, and Di-
ane Napolitano. 2013. Robust systems for preposi-
tion error correction using wikipedia revisions. In In
Proceedings of the Conference of the North Ameri-
can Chapter of the Association for Computational
Linguistics: Human Language Technologies.
Andrew Carlson, Justin Betteridge, Bryan Kisiel,
Burr Settles, Estevam R Hruschka Jr, and Tom M
Mitchell. 2010. Toward an architecture for never-
ending language learning. In AAAI, volume 5,
page 3.
Timothy Chklovski and Patrick Pantel. 2004. Verbo-
cean: Mining the web for fine-grained semantic verb
relations. In Proceedings of EMNLP 2004, pages
33–40.
Johannes Daxenberger and Iryna Gurevych. 2013. Au-
tomatically classifying edit categories in wikipedia
revisions. In EMNLP, pages 578–589.
Anthony Fader, Stephen Soderland, and Oren Etzioni.
2011. Identifying relations for open information ex-
traction. In Proceedings of the Conference on Em-
pirical Methods in Natural Language Processing,
pages 1535–1545. Association for Computational
Linguistics.
Mohammad Javad Hosseini, Hannaneh Hajishirzi,
Oren Etzioni, and Nate Kushman. 2014. Learning
to solve arithmetic word problems with verb catego-
rization. In EMNLP, pages 523–533. ACL.
Christopher D. Manning, Mihai Surdeanu, John Bauer,
Jenny Finkel, Steven J. Bethard, and David Mc-
Closky. 2014. The Stanford CoreNLP natural lan-
guage processing toolkit. In Proceedings of 52nd
Annual Meeting of the Association for Computa-
tional Linguistics: System Demonstrations, pages
55–60.
Tom M. Mitchell, William W. Cohen, Estevam R. Hr-
uschka Jr., Partha Pratim Talukdar, Justin Bet-
teridge, Andrew Carlson, Bhavana Dalvi Mishra,
Matthew Gardner, Bryan Kisiel, Jayant Krishna-
murthy, Ni Lao, Kathryn Mazaitis, Thahir Mo-
hamed, Ndapandula Nakashole, Emmanouil Anto-
nios Platanios, Alan Ritter, Mehdi Samadi, Burr Set-
tles, Richard C. Wang, Derry Tanti Wijaya, Abhi-
nav Gupta, Xinlei Chen, Abulhair Saparov, Malcolm
Greaves, and Joel Welling. 2015. Never-ending
learning. In Proceedings of the Twenty-Ninth AAAI
Conference on Artificial Intelligence, January 25-
30, 2015, Austin, Texas, USA., pages 2302–2310.
Ndapandula Nakashole and Gerhard Weikum. 2012.
Real-time population of knowledge bases: opportu-
nities and challenges. In Proceedings of the Joint
Workshop on Automatic Knowledge Base Construc-
tion and Web-scale Knowledge Extraction, pages
41–45. Association for Computational Linguistics.
Rani Nelken and Elif Yamangil. 2008. Mining
wikipedias article revision history for training com-
putational linguistics algorithms.
Marta Recasens, Cristian Danescu-Niculescu-Mizil,
and Dan Jurafsky. 2013. Linguistic models for an-
alyzing and detecting biased language. In ACL (1),
pages 1650–1659.
Fabian M Suchanek, Gjergji Kasneci, and Gerhard
Weikum. 2007. Yago: a core of semantic knowl-
edge. In Proceedings of the 16th international con-
ference on World Wide Web, pages 697–706. ACM.
Derry Wijaya, Ndapa Nakashole, and Tom Mitchell.
2014a. CTPs: Contextual temporal profiles for time
scoping facts via entity state change detection. In
Proceedings of the Conference on Empirical Meth-
ods in Natural Language Processing. Association
for Computational Linguistics.
Derry Tanti Wijaya, Ndapandula Nakashole, and
Tom M Mitchell. 2014b. Mining and organizing a
resource of state-changing verbs. In Proceedings of
the Joint Workshop on Automatic Knowledge Base
Construction and Web-scale Knowledge Extraction.
Fabio Massimo Zanzotto and Marco Pennacchiotti.
2010. Expanding textual entailment corpora from
wikipedia using co-training.
</reference>
<page confidence="0.998932">
523
</page>
</variant>
</algorithm>
<algorithm name="ParsHed" version="110505">
<variant no="0" confidence="0.673312">
<title confidence="0.994036666666667">Spousal Relation Begins with a Deletion of Ends with an Addition of Learning State Changing Verbs from Wikipedia Revision History</title>
<author confidence="0.999773">Derry Tanti</author>
<affiliation confidence="0.996638">Carnegie Mellon</affiliation>
<address confidence="0.9982745">5000 Forbes Pittsburgh, PA, 15213</address>
<email confidence="0.99873">dwijaya@cs.cmu.edu</email>
<author confidence="0.780243">Ndapandula</author>
<affiliation confidence="0.994513">Carnegie Mellon</affiliation>
<address confidence="0.9982055">5000 Forbes Pittsburgh, PA, 15213</address>
<email confidence="0.998558">ndapa@cs.cmu.edu</email>
<author confidence="0.999402">M Tom</author>
<affiliation confidence="0.995951">Carnegie Mellon</affiliation>
<address confidence="0.9982005">5000 Forbes Pittsburgh, PA, 15213</address>
<email confidence="0.998441">tom.mitchell@cs.cmu.edu</email>
<abstract confidence="0.99616937037037">Learning to determine when the timevarying facts of a Knowledge Base (KB) have to be updated is a challenging task. We propose to learn state changing verbs from Wikipedia edit history. When a state-changing event, such as a marriage or death, happens to an entity, the infobox on the entity’s Wikipedia page usually gets updated. At the same time, the article text may be updated with verbs either being added or deleted to reflect the changes made to the infobox. We use Wikipedia edit history to distantly supervise a method for automatically learning verbs and state changes. Additionally, our method uses constraints to effectively map verbs to infobox changes. We observe in our experiments that when state-changing verbs are added or deleted from an entity’s Wikipedia page text, we can predict the entity’s infobox updates with 88% precision and 76% recall. One compelling application of our verbs is to incorporate them as triggers in methods for updating existing KBs, which are currently mostly static.</abstract>
</variant>
</algorithm>
<algorithm name="ParsCit" version="110505">
<citationList>
<citation valid="true">
<authors>
<author>Aoife Cahill</author>
<author>Nitin Madnani</author>
<author>Joel Tetreault</author>
<author>Diane Napolitano</author>
</authors>
<title>Robust systems for preposition error correction using wikipedia revisions. In</title>
<date>2013</date>
<booktitle>In Proceedings of the Conference of the North American Chapter of</booktitle>
<contexts>
<context position="16772" citStr="Cahill et al., 2013" startWordPosition="2811" endWordPosition="2814">labels in (Table 1). In average, we have about 18 verbs per infobox state change in our state changing verb resource that we make available for future research. 4 Related Work Learning from Wikipedia Revision History. Wikipedia edit history has been exploited in a number of problems. A popular task in this regard is that of Wikipedia edit history categorization (Daxenberger and Gurevych, 2013). This task involves characterizing a given edit instance as one of many possible categories such as spelling error correction, paraphrasing, vandalism, and textual entailment (Nelken and Yamangil, 2008; Cahill et al., 2013; Zanzotto and Pennacchiotti, 2010; Recasens et al., 2013). Prior methods target various tasks different from ours. Learning State Changing Verbs. Very few works have studied the problem of learning state changing verbs. (Hosseini et al., 2014) learned state changing verbs in the context of solving arithmetic word problems. They learned the effect of words such as add, subtract on the current state. The VerbOcean resource was automatically generated from the Web (Chklovski and Pantel, 2004). The authors studied the problem of fine-grained semantic relationships between verbs. They learn relati</context>
</contexts>
<marker>Cahill, Madnani, Tetreault, Napolitano, 2013</marker>
<rawString>Aoife Cahill, Nitin Madnani, Joel Tetreault, and Diane Napolitano. 2013. Robust systems for preposition error correction using wikipedia revisions. In In Proceedings of the Conference of the North American Chapter of the Association for Computational Linguistics: Human Language Technologies.</rawString>
</citation>
<citation valid="true">
<authors>
<author>Andrew Carlson</author>
<author>Justin Betteridge</author>
<author>Bryan Kisiel</author>
<author>Burr Settles</author>
<author>Estevam R Hruschka Jr</author>
<author>Tom M Mitchell</author>
</authors>
<title>Toward an architecture for neverending language learning.</title>
<date>2010</date>
<booktitle>In AAAI,</booktitle>
<volume>5</volume>
<pages>3</pages>
<contexts>
<context position="1775" citStr="Carlson et al., 2010" startWordPosition="277" endWordPosition="281">serve in our experiments that when state-changing verbs are added or deleted from an entity’s Wikipedia page text, we can predict the entity’s infobox updates with 88% precision and 76% recall. One compelling application of our verbs is to incorporate them as triggers in methods for updating existing KBs, which are currently mostly static. 1 Introduction Extracting relational facts between entities and storing them in knowledge bases (KBs) has been a topic of active research in recent years. The resulting KBs are generally static and are not updated as the facts change (Suchanek et al., 2007; Carlson et al., 2010; Fader et al., 2011; Mitchell et al., 2015). One possible approach to updating KBs is to extract facts from dynamic Web content such as news (Nakashole and Weikum, 2012). In this paper, we propose to predict state changes caused by verbs acting on entities in text. This is different from simply applying the same text extraction pipeline, that created the original KB, to dynamic Web content. In particular, our approach has the following advantages: (1) Consider for example the SPOUSE relation, both marry and divorce are good patterns for extracting this relation. In our work, we wish to learn </context>
</contexts>
<marker>Carlson, Betteridge, Kisiel, Settles, Jr, Mitchell, 2010</marker>
<rawString>Andrew Carlson, Justin Betteridge, Bryan Kisiel, Burr Settles, Estevam R Hruschka Jr, and Tom M Mitchell. 2010. Toward an architecture for neverending language learning. In AAAI, volume 5, page 3.</rawString>
</citation>
<citation valid="true">
<authors>
<author>Timothy Chklovski</author>
<author>Patrick Pantel</author>
</authors>
<title>Verbocean: Mining the web for fine-grained semantic verb relations.</title>
<date>2004</date>
<booktitle>In Proceedings of EMNLP</booktitle>
<pages>33--40</pages>
<contexts>
<context position="17267" citStr="Chklovski and Pantel, 2004" startWordPosition="2888" endWordPosition="2891">s such as spelling error correction, paraphrasing, vandalism, and textual entailment (Nelken and Yamangil, 2008; Cahill et al., 2013; Zanzotto and Pennacchiotti, 2010; Recasens et al., 2013). Prior methods target various tasks different from ours. Learning State Changing Verbs. Very few works have studied the problem of learning state changing verbs. (Hosseini et al., 2014) learned state changing verbs in the context of solving arithmetic word problems. They learned the effect of words such as add, subtract on the current state. The VerbOcean resource was automatically generated from the Web (Chklovski and Pantel, 2004). The authors studied the problem of fine-grained semantic relationships between verbs. They learn relations such as if someone has bought an item, they may sell it at a later time. This then involves capturing empirical regularities such as “X buys Y” happens before “X sells Y”. Unlike the work we present here, the methods of (Chklovski and Pantel, 2004; Hosseini et al., 2014) do not make a connection to KB relations such as Wikipedia infoboxes. In a vision paper, (Wijaya et al., 2014b) give high level descriptions of a number of possible methods for learning state changing methods. They did </context>
</contexts>
<marker>Chklovski, Pantel, 2004</marker>
<rawString>Timothy Chklovski and Patrick Pantel. 2004. Verbocean: Mining the web for fine-grained semantic verb relations. In Proceedings of EMNLP 2004, pages 33–40.</rawString>
</citation>
<citation valid="true">
<authors>
<author>Johannes Daxenberger</author>
<author>Iryna Gurevych</author>
</authors>
<title>Automatically classifying edit categories in wikipedia revisions.</title>
<date>2013</date>
<booktitle>In EMNLP,</booktitle>
<pages>578--589</pages>
<contexts>
<context position="16549" citStr="Daxenberger and Gurevych, 2013" startWordPosition="2779" endWordPosition="2782">ing constraints that begin-spouse is mutex with end-spouse, these verbs (whose base form is “marry”) are filtered out from the features of end-spouse. We show some of the learned verb features (after feature selection) for some of the labels in (Table 1). In average, we have about 18 verbs per infobox state change in our state changing verb resource that we make available for future research. 4 Related Work Learning from Wikipedia Revision History. Wikipedia edit history has been exploited in a number of problems. A popular task in this regard is that of Wikipedia edit history categorization (Daxenberger and Gurevych, 2013). This task involves characterizing a given edit instance as one of many possible categories such as spelling error correction, paraphrasing, vandalism, and textual entailment (Nelken and Yamangil, 2008; Cahill et al., 2013; Zanzotto and Pennacchiotti, 2010; Recasens et al., 2013). Prior methods target various tasks different from ours. Learning State Changing Verbs. Very few works have studied the problem of learning state changing verbs. (Hosseini et al., 2014) learned state changing verbs in the context of solving arithmetic word problems. They learned the effect of words such as add, subtr</context>
</contexts>
<marker>Daxenberger, Gurevych, 2013</marker>
<rawString>Johannes Daxenberger and Iryna Gurevych. 2013. Automatically classifying edit categories in wikipedia revisions. In EMNLP, pages 578–589.</rawString>
</citation>
<citation valid="true">
<authors>
<author>Anthony Fader</author>
<author>Stephen Soderland</author>
<author>Oren Etzioni</author>
</authors>
<title>Identifying relations for open information extraction.</title>
<date>2011</date>
<booktitle>In Proceedings of the Conference on Empirical Methods in Natural Language Processing,</booktitle>
<pages>1535--1545</pages>
<publisher>Association</publisher>
<institution>for Computational Linguistics.</institution>
<contexts>
<context position="1795" citStr="Fader et al., 2011" startWordPosition="282" endWordPosition="285">ts that when state-changing verbs are added or deleted from an entity’s Wikipedia page text, we can predict the entity’s infobox updates with 88% precision and 76% recall. One compelling application of our verbs is to incorporate them as triggers in methods for updating existing KBs, which are currently mostly static. 1 Introduction Extracting relational facts between entities and storing them in knowledge bases (KBs) has been a topic of active research in recent years. The resulting KBs are generally static and are not updated as the facts change (Suchanek et al., 2007; Carlson et al., 2010; Fader et al., 2011; Mitchell et al., 2015). One possible approach to updating KBs is to extract facts from dynamic Web content such as news (Nakashole and Weikum, 2012). In this paper, we propose to predict state changes caused by verbs acting on entities in text. This is different from simply applying the same text extraction pipeline, that created the original KB, to dynamic Web content. In particular, our approach has the following advantages: (1) Consider for example the SPOUSE relation, both marry and divorce are good patterns for extracting this relation. In our work, we wish to learn that they cause diff</context>
</contexts>
<marker>Fader, Soderland, Etzioni, 2011</marker>
<rawString>Anthony Fader, Stephen Soderland, and Oren Etzioni. 2011. Identifying relations for open information extraction. In Proceedings of the Conference on Empirical Methods in Natural Language Processing, pages 1535–1545. Association for Computational Linguistics.</rawString>
</citation>
<citation valid="true">
<authors>
<author>Mohammad Javad Hosseini</author>
<author>Hannaneh Hajishirzi</author>
<author>Oren Etzioni</author>
<author>Nate Kushman</author>
</authors>
<title>Learning to solve arithmetic word problems with verb categorization.</title>
<date>2014</date>
<booktitle>In EMNLP,</booktitle>
<pages>523--533</pages>
<publisher>ACL.</publisher>
<contexts>
<context position="17016" citStr="Hosseini et al., 2014" startWordPosition="2848" endWordPosition="2851">has been exploited in a number of problems. A popular task in this regard is that of Wikipedia edit history categorization (Daxenberger and Gurevych, 2013). This task involves characterizing a given edit instance as one of many possible categories such as spelling error correction, paraphrasing, vandalism, and textual entailment (Nelken and Yamangil, 2008; Cahill et al., 2013; Zanzotto and Pennacchiotti, 2010; Recasens et al., 2013). Prior methods target various tasks different from ours. Learning State Changing Verbs. Very few works have studied the problem of learning state changing verbs. (Hosseini et al., 2014) learned state changing verbs in the context of solving arithmetic word problems. They learned the effect of words such as add, subtract on the current state. The VerbOcean resource was automatically generated from the Web (Chklovski and Pantel, 2004). The authors studied the problem of fine-grained semantic relationships between verbs. They learn relations such as if someone has bought an item, they may sell it at a later time. This then involves capturing empirical regularities such as “X buys Y” happens before “X sells Y”. Unlike the work we present here, the methods of (Chklovski and Pante</context>
</contexts>
<marker>Hosseini, Hajishirzi, Etzioni, Kushman, 2014</marker>
<rawString>Mohammad Javad Hosseini, Hannaneh Hajishirzi, Oren Etzioni, and Nate Kushman. 2014. Learning to solve arithmetic word problems with verb categorization. In EMNLP, pages 523–533. ACL.</rawString>
</citation>
<citation valid="true">
<authors>
<author>Christopher D Manning</author>
<author>Mihai Surdeanu</author>
<author>John Bauer</author>
<author>Jenny Finkel</author>
<author>Steven J Bethard</author>
<author>David McClosky</author>
</authors>
<title>The Stanford CoreNLP natural language processing toolkit.</title>
<date>2014</date>
<booktitle>In Proceedings of 52nd Annual Meeting of the Association for Computational Linguistics: System Demonstrations,</booktitle>
<pages>55--60</pages>
<contexts>
<context position="8641" citStr="Manning et al., 2014" startWordPosition="1456" endWordPosition="1459">he dataset that we release can be seen in Figure 2. We show only labels that we evaluate in our task. For our task of learning state changing verbs from this revision history dataset, for each labeled dp,t, we extract as features, verbs (or verbs+prepositions) v E OCp,t of which its subject (or object) matches the Wikipedia entity p and its object (or subject resp.) matches an infobox value, start or end time: (vsubject, vobject) = (arg1, arg2) or (vsubject, vobject) = (arg2, arg1), where arg1= p and (Satt,arg2, *, *) or (Satt, *,arg2, *) or (Satt, *, *,arg2) E OSp,t. We use Stanford CoreNLP (Manning et al., 2014) to dependency parse sentences and extract the subjects and objects of verbs. We find that 27,044 out of the 41,139 labeled documents contain verb edits, but only 4,735 contain verb edits with two arguments, where one argument matches the entity and another matches the value of the infobox change. We use the latter for our task, to improve the chance that the verb edits used as features are related to the infobox change. 2.2 Model We use a Maximum Entropy (MAXENT) classifier3 given a set of training data = {(vd,, y)} where vd,, = (v1, v2, ... v|V |) E R|V |is the JVJdimensional representation </context>
</contexts>
<marker>Manning, Surdeanu, Bauer, Finkel, Bethard, McClosky, 2014</marker>
<rawString>Christopher D. Manning, Mihai Surdeanu, John Bauer, Jenny Finkel, Steven J. Bethard, and David McClosky. 2014. The Stanford CoreNLP natural language processing toolkit. In Proceedings of 52nd Annual Meeting of the Association for Computational Linguistics: System Demonstrations, pages 55–60.</rawString>
</citation>
<citation valid="false">
<authors>
<author>Tom M Mitchell</author>
<author>William W Cohen</author>
<author>Estevam R Hruschka Jr</author>
<author>Partha Pratim Talukdar</author>
<author>Justin Betteridge</author>
<author>Andrew Carlson</author>
<author>Bhavana Dalvi Mishra</author>
<author>Matthew Gardner</author>
<author>Bryan Kisiel</author>
<author>Jayant Krishnamurthy</author>
<author>Ni Lao</author>
</authors>
<title>Kathryn Mazaitis, Thahir Mohamed, Ndapandula Nakashole, Emmanouil Antonios Platanios,</title>
<date>2015</date>
<booktitle>In Proceedings of the Twenty-Ninth AAAI Conference on Artificial Intelligence,</booktitle>
<pages>2302--2310</pages>
<institution>Derry Tanti Wijaya, Abhinav Gupta, Xinlei Chen, Abulhair</institution>
<location>Alan Ritter, Mehdi</location>
<contexts>
<context position="1819" citStr="Mitchell et al., 2015" startWordPosition="286" endWordPosition="289">hanging verbs are added or deleted from an entity’s Wikipedia page text, we can predict the entity’s infobox updates with 88% precision and 76% recall. One compelling application of our verbs is to incorporate them as triggers in methods for updating existing KBs, which are currently mostly static. 1 Introduction Extracting relational facts between entities and storing them in knowledge bases (KBs) has been a topic of active research in recent years. The resulting KBs are generally static and are not updated as the facts change (Suchanek et al., 2007; Carlson et al., 2010; Fader et al., 2011; Mitchell et al., 2015). One possible approach to updating KBs is to extract facts from dynamic Web content such as news (Nakashole and Weikum, 2012). In this paper, we propose to predict state changes caused by verbs acting on entities in text. This is different from simply applying the same text extraction pipeline, that created the original KB, to dynamic Web content. In particular, our approach has the following advantages: (1) Consider for example the SPOUSE relation, both marry and divorce are good patterns for extracting this relation. In our work, we wish to learn that they cause different state changes. Thu</context>
</contexts>
<marker>Mitchell, Cohen, Jr, Talukdar, Betteridge, Carlson, Mishra, Gardner, Kisiel, Krishnamurthy, Lao, 2015</marker>
<rawString>Tom M. Mitchell, William W. Cohen, Estevam R. Hruschka Jr., Partha Pratim Talukdar, Justin Betteridge, Andrew Carlson, Bhavana Dalvi Mishra, Matthew Gardner, Bryan Kisiel, Jayant Krishnamurthy, Ni Lao, Kathryn Mazaitis, Thahir Mohamed, Ndapandula Nakashole, Emmanouil Antonios Platanios, Alan Ritter, Mehdi Samadi, Burr Settles, Richard C. Wang, Derry Tanti Wijaya, Abhinav Gupta, Xinlei Chen, Abulhair Saparov, Malcolm Greaves, and Joel Welling. 2015. Never-ending learning. In Proceedings of the Twenty-Ninth AAAI Conference on Artificial Intelligence, January 25-30, 2015, Austin, Texas, USA., pages 2302–2310.</rawString>
</citation>
<citation valid="true">
<authors>
<author>Ndapandula Nakashole</author>
<author>Gerhard Weikum</author>
</authors>
<title>Real-time population of knowledge bases: opportunities and challenges.</title>
<date>2012</date>
<booktitle>In Proceedings of the Joint Workshop on Automatic Knowledge Base Construction and Web-scale Knowledge Extraction,</booktitle>
<pages>41--45</pages>
<publisher>Association</publisher>
<institution>for Computational Linguistics.</institution>
<contexts>
<context position="1945" citStr="Nakashole and Weikum, 2012" startWordPosition="307" endWordPosition="310"> 88% precision and 76% recall. One compelling application of our verbs is to incorporate them as triggers in methods for updating existing KBs, which are currently mostly static. 1 Introduction Extracting relational facts between entities and storing them in knowledge bases (KBs) has been a topic of active research in recent years. The resulting KBs are generally static and are not updated as the facts change (Suchanek et al., 2007; Carlson et al., 2010; Fader et al., 2011; Mitchell et al., 2015). One possible approach to updating KBs is to extract facts from dynamic Web content such as news (Nakashole and Weikum, 2012). In this paper, we propose to predict state changes caused by verbs acting on entities in text. This is different from simply applying the same text extraction pipeline, that created the original KB, to dynamic Web content. In particular, our approach has the following advantages: (1) Consider for example the SPOUSE relation, both marry and divorce are good patterns for extracting this relation. In our work, we wish to learn that they cause different state changes. Thus, we can update the entity’s fact and its temporal scope (Wijaya et al., 2014a). (2) Learning state changing verbs can pave t</context>
</contexts>
<marker>Nakashole, Weikum, 2012</marker>
<rawString>Ndapandula Nakashole and Gerhard Weikum. 2012. Real-time population of knowledge bases: opportunities and challenges. In Proceedings of the Joint Workshop on Automatic Knowledge Base Construction and Web-scale Knowledge Extraction, pages 41–45. Association for Computational Linguistics.</rawString>
</citation>
<citation valid="true">
<authors>
<author>Rani Nelken</author>
<author>Elif Yamangil</author>
</authors>
<title>Mining wikipedias article revision history for training computational linguistics algorithms.</title>
<date>2008</date>
<contexts>
<context position="16751" citStr="Nelken and Yamangil, 2008" startWordPosition="2807" endWordPosition="2810">selection) for some of the labels in (Table 1). In average, we have about 18 verbs per infobox state change in our state changing verb resource that we make available for future research. 4 Related Work Learning from Wikipedia Revision History. Wikipedia edit history has been exploited in a number of problems. A popular task in this regard is that of Wikipedia edit history categorization (Daxenberger and Gurevych, 2013). This task involves characterizing a given edit instance as one of many possible categories such as spelling error correction, paraphrasing, vandalism, and textual entailment (Nelken and Yamangil, 2008; Cahill et al., 2013; Zanzotto and Pennacchiotti, 2010; Recasens et al., 2013). Prior methods target various tasks different from ours. Learning State Changing Verbs. Very few works have studied the problem of learning state changing verbs. (Hosseini et al., 2014) learned state changing verbs in the context of solving arithmetic word problems. They learned the effect of words such as add, subtract on the current state. The VerbOcean resource was automatically generated from the Web (Chklovski and Pantel, 2004). The authors studied the problem of fine-grained semantic relationships between ver</context>
</contexts>
<marker>Nelken, Yamangil, 2008</marker>
<rawString>Rani Nelken and Elif Yamangil. 2008. Mining wikipedias article revision history for training computational linguistics algorithms.</rawString>
</citation>
<citation valid="true">
<authors>
<author>Marta Recasens</author>
<author>Cristian Danescu-Niculescu-Mizil</author>
<author>Dan Jurafsky</author>
</authors>
<title>Linguistic models for analyzing and detecting biased language.</title>
<date>2013</date>
<journal>In ACL</journal>
<volume>1</volume>
<pages>1650--1659</pages>
<contexts>
<context position="16830" citStr="Recasens et al., 2013" startWordPosition="2819" endWordPosition="2823"> per infobox state change in our state changing verb resource that we make available for future research. 4 Related Work Learning from Wikipedia Revision History. Wikipedia edit history has been exploited in a number of problems. A popular task in this regard is that of Wikipedia edit history categorization (Daxenberger and Gurevych, 2013). This task involves characterizing a given edit instance as one of many possible categories such as spelling error correction, paraphrasing, vandalism, and textual entailment (Nelken and Yamangil, 2008; Cahill et al., 2013; Zanzotto and Pennacchiotti, 2010; Recasens et al., 2013). Prior methods target various tasks different from ours. Learning State Changing Verbs. Very few works have studied the problem of learning state changing verbs. (Hosseini et al., 2014) learned state changing verbs in the context of solving arithmetic word problems. They learned the effect of words such as add, subtract on the current state. The VerbOcean resource was automatically generated from the Web (Chklovski and Pantel, 2004). The authors studied the problem of fine-grained semantic relationships between verbs. They learn relations such as if someone has bought an item, they may sell i</context>
</contexts>
<marker>Recasens, Danescu-Niculescu-Mizil, Jurafsky, 2013</marker>
<rawString>Marta Recasens, Cristian Danescu-Niculescu-Mizil, and Dan Jurafsky. 2013. Linguistic models for analyzing and detecting biased language. In ACL (1), pages 1650–1659.</rawString>
</citation>
<citation valid="true">
<authors>
<author>Fabian M Suchanek</author>
<author>Gjergji Kasneci</author>
<author>Gerhard Weikum</author>
</authors>
<title>Yago: a core of semantic knowledge.</title>
<date>2007</date>
<booktitle>In Proceedings of the 16th international conference on World Wide Web,</booktitle>
<pages>697--706</pages>
<publisher>ACM.</publisher>
<contexts>
<context position="1753" citStr="Suchanek et al., 2007" startWordPosition="273" endWordPosition="276"> infobox changes. We observe in our experiments that when state-changing verbs are added or deleted from an entity’s Wikipedia page text, we can predict the entity’s infobox updates with 88% precision and 76% recall. One compelling application of our verbs is to incorporate them as triggers in methods for updating existing KBs, which are currently mostly static. 1 Introduction Extracting relational facts between entities and storing them in knowledge bases (KBs) has been a topic of active research in recent years. The resulting KBs are generally static and are not updated as the facts change (Suchanek et al., 2007; Carlson et al., 2010; Fader et al., 2011; Mitchell et al., 2015). One possible approach to updating KBs is to extract facts from dynamic Web content such as news (Nakashole and Weikum, 2012). In this paper, we propose to predict state changes caused by verbs acting on entities in text. This is different from simply applying the same text extraction pipeline, that created the original KB, to dynamic Web content. In particular, our approach has the following advantages: (1) Consider for example the SPOUSE relation, both marry and divorce are good patterns for extracting this relation. In our w</context>
<context position="5083" citStr="Suchanek et al., 2007" startWordPosition="829" endWordPosition="832"> an algorithm that uses Wikipedia edit histories as distantly labeled data to learn which verbs result in which state changes to entities, and experimentally demonstrate its success, (2) we make available this set of distantly labeled training data on our website1, and (3) we also make available our learned mappings from verbs to state changes, as a resource for other researchers, on the same website. 2 Method 2.1 Data Construction We construct a dataset from Wikipedia edit histories of person entities whose facts change between the year 2007 and 2012 (i.e., have at least one fact in YAGO KB (Suchanek et al., 2007) with a start or end time in this period). We obtain Wikipedia URLs of this set of entities P from YAGO and crawl their article’s revision history. Given a person p, his/her Wikipedia revision history Rp has a set of ordered dates Tp on which revisions are made to his/her Wikipedia page (we consider date granularity). Each revision rp,t E Rp is his/her Wikipedia page at date t where t E Tp. Each Wikipedia revision rp,t is a set of infobox slots Sp,t and textual content Cp,t. Each infobox slot s E Sp,t is a quadruple, (satt, svalue, sstart, send) containing the attribute name (non-empty), the a</context>
</contexts>
<marker>Suchanek, Kasneci, Weikum, 2007</marker>
<rawString>Fabian M Suchanek, Gjergji Kasneci, and Gerhard Weikum. 2007. Yago: a core of semantic knowledge. In Proceedings of the 16th international conference on World Wide Web, pages 697–706. ACM.</rawString>
</citation>
<citation valid="true">
<authors>
<author>Derry Wijaya</author>
<author>Ndapa Nakashole</author>
<author>Tom Mitchell</author>
</authors>
<title>CTPs: Contextual temporal profiles for time scoping facts via entity state change detection.</title>
<date>2014</date>
<booktitle>In Proceedings of the Conference on Empirical Methods in Natural Language Processing. Association for Computational Linguistics.</booktitle>
<contexts>
<context position="2497" citStr="Wijaya et al., 2014" startWordPosition="401" endWordPosition="404"> from dynamic Web content such as news (Nakashole and Weikum, 2012). In this paper, we propose to predict state changes caused by verbs acting on entities in text. This is different from simply applying the same text extraction pipeline, that created the original KB, to dynamic Web content. In particular, our approach has the following advantages: (1) Consider for example the SPOUSE relation, both marry and divorce are good patterns for extracting this relation. In our work, we wish to learn that they cause different state changes. Thus, we can update the entity’s fact and its temporal scope (Wijaya et al., 2014a). (2) Learning state changing verbs can pave the way for learning the ordering of verbs in terms of their pre- and post-conditions. Our approach learns state changing verbs from Wikipedia revision history. In particular, we seek to establish a correspondence between infobox edits and verbs edits in the same article. The infobox of a Wikipedia article is a structured box that summarizes an entity as a set of facts (attribute-value pairs) . Our assumption is that when a statechanging event happens to an entity e.g., a marriage, its Wikipedia infobox is updated by adding a new SPOUSE value. At </context>
<context position="17757" citStr="Wijaya et al., 2014" startWordPosition="2972" endWordPosition="2975"> add, subtract on the current state. The VerbOcean resource was automatically generated from the Web (Chklovski and Pantel, 2004). The authors studied the problem of fine-grained semantic relationships between verbs. They learn relations such as if someone has bought an item, they may sell it at a later time. This then involves capturing empirical regularities such as “X buys Y” happens before “X sells Y”. Unlike the work we present here, the methods of (Chklovski and Pantel, 2004; Hosseini et al., 2014) do not make a connection to KB relations such as Wikipedia infoboxes. In a vision paper, (Wijaya et al., 2014b) give high level descriptions of a number of possible methods for learning state changing methods. They did not implement any of them. 5 Conclusion In this paper we presented a method that learns state changing verb phrases from Wikipedia revision history. We first constructed and curated a novel dataset from Wikipedia revision history that is tailored to our task. We showed that this dataset is useful for learning verb phrase features that are effective for predicting state changes in the knowledge base (KB), where we considered the KB to be infoboxes and their values. As future work we wis</context>
</contexts>
<marker>Wijaya, Nakashole, Mitchell, 2014</marker>
<rawString>Derry Wijaya, Ndapa Nakashole, and Tom Mitchell. 2014a. CTPs: Contextual temporal profiles for time scoping facts via entity state change detection. In Proceedings of the Conference on Empirical Methods in Natural Language Processing. Association for Computational Linguistics.</rawString>
</citation>
<citation valid="true">
<authors>
<author>Derry Tanti Wijaya</author>
<author>Ndapandula Nakashole</author>
<author>Tom M Mitchell</author>
</authors>
<title>Mining and organizing a resource of state-changing verbs.</title>
<date>2014</date>
<booktitle>In Proceedings of the Joint Workshop on Automatic Knowledge Base Construction and Web-scale Knowledge Extraction.</booktitle>
<contexts>
<context position="2497" citStr="Wijaya et al., 2014" startWordPosition="401" endWordPosition="404"> from dynamic Web content such as news (Nakashole and Weikum, 2012). In this paper, we propose to predict state changes caused by verbs acting on entities in text. This is different from simply applying the same text extraction pipeline, that created the original KB, to dynamic Web content. In particular, our approach has the following advantages: (1) Consider for example the SPOUSE relation, both marry and divorce are good patterns for extracting this relation. In our work, we wish to learn that they cause different state changes. Thus, we can update the entity’s fact and its temporal scope (Wijaya et al., 2014a). (2) Learning state changing verbs can pave the way for learning the ordering of verbs in terms of their pre- and post-conditions. Our approach learns state changing verbs from Wikipedia revision history. In particular, we seek to establish a correspondence between infobox edits and verbs edits in the same article. The infobox of a Wikipedia article is a structured box that summarizes an entity as a set of facts (attribute-value pairs) . Our assumption is that when a statechanging event happens to an entity e.g., a marriage, its Wikipedia infobox is updated by adding a new SPOUSE value. At </context>
<context position="17757" citStr="Wijaya et al., 2014" startWordPosition="2972" endWordPosition="2975"> add, subtract on the current state. The VerbOcean resource was automatically generated from the Web (Chklovski and Pantel, 2004). The authors studied the problem of fine-grained semantic relationships between verbs. They learn relations such as if someone has bought an item, they may sell it at a later time. This then involves capturing empirical regularities such as “X buys Y” happens before “X sells Y”. Unlike the work we present here, the methods of (Chklovski and Pantel, 2004; Hosseini et al., 2014) do not make a connection to KB relations such as Wikipedia infoboxes. In a vision paper, (Wijaya et al., 2014b) give high level descriptions of a number of possible methods for learning state changing methods. They did not implement any of them. 5 Conclusion In this paper we presented a method that learns state changing verb phrases from Wikipedia revision history. We first constructed and curated a novel dataset from Wikipedia revision history that is tailored to our task. We showed that this dataset is useful for learning verb phrase features that are effective for predicting state changes in the knowledge base (KB), where we considered the KB to be infoboxes and their values. As future work we wis</context>
</contexts>
<marker>Wijaya, Nakashole, Mitchell, 2014</marker>
<rawString>Derry Tanti Wijaya, Ndapandula Nakashole, and Tom M Mitchell. 2014b. Mining and organizing a resource of state-changing verbs. In Proceedings of the Joint Workshop on Automatic Knowledge Base Construction and Web-scale Knowledge Extraction.</rawString>
</citation>
<citation valid="true">
<authors>
<author>Fabio Massimo Zanzotto</author>
<author>Marco Pennacchiotti</author>
</authors>
<title>Expanding textual entailment corpora from wikipedia using co-training.</title>
<date>2010</date>
<contexts>
<context position="16806" citStr="Zanzotto and Pennacchiotti, 2010" startWordPosition="2815" endWordPosition="2818">In average, we have about 18 verbs per infobox state change in our state changing verb resource that we make available for future research. 4 Related Work Learning from Wikipedia Revision History. Wikipedia edit history has been exploited in a number of problems. A popular task in this regard is that of Wikipedia edit history categorization (Daxenberger and Gurevych, 2013). This task involves characterizing a given edit instance as one of many possible categories such as spelling error correction, paraphrasing, vandalism, and textual entailment (Nelken and Yamangil, 2008; Cahill et al., 2013; Zanzotto and Pennacchiotti, 2010; Recasens et al., 2013). Prior methods target various tasks different from ours. Learning State Changing Verbs. Very few works have studied the problem of learning state changing verbs. (Hosseini et al., 2014) learned state changing verbs in the context of solving arithmetic word problems. They learned the effect of words such as add, subtract on the current state. The VerbOcean resource was automatically generated from the Web (Chklovski and Pantel, 2004). The authors studied the problem of fine-grained semantic relationships between verbs. They learn relations such as if someone has bought </context>
</contexts>
<marker>Zanzotto, Pennacchiotti, 2010</marker>
<rawString>Fabio Massimo Zanzotto and Marco Pennacchiotti. 2010. Expanding textual entailment corpora from wikipedia using co-training.</rawString>
</citation>
</citationList>
</algorithm>
</algorithms>
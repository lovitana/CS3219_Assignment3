<?xml version="1.0" encoding="UTF-8"?>
<algorithms version="110505">
<algorithm name="SectLabel" version="110505">
<variant no="0" confidence="0.001193">
<title confidence="0.865817">
Trans-gram, Fast Cross-lingual Word-embeddings
</title>
<note confidence="0.4939085">
reyes − Mannde = reginait − femmefr
Jocelyn Coulmance Jean-Marc Marty ∗ Guillaume Wenzek ∗ Amine Benhalloum
105 rue La Fayette 105 rue La Fayette 105 rue La Fayette 105 rue La Fayette
75010 Paris 75010 Paris 75010 Paris 75010 Paris
</note>
<email confidence="0.987671">
joc@proxem.com jmm@proxem.com guw@proxem.com aba@proxem.com
</email>
<sectionHeader confidence="0.994544" genericHeader="abstract">
Abstract
</sectionHeader>
<bodyText confidence="0.9998729375">
We introduce Trans-gram, a simple
and computationally-efficient method to
simultaneously learn and align word-
embeddings for a variety of languages, us-
ing only monolingual data and a smaller
set of sentence-aligned data. We use our
new method to compute aligned word-
embeddings for twenty-one languages us-
ing English as a pivot language. We show
that some linguistic features are aligned
across languages for which we do not have
aligned data, even though those properties
do not exist in the pivot language. We also
achieve state of the art results on standard
cross-lingual text classification and word
translation tasks.
</bodyText>
<sectionHeader confidence="0.998782" genericHeader="introduction">
1 Introduction
</sectionHeader>
<bodyText confidence="0.987146088235294">
Word-embeddings are a representation of words
with fixed-sized vectors. It is a distributed rep-
resentation (Hinton, 1984) in the sense that there
is not necessarily a one-to-one correspondence be-
tween vector dimensions and linguistic properties.
The linguistic properties are distributed along the
dimensions of the space.
A popular method to compute word-
embeddings is the Skip-gram model (Mikolov et
al., 2013a). This algorithm learns high-quality
word vectors with a computation cost much lower
than previous methods. This allows the processing
of very important amounts of data. For instance, a
1.6 billion words dataset can be processed in less
than one day.
Several authors came up with different methods
to align word-embeddings across two languages
(Klementiev et al., 2012; Mikolov et al., 2013b;
Lauly et al., 2014; Gouws et al., 2015).
∗These authors contributed equally.
In this article, we introduce a new method
called Trans-gram, which learns word embed-
dings aligned across many languages, in a simple
and efficient fashion, using only sentence align-
ments rather than word alignments. We compare
our method with previous approaches on a cross-
lingual document classification task and on a word
translation task and obtain state of the art results
on these tasks. Additionally, word-embeddings for
twenty-one languages are learned simultaneously
- to our knowledge - for the first time, in less than
two and a half hours. Furthermore, we illustrate
some interesting properties that are captured such
as cross-lingual analogies, e.g reyes −
</bodyText>
<equation confidence="0.700173">
� �
</equation>
<bodyText confidence="0.8842075">
femmefr ≈ reginait which can be used for dis-
ambiguation.
</bodyText>
<sectionHeader confidence="0.700148" genericHeader="method">
2 Review of Previous Work
</sectionHeader>
<bodyText confidence="0.999760130434783">
A number of methods have been explored to
train and align bilingual word-embeddings. These
methods pursue two objectives: first, similar rep-
resentations (i.e. spatially close) must be assigned
to similar words (i.e. “semantically close”) within
each language - this is the mono-lingual objec-
tive; second, similar representations must be as-
signed to similar words across languages - this is
the cross-lingual objective.
The simplest approach consists in separating the
mono-lingual optimization task from the cross-
lingual optimization task. This is for example the
case in (Mikolov et al., 2013b). The idea is to sep-
arately train two sets of word-embeddings for each
language and then to do a parametric estimation
of the mapping between word-embeddings across
languages. This method was further extended by
(Faruqui and Dyer, 2014). Even though those al-
gorithms proved to be viable and fast, it is not
clear whether or not a simple mapping between
whole languages exists. Moreover, they require
word alignments which are a rare and expensive
resource.
</bodyText>
<equation confidence="0.744353">
Mannde +
</equation>
<page confidence="0.934133">
1109
</page>
<note confidence="0.6414355">
Proceedings of the 2015 Conference on Empirical Methods in Natural Language Processing, pages 1109–1113,
Lisbon, Portugal, 17-21 September 2015. c�2015 Association for Computational Linguistics.
</note>
<bodyText confidence="0.99993455">
Another approach consists in focusing entirely
on the cross-lingual objective. This was explored
in (Hermann and Blunsom, 2013; Lauly et al.,
2014) where every couple of aligned sentences is
transformed into two fixed-size vectors. Then, the
model minimizes the Euclidean distance between
both vectors. This idea allows processing corpus
aligned at sentence-level rather than word-level.
However, it does not leverage the abundance of ex-
isting mono-lingual corpora.
A popular approach is to jointly optimize the
mono-lingual and cross-lingual objectives simul-
taneously. This is mostly done by minimizing the
sum of mono-lingual loss functions for each lan-
guage and the cross-lingual loss function. (Kle-
mentiev et al., 2012) proved this approach to be
useful by obtaining state-of-the-art results on sev-
eral tasks. (Gouws et al., 2015) extends their work
with a more computationally-efficient implemen-
tation.
</bodyText>
<sectionHeader confidence="0.947621" genericHeader="method">
3 From Skip-Gram to Trans-Gram
</sectionHeader>
<subsectionHeader confidence="0.99965">
3.1 Skip-gram
</subsectionHeader>
<bodyText confidence="0.9994235">
We briefly introduce the Skip-gram algorithm, as
we will need it for further explanations. Skip-
gram allows to train word embeddings for a lan-
guage using mono-lingual data. This method uses
a dual representation for words. Each word w
has two embeddings: a target vector, w~ (E RD),
and a context vector,w (E RD). The algorithm
tries to estimate the probability of a word w to
appear in the context of a word c. More pre-
cisely we are learning the embeddings ~w,c so that:
σ(~w c) = P(w|c) where σ is the sigmoid func-
tion.
A simplified version of the loss function mini-
mized by Skip-gram is the following:
</bodyText>
<equation confidence="0.972648">
J = X X X − log σ(~w c) (1)
sEC wEs cEs[w−l:w+l]
</equation>
<bodyText confidence="0.994884363636363">
where C is the set of sentences constituting the
training corpus, and s[w −l : w +l] is a word win-
dow on the sentence s centered around w. For the
sake of simplicity this equation does not include
the “negative-sampling” term, see (Mikolov et al.,
2013a) for more details.
Skip-gram can be seen as a materialization
of the distributional hypothesis (Harris, 1968):
“Words used in similar contexts have similar
meanings”. We will now see how to extend this
idea to cross-lingual contexts.
</bodyText>
<subsectionHeader confidence="0.999676">
3.2 Trans-gram
</subsectionHeader>
<bodyText confidence="0.999962214285714">
In this section we introduce Trans-gram, a new
method to compute aligned word-embeddings for
a variety of languages.
Our method will minimize the summation of
mono-lingual losses and cross-lingual losses. Like
in BilBOWA (Gouws et al., 2015), we use Skip-
gram as a mono-lingual loss. Assuming we are
trying to learn aligned word vectors for languages
e (e.g. English) and f (e.g. French), we note Je
and Jf the two mono-lingual losses.
In BilBOWA, the cross-lingual loss function
is a distance between bag-of-words representa-
tions of two aligned sentences. But as (Levy and
Goldberg, 2014) showed that the Skip-gram loss
function extracts interesting linguistic features, we
wanted to use a loss function for the cross-lingual
objective that will be closer to Skip-gram than Bil-
BOWA.
Therefore, we introduce a new task, Trans-
gram, similar to Skip-gram. Each English sen-
tence se in our aligned corpus Ae,f is aligned with
a French sentence sf. In Skip-gram, the context
picked for a target word we in a sentence se is the
set of words ce appearing in the window centered
around we: se[we − l : we + l]. In Trans-gram, the
context picked for a target word we in a sentence
se will be all the words cf appearing in sf. The
loss can thus be written as:
</bodyText>
<equation confidence="0.835512">
XΩe,f =
(se,sf)EAe,f
(2)
</equation>
<bodyText confidence="0.999350666666666">
This loss isn’t symmetric with respect to the lan-
guages. We, therefore, use two cross-lingual ob-
jectives: Ωe,f aligning e’s target vectors and f’s
context vectors and Ωf,e aligning f’s target vectors
and e’s context vectors. By comparison BilBOWA
only aligns e’s target vectors and f’s target vec-
tors. The figure 1 illustrates the four objectives.
Notice that we make the assumption that the
meaning of a word is uniformly distributed in
the whole sentence. This assumption, although
a naive one, gave us in practice excellent results.
Also our method uses only sentence-aligned cor-
pus and not word-aligned corpus which are rarer.
To add a third language i (e.g. Italian), we just
have to add 3 new objectives (Ji, Ωe,i and Ωi,e)
to the global loss. If available we could also add
Ωf,i or Ωi,f but in our case we only used corpora
aligned with English.
</bodyText>
<figure confidence="0.948283533333333">
−log σ( ~we cf)
X
cfEsf
X
weEse
1110
~sitse
Je
~assisf
Qf,e
Qe,f
~sitse
~assisf
Jf
thecat onthee thecatsitsonthemate lechatestassissurletapisf chatest surlef
</figure>
<figureCaption confidence="0.9918765">
Figure 1: The four partial objectives contributing to the alignment of English and French: a Skip-gram
objective per language (Je and Jf) over a window surrounding a target word (blue) and two Trans-gram
objectives (ne,f and nf,e) over the whole sentence aligned with the sentence from which the target word
is extracted (red).
</figureCaption>
<sectionHeader confidence="0.997617" genericHeader="method">
4 Implementation
</sectionHeader>
<bodyText confidence="0.9999895">
In our experiments, we used the Europarl (Koehn,
2005) aligned corpora. Europarl-v7 has two pe-
culiarities: firstly, the corpora are aligned at
sentence-level; secondly each pair of languages
contains English as one of its members: for in-
stance, there is no French/Italian pair. In other
words, English is used as a pivot language. No
bi-lingual lexicons nor other bi-lingual datasets
aligned at the word level were used.
Using only the Europarl-v7 texts as both mono-
lingual and bilingual data, it took 10 minutes to
align 2 languages, and two and a half hours to
align the 21 languages of the corpus, in a 40
dimensional space on a 6 core computer. We
also computed 300 dimensions vectors using the
Wikipedia extracts provided by (Al-Rfou et al.,
2013) as monolingual data for each language. The
training time was 21 hours.
</bodyText>
<sectionHeader confidence="0.999788" genericHeader="method">
5 Experiments
</sectionHeader>
<subsectionHeader confidence="0.945866">
5.1 Reuters Cross-lingual Document
Classification
</subsectionHeader>
<bodyText confidence="0.99996">
We used a subset of the English and German sec-
tions of the Reuters RCV1/RCV2 corpora (Lewis
and Li, 2004) (10000 documents each), as in (Kle-
mentiev et al., 2012), and we replicated the exper-
imental setting. In the English dataset, there are
four topics: CCAT (Corporate/Industrial), ECAT
(Economics), GCAT (Government/Social), and
MCAT (Markets). We used these topics as our la-
bels and we only selected documents labeled with
a single topic. We trained our classifier on the ar-
ticles of one language, where each document was
represented using an IDF weighted sum of the vec-
tors of its words, we then tested it on the articles
of the other language. The classifier used was an
averaged perceptron, and we used the implemen-
tation from (Klementiev et al., 2012)1. The word
vectors were computed on the Europarl-v7 paral-
lel corpus with size 40 like other methods. For this
task only the target vectors where used.
We report the percentage precision obtained
with our method, in comparison with other meth-
ods, in Table 1. The table also include results
obtained with 300 dimensions vectors trained by
Trans-gram with the Europarl-v7 as parallel cor-
pus and the Wikipedia as mono-lingual corpus.
The previous state of the art results were detained
(Gouws et al., 2015) with BilBOWA and (Lauly
et al., 2014) with their Bilingual Auto-encoder
model. This model learns word embeddings dur-
ing a translation task that uses an encoder-decoder
approach. We also report the scores from Kle-
mentiev et al. who introduced the task and the
BiCVM model scores from (Hermann and Blun-
som, 2013).
The results show an overall significant improve-
ment over the other methods, with the added ad-
vantage of being computationally efficient.
</bodyText>
<subsectionHeader confidence="0.999719">
5.2 P@k Word Translation
</subsectionHeader>
<bodyText confidence="0.989662272727273">
Next we evaluated our method on a word transla-
tion task, introduced in (Mikolov et al., 2013b) and
used in (Gouws et al., 2015). The words were ex-
tracted from the publicly available WMT112 cor-
pus. The experiments were done for two sets of
translation: English to Spanish and Spanish to En-
glish. (Mikolov et al., 2013b) extracted the top
6K most frequent words and translated them with
Google Translate. They used the top 5K pairs
to train a translation matrix, and evaluated their
method on the remaining 1K. As our English and
</bodyText>
<footnote confidence="0.99948">
1Thanks to S. Gouws for providing this implementation
2http://www.statmt.org/wmt11/
</footnote>
<page confidence="0.950136">
1111
</page>
<table confidence="0.999126571428571">
Method En --+ De De --+ En Speed-up in training time
Klementiev et al. 77.6% 71.1% x1
Bilingual Auto-encoder 91.8% 72.8% x3
BiCVM 83.7% 71.4% x320
BilBOWA 86,5% 75% x800
Trans-gram 87,8% 78,7% x600
Trans-gram (size 300 vectors EP+WIKI) 91,1% 78,4%
</table>
<tableCaption confidence="0.999604">
Table 1: Comparison of Trans-gram with various methods for Reuters English/German classification
</tableCaption>
<table confidence="0.999745">
Method En --+ Es P@1 En --+ Es P@5 Es --+ En P@1 Es --+ En P@5
Edit distance 13% 18% 24% 27%
Bing 55% 71%
Translation Matrix 33% 35% 51% 52%
BilBOWA 39% 44% 51% 55%
Trans-gram 45% 61% 47% 62%
</table>
<tableCaption confidence="0.999849">
Table 2: Results on the translation task
</tableCaption>
<bodyText confidence="0.999834133333333">
Spanish vectors are already aligned we don’t need
the 5K training pairs and use only the 1K test
pairs.
The reported score, the translation precision
P@k, is the fraction of test-pairs where the tar-
get translation (Google Translate) is one of the k
translations proposed by our model. For a given
English word, w, our model takes its target vectors
w� and proposes the k closest Spanish word using
the co-similarity of their vectors to w. We com-
pare ourselves to the “translation matrix” method
and to the BilBowa aligned vectors. We also re-
port the scores obtained by a trivial algorithm that
uses edit-distance to determine the closest transla-
tion and by the Bing Translator service.
</bodyText>
<sectionHeader confidence="0.995097" genericHeader="method">
6 Interesting properties
</sectionHeader>
<subsectionHeader confidence="0.999483">
6.1 Cross-lingual disambiguation
</subsectionHeader>
<bodyText confidence="0.996387846153846">
We now present the task of cross-lingual dis-
ambiguation as an example of possible uses of
aligned multilingual vectors. The goal of this task
is to find a suitable representation of each sense of
a given polysemous word. The idea of our method
is to look for a language in which the undesired
senses are represented by unambiguous words and
then to perform some arithmetic operation.
Let’s illustrate the process with a concrete ex-
ample: consider the French word “train”, trainfr.
�
The three closest Polish words to trainfr translate
in English into “now”, “a train” and “when”. This
seems a poor matching. In fact, trainfr is polyse-
mous. It can name a line of railroad cars, but it is
also used to form progressive tenses. The French
“Il est en train de manger” translates into “he is
eating”, or in Italian “sta mangiando”.
As the Italian word “sta” is used to form pro-
gressive tenses, it’s a good candidate to disam-
biguate trainfr. Let’s introduce the vector v� =
�
trainfr − stait. Now the three polish words clos-
est to v� translate in English into “a train”, “a train”
and “railroad”. Therefore v� is a better representa-
tion for the railroad sense of trainfr.
</bodyText>
<subsectionHeader confidence="0.998843">
6.2 Transfer of linguistic features
</subsectionHeader>
<bodyText confidence="0.999787421052632">
Another interesting property of the vectors gener-
ated by Trans-gram is the transfer of linguistic fea-
tures through a pivot language that does not pos-
sess these features.
Let’s illustrate this by focusing on Latin lan-
guages, which possess some features that En-
glish does not, like rich conjugations. For ex-
ample, in French and Italian the infinitives of
“eat” are mangerfr and mangiareit, and the first
plural persons are mangeonsfr and mangiamoit.
Actually in our models we observe the follow-
mangerfr ≈ mangiareit and
� mangiamoit. It is thus remark-
able to see that features not present in English
match in languages aligned through English as
the only pivot language. We also found similar
transfers for the genders of adjectives and are cur-
rently studying other similar properties captured
by Trans-gram.
</bodyText>
<figure confidence="0.609135666666667">
ing alignments:
�
mangeonsfr ≈
</figure>
<page confidence="0.992066">
1112
</page>
<sectionHeader confidence="0.997288" genericHeader="conclusions">
7 Conclusion
</sectionHeader>
<bodyText confidence="0.999588090909091">
In this paper we provided the following contri-
butions: Trans-gram, a new method to compute
cross-lingual word-embeddings in a single word
space; state of the art results on cross-lingual NLP
tasks; a sketch of a cross-lingual calculus to help
disambiguate polysemous words; the exhibition
of linguistic features transfers through a pivot-
language not possessing those features.
We are still exploring promising properties of
the generated vectors and their applications in
other NLP tasks (Sentiment Analysis, NER...).
</bodyText>
<sectionHeader confidence="0.999179" genericHeader="references">
References
</sectionHeader>
<reference confidence="0.999747933333333">
Rami Al-Rfou, Bryan Perozzi, and Steven Skiena.
2013. Polyglot: Distributed word represen-
tations for multilingual nlp. arXiv preprint
arXiv:1307.1662.
Manaal Faruqui and Chris Dyer. 2014. Improving
vector space word representations using multilingual
correlation. Association for Computational Linguis-
tics.
Stephan Gouws, Yoshua Bengio, and Greg Corrado.
2015. Bilbowa: Fast bilingual distributed represen-
tations without word alignments. In Proceedings of
the 25th international conference on Machine learn-
ing, volume 15, pages 748–756.
Zellig Sabbettai Harris. 1968. Mathematical structures
of language.
Karl Moritz Hermann and Phil Blunsom. 2013. Mul-
tilingual distributed representations without word
alignment. arXiv preprint arXiv:1312.6173.
Geoffrey E Hinton. 1984. Distributed representations.
Alexandre Klementiev, Ivan Titov, and Binod Bhat-
tarai. 2012. Inducing crosslingual distributed rep-
resentations of words.
Philipp Koehn. 2005. A parallel corpus for statistical
machine translation. In MT Summit.
Stanislas Lauly, Hugo Larochelle, Mitesh Khapra,
Balaraman Ravindran, Vikas C Raykar, and Amrita
Saha. 2014. An autoencoder approach to learning
bilingual word representations. In Advances in Neu-
ral Information Processing Systems, pages 1853–
1861.
Omer Levy and Yoav Goldberg. 2014. Neural word
embedding as implicit matrix factorization. In Ad-
vances in Neural Information Processing Systems,
pages 2177–2185.
Y.; Rose T.; Lewis, D. D.; Yang and F. Li. 2004. Rcv1:
A new benchmark collection for text categorization
research. In Journal of Machine Learning Research.
Tomas Mikolov, Kai Chen, Greg Corrado, and Jef-
frey Dean. 2013a. Efficient estimation of word
representations in vector space. arXiv preprint
arXiv:1301.3781.
Tomas Mikolov, Quoc V Le, and Ilya Sutskever.
2013b. Exploiting similarities among lan-
guages for machine translation. arXiv preprint
arXiv:1309.4168.
</reference>
<page confidence="0.985373">
1113
</page>
</variant>
</algorithm>
<algorithm name="ParsHed" version="110505">
<variant no="0" confidence="0.528013">
<title confidence="0.966714">Trans-gram, Fast Cross-lingual</title>
<affiliation confidence="0.839908">Coulmance Jean-Marc Marty Wenzek Benhalloum</affiliation>
<address confidence="0.9989155">105 rue La Fayette 105 rue La Fayette 105 rue La Fayette 105 rue La Fayette 75010 Paris 75010 Paris 75010 Paris 75010</address>
<email confidence="0.99735">joc@proxem.comjmm@proxem.comguw@proxem.comaba@proxem.com</email>
<abstract confidence="0.999547764705882">introduce a simple and computationally-efficient method to simultaneously learn and align wordembeddings for a variety of languages, using only monolingual data and a smaller set of sentence-aligned data. We use our new method to compute aligned wordembeddings for twenty-one languages using English as a pivot language. We show that some linguistic features are aligned across languages for which we do not have aligned data, even though those properties do not exist in the pivot language. We also achieve state of the art results on standard cross-lingual text classification and word translation tasks.</abstract>
</variant>
</algorithm>
<algorithm name="ParsCit" version="110505">
<citationList>
<citation valid="true">
<authors>
<author>Rami Al-Rfou</author>
<author>Bryan Perozzi</author>
<author>Steven Skiena</author>
</authors>
<title>Polyglot: Distributed word representations for multilingual nlp. arXiv preprint arXiv:1307.1662.</title>
<date>2013</date>
<contexts>
<context position="9375" citStr="Al-Rfou et al., 2013" startWordPosition="1514" endWordPosition="1517"> aligned at sentence-level; secondly each pair of languages contains English as one of its members: for instance, there is no French/Italian pair. In other words, English is used as a pivot language. No bi-lingual lexicons nor other bi-lingual datasets aligned at the word level were used. Using only the Europarl-v7 texts as both monolingual and bilingual data, it took 10 minutes to align 2 languages, and two and a half hours to align the 21 languages of the corpus, in a 40 dimensional space on a 6 core computer. We also computed 300 dimensions vectors using the Wikipedia extracts provided by (Al-Rfou et al., 2013) as monolingual data for each language. The training time was 21 hours. 5 Experiments 5.1 Reuters Cross-lingual Document Classification We used a subset of the English and German sections of the Reuters RCV1/RCV2 corpora (Lewis and Li, 2004) (10000 documents each), as in (Klementiev et al., 2012), and we replicated the experimental setting. In the English dataset, there are four topics: CCAT (Corporate/Industrial), ECAT (Economics), GCAT (Government/Social), and MCAT (Markets). We used these topics as our labels and we only selected documents labeled with a single topic. We trained our classif</context>
</contexts>
<marker>Al-Rfou, Perozzi, Skiena, 2013</marker>
<rawString>Rami Al-Rfou, Bryan Perozzi, and Steven Skiena. 2013. Polyglot: Distributed word representations for multilingual nlp. arXiv preprint arXiv:1307.1662.</rawString>
</citation>
<citation valid="true">
<authors>
<author>Manaal Faruqui</author>
<author>Chris Dyer</author>
</authors>
<title>Improving vector space word representations using multilingual correlation. Association for Computational Linguistics.</title>
<date>2014</date>
<contexts>
<context position="3455" citStr="Faruqui and Dyer, 2014" startWordPosition="529" endWordPosition="532">s (i.e. “semantically close”) within each language - this is the mono-lingual objective; second, similar representations must be assigned to similar words across languages - this is the cross-lingual objective. The simplest approach consists in separating the mono-lingual optimization task from the crosslingual optimization task. This is for example the case in (Mikolov et al., 2013b). The idea is to separately train two sets of word-embeddings for each language and then to do a parametric estimation of the mapping between word-embeddings across languages. This method was further extended by (Faruqui and Dyer, 2014). Even though those algorithms proved to be viable and fast, it is not clear whether or not a simple mapping between whole languages exists. Moreover, they require word alignments which are a rare and expensive resource. Mannde + 1109 Proceedings of the 2015 Conference on Empirical Methods in Natural Language Processing, pages 1109–1113, Lisbon, Portugal, 17-21 September 2015. c�2015 Association for Computational Linguistics. Another approach consists in focusing entirely on the cross-lingual objective. This was explored in (Hermann and Blunsom, 2013; Lauly et al., 2014) where every couple of </context>
</contexts>
<marker>Faruqui, Dyer, 2014</marker>
<rawString>Manaal Faruqui and Chris Dyer. 2014. Improving vector space word representations using multilingual correlation. Association for Computational Linguistics.</rawString>
</citation>
<citation valid="true">
<authors>
<author>Stephan Gouws</author>
<author>Yoshua Bengio</author>
<author>Greg Corrado</author>
</authors>
<title>Bilbowa: Fast bilingual distributed representations without word alignments.</title>
<date>2015</date>
<booktitle>In Proceedings of the 25th international conference on Machine learning,</booktitle>
<volume>15</volume>
<pages>748--756</pages>
<contexts>
<context position="1829" citStr="Gouws et al., 2015" startWordPosition="276" endWordPosition="279">nguistic properties. The linguistic properties are distributed along the dimensions of the space. A popular method to compute wordembeddings is the Skip-gram model (Mikolov et al., 2013a). This algorithm learns high-quality word vectors with a computation cost much lower than previous methods. This allows the processing of very important amounts of data. For instance, a 1.6 billion words dataset can be processed in less than one day. Several authors came up with different methods to align word-embeddings across two languages (Klementiev et al., 2012; Mikolov et al., 2013b; Lauly et al., 2014; Gouws et al., 2015). ∗These authors contributed equally. In this article, we introduce a new method called Trans-gram, which learns word embeddings aligned across many languages, in a simple and efficient fashion, using only sentence alignments rather than word alignments. We compare our method with previous approaches on a crosslingual document classification task and on a word translation task and obtain state of the art results on these tasks. Additionally, word-embeddings for twenty-one languages are learned simultaneously - to our knowledge - for the first time, in less than two and a half hours. Furthermor</context>
<context position="4719" citStr="Gouws et al., 2015" startWordPosition="718" endWordPosition="721">ed-size vectors. Then, the model minimizes the Euclidean distance between both vectors. This idea allows processing corpus aligned at sentence-level rather than word-level. However, it does not leverage the abundance of existing mono-lingual corpora. A popular approach is to jointly optimize the mono-lingual and cross-lingual objectives simultaneously. This is mostly done by minimizing the sum of mono-lingual loss functions for each language and the cross-lingual loss function. (Klementiev et al., 2012) proved this approach to be useful by obtaining state-of-the-art results on several tasks. (Gouws et al., 2015) extends their work with a more computationally-efficient implementation. 3 From Skip-Gram to Trans-Gram 3.1 Skip-gram We briefly introduce the Skip-gram algorithm, as we will need it for further explanations. Skipgram allows to train word embeddings for a language using mono-lingual data. This method uses a dual representation for words. Each word w has two embeddings: a target vector, w~ (E RD), and a context vector,w (E RD). The algorithm tries to estimate the probability of a word w to appear in the context of a word c. More precisely we are learning the embeddings ~w,c so that: σ(~w c) = </context>
<context position="6235" citStr="Gouws et al., 2015" startWordPosition="979" endWordPosition="982">red around w. For the sake of simplicity this equation does not include the “negative-sampling” term, see (Mikolov et al., 2013a) for more details. Skip-gram can be seen as a materialization of the distributional hypothesis (Harris, 1968): “Words used in similar contexts have similar meanings”. We will now see how to extend this idea to cross-lingual contexts. 3.2 Trans-gram In this section we introduce Trans-gram, a new method to compute aligned word-embeddings for a variety of languages. Our method will minimize the summation of mono-lingual losses and cross-lingual losses. Like in BilBOWA (Gouws et al., 2015), we use Skipgram as a mono-lingual loss. Assuming we are trying to learn aligned word vectors for languages e (e.g. English) and f (e.g. French), we note Je and Jf the two mono-lingual losses. In BilBOWA, the cross-lingual loss function is a distance between bag-of-words representations of two aligned sentences. But as (Levy and Goldberg, 2014) showed that the Skip-gram loss function extracts interesting linguistic features, we wanted to use a loss function for the cross-lingual objective that will be closer to Skip-gram than BilBOWA. Therefore, we introduce a new task, Transgram, similar to </context>
<context position="10772" citStr="Gouws et al., 2015" startWordPosition="1745" endWordPosition="1748">guage. The classifier used was an averaged perceptron, and we used the implementation from (Klementiev et al., 2012)1. The word vectors were computed on the Europarl-v7 parallel corpus with size 40 like other methods. For this task only the target vectors where used. We report the percentage precision obtained with our method, in comparison with other methods, in Table 1. The table also include results obtained with 300 dimensions vectors trained by Trans-gram with the Europarl-v7 as parallel corpus and the Wikipedia as mono-lingual corpus. The previous state of the art results were detained (Gouws et al., 2015) with BilBOWA and (Lauly et al., 2014) with their Bilingual Auto-encoder model. This model learns word embeddings during a translation task that uses an encoder-decoder approach. We also report the scores from Klementiev et al. who introduced the task and the BiCVM model scores from (Hermann and Blunsom, 2013). The results show an overall significant improvement over the other methods, with the added advantage of being computationally efficient. 5.2 P@k Word Translation Next we evaluated our method on a word translation task, introduced in (Mikolov et al., 2013b) and used in (Gouws et al., 201</context>
</contexts>
<marker>Gouws, Bengio, Corrado, 2015</marker>
<rawString>Stephan Gouws, Yoshua Bengio, and Greg Corrado. 2015. Bilbowa: Fast bilingual distributed representations without word alignments. In Proceedings of the 25th international conference on Machine learning, volume 15, pages 748–756.</rawString>
</citation>
<citation valid="true">
<authors>
<author>Zellig Sabbettai Harris</author>
</authors>
<date>1968</date>
<note>Mathematical structures of language.</note>
<contexts>
<context position="5854" citStr="Harris, 1968" startWordPosition="922" endWordPosition="923">word c. More precisely we are learning the embeddings ~w,c so that: σ(~w c) = P(w|c) where σ is the sigmoid function. A simplified version of the loss function minimized by Skip-gram is the following: J = X X X − log σ(~w c) (1) sEC wEs cEs[w−l:w+l] where C is the set of sentences constituting the training corpus, and s[w −l : w +l] is a word window on the sentence s centered around w. For the sake of simplicity this equation does not include the “negative-sampling” term, see (Mikolov et al., 2013a) for more details. Skip-gram can be seen as a materialization of the distributional hypothesis (Harris, 1968): “Words used in similar contexts have similar meanings”. We will now see how to extend this idea to cross-lingual contexts. 3.2 Trans-gram In this section we introduce Trans-gram, a new method to compute aligned word-embeddings for a variety of languages. Our method will minimize the summation of mono-lingual losses and cross-lingual losses. Like in BilBOWA (Gouws et al., 2015), we use Skipgram as a mono-lingual loss. Assuming we are trying to learn aligned word vectors for languages e (e.g. English) and f (e.g. French), we note Je and Jf the two mono-lingual losses. In BilBOWA, the cross-lin</context>
</contexts>
<marker>Harris, 1968</marker>
<rawString>Zellig Sabbettai Harris. 1968. Mathematical structures of language.</rawString>
</citation>
<citation valid="true">
<authors>
<author>Karl Moritz Hermann</author>
<author>Phil Blunsom</author>
</authors>
<title>Multilingual distributed representations without word alignment. arXiv preprint arXiv:1312.6173.</title>
<date>2013</date>
<contexts>
<context position="4011" citStr="Hermann and Blunsom, 2013" startWordPosition="611" endWordPosition="614">nguages. This method was further extended by (Faruqui and Dyer, 2014). Even though those algorithms proved to be viable and fast, it is not clear whether or not a simple mapping between whole languages exists. Moreover, they require word alignments which are a rare and expensive resource. Mannde + 1109 Proceedings of the 2015 Conference on Empirical Methods in Natural Language Processing, pages 1109–1113, Lisbon, Portugal, 17-21 September 2015. c�2015 Association for Computational Linguistics. Another approach consists in focusing entirely on the cross-lingual objective. This was explored in (Hermann and Blunsom, 2013; Lauly et al., 2014) where every couple of aligned sentences is transformed into two fixed-size vectors. Then, the model minimizes the Euclidean distance between both vectors. This idea allows processing corpus aligned at sentence-level rather than word-level. However, it does not leverage the abundance of existing mono-lingual corpora. A popular approach is to jointly optimize the mono-lingual and cross-lingual objectives simultaneously. This is mostly done by minimizing the sum of mono-lingual loss functions for each language and the cross-lingual loss function. (Klementiev et al., 2012) pr</context>
<context position="11083" citStr="Hermann and Blunsom, 2013" startWordPosition="1796" endWordPosition="1800">btained with our method, in comparison with other methods, in Table 1. The table also include results obtained with 300 dimensions vectors trained by Trans-gram with the Europarl-v7 as parallel corpus and the Wikipedia as mono-lingual corpus. The previous state of the art results were detained (Gouws et al., 2015) with BilBOWA and (Lauly et al., 2014) with their Bilingual Auto-encoder model. This model learns word embeddings during a translation task that uses an encoder-decoder approach. We also report the scores from Klementiev et al. who introduced the task and the BiCVM model scores from (Hermann and Blunsom, 2013). The results show an overall significant improvement over the other methods, with the added advantage of being computationally efficient. 5.2 P@k Word Translation Next we evaluated our method on a word translation task, introduced in (Mikolov et al., 2013b) and used in (Gouws et al., 2015). The words were extracted from the publicly available WMT112 corpus. The experiments were done for two sets of translation: English to Spanish and Spanish to English. (Mikolov et al., 2013b) extracted the top 6K most frequent words and translated them with Google Translate. They used the top 5K pairs to tra</context>
</contexts>
<marker>Hermann, Blunsom, 2013</marker>
<rawString>Karl Moritz Hermann and Phil Blunsom. 2013. Multilingual distributed representations without word alignment. arXiv preprint arXiv:1312.6173.</rawString>
</citation>
<citation valid="true">
<authors>
<author>Geoffrey E Hinton</author>
</authors>
<date>1984</date>
<note>Distributed representations.</note>
<contexts>
<context position="1106" citStr="Hinton, 1984" startWordPosition="165" endWordPosition="166">s, using only monolingual data and a smaller set of sentence-aligned data. We use our new method to compute aligned wordembeddings for twenty-one languages using English as a pivot language. We show that some linguistic features are aligned across languages for which we do not have aligned data, even though those properties do not exist in the pivot language. We also achieve state of the art results on standard cross-lingual text classification and word translation tasks. 1 Introduction Word-embeddings are a representation of words with fixed-sized vectors. It is a distributed representation (Hinton, 1984) in the sense that there is not necessarily a one-to-one correspondence between vector dimensions and linguistic properties. The linguistic properties are distributed along the dimensions of the space. A popular method to compute wordembeddings is the Skip-gram model (Mikolov et al., 2013a). This algorithm learns high-quality word vectors with a computation cost much lower than previous methods. This allows the processing of very important amounts of data. For instance, a 1.6 billion words dataset can be processed in less than one day. Several authors came up with different methods to align wo</context>
</contexts>
<marker>Hinton, 1984</marker>
<rawString>Geoffrey E Hinton. 1984. Distributed representations.</rawString>
</citation>
<citation valid="true">
<authors>
<author>Alexandre Klementiev</author>
<author>Ivan Titov</author>
<author>Binod Bhattarai</author>
</authors>
<title>Inducing crosslingual distributed representations of words.</title>
<date>2012</date>
<contexts>
<context position="1765" citStr="Klementiev et al., 2012" startWordPosition="264" endWordPosition="267">ssarily a one-to-one correspondence between vector dimensions and linguistic properties. The linguistic properties are distributed along the dimensions of the space. A popular method to compute wordembeddings is the Skip-gram model (Mikolov et al., 2013a). This algorithm learns high-quality word vectors with a computation cost much lower than previous methods. This allows the processing of very important amounts of data. For instance, a 1.6 billion words dataset can be processed in less than one day. Several authors came up with different methods to align word-embeddings across two languages (Klementiev et al., 2012; Mikolov et al., 2013b; Lauly et al., 2014; Gouws et al., 2015). ∗These authors contributed equally. In this article, we introduce a new method called Trans-gram, which learns word embeddings aligned across many languages, in a simple and efficient fashion, using only sentence alignments rather than word alignments. We compare our method with previous approaches on a crosslingual document classification task and on a word translation task and obtain state of the art results on these tasks. Additionally, word-embeddings for twenty-one languages are learned simultaneously - to our knowledge - f</context>
<context position="4608" citStr="Klementiev et al., 2012" startWordPosition="699" endWordPosition="703"> (Hermann and Blunsom, 2013; Lauly et al., 2014) where every couple of aligned sentences is transformed into two fixed-size vectors. Then, the model minimizes the Euclidean distance between both vectors. This idea allows processing corpus aligned at sentence-level rather than word-level. However, it does not leverage the abundance of existing mono-lingual corpora. A popular approach is to jointly optimize the mono-lingual and cross-lingual objectives simultaneously. This is mostly done by minimizing the sum of mono-lingual loss functions for each language and the cross-lingual loss function. (Klementiev et al., 2012) proved this approach to be useful by obtaining state-of-the-art results on several tasks. (Gouws et al., 2015) extends their work with a more computationally-efficient implementation. 3 From Skip-Gram to Trans-Gram 3.1 Skip-gram We briefly introduce the Skip-gram algorithm, as we will need it for further explanations. Skipgram allows to train word embeddings for a language using mono-lingual data. This method uses a dual representation for words. Each word w has two embeddings: a target vector, w~ (E RD), and a context vector,w (E RD). The algorithm tries to estimate the probability of a word</context>
<context position="9672" citStr="Klementiev et al., 2012" startWordPosition="1562" endWordPosition="1566">g only the Europarl-v7 texts as both monolingual and bilingual data, it took 10 minutes to align 2 languages, and two and a half hours to align the 21 languages of the corpus, in a 40 dimensional space on a 6 core computer. We also computed 300 dimensions vectors using the Wikipedia extracts provided by (Al-Rfou et al., 2013) as monolingual data for each language. The training time was 21 hours. 5 Experiments 5.1 Reuters Cross-lingual Document Classification We used a subset of the English and German sections of the Reuters RCV1/RCV2 corpora (Lewis and Li, 2004) (10000 documents each), as in (Klementiev et al., 2012), and we replicated the experimental setting. In the English dataset, there are four topics: CCAT (Corporate/Industrial), ECAT (Economics), GCAT (Government/Social), and MCAT (Markets). We used these topics as our labels and we only selected documents labeled with a single topic. We trained our classifier on the articles of one language, where each document was represented using an IDF weighted sum of the vectors of its words, we then tested it on the articles of the other language. The classifier used was an averaged perceptron, and we used the implementation from (Klementiev et al., 2012)1. </context>
</contexts>
<marker>Klementiev, Titov, Bhattarai, 2012</marker>
<rawString>Alexandre Klementiev, Ivan Titov, and Binod Bhattarai. 2012. Inducing crosslingual distributed representations of words.</rawString>
</citation>
<citation valid="true">
<authors>
<author>Philipp Koehn</author>
</authors>
<title>A parallel corpus for statistical machine translation.</title>
<date>2005</date>
<booktitle>In MT Summit.</booktitle>
<contexts>
<context position="8677" citStr="Koehn, 2005" startWordPosition="1398" endWordPosition="1399">in our case we only used corpora aligned with English. −log σ( ~we cf) X cfEsf X weEse 1110 ~sitse Je ~assisf Qf,e Qe,f ~sitse ~assisf Jf thecat onthee thecatsitsonthemate lechatestassissurletapisf chatest surlef Figure 1: The four partial objectives contributing to the alignment of English and French: a Skip-gram objective per language (Je and Jf) over a window surrounding a target word (blue) and two Trans-gram objectives (ne,f and nf,e) over the whole sentence aligned with the sentence from which the target word is extracted (red). 4 Implementation In our experiments, we used the Europarl (Koehn, 2005) aligned corpora. Europarl-v7 has two peculiarities: firstly, the corpora are aligned at sentence-level; secondly each pair of languages contains English as one of its members: for instance, there is no French/Italian pair. In other words, English is used as a pivot language. No bi-lingual lexicons nor other bi-lingual datasets aligned at the word level were used. Using only the Europarl-v7 texts as both monolingual and bilingual data, it took 10 minutes to align 2 languages, and two and a half hours to align the 21 languages of the corpus, in a 40 dimensional space on a 6 core computer. We al</context>
</contexts>
<marker>Koehn, 2005</marker>
<rawString>Philipp Koehn. 2005. A parallel corpus for statistical machine translation. In MT Summit.</rawString>
</citation>
<citation valid="true">
<authors>
<author>Stanislas Lauly</author>
<author>Hugo Larochelle</author>
<author>Mitesh Khapra</author>
<author>Balaraman Ravindran</author>
<author>Vikas C Raykar</author>
<author>Amrita Saha</author>
</authors>
<title>An autoencoder approach to learning bilingual word representations.</title>
<date>2014</date>
<booktitle>In Advances in Neural Information Processing Systems,</booktitle>
<pages>1853--1861</pages>
<contexts>
<context position="1808" citStr="Lauly et al., 2014" startWordPosition="272" endWordPosition="275">or dimensions and linguistic properties. The linguistic properties are distributed along the dimensions of the space. A popular method to compute wordembeddings is the Skip-gram model (Mikolov et al., 2013a). This algorithm learns high-quality word vectors with a computation cost much lower than previous methods. This allows the processing of very important amounts of data. For instance, a 1.6 billion words dataset can be processed in less than one day. Several authors came up with different methods to align word-embeddings across two languages (Klementiev et al., 2012; Mikolov et al., 2013b; Lauly et al., 2014; Gouws et al., 2015). ∗These authors contributed equally. In this article, we introduce a new method called Trans-gram, which learns word embeddings aligned across many languages, in a simple and efficient fashion, using only sentence alignments rather than word alignments. We compare our method with previous approaches on a crosslingual document classification task and on a word translation task and obtain state of the art results on these tasks. Additionally, word-embeddings for twenty-one languages are learned simultaneously - to our knowledge - for the first time, in less than two and a h</context>
<context position="4032" citStr="Lauly et al., 2014" startWordPosition="615" endWordPosition="618">rther extended by (Faruqui and Dyer, 2014). Even though those algorithms proved to be viable and fast, it is not clear whether or not a simple mapping between whole languages exists. Moreover, they require word alignments which are a rare and expensive resource. Mannde + 1109 Proceedings of the 2015 Conference on Empirical Methods in Natural Language Processing, pages 1109–1113, Lisbon, Portugal, 17-21 September 2015. c�2015 Association for Computational Linguistics. Another approach consists in focusing entirely on the cross-lingual objective. This was explored in (Hermann and Blunsom, 2013; Lauly et al., 2014) where every couple of aligned sentences is transformed into two fixed-size vectors. Then, the model minimizes the Euclidean distance between both vectors. This idea allows processing corpus aligned at sentence-level rather than word-level. However, it does not leverage the abundance of existing mono-lingual corpora. A popular approach is to jointly optimize the mono-lingual and cross-lingual objectives simultaneously. This is mostly done by minimizing the sum of mono-lingual loss functions for each language and the cross-lingual loss function. (Klementiev et al., 2012) proved this approach to</context>
<context position="10810" citStr="Lauly et al., 2014" startWordPosition="1752" endWordPosition="1755">aged perceptron, and we used the implementation from (Klementiev et al., 2012)1. The word vectors were computed on the Europarl-v7 parallel corpus with size 40 like other methods. For this task only the target vectors where used. We report the percentage precision obtained with our method, in comparison with other methods, in Table 1. The table also include results obtained with 300 dimensions vectors trained by Trans-gram with the Europarl-v7 as parallel corpus and the Wikipedia as mono-lingual corpus. The previous state of the art results were detained (Gouws et al., 2015) with BilBOWA and (Lauly et al., 2014) with their Bilingual Auto-encoder model. This model learns word embeddings during a translation task that uses an encoder-decoder approach. We also report the scores from Klementiev et al. who introduced the task and the BiCVM model scores from (Hermann and Blunsom, 2013). The results show an overall significant improvement over the other methods, with the added advantage of being computationally efficient. 5.2 P@k Word Translation Next we evaluated our method on a word translation task, introduced in (Mikolov et al., 2013b) and used in (Gouws et al., 2015). The words were extracted from the </context>
</contexts>
<marker>Lauly, Larochelle, Khapra, Ravindran, Raykar, Saha, 2014</marker>
<rawString>Stanislas Lauly, Hugo Larochelle, Mitesh Khapra, Balaraman Ravindran, Vikas C Raykar, and Amrita Saha. 2014. An autoencoder approach to learning bilingual word representations. In Advances in Neural Information Processing Systems, pages 1853– 1861.</rawString>
</citation>
<citation valid="true">
<authors>
<author>Omer Levy</author>
<author>Yoav Goldberg</author>
</authors>
<title>Neural word embedding as implicit matrix factorization.</title>
<date>2014</date>
<booktitle>In Advances in Neural Information Processing Systems,</booktitle>
<pages>2177--2185</pages>
<contexts>
<context position="6582" citStr="Levy and Goldberg, 2014" startWordPosition="1037" endWordPosition="1040">ss-lingual contexts. 3.2 Trans-gram In this section we introduce Trans-gram, a new method to compute aligned word-embeddings for a variety of languages. Our method will minimize the summation of mono-lingual losses and cross-lingual losses. Like in BilBOWA (Gouws et al., 2015), we use Skipgram as a mono-lingual loss. Assuming we are trying to learn aligned word vectors for languages e (e.g. English) and f (e.g. French), we note Je and Jf the two mono-lingual losses. In BilBOWA, the cross-lingual loss function is a distance between bag-of-words representations of two aligned sentences. But as (Levy and Goldberg, 2014) showed that the Skip-gram loss function extracts interesting linguistic features, we wanted to use a loss function for the cross-lingual objective that will be closer to Skip-gram than BilBOWA. Therefore, we introduce a new task, Transgram, similar to Skip-gram. Each English sentence se in our aligned corpus Ae,f is aligned with a French sentence sf. In Skip-gram, the context picked for a target word we in a sentence se is the set of words ce appearing in the window centered around we: se[we − l : we + l]. In Trans-gram, the context picked for a target word we in a sentence se will be all the</context>
</contexts>
<marker>Levy, Goldberg, 2014</marker>
<rawString>Omer Levy and Yoav Goldberg. 2014. Neural word embedding as implicit matrix factorization. In Advances in Neural Information Processing Systems, pages 2177–2185.</rawString>
</citation>
<citation valid="true">
<authors>
<author>T Y Rose</author>
<author>D D Lewis</author>
<author>Yang</author>
<author>F Li</author>
</authors>
<title>Rcv1: A new benchmark collection for text categorization research.</title>
<date>2004</date>
<journal>In Journal of Machine Learning Research.</journal>
<marker>Rose, Lewis, Yang, Li, 2004</marker>
<rawString>Y.; Rose T.; Lewis, D. D.; Yang and F. Li. 2004. Rcv1: A new benchmark collection for text categorization research. In Journal of Machine Learning Research.</rawString>
</citation>
<citation valid="true">
<authors>
<author>Tomas Mikolov</author>
<author>Kai Chen</author>
<author>Greg Corrado</author>
<author>Jeffrey Dean</author>
</authors>
<title>Efficient estimation of word representations in vector space. arXiv preprint arXiv:1301.3781.</title>
<date>2013</date>
<contexts>
<context position="1395" citStr="Mikolov et al., 2013" startWordPosition="207" endWordPosition="210"> aligned data, even though those properties do not exist in the pivot language. We also achieve state of the art results on standard cross-lingual text classification and word translation tasks. 1 Introduction Word-embeddings are a representation of words with fixed-sized vectors. It is a distributed representation (Hinton, 1984) in the sense that there is not necessarily a one-to-one correspondence between vector dimensions and linguistic properties. The linguistic properties are distributed along the dimensions of the space. A popular method to compute wordembeddings is the Skip-gram model (Mikolov et al., 2013a). This algorithm learns high-quality word vectors with a computation cost much lower than previous methods. This allows the processing of very important amounts of data. For instance, a 1.6 billion words dataset can be processed in less than one day. Several authors came up with different methods to align word-embeddings across two languages (Klementiev et al., 2012; Mikolov et al., 2013b; Lauly et al., 2014; Gouws et al., 2015). ∗These authors contributed equally. In this article, we introduce a new method called Trans-gram, which learns word embeddings aligned across many languages, in a s</context>
<context position="3217" citStr="Mikolov et al., 2013" startWordPosition="491" endWordPosition="494"> 2 Review of Previous Work A number of methods have been explored to train and align bilingual word-embeddings. These methods pursue two objectives: first, similar representations (i.e. spatially close) must be assigned to similar words (i.e. “semantically close”) within each language - this is the mono-lingual objective; second, similar representations must be assigned to similar words across languages - this is the cross-lingual objective. The simplest approach consists in separating the mono-lingual optimization task from the crosslingual optimization task. This is for example the case in (Mikolov et al., 2013b). The idea is to separately train two sets of word-embeddings for each language and then to do a parametric estimation of the mapping between word-embeddings across languages. This method was further extended by (Faruqui and Dyer, 2014). Even though those algorithms proved to be viable and fast, it is not clear whether or not a simple mapping between whole languages exists. Moreover, they require word alignments which are a rare and expensive resource. Mannde + 1109 Proceedings of the 2015 Conference on Empirical Methods in Natural Language Processing, pages 1109–1113, Lisbon, Portugal, 17-2</context>
<context position="5743" citStr="Mikolov et al., 2013" startWordPosition="904" endWordPosition="907"> a context vector,w (E RD). The algorithm tries to estimate the probability of a word w to appear in the context of a word c. More precisely we are learning the embeddings ~w,c so that: σ(~w c) = P(w|c) where σ is the sigmoid function. A simplified version of the loss function minimized by Skip-gram is the following: J = X X X − log σ(~w c) (1) sEC wEs cEs[w−l:w+l] where C is the set of sentences constituting the training corpus, and s[w −l : w +l] is a word window on the sentence s centered around w. For the sake of simplicity this equation does not include the “negative-sampling” term, see (Mikolov et al., 2013a) for more details. Skip-gram can be seen as a materialization of the distributional hypothesis (Harris, 1968): “Words used in similar contexts have similar meanings”. We will now see how to extend this idea to cross-lingual contexts. 3.2 Trans-gram In this section we introduce Trans-gram, a new method to compute aligned word-embeddings for a variety of languages. Our method will minimize the summation of mono-lingual losses and cross-lingual losses. Like in BilBOWA (Gouws et al., 2015), we use Skipgram as a mono-lingual loss. Assuming we are trying to learn aligned word vectors for languages</context>
<context position="11339" citStr="Mikolov et al., 2013" startWordPosition="1839" endWordPosition="1842">of the art results were detained (Gouws et al., 2015) with BilBOWA and (Lauly et al., 2014) with their Bilingual Auto-encoder model. This model learns word embeddings during a translation task that uses an encoder-decoder approach. We also report the scores from Klementiev et al. who introduced the task and the BiCVM model scores from (Hermann and Blunsom, 2013). The results show an overall significant improvement over the other methods, with the added advantage of being computationally efficient. 5.2 P@k Word Translation Next we evaluated our method on a word translation task, introduced in (Mikolov et al., 2013b) and used in (Gouws et al., 2015). The words were extracted from the publicly available WMT112 corpus. The experiments were done for two sets of translation: English to Spanish and Spanish to English. (Mikolov et al., 2013b) extracted the top 6K most frequent words and translated them with Google Translate. They used the top 5K pairs to train a translation matrix, and evaluated their method on the remaining 1K. As our English and 1Thanks to S. Gouws for providing this implementation 2http://www.statmt.org/wmt11/ 1111 Method En --+ De De --+ En Speed-up in training time Klementiev et al. 77.6</context>
</contexts>
<marker>Mikolov, Chen, Corrado, Dean, 2013</marker>
<rawString>Tomas Mikolov, Kai Chen, Greg Corrado, and Jeffrey Dean. 2013a. Efficient estimation of word representations in vector space. arXiv preprint arXiv:1301.3781.</rawString>
</citation>
<citation valid="true">
<authors>
<author>Tomas Mikolov</author>
<author>Quoc V Le</author>
<author>Ilya Sutskever</author>
</authors>
<title>Exploiting similarities among languages for machine translation. arXiv preprint arXiv:1309.4168.</title>
<date>2013</date>
<contexts>
<context position="1395" citStr="Mikolov et al., 2013" startWordPosition="207" endWordPosition="210"> aligned data, even though those properties do not exist in the pivot language. We also achieve state of the art results on standard cross-lingual text classification and word translation tasks. 1 Introduction Word-embeddings are a representation of words with fixed-sized vectors. It is a distributed representation (Hinton, 1984) in the sense that there is not necessarily a one-to-one correspondence between vector dimensions and linguistic properties. The linguistic properties are distributed along the dimensions of the space. A popular method to compute wordembeddings is the Skip-gram model (Mikolov et al., 2013a). This algorithm learns high-quality word vectors with a computation cost much lower than previous methods. This allows the processing of very important amounts of data. For instance, a 1.6 billion words dataset can be processed in less than one day. Several authors came up with different methods to align word-embeddings across two languages (Klementiev et al., 2012; Mikolov et al., 2013b; Lauly et al., 2014; Gouws et al., 2015). ∗These authors contributed equally. In this article, we introduce a new method called Trans-gram, which learns word embeddings aligned across many languages, in a s</context>
<context position="3217" citStr="Mikolov et al., 2013" startWordPosition="491" endWordPosition="494"> 2 Review of Previous Work A number of methods have been explored to train and align bilingual word-embeddings. These methods pursue two objectives: first, similar representations (i.e. spatially close) must be assigned to similar words (i.e. “semantically close”) within each language - this is the mono-lingual objective; second, similar representations must be assigned to similar words across languages - this is the cross-lingual objective. The simplest approach consists in separating the mono-lingual optimization task from the crosslingual optimization task. This is for example the case in (Mikolov et al., 2013b). The idea is to separately train two sets of word-embeddings for each language and then to do a parametric estimation of the mapping between word-embeddings across languages. This method was further extended by (Faruqui and Dyer, 2014). Even though those algorithms proved to be viable and fast, it is not clear whether or not a simple mapping between whole languages exists. Moreover, they require word alignments which are a rare and expensive resource. Mannde + 1109 Proceedings of the 2015 Conference on Empirical Methods in Natural Language Processing, pages 1109–1113, Lisbon, Portugal, 17-2</context>
<context position="5743" citStr="Mikolov et al., 2013" startWordPosition="904" endWordPosition="907"> a context vector,w (E RD). The algorithm tries to estimate the probability of a word w to appear in the context of a word c. More precisely we are learning the embeddings ~w,c so that: σ(~w c) = P(w|c) where σ is the sigmoid function. A simplified version of the loss function minimized by Skip-gram is the following: J = X X X − log σ(~w c) (1) sEC wEs cEs[w−l:w+l] where C is the set of sentences constituting the training corpus, and s[w −l : w +l] is a word window on the sentence s centered around w. For the sake of simplicity this equation does not include the “negative-sampling” term, see (Mikolov et al., 2013a) for more details. Skip-gram can be seen as a materialization of the distributional hypothesis (Harris, 1968): “Words used in similar contexts have similar meanings”. We will now see how to extend this idea to cross-lingual contexts. 3.2 Trans-gram In this section we introduce Trans-gram, a new method to compute aligned word-embeddings for a variety of languages. Our method will minimize the summation of mono-lingual losses and cross-lingual losses. Like in BilBOWA (Gouws et al., 2015), we use Skipgram as a mono-lingual loss. Assuming we are trying to learn aligned word vectors for languages</context>
<context position="11339" citStr="Mikolov et al., 2013" startWordPosition="1839" endWordPosition="1842">of the art results were detained (Gouws et al., 2015) with BilBOWA and (Lauly et al., 2014) with their Bilingual Auto-encoder model. This model learns word embeddings during a translation task that uses an encoder-decoder approach. We also report the scores from Klementiev et al. who introduced the task and the BiCVM model scores from (Hermann and Blunsom, 2013). The results show an overall significant improvement over the other methods, with the added advantage of being computationally efficient. 5.2 P@k Word Translation Next we evaluated our method on a word translation task, introduced in (Mikolov et al., 2013b) and used in (Gouws et al., 2015). The words were extracted from the publicly available WMT112 corpus. The experiments were done for two sets of translation: English to Spanish and Spanish to English. (Mikolov et al., 2013b) extracted the top 6K most frequent words and translated them with Google Translate. They used the top 5K pairs to train a translation matrix, and evaluated their method on the remaining 1K. As our English and 1Thanks to S. Gouws for providing this implementation 2http://www.statmt.org/wmt11/ 1111 Method En --+ De De --+ En Speed-up in training time Klementiev et al. 77.6</context>
</contexts>
<marker>Mikolov, Le, Sutskever, 2013</marker>
<rawString>Tomas Mikolov, Quoc V Le, and Ilya Sutskever. 2013b. Exploiting similarities among languages for machine translation. arXiv preprint arXiv:1309.4168.</rawString>
</citation>
</citationList>
</algorithm>
</algorithms>
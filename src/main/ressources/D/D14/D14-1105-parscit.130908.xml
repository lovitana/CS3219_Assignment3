<?xml version="1.0" encoding="UTF-8"?>
<algorithms version="110505">
<algorithm name="SectLabel" version="110505">
<variant no="0" confidence="0.059482">
<title confidence="0.965271">
POS Tagging of English-Hindi Code-Mixed Social Media Content
</title>
<author confidence="0.998196">
Yogarshi Vyas* Spandana Gella*
</author>
<affiliation confidence="0.998835">
University of Maryland Xerox Research Centre Europe
</affiliation>
<email confidence="0.992444">
yogarshi@cs.umd.edu spandanagella@gmail.com
</email>
<author confidence="0.986306">
Jatin Sharma Kalika Bali Monojit Choudhury
</author>
<affiliation confidence="0.984328">
Microsoft Research India
</affiliation>
<email confidence="0.998759">
{jatin.sharma,kalikab,monojitc}@microsoft.com
</email>
<sectionHeader confidence="0.997387" genericHeader="abstract">
Abstract
</sectionHeader>
<bodyText confidence="0.999677176470589">
Code-mixing is frequently observed in
user generated content on social media,
especially from multilingual users. The
linguistic complexity of such content is
compounded by presence of spelling vari-
ations, transliteration and non-adherance
to formal grammar. We describe our
initial efforts to create a multi-level an-
notated corpus of Hindi-English code-
mixed text collated from Facebook fo-
rums, and explore language identifica-
tion, back-transliteration, normalization
and POS tagging of this data. Our re-
sults show that language identification and
transliteration for Hindi are two major
challenges that impact POS tagging accu-
racy.
</bodyText>
<sectionHeader confidence="0.999514" genericHeader="introduction">
1 Introduction
</sectionHeader>
<bodyText confidence="0.985031540983606">
Code-Switching and Code-Mixing are typical
and well-studied phenomena of multilingual so-
cieties (Gumperz, 1964; Auer, 1984; Myers-
Scotton, 1993; Danet and Herring, 2007;
Cardenas-Claros and Isharyanti, 2009). Lin-
guists differentiate between the two, where
Code-Switching is juxtaposition within the same
speech exchange of passages of speech be-
longing to two different grammatical systems
or sub-systems (Gumperz, 1982), and Code-
Mixing (CM) refers to the embedding of linguis-
tic units such as phrases, words and morphemes
of one language into an utterance of another lan-
guage (Myers-Scotton, 1993). The first exam-
ple in Fig. 1 features CM where English words
are embedded in a Hindi sentence, whereas the
second example shows codeswitching. Here, we
will use CM to imply both. Work on computa-
∗This work was done during authors’ internship at Mi-
crosoft Research India.
tional models of CM have been few and far be-
tween (Solorio and Liu, 2008a; Solorio and Liu,
2008b; Nguyen and Dogruoz, 2013), primarily
due to the paucity of CM data in conventional
text-corpora which makes data-intensive methods
hard to apply. Solorio and Liu (2008a) in their
work on English-Spanish CM use models built on
smaller datasets to predict valid switching points
to synthetically generate data from monolingual
corpora, and in another work (2008b) describe
parts-of-speech (POS) tagging of CM text.
CM though typically observed in spoken lan-
guage is now increasingly more common in text,
thanks to the proliferation of the Computer Me-
diated Communication channels, especially so-
cial media like Twitter and Facebook (Crys-
tal, 2001; Herring, 2003; Danet and Herring,
2007; Cardenas-Claros and Isharyanti, 2009).
Social media content is tremendously important
for studying trends, reviews, events, human-
behaviour as well as linguistic analysis, and there-
fore in recent times has spurred a lot of interest
in automatic processing of such data. Neverthe-
less, CM on social media has not been studied
from a computational aspect. Moreover, social
media content presents additional challenges due
to contractions, non-standard spellings and non-
grammatical constructions. Furthermore, for lan-
guages written in scripts other than Roman, like
Hindi, Bangla, Japanese, Chinese and Arabic, Ro-
man transliterations are typically used for repre-
senting the words (Sowmya et al., 2010). This can
prove a challenge for language identification and
segregation of the two languages.
In this paper, we describe our initial efforts to
POS tag social media content from English-Hindi
(henceforth En-Hi) bilinguals while trying to ad-
dress the challenges of CM, transliteration and
non-standard spelling, as well as lack of anno-
tated data. POS tagging is one of the fundamen-
tal pre-processing steps for NLP, and while there
</bodyText>
<page confidence="0.975268">
974
</page>
<bodyText confidence="0.919659307692308">
Proceedings of the 2014 Conference on Empirical Methods in Natural Language Processing (EMNLP), pages 974–979,
October 25-29, 2014, Doha, Qatar. c�2014 Association for Computational Linguistics
have been works on POS tagging of social media
data (Gimpel et al., 2011; Owoputi et al., 2013)
and of CM (Solorio and Liu, 2008b), but we do
not know of any work on POS tagging of CM
text from social media that involves transliteration.
The salient contributions of this work are in for-
malizing the problem and related challenges for
processing of En-Hi social media data, creation
of an annotated dataset and some initial experi-
ments for language identification, transliteration,
normalization and POS tagging of this data.
</bodyText>
<sectionHeader confidence="0.963986" genericHeader="method">
2 Corpus Creation
</sectionHeader>
<bodyText confidence="0.999905647058823">
For this study, we collected data from Face-
book public pages of three celebrities: Amitabh
Bachchan, Shahrukh Khan, Narendra Modi, and
the BBC Hindi news page. All these pages are
very popular with 1.8 to 15.5 million “likes”. A to-
tal of 40 posts were manually selected from these
pages, which were published between 22nd – 28th
October 2013. The posts having a long thread of
comments (50+) were preferred, because CM and
non-standard usage of language is more common
in the comments. We shall use the term post to re-
fer to either a post or a comment. The corpus thus
created has 6,983 posts and 113,578 words. The
data was semi-automatically cleaned and format-
ted. The user names were removed for anonymity,
but the names appearing in comments, which are
mostly of celebrities, were retained.
</bodyText>
<subsectionHeader confidence="0.985268">
2.1 Annotation
</subsectionHeader>
<bodyText confidence="0.999859611111111">
There are various interesting linguistic as well as
socio-pragmatic features (e.g., user demograph-
ics, presence of sarcasm or humor, polarity) for
which this corpus could be annotated because CM
is influenced by both linguistic as well as extra-
linguistic features. However, initial attempts at
such detailed and layered annotation soon revealed
the resource-intensiveness of the task. We, thus,
scaled down the annotation to the following four
layers:
Matrix: The posts are split into contiguous
fragments of words such that each fragment has
a unique matrix language (either En or Hi). The
matrix language is defined as the language which
governs the grammatical relation between the con-
stituents of the utterance. Any other language
words that are nested into the matrix constitute the
embedded language(s). Usually, matrix language
can be assigned to clauses or sentences.
Word origin: Every word is marked for its ori-
gin or source language, En or Hi, depending on
whether it is an English or Hindi word. Words that
are of neither Hindi nor English origin are marked
as Ot or Other. Here, we assume that code-mixing
does not happen at sublexical levels, as it is un-
common in this data; Hi and En have a sim-
pler inflectional morphology and thus, sub-lexical
mixing though present (e.g., computeron has
a En root - computer and a Hi plural marker
on) is relatively less common. In languages with
richer morphology and agglutination, like Bangla
and most Dravidian languages, more frequent sub-
lexical mixing may be observed. Also note that
words are borrowed extensively between Hi and
En such that certain English words (e.g., bus,
party, vote etc) are no longer perceived as English
words by the Hindi speakers. However, here we
will not distinguish between CM and borrowing,
and such borrowed English words have also been
labeled as En words.
Normalization/Transliteration: Whenever the
word is in a transliterated form, which is often the
case for the Hi words, it is labeled with the in-
tended word in the native script (e.g., Devanagari
for Hi). If the word is in native script, but uses
a non-standard spelling, it is labeled with the cor-
rect standard spelling. We call this the spelling
normalization layer.
Parts-of-Speech (POS): Finally, each word is
also labeled with its POS. We use the Universal
POS tagset proposed by Petrov et al. (2011) which
has 12 POS tags that are applicable to both En
and Hi. The POS labels are decided based on the
function of a word in the context, rather than a
decontextualized lexical category. This is an im-
portant notion, especially for CM text, because of-
ten the original lexical category of an embedded
word is lost in the context of the matrix language,
and it plays the role of a different lexical category.
Though the Universal POS tagset does not pre-
scribe a separate tag for Named Entities, we felt
the necessity of marking three different kinds of
NEs - people, location and organization, because
almost every comment has one or more NEs and
strictly speaking word origin does not make sense
for these words.
Annotation Scheme: Fig. 1 illustrates the an-
notation scheme through two examples. Each
post is enclosed within &lt;s&gt;&lt;/s&gt; tags. The
matrices within a post are separated by the
&lt;matrix&gt;&lt;/matrix&gt; tags which take the matrix
language as an argument. Each word is anno-
</bodyText>
<page confidence="0.99762">
975
</page>
<figureCaption confidence="0.999862">
Figure 1: Two example annotations.
</figureCaption>
<bodyText confidence="0.999756590909091">
tated for POS, and the language (/E or /H for En
or Hi respectively) only if it is different from the
language of the matrix. In case of non-standard
spelling in English, the correct spelling is ap-
pended as “sol NOUN=soul”, while for the
Hindi words, the correct Devanagari translitera-
tion is appended. The NEs are marked with the
tags P (person), L (location) or O (organization)
and multiword NEs are enclosed within square
brackets “[]”.
A random subsample of 1062 posts consisting
of 10171 words were annotated by a linguist who
is a native speaker of Hi and proficient in En. The
annotations were reviewed and corrected by two
experts linguists. During this phase, it was also
observed that a large number of comments were
very short, typically an eulogism of their favorite
celebrity and hence were not interesting from a
linguistic point of view. For our experiments, we
removed all posts that had fewer than 5 words.
The resulting corpus had 381 comments/posts and
4135 words.
</bodyText>
<subsectionHeader confidence="0.988096">
2.2 CM Distribution
</subsectionHeader>
<bodyText confidence="0.999974097560976">
Most of the posts (93.17%) are in Roman script,
and only 2.93% were in Devanagari. Around 3.5%
of the posts contain words in both the scripts (typ-
ically a post in Devanagari with hashtags or urls in
Roman script), and a very small fraction of the text
(0.4% of comments/posts and 0.6% words) was in
some other script. The fraction of words present
in Roman and Devanagri scripts are 80.76% and
15.32% respectively, which shows that the De-
vanagari posts are relatively longer than the Ro-
man posts. Due to their relative rarity, the posts
containing words in Devanagari or any other script
were not considered for annotation.
In the annotated data, 1102 sentences are in a
single matrix (398 Hi, 698 En and 6 Ot) and in
45 posts there is at least one switch of matrix
(mostly between Hi and En. Thus, 4.2% of the
data shows code-switching. This is a strict defi-
nition of code-switching; if we consider a change
in matrix within a conversation thread as a code-
switch, then in this data all the threads exhibit
code-switching. However, out of the 398 com-
ments in Hi-matrix, 23.37% feature CM (i.e., they
have at least one or more non-Hi (or rather, al-
most always En) words embedded. On the other
hand, only 7.34% En-matrix comments feature
CM (again almost always with Hi). Thus, a total
of 17.2% comments/posts, which contains a quar-
ter of all the words in the annotated corpus, fea-
ture either CM or code-switching or both. We also
note that more than 40% words in the corpus are
in Hi or other Indian languages, but written in Ro-
man script; hence, they are in transliterated form.
See (Bali et al., 2014) for an in-depth discussion
on the characteristics of the CM data.
This analysis demonstrates the necessity of CM
and transliterated text processing in the context of
Indian user-generated social media content. Per-
haps, the numbers are not too different for such
content generated by the users of any other bilin-
gual and multilingual societies.
</bodyText>
<sectionHeader confidence="0.992046" genericHeader="method">
3 Models and Experiments
</sectionHeader>
<bodyText confidence="0.999548">
POS tagging of En-Hi code-mixed data requires
language identification at both word and matrix
level as well back-transliteration of the text into
</bodyText>
<page confidence="0.989161">
976
</page>
<table confidence="0.9999136">
Actual Predicted Label Recall
Label Hi En
Hi 1057 515 0.672
En 45 2023 0.978
Precision 0.959 0.797
</table>
<tableCaption confidence="0.99967">
Table 1: Confusion matrix, precision and recall of
</tableCaption>
<bodyText confidence="0.976291789473684">
the language identification module.
the native script. Additionaly, since we are work-
ing with content from social media, the usage of
non-standard spelling is rampant and thus, nor-
malization of text into some standard form is re-
quired. Ideally, these tasks should be performed
jointly since they are interdependent. However,
due to lack of resources, we implement a pipelined
approach in which the tasks - language identifica-
tion, text normalization and POS tagging - are per-
formed sequentially, in that order. This pipelined
approach also allows us to use various off-the-
shelf tools for solving these subtasks and quickly
create a baseline system. The baseline results can
also provide useful insight into the inherent hard-
ness of POS tagging of code-mixed social media
text. In this section, we first describe our approach
to solve these three tasks, and then discuss the ex-
periments and results.
</bodyText>
<subsectionHeader confidence="0.99804">
3.1 Language identification
</subsectionHeader>
<bodyText confidence="0.999962333333334">
Langauge identification is a well studied prob-
lem (King and Abney, 2013; Carter et al., 2013;
Goldszmidt et al., 2013; Nguyen and Dogruoz,
2013), though for CM text, especially those in-
volving transliterations and orthographic varia-
tion, this is far from a solved problem (Nguyen and
Dogruoz, 2013). There was a shared task in FIRE
2013 (Saha Roy et al., 2013) on language iden-
tification and back transliteration for En mixed
with Hi, Bangla and Gujarati. Along the lines
of Gella et al (Gella et al., 2013), which was the
best performing system in this shared task, we
used the word-level logistic regression classifier
built by King and Abney (2013). This system pro-
vides a source language with a confidence prob-
ability for each word in the test set. We trained
the classifier on 3201 English words extracted
from the SMS corpus developed by Choudhury
et al (2007), while the Hindi data was obtained
by sampling 3218 Hindi transliterations out of the
En-Hi transliteration pairs developed by Sowmya
et al. (Sowmya et al., 2010). Ideally, the context of
a token is important for identifying the language.
Again, following (Gella et al., 2013) we incorpo-
rate context information through a code-switching
probability, P3. A higher value of P3 implies a
lower probability of code-switching, i.e., adjacent
words are more likely to be in the same language.
Table 1 shows the token (word) level confusion
matrix for the language identification task on our
dataset. The language labels of 84.6% of the to-
kens were correctly predicted by the system. As
can be seen from the Table, the precision for pre-
dicting Hi is high, whereas that for En is low. This
is mainly due to the presence of a large number of
contracted and distorted Hi words in the dataset,
e.g. h for hai (Fig. 1), which were tagged as
En by our system because the training examples
had no contracted Hi words, but short and non-
conventional spellings were in plenty in the En
training examples as those were extracted from the
SMS corpus.
</bodyText>
<subsectionHeader confidence="0.988939">
3.2 Normalization
</subsectionHeader>
<bodyText confidence="0.999967285714286">
In our dataset, if a word is identified as Hi, then
it must be back-transliterated to Devanagari script
so that any off-the-shelf Hindi POS tagger can be
used. We used the system by Gella et al. (Gella
et al., 2013) for this task, which is part rule-based
and part statistical. The system was trained on the
35000 unique transliteration pairs extracted from
Hindi song lyrics (Gupta et al., 2012). This corpus
has a reasonably wide coverage of Hindi words,
and past researchers have also shown that translit-
eration does not require a very large amount of
training data. Normalization of the En text was
not needed because the POS tagger (Owoputi et
al., 2013) could handle unnormalized text.
</bodyText>
<subsectionHeader confidence="0.997898">
3.3 POS tagging
</subsectionHeader>
<bodyText confidence="0.999932133333333">
Solorio and Liu (2008b) describes a few ap-
proaches to POS-tagging of code-switched Span-
glish text, all of which primarily relies on two
monolingual taggers and certain heuristics to com-
bine the output from the two. One of the sim-
pler heuristics is based on language identification,
where the POS tag of a word is the output of the
monolingual tagger of the language in which the
word is. In this initial study, we apply this ba-
sic idea for POS tagging of CM data. We divide
the text (which is already sentence-separated) into
contiguous maximal chunks of words which are in
the same language. Then we apply a Hi POS tag-
ger to the Hi chunks, and an En POS tagger to the
En chunks.
</bodyText>
<page confidence="0.992918">
977
</page>
<table confidence="0.9978222">
Model LI HN Tagger Hi Acc. En Acc. Total Acc. Hi CA En CA Total CA
1a K K Standard 75.14 81.91 79.02 27.34 39.67 34.05
1b K K Twitter 75.14 82.66 79.02 27.34 35.74 31.91
2 K NK Twitter 65.61 81.73 74.87 17.58 33.77 26.38
3 NK NK Twitter 44.74 80.68 65.39 40.00 13.17 25.00
</table>
<tableCaption confidence="0.85016">
Table 2: POS Tagging accuracies for the different models. K=Known, NK = Not Known. LI = Language
labels, HN = Hindi normalized forms, Acc. = Token level accuracy, CA = Chunk level accuracy.
</tableCaption>
<bodyText confidence="0.99993">
We use a CRF++ based POS tagger for Hi,
which is freely available from http://nltr.
org/snltr-software/. For En, we use the
Twitter POS tagger (Owoputi et al., 2013). It
also has an inbuilt tokenizer and can work di-
rectly on unnormalized text. This tagger has been
chosen because Facebook posts and comments
are more Twitter-like. We also use the Stanford
POS Tagger (Toutanova et al., 2003) which, un-
like the Twitter POS Tagger, has not been tuned
for Twitter-like text. These taggers use different
tagsets - the ILPOST for Hi (Sankaran et al., 2008)
and Penn-TreeBank for En (Marcus et al., 1993).
The output tags are appropriately mapped to the
smaller Universal tagset (Petrov et al., 2011).
</bodyText>
<subsectionHeader confidence="0.969745">
3.4 Experiments and Results
</subsectionHeader>
<bodyText confidence="0.999956816326531">
We conducted three different experiments as fol-
lows. In the first experiment, we assume that
we know the language identities and normal-
ized/transliterated forms of the words, and only do
the POS tagging. This experiment gives us an idea
of the accuracy of POS tagging task, if normal-
ization, transliteration and language identification
could be done perfectly. We conduct this exper-
iments with two different En POS taggers: the
Stanford POS tagger which is trained on formal
English text (Model 1a) and the Twitter POS tag-
ger (Model 1b). In the next experiment (Model
2), we assume that only the language identity of
the words are known, but for Hindi we apply our
model to generate the back transliterations. For
English, we apply the Twitter POS tagger directly
because it can handle unnormalized social media
text. The third experiment (Model 3) assumes that
nothing is known. So language identifier is first
applied, and based on the language detected, we
apply the Hi translitertaion module, and Hi POS
tagger, or the En tagger. This is the most chal-
lenging and realistic setting. Note that the matrix
information is not used in any of our experiments,
though it could be potentially useful for POS tag-
ging and could be explored in future.
Table 2 gives a summary of the four models
along with the POS tagging accuracies (in %). It
shows token level as well as chunk leve accuracies
(CA), i.e., what percentage of chunks have been
correctly POS tagged. As can be seen, Hi POS
tagging has relatively low accuracies than En POS
tagging at word level for all cases. This is primar-
ily due to the errors of the transliteration module,
which in turn, is because the transliteration does
not address spelling contractions. This is also re-
flected in the drop in the accuracies for the case
where LI is unknown. The very low CA for En
for model 3 is primarily because some of the Hi
chunks are incorrectly identified as En by the lan-
guage identification module (see Table 1). How-
ever, the gradual drop of token and chunk level
accuracies from model 1 to model 3 clearly shows
the effect of gradual error accumulation from each
of the modules. We observe that Nouns were
usually confused most with Verbs and vice versa,
while the Adj were mostly confused with Nouns,
Pronouns with Determiners, and Adpositions with
Conjunctions.
</bodyText>
<sectionHeader confidence="0.999612" genericHeader="conclusions">
4 Conclusion
</sectionHeader>
<bodyText confidence="0.999942823529412">
This is a work in progress. We have identified
normalization and transliteration as two very chal-
lenging problems for En-Hi CM text. Joint mod-
elling of language identification, normalization,
transliteration as well as POS tagging is expected
to yield better results. We plan to continue our
work in that direction, specifically for conversa-
tional text in social media in a multilingual con-
text. CM is a common phenomenon found in all
bilingual and multilingual societies. The issue of
transliteration exist for most of the South Asian
languages as well as many other languages such as
Arabic and Greek, which use a non-Roman based
script (Gupta et al., 2014). The challenges and is-
sues identified in this study are likely to hold for
many other languages as well, which makes this a
very important and globally prevalent problem.
</bodyText>
<page confidence="0.997493">
978
</page>
<sectionHeader confidence="0.996335" genericHeader="references">
References
</sectionHeader>
<reference confidence="0.999678130841121">
Peter Auer. 1984. The Pragmatics of Code-Switching:
A Sequential Approach. Cambridge University
Press.
Kalika Bali, Yogarshi Vyas, Jatin Sharma, and Monojit
Choudhury. 2014. ‘’i am borrowing ya mixing?” an
analysis of English-Hindi code mixing in Facebook.
In Proceedings of the First Workshop on Computa-
tional Approaches to Code Switching, EMNLP.
M´onica Stella Cardenas-Claros and Neny Isharyanti.
2009. Code-switching and code-mixing in internet
chatting: Between yes, ya, and si a case study. In
The JALT CALL Journal, 5.
Simon Carter, Wouter Weerkamp, and Manos
Tsagkias. 2013. Microblog language identification:
Overcoming the limitations of short, unedited and
idiomatic text. Language Resources and Evaluation
Journal, 47:195–215.
Monojit Choudhury, Rahul Saraf, Vijit Jain, Animesh
Mukherjee, Sudeshna Sarkar, and Anupam Basu.
2007. Investigation and modeling of the structure
of texting language. IJDAR, 10(3-4):157–174.
David Crystal. 2001. Language and the Internet.
Cambridge University Press.
Brenda Danet and Susan Herring. 2007. The Multilin-
gual Internet: Language, Culture, and Communica-
tion Online. Oxford University Press., New York.
Spandana Gella, Jatin Sharma, and Kalika Bali. 2013.
Query word labeling and back transliteration for in-
dian languages: Shared task system description. In
FIRE Working Notes.
Kevin Gimpel, N. Schneider, B. O’Connor, D. Das,
D. Mills, J. Eisenstein, M. Heilman, D. Yogatama,
J. Flanigan, and N. A. Smith. 2011. Part-of-speech
tagging for twitter: Annotation, features, and exper-
iments. In Proceedings of ACL.
Moises Goldszmidt, Marc Najork, and Stelios Papari-
zos. 2013. Boot-strapping language identifiers for
short colloquial postings. In Machine Learning and
Knowledge Discovery in Databases, volume 8189 of
Lecture Notes in Computer Science, pages 95–111.
John J. Gumperz. 1964. Hindi-punjabi code-switching
in Delhi. In Proceedings of the Ninth International
Congress of Linguistics. Mouton:The Hague.
John J. Gumperz. 1982. Discourse Strategies. Oxford
University Press.
Kanika Gupta, Monojit Choudhury, and Kalika Bali.
2012. Mining Hindi-English transliteration pairs
from online Hindi lyrics. In Proceedings of LREC.
Parth Gupta, Kalika Bali, Rafael E. Banchs, Monojit
Choudhury, and Paolo Rosso. 2014. Query ex-
pansion for mixed-script information retrieval. In
Proc. of SIGIR, pages 677–686. ACM Association
for Computing Machinery.
Susan Herring, editor. 2003. Media and Language
Change. Special issue of Journal of Historical Prag-
matics 4:1.
Ben King and Steven Abney. 2013. Labeling the lan-
guages of words in mixed-language documents us-
ing weakly supervised methods. In Proceedings of
NAACL-HLT, pages 1110–1119.
Mitchell P Marcus, Mary Ann Marcinkiewicz, and
Beatrice Santorini. 1993. Building a large anno-
tated corpus of english: The penn treebank. Compu-
tational linguistics, 19(2):313–330.
Carol Myers-Scotton. 1993. Dueling Languages:
Grammatical Structure in Code-Switching. Clare-
don, Oxford.
Dong Nguyen and A. Seza Dogruoz. 2013. Word
level language identification in online multilingual
communication. In Proceedings of the 2013 Con-
ference on Empirical Methods in Natural Language
Processing, pages 857–862.
Olutobi Owoputi, Brendan OConnor, Chris Dyer,
Kevin Gimpel, Nathan Schneider, and Noah A.
Smith. 2013. Improved part-of-speech tagging for
online conversational text with word clusters. In
Proceedings of NAACL.
Slav Petrov, Dipanjan Das, and Ryan McDonald. 2011.
A universal part-of-speech tagset. arXiv preprint
arXiv:1104.2086.
Rishiraj Saha Roy, Monojit Choudhury, Prasenjit Ma-
jumder, and Komal Agarwal. 2013. Overview and
datasets of fire 2013 track on transliterated search.
In FIRE Working Notes.
Bhaskaran Sankaran, Kalika Bali, Monojit Choudhury,
Tanmoy Bhattacharya, Pushpak Bhattacharyya,
Girish Nath Jha, S. Rajendran, K. Saravanan,
L. Sobha, and K. V. Subbarao. 2008. A com-
mon parts-of-speech tagset framework for indian
languages. In Proceedings of LREC.
Thamar Solorio and Yang Liu. 2008a. Learning to
predict code-switching points. In Proceedings of the
Empirical Methods in natural Language Processing.
Thamar Solorio and Yang Liu. 2008b. Parts-of-speech
tagging for English-Spanish code-switched text. In
Proceedings of the Empirical Methods in natural
Language Processing.
V. B. Sowmya, Monojit Choudhury, Kalika Bali,
Tirthankar Dasgupta, and Anupam Basu. 2010. Re-
source creation for training and testing of translitera-
tion systems for indian languages. In Proceedings of
the Language Resource and Evaluation Conference
(LREC).
Kristina Toutanova, Dan Klein, Christopher Manning,
and Yoram Singer. 2003. Feature-rich part-of-
speech tagging with a cyclic dependency network.
In Proceedings of HLT-NAACL.
</reference>
<page confidence="0.998933">
979
</page>
</variant>
</algorithm>
<algorithm name="ParsHed" version="110505">
<variant no="0" confidence="0.370565">
<affiliation confidence="0.750154">POS Tagging of English-Hindi Code-Mixed Social Media Content University of Maryland Xerox Research Centre Europe</affiliation>
<email confidence="0.987694">yogarshi@cs.umd.eduspandanagella@gmail.com</email>
<author confidence="0.937507">Jatin Sharma Kalika Bali Monojit</author>
<affiliation confidence="0.995472">Microsoft Research India</affiliation>
<abstract confidence="0.987661722222222">Code-mixing is frequently observed in user generated content on social media, especially from multilingual users. The linguistic complexity of such content is compounded by presence of spelling variations, transliteration and non-adherance to formal grammar. We describe our initial efforts to create a multi-level annotated corpus of Hindi-English codemixed text collated from Facebook forums, and explore language identification, back-transliteration, normalization and POS tagging of this data. Our results show that language identification and transliteration for Hindi are two major challenges that impact POS tagging accuracy.</abstract>
</variant>
</algorithm>
<algorithm name="ParsCit" version="110505">
<citationList>
<citation valid="true">
<authors>
<author>Peter Auer</author>
</authors>
<title>The Pragmatics of Code-Switching: A Sequential Approach.</title>
<date>1984</date>
<publisher>Cambridge University Press.</publisher>
<contexts>
<context position="1081" citStr="Auer, 1984" startWordPosition="139" endWordPosition="140">ompounded by presence of spelling variations, transliteration and non-adherance to formal grammar. We describe our initial efforts to create a multi-level annotated corpus of Hindi-English codemixed text collated from Facebook forums, and explore language identification, back-transliteration, normalization and POS tagging of this data. Our results show that language identification and transliteration for Hindi are two major challenges that impact POS tagging accuracy. 1 Introduction Code-Switching and Code-Mixing are typical and well-studied phenomena of multilingual societies (Gumperz, 1964; Auer, 1984; MyersScotton, 1993; Danet and Herring, 2007; Cardenas-Claros and Isharyanti, 2009). Linguists differentiate between the two, where Code-Switching is juxtaposition within the same speech exchange of passages of speech belonging to two different grammatical systems or sub-systems (Gumperz, 1982), and CodeMixing (CM) refers to the embedding of linguistic units such as phrases, words and morphemes of one language into an utterance of another language (Myers-Scotton, 1993). The first example in Fig. 1 features CM where English words are embedded in a Hindi sentence, whereas the second example sho</context>
</contexts>
<marker>Auer, 1984</marker>
<rawString>Peter Auer. 1984. The Pragmatics of Code-Switching: A Sequential Approach. Cambridge University Press.</rawString>
</citation>
<citation valid="true">
<authors>
<author>Kalika Bali</author>
<author>Yogarshi Vyas</author>
<author>Jatin Sharma</author>
<author>Monojit Choudhury</author>
</authors>
<title>i am borrowing ya mixing?” an analysis of English-Hindi code mixing in Facebook.</title>
<date>2014</date>
<booktitle>In Proceedings of the First Workshop on Computational Approaches to Code Switching, EMNLP.</booktitle>
<contexts>
<context position="11256" citStr="Bali et al., 2014" startWordPosition="1826" endWordPosition="1829">ll the threads exhibit code-switching. However, out of the 398 comments in Hi-matrix, 23.37% feature CM (i.e., they have at least one or more non-Hi (or rather, almost always En) words embedded. On the other hand, only 7.34% En-matrix comments feature CM (again almost always with Hi). Thus, a total of 17.2% comments/posts, which contains a quarter of all the words in the annotated corpus, feature either CM or code-switching or both. We also note that more than 40% words in the corpus are in Hi or other Indian languages, but written in Roman script; hence, they are in transliterated form. See (Bali et al., 2014) for an in-depth discussion on the characteristics of the CM data. This analysis demonstrates the necessity of CM and transliterated text processing in the context of Indian user-generated social media content. Perhaps, the numbers are not too different for such content generated by the users of any other bilingual and multilingual societies. 3 Models and Experiments POS tagging of En-Hi code-mixed data requires language identification at both word and matrix level as well back-transliteration of the text into 976 Actual Predicted Label Recall Label Hi En Hi 1057 515 0.672 En 45 2023 0.978 Pre</context>
</contexts>
<marker>Bali, Vyas, Sharma, Choudhury, 2014</marker>
<rawString>Kalika Bali, Yogarshi Vyas, Jatin Sharma, and Monojit Choudhury. 2014. ‘’i am borrowing ya mixing?” an analysis of English-Hindi code mixing in Facebook. In Proceedings of the First Workshop on Computational Approaches to Code Switching, EMNLP.</rawString>
</citation>
<citation valid="true">
<authors>
<author>M´onica Stella Cardenas-Claros</author>
<author>Neny Isharyanti</author>
</authors>
<title>Code-switching and code-mixing in internet chatting: Between yes, ya, and si a case study.</title>
<date>2009</date>
<journal>In The JALT CALL Journal,</journal>
<volume>5</volume>
<contexts>
<context position="1165" citStr="Cardenas-Claros and Isharyanti, 2009" startWordPosition="148" endWordPosition="151">tion and non-adherance to formal grammar. We describe our initial efforts to create a multi-level annotated corpus of Hindi-English codemixed text collated from Facebook forums, and explore language identification, back-transliteration, normalization and POS tagging of this data. Our results show that language identification and transliteration for Hindi are two major challenges that impact POS tagging accuracy. 1 Introduction Code-Switching and Code-Mixing are typical and well-studied phenomena of multilingual societies (Gumperz, 1964; Auer, 1984; MyersScotton, 1993; Danet and Herring, 2007; Cardenas-Claros and Isharyanti, 2009). Linguists differentiate between the two, where Code-Switching is juxtaposition within the same speech exchange of passages of speech belonging to two different grammatical systems or sub-systems (Gumperz, 1982), and CodeMixing (CM) refers to the embedding of linguistic units such as phrases, words and morphemes of one language into an utterance of another language (Myers-Scotton, 1993). The first example in Fig. 1 features CM where English words are embedded in a Hindi sentence, whereas the second example shows codeswitching. Here, we will use CM to imply both. Work on computa∗This work was </context>
<context position="2647" citStr="Cardenas-Claros and Isharyanti, 2009" startWordPosition="383" endWordPosition="386">xt-corpora which makes data-intensive methods hard to apply. Solorio and Liu (2008a) in their work on English-Spanish CM use models built on smaller datasets to predict valid switching points to synthetically generate data from monolingual corpora, and in another work (2008b) describe parts-of-speech (POS) tagging of CM text. CM though typically observed in spoken language is now increasingly more common in text, thanks to the proliferation of the Computer Mediated Communication channels, especially social media like Twitter and Facebook (Crystal, 2001; Herring, 2003; Danet and Herring, 2007; Cardenas-Claros and Isharyanti, 2009). Social media content is tremendously important for studying trends, reviews, events, humanbehaviour as well as linguistic analysis, and therefore in recent times has spurred a lot of interest in automatic processing of such data. Nevertheless, CM on social media has not been studied from a computational aspect. Moreover, social media content presents additional challenges due to contractions, non-standard spellings and nongrammatical constructions. Furthermore, for languages written in scripts other than Roman, like Hindi, Bangla, Japanese, Chinese and Arabic, Roman transliterations are typi</context>
</contexts>
<marker>Cardenas-Claros, Isharyanti, 2009</marker>
<rawString>M´onica Stella Cardenas-Claros and Neny Isharyanti. 2009. Code-switching and code-mixing in internet chatting: Between yes, ya, and si a case study. In The JALT CALL Journal, 5.</rawString>
</citation>
<citation valid="true">
<authors>
<author>Simon Carter</author>
<author>Wouter Weerkamp</author>
<author>Manos Tsagkias</author>
</authors>
<title>Microblog language identification: Overcoming the limitations of short, unedited and idiomatic text.</title>
<date>2013</date>
<journal>Language Resources and Evaluation Journal,</journal>
<pages>47--195</pages>
<contexts>
<context position="12945" citStr="Carter et al., 2013" startWordPosition="2097" endWordPosition="2100">sks - language identification, text normalization and POS tagging - are performed sequentially, in that order. This pipelined approach also allows us to use various off-theshelf tools for solving these subtasks and quickly create a baseline system. The baseline results can also provide useful insight into the inherent hardness of POS tagging of code-mixed social media text. In this section, we first describe our approach to solve these three tasks, and then discuss the experiments and results. 3.1 Language identification Langauge identification is a well studied problem (King and Abney, 2013; Carter et al., 2013; Goldszmidt et al., 2013; Nguyen and Dogruoz, 2013), though for CM text, especially those involving transliterations and orthographic variation, this is far from a solved problem (Nguyen and Dogruoz, 2013). There was a shared task in FIRE 2013 (Saha Roy et al., 2013) on language identification and back transliteration for En mixed with Hi, Bangla and Gujarati. Along the lines of Gella et al (Gella et al., 2013), which was the best performing system in this shared task, we used the word-level logistic regression classifier built by King and Abney (2013). This system provides a source language </context>
</contexts>
<marker>Carter, Weerkamp, Tsagkias, 2013</marker>
<rawString>Simon Carter, Wouter Weerkamp, and Manos Tsagkias. 2013. Microblog language identification: Overcoming the limitations of short, unedited and idiomatic text. Language Resources and Evaluation Journal, 47:195–215.</rawString>
</citation>
<citation valid="true">
<authors>
<author>Monojit Choudhury</author>
<author>Rahul Saraf</author>
<author>Vijit Jain</author>
<author>Animesh Mukherjee</author>
<author>Sudeshna Sarkar</author>
<author>Anupam Basu</author>
</authors>
<title>Investigation and modeling of the structure of texting language.</title>
<date>2007</date>
<journal>IJDAR,</journal>
<pages>10--3</pages>
<contexts>
<context position="13719" citStr="Choudhury et al (2007)" startWordPosition="2229" endWordPosition="2232">s far from a solved problem (Nguyen and Dogruoz, 2013). There was a shared task in FIRE 2013 (Saha Roy et al., 2013) on language identification and back transliteration for En mixed with Hi, Bangla and Gujarati. Along the lines of Gella et al (Gella et al., 2013), which was the best performing system in this shared task, we used the word-level logistic regression classifier built by King and Abney (2013). This system provides a source language with a confidence probability for each word in the test set. We trained the classifier on 3201 English words extracted from the SMS corpus developed by Choudhury et al (2007), while the Hindi data was obtained by sampling 3218 Hindi transliterations out of the En-Hi transliteration pairs developed by Sowmya et al. (Sowmya et al., 2010). Ideally, the context of a token is important for identifying the language. Again, following (Gella et al., 2013) we incorporate context information through a code-switching probability, P3. A higher value of P3 implies a lower probability of code-switching, i.e., adjacent words are more likely to be in the same language. Table 1 shows the token (word) level confusion matrix for the language identification task on our dataset. The l</context>
</contexts>
<marker>Choudhury, Saraf, Jain, Mukherjee, Sarkar, Basu, 2007</marker>
<rawString>Monojit Choudhury, Rahul Saraf, Vijit Jain, Animesh Mukherjee, Sudeshna Sarkar, and Anupam Basu. 2007. Investigation and modeling of the structure of texting language. IJDAR, 10(3-4):157–174.</rawString>
</citation>
<citation valid="true">
<authors>
<author>David Crystal</author>
</authors>
<title>Language and the Internet.</title>
<date>2001</date>
<publisher>Cambridge University Press.</publisher>
<contexts>
<context position="2568" citStr="Crystal, 2001" startWordPosition="374" endWordPosition="376">marily due to the paucity of CM data in conventional text-corpora which makes data-intensive methods hard to apply. Solorio and Liu (2008a) in their work on English-Spanish CM use models built on smaller datasets to predict valid switching points to synthetically generate data from monolingual corpora, and in another work (2008b) describe parts-of-speech (POS) tagging of CM text. CM though typically observed in spoken language is now increasingly more common in text, thanks to the proliferation of the Computer Mediated Communication channels, especially social media like Twitter and Facebook (Crystal, 2001; Herring, 2003; Danet and Herring, 2007; Cardenas-Claros and Isharyanti, 2009). Social media content is tremendously important for studying trends, reviews, events, humanbehaviour as well as linguistic analysis, and therefore in recent times has spurred a lot of interest in automatic processing of such data. Nevertheless, CM on social media has not been studied from a computational aspect. Moreover, social media content presents additional challenges due to contractions, non-standard spellings and nongrammatical constructions. Furthermore, for languages written in scripts other than Roman, li</context>
</contexts>
<marker>Crystal, 2001</marker>
<rawString>David Crystal. 2001. Language and the Internet. Cambridge University Press.</rawString>
</citation>
<citation valid="true">
<authors>
<author>Brenda Danet</author>
<author>Susan Herring</author>
</authors>
<title>The Multilingual Internet: Language, Culture, and Communication Online.</title>
<date>2007</date>
<publisher>Oxford University Press.,</publisher>
<location>New York.</location>
<contexts>
<context position="1126" citStr="Danet and Herring, 2007" startWordPosition="144" endWordPosition="147">g variations, transliteration and non-adherance to formal grammar. We describe our initial efforts to create a multi-level annotated corpus of Hindi-English codemixed text collated from Facebook forums, and explore language identification, back-transliteration, normalization and POS tagging of this data. Our results show that language identification and transliteration for Hindi are two major challenges that impact POS tagging accuracy. 1 Introduction Code-Switching and Code-Mixing are typical and well-studied phenomena of multilingual societies (Gumperz, 1964; Auer, 1984; MyersScotton, 1993; Danet and Herring, 2007; Cardenas-Claros and Isharyanti, 2009). Linguists differentiate between the two, where Code-Switching is juxtaposition within the same speech exchange of passages of speech belonging to two different grammatical systems or sub-systems (Gumperz, 1982), and CodeMixing (CM) refers to the embedding of linguistic units such as phrases, words and morphemes of one language into an utterance of another language (Myers-Scotton, 1993). The first example in Fig. 1 features CM where English words are embedded in a Hindi sentence, whereas the second example shows codeswitching. Here, we will use CM to imp</context>
<context position="2608" citStr="Danet and Herring, 2007" startWordPosition="379" endWordPosition="382">M data in conventional text-corpora which makes data-intensive methods hard to apply. Solorio and Liu (2008a) in their work on English-Spanish CM use models built on smaller datasets to predict valid switching points to synthetically generate data from monolingual corpora, and in another work (2008b) describe parts-of-speech (POS) tagging of CM text. CM though typically observed in spoken language is now increasingly more common in text, thanks to the proliferation of the Computer Mediated Communication channels, especially social media like Twitter and Facebook (Crystal, 2001; Herring, 2003; Danet and Herring, 2007; Cardenas-Claros and Isharyanti, 2009). Social media content is tremendously important for studying trends, reviews, events, humanbehaviour as well as linguistic analysis, and therefore in recent times has spurred a lot of interest in automatic processing of such data. Nevertheless, CM on social media has not been studied from a computational aspect. Moreover, social media content presents additional challenges due to contractions, non-standard spellings and nongrammatical constructions. Furthermore, for languages written in scripts other than Roman, like Hindi, Bangla, Japanese, Chinese and </context>
</contexts>
<marker>Danet, Herring, 2007</marker>
<rawString>Brenda Danet and Susan Herring. 2007. The Multilingual Internet: Language, Culture, and Communication Online. Oxford University Press., New York.</rawString>
</citation>
<citation valid="true">
<authors>
<author>Spandana Gella</author>
<author>Jatin Sharma</author>
<author>Kalika Bali</author>
</authors>
<title>Query word labeling and back transliteration for indian languages: Shared task system description.</title>
<date>2013</date>
<booktitle>In FIRE Working Notes.</booktitle>
<contexts>
<context position="13360" citStr="Gella et al., 2013" startWordPosition="2168" endWordPosition="2171"> approach to solve these three tasks, and then discuss the experiments and results. 3.1 Language identification Langauge identification is a well studied problem (King and Abney, 2013; Carter et al., 2013; Goldszmidt et al., 2013; Nguyen and Dogruoz, 2013), though for CM text, especially those involving transliterations and orthographic variation, this is far from a solved problem (Nguyen and Dogruoz, 2013). There was a shared task in FIRE 2013 (Saha Roy et al., 2013) on language identification and back transliteration for En mixed with Hi, Bangla and Gujarati. Along the lines of Gella et al (Gella et al., 2013), which was the best performing system in this shared task, we used the word-level logistic regression classifier built by King and Abney (2013). This system provides a source language with a confidence probability for each word in the test set. We trained the classifier on 3201 English words extracted from the SMS corpus developed by Choudhury et al (2007), while the Hindi data was obtained by sampling 3218 Hindi transliterations out of the En-Hi transliteration pairs developed by Sowmya et al. (Sowmya et al., 2010). Ideally, the context of a token is important for identifying the language. A</context>
<context position="15085" citStr="Gella et al., 2013" startWordPosition="2466" endWordPosition="2469">h, whereas that for En is low. This is mainly due to the presence of a large number of contracted and distorted Hi words in the dataset, e.g. h for hai (Fig. 1), which were tagged as En by our system because the training examples had no contracted Hi words, but short and nonconventional spellings were in plenty in the En training examples as those were extracted from the SMS corpus. 3.2 Normalization In our dataset, if a word is identified as Hi, then it must be back-transliterated to Devanagari script so that any off-the-shelf Hindi POS tagger can be used. We used the system by Gella et al. (Gella et al., 2013) for this task, which is part rule-based and part statistical. The system was trained on the 35000 unique transliteration pairs extracted from Hindi song lyrics (Gupta et al., 2012). This corpus has a reasonably wide coverage of Hindi words, and past researchers have also shown that transliteration does not require a very large amount of training data. Normalization of the En text was not needed because the POS tagger (Owoputi et al., 2013) could handle unnormalized text. 3.3 POS tagging Solorio and Liu (2008b) describes a few approaches to POS-tagging of code-switched Spanglish text, all of w</context>
</contexts>
<marker>Gella, Sharma, Bali, 2013</marker>
<rawString>Spandana Gella, Jatin Sharma, and Kalika Bali. 2013. Query word labeling and back transliteration for indian languages: Shared task system description. In FIRE Working Notes.</rawString>
</citation>
<citation valid="true">
<authors>
<author>Kevin Gimpel</author>
<author>N Schneider</author>
<author>B O’Connor</author>
<author>D Das</author>
<author>D Mills</author>
<author>J Eisenstein</author>
<author>M Heilman</author>
<author>D Yogatama</author>
<author>J Flanigan</author>
<author>N A Smith</author>
</authors>
<title>Part-of-speech tagging for twitter: Annotation, features, and experiments.</title>
<date>2011</date>
<booktitle>In Proceedings of ACL.</booktitle>
<marker>Gimpel, Schneider, O’Connor, Das, Mills, Eisenstein, Heilman, Yogatama, Flanigan, Smith, 2011</marker>
<rawString>Kevin Gimpel, N. Schneider, B. O’Connor, D. Das, D. Mills, J. Eisenstein, M. Heilman, D. Yogatama, J. Flanigan, and N. A. Smith. 2011. Part-of-speech tagging for twitter: Annotation, features, and experiments. In Proceedings of ACL.</rawString>
</citation>
<citation valid="true">
<authors>
<author>Moises Goldszmidt</author>
<author>Marc Najork</author>
<author>Stelios Paparizos</author>
</authors>
<title>Boot-strapping language identifiers for short colloquial postings.</title>
<date>2013</date>
<booktitle>In Machine Learning and Knowledge Discovery in Databases,</booktitle>
<volume>8189</volume>
<pages>95--111</pages>
<contexts>
<context position="12970" citStr="Goldszmidt et al., 2013" startWordPosition="2101" endWordPosition="2104">fication, text normalization and POS tagging - are performed sequentially, in that order. This pipelined approach also allows us to use various off-theshelf tools for solving these subtasks and quickly create a baseline system. The baseline results can also provide useful insight into the inherent hardness of POS tagging of code-mixed social media text. In this section, we first describe our approach to solve these three tasks, and then discuss the experiments and results. 3.1 Language identification Langauge identification is a well studied problem (King and Abney, 2013; Carter et al., 2013; Goldszmidt et al., 2013; Nguyen and Dogruoz, 2013), though for CM text, especially those involving transliterations and orthographic variation, this is far from a solved problem (Nguyen and Dogruoz, 2013). There was a shared task in FIRE 2013 (Saha Roy et al., 2013) on language identification and back transliteration for En mixed with Hi, Bangla and Gujarati. Along the lines of Gella et al (Gella et al., 2013), which was the best performing system in this shared task, we used the word-level logistic regression classifier built by King and Abney (2013). This system provides a source language with a confidence probabi</context>
</contexts>
<marker>Goldszmidt, Najork, Paparizos, 2013</marker>
<rawString>Moises Goldszmidt, Marc Najork, and Stelios Paparizos. 2013. Boot-strapping language identifiers for short colloquial postings. In Machine Learning and Knowledge Discovery in Databases, volume 8189 of Lecture Notes in Computer Science, pages 95–111.</rawString>
</citation>
<citation valid="true">
<authors>
<author>John J Gumperz</author>
</authors>
<title>Hindi-punjabi code-switching in Delhi.</title>
<date>1964</date>
<booktitle>In Proceedings of the Ninth International Congress of Linguistics. Mouton:The Hague.</booktitle>
<contexts>
<context position="1069" citStr="Gumperz, 1964" startWordPosition="137" endWordPosition="138">ch content is compounded by presence of spelling variations, transliteration and non-adherance to formal grammar. We describe our initial efforts to create a multi-level annotated corpus of Hindi-English codemixed text collated from Facebook forums, and explore language identification, back-transliteration, normalization and POS tagging of this data. Our results show that language identification and transliteration for Hindi are two major challenges that impact POS tagging accuracy. 1 Introduction Code-Switching and Code-Mixing are typical and well-studied phenomena of multilingual societies (Gumperz, 1964; Auer, 1984; MyersScotton, 1993; Danet and Herring, 2007; Cardenas-Claros and Isharyanti, 2009). Linguists differentiate between the two, where Code-Switching is juxtaposition within the same speech exchange of passages of speech belonging to two different grammatical systems or sub-systems (Gumperz, 1982), and CodeMixing (CM) refers to the embedding of linguistic units such as phrases, words and morphemes of one language into an utterance of another language (Myers-Scotton, 1993). The first example in Fig. 1 features CM where English words are embedded in a Hindi sentence, whereas the second</context>
</contexts>
<marker>Gumperz, 1964</marker>
<rawString>John J. Gumperz. 1964. Hindi-punjabi code-switching in Delhi. In Proceedings of the Ninth International Congress of Linguistics. Mouton:The Hague.</rawString>
</citation>
<citation valid="true">
<authors>
<author>John J Gumperz</author>
</authors>
<title>Discourse Strategies.</title>
<date>1982</date>
<publisher>Oxford University Press.</publisher>
<contexts>
<context position="1377" citStr="Gumperz, 1982" startWordPosition="180" endWordPosition="181">, normalization and POS tagging of this data. Our results show that language identification and transliteration for Hindi are two major challenges that impact POS tagging accuracy. 1 Introduction Code-Switching and Code-Mixing are typical and well-studied phenomena of multilingual societies (Gumperz, 1964; Auer, 1984; MyersScotton, 1993; Danet and Herring, 2007; Cardenas-Claros and Isharyanti, 2009). Linguists differentiate between the two, where Code-Switching is juxtaposition within the same speech exchange of passages of speech belonging to two different grammatical systems or sub-systems (Gumperz, 1982), and CodeMixing (CM) refers to the embedding of linguistic units such as phrases, words and morphemes of one language into an utterance of another language (Myers-Scotton, 1993). The first example in Fig. 1 features CM where English words are embedded in a Hindi sentence, whereas the second example shows codeswitching. Here, we will use CM to imply both. Work on computa∗This work was done during authors’ internship at Microsoft Research India. tional models of CM have been few and far between (Solorio and Liu, 2008a; Solorio and Liu, 2008b; Nguyen and Dogruoz, 2013), primarily due to the pauc</context>
</contexts>
<marker>Gumperz, 1982</marker>
<rawString>John J. Gumperz. 1982. Discourse Strategies. Oxford University Press.</rawString>
</citation>
<citation valid="true">
<authors>
<author>Kanika Gupta</author>
<author>Monojit Choudhury</author>
<author>Kalika Bali</author>
</authors>
<title>Mining Hindi-English transliteration pairs from online Hindi lyrics.</title>
<date>2012</date>
<booktitle>In Proceedings of LREC.</booktitle>
<contexts>
<context position="15266" citStr="Gupta et al., 2012" startWordPosition="2495" endWordPosition="2498">s En by our system because the training examples had no contracted Hi words, but short and nonconventional spellings were in plenty in the En training examples as those were extracted from the SMS corpus. 3.2 Normalization In our dataset, if a word is identified as Hi, then it must be back-transliterated to Devanagari script so that any off-the-shelf Hindi POS tagger can be used. We used the system by Gella et al. (Gella et al., 2013) for this task, which is part rule-based and part statistical. The system was trained on the 35000 unique transliteration pairs extracted from Hindi song lyrics (Gupta et al., 2012). This corpus has a reasonably wide coverage of Hindi words, and past researchers have also shown that transliteration does not require a very large amount of training data. Normalization of the En text was not needed because the POS tagger (Owoputi et al., 2013) could handle unnormalized text. 3.3 POS tagging Solorio and Liu (2008b) describes a few approaches to POS-tagging of code-switched Spanglish text, all of which primarily relies on two monolingual taggers and certain heuristics to combine the output from the two. One of the simpler heuristics is based on language identification, where </context>
</contexts>
<marker>Gupta, Choudhury, Bali, 2012</marker>
<rawString>Kanika Gupta, Monojit Choudhury, and Kalika Bali. 2012. Mining Hindi-English transliteration pairs from online Hindi lyrics. In Proceedings of LREC.</rawString>
</citation>
<citation valid="true">
<authors>
<author>Parth Gupta</author>
<author>Kalika Bali</author>
<author>Rafael E Banchs</author>
<author>Monojit Choudhury</author>
<author>Paolo Rosso</author>
</authors>
<title>Query expansion for mixed-script information retrieval.</title>
<date>2014</date>
<booktitle>In Proc. of SIGIR,</booktitle>
<pages>677--686</pages>
<publisher>ACM Association for Computing Machinery.</publisher>
<marker>Gupta, Bali, Banchs, Choudhury, Rosso, 2014</marker>
<rawString>Parth Gupta, Kalika Bali, Rafael E. Banchs, Monojit Choudhury, and Paolo Rosso. 2014. Query expansion for mixed-script information retrieval. In Proc. of SIGIR, pages 677–686. ACM Association for Computing Machinery.</rawString>
</citation>
<citation valid="true">
<title>Media and Language Change.</title>
<date>2003</date>
<journal>Special issue of Journal of Historical Pragmatics</journal>
<volume>4</volume>
<editor>Susan Herring, editor.</editor>
<marker>2003</marker>
<rawString>Susan Herring, editor. 2003. Media and Language Change. Special issue of Journal of Historical Pragmatics 4:1.</rawString>
</citation>
<citation valid="true">
<authors>
<author>Ben King</author>
<author>Steven Abney</author>
</authors>
<title>Labeling the languages of words in mixed-language documents using weakly supervised methods.</title>
<date>2013</date>
<booktitle>In Proceedings of NAACL-HLT,</booktitle>
<pages>1110--1119</pages>
<contexts>
<context position="12924" citStr="King and Abney, 2013" startWordPosition="2093" endWordPosition="2096">proach in which the tasks - language identification, text normalization and POS tagging - are performed sequentially, in that order. This pipelined approach also allows us to use various off-theshelf tools for solving these subtasks and quickly create a baseline system. The baseline results can also provide useful insight into the inherent hardness of POS tagging of code-mixed social media text. In this section, we first describe our approach to solve these three tasks, and then discuss the experiments and results. 3.1 Language identification Langauge identification is a well studied problem (King and Abney, 2013; Carter et al., 2013; Goldszmidt et al., 2013; Nguyen and Dogruoz, 2013), though for CM text, especially those involving transliterations and orthographic variation, this is far from a solved problem (Nguyen and Dogruoz, 2013). There was a shared task in FIRE 2013 (Saha Roy et al., 2013) on language identification and back transliteration for En mixed with Hi, Bangla and Gujarati. Along the lines of Gella et al (Gella et al., 2013), which was the best performing system in this shared task, we used the word-level logistic regression classifier built by King and Abney (2013). This system provid</context>
</contexts>
<marker>King, Abney, 2013</marker>
<rawString>Ben King and Steven Abney. 2013. Labeling the languages of words in mixed-language documents using weakly supervised methods. In Proceedings of NAACL-HLT, pages 1110–1119.</rawString>
</citation>
<citation valid="true">
<authors>
<author>Mitchell P Marcus</author>
<author>Mary Ann Marcinkiewicz</author>
<author>Beatrice Santorini</author>
</authors>
<title>Building a large annotated corpus of english: The penn treebank.</title>
<date>1993</date>
<booktitle>Computational linguistics,</booktitle>
<pages>19--2</pages>
<contexts>
<context position="17323" citStr="Marcus et al., 1993" startWordPosition="2865" endWordPosition="2868">Chunk level accuracy. We use a CRF++ based POS tagger for Hi, which is freely available from http://nltr. org/snltr-software/. For En, we use the Twitter POS tagger (Owoputi et al., 2013). It also has an inbuilt tokenizer and can work directly on unnormalized text. This tagger has been chosen because Facebook posts and comments are more Twitter-like. We also use the Stanford POS Tagger (Toutanova et al., 2003) which, unlike the Twitter POS Tagger, has not been tuned for Twitter-like text. These taggers use different tagsets - the ILPOST for Hi (Sankaran et al., 2008) and Penn-TreeBank for En (Marcus et al., 1993). The output tags are appropriately mapped to the smaller Universal tagset (Petrov et al., 2011). 3.4 Experiments and Results We conducted three different experiments as follows. In the first experiment, we assume that we know the language identities and normalized/transliterated forms of the words, and only do the POS tagging. This experiment gives us an idea of the accuracy of POS tagging task, if normalization, transliteration and language identification could be done perfectly. We conduct this experiments with two different En POS taggers: the Stanford POS tagger which is trained on formal</context>
</contexts>
<marker>Marcus, Marcinkiewicz, Santorini, 1993</marker>
<rawString>Mitchell P Marcus, Mary Ann Marcinkiewicz, and Beatrice Santorini. 1993. Building a large annotated corpus of english: The penn treebank. Computational linguistics, 19(2):313–330.</rawString>
</citation>
<citation valid="true">
<authors>
<author>Carol Myers-Scotton</author>
</authors>
<title>Dueling Languages: Grammatical Structure in Code-Switching. Claredon,</title>
<date>1993</date>
<location>Oxford.</location>
<contexts>
<context position="1555" citStr="Myers-Scotton, 1993" startWordPosition="210" endWordPosition="211">ccuracy. 1 Introduction Code-Switching and Code-Mixing are typical and well-studied phenomena of multilingual societies (Gumperz, 1964; Auer, 1984; MyersScotton, 1993; Danet and Herring, 2007; Cardenas-Claros and Isharyanti, 2009). Linguists differentiate between the two, where Code-Switching is juxtaposition within the same speech exchange of passages of speech belonging to two different grammatical systems or sub-systems (Gumperz, 1982), and CodeMixing (CM) refers to the embedding of linguistic units such as phrases, words and morphemes of one language into an utterance of another language (Myers-Scotton, 1993). The first example in Fig. 1 features CM where English words are embedded in a Hindi sentence, whereas the second example shows codeswitching. Here, we will use CM to imply both. Work on computa∗This work was done during authors’ internship at Microsoft Research India. tional models of CM have been few and far between (Solorio and Liu, 2008a; Solorio and Liu, 2008b; Nguyen and Dogruoz, 2013), primarily due to the paucity of CM data in conventional text-corpora which makes data-intensive methods hard to apply. Solorio and Liu (2008a) in their work on English-Spanish CM use models built on smal</context>
</contexts>
<marker>Myers-Scotton, 1993</marker>
<rawString>Carol Myers-Scotton. 1993. Dueling Languages: Grammatical Structure in Code-Switching. Claredon, Oxford.</rawString>
</citation>
<citation valid="true">
<authors>
<author>Dong Nguyen</author>
<author>A Seza Dogruoz</author>
</authors>
<title>Word level language identification in online multilingual communication.</title>
<date>2013</date>
<booktitle>In Proceedings of the 2013 Conference on Empirical Methods in Natural Language Processing,</booktitle>
<pages>857--862</pages>
<contexts>
<context position="1950" citStr="Nguyen and Dogruoz, 2013" startWordPosition="278" endWordPosition="281">t grammatical systems or sub-systems (Gumperz, 1982), and CodeMixing (CM) refers to the embedding of linguistic units such as phrases, words and morphemes of one language into an utterance of another language (Myers-Scotton, 1993). The first example in Fig. 1 features CM where English words are embedded in a Hindi sentence, whereas the second example shows codeswitching. Here, we will use CM to imply both. Work on computa∗This work was done during authors’ internship at Microsoft Research India. tional models of CM have been few and far between (Solorio and Liu, 2008a; Solorio and Liu, 2008b; Nguyen and Dogruoz, 2013), primarily due to the paucity of CM data in conventional text-corpora which makes data-intensive methods hard to apply. Solorio and Liu (2008a) in their work on English-Spanish CM use models built on smaller datasets to predict valid switching points to synthetically generate data from monolingual corpora, and in another work (2008b) describe parts-of-speech (POS) tagging of CM text. CM though typically observed in spoken language is now increasingly more common in text, thanks to the proliferation of the Computer Mediated Communication channels, especially social media like Twitter and Faceb</context>
<context position="12997" citStr="Nguyen and Dogruoz, 2013" startWordPosition="2105" endWordPosition="2108">ion and POS tagging - are performed sequentially, in that order. This pipelined approach also allows us to use various off-theshelf tools for solving these subtasks and quickly create a baseline system. The baseline results can also provide useful insight into the inherent hardness of POS tagging of code-mixed social media text. In this section, we first describe our approach to solve these three tasks, and then discuss the experiments and results. 3.1 Language identification Langauge identification is a well studied problem (King and Abney, 2013; Carter et al., 2013; Goldszmidt et al., 2013; Nguyen and Dogruoz, 2013), though for CM text, especially those involving transliterations and orthographic variation, this is far from a solved problem (Nguyen and Dogruoz, 2013). There was a shared task in FIRE 2013 (Saha Roy et al., 2013) on language identification and back transliteration for En mixed with Hi, Bangla and Gujarati. Along the lines of Gella et al (Gella et al., 2013), which was the best performing system in this shared task, we used the word-level logistic regression classifier built by King and Abney (2013). This system provides a source language with a confidence probability for each word in the t</context>
</contexts>
<marker>Nguyen, Dogruoz, 2013</marker>
<rawString>Dong Nguyen and A. Seza Dogruoz. 2013. Word level language identification in online multilingual communication. In Proceedings of the 2013 Conference on Empirical Methods in Natural Language Processing, pages 857–862.</rawString>
</citation>
<citation valid="true">
<authors>
<author>Olutobi Owoputi</author>
<author>Brendan OConnor</author>
<author>Chris Dyer</author>
<author>Kevin Gimpel</author>
<author>Nathan Schneider</author>
<author>Noah A Smith</author>
</authors>
<title>Improved part-of-speech tagging for online conversational text with word clusters.</title>
<date>2013</date>
<booktitle>In Proceedings of NAACL.</booktitle>
<contexts>
<context position="4030" citStr="Owoputi et al., 2013" startWordPosition="594" endWordPosition="597">paper, we describe our initial efforts to POS tag social media content from English-Hindi (henceforth En-Hi) bilinguals while trying to address the challenges of CM, transliteration and non-standard spelling, as well as lack of annotated data. POS tagging is one of the fundamental pre-processing steps for NLP, and while there 974 Proceedings of the 2014 Conference on Empirical Methods in Natural Language Processing (EMNLP), pages 974–979, October 25-29, 2014, Doha, Qatar. c�2014 Association for Computational Linguistics have been works on POS tagging of social media data (Gimpel et al., 2011; Owoputi et al., 2013) and of CM (Solorio and Liu, 2008b), but we do not know of any work on POS tagging of CM text from social media that involves transliteration. The salient contributions of this work are in formalizing the problem and related challenges for processing of En-Hi social media data, creation of an annotated dataset and some initial experiments for language identification, transliteration, normalization and POS tagging of this data. 2 Corpus Creation For this study, we collected data from Facebook public pages of three celebrities: Amitabh Bachchan, Shahrukh Khan, Narendra Modi, and the BBC Hindi ne</context>
<context position="15529" citStr="Owoputi et al., 2013" startWordPosition="2540" endWordPosition="2543">d as Hi, then it must be back-transliterated to Devanagari script so that any off-the-shelf Hindi POS tagger can be used. We used the system by Gella et al. (Gella et al., 2013) for this task, which is part rule-based and part statistical. The system was trained on the 35000 unique transliteration pairs extracted from Hindi song lyrics (Gupta et al., 2012). This corpus has a reasonably wide coverage of Hindi words, and past researchers have also shown that transliteration does not require a very large amount of training data. Normalization of the En text was not needed because the POS tagger (Owoputi et al., 2013) could handle unnormalized text. 3.3 POS tagging Solorio and Liu (2008b) describes a few approaches to POS-tagging of code-switched Spanglish text, all of which primarily relies on two monolingual taggers and certain heuristics to combine the output from the two. One of the simpler heuristics is based on language identification, where the POS tag of a word is the output of the monolingual tagger of the language in which the word is. In this initial study, we apply this basic idea for POS tagging of CM data. We divide the text (which is already sentence-separated) into contiguous maximal chunks</context>
<context position="16890" citStr="Owoputi et al., 2013" startWordPosition="2791" endWordPosition="2794">I HN Tagger Hi Acc. En Acc. Total Acc. Hi CA En CA Total CA 1a K K Standard 75.14 81.91 79.02 27.34 39.67 34.05 1b K K Twitter 75.14 82.66 79.02 27.34 35.74 31.91 2 K NK Twitter 65.61 81.73 74.87 17.58 33.77 26.38 3 NK NK Twitter 44.74 80.68 65.39 40.00 13.17 25.00 Table 2: POS Tagging accuracies for the different models. K=Known, NK = Not Known. LI = Language labels, HN = Hindi normalized forms, Acc. = Token level accuracy, CA = Chunk level accuracy. We use a CRF++ based POS tagger for Hi, which is freely available from http://nltr. org/snltr-software/. For En, we use the Twitter POS tagger (Owoputi et al., 2013). It also has an inbuilt tokenizer and can work directly on unnormalized text. This tagger has been chosen because Facebook posts and comments are more Twitter-like. We also use the Stanford POS Tagger (Toutanova et al., 2003) which, unlike the Twitter POS Tagger, has not been tuned for Twitter-like text. These taggers use different tagsets - the ILPOST for Hi (Sankaran et al., 2008) and Penn-TreeBank for En (Marcus et al., 1993). The output tags are appropriately mapped to the smaller Universal tagset (Petrov et al., 2011). 3.4 Experiments and Results We conducted three different experiments </context>
</contexts>
<marker>Owoputi, OConnor, Dyer, Gimpel, Schneider, Smith, 2013</marker>
<rawString>Olutobi Owoputi, Brendan OConnor, Chris Dyer, Kevin Gimpel, Nathan Schneider, and Noah A. Smith. 2013. Improved part-of-speech tagging for online conversational text with word clusters. In Proceedings of NAACL.</rawString>
</citation>
<citation valid="true">
<authors>
<author>Slav Petrov</author>
<author>Dipanjan Das</author>
<author>Ryan McDonald</author>
</authors>
<title>A universal part-of-speech tagset. arXiv preprint arXiv:1104.2086.</title>
<date>2011</date>
<contexts>
<context position="7640" citStr="Petrov et al. (2011)" startWordPosition="1193" endWordPosition="1196">will not distinguish between CM and borrowing, and such borrowed English words have also been labeled as En words. Normalization/Transliteration: Whenever the word is in a transliterated form, which is often the case for the Hi words, it is labeled with the intended word in the native script (e.g., Devanagari for Hi). If the word is in native script, but uses a non-standard spelling, it is labeled with the correct standard spelling. We call this the spelling normalization layer. Parts-of-Speech (POS): Finally, each word is also labeled with its POS. We use the Universal POS tagset proposed by Petrov et al. (2011) which has 12 POS tags that are applicable to both En and Hi. The POS labels are decided based on the function of a word in the context, rather than a decontextualized lexical category. This is an important notion, especially for CM text, because often the original lexical category of an embedded word is lost in the context of the matrix language, and it plays the role of a different lexical category. Though the Universal POS tagset does not prescribe a separate tag for Named Entities, we felt the necessity of marking three different kinds of NEs - people, location and organization, because al</context>
<context position="17419" citStr="Petrov et al., 2011" startWordPosition="2880" endWordPosition="2883">p://nltr. org/snltr-software/. For En, we use the Twitter POS tagger (Owoputi et al., 2013). It also has an inbuilt tokenizer and can work directly on unnormalized text. This tagger has been chosen because Facebook posts and comments are more Twitter-like. We also use the Stanford POS Tagger (Toutanova et al., 2003) which, unlike the Twitter POS Tagger, has not been tuned for Twitter-like text. These taggers use different tagsets - the ILPOST for Hi (Sankaran et al., 2008) and Penn-TreeBank for En (Marcus et al., 1993). The output tags are appropriately mapped to the smaller Universal tagset (Petrov et al., 2011). 3.4 Experiments and Results We conducted three different experiments as follows. In the first experiment, we assume that we know the language identities and normalized/transliterated forms of the words, and only do the POS tagging. This experiment gives us an idea of the accuracy of POS tagging task, if normalization, transliteration and language identification could be done perfectly. We conduct this experiments with two different En POS taggers: the Stanford POS tagger which is trained on formal English text (Model 1a) and the Twitter POS tagger (Model 1b). In the next experiment (Model 2)</context>
</contexts>
<marker>Petrov, Das, McDonald, 2011</marker>
<rawString>Slav Petrov, Dipanjan Das, and Ryan McDonald. 2011. A universal part-of-speech tagset. arXiv preprint arXiv:1104.2086.</rawString>
</citation>
<citation valid="true">
<authors>
<author>Rishiraj Saha Roy</author>
<author>Monojit Choudhury</author>
<author>Prasenjit Majumder</author>
<author>Komal Agarwal</author>
</authors>
<title>Overview and datasets of fire 2013 track on transliterated search.</title>
<date>2013</date>
<booktitle>In FIRE Working Notes.</booktitle>
<contexts>
<context position="13213" citStr="Roy et al., 2013" startWordPosition="2142" endWordPosition="2145">can also provide useful insight into the inherent hardness of POS tagging of code-mixed social media text. In this section, we first describe our approach to solve these three tasks, and then discuss the experiments and results. 3.1 Language identification Langauge identification is a well studied problem (King and Abney, 2013; Carter et al., 2013; Goldszmidt et al., 2013; Nguyen and Dogruoz, 2013), though for CM text, especially those involving transliterations and orthographic variation, this is far from a solved problem (Nguyen and Dogruoz, 2013). There was a shared task in FIRE 2013 (Saha Roy et al., 2013) on language identification and back transliteration for En mixed with Hi, Bangla and Gujarati. Along the lines of Gella et al (Gella et al., 2013), which was the best performing system in this shared task, we used the word-level logistic regression classifier built by King and Abney (2013). This system provides a source language with a confidence probability for each word in the test set. We trained the classifier on 3201 English words extracted from the SMS corpus developed by Choudhury et al (2007), while the Hindi data was obtained by sampling 3218 Hindi transliterations out of the En-Hi t</context>
</contexts>
<marker>Roy, Choudhury, Majumder, Agarwal, 2013</marker>
<rawString>Rishiraj Saha Roy, Monojit Choudhury, Prasenjit Majumder, and Komal Agarwal. 2013. Overview and datasets of fire 2013 track on transliterated search. In FIRE Working Notes.</rawString>
</citation>
<citation valid="true">
<authors>
<author>Bhaskaran Sankaran</author>
<author>Kalika Bali</author>
<author>Monojit Choudhury</author>
<author>Tanmoy Bhattacharya</author>
<author>Pushpak Bhattacharyya</author>
<author>Girish Nath Jha</author>
<author>S Rajendran</author>
<author>K Saravanan</author>
<author>L Sobha</author>
<author>K V Subbarao</author>
</authors>
<title>A common parts-of-speech tagset framework for indian languages.</title>
<date>2008</date>
<booktitle>In Proceedings of LREC.</booktitle>
<contexts>
<context position="17276" citStr="Sankaran et al., 2008" startWordPosition="2857" endWordPosition="2860">malized forms, Acc. = Token level accuracy, CA = Chunk level accuracy. We use a CRF++ based POS tagger for Hi, which is freely available from http://nltr. org/snltr-software/. For En, we use the Twitter POS tagger (Owoputi et al., 2013). It also has an inbuilt tokenizer and can work directly on unnormalized text. This tagger has been chosen because Facebook posts and comments are more Twitter-like. We also use the Stanford POS Tagger (Toutanova et al., 2003) which, unlike the Twitter POS Tagger, has not been tuned for Twitter-like text. These taggers use different tagsets - the ILPOST for Hi (Sankaran et al., 2008) and Penn-TreeBank for En (Marcus et al., 1993). The output tags are appropriately mapped to the smaller Universal tagset (Petrov et al., 2011). 3.4 Experiments and Results We conducted three different experiments as follows. In the first experiment, we assume that we know the language identities and normalized/transliterated forms of the words, and only do the POS tagging. This experiment gives us an idea of the accuracy of POS tagging task, if normalization, transliteration and language identification could be done perfectly. We conduct this experiments with two different En POS taggers: the</context>
</contexts>
<marker>Sankaran, Bali, Choudhury, Bhattacharya, Bhattacharyya, Jha, Rajendran, Saravanan, Sobha, Subbarao, 2008</marker>
<rawString>Bhaskaran Sankaran, Kalika Bali, Monojit Choudhury, Tanmoy Bhattacharya, Pushpak Bhattacharyya, Girish Nath Jha, S. Rajendran, K. Saravanan, L. Sobha, and K. V. Subbarao. 2008. A common parts-of-speech tagset framework for indian languages. In Proceedings of LREC.</rawString>
</citation>
<citation valid="true">
<authors>
<author>Thamar Solorio</author>
<author>Yang Liu</author>
</authors>
<title>Learning to predict code-switching points.</title>
<date>2008</date>
<booktitle>In Proceedings of the Empirical Methods in natural Language Processing.</booktitle>
<contexts>
<context position="1898" citStr="Solorio and Liu, 2008" startWordPosition="270" endWordPosition="273"> of passages of speech belonging to two different grammatical systems or sub-systems (Gumperz, 1982), and CodeMixing (CM) refers to the embedding of linguistic units such as phrases, words and morphemes of one language into an utterance of another language (Myers-Scotton, 1993). The first example in Fig. 1 features CM where English words are embedded in a Hindi sentence, whereas the second example shows codeswitching. Here, we will use CM to imply both. Work on computa∗This work was done during authors’ internship at Microsoft Research India. tional models of CM have been few and far between (Solorio and Liu, 2008a; Solorio and Liu, 2008b; Nguyen and Dogruoz, 2013), primarily due to the paucity of CM data in conventional text-corpora which makes data-intensive methods hard to apply. Solorio and Liu (2008a) in their work on English-Spanish CM use models built on smaller datasets to predict valid switching points to synthetically generate data from monolingual corpora, and in another work (2008b) describe parts-of-speech (POS) tagging of CM text. CM though typically observed in spoken language is now increasingly more common in text, thanks to the proliferation of the Computer Mediated Communication chan</context>
<context position="4063" citStr="Solorio and Liu, 2008" startWordPosition="601" endWordPosition="604">forts to POS tag social media content from English-Hindi (henceforth En-Hi) bilinguals while trying to address the challenges of CM, transliteration and non-standard spelling, as well as lack of annotated data. POS tagging is one of the fundamental pre-processing steps for NLP, and while there 974 Proceedings of the 2014 Conference on Empirical Methods in Natural Language Processing (EMNLP), pages 974–979, October 25-29, 2014, Doha, Qatar. c�2014 Association for Computational Linguistics have been works on POS tagging of social media data (Gimpel et al., 2011; Owoputi et al., 2013) and of CM (Solorio and Liu, 2008b), but we do not know of any work on POS tagging of CM text from social media that involves transliteration. The salient contributions of this work are in formalizing the problem and related challenges for processing of En-Hi social media data, creation of an annotated dataset and some initial experiments for language identification, transliteration, normalization and POS tagging of this data. 2 Corpus Creation For this study, we collected data from Facebook public pages of three celebrities: Amitabh Bachchan, Shahrukh Khan, Narendra Modi, and the BBC Hindi news page. All these pages are very</context>
<context position="15599" citStr="Solorio and Liu (2008" startWordPosition="2551" endWordPosition="2554">hat any off-the-shelf Hindi POS tagger can be used. We used the system by Gella et al. (Gella et al., 2013) for this task, which is part rule-based and part statistical. The system was trained on the 35000 unique transliteration pairs extracted from Hindi song lyrics (Gupta et al., 2012). This corpus has a reasonably wide coverage of Hindi words, and past researchers have also shown that transliteration does not require a very large amount of training data. Normalization of the En text was not needed because the POS tagger (Owoputi et al., 2013) could handle unnormalized text. 3.3 POS tagging Solorio and Liu (2008b) describes a few approaches to POS-tagging of code-switched Spanglish text, all of which primarily relies on two monolingual taggers and certain heuristics to combine the output from the two. One of the simpler heuristics is based on language identification, where the POS tag of a word is the output of the monolingual tagger of the language in which the word is. In this initial study, we apply this basic idea for POS tagging of CM data. We divide the text (which is already sentence-separated) into contiguous maximal chunks of words which are in the same language. Then we apply a Hi POS tagge</context>
</contexts>
<marker>Solorio, Liu, 2008</marker>
<rawString>Thamar Solorio and Yang Liu. 2008a. Learning to predict code-switching points. In Proceedings of the Empirical Methods in natural Language Processing.</rawString>
</citation>
<citation valid="true">
<authors>
<author>Thamar Solorio</author>
<author>Yang Liu</author>
</authors>
<title>Parts-of-speech tagging for English-Spanish code-switched text.</title>
<date>2008</date>
<booktitle>In Proceedings of the Empirical Methods in natural Language Processing.</booktitle>
<contexts>
<context position="1898" citStr="Solorio and Liu, 2008" startWordPosition="270" endWordPosition="273"> of passages of speech belonging to two different grammatical systems or sub-systems (Gumperz, 1982), and CodeMixing (CM) refers to the embedding of linguistic units such as phrases, words and morphemes of one language into an utterance of another language (Myers-Scotton, 1993). The first example in Fig. 1 features CM where English words are embedded in a Hindi sentence, whereas the second example shows codeswitching. Here, we will use CM to imply both. Work on computa∗This work was done during authors’ internship at Microsoft Research India. tional models of CM have been few and far between (Solorio and Liu, 2008a; Solorio and Liu, 2008b; Nguyen and Dogruoz, 2013), primarily due to the paucity of CM data in conventional text-corpora which makes data-intensive methods hard to apply. Solorio and Liu (2008a) in their work on English-Spanish CM use models built on smaller datasets to predict valid switching points to synthetically generate data from monolingual corpora, and in another work (2008b) describe parts-of-speech (POS) tagging of CM text. CM though typically observed in spoken language is now increasingly more common in text, thanks to the proliferation of the Computer Mediated Communication chan</context>
<context position="4063" citStr="Solorio and Liu, 2008" startWordPosition="601" endWordPosition="604">forts to POS tag social media content from English-Hindi (henceforth En-Hi) bilinguals while trying to address the challenges of CM, transliteration and non-standard spelling, as well as lack of annotated data. POS tagging is one of the fundamental pre-processing steps for NLP, and while there 974 Proceedings of the 2014 Conference on Empirical Methods in Natural Language Processing (EMNLP), pages 974–979, October 25-29, 2014, Doha, Qatar. c�2014 Association for Computational Linguistics have been works on POS tagging of social media data (Gimpel et al., 2011; Owoputi et al., 2013) and of CM (Solorio and Liu, 2008b), but we do not know of any work on POS tagging of CM text from social media that involves transliteration. The salient contributions of this work are in formalizing the problem and related challenges for processing of En-Hi social media data, creation of an annotated dataset and some initial experiments for language identification, transliteration, normalization and POS tagging of this data. 2 Corpus Creation For this study, we collected data from Facebook public pages of three celebrities: Amitabh Bachchan, Shahrukh Khan, Narendra Modi, and the BBC Hindi news page. All these pages are very</context>
<context position="15599" citStr="Solorio and Liu (2008" startWordPosition="2551" endWordPosition="2554">hat any off-the-shelf Hindi POS tagger can be used. We used the system by Gella et al. (Gella et al., 2013) for this task, which is part rule-based and part statistical. The system was trained on the 35000 unique transliteration pairs extracted from Hindi song lyrics (Gupta et al., 2012). This corpus has a reasonably wide coverage of Hindi words, and past researchers have also shown that transliteration does not require a very large amount of training data. Normalization of the En text was not needed because the POS tagger (Owoputi et al., 2013) could handle unnormalized text. 3.3 POS tagging Solorio and Liu (2008b) describes a few approaches to POS-tagging of code-switched Spanglish text, all of which primarily relies on two monolingual taggers and certain heuristics to combine the output from the two. One of the simpler heuristics is based on language identification, where the POS tag of a word is the output of the monolingual tagger of the language in which the word is. In this initial study, we apply this basic idea for POS tagging of CM data. We divide the text (which is already sentence-separated) into contiguous maximal chunks of words which are in the same language. Then we apply a Hi POS tagge</context>
</contexts>
<marker>Solorio, Liu, 2008</marker>
<rawString>Thamar Solorio and Yang Liu. 2008b. Parts-of-speech tagging for English-Spanish code-switched text. In Proceedings of the Empirical Methods in natural Language Processing.</rawString>
</citation>
<citation valid="true">
<authors>
<author>V B Sowmya</author>
<author>Monojit Choudhury</author>
<author>Kalika Bali</author>
<author>Tirthankar Dasgupta</author>
<author>Anupam Basu</author>
</authors>
<title>Resource creation for training and testing of transliteration systems for indian languages.</title>
<date>2010</date>
<booktitle>In Proceedings of the Language Resource and Evaluation Conference (LREC).</booktitle>
<contexts>
<context position="3306" citStr="Sowmya et al., 2010" startWordPosition="481" endWordPosition="484">important for studying trends, reviews, events, humanbehaviour as well as linguistic analysis, and therefore in recent times has spurred a lot of interest in automatic processing of such data. Nevertheless, CM on social media has not been studied from a computational aspect. Moreover, social media content presents additional challenges due to contractions, non-standard spellings and nongrammatical constructions. Furthermore, for languages written in scripts other than Roman, like Hindi, Bangla, Japanese, Chinese and Arabic, Roman transliterations are typically used for representing the words (Sowmya et al., 2010). This can prove a challenge for language identification and segregation of the two languages. In this paper, we describe our initial efforts to POS tag social media content from English-Hindi (henceforth En-Hi) bilinguals while trying to address the challenges of CM, transliteration and non-standard spelling, as well as lack of annotated data. POS tagging is one of the fundamental pre-processing steps for NLP, and while there 974 Proceedings of the 2014 Conference on Empirical Methods in Natural Language Processing (EMNLP), pages 974–979, October 25-29, 2014, Doha, Qatar. c�2014 Association f</context>
<context position="13882" citStr="Sowmya et al., 2010" startWordPosition="2255" endWordPosition="2258">ion for En mixed with Hi, Bangla and Gujarati. Along the lines of Gella et al (Gella et al., 2013), which was the best performing system in this shared task, we used the word-level logistic regression classifier built by King and Abney (2013). This system provides a source language with a confidence probability for each word in the test set. We trained the classifier on 3201 English words extracted from the SMS corpus developed by Choudhury et al (2007), while the Hindi data was obtained by sampling 3218 Hindi transliterations out of the En-Hi transliteration pairs developed by Sowmya et al. (Sowmya et al., 2010). Ideally, the context of a token is important for identifying the language. Again, following (Gella et al., 2013) we incorporate context information through a code-switching probability, P3. A higher value of P3 implies a lower probability of code-switching, i.e., adjacent words are more likely to be in the same language. Table 1 shows the token (word) level confusion matrix for the language identification task on our dataset. The language labels of 84.6% of the tokens were correctly predicted by the system. As can be seen from the Table, the precision for predicting Hi is high, whereas that </context>
</contexts>
<marker>Sowmya, Choudhury, Bali, Dasgupta, Basu, 2010</marker>
<rawString>V. B. Sowmya, Monojit Choudhury, Kalika Bali, Tirthankar Dasgupta, and Anupam Basu. 2010. Resource creation for training and testing of transliteration systems for indian languages. In Proceedings of the Language Resource and Evaluation Conference (LREC).</rawString>
</citation>
<citation valid="true">
<authors>
<author>Kristina Toutanova</author>
<author>Dan Klein</author>
<author>Christopher Manning</author>
<author>Yoram Singer</author>
</authors>
<title>Feature-rich part-ofspeech tagging with a cyclic dependency network.</title>
<date>2003</date>
<booktitle>In Proceedings of HLT-NAACL.</booktitle>
<contexts>
<context position="17116" citStr="Toutanova et al., 2003" startWordPosition="2829" endWordPosition="2832">itter 44.74 80.68 65.39 40.00 13.17 25.00 Table 2: POS Tagging accuracies for the different models. K=Known, NK = Not Known. LI = Language labels, HN = Hindi normalized forms, Acc. = Token level accuracy, CA = Chunk level accuracy. We use a CRF++ based POS tagger for Hi, which is freely available from http://nltr. org/snltr-software/. For En, we use the Twitter POS tagger (Owoputi et al., 2013). It also has an inbuilt tokenizer and can work directly on unnormalized text. This tagger has been chosen because Facebook posts and comments are more Twitter-like. We also use the Stanford POS Tagger (Toutanova et al., 2003) which, unlike the Twitter POS Tagger, has not been tuned for Twitter-like text. These taggers use different tagsets - the ILPOST for Hi (Sankaran et al., 2008) and Penn-TreeBank for En (Marcus et al., 1993). The output tags are appropriately mapped to the smaller Universal tagset (Petrov et al., 2011). 3.4 Experiments and Results We conducted three different experiments as follows. In the first experiment, we assume that we know the language identities and normalized/transliterated forms of the words, and only do the POS tagging. This experiment gives us an idea of the accuracy of POS tagging</context>
</contexts>
<marker>Toutanova, Klein, Manning, Singer, 2003</marker>
<rawString>Kristina Toutanova, Dan Klein, Christopher Manning, and Yoram Singer. 2003. Feature-rich part-ofspeech tagging with a cyclic dependency network. In Proceedings of HLT-NAACL.</rawString>
</citation>
</citationList>
</algorithm>
</algorithms>
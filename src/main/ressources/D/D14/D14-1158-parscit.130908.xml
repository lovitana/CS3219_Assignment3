<?xml version="1.0" encoding="UTF-8"?>
<algorithms version="110505">
<algorithm name="SectLabel" version="110505">
<variant no="0" confidence="0.000000">
<title confidence="0.83801">
Language Modeling with Power Low Rank Ensembles
</title>
<author confidence="0.976573">
Ankur P. Parikh
</author>
<affiliation confidence="0.990793">
School of Computer Science
Carnegie Mellon University
</affiliation>
<email confidence="0.978992">
apparikh@cs.cmu.edu
</email>
<author confidence="0.990325">
Chris Dyer
</author>
<affiliation confidence="0.991085">
School of Computer Science
Carnegie Mellon University
</affiliation>
<email confidence="0.996598">
cdyer@cs.cmu.edu
</email>
<sectionHeader confidence="0.994767" genericHeader="abstract">
Abstract
</sectionHeader>
<bodyText confidence="0.999837625">
We present power low rank ensembles
(PLRE), a flexible framework for n-gram
language modeling where ensembles of
low rank matrices and tensors are used
to obtain smoothed probability estimates
of words in context. Our method can
be understood as a generalization of n-
gram modeling to non-integer n, and in-
cludes standard techniques such as abso-
lute discounting and Kneser-Ney smooth-
ing as special cases. PLRE training is effi-
cient and our approach outperforms state-
of-the-art modified Kneser Ney baselines
in terms of perplexity on large corpora as
well as on BLEU score in a downstream
machine translation task.
</bodyText>
<sectionHeader confidence="0.998783" genericHeader="keywords">
1 Introduction
</sectionHeader>
<bodyText confidence="0.97465935">
Language modeling is the task of estimating the
probability of sequences of words in a language
and is an important component in, among other
applications, automatic speech recognition (Ra-
biner and Juang, 1993) and machine translation
(Koehn, 2010). The predominant approach to lan-
guage modeling is the n-gram model, wherein
the probability of a word sequence P(w1, ... , w`)
is decomposed using the chain rule, and then a
Markov assumption is made: P(w1, ... , w`) ≈
H`i=1 P(wi|wi−1
i−n+1). While this assumption sub-
stantially reduces the modeling complexity, pa-
rameter estimation remains a major challenge.
Due to the power-law nature of language (Zipf,
1949), the maximum likelihood estimator mas-
sively overestimates the probability of rare events
and assigns zero probability to legitimate word se-
quences that happen not to have been observed in
the training data (Manning and Sch¨utze, 1999).
</bodyText>
<note confidence="0.880267333333333">
Avneesh Saluja
Electrical &amp; Computer Engineering
Carnegie Mellon University
</note>
<email confidence="0.560507">
avneesh@cs.cmu.edu
</email>
<author confidence="0.987495">
Eric P. Xing
</author>
<affiliation confidence="0.9961015">
School of Computer Science
Carnegie Mellon University
</affiliation>
<email confidence="0.988751">
epxing@cs.cmu.edu
</email>
<bodyText confidence="0.999213846153846">
Many smoothing techniques have been pro-
posed to address the estimation challenge. These
reassign probability mass (generally from over-
estimated events) to unseen word sequences,
whose probabilities are estimated by interpolating
with or backing off to lower order n-gram models
(Chen and Goodman, 1999).
Somewhat surprisingly, these widely used
smoothing techniques differ substantially from
techniques for coping with data sparsity in other
domains, such as collaborative filtering (Koren et
al., 2009; Su and Khoshgoftaar, 2009) or matrix
completion (Cand`es and Recht, 2009; Cai et al.,
2010). In these areas, low rank approaches based
on matrix factorization play a central role (Lee
and Seung, 2001; Salakhutdinov and Mnih, 2008;
Mackey et al., 2011). For example, in recom-
mender systems, a key challenge is dealing with
the sparsity of ratings from a single user, since
typical users will have rated only a few items. By
projecting the low rank representation of a user’s
(sparse) preferences into the original space, an es-
timate of ratings for new items is obtained. These
methods are attractive due to their computational
efficiency and mathematical well-foundedness.
In this paper, we introduce power low rank en-
sembles (PLRE), in which low rank tensors are
used to produce smoothed estimates for n-gram
probabilities. Ideally, we would like the low rank
structures to discover semantic and syntactic relat-
edness among words and n-grams, which are used
to produce smoothed estimates for word sequence
probabilities. In contrast to the few previous low
rank language modeling approaches, PLRE is not
orthogonal to n-gram models, but rather a gen-
eral framework where existing n-gram smoothing
methods such as Kneser-Ney smoothing are spe-
cial cases. A key insight is that PLRE does not
compute low rank approximations of the original
</bodyText>
<page confidence="0.943768">
1487
</page>
<note confidence="0.899183">
Proceedings of the 2014 Conference on Empirical Methods in Natural Language Processing (EMNLP), pages 1487–1498,
October 25-29, 2014, Doha, Qatar. c�2014 Association for Computational Linguistics
</note>
<bodyText confidence="0.999692409090909">
joint count matrices (in the case of bigrams) or ten-
sors i.e. multi-way arrays (in the case of 3-grams
and above), but instead altered quantities of these
counts based on an element-wise power operation,
similar to how some smoothing methods modify
their lower order distributions.
Moreover, PLRE has two key aspects that lead
to easy scalability for large corpora and vocabu-
laries. First, since it utilizes the original n-grams,
the ranks required for the low rank matrices and
tensors tend to be remain tractable (e.g. around
100 for a vocabulary size V ≈ 1 × 106) leading
to fast training times. This differentiates our ap-
proach over other methods that leverage an under-
lying latent space such as neural networks (Bengio
et al., 2003; Mnih and Hinton, 2007; Mikolov et
al., 2010) or soft-class models (Saul and Pereira,
1997) where the underlying dimension is required
to be quite large to obtain good performance.
Moreover, at test time, the probability of a se-
quence can be queried in time O(κmax) where
κmax is the maximum rank of the low rank matri-
ces/tensors used. While this is larger than Kneser
Ney’s virtually constant query time, it is substan-
tially faster than conditional exponential family
models (Chen and Rosenfeld, 2000; Chen, 2009;
Nelakanti et al., 2013) and neural networks which
require O(V ) for exact computation of the nor-
malization constant. See Section 7 for a more de-
tailed discussion of related work.
Outline: We first review existing n-gram
smoothing methods (§2) and then present the in-
tuition behind the key components of our tech-
nique: rank (§3.1) and power (§3.2). We then
show how these can be interpolated into an ensem-
ble (§4). In the experimental evaluation on English
and Russian corpora (§5), we find that PLRE out-
performs Kneser-Ney smoothing and all its vari-
ants, as well as class-based language models. We
also include a comparison to the log-bilinear neu-
ral language model (Mnih and Hinton, 2007) and
evaluate performance on a downstream machine
translation task (§6) where our method achieves
consistent improvements in BLEU.
</bodyText>
<sectionHeader confidence="0.997438" genericHeader="introduction">
2 Discount-based Smoothing
</sectionHeader>
<bodyText confidence="0.983189714285714">
We first provide background on absolute discount-
ing (Ney et al., 1994) and Kneser-Ney smooth-
ing (Kneser and Ney, 1995), two common n-gram
smoothing methods. Both methods can be formu-
lated as back-off or interpolated models; we de-
scribe the latter here since that is the basis of our
low rank approach.
</bodyText>
<subsectionHeader confidence="0.742389">
2.1 Notation
</subsectionHeader>
<bodyText confidence="0.90885075">
Let c(w) be the count of word w, and similarly
c(w, wi−1) for the joint count of words w and
wi−1. For shorthand we will define wji to denote
the word sequence {wi, wi+1, ..., wj−1, wj}. Let
P(wi) refer to the maximum likelihood estimate
(MLE) of the probability of word wi, and simi-
larly P�(wi|wi−1) for the probability conditioned
P� (wi|wi−1
i−n+1).
Let N−(wi) := |{w : c(wi, w) &gt; 0} |be
the number of distinct words that appear be-
fore wi. More generally, let N−(wii−n+1) =
</bodyText>
<equation confidence="0.898361">
|{w : c(wii−n+1,w) &gt; 0}|. Similarly, let
N+(wi−1
i−n+1) = |{w : c(w, wi−1
i−n+1) &gt; 0}|. V
denotes the vocabulary size.
</equation>
<subsectionHeader confidence="0.998545">
2.2 Absolute Discounting
</subsectionHeader>
<bodyText confidence="0.999835375">
Absolute discounting works on the idea of inter-
polating higher order n-gram models with lower-
order n-gram models. However, first some prob-
ability mass must be “subtracted” from the higher
order n-grams so that the leftover probability can
be allocated to the lower order n-grams. More
specifically, define the following discounted con-
ditional probability:
</bodyText>
<equation confidence="0.989257545454545">
�PD(wi|wi−1
i−n+1) = i−1
c(wi−n+1)
Then absolute discounting Pabs(·) uses the follow-
ing (recursive) equation:
Pabs(wi|wi−1
i−n+1) = �PD(wi|wi−1
i−n+1)
+ γ(wi−1
i−n+1)Pabs(wi|wi−1
i−n+2)
</equation>
<bodyText confidence="0.9706">
where γ(wi−1
i−n+1) is the leftover weight (due to
the discounting) that is chosen so that the con-
ditional distribution sums to one: γ(wi−1
</bodyText>
<equation confidence="0.889153333333333">
i−n+1) =
iD N+ (wi+1). For the base case, we set
c(wi−n+1
Pabs(wi) =
Discontinuity: Note that if c(wi−1
i−n+1) = 0, then
γ(wi−1
i−n+1) = 00, in which case γ(wi−1
i−n+1) is set
to 1. We will see that this discontinuity appears in
PLRE as well.
on a history, or more generally,
max{c(wi, wi−1
i−n+1) − D, 0}
P(wi).
</equation>
<page confidence="0.983334">
1488
</page>
<subsectionHeader confidence="0.98556">
2.3 Kneser Ney Smoothing
</subsectionHeader>
<bodyText confidence="0.999388">
Ideally, the smoothed probability should preserve
the observed unigram distribution:
</bodyText>
<equation confidence="0.9980552">
X Psm(wi|wi−1
Pb(wi) = i−n+1) Pb(wi−1
i−1 i−n+1) (1)
w
i−n+1
</equation>
<bodyText confidence="0.99188425">
where Psm(wi|wi−1
i−n+1) is the smoothed condi-
tional probability that a model outputs. Unfortu-
nately, absolute discounting does not satisfy this
property, since it exclusively uses the unaltered
MLE unigram model as its lower order model. In
practice, the lower order distribution is only uti-
lized when we are unsure about the higher order
distribution (i.e., when γ(·) is large). Therefore,
the unigram model should be altered to condition
on this fact.
This is the inspiration behind Kneser-Ney (KN)
smoothing, an elegant algorithm with robust per-
formance in n-gram language modeling. KN
smoothing defines alternate probabilities Palt(·):
The base case for unigrams reduces to
</bodyText>
<equation confidence="0.985554">
Palt(wi) = N−(wi) Intuitively Palt(wi) is
�wi N−(wi)
</equation>
<bodyText confidence="0.9979375">
proportional to the number of unique words that
precede wi. Thus, words that appear in many dif-
ferent contexts will be given higher weight than
words that consistently appear after only a few
contexts. These alternate distributions are then
used with absolute discounting:
</bodyText>
<equation confidence="0.99962575">
i−1alt i−1
(iγ+ Pkn(wi  |wi−n+1) = PD (wi  |wi−n+1)
−1 i−1
wi−n+1)Pkn(wi |wi−n+2) (2)
</equation>
<bodyText confidence="0.994586666666667">
where we set Pkn(wi) = Palt(wi). By definition,
KN smoothing satisfies the marginal constraint in
Eq. 1 (Kneser and Ney, 1995).
</bodyText>
<sectionHeader confidence="0.98367" genericHeader="method">
3 Power Low Rank Ensembles
</sectionHeader>
<bodyText confidence="0.999497">
In n-gram smoothing methods, if a bigram count
c(wi, wi−1) is zero, the unigram probabilities are
used, which is equivalent to assuming that wi and
wi−1 are independent ( and similarly for general
n). However, in this situation, instead of back-
ing off to a 1-gram, we may like to back off to a
“1.5-gram” or more generally an order between 1
and 2 that captures a coarser level of dependence
between wi and wi−1 and does not assume full in-
dependence.
Inspired by this intuition, our strategy is to con-
struct an ensemble of matrices and tensors that
not only consists of MLE-based count informa-
tion, but also contains quantities that represent lev-
els of dependence in-between the various orders in
the model. We call these combinations power low
rank ensembles (PLRE), and they can be thought
of as n-gram models with non-integer n. Our ap-
proach can be recursively formulated as:
</bodyText>
<equation confidence="0.999275777777778">
Pplre(wi|wi−1
i−n+1) = P alt D0(wi|wi−1
i−n+1)
i−1 i−1
+ γ0 (wi−n+1) ZD1 (wi  |wi−n+1) +
i−1 i−1
+ γη−1 (wi−n+1) ZDη (wi  |wi−n+1)
i−1 i−1
+ γη (wi−n+1)(Pplre(wi|wi−n+2))) ... (3)
</equation>
<bodyText confidence="0.987508153846154">
where Z1, ..., Zη are conditional probability ma-
trices that represent the intermediate n-gram or-
ders1 and D is a discount function (specified in
§4).
This formulation begs answers to a few crit-
ical questions. How to construct matrices that
represent conditional probabilities for intermedi-
ate n? How to transform them in a way that
generalizes the altered lower order distributions
in KN smoothing? How to combine these matri-
ces such that the marginal constraint in Eq. 1 still
holds? The following propose solutions to these
three queries:
</bodyText>
<listItem confidence="0.998746857142857">
1. Rank (Section 3.1): Rank gives us a concrete
measurement of the dependence between wi
and wi−1. By constructing low rank ap-
proximations of the bigram count matrix and
higher-order count tensors, we obtain matri-
ces that represent coarser dependencies, with
a rank one approximation implying that the
variables are independent.
2. Power (Section 3.2): In KN smoothing, the
lower order distributions are not the original
counts but rather altered estimates. We pro-
pose a continuous generalization of this alter-
ation by taking the element-wise power of the
counts.
</listItem>
<footnote confidence="0.9440705">
1with a slight abuse of notation, let ZDj be shorthand
for Zj,Dj
</footnote>
<equation confidence="0.5264668">
alt i−1 ⎧ bPD(wi|wi−1
PD (wi |wi−n0+1) = ⎨⎪⎪ i−n0+1), if n&apos;= n
⎪⎪⎩ max{N−(wii−n0+1)−D,0}
if n&apos;&lt; n
Ewi N−(wii−n0+1)
</equation>
<page confidence="0.984552">
1489
</page>
<bodyText confidence="0.92752425">
3. Creating the Ensemble (Section 4): Lastly,
PLRE also defines a way to interpolate the
specifically constructed intermediate n-gram
matrices. Unfortunately a constant discount,
as presented in Section 2, will not in general
preserve the lower order marginal constraint
(Eq. 1). We propose a generalized discount-
ing scheme to ensure the constraint holds.
</bodyText>
<subsectionHeader confidence="0.996954">
3.1 Rank
</subsectionHeader>
<bodyText confidence="0.996641083333333">
We first show how rank can be utilized to construct
quantities between an n-gram and an n − 1-gram.
In general, we think of an n-gram as an nth or-
der tensor i.e. a multi-way array with n indices
{i1, ..., in}. (A vector is a tensor of order 1, a ma-
trix is a tensor of order 2 etc.) Computing a spe-
cial rank one approximation of slices of this tensor
produces the n − 1-gram. Thus, taking rank κ ap-
proximations in this fashion allows us to represent
dependencies between an n-gram and n−1-gram.
Consider the bigram count matrix B with
N_ counts which has rank V . Note that
</bodyText>
<equation confidence="0.992935">
P(wi |wi−1) = B(wi,wi−1) Additionally, B
Ew B(w,wi−1)
</equation>
<bodyText confidence="0.965039384615385">
can be considered a random variable that is the re-
sult of sampling N tuples of (wi, wi−1) and ag-
glomerating them into a count matrix. Assum-
ing wi and wi−1 are independent, the expected
value (with respect to the empirical distribution)
]E[B] = NP(wi)P(wi−1), which can be rewrit-
ten as being proportional to the outer product of
the unigram probability vector with itself, and is
thus rank one.
This observation extends to higher order
n-grams as well. Let Cn be the nth order tensor
where Cn(wi, ...., wi−n+1) = c(wi, ..., wi−n+1).
Furthermore denote Cn(:,˜wi−1
</bodyText>
<equation confidence="0.989879666666667">
i−n+2,:) to
be the V x V matrix slice of Cn where
wi−n+2, ..., wi−1 are held fixed to a particular
</equation>
<bodyText confidence="0.9919714">
˜wi−1. Then if wi is con-
ditionally independent of wi−n+1 given wi−1
i−n+2,
then ]E[Cn(:, ˜wi−1
i−n+2, :)] is rank one b ˜wi−1
i−n+2.
However, it is rare that these matrices are ac-
tually rank one, either due to sampling vari-
ance or the fact that wi and wi−1 are not in-
dependent. What we would really like to say
is that the best rank one approximation B(1)
P(wi−1).
While this statement is not true under the E2
norm, it is true under generalized KL diver-
gence (Lee and Seung, 2001): gKL(A||B) =
</bodyText>
<subsectionHeader confidence="0.338896">
Eij (Aij log(Aij
</subsectionHeader>
<bodyText confidence="0.354217">
Bi,. − Aij + Bij)).
</bodyText>
<page confidence="0.976553">
7
</page>
<bodyText confidence="0.998344857142857">
In particular, generalized KL divergence pre-
serves row and column sums: if M(κ) is the best
rank κ approximation of M under gKL then the
row sums and column sums of M(κ) and M are
equal (Ho and Van Dooren, 2008). Leveraging
this property, it is straightforward to prove the fol-
lowing lemma:
</bodyText>
<equation confidence="0.8300372">
Lemma 1. Let B(κ) be the best rank κ ap-
proximation of B under gKL. Then B(1) a
P(wi)
P�(wi−1) and bwi−1 s.t. c(wi−1) =�
P� (wi) = B(1)(wi, wi−1) Ew B(1)(w, wi−1)
</equation>
<bodyText confidence="0.589694666666667">
For more general n, let Cn,(κ)
i−1,...,i−n+2 be the
best rank κ approximation of Cn(:, ˜wi−1
</bodyText>
<equation confidence="0.992220461538462">
i−n+2,
:
)under gKL. Then similarly, bwi−1
i−n+1 s.t.
c(wi−1
i−n+1) &gt; 0:
P�(wi|wi−1, ..., wi−n+2)
Cn,(1)
i−1,...,i−n+2(wi, wi−1
i−n+1) (4)
Ew
Cn,(1) i−1
i−1,...,i−n+2(w, wi−n+1)
</equation>
<bodyText confidence="0.9312822">
Thus, by selecting 1 &lt; κ &lt; V , we obtain count
matrices and tensors between n and n − 1-grams.
The condition that c(wi−1
i−n+1) &gt; 0 corresponds to
the discontinuity discussed in §2.2.
</bodyText>
<subsectionHeader confidence="0.997053">
3.2 Power
</subsectionHeader>
<bodyText confidence="0.999986888888889">
Since KN smoothing alters the lower order distri-
butions instead of simply using the MLE, vary-
ing the rank is not sufficient in order to generalize
this suite of techniques. Thus, PLRE computes
low rank approximations of altered count matri-
ces. Consider taking the elementwise power p of
the bigram count matrix, which is denoted by B·ρ.
For example, the observed bigram count matrix
and associated row sum:
</bodyText>
<equation confidence="0.994003">
� �
1.0 2.0 1.0 4.0
row sum
B·1 = 0 5.0 0 5.0
</equation>
<bodyText confidence="0.89035025">
2.0 0 0 2.0
As expected the row sum is equal to the uni-
gram counts (which we denote as u). Now con-
sider B·0.5:
</bodyText>
<equation confidence="0.9602246">
� �
1.0 1.4 1.0 3.4
B·0.5 = row sum
0 2.2 0 � 2.2
1.4 0 0 1.4
</equation>
<bodyText confidence="0.999538">
Note how the row sum vector has been altered.
In particular since w1 (corresponding to the first
</bodyText>
<figure confidence="0.445832333333333">
sequence ˜wi−n+2, ...,
(under some norm) of B is a P(wi)
0:
</figure>
<page confidence="0.872281">
1490
</page>
<bodyText confidence="0.998092307692308">
row) has a more diverse history than w2, it has
a higher row sum (compared to in u where w2
has the higher row sum). Lastly, consider the case
when p = 0:
1.0 1.0 1.0 3.0
B·0 = 0 1.0 0 ro su m 1.0
!
1.0 0 0 1.0
The row sum is now the number of unique words
that precede wi (since B0 is binary) and is thus
equal to the (unnormalized) Kneser Ney unigram.
This idea also generalizes to higher order n-grams
and leads us to the following lemma:
</bodyText>
<equation confidence="0.87545625">
Lemma 2. Let B(ρ,κ) be the best rank κ ap-
proximation of B·ρ under gKL. Then bwi−1 s.t.
c(wi−1) =� 0:
Pw B(0,1)(w, wi−1)
</equation>
<bodyText confidence="0.787753">
For more general n, let Cn,(ρ,κ)
i−1,...,i−n+2 be the best
</bodyText>
<equation confidence="0.943905888888889">
rank κ approximation of Cn,(ρ)(:, ˜wi−1
i−n+2, :) un-
dergKL. Similarly, bwi−1
i−n+1 s.t. c(wi−1
i−n+1) &gt; 0:
Palt(wi|wi−1, ..., wi−n+2)
n,(0,1) ( i−1 )
Ci−1,...,i−n+2 wi, wi−n+1
=
</equation>
<sectionHeader confidence="0.973516" genericHeader="method">
4 Creating the Ensemble
</sectionHeader>
<bodyText confidence="0.99767025">
Recall our overall formulation in Eq. 3; a naive
solution would be to set Z1, ..., Zη to low rank
approximations of the count matrices/tensors un-
der varying powers, and then interpolate through
constant absolute discounting. Unfortunately, the
marginal constraint in Eq. 1 will generally not hold
if this strategy is used. Therefore, we propose a
generalized discounting scheme where each non-
zero n-gram count is associated with a different
discount Dj(wi, wi−1
i−n0+1). The low rank approxi-
mations are then computed on the discounted ma-
trices, leaving the marginal constraint intact.
For clarity of exposition, we focus on the spe-
cial case where n = 2 with only one low rank
matrix before stating our general algorithm:
</bodyText>
<equation confidence="0.994511">
Pplre(wi|wi−1) = bPD0(wi|wi−1)
� �
+ γ0(wi−1) ZD1(wi|wi−1) + γ1(wi−1)Palt(wi)
(6)
</equation>
<bodyText confidence="0.964674666666667">
Our goal is to compute D0, D1 and Z1 so
that the following lower order marginal constraint
holds:
</bodyText>
<equation confidence="0.9986385">
Pb(wi) = X Pplre(wi|wi−1) Pb(wi−1) (7)
wi−1
</equation>
<bodyText confidence="0.99402925">
Our solution can be thought of as a two-
step procedure where we compute the discounts
D0, D1 (and the γ(wi−1) weights as a by-
product), followed by the low rank quantity Z1.
First, we construct the following intermediate en-
semble of powered, but full rank terms. Let
Y ρj be the matrix such that Y ρj(wi, wi−1) :=
c(wi, wi−1)ρj. Then define
</bodyText>
<equation confidence="0.9986264">
Ppwr(wi|wi−1) := Y (ρ0 =1)(wi|wi−1)
D0
+ γ0 (wi−1) (YD11)
(wi  |wi−1)
�+ γ1(wi−1)Y (ρ2=0)(wi|wi−1) (8)
</equation>
<bodyText confidence="0.890712">
where with a little abuse of notation:
</bodyText>
<equation confidence="0.972401333333333">
ρj
Y�. (wi |wi−1) = C(wi, wi−1 − Dj(wi, wi−1)
Ewi C(wi, wi−1)ρj
</equation>
<bodyText confidence="0.999939384615385">
Note that Palt(wi) has been replaced with
Y (ρ2=0)(wi|wi−1), based on Lemma 2, and will
equal Palt(wi) once the low rank approximation is
taken as discussed in § 4.2).
Since we have only combined terms of differ-
ent power (but all full rank), it is natural choose
the discounts so that the result remains unchanged
i.e., Ppwr(wi|wi−1) = Pb(wi|wi−1), since the low
rank approximation (not the power) will imple-
ment smoothing. Enforcing this constraint gives
rise to a set of linear equations that can be solved
(in closed form) to obtain the discounts as we now
show below.
</bodyText>
<subsectionHeader confidence="0.995183">
4.1 Step 1: Computing the Discounts
</subsectionHeader>
<bodyText confidence="0.997019">
To ensure the constraint that Ppwr(wi|wi−1) =
Pb(wi|wi−1), it is sufficient to enforce the follow-
ing two local constraints:
</bodyText>
<equation confidence="0.99623925">
Y (ρj)(wi|wi−1) = Y (ρj)
Dj (wi|wi−1)
+ γj(wi−1)Y (ρj+1)(wi|wi−1) for j = 0,1
(9)
</equation>
<bodyText confidence="0.842006">
This allows each Dj to be solved for indepen-
dently of the other {Dj0}j06=j. Let ci,i−1 =
c(wi, wi−1), cji,i−1 = c(wi, wi−1)ρj, and dji,i−1 =
</bodyText>
<equation confidence="0.974976545454545">
Palt(wi) = B(0,1)(wi, wi−1)
n,(0,1) i−1 (5)
Pw Ci−1,...,i−n+2(w, wi−n+1)
1491
Dj(wi, wi−1). Expanding Eq. 9 yields that
∀wi, wi−1:
cj
i,i−1 =
Pi cji,i−1
P !
cji,i−1 − dj i dj cj+1
i,i−1 i,i−1 i,i−1
P + P P (10)
i cj i cj i cj+1
i,i−1 i,i−1 i,i−1
which can be rewritten as:
X ! +1
−dj j
i,i−1+ ci,i−1
i P=0(11)
i,i−1 i cj+1
i,i−1
</equation>
<bodyText confidence="0.999731916666667">
Note that Eq. 11 decouples across wi−1 since the
only dj i,i−1 terms that are dependent are the ones
that share the preceding context wi−1.
It is straightforward to see that setting dji,i−1
proportional to cj+1
i,i−1 satisfies Eq. 11. Furthermore
it can be shown that all solutions are of this form
(i.e., the linear system has a null space of exactly
one). Moreover, we are interested in a particular
subset of solutions where a single parameter d∗
(independent of wi−1) controls the scaling as in-
dicated by the following lemma:
</bodyText>
<equation confidence="0.754710375">
Lemma 3. Assume that ρj ≥ ρj+1. Choose any
0 ≤ d∗ ≤ 1. Set dji,i−1 = d∗cj+1
i,i−1 ∀i, j. The
resulting discounts satisfy Eq. 11 as well as the
inequality constraints 0 ≤ dj i,i−1 ≤ cj i,i−1. Fur-
thermore, the leftover weight γj takes the form:
P i dj i,i−1
γj(wi−1) =Pi cji,i−1
</equation>
<bodyText confidence="0.8805135">
Proof. Clearly this choice of dji,i−1 satisfies
Eq. 11. The largest possible value of dji,i−1 is
</bodyText>
<equation confidence="0.9674065">
cj+1
i,i−1. ρj ≥ ρj+1, implies cj i,i−1 ≥ cj+1
</equation>
<bodyText confidence="0.988554105263158">
i,i−1. Thus
the inequality constraints are met. It is then easy
to verify that γ takes the above form.
The above lemma generalizes to longer contexts
(i.e. n &gt; 2) as shown in Algorithm 1. Note that if
ρj = ρj+1 then Algorithm 1 is equivalent to scal-
ing the counts e.g. deleted-interpolation/Jelinek
Mercer smoothing (Jelinek and Mercer, 1980). On
the other hand, when ρj+1 = 0, Algorithm 1
is equal to the absolute discounting that is used
in Kneser-Ney. Thus, depending on ρj+1, our
method generalizes different types of interpola-
tion schemes to construct an ensemble so that the
marginal constraint is satisfied.
Algorithm 1 Compute D
In: Count tensor Cn, powers ρj, ρj+1 such that
ρj ≥ ρj+1, and parameter d∗.
Out: Discount Dj for powered counts Cn,(ρj)
and associated leftover weight γj
</bodyText>
<equation confidence="0.935314375">
1: Set Dj(wi, wi−1
i−n+1) = d∗c(wi, wi−1
i−n+1)ρj+1.
P
d∗ wi c(wi, wi−1
i−n+1)ρj+1
Pwi c(wi, wi−1
i−n+1)ρj
</equation>
<bodyText confidence="0.2955515">
Algorithm 2 Compute Z
In: Count tensor Cn, power ρ, discounts D, rank
κ
Out: Discounted low rank conditional probability
</bodyText>
<equation confidence="0.619181333333333">
table Z(ρ,κ)
D (wi|wi−1
i−n+1) (represented implicitly)
</equation>
<listItem confidence="0.736425333333333">
1: Compute powered counts Cn,(·ρ).
2: Compute denominators Pwi c(wi, wi−1
i−n+1)ρ
</listItem>
<equation confidence="0.970375428571429">
∀wi−1
i−n+1 s.t. c(wi−1
i−n+1) &gt; 0.
3: Compute discounted powered counts
Cn,(·ρ)
D = Cn,(·ρ) − D.
4: For each slice M˜wi−1i−n+2 := Cn,(·ρ)
D (:
, ˜wi−1
i−n+2, :) compute
k M˜i−1 − Ak KL
A≥0:rank(A)=κ wi−n+2
(stored implicitly as M(κ) = LR)
Set ZD,κ)(:,˜wi−n+2,:) = M(κ)
</equation>
<sectionHeader confidence="0.554523" genericHeader="method">
5: Note that
</sectionHeader>
<subsectionHeader confidence="0.948614">
4.2 Step 2: Computing Low Rank Quantities
</subsectionHeader>
<bodyText confidence="0.955916428571429">
The next step is to compute low rank approxi-
mations of Y (ρj) Dj to obtain ZDj such that the inter-
mediate marginal constraint in Eq. 7 is preserved.
This constraint trivially holds for the intermediate
ensemble Ppwr(wi|wi−1) due to how the discounts
were derived in § 4.1. For our running bigram ex-
ample, define Z(ρj,κj)
</bodyText>
<equation confidence="0.947114857142857">
Dj to be the best rank κj ap-
proximation to Y (ρj,κj) according to gKL and let
j
Zρj,κjDj (wi, wi−1)
Pwi c(wi, wi−1)ρj
Note that Zρj,κj
Dj (wi|wi−1) is a valid (discounted)
</equation>
<bodyText confidence="0.985935333333333">
conditional probability since gKL preserves
row/column sums so the denominator remains un-
changed under the low rank approximation. Then
</bodyText>
<equation confidence="0.993156272727273">
Z(ρ,κ) i−n+1)
D (wi|wi−1
i−n+1) = Z(ρ,κ)
D (wi, wi−1
P
c(wi, wi−1
i−n+1)ρ
wi
P
d∗ i cj+1
i,i−1
=
Pi cji,i−1
γj(wi, wi−1
i−n+1) =
M(κ) := min
Zρj,κjDj (wi|wi−1) =
1492
using the fact that Z(0,1)(wi|wi−1) = Palt(wi)
(Lemma 2) we can embellish Eq. 6 as
Pplre(wi|wi−1) = PD0(wi|wi−1)+
1)γ0(wi−1)((ρ1,κZD1 (wi|wi−1) + γ1(wi−1)Palt(wi)
</equation>
<bodyText confidence="0.996115375">
Leveraging the form of the discounts and
row/column sum preserving property of gKL, we
then have the following lemma (the proof is in the
supplementary material):
Lemma 4. Let Pplre(wi|wi−1) indicate the PLRE
smoothed conditional probability as computed by
Eq. 6 and Algorithms 1 and 2. Then, the marginal
constraint in Eq. 7 holds.
</bodyText>
<subsectionHeader confidence="0.995153">
4.3 More general algorithm
</subsectionHeader>
<bodyText confidence="0.996901909090909">
In general, the principles outlined in the previ-
ous sections hold for higher order n-grams. As-
sume that the discounts are computed according
to Algorithm 1 with parameter d* and ZD is
computed according to Algorithm 2. Note that, as
shown in Algorithm 2, for higher order n-grams,
the Z(ρj,κj)
Dj are created by taking low rank approx-
imations of slices of the (powered) count tensors
(see Lemma 2 for intuition). Eq. 3 can now be
embellished:
</bodyText>
<equation confidence="0.955118181818182">
Pplre(wi|wi−1
i−n+1) = P alt D0(wi|wi−1
i−n+1)
+ γ0(i−1 (ρ1,κ1) i−1
wi−n+1) (ZD1 (wi  |wi−n+1) + .....
i−1 (ρη,κη) i−1
+ γη−1(wi−n+1) ZDη (wi  |wi−n+1)
+ γη(i−1 i−1
wi−n+1) Pplre(wi|wi−n+2) ... (12)
Lemma 4 also applies in this case and is given in
Theorem 1 in the supplementary material.
</equation>
<subsectionHeader confidence="0.997766">
4.4 Links with KN Smoothing
</subsectionHeader>
<bodyText confidence="0.994023">
In this section, we explicitly show the relation-
ship between PLRE and KN smoothing. Rewrit-
ing Eq. 12 in the following form:
</bodyText>
<equation confidence="0.997486428571428">
Pplre(wi|wi−1
i−n+1) = P terms
plre (wi|wi−1
i−n+1)
+γ0:η(wi−1
i−n+1)Pplre(wi|wi−1
i−n+2) (13)
</equation>
<bodyText confidence="0.86984">
where P terms
</bodyText>
<equation confidence="0.7446096">
plre (wi|wi−1
i−n+1) contains the terms in
Eq. 12 except the last, and γ0:η(wi−1
i−n+1) =
Qη h=0 γh(wi−1
</equation>
<bodyText confidence="0.883613">
i−n+1), we can leverage the form of
the discount, and using the fact that ρη+1 = 02:
</bodyText>
<equation confidence="0.99702825">
i−1
i−1 d*η+1 N+(wi−n+1)
γ0:η(wi−n−1) = i−1
c(wi−n+1)
</equation>
<bodyText confidence="0.99817595">
With this form of γ(·), Eq. 13 is remarkably sim-
ilar to KN smoothing (Eq. 2) if KN’s discount pa-
rameter D is chosen to equal (d*)η+1.
The difference is that Palt(·) has been replaced
with the alternate estimate Pterms
plre (wi|wi−1
i−n+1),
which have been enriched via the low rank struc-
ture. Since these alternate estimates were con-
structed via our ensemble strategy they contain
both very fine-grained dependencies (the origi-
nal n-grams) as well as coarser dependencies (the
lower rank n-grams) and is thus fundamentally
different than simply taking a single matrix/tensor
decomposition of the trigram/bigram matrices.
Moreover, it provides a natural way of setting
d* based on the Good-Turing (GT) estimates em-
ployed by KN smoothing. In particular, we can set
d* to be the (η + 1)th root of the KN discount D
that can be estimated via the GT estimates.
</bodyText>
<subsectionHeader confidence="0.979091">
4.5 Computational Considerations
</subsectionHeader>
<bodyText confidence="0.981114083333333">
PLRE scales well even as the order n increases.
To compute a low rank bigram, one low rank ap-
proximation of a V × V matrix is required. For
the low rank trigram, we need to compute a low
rank approximation of each slice Cn,(·p)
D (:, ˜wi−1, :
) ∀ ˜wi−1. While this may seem daunting at first, in
practice the size of each slice (number of non-zero
rows/columns) is usually much, much smaller than
V , keeping the computation tractable.
Similarly, PLRE also evaluates conditional
probabilities at evaluation time efficiently. As
shown in Algorithm 2, the normalizer can be pre-
computed on the sparse powered matrix/tensor. As
a result our test complexity is O(Pηtotal
i=1 κi) where
ηtotal is the total number of matrices/tensors in
the ensemble. While this is larger than Kneser
Ney’s practically constant complexity of O(n),
it is much faster than other recent methods for
language modeling such as neural networks and
conditional exponential family models where ex-
act computation of the normalizing constant costs
O(V ).
</bodyText>
<sectionHeader confidence="0.999462" genericHeader="method">
5 Experiments
</sectionHeader>
<bodyText confidence="0.977712">
To evaluate PLRE, we compared its performance
on English and Russian corpora with several vari-
</bodyText>
<footnote confidence="0.9433795">
2for derivation see proof of Lemma 4 in the supplemen-
tary material
</footnote>
<page confidence="0.987407">
1493
</page>
<bodyText confidence="0.999887045454545">
ants of KN smoothing, class-based models, and
the log-bilinear neural language model (Mnih and
Hinton, 2007). We evaluated with perplexity in
most of our experiments, but also provide results
evaluated with BLEU (Papineni et al., 2002) on a
downstream machine translation (MT) task. We
have made the code for our approach publicly
available 3.
To build the hard class-based LMs, we utilized
mkcls4, a tool to train word classes that uses
the maximum likelihood criterion (Och, 1995) for
classing. We subsequently trained trigram class
language models on these classes (correspond-
ing to 2nd-order HMMs) using SRILM (Stolcke,
2002), with KN-smoothing for the class transition
probabilities. SRILM was also used for the base-
line KN-smoothed models.
For our MT evaluation, we built a hierarchi-
cal phrase translation (Chiang, 2007) system us-
ing cdec (Dyer et al., 2010). The KN-smoothed
models in the MT experiments were compiled us-
ing KenLM (Heafield, 2011).
</bodyText>
<subsectionHeader confidence="0.985923">
5.1 Datasets
</subsectionHeader>
<bodyText confidence="0.9998264">
For the perplexity experiments, we evaluated our
proposed approach on 4 datasets, 2 in English and
2 in Russian. In all cases, the singletons were re-
placed with “&lt;unk&gt;” tokens in the training cor-
pus, and any word not in the vocabulary was re-
placed with this token during evaluation. There is
a general dearth of evaluation on large-scale cor-
pora in morphologically rich languages such as
Russian, and thus we have made the processed
Large-Russian corpus available for comparison 3.
</bodyText>
<listItem confidence="0.989146642857143">
• Small-English: APNews corpus (Bengio et al.,
2003): Train - 14 million words, Dev - 963,000,
Test - 963,000. Vocabulary- 18,000 types.
• Small-Russian: Subset of Russian news com-
mentary data from 2013 WMT translation task5:
Train- 3.5 million words, Dev - 400,000 Test -
400,000. Vocabulary - 77,000 types.
• Large-English: English Gigaword, Training -
837 million words, Dev - 8.7 million, Test - 8.7
million. Vocabulary- 836,980 types.
• Large-Russian: Monolingual data from WMT
2013 task. Training - 521 million words, Vali-
dation - 50,000, Test - 50,000. Vocabulary- 1.3
million types.
</listItem>
<footnote confidence="0.98379425">
3http://www.cs.cmu.edu/∼apparikh/plre.html
4http://code.google.com/p/giza-pp/
5http://www.statmt.org/wmt13/training-monolingual-
nc-v8.tgz
</footnote>
<bodyText confidence="0.9993776">
For the MT evaluation, we used the parallel data
from the WMT 2013 shared task, excluding the
Common Crawl corpus data. The newstest2012
and newstest2013 evaluation sets were used as the
development and test sets respectively.
</bodyText>
<subsectionHeader confidence="0.999363">
5.2 Small Corpora
</subsectionHeader>
<bodyText confidence="0.999900488888889">
For the class-based baseline LMs, the
number of classes was selected from
{32, 64,128, 256, 512,1024} (Small-English)
and {512,1024} (Small-Russian). We could not
go higher due to the computationally laborious
process of hard clustering. For Kneser-Ney, we
explore four different variants: back-off (BO-KN)
interpolated (int-KN), modified back-off (BO-
MKN), and modified interpolated (int-MKN).
Good-Turing estimates were used for discounts.
All models trained on the small corpora are of
order 3 (trigrams).
For PLRE, we used one low rank bigram and
one low rank trigram in addition to the MLE n-
gram estimates. The powers of the intermediate
matrices/tensors were fixed to be 0.5 and the dis-
counts were set to be square roots of the Good Tur-
ing estimates (as explained in § 4.4). The ranks
were tuned on the development set. For Small-
English, the ranges were {1e − 3,5e − 3} (as a
fraction of the vocabulary size) for both the low
rank bigram and low rank trigram models. For
Small-Russian the ranges were {5e − 4, 1e − 3}
for both the low rank bigram and the low rank tri-
gram models.
The results are shown in Table 1. The best class-
based LM is reported, but is not competitive with
the KN baselines. PLRE outperforms all of the
baselines comfortably. Moreover, PLRE’s perfor-
mance over the baselines is highlighted in Russian.
With larger vocabulary sizes, the low rank ap-
proach is more effective as it can capture linguistic
similarities between rare and common words.
Next we discuss how the maximum n-gram or-
der affects performance. Figure 1 shows the rela-
tive percentage improvement of our approach over
int-MKN as the order is increased from 2 to 4 for
both methods. The Small-English dataset has a
rather small vocabulary compared to the number
of tokens, leading to lower data sparsity in the bi-
gram. Thus the PLRE improvement is small for
order = 2, but more substantial for order = 3. On
the other hand, for the Small-Russian dataset, the
vocabulary size is much larger and consequently
the bigram counts are sparser. This leads to sim-
</bodyText>
<page confidence="0.976413">
1494
</page>
<table confidence="0.9998372">
Dataset class-1024(3) BO-KN(3) int-KN(3) BO-MKN(3) int-MKN(3) PLRE(3)
Small-English Dev 115.64 99.20 99.73 99.95 95.63 91.18
Small-English Test 119.70 103.86 104.56 104.55 100.07 95.15
Small-Russian Dev 286.38 281.29 265.71 287.19 263.25 241.66
Small-Russian Test 284.09 277.74 262.02 283.70 260.19 238.96
</table>
<tableCaption confidence="0.999352">
Table 1: Perplexity results on small corpora for all methods.
</tableCaption>
<figureCaption confidence="0.9173635">
Figure 1: Relative percentage improvement of
PLRE over int-MKN as the maximum n-gram or-
</figureCaption>
<bodyText confidence="0.983867411764706">
der for both methods is increased.
ilar improvements for all orders (which are larger
than that for Small-English).
On both these datasets, we also experimented
with tuning the discounts for int-MKN to see if
the baseline could be improved with more careful
choices of discounts. However, this achieved only
marginal gains (reducing the perplexity to 98.94
on the Small-English test set and 259.0 on the
Small-Russian test set).
Comparison to LBL (Mnih and Hinton,
2007): Mnih and Hinton (2007) evaluate on the
Small-English dataset (but remove end markers
and concatenate the sentences). They obtain per-
plexities 117.0 and 107.8 using contexts of size 5
and 10 respectively. With this preprocessing, a 4-
gram (context 3) PLRE achieves 108.4 perplexity.
</bodyText>
<subsectionHeader confidence="0.999459">
5.3 Large Corpora
</subsectionHeader>
<bodyText confidence="0.99996">
Results on the larger corpora for the top 2 per-
forming methods “PLRE” and “int-MKN” are pre-
sented in Table 2. Due to the larger training size,
we use 4-gram models in these experiments. How-
ever, including the low rank 4-gram tensor pro-
vided little gain and therefore, the 4-gram PLRE
only has additional low rank bigram and low rank
trigram matrices/tensors. As above, ranks were
tuned on the development set. For Large-English,
the ranges were {1e − 4, 5e − 4, 1e − 3} (as a frac-
tion of the vocabulary size) for both the low rank
</bodyText>
<table confidence="0.9988272">
Dataset int-MKN(4) PLRE(4)
Large-English Dev 73.21 71.21
Large-English Test 77.90 f 0.203 75.66 f 0.189
Large-Russian Dev 326.9 297.11
Large-Russian Test 289.63 f 6.82 264.59 f 5.839
</table>
<tableCaption confidence="0.9594695">
Table 2: Mean perplexity results on large corpora,
with standard deviation.
</tableCaption>
<table confidence="0.9999332">
Dataset PLRE Training Time
Small-English 3.96 min ( order 3) / 8.3 min (order 4)
Small-Russian 4.0 min (order 3) / 4.75 min (order 4)
Large-English 3.2 hrs (order 4)
Large-Russian 8.3 hrs (order 4)
</table>
<tableCaption confidence="0.973433">
Table 3: PLRE training times for a fixed parameter
setting6. 8 Intel Xeon CPUs were used.
</tableCaption>
<table confidence="0.9997526">
Method BLEU
int-MKN(4) 17.63 f 0.11
PLRE(4) 17.79 f 0.07
Smallest Diff PLRE+0.05
Largest Diff PLRE+0.29
</table>
<tableCaption confidence="0.985202">
Table 4: Results on English-Russian translation
task (mean f stdev). See text for details.
</tableCaption>
<bodyText confidence="0.9992127">
bigram and low rank trigram models. For Small-
Russian the ranges were {1e−5, 5e−5, 1e−4} for
both the low rank bigram and the low rank trigram
models. For statistical validity, 10 test sets of size
equal to the original test set were generated by ran-
domly sampling sentences with replacement from
the original test set. Our method outperforms “int-
MKN” with gains similar to that on the smaller
datasets. As shown in Table 3, our method obtains
fast training times even for large datasets.
</bodyText>
<sectionHeader confidence="0.987463" genericHeader="method">
6 Machine Translation Task
</sectionHeader>
<bodyText confidence="0.999921142857143">
Table 4 presents results for the MT task, trans-
lating from English to Russian7. We used
MIRA (Chiang et al., 2008) to learn the feature
weights. To control for the randomness in MIRA,
we avoid retuning when switching LMs - the set
of feature weights obtained using int-MKN is the
same, only the language model changes. The
</bodyText>
<footnote confidence="0.9354934">
6As described earlier, only the ranks need to be tuned, so
only 2-3 low rank bigrams and 2-3 low rank trigrams need to
be computed (and combined depending on the setting).
7the best score at WMT 2013 was 19.9 (Bojar et al.,
2013)
</footnote>
<figure confidence="0.696732">
Small-English
Small-Russian
</figure>
<page confidence="0.970484">
1495
</page>
<bodyText confidence="0.9999385">
procedure is repeated 10 times to control for op-
timizer instability (Clark et al., 2011). Unlike
other recent approaches where an additional fea-
ture weight is tuned for the proposed model and
used in conjunction with KN smoothing (Vaswani
et al., 2013), our aim is to show the improvements
that PLRE provides as a substitute for KN. On av-
erage, PLRE outperforms the KN baseline by 0.16
BLEU, and this improvement is consistent in that
PLRE never gets a worse BLEU score.
</bodyText>
<sectionHeader confidence="0.999935" genericHeader="related work">
7 Related Work
</sectionHeader>
<bodyText confidence="0.995448833333333">
Recent attempts to revisit the language model-
ing problem have largely come from two direc-
tions: Bayesian nonparametrics and neural net-
works. Teh (2006) and Goldwater et al. (2006)
discovered the connection between interpolated
Kneser Ney and the hierarchical Pitman-Yor pro-
cess. These have led to generalizations that ac-
count for domain effects (Wood and Teh, 2009)
and unbounded contexts (Wood et al., 2009).
The idea of using neural networks for language
modeling is not new (Miikkulainen and Dyer,
1991), but recent efforts (Mnih and Hinton, 2007;
Mikolov et al., 2010) have achieved impressive
performance. These methods can be quite expen-
sive to train and query (especially as the vocab-
ulary size increases). Techniques such as noise
contrastive estimation (Gutmann and Hyv¨arinen,
2012; Mnih and Teh, 2012; Vaswani et al., 2013),
subsampling (Xu et al., 2011), or careful engi-
neering approaches for maximum entropy LMs
(which can also be applied to neural networks)
(Wu and Khudanpur, 2000) have improved train-
ing of these models, but querying the probabil-
ity of the next word given still requires explicitly
normalizing over the vocabulary, which is expen-
sive for big corpora or in languages with a large
number of word types. Mnih and Teh (2012) and
Vaswani et al. (2013) propose setting the normal-
ization constant to 1, but this is approximate and
thus can only be used for downstream evaluation,
not for perplexity computation. An alternate tech-
nique is to use word-classing (Goodman, 2001;
Mikolov et al., 2011), which can reduce the cost
√
of exact normalization to O( V ). In contrast, our
approach is much more scalable, since it is triv-
ially parallelized in training and does not require
explicit normalization during evaluation.
There are a few low rank approaches (Saul and
Pereira, 1997; Bellegarda, 2000; Hutchinson et al.,
2011), but they are only effective in restricted set-
tings (e.g. small training sets, or corpora divided
into documents) and do not generally perform
comparably to state-of-the-art models. Roark et
al. (2013) also use the idea of marginal constraints
for re-estimating back-off parameters for heavily-
pruned language models, whereas we use this con-
cept to estimate n-gram specific discounts.
</bodyText>
<sectionHeader confidence="0.99733" genericHeader="conclusions">
8 Conclusion
</sectionHeader>
<bodyText confidence="0.999914692307692">
We presented power low rank ensembles, a tech-
nique that generalizes existing n-gram smoothing
techniques to non-integer n. By using ensembles
of sparse as well as low rank matrices and ten-
sors, our method captures both the fine-grained
and coarse structures in word sequences. Our
discounting strategy preserves the marginal con-
straint and thus generalizes Kneser Ney, and un-
der slight changes can also extend other smooth-
ing methods such as deleted-interpolation/Jelinek-
Mercer smoothing. Experimentally, PLRE con-
vincingly outperforms Kneser-Ney smoothing as
well as class-based baselines.
</bodyText>
<sectionHeader confidence="0.994589" genericHeader="acknowledgments">
Acknowledgements
</sectionHeader>
<bodyText confidence="0.999882">
This work was supported by NSF IIS1218282,
NSF IIS1218749, NSF IIS1111142, NIH
R01GM093156, the U. S. Army Research Labo-
ratory and the U. S. Army Research Office under
contract/grant number W911NF-10-1-0533, the
NSF Graduate Research Fellowship Program
under Grant No. 0946825 (NSF Fellowship to
APP), and a grant from Ebay Inc. (to AS).
</bodyText>
<sectionHeader confidence="0.997759" genericHeader="references">
References
</sectionHeader>
<reference confidence="0.998432777777778">
Jerome R. Bellegarda. 2000. Large vocabulary speech
recognition with multispan statistical language mod-
els. IEEE Transactions on Speech and Audio Pro-
cessing, 8(1):76–84.
Yoshua Bengio, R´ejean Ducharme, Pascal Vincent, and
Christian Janvin. 2003. A neural probabilistic lan-
guage model. J. Mach. Learn. Res., 3:1137–1155,
March.
Ondˇrej Bojar, Christian Buck, Chris Callison-Burch,
Christian Federmann, Barry Haddow, Philipp
Koehn, Christof Monz, Matt Post, Radu Soricut, and
Lucia Specia. 2013. Findings of the 2013 Work-
shop on Statistical Machine Translation. In Pro-
ceedings of the Eighth Workshop on Statistical Ma-
chine Translation, pages 1–44, Sofia, Bulgaria, Au-
gust. Association for Computational Linguistics.
Jian-Feng Cai, Emmanuel J Cand`es, and Zuowei Shen.
2010. A singular value thresholding algorithm for
</reference>
<page confidence="0.929748">
1496
</page>
<reference confidence="0.997865283018868">
matrix completion. SIAM Journal on Optimization,
20(4):1956–1982.
Emmanuel J Cand`es and Benjamin Recht. 2009. Exact
matrix completion via convex optimization. Foun-
dations of Computational mathematics, 9(6):717–
772.
Stanley F. Chen and Joshua Goodman. 1999. An
empirical study of smoothing techniques for lan-
guage modeling. Computer Speech &amp; Language,
13(4):359–393.
Stanley F Chen and Ronald Rosenfeld. 2000. A survey
of smoothing techniques for me models. Speech and
Audio Processing, IEEE Transactions on, 8(1):37–
50.
Stanley F. Chen. 2009. Shrinking exponential lan-
guage models. In Proceedings of Human Lan-
guage Technologies: The 2009 Annual Conference
of the North American Chapter of the Association
for Computational Linguistics, NAACL ’09, pages
468–476, Stroudsburg, PA, USA. Association for
Computational Linguistics.
David Chiang, Yuval Marton, and Philip Resnik. 2008.
Online large-margin training of syntactic and struc-
tural translation features. In Proceedings of the Con-
ference on Empirical Methods in Natural Language
Processing, pages 224–233. Association for Com-
putational Linguistics.
David Chiang. 2007. Hierarchical phrase-based trans-
lation. Comput. Linguist., 33(2):201–228, June.
Jonathan H. Clark, Chris Dyer, Alon Lavie, and
Noah A. Smith. 2011. Better hypothesis testing for
statistical machine translation: Controlling for op-
timizer instability. In Proceedings of the 49th An-
nual Meeting of the Association for Computational
Linguistics: Human Language Technologies: Short
Papers - Volume 2, HLT ’11, pages 176–181.
Chris Dyer, Jonathan Weese, Hendra Setiawan, Adam
Lopez, Ferhan Ture, Vladimir Eidelman, Juri Gan-
itkevitch, Phil Blunsom, and Philip Resnik. 2010.
cdec: A decoder, alignment, and learning framework
for finite-state and context-free translation models.
In Proceedings of the ACL 2010 System Demonstra-
tions, pages 7–12. Association for Computational
Linguistics.
Sharon Goldwater, Thomas Griffiths, and Mark John-
son. 2006. Interpolating between types and tokens
by estimating power-law generators. In Advances in
Neural Information Processing Systems, volume 18.
Joshua Goodman. 2001. Classes for fast maximum
entropy training. In Acoustics, Speech, and Signal
Processing, 2001. Proceedings.(ICASSP’01). 2001
IEEE International Conference on, volume 1, pages
561–564. IEEE.
Michael Gutmann and Aapo Hyv¨arinen. 2012. Noise-
contrastive estimation of unnormalized statistical
models, with applications to natural image statistics.
Journal of Machine Learning Research, 13:307–
361.
Kenneth Heafield. 2011. KenLM: faster and smaller
language model queries. In Proceedings of the
EMNLP 2011 Sixth Workshop on Statistical Ma-
chine Translation, pages 187–197, Edinburgh, Scot-
land, United Kingdom, July.
Ngoc-Diep Ho and Paul Van Dooren. 2008. Non-
negative matrix factorization with fixed row and col-
umn sums. Linear Algebra and its Applications,
429(5):1020–1025.
Brian Hutchinson, Mari Ostendorf, and Maryam Fazel.
2011. Low rank language models for small training
sets. Signal Processing Letters, IEEE, 18(9):489–
492.
Frederick Jelinek and Robert Mercer. 1980. Interpo-
lated estimation of markov source parameters from
sparse data. Pattern recognition in practice.
Reinhard Kneser and Hermann Ney. 1995. Im-
proved backing-off for m-gram language modeling.
In Acoustics, Speech, and Signal Processing, 1995.
ICASSP-95., 1995 International Conference on, vol-
ume 1, pages 181–184. IEEE.
Philipp Koehn. 2010. Statistical Machine Translation.
Cambridge University Press, New York, NY, USA,
1st edition.
Yehuda Koren, Robert Bell, and Chris Volinsky. 2009.
Matrix factorization techniques for recommender
systems. Computer, 42(8):30–37.
Daniel D. Lee and H. Sebastian Seung. 2001. Algo-
rithms for non-negative matrix factorization. Ad-
vances in Neural Information Processing Systems,
13:556–562.
Lester Mackey, Ameet Talwalkar, and Michael I Jor-
dan. 2011. Divide-and-conquer matrix factoriza-
tion. arXiv preprint arXiv:1107.0789.
Christopher D Manning and Hinrich Sch¨utze. 1999.
Foundations of statistical natural language process-
ing, volume 999. MIT Press.
Risto Miikkulainen and Michael G. Dyer. 1991. Natu-
ral language processing with modular pdp networks
and distributed lexicon. Cognitive Science, 15:343–
399.
Tom Mikolov, Martin Karafit, Luk Burget, Jan ernock,
and Sanjeev Khudanpur. 2010. Recurrent neu-
ral network based language model. In Proceed-
ings of the 11th Annual Conference of the Interna-
tional Speech Communication Association (INTER-
SPEECH 2010), volume 2010, pages 1045–1048.
International Speech Communication Association.
</reference>
<page confidence="0.845883">
1497
</page>
<reference confidence="0.999815408602151">
Tomas Mikolov, Stefan Kombrink, Lukas Burget,
JH Cernocky, and Sanjeev Khudanpur. 2011.
Extensions of recurrent neural network language
model. In Acoustics, Speech and Signal Processing
(ICASSP), 2011 IEEE International Conference on,
pages 5528–5531. IEEE.
Andriy Mnih and Geoffrey Hinton. 2007. Three new
graphical models for statistical language modelling.
In Proceedings of the 24th international conference
on Machine learning, pages 641–648. ACM.
Andriy Mnih and Yee Whye Teh. 2012. A fast and
simple algorithm for training neural probabilistic
language models. In Proceedings of the Interna-
tional Conference on Machine Learning.
Anil Kumar Nelakanti, Cedric Archambeau, Julien
Mairal, Francis Bach, and Guillaume Bouchard.
2013. Structured penalties for log-linear language
models. In Proceedings of the 2013 Conference on
Empirical Methods in Natural Language Process-
ing, pages 233–243, Seattle, Washington, USA, Oc-
tober. Association for Computational Linguistics.
Hermann Ney, Ute Essen, and Reinhard Kneser.
1994. On Structuring Probabilistic Dependencies in
Stochastic Language Modelling. Computer Speech
and Language, 8:1–38.
Franz Josef Och. 1995. Maximum-likelihood-
sch¨atzung von wortkategorien mit verfahren der
kombinatorischen optimierung. Bachelor’s thesis
(Studienarbeit), University of Erlangen.
Kishore Papineni, Salim Roukos, Todd Ward, and Wei
jing Zhu. 2002. Bleu: a method for automatic eval-
uation of machine translation. pages 311–318.
Lawrence Rabiner and Biing-Hwang Juang. 1993.
Fundamentals of speech recognition.
Brian Roark, Cyril Allauzen, and Michael Riley. 2013.
Smoothed marginal distribution constraints for lan-
guage modeling. In Proceedings of the 51st Annual
Meeting of the Association for Computational Lin-
guistics (ACL), pages 43–52.
Ruslan Salakhutdinov and Andriy Mnih. 2008.
Bayesian probabilistic matrix factorization using
Markov chain Monte Carlo. In Proceedings of the
25th international conference on Machine learning,
pages 880–887. ACM.
Lawrence Saul and Fernando Pereira. 1997. Aggre-
gate and mixed-order markov models for statistical
language processing. In Proceedings of the sec-
ond conference on empirical methods in natural lan-
guage processing, pages 81–89. Somerset, New Jer-
sey: Association for Computational Linguistics.
Andreas Stolcke. 2002. SRILM - An Extensible Lan-
guage Modeling Toolkit. In Proceedings of the In-
ternational Conference in Spoken Language Pro-
cessing.
Xiaoyuan Su and Taghi M Khoshgoftaar. 2009. A sur-
vey of collaborative filtering techniques. Advances
in artificial intelligence, 2009:4.
Yee Whye Teh. 2006. A hierarchical bayesian lan-
guage model based on pitman-yor processes. In
Proceedings of the 21st International Conference
on Computational Linguistics and the 44th annual
meeting of the Association for Computational Lin-
guistics, pages 985–992. Association for Computa-
tional Linguistics.
Ashish Vaswani, Yinggong Zhao, Victoria Fossum,
and David Chiang. 2013. Decoding with large-
scale neural language models improves translation.
In Proceedings of the 2013 Conference on Em-
pirical Methods in Natural Language Processing,
pages 1387–1392, Seattle, Washington, USA, Oc-
tober. Association for Computational Linguistics.
Frank Wood and Yee Whye Teh. 2009. A hierarchical
nonparametric Bayesian approach to statistical lan-
guage model domain adaptation. In Artificial Intel-
ligence and Statistics, pages 607–614.
Frank Wood, C´edric Archambeau, Jan Gasthaus,
Lancelot James, and Yee Whye Teh. 2009. A
stochastic memoizer for sequence data. In Proceed-
ings of the 26th Annual International Conference on
Machine Learning, pages 1129–1136. ACM.
Jun Wu and Sanjeev Khudanpur. 2000. Efficient train-
ing methods for maximum entropy language model-
ing. In Interspeech, pages 114–118.
Puyang Xu, Asela Gunawardana, and Sanjeev Khu-
danpur. 2011. Efficient subsampling for training
complex language models. In Proceedings of the
Conference on Empirical Methods in Natural Lan-
guage Processing, EMNLP ’11, pages 1128–1136,
Stroudsburg, PA, USA. Association for Computa-
tional Linguistics.
George Zipf. 1949. Human behaviour and the prin-
ciple of least-effort. Addison-Wesley, Cambridge,
MA.
</reference>
<page confidence="0.994212">
1498
</page>
</variant>
</algorithm>
<algorithm name="ParsHed" version="110505">
<variant no="0" confidence="0.925237">
<title confidence="0.999784">Language Modeling with Power Low Rank Ensembles</title>
<author confidence="0.988716">P Ankur</author>
<affiliation confidence="0.997355">School of Computer Carnegie Mellon</affiliation>
<email confidence="0.996586">apparikh@cs.cmu.edu</email>
<author confidence="0.958991">Chris</author>
<affiliation confidence="0.9978505">School of Computer Carnegie Mellon</affiliation>
<email confidence="0.998742">cdyer@cs.cmu.edu</email>
<abstract confidence="0.999339882352941">We present power low rank ensembles a flexible framework for language modeling where ensembles of low rank matrices and tensors are used to obtain smoothed probability estimates of words in context. Our method can understood as a generalization of modeling to non-integer and includes standard techniques such as absolute discounting and Kneser-Ney smoothing as special cases. PLRE training is efficient and our approach outperforms stateof-the-art modified Kneser Ney baselines in terms of perplexity on large corpora as well as on BLEU score in a downstream machine translation task.</abstract>
</variant>
</algorithm>
<algorithm name="ParsCit" version="110505">
<citationList>
<citation valid="true">
<authors>
<author>Jerome R Bellegarda</author>
</authors>
<title>Large vocabulary speech recognition with multispan statistical language models.</title>
<date>2000</date>
<journal>IEEE Transactions on Speech and Audio Processing,</journal>
<volume>8</volume>
<issue>1</issue>
<contexts>
<context position="36722" citStr="Bellegarda, 2000" startWordPosition="6157" endWordPosition="6158">large number of word types. Mnih and Teh (2012) and Vaswani et al. (2013) propose setting the normalization constant to 1, but this is approximate and thus can only be used for downstream evaluation, not for perplexity computation. An alternate technique is to use word-classing (Goodman, 2001; Mikolov et al., 2011), which can reduce the cost √ of exact normalization to O( V ). In contrast, our approach is much more scalable, since it is trivially parallelized in training and does not require explicit normalization during evaluation. There are a few low rank approaches (Saul and Pereira, 1997; Bellegarda, 2000; Hutchinson et al., 2011), but they are only effective in restricted settings (e.g. small training sets, or corpora divided into documents) and do not generally perform comparably to state-of-the-art models. Roark et al. (2013) also use the idea of marginal constraints for re-estimating back-off parameters for heavilypruned language models, whereas we use this concept to estimate n-gram specific discounts. 8 Conclusion We presented power low rank ensembles, a technique that generalizes existing n-gram smoothing techniques to non-integer n. By using ensembles of sparse as well as low rank matr</context>
</contexts>
<marker>Bellegarda, 2000</marker>
<rawString>Jerome R. Bellegarda. 2000. Large vocabulary speech recognition with multispan statistical language models. IEEE Transactions on Speech and Audio Processing, 8(1):76–84.</rawString>
</citation>
<citation valid="true">
<authors>
<author>Yoshua Bengio</author>
<author>R´ejean Ducharme</author>
<author>Pascal Vincent</author>
<author>Christian Janvin</author>
</authors>
<title>A neural probabilistic language model.</title>
<date>2003</date>
<journal>J. Mach. Learn. Res.,</journal>
<pages>3--1137</pages>
<contexts>
<context position="4712" citStr="Bengio et al., 2003" startWordPosition="725" endWordPosition="728">t instead altered quantities of these counts based on an element-wise power operation, similar to how some smoothing methods modify their lower order distributions. Moreover, PLRE has two key aspects that lead to easy scalability for large corpora and vocabularies. First, since it utilizes the original n-grams, the ranks required for the low rank matrices and tensors tend to be remain tractable (e.g. around 100 for a vocabulary size V ≈ 1 × 106) leading to fast training times. This differentiates our approach over other methods that leverage an underlying latent space such as neural networks (Bengio et al., 2003; Mnih and Hinton, 2007; Mikolov et al., 2010) or soft-class models (Saul and Pereira, 1997) where the underlying dimension is required to be quite large to obtain good performance. Moreover, at test time, the probability of a sequence can be queried in time O(κmax) where κmax is the maximum rank of the low rank matrices/tensors used. While this is larger than Kneser Ney’s virtually constant query time, it is substantially faster than conditional exponential family models (Chen and Rosenfeld, 2000; Chen, 2009; Nelakanti et al., 2013) and neural networks which require O(V ) for exact computatio</context>
<context position="27842" citStr="Bengio et al., 2003" startWordPosition="4702" endWordPosition="4705">models in the MT experiments were compiled using KenLM (Heafield, 2011). 5.1 Datasets For the perplexity experiments, we evaluated our proposed approach on 4 datasets, 2 in English and 2 in Russian. In all cases, the singletons were replaced with “&lt;unk&gt;” tokens in the training corpus, and any word not in the vocabulary was replaced with this token during evaluation. There is a general dearth of evaluation on large-scale corpora in morphologically rich languages such as Russian, and thus we have made the processed Large-Russian corpus available for comparison 3. • Small-English: APNews corpus (Bengio et al., 2003): Train - 14 million words, Dev - 963,000, Test - 963,000. Vocabulary- 18,000 types. • Small-Russian: Subset of Russian news commentary data from 2013 WMT translation task5: Train- 3.5 million words, Dev - 400,000 Test - 400,000. Vocabulary - 77,000 types. • Large-English: English Gigaword, Training - 837 million words, Dev - 8.7 million, Test - 8.7 million. Vocabulary- 836,980 types. • Large-Russian: Monolingual data from WMT 2013 task. Training - 521 million words, Validation - 50,000, Test - 50,000. Vocabulary- 1.3 million types. 3http://www.cs.cmu.edu/∼apparikh/plre.html 4http://code.googl</context>
</contexts>
<marker>Bengio, Ducharme, Vincent, Janvin, 2003</marker>
<rawString>Yoshua Bengio, R´ejean Ducharme, Pascal Vincent, and Christian Janvin. 2003. A neural probabilistic language model. J. Mach. Learn. Res., 3:1137–1155, March.</rawString>
</citation>
<citation valid="true">
<authors>
<author>Ondˇrej Bojar</author>
<author>Christian Buck</author>
<author>Chris Callison-Burch</author>
<author>Christian Federmann</author>
<author>Barry Haddow</author>
<author>Philipp Koehn</author>
<author>Christof Monz</author>
<author>Matt Post</author>
<author>Radu Soricut</author>
<author>Lucia Specia</author>
</authors>
<date>2013</date>
<booktitle>Findings of the 2013 Workshop on Statistical Machine Translation. In Proceedings of the Eighth Workshop on Statistical Machine Translation,</booktitle>
<pages>1--44</pages>
<publisher>Association</publisher>
<institution>for Computational Linguistics.</institution>
<location>Sofia, Bulgaria,</location>
<contexts>
<context position="34380" citStr="Bojar et al., 2013" startWordPosition="5771" endWordPosition="5774">training times even for large datasets. 6 Machine Translation Task Table 4 presents results for the MT task, translating from English to Russian7. We used MIRA (Chiang et al., 2008) to learn the feature weights. To control for the randomness in MIRA, we avoid retuning when switching LMs - the set of feature weights obtained using int-MKN is the same, only the language model changes. The 6As described earlier, only the ranks need to be tuned, so only 2-3 low rank bigrams and 2-3 low rank trigrams need to be computed (and combined depending on the setting). 7the best score at WMT 2013 was 19.9 (Bojar et al., 2013) Small-English Small-Russian 1495 procedure is repeated 10 times to control for optimizer instability (Clark et al., 2011). Unlike other recent approaches where an additional feature weight is tuned for the proposed model and used in conjunction with KN smoothing (Vaswani et al., 2013), our aim is to show the improvements that PLRE provides as a substitute for KN. On average, PLRE outperforms the KN baseline by 0.16 BLEU, and this improvement is consistent in that PLRE never gets a worse BLEU score. 7 Related Work Recent attempts to revisit the language modeling problem have largely come from </context>
</contexts>
<marker>Bojar, Buck, Callison-Burch, Federmann, Haddow, Koehn, Monz, Post, Soricut, Specia, 2013</marker>
<rawString>Ondˇrej Bojar, Christian Buck, Chris Callison-Burch, Christian Federmann, Barry Haddow, Philipp Koehn, Christof Monz, Matt Post, Radu Soricut, and Lucia Specia. 2013. Findings of the 2013 Workshop on Statistical Machine Translation. In Proceedings of the Eighth Workshop on Statistical Machine Translation, pages 1–44, Sofia, Bulgaria, August. Association for Computational Linguistics.</rawString>
</citation>
<citation valid="true">
<authors>
<author>Jian-Feng Cai</author>
<author>Emmanuel J Cand`es</author>
<author>Zuowei Shen</author>
</authors>
<title>A singular value thresholding algorithm for matrix completion.</title>
<date>2010</date>
<journal>SIAM Journal on Optimization,</journal>
<volume>20</volume>
<issue>4</issue>
<marker>Cai, Cand`es, Shen, 2010</marker>
<rawString>Jian-Feng Cai, Emmanuel J Cand`es, and Zuowei Shen. 2010. A singular value thresholding algorithm for matrix completion. SIAM Journal on Optimization, 20(4):1956–1982.</rawString>
</citation>
<citation valid="true">
<authors>
<author>Emmanuel J Cand`es</author>
<author>Benjamin Recht</author>
</authors>
<title>Exact matrix completion via convex optimization.</title>
<date>2009</date>
<journal>Foundations of Computational mathematics,</journal>
<volume>9</volume>
<issue>6</issue>
<pages>772</pages>
<marker>Cand`es, Recht, 2009</marker>
<rawString>Emmanuel J Cand`es and Benjamin Recht. 2009. Exact matrix completion via convex optimization. Foundations of Computational mathematics, 9(6):717– 772.</rawString>
</citation>
<citation valid="true">
<authors>
<author>Stanley F Chen</author>
<author>Joshua Goodman</author>
</authors>
<title>An empirical study of smoothing techniques for language modeling.</title>
<date>1999</date>
<journal>Computer Speech &amp; Language,</journal>
<volume>13</volume>
<issue>4</issue>
<contexts>
<context position="2237" citStr="Chen and Goodman, 1999" startWordPosition="333" endWordPosition="336">signs zero probability to legitimate word sequences that happen not to have been observed in the training data (Manning and Sch¨utze, 1999). Avneesh Saluja Electrical &amp; Computer Engineering Carnegie Mellon University avneesh@cs.cmu.edu Eric P. Xing School of Computer Science Carnegie Mellon University epxing@cs.cmu.edu Many smoothing techniques have been proposed to address the estimation challenge. These reassign probability mass (generally from overestimated events) to unseen word sequences, whose probabilities are estimated by interpolating with or backing off to lower order n-gram models (Chen and Goodman, 1999). Somewhat surprisingly, these widely used smoothing techniques differ substantially from techniques for coping with data sparsity in other domains, such as collaborative filtering (Koren et al., 2009; Su and Khoshgoftaar, 2009) or matrix completion (Cand`es and Recht, 2009; Cai et al., 2010). In these areas, low rank approaches based on matrix factorization play a central role (Lee and Seung, 2001; Salakhutdinov and Mnih, 2008; Mackey et al., 2011). For example, in recommender systems, a key challenge is dealing with the sparsity of ratings from a single user, since typical users will have ra</context>
</contexts>
<marker>Chen, Goodman, 1999</marker>
<rawString>Stanley F. Chen and Joshua Goodman. 1999. An empirical study of smoothing techniques for language modeling. Computer Speech &amp; Language, 13(4):359–393.</rawString>
</citation>
<citation valid="true">
<authors>
<author>Stanley F Chen</author>
<author>Ronald Rosenfeld</author>
</authors>
<title>A survey of smoothing techniques for me models. Speech and Audio Processing,</title>
<date>2000</date>
<journal>IEEE Transactions on,</journal>
<volume>8</volume>
<issue>1</issue>
<pages>50</pages>
<contexts>
<context position="5214" citStr="Chen and Rosenfeld, 2000" startWordPosition="808" endWordPosition="811">ates our approach over other methods that leverage an underlying latent space such as neural networks (Bengio et al., 2003; Mnih and Hinton, 2007; Mikolov et al., 2010) or soft-class models (Saul and Pereira, 1997) where the underlying dimension is required to be quite large to obtain good performance. Moreover, at test time, the probability of a sequence can be queried in time O(κmax) where κmax is the maximum rank of the low rank matrices/tensors used. While this is larger than Kneser Ney’s virtually constant query time, it is substantially faster than conditional exponential family models (Chen and Rosenfeld, 2000; Chen, 2009; Nelakanti et al., 2013) and neural networks which require O(V ) for exact computation of the normalization constant. See Section 7 for a more detailed discussion of related work. Outline: We first review existing n-gram smoothing methods (§2) and then present the intuition behind the key components of our technique: rank (§3.1) and power (§3.2). We then show how these can be interpolated into an ensemble (§4). In the experimental evaluation on English and Russian corpora (§5), we find that PLRE outperforms Kneser-Ney smoothing and all its variants, as well as class-based language</context>
</contexts>
<marker>Chen, Rosenfeld, 2000</marker>
<rawString>Stanley F Chen and Ronald Rosenfeld. 2000. A survey of smoothing techniques for me models. Speech and Audio Processing, IEEE Transactions on, 8(1):37– 50.</rawString>
</citation>
<citation valid="true">
<authors>
<author>Stanley F Chen</author>
</authors>
<title>Shrinking exponential language models.</title>
<date>2009</date>
<booktitle>In Proceedings of Human Language Technologies: The 2009 Annual Conference of the North American Chapter of the Association for Computational Linguistics, NAACL ’09,</booktitle>
<pages>468--476</pages>
<publisher>Association</publisher>
<institution>for Computational Linguistics.</institution>
<location>Stroudsburg, PA, USA.</location>
<contexts>
<context position="5226" citStr="Chen, 2009" startWordPosition="812" endWordPosition="813">er methods that leverage an underlying latent space such as neural networks (Bengio et al., 2003; Mnih and Hinton, 2007; Mikolov et al., 2010) or soft-class models (Saul and Pereira, 1997) where the underlying dimension is required to be quite large to obtain good performance. Moreover, at test time, the probability of a sequence can be queried in time O(κmax) where κmax is the maximum rank of the low rank matrices/tensors used. While this is larger than Kneser Ney’s virtually constant query time, it is substantially faster than conditional exponential family models (Chen and Rosenfeld, 2000; Chen, 2009; Nelakanti et al., 2013) and neural networks which require O(V ) for exact computation of the normalization constant. See Section 7 for a more detailed discussion of related work. Outline: We first review existing n-gram smoothing methods (§2) and then present the intuition behind the key components of our technique: rank (§3.1) and power (§3.2). We then show how these can be interpolated into an ensemble (§4). In the experimental evaluation on English and Russian corpora (§5), we find that PLRE outperforms Kneser-Ney smoothing and all its variants, as well as class-based language models. We </context>
</contexts>
<marker>Chen, 2009</marker>
<rawString>Stanley F. Chen. 2009. Shrinking exponential language models. In Proceedings of Human Language Technologies: The 2009 Annual Conference of the North American Chapter of the Association for Computational Linguistics, NAACL ’09, pages 468–476, Stroudsburg, PA, USA. Association for Computational Linguistics.</rawString>
</citation>
<citation valid="true">
<authors>
<author>David Chiang</author>
<author>Yuval Marton</author>
<author>Philip Resnik</author>
</authors>
<title>Online large-margin training of syntactic and structural translation features.</title>
<date>2008</date>
<booktitle>In Proceedings of the Conference on Empirical Methods in Natural Language Processing,</booktitle>
<pages>224--233</pages>
<publisher>Association</publisher>
<institution>for Computational Linguistics.</institution>
<contexts>
<context position="33942" citStr="Chiang et al., 2008" startWordPosition="5692" endWordPosition="5695">d low rank trigram models. For SmallRussian the ranges were {1e−5, 5e−5, 1e−4} for both the low rank bigram and the low rank trigram models. For statistical validity, 10 test sets of size equal to the original test set were generated by randomly sampling sentences with replacement from the original test set. Our method outperforms “intMKN” with gains similar to that on the smaller datasets. As shown in Table 3, our method obtains fast training times even for large datasets. 6 Machine Translation Task Table 4 presents results for the MT task, translating from English to Russian7. We used MIRA (Chiang et al., 2008) to learn the feature weights. To control for the randomness in MIRA, we avoid retuning when switching LMs - the set of feature weights obtained using int-MKN is the same, only the language model changes. The 6As described earlier, only the ranks need to be tuned, so only 2-3 low rank bigrams and 2-3 low rank trigrams need to be computed (and combined depending on the setting). 7the best score at WMT 2013 was 19.9 (Bojar et al., 2013) Small-English Small-Russian 1495 procedure is repeated 10 times to control for optimizer instability (Clark et al., 2011). Unlike other recent approaches where a</context>
</contexts>
<marker>Chiang, Marton, Resnik, 2008</marker>
<rawString>David Chiang, Yuval Marton, and Philip Resnik. 2008. Online large-margin training of syntactic and structural translation features. In Proceedings of the Conference on Empirical Methods in Natural Language Processing, pages 224–233. Association for Computational Linguistics.</rawString>
</citation>
<citation valid="true">
<authors>
<author>David Chiang</author>
</authors>
<title>Hierarchical phrase-based translation.</title>
<date>2007</date>
<journal>Comput. Linguist.,</journal>
<volume>33</volume>
<issue>2</issue>
<contexts>
<context position="27166" citStr="Chiang, 2007" startWordPosition="4591" endWordPosition="4592">Papineni et al., 2002) on a downstream machine translation (MT) task. We have made the code for our approach publicly available 3. To build the hard class-based LMs, we utilized mkcls4, a tool to train word classes that uses the maximum likelihood criterion (Och, 1995) for classing. We subsequently trained trigram class language models on these classes (corresponding to 2nd-order HMMs) using SRILM (Stolcke, 2002), with KN-smoothing for the class transition probabilities. SRILM was also used for the baseline KN-smoothed models. For our MT evaluation, we built a hierarchical phrase translation (Chiang, 2007) system using cdec (Dyer et al., 2010). The KN-smoothed models in the MT experiments were compiled using KenLM (Heafield, 2011). 5.1 Datasets For the perplexity experiments, we evaluated our proposed approach on 4 datasets, 2 in English and 2 in Russian. In all cases, the singletons were replaced with “&lt;unk&gt;” tokens in the training corpus, and any word not in the vocabulary was replaced with this token during evaluation. There is a general dearth of evaluation on large-scale corpora in morphologically rich languages such as Russian, and thus we have made the processed Large-Russian corpus avai</context>
</contexts>
<marker>Chiang, 2007</marker>
<rawString>David Chiang. 2007. Hierarchical phrase-based translation. Comput. Linguist., 33(2):201–228, June.</rawString>
</citation>
<citation valid="true">
<authors>
<author>Jonathan H Clark</author>
<author>Chris Dyer</author>
<author>Alon Lavie</author>
<author>Noah A Smith</author>
</authors>
<title>Better hypothesis testing for statistical machine translation: Controlling for optimizer instability.</title>
<date>2011</date>
<booktitle>In Proceedings of the 49th Annual Meeting of the Association for Computational Linguistics: Human Language Technologies: Short Papers - Volume 2, HLT ’11,</booktitle>
<pages>176--181</pages>
<contexts>
<context position="34502" citStr="Clark et al., 2011" startWordPosition="5789" endWordPosition="5792">rom English to Russian7. We used MIRA (Chiang et al., 2008) to learn the feature weights. To control for the randomness in MIRA, we avoid retuning when switching LMs - the set of feature weights obtained using int-MKN is the same, only the language model changes. The 6As described earlier, only the ranks need to be tuned, so only 2-3 low rank bigrams and 2-3 low rank trigrams need to be computed (and combined depending on the setting). 7the best score at WMT 2013 was 19.9 (Bojar et al., 2013) Small-English Small-Russian 1495 procedure is repeated 10 times to control for optimizer instability (Clark et al., 2011). Unlike other recent approaches where an additional feature weight is tuned for the proposed model and used in conjunction with KN smoothing (Vaswani et al., 2013), our aim is to show the improvements that PLRE provides as a substitute for KN. On average, PLRE outperforms the KN baseline by 0.16 BLEU, and this improvement is consistent in that PLRE never gets a worse BLEU score. 7 Related Work Recent attempts to revisit the language modeling problem have largely come from two directions: Bayesian nonparametrics and neural networks. Teh (2006) and Goldwater et al. (2006) discovered the connect</context>
</contexts>
<marker>Clark, Dyer, Lavie, Smith, 2011</marker>
<rawString>Jonathan H. Clark, Chris Dyer, Alon Lavie, and Noah A. Smith. 2011. Better hypothesis testing for statistical machine translation: Controlling for optimizer instability. In Proceedings of the 49th Annual Meeting of the Association for Computational Linguistics: Human Language Technologies: Short Papers - Volume 2, HLT ’11, pages 176–181.</rawString>
</citation>
<citation valid="true">
<authors>
<author>Chris Dyer</author>
<author>Jonathan Weese</author>
<author>Hendra Setiawan</author>
<author>Adam Lopez</author>
<author>Ferhan Ture</author>
<author>Vladimir Eidelman</author>
<author>Juri Ganitkevitch</author>
<author>Phil Blunsom</author>
<author>Philip Resnik</author>
</authors>
<title>cdec: A decoder, alignment, and learning framework for finite-state and context-free translation models.</title>
<date>2010</date>
<booktitle>In Proceedings of the ACL 2010 System Demonstrations,</booktitle>
<pages>7--12</pages>
<publisher>Association</publisher>
<institution>for Computational Linguistics.</institution>
<contexts>
<context position="27204" citStr="Dyer et al., 2010" startWordPosition="4597" endWordPosition="4600">tream machine translation (MT) task. We have made the code for our approach publicly available 3. To build the hard class-based LMs, we utilized mkcls4, a tool to train word classes that uses the maximum likelihood criterion (Och, 1995) for classing. We subsequently trained trigram class language models on these classes (corresponding to 2nd-order HMMs) using SRILM (Stolcke, 2002), with KN-smoothing for the class transition probabilities. SRILM was also used for the baseline KN-smoothed models. For our MT evaluation, we built a hierarchical phrase translation (Chiang, 2007) system using cdec (Dyer et al., 2010). The KN-smoothed models in the MT experiments were compiled using KenLM (Heafield, 2011). 5.1 Datasets For the perplexity experiments, we evaluated our proposed approach on 4 datasets, 2 in English and 2 in Russian. In all cases, the singletons were replaced with “&lt;unk&gt;” tokens in the training corpus, and any word not in the vocabulary was replaced with this token during evaluation. There is a general dearth of evaluation on large-scale corpora in morphologically rich languages such as Russian, and thus we have made the processed Large-Russian corpus available for comparison 3. • Small-Englis</context>
</contexts>
<marker>Dyer, Weese, Setiawan, Lopez, Ture, Eidelman, Ganitkevitch, Blunsom, Resnik, 2010</marker>
<rawString>Chris Dyer, Jonathan Weese, Hendra Setiawan, Adam Lopez, Ferhan Ture, Vladimir Eidelman, Juri Ganitkevitch, Phil Blunsom, and Philip Resnik. 2010. cdec: A decoder, alignment, and learning framework for finite-state and context-free translation models. In Proceedings of the ACL 2010 System Demonstrations, pages 7–12. Association for Computational Linguistics.</rawString>
</citation>
<citation valid="true">
<authors>
<author>Sharon Goldwater</author>
<author>Thomas Griffiths</author>
<author>Mark Johnson</author>
</authors>
<title>Interpolating between types and tokens by estimating power-law generators.</title>
<date>2006</date>
<booktitle>In Advances in Neural Information Processing Systems,</booktitle>
<volume>18</volume>
<contexts>
<context position="35079" citStr="Goldwater et al. (2006)" startWordPosition="5888" endWordPosition="5891">for optimizer instability (Clark et al., 2011). Unlike other recent approaches where an additional feature weight is tuned for the proposed model and used in conjunction with KN smoothing (Vaswani et al., 2013), our aim is to show the improvements that PLRE provides as a substitute for KN. On average, PLRE outperforms the KN baseline by 0.16 BLEU, and this improvement is consistent in that PLRE never gets a worse BLEU score. 7 Related Work Recent attempts to revisit the language modeling problem have largely come from two directions: Bayesian nonparametrics and neural networks. Teh (2006) and Goldwater et al. (2006) discovered the connection between interpolated Kneser Ney and the hierarchical Pitman-Yor process. These have led to generalizations that account for domain effects (Wood and Teh, 2009) and unbounded contexts (Wood et al., 2009). The idea of using neural networks for language modeling is not new (Miikkulainen and Dyer, 1991), but recent efforts (Mnih and Hinton, 2007; Mikolov et al., 2010) have achieved impressive performance. These methods can be quite expensive to train and query (especially as the vocabulary size increases). Techniques such as noise contrastive estimation (Gutmann and Hyv¨</context>
</contexts>
<marker>Goldwater, Griffiths, Johnson, 2006</marker>
<rawString>Sharon Goldwater, Thomas Griffiths, and Mark Johnson. 2006. Interpolating between types and tokens by estimating power-law generators. In Advances in Neural Information Processing Systems, volume 18.</rawString>
</citation>
<citation valid="true">
<authors>
<author>Joshua Goodman</author>
</authors>
<title>Classes for fast maximum entropy training.</title>
<date>2001</date>
<booktitle>In Acoustics, Speech, and Signal Processing,</booktitle>
<volume>1</volume>
<pages>561--564</pages>
<publisher>IEEE.</publisher>
<contexts>
<context position="36399" citStr="Goodman, 2001" startWordPosition="6103" endWordPosition="6104">ing approaches for maximum entropy LMs (which can also be applied to neural networks) (Wu and Khudanpur, 2000) have improved training of these models, but querying the probability of the next word given still requires explicitly normalizing over the vocabulary, which is expensive for big corpora or in languages with a large number of word types. Mnih and Teh (2012) and Vaswani et al. (2013) propose setting the normalization constant to 1, but this is approximate and thus can only be used for downstream evaluation, not for perplexity computation. An alternate technique is to use word-classing (Goodman, 2001; Mikolov et al., 2011), which can reduce the cost √ of exact normalization to O( V ). In contrast, our approach is much more scalable, since it is trivially parallelized in training and does not require explicit normalization during evaluation. There are a few low rank approaches (Saul and Pereira, 1997; Bellegarda, 2000; Hutchinson et al., 2011), but they are only effective in restricted settings (e.g. small training sets, or corpora divided into documents) and do not generally perform comparably to state-of-the-art models. Roark et al. (2013) also use the idea of marginal constraints for re</context>
</contexts>
<marker>Goodman, 2001</marker>
<rawString>Joshua Goodman. 2001. Classes for fast maximum entropy training. In Acoustics, Speech, and Signal Processing, 2001. Proceedings.(ICASSP’01). 2001 IEEE International Conference on, volume 1, pages 561–564. IEEE.</rawString>
</citation>
<citation valid="true">
<authors>
<author>Michael Gutmann</author>
<author>Aapo Hyv¨arinen</author>
</authors>
<title>Noisecontrastive estimation of unnormalized statistical models, with applications to natural image statistics.</title>
<date>2012</date>
<journal>Journal of Machine Learning Research,</journal>
<volume>13</volume>
<pages>361</pages>
<marker>Gutmann, Hyv¨arinen, 2012</marker>
<rawString>Michael Gutmann and Aapo Hyv¨arinen. 2012. Noisecontrastive estimation of unnormalized statistical models, with applications to natural image statistics. Journal of Machine Learning Research, 13:307– 361.</rawString>
</citation>
<citation valid="true">
<authors>
<author>Kenneth Heafield</author>
</authors>
<title>KenLM: faster and smaller language model queries.</title>
<date>2011</date>
<booktitle>In Proceedings of the EMNLP 2011 Sixth Workshop on Statistical Machine Translation,</booktitle>
<pages>187--197</pages>
<location>Edinburgh, Scotland, United Kingdom,</location>
<contexts>
<context position="27293" citStr="Heafield, 2011" startWordPosition="4613" endWordPosition="4614">le 3. To build the hard class-based LMs, we utilized mkcls4, a tool to train word classes that uses the maximum likelihood criterion (Och, 1995) for classing. We subsequently trained trigram class language models on these classes (corresponding to 2nd-order HMMs) using SRILM (Stolcke, 2002), with KN-smoothing for the class transition probabilities. SRILM was also used for the baseline KN-smoothed models. For our MT evaluation, we built a hierarchical phrase translation (Chiang, 2007) system using cdec (Dyer et al., 2010). The KN-smoothed models in the MT experiments were compiled using KenLM (Heafield, 2011). 5.1 Datasets For the perplexity experiments, we evaluated our proposed approach on 4 datasets, 2 in English and 2 in Russian. In all cases, the singletons were replaced with “&lt;unk&gt;” tokens in the training corpus, and any word not in the vocabulary was replaced with this token during evaluation. There is a general dearth of evaluation on large-scale corpora in morphologically rich languages such as Russian, and thus we have made the processed Large-Russian corpus available for comparison 3. • Small-English: APNews corpus (Bengio et al., 2003): Train - 14 million words, Dev - 963,000, Test - 9</context>
</contexts>
<marker>Heafield, 2011</marker>
<rawString>Kenneth Heafield. 2011. KenLM: faster and smaller language model queries. In Proceedings of the EMNLP 2011 Sixth Workshop on Statistical Machine Translation, pages 187–197, Edinburgh, Scotland, United Kingdom, July.</rawString>
</citation>
<citation valid="true">
<authors>
<author>Ngoc-Diep Ho</author>
<author>Paul Van Dooren</author>
</authors>
<title>Nonnegative matrix factorization with fixed row and column sums. Linear Algebra and its Applications,</title>
<date>2008</date>
<pages>429--5</pages>
<marker>Ho, Van Dooren, 2008</marker>
<rawString>Ngoc-Diep Ho and Paul Van Dooren. 2008. Nonnegative matrix factorization with fixed row and column sums. Linear Algebra and its Applications, 429(5):1020–1025.</rawString>
</citation>
<citation valid="true">
<authors>
<author>Brian Hutchinson</author>
<author>Mari Ostendorf</author>
<author>Maryam Fazel</author>
</authors>
<title>Low rank language models for small training sets. Signal Processing Letters,</title>
<date>2011</date>
<journal>IEEE,</journal>
<volume>18</volume>
<issue>9</issue>
<pages>492</pages>
<contexts>
<context position="36748" citStr="Hutchinson et al., 2011" startWordPosition="6159" endWordPosition="6162">rd types. Mnih and Teh (2012) and Vaswani et al. (2013) propose setting the normalization constant to 1, but this is approximate and thus can only be used for downstream evaluation, not for perplexity computation. An alternate technique is to use word-classing (Goodman, 2001; Mikolov et al., 2011), which can reduce the cost √ of exact normalization to O( V ). In contrast, our approach is much more scalable, since it is trivially parallelized in training and does not require explicit normalization during evaluation. There are a few low rank approaches (Saul and Pereira, 1997; Bellegarda, 2000; Hutchinson et al., 2011), but they are only effective in restricted settings (e.g. small training sets, or corpora divided into documents) and do not generally perform comparably to state-of-the-art models. Roark et al. (2013) also use the idea of marginal constraints for re-estimating back-off parameters for heavilypruned language models, whereas we use this concept to estimate n-gram specific discounts. 8 Conclusion We presented power low rank ensembles, a technique that generalizes existing n-gram smoothing techniques to non-integer n. By using ensembles of sparse as well as low rank matrices and tensors, our meth</context>
</contexts>
<marker>Hutchinson, Ostendorf, Fazel, 2011</marker>
<rawString>Brian Hutchinson, Mari Ostendorf, and Maryam Fazel. 2011. Low rank language models for small training sets. Signal Processing Letters, IEEE, 18(9):489– 492.</rawString>
</citation>
<citation valid="true">
<authors>
<author>Frederick Jelinek</author>
<author>Robert Mercer</author>
</authors>
<title>Interpolated estimation of markov source parameters from sparse data. Pattern recognition in practice.</title>
<date>1980</date>
<contexts>
<context position="20555" citStr="Jelinek and Mercer, 1980" startWordPosition="3484" endWordPosition="3487"> the inequality constraints 0 ≤ dj i,i−1 ≤ cj i,i−1. Furthermore, the leftover weight γj takes the form: P i dj i,i−1 γj(wi−1) =Pi cji,i−1 Proof. Clearly this choice of dji,i−1 satisfies Eq. 11. The largest possible value of dji,i−1 is cj+1 i,i−1. ρj ≥ ρj+1, implies cj i,i−1 ≥ cj+1 i,i−1. Thus the inequality constraints are met. It is then easy to verify that γ takes the above form. The above lemma generalizes to longer contexts (i.e. n &gt; 2) as shown in Algorithm 1. Note that if ρj = ρj+1 then Algorithm 1 is equivalent to scaling the counts e.g. deleted-interpolation/Jelinek Mercer smoothing (Jelinek and Mercer, 1980). On the other hand, when ρj+1 = 0, Algorithm 1 is equal to the absolute discounting that is used in Kneser-Ney. Thus, depending on ρj+1, our method generalizes different types of interpolation schemes to construct an ensemble so that the marginal constraint is satisfied. Algorithm 1 Compute D In: Count tensor Cn, powers ρj, ρj+1 such that ρj ≥ ρj+1, and parameter d∗. Out: Discount Dj for powered counts Cn,(ρj) and associated leftover weight γj 1: Set Dj(wi, wi−1 i−n+1) = d∗c(wi, wi−1 i−n+1)ρj+1. P d∗ wi c(wi, wi−1 i−n+1)ρj+1 Pwi c(wi, wi−1 i−n+1)ρj Algorithm 2 Compute Z In: Count tensor Cn, p</context>
</contexts>
<marker>Jelinek, Mercer, 1980</marker>
<rawString>Frederick Jelinek and Robert Mercer. 1980. Interpolated estimation of markov source parameters from sparse data. Pattern recognition in practice.</rawString>
</citation>
<citation valid="true">
<authors>
<author>Reinhard Kneser</author>
<author>Hermann Ney</author>
</authors>
<title>Improved backing-off for m-gram language modeling.</title>
<date>1995</date>
<booktitle>In Acoustics, Speech, and Signal Processing,</booktitle>
<volume>1</volume>
<pages>181--184</pages>
<publisher>IEEE.</publisher>
<contexts>
<context position="6193" citStr="Kneser and Ney, 1995" startWordPosition="968" endWordPosition="971">2). We then show how these can be interpolated into an ensemble (§4). In the experimental evaluation on English and Russian corpora (§5), we find that PLRE outperforms Kneser-Ney smoothing and all its variants, as well as class-based language models. We also include a comparison to the log-bilinear neural language model (Mnih and Hinton, 2007) and evaluate performance on a downstream machine translation task (§6) where our method achieves consistent improvements in BLEU. 2 Discount-based Smoothing We first provide background on absolute discounting (Ney et al., 1994) and Kneser-Ney smoothing (Kneser and Ney, 1995), two common n-gram smoothing methods. Both methods can be formulated as back-off or interpolated models; we describe the latter here since that is the basis of our low rank approach. 2.1 Notation Let c(w) be the count of word w, and similarly c(w, wi−1) for the joint count of words w and wi−1. For shorthand we will define wji to denote the word sequence {wi, wi+1, ..., wj−1, wj}. Let P(wi) refer to the maximum likelihood estimate (MLE) of the probability of word wi, and similarly P�(wi|wi−1) for the probability conditioned P� (wi|wi−1 i−n+1). Let N−(wi) := |{w : c(wi, w) &gt; 0} |be the number o</context>
<context position="9397" citStr="Kneser and Ney, 1995" startWordPosition="1493" endWordPosition="1496">thing defines alternate probabilities Palt(·): The base case for unigrams reduces to Palt(wi) = N−(wi) Intuitively Palt(wi) is �wi N−(wi) proportional to the number of unique words that precede wi. Thus, words that appear in many different contexts will be given higher weight than words that consistently appear after only a few contexts. These alternate distributions are then used with absolute discounting: i−1alt i−1 (iγ+ Pkn(wi |wi−n+1) = PD (wi |wi−n+1) −1 i−1 wi−n+1)Pkn(wi |wi−n+2) (2) where we set Pkn(wi) = Palt(wi). By definition, KN smoothing satisfies the marginal constraint in Eq. 1 (Kneser and Ney, 1995). 3 Power Low Rank Ensembles In n-gram smoothing methods, if a bigram count c(wi, wi−1) is zero, the unigram probabilities are used, which is equivalent to assuming that wi and wi−1 are independent ( and similarly for general n). However, in this situation, instead of backing off to a 1-gram, we may like to back off to a “1.5-gram” or more generally an order between 1 and 2 that captures a coarser level of dependence between wi and wi−1 and does not assume full independence. Inspired by this intuition, our strategy is to construct an ensemble of matrices and tensors that not only consists of M</context>
</contexts>
<marker>Kneser, Ney, 1995</marker>
<rawString>Reinhard Kneser and Hermann Ney. 1995. Improved backing-off for m-gram language modeling. In Acoustics, Speech, and Signal Processing, 1995. ICASSP-95., 1995 International Conference on, volume 1, pages 181–184. IEEE.</rawString>
</citation>
<citation valid="true">
<authors>
<author>Philipp Koehn</author>
</authors>
<title>Statistical Machine Translation.</title>
<date>2010</date>
<publisher>Cambridge University Press,</publisher>
<location>New York, NY, USA,</location>
<contexts>
<context position="1105" citStr="Koehn, 2010" startWordPosition="166" endWordPosition="167">f ngram modeling to non-integer n, and includes standard techniques such as absolute discounting and Kneser-Ney smoothing as special cases. PLRE training is efficient and our approach outperforms stateof-the-art modified Kneser Ney baselines in terms of perplexity on large corpora as well as on BLEU score in a downstream machine translation task. 1 Introduction Language modeling is the task of estimating the probability of sequences of words in a language and is an important component in, among other applications, automatic speech recognition (Rabiner and Juang, 1993) and machine translation (Koehn, 2010). The predominant approach to language modeling is the n-gram model, wherein the probability of a word sequence P(w1, ... , w`) is decomposed using the chain rule, and then a Markov assumption is made: P(w1, ... , w`) ≈ H`i=1 P(wi|wi−1 i−n+1). While this assumption substantially reduces the modeling complexity, parameter estimation remains a major challenge. Due to the power-law nature of language (Zipf, 1949), the maximum likelihood estimator massively overestimates the probability of rare events and assigns zero probability to legitimate word sequences that happen not to have been observed i</context>
</contexts>
<marker>Koehn, 2010</marker>
<rawString>Philipp Koehn. 2010. Statistical Machine Translation. Cambridge University Press, New York, NY, USA, 1st edition.</rawString>
</citation>
<citation valid="true">
<authors>
<author>Yehuda Koren</author>
<author>Robert Bell</author>
<author>Chris Volinsky</author>
</authors>
<title>Matrix factorization techniques for recommender systems.</title>
<date>2009</date>
<journal>Computer,</journal>
<volume>42</volume>
<issue>8</issue>
<contexts>
<context position="2437" citStr="Koren et al., 2009" startWordPosition="360" endWordPosition="363"> University avneesh@cs.cmu.edu Eric P. Xing School of Computer Science Carnegie Mellon University epxing@cs.cmu.edu Many smoothing techniques have been proposed to address the estimation challenge. These reassign probability mass (generally from overestimated events) to unseen word sequences, whose probabilities are estimated by interpolating with or backing off to lower order n-gram models (Chen and Goodman, 1999). Somewhat surprisingly, these widely used smoothing techniques differ substantially from techniques for coping with data sparsity in other domains, such as collaborative filtering (Koren et al., 2009; Su and Khoshgoftaar, 2009) or matrix completion (Cand`es and Recht, 2009; Cai et al., 2010). In these areas, low rank approaches based on matrix factorization play a central role (Lee and Seung, 2001; Salakhutdinov and Mnih, 2008; Mackey et al., 2011). For example, in recommender systems, a key challenge is dealing with the sparsity of ratings from a single user, since typical users will have rated only a few items. By projecting the low rank representation of a user’s (sparse) preferences into the original space, an estimate of ratings for new items is obtained. These methods are attractive</context>
</contexts>
<marker>Koren, Bell, Volinsky, 2009</marker>
<rawString>Yehuda Koren, Robert Bell, and Chris Volinsky. 2009. Matrix factorization techniques for recommender systems. Computer, 42(8):30–37.</rawString>
</citation>
<citation valid="true">
<authors>
<author>Daniel D Lee</author>
<author>H Sebastian Seung</author>
</authors>
<title>Algorithms for non-negative matrix factorization.</title>
<date>2001</date>
<booktitle>Advances in Neural Information Processing Systems,</booktitle>
<pages>13--556</pages>
<contexts>
<context position="2638" citStr="Lee and Seung, 2001" startWordPosition="393" endWordPosition="396">ese reassign probability mass (generally from overestimated events) to unseen word sequences, whose probabilities are estimated by interpolating with or backing off to lower order n-gram models (Chen and Goodman, 1999). Somewhat surprisingly, these widely used smoothing techniques differ substantially from techniques for coping with data sparsity in other domains, such as collaborative filtering (Koren et al., 2009; Su and Khoshgoftaar, 2009) or matrix completion (Cand`es and Recht, 2009; Cai et al., 2010). In these areas, low rank approaches based on matrix factorization play a central role (Lee and Seung, 2001; Salakhutdinov and Mnih, 2008; Mackey et al., 2011). For example, in recommender systems, a key challenge is dealing with the sparsity of ratings from a single user, since typical users will have rated only a few items. By projecting the low rank representation of a user’s (sparse) preferences into the original space, an estimate of ratings for new items is obtained. These methods are attractive due to their computational efficiency and mathematical well-foundedness. In this paper, we introduce power low rank ensembles (PLRE), in which low rank tensors are used to produce smoothed estimates f</context>
<context position="13907" citStr="Lee and Seung, 2001" startWordPosition="2275" endWordPosition="2278"> wi−n+1). Furthermore denote Cn(:,˜wi−1 i−n+2,:) to be the V x V matrix slice of Cn where wi−n+2, ..., wi−1 are held fixed to a particular ˜wi−1. Then if wi is conditionally independent of wi−n+1 given wi−1 i−n+2, then ]E[Cn(:, ˜wi−1 i−n+2, :)] is rank one b ˜wi−1 i−n+2. However, it is rare that these matrices are actually rank one, either due to sampling variance or the fact that wi and wi−1 are not independent. What we would really like to say is that the best rank one approximation B(1) P(wi−1). While this statement is not true under the E2 norm, it is true under generalized KL divergence (Lee and Seung, 2001): gKL(A||B) = Eij (Aij log(Aij Bi,. − Aij + Bij)). 7 In particular, generalized KL divergence preserves row and column sums: if M(κ) is the best rank κ approximation of M under gKL then the row sums and column sums of M(κ) and M are equal (Ho and Van Dooren, 2008). Leveraging this property, it is straightforward to prove the following lemma: Lemma 1. Let B(κ) be the best rank κ approximation of B under gKL. Then B(1) a P(wi) P�(wi−1) and bwi−1 s.t. c(wi−1) =� P� (wi) = B(1)(wi, wi−1) Ew B(1)(w, wi−1) For more general n, let Cn,(κ) i−1,...,i−n+2 be the best rank κ approximation of Cn(:, ˜wi−1 i</context>
</contexts>
<marker>Lee, Seung, 2001</marker>
<rawString>Daniel D. Lee and H. Sebastian Seung. 2001. Algorithms for non-negative matrix factorization. Advances in Neural Information Processing Systems, 13:556–562.</rawString>
</citation>
<citation valid="true">
<authors>
<author>Lester Mackey</author>
<author>Ameet Talwalkar</author>
<author>Michael I Jordan</author>
</authors>
<title>Divide-and-conquer matrix factorization. arXiv preprint arXiv:1107.0789.</title>
<date>2011</date>
<contexts>
<context position="2690" citStr="Mackey et al., 2011" startWordPosition="401" endWordPosition="404">stimated events) to unseen word sequences, whose probabilities are estimated by interpolating with or backing off to lower order n-gram models (Chen and Goodman, 1999). Somewhat surprisingly, these widely used smoothing techniques differ substantially from techniques for coping with data sparsity in other domains, such as collaborative filtering (Koren et al., 2009; Su and Khoshgoftaar, 2009) or matrix completion (Cand`es and Recht, 2009; Cai et al., 2010). In these areas, low rank approaches based on matrix factorization play a central role (Lee and Seung, 2001; Salakhutdinov and Mnih, 2008; Mackey et al., 2011). For example, in recommender systems, a key challenge is dealing with the sparsity of ratings from a single user, since typical users will have rated only a few items. By projecting the low rank representation of a user’s (sparse) preferences into the original space, an estimate of ratings for new items is obtained. These methods are attractive due to their computational efficiency and mathematical well-foundedness. In this paper, we introduce power low rank ensembles (PLRE), in which low rank tensors are used to produce smoothed estimates for n-gram probabilities. Ideally, we would like the </context>
</contexts>
<marker>Mackey, Talwalkar, Jordan, 2011</marker>
<rawString>Lester Mackey, Ameet Talwalkar, and Michael I Jordan. 2011. Divide-and-conquer matrix factorization. arXiv preprint arXiv:1107.0789.</rawString>
</citation>
<citation valid="true">
<authors>
<author>Christopher D Manning</author>
<author>Hinrich Sch¨utze</author>
</authors>
<title>Foundations of statistical natural language processing, volume 999.</title>
<date>1999</date>
<publisher>MIT Press.</publisher>
<marker>Manning, Sch¨utze, 1999</marker>
<rawString>Christopher D Manning and Hinrich Sch¨utze. 1999. Foundations of statistical natural language processing, volume 999. MIT Press.</rawString>
</citation>
<citation valid="true">
<authors>
<author>Risto Miikkulainen</author>
<author>Michael G Dyer</author>
</authors>
<title>Natural language processing with modular pdp networks and distributed lexicon.</title>
<date>1991</date>
<journal>Cognitive Science,</journal>
<volume>15</volume>
<pages>399</pages>
<contexts>
<context position="35406" citStr="Miikkulainen and Dyer, 1991" startWordPosition="5939" endWordPosition="5942">e KN baseline by 0.16 BLEU, and this improvement is consistent in that PLRE never gets a worse BLEU score. 7 Related Work Recent attempts to revisit the language modeling problem have largely come from two directions: Bayesian nonparametrics and neural networks. Teh (2006) and Goldwater et al. (2006) discovered the connection between interpolated Kneser Ney and the hierarchical Pitman-Yor process. These have led to generalizations that account for domain effects (Wood and Teh, 2009) and unbounded contexts (Wood et al., 2009). The idea of using neural networks for language modeling is not new (Miikkulainen and Dyer, 1991), but recent efforts (Mnih and Hinton, 2007; Mikolov et al., 2010) have achieved impressive performance. These methods can be quite expensive to train and query (especially as the vocabulary size increases). Techniques such as noise contrastive estimation (Gutmann and Hyv¨arinen, 2012; Mnih and Teh, 2012; Vaswani et al., 2013), subsampling (Xu et al., 2011), or careful engineering approaches for maximum entropy LMs (which can also be applied to neural networks) (Wu and Khudanpur, 2000) have improved training of these models, but querying the probability of the next word given still requires ex</context>
</contexts>
<marker>Miikkulainen, Dyer, 1991</marker>
<rawString>Risto Miikkulainen and Michael G. Dyer. 1991. Natural language processing with modular pdp networks and distributed lexicon. Cognitive Science, 15:343– 399.</rawString>
</citation>
<citation valid="true">
<authors>
<author>Tom Mikolov</author>
<author>Martin Karafit</author>
<author>Luk Burget</author>
<author>Jan ernock</author>
<author>Sanjeev Khudanpur</author>
</authors>
<title>Recurrent neural network based language model.</title>
<date>2010</date>
<journal>International Speech Communication Association.</journal>
<booktitle>In Proceedings of the 11th Annual Conference of the International Speech Communication Association (INTERSPEECH</booktitle>
<volume>volume</volume>
<pages>1045--1048</pages>
<contexts>
<context position="4758" citStr="Mikolov et al., 2010" startWordPosition="733" endWordPosition="736"> based on an element-wise power operation, similar to how some smoothing methods modify their lower order distributions. Moreover, PLRE has two key aspects that lead to easy scalability for large corpora and vocabularies. First, since it utilizes the original n-grams, the ranks required for the low rank matrices and tensors tend to be remain tractable (e.g. around 100 for a vocabulary size V ≈ 1 × 106) leading to fast training times. This differentiates our approach over other methods that leverage an underlying latent space such as neural networks (Bengio et al., 2003; Mnih and Hinton, 2007; Mikolov et al., 2010) or soft-class models (Saul and Pereira, 1997) where the underlying dimension is required to be quite large to obtain good performance. Moreover, at test time, the probability of a sequence can be queried in time O(κmax) where κmax is the maximum rank of the low rank matrices/tensors used. While this is larger than Kneser Ney’s virtually constant query time, it is substantially faster than conditional exponential family models (Chen and Rosenfeld, 2000; Chen, 2009; Nelakanti et al., 2013) and neural networks which require O(V ) for exact computation of the normalization constant. See Section 7</context>
<context position="35472" citStr="Mikolov et al., 2010" startWordPosition="5950" endWordPosition="5953">RE never gets a worse BLEU score. 7 Related Work Recent attempts to revisit the language modeling problem have largely come from two directions: Bayesian nonparametrics and neural networks. Teh (2006) and Goldwater et al. (2006) discovered the connection between interpolated Kneser Ney and the hierarchical Pitman-Yor process. These have led to generalizations that account for domain effects (Wood and Teh, 2009) and unbounded contexts (Wood et al., 2009). The idea of using neural networks for language modeling is not new (Miikkulainen and Dyer, 1991), but recent efforts (Mnih and Hinton, 2007; Mikolov et al., 2010) have achieved impressive performance. These methods can be quite expensive to train and query (especially as the vocabulary size increases). Techniques such as noise contrastive estimation (Gutmann and Hyv¨arinen, 2012; Mnih and Teh, 2012; Vaswani et al., 2013), subsampling (Xu et al., 2011), or careful engineering approaches for maximum entropy LMs (which can also be applied to neural networks) (Wu and Khudanpur, 2000) have improved training of these models, but querying the probability of the next word given still requires explicitly normalizing over the vocabulary, which is expensive for b</context>
</contexts>
<marker>Mikolov, Karafit, Burget, ernock, Khudanpur, 2010</marker>
<rawString>Tom Mikolov, Martin Karafit, Luk Burget, Jan ernock, and Sanjeev Khudanpur. 2010. Recurrent neural network based language model. In Proceedings of the 11th Annual Conference of the International Speech Communication Association (INTERSPEECH 2010), volume 2010, pages 1045–1048. International Speech Communication Association.</rawString>
</citation>
<citation valid="true">
<authors>
<author>Tomas Mikolov</author>
<author>Stefan Kombrink</author>
<author>Lukas Burget</author>
<author>JH Cernocky</author>
<author>Sanjeev Khudanpur</author>
</authors>
<title>Extensions of recurrent neural network language model.</title>
<date>2011</date>
<booktitle>In Acoustics, Speech and Signal Processing (ICASSP), 2011 IEEE International Conference on,</booktitle>
<pages>5528--5531</pages>
<publisher>IEEE.</publisher>
<contexts>
<context position="36422" citStr="Mikolov et al., 2011" startWordPosition="6105" endWordPosition="6108">for maximum entropy LMs (which can also be applied to neural networks) (Wu and Khudanpur, 2000) have improved training of these models, but querying the probability of the next word given still requires explicitly normalizing over the vocabulary, which is expensive for big corpora or in languages with a large number of word types. Mnih and Teh (2012) and Vaswani et al. (2013) propose setting the normalization constant to 1, but this is approximate and thus can only be used for downstream evaluation, not for perplexity computation. An alternate technique is to use word-classing (Goodman, 2001; Mikolov et al., 2011), which can reduce the cost √ of exact normalization to O( V ). In contrast, our approach is much more scalable, since it is trivially parallelized in training and does not require explicit normalization during evaluation. There are a few low rank approaches (Saul and Pereira, 1997; Bellegarda, 2000; Hutchinson et al., 2011), but they are only effective in restricted settings (e.g. small training sets, or corpora divided into documents) and do not generally perform comparably to state-of-the-art models. Roark et al. (2013) also use the idea of marginal constraints for re-estimating back-off pa</context>
</contexts>
<marker>Mikolov, Kombrink, Burget, Cernocky, Khudanpur, 2011</marker>
<rawString>Tomas Mikolov, Stefan Kombrink, Lukas Burget, JH Cernocky, and Sanjeev Khudanpur. 2011. Extensions of recurrent neural network language model. In Acoustics, Speech and Signal Processing (ICASSP), 2011 IEEE International Conference on, pages 5528–5531. IEEE.</rawString>
</citation>
<citation valid="true">
<authors>
<author>Andriy Mnih</author>
<author>Geoffrey Hinton</author>
</authors>
<title>Three new graphical models for statistical language modelling.</title>
<date>2007</date>
<booktitle>In Proceedings of the 24th international conference on Machine learning,</booktitle>
<pages>641--648</pages>
<publisher>ACM.</publisher>
<contexts>
<context position="4735" citStr="Mnih and Hinton, 2007" startWordPosition="729" endWordPosition="732">ntities of these counts based on an element-wise power operation, similar to how some smoothing methods modify their lower order distributions. Moreover, PLRE has two key aspects that lead to easy scalability for large corpora and vocabularies. First, since it utilizes the original n-grams, the ranks required for the low rank matrices and tensors tend to be remain tractable (e.g. around 100 for a vocabulary size V ≈ 1 × 106) leading to fast training times. This differentiates our approach over other methods that leverage an underlying latent space such as neural networks (Bengio et al., 2003; Mnih and Hinton, 2007; Mikolov et al., 2010) or soft-class models (Saul and Pereira, 1997) where the underlying dimension is required to be quite large to obtain good performance. Moreover, at test time, the probability of a sequence can be queried in time O(κmax) where κmax is the maximum rank of the low rank matrices/tensors used. While this is larger than Kneser Ney’s virtually constant query time, it is substantially faster than conditional exponential family models (Chen and Rosenfeld, 2000; Chen, 2009; Nelakanti et al., 2013) and neural networks which require O(V ) for exact computation of the normalization </context>
<context position="26448" citStr="Mnih and Hinton, 2007" startWordPosition="4477" endWordPosition="4480">is the total number of matrices/tensors in the ensemble. While this is larger than Kneser Ney’s practically constant complexity of O(n), it is much faster than other recent methods for language modeling such as neural networks and conditional exponential family models where exact computation of the normalizing constant costs O(V ). 5 Experiments To evaluate PLRE, we compared its performance on English and Russian corpora with several vari2for derivation see proof of Lemma 4 in the supplementary material 1493 ants of KN smoothing, class-based models, and the log-bilinear neural language model (Mnih and Hinton, 2007). We evaluated with perplexity in most of our experiments, but also provide results evaluated with BLEU (Papineni et al., 2002) on a downstream machine translation (MT) task. We have made the code for our approach publicly available 3. To build the hard class-based LMs, we utilized mkcls4, a tool to train word classes that uses the maximum likelihood criterion (Och, 1995) for classing. We subsequently trained trigram class language models on these classes (corresponding to 2nd-order HMMs) using SRILM (Stolcke, 2002), with KN-smoothing for the class transition probabilities. SRILM was also used</context>
<context position="31737" citStr="Mnih and Hinton, 2007" startWordPosition="5319" endWordPosition="5322">96 Table 1: Perplexity results on small corpora for all methods. Figure 1: Relative percentage improvement of PLRE over int-MKN as the maximum n-gram order for both methods is increased. ilar improvements for all orders (which are larger than that for Small-English). On both these datasets, we also experimented with tuning the discounts for int-MKN to see if the baseline could be improved with more careful choices of discounts. However, this achieved only marginal gains (reducing the perplexity to 98.94 on the Small-English test set and 259.0 on the Small-Russian test set). Comparison to LBL (Mnih and Hinton, 2007): Mnih and Hinton (2007) evaluate on the Small-English dataset (but remove end markers and concatenate the sentences). They obtain perplexities 117.0 and 107.8 using contexts of size 5 and 10 respectively. With this preprocessing, a 4- gram (context 3) PLRE achieves 108.4 perplexity. 5.3 Large Corpora Results on the larger corpora for the top 2 performing methods “PLRE” and “int-MKN” are presented in Table 2. Due to the larger training size, we use 4-gram models in these experiments. However, including the low rank 4-gram tensor provided little gain and therefore, the 4-gram PLRE only has addi</context>
<context position="35449" citStr="Mnih and Hinton, 2007" startWordPosition="5946" endWordPosition="5949">s consistent in that PLRE never gets a worse BLEU score. 7 Related Work Recent attempts to revisit the language modeling problem have largely come from two directions: Bayesian nonparametrics and neural networks. Teh (2006) and Goldwater et al. (2006) discovered the connection between interpolated Kneser Ney and the hierarchical Pitman-Yor process. These have led to generalizations that account for domain effects (Wood and Teh, 2009) and unbounded contexts (Wood et al., 2009). The idea of using neural networks for language modeling is not new (Miikkulainen and Dyer, 1991), but recent efforts (Mnih and Hinton, 2007; Mikolov et al., 2010) have achieved impressive performance. These methods can be quite expensive to train and query (especially as the vocabulary size increases). Techniques such as noise contrastive estimation (Gutmann and Hyv¨arinen, 2012; Mnih and Teh, 2012; Vaswani et al., 2013), subsampling (Xu et al., 2011), or careful engineering approaches for maximum entropy LMs (which can also be applied to neural networks) (Wu and Khudanpur, 2000) have improved training of these models, but querying the probability of the next word given still requires explicitly normalizing over the vocabulary, w</context>
</contexts>
<marker>Mnih, Hinton, 2007</marker>
<rawString>Andriy Mnih and Geoffrey Hinton. 2007. Three new graphical models for statistical language modelling. In Proceedings of the 24th international conference on Machine learning, pages 641–648. ACM.</rawString>
</citation>
<citation valid="true">
<authors>
<author>Andriy Mnih</author>
<author>Yee Whye Teh</author>
</authors>
<title>A fast and simple algorithm for training neural probabilistic language models.</title>
<date>2012</date>
<booktitle>In Proceedings of the International Conference on Machine Learning.</booktitle>
<contexts>
<context position="35711" citStr="Mnih and Teh, 2012" startWordPosition="5986" endWordPosition="5989">e connection between interpolated Kneser Ney and the hierarchical Pitman-Yor process. These have led to generalizations that account for domain effects (Wood and Teh, 2009) and unbounded contexts (Wood et al., 2009). The idea of using neural networks for language modeling is not new (Miikkulainen and Dyer, 1991), but recent efforts (Mnih and Hinton, 2007; Mikolov et al., 2010) have achieved impressive performance. These methods can be quite expensive to train and query (especially as the vocabulary size increases). Techniques such as noise contrastive estimation (Gutmann and Hyv¨arinen, 2012; Mnih and Teh, 2012; Vaswani et al., 2013), subsampling (Xu et al., 2011), or careful engineering approaches for maximum entropy LMs (which can also be applied to neural networks) (Wu and Khudanpur, 2000) have improved training of these models, but querying the probability of the next word given still requires explicitly normalizing over the vocabulary, which is expensive for big corpora or in languages with a large number of word types. Mnih and Teh (2012) and Vaswani et al. (2013) propose setting the normalization constant to 1, but this is approximate and thus can only be used for downstream evaluation, not f</context>
</contexts>
<marker>Mnih, Teh, 2012</marker>
<rawString>Andriy Mnih and Yee Whye Teh. 2012. A fast and simple algorithm for training neural probabilistic language models. In Proceedings of the International Conference on Machine Learning.</rawString>
</citation>
<citation valid="true">
<authors>
<author>Anil Kumar Nelakanti</author>
<author>Cedric Archambeau</author>
<author>Julien Mairal</author>
<author>Francis Bach</author>
<author>Guillaume Bouchard</author>
</authors>
<title>Structured penalties for log-linear language models.</title>
<date>2013</date>
<booktitle>In Proceedings of the 2013 Conference on Empirical Methods in Natural Language Processing,</booktitle>
<pages>233--243</pages>
<publisher>Association</publisher>
<institution>for Computational Linguistics.</institution>
<location>Seattle, Washington, USA,</location>
<contexts>
<context position="5251" citStr="Nelakanti et al., 2013" startWordPosition="814" endWordPosition="817">hat leverage an underlying latent space such as neural networks (Bengio et al., 2003; Mnih and Hinton, 2007; Mikolov et al., 2010) or soft-class models (Saul and Pereira, 1997) where the underlying dimension is required to be quite large to obtain good performance. Moreover, at test time, the probability of a sequence can be queried in time O(κmax) where κmax is the maximum rank of the low rank matrices/tensors used. While this is larger than Kneser Ney’s virtually constant query time, it is substantially faster than conditional exponential family models (Chen and Rosenfeld, 2000; Chen, 2009; Nelakanti et al., 2013) and neural networks which require O(V ) for exact computation of the normalization constant. See Section 7 for a more detailed discussion of related work. Outline: We first review existing n-gram smoothing methods (§2) and then present the intuition behind the key components of our technique: rank (§3.1) and power (§3.2). We then show how these can be interpolated into an ensemble (§4). In the experimental evaluation on English and Russian corpora (§5), we find that PLRE outperforms Kneser-Ney smoothing and all its variants, as well as class-based language models. We also include a comparison</context>
</contexts>
<marker>Nelakanti, Archambeau, Mairal, Bach, Bouchard, 2013</marker>
<rawString>Anil Kumar Nelakanti, Cedric Archambeau, Julien Mairal, Francis Bach, and Guillaume Bouchard. 2013. Structured penalties for log-linear language models. In Proceedings of the 2013 Conference on Empirical Methods in Natural Language Processing, pages 233–243, Seattle, Washington, USA, October. Association for Computational Linguistics.</rawString>
</citation>
<citation valid="true">
<authors>
<author>Hermann Ney</author>
<author>Ute Essen</author>
<author>Reinhard Kneser</author>
</authors>
<date>1994</date>
<booktitle>On Structuring Probabilistic Dependencies in Stochastic Language Modelling. Computer Speech and Language,</booktitle>
<pages>8--1</pages>
<contexts>
<context position="6145" citStr="Ney et al., 1994" startWordPosition="960" endWordPosition="963">of our technique: rank (§3.1) and power (§3.2). We then show how these can be interpolated into an ensemble (§4). In the experimental evaluation on English and Russian corpora (§5), we find that PLRE outperforms Kneser-Ney smoothing and all its variants, as well as class-based language models. We also include a comparison to the log-bilinear neural language model (Mnih and Hinton, 2007) and evaluate performance on a downstream machine translation task (§6) where our method achieves consistent improvements in BLEU. 2 Discount-based Smoothing We first provide background on absolute discounting (Ney et al., 1994) and Kneser-Ney smoothing (Kneser and Ney, 1995), two common n-gram smoothing methods. Both methods can be formulated as back-off or interpolated models; we describe the latter here since that is the basis of our low rank approach. 2.1 Notation Let c(w) be the count of word w, and similarly c(w, wi−1) for the joint count of words w and wi−1. For shorthand we will define wji to denote the word sequence {wi, wi+1, ..., wj−1, wj}. Let P(wi) refer to the maximum likelihood estimate (MLE) of the probability of word wi, and similarly P�(wi|wi−1) for the probability conditioned P� (wi|wi−1 i−n+1). Le</context>
</contexts>
<marker>Ney, Essen, Kneser, 1994</marker>
<rawString>Hermann Ney, Ute Essen, and Reinhard Kneser. 1994. On Structuring Probabilistic Dependencies in Stochastic Language Modelling. Computer Speech and Language, 8:1–38.</rawString>
</citation>
<citation valid="true">
<authors>
<author>Franz Josef Och</author>
</authors>
<title>Maximum-likelihoodsch¨atzung von wortkategorien mit verfahren der kombinatorischen optimierung. Bachelor’s thesis (Studienarbeit),</title>
<date>1995</date>
<institution>University of Erlangen.</institution>
<contexts>
<context position="26822" citStr="Och, 1995" startWordPosition="4540" endWordPosition="4541">ormance on English and Russian corpora with several vari2for derivation see proof of Lemma 4 in the supplementary material 1493 ants of KN smoothing, class-based models, and the log-bilinear neural language model (Mnih and Hinton, 2007). We evaluated with perplexity in most of our experiments, but also provide results evaluated with BLEU (Papineni et al., 2002) on a downstream machine translation (MT) task. We have made the code for our approach publicly available 3. To build the hard class-based LMs, we utilized mkcls4, a tool to train word classes that uses the maximum likelihood criterion (Och, 1995) for classing. We subsequently trained trigram class language models on these classes (corresponding to 2nd-order HMMs) using SRILM (Stolcke, 2002), with KN-smoothing for the class transition probabilities. SRILM was also used for the baseline KN-smoothed models. For our MT evaluation, we built a hierarchical phrase translation (Chiang, 2007) system using cdec (Dyer et al., 2010). The KN-smoothed models in the MT experiments were compiled using KenLM (Heafield, 2011). 5.1 Datasets For the perplexity experiments, we evaluated our proposed approach on 4 datasets, 2 in English and 2 in Russian. I</context>
</contexts>
<marker>Och, 1995</marker>
<rawString>Franz Josef Och. 1995. Maximum-likelihoodsch¨atzung von wortkategorien mit verfahren der kombinatorischen optimierung. Bachelor’s thesis (Studienarbeit), University of Erlangen.</rawString>
</citation>
<citation valid="true">
<authors>
<author>Kishore Papineni</author>
<author>Salim Roukos</author>
<author>Todd Ward</author>
<author>Wei jing Zhu</author>
</authors>
<title>Bleu: a method for automatic evaluation of machine translation.</title>
<date>2002</date>
<pages>311--318</pages>
<contexts>
<context position="26575" citStr="Papineni et al., 2002" startWordPosition="4497" endWordPosition="4500"> of O(n), it is much faster than other recent methods for language modeling such as neural networks and conditional exponential family models where exact computation of the normalizing constant costs O(V ). 5 Experiments To evaluate PLRE, we compared its performance on English and Russian corpora with several vari2for derivation see proof of Lemma 4 in the supplementary material 1493 ants of KN smoothing, class-based models, and the log-bilinear neural language model (Mnih and Hinton, 2007). We evaluated with perplexity in most of our experiments, but also provide results evaluated with BLEU (Papineni et al., 2002) on a downstream machine translation (MT) task. We have made the code for our approach publicly available 3. To build the hard class-based LMs, we utilized mkcls4, a tool to train word classes that uses the maximum likelihood criterion (Och, 1995) for classing. We subsequently trained trigram class language models on these classes (corresponding to 2nd-order HMMs) using SRILM (Stolcke, 2002), with KN-smoothing for the class transition probabilities. SRILM was also used for the baseline KN-smoothed models. For our MT evaluation, we built a hierarchical phrase translation (Chiang, 2007) system u</context>
</contexts>
<marker>Papineni, Roukos, Ward, Zhu, 2002</marker>
<rawString>Kishore Papineni, Salim Roukos, Todd Ward, and Wei jing Zhu. 2002. Bleu: a method for automatic evaluation of machine translation. pages 311–318.</rawString>
</citation>
<citation valid="true">
<authors>
<author>Lawrence Rabiner</author>
<author>Biing-Hwang Juang</author>
</authors>
<title>Fundamentals of speech recognition.</title>
<date>1993</date>
<contexts>
<context position="1067" citStr="Rabiner and Juang, 1993" startWordPosition="158" endWordPosition="162">Our method can be understood as a generalization of ngram modeling to non-integer n, and includes standard techniques such as absolute discounting and Kneser-Ney smoothing as special cases. PLRE training is efficient and our approach outperforms stateof-the-art modified Kneser Ney baselines in terms of perplexity on large corpora as well as on BLEU score in a downstream machine translation task. 1 Introduction Language modeling is the task of estimating the probability of sequences of words in a language and is an important component in, among other applications, automatic speech recognition (Rabiner and Juang, 1993) and machine translation (Koehn, 2010). The predominant approach to language modeling is the n-gram model, wherein the probability of a word sequence P(w1, ... , w`) is decomposed using the chain rule, and then a Markov assumption is made: P(w1, ... , w`) ≈ H`i=1 P(wi|wi−1 i−n+1). While this assumption substantially reduces the modeling complexity, parameter estimation remains a major challenge. Due to the power-law nature of language (Zipf, 1949), the maximum likelihood estimator massively overestimates the probability of rare events and assigns zero probability to legitimate word sequences t</context>
</contexts>
<marker>Rabiner, Juang, 1993</marker>
<rawString>Lawrence Rabiner and Biing-Hwang Juang. 1993. Fundamentals of speech recognition.</rawString>
</citation>
<citation valid="true">
<authors>
<author>Brian Roark</author>
<author>Cyril Allauzen</author>
<author>Michael Riley</author>
</authors>
<title>Smoothed marginal distribution constraints for language modeling.</title>
<date>2013</date>
<booktitle>In Proceedings of the 51st Annual Meeting of the Association for Computational Linguistics (ACL),</booktitle>
<pages>43--52</pages>
<contexts>
<context position="36950" citStr="Roark et al. (2013)" startWordPosition="6190" endWordPosition="6193">ation. An alternate technique is to use word-classing (Goodman, 2001; Mikolov et al., 2011), which can reduce the cost √ of exact normalization to O( V ). In contrast, our approach is much more scalable, since it is trivially parallelized in training and does not require explicit normalization during evaluation. There are a few low rank approaches (Saul and Pereira, 1997; Bellegarda, 2000; Hutchinson et al., 2011), but they are only effective in restricted settings (e.g. small training sets, or corpora divided into documents) and do not generally perform comparably to state-of-the-art models. Roark et al. (2013) also use the idea of marginal constraints for re-estimating back-off parameters for heavilypruned language models, whereas we use this concept to estimate n-gram specific discounts. 8 Conclusion We presented power low rank ensembles, a technique that generalizes existing n-gram smoothing techniques to non-integer n. By using ensembles of sparse as well as low rank matrices and tensors, our method captures both the fine-grained and coarse structures in word sequences. Our discounting strategy preserves the marginal constraint and thus generalizes Kneser Ney, and under slight changes can also e</context>
</contexts>
<marker>Roark, Allauzen, Riley, 2013</marker>
<rawString>Brian Roark, Cyril Allauzen, and Michael Riley. 2013. Smoothed marginal distribution constraints for language modeling. In Proceedings of the 51st Annual Meeting of the Association for Computational Linguistics (ACL), pages 43–52.</rawString>
</citation>
<citation valid="true">
<authors>
<author>Ruslan Salakhutdinov</author>
<author>Andriy Mnih</author>
</authors>
<title>Bayesian probabilistic matrix factorization using Markov chain Monte Carlo.</title>
<date>2008</date>
<booktitle>In Proceedings of the 25th international conference on Machine learning,</booktitle>
<pages>880--887</pages>
<publisher>ACM.</publisher>
<contexts>
<context position="2668" citStr="Salakhutdinov and Mnih, 2008" startWordPosition="397" endWordPosition="400">ity mass (generally from overestimated events) to unseen word sequences, whose probabilities are estimated by interpolating with or backing off to lower order n-gram models (Chen and Goodman, 1999). Somewhat surprisingly, these widely used smoothing techniques differ substantially from techniques for coping with data sparsity in other domains, such as collaborative filtering (Koren et al., 2009; Su and Khoshgoftaar, 2009) or matrix completion (Cand`es and Recht, 2009; Cai et al., 2010). In these areas, low rank approaches based on matrix factorization play a central role (Lee and Seung, 2001; Salakhutdinov and Mnih, 2008; Mackey et al., 2011). For example, in recommender systems, a key challenge is dealing with the sparsity of ratings from a single user, since typical users will have rated only a few items. By projecting the low rank representation of a user’s (sparse) preferences into the original space, an estimate of ratings for new items is obtained. These methods are attractive due to their computational efficiency and mathematical well-foundedness. In this paper, we introduce power low rank ensembles (PLRE), in which low rank tensors are used to produce smoothed estimates for n-gram probabilities. Ideal</context>
</contexts>
<marker>Salakhutdinov, Mnih, 2008</marker>
<rawString>Ruslan Salakhutdinov and Andriy Mnih. 2008. Bayesian probabilistic matrix factorization using Markov chain Monte Carlo. In Proceedings of the 25th international conference on Machine learning, pages 880–887. ACM.</rawString>
</citation>
<citation valid="true">
<authors>
<author>Lawrence Saul</author>
<author>Fernando Pereira</author>
</authors>
<title>Aggregate and mixed-order markov models for statistical language processing.</title>
<date>1997</date>
<booktitle>In Proceedings of the second conference on empirical methods in natural language processing,</booktitle>
<pages>81--89</pages>
<publisher>Association</publisher>
<institution>for Computational Linguistics.</institution>
<location>Somerset, New Jersey:</location>
<contexts>
<context position="4804" citStr="Saul and Pereira, 1997" startWordPosition="740" endWordPosition="743">imilar to how some smoothing methods modify their lower order distributions. Moreover, PLRE has two key aspects that lead to easy scalability for large corpora and vocabularies. First, since it utilizes the original n-grams, the ranks required for the low rank matrices and tensors tend to be remain tractable (e.g. around 100 for a vocabulary size V ≈ 1 × 106) leading to fast training times. This differentiates our approach over other methods that leverage an underlying latent space such as neural networks (Bengio et al., 2003; Mnih and Hinton, 2007; Mikolov et al., 2010) or soft-class models (Saul and Pereira, 1997) where the underlying dimension is required to be quite large to obtain good performance. Moreover, at test time, the probability of a sequence can be queried in time O(κmax) where κmax is the maximum rank of the low rank matrices/tensors used. While this is larger than Kneser Ney’s virtually constant query time, it is substantially faster than conditional exponential family models (Chen and Rosenfeld, 2000; Chen, 2009; Nelakanti et al., 2013) and neural networks which require O(V ) for exact computation of the normalization constant. See Section 7 for a more detailed discussion of related wor</context>
<context position="36704" citStr="Saul and Pereira, 1997" startWordPosition="6153" endWordPosition="6156"> or in languages with a large number of word types. Mnih and Teh (2012) and Vaswani et al. (2013) propose setting the normalization constant to 1, but this is approximate and thus can only be used for downstream evaluation, not for perplexity computation. An alternate technique is to use word-classing (Goodman, 2001; Mikolov et al., 2011), which can reduce the cost √ of exact normalization to O( V ). In contrast, our approach is much more scalable, since it is trivially parallelized in training and does not require explicit normalization during evaluation. There are a few low rank approaches (Saul and Pereira, 1997; Bellegarda, 2000; Hutchinson et al., 2011), but they are only effective in restricted settings (e.g. small training sets, or corpora divided into documents) and do not generally perform comparably to state-of-the-art models. Roark et al. (2013) also use the idea of marginal constraints for re-estimating back-off parameters for heavilypruned language models, whereas we use this concept to estimate n-gram specific discounts. 8 Conclusion We presented power low rank ensembles, a technique that generalizes existing n-gram smoothing techniques to non-integer n. By using ensembles of sparse as wel</context>
</contexts>
<marker>Saul, Pereira, 1997</marker>
<rawString>Lawrence Saul and Fernando Pereira. 1997. Aggregate and mixed-order markov models for statistical language processing. In Proceedings of the second conference on empirical methods in natural language processing, pages 81–89. Somerset, New Jersey: Association for Computational Linguistics.</rawString>
</citation>
<citation valid="true">
<authors>
<author>Andreas Stolcke</author>
</authors>
<title>SRILM - An Extensible Language Modeling Toolkit.</title>
<date>2002</date>
<booktitle>In Proceedings of the International Conference in Spoken Language Processing.</booktitle>
<contexts>
<context position="26969" citStr="Stolcke, 2002" startWordPosition="4561" endWordPosition="4562">thing, class-based models, and the log-bilinear neural language model (Mnih and Hinton, 2007). We evaluated with perplexity in most of our experiments, but also provide results evaluated with BLEU (Papineni et al., 2002) on a downstream machine translation (MT) task. We have made the code for our approach publicly available 3. To build the hard class-based LMs, we utilized mkcls4, a tool to train word classes that uses the maximum likelihood criterion (Och, 1995) for classing. We subsequently trained trigram class language models on these classes (corresponding to 2nd-order HMMs) using SRILM (Stolcke, 2002), with KN-smoothing for the class transition probabilities. SRILM was also used for the baseline KN-smoothed models. For our MT evaluation, we built a hierarchical phrase translation (Chiang, 2007) system using cdec (Dyer et al., 2010). The KN-smoothed models in the MT experiments were compiled using KenLM (Heafield, 2011). 5.1 Datasets For the perplexity experiments, we evaluated our proposed approach on 4 datasets, 2 in English and 2 in Russian. In all cases, the singletons were replaced with “&lt;unk&gt;” tokens in the training corpus, and any word not in the vocabulary was replaced with this tok</context>
</contexts>
<marker>Stolcke, 2002</marker>
<rawString>Andreas Stolcke. 2002. SRILM - An Extensible Language Modeling Toolkit. In Proceedings of the International Conference in Spoken Language Processing.</rawString>
</citation>
<citation valid="true">
<authors>
<author>Xiaoyuan Su</author>
<author>Taghi M Khoshgoftaar</author>
</authors>
<title>A survey of collaborative filtering techniques.</title>
<date>2009</date>
<booktitle>Advances in artificial intelligence,</booktitle>
<pages>2009--4</pages>
<contexts>
<context position="2465" citStr="Su and Khoshgoftaar, 2009" startWordPosition="364" endWordPosition="367">cs.cmu.edu Eric P. Xing School of Computer Science Carnegie Mellon University epxing@cs.cmu.edu Many smoothing techniques have been proposed to address the estimation challenge. These reassign probability mass (generally from overestimated events) to unseen word sequences, whose probabilities are estimated by interpolating with or backing off to lower order n-gram models (Chen and Goodman, 1999). Somewhat surprisingly, these widely used smoothing techniques differ substantially from techniques for coping with data sparsity in other domains, such as collaborative filtering (Koren et al., 2009; Su and Khoshgoftaar, 2009) or matrix completion (Cand`es and Recht, 2009; Cai et al., 2010). In these areas, low rank approaches based on matrix factorization play a central role (Lee and Seung, 2001; Salakhutdinov and Mnih, 2008; Mackey et al., 2011). For example, in recommender systems, a key challenge is dealing with the sparsity of ratings from a single user, since typical users will have rated only a few items. By projecting the low rank representation of a user’s (sparse) preferences into the original space, an estimate of ratings for new items is obtained. These methods are attractive due to their computational </context>
</contexts>
<marker>Su, Khoshgoftaar, 2009</marker>
<rawString>Xiaoyuan Su and Taghi M Khoshgoftaar. 2009. A survey of collaborative filtering techniques. Advances in artificial intelligence, 2009:4.</rawString>
</citation>
<citation valid="true">
<authors>
<author>Yee Whye Teh</author>
</authors>
<title>A hierarchical bayesian language model based on pitman-yor processes.</title>
<date>2006</date>
<booktitle>In Proceedings of the 21st International Conference on Computational Linguistics and the 44th annual meeting of the Association for Computational Linguistics,</booktitle>
<pages>985--992</pages>
<publisher>Association</publisher>
<institution>for Computational Linguistics.</institution>
<contexts>
<context position="35051" citStr="Teh (2006)" startWordPosition="5885" endWordPosition="5886">mes to control for optimizer instability (Clark et al., 2011). Unlike other recent approaches where an additional feature weight is tuned for the proposed model and used in conjunction with KN smoothing (Vaswani et al., 2013), our aim is to show the improvements that PLRE provides as a substitute for KN. On average, PLRE outperforms the KN baseline by 0.16 BLEU, and this improvement is consistent in that PLRE never gets a worse BLEU score. 7 Related Work Recent attempts to revisit the language modeling problem have largely come from two directions: Bayesian nonparametrics and neural networks. Teh (2006) and Goldwater et al. (2006) discovered the connection between interpolated Kneser Ney and the hierarchical Pitman-Yor process. These have led to generalizations that account for domain effects (Wood and Teh, 2009) and unbounded contexts (Wood et al., 2009). The idea of using neural networks for language modeling is not new (Miikkulainen and Dyer, 1991), but recent efforts (Mnih and Hinton, 2007; Mikolov et al., 2010) have achieved impressive performance. These methods can be quite expensive to train and query (especially as the vocabulary size increases). Techniques such as noise contrastive </context>
</contexts>
<marker>Teh, 2006</marker>
<rawString>Yee Whye Teh. 2006. A hierarchical bayesian language model based on pitman-yor processes. In Proceedings of the 21st International Conference on Computational Linguistics and the 44th annual meeting of the Association for Computational Linguistics, pages 985–992. Association for Computational Linguistics.</rawString>
</citation>
<citation valid="true">
<authors>
<author>Ashish Vaswani</author>
<author>Yinggong Zhao</author>
<author>Victoria Fossum</author>
<author>David Chiang</author>
</authors>
<title>Decoding with largescale neural language models improves translation.</title>
<date>2013</date>
<booktitle>In Proceedings of the 2013 Conference on Empirical Methods in Natural Language Processing,</booktitle>
<pages>1387--1392</pages>
<publisher>Association</publisher>
<institution>for Computational Linguistics.</institution>
<location>Seattle, Washington, USA,</location>
<contexts>
<context position="34666" citStr="Vaswani et al., 2013" startWordPosition="5816" endWordPosition="5819">LMs - the set of feature weights obtained using int-MKN is the same, only the language model changes. The 6As described earlier, only the ranks need to be tuned, so only 2-3 low rank bigrams and 2-3 low rank trigrams need to be computed (and combined depending on the setting). 7the best score at WMT 2013 was 19.9 (Bojar et al., 2013) Small-English Small-Russian 1495 procedure is repeated 10 times to control for optimizer instability (Clark et al., 2011). Unlike other recent approaches where an additional feature weight is tuned for the proposed model and used in conjunction with KN smoothing (Vaswani et al., 2013), our aim is to show the improvements that PLRE provides as a substitute for KN. On average, PLRE outperforms the KN baseline by 0.16 BLEU, and this improvement is consistent in that PLRE never gets a worse BLEU score. 7 Related Work Recent attempts to revisit the language modeling problem have largely come from two directions: Bayesian nonparametrics and neural networks. Teh (2006) and Goldwater et al. (2006) discovered the connection between interpolated Kneser Ney and the hierarchical Pitman-Yor process. These have led to generalizations that account for domain effects (Wood and Teh, 2009) </context>
<context position="36179" citStr="Vaswani et al. (2013)" startWordPosition="6066" endWordPosition="6069">nd query (especially as the vocabulary size increases). Techniques such as noise contrastive estimation (Gutmann and Hyv¨arinen, 2012; Mnih and Teh, 2012; Vaswani et al., 2013), subsampling (Xu et al., 2011), or careful engineering approaches for maximum entropy LMs (which can also be applied to neural networks) (Wu and Khudanpur, 2000) have improved training of these models, but querying the probability of the next word given still requires explicitly normalizing over the vocabulary, which is expensive for big corpora or in languages with a large number of word types. Mnih and Teh (2012) and Vaswani et al. (2013) propose setting the normalization constant to 1, but this is approximate and thus can only be used for downstream evaluation, not for perplexity computation. An alternate technique is to use word-classing (Goodman, 2001; Mikolov et al., 2011), which can reduce the cost √ of exact normalization to O( V ). In contrast, our approach is much more scalable, since it is trivially parallelized in training and does not require explicit normalization during evaluation. There are a few low rank approaches (Saul and Pereira, 1997; Bellegarda, 2000; Hutchinson et al., 2011), but they are only effective i</context>
</contexts>
<marker>Vaswani, Zhao, Fossum, Chiang, 2013</marker>
<rawString>Ashish Vaswani, Yinggong Zhao, Victoria Fossum, and David Chiang. 2013. Decoding with largescale neural language models improves translation. In Proceedings of the 2013 Conference on Empirical Methods in Natural Language Processing, pages 1387–1392, Seattle, Washington, USA, October. Association for Computational Linguistics.</rawString>
</citation>
<citation valid="true">
<authors>
<author>Frank Wood</author>
<author>Yee Whye Teh</author>
</authors>
<title>A hierarchical nonparametric Bayesian approach to statistical language model domain adaptation.</title>
<date>2009</date>
<booktitle>In Artificial Intelligence and Statistics,</booktitle>
<pages>607--614</pages>
<contexts>
<context position="35265" citStr="Wood and Teh, 2009" startWordPosition="5916" endWordPosition="5919">aswani et al., 2013), our aim is to show the improvements that PLRE provides as a substitute for KN. On average, PLRE outperforms the KN baseline by 0.16 BLEU, and this improvement is consistent in that PLRE never gets a worse BLEU score. 7 Related Work Recent attempts to revisit the language modeling problem have largely come from two directions: Bayesian nonparametrics and neural networks. Teh (2006) and Goldwater et al. (2006) discovered the connection between interpolated Kneser Ney and the hierarchical Pitman-Yor process. These have led to generalizations that account for domain effects (Wood and Teh, 2009) and unbounded contexts (Wood et al., 2009). The idea of using neural networks for language modeling is not new (Miikkulainen and Dyer, 1991), but recent efforts (Mnih and Hinton, 2007; Mikolov et al., 2010) have achieved impressive performance. These methods can be quite expensive to train and query (especially as the vocabulary size increases). Techniques such as noise contrastive estimation (Gutmann and Hyv¨arinen, 2012; Mnih and Teh, 2012; Vaswani et al., 2013), subsampling (Xu et al., 2011), or careful engineering approaches for maximum entropy LMs (which can also be applied to neural net</context>
</contexts>
<marker>Wood, Teh, 2009</marker>
<rawString>Frank Wood and Yee Whye Teh. 2009. A hierarchical nonparametric Bayesian approach to statistical language model domain adaptation. In Artificial Intelligence and Statistics, pages 607–614.</rawString>
</citation>
<citation valid="true">
<authors>
<author>Frank Wood</author>
<author>C´edric Archambeau</author>
<author>Jan Gasthaus</author>
<author>Lancelot James</author>
<author>Yee Whye Teh</author>
</authors>
<title>A stochastic memoizer for sequence data.</title>
<date>2009</date>
<booktitle>In Proceedings of the 26th Annual International Conference on Machine Learning,</booktitle>
<pages>1129--1136</pages>
<publisher>ACM.</publisher>
<contexts>
<context position="35308" citStr="Wood et al., 2009" startWordPosition="5923" endWordPosition="5926"> improvements that PLRE provides as a substitute for KN. On average, PLRE outperforms the KN baseline by 0.16 BLEU, and this improvement is consistent in that PLRE never gets a worse BLEU score. 7 Related Work Recent attempts to revisit the language modeling problem have largely come from two directions: Bayesian nonparametrics and neural networks. Teh (2006) and Goldwater et al. (2006) discovered the connection between interpolated Kneser Ney and the hierarchical Pitman-Yor process. These have led to generalizations that account for domain effects (Wood and Teh, 2009) and unbounded contexts (Wood et al., 2009). The idea of using neural networks for language modeling is not new (Miikkulainen and Dyer, 1991), but recent efforts (Mnih and Hinton, 2007; Mikolov et al., 2010) have achieved impressive performance. These methods can be quite expensive to train and query (especially as the vocabulary size increases). Techniques such as noise contrastive estimation (Gutmann and Hyv¨arinen, 2012; Mnih and Teh, 2012; Vaswani et al., 2013), subsampling (Xu et al., 2011), or careful engineering approaches for maximum entropy LMs (which can also be applied to neural networks) (Wu and Khudanpur, 2000) have improv</context>
</contexts>
<marker>Wood, Archambeau, Gasthaus, James, Teh, 2009</marker>
<rawString>Frank Wood, C´edric Archambeau, Jan Gasthaus, Lancelot James, and Yee Whye Teh. 2009. A stochastic memoizer for sequence data. In Proceedings of the 26th Annual International Conference on Machine Learning, pages 1129–1136. ACM.</rawString>
</citation>
<citation valid="true">
<authors>
<author>Jun Wu</author>
<author>Sanjeev Khudanpur</author>
</authors>
<title>Efficient training methods for maximum entropy language modeling.</title>
<date>2000</date>
<booktitle>In Interspeech,</booktitle>
<pages>114--118</pages>
<contexts>
<context position="35896" citStr="Wu and Khudanpur, 2000" startWordPosition="6016" endWordPosition="6019">ounded contexts (Wood et al., 2009). The idea of using neural networks for language modeling is not new (Miikkulainen and Dyer, 1991), but recent efforts (Mnih and Hinton, 2007; Mikolov et al., 2010) have achieved impressive performance. These methods can be quite expensive to train and query (especially as the vocabulary size increases). Techniques such as noise contrastive estimation (Gutmann and Hyv¨arinen, 2012; Mnih and Teh, 2012; Vaswani et al., 2013), subsampling (Xu et al., 2011), or careful engineering approaches for maximum entropy LMs (which can also be applied to neural networks) (Wu and Khudanpur, 2000) have improved training of these models, but querying the probability of the next word given still requires explicitly normalizing over the vocabulary, which is expensive for big corpora or in languages with a large number of word types. Mnih and Teh (2012) and Vaswani et al. (2013) propose setting the normalization constant to 1, but this is approximate and thus can only be used for downstream evaluation, not for perplexity computation. An alternate technique is to use word-classing (Goodman, 2001; Mikolov et al., 2011), which can reduce the cost √ of exact normalization to O( V ). In contras</context>
</contexts>
<marker>Wu, Khudanpur, 2000</marker>
<rawString>Jun Wu and Sanjeev Khudanpur. 2000. Efficient training methods for maximum entropy language modeling. In Interspeech, pages 114–118.</rawString>
</citation>
<citation valid="true">
<authors>
<author>Puyang Xu</author>
<author>Asela Gunawardana</author>
<author>Sanjeev Khudanpur</author>
</authors>
<title>Efficient subsampling for training complex language models.</title>
<date>2011</date>
<booktitle>In Proceedings of the Conference on Empirical Methods in Natural Language Processing, EMNLP ’11,</booktitle>
<pages>1128--1136</pages>
<publisher>Association</publisher>
<institution>for Computational Linguistics.</institution>
<location>Stroudsburg, PA, USA.</location>
<contexts>
<context position="35765" citStr="Xu et al., 2011" startWordPosition="5995" endWordPosition="5998">rarchical Pitman-Yor process. These have led to generalizations that account for domain effects (Wood and Teh, 2009) and unbounded contexts (Wood et al., 2009). The idea of using neural networks for language modeling is not new (Miikkulainen and Dyer, 1991), but recent efforts (Mnih and Hinton, 2007; Mikolov et al., 2010) have achieved impressive performance. These methods can be quite expensive to train and query (especially as the vocabulary size increases). Techniques such as noise contrastive estimation (Gutmann and Hyv¨arinen, 2012; Mnih and Teh, 2012; Vaswani et al., 2013), subsampling (Xu et al., 2011), or careful engineering approaches for maximum entropy LMs (which can also be applied to neural networks) (Wu and Khudanpur, 2000) have improved training of these models, but querying the probability of the next word given still requires explicitly normalizing over the vocabulary, which is expensive for big corpora or in languages with a large number of word types. Mnih and Teh (2012) and Vaswani et al. (2013) propose setting the normalization constant to 1, but this is approximate and thus can only be used for downstream evaluation, not for perplexity computation. An alternate technique is t</context>
</contexts>
<marker>Xu, Gunawardana, Khudanpur, 2011</marker>
<rawString>Puyang Xu, Asela Gunawardana, and Sanjeev Khudanpur. 2011. Efficient subsampling for training complex language models. In Proceedings of the Conference on Empirical Methods in Natural Language Processing, EMNLP ’11, pages 1128–1136, Stroudsburg, PA, USA. Association for Computational Linguistics.</rawString>
</citation>
<citation valid="true">
<authors>
<author>George Zipf</author>
</authors>
<title>Human behaviour and the principle of least-effort.</title>
<date>1949</date>
<publisher>Addison-Wesley,</publisher>
<location>Cambridge, MA.</location>
<contexts>
<context position="1518" citStr="Zipf, 1949" startWordPosition="234" endWordPosition="235">obability of sequences of words in a language and is an important component in, among other applications, automatic speech recognition (Rabiner and Juang, 1993) and machine translation (Koehn, 2010). The predominant approach to language modeling is the n-gram model, wherein the probability of a word sequence P(w1, ... , w`) is decomposed using the chain rule, and then a Markov assumption is made: P(w1, ... , w`) ≈ H`i=1 P(wi|wi−1 i−n+1). While this assumption substantially reduces the modeling complexity, parameter estimation remains a major challenge. Due to the power-law nature of language (Zipf, 1949), the maximum likelihood estimator massively overestimates the probability of rare events and assigns zero probability to legitimate word sequences that happen not to have been observed in the training data (Manning and Sch¨utze, 1999). Avneesh Saluja Electrical &amp; Computer Engineering Carnegie Mellon University avneesh@cs.cmu.edu Eric P. Xing School of Computer Science Carnegie Mellon University epxing@cs.cmu.edu Many smoothing techniques have been proposed to address the estimation challenge. These reassign probability mass (generally from overestimated events) to unseen word sequences, whose</context>
</contexts>
<marker>Zipf, 1949</marker>
<rawString>George Zipf. 1949. Human behaviour and the principle of least-effort. Addison-Wesley, Cambridge, MA.</rawString>
</citation>
</citationList>
</algorithm>
</algorithms>
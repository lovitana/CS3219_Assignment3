<?xml version="1.0" encoding="UTF-8"?>
<algorithms version="110505">
<algorithm name="SectLabel" version="110505">
<variant no="0" confidence="0.013997">
<title confidence="0.997629">
Language Modeling with Functional Head Constraint for Code Switching
Speech Recognition
</title>
<author confidence="0.999305">
Ying Li and Pascale Fung
</author>
<affiliation confidence="0.995063666666667">
Human Language Technology Center
Department of Electronic and Computer Engineering
The Hong Kong University of Science and Technology
</affiliation>
<email confidence="0.997258">
eewing@ee.ust.hk, pascale@ece.ust.hk
</email>
<sectionHeader confidence="0.997376" genericHeader="abstract">
Abstract
</sectionHeader>
<bodyText confidence="0.999950916666667">
In this paper, we propose novel struc-
tured language modeling methods for code
mixing speech recognition by incorporat-
ing a well-known syntactic constraint for
switching code, namely the Functional
Head Constraint (FHC). Code mixing data
is not abundantly available for training
language models. Our proposed meth-
ods successfully alleviate this core prob-
lem for code mixing speech recognition
by using bilingual data to train a struc-
tured language model with syntactic con-
straint. Linguists and bilingual speakers
found that code switch do not happen be-
tween the functional head and its comple-
ments. We propose to learn the code mix-
ing language model from bilingual data
with this constraint in a weighted finite
state transducer (WFST) framework. The
constrained code switch language model is
obtained by first expanding the search net-
work with a translation model, and then
using parsing to restrict paths to those per-
missible under the constraint. We im-
plement and compare two approaches -
lattice parsing enables a sequential cou-
pling whereas partial parsing enables a
tight coupling between parsing and fil-
tering. We tested our system on a lec-
ture speech dataset with 16% embedded
second language, and on a lunch conver-
sation dataset with 20% embedded lan-
guage. Our language models with lattice
parsing and partial parsing reduce word
error rates from a baseline mixed lan-
guage model by 3.8% and 3.9% in terms
of word error rate relatively on the aver-
age on the first and second tasks respec-
tively. It outperforms the interpolated lan-
guage model by 3.7% and 5.6% in terms of
word error rate relatively, and outperforms
the adapted language model by 2.6% and
4.6% relatively. Our proposed approach
avoids making early decisions on code-
switch boundaries and is therefore more
robust. We address the code switch data
scarcity challenge by using bilingual data
with syntactic structure.
</bodyText>
<sectionHeader confidence="0.999649" genericHeader="keywords">
1 Introduction
</sectionHeader>
<bodyText confidence="0.999959096774193">
In multilingual communities, it is common for
people to mix two or more languages in their
speech. A single sentence spoken by bilingual
speakers often contains the main, matrix language
and an embedded second language. This type
of linguistic phenomenon is called ”code switch-
ing” by linguists. It is increasingly important for
automatic speech recognition (ASR) systems to
recognize code switching speech as they exist in
scenarios such as meeting and interview speech,
lecture speech, and conversational speech. Code
switching is common among bilingual speakers of
Spanish-English, Hindi-English, Chinese-English,
and Arabic-English, among others. In China,
lectures, meetings and conversations with techni-
cal contents are frequently peppered with English
terms even though the general population is not
considered bilingual in Chinese and English. Un-
like the thousands and tens of thousands of hours
of monolingual data available to train, for exam-
ple, voice search engines, transcribed code switch
data necessary for training language models is
hard to come by. Code switch language modeling
is therefore an even harder problem than acoustic
modeling.
One approach for code switch speech recogni-
tion is to explicitly recognizing the code switch
points by language identification first using pho-
netic or acoustic information, before applying
speech recognizers for the matrix and embed-
ded languages (Chan et. al, 2004; Shia et. al,
</bodyText>
<page confidence="0.944961">
907
</page>
<note confidence="0.912464">
Proceedings of the 2014 Conference on Empirical Methods in Natural Language Processing (EMNLP), pages 907–916,
October 25-29, 2014, Doha, Qatar. c�2014 Association for Computational Linguistics
</note>
<bodyText confidence="0.998973205882353">
2004; Lyu and Lyu, 2008). This approach is ex-
tremely error-prone as language identification at
each frame of the speech is necessary and any er-
ror will be propagated in the second speech recog-
nition stage leading to fatal and irrecoverable er-
rors.
Meanwhile, there are two general approaches to
solve the problem of lack of training data for lan-
guage modeling. In a first approach, two language
models are trained from both the matrix and em-
bedded language separately and then interpolated
together (Vu et. al, 2012; Chan et. al, 2006). How-
ever, an interpolated language model effectively
allows code switch at all word boundaries without
much of a constraint. Another approach is to adapt
the matrix language language model with a small
amount of code switch data (Tsai et. al, 2010; Yeh
et. al, 2010; Bhuvanagiri and Kopparapu, 2010;
Cao et. al, 2010). The effectiveness of adapta-
tion is also limited as positions of code switch-
ing points are not generalizable from the limited
data. Significant progress in speech recognition
has been made by using deep neural networks for
acoustic modeling and language model. However,
improvement thus gained on code switch speech
recognition remains very small. Again, we pro-
pose that syntactic constraints of the code switch-
ing phenomenon can help improve performance
and model accuracy. Previous work of using part-
of-speech tags (Zhang et. al, 2008; Vu et al 2012)
and our previous work using syntactic constraints
(Li and Fung, 2012, 2013) have made progress
in this area. Part-of-speech is relatively weak in
predicting code switching points. It is generally
accepted by linguists that code switching follows
the so-called Functional Head Constraint, where
words on the nodes of a syntactic sub tree must
follow the language of that of the headword. If the
headword is in the matrix language then none of
its complements can switch to the embedded lan-
guage.
In this work, we propose two ways to incorpo-
rate the Functional Head Constraint into speech
recognition and compare them. We suggest two
approaches of introducing syntactic constraints
into the speech recognition system. One is to ap-
ply the knowledge sources in a sequential order.
The acoustic model and a monolingual language
model are used first to produce an intermediate
lattice, then a second pass choose the best result
using the syntactic constraints. Another approach
uses tight coupling. We propose using structured
language model (Chelba and Jelinek, 2000) to
build the syntactic structure incrementally.
Following our previous work, we suggest in-
corporating the acoustic model, the monolingual
language model and a translation model into a
WFST framework. Using a translation model al-
lows us to learn what happens when a language
switches to another with context information. We
will motivate and describe this WFST framework
for code switching speech recognition in the next
section. The Functional Head Constraint is de-
scribed in Section 3. The proposed code switch
language models and speech recognition coupling
is described in Section 4. Experimental setup and
results are presented in Section 5. Finally we con-
clude in Section 6.
</bodyText>
<sectionHeader confidence="0.9947185" genericHeader="introduction">
2 Code Switch Language Modeling in a
WFST Framework
</sectionHeader>
<bodyText confidence="0.999973076923077">
As code switch text data is scarce, we do not have
enough data to train the language model for code
switch speech recognition. We propose instead to
incorporate language model trained in the matrix
language with a translation model to obtain a code
switch language model. We propose to integrate a
bilingual acoustic model (Li et. al, 2011) and the
code switch language model in a weighted finite
state transducer framework as follows.
Suppose X denotes the observed code switch
speech vector, wi denotes a word sequence in the
matrix language, the hypothesis transcript vi is as
follows:
</bodyText>
<equation confidence="0.996840583333333">
ˆvi = arg max
vI
1
=arg max
vI
1
=arg max
vI
1
∼= arg max
vI
1
</equation>
<bodyText confidence="0.9998726">
where P(X|vf) is the acoustic model and P(vf)
is the language model in the mixed language.
Our code switch language model is obtained
from a translation model P(vf|wi ) from the ma-
trix language to the mixed language, and the lan-
guage model in the matrix language P(wi ).
Instead of word-to-word translation, the trans-
duction of the context dependent lexicon trans-
fer is constrained by previous words. Assume the
transduction depends on the previous n words:
</bodyText>
<equation confidence="0.981074823529412">
P(vi|X)
P(X|vi)P(vi)
P(X|v� �) � P(vf|wi )P(wi )
wJ
1
P(X|vf)P(vf|wi )P(wi ) (1)
908
P(vi|vi−1
1 ,wi 1)
P(vi−1 i−n+1|wii−n+1)
i−1 i−1
P(wi  |vi−n+1 , wi−n+1 )
i−1 i−1
P (vi , wi  |vi−n+1 , wi−n+1 )
[� i−1 i−1
P(wi  |Levi vi−n+1 , wi−n+1 )
(2)
</equation>
<bodyText confidence="0.9958302">
There are C-level and H-level search networks
in the WFST framework. The C-level search net-
work is composed of the universal phone model
P, the context model C, the lexicon L, and the
grammar G
</bodyText>
<equation confidence="0.992319">
N=PoCoLoG (3)
</equation>
<bodyText confidence="0.999004333333333">
The H-level search network is composed of the
state model H, the phoneme model P, the context
model C, the lexicon L, and the grammar G
</bodyText>
<equation confidence="0.991204">
N=HoPoCoLoG (4)
</equation>
<bodyText confidence="0.999930444444444">
The C-level requires less memory then the H-level
search network. We propose to use a weighted fi-
nite state transducer framework incorporating the
bilingual acoustic model P, the context model C,
the lexicon L, and the code switching language
models GCS into a C-level search network for
mixed language speech recognition. The output
of the recognition result is in the mixed language
after projection π(GCS).
</bodyText>
<equation confidence="0.98932">
N = P o C o L o π(GCS) (5)
</equation>
<bodyText confidence="0.9961595">
The WFST implementation to obtain the code
switch language model GCS is as follows:
</bodyText>
<equation confidence="0.965398">
Gcs = T o G
(6)
where T is the translation model
L
P(˜vL 1 |wJ1 ) = Pl(˜vl|wl) (7)
l=1
</equation>
<bodyText confidence="0.999432166666667">
Pl(˜vl|wl) is the probability of wl translated into ˜vl.
In order to make use of the text data in the ma-
trix language to recognize speech in the mixed lan-
guage, the translation model P(vI1|wJ1 ) transduce
the language model in the matrix language to the
mixed language.
</bodyText>
<equation confidence="0.868683133333333">
P(vI1|wJ1 ) =
&apos;P(rK1  |˜wK1 , wJ1 )
P(˜&apos;P(LK˜K J
c1 ,r1 ,w1 ,w1 )
K L K˜K J
v1 |c1 ,r1 ,w1 ,w1 )
&apos;P(I˜K K˜K J
v1 |v1 ,r1 ,w1 ,w1 ) (8)
where P( ˜wK 1 |wJ1 ) is the word-to-phrase segmen-
tation model, P(rK 1  |˜wK1 , wJ1 ) is the phrasal re-
ordering model, P(cL 1
, rK1 , ˜wK1 , wJ1 ) is the chunk
segmentation model, P(˜vK1 |cL1 , rK1 , ˜wK1 , wJ1 )
is the chunk-to-chunk transduction model,
P(vI1|˜vK1 ,rK1 ,˜wK1 ,wJ1 ) is the chunk-to-word
</equation>
<bodyText confidence="0.987147684210526">
reconstruction model.
The word-to-phrase segmentation model ex-
tracts a table of phrases {˜v1, ˜v2, ..., ˜vK} for
the transcript in the embedded language and
{ ˜w1, ˜w2, ..., ˜wK} for the transcript in the ma-
trix language based on word-to-word alignments
trained in both directions with GIZA++ (Och and
Ney, 2003). The chunk segmentation model per-
forms the segmentation of a phrase sequence ˜wK1
into L phrases {c1, c2, ..., cL} using a segmenta-
tion weighted finite-state transducer. Assumes that
a chunk cl is code-switched to the embedded lan-
guage independently by each chunk, the chunk-
to-chunk transduction model is the probability of
a chunk to be code switched to the embedded lan-
guage trained on parallel data. The reconstruction
model generates word sequence from chunk se-
quences and operates in the opposite direction to
the segmentation model.
</bodyText>
<sectionHeader confidence="0.996224" genericHeader="method">
3 Functional Head Constraint
</sectionHeader>
<bodyText confidence="0.999941928571429">
Many linguistics (Abney 1986; Belazi et. al, 1994;
Bhatt 1994) have discovered the so-called Func-
tional Head Constraint in code switching. They
have found that code switches between a func-
tional head (a complementizer, a determiner, an
inflection, etc.) and its complement (sentence,
noun-phrase, verb-phrase) do not happen in natu-
ral speech. In addition, the Functional Head Con-
straint is language independent.
In this work, we propose to investigate and
incorporate the Functional Head Constraint into
code switching language modeling in a WFST
framework. Figure 1 shows one of the Functional
Head Constraint examples. Functional heads are
</bodyText>
<equation confidence="0.9836763">
P(vi, wi|vi−1
i−n+1, wi−n+1 )
−
i
1
I
P(vI1|wJ1 ) =
i=1
I
N=
i=1
I
=
i=1
I
=
i=1
E P(˜wK 1 |wJ1 )
L L K˜K
v1 ,c1 ,r1 ,w1
</equation>
<page confidence="0.986619">
909
</page>
<bodyText confidence="0.999981068965517">
the roots of the sub trees and complements are part
of the sub trees. Actual words are the leaf nodes.
According to the Functional Head Constraint, the
leave nodes of a sub tree must be in either the
matrix language or embedded language, following
the language of the functional head. For instance,
the third word “東#q/something” is the head of
the constituents “非常/very 重要的/important 東
#q/something”. These three constituent words
cannot be switched. Thus, it is not permissible
to code switch in the constituent. More precisely,
the language of the constituent is constrained to be
the same as the language of the headword. In the
following sections, we describe the integration of
the Functional Head Constraint and the language
model.
We have found this constraint to be empirically
sound as we look into our collected code mixing
speech and language data. The only violation of
the constraint comes from rare cases of borrowed
words such as brand names with no translation in
the local, matrix language. Borrowed words are
used even by monolingual speakers so they are in
general part of the matrix language lexicon and
require little, if any, special treatment in speech
recognition.
In the following sections, we describe the inte-
gration of Functional Head Constraint and the lan-
guage model.
</bodyText>
<sectionHeader confidence="0.9941045" genericHeader="method">
4 Code Switching Language Modeling
with Functional Head Constraint
</sectionHeader>
<bodyText confidence="0.999841333333333">
We propose two approaches of language model-
ing with Functional Head Constraint: 1) lattice-
parsing and sequential-coupling (Chapplerler et.
al, 1999); 2) partial-parsing and tight-coupling
(Chapplerler et. al, 1999). The two approaches
will be described in the followed sections.
</bodyText>
<subsectionHeader confidence="0.998756">
4.1 Sequential-coupling by Lattice-based
Parsing
</subsectionHeader>
<bodyText confidence="0.995821639344263">
In this first approach, the acoustic models, the
code switch language model and the syntactic con-
straint are incorporated in a sequential order to
progressively constrain the search. The acoustic
models and the matrix language model are used
first to produce an intermediate output. The in-
termediate output is a lattice in which word se-
quences are compactly presented. Lattice-based
parsing is used to expand the word lattice gener-
ated from the first decoding step according to the
Functional Head Constraint.
We have reasons to use word lattice instead
of N-best hypothesis. The number of hypothesis
of word lattice is larger than N-best hypothesis.
Moreover, different kinds of errors correspond to
the language model would be observed if N-best
list is extracted after the first decoding step. The
second pass run over the N-best list will prevent
the language model with Functional Head Con-
straint from correcting the errors. In order to ob-
tain a computational feasible number of hypothe-
ses without bias to the language model in the first
decoding step, word lattice is used as the interme-
diate output of the first decoding step.
A Probabilistic Context-Free Grammar (PCFG)
parser is trained on Penn Treebank data. The
PCFG parser is generalized to take the lattice gen-
erated by the recognizer as the input. Figure 2 il-
lustrates a word lattice which is a compact repre-
sentation of the hypothesis transcriptions of a an
input sentence. All the nodes of the word-lattice
are ordered by increasing depth.
A CYK table is obtained by associating the arcs
with their start and end states in the lattice instead
of their sentence position and initialized all the
cells in the table corresponding to the arcs (Chap-
plerler et. al, 1999). Each cell Ck,j of the ta-
ble is filled by a n-tuple of the non-terminal A,
the length k and the starting position of the word
sequence wj...wj+k if there exists a PCFG rule
A → wj...wj+k, where A is a non-terminal which
parse sequences of words wj...wj+k. In order to
allow all hypothesis transcriptions of word lattice
to be taken into account, multiple word sequences
of the same length and starting point are initialized
in the same cell. Figure 2 mapped the word lattice
of the example to the table, where the starting node
label of the arc is the column index and the length
of the arc is the row index.
The sequential-coupling by lattice-parsing con-
sists of the standard cell-filling and the self-filling
steps. First, the cells Ck,j and Ci−k,j+k are com-
bined to produce a new interpretation for cell Ci,j
. In order to handle the unary context-free produc-
tion A → B and update the cells after the stan-
dard cell-filling, a n-tuple of A, i and j is added
for each n-tuple of the non-terminal B, the length
i and the start j in the cell Ci,j . The parse trees
extracted are associated with the input lattice from
the table starting from the non-terminal label of
the top cell. After the parse tree is obtained, we re-
</bodyText>
<page confidence="0.960733">
910
</page>
<figure confidence="0.86941525">
Hypotheses: EM Jg!R.
,4�910n)?BMM EM theory.
910nXNA this EM theory.
j910nXX is this EM theory.
4910n something is this EM theory. (not permissible)
.
.
.
</figure>
<figureCaption confidence="0.995043">
Figure 1: A Functional Head Constraint example.
</figureCaption>
<figure confidence="0.99972856">
lion
JJ
important
非常
AD
very
lion
JJ
important
東西
NN
something
東西
NN
something
是
VP
is 是
VP
is
是
VC
is
這個
DT
this
最大期望
NN
EM
理MU
NP
this
理MU
NN
theory
理MU
NP
theory
非常/very
重要的/
important
東西/
something 是/is
包括/
conclude這個/最大期望/
this EM
理論/
theory
0 1 2 3 4 5 6 7 8
很/very 重要的/important
</figure>
<figureCaption confidence="0.935479">
Figure 2: An example word lattice in the matrix language.
</figureCaption>
<table confidence="0.711676166666667">
10n A/is
/import
ant
44M 10n 东X A/is In# 31a JVQM J9100
/very /import /somet /includ /this W-/EM /theory
&amp; ant hing e
</table>
<figure confidence="0.770355666666667">
是/is
2 3 4 5 6 7
1
</figure>
<figureCaption confidence="0.999013">
Figure 3: The mapping of the example word lattice to the table.
</figureCaption>
<page confidence="0.994391">
911
</page>
<bodyText confidence="0.999362">
cursively enumerate all its subtrees. Each subtree
is able to code-switch to the embedded language
with a translation probability Pl(˜vl|wl).
The lattice parsing operation consists of the an
encoding of a given word sequence along with
a parse tree (W, T) and a sequence of elemen-
tary model actions. In order to obtain a correct
probability assignment P(W,T) one simply as-
sign proper conditional probabilities to each tran-
sition in the weighted finite states.
The probability of a parse T of a word sequence
WP(W, T) can be calculated as the product of the
probabilities of the subtrees.
</bodyText>
<equation confidence="0.9995815">
P(W,T) = n+1H [P(wk|Wk−1Tk−1) (9)
k=1
</equation>
<bodyText confidence="0.999902266666667">
Where Wk = w0...wk is the first k words in the
sentence, and (Wk, Tk) is the word-and-parse k-
prefix. The probability of the n-tuple of the non-
terminal A, the length i and the starting position j
is the probability of the subtree corresponding to
A parsing throughout the sequence wj...wj+i−1.
The probability of the partial parsing is the product
of probabilities of the subtree parses it is made of.
The probability of an n-tuple is the maximum over
the probabilities of probable parsing path.
The N most probable parses are obtained during
the lattice-parsing.
The probability of a sentence is computed by
adding on the probability of each new context-free
rule in the sentences.
</bodyText>
<subsectionHeader confidence="0.99653">
4.2 Tight-coupling by Incremental Parsing
</subsectionHeader>
<bodyText confidence="0.9880408125">
To integrate the acoustic models, language model
and the syntactic constraint in time synchronous
decoding, an incremental operation is used in this
approach. The final word-level probability as-
signed by our model is calculated using the acous-
tic models, the matrix language model, the struc-
tured language model and the translation model.
The structured language model uses probabilistic
parameterization of a shift-reduce parse (Chelba
and Jelinek, 2000). The tight-coupled language
model consists of three transducers, the word pre-
dictor, the tagger and the constructor. As shown
in Figure 3, Wk = w0...wk is the first k words of
the sentence, Tk contains only those binary sub-
trees whose leaves are completely included in Wk,
excluding w0 =&lt;s&gt;. Single words along with
their POS tag can be regarded as root-only trees.
The exposed head hk is a pair of the headword
of the constituent Wk and the non-terminal label.
The exposed head of single words are pairs of the
words and their POS tags.
Given the word-and-parse (k-1)-prefix
Wk−1Tk−1, the new word wk is predicted by
the word-predictor P(wk|Wk−1Tk−1). Taking
the word-and-parse k − 1-prefix and the next
word as input, the tagger P(tk|wk,Wk−1Tk−1)
gives the POS tag tk of the word wk. Constructor
P(pki |WkTk) assigns a non-terminal label to the
constituent Wk+1. The headword of the newly
built constituent is inherited from either the
headword of the constituent Wk or the next word
wk+1.
</bodyText>
<equation confidence="0.999835333333333">
P(wk|Wk−1Tk−1)
= P(wk|[Wk−1Tk−1])
= P(wk|h0, h−1) (10)
P(tk|wk, Wk−1Tk−1)
= P(tk|wk, [Wk−1Tk−1])
= P(tk|wk,h0.tag,h−1.tag) (11)
P(pki |WkTk)
= P(pki |[WkTk])
= P(pki |h0, h1) (12)
</equation>
<bodyText confidence="0.998653">
The probability of a parse tree T P(W,T) of a
word sequence W and a complete parse T can be
calculated as:
</bodyText>
<equation confidence="0.9997106">
P(W,T) = n+1H [P(wk|Wk−1Tk−1)
k=1
P(tk|Wk−1Tk−1, wk)
P(Tk|Wk−1Tk−1, wk, tk)](13)
P (Tkk−1|Wk−1Tk−1, wk, tk)
Nk
k k
H P(pk |Wk−1Tk−1, wk, tk,p1...pi−1)
i=1
(14)
</equation>
<bodyText confidence="0.999385111111111">
Where wk is the word predicted by the word-
predictor, tk is the POS tag of the word wk pre-
dicted by the tagger, Wk−1Tk−1 is the word-parse
(k - 1)-prefix, Tkk−1 is the incremental parse struc-
ture that generates Tk = Tk−1||Tkk−1 when at-
tached to Tk−1; it is the parse structure built on
top of Tk−1 and the newly predicted word wk; the
 ||notation stands for concatenation; Nk−1 is the
number of operations the constructor executes at
</bodyText>
<page confidence="0.992517">
912
</page>
<figureCaption confidence="0.999939">
Figure 4: A word-and-parse example.
</figureCaption>
<bodyText confidence="0.99769971875">
position k of the input string before passing con-
trol to the word-predictor (the Nk th operation at
position k is the null transition); Nk is a function
of T ; pki denotes the i th constructor action carried
out at position k in the word string.
The probability models of word-predictor, tag-
ger and constructor are initialized from the Upenn
Treebank with headword percolation and bina-
rization. The headwords are percolated using a
context-free approach based on rules of predict-
ing the position of the headword of the constituent.
The approach consists of three steps. First a parse
tree is decomposed to phrase constituents. Then
the headword position is identified and filled in
with the actual word percolated up from the leaves
of the tree recursively.
Instead of the UPenn Treebank-style, we use a
more convenient binary branching tree. The parse
trees are binarized using a rule-based approach.
The probability models of the word-predictor,
tagger and constructor are trained in a maximiza-
tion likelihood manner. The possible POS tag as-
signments, binary branching parse, non-terminal
labels and the head-word annotation for a given
sentence are hidden. We re-estimate them using
EM algorithm.
Instead of generating only the complete parse,
all parses for all the subsequences of the sen-
tence are produced. The headwords of the subtrees
are code switched to the embedded language with
a translation probability Pl(˜vl|wl) as well as the
leaves.
</bodyText>
<subsectionHeader confidence="0.999453">
4.3 Decoding by Translation
</subsectionHeader>
<bodyText confidence="0.999876692307692">
Using either lattice parsing or partial parsing, a
two-pass decoding is needed to recognize code
switch speech. A computationally feasible first
pass generates an intermediate result so that the
language model with Functional Head constraint
can be used in the second pass. The first decoding
pass composes of the transducer of the universal
phoneme model P, the transducer C from context-
dependent phones to context-independent phones,
the lexicon transducer L which maps context-
independent phone sequences to word strings and
the transducer of the language model G. A T3 de-
coder is used in the first pass.
</bodyText>
<equation confidence="0.973187">
A5R1=PoCoLoG (15)
</equation>
<bodyText confidence="0.998556285714286">
Instead of N-best list, word lattice is used as the
intermediate output of the first decoding step.
The language model GCS of the transducer in
the second pass is improved from G by compos-
ing with the translation model Pl(˜vl|wl). Finally,
the recognition transducer is optimized by deter-
mination and minimization operations.
</bodyText>
<equation confidence="0.983951">
A5R2 = PoComin(det(Lomin(det(π(GCS)))))
(16)
</equation>
<sectionHeader confidence="0.999607" genericHeader="evaluation">
5 Experiments
</sectionHeader>
<subsectionHeader confidence="0.997971">
5.1 Experimental Setup
</subsectionHeader>
<bodyText confidence="0.994673473684211">
The bilingual acoustic model used for our mixed
language ASR is trained from 160 hours of speech
from GALE Phase 1 Chinese broadcast conver-
sation, 40 hours of speech from GALE Phase 1
English broadcast conversation, and 3 hours of
in-house nonnative English data. The acoustic
features used in our experiments consist of 39
components (13MFCC, 13MFCC, 13MFCC us-
ing cepstral mean normalization), which are an-
alyzed at a 10msec frame rate with a 25msec win-
dow size. The acoustic models used throughout
our paper are state-clustered crossword tri-phone
HMMs with 16 Gaussian mixture output densi-
ties per state. We use the phone set consists of
21 Mandarin standard initials, 37 Mandarin finals,
6 zero initials and 6 extended English phones. The
pronunciation dictionary is obtained by modify-
ing Mandarin and English dictionaries using the
phone set. The acoustic models are reconstructed
</bodyText>
<page confidence="0.999131">
913
</page>
<tableCaption confidence="0.999803">
Table 1: Code switching point detection evaluation (Precision/Recall/F-measure)
</tableCaption>
<table confidence="0.998691166666667">
Lecture speech Lunch conversation
MixedLM 0.61/0.64/0.64 0.54/0.63/0.58
InterpolatedLM 0.62/0.66/0.64 0.55/0.63/0.58
AdaptedLM 0.63/0.71/0.67 0.54/0.63/0.58
Sequential coupling 0.66/0.71/0.68 0.55/0.70/0.61
Tight coupling 0.68/0.71/0.70 0.56/0.70/0.62
</table>
<bodyText confidence="0.999860340909091">
by decision tree tying. We also collected two
speech databases with Chinese to English code
switching - namely, 20 hours of lecture speech cor-
pus (Data 1) and 3 hours of lunch conversation
corpus (Data 2). 18 hours of Data 1 is used for
acoustic model adaptation and 1 hour of data are
used as the test set (Test 1). 2 hours of Data 2 con-
taining 2389 utterances is used to adapt the acous-
tic model and 280 utterances are used as the test
set (Test 2). To train the parser, we use Chinese
Treebank Version 5.0 which consists of 500 thou-
sand words and use the standard data split (Petrov
and Klein, 2007).
For the language models, transcriptions of 18
hours of Data 1 are trained as a baseline mixed
language model for the lecture speech domain.
250,000 sentences from Chinese speech confer-
ence papers, power point slides and web data
are used for training a baseline Chinese matrix
language model for the lecture speech domain
(LM 1). Transcriptions of 2 hours of Data 2 are
used as the baseline mixed language model in the
lunch conversation domain. 250,000 sentences of
the GALE Phase 1 Chinese conversational speech
transcriptions are used to train a Chinese ma-
trix language model (LM 2). 250,000 of GALE
Phase 1 English conversational speech transcrip-
tion are used to train the English embedded lan-
guage model (LM 3). To train the bilingual trans-
lation model, the Chinese Gale Phase 1 conversa-
tional speech transcriptions are used to generate
a bilingual corpus using machine translation. For
comparison, an interpolated language model for
the lunch conversation domain is trained from in-
terpolating LM 2 with LM 3. Also for comparison,
an adapted language model for lecture speech is
trained from LM 1 and transcriptions of 18 hours
of Data 1. An adapted language mode l for conver-
sation is trained from LM 2 and 2 hours of Data 2.
The size of the vocabulary for recognition is 20k
words. The perplexity of the baseline language
model trained on the code switching speech tran-
scription is 236 on the lecture speech and 279 on
the conversation speech test sets.
</bodyText>
<subsectionHeader confidence="0.992699">
5.2 Experimental Results
</subsectionHeader>
<bodyText confidence="0.999912944444444">
Table 1 reports precision, recall and F-measure
of code switching point in the recognition results
of the baseline and our proposed language mod-
els. Our proposed code switching language mod-
els with functional head constraint improve both
precision and recall of the code switching point
detection on the code switching lecture speech and
lunch conversation 4.48%. Our method by tight-
coupling increases the F-measure by 9.38% rela-
tively on the lecture speech and by 6.90% rela-
tively on the lunch conversation compared to the
baseline adapted language model.
The Table 2 shows the word error rates (WERs)
of experiments on the code switching lecture
speech and Table 3 shows the WERs on the code
switching lunch conversations. Our proposed code
switching language model with Functional Head
Constraints by sequential-coupling reduces the
WERs in the baseline mixed language model by
3.72% relative on Test 1, and 5.85% on Test 2. Our
method by tight-coupling also reduces WER by
2.51% relative compared to the baseline language
model on Test 1, and by 4.57% on Test 2. We
use the speech recognition scoring toolkit (SCTK)
developed by the National Institute of Standards
and Technology to compute the significance lev-
els, which is based on two-proportion z-test com-
paring the difference between the recognition re-
sults of our proposed approach and the baseline.
All the WER reductions are statistically signifi-
cant. For our reference, we also compare the per-
formance of using Functional Head Constraint to
that of using inversion constraint in (Li and Fung,
2012, 2013) and found that the present model re-
duces WER by 0.85% on Test 2 but gives no im-
provement on Test 1. We hypothesize that since
</bodyText>
<page confidence="0.997757">
914
</page>
<tableCaption confidence="0.999279">
Table 2: Our proposed system outperforms the baselines in terms of WER on the lecture speech
</tableCaption>
<table confidence="0.999835">
Matrix Embedded Overall
MixedLM 34.41% 39.16% 35.17%
InterpolatedLM 34.11% 40.28% 35.10%
AdaptedLM 35.11% 38.41% 34.73%
Sequential coupling 33.17% 36.84% 33.76%
Tight coupling 33.14% 36.65% 33.70%
</table>
<tableCaption confidence="0.998336">
Table 3: Our proposed system outperforms the baselines in terms of WER on the lunch conversation
</tableCaption>
<table confidence="0.998810833333333">
Matrix Embedded Overall
MixedLM 46.4% 48.55% 46.83%
InterpolatedLM 46.04% 49.04% 46.64%
AdaptedLM 46.64% 48.39% 46.20%
Sequential coupling 43.24% 46.27% 43.89%
Tight coupling 42.97% 46.03% 43.58%
</table>
<bodyText confidence="0.995962166666667">
Test 1 has mostly Chinese words, the proposed
method is not as advantageous compared to our
previous work. Another future direction is for us
to improve the lattice parser as we believe it will
lead to further improvement on the final result of
our proposed method.
</bodyText>
<sectionHeader confidence="0.999429" genericHeader="conclusions">
6 Conclusion
</sectionHeader>
<bodyText confidence="0.999991764705882">
In this paper, we propose using lattice parsing and
partial parsing to incorporate a well-known syn-
tactic constraint for code mixing speech, namely
the Functional Head Constraint, into a continu-
ous speech recognition system. Under the Func-
tional Head Constraint, code switch cannot occur
between the functional head and its complements.
Since code mixing speech data is scarce, we pro-
pose to instead learn the code mixing language
model from bilingual data with this constraint.
The constrained code switching language model
is obtained by first expanding the search network
with a translation model, and then using parsing to
restrict paths to those permissible under the con-
straint. Lattice parsing enables a sequential cou-
pling of parsing then constraint filtering whereas
partial parsing enables a tight coupling between
parsing and filtering. A WFST-based decoder
then combines a bilingual acoustic model and the
proposed code-switch language model in an inte-
grated approach. Lattice-based parsing and partial
parsing are used to provide the syntactic structure
of the matrix language. Matrix words at the leave
nodes of the syntax tree are permitted to switch to
the embedded language if the switch does not vio-
late the Functional Head Constraint. This reduces
the permissible search paths from those expanded
by the bilingual language model. We tested our
system on a lecture speech dataset with 16% em-
bedded second language, and on a lunch conversa-
tion dataset with 20% embedded second language.
Our language models with lattice parsing and par-
tial parsing reduce word error rates from a baseline
mixed language model by 3.72% to 3.89% rela-
tive in the first task, and by 5.85% to 5.97% in
the second task. They are reduced from an inter-
polated language model by 3.69% to 3.74%, and
by 5.46% to 5.77% in the first and second task re-
spectively. WER reductions from an adapted lan-
guage model are 2.51% to 2.63%, and by 4.47%
to 4.74% in the two tasks. The F-measure for code
switch point detection is improved from 0.64 by
the interpolated model to 0.68, and from 0.67 by
the adapted model to 0.70 by our method. Our
proposed approach avoids making early decisions
on code-switch boundaries and is therefore more
robust. Our approach also avoids the bottleneck of
code switch data scarcity by using bilingual data
with syntactic structure. Moreover, our method re-
duces word error rates for both the matrix and the
embedded language.
</bodyText>
<sectionHeader confidence="0.99935" genericHeader="acknowledgments">
Acknowledgments
</sectionHeader>
<bodyText confidence="0.9675948">
This work is partially supported by grant number
RGF 612211 of the Hong Kong Research Grants
Council, by 1314159-0PAFT20F003 of the Ping
An Research Institute and by 13140910 of the
Huawei Noah’s Ark Lab.
</bodyText>
<page confidence="0.99814">
915
</page>
<sectionHeader confidence="0.998341" genericHeader="references">
References
</sectionHeader>
<reference confidence="0.999775085365853">
J.J. Gumperz, “Discourse strategies”, Cambridge Uni-
versity Press, 1, 1982.
Coulmas, F., “The handbook of sociolinguistics”,
Wiley-Blackwell, 1998.
Vu, N.T. and Lyu, D.C. and Weiner, J. and Telaar, D.
and Schlippe, T. and Blaicher, F. and Chng, E.S. and
Schultz, T. and Li, H. A first speech recognition
system for Mandarin-English code-switch conversa-
tional speech’, ICASSP, 2012
J.Y.C. Chan and PC Ching and T. Lee and H.M. Meng
“Detection of language boundary in code-switching
utterances by bi-phone probabilities” Chinese Spo-
ken Language Processing, 2004 International Sym-
posium on, 293–296.
C.J. Shia and Y.H. Chiu and J.H. Hsieh and C.H.
Wu “Language boundary detection and identifica-
tion of mixed-language speech based on MAP es-
timation””, ICASSP 2004.
D.C. Lyu and R.Y. Lyu “Language identification
on code-switching utterances using multiple cues”
Ninth Annual Conference of the International
Speech Communication Association, 2008.
Tsai, T.L. and Chiang, C.Y. and Yu, H.M. and Lo,
L.S. and Wang, Y.R. and Chen, S.H. “A study on
Hakka and mixed Hakka-Mandarin speech recogni-
tion” Chinese Spoken Language Processing (ISC-
SLP), 2010 7th International Symposium on, 199–
204
Yeh, C.F. and Huang, C.Y. and Sun, L.C. and
Lee, L.S. “An integrated framework for transcrib-
ing Mandarin-English code-mixed lectures with im-
proved acoustic and language modeling” Chinese
Spoken Language Processing (ISCSLP), 2010 7th
International Symposium on, 214–219
K. Bhuvanagiri and S. Kopparapu, “An Approach to
Mixed Language Automatic Speech Recognition”,
Oriental COCOSDA, Kathmandu, Nepal, 2010
Cao, H. and Ching, PC and Lee, T. and Ye-
ung, Y.T. “Semantics-based language modeling for
Cantonese-English code-mixing speech recognition
Chinese Spoken Language Processing (ISCSLP),
2010 7th International Symposium on,246–250
Chelba, Ciprian, and Frederick Jelinek. ”Structured
language modeling.” Computer Speech &amp; Language
14, no. 4 (2000): 283-332.
Imseng, D. and Bourlard, H. and Magimai-Doss,
M. and Dines, J., “Language dependent universal
phoneme posterior estimation for mixed language
speech recognition”, ICASSP, 2011.
Q. Zhang and J. Pan and Y. Yan, “Mandarin-English
bilingual speech recognition for real world music re-
trieval”, ICASSP, 2008.
Bouselmi, G. and Fohr, D. and Illina, I., “Combined
acoustic and pronunciation modelling for non-native
speech recognition”, Eighth Annual Conference of
the International Speech Communication Associa-
tion, 2007.
Woolford, E., “Bilingual code-switching and syntactic
theory”, in Linguistic Inquiry, 14(3):520–536, JS-
TOR, 1983.
MacSwan, J., “13 Code-switching and grammatical
theory”, in The Handbook of Bilingualism and Mul-
tilingualism, 323 Wiley-Blackwell, 2012.
Poplack, S. and Sankoff, D., “A formal grammar for
code-switching”, in Papers in Linguistics: Inter-
national Journal of Human Communication, 3–45,
1980.
Moore, Robert C and Lewis, William, “Intelligent se-
lection of language model training data” Proceed-
ings of the ACL 2010 Conference Short Papers,
220–224.
Belazi, Heidi; Edward Rubin; Almeida Jacqueline
Toribio “Code switching and X-Bar theory: The
functional head constraint”. Linguistic Inquiry 25
(2): 221-37, 1994.
Bhatt, Rakesh M., “Code-switching and the functional
head constraint” In Janet Fuller et al. Proceedings of
the Eleventh Eastern States Conference on Linguis-
tics. Ithaca, NY: Department of Modern Languages
and Linguistics. pp. 1-12, 1995
Chappelier, Jean-C?dric, et al., “Lattice parsing for
speech recognition.” TALN 1999.
</reference>
<page confidence="0.998593">
916
</page>
</variant>
</algorithm>
<algorithm name="ParsHed" version="110505">
<variant no="0" confidence="0.330684">
<title confidence="0.9989415">Language Modeling with Functional Head Constraint for Code Switching Speech Recognition</title>
<author confidence="0.6887615">Ying Li</author>
<author confidence="0.6887615">Pascale Human Language Technology</author>
<affiliation confidence="0.943072">Department of Electronic and Computer The Hong Kong University of Science and</affiliation>
<email confidence="0.919213">eewing@ee.ust.hk,pascale@ece.ust.hk</email>
<abstract confidence="0.999588020408163">In this paper, we propose novel structured language modeling methods for code mixing speech recognition by incorporating a well-known syntactic constraint for switching code, namely the Functional Head Constraint (FHC). Code mixing data is not abundantly available for training language models. Our proposed methods successfully alleviate this core problem for code mixing speech recognition by using bilingual data to train a structured language model with syntactic constraint. Linguists and bilingual speakers found that code switch do not happen between the functional head and its complements. We propose to learn the code mixing language model from bilingual data with this constraint in a weighted finite state transducer (WFST) framework. The constrained code switch language model is obtained by first expanding the search network with a translation model, and then using parsing to restrict paths to those permissible under the constraint. We implement and compare two approaches lattice parsing enables a sequential coupling whereas partial parsing enables a tight coupling between parsing and filtering. We tested our system on a lecture speech dataset with 16% embedded second language, and on a lunch conversation dataset with 20% embedded language. Our language models with lattice parsing and partial parsing reduce word error rates from a baseline mixed language model by 3.8% and 3.9% in terms of word error rate relatively on the average on the first and second tasks respectively. It outperforms the interpolated language model by 3.7% and 5.6% in terms of word error rate relatively, and outperforms the adapted language model by 2.6% and 4.6% relatively. Our proposed approach avoids making early decisions on codeswitch boundaries and is therefore more robust. We address the code switch data scarcity challenge by using bilingual data with syntactic structure.</abstract>
</variant>
</algorithm>
<algorithm name="ParsCit" version="110505">
<citationList>
<citation valid="false">
<authors>
<author>J J Gumperz</author>
</authors>
<title>Discourse strategies”,</title>
<volume>1</volume>
<pages>1982</pages>
<publisher>Cambridge University Press,</publisher>
<marker>Gumperz, </marker>
<rawString>J.J. Gumperz, “Discourse strategies”, Cambridge University Press, 1, 1982.</rawString>
</citation>
<citation valid="true">
<authors>
<author>F Coulmas</author>
</authors>
<title>The handbook of sociolinguistics”,</title>
<date>1998</date>
<location>Wiley-Blackwell,</location>
<marker>Coulmas, 1998</marker>
<rawString>Coulmas, F., “The handbook of sociolinguistics”, Wiley-Blackwell, 1998.</rawString>
</citation>
<citation valid="true">
<authors>
<author>N T Vu</author>
<author>D C Lyu</author>
<author>J Weiner</author>
<author>D Telaar</author>
<author>T Schlippe</author>
<author>F Blaicher</author>
<author>E S Chng</author>
<author>T Schultz</author>
<author>H Li</author>
</authors>
<title>A first speech recognition system for Mandarin-English code-switch conversational speech’, ICASSP,</title>
<date>2012</date>
<contexts>
<context position="5235" citStr="Vu et al 2012" startWordPosition="816" endWordPosition="819"> al, 2010; Bhuvanagiri and Kopparapu, 2010; Cao et. al, 2010). The effectiveness of adaptation is also limited as positions of code switching points are not generalizable from the limited data. Significant progress in speech recognition has been made by using deep neural networks for acoustic modeling and language model. However, improvement thus gained on code switch speech recognition remains very small. Again, we propose that syntactic constraints of the code switching phenomenon can help improve performance and model accuracy. Previous work of using partof-speech tags (Zhang et. al, 2008; Vu et al 2012) and our previous work using syntactic constraints (Li and Fung, 2012, 2013) have made progress in this area. Part-of-speech is relatively weak in predicting code switching points. It is generally accepted by linguists that code switching follows the so-called Functional Head Constraint, where words on the nodes of a syntactic sub tree must follow the language of that of the headword. If the headword is in the matrix language then none of its complements can switch to the embedded language. In this work, we propose two ways to incorporate the Functional Head Constraint into speech recognition </context>
</contexts>
<marker>Vu, Lyu, Weiner, Telaar, Schlippe, Blaicher, Chng, Schultz, Li, 2012</marker>
<rawString>Vu, N.T. and Lyu, D.C. and Weiner, J. and Telaar, D. and Schlippe, T. and Blaicher, F. and Chng, E.S. and Schultz, T. and Li, H. A first speech recognition system for Mandarin-English code-switch conversational speech’, ICASSP, 2012</rawString>
</citation>
<citation valid="true">
<authors>
<author>J Y C Chan</author>
<author>PC Ching</author>
<author>T Lee</author>
<author>H M Meng</author>
</authors>
<title>Detection of language boundary in code-switching utterances by bi-phone probabilities” Chinese Spoken Language Processing,</title>
<date>2004</date>
<booktitle>International Symposium on,</booktitle>
<pages>293--296</pages>
<marker>Chan, Ching, Lee, Meng, 2004</marker>
<rawString>J.Y.C. Chan and PC Ching and T. Lee and H.M. Meng “Detection of language boundary in code-switching utterances by bi-phone probabilities” Chinese Spoken Language Processing, 2004 International Symposium on, 293–296.</rawString>
</citation>
<citation valid="true">
<authors>
<author>C J Shia</author>
<author>Y H Chiu</author>
<author>J H Hsieh</author>
<author>C H Wu</author>
</authors>
<title>Language boundary detection and identification of mixed-language speech based on MAP estimation””,</title>
<date>2004</date>
<location>ICASSP</location>
<marker>Shia, Chiu, Hsieh, Wu, 2004</marker>
<rawString>C.J. Shia and Y.H. Chiu and J.H. Hsieh and C.H. Wu “Language boundary detection and identification of mixed-language speech based on MAP estimation””, ICASSP 2004.</rawString>
</citation>
<citation valid="true">
<authors>
<author>D C Lyu</author>
<author>R Y Lyu</author>
</authors>
<title>Language identification on code-switching utterances using multiple cues”</title>
<date>2008</date>
<booktitle>Ninth Annual Conference of the International Speech Communication Association,</booktitle>
<contexts>
<context position="3853" citStr="Lyu and Lyu, 2008" startWordPosition="588" endWordPosition="591">models is hard to come by. Code switch language modeling is therefore an even harder problem than acoustic modeling. One approach for code switch speech recognition is to explicitly recognizing the code switch points by language identification first using phonetic or acoustic information, before applying speech recognizers for the matrix and embedded languages (Chan et. al, 2004; Shia et. al, 907 Proceedings of the 2014 Conference on Empirical Methods in Natural Language Processing (EMNLP), pages 907–916, October 25-29, 2014, Doha, Qatar. c�2014 Association for Computational Linguistics 2004; Lyu and Lyu, 2008). This approach is extremely error-prone as language identification at each frame of the speech is necessary and any error will be propagated in the second speech recognition stage leading to fatal and irrecoverable errors. Meanwhile, there are two general approaches to solve the problem of lack of training data for language modeling. In a first approach, two language models are trained from both the matrix and embedded language separately and then interpolated together (Vu et. al, 2012; Chan et. al, 2006). However, an interpolated language model effectively allows code switch at all word boun</context>
</contexts>
<marker>Lyu, Lyu, 2008</marker>
<rawString>D.C. Lyu and R.Y. Lyu “Language identification on code-switching utterances using multiple cues” Ninth Annual Conference of the International Speech Communication Association, 2008.</rawString>
</citation>
<citation valid="false">
<authors>
<author>T L Tsai</author>
<author>C Y Chiang</author>
<author>H M Yu</author>
<author>L S Lo</author>
<author>Y R Wang</author>
<author>S H Chen</author>
</authors>
<booktitle>A study on Hakka and mixed Hakka-Mandarin speech recognition” Chinese Spoken Language Processing (ISCSLP), 2010 7th International Symposium on, 199–</booktitle>
<pages>204</pages>
<marker>Tsai, Chiang, Yu, Lo, Wang, Chen, </marker>
<rawString>Tsai, T.L. and Chiang, C.Y. and Yu, H.M. and Lo, L.S. and Wang, Y.R. and Chen, S.H. “A study on Hakka and mixed Hakka-Mandarin speech recognition” Chinese Spoken Language Processing (ISCSLP), 2010 7th International Symposium on, 199– 204</rawString>
</citation>
<citation valid="false">
<authors>
<author>C F Yeh</author>
<author>C Y Huang</author>
<author>L C Sun</author>
<author>L S Lee</author>
</authors>
<title>An integrated framework for transcribing Mandarin-English code-mixed lectures with improved acoustic and language modeling”</title>
<booktitle>Chinese Spoken Language Processing (ISCSLP), 2010 7th International Symposium on,</booktitle>
<pages>214--219</pages>
<marker>Yeh, Huang, Sun, Lee, </marker>
<rawString>Yeh, C.F. and Huang, C.Y. and Sun, L.C. and Lee, L.S. “An integrated framework for transcribing Mandarin-English code-mixed lectures with improved acoustic and language modeling” Chinese Spoken Language Processing (ISCSLP), 2010 7th International Symposium on, 214–219</rawString>
</citation>
<citation valid="true">
<authors>
<author>K Bhuvanagiri</author>
<author>S Kopparapu</author>
</authors>
<title>An Approach to Mixed Language Automatic Speech Recognition”,</title>
<date>2010</date>
<location>Oriental COCOSDA, Kathmandu, Nepal,</location>
<contexts>
<context position="4663" citStr="Bhuvanagiri and Kopparapu, 2010" startWordPosition="724" endWordPosition="727">e leading to fatal and irrecoverable errors. Meanwhile, there are two general approaches to solve the problem of lack of training data for language modeling. In a first approach, two language models are trained from both the matrix and embedded language separately and then interpolated together (Vu et. al, 2012; Chan et. al, 2006). However, an interpolated language model effectively allows code switch at all word boundaries without much of a constraint. Another approach is to adapt the matrix language language model with a small amount of code switch data (Tsai et. al, 2010; Yeh et. al, 2010; Bhuvanagiri and Kopparapu, 2010; Cao et. al, 2010). The effectiveness of adaptation is also limited as positions of code switching points are not generalizable from the limited data. Significant progress in speech recognition has been made by using deep neural networks for acoustic modeling and language model. However, improvement thus gained on code switch speech recognition remains very small. Again, we propose that syntactic constraints of the code switching phenomenon can help improve performance and model accuracy. Previous work of using partof-speech tags (Zhang et. al, 2008; Vu et al 2012) and our previous work using</context>
</contexts>
<marker>Bhuvanagiri, Kopparapu, 2010</marker>
<rawString>K. Bhuvanagiri and S. Kopparapu, “An Approach to Mixed Language Automatic Speech Recognition”, Oriental COCOSDA, Kathmandu, Nepal, 2010</rawString>
</citation>
<citation valid="true">
<authors>
<author>H Cao</author>
<author>PC Ching</author>
<author>T Lee</author>
<author>Y T Yeung</author>
</authors>
<title>Semantics-based language modeling for Cantonese-English code-mixing speech recognition Chinese Spoken Language Processing (ISCSLP),</title>
<date>2010</date>
<booktitle>7th International Symposium on,246–250</booktitle>
<marker>Cao, Ching, Lee, Yeung, 2010</marker>
<rawString>Cao, H. and Ching, PC and Lee, T. and Yeung, Y.T. “Semantics-based language modeling for Cantonese-English code-mixing speech recognition Chinese Spoken Language Processing (ISCSLP), 2010 7th International Symposium on,246–250</rawString>
</citation>
<citation valid="true">
<authors>
<author>Ciprian Chelba</author>
<author>Frederick Jelinek</author>
</authors>
<title>Structured language modeling.”</title>
<date>2000</date>
<journal>Computer Speech &amp; Language</journal>
<volume>14</volume>
<pages>283--332</pages>
<contexts>
<context position="6298" citStr="Chelba and Jelinek, 2000" startWordPosition="985" endWordPosition="988">e of its complements can switch to the embedded language. In this work, we propose two ways to incorporate the Functional Head Constraint into speech recognition and compare them. We suggest two approaches of introducing syntactic constraints into the speech recognition system. One is to apply the knowledge sources in a sequential order. The acoustic model and a monolingual language model are used first to produce an intermediate lattice, then a second pass choose the best result using the syntactic constraints. Another approach uses tight coupling. We propose using structured language model (Chelba and Jelinek, 2000) to build the syntactic structure incrementally. Following our previous work, we suggest incorporating the acoustic model, the monolingual language model and a translation model into a WFST framework. Using a translation model allows us to learn what happens when a language switches to another with context information. We will motivate and describe this WFST framework for code switching speech recognition in the next section. The Functional Head Constraint is described in Section 3. The proposed code switch language models and speech recognition coupling is described in Section 4. Experimental</context>
<context position="18912" citStr="Chelba and Jelinek, 2000" startWordPosition="3155" endWordPosition="3158">parsing. The probability of a sentence is computed by adding on the probability of each new context-free rule in the sentences. 4.2 Tight-coupling by Incremental Parsing To integrate the acoustic models, language model and the syntactic constraint in time synchronous decoding, an incremental operation is used in this approach. The final word-level probability assigned by our model is calculated using the acoustic models, the matrix language model, the structured language model and the translation model. The structured language model uses probabilistic parameterization of a shift-reduce parse (Chelba and Jelinek, 2000). The tight-coupled language model consists of three transducers, the word predictor, the tagger and the constructor. As shown in Figure 3, Wk = w0...wk is the first k words of the sentence, Tk contains only those binary subtrees whose leaves are completely included in Wk, excluding w0 =&lt;s&gt;. Single words along with their POS tag can be regarded as root-only trees. The exposed head hk is a pair of the headword of the constituent Wk and the non-terminal label. The exposed head of single words are pairs of the words and their POS tags. Given the word-and-parse (k-1)-prefix Wk−1Tk−1, the new word </context>
</contexts>
<marker>Chelba, Jelinek, 2000</marker>
<rawString>Chelba, Ciprian, and Frederick Jelinek. ”Structured language modeling.” Computer Speech &amp; Language 14, no. 4 (2000): 283-332.</rawString>
</citation>
<citation valid="true">
<authors>
<author>D Imseng</author>
<author>H Bourlard</author>
<author>M Magimai-Doss</author>
<author>J Dines</author>
</authors>
<title>Language dependent universal phoneme posterior estimation for mixed language speech recognition”, ICASSP,</title>
<date>2011</date>
<marker>Imseng, Bourlard, Magimai-Doss, Dines, 2011</marker>
<rawString>Imseng, D. and Bourlard, H. and Magimai-Doss, M. and Dines, J., “Language dependent universal phoneme posterior estimation for mixed language speech recognition”, ICASSP, 2011.</rawString>
</citation>
<citation valid="true">
<authors>
<author>Q Zhang</author>
<author>J Pan</author>
<author>Y Yan</author>
</authors>
<title>Mandarin-English bilingual speech recognition for real world music retrieval”, ICASSP,</title>
<date>2008</date>
<marker>Zhang, Pan, Yan, 2008</marker>
<rawString>Q. Zhang and J. Pan and Y. Yan, “Mandarin-English bilingual speech recognition for real world music retrieval”, ICASSP, 2008.</rawString>
</citation>
<citation valid="true">
<authors>
<author>G Bouselmi</author>
<author>D Fohr</author>
<author>I Illina</author>
</authors>
<title>Combined acoustic and pronunciation modelling for non-native speech recognition”,</title>
<date>2007</date>
<booktitle>Eighth Annual Conference of the International Speech Communication Association,</booktitle>
<marker>Bouselmi, Fohr, Illina, 2007</marker>
<rawString>Bouselmi, G. and Fohr, D. and Illina, I., “Combined acoustic and pronunciation modelling for non-native speech recognition”, Eighth Annual Conference of the International Speech Communication Association, 2007.</rawString>
</citation>
<citation valid="true">
<authors>
<author>E Woolford</author>
</authors>
<title>Bilingual code-switching and syntactic theory”,</title>
<date>1983</date>
<booktitle>in Linguistic Inquiry, 14(3):520–536, JSTOR,</booktitle>
<marker>Woolford, 1983</marker>
<rawString>Woolford, E., “Bilingual code-switching and syntactic theory”, in Linguistic Inquiry, 14(3):520–536, JSTOR, 1983.</rawString>
</citation>
<citation valid="true">
<authors>
<author>J MacSwan</author>
</authors>
<title>13 Code-switching and grammatical theory”,</title>
<date>2012</date>
<booktitle>in The Handbook of Bilingualism and Multilingualism, 323 Wiley-Blackwell,</booktitle>
<marker>MacSwan, 2012</marker>
<rawString>MacSwan, J., “13 Code-switching and grammatical theory”, in The Handbook of Bilingualism and Multilingualism, 323 Wiley-Blackwell, 2012.</rawString>
</citation>
<citation valid="true">
<authors>
<author>S Poplack</author>
<author>D Sankoff</author>
</authors>
<title>A formal grammar for code-switching”, in Papers in Linguistics:</title>
<date>1980</date>
<journal>International Journal of Human Communication,</journal>
<volume>3</volume>
<marker>Poplack, Sankoff, 1980</marker>
<rawString>Poplack, S. and Sankoff, D., “A formal grammar for code-switching”, in Papers in Linguistics: International Journal of Human Communication, 3–45, 1980.</rawString>
</citation>
<citation valid="false">
<authors>
<author>Robert C Moore</author>
<author>William Lewis</author>
</authors>
<title>Intelligent selection of language model training data”</title>
<booktitle>Proceedings of the ACL 2010 Conference Short Papers,</booktitle>
<pages>220--224</pages>
<marker>Moore, Lewis, </marker>
<rawString>Moore, Robert C and Lewis, William, “Intelligent selection of language model training data” Proceedings of the ACL 2010 Conference Short Papers, 220–224.</rawString>
</citation>
<citation valid="true">
<authors>
<author>Heidi Belazi</author>
</authors>
<title>Edward Rubin; Almeida Jacqueline Toribio “Code switching and X-Bar theory: The functional head constraint”.</title>
<date>1994</date>
<journal>Linguistic Inquiry</journal>
<volume>25</volume>
<issue>2</issue>
<pages>221--37</pages>
<marker>Belazi, 1994</marker>
<rawString>Belazi, Heidi; Edward Rubin; Almeida Jacqueline Toribio “Code switching and X-Bar theory: The functional head constraint”. Linguistic Inquiry 25 (2): 221-37, 1994.</rawString>
</citation>
<citation valid="true">
<authors>
<author>Rakesh M Bhatt</author>
</authors>
<title>Code-switching and the functional head constraint”</title>
<date>1995</date>
<booktitle>In Janet Fuller et al. Proceedings of the Eleventh Eastern States Conference on Linguistics. Ithaca, NY: Department of Modern Languages and Linguistics.</booktitle>
<pages>1--12</pages>
<marker>Bhatt, 1995</marker>
<rawString>Bhatt, Rakesh M., “Code-switching and the functional head constraint” In Janet Fuller et al. Proceedings of the Eleventh Eastern States Conference on Linguistics. Ithaca, NY: Department of Modern Languages and Linguistics. pp. 1-12, 1995</rawString>
</citation>
<citation valid="true">
<authors>
<author>Jean-Cdric Chappelier</author>
</authors>
<title>Lattice parsing for speech recognition.” TALN</title>
<date>1999</date>
<marker>Chappelier, 1999</marker>
<rawString>Chappelier, Jean-C?dric, et al., “Lattice parsing for speech recognition.” TALN 1999.</rawString>
</citation>
</citationList>
</algorithm>
</algorithms>
<?xml version="1.0" encoding="UTF-8"?>
<algorithms version="110505">
<algorithm name="SectLabel" version="110505">
<variant no="0" confidence="0.030734">
<title confidence="0.973106">
Event Role Extraction using Domain-Relevant Word Representations
</title>
<author confidence="0.553097">
Emanuela Boros¸†$ Romaric Besanc¸on† Olivier Ferret† Brigitte Grau$*
</author>
<note confidence="0.978202">
†CEA, LIST, Vision and Content Engineering Laboratory, F-91191, Gif-sur-Yvette, France
$LIMSI, rue John von Neumann, Campus Universitaire d’Orsay, F-91405 Orsay cedex
*ENSIIE, 1 square de la r´esistance F-91025 ´Evry cedex
</note>
<email confidence="0.95336">
firstname.lastname@cea.fr firstname.lastname@limsi.fr
</email>
<sectionHeader confidence="0.992401" genericHeader="abstract">
Abstract
</sectionHeader>
<bodyText confidence="0.999854454545455">
The efficiency of Information Extraction
systems is known to be heavily influenced
by domain-specific knowledge but the cost
of developing such systems is consider-
ably high. In this article, we consider the
problem of event extraction and show that
learning word representations from unla-
beled domain-specific data and using them
for representing event roles enable to out-
perform previous state-of-the-art event ex-
traction models on the MUC-4 data set.
</bodyText>
<sectionHeader confidence="0.998799" genericHeader="keywords">
1 Introduction
</sectionHeader>
<bodyText confidence="0.999985044776119">
In the Information Extraction (IE) field, event ex-
traction constitutes a challenging task. An event
is described by a set of participants (i.e. at-
tributes or roles) whose values are text excerpts.
The event extraction task is related to several sub-
tasks: event mention detection, candidate role-
filler extraction, relation extraction and event tem-
plate filling. The problem we address here is the
detection of role-filler candidates and their associ-
ation with specific roles in event templates. For
this task, IE systems adopt various ways of ex-
tracting patterns or generating rules based on the
surrounding context, local context and global con-
text (Patwardhan and Riloff, 2009). Current ap-
proaches for learning such patterns include boot-
strapping techniques (Huang and Riloff, 2012a;
Yangarber et al., 2000), weakly supervised learn-
ing algorithms (Huang and Riloff, 2011; Sudo et
al., 2003; Surdeanu et al., 2006), fully supervised
learning approaches (Chieu et al., 2003; Freitag,
1998; Bunescu and Mooney, 2004; Patwardhan
and Riloff, 2009) and other variations. All these
methods rely on substantial amounts of manually
annotated corpora and use a large body of lin-
guistic knowledge. The performance of these ap-
proaches is related to the amount of knowledge
engineering deployed and a good choice of fea-
tures and classifiers. Furthermore, the efficiency
of the system relies on the a priori knowledge of
the applicative domain (the nature of the events)
and it is generally difficult to apply a system on
a different domain with less annotated data with-
out reconsidering the design of the features used.
An important step forwards is TIERlight (Huang
and Riloff, 2012a) that targeted the minimization
of human supervision with a bootstrapping tech-
nique for event roles detection. Also, PIPER (Pat-
wardhan and Riloff, 2007; Patwardhan, 2010) dis-
tinguishes between relevant and irrelevant regions
and learns domain-relevant extraction patterns us-
ing a semantic affinity measure. Another possi-
ble approach for dealing with this problem is to
combine the use a restricted set of manually anno-
tated data with a much larger set of data extracted
in an unsupervised way from a corpus. This ap-
proach was experimented for relations in the con-
text of Open Information Extraction (Soderland et
al., 2010) but not for extracting events and their
participants to our knowledge.
In this paper, we propose to approach the task
of labeling text spans with event roles by auto-
matically learning relevant features that requires
limited prior knowledge, using a neural model to
induce semantic word representations (commonly
referred as word embeddings) in an unsupervised
fashion, as in (Bengio et al., 2006; Collobert and
Weston, 2008). We exploit these word embed-
dings as features for a supervised event role (mul-
ticlass) classifier. This type of approach has been
proved efficient for numerous tasks in natural lan-
guage processing, including named entity recog-
nition (Turian et al., 2010), semantic role label-
ing (Collobert et al., 2011), machine translation
(Schwenk and Koehn, 2008; Lambert et al., 2012),
word sense disambiguation (Bordes et al., 2012) or
sentiment analysis (Glorot et al., 2011; Socher et
al., 2011) but has never been used, to our knowl-
</bodyText>
<page confidence="0.930341">
1852
</page>
<bodyText confidence="0.975759307692308">
Proceedings of the 2014 Conference on Empirical Methods in Natural Language Processing (EMNLP), pages 1852–1857,
October 25-29, 2014, Doha, Qatar. c�2014 Association for Computational Linguistics
edge, for an event extraction task. Our goal is two-
fold: (1) to prove that using as only features word
vector representations makes the approach com-
petitive in the event extraction task; (2) to show
that these word representations are scalable and
robust when varying the size of the training data.
Focusing on the data provided in MUC-4 (Lehnert
et al., 1992), we prove the relevance of our ap-
proach by outperforming state-of-the-art methods,
in the same evaluation environment as in previous
works.
</bodyText>
<sectionHeader confidence="0.981816" genericHeader="introduction">
2 Approach
</sectionHeader>
<bodyText confidence="0.999981551724138">
In this work, we approach the event extraction task
by learning word representations from a domain-
specific data set and by using these representa-
tions to identify the event roles. This idea relies
on the assumption that the different words used
for a given event role in the text share some se-
mantic properties, related to their context of use
and that these similarities can be captured by spe-
cific representations that can be automatically in-
duced from the text, in an unsupervised way. We
then propose to rely only on these word repre-
sentations to detect the event roles whereas, in
most works (Riloff, 1996; Patwardhan and Riloff,
2007; Huang and Riloff, 2012a; Huang and Riloff,
2012b), the role fillers are represented by a set
of different features (raw words, their parts-of-
speech, syntactic or semantic roles in the sen-
tence).
Furthermore, we propose two additional contri-
butions to the construction of the word representa-
tions. The first one is to exploit limited knowledge
about the event types (seed words) to improve the
learning procedure by better selecting the dictio-
nary. The second one is to use a max operation1 on
the word vector representations in order to build
noun phrase representations (since slot fillers are
generally noun phrases), which represents a better
way of aggregating the semantic information born
by the word representations.
</bodyText>
<subsectionHeader confidence="0.8038665">
2.1 Inducing Domain-Relevant Word
Representations
</subsectionHeader>
<bodyText confidence="0.999564333333333">
In order to induce the domain-specific word rep-
resentations, we project the words into a 50-
dimensional word space. We chose a single
</bodyText>
<footnote confidence="0.977902333333333">
1This max operation consists in taking, for each compo-
nent of the vector, the max value of this component for each
word vector representation.
</footnote>
<bodyText confidence="0.999926725490196">
layer neural network (NN) architecture that avoids
strongly engineered features, assumes little prior
knowledge about the task, but is powerful enough
to capture relevant domain information. Follow-
ing (Collobert et al., 2011), we use an NN which
learns to predict whether a given text sequence
(short word window) exists naturally in the consid-
ered domain. We represent an input sequence of n
words as (wi) = (wi−(n/2) ... , wi, ... wi+(n/2)).
The main idea is that each sequence of words in
the training set should receive a higher score than
a sequence in which one word is replaced with
a random one. We call the sequence with a ran-
dom word corrupted (¯(wi)) and denote as correct
((wi)) all the sequences of words from the data
set. The goal of the training step is then to min-
imize the following loss function for a word wi
in the dictionary D: Cw.z = Ew.z∈D max(0,1 −
g((wi))+g(¯(wi))), where g(�) is the scoring func-
tion given by the neural network. Further details
and evaluations of these embeddings can be found
in (Bengio et al., 2003; Bengio et al., 2006; Col-
lobert and Weston, 2008; Turian et al., 2010). For
efficiency, words are fed to our architecture as in-
dices taken from a finite dictionary. Obviously,
a simple index does not carry much useful infor-
mation about the word. So, the first layer of our
network maps each of these word indices into a
feature vector, by a lookup table operation. Our
first contribution intervenes in the process of the
choosing the proper dictionary. (Bengio, 2009)
has shown that the order of the words in the dic-
tionary of the neural network is not indifferent to
the quality of the achieved representations: he pro-
posed to order the dictionary by frequency and se-
lect the words for the corrupted sequence accord-
ing to this order. In our case, the most frequent
words are not always the most relevant for the task
of event role detection. Since we want to have a
training more focused to the domain specific task,
we chose to order the dictionary by word relevance
to the domain. We accomplish this by considering
a limited number of seed words for each event type
that needs to be discovered in text (e.g. attack,
bombing, kidnapping, arson). We then rate with
higher values the words that are more similar to the
event types words, according to a given semantic
similarity, and we rank them accordingly. We use
the “Leacock Chodorow” similarity from Word-
net 3.0 (Leacock and Chodorow, 1998). Initial ex-
perimental results proved that using this domain-
</bodyText>
<page confidence="0.934147">
1853
</page>
<bodyText confidence="0.999244">
oriented order leads to better performance for the
task than the order by frequency.
</bodyText>
<subsectionHeader confidence="0.9968465">
2.2 Using Word Representations to Identify
Event Roles
</subsectionHeader>
<bodyText confidence="0.9999821">
After having generated for each word their vec-
tor representation, we use them as features for the
annotated data to classify event roles. However,
event role fillers are not generally single words but
noun phrases that can be, in some cases, identi-
fied as named entities. For identifying the event
roles, we therefore apply a two-step strategy. First,
we extract the noun chunks using SENNA2 parser
(Collobert et al., 2011; Collobert, 2011) and we
build a representation for these chunks defined as
the maximum, per column, of the vector represen-
tations of the words it contains. Second, we use
a statistical classifier to recognize the slot fillers,
using this representation as features. We chose
the extra-trees ensemble classifier (Geurts et al.,
2006), which is a meta estimator that fits a num-
ber of randomized decision trees (extra-trees) on
various sub-samples of the data set and use averag-
ing to improve the predictive accuracy and control
over-fitting.
</bodyText>
<sectionHeader confidence="0.997917" genericHeader="related work">
3 Experiments and Results
</sectionHeader>
<subsectionHeader confidence="0.989859">
3.1 Task Description
</subsectionHeader>
<bodyText confidence="0.999825142857143">
We conducted the experiments on the official
MUC-4 training corpus that consists of 1,700 doc-
uments and instantiated templates for each doc-
ument. The task consists in extracting informa-
tion about terrorist events in Latin America from
news articles. We classically considered the fol-
lowing 4 types of events: attack, bombing, kid-
napping and arson. These are represented by tem-
plates containing various slots for each piece of
information that should be extracted from the doc-
ument (perpetrators, human targets, physical tar-
gets, etc). Following previous works (Huang and
Riloff, 2011; Huang and Riloff, 2012a), we only
consider the “String Slots” in this work (other slots
need different treatments) and we group certain
slots to finally consider the five slot types PerpInd
(individual perpetrator), PerpOrg (organizational
perpetrator), Target (physical target), Victim (hu-
man target name or description) and Weapon (in-
strument id or type). We used 1,300 documents
(DEV) for training, 200 documents (TST1+TST2)
</bodyText>
<footnote confidence="0.7853275">
2Code and resources can be found at http://ml.
nec-labs.com/senna/
</footnote>
<bodyText confidence="0.99949485">
for tuning, and 200 documents (TST3+TST4) as
the blind test set. To compare with similar works,
we do not evaluate the template construction and
only focus on the identification of the slot fillers:
for each answer key in a reference template, we
check if we find it correctly with our extraction
method, using head noun matching (e.g., the vic-
tim her mother Martha Lopez Orozco de Lopez is
considered to match Matha Lopez), and merging
duplicate extractions (so that different extracted
slot fillers sharing the same head noun are counted
only once). We also took into account the answer
keys with multiple values in the reference, deal-
ing with conjunctions (when several victims are
named, we need to find all of them) and disjunc-
tions (when several names for the same organiza-
tion are possible, we need to find any of them).
Our results are reported as Precision/Recall/F1-
score for each event role separately and averaged
on all roles.
</bodyText>
<subsectionHeader confidence="0.967072">
3.2 Experiments
</subsectionHeader>
<bodyText confidence="0.999939">
In all the experiments involving our model, we es-
tablished the following stable choices of parame-
ters: 50-dimensional vectors obtained by training
on sequences of 5 words, which is consistent with
previous studies (Turian et al., 2010; Collobert
and Weston, 2008). All the hyper-parameters of
our model (e.g. learning rate, size of the hidden
layer, size of the word vectors) have been chosen
by finetuning our event extraction system on the
TST1+TST2 data set. For DRVR-50 and W2V-50,
the embeddings were built from the whole training
corpus (1,300 documents) and the dictionary was
made of all the words of this corpus under their
inflected form.
We used the extra-trees ensemble classifier im-
plemented in (Pedregosa et al., 2011), with hyper-
parameters optimized on the validation data: for-
est of 500 trees and the maximum number of
features to consider when looking for the best
split is √number features. We present a 3-
fold evaluation: first, we compare our system with
state-of-the-art systems on the same task, then we
compare our domain-relevant vector representa-
tions (DRVR-50) to more generic word embed-
dings (C&amp;W50, HLBL-50)3 and finally to another
</bodyText>
<footnote confidence="0.9556816">
3C&amp;W-50 are described in (Collobert and Weston,
2008), HLBL-50 are the Hierarchical log-bilinear embed-
dings (Mnih and Hinton, 2007), provided by (Turian et
al., 2010), available at http://metaoptimize.com/
projects/wordreprs induced from the Reuters-RCV1
</footnote>
<page confidence="0.963998">
1854
</page>
<table confidence="0.999741">
State-of-the-art systems
PerpInd PerpOrg Target Victim Weapon Average
(Riloff, 1996) 33/49/40 53/33/41 54/59/56 49/54/51 38/44/41 45/48/46
(Patwardhan and Riloff, 2007) 39/48/43 55/31/40 37/60/46 44/46/45 47/47/47 44/36/40
(Patwardhan and Riloff, 2009) 51/58/54 34/45/38 43/72/53 55/58/56 57/53/55 48/57/52
(Huang and Riloff, 2011) 48/57/52 46/53/50 51/73/60 56/60/58 53/64/58 51/62/56
(Huang and Riloff, 2012a) 47/51/47 60/39/47 37/65/47 39/53/45 53/55/54 47/53/50
(Huang and Riloff, 2012b) 54/57/56 55/49/51 55/68/61 63/59/61 62/64/63 58/60/59
Models based on word embeddings
C&amp;W-50 80/55/65 64/65/64 76/72/74 53/63/57 85/64/73 68/63/65
HLBL-50 81/53/64 63/67/65 78/72/75 53/63/58 93/64/75 69/62/66
W2V-50 79/57/66 88/71/79 74/72/73 69/75/71 97/65/78 77/68/72
DRVR-50 79/57/66 91/74/81 79/57/66 77/75/76 92/58/81 80/67/73
</table>
<tableCaption confidence="0.99906">
Table 1: Accuracy of “String Slots” on the TST3 + TST4 test set P/R/F1 (Precision/Recall/F1-Score)
</tableCaption>
<figureCaption confidence="0.944557">
word representation construction on the domain-
specific data (W2V-50)4.
Figure 1: F1-score results for event role labeling
</figureCaption>
<bodyText confidence="0.7802475">
on MUC-4 data, for different size of training data,
of “String Slots” on the TST3+TST4 with differ-
ent parameters, compared to the learning curve of
TIER (Huang and Riloff, 2012a). The grey points
represent the performances of other IE systems.
Figure 1 presents the average F1-score results,
computed over the slots PerpInd, PerpOrg, Tar-
get, Victim and Weapon. We observe that mod-
els relying on word embeddings globally outper-
form the state-of-the-art results, which demon-
strates that the word embeddings capture enough
semantic information to perform the task of event
</bodyText>
<footnote confidence="0.894083666666667">
newswire corpus
4W2V-50 are the embeddings induced from the MUC4
data set using the negative sampling training algorithm
(Mikolov et al., 2013a; Mikolov et al., 2013b; Mikolov et
al., 2013c), available at https://code.google.com/
p/word2vec/
</footnote>
<bodyText confidence="0.999856941176471">
role labeling on “String Slots” without using any
additional hand-engineered features. Moreover,
our representations (DRVR-50) clearly surpass the
models based on generic embeddings (C&amp;W-50
and HLBL-50) and obtain better results than W2V-
50, based the competitive model of (Mikolov et
al., 2013a), even if the difference is small. We
can also note that the performance of our model
is good even with a small amount of training data,
which makes it a good candidate to easily develop
an event extraction system on a new domain.
Table 1 provides a more detailed analysis of the
comparative results. We can see in this table that
our results surpass those of previous systems (0.73
vs. 0.59) with, particularly, a consistently higher
precision on all roles, whereas recall is smaller for
certain roles (Target and Weapon). To further ex-
plore the impact of these representations, we com-
pared our word embeddings with other word em-
beddings (C&amp;W-50, HLBL-50) and report the re-
sults in Figure 1 and Table 1. The results show
that our model also outperforms the models using
others word embeddings (F1-score of 0.73 against
0.65, 0.66). This proves that a model learned
on a domain-specific data set does indeed pro-
vide better results, even if its size is much smaller
(whereas it is usually considered that neural mod-
els require often important training data). Finally,
we also achieve slightly better results than W2V-50
with other word representations built on the same
corpus, which shows that the choices made for the
word representation construction, such as the use
of domain information for word ordering, tend to
have a positive impact.
</bodyText>
<page confidence="0.991911">
1855
</page>
<sectionHeader confidence="0.970766" genericHeader="conclusions">
4 Conclusions and Perspectives
</sectionHeader>
<bodyText confidence="0.999975285714286">
We presented in this paper a new approach for
event extraction by reducing the features to only
use unsupervised word representations and a small
set of seed words. The word embeddings induced
from a domain-specific corpus bring improvement
over state-of-art models on the standard MUC-
4 corpus and demonstrate a good scalability on
different sizes of training data sets. Therefore,
our proposal offers a promising path towards eas-
ier and faster domain adaptation. We also prove
that using a domain-specific corpus leads to bet-
ter word vector representations for this task than
using other publicly-available word embeddings
(even if they are induced from a larger corpus).
As future work, we will reconsider the archi-
tecture of the neural network and we will refo-
cus on creating a deep learning model while tak-
ing advantage of a larger set of types of infor-
mation such as syntactic information, following
(Levy and Goldberg, 2014), or semantic informa-
tion, following (Yu and Dredze, 2014).
</bodyText>
<sectionHeader confidence="0.98837" genericHeader="references">
References
</sectionHeader>
<reference confidence="0.998698784810126">
Yoshua Bengio, Rejean Ducharme, and Pascal Vincent.
2003. A neural probabilistic language model. Jour-
nal of Machine Learning Research, 3:1137–1155.
Yoshua Bengio, Holger Schwenk, Jean-S´ebastian
Sen´ecal, Fr´ederic Morin, and Jean-Luc Gauvain.
2006. Neural probabilistic language models. In
DawnE. Holmes and LakhmiC. Jain, editors, Inno-
vations in Machine Learning, volume 194 of Studies
in Fuzziness and Soft Computing, pages 138–186.
Springer Berlin Heidelberg.
Yoshua Bengio. 2009. Learning deep architectures for
AI. Foundations and trends in Machine Learning,
2(1).
Antoine Bordes, Xavier Glorot, Jason Weston, and
Yoshua Bengio. 2012. Joint learning of words
and meaning representations for open-text seman-
tic parsing. In Fifteenth International Conference
on Artificial Intelligence and Statistics (AISTATS
2012), pages 127–135.
Razvan Bunescu and Raymond J Mooney. 2004.
Collective information extraction with relational
markov networks. In 42nd Annual Meeting on As-
sociation for Computational Linguistics (ACL-04),
pages 438–445.
Hai Leong Chieu, Hwee Tou Ng, and Yoong Keok Lee.
2003. Closing the gap: Learning-based information
extraction rivaling knowledge-engineering methods.
In 41st international Annual Meeting on Association
for Computational Linguistics (ACL-2003), pages
216–223.
Ronan Collobert and Jason Weston. 2008. A unified
architecture for natural language processing: Deep
neural networks with multitask learning. In 25th In-
ternational Conference of Machine learning (ICML-
08), pages 160–167. ACM.
Ronan Collobert, Jason Weston, L´eon Battou, Michael
Karlen, Koray Kavukcuoglu, and Pavel Kuksa.
2011. Natural language processing (almost) from
scratch. Journal of Machine Learning Research,
12:2493–2537.
Ronan Collobert. 2011. Deep learning for efficient
discriminative parsing. In 14th International Con-
ference on Artificial Intelligence and Statistics (AIS-
TATS 2011).
Dayne Freitag. 1998. Information extraction from
HTML: Application of a general machine learning
approach. In AAAI’98, pages 517–523.
Pierre Geurts, Damien Ernst, and Louis Wehenkel.
2006. Extremely randomized trees. Machine Learn-
ing, 63(1):3–42.
Xavier Glorot, Antoine Bordes, and Yoshua Bengio.
2011. Domain adaptation for large-scale senti-
ment classification: A deep learning approach. In
28th International Conference on Machine Learning
(ICML-11), pages 513–520.
Ruihong Huang and Ellen Riloff. 2011. Peeling back
the layers: Detecting event role fillers in secondary
contexts. In ACL 2011, pages 1137–1147.
Ruihong Huang and Ellen Riloff. 2012a. Bootstrapped
training of event extraction classifiers. In 13th Con-
ference of the European Chapter of the Association
for Computational Linguistics (EACL 2012), pages
286–295.
Ruihong Huang and Ellen Riloff. 2012b. Modeling
textual cohesion for event extraction. In 26th Con-
ference on Artificial Intelligence (AAAI 2012).
Patrik Lambert, Holger Schwenk, and Fr´ed´eric Blain.
2012. Automatic translation of scientific documents
in the hal archive. In LREC 2012, pages 3933–3936.
Claudia Leacock and Martin Chodorow. 1998. Com-
bining local context and Wordnet similarity for word
sense identification. In Christiane Fellbaum, edi-
tor, WordNet: An electronic lexical database., pages
265–283. MIT Press.
Wendy Lehnert, Claire Cardie, David Fisher, John Mc-
Carthy, Ellen Riloff, and Stephen Soderland. 1992.
University of Massachusetts: MUC-4 test results
and analysis. In 4th Conference on Message under-
standing, pages 151–158.
</reference>
<page confidence="0.931717">
1856
</page>
<reference confidence="0.998254130952381">
Omer Levy and Yoav Goldberg. 2014. Dependency-
based word embeddings. In 52nd Annual Meet-
ing of the Association for Computational Linguis-
tics (ACL 2014), Short Papers, pages 302–308, Bal-
timore, Maryland, June.
Tomas Mikolov, Kai Chen, Greg Corrado, and Jeffrey
Dean. 2013a. Efficient estimation of word represen-
tations in vector space. In International Conference
on Learning Representations (ICLR 20013), work-
shop track.
Tomas Mikolov, Ilya Sutskever, Kai Chen, Greg S Cor-
rado, and Jeff Dean. 2013b. Distributed representa-
tions of words and phrases and their compositional-
ity. In Advances in Neural Information Processing
Systems 26 (NIPS 2013), pages 3111–3119.
Tomas Mikolov, Wen-tau Yih, and Geoffrey Zweig.
2013c. Linguistic regularities in continuous space
word representations. In NAACL-HLT 2013, pages
746–751.
Andriy Mnih and Geoffrey Hinton. 2007. Three
new graphical models for statistical modelling. In
24th International Conference of Machine learning
(ICML 2007), pages 641–648. ACM.
Siddharth Patwardhan and Ellen Riloff. 2007. Ef-
fective information extraction with semantic affinity
patterns and relevant regions. In 2007 Joint Con-
ference on Empirical Methods in Natural Language
Processing and Computational Natural Language
Learning (EMNLP-CoNLL 2007), pages 717–727.
Siddharth Patwardhan and Ellen Riloff. 2009. A uni-
fied model of phrasal and sentential evidence for in-
formation extraction. In 2009 Conference on Em-
pirical Methods in Natural Language Processing
(EMNLP 2009), pages 151–160.
Siddharth Patwardhan. 2010. Widening the field of
view of information extraction through sentential
event recognition. Ph.D. thesis, University of Utah.
F. Pedregosa, G. Varoquaux, A. Gramfort, V. Michel,
B. Thirion, O. Grisel, M. Blondel, P. Pretten-
hofer, R. Weiss, V. Dubourg, J. Vanderplas, A. Pas-
sos, D. Cournapeau, M. Brucher, M. Perrot, and
E. Duchesnay. 2011. Scikit-learn: Machine learn-
ing in Python. Journal of Machine Learning Re-
search, 12:2825–2830.
Ellen Riloff. 1996. Automatically generating extrac-
tion patterns from untagged text. In AAAI’96, pages
1044–1049.
Holger Schwenk and Philipp Koehn. 2008. Large
and diverse language models for statistical machine
translation. In IJCNLP 2008, pages 661–666.
Richard Socher, Cliff C Lin, Chris Manning, and An-
drew Y Ng. 2011. Parsing natural scenes and nat-
ural language with recursive neural networks. In
28th International Conference on Machine Learning
(ICML-11), pages 129–136.
Stephen Soderland, Brendan Roof, Bo Qin, Shi Xu,
Mausam, and Oren Etzioni. 2010. Adapting open
information extraction to domain-specific relations.
AI Magazine, 31(3):93–102.
Kiyoshi Sudo, Satoshi Sekine, and Ralph Grishman.
2003. An improved extraction pattern representa-
tion model for automatic ie pattern acquisition. In
41st Annual Meeting on Association for Computa-
tional Linguistics (ACL-03), pages 224–231.
Mihai Surdeanu, Jordi Turmo, and Alicia Ageno.
2006. A hybrid approach for the acquisition of
information extraction patterns. In EACL-2006
Workshop on Adaptive Text Extraction and Mining
(ATEM 2006), pages 48–55.
Joseph Turian, Lev Ratinov, and Yoshua Bengio. 2010.
Word representations: a simple and general method
for semi-supervised learning. In 48th international
Annual Meeting on Association for Computational
Linguistics (ACL 2010), pages 384–394.
Roman Yangarber, Ralph Grishman, Pasi Tapanainen,
and Silja Huttunen. 2000. Automatic acquisition
of domain knowledge for information extraction. In
18th Internation Conference on Computational Lin-
guistics (COLING 2000), pages 940–946.
Mo Yu and Mark Dredze. 2014. Improving lexical em-
beddings with semantic knowledge. In 52nd Annual
Meeting of the Association for Computational Lin-
guistics (ACL 2014), Short Papers, pages 545–550,
Baltimore, Maryland, June.
</reference>
<page confidence="0.994212">
1857
</page>
</variant>
</algorithm>
<algorithm name="ParsHed" version="110505">
<variant no="0" confidence="0.142294">
<title confidence="0.836695">Event Role Extraction using Domain-Relevant Word Representations</title>
<note confidence="0.50931925">LIST, Vision and Content Engineering Laboratory, F-91191, Gif-sur-Yvette, rue John von Neumann, Campus Universitaire d’Orsay, F-91405 Orsay 1 square de la r´esistance F-91025 ´Evry firstname.lastname@cea.fr firstname.lastname@limsi.fr</note>
<abstract confidence="0.995206">The efficiency of Information Extraction systems is known to be heavily influenced by domain-specific knowledge but the cost of developing such systems is considerably high. In this article, we consider the problem of event extraction and show that learning word representations from unlabeled domain-specific data and using them for representing event roles enable to outperform previous state-of-the-art event extraction models on the MUC-4 data set.</abstract>
</variant>
</algorithm>
<algorithm name="ParsCit" version="110505">
<citationList>
<citation valid="true">
<authors>
<author>Yoshua Bengio</author>
<author>Rejean Ducharme</author>
<author>Pascal Vincent</author>
</authors>
<title>A neural probabilistic language model.</title>
<date>2003</date>
<journal>Journal of Machine Learning Research,</journal>
<pages>3--1137</pages>
<contexts>
<context position="7585" citStr="Bengio et al., 2003" startWordPosition="1202" endWordPosition="1205">i, ... wi+(n/2)). The main idea is that each sequence of words in the training set should receive a higher score than a sequence in which one word is replaced with a random one. We call the sequence with a random word corrupted (¯(wi)) and denote as correct ((wi)) all the sequences of words from the data set. The goal of the training step is then to minimize the following loss function for a word wi in the dictionary D: Cw.z = Ew.z∈D max(0,1 − g((wi))+g(¯(wi))), where g(�) is the scoring function given by the neural network. Further details and evaluations of these embeddings can be found in (Bengio et al., 2003; Bengio et al., 2006; Collobert and Weston, 2008; Turian et al., 2010). For efficiency, words are fed to our architecture as indices taken from a finite dictionary. Obviously, a simple index does not carry much useful information about the word. So, the first layer of our network maps each of these word indices into a feature vector, by a lookup table operation. Our first contribution intervenes in the process of the choosing the proper dictionary. (Bengio, 2009) has shown that the order of the words in the dictionary of the neural network is not indifferent to the quality of the achieved rep</context>
</contexts>
<marker>Bengio, Ducharme, Vincent, 2003</marker>
<rawString>Yoshua Bengio, Rejean Ducharme, and Pascal Vincent. 2003. A neural probabilistic language model. Journal of Machine Learning Research, 3:1137–1155.</rawString>
</citation>
<citation valid="true">
<authors>
<author>Yoshua Bengio</author>
<author>Holger Schwenk</author>
<author>Jean-S´ebastian Sen´ecal</author>
<author>Fr´ederic Morin</author>
<author>Jean-Luc Gauvain</author>
</authors>
<title>Neural probabilistic language models.</title>
<date>2006</date>
<booktitle>Innovations in Machine Learning, volume 194 of Studies in Fuzziness and Soft Computing,</booktitle>
<pages>138--186</pages>
<editor>In DawnE. Holmes and LakhmiC. Jain, editors,</editor>
<publisher>Springer</publisher>
<location>Berlin Heidelberg.</location>
<marker>Bengio, Schwenk, Sen´ecal, Morin, Gauvain, 2006</marker>
<rawString>Yoshua Bengio, Holger Schwenk, Jean-S´ebastian Sen´ecal, Fr´ederic Morin, and Jean-Luc Gauvain. 2006. Neural probabilistic language models. In DawnE. Holmes and LakhmiC. Jain, editors, Innovations in Machine Learning, volume 194 of Studies in Fuzziness and Soft Computing, pages 138–186. Springer Berlin Heidelberg.</rawString>
</citation>
<citation valid="true">
<authors>
<author>Yoshua Bengio</author>
</authors>
<title>Learning deep architectures for AI. Foundations and trends</title>
<date>2009</date>
<booktitle>in Machine Learning,</booktitle>
<volume>2</volume>
<issue>1</issue>
<contexts>
<context position="8053" citStr="Bengio, 2009" startWordPosition="1284" endWordPosition="1285">(�) is the scoring function given by the neural network. Further details and evaluations of these embeddings can be found in (Bengio et al., 2003; Bengio et al., 2006; Collobert and Weston, 2008; Turian et al., 2010). For efficiency, words are fed to our architecture as indices taken from a finite dictionary. Obviously, a simple index does not carry much useful information about the word. So, the first layer of our network maps each of these word indices into a feature vector, by a lookup table operation. Our first contribution intervenes in the process of the choosing the proper dictionary. (Bengio, 2009) has shown that the order of the words in the dictionary of the neural network is not indifferent to the quality of the achieved representations: he proposed to order the dictionary by frequency and select the words for the corrupted sequence according to this order. In our case, the most frequent words are not always the most relevant for the task of event role detection. Since we want to have a training more focused to the domain specific task, we chose to order the dictionary by word relevance to the domain. We accomplish this by considering a limited number of seed words for each event typ</context>
</contexts>
<marker>Bengio, 2009</marker>
<rawString>Yoshua Bengio. 2009. Learning deep architectures for AI. Foundations and trends in Machine Learning, 2(1).</rawString>
</citation>
<citation valid="true">
<authors>
<author>Antoine Bordes</author>
<author>Xavier Glorot</author>
<author>Jason Weston</author>
<author>Yoshua Bengio</author>
</authors>
<title>Joint learning of words and meaning representations for open-text semantic parsing.</title>
<date>2012</date>
<booktitle>In Fifteenth International Conference on Artificial Intelligence and Statistics (AISTATS</booktitle>
<pages>127--135</pages>
<contexts>
<context position="4031" citStr="Bordes et al., 2012" startWordPosition="611" endWordPosition="614">ior knowledge, using a neural model to induce semantic word representations (commonly referred as word embeddings) in an unsupervised fashion, as in (Bengio et al., 2006; Collobert and Weston, 2008). We exploit these word embeddings as features for a supervised event role (multiclass) classifier. This type of approach has been proved efficient for numerous tasks in natural language processing, including named entity recognition (Turian et al., 2010), semantic role labeling (Collobert et al., 2011), machine translation (Schwenk and Koehn, 2008; Lambert et al., 2012), word sense disambiguation (Bordes et al., 2012) or sentiment analysis (Glorot et al., 2011; Socher et al., 2011) but has never been used, to our knowl1852 Proceedings of the 2014 Conference on Empirical Methods in Natural Language Processing (EMNLP), pages 1852–1857, October 25-29, 2014, Doha, Qatar. c�2014 Association for Computational Linguistics edge, for an event extraction task. Our goal is twofold: (1) to prove that using as only features word vector representations makes the approach competitive in the event extraction task; (2) to show that these word representations are scalable and robust when varying the size of the training dat</context>
</contexts>
<marker>Bordes, Glorot, Weston, Bengio, 2012</marker>
<rawString>Antoine Bordes, Xavier Glorot, Jason Weston, and Yoshua Bengio. 2012. Joint learning of words and meaning representations for open-text semantic parsing. In Fifteenth International Conference on Artificial Intelligence and Statistics (AISTATS 2012), pages 127–135.</rawString>
</citation>
<citation valid="true">
<authors>
<author>Razvan Bunescu</author>
<author>Raymond J Mooney</author>
</authors>
<title>Collective information extraction with relational markov networks.</title>
<date>2004</date>
<booktitle>In 42nd Annual Meeting on Association for Computational Linguistics (ACL-04),</booktitle>
<pages>438--445</pages>
<contexts>
<context position="1901" citStr="Bunescu and Mooney, 2004" startWordPosition="272" endWordPosition="275">is the detection of role-filler candidates and their association with specific roles in event templates. For this task, IE systems adopt various ways of extracting patterns or generating rules based on the surrounding context, local context and global context (Patwardhan and Riloff, 2009). Current approaches for learning such patterns include bootstrapping techniques (Huang and Riloff, 2012a; Yangarber et al., 2000), weakly supervised learning algorithms (Huang and Riloff, 2011; Sudo et al., 2003; Surdeanu et al., 2006), fully supervised learning approaches (Chieu et al., 2003; Freitag, 1998; Bunescu and Mooney, 2004; Patwardhan and Riloff, 2009) and other variations. All these methods rely on substantial amounts of manually annotated corpora and use a large body of linguistic knowledge. The performance of these approaches is related to the amount of knowledge engineering deployed and a good choice of features and classifiers. Furthermore, the efficiency of the system relies on the a priori knowledge of the applicative domain (the nature of the events) and it is generally difficult to apply a system on a different domain with less annotated data without reconsidering the design of the features used. An im</context>
</contexts>
<marker>Bunescu, Mooney, 2004</marker>
<rawString>Razvan Bunescu and Raymond J Mooney. 2004. Collective information extraction with relational markov networks. In 42nd Annual Meeting on Association for Computational Linguistics (ACL-04), pages 438–445.</rawString>
</citation>
<citation valid="true">
<authors>
<author>Hai Leong Chieu</author>
<author>Hwee Tou Ng</author>
<author>Yoong Keok Lee</author>
</authors>
<title>Closing the gap: Learning-based information extraction rivaling knowledge-engineering methods.</title>
<date>2003</date>
<booktitle>In 41st international Annual Meeting on Association for Computational Linguistics (ACL-2003),</booktitle>
<pages>216--223</pages>
<contexts>
<context position="1860" citStr="Chieu et al., 2003" startWordPosition="266" endWordPosition="269">lling. The problem we address here is the detection of role-filler candidates and their association with specific roles in event templates. For this task, IE systems adopt various ways of extracting patterns or generating rules based on the surrounding context, local context and global context (Patwardhan and Riloff, 2009). Current approaches for learning such patterns include bootstrapping techniques (Huang and Riloff, 2012a; Yangarber et al., 2000), weakly supervised learning algorithms (Huang and Riloff, 2011; Sudo et al., 2003; Surdeanu et al., 2006), fully supervised learning approaches (Chieu et al., 2003; Freitag, 1998; Bunescu and Mooney, 2004; Patwardhan and Riloff, 2009) and other variations. All these methods rely on substantial amounts of manually annotated corpora and use a large body of linguistic knowledge. The performance of these approaches is related to the amount of knowledge engineering deployed and a good choice of features and classifiers. Furthermore, the efficiency of the system relies on the a priori knowledge of the applicative domain (the nature of the events) and it is generally difficult to apply a system on a different domain with less annotated data without reconsideri</context>
</contexts>
<marker>Chieu, Ng, Lee, 2003</marker>
<rawString>Hai Leong Chieu, Hwee Tou Ng, and Yoong Keok Lee. 2003. Closing the gap: Learning-based information extraction rivaling knowledge-engineering methods. In 41st international Annual Meeting on Association for Computational Linguistics (ACL-2003), pages 216–223.</rawString>
</citation>
<citation valid="true">
<authors>
<author>Ronan Collobert</author>
<author>Jason Weston</author>
</authors>
<title>A unified architecture for natural language processing: Deep neural networks with multitask learning.</title>
<date>2008</date>
<booktitle>In 25th International Conference of Machine learning (ICML08),</booktitle>
<pages>160--167</pages>
<publisher>ACM.</publisher>
<contexts>
<context position="3609" citStr="Collobert and Weston, 2008" startWordPosition="545" endWordPosition="548">ed data with a much larger set of data extracted in an unsupervised way from a corpus. This approach was experimented for relations in the context of Open Information Extraction (Soderland et al., 2010) but not for extracting events and their participants to our knowledge. In this paper, we propose to approach the task of labeling text spans with event roles by automatically learning relevant features that requires limited prior knowledge, using a neural model to induce semantic word representations (commonly referred as word embeddings) in an unsupervised fashion, as in (Bengio et al., 2006; Collobert and Weston, 2008). We exploit these word embeddings as features for a supervised event role (multiclass) classifier. This type of approach has been proved efficient for numerous tasks in natural language processing, including named entity recognition (Turian et al., 2010), semantic role labeling (Collobert et al., 2011), machine translation (Schwenk and Koehn, 2008; Lambert et al., 2012), word sense disambiguation (Bordes et al., 2012) or sentiment analysis (Glorot et al., 2011; Socher et al., 2011) but has never been used, to our knowl1852 Proceedings of the 2014 Conference on Empirical Methods in Natural Lan</context>
<context position="7634" citStr="Collobert and Weston, 2008" startWordPosition="1210" endWordPosition="1214">ch sequence of words in the training set should receive a higher score than a sequence in which one word is replaced with a random one. We call the sequence with a random word corrupted (¯(wi)) and denote as correct ((wi)) all the sequences of words from the data set. The goal of the training step is then to minimize the following loss function for a word wi in the dictionary D: Cw.z = Ew.z∈D max(0,1 − g((wi))+g(¯(wi))), where g(�) is the scoring function given by the neural network. Further details and evaluations of these embeddings can be found in (Bengio et al., 2003; Bengio et al., 2006; Collobert and Weston, 2008; Turian et al., 2010). For efficiency, words are fed to our architecture as indices taken from a finite dictionary. Obviously, a simple index does not carry much useful information about the word. So, the first layer of our network maps each of these word indices into a feature vector, by a lookup table operation. Our first contribution intervenes in the process of the choosing the proper dictionary. (Bengio, 2009) has shown that the order of the words in the dictionary of the neural network is not indifferent to the quality of the achieved representations: he proposed to order the dictionary</context>
<context position="12498" citStr="Collobert and Weston, 2008" startWordPosition="2006" endWordPosition="2009"> with multiple values in the reference, dealing with conjunctions (when several victims are named, we need to find all of them) and disjunctions (when several names for the same organization are possible, we need to find any of them). Our results are reported as Precision/Recall/F1- score for each event role separately and averaged on all roles. 3.2 Experiments In all the experiments involving our model, we established the following stable choices of parameters: 50-dimensional vectors obtained by training on sequences of 5 words, which is consistent with previous studies (Turian et al., 2010; Collobert and Weston, 2008). All the hyper-parameters of our model (e.g. learning rate, size of the hidden layer, size of the word vectors) have been chosen by finetuning our event extraction system on the TST1+TST2 data set. For DRVR-50 and W2V-50, the embeddings were built from the whole training corpus (1,300 documents) and the dictionary was made of all the words of this corpus under their inflected form. We used the extra-trees ensemble classifier implemented in (Pedregosa et al., 2011), with hyperparameters optimized on the validation data: forest of 500 trees and the maximum number of features to consider when lo</context>
</contexts>
<marker>Collobert, Weston, 2008</marker>
<rawString>Ronan Collobert and Jason Weston. 2008. A unified architecture for natural language processing: Deep neural networks with multitask learning. In 25th International Conference of Machine learning (ICML08), pages 160–167. ACM.</rawString>
</citation>
<citation valid="true">
<authors>
<author>Ronan Collobert</author>
<author>Jason Weston</author>
<author>L´eon Battou</author>
<author>Michael Karlen</author>
<author>Koray Kavukcuoglu</author>
<author>Pavel Kuksa</author>
</authors>
<title>Natural language processing (almost) from scratch.</title>
<date>2011</date>
<journal>Journal of Machine Learning Research,</journal>
<pages>12--2493</pages>
<contexts>
<context position="3913" citStr="Collobert et al., 2011" startWordPosition="594" endWordPosition="597">ach the task of labeling text spans with event roles by automatically learning relevant features that requires limited prior knowledge, using a neural model to induce semantic word representations (commonly referred as word embeddings) in an unsupervised fashion, as in (Bengio et al., 2006; Collobert and Weston, 2008). We exploit these word embeddings as features for a supervised event role (multiclass) classifier. This type of approach has been proved efficient for numerous tasks in natural language processing, including named entity recognition (Turian et al., 2010), semantic role labeling (Collobert et al., 2011), machine translation (Schwenk and Koehn, 2008; Lambert et al., 2012), word sense disambiguation (Bordes et al., 2012) or sentiment analysis (Glorot et al., 2011; Socher et al., 2011) but has never been used, to our knowl1852 Proceedings of the 2014 Conference on Empirical Methods in Natural Language Processing (EMNLP), pages 1852–1857, October 25-29, 2014, Doha, Qatar. c�2014 Association for Computational Linguistics edge, for an event extraction task. Our goal is twofold: (1) to prove that using as only features word vector representations makes the approach competitive in the event extracti</context>
<context position="6765" citStr="Collobert et al., 2011" startWordPosition="1050" endWordPosition="1053">ggregating the semantic information born by the word representations. 2.1 Inducing Domain-Relevant Word Representations In order to induce the domain-specific word representations, we project the words into a 50- dimensional word space. We chose a single 1This max operation consists in taking, for each component of the vector, the max value of this component for each word vector representation. layer neural network (NN) architecture that avoids strongly engineered features, assumes little prior knowledge about the task, but is powerful enough to capture relevant domain information. Following (Collobert et al., 2011), we use an NN which learns to predict whether a given text sequence (short word window) exists naturally in the considered domain. We represent an input sequence of n words as (wi) = (wi−(n/2) ... , wi, ... wi+(n/2)). The main idea is that each sequence of words in the training set should receive a higher score than a sequence in which one word is replaced with a random one. We call the sequence with a random word corrupted (¯(wi)) and denote as correct ((wi)) all the sequences of words from the data set. The goal of the training step is then to minimize the following loss function for a word</context>
<context position="9609" citStr="Collobert et al., 2011" startWordPosition="1548" endWordPosition="1551">tial experimental results proved that using this domain1853 oriented order leads to better performance for the task than the order by frequency. 2.2 Using Word Representations to Identify Event Roles After having generated for each word their vector representation, we use them as features for the annotated data to classify event roles. However, event role fillers are not generally single words but noun phrases that can be, in some cases, identified as named entities. For identifying the event roles, we therefore apply a two-step strategy. First, we extract the noun chunks using SENNA2 parser (Collobert et al., 2011; Collobert, 2011) and we build a representation for these chunks defined as the maximum, per column, of the vector representations of the words it contains. Second, we use a statistical classifier to recognize the slot fillers, using this representation as features. We chose the extra-trees ensemble classifier (Geurts et al., 2006), which is a meta estimator that fits a number of randomized decision trees (extra-trees) on various sub-samples of the data set and use averaging to improve the predictive accuracy and control over-fitting. 3 Experiments and Results 3.1 Task Description We conducte</context>
</contexts>
<marker>Collobert, Weston, Battou, Karlen, Kavukcuoglu, Kuksa, 2011</marker>
<rawString>Ronan Collobert, Jason Weston, L´eon Battou, Michael Karlen, Koray Kavukcuoglu, and Pavel Kuksa. 2011. Natural language processing (almost) from scratch. Journal of Machine Learning Research, 12:2493–2537.</rawString>
</citation>
<citation valid="true">
<authors>
<author>Ronan Collobert</author>
</authors>
<title>Deep learning for efficient discriminative parsing.</title>
<date>2011</date>
<booktitle>In 14th International Conference on Artificial Intelligence and Statistics (AISTATS</booktitle>
<contexts>
<context position="9627" citStr="Collobert, 2011" startWordPosition="1552" endWordPosition="1553">s proved that using this domain1853 oriented order leads to better performance for the task than the order by frequency. 2.2 Using Word Representations to Identify Event Roles After having generated for each word their vector representation, we use them as features for the annotated data to classify event roles. However, event role fillers are not generally single words but noun phrases that can be, in some cases, identified as named entities. For identifying the event roles, we therefore apply a two-step strategy. First, we extract the noun chunks using SENNA2 parser (Collobert et al., 2011; Collobert, 2011) and we build a representation for these chunks defined as the maximum, per column, of the vector representations of the words it contains. Second, we use a statistical classifier to recognize the slot fillers, using this representation as features. We chose the extra-trees ensemble classifier (Geurts et al., 2006), which is a meta estimator that fits a number of randomized decision trees (extra-trees) on various sub-samples of the data set and use averaging to improve the predictive accuracy and control over-fitting. 3 Experiments and Results 3.1 Task Description We conducted the experiments </context>
</contexts>
<marker>Collobert, 2011</marker>
<rawString>Ronan Collobert. 2011. Deep learning for efficient discriminative parsing. In 14th International Conference on Artificial Intelligence and Statistics (AISTATS 2011).</rawString>
</citation>
<citation valid="true">
<authors>
<author>Dayne Freitag</author>
</authors>
<title>Information extraction from HTML: Application of a general machine learning approach.</title>
<date>1998</date>
<booktitle>In AAAI’98,</booktitle>
<pages>517--523</pages>
<contexts>
<context position="1875" citStr="Freitag, 1998" startWordPosition="270" endWordPosition="271">e address here is the detection of role-filler candidates and their association with specific roles in event templates. For this task, IE systems adopt various ways of extracting patterns or generating rules based on the surrounding context, local context and global context (Patwardhan and Riloff, 2009). Current approaches for learning such patterns include bootstrapping techniques (Huang and Riloff, 2012a; Yangarber et al., 2000), weakly supervised learning algorithms (Huang and Riloff, 2011; Sudo et al., 2003; Surdeanu et al., 2006), fully supervised learning approaches (Chieu et al., 2003; Freitag, 1998; Bunescu and Mooney, 2004; Patwardhan and Riloff, 2009) and other variations. All these methods rely on substantial amounts of manually annotated corpora and use a large body of linguistic knowledge. The performance of these approaches is related to the amount of knowledge engineering deployed and a good choice of features and classifiers. Furthermore, the efficiency of the system relies on the a priori knowledge of the applicative domain (the nature of the events) and it is generally difficult to apply a system on a different domain with less annotated data without reconsidering the design o</context>
</contexts>
<marker>Freitag, 1998</marker>
<rawString>Dayne Freitag. 1998. Information extraction from HTML: Application of a general machine learning approach. In AAAI’98, pages 517–523.</rawString>
</citation>
<citation valid="true">
<authors>
<author>Pierre Geurts</author>
<author>Damien Ernst</author>
<author>Louis Wehenkel</author>
</authors>
<title>Extremely randomized trees.</title>
<date>2006</date>
<booktitle>Machine Learning,</booktitle>
<pages>63--1</pages>
<contexts>
<context position="9943" citStr="Geurts et al., 2006" startWordPosition="1600" endWordPosition="1603">. However, event role fillers are not generally single words but noun phrases that can be, in some cases, identified as named entities. For identifying the event roles, we therefore apply a two-step strategy. First, we extract the noun chunks using SENNA2 parser (Collobert et al., 2011; Collobert, 2011) and we build a representation for these chunks defined as the maximum, per column, of the vector representations of the words it contains. Second, we use a statistical classifier to recognize the slot fillers, using this representation as features. We chose the extra-trees ensemble classifier (Geurts et al., 2006), which is a meta estimator that fits a number of randomized decision trees (extra-trees) on various sub-samples of the data set and use averaging to improve the predictive accuracy and control over-fitting. 3 Experiments and Results 3.1 Task Description We conducted the experiments on the official MUC-4 training corpus that consists of 1,700 documents and instantiated templates for each document. The task consists in extracting information about terrorist events in Latin America from news articles. We classically considered the following 4 types of events: attack, bombing, kidnapping and arso</context>
</contexts>
<marker>Geurts, Ernst, Wehenkel, 2006</marker>
<rawString>Pierre Geurts, Damien Ernst, and Louis Wehenkel. 2006. Extremely randomized trees. Machine Learning, 63(1):3–42.</rawString>
</citation>
<citation valid="true">
<authors>
<author>Xavier Glorot</author>
<author>Antoine Bordes</author>
<author>Yoshua Bengio</author>
</authors>
<title>Domain adaptation for large-scale sentiment classification: A deep learning approach.</title>
<date>2011</date>
<booktitle>In 28th International Conference on Machine Learning (ICML-11),</booktitle>
<pages>513--520</pages>
<contexts>
<context position="4074" citStr="Glorot et al., 2011" startWordPosition="618" endWordPosition="621">e semantic word representations (commonly referred as word embeddings) in an unsupervised fashion, as in (Bengio et al., 2006; Collobert and Weston, 2008). We exploit these word embeddings as features for a supervised event role (multiclass) classifier. This type of approach has been proved efficient for numerous tasks in natural language processing, including named entity recognition (Turian et al., 2010), semantic role labeling (Collobert et al., 2011), machine translation (Schwenk and Koehn, 2008; Lambert et al., 2012), word sense disambiguation (Bordes et al., 2012) or sentiment analysis (Glorot et al., 2011; Socher et al., 2011) but has never been used, to our knowl1852 Proceedings of the 2014 Conference on Empirical Methods in Natural Language Processing (EMNLP), pages 1852–1857, October 25-29, 2014, Doha, Qatar. c�2014 Association for Computational Linguistics edge, for an event extraction task. Our goal is twofold: (1) to prove that using as only features word vector representations makes the approach competitive in the event extraction task; (2) to show that these word representations are scalable and robust when varying the size of the training data. Focusing on the data provided in MUC-4 (</context>
</contexts>
<marker>Glorot, Bordes, Bengio, 2011</marker>
<rawString>Xavier Glorot, Antoine Bordes, and Yoshua Bengio. 2011. Domain adaptation for large-scale sentiment classification: A deep learning approach. In 28th International Conference on Machine Learning (ICML-11), pages 513–520.</rawString>
</citation>
<citation valid="true">
<authors>
<author>Ruihong Huang</author>
<author>Ellen Riloff</author>
</authors>
<title>Peeling back the layers: Detecting event role fillers in secondary contexts.</title>
<date>2011</date>
<booktitle>In ACL 2011,</booktitle>
<pages>1137--1147</pages>
<contexts>
<context position="1759" citStr="Huang and Riloff, 2011" startWordPosition="250" endWordPosition="253">asks: event mention detection, candidate rolefiller extraction, relation extraction and event template filling. The problem we address here is the detection of role-filler candidates and their association with specific roles in event templates. For this task, IE systems adopt various ways of extracting patterns or generating rules based on the surrounding context, local context and global context (Patwardhan and Riloff, 2009). Current approaches for learning such patterns include bootstrapping techniques (Huang and Riloff, 2012a; Yangarber et al., 2000), weakly supervised learning algorithms (Huang and Riloff, 2011; Sudo et al., 2003; Surdeanu et al., 2006), fully supervised learning approaches (Chieu et al., 2003; Freitag, 1998; Bunescu and Mooney, 2004; Patwardhan and Riloff, 2009) and other variations. All these methods rely on substantial amounts of manually annotated corpora and use a large body of linguistic knowledge. The performance of these approaches is related to the amount of knowledge engineering deployed and a good choice of features and classifiers. Furthermore, the efficiency of the system relies on the a priori knowledge of the applicative domain (the nature of the events) and it is gen</context>
<context position="10781" citStr="Huang and Riloff, 2011" startWordPosition="1732" endWordPosition="1735">ments and Results 3.1 Task Description We conducted the experiments on the official MUC-4 training corpus that consists of 1,700 documents and instantiated templates for each document. The task consists in extracting information about terrorist events in Latin America from news articles. We classically considered the following 4 types of events: attack, bombing, kidnapping and arson. These are represented by templates containing various slots for each piece of information that should be extracted from the document (perpetrators, human targets, physical targets, etc). Following previous works (Huang and Riloff, 2011; Huang and Riloff, 2012a), we only consider the “String Slots” in this work (other slots need different treatments) and we group certain slots to finally consider the five slot types PerpInd (individual perpetrator), PerpOrg (organizational perpetrator), Target (physical target), Victim (human target name or description) and Weapon (instrument id or type). We used 1,300 documents (DEV) for training, 200 documents (TST1+TST2) 2Code and resources can be found at http://ml. nec-labs.com/senna/ for tuning, and 200 documents (TST3+TST4) as the blind test set. To compare with similar works, we do n</context>
<context position="13987" citStr="Huang and Riloff, 2011" startWordPosition="2226" endWordPosition="2229">LBL-50)3 and finally to another 3C&amp;W-50 are described in (Collobert and Weston, 2008), HLBL-50 are the Hierarchical log-bilinear embeddings (Mnih and Hinton, 2007), provided by (Turian et al., 2010), available at http://metaoptimize.com/ projects/wordreprs induced from the Reuters-RCV1 1854 State-of-the-art systems PerpInd PerpOrg Target Victim Weapon Average (Riloff, 1996) 33/49/40 53/33/41 54/59/56 49/54/51 38/44/41 45/48/46 (Patwardhan and Riloff, 2007) 39/48/43 55/31/40 37/60/46 44/46/45 47/47/47 44/36/40 (Patwardhan and Riloff, 2009) 51/58/54 34/45/38 43/72/53 55/58/56 57/53/55 48/57/52 (Huang and Riloff, 2011) 48/57/52 46/53/50 51/73/60 56/60/58 53/64/58 51/62/56 (Huang and Riloff, 2012a) 47/51/47 60/39/47 37/65/47 39/53/45 53/55/54 47/53/50 (Huang and Riloff, 2012b) 54/57/56 55/49/51 55/68/61 63/59/61 62/64/63 58/60/59 Models based on word embeddings C&amp;W-50 80/55/65 64/65/64 76/72/74 53/63/57 85/64/73 68/63/65 HLBL-50 81/53/64 63/67/65 78/72/75 53/63/58 93/64/75 69/62/66 W2V-50 79/57/66 88/71/79 74/72/73 69/75/71 97/65/78 77/68/72 DRVR-50 79/57/66 91/74/81 79/57/66 77/75/76 92/58/81 80/67/73 Table 1: Accuracy of “String Slots” on the TST3 + TST4 test set P/R/F1 (Precision/Recall/F1-Score) word rep</context>
</contexts>
<marker>Huang, Riloff, 2011</marker>
<rawString>Ruihong Huang and Ellen Riloff. 2011. Peeling back the layers: Detecting event role fillers in secondary contexts. In ACL 2011, pages 1137–1147.</rawString>
</citation>
<citation valid="true">
<authors>
<author>Ruihong Huang</author>
<author>Ellen Riloff</author>
</authors>
<title>Bootstrapped training of event extraction classifiers.</title>
<date>2012</date>
<booktitle>In 13th Conference of the European Chapter of the Association for Computational Linguistics (EACL</booktitle>
<pages>286--295</pages>
<contexts>
<context position="1670" citStr="Huang and Riloff, 2012" startWordPosition="237" endWordPosition="240">les) whose values are text excerpts. The event extraction task is related to several subtasks: event mention detection, candidate rolefiller extraction, relation extraction and event template filling. The problem we address here is the detection of role-filler candidates and their association with specific roles in event templates. For this task, IE systems adopt various ways of extracting patterns or generating rules based on the surrounding context, local context and global context (Patwardhan and Riloff, 2009). Current approaches for learning such patterns include bootstrapping techniques (Huang and Riloff, 2012a; Yangarber et al., 2000), weakly supervised learning algorithms (Huang and Riloff, 2011; Sudo et al., 2003; Surdeanu et al., 2006), fully supervised learning approaches (Chieu et al., 2003; Freitag, 1998; Bunescu and Mooney, 2004; Patwardhan and Riloff, 2009) and other variations. All these methods rely on substantial amounts of manually annotated corpora and use a large body of linguistic knowledge. The performance of these approaches is related to the amount of knowledge engineering deployed and a good choice of features and classifiers. Furthermore, the efficiency of the system relies on </context>
<context position="5510" citStr="Huang and Riloff, 2012" startWordPosition="855" endWordPosition="858">ction task by learning word representations from a domainspecific data set and by using these representations to identify the event roles. This idea relies on the assumption that the different words used for a given event role in the text share some semantic properties, related to their context of use and that these similarities can be captured by specific representations that can be automatically induced from the text, in an unsupervised way. We then propose to rely only on these word representations to detect the event roles whereas, in most works (Riloff, 1996; Patwardhan and Riloff, 2007; Huang and Riloff, 2012a; Huang and Riloff, 2012b), the role fillers are represented by a set of different features (raw words, their parts-ofspeech, syntactic or semantic roles in the sentence). Furthermore, we propose two additional contributions to the construction of the word representations. The first one is to exploit limited knowledge about the event types (seed words) to improve the learning procedure by better selecting the dictionary. The second one is to use a max operation1 on the word vector representations in order to build noun phrase representations (since slot fillers are generally noun phrases), wh</context>
<context position="10805" citStr="Huang and Riloff, 2012" startWordPosition="1736" endWordPosition="1739">sk Description We conducted the experiments on the official MUC-4 training corpus that consists of 1,700 documents and instantiated templates for each document. The task consists in extracting information about terrorist events in Latin America from news articles. We classically considered the following 4 types of events: attack, bombing, kidnapping and arson. These are represented by templates containing various slots for each piece of information that should be extracted from the document (perpetrators, human targets, physical targets, etc). Following previous works (Huang and Riloff, 2011; Huang and Riloff, 2012a), we only consider the “String Slots” in this work (other slots need different treatments) and we group certain slots to finally consider the five slot types PerpInd (individual perpetrator), PerpOrg (organizational perpetrator), Target (physical target), Victim (human target name or description) and Weapon (instrument id or type). We used 1,300 documents (DEV) for training, 200 documents (TST1+TST2) 2Code and resources can be found at http://ml. nec-labs.com/senna/ for tuning, and 200 documents (TST3+TST4) as the blind test set. To compare with similar works, we do not evaluate the template</context>
<context position="14065" citStr="Huang and Riloff, 2012" startWordPosition="2236" endWordPosition="2239"> 2008), HLBL-50 are the Hierarchical log-bilinear embeddings (Mnih and Hinton, 2007), provided by (Turian et al., 2010), available at http://metaoptimize.com/ projects/wordreprs induced from the Reuters-RCV1 1854 State-of-the-art systems PerpInd PerpOrg Target Victim Weapon Average (Riloff, 1996) 33/49/40 53/33/41 54/59/56 49/54/51 38/44/41 45/48/46 (Patwardhan and Riloff, 2007) 39/48/43 55/31/40 37/60/46 44/46/45 47/47/47 44/36/40 (Patwardhan and Riloff, 2009) 51/58/54 34/45/38 43/72/53 55/58/56 57/53/55 48/57/52 (Huang and Riloff, 2011) 48/57/52 46/53/50 51/73/60 56/60/58 53/64/58 51/62/56 (Huang and Riloff, 2012a) 47/51/47 60/39/47 37/65/47 39/53/45 53/55/54 47/53/50 (Huang and Riloff, 2012b) 54/57/56 55/49/51 55/68/61 63/59/61 62/64/63 58/60/59 Models based on word embeddings C&amp;W-50 80/55/65 64/65/64 76/72/74 53/63/57 85/64/73 68/63/65 HLBL-50 81/53/64 63/67/65 78/72/75 53/63/58 93/64/75 69/62/66 W2V-50 79/57/66 88/71/79 74/72/73 69/75/71 97/65/78 77/68/72 DRVR-50 79/57/66 91/74/81 79/57/66 77/75/76 92/58/81 80/67/73 Table 1: Accuracy of “String Slots” on the TST3 + TST4 test set P/R/F1 (Precision/Recall/F1-Score) word representation construction on the domainspecific data (W2V-50)4. Figure 1: F1-sc</context>
</contexts>
<marker>Huang, Riloff, 2012</marker>
<rawString>Ruihong Huang and Ellen Riloff. 2012a. Bootstrapped training of event extraction classifiers. In 13th Conference of the European Chapter of the Association for Computational Linguistics (EACL 2012), pages 286–295.</rawString>
</citation>
<citation valid="true">
<authors>
<author>Ruihong Huang</author>
<author>Ellen Riloff</author>
</authors>
<title>Modeling textual cohesion for event extraction.</title>
<date>2012</date>
<booktitle>In 26th Conference on Artificial Intelligence (AAAI</booktitle>
<contexts>
<context position="1670" citStr="Huang and Riloff, 2012" startWordPosition="237" endWordPosition="240">les) whose values are text excerpts. The event extraction task is related to several subtasks: event mention detection, candidate rolefiller extraction, relation extraction and event template filling. The problem we address here is the detection of role-filler candidates and their association with specific roles in event templates. For this task, IE systems adopt various ways of extracting patterns or generating rules based on the surrounding context, local context and global context (Patwardhan and Riloff, 2009). Current approaches for learning such patterns include bootstrapping techniques (Huang and Riloff, 2012a; Yangarber et al., 2000), weakly supervised learning algorithms (Huang and Riloff, 2011; Sudo et al., 2003; Surdeanu et al., 2006), fully supervised learning approaches (Chieu et al., 2003; Freitag, 1998; Bunescu and Mooney, 2004; Patwardhan and Riloff, 2009) and other variations. All these methods rely on substantial amounts of manually annotated corpora and use a large body of linguistic knowledge. The performance of these approaches is related to the amount of knowledge engineering deployed and a good choice of features and classifiers. Furthermore, the efficiency of the system relies on </context>
<context position="5510" citStr="Huang and Riloff, 2012" startWordPosition="855" endWordPosition="858">ction task by learning word representations from a domainspecific data set and by using these representations to identify the event roles. This idea relies on the assumption that the different words used for a given event role in the text share some semantic properties, related to their context of use and that these similarities can be captured by specific representations that can be automatically induced from the text, in an unsupervised way. We then propose to rely only on these word representations to detect the event roles whereas, in most works (Riloff, 1996; Patwardhan and Riloff, 2007; Huang and Riloff, 2012a; Huang and Riloff, 2012b), the role fillers are represented by a set of different features (raw words, their parts-ofspeech, syntactic or semantic roles in the sentence). Furthermore, we propose two additional contributions to the construction of the word representations. The first one is to exploit limited knowledge about the event types (seed words) to improve the learning procedure by better selecting the dictionary. The second one is to use a max operation1 on the word vector representations in order to build noun phrase representations (since slot fillers are generally noun phrases), wh</context>
<context position="10805" citStr="Huang and Riloff, 2012" startWordPosition="1736" endWordPosition="1739">sk Description We conducted the experiments on the official MUC-4 training corpus that consists of 1,700 documents and instantiated templates for each document. The task consists in extracting information about terrorist events in Latin America from news articles. We classically considered the following 4 types of events: attack, bombing, kidnapping and arson. These are represented by templates containing various slots for each piece of information that should be extracted from the document (perpetrators, human targets, physical targets, etc). Following previous works (Huang and Riloff, 2011; Huang and Riloff, 2012a), we only consider the “String Slots” in this work (other slots need different treatments) and we group certain slots to finally consider the five slot types PerpInd (individual perpetrator), PerpOrg (organizational perpetrator), Target (physical target), Victim (human target name or description) and Weapon (instrument id or type). We used 1,300 documents (DEV) for training, 200 documents (TST1+TST2) 2Code and resources can be found at http://ml. nec-labs.com/senna/ for tuning, and 200 documents (TST3+TST4) as the blind test set. To compare with similar works, we do not evaluate the template</context>
<context position="14065" citStr="Huang and Riloff, 2012" startWordPosition="2236" endWordPosition="2239"> 2008), HLBL-50 are the Hierarchical log-bilinear embeddings (Mnih and Hinton, 2007), provided by (Turian et al., 2010), available at http://metaoptimize.com/ projects/wordreprs induced from the Reuters-RCV1 1854 State-of-the-art systems PerpInd PerpOrg Target Victim Weapon Average (Riloff, 1996) 33/49/40 53/33/41 54/59/56 49/54/51 38/44/41 45/48/46 (Patwardhan and Riloff, 2007) 39/48/43 55/31/40 37/60/46 44/46/45 47/47/47 44/36/40 (Patwardhan and Riloff, 2009) 51/58/54 34/45/38 43/72/53 55/58/56 57/53/55 48/57/52 (Huang and Riloff, 2011) 48/57/52 46/53/50 51/73/60 56/60/58 53/64/58 51/62/56 (Huang and Riloff, 2012a) 47/51/47 60/39/47 37/65/47 39/53/45 53/55/54 47/53/50 (Huang and Riloff, 2012b) 54/57/56 55/49/51 55/68/61 63/59/61 62/64/63 58/60/59 Models based on word embeddings C&amp;W-50 80/55/65 64/65/64 76/72/74 53/63/57 85/64/73 68/63/65 HLBL-50 81/53/64 63/67/65 78/72/75 53/63/58 93/64/75 69/62/66 W2V-50 79/57/66 88/71/79 74/72/73 69/75/71 97/65/78 77/68/72 DRVR-50 79/57/66 91/74/81 79/57/66 77/75/76 92/58/81 80/67/73 Table 1: Accuracy of “String Slots” on the TST3 + TST4 test set P/R/F1 (Precision/Recall/F1-Score) word representation construction on the domainspecific data (W2V-50)4. Figure 1: F1-sc</context>
</contexts>
<marker>Huang, Riloff, 2012</marker>
<rawString>Ruihong Huang and Ellen Riloff. 2012b. Modeling textual cohesion for event extraction. In 26th Conference on Artificial Intelligence (AAAI 2012).</rawString>
</citation>
<citation valid="true">
<authors>
<author>Patrik Lambert</author>
<author>Holger Schwenk</author>
<author>Fr´ed´eric Blain</author>
</authors>
<title>Automatic translation of scientific documents in the hal archive.</title>
<date>2012</date>
<booktitle>In LREC 2012,</booktitle>
<pages>3933--3936</pages>
<contexts>
<context position="3982" citStr="Lambert et al., 2012" startWordPosition="604" endWordPosition="607">earning relevant features that requires limited prior knowledge, using a neural model to induce semantic word representations (commonly referred as word embeddings) in an unsupervised fashion, as in (Bengio et al., 2006; Collobert and Weston, 2008). We exploit these word embeddings as features for a supervised event role (multiclass) classifier. This type of approach has been proved efficient for numerous tasks in natural language processing, including named entity recognition (Turian et al., 2010), semantic role labeling (Collobert et al., 2011), machine translation (Schwenk and Koehn, 2008; Lambert et al., 2012), word sense disambiguation (Bordes et al., 2012) or sentiment analysis (Glorot et al., 2011; Socher et al., 2011) but has never been used, to our knowl1852 Proceedings of the 2014 Conference on Empirical Methods in Natural Language Processing (EMNLP), pages 1852–1857, October 25-29, 2014, Doha, Qatar. c�2014 Association for Computational Linguistics edge, for an event extraction task. Our goal is twofold: (1) to prove that using as only features word vector representations makes the approach competitive in the event extraction task; (2) to show that these word representations are scalable and</context>
</contexts>
<marker>Lambert, Schwenk, Blain, 2012</marker>
<rawString>Patrik Lambert, Holger Schwenk, and Fr´ed´eric Blain. 2012. Automatic translation of scientific documents in the hal archive. In LREC 2012, pages 3933–3936.</rawString>
</citation>
<citation valid="true">
<authors>
<author>Claudia Leacock</author>
<author>Martin Chodorow</author>
</authors>
<title>Combining local context and Wordnet similarity for word sense identification.</title>
<date>1998</date>
<booktitle>In Christiane Fellbaum, editor, WordNet: An electronic lexical database.,</booktitle>
<pages>265--283</pages>
<publisher>MIT Press.</publisher>
<contexts>
<context position="8982" citStr="Leacock and Chodorow, 1998" startWordPosition="1446" endWordPosition="1449">ds are not always the most relevant for the task of event role detection. Since we want to have a training more focused to the domain specific task, we chose to order the dictionary by word relevance to the domain. We accomplish this by considering a limited number of seed words for each event type that needs to be discovered in text (e.g. attack, bombing, kidnapping, arson). We then rate with higher values the words that are more similar to the event types words, according to a given semantic similarity, and we rank them accordingly. We use the “Leacock Chodorow” similarity from Wordnet 3.0 (Leacock and Chodorow, 1998). Initial experimental results proved that using this domain1853 oriented order leads to better performance for the task than the order by frequency. 2.2 Using Word Representations to Identify Event Roles After having generated for each word their vector representation, we use them as features for the annotated data to classify event roles. However, event role fillers are not generally single words but noun phrases that can be, in some cases, identified as named entities. For identifying the event roles, we therefore apply a two-step strategy. First, we extract the noun chunks using SENNA2 par</context>
</contexts>
<marker>Leacock, Chodorow, 1998</marker>
<rawString>Claudia Leacock and Martin Chodorow. 1998. Combining local context and Wordnet similarity for word sense identification. In Christiane Fellbaum, editor, WordNet: An electronic lexical database., pages 265–283. MIT Press.</rawString>
</citation>
<citation valid="true">
<authors>
<author>Wendy Lehnert</author>
<author>Claire Cardie</author>
<author>David Fisher</author>
<author>John McCarthy</author>
<author>Ellen Riloff</author>
<author>Stephen Soderland</author>
</authors>
<title>test results and analysis.</title>
<date>1992</date>
<booktitle>In 4th Conference on Message understanding,</booktitle>
<pages>151--158</pages>
<institution>University of Massachusetts:</institution>
<contexts>
<context position="4695" citStr="Lehnert et al., 1992" startWordPosition="718" endWordPosition="721">; Socher et al., 2011) but has never been used, to our knowl1852 Proceedings of the 2014 Conference on Empirical Methods in Natural Language Processing (EMNLP), pages 1852–1857, October 25-29, 2014, Doha, Qatar. c�2014 Association for Computational Linguistics edge, for an event extraction task. Our goal is twofold: (1) to prove that using as only features word vector representations makes the approach competitive in the event extraction task; (2) to show that these word representations are scalable and robust when varying the size of the training data. Focusing on the data provided in MUC-4 (Lehnert et al., 1992), we prove the relevance of our approach by outperforming state-of-the-art methods, in the same evaluation environment as in previous works. 2 Approach In this work, we approach the event extraction task by learning word representations from a domainspecific data set and by using these representations to identify the event roles. This idea relies on the assumption that the different words used for a given event role in the text share some semantic properties, related to their context of use and that these similarities can be captured by specific representations that can be automatically induce</context>
</contexts>
<marker>Lehnert, Cardie, Fisher, McCarthy, Riloff, Soderland, 1992</marker>
<rawString>Wendy Lehnert, Claire Cardie, David Fisher, John McCarthy, Ellen Riloff, and Stephen Soderland. 1992. University of Massachusetts: MUC-4 test results and analysis. In 4th Conference on Message understanding, pages 151–158.</rawString>
</citation>
<citation valid="true">
<authors>
<author>Omer Levy</author>
<author>Yoav Goldberg</author>
</authors>
<title>Dependencybased word embeddings.</title>
<date>2014</date>
<booktitle>In 52nd Annual Meeting of the Association for Computational Linguistics (ACL 2014), Short Papers,</booktitle>
<pages>302--308</pages>
<location>Baltimore, Maryland,</location>
<marker>Levy, Goldberg, 2014</marker>
<rawString>Omer Levy and Yoav Goldberg. 2014. Dependencybased word embeddings. In 52nd Annual Meeting of the Association for Computational Linguistics (ACL 2014), Short Papers, pages 302–308, Baltimore, Maryland, June.</rawString>
</citation>
<citation valid="true">
<authors>
<author>Tomas Mikolov</author>
<author>Kai Chen</author>
<author>Greg Corrado</author>
<author>Jeffrey Dean</author>
</authors>
<title>Efficient estimation of word representations in vector space.</title>
<date>2013</date>
<booktitle>In International Conference on Learning Representations (ICLR 20013), workshop track.</booktitle>
<contexts>
<context position="15413" citStr="Mikolov et al., 2013" startWordPosition="2429" endWordPosition="2432">different parameters, compared to the learning curve of TIER (Huang and Riloff, 2012a). The grey points represent the performances of other IE systems. Figure 1 presents the average F1-score results, computed over the slots PerpInd, PerpOrg, Target, Victim and Weapon. We observe that models relying on word embeddings globally outperform the state-of-the-art results, which demonstrates that the word embeddings capture enough semantic information to perform the task of event newswire corpus 4W2V-50 are the embeddings induced from the MUC4 data set using the negative sampling training algorithm (Mikolov et al., 2013a; Mikolov et al., 2013b; Mikolov et al., 2013c), available at https://code.google.com/ p/word2vec/ role labeling on “String Slots” without using any additional hand-engineered features. Moreover, our representations (DRVR-50) clearly surpass the models based on generic embeddings (C&amp;W-50 and HLBL-50) and obtain better results than W2V50, based the competitive model of (Mikolov et al., 2013a), even if the difference is small. We can also note that the performance of our model is good even with a small amount of training data, which makes it a good candidate to easily develop an event extractio</context>
</contexts>
<marker>Mikolov, Chen, Corrado, Dean, 2013</marker>
<rawString>Tomas Mikolov, Kai Chen, Greg Corrado, and Jeffrey Dean. 2013a. Efficient estimation of word representations in vector space. In International Conference on Learning Representations (ICLR 20013), workshop track.</rawString>
</citation>
<citation valid="true">
<authors>
<author>Tomas Mikolov</author>
<author>Ilya Sutskever</author>
<author>Kai Chen</author>
<author>Greg S Corrado</author>
<author>Jeff Dean</author>
</authors>
<title>Distributed representations of words and phrases and their compositionality.</title>
<date>2013</date>
<booktitle>In Advances in Neural Information Processing Systems 26 (NIPS</booktitle>
<pages>3111--3119</pages>
<contexts>
<context position="15413" citStr="Mikolov et al., 2013" startWordPosition="2429" endWordPosition="2432">different parameters, compared to the learning curve of TIER (Huang and Riloff, 2012a). The grey points represent the performances of other IE systems. Figure 1 presents the average F1-score results, computed over the slots PerpInd, PerpOrg, Target, Victim and Weapon. We observe that models relying on word embeddings globally outperform the state-of-the-art results, which demonstrates that the word embeddings capture enough semantic information to perform the task of event newswire corpus 4W2V-50 are the embeddings induced from the MUC4 data set using the negative sampling training algorithm (Mikolov et al., 2013a; Mikolov et al., 2013b; Mikolov et al., 2013c), available at https://code.google.com/ p/word2vec/ role labeling on “String Slots” without using any additional hand-engineered features. Moreover, our representations (DRVR-50) clearly surpass the models based on generic embeddings (C&amp;W-50 and HLBL-50) and obtain better results than W2V50, based the competitive model of (Mikolov et al., 2013a), even if the difference is small. We can also note that the performance of our model is good even with a small amount of training data, which makes it a good candidate to easily develop an event extractio</context>
</contexts>
<marker>Mikolov, Sutskever, Chen, Corrado, Dean, 2013</marker>
<rawString>Tomas Mikolov, Ilya Sutskever, Kai Chen, Greg S Corrado, and Jeff Dean. 2013b. Distributed representations of words and phrases and their compositionality. In Advances in Neural Information Processing Systems 26 (NIPS 2013), pages 3111–3119.</rawString>
</citation>
<citation valid="true">
<authors>
<author>Tomas Mikolov</author>
<author>Wen-tau Yih</author>
<author>Geoffrey Zweig</author>
</authors>
<title>Linguistic regularities in continuous space word representations.</title>
<date>2013</date>
<booktitle>In NAACL-HLT 2013,</booktitle>
<pages>746--751</pages>
<contexts>
<context position="15413" citStr="Mikolov et al., 2013" startWordPosition="2429" endWordPosition="2432">different parameters, compared to the learning curve of TIER (Huang and Riloff, 2012a). The grey points represent the performances of other IE systems. Figure 1 presents the average F1-score results, computed over the slots PerpInd, PerpOrg, Target, Victim and Weapon. We observe that models relying on word embeddings globally outperform the state-of-the-art results, which demonstrates that the word embeddings capture enough semantic information to perform the task of event newswire corpus 4W2V-50 are the embeddings induced from the MUC4 data set using the negative sampling training algorithm (Mikolov et al., 2013a; Mikolov et al., 2013b; Mikolov et al., 2013c), available at https://code.google.com/ p/word2vec/ role labeling on “String Slots” without using any additional hand-engineered features. Moreover, our representations (DRVR-50) clearly surpass the models based on generic embeddings (C&amp;W-50 and HLBL-50) and obtain better results than W2V50, based the competitive model of (Mikolov et al., 2013a), even if the difference is small. We can also note that the performance of our model is good even with a small amount of training data, which makes it a good candidate to easily develop an event extractio</context>
</contexts>
<marker>Mikolov, Yih, Zweig, 2013</marker>
<rawString>Tomas Mikolov, Wen-tau Yih, and Geoffrey Zweig. 2013c. Linguistic regularities in continuous space word representations. In NAACL-HLT 2013, pages 746–751.</rawString>
</citation>
<citation valid="true">
<authors>
<author>Andriy Mnih</author>
<author>Geoffrey Hinton</author>
</authors>
<title>Three new graphical models for statistical modelling.</title>
<date>2007</date>
<booktitle>In 24th International Conference of Machine learning (ICML</booktitle>
<pages>641--648</pages>
<publisher>ACM.</publisher>
<contexts>
<context position="13527" citStr="Mnih and Hinton, 2007" startWordPosition="2171" endWordPosition="2174">es ensemble classifier implemented in (Pedregosa et al., 2011), with hyperparameters optimized on the validation data: forest of 500 trees and the maximum number of features to consider when looking for the best split is √number features. We present a 3- fold evaluation: first, we compare our system with state-of-the-art systems on the same task, then we compare our domain-relevant vector representations (DRVR-50) to more generic word embeddings (C&amp;W50, HLBL-50)3 and finally to another 3C&amp;W-50 are described in (Collobert and Weston, 2008), HLBL-50 are the Hierarchical log-bilinear embeddings (Mnih and Hinton, 2007), provided by (Turian et al., 2010), available at http://metaoptimize.com/ projects/wordreprs induced from the Reuters-RCV1 1854 State-of-the-art systems PerpInd PerpOrg Target Victim Weapon Average (Riloff, 1996) 33/49/40 53/33/41 54/59/56 49/54/51 38/44/41 45/48/46 (Patwardhan and Riloff, 2007) 39/48/43 55/31/40 37/60/46 44/46/45 47/47/47 44/36/40 (Patwardhan and Riloff, 2009) 51/58/54 34/45/38 43/72/53 55/58/56 57/53/55 48/57/52 (Huang and Riloff, 2011) 48/57/52 46/53/50 51/73/60 56/60/58 53/64/58 51/62/56 (Huang and Riloff, 2012a) 47/51/47 60/39/47 37/65/47 39/53/45 53/55/54 47/53/50 (Huan</context>
</contexts>
<marker>Mnih, Hinton, 2007</marker>
<rawString>Andriy Mnih and Geoffrey Hinton. 2007. Three new graphical models for statistical modelling. In 24th International Conference of Machine learning (ICML 2007), pages 641–648. ACM.</rawString>
</citation>
<citation valid="true">
<authors>
<author>Siddharth Patwardhan</author>
<author>Ellen Riloff</author>
</authors>
<title>Effective information extraction with semantic affinity patterns and relevant regions. In</title>
<date>2007</date>
<booktitle>Joint Conference on Empirical Methods in Natural Language Processing and Computational Natural Language Learning</booktitle>
<pages>717--727</pages>
<contexts>
<context position="2712" citStr="Patwardhan and Riloff, 2007" startWordPosition="402" endWordPosition="406">rmance of these approaches is related to the amount of knowledge engineering deployed and a good choice of features and classifiers. Furthermore, the efficiency of the system relies on the a priori knowledge of the applicative domain (the nature of the events) and it is generally difficult to apply a system on a different domain with less annotated data without reconsidering the design of the features used. An important step forwards is TIERlight (Huang and Riloff, 2012a) that targeted the minimization of human supervision with a bootstrapping technique for event roles detection. Also, PIPER (Patwardhan and Riloff, 2007; Patwardhan, 2010) distinguishes between relevant and irrelevant regions and learns domain-relevant extraction patterns using a semantic affinity measure. Another possible approach for dealing with this problem is to combine the use a restricted set of manually annotated data with a much larger set of data extracted in an unsupervised way from a corpus. This approach was experimented for relations in the context of Open Information Extraction (Soderland et al., 2010) but not for extracting events and their participants to our knowledge. In this paper, we propose to approach the task of labeli</context>
<context position="5486" citStr="Patwardhan and Riloff, 2007" startWordPosition="851" endWordPosition="854">, we approach the event extraction task by learning word representations from a domainspecific data set and by using these representations to identify the event roles. This idea relies on the assumption that the different words used for a given event role in the text share some semantic properties, related to their context of use and that these similarities can be captured by specific representations that can be automatically induced from the text, in an unsupervised way. We then propose to rely only on these word representations to detect the event roles whereas, in most works (Riloff, 1996; Patwardhan and Riloff, 2007; Huang and Riloff, 2012a; Huang and Riloff, 2012b), the role fillers are represented by a set of different features (raw words, their parts-ofspeech, syntactic or semantic roles in the sentence). Furthermore, we propose two additional contributions to the construction of the word representations. The first one is to exploit limited knowledge about the event types (seed words) to improve the learning procedure by better selecting the dictionary. The second one is to use a max operation1 on the word vector representations in order to build noun phrase representations (since slot fillers are gen</context>
<context position="13824" citStr="Patwardhan and Riloff, 2007" startWordPosition="2206" endWordPosition="2209">ur system with state-of-the-art systems on the same task, then we compare our domain-relevant vector representations (DRVR-50) to more generic word embeddings (C&amp;W50, HLBL-50)3 and finally to another 3C&amp;W-50 are described in (Collobert and Weston, 2008), HLBL-50 are the Hierarchical log-bilinear embeddings (Mnih and Hinton, 2007), provided by (Turian et al., 2010), available at http://metaoptimize.com/ projects/wordreprs induced from the Reuters-RCV1 1854 State-of-the-art systems PerpInd PerpOrg Target Victim Weapon Average (Riloff, 1996) 33/49/40 53/33/41 54/59/56 49/54/51 38/44/41 45/48/46 (Patwardhan and Riloff, 2007) 39/48/43 55/31/40 37/60/46 44/46/45 47/47/47 44/36/40 (Patwardhan and Riloff, 2009) 51/58/54 34/45/38 43/72/53 55/58/56 57/53/55 48/57/52 (Huang and Riloff, 2011) 48/57/52 46/53/50 51/73/60 56/60/58 53/64/58 51/62/56 (Huang and Riloff, 2012a) 47/51/47 60/39/47 37/65/47 39/53/45 53/55/54 47/53/50 (Huang and Riloff, 2012b) 54/57/56 55/49/51 55/68/61 63/59/61 62/64/63 58/60/59 Models based on word embeddings C&amp;W-50 80/55/65 64/65/64 76/72/74 53/63/57 85/64/73 68/63/65 HLBL-50 81/53/64 63/67/65 78/72/75 53/63/58 93/64/75 69/62/66 W2V-50 79/57/66 88/71/79 74/72/73 69/75/71 97/65/78 77/68/72 DRVR-5</context>
</contexts>
<marker>Patwardhan, Riloff, 2007</marker>
<rawString>Siddharth Patwardhan and Ellen Riloff. 2007. Effective information extraction with semantic affinity patterns and relevant regions. In 2007 Joint Conference on Empirical Methods in Natural Language Processing and Computational Natural Language Learning (EMNLP-CoNLL 2007), pages 717–727.</rawString>
</citation>
<citation valid="true">
<authors>
<author>Siddharth Patwardhan</author>
<author>Ellen Riloff</author>
</authors>
<title>A unified model of phrasal and sentential evidence for information extraction.</title>
<date>2009</date>
<booktitle>In 2009 Conference on Empirical Methods in Natural Language Processing (EMNLP</booktitle>
<pages>151--160</pages>
<contexts>
<context position="1566" citStr="Patwardhan and Riloff, 2009" startWordPosition="222" endWordPosition="225">traction constitutes a challenging task. An event is described by a set of participants (i.e. attributes or roles) whose values are text excerpts. The event extraction task is related to several subtasks: event mention detection, candidate rolefiller extraction, relation extraction and event template filling. The problem we address here is the detection of role-filler candidates and their association with specific roles in event templates. For this task, IE systems adopt various ways of extracting patterns or generating rules based on the surrounding context, local context and global context (Patwardhan and Riloff, 2009). Current approaches for learning such patterns include bootstrapping techniques (Huang and Riloff, 2012a; Yangarber et al., 2000), weakly supervised learning algorithms (Huang and Riloff, 2011; Sudo et al., 2003; Surdeanu et al., 2006), fully supervised learning approaches (Chieu et al., 2003; Freitag, 1998; Bunescu and Mooney, 2004; Patwardhan and Riloff, 2009) and other variations. All these methods rely on substantial amounts of manually annotated corpora and use a large body of linguistic knowledge. The performance of these approaches is related to the amount of knowledge engineering depl</context>
<context position="13908" citStr="Patwardhan and Riloff, 2009" startWordPosition="2216" endWordPosition="2219">-relevant vector representations (DRVR-50) to more generic word embeddings (C&amp;W50, HLBL-50)3 and finally to another 3C&amp;W-50 are described in (Collobert and Weston, 2008), HLBL-50 are the Hierarchical log-bilinear embeddings (Mnih and Hinton, 2007), provided by (Turian et al., 2010), available at http://metaoptimize.com/ projects/wordreprs induced from the Reuters-RCV1 1854 State-of-the-art systems PerpInd PerpOrg Target Victim Weapon Average (Riloff, 1996) 33/49/40 53/33/41 54/59/56 49/54/51 38/44/41 45/48/46 (Patwardhan and Riloff, 2007) 39/48/43 55/31/40 37/60/46 44/46/45 47/47/47 44/36/40 (Patwardhan and Riloff, 2009) 51/58/54 34/45/38 43/72/53 55/58/56 57/53/55 48/57/52 (Huang and Riloff, 2011) 48/57/52 46/53/50 51/73/60 56/60/58 53/64/58 51/62/56 (Huang and Riloff, 2012a) 47/51/47 60/39/47 37/65/47 39/53/45 53/55/54 47/53/50 (Huang and Riloff, 2012b) 54/57/56 55/49/51 55/68/61 63/59/61 62/64/63 58/60/59 Models based on word embeddings C&amp;W-50 80/55/65 64/65/64 76/72/74 53/63/57 85/64/73 68/63/65 HLBL-50 81/53/64 63/67/65 78/72/75 53/63/58 93/64/75 69/62/66 W2V-50 79/57/66 88/71/79 74/72/73 69/75/71 97/65/78 77/68/72 DRVR-50 79/57/66 91/74/81 79/57/66 77/75/76 92/58/81 80/67/73 Table 1: Accuracy of “String</context>
</contexts>
<marker>Patwardhan, Riloff, 2009</marker>
<rawString>Siddharth Patwardhan and Ellen Riloff. 2009. A unified model of phrasal and sentential evidence for information extraction. In 2009 Conference on Empirical Methods in Natural Language Processing (EMNLP 2009), pages 151–160.</rawString>
</citation>
<citation valid="true">
<authors>
<author>Siddharth Patwardhan</author>
</authors>
<title>Widening the field of view of information extraction through sentential event recognition.</title>
<date>2010</date>
<tech>Ph.D. thesis,</tech>
<institution>University of Utah.</institution>
<contexts>
<context position="2731" citStr="Patwardhan, 2010" startWordPosition="407" endWordPosition="408"> related to the amount of knowledge engineering deployed and a good choice of features and classifiers. Furthermore, the efficiency of the system relies on the a priori knowledge of the applicative domain (the nature of the events) and it is generally difficult to apply a system on a different domain with less annotated data without reconsidering the design of the features used. An important step forwards is TIERlight (Huang and Riloff, 2012a) that targeted the minimization of human supervision with a bootstrapping technique for event roles detection. Also, PIPER (Patwardhan and Riloff, 2007; Patwardhan, 2010) distinguishes between relevant and irrelevant regions and learns domain-relevant extraction patterns using a semantic affinity measure. Another possible approach for dealing with this problem is to combine the use a restricted set of manually annotated data with a much larger set of data extracted in an unsupervised way from a corpus. This approach was experimented for relations in the context of Open Information Extraction (Soderland et al., 2010) but not for extracting events and their participants to our knowledge. In this paper, we propose to approach the task of labeling text spans with </context>
</contexts>
<marker>Patwardhan, 2010</marker>
<rawString>Siddharth Patwardhan. 2010. Widening the field of view of information extraction through sentential event recognition. Ph.D. thesis, University of Utah.</rawString>
</citation>
<citation valid="true">
<authors>
<author>F Pedregosa</author>
<author>G Varoquaux</author>
<author>A Gramfort</author>
<author>V Michel</author>
<author>B Thirion</author>
<author>O Grisel</author>
<author>M Blondel</author>
<author>P Prettenhofer</author>
<author>R Weiss</author>
<author>V Dubourg</author>
<author>J Vanderplas</author>
<author>A Passos</author>
<author>D Cournapeau</author>
<author>M Brucher</author>
<author>M Perrot</author>
<author>E Duchesnay</author>
</authors>
<title>Scikit-learn: Machine learning in Python.</title>
<date>2011</date>
<journal>Journal of Machine Learning Research,</journal>
<pages>12--2825</pages>
<contexts>
<context position="12967" citStr="Pedregosa et al., 2011" startWordPosition="2083" endWordPosition="2086">ensional vectors obtained by training on sequences of 5 words, which is consistent with previous studies (Turian et al., 2010; Collobert and Weston, 2008). All the hyper-parameters of our model (e.g. learning rate, size of the hidden layer, size of the word vectors) have been chosen by finetuning our event extraction system on the TST1+TST2 data set. For DRVR-50 and W2V-50, the embeddings were built from the whole training corpus (1,300 documents) and the dictionary was made of all the words of this corpus under their inflected form. We used the extra-trees ensemble classifier implemented in (Pedregosa et al., 2011), with hyperparameters optimized on the validation data: forest of 500 trees and the maximum number of features to consider when looking for the best split is √number features. We present a 3- fold evaluation: first, we compare our system with state-of-the-art systems on the same task, then we compare our domain-relevant vector representations (DRVR-50) to more generic word embeddings (C&amp;W50, HLBL-50)3 and finally to another 3C&amp;W-50 are described in (Collobert and Weston, 2008), HLBL-50 are the Hierarchical log-bilinear embeddings (Mnih and Hinton, 2007), provided by (Turian et al., 2010), ava</context>
</contexts>
<marker>Pedregosa, Varoquaux, Gramfort, Michel, Thirion, Grisel, Blondel, Prettenhofer, Weiss, Dubourg, Vanderplas, Passos, Cournapeau, Brucher, Perrot, Duchesnay, 2011</marker>
<rawString>F. Pedregosa, G. Varoquaux, A. Gramfort, V. Michel, B. Thirion, O. Grisel, M. Blondel, P. Prettenhofer, R. Weiss, V. Dubourg, J. Vanderplas, A. Passos, D. Cournapeau, M. Brucher, M. Perrot, and E. Duchesnay. 2011. Scikit-learn: Machine learning in Python. Journal of Machine Learning Research, 12:2825–2830.</rawString>
</citation>
<citation valid="true">
<authors>
<author>Ellen Riloff</author>
</authors>
<title>Automatically generating extraction patterns from untagged text.</title>
<date>1996</date>
<booktitle>In AAAI’96,</booktitle>
<pages>1044--1049</pages>
<contexts>
<context position="5457" citStr="Riloff, 1996" startWordPosition="849" endWordPosition="850">h In this work, we approach the event extraction task by learning word representations from a domainspecific data set and by using these representations to identify the event roles. This idea relies on the assumption that the different words used for a given event role in the text share some semantic properties, related to their context of use and that these similarities can be captured by specific representations that can be automatically induced from the text, in an unsupervised way. We then propose to rely only on these word representations to detect the event roles whereas, in most works (Riloff, 1996; Patwardhan and Riloff, 2007; Huang and Riloff, 2012a; Huang and Riloff, 2012b), the role fillers are represented by a set of different features (raw words, their parts-ofspeech, syntactic or semantic roles in the sentence). Furthermore, we propose two additional contributions to the construction of the word representations. The first one is to exploit limited knowledge about the event types (seed words) to improve the learning procedure by better selecting the dictionary. The second one is to use a max operation1 on the word vector representations in order to build noun phrase representation</context>
<context position="13740" citStr="Riloff, 1996" startWordPosition="2198" endWordPosition="2199">number features. We present a 3- fold evaluation: first, we compare our system with state-of-the-art systems on the same task, then we compare our domain-relevant vector representations (DRVR-50) to more generic word embeddings (C&amp;W50, HLBL-50)3 and finally to another 3C&amp;W-50 are described in (Collobert and Weston, 2008), HLBL-50 are the Hierarchical log-bilinear embeddings (Mnih and Hinton, 2007), provided by (Turian et al., 2010), available at http://metaoptimize.com/ projects/wordreprs induced from the Reuters-RCV1 1854 State-of-the-art systems PerpInd PerpOrg Target Victim Weapon Average (Riloff, 1996) 33/49/40 53/33/41 54/59/56 49/54/51 38/44/41 45/48/46 (Patwardhan and Riloff, 2007) 39/48/43 55/31/40 37/60/46 44/46/45 47/47/47 44/36/40 (Patwardhan and Riloff, 2009) 51/58/54 34/45/38 43/72/53 55/58/56 57/53/55 48/57/52 (Huang and Riloff, 2011) 48/57/52 46/53/50 51/73/60 56/60/58 53/64/58 51/62/56 (Huang and Riloff, 2012a) 47/51/47 60/39/47 37/65/47 39/53/45 53/55/54 47/53/50 (Huang and Riloff, 2012b) 54/57/56 55/49/51 55/68/61 63/59/61 62/64/63 58/60/59 Models based on word embeddings C&amp;W-50 80/55/65 64/65/64 76/72/74 53/63/57 85/64/73 68/63/65 HLBL-50 81/53/64 63/67/65 78/72/75 53/63/58 9</context>
</contexts>
<marker>Riloff, 1996</marker>
<rawString>Ellen Riloff. 1996. Automatically generating extraction patterns from untagged text. In AAAI’96, pages 1044–1049.</rawString>
</citation>
<citation valid="true">
<authors>
<author>Holger Schwenk</author>
<author>Philipp Koehn</author>
</authors>
<title>Large and diverse language models for statistical machine translation.</title>
<date>2008</date>
<booktitle>In IJCNLP</booktitle>
<pages>661--666</pages>
<contexts>
<context position="3959" citStr="Schwenk and Koehn, 2008" startWordPosition="600" endWordPosition="603"> roles by automatically learning relevant features that requires limited prior knowledge, using a neural model to induce semantic word representations (commonly referred as word embeddings) in an unsupervised fashion, as in (Bengio et al., 2006; Collobert and Weston, 2008). We exploit these word embeddings as features for a supervised event role (multiclass) classifier. This type of approach has been proved efficient for numerous tasks in natural language processing, including named entity recognition (Turian et al., 2010), semantic role labeling (Collobert et al., 2011), machine translation (Schwenk and Koehn, 2008; Lambert et al., 2012), word sense disambiguation (Bordes et al., 2012) or sentiment analysis (Glorot et al., 2011; Socher et al., 2011) but has never been used, to our knowl1852 Proceedings of the 2014 Conference on Empirical Methods in Natural Language Processing (EMNLP), pages 1852–1857, October 25-29, 2014, Doha, Qatar. c�2014 Association for Computational Linguistics edge, for an event extraction task. Our goal is twofold: (1) to prove that using as only features word vector representations makes the approach competitive in the event extraction task; (2) to show that these word represent</context>
</contexts>
<marker>Schwenk, Koehn, 2008</marker>
<rawString>Holger Schwenk and Philipp Koehn. 2008. Large and diverse language models for statistical machine translation. In IJCNLP 2008, pages 661–666.</rawString>
</citation>
<citation valid="true">
<authors>
<author>Richard Socher</author>
<author>Cliff C Lin</author>
<author>Chris Manning</author>
<author>Andrew Y Ng</author>
</authors>
<title>Parsing natural scenes and natural language with recursive neural networks.</title>
<date>2011</date>
<booktitle>In 28th International Conference on Machine Learning (ICML-11),</booktitle>
<pages>129--136</pages>
<contexts>
<context position="4096" citStr="Socher et al., 2011" startWordPosition="622" endWordPosition="625">sentations (commonly referred as word embeddings) in an unsupervised fashion, as in (Bengio et al., 2006; Collobert and Weston, 2008). We exploit these word embeddings as features for a supervised event role (multiclass) classifier. This type of approach has been proved efficient for numerous tasks in natural language processing, including named entity recognition (Turian et al., 2010), semantic role labeling (Collobert et al., 2011), machine translation (Schwenk and Koehn, 2008; Lambert et al., 2012), word sense disambiguation (Bordes et al., 2012) or sentiment analysis (Glorot et al., 2011; Socher et al., 2011) but has never been used, to our knowl1852 Proceedings of the 2014 Conference on Empirical Methods in Natural Language Processing (EMNLP), pages 1852–1857, October 25-29, 2014, Doha, Qatar. c�2014 Association for Computational Linguistics edge, for an event extraction task. Our goal is twofold: (1) to prove that using as only features word vector representations makes the approach competitive in the event extraction task; (2) to show that these word representations are scalable and robust when varying the size of the training data. Focusing on the data provided in MUC-4 (Lehnert et al., 1992),</context>
</contexts>
<marker>Socher, Lin, Manning, Ng, 2011</marker>
<rawString>Richard Socher, Cliff C Lin, Chris Manning, and Andrew Y Ng. 2011. Parsing natural scenes and natural language with recursive neural networks. In 28th International Conference on Machine Learning (ICML-11), pages 129–136.</rawString>
</citation>
<citation valid="true">
<authors>
<author>Stephen Soderland</author>
<author>Brendan Roof</author>
<author>Bo Qin</author>
<author>Shi Xu</author>
<author>Mausam</author>
<author>Oren Etzioni</author>
</authors>
<title>Adapting open information extraction to domain-specific relations.</title>
<date>2010</date>
<journal>AI Magazine,</journal>
<volume>31</volume>
<issue>3</issue>
<contexts>
<context position="3184" citStr="Soderland et al., 2010" startWordPosition="479" endWordPosition="482"> that targeted the minimization of human supervision with a bootstrapping technique for event roles detection. Also, PIPER (Patwardhan and Riloff, 2007; Patwardhan, 2010) distinguishes between relevant and irrelevant regions and learns domain-relevant extraction patterns using a semantic affinity measure. Another possible approach for dealing with this problem is to combine the use a restricted set of manually annotated data with a much larger set of data extracted in an unsupervised way from a corpus. This approach was experimented for relations in the context of Open Information Extraction (Soderland et al., 2010) but not for extracting events and their participants to our knowledge. In this paper, we propose to approach the task of labeling text spans with event roles by automatically learning relevant features that requires limited prior knowledge, using a neural model to induce semantic word representations (commonly referred as word embeddings) in an unsupervised fashion, as in (Bengio et al., 2006; Collobert and Weston, 2008). We exploit these word embeddings as features for a supervised event role (multiclass) classifier. This type of approach has been proved efficient for numerous tasks in natur</context>
</contexts>
<marker>Soderland, Roof, Qin, Xu, Mausam, Etzioni, 2010</marker>
<rawString>Stephen Soderland, Brendan Roof, Bo Qin, Shi Xu, Mausam, and Oren Etzioni. 2010. Adapting open information extraction to domain-specific relations. AI Magazine, 31(3):93–102.</rawString>
</citation>
<citation valid="true">
<authors>
<author>Kiyoshi Sudo</author>
<author>Satoshi Sekine</author>
<author>Ralph Grishman</author>
</authors>
<title>An improved extraction pattern representation model for automatic ie pattern acquisition.</title>
<date>2003</date>
<booktitle>In 41st Annual Meeting on Association for Computational Linguistics (ACL-03),</booktitle>
<pages>224--231</pages>
<contexts>
<context position="1778" citStr="Sudo et al., 2003" startWordPosition="254" endWordPosition="257">ction, candidate rolefiller extraction, relation extraction and event template filling. The problem we address here is the detection of role-filler candidates and their association with specific roles in event templates. For this task, IE systems adopt various ways of extracting patterns or generating rules based on the surrounding context, local context and global context (Patwardhan and Riloff, 2009). Current approaches for learning such patterns include bootstrapping techniques (Huang and Riloff, 2012a; Yangarber et al., 2000), weakly supervised learning algorithms (Huang and Riloff, 2011; Sudo et al., 2003; Surdeanu et al., 2006), fully supervised learning approaches (Chieu et al., 2003; Freitag, 1998; Bunescu and Mooney, 2004; Patwardhan and Riloff, 2009) and other variations. All these methods rely on substantial amounts of manually annotated corpora and use a large body of linguistic knowledge. The performance of these approaches is related to the amount of knowledge engineering deployed and a good choice of features and classifiers. Furthermore, the efficiency of the system relies on the a priori knowledge of the applicative domain (the nature of the events) and it is generally difficult to</context>
</contexts>
<marker>Sudo, Sekine, Grishman, 2003</marker>
<rawString>Kiyoshi Sudo, Satoshi Sekine, and Ralph Grishman. 2003. An improved extraction pattern representation model for automatic ie pattern acquisition. In 41st Annual Meeting on Association for Computational Linguistics (ACL-03), pages 224–231.</rawString>
</citation>
<citation valid="true">
<authors>
<author>Mihai Surdeanu</author>
<author>Jordi Turmo</author>
<author>Alicia Ageno</author>
</authors>
<title>A hybrid approach for the acquisition of information extraction patterns.</title>
<date>2006</date>
<booktitle>In EACL-2006 Workshop on Adaptive Text Extraction and Mining (ATEM</booktitle>
<pages>48--55</pages>
<contexts>
<context position="1802" citStr="Surdeanu et al., 2006" startWordPosition="258" endWordPosition="261">lefiller extraction, relation extraction and event template filling. The problem we address here is the detection of role-filler candidates and their association with specific roles in event templates. For this task, IE systems adopt various ways of extracting patterns or generating rules based on the surrounding context, local context and global context (Patwardhan and Riloff, 2009). Current approaches for learning such patterns include bootstrapping techniques (Huang and Riloff, 2012a; Yangarber et al., 2000), weakly supervised learning algorithms (Huang and Riloff, 2011; Sudo et al., 2003; Surdeanu et al., 2006), fully supervised learning approaches (Chieu et al., 2003; Freitag, 1998; Bunescu and Mooney, 2004; Patwardhan and Riloff, 2009) and other variations. All these methods rely on substantial amounts of manually annotated corpora and use a large body of linguistic knowledge. The performance of these approaches is related to the amount of knowledge engineering deployed and a good choice of features and classifiers. Furthermore, the efficiency of the system relies on the a priori knowledge of the applicative domain (the nature of the events) and it is generally difficult to apply a system on a dif</context>
</contexts>
<marker>Surdeanu, Turmo, Ageno, 2006</marker>
<rawString>Mihai Surdeanu, Jordi Turmo, and Alicia Ageno. 2006. A hybrid approach for the acquisition of information extraction patterns. In EACL-2006 Workshop on Adaptive Text Extraction and Mining (ATEM 2006), pages 48–55.</rawString>
</citation>
<citation valid="true">
<authors>
<author>Joseph Turian</author>
<author>Lev Ratinov</author>
<author>Yoshua Bengio</author>
</authors>
<title>Word representations: a simple and general method for semi-supervised learning.</title>
<date>2010</date>
<booktitle>In 48th international Annual Meeting on Association for Computational Linguistics (ACL</booktitle>
<pages>384--394</pages>
<contexts>
<context position="3864" citStr="Turian et al., 2010" startWordPosition="586" endWordPosition="589"> knowledge. In this paper, we propose to approach the task of labeling text spans with event roles by automatically learning relevant features that requires limited prior knowledge, using a neural model to induce semantic word representations (commonly referred as word embeddings) in an unsupervised fashion, as in (Bengio et al., 2006; Collobert and Weston, 2008). We exploit these word embeddings as features for a supervised event role (multiclass) classifier. This type of approach has been proved efficient for numerous tasks in natural language processing, including named entity recognition (Turian et al., 2010), semantic role labeling (Collobert et al., 2011), machine translation (Schwenk and Koehn, 2008; Lambert et al., 2012), word sense disambiguation (Bordes et al., 2012) or sentiment analysis (Glorot et al., 2011; Socher et al., 2011) but has never been used, to our knowl1852 Proceedings of the 2014 Conference on Empirical Methods in Natural Language Processing (EMNLP), pages 1852–1857, October 25-29, 2014, Doha, Qatar. c�2014 Association for Computational Linguistics edge, for an event extraction task. Our goal is twofold: (1) to prove that using as only features word vector representations mak</context>
<context position="7656" citStr="Turian et al., 2010" startWordPosition="1215" endWordPosition="1218">training set should receive a higher score than a sequence in which one word is replaced with a random one. We call the sequence with a random word corrupted (¯(wi)) and denote as correct ((wi)) all the sequences of words from the data set. The goal of the training step is then to minimize the following loss function for a word wi in the dictionary D: Cw.z = Ew.z∈D max(0,1 − g((wi))+g(¯(wi))), where g(�) is the scoring function given by the neural network. Further details and evaluations of these embeddings can be found in (Bengio et al., 2003; Bengio et al., 2006; Collobert and Weston, 2008; Turian et al., 2010). For efficiency, words are fed to our architecture as indices taken from a finite dictionary. Obviously, a simple index does not carry much useful information about the word. So, the first layer of our network maps each of these word indices into a feature vector, by a lookup table operation. Our first contribution intervenes in the process of the choosing the proper dictionary. (Bengio, 2009) has shown that the order of the words in the dictionary of the neural network is not indifferent to the quality of the achieved representations: he proposed to order the dictionary by frequency and sele</context>
<context position="12469" citStr="Turian et al., 2010" startWordPosition="2002" endWordPosition="2005">count the answer keys with multiple values in the reference, dealing with conjunctions (when several victims are named, we need to find all of them) and disjunctions (when several names for the same organization are possible, we need to find any of them). Our results are reported as Precision/Recall/F1- score for each event role separately and averaged on all roles. 3.2 Experiments In all the experiments involving our model, we established the following stable choices of parameters: 50-dimensional vectors obtained by training on sequences of 5 words, which is consistent with previous studies (Turian et al., 2010; Collobert and Weston, 2008). All the hyper-parameters of our model (e.g. learning rate, size of the hidden layer, size of the word vectors) have been chosen by finetuning our event extraction system on the TST1+TST2 data set. For DRVR-50 and W2V-50, the embeddings were built from the whole training corpus (1,300 documents) and the dictionary was made of all the words of this corpus under their inflected form. We used the extra-trees ensemble classifier implemented in (Pedregosa et al., 2011), with hyperparameters optimized on the validation data: forest of 500 trees and the maximum number of</context>
</contexts>
<marker>Turian, Ratinov, Bengio, 2010</marker>
<rawString>Joseph Turian, Lev Ratinov, and Yoshua Bengio. 2010. Word representations: a simple and general method for semi-supervised learning. In 48th international Annual Meeting on Association for Computational Linguistics (ACL 2010), pages 384–394.</rawString>
</citation>
<citation valid="true">
<authors>
<author>Roman Yangarber</author>
<author>Ralph Grishman</author>
<author>Pasi Tapanainen</author>
<author>Silja Huttunen</author>
</authors>
<title>Automatic acquisition of domain knowledge for information extraction.</title>
<date>2000</date>
<booktitle>In 18th Internation Conference on Computational Linguistics (COLING</booktitle>
<pages>940--946</pages>
<contexts>
<context position="1696" citStr="Yangarber et al., 2000" startWordPosition="241" endWordPosition="244">t excerpts. The event extraction task is related to several subtasks: event mention detection, candidate rolefiller extraction, relation extraction and event template filling. The problem we address here is the detection of role-filler candidates and their association with specific roles in event templates. For this task, IE systems adopt various ways of extracting patterns or generating rules based on the surrounding context, local context and global context (Patwardhan and Riloff, 2009). Current approaches for learning such patterns include bootstrapping techniques (Huang and Riloff, 2012a; Yangarber et al., 2000), weakly supervised learning algorithms (Huang and Riloff, 2011; Sudo et al., 2003; Surdeanu et al., 2006), fully supervised learning approaches (Chieu et al., 2003; Freitag, 1998; Bunescu and Mooney, 2004; Patwardhan and Riloff, 2009) and other variations. All these methods rely on substantial amounts of manually annotated corpora and use a large body of linguistic knowledge. The performance of these approaches is related to the amount of knowledge engineering deployed and a good choice of features and classifiers. Furthermore, the efficiency of the system relies on the a priori knowledge of </context>
</contexts>
<marker>Yangarber, Grishman, Tapanainen, Huttunen, 2000</marker>
<rawString>Roman Yangarber, Ralph Grishman, Pasi Tapanainen, and Silja Huttunen. 2000. Automatic acquisition of domain knowledge for information extraction. In 18th Internation Conference on Computational Linguistics (COLING 2000), pages 940–946.</rawString>
</citation>
<citation valid="true">
<authors>
<author>Mo Yu</author>
<author>Mark Dredze</author>
</authors>
<title>Improving lexical embeddings with semantic knowledge.</title>
<date>2014</date>
<booktitle>In 52nd Annual Meeting of the Association for Computational Linguistics (ACL 2014), Short Papers,</booktitle>
<pages>545--550</pages>
<location>Baltimore, Maryland,</location>
<marker>Yu, Dredze, 2014</marker>
<rawString>Mo Yu and Mark Dredze. 2014. Improving lexical embeddings with semantic knowledge. In 52nd Annual Meeting of the Association for Computational Linguistics (ACL 2014), Short Papers, pages 545–550, Baltimore, Maryland, June.</rawString>
</citation>
</citationList>
</algorithm>
</algorithms>
<?xml version="1.0" encoding="UTF-8"?>
<algorithms version="110505">
<algorithm name="SectLabel" version="110505">
<variant no="0" confidence="0.000027">
<title confidence="0.946466">
Chinese Zero Pronoun Resolution: An Unsupervised Probabilistic
Model Rivaling Supervised Resolvers
</title>
<author confidence="0.975414">
Chen Chen and Vincent Ng
</author>
<affiliation confidence="0.9805395">
Human Language Technology Research Institute
University of Texas at Dallas
</affiliation>
<address confidence="0.897685">
Richardson, TX 75083-0688
</address>
<email confidence="0.9995">
{yzcchen,vince}@hlt.utdallas.edu
</email>
<sectionHeader confidence="0.994815" genericHeader="abstract">
Abstract
</sectionHeader>
<bodyText confidence="0.9999274375">
State-of-the-art Chinese zero pronoun res-
olution systems are supervised, thus re-
lying on training data containing manu-
ally resolved zero pronouns. To elimi-
nate the reliance on annotated data, we
present a generative model for unsuper-
vised Chinese zero pronoun resolution.
At the core of our model is a novel hy-
pothesis: a probabilistic pronoun resolver
trained on overt pronouns in an unsuper-
vised manner can be used to resolve zero
pronouns. Experiments demonstrate that
our unsupervised model rivals its state-of-
the-art supervised counterparts in perfor-
mance when resolving the Chinese zero
pronouns in the OntoNotes corpus.
</bodyText>
<sectionHeader confidence="0.998781" genericHeader="keywords">
1 Introduction
</sectionHeader>
<bodyText confidence="0.995020631578948">
A zero pronoun (ZP) is a gap in a sentence that
is found when a phonetically null form is used to
refer to a real-world entity. An anaphoric zero
pronoun (AZP) is a ZP that corefers with one or
more preceding noun phrases (NPs) in the asso-
ciated text. Below is an example taken from the
Chinese TreeBank (CTB), where the ZP (denoted
as *pro*) refers to 4 VWTr (Russia).
[4 VWTr] 4,�) )K W-A-I—AMjC44,
*pro* a [him.
([Russia] is a consistent supporter of Milošević,
*pro* has proposed to mediate the political crisis.)
As we can see, ZPs lack grammatical attributes
that are useful for overt pronoun resolution such as
NUMBER and GENDER. This makes ZP resolution
more challenging than overt pronoun resolution.
Automatic ZP resolution is typically composed
of two steps. The first step, AZP identification, in-
volves extracting ZPs that are anaphoric. The sec-
ond step, AZP resolution, aims to identify an an-
tecedent of an AZP. State-of-the-art ZP resolvers
have tackled both of these steps in a supervised
manner, training a classifier for AZP identification
and another one for AZP resolution (e.g., Zhao and
Ng (2007), Chen and Ng (2013)).
In this paper, we focus on the second task, AZP
resolution, designing a model that assumes as in-
put the AZPs in a document and resolves each of
them. Note that the task of AZP resolution alone is
by no means easy: even when gold-standard AZPs
are given, state-of-the-art supervised resolvers can
only achieve an F-score of 47.7% for resolving
Chinese AZPs (Chen and Ng, 2013). For the sake
of completeness, we will evaluate our AZP resolu-
tion model using both gold-standard AZPs as well
as AZPs automatically identified by a rule-based
approach that we propose in this paper.
Our contribution lies in the proposal of the first
unsupervised probabilistic model for AZP resolu-
tion that rivals its supervised counterparts in per-
formance when evaluated on the Chinese portion
of the OntoNotes 5.0 corpus. Its main advan-
tage is that it does not require training data with
manually resolved AZPs. This, together with the
fact that its underlying generative process is not
language-dependent, enables it to be applied to
languages where such annotated data is not read-
ily available. At its core is a novel hypothesis:
we can apply a probabilistic pronoun resolution
model trained on overt pronouns in an unsuper-
vised manner to resolve zero pronouns. Moti-
vated by Cherry and Bergsma&apos;s (2005) and Char-
niak and Elsner&apos;s (2009) work on unsupervised
English pronoun resolution, we train our unsu-
pervised resolver on Chinese overt pronouns us-
ing the Expectation-Maximization (EM) algorithm
(Dempster et al., 1977).
</bodyText>
<sectionHeader confidence="0.997914" genericHeader="introduction">
2 Related Work
</sectionHeader>
<bodyText confidence="0.476331">
Chinese ZP resolution. Early approaches to
Chinese ZP resolution are rule-based. Con-
verse (2006) applied Hobbs&apos; algorithm (Hobbs,
</bodyText>
<page confidence="0.959072">
763
</page>
<note confidence="0.9119715">
Proceedings of the 2014 Conference on Empirical Methods in Natural Language Processing (EMNLP), pages 763–774,
October 25-29, 2014, Doha, Qatar. c�2014 Association for Computational Linguistics
</note>
<bodyText confidence="0.994208727272727">
1978) to resolve the ZPs in the CTB documents.
Yeh and Chen (2007) hand-engineered a set of
rules for ZP resolution based on Centering The-
ory (Grosz et al., 1995).
In contrast, virtually all recent approaches to
this task are based on supervised learning. Zhao
and Ng (2007) are the first to employ a supervised
learning approach to Chinese ZP resolution. They
trained an AZP resolver by employing syntactic
and positional features in combination with a de-
cision tree learner. Unlike Zhao and Ng, Kong
and Zhou (2010) employed context-sensitive con-
volution tree kernels (Zhou et al., 2008) in their
resolver to model syntactic information. More re-
cently, we extended Zhao and Ng&apos;s feature set with
novel features that encode the context surrounding
a ZP and its candidate antecedents, and exploited
the coreference links between ZPs as bridges to
find textually distant antecedents for ZPs (Chen
and Ng, 2013).
ZP resolution for other languages. There have
been rule-based and supervised machine learning
approaches for resolving ZPs in other languages.
For example, to resolve ZPs in Spanish texts,
Ferrández and Peral (2000) proposed a set of hand-
crafted rules that encode preferences for candidate
antecedents. In addition, supervised approaches
have been extensively employed to resolve ZPs
in Korean (e.g., Han (2006)), Japanese (e.g., Seki
et al. (2002), Isozaki and Hirao (2003), Iida et
al. (2006; 2007), Imamura et al. (2009), Iida and
Poesio (2011), Sasano and Kurohashi (2011)), and
Italian (e.g., Iida and Poesio (2011)).
</bodyText>
<sectionHeader confidence="0.993534" genericHeader="method">
3 Chinese Overt Pronouns
</sectionHeader>
<bodyText confidence="0.998459375">
Since our approach relies heavily on Chinese
overt pronouns, in this section we introduce them
by describing their four grammatical attributes,
namely NUMBER, GENDER, PERSON and ANI-
MACY. NUMBER has two values, singular and
plural. GENDER has three values, neuter, mascu-
line and feminine. PERSON has three values, first,
second and third. Finally, ANIMACY has two val-
ues, animate and inanimate.
We exploit ten personal pronouns that have
well-defined grammatical attribute values, namely
你 (singular you), 我 (I), 他 (he), 她 (she), 它 (it),
你们 (plural you), 我们 (we), 他们 (masculine
they), 她们 (feminine they), and 它们 (impersonal
they). As can be seen in Table 1, each of them can
be uniquely identified using these four attributes.
</bodyText>
<table confidence="0.995356545454546">
Pronouns NUMBER GENDER PERSON ANIMACY
我 (I) singular neuter first animate
你 (you) singular neuter second animate
他 (he) singular masculine third animate
她 (she) singular feminine third animate
它 (it) singular neuter third inanimate
你们 (you) plural neuter second animate
我们 (we) plural neuter first animate
他们 (they) plural masculine third animate
她们 (they) plural feminine third animate
它们 (they) plural neuter third inanimate
</table>
<tableCaption confidence="0.999831">
Table 1: Attribute values of Chinese overt pronouns.
</tableCaption>
<sectionHeader confidence="0.981653" genericHeader="method">
4 The Generative Model
</sectionHeader>
<subsectionHeader confidence="0.950623">
4.1 Notation
</subsectionHeader>
<bodyText confidence="0.999577533333333">
Let p be an overt pronoun in PR, the set of the
10 overt pronouns described in Section 3. C, the
set of candidate antecedents of p, contains all and
only those maximal or modifier NPs that precede
p in the associated text and are at most two sen-
tences away from it.1 k is the context surround-
ing p as well as every candidate antecedent c in
C; k, is the context surrounding p and candidate
antecedent c; and l is a binary variable indicat-
ing whether c is the correct antecedent of p. The
set A = {Num, Gen, Per, Ani} has four ele-
ments, which correspond to NUMBER, GENDER,
PERSON and ANIMACY respectively. a is an at-
tribute in A. Finally, pa and ca are the attribute
values of p and c with respect to a respectively.
</bodyText>
<subsectionHeader confidence="0.986564">
4.2 Training
</subsectionHeader>
<bodyText confidence="0.999924941176471">
Our model estimates P(p, k, c, l), the probability
of seeing (1) the overt pronoun p; (2) the context
k surrounding p and its candidate antecedents; (3)
a candidate antecedent c of p; and (4) whether c is
the correct antecedent of p. Since we estimate this
probability from a raw, unannotated corpus, we are
effectively treating p, k, and c as observed data and
l as hidden data.
Owing to the presence of hidden data, we es-
timate the model parameters using the EM algo-
rithm. Specifically, we use EM to iteratively es-
timate the model parameters from data in which
each overt pronoun is labeled with the probability
it corefers with each of its candidate antecedents
and apply the resulting model to re-label each overt
pronoun with the probability it corefers with each
of its candidate antecedents. Below we describe
</bodyText>
<footnote confidence="0.990077">
1Only 8% of the overt pronouns in our corpus, the Chi-
nese portion of the OntoNotes 5.0 corpus, do not have any
antecedent in the preceding two sentences.
</footnote>
<page confidence="0.995839">
764
</page>
<bodyText confidence="0.972306">
the details of the E-step and the M-step.
4.2.1 E-Step
The goal of the E-step is to compute
P(l=1|p, k, c), the probability that a candi-
date antecedent c is the correct antecedent of p
given context k. Assuming that exactly one of the
p&apos;s candidate antecedents is its correct antecedent,
we can rewrite P(l=1|p, k, c) as follows:
</bodyText>
<equation confidence="0.983356">
P(p, k, c, l=1)
P(l=1|p, k, c) =
_ (1)
Ec′∈C P(p, k, c′, l-1)
Applying Chain Rule, we can rewrite
P(p, k, c, l=1) as follows:
P(p, k, c, l=1) = P(p|k, c,l=1) ∗ P(l=1|k, c)
∗ P(c|k) ∗ P(k)
(2)
</equation>
<bodyText confidence="0.9888705">
Next, given l = 1 (i.e., c is the antecedent of p),
we assume that we can generate p from c without
looking at the context.2 Then we represent p using
its grammatical attributes A. We further assume
that p&apos;s value with respect to attribute a ∈ A is
independent of the value of each of its remaining
attributes given the antecedent&apos;s value with respect
to a. So we can rewrite P(p|k, c, l=1) as follows:
</bodyText>
<equation confidence="0.992875">
P(p|k, c, l=1) ≈ P(p|c,l=1)
≈ P(pNum, pGen, pPer, pAni|c, l=1) (3)
ri≈ P(pa|ca, l=1)
a∈A
</equation>
<bodyText confidence="0.9997386">
Moreover, we assume that (1) given p and c&apos;s
context, the probability of c being the antecedent
of p is not affected by the context of the other can-
didate antecedents; and (2) kc is sufficient for de-
termining whether c is the antecedent of p. So,
</bodyText>
<equation confidence="0.99251">
P(l=1|k, c) ≈ P(l=1|kc, c) ≈ P(l=1|kc) (4)
</equation>
<bodyText confidence="0.999579333333333">
Furthermore, we assume that given context k,
each candidate antecedent of p is generated with
equal probability. In other words,
</bodyText>
<equation confidence="0.984470666666667">
P(c|k) ≈ P(c′|k) ∀ c, c′ ∈ C (5)
Given Equations (2), (3), (4) and (5), we can
rewrite P(l=1|p, k, c) as:
P(l=1|p, k, c) = P(p, c,l=1)
Ec′∈C (p, k, c , l=1)
Ha∈A P(pa|ca,l=1) ∗ P(l=1|kc)
≈ E Ha∈A P(pa|c′a, l=1) ∗ P(l=1|kc′)
c′∈C
(6)
</equation>
<footnote confidence="0.871783">
2This assumption is reasonable because it is fairly easy to
determine which pronoun can be used to refer to a given NP.
</footnote>
<bodyText confidence="0.999404193548387">
As we can see from Equation (6), our model has
two groups of parameters, namely P(pa|ca, l=1)
and P(l=1|kc). Since we have four grammatical
attributes, P(pa|ca, l=1) contains four sets of pa-
rameters, with one set per attribute. Using Equa-
tion (6) and the current parameter estimates, we
can compute P(l=1|p, k, c).
Two points deserve mention before we describe
the M-step. First, we estimate P(l=1|p, k, c) from
all and only those overt pronouns p ∈ PR that
are surface or deep subjects in their correspond-
ing sentences. This condition is motivated by our
observation that 99.56% of the ZPs in our evalu-
ation corpus (i.e., OntoNotes 5.0) are surface or
deep subjects. In other words, we impose this con-
dition so that we can focus our efforts on learn-
ing a model for resolving overt pronouns that are
subjects. This is by no means a limitation of our
model: if we were given a corpus in which many
ZPs occur as grammatical objects, we could sim-
ilarly train another model on overt objects. Sec-
ond, since in the E-step we attempt to probabilisti-
cally label every overt pronoun p that satisfies the
condition above, our model is effectively making
the simplifying assumption that every overt pro-
noun is anaphoric. This is clearly an overly sim-
plistic assumption. One way to relax this assump-
tion, which we leave as future work, is to first iden-
tify those pronouns that are anaphoric and then use
EM to estimate the joint probability only from the
anaphoric pronouns.
</bodyText>
<subsectionHeader confidence="0.833222">
4.2.2 M-Step
</subsectionHeader>
<bodyText confidence="0.9768162">
Given P(l=1|p, k, c), the goal of the M-step is to
(re)estimate the model parameters, P(pa|ca, l=1)
and P(l=1|kc), using maximum likelihood esti-
mation. Specifically, P(pa|ca, l=1) is estimated
as follows:
</bodyText>
<equation confidence="0.9996455">
Count(pa, ca, l=1) + θ
P(pa|ca, l=1) = Count(ca, l=1) + θ ∗ |a |(7)
</equation>
<bodyText confidence="0.918416375">
where Count(ca, l=1) is the expected number of
times c has attribute value ca when it is the an-
tecedent of p; |a |is the number of possible values
of attribute a; θ is the Laplace smoothing param-
eter, which we set to 1; and Count(pa, ca, l=1)
is the expected number of times p has attribute
value pa when its antecedent c has attribute value
ca. Given attribute values p′a and c′a, we compute
</bodyText>
<page confidence="0.918874">
765
</page>
<equation confidence="0.965335285714286">
Count(p′a, c′a, l=1) as follows: P(l=1|p, k, c)
Count(p′a, c′a, l=1) = �
Ac:Pa=Pa,ca=c′a
(8)
Similarly, P(l=1|kc) is estimated as follows:
Count(kc, l=1) + θ
P (l=1|kc) = Count(kc) + θ ∗ 2 (9)
</equation>
<bodyText confidence="0.958201333333333">
where Count(kc) is the number of times kc ap-
pears in the training data, and Count(kc, l=1) is
the expected number of times kc is the context sur-
rounding a pronoun and its antecedent c. Given
context k′c, we compute Count(k′c, l=1) as fol-
lows:
</bodyText>
<equation confidence="0.9564495">
Count(k′c, l=1) = � P(l=1|p, k, c) (10)
k:kc=k′c
</equation>
<bodyText confidence="0.9864215">
To start the induction process, we initialize all
parameters with uniform values. Specifically,
P(pa|ca, l=1) is set to 1
|a|, and P(l=1|kc) is set
to 0.5. Then we iteratively run the E-step and the
M-step until convergence.
There are two important questions we have not
addressed. First, how can we compute the four at-
tribute values of a candidate antecedent (i.e., ca
for each attribute a), which we need to estimate
P(pa|ca, l=1)? Second, what features should we
use to represent context kc, which we need to esti-
mate P(l=1|kc)? We defer the discussion of these
questions to Sections 5 and 6.
</bodyText>
<subsectionHeader confidence="0.950839">
4.3 Inference
</subsectionHeader>
<bodyText confidence="0.999904666666667">
After training, we can apply the resulting model to
resolve AZPs. Given an AZP z, we determine its
antecedent as follows:
</bodyText>
<equation confidence="0.9063705">
(�c, �p) =arg max P(l=1|p, k, c) (11)
cEC, PEPR
</equation>
<bodyText confidence="0.999995023255814">
where PR is our set of 10 Chinese overt pronouns
and C is the set of candidate antecedents of z. In
other words, we apply Formula (11) to each AZP z,
searching for the candidate antecedent c and overt
pronoun p that maximize P(l=1|p, k, c) when p is
used to fill the ZP gap left behind by z. The c that
results in the maximum probability value over all
overt pronouns in PR is chosen as the antecedent
of z. In essence, since the model is trained on
overt pronouns but is applied to ZPs, we have to
exhaustively fill the ZP&apos;s gap under consideration
with each of the 10 overt pronouns in PR during
inference.
Although we can now apply our generative
model to resolve AZPs, the resolution procedure
can be improved further. The improvement is
motivated by a problem we observed previously
(Chen and Ng, 2013): an AZP and its closest an-
tecedent can sometimes be far away from each
other, thus making it difficult to correctly resolve
the AZP. To address this problem, we employ the
following resolution procedure in our experiments.
Given a test document, we process its AZPs in a
left-to-right manner. As soon as we resolve an
AZP to a preceding NP c, we fill the correspond-
ing AZP&apos;s gap with c. Hence, when we process
an AZP z, all of its preceding AZPs in the associ-
ated text have been resolved, with their gaps filled
by the NPs they are resolved to. To resolve z, we
create test instances between z and its candidate
antecedents in the same way as described before.
The only difference is that the set of candidate an-
tecedents of z may now include those NPs that are
used to fill the gaps of the AZPs resolved so far. In
other words, this incremental resolution procedure
may increase the number of candidate antecedents
of each AZP z. Some of these additional candidate
antecedents are closer to z than the original candi-
date antecedents, thereby facilitating the resolution
of z. If the model resolves z to the additional can-
didate antecedent that fills the gap left behind by,
say, AZP z′, we postprocess the output by resolv-
ing z to the NP that z′ is resolved to.3
</bodyText>
<sectionHeader confidence="0.829208" genericHeader="method">
5 Attributes of Candidate Antecedents
</sectionHeader>
<bodyText confidence="0.9991802">
In this section, we describe how we determine
the four grammatical attribute values (NUMBER,
GENDER, PERSON and ANIMACY) of a candidate
antecedent c, as they are used to represent c when
estimating P(pa|ca, l=1) for each attribute a.
</bodyText>
<subsectionHeader confidence="0.926377">
5.1 ANIMACY
</subsectionHeader>
<bodyText confidence="0.999926375">
We determine the ANIMACY of a candidate an-
tecedent c heuristically. Specifically, we first
check the NP type of c. If c is a pronoun, we look
up its ANIMACY in Table 1. If c is a named en-
tity, there are two cases to consider: if c is a per-
son4, we label it as animate; otherwise, we label it
as inanimate. If c is a common noun, we look up
the ANIMACY of its head noun in an automatically
</bodyText>
<footnote confidence="0.99534325">
3This postprocessing step is needed because the additional
candidate antecedents are only gap fillers.
4A detailed description of our named entity recognizer can
be found in Chen and Ng (2014).
</footnote>
<page confidence="0.996567">
766
</page>
<bodyText confidence="0.999907227272727">
constructed word list WL. If the head noun is not
in WL, we set its ANIMACY to unknown.
Our method for constructing WL is motivated
by an observation of measure words in Chinese:
some of them only modify inanimate nouns while
others only modify animate nouns. For example,
the nouns modified by the measure word t are al-
ways inanimate, as in —tom (one piece of paper).
On the other hand, the nouns modified by the mea-
sure word are always animate, as in — Z,k
(one worker).
Given this observation, we first define two lists,
Mani and Minani. Mani is a list of measure words
that can only modify animate nouns. Minani is a
list of measure words that can only modify inan-
imate nouns.5 There exists a special measure
word in Chinese, -&apos;I`, which can be used to mod-
ify most of the common nouns regardless of their
ANIMACY. As a result, we remove -&apos;I` from both
lists. After constructing Mani and Minani, we (1)
parse the Chinese Gigaword corpus (Parker et al.,
2009), which contains 4,370,600 documents, using
an efficient dependency parser, ctbparser6 (Qian et
al., 2010), and then (2) collect all pairs of words
(m, n), where m is a measure word, n is a com-
mon noun, and there is a NMOD dependency re-
lation between m and n. Finally, we determine
the ANIMACY of a given common noun n as fol-
lows. First, we retrieve all of the pairs contain-
ing n. Then, we sum over all occurrences of m
in Mani (call the sum Cani), as well as all occur-
rences of m in Minani (call the sum Cinani). If
Cani &gt; Cinani, we label this common noun as an-
imate; otherwise, we label it as inanimate.
Table 2 shows the learned values of
P(pAni|cAni, l=1). These results are consis-
tent with our intuition: an animate (inanimate)
pronoun is more likely to be generated from
an animate (inanimate) antecedent than from an
inanimate (animate) antecedent. Note that animate
pronouns are more likely to be generated than
inanimate pronouns regardless of the antecedent&apos;s
ANIMACY. This can be attributed to the fact that
94.6% of the pronouns in our corpus are animate.
</bodyText>
<subsectionHeader confidence="0.951815">
5.2 GENDER
</subsectionHeader>
<bodyText confidence="0.999704333333333">
We determine the GENDER of a candidate an-
tecedent c as follows. If c is a pronoun, we look up
its GENDER in Table 1. Otherwise, we determine
</bodyText>
<footnote confidence="0.999959">
5We create these two lists with the help of this page:
http://chinesenotes.com/ref_measure_words.htm
6http://code.google.com/p/ctbparser/
</footnote>
<table confidence="0.99971">
_tecedent Pronoun animate inanimate
An
animate 0.999 0.001
inanimate 0.858 0.142
unknown 0.945 0.055
</table>
<tableCaption confidence="0.994105">
Table 2: Learned values of P(pAni|cAni, l=1).
</tableCaption>
<bodyText confidence="0.999725">
its GENDER based on its ANIMACY. Specifically,
if c is inanimate, we set its GENDER to neuter.
Otherwise, we determine its gender by looking up
a gender word list constructed by Bergsma and
Lin&apos;s (2006) approach. If the word is not in the
list, we set its GENDER to masculine by default.
Next, we describe how the aforementioned gen-
der word list is constructed. Following Bergsma
and Lin (2006), we define a dependency path as the
sequence of non-terminal nodes and dependency
labels between two potentially coreferent entities
in a dependency parse tree. From the parsed Chi-
nese Gigaword corpus, we first collect every de-
pendency path that connects two pronouns. For
each path P collected, we compute CL(P), the
coreference likelihood of P, as follows:
</bodyText>
<equation confidence="0.9043685">
NI(P )
CL(P) = NI(P ) + ND(P) (12)
</equation>
<bodyText confidence="0.999914291666667">
where NI(P) is the number of times P connects
two identical pronouns, and ND(P) is the number
of times it connects two different pronouns. As-
suming that two identical pronouns in a sentence
are coreferent (Bergsma and Lin, 2006), we can
see that the larger a path&apos;s CL value is, the more
likely it is that the two NPs it connects are corefer-
ent. To ensure that we have dependency paths that
are strongly indicative of coreference relations, we
consider a dependency path P a coreferent path if
and only if CL(P) &gt; 0.8.
Given these coreferent paths, we can compute
the GENDER of a noun n as follows. First, we com-
pute (1) Nm(n), the number of coreferent paths
connecting n with a masculine pronoun; and (2)
NF(n), the number of coreferent paths connect-
ing n with a feminine pronoun. Then, if NF (n) &gt;
Nm(n), we set n&apos;s gender to feminine; otherwise,
we set it to masculine.
Table 3 shows the learned values of
P(pGen|cGen,l=1). These results are con-
sistent with our intuition: a pronoun is a lot more
likely to be generated from an antecedent with the
same GENDER than one with a different GENDER.
</bodyText>
<page confidence="0.971489">
767
</page>
<table confidence="0.999914">
Pronoun neuter feminine masculine
����������
Antecedent
neuter 0.864 0.018 0.117
feminine 0.065 0.930 0.005
masculine 0.130 0.041 0.828
</table>
<tableCaption confidence="0.891163">
Table 3: Learned values of P(pGen|cGen, l=1).
</tableCaption>
<table confidence="0.951656526315789">
_tecedent Pronoun first second third
An
first 0.856 0.119 0.025
second 0.219 0.766 0.016
third 0.289 0.077 0.634
Table 5: Learned values of P(pPer|cPer, l=1)
(same speaker).
Pronoun singular plural
����������
Antecedent
singular 0.861 0.139
plural 0.26 0.74
Table 4: Learned values of P(pNum|cNum, l=1).
Pronoun first second third
����������
Antecedent
first 0.417 0.525 0.057
second 0.75 0.23 0.02
third 0.437 0.229 0.334
</table>
<subsectionHeader confidence="0.878775">
5.3 NUMBER
</subsectionHeader>
<bodyText confidence="0.999888421052632">
When computing the NUMBER of a candidate an-
tecedent in English, Charniak and Elsner (2009)
rely on part-of-speech information. For example,
NN and NNP denote singular nouns, whereas NNS
and NNPS denote plural nouns. However, Chi-
nese part-of-speech tags do not provide such in-
formation. Hence, we need a different method for
finding the NUMBER of a candidate antecedent c in
Chinese. If c is a pronoun, we look up its NUMBER
in Table 1. If c is a named entity, its NUMBER is
singular. If c is a common noun, we infer its NUM-
BER from its string: if the string ends with 417 or is
modified by a quantity word (e.g., —4b-, i&apos;F_-��), c
is plural; otherwise, c is singular.
Table 4 shows the learned values of
P(pNum|cNum, l=1). These results are con-
sistent with our intuition: a pronoun is more likely
to be generated from an antecedent with the same
NUMBER than one with a different NUMBER.
</bodyText>
<subsectionHeader confidence="0.76244">
5.4 PERSON
</subsectionHeader>
<bodyText confidence="0.999918">
Finally, we compute the PERSON of a candi-
date antecedent c. Similar to Charniak and El-
sner (2009), we set A (I) and �&apos;c417 (we) to first
person, fj&apos;. (singular you) and fj&apos;.417 (plural you)
to second person, and everything else to third
person. We estimate two sets of probabilities
P(pPer|cPer, l=1), one where p and c are from the
same speaker, and the other where they are from
different speakers.7 This is based on our observa-
tion that P (pPer|cPer, l=1) could be very different
in these two cases.
</bodyText>
<footnote confidence="0.9863254">
7We employ a simple heuristic to identify the speaker of
NPs occurring in direct speech: we assume that the speaker
is the subject of the speech&apos;s reporting verb. So for example,
we identify Jack as the speaker of This book in the sentence
&amp;quot;This book is good,&amp;quot; Jack said.
</footnote>
<tableCaption confidence="0.915533">
Table 6: Learned values of P(pPer|cPer,l=1)
</tableCaption>
<bodyText confidence="0.954674666666667">
(different speakers).
Tables 5 and 6 show the learned values of these
two sets of probabilities. These results are consis-
tent with our intuition. In the same-speaker case, a
pronoun is a lot more likely to be generated from
an antecedent with the same speaker than one with
a different speaker. In the different-speaker case,
a first (second) person pronoun is most likely to be
generated from a second (first) person pronoun.
</bodyText>
<sectionHeader confidence="0.985642" genericHeader="method">
6 Context Features
</sectionHeader>
<bodyText confidence="0.999848909090909">
To fully specify our model, we need to describe
how to represent ke, which is needed to compute
P(l=1|ke). Recall that ke encodes the context sur-
rounding candidate antecedent c and the associated
pronoun p. As described below, we represent ke
using eight features, some of which are motivated
by previous work on supervised AZP resolution
(e.g., Zhao and Ng (2007), Chen and Ng (2013)).
Note that (1) all but feature 1 are computed based
on syntactic parse trees, and (2) features 2, 3, 6,
and 8 are ternary-valued features.
</bodyText>
<listItem confidence="0.999732636363636">
1. the sentence distance between c and p;
2. whether the node spanning c has an ancestor
NP node; if so, whether this NP node is a de-
scendant of c&apos;s lowest ancestor IP node;
3. whether the node spanning c has an ancestor
VP node; if so, whether this VP node is a de-
scendant of c&apos;s lowest ancestor IP node;
4. whether vp has an ancestor NP node, where
vp is the VP node spanning the VP that fol-
lows p;
5. whether vp has an ancestor VP node;
</listItem>
<page confidence="0.989337">
768
</page>
<table confidence="0.999730333333333">
Training Test
Documents 1,391 172
Sentences 36,487 6,083
Words 756,063 110,034
Overt Subject Pronouns 13,418 −
AZPs − 1,713
</table>
<tableCaption confidence="0.997952">
Table 7: Statistics on the training and test sets.
</tableCaption>
<listItem confidence="0.833661666666667">
6. whether p is the first word of a sentence; if
not, whether p is the first word of an IP clause;
7. whether c is a subject whose governing verb
is lexically identical to the verb governing p;
8. whether c is the closest candidate antecedent
with subject grammatical role and is seman-
tically compatible with p&apos;s governing verb; if
not, whether c is the first semantically com-
patible candidate antecedent8.
</listItem>
<bodyText confidence="0.999932909090909">
Our approach to determine semantic compatibil-
ity (in feature 8) resembles Kehler et al.&apos;s (2004)
and Yang et al.&apos;s (2005) methods for computing se-
lectional preferences. Specifically, for each verb
and each noun that serves as a subject in Chinese
Gigaword, we compute their mutual information
(MI). Now, given a pronoun p and a candidate an-
tecedent c in the training/test corpus, we retrieve
the MI value of c and p&apos;s governing verb. We then
consider them semantically compatible if and only
if their MI value is greater than zero.
</bodyText>
<sectionHeader confidence="0.999434" genericHeader="method">
7 Evaluation
</sectionHeader>
<subsectionHeader confidence="0.995552">
7.1 Experimental Setup
</subsectionHeader>
<bodyText confidence="0.999606333333333">
Datasets. We employ the Chinese portion of the
OntoNotes 5.0 corpus that was used in the official
CoNLL-2012 shared task (Pradhan et al., 2012).
In the CoNLL-2012 data, the training set and de-
velopment set contain ZP coreference annotations,
but the test set does not. Therefore, we train our
models on the training set and perform evaluation
on the development set. Statistics on the datasets
are shown in Table 7. The documents in these
datasets come from six sources, namely Broadcast
News (BN), Newswire (NW), Broadcast Conver-
sation (BC), Telephone Conversation (TC), Web
Blog (WB) and Magazine (MZ).
8 We sort the candidate antecedents of p as follows. We
first consider the subject candidate antecedents in the same
sentence as p from right to left, then the other candidate an-
tecedents in the same sentence from right to left. Next, we
consider the candidate antecedents in the previous sentence,
also preferring candidates that are subjects, but in left-to-right
order. Finally, we consider the candidate antecedents two
sentences back, following the subject-first, left-to-right order.
Evaluation measures. We express the results of
ZP resolution in terms of recall (R), precision (P)
and F-score (F).
Evaluation settings. Following Chen and Ng
(2013), we evaluate our model in three settings.
In Setting 1, we assume the availability of gold
syntactic parse trees and gold AZPs. In Setting 2,
we employ gold syntactic parse trees and system
(i.e., automatically identified) AZPs. Finally, in
Setting 3, we employ system syntactic parse trees
and system AZPs. The gold and system syntactic
parse trees, as well as the gold AZPs, are obtained
from the CoNLL-2012 shared task dataset, while
the system AZPs are identified by the rule-based
approach described in the Appendix.9 Since our
AZP identification approach does not rely on any
labeled data, we are effectively evaluating an end-
to-end unsupervised AZP resolver in Setting 3.
</bodyText>
<sectionHeader confidence="0.606926" genericHeader="evaluation">
7.2 Results
</sectionHeader>
<bodyText confidence="0.999925884615385">
Baseline systems. We employ seven resolvers
as baseline systems. To gauge the difficulty of
the task, we employ four simple rule-based re-
solvers, which resolve an AZP z to (1) the can-
didate antecedent closest to z (Baseline 1); (2) the
subject NP closest to z (Baseline 2); (3) the clos-
est candidate antecedent that is semantically com-
patible with z (Baseline 3); and (4) the first can-
didate antecedent that is semantically compatible
with z, where the candidate antecedents are vis-
ited according to the order described in Footnote 8
(Baseline 4). These four baselines allow us to
study the role of (1) recency, (2) salience, (3) re-
cency combined with semantic compatibility, and
(4) salience combined with semantic compatibil-
ity in AZP resolution respectively. The remaining
three baselines are state-of-the-art supervised AZP
resolvers, which include our own resolver (Chen
and Ng, 2013) as well as our re-implementations
of Zhao and Ng&apos;s (2007) resolver and Kong and
Zhou&apos;s (2010) resolver.
The test set results of these seven baseline re-
solvers when evaluated under the three afore-
mentioned evaluation settings are shown in Ta-
ble 8. The system AZPs employed by the rule-
based resolvers are obtained using our rule-based
</bodyText>
<footnote confidence="0.800949714285714">
9One may wonder why we do not train a supervised sys-
tem for identifying AZPs and instead experiment with a rule-
based AZP identification system. The reason is that employ-
ing labeled data defeats the whole purpose of having an unsu-
pervised AZP resolution model: if annotated data is available
for training an AZP identification system, the same data can
be used to train an AZP resolution system.
</footnote>
<page confidence="0.985987">
769
</page>
<table confidence="0.995632272727273">
Setting 1: Setting 2: Setting 3:
Gold Parses, Gold Parses, System Parses,
Gold AZPs System AZPs System AZPs
Baseline R P F R P F R P F
Selecting closest candidate antecedent 25.0 25.2 25.1 18.3 10.8 13.6 10.3 6.7 8.1
Selecting closest subject 42.0 43.6 42.8 31.8 19.2 23.9 18.0 11.9 14.4
Selecting closest semantically compatible candidate antecedent 28.5 28.8 28.7 20.5 12.2 15.3 11.7 7.6 9.2
Selecting first semantically compatible candidate antecedent 45.2 45.7 45.5 33.6 20.0 25.1 18.9 12.3 14.9
Zhao and Ng (2007) 41.5 41.5 41.5 22.4 24.4 23.3 12.7 14.2 13.4
Kong and Zhou (2010) 44.9 44.9 44.9 33.0 19.3 24.4 18.7 11.9 14.5
Chen and Ng (2013) 47.7 47.7 47.7 25.3 27.6 26.4 14.9 16.7 15.7
</table>
<tableCaption confidence="0.939833">
Table 8: AZP resolution results of the baseline systems on the test set.
</tableCaption>
<table confidence="0.9983163">
Setting 1: Gold Parses, Gold AZPs Setting 2: Gold Parses, System AZPs Setting 3: System Parses, System AZPs
Best Baseline Our Model Best Baseline Our Model Best Baseline Our Model
Source R P F R P F R P F R P F R P F R P F
Overall 47.7 47.7 47.7 47.5 47.9 47.7 25.3 27.6 26.4 35.4 21.0 26.4 14.9 16.7 15.7 19.9 12.9 15.7
NW 38.1 38.1 38.1 41.7 41.7 41.7 15.5 21.7 18.1 29.8 24.8 27.0 6.0 12.2 8.0 11.9 13.0 12.4
MZ 34.6 34.6 34.6 34.0 34.2 34.1 18.5 19.6 19.0 24.1 14.5 18.1 6.2 9.4 7.5 6.2 5.2 5.7
WB 46.1 46.1 46.1 47.9 47.9 47.9 21.8 22.0 21.8 37.3 18.7 24.9 8.5 11.4 9.7 19.0 11.3 14.2
BN 47.2 47.2 47.2 52.8 52.8 52.8 21.8 33.2 26.3 31.5 28.1 29.7 14.6 26.3 18.8 18.2 19.5 18.8
BC 52.7 52.7 52.7 49.8 50.3 50.0 23.3 30.7 26.5 38.0 21.0 27.0 12.7 16.2 14.3 20.6 12.4 15.5
TC 51.2 51.2 51.2 45.2 46.7 46.0 43.1 28.2 34.1 42.4 20.3 27.4 33.2 17.1 22.5 32.2 13.3 18.8
</table>
<tableCaption confidence="0.999088">
Table 9: AZP resolution results of the best baseline and our unsupervised model on the test set.
</tableCaption>
<bodyText confidence="0.999969181818182">
AZP identification system. On the other hand,
since our supervised resolvers are meant to be re-
implementations of existing resolvers, we follow
previous work and let them employ a supervised
AZP identification system. In particular, we em-
ploy the one described in Chen and Ng (2013).
Several observations can be made about these
results. First, among the rule-based resolvers,
Baseline 4 achieves the best performance, outper-
forming Baselines 1, 2, and 3 by 12.9%, 1.5%,
and 10.8% in F-score respectively when averaged
over the three evaluation settings. From their
relative performance, which remains the same in
the three settings, we can conclude that as far as
AZP resolution is concerned, (1) salience plays a
greater role than recency; and (2) semantic com-
patibility is useful. Second, among the super-
vised baselines, our supervised resolver (Chen and
Ng, 2013) achieves the best performance, outper-
forming Zhao and Ng&apos;s resolver and Kong and
Zhou&apos;s resolver by 3.9% and 2.0% in F-score re-
spectively when averaged over the three evalua-
tion settings. Finally, comparing the rule-based
resolvers and the learning-based resolvers, we can
see that the best rule-based baseline (Baseline 4)
performs even better than Zhao and Ng&apos;s resolver
and Kong and Zhou&apos;s resolver.
In the rest of this subsection, we will compare
our unsupervised model against the best baseline,
Chen and Ng&apos;s (2013) supervised resolver.
Our model. Results of the best baseline and our
model on the entire test set and each of the six
sources are shown in Table 9. As we can see, our
model achieves the same overall F-score as the best
baseline under all three settings, despite the fact
that it is unsupervised. In fact, our model even out-
performs the best baseline on NW, WB and BN in
Setting 1, NW, WB, BN and BC in Setting 2, and
NW, WB and BC in Setting 3.
It is worth mentioning that while the two re-
solvers achieved the same overall performance,
their outputs differ a lot from each other. Specifi-
cally, the two models only agree on the antecedents
of 55% of the AZPs in Setting 1.10
</bodyText>
<subsectionHeader confidence="0.999433">
7.3 Ablation Experiments
</subsectionHeader>
<bodyText confidence="0.988452444444444">
Impact of P(pa|ca, l=1) and P(l=1|k,). Re-
call that our model is composed of five probability
terms, P (pa|ca,l=1) for each of the four grammat-
ical attributes and P(l=1|k,), the context proba-
bility. To investigate the contribution of context
and each attribute to overall performance, we con-
duct ablation experiments. Specifically, in each
ablation experiment, we remove exactly one prob-
ability term from the model and retrain it.
</bodyText>
<footnote confidence="0.9831898">
10Note that it is difficult to directly compare the outputs
produced under Settings 2 and 3: the AZPs identified by the
best baseline are quite different from those identified by our
rule-based system, as can be inferred from the AZP identifi-
cation results in Table 12.
</footnote>
<page confidence="0.966142">
770
</page>
<table confidence="0.999904375">
System Setting 1 Setting 3 F
R P F R P
Full model 47.5 47.9 47.7 19.9 12.9 15.7
− NUMBER 47.5 47.9 47.7 19.7 12.8 15.5
− GENDER 44.5 45.0 44.7 19.2 12.5 15.1
− PERSON 45.2 45.6 45.4 19.1 12.4 15.1
− ANIMACY 45.1 45.5 45.3 19.1 12.4 15.1
− Context Features 32.9 33.1 33.0 15.2 9.8 11.9
</table>
<tableCaption confidence="0.999548">
Table 10: Probability term ablation results.
</tableCaption>
<bodyText confidence="0.999622636363636">
Ablation results under Settings 1 and 3 are
shown in Table 10. As we can see, under Set-
ting 1, after NUMBER is ablated, performance does
not drop. We attribute this to the fact that al-
most all candidate antecedents are singular. On the
other hand, when we ablate any of the remaining
three attributes, performance drops significantly
by 2.3−3.0% in overall F-score.11 Similar trends
can be observed with respect to Setting 3: after
NUMBER is ablated, performance only decreases
by 0.2%, while ablating any of the other three at-
tributes results in a drop of 0.6%.
Results after ablating context are shown in the
last row of Table 10. As we can see, the F-score
drops significantly by 14.7% and 3.8% under Set-
tings 1 and 3 respectively. These results illustrate
the importance of context features in our model.
Context feature ablation. Recall that we em-
ployed eight context features to encode the rela-
tionship between a pronoun and a candidate an-
tecedent. To determine the relative contribution
of these eight features to overall performance,
we conduct ablation experiments under Settings 1
and 3. In these ablation experiments, all four gram-
matical attributes are retained in the model.
Ablation results are shown in rows 2−9 of Ta-
ble 11. To facilitate comparison, the F-score of the
model in which all eight context features are used
is shown in row 1. As we can see, feature 8 (the
rule-based feature) is the most useful feature: its
removal causes the F-scores of our resolver to drop
significantly by 6.4% under Setting 1 and 1.5% un-
der Setting 3.
</bodyText>
<subsectionHeader confidence="0.961891">
7.4 Error Analysis
</subsectionHeader>
<bodyText confidence="0.9919605">
To gain additional insights into our full model, we
examine its major sources of error below. To focus
on errors attributable to AZP resolution, we ana-
lyze our full model under Setting 1.
Specifically, we randomly select 100 AZPs that
our model incorrectly resolves under Setting 1.
</bodyText>
<table confidence="0.954102166666667">
11All significance tests are paired t-tests, with p &lt; 0.05.
System Setting 1 Setting 3 F
R P F R P
Full model 47.5 47.9 47.7 19.9 12.9 15.7
− Feature 1 46.1 46.5 46.3 19.4 12.6 15.3
− Feature 2 46.5 46.9 46.7 19.4 12.6 15.3
− Feature 3 45.3 45.7 45.5 19.1 12.4 15.1
− Feature 4 47.4 47.8 47.6 20.1 13.0 15.8
− Feature 5 47.4 47.8 47.6 19.7 12.8 15.5
− Feature 6 47.1 47.5 47.3 19.6 12.7 15.4
− Feature 7 47.1 47.5 47.3 20.1 13.0 15.8
− Feature 8 41.2 41.6 41.4 18.0 11.8 14.2
</table>
<tableCaption confidence="0.9997">
Table 11: Context feature ablation results.
</tableCaption>
<bodyText confidence="0.999379722222222">
We found that 17 errors are attributable to dis-
course disfluency, lack of background knowledge
and subject detection, while the remaining 83 er-
rors can be divided into three types:
Failure to recognize the topics of a document.
Our model incorrectly resolves 32 AZPs that are
coreferent with NPs corresponding to the topics of
the associated documents. Consider the following
example:
[八里乡] 位于台北盆地西北端。行政区隶属于
台北县,*pro* 为台北县廿九个乡镇市之一。
([Bali Town] is located in the Northwest of Taipei
Basin. Its administrative area is affiliated with
Taipei County, *pro* is one of Taipei County&apos;s 29
towns and cities.)12
The model incorrectly resolves the AZP *pro*
to 行政区 (Its administrative area). The reason is
that the correct antecedent, 八里乡 (Bali Town),
is far from *pro*: there are five candidate an-
tecedents between *pro* and 八里乡 (Bali Town).
Note, however, that it is easy for a human to re-
solve *pro* to 八里乡 (Bali Town) because the
whole passage is discussing 八里乡 (Bali Town).
Hence, to correctly handle such cases, one may
construct a topic model over the passage and as-
sign each candidate antecedent a prior probability
so that the resulting system favors the selection of
candidates representing the topics as antecedents.
Errors in computing semantic compatibility.
This type of error contributes to 28 of the incor-
rectly resolved AZPs. When computing seman-
tic compatibility in our model, we only consider
the mutual information between a candidate an-
tecedent and the pronoun&apos;s governing verb, but in
some cases, additional context needs to be taken
into account. Consider the following example:
</bodyText>
<footnote confidence="0.969068">
12The pronoun Its in the phrase Its administrative area is
inserted into the English translation for the sake of grammat-
icality and correct understanding of the sentence. The corre-
sponding Chinese phrase does not contain any pronoun.
</footnote>
<page confidence="0.994966">
771
</page>
<bodyText confidence="0.994022023809524">
[一支海军陆战队] 杀死了约 [24 名手无寸铁的
伊拉克人],*pro* 包括妇女和六名儿童。
([Marines] killed about [24 unarmed Iraqis], *pro*
include women and six children.)
There are two candidate antecedents in this ex-
ample, 一支海军陆战队 (Marines) and 24 名手无
寸铁的伊拉克人 (24 unarmed Iraqis), which we
denote as c1 and c2 respectively. The correct an-
tecedent of *pro* is c2 , while our model wrongly
resolves *pro* to c1. Note that both c1 and c2 are
compatible with the AZP&apos;s governing verb 包括
(include). However, if the object of the govern-
ing verb, i.e., 妇女和六名儿童 (women and six
children), were also considered, the model could
determine that c1 is not compatible with the object
while c2 is, and then correctly resolve *pro* to c2.
Failure to recognize and exploit semantically
similar sentences. This type of error contributes
to 23 wrongly resolved AZPs. Recall that an AZP
is omitted for brevity, so the sentence it appears in
often expresses similar meaning to an earlier sen-
tence. However, our model fails to handle such
cases. Consider the following example:
[指挥部和突进的部队] 之间也会失去联络。.....
*pro* 就联络不上了。
([The command and the onrush of troops] lost con-
nection with each other.... *pro* cannot connect
with each other.)
The above example shows two sentences that
are separated by some other sentences. The AZP
under consideration is in the last sentence, while
the first sentence contains the correct antecedent
指挥部和突进的部队 (the command and the on-
rush troops), denoted as c1. Our model fails to re-
solve *pro* to c1, because there are many com-
peting candidate antecedents between c1 and AZP.
However, if our model were aware of the similarity
between the constructions appearing after c1 and
*pro*, i.e., 之间也会失去联络 (lost connection
with each other) and 就联络不上了 (cannot con-
nect with each other), then it might be able to cor-
rectly resolve the AZP.
</bodyText>
<sectionHeader confidence="0.998274" genericHeader="conclusions">
8 Conclusion
</sectionHeader>
<bodyText confidence="0.999777888888889">
We proposed an unsupervised model for Chinese
zero pronoun resolution, investigating the novel
hypothesis that an unsupervised probabilistic re-
solver trained on overt pronouns can be applied to
resolve ZPs. To our knowledge, this is the first un-
supervised probabilistic model for this task. Ex-
periments on the OntoNotes 5.0 corpus showed
that our unsupervised model rivaled its state-of-
the-art supervised counterparts in performance.
</bodyText>
<sectionHeader confidence="0.99547" genericHeader="acknowledgments">
Acknowledgments
</sectionHeader>
<bodyText confidence="0.999651">
We thank the three anonymous reviewers for their
detailed comments. This work was supported in
part by NSF Grants IIS-1147644 and IIS-1219142.
</bodyText>
<sectionHeader confidence="0.995022" genericHeader="references">
References
</sectionHeader>
<reference confidence="0.987834186046512">
Shane Bergsma and Dekang Lin. 2006. Bootstrapping
path-based pronoun resolution. In Proceedings of
the 21st International Conference on Computational
Linguistics and the 44th Annual Meeting of the Asso-
ciation for Computational Linguistics, pages 33--40.
Eugene Charniak and Micha Elsner. 2009. EM works
for pronoun anaphora resolution. In Proceedings
of the 12th Conference of the European Chapter
of the Association for Computational Linguistics,
pages 148--156.
Chen Chen and Vincent Ng. 2013. Chinese zero pro-
noun resolution: Some recent advances. In Proceed-
ings of the 2013 Conference on Empirical Methods
in Natural Language Processing, pages 1360--1365.
Chen Chen and Vincent Ng. 2014. SinoCorefer-
encer: An end-to-end Chinese event coreference
resolver. In Proceedings of the 9th International
Conference on Language Resources and Evaluation,
pages 4532--4538.
Colin Cherry and Shane Bergsma. 2005. An expecta-
tion maximization approach to pronoun resolution.
In Proceedings of the Ninth Conference on Natural
Language Learning, pages 88--95.
Susan Converse. 2006. Pronominal Anaphora Resolu-
tion in Chinese. Ph.D. thesis, University of Pennsyl-
vania.
Arthur P. Dempster, Nan M. Laird, and Donald B. Ru-
bin. 1977. Maximum likelihood from incomplete
data via the EM algorithm. Journal of the Royal Sta-
tistical Society. Series B (Methodological), 39:1--38.
Antonio Ferrández and Jesús Peral. 2000. A compu-
tational approach to zero-pronouns in Spanish. In
Proceedings of the 38th Annual Meeting on Associa-
tion for Computational Linguistics, pages 166--172.
Barbara J. Grosz, Aravind K. Joshi, and Scott Wein-
stein. 1995. Centering: A framework for model-
ing the local coherence of discourse. Computational
Linguistics, 21(2):203--226.
Na-Rae Han. 2006. Korean Zero Pronouns: Analysis
and Resolution. Ph.D. thesis, University of Pennsyl-
vania.
Jerry Hobbs. 1978. Resolving pronoun references.
Lingua, 44:311--338.
</reference>
<page confidence="0.970469">
772
</page>
<reference confidence="0.998988048192771">
Ryu Iida and Massimo Poesio. 2011. A cross-lingual
ILP solution to zero anaphora resolution. In Pro-
ceedings of the 49th Annual Meeting of the Associ-
ation for Computational Linguistics: Human Lan-
guage Technologies, pages 804--813.
Ryu Iida, Kentaro Inui, and Yuji Matsumoto. 2006. Ex-
ploiting syntactic patterns as clues in zero-anaphora
resolution. In Proceedings of the 21st International
Conference on Computational Linguistics and the
44th Annual Meeting of the Association for Compu-
tational Linguistics, pages 625--632.
Ryu Iida, Kentaro Inui, and Yuji Matsumoto. 2007.
Zero-anaphora resolution by learning rich syntactic
pattern features. ACM Transactions on Asian Lan-
guage Information Processing, 6(4).
Kenji Imamura, Kuniko Saito, and Tomoko Izumi.
2009. Discriminative approach to predicate-
argument structure analysis with zero-anaphora res-
olution. In Proceedings of the ACL-IJCNLP 2009
Conference Short Papers, pages 85--88.
Hideki Isozaki and Tsutomu Hirao. 2003. Japanese
zero pronoun resolution based on ranking rules and
machine learning. In Proceedings of the 2003 Con-
ference on Empirical Methods in Natural Language
Processing, pages 184--191.
Andrew Kehler, Douglas Appelt, Lara Taylor, and
Aleksandr Simma. 2004. The (non)utility of
predicate-argument frequencies for pronoun inter-
pretation. In Proceedings of 2004 Human Lan-
guage Technology Conference of the North Ameri-
can Chapter of the Association for Computational
Linguistics, pages 289--296.
Fang Kong and GuoDong Zhou. 2010. A tree kernel-
based unified framework for Chinese zero anaphora
resolution. In Proceedings of the 2010 Conference
on Empirical Methods in Natural Language Process-
ing, pages 882--891.
Robert Parker, David Graff, Ke Chen, Junbo Kong, and
Kazuaki Maeda. 2009. Chinese Gigaword fourth
edition. Linguistic Data Consortium. Philadelphia,
PA.
Sameer Pradhan, Alessandro Moschitti, Nianwen Xue,
Olga Uryupina, and Yuchen Zhang. 2012. CoNLL-
2012 shared task: Modeling multilingual unre-
stricted coreference in OntoNotes. In Proceedings of
2012 Joint Conference on Empirical Methods in Nat-
ural Language Processing and Computational Nat-
ural Language Learning: Shared Task, pages 1--40.
Xian Qian, Qi Zhang, Xuangjing Huang, and Lide Wu.
2010. 2d trie for fast parsing. In Proceedings of
the 23rd International Conference on Computational
Linguistics, pages 904--912.
Ryohei Sasano and Sadao Kurohashi. 2011. A dis-
criminative approach to Japanese zero anaphora res-
olution with large-scale lexicalized case frames. In
Proceedings of the 5th International Joint Confer-
ence on Natural Language Processing, pages 758--
766.
Kazuhiro Seki, Atsushi Fujii, and Tetsuya Ishikawa.
2002. A probabilistic method for analyzing Japanese
anaphora integrating zero pronoun detection and res-
olution. In Proceedings of the 19th International
Conference on Computational Linguistics.
Xiaofeng Yang, Jian Su, and Chew Lim Tan. 2005. Im-
proving pronoun resolution using statistics-based se-
mantic compatibility information. In Proceedings of
the 43rd Annual Meeting ofthe Associationfor Com-
putational Linguistics, pages 165--172.
Ching-Long Yeh and Yi-Chun Chen. 2007. Zero
anaphora resolution in Chinese with shallow pars-
ing. Journal of Chinese Language and Computing,
17(1):41--56.
Shanheng Zhao and Hwee Tou Ng. 2007. Identifica-
tion and resolution of Chinese zero pronouns: A ma-
chine learning approach. In Proceedings of the 2007
Joint Conference on Empirical Methods on Natu-
ral Language Processing and Computational Natu-
ral Language Learning, pages 541--550.
GuoDong Zhou, Fang Kong, and Qiaoming Zhu. 2008.
Context-sensitive convolution tree kernel for pro-
noun resolution. In Proceedings of the 3rd Interna-
tional Joint Conference on Natural Language Pro-
cessing, pages 25--31.
</reference>
<sectionHeader confidence="0.901844" genericHeader="references">
Appendix: Automatic AZP Identification
</sectionHeader>
<bodyText confidence="0.997355636363636">
Our automatic AZP identification system employs
an ordered set of rules. The first rule is a positive
rule that aims to extract as many candidate AZPs
as possible. It is followed by seven negative rules
that aim to improve precision by filtering out er-
roneous candidate AZPs. Below we first describe
the rules and then evaluate this rule-based system.
Rule 1. Add candidate AZP z if it occurs before
the leftmost word spanned by a VP node vp.
Rule 2. Remove z if its associated vp is in a coor-
dinate structure or modified by an adverbial node.
Rule 3. Remove z if the parent of its associated
vp node is not an IP node.
Rule 4. Remove z if its associated vp has a NP
or QP node as an ancestor.
Rule 5. Remove z if one of the left sibling nodes
of vp is NP, QP, IP or ICP.
Rule 6. Remove z if (1) z does not begin a sen-
tence, (2) the highest node whose spanning word
sequence ends with the left non-comma neighbor
word of z is either NP, QP or IP, and (3) the parent
of this node is VP.
</bodyText>
<page confidence="0.997787">
773
</page>
<table confidence="0.99925675">
Gold Parses System Parses
Systems R P F R P F
Rule-based 72.4 42.3 53.4 42.3 26.8 32.8
Supervised 50.6 55.1 52.8 30.8 34.4 32.5
</table>
<tableCaption confidence="0.996056">
Table 12: AZP identification results on the test set.
</tableCaption>
<figureCaption confidence="0.449495">
Rule 7. Remove z if vp&apos;s lowest IP ancestor has
(1) a VP node as its parent and (2) a VV node as
its left sibling.
Rule 8. Remove z if it begins a document.
</figureCaption>
<bodyText confidence="0.999823857142857">
To gauge the performance of our rule-based
AZP identification system, we compare it with our
supervised AZP identification system (Chen and
Ng, 2013). Results of the two systems on our test
set are shown in Table 12. As we can see, the F-
scores achieved by the rule-based system is com-
parable to those of the supervised system.
</bodyText>
<page confidence="0.997162">
774
</page>
</variant>
</algorithm>
<algorithm name="ParsHed" version="110505">
<variant no="0" confidence="0.798873">
<title confidence="0.9968785">Chinese Zero Pronoun Resolution: An Unsupervised Model Rivaling Supervised Resolvers</title>
<author confidence="0.999918">Chen Ng</author>
<affiliation confidence="0.994559">Human Language Technology Research Institute University of Texas at</affiliation>
<address confidence="0.874323">Richardson, TX 75083-0688</address>
<email confidence="0.999774">yzcchen@hlt.utdallas.edu</email>
<email confidence="0.999774">vince@hlt.utdallas.edu</email>
<abstract confidence="0.99583">State-of-the-art Chinese zero pronoun resolution systems are supervised, thus relying on training data containing manually resolved zero pronouns. To eliminate the reliance on annotated data, we present a generative model for unsupervised Chinese zero pronoun resolution. At the core of our model is a novel hypothesis: a probabilistic pronoun resolver trained on overt pronouns in an unsupervised manner can be used to resolve zero pronouns. Experiments demonstrate that our unsupervised model rivals its state-ofthe-art supervised counterparts in performance when resolving the Chinese zero pronouns in the OntoNotes corpus.</abstract>
</variant>
</algorithm>
<algorithm name="ParsCit" version="110505">
<citationList>
<citation valid="true">
<authors>
<author>Shane Bergsma</author>
<author>Dekang Lin</author>
</authors>
<title>Bootstrapping path-based pronoun resolution.</title>
<date>2006</date>
<booktitle>In Proceedings of the 21st International Conference on Computational Linguistics and the 44th Annual Meeting of the Association for Computational Linguistics,</booktitle>
<pages>33--40</pages>
<contexts>
<context position="19400" citStr="Bergsma and Lin (2006)" startWordPosition="3345" endWordPosition="3348">tp://chinesenotes.com/ref_measure_words.htm 6http://code.google.com/p/ctbparser/ _tecedent Pronoun animate inanimate An animate 0.999 0.001 inanimate 0.858 0.142 unknown 0.945 0.055 Table 2: Learned values of P(pAni|cAni, l=1). its GENDER based on its ANIMACY. Specifically, if c is inanimate, we set its GENDER to neuter. Otherwise, we determine its gender by looking up a gender word list constructed by Bergsma and Lin&apos;s (2006) approach. If the word is not in the list, we set its GENDER to masculine by default. Next, we describe how the aforementioned gender word list is constructed. Following Bergsma and Lin (2006), we define a dependency path as the sequence of non-terminal nodes and dependency labels between two potentially coreferent entities in a dependency parse tree. From the parsed Chinese Gigaword corpus, we first collect every dependency path that connects two pronouns. For each path P collected, we compute CL(P), the coreference likelihood of P, as follows: NI(P ) CL(P) = NI(P ) + ND(P) (12) where NI(P) is the number of times P connects two identical pronouns, and ND(P) is the number of times it connects two different pronouns. Assuming that two identical pronouns in a sentence are coreferent </context>
</contexts>
<marker>Bergsma, Lin, 2006</marker>
<rawString>Shane Bergsma and Dekang Lin. 2006. Bootstrapping path-based pronoun resolution. In Proceedings of the 21st International Conference on Computational Linguistics and the 44th Annual Meeting of the Association for Computational Linguistics, pages 33--40.</rawString>
</citation>
<citation valid="true">
<authors>
<author>Eugene Charniak</author>
<author>Micha Elsner</author>
</authors>
<title>EM works for pronoun anaphora resolution.</title>
<date>2009</date>
<booktitle>In Proceedings of the 12th Conference of the European Chapter of the Association for Computational Linguistics,</booktitle>
<pages>148--156</pages>
<contexts>
<context position="21602" citStr="Charniak and Elsner (2009)" startWordPosition="3715" endWordPosition="3718"> feminine 0.065 0.930 0.005 masculine 0.130 0.041 0.828 Table 3: Learned values of P(pGen|cGen, l=1). _tecedent Pronoun first second third An first 0.856 0.119 0.025 second 0.219 0.766 0.016 third 0.289 0.077 0.634 Table 5: Learned values of P(pPer|cPer, l=1) (same speaker). Pronoun singular plural ���������� Antecedent singular 0.861 0.139 plural 0.26 0.74 Table 4: Learned values of P(pNum|cNum, l=1). Pronoun first second third ���������� Antecedent first 0.417 0.525 0.057 second 0.75 0.23 0.02 third 0.437 0.229 0.334 5.3 NUMBER When computing the NUMBER of a candidate antecedent in English, Charniak and Elsner (2009) rely on part-of-speech information. For example, NN and NNP denote singular nouns, whereas NNS and NNPS denote plural nouns. However, Chinese part-of-speech tags do not provide such information. Hence, we need a different method for finding the NUMBER of a candidate antecedent c in Chinese. If c is a pronoun, we look up its NUMBER in Table 1. If c is a named entity, its NUMBER is singular. If c is a common noun, we infer its NUMBER from its string: if the string ends with 417 or is modified by a quantity word (e.g., —4b-, i&apos;F_-��), c is plural; otherwise, c is singular. Table 4 shows the lear</context>
</contexts>
<marker>Charniak, Elsner, 2009</marker>
<rawString>Eugene Charniak and Micha Elsner. 2009. EM works for pronoun anaphora resolution. In Proceedings of the 12th Conference of the European Chapter of the Association for Computational Linguistics, pages 148--156.</rawString>
</citation>
<citation valid="true">
<authors>
<author>Chen Chen</author>
<author>Vincent Ng</author>
</authors>
<title>Chinese zero pronoun resolution: Some recent advances.</title>
<date>2013</date>
<booktitle>In Proceedings of the 2013 Conference on Empirical Methods in Natural Language Processing,</booktitle>
<pages>1360--1365</pages>
<contexts>
<context position="2046" citStr="Chen and Ng (2013)" startWordPosition="319" endWordPosition="322">s we can see, ZPs lack grammatical attributes that are useful for overt pronoun resolution such as NUMBER and GENDER. This makes ZP resolution more challenging than overt pronoun resolution. Automatic ZP resolution is typically composed of two steps. The first step, AZP identification, involves extracting ZPs that are anaphoric. The second step, AZP resolution, aims to identify an antecedent of an AZP. State-of-the-art ZP resolvers have tackled both of these steps in a supervised manner, training a classifier for AZP identification and another one for AZP resolution (e.g., Zhao and Ng (2007), Chen and Ng (2013)). In this paper, we focus on the second task, AZP resolution, designing a model that assumes as input the AZPs in a document and resolves each of them. Note that the task of AZP resolution alone is by no means easy: even when gold-standard AZPs are given, state-of-the-art supervised resolvers can only achieve an F-score of 47.7% for resolving Chinese AZPs (Chen and Ng, 2013). For the sake of completeness, we will evaluate our AZP resolution model using both gold-standard AZPs as well as AZPs automatically identified by a rule-based approach that we propose in this paper. Our contribution lies</context>
<context position="4803" citStr="Chen and Ng, 2013" startWordPosition="761" endWordPosition="764"> employ a supervised learning approach to Chinese ZP resolution. They trained an AZP resolver by employing syntactic and positional features in combination with a decision tree learner. Unlike Zhao and Ng, Kong and Zhou (2010) employed context-sensitive convolution tree kernels (Zhou et al., 2008) in their resolver to model syntactic information. More recently, we extended Zhao and Ng&apos;s feature set with novel features that encode the context surrounding a ZP and its candidate antecedents, and exploited the coreference links between ZPs as bridges to find textually distant antecedents for ZPs (Chen and Ng, 2013). ZP resolution for other languages. There have been rule-based and supervised machine learning approaches for resolving ZPs in other languages. For example, to resolve ZPs in Spanish texts, Ferrández and Peral (2000) proposed a set of handcrafted rules that encode preferences for candidate antecedents. In addition, supervised approaches have been extensively employed to resolve ZPs in Korean (e.g., Han (2006)), Japanese (e.g., Seki et al. (2002), Isozaki and Hirao (2003), Iida et al. (2006; 2007), Imamura et al. (2009), Iida and Poesio (2011), Sasano and Kurohashi (2011)), and Italian (e.g., </context>
<context position="14403" citStr="Chen and Ng, 2013" startWordPosition="2459" endWordPosition="2462">c and overt pronoun p that maximize P(l=1|p, k, c) when p is used to fill the ZP gap left behind by z. The c that results in the maximum probability value over all overt pronouns in PR is chosen as the antecedent of z. In essence, since the model is trained on overt pronouns but is applied to ZPs, we have to exhaustively fill the ZP&apos;s gap under consideration with each of the 10 overt pronouns in PR during inference. Although we can now apply our generative model to resolve AZPs, the resolution procedure can be improved further. The improvement is motivated by a problem we observed previously (Chen and Ng, 2013): an AZP and its closest antecedent can sometimes be far away from each other, thus making it difficult to correctly resolve the AZP. To address this problem, we employ the following resolution procedure in our experiments. Given a test document, we process its AZPs in a left-to-right manner. As soon as we resolve an AZP to a preceding NP c, we fill the corresponding AZP&apos;s gap with c. Hence, when we process an AZP z, all of its preceding AZPs in the associated text have been resolved, with their gaps filled by the NPs they are resolved to. To resolve z, we create test instances between z and i</context>
<context position="24059" citStr="Chen and Ng (2013)" startWordPosition="4147" endWordPosition="4150">erated from an antecedent with the same speaker than one with a different speaker. In the different-speaker case, a first (second) person pronoun is most likely to be generated from a second (first) person pronoun. 6 Context Features To fully specify our model, we need to describe how to represent ke, which is needed to compute P(l=1|ke). Recall that ke encodes the context surrounding candidate antecedent c and the associated pronoun p. As described below, we represent ke using eight features, some of which are motivated by previous work on supervised AZP resolution (e.g., Zhao and Ng (2007), Chen and Ng (2013)). Note that (1) all but feature 1 are computed based on syntactic parse trees, and (2) features 2, 3, 6, and 8 are ternary-valued features. 1. the sentence distance between c and p; 2. whether the node spanning c has an ancestor NP node; if so, whether this NP node is a descendant of c&apos;s lowest ancestor IP node; 3. whether the node spanning c has an ancestor VP node; if so, whether this VP node is a descendant of c&apos;s lowest ancestor IP node; 4. whether vp has an ancestor NP node, where vp is the VP node spanning the VP that follows p; 5. whether vp has an ancestor VP node; 768 Training Test D</context>
<context position="27054" citStr="Chen and Ng (2013)" startWordPosition="4658" endWordPosition="4661">f p as follows. We first consider the subject candidate antecedents in the same sentence as p from right to left, then the other candidate antecedents in the same sentence from right to left. Next, we consider the candidate antecedents in the previous sentence, also preferring candidates that are subjects, but in left-to-right order. Finally, we consider the candidate antecedents two sentences back, following the subject-first, left-to-right order. Evaluation measures. We express the results of ZP resolution in terms of recall (R), precision (P) and F-score (F). Evaluation settings. Following Chen and Ng (2013), we evaluate our model in three settings. In Setting 1, we assume the availability of gold syntactic parse trees and gold AZPs. In Setting 2, we employ gold syntactic parse trees and system (i.e., automatically identified) AZPs. Finally, in Setting 3, we employ system syntactic parse trees and system AZPs. The gold and system syntactic parse trees, as well as the gold AZPs, are obtained from the CoNLL-2012 shared task dataset, while the system AZPs are identified by the rule-based approach described in the Appendix.9 Since our AZP identification approach does not rely on any labeled data, we </context>
<context position="28635" citStr="Chen and Ng, 2013" startWordPosition="4914" endWordPosition="4917">); (3) the closest candidate antecedent that is semantically compatible with z (Baseline 3); and (4) the first candidate antecedent that is semantically compatible with z, where the candidate antecedents are visited according to the order described in Footnote 8 (Baseline 4). These four baselines allow us to study the role of (1) recency, (2) salience, (3) recency combined with semantic compatibility, and (4) salience combined with semantic compatibility in AZP resolution respectively. The remaining three baselines are state-of-the-art supervised AZP resolvers, which include our own resolver (Chen and Ng, 2013) as well as our re-implementations of Zhao and Ng&apos;s (2007) resolver and Kong and Zhou&apos;s (2010) resolver. The test set results of these seven baseline resolvers when evaluated under the three aforementioned evaluation settings are shown in Table 8. The system AZPs employed by the rulebased resolvers are obtained using our rule-based 9One may wonder why we do not train a supervised system for identifying AZPs and instead experiment with a rulebased AZP identification system. The reason is that employing labeled data defeats the whole purpose of having an unsupervised AZP resolution model: if ann</context>
<context position="30016" citStr="Chen and Ng (2013)" startWordPosition="5152" endWordPosition="5155">3: Gold Parses, Gold Parses, System Parses, Gold AZPs System AZPs System AZPs Baseline R P F R P F R P F Selecting closest candidate antecedent 25.0 25.2 25.1 18.3 10.8 13.6 10.3 6.7 8.1 Selecting closest subject 42.0 43.6 42.8 31.8 19.2 23.9 18.0 11.9 14.4 Selecting closest semantically compatible candidate antecedent 28.5 28.8 28.7 20.5 12.2 15.3 11.7 7.6 9.2 Selecting first semantically compatible candidate antecedent 45.2 45.7 45.5 33.6 20.0 25.1 18.9 12.3 14.9 Zhao and Ng (2007) 41.5 41.5 41.5 22.4 24.4 23.3 12.7 14.2 13.4 Kong and Zhou (2010) 44.9 44.9 44.9 33.0 19.3 24.4 18.7 11.9 14.5 Chen and Ng (2013) 47.7 47.7 47.7 25.3 27.6 26.4 14.9 16.7 15.7 Table 8: AZP resolution results of the baseline systems on the test set. Setting 1: Gold Parses, Gold AZPs Setting 2: Gold Parses, System AZPs Setting 3: System Parses, System AZPs Best Baseline Our Model Best Baseline Our Model Best Baseline Our Model Source R P F R P F R P F R P F R P F R P F Overall 47.7 47.7 47.7 47.5 47.9 47.7 25.3 27.6 26.4 35.4 21.0 26.4 14.9 16.7 15.7 19.9 12.9 15.7 NW 38.1 38.1 38.1 41.7 41.7 41.7 15.5 21.7 18.1 29.8 24.8 27.0 6.0 12.2 8.0 11.9 13.0 12.4 MZ 34.6 34.6 34.6 34.0 34.2 34.1 18.5 19.6 19.0 24.1 14.5 18.1 6.2 9.</context>
<context position="31383" citStr="Chen and Ng (2013)" startWordPosition="5419" endWordPosition="5422">.2 26.3 31.5 28.1 29.7 14.6 26.3 18.8 18.2 19.5 18.8 BC 52.7 52.7 52.7 49.8 50.3 50.0 23.3 30.7 26.5 38.0 21.0 27.0 12.7 16.2 14.3 20.6 12.4 15.5 TC 51.2 51.2 51.2 45.2 46.7 46.0 43.1 28.2 34.1 42.4 20.3 27.4 33.2 17.1 22.5 32.2 13.3 18.8 Table 9: AZP resolution results of the best baseline and our unsupervised model on the test set. AZP identification system. On the other hand, since our supervised resolvers are meant to be reimplementations of existing resolvers, we follow previous work and let them employ a supervised AZP identification system. In particular, we employ the one described in Chen and Ng (2013). Several observations can be made about these results. First, among the rule-based resolvers, Baseline 4 achieves the best performance, outperforming Baselines 1, 2, and 3 by 12.9%, 1.5%, and 10.8% in F-score respectively when averaged over the three evaluation settings. From their relative performance, which remains the same in the three settings, we can conclude that as far as AZP resolution is concerned, (1) salience plays a greater role than recency; and (2) semantic compatibility is useful. Second, among the supervised baselines, our supervised resolver (Chen and Ng, 2013) achieves the b</context>
</contexts>
<marker>Chen, Ng, 2013</marker>
<rawString>Chen Chen and Vincent Ng. 2013. Chinese zero pronoun resolution: Some recent advances. In Proceedings of the 2013 Conference on Empirical Methods in Natural Language Processing, pages 1360--1365.</rawString>
</citation>
<citation valid="true">
<authors>
<author>Chen Chen</author>
<author>Vincent Ng</author>
</authors>
<title>SinoCoreferencer: An end-to-end Chinese event coreference resolver.</title>
<date>2014</date>
<booktitle>In Proceedings of the 9th International Conference on Language Resources and Evaluation,</booktitle>
<pages>4532--4538</pages>
<contexts>
<context position="16533" citStr="Chen and Ng (2014)" startWordPosition="2844" endWordPosition="2847">1) for each attribute a. 5.1 ANIMACY We determine the ANIMACY of a candidate antecedent c heuristically. Specifically, we first check the NP type of c. If c is a pronoun, we look up its ANIMACY in Table 1. If c is a named entity, there are two cases to consider: if c is a person4, we label it as animate; otherwise, we label it as inanimate. If c is a common noun, we look up the ANIMACY of its head noun in an automatically 3This postprocessing step is needed because the additional candidate antecedents are only gap fillers. 4A detailed description of our named entity recognizer can be found in Chen and Ng (2014). 766 constructed word list WL. If the head noun is not in WL, we set its ANIMACY to unknown. Our method for constructing WL is motivated by an observation of measure words in Chinese: some of them only modify inanimate nouns while others only modify animate nouns. For example, the nouns modified by the measure word t are always inanimate, as in —tom (one piece of paper). On the other hand, the nouns modified by the measure word are always animate, as in — Z,k (one worker). Given this observation, we first define two lists, Mani and Minani. Mani is a list of measure words that can only modify </context>
</contexts>
<marker>Chen, Ng, 2014</marker>
<rawString>Chen Chen and Vincent Ng. 2014. SinoCoreferencer: An end-to-end Chinese event coreference resolver. In Proceedings of the 9th International Conference on Language Resources and Evaluation, pages 4532--4538.</rawString>
</citation>
<citation valid="true">
<authors>
<author>Colin Cherry</author>
<author>Shane Bergsma</author>
</authors>
<title>An expectation maximization approach to pronoun resolution.</title>
<date>2005</date>
<booktitle>In Proceedings of the Ninth Conference on Natural Language Learning,</booktitle>
<pages>88--95</pages>
<marker>Cherry, Bergsma, 2005</marker>
<rawString>Colin Cherry and Shane Bergsma. 2005. An expectation maximization approach to pronoun resolution. In Proceedings of the Ninth Conference on Natural Language Learning, pages 88--95.</rawString>
</citation>
<citation valid="true">
<authors>
<author>Susan Converse</author>
</authors>
<title>Pronominal Anaphora Resolution in Chinese.</title>
<date>2006</date>
<tech>Ph.D. thesis,</tech>
<institution>University of Pennsylvania.</institution>
<contexts>
<context position="3662" citStr="Converse (2006)" startWordPosition="583" endWordPosition="585">o be applied to languages where such annotated data is not readily available. At its core is a novel hypothesis: we can apply a probabilistic pronoun resolution model trained on overt pronouns in an unsupervised manner to resolve zero pronouns. Motivated by Cherry and Bergsma&apos;s (2005) and Charniak and Elsner&apos;s (2009) work on unsupervised English pronoun resolution, we train our unsupervised resolver on Chinese overt pronouns using the Expectation-Maximization (EM) algorithm (Dempster et al., 1977). 2 Related Work Chinese ZP resolution. Early approaches to Chinese ZP resolution are rule-based. Converse (2006) applied Hobbs&apos; algorithm (Hobbs, 763 Proceedings of the 2014 Conference on Empirical Methods in Natural Language Processing (EMNLP), pages 763–774, October 25-29, 2014, Doha, Qatar. c�2014 Association for Computational Linguistics 1978) to resolve the ZPs in the CTB documents. Yeh and Chen (2007) hand-engineered a set of rules for ZP resolution based on Centering Theory (Grosz et al., 1995). In contrast, virtually all recent approaches to this task are based on supervised learning. Zhao and Ng (2007) are the first to employ a supervised learning approach to Chinese ZP resolution. They trained</context>
</contexts>
<marker>Converse, 2006</marker>
<rawString>Susan Converse. 2006. Pronominal Anaphora Resolution in Chinese. Ph.D. thesis, University of Pennsylvania.</rawString>
</citation>
<citation valid="true">
<authors>
<author>Arthur P Dempster</author>
<author>Nan M Laird</author>
<author>Donald B Rubin</author>
</authors>
<title>Maximum likelihood from incomplete data via the EM algorithm.</title>
<date>1977</date>
<journal>Journal of the Royal Statistical Society. Series B (Methodological),</journal>
<pages>39--1</pages>
<contexts>
<context position="3549" citStr="Dempster et al., 1977" startWordPosition="565" endWordPosition="568">solved AZPs. This, together with the fact that its underlying generative process is not language-dependent, enables it to be applied to languages where such annotated data is not readily available. At its core is a novel hypothesis: we can apply a probabilistic pronoun resolution model trained on overt pronouns in an unsupervised manner to resolve zero pronouns. Motivated by Cherry and Bergsma&apos;s (2005) and Charniak and Elsner&apos;s (2009) work on unsupervised English pronoun resolution, we train our unsupervised resolver on Chinese overt pronouns using the Expectation-Maximization (EM) algorithm (Dempster et al., 1977). 2 Related Work Chinese ZP resolution. Early approaches to Chinese ZP resolution are rule-based. Converse (2006) applied Hobbs&apos; algorithm (Hobbs, 763 Proceedings of the 2014 Conference on Empirical Methods in Natural Language Processing (EMNLP), pages 763–774, October 25-29, 2014, Doha, Qatar. c�2014 Association for Computational Linguistics 1978) to resolve the ZPs in the CTB documents. Yeh and Chen (2007) hand-engineered a set of rules for ZP resolution based on Centering Theory (Grosz et al., 1995). In contrast, virtually all recent approaches to this task are based on supervised learning.</context>
</contexts>
<marker>Dempster, Laird, Rubin, 1977</marker>
<rawString>Arthur P. Dempster, Nan M. Laird, and Donald B. Rubin. 1977. Maximum likelihood from incomplete data via the EM algorithm. Journal of the Royal Statistical Society. Series B (Methodological), 39:1--38.</rawString>
</citation>
<citation valid="true">
<authors>
<author>Antonio Ferrández</author>
<author>Jesús Peral</author>
</authors>
<title>A computational approach to zero-pronouns in Spanish.</title>
<date>2000</date>
<booktitle>In Proceedings of the 38th Annual Meeting on Association for Computational Linguistics,</booktitle>
<pages>166--172</pages>
<contexts>
<context position="5020" citStr="Ferrández and Peral (2000)" startWordPosition="793" endWordPosition="796">g and Zhou (2010) employed context-sensitive convolution tree kernels (Zhou et al., 2008) in their resolver to model syntactic information. More recently, we extended Zhao and Ng&apos;s feature set with novel features that encode the context surrounding a ZP and its candidate antecedents, and exploited the coreference links between ZPs as bridges to find textually distant antecedents for ZPs (Chen and Ng, 2013). ZP resolution for other languages. There have been rule-based and supervised machine learning approaches for resolving ZPs in other languages. For example, to resolve ZPs in Spanish texts, Ferrández and Peral (2000) proposed a set of handcrafted rules that encode preferences for candidate antecedents. In addition, supervised approaches have been extensively employed to resolve ZPs in Korean (e.g., Han (2006)), Japanese (e.g., Seki et al. (2002), Isozaki and Hirao (2003), Iida et al. (2006; 2007), Imamura et al. (2009), Iida and Poesio (2011), Sasano and Kurohashi (2011)), and Italian (e.g., Iida and Poesio (2011)). 3 Chinese Overt Pronouns Since our approach relies heavily on Chinese overt pronouns, in this section we introduce them by describing their four grammatical attributes, namely NUMBER, GENDER, </context>
</contexts>
<marker>Ferrández, Peral, 2000</marker>
<rawString>Antonio Ferrández and Jesús Peral. 2000. A computational approach to zero-pronouns in Spanish. In Proceedings of the 38th Annual Meeting on Association for Computational Linguistics, pages 166--172.</rawString>
</citation>
<citation valid="true">
<authors>
<author>Barbara J Grosz</author>
<author>Aravind K Joshi</author>
<author>Scott Weinstein</author>
</authors>
<title>Centering: A framework for modeling the local coherence of discourse.</title>
<date>1995</date>
<journal>Computational Linguistics,</journal>
<pages>21--2</pages>
<contexts>
<context position="4056" citStr="Grosz et al., 1995" startWordPosition="642" endWordPosition="645">sed resolver on Chinese overt pronouns using the Expectation-Maximization (EM) algorithm (Dempster et al., 1977). 2 Related Work Chinese ZP resolution. Early approaches to Chinese ZP resolution are rule-based. Converse (2006) applied Hobbs&apos; algorithm (Hobbs, 763 Proceedings of the 2014 Conference on Empirical Methods in Natural Language Processing (EMNLP), pages 763–774, October 25-29, 2014, Doha, Qatar. c�2014 Association for Computational Linguistics 1978) to resolve the ZPs in the CTB documents. Yeh and Chen (2007) hand-engineered a set of rules for ZP resolution based on Centering Theory (Grosz et al., 1995). In contrast, virtually all recent approaches to this task are based on supervised learning. Zhao and Ng (2007) are the first to employ a supervised learning approach to Chinese ZP resolution. They trained an AZP resolver by employing syntactic and positional features in combination with a decision tree learner. Unlike Zhao and Ng, Kong and Zhou (2010) employed context-sensitive convolution tree kernels (Zhou et al., 2008) in their resolver to model syntactic information. More recently, we extended Zhao and Ng&apos;s feature set with novel features that encode the context surrounding a ZP and its </context>
</contexts>
<marker>Grosz, Joshi, Weinstein, 1995</marker>
<rawString>Barbara J. Grosz, Aravind K. Joshi, and Scott Weinstein. 1995. Centering: A framework for modeling the local coherence of discourse. Computational Linguistics, 21(2):203--226.</rawString>
</citation>
<citation valid="true">
<authors>
<author>Na-Rae Han</author>
</authors>
<title>Korean Zero Pronouns: Analysis and Resolution.</title>
<date>2006</date>
<tech>Ph.D. thesis,</tech>
<institution>University of Pennsylvania.</institution>
<contexts>
<context position="5216" citStr="Han (2006)" startWordPosition="824" endWordPosition="825"> that encode the context surrounding a ZP and its candidate antecedents, and exploited the coreference links between ZPs as bridges to find textually distant antecedents for ZPs (Chen and Ng, 2013). ZP resolution for other languages. There have been rule-based and supervised machine learning approaches for resolving ZPs in other languages. For example, to resolve ZPs in Spanish texts, Ferrández and Peral (2000) proposed a set of handcrafted rules that encode preferences for candidate antecedents. In addition, supervised approaches have been extensively employed to resolve ZPs in Korean (e.g., Han (2006)), Japanese (e.g., Seki et al. (2002), Isozaki and Hirao (2003), Iida et al. (2006; 2007), Imamura et al. (2009), Iida and Poesio (2011), Sasano and Kurohashi (2011)), and Italian (e.g., Iida and Poesio (2011)). 3 Chinese Overt Pronouns Since our approach relies heavily on Chinese overt pronouns, in this section we introduce them by describing their four grammatical attributes, namely NUMBER, GENDER, PERSON and ANIMACY. NUMBER has two values, singular and plural. GENDER has three values, neuter, masculine and feminine. PERSON has three values, first, second and third. Finally, ANIMACY has two </context>
</contexts>
<marker>Han, 2006</marker>
<rawString>Na-Rae Han. 2006. Korean Zero Pronouns: Analysis and Resolution. Ph.D. thesis, University of Pennsylvania.</rawString>
</citation>
<citation valid="true">
<authors>
<author>Jerry Hobbs</author>
</authors>
<title>Resolving pronoun references.</title>
<date>1978</date>
<journal>Lingua,</journal>
<pages>44--311</pages>
<marker>Hobbs, 1978</marker>
<rawString>Jerry Hobbs. 1978. Resolving pronoun references. Lingua, 44:311--338.</rawString>
</citation>
<citation valid="true">
<authors>
<author>Ryu Iida</author>
<author>Massimo Poesio</author>
</authors>
<title>A cross-lingual ILP solution to zero anaphora resolution.</title>
<date>2011</date>
<booktitle>In Proceedings of the 49th Annual Meeting of the Association for Computational Linguistics: Human Language Technologies,</booktitle>
<pages>804--813</pages>
<contexts>
<context position="5352" citStr="Iida and Poesio (2011)" startWordPosition="845" endWordPosition="848">bridges to find textually distant antecedents for ZPs (Chen and Ng, 2013). ZP resolution for other languages. There have been rule-based and supervised machine learning approaches for resolving ZPs in other languages. For example, to resolve ZPs in Spanish texts, Ferrández and Peral (2000) proposed a set of handcrafted rules that encode preferences for candidate antecedents. In addition, supervised approaches have been extensively employed to resolve ZPs in Korean (e.g., Han (2006)), Japanese (e.g., Seki et al. (2002), Isozaki and Hirao (2003), Iida et al. (2006; 2007), Imamura et al. (2009), Iida and Poesio (2011), Sasano and Kurohashi (2011)), and Italian (e.g., Iida and Poesio (2011)). 3 Chinese Overt Pronouns Since our approach relies heavily on Chinese overt pronouns, in this section we introduce them by describing their four grammatical attributes, namely NUMBER, GENDER, PERSON and ANIMACY. NUMBER has two values, singular and plural. GENDER has three values, neuter, masculine and feminine. PERSON has three values, first, second and third. Finally, ANIMACY has two values, animate and inanimate. We exploit ten personal pronouns that have well-defined grammatical attribute values, namely 你 (singular </context>
</contexts>
<marker>Iida, Poesio, 2011</marker>
<rawString>Ryu Iida and Massimo Poesio. 2011. A cross-lingual ILP solution to zero anaphora resolution. In Proceedings of the 49th Annual Meeting of the Association for Computational Linguistics: Human Language Technologies, pages 804--813.</rawString>
</citation>
<citation valid="true">
<authors>
<author>Ryu Iida</author>
<author>Kentaro Inui</author>
<author>Yuji Matsumoto</author>
</authors>
<title>Exploiting syntactic patterns as clues in zero-anaphora resolution.</title>
<date>2006</date>
<booktitle>In Proceedings of the 21st International Conference on Computational Linguistics and the 44th Annual Meeting of the Association for Computational Linguistics,</booktitle>
<pages>625--632</pages>
<contexts>
<context position="5298" citStr="Iida et al. (2006" startWordPosition="836" endWordPosition="839">d exploited the coreference links between ZPs as bridges to find textually distant antecedents for ZPs (Chen and Ng, 2013). ZP resolution for other languages. There have been rule-based and supervised machine learning approaches for resolving ZPs in other languages. For example, to resolve ZPs in Spanish texts, Ferrández and Peral (2000) proposed a set of handcrafted rules that encode preferences for candidate antecedents. In addition, supervised approaches have been extensively employed to resolve ZPs in Korean (e.g., Han (2006)), Japanese (e.g., Seki et al. (2002), Isozaki and Hirao (2003), Iida et al. (2006; 2007), Imamura et al. (2009), Iida and Poesio (2011), Sasano and Kurohashi (2011)), and Italian (e.g., Iida and Poesio (2011)). 3 Chinese Overt Pronouns Since our approach relies heavily on Chinese overt pronouns, in this section we introduce them by describing their four grammatical attributes, namely NUMBER, GENDER, PERSON and ANIMACY. NUMBER has two values, singular and plural. GENDER has three values, neuter, masculine and feminine. PERSON has three values, first, second and third. Finally, ANIMACY has two values, animate and inanimate. We exploit ten personal pronouns that have well-def</context>
</contexts>
<marker>Iida, Inui, Matsumoto, 2006</marker>
<rawString>Ryu Iida, Kentaro Inui, and Yuji Matsumoto. 2006. Exploiting syntactic patterns as clues in zero-anaphora resolution. In Proceedings of the 21st International Conference on Computational Linguistics and the 44th Annual Meeting of the Association for Computational Linguistics, pages 625--632.</rawString>
</citation>
<citation valid="true">
<authors>
<author>Ryu Iida</author>
<author>Kentaro Inui</author>
<author>Yuji Matsumoto</author>
</authors>
<title>Zero-anaphora resolution by learning rich syntactic pattern features.</title>
<date>2007</date>
<journal>ACM Transactions on Asian Language Information Processing,</journal>
<volume>6</volume>
<issue>4</issue>
<marker>Iida, Inui, Matsumoto, 2007</marker>
<rawString>Ryu Iida, Kentaro Inui, and Yuji Matsumoto. 2007. Zero-anaphora resolution by learning rich syntactic pattern features. ACM Transactions on Asian Language Information Processing, 6(4).</rawString>
</citation>
<citation valid="true">
<authors>
<author>Kenji Imamura</author>
<author>Kuniko Saito</author>
<author>Tomoko Izumi</author>
</authors>
<title>Discriminative approach to predicateargument structure analysis with zero-anaphora resolution.</title>
<date>2009</date>
<booktitle>In Proceedings of the ACL-IJCNLP 2009 Conference Short Papers,</booktitle>
<pages>85--88</pages>
<contexts>
<context position="5328" citStr="Imamura et al. (2009)" startWordPosition="841" endWordPosition="844">e links between ZPs as bridges to find textually distant antecedents for ZPs (Chen and Ng, 2013). ZP resolution for other languages. There have been rule-based and supervised machine learning approaches for resolving ZPs in other languages. For example, to resolve ZPs in Spanish texts, Ferrández and Peral (2000) proposed a set of handcrafted rules that encode preferences for candidate antecedents. In addition, supervised approaches have been extensively employed to resolve ZPs in Korean (e.g., Han (2006)), Japanese (e.g., Seki et al. (2002), Isozaki and Hirao (2003), Iida et al. (2006; 2007), Imamura et al. (2009), Iida and Poesio (2011), Sasano and Kurohashi (2011)), and Italian (e.g., Iida and Poesio (2011)). 3 Chinese Overt Pronouns Since our approach relies heavily on Chinese overt pronouns, in this section we introduce them by describing their four grammatical attributes, namely NUMBER, GENDER, PERSON and ANIMACY. NUMBER has two values, singular and plural. GENDER has three values, neuter, masculine and feminine. PERSON has three values, first, second and third. Finally, ANIMACY has two values, animate and inanimate. We exploit ten personal pronouns that have well-defined grammatical attribute val</context>
</contexts>
<marker>Imamura, Saito, Izumi, 2009</marker>
<rawString>Kenji Imamura, Kuniko Saito, and Tomoko Izumi. 2009. Discriminative approach to predicateargument structure analysis with zero-anaphora resolution. In Proceedings of the ACL-IJCNLP 2009 Conference Short Papers, pages 85--88.</rawString>
</citation>
<citation valid="true">
<authors>
<author>Hideki Isozaki</author>
<author>Tsutomu Hirao</author>
</authors>
<title>Japanese zero pronoun resolution based on ranking rules and machine learning.</title>
<date>2003</date>
<booktitle>In Proceedings of the 2003 Conference on Empirical Methods in Natural Language Processing,</booktitle>
<pages>184--191</pages>
<contexts>
<context position="5279" citStr="Isozaki and Hirao (2003)" startWordPosition="832" endWordPosition="835"> candidate antecedents, and exploited the coreference links between ZPs as bridges to find textually distant antecedents for ZPs (Chen and Ng, 2013). ZP resolution for other languages. There have been rule-based and supervised machine learning approaches for resolving ZPs in other languages. For example, to resolve ZPs in Spanish texts, Ferrández and Peral (2000) proposed a set of handcrafted rules that encode preferences for candidate antecedents. In addition, supervised approaches have been extensively employed to resolve ZPs in Korean (e.g., Han (2006)), Japanese (e.g., Seki et al. (2002), Isozaki and Hirao (2003), Iida et al. (2006; 2007), Imamura et al. (2009), Iida and Poesio (2011), Sasano and Kurohashi (2011)), and Italian (e.g., Iida and Poesio (2011)). 3 Chinese Overt Pronouns Since our approach relies heavily on Chinese overt pronouns, in this section we introduce them by describing their four grammatical attributes, namely NUMBER, GENDER, PERSON and ANIMACY. NUMBER has two values, singular and plural. GENDER has three values, neuter, masculine and feminine. PERSON has three values, first, second and third. Finally, ANIMACY has two values, animate and inanimate. We exploit ten personal pronouns</context>
</contexts>
<marker>Isozaki, Hirao, 2003</marker>
<rawString>Hideki Isozaki and Tsutomu Hirao. 2003. Japanese zero pronoun resolution based on ranking rules and machine learning. In Proceedings of the 2003 Conference on Empirical Methods in Natural Language Processing, pages 184--191.</rawString>
</citation>
<citation valid="true">
<authors>
<author>Andrew Kehler</author>
<author>Douglas Appelt</author>
<author>Lara Taylor</author>
<author>Aleksandr Simma</author>
</authors>
<title>The (non)utility of predicate-argument frequencies for pronoun interpretation.</title>
<date>2004</date>
<booktitle>In Proceedings of 2004 Human Language Technology Conference of the North American Chapter of the Association for Computational Linguistics,</booktitle>
<pages>289--296</pages>
<marker>Kehler, Appelt, Taylor, Simma, 2004</marker>
<rawString>Andrew Kehler, Douglas Appelt, Lara Taylor, and Aleksandr Simma. 2004. The (non)utility of predicate-argument frequencies for pronoun interpretation. In Proceedings of 2004 Human Language Technology Conference of the North American Chapter of the Association for Computational Linguistics, pages 289--296.</rawString>
</citation>
<citation valid="true">
<authors>
<author>Fang Kong</author>
<author>GuoDong Zhou</author>
</authors>
<title>A tree kernelbased unified framework for Chinese zero anaphora resolution.</title>
<date>2010</date>
<booktitle>In Proceedings of the 2010 Conference on Empirical Methods in Natural Language Processing,</booktitle>
<pages>882--891</pages>
<contexts>
<context position="4411" citStr="Kong and Zhou (2010)" startWordPosition="700" endWordPosition="703">LP), pages 763–774, October 25-29, 2014, Doha, Qatar. c�2014 Association for Computational Linguistics 1978) to resolve the ZPs in the CTB documents. Yeh and Chen (2007) hand-engineered a set of rules for ZP resolution based on Centering Theory (Grosz et al., 1995). In contrast, virtually all recent approaches to this task are based on supervised learning. Zhao and Ng (2007) are the first to employ a supervised learning approach to Chinese ZP resolution. They trained an AZP resolver by employing syntactic and positional features in combination with a decision tree learner. Unlike Zhao and Ng, Kong and Zhou (2010) employed context-sensitive convolution tree kernels (Zhou et al., 2008) in their resolver to model syntactic information. More recently, we extended Zhao and Ng&apos;s feature set with novel features that encode the context surrounding a ZP and its candidate antecedents, and exploited the coreference links between ZPs as bridges to find textually distant antecedents for ZPs (Chen and Ng, 2013). ZP resolution for other languages. There have been rule-based and supervised machine learning approaches for resolving ZPs in other languages. For example, to resolve ZPs in Spanish texts, Ferrández and Per</context>
<context position="29952" citStr="Kong and Zhou (2010)" startWordPosition="5139" endWordPosition="5142">train an AZP resolution system. 769 Setting 1: Setting 2: Setting 3: Gold Parses, Gold Parses, System Parses, Gold AZPs System AZPs System AZPs Baseline R P F R P F R P F Selecting closest candidate antecedent 25.0 25.2 25.1 18.3 10.8 13.6 10.3 6.7 8.1 Selecting closest subject 42.0 43.6 42.8 31.8 19.2 23.9 18.0 11.9 14.4 Selecting closest semantically compatible candidate antecedent 28.5 28.8 28.7 20.5 12.2 15.3 11.7 7.6 9.2 Selecting first semantically compatible candidate antecedent 45.2 45.7 45.5 33.6 20.0 25.1 18.9 12.3 14.9 Zhao and Ng (2007) 41.5 41.5 41.5 22.4 24.4 23.3 12.7 14.2 13.4 Kong and Zhou (2010) 44.9 44.9 44.9 33.0 19.3 24.4 18.7 11.9 14.5 Chen and Ng (2013) 47.7 47.7 47.7 25.3 27.6 26.4 14.9 16.7 15.7 Table 8: AZP resolution results of the baseline systems on the test set. Setting 1: Gold Parses, Gold AZPs Setting 2: Gold Parses, System AZPs Setting 3: System Parses, System AZPs Best Baseline Our Model Best Baseline Our Model Best Baseline Our Model Source R P F R P F R P F R P F R P F R P F Overall 47.7 47.7 47.7 47.5 47.9 47.7 25.3 27.6 26.4 35.4 21.0 26.4 14.9 16.7 15.7 19.9 12.9 15.7 NW 38.1 38.1 38.1 41.7 41.7 41.7 15.5 21.7 18.1 29.8 24.8 27.0 6.0 12.2 8.0 11.9 13.0 12.4 MZ 34</context>
</contexts>
<marker>Kong, Zhou, 2010</marker>
<rawString>Fang Kong and GuoDong Zhou. 2010. A tree kernelbased unified framework for Chinese zero anaphora resolution. In Proceedings of the 2010 Conference on Empirical Methods in Natural Language Processing, pages 882--891.</rawString>
</citation>
<citation valid="true">
<authors>
<author>Robert Parker</author>
<author>David Graff</author>
<author>Ke Chen</author>
<author>Junbo Kong</author>
<author>Kazuaki Maeda</author>
</authors>
<title>Chinese Gigaword fourth edition. Linguistic Data Consortium.</title>
<date>2009</date>
<location>Philadelphia, PA.</location>
<contexts>
<context position="17500" citStr="Parker et al., 2009" startWordPosition="3021" endWordPosition="3024"> —tom (one piece of paper). On the other hand, the nouns modified by the measure word are always animate, as in — Z,k (one worker). Given this observation, we first define two lists, Mani and Minani. Mani is a list of measure words that can only modify animate nouns. Minani is a list of measure words that can only modify inanimate nouns.5 There exists a special measure word in Chinese, -&apos;I`, which can be used to modify most of the common nouns regardless of their ANIMACY. As a result, we remove -&apos;I` from both lists. After constructing Mani and Minani, we (1) parse the Chinese Gigaword corpus (Parker et al., 2009), which contains 4,370,600 documents, using an efficient dependency parser, ctbparser6 (Qian et al., 2010), and then (2) collect all pairs of words (m, n), where m is a measure word, n is a common noun, and there is a NMOD dependency relation between m and n. Finally, we determine the ANIMACY of a given common noun n as follows. First, we retrieve all of the pairs containing n. Then, we sum over all occurrences of m in Mani (call the sum Cani), as well as all occurrences of m in Minani (call the sum Cinani). If Cani &gt; Cinani, we label this common noun as animate; otherwise, we label it as inan</context>
</contexts>
<marker>Parker, Graff, Chen, Kong, Maeda, 2009</marker>
<rawString>Robert Parker, David Graff, Ke Chen, Junbo Kong, and Kazuaki Maeda. 2009. Chinese Gigaword fourth edition. Linguistic Data Consortium. Philadelphia, PA.</rawString>
</citation>
<citation valid="true">
<authors>
<author>Sameer Pradhan</author>
<author>Alessandro Moschitti</author>
<author>Nianwen Xue</author>
<author>Olga Uryupina</author>
<author>Yuchen Zhang</author>
</authors>
<title>CoNLL2012 shared task: Modeling multilingual unrestricted coreference in OntoNotes.</title>
<date>2012</date>
<booktitle>In Proceedings of 2012 Joint Conference on Empirical Methods in Natural Language Processing and Computational Natural Language Learning: Shared Task,</booktitle>
<pages>1--40</pages>
<contexts>
<context position="25937" citStr="Pradhan et al., 2012" startWordPosition="4483" endWordPosition="4486">(2004) and Yang et al.&apos;s (2005) methods for computing selectional preferences. Specifically, for each verb and each noun that serves as a subject in Chinese Gigaword, we compute their mutual information (MI). Now, given a pronoun p and a candidate antecedent c in the training/test corpus, we retrieve the MI value of c and p&apos;s governing verb. We then consider them semantically compatible if and only if their MI value is greater than zero. 7 Evaluation 7.1 Experimental Setup Datasets. We employ the Chinese portion of the OntoNotes 5.0 corpus that was used in the official CoNLL-2012 shared task (Pradhan et al., 2012). In the CoNLL-2012 data, the training set and development set contain ZP coreference annotations, but the test set does not. Therefore, we train our models on the training set and perform evaluation on the development set. Statistics on the datasets are shown in Table 7. The documents in these datasets come from six sources, namely Broadcast News (BN), Newswire (NW), Broadcast Conversation (BC), Telephone Conversation (TC), Web Blog (WB) and Magazine (MZ). 8 We sort the candidate antecedents of p as follows. We first consider the subject candidate antecedents in the same sentence as p from ri</context>
</contexts>
<marker>Pradhan, Moschitti, Xue, Uryupina, Zhang, 2012</marker>
<rawString>Sameer Pradhan, Alessandro Moschitti, Nianwen Xue, Olga Uryupina, and Yuchen Zhang. 2012. CoNLL2012 shared task: Modeling multilingual unrestricted coreference in OntoNotes. In Proceedings of 2012 Joint Conference on Empirical Methods in Natural Language Processing and Computational Natural Language Learning: Shared Task, pages 1--40.</rawString>
</citation>
<citation valid="true">
<authors>
<author>Xian Qian</author>
<author>Qi Zhang</author>
<author>Xuangjing Huang</author>
<author>Lide Wu</author>
</authors>
<title>2d trie for fast parsing.</title>
<date>2010</date>
<booktitle>In Proceedings of the 23rd International Conference on Computational Linguistics,</booktitle>
<pages>904--912</pages>
<contexts>
<context position="17606" citStr="Qian et al., 2010" startWordPosition="3035" endWordPosition="3038"> in — Z,k (one worker). Given this observation, we first define two lists, Mani and Minani. Mani is a list of measure words that can only modify animate nouns. Minani is a list of measure words that can only modify inanimate nouns.5 There exists a special measure word in Chinese, -&apos;I`, which can be used to modify most of the common nouns regardless of their ANIMACY. As a result, we remove -&apos;I` from both lists. After constructing Mani and Minani, we (1) parse the Chinese Gigaword corpus (Parker et al., 2009), which contains 4,370,600 documents, using an efficient dependency parser, ctbparser6 (Qian et al., 2010), and then (2) collect all pairs of words (m, n), where m is a measure word, n is a common noun, and there is a NMOD dependency relation between m and n. Finally, we determine the ANIMACY of a given common noun n as follows. First, we retrieve all of the pairs containing n. Then, we sum over all occurrences of m in Mani (call the sum Cani), as well as all occurrences of m in Minani (call the sum Cinani). If Cani &gt; Cinani, we label this common noun as animate; otherwise, we label it as inanimate. Table 2 shows the learned values of P(pAni|cAni, l=1). These results are consistent with our intuit</context>
</contexts>
<marker>Qian, Zhang, Huang, Wu, 2010</marker>
<rawString>Xian Qian, Qi Zhang, Xuangjing Huang, and Lide Wu. 2010. 2d trie for fast parsing. In Proceedings of the 23rd International Conference on Computational Linguistics, pages 904--912.</rawString>
</citation>
<citation valid="true">
<authors>
<author>Ryohei Sasano</author>
<author>Sadao Kurohashi</author>
</authors>
<title>A discriminative approach to Japanese zero anaphora resolution with large-scale lexicalized case frames.</title>
<date>2011</date>
<booktitle>In Proceedings of the 5th International Joint Conference on Natural Language Processing,</booktitle>
<pages>758--766</pages>
<contexts>
<context position="5381" citStr="Sasano and Kurohashi (2011)" startWordPosition="849" endWordPosition="852">y distant antecedents for ZPs (Chen and Ng, 2013). ZP resolution for other languages. There have been rule-based and supervised machine learning approaches for resolving ZPs in other languages. For example, to resolve ZPs in Spanish texts, Ferrández and Peral (2000) proposed a set of handcrafted rules that encode preferences for candidate antecedents. In addition, supervised approaches have been extensively employed to resolve ZPs in Korean (e.g., Han (2006)), Japanese (e.g., Seki et al. (2002), Isozaki and Hirao (2003), Iida et al. (2006; 2007), Imamura et al. (2009), Iida and Poesio (2011), Sasano and Kurohashi (2011)), and Italian (e.g., Iida and Poesio (2011)). 3 Chinese Overt Pronouns Since our approach relies heavily on Chinese overt pronouns, in this section we introduce them by describing their four grammatical attributes, namely NUMBER, GENDER, PERSON and ANIMACY. NUMBER has two values, singular and plural. GENDER has three values, neuter, masculine and feminine. PERSON has three values, first, second and third. Finally, ANIMACY has two values, animate and inanimate. We exploit ten personal pronouns that have well-defined grammatical attribute values, namely 你 (singular you), 我 (I), 他 (he), 她 (she),</context>
</contexts>
<marker>Sasano, Kurohashi, 2011</marker>
<rawString>Ryohei Sasano and Sadao Kurohashi. 2011. A discriminative approach to Japanese zero anaphora resolution with large-scale lexicalized case frames. In Proceedings of the 5th International Joint Conference on Natural Language Processing, pages 758--766.</rawString>
</citation>
<citation valid="true">
<authors>
<author>Kazuhiro Seki</author>
<author>Atsushi Fujii</author>
<author>Tetsuya Ishikawa</author>
</authors>
<title>A probabilistic method for analyzing Japanese anaphora integrating zero pronoun detection and resolution.</title>
<date>2002</date>
<booktitle>In Proceedings of the 19th International Conference on Computational Linguistics.</booktitle>
<contexts>
<context position="5253" citStr="Seki et al. (2002)" startWordPosition="828" endWordPosition="831">ounding a ZP and its candidate antecedents, and exploited the coreference links between ZPs as bridges to find textually distant antecedents for ZPs (Chen and Ng, 2013). ZP resolution for other languages. There have been rule-based and supervised machine learning approaches for resolving ZPs in other languages. For example, to resolve ZPs in Spanish texts, Ferrández and Peral (2000) proposed a set of handcrafted rules that encode preferences for candidate antecedents. In addition, supervised approaches have been extensively employed to resolve ZPs in Korean (e.g., Han (2006)), Japanese (e.g., Seki et al. (2002), Isozaki and Hirao (2003), Iida et al. (2006; 2007), Imamura et al. (2009), Iida and Poesio (2011), Sasano and Kurohashi (2011)), and Italian (e.g., Iida and Poesio (2011)). 3 Chinese Overt Pronouns Since our approach relies heavily on Chinese overt pronouns, in this section we introduce them by describing their four grammatical attributes, namely NUMBER, GENDER, PERSON and ANIMACY. NUMBER has two values, singular and plural. GENDER has three values, neuter, masculine and feminine. PERSON has three values, first, second and third. Finally, ANIMACY has two values, animate and inanimate. We exp</context>
</contexts>
<marker>Seki, Fujii, Ishikawa, 2002</marker>
<rawString>Kazuhiro Seki, Atsushi Fujii, and Tetsuya Ishikawa. 2002. A probabilistic method for analyzing Japanese anaphora integrating zero pronoun detection and resolution. In Proceedings of the 19th International Conference on Computational Linguistics.</rawString>
</citation>
<citation valid="true">
<authors>
<author>Xiaofeng Yang</author>
<author>Jian Su</author>
<author>Chew Lim Tan</author>
</authors>
<title>Improving pronoun resolution using statistics-based semantic compatibility information.</title>
<date>2005</date>
<booktitle>In Proceedings of the 43rd Annual Meeting ofthe Associationfor Computational Linguistics,</booktitle>
<pages>165--172</pages>
<marker>Yang, Su, Tan, 2005</marker>
<rawString>Xiaofeng Yang, Jian Su, and Chew Lim Tan. 2005. Improving pronoun resolution using statistics-based semantic compatibility information. In Proceedings of the 43rd Annual Meeting ofthe Associationfor Computational Linguistics, pages 165--172.</rawString>
</citation>
<citation valid="true">
<authors>
<author>Ching-Long Yeh</author>
<author>Yi-Chun Chen</author>
</authors>
<title>Zero anaphora resolution in Chinese with shallow parsing.</title>
<date>2007</date>
<journal>Journal of Chinese Language and Computing,</journal>
<volume>17</volume>
<issue>1</issue>
<contexts>
<context position="3960" citStr="Yeh and Chen (2007)" startWordPosition="625" endWordPosition="628">niak and Elsner&apos;s (2009) work on unsupervised English pronoun resolution, we train our unsupervised resolver on Chinese overt pronouns using the Expectation-Maximization (EM) algorithm (Dempster et al., 1977). 2 Related Work Chinese ZP resolution. Early approaches to Chinese ZP resolution are rule-based. Converse (2006) applied Hobbs&apos; algorithm (Hobbs, 763 Proceedings of the 2014 Conference on Empirical Methods in Natural Language Processing (EMNLP), pages 763–774, October 25-29, 2014, Doha, Qatar. c�2014 Association for Computational Linguistics 1978) to resolve the ZPs in the CTB documents. Yeh and Chen (2007) hand-engineered a set of rules for ZP resolution based on Centering Theory (Grosz et al., 1995). In contrast, virtually all recent approaches to this task are based on supervised learning. Zhao and Ng (2007) are the first to employ a supervised learning approach to Chinese ZP resolution. They trained an AZP resolver by employing syntactic and positional features in combination with a decision tree learner. Unlike Zhao and Ng, Kong and Zhou (2010) employed context-sensitive convolution tree kernels (Zhou et al., 2008) in their resolver to model syntactic information. More recently, we extended</context>
</contexts>
<marker>Yeh, Chen, 2007</marker>
<rawString>Ching-Long Yeh and Yi-Chun Chen. 2007. Zero anaphora resolution in Chinese with shallow parsing. Journal of Chinese Language and Computing, 17(1):41--56.</rawString>
</citation>
<citation valid="true">
<authors>
<author>Shanheng Zhao</author>
<author>Hwee Tou Ng</author>
</authors>
<title>Identification and resolution of Chinese zero pronouns: A machine learning approach.</title>
<date>2007</date>
<booktitle>In Proceedings of the 2007 Joint Conference on Empirical Methods on Natural Language Processing and Computational Natural Language Learning,</booktitle>
<pages>541--550</pages>
<contexts>
<context position="2026" citStr="Zhao and Ng (2007)" startWordPosition="315" endWordPosition="318">political crisis.) As we can see, ZPs lack grammatical attributes that are useful for overt pronoun resolution such as NUMBER and GENDER. This makes ZP resolution more challenging than overt pronoun resolution. Automatic ZP resolution is typically composed of two steps. The first step, AZP identification, involves extracting ZPs that are anaphoric. The second step, AZP resolution, aims to identify an antecedent of an AZP. State-of-the-art ZP resolvers have tackled both of these steps in a supervised manner, training a classifier for AZP identification and another one for AZP resolution (e.g., Zhao and Ng (2007), Chen and Ng (2013)). In this paper, we focus on the second task, AZP resolution, designing a model that assumes as input the AZPs in a document and resolves each of them. Note that the task of AZP resolution alone is by no means easy: even when gold-standard AZPs are given, state-of-the-art supervised resolvers can only achieve an F-score of 47.7% for resolving Chinese AZPs (Chen and Ng, 2013). For the sake of completeness, we will evaluate our AZP resolution model using both gold-standard AZPs as well as AZPs automatically identified by a rule-based approach that we propose in this paper. O</context>
<context position="4168" citStr="Zhao and Ng (2007)" startWordPosition="660" endWordPosition="663"> 2 Related Work Chinese ZP resolution. Early approaches to Chinese ZP resolution are rule-based. Converse (2006) applied Hobbs&apos; algorithm (Hobbs, 763 Proceedings of the 2014 Conference on Empirical Methods in Natural Language Processing (EMNLP), pages 763–774, October 25-29, 2014, Doha, Qatar. c�2014 Association for Computational Linguistics 1978) to resolve the ZPs in the CTB documents. Yeh and Chen (2007) hand-engineered a set of rules for ZP resolution based on Centering Theory (Grosz et al., 1995). In contrast, virtually all recent approaches to this task are based on supervised learning. Zhao and Ng (2007) are the first to employ a supervised learning approach to Chinese ZP resolution. They trained an AZP resolver by employing syntactic and positional features in combination with a decision tree learner. Unlike Zhao and Ng, Kong and Zhou (2010) employed context-sensitive convolution tree kernels (Zhou et al., 2008) in their resolver to model syntactic information. More recently, we extended Zhao and Ng&apos;s feature set with novel features that encode the context surrounding a ZP and its candidate antecedents, and exploited the coreference links between ZPs as bridges to find textually distant ante</context>
<context position="24039" citStr="Zhao and Ng (2007)" startWordPosition="4143" endWordPosition="4146">ore likely to be generated from an antecedent with the same speaker than one with a different speaker. In the different-speaker case, a first (second) person pronoun is most likely to be generated from a second (first) person pronoun. 6 Context Features To fully specify our model, we need to describe how to represent ke, which is needed to compute P(l=1|ke). Recall that ke encodes the context surrounding candidate antecedent c and the associated pronoun p. As described below, we represent ke using eight features, some of which are motivated by previous work on supervised AZP resolution (e.g., Zhao and Ng (2007), Chen and Ng (2013)). Note that (1) all but feature 1 are computed based on syntactic parse trees, and (2) features 2, 3, 6, and 8 are ternary-valued features. 1. the sentence distance between c and p; 2. whether the node spanning c has an ancestor NP node; if so, whether this NP node is a descendant of c&apos;s lowest ancestor IP node; 3. whether the node spanning c has an ancestor VP node; if so, whether this VP node is a descendant of c&apos;s lowest ancestor IP node; 4. whether vp has an ancestor NP node, where vp is the VP node spanning the VP that follows p; 5. whether vp has an ancestor VP node;</context>
<context position="29886" citStr="Zhao and Ng (2007)" startWordPosition="5126" endWordPosition="5129">ning an AZP identification system, the same data can be used to train an AZP resolution system. 769 Setting 1: Setting 2: Setting 3: Gold Parses, Gold Parses, System Parses, Gold AZPs System AZPs System AZPs Baseline R P F R P F R P F Selecting closest candidate antecedent 25.0 25.2 25.1 18.3 10.8 13.6 10.3 6.7 8.1 Selecting closest subject 42.0 43.6 42.8 31.8 19.2 23.9 18.0 11.9 14.4 Selecting closest semantically compatible candidate antecedent 28.5 28.8 28.7 20.5 12.2 15.3 11.7 7.6 9.2 Selecting first semantically compatible candidate antecedent 45.2 45.7 45.5 33.6 20.0 25.1 18.9 12.3 14.9 Zhao and Ng (2007) 41.5 41.5 41.5 22.4 24.4 23.3 12.7 14.2 13.4 Kong and Zhou (2010) 44.9 44.9 44.9 33.0 19.3 24.4 18.7 11.9 14.5 Chen and Ng (2013) 47.7 47.7 47.7 25.3 27.6 26.4 14.9 16.7 15.7 Table 8: AZP resolution results of the baseline systems on the test set. Setting 1: Gold Parses, Gold AZPs Setting 2: Gold Parses, System AZPs Setting 3: System Parses, System AZPs Best Baseline Our Model Best Baseline Our Model Best Baseline Our Model Source R P F R P F R P F R P F R P F R P F Overall 47.7 47.7 47.7 47.5 47.9 47.7 25.3 27.6 26.4 35.4 21.0 26.4 14.9 16.7 15.7 19.9 12.9 15.7 NW 38.1 38.1 38.1 41.7 41.7 41</context>
</contexts>
<marker>Zhao, Ng, 2007</marker>
<rawString>Shanheng Zhao and Hwee Tou Ng. 2007. Identification and resolution of Chinese zero pronouns: A machine learning approach. In Proceedings of the 2007 Joint Conference on Empirical Methods on Natural Language Processing and Computational Natural Language Learning, pages 541--550.</rawString>
</citation>
<citation valid="true">
<authors>
<author>GuoDong Zhou</author>
<author>Fang Kong</author>
<author>Qiaoming Zhu</author>
</authors>
<title>Context-sensitive convolution tree kernel for pronoun resolution.</title>
<date>2008</date>
<booktitle>In Proceedings of the 3rd International Joint Conference on Natural Language Processing,</booktitle>
<pages>25--31</pages>
<contexts>
<context position="4483" citStr="Zhou et al., 2008" startWordPosition="710" endWordPosition="713">or Computational Linguistics 1978) to resolve the ZPs in the CTB documents. Yeh and Chen (2007) hand-engineered a set of rules for ZP resolution based on Centering Theory (Grosz et al., 1995). In contrast, virtually all recent approaches to this task are based on supervised learning. Zhao and Ng (2007) are the first to employ a supervised learning approach to Chinese ZP resolution. They trained an AZP resolver by employing syntactic and positional features in combination with a decision tree learner. Unlike Zhao and Ng, Kong and Zhou (2010) employed context-sensitive convolution tree kernels (Zhou et al., 2008) in their resolver to model syntactic information. More recently, we extended Zhao and Ng&apos;s feature set with novel features that encode the context surrounding a ZP and its candidate antecedents, and exploited the coreference links between ZPs as bridges to find textually distant antecedents for ZPs (Chen and Ng, 2013). ZP resolution for other languages. There have been rule-based and supervised machine learning approaches for resolving ZPs in other languages. For example, to resolve ZPs in Spanish texts, Ferrández and Peral (2000) proposed a set of handcrafted rules that encode preferences fo</context>
</contexts>
<marker>Zhou, Kong, Zhu, 2008</marker>
<rawString>GuoDong Zhou, Fang Kong, and Qiaoming Zhu. 2008. Context-sensitive convolution tree kernel for pronoun resolution. In Proceedings of the 3rd International Joint Conference on Natural Language Processing, pages 25--31.</rawString>
</citation>
</citationList>
</algorithm>
</algorithms>
<?xml version="1.0" encoding="UTF-8"?>
<algorithms version="110505">
<algorithm name="SectLabel" version="110505">
<variant no="0" confidence="0.000009">
<title confidence="0.975534">
Prune-and-Score: Learning for Greedy Coreference Resolution
</title>
<author confidence="0.970441">
Chao Ma, Janardhan Rao Doppa†, J. Walker Orr, Prashanth Mannem
Xiaoli Fern, Tom Dietterich and Prasad Tadepalli
</author>
<affiliation confidence="0.996104">
School of Electrical Engineering and Computer Science, Oregon State University
</affiliation>
<email confidence="0.976215">
{machao,orr,mannemp,xfern,tgd,tadepall}@eecs.oregonstate.edu
</email>
<affiliation confidence="0.457803">
† School of Electrical Engineering and Computer Science, Washington State University
</affiliation>
<email confidence="0.997025">
jana@eecs.wsu.edu
</email>
<sectionHeader confidence="0.997367" genericHeader="abstract">
Abstract
</sectionHeader>
<bodyText confidence="0.999824736842105">
We propose a novel search-based approach
for greedy coreference resolution, where
the mentions are processed in order and
added to previous coreference clusters.
Our method is distinguished by the use
of two functions to make each corefer-
ence decision: a pruning function that
prunes bad coreference decisions from fur-
ther consideration, and a scoring function
that then selects the best among the re-
maining decisions. Our framework re-
duces learning of these functions to rank
learning, which helps leverage powerful
off-the-shelf rank-learners. We show that
our Prune-and-Score approach is superior
to using a single scoring function to make
both decisions and outperforms sever-
al state-of-the-art approaches on multiple
benchmark corpora including OntoNotes.
</bodyText>
<sectionHeader confidence="0.999517" genericHeader="introduction">
1 Introduction
</sectionHeader>
<bodyText confidence="0.999767440677966">
Coreference resolution is the task of clustering a
set of mentions in the text such that all mentions in
the same cluster refer to the same entity. It is one
of the first stages in deep language understanding
and has a big potential impact on the rest of the
stages. Several of the state-of-the-art approaches
learn a scoring function defined over mention pair,
cluster-mention or cluster-cluster pair to guide the
coreference decision-making process (Daum´e II-
I, 2006; Bengtson and Roth, 2008; Rahman and
Ng, 2011b; Stoyanov and Eisner, 2012; Chang et
al., 2013; Durrett et al., 2013; Durrett and Klein,
2013). One common and persistent problem with
these approaches is that the scoring function has to
make all the coreference decisions, which leads to
a highly non-realizable learning problem.
Inspired by the recent success of the HC-Search
Framework (Doppa et al., 2014a) for studying a
variety of structured prediction problems (Lam et
al., 2013; Doppa et al., 2014c), we study a novel
approach for search-based coreference resolution
called Prune-and-Score. HC-Search is a divide-
and-conquer solution that learns multiple compo-
nents with pre-defined roles, and each of them
contribute towards the overall goal by making the
role of the other components easier. The HC-
Search framework operates in the space of com-
plete outputs, and relies on the loss function which
is only defined on the complete outputs to drive it-
s learning. Unfortunately, this method does not
work for incremental coreference resolution since
the search space for coreference resolution con-
sists of partial outputs, i.e., a set of mentions only
some of which have been clustered so far.
We develop an alternative framework to HC-
Search that allows us to effectively learn from par-
tial output spaces and apply it to greedy corefer-
ence resolution. The key idea of our work is to
address the problem of non-realizability of the s-
coring function by learning two different function-
s: 1) a pruning function to prune most of the bad
decisions, and 2) a scoring function to pick the
best decision among those that are remaining. Our
Prune-and-Score approach is a particular instanti-
ation of the general idea of learning nearly-sound
constraints for pruning, and leveraging the learned
constraints to learn improved heuristic function-
s for guiding the search. The pruning constraints
can take different forms (e.g., classifiers, decision-
list, or ranking functions) depending on the search
architecture. Therefore, other coreference resolu-
tion systems (Chang et al., 2013; Durrett and K-
lein, 2013; Bj¨orkelund and Kuhn, 2014) can also
benefit from this idea. While our basic idea of two-
level selection might appear similar to the coarse-
to-fine inference architectures (Felzenszwalb and
McAllester, 2007; Weiss and Taskar, 2010), the
details differ significantly. Importantly, our prun-
ing and scoring functions operate sequentially at
</bodyText>
<page confidence="0.931647">
2115
</page>
<note confidence="0.8952995">
Proceedings of the 2014 Conference on Empirical Methods in Natural Language Processing (EMNLP), pages 2115–2126,
October 25-29, 2014, Doha, Qatar. c�2014 Association for Computational Linguistics
</note>
<bodyText confidence="0.999894592592593">
each greedy search step, whereas in the cascades
approach, the second level function makes its pre-
diction only when the first level decision-making
is done.
Summary of Contributions. The main contribu-
tions of our work are as follows. First, we moti-
vate and introduce the Prune-and-Score approach
to search-based coreference resolution. Second,
we identify a decomposition of the overall loss
of the Prune-and-Score approach into the pruning
loss and the scoring loss, and reduce the problem
of learning these two functions to rank learning,
which allows us to leverage powerful and efficien-
t off-the-shelf rank learners. Third, we evaluate
our approach on OntoNotes, ACE, and MUC da-
ta, and show that it compares favorably to sever-
al state-of-the-art approaches as well as a greedy
search-based approach that uses a single scoring
function.
The remainder of the paper proceeds as follows.
In Section 2, we dicuss the related work. We intro-
duce our problem setup in Section 3 and then de-
scribe our Prune-and-Score approach in Section 4.
We explain our approaches for learning the prun-
ing and scoring functions in Section 5. Section 6
presents our experimental results followed by the
conclusions in Section 7.
</bodyText>
<sectionHeader confidence="0.999931" genericHeader="related work">
2 Related Work
</sectionHeader>
<bodyText confidence="0.99991430882353">
The work on learning-based coreference resolu-
tion can be broadly classified into three types.
First, the pair-wise classifier approaches learn a
classifier on mention pairs (edges) (Soon et al.,
2001; Ng and Cardie, 2002; Bengtson and Roth,
2008), and perform some form of approximate de-
coding or post-processing using the pair-wise s-
cores to make predictions. However, the pair-wise
classifier approach suffers from several drawback-
s including class imbalance (fewer positive edges
compared to negative edges) and not being able to
leverage the global structure (instead making in-
dependent local decisions).
Second, the global approaches such as Struc-
tured SVMs and Conditional Random Fields
(CRFs) learn a cost function to score a potential
clustering output for a given input set of men-
tions (Mccallum and Wellner, 2003; Finley and
Joachims, 2005; Culotta et al., 2007; Yu and
Joachims, 2009; Haghighi and Klein, 2010; Wick
et al., 2011; Wick et al., 2012; Fernandes et al.,
2012). These methods address some of the prob-
lems with pair-wise classifiers, however, they suf-
fer from the intractability of “Argmin” inference
(finding the least cost clustering output among ex-
ponential possibilities) that is encountered during
both training and testing. As a result, they resort to
approximate inference algorithms (e.g., MCMC,
loopy belief propagation), which can suffer from
local optima.
Third, the incremental approaches construct the
clustering output incrementally by processing the
mentions in some order (Daum´e III, 2006; De-
nis and Baldridge, 2008; Rahman and Ng, 2011b;
Stoyanov and Eisner, 2012; Chang et al., 2013;
Durrett et al., 2013; Durrett and Klein, 2013).
These methods learn a scoring function to guide
the decision-making process and differ in the form
of the scoring function (e.g., mention pair, cluster-
mention or cluster-cluster pair) and how it is being
learned. They have shown great success and are
very efficient. Indeed, several of the approach-
es that have achieved state-of-the-art results on
OntoNotes fall under this category (Chang et al.,
2013; Durrett et al., 2013; Durrett and Klein,
2013; Bj¨orkelund and Kuhn, 2014). However,
their efficiency requirement leads to a highly non-
realizable learning problem. Our Prune-and-Score
approach is complementary to these methods, as
we show that having a pruning function (or a set
of learned pruning rules) makes the learning prob-
lem easier and can improve over the performance
of scoring-only approaches. Also, the models in
(Chang et al., 2013; Durrett et al., 2013) try to
leverage cluster-level information implicitly (vi-
a latent antecedents) from mention-pair features,
whereas our model explicitly leverages the cluster
level information.
Coreference resolution systems can benefit
by incorporating the world knowledge including
rules, constraints, and additional information from
external knowledge bases (Lee et al., 2013; Rah-
man and Ng, 2011a; Ratinov and Roth, 2012;
Chang et al., 2013; Zheng et al., 2013; Hajishirzi
et al., 2013). Our work is orthogonal to this line
of work, but domain constraints and rules can be
incorporated into our model as done in (Chang et
al., 2013).
</bodyText>
<sectionHeader confidence="0.990641" genericHeader="method">
3 Problem Setup
</sectionHeader>
<bodyText confidence="0.678411666666667">
Coreference resolution is a structured pre-
diction problem where the set of mentions
m1, m2, · · · , mD extracted from a document cor-
</bodyText>
<page confidence="0.985751">
2116
</page>
<bodyText confidence="0.998540777777778">
reponds to a structured input x and the structured
output y corresponds to a partition of the men-
tions into a set of clusters C1, C2, · · · , Ck. Each
mention mi belongs to exactly one of the clusters
Cj. We are provided with a training set of input-
output pairs drawn from an unknown distribution
D, and the goal is to return a function/predictor
from inputs to outputs. The learned predictor
is evaluated against a non-negative loss function
</bodyText>
<equation confidence="0.7778035">
L : X × Y × Y 7→ &lt;+, L(x, y&apos;, y) is the loss asso-
ciated with predicting incorrect output y&apos; for input
</equation>
<bodyText confidence="0.999672166666667">
x when the true output is y (e.g., B-Cubed Score).
In this work, we formulate the coreference
resolution problem in a search-based framework.
There are three key elements in this framework:
1) the Search space Sp whose states correspond
to partial clustering outputs; 2) the Action prun-
ing function Fprune that is used to prune irrelevant
actions at each state; and 3) the Action scoring
function Fscore that is used to construct a com-
plete clustering output by selecting actions from
those that are left after pruning. Sp is a 3-tuple
hI, A, Ti, where I is the initial state function, A
gives the set of possible actions in a given state,
and T is a predicate which is true for terminal s-
tates. In our case, s0 = I(x) corresponds to a s-
tate where every mention is unresolved, and A(si)
consists of actions to place the next mention mi+1
in each cluster in si or a NEW action which creates
a new cluster for it. Terminal nodes correspond to
states with all mentions resolved.
We focus on greedy search. The decision pro-
cess for constructing an output corresponds to s-
electing a sequence of actions leading from the
initial state to a terminal state using bothFprune
and Fscore, which are parameterized functions
over state-action pairs (Fprune(01(s, a)) ∈ &lt; and
Fscore(02(s, a)) ∈ &lt;), where 01 and 02 stand for
feature functions. We want to learn the parameters
of both Fprune and Fscore such that the predicted
outputs on unseen inputs have low expected loss.
</bodyText>
<sectionHeader confidence="0.984143" genericHeader="method">
4 Greedy Prune-and-Score Approach
</sectionHeader>
<bodyText confidence="0.880960538461539">
Our greedy Prune-and-Score approach for coref-
erence resolution is parameterized by a pruning
function Fprune : S × A 7→ &lt;, a scoring func-
tion Fscore : S × A 7→ &lt;, and a pruning param-
eter b ∈ [1, Amax], where Amax is the maximum
number of actions at any state s ∈ S. Given a
set of input mentions m1, m2, · · · , mD extracted
from a document (input x), and a pruning param-
Algorithm 1 Greedy Prune-and-Score Resolver
Input: x = set of mentions m1, m2, · · · , mD from
a document D, hI, A, Ti = Search space defini-
tion, Fprune = learned pruning function, b = prun-
ing parameter, Fscore = learned scoring function
</bodyText>
<listItem confidence="0.993141125">
1: s ← I(x) //initial state
2: while not T(s) do
3: A&apos; ← Top b actions from A(s) according to
Fprune //prune
4: ap ← arg maxa∈A&apos; Fscore(s,a) //score
5: s ← Apply ap on s
6: end while
7: return coreference output corresponding to s
</listItem>
<bodyText confidence="0.999529277777778">
eter b, our Prune-and-Score approach makes pre-
dictions as follows. The search starts at the ini-
tial state s0 = I(x) (see Algorithm 1). At each
non-terminal state s, the pruning function Fprune
retains only the top b actions (A&apos;) from A(s) (Step
3), and the scoring functionFscore picks the best
scoring action ap ∈ A&apos; (Step 4) to reach the next
state. When a terminal state is reached its con-
tents are returned as the prediction. Figure 1 illus-
trates the decision-making process of our Prune-
and-Score approach for an example state.
We now formalize the learning objective of our
Prune-and-Score approach. Let yˆ be the predicted
coreference output for a coreference input-output
pair (x, y*). The expected loss of the greedy
Prune-and-Score approach E(Fprune, Fscore) for a
given pruning function Fprune and scoring func-
tionFscore can be defined as follows.
</bodyText>
<equation confidence="0.982559">
E(Fprune,Fscore) = E(x,y∗)—D L (x, ˆy, y*)
</equation>
<bodyText confidence="0.999932611111111">
Our goal is to learn an optimal pair of pruning
and scoring functions (Fporune,-F core) that min-
imizes the expected loss of the Prune-and-Score
approach. The behavior of our Prune-and-Score
approach depends on the pruning parameter b,
which dictates the workload of pruning and scor-
ing functions. For small values of b (aggressive
pruning), pruning function learning may be harder,
but scoring function learning will be easier. Simi-
larly, for large values of b (conservative pruning),
scoring function learning becomes hard, but prun-
ing function learning is easy. Therefore, we would
expect beneficial behavior if pruning function can
aggressively prune (small values of b) with little
loss in accuracy. It is interesting to note that our
Prune-and-Score approach degenerates to existing
incremental approaches that use only the scoring
function for search (Daum´e III, 2006; Rahman and
</bodyText>
<page confidence="0.987511">
2117
</page>
<figure confidence="0.905945">
(a) Text with input set of mentions
Ramallah ( West Bank 2 )1 10-15 ( AFP3) - Eyewitnesses4 reported that Palestinians5
</figure>
<bodyText confidence="0.925792666666667">
demonstrated today Sunday in the West Bank6 against the Sharm el-Sheikh7 summit to be
held in Egypt8 tomorrow Monday. In Ramallahy, around 500 people10 took to the town11’s
streets chanting slogans denouncing the summit ...
</bodyText>
<figure confidence="0.964327434782609">
(b) Illustration of Prune-and-Score approach
C1 C2 C3 C4 C5 C6
m1
m9 m
a7
a1 a2 a3 a4 a5 a6
m2
m3 m4
6
m7 m11
m10
m5
State: s = {C1, C2, C3, C4, C5, C6} Actions: A(s) = {a1, a2, a3, a4, a5, a6, a7}
Pruning step:
a2 a1 a7 a5 a6 a3 a4
1.4 0.7 0.4
b = 3
2.5 2.2 1.9 1.5
Fprune values
A&apos;(s) = {a2, a1, a7}
Scoring step: a1 a2 a7 Fscore values
4.5 3.1 2.6
Decision: a1 is the best action for state s
</figure>
<figureCaption confidence="0.858217142857143">
Figure 1: Illustration of Prune-and-Score approach. (a) Text with input set of mentions. Mentions are highlighted
and numbered. (b) Illustration of decision-making process for mention m11. The partial clustering output corre-
sponding to the current state s consists of six clusters denoted by C1, C2, · · · , Cs. Highlighted circles correspond
to the clusters. Edges from mention m11 to each of the six clusters and to itself stand for the set of possible actions
A(s) in state s, and are denoted by a1, a2, · · · , a7. The pruning function Fprune scores all the actions in A(s) and
only keeps the top 3 actions A&apos; = {a2, a1, a7} as specified by the pruning parameter b. The scoring function picks
the best scoring action a1 ∈ A&apos; as the final decision, and mention m11 is merged with cluster C1.
</figureCaption>
<bodyText confidence="0.984808785714286">
Ng, 2011b) when b = ∞. Additionally, for b = 1,
our pruning function coincides with the scoring
function.
Analysis of Representational Power. The fol-
lowing proposition formalizes the intuition that t-
wo functions are strictly better than one in expres-
sive power. See Appendix for the proof.
Proposition 1. Let Fprune andFscore be func-
tions from the same function space. Then for all
learning problems, min.Tscore E(Fscore, Fscore) ≥
min(.Tprune,.Tscore) E(Fprune,Fscore). More-
over there exist learning problems for which
min.Tscore E(Fscore,Fscore) can be arbitrarily
worse than min(.Tprune,.Tscore) E(Fprune, Fscore).
</bodyText>
<sectionHeader confidence="0.971663" genericHeader="method">
5 Learning Algorithms
</sectionHeader>
<bodyText confidence="0.996180083333333">
)
In general, learning the optimal (Fo prune, Fo score
pair can be intractable due to their potential inter-
dependence. Specifically, when learning Fprune
in the worst case there can be ambiguity about
which of the non-optimal actions to retain, and
for only some of those an effective Fscore can be
found. However, we observe a loss decomposi-
tion in terms of the individual losses due to Fprune
and Fscore, and develop a stage-wise learning ap-
proach that first learns Fprune and then learns a
corresponding Fscore.
</bodyText>
<subsectionHeader confidence="0.996689">
5.1 Loss Decomposition
</subsectionHeader>
<bodyText confidence="0.999927571428571">
The overall loss of the Prune-and-Score approach
E (Fprune, Fscore) can be decomposed into prun-
ing loss Eprune, the loss due to Fprune not be-
ing able to retain the optimal terminal state in
the search space; and scoring loss Escore|.Tprune,
the additional loss due to Fscore not guiding the
greedy search to the best terminal state after prun-
ing using Fprune. Below, we will define these
losses more formally.
Pruning Loss is defined as the expected loss of
the Prune-and-Score approach when we perform
greedy search with Fprune and Fscore, the opti-
mal scoring function. A scoring function is said to
be optimal if at every state s in the search space
</bodyText>
<page confidence="0.949705">
2118
</page>
<bodyText confidence="0.999716095238095">
Sp, and for any set of remaining actions A(s), it
can score each action a E A(s) such that greedy
search can reach the best terminal state (as eval-
uated by task loss function L) that is reachable
from s through A(s). Unfortunately, computing
the optimal scoring function is highly intractable
for the non-decomposable loss functions that are
employed in coreference resolution (e.g., B-Cubed
F1). The main difficulty is that the decision at any
one state has interdependencies with future deci-
sions (see Section 5.5 in (Daum´e III, 2006) for
more details). So we need to resort to some form
of approximate optimal scoring function that ex-
hibits the intended behavior. This is very similar
to the dynamic oracle concept developed for de-
pendency parsing (Goldberg and Nivre, 2013).
Let y∗prune be the coreference output corre-
sponding to the terminal state reached from input
x by Prune-and-Score approach when performing
search using Fprune and F∗score. Then the pruning
loss can be expressed as follows.
</bodyText>
<equation confidence="0.840561">
Eprune = E(x,y*)∼D L (x, y∗prune, y∗)
</equation>
<bodyText confidence="0.91425875">
Scoring Loss is defined as the additional loss due
to Fscore not guiding the greedy search to the best
terminal state reachable via the pruning function
Fscore (i.e., y∗prune). Let yˆ be the coreference out-
put corresponding to the terminal state reached by
Prune-and-Score approach by performing search
with Fprune and Fscore for an input x. Then the
scoring loss can be expressed as follows:
Escore|Fprune
= E(x,y*)∼D L (x, ˆy, y∗) − L (x, y∗prune, y∗)
The overall loss decomposition of our Prune-and-
Score approach can be expressed as follows.
</bodyText>
<figure confidence="0.9339505">
� (Fprune, Fscore)
= E(x,y*)∼D L (x, y∗prune, y∗)
• v • +
Eprune
E(x,y*)∼D L (x, ˆy, y∗) − L (x, y∗prune, y∗)
Escore|,�prune
</figure>
<subsectionHeader confidence="0.996776">
5.2 Stage-wise Learning
</subsectionHeader>
<bodyText confidence="0.999928">
The loss decomposition motivates a learning ap-
proach that targets minimizing the errors of prun-
ing and scoring functions independently. In par-
ticular, we optimize the overall loss of the Prune-
and-Score approach in a stage-wise manner. We
first train a pruning function ˆFprune to optimize
the pruning loss component Eprune and then train
a scoring function ˆFscore to optimize the scoring
loss Escore |ˆFprune conditioned on ˆFprune.
</bodyText>
<equation confidence="0.5318445">
ˆFprune ^ arg minFprune∈Fp Eprune
ˆFscore Pz� arg minFscore∈Fs Escore |ˆFprune
</equation>
<bodyText confidence="0.999829333333333">
Note that this approach is myopic in the sense that
ˆFprune is learned without considering the impli-
cations for learningˆFscore. Below, we first de-
scribe our approach for pruning function learning,
and then explain our scoring function learning al-
gorithm.
</bodyText>
<subsectionHeader confidence="0.997017">
5.3 Pruning Function Learning
</subsectionHeader>
<bodyText confidence="0.999704542857143">
In our greedy Prune-and-Score approach, the role
of the pruning function Fprune is to prune away
irrelevant actions (as specified by the pruning pa-
rameter b) at each search step. More specifically,
we want Fprune to score actions A(s) at each s-
tate s such that the optimal action a∗ E A(s) is
ranked within the top b actions to minimize Eprune.
For this, we assume that for any training input-
output pair (x, y∗) there exists a unique action se-
quence, or solution path (initial state to terminal
state), for producing y∗ from x. More formally, let
(s∗0, a∗0), (s∗1, a∗1), , (s∗D, ∅) correspond to the
sequence of state-action pairs along this solution
path, where s∗ 0 is the initial state and s∗D is the ter-
minal state. The goal is to learn the parameters of
Fprune such that at each state s∗i , a∗i E A(s∗i) is
ranked among the top b actions.
While we can employ an online-LaSO style ap-
proach (III and Marcu, 2005; Xu et al., 2009) to
learn the parameters of the pruning function, it is
quite inefficient, as it must regenerate the same
search trajectory again and again until it learn-
s to make the right decision. Additionally, this
approach limits applicability of the off-the-shelf
learners to learn the parameters of Fprune. To
overcome these drawbacks, we apply offline train-
ing.
Reduction to Rank Learning. We reduce the
pruning function learning to a rank learning prob-
lem. This allows us to leverage powerful and effi-
cient off-the-shelf rank-learners (Liu, 2009). The
reduction is as follows. At each state s∗i on the so-
lution path of a training example (x, y∗), we create
an example by labeling optimal action a∗i E A(s∗i )
as the only relevant action, and then try to learn
</bodyText>
<page confidence="0.979695">
2119
</page>
<bodyText confidence="0.999464083333334">
a ranking function that can rank actions such that
the relevant action a∗i is in the top b actions, where
b is the input pruning paramter. In other word-
s, we have a rank learning problem, where the
learner’s goal is to optimize the Precision at Top-
b. The training approach creates such an exam-
ple for each state s in the solution path. The set
of aggregate imitation examples collected over al-
l the training data is then given to a rank learner
(e.g., LambdaMART (Burges, 2010)) to learn the
parameters of Fprune by optimizing the Precision
at Top-b loss. See appendix for the pseudocode.
If we can learn a function Fprune that is con-
sistent with these imitation examples, then the
learned pruning function is guaranteed to keep
the solution path within the pruned space for al-
l the training examples. We can also employ
more advanced imitation learning algorithms in-
cluding DAgger (Ross et al., 2011) and SEARN
(Hal Daum´e III et al., 2009) if we are provid-
ed with an (approximate) optimal scoring function
F∗score that can pick optimal actions at states that
are not in the solution path (i.e., off-trajectory s-
tates).
</bodyText>
<subsectionHeader confidence="0.996672">
5.4 Scoring Function Learning
</subsectionHeader>
<bodyText confidence="0.999840953125">
Given a learned pruning function Fprune, we want
to learn a scoring function that can pick the best
action from the b actions that remain after prun-
ing at each state. We formulate this problem in the
framework of imitation learning (Khardon, 1999).
More formally, let (ˆs0, a∗0), (ˆs1, a∗1), · · · , (ˆs∗ D, ∅)
correspond to the sequence of state-action pairs
along the greedy trajectory obtained by running
the Prune-and-Score approach with Fprune and
F∗score, the optimal scoring function, on a train-
ing example (x, y∗), where ˆs∗ D is the best terminal
state in the pruned space. The goal of our imita-
tion training approach is to learn the parameters
of Fscore such that at each state ˆsi, a∗i ∈ A0 is
ranked higher than all other actions in A0, where
A0 C A(ˆsi) is the set of b actions that remain after
pruning.
It is important to note that the distribution of
states in the pruned space due to Fprune on the
testing data may be somewhat different from those
on training data. Therefore, we train our scoring
function via cross-validation by training the scor-
ing function on heldout data that was not used to
train the pruning function. This methodology is
commonly employed in Re-Ranking and Stacking
approaches (Collins, 2000; Cohen and de Carval-
ho, 2005).
Our scoring function learning procedure uses
cross validation and consists of the following four
steps. First, we divide the training data D in-
to k folds. Second, we learn k different pruners,
where each pruning functionFiprune is learned us-
ing the data from all the folds excluding the ith
fold. Third, we generate ranking examples for
scoring function learning as described above us-
ing each pruning functionFiprune on the data it
was not trained on. Finally, we give the aggregate
set of ranking examples R to a rank learner (e.g.,
SVM-Rank or LambdaMART) to learn the scoring
function Fscore. See appendix for the pseudocode.
Approximate Optimal Scoring Function. If the
learned pruning function is not consistent with the
training data, we will encounter states ˆsi that are
not on the target path, and we will need some su-
pervision for learning in those cases. As discussed
before in Section 5.1, computing an optimal scor-
ing function F∗score is intractable for combinatorial
loss functions that are used for coreference resolu-
tion. So we employ an approximate function from
existing work that is amenable to evaluate partial
outputs (Daum´e III, 2006). It is a variant of the
ACE scoring function that removes the bipartite
matching step from the ACE metric. Moreover
this score is computed only on the partial coref-
erence output corresponding to the “after state”
s0 resulting from taking action a in state s, i.e.,
F∗score(s, a) = F∗score(s0). To further simplify the
computation, we give uniform weight to the three
types of costs: 1) Credit for correct linking, 2)
Penalty for incorrect linking, and 3) Penalty for
missing links. Intuitively, this is similar to the
correct-link count computed only on a subgraph.
We direct the reader to (Daum´e III, 2006) for more
details (see Section 5.5).
</bodyText>
<sectionHeader confidence="0.99891" genericHeader="evaluation">
6 Experiments and Results
</sectionHeader>
<bodyText confidence="0.999797">
In this section, we evaluate our greedy Prune-
and-Score approach on three benchmark corpora
– OntoNotes 5.0 (Pradhan et al., 2012), ACE 2004
(NIST, 2004), and MUC6 (MUC6, 1995) – and
compare it against the state-of-the-art approaches
for coreference resolution. For OntoNotes data,
we report the results on both gold mentions and
predicted mentions. We also report the results on
gold mentions for ACE 2004 and MUC6 data.
</bodyText>
<page confidence="0.945277">
2120
</page>
<subsectionHeader confidence="0.982955">
6.1 Experimental Setup
</subsectionHeader>
<bodyText confidence="0.999945468085107">
Datasets. For OntoNotes corpus, we employ the
official split for training, validation, and testing.
There are 2802 documents in the training set; 343
documents in the validation set; and 345 docu-
ments in the testing set. The ACE 2004 corpus
contains 443 documents. We follow the (Culot-
ta et al., 2007; Bengtson and Roth, 2008) split
in our experiments by employing 268 documents
for training, 68 documents for validation, and 107
documents (ACE2004-CULOTTA-TEST) for test-
ing. We also evaluate our system on the 128
newswire documents in ACE 2004 corpus for a
fair comparison with the state-of-the-art. The
MUC6 corpus containts 255 documents. We em-
ploy the official test set of 30 documents (MUC6-
TEST) for testing purposes. From the remaining
225 documents, which includes 195 official train-
ing documents and 30 dry-run test documents, we
randomly pick 30 documents for validation, and
use the remaining ones for training.
Evaluation Metrics. We compute three most pop-
ular performance metrics for coreference resolu-
tion: MUC (Vilain et al., 1995), B-Cubed (Bag-
ga and Baldwin, 1998), and Entity-based CEAF
(CEAFφ4) (Luo, 2005). As it is commonly done
in CoNLL shared tasks (Pradhan et al., 2012), we
employ the average F1 score (CoNLL F1) of these
three metrics for comparison purposes. We evalu-
ate all the results using the updated version1 (7.0)
of the coreference scorer.
Features. We built2 our coreference resolver
based on the Easy-first coreference system (Stoy-
anov and Eisner, 2012), which is derived from the
Reconcile system (Stoyanov et al., 2010). We es-
sentially employ the same features as in the Easy-
first system. However, we provide some high-
level details that are necessary for subsequent dis-
cussion. Recall that our features 0(s, a) for both
pruning and scoring functions are defined over
state-action pairs, where each state s consists of
a set of clusters and an action a corresponds to
merging an unprocessed mention m with a clus-
ter C in state s or create one for itself. Therefore,
0(s, a) defines features over cluster-mention pairs
(C, m). Our feature vector consists of three part-
s: a) mention pair features; b) entity pair features;
and c) a single indicator feature to represent NEW
</bodyText>
<footnote confidence="0.986226333333333">
1http://code.google.com/p/reference-coreference-scorers/
2See http://research.engr.oregonstate.edu/dral/ for our
software.
</footnote>
<bodyText confidence="0.994893688888889">
action (i.e., mention m starts its own cluster). For
mention pair features, we average the pair-wise
features over all links between m and every men-
tion mc in cluster C (often referred to as average-
link). Note that, we cannot employ the best-link
feature representation because we perform offline
training and do not have weights for scoring the
links. For entity pair features, we treat mention
m as a singleton entity and compute features by
pairing it with the entity represented by cluster C
(exactly as in the Easy-first system). The indica-
tor feature will be 1 for the NEW action and 0 for
all other actions.We have a total of 140 features:
90 mention pair features; 49 entity pair features;
and one NEW indicator feature. We believe that
our approach can benefit from employing features
of the mention for the NEW action (Rahman and
Ng, 2011b; Durrett and Klein, 2013). However,
we were constrained by the Reconcile system and
could not leverage these features for the NEW ac-
tion.
Base Rank-Learner. Our pruning and scoring
function learning algorithms need a base rank-
learner. We employ LambdaMART (Burges,
2010), a state-of-the art rank learner from the
RankLib3 library. LambdaMART is a variant of
boosted regression trees. We use a learning rate
of 0.1, specify the maximum number of boost-
ing iterations (or trees) as 1000 noting that its ac-
tual value is automatically decided based on the
validation set, and tune the number of leaves per
tree based on the validation data. Once we fix
the hyper-parameters of LambdaMART, we train
the final model on all of the training data. Lamb-
daMART uses an internal train/validation split of
the input ranking examples to decide when to stop
the boosting iterations. We fixed this ratio to 0.8
noting that the performance is not sensitive to this
parameter. For scoring function learning, we used
5 folds for the cross-validation training.
Pruning Parameter b. The hyper-parameter b
controls the amount of pruning in our Prune-and-
Score approach. We perform experiments with d-
ifferent values of b and pick the best value based
on the performance on the validation set.
</bodyText>
<subsectionHeader confidence="0.502834">
Singleton Mention Filter for OntoNotes Cor-
</subsectionHeader>
<bodyText confidence="0.997227666666667">
pus. We employ the Illinois-Coref system (Chang
et al., 2012) to extract system mentions for our
OntoNotes experiments, and observe that the num-
</bodyText>
<footnote confidence="0.952243">
3http://sourceforge.net/p/lemur/wiki/RankLib/
</footnote>
<page confidence="0.996277">
2121
</page>
<bodyText confidence="0.999687018181818">
ber of predicted mentions is thrice the number of
gold mentions. Since the training data provides the
clustering supervision for only gold mentions, it is
not clear how to train with the system mention-
s that are not part of gold mentions. A common
way of dealing with this problem is to treat all the
extra system mentions as singleton clusters (Dur-
rett and Klein, 2013; Chang et al., 2013). Howev-
er, this solution most likely will not work with our
current feature representation (i.e., NEW action is
represented as a single indicator feature). Recall
that to predict these extra system mentions as s-
ingleton clusters with our incremental clustering
approach, the learned model should first predic-
t a NEW action while processing these mention-
s to form a temporary singleton cluster, and then
refrain from merging any of the subsequent men-
tions with that cluster so that it becomes a single-
ton cluster in the final clustering output. Howev-
er, in OntoNotes corpus, the training data does not
include singleton clusters for the gold mentions.
Therefore, only the large number (57%) of system
mentions that are not part of gold mentions will
constitute the set of singleton clusters. This leads
to a highly imbalanced learning problem because
our model needs to learn (the weight of the sin-
gle indicator feature) to predict NEW as the best
action for a large set of mentions, which will bias
our model to predict large number of NEW actions
during testing. As a result, we will generate many
singleton clusters, which will hurt the recall of the
mention detection after post-processing. There-
fore, we aim to learn a singleton mention filter
that will be used as a pre-processor before training
and testing to overcome this problem. We would
like to point out that our filter is complementary to
other solutions (e.g., employing features that can
discriminate a given mention to be anaphoric or
not in place of our single indicator feature, or us-
ing a customized loss to weight our ranking exam-
ples for cost-sensitive training)(Durrett and Klein,
2013).
Filter Learning. The singleton mention filter is
a classifier that will label a given mention as “s-
ingleton” or not. We represent each mention m
in a document by averaging the mention-pair fea-
tures φ(m, m&apos;) of the k-most similar mentions
(obtained by ranking all other mentions m&apos; in the
document with a learned ranking function R given
m) and then learn a decision-tree classifier by opti-
mizing the F1 loss. We learn the mention-ranking
function R by optimizing the recall of positive
pairs for a given k, and employ LambdaMART as
our base ranker. The hyper-parameters are tuned
based on the performance on the validation set.
</bodyText>
<subsectionHeader confidence="0.977039">
6.2 Results
</subsectionHeader>
<bodyText confidence="0.977896">
We first describe the results of the learned single-
ton mention filter, and then the performance of
our Prune-and-Score approach with and without
the filter. Next, we compare the results of our ap-
proach with several state-of-the-art approaches for
coreference resolution.
Singleton Mention Filter Results. Table 1 shows
the performance of the learned singleton mention
filter with k = 2 noting that the results are ro-
bust for all values of k ≥ 2. As we can see, the
learned filter improves the precision of the men-
tion detection with only small loss in the recall of
gold mentions.
</bodyText>
<table confidence="0.9989155">
Mention Detection Accuracy
P R F1
Before- 43.18% 86.99% 57.71%
filtering (16664/38596) (16664/19156)
After- 79.02% 80.98% 79.97%
filtering (15516/19640) (15516/19156)
</table>
<tableCaption confidence="0.867938">
Table 1: Performance of the singleton mention filter on
the OntoNotes 5.0 development set. The numerators of
the fractions in the brackets show the exact numbers of
mentions that are matched with the gold mentions.
</tableCaption>
<bodyText confidence="0.999743285714286">
Prune-and-Score Results. Table 2 shows the per-
formance of Prune-and-Score approach with and
without the singleton mention filter. We can see
that the results with filter are much better than the
corresponding results without the filter. These re-
sults show that our approach can benefit from hav-
ing a good singleton mention filter.
</bodyText>
<table confidence="0.8592882">
Filter settings MUC B3 CEAFφ4 CoNLL
OntoNotes 5.0 Dev Set w. Predict Ment.
O.S. (w.o. Filter) 66.73 53.40 44.23 54.79
P&amp;S (w.o. Filter) 65.93 52.96 50.24 56.38
P&amp;S (w. Filter) 71.18 58.87 57.88 62.64
</table>
<tableCaption confidence="0.996303">
Table 2: Performance of Prune-and-Score approach
</tableCaption>
<bodyText confidence="0.952321285714286">
with and without the singleton mention filter, and Only-
Score approach without the filter.
Table 3 shows the performance of different con-
figurations of our Prune-and-Score approach. As
we can see, Prune-and-Score gives better results
than the configuration where we employ only the
scoring function (b = ∞) for small values of b.
</bodyText>
<page confidence="0.974991">
2122
</page>
<table confidence="0.997753870967742">
MUC B3 CEAF04 CoNLL
P R F1 P R F1 P R F1 Avg-F1
a. Results on OntoNotes 5.0 Test Set with Predicted Mentions
Prune-and-Score 81.03 66.16 72.84 66.90 51.10 57.94 68.75 44.34 53.91 61.56
Only-Scoring 75.95 61.53 67.98 63.94 47.37 54.42 58.54 49.76 53.79 58.73
HOTCoref 67.46 74.3 70.72 54.96 62.71 58.58 52.27 59.4 55.61 61.63
CPL3M - - 69.48 - - 57.44 - - 53.07 60.00
Berkeley 74.89 67.17 70.82 64.26 53.09 58.14 58.12 52.67 55.27 61.41
Fernandes et al., 2012 75.91 65.83 70.51 65.19 51.55 57.58 57.28 50.82 53.86 60.65
Stanford 65.31 64.11 64.71 56.54 48.58 52.26 46.67 52.29 49.32 55.43
b. Results on OntoNotes 5.0 Test Set with Gold Mentions
Prune-and-Score 88.10 85.85 86.96 76.82 76.16 76.49 80.90 74.06 77.33 80.26
Only-Scoring 86.96 84.52 85.73 74.51 74.25 74.38 79.04 70.67 74.62 78.24
CPL3M - - 84.80 - - 78.74 - - 68.75 77.43
Berkeley 85.73 89.26 87.46 78.23 75.11 76.63 82.89 70.86 76.40 80.16
Stanford 89.94 78.17 83.64 81.75 68.95 74.81 73.97 61.20 66.98 75.14
c. Results on ACE2004 Culotta Test Set with Gold Mentions
Prune-and-Score 85.57 72.68 78.60 90.09 77.02 83.04 74.64 86.02 79.42 80.35
Only-Scoring 82.75 69.25 75.40 88.54 74.22 80.75 73.69 85.22 78.58 78.24
CPL3M - - 78.29 - - 82.20 - - 79.26 79.91
Stanford 82.91 69.90 75.85 89.14 74.05 80.90 75.67 77.45 76.55 77.77
d. Results on ACE2004 Newswire with Gold Mentions
Prune-and-Score 89.72 75.72 82.13 90.89 76.15 82.87 72.43 86.83 78.69 81.23
Only-Scoring 86.92 76.49 81.37 88.10 75.83 81.51 73.15 84.31 78.05 80.31
Easy-first - - 80.1 - - 81.8 - - - -
Stanford 84.75 75.34 79.77 87.50 74.59 80.53 73.32 81.49 77.19 79.16
e. Results on MUC6 Test Set with Gold Mentions
Prune-and-Score 89.53 82.75 86.01 86.48 76.18 81.00 60.74 80.33 68.68 78.56
Only-Scoring 86.77 80.96 83.76 81.72 72.99 77.11 57.56 75.38 64.91 75.26
Easy-first - - 88.2 - - 77.5 - - - -
Stanford 91.19 69.54 78.91 91.07 63.39 74.75 62.43 69.62 65.83 73.16
</table>
<tableCaption confidence="0.9939685">
Table 4: Comparison of Prune-and-Score with state-of-the-art approaches. Metric values reflect version 7 of
CoNLL scorer.
</tableCaption>
<bodyText confidence="0.99929575">
The performance is clearly better than the degen-
erate case (b = ∞) over a wide range of b values,
suggesting that it is not necessary to carefully tune
the parameter b.
</bodyText>
<table confidence="0.901163909090909">
Pruning param. b MUC B3 CEAF04 CoNLL
OntoNotes 5.0 Dev Set w. Predict Ment.
2 69.12 56.80 56.30 60.74
3 70.50 57.89 57.24 61.88
4 71.00 58.65 57.41 62.35
5 71.18 58.87 57.88 62.64
6 70.93 58.66 57.85 62.48
8 70.12 58.13 57.37 61.87
10 70.24 58.34 56.27 61.61
20 67.97 57.73 56.63 60.78
∞ 67.03 56.31 55.56 59.63
</table>
<tableCaption confidence="0.92426475">
Table 3: Performance of Prune-and-Score approach
with different values of the pruning parameter b. For
b = ∞, Prune-and-Score becomes an Only-Scoring al-
gorithm.
</tableCaption>
<bodyText confidence="0.970285739130435">
Comparison to State-of-the-Art. Table 4
shows the results of our Prune-and-Score ap-
proach compared with the following state-of-the-
art coreference resolution approaches: HOTCoref
system (Bj¨orkelund and Kuhn, 2014); Berkeley
system with the FINAL feature set (Durrett and K-
lein, 2013); CPL3M system (Chang et al., 2013);
Stanford system (Lee et al., 2013); Easy-first sys-
tem (Stoyanov and Eisner, 2012); and Fernan-
des et al., 2012 (Fernandes et al., 2012). On-
ly Scoring is the special case of our Prune-and-
Score approach where we employ only the scoring
function. This corresponds to existing incremen-
tal approaches (Daum´e III, 2006; Rahman and Ng,
2011b). We report the best published results for
CPL3M system, Easy-first, and Fernandes et al.,
2012. We ran the publicly available software to
generate the results for Berkeley and Stanford sys-
tems with the updated CoNLL scorer. We include
the results of Prune-and-Score for best b on the de-
velopment set with singleton mention filter for the
comparison. In Table 4, ’-’ indicates that we could
not find published results for those cases. We see
</bodyText>
<page confidence="0.954391">
2123
</page>
<bodyText confidence="0.986013">
that results of the Prune-and-Score approach are
comparable to or better than the state-of-the-art in-
cluding Only-Scoring.
</bodyText>
<sectionHeader confidence="0.977169" genericHeader="conclusions">
7 Conclusions and Future Work
</sectionHeader>
<bodyText confidence="0.999991303030303">
We introduced the Prune-and-Score approach for
greedy coreference resolution whose main idea
is to learn a pruning function along with a scor-
ing function to effectively guide the search. We
showed that our approach improves over the meth-
ods that only learn a scoring function, and gives
comparable or better results than several state-of-
the-art coreference resolution systems.
Our Prune-and-Score approach is a particular
instantiation of the general idea of learning nearly-
sound constraints for pruning, and leveraging the
learned constraints to learn improved heuristic
functions for guiding the search (See (Chen et
al., 2014) for another instantiation of this idea for
multi-object tracking in videos). Therefore, oth-
er coreference resolution systems (Chang et al.,
2013; Durrett and Klein, 2013; Bj¨orkelund and
Kuhn, 2014) can also benefit from this idea. One
way to further improve the peformance of our
approach is to perform a search in the Limited
Discrepancy Search (LDS) space (Doppa et al.,
2014b) using the learned functions.
Future work should apply this general idea to
other natural language processing tasks including
dependency parsing (Nivre et al., 2007) and in-
formation extraction (Li et al., 2013). We would
expect more beneficial behavior with the prun-
ing constraints for problems with large action sets
(e.g., labeled dependency parsing). It would be in-
teresting and useful to generalize this approach to
search spaces where there are multiple target paths
from the initial state to the terminal state, e.g., as
in the Easy-first framework.
</bodyText>
<sectionHeader confidence="0.998137" genericHeader="acknowledgments">
Acknowledgments
</sectionHeader>
<bodyText confidence="0.999757444444445">
Authors would like to thank Veselin Stoyanov
(JHU) for answering several questions related to
the Easy-first and Reconcile systems; Van Dang
(UMass, Amherst) for technical discussions relat-
ed to the RankLib library; Kai-Wei Chang (UIUC)
for the help related to the Illinois-Coref mention
extractor; and Greg Durrett (UC Berkeley) for his
help with the Berkeley system. This work was
supported in part by NSF grants IIS 1219258, I-
IS 1018490 and in part by the Defense Advanced
Research Projects Agency (DARPA) and the Air
Force Research Laboratory (AFRL) under Con-
tract No. FA8750-13-2-0033. Any opinions,
findings and conclusions or recommendations ex-
pressed in this material are those of the author(s)
and do not necessarily reflect the views of the NS-
F, the DARPA, the Air Force Research Laboratory
(AFRL), or the US government.
</bodyText>
<sectionHeader confidence="0.998908" genericHeader="references">
References
</sectionHeader>
<reference confidence="0.998776627906977">
Amit Bagga and Breck Baldwin. 1998. Algorithms for
scoring coreference chains. In In The First Interna-
tional Conference on Language Resources and Eval-
uation Workshop on Linguistics Coreference, pages
563–566.
Eric Bengtson and Dan Roth. 2008. Understanding the
value of features for coreference resolution. In Pro-
ceedings of Empirical Methods in Natural Language
Processing (EMNLP), pages 294–303.
Anders Bj¨orkelund and Jonas Kuhn. 2014. Learn-
ing structured perceptrons for coreference resolution
with latent antecedents and non-local features. In
Proceedings of the 52nd Annual Meeting of the As-
sociation for Computational Linguistics (Volume 1:
Long Papers), pages 47–57, Baltimore, Maryland,
June. Association for Computational Linguistics.
Christopher Burges. 2010. From RankNet to Lamb-
daRank to LambdaMART: An overview. Microsoft
Technical Report, (MSR-TR-2010).
Kai-Wei Chang, Rajhans Samdani, Alla Rozovskaya,
Mark Sammons, and Dan Roth. 2012. Illinois-
Coref: The UI system in the CoNLL-2012 shared
task. In Joint Conference on EMNLP and CoNLL
- Shared Task, pages 113–117, Jeju Island, Korea,
July. Association for Computational Linguistics.
Kai-Wei Chang, Rajhans Samdani, and Dan Roth.
2013. A constrained latent variable model for
coreference resolution. In Proceedings of Em-
pirical Methods in Natural Language Processing
(EMNLP), pages 601–612.
Sheng Chen, Alan Fern, and Sinisa Todorovic. 2014.
Multi-object tracking via constrained sequential la-
beling. In To appear in Proceedings of IEEE Con-
ference on Computer Vision and Pattern Recognition
(CVPR).
William W. Cohen and Vitor Rocha de Carvalho. 2005.
Stacked sequential learning. In Proceedings of In-
ternational Joint Conference on Artificial Intelli-
gence (IJCAI), pages 671–676.
Michael Collins. 2000. Discriminative reranking for
natural language parsing. In Proceedings of Inter-
national Conference on Machine Learning (ICML),
pages 175–182.
</reference>
<page confidence="0.947353">
2124
</page>
<reference confidence="0.999675801886793">
Aron Culotta, Michael L. Wick, and Andrew Mc-
Callum. 2007. First-order probabilistic models
for coreference resolution. In Proceedings of Hu-
man Language Technology Conference of the North
American Chapter of the Association of Computa-
tional Linguistics (HLT-NAACL), pages 81–88.
Hal Daum´e III. 2006. Practical Structured Learning
Techniques for Natural Language Processing. Ph.D.
thesis, University of Southern California, Los Ange-
les, CA.
Pascal Denis and Jason Baldridge. 2008. Specialized
models and ranking for coreference resolution. In
Proceedings of Empirical Methods in Natural Lan-
guage Processing (EMNLP), pages 660–669.
Janardhan Rao Doppa, Alan Fern, and Prasad Tadepal-
li. 2014a. HC-Search: A learning framework for
search-based structured prediction. Journal of Arti-
ficial Intelligence Research (JAIR), 50:369–407.
Janardhan Rao Doppa, Alan Fern, and Prasad Tade-
palli. 2014b. Structured prediction via output s-
pace search. Journal of Machine Learning Research
(JMLR), 15:1317–1350.
Janardhan Rao Doppa, Jun Yu, Chao Ma, Alan Fern,
and Prasad Tadepalli. 2014c. HC-Search for multi-
label prediction: An empirical study. In Proceed-
ings of AAAI Conference on Artificial Intelligence
(AAAI).
Greg Durrett and Dan Klein. 2013. Easy victories and
uphill battles in coreference resolution. In Proceed-
ings of the Conference on Empirical Methods in Nat-
ural Language Processing (EMNLP), pages 1971–
1982.
Greg Durrett, David Leo Wright Hall, and Dan Klein.
2013. Decentralized entity-level modeling for coref-
erence resolution. In Proceedings of Association of
Computational Linguistics (ACL) Conference, pages
114–124.
Pedro F. Felzenszwalb and David A. McAllester. 2007.
The generalized A* architecture. Journal of Artifi-
cial Intelligence Research (JAIR), 29:153–190.
Eraldo Rezende Fernandes, Cicero Nogueira dos San-
tos, and Ruy Luiz Milidi´u. 2012. Latent structure
perceptron with feature induction for unrestricted
coreference resolution. International Conference on
Computational Natural Language Learning (CoNL-
L), pages 41–48.
Thomas Finley and Thorsten Joachims. 2005. Su-
pervised clustering with support vector machines.
In Proceedings of International Conference on Ma-
chine Learning (ICML), pages 217–224.
Yoav Goldberg and Joakim Nivre. 2013. Training
deterministic parsers with non-deterministic oracles.
Transactions of the Association for Computational
Linguistics, 1:403–414.
Aria Haghighi and Dan Klein. 2010. Coreference res-
olution in a modular, entity-centered model. In Pro-
ceedings of Human Language Technology Confer-
ence of the North American Chapter of the Associa-
tion of Computational Linguistics (HLT-NAACL).
Hannaneh Hajishirzi, Leila Zilles, Daniel S. Weld,
and Luke S. Zettlemoyer. 2013. Joint corefer-
ence resolution and named-entity linking with multi-
pass sieves. In Proceedings of Conference on Em-
pirical Methods in Natural Language Processing
(EMNLP), pages 289–299.
Hal Daum´e III, John Langford, and Daniel Marcu.
2009. Search-based structured prediction. Machine
Learning Journal (MLJ), 75(3):297–325.
Hal Daum´e III and Daniel Marcu. 2005. Learning
as search optimization: Approximate large margin
methods for structured prediction. In ICML.
Roni Khardon. 1999. Learning to take actions. Ma-
chine Learning Journal (MLJ), 35(1):57–90.
Michael Lam, Janardhan Rao Doppa, Xu Hu, Sinisa
Todorovic, Thomas Dietterich, Abigail Reft, and
Marymegan Daly. 2013. Learning to detect basal
tubules of nematocysts in sem images. In ICCV
Workshop on Computer Vision for Accelerated Bio-
sciences (CVAB). IEEE.
Heeyoung Lee, Angel X. Chang, Yves Peirsman,
Nathanael Chambers, Mihai Surdeanu, and Dan Ju-
rafsky. 2013. Deterministic coreference resolu-
tion based on entity-centric, precision-ranked rules.
Computational Linguistics, 39(4):885–916.
Qi Li, Heng Ji, and Liang Huang. 2013. Joint event
extraction via structured prediction with global fea-
tures. In Proceedings of the 51st Annual Meeting
of the Association for Computational Linguistics (A-
CL), pages 73–82.
Tie-Yan Liu. 2009. Learning to rank for information
retrieval. Foundations and Trends in Information
Retrieval, 3(3):225–331.
Xiaoqiang Luo. 2005. On coreference resolution per-
formance metrics. In Proceedings of the Confer-
ence on Human Language Technology and Empir-
ical Methods in Natural Language Processing, HLT
’05, pages 25–32, Stroudsburg, PA, USA. Associa-
tion for Computational Linguistics.
Andrew Mccallum and Ben Wellner. 2003. To-
ward conditional models of identity uncertainty with
application to proper noun coreference. In Pro-
ceedings of Neural Information Processing Systems
(NIPS), pages 905–912. MIT Press.
MUC6. 1995. Coreference task definition. In Pro-
ceedings of the Sixth Message Understanding Con-
ference (MUC-6), pages 335–344.
</reference>
<page confidence="0.811687">
2125
</page>
<reference confidence="0.99976641025641">
Vincent Ng and Claire Cardie. 2002. Improving
machine learning approaches to coreference resolu-
tion. In Proceedings ofAssociation of Computation-
al Linguistics (ACL) Conference, pages 104–111.
NIST. 2004. The ACE evaluation plan.
Joakim Nivre, Johan Hall, Jens Nilsson, Atanas
Chanev, G¨ulsen Eryigit, Sandra K¨ubler, Svetoslav
Marinov, and Erwin Marsi. 2007. Maltparser:
A language-independent system for data-driven de-
pendency parsing. Natural Language Engineering,
13(2):95–135.
Sameer Pradhan, Alessandro Moschitti, Nianwen Xue,
Olga Uryupina, and Yuchen Zhang. 2012. Conll-
2012 shared task: Modeling multilingual unrestrict-
ed coreference in ontonotes. In Proceedings of the
Joint Conference on EMNLP and CoNLL: Shared
Task, pages 1–40.
Altaf Rahman and Vincent Ng. 2011a. Coreference
resolution with world knowledge. In Proceedings
of Association of Computational Linguistics (ACL)
Conference, pages 814–824.
Altaf Rahman and Vincent Ng. 2011b. Narrowing the
modeling gap: A cluster-ranking approach to coref-
erence resolution. Journal of Artificial Intelligence
Research (JAIR), 40:469–521.
Lev-Arie Ratinov and Dan Roth. 2012. Learning-
based multi-sieve co-reference resolution with
knowledge. In Proceedings of Empirical Methods
in Natural Language Processing (EMNLP) Confer-
ence, pages 1234–1244.
St´ephane Ross, Geoffrey J. Gordon, and Drew Bagnell.
2011. A reduction of imitation learning and struc-
tured prediction to no-regret online learning. Jour-
nal of Machine Learning Research - Proceedings
Track, 15:627–635.
Wee Meng Soon, Daniel Chung, Daniel Chung Yong
Lim, Yong Lim, and Hwee Tou Ng. 2001. A ma-
chine learning approach to coreference resolution of
noun phrases.
Veselin Stoyanov and Jason Eisner. 2012. Easy-first
coreference resolution. In Proceedings of Inter-
national Conference on Computational Linguistics
(COLING), pages 2519–2534.
Veselin Stoyanov, Claire Cardie, Nathan Gilbert, Ellen
Riloff, David Buttler, and David Hysom. 2010.
Coreference resolution with reconcile. In Proceed-
ings ofAssociation of Computational Linguistics (A-
CL) Conference, pages 156–161.
Marc B. Vilain, John D. Burger, John S. Aberdeen,
Dennis Connolly, and Lynette Hirschman. 1995.
A model-theoretic coreference scoring scheme. In
MUC, pages 45–52.
David Weiss and Benjamin Taskar. 2010. Structured
prediction cascades. Journal of Machine Learning
Research - Proceedings Track, 9:916–923.
Michael L. Wick, Khashayar Rohanimanesh, Kedar
Bellare, Aron Culotta, and Andrew McCallum.
2011. SampleRank: Training factor graphs with
atomic gradients. In Proceedings of International
Conference on Machine Learning (ICML).
Michael L. Wick, Sameer Singh, and Andrew McCal-
lum. 2012. A discriminative hierarchical model for
fast coreference at large scale. In Proceedings ofAs-
sociation of Computational Linguistics (ACL) Con-
ference, pages 379–388.
Yuehua Xu, Alan Fern, and Sung Wook Yoon. 2009.
Learning linear ranking functions for beam search
with application to planning. Journal of Machine
Learning Research (JMLR), 10:1571–1610.
Chun-Nam John Yu and Thorsten Joachims. 2009.
Learning structural SVMs with latent variables. In
Proceedings of International Conference on Ma-
chine Learning (ICML).
Jiaping Zheng, Luke Vilnis, Sameer Singh, Jinho D.
Choi, and Andrew McCallum. 2013. Dynamic
knowledge-base alignment for coreference resolu-
tion. In Conference on Computational Natural Lan-
guage Learning (CoNLL).
</reference>
<page confidence="0.993981">
2126
</page>
</variant>
</algorithm>
<algorithm name="ParsHed" version="110505">
<variant no="0" confidence="0.398014">
<title confidence="0.999245">Prune-and-Score: Learning for Greedy Coreference Resolution</title>
<author confidence="0.794232">Janardhan Rao J Walker Orr Ma</author>
<author confidence="0.794232">Prashanth Fern</author>
<author confidence="0.794232">Tom Dietterich</author>
<affiliation confidence="0.8050355">School of Electrical Engineering and Computer Science, Oregon State of Electrical Engineering and Computer Science, Washington State</affiliation>
<email confidence="0.999601">jana@eecs.wsu.edu</email>
<abstract confidence="0.99918325">We propose a novel search-based approach for greedy coreference resolution, where the mentions are processed in order and added to previous coreference clusters. Our method is distinguished by the use of two functions to make each coreferdecision: a that prunes bad coreference decisions from furconsideration, and a that then selects the best among the remaining decisions. Our framework reduces learning of these functions to rank learning, which helps leverage powerful off-the-shelf rank-learners. We show that is superior to using a single scoring function to make both decisions and outperforms several state-of-the-art approaches on multiple benchmark corpora including OntoNotes.</abstract>
</variant>
</algorithm>
<algorithm name="ParsCit" version="110505">
<citationList>
<citation valid="true">
<authors>
<author>Amit Bagga</author>
<author>Breck Baldwin</author>
</authors>
<title>Algorithms for scoring coreference chains. In</title>
<date>1998</date>
<booktitle>In The First International Conference on Language Resources and Evaluation Workshop on Linguistics Coreference,</booktitle>
<pages>563--566</pages>
<contexts>
<context position="26885" citStr="Bagga and Baldwin, 1998" startWordPosition="4455" endWordPosition="4459">ST) for testing. We also evaluate our system on the 128 newswire documents in ACE 2004 corpus for a fair comparison with the state-of-the-art. The MUC6 corpus containts 255 documents. We employ the official test set of 30 documents (MUC6- TEST) for testing purposes. From the remaining 225 documents, which includes 195 official training documents and 30 dry-run test documents, we randomly pick 30 documents for validation, and use the remaining ones for training. Evaluation Metrics. We compute three most popular performance metrics for coreference resolution: MUC (Vilain et al., 1995), B-Cubed (Bagga and Baldwin, 1998), and Entity-based CEAF (CEAFφ4) (Luo, 2005). As it is commonly done in CoNLL shared tasks (Pradhan et al., 2012), we employ the average F1 score (CoNLL F1) of these three metrics for comparison purposes. We evaluate all the results using the updated version1 (7.0) of the coreference scorer. Features. We built2 our coreference resolver based on the Easy-first coreference system (Stoyanov and Eisner, 2012), which is derived from the Reconcile system (Stoyanov et al., 2010). We essentially employ the same features as in the Easyfirst system. However, we provide some highlevel details that are ne</context>
</contexts>
<marker>Bagga, Baldwin, 1998</marker>
<rawString>Amit Bagga and Breck Baldwin. 1998. Algorithms for scoring coreference chains. In In The First International Conference on Language Resources and Evaluation Workshop on Linguistics Coreference, pages 563–566.</rawString>
</citation>
<citation valid="true">
<authors>
<author>Eric Bengtson</author>
<author>Dan Roth</author>
</authors>
<title>Understanding the value of features for coreference resolution.</title>
<date>2008</date>
<booktitle>In Proceedings of Empirical Methods in Natural Language Processing (EMNLP),</booktitle>
<pages>294--303</pages>
<contexts>
<context position="1693" citStr="Bengtson and Roth, 2008" startWordPosition="238" endWordPosition="241">h decisions and outperforms several state-of-the-art approaches on multiple benchmark corpora including OntoNotes. 1 Introduction Coreference resolution is the task of clustering a set of mentions in the text such that all mentions in the same cluster refer to the same entity. It is one of the first stages in deep language understanding and has a big potential impact on the rest of the stages. Several of the state-of-the-art approaches learn a scoring function defined over mention pair, cluster-mention or cluster-cluster pair to guide the coreference decision-making process (Daum´e III, 2006; Bengtson and Roth, 2008; Rahman and Ng, 2011b; Stoyanov and Eisner, 2012; Chang et al., 2013; Durrett et al., 2013; Durrett and Klein, 2013). One common and persistent problem with these approaches is that the scoring function has to make all the coreference decisions, which leads to a highly non-realizable learning problem. Inspired by the recent success of the HC-Search Framework (Doppa et al., 2014a) for studying a variety of structured prediction problems (Lam et al., 2013; Doppa et al., 2014c), we study a novel approach for search-based coreference resolution called Prune-and-Score. HC-Search is a divideand-con</context>
<context position="5770" citStr="Bengtson and Roth, 2008" startWordPosition="880" endWordPosition="883">emainder of the paper proceeds as follows. In Section 2, we dicuss the related work. We introduce our problem setup in Section 3 and then describe our Prune-and-Score approach in Section 4. We explain our approaches for learning the pruning and scoring functions in Section 5. Section 6 presents our experimental results followed by the conclusions in Section 7. 2 Related Work The work on learning-based coreference resolution can be broadly classified into three types. First, the pair-wise classifier approaches learn a classifier on mention pairs (edges) (Soon et al., 2001; Ng and Cardie, 2002; Bengtson and Roth, 2008), and perform some form of approximate decoding or post-processing using the pair-wise scores to make predictions. However, the pair-wise classifier approach suffers from several drawbacks including class imbalance (fewer positive edges compared to negative edges) and not being able to leverage the global structure (instead making independent local decisions). Second, the global approaches such as Structured SVMs and Conditional Random Fields (CRFs) learn a cost function to score a potential clustering output for a given input set of mentions (Mccallum and Wellner, 2003; Finley and Joachims, 2</context>
<context position="26128" citStr="Bengtson and Roth, 2008" startWordPosition="4337" endWordPosition="4340">), and MUC6 (MUC6, 1995) – and compare it against the state-of-the-art approaches for coreference resolution. For OntoNotes data, we report the results on both gold mentions and predicted mentions. We also report the results on gold mentions for ACE 2004 and MUC6 data. 2120 6.1 Experimental Setup Datasets. For OntoNotes corpus, we employ the official split for training, validation, and testing. There are 2802 documents in the training set; 343 documents in the validation set; and 345 documents in the testing set. The ACE 2004 corpus contains 443 documents. We follow the (Culotta et al., 2007; Bengtson and Roth, 2008) split in our experiments by employing 268 documents for training, 68 documents for validation, and 107 documents (ACE2004-CULOTTA-TEST) for testing. We also evaluate our system on the 128 newswire documents in ACE 2004 corpus for a fair comparison with the state-of-the-art. The MUC6 corpus containts 255 documents. We employ the official test set of 30 documents (MUC6- TEST) for testing purposes. From the remaining 225 documents, which includes 195 official training documents and 30 dry-run test documents, we randomly pick 30 documents for validation, and use the remaining ones for training. E</context>
</contexts>
<marker>Bengtson, Roth, 2008</marker>
<rawString>Eric Bengtson and Dan Roth. 2008. Understanding the value of features for coreference resolution. In Proceedings of Empirical Methods in Natural Language Processing (EMNLP), pages 294–303.</rawString>
</citation>
<citation valid="true">
<authors>
<author>Anders Bj¨orkelund</author>
<author>Jonas Kuhn</author>
</authors>
<title>Learning structured perceptrons for coreference resolution with latent antecedents and non-local features.</title>
<date>2014</date>
<booktitle>In Proceedings of the 52nd Annual Meeting of the Association for Computational Linguistics (Volume 1: Long Papers),</booktitle>
<pages>47--57</pages>
<publisher>Association</publisher>
<institution>for Computational Linguistics.</institution>
<location>Baltimore, Maryland,</location>
<marker>Bj¨orkelund, Kuhn, 2014</marker>
<rawString>Anders Bj¨orkelund and Jonas Kuhn. 2014. Learning structured perceptrons for coreference resolution with latent antecedents and non-local features. In Proceedings of the 52nd Annual Meeting of the Association for Computational Linguistics (Volume 1: Long Papers), pages 47–57, Baltimore, Maryland, June. Association for Computational Linguistics.</rawString>
</citation>
<citation valid="true">
<authors>
<author>Christopher Burges</author>
</authors>
<title>From RankNet to LambdaRank to LambdaMART: An overview.</title>
<date>2010</date>
<tech>Microsoft Technical Report, (MSR-TR-2010).</tech>
<contexts>
<context position="21585" citStr="Burges, 2010" startWordPosition="3576" endWordPosition="3577"> example (x, y∗), we create an example by labeling optimal action a∗i E A(s∗i ) as the only relevant action, and then try to learn 2119 a ranking function that can rank actions such that the relevant action a∗i is in the top b actions, where b is the input pruning paramter. In other words, we have a rank learning problem, where the learner’s goal is to optimize the Precision at Topb. The training approach creates such an example for each state s in the solution path. The set of aggregate imitation examples collected over all the training data is then given to a rank learner (e.g., LambdaMART (Burges, 2010)) to learn the parameters of Fprune by optimizing the Precision at Top-b loss. See appendix for the pseudocode. If we can learn a function Fprune that is consistent with these imitation examples, then the learned pruning function is guaranteed to keep the solution path within the pruned space for all the training examples. We can also employ more advanced imitation learning algorithms including DAgger (Ross et al., 2011) and SEARN (Hal Daum´e III et al., 2009) if we are provided with an (approximate) optimal scoring function F∗score that can pick optimal actions at states that are not in the s</context>
<context position="29256" citStr="Burges, 2010" startWordPosition="4843" endWordPosition="4844"> in the Easy-first system). The indicator feature will be 1 for the NEW action and 0 for all other actions.We have a total of 140 features: 90 mention pair features; 49 entity pair features; and one NEW indicator feature. We believe that our approach can benefit from employing features of the mention for the NEW action (Rahman and Ng, 2011b; Durrett and Klein, 2013). However, we were constrained by the Reconcile system and could not leverage these features for the NEW action. Base Rank-Learner. Our pruning and scoring function learning algorithms need a base ranklearner. We employ LambdaMART (Burges, 2010), a state-of-the art rank learner from the RankLib3 library. LambdaMART is a variant of boosted regression trees. We use a learning rate of 0.1, specify the maximum number of boosting iterations (or trees) as 1000 noting that its actual value is automatically decided based on the validation set, and tune the number of leaves per tree based on the validation data. Once we fix the hyper-parameters of LambdaMART, we train the final model on all of the training data. LambdaMART uses an internal train/validation split of the input ranking examples to decide when to stop the boosting iterations. We </context>
</contexts>
<marker>Burges, 2010</marker>
<rawString>Christopher Burges. 2010. From RankNet to LambdaRank to LambdaMART: An overview. Microsoft Technical Report, (MSR-TR-2010).</rawString>
</citation>
<citation valid="true">
<authors>
<author>Kai-Wei Chang</author>
<author>Rajhans Samdani</author>
<author>Alla Rozovskaya</author>
<author>Mark Sammons</author>
<author>Dan Roth</author>
</authors>
<title>IllinoisCoref: The UI system in the CoNLL-2012 shared task.</title>
<date>2012</date>
<booktitle>In Joint Conference on EMNLP and CoNLL - Shared Task,</booktitle>
<pages>113--117</pages>
<institution>Jeju Island, Korea, July. Association for Computational Linguistics.</institution>
<contexts>
<context position="30357" citStr="Chang et al., 2012" startWordPosition="5024" endWordPosition="5027">s an internal train/validation split of the input ranking examples to decide when to stop the boosting iterations. We fixed this ratio to 0.8 noting that the performance is not sensitive to this parameter. For scoring function learning, we used 5 folds for the cross-validation training. Pruning Parameter b. The hyper-parameter b controls the amount of pruning in our Prune-andScore approach. We perform experiments with different values of b and pick the best value based on the performance on the validation set. Singleton Mention Filter for OntoNotes Corpus. We employ the Illinois-Coref system (Chang et al., 2012) to extract system mentions for our OntoNotes experiments, and observe that the num3http://sourceforge.net/p/lemur/wiki/RankLib/ 2121 ber of predicted mentions is thrice the number of gold mentions. Since the training data provides the clustering supervision for only gold mentions, it is not clear how to train with the system mentions that are not part of gold mentions. A common way of dealing with this problem is to treat all the extra system mentions as singleton clusters (Durrett and Klein, 2013; Chang et al., 2013). However, this solution most likely will not work with our current feature </context>
</contexts>
<marker>Chang, Samdani, Rozovskaya, Sammons, Roth, 2012</marker>
<rawString>Kai-Wei Chang, Rajhans Samdani, Alla Rozovskaya, Mark Sammons, and Dan Roth. 2012. IllinoisCoref: The UI system in the CoNLL-2012 shared task. In Joint Conference on EMNLP and CoNLL - Shared Task, pages 113–117, Jeju Island, Korea, July. Association for Computational Linguistics.</rawString>
</citation>
<citation valid="true">
<authors>
<author>Kai-Wei Chang</author>
<author>Rajhans Samdani</author>
<author>Dan Roth</author>
</authors>
<title>A constrained latent variable model for coreference resolution.</title>
<date>2013</date>
<booktitle>In Proceedings of Empirical Methods in Natural Language Processing (EMNLP),</booktitle>
<pages>601--612</pages>
<contexts>
<context position="1762" citStr="Chang et al., 2013" startWordPosition="250" endWordPosition="253">e benchmark corpora including OntoNotes. 1 Introduction Coreference resolution is the task of clustering a set of mentions in the text such that all mentions in the same cluster refer to the same entity. It is one of the first stages in deep language understanding and has a big potential impact on the rest of the stages. Several of the state-of-the-art approaches learn a scoring function defined over mention pair, cluster-mention or cluster-cluster pair to guide the coreference decision-making process (Daum´e III, 2006; Bengtson and Roth, 2008; Rahman and Ng, 2011b; Stoyanov and Eisner, 2012; Chang et al., 2013; Durrett et al., 2013; Durrett and Klein, 2013). One common and persistent problem with these approaches is that the scoring function has to make all the coreference decisions, which leads to a highly non-realizable learning problem. Inspired by the recent success of the HC-Search Framework (Doppa et al., 2014a) for studying a variety of structured prediction problems (Lam et al., 2013; Doppa et al., 2014c), we study a novel approach for search-based coreference resolution called Prune-and-Score. HC-Search is a divideand-conquer solution that learns multiple components with pre-defined roles,</context>
<context position="3737" citStr="Chang et al., 2013" startWordPosition="564" endWordPosition="567">y learning two different functions: 1) a pruning function to prune most of the bad decisions, and 2) a scoring function to pick the best decision among those that are remaining. Our Prune-and-Score approach is a particular instantiation of the general idea of learning nearly-sound constraints for pruning, and leveraging the learned constraints to learn improved heuristic functions for guiding the search. The pruning constraints can take different forms (e.g., classifiers, decisionlist, or ranking functions) depending on the search architecture. Therefore, other coreference resolution systems (Chang et al., 2013; Durrett and Klein, 2013; Bj¨orkelund and Kuhn, 2014) can also benefit from this idea. While our basic idea of twolevel selection might appear similar to the coarseto-fine inference architectures (Felzenszwalb and McAllester, 2007; Weiss and Taskar, 2010), the details differ significantly. Importantly, our pruning and scoring functions operate sequentially at 2115 Proceedings of the 2014 Conference on Empirical Methods in Natural Language Processing (EMNLP), pages 2115–2126, October 25-29, 2014, Doha, Qatar. c�2014 Association for Computational Linguistics each greedy search step, whereas in </context>
<context position="7147" citStr="Chang et al., 2013" startWordPosition="1092" endWordPosition="1095">s some of the problems with pair-wise classifiers, however, they suffer from the intractability of “Argmin” inference (finding the least cost clustering output among exponential possibilities) that is encountered during both training and testing. As a result, they resort to approximate inference algorithms (e.g., MCMC, loopy belief propagation), which can suffer from local optima. Third, the incremental approaches construct the clustering output incrementally by processing the mentions in some order (Daum´e III, 2006; Denis and Baldridge, 2008; Rahman and Ng, 2011b; Stoyanov and Eisner, 2012; Chang et al., 2013; Durrett et al., 2013; Durrett and Klein, 2013). These methods learn a scoring function to guide the decision-making process and differ in the form of the scoring function (e.g., mention pair, clustermention or cluster-cluster pair) and how it is being learned. They have shown great success and are very efficient. Indeed, several of the approaches that have achieved state-of-the-art results on OntoNotes fall under this category (Chang et al., 2013; Durrett et al., 2013; Durrett and Klein, 2013; Bj¨orkelund and Kuhn, 2014). However, their efficiency requirement leads to a highly nonrealizable </context>
<context position="8495" citStr="Chang et al., 2013" startWordPosition="1297" endWordPosition="1300">set of learned pruning rules) makes the learning problem easier and can improve over the performance of scoring-only approaches. Also, the models in (Chang et al., 2013; Durrett et al., 2013) try to leverage cluster-level information implicitly (via latent antecedents) from mention-pair features, whereas our model explicitly leverages the cluster level information. Coreference resolution systems can benefit by incorporating the world knowledge including rules, constraints, and additional information from external knowledge bases (Lee et al., 2013; Rahman and Ng, 2011a; Ratinov and Roth, 2012; Chang et al., 2013; Zheng et al., 2013; Hajishirzi et al., 2013). Our work is orthogonal to this line of work, but domain constraints and rules can be incorporated into our model as done in (Chang et al., 2013). 3 Problem Setup Coreference resolution is a structured prediction problem where the set of mentions m1, m2, · · · , mD extracted from a document cor2116 reponds to a structured input x and the structured output y corresponds to a partition of the mentions into a set of clusters C1, C2, · · · , Ck. Each mention mi belongs to exactly one of the clusters Cj. We are provided with a training set of inputoutp</context>
<context position="30881" citStr="Chang et al., 2013" startWordPosition="5109" endWordPosition="5112">n Mention Filter for OntoNotes Corpus. We employ the Illinois-Coref system (Chang et al., 2012) to extract system mentions for our OntoNotes experiments, and observe that the num3http://sourceforge.net/p/lemur/wiki/RankLib/ 2121 ber of predicted mentions is thrice the number of gold mentions. Since the training data provides the clustering supervision for only gold mentions, it is not clear how to train with the system mentions that are not part of gold mentions. A common way of dealing with this problem is to treat all the extra system mentions as singleton clusters (Durrett and Klein, 2013; Chang et al., 2013). However, this solution most likely will not work with our current feature representation (i.e., NEW action is represented as a single indicator feature). Recall that to predict these extra system mentions as singleton clusters with our incremental clustering approach, the learned model should first predict a NEW action while processing these mentions to form a temporary singleton cluster, and then refrain from merging any of the subsequent mentions with that cluster so that it becomes a singleton cluster in the final clustering output. However, in OntoNotes corpus, the training data does not</context>
<context position="38038" citStr="Chang et al., 2013" startWordPosition="6300" endWordPosition="6303">8 62.64 6 70.93 58.66 57.85 62.48 8 70.12 58.13 57.37 61.87 10 70.24 58.34 56.27 61.61 20 67.97 57.73 56.63 60.78 ∞ 67.03 56.31 55.56 59.63 Table 3: Performance of Prune-and-Score approach with different values of the pruning parameter b. For b = ∞, Prune-and-Score becomes an Only-Scoring algorithm. Comparison to State-of-the-Art. Table 4 shows the results of our Prune-and-Score approach compared with the following state-of-theart coreference resolution approaches: HOTCoref system (Bj¨orkelund and Kuhn, 2014); Berkeley system with the FINAL feature set (Durrett and Klein, 2013); CPL3M system (Chang et al., 2013); Stanford system (Lee et al., 2013); Easy-first system (Stoyanov and Eisner, 2012); and Fernandes et al., 2012 (Fernandes et al., 2012). Only Scoring is the special case of our Prune-andScore approach where we employ only the scoring function. This corresponds to existing incremental approaches (Daum´e III, 2006; Rahman and Ng, 2011b). We report the best published results for CPL3M system, Easy-first, and Fernandes et al., 2012. We ran the publicly available software to generate the results for Berkeley and Stanford systems with the updated CoNLL scorer. We include the results of Prune-and-Sc</context>
<context position="39749" citStr="Chang et al., 2013" startWordPosition="6569" endWordPosition="6572"> effectively guide the search. We showed that our approach improves over the methods that only learn a scoring function, and gives comparable or better results than several state-ofthe-art coreference resolution systems. Our Prune-and-Score approach is a particular instantiation of the general idea of learning nearlysound constraints for pruning, and leveraging the learned constraints to learn improved heuristic functions for guiding the search (See (Chen et al., 2014) for another instantiation of this idea for multi-object tracking in videos). Therefore, other coreference resolution systems (Chang et al., 2013; Durrett and Klein, 2013; Bj¨orkelund and Kuhn, 2014) can also benefit from this idea. One way to further improve the peformance of our approach is to perform a search in the Limited Discrepancy Search (LDS) space (Doppa et al., 2014b) using the learned functions. Future work should apply this general idea to other natural language processing tasks including dependency parsing (Nivre et al., 2007) and information extraction (Li et al., 2013). We would expect more beneficial behavior with the pruning constraints for problems with large action sets (e.g., labeled dependency parsing). It would b</context>
</contexts>
<marker>Chang, Samdani, Roth, 2013</marker>
<rawString>Kai-Wei Chang, Rajhans Samdani, and Dan Roth. 2013. A constrained latent variable model for coreference resolution. In Proceedings of Empirical Methods in Natural Language Processing (EMNLP), pages 601–612.</rawString>
</citation>
<citation valid="true">
<authors>
<author>Sheng Chen</author>
<author>Alan Fern</author>
<author>Sinisa Todorovic</author>
</authors>
<title>Multi-object tracking via constrained sequential labeling.</title>
<date>2014</date>
<booktitle>In To appear in Proceedings of IEEE Conference on Computer Vision and Pattern Recognition (CVPR).</booktitle>
<contexts>
<context position="39604" citStr="Chen et al., 2014" startWordPosition="6548" endWordPosition="6551">ed the Prune-and-Score approach for greedy coreference resolution whose main idea is to learn a pruning function along with a scoring function to effectively guide the search. We showed that our approach improves over the methods that only learn a scoring function, and gives comparable or better results than several state-ofthe-art coreference resolution systems. Our Prune-and-Score approach is a particular instantiation of the general idea of learning nearlysound constraints for pruning, and leveraging the learned constraints to learn improved heuristic functions for guiding the search (See (Chen et al., 2014) for another instantiation of this idea for multi-object tracking in videos). Therefore, other coreference resolution systems (Chang et al., 2013; Durrett and Klein, 2013; Bj¨orkelund and Kuhn, 2014) can also benefit from this idea. One way to further improve the peformance of our approach is to perform a search in the Limited Discrepancy Search (LDS) space (Doppa et al., 2014b) using the learned functions. Future work should apply this general idea to other natural language processing tasks including dependency parsing (Nivre et al., 2007) and information extraction (Li et al., 2013). We woul</context>
</contexts>
<marker>Chen, Fern, Todorovic, 2014</marker>
<rawString>Sheng Chen, Alan Fern, and Sinisa Todorovic. 2014. Multi-object tracking via constrained sequential labeling. In To appear in Proceedings of IEEE Conference on Computer Vision and Pattern Recognition (CVPR).</rawString>
</citation>
<citation valid="true">
<authors>
<author>William W Cohen</author>
<author>Vitor Rocha de Carvalho</author>
</authors>
<title>Stacked sequential learning.</title>
<date>2005</date>
<booktitle>In Proceedings of International Joint Conference on Artificial Intelligence (IJCAI),</booktitle>
<pages>671--676</pages>
<marker>Cohen, de Carvalho, 2005</marker>
<rawString>William W. Cohen and Vitor Rocha de Carvalho. 2005. Stacked sequential learning. In Proceedings of International Joint Conference on Artificial Intelligence (IJCAI), pages 671–676.</rawString>
</citation>
<citation valid="true">
<authors>
<author>Michael Collins</author>
</authors>
<title>Discriminative reranking for natural language parsing.</title>
<date>2000</date>
<booktitle>In Proceedings of International Conference on Machine Learning (ICML),</booktitle>
<pages>175--182</pages>
<contexts>
<context position="23492" citStr="Collins, 2000" startWordPosition="3905" endWordPosition="3906">proach is to learn the parameters of Fscore such that at each state ˆsi, a∗i ∈ A0 is ranked higher than all other actions in A0, where A0 C A(ˆsi) is the set of b actions that remain after pruning. It is important to note that the distribution of states in the pruned space due to Fprune on the testing data may be somewhat different from those on training data. Therefore, we train our scoring function via cross-validation by training the scoring function on heldout data that was not used to train the pruning function. This methodology is commonly employed in Re-Ranking and Stacking approaches (Collins, 2000; Cohen and de Carvalho, 2005). Our scoring function learning procedure uses cross validation and consists of the following four steps. First, we divide the training data D into k folds. Second, we learn k different pruners, where each pruning functionFiprune is learned using the data from all the folds excluding the ith fold. Third, we generate ranking examples for scoring function learning as described above using each pruning functionFiprune on the data it was not trained on. Finally, we give the aggregate set of ranking examples R to a rank learner (e.g., SVM-Rank or LambdaMART) to learn t</context>
</contexts>
<marker>Collins, 2000</marker>
<rawString>Michael Collins. 2000. Discriminative reranking for natural language parsing. In Proceedings of International Conference on Machine Learning (ICML), pages 175–182.</rawString>
</citation>
<citation valid="true">
<authors>
<author>Aron Culotta</author>
<author>Michael L Wick</author>
<author>Andrew McCallum</author>
</authors>
<title>First-order probabilistic models for coreference resolution.</title>
<date>2007</date>
<booktitle>In Proceedings of Human Language Technology Conference of the North American Chapter of the Association of Computational Linguistics (HLT-NAACL),</booktitle>
<pages>81--88</pages>
<contexts>
<context position="6395" citStr="Culotta et al., 2007" startWordPosition="977" endWordPosition="980"> perform some form of approximate decoding or post-processing using the pair-wise scores to make predictions. However, the pair-wise classifier approach suffers from several drawbacks including class imbalance (fewer positive edges compared to negative edges) and not being able to leverage the global structure (instead making independent local decisions). Second, the global approaches such as Structured SVMs and Conditional Random Fields (CRFs) learn a cost function to score a potential clustering output for a given input set of mentions (Mccallum and Wellner, 2003; Finley and Joachims, 2005; Culotta et al., 2007; Yu and Joachims, 2009; Haghighi and Klein, 2010; Wick et al., 2011; Wick et al., 2012; Fernandes et al., 2012). These methods address some of the problems with pair-wise classifiers, however, they suffer from the intractability of “Argmin” inference (finding the least cost clustering output among exponential possibilities) that is encountered during both training and testing. As a result, they resort to approximate inference algorithms (e.g., MCMC, loopy belief propagation), which can suffer from local optima. Third, the incremental approaches construct the clustering output incrementally by</context>
<context position="26102" citStr="Culotta et al., 2007" startWordPosition="4332" endWordPosition="4336">, ACE 2004 (NIST, 2004), and MUC6 (MUC6, 1995) – and compare it against the state-of-the-art approaches for coreference resolution. For OntoNotes data, we report the results on both gold mentions and predicted mentions. We also report the results on gold mentions for ACE 2004 and MUC6 data. 2120 6.1 Experimental Setup Datasets. For OntoNotes corpus, we employ the official split for training, validation, and testing. There are 2802 documents in the training set; 343 documents in the validation set; and 345 documents in the testing set. The ACE 2004 corpus contains 443 documents. We follow the (Culotta et al., 2007; Bengtson and Roth, 2008) split in our experiments by employing 268 documents for training, 68 documents for validation, and 107 documents (ACE2004-CULOTTA-TEST) for testing. We also evaluate our system on the 128 newswire documents in ACE 2004 corpus for a fair comparison with the state-of-the-art. The MUC6 corpus containts 255 documents. We employ the official test set of 30 documents (MUC6- TEST) for testing purposes. From the remaining 225 documents, which includes 195 official training documents and 30 dry-run test documents, we randomly pick 30 documents for validation, and use the rema</context>
</contexts>
<marker>Culotta, Wick, McCallum, 2007</marker>
<rawString>Aron Culotta, Michael L. Wick, and Andrew McCallum. 2007. First-order probabilistic models for coreference resolution. In Proceedings of Human Language Technology Conference of the North American Chapter of the Association of Computational Linguistics (HLT-NAACL), pages 81–88.</rawString>
</citation>
<citation valid="true">
<authors>
<author>Hal Daum´e</author>
</authors>
<title>Practical Structured Learning Techniques for Natural Language Processing.</title>
<date>2006</date>
<tech>Ph.D. thesis,</tech>
<institution>University of Southern</institution>
<location>California, Los Angeles, CA.</location>
<marker>Daum´e, 2006</marker>
<rawString>Hal Daum´e III. 2006. Practical Structured Learning Techniques for Natural Language Processing. Ph.D. thesis, University of Southern California, Los Angeles, CA.</rawString>
</citation>
<citation valid="true">
<authors>
<author>Pascal Denis</author>
<author>Jason Baldridge</author>
</authors>
<title>Specialized models and ranking for coreference resolution.</title>
<date>2008</date>
<booktitle>In Proceedings of Empirical Methods in Natural Language Processing (EMNLP),</booktitle>
<pages>660--669</pages>
<contexts>
<context position="7078" citStr="Denis and Baldridge, 2008" startWordPosition="1079" endWordPosition="1083"> al., 2011; Wick et al., 2012; Fernandes et al., 2012). These methods address some of the problems with pair-wise classifiers, however, they suffer from the intractability of “Argmin” inference (finding the least cost clustering output among exponential possibilities) that is encountered during both training and testing. As a result, they resort to approximate inference algorithms (e.g., MCMC, loopy belief propagation), which can suffer from local optima. Third, the incremental approaches construct the clustering output incrementally by processing the mentions in some order (Daum´e III, 2006; Denis and Baldridge, 2008; Rahman and Ng, 2011b; Stoyanov and Eisner, 2012; Chang et al., 2013; Durrett et al., 2013; Durrett and Klein, 2013). These methods learn a scoring function to guide the decision-making process and differ in the form of the scoring function (e.g., mention pair, clustermention or cluster-cluster pair) and how it is being learned. They have shown great success and are very efficient. Indeed, several of the approaches that have achieved state-of-the-art results on OntoNotes fall under this category (Chang et al., 2013; Durrett et al., 2013; Durrett and Klein, 2013; Bj¨orkelund and Kuhn, 2014). H</context>
</contexts>
<marker>Denis, Baldridge, 2008</marker>
<rawString>Pascal Denis and Jason Baldridge. 2008. Specialized models and ranking for coreference resolution. In Proceedings of Empirical Methods in Natural Language Processing (EMNLP), pages 660–669.</rawString>
</citation>
<citation valid="true">
<authors>
<author>Janardhan Rao Doppa</author>
<author>Alan Fern</author>
<author>Prasad Tadepalli</author>
</authors>
<title>HC-Search: A learning framework for search-based structured prediction.</title>
<date>2014</date>
<journal>Journal of Artificial Intelligence Research (JAIR),</journal>
<pages>50--369</pages>
<contexts>
<context position="2074" citStr="Doppa et al., 2014" startWordPosition="299" endWordPosition="302">he stages. Several of the state-of-the-art approaches learn a scoring function defined over mention pair, cluster-mention or cluster-cluster pair to guide the coreference decision-making process (Daum´e III, 2006; Bengtson and Roth, 2008; Rahman and Ng, 2011b; Stoyanov and Eisner, 2012; Chang et al., 2013; Durrett et al., 2013; Durrett and Klein, 2013). One common and persistent problem with these approaches is that the scoring function has to make all the coreference decisions, which leads to a highly non-realizable learning problem. Inspired by the recent success of the HC-Search Framework (Doppa et al., 2014a) for studying a variety of structured prediction problems (Lam et al., 2013; Doppa et al., 2014c), we study a novel approach for search-based coreference resolution called Prune-and-Score. HC-Search is a divideand-conquer solution that learns multiple components with pre-defined roles, and each of them contribute towards the overall goal by making the role of the other components easier. The HCSearch framework operates in the space of complete outputs, and relies on the loss function which is only defined on the complete outputs to drive its learning. Unfortunately, this method does not work</context>
<context position="39983" citStr="Doppa et al., 2014" startWordPosition="6609" endWordPosition="6612">-Score approach is a particular instantiation of the general idea of learning nearlysound constraints for pruning, and leveraging the learned constraints to learn improved heuristic functions for guiding the search (See (Chen et al., 2014) for another instantiation of this idea for multi-object tracking in videos). Therefore, other coreference resolution systems (Chang et al., 2013; Durrett and Klein, 2013; Bj¨orkelund and Kuhn, 2014) can also benefit from this idea. One way to further improve the peformance of our approach is to perform a search in the Limited Discrepancy Search (LDS) space (Doppa et al., 2014b) using the learned functions. Future work should apply this general idea to other natural language processing tasks including dependency parsing (Nivre et al., 2007) and information extraction (Li et al., 2013). We would expect more beneficial behavior with the pruning constraints for problems with large action sets (e.g., labeled dependency parsing). It would be interesting and useful to generalize this approach to search spaces where there are multiple target paths from the initial state to the terminal state, e.g., as in the Easy-first framework. Acknowledgments Authors would like to than</context>
</contexts>
<marker>Doppa, Fern, Tadepalli, 2014</marker>
<rawString>Janardhan Rao Doppa, Alan Fern, and Prasad Tadepalli. 2014a. HC-Search: A learning framework for search-based structured prediction. Journal of Artificial Intelligence Research (JAIR), 50:369–407.</rawString>
</citation>
<citation valid="true">
<authors>
<author>Janardhan Rao Doppa</author>
<author>Alan Fern</author>
<author>Prasad Tadepalli</author>
</authors>
<title>Structured prediction via output space search.</title>
<date>2014</date>
<journal>Journal of Machine Learning Research (JMLR),</journal>
<pages>15--1317</pages>
<contexts>
<context position="2074" citStr="Doppa et al., 2014" startWordPosition="299" endWordPosition="302">he stages. Several of the state-of-the-art approaches learn a scoring function defined over mention pair, cluster-mention or cluster-cluster pair to guide the coreference decision-making process (Daum´e III, 2006; Bengtson and Roth, 2008; Rahman and Ng, 2011b; Stoyanov and Eisner, 2012; Chang et al., 2013; Durrett et al., 2013; Durrett and Klein, 2013). One common and persistent problem with these approaches is that the scoring function has to make all the coreference decisions, which leads to a highly non-realizable learning problem. Inspired by the recent success of the HC-Search Framework (Doppa et al., 2014a) for studying a variety of structured prediction problems (Lam et al., 2013; Doppa et al., 2014c), we study a novel approach for search-based coreference resolution called Prune-and-Score. HC-Search is a divideand-conquer solution that learns multiple components with pre-defined roles, and each of them contribute towards the overall goal by making the role of the other components easier. The HCSearch framework operates in the space of complete outputs, and relies on the loss function which is only defined on the complete outputs to drive its learning. Unfortunately, this method does not work</context>
<context position="39983" citStr="Doppa et al., 2014" startWordPosition="6609" endWordPosition="6612">-Score approach is a particular instantiation of the general idea of learning nearlysound constraints for pruning, and leveraging the learned constraints to learn improved heuristic functions for guiding the search (See (Chen et al., 2014) for another instantiation of this idea for multi-object tracking in videos). Therefore, other coreference resolution systems (Chang et al., 2013; Durrett and Klein, 2013; Bj¨orkelund and Kuhn, 2014) can also benefit from this idea. One way to further improve the peformance of our approach is to perform a search in the Limited Discrepancy Search (LDS) space (Doppa et al., 2014b) using the learned functions. Future work should apply this general idea to other natural language processing tasks including dependency parsing (Nivre et al., 2007) and information extraction (Li et al., 2013). We would expect more beneficial behavior with the pruning constraints for problems with large action sets (e.g., labeled dependency parsing). It would be interesting and useful to generalize this approach to search spaces where there are multiple target paths from the initial state to the terminal state, e.g., as in the Easy-first framework. Acknowledgments Authors would like to than</context>
</contexts>
<marker>Doppa, Fern, Tadepalli, 2014</marker>
<rawString>Janardhan Rao Doppa, Alan Fern, and Prasad Tadepalli. 2014b. Structured prediction via output space search. Journal of Machine Learning Research (JMLR), 15:1317–1350.</rawString>
</citation>
<citation valid="true">
<authors>
<author>Janardhan Rao Doppa</author>
<author>Jun Yu</author>
<author>Chao Ma</author>
<author>Alan Fern</author>
<author>Prasad Tadepalli</author>
</authors>
<title>HC-Search for multilabel prediction: An empirical study.</title>
<date>2014</date>
<booktitle>In Proceedings of AAAI Conference on Artificial Intelligence (AAAI).</booktitle>
<contexts>
<context position="2074" citStr="Doppa et al., 2014" startWordPosition="299" endWordPosition="302">he stages. Several of the state-of-the-art approaches learn a scoring function defined over mention pair, cluster-mention or cluster-cluster pair to guide the coreference decision-making process (Daum´e III, 2006; Bengtson and Roth, 2008; Rahman and Ng, 2011b; Stoyanov and Eisner, 2012; Chang et al., 2013; Durrett et al., 2013; Durrett and Klein, 2013). One common and persistent problem with these approaches is that the scoring function has to make all the coreference decisions, which leads to a highly non-realizable learning problem. Inspired by the recent success of the HC-Search Framework (Doppa et al., 2014a) for studying a variety of structured prediction problems (Lam et al., 2013; Doppa et al., 2014c), we study a novel approach for search-based coreference resolution called Prune-and-Score. HC-Search is a divideand-conquer solution that learns multiple components with pre-defined roles, and each of them contribute towards the overall goal by making the role of the other components easier. The HCSearch framework operates in the space of complete outputs, and relies on the loss function which is only defined on the complete outputs to drive its learning. Unfortunately, this method does not work</context>
<context position="39983" citStr="Doppa et al., 2014" startWordPosition="6609" endWordPosition="6612">-Score approach is a particular instantiation of the general idea of learning nearlysound constraints for pruning, and leveraging the learned constraints to learn improved heuristic functions for guiding the search (See (Chen et al., 2014) for another instantiation of this idea for multi-object tracking in videos). Therefore, other coreference resolution systems (Chang et al., 2013; Durrett and Klein, 2013; Bj¨orkelund and Kuhn, 2014) can also benefit from this idea. One way to further improve the peformance of our approach is to perform a search in the Limited Discrepancy Search (LDS) space (Doppa et al., 2014b) using the learned functions. Future work should apply this general idea to other natural language processing tasks including dependency parsing (Nivre et al., 2007) and information extraction (Li et al., 2013). We would expect more beneficial behavior with the pruning constraints for problems with large action sets (e.g., labeled dependency parsing). It would be interesting and useful to generalize this approach to search spaces where there are multiple target paths from the initial state to the terminal state, e.g., as in the Easy-first framework. Acknowledgments Authors would like to than</context>
</contexts>
<marker>Doppa, Yu, Ma, Fern, Tadepalli, 2014</marker>
<rawString>Janardhan Rao Doppa, Jun Yu, Chao Ma, Alan Fern, and Prasad Tadepalli. 2014c. HC-Search for multilabel prediction: An empirical study. In Proceedings of AAAI Conference on Artificial Intelligence (AAAI).</rawString>
</citation>
<citation valid="true">
<authors>
<author>Greg Durrett</author>
<author>Dan Klein</author>
</authors>
<title>Easy victories and uphill battles in coreference resolution.</title>
<date>2013</date>
<booktitle>In Proceedings of the Conference on Empirical Methods in Natural Language Processing (EMNLP),</booktitle>
<pages>pages</pages>
<contexts>
<context position="1810" citStr="Durrett and Klein, 2013" startWordPosition="258" endWordPosition="261"> Introduction Coreference resolution is the task of clustering a set of mentions in the text such that all mentions in the same cluster refer to the same entity. It is one of the first stages in deep language understanding and has a big potential impact on the rest of the stages. Several of the state-of-the-art approaches learn a scoring function defined over mention pair, cluster-mention or cluster-cluster pair to guide the coreference decision-making process (Daum´e III, 2006; Bengtson and Roth, 2008; Rahman and Ng, 2011b; Stoyanov and Eisner, 2012; Chang et al., 2013; Durrett et al., 2013; Durrett and Klein, 2013). One common and persistent problem with these approaches is that the scoring function has to make all the coreference decisions, which leads to a highly non-realizable learning problem. Inspired by the recent success of the HC-Search Framework (Doppa et al., 2014a) for studying a variety of structured prediction problems (Lam et al., 2013; Doppa et al., 2014c), we study a novel approach for search-based coreference resolution called Prune-and-Score. HC-Search is a divideand-conquer solution that learns multiple components with pre-defined roles, and each of them contribute towards the overall</context>
<context position="3762" citStr="Durrett and Klein, 2013" startWordPosition="568" endWordPosition="572">rent functions: 1) a pruning function to prune most of the bad decisions, and 2) a scoring function to pick the best decision among those that are remaining. Our Prune-and-Score approach is a particular instantiation of the general idea of learning nearly-sound constraints for pruning, and leveraging the learned constraints to learn improved heuristic functions for guiding the search. The pruning constraints can take different forms (e.g., classifiers, decisionlist, or ranking functions) depending on the search architecture. Therefore, other coreference resolution systems (Chang et al., 2013; Durrett and Klein, 2013; Bj¨orkelund and Kuhn, 2014) can also benefit from this idea. While our basic idea of twolevel selection might appear similar to the coarseto-fine inference architectures (Felzenszwalb and McAllester, 2007; Weiss and Taskar, 2010), the details differ significantly. Importantly, our pruning and scoring functions operate sequentially at 2115 Proceedings of the 2014 Conference on Empirical Methods in Natural Language Processing (EMNLP), pages 2115–2126, October 25-29, 2014, Doha, Qatar. c�2014 Association for Computational Linguistics each greedy search step, whereas in the cascades approach, th</context>
<context position="7195" citStr="Durrett and Klein, 2013" startWordPosition="1100" endWordPosition="1103">sifiers, however, they suffer from the intractability of “Argmin” inference (finding the least cost clustering output among exponential possibilities) that is encountered during both training and testing. As a result, they resort to approximate inference algorithms (e.g., MCMC, loopy belief propagation), which can suffer from local optima. Third, the incremental approaches construct the clustering output incrementally by processing the mentions in some order (Daum´e III, 2006; Denis and Baldridge, 2008; Rahman and Ng, 2011b; Stoyanov and Eisner, 2012; Chang et al., 2013; Durrett et al., 2013; Durrett and Klein, 2013). These methods learn a scoring function to guide the decision-making process and differ in the form of the scoring function (e.g., mention pair, clustermention or cluster-cluster pair) and how it is being learned. They have shown great success and are very efficient. Indeed, several of the approaches that have achieved state-of-the-art results on OntoNotes fall under this category (Chang et al., 2013; Durrett et al., 2013; Durrett and Klein, 2013; Bj¨orkelund and Kuhn, 2014). However, their efficiency requirement leads to a highly nonrealizable learning problem. Our Prune-and-Score approach i</context>
<context position="29011" citStr="Durrett and Klein, 2013" startWordPosition="4803" endWordPosition="4806">ure representation because we perform offline training and do not have weights for scoring the links. For entity pair features, we treat mention m as a singleton entity and compute features by pairing it with the entity represented by cluster C (exactly as in the Easy-first system). The indicator feature will be 1 for the NEW action and 0 for all other actions.We have a total of 140 features: 90 mention pair features; 49 entity pair features; and one NEW indicator feature. We believe that our approach can benefit from employing features of the mention for the NEW action (Rahman and Ng, 2011b; Durrett and Klein, 2013). However, we were constrained by the Reconcile system and could not leverage these features for the NEW action. Base Rank-Learner. Our pruning and scoring function learning algorithms need a base ranklearner. We employ LambdaMART (Burges, 2010), a state-of-the art rank learner from the RankLib3 library. LambdaMART is a variant of boosted regression trees. We use a learning rate of 0.1, specify the maximum number of boosting iterations (or trees) as 1000 noting that its actual value is automatically decided based on the validation set, and tune the number of leaves per tree based on the valida</context>
<context position="30860" citStr="Durrett and Klein, 2013" startWordPosition="5104" endWordPosition="5108"> validation set. Singleton Mention Filter for OntoNotes Corpus. We employ the Illinois-Coref system (Chang et al., 2012) to extract system mentions for our OntoNotes experiments, and observe that the num3http://sourceforge.net/p/lemur/wiki/RankLib/ 2121 ber of predicted mentions is thrice the number of gold mentions. Since the training data provides the clustering supervision for only gold mentions, it is not clear how to train with the system mentions that are not part of gold mentions. A common way of dealing with this problem is to treat all the extra system mentions as singleton clusters (Durrett and Klein, 2013; Chang et al., 2013). However, this solution most likely will not work with our current feature representation (i.e., NEW action is represented as a single indicator feature). Recall that to predict these extra system mentions as singleton clusters with our incremental clustering approach, the learned model should first predict a NEW action while processing these mentions to form a temporary singleton cluster, and then refrain from merging any of the subsequent mentions with that cluster so that it becomes a singleton cluster in the final clustering output. However, in OntoNotes corpus, the t</context>
<context position="32539" citStr="Durrett and Klein, 2013" startWordPosition="5385" endWordPosition="5388">f NEW actions during testing. As a result, we will generate many singleton clusters, which will hurt the recall of the mention detection after post-processing. Therefore, we aim to learn a singleton mention filter that will be used as a pre-processor before training and testing to overcome this problem. We would like to point out that our filter is complementary to other solutions (e.g., employing features that can discriminate a given mention to be anaphoric or not in place of our single indicator feature, or using a customized loss to weight our ranking examples for cost-sensitive training)(Durrett and Klein, 2013). Filter Learning. The singleton mention filter is a classifier that will label a given mention as “singleton” or not. We represent each mention m in a document by averaging the mention-pair features φ(m, m&apos;) of the k-most similar mentions (obtained by ranking all other mentions m&apos; in the document with a learned ranking function R given m) and then learn a decision-tree classifier by optimizing the F1 loss. We learn the mention-ranking function R by optimizing the recall of positive pairs for a given k, and employ LambdaMART as our base ranker. The hyper-parameters are tuned based on the perfo</context>
<context position="38003" citStr="Durrett and Klein, 2013" startWordPosition="6293" endWordPosition="6297">.00 58.65 57.41 62.35 5 71.18 58.87 57.88 62.64 6 70.93 58.66 57.85 62.48 8 70.12 58.13 57.37 61.87 10 70.24 58.34 56.27 61.61 20 67.97 57.73 56.63 60.78 ∞ 67.03 56.31 55.56 59.63 Table 3: Performance of Prune-and-Score approach with different values of the pruning parameter b. For b = ∞, Prune-and-Score becomes an Only-Scoring algorithm. Comparison to State-of-the-Art. Table 4 shows the results of our Prune-and-Score approach compared with the following state-of-theart coreference resolution approaches: HOTCoref system (Bj¨orkelund and Kuhn, 2014); Berkeley system with the FINAL feature set (Durrett and Klein, 2013); CPL3M system (Chang et al., 2013); Stanford system (Lee et al., 2013); Easy-first system (Stoyanov and Eisner, 2012); and Fernandes et al., 2012 (Fernandes et al., 2012). Only Scoring is the special case of our Prune-andScore approach where we employ only the scoring function. This corresponds to existing incremental approaches (Daum´e III, 2006; Rahman and Ng, 2011b). We report the best published results for CPL3M system, Easy-first, and Fernandes et al., 2012. We ran the publicly available software to generate the results for Berkeley and Stanford systems with the updated CoNLL scorer. We </context>
<context position="39774" citStr="Durrett and Klein, 2013" startWordPosition="6573" endWordPosition="6576">he search. We showed that our approach improves over the methods that only learn a scoring function, and gives comparable or better results than several state-ofthe-art coreference resolution systems. Our Prune-and-Score approach is a particular instantiation of the general idea of learning nearlysound constraints for pruning, and leveraging the learned constraints to learn improved heuristic functions for guiding the search (See (Chen et al., 2014) for another instantiation of this idea for multi-object tracking in videos). Therefore, other coreference resolution systems (Chang et al., 2013; Durrett and Klein, 2013; Bj¨orkelund and Kuhn, 2014) can also benefit from this idea. One way to further improve the peformance of our approach is to perform a search in the Limited Discrepancy Search (LDS) space (Doppa et al., 2014b) using the learned functions. Future work should apply this general idea to other natural language processing tasks including dependency parsing (Nivre et al., 2007) and information extraction (Li et al., 2013). We would expect more beneficial behavior with the pruning constraints for problems with large action sets (e.g., labeled dependency parsing). It would be interesting and useful </context>
</contexts>
<marker>Durrett, Klein, 2013</marker>
<rawString>Greg Durrett and Dan Klein. 2013. Easy victories and uphill battles in coreference resolution. In Proceedings of the Conference on Empirical Methods in Natural Language Processing (EMNLP), pages 1971– 1982.</rawString>
</citation>
<citation valid="true">
<authors>
<author>Greg Durrett</author>
<author>David Leo Wright Hall</author>
<author>Dan Klein</author>
</authors>
<title>Decentralized entity-level modeling for coreference resolution.</title>
<date>2013</date>
<booktitle>In Proceedings of Association of Computational Linguistics (ACL) Conference,</booktitle>
<pages>114--124</pages>
<contexts>
<context position="1784" citStr="Durrett et al., 2013" startWordPosition="254" endWordPosition="257">including OntoNotes. 1 Introduction Coreference resolution is the task of clustering a set of mentions in the text such that all mentions in the same cluster refer to the same entity. It is one of the first stages in deep language understanding and has a big potential impact on the rest of the stages. Several of the state-of-the-art approaches learn a scoring function defined over mention pair, cluster-mention or cluster-cluster pair to guide the coreference decision-making process (Daum´e III, 2006; Bengtson and Roth, 2008; Rahman and Ng, 2011b; Stoyanov and Eisner, 2012; Chang et al., 2013; Durrett et al., 2013; Durrett and Klein, 2013). One common and persistent problem with these approaches is that the scoring function has to make all the coreference decisions, which leads to a highly non-realizable learning problem. Inspired by the recent success of the HC-Search Framework (Doppa et al., 2014a) for studying a variety of structured prediction problems (Lam et al., 2013; Doppa et al., 2014c), we study a novel approach for search-based coreference resolution called Prune-and-Score. HC-Search is a divideand-conquer solution that learns multiple components with pre-defined roles, and each of them cont</context>
<context position="7169" citStr="Durrett et al., 2013" startWordPosition="1096" endWordPosition="1099">ms with pair-wise classifiers, however, they suffer from the intractability of “Argmin” inference (finding the least cost clustering output among exponential possibilities) that is encountered during both training and testing. As a result, they resort to approximate inference algorithms (e.g., MCMC, loopy belief propagation), which can suffer from local optima. Third, the incremental approaches construct the clustering output incrementally by processing the mentions in some order (Daum´e III, 2006; Denis and Baldridge, 2008; Rahman and Ng, 2011b; Stoyanov and Eisner, 2012; Chang et al., 2013; Durrett et al., 2013; Durrett and Klein, 2013). These methods learn a scoring function to guide the decision-making process and differ in the form of the scoring function (e.g., mention pair, clustermention or cluster-cluster pair) and how it is being learned. They have shown great success and are very efficient. Indeed, several of the approaches that have achieved state-of-the-art results on OntoNotes fall under this category (Chang et al., 2013; Durrett et al., 2013; Durrett and Klein, 2013; Bj¨orkelund and Kuhn, 2014). However, their efficiency requirement leads to a highly nonrealizable learning problem. Our </context>
</contexts>
<marker>Durrett, Hall, Klein, 2013</marker>
<rawString>Greg Durrett, David Leo Wright Hall, and Dan Klein. 2013. Decentralized entity-level modeling for coreference resolution. In Proceedings of Association of Computational Linguistics (ACL) Conference, pages 114–124.</rawString>
</citation>
<citation valid="true">
<authors>
<author>Pedro F Felzenszwalb</author>
<author>David A McAllester</author>
</authors>
<title>The generalized A* architecture.</title>
<date>2007</date>
<journal>Journal of Artificial Intelligence Research (JAIR),</journal>
<pages>29--153</pages>
<contexts>
<context position="3968" citStr="Felzenszwalb and McAllester, 2007" startWordPosition="600" endWordPosition="603">icular instantiation of the general idea of learning nearly-sound constraints for pruning, and leveraging the learned constraints to learn improved heuristic functions for guiding the search. The pruning constraints can take different forms (e.g., classifiers, decisionlist, or ranking functions) depending on the search architecture. Therefore, other coreference resolution systems (Chang et al., 2013; Durrett and Klein, 2013; Bj¨orkelund and Kuhn, 2014) can also benefit from this idea. While our basic idea of twolevel selection might appear similar to the coarseto-fine inference architectures (Felzenszwalb and McAllester, 2007; Weiss and Taskar, 2010), the details differ significantly. Importantly, our pruning and scoring functions operate sequentially at 2115 Proceedings of the 2014 Conference on Empirical Methods in Natural Language Processing (EMNLP), pages 2115–2126, October 25-29, 2014, Doha, Qatar. c�2014 Association for Computational Linguistics each greedy search step, whereas in the cascades approach, the second level function makes its prediction only when the first level decision-making is done. Summary of Contributions. The main contributions of our work are as follows. First, we motivate and introduce </context>
</contexts>
<marker>Felzenszwalb, McAllester, 2007</marker>
<rawString>Pedro F. Felzenszwalb and David A. McAllester. 2007. The generalized A* architecture. Journal of Artificial Intelligence Research (JAIR), 29:153–190.</rawString>
</citation>
<citation valid="true">
<title>Eraldo Rezende Fernandes, Cicero Nogueira dos Santos, and Ruy Luiz Milidi´u.</title>
<date>2012</date>
<booktitle>International Conference on Computational Natural Language Learning (CoNLL),</booktitle>
<pages>41--48</pages>
<marker>2012</marker>
<rawString>Eraldo Rezende Fernandes, Cicero Nogueira dos Santos, and Ruy Luiz Milidi´u. 2012. Latent structure perceptron with feature induction for unrestricted coreference resolution. International Conference on Computational Natural Language Learning (CoNLL), pages 41–48.</rawString>
</citation>
<citation valid="true">
<authors>
<author>Thomas Finley</author>
<author>Thorsten Joachims</author>
</authors>
<title>Supervised clustering with support vector machines.</title>
<date>2005</date>
<booktitle>In Proceedings of International Conference on Machine Learning (ICML),</booktitle>
<pages>217--224</pages>
<contexts>
<context position="6373" citStr="Finley and Joachims, 2005" startWordPosition="973" endWordPosition="976">ngtson and Roth, 2008), and perform some form of approximate decoding or post-processing using the pair-wise scores to make predictions. However, the pair-wise classifier approach suffers from several drawbacks including class imbalance (fewer positive edges compared to negative edges) and not being able to leverage the global structure (instead making independent local decisions). Second, the global approaches such as Structured SVMs and Conditional Random Fields (CRFs) learn a cost function to score a potential clustering output for a given input set of mentions (Mccallum and Wellner, 2003; Finley and Joachims, 2005; Culotta et al., 2007; Yu and Joachims, 2009; Haghighi and Klein, 2010; Wick et al., 2011; Wick et al., 2012; Fernandes et al., 2012). These methods address some of the problems with pair-wise classifiers, however, they suffer from the intractability of “Argmin” inference (finding the least cost clustering output among exponential possibilities) that is encountered during both training and testing. As a result, they resort to approximate inference algorithms (e.g., MCMC, loopy belief propagation), which can suffer from local optima. Third, the incremental approaches construct the clustering o</context>
</contexts>
<marker>Finley, Joachims, 2005</marker>
<rawString>Thomas Finley and Thorsten Joachims. 2005. Supervised clustering with support vector machines. In Proceedings of International Conference on Machine Learning (ICML), pages 217–224.</rawString>
</citation>
<citation valid="true">
<authors>
<author>Yoav Goldberg</author>
<author>Joakim Nivre</author>
</authors>
<title>Training deterministic parsers with non-deterministic oracles.</title>
<date>2013</date>
<journal>Transactions of the Association for Computational Linguistics,</journal>
<pages>1--403</pages>
<contexts>
<context position="17662" citStr="Goldberg and Nivre, 2013" startWordPosition="2901" endWordPosition="2904">by task loss function L) that is reachable from s through A(s). Unfortunately, computing the optimal scoring function is highly intractable for the non-decomposable loss functions that are employed in coreference resolution (e.g., B-Cubed F1). The main difficulty is that the decision at any one state has interdependencies with future decisions (see Section 5.5 in (Daum´e III, 2006) for more details). So we need to resort to some form of approximate optimal scoring function that exhibits the intended behavior. This is very similar to the dynamic oracle concept developed for dependency parsing (Goldberg and Nivre, 2013). Let y∗prune be the coreference output corresponding to the terminal state reached from input x by Prune-and-Score approach when performing search using Fprune and F∗score. Then the pruning loss can be expressed as follows. Eprune = E(x,y*)∼D L (x, y∗prune, y∗) Scoring Loss is defined as the additional loss due to Fscore not guiding the greedy search to the best terminal state reachable via the pruning function Fscore (i.e., y∗prune). Let yˆ be the coreference output corresponding to the terminal state reached by Prune-and-Score approach by performing search with Fprune and Fscore for an inpu</context>
</contexts>
<marker>Goldberg, Nivre, 2013</marker>
<rawString>Yoav Goldberg and Joakim Nivre. 2013. Training deterministic parsers with non-deterministic oracles. Transactions of the Association for Computational Linguistics, 1:403–414.</rawString>
</citation>
<citation valid="true">
<authors>
<author>Aria Haghighi</author>
<author>Dan Klein</author>
</authors>
<title>Coreference resolution in a modular, entity-centered model.</title>
<date>2010</date>
<booktitle>In Proceedings of Human Language Technology Conference of the North American Chapter of the Association of Computational Linguistics (HLT-NAACL).</booktitle>
<contexts>
<context position="6444" citStr="Haghighi and Klein, 2010" startWordPosition="985" endWordPosition="988"> post-processing using the pair-wise scores to make predictions. However, the pair-wise classifier approach suffers from several drawbacks including class imbalance (fewer positive edges compared to negative edges) and not being able to leverage the global structure (instead making independent local decisions). Second, the global approaches such as Structured SVMs and Conditional Random Fields (CRFs) learn a cost function to score a potential clustering output for a given input set of mentions (Mccallum and Wellner, 2003; Finley and Joachims, 2005; Culotta et al., 2007; Yu and Joachims, 2009; Haghighi and Klein, 2010; Wick et al., 2011; Wick et al., 2012; Fernandes et al., 2012). These methods address some of the problems with pair-wise classifiers, however, they suffer from the intractability of “Argmin” inference (finding the least cost clustering output among exponential possibilities) that is encountered during both training and testing. As a result, they resort to approximate inference algorithms (e.g., MCMC, loopy belief propagation), which can suffer from local optima. Third, the incremental approaches construct the clustering output incrementally by processing the mentions in some order (Daum´e II</context>
</contexts>
<marker>Haghighi, Klein, 2010</marker>
<rawString>Aria Haghighi and Dan Klein. 2010. Coreference resolution in a modular, entity-centered model. In Proceedings of Human Language Technology Conference of the North American Chapter of the Association of Computational Linguistics (HLT-NAACL).</rawString>
</citation>
<citation valid="true">
<authors>
<author>Hannaneh Hajishirzi</author>
<author>Leila Zilles</author>
<author>Daniel S Weld</author>
<author>Luke S Zettlemoyer</author>
</authors>
<title>Joint coreference resolution and named-entity linking with multipass sieves.</title>
<date>2013</date>
<booktitle>In Proceedings of Conference on Empirical Methods in Natural Language Processing (EMNLP),</booktitle>
<pages>289--299</pages>
<contexts>
<context position="8541" citStr="Hajishirzi et al., 2013" startWordPosition="1305" endWordPosition="1308">learning problem easier and can improve over the performance of scoring-only approaches. Also, the models in (Chang et al., 2013; Durrett et al., 2013) try to leverage cluster-level information implicitly (via latent antecedents) from mention-pair features, whereas our model explicitly leverages the cluster level information. Coreference resolution systems can benefit by incorporating the world knowledge including rules, constraints, and additional information from external knowledge bases (Lee et al., 2013; Rahman and Ng, 2011a; Ratinov and Roth, 2012; Chang et al., 2013; Zheng et al., 2013; Hajishirzi et al., 2013). Our work is orthogonal to this line of work, but domain constraints and rules can be incorporated into our model as done in (Chang et al., 2013). 3 Problem Setup Coreference resolution is a structured prediction problem where the set of mentions m1, m2, · · · , mD extracted from a document cor2116 reponds to a structured input x and the structured output y corresponds to a partition of the mentions into a set of clusters C1, C2, · · · , Ck. Each mention mi belongs to exactly one of the clusters Cj. We are provided with a training set of inputoutput pairs drawn from an unknown distribution D,</context>
</contexts>
<marker>Hajishirzi, Zilles, Weld, Zettlemoyer, 2013</marker>
<rawString>Hannaneh Hajishirzi, Leila Zilles, Daniel S. Weld, and Luke S. Zettlemoyer. 2013. Joint coreference resolution and named-entity linking with multipass sieves. In Proceedings of Conference on Empirical Methods in Natural Language Processing (EMNLP), pages 289–299.</rawString>
</citation>
<citation valid="true">
<authors>
<author>Hal Daum´e John Langford</author>
<author>Daniel Marcu</author>
</authors>
<title>Search-based structured prediction.</title>
<date>2009</date>
<journal>Machine Learning Journal (MLJ),</journal>
<volume>75</volume>
<issue>3</issue>
<marker>Langford, Marcu, 2009</marker>
<rawString>Hal Daum´e III, John Langford, and Daniel Marcu. 2009. Search-based structured prediction. Machine Learning Journal (MLJ), 75(3):297–325.</rawString>
</citation>
<citation valid="true">
<authors>
<author>Hal Daum´e</author>
<author>Daniel Marcu</author>
</authors>
<title>Learning as search optimization: Approximate large margin methods for structured prediction.</title>
<date>2005</date>
<booktitle>In ICML.</booktitle>
<marker>Daum´e, Marcu, 2005</marker>
<rawString>Hal Daum´e III and Daniel Marcu. 2005. Learning as search optimization: Approximate large margin methods for structured prediction. In ICML.</rawString>
</citation>
<citation valid="true">
<authors>
<author>Roni Khardon</author>
</authors>
<title>Learning to take actions.</title>
<date>1999</date>
<journal>Machine Learning Journal (MLJ),</journal>
<volume>35</volume>
<issue>1</issue>
<contexts>
<context position="22506" citStr="Khardon, 1999" startWordPosition="3732" endWordPosition="3733">raining examples. We can also employ more advanced imitation learning algorithms including DAgger (Ross et al., 2011) and SEARN (Hal Daum´e III et al., 2009) if we are provided with an (approximate) optimal scoring function F∗score that can pick optimal actions at states that are not in the solution path (i.e., off-trajectory states). 5.4 Scoring Function Learning Given a learned pruning function Fprune, we want to learn a scoring function that can pick the best action from the b actions that remain after pruning at each state. We formulate this problem in the framework of imitation learning (Khardon, 1999). More formally, let (ˆs0, a∗0), (ˆs1, a∗1), · · · , (ˆs∗ D, ∅) correspond to the sequence of state-action pairs along the greedy trajectory obtained by running the Prune-and-Score approach with Fprune and F∗score, the optimal scoring function, on a training example (x, y∗), where ˆs∗ D is the best terminal state in the pruned space. The goal of our imitation training approach is to learn the parameters of Fscore such that at each state ˆsi, a∗i ∈ A0 is ranked higher than all other actions in A0, where A0 C A(ˆsi) is the set of b actions that remain after pruning. It is important to note that </context>
</contexts>
<marker>Khardon, 1999</marker>
<rawString>Roni Khardon. 1999. Learning to take actions. Machine Learning Journal (MLJ), 35(1):57–90.</rawString>
</citation>
<citation valid="true">
<authors>
<author>Michael Lam</author>
<author>Janardhan Rao Doppa</author>
<author>Xu Hu</author>
<author>Sinisa Todorovic</author>
<author>Thomas Dietterich</author>
<author>Abigail Reft</author>
<author>Marymegan Daly</author>
</authors>
<title>Learning to detect basal tubules of nematocysts in sem images.</title>
<date>2013</date>
<booktitle>In ICCV Workshop on Computer Vision for Accelerated Biosciences (CVAB).</booktitle>
<publisher>IEEE.</publisher>
<contexts>
<context position="2151" citStr="Lam et al., 2013" startWordPosition="311" endWordPosition="314">defined over mention pair, cluster-mention or cluster-cluster pair to guide the coreference decision-making process (Daum´e III, 2006; Bengtson and Roth, 2008; Rahman and Ng, 2011b; Stoyanov and Eisner, 2012; Chang et al., 2013; Durrett et al., 2013; Durrett and Klein, 2013). One common and persistent problem with these approaches is that the scoring function has to make all the coreference decisions, which leads to a highly non-realizable learning problem. Inspired by the recent success of the HC-Search Framework (Doppa et al., 2014a) for studying a variety of structured prediction problems (Lam et al., 2013; Doppa et al., 2014c), we study a novel approach for search-based coreference resolution called Prune-and-Score. HC-Search is a divideand-conquer solution that learns multiple components with pre-defined roles, and each of them contribute towards the overall goal by making the role of the other components easier. The HCSearch framework operates in the space of complete outputs, and relies on the loss function which is only defined on the complete outputs to drive its learning. Unfortunately, this method does not work for incremental coreference resolution since the search space for coreferenc</context>
</contexts>
<marker>Lam, Doppa, Hu, Todorovic, Dietterich, Reft, Daly, 2013</marker>
<rawString>Michael Lam, Janardhan Rao Doppa, Xu Hu, Sinisa Todorovic, Thomas Dietterich, Abigail Reft, and Marymegan Daly. 2013. Learning to detect basal tubules of nematocysts in sem images. In ICCV Workshop on Computer Vision for Accelerated Biosciences (CVAB). IEEE.</rawString>
</citation>
<citation valid="true">
<authors>
<author>Heeyoung Lee</author>
<author>Angel X Chang</author>
<author>Yves Peirsman</author>
<author>Nathanael Chambers</author>
<author>Mihai Surdeanu</author>
<author>Dan Jurafsky</author>
</authors>
<title>Deterministic coreference resolution based on entity-centric, precision-ranked rules.</title>
<date>2013</date>
<journal>Computational Linguistics,</journal>
<volume>39</volume>
<issue>4</issue>
<contexts>
<context position="8429" citStr="Lee et al., 2013" startWordPosition="1284" endWordPosition="1287"> these methods, as we show that having a pruning function (or a set of learned pruning rules) makes the learning problem easier and can improve over the performance of scoring-only approaches. Also, the models in (Chang et al., 2013; Durrett et al., 2013) try to leverage cluster-level information implicitly (via latent antecedents) from mention-pair features, whereas our model explicitly leverages the cluster level information. Coreference resolution systems can benefit by incorporating the world knowledge including rules, constraints, and additional information from external knowledge bases (Lee et al., 2013; Rahman and Ng, 2011a; Ratinov and Roth, 2012; Chang et al., 2013; Zheng et al., 2013; Hajishirzi et al., 2013). Our work is orthogonal to this line of work, but domain constraints and rules can be incorporated into our model as done in (Chang et al., 2013). 3 Problem Setup Coreference resolution is a structured prediction problem where the set of mentions m1, m2, · · · , mD extracted from a document cor2116 reponds to a structured input x and the structured output y corresponds to a partition of the mentions into a set of clusters C1, C2, · · · , Ck. Each mention mi belongs to exactly one of</context>
<context position="38074" citStr="Lee et al., 2013" startWordPosition="6306" endWordPosition="6309">.12 58.13 57.37 61.87 10 70.24 58.34 56.27 61.61 20 67.97 57.73 56.63 60.78 ∞ 67.03 56.31 55.56 59.63 Table 3: Performance of Prune-and-Score approach with different values of the pruning parameter b. For b = ∞, Prune-and-Score becomes an Only-Scoring algorithm. Comparison to State-of-the-Art. Table 4 shows the results of our Prune-and-Score approach compared with the following state-of-theart coreference resolution approaches: HOTCoref system (Bj¨orkelund and Kuhn, 2014); Berkeley system with the FINAL feature set (Durrett and Klein, 2013); CPL3M system (Chang et al., 2013); Stanford system (Lee et al., 2013); Easy-first system (Stoyanov and Eisner, 2012); and Fernandes et al., 2012 (Fernandes et al., 2012). Only Scoring is the special case of our Prune-andScore approach where we employ only the scoring function. This corresponds to existing incremental approaches (Daum´e III, 2006; Rahman and Ng, 2011b). We report the best published results for CPL3M system, Easy-first, and Fernandes et al., 2012. We ran the publicly available software to generate the results for Berkeley and Stanford systems with the updated CoNLL scorer. We include the results of Prune-and-Score for best b on the development se</context>
</contexts>
<marker>Lee, Chang, Peirsman, Chambers, Surdeanu, Jurafsky, 2013</marker>
<rawString>Heeyoung Lee, Angel X. Chang, Yves Peirsman, Nathanael Chambers, Mihai Surdeanu, and Dan Jurafsky. 2013. Deterministic coreference resolution based on entity-centric, precision-ranked rules. Computational Linguistics, 39(4):885–916.</rawString>
</citation>
<citation valid="true">
<authors>
<author>Qi Li</author>
<author>Heng Ji</author>
<author>Liang Huang</author>
</authors>
<title>Joint event extraction via structured prediction with global features.</title>
<date>2013</date>
<booktitle>In Proceedings of the 51st Annual Meeting of the Association for Computational Linguistics (ACL),</booktitle>
<pages>73--82</pages>
<contexts>
<context position="40195" citStr="Li et al., 2013" startWordPosition="6641" endWordPosition="6644"> (See (Chen et al., 2014) for another instantiation of this idea for multi-object tracking in videos). Therefore, other coreference resolution systems (Chang et al., 2013; Durrett and Klein, 2013; Bj¨orkelund and Kuhn, 2014) can also benefit from this idea. One way to further improve the peformance of our approach is to perform a search in the Limited Discrepancy Search (LDS) space (Doppa et al., 2014b) using the learned functions. Future work should apply this general idea to other natural language processing tasks including dependency parsing (Nivre et al., 2007) and information extraction (Li et al., 2013). We would expect more beneficial behavior with the pruning constraints for problems with large action sets (e.g., labeled dependency parsing). It would be interesting and useful to generalize this approach to search spaces where there are multiple target paths from the initial state to the terminal state, e.g., as in the Easy-first framework. Acknowledgments Authors would like to thank Veselin Stoyanov (JHU) for answering several questions related to the Easy-first and Reconcile systems; Van Dang (UMass, Amherst) for technical discussions related to the RankLib library; Kai-Wei Chang (UIUC) f</context>
</contexts>
<marker>Li, Ji, Huang, 2013</marker>
<rawString>Qi Li, Heng Ji, and Liang Huang. 2013. Joint event extraction via structured prediction with global features. In Proceedings of the 51st Annual Meeting of the Association for Computational Linguistics (ACL), pages 73–82.</rawString>
</citation>
<citation valid="true">
<authors>
<author>Tie-Yan Liu</author>
</authors>
<title>Learning to rank for information retrieval.</title>
<date>2009</date>
<booktitle>Foundations and Trends in Information Retrieval,</booktitle>
<volume>3</volume>
<issue>3</issue>
<contexts>
<context position="20889" citStr="Liu, 2009" startWordPosition="3445" endWordPosition="3446">oy an online-LaSO style approach (III and Marcu, 2005; Xu et al., 2009) to learn the parameters of the pruning function, it is quite inefficient, as it must regenerate the same search trajectory again and again until it learns to make the right decision. Additionally, this approach limits applicability of the off-the-shelf learners to learn the parameters of Fprune. To overcome these drawbacks, we apply offline training. Reduction to Rank Learning. We reduce the pruning function learning to a rank learning problem. This allows us to leverage powerful and efficient off-the-shelf rank-learners (Liu, 2009). The reduction is as follows. At each state s∗i on the solution path of a training example (x, y∗), we create an example by labeling optimal action a∗i E A(s∗i ) as the only relevant action, and then try to learn 2119 a ranking function that can rank actions such that the relevant action a∗i is in the top b actions, where b is the input pruning paramter. In other words, we have a rank learning problem, where the learner’s goal is to optimize the Precision at Topb. The training approach creates such an example for each state s in the solution path. The set of aggregate imitation examples colle</context>
</contexts>
<marker>Liu, 2009</marker>
<rawString>Tie-Yan Liu. 2009. Learning to rank for information retrieval. Foundations and Trends in Information Retrieval, 3(3):225–331.</rawString>
</citation>
<citation valid="true">
<authors>
<author>Xiaoqiang Luo</author>
</authors>
<title>On coreference resolution performance metrics.</title>
<date>2005</date>
<booktitle>In Proceedings of the Conference on Human Language Technology and Empirical Methods in Natural Language Processing, HLT ’05,</booktitle>
<pages>25--32</pages>
<publisher>Association</publisher>
<institution>for Computational Linguistics.</institution>
<location>Stroudsburg, PA, USA.</location>
<contexts>
<context position="26929" citStr="Luo, 2005" startWordPosition="4464" endWordPosition="4465">wswire documents in ACE 2004 corpus for a fair comparison with the state-of-the-art. The MUC6 corpus containts 255 documents. We employ the official test set of 30 documents (MUC6- TEST) for testing purposes. From the remaining 225 documents, which includes 195 official training documents and 30 dry-run test documents, we randomly pick 30 documents for validation, and use the remaining ones for training. Evaluation Metrics. We compute three most popular performance metrics for coreference resolution: MUC (Vilain et al., 1995), B-Cubed (Bagga and Baldwin, 1998), and Entity-based CEAF (CEAFφ4) (Luo, 2005). As it is commonly done in CoNLL shared tasks (Pradhan et al., 2012), we employ the average F1 score (CoNLL F1) of these three metrics for comparison purposes. We evaluate all the results using the updated version1 (7.0) of the coreference scorer. Features. We built2 our coreference resolver based on the Easy-first coreference system (Stoyanov and Eisner, 2012), which is derived from the Reconcile system (Stoyanov et al., 2010). We essentially employ the same features as in the Easyfirst system. However, we provide some highlevel details that are necessary for subsequent discussion. Recall th</context>
</contexts>
<marker>Luo, 2005</marker>
<rawString>Xiaoqiang Luo. 2005. On coreference resolution performance metrics. In Proceedings of the Conference on Human Language Technology and Empirical Methods in Natural Language Processing, HLT ’05, pages 25–32, Stroudsburg, PA, USA. Association for Computational Linguistics.</rawString>
</citation>
<citation valid="true">
<authors>
<author>Andrew Mccallum</author>
<author>Ben Wellner</author>
</authors>
<title>Toward conditional models of identity uncertainty with application to proper noun coreference.</title>
<date>2003</date>
<booktitle>In Proceedings of Neural Information Processing Systems (NIPS),</booktitle>
<pages>905--912</pages>
<publisher>MIT Press.</publisher>
<contexts>
<context position="6346" citStr="Mccallum and Wellner, 2003" startWordPosition="969" endWordPosition="972">001; Ng and Cardie, 2002; Bengtson and Roth, 2008), and perform some form of approximate decoding or post-processing using the pair-wise scores to make predictions. However, the pair-wise classifier approach suffers from several drawbacks including class imbalance (fewer positive edges compared to negative edges) and not being able to leverage the global structure (instead making independent local decisions). Second, the global approaches such as Structured SVMs and Conditional Random Fields (CRFs) learn a cost function to score a potential clustering output for a given input set of mentions (Mccallum and Wellner, 2003; Finley and Joachims, 2005; Culotta et al., 2007; Yu and Joachims, 2009; Haghighi and Klein, 2010; Wick et al., 2011; Wick et al., 2012; Fernandes et al., 2012). These methods address some of the problems with pair-wise classifiers, however, they suffer from the intractability of “Argmin” inference (finding the least cost clustering output among exponential possibilities) that is encountered during both training and testing. As a result, they resort to approximate inference algorithms (e.g., MCMC, loopy belief propagation), which can suffer from local optima. Third, the incremental approaches</context>
</contexts>
<marker>Mccallum, Wellner, 2003</marker>
<rawString>Andrew Mccallum and Ben Wellner. 2003. Toward conditional models of identity uncertainty with application to proper noun coreference. In Proceedings of Neural Information Processing Systems (NIPS), pages 905–912. MIT Press.</rawString>
</citation>
<citation valid="true">
<authors>
<author>MUC6</author>
</authors>
<title>Coreference task definition.</title>
<date>1995</date>
<booktitle>In Proceedings of the Sixth Message Understanding Conference (MUC-6),</booktitle>
<pages>335--344</pages>
<marker>MUC6, 1995</marker>
<rawString>MUC6. 1995. Coreference task definition. In Proceedings of the Sixth Message Understanding Conference (MUC-6), pages 335–344.</rawString>
</citation>
<citation valid="true">
<authors>
<author>Vincent Ng</author>
<author>Claire Cardie</author>
</authors>
<title>Improving machine learning approaches to coreference resolution.</title>
<date>2002</date>
<booktitle>In Proceedings ofAssociation of Computational Linguistics (ACL) Conference,</booktitle>
<pages>104--111</pages>
<contexts>
<context position="5744" citStr="Ng and Cardie, 2002" startWordPosition="876" endWordPosition="879">oring function. The remainder of the paper proceeds as follows. In Section 2, we dicuss the related work. We introduce our problem setup in Section 3 and then describe our Prune-and-Score approach in Section 4. We explain our approaches for learning the pruning and scoring functions in Section 5. Section 6 presents our experimental results followed by the conclusions in Section 7. 2 Related Work The work on learning-based coreference resolution can be broadly classified into three types. First, the pair-wise classifier approaches learn a classifier on mention pairs (edges) (Soon et al., 2001; Ng and Cardie, 2002; Bengtson and Roth, 2008), and perform some form of approximate decoding or post-processing using the pair-wise scores to make predictions. However, the pair-wise classifier approach suffers from several drawbacks including class imbalance (fewer positive edges compared to negative edges) and not being able to leverage the global structure (instead making independent local decisions). Second, the global approaches such as Structured SVMs and Conditional Random Fields (CRFs) learn a cost function to score a potential clustering output for a given input set of mentions (Mccallum and Wellner, 20</context>
</contexts>
<marker>Ng, Cardie, 2002</marker>
<rawString>Vincent Ng and Claire Cardie. 2002. Improving machine learning approaches to coreference resolution. In Proceedings ofAssociation of Computational Linguistics (ACL) Conference, pages 104–111.</rawString>
</citation>
<citation valid="true">
<authors>
<author>NIST</author>
</authors>
<title>The ACE evaluation plan.</title>
<date>2004</date>
<contexts>
<context position="25505" citStr="NIST, 2004" startWordPosition="4237" endWordPosition="4238"> s0 resulting from taking action a in state s, i.e., F∗score(s, a) = F∗score(s0). To further simplify the computation, we give uniform weight to the three types of costs: 1) Credit for correct linking, 2) Penalty for incorrect linking, and 3) Penalty for missing links. Intuitively, this is similar to the correct-link count computed only on a subgraph. We direct the reader to (Daum´e III, 2006) for more details (see Section 5.5). 6 Experiments and Results In this section, we evaluate our greedy Pruneand-Score approach on three benchmark corpora – OntoNotes 5.0 (Pradhan et al., 2012), ACE 2004 (NIST, 2004), and MUC6 (MUC6, 1995) – and compare it against the state-of-the-art approaches for coreference resolution. For OntoNotes data, we report the results on both gold mentions and predicted mentions. We also report the results on gold mentions for ACE 2004 and MUC6 data. 2120 6.1 Experimental Setup Datasets. For OntoNotes corpus, we employ the official split for training, validation, and testing. There are 2802 documents in the training set; 343 documents in the validation set; and 345 documents in the testing set. The ACE 2004 corpus contains 443 documents. We follow the (Culotta et al., 2007; B</context>
</contexts>
<marker>NIST, 2004</marker>
<rawString>NIST. 2004. The ACE evaluation plan.</rawString>
</citation>
<citation valid="true">
<authors>
<author>Joakim Nivre</author>
<author>Johan Hall</author>
<author>Jens Nilsson</author>
</authors>
<title>Atanas Chanev, G¨ulsen Eryigit, Sandra K¨ubler, Svetoslav Marinov, and Erwin Marsi.</title>
<date>2007</date>
<journal>Natural Language Engineering,</journal>
<volume>13</volume>
<issue>2</issue>
<contexts>
<context position="40150" citStr="Nivre et al., 2007" startWordPosition="6633" endWordPosition="6636">roved heuristic functions for guiding the search (See (Chen et al., 2014) for another instantiation of this idea for multi-object tracking in videos). Therefore, other coreference resolution systems (Chang et al., 2013; Durrett and Klein, 2013; Bj¨orkelund and Kuhn, 2014) can also benefit from this idea. One way to further improve the peformance of our approach is to perform a search in the Limited Discrepancy Search (LDS) space (Doppa et al., 2014b) using the learned functions. Future work should apply this general idea to other natural language processing tasks including dependency parsing (Nivre et al., 2007) and information extraction (Li et al., 2013). We would expect more beneficial behavior with the pruning constraints for problems with large action sets (e.g., labeled dependency parsing). It would be interesting and useful to generalize this approach to search spaces where there are multiple target paths from the initial state to the terminal state, e.g., as in the Easy-first framework. Acknowledgments Authors would like to thank Veselin Stoyanov (JHU) for answering several questions related to the Easy-first and Reconcile systems; Van Dang (UMass, Amherst) for technical discussions related t</context>
</contexts>
<marker>Nivre, Hall, Nilsson, 2007</marker>
<rawString>Joakim Nivre, Johan Hall, Jens Nilsson, Atanas Chanev, G¨ulsen Eryigit, Sandra K¨ubler, Svetoslav Marinov, and Erwin Marsi. 2007. Maltparser: A language-independent system for data-driven dependency parsing. Natural Language Engineering, 13(2):95–135.</rawString>
</citation>
<citation valid="true">
<authors>
<author>Sameer Pradhan</author>
<author>Alessandro Moschitti</author>
<author>Nianwen Xue</author>
<author>Olga Uryupina</author>
<author>Yuchen Zhang</author>
</authors>
<title>Conll2012 shared task: Modeling multilingual unrestricted coreference in ontonotes.</title>
<date>2012</date>
<booktitle>In Proceedings of the Joint Conference on EMNLP and CoNLL: Shared Task,</booktitle>
<pages>1--40</pages>
<contexts>
<context position="25482" citStr="Pradhan et al., 2012" startWordPosition="4231" endWordPosition="4234">orresponding to the “after state” s0 resulting from taking action a in state s, i.e., F∗score(s, a) = F∗score(s0). To further simplify the computation, we give uniform weight to the three types of costs: 1) Credit for correct linking, 2) Penalty for incorrect linking, and 3) Penalty for missing links. Intuitively, this is similar to the correct-link count computed only on a subgraph. We direct the reader to (Daum´e III, 2006) for more details (see Section 5.5). 6 Experiments and Results In this section, we evaluate our greedy Pruneand-Score approach on three benchmark corpora – OntoNotes 5.0 (Pradhan et al., 2012), ACE 2004 (NIST, 2004), and MUC6 (MUC6, 1995) – and compare it against the state-of-the-art approaches for coreference resolution. For OntoNotes data, we report the results on both gold mentions and predicted mentions. We also report the results on gold mentions for ACE 2004 and MUC6 data. 2120 6.1 Experimental Setup Datasets. For OntoNotes corpus, we employ the official split for training, validation, and testing. There are 2802 documents in the training set; 343 documents in the validation set; and 345 documents in the testing set. The ACE 2004 corpus contains 443 documents. We follow the (</context>
<context position="26998" citStr="Pradhan et al., 2012" startWordPosition="4475" endWordPosition="4478">with the state-of-the-art. The MUC6 corpus containts 255 documents. We employ the official test set of 30 documents (MUC6- TEST) for testing purposes. From the remaining 225 documents, which includes 195 official training documents and 30 dry-run test documents, we randomly pick 30 documents for validation, and use the remaining ones for training. Evaluation Metrics. We compute three most popular performance metrics for coreference resolution: MUC (Vilain et al., 1995), B-Cubed (Bagga and Baldwin, 1998), and Entity-based CEAF (CEAFφ4) (Luo, 2005). As it is commonly done in CoNLL shared tasks (Pradhan et al., 2012), we employ the average F1 score (CoNLL F1) of these three metrics for comparison purposes. We evaluate all the results using the updated version1 (7.0) of the coreference scorer. Features. We built2 our coreference resolver based on the Easy-first coreference system (Stoyanov and Eisner, 2012), which is derived from the Reconcile system (Stoyanov et al., 2010). We essentially employ the same features as in the Easyfirst system. However, we provide some highlevel details that are necessary for subsequent discussion. Recall that our features 0(s, a) for both pruning and scoring functions are de</context>
</contexts>
<marker>Pradhan, Moschitti, Xue, Uryupina, Zhang, 2012</marker>
<rawString>Sameer Pradhan, Alessandro Moschitti, Nianwen Xue, Olga Uryupina, and Yuchen Zhang. 2012. Conll2012 shared task: Modeling multilingual unrestricted coreference in ontonotes. In Proceedings of the Joint Conference on EMNLP and CoNLL: Shared Task, pages 1–40.</rawString>
</citation>
<citation valid="true">
<authors>
<author>Altaf Rahman</author>
<author>Vincent Ng</author>
</authors>
<title>Coreference resolution with world knowledge.</title>
<date>2011</date>
<booktitle>In Proceedings of Association of Computational Linguistics (ACL) Conference,</booktitle>
<pages>814--824</pages>
<contexts>
<context position="1714" citStr="Rahman and Ng, 2011" startWordPosition="242" endWordPosition="245">ms several state-of-the-art approaches on multiple benchmark corpora including OntoNotes. 1 Introduction Coreference resolution is the task of clustering a set of mentions in the text such that all mentions in the same cluster refer to the same entity. It is one of the first stages in deep language understanding and has a big potential impact on the rest of the stages. Several of the state-of-the-art approaches learn a scoring function defined over mention pair, cluster-mention or cluster-cluster pair to guide the coreference decision-making process (Daum´e III, 2006; Bengtson and Roth, 2008; Rahman and Ng, 2011b; Stoyanov and Eisner, 2012; Chang et al., 2013; Durrett et al., 2013; Durrett and Klein, 2013). One common and persistent problem with these approaches is that the scoring function has to make all the coreference decisions, which leads to a highly non-realizable learning problem. Inspired by the recent success of the HC-Search Framework (Doppa et al., 2014a) for studying a variety of structured prediction problems (Lam et al., 2013; Doppa et al., 2014c), we study a novel approach for search-based coreference resolution called Prune-and-Score. HC-Search is a divideand-conquer solution that le</context>
<context position="7099" citStr="Rahman and Ng, 2011" startWordPosition="1084" endWordPosition="1087">12; Fernandes et al., 2012). These methods address some of the problems with pair-wise classifiers, however, they suffer from the intractability of “Argmin” inference (finding the least cost clustering output among exponential possibilities) that is encountered during both training and testing. As a result, they resort to approximate inference algorithms (e.g., MCMC, loopy belief propagation), which can suffer from local optima. Third, the incremental approaches construct the clustering output incrementally by processing the mentions in some order (Daum´e III, 2006; Denis and Baldridge, 2008; Rahman and Ng, 2011b; Stoyanov and Eisner, 2012; Chang et al., 2013; Durrett et al., 2013; Durrett and Klein, 2013). These methods learn a scoring function to guide the decision-making process and differ in the form of the scoring function (e.g., mention pair, clustermention or cluster-cluster pair) and how it is being learned. They have shown great success and are very efficient. Indeed, several of the approaches that have achieved state-of-the-art results on OntoNotes fall under this category (Chang et al., 2013; Durrett et al., 2013; Durrett and Klein, 2013; Bj¨orkelund and Kuhn, 2014). However, their efficie</context>
<context position="8450" citStr="Rahman and Ng, 2011" startWordPosition="1288" endWordPosition="1292"> we show that having a pruning function (or a set of learned pruning rules) makes the learning problem easier and can improve over the performance of scoring-only approaches. Also, the models in (Chang et al., 2013; Durrett et al., 2013) try to leverage cluster-level information implicitly (via latent antecedents) from mention-pair features, whereas our model explicitly leverages the cluster level information. Coreference resolution systems can benefit by incorporating the world knowledge including rules, constraints, and additional information from external knowledge bases (Lee et al., 2013; Rahman and Ng, 2011a; Ratinov and Roth, 2012; Chang et al., 2013; Zheng et al., 2013; Hajishirzi et al., 2013). Our work is orthogonal to this line of work, but domain constraints and rules can be incorporated into our model as done in (Chang et al., 2013). 3 Problem Setup Coreference resolution is a structured prediction problem where the set of mentions m1, m2, · · · , mD extracted from a document cor2116 reponds to a structured input x and the structured output y corresponds to a partition of the mentions into a set of clusters C1, C2, · · · , Ck. Each mention mi belongs to exactly one of the clusters Cj. We </context>
<context position="28984" citStr="Rahman and Ng, 2011" startWordPosition="4799" endWordPosition="4802">loy the best-link feature representation because we perform offline training and do not have weights for scoring the links. For entity pair features, we treat mention m as a singleton entity and compute features by pairing it with the entity represented by cluster C (exactly as in the Easy-first system). The indicator feature will be 1 for the NEW action and 0 for all other actions.We have a total of 140 features: 90 mention pair features; 49 entity pair features; and one NEW indicator feature. We believe that our approach can benefit from employing features of the mention for the NEW action (Rahman and Ng, 2011b; Durrett and Klein, 2013). However, we were constrained by the Reconcile system and could not leverage these features for the NEW action. Base Rank-Learner. Our pruning and scoring function learning algorithms need a base ranklearner. We employ LambdaMART (Burges, 2010), a state-of-the art rank learner from the RankLib3 library. LambdaMART is a variant of boosted regression trees. We use a learning rate of 0.1, specify the maximum number of boosting iterations (or trees) as 1000 noting that its actual value is automatically decided based on the validation set, and tune the number of leaves p</context>
<context position="38373" citStr="Rahman and Ng, 2011" startWordPosition="6356" endWordPosition="6359">ble 4 shows the results of our Prune-and-Score approach compared with the following state-of-theart coreference resolution approaches: HOTCoref system (Bj¨orkelund and Kuhn, 2014); Berkeley system with the FINAL feature set (Durrett and Klein, 2013); CPL3M system (Chang et al., 2013); Stanford system (Lee et al., 2013); Easy-first system (Stoyanov and Eisner, 2012); and Fernandes et al., 2012 (Fernandes et al., 2012). Only Scoring is the special case of our Prune-andScore approach where we employ only the scoring function. This corresponds to existing incremental approaches (Daum´e III, 2006; Rahman and Ng, 2011b). We report the best published results for CPL3M system, Easy-first, and Fernandes et al., 2012. We ran the publicly available software to generate the results for Berkeley and Stanford systems with the updated CoNLL scorer. We include the results of Prune-and-Score for best b on the development set with singleton mention filter for the comparison. In Table 4, ’-’ indicates that we could not find published results for those cases. We see 2123 that results of the Prune-and-Score approach are comparable to or better than the state-of-the-art including Only-Scoring. 7 Conclusions and Future Wor</context>
</contexts>
<marker>Rahman, Ng, 2011</marker>
<rawString>Altaf Rahman and Vincent Ng. 2011a. Coreference resolution with world knowledge. In Proceedings of Association of Computational Linguistics (ACL) Conference, pages 814–824.</rawString>
</citation>
<citation valid="true">
<authors>
<author>Altaf Rahman</author>
<author>Vincent Ng</author>
</authors>
<title>Narrowing the modeling gap: A cluster-ranking approach to coreference resolution.</title>
<date>2011</date>
<journal>Journal of Artificial Intelligence Research (JAIR),</journal>
<pages>40--469</pages>
<contexts>
<context position="1714" citStr="Rahman and Ng, 2011" startWordPosition="242" endWordPosition="245">ms several state-of-the-art approaches on multiple benchmark corpora including OntoNotes. 1 Introduction Coreference resolution is the task of clustering a set of mentions in the text such that all mentions in the same cluster refer to the same entity. It is one of the first stages in deep language understanding and has a big potential impact on the rest of the stages. Several of the state-of-the-art approaches learn a scoring function defined over mention pair, cluster-mention or cluster-cluster pair to guide the coreference decision-making process (Daum´e III, 2006; Bengtson and Roth, 2008; Rahman and Ng, 2011b; Stoyanov and Eisner, 2012; Chang et al., 2013; Durrett et al., 2013; Durrett and Klein, 2013). One common and persistent problem with these approaches is that the scoring function has to make all the coreference decisions, which leads to a highly non-realizable learning problem. Inspired by the recent success of the HC-Search Framework (Doppa et al., 2014a) for studying a variety of structured prediction problems (Lam et al., 2013; Doppa et al., 2014c), we study a novel approach for search-based coreference resolution called Prune-and-Score. HC-Search is a divideand-conquer solution that le</context>
<context position="7099" citStr="Rahman and Ng, 2011" startWordPosition="1084" endWordPosition="1087">12; Fernandes et al., 2012). These methods address some of the problems with pair-wise classifiers, however, they suffer from the intractability of “Argmin” inference (finding the least cost clustering output among exponential possibilities) that is encountered during both training and testing. As a result, they resort to approximate inference algorithms (e.g., MCMC, loopy belief propagation), which can suffer from local optima. Third, the incremental approaches construct the clustering output incrementally by processing the mentions in some order (Daum´e III, 2006; Denis and Baldridge, 2008; Rahman and Ng, 2011b; Stoyanov and Eisner, 2012; Chang et al., 2013; Durrett et al., 2013; Durrett and Klein, 2013). These methods learn a scoring function to guide the decision-making process and differ in the form of the scoring function (e.g., mention pair, clustermention or cluster-cluster pair) and how it is being learned. They have shown great success and are very efficient. Indeed, several of the approaches that have achieved state-of-the-art results on OntoNotes fall under this category (Chang et al., 2013; Durrett et al., 2013; Durrett and Klein, 2013; Bj¨orkelund and Kuhn, 2014). However, their efficie</context>
<context position="8450" citStr="Rahman and Ng, 2011" startWordPosition="1288" endWordPosition="1292"> we show that having a pruning function (or a set of learned pruning rules) makes the learning problem easier and can improve over the performance of scoring-only approaches. Also, the models in (Chang et al., 2013; Durrett et al., 2013) try to leverage cluster-level information implicitly (via latent antecedents) from mention-pair features, whereas our model explicitly leverages the cluster level information. Coreference resolution systems can benefit by incorporating the world knowledge including rules, constraints, and additional information from external knowledge bases (Lee et al., 2013; Rahman and Ng, 2011a; Ratinov and Roth, 2012; Chang et al., 2013; Zheng et al., 2013; Hajishirzi et al., 2013). Our work is orthogonal to this line of work, but domain constraints and rules can be incorporated into our model as done in (Chang et al., 2013). 3 Problem Setup Coreference resolution is a structured prediction problem where the set of mentions m1, m2, · · · , mD extracted from a document cor2116 reponds to a structured input x and the structured output y corresponds to a partition of the mentions into a set of clusters C1, C2, · · · , Ck. Each mention mi belongs to exactly one of the clusters Cj. We </context>
<context position="28984" citStr="Rahman and Ng, 2011" startWordPosition="4799" endWordPosition="4802">loy the best-link feature representation because we perform offline training and do not have weights for scoring the links. For entity pair features, we treat mention m as a singleton entity and compute features by pairing it with the entity represented by cluster C (exactly as in the Easy-first system). The indicator feature will be 1 for the NEW action and 0 for all other actions.We have a total of 140 features: 90 mention pair features; 49 entity pair features; and one NEW indicator feature. We believe that our approach can benefit from employing features of the mention for the NEW action (Rahman and Ng, 2011b; Durrett and Klein, 2013). However, we were constrained by the Reconcile system and could not leverage these features for the NEW action. Base Rank-Learner. Our pruning and scoring function learning algorithms need a base ranklearner. We employ LambdaMART (Burges, 2010), a state-of-the art rank learner from the RankLib3 library. LambdaMART is a variant of boosted regression trees. We use a learning rate of 0.1, specify the maximum number of boosting iterations (or trees) as 1000 noting that its actual value is automatically decided based on the validation set, and tune the number of leaves p</context>
<context position="38373" citStr="Rahman and Ng, 2011" startWordPosition="6356" endWordPosition="6359">ble 4 shows the results of our Prune-and-Score approach compared with the following state-of-theart coreference resolution approaches: HOTCoref system (Bj¨orkelund and Kuhn, 2014); Berkeley system with the FINAL feature set (Durrett and Klein, 2013); CPL3M system (Chang et al., 2013); Stanford system (Lee et al., 2013); Easy-first system (Stoyanov and Eisner, 2012); and Fernandes et al., 2012 (Fernandes et al., 2012). Only Scoring is the special case of our Prune-andScore approach where we employ only the scoring function. This corresponds to existing incremental approaches (Daum´e III, 2006; Rahman and Ng, 2011b). We report the best published results for CPL3M system, Easy-first, and Fernandes et al., 2012. We ran the publicly available software to generate the results for Berkeley and Stanford systems with the updated CoNLL scorer. We include the results of Prune-and-Score for best b on the development set with singleton mention filter for the comparison. In Table 4, ’-’ indicates that we could not find published results for those cases. We see 2123 that results of the Prune-and-Score approach are comparable to or better than the state-of-the-art including Only-Scoring. 7 Conclusions and Future Wor</context>
</contexts>
<marker>Rahman, Ng, 2011</marker>
<rawString>Altaf Rahman and Vincent Ng. 2011b. Narrowing the modeling gap: A cluster-ranking approach to coreference resolution. Journal of Artificial Intelligence Research (JAIR), 40:469–521.</rawString>
</citation>
<citation valid="true">
<authors>
<author>Lev-Arie Ratinov</author>
<author>Dan Roth</author>
</authors>
<title>Learningbased multi-sieve co-reference resolution with knowledge.</title>
<date>2012</date>
<booktitle>In Proceedings of Empirical Methods in Natural Language Processing (EMNLP) Conference,</booktitle>
<pages>1234--1244</pages>
<contexts>
<context position="8475" citStr="Ratinov and Roth, 2012" startWordPosition="1293" endWordPosition="1296"> pruning function (or a set of learned pruning rules) makes the learning problem easier and can improve over the performance of scoring-only approaches. Also, the models in (Chang et al., 2013; Durrett et al., 2013) try to leverage cluster-level information implicitly (via latent antecedents) from mention-pair features, whereas our model explicitly leverages the cluster level information. Coreference resolution systems can benefit by incorporating the world knowledge including rules, constraints, and additional information from external knowledge bases (Lee et al., 2013; Rahman and Ng, 2011a; Ratinov and Roth, 2012; Chang et al., 2013; Zheng et al., 2013; Hajishirzi et al., 2013). Our work is orthogonal to this line of work, but domain constraints and rules can be incorporated into our model as done in (Chang et al., 2013). 3 Problem Setup Coreference resolution is a structured prediction problem where the set of mentions m1, m2, · · · , mD extracted from a document cor2116 reponds to a structured input x and the structured output y corresponds to a partition of the mentions into a set of clusters C1, C2, · · · , Ck. Each mention mi belongs to exactly one of the clusters Cj. We are provided with a train</context>
</contexts>
<marker>Ratinov, Roth, 2012</marker>
<rawString>Lev-Arie Ratinov and Dan Roth. 2012. Learningbased multi-sieve co-reference resolution with knowledge. In Proceedings of Empirical Methods in Natural Language Processing (EMNLP) Conference, pages 1234–1244.</rawString>
</citation>
<citation valid="true">
<authors>
<author>St´ephane Ross</author>
<author>Geoffrey J Gordon</author>
<author>Drew Bagnell</author>
</authors>
<title>A reduction of imitation learning and structured prediction to no-regret online learning.</title>
<date>2011</date>
<journal>Journal of Machine Learning Research - Proceedings Track,</journal>
<pages>15--627</pages>
<contexts>
<context position="22009" citStr="Ross et al., 2011" startWordPosition="3645" endWordPosition="3648">ch an example for each state s in the solution path. The set of aggregate imitation examples collected over all the training data is then given to a rank learner (e.g., LambdaMART (Burges, 2010)) to learn the parameters of Fprune by optimizing the Precision at Top-b loss. See appendix for the pseudocode. If we can learn a function Fprune that is consistent with these imitation examples, then the learned pruning function is guaranteed to keep the solution path within the pruned space for all the training examples. We can also employ more advanced imitation learning algorithms including DAgger (Ross et al., 2011) and SEARN (Hal Daum´e III et al., 2009) if we are provided with an (approximate) optimal scoring function F∗score that can pick optimal actions at states that are not in the solution path (i.e., off-trajectory states). 5.4 Scoring Function Learning Given a learned pruning function Fprune, we want to learn a scoring function that can pick the best action from the b actions that remain after pruning at each state. We formulate this problem in the framework of imitation learning (Khardon, 1999). More formally, let (ˆs0, a∗0), (ˆs1, a∗1), · · · , (ˆs∗ D, ∅) correspond to the sequence of state-act</context>
</contexts>
<marker>Ross, Gordon, Bagnell, 2011</marker>
<rawString>St´ephane Ross, Geoffrey J. Gordon, and Drew Bagnell. 2011. A reduction of imitation learning and structured prediction to no-regret online learning. Journal of Machine Learning Research - Proceedings Track, 15:627–635.</rawString>
</citation>
<citation valid="true">
<authors>
<author>Wee Meng Soon</author>
<author>Daniel Chung</author>
</authors>
<title>Daniel Chung Yong Lim, Yong Lim, and Hwee Tou Ng.</title>
<date>2001</date>
<marker>Soon, Chung, 2001</marker>
<rawString>Wee Meng Soon, Daniel Chung, Daniel Chung Yong Lim, Yong Lim, and Hwee Tou Ng. 2001. A machine learning approach to coreference resolution of noun phrases.</rawString>
</citation>
<citation valid="true">
<authors>
<author>Veselin Stoyanov</author>
<author>Jason Eisner</author>
</authors>
<title>Easy-first coreference resolution.</title>
<date>2012</date>
<booktitle>In Proceedings of International Conference on Computational Linguistics (COLING),</booktitle>
<pages>2519--2534</pages>
<contexts>
<context position="1742" citStr="Stoyanov and Eisner, 2012" startWordPosition="246" endWordPosition="249">e-art approaches on multiple benchmark corpora including OntoNotes. 1 Introduction Coreference resolution is the task of clustering a set of mentions in the text such that all mentions in the same cluster refer to the same entity. It is one of the first stages in deep language understanding and has a big potential impact on the rest of the stages. Several of the state-of-the-art approaches learn a scoring function defined over mention pair, cluster-mention or cluster-cluster pair to guide the coreference decision-making process (Daum´e III, 2006; Bengtson and Roth, 2008; Rahman and Ng, 2011b; Stoyanov and Eisner, 2012; Chang et al., 2013; Durrett et al., 2013; Durrett and Klein, 2013). One common and persistent problem with these approaches is that the scoring function has to make all the coreference decisions, which leads to a highly non-realizable learning problem. Inspired by the recent success of the HC-Search Framework (Doppa et al., 2014a) for studying a variety of structured prediction problems (Lam et al., 2013; Doppa et al., 2014c), we study a novel approach for search-based coreference resolution called Prune-and-Score. HC-Search is a divideand-conquer solution that learns multiple components wit</context>
<context position="7127" citStr="Stoyanov and Eisner, 2012" startWordPosition="1088" endWordPosition="1091">2012). These methods address some of the problems with pair-wise classifiers, however, they suffer from the intractability of “Argmin” inference (finding the least cost clustering output among exponential possibilities) that is encountered during both training and testing. As a result, they resort to approximate inference algorithms (e.g., MCMC, loopy belief propagation), which can suffer from local optima. Third, the incremental approaches construct the clustering output incrementally by processing the mentions in some order (Daum´e III, 2006; Denis and Baldridge, 2008; Rahman and Ng, 2011b; Stoyanov and Eisner, 2012; Chang et al., 2013; Durrett et al., 2013; Durrett and Klein, 2013). These methods learn a scoring function to guide the decision-making process and differ in the form of the scoring function (e.g., mention pair, clustermention or cluster-cluster pair) and how it is being learned. They have shown great success and are very efficient. Indeed, several of the approaches that have achieved state-of-the-art results on OntoNotes fall under this category (Chang et al., 2013; Durrett et al., 2013; Durrett and Klein, 2013; Bj¨orkelund and Kuhn, 2014). However, their efficiency requirement leads to a h</context>
<context position="27293" citStr="Stoyanov and Eisner, 2012" startWordPosition="4521" endWordPosition="4525">ts for validation, and use the remaining ones for training. Evaluation Metrics. We compute three most popular performance metrics for coreference resolution: MUC (Vilain et al., 1995), B-Cubed (Bagga and Baldwin, 1998), and Entity-based CEAF (CEAFφ4) (Luo, 2005). As it is commonly done in CoNLL shared tasks (Pradhan et al., 2012), we employ the average F1 score (CoNLL F1) of these three metrics for comparison purposes. We evaluate all the results using the updated version1 (7.0) of the coreference scorer. Features. We built2 our coreference resolver based on the Easy-first coreference system (Stoyanov and Eisner, 2012), which is derived from the Reconcile system (Stoyanov et al., 2010). We essentially employ the same features as in the Easyfirst system. However, we provide some highlevel details that are necessary for subsequent discussion. Recall that our features 0(s, a) for both pruning and scoring functions are defined over state-action pairs, where each state s consists of a set of clusters and an action a corresponds to merging an unprocessed mention m with a cluster C in state s or create one for itself. Therefore, 0(s, a) defines features over cluster-mention pairs (C, m). Our feature vector consist</context>
<context position="38121" citStr="Stoyanov and Eisner, 2012" startWordPosition="6313" endWordPosition="6316">6.27 61.61 20 67.97 57.73 56.63 60.78 ∞ 67.03 56.31 55.56 59.63 Table 3: Performance of Prune-and-Score approach with different values of the pruning parameter b. For b = ∞, Prune-and-Score becomes an Only-Scoring algorithm. Comparison to State-of-the-Art. Table 4 shows the results of our Prune-and-Score approach compared with the following state-of-theart coreference resolution approaches: HOTCoref system (Bj¨orkelund and Kuhn, 2014); Berkeley system with the FINAL feature set (Durrett and Klein, 2013); CPL3M system (Chang et al., 2013); Stanford system (Lee et al., 2013); Easy-first system (Stoyanov and Eisner, 2012); and Fernandes et al., 2012 (Fernandes et al., 2012). Only Scoring is the special case of our Prune-andScore approach where we employ only the scoring function. This corresponds to existing incremental approaches (Daum´e III, 2006; Rahman and Ng, 2011b). We report the best published results for CPL3M system, Easy-first, and Fernandes et al., 2012. We ran the publicly available software to generate the results for Berkeley and Stanford systems with the updated CoNLL scorer. We include the results of Prune-and-Score for best b on the development set with singleton mention filter for the compari</context>
</contexts>
<marker>Stoyanov, Eisner, 2012</marker>
<rawString>Veselin Stoyanov and Jason Eisner. 2012. Easy-first coreference resolution. In Proceedings of International Conference on Computational Linguistics (COLING), pages 2519–2534.</rawString>
</citation>
<citation valid="true">
<authors>
<author>Veselin Stoyanov</author>
<author>Claire Cardie</author>
<author>Nathan Gilbert</author>
<author>Ellen Riloff</author>
<author>David Buttler</author>
<author>David Hysom</author>
</authors>
<title>Coreference resolution with reconcile.</title>
<date>2010</date>
<booktitle>In Proceedings ofAssociation of Computational Linguistics (ACL) Conference,</booktitle>
<pages>156--161</pages>
<contexts>
<context position="27361" citStr="Stoyanov et al., 2010" startWordPosition="4533" endWordPosition="4536">etrics. We compute three most popular performance metrics for coreference resolution: MUC (Vilain et al., 1995), B-Cubed (Bagga and Baldwin, 1998), and Entity-based CEAF (CEAFφ4) (Luo, 2005). As it is commonly done in CoNLL shared tasks (Pradhan et al., 2012), we employ the average F1 score (CoNLL F1) of these three metrics for comparison purposes. We evaluate all the results using the updated version1 (7.0) of the coreference scorer. Features. We built2 our coreference resolver based on the Easy-first coreference system (Stoyanov and Eisner, 2012), which is derived from the Reconcile system (Stoyanov et al., 2010). We essentially employ the same features as in the Easyfirst system. However, we provide some highlevel details that are necessary for subsequent discussion. Recall that our features 0(s, a) for both pruning and scoring functions are defined over state-action pairs, where each state s consists of a set of clusters and an action a corresponds to merging an unprocessed mention m with a cluster C in state s or create one for itself. Therefore, 0(s, a) defines features over cluster-mention pairs (C, m). Our feature vector consists of three parts: a) mention pair features; b) entity pair features;</context>
</contexts>
<marker>Stoyanov, Cardie, Gilbert, Riloff, Buttler, Hysom, 2010</marker>
<rawString>Veselin Stoyanov, Claire Cardie, Nathan Gilbert, Ellen Riloff, David Buttler, and David Hysom. 2010. Coreference resolution with reconcile. In Proceedings ofAssociation of Computational Linguistics (ACL) Conference, pages 156–161.</rawString>
</citation>
<citation valid="true">
<authors>
<author>Marc B Vilain</author>
<author>John D Burger</author>
<author>John S Aberdeen</author>
<author>Dennis Connolly</author>
<author>Lynette Hirschman</author>
</authors>
<title>A model-theoretic coreference scoring scheme.</title>
<date>1995</date>
<booktitle>In MUC,</booktitle>
<pages>45--52</pages>
<contexts>
<context position="26850" citStr="Vilain et al., 1995" startWordPosition="4450" endWordPosition="4453">7 documents (ACE2004-CULOTTA-TEST) for testing. We also evaluate our system on the 128 newswire documents in ACE 2004 corpus for a fair comparison with the state-of-the-art. The MUC6 corpus containts 255 documents. We employ the official test set of 30 documents (MUC6- TEST) for testing purposes. From the remaining 225 documents, which includes 195 official training documents and 30 dry-run test documents, we randomly pick 30 documents for validation, and use the remaining ones for training. Evaluation Metrics. We compute three most popular performance metrics for coreference resolution: MUC (Vilain et al., 1995), B-Cubed (Bagga and Baldwin, 1998), and Entity-based CEAF (CEAFφ4) (Luo, 2005). As it is commonly done in CoNLL shared tasks (Pradhan et al., 2012), we employ the average F1 score (CoNLL F1) of these three metrics for comparison purposes. We evaluate all the results using the updated version1 (7.0) of the coreference scorer. Features. We built2 our coreference resolver based on the Easy-first coreference system (Stoyanov and Eisner, 2012), which is derived from the Reconcile system (Stoyanov et al., 2010). We essentially employ the same features as in the Easyfirst system. However, we provide</context>
</contexts>
<marker>Vilain, Burger, Aberdeen, Connolly, Hirschman, 1995</marker>
<rawString>Marc B. Vilain, John D. Burger, John S. Aberdeen, Dennis Connolly, and Lynette Hirschman. 1995. A model-theoretic coreference scoring scheme. In MUC, pages 45–52.</rawString>
</citation>
<citation valid="true">
<authors>
<author>David Weiss</author>
<author>Benjamin Taskar</author>
</authors>
<title>Structured prediction cascades.</title>
<date>2010</date>
<journal>Journal of Machine Learning Research - Proceedings Track,</journal>
<pages>9--916</pages>
<contexts>
<context position="3993" citStr="Weiss and Taskar, 2010" startWordPosition="604" endWordPosition="607"> idea of learning nearly-sound constraints for pruning, and leveraging the learned constraints to learn improved heuristic functions for guiding the search. The pruning constraints can take different forms (e.g., classifiers, decisionlist, or ranking functions) depending on the search architecture. Therefore, other coreference resolution systems (Chang et al., 2013; Durrett and Klein, 2013; Bj¨orkelund and Kuhn, 2014) can also benefit from this idea. While our basic idea of twolevel selection might appear similar to the coarseto-fine inference architectures (Felzenszwalb and McAllester, 2007; Weiss and Taskar, 2010), the details differ significantly. Importantly, our pruning and scoring functions operate sequentially at 2115 Proceedings of the 2014 Conference on Empirical Methods in Natural Language Processing (EMNLP), pages 2115–2126, October 25-29, 2014, Doha, Qatar. c�2014 Association for Computational Linguistics each greedy search step, whereas in the cascades approach, the second level function makes its prediction only when the first level decision-making is done. Summary of Contributions. The main contributions of our work are as follows. First, we motivate and introduce the Prune-and-Score appro</context>
</contexts>
<marker>Weiss, Taskar, 2010</marker>
<rawString>David Weiss and Benjamin Taskar. 2010. Structured prediction cascades. Journal of Machine Learning Research - Proceedings Track, 9:916–923.</rawString>
</citation>
<citation valid="true">
<authors>
<author>Michael L Wick</author>
<author>Khashayar Rohanimanesh</author>
<author>Kedar Bellare</author>
<author>Aron Culotta</author>
<author>Andrew McCallum</author>
</authors>
<title>SampleRank: Training factor graphs with atomic gradients.</title>
<date>2011</date>
<booktitle>In Proceedings of International Conference on Machine Learning (ICML).</booktitle>
<contexts>
<context position="6463" citStr="Wick et al., 2011" startWordPosition="989" endWordPosition="992"> pair-wise scores to make predictions. However, the pair-wise classifier approach suffers from several drawbacks including class imbalance (fewer positive edges compared to negative edges) and not being able to leverage the global structure (instead making independent local decisions). Second, the global approaches such as Structured SVMs and Conditional Random Fields (CRFs) learn a cost function to score a potential clustering output for a given input set of mentions (Mccallum and Wellner, 2003; Finley and Joachims, 2005; Culotta et al., 2007; Yu and Joachims, 2009; Haghighi and Klein, 2010; Wick et al., 2011; Wick et al., 2012; Fernandes et al., 2012). These methods address some of the problems with pair-wise classifiers, however, they suffer from the intractability of “Argmin” inference (finding the least cost clustering output among exponential possibilities) that is encountered during both training and testing. As a result, they resort to approximate inference algorithms (e.g., MCMC, loopy belief propagation), which can suffer from local optima. Third, the incremental approaches construct the clustering output incrementally by processing the mentions in some order (Daum´e III, 2006; Denis and </context>
</contexts>
<marker>Wick, Rohanimanesh, Bellare, Culotta, McCallum, 2011</marker>
<rawString>Michael L. Wick, Khashayar Rohanimanesh, Kedar Bellare, Aron Culotta, and Andrew McCallum. 2011. SampleRank: Training factor graphs with atomic gradients. In Proceedings of International Conference on Machine Learning (ICML).</rawString>
</citation>
<citation valid="true">
<authors>
<author>Michael L Wick</author>
<author>Sameer Singh</author>
<author>Andrew McCallum</author>
</authors>
<title>A discriminative hierarchical model for fast coreference at large scale.</title>
<date>2012</date>
<booktitle>In Proceedings ofAssociation of Computational Linguistics (ACL) Conference,</booktitle>
<pages>379--388</pages>
<contexts>
<context position="6482" citStr="Wick et al., 2012" startWordPosition="993" endWordPosition="996">o make predictions. However, the pair-wise classifier approach suffers from several drawbacks including class imbalance (fewer positive edges compared to negative edges) and not being able to leverage the global structure (instead making independent local decisions). Second, the global approaches such as Structured SVMs and Conditional Random Fields (CRFs) learn a cost function to score a potential clustering output for a given input set of mentions (Mccallum and Wellner, 2003; Finley and Joachims, 2005; Culotta et al., 2007; Yu and Joachims, 2009; Haghighi and Klein, 2010; Wick et al., 2011; Wick et al., 2012; Fernandes et al., 2012). These methods address some of the problems with pair-wise classifiers, however, they suffer from the intractability of “Argmin” inference (finding the least cost clustering output among exponential possibilities) that is encountered during both training and testing. As a result, they resort to approximate inference algorithms (e.g., MCMC, loopy belief propagation), which can suffer from local optima. Third, the incremental approaches construct the clustering output incrementally by processing the mentions in some order (Daum´e III, 2006; Denis and Baldridge, 2008; Ra</context>
</contexts>
<marker>Wick, Singh, McCallum, 2012</marker>
<rawString>Michael L. Wick, Sameer Singh, and Andrew McCallum. 2012. A discriminative hierarchical model for fast coreference at large scale. In Proceedings ofAssociation of Computational Linguistics (ACL) Conference, pages 379–388.</rawString>
</citation>
<citation valid="true">
<authors>
<author>Yuehua Xu</author>
<author>Alan Fern</author>
<author>Sung Wook Yoon</author>
</authors>
<title>Learning linear ranking functions for beam search with application to planning.</title>
<date>2009</date>
<journal>Journal of Machine Learning Research (JMLR),</journal>
<pages>10--1571</pages>
<contexts>
<context position="20350" citStr="Xu et al., 2009" startWordPosition="3358" endWordPosition="3361"> actions to minimize Eprune. For this, we assume that for any training inputoutput pair (x, y∗) there exists a unique action sequence, or solution path (initial state to terminal state), for producing y∗ from x. More formally, let (s∗0, a∗0), (s∗1, a∗1), , (s∗D, ∅) correspond to the sequence of state-action pairs along this solution path, where s∗ 0 is the initial state and s∗D is the terminal state. The goal is to learn the parameters of Fprune such that at each state s∗i , a∗i E A(s∗i) is ranked among the top b actions. While we can employ an online-LaSO style approach (III and Marcu, 2005; Xu et al., 2009) to learn the parameters of the pruning function, it is quite inefficient, as it must regenerate the same search trajectory again and again until it learns to make the right decision. Additionally, this approach limits applicability of the off-the-shelf learners to learn the parameters of Fprune. To overcome these drawbacks, we apply offline training. Reduction to Rank Learning. We reduce the pruning function learning to a rank learning problem. This allows us to leverage powerful and efficient off-the-shelf rank-learners (Liu, 2009). The reduction is as follows. At each state s∗i on the solut</context>
</contexts>
<marker>Xu, Fern, Yoon, 2009</marker>
<rawString>Yuehua Xu, Alan Fern, and Sung Wook Yoon. 2009. Learning linear ranking functions for beam search with application to planning. Journal of Machine Learning Research (JMLR), 10:1571–1610.</rawString>
</citation>
<citation valid="true">
<authors>
<author>Chun-Nam John Yu</author>
<author>Thorsten Joachims</author>
</authors>
<title>Learning structural SVMs with latent variables.</title>
<date>2009</date>
<booktitle>In Proceedings of International Conference on Machine Learning (ICML).</booktitle>
<contexts>
<context position="6418" citStr="Yu and Joachims, 2009" startWordPosition="981" endWordPosition="984">approximate decoding or post-processing using the pair-wise scores to make predictions. However, the pair-wise classifier approach suffers from several drawbacks including class imbalance (fewer positive edges compared to negative edges) and not being able to leverage the global structure (instead making independent local decisions). Second, the global approaches such as Structured SVMs and Conditional Random Fields (CRFs) learn a cost function to score a potential clustering output for a given input set of mentions (Mccallum and Wellner, 2003; Finley and Joachims, 2005; Culotta et al., 2007; Yu and Joachims, 2009; Haghighi and Klein, 2010; Wick et al., 2011; Wick et al., 2012; Fernandes et al., 2012). These methods address some of the problems with pair-wise classifiers, however, they suffer from the intractability of “Argmin” inference (finding the least cost clustering output among exponential possibilities) that is encountered during both training and testing. As a result, they resort to approximate inference algorithms (e.g., MCMC, loopy belief propagation), which can suffer from local optima. Third, the incremental approaches construct the clustering output incrementally by processing the mention</context>
</contexts>
<marker>Yu, Joachims, 2009</marker>
<rawString>Chun-Nam John Yu and Thorsten Joachims. 2009. Learning structural SVMs with latent variables. In Proceedings of International Conference on Machine Learning (ICML).</rawString>
</citation>
<citation valid="true">
<authors>
<author>Jiaping Zheng</author>
<author>Luke Vilnis</author>
<author>Sameer Singh</author>
<author>Jinho D Choi</author>
<author>Andrew McCallum</author>
</authors>
<title>Dynamic knowledge-base alignment for coreference resolution.</title>
<date>2013</date>
<booktitle>In Conference on Computational Natural Language Learning (CoNLL).</booktitle>
<contexts>
<context position="8515" citStr="Zheng et al., 2013" startWordPosition="1301" endWordPosition="1304">ng rules) makes the learning problem easier and can improve over the performance of scoring-only approaches. Also, the models in (Chang et al., 2013; Durrett et al., 2013) try to leverage cluster-level information implicitly (via latent antecedents) from mention-pair features, whereas our model explicitly leverages the cluster level information. Coreference resolution systems can benefit by incorporating the world knowledge including rules, constraints, and additional information from external knowledge bases (Lee et al., 2013; Rahman and Ng, 2011a; Ratinov and Roth, 2012; Chang et al., 2013; Zheng et al., 2013; Hajishirzi et al., 2013). Our work is orthogonal to this line of work, but domain constraints and rules can be incorporated into our model as done in (Chang et al., 2013). 3 Problem Setup Coreference resolution is a structured prediction problem where the set of mentions m1, m2, · · · , mD extracted from a document cor2116 reponds to a structured input x and the structured output y corresponds to a partition of the mentions into a set of clusters C1, C2, · · · , Ck. Each mention mi belongs to exactly one of the clusters Cj. We are provided with a training set of inputoutput pairs drawn from </context>
</contexts>
<marker>Zheng, Vilnis, Singh, Choi, McCallum, 2013</marker>
<rawString>Jiaping Zheng, Luke Vilnis, Sameer Singh, Jinho D. Choi, and Andrew McCallum. 2013. Dynamic knowledge-base alignment for coreference resolution. In Conference on Computational Natural Language Learning (CoNLL).</rawString>
</citation>
</citationList>
</algorithm>
</algorithms>
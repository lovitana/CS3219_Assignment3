<?xml version="1.0" encoding="UTF-8"?>
<algorithms version="110505">
<algorithm name="SectLabel" version="110505">
<variant no="0" confidence="0.000307">
<title confidence="0.99574">
A∗ CCG Parsing with a Supertag-factored Model
</title>
<author confidence="0.993303">
Mike Lewis
</author>
<affiliation confidence="0.997782">
School of Informatics
University of Edinburgh
</affiliation>
<address confidence="0.982642">
Edinburgh, E118 9AB, UK
</address>
<email confidence="0.999221">
mike.lewis@ed.ac.uk
</email>
<author confidence="0.997833">
Mark Steedman
</author>
<affiliation confidence="0.998068">
School of Informatics
University of Edinburgh
</affiliation>
<address confidence="0.982569">
Edinburgh, E118 9AB, UK
</address>
<email confidence="0.99915">
steedman@inf.ed.ac.uk
</email>
<sectionHeader confidence="0.993902" genericHeader="abstract">
Abstract
</sectionHeader>
<bodyText confidence="0.999807888888889">
We introduce a new CCG parsing model
which is factored on lexical category as-
signments. Parsing is then simply a de-
terministic search for the most probable
category sequence that supports a CCG
derivation. The parser is extremely simple,
with a tiny feature set, no POS tagger, and
no statistical model of the derivation or
dependencies. Formulating the model in
this way allows a highly effective heuris-
tic for A* parsing, which makes parsing
extremely fast. Compared to the standard
C&amp;C CCG parser, our model is more ac-
curate out-of-domain, is four times faster,
has higher coverage, and is greatly simpli-
fied. We also show that using our parser
improves the performance of a state-of-
the-art question answering system.
</bodyText>
<sectionHeader confidence="0.998989" genericHeader="keywords">
1 Introduction
</sectionHeader>
<bodyText confidence="0.999828000000001">
CCG is a strongly lexicalized grammatical formal-
ism, in which the vast majority of the decisions
made during interpretation involve choosing the
correct definitions of words. We explore the ef-
fect of modelling this explicitly in a parser, by
only using a probabilistic model of lexical cate-
gories (based on a local context window), rather
than modelling the derivation or dependencies.
Existing state-of-the-art CCG parsers use com-
plex pipelines of POS-tagging, supertagging and
parsing—each with its own feature sets and pa-
rameters (and sources of error)—together with fur-
ther parameters governing their integration (Clark
and Curran, 2007). We show that much simpler
models can achieve high performance. Our model
predicts lexical categories based on a tiny fea-
ture set of word embeddings, capitalization, and 2-
character suffixes—with no parsing model beyond
a small set of CCG combinators, and no POS-
tagger. Simpler models are easier to implement,
replicate and extend.
Another goal of our model is to parse CCG
optimally and efficiently, without using excessive
pruning. CCG’s large set of lexical categories,
and generalized notion of constituency, mean that
sentences can have a huge number of potential
parses. Fast existing CCG parsers rely on aggres-
sive pruning—for example, the C&amp;C parser uses
a supertagger to dramatically cut the search space
considered by the parser. Even the loosest beam
setting for their supertagger discards the correct
parse for 20% of sentences. The structure of our
model allows us to introduce a simple but power-
ful heuristic for A* parsing, meaning it can parse
almost 50 sentences per second exactly, with no
beam-search or pruning. Adding very mild prun-
ing increases the speed to 186 sentences per sec-
ond with minimal loss of accuracy.
Our approach faces two obvious challenges.
Firstly, categories are assigned based on a local
window, which may not contain the necessary con-
text for resolving some attachment decisions. For
example, in I saw a squirrel 2 weeks ago with a
nut, the model cannot make an informed decision
on whether to assign with an adverbial or adnomi-
nal preposition category, as the crucial words saw
and squirrel fall outside the local context window.
Secondly, even if the supertagger makes all lexical
category decisions correctly, then the parser can
still make erroneous decisions. One example is
in coordination-scope ambiguities, such as clever
boys and girls, where the two interpretations use
the same assignment of categories.
We hypothesise that such decisions are rela-
tively rare, and are challenging for any parsing
model, so a weak model is unlikely to result in
substantially lower accuracy. Our implementation
of this model1, which we call EASYCCG, has high
</bodyText>
<footnote confidence="0.9849015">
1Available from https://github.com/
mikelewis0/easyccg
</footnote>
<page confidence="0.886438">
990
</page>
<note confidence="0.9120685">
Proceedings of the 2014 Conference on Empirical Methods in Natural Language Processing (EMNLP), pages 990–1000,
October 25-29, 2014, Doha, Qatar. c�2014 Association for Computational Linguistics
</note>
<bodyText confidence="0.99965075">
accuracy—suggesting that most parsing decisions
can be made accurately based on a local context
window.
Of course, there are many parsing decisions that
can only be made accurately with more complex
models. However, exploring the power and lim-
itations of simpler models may help focus future
research on the more challenging cases.
</bodyText>
<sectionHeader confidence="0.995917" genericHeader="introduction">
2 Background
</sectionHeader>
<subsectionHeader confidence="0.982693">
2.1 Combinatory Categorial Grammar
</subsectionHeader>
<bodyText confidence="0.999975416666667">
CCG (Steedman, 2000) is a strongly lexicalized
grammatical formalism. Words have categories
representing their syntactic role, which are either
atomic, or functions from one category to another.
Phrase-structure grammars have a relatively
small number of lexical categories types (e.g.
POS-tags), and a large set of rules used to build
a syntactic analysis of a complete sentence (e.g.
an adjective and noun can combine into a noun).
In contrast, CCG parsing has many lexical cate-
gory types (we use 425), but a small set of combi-
natory rule types (we use 10 binary and 13 unary
rule schemata). This means that, aside from the
lexicon, the grammar is small enough to be hand-
coded—which allows us, in this paper, to confine
the entire statistical model to the lexicon.
CCG’s generalized notion of constituency
means that many derivations are possible for
a given a set of lexical categories. However,
most of these derivations will be semantically
equivalent—for example, deriving the same de-
pendency structures—in which case the actual
choice of derivation is unimportant. Such ambi-
guity is often called spurious.
</bodyText>
<subsectionHeader confidence="0.993031">
2.2 Existing CCG Parsing Models
</subsectionHeader>
<bodyText confidence="0.999884916666667">
The seminal C&amp;C parser is by far the most pop-
ular choice of CCG parser (Clark and Curran,
2007). It showed that it was possible to parse to
an expressive linguistic formalism with high speed
and accuracy. The performance of the parser has
enabled large-scale logic-based distributional re-
search (Harrington, 2010; Lewis and Steedman,
2013a; Lewis and Steedman, 2013b; Reddy et al.,
2014), and it is a key component of Boxer (Bos,
2008).
The C&amp;C parser uses CKY chart parsing, with a
log-linear model to rank parses. The vast number
of possible parses means that computing the com-
plete chart is impractical. To resolve this prob-
lem, a supertagger is first run over the sentence to
prune the set of lexical categories considered by
the parser for each word. The initial beam out-
puts an average of just 1.2 categories per word,
rather than the 425 possible categories—making
the standard CKY parsing algorithm very efficient.
If the parser fails to find any analysis of the com-
plete sentence with this set of supertags, the su-
pertagger re-analyses the sentence with a more re-
laxed beam (adaptive supertagging).
</bodyText>
<subsectionHeader confidence="0.99937">
2.3 A* Parsing
</subsectionHeader>
<bodyText confidence="0.999979310344828">
Klein and Manning (2003a) introduce A* parsing
for PCFGs. The parser maintains a chart and an
agenda, which is a priority queue of items to add to
the chart. The agenda is sorted based on the items’
inside probability, and a heuristic upper-bound on
the outside probability—to give an upper bound
on the probability of the complete parse. The chart
is then expanded in best-first order, until a com-
plete parse for the sentence is found.
Klein and Manning calculate an upper bound on
the outside probability of a span based on a sum-
mary of the context. For example, the summary
for the SX heuristic is the category of the span, and
the number of words in the sentence before and af-
ter the span. The value of the heuristic is the prob-
ability of the best possible sentence meeting these
restrictions. These probabilities are pre-computed
for every non-terminal symbol and for every pos-
sible number of preceding and succeeding words,
leading to large look-up tables.
Auli and Lopez (2011b) find that A* CCG pars-
ing with this heuristic is very slow. However,
they achieve a modest 15% speed improvement
over CKY when A* is combined with adaptive su-
pertagging. One reason is that the heuristic esti-
mate is rather coarse, as it deals with the best pos-
sible outside context, rather than the actual sen-
tence. We introduce a new heuristic which gives a
tighter upper bound on the outside probability.
</bodyText>
<sectionHeader confidence="0.999251" genericHeader="method">
3 Model
</sectionHeader>
<subsectionHeader confidence="0.999507">
3.1 Lexical Category Model
</subsectionHeader>
<bodyText confidence="0.995964714285714">
As input, our parser takes a distribution over all
CCG lexical categories for each word in the sen-
tence. These distributions are assigned using
Lewis and Steedman (2014)’s semi-supervised su-
pertagging model. The supertagger is a unigram
log-linear classifier that uses features of the ±3
word context window surrounding a word. The
</bodyText>
<page confidence="0.996787">
991
</page>
<bodyText confidence="0.999841428571429">
key feature is word embeddings, initialized with
the 50-dimensional embeddings trained in Turian
et al. (2010), and fine-tuned during supervised
training. The model also uses 2-character suffixes
and capitalization features.
The use of word embeddings, which are trained
on a large unlabelled corpus, allows the supertag-
ger to generalize well to words not present in the
labelled data. It does not use a POS-tagger, which
avoids problems caused by POS-tagging errors.
Our methods could be applied to any supertag-
ging model, but we find empirically that this
model gives higher performance than the C&amp;C su-
pertagger.
</bodyText>
<subsectionHeader confidence="0.996939">
3.2 Parsing Model
</subsectionHeader>
<bodyText confidence="0.984048714285714">
Let a CCG parse y of a sentence S be a list of
lexical categories cl ... cn and a derivation. If we
assume all derivations licensed by our grammar
are equally likely, and that lexical category assign-
ments are conditionally independent given the sen-
tence, we can compute the optimal parse yˆ as:
�n
</bodyText>
<equation confidence="0.984901">
yˆ = argmaxy i i p(ci|S)
</equation>
<bodyText confidence="0.999948269230769">
As discussed in Section 2.1, many derivations
are possible given a sequence of lexical categories,
some of which may be semantically distinct. How-
ever, our model will assign all of these an equal
score, as they use the same sequence of lexical
categories. Therefore we extend our model with
a simple deterministic heuristic for ranking parses
that use the same lexical categories. Given a set of
derivations with equal probability, we output the
one maximizing the sums of the length of all arcs
in the corresponding dependency tree.
The effect of this heuristic is to prefer non-
local attachments in cases of ambiguity, which
we found worked better on development data than
favouring local attachments. In cases of spurious
ambiguity, all parses will have the same value of
this heuristic, so one is chosen arbitrarily. For
example, one of the parses in Figures 1a and 1b
would be selected over the parse in Figure 1c.
Of course, we could use any function of the
parses in place of this heuristic, for example a
head-dependency model. However, one aim of
this paper is to demonstrate that an extremely sim-
ple parsing model can achieve high performance,
so we leave more sophisticated alternatives to fu-
ture work.
</bodyText>
<figure confidence="0.391159428571429">
a house in Paris in France
NP (NP\NP)/NP NP (NP\NP)/NP NP
&gt; &gt;
NP\NP NP\NP
&lt;
&lt;
(a) A standard derivation of a house in Paris in France, with a
dependency from in France to house
a house in Paris in France
NP (NP\NP)/NP NP (NP\NP)/NP NP
&gt; &gt;
NP\NP NP\NP
NP\NP
&lt;
</figure>
<listItem confidence="0.9690644">
(b) A derivation of a house in Paris in France, which is spu-
riously equivalent to Figure 1a. A composition combinator is
used to compose the predicates in Paris and in France, creating
a constituent which creates dependencies to its argument from
both in Paris and in France.
a house in Paris in France
NP (NP\NP)/NP NP (NP\NP)/NP NP
(c) A derivation of a house in Paris in France, which yields
different dependencies to Figures 1a and 1b: here, there is a
dependency from in France to Paris, not house.
</listItem>
<figureCaption confidence="0.62101725">
Figure 1: Three CCG parses of a house in Paris
in France, given the same set of supertags. The
first two are spuriously equivalent, but the third is
semantically distinct.
</figureCaption>
<subsectionHeader confidence="0.997613">
3.3 A* Search
</subsectionHeader>
<bodyText confidence="0.999641823529412">
For parsing, we use an A* search for the most-
probable complete CCG derivation of a sentence.
A key advantage of A* parsing over CKY parsing
is that it does not require us to prune the search
space first with a supertagger, allowing the parser
to consider the complete distribution of 425 cate-
gories for each word (in contrast to an average of
3.57 categories per word considered by the C&amp;C
parser’s most relaxed beam). This is possible be-
cause A* only searches for the Viterbi parse of
a sentence, rather than building a complete chart
with every possible category per word (another al-
ternative, used by Hockenmaier (2003), is to use a
highly aggressive beam search in the parser).
In A* parsing, items on the agenda are sorted by
their cost; the product of their inside probability
and an upper bound on their outside probability.
</bodyText>
<figure confidence="0.99764875">
&gt;
NP\NP
&lt;
NP
&gt;
NP\NP
&lt;
NP
NP
NP
&gt;B
NP
</figure>
<page confidence="0.987805">
992
</page>
<bodyText confidence="0.999627">
For a span wi ... wj with lexical categories
ci ... cj in a sentence S = w1 ... wn, the inside
probability is simply: jljk=i p(ck|S)
The factorization of our model lets us give the
following upper-bound on the outside probability:
</bodyText>
<equation confidence="0.9988155">
h(wi ... wj) = Ilk&lt;i
k=1 maxckp(ck|S)×
�k≤n
k=j+1 maxckp(ck|S)
</equation>
<bodyText confidence="0.999990130434783">
This heuristic assumes that all words outside the
span will take their highest-probability supertag.
Because the model is factored on lexical cate-
gories, this estimate is clearly an upper bound.
As supertagging is over 90% accurate, the upper
bound will often be exact, and in Section 4.3 we
show empirically that it is extremely efficient. The
values of the heuristic can be computed once for
each sentence and cached.
To implement the preference for non-local at-
tachment described in Section 3.2, if two agenda
items have the same cost, the one with the longer
dependencies is preferred.
Intuitively, the parser first attempts to find a
parse for the sentence using the 1-best category for
each word, by building as complete a chart as pos-
sible. If it fails to find a parse for the complete
sentence, it adds one more supertag to the chart
(choosing the most probable tag not already in the
chart), and tries again. This strategy allows the
parser to consider an unbounded number of cate-
gories for each word, as it does not build a com-
plete chart with all supertags.
</bodyText>
<subsectionHeader confidence="0.913549">
3.4 Grammar
</subsectionHeader>
<bodyText confidence="0.999868526315789">
Here, we describe the set of combinators and
unary rules in the EASYCCG grammar. Because
we do not have any probabilistic model of the
derivation, all rules can apply with equal probabil-
ity. This means that some care needs to be taken
in designing the grammar to ensure that all the
rules are generally applicable. We also try to limit
spurious ambiguity, and build derivations which
are compatible with the C&amp;C parser’s scripts for
extracting dependencies (for evaluation). We de-
scribe the grammar in detail, to ensure replicabil-
ity.
Our parser uses the following binary combi-
nators from Steedman (2012): forward applica-
tion, backward application, forward composition,
backward crossed composition, generalized for-
ward composition, generalized backward crossed
composition. These combinators are posited to
be linguistically universal. The generalized rules
</bodyText>
<table confidence="0.9978221875">
Initial Result Usage
N NP Bare
noun
phrases
NP S/(S\NP) Type
NP (S\NP)/((S\NP)/NP) raising
PP (S\NP)/((S\NP)/PP)
Spss\NP NP\NP Reduced
Sng\NP NP\NP relative
Sadj\NP NP\NP clauses
Sto\NP NP\NP
Sto\NP N\N
Sdcl/NP NP\NP
Spss\NP S/S VP
Sng\NP S/S Sentence
Sto\NP S/S Modifiers
</table>
<tableCaption confidence="0.9999">
Table 1: Set of unary rules used by the parser.
</tableCaption>
<bodyText confidence="0.988983714285714">
are generalized to degree 2. Following Steedman
(2000) and Clark and Curran (2007), backward
composition is blocked where the argument of the
right-hand category is an N or NP. The unhelpful
[nb] feature is ignored.
As in the C&amp;C parser, we add a special Con-
junction rule:
</bodyText>
<equation confidence="0.980579333333333">
Y X
&gt;
X\X
</equation>
<bodyText confidence="0.974216739130435">
Where Y E {conj, comma, semicolon}. We
block conjunctions where the right-hand category
is type-raised, punctuation, N, or NP\NP. This
rule (and the restrictions) could be removed by
changing CCGBank to analyse conjunctions with
(X\X)/X categories.
We also add syntagmatic rules for removing any
punctuation to the right, and for removing open-
brackets and open-quotes to the left
The grammar also contains 13 unary rules,
listed in Table 1. These rules were chosen based
on their frequency in the training data, and their
clear semantic interpretations.
Following Clark and Curran (2007), we also add
a (switchable) constraint that only category com-
binations that have combined in the training data
may combine in the test data. We found that this
was necessary for evaluation, as the C&amp;C conver-
sion tool for extracting predicate-argument depen-
dencies had relatively low coverage on the CCG
derivations produced by our parser. While this
restriction is theoretically inelegant, we found it
did increase parsing speed without lowering lexi-
</bodyText>
<page confidence="0.995007">
993
</page>
<bodyText confidence="0.999251909090909">
cal category accuracy.
We also use Eisner Normal Form Constraints
(Eisner, 1996), and Hockenmaier and Bisk’s
(2010) Constraint 5, which automatically rule out
certain spuriously equivalent derivations, improv-
ing parsing speed.
We add a hard constraint that the root category
of the sentence must be a declarative sentence, a
question, or a noun-phrase.
This grammar is smaller and cleaner than that
used by the C&amp;C parser, which uses 32 unary
rules (some of which are semantically dubious,
such as S[dcl] → NP\NP), and non-standard bi-
nary combinators such as merging two NPs into
an NP. The C&amp;C parser also has a large num-
ber of special case rules for handling punctua-
tion. Our smaller grammar reduces the grammar
constant, eases implementation, and simplifies the
job of building downstream semantic parsers such
as those of Bos (2008) or Lewis and Steedman
(2013a) (which must implement semantic analogs
of each syntactic rule).
</bodyText>
<subsectionHeader confidence="0.990105">
3.5 Extracting Dependency Structures
</subsectionHeader>
<bodyText confidence="0.999924869565217">
The parsing model defined in Section 3.2 re-
quires us to compute unlabelled dependency trees
from CCG derivations (to prefer non-local attach-
ments). It is simple to extract an unlabelled depen-
dency tree from a CCG parse, by defining one ar-
gument of each binary rule instantiation to be the
head. For forward application and (generalized)
forward composition, we define the head to be the
left argument, unless the left argument is an endo-
centric head-passing modifier category X/X. We
do the inverse for the corresponding ‘backward’
combinators. For punctuation rules, the head is the
argument which is not punctuation, and the head
of a Conjunction rule is the right-hand argument.
The standard CCG parsing evaluation uses a
different concept of dependencies, correspond-
ing to the predicate-argument structure defined by
CCGBank. These dependencies capture a deeper
information—for example by assigning both boy
and girl as subjects of talk in a boy and a girl
talked. We extract these dependencies using
the generate program supplied with the C&amp;C
parser.
</bodyText>
<subsectionHeader confidence="0.979442">
3.6 Pruning
</subsectionHeader>
<bodyText confidence="0.999922833333333">
Our parsing model is able to efficiently and op-
timally search for the best parse. However,
we found that over 80% of the run-time of our
pipeline was spent during supertagging. Naively,
the log-linear model needs to output a probability
for each of the 425 categories. This is expensive
both in terms of the number of dot products re-
quired, and the cost of building the initial priority-
queue for the A∗ parsing agenda. It is also largely
unnecessary—for example, periods at the end of
sentences always have the same category, but our
supertagger calculates a distribution over all pos-
sible categories.
Note that the motivation for introducing prun-
ing here is fundamentally different from for the
C&amp;C pipeline. The C&amp;C supertagger prunes the
the categories so that the parser can build the com-
plete set of derivations given those categories. In
contrast, our parser can efficiently search large (or
infinite) spaces of categories, but pruning is help-
ful for making supertagging itself more efficient,
and for building the initial agenda.
We therefore implemented the following strate-
gies to improve efficiency:
</bodyText>
<listItem confidence="0.876938068965518">
• Only allowing at most 50 categories per
word. The C&amp;C parser takes on average 1.27
tags per word (and an average of 3.57 at its
loosest beam setting), so this restriction is a
very mild one. Nevertheless, it considerably
reduces the potential size of the agenda.
• Using a variable-width beam Q which prunes
categories less likely than Q times the prob-
ability of the best category. We set Q =
0.00001, which is two orders-of-magnitude
smaller than the equivalent C&amp;C beam.
Again, this heuristic is useful for reducing the
length of the agenda.
• Using a tag dictionary of possible categories
for each word, so that weights are only cal-
culated for a subset of the categories. Unlike
the other methods, this approach does affect
the probabilities which are calculated, as the
normalizing constant is only computed for a
subset of the categories. However, the proba-
bility mass contained in the pruned categories
is small, and it only slightly decreases pars-
ing accuracy. To build the tag dictionary, we
parsed 42 million sentences of Wikipedia us-
ing our parser, and for all words occurring at
least 500 times, we stored the set of observed
word-category combinations. When parsing
new sentences, these words are only allowed
to occur with one of these categories.
</listItem>
<page confidence="0.97322">
994
</page>
<table confidence="0.9998945">
Supertagger Parser CCGBank Wikipedia Bioinfer
F1 COV F1 Time F1 COV F1 F1 COV F1
(cov) (all) (cov) (all) (cov) (all)
C&amp;C C&amp;C 85.47 99.63 85.30 54s 81.19 99.0 80.64 76.08 97.2 74.88
EASYCCG EASYCCG 83.37 99.96 83.37 13s 81.75 100 81.75 77.24 100 77.24
EASYCCG C&amp;C 86.14 99.96 86.11 69s 82.46 100 82.46 78.00 99.8 77.88
</table>
<tableCaption confidence="0.851382">
Table 2: Parsing F1-scores for labelled dependencies across a range of domains. F1 (cov) refers to
results on sentences which the parser is able to parse, and F1 (all) gives results over all sentences. For
</tableCaption>
<bodyText confidence="0.85913">
the EASYCCG results, scores are only over parses where the C&amp;C dependency extraction script was
successful, which was 99.3% on CCGBank, 99.5% on Wikipedia, and 100% on Bioinfer.
</bodyText>
<sectionHeader confidence="0.999756" genericHeader="method">
4 Experiments
</sectionHeader>
<subsectionHeader confidence="0.994874">
4.1 Experimental Setup
</subsectionHeader>
<bodyText confidence="0.9997503125">
We trained our model on Sections 02-21 of CCG-
Bank (Hockenmaier and Steedman, 2007), using
Section 00 for development. For testing, we used
Section 23 of CCGBank, a Wikipedia corpus an-
notated by Honnibal and Curran (2009), and the
Bioinfer corpus of biomedical abstracts (Pyysalo
et al., 2007). The latter two are out-of-domain, so
are more challenging for the parsers.
We compare the performance of our model
against both the C&amp;C parser, and the system de-
scribed in Lewis and Steedman (2014). This
model uses the same supertagger as used in EASY-
CCG, but uses the C&amp;C parser for parsing, using
adaptive supertagging with the default values.
All timing experiments used the same 1.8Ghz
AMD machine.
</bodyText>
<subsectionHeader confidence="0.999258">
4.2 Parsing Accuracy
</subsectionHeader>
<bodyText confidence="0.998158833333333">
Results are shown in Table 2. Our parser per-
forms competitively with a much more complex
parsing model, and outperforms the C&amp;C pipeline
on both out-of-domain datasets. This result con-
firms our hypothesis that the majority of parsing
decisions can be made accurately with a simple
tagging model and a deterministic parser.
We see that the combination of the EASYCCG
supertagger and the C&amp;C parser achieves the best
accuracy across all domains. This result shows
that, unsurprisingly, there is some value to hav-
ing a statistical model of the dependencies that the
parser is evaluated on. However, the difference is
not large, particularly out-of-domain, considering
that a sophisticated and complex statistical parser
is being compared with a deterministic one. Our
parser is also far faster than this baseline.
It is interesting that the performance gap is
</bodyText>
<table confidence="0.998700727272727">
Speed (sentences/second)
System Tagger Parser Total
C&amp;C 343 52 45
EASYCCG tagger + 299 58 49
C&amp;C parser
EASYCCG baseline 56 222 45
+Tag Dictionary 185 217 99
+Max 50 tags/word 238 345 141
+β=0.00001 299 493 186
EASYCCG — null 300 221 127
heuristic
</table>
<tableCaption confidence="0.8308965">
Table 3: Effect of our optimizations of parsing
speed.
</tableCaption>
<bodyText confidence="0.9990745">
much lower on out-of-domain datasets (2.8 points
in domain, but only 0.65-0.75 out-of-domain),
suggesting that much of the C&amp;C parser’s depen-
dency model is domain specific, and does not gen-
eralize well to other domains.
We also briefly experimented using the C&amp;C
supertagger (with a beam of β = 10−5) with the
EASYCCG parser. Performance was much worse,
with an F-score of 79.63% on the 97.8% of sen-
tences it parsed on CCGBank Section 23. This
shows that our model is reliant on the accuracy of
the supertagger.
</bodyText>
<subsectionHeader confidence="0.99989">
4.3 Parsing Speed
</subsectionHeader>
<bodyText confidence="0.999765375">
CCG parsers have been used in distributional
approaches to semantics (Lewis and Steedman,
2013a; Lewis and Steedman, 2013b), which bene-
fit from large corpora. However, even though the
C&amp;C parser is relatively fast, it will still take over
40 CPU-days to parse the Gigaword corpus on our
hardware, which is slow enough to be an obstacle
to scaling distributional semantics to larger cor-
</bodyText>
<page confidence="0.993891">
995
</page>
<figure confidence="0.992934625">
50
Average Parse Time (ms)
40
30
20
10
0 20 40 60 80 100
Sentence Length
</figure>
<figureCaption confidence="0.997444">
Figure 2: Average parse times in milliseconds, by
sentence length.
</figureCaption>
<bodyText confidence="0.982684848484848">
pora such as ClueWeb. Therefore, it is important
to be able to parse sentences at a high speed.
We measured parsing times on Section 23 of
CCGBank (after developing against Section 00),
using the optimizations described in Section 4.3.
We also experimented with the null heuristic,
which always estimates the outside probability as
being 1.0. Times exclude the time taken to load
models.
Results are shown in Table 3. The best EASY-
CCG model is roughly four times faster than the
C&amp;C parser2. Adding the tag dictionary caused
accuracy to drop slightly from 83.46 to 83.37, and
meant the parser failed to parse a single sentence
in the test set (“Among its provisions :”) but other
changes did not affect accuracy. The pruning in
the supertagger improves parsing speed, by limit-
ing the length of the priority queue it builds for the
agenda. Of course, we could use a backoff model
to ensure full coverage (analogously to adaptive
supertagging), but we leave that to future work.
Using our A* heuristic doubles the speed of pars-
ing (excluding supertagging).
To better understand the properties of our
model, we also investigate how parsing time varies
with sentence length. Unlike the cubic CKY al-
gorithm typically used by chart parsers, our A*
search potentially takes exponential time in the
sentence length. For this experiment, we used the
Sections 02-21 of CCGBank. Sentences were di-
vided into bins of width 10, and we calculated the
average parsing time for sentences in each bin.
Results are shown in Figure 2, and demon-
</bodyText>
<footnote confidence="0.974618">
2It is worth noting that the C&amp;C parser code is written in
highly-optimized C++, compared to our simple Java imple-
mentation. It seems likely that our parser could be made sub-
stantially faster with a similar level of engineering effort.
</footnote>
<bodyText confidence="0.999970888888889">
strate that while parsing is highly efficient for sen-
tences of up to 50 words (over 95% of CCGBank),
it scales super-linearly with long sentences. In
fact, Section 00 contains a sentence of 249 words,
which took 37 seconds to parse (3 times longer
than the other 1912 sentences put together). In
practice, this scaling is unlikely to be problematic,
as long sentences are typically filtered when pro-
cessing large corpora.
</bodyText>
<subsectionHeader confidence="0.99864">
4.4 Semantic Parsing
</subsectionHeader>
<bodyText confidence="0.999987411764706">
A major motivation for CCG parsing is to exploit
its transparent interface to the semantics, allowing
syntactic parsers to do much of the work of seman-
tic parsers. Therefore, perhaps the most relevant
measure of the performance of a CCG parser is its
effect on the accuracy of downstream applications.
We experimented with a supervised version
of Reddy et al. (2014)’s model for question-
answering on Freebase (i.e. without using Reddy
et al.’s lexicon derived from unlabelled text), us-
ing the WEBQUESTIONS dataset (Berant et al.,
2013)3. The model learns to map CCG parses to
database queries. We compare the performance of
the QA system using both our parser and C&amp;C,
taking the 10-best parses from each parser for
each sentence. Syntactic question parsing models
were trained from the combination of 10 copies
of Rimell and Clark (2008)’s question dataset and
one copy of the CCGBank
The accuracy of Reddy et al. (2014)’s model
varies significantly between iterations of the train-
ing data. Rather than tune the number of iterations,
we instead measure the accuracy after each iter-
ation. We experimented with the models’ 1-best
answers, and the oracle accuracy of their 100 best
answers. The oracle accuracy gives a better indi-
cation of the performance of the parser, by miti-
gating errors caused by the semantic component.
Results are shown in Figure 3, and demonstrate
that using EASYCCG can lead to better down-
stream performance than the C&amp;C parser. The im-
provement is particularly large on oracle accuracy,
increasing the upper bound on the performance of
the semantic parser by around 4 points.
</bodyText>
<sectionHeader confidence="0.999208" genericHeader="method">
5 Related Work
</sectionHeader>
<bodyText confidence="0.5435895">
CCG parsing has been the subject of much re-
search. We have already described the C&amp;C pars-
</bodyText>
<footnote confidence="0.958069">
3Using the Business, Film and People domains, with 1115
questions for training and 570 for testing.
</footnote>
<page confidence="0.995686">
996
</page>
<figure confidence="0.845509136363636">
F1-score
100-best Oracle F1-score
Iteration
Iteration
2 4 6 8 10
50
48
46
44
42
40
C&amp;C
EASYCCG
2 4 6 8 10
64
62
60
58
56
54
C&amp;C
EASYCCG
</figure>
<figureCaption confidence="0.999973">
Figure 3: Question Answering accuracy per iteration of Reddy et al. (2014)’s supervised model.
</figureCaption>
<bodyText confidence="0.999959016129033">
ing model. Kummerfeld et al. (2010) showed that
the speed of the C&amp;C parser can be improved
with domain-specific self-training—similar im-
provements may be possible applying this tech-
nique to our model. Auli and Lopez (2011a)
have achieved the best CCG parsing accuracy, by
allowing the parser and supertagger to perform
joint inference (though there is a significant speed
penalty). Auli and Lopez (2011b) were the first to
use A* parsing for CCG, but their system is both
much slower and less accurate than ours (due to a
different model and a different A* heuristic). Kr-
ishnamurthy and Mitchell (2014) show how CCG
parsing can be improved by jointly modelling the
syntax and semantics. Fowler and Penn (2010)
apply the Petrov parser to CCG, making a small
improvement in accuracy over the C&amp;C parser,
at the cost of a 300-fold speed decrease. Zhang
and Clark (2011) and Xu et al. (2014) explored
shift-reduce CCG parsing, but despite the use of a
linear-time algorithm, parsing speed in practice is
significantly slower than the C&amp;C parser.
Parsers based on supertagging models have pre-
viously been applied to other strongly lexical-
ized formalisms, such as to LTAG (Bangalore and
Joshi, 1999) and to HPSG (Ninomiya et al., 2006).
A major contribution of our work over these is
showing that factoring models on lexical cate-
gories allows fast and exact A* parsing, without
the need for beam search. Our parsing approach
could be applied to any strongly lexicalized for-
malism.
Our work fits into a tradition of attempting to
simplify complex models without sacrificing per-
formance. Klein and Manning (2003b) showed
that unlexicalized parsers were only slightly less
accurate than their lexicalized counterparts. Col-
lobert et al. (2011) showed how a range of NLP
tagging tasks could be performed at high accu-
racy using a small feature set based on vector-
space word embeddings. However, the extension
of this work to phrase-structure parsing (Collobert,
2011) required a more complex model, and did not
match the performance of traditional parsing tech-
niques. We achieve state-of-the-art results using
the same feature set and a simpler model by ex-
ploiting CCG’s lexicalized nature, which makes it
more natural to delegate parsing decisions to a tag-
ging model.
Other parsing research has focused on build-
ing fast parsers for web-scale processing, typically
using dependency grammars (e.g. Nivre (2003)).
CCG has some advantages over dependency gram-
mars, such as supporting surface-compositional
semantics. The fastest dependency parsers use
an easyfirst strategy, in which edges are added
greedily in order of their score, with 0(nlog(n))
complexity (Goldberg and Elhadad, 2010; Tratz
and Hovy, 2011). This strategy is reminiscent of
our A* search, which expands the chart in a best-
first order. A* has higher asymptotic complexity,
but finds a globally optimal solution.
</bodyText>
<sectionHeader confidence="0.998547" genericHeader="method">
6 Future Work
</sectionHeader>
<bodyText confidence="0.9998711">
We believe that our model opens several interest-
ing directions for future research.
One interesting angle would be to increase the
amount of information in CCGBank’s lexical en-
tries, to further reduce the search space for the
parser. For example, PP categories could be dis-
tinguished with the relevant preposition as a fea-
ture; punctuation and coordination could be given
more detailed categories to avoid needing their
own combinators, and slashes could be extended
</bodyText>
<page confidence="0.99097">
997
</page>
<bodyText confidence="0.999955111111111">
with Baldridge and Kruijff (2003)’s multi-modal
extensions to limit over-generation. Honnibal and
Curran (2009) show how unary rules can be lexi-
calized in CCG. Such improvements may improve
both the speed and accuracy of our model.
Because our parser is factored on a unigram tag-
ging model, it can be trained from isolated anno-
tated words, and does not require annotated parse
trees or full sentences. Reducing the requirements
for training data eases the task for human annota-
tors. It may also make the model more amenable
to semi-supervised approaches to CCG parsing,
which have typically focused on extending the lex-
icon (Thomforde and Steedman, 2011; Deoskar et
al., 2014). Finally, it may make it easier to convert
other annotated resources, such as UCCA (Abend
and Rappoport, 2013) or AMR (Banarescu et al.,
2013), to CCG training data—as only specific
words need to be converted, rather than full sen-
tences.
Our model is weak at certain kinds of deci-
sions, e.g. coordination-scope ambiguities or non-
local attachments. Incorporating specific models
for such decisions may improve accuracy, while
still allowing fast and exact search—for example,
we intend to try including Coppola et al. (2011)’s
model for prepositional phrase attachment.
</bodyText>
<sectionHeader confidence="0.999343" genericHeader="conclusions">
7 Conclusions
</sectionHeader>
<bodyText confidence="0.999652785714286">
We have shown that a simple, principled, deter-
ministic parser combined with a tagging model
can parse an expressive linguistic formalism with
high speed and accuracy. Although accuracy
is not state-of-the-art on CCGBank, our model
gives excellent performance on two out-of-domain
datasets, and improves the accuracy of a question-
answering system. We have shown that this model
allows an efficient heuristic for A∗ parsing, which
makes parsing extremely fast, and may enable
logic-based distributional semantics to scale to
larger corpora. Our methods are directly applica-
ble to other lexicalized formalisms, such as LTAG,
LFG and HPSG.
</bodyText>
<sectionHeader confidence="0.998264" genericHeader="acknowledgments">
Acknowledgments
</sectionHeader>
<bodyText confidence="0.9997332">
We would like to thank Tejaswini Deoskar, Bharat
Ram Ambati, Michael Roth and the anonymous
reviewers for helpful feedback on an earlier ver-
sion of this paper, and Siva Reddy for running the
semantic parsing experiments.
</bodyText>
<sectionHeader confidence="0.989364" genericHeader="references">
References
</sectionHeader>
<reference confidence="0.999570909090909">
Omri Abend and Ari Rappoport. 2013. Universal con-
ceptual cognitive annotation (ucca). In Proceedings
ofACL.
Michael Auli and Adam Lopez. 2011a. A compari-
son of loopy belief propagation and dual decompo-
sition for integrated CCG supertagging and parsing.
In Proceedings of the 49th Annual Meeting of the
Association for Computational Linguistics: Human
Language Technologies-Volume 1, pages 470–480.
Association for Computational Linguistics.
Michael Auli and Adam Lopez. 2011b. Efficient CCG
parsing: A* versus adaptive supertagging. In Pro-
ceedings of the 49th Annual Meeting of the Associ-
ation for Computational Linguistics: Human Lan-
guage Technologies-Volume 1, pages 1577–1585.
Association for Computational Linguistics.
Jason Baldridge and Geert-Jan M Kruijff. 2003. Multi-
modal combinatory categorial grammar. In Pro-
ceedings of the tenth conference on European chap-
ter of the Association for Computational Linguistics-
Volume 1, pages 211–218. Association for Compu-
tational Linguistics.
Laura Banarescu, Claire Bonial, Shu Cai, Madalina
Georgescu, Kira Griffitt, Ulf Hermjakob, Kevin
Knight, Philipp Koehn, Martha Palmer, and Nathan
Schneider. 2013. Abstract Meaning Representation
for sembanking. In Proceedings of the 7th Linguis-
tic Annotation Workshop and Interoperability with
Discourse, Sofia, Bulgaria, August. Association for
Computational Linguistics.
Srinivas Bangalore and Aravind K Joshi. 1999. Su-
pertagging: An approach to almost parsing. Com-
putational linguistics, 25(2):237–265.
Jonathan Berant, Andrew Chou, Roy Frostig, and Percy
Liang. 2013. Semantic parsing on freebase from
question-answer pairs. In Proceedings of EMNLP.
Johan Bos. 2008. Wide-coverage semantic analy-
sis with boxer. In Johan Bos and Rodolfo Del-
monte, editors, Semantics in Text Processing. STEP
2008 Conference Proceedings, Research in Compu-
tational Semantics, pages 277–286. College Publi-
cations.
Stephen Clark and James R Curran. 2007. Wide-
coverage efficient statistical parsing with CCG
and log-linear models. Computational Linguistics,
33(4):493–552.
Ronan Collobert, Jason Weston, Léon Bottou, Michael
Karlen, Koray Kavukcuoglu, and Pavel Kuksa.
2011. Natural language processing (almost) from
scratch. The Journal of Machine Learning Re-
search, 12:2493–2537.
Ronan Collobert. 2011. Deep learning for efficient dis-
criminative parsing. In International Conference on
Artificial Intelligence and Statistics, number EPFL-
CONF-192374.
</reference>
<page confidence="0.990708">
998
</page>
<reference confidence="0.999541598214286">
Gregory F Coppola, Alexandra Birch, Tejaswini De-
oskar, and Mark Steedman. 2011. Simple semi-
supervised learning for prepositional phrase attach-
ment. In Proceedings of the 12th International Con-
ference on Parsing Technologies, pages 129–139.
Association for Computational Linguistics.
Tejaswini Deoskar, Christos Christodoulopoulos,
Alexandra Birch, and Mark Steedman. 2014.
Generalizing a Strongly Lexicalized Parser using
Unlabeled Data. In Proceedings of the 14th Con-
ference of the European Chapter of the Association
for Computational Linguistics. Association for
Computational Linguistics.
Jason Eisner. 1996. Efficient normal-form parsing for
combinatory categorial grammar. In Proceedings of
the 34th annual meeting on Association for Com-
putational Linguistics, pages 79–86. Association for
Computational Linguistics.
Timothy AD Fowler and Gerald Penn. 2010. Accu-
rate context-free parsing with combinatory catego-
rial grammar. In Proceedings of the 48th Annual
Meeting of the Association for Computational Lin-
guistics, pages 335–344. Association for Computa-
tional Linguistics.
Yoav Goldberg and Michael Elhadad. 2010. An effi-
cient algorithm for easy-first non-directional depen-
dency parsing. In Human Language Technologies:
The 2010 Annual Conference of the North American
Chapter of the Association for Computational Lin-
guistics, pages 742–750. Association for Computa-
tional Linguistics.
Brian Harrington. 2010. A semantic network ap-
proach to measuring relatedness. In Proceedings
of the 23rd International Conference on Compu-
tational Linguistics: Posters, COLING ’10, pages
356–364. Association for Computational Linguis-
tics.
Julia Hockenmaier and Mark Steedman. 2007. CCG-
bank: a corpus of CCG derivations and dependency
structures extracted from the Penn Treebank. Com-
putational Linguistics, 33(3):355–396.
Julia Hockenmaier. 2003. Data and models for statis-
tical parsing with combinatory categorial grammar.
Matthew Honnibal and James R Curran. 2009. Fully
lexicalising CCGbank with hat categories. In Pro-
ceedings of the 2009 Conference on Empirical Meth-
ods in Natural Language Processing: Volume 3-
Volume 3, pages 1212–1221. Association for Com-
putational Linguistics.
Dan Klein and Christopher D Manning. 2003a. A*
parsing: fast exact viterbi parse selection. In
Proceedings of the 2003 Conference of the North
American Chapter of the Association for Computa-
tional Linguistics on Human Language Technology-
Volume 1, pages 40–47. Association for Computa-
tional Linguistics.
Dan Klein and Christopher D Manning. 2003b. Ac-
curate unlexicalized parsing. In Proceedings of the
41st Annual Meeting on Association for Computa-
tional Linguistics-Volume 1, pages 423–430. Asso-
ciation for Computational Linguistics.
Jayant Krishnamurthy and Tom M Mitchell. 2014.
Joint syntactic and semantic parsing with combina-
tory categorial grammar. June.
Jonathan K. Kummerfeld, Jessika Roesner, Tim Daw-
born, James Haggerty, James R. Curran, and
Stephen Clark. 2010. Faster parsing by supertag-
ger adaptation. In Proceedings of the 48th Annual
Meeting of the Association for Computational Lin-
guistics, ACL ’10, pages 345–355. Association for
Computational Linguistics.
Mike Lewis and Mark Steedman. 2013a. Combined
Distributional and Logical Semantics. Transactions
of the Association for Computational Linguistics,
1:179–192.
Mike Lewis and Mark Steedman. 2013b. Unsuper-
vised induction of cross-lingual semantic relations.
In Proceedings of the 2013 Conference on Empiri-
cal Methods in Natural Language Processing, pages
681–692, Seattle, Washington, USA, October. Asso-
ciation for Computational Linguistics.
Mike Lewis and Mark Steedman. 2014. Improved
CCG parsing with Semi-supervised Supertagging.
Transactions of the Association for Computational
Linguistics (to appear).
Takashi Ninomiya, Takuya Matsuzaki, Yoshimasa Tsu-
ruoka, Yusuke Miyao, and Jun’ichi Tsujii. 2006.
Extremely lexicalized models for accurate and fast
hpsg parsing. In Proceedings of the 2006 Confer-
ence on Empirical Methods in Natural Language
Processing, EMNLP ’06, pages 155–163, Strouds-
burg, PA, USA. Association for Computational Lin-
guistics.
Joakim Nivre. 2003. An efficient algorithm for pro-
jective dependency parsing. In Proceedings of the
8th International Workshop on Parsing Technologies
(IWPT.
Sampo Pyysalo, Filip Ginter, Juho Heimonen, Jari
Björne, Jorma Boberg, Jouni Järvinen, and Tapio
Salakoski. 2007. Bioinfer: a corpus for information
extraction in the biomedical domain. BMC bioinfor-
matics, 8(1):50.
Siva Reddy, Mirella Lapata, and Mark Steedman.
2014. Large-scale Semantic Parsing without
Question-Answer Pairs. Transactions of the Asso-
ciation for Computational Linguistics (to appear).
Laura Rimell and Stephen Clark. 2008. Adapt-
ing a lexicalized-grammar parser to contrasting do-
mains. In Proceedings of the Conference on Em-
pirical Methods in Natural Language Processing,
pages 475–484. Association for Computational Lin-
guistics.
</reference>
<page confidence="0.985783">
999
</page>
<reference confidence="0.9995726">
Mark Steedman. 2000. The Syntactic Process. MIT
Press.
Mark Steedman. 2012. Taking Scope: The Natural
Semantics of Quantifiers. MIT Press.
Emily Thomforde and Mark Steedman. 2011. Semi-
supervised CCG lexicon extension. In Proceedings
of the Conference on Empirical Methods in Natu-
ral Language Processing, pages 1246–1256. Asso-
ciation for Computational Linguistics.
Stephen Tratz and Eduard Hovy. 2011. A fast, effec-
tive, non-projective, semantically-enriched parser.
In Proceedings of EMNLP.
Joseph Turian, Lev Ratinov, and Yoshua Bengio. 2010.
Word representations: a simple and general method
for semi-supervised learning. In Proceedings of the
48th Annual Meeting of the Association for Compu-
tational Linguistics, pages 384–394. Association for
Computational Linguistics.
Wenduan Xu, Stephen Clark, and Yue Zhang. 2014.
Shift-reduce ccg parsing with a dependency model.
In Proceedings of the 52nd Annual Meeting of
the Association for Computational Linguistics (ACL
2014). Association for Computational Linguistics,
June.
Yue Zhang and Stephen Clark. 2011. Shift-reduce
CCG parsing. In Proceedings of the 49th Annual
Meeting of the Association for Computational Lin-
guistics: Human Language Technologies-Volume 1,
pages 683–692. Association for Computational Lin-
guistics.
</reference>
<page confidence="0.974204">
1000
</page>
</variant>
</algorithm>
<algorithm name="ParsHed" version="110505">
<variant no="0" confidence="0.367432">
<title confidence="0.997842">Parsing with a Supertag-factored Model</title>
<author confidence="0.995027">Mike</author>
<affiliation confidence="0.843858666666667">School of University of Edinburgh, E118 9AB,</affiliation>
<email confidence="0.994745">mike.lewis@ed.ac.uk</email>
<author confidence="0.99538">Mark</author>
<affiliation confidence="0.9992835">School of University of</affiliation>
<address confidence="0.716346">Edinburgh, E118 9AB,</address>
<email confidence="0.997869">steedman@inf.ed.ac.uk</email>
<abstract confidence="0.999093947368421">We introduce a new CCG parsing model which is factored on lexical category assignments. Parsing is then simply a deterministic search for the most probable category sequence that supports a CCG derivation. The parser is extremely simple, with a tiny feature set, no POS tagger, and no statistical model of the derivation or dependencies. Formulating the model in this way allows a highly effective heurisfor parsing, which makes parsing extremely fast. Compared to the standard C&amp;C CCG parser, our model is more accurate out-of-domain, is four times faster, has higher coverage, and is greatly simplified. We also show that using our parser improves the performance of a state-ofthe-art question answering system.</abstract>
</variant>
</algorithm>
<algorithm name="ParsCit" version="110505">
<citationList>
<citation valid="true">
<authors>
<author>Omri Abend</author>
<author>Ari Rappoport</author>
</authors>
<title>Universal conceptual cognitive annotation (ucca).</title>
<date>2013</date>
<booktitle>In Proceedings ofACL.</booktitle>
<contexts>
<context position="32742" citStr="Abend and Rappoport, 2013" startWordPosition="5402" endWordPosition="5405">uch improvements may improve both the speed and accuracy of our model. Because our parser is factored on a unigram tagging model, it can be trained from isolated annotated words, and does not require annotated parse trees or full sentences. Reducing the requirements for training data eases the task for human annotators. It may also make the model more amenable to semi-supervised approaches to CCG parsing, which have typically focused on extending the lexicon (Thomforde and Steedman, 2011; Deoskar et al., 2014). Finally, it may make it easier to convert other annotated resources, such as UCCA (Abend and Rappoport, 2013) or AMR (Banarescu et al., 2013), to CCG training data—as only specific words need to be converted, rather than full sentences. Our model is weak at certain kinds of decisions, e.g. coordination-scope ambiguities or nonlocal attachments. Incorporating specific models for such decisions may improve accuracy, while still allowing fast and exact search—for example, we intend to try including Coppola et al. (2011)’s model for prepositional phrase attachment. 7 Conclusions We have shown that a simple, principled, deterministic parser combined with a tagging model can parse an expressive linguistic </context>
</contexts>
<marker>Abend, Rappoport, 2013</marker>
<rawString>Omri Abend and Ari Rappoport. 2013. Universal conceptual cognitive annotation (ucca). In Proceedings ofACL.</rawString>
</citation>
<citation valid="true">
<authors>
<author>Michael Auli</author>
<author>Adam Lopez</author>
</authors>
<title>A comparison of loopy belief propagation and dual decomposition for integrated CCG supertagging and parsing.</title>
<date>2011</date>
<booktitle>In Proceedings of the 49th Annual Meeting of the Association for Computational Linguistics: Human Language Technologies-Volume 1,</booktitle>
<pages>470--480</pages>
<publisher>Association</publisher>
<institution>for Computational Linguistics.</institution>
<contexts>
<context position="7620" citStr="Auli and Lopez (2011" startWordPosition="1218" endWordPosition="1221">ed in best-first order, until a complete parse for the sentence is found. Klein and Manning calculate an upper bound on the outside probability of a span based on a summary of the context. For example, the summary for the SX heuristic is the category of the span, and the number of words in the sentence before and after the span. The value of the heuristic is the probability of the best possible sentence meeting these restrictions. These probabilities are pre-computed for every non-terminal symbol and for every possible number of preceding and succeeding words, leading to large look-up tables. Auli and Lopez (2011b) find that A* CCG parsing with this heuristic is very slow. However, they achieve a modest 15% speed improvement over CKY when A* is combined with adaptive supertagging. One reason is that the heuristic estimate is rather coarse, as it deals with the best possible outside context, rather than the actual sentence. We introduce a new heuristic which gives a tighter upper bound on the outside probability. 3 Model 3.1 Lexical Category Model As input, our parser takes a distribution over all CCG lexical categories for each word in the sentence. These distributions are assigned using Lewis and Ste</context>
<context position="28824" citStr="Auli and Lopez (2011" startWordPosition="4771" endWordPosition="4774">ubject of much research. We have already described the C&amp;C pars3Using the Business, Film and People domains, with 1115 questions for training and 570 for testing. 996 F1-score 100-best Oracle F1-score Iteration Iteration 2 4 6 8 10 50 48 46 44 42 40 C&amp;C EASYCCG 2 4 6 8 10 64 62 60 58 56 54 C&amp;C EASYCCG Figure 3: Question Answering accuracy per iteration of Reddy et al. (2014)’s supervised model. ing model. Kummerfeld et al. (2010) showed that the speed of the C&amp;C parser can be improved with domain-specific self-training—similar improvements may be possible applying this technique to our model. Auli and Lopez (2011a) have achieved the best CCG parsing accuracy, by allowing the parser and supertagger to perform joint inference (though there is a significant speed penalty). Auli and Lopez (2011b) were the first to use A* parsing for CCG, but their system is both much slower and less accurate than ours (due to a different model and a different A* heuristic). Krishnamurthy and Mitchell (2014) show how CCG parsing can be improved by jointly modelling the syntax and semantics. Fowler and Penn (2010) apply the Petrov parser to CCG, making a small improvement in accuracy over the C&amp;C parser, at the cost of a 30</context>
</contexts>
<marker>Auli, Lopez, 2011</marker>
<rawString>Michael Auli and Adam Lopez. 2011a. A comparison of loopy belief propagation and dual decomposition for integrated CCG supertagging and parsing. In Proceedings of the 49th Annual Meeting of the Association for Computational Linguistics: Human Language Technologies-Volume 1, pages 470–480. Association for Computational Linguistics.</rawString>
</citation>
<citation valid="true">
<authors>
<author>Michael Auli</author>
<author>Adam Lopez</author>
</authors>
<title>Efficient CCG parsing: A* versus adaptive supertagging.</title>
<date>2011</date>
<booktitle>In Proceedings of the 49th Annual Meeting of the Association for Computational Linguistics: Human Language Technologies-Volume 1,</booktitle>
<pages>1577--1585</pages>
<publisher>Association</publisher>
<institution>for Computational Linguistics.</institution>
<contexts>
<context position="7620" citStr="Auli and Lopez (2011" startWordPosition="1218" endWordPosition="1221">ed in best-first order, until a complete parse for the sentence is found. Klein and Manning calculate an upper bound on the outside probability of a span based on a summary of the context. For example, the summary for the SX heuristic is the category of the span, and the number of words in the sentence before and after the span. The value of the heuristic is the probability of the best possible sentence meeting these restrictions. These probabilities are pre-computed for every non-terminal symbol and for every possible number of preceding and succeeding words, leading to large look-up tables. Auli and Lopez (2011b) find that A* CCG parsing with this heuristic is very slow. However, they achieve a modest 15% speed improvement over CKY when A* is combined with adaptive supertagging. One reason is that the heuristic estimate is rather coarse, as it deals with the best possible outside context, rather than the actual sentence. We introduce a new heuristic which gives a tighter upper bound on the outside probability. 3 Model 3.1 Lexical Category Model As input, our parser takes a distribution over all CCG lexical categories for each word in the sentence. These distributions are assigned using Lewis and Ste</context>
<context position="28824" citStr="Auli and Lopez (2011" startWordPosition="4771" endWordPosition="4774">ubject of much research. We have already described the C&amp;C pars3Using the Business, Film and People domains, with 1115 questions for training and 570 for testing. 996 F1-score 100-best Oracle F1-score Iteration Iteration 2 4 6 8 10 50 48 46 44 42 40 C&amp;C EASYCCG 2 4 6 8 10 64 62 60 58 56 54 C&amp;C EASYCCG Figure 3: Question Answering accuracy per iteration of Reddy et al. (2014)’s supervised model. ing model. Kummerfeld et al. (2010) showed that the speed of the C&amp;C parser can be improved with domain-specific self-training—similar improvements may be possible applying this technique to our model. Auli and Lopez (2011a) have achieved the best CCG parsing accuracy, by allowing the parser and supertagger to perform joint inference (though there is a significant speed penalty). Auli and Lopez (2011b) were the first to use A* parsing for CCG, but their system is both much slower and less accurate than ours (due to a different model and a different A* heuristic). Krishnamurthy and Mitchell (2014) show how CCG parsing can be improved by jointly modelling the syntax and semantics. Fowler and Penn (2010) apply the Petrov parser to CCG, making a small improvement in accuracy over the C&amp;C parser, at the cost of a 30</context>
</contexts>
<marker>Auli, Lopez, 2011</marker>
<rawString>Michael Auli and Adam Lopez. 2011b. Efficient CCG parsing: A* versus adaptive supertagging. In Proceedings of the 49th Annual Meeting of the Association for Computational Linguistics: Human Language Technologies-Volume 1, pages 1577–1585. Association for Computational Linguistics.</rawString>
</citation>
<citation valid="true">
<authors>
<author>Jason Baldridge</author>
<author>Geert-Jan M Kruijff</author>
</authors>
<title>Multimodal combinatory categorial grammar.</title>
<date>2003</date>
<booktitle>In Proceedings of the tenth conference on European chapter of the Association for Computational LinguisticsVolume 1,</booktitle>
<pages>211--218</pages>
<publisher>Association</publisher>
<institution>for Computational Linguistics.</institution>
<contexts>
<context position="31988" citStr="Baldridge and Kruijff (2003)" startWordPosition="5279" endWordPosition="5282">s the chart in a bestfirst order. A* has higher asymptotic complexity, but finds a globally optimal solution. 6 Future Work We believe that our model opens several interesting directions for future research. One interesting angle would be to increase the amount of information in CCGBank’s lexical entries, to further reduce the search space for the parser. For example, PP categories could be distinguished with the relevant preposition as a feature; punctuation and coordination could be given more detailed categories to avoid needing their own combinators, and slashes could be extended 997 with Baldridge and Kruijff (2003)’s multi-modal extensions to limit over-generation. Honnibal and Curran (2009) show how unary rules can be lexicalized in CCG. Such improvements may improve both the speed and accuracy of our model. Because our parser is factored on a unigram tagging model, it can be trained from isolated annotated words, and does not require annotated parse trees or full sentences. Reducing the requirements for training data eases the task for human annotators. It may also make the model more amenable to semi-supervised approaches to CCG parsing, which have typically focused on extending the lexicon (Thomford</context>
</contexts>
<marker>Baldridge, Kruijff, 2003</marker>
<rawString>Jason Baldridge and Geert-Jan M Kruijff. 2003. Multimodal combinatory categorial grammar. In Proceedings of the tenth conference on European chapter of the Association for Computational LinguisticsVolume 1, pages 211–218. Association for Computational Linguistics.</rawString>
</citation>
<citation valid="true">
<authors>
<author>Laura Banarescu</author>
<author>Claire Bonial</author>
<author>Shu Cai</author>
<author>Madalina Georgescu</author>
<author>Kira Griffitt</author>
<author>Ulf Hermjakob</author>
<author>Kevin Knight</author>
<author>Philipp Koehn</author>
<author>Martha Palmer</author>
<author>Nathan Schneider</author>
</authors>
<title>Abstract Meaning Representation for sembanking.</title>
<date>2013</date>
<booktitle>In Proceedings of the 7th Linguistic Annotation Workshop and Interoperability with Discourse,</booktitle>
<publisher>Association</publisher>
<institution>for Computational Linguistics.</institution>
<location>Sofia, Bulgaria,</location>
<contexts>
<context position="32774" citStr="Banarescu et al., 2013" startWordPosition="5408" endWordPosition="5411">he speed and accuracy of our model. Because our parser is factored on a unigram tagging model, it can be trained from isolated annotated words, and does not require annotated parse trees or full sentences. Reducing the requirements for training data eases the task for human annotators. It may also make the model more amenable to semi-supervised approaches to CCG parsing, which have typically focused on extending the lexicon (Thomforde and Steedman, 2011; Deoskar et al., 2014). Finally, it may make it easier to convert other annotated resources, such as UCCA (Abend and Rappoport, 2013) or AMR (Banarescu et al., 2013), to CCG training data—as only specific words need to be converted, rather than full sentences. Our model is weak at certain kinds of decisions, e.g. coordination-scope ambiguities or nonlocal attachments. Incorporating specific models for such decisions may improve accuracy, while still allowing fast and exact search—for example, we intend to try including Coppola et al. (2011)’s model for prepositional phrase attachment. 7 Conclusions We have shown that a simple, principled, deterministic parser combined with a tagging model can parse an expressive linguistic formalism with high speed and ac</context>
</contexts>
<marker>Banarescu, Bonial, Cai, Georgescu, Griffitt, Hermjakob, Knight, Koehn, Palmer, Schneider, 2013</marker>
<rawString>Laura Banarescu, Claire Bonial, Shu Cai, Madalina Georgescu, Kira Griffitt, Ulf Hermjakob, Kevin Knight, Philipp Koehn, Martha Palmer, and Nathan Schneider. 2013. Abstract Meaning Representation for sembanking. In Proceedings of the 7th Linguistic Annotation Workshop and Interoperability with Discourse, Sofia, Bulgaria, August. Association for Computational Linguistics.</rawString>
</citation>
<citation valid="true">
<authors>
<author>Srinivas Bangalore</author>
<author>Aravind K Joshi</author>
</authors>
<title>Supertagging: An approach to almost parsing.</title>
<date>1999</date>
<journal>Computational linguistics,</journal>
<pages>25--2</pages>
<contexts>
<context position="29796" citStr="Bangalore and Joshi, 1999" startWordPosition="4933" endWordPosition="4936">. Krishnamurthy and Mitchell (2014) show how CCG parsing can be improved by jointly modelling the syntax and semantics. Fowler and Penn (2010) apply the Petrov parser to CCG, making a small improvement in accuracy over the C&amp;C parser, at the cost of a 300-fold speed decrease. Zhang and Clark (2011) and Xu et al. (2014) explored shift-reduce CCG parsing, but despite the use of a linear-time algorithm, parsing speed in practice is significantly slower than the C&amp;C parser. Parsers based on supertagging models have previously been applied to other strongly lexicalized formalisms, such as to LTAG (Bangalore and Joshi, 1999) and to HPSG (Ninomiya et al., 2006). A major contribution of our work over these is showing that factoring models on lexical categories allows fast and exact A* parsing, without the need for beam search. Our parsing approach could be applied to any strongly lexicalized formalism. Our work fits into a tradition of attempting to simplify complex models without sacrificing performance. Klein and Manning (2003b) showed that unlexicalized parsers were only slightly less accurate than their lexicalized counterparts. Collobert et al. (2011) showed how a range of NLP tagging tasks could be performed </context>
</contexts>
<marker>Bangalore, Joshi, 1999</marker>
<rawString>Srinivas Bangalore and Aravind K Joshi. 1999. Supertagging: An approach to almost parsing. Computational linguistics, 25(2):237–265.</rawString>
</citation>
<citation valid="true">
<authors>
<author>Jonathan Berant</author>
<author>Andrew Chou</author>
<author>Roy Frostig</author>
<author>Percy Liang</author>
</authors>
<title>Semantic parsing on freebase from question-answer pairs.</title>
<date>2013</date>
<booktitle>In Proceedings of EMNLP.</booktitle>
<contexts>
<context position="27098" citStr="Berant et al., 2013" startWordPosition="4475" endWordPosition="4478">sentences are typically filtered when processing large corpora. 4.4 Semantic Parsing A major motivation for CCG parsing is to exploit its transparent interface to the semantics, allowing syntactic parsers to do much of the work of semantic parsers. Therefore, perhaps the most relevant measure of the performance of a CCG parser is its effect on the accuracy of downstream applications. We experimented with a supervised version of Reddy et al. (2014)’s model for questionanswering on Freebase (i.e. without using Reddy et al.’s lexicon derived from unlabelled text), using the WEBQUESTIONS dataset (Berant et al., 2013)3. The model learns to map CCG parses to database queries. We compare the performance of the QA system using both our parser and C&amp;C, taking the 10-best parses from each parser for each sentence. Syntactic question parsing models were trained from the combination of 10 copies of Rimell and Clark (2008)’s question dataset and one copy of the CCGBank The accuracy of Reddy et al. (2014)’s model varies significantly between iterations of the training data. Rather than tune the number of iterations, we instead measure the accuracy after each iteration. We experimented with the models’ 1-best answer</context>
</contexts>
<marker>Berant, Chou, Frostig, Liang, 2013</marker>
<rawString>Jonathan Berant, Andrew Chou, Roy Frostig, and Percy Liang. 2013. Semantic parsing on freebase from question-answer pairs. In Proceedings of EMNLP.</rawString>
</citation>
<citation valid="true">
<authors>
<author>Johan Bos</author>
</authors>
<title>Wide-coverage semantic analysis with boxer.</title>
<date>2008</date>
<booktitle>Semantics in Text Processing. STEP 2008 Conference Proceedings, Research in Computational Semantics,</booktitle>
<pages>277--286</pages>
<editor>In Johan Bos and Rodolfo Delmonte, editors,</editor>
<publisher>College Publications.</publisher>
<contexts>
<context position="5949" citStr="Bos, 2008" startWordPosition="932" endWordPosition="933">deriving the same dependency structures—in which case the actual choice of derivation is unimportant. Such ambiguity is often called spurious. 2.2 Existing CCG Parsing Models The seminal C&amp;C parser is by far the most popular choice of CCG parser (Clark and Curran, 2007). It showed that it was possible to parse to an expressive linguistic formalism with high speed and accuracy. The performance of the parser has enabled large-scale logic-based distributional research (Harrington, 2010; Lewis and Steedman, 2013a; Lewis and Steedman, 2013b; Reddy et al., 2014), and it is a key component of Boxer (Bos, 2008). The C&amp;C parser uses CKY chart parsing, with a log-linear model to rank parses. The vast number of possible parses means that computing the complete chart is impractical. To resolve this problem, a supertagger is first run over the sentence to prune the set of lexical categories considered by the parser for each word. The initial beam outputs an average of just 1.2 categories per word, rather than the 425 possible categories—making the standard CKY parsing algorithm very efficient. If the parser fails to find any analysis of the complete sentence with this set of supertags, the supertagger re</context>
<context position="17107" citStr="Bos (2008)" startWordPosition="2819" endWordPosition="2820"> We add a hard constraint that the root category of the sentence must be a declarative sentence, a question, or a noun-phrase. This grammar is smaller and cleaner than that used by the C&amp;C parser, which uses 32 unary rules (some of which are semantically dubious, such as S[dcl] → NP\NP), and non-standard binary combinators such as merging two NPs into an NP. The C&amp;C parser also has a large number of special case rules for handling punctuation. Our smaller grammar reduces the grammar constant, eases implementation, and simplifies the job of building downstream semantic parsers such as those of Bos (2008) or Lewis and Steedman (2013a) (which must implement semantic analogs of each syntactic rule). 3.5 Extracting Dependency Structures The parsing model defined in Section 3.2 requires us to compute unlabelled dependency trees from CCG derivations (to prefer non-local attachments). It is simple to extract an unlabelled dependency tree from a CCG parse, by defining one argument of each binary rule instantiation to be the head. For forward application and (generalized) forward composition, we define the head to be the left argument, unless the left argument is an endocentric head-passing modifier c</context>
</contexts>
<marker>Bos, 2008</marker>
<rawString>Johan Bos. 2008. Wide-coverage semantic analysis with boxer. In Johan Bos and Rodolfo Delmonte, editors, Semantics in Text Processing. STEP 2008 Conference Proceedings, Research in Computational Semantics, pages 277–286. College Publications.</rawString>
</citation>
<citation valid="true">
<authors>
<author>Stephen Clark</author>
<author>James R Curran</author>
</authors>
<title>Widecoverage efficient statistical parsing with CCG and log-linear models.</title>
<date>2007</date>
<journal>Computational Linguistics,</journal>
<volume>33</volume>
<issue>4</issue>
<contexts>
<context position="1639" citStr="Clark and Curran, 2007" startWordPosition="248" endWordPosition="251">lexicalized grammatical formalism, in which the vast majority of the decisions made during interpretation involve choosing the correct definitions of words. We explore the effect of modelling this explicitly in a parser, by only using a probabilistic model of lexical categories (based on a local context window), rather than modelling the derivation or dependencies. Existing state-of-the-art CCG parsers use complex pipelines of POS-tagging, supertagging and parsing—each with its own feature sets and parameters (and sources of error)—together with further parameters governing their integration (Clark and Curran, 2007). We show that much simpler models can achieve high performance. Our model predicts lexical categories based on a tiny feature set of word embeddings, capitalization, and 2- character suffixes—with no parsing model beyond a small set of CCG combinators, and no POStagger. Simpler models are easier to implement, replicate and extend. Another goal of our model is to parse CCG optimally and efficiently, without using excessive pruning. CCG’s large set of lexical categories, and generalized notion of constituency, mean that sentences can have a huge number of potential parses. Fast existing CCG par</context>
<context position="5609" citStr="Clark and Curran, 2007" startWordPosition="876" endWordPosition="879">from the lexicon, the grammar is small enough to be handcoded—which allows us, in this paper, to confine the entire statistical model to the lexicon. CCG’s generalized notion of constituency means that many derivations are possible for a given a set of lexical categories. However, most of these derivations will be semantically equivalent—for example, deriving the same dependency structures—in which case the actual choice of derivation is unimportant. Such ambiguity is often called spurious. 2.2 Existing CCG Parsing Models The seminal C&amp;C parser is by far the most popular choice of CCG parser (Clark and Curran, 2007). It showed that it was possible to parse to an expressive linguistic formalism with high speed and accuracy. The performance of the parser has enabled large-scale logic-based distributional research (Harrington, 2010; Lewis and Steedman, 2013a; Lewis and Steedman, 2013b; Reddy et al., 2014), and it is a key component of Boxer (Bos, 2008). The C&amp;C parser uses CKY chart parsing, with a log-linear model to rank parses. The vast number of possible parses means that computing the complete chart is impractical. To resolve this problem, a supertagger is first run over the sentence to prune the set o</context>
<context position="15028" citStr="Clark and Curran (2007)" startWordPosition="2480" endWordPosition="2483">lication, forward composition, backward crossed composition, generalized forward composition, generalized backward crossed composition. These combinators are posited to be linguistically universal. The generalized rules Initial Result Usage N NP Bare noun phrases NP S/(S\NP) Type NP (S\NP)/((S\NP)/NP) raising PP (S\NP)/((S\NP)/PP) Spss\NP NP\NP Reduced Sng\NP NP\NP relative Sadj\NP NP\NP clauses Sto\NP NP\NP Sto\NP N\N Sdcl/NP NP\NP Spss\NP S/S VP Sng\NP S/S Sentence Sto\NP S/S Modifiers Table 1: Set of unary rules used by the parser. are generalized to degree 2. Following Steedman (2000) and Clark and Curran (2007), backward composition is blocked where the argument of the right-hand category is an N or NP. The unhelpful [nb] feature is ignored. As in the C&amp;C parser, we add a special Conjunction rule: Y X &gt; X\X Where Y E {conj, comma, semicolon}. We block conjunctions where the right-hand category is type-raised, punctuation, N, or NP\NP. This rule (and the restrictions) could be removed by changing CCGBank to analyse conjunctions with (X\X)/X categories. We also add syntagmatic rules for removing any punctuation to the right, and for removing openbrackets and open-quotes to the left The grammar also co</context>
</contexts>
<marker>Clark, Curran, 2007</marker>
<rawString>Stephen Clark and James R Curran. 2007. Widecoverage efficient statistical parsing with CCG and log-linear models. Computational Linguistics, 33(4):493–552.</rawString>
</citation>
<citation valid="true">
<authors>
<author>Ronan Collobert</author>
<author>Jason Weston</author>
<author>Léon Bottou</author>
<author>Michael Karlen</author>
<author>Koray Kavukcuoglu</author>
<author>Pavel Kuksa</author>
</authors>
<title>Natural language processing (almost) from scratch.</title>
<date>2011</date>
<journal>The Journal of Machine Learning Research,</journal>
<pages>12--2493</pages>
<contexts>
<context position="30336" citStr="Collobert et al. (2011)" startWordPosition="5018" endWordPosition="5022"> other strongly lexicalized formalisms, such as to LTAG (Bangalore and Joshi, 1999) and to HPSG (Ninomiya et al., 2006). A major contribution of our work over these is showing that factoring models on lexical categories allows fast and exact A* parsing, without the need for beam search. Our parsing approach could be applied to any strongly lexicalized formalism. Our work fits into a tradition of attempting to simplify complex models without sacrificing performance. Klein and Manning (2003b) showed that unlexicalized parsers were only slightly less accurate than their lexicalized counterparts. Collobert et al. (2011) showed how a range of NLP tagging tasks could be performed at high accuracy using a small feature set based on vectorspace word embeddings. However, the extension of this work to phrase-structure parsing (Collobert, 2011) required a more complex model, and did not match the performance of traditional parsing techniques. We achieve state-of-the-art results using the same feature set and a simpler model by exploiting CCG’s lexicalized nature, which makes it more natural to delegate parsing decisions to a tagging model. Other parsing research has focused on building fast parsers for web-scale pr</context>
</contexts>
<marker>Collobert, Weston, Bottou, Karlen, Kavukcuoglu, Kuksa, 2011</marker>
<rawString>Ronan Collobert, Jason Weston, Léon Bottou, Michael Karlen, Koray Kavukcuoglu, and Pavel Kuksa. 2011. Natural language processing (almost) from scratch. The Journal of Machine Learning Research, 12:2493–2537.</rawString>
</citation>
<citation valid="true">
<authors>
<author>Ronan Collobert</author>
</authors>
<title>Deep learning for efficient discriminative parsing.</title>
<date>2011</date>
<booktitle>In International Conference on Artificial Intelligence and Statistics, number EPFLCONF-192374.</booktitle>
<contexts>
<context position="30558" citStr="Collobert, 2011" startWordPosition="5058" endWordPosition="5059">st and exact A* parsing, without the need for beam search. Our parsing approach could be applied to any strongly lexicalized formalism. Our work fits into a tradition of attempting to simplify complex models without sacrificing performance. Klein and Manning (2003b) showed that unlexicalized parsers were only slightly less accurate than their lexicalized counterparts. Collobert et al. (2011) showed how a range of NLP tagging tasks could be performed at high accuracy using a small feature set based on vectorspace word embeddings. However, the extension of this work to phrase-structure parsing (Collobert, 2011) required a more complex model, and did not match the performance of traditional parsing techniques. We achieve state-of-the-art results using the same feature set and a simpler model by exploiting CCG’s lexicalized nature, which makes it more natural to delegate parsing decisions to a tagging model. Other parsing research has focused on building fast parsers for web-scale processing, typically using dependency grammars (e.g. Nivre (2003)). CCG has some advantages over dependency grammars, such as supporting surface-compositional semantics. The fastest dependency parsers use an easyfirst strat</context>
</contexts>
<marker>Collobert, 2011</marker>
<rawString>Ronan Collobert. 2011. Deep learning for efficient discriminative parsing. In International Conference on Artificial Intelligence and Statistics, number EPFLCONF-192374.</rawString>
</citation>
<citation valid="true">
<authors>
<author>Gregory F Coppola</author>
<author>Alexandra Birch</author>
<author>Tejaswini Deoskar</author>
<author>Mark Steedman</author>
</authors>
<title>Simple semisupervised learning for prepositional phrase attachment.</title>
<date>2011</date>
<booktitle>In Proceedings of the 12th International Conference on Parsing Technologies,</booktitle>
<pages>129--139</pages>
<publisher>Association</publisher>
<institution>for Computational Linguistics.</institution>
<contexts>
<context position="33155" citStr="Coppola et al. (2011)" startWordPosition="5467" endWordPosition="5470">e typically focused on extending the lexicon (Thomforde and Steedman, 2011; Deoskar et al., 2014). Finally, it may make it easier to convert other annotated resources, such as UCCA (Abend and Rappoport, 2013) or AMR (Banarescu et al., 2013), to CCG training data—as only specific words need to be converted, rather than full sentences. Our model is weak at certain kinds of decisions, e.g. coordination-scope ambiguities or nonlocal attachments. Incorporating specific models for such decisions may improve accuracy, while still allowing fast and exact search—for example, we intend to try including Coppola et al. (2011)’s model for prepositional phrase attachment. 7 Conclusions We have shown that a simple, principled, deterministic parser combined with a tagging model can parse an expressive linguistic formalism with high speed and accuracy. Although accuracy is not state-of-the-art on CCGBank, our model gives excellent performance on two out-of-domain datasets, and improves the accuracy of a questionanswering system. We have shown that this model allows an efficient heuristic for A∗ parsing, which makes parsing extremely fast, and may enable logic-based distributional semantics to scale to larger corpora. O</context>
</contexts>
<marker>Coppola, Birch, Deoskar, Steedman, 2011</marker>
<rawString>Gregory F Coppola, Alexandra Birch, Tejaswini Deoskar, and Mark Steedman. 2011. Simple semisupervised learning for prepositional phrase attachment. In Proceedings of the 12th International Conference on Parsing Technologies, pages 129–139. Association for Computational Linguistics.</rawString>
</citation>
<citation valid="true">
<authors>
<author>Tejaswini Deoskar</author>
<author>Christos Christodoulopoulos</author>
<author>Alexandra Birch</author>
<author>Mark Steedman</author>
</authors>
<title>Generalizing a Strongly Lexicalized Parser using Unlabeled Data.</title>
<date>2014</date>
<booktitle>In Proceedings of the 14th Conference of the European Chapter of the Association for Computational Linguistics. Association for Computational Linguistics.</booktitle>
<contexts>
<context position="32631" citStr="Deoskar et al., 2014" startWordPosition="5384" endWordPosition="5387">ons to limit over-generation. Honnibal and Curran (2009) show how unary rules can be lexicalized in CCG. Such improvements may improve both the speed and accuracy of our model. Because our parser is factored on a unigram tagging model, it can be trained from isolated annotated words, and does not require annotated parse trees or full sentences. Reducing the requirements for training data eases the task for human annotators. It may also make the model more amenable to semi-supervised approaches to CCG parsing, which have typically focused on extending the lexicon (Thomforde and Steedman, 2011; Deoskar et al., 2014). Finally, it may make it easier to convert other annotated resources, such as UCCA (Abend and Rappoport, 2013) or AMR (Banarescu et al., 2013), to CCG training data—as only specific words need to be converted, rather than full sentences. Our model is weak at certain kinds of decisions, e.g. coordination-scope ambiguities or nonlocal attachments. Incorporating specific models for such decisions may improve accuracy, while still allowing fast and exact search—for example, we intend to try including Coppola et al. (2011)’s model for prepositional phrase attachment. 7 Conclusions We have shown th</context>
</contexts>
<marker>Deoskar, Christodoulopoulos, Birch, Steedman, 2014</marker>
<rawString>Tejaswini Deoskar, Christos Christodoulopoulos, Alexandra Birch, and Mark Steedman. 2014. Generalizing a Strongly Lexicalized Parser using Unlabeled Data. In Proceedings of the 14th Conference of the European Chapter of the Association for Computational Linguistics. Association for Computational Linguistics.</rawString>
</citation>
<citation valid="true">
<authors>
<author>Jason Eisner</author>
</authors>
<title>Efficient normal-form parsing for combinatory categorial grammar.</title>
<date>1996</date>
<booktitle>In Proceedings of the 34th annual meeting on Association for Computational Linguistics,</booktitle>
<pages>79--86</pages>
<publisher>Association</publisher>
<institution>for Computational Linguistics.</institution>
<contexts>
<context position="16351" citStr="Eisner, 1996" startWordPosition="2695" endWordPosition="2696">a, and their clear semantic interpretations. Following Clark and Curran (2007), we also add a (switchable) constraint that only category combinations that have combined in the training data may combine in the test data. We found that this was necessary for evaluation, as the C&amp;C conversion tool for extracting predicate-argument dependencies had relatively low coverage on the CCG derivations produced by our parser. While this restriction is theoretically inelegant, we found it did increase parsing speed without lowering lexi993 cal category accuracy. We also use Eisner Normal Form Constraints (Eisner, 1996), and Hockenmaier and Bisk’s (2010) Constraint 5, which automatically rule out certain spuriously equivalent derivations, improving parsing speed. We add a hard constraint that the root category of the sentence must be a declarative sentence, a question, or a noun-phrase. This grammar is smaller and cleaner than that used by the C&amp;C parser, which uses 32 unary rules (some of which are semantically dubious, such as S[dcl] → NP\NP), and non-standard binary combinators such as merging two NPs into an NP. The C&amp;C parser also has a large number of special case rules for handling punctuation. Our sm</context>
</contexts>
<marker>Eisner, 1996</marker>
<rawString>Jason Eisner. 1996. Efficient normal-form parsing for combinatory categorial grammar. In Proceedings of the 34th annual meeting on Association for Computational Linguistics, pages 79–86. Association for Computational Linguistics.</rawString>
</citation>
<citation valid="true">
<authors>
<author>Timothy AD Fowler</author>
<author>Gerald Penn</author>
</authors>
<title>Accurate context-free parsing with combinatory categorial grammar.</title>
<date>2010</date>
<booktitle>In Proceedings of the 48th Annual Meeting of the Association for Computational Linguistics,</booktitle>
<pages>335--344</pages>
<publisher>Association</publisher>
<institution>for Computational Linguistics.</institution>
<contexts>
<context position="29312" citStr="Fowler and Penn (2010)" startWordPosition="4853" endWordPosition="4856">ved with domain-specific self-training—similar improvements may be possible applying this technique to our model. Auli and Lopez (2011a) have achieved the best CCG parsing accuracy, by allowing the parser and supertagger to perform joint inference (though there is a significant speed penalty). Auli and Lopez (2011b) were the first to use A* parsing for CCG, but their system is both much slower and less accurate than ours (due to a different model and a different A* heuristic). Krishnamurthy and Mitchell (2014) show how CCG parsing can be improved by jointly modelling the syntax and semantics. Fowler and Penn (2010) apply the Petrov parser to CCG, making a small improvement in accuracy over the C&amp;C parser, at the cost of a 300-fold speed decrease. Zhang and Clark (2011) and Xu et al. (2014) explored shift-reduce CCG parsing, but despite the use of a linear-time algorithm, parsing speed in practice is significantly slower than the C&amp;C parser. Parsers based on supertagging models have previously been applied to other strongly lexicalized formalisms, such as to LTAG (Bangalore and Joshi, 1999) and to HPSG (Ninomiya et al., 2006). A major contribution of our work over these is showing that factoring models o</context>
</contexts>
<marker>Fowler, Penn, 2010</marker>
<rawString>Timothy AD Fowler and Gerald Penn. 2010. Accurate context-free parsing with combinatory categorial grammar. In Proceedings of the 48th Annual Meeting of the Association for Computational Linguistics, pages 335–344. Association for Computational Linguistics.</rawString>
</citation>
<citation valid="true">
<authors>
<author>Yoav Goldberg</author>
<author>Michael Elhadad</author>
</authors>
<title>An efficient algorithm for easy-first non-directional dependency parsing.</title>
<date>2010</date>
<booktitle>In Human Language Technologies: The 2010 Annual Conference of the North American Chapter of the Association for Computational Linguistics,</booktitle>
<pages>742--750</pages>
<publisher>Association</publisher>
<institution>for Computational Linguistics.</institution>
<contexts>
<context position="31276" citStr="Goldberg and Elhadad, 2010" startWordPosition="5164" endWordPosition="5167">hniques. We achieve state-of-the-art results using the same feature set and a simpler model by exploiting CCG’s lexicalized nature, which makes it more natural to delegate parsing decisions to a tagging model. Other parsing research has focused on building fast parsers for web-scale processing, typically using dependency grammars (e.g. Nivre (2003)). CCG has some advantages over dependency grammars, such as supporting surface-compositional semantics. The fastest dependency parsers use an easyfirst strategy, in which edges are added greedily in order of their score, with 0(nlog(n)) complexity (Goldberg and Elhadad, 2010; Tratz and Hovy, 2011). This strategy is reminiscent of our A* search, which expands the chart in a bestfirst order. A* has higher asymptotic complexity, but finds a globally optimal solution. 6 Future Work We believe that our model opens several interesting directions for future research. One interesting angle would be to increase the amount of information in CCGBank’s lexical entries, to further reduce the search space for the parser. For example, PP categories could be distinguished with the relevant preposition as a feature; punctuation and coordination could be given more detailed catego</context>
</contexts>
<marker>Goldberg, Elhadad, 2010</marker>
<rawString>Yoav Goldberg and Michael Elhadad. 2010. An efficient algorithm for easy-first non-directional dependency parsing. In Human Language Technologies: The 2010 Annual Conference of the North American Chapter of the Association for Computational Linguistics, pages 742–750. Association for Computational Linguistics.</rawString>
</citation>
<citation valid="true">
<authors>
<author>Brian Harrington</author>
</authors>
<title>A semantic network approach to measuring relatedness.</title>
<date>2010</date>
<booktitle>In Proceedings of the 23rd International Conference on Computational Linguistics: Posters, COLING ’10,</booktitle>
<pages>356--364</pages>
<publisher>Association</publisher>
<institution>for Computational Linguistics.</institution>
<contexts>
<context position="5826" citStr="Harrington, 2010" startWordPosition="910" endWordPosition="911">ossible for a given a set of lexical categories. However, most of these derivations will be semantically equivalent—for example, deriving the same dependency structures—in which case the actual choice of derivation is unimportant. Such ambiguity is often called spurious. 2.2 Existing CCG Parsing Models The seminal C&amp;C parser is by far the most popular choice of CCG parser (Clark and Curran, 2007). It showed that it was possible to parse to an expressive linguistic formalism with high speed and accuracy. The performance of the parser has enabled large-scale logic-based distributional research (Harrington, 2010; Lewis and Steedman, 2013a; Lewis and Steedman, 2013b; Reddy et al., 2014), and it is a key component of Boxer (Bos, 2008). The C&amp;C parser uses CKY chart parsing, with a log-linear model to rank parses. The vast number of possible parses means that computing the complete chart is impractical. To resolve this problem, a supertagger is first run over the sentence to prune the set of lexical categories considered by the parser for each word. The initial beam outputs an average of just 1.2 categories per word, rather than the 425 possible categories—making the standard CKY parsing algorithm very </context>
</contexts>
<marker>Harrington, 2010</marker>
<rawString>Brian Harrington. 2010. A semantic network approach to measuring relatedness. In Proceedings of the 23rd International Conference on Computational Linguistics: Posters, COLING ’10, pages 356–364. Association for Computational Linguistics.</rawString>
</citation>
<citation valid="true">
<authors>
<author>Julia Hockenmaier</author>
<author>Mark Steedman</author>
</authors>
<title>CCGbank: a corpus of CCG derivations and dependency structures extracted from the Penn Treebank.</title>
<date>2007</date>
<journal>Computational Linguistics,</journal>
<volume>33</volume>
<issue>3</issue>
<contexts>
<context position="21505" citStr="Hockenmaier and Steedman, 2007" startWordPosition="3540" endWordPosition="3543">ASYCCG 83.37 99.96 83.37 13s 81.75 100 81.75 77.24 100 77.24 EASYCCG C&amp;C 86.14 99.96 86.11 69s 82.46 100 82.46 78.00 99.8 77.88 Table 2: Parsing F1-scores for labelled dependencies across a range of domains. F1 (cov) refers to results on sentences which the parser is able to parse, and F1 (all) gives results over all sentences. For the EASYCCG results, scores are only over parses where the C&amp;C dependency extraction script was successful, which was 99.3% on CCGBank, 99.5% on Wikipedia, and 100% on Bioinfer. 4 Experiments 4.1 Experimental Setup We trained our model on Sections 02-21 of CCGBank (Hockenmaier and Steedman, 2007), using Section 00 for development. For testing, we used Section 23 of CCGBank, a Wikipedia corpus annotated by Honnibal and Curran (2009), and the Bioinfer corpus of biomedical abstracts (Pyysalo et al., 2007). The latter two are out-of-domain, so are more challenging for the parsers. We compare the performance of our model against both the C&amp;C parser, and the system described in Lewis and Steedman (2014). This model uses the same supertagger as used in EASYCCG, but uses the C&amp;C parser for parsing, using adaptive supertagging with the default values. All timing experiments used the same 1.8Gh</context>
</contexts>
<marker>Hockenmaier, Steedman, 2007</marker>
<rawString>Julia Hockenmaier and Mark Steedman. 2007. CCGbank: a corpus of CCG derivations and dependency structures extracted from the Penn Treebank. Computational Linguistics, 33(3):355–396.</rawString>
</citation>
<citation valid="true">
<authors>
<author>Julia Hockenmaier</author>
</authors>
<title>Data and models for statistical parsing with combinatory categorial grammar.</title>
<date>2003</date>
<contexts>
<context position="12139" citStr="Hockenmaier (2003)" startWordPosition="2001" endWordPosition="2002">parsing, we use an A* search for the mostprobable complete CCG derivation of a sentence. A key advantage of A* parsing over CKY parsing is that it does not require us to prune the search space first with a supertagger, allowing the parser to consider the complete distribution of 425 categories for each word (in contrast to an average of 3.57 categories per word considered by the C&amp;C parser’s most relaxed beam). This is possible because A* only searches for the Viterbi parse of a sentence, rather than building a complete chart with every possible category per word (another alternative, used by Hockenmaier (2003), is to use a highly aggressive beam search in the parser). In A* parsing, items on the agenda are sorted by their cost; the product of their inside probability and an upper bound on their outside probability. &gt; NP\NP &lt; NP &gt; NP\NP &lt; NP NP NP &gt;B NP 992 For a span wi ... wj with lexical categories ci ... cj in a sentence S = w1 ... wn, the inside probability is simply: jljk=i p(ck|S) The factorization of our model lets us give the following upper-bound on the outside probability: h(wi ... wj) = Ilk&lt;i k=1 maxckp(ck|S)× �k≤n k=j+1 maxckp(ck|S) This heuristic assumes that all words outside the span</context>
</contexts>
<marker>Hockenmaier, 2003</marker>
<rawString>Julia Hockenmaier. 2003. Data and models for statistical parsing with combinatory categorial grammar.</rawString>
</citation>
<citation valid="true">
<authors>
<author>Matthew Honnibal</author>
<author>James R Curran</author>
</authors>
<title>Fully lexicalising CCGbank with hat categories.</title>
<date>2009</date>
<booktitle>In Proceedings of the 2009 Conference on Empirical Methods in Natural Language Processing: Volume</booktitle>
<volume>3</volume>
<pages>1212--1221</pages>
<publisher>Association</publisher>
<institution>for Computational Linguistics.</institution>
<contexts>
<context position="21643" citStr="Honnibal and Curran (2009)" startWordPosition="3563" endWordPosition="3566">g F1-scores for labelled dependencies across a range of domains. F1 (cov) refers to results on sentences which the parser is able to parse, and F1 (all) gives results over all sentences. For the EASYCCG results, scores are only over parses where the C&amp;C dependency extraction script was successful, which was 99.3% on CCGBank, 99.5% on Wikipedia, and 100% on Bioinfer. 4 Experiments 4.1 Experimental Setup We trained our model on Sections 02-21 of CCGBank (Hockenmaier and Steedman, 2007), using Section 00 for development. For testing, we used Section 23 of CCGBank, a Wikipedia corpus annotated by Honnibal and Curran (2009), and the Bioinfer corpus of biomedical abstracts (Pyysalo et al., 2007). The latter two are out-of-domain, so are more challenging for the parsers. We compare the performance of our model against both the C&amp;C parser, and the system described in Lewis and Steedman (2014). This model uses the same supertagger as used in EASYCCG, but uses the C&amp;C parser for parsing, using adaptive supertagging with the default values. All timing experiments used the same 1.8Ghz AMD machine. 4.2 Parsing Accuracy Results are shown in Table 2. Our parser performs competitively with a much more complex parsing model</context>
<context position="32066" citStr="Honnibal and Curran (2009)" startWordPosition="5288" endWordPosition="5291"> a globally optimal solution. 6 Future Work We believe that our model opens several interesting directions for future research. One interesting angle would be to increase the amount of information in CCGBank’s lexical entries, to further reduce the search space for the parser. For example, PP categories could be distinguished with the relevant preposition as a feature; punctuation and coordination could be given more detailed categories to avoid needing their own combinators, and slashes could be extended 997 with Baldridge and Kruijff (2003)’s multi-modal extensions to limit over-generation. Honnibal and Curran (2009) show how unary rules can be lexicalized in CCG. Such improvements may improve both the speed and accuracy of our model. Because our parser is factored on a unigram tagging model, it can be trained from isolated annotated words, and does not require annotated parse trees or full sentences. Reducing the requirements for training data eases the task for human annotators. It may also make the model more amenable to semi-supervised approaches to CCG parsing, which have typically focused on extending the lexicon (Thomforde and Steedman, 2011; Deoskar et al., 2014). Finally, it may make it easier to</context>
</contexts>
<marker>Honnibal, Curran, 2009</marker>
<rawString>Matthew Honnibal and James R Curran. 2009. Fully lexicalising CCGbank with hat categories. In Proceedings of the 2009 Conference on Empirical Methods in Natural Language Processing: Volume 3-Volume 3, pages 1212–1221. Association for Computational Linguistics.</rawString>
</citation>
<citation valid="true">
<authors>
<author>Dan Klein</author>
<author>Christopher D Manning</author>
</authors>
<title>A* parsing: fast exact viterbi parse selection.</title>
<date>2003</date>
<booktitle>In Proceedings of the 2003 Conference of the North American Chapter of the Association for Computational Linguistics on Human Language TechnologyVolume 1,</booktitle>
<pages>40--47</pages>
<publisher>Association</publisher>
<institution>for Computational Linguistics.</institution>
<contexts>
<context position="6660" citStr="Klein and Manning (2003" startWordPosition="1051" endWordPosition="1054">t number of possible parses means that computing the complete chart is impractical. To resolve this problem, a supertagger is first run over the sentence to prune the set of lexical categories considered by the parser for each word. The initial beam outputs an average of just 1.2 categories per word, rather than the 425 possible categories—making the standard CKY parsing algorithm very efficient. If the parser fails to find any analysis of the complete sentence with this set of supertags, the supertagger re-analyses the sentence with a more relaxed beam (adaptive supertagging). 2.3 A* Parsing Klein and Manning (2003a) introduce A* parsing for PCFGs. The parser maintains a chart and an agenda, which is a priority queue of items to add to the chart. The agenda is sorted based on the items’ inside probability, and a heuristic upper-bound on the outside probability—to give an upper bound on the probability of the complete parse. The chart is then expanded in best-first order, until a complete parse for the sentence is found. Klein and Manning calculate an upper bound on the outside probability of a span based on a summary of the context. For example, the summary for the SX heuristic is the category of the sp</context>
<context position="30206" citStr="Klein and Manning (2003" startWordPosition="5001" endWordPosition="5004">peed in practice is significantly slower than the C&amp;C parser. Parsers based on supertagging models have previously been applied to other strongly lexicalized formalisms, such as to LTAG (Bangalore and Joshi, 1999) and to HPSG (Ninomiya et al., 2006). A major contribution of our work over these is showing that factoring models on lexical categories allows fast and exact A* parsing, without the need for beam search. Our parsing approach could be applied to any strongly lexicalized formalism. Our work fits into a tradition of attempting to simplify complex models without sacrificing performance. Klein and Manning (2003b) showed that unlexicalized parsers were only slightly less accurate than their lexicalized counterparts. Collobert et al. (2011) showed how a range of NLP tagging tasks could be performed at high accuracy using a small feature set based on vectorspace word embeddings. However, the extension of this work to phrase-structure parsing (Collobert, 2011) required a more complex model, and did not match the performance of traditional parsing techniques. We achieve state-of-the-art results using the same feature set and a simpler model by exploiting CCG’s lexicalized nature, which makes it more natu</context>
</contexts>
<marker>Klein, Manning, 2003</marker>
<rawString>Dan Klein and Christopher D Manning. 2003a. A* parsing: fast exact viterbi parse selection. In Proceedings of the 2003 Conference of the North American Chapter of the Association for Computational Linguistics on Human Language TechnologyVolume 1, pages 40–47. Association for Computational Linguistics.</rawString>
</citation>
<citation valid="true">
<authors>
<author>Dan Klein</author>
<author>Christopher D Manning</author>
</authors>
<title>Accurate unlexicalized parsing.</title>
<date>2003</date>
<booktitle>In Proceedings of the 41st Annual Meeting on Association for Computational Linguistics-Volume 1,</booktitle>
<pages>423--430</pages>
<publisher>Association</publisher>
<institution>for Computational Linguistics.</institution>
<contexts>
<context position="6660" citStr="Klein and Manning (2003" startWordPosition="1051" endWordPosition="1054">t number of possible parses means that computing the complete chart is impractical. To resolve this problem, a supertagger is first run over the sentence to prune the set of lexical categories considered by the parser for each word. The initial beam outputs an average of just 1.2 categories per word, rather than the 425 possible categories—making the standard CKY parsing algorithm very efficient. If the parser fails to find any analysis of the complete sentence with this set of supertags, the supertagger re-analyses the sentence with a more relaxed beam (adaptive supertagging). 2.3 A* Parsing Klein and Manning (2003a) introduce A* parsing for PCFGs. The parser maintains a chart and an agenda, which is a priority queue of items to add to the chart. The agenda is sorted based on the items’ inside probability, and a heuristic upper-bound on the outside probability—to give an upper bound on the probability of the complete parse. The chart is then expanded in best-first order, until a complete parse for the sentence is found. Klein and Manning calculate an upper bound on the outside probability of a span based on a summary of the context. For example, the summary for the SX heuristic is the category of the sp</context>
<context position="30206" citStr="Klein and Manning (2003" startWordPosition="5001" endWordPosition="5004">peed in practice is significantly slower than the C&amp;C parser. Parsers based on supertagging models have previously been applied to other strongly lexicalized formalisms, such as to LTAG (Bangalore and Joshi, 1999) and to HPSG (Ninomiya et al., 2006). A major contribution of our work over these is showing that factoring models on lexical categories allows fast and exact A* parsing, without the need for beam search. Our parsing approach could be applied to any strongly lexicalized formalism. Our work fits into a tradition of attempting to simplify complex models without sacrificing performance. Klein and Manning (2003b) showed that unlexicalized parsers were only slightly less accurate than their lexicalized counterparts. Collobert et al. (2011) showed how a range of NLP tagging tasks could be performed at high accuracy using a small feature set based on vectorspace word embeddings. However, the extension of this work to phrase-structure parsing (Collobert, 2011) required a more complex model, and did not match the performance of traditional parsing techniques. We achieve state-of-the-art results using the same feature set and a simpler model by exploiting CCG’s lexicalized nature, which makes it more natu</context>
</contexts>
<marker>Klein, Manning, 2003</marker>
<rawString>Dan Klein and Christopher D Manning. 2003b. Accurate unlexicalized parsing. In Proceedings of the 41st Annual Meeting on Association for Computational Linguistics-Volume 1, pages 423–430. Association for Computational Linguistics.</rawString>
</citation>
<citation valid="true">
<authors>
<author>Jayant Krishnamurthy</author>
<author>Tom M Mitchell</author>
</authors>
<title>Joint syntactic and semantic parsing with combinatory categorial grammar.</title>
<date>2014</date>
<contexts>
<context position="29205" citStr="Krishnamurthy and Mitchell (2014)" startWordPosition="4834" endWordPosition="4838">l. (2014)’s supervised model. ing model. Kummerfeld et al. (2010) showed that the speed of the C&amp;C parser can be improved with domain-specific self-training—similar improvements may be possible applying this technique to our model. Auli and Lopez (2011a) have achieved the best CCG parsing accuracy, by allowing the parser and supertagger to perform joint inference (though there is a significant speed penalty). Auli and Lopez (2011b) were the first to use A* parsing for CCG, but their system is both much slower and less accurate than ours (due to a different model and a different A* heuristic). Krishnamurthy and Mitchell (2014) show how CCG parsing can be improved by jointly modelling the syntax and semantics. Fowler and Penn (2010) apply the Petrov parser to CCG, making a small improvement in accuracy over the C&amp;C parser, at the cost of a 300-fold speed decrease. Zhang and Clark (2011) and Xu et al. (2014) explored shift-reduce CCG parsing, but despite the use of a linear-time algorithm, parsing speed in practice is significantly slower than the C&amp;C parser. Parsers based on supertagging models have previously been applied to other strongly lexicalized formalisms, such as to LTAG (Bangalore and Joshi, 1999) and to H</context>
</contexts>
<marker>Krishnamurthy, Mitchell, 2014</marker>
<rawString>Jayant Krishnamurthy and Tom M Mitchell. 2014. Joint syntactic and semantic parsing with combinatory categorial grammar. June.</rawString>
</citation>
<citation valid="true">
<authors>
<author>Jonathan K Kummerfeld</author>
<author>Jessika Roesner</author>
<author>Tim Dawborn</author>
<author>James Haggerty</author>
<author>James R Curran</author>
<author>Stephen Clark</author>
</authors>
<title>Faster parsing by supertagger adaptation.</title>
<date>2010</date>
<booktitle>In Proceedings of the 48th Annual Meeting of the Association for Computational Linguistics, ACL ’10,</booktitle>
<pages>345--355</pages>
<publisher>Association</publisher>
<institution>for Computational Linguistics.</institution>
<contexts>
<context position="28637" citStr="Kummerfeld et al. (2010)" startWordPosition="4741" endWordPosition="4744">r. The improvement is particularly large on oracle accuracy, increasing the upper bound on the performance of the semantic parser by around 4 points. 5 Related Work CCG parsing has been the subject of much research. We have already described the C&amp;C pars3Using the Business, Film and People domains, with 1115 questions for training and 570 for testing. 996 F1-score 100-best Oracle F1-score Iteration Iteration 2 4 6 8 10 50 48 46 44 42 40 C&amp;C EASYCCG 2 4 6 8 10 64 62 60 58 56 54 C&amp;C EASYCCG Figure 3: Question Answering accuracy per iteration of Reddy et al. (2014)’s supervised model. ing model. Kummerfeld et al. (2010) showed that the speed of the C&amp;C parser can be improved with domain-specific self-training—similar improvements may be possible applying this technique to our model. Auli and Lopez (2011a) have achieved the best CCG parsing accuracy, by allowing the parser and supertagger to perform joint inference (though there is a significant speed penalty). Auli and Lopez (2011b) were the first to use A* parsing for CCG, but their system is both much slower and less accurate than ours (due to a different model and a different A* heuristic). Krishnamurthy and Mitchell (2014) show how CCG parsing can be imp</context>
</contexts>
<marker>Kummerfeld, Roesner, Dawborn, Haggerty, Curran, Clark, 2010</marker>
<rawString>Jonathan K. Kummerfeld, Jessika Roesner, Tim Dawborn, James Haggerty, James R. Curran, and Stephen Clark. 2010. Faster parsing by supertagger adaptation. In Proceedings of the 48th Annual Meeting of the Association for Computational Linguistics, ACL ’10, pages 345–355. Association for Computational Linguistics.</rawString>
</citation>
<citation valid="false">
<authors>
<author>Mike Lewis</author>
<author>Mark Steedman</author>
</authors>
<booktitle>2013a. Combined Distributional and Logical Semantics. Transactions of the Association for Computational Linguistics,</booktitle>
<pages>1--179</pages>
<marker>Lewis, Steedman, </marker>
<rawString>Mike Lewis and Mark Steedman. 2013a. Combined Distributional and Logical Semantics. Transactions of the Association for Computational Linguistics, 1:179–192.</rawString>
</citation>
<citation valid="true">
<authors>
<author>Mike Lewis</author>
<author>Mark Steedman</author>
</authors>
<title>Unsupervised induction of cross-lingual semantic relations.</title>
<date>2013</date>
<booktitle>In Proceedings of the 2013 Conference on Empirical Methods in Natural Language Processing,</booktitle>
<pages>681--692</pages>
<publisher>Association</publisher>
<institution>for Computational Linguistics.</institution>
<location>Seattle, Washington, USA,</location>
<contexts>
<context position="5852" citStr="Lewis and Steedman, 2013" startWordPosition="912" endWordPosition="915">n a set of lexical categories. However, most of these derivations will be semantically equivalent—for example, deriving the same dependency structures—in which case the actual choice of derivation is unimportant. Such ambiguity is often called spurious. 2.2 Existing CCG Parsing Models The seminal C&amp;C parser is by far the most popular choice of CCG parser (Clark and Curran, 2007). It showed that it was possible to parse to an expressive linguistic formalism with high speed and accuracy. The performance of the parser has enabled large-scale logic-based distributional research (Harrington, 2010; Lewis and Steedman, 2013a; Lewis and Steedman, 2013b; Reddy et al., 2014), and it is a key component of Boxer (Bos, 2008). The C&amp;C parser uses CKY chart parsing, with a log-linear model to rank parses. The vast number of possible parses means that computing the complete chart is impractical. To resolve this problem, a supertagger is first run over the sentence to prune the set of lexical categories considered by the parser for each word. The initial beam outputs an average of just 1.2 categories per word, rather than the 425 possible categories—making the standard CKY parsing algorithm very efficient. If the parser f</context>
<context position="17135" citStr="Lewis and Steedman (2013" startWordPosition="2822" endWordPosition="2825"> constraint that the root category of the sentence must be a declarative sentence, a question, or a noun-phrase. This grammar is smaller and cleaner than that used by the C&amp;C parser, which uses 32 unary rules (some of which are semantically dubious, such as S[dcl] → NP\NP), and non-standard binary combinators such as merging two NPs into an NP. The C&amp;C parser also has a large number of special case rules for handling punctuation. Our smaller grammar reduces the grammar constant, eases implementation, and simplifies the job of building downstream semantic parsers such as those of Bos (2008) or Lewis and Steedman (2013a) (which must implement semantic analogs of each syntactic rule). 3.5 Extracting Dependency Structures The parsing model defined in Section 3.2 requires us to compute unlabelled dependency trees from CCG derivations (to prefer non-local attachments). It is simple to extract an unlabelled dependency tree from a CCG parse, by defining one argument of each binary rule instantiation to be the head. For forward application and (generalized) forward composition, we define the head to be the left argument, unless the left argument is an endocentric head-passing modifier category X/X. We do the inver</context>
<context position="23925" citStr="Lewis and Steedman, 2013" startWordPosition="3940" endWordPosition="3943">much lower on out-of-domain datasets (2.8 points in domain, but only 0.65-0.75 out-of-domain), suggesting that much of the C&amp;C parser’s dependency model is domain specific, and does not generalize well to other domains. We also briefly experimented using the C&amp;C supertagger (with a beam of β = 10−5) with the EASYCCG parser. Performance was much worse, with an F-score of 79.63% on the 97.8% of sentences it parsed on CCGBank Section 23. This shows that our model is reliant on the accuracy of the supertagger. 4.3 Parsing Speed CCG parsers have been used in distributional approaches to semantics (Lewis and Steedman, 2013a; Lewis and Steedman, 2013b), which benefit from large corpora. However, even though the C&amp;C parser is relatively fast, it will still take over 40 CPU-days to parse the Gigaword corpus on our hardware, which is slow enough to be an obstacle to scaling distributional semantics to larger cor995 50 Average Parse Time (ms) 40 30 20 10 0 20 40 60 80 100 Sentence Length Figure 2: Average parse times in milliseconds, by sentence length. pora such as ClueWeb. Therefore, it is important to be able to parse sentences at a high speed. We measured parsing times on Section 23 of CCGBank (after developing </context>
</contexts>
<marker>Lewis, Steedman, 2013</marker>
<rawString>Mike Lewis and Mark Steedman. 2013b. Unsupervised induction of cross-lingual semantic relations. In Proceedings of the 2013 Conference on Empirical Methods in Natural Language Processing, pages 681–692, Seattle, Washington, USA, October. Association for Computational Linguistics.</rawString>
</citation>
<citation valid="true">
<authors>
<author>Mike Lewis</author>
<author>Mark Steedman</author>
</authors>
<title>Improved CCG parsing with Semi-supervised Supertagging.</title>
<date>2014</date>
<journal>Transactions of the Association for Computational Linguistics</journal>
<note>(to appear).</note>
<contexts>
<context position="8232" citStr="Lewis and Steedman (2014)" startWordPosition="1324" endWordPosition="1327">d Lopez (2011b) find that A* CCG parsing with this heuristic is very slow. However, they achieve a modest 15% speed improvement over CKY when A* is combined with adaptive supertagging. One reason is that the heuristic estimate is rather coarse, as it deals with the best possible outside context, rather than the actual sentence. We introduce a new heuristic which gives a tighter upper bound on the outside probability. 3 Model 3.1 Lexical Category Model As input, our parser takes a distribution over all CCG lexical categories for each word in the sentence. These distributions are assigned using Lewis and Steedman (2014)’s semi-supervised supertagging model. The supertagger is a unigram log-linear classifier that uses features of the ±3 word context window surrounding a word. The 991 key feature is word embeddings, initialized with the 50-dimensional embeddings trained in Turian et al. (2010), and fine-tuned during supervised training. The model also uses 2-character suffixes and capitalization features. The use of word embeddings, which are trained on a large unlabelled corpus, allows the supertagger to generalize well to words not present in the labelled data. It does not use a POS-tagger, which avoids prob</context>
<context position="21914" citStr="Lewis and Steedman (2014)" startWordPosition="3608" endWordPosition="3611">ion script was successful, which was 99.3% on CCGBank, 99.5% on Wikipedia, and 100% on Bioinfer. 4 Experiments 4.1 Experimental Setup We trained our model on Sections 02-21 of CCGBank (Hockenmaier and Steedman, 2007), using Section 00 for development. For testing, we used Section 23 of CCGBank, a Wikipedia corpus annotated by Honnibal and Curran (2009), and the Bioinfer corpus of biomedical abstracts (Pyysalo et al., 2007). The latter two are out-of-domain, so are more challenging for the parsers. We compare the performance of our model against both the C&amp;C parser, and the system described in Lewis and Steedman (2014). This model uses the same supertagger as used in EASYCCG, but uses the C&amp;C parser for parsing, using adaptive supertagging with the default values. All timing experiments used the same 1.8Ghz AMD machine. 4.2 Parsing Accuracy Results are shown in Table 2. Our parser performs competitively with a much more complex parsing model, and outperforms the C&amp;C pipeline on both out-of-domain datasets. This result confirms our hypothesis that the majority of parsing decisions can be made accurately with a simple tagging model and a deterministic parser. We see that the combination of the EASYCCG superta</context>
</contexts>
<marker>Lewis, Steedman, 2014</marker>
<rawString>Mike Lewis and Mark Steedman. 2014. Improved CCG parsing with Semi-supervised Supertagging. Transactions of the Association for Computational Linguistics (to appear).</rawString>
</citation>
<citation valid="true">
<authors>
<author>Takashi Ninomiya</author>
<author>Takuya Matsuzaki</author>
<author>Yoshimasa Tsuruoka</author>
<author>Yusuke Miyao</author>
<author>Jun’ichi Tsujii</author>
</authors>
<title>Extremely lexicalized models for accurate and fast hpsg parsing.</title>
<date>2006</date>
<booktitle>In Proceedings of the 2006 Conference on Empirical Methods in Natural Language Processing, EMNLP ’06,</booktitle>
<pages>155--163</pages>
<publisher>Association</publisher>
<institution>for Computational Linguistics.</institution>
<location>Stroudsburg, PA, USA.</location>
<contexts>
<context position="29832" citStr="Ninomiya et al., 2006" startWordPosition="4940" endWordPosition="4943"> how CCG parsing can be improved by jointly modelling the syntax and semantics. Fowler and Penn (2010) apply the Petrov parser to CCG, making a small improvement in accuracy over the C&amp;C parser, at the cost of a 300-fold speed decrease. Zhang and Clark (2011) and Xu et al. (2014) explored shift-reduce CCG parsing, but despite the use of a linear-time algorithm, parsing speed in practice is significantly slower than the C&amp;C parser. Parsers based on supertagging models have previously been applied to other strongly lexicalized formalisms, such as to LTAG (Bangalore and Joshi, 1999) and to HPSG (Ninomiya et al., 2006). A major contribution of our work over these is showing that factoring models on lexical categories allows fast and exact A* parsing, without the need for beam search. Our parsing approach could be applied to any strongly lexicalized formalism. Our work fits into a tradition of attempting to simplify complex models without sacrificing performance. Klein and Manning (2003b) showed that unlexicalized parsers were only slightly less accurate than their lexicalized counterparts. Collobert et al. (2011) showed how a range of NLP tagging tasks could be performed at high accuracy using a small featu</context>
</contexts>
<marker>Ninomiya, Matsuzaki, Tsuruoka, Miyao, Tsujii, 2006</marker>
<rawString>Takashi Ninomiya, Takuya Matsuzaki, Yoshimasa Tsuruoka, Yusuke Miyao, and Jun’ichi Tsujii. 2006. Extremely lexicalized models for accurate and fast hpsg parsing. In Proceedings of the 2006 Conference on Empirical Methods in Natural Language Processing, EMNLP ’06, pages 155–163, Stroudsburg, PA, USA. Association for Computational Linguistics.</rawString>
</citation>
<citation valid="true">
<authors>
<author>Joakim Nivre</author>
</authors>
<title>An efficient algorithm for projective dependency parsing.</title>
<date>2003</date>
<booktitle>In Proceedings of the 8th International Workshop on Parsing Technologies (IWPT.</booktitle>
<contexts>
<context position="31000" citStr="Nivre (2003)" startWordPosition="5127" endWordPosition="5128">rformed at high accuracy using a small feature set based on vectorspace word embeddings. However, the extension of this work to phrase-structure parsing (Collobert, 2011) required a more complex model, and did not match the performance of traditional parsing techniques. We achieve state-of-the-art results using the same feature set and a simpler model by exploiting CCG’s lexicalized nature, which makes it more natural to delegate parsing decisions to a tagging model. Other parsing research has focused on building fast parsers for web-scale processing, typically using dependency grammars (e.g. Nivre (2003)). CCG has some advantages over dependency grammars, such as supporting surface-compositional semantics. The fastest dependency parsers use an easyfirst strategy, in which edges are added greedily in order of their score, with 0(nlog(n)) complexity (Goldberg and Elhadad, 2010; Tratz and Hovy, 2011). This strategy is reminiscent of our A* search, which expands the chart in a bestfirst order. A* has higher asymptotic complexity, but finds a globally optimal solution. 6 Future Work We believe that our model opens several interesting directions for future research. One interesting angle would be t</context>
</contexts>
<marker>Nivre, 2003</marker>
<rawString>Joakim Nivre. 2003. An efficient algorithm for projective dependency parsing. In Proceedings of the 8th International Workshop on Parsing Technologies (IWPT.</rawString>
</citation>
<citation valid="true">
<authors>
<author>Sampo Pyysalo</author>
<author>Filip Ginter</author>
<author>Juho Heimonen</author>
<author>Jari Björne</author>
<author>Jorma Boberg</author>
<author>Jouni Järvinen</author>
<author>Tapio Salakoski</author>
</authors>
<title>Bioinfer: a corpus for information extraction in the biomedical domain.</title>
<date>2007</date>
<journal>BMC bioinformatics,</journal>
<volume>8</volume>
<issue>1</issue>
<contexts>
<context position="21715" citStr="Pyysalo et al., 2007" startWordPosition="3574" endWordPosition="3577">ers to results on sentences which the parser is able to parse, and F1 (all) gives results over all sentences. For the EASYCCG results, scores are only over parses where the C&amp;C dependency extraction script was successful, which was 99.3% on CCGBank, 99.5% on Wikipedia, and 100% on Bioinfer. 4 Experiments 4.1 Experimental Setup We trained our model on Sections 02-21 of CCGBank (Hockenmaier and Steedman, 2007), using Section 00 for development. For testing, we used Section 23 of CCGBank, a Wikipedia corpus annotated by Honnibal and Curran (2009), and the Bioinfer corpus of biomedical abstracts (Pyysalo et al., 2007). The latter two are out-of-domain, so are more challenging for the parsers. We compare the performance of our model against both the C&amp;C parser, and the system described in Lewis and Steedman (2014). This model uses the same supertagger as used in EASYCCG, but uses the C&amp;C parser for parsing, using adaptive supertagging with the default values. All timing experiments used the same 1.8Ghz AMD machine. 4.2 Parsing Accuracy Results are shown in Table 2. Our parser performs competitively with a much more complex parsing model, and outperforms the C&amp;C pipeline on both out-of-domain datasets. This </context>
</contexts>
<marker>Pyysalo, Ginter, Heimonen, Björne, Boberg, Järvinen, Salakoski, 2007</marker>
<rawString>Sampo Pyysalo, Filip Ginter, Juho Heimonen, Jari Björne, Jorma Boberg, Jouni Järvinen, and Tapio Salakoski. 2007. Bioinfer: a corpus for information extraction in the biomedical domain. BMC bioinformatics, 8(1):50.</rawString>
</citation>
<citation valid="true">
<authors>
<author>Siva Reddy</author>
<author>Mirella Lapata</author>
<author>Mark Steedman</author>
</authors>
<title>Large-scale Semantic Parsing without Question-Answer Pairs.</title>
<date>2014</date>
<journal>Transactions of the Association for Computational Linguistics</journal>
<note>(to appear).</note>
<contexts>
<context position="5901" citStr="Reddy et al., 2014" startWordPosition="920" endWordPosition="923">derivations will be semantically equivalent—for example, deriving the same dependency structures—in which case the actual choice of derivation is unimportant. Such ambiguity is often called spurious. 2.2 Existing CCG Parsing Models The seminal C&amp;C parser is by far the most popular choice of CCG parser (Clark and Curran, 2007). It showed that it was possible to parse to an expressive linguistic formalism with high speed and accuracy. The performance of the parser has enabled large-scale logic-based distributional research (Harrington, 2010; Lewis and Steedman, 2013a; Lewis and Steedman, 2013b; Reddy et al., 2014), and it is a key component of Boxer (Bos, 2008). The C&amp;C parser uses CKY chart parsing, with a log-linear model to rank parses. The vast number of possible parses means that computing the complete chart is impractical. To resolve this problem, a supertagger is first run over the sentence to prune the set of lexical categories considered by the parser for each word. The initial beam outputs an average of just 1.2 categories per word, rather than the 425 possible categories—making the standard CKY parsing algorithm very efficient. If the parser fails to find any analysis of the complete sentenc</context>
<context position="26929" citStr="Reddy et al. (2014)" startWordPosition="4449" endWordPosition="4452">249 words, which took 37 seconds to parse (3 times longer than the other 1912 sentences put together). In practice, this scaling is unlikely to be problematic, as long sentences are typically filtered when processing large corpora. 4.4 Semantic Parsing A major motivation for CCG parsing is to exploit its transparent interface to the semantics, allowing syntactic parsers to do much of the work of semantic parsers. Therefore, perhaps the most relevant measure of the performance of a CCG parser is its effect on the accuracy of downstream applications. We experimented with a supervised version of Reddy et al. (2014)’s model for questionanswering on Freebase (i.e. without using Reddy et al.’s lexicon derived from unlabelled text), using the WEBQUESTIONS dataset (Berant et al., 2013)3. The model learns to map CCG parses to database queries. We compare the performance of the QA system using both our parser and C&amp;C, taking the 10-best parses from each parser for each sentence. Syntactic question parsing models were trained from the combination of 10 copies of Rimell and Clark (2008)’s question dataset and one copy of the CCGBank The accuracy of Reddy et al. (2014)’s model varies significantly between iterati</context>
<context position="28581" citStr="Reddy et al. (2014)" startWordPosition="4733" endWordPosition="4736">to better downstream performance than the C&amp;C parser. The improvement is particularly large on oracle accuracy, increasing the upper bound on the performance of the semantic parser by around 4 points. 5 Related Work CCG parsing has been the subject of much research. We have already described the C&amp;C pars3Using the Business, Film and People domains, with 1115 questions for training and 570 for testing. 996 F1-score 100-best Oracle F1-score Iteration Iteration 2 4 6 8 10 50 48 46 44 42 40 C&amp;C EASYCCG 2 4 6 8 10 64 62 60 58 56 54 C&amp;C EASYCCG Figure 3: Question Answering accuracy per iteration of Reddy et al. (2014)’s supervised model. ing model. Kummerfeld et al. (2010) showed that the speed of the C&amp;C parser can be improved with domain-specific self-training—similar improvements may be possible applying this technique to our model. Auli and Lopez (2011a) have achieved the best CCG parsing accuracy, by allowing the parser and supertagger to perform joint inference (though there is a significant speed penalty). Auli and Lopez (2011b) were the first to use A* parsing for CCG, but their system is both much slower and less accurate than ours (due to a different model and a different A* heuristic). Krishnamu</context>
</contexts>
<marker>Reddy, Lapata, Steedman, 2014</marker>
<rawString>Siva Reddy, Mirella Lapata, and Mark Steedman. 2014. Large-scale Semantic Parsing without Question-Answer Pairs. Transactions of the Association for Computational Linguistics (to appear).</rawString>
</citation>
<citation valid="true">
<authors>
<author>Laura Rimell</author>
<author>Stephen Clark</author>
</authors>
<title>Adapting a lexicalized-grammar parser to contrasting domains.</title>
<date>2008</date>
<booktitle>In Proceedings of the Conference on Empirical Methods in Natural Language Processing,</booktitle>
<pages>475--484</pages>
<publisher>Association</publisher>
<institution>for Computational Linguistics.</institution>
<contexts>
<context position="27401" citStr="Rimell and Clark (2008)" startWordPosition="4526" endWordPosition="4529"> performance of a CCG parser is its effect on the accuracy of downstream applications. We experimented with a supervised version of Reddy et al. (2014)’s model for questionanswering on Freebase (i.e. without using Reddy et al.’s lexicon derived from unlabelled text), using the WEBQUESTIONS dataset (Berant et al., 2013)3. The model learns to map CCG parses to database queries. We compare the performance of the QA system using both our parser and C&amp;C, taking the 10-best parses from each parser for each sentence. Syntactic question parsing models were trained from the combination of 10 copies of Rimell and Clark (2008)’s question dataset and one copy of the CCGBank The accuracy of Reddy et al. (2014)’s model varies significantly between iterations of the training data. Rather than tune the number of iterations, we instead measure the accuracy after each iteration. We experimented with the models’ 1-best answers, and the oracle accuracy of their 100 best answers. The oracle accuracy gives a better indication of the performance of the parser, by mitigating errors caused by the semantic component. Results are shown in Figure 3, and demonstrate that using EASYCCG can lead to better downstream performance than t</context>
</contexts>
<marker>Rimell, Clark, 2008</marker>
<rawString>Laura Rimell and Stephen Clark. 2008. Adapting a lexicalized-grammar parser to contrasting domains. In Proceedings of the Conference on Empirical Methods in Natural Language Processing, pages 475–484. Association for Computational Linguistics.</rawString>
</citation>
<citation valid="true">
<authors>
<author>Mark Steedman</author>
</authors>
<title>The Syntactic Process.</title>
<date>2000</date>
<publisher>MIT Press.</publisher>
<contexts>
<context position="4389" citStr="Steedman, 2000" startWordPosition="679" endWordPosition="680">elewis0/easyccg 990 Proceedings of the 2014 Conference on Empirical Methods in Natural Language Processing (EMNLP), pages 990–1000, October 25-29, 2014, Doha, Qatar. c�2014 Association for Computational Linguistics accuracy—suggesting that most parsing decisions can be made accurately based on a local context window. Of course, there are many parsing decisions that can only be made accurately with more complex models. However, exploring the power and limitations of simpler models may help focus future research on the more challenging cases. 2 Background 2.1 Combinatory Categorial Grammar CCG (Steedman, 2000) is a strongly lexicalized grammatical formalism. Words have categories representing their syntactic role, which are either atomic, or functions from one category to another. Phrase-structure grammars have a relatively small number of lexical categories types (e.g. POS-tags), and a large set of rules used to build a syntactic analysis of a complete sentence (e.g. an adjective and noun can combine into a noun). In contrast, CCG parsing has many lexical category types (we use 425), but a small set of combinatory rule types (we use 10 binary and 13 unary rule schemata). This means that, aside fro</context>
<context position="15000" citStr="Steedman (2000)" startWordPosition="2477" endWordPosition="2478">cation, backward application, forward composition, backward crossed composition, generalized forward composition, generalized backward crossed composition. These combinators are posited to be linguistically universal. The generalized rules Initial Result Usage N NP Bare noun phrases NP S/(S\NP) Type NP (S\NP)/((S\NP)/NP) raising PP (S\NP)/((S\NP)/PP) Spss\NP NP\NP Reduced Sng\NP NP\NP relative Sadj\NP NP\NP clauses Sto\NP NP\NP Sto\NP N\N Sdcl/NP NP\NP Spss\NP S/S VP Sng\NP S/S Sentence Sto\NP S/S Modifiers Table 1: Set of unary rules used by the parser. are generalized to degree 2. Following Steedman (2000) and Clark and Curran (2007), backward composition is blocked where the argument of the right-hand category is an N or NP. The unhelpful [nb] feature is ignored. As in the C&amp;C parser, we add a special Conjunction rule: Y X &gt; X\X Where Y E {conj, comma, semicolon}. We block conjunctions where the right-hand category is type-raised, punctuation, N, or NP\NP. This rule (and the restrictions) could be removed by changing CCGBank to analyse conjunctions with (X\X)/X categories. We also add syntagmatic rules for removing any punctuation to the right, and for removing openbrackets and open-quotes to </context>
</contexts>
<marker>Steedman, 2000</marker>
<rawString>Mark Steedman. 2000. The Syntactic Process. MIT Press.</rawString>
</citation>
<citation valid="true">
<authors>
<author>Mark Steedman</author>
</authors>
<title>Taking Scope: The Natural Semantics of Quantifiers.</title>
<date>2012</date>
<publisher>MIT Press.</publisher>
<contexts>
<context position="14370" citStr="Steedman (2012)" startWordPosition="2390" endWordPosition="2391">.4 Grammar Here, we describe the set of combinators and unary rules in the EASYCCG grammar. Because we do not have any probabilistic model of the derivation, all rules can apply with equal probability. This means that some care needs to be taken in designing the grammar to ensure that all the rules are generally applicable. We also try to limit spurious ambiguity, and build derivations which are compatible with the C&amp;C parser’s scripts for extracting dependencies (for evaluation). We describe the grammar in detail, to ensure replicability. Our parser uses the following binary combinators from Steedman (2012): forward application, backward application, forward composition, backward crossed composition, generalized forward composition, generalized backward crossed composition. These combinators are posited to be linguistically universal. The generalized rules Initial Result Usage N NP Bare noun phrases NP S/(S\NP) Type NP (S\NP)/((S\NP)/NP) raising PP (S\NP)/((S\NP)/PP) Spss\NP NP\NP Reduced Sng\NP NP\NP relative Sadj\NP NP\NP clauses Sto\NP NP\NP Sto\NP N\N Sdcl/NP NP\NP Spss\NP S/S VP Sng\NP S/S Sentence Sto\NP S/S Modifiers Table 1: Set of unary rules used by the parser. are generalized to degre</context>
</contexts>
<marker>Steedman, 2012</marker>
<rawString>Mark Steedman. 2012. Taking Scope: The Natural Semantics of Quantifiers. MIT Press.</rawString>
</citation>
<citation valid="true">
<authors>
<author>Emily Thomforde</author>
<author>Mark Steedman</author>
</authors>
<title>Semisupervised CCG lexicon extension.</title>
<date>2011</date>
<booktitle>In Proceedings of the Conference on Empirical Methods in Natural Language Processing,</booktitle>
<pages>1246--1256</pages>
<publisher>Association</publisher>
<institution>for Computational Linguistics.</institution>
<contexts>
<context position="32608" citStr="Thomforde and Steedman, 2011" startWordPosition="5380" endWordPosition="5383">f (2003)’s multi-modal extensions to limit over-generation. Honnibal and Curran (2009) show how unary rules can be lexicalized in CCG. Such improvements may improve both the speed and accuracy of our model. Because our parser is factored on a unigram tagging model, it can be trained from isolated annotated words, and does not require annotated parse trees or full sentences. Reducing the requirements for training data eases the task for human annotators. It may also make the model more amenable to semi-supervised approaches to CCG parsing, which have typically focused on extending the lexicon (Thomforde and Steedman, 2011; Deoskar et al., 2014). Finally, it may make it easier to convert other annotated resources, such as UCCA (Abend and Rappoport, 2013) or AMR (Banarescu et al., 2013), to CCG training data—as only specific words need to be converted, rather than full sentences. Our model is weak at certain kinds of decisions, e.g. coordination-scope ambiguities or nonlocal attachments. Incorporating specific models for such decisions may improve accuracy, while still allowing fast and exact search—for example, we intend to try including Coppola et al. (2011)’s model for prepositional phrase attachment. 7 Concl</context>
</contexts>
<marker>Thomforde, Steedman, 2011</marker>
<rawString>Emily Thomforde and Mark Steedman. 2011. Semisupervised CCG lexicon extension. In Proceedings of the Conference on Empirical Methods in Natural Language Processing, pages 1246–1256. Association for Computational Linguistics.</rawString>
</citation>
<citation valid="true">
<authors>
<author>Stephen Tratz</author>
<author>Eduard Hovy</author>
</authors>
<title>A fast, effective, non-projective, semantically-enriched parser.</title>
<date>2011</date>
<booktitle>In Proceedings of EMNLP.</booktitle>
<contexts>
<context position="31299" citStr="Tratz and Hovy, 2011" startWordPosition="5168" endWordPosition="5171">-the-art results using the same feature set and a simpler model by exploiting CCG’s lexicalized nature, which makes it more natural to delegate parsing decisions to a tagging model. Other parsing research has focused on building fast parsers for web-scale processing, typically using dependency grammars (e.g. Nivre (2003)). CCG has some advantages over dependency grammars, such as supporting surface-compositional semantics. The fastest dependency parsers use an easyfirst strategy, in which edges are added greedily in order of their score, with 0(nlog(n)) complexity (Goldberg and Elhadad, 2010; Tratz and Hovy, 2011). This strategy is reminiscent of our A* search, which expands the chart in a bestfirst order. A* has higher asymptotic complexity, but finds a globally optimal solution. 6 Future Work We believe that our model opens several interesting directions for future research. One interesting angle would be to increase the amount of information in CCGBank’s lexical entries, to further reduce the search space for the parser. For example, PP categories could be distinguished with the relevant preposition as a feature; punctuation and coordination could be given more detailed categories to avoid needing t</context>
</contexts>
<marker>Tratz, Hovy, 2011</marker>
<rawString>Stephen Tratz and Eduard Hovy. 2011. A fast, effective, non-projective, semantically-enriched parser. In Proceedings of EMNLP.</rawString>
</citation>
<citation valid="true">
<authors>
<author>Joseph Turian</author>
<author>Lev Ratinov</author>
<author>Yoshua Bengio</author>
</authors>
<title>Word representations: a simple and general method for semi-supervised learning.</title>
<date>2010</date>
<booktitle>In Proceedings of the 48th Annual Meeting of the Association for Computational Linguistics,</booktitle>
<pages>384--394</pages>
<publisher>Association</publisher>
<institution>for Computational Linguistics.</institution>
<contexts>
<context position="8509" citStr="Turian et al. (2010)" startWordPosition="1365" endWordPosition="1368">tside context, rather than the actual sentence. We introduce a new heuristic which gives a tighter upper bound on the outside probability. 3 Model 3.1 Lexical Category Model As input, our parser takes a distribution over all CCG lexical categories for each word in the sentence. These distributions are assigned using Lewis and Steedman (2014)’s semi-supervised supertagging model. The supertagger is a unigram log-linear classifier that uses features of the ±3 word context window surrounding a word. The 991 key feature is word embeddings, initialized with the 50-dimensional embeddings trained in Turian et al. (2010), and fine-tuned during supervised training. The model also uses 2-character suffixes and capitalization features. The use of word embeddings, which are trained on a large unlabelled corpus, allows the supertagger to generalize well to words not present in the labelled data. It does not use a POS-tagger, which avoids problems caused by POS-tagging errors. Our methods could be applied to any supertagging model, but we find empirically that this model gives higher performance than the C&amp;C supertagger. 3.2 Parsing Model Let a CCG parse y of a sentence S be a list of lexical categories cl ... cn a</context>
</contexts>
<marker>Turian, Ratinov, Bengio, 2010</marker>
<rawString>Joseph Turian, Lev Ratinov, and Yoshua Bengio. 2010. Word representations: a simple and general method for semi-supervised learning. In Proceedings of the 48th Annual Meeting of the Association for Computational Linguistics, pages 384–394. Association for Computational Linguistics.</rawString>
</citation>
<citation valid="true">
<authors>
<author>Wenduan Xu</author>
<author>Stephen Clark</author>
<author>Yue Zhang</author>
</authors>
<title>Shift-reduce ccg parsing with a dependency model.</title>
<date>2014</date>
<booktitle>In Proceedings of the 52nd Annual Meeting of the Association for Computational Linguistics (ACL 2014). Association for Computational Linguistics,</booktitle>
<contexts>
<context position="29490" citStr="Xu et al. (2014)" startWordPosition="4886" endWordPosition="4889"> allowing the parser and supertagger to perform joint inference (though there is a significant speed penalty). Auli and Lopez (2011b) were the first to use A* parsing for CCG, but their system is both much slower and less accurate than ours (due to a different model and a different A* heuristic). Krishnamurthy and Mitchell (2014) show how CCG parsing can be improved by jointly modelling the syntax and semantics. Fowler and Penn (2010) apply the Petrov parser to CCG, making a small improvement in accuracy over the C&amp;C parser, at the cost of a 300-fold speed decrease. Zhang and Clark (2011) and Xu et al. (2014) explored shift-reduce CCG parsing, but despite the use of a linear-time algorithm, parsing speed in practice is significantly slower than the C&amp;C parser. Parsers based on supertagging models have previously been applied to other strongly lexicalized formalisms, such as to LTAG (Bangalore and Joshi, 1999) and to HPSG (Ninomiya et al., 2006). A major contribution of our work over these is showing that factoring models on lexical categories allows fast and exact A* parsing, without the need for beam search. Our parsing approach could be applied to any strongly lexicalized formalism. Our work fit</context>
</contexts>
<marker>Xu, Clark, Zhang, 2014</marker>
<rawString>Wenduan Xu, Stephen Clark, and Yue Zhang. 2014. Shift-reduce ccg parsing with a dependency model. In Proceedings of the 52nd Annual Meeting of the Association for Computational Linguistics (ACL 2014). Association for Computational Linguistics, June.</rawString>
</citation>
<citation valid="true">
<authors>
<author>Yue Zhang</author>
<author>Stephen Clark</author>
</authors>
<title>Shift-reduce CCG parsing.</title>
<date>2011</date>
<booktitle>In Proceedings of the 49th Annual Meeting of the Association for Computational Linguistics: Human Language Technologies-Volume 1,</booktitle>
<pages>683--692</pages>
<publisher>Association</publisher>
<institution>for Computational Linguistics.</institution>
<contexts>
<context position="29469" citStr="Zhang and Clark (2011)" startWordPosition="4881" endWordPosition="4884">st CCG parsing accuracy, by allowing the parser and supertagger to perform joint inference (though there is a significant speed penalty). Auli and Lopez (2011b) were the first to use A* parsing for CCG, but their system is both much slower and less accurate than ours (due to a different model and a different A* heuristic). Krishnamurthy and Mitchell (2014) show how CCG parsing can be improved by jointly modelling the syntax and semantics. Fowler and Penn (2010) apply the Petrov parser to CCG, making a small improvement in accuracy over the C&amp;C parser, at the cost of a 300-fold speed decrease. Zhang and Clark (2011) and Xu et al. (2014) explored shift-reduce CCG parsing, but despite the use of a linear-time algorithm, parsing speed in practice is significantly slower than the C&amp;C parser. Parsers based on supertagging models have previously been applied to other strongly lexicalized formalisms, such as to LTAG (Bangalore and Joshi, 1999) and to HPSG (Ninomiya et al., 2006). A major contribution of our work over these is showing that factoring models on lexical categories allows fast and exact A* parsing, without the need for beam search. Our parsing approach could be applied to any strongly lexicalized fo</context>
</contexts>
<marker>Zhang, Clark, 2011</marker>
<rawString>Yue Zhang and Stephen Clark. 2011. Shift-reduce CCG parsing. In Proceedings of the 49th Annual Meeting of the Association for Computational Linguistics: Human Language Technologies-Volume 1, pages 683–692. Association for Computational Linguistics.</rawString>
</citation>
</citationList>
</algorithm>
</algorithms>
<?xml version="1.0" encoding="UTF-8"?>
<algorithms version="110505">
<algorithm name="SectLabel" version="110505">
<variant no="0" confidence="0.000046">
<title confidence="0.995815">
A Model of Coherence Based on Distributed Sentence Representation
</title>
<author confidence="0.996539">
Jiwei Li&apos; and Eduard Hovy3
</author>
<affiliation confidence="0.9991495">
&apos;Computer Science Department, Stanford University, Stanford, CA 94305, USA
3Language Technology Institute, Carnegie Mellon University, Pittsburgh, PA 15213, USA
</affiliation>
<email confidence="0.993268">
jiweil@stanford.edu ehovy@andrew.cmu.edu
</email>
<sectionHeader confidence="0.993757" genericHeader="abstract">
Abstract
</sectionHeader>
<bodyText confidence="0.999731846153846">
Coherence is what makes a multi-sentence
text meaningful, both logically and syn-
tactically. To solve the challenge of or-
dering a set of sentences into coherent or-
der, existing approaches focus mostly on
defining and using sophisticated features
to capture the cross-sentence argumenta-
tion logic and syntactic relationships. But
both argumentation semantics and cross-
sentence syntax (such as coreference and
tense rules) are very hard to formalize. In
this paper, we introduce a neural network
model for the coherence task based on
distributed sentence representation. The
proposed approach learns a syntactico-
semantic representation for sentences au-
tomatically, using either recurrent or re-
cursive neural networks. The architecture
obviated the need for feature engineering,
and learns sentence representations, which
are to some extent able to capture the
‘rules’ governing coherent sentence struc-
ture. The proposed approach outperforms
existing baselines and generates the state-
of-art performance in standard coherence
evaluation tasks1.
</bodyText>
<sectionHeader confidence="0.99913" genericHeader="introduction">
1 Introduction
</sectionHeader>
<bodyText confidence="0.9997158">
Coherence is a central aspect in natural language
processing of multi-sentence texts. It is essen-
tial in generating readable text that the text plan-
ner compute which ordering of clauses (or sen-
tences; we use them interchangeably in this paper)
is likely to support understanding and avoid con-
fusion. As Mann and Thompson (1988) define it,
A text is coherent when it can be ex-
plained what role each clause plays with
regard to the whole.
</bodyText>
<footnote confidence="0.7331975">
1Code available at stanford.edu/˜jiweil/ or by
request from the first author.
</footnote>
<bodyText confidence="0.999832219512195">
Several researchers in the 1980s and 1990s ad-
dressed the problem, the most influential of
which include: Rhetorical Structure Theory (RST;
(Mann and Thompson, 1988)), which defined
about 25 relations that govern clause interde-
pendencies and ordering and give rise to text
tree structures; the stepwise assembly of seman-
tic graphs to support adductive inference toward
the best explanation (Hobbs et al., 1988); Dis-
course Representation Theory (DRT; (Lascarides
and Asher, 1991)), a formal semantic model of
discourse contexts that constrain coreference and
quantification scoping; the model of intention-
oriented conversation blocks and their stack-based
queueing to model attention flow (Grosz and Sid-
ner, 1986), and more recently an inventory of a
hundred or so binary inter-clause relations and as-
sociated annotated corpus (Penn Discourse Tree-
bank. Work in text planning implemented some
of these models, especially operationalized RST
(Hovy, 1988) and explanation relations (Moore
and Paris, 1989) to govern the planning of coher-
ent paragraphs. Other computational work defined
so called schemas (McKeown, 1985), frames with
fixed sequences of clause types to achieve stereo-
typical communicative intentions.
Little of this work survives. Modern research
tries simply to order a collection of clauses or sen-
tences without giving an account of which order(s)
is/are coherent or what the overall text structure
is. The research focuses on identifying and defin-
ing a set of increasingly sophisticated features by
which algorithms can be trained to propose order-
ings. Features being explored include the clause
entities, organized into a grid (Lapata and Barzi-
lay, 2005; Barzilay and Lapata, 2008), coreference
clues to ordering (Elsner and Charniak, 2008),
named-entity categories (Eisner and Charniak,
2011), syntactic features (Louis and Nenkova,
2012), and others. Besides being time-intensive
(feature engineering usually requites considerable
</bodyText>
<page confidence="0.965734">
2039
</page>
<note confidence="0.991016">
Proceedings of the 2014 Conference on Empirical Methods in Natural Language Processing (EMNLP), pages 2039–2048,
October 25-29, 2014, Doha, Qatar. c�2014 Association for Computational Linguistics
</note>
<figureCaption confidence="0.999937">
Figure 1: Illustrations of coherent (positive) vs not-coherent (negative) training examples.
</figureCaption>
<bodyText confidence="0.999961657894737">
effort and can depend greatly on upstream feature
extraction algorithms), it is not immediately ap-
parent which aspects of a clause or a coherent text
to consider when deciding on ordering. More im-
portantly, the features developed to date are still
incapable of fully specifying the acceptable order-
ing(s) within a context, let alone describe why they
are coherent.
Recently, deep architectures, have been applied
to various natural language processing tasks (see
Section 2). Such deep connectionist architectures
learn a dense, low-dimensional representation of
their problem in a hierarchical way that is capa-
ble of capturing both semantic and syntactic as-
pects of tokens (e.g., (Bengio et al., 2006)), en-
tities, N-grams (Wang and Manning, 2012), or
phrases (Socher et al., 2013). More recent re-
searches have begun looking at higher level dis-
tributed representations that transcend the token
level, such as sentence-level (Le and Mikolov,
2014) or even discourse-level (Kalchbrenner and
Blunsom, 2013) aspects. Just as words combine
to form meaningful sentences, can we take advan-
tage of distributional semantic representations to
explore the composition of sentences to form co-
herent meanings in paragraphs?
In this paper, we demonstrate that it is feasi-
ble to discover the coherent structure of a text
using distributed sentence representations learned
in a deep learning framework. Specifically, we
consider a WINDOW approach for sentences, as
shown in Figure 1, where positive examples are
windows of sentences selected from original arti-
cles generated by humans, and negatives examples
are generated by random replacements2. The se-
mantic representations for terms and sentences are
obtained through optimizing the neural network
framework based on these positive vs negative ex-
</bodyText>
<footnote confidence="0.9894238">
2Our approach is inspired by Collobert et al.’s idea (2011)
that a word and its context form a positive training sample
while a random word in that same context gives a negative
training sample, when training word embeddings in the deep
learning framework.
</footnote>
<bodyText confidence="0.999694777777778">
amples and the proposed model produces state-of-
art performance in multiple standard evaluations
for coherence models (Barzilay and Lee, 2004).
The rest of this paper is organized as follows:
We describe related work in Section 2, then de-
scribe how to obtain a distributed representation
for sentences in Section 3, and the window compo-
sition in Section 4. Experimental results are shown
in Section 5, followed by a conclusion.
</bodyText>
<sectionHeader confidence="0.999781" genericHeader="related work">
2 Related Work
</sectionHeader>
<bodyText confidence="0.999693258064516">
Coherence In addition to the early computa-
tional work discussed above, local coherence was
extensively studied within the modeling frame-
work of Centering Theory (Grosz et al., 1995;
Walker et al., 1998; Strube and Hahn, 1999; Poe-
sio et al., 2004), which provides principles to form
a coherence metric (Miltsakaki and Kukich, 2000;
Hasler, 2004). Centering approaches suffer from a
severe dependence on manually annotated input.
A recent popular approach is the entity grid
model introduced by Barzilay and Lapata (2008)
, in which sentences are represented by a vec-
tor of discourse entities along with their gram-
matical roles (e.g., subject or object). Proba-
bilities of transitions between adjacent sentences
are derived from entity features and then concate-
nated to a document vector representation, which
is used as input to machine learning classifiers
such as SVM. Many frameworks have extended
the entity approach, for example, by pre-grouping
entities based on semantic relatedness (Filippova
and Strube, 2007) or adding more useful types
of features such as coreference (Elsner and Char-
niak, 2008), named entities (Eisner and Charniak,
2011), and discourse relations (Lin et al., 2011).
Other systems include the global graph model
(Guinaudeau and Strube, 2013) which projects en-
tities into a global graph. Louis and Nenkova
(2012) introduced an HMM system in which the
coherence between adjacent sentences is modeled
by a hidden Markov framework captured by the
</bodyText>
<page confidence="0.993566">
2040
</page>
<figureCaption confidence="0.992943333333333">
Figure 2: Sentential compositionality obtained from (a) recurrent / (b) recursive neural network. The
bottom layer represents word vectors in the sentence. The top layer hs denotes the resulting sentence
vector.
</figureCaption>
<bodyText confidence="0.98417093939394">
transition rules of different topics.
Recurrent and Recursive Neural Networks In
the context of NLP, recurrent neural networks
view a sentence as a sequence of tokens and in-
corporate information from the past (i.e., preced-
ing tokens) (Schuster and Paliwal, 1997; Sutskever
et al., 2011) for acquisition of the current output.
At each step, the recurrent network takes as input
both the output of previous steps and the current
token, convolutes the inputs, and forwards the re-
sult to the next step. It has been successfully ap-
plied to tasks such as language modeling (Mikolov
et al., 2010) and spoken language understanding
(Mesnil et al., 2013). The advantage of recur-
rent network is that it does not depend on exter-
nal deeper structure (e.g., parse tree) and is easy to
implement. However, in the recurrent framework,
long-distance dependencies are difficult to capture
due to the vanishing gradient problem (Bengio et
al., 1994); two tokens may be structurally close to
each other, even though they are far away in word
sequence3.
Recursive neural networks comprise another
class of architecture, one that relies and operates
on structured inputs (e.g., parse trees). It com-
putes the representation for each parent based on
its children iteratively in a bottom-up fashion. A
series of variations have been proposed, each tai-
lored to different task-specific requirements, such
as Matrix-Vector RNN (Socher et al., 2012) that
represents every word as both a vector and a ma-
trix, or Recursive Neural Tensor Networks (Socher
et al., 2013) that allow the model to have greater
</bodyText>
<footnote confidence="0.97541325">
3For example, a verb and its corresponding direct object
can be far away in terms of tokens if many adjectives lies in
between, but they are adjacent in the parse tree (Irsoy and
Cardie, 2013).
</footnote>
<bodyText confidence="0.9981144">
interactions between the input vectors. Many tasks
have benefited from this recursive framework, in-
cluding parsing (Socher et al., 2011b), sentiment
analysis (Socher et al., 2013), and paraphrase de-
tection (Socher et al., 2011a).
</bodyText>
<subsectionHeader confidence="0.961006">
2.1 Distributed Representations
</subsectionHeader>
<bodyText confidence="0.999464952380952">
Both recurrent and recursive networks require a
vector representation of each input token. Dis-
tributed representations for words were first pro-
posed in (Rumelhart et al., 1988) and have been
successful for statistical language modeling (El-
man, 1990). Various deep learning architectures
have been explored to learn these embeddings in
an unsupervised manner from a large corpus (Ben-
gio et al., 2006; Collobert and Weston, 2008;
Mnih and Hinton, 2007; Mikolov et al., 2013),
which might have different generalization capabil-
ities and are able to capture the semantic mean-
ings depending on the specific task at hand. These
vector representations can to some extent cap-
ture interesting semantic relationships, such as
King−man ≈ Queue−woman (Mikolov et al.,
2010), and recently have been successfully used
in various NLP applications, including named en-
tity recognition, tagging, segmentation (Wang et
al., 2013), and machine translation (e.g.,(Collobert
and Weston, 2008; Zou et al., 2013)).
</bodyText>
<sectionHeader confidence="0.998504" genericHeader="method">
3 Sentence Model
</sectionHeader>
<bodyText confidence="0.999658857142857">
In this section, we demonstrate the strategy
adopted to compute a vector for a sentence given
the sequence of its words and their embeddings.
We implemented two approaches, Recurrent and
Recursive neural networks, following the de-
scriptions in for example (Mikolov et al., 2010;
Sutskever et al., 2011; Socher et al., 2013). As
</bodyText>
<page confidence="0.951297">
2041
</page>
<bodyText confidence="0.999731909090909">
the details of both approaches can be readily found
there, we make this section brief and omit the de-
tails for brevity.
Let s denote a sentence, comprised of a se-
quence of words s = {w1, w2,..., wns}, where ns
denotes the number of words within sentence s.
Each word w is associated with a specific vector
embedding e, = {e1w, e2 w, ..., eKw }, where K de-
notes the dimension of the word embedding. We
wish to compute the vector representation for cur-
rent sentence hs = {h1s, h2s, ..., hKs }.
</bodyText>
<subsectionHeader confidence="0.567937">
Recurrent Sentence Representation (Recur-
</subsectionHeader>
<bodyText confidence="0.999726888888889">
rent) The recurrent network captures certain
general considerations regarding sentential com-
positionality. As shown in Figure 2 (a), for sen-
tence s, recurrent network successively takes word
wi at step i, combines its vector representation etw
with former input hi−1 from step i − 1, calculates
the resulting current embedding ht, and passes it
to the next step. The standard recurrent network
calculates ht as follows:
</bodyText>
<equation confidence="0.9433325">
h1 = f(VRecurrent&apos;h0+WRecurrent&apos;e1w+bRecurrent)
(2)
</equation>
<bodyText confidence="0.971826928571429">
where h0 denotes the global sentence starting vec-
tor.
Recursive Sentence Representation (Recursive)
Recursive sentence representation relies on the
structure of parse trees, where each leaf node of
the tree corresponds to a word from the original
sentence. It computes a representation for each
parent node based on its immediate children re-
cursively in a bottom-up fashion until reaching the
root of the tree. Concretely, for a given parent p
in the tree and its two children c1 (associated with
vector representation hc1) and c2 (associated with
vector representation hc2), standard recursive net-
works calculates hp for p as follows:
</bodyText>
<equation confidence="0.82931">
hp = f(WRecursive &apos; [hc1, hc2] + bRecursive) (3)
</equation>
<bodyText confidence="0.999874125">
where [hc1, hc2] denotes the concatenating vec-
tor for children vector representation hc1 and hc2.
WRecursive is a K x 2K matrix and bRecursive is
the 1 x K bias vector. f(&apos;) is tanh function.
Recursive neural models compute parent vec-
tors iteratively until the root node’s representation
is obtained, and use the root embedding to repre-
sent the whole sentence, as shown in Figure 2 (b).
</bodyText>
<sectionHeader confidence="0.997013" genericHeader="method">
4 Coherence Model
</sectionHeader>
<bodyText confidence="0.9999605">
The proposed coherence model adopts a window
approach (Collobert et al., 2011), in which we
train a three-layer neural network based on a slid-
ing windows of L sentences.
</bodyText>
<subsectionHeader confidence="0.997417">
4.1 Sentence Convolution
</subsectionHeader>
<bodyText confidence="0.99973764">
We treat a window of sentences as a clique C and
associate each clique with a tag yC that takes the
value 1 if coherent, and 0 otherwise4. As shown in
Figure 1, cliques taken from original articles are
treated as coherent and those with sentences ran-
domly replaced are used as negative examples. .
The sentence convolution algorithm adopted in
this paper is defined by a three-layer neural net-
work, i.e., sentence-level input layer, hidden layer,
and overall output layer as shown in Figure 3. For-
mally, each clique C takes as input a (L x K) x 1
vector hC by concatenating the embeddings of
all its contained sentences, denoted as hC =
[hs1, hs2, ..., hsL]. (Note that if we wish to clas-
sify the first and last sentences and include their
context, we require special beginning and ending
sentence vectors, which are defined as h&lt;S&gt; for
sstart and h&lt;/S&gt; for send respectively.)
Let H denote the number of neurons in the hid-
den (second) layer. Then each of the hidden lay-
ers takes as input hC and performs the convolution
using a non-linear tanh function, parametrized by
Wsen and bsen. The concatenating output vector
for hidden layers, defined as qC, can therefore be
rewritten as:
</bodyText>
<equation confidence="0.996507">
qC = f(Wsen x hC + bsen) (4)
</equation>
<bodyText confidence="0.998172916666667">
where Wsen is a H x (L x K) dimensional matrix
and bsen is a H x 1 dimensional bias vector.
4instead of a binary classification (correct/incorrect), an-
other commonly used approach is the contrastive approach
that minimizes the score function max(0, 1 − s + s.) (Col-
lobert et al., 2011; Smith and Eisner, 2005). s denotes the
score of a true (coherent) window and s. the score of a cor-
rupt (containing incoherence) one) in an attempt to make the
score of true windows larger and corrupt windows smaller.
We tried the contrastive one for both recurrent and recursive
networks but the binary approach constantly outperformed
the contrastive one in this task.
</bodyText>
<equation confidence="0.9988845">
ht = f(VRecurrent&apos;ht−1+WRecurrent&apos;etw+bRecurrent)
(1)
</equation>
<bodyText confidence="0.9958838">
where WRecurrent and VRecurrent are K x K ma-
trixes. bRecurrent denotes K x 1 bias vector and
f = tanh is a standard element-wise nonlinearity.
Note that calculation for representation at time
t = 1 is given by:
</bodyText>
<page confidence="0.992991">
2042
</page>
<figureCaption confidence="0.999262">
Figure 3: An example of coherence model based on a window of sentences (clique).
</figureCaption>
<bodyText confidence="0.999605">
The output layer takes as input qC and generates
a scalar using linear function UT qC +b. A sigmod
function is then adopted to project the value to a
[0,1] probability space, which can be interpreted
as the probability of whether one clique is coher-
ent or not. The execution at the output layer can
be summarized as:
</bodyText>
<equation confidence="0.999886">
p(yC = 1) = sigmod(UTqC + b) (5)
</equation>
<bodyText confidence="0.998167">
where U is an H x1 vector and b denotes the bias.
</bodyText>
<subsectionHeader confidence="0.915573">
4.2 Training
</subsectionHeader>
<bodyText confidence="0.999902">
In the proposed framework, suppose we have M
training samples, the cost function for recurrent
neural network with regularization on the training
set is given by:
</bodyText>
<equation confidence="0.9958795">
1 �
J(Θ) =
M
CEtrainset
�
− (1 − yC) log[1 − p(yC = 1)]} + Q
2M
BEO
</equation>
<bodyText confidence="0.706517">
where
</bodyText>
<equation confidence="0.962431">
Θ = [WRecurrent, Wsen, Usen]
</equation>
<bodyText confidence="0.999881857142857">
The regularization part is paralyzed by Q to avoid
overfitting. A similar loss function is applied to
the recursive network with only minor parameter
altering that is excluded for brevity.
To minimize the objective J(Θ), we use the di-
agonal variant of AdaGrad (Duchi et al., 2011)
with minibatches, which is widely applied in deep
learning literature (e.g.,(Socher et al., 2011a; Pei
et al., 2014)). The learning rate in AdaGrad is
adapting differently for different parameters at dif-
ferent steps. Concretely, for parameter updates, let
giτ denote the subgradient at time step for param-
eter θi, which is obtained from backpropagation5,
the parameter update at time step t is given by:
</bodyText>
<equation confidence="0.978948666666667">
(
θτ = θτ−1 α �τ V i2 gτi l7)
t=o gτ
</equation>
<bodyText confidence="0.999917666666667">
where α denotes the learning rate and is set to 0.01
in our approach. Optimal performance is achieved
when batch size is set between 20 and 30.
</bodyText>
<subsectionHeader confidence="0.984322">
4.3 Initialization
</subsectionHeader>
<bodyText confidence="0.9621235">
Elements in Wsen are initialized by randomly
drawing from the uniform distribution [−�, �],
\/6
where � = \/H+KxL as suggested in (Collobert
et al., 2011). Wrecurrent, Vrecurrent, Wrecursive
and h0 are initialized by randomly sampling from
a uniform distribution U(−0.2,0.2). All bias vec-
tors are initialized with 0. Hidden layer number H
is set to 100.
Word embeddings {e} are borrowed from
Senna (Collobert et al., 2011; Collobert, 2011).
The dimension for these embeddings is 50.
</bodyText>
<sectionHeader confidence="0.999611" genericHeader="method">
5 Experiments
</sectionHeader>
<bodyText confidence="0.999207166666667">
We evaluate the proposed coherence model on two
common evaluation approaches adopted in exist-
ing work (Barzilay and Lapata, 2008; Louis and
Nenkova, 2012; Elsner et al., 2007; Lin et al.,
2011): Sentence Ordering and Readability Assess-
ment.
</bodyText>
<subsectionHeader confidence="0.993678">
5.1 Sentence Ordering
</subsectionHeader>
<bodyText confidence="0.989075">
We follow (Barzilay and Lapata, 2008; Louis and
Nenkova, 2012; Elsner et al., 2007; Lin et al.,
</bodyText>
<footnote confidence="0.9728275">
5For more details on backpropagation through RNNs, see
Socher et al. (2010).
</footnote>
<equation confidence="0.957015666666667">
{−yC log[p(yC = 1)]
θ2
(6)
</equation>
<page confidence="0.875121">
2043
</page>
<bodyText confidence="0.9996064375">
2011) that all use pairs of articles, one containing
the original document order and the other a ran-
dom permutation of the sentences from the same
document. The pairwise approach is predicated
on the assumption that the original article is al-
ways more coherent than a random permutation;
this assumption has been verified in Lin et al.’s
work (2011).
We need to define the coherence score Sd for
a given document d, where d is comprised of a
series of sentences, d = {s1, s2,.., sNd}, and Nd
denotes the number of sentences within d. Based
on our clique definition, document d is comprised
of Nd cliques. Taking window size L = 3 as ex-
ample, cliques generated from document d appear
as follows:
</bodyText>
<equation confidence="0.9775665">
&lt; sstart, s1, s2 &gt;, &lt; s1, s2, s3 &gt;, ...,
&lt; sNd−2, sNd−1, sNd &gt;, &lt; sNd−1, sNd, send &gt;
</equation>
<bodyText confidence="0.999420666666667">
The coherence score for a given document Sd is
the probability that all cliques within d are coher-
ent, which is given by:
</bodyText>
<equation confidence="0.983563">
�Sd = P(YC = 1) (8)
CEd
</equation>
<bodyText confidence="0.997312333333333">
For document pair &lt; d1, d2 &gt; in our task, we
would say document d1 is more coherent than d2
if
</bodyText>
<equation confidence="0.918626">
Sd1 &gt; Sd2 (9)
</equation>
<sectionHeader confidence="0.545434" genericHeader="method">
5.1.1 Dataset
</sectionHeader>
<bodyText confidence="0.999944176470588">
We use two corpora that are widely employed
for coherence prediction (Barzilay and Lee, 2004;
Barzilay and Lapata, 2008; Elsner et al., 2007).
One contains reports on airplane accidents from
the National Transportation Safety Board and the
other contains reports about earthquakes from the
Associated Press. These articles are about 10
sentences long and usually exhibit clear sentence
structure. For preprocessing, we only lowercase
the capital letters to match with tokens in Senna
word embeddings. In the recursive network, sen-
tences are parsed using the Stanford Parser6 and
then transformed into binary trees. The accident
corpus ends up with a vocabulary size of 4758 and
an average of 10.6 sentences per document. The
earthquake corpus contains 3287 distinct terms
and an average of 11.5 sentences per document.
</bodyText>
<footnote confidence="0.808142">
6http://nlp.stanford.edu/software/
lex-parser.shtml
</footnote>
<bodyText confidence="0.999838384615385">
For each of the two corpora, we have 100 arti-
cles for training and 100 (accidents) and 99 (earth-
quakes) for testing. A maximum of 20 random
permutations were generated for each test arti-
cle to create the pairwise data (total of 1986 test
pairs for the accident corpus and 1956 for earth-
quakes)7.
Positive cliques are taken from original training
documents. For easy training, rather than creating
negative examples by replacing centered sentences
randomly, the negative dataset contains cliques
where centered sentences are replaced only by
other sentences within the same document.
</bodyText>
<subsubsectionHeader confidence="0.810431">
5.1.2 Training and Testing
</subsubsectionHeader>
<bodyText confidence="0.999982090909091">
Despite the numerous parameters in the deep
learning framework, we tune only two principal
ones for each setting: window size L (tried on
{3, 5, 7}) and regularization parameter Q (tried on
{0.01, 0.1, 0.25, 0.5,1.0,1.25, 2.0, 2.5, 5.0}). We
trained parameters using 10-fold cross-validation
on the training data. Concretely, in each setting,
90 documents were used for training and evalua-
tion was done on the remaining articles, following
(Louis and Nenkova, 2012). After tuning, the final
model was tested on the testing set.
</bodyText>
<subsectionHeader confidence="0.54395">
5.1.3 Model Comparison
</subsectionHeader>
<bodyText confidence="0.99950775">
We report performance of recursive and recurrent
networks. We also report results from some popu-
lar approaches in the literature, including:
Entity Grid Model : Grid model (Barzilay and
Lapata, 2008) obtains the best performance when
coreference resolution, expressive syntactic infor-
mation, and salience-based features are incorpo-
rated. Entity grid models represent each sentence
as a column of a grid of features and apply ma-
chine learning methods (e.g., SVM) to identify the
coherent transitions based on entity features (for
details of entity models see (Barzilay and Lapata,
2008)). Results are directly taken from Barzilay
and Lapata’s paper (2008).
HMM : Hidden-Markov approach proposed by
Louis and Nenkova (2012) to model the state
(cluster) transition probability in the coherent con-
text using syntactic features. Sentences need to be
clustered in advance where the number of clus-
ters is tuned as a parameter. We directly take
</bodyText>
<footnote confidence="0.997353666666667">
7Permutations are downloaded from http:
//people.csail.mit.edu/regina/coherence/
CLsubmission/.
</footnote>
<page confidence="0.976467">
2044
</page>
<table confidence="0.99980825">
Acci Earthquake Average
Recursive 0.864 0.976 0.920
Recurrent 0.840 0.951 0.895
Entity Grid 0.904 0.872 0.888
HMM 0.822 0.938 0.880
HMM+Entity 0.842 0.911 0.877
HMM+Content 0.742 0.953 0.848
Graph 0.846 0.635 0.740
</table>
<tableCaption confidence="0.999183">
Table 1: Comparison of Different Coherence
</tableCaption>
<bodyText confidence="0.98510845">
Frameworks. Reported baseline results are among
the best performance regarding each approach is
reprinted from prior work from (Barzilay and Lap-
ata, 2008; Louis and Nenkova, 2012; Guinaudeau
and Strube, 2013).
the results from Louis and Nenkova’s paper and
report the best results among different combi-
nations of parameter and feature settings8. We
also report performances of models from Louis
and Nenkova’s work that combine HMM and en-
tity/content models in a unified framework.
Graph Based Approach : Guinaudeau and
Strube (2013) extended the entity grid model to
a bipartite graph representing the text, where the
entity transition information needed for local co-
herence computation is embedded in the bipartite
graph. The Graph Based Approach outperforms
the original entity approach in some of feature set-
tings (Guinaudeau and Strube, 2013).
As can be seen in Table 1, the proposed frame-
works (both recurrent and recursive) obtain state-
of-art performance and outperform all existing
baselines by a large margin. One interpretation
is that the abstract sentence vector representations
computed by the deep learning framework is more
powerful in capturing exactly the relevant the se-
mantic/logical/syntactic features in coherent con-
texts than features or other representations devel-
oped by human feature engineering are.
Another good quality of the deep learning
framework is that it can be trained easily and
makes unnecessary the effort required of feature
engineering. In contrast, almost all existing base-
lines and other coherence methods require sophis-
ticated feature selection processes and greatly rely
on external feature extraction algorithm.
The recurrent network is easier to implement
than the recursive network and does not rely on
external resources (i.e., parse trees), but the recur-
sive network obtains better performance by build-
</bodyText>
<footnote confidence="0.630737">
8The details for information about parameter and feature
of best setting can be found in (Louis and Nenkova, 2012).
</footnote>
<bodyText confidence="0.999256545454545">
ing the convolution on parse trees rather than sim-
ply piling up terms within the sentence, which is
in line with common expectation.
Both recurrent and recursive models obtain bet-
ter performance on the Earthquake than the Acci-
dent dataset. Scrutiny of the corpus reveals that
articles reporting earthquakes exhibit a more con-
sistent structure: earthquake outbreak, describing
the center and intensity of the earthquake, injuries
and rescue operations, etc., while accident articles
usually exhibit more diverse scenarios.
</bodyText>
<subsectionHeader confidence="0.999085">
5.2 Readability Assessment
</subsectionHeader>
<bodyText confidence="0.999968">
Barzilay and Lapata (2008) proposed a readability
assessment task for stylistic judgments about the
difficulty of reading a document. Their approach
combines a coherence system with Schwarm and
Ostendorf’s (2005) readability features to clas-
sify documents into two categories, more read-
able (coherent) documents and less readable ones.
The evaluation accesses the ability to differentiate
“easy to read” documents from difficult ones of
each model.
</bodyText>
<subsectionHeader confidence="0.748064">
5.2.1 Dataset
</subsectionHeader>
<bodyText confidence="0.9999611875">
Barzilay and Lapata’s (2008) data corpus is
from the Encyclopedia Britannica and the
Britannica Elementary, the latter being a new
version targeted at children. Both versions con-
tain 107 articles. The Encyclopedia Britannica
corpus contains an average of 83.1 sentences
per document and the Britannica Elementary
contains 36.6. The encyclopedia lemmas are
written by different authors and consequently
vary considerably in structure and vocabulary
choice. Early researchers assumed that the chil-
dren version (Britannica Elementary) is easier
to read, hence more coherent than documents in
Encyclopedia Britannica. This is a somewhat
questionable assumption that needs further inves-
tigation.
</bodyText>
<subsectionHeader confidence="0.491709">
5.2.2 Training and Testing
</subsectionHeader>
<bodyText confidence="0.999883888888889">
Existing coherence approaches again apply a pair-
wise ranking strategy and the article associated
with the higher score is considered to be the more
readable. As the replacement strategy for gener-
ating negative example is apparently not well fit-
ted to this task, we adopted the following training
framework: we use all sliding windows of sen-
tences from coherent documents (documents from
Britannica Elementary) as positive examples,
</bodyText>
<page confidence="0.979584">
2045
</page>
<table confidence="0.999640142857143">
Approach Accuracy
Recurrent 0.803
Recursive 0.828
Graph Approach 0.786
Entity 0.509
S&amp;O 0.786
Entity+S&amp;O 0.888
</table>
<tableCaption confidence="0.8468038">
Table 2: Comparison of Different Coherence
Frameworks on Readability Assessment. Re-
ported baselines results are are taken from (Barzi-
lay and Lapata, 2008; Guinaudeau and Strube,
2013). S&amp;O: Schwarm and Ostendorf (2005).
</tableCaption>
<bodyText confidence="0.996134">
and cliques from Encyclopedia Britannica as
negative examples, and again apply Eq. 6 for train-
ing and optimization. During testing, we turn to
Equations 8 and 9 for pairwise comparison. We
adopted five-fold cross-validation in the same way
as in (Barzilay and Lapata, 2008; Guinaudeau and
Strube, 2013) for fair comparison. Parameters
were tuned within each training set also using five-
fold cross-validation. Parameters to tune included
window size L and regularization parameter Q.
</bodyText>
<subsectionHeader confidence="0.885086">
5.3 Results
</subsectionHeader>
<bodyText confidence="0.9999437">
We report results of the proposed approaches in
the work along with entity model (Barzilay and
Lapata, 2008) and graph based approach (Elsner
and Charniak, 2008) in Table 2. The tabs shows
that deep learning approaches again significantly
outperform Entry and Global Approach baselines
and are nearly comparable to the combination of
entity and S&amp;O features. Again, the recursive
network outperforms the recurrent network in this
task.
</bodyText>
<sectionHeader confidence="0.995038" genericHeader="conclusions">
6 Conclusion
</sectionHeader>
<bodyText confidence="0.999838571428571">
In this paper, we apply two neural network
approaches to the sentence-ordering (coherence)
task, using compositional sentence representations
learned by recurrent and recursive composition.
The proposed approach obtains state-of-art per-
formance on the standard coherence evaluation
tasks.
</bodyText>
<sectionHeader confidence="0.980156" genericHeader="acknowledgments">
Acknowledgements
</sectionHeader>
<bodyText confidence="0.99988525">
The authors want to thank Richard Socher and
Pradeep Dasigi for the clarification of deep learn-
ing techniques. We also thank the three anony-
mous EMNLP reviewers for helpful comments.
</bodyText>
<sectionHeader confidence="0.956863" genericHeader="references">
References
</sectionHeader>
<reference confidence="0.998568245283019">
Regina Barzilay and Mirella Lapata. 2008. Modeling
local coherence: An entity-based approach. Compu-
tational Linguistics, 34(1):1–34.
Regina Barzilay and Lillian Lee. 2004. Catching the
drift: Probabilistic content models, with applications
to generation and summarization. In HLT-NAACL,
pages 113–120.
Yoshua Bengio, Patrice Simard, and Paolo Frasconi.
1994. Learning long-term dependencies with gra-
dient descent is difficult. Neural Networks, IEEE
Transactions on, 5(2):157–166.
Yoshua Bengio, Holger Schwenk, Jean-S´ebastien
Sen´ecal, Fr´ederic Morin, and Jean-Luc Gauvain.
2006. Neural probabilistic language models. In
Innovations in Machine Learning, pages 137–186.
Springer.
Ronan Collobert and Jason Weston. 2008. A unified
architecture for natural language processing: Deep
neural networks with multitask learning. In Pro-
ceedings of the 25th international conference on
Machine learning, pages 160–167. ACM.
Ronan Collobert, Jason Weston, L´eon Bottou, Michael
Karlen, Koray Kavukcuoglu, and Pavel Kuksa.
2011. Natural language processing (almost) from
scratch. The Journal of Machine Learning Re-
search, 12:2493–2537.
Ronan Collobert. 2011. Deep learning for efficient dis-
criminative parsing. In International Conference on
Artificial Intelligence and Statistics, number EPFL-
CONF-192374.
John Duchi, Elad Hazan, and Yoram Singer. 2011.
Adaptive subgradient methods for online learning
and stochastic optimization. The Journal of Ma-
chine Learning Research, 12:2121–2159.
Micha Eisner and Eugene Charniak. 2011. Extending
the entity grid with entity-specific features. In Pro-
ceedings of the 49th Annual Meeting of the Associ-
ation for Computational Linguistics: Human Lan-
guage Technologies: short papers-Volume 2, pages
125–129. Association for Computational Linguis-
tics.
Jeffrey L Elman. 1990. Finding structure in time.
Cognitive science, 14(2):179–211.
Micha Elsner and Eugene Charniak. 2008.
Coreference-inspired coherence modeling. In
Proceedings of the 46th Annual Meeting of the
Association for Computational Linguistics on Hu-
man Language Technologies: Short Papers, pages
41–44. Association for Computational Linguistics.
Micha Elsner, Joseph L Austerweil, and Eugene Char-
niak. 2007. A unified local and global model for
discourse coherence. In HLT-NAACL, pages 436–
443.
</reference>
<page confidence="0.930122">
2046
</page>
<reference confidence="0.943503794392523">
Katja Filippova and Michael Strube. 2007. Extend-
ing the entity-grid coherence model to semantically
related entities. In Proceedings of the Eleventh Eu-
ropean Workshop on Natural Language Generation,
pages 139–142. Association for Computational Lin-
guistics.
Barbara J Grosz and Candace L Sidner. 1986. Atten-
tion, intentions, and the structure of discourse. Com-
putational linguistics, 12(3):175–204.
Barbara J Grosz, Scott Weinstein, and Aravind K Joshi.
1995. Centering: A framework for modeling the lo-
cal coherence of discourse. Computational linguis-
tics, 21(2):203–225.
Camille Guinaudeau and Michael Strube. 2013.
Graph-based local coherence modeling. In Proceed-
ings of the 51st Annual Meeting of the Association
for Computational Linguistics, pages 93–103.
Laura Hasler. 2004. An investigation into the use of
centering transitions for summarisation. In Proceed-
ings of the 7th Annual CLUK Research Colloquium,
pages 100–107.
Jerry R Hobbs, Mark Stickel, Paul Martin, and Dou-
glas Edwards. 1988. Interpretation as abduction. In
Proceedings of the 26th annual meeting on Associ-
ation for Computational Linguistics, pages 95–103.
Association for Computational Linguistics.
Eduard H Hovy. 1988. Planning coherent multisenten-
tial text. In Proceedings of the 26th annual meet-
ing on Association for Computational Linguistics,
pages 163–169. Association for Computational Lin-
guistics.
Ozan Irsoy and Claire Cardie. 2013. Bidirectional re-
cursive neural networks for token-level labeling with
structure. arXiv preprint arXiv:1312.0493.
Nal Kalchbrenner and Phil Blunsom. 2013. Recurrent
convolutional neural networks for discourse compo-
sitionality. arXiv preprint arXiv:1306.3584.
Mirella Lapata and Regina Barzilay. 2005. Automatic
evaluation of text coherence: Models and represen-
tations. In IJCAI, volume 5, pages 1085–1090.
Alex Lascarides and Nicholas Asher. 1991. Discourse
relations and defeasible knowledge. In Proceedings
of the 29th annual meeting on Association for Com-
putational Linguistics, pages 55–62. Association for
Computational Linguistics.
Quoc Le and Tomas Mikolov. 2014. Distributed repre-
sentations of sentences and documents.
Ziheng Lin, Hwee Tou Ng, and Min-Yen Kan. 2011.
Automatically evaluating text coherence using dis-
course relations. In Proceedings of the 49th An-
nual Meeting of the Association for Computational
Linguistics: Human Language Technologies-Volume
1, pages 997–1006. Association for Computational
Linguistics.
Annie Louis and Ani Nenkova. 2012. A coherence
model based on syntactic patterns. In Proceedings
of the 2012 Joint Conference on Empirical Methods
in Natural Language Processing and Computational
Natural Language Learning, pages 1157–1168. As-
sociation for Computational Linguistics.
William C Mann and Sandra A Thompson. 1988.
Rhetorical structure theory: Toward a functional the-
ory of text organization. Text, 8(3):243–281.
Kathleen R McKeown. 1985. Discourse strategies for
generating natural-language text. Artificial Intelli-
gence, 27(1):1–41.
Gr´egoire Mesnil, Xiaodong He, Li Deng, and Yoshua
Bengio. 2013. Investigation of recurrent-neural-
network architectures and learning methods for spo-
ken language understanding. Interspeech.
Tomas Mikolov, Martin Karafi´at, Lukas Burget, Jan
Cernock`y, and Sanjeev Khudanpur. 2010. Recur-
rent neural network based language model. In IN-
TERSPEECH, pages 1045–1048.
Tomas Mikolov, Kai Chen, Greg Corrado, and Jef-
frey Dean. 2013. Efficient estimation of word
representations in vector space. arXiv preprint
arXiv:1301.3781.
Eleni Miltsakaki and Karen Kukich. 2000. The role
of centering theory’s rough-shift in the teaching and
evaluation of writing skills. In Proceedings of the
38th Annual Meeting on Association for Computa-
tional Linguistics, pages 408–415. Association for
Computational Linguistics.
Andriy Mnih and Geoffrey Hinton. 2007. Three new
graphical models for statistical language modelling.
In Proceedings of the 24th international conference
on Machine learning, pages 641–648. ACM.
Johanna D Moore and Cecile L Paris. 1989. Planning
text for advisory dialogues. In Proceedings of the
27th annual meeting on Association for Computa-
tional Linguistics, pages 203–211. Association for
Computational Linguistics.
Wenzhe Pei, Tao Ge, and Chang Baobao. 2014. Max-
margin tensor neural network for chinese word seg-
mentation. In Proceedings of ACL.
Massimo Poesio, Rosemary Stevenson, Barbara Di Eu-
genio, and Janet Hitzeman. 2004. Centering: A
parametric theory and its instantiations. Computa-
tional linguistics, 30(3):309–363.
David E Rumelhart, Geoffrey E Hinton, and Ronald J
Williams. 1988. Learning representations by back-
propagating errors. MIT Press, Cambridge, MA,
USA.
Mike Schuster and Kuldip K Paliwal. 1997. Bidirec-
tional recurrent neural networks. Signal Processing,
IEEE Transactions on, 45(11):2673–2681.
</reference>
<page confidence="0.858742">
2047
</page>
<reference confidence="0.994266695652174">
Sarah E Schwarm and Mari Ostendorf. 2005. Read-
ing level assessment using support vector machines
and statistical language models. In Proceedings of
the 43rd Annual Meeting on Association for Com-
putational Linguistics, pages 523–530. Association
for Computational Linguistics.
Noah A Smith and Jason Eisner. 2005. Contrastive
estimation: Training log-linear models on unlabeled
data. In Proceedings of the 43rd Annual Meeting
on Association for Computational Linguistics, pages
354–362. Association for Computational Linguis-
tics.
Richard Socher, Christopher D Manning, and An-
drew Y Ng. 2010. Learning continuous phrase
representations and syntactic parsing with recursive
neural networks. In Proceedings of the NIPS-2010
Deep Learning and Unsupervised Feature Learning
Workshop, pages 1–9.
Richard Socher, Eric H Huang, Jeffrey Pennington,
Andrew Y Ng, and Christopher D Manning. 2011a.
Dynamic pooling and unfolding recursive autoen-
coders for paraphrase detection. In NIPS, vol-
ume 24, pages 801–809.
Richard Socher, Cliff C Lin, Chris Manning, and An-
drew Y Ng. 2011b. Parsing natural scenes and nat-
ural language with recursive neural networks. In
Proceedings of the 28th International Conference on
Machine Learning (ICML-11), pages 129–136.
Richard Socher, Brody Huval, Christopher D Manning,
and Andrew Y Ng. 2012. Semantic compositional-
ity through recursive matrix-vector spaces. In Pro-
ceedings of the 2012 Joint Conference on Empiri-
cal Methods in Natural Language Processing and
Computational Natural Language Learning, pages
1201–1211. Association for Computational Linguis-
tics.
Richard Socher, Alex Perelygin, Jean Y Wu, Jason
Chuang, Christopher D Manning, Andrew Y Ng,
and Christopher Potts. 2013. Recursive deep mod-
els for semantic compositionality over a sentiment
treebank. In Proceedings of the Conference on Em-
pirical Methods in Natural Language Processing
(EMNLP), pages 1631–1642.
Michael Strube and Udo Hahn. 1999. Functional
centering: Grounding referential coherence in in-
formation structure. Computational linguistics,
25(3):309–344.
Ilya Sutskever, James Martens, and Geoffrey E Hin-
ton. 2011. Generating text with recurrent neural
networks. In Proceedings of the 28th International
Conference on Machine Learning (ICML-11), pages
1017–1024.
Marilyn A Walker, Aravind Krishna Joshi, and
Ellen Friedman Prince. 1998. Centering theory in
discourse. Oxford University Press.
Sida Wang and Christopher D Manning. 2012. Base-
lines and bigrams: Simple, good sentiment and topic
classification. In Proceedings of the 50th Annual
Meeting of the Association for Computational Lin-
guistics: Short Papers-Volume 2, pages 90–94. As-
sociation for Computational Linguistics.
Houfeng Wang, Longkai Zhang, Li Li, He Zhengyan,
and Ni Sun. 2013. Improving chinese word seg-
mentation on micro-blog using rich punctuations.
Will Y Zou, Richard Socher, Daniel Cer, and Christo-
pher D Manning. 2013. Bilingual word embed-
dings for phrase-based machine translation. In Con-
ference on Empirical Methods in Natural Language
Processing (EMNLP 2013).
</reference>
<page confidence="0.994013">
2048
</page>
</variant>
</algorithm>
<algorithm name="ParsHed" version="110505">
<variant no="0" confidence="0.470235">
<title confidence="0.999267">A Model of Coherence Based on Distributed Sentence Representation</title>
<author confidence="0.964065">Eduard</author>
<affiliation confidence="0.856521">Science Department, Stanford University, Stanford, CA 94305,</affiliation>
<address confidence="0.970743">Technology Institute, Carnegie Mellon University, Pittsburgh, PA 15213,</address>
<email confidence="0.998826">jiweil@stanford.eduehovy@andrew.cmu.edu</email>
<abstract confidence="0.99992492">Coherence is what makes a multi-sentence text meaningful, both logically and syntactically. To solve the challenge of ordering a set of sentences into coherent order, existing approaches focus mostly on defining and using sophisticated features to capture the cross-sentence argumentation logic and syntactic relationships. But both argumentation semantics and crosssentence syntax (such as coreference and tense rules) are very hard to formalize. In this paper, we introduce a neural network model for the coherence task based on distributed sentence representation. The proposed approach learns a syntacticosemantic representation for sentences automatically, using either recurrent or recursive neural networks. The architecture obviated the need for feature engineering, and learns sentence representations, which are to some extent able to capture the ‘rules’ governing coherent sentence structure. The proposed approach outperforms existing baselines and generates the state-</abstract>
<intro confidence="0.579207">of-art performance in standard coherence</intro>
</variant>
</algorithm>
<algorithm name="ParsCit" version="110505">
<citationList>
<citation valid="true">
<authors>
<author>Regina Barzilay</author>
<author>Mirella Lapata</author>
</authors>
<title>Modeling local coherence: An entity-based approach.</title>
<date>2008</date>
<journal>Computational Linguistics,</journal>
<volume>34</volume>
<issue>1</issue>
<contexts>
<context position="3569" citStr="Barzilay and Lapata, 2008" startWordPosition="526" endWordPosition="529">ork defined so called schemas (McKeown, 1985), frames with fixed sequences of clause types to achieve stereotypical communicative intentions. Little of this work survives. Modern research tries simply to order a collection of clauses or sentences without giving an account of which order(s) is/are coherent or what the overall text structure is. The research focuses on identifying and defining a set of increasingly sophisticated features by which algorithms can be trained to propose orderings. Features being explored include the clause entities, organized into a grid (Lapata and Barzilay, 2005; Barzilay and Lapata, 2008), coreference clues to ordering (Elsner and Charniak, 2008), named-entity categories (Eisner and Charniak, 2011), syntactic features (Louis and Nenkova, 2012), and others. Besides being time-intensive (feature engineering usually requites considerable 2039 Proceedings of the 2014 Conference on Empirical Methods in Natural Language Processing (EMNLP), pages 2039–2048, October 25-29, 2014, Doha, Qatar. c�2014 Association for Computational Linguistics Figure 1: Illustrations of coherent (positive) vs not-coherent (negative) training examples. effort and can depend greatly on upstream feature extr</context>
<context position="7116" citStr="Barzilay and Lapata (2008)" startWordPosition="1061" endWordPosition="1064">sition in Section 4. Experimental results are shown in Section 5, followed by a conclusion. 2 Related Work Coherence In addition to the early computational work discussed above, local coherence was extensively studied within the modeling framework of Centering Theory (Grosz et al., 1995; Walker et al., 1998; Strube and Hahn, 1999; Poesio et al., 2004), which provides principles to form a coherence metric (Miltsakaki and Kukich, 2000; Hasler, 2004). Centering approaches suffer from a severe dependence on manually annotated input. A recent popular approach is the entity grid model introduced by Barzilay and Lapata (2008) , in which sentences are represented by a vector of discourse entities along with their grammatical roles (e.g., subject or object). Probabilities of transitions between adjacent sentences are derived from entity features and then concatenated to a document vector representation, which is used as input to machine learning classifiers such as SVM. Many frameworks have extended the entity approach, for example, by pre-grouping entities based on semantic relatedness (Filippova and Strube, 2007) or adding more useful types of features such as coreference (Elsner and Charniak, 2008), named entitie</context>
<context position="18352" citStr="Barzilay and Lapata, 2008" startWordPosition="2928" endWordPosition="2931">Elements in Wsen are initialized by randomly drawing from the uniform distribution [−�, �], \/6 where � = \/H+KxL as suggested in (Collobert et al., 2011). Wrecurrent, Vrecurrent, Wrecursive and h0 are initialized by randomly sampling from a uniform distribution U(−0.2,0.2). All bias vectors are initialized with 0. Hidden layer number H is set to 100. Word embeddings {e} are borrowed from Senna (Collobert et al., 2011; Collobert, 2011). The dimension for these embeddings is 50. 5 Experiments We evaluate the proposed coherence model on two common evaluation approaches adopted in existing work (Barzilay and Lapata, 2008; Louis and Nenkova, 2012; Elsner et al., 2007; Lin et al., 2011): Sentence Ordering and Readability Assessment. 5.1 Sentence Ordering We follow (Barzilay and Lapata, 2008; Louis and Nenkova, 2012; Elsner et al., 2007; Lin et al., 5For more details on backpropagation through RNNs, see Socher et al. (2010). {−yC log[p(yC = 1)] θ2 (6) 2043 2011) that all use pairs of articles, one containing the original document order and the other a random permutation of the sentences from the same document. The pairwise approach is predicated on the assumption that the original article is always more coherent</context>
<context position="19860" citStr="Barzilay and Lapata, 2008" startWordPosition="3206" endWordPosition="3209">our clique definition, document d is comprised of Nd cliques. Taking window size L = 3 as example, cliques generated from document d appear as follows: &lt; sstart, s1, s2 &gt;, &lt; s1, s2, s3 &gt;, ..., &lt; sNd−2, sNd−1, sNd &gt;, &lt; sNd−1, sNd, send &gt; The coherence score for a given document Sd is the probability that all cliques within d are coherent, which is given by: �Sd = P(YC = 1) (8) CEd For document pair &lt; d1, d2 &gt; in our task, we would say document d1 is more coherent than d2 if Sd1 &gt; Sd2 (9) 5.1.1 Dataset We use two corpora that are widely employed for coherence prediction (Barzilay and Lee, 2004; Barzilay and Lapata, 2008; Elsner et al., 2007). One contains reports on airplane accidents from the National Transportation Safety Board and the other contains reports about earthquakes from the Associated Press. These articles are about 10 sentences long and usually exhibit clear sentence structure. For preprocessing, we only lowercase the capital letters to match with tokens in Senna word embeddings. In the recursive network, sentences are parsed using the Stanford Parser6 and then transformed into binary trees. The accident corpus ends up with a vocabulary size of 4758 and an average of 10.6 sentences per document</context>
<context position="21972" citStr="Barzilay and Lapata, 2008" startWordPosition="3531" endWordPosition="3534">e L (tried on {3, 5, 7}) and regularization parameter Q (tried on {0.01, 0.1, 0.25, 0.5,1.0,1.25, 2.0, 2.5, 5.0}). We trained parameters using 10-fold cross-validation on the training data. Concretely, in each setting, 90 documents were used for training and evaluation was done on the remaining articles, following (Louis and Nenkova, 2012). After tuning, the final model was tested on the testing set. 5.1.3 Model Comparison We report performance of recursive and recurrent networks. We also report results from some popular approaches in the literature, including: Entity Grid Model : Grid model (Barzilay and Lapata, 2008) obtains the best performance when coreference resolution, expressive syntactic information, and salience-based features are incorporated. Entity grid models represent each sentence as a column of a grid of features and apply machine learning methods (e.g., SVM) to identify the coherent transitions based on entity features (for details of entity models see (Barzilay and Lapata, 2008)). Results are directly taken from Barzilay and Lapata’s paper (2008). HMM : Hidden-Markov approach proposed by Louis and Nenkova (2012) to model the state (cluster) transition probability in the coherent context u</context>
<context position="23222" citStr="Barzilay and Lapata, 2008" startWordPosition="3713" endWordPosition="3717">Sentences need to be clustered in advance where the number of clusters is tuned as a parameter. We directly take 7Permutations are downloaded from http: //people.csail.mit.edu/regina/coherence/ CLsubmission/. 2044 Acci Earthquake Average Recursive 0.864 0.976 0.920 Recurrent 0.840 0.951 0.895 Entity Grid 0.904 0.872 0.888 HMM 0.822 0.938 0.880 HMM+Entity 0.842 0.911 0.877 HMM+Content 0.742 0.953 0.848 Graph 0.846 0.635 0.740 Table 1: Comparison of Different Coherence Frameworks. Reported baseline results are among the best performance regarding each approach is reprinted from prior work from (Barzilay and Lapata, 2008; Louis and Nenkova, 2012; Guinaudeau and Strube, 2013). the results from Louis and Nenkova’s paper and report the best results among different combinations of parameter and feature settings8. We also report performances of models from Louis and Nenkova’s work that combine HMM and entity/content models in a unified framework. Graph Based Approach : Guinaudeau and Strube (2013) extended the entity grid model to a bipartite graph representing the text, where the entity transition information needed for local coherence computation is embedded in the bipartite graph. The Graph Based Approach outpe</context>
<context position="25610" citStr="Barzilay and Lapata (2008)" startWordPosition="4076" endWordPosition="4079">setting can be found in (Louis and Nenkova, 2012). ing the convolution on parse trees rather than simply piling up terms within the sentence, which is in line with common expectation. Both recurrent and recursive models obtain better performance on the Earthquake than the Accident dataset. Scrutiny of the corpus reveals that articles reporting earthquakes exhibit a more consistent structure: earthquake outbreak, describing the center and intensity of the earthquake, injuries and rescue operations, etc., while accident articles usually exhibit more diverse scenarios. 5.2 Readability Assessment Barzilay and Lapata (2008) proposed a readability assessment task for stylistic judgments about the difficulty of reading a document. Their approach combines a coherence system with Schwarm and Ostendorf’s (2005) readability features to classify documents into two categories, more readable (coherent) documents and less readable ones. The evaluation accesses the ability to differentiate “easy to read” documents from difficult ones of each model. 5.2.1 Dataset Barzilay and Lapata’s (2008) data corpus is from the Encyclopedia Britannica and the Britannica Elementary, the latter being a new version targeted at children. Bo</context>
<context position="27466" citStr="Barzilay and Lapata, 2008" startWordPosition="4346" endWordPosition="4350">e article associated with the higher score is considered to be the more readable. As the replacement strategy for generating negative example is apparently not well fitted to this task, we adopted the following training framework: we use all sliding windows of sentences from coherent documents (documents from Britannica Elementary) as positive examples, 2045 Approach Accuracy Recurrent 0.803 Recursive 0.828 Graph Approach 0.786 Entity 0.509 S&amp;O 0.786 Entity+S&amp;O 0.888 Table 2: Comparison of Different Coherence Frameworks on Readability Assessment. Reported baselines results are are taken from (Barzilay and Lapata, 2008; Guinaudeau and Strube, 2013). S&amp;O: Schwarm and Ostendorf (2005). and cliques from Encyclopedia Britannica as negative examples, and again apply Eq. 6 for training and optimization. During testing, we turn to Equations 8 and 9 for pairwise comparison. We adopted five-fold cross-validation in the same way as in (Barzilay and Lapata, 2008; Guinaudeau and Strube, 2013) for fair comparison. Parameters were tuned within each training set also using fivefold cross-validation. Parameters to tune included window size L and regularization parameter Q. 5.3 Results We report results of the proposed appr</context>
</contexts>
<marker>Barzilay, Lapata, 2008</marker>
<rawString>Regina Barzilay and Mirella Lapata. 2008. Modeling local coherence: An entity-based approach. Computational Linguistics, 34(1):1–34.</rawString>
</citation>
<citation valid="true">
<authors>
<author>Regina Barzilay</author>
<author>Lillian Lee</author>
</authors>
<title>Catching the drift: Probabilistic content models, with applications to generation and summarization.</title>
<date>2004</date>
<booktitle>In HLT-NAACL,</booktitle>
<pages>113--120</pages>
<contexts>
<context position="6296" citStr="Barzilay and Lee, 2004" startWordPosition="928" endWordPosition="931">by humans, and negatives examples are generated by random replacements2. The semantic representations for terms and sentences are obtained through optimizing the neural network framework based on these positive vs negative ex2Our approach is inspired by Collobert et al.’s idea (2011) that a word and its context form a positive training sample while a random word in that same context gives a negative training sample, when training word embeddings in the deep learning framework. amples and the proposed model produces state-ofart performance in multiple standard evaluations for coherence models (Barzilay and Lee, 2004). The rest of this paper is organized as follows: We describe related work in Section 2, then describe how to obtain a distributed representation for sentences in Section 3, and the window composition in Section 4. Experimental results are shown in Section 5, followed by a conclusion. 2 Related Work Coherence In addition to the early computational work discussed above, local coherence was extensively studied within the modeling framework of Centering Theory (Grosz et al., 1995; Walker et al., 1998; Strube and Hahn, 1999; Poesio et al., 2004), which provides principles to form a coherence metri</context>
<context position="19833" citStr="Barzilay and Lee, 2004" startWordPosition="3202" endWordPosition="3205">nces within d. Based on our clique definition, document d is comprised of Nd cliques. Taking window size L = 3 as example, cliques generated from document d appear as follows: &lt; sstart, s1, s2 &gt;, &lt; s1, s2, s3 &gt;, ..., &lt; sNd−2, sNd−1, sNd &gt;, &lt; sNd−1, sNd, send &gt; The coherence score for a given document Sd is the probability that all cliques within d are coherent, which is given by: �Sd = P(YC = 1) (8) CEd For document pair &lt; d1, d2 &gt; in our task, we would say document d1 is more coherent than d2 if Sd1 &gt; Sd2 (9) 5.1.1 Dataset We use two corpora that are widely employed for coherence prediction (Barzilay and Lee, 2004; Barzilay and Lapata, 2008; Elsner et al., 2007). One contains reports on airplane accidents from the National Transportation Safety Board and the other contains reports about earthquakes from the Associated Press. These articles are about 10 sentences long and usually exhibit clear sentence structure. For preprocessing, we only lowercase the capital letters to match with tokens in Senna word embeddings. In the recursive network, sentences are parsed using the Stanford Parser6 and then transformed into binary trees. The accident corpus ends up with a vocabulary size of 4758 and an average of </context>
</contexts>
<marker>Barzilay, Lee, 2004</marker>
<rawString>Regina Barzilay and Lillian Lee. 2004. Catching the drift: Probabilistic content models, with applications to generation and summarization. In HLT-NAACL, pages 113–120.</rawString>
</citation>
<citation valid="true">
<authors>
<author>Yoshua Bengio</author>
<author>Patrice Simard</author>
<author>Paolo Frasconi</author>
</authors>
<title>Learning long-term dependencies with gradient descent is difficult. Neural Networks,</title>
<date>1994</date>
<journal>IEEE Transactions on,</journal>
<volume>5</volume>
<issue>2</issue>
<contexts>
<context position="9215" citStr="Bengio et al., 1994" startWordPosition="1393" endWordPosition="1396">urrent output. At each step, the recurrent network takes as input both the output of previous steps and the current token, convolutes the inputs, and forwards the result to the next step. It has been successfully applied to tasks such as language modeling (Mikolov et al., 2010) and spoken language understanding (Mesnil et al., 2013). The advantage of recurrent network is that it does not depend on external deeper structure (e.g., parse tree) and is easy to implement. However, in the recurrent framework, long-distance dependencies are difficult to capture due to the vanishing gradient problem (Bengio et al., 1994); two tokens may be structurally close to each other, even though they are far away in word sequence3. Recursive neural networks comprise another class of architecture, one that relies and operates on structured inputs (e.g., parse trees). It computes the representation for each parent based on its children iteratively in a bottom-up fashion. A series of variations have been proposed, each tailored to different task-specific requirements, such as Matrix-Vector RNN (Socher et al., 2012) that represents every word as both a vector and a matrix, or Recursive Neural Tensor Networks (Socher et al.,</context>
</contexts>
<marker>Bengio, Simard, Frasconi, 1994</marker>
<rawString>Yoshua Bengio, Patrice Simard, and Paolo Frasconi. 1994. Learning long-term dependencies with gradient descent is difficult. Neural Networks, IEEE Transactions on, 5(2):157–166.</rawString>
</citation>
<citation valid="true">
<authors>
<author>Yoshua Bengio</author>
<author>Holger Schwenk</author>
<author>Jean-S´ebastien Sen´ecal</author>
<author>Fr´ederic Morin</author>
<author>Jean-Luc Gauvain</author>
</authors>
<title>Neural probabilistic language models.</title>
<date>2006</date>
<booktitle>In Innovations in Machine Learning,</booktitle>
<pages>137--186</pages>
<publisher>Springer.</publisher>
<marker>Bengio, Schwenk, Sen´ecal, Morin, Gauvain, 2006</marker>
<rawString>Yoshua Bengio, Holger Schwenk, Jean-S´ebastien Sen´ecal, Fr´ederic Morin, and Jean-Luc Gauvain. 2006. Neural probabilistic language models. In Innovations in Machine Learning, pages 137–186. Springer.</rawString>
</citation>
<citation valid="true">
<authors>
<author>Ronan Collobert</author>
<author>Jason Weston</author>
</authors>
<title>A unified architecture for natural language processing: Deep neural networks with multitask learning.</title>
<date>2008</date>
<booktitle>In Proceedings of the 25th international conference on Machine learning,</booktitle>
<pages>160--167</pages>
<publisher>ACM.</publisher>
<contexts>
<context position="10741" citStr="Collobert and Weston, 2008" startWordPosition="1635" endWordPosition="1638">ted from this recursive framework, including parsing (Socher et al., 2011b), sentiment analysis (Socher et al., 2013), and paraphrase detection (Socher et al., 2011a). 2.1 Distributed Representations Both recurrent and recursive networks require a vector representation of each input token. Distributed representations for words were first proposed in (Rumelhart et al., 1988) and have been successful for statistical language modeling (Elman, 1990). Various deep learning architectures have been explored to learn these embeddings in an unsupervised manner from a large corpus (Bengio et al., 2006; Collobert and Weston, 2008; Mnih and Hinton, 2007; Mikolov et al., 2013), which might have different generalization capabilities and are able to capture the semantic meanings depending on the specific task at hand. These vector representations can to some extent capture interesting semantic relationships, such as King−man ≈ Queue−woman (Mikolov et al., 2010), and recently have been successfully used in various NLP applications, including named entity recognition, tagging, segmentation (Wang et al., 2013), and machine translation (e.g.,(Collobert and Weston, 2008; Zou et al., 2013)). 3 Sentence Model In this section, we</context>
</contexts>
<marker>Collobert, Weston, 2008</marker>
<rawString>Ronan Collobert and Jason Weston. 2008. A unified architecture for natural language processing: Deep neural networks with multitask learning. In Proceedings of the 25th international conference on Machine learning, pages 160–167. ACM.</rawString>
</citation>
<citation valid="true">
<authors>
<author>Ronan Collobert</author>
<author>Jason Weston</author>
<author>L´eon Bottou</author>
<author>Michael Karlen</author>
<author>Koray Kavukcuoglu</author>
<author>Pavel Kuksa</author>
</authors>
<title>Natural language processing (almost) from scratch.</title>
<date>2011</date>
<journal>The Journal of Machine Learning Research,</journal>
<pages>12--2493</pages>
<contexts>
<context position="13827" citStr="Collobert et al., 2011" startWordPosition="2136" endWordPosition="2139">th vector representation hc2), standard recursive networks calculates hp for p as follows: hp = f(WRecursive &apos; [hc1, hc2] + bRecursive) (3) where [hc1, hc2] denotes the concatenating vector for children vector representation hc1 and hc2. WRecursive is a K x 2K matrix and bRecursive is the 1 x K bias vector. f(&apos;) is tanh function. Recursive neural models compute parent vectors iteratively until the root node’s representation is obtained, and use the root embedding to represent the whole sentence, as shown in Figure 2 (b). 4 Coherence Model The proposed coherence model adopts a window approach (Collobert et al., 2011), in which we train a three-layer neural network based on a sliding windows of L sentences. 4.1 Sentence Convolution We treat a window of sentences as a clique C and associate each clique with a tag yC that takes the value 1 if coherent, and 0 otherwise4. As shown in Figure 1, cliques taken from original articles are treated as coherent and those with sentences randomly replaced are used as negative examples. . The sentence convolution algorithm adopted in this paper is defined by a three-layer neural network, i.e., sentence-level input layer, hidden layer, and overall output layer as shown in</context>
<context position="15440" citStr="Collobert et al., 2011" startWordPosition="2424" endWordPosition="2428">y.) Let H denote the number of neurons in the hidden (second) layer. Then each of the hidden layers takes as input hC and performs the convolution using a non-linear tanh function, parametrized by Wsen and bsen. The concatenating output vector for hidden layers, defined as qC, can therefore be rewritten as: qC = f(Wsen x hC + bsen) (4) where Wsen is a H x (L x K) dimensional matrix and bsen is a H x 1 dimensional bias vector. 4instead of a binary classification (correct/incorrect), another commonly used approach is the contrastive approach that minimizes the score function max(0, 1 − s + s.) (Collobert et al., 2011; Smith and Eisner, 2005). s denotes the score of a true (coherent) window and s. the score of a corrupt (containing incoherence) one) in an attempt to make the score of true windows larger and corrupt windows smaller. We tried the contrastive one for both recurrent and recursive networks but the binary approach constantly outperformed the contrastive one in this task. ht = f(VRecurrent&apos;ht−1+WRecurrent&apos;etw+bRecurrent) (1) where WRecurrent and VRecurrent are K x K matrixes. bRecurrent denotes K x 1 bias vector and f = tanh is a standard element-wise nonlinearity. Note that calculation for repre</context>
<context position="17881" citStr="Collobert et al., 2011" startWordPosition="2855" endWordPosition="2858">e in AdaGrad is adapting differently for different parameters at different steps. Concretely, for parameter updates, let giτ denote the subgradient at time step for parameter θi, which is obtained from backpropagation5, the parameter update at time step t is given by: ( θτ = θτ−1 α �τ V i2 gτi l7) t=o gτ where α denotes the learning rate and is set to 0.01 in our approach. Optimal performance is achieved when batch size is set between 20 and 30. 4.3 Initialization Elements in Wsen are initialized by randomly drawing from the uniform distribution [−�, �], \/6 where � = \/H+KxL as suggested in (Collobert et al., 2011). Wrecurrent, Vrecurrent, Wrecursive and h0 are initialized by randomly sampling from a uniform distribution U(−0.2,0.2). All bias vectors are initialized with 0. Hidden layer number H is set to 100. Word embeddings {e} are borrowed from Senna (Collobert et al., 2011; Collobert, 2011). The dimension for these embeddings is 50. 5 Experiments We evaluate the proposed coherence model on two common evaluation approaches adopted in existing work (Barzilay and Lapata, 2008; Louis and Nenkova, 2012; Elsner et al., 2007; Lin et al., 2011): Sentence Ordering and Readability Assessment. 5.1 Sentence Ord</context>
</contexts>
<marker>Collobert, Weston, Bottou, Karlen, Kavukcuoglu, Kuksa, 2011</marker>
<rawString>Ronan Collobert, Jason Weston, L´eon Bottou, Michael Karlen, Koray Kavukcuoglu, and Pavel Kuksa. 2011. Natural language processing (almost) from scratch. The Journal of Machine Learning Research, 12:2493–2537.</rawString>
</citation>
<citation valid="true">
<authors>
<author>Ronan Collobert</author>
</authors>
<title>Deep learning for efficient discriminative parsing.</title>
<date>2011</date>
<booktitle>In International Conference on Artificial Intelligence and Statistics, number EPFLCONF-192374.</booktitle>
<contexts>
<context position="18166" citStr="Collobert, 2011" startWordPosition="2901" endWordPosition="2902">τi l7) t=o gτ where α denotes the learning rate and is set to 0.01 in our approach. Optimal performance is achieved when batch size is set between 20 and 30. 4.3 Initialization Elements in Wsen are initialized by randomly drawing from the uniform distribution [−�, �], \/6 where � = \/H+KxL as suggested in (Collobert et al., 2011). Wrecurrent, Vrecurrent, Wrecursive and h0 are initialized by randomly sampling from a uniform distribution U(−0.2,0.2). All bias vectors are initialized with 0. Hidden layer number H is set to 100. Word embeddings {e} are borrowed from Senna (Collobert et al., 2011; Collobert, 2011). The dimension for these embeddings is 50. 5 Experiments We evaluate the proposed coherence model on two common evaluation approaches adopted in existing work (Barzilay and Lapata, 2008; Louis and Nenkova, 2012; Elsner et al., 2007; Lin et al., 2011): Sentence Ordering and Readability Assessment. 5.1 Sentence Ordering We follow (Barzilay and Lapata, 2008; Louis and Nenkova, 2012; Elsner et al., 2007; Lin et al., 5For more details on backpropagation through RNNs, see Socher et al. (2010). {−yC log[p(yC = 1)] θ2 (6) 2043 2011) that all use pairs of articles, one containing the original document</context>
</contexts>
<marker>Collobert, 2011</marker>
<rawString>Ronan Collobert. 2011. Deep learning for efficient discriminative parsing. In International Conference on Artificial Intelligence and Statistics, number EPFLCONF-192374.</rawString>
</citation>
<citation valid="true">
<authors>
<author>John Duchi</author>
<author>Elad Hazan</author>
<author>Yoram Singer</author>
</authors>
<title>Adaptive subgradient methods for online learning and stochastic optimization.</title>
<date>2011</date>
<journal>The Journal of Machine Learning Research,</journal>
<pages>12--2121</pages>
<contexts>
<context position="17122" citStr="Duchi et al., 2011" startWordPosition="2724" endWordPosition="2727"> + b) (5) where U is an H x1 vector and b denotes the bias. 4.2 Training In the proposed framework, suppose we have M training samples, the cost function for recurrent neural network with regularization on the training set is given by: 1 � J(Θ) = M CEtrainset � − (1 − yC) log[1 − p(yC = 1)]} + Q 2M BEO where Θ = [WRecurrent, Wsen, Usen] The regularization part is paralyzed by Q to avoid overfitting. A similar loss function is applied to the recursive network with only minor parameter altering that is excluded for brevity. To minimize the objective J(Θ), we use the diagonal variant of AdaGrad (Duchi et al., 2011) with minibatches, which is widely applied in deep learning literature (e.g.,(Socher et al., 2011a; Pei et al., 2014)). The learning rate in AdaGrad is adapting differently for different parameters at different steps. Concretely, for parameter updates, let giτ denote the subgradient at time step for parameter θi, which is obtained from backpropagation5, the parameter update at time step t is given by: ( θτ = θτ−1 α �τ V i2 gτi l7) t=o gτ where α denotes the learning rate and is set to 0.01 in our approach. Optimal performance is achieved when batch size is set between 20 and 30. 4.3 Initializa</context>
</contexts>
<marker>Duchi, Hazan, Singer, 2011</marker>
<rawString>John Duchi, Elad Hazan, and Yoram Singer. 2011. Adaptive subgradient methods for online learning and stochastic optimization. The Journal of Machine Learning Research, 12:2121–2159.</rawString>
</citation>
<citation valid="true">
<authors>
<author>Micha Eisner</author>
<author>Eugene Charniak</author>
</authors>
<title>Extending the entity grid with entity-specific features.</title>
<date>2011</date>
<booktitle>In Proceedings of the 49th Annual Meeting of the Association for Computational Linguistics: Human Language Technologies: short papers-Volume 2,</booktitle>
<pages>125--129</pages>
<publisher>Association</publisher>
<institution>for Computational Linguistics.</institution>
<contexts>
<context position="3681" citStr="Eisner and Charniak, 2011" startWordPosition="540" endWordPosition="543">cal communicative intentions. Little of this work survives. Modern research tries simply to order a collection of clauses or sentences without giving an account of which order(s) is/are coherent or what the overall text structure is. The research focuses on identifying and defining a set of increasingly sophisticated features by which algorithms can be trained to propose orderings. Features being explored include the clause entities, organized into a grid (Lapata and Barzilay, 2005; Barzilay and Lapata, 2008), coreference clues to ordering (Elsner and Charniak, 2008), named-entity categories (Eisner and Charniak, 2011), syntactic features (Louis and Nenkova, 2012), and others. Besides being time-intensive (feature engineering usually requites considerable 2039 Proceedings of the 2014 Conference on Empirical Methods in Natural Language Processing (EMNLP), pages 2039–2048, October 25-29, 2014, Doha, Qatar. c�2014 Association for Computational Linguistics Figure 1: Illustrations of coherent (positive) vs not-coherent (negative) training examples. effort and can depend greatly on upstream feature extraction algorithms), it is not immediately apparent which aspects of a clause or a coherent text to consider when</context>
<context position="7745" citStr="Eisner and Charniak, 2011" startWordPosition="1158" endWordPosition="1161">in which sentences are represented by a vector of discourse entities along with their grammatical roles (e.g., subject or object). Probabilities of transitions between adjacent sentences are derived from entity features and then concatenated to a document vector representation, which is used as input to machine learning classifiers such as SVM. Many frameworks have extended the entity approach, for example, by pre-grouping entities based on semantic relatedness (Filippova and Strube, 2007) or adding more useful types of features such as coreference (Elsner and Charniak, 2008), named entities (Eisner and Charniak, 2011), and discourse relations (Lin et al., 2011). Other systems include the global graph model (Guinaudeau and Strube, 2013) which projects entities into a global graph. Louis and Nenkova (2012) introduced an HMM system in which the coherence between adjacent sentences is modeled by a hidden Markov framework captured by the 2040 Figure 2: Sentential compositionality obtained from (a) recurrent / (b) recursive neural network. The bottom layer represents word vectors in the sentence. The top layer hs denotes the resulting sentence vector. transition rules of different topics. Recurrent and Recursive</context>
</contexts>
<marker>Eisner, Charniak, 2011</marker>
<rawString>Micha Eisner and Eugene Charniak. 2011. Extending the entity grid with entity-specific features. In Proceedings of the 49th Annual Meeting of the Association for Computational Linguistics: Human Language Technologies: short papers-Volume 2, pages 125–129. Association for Computational Linguistics.</rawString>
</citation>
<citation valid="true">
<authors>
<author>Jeffrey L Elman</author>
</authors>
<title>Finding structure in time.</title>
<date>1990</date>
<journal>Cognitive science,</journal>
<volume>14</volume>
<issue>2</issue>
<contexts>
<context position="10564" citStr="Elman, 1990" startWordPosition="1608" endWordPosition="1610">f many adjectives lies in between, but they are adjacent in the parse tree (Irsoy and Cardie, 2013). interactions between the input vectors. Many tasks have benefited from this recursive framework, including parsing (Socher et al., 2011b), sentiment analysis (Socher et al., 2013), and paraphrase detection (Socher et al., 2011a). 2.1 Distributed Representations Both recurrent and recursive networks require a vector representation of each input token. Distributed representations for words were first proposed in (Rumelhart et al., 1988) and have been successful for statistical language modeling (Elman, 1990). Various deep learning architectures have been explored to learn these embeddings in an unsupervised manner from a large corpus (Bengio et al., 2006; Collobert and Weston, 2008; Mnih and Hinton, 2007; Mikolov et al., 2013), which might have different generalization capabilities and are able to capture the semantic meanings depending on the specific task at hand. These vector representations can to some extent capture interesting semantic relationships, such as King−man ≈ Queue−woman (Mikolov et al., 2010), and recently have been successfully used in various NLP applications, including named e</context>
</contexts>
<marker>Elman, 1990</marker>
<rawString>Jeffrey L Elman. 1990. Finding structure in time. Cognitive science, 14(2):179–211.</rawString>
</citation>
<citation valid="true">
<authors>
<author>Micha Elsner</author>
<author>Eugene Charniak</author>
</authors>
<title>Coreference-inspired coherence modeling.</title>
<date>2008</date>
<booktitle>In Proceedings of the 46th Annual Meeting of the Association for Computational Linguistics on Human Language Technologies: Short Papers,</booktitle>
<pages>41--44</pages>
<publisher>Association</publisher>
<institution>for Computational Linguistics.</institution>
<contexts>
<context position="3628" citStr="Elsner and Charniak, 2008" startWordPosition="534" endWordPosition="537">fixed sequences of clause types to achieve stereotypical communicative intentions. Little of this work survives. Modern research tries simply to order a collection of clauses or sentences without giving an account of which order(s) is/are coherent or what the overall text structure is. The research focuses on identifying and defining a set of increasingly sophisticated features by which algorithms can be trained to propose orderings. Features being explored include the clause entities, organized into a grid (Lapata and Barzilay, 2005; Barzilay and Lapata, 2008), coreference clues to ordering (Elsner and Charniak, 2008), named-entity categories (Eisner and Charniak, 2011), syntactic features (Louis and Nenkova, 2012), and others. Besides being time-intensive (feature engineering usually requites considerable 2039 Proceedings of the 2014 Conference on Empirical Methods in Natural Language Processing (EMNLP), pages 2039–2048, October 25-29, 2014, Doha, Qatar. c�2014 Association for Computational Linguistics Figure 1: Illustrations of coherent (positive) vs not-coherent (negative) training examples. effort and can depend greatly on upstream feature extraction algorithms), it is not immediately apparent which as</context>
<context position="7701" citStr="Elsner and Charniak, 2008" startWordPosition="1151" endWordPosition="1155"> introduced by Barzilay and Lapata (2008) , in which sentences are represented by a vector of discourse entities along with their grammatical roles (e.g., subject or object). Probabilities of transitions between adjacent sentences are derived from entity features and then concatenated to a document vector representation, which is used as input to machine learning classifiers such as SVM. Many frameworks have extended the entity approach, for example, by pre-grouping entities based on semantic relatedness (Filippova and Strube, 2007) or adding more useful types of features such as coreference (Elsner and Charniak, 2008), named entities (Eisner and Charniak, 2011), and discourse relations (Lin et al., 2011). Other systems include the global graph model (Guinaudeau and Strube, 2013) which projects entities into a global graph. Louis and Nenkova (2012) introduced an HMM system in which the coherence between adjacent sentences is modeled by a hidden Markov framework captured by the 2040 Figure 2: Sentential compositionality obtained from (a) recurrent / (b) recursive neural network. The bottom layer represents word vectors in the sentence. The top layer hs denotes the resulting sentence vector. transition rules </context>
<context position="28189" citStr="Elsner and Charniak, 2008" startWordPosition="4458" endWordPosition="4461">Britannica as negative examples, and again apply Eq. 6 for training and optimization. During testing, we turn to Equations 8 and 9 for pairwise comparison. We adopted five-fold cross-validation in the same way as in (Barzilay and Lapata, 2008; Guinaudeau and Strube, 2013) for fair comparison. Parameters were tuned within each training set also using fivefold cross-validation. Parameters to tune included window size L and regularization parameter Q. 5.3 Results We report results of the proposed approaches in the work along with entity model (Barzilay and Lapata, 2008) and graph based approach (Elsner and Charniak, 2008) in Table 2. The tabs shows that deep learning approaches again significantly outperform Entry and Global Approach baselines and are nearly comparable to the combination of entity and S&amp;O features. Again, the recursive network outperforms the recurrent network in this task. 6 Conclusion In this paper, we apply two neural network approaches to the sentence-ordering (coherence) task, using compositional sentence representations learned by recurrent and recursive composition. The proposed approach obtains state-of-art performance on the standard coherence evaluation tasks. Acknowledgements The au</context>
</contexts>
<marker>Elsner, Charniak, 2008</marker>
<rawString>Micha Elsner and Eugene Charniak. 2008. Coreference-inspired coherence modeling. In Proceedings of the 46th Annual Meeting of the Association for Computational Linguistics on Human Language Technologies: Short Papers, pages 41–44. Association for Computational Linguistics.</rawString>
</citation>
<citation valid="true">
<authors>
<author>Micha Elsner</author>
<author>Joseph L Austerweil</author>
<author>Eugene Charniak</author>
</authors>
<title>A unified local and global model for discourse coherence. In</title>
<date>2007</date>
<booktitle>HLT-NAACL,</booktitle>
<pages>436--443</pages>
<contexts>
<context position="18398" citStr="Elsner et al., 2007" startWordPosition="2936" endWordPosition="2939"> from the uniform distribution [−�, �], \/6 where � = \/H+KxL as suggested in (Collobert et al., 2011). Wrecurrent, Vrecurrent, Wrecursive and h0 are initialized by randomly sampling from a uniform distribution U(−0.2,0.2). All bias vectors are initialized with 0. Hidden layer number H is set to 100. Word embeddings {e} are borrowed from Senna (Collobert et al., 2011; Collobert, 2011). The dimension for these embeddings is 50. 5 Experiments We evaluate the proposed coherence model on two common evaluation approaches adopted in existing work (Barzilay and Lapata, 2008; Louis and Nenkova, 2012; Elsner et al., 2007; Lin et al., 2011): Sentence Ordering and Readability Assessment. 5.1 Sentence Ordering We follow (Barzilay and Lapata, 2008; Louis and Nenkova, 2012; Elsner et al., 2007; Lin et al., 5For more details on backpropagation through RNNs, see Socher et al. (2010). {−yC log[p(yC = 1)] θ2 (6) 2043 2011) that all use pairs of articles, one containing the original document order and the other a random permutation of the sentences from the same document. The pairwise approach is predicated on the assumption that the original article is always more coherent than a random permutation; this assumption ha</context>
<context position="19882" citStr="Elsner et al., 2007" startWordPosition="3210" endWordPosition="3213">ment d is comprised of Nd cliques. Taking window size L = 3 as example, cliques generated from document d appear as follows: &lt; sstart, s1, s2 &gt;, &lt; s1, s2, s3 &gt;, ..., &lt; sNd−2, sNd−1, sNd &gt;, &lt; sNd−1, sNd, send &gt; The coherence score for a given document Sd is the probability that all cliques within d are coherent, which is given by: �Sd = P(YC = 1) (8) CEd For document pair &lt; d1, d2 &gt; in our task, we would say document d1 is more coherent than d2 if Sd1 &gt; Sd2 (9) 5.1.1 Dataset We use two corpora that are widely employed for coherence prediction (Barzilay and Lee, 2004; Barzilay and Lapata, 2008; Elsner et al., 2007). One contains reports on airplane accidents from the National Transportation Safety Board and the other contains reports about earthquakes from the Associated Press. These articles are about 10 sentences long and usually exhibit clear sentence structure. For preprocessing, we only lowercase the capital letters to match with tokens in Senna word embeddings. In the recursive network, sentences are parsed using the Stanford Parser6 and then transformed into binary trees. The accident corpus ends up with a vocabulary size of 4758 and an average of 10.6 sentences per document. The earthquake corpu</context>
</contexts>
<marker>Elsner, Austerweil, Charniak, 2007</marker>
<rawString>Micha Elsner, Joseph L Austerweil, and Eugene Charniak. 2007. A unified local and global model for discourse coherence. In HLT-NAACL, pages 436– 443.</rawString>
</citation>
<citation valid="true">
<authors>
<author>Katja Filippova</author>
<author>Michael Strube</author>
</authors>
<title>Extending the entity-grid coherence model to semantically related entities.</title>
<date>2007</date>
<booktitle>In Proceedings of the Eleventh European Workshop on Natural Language Generation,</booktitle>
<pages>139--142</pages>
<publisher>Association</publisher>
<institution>for Computational Linguistics.</institution>
<contexts>
<context position="7613" citStr="Filippova and Strube, 2007" startWordPosition="1137" endWordPosition="1140">ependence on manually annotated input. A recent popular approach is the entity grid model introduced by Barzilay and Lapata (2008) , in which sentences are represented by a vector of discourse entities along with their grammatical roles (e.g., subject or object). Probabilities of transitions between adjacent sentences are derived from entity features and then concatenated to a document vector representation, which is used as input to machine learning classifiers such as SVM. Many frameworks have extended the entity approach, for example, by pre-grouping entities based on semantic relatedness (Filippova and Strube, 2007) or adding more useful types of features such as coreference (Elsner and Charniak, 2008), named entities (Eisner and Charniak, 2011), and discourse relations (Lin et al., 2011). Other systems include the global graph model (Guinaudeau and Strube, 2013) which projects entities into a global graph. Louis and Nenkova (2012) introduced an HMM system in which the coherence between adjacent sentences is modeled by a hidden Markov framework captured by the 2040 Figure 2: Sentential compositionality obtained from (a) recurrent / (b) recursive neural network. The bottom layer represents word vectors in</context>
</contexts>
<marker>Filippova, Strube, 2007</marker>
<rawString>Katja Filippova and Michael Strube. 2007. Extending the entity-grid coherence model to semantically related entities. In Proceedings of the Eleventh European Workshop on Natural Language Generation, pages 139–142. Association for Computational Linguistics.</rawString>
</citation>
<citation valid="true">
<authors>
<author>Barbara J Grosz</author>
<author>Candace L Sidner</author>
</authors>
<title>Attention, intentions, and the structure of discourse.</title>
<date>1986</date>
<journal>Computational linguistics,</journal>
<pages>12--3</pages>
<contexts>
<context position="2585" citStr="Grosz and Sidner, 1986" startWordPosition="374" endWordPosition="378">f which include: Rhetorical Structure Theory (RST; (Mann and Thompson, 1988)), which defined about 25 relations that govern clause interdependencies and ordering and give rise to text tree structures; the stepwise assembly of semantic graphs to support adductive inference toward the best explanation (Hobbs et al., 1988); Discourse Representation Theory (DRT; (Lascarides and Asher, 1991)), a formal semantic model of discourse contexts that constrain coreference and quantification scoping; the model of intentionoriented conversation blocks and their stack-based queueing to model attention flow (Grosz and Sidner, 1986), and more recently an inventory of a hundred or so binary inter-clause relations and associated annotated corpus (Penn Discourse Treebank. Work in text planning implemented some of these models, especially operationalized RST (Hovy, 1988) and explanation relations (Moore and Paris, 1989) to govern the planning of coherent paragraphs. Other computational work defined so called schemas (McKeown, 1985), frames with fixed sequences of clause types to achieve stereotypical communicative intentions. Little of this work survives. Modern research tries simply to order a collection of clauses or sente</context>
</contexts>
<marker>Grosz, Sidner, 1986</marker>
<rawString>Barbara J Grosz and Candace L Sidner. 1986. Attention, intentions, and the structure of discourse. Computational linguistics, 12(3):175–204.</rawString>
</citation>
<citation valid="true">
<authors>
<author>Barbara J Grosz</author>
<author>Scott Weinstein</author>
<author>Aravind K Joshi</author>
</authors>
<title>Centering: A framework for modeling the local coherence of discourse.</title>
<date>1995</date>
<journal>Computational linguistics,</journal>
<pages>21--2</pages>
<contexts>
<context position="6777" citStr="Grosz et al., 1995" startWordPosition="1008" endWordPosition="1011">es and the proposed model produces state-ofart performance in multiple standard evaluations for coherence models (Barzilay and Lee, 2004). The rest of this paper is organized as follows: We describe related work in Section 2, then describe how to obtain a distributed representation for sentences in Section 3, and the window composition in Section 4. Experimental results are shown in Section 5, followed by a conclusion. 2 Related Work Coherence In addition to the early computational work discussed above, local coherence was extensively studied within the modeling framework of Centering Theory (Grosz et al., 1995; Walker et al., 1998; Strube and Hahn, 1999; Poesio et al., 2004), which provides principles to form a coherence metric (Miltsakaki and Kukich, 2000; Hasler, 2004). Centering approaches suffer from a severe dependence on manually annotated input. A recent popular approach is the entity grid model introduced by Barzilay and Lapata (2008) , in which sentences are represented by a vector of discourse entities along with their grammatical roles (e.g., subject or object). Probabilities of transitions between adjacent sentences are derived from entity features and then concatenated to a document ve</context>
</contexts>
<marker>Grosz, Weinstein, Joshi, 1995</marker>
<rawString>Barbara J Grosz, Scott Weinstein, and Aravind K Joshi. 1995. Centering: A framework for modeling the local coherence of discourse. Computational linguistics, 21(2):203–225.</rawString>
</citation>
<citation valid="true">
<authors>
<author>Camille Guinaudeau</author>
<author>Michael Strube</author>
</authors>
<title>Graph-based local coherence modeling.</title>
<date>2013</date>
<booktitle>In Proceedings of the 51st Annual Meeting of the Association for Computational Linguistics,</booktitle>
<pages>93--103</pages>
<contexts>
<context position="7865" citStr="Guinaudeau and Strube, 2013" startWordPosition="1176" endWordPosition="1179"> or object). Probabilities of transitions between adjacent sentences are derived from entity features and then concatenated to a document vector representation, which is used as input to machine learning classifiers such as SVM. Many frameworks have extended the entity approach, for example, by pre-grouping entities based on semantic relatedness (Filippova and Strube, 2007) or adding more useful types of features such as coreference (Elsner and Charniak, 2008), named entities (Eisner and Charniak, 2011), and discourse relations (Lin et al., 2011). Other systems include the global graph model (Guinaudeau and Strube, 2013) which projects entities into a global graph. Louis and Nenkova (2012) introduced an HMM system in which the coherence between adjacent sentences is modeled by a hidden Markov framework captured by the 2040 Figure 2: Sentential compositionality obtained from (a) recurrent / (b) recursive neural network. The bottom layer represents word vectors in the sentence. The top layer hs denotes the resulting sentence vector. transition rules of different topics. Recurrent and Recursive Neural Networks In the context of NLP, recurrent neural networks view a sentence as a sequence of tokens and incorporat</context>
<context position="23277" citStr="Guinaudeau and Strube, 2013" startWordPosition="3722" endWordPosition="3725">number of clusters is tuned as a parameter. We directly take 7Permutations are downloaded from http: //people.csail.mit.edu/regina/coherence/ CLsubmission/. 2044 Acci Earthquake Average Recursive 0.864 0.976 0.920 Recurrent 0.840 0.951 0.895 Entity Grid 0.904 0.872 0.888 HMM 0.822 0.938 0.880 HMM+Entity 0.842 0.911 0.877 HMM+Content 0.742 0.953 0.848 Graph 0.846 0.635 0.740 Table 1: Comparison of Different Coherence Frameworks. Reported baseline results are among the best performance regarding each approach is reprinted from prior work from (Barzilay and Lapata, 2008; Louis and Nenkova, 2012; Guinaudeau and Strube, 2013). the results from Louis and Nenkova’s paper and report the best results among different combinations of parameter and feature settings8. We also report performances of models from Louis and Nenkova’s work that combine HMM and entity/content models in a unified framework. Graph Based Approach : Guinaudeau and Strube (2013) extended the entity grid model to a bipartite graph representing the text, where the entity transition information needed for local coherence computation is embedded in the bipartite graph. The Graph Based Approach outperforms the original entity approach in some of feature </context>
<context position="27496" citStr="Guinaudeau and Strube, 2013" startWordPosition="4351" endWordPosition="4354">he higher score is considered to be the more readable. As the replacement strategy for generating negative example is apparently not well fitted to this task, we adopted the following training framework: we use all sliding windows of sentences from coherent documents (documents from Britannica Elementary) as positive examples, 2045 Approach Accuracy Recurrent 0.803 Recursive 0.828 Graph Approach 0.786 Entity 0.509 S&amp;O 0.786 Entity+S&amp;O 0.888 Table 2: Comparison of Different Coherence Frameworks on Readability Assessment. Reported baselines results are are taken from (Barzilay and Lapata, 2008; Guinaudeau and Strube, 2013). S&amp;O: Schwarm and Ostendorf (2005). and cliques from Encyclopedia Britannica as negative examples, and again apply Eq. 6 for training and optimization. During testing, we turn to Equations 8 and 9 for pairwise comparison. We adopted five-fold cross-validation in the same way as in (Barzilay and Lapata, 2008; Guinaudeau and Strube, 2013) for fair comparison. Parameters were tuned within each training set also using fivefold cross-validation. Parameters to tune included window size L and regularization parameter Q. 5.3 Results We report results of the proposed approaches in the work along with </context>
</contexts>
<marker>Guinaudeau, Strube, 2013</marker>
<rawString>Camille Guinaudeau and Michael Strube. 2013. Graph-based local coherence modeling. In Proceedings of the 51st Annual Meeting of the Association for Computational Linguistics, pages 93–103.</rawString>
</citation>
<citation valid="true">
<authors>
<author>Laura Hasler</author>
</authors>
<title>An investigation into the use of centering transitions for summarisation.</title>
<date>2004</date>
<booktitle>In Proceedings of the 7th Annual CLUK Research Colloquium,</booktitle>
<pages>100--107</pages>
<contexts>
<context position="6941" citStr="Hasler, 2004" startWordPosition="1037" endWordPosition="1038">nized as follows: We describe related work in Section 2, then describe how to obtain a distributed representation for sentences in Section 3, and the window composition in Section 4. Experimental results are shown in Section 5, followed by a conclusion. 2 Related Work Coherence In addition to the early computational work discussed above, local coherence was extensively studied within the modeling framework of Centering Theory (Grosz et al., 1995; Walker et al., 1998; Strube and Hahn, 1999; Poesio et al., 2004), which provides principles to form a coherence metric (Miltsakaki and Kukich, 2000; Hasler, 2004). Centering approaches suffer from a severe dependence on manually annotated input. A recent popular approach is the entity grid model introduced by Barzilay and Lapata (2008) , in which sentences are represented by a vector of discourse entities along with their grammatical roles (e.g., subject or object). Probabilities of transitions between adjacent sentences are derived from entity features and then concatenated to a document vector representation, which is used as input to machine learning classifiers such as SVM. Many frameworks have extended the entity approach, for example, by pre-grou</context>
</contexts>
<marker>Hasler, 2004</marker>
<rawString>Laura Hasler. 2004. An investigation into the use of centering transitions for summarisation. In Proceedings of the 7th Annual CLUK Research Colloquium, pages 100–107.</rawString>
</citation>
<citation valid="true">
<authors>
<author>Jerry R Hobbs</author>
<author>Mark Stickel</author>
<author>Paul Martin</author>
<author>Douglas Edwards</author>
</authors>
<title>Interpretation as abduction.</title>
<date>1988</date>
<booktitle>In Proceedings of the 26th annual meeting on Association for Computational Linguistics,</booktitle>
<pages>95--103</pages>
<publisher>Association</publisher>
<institution>for Computational Linguistics.</institution>
<contexts>
<context position="2283" citStr="Hobbs et al., 1988" startWordPosition="333" endWordPosition="336">nn and Thompson (1988) define it, A text is coherent when it can be explained what role each clause plays with regard to the whole. 1Code available at stanford.edu/˜jiweil/ or by request from the first author. Several researchers in the 1980s and 1990s addressed the problem, the most influential of which include: Rhetorical Structure Theory (RST; (Mann and Thompson, 1988)), which defined about 25 relations that govern clause interdependencies and ordering and give rise to text tree structures; the stepwise assembly of semantic graphs to support adductive inference toward the best explanation (Hobbs et al., 1988); Discourse Representation Theory (DRT; (Lascarides and Asher, 1991)), a formal semantic model of discourse contexts that constrain coreference and quantification scoping; the model of intentionoriented conversation blocks and their stack-based queueing to model attention flow (Grosz and Sidner, 1986), and more recently an inventory of a hundred or so binary inter-clause relations and associated annotated corpus (Penn Discourse Treebank. Work in text planning implemented some of these models, especially operationalized RST (Hovy, 1988) and explanation relations (Moore and Paris, 1989) to gover</context>
</contexts>
<marker>Hobbs, Stickel, Martin, Edwards, 1988</marker>
<rawString>Jerry R Hobbs, Mark Stickel, Paul Martin, and Douglas Edwards. 1988. Interpretation as abduction. In Proceedings of the 26th annual meeting on Association for Computational Linguistics, pages 95–103. Association for Computational Linguistics.</rawString>
</citation>
<citation valid="true">
<authors>
<author>Eduard H Hovy</author>
</authors>
<title>Planning coherent multisentential text.</title>
<date>1988</date>
<booktitle>In Proceedings of the 26th annual meeting on Association for Computational Linguistics,</booktitle>
<pages>163--169</pages>
<publisher>Association</publisher>
<institution>for Computational Linguistics.</institution>
<contexts>
<context position="2824" citStr="Hovy, 1988" startWordPosition="413" endWordPosition="414">t adductive inference toward the best explanation (Hobbs et al., 1988); Discourse Representation Theory (DRT; (Lascarides and Asher, 1991)), a formal semantic model of discourse contexts that constrain coreference and quantification scoping; the model of intentionoriented conversation blocks and their stack-based queueing to model attention flow (Grosz and Sidner, 1986), and more recently an inventory of a hundred or so binary inter-clause relations and associated annotated corpus (Penn Discourse Treebank. Work in text planning implemented some of these models, especially operationalized RST (Hovy, 1988) and explanation relations (Moore and Paris, 1989) to govern the planning of coherent paragraphs. Other computational work defined so called schemas (McKeown, 1985), frames with fixed sequences of clause types to achieve stereotypical communicative intentions. Little of this work survives. Modern research tries simply to order a collection of clauses or sentences without giving an account of which order(s) is/are coherent or what the overall text structure is. The research focuses on identifying and defining a set of increasingly sophisticated features by which algorithms can be trained to pro</context>
</contexts>
<marker>Hovy, 1988</marker>
<rawString>Eduard H Hovy. 1988. Planning coherent multisentential text. In Proceedings of the 26th annual meeting on Association for Computational Linguistics, pages 163–169. Association for Computational Linguistics.</rawString>
</citation>
<citation valid="true">
<authors>
<author>Ozan Irsoy</author>
<author>Claire Cardie</author>
</authors>
<title>Bidirectional recursive neural networks for token-level labeling with structure. arXiv preprint arXiv:1312.0493.</title>
<date>2013</date>
<contexts>
<context position="10051" citStr="Irsoy and Cardie, 2013" startWordPosition="1532" endWordPosition="1535">nputs (e.g., parse trees). It computes the representation for each parent based on its children iteratively in a bottom-up fashion. A series of variations have been proposed, each tailored to different task-specific requirements, such as Matrix-Vector RNN (Socher et al., 2012) that represents every word as both a vector and a matrix, or Recursive Neural Tensor Networks (Socher et al., 2013) that allow the model to have greater 3For example, a verb and its corresponding direct object can be far away in terms of tokens if many adjectives lies in between, but they are adjacent in the parse tree (Irsoy and Cardie, 2013). interactions between the input vectors. Many tasks have benefited from this recursive framework, including parsing (Socher et al., 2011b), sentiment analysis (Socher et al., 2013), and paraphrase detection (Socher et al., 2011a). 2.1 Distributed Representations Both recurrent and recursive networks require a vector representation of each input token. Distributed representations for words were first proposed in (Rumelhart et al., 1988) and have been successful for statistical language modeling (Elman, 1990). Various deep learning architectures have been explored to learn these embeddings in a</context>
</contexts>
<marker>Irsoy, Cardie, 2013</marker>
<rawString>Ozan Irsoy and Claire Cardie. 2013. Bidirectional recursive neural networks for token-level labeling with structure. arXiv preprint arXiv:1312.0493.</rawString>
</citation>
<citation valid="true">
<authors>
<author>Nal Kalchbrenner</author>
<author>Phil Blunsom</author>
</authors>
<title>Recurrent convolutional neural networks for discourse compositionality. arXiv preprint arXiv:1306.3584.</title>
<date>2013</date>
<contexts>
<context position="5117" citStr="Kalchbrenner and Blunsom, 2013" startWordPosition="747" endWordPosition="750">ly, deep architectures, have been applied to various natural language processing tasks (see Section 2). Such deep connectionist architectures learn a dense, low-dimensional representation of their problem in a hierarchical way that is capable of capturing both semantic and syntactic aspects of tokens (e.g., (Bengio et al., 2006)), entities, N-grams (Wang and Manning, 2012), or phrases (Socher et al., 2013). More recent researches have begun looking at higher level distributed representations that transcend the token level, such as sentence-level (Le and Mikolov, 2014) or even discourse-level (Kalchbrenner and Blunsom, 2013) aspects. Just as words combine to form meaningful sentences, can we take advantage of distributional semantic representations to explore the composition of sentences to form coherent meanings in paragraphs? In this paper, we demonstrate that it is feasible to discover the coherent structure of a text using distributed sentence representations learned in a deep learning framework. Specifically, we consider a WINDOW approach for sentences, as shown in Figure 1, where positive examples are windows of sentences selected from original articles generated by humans, and negatives examples are genera</context>
</contexts>
<marker>Kalchbrenner, Blunsom, 2013</marker>
<rawString>Nal Kalchbrenner and Phil Blunsom. 2013. Recurrent convolutional neural networks for discourse compositionality. arXiv preprint arXiv:1306.3584.</rawString>
</citation>
<citation valid="true">
<authors>
<author>Mirella Lapata</author>
<author>Regina Barzilay</author>
</authors>
<title>Automatic evaluation of text coherence: Models and representations.</title>
<date>2005</date>
<booktitle>In IJCAI,</booktitle>
<volume>5</volume>
<pages>1085--1090</pages>
<contexts>
<context position="3541" citStr="Lapata and Barzilay, 2005" startWordPosition="521" endWordPosition="525">aphs. Other computational work defined so called schemas (McKeown, 1985), frames with fixed sequences of clause types to achieve stereotypical communicative intentions. Little of this work survives. Modern research tries simply to order a collection of clauses or sentences without giving an account of which order(s) is/are coherent or what the overall text structure is. The research focuses on identifying and defining a set of increasingly sophisticated features by which algorithms can be trained to propose orderings. Features being explored include the clause entities, organized into a grid (Lapata and Barzilay, 2005; Barzilay and Lapata, 2008), coreference clues to ordering (Elsner and Charniak, 2008), named-entity categories (Eisner and Charniak, 2011), syntactic features (Louis and Nenkova, 2012), and others. Besides being time-intensive (feature engineering usually requites considerable 2039 Proceedings of the 2014 Conference on Empirical Methods in Natural Language Processing (EMNLP), pages 2039–2048, October 25-29, 2014, Doha, Qatar. c�2014 Association for Computational Linguistics Figure 1: Illustrations of coherent (positive) vs not-coherent (negative) training examples. effort and can depend grea</context>
</contexts>
<marker>Lapata, Barzilay, 2005</marker>
<rawString>Mirella Lapata and Regina Barzilay. 2005. Automatic evaluation of text coherence: Models and representations. In IJCAI, volume 5, pages 1085–1090.</rawString>
</citation>
<citation valid="true">
<authors>
<author>Alex Lascarides</author>
<author>Nicholas Asher</author>
</authors>
<title>Discourse relations and defeasible knowledge.</title>
<date>1991</date>
<booktitle>In Proceedings of the 29th annual meeting on Association for Computational Linguistics,</booktitle>
<pages>55--62</pages>
<publisher>Association</publisher>
<institution>for Computational Linguistics.</institution>
<contexts>
<context position="2351" citStr="Lascarides and Asher, 1991" startWordPosition="342" endWordPosition="345"> can be explained what role each clause plays with regard to the whole. 1Code available at stanford.edu/˜jiweil/ or by request from the first author. Several researchers in the 1980s and 1990s addressed the problem, the most influential of which include: Rhetorical Structure Theory (RST; (Mann and Thompson, 1988)), which defined about 25 relations that govern clause interdependencies and ordering and give rise to text tree structures; the stepwise assembly of semantic graphs to support adductive inference toward the best explanation (Hobbs et al., 1988); Discourse Representation Theory (DRT; (Lascarides and Asher, 1991)), a formal semantic model of discourse contexts that constrain coreference and quantification scoping; the model of intentionoriented conversation blocks and their stack-based queueing to model attention flow (Grosz and Sidner, 1986), and more recently an inventory of a hundred or so binary inter-clause relations and associated annotated corpus (Penn Discourse Treebank. Work in text planning implemented some of these models, especially operationalized RST (Hovy, 1988) and explanation relations (Moore and Paris, 1989) to govern the planning of coherent paragraphs. Other computational work defi</context>
</contexts>
<marker>Lascarides, Asher, 1991</marker>
<rawString>Alex Lascarides and Nicholas Asher. 1991. Discourse relations and defeasible knowledge. In Proceedings of the 29th annual meeting on Association for Computational Linguistics, pages 55–62. Association for Computational Linguistics.</rawString>
</citation>
<citation valid="true">
<authors>
<author>Quoc Le and Tomas Mikolov</author>
</authors>
<title>Distributed representations of sentences and documents.</title>
<date>2014</date>
<contexts>
<context position="5060" citStr="Mikolov, 2014" startWordPosition="742" endWordPosition="743">e describe why they are coherent. Recently, deep architectures, have been applied to various natural language processing tasks (see Section 2). Such deep connectionist architectures learn a dense, low-dimensional representation of their problem in a hierarchical way that is capable of capturing both semantic and syntactic aspects of tokens (e.g., (Bengio et al., 2006)), entities, N-grams (Wang and Manning, 2012), or phrases (Socher et al., 2013). More recent researches have begun looking at higher level distributed representations that transcend the token level, such as sentence-level (Le and Mikolov, 2014) or even discourse-level (Kalchbrenner and Blunsom, 2013) aspects. Just as words combine to form meaningful sentences, can we take advantage of distributional semantic representations to explore the composition of sentences to form coherent meanings in paragraphs? In this paper, we demonstrate that it is feasible to discover the coherent structure of a text using distributed sentence representations learned in a deep learning framework. Specifically, we consider a WINDOW approach for sentences, as shown in Figure 1, where positive examples are windows of sentences selected from original articl</context>
</contexts>
<marker>Mikolov, 2014</marker>
<rawString>Quoc Le and Tomas Mikolov. 2014. Distributed representations of sentences and documents.</rawString>
</citation>
<citation valid="true">
<authors>
<author>Ziheng Lin</author>
<author>Hwee Tou Ng</author>
<author>Min-Yen Kan</author>
</authors>
<title>Automatically evaluating text coherence using discourse relations.</title>
<date>2011</date>
<booktitle>In Proceedings of the 49th Annual Meeting of the Association for Computational Linguistics: Human Language Technologies-Volume 1,</booktitle>
<pages>997--1006</pages>
<publisher>Association</publisher>
<institution>for Computational Linguistics.</institution>
<contexts>
<context position="7789" citStr="Lin et al., 2011" startWordPosition="1165" endWordPosition="1168">course entities along with their grammatical roles (e.g., subject or object). Probabilities of transitions between adjacent sentences are derived from entity features and then concatenated to a document vector representation, which is used as input to machine learning classifiers such as SVM. Many frameworks have extended the entity approach, for example, by pre-grouping entities based on semantic relatedness (Filippova and Strube, 2007) or adding more useful types of features such as coreference (Elsner and Charniak, 2008), named entities (Eisner and Charniak, 2011), and discourse relations (Lin et al., 2011). Other systems include the global graph model (Guinaudeau and Strube, 2013) which projects entities into a global graph. Louis and Nenkova (2012) introduced an HMM system in which the coherence between adjacent sentences is modeled by a hidden Markov framework captured by the 2040 Figure 2: Sentential compositionality obtained from (a) recurrent / (b) recursive neural network. The bottom layer represents word vectors in the sentence. The top layer hs denotes the resulting sentence vector. transition rules of different topics. Recurrent and Recursive Neural Networks In the context of NLP, recu</context>
<context position="18417" citStr="Lin et al., 2011" startWordPosition="2940" endWordPosition="2943">tribution [−�, �], \/6 where � = \/H+KxL as suggested in (Collobert et al., 2011). Wrecurrent, Vrecurrent, Wrecursive and h0 are initialized by randomly sampling from a uniform distribution U(−0.2,0.2). All bias vectors are initialized with 0. Hidden layer number H is set to 100. Word embeddings {e} are borrowed from Senna (Collobert et al., 2011; Collobert, 2011). The dimension for these embeddings is 50. 5 Experiments We evaluate the proposed coherence model on two common evaluation approaches adopted in existing work (Barzilay and Lapata, 2008; Louis and Nenkova, 2012; Elsner et al., 2007; Lin et al., 2011): Sentence Ordering and Readability Assessment. 5.1 Sentence Ordering We follow (Barzilay and Lapata, 2008; Louis and Nenkova, 2012; Elsner et al., 2007; Lin et al., 5For more details on backpropagation through RNNs, see Socher et al. (2010). {−yC log[p(yC = 1)] θ2 (6) 2043 2011) that all use pairs of articles, one containing the original document order and the other a random permutation of the sentences from the same document. The pairwise approach is predicated on the assumption that the original article is always more coherent than a random permutation; this assumption has been verified in </context>
</contexts>
<marker>Lin, Ng, Kan, 2011</marker>
<rawString>Ziheng Lin, Hwee Tou Ng, and Min-Yen Kan. 2011. Automatically evaluating text coherence using discourse relations. In Proceedings of the 49th Annual Meeting of the Association for Computational Linguistics: Human Language Technologies-Volume 1, pages 997–1006. Association for Computational Linguistics.</rawString>
</citation>
<citation valid="true">
<authors>
<author>Annie Louis</author>
<author>Ani Nenkova</author>
</authors>
<title>A coherence model based on syntactic patterns.</title>
<date>2012</date>
<booktitle>In Proceedings of the 2012 Joint Conference on Empirical Methods in Natural Language Processing and Computational Natural Language Learning,</booktitle>
<pages>1157--1168</pages>
<publisher>Association</publisher>
<institution>for Computational Linguistics.</institution>
<contexts>
<context position="3727" citStr="Louis and Nenkova, 2012" startWordPosition="546" endWordPosition="549">k survives. Modern research tries simply to order a collection of clauses or sentences without giving an account of which order(s) is/are coherent or what the overall text structure is. The research focuses on identifying and defining a set of increasingly sophisticated features by which algorithms can be trained to propose orderings. Features being explored include the clause entities, organized into a grid (Lapata and Barzilay, 2005; Barzilay and Lapata, 2008), coreference clues to ordering (Elsner and Charniak, 2008), named-entity categories (Eisner and Charniak, 2011), syntactic features (Louis and Nenkova, 2012), and others. Besides being time-intensive (feature engineering usually requites considerable 2039 Proceedings of the 2014 Conference on Empirical Methods in Natural Language Processing (EMNLP), pages 2039–2048, October 25-29, 2014, Doha, Qatar. c�2014 Association for Computational Linguistics Figure 1: Illustrations of coherent (positive) vs not-coherent (negative) training examples. effort and can depend greatly on upstream feature extraction algorithms), it is not immediately apparent which aspects of a clause or a coherent text to consider when deciding on ordering. More importantly, the f</context>
<context position="7935" citStr="Louis and Nenkova (2012)" startWordPosition="1188" endWordPosition="1191">erived from entity features and then concatenated to a document vector representation, which is used as input to machine learning classifiers such as SVM. Many frameworks have extended the entity approach, for example, by pre-grouping entities based on semantic relatedness (Filippova and Strube, 2007) or adding more useful types of features such as coreference (Elsner and Charniak, 2008), named entities (Eisner and Charniak, 2011), and discourse relations (Lin et al., 2011). Other systems include the global graph model (Guinaudeau and Strube, 2013) which projects entities into a global graph. Louis and Nenkova (2012) introduced an HMM system in which the coherence between adjacent sentences is modeled by a hidden Markov framework captured by the 2040 Figure 2: Sentential compositionality obtained from (a) recurrent / (b) recursive neural network. The bottom layer represents word vectors in the sentence. The top layer hs denotes the resulting sentence vector. transition rules of different topics. Recurrent and Recursive Neural Networks In the context of NLP, recurrent neural networks view a sentence as a sequence of tokens and incorporate information from the past (i.e., preceding tokens) (Schuster and Pal</context>
<context position="18377" citStr="Louis and Nenkova, 2012" startWordPosition="2932" endWordPosition="2935">lized by randomly drawing from the uniform distribution [−�, �], \/6 where � = \/H+KxL as suggested in (Collobert et al., 2011). Wrecurrent, Vrecurrent, Wrecursive and h0 are initialized by randomly sampling from a uniform distribution U(−0.2,0.2). All bias vectors are initialized with 0. Hidden layer number H is set to 100. Word embeddings {e} are borrowed from Senna (Collobert et al., 2011; Collobert, 2011). The dimension for these embeddings is 50. 5 Experiments We evaluate the proposed coherence model on two common evaluation approaches adopted in existing work (Barzilay and Lapata, 2008; Louis and Nenkova, 2012; Elsner et al., 2007; Lin et al., 2011): Sentence Ordering and Readability Assessment. 5.1 Sentence Ordering We follow (Barzilay and Lapata, 2008; Louis and Nenkova, 2012; Elsner et al., 2007; Lin et al., 5For more details on backpropagation through RNNs, see Socher et al. (2010). {−yC log[p(yC = 1)] θ2 (6) 2043 2011) that all use pairs of articles, one containing the original document order and the other a random permutation of the sentences from the same document. The pairwise approach is predicated on the assumption that the original article is always more coherent than a random permutatio</context>
<context position="21687" citStr="Louis and Nenkova, 2012" startWordPosition="3486" endWordPosition="3489">omly, the negative dataset contains cliques where centered sentences are replaced only by other sentences within the same document. 5.1.2 Training and Testing Despite the numerous parameters in the deep learning framework, we tune only two principal ones for each setting: window size L (tried on {3, 5, 7}) and regularization parameter Q (tried on {0.01, 0.1, 0.25, 0.5,1.0,1.25, 2.0, 2.5, 5.0}). We trained parameters using 10-fold cross-validation on the training data. Concretely, in each setting, 90 documents were used for training and evaluation was done on the remaining articles, following (Louis and Nenkova, 2012). After tuning, the final model was tested on the testing set. 5.1.3 Model Comparison We report performance of recursive and recurrent networks. We also report results from some popular approaches in the literature, including: Entity Grid Model : Grid model (Barzilay and Lapata, 2008) obtains the best performance when coreference resolution, expressive syntactic information, and salience-based features are incorporated. Entity grid models represent each sentence as a column of a grid of features and apply machine learning methods (e.g., SVM) to identify the coherent transitions based on entity</context>
<context position="23247" citStr="Louis and Nenkova, 2012" startWordPosition="3718" endWordPosition="3721">red in advance where the number of clusters is tuned as a parameter. We directly take 7Permutations are downloaded from http: //people.csail.mit.edu/regina/coherence/ CLsubmission/. 2044 Acci Earthquake Average Recursive 0.864 0.976 0.920 Recurrent 0.840 0.951 0.895 Entity Grid 0.904 0.872 0.888 HMM 0.822 0.938 0.880 HMM+Entity 0.842 0.911 0.877 HMM+Content 0.742 0.953 0.848 Graph 0.846 0.635 0.740 Table 1: Comparison of Different Coherence Frameworks. Reported baseline results are among the best performance regarding each approach is reprinted from prior work from (Barzilay and Lapata, 2008; Louis and Nenkova, 2012; Guinaudeau and Strube, 2013). the results from Louis and Nenkova’s paper and report the best results among different combinations of parameter and feature settings8. We also report performances of models from Louis and Nenkova’s work that combine HMM and entity/content models in a unified framework. Graph Based Approach : Guinaudeau and Strube (2013) extended the entity grid model to a bipartite graph representing the text, where the entity transition information needed for local coherence computation is embedded in the bipartite graph. The Graph Based Approach outperforms the original entit</context>
<context position="25033" citStr="Louis and Nenkova, 2012" startWordPosition="3991" endWordPosition="3994">ality of the deep learning framework is that it can be trained easily and makes unnecessary the effort required of feature engineering. In contrast, almost all existing baselines and other coherence methods require sophisticated feature selection processes and greatly rely on external feature extraction algorithm. The recurrent network is easier to implement than the recursive network and does not rely on external resources (i.e., parse trees), but the recursive network obtains better performance by build8The details for information about parameter and feature of best setting can be found in (Louis and Nenkova, 2012). ing the convolution on parse trees rather than simply piling up terms within the sentence, which is in line with common expectation. Both recurrent and recursive models obtain better performance on the Earthquake than the Accident dataset. Scrutiny of the corpus reveals that articles reporting earthquakes exhibit a more consistent structure: earthquake outbreak, describing the center and intensity of the earthquake, injuries and rescue operations, etc., while accident articles usually exhibit more diverse scenarios. 5.2 Readability Assessment Barzilay and Lapata (2008) proposed a readability</context>
</contexts>
<marker>Louis, Nenkova, 2012</marker>
<rawString>Annie Louis and Ani Nenkova. 2012. A coherence model based on syntactic patterns. In Proceedings of the 2012 Joint Conference on Empirical Methods in Natural Language Processing and Computational Natural Language Learning, pages 1157–1168. Association for Computational Linguistics.</rawString>
</citation>
<citation valid="true">
<authors>
<author>William C Mann</author>
<author>Sandra A Thompson</author>
</authors>
<title>Rhetorical structure theory: Toward a functional theory of text organization.</title>
<date>1988</date>
<tech>Text, 8(3):243–281.</tech>
<contexts>
<context position="1686" citStr="Mann and Thompson (1988)" startWordPosition="237" endWordPosition="240">ure engineering, and learns sentence representations, which are to some extent able to capture the ‘rules’ governing coherent sentence structure. The proposed approach outperforms existing baselines and generates the stateof-art performance in standard coherence evaluation tasks1. 1 Introduction Coherence is a central aspect in natural language processing of multi-sentence texts. It is essential in generating readable text that the text planner compute which ordering of clauses (or sentences; we use them interchangeably in this paper) is likely to support understanding and avoid confusion. As Mann and Thompson (1988) define it, A text is coherent when it can be explained what role each clause plays with regard to the whole. 1Code available at stanford.edu/˜jiweil/ or by request from the first author. Several researchers in the 1980s and 1990s addressed the problem, the most influential of which include: Rhetorical Structure Theory (RST; (Mann and Thompson, 1988)), which defined about 25 relations that govern clause interdependencies and ordering and give rise to text tree structures; the stepwise assembly of semantic graphs to support adductive inference toward the best explanation (Hobbs et al., 1988); D</context>
</contexts>
<marker>Mann, Thompson, 1988</marker>
<rawString>William C Mann and Sandra A Thompson. 1988. Rhetorical structure theory: Toward a functional theory of text organization. Text, 8(3):243–281.</rawString>
</citation>
<citation valid="true">
<authors>
<author>Kathleen R McKeown</author>
</authors>
<title>Discourse strategies for generating natural-language text.</title>
<date>1985</date>
<journal>Artificial Intelligence,</journal>
<volume>27</volume>
<issue>1</issue>
<contexts>
<context position="2988" citStr="McKeown, 1985" startWordPosition="437" endWordPosition="438">del of discourse contexts that constrain coreference and quantification scoping; the model of intentionoriented conversation blocks and their stack-based queueing to model attention flow (Grosz and Sidner, 1986), and more recently an inventory of a hundred or so binary inter-clause relations and associated annotated corpus (Penn Discourse Treebank. Work in text planning implemented some of these models, especially operationalized RST (Hovy, 1988) and explanation relations (Moore and Paris, 1989) to govern the planning of coherent paragraphs. Other computational work defined so called schemas (McKeown, 1985), frames with fixed sequences of clause types to achieve stereotypical communicative intentions. Little of this work survives. Modern research tries simply to order a collection of clauses or sentences without giving an account of which order(s) is/are coherent or what the overall text structure is. The research focuses on identifying and defining a set of increasingly sophisticated features by which algorithms can be trained to propose orderings. Features being explored include the clause entities, organized into a grid (Lapata and Barzilay, 2005; Barzilay and Lapata, 2008), coreference clues</context>
</contexts>
<marker>McKeown, 1985</marker>
<rawString>Kathleen R McKeown. 1985. Discourse strategies for generating natural-language text. Artificial Intelligence, 27(1):1–41.</rawString>
</citation>
<citation valid="true">
<authors>
<author>Gr´egoire Mesnil</author>
<author>Xiaodong He</author>
<author>Li Deng</author>
<author>Yoshua Bengio</author>
</authors>
<title>Investigation of recurrent-neuralnetwork architectures and learning methods for spoken language understanding.</title>
<date>2013</date>
<journal>Interspeech.</journal>
<contexts>
<context position="8929" citStr="Mesnil et al., 2013" startWordPosition="1347" endWordPosition="1350">rent topics. Recurrent and Recursive Neural Networks In the context of NLP, recurrent neural networks view a sentence as a sequence of tokens and incorporate information from the past (i.e., preceding tokens) (Schuster and Paliwal, 1997; Sutskever et al., 2011) for acquisition of the current output. At each step, the recurrent network takes as input both the output of previous steps and the current token, convolutes the inputs, and forwards the result to the next step. It has been successfully applied to tasks such as language modeling (Mikolov et al., 2010) and spoken language understanding (Mesnil et al., 2013). The advantage of recurrent network is that it does not depend on external deeper structure (e.g., parse tree) and is easy to implement. However, in the recurrent framework, long-distance dependencies are difficult to capture due to the vanishing gradient problem (Bengio et al., 1994); two tokens may be structurally close to each other, even though they are far away in word sequence3. Recursive neural networks comprise another class of architecture, one that relies and operates on structured inputs (e.g., parse trees). It computes the representation for each parent based on its children itera</context>
</contexts>
<marker>Mesnil, He, Deng, Bengio, 2013</marker>
<rawString>Gr´egoire Mesnil, Xiaodong He, Li Deng, and Yoshua Bengio. 2013. Investigation of recurrent-neuralnetwork architectures and learning methods for spoken language understanding. Interspeech.</rawString>
</citation>
<citation valid="true">
<authors>
<author>Tomas Mikolov</author>
<author>Martin Karafi´at</author>
<author>Lukas Burget</author>
<author>Jan Cernock`y</author>
<author>Sanjeev Khudanpur</author>
</authors>
<title>Recurrent neural network based language model.</title>
<date>2010</date>
<booktitle>In INTERSPEECH,</booktitle>
<pages>1045--1048</pages>
<marker>Mikolov, Karafi´at, Burget, Cernock`y, Khudanpur, 2010</marker>
<rawString>Tomas Mikolov, Martin Karafi´at, Lukas Burget, Jan Cernock`y, and Sanjeev Khudanpur. 2010. Recurrent neural network based language model. In INTERSPEECH, pages 1045–1048.</rawString>
</citation>
<citation valid="true">
<authors>
<author>Tomas Mikolov</author>
<author>Kai Chen</author>
<author>Greg Corrado</author>
<author>Jeffrey Dean</author>
</authors>
<title>Efficient estimation of word representations in vector space. arXiv preprint arXiv:1301.3781.</title>
<date>2013</date>
<contexts>
<context position="10787" citStr="Mikolov et al., 2013" startWordPosition="1643" endWordPosition="1646">g (Socher et al., 2011b), sentiment analysis (Socher et al., 2013), and paraphrase detection (Socher et al., 2011a). 2.1 Distributed Representations Both recurrent and recursive networks require a vector representation of each input token. Distributed representations for words were first proposed in (Rumelhart et al., 1988) and have been successful for statistical language modeling (Elman, 1990). Various deep learning architectures have been explored to learn these embeddings in an unsupervised manner from a large corpus (Bengio et al., 2006; Collobert and Weston, 2008; Mnih and Hinton, 2007; Mikolov et al., 2013), which might have different generalization capabilities and are able to capture the semantic meanings depending on the specific task at hand. These vector representations can to some extent capture interesting semantic relationships, such as King−man ≈ Queue−woman (Mikolov et al., 2010), and recently have been successfully used in various NLP applications, including named entity recognition, tagging, segmentation (Wang et al., 2013), and machine translation (e.g.,(Collobert and Weston, 2008; Zou et al., 2013)). 3 Sentence Model In this section, we demonstrate the strategy adopted to compute a</context>
</contexts>
<marker>Mikolov, Chen, Corrado, Dean, 2013</marker>
<rawString>Tomas Mikolov, Kai Chen, Greg Corrado, and Jeffrey Dean. 2013. Efficient estimation of word representations in vector space. arXiv preprint arXiv:1301.3781.</rawString>
</citation>
<citation valid="true">
<authors>
<author>Eleni Miltsakaki</author>
<author>Karen Kukich</author>
</authors>
<title>The role of centering theory’s rough-shift in the teaching and evaluation of writing skills.</title>
<date>2000</date>
<booktitle>In Proceedings of the 38th Annual Meeting on Association for Computational Linguistics,</booktitle>
<pages>408--415</pages>
<publisher>Association</publisher>
<institution>for Computational Linguistics.</institution>
<contexts>
<context position="6926" citStr="Miltsakaki and Kukich, 2000" startWordPosition="1033" endWordPosition="1036">he rest of this paper is organized as follows: We describe related work in Section 2, then describe how to obtain a distributed representation for sentences in Section 3, and the window composition in Section 4. Experimental results are shown in Section 5, followed by a conclusion. 2 Related Work Coherence In addition to the early computational work discussed above, local coherence was extensively studied within the modeling framework of Centering Theory (Grosz et al., 1995; Walker et al., 1998; Strube and Hahn, 1999; Poesio et al., 2004), which provides principles to form a coherence metric (Miltsakaki and Kukich, 2000; Hasler, 2004). Centering approaches suffer from a severe dependence on manually annotated input. A recent popular approach is the entity grid model introduced by Barzilay and Lapata (2008) , in which sentences are represented by a vector of discourse entities along with their grammatical roles (e.g., subject or object). Probabilities of transitions between adjacent sentences are derived from entity features and then concatenated to a document vector representation, which is used as input to machine learning classifiers such as SVM. Many frameworks have extended the entity approach, for examp</context>
</contexts>
<marker>Miltsakaki, Kukich, 2000</marker>
<rawString>Eleni Miltsakaki and Karen Kukich. 2000. The role of centering theory’s rough-shift in the teaching and evaluation of writing skills. In Proceedings of the 38th Annual Meeting on Association for Computational Linguistics, pages 408–415. Association for Computational Linguistics.</rawString>
</citation>
<citation valid="true">
<authors>
<author>Andriy Mnih</author>
<author>Geoffrey Hinton</author>
</authors>
<title>Three new graphical models for statistical language modelling.</title>
<date>2007</date>
<booktitle>In Proceedings of the 24th international conference on Machine learning,</booktitle>
<pages>641--648</pages>
<publisher>ACM.</publisher>
<contexts>
<context position="10764" citStr="Mnih and Hinton, 2007" startWordPosition="1639" endWordPosition="1642">ework, including parsing (Socher et al., 2011b), sentiment analysis (Socher et al., 2013), and paraphrase detection (Socher et al., 2011a). 2.1 Distributed Representations Both recurrent and recursive networks require a vector representation of each input token. Distributed representations for words were first proposed in (Rumelhart et al., 1988) and have been successful for statistical language modeling (Elman, 1990). Various deep learning architectures have been explored to learn these embeddings in an unsupervised manner from a large corpus (Bengio et al., 2006; Collobert and Weston, 2008; Mnih and Hinton, 2007; Mikolov et al., 2013), which might have different generalization capabilities and are able to capture the semantic meanings depending on the specific task at hand. These vector representations can to some extent capture interesting semantic relationships, such as King−man ≈ Queue−woman (Mikolov et al., 2010), and recently have been successfully used in various NLP applications, including named entity recognition, tagging, segmentation (Wang et al., 2013), and machine translation (e.g.,(Collobert and Weston, 2008; Zou et al., 2013)). 3 Sentence Model In this section, we demonstrate the strate</context>
</contexts>
<marker>Mnih, Hinton, 2007</marker>
<rawString>Andriy Mnih and Geoffrey Hinton. 2007. Three new graphical models for statistical language modelling. In Proceedings of the 24th international conference on Machine learning, pages 641–648. ACM.</rawString>
</citation>
<citation valid="true">
<authors>
<author>Johanna D Moore</author>
<author>Cecile L Paris</author>
</authors>
<title>Planning text for advisory dialogues.</title>
<date>1989</date>
<booktitle>In Proceedings of the 27th annual meeting on Association for Computational Linguistics,</booktitle>
<pages>203--211</pages>
<publisher>Association</publisher>
<institution>for Computational Linguistics.</institution>
<contexts>
<context position="2874" citStr="Moore and Paris, 1989" startWordPosition="418" endWordPosition="421">xplanation (Hobbs et al., 1988); Discourse Representation Theory (DRT; (Lascarides and Asher, 1991)), a formal semantic model of discourse contexts that constrain coreference and quantification scoping; the model of intentionoriented conversation blocks and their stack-based queueing to model attention flow (Grosz and Sidner, 1986), and more recently an inventory of a hundred or so binary inter-clause relations and associated annotated corpus (Penn Discourse Treebank. Work in text planning implemented some of these models, especially operationalized RST (Hovy, 1988) and explanation relations (Moore and Paris, 1989) to govern the planning of coherent paragraphs. Other computational work defined so called schemas (McKeown, 1985), frames with fixed sequences of clause types to achieve stereotypical communicative intentions. Little of this work survives. Modern research tries simply to order a collection of clauses or sentences without giving an account of which order(s) is/are coherent or what the overall text structure is. The research focuses on identifying and defining a set of increasingly sophisticated features by which algorithms can be trained to propose orderings. Features being explored include th</context>
</contexts>
<marker>Moore, Paris, 1989</marker>
<rawString>Johanna D Moore and Cecile L Paris. 1989. Planning text for advisory dialogues. In Proceedings of the 27th annual meeting on Association for Computational Linguistics, pages 203–211. Association for Computational Linguistics.</rawString>
</citation>
<citation valid="true">
<authors>
<author>Wenzhe Pei</author>
<author>Tao Ge</author>
<author>Chang Baobao</author>
</authors>
<title>Maxmargin tensor neural network for chinese word segmentation.</title>
<date>2014</date>
<booktitle>In Proceedings of ACL.</booktitle>
<contexts>
<context position="17239" citStr="Pei et al., 2014" startWordPosition="2742" endWordPosition="2745">raining samples, the cost function for recurrent neural network with regularization on the training set is given by: 1 � J(Θ) = M CEtrainset � − (1 − yC) log[1 − p(yC = 1)]} + Q 2M BEO where Θ = [WRecurrent, Wsen, Usen] The regularization part is paralyzed by Q to avoid overfitting. A similar loss function is applied to the recursive network with only minor parameter altering that is excluded for brevity. To minimize the objective J(Θ), we use the diagonal variant of AdaGrad (Duchi et al., 2011) with minibatches, which is widely applied in deep learning literature (e.g.,(Socher et al., 2011a; Pei et al., 2014)). The learning rate in AdaGrad is adapting differently for different parameters at different steps. Concretely, for parameter updates, let giτ denote the subgradient at time step for parameter θi, which is obtained from backpropagation5, the parameter update at time step t is given by: ( θτ = θτ−1 α �τ V i2 gτi l7) t=o gτ where α denotes the learning rate and is set to 0.01 in our approach. Optimal performance is achieved when batch size is set between 20 and 30. 4.3 Initialization Elements in Wsen are initialized by randomly drawing from the uniform distribution [−�, �], \/6 where � = \/H+Kx</context>
</contexts>
<marker>Pei, Ge, Baobao, 2014</marker>
<rawString>Wenzhe Pei, Tao Ge, and Chang Baobao. 2014. Maxmargin tensor neural network for chinese word segmentation. In Proceedings of ACL.</rawString>
</citation>
<citation valid="true">
<authors>
<author>Massimo Poesio</author>
<author>Rosemary Stevenson</author>
<author>Barbara Di Eugenio</author>
<author>Janet Hitzeman</author>
</authors>
<title>Centering: A parametric theory and its instantiations.</title>
<date>2004</date>
<journal>Computational linguistics,</journal>
<volume>30</volume>
<issue>3</issue>
<marker>Poesio, Stevenson, Di Eugenio, Hitzeman, 2004</marker>
<rawString>Massimo Poesio, Rosemary Stevenson, Barbara Di Eugenio, and Janet Hitzeman. 2004. Centering: A parametric theory and its instantiations. Computational linguistics, 30(3):309–363.</rawString>
</citation>
<citation valid="true">
<authors>
<author>David E Rumelhart</author>
<author>Geoffrey E Hinton</author>
<author>Ronald J Williams</author>
</authors>
<title>Learning representations by backpropagating errors.</title>
<date>1988</date>
<publisher>MIT Press,</publisher>
<location>Cambridge, MA, USA.</location>
<contexts>
<context position="10491" citStr="Rumelhart et al., 1988" startWordPosition="1596" endWordPosition="1599">ple, a verb and its corresponding direct object can be far away in terms of tokens if many adjectives lies in between, but they are adjacent in the parse tree (Irsoy and Cardie, 2013). interactions between the input vectors. Many tasks have benefited from this recursive framework, including parsing (Socher et al., 2011b), sentiment analysis (Socher et al., 2013), and paraphrase detection (Socher et al., 2011a). 2.1 Distributed Representations Both recurrent and recursive networks require a vector representation of each input token. Distributed representations for words were first proposed in (Rumelhart et al., 1988) and have been successful for statistical language modeling (Elman, 1990). Various deep learning architectures have been explored to learn these embeddings in an unsupervised manner from a large corpus (Bengio et al., 2006; Collobert and Weston, 2008; Mnih and Hinton, 2007; Mikolov et al., 2013), which might have different generalization capabilities and are able to capture the semantic meanings depending on the specific task at hand. These vector representations can to some extent capture interesting semantic relationships, such as King−man ≈ Queue−woman (Mikolov et al., 2010), and recently h</context>
</contexts>
<marker>Rumelhart, Hinton, Williams, 1988</marker>
<rawString>David E Rumelhart, Geoffrey E Hinton, and Ronald J Williams. 1988. Learning representations by backpropagating errors. MIT Press, Cambridge, MA, USA.</rawString>
</citation>
<citation valid="true">
<authors>
<author>Mike Schuster</author>
<author>Kuldip K Paliwal</author>
</authors>
<title>Bidirectional recurrent neural networks.</title>
<date>1997</date>
<journal>Signal Processing, IEEE Transactions on,</journal>
<volume>45</volume>
<issue>11</issue>
<contexts>
<context position="8545" citStr="Schuster and Paliwal, 1997" startWordPosition="1282" endWordPosition="1285">d Nenkova (2012) introduced an HMM system in which the coherence between adjacent sentences is modeled by a hidden Markov framework captured by the 2040 Figure 2: Sentential compositionality obtained from (a) recurrent / (b) recursive neural network. The bottom layer represents word vectors in the sentence. The top layer hs denotes the resulting sentence vector. transition rules of different topics. Recurrent and Recursive Neural Networks In the context of NLP, recurrent neural networks view a sentence as a sequence of tokens and incorporate information from the past (i.e., preceding tokens) (Schuster and Paliwal, 1997; Sutskever et al., 2011) for acquisition of the current output. At each step, the recurrent network takes as input both the output of previous steps and the current token, convolutes the inputs, and forwards the result to the next step. It has been successfully applied to tasks such as language modeling (Mikolov et al., 2010) and spoken language understanding (Mesnil et al., 2013). The advantage of recurrent network is that it does not depend on external deeper structure (e.g., parse tree) and is easy to implement. However, in the recurrent framework, long-distance dependencies are difficult </context>
</contexts>
<marker>Schuster, Paliwal, 1997</marker>
<rawString>Mike Schuster and Kuldip K Paliwal. 1997. Bidirectional recurrent neural networks. Signal Processing, IEEE Transactions on, 45(11):2673–2681.</rawString>
</citation>
<citation valid="true">
<authors>
<author>Sarah E Schwarm</author>
<author>Mari Ostendorf</author>
</authors>
<title>Reading level assessment using support vector machines and statistical language models.</title>
<date>2005</date>
<booktitle>In Proceedings of the 43rd Annual Meeting on Association for Computational Linguistics,</booktitle>
<pages>523--530</pages>
<publisher>Association</publisher>
<institution>for Computational Linguistics.</institution>
<contexts>
<context position="27531" citStr="Schwarm and Ostendorf (2005)" startWordPosition="4356" endWordPosition="4359"> the more readable. As the replacement strategy for generating negative example is apparently not well fitted to this task, we adopted the following training framework: we use all sliding windows of sentences from coherent documents (documents from Britannica Elementary) as positive examples, 2045 Approach Accuracy Recurrent 0.803 Recursive 0.828 Graph Approach 0.786 Entity 0.509 S&amp;O 0.786 Entity+S&amp;O 0.888 Table 2: Comparison of Different Coherence Frameworks on Readability Assessment. Reported baselines results are are taken from (Barzilay and Lapata, 2008; Guinaudeau and Strube, 2013). S&amp;O: Schwarm and Ostendorf (2005). and cliques from Encyclopedia Britannica as negative examples, and again apply Eq. 6 for training and optimization. During testing, we turn to Equations 8 and 9 for pairwise comparison. We adopted five-fold cross-validation in the same way as in (Barzilay and Lapata, 2008; Guinaudeau and Strube, 2013) for fair comparison. Parameters were tuned within each training set also using fivefold cross-validation. Parameters to tune included window size L and regularization parameter Q. 5.3 Results We report results of the proposed approaches in the work along with entity model (Barzilay and Lapata, </context>
</contexts>
<marker>Schwarm, Ostendorf, 2005</marker>
<rawString>Sarah E Schwarm and Mari Ostendorf. 2005. Reading level assessment using support vector machines and statistical language models. In Proceedings of the 43rd Annual Meeting on Association for Computational Linguistics, pages 523–530. Association for Computational Linguistics.</rawString>
</citation>
<citation valid="true">
<authors>
<author>Noah A Smith</author>
<author>Jason Eisner</author>
</authors>
<title>Contrastive estimation: Training log-linear models on unlabeled data.</title>
<date>2005</date>
<booktitle>In Proceedings of the 43rd Annual Meeting on Association for Computational Linguistics,</booktitle>
<pages>354--362</pages>
<publisher>Association</publisher>
<institution>for Computational Linguistics.</institution>
<contexts>
<context position="15465" citStr="Smith and Eisner, 2005" startWordPosition="2429" endWordPosition="2432">ber of neurons in the hidden (second) layer. Then each of the hidden layers takes as input hC and performs the convolution using a non-linear tanh function, parametrized by Wsen and bsen. The concatenating output vector for hidden layers, defined as qC, can therefore be rewritten as: qC = f(Wsen x hC + bsen) (4) where Wsen is a H x (L x K) dimensional matrix and bsen is a H x 1 dimensional bias vector. 4instead of a binary classification (correct/incorrect), another commonly used approach is the contrastive approach that minimizes the score function max(0, 1 − s + s.) (Collobert et al., 2011; Smith and Eisner, 2005). s denotes the score of a true (coherent) window and s. the score of a corrupt (containing incoherence) one) in an attempt to make the score of true windows larger and corrupt windows smaller. We tried the contrastive one for both recurrent and recursive networks but the binary approach constantly outperformed the contrastive one in this task. ht = f(VRecurrent&apos;ht−1+WRecurrent&apos;etw+bRecurrent) (1) where WRecurrent and VRecurrent are K x K matrixes. bRecurrent denotes K x 1 bias vector and f = tanh is a standard element-wise nonlinearity. Note that calculation for representation at time t = 1 i</context>
</contexts>
<marker>Smith, Eisner, 2005</marker>
<rawString>Noah A Smith and Jason Eisner. 2005. Contrastive estimation: Training log-linear models on unlabeled data. In Proceedings of the 43rd Annual Meeting on Association for Computational Linguistics, pages 354–362. Association for Computational Linguistics.</rawString>
</citation>
<citation valid="true">
<authors>
<author>Richard Socher</author>
<author>Christopher D Manning</author>
<author>Andrew Y Ng</author>
</authors>
<title>Learning continuous phrase representations and syntactic parsing with recursive neural networks.</title>
<date>2010</date>
<booktitle>In Proceedings of the NIPS-2010 Deep Learning and Unsupervised Feature Learning Workshop,</booktitle>
<pages>1--9</pages>
<contexts>
<context position="18658" citStr="Socher et al. (2010)" startWordPosition="2978" endWordPosition="2981">th 0. Hidden layer number H is set to 100. Word embeddings {e} are borrowed from Senna (Collobert et al., 2011; Collobert, 2011). The dimension for these embeddings is 50. 5 Experiments We evaluate the proposed coherence model on two common evaluation approaches adopted in existing work (Barzilay and Lapata, 2008; Louis and Nenkova, 2012; Elsner et al., 2007; Lin et al., 2011): Sentence Ordering and Readability Assessment. 5.1 Sentence Ordering We follow (Barzilay and Lapata, 2008; Louis and Nenkova, 2012; Elsner et al., 2007; Lin et al., 5For more details on backpropagation through RNNs, see Socher et al. (2010). {−yC log[p(yC = 1)] θ2 (6) 2043 2011) that all use pairs of articles, one containing the original document order and the other a random permutation of the sentences from the same document. The pairwise approach is predicated on the assumption that the original article is always more coherent than a random permutation; this assumption has been verified in Lin et al.’s work (2011). We need to define the coherence score Sd for a given document d, where d is comprised of a series of sentences, d = {s1, s2,.., sNd}, and Nd denotes the number of sentences within d. Based on our clique definition, </context>
</contexts>
<marker>Socher, Manning, Ng, 2010</marker>
<rawString>Richard Socher, Christopher D Manning, and Andrew Y Ng. 2010. Learning continuous phrase representations and syntactic parsing with recursive neural networks. In Proceedings of the NIPS-2010 Deep Learning and Unsupervised Feature Learning Workshop, pages 1–9.</rawString>
</citation>
<citation valid="true">
<authors>
<author>Richard Socher</author>
<author>Eric H Huang</author>
<author>Jeffrey Pennington</author>
<author>Andrew Y Ng</author>
<author>Christopher D Manning</author>
</authors>
<title>Dynamic pooling and unfolding recursive autoencoders for paraphrase detection.</title>
<date>2011</date>
<booktitle>In NIPS,</booktitle>
<volume>24</volume>
<pages>801--809</pages>
<contexts>
<context position="10188" citStr="Socher et al., 2011" startWordPosition="1552" endWordPosition="1555">of variations have been proposed, each tailored to different task-specific requirements, such as Matrix-Vector RNN (Socher et al., 2012) that represents every word as both a vector and a matrix, or Recursive Neural Tensor Networks (Socher et al., 2013) that allow the model to have greater 3For example, a verb and its corresponding direct object can be far away in terms of tokens if many adjectives lies in between, but they are adjacent in the parse tree (Irsoy and Cardie, 2013). interactions between the input vectors. Many tasks have benefited from this recursive framework, including parsing (Socher et al., 2011b), sentiment analysis (Socher et al., 2013), and paraphrase detection (Socher et al., 2011a). 2.1 Distributed Representations Both recurrent and recursive networks require a vector representation of each input token. Distributed representations for words were first proposed in (Rumelhart et al., 1988) and have been successful for statistical language modeling (Elman, 1990). Various deep learning architectures have been explored to learn these embeddings in an unsupervised manner from a large corpus (Bengio et al., 2006; Collobert and Weston, 2008; Mnih and Hinton, 2007; Mikolov et al., 2013),</context>
<context position="17219" citStr="Socher et al., 2011" startWordPosition="2738" endWordPosition="2741">k, suppose we have M training samples, the cost function for recurrent neural network with regularization on the training set is given by: 1 � J(Θ) = M CEtrainset � − (1 − yC) log[1 − p(yC = 1)]} + Q 2M BEO where Θ = [WRecurrent, Wsen, Usen] The regularization part is paralyzed by Q to avoid overfitting. A similar loss function is applied to the recursive network with only minor parameter altering that is excluded for brevity. To minimize the objective J(Θ), we use the diagonal variant of AdaGrad (Duchi et al., 2011) with minibatches, which is widely applied in deep learning literature (e.g.,(Socher et al., 2011a; Pei et al., 2014)). The learning rate in AdaGrad is adapting differently for different parameters at different steps. Concretely, for parameter updates, let giτ denote the subgradient at time step for parameter θi, which is obtained from backpropagation5, the parameter update at time step t is given by: ( θτ = θτ−1 α �τ V i2 gτi l7) t=o gτ where α denotes the learning rate and is set to 0.01 in our approach. Optimal performance is achieved when batch size is set between 20 and 30. 4.3 Initialization Elements in Wsen are initialized by randomly drawing from the uniform distribution [−�, �], </context>
</contexts>
<marker>Socher, Huang, Pennington, Ng, Manning, 2011</marker>
<rawString>Richard Socher, Eric H Huang, Jeffrey Pennington, Andrew Y Ng, and Christopher D Manning. 2011a. Dynamic pooling and unfolding recursive autoencoders for paraphrase detection. In NIPS, volume 24, pages 801–809.</rawString>
</citation>
<citation valid="true">
<authors>
<author>Richard Socher</author>
<author>Cliff C Lin</author>
<author>Chris Manning</author>
<author>Andrew Y Ng</author>
</authors>
<title>Parsing natural scenes and natural language with recursive neural networks.</title>
<date>2011</date>
<booktitle>In Proceedings of the 28th International Conference on Machine Learning (ICML-11),</booktitle>
<pages>129--136</pages>
<contexts>
<context position="10188" citStr="Socher et al., 2011" startWordPosition="1552" endWordPosition="1555">of variations have been proposed, each tailored to different task-specific requirements, such as Matrix-Vector RNN (Socher et al., 2012) that represents every word as both a vector and a matrix, or Recursive Neural Tensor Networks (Socher et al., 2013) that allow the model to have greater 3For example, a verb and its corresponding direct object can be far away in terms of tokens if many adjectives lies in between, but they are adjacent in the parse tree (Irsoy and Cardie, 2013). interactions between the input vectors. Many tasks have benefited from this recursive framework, including parsing (Socher et al., 2011b), sentiment analysis (Socher et al., 2013), and paraphrase detection (Socher et al., 2011a). 2.1 Distributed Representations Both recurrent and recursive networks require a vector representation of each input token. Distributed representations for words were first proposed in (Rumelhart et al., 1988) and have been successful for statistical language modeling (Elman, 1990). Various deep learning architectures have been explored to learn these embeddings in an unsupervised manner from a large corpus (Bengio et al., 2006; Collobert and Weston, 2008; Mnih and Hinton, 2007; Mikolov et al., 2013),</context>
<context position="17219" citStr="Socher et al., 2011" startWordPosition="2738" endWordPosition="2741">k, suppose we have M training samples, the cost function for recurrent neural network with regularization on the training set is given by: 1 � J(Θ) = M CEtrainset � − (1 − yC) log[1 − p(yC = 1)]} + Q 2M BEO where Θ = [WRecurrent, Wsen, Usen] The regularization part is paralyzed by Q to avoid overfitting. A similar loss function is applied to the recursive network with only minor parameter altering that is excluded for brevity. To minimize the objective J(Θ), we use the diagonal variant of AdaGrad (Duchi et al., 2011) with minibatches, which is widely applied in deep learning literature (e.g.,(Socher et al., 2011a; Pei et al., 2014)). The learning rate in AdaGrad is adapting differently for different parameters at different steps. Concretely, for parameter updates, let giτ denote the subgradient at time step for parameter θi, which is obtained from backpropagation5, the parameter update at time step t is given by: ( θτ = θτ−1 α �τ V i2 gτi l7) t=o gτ where α denotes the learning rate and is set to 0.01 in our approach. Optimal performance is achieved when batch size is set between 20 and 30. 4.3 Initialization Elements in Wsen are initialized by randomly drawing from the uniform distribution [−�, �], </context>
</contexts>
<marker>Socher, Lin, Manning, Ng, 2011</marker>
<rawString>Richard Socher, Cliff C Lin, Chris Manning, and Andrew Y Ng. 2011b. Parsing natural scenes and natural language with recursive neural networks. In Proceedings of the 28th International Conference on Machine Learning (ICML-11), pages 129–136.</rawString>
</citation>
<citation valid="true">
<authors>
<author>Richard Socher</author>
<author>Brody Huval</author>
<author>Christopher D Manning</author>
<author>Andrew Y Ng</author>
</authors>
<title>Semantic compositionality through recursive matrix-vector spaces.</title>
<date>2012</date>
<booktitle>In Proceedings of the 2012 Joint Conference on Empirical Methods in Natural Language Processing and Computational Natural Language Learning,</booktitle>
<pages>1201--1211</pages>
<publisher>Association</publisher>
<institution>for Computational Linguistics.</institution>
<contexts>
<context position="9705" citStr="Socher et al., 2012" startWordPosition="1469" endWordPosition="1472">current framework, long-distance dependencies are difficult to capture due to the vanishing gradient problem (Bengio et al., 1994); two tokens may be structurally close to each other, even though they are far away in word sequence3. Recursive neural networks comprise another class of architecture, one that relies and operates on structured inputs (e.g., parse trees). It computes the representation for each parent based on its children iteratively in a bottom-up fashion. A series of variations have been proposed, each tailored to different task-specific requirements, such as Matrix-Vector RNN (Socher et al., 2012) that represents every word as both a vector and a matrix, or Recursive Neural Tensor Networks (Socher et al., 2013) that allow the model to have greater 3For example, a verb and its corresponding direct object can be far away in terms of tokens if many adjectives lies in between, but they are adjacent in the parse tree (Irsoy and Cardie, 2013). interactions between the input vectors. Many tasks have benefited from this recursive framework, including parsing (Socher et al., 2011b), sentiment analysis (Socher et al., 2013), and paraphrase detection (Socher et al., 2011a). 2.1 Distributed Repres</context>
</contexts>
<marker>Socher, Huval, Manning, Ng, 2012</marker>
<rawString>Richard Socher, Brody Huval, Christopher D Manning, and Andrew Y Ng. 2012. Semantic compositionality through recursive matrix-vector spaces. In Proceedings of the 2012 Joint Conference on Empirical Methods in Natural Language Processing and Computational Natural Language Learning, pages 1201–1211. Association for Computational Linguistics.</rawString>
</citation>
<citation valid="true">
<authors>
<author>Richard Socher</author>
<author>Alex Perelygin</author>
<author>Jean Y Wu</author>
<author>Jason Chuang</author>
<author>Christopher D Manning</author>
<author>Andrew Y Ng</author>
<author>Christopher Potts</author>
</authors>
<title>Recursive deep models for semantic compositionality over a sentiment treebank.</title>
<date>2013</date>
<booktitle>In Proceedings of the Conference on Empirical Methods in Natural Language Processing (EMNLP),</booktitle>
<pages>1631--1642</pages>
<contexts>
<context position="4895" citStr="Socher et al., 2013" startWordPosition="715" endWordPosition="718">r when deciding on ordering. More importantly, the features developed to date are still incapable of fully specifying the acceptable ordering(s) within a context, let alone describe why they are coherent. Recently, deep architectures, have been applied to various natural language processing tasks (see Section 2). Such deep connectionist architectures learn a dense, low-dimensional representation of their problem in a hierarchical way that is capable of capturing both semantic and syntactic aspects of tokens (e.g., (Bengio et al., 2006)), entities, N-grams (Wang and Manning, 2012), or phrases (Socher et al., 2013). More recent researches have begun looking at higher level distributed representations that transcend the token level, such as sentence-level (Le and Mikolov, 2014) or even discourse-level (Kalchbrenner and Blunsom, 2013) aspects. Just as words combine to form meaningful sentences, can we take advantage of distributional semantic representations to explore the composition of sentences to form coherent meanings in paragraphs? In this paper, we demonstrate that it is feasible to discover the coherent structure of a text using distributed sentence representations learned in a deep learning frame</context>
<context position="9821" citStr="Socher et al., 2013" startWordPosition="1490" endWordPosition="1493"> et al., 1994); two tokens may be structurally close to each other, even though they are far away in word sequence3. Recursive neural networks comprise another class of architecture, one that relies and operates on structured inputs (e.g., parse trees). It computes the representation for each parent based on its children iteratively in a bottom-up fashion. A series of variations have been proposed, each tailored to different task-specific requirements, such as Matrix-Vector RNN (Socher et al., 2012) that represents every word as both a vector and a matrix, or Recursive Neural Tensor Networks (Socher et al., 2013) that allow the model to have greater 3For example, a verb and its corresponding direct object can be far away in terms of tokens if many adjectives lies in between, but they are adjacent in the parse tree (Irsoy and Cardie, 2013). interactions between the input vectors. Many tasks have benefited from this recursive framework, including parsing (Socher et al., 2011b), sentiment analysis (Socher et al., 2013), and paraphrase detection (Socher et al., 2011a). 2.1 Distributed Representations Both recurrent and recursive networks require a vector representation of each input token. Distributed rep</context>
<context position="11645" citStr="Socher et al., 2013" startWordPosition="1774" endWordPosition="1777">h as King−man ≈ Queue−woman (Mikolov et al., 2010), and recently have been successfully used in various NLP applications, including named entity recognition, tagging, segmentation (Wang et al., 2013), and machine translation (e.g.,(Collobert and Weston, 2008; Zou et al., 2013)). 3 Sentence Model In this section, we demonstrate the strategy adopted to compute a vector for a sentence given the sequence of its words and their embeddings. We implemented two approaches, Recurrent and Recursive neural networks, following the descriptions in for example (Mikolov et al., 2010; Sutskever et al., 2011; Socher et al., 2013). As 2041 the details of both approaches can be readily found there, we make this section brief and omit the details for brevity. Let s denote a sentence, comprised of a sequence of words s = {w1, w2,..., wns}, where ns denotes the number of words within sentence s. Each word w is associated with a specific vector embedding e, = {e1w, e2 w, ..., eKw }, where K denotes the dimension of the word embedding. We wish to compute the vector representation for current sentence hs = {h1s, h2s, ..., hKs }. Recurrent Sentence Representation (Recurrent) The recurrent network captures certain general consi</context>
</contexts>
<marker>Socher, Perelygin, Wu, Chuang, Manning, Ng, Potts, 2013</marker>
<rawString>Richard Socher, Alex Perelygin, Jean Y Wu, Jason Chuang, Christopher D Manning, Andrew Y Ng, and Christopher Potts. 2013. Recursive deep models for semantic compositionality over a sentiment treebank. In Proceedings of the Conference on Empirical Methods in Natural Language Processing (EMNLP), pages 1631–1642.</rawString>
</citation>
<citation valid="true">
<authors>
<author>Michael Strube</author>
<author>Udo Hahn</author>
</authors>
<title>Functional centering: Grounding referential coherence in information structure.</title>
<date>1999</date>
<booktitle>Computational linguistics,</booktitle>
<pages>25--3</pages>
<contexts>
<context position="6821" citStr="Strube and Hahn, 1999" startWordPosition="1016" endWordPosition="1019">ofart performance in multiple standard evaluations for coherence models (Barzilay and Lee, 2004). The rest of this paper is organized as follows: We describe related work in Section 2, then describe how to obtain a distributed representation for sentences in Section 3, and the window composition in Section 4. Experimental results are shown in Section 5, followed by a conclusion. 2 Related Work Coherence In addition to the early computational work discussed above, local coherence was extensively studied within the modeling framework of Centering Theory (Grosz et al., 1995; Walker et al., 1998; Strube and Hahn, 1999; Poesio et al., 2004), which provides principles to form a coherence metric (Miltsakaki and Kukich, 2000; Hasler, 2004). Centering approaches suffer from a severe dependence on manually annotated input. A recent popular approach is the entity grid model introduced by Barzilay and Lapata (2008) , in which sentences are represented by a vector of discourse entities along with their grammatical roles (e.g., subject or object). Probabilities of transitions between adjacent sentences are derived from entity features and then concatenated to a document vector representation, which is used as input </context>
</contexts>
<marker>Strube, Hahn, 1999</marker>
<rawString>Michael Strube and Udo Hahn. 1999. Functional centering: Grounding referential coherence in information structure. Computational linguistics, 25(3):309–344.</rawString>
</citation>
<citation valid="true">
<authors>
<author>Ilya Sutskever</author>
<author>James Martens</author>
<author>Geoffrey E Hinton</author>
</authors>
<title>Generating text with recurrent neural networks.</title>
<date>2011</date>
<booktitle>In Proceedings of the 28th International Conference on Machine Learning (ICML-11),</booktitle>
<pages>1017--1024</pages>
<contexts>
<context position="8570" citStr="Sutskever et al., 2011" startWordPosition="1286" endWordPosition="1289">an HMM system in which the coherence between adjacent sentences is modeled by a hidden Markov framework captured by the 2040 Figure 2: Sentential compositionality obtained from (a) recurrent / (b) recursive neural network. The bottom layer represents word vectors in the sentence. The top layer hs denotes the resulting sentence vector. transition rules of different topics. Recurrent and Recursive Neural Networks In the context of NLP, recurrent neural networks view a sentence as a sequence of tokens and incorporate information from the past (i.e., preceding tokens) (Schuster and Paliwal, 1997; Sutskever et al., 2011) for acquisition of the current output. At each step, the recurrent network takes as input both the output of previous steps and the current token, convolutes the inputs, and forwards the result to the next step. It has been successfully applied to tasks such as language modeling (Mikolov et al., 2010) and spoken language understanding (Mesnil et al., 2013). The advantage of recurrent network is that it does not depend on external deeper structure (e.g., parse tree) and is easy to implement. However, in the recurrent framework, long-distance dependencies are difficult to capture due to the van</context>
<context position="11623" citStr="Sutskever et al., 2011" startWordPosition="1770" endWordPosition="1773">antic relationships, such as King−man ≈ Queue−woman (Mikolov et al., 2010), and recently have been successfully used in various NLP applications, including named entity recognition, tagging, segmentation (Wang et al., 2013), and machine translation (e.g.,(Collobert and Weston, 2008; Zou et al., 2013)). 3 Sentence Model In this section, we demonstrate the strategy adopted to compute a vector for a sentence given the sequence of its words and their embeddings. We implemented two approaches, Recurrent and Recursive neural networks, following the descriptions in for example (Mikolov et al., 2010; Sutskever et al., 2011; Socher et al., 2013). As 2041 the details of both approaches can be readily found there, we make this section brief and omit the details for brevity. Let s denote a sentence, comprised of a sequence of words s = {w1, w2,..., wns}, where ns denotes the number of words within sentence s. Each word w is associated with a specific vector embedding e, = {e1w, e2 w, ..., eKw }, where K denotes the dimension of the word embedding. We wish to compute the vector representation for current sentence hs = {h1s, h2s, ..., hKs }. Recurrent Sentence Representation (Recurrent) The recurrent network captures</context>
</contexts>
<marker>Sutskever, Martens, Hinton, 2011</marker>
<rawString>Ilya Sutskever, James Martens, and Geoffrey E Hinton. 2011. Generating text with recurrent neural networks. In Proceedings of the 28th International Conference on Machine Learning (ICML-11), pages 1017–1024.</rawString>
</citation>
<citation valid="true">
<authors>
<author>Marilyn A Walker</author>
<author>Aravind Krishna Joshi</author>
<author>Ellen Friedman Prince</author>
</authors>
<title>Centering theory in discourse.</title>
<date>1998</date>
<publisher>Oxford University Press.</publisher>
<contexts>
<context position="6798" citStr="Walker et al., 1998" startWordPosition="1012" endWordPosition="1015">model produces state-ofart performance in multiple standard evaluations for coherence models (Barzilay and Lee, 2004). The rest of this paper is organized as follows: We describe related work in Section 2, then describe how to obtain a distributed representation for sentences in Section 3, and the window composition in Section 4. Experimental results are shown in Section 5, followed by a conclusion. 2 Related Work Coherence In addition to the early computational work discussed above, local coherence was extensively studied within the modeling framework of Centering Theory (Grosz et al., 1995; Walker et al., 1998; Strube and Hahn, 1999; Poesio et al., 2004), which provides principles to form a coherence metric (Miltsakaki and Kukich, 2000; Hasler, 2004). Centering approaches suffer from a severe dependence on manually annotated input. A recent popular approach is the entity grid model introduced by Barzilay and Lapata (2008) , in which sentences are represented by a vector of discourse entities along with their grammatical roles (e.g., subject or object). Probabilities of transitions between adjacent sentences are derived from entity features and then concatenated to a document vector representation, </context>
</contexts>
<marker>Walker, Joshi, Prince, 1998</marker>
<rawString>Marilyn A Walker, Aravind Krishna Joshi, and Ellen Friedman Prince. 1998. Centering theory in discourse. Oxford University Press.</rawString>
</citation>
<citation valid="true">
<authors>
<author>Sida Wang</author>
<author>Christopher D Manning</author>
</authors>
<title>Baselines and bigrams: Simple, good sentiment and topic classification.</title>
<date>2012</date>
<booktitle>In Proceedings of the 50th Annual Meeting of the Association for Computational Linguistics: Short Papers-Volume 2,</booktitle>
<pages>90--94</pages>
<publisher>Association</publisher>
<institution>for Computational Linguistics.</institution>
<contexts>
<context position="4861" citStr="Wang and Manning, 2012" startWordPosition="709" endWordPosition="712"> clause or a coherent text to consider when deciding on ordering. More importantly, the features developed to date are still incapable of fully specifying the acceptable ordering(s) within a context, let alone describe why they are coherent. Recently, deep architectures, have been applied to various natural language processing tasks (see Section 2). Such deep connectionist architectures learn a dense, low-dimensional representation of their problem in a hierarchical way that is capable of capturing both semantic and syntactic aspects of tokens (e.g., (Bengio et al., 2006)), entities, N-grams (Wang and Manning, 2012), or phrases (Socher et al., 2013). More recent researches have begun looking at higher level distributed representations that transcend the token level, such as sentence-level (Le and Mikolov, 2014) or even discourse-level (Kalchbrenner and Blunsom, 2013) aspects. Just as words combine to form meaningful sentences, can we take advantage of distributional semantic representations to explore the composition of sentences to form coherent meanings in paragraphs? In this paper, we demonstrate that it is feasible to discover the coherent structure of a text using distributed sentence representation</context>
</contexts>
<marker>Wang, Manning, 2012</marker>
<rawString>Sida Wang and Christopher D Manning. 2012. Baselines and bigrams: Simple, good sentiment and topic classification. In Proceedings of the 50th Annual Meeting of the Association for Computational Linguistics: Short Papers-Volume 2, pages 90–94. Association for Computational Linguistics.</rawString>
</citation>
<citation valid="true">
<authors>
<author>Houfeng Wang</author>
<author>Longkai Zhang</author>
<author>Li Li</author>
<author>He Zhengyan</author>
<author>Ni Sun</author>
</authors>
<title>Improving chinese word segmentation on micro-blog using rich punctuations.</title>
<date>2013</date>
<contexts>
<context position="11224" citStr="Wang et al., 2013" startWordPosition="1708" endWordPosition="1711"> been explored to learn these embeddings in an unsupervised manner from a large corpus (Bengio et al., 2006; Collobert and Weston, 2008; Mnih and Hinton, 2007; Mikolov et al., 2013), which might have different generalization capabilities and are able to capture the semantic meanings depending on the specific task at hand. These vector representations can to some extent capture interesting semantic relationships, such as King−man ≈ Queue−woman (Mikolov et al., 2010), and recently have been successfully used in various NLP applications, including named entity recognition, tagging, segmentation (Wang et al., 2013), and machine translation (e.g.,(Collobert and Weston, 2008; Zou et al., 2013)). 3 Sentence Model In this section, we demonstrate the strategy adopted to compute a vector for a sentence given the sequence of its words and their embeddings. We implemented two approaches, Recurrent and Recursive neural networks, following the descriptions in for example (Mikolov et al., 2010; Sutskever et al., 2011; Socher et al., 2013). As 2041 the details of both approaches can be readily found there, we make this section brief and omit the details for brevity. Let s denote a sentence, comprised of a sequence </context>
</contexts>
<marker>Wang, Zhang, Li, Zhengyan, Sun, 2013</marker>
<rawString>Houfeng Wang, Longkai Zhang, Li Li, He Zhengyan, and Ni Sun. 2013. Improving chinese word segmentation on micro-blog using rich punctuations.</rawString>
</citation>
<citation valid="true">
<authors>
<author>Will Y Zou</author>
<author>Richard Socher</author>
<author>Daniel Cer</author>
<author>Christopher D Manning</author>
</authors>
<title>Bilingual word embeddings for phrase-based machine translation.</title>
<date>2013</date>
<booktitle>In Conference on Empirical Methods in Natural Language Processing (EMNLP</booktitle>
<contexts>
<context position="11302" citStr="Zou et al., 2013" startWordPosition="1719" endWordPosition="1722"> corpus (Bengio et al., 2006; Collobert and Weston, 2008; Mnih and Hinton, 2007; Mikolov et al., 2013), which might have different generalization capabilities and are able to capture the semantic meanings depending on the specific task at hand. These vector representations can to some extent capture interesting semantic relationships, such as King−man ≈ Queue−woman (Mikolov et al., 2010), and recently have been successfully used in various NLP applications, including named entity recognition, tagging, segmentation (Wang et al., 2013), and machine translation (e.g.,(Collobert and Weston, 2008; Zou et al., 2013)). 3 Sentence Model In this section, we demonstrate the strategy adopted to compute a vector for a sentence given the sequence of its words and their embeddings. We implemented two approaches, Recurrent and Recursive neural networks, following the descriptions in for example (Mikolov et al., 2010; Sutskever et al., 2011; Socher et al., 2013). As 2041 the details of both approaches can be readily found there, we make this section brief and omit the details for brevity. Let s denote a sentence, comprised of a sequence of words s = {w1, w2,..., wns}, where ns denotes the number of words within se</context>
</contexts>
<marker>Zou, Socher, Cer, Manning, 2013</marker>
<rawString>Will Y Zou, Richard Socher, Daniel Cer, and Christopher D Manning. 2013. Bilingual word embeddings for phrase-based machine translation. In Conference on Empirical Methods in Natural Language Processing (EMNLP 2013).</rawString>
</citation>
</citationList>
</algorithm>
</algorithms>
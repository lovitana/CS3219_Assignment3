<?xml version="1.0" encoding="UTF-8"?>
<algorithms version="110505">
<algorithm name="SectLabel" version="110505">
<variant no="0" confidence="0.000001">
<title confidence="0.998142">
A convex relaxation for weakly supervised relation extraction
</title>
<author confidence="0.941826">
´Edouard Grave
</author>
<affiliation confidence="0.956724">
EECS Department
University of California, Berkeley
</affiliation>
<email confidence="0.992409">
grave@berkeley.edu
</email>
<sectionHeader confidence="0.99731" genericHeader="abstract">
Abstract
</sectionHeader>
<bodyText confidence="0.999855190476191">
A promising approach to relation extrac-
tion, called weak or distant supervision,
exploits an existing database of facts as
training data, by aligning it to an unla-
beled collection of text documents. Using
this approach, the task of relation extrac-
tion can easily be scaled to hundreds of
different relationships. However, distant
supervision leads to a challenging multi-
ple instance, multiple label learning prob-
lem. Most of the proposed solutions to this
problem are based on non-convex formu-
lations, and are thus prone to local min-
ima. In this article, we propose a new
approach to the problem of weakly su-
pervised relation extraction, based on dis-
criminative clustering and leading to a
convex formulation. We demonstrate that
our approach outperforms state-of-the-art
methods on the challenging dataset intro-
duced by Riedel et al. (2010).
</bodyText>
<sectionHeader confidence="0.999516" genericHeader="keywords">
1 Introduction
</sectionHeader>
<bodyText confidence="0.999906625">
Information extraction refers to the broad task
of automatically extracting structured information
from unstructured documents. An example is the
extraction of named entities and the relations be-
tween those entities from natural language texts.
In the age of the world wide web and big data,
information extraction is quickly becoming perva-
sive. For example, in 2013, more than 130, 000
scientific articles were published about cancer.
Keeping track with that quantity of information
is almost impossible, and it is thus of utmost im-
portance to transform the knowledge contained in
this massive amount of documents into structured
databases.
Traditional approaches to information extrac-
tion relies on supervised learning, yielding high
</bodyText>
<figure confidence="0.712483071428571">
Knowledge base
r e1 e2
BornIn Lichtenstein New York City
DiedIn Lichtenstein New York City
Sentences Latent labels
Roy Lichtenstein was born in
New York City, into an upper- BornIn
middle-class family.
In 1961, Leo Castelli started
displaying Lichtenstein’s work None
at his gallery in New York.
Lichtenstein died ofpneumonia
DiedIn
in 1997 in New York City.
</figure>
<figureCaption confidence="0.985453666666667">
Figure 1: An example of a knowledge database
comprising two facts and training sentences ob-
tained by aligning this database to unlabeled text.
</figureCaption>
<bodyText confidence="0.98559285">
precision and recall results (Zelenko et al.,
2003). Unfortunately, these approaches need large
amount of labeled data, and thus do not scale well
to the great number of different types of fact found
on the Web or in scientific articles. A promising
approach, called distant or weak supervision, is
to exploit an existing database of facts as training
data, by aligning it to an unlabeled collection of
text documents (Craven and Kumlien, 1999).
In this article, we are interested in weakly super-
vised extraction of binary relations. A challenge
pertaining to weak supervision is that the obtained
training data is noisy and ambiguous (Riedel et
al., 2010). Let us start with an example: if the
fact Attended(Turing, King�s College) exists
in the knowledge database and we observe the sen-
tence
Turing studied as an undergraduate from
1931 to 1934 at King’s College, Cambridge.
which contains mentions of both entities Turing
</bodyText>
<page confidence="0.909918">
1580
</page>
<note confidence="0.899356">
Proceedings of the 2014 Conference on Empirical Methods in Natural Language Processing (EMNLP), pages 1580–1590,
October 25-29, 2014, Doha, Qatar. c�2014 Association for Computational Linguistics
</note>
<bodyText confidence="0.997570407407407">
and King&apos;s College, then this sentence might ex-
press the fact that Alan Turing attended King’s
College, and thus, might be a useful example for
learning to extract the relation Attended. How-
ever, the sentence
Celebrations for the centenary of Alan Tur-
ing are being planned at King’s College.
also contains mentions of Turing and
King&apos;s College, but do not express the re-
lation Attended. Thus, weak supervision lead
to noisy examples. As noted by Riedel et al.
(2010), such negative extracted sentences for
existing facts can represent more than 30% of
the data. Moreover, a given pair of entities,
such as (Roy Lichtenstein, New York City),
car verify multiple relations, such as BornIn
and DiedIn. Weak supervision thus lead to
ambiguous examples.
This challenge is illustrated in Fig. 1. A solution
to address it is to formulate the task of weakly su-
pervised relation extraction as a multiple instance,
multiple label learning problem (Hoffmann et al.,
2011; Surdeanu et al., 2012). However, these for-
mulations are often non-convex and thus suffer
from local minimum.
In this article, we make the following contribu-
tions:
</bodyText>
<listItem confidence="0.971486125">
• We propose a new convex relaxation for the
problem of weakly supervised relation ex-
traction, based on discriminative clustering,
• We propose an efficient algorithm to solve the
associated convex program,
• We demonstrate that our approach obtains
state-of-the-art results on the dataset intro-
duced by Riedel et al. (2010).
</listItem>
<bodyText confidence="0.993209666666667">
To our knowledge, this paper is the first to propose
a convex formulation for solving the problem of
weakly supervised relation extraction.
</bodyText>
<sectionHeader confidence="0.9999" genericHeader="introduction">
2 Related work
</sectionHeader>
<bodyText confidence="0.999292821428571">
Supervised learning. Many approaches based
on supervised learning have been proposed to
solve the problem of relation extraction, and the
corresponding literature is to large to be summa-
rized here. One of the first supervised method for
relation extraction was inspired by syntactic pars-
ing: the system described by Miller et al. (1998)
combines syntactic and semantic knowledge, and
thus, part-of-speech tagging, parsing, named en-
tity recognition and relation extraction all happen
at the same time. The problem of relation ex-
traction was later formulated as a classification
problem: Kambhatla (2004) proposed to solve this
problem using maximum entropy models using
lexical, syntactic and semantic features. Kernel
methods for relation extraction, based on shallow
parse trees or dependency trees were introduced
by Zelenko et al. (2003), Culotta and Sorensen
(2004) and Bunescu and Mooney (2005).
Unsupervised learning. The open information
extraction paradigm, simultaneously proposed by
Shinyama and Sekine (2006) and Banko et al.
(2007), does not rely on any labeled data or even
existing relations. Instead, open information ex-
traction systems only use an unlabeled corpus, and
output a set of extracted relations. Such systems
are based on clustering (Shinyama and Sekine,
2006) or self-supervision (Banko et al., 2007).
One of the limitations of these systems is the fact
that they extract uncanonicalized relations.
Weakly supervised learning. Weakly super-
vised learning refers to a broad class of meth-
ods, in which the learning system only have ac-
cess to partial, ambiguous and noisy labeling.
Craven and Kumlien (1999) were the first to pro-
pose a weakly supervised relation extractor. They
aligned a knowledge database (the Yeast Protein
Database) with scientific articles mentioning a par-
ticular relation, and then used the extracted sen-
tences to learn a classifier for extracting that rela-
tion.
Later, many different sources of weak label-
ings have been considered. Bellare and McCallum
(2007) proposed a method to extract bibliographic
relations based on conditional random fields and
used a database of BibTex entries as weak super-
vision. Wu and Weld (2007) described a method
to learn relations based on Wikipedia infoboxes.
Knowledge databases, such as Freebase1 (Mintz et
al., 2009; Sun et al., 2011) and YAGO2 (Nguyen
and Moschitti, 2011) were also considered as a
source of weak supervision.
Multiple instance learning. The methods we
previously mentionned transform the weakly su-
pervised problem into a fully supervised one, lead-
ing to noisy training datasets (see Fig. 1). Mul-
</bodyText>
<footnote confidence="0.9997265">
1www.freebase.com
2www.mpi-inf.mpg.de/yago-naga/yago
</footnote>
<page confidence="0.995562">
1581
</page>
<bodyText confidence="0.996907108108108">
tiple instance learning (Dietterich et al., 1997) is
a paradigm in which the learner receives bags of
examples instead of individual examples. A pos-
itively labeled bag contains at least one positive
example, but might also contains negative exam-
ples. In the context of relation extraction, Bunescu
and Mooney (2007) introduced a kernel method
for multiple instance learning, while Riedel et al.
(2010) proposed a solution based on a graphical
model.
Both these methods allow only one label per
bag, which is an asumption that is not true for
relation extraction (see Fig. 1). Thus, Hoffmann
et al. (2011) proposed a multiple instance, multi-
ple label method, based on an undirected graphical
model, to solve the problem of weakly supervised
relation extraction. Finally, Surdeanu et al. (2012)
also proposed a graphical model to solve this prob-
lem. One of their main contributions is to cap-
ture dependencies between relation labels, such as
the fact that two labels cannot be generated jointly
(e.g. the relations SpouseOf and BornIn).
Discriminative clustering. Our approach is
based on the discriminative clustering framework,
introduced by Xu et al. (2004). The goal of dis-
criminative clustering is to find a labeling of the
data points leading to a classifier with low classifi-
cation error. Different formulations of discrimina-
tive clustering have been proposed, based on sup-
port vector machines (Xu et al., 2004), the squared
loss (Bach and Harchaoui, 2007) or the logistic
loss (Joulin et al., 2010). A big advantage of dis-
criminative clustering is that weak supervision or
prior information can easily be incorporated. Our
work is closely related to the method proposed by
Bojanowski et al. (2013) for learning the names of
characters in movies.
</bodyText>
<sectionHeader confidence="0.935013" genericHeader="method">
3 Weakly supervised relation extraction
</sectionHeader>
<bodyText confidence="0.997879208333334">
In this article, our goal is to extract binary
relations between entities from natural lan-
guage text. Given a set of entities, a binary
relation r is a collection of ordered pairs of
entities. The statement that a pair of entities
(e1, e2) belongs to the relation r is denoted by
r(e1, e2) and this triple is called a fact or relation
instance. For example, the fact that Ernest
Hemingway was born in Oak Park is denoted
by BornIn(Ernest Hemingway, Oak Park).
A given pair of entities, such as
(Edouard Manet, Paris), can belong to
different relations, such as BornIn and DiedIn.
An entity mention is a contiguous sequence of
tokens refering to an entity, while a pair mention
or relation mention candidate is a sequence of text
in which a pair of entities is mentioned. In the
following, relation mention candidates will be re-
stricted to pair of entities that are mentioned in the
same sentence. For example, the sentence:
Ernest Hemingway was born in Oak Park.
contains two entity mentions, corresponding
to two relation mention candidates. In-
deed, the pairs (Hemingway, Oak Park) and
(Oak Park, Hemingway) are two distinct pairs of
entities, where only the first one verifies the rela-
tion BornIn.
Given a text corpus, aggregate extraction corre-
sponds to the task of extracting a set of facts, such
that each extracted fact is expressed at least once in
the corpus. On the other hand, the task of senten-
tial extraction corresponds to labeling each rela-
tion mention candidate by the relation it expresses,
or by a None label if it does not express any rela-
tion. Given a solution to the sentential extraction
problem, it is possible to construct a solution for
the aggregate extraction problem by returning all
the facts that were detected. We will follow this
approach, by building an instance level classifier,
and aggregating the results by extracting the facts
that were detected at least once in the corpus.
In the following, we will describe a method to
learn such a classifier using a database of facts in-
stead of a set of labeled sentences. This setting
is known as distant supervision or weak supervi-
sion, since we do not have access to labeled data
on which we could directly train a sentence level
relation extractor.
</bodyText>
<sectionHeader confidence="0.980363" genericHeader="method">
4 General approach
</sectionHeader>
<bodyText confidence="0.991425">
In this section, we propose a two step procedure to
solve the problem of weakly supervised relation
extraction:
</bodyText>
<listItem confidence="0.8400355">
1. First, we describe a method to infer the re-
lation labels corresponding to each relation
mention candidate of our training set,
2. Second, we train a supervised instance level
relation extractor, using the labels infered
during step 1.
</listItem>
<bodyText confidence="0.868186">
In the second step of our approach, we will simply
use a multinomial logistic regression model. We
</bodyText>
<page confidence="0.975505">
1582
</page>
<figure confidence="0.8664804">
Roy Lichtenstein was
born in New York City.
Lichtenstein left New
York to study in Ohio.
Ein Rik
(Lichtenstein, New York City)
BornIn
DiedIn
N relation mention candidates I pairs of entities pi K relations
represented by vectors xn
</figure>
<figureCaption confidence="0.999951">
Figure 2: Instance of the weakly supervised relation extraction problem, with notations used in the text.
</figureCaption>
<bodyText confidence="0.999101">
now describe the approach we propose for the first
step.
</bodyText>
<subsectionHeader confidence="0.982458">
4.1 Notations
</subsectionHeader>
<bodyText confidence="0.999710714285714">
Let (pi)1≤i≤, be a collection of I pairs of entities.
We suppose that we have N relation mention can-
didates, represented by the vectors (xn)1≤n≤N.
Let E ∈ R,×N be a matrix such that Ein = 1 if the
relation mention candidate n corresponds to the
pair of entities i, and Ein = 0 otherwise. The ma-
trix E thus indicates which relation mention can-
didate corresponds to which pair of entities. We
suppose that we have K relations, indexed by the
integers {1, ..., K}. Let R ∈ R,×K be a matrix
such that Rik = 1 if the pair of entities i verifies
the relation k, and Rik = 0 otherwise. The matrix
R thus represents the knowledge database. See
Fig. 2 for an illustration of these notations.
</bodyText>
<subsectionHeader confidence="0.986629">
4.2 Problem formulation
</subsectionHeader>
<bodyText confidence="0.999950333333333">
Our goal is to infer a binary matrix
Y ∈ {0, 1}N×(K+1), such that Ynk = 1 if
the relation mention candidate n express the
relation k and Ynk = 0 otherwise (and thus, the
integer K + 1 represents the relation None).
We take an approach inspired by the discrimi-
native clustering framework of Xu et al. (2004).
We are thus looking for a (K + 1)-class indicator
matrix Y, such that the classification error of an
optimal multiclass classifier f is minimum. Given
a multiclass loss function ` and a regularizer Q,
this problem can be formulated as:
</bodyText>
<equation confidence="0.976375">
`(yn, f(xn)) + Q(f),
s.t. Y ∈ Y
</equation>
<bodyText confidence="0.9991418">
where yn is the nth line of Y. The constraints
Y ∈ Y are added in order to take into account
the information from the weak supervision. We
will describe in the next section what kind of con-
straints are considered.
</bodyText>
<subsectionHeader confidence="0.987385">
4.3 Weak supervision by constraining Y
</subsectionHeader>
<bodyText confidence="0.999530875">
In this section, we show how the information
from the knowledge base can be expressed as con-
straints on the matrix Y.
First, we suppose that each relation mention
candidate express exactly one relation (including
the None relation). This means that the matrix Y
contains exactly one 1 per line, which is equivalent
to the constraint:
</bodyText>
<equation confidence="0.999194">
K
∀n ∈ {1, ..., N}, Ynk = 1.
k=1
</equation>
<bodyText confidence="0.999556666666667">
Second, if the pair i of entities verifies the rela-
tion k we suppose that at least one relation men-
tion candidate indeed express that relation. Thus
we want to impose that for at least one relation
mention candidate n such that Ein = 1, we have
Ynk = 1. This is equivalent to the constraint:
</bodyText>
<equation confidence="0.984754">
N
∀(i, k) such that Rik = 1, EinYnk ≥ 1.
n=1
</equation>
<bodyText confidence="0.9991624">
Third, if the pair i of entities does not verify the re-
lation k, we suppose that no relation mention can-
didate express that relation. Thus, we impose that
for all mention candidate n such that Ein = 1, we
have Ynk = 0. This is equivalent to the constraint:
</bodyText>
<equation confidence="0.993919">
N
∀(i, k) such that Rik = 0, EinYnk = 0.
n=1
</equation>
<bodyText confidence="0.999614">
Finally, we do not want too many relation men-
tion candidates to be classified as None. We thus
impose
</bodyText>
<equation confidence="0.986183">
N N
∀i ∈ {1, ..., I}, EinYn(K+1) ≤ c Ein,
n=1 n=1
</equation>
<bodyText confidence="0.999226666666667">
where c is the proportion of relation mention can-
didates that do not express a relation, for entity
pairs that appears in the knowledge database.
</bodyText>
<figure confidence="0.709058333333333">
N
min min
Y f n=1
</figure>
<page confidence="0.845532">
1583
</page>
<bodyText confidence="0.8991315">
We can rewrite these constraints using only ma-
trix operations in the following way:
</bodyText>
<equation confidence="0.9852695">
Y1 = 1
(EY) o S &gt; ˜R, (1)
</equation>
<bodyText confidence="0.999620375">
where o is the Hadamard product (a.k.a. the ele-
mentwise product), the matrix S E RIx(K+1) is
defined by
The set Y is thus defined as the set of matrices
Y E 10, 1INx(K+1) that verifies those two linear
constraints. It is important to note that besides the
boolean constraints, the two other constraints are
convex.
</bodyText>
<sectionHeader confidence="0.959004" genericHeader="method">
5 Squared loss and convex relaxation
</sectionHeader>
<bodyText confidence="0.9998062">
In this section, we describe the problem we ob-
tain when using the squared loss, and its associated
convex relaxation. We then introduce an efficient
algorithm to solve this problem, by computing its
dual.
</bodyText>
<subsectionHeader confidence="0.992938">
5.1 Primal problem
</subsectionHeader>
<bodyText confidence="0.99985075">
Following Bach and Harchaoui (2007), we use lin-
ear classifiers W E RDx(K+1), the squared loss
and the squared `2-norm as the regularizer. In that
case, our formulation becomes:
</bodyText>
<equation confidence="0.9953398">
2IIY − XWII2
1 F + A 2 IIWII2 F ,
s.t. Y E 10, 1INx(K+1)
Y1 = 1,
(EY) o S &gt; R.
</equation>
<bodyText confidence="0.9994942">
where II - IIF is the Frobenius norm and the ma-
trix X = [x1, ..., xN]T E RNxD represents the
relation mention candidates. Thanks to using the
squared loss, we have a closed form solution for
the matrix W:
</bodyText>
<equation confidence="0.989269">
W = (XTX + AID)_1XTY.
</equation>
<bodyText confidence="0.990677">
Replacing the matrix W by its optimal solution,
we obtain the following cost function:
</bodyText>
<equation confidence="0.649236">
YT(IN − X(XTX + AID)_1XT)Y.
</equation>
<bodyText confidence="0.99080325">
Then, by applying the Woodbury matrix identity
and relaxing the constraint Y E 10, 1INx(K+1)
into Y E [0,1]Nx(K+1), we obtain the following
convex quadratic problem in Y:
</bodyText>
<equation confidence="0.9713174">
( )
1 2tr YT(XXT + AIN)_1Y ,
s.t. Y &gt; 0,
Y1 = 1,
(EY) o S &gt; R.
</equation>
<bodyText confidence="0.985657">
Since the inequality constraints might be in-
feasible, we add the penalized slack variables
ξ E RIx(K+1), finally obtaining:
</bodyText>
<equation confidence="0.9790146">
( )
1 2tr YT(XXT + AIN)_1Y + µIIξII1
s.t. Y &gt; 0, ξ &gt; 0,
Y1 = 1,
(EY) o S &gt; R −ξ.
</equation>
<bodyText confidence="0.9996875">
This convex problem is a quadratic program. In
the following section, we will describe how to
solve this problem efficiently, by exploiting the
structure of its dual problem.
</bodyText>
<subsectionHeader confidence="0.997673">
5.2 Dual problem
</subsectionHeader>
<bodyText confidence="0.993855125">
The matrix Q = (XXT + AIN) appearing in the
quadratic program is an N by N matrix, where
N is the number of mention relation candidates.
Computing its inverse is thus expensive, since N
can be large. Instead, we propose to solve the
dual of this problem. Introducing dual variables
A E RIx(K+1), E E RNx(K+1) and v E RN,
the dual problem is equal to
</bodyText>
<equation confidence="0.9934525">
( ) ( )
1 2tr ZTQZ − tr ATR − vT1
</equation>
<bodyText confidence="0.960029923076923">
s.t. 0 G Aik G µ, 0 G Enk,
where
Z=ET(SoA)+E+v1T.
The derivation of this dual problem is given in Ap-
pendix A.
Solving the dual problem instead of the primal
has two main advantages. First, the dual does not
depend on the inverse of the matrix Q, while the
primal does. Since traditional features used for re-
lation extraction are indicators of lexical, syntactic
and named entities properties of the relation men-
tion candidates, the matrix X is extremely sparse.
and the matrix R˜ E RIx(K+1) is defined by
</bodyText>
<equation confidence="0.832958333333333">
R˜ = [R, −cE1].
�1 if Rik = 1
−1 if Rik = 0
Sik = or k = K + 1,
min
Y,W
1
min
Y 2
min
Y
min
Y,ξ
min
A,E,ν
</equation>
<page confidence="0.921905">
1584
</page>
<bodyText confidence="0.9998545">
Using the dual problem, we can thus exploit the
sparsity of the matrix X in the optimization pro-
cedure. Second, the constraints imposed on dual
variables are simpler than constraints imposed on
primal variables. Again, we will exploit this struc-
ture in the proposed optimization procedure.
Given a solution of the dual problem, the asso-
ciated primal variable Y is equal to:
</bodyText>
<equation confidence="0.996185">
Y = (XXT + AIN)Z.
</equation>
<bodyText confidence="0.998658333333333">
Thus, we do not need to compute the inverse of the
matrix (XXT + AIN) to obtain a solution to the
primal problem once we have solved the dual.
</bodyText>
<subsectionHeader confidence="0.997857">
5.3 Optimization of the dual problem
</subsectionHeader>
<bodyText confidence="0.999976944444444">
We propose to solve the dual problem using
the accelerated projected gradient descent algo-
rithm (Nesterov, 2007; Beck and Teboulle, 2009).
Indeed, computing the gradient of the dual cost
function is efficient, since the matrix X is sparse.
Moreover, the constraints on the dual variables are
simple and it is thus efficient to project onto this
set of constraints. See Appendix B for more de-
tails.
Complexity. The overall complexity of one step
of the accelerated projected gradient descent al-
gorithm is O(NFK), where F is the average
number of features per relation mention candi-
date. This means that the complexity of solving
the quadratic problem corresponding to our ap-
proach is linear with respect to the number N of
relation mention candidates, and thus our algo-
rithm can scale to large datasets.
</bodyText>
<subsectionHeader confidence="0.929652">
5.4 Discussion
</subsectionHeader>
<bodyText confidence="0.999980578947368">
Before moving to the experimental sections of this
article, we would like to discuss some properties
of our approach.
Kernels. First of all, one should note that our
proposed formulation only depends on the (lin-
ear) kernel matrix XXT. It is thus possible to re-
place this matrix by any other kernel. However,
in the case of a general kernel, the optimization
algorithm presented in the previous section has a
quadratic complexity O(KN2) with respect to the
number N of relation mention candidates, and it
is thus not applicable as is. We plan to explore the
use of kernels in future work.
Rounding. Given a continuous solution Y E
[0,1]Nx(K+1) of the relaxed problem, a very sim-
ple way to obtain a relation label for each relation
mention candidate of the training set is to com-
pute the orthogonal projection of the matrix Y on
the set of indicator matrices
</bodyText>
<equation confidence="0.925972">
{M E {0, 1}Nx(K+1)  |M1 = 1 .
</equation>
<bodyText confidence="0.999952">
This projection consists in taking the maximum
value along the rows of the matrix Y. It should
be noted that the obtained matrix does not neces-
sarily verify the inequality constraints defined in
Eq. 1. In the following, we will use this rounding,
refered to as argmax rounding, to obtain relation
labels for each relation mention candidate.
</bodyText>
<sectionHeader confidence="0.992218" genericHeader="method">
6 Dataset and features
</sectionHeader>
<bodyText confidence="0.999893">
In this section, we describe the dataset used in the
experimental section and the features used to rep-
resent the data.
</bodyText>
<subsectionHeader confidence="0.985501">
6.1 Dataset
</subsectionHeader>
<bodyText confidence="0.999984181818182">
We consider the dataset introduced by Riedel et
al. (2010). This dataset consists of articles from
the New York Times corpus (Sandhaus, 2008),
from which named entities where extracted and
tagged using the Stanford named entity recog-
nizer (Finkel et al., 2005). Consecutive tokens
with the same category were treated as a single
mention. These named entity mentions were then
aligned with the Freebase knowledge database, by
using a string match between the mentions and the
canonical names of entities in Freebase.
</bodyText>
<subsectionHeader confidence="0.977922">
6.2 Features
</subsectionHeader>
<bodyText confidence="0.999010866666667">
We use the features extracted by Riedel et al.
(2010), which were first introduced by Mintz et
al. (2009). These features capture how two en-
tity mentions are related in a given sentence, based
on syntactic and lexical properties. Lexical fea-
tures include: the sequence of words between the
two entities, a window of k words before the first
entity and after the second entity, the correspond-
ing part-of-speech tags, etc.. Syntactic features are
based on the dependency tree of the sentence, and
include: the path between the two entities, neigh-
bors of the two entities that do not belong to the
path. The OpenNLP3 part-of-speech tagger and
the Malt parser (Nivre et al., 2007) were used to
extract those features.
</bodyText>
<footnote confidence="0.584632">
3opennlp.apache.org
</footnote>
<page confidence="0.948532">
1585
</page>
<figure confidence="0.989373">
0.00 0.05 0.10 0.15 0.20 0.25 0.30
Recall
</figure>
<figureCaption confidence="0.9916815">
Figure 3: Precision/recall curves for different methods on the Riedel et al. (2010) dataset, for the task of
aggregate extraction.
</figureCaption>
<figure confidence="0.976389125">
0.9
0.7
0.6
0.4
0.2
0.8
0.5
0.3
</figure>
<note confidence="0.75166275">
Mintz et al. (2009)
Hoffmann et al. (2011)
Surdeanu et al. (2012)
This work
</note>
<sectionHeader confidence="0.288878" genericHeader="method">
Precision
</sectionHeader>
<subsectionHeader confidence="0.996048">
6.3 Implementation details
</subsectionHeader>
<bodyText confidence="0.990346090909091">
In this section, we discuss some important imple-
mentation details.
Kernel normalization. We normalized the ker-
nel matrix XXT, so that its diagonal coefficients
are equal to 1. This corresponds to normalizing
the vectors x,,, so that they have a unit E2-norm.
Choice of parameters. We kept 20% of the ex-
amples from the training set as a validation set, in
order to choose the parameters of our method. We
then re-train a model on the whole training set, us-
ing the chosen parameters.
</bodyText>
<sectionHeader confidence="0.987079" genericHeader="method">
7 Experimental evaluation
</sectionHeader>
<bodyText confidence="0.999726666666667">
In this section, we evaluate our approach to weakly
supervised relation extraction by comparing it to
state-of-the art methods.
</bodyText>
<subsectionHeader confidence="0.992873">
7.1 Baselines
</subsectionHeader>
<bodyText confidence="0.997154565217391">
We now briefly present the different methods we
compare to.
Mintz et al. This baseline corresponds to the
method described by Mintz et al. (2009). We
use the implementation of Surdeanu et al. (2012),
which slightly differs from the original method:
each relation mention candidate is treated inde-
pendently (and not collapsed across mentions for
a given entity pair). This strategy allows to predict
multiple labels for a given entity pair, by OR-ing
the predictions for the different mentions.
Hoffmann et al. This method, introduced by
Hoffmann et al. (2011), is based on probabilis-
tic graphical model of multi-instance multi-label
learning. They proposed a learning method
for this model, based on the perceptron algo-
rithm (Collins, 2002) and a greedy search for the
inference. We use the publicly available code of
Hoffmann et al.4.
Surdeanu et al. Finally, we compare our
method to the one described by Surdeanu et al.
(2012). This method is based on a two-layer
graphical model, the first layer corresponding to
</bodyText>
<footnote confidence="0.978272">
4www.cs.washington.edu/ai/raphaelh/mr/
</footnote>
<page confidence="0.989153">
1586
</page>
<figure confidence="0.972077">
0.0 0.1 0.2 0.3 0.4 0.5 0.6
Recall
</figure>
<figureCaption confidence="0.995707">
Figure 4: Precision/recall curves per relation for our method, on the Riedel et al. (2010) dataset, for the
task of aggregate extraction.
</figureCaption>
<figure confidence="0.993226166666667">
0.6
0.4
0.2
0.0
0.8
1.0
/location/location/contains
/people/person/place_lived
/person/person/nationality
/people/person/place_of_birth
/business/person/company
Precision
</figure>
<bodyText confidence="0.993718888888889">
a relation classifier at the mention level, while the
second layer is aggregating the different predic-
tion for a given entity pair. In particular, this sec-
ond layer capture dependencies between relation
labels, such as the fact that two labels cannot be
generated jointly (e.g. the relations SpouseOf and
BornIn). This model is trained by using hard
discriminative Expectation-Maximization. We use
the publicly available code of Surdeanu et al.5.
</bodyText>
<subsectionHeader confidence="0.999137">
7.2 Precision / recall curves
</subsectionHeader>
<bodyText confidence="0.999958285714286">
Following standard practices in relation extrac-
tion, we report precision/recall curves for the dif-
ferent models. In order to rank aggregate extrac-
tions for our model, the score of an extracted fact
r(e1, e2) is set to the maximal score of the differ-
ent extractions of that fact. This is sometimes ref-
ered to as the soft-OR function.
</bodyText>
<subsectionHeader confidence="0.980581">
7.3 Discussion
</subsectionHeader>
<bodyText confidence="0.945881384615385">
Comparison with the state-of-the-art. We re-
port results for the different methods on the dataset
5nlp.stanford.edu/software/mimlre.shtml
introduced by Riedel et al. (2010) in Fig. 3. We
observe that our approach generally outperforms
the state of the art. Indeed, at equivalent recall,
our method achieves better (or similar) precision
than the other methods, except for very low re-
call (smaller than 0.05). The improvement over
the methods proposed by Hoffmann et al. (2011)
and Surdeanu et al. (2012), which are currently
the best published results on this dataset, can be
as high as 5 points in precision for the same recall
point. Moreover, our method achieves a higher re-
call (0.30) than these two methods (0.25).
Performance per relation. The dataset in-
troduced by Riedel et al. (2010) is highly
unbalanced: for example, the most common
relation, /location/location/contains, rep-
resents almost half of the positive relations, while
some relations are mentioned less than ten times.
We thus decided to also report precision/recall
curves for the five most common relations of
that dataset in Fig. 4. First, we observe that the
perfomances vary a lot from a relation to another.
The frequence of the different relations is not the
</bodyText>
<page confidence="0.981209">
1587
</page>
<figure confidence="0.793642">
Recall
</figure>
<figureCaption confidence="0.916204">
Figure 5: Precision/recall curves for the task
</figureCaption>
<bodyText confidence="0.957265393939394">
of sentential extraction, on the manually labeled
dataset of Hoffmann et al. (2011).
only factor in those discrepancies. Indeed, the
relation /people/person/place lived and the
relation /people/person/place of birth
are more frequent than the relation
/business/person/company, but the ex-
traction of the later works much better than the
extraction of the two first.
Upon examination of the data, this can
partly be explained by the fact that al-
most no sentences extracted for the relation
/people/person/place of birth in fact
express this relation. In other words, many
facts present in Freebase are not expressed in
the corpus, and are thus impossible to extract.
On the other hand, most facts for the relation
/people/person/place lived are missing in
Freebase. Therefore, many extractions produced
by our system are considered false, but are in
fact true positives. The problem of incomplete
knowledge base was studied by Min et al. (2013).
Sentential extraction. We finally report preci-
sion/recall curves for the task of sentential extrac-
tion, in Fig. 5, using the manually labeled dataset
of Hoffmann et al. (2011). We observe that for
most values of recall, our method achieves simi-
lar precision that the one proposed by Hoffmann
et al. (2011), while extending the highest recall
from 0.52 to 0.68. Thanks to this higher recall, our
method achieves a highest F1 score of 0.66, com-
pared to 0.61 obtained by the method proposed by
Hoffmann et al. (2011).
</bodyText>
<table confidence="0.975582">
Method Runtime
Mintz et al. (2009) 7 min
Hoffmann et al. (2011) 2 min
Surdeanu et al. (2012) 3 hours
This work 3 hours
</table>
<tableCaption confidence="0.970133666666667">
Table 1: Comparison of running times for the dif-
ferent methods compared in the experimental sec-
tion.
</tableCaption>
<sectionHeader confidence="0.996221" genericHeader="method">
8 Conclusion
</sectionHeader>
<bodyText confidence="0.999987652173913">
In this article, we introduced a new formulation
for weakly supervised relation extraction. Our
method is based on a constrained discriminative
formulation of the multiple instance, multiple la-
bel learning problem. Using the squared loss,
we obtained a convex relaxation of this formula-
tion, allowing us to obtain an approximate solu-
tion to the initial integer quadratic program. Thus,
our method is not sensitive to initialization. We
demonstrated the competitiveness of our approach
on the dataset introduced by Riedel et al. (2010),
on which our method outperforms the state of the
art methods for weakly supervised relation extrac-
tion, on both aggregate and sentential extraction.
As noted earlier, another advantage of our
method is the fact that it is easily kernelizable.
We would like to explore the use of kernels, such
as the ones introduced by Zelenko et al. (2003),
Culotta and Sorensen (2004) and Bunescu and
Mooney (2005), in future work. We believe that
such kernels could improve the relatively low re-
call obtained so far by weakly supervised method
for relation extraction.
</bodyText>
<sectionHeader confidence="0.998162" genericHeader="method">
Acknowledgments
</sectionHeader>
<bodyText confidence="0.982617">
The author is supported by a grant from Inria
(Associated-team STATWEB) and would like to
thank Armand Joulin for helpful discussions.
</bodyText>
<sectionHeader confidence="0.99895" genericHeader="method">
References
</sectionHeader>
<reference confidence="0.942692777777778">
Francis Bach and Za¨ıd Harchaoui. 2007. DIFFRAC: a
discriminative and flexible framework for clustering.
In Adv. NIPS.
Michele Banko, Michael J Cafarella, Stephen Soder-
land, Matthew Broadhead, and Oren Etzioni. 2007.
Open information extraction for the web. In IJCAI.
Amir Beck and Marc Teboulle. 2009. A fast iterative
shrinkage-thresholding algorithm for linear inverse
problems. SIAM Journal on Imaging Sciences, 2(1).
</reference>
<figure confidence="0.9926355">
0.6
0.40.0 0.1 0.2 0.3 0.4 0.5 0.6 0.7
0.9
0.8
0.7
0.5
1.0
Hoffmann et al. (2011)
This work
Precision
</figure>
<page confidence="0.9447">
1588
</page>
<reference confidence="0.98628021875">
Kedar Bellare and Andrew McCallum. 2007. Learn-
ing extractors from unlabeled text using relevant
databases. In Sixth international workshop on in-
formation integration on the web.
Piotr Bojanowski, Francis Bach, Ivan Laptev, Jean
Ponce, Cordelia Schmid, and Josef Sivic. 2013.
Finding actors and actions in movies. In Proceed-
ings of ICCV.
Razvan Bunescu and Raymond Mooney. 2005. A
shortest path dependency kernel for relation extrac-
tion. In Proceedings of HLT-EMNLP.
Razvan Bunescu and Raymond Mooney. 2007. Learn-
ing to extract relations from the web using minimal
supervision. In Proceedings of the ACL.
Michael Collins. 2002. Discriminative training meth-
ods for hidden markov models: Theory and exper-
iments with perceptron algorithms. In Proceedings
of EMNLP.
Mark Craven and Johan Kumlien. 1999. Constructing
biological knowledge bases by extracting informa-
tion from text sources. In ISMB, volume 1999.
Aron Culotta and Jeffrey Sorensen. 2004. Dependency
tree kernels for relation extraction. In Proceedings
of the ACL.
Thomas G Dietterich, Richard H Lathrop, and Tom´as
Lozano-P´erez. 1997. Solving the multiple instance
problem with axis-parallel rectangles. Artificial in-
telligence, 89(1).
Jenny Rose Finkel, Trond Grenager, and Christopher
Manning. 2005. Incorporating non-local informa-
tion into information extraction systems by Gibbs
sampling. In Proceedings of the ACL.
Raphael Hoffmann, Congle Zhang, Xiao Ling, Luke
Zettlemoyer, and Daniel S Weld. 2011. Knowledge-
based weak supervision for information extraction
of overlapping relations. In Proceedings of the ACL.
Armand Joulin, Jean Ponce, and Francis Bach. 2010.
Efficient optimization for discriminative latent class
models. In Adv. NIPS.
Nanda Kambhatla. 2004. Combining lexical, syntac-
tic, and semantic features with maximum entropy
models for information extraction. In Proceedings
of the ACL.
Scott Miller, Michael Crystal, Heidi Fox, Lance
Ramshaw, Richard Schwartz, Rebecca Stone, and
Ralph Weischedel. 1998. Algorithms that learn to
extract information. In Proceedings of MUC-7.
Bonan Min, Ralph Grishman, Li Wan, Chang Wang,
and David Gondek. 2013. Distant supervision for
relation extraction with an incomplete knowledge
base. In Proceedings of HLT-NAACL.
Mike Mintz, Steven Bills, Rion Snow, and Dan Ju-
rafsky. 2009. Distant supervision for relation ex-
traction without labeled data. In Proceedings of the
ACL-IJCNLP.
Yurii Nesterov. 2007. Gradient methods for minimiz-
ing composite objective function.
Truc-Vien T Nguyen and Alessandro Moschitti. 2011.
End-to-end relation extraction using distant super-
vision from external semantic repositories. In Pro-
ceedings of the ACL.
Joakim Nivre, Johan Hall, Jens Nilsson, Atanas
Chanev, G¨ulsen Eryigit, Sandra K¨ubler, Svetoslav
Marinov, and Erwin Marsi. 2007. MaltParser:
A language-independent system for data-driven de-
pendency parsing. Natural Language Engineering,
13(02).
Sebastian Riedel, Limin Yao, and Andrew McCallum.
2010. Modeling relations and their mentions with-
out labeled text. In Machine Learning and Knowl-
edge Discovery in Databases.
Evan Sandhaus. 2008. The new york times annotated
corpus. Linguistic Data Consortium, Philadelphia,
6(12).
Yusuke Shinyama and Satoshi Sekine. 2006. Preemp-
tive information extraction using unrestricted rela-
tion discovery. In Proceedings of the HLT-NAACL.
Ang Sun, Ralph Grishman, Wei Xu, and Bonan Min.
2011. New york university 2011 system for kbp slot
filling. In Proceedings of the Text Analytics Confer-
ence.
Mihai Surdeanu, Julie Tibshirani, Ramesh Nallapati,
and Christopher D Manning. 2012. Multi-instance
multi-label learning for relation extraction. In Pro-
ceedings of EMNLP-CoNLL.
Fei Wu and Daniel S Weld. 2007. Autonomously
semantifying wikipedia. In Proceedings of the six-
teenth ACM conference on Conference on informa-
tion and knowledge management.
Linli Xu, James Neufeld, Bryce Larson, and Dale
Schuurmans. 2004. Maximum margin clustering.
In Adv. NIPS.
Dmitry Zelenko, Chinatsu Aone, and Anthony
Richardella. 2003. Kernel methods for relation
extraction. The Journal of Machine Learning Re-
search, 3.
</reference>
<page confidence="0.998532">
1589
</page>
<sectionHeader confidence="0.993236" genericHeader="method">
Appendix A Derivation of the dual
</sectionHeader>
<bodyText confidence="0.9963375">
In this section, we derive the dual problem of the
quadratic program of section 5. We introduce dual
variables A E RIx(K+1), E E RNx(K+1),
Q E RIx(K+1) and ν E RN, such that A &gt; 0,
E &gt; 0 and Q &gt; 0.
The Lagrangian of the problem is
</bodyText>
<equation confidence="0.869358833333333">
1�
2tr (YT (XXT + λIN)_1Y) + µ
i,k
( )
− tr AT((EY) o S − R + ξ)
− tr(ETY) − tr(QTξ) − νT(Y1 − 1).
</equation>
<bodyText confidence="0.9279456">
To find the dual function g we minimize the La-
grangian over Y and ξ. Minimizing over ξ, we
find that the dual function is equal to −oc unless
µ − Aik − Qik = 0, in which case, we are left with
( )
</bodyText>
<equation confidence="0.9879274">
1 2tr YT(XXT + λIN)_1Y
− tr((A o S)TEY) − tr(ETY) − tr(1νTY)
+ tr(ATR) + νT1.
Minimizing over Y, we then obtain
Y = (XXT + λIN)(ET(S o A) + E + ν1T).
</equation>
<bodyText confidence="0.989168">
Replacing Y by its optimal value, we then obtain
the dual function
</bodyText>
<equation confidence="0.879569">
( ) ( )
1 2tr ZTQZ + tr ATR + νT1.
</equation>
<bodyText confidence="0.929071">
where
</bodyText>
<equation confidence="0.9998605">
Q = (XXT + λIN),
Z = ET(S o A) + E + ν1T.
</equation>
<bodyText confidence="0.798431">
Thus, the dual problem is
</bodyText>
<equation confidence="0.950583">
( ) ( )
1 2tr ZTQZ + tr ATR + νT1
</equation>
<bodyText confidence="0.9959348">
s.t. 0 G Aik, 0 G Enk, 0 G Qik,
µ − Aik − Qik = 0.
We can then eliminate the dual variable Q, since
the constraints Qik = µ − Aik and Qik &gt; 0 are
equivalent to µ &gt; Aik. We finally obtain
</bodyText>
<equation confidence="0.8095165">
( ) ( )
1 2tr ZTQZ + tr ATR + νT1
</equation>
<bodyText confidence="0.951353">
s.t. 0 G Aik G µ, 0 G Enk.
</bodyText>
<sectionHeader confidence="0.985895" genericHeader="method">
Appendix B Optimization details
</sectionHeader>
<bodyText confidence="0.990711333333333">
Gradient of the dual cost function. The gradi-
ent of the dual cost function f with respect to the
dual variables E, A and ν is equal to
</bodyText>
<equation confidence="0.9996645">
VEf = (XXT + λIN)Z,
((XXT + λIN)ZET)
VAf = o S − R,
Vνf = (XXT + λIN)Z1 − 1.
</equation>
<bodyText confidence="0.987331">
The most expensive step to compute those gra-
dients is to compute the matrix product XXTZ.
Since the matrix X is sparse, we efficiently com-
pute this product by first computing the product
XTZ, and then by left multiplying the result by
X. The complexity of these two operations is
O(NFK), where F is the average number of fea-
tures per relation mention candidate.
Projecting E and A. The componentwise pro-
jection operators associated to the constraints on
E and A are defined by:
</bodyText>
<equation confidence="0.9987125">
projE(Enk) = max(0, Enk),
projA(Aik) = max(0, min(µ, Aik)).
</equation>
<bodyText confidence="0.939918888888889">
The complexity of projecting E and A is O(NK).
Thus, the cost of those operations is ne gligible
compared to the cost of computing the gradients
of the dual cost function.
ξik
max
A,E,ν
max
A,E,ν
</bodyText>
<page confidence="0.978676">
1590
</page>
</variant>
</algorithm>
<algorithm name="ParsHed" version="110505">
<variant no="0" confidence="0.845417">
<title confidence="0.99902">A convex relaxation for weakly supervised relation extraction</title>
<author confidence="0.995452">´Edouard</author>
<affiliation confidence="0.9969935">EECS University of California,</affiliation>
<email confidence="0.999905">grave@berkeley.edu</email>
<abstract confidence="0.99975719047619">A promising approach to relation extraction, called weak or distant supervision, exploits an existing database of facts as training data, by aligning it to an unlabeled collection of text documents. Using this approach, the task of relation extraction can easily be scaled to hundreds of different relationships. However, distant supervision leads to a challenging multiple instance, multiple label learning problem. Most of the proposed solutions to this problem are based on non-convex formulations, and are thus prone to local minima. In this article, we propose a new approach to the problem of weakly supervised relation extraction, based on discriminative clustering and leading to a convex formulation. We demonstrate that our approach outperforms state-of-the-art methods on the challenging dataset intro-</abstract>
<note confidence="0.859908">duced by Riedel et al. (2010).</note>
</variant>
</algorithm>
<algorithm name="ParsCit" version="110505">
<citationList>
<citation valid="true">
<authors>
<author>Francis Bach</author>
<author>Za¨ıd Harchaoui</author>
</authors>
<title>DIFFRAC: a discriminative and flexible framework for clustering. In Adv.</title>
<date>2007</date>
<publisher>NIPS.</publisher>
<contexts>
<context position="9101" citStr="Bach and Harchaoui, 2007" startWordPosition="1414" endWordPosition="1417">is problem. One of their main contributions is to capture dependencies between relation labels, such as the fact that two labels cannot be generated jointly (e.g. the relations SpouseOf and BornIn). Discriminative clustering. Our approach is based on the discriminative clustering framework, introduced by Xu et al. (2004). The goal of discriminative clustering is to find a labeling of the data points leading to a classifier with low classification error. Different formulations of discriminative clustering have been proposed, based on support vector machines (Xu et al., 2004), the squared loss (Bach and Harchaoui, 2007) or the logistic loss (Joulin et al., 2010). A big advantage of discriminative clustering is that weak supervision or prior information can easily be incorporated. Our work is closely related to the method proposed by Bojanowski et al. (2013) for learning the names of characters in movies. 3 Weakly supervised relation extraction In this article, our goal is to extract binary relations between entities from natural language text. Given a set of entities, a binary relation r is a collection of ordered pairs of entities. The statement that a pair of entities (e1, e2) belongs to the relation r is </context>
<context position="16127" citStr="Bach and Harchaoui (2007)" startWordPosition="2671" endWordPosition="2674">o S &gt; ˜R, (1) where o is the Hadamard product (a.k.a. the elementwise product), the matrix S E RIx(K+1) is defined by The set Y is thus defined as the set of matrices Y E 10, 1INx(K+1) that verifies those two linear constraints. It is important to note that besides the boolean constraints, the two other constraints are convex. 5 Squared loss and convex relaxation In this section, we describe the problem we obtain when using the squared loss, and its associated convex relaxation. We then introduce an efficient algorithm to solve this problem, by computing its dual. 5.1 Primal problem Following Bach and Harchaoui (2007), we use linear classifiers W E RDx(K+1), the squared loss and the squared `2-norm as the regularizer. In that case, our formulation becomes: 2IIY − XWII2 1 F + A 2 IIWII2 F , s.t. Y E 10, 1INx(K+1) Y1 = 1, (EY) o S &gt; R. where II - IIF is the Frobenius norm and the matrix X = [x1, ..., xN]T E RNxD represents the relation mention candidates. Thanks to using the squared loss, we have a closed form solution for the matrix W: W = (XTX + AID)_1XTY. Replacing the matrix W by its optimal solution, we obtain the following cost function: YT(IN − X(XTX + AID)_1XT)Y. Then, by applying the Woodbury matrix</context>
</contexts>
<marker>Bach, Harchaoui, 2007</marker>
<rawString>Francis Bach and Za¨ıd Harchaoui. 2007. DIFFRAC: a discriminative and flexible framework for clustering. In Adv. NIPS.</rawString>
</citation>
<citation valid="true">
<authors>
<author>Michele Banko</author>
<author>Michael J Cafarella</author>
<author>Stephen Soderland</author>
<author>Matthew Broadhead</author>
<author>Oren Etzioni</author>
</authors>
<title>Open information extraction for the web. In IJCAI.</title>
<date>2007</date>
<contexts>
<context position="6026" citStr="Banko et al. (2007)" startWordPosition="930" endWordPosition="933">amed entity recognition and relation extraction all happen at the same time. The problem of relation extraction was later formulated as a classification problem: Kambhatla (2004) proposed to solve this problem using maximum entropy models using lexical, syntactic and semantic features. Kernel methods for relation extraction, based on shallow parse trees or dependency trees were introduced by Zelenko et al. (2003), Culotta and Sorensen (2004) and Bunescu and Mooney (2005). Unsupervised learning. The open information extraction paradigm, simultaneously proposed by Shinyama and Sekine (2006) and Banko et al. (2007), does not rely on any labeled data or even existing relations. Instead, open information extraction systems only use an unlabeled corpus, and output a set of extracted relations. Such systems are based on clustering (Shinyama and Sekine, 2006) or self-supervision (Banko et al., 2007). One of the limitations of these systems is the fact that they extract uncanonicalized relations. Weakly supervised learning. Weakly supervised learning refers to a broad class of methods, in which the learning system only have access to partial, ambiguous and noisy labeling. Craven and Kumlien (1999) were the fi</context>
</contexts>
<marker>Banko, Cafarella, Soderland, Broadhead, Etzioni, 2007</marker>
<rawString>Michele Banko, Michael J Cafarella, Stephen Soderland, Matthew Broadhead, and Oren Etzioni. 2007. Open information extraction for the web. In IJCAI.</rawString>
</citation>
<citation valid="true">
<authors>
<author>Amir Beck</author>
<author>Marc Teboulle</author>
</authors>
<title>A fast iterative shrinkage-thresholding algorithm for linear inverse problems.</title>
<date>2009</date>
<journal>SIAM Journal on Imaging Sciences,</journal>
<volume>2</volume>
<issue>1</issue>
<contexts>
<context position="19030" citStr="Beck and Teboulle, 2009" startWordPosition="3233" endWordPosition="3236">ization procedure. Second, the constraints imposed on dual variables are simpler than constraints imposed on primal variables. Again, we will exploit this structure in the proposed optimization procedure. Given a solution of the dual problem, the associated primal variable Y is equal to: Y = (XXT + AIN)Z. Thus, we do not need to compute the inverse of the matrix (XXT + AIN) to obtain a solution to the primal problem once we have solved the dual. 5.3 Optimization of the dual problem We propose to solve the dual problem using the accelerated projected gradient descent algorithm (Nesterov, 2007; Beck and Teboulle, 2009). Indeed, computing the gradient of the dual cost function is efficient, since the matrix X is sparse. Moreover, the constraints on the dual variables are simple and it is thus efficient to project onto this set of constraints. See Appendix B for more details. Complexity. The overall complexity of one step of the accelerated projected gradient descent algorithm is O(NFK), where F is the average number of features per relation mention candidate. This means that the complexity of solving the quadratic problem corresponding to our approach is linear with respect to the number N of relation mentio</context>
</contexts>
<marker>Beck, Teboulle, 2009</marker>
<rawString>Amir Beck and Marc Teboulle. 2009. A fast iterative shrinkage-thresholding algorithm for linear inverse problems. SIAM Journal on Imaging Sciences, 2(1).</rawString>
</citation>
<citation valid="true">
<authors>
<author>Kedar Bellare</author>
<author>Andrew McCallum</author>
</authors>
<title>Learning extractors from unlabeled text using relevant databases.</title>
<date>2007</date>
<booktitle>In Sixth international workshop on information integration on the web.</booktitle>
<contexts>
<context position="6990" citStr="Bellare and McCallum (2007)" startWordPosition="1083" endWordPosition="1086">t that they extract uncanonicalized relations. Weakly supervised learning. Weakly supervised learning refers to a broad class of methods, in which the learning system only have access to partial, ambiguous and noisy labeling. Craven and Kumlien (1999) were the first to propose a weakly supervised relation extractor. They aligned a knowledge database (the Yeast Protein Database) with scientific articles mentioning a particular relation, and then used the extracted sentences to learn a classifier for extracting that relation. Later, many different sources of weak labelings have been considered. Bellare and McCallum (2007) proposed a method to extract bibliographic relations based on conditional random fields and used a database of BibTex entries as weak supervision. Wu and Weld (2007) described a method to learn relations based on Wikipedia infoboxes. Knowledge databases, such as Freebase1 (Mintz et al., 2009; Sun et al., 2011) and YAGO2 (Nguyen and Moschitti, 2011) were also considered as a source of weak supervision. Multiple instance learning. The methods we previously mentionned transform the weakly supervised problem into a fully supervised one, leading to noisy training datasets (see Fig. 1). Mul1www.fre</context>
</contexts>
<marker>Bellare, McCallum, 2007</marker>
<rawString>Kedar Bellare and Andrew McCallum. 2007. Learning extractors from unlabeled text using relevant databases. In Sixth international workshop on information integration on the web.</rawString>
</citation>
<citation valid="true">
<authors>
<author>Piotr Bojanowski</author>
<author>Francis Bach</author>
<author>Ivan Laptev</author>
<author>Jean Ponce</author>
<author>Cordelia Schmid</author>
<author>Josef Sivic</author>
</authors>
<title>Finding actors and actions in movies.</title>
<date>2013</date>
<booktitle>In Proceedings of ICCV.</booktitle>
<contexts>
<context position="9343" citStr="Bojanowski et al. (2013)" startWordPosition="1454" endWordPosition="1457">ased on the discriminative clustering framework, introduced by Xu et al. (2004). The goal of discriminative clustering is to find a labeling of the data points leading to a classifier with low classification error. Different formulations of discriminative clustering have been proposed, based on support vector machines (Xu et al., 2004), the squared loss (Bach and Harchaoui, 2007) or the logistic loss (Joulin et al., 2010). A big advantage of discriminative clustering is that weak supervision or prior information can easily be incorporated. Our work is closely related to the method proposed by Bojanowski et al. (2013) for learning the names of characters in movies. 3 Weakly supervised relation extraction In this article, our goal is to extract binary relations between entities from natural language text. Given a set of entities, a binary relation r is a collection of ordered pairs of entities. The statement that a pair of entities (e1, e2) belongs to the relation r is denoted by r(e1, e2) and this triple is called a fact or relation instance. For example, the fact that Ernest Hemingway was born in Oak Park is denoted by BornIn(Ernest Hemingway, Oak Park). A given pair of entities, such as (Edouard Manet, P</context>
</contexts>
<marker>Bojanowski, Bach, Laptev, Ponce, Schmid, Sivic, 2013</marker>
<rawString>Piotr Bojanowski, Francis Bach, Ivan Laptev, Jean Ponce, Cordelia Schmid, and Josef Sivic. 2013. Finding actors and actions in movies. In Proceedings of ICCV.</rawString>
</citation>
<citation valid="true">
<authors>
<author>Razvan Bunescu</author>
<author>Raymond Mooney</author>
</authors>
<title>A shortest path dependency kernel for relation extraction.</title>
<date>2005</date>
<booktitle>In Proceedings of HLT-EMNLP.</booktitle>
<contexts>
<context position="5882" citStr="Bunescu and Mooney (2005)" startWordPosition="911" endWordPosition="914">yntactic parsing: the system described by Miller et al. (1998) combines syntactic and semantic knowledge, and thus, part-of-speech tagging, parsing, named entity recognition and relation extraction all happen at the same time. The problem of relation extraction was later formulated as a classification problem: Kambhatla (2004) proposed to solve this problem using maximum entropy models using lexical, syntactic and semantic features. Kernel methods for relation extraction, based on shallow parse trees or dependency trees were introduced by Zelenko et al. (2003), Culotta and Sorensen (2004) and Bunescu and Mooney (2005). Unsupervised learning. The open information extraction paradigm, simultaneously proposed by Shinyama and Sekine (2006) and Banko et al. (2007), does not rely on any labeled data or even existing relations. Instead, open information extraction systems only use an unlabeled corpus, and output a set of extracted relations. Such systems are based on clustering (Shinyama and Sekine, 2006) or self-supervision (Banko et al., 2007). One of the limitations of these systems is the fact that they extract uncanonicalized relations. Weakly supervised learning. Weakly supervised learning refers to a broad</context>
</contexts>
<marker>Bunescu, Mooney, 2005</marker>
<rawString>Razvan Bunescu and Raymond Mooney. 2005. A shortest path dependency kernel for relation extraction. In Proceedings of HLT-EMNLP.</rawString>
</citation>
<citation valid="true">
<authors>
<author>Razvan Bunescu</author>
<author>Raymond Mooney</author>
</authors>
<title>Learning to extract relations from the web using minimal supervision.</title>
<date>2007</date>
<booktitle>In Proceedings of the ACL.</booktitle>
<contexts>
<context position="7955" citStr="Bunescu and Mooney (2007)" startWordPosition="1230" endWordPosition="1233">itti, 2011) were also considered as a source of weak supervision. Multiple instance learning. The methods we previously mentionned transform the weakly supervised problem into a fully supervised one, leading to noisy training datasets (see Fig. 1). Mul1www.freebase.com 2www.mpi-inf.mpg.de/yago-naga/yago 1581 tiple instance learning (Dietterich et al., 1997) is a paradigm in which the learner receives bags of examples instead of individual examples. A positively labeled bag contains at least one positive example, but might also contains negative examples. In the context of relation extraction, Bunescu and Mooney (2007) introduced a kernel method for multiple instance learning, while Riedel et al. (2010) proposed a solution based on a graphical model. Both these methods allow only one label per bag, which is an asumption that is not true for relation extraction (see Fig. 1). Thus, Hoffmann et al. (2011) proposed a multiple instance, multiple label method, based on an undirected graphical model, to solve the problem of weakly supervised relation extraction. Finally, Surdeanu et al. (2012) also proposed a graphical model to solve this problem. One of their main contributions is to capture dependencies between </context>
</contexts>
<marker>Bunescu, Mooney, 2007</marker>
<rawString>Razvan Bunescu and Raymond Mooney. 2007. Learning to extract relations from the web using minimal supervision. In Proceedings of the ACL.</rawString>
</citation>
<citation valid="true">
<authors>
<author>Michael Collins</author>
</authors>
<title>Discriminative training methods for hidden markov models: Theory and experiments with perceptron algorithms.</title>
<date>2002</date>
<booktitle>In Proceedings of EMNLP.</booktitle>
<contexts>
<context position="24068" citStr="Collins, 2002" startWordPosition="4077" endWordPosition="4078">Mintz et al. (2009). We use the implementation of Surdeanu et al. (2012), which slightly differs from the original method: each relation mention candidate is treated independently (and not collapsed across mentions for a given entity pair). This strategy allows to predict multiple labels for a given entity pair, by OR-ing the predictions for the different mentions. Hoffmann et al. This method, introduced by Hoffmann et al. (2011), is based on probabilistic graphical model of multi-instance multi-label learning. They proposed a learning method for this model, based on the perceptron algorithm (Collins, 2002) and a greedy search for the inference. We use the publicly available code of Hoffmann et al.4. Surdeanu et al. Finally, we compare our method to the one described by Surdeanu et al. (2012). This method is based on a two-layer graphical model, the first layer corresponding to 4www.cs.washington.edu/ai/raphaelh/mr/ 1586 0.0 0.1 0.2 0.3 0.4 0.5 0.6 Recall Figure 4: Precision/recall curves per relation for our method, on the Riedel et al. (2010) dataset, for the task of aggregate extraction. 0.6 0.4 0.2 0.0 0.8 1.0 /location/location/contains /people/person/place_lived /person/person/nationality </context>
</contexts>
<marker>Collins, 2002</marker>
<rawString>Michael Collins. 2002. Discriminative training methods for hidden markov models: Theory and experiments with perceptron algorithms. In Proceedings of EMNLP.</rawString>
</citation>
<citation valid="true">
<authors>
<author>Mark Craven</author>
<author>Johan Kumlien</author>
</authors>
<title>Constructing biological knowledge bases by extracting information from text sources.</title>
<date>1999</date>
<booktitle>In ISMB,</booktitle>
<volume>volume</volume>
<contexts>
<context position="2696" citStr="Craven and Kumlien, 1999" startWordPosition="413" endWordPosition="416">onia DiedIn in 1997 in New York City. Figure 1: An example of a knowledge database comprising two facts and training sentences obtained by aligning this database to unlabeled text. precision and recall results (Zelenko et al., 2003). Unfortunately, these approaches need large amount of labeled data, and thus do not scale well to the great number of different types of fact found on the Web or in scientific articles. A promising approach, called distant or weak supervision, is to exploit an existing database of facts as training data, by aligning it to an unlabeled collection of text documents (Craven and Kumlien, 1999). In this article, we are interested in weakly supervised extraction of binary relations. A challenge pertaining to weak supervision is that the obtained training data is noisy and ambiguous (Riedel et al., 2010). Let us start with an example: if the fact Attended(Turing, King�s College) exists in the knowledge database and we observe the sentence Turing studied as an undergraduate from 1931 to 1934 at King’s College, Cambridge. which contains mentions of both entities Turing 1580 Proceedings of the 2014 Conference on Empirical Methods in Natural Language Processing (EMNLP), pages 1580–1590, O</context>
<context position="6614" citStr="Craven and Kumlien (1999)" startWordPosition="1024" endWordPosition="1027">Sekine (2006) and Banko et al. (2007), does not rely on any labeled data or even existing relations. Instead, open information extraction systems only use an unlabeled corpus, and output a set of extracted relations. Such systems are based on clustering (Shinyama and Sekine, 2006) or self-supervision (Banko et al., 2007). One of the limitations of these systems is the fact that they extract uncanonicalized relations. Weakly supervised learning. Weakly supervised learning refers to a broad class of methods, in which the learning system only have access to partial, ambiguous and noisy labeling. Craven and Kumlien (1999) were the first to propose a weakly supervised relation extractor. They aligned a knowledge database (the Yeast Protein Database) with scientific articles mentioning a particular relation, and then used the extracted sentences to learn a classifier for extracting that relation. Later, many different sources of weak labelings have been considered. Bellare and McCallum (2007) proposed a method to extract bibliographic relations based on conditional random fields and used a database of BibTex entries as weak supervision. Wu and Weld (2007) described a method to learn relations based on Wikipedia </context>
</contexts>
<marker>Craven, Kumlien, 1999</marker>
<rawString>Mark Craven and Johan Kumlien. 1999. Constructing biological knowledge bases by extracting information from text sources. In ISMB, volume 1999.</rawString>
</citation>
<citation valid="true">
<authors>
<author>Aron Culotta</author>
<author>Jeffrey Sorensen</author>
</authors>
<title>Dependency tree kernels for relation extraction.</title>
<date>2004</date>
<booktitle>In Proceedings of the ACL.</booktitle>
<contexts>
<context position="5852" citStr="Culotta and Sorensen (2004)" startWordPosition="906" endWordPosition="909">ion extraction was inspired by syntactic parsing: the system described by Miller et al. (1998) combines syntactic and semantic knowledge, and thus, part-of-speech tagging, parsing, named entity recognition and relation extraction all happen at the same time. The problem of relation extraction was later formulated as a classification problem: Kambhatla (2004) proposed to solve this problem using maximum entropy models using lexical, syntactic and semantic features. Kernel methods for relation extraction, based on shallow parse trees or dependency trees were introduced by Zelenko et al. (2003), Culotta and Sorensen (2004) and Bunescu and Mooney (2005). Unsupervised learning. The open information extraction paradigm, simultaneously proposed by Shinyama and Sekine (2006) and Banko et al. (2007), does not rely on any labeled data or even existing relations. Instead, open information extraction systems only use an unlabeled corpus, and output a set of extracted relations. Such systems are based on clustering (Shinyama and Sekine, 2006) or self-supervision (Banko et al., 2007). One of the limitations of these systems is the fact that they extract uncanonicalized relations. Weakly supervised learning. Weakly supervi</context>
</contexts>
<marker>Culotta, Sorensen, 2004</marker>
<rawString>Aron Culotta and Jeffrey Sorensen. 2004. Dependency tree kernels for relation extraction. In Proceedings of the ACL.</rawString>
</citation>
<citation valid="true">
<authors>
<author>Thomas G Dietterich</author>
<author>Richard H Lathrop</author>
<author>Tom´as Lozano-P´erez</author>
</authors>
<title>Solving the multiple instance problem with axis-parallel rectangles.</title>
<date>1997</date>
<journal>Artificial intelligence,</journal>
<volume>89</volume>
<issue>1</issue>
<marker>Dietterich, Lathrop, Lozano-P´erez, 1997</marker>
<rawString>Thomas G Dietterich, Richard H Lathrop, and Tom´as Lozano-P´erez. 1997. Solving the multiple instance problem with axis-parallel rectangles. Artificial intelligence, 89(1).</rawString>
</citation>
<citation valid="true">
<authors>
<author>Jenny Rose Finkel</author>
<author>Trond Grenager</author>
<author>Christopher Manning</author>
</authors>
<title>Incorporating non-local information into information extraction systems by Gibbs sampling.</title>
<date>2005</date>
<booktitle>In Proceedings of the ACL.</booktitle>
<contexts>
<context position="21352" citStr="Finkel et al., 2005" startWordPosition="3631" endWordPosition="3634">ot necessarily verify the inequality constraints defined in Eq. 1. In the following, we will use this rounding, refered to as argmax rounding, to obtain relation labels for each relation mention candidate. 6 Dataset and features In this section, we describe the dataset used in the experimental section and the features used to represent the data. 6.1 Dataset We consider the dataset introduced by Riedel et al. (2010). This dataset consists of articles from the New York Times corpus (Sandhaus, 2008), from which named entities where extracted and tagged using the Stanford named entity recognizer (Finkel et al., 2005). Consecutive tokens with the same category were treated as a single mention. These named entity mentions were then aligned with the Freebase knowledge database, by using a string match between the mentions and the canonical names of entities in Freebase. 6.2 Features We use the features extracted by Riedel et al. (2010), which were first introduced by Mintz et al. (2009). These features capture how two entity mentions are related in a given sentence, based on syntactic and lexical properties. Lexical features include: the sequence of words between the two entities, a window of k words before </context>
</contexts>
<marker>Finkel, Grenager, Manning, 2005</marker>
<rawString>Jenny Rose Finkel, Trond Grenager, and Christopher Manning. 2005. Incorporating non-local information into information extraction systems by Gibbs sampling. In Proceedings of the ACL.</rawString>
</citation>
<citation valid="true">
<authors>
<author>Raphael Hoffmann</author>
<author>Congle Zhang</author>
<author>Xiao Ling</author>
<author>Luke Zettlemoyer</author>
<author>Daniel S Weld</author>
</authors>
<title>Knowledgebased weak supervision for information extraction of overlapping relations.</title>
<date>2011</date>
<booktitle>In Proceedings of the ACL.</booktitle>
<contexts>
<context position="4337" citStr="Hoffmann et al., 2011" startWordPosition="674" endWordPosition="677">llege, but do not express the relation Attended. Thus, weak supervision lead to noisy examples. As noted by Riedel et al. (2010), such negative extracted sentences for existing facts can represent more than 30% of the data. Moreover, a given pair of entities, such as (Roy Lichtenstein, New York City), car verify multiple relations, such as BornIn and DiedIn. Weak supervision thus lead to ambiguous examples. This challenge is illustrated in Fig. 1. A solution to address it is to formulate the task of weakly supervised relation extraction as a multiple instance, multiple label learning problem (Hoffmann et al., 2011; Surdeanu et al., 2012). However, these formulations are often non-convex and thus suffer from local minimum. In this article, we make the following contributions: • We propose a new convex relaxation for the problem of weakly supervised relation extraction, based on discriminative clustering, • We propose an efficient algorithm to solve the associated convex program, • We demonstrate that our approach obtains state-of-the-art results on the dataset introduced by Riedel et al. (2010). To our knowledge, this paper is the first to propose a convex formulation for solving the problem of weakly s</context>
<context position="8244" citStr="Hoffmann et al. (2011)" startWordPosition="1279" endWordPosition="1282">go-naga/yago 1581 tiple instance learning (Dietterich et al., 1997) is a paradigm in which the learner receives bags of examples instead of individual examples. A positively labeled bag contains at least one positive example, but might also contains negative examples. In the context of relation extraction, Bunescu and Mooney (2007) introduced a kernel method for multiple instance learning, while Riedel et al. (2010) proposed a solution based on a graphical model. Both these methods allow only one label per bag, which is an asumption that is not true for relation extraction (see Fig. 1). Thus, Hoffmann et al. (2011) proposed a multiple instance, multiple label method, based on an undirected graphical model, to solve the problem of weakly supervised relation extraction. Finally, Surdeanu et al. (2012) also proposed a graphical model to solve this problem. One of their main contributions is to capture dependencies between relation labels, such as the fact that two labels cannot be generated jointly (e.g. the relations SpouseOf and BornIn). Discriminative clustering. Our approach is based on the discriminative clustering framework, introduced by Xu et al. (2004). The goal of discriminative clustering is to </context>
<context position="22607" citStr="Hoffmann et al. (2011)" startWordPosition="3840" endWordPosition="3843">cond entity, the corresponding part-of-speech tags, etc.. Syntactic features are based on the dependency tree of the sentence, and include: the path between the two entities, neighbors of the two entities that do not belong to the path. The OpenNLP3 part-of-speech tagger and the Malt parser (Nivre et al., 2007) were used to extract those features. 3opennlp.apache.org 1585 0.00 0.05 0.10 0.15 0.20 0.25 0.30 Recall Figure 3: Precision/recall curves for different methods on the Riedel et al. (2010) dataset, for the task of aggregate extraction. 0.9 0.7 0.6 0.4 0.2 0.8 0.5 0.3 Mintz et al. (2009) Hoffmann et al. (2011) Surdeanu et al. (2012) This work Precision 6.3 Implementation details In this section, we discuss some important implementation details. Kernel normalization. We normalized the kernel matrix XXT, so that its diagonal coefficients are equal to 1. This corresponds to normalizing the vectors x,,, so that they have a unit E2-norm. Choice of parameters. We kept 20% of the examples from the training set as a validation set, in order to choose the parameters of our method. We then re-train a model on the whole training set, using the chosen parameters. 7 Experimental evaluation In this section, we e</context>
<context position="23887" citStr="Hoffmann et al. (2011)" startWordPosition="4048" endWordPosition="4051">ion by comparing it to state-of-the art methods. 7.1 Baselines We now briefly present the different methods we compare to. Mintz et al. This baseline corresponds to the method described by Mintz et al. (2009). We use the implementation of Surdeanu et al. (2012), which slightly differs from the original method: each relation mention candidate is treated independently (and not collapsed across mentions for a given entity pair). This strategy allows to predict multiple labels for a given entity pair, by OR-ing the predictions for the different mentions. Hoffmann et al. This method, introduced by Hoffmann et al. (2011), is based on probabilistic graphical model of multi-instance multi-label learning. They proposed a learning method for this model, based on the perceptron algorithm (Collins, 2002) and a greedy search for the inference. We use the publicly available code of Hoffmann et al.4. Surdeanu et al. Finally, we compare our method to the one described by Surdeanu et al. (2012). This method is based on a two-layer graphical model, the first layer corresponding to 4www.cs.washington.edu/ai/raphaelh/mr/ 1586 0.0 0.1 0.2 0.3 0.4 0.5 0.6 Recall Figure 4: Precision/recall curves per relation for our method, </context>
<context position="26033" citStr="Hoffmann et al. (2011)" startWordPosition="4373" endWordPosition="4376">1, e2) is set to the maximal score of the different extractions of that fact. This is sometimes refered to as the soft-OR function. 7.3 Discussion Comparison with the state-of-the-art. We report results for the different methods on the dataset 5nlp.stanford.edu/software/mimlre.shtml introduced by Riedel et al. (2010) in Fig. 3. We observe that our approach generally outperforms the state of the art. Indeed, at equivalent recall, our method achieves better (or similar) precision than the other methods, except for very low recall (smaller than 0.05). The improvement over the methods proposed by Hoffmann et al. (2011) and Surdeanu et al. (2012), which are currently the best published results on this dataset, can be as high as 5 points in precision for the same recall point. Moreover, our method achieves a higher recall (0.30) than these two methods (0.25). Performance per relation. The dataset introduced by Riedel et al. (2010) is highly unbalanced: for example, the most common relation, /location/location/contains, represents almost half of the positive relations, while some relations are mentioned less than ten times. We thus decided to also report precision/recall curves for the five most common relatio</context>
<context position="27972" citStr="Hoffmann et al. (2011)" startWordPosition="4682" endWordPosition="4685">erson/place of birth in fact express this relation. In other words, many facts present in Freebase are not expressed in the corpus, and are thus impossible to extract. On the other hand, most facts for the relation /people/person/place lived are missing in Freebase. Therefore, many extractions produced by our system are considered false, but are in fact true positives. The problem of incomplete knowledge base was studied by Min et al. (2013). Sentential extraction. We finally report precision/recall curves for the task of sentential extraction, in Fig. 5, using the manually labeled dataset of Hoffmann et al. (2011). We observe that for most values of recall, our method achieves similar precision that the one proposed by Hoffmann et al. (2011), while extending the highest recall from 0.52 to 0.68. Thanks to this higher recall, our method achieves a highest F1 score of 0.66, compared to 0.61 obtained by the method proposed by Hoffmann et al. (2011). Method Runtime Mintz et al. (2009) 7 min Hoffmann et al. (2011) 2 min Surdeanu et al. (2012) 3 hours This work 3 hours Table 1: Comparison of running times for the different methods compared in the experimental section. 8 Conclusion In this article, we introdu</context>
</contexts>
<marker>Hoffmann, Zhang, Ling, Zettlemoyer, Weld, 2011</marker>
<rawString>Raphael Hoffmann, Congle Zhang, Xiao Ling, Luke Zettlemoyer, and Daniel S Weld. 2011. Knowledgebased weak supervision for information extraction of overlapping relations. In Proceedings of the ACL.</rawString>
</citation>
<citation valid="true">
<authors>
<author>Armand Joulin</author>
<author>Jean Ponce</author>
<author>Francis Bach</author>
</authors>
<title>Efficient optimization for discriminative latent class models.</title>
<date>2010</date>
<booktitle>In Adv. NIPS.</booktitle>
<contexts>
<context position="9144" citStr="Joulin et al., 2010" startWordPosition="1422" endWordPosition="1425">o capture dependencies between relation labels, such as the fact that two labels cannot be generated jointly (e.g. the relations SpouseOf and BornIn). Discriminative clustering. Our approach is based on the discriminative clustering framework, introduced by Xu et al. (2004). The goal of discriminative clustering is to find a labeling of the data points leading to a classifier with low classification error. Different formulations of discriminative clustering have been proposed, based on support vector machines (Xu et al., 2004), the squared loss (Bach and Harchaoui, 2007) or the logistic loss (Joulin et al., 2010). A big advantage of discriminative clustering is that weak supervision or prior information can easily be incorporated. Our work is closely related to the method proposed by Bojanowski et al. (2013) for learning the names of characters in movies. 3 Weakly supervised relation extraction In this article, our goal is to extract binary relations between entities from natural language text. Given a set of entities, a binary relation r is a collection of ordered pairs of entities. The statement that a pair of entities (e1, e2) belongs to the relation r is denoted by r(e1, e2) and this triple is cal</context>
</contexts>
<marker>Joulin, Ponce, Bach, 2010</marker>
<rawString>Armand Joulin, Jean Ponce, and Francis Bach. 2010. Efficient optimization for discriminative latent class models. In Adv. NIPS.</rawString>
</citation>
<citation valid="true">
<authors>
<author>Nanda Kambhatla</author>
</authors>
<title>Combining lexical, syntactic, and semantic features with maximum entropy models for information extraction.</title>
<date>2004</date>
<booktitle>In Proceedings of the ACL.</booktitle>
<contexts>
<context position="5585" citStr="Kambhatla (2004)" startWordPosition="869" endWordPosition="870"> Related work Supervised learning. Many approaches based on supervised learning have been proposed to solve the problem of relation extraction, and the corresponding literature is to large to be summarized here. One of the first supervised method for relation extraction was inspired by syntactic parsing: the system described by Miller et al. (1998) combines syntactic and semantic knowledge, and thus, part-of-speech tagging, parsing, named entity recognition and relation extraction all happen at the same time. The problem of relation extraction was later formulated as a classification problem: Kambhatla (2004) proposed to solve this problem using maximum entropy models using lexical, syntactic and semantic features. Kernel methods for relation extraction, based on shallow parse trees or dependency trees were introduced by Zelenko et al. (2003), Culotta and Sorensen (2004) and Bunescu and Mooney (2005). Unsupervised learning. The open information extraction paradigm, simultaneously proposed by Shinyama and Sekine (2006) and Banko et al. (2007), does not rely on any labeled data or even existing relations. Instead, open information extraction systems only use an unlabeled corpus, and output a set of </context>
</contexts>
<marker>Kambhatla, 2004</marker>
<rawString>Nanda Kambhatla. 2004. Combining lexical, syntactic, and semantic features with maximum entropy models for information extraction. In Proceedings of the ACL.</rawString>
</citation>
<citation valid="true">
<authors>
<author>Scott Miller</author>
<author>Michael Crystal</author>
<author>Heidi Fox</author>
<author>Lance Ramshaw</author>
<author>Richard Schwartz</author>
<author>Rebecca Stone</author>
<author>Ralph Weischedel</author>
</authors>
<title>Algorithms that learn to extract information.</title>
<date>1998</date>
<booktitle>In Proceedings of MUC-7.</booktitle>
<contexts>
<context position="5319" citStr="Miller et al. (1998)" startWordPosition="829" endWordPosition="832"> program, • We demonstrate that our approach obtains state-of-the-art results on the dataset introduced by Riedel et al. (2010). To our knowledge, this paper is the first to propose a convex formulation for solving the problem of weakly supervised relation extraction. 2 Related work Supervised learning. Many approaches based on supervised learning have been proposed to solve the problem of relation extraction, and the corresponding literature is to large to be summarized here. One of the first supervised method for relation extraction was inspired by syntactic parsing: the system described by Miller et al. (1998) combines syntactic and semantic knowledge, and thus, part-of-speech tagging, parsing, named entity recognition and relation extraction all happen at the same time. The problem of relation extraction was later formulated as a classification problem: Kambhatla (2004) proposed to solve this problem using maximum entropy models using lexical, syntactic and semantic features. Kernel methods for relation extraction, based on shallow parse trees or dependency trees were introduced by Zelenko et al. (2003), Culotta and Sorensen (2004) and Bunescu and Mooney (2005). Unsupervised learning. The open inf</context>
</contexts>
<marker>Miller, Crystal, Fox, Ramshaw, Schwartz, Stone, Weischedel, 1998</marker>
<rawString>Scott Miller, Michael Crystal, Heidi Fox, Lance Ramshaw, Richard Schwartz, Rebecca Stone, and Ralph Weischedel. 1998. Algorithms that learn to extract information. In Proceedings of MUC-7.</rawString>
</citation>
<citation valid="true">
<authors>
<author>Bonan Min</author>
<author>Ralph Grishman</author>
<author>Li Wan</author>
<author>Chang Wang</author>
<author>David Gondek</author>
</authors>
<title>Distant supervision for relation extraction with an incomplete knowledge base.</title>
<date>2013</date>
<booktitle>In Proceedings of HLT-NAACL.</booktitle>
<contexts>
<context position="27795" citStr="Min et al. (2013)" startWordPosition="4654" endWordPosition="4657">r than the extraction of the two first. Upon examination of the data, this can partly be explained by the fact that almost no sentences extracted for the relation /people/person/place of birth in fact express this relation. In other words, many facts present in Freebase are not expressed in the corpus, and are thus impossible to extract. On the other hand, most facts for the relation /people/person/place lived are missing in Freebase. Therefore, many extractions produced by our system are considered false, but are in fact true positives. The problem of incomplete knowledge base was studied by Min et al. (2013). Sentential extraction. We finally report precision/recall curves for the task of sentential extraction, in Fig. 5, using the manually labeled dataset of Hoffmann et al. (2011). We observe that for most values of recall, our method achieves similar precision that the one proposed by Hoffmann et al. (2011), while extending the highest recall from 0.52 to 0.68. Thanks to this higher recall, our method achieves a highest F1 score of 0.66, compared to 0.61 obtained by the method proposed by Hoffmann et al. (2011). Method Runtime Mintz et al. (2009) 7 min Hoffmann et al. (2011) 2 min Surdeanu et a</context>
</contexts>
<marker>Min, Grishman, Wan, Wang, Gondek, 2013</marker>
<rawString>Bonan Min, Ralph Grishman, Li Wan, Chang Wang, and David Gondek. 2013. Distant supervision for relation extraction with an incomplete knowledge base. In Proceedings of HLT-NAACL.</rawString>
</citation>
<citation valid="true">
<authors>
<author>Mike Mintz</author>
<author>Steven Bills</author>
<author>Rion Snow</author>
<author>Dan Jurafsky</author>
</authors>
<title>Distant supervision for relation extraction without labeled data.</title>
<date>2009</date>
<booktitle>In Proceedings of the ACL-IJCNLP.</booktitle>
<contexts>
<context position="7283" citStr="Mintz et al., 2009" startWordPosition="1129" endWordPosition="1132">tion extractor. They aligned a knowledge database (the Yeast Protein Database) with scientific articles mentioning a particular relation, and then used the extracted sentences to learn a classifier for extracting that relation. Later, many different sources of weak labelings have been considered. Bellare and McCallum (2007) proposed a method to extract bibliographic relations based on conditional random fields and used a database of BibTex entries as weak supervision. Wu and Weld (2007) described a method to learn relations based on Wikipedia infoboxes. Knowledge databases, such as Freebase1 (Mintz et al., 2009; Sun et al., 2011) and YAGO2 (Nguyen and Moschitti, 2011) were also considered as a source of weak supervision. Multiple instance learning. The methods we previously mentionned transform the weakly supervised problem into a fully supervised one, leading to noisy training datasets (see Fig. 1). Mul1www.freebase.com 2www.mpi-inf.mpg.de/yago-naga/yago 1581 tiple instance learning (Dietterich et al., 1997) is a paradigm in which the learner receives bags of examples instead of individual examples. A positively labeled bag contains at least one positive example, but might also contains negative ex</context>
<context position="21726" citStr="Mintz et al. (2009)" startWordPosition="3692" endWordPosition="3695"> dataset introduced by Riedel et al. (2010). This dataset consists of articles from the New York Times corpus (Sandhaus, 2008), from which named entities where extracted and tagged using the Stanford named entity recognizer (Finkel et al., 2005). Consecutive tokens with the same category were treated as a single mention. These named entity mentions were then aligned with the Freebase knowledge database, by using a string match between the mentions and the canonical names of entities in Freebase. 6.2 Features We use the features extracted by Riedel et al. (2010), which were first introduced by Mintz et al. (2009). These features capture how two entity mentions are related in a given sentence, based on syntactic and lexical properties. Lexical features include: the sequence of words between the two entities, a window of k words before the first entity and after the second entity, the corresponding part-of-speech tags, etc.. Syntactic features are based on the dependency tree of the sentence, and include: the path between the two entities, neighbors of the two entities that do not belong to the path. The OpenNLP3 part-of-speech tagger and the Malt parser (Nivre et al., 2007) were used to extract those f</context>
<context position="23473" citStr="Mintz et al. (2009)" startWordPosition="3983" endWordPosition="3986">This corresponds to normalizing the vectors x,,, so that they have a unit E2-norm. Choice of parameters. We kept 20% of the examples from the training set as a validation set, in order to choose the parameters of our method. We then re-train a model on the whole training set, using the chosen parameters. 7 Experimental evaluation In this section, we evaluate our approach to weakly supervised relation extraction by comparing it to state-of-the art methods. 7.1 Baselines We now briefly present the different methods we compare to. Mintz et al. This baseline corresponds to the method described by Mintz et al. (2009). We use the implementation of Surdeanu et al. (2012), which slightly differs from the original method: each relation mention candidate is treated independently (and not collapsed across mentions for a given entity pair). This strategy allows to predict multiple labels for a given entity pair, by OR-ing the predictions for the different mentions. Hoffmann et al. This method, introduced by Hoffmann et al. (2011), is based on probabilistic graphical model of multi-instance multi-label learning. They proposed a learning method for this model, based on the perceptron algorithm (Collins, 2002) and </context>
<context position="28346" citStr="Mintz et al. (2009)" startWordPosition="4748" endWordPosition="4751">roblem of incomplete knowledge base was studied by Min et al. (2013). Sentential extraction. We finally report precision/recall curves for the task of sentential extraction, in Fig. 5, using the manually labeled dataset of Hoffmann et al. (2011). We observe that for most values of recall, our method achieves similar precision that the one proposed by Hoffmann et al. (2011), while extending the highest recall from 0.52 to 0.68. Thanks to this higher recall, our method achieves a highest F1 score of 0.66, compared to 0.61 obtained by the method proposed by Hoffmann et al. (2011). Method Runtime Mintz et al. (2009) 7 min Hoffmann et al. (2011) 2 min Surdeanu et al. (2012) 3 hours This work 3 hours Table 1: Comparison of running times for the different methods compared in the experimental section. 8 Conclusion In this article, we introduced a new formulation for weakly supervised relation extraction. Our method is based on a constrained discriminative formulation of the multiple instance, multiple label learning problem. Using the squared loss, we obtained a convex relaxation of this formulation, allowing us to obtain an approximate solution to the initial integer quadratic program. Thus, our method is n</context>
</contexts>
<marker>Mintz, Bills, Snow, Jurafsky, 2009</marker>
<rawString>Mike Mintz, Steven Bills, Rion Snow, and Dan Jurafsky. 2009. Distant supervision for relation extraction without labeled data. In Proceedings of the ACL-IJCNLP.</rawString>
</citation>
<citation valid="true">
<authors>
<author>Yurii Nesterov</author>
</authors>
<title>Gradient methods for minimizing composite objective function.</title>
<date>2007</date>
<contexts>
<context position="19004" citStr="Nesterov, 2007" startWordPosition="3231" endWordPosition="3232">x X in the optimization procedure. Second, the constraints imposed on dual variables are simpler than constraints imposed on primal variables. Again, we will exploit this structure in the proposed optimization procedure. Given a solution of the dual problem, the associated primal variable Y is equal to: Y = (XXT + AIN)Z. Thus, we do not need to compute the inverse of the matrix (XXT + AIN) to obtain a solution to the primal problem once we have solved the dual. 5.3 Optimization of the dual problem We propose to solve the dual problem using the accelerated projected gradient descent algorithm (Nesterov, 2007; Beck and Teboulle, 2009). Indeed, computing the gradient of the dual cost function is efficient, since the matrix X is sparse. Moreover, the constraints on the dual variables are simple and it is thus efficient to project onto this set of constraints. See Appendix B for more details. Complexity. The overall complexity of one step of the accelerated projected gradient descent algorithm is O(NFK), where F is the average number of features per relation mention candidate. This means that the complexity of solving the quadratic problem corresponding to our approach is linear with respect to the n</context>
</contexts>
<marker>Nesterov, 2007</marker>
<rawString>Yurii Nesterov. 2007. Gradient methods for minimizing composite objective function.</rawString>
</citation>
<citation valid="true">
<authors>
<author>Truc-Vien T Nguyen</author>
<author>Alessandro Moschitti</author>
</authors>
<title>End-to-end relation extraction using distant supervision from external semantic repositories.</title>
<date>2011</date>
<booktitle>In Proceedings of the ACL.</booktitle>
<contexts>
<context position="7341" citStr="Nguyen and Moschitti, 2011" startWordPosition="1139" endWordPosition="1142"> (the Yeast Protein Database) with scientific articles mentioning a particular relation, and then used the extracted sentences to learn a classifier for extracting that relation. Later, many different sources of weak labelings have been considered. Bellare and McCallum (2007) proposed a method to extract bibliographic relations based on conditional random fields and used a database of BibTex entries as weak supervision. Wu and Weld (2007) described a method to learn relations based on Wikipedia infoboxes. Knowledge databases, such as Freebase1 (Mintz et al., 2009; Sun et al., 2011) and YAGO2 (Nguyen and Moschitti, 2011) were also considered as a source of weak supervision. Multiple instance learning. The methods we previously mentionned transform the weakly supervised problem into a fully supervised one, leading to noisy training datasets (see Fig. 1). Mul1www.freebase.com 2www.mpi-inf.mpg.de/yago-naga/yago 1581 tiple instance learning (Dietterich et al., 1997) is a paradigm in which the learner receives bags of examples instead of individual examples. A positively labeled bag contains at least one positive example, but might also contains negative examples. In the context of relation extraction, Bunescu and</context>
</contexts>
<marker>Nguyen, Moschitti, 2011</marker>
<rawString>Truc-Vien T Nguyen and Alessandro Moschitti. 2011. End-to-end relation extraction using distant supervision from external semantic repositories. In Proceedings of the ACL.</rawString>
</citation>
<citation valid="true">
<authors>
<author>Joakim Nivre</author>
<author>Johan Hall</author>
<author>Jens Nilsson</author>
</authors>
<title>Atanas Chanev, G¨ulsen Eryigit, Sandra K¨ubler, Svetoslav Marinov, and Erwin Marsi.</title>
<date>2007</date>
<journal>Natural Language Engineering,</journal>
<volume>13</volume>
<issue>02</issue>
<contexts>
<context position="22297" citStr="Nivre et al., 2007" startWordPosition="3788" endWordPosition="3791">ich were first introduced by Mintz et al. (2009). These features capture how two entity mentions are related in a given sentence, based on syntactic and lexical properties. Lexical features include: the sequence of words between the two entities, a window of k words before the first entity and after the second entity, the corresponding part-of-speech tags, etc.. Syntactic features are based on the dependency tree of the sentence, and include: the path between the two entities, neighbors of the two entities that do not belong to the path. The OpenNLP3 part-of-speech tagger and the Malt parser (Nivre et al., 2007) were used to extract those features. 3opennlp.apache.org 1585 0.00 0.05 0.10 0.15 0.20 0.25 0.30 Recall Figure 3: Precision/recall curves for different methods on the Riedel et al. (2010) dataset, for the task of aggregate extraction. 0.9 0.7 0.6 0.4 0.2 0.8 0.5 0.3 Mintz et al. (2009) Hoffmann et al. (2011) Surdeanu et al. (2012) This work Precision 6.3 Implementation details In this section, we discuss some important implementation details. Kernel normalization. We normalized the kernel matrix XXT, so that its diagonal coefficients are equal to 1. This corresponds to normalizing the vectors</context>
</contexts>
<marker>Nivre, Hall, Nilsson, 2007</marker>
<rawString>Joakim Nivre, Johan Hall, Jens Nilsson, Atanas Chanev, G¨ulsen Eryigit, Sandra K¨ubler, Svetoslav Marinov, and Erwin Marsi. 2007. MaltParser: A language-independent system for data-driven dependency parsing. Natural Language Engineering, 13(02).</rawString>
</citation>
<citation valid="true">
<authors>
<author>Sebastian Riedel</author>
<author>Limin Yao</author>
<author>Andrew McCallum</author>
</authors>
<title>Modeling relations and their mentions without labeled text.</title>
<date>2010</date>
<booktitle>In Machine Learning and Knowledge Discovery in Databases.</booktitle>
<contexts>
<context position="997" citStr="Riedel et al. (2010)" startWordPosition="148" endWordPosition="151">he task of relation extraction can easily be scaled to hundreds of different relationships. However, distant supervision leads to a challenging multiple instance, multiple label learning problem. Most of the proposed solutions to this problem are based on non-convex formulations, and are thus prone to local minima. In this article, we propose a new approach to the problem of weakly supervised relation extraction, based on discriminative clustering and leading to a convex formulation. We demonstrate that our approach outperforms state-of-the-art methods on the challenging dataset introduced by Riedel et al. (2010). 1 Introduction Information extraction refers to the broad task of automatically extracting structured information from unstructured documents. An example is the extraction of named entities and the relations between those entities from natural language texts. In the age of the world wide web and big data, information extraction is quickly becoming pervasive. For example, in 2013, more than 130, 000 scientific articles were published about cancer. Keeping track with that quantity of information is almost impossible, and it is thus of utmost importance to transform the knowledge contained in t</context>
<context position="2908" citStr="Riedel et al., 2010" startWordPosition="447" endWordPosition="450">o et al., 2003). Unfortunately, these approaches need large amount of labeled data, and thus do not scale well to the great number of different types of fact found on the Web or in scientific articles. A promising approach, called distant or weak supervision, is to exploit an existing database of facts as training data, by aligning it to an unlabeled collection of text documents (Craven and Kumlien, 1999). In this article, we are interested in weakly supervised extraction of binary relations. A challenge pertaining to weak supervision is that the obtained training data is noisy and ambiguous (Riedel et al., 2010). Let us start with an example: if the fact Attended(Turing, King�s College) exists in the knowledge database and we observe the sentence Turing studied as an undergraduate from 1931 to 1934 at King’s College, Cambridge. which contains mentions of both entities Turing 1580 Proceedings of the 2014 Conference on Empirical Methods in Natural Language Processing (EMNLP), pages 1580–1590, October 25-29, 2014, Doha, Qatar. c�2014 Association for Computational Linguistics and King&apos;s College, then this sentence might express the fact that Alan Turing attended King’s College, and thus, might be a usefu</context>
<context position="4826" citStr="Riedel et al. (2010)" startWordPosition="751" endWordPosition="754">te the task of weakly supervised relation extraction as a multiple instance, multiple label learning problem (Hoffmann et al., 2011; Surdeanu et al., 2012). However, these formulations are often non-convex and thus suffer from local minimum. In this article, we make the following contributions: • We propose a new convex relaxation for the problem of weakly supervised relation extraction, based on discriminative clustering, • We propose an efficient algorithm to solve the associated convex program, • We demonstrate that our approach obtains state-of-the-art results on the dataset introduced by Riedel et al. (2010). To our knowledge, this paper is the first to propose a convex formulation for solving the problem of weakly supervised relation extraction. 2 Related work Supervised learning. Many approaches based on supervised learning have been proposed to solve the problem of relation extraction, and the corresponding literature is to large to be summarized here. One of the first supervised method for relation extraction was inspired by syntactic parsing: the system described by Miller et al. (1998) combines syntactic and semantic knowledge, and thus, part-of-speech tagging, parsing, named entity recogni</context>
<context position="8041" citStr="Riedel et al. (2010)" startWordPosition="1243" endWordPosition="1246">g. The methods we previously mentionned transform the weakly supervised problem into a fully supervised one, leading to noisy training datasets (see Fig. 1). Mul1www.freebase.com 2www.mpi-inf.mpg.de/yago-naga/yago 1581 tiple instance learning (Dietterich et al., 1997) is a paradigm in which the learner receives bags of examples instead of individual examples. A positively labeled bag contains at least one positive example, but might also contains negative examples. In the context of relation extraction, Bunescu and Mooney (2007) introduced a kernel method for multiple instance learning, while Riedel et al. (2010) proposed a solution based on a graphical model. Both these methods allow only one label per bag, which is an asumption that is not true for relation extraction (see Fig. 1). Thus, Hoffmann et al. (2011) proposed a multiple instance, multiple label method, based on an undirected graphical model, to solve the problem of weakly supervised relation extraction. Finally, Surdeanu et al. (2012) also proposed a graphical model to solve this problem. One of their main contributions is to capture dependencies between relation labels, such as the fact that two labels cannot be generated jointly (e.g. th</context>
<context position="21150" citStr="Riedel et al. (2010)" startWordPosition="3599" endWordPosition="3602">ix Y on the set of indicator matrices {M E {0, 1}Nx(K+1) |M1 = 1 . This projection consists in taking the maximum value along the rows of the matrix Y. It should be noted that the obtained matrix does not necessarily verify the inequality constraints defined in Eq. 1. In the following, we will use this rounding, refered to as argmax rounding, to obtain relation labels for each relation mention candidate. 6 Dataset and features In this section, we describe the dataset used in the experimental section and the features used to represent the data. 6.1 Dataset We consider the dataset introduced by Riedel et al. (2010). This dataset consists of articles from the New York Times corpus (Sandhaus, 2008), from which named entities where extracted and tagged using the Stanford named entity recognizer (Finkel et al., 2005). Consecutive tokens with the same category were treated as a single mention. These named entity mentions were then aligned with the Freebase knowledge database, by using a string match between the mentions and the canonical names of entities in Freebase. 6.2 Features We use the features extracted by Riedel et al. (2010), which were first introduced by Mintz et al. (2009). These features capture</context>
<context position="22485" citStr="Riedel et al. (2010)" startWordPosition="3817" endWordPosition="3820">es include: the sequence of words between the two entities, a window of k words before the first entity and after the second entity, the corresponding part-of-speech tags, etc.. Syntactic features are based on the dependency tree of the sentence, and include: the path between the two entities, neighbors of the two entities that do not belong to the path. The OpenNLP3 part-of-speech tagger and the Malt parser (Nivre et al., 2007) were used to extract those features. 3opennlp.apache.org 1585 0.00 0.05 0.10 0.15 0.20 0.25 0.30 Recall Figure 3: Precision/recall curves for different methods on the Riedel et al. (2010) dataset, for the task of aggregate extraction. 0.9 0.7 0.6 0.4 0.2 0.8 0.5 0.3 Mintz et al. (2009) Hoffmann et al. (2011) Surdeanu et al. (2012) This work Precision 6.3 Implementation details In this section, we discuss some important implementation details. Kernel normalization. We normalized the kernel matrix XXT, so that its diagonal coefficients are equal to 1. This corresponds to normalizing the vectors x,,, so that they have a unit E2-norm. Choice of parameters. We kept 20% of the examples from the training set as a validation set, in order to choose the parameters of our method. We the</context>
<context position="24514" citStr="Riedel et al. (2010)" startWordPosition="4148" endWordPosition="4151">sed on probabilistic graphical model of multi-instance multi-label learning. They proposed a learning method for this model, based on the perceptron algorithm (Collins, 2002) and a greedy search for the inference. We use the publicly available code of Hoffmann et al.4. Surdeanu et al. Finally, we compare our method to the one described by Surdeanu et al. (2012). This method is based on a two-layer graphical model, the first layer corresponding to 4www.cs.washington.edu/ai/raphaelh/mr/ 1586 0.0 0.1 0.2 0.3 0.4 0.5 0.6 Recall Figure 4: Precision/recall curves per relation for our method, on the Riedel et al. (2010) dataset, for the task of aggregate extraction. 0.6 0.4 0.2 0.0 0.8 1.0 /location/location/contains /people/person/place_lived /person/person/nationality /people/person/place_of_birth /business/person/company Precision a relation classifier at the mention level, while the second layer is aggregating the different prediction for a given entity pair. In particular, this second layer capture dependencies between relation labels, such as the fact that two labels cannot be generated jointly (e.g. the relations SpouseOf and BornIn). This model is trained by using hard discriminative Expectation-Maxi</context>
<context position="26349" citStr="Riedel et al. (2010)" startWordPosition="4428" endWordPosition="4431"> in Fig. 3. We observe that our approach generally outperforms the state of the art. Indeed, at equivalent recall, our method achieves better (or similar) precision than the other methods, except for very low recall (smaller than 0.05). The improvement over the methods proposed by Hoffmann et al. (2011) and Surdeanu et al. (2012), which are currently the best published results on this dataset, can be as high as 5 points in precision for the same recall point. Moreover, our method achieves a higher recall (0.30) than these two methods (0.25). Performance per relation. The dataset introduced by Riedel et al. (2010) is highly unbalanced: for example, the most common relation, /location/location/contains, represents almost half of the positive relations, while some relations are mentioned less than ten times. We thus decided to also report precision/recall curves for the five most common relations of that dataset in Fig. 4. First, we observe that the perfomances vary a lot from a relation to another. The frequence of the different relations is not the 1587 Recall Figure 5: Precision/recall curves for the task of sentential extraction, on the manually labeled dataset of Hoffmann et al. (2011). only factor </context>
<context position="29079" citStr="Riedel et al. (2010)" startWordPosition="4867" endWordPosition="4870">ng times for the different methods compared in the experimental section. 8 Conclusion In this article, we introduced a new formulation for weakly supervised relation extraction. Our method is based on a constrained discriminative formulation of the multiple instance, multiple label learning problem. Using the squared loss, we obtained a convex relaxation of this formulation, allowing us to obtain an approximate solution to the initial integer quadratic program. Thus, our method is not sensitive to initialization. We demonstrated the competitiveness of our approach on the dataset introduced by Riedel et al. (2010), on which our method outperforms the state of the art methods for weakly supervised relation extraction, on both aggregate and sentential extraction. As noted earlier, another advantage of our method is the fact that it is easily kernelizable. We would like to explore the use of kernels, such as the ones introduced by Zelenko et al. (2003), Culotta and Sorensen (2004) and Bunescu and Mooney (2005), in future work. We believe that such kernels could improve the relatively low recall obtained so far by weakly supervised method for relation extraction. Acknowledgments The author is supported by </context>
</contexts>
<marker>Riedel, Yao, McCallum, 2010</marker>
<rawString>Sebastian Riedel, Limin Yao, and Andrew McCallum. 2010. Modeling relations and their mentions without labeled text. In Machine Learning and Knowledge Discovery in Databases.</rawString>
</citation>
<citation valid="true">
<authors>
<author>Evan Sandhaus</author>
</authors>
<title>The new york times annotated corpus. Linguistic Data Consortium,</title>
<date>2008</date>
<location>Philadelphia, 6(12).</location>
<contexts>
<context position="21233" citStr="Sandhaus, 2008" startWordPosition="3614" endWordPosition="3615">sts in taking the maximum value along the rows of the matrix Y. It should be noted that the obtained matrix does not necessarily verify the inequality constraints defined in Eq. 1. In the following, we will use this rounding, refered to as argmax rounding, to obtain relation labels for each relation mention candidate. 6 Dataset and features In this section, we describe the dataset used in the experimental section and the features used to represent the data. 6.1 Dataset We consider the dataset introduced by Riedel et al. (2010). This dataset consists of articles from the New York Times corpus (Sandhaus, 2008), from which named entities where extracted and tagged using the Stanford named entity recognizer (Finkel et al., 2005). Consecutive tokens with the same category were treated as a single mention. These named entity mentions were then aligned with the Freebase knowledge database, by using a string match between the mentions and the canonical names of entities in Freebase. 6.2 Features We use the features extracted by Riedel et al. (2010), which were first introduced by Mintz et al. (2009). These features capture how two entity mentions are related in a given sentence, based on syntactic and le</context>
</contexts>
<marker>Sandhaus, 2008</marker>
<rawString>Evan Sandhaus. 2008. The new york times annotated corpus. Linguistic Data Consortium, Philadelphia, 6(12).</rawString>
</citation>
<citation valid="true">
<authors>
<author>Yusuke Shinyama</author>
<author>Satoshi Sekine</author>
</authors>
<title>Preemptive information extraction using unrestricted relation discovery.</title>
<date>2006</date>
<booktitle>In Proceedings of the HLT-NAACL.</booktitle>
<contexts>
<context position="6002" citStr="Shinyama and Sekine (2006)" startWordPosition="925" endWordPosition="928">t-of-speech tagging, parsing, named entity recognition and relation extraction all happen at the same time. The problem of relation extraction was later formulated as a classification problem: Kambhatla (2004) proposed to solve this problem using maximum entropy models using lexical, syntactic and semantic features. Kernel methods for relation extraction, based on shallow parse trees or dependency trees were introduced by Zelenko et al. (2003), Culotta and Sorensen (2004) and Bunescu and Mooney (2005). Unsupervised learning. The open information extraction paradigm, simultaneously proposed by Shinyama and Sekine (2006) and Banko et al. (2007), does not rely on any labeled data or even existing relations. Instead, open information extraction systems only use an unlabeled corpus, and output a set of extracted relations. Such systems are based on clustering (Shinyama and Sekine, 2006) or self-supervision (Banko et al., 2007). One of the limitations of these systems is the fact that they extract uncanonicalized relations. Weakly supervised learning. Weakly supervised learning refers to a broad class of methods, in which the learning system only have access to partial, ambiguous and noisy labeling. Craven and Ku</context>
</contexts>
<marker>Shinyama, Sekine, 2006</marker>
<rawString>Yusuke Shinyama and Satoshi Sekine. 2006. Preemptive information extraction using unrestricted relation discovery. In Proceedings of the HLT-NAACL.</rawString>
</citation>
<citation valid="true">
<authors>
<author>Ang Sun</author>
<author>Ralph Grishman</author>
<author>Wei Xu</author>
<author>Bonan Min</author>
</authors>
<title>New york university 2011 system for kbp slot filling.</title>
<date>2011</date>
<booktitle>In Proceedings of the Text Analytics Conference.</booktitle>
<contexts>
<context position="7302" citStr="Sun et al., 2011" startWordPosition="1133" endWordPosition="1136"> aligned a knowledge database (the Yeast Protein Database) with scientific articles mentioning a particular relation, and then used the extracted sentences to learn a classifier for extracting that relation. Later, many different sources of weak labelings have been considered. Bellare and McCallum (2007) proposed a method to extract bibliographic relations based on conditional random fields and used a database of BibTex entries as weak supervision. Wu and Weld (2007) described a method to learn relations based on Wikipedia infoboxes. Knowledge databases, such as Freebase1 (Mintz et al., 2009; Sun et al., 2011) and YAGO2 (Nguyen and Moschitti, 2011) were also considered as a source of weak supervision. Multiple instance learning. The methods we previously mentionned transform the weakly supervised problem into a fully supervised one, leading to noisy training datasets (see Fig. 1). Mul1www.freebase.com 2www.mpi-inf.mpg.de/yago-naga/yago 1581 tiple instance learning (Dietterich et al., 1997) is a paradigm in which the learner receives bags of examples instead of individual examples. A positively labeled bag contains at least one positive example, but might also contains negative examples. In the cont</context>
</contexts>
<marker>Sun, Grishman, Xu, Min, 2011</marker>
<rawString>Ang Sun, Ralph Grishman, Wei Xu, and Bonan Min. 2011. New york university 2011 system for kbp slot filling. In Proceedings of the Text Analytics Conference.</rawString>
</citation>
<citation valid="true">
<authors>
<author>Mihai Surdeanu</author>
<author>Julie Tibshirani</author>
<author>Ramesh Nallapati</author>
<author>Christopher D Manning</author>
</authors>
<title>Multi-instance multi-label learning for relation extraction.</title>
<date>2012</date>
<booktitle>In Proceedings of EMNLP-CoNLL.</booktitle>
<contexts>
<context position="4361" citStr="Surdeanu et al., 2012" startWordPosition="678" endWordPosition="681">ss the relation Attended. Thus, weak supervision lead to noisy examples. As noted by Riedel et al. (2010), such negative extracted sentences for existing facts can represent more than 30% of the data. Moreover, a given pair of entities, such as (Roy Lichtenstein, New York City), car verify multiple relations, such as BornIn and DiedIn. Weak supervision thus lead to ambiguous examples. This challenge is illustrated in Fig. 1. A solution to address it is to formulate the task of weakly supervised relation extraction as a multiple instance, multiple label learning problem (Hoffmann et al., 2011; Surdeanu et al., 2012). However, these formulations are often non-convex and thus suffer from local minimum. In this article, we make the following contributions: • We propose a new convex relaxation for the problem of weakly supervised relation extraction, based on discriminative clustering, • We propose an efficient algorithm to solve the associated convex program, • We demonstrate that our approach obtains state-of-the-art results on the dataset introduced by Riedel et al. (2010). To our knowledge, this paper is the first to propose a convex formulation for solving the problem of weakly supervised relation extra</context>
<context position="8432" citStr="Surdeanu et al. (2012)" startWordPosition="1307" endWordPosition="1310">ntains at least one positive example, but might also contains negative examples. In the context of relation extraction, Bunescu and Mooney (2007) introduced a kernel method for multiple instance learning, while Riedel et al. (2010) proposed a solution based on a graphical model. Both these methods allow only one label per bag, which is an asumption that is not true for relation extraction (see Fig. 1). Thus, Hoffmann et al. (2011) proposed a multiple instance, multiple label method, based on an undirected graphical model, to solve the problem of weakly supervised relation extraction. Finally, Surdeanu et al. (2012) also proposed a graphical model to solve this problem. One of their main contributions is to capture dependencies between relation labels, such as the fact that two labels cannot be generated jointly (e.g. the relations SpouseOf and BornIn). Discriminative clustering. Our approach is based on the discriminative clustering framework, introduced by Xu et al. (2004). The goal of discriminative clustering is to find a labeling of the data points leading to a classifier with low classification error. Different formulations of discriminative clustering have been proposed, based on support vector ma</context>
<context position="22630" citStr="Surdeanu et al. (2012)" startWordPosition="3844" endWordPosition="3847">ponding part-of-speech tags, etc.. Syntactic features are based on the dependency tree of the sentence, and include: the path between the two entities, neighbors of the two entities that do not belong to the path. The OpenNLP3 part-of-speech tagger and the Malt parser (Nivre et al., 2007) were used to extract those features. 3opennlp.apache.org 1585 0.00 0.05 0.10 0.15 0.20 0.25 0.30 Recall Figure 3: Precision/recall curves for different methods on the Riedel et al. (2010) dataset, for the task of aggregate extraction. 0.9 0.7 0.6 0.4 0.2 0.8 0.5 0.3 Mintz et al. (2009) Hoffmann et al. (2011) Surdeanu et al. (2012) This work Precision 6.3 Implementation details In this section, we discuss some important implementation details. Kernel normalization. We normalized the kernel matrix XXT, so that its diagonal coefficients are equal to 1. This corresponds to normalizing the vectors x,,, so that they have a unit E2-norm. Choice of parameters. We kept 20% of the examples from the training set as a validation set, in order to choose the parameters of our method. We then re-train a model on the whole training set, using the chosen parameters. 7 Experimental evaluation In this section, we evaluate our approach to</context>
<context position="24257" citStr="Surdeanu et al. (2012)" startWordPosition="4109" endWordPosition="4112">and not collapsed across mentions for a given entity pair). This strategy allows to predict multiple labels for a given entity pair, by OR-ing the predictions for the different mentions. Hoffmann et al. This method, introduced by Hoffmann et al. (2011), is based on probabilistic graphical model of multi-instance multi-label learning. They proposed a learning method for this model, based on the perceptron algorithm (Collins, 2002) and a greedy search for the inference. We use the publicly available code of Hoffmann et al.4. Surdeanu et al. Finally, we compare our method to the one described by Surdeanu et al. (2012). This method is based on a two-layer graphical model, the first layer corresponding to 4www.cs.washington.edu/ai/raphaelh/mr/ 1586 0.0 0.1 0.2 0.3 0.4 0.5 0.6 Recall Figure 4: Precision/recall curves per relation for our method, on the Riedel et al. (2010) dataset, for the task of aggregate extraction. 0.6 0.4 0.2 0.0 0.8 1.0 /location/location/contains /people/person/place_lived /person/person/nationality /people/person/place_of_birth /business/person/company Precision a relation classifier at the mention level, while the second layer is aggregating the different prediction for a given entit</context>
<context position="26060" citStr="Surdeanu et al. (2012)" startWordPosition="4378" endWordPosition="4381">l score of the different extractions of that fact. This is sometimes refered to as the soft-OR function. 7.3 Discussion Comparison with the state-of-the-art. We report results for the different methods on the dataset 5nlp.stanford.edu/software/mimlre.shtml introduced by Riedel et al. (2010) in Fig. 3. We observe that our approach generally outperforms the state of the art. Indeed, at equivalent recall, our method achieves better (or similar) precision than the other methods, except for very low recall (smaller than 0.05). The improvement over the methods proposed by Hoffmann et al. (2011) and Surdeanu et al. (2012), which are currently the best published results on this dataset, can be as high as 5 points in precision for the same recall point. Moreover, our method achieves a higher recall (0.30) than these two methods (0.25). Performance per relation. The dataset introduced by Riedel et al. (2010) is highly unbalanced: for example, the most common relation, /location/location/contains, represents almost half of the positive relations, while some relations are mentioned less than ten times. We thus decided to also report precision/recall curves for the five most common relations of that dataset in Fig. </context>
<context position="28404" citStr="Surdeanu et al. (2012)" startWordPosition="4760" endWordPosition="4763">et al. (2013). Sentential extraction. We finally report precision/recall curves for the task of sentential extraction, in Fig. 5, using the manually labeled dataset of Hoffmann et al. (2011). We observe that for most values of recall, our method achieves similar precision that the one proposed by Hoffmann et al. (2011), while extending the highest recall from 0.52 to 0.68. Thanks to this higher recall, our method achieves a highest F1 score of 0.66, compared to 0.61 obtained by the method proposed by Hoffmann et al. (2011). Method Runtime Mintz et al. (2009) 7 min Hoffmann et al. (2011) 2 min Surdeanu et al. (2012) 3 hours This work 3 hours Table 1: Comparison of running times for the different methods compared in the experimental section. 8 Conclusion In this article, we introduced a new formulation for weakly supervised relation extraction. Our method is based on a constrained discriminative formulation of the multiple instance, multiple label learning problem. Using the squared loss, we obtained a convex relaxation of this formulation, allowing us to obtain an approximate solution to the initial integer quadratic program. Thus, our method is not sensitive to initialization. We demonstrated the compet</context>
</contexts>
<marker>Surdeanu, Tibshirani, Nallapati, Manning, 2012</marker>
<rawString>Mihai Surdeanu, Julie Tibshirani, Ramesh Nallapati, and Christopher D Manning. 2012. Multi-instance multi-label learning for relation extraction. In Proceedings of EMNLP-CoNLL.</rawString>
</citation>
<citation valid="true">
<authors>
<author>Fei Wu</author>
<author>Daniel S Weld</author>
</authors>
<title>Autonomously semantifying wikipedia.</title>
<date>2007</date>
<booktitle>In Proceedings of the sixteenth ACM conference on Conference on information and knowledge management.</booktitle>
<contexts>
<context position="7156" citStr="Wu and Weld (2007)" startWordPosition="1110" endWordPosition="1113">e access to partial, ambiguous and noisy labeling. Craven and Kumlien (1999) were the first to propose a weakly supervised relation extractor. They aligned a knowledge database (the Yeast Protein Database) with scientific articles mentioning a particular relation, and then used the extracted sentences to learn a classifier for extracting that relation. Later, many different sources of weak labelings have been considered. Bellare and McCallum (2007) proposed a method to extract bibliographic relations based on conditional random fields and used a database of BibTex entries as weak supervision. Wu and Weld (2007) described a method to learn relations based on Wikipedia infoboxes. Knowledge databases, such as Freebase1 (Mintz et al., 2009; Sun et al., 2011) and YAGO2 (Nguyen and Moschitti, 2011) were also considered as a source of weak supervision. Multiple instance learning. The methods we previously mentionned transform the weakly supervised problem into a fully supervised one, leading to noisy training datasets (see Fig. 1). Mul1www.freebase.com 2www.mpi-inf.mpg.de/yago-naga/yago 1581 tiple instance learning (Dietterich et al., 1997) is a paradigm in which the learner receives bags of examples inste</context>
</contexts>
<marker>Wu, Weld, 2007</marker>
<rawString>Fei Wu and Daniel S Weld. 2007. Autonomously semantifying wikipedia. In Proceedings of the sixteenth ACM conference on Conference on information and knowledge management.</rawString>
</citation>
<citation valid="true">
<authors>
<author>Linli Xu</author>
<author>James Neufeld</author>
<author>Bryce Larson</author>
<author>Dale Schuurmans</author>
</authors>
<title>Maximum margin clustering.</title>
<date>2004</date>
<booktitle>In Adv. NIPS.</booktitle>
<contexts>
<context position="8798" citStr="Xu et al. (2004)" startWordPosition="1364" endWordPosition="1367">relation extraction (see Fig. 1). Thus, Hoffmann et al. (2011) proposed a multiple instance, multiple label method, based on an undirected graphical model, to solve the problem of weakly supervised relation extraction. Finally, Surdeanu et al. (2012) also proposed a graphical model to solve this problem. One of their main contributions is to capture dependencies between relation labels, such as the fact that two labels cannot be generated jointly (e.g. the relations SpouseOf and BornIn). Discriminative clustering. Our approach is based on the discriminative clustering framework, introduced by Xu et al. (2004). The goal of discriminative clustering is to find a labeling of the data points leading to a classifier with low classification error. Different formulations of discriminative clustering have been proposed, based on support vector machines (Xu et al., 2004), the squared loss (Bach and Harchaoui, 2007) or the logistic loss (Joulin et al., 2010). A big advantage of discriminative clustering is that weak supervision or prior information can easily be incorporated. Our work is closely related to the method proposed by Bojanowski et al. (2013) for learning the names of characters in movies. 3 Weak</context>
<context position="13556" citStr="Xu et al. (2004)" startWordPosition="2188" endWordPosition="2191">e that we have K relations, indexed by the integers {1, ..., K}. Let R ∈ R,×K be a matrix such that Rik = 1 if the pair of entities i verifies the relation k, and Rik = 0 otherwise. The matrix R thus represents the knowledge database. See Fig. 2 for an illustration of these notations. 4.2 Problem formulation Our goal is to infer a binary matrix Y ∈ {0, 1}N×(K+1), such that Ynk = 1 if the relation mention candidate n express the relation k and Ynk = 0 otherwise (and thus, the integer K + 1 represents the relation None). We take an approach inspired by the discriminative clustering framework of Xu et al. (2004). We are thus looking for a (K + 1)-class indicator matrix Y, such that the classification error of an optimal multiclass classifier f is minimum. Given a multiclass loss function ` and a regularizer Q, this problem can be formulated as: `(yn, f(xn)) + Q(f), s.t. Y ∈ Y where yn is the nth line of Y. The constraints Y ∈ Y are added in order to take into account the information from the weak supervision. We will describe in the next section what kind of constraints are considered. 4.3 Weak supervision by constraining Y In this section, we show how the information from the knowledge base can be e</context>
</contexts>
<marker>Xu, Neufeld, Larson, Schuurmans, 2004</marker>
<rawString>Linli Xu, James Neufeld, Bryce Larson, and Dale Schuurmans. 2004. Maximum margin clustering. In Adv. NIPS.</rawString>
</citation>
<citation valid="true">
<authors>
<author>Dmitry Zelenko</author>
<author>Chinatsu Aone</author>
<author>Anthony Richardella</author>
</authors>
<title>Kernel methods for relation extraction.</title>
<date>2003</date>
<journal>The Journal of Machine Learning Research,</journal>
<volume>3</volume>
<contexts>
<context position="2303" citStr="Zelenko et al., 2003" startWordPosition="348" endWordPosition="351">information extraction relies on supervised learning, yielding high Knowledge base r e1 e2 BornIn Lichtenstein New York City DiedIn Lichtenstein New York City Sentences Latent labels Roy Lichtenstein was born in New York City, into an upper- BornIn middle-class family. In 1961, Leo Castelli started displaying Lichtenstein’s work None at his gallery in New York. Lichtenstein died ofpneumonia DiedIn in 1997 in New York City. Figure 1: An example of a knowledge database comprising two facts and training sentences obtained by aligning this database to unlabeled text. precision and recall results (Zelenko et al., 2003). Unfortunately, these approaches need large amount of labeled data, and thus do not scale well to the great number of different types of fact found on the Web or in scientific articles. A promising approach, called distant or weak supervision, is to exploit an existing database of facts as training data, by aligning it to an unlabeled collection of text documents (Craven and Kumlien, 1999). In this article, we are interested in weakly supervised extraction of binary relations. A challenge pertaining to weak supervision is that the obtained training data is noisy and ambiguous (Riedel et al., </context>
<context position="5823" citStr="Zelenko et al. (2003)" startWordPosition="902" endWordPosition="905">rvised method for relation extraction was inspired by syntactic parsing: the system described by Miller et al. (1998) combines syntactic and semantic knowledge, and thus, part-of-speech tagging, parsing, named entity recognition and relation extraction all happen at the same time. The problem of relation extraction was later formulated as a classification problem: Kambhatla (2004) proposed to solve this problem using maximum entropy models using lexical, syntactic and semantic features. Kernel methods for relation extraction, based on shallow parse trees or dependency trees were introduced by Zelenko et al. (2003), Culotta and Sorensen (2004) and Bunescu and Mooney (2005). Unsupervised learning. The open information extraction paradigm, simultaneously proposed by Shinyama and Sekine (2006) and Banko et al. (2007), does not rely on any labeled data or even existing relations. Instead, open information extraction systems only use an unlabeled corpus, and output a set of extracted relations. Such systems are based on clustering (Shinyama and Sekine, 2006) or self-supervision (Banko et al., 2007). One of the limitations of these systems is the fact that they extract uncanonicalized relations. Weakly superv</context>
</contexts>
<marker>Zelenko, Aone, Richardella, 2003</marker>
<rawString>Dmitry Zelenko, Chinatsu Aone, and Anthony Richardella. 2003. Kernel methods for relation extraction. The Journal of Machine Learning Research, 3.</rawString>
</citation>
</citationList>
</algorithm>
</algorithms>
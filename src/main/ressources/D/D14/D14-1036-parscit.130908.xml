<?xml version="1.0" encoding="UTF-8"?>
<algorithms version="110505">
<algorithm name="SectLabel" version="110505">
<variant no="0" confidence="0.002793">
<title confidence="0.977744">
Incremental Semantic Role Labeling with Tree Adjoining Grammar
</title>
<author confidence="0.978794">
Ioannis Konstas*, Frank Keller*, Vera Demberg† and Mirella Lapata*
</author>
<affiliation confidence="0.985188">
*: Institute for Language, Cognition and Computation
School of Informatics, University of Edinburgh
</affiliation>
<email confidence="0.968574">
{ikonstas,keller,mlap}@inf.ed.ac.uk
</email>
<affiliation confidence="0.578431">
†: Cluster of Excellence Multimodal Computing and Interaction,
Saarland University
</affiliation>
<email confidence="0.976069">
vera@coli.uni-saarland.de
</email>
<sectionHeader confidence="0.993259" genericHeader="abstract">
Abstract
</sectionHeader>
<bodyText confidence="0.9998589">
We introduce the task of incremental se-
mantic role labeling (iSRL), in which se-
mantic roles are assigned to incomplete
input (sentence prefixes). iSRL is the
semantic equivalent of incremental pars-
ing, and is useful for language model-
ing, sentence completion, machine trans-
lation, and psycholinguistic modeling. We
propose an iSRL system that combines
an incremental TAG parser with a seman-
tically enriched lexicon, a role propaga-
tion algorithm, and a cascade of classi-
fiers. Our approach achieves an SRL F-
score of 78.38% on the standard CoNLL
2009 dataset. It substantially outper-
forms a strong baseline that combines
gold-standard syntactic dependencies with
heuristic role assignment, as well as a
baseline based on Nivre’s incremental de-
pendency parser.
</bodyText>
<sectionHeader confidence="0.998991" genericHeader="keywords">
1 Introduction
</sectionHeader>
<bodyText confidence="0.9994012">
Humans are able to assign semantic roles such as
agent, patient, and theme to an incoming sentence
before it is complete, i.e., they incrementally build
up a partial semantic representation of a sentence
prefix. As an example, consider:
</bodyText>
<sectionHeader confidence="0.423979" genericHeader="introduction">
(1) The
</sectionHeader>
<bodyText confidence="0.99216912962963">
11 athlete realized [her
goalS]PATIENT/THEME were out of reach.
When reaching the noun phrase her goals, the hu-
man language processor is faced with a semantic
role ambiguity: her goals can either be the PA-
TIENT of the verb realize, or it can be the THEME
of a subsequent verb that has not been encoun-
tered yet. Experimental evidence shows that the
human language processor initially prefers the PA-
TIENT role, but switches its preference to the
theme role when it reaches the subordinate verb
were. Such semantic garden paths occur because
human language processing occurs word-by-word,
and are well attested in the psycholinguistic litera-
ture (e.g., Pickering et al., 2000).
Computational systems for performing seman-
tic role labeling (SRL), on the other hand, proceed
non-incrementally. They require the whole sen-
tence (typically together with its complete syntac-
tic structure) as input and assign all semantic roles
at once. The reason for this is that most features
used by current SRL systems are defined globally,
and cannot be computed on sentence prefixes.
In this paper, we propose incremental SRL
(iSRL) as a new computational task that mimics
human semantic role assignment. The aim of an
iSRL system is to determine semantic roles while
the input unfolds: given a sentence prefix and its
partial syntactic structure (typically generated by
an incremental parser), we need to (a) identify
which words in the input participate in the seman-
tic roles as arguments and predicates (the task of
role identification), and (b) assign correct seman-
tic labels to these predicate/argument pairs (the
task of role labeling). Performing these two tasks
incrementally is substantially harder than doing it
non-incrementally, as the processor needs to com-
mit to a role assignment on the basis of incom-
plete syntactic and semantic information. As an
example, take (1): on reaching athlete, the proces-
sor should assign this word the AGENT role, even
though it has not seen the corresponding predicate
yet. Similarly, upon reaching realized, the pro-
cessor can complete the AGENT role, but it should
also predict that this verb also has a PATIENT role,
even though it has not yet encountered the argu-
ment that fills this role. A system that performs
SRL in a fully incremental fashion therefore needs
to be able to assign incomplete semantic roles,
unlike existing full-sentence SRL models.
The uses of incremental SRL mirror the applica-
tions of incremental parsing: iSRL models can be
used in language modeling to assign better string
probabilities, in sentence completion systems to
</bodyText>
<page confidence="0.981362">
301
</page>
<note confidence="0.9106145">
Proceedings of the 2014 Conference on Empirical Methods in Natural Language Processing (EMNLP), pages 301–312,
October 25-29, 2014, Doha, Qatar. c�2014 Association for Computational Linguistics
</note>
<bodyText confidence="0.999942">
provide semantically informed completions, in
any real time application systems, such as dia-
log processing, and to incrementalize applications
such as machine translation (e.g., in speech-to-
speech MT). Crucially, any comprehensive model
of human language understanding needs to com-
bine an incremental parser with an incremental se-
mantic processor (Pad´o et al., 2009; Keller, 2010).
The present work takes inspiration from the
psycholinguistic modeling literature by proposing
an iSRL system that is built on top of a cogni-
tively motivated incremental parser, viz., the Psy-
cholinguistically Motivated Tree Adjoining Gram-
mar parser of Demberg et al. (2013). This parser
includes a predictive component, i.e., it predicts
syntactic structure for upcoming input during in-
cremental processing. This makes PLTAG par-
ticularly suitable for iSRL, allowing it to predict
incomplete semantic roles as the input string un-
folds. Competing approaches, such as iSRL based
on an incremental dependency parser, do not share
this advantage, as we will discuss in Section 4.3.
</bodyText>
<sectionHeader confidence="0.999748" genericHeader="related work">
2 Related Work
</sectionHeader>
<bodyText confidence="0.999984341463415">
Most SRL systems to date conceptualize seman-
tic role labeling as a supervised learning prob-
lem and rely on role-annotated data for model
training. Existing models often implement a
two-stage architecture in which role identification
and role labeling are performed in sequence. Su-
pervised methods deliver reasonably good perfor-
mance with F-scores in the low eighties on stan-
dard test collections for English (M`arquez et al.,
2008; Bj¨orkelund et al., 2009).
Current approaches rely primarily on syntactic
features (such as path features) in order to iden-
tify and label roles. This has been a mixed bless-
ing as the path from an argument to the predi-
cate can be very informative but is often quite
complicated, and depends on the syntactic formal-
ism used. Many paths through the parse tree are
likely to occur infrequently (or not at all), result-
ing in very sparse information for the classifier to
learn from. Moreover, as we will discuss in Sec-
tion 4.4, such path information is not always avail-
able when the input is processed incrementally.
There is previous SRL work employing Tree Ad-
joining Grammar, albeit in a non-incremental set-
ting, as a means to reduce the sparsity of syntax-
based features. Liu and Sarkar (2007) extract a
rich feature set from TAG derivations and demon-
strate that this improves SRL performance.
In contrast to incremental parsing, incremental
semantic role labeling is a novel task. Our model
builds on an incremental Tree Adjoining Gram-
mar parser (Demberg et al., 2013) which predicts
the syntactic structure of upcoming input. This al-
lows us to perform incremental parsing and incre-
mental SRL in tandem, exploiting the predictive
component of the parser to assign (potentially in-
complete) semantic roles on a word-by-word ba-
sis. Similar to work on incremental parsing that
evaluates incomplete trees (Sangati and Keller,
2013), we evaluate the incomplete semantic struc-
tures produced by our model.
</bodyText>
<sectionHeader confidence="0.986265" genericHeader="method">
3 Psycholinguistically Motivated TAG
</sectionHeader>
<bodyText confidence="0.99997682051282">
Demberg et al. (2013) introduce Psycholin-
guistically Motivated Tree Adjoining Grammar
(PLTAG), a grammar formalism that extends stan-
dard TAG (Joshi and Schabes, 1992) in order to
enable incremental parsing. Standard TAG as-
sumes a lexicon of elementary trees, each of
which contains at least one lexical item as an an-
chor and at most one leaf node as a foot node,
marked with A∗. All other leaves are marked with
A↓ and are called substitution nodes. Elementary
trees that contain a foot node are called auxiliary
trees; those that do not are called initial trees. Ex-
amples for TAG elementary trees are given in Fig-
ure 1a–c.
To derive a TAG parse for a sentence, we start
with the elementary tree of the head of the sen-
tence and integrate the elementary trees of the
other lexical items of the sentence using two oper-
ations: adjunction at an internal node and substi-
tution at a substitution node (the node at which the
operation applies is the integration point). Stan-
dard TAG derivations are not guaranteed to be in-
cremental, as adjunction can happen anywhere in
a sentence, possibly violating left-to-right process-
ing order. PLTAG addresses this limitation by in-
troducing prediction trees, elementary trees with-
out a lexical anchor. These can be used to predict
syntactic structure anchored by words that appear
later in an incremental derivation. The use of pre-
diction trees ensures that fully connected prefix
trees can be built for every prefix of the input sen-
tence.
Each node in a prediction tree carries mark-
ers to indicate that this node was predicted, rather
than being anchored by the current sentence pre-
fix. An example is Figure 1d, which contains a
prediction tree with marker “1”. In PLTAG, mark-
ers are eliminated through a new operation called
verification, which matches them with the nodes
</bodyText>
<page confidence="0.995183">
302
</page>
<figure confidence="0.996136470588235">
Banks refused to open today
nsbj aux
(A0,Banks,refused)
(A1,to,refused)
(A1,Banks,open)
(AM-TMP,today,open)
A1
A1 AM-TMP
A0
(a) NP (b) S (c) VP (d) S1
NNS
Banks
NP1 VP AP VP* NP11 VP11
VB RB
tmod
rarely
open xcomp
</figure>
<figureCaption confidence="0.996865">
Figure 1: PLTAG lexicon entries: (a) and (b) ini-
tial trees, (c) auxiliary tree, (d) prediction tree.
</figureCaption>
<figure confidence="0.996598">
(a) valid (b) invalid
</figure>
<figureCaption confidence="0.872976666666667">
Figure 3: The current fringe (dashed line) indi-
cates where valid substitutions can occur. Other
substitutions result in an invalid prefix tree.
</figureCaption>
<bodyText confidence="0.999784787878788">
of non-predictive elementary trees. An example
of a PLTAG derivation is given in Figure 2. In
step 1, a prediction tree is introduced through sub-
stitution, which then allows the adjunction of an
adverb in step 2. Step 3 involves the verification
of the marker introduced by the prediction tree
against the elementary tree for open.
In order to efficiently parse PLTAG, Demberg
et al. (2013) introduce the concept of fringes.
Fringes capture the fact that in an incremental
derivation, a prefix tree can only be combined with
an elementary tree at a limited set of nodes. For
instance, the prefix tree in Figure 3 has two substi-
tution nodes, for B and C. However, only substi-
tution into B leads to a valid new prefix tree; if we
substitute into C, we obtain the tree in Figure 3b,
which is not a valid prefix tree (i.e., it represents a
non-incremental derivation).
The parsing algorithm proposed by Demberg
et al. (2013) exploits fringes to tabulate interme-
diate results. It manipulates a chart in which each
cell (i, f) contains all the prefix trees whose first
i leaves are the first i words and whose current
fringe is f. To extend the prefix trees for i to
the prefix trees for i + 1, the algorithm retrieves
all current fringes f such that the chart has entries
in the cell (i, f ). For each such fringe, it needs
to determine the elementary trees in the lexicon
that can be combined with f using substitution or
adjunction. In spite of the large size of a typi-
cal TAG lexicon, this can be done efficiently, as
it only requires matching the current fringes. For
each match, the parser then computes the new pre-
</bodyText>
<figureCaption confidence="0.947516666666667">
Figure 4: Syntactic dependency graph with se-
mantic role annotation and the accompanying se-
mantic triples, for Banks refused to open today.
</figureCaption>
<bodyText confidence="0.999690666666667">
fix trees and its new current fringe f&apos; and enters it
into cell (i+ 1, f&apos;).
Demberg et al. (2013) convert the Penn Tree-
bank (Marcus et al., 1993) into TAG for-
mat by enriching it with head information and
argument/modifier information from Propbank
(Palmer et al., 2005). This makes it possible
to decompose the Treebank trees into elementary
trees as proposed by Xia et al. (2000). Predic-
tion trees can be learned from the converted Tree-
bank by calculating the connection path (Mazzei
et al., 2007) at each word in a tree. Intuitively,
a prediction tree for word wn contains the struc-
ture that is necessary to connect wn to the prefix
tree w1 ...wn_1, but is not part of any of the ele-
mentary trees of w1 ...wn_1. Using this lexicon, a
probabilistic model over PLTAG operations can be
estimated following Chiang (2000).
</bodyText>
<sectionHeader confidence="0.998833" genericHeader="method">
4 Model
</sectionHeader>
<subsectionHeader confidence="0.999432">
4.1 Problem Formulation
</subsectionHeader>
<bodyText confidence="0.9999476">
In a typical semantic role labeling scenario, the
goal is to first identify words that are predicates
in the sentence and then identify and label all the
arguments for each predicate. This translates into
spotting specific words in a sentence that repre-
sent the predicate’s arguments, and assigning pre-
defined semantic role labels to them. Note that in
this work we focus on verb predicates only. The
output of a semantic role labeler is a set of seman-
tic dependency triples (l,a, p), with l E R,, and
a, p E w, where R, is a set of semantic role labels
denoting a specific relationship between a predi-
cate and an argument (e.g., ARG0, ARG1, ARGM
in Propbank), w is the list of words in the sentence,
l denotes a specific role label, a the argument, and
p the predicate. An example is shown in Figure 4.
As discussed in the introduction, standard se-
mantic role labelers make their decisions based on
evidence from the whole sentence. In contrast, our
aim is to assign semantic roles incrementally, i.e.,
</bodyText>
<figure confidence="0.98029016">
a
S
By Cy a
a
S
By C
c
S
B Cy
b
303
S1
VP1
AP VP
Banks RB
r
NP
NNS
Banks
1
NP
NNS
S1
VPi
a re
</figure>
<page confidence="0.995997">
304
</page>
<bodyText confidence="0.966418083333333">
Banks refused to open. Naturally, some nodes in
the lexicon trees might have multiple candidate
role labels. For example, the substitution NP node
of the second tree takes two labels, namely A0
and A1. These stem from different role signatures
when the same elementary tree occurs in differ-
ent contexts during training (A1 only on the NP;
A0 on the NP and A1 on S). For simplicity’s sake,
we collapse different signatures, and let a classi-
fier labeller to disambiguate such cases (see Sec-
tion 4.4).
Algorithm 1 Incremental Role Propagation Alg.
</bodyText>
<listItem confidence="0.99552402173913">
1: procedure IRPA(πip, T , Tpr)
2: Σ ← ∅ . Σ is a dictionary of (πip, (l,a, p)) pairs
3: if parser operation is substitution or adjunction then
4: CREATE-TRIPLES(πip, T)
5: else if parser operation is verification then
6: CREATE-TRIPLES-VERIF(πip, T , Tpr)
return set of triples (l,a, p) for prefix tree π
7: procedure CREATE-TRIPLES(πip, T)
8: if HAS-ROLES(πip) then
9: UPDATE-TRIPLE(πip, T)
10: else if HAS-ROLES(T) then
11: Tip← substitution or foot node of T
12: ADD-TRIPLE(πip, Tip, T )
13: for all remaining nodes n E T with roles do
14: ADD-TRIPLE(πip, n, T ) . incomplete triples
15: procedure CREATE-TRIPLES-VERIF(πip, Tv, Tpr)
16: if HAS-ROLES(Tv) then
17: anchor ← lexeme of Tv
18: for all Tip ← node in Tv with role do
19: Tpr,ip← matching node of Tip in Tpr
20: CREATE-TRIPLES(Tpr,ip, Tv)
. Process the rest of covered nodes in Tpr with roles
21: for all remaining Tpr,ip← node in Tpr with role do
22: UPDATE-TRIPLE(Tpr,ip, Tpr)
23: function UPDATE-TRIPLE(πip, T)
24: dep ← FIND-INCOMPLETE(Σ, Tip)
25: anchor ← lexeme of T
26: if anchor of T is predicate then
27: SET-PREDICATE(dep, anchor)
28: else if anchor of T is argument then
29: SET-ARGUMENT(dep, anchor)
return dep
30: procedure ADD-TRIPLE(πip, Tip, T )
31: dep ← ([roles of Tip], nil, nil)
32: anchor ← lexeme of T
33: if anchor of T is predicate then
34: SET-PREDICATE(dep, anchor)
35: SET-ARGUMENT(dep, head of πip)
36: else if anchor of T is argument then
37: if Tis auxiliary then . adjunction
38: SET-ARGUMENT(dep, anchor)
39: else . substitution: arg is head of prefix tree
40: SET-ARGUMENT(dep, head of Tip)
41: pred ← find dep E Σ with matching πip
42: SET-PREDICATE(dep, pred)
43: Σ ← (πip, dep)
</listItem>
<bodyText confidence="0.983765655172414">
Once we process Banks, the prefix tree becomes
the lexical entry for this word, see the first col-
umn of Figure 5b. Next, we process refused:
the parser substitutes the prefix tree into the ele-
mentary tree T of refused;3 the integration point
πip on the prefix tree is the topmost NP. Since
the operation is a substitution (line 3), we create
triples between T and πip via CREATE-TRIPLES
(lines 7–12). πip does not have any role infor-
mation (line 8), so we proceed to add a new se-
mantic triple between the role-labeled integration
point Tip, i.e., substitution NP node of T , and πip,
via ADD-TRIPLE (lines 30–43). First, we create
an incomplete semantic triple with all roles from
Tip (line 31). Then we set the predicate to the an-
chor of T to be the word refused, and the argu-
ment to be the head word of the prefix tree, Banks
(lines 34–35). Note that predicate identification is
a trivial task based on part-of-speech information
in the elementary tree.4
Then, we add the pair (NP → ({A0,A1},Banks,
refused)) to a dictionary (line 43). Storing the in-
tegration point along with the semantic triple is
essential, to be able to recover incomplete triples
in later stages of the algorithm. Finally, we re-
peat this process for all remaining nodes on T that
have roles, in our example the substitution node S
(lines 13–14). This outputs an incomplete triple,
({A1},nil,refused).
Next, the parser decides to substitute a predic-
tion tree (third tree in Figure 5a) into the substitu-
tion node S of the prefix tree. Since the integration
point is on the prefix tree and has role information
(line 8), the corresponding triple should already be
present in our dictionary. Upon retrieving it, we
set the nil argument to the anchor of the incoming
tree. Since it is a prediction tree, we set it to the
root of the tree, namely S2 (phrase labels in triples
are denoted by italics), but mark the triple as yet
incomplete. This distinction allows us to fill in the
correct lexical information once it becomes avail-
able, i.e, when the tree gets verified. We also add
an incomplete triple for the trace t in the subject
position of the prediction tree, as described above.
Note that this triple contains multiple roles; this is
expected given that prediction trees are unlexical-
ized and occur in a wide variety of contexts.
When the next verb arrives, the parser success-
fully verifies it against the embedded prediction
3PLTAG parsing operations can occur in two ways: An
elementary tree can be substituted into the substitution node
of the prefix tree, or the prefix tree can be substituted into a
node of an elementary tree. The same holds for adjunction.
4Most predicates can be identified as anchors of non-
modifier auxiliary trees. However, there are exceptions to
this rule, i.e., modifier auxiliary trees and non-modifier non-
auxiliary trees being also verbs in our lexicon, hence the use
of the more reliable POS tags.
</bodyText>
<page confidence="0.989353">
305
</page>
<table confidence="0.999605">
IRPA MaltParser
Banks – –
refused ({A0,A1},Banks,refused), (A0,Banks,refused)
(A1,S2,refused), –
({A0,A1,A2},t,nil)
to –
open (A1,to,refused), (A1,to,refused),
(A1,Banks,open) (A0,Banks,open)
today (AM-TMP,today,open) (AM-TMP,today,open)
</table>
<tableCaption confidence="0.999147">
Table 1: Complete and incomplete semantic triple
</tableCaption>
<bodyText confidence="0.983789428571429">
generation, comparing IRPA and a system that
maps gold-standard role labels onto MaltParser in-
cremental dependencies for Figure 4.
tree within the prefix tree (last step of Figure 5b).
Our algorithm first cycles through all nodes that
match between the verification tree 7v and the pre-
diction tree 7pr and will complete or create new
triples via CREATE-TRIPLES (lines 18–20). In
our example, the second semantic triple gets com-
pleted by replacing S2 with the head of the sub-
tree rooted in S. Normally, this would be the verb
open, but in this case the verb is followed by the
infinitive marker to, hence we heuristically set it
to be the argument of the triple instead, following
Carreras and M`arquez (2005). For the last triple,
we set the predicate to the anchor of 7v open, and
now are able to remove the excess role labels A0
and A2. This illustrated how the lexicalized veri-
fication tree disambiguates the semantic informa-
tion stored in the prediction tree. Finally, trace t is
set to the closest NP head that is below the same
phrase subtree, in this case Banks. Note that Banks
is part of two triples as shown in the last tree of
Figure 5b: it is either an A0 or an A1 for refused
and an A1 for open.
We are able to create incomplete semantic
triples after the prediction of the upcoming verb at
step 2, as shown in Figure 5b. This is not possible
using an incremental dependency parser such as
MaltParser (Nivre et al., 2007) that lacks a predic-
tive component. Table 1 illustrates this by compar-
ing the output of IRPA for Figure 5b with the out-
put of a baseline system that maps role labels onto
the syntactic dependencies in Figure 4, generated
incrementally by MaltParser (see Section 5.3 for
a description of the MaltParser baseline). Malt-
Parser has to wait for the verb open before out-
putting the relevant semantic triples. In contrast,
IRPA outputs incomplete triples as soon as the in-
formation is available, and later on updates its de-
cision. (MaltParser also incorrectly assigns A0 for
the Banks–open pair.)
</bodyText>
<subsectionHeader confidence="0.9159065">
4.4 Argument Identification and Role Label
Disambiguation
</subsectionHeader>
<bodyText confidence="0.999983254901961">
IRPA produces semantic triples for every role an-
notation present in the lexicon entries, which will
often overgenerate role information. Furthermore,
some triples have more than one role label at-
tached to them. During verification, we are able to
filter out the majority of labels in the correspond-
ing prediction trees; However, most triples are cre-
ated via substitution and adjunction.
In order to address these problems we adhere to
the following classification and ranking strategy:
after each semantic triple gets completed, we per-
form a binary classification that evaluates its suit-
ability as a whole, given bilexical and syntactic in-
formation. If the triple is identified as a good can-
didate, then we perform multi-class classification
over role labels: we feed the same bilexical and
syntactic information to a logistic classifier, and
get a ranked list of labels. We then use this list to
re-rank the existing ambiguous role labels in the
semantic triple, and output the top scoring ones.
The identifier is a binary L2-loss support vec-
tor classifier, and the role disambiguator an L2-
regularized logistic regression classifier, both im-
plemented using the efficient LIBLINEAR frame-
work of Fan et al. (2008). The features used are
based on Bj¨orkelund et al. (2009) and Liu and
Sarkar (2007), and are listed in Table 2.
The bilexical features are: predicate POS tag,
predicate lemma, argument word form, argument
POS tag, and position. The latter indicates the po-
sition of the argument relative to the predicate, i.e.,
before, on, or after. The syntactic features are:
the predicate and argument elementary trees with-
out the anchors (to avoid sparsity), the category of
the integration point node on the prefix tree where
the elementary tree of the argument attaches to,
an alphabetically ordered set of the categories of
the fringe nodes of the prefix tree after attaching
the argument tree, and the path of PLTAG opera-
tions applied between the argument and the pred-
icate. Note that most of the original features used
by Bj¨orkelund et al. (2009) and others are not ap-
plicable in our context, as they exploit information
that is not accessible incrementally. For example,
sibling information to the right of the word is not
available. Furthermore, our PLTAG parser does
not compute syntactic dependencies, hence these
cannot serve as features (and in any case not all
dependencies are available incrementally, see Fig-
ure 4). To counterbalance this, we use local syn-
tactic information stored in the fringe of the pre-
</bodyText>
<page confidence="0.994717">
306
</page>
<figure confidence="0.997856108108108">
S
NP
NNS
S
VP
NP
VP
S
VP22
VB22
VP
Banks
VB
t
{A1}
VP
TO VP∗
to
NP↓
{A0,A1}
S2
NPi
t1
1
{A0,A1,A2}
VBD open
refused
(a) Lexicon entries
NP
NNS
S
S
NP
VP
VP
↓
{A1}
</figure>
<equation confidence="0.721572454545455">
2 {A 1 }
Banks NNS VP S S
Banks VBD VP
{A0,A1}
refused
↓
{A1}
2
2
p
VB2
</equation>
<page confidence="0.992404">
307
</page>
<bodyText confidence="0.999959558823529">
Nugues (2007), accompanied by semantic role la-
bel annotation for every argument pair. The latter
is converted from Propbank based on Carreras and
M`arquez (2005). We extracted the bilexical fea-
tures for the classifiers directly from the gold stan-
dard annotation of the training set. The syntactic
features were obtained as follows: for every sen-
tence in the training set we applied IRPA using the
trained PLTAG parser, with gold standard lexicon
entries for each word of the input sentence. This
ensures near perfect parsing accuracy. Then for
each semantic triple predicted incrementally, we
extracted the relevant syntactic information in or-
der to construct training vectors. If the identified
predicate-argument pair was in the gold standard
then we assigned a positive label for the identifi-
cation classifier, otherwise we flagged it as nega-
tive. For those pairs that are not identified by IRPA
but exist in the gold standard (false negatives), we
extracted syntactic information from already iden-
tified similar triples, as follows: We first look for
correctly identified arguments, wrongly attached
to a different predicate and re-create the triple with
correct predicate/argument information. If no ar-
gument is found, we then pick the argument in the
list of identified arguments for a correct predicate
with the same POS-tag as the gold-standard argu-
ment. In the case of the role label disambigua-
tion classifier we just assign the gold label for ev-
ery correctly identified pair, and ignore the (possi-
bly ambiguous) predicted one. After tuning on the
development set, the argument identifier achieved
an accuracy of 92.18%, and the role label disam-
biguation classifier, 82.37%.
</bodyText>
<subsectionHeader confidence="0.989358">
5.2 Evaluation
</subsectionHeader>
<bodyText confidence="0.999857846153846">
The focus of this paper is to build a system that is
able to output semantic role labels for predicate-
argument pairs incrementally, as soon as they be-
come available. In order to properly evaluate such
a system, we need to measure its performance in-
crementally. We propose two different cumulative
scores for assessing the (possibly incomplete) se-
mantic triples that have been created so far, as the
input is processed from left to right, per word. The
first metric is called Unlabeled Prediction Score
(UPS) and gets updated for every identified argu-
ment or predicate, even if the corresponding se-
mantic triple is incomplete. Note that UPS does
not take into account the role label, it only mea-
sures predicate and argument identification. In this
respect it is analogous to unlabeled dependency
accuracy reported in the parsing literature. We ex-
pect a model that is able to predict semantic roles
to achieve an improved UPS result compared to a
system that does not do prediction, as illustrated in
Table 1. Our second score, Combined Incremental
SRL Score (CISS), measures the identification of
complete semantic role triples (i.e., correct predi-
cate, predicate sense, argument, and role label) per
word; by the end of the sentence, CISS coincides
with standard combined SRL accuracy, as reported
in CoNLL 2009 SRL-only task. This score is anal-
ogous to labeled dependency accuracy in parsing.
Note that conventional SRL systems such as
Bj¨orkelund et al. (2009) typically assume gold
standard syntactic information. In order to emu-
late this, we give our parser gold standard lexicon
entries for each word in the test set; these contain
all possible roles observed in the training set for
a given elementary tree (and all possible senses
for each predicate). This way the parser achieves
a syntactic parsing F1 score of 94.24, thus ensur-
ing the errors of our system can be attributed to
IRPA and the classifiers. Also note that we evalu-
ate on verb predicates only, therefore trivially re-
ducing the task of predicate identification to the
simple heuristic of looking for words in the sen-
tence with a verb-related POS tag and excluding
auxiliaries and modals. Likewise, predicate sense
disambiguation on verbs presumably is trivial, as
we observed almost no ambiguity of senses among
lexicon entries of the same verb (we adhered to a
simple majority baseline, by picking the most fre-
quent sense, given the lexeme of the verb, in the
few ambiguous cases). It seems that the syntactic
information held in the elementary trees discrimi-
nates well among different senses.
</bodyText>
<subsectionHeader confidence="0.999304">
5.3 System Comparison
</subsectionHeader>
<bodyText confidence="0.999993875">
We evaluated three configurations of our system.
The first configuration (iSRL) uses all seman-
tic roles for each PLTAG lexicon entry, applies
the PLTAG parser, IRPA, and both classifiers to
perform identification and disambiguation, as de-
scribed in Section 4. The second one (Majority-
Baseline), solves the problem of argument identifi-
cation and role disambiguation without the classi-
fiers. For the former we employ a set of heuristics
according to Lang and Lapata (2014), that rely on
gold syntactic dependency information, sourced
from CoNLL input. For the latter, we choose the
most frequent role given the gold standard depen-
dency relation label for the particular argument.
Note that dependencies have been produced in
view of the whole sentence and not incrementally.
</bodyText>
<page confidence="0.995694">
308
</page>
<table confidence="0.9996528">
System Prec Rec F1
iSRL-Oracle 91.00 80.26 85.29
iSRL 81.48 75.51 78.38
Majority-Baseline 71.05 58.10 63.92
Malt-Baseline 60.90 46.14 52.50
</table>
<tableCaption confidence="0.999696">
Table 3: Full-sentence combined SRL score
</tableCaption>
<bodyText confidence="0.9998518">
This gives the baseline a considerable advantage
especially in case of longer range dependencies.
The third configuration (iSRL-Oracle), is identical
to iSRL, but uses the gold standard roles for each
PLTAG lexicon entry, and thus provides an upper-
bound for our methodology. Finally, we evalu-
ated against Malt-Baseline, a variant of Majority-
Baseline that uses the MaltParser of Nivre et al.
(2007) to provide labeled syntactic dependencies
MaltParser is a state-of-the-art shift-reduce depen-
dency parser which uses an incremental algorithm.
Following Beuck et al. (2011), we modified the
parser to provide intermediate output at each word
by emitting the current state of the dependency
graph before each shift step. We trained Malt-
Parser using the arc-eager algorithm (which out-
performed the other parsing algorithms available
with MaltParser) on the CoNLL dataset, achiev-
ing a labeled dependency accuracy of 89.66% on
section 23.
</bodyText>
<sectionHeader confidence="0.999903" genericHeader="evaluation">
6 Results
</sectionHeader>
<bodyText confidence="0.999930337837838">
Figures 6 and 7 show the results on the incremen-
tal SRL task. We plot the F1 for Unlabeled Predic-
tion Score (UPS) and Combined Incremental SRL
Score (CISS) per word, separately for sentences
of lengths 10, 20, 30, and 40 words. The task gets
harder with increasing sentence length, hence we
can only meaningfully compare the average scores
for sentence of the same length. (This approach
was proposed by Sangati and Keller 2013 for eval-
uating the performance of incremental parsers.)
The UPS results in Figure 6 clearly show that
our system (iSRL) outperforms both baselines
on unlabeled argument and predicate prediction,
across all four sentence lengths. Furthermore,
we note that the iSRL system achieves a near-
constant performance for all sentence prefixes.
Our PLTAG-based prediction/verification archi-
tecture allows us to correctly predict incomplete
semantic role triples, even at the beginning of the
sentence. Both baselines perform worse than the
iSRL system in general. Moreover, the Malt-
Baseline performs badly on the initial sentence
prefixes (up to word 10), presumably as it does
not benefit from syntactic prediction, and thus can-
not generate incomplete triples early in the sen-
tence, as illustrated in Table 1. The Majority-
Baseline also does not do prediction, but it has ac-
cess to gold-standard syntactic dependencies, and
thus outperforms the Malt-Baseline on initial sen-
tence prefixes. Note that due to prediction, our
system tends to over-generate incomplete triples
in the beginning of sentences, compared to non-
incremental output, which may inflate UPS for
the first words. However, this cancels out later
in the sentence if triples are correctly completed;
failure to do so would decrease UPS. The near-
constant performance of our output illustrates this
phenomenon. Finally, the iSRL-Oracle outper-
forms all other systems, as it benefits from correct
role labels and correct PLTAG syntax, thus provid-
ing an upper limit on performance.
The CISS results in Figure 7 present a simi-
lar picture. Again, the iSRL system outperforms
both baselines at all sentence lengths. In addition,
it shows particularly strong performance (almost
at the level of the iSRL-Oracle) at the beginning
of the sentence. This presumably is due to the
fact that our system uses prediction and is able to
identify correct semantic role triples earlier in the
sentence. The baselines also show higher perfor-
mance early in the sentence, but to a lesser degree.
Table 3 reports traditional combined SRL scores
for full sentences over all sentence lengths, as
defined for the CoNLL task. Our iSRL system
outperforms the Majority-Baseline by almost 15
points, and the Malt-Baseline by 25 points. It re-
mains seven points below the iSRL-Oracle upper
limit.
Finally, in order to test the effect of syntactic
parsing on our system, we also experimented with
a variant of our iSRL system that utilizes all lex-
icon entries for each word in the test set. This is
similar to performing the CoNLL 2009 joint task,
which is designed for systems that carry out both
syntactic parsing and semantic role labeling. This
variant achieved a full sentence F-score of 68.0%,
i.e., around 10 points lower than our iSRL system.
This drop in score correlates with the difference
in syntactic parsing F-score between the two ver-
sions of PLTAG parser (94.24 versus 79.41), and
is expected given the high ambiguity of the lex-
icon entries for each word. Note, however, that
the full-parsing version of our system still outper-
forms Malt-Baseline by 15 points.
</bodyText>
<page confidence="0.990806">
309
</page>
<figure confidence="0.99959853030303">
iSRL-Oracle iSRL
Majority-Baseline Malt-Baseline
iSRL-Oracle iSRL
Majority-Baseline Malt-Baseline
F1
0.8
0.6
0.4
0.2
1
2 4 6
8 10
F1
0.8
0.6
0.4
0.2
1
5 10 15 20
1
1
0.8
0.8
F1
0.6
F1
0.6
0.4
0.4
2 4 6
8 10
5 10 15 20
0.2
0.2
words words words words
(a) 10 words (b) 20 words (a) 10 words (b) 20 words
0.8
0.6
0.4
0.2
1
F1
0.8
0.6
0.4
0.2
1
5 10 15 20 25 30
F1
0.8
0.6
0.4
0.2
1
10 20 30 40
F1
0.8
0.6
0.4
0.2
1
5 10 15 20 25 30
F1
10 20 30 40
words words words words
(c) 30 words (d) 40 words (c) 30 words (d) 40 words
</figure>
<figureCaption confidence="0.999787">
Figure 6: Unlabeled Prediction Score (UPS) Figure 7: Combined iSRL Score (CISS)
</figureCaption>
<sectionHeader confidence="0.998554" genericHeader="conclusions">
7 Conclusions
</sectionHeader>
<bodyText confidence="0.999981277777778">
In this paper, we introduced the new task of incre-
mental semantic role labeling and proposed a sys-
tem that solves this task by combining an incre-
mental TAG parser with a semantically enriched
lexicon, a role propagation algorithm, and a cas-
cade of classifiers. This system achieved a full-
sentence SRL F-score of 78.38% on the standard
CoNLL dataset. Not only is the full-sentence
score considerably higher than the Majority-
Baseline (which is a strong baseline, as it uses
gold-standard syntactic dependencies), but we
also observe that our iSRL system performs well
incrementally, i.e., it predicts both complete and
incomplete semantic role triples correctly early on
in the sentence. We attributed this to the fact that
our TAG-based architecture makes it possible to
predict upcoming syntactic structure together with
the corresponding semantic roles.
</bodyText>
<sectionHeader confidence="0.998297" genericHeader="acknowledgments">
Acknowledgments
</sectionHeader>
<bodyText confidence="0.999434">
EPSRC support through grant EP/I032916/1 “An
integrated model of syntactic and semantic predic-
tion in human language processing” to FK and ML
is gratefully acknowledged.
</bodyText>
<sectionHeader confidence="0.998927" genericHeader="references">
References
</sectionHeader>
<reference confidence="0.999313032258064">
Beuck, Niels, Arne Khn, and Wolfgang Menzel.
2011. Incremental parsing and the evaluation
of partial dependency analyses. In Proceedings
of the 1st International Conference on Depen-
dency Linguistics. Depling 2011.
Bj¨orkelund, Anders, Love Hafdell, and Pierre
Nugues. 2009. Multilingual semantic role la-
beling. In Proceedings of the Thirteenth Con-
ference on Computational Natural Language
Learning: Shared Task. Association for Com-
putational Linguistics, Stroudsburg, PA, USA,
CoNLL ’09, pages 43–48.
Carreras, Xavier and Lluis M`arquez. 2005. Intro-
duction to the conll-2005 shared task: Semantic
role labeling. In Proceedings of the Ninth Con-
ference on Computational Natural Language
Learning. Association for Computational Lin-
guistics, Stroudsburg, PA, USA, CONLL ’05,
pages 152–164.
Chiang, David. 2000. Statistical parsing with
an automatically-extracted tree adjoining gram-
mar. In Proceedings of the 38th Annual Meeting
on Association for Computational Linguistics.
pages 456–463.
Demberg, Vera, Frank Keller, and Alexander
Koller. 2013. Incremental, predictive pars-
ing with psycholinguistically motivated tree-
adjoining grammar. Computational Linguistics
39(4):1025–1066.
Fan, Rong-En, Kai-Wei Chang, Cho-Jui Hsieh,
Xiang-Rui Wang, and Chih-Jen Lin. 2008. Li-
</reference>
<page confidence="0.979149">
310
</page>
<reference confidence="0.995739099009901">
blinear: A library for large linear classification.
Journal of Machine Learning Research 9:1871–
1874.
Hajiˇc, Jan, Massimiliano Ciaramita, Richard Jo-
hansson, Daisuke Kawahara, Maria Ant`onia
Marti, Lluis M`arquez, Adam Meyers, Joakim
Nivre, Sebastian Pad´o, Jan ˇStˇep´anek, Pavel
Straˇn´ak, Mihai Surdeanu, Nianwen Xue, and
Yi Zhang. 2009. The CoNLL-2009 shared task:
Syntactic and semantic dependencies in multi-
ple languages. In Proceedings of the 13th Con-
ference on Computational Natural Language
Learning (CoNLL-2009), June 4-5. Boulder,
Colorado, USA.
Johansson, Richard and Pierre Nugues. 2007. Ex-
tended constituent-to-dependency conversion
for english. In Joakim Nivre, Heiki-Jaan
Kalep, Kadri Muischnek, and Mare Koit, edi-
tors, NODALIDA 2007 Proceedings. University
of Tartu, pages 105–112.
Joshi, Aravind K. and Yves Schabes. 1992. Tree
adjoining grammars and lexicalized grammars.
In Maurice Nivat and Andreas Podelski, editors,
Tree Automata and Languages, North-Holland,
Amsterdam, pages 409–432.
Keller, Frank. 2010. Cognitively plausible models
of human language processing. In Proceedings
of the 48th Annual Meeting of the Association
for Computational Linguistics, Companion Vol-
ume: Short Papers. Uppsala, pages 60–67.
Lang, Joel and Mirella Lapata. 2014. Similarity-
driven semantic role induction via graph par-
titioning. Computational Linguistics Accepted
pages 1–62. To appear.
Liu, Yudong and Anoop Sarkar. 2007. Experimen-
tal evaluation of LTAG-based features for se-
mantic role labeling. In Proceedings of the 2007
Joint Conference on Empirical Methods in Nat-
ural Language Processing and Computational
Natural Language Learning (EMNLP-CoNLL).
Prague, Czech Republic, pages 590–599.
Marcus, Mitchell P., Mary Ann Marcinkiewicz,
and Beatrice Santorini. 1993. Building a large
annotated corpus of english: The penn treebank.
Computational Linguistics 19(2):313–330.
M`arquez, Lluis, Xavier Carreras, Kenneth C.
Litkowski, and Suzanne Stevenson. 2008. Se-
mantic Role Labeling: An Introduction to
the Special Issue. Computational Linguistics
34(2):145–159.
Mazzei, Alessandro, Vincenzo Lombardo, and
Patrick Sturt. 2007. Dynamic TAG and lexi-
cal dependencies. Research on Language and
Computation 5:309–332.
Nivre, Joakim, Johan Hall, Jens Nilsson, Atanas
Chanev, G¨ulsen Eryigit, Sandra K¨ubler, Sve-
toslav Marinov, and Erwin Marsi. 2007. Malt-
parser: A language-independent system for
data-driven dependency parsing. Natural Lan-
guage Engineering 13:95–135.
Pad´o, Ulrike, Matthew W. Crocker, and Frank
Keller. 2009. A probabilistic model of semantic
plausibility in sentence processing. Cognitive
Science 33(5):794–838.
Palmer, Martha, Daniel Gildea, and Paul Kings-
bury. 2005. The proposition bank: An anno-
tated corpus of semantic roles. Computational
Linguistics 31(1):71–106.
Pickering, Martin J., Matthew J. Traxler, and
Matthew W. Crocker. 2000. Ambiguity reso-
lution in sentence processing: Evidence against
frequency-based accounts. Journal of Memory
and Language 43(3):447–475.
Sangati, Federico and Frank Keller. 2013. In-
cremental tree substitution grammar for pars-
ing and word prediction. Transactions of
the Association for Computational Linguistics
1(May):111–124.
Sayeed, Asad and Vera Demberg. 2013. The se-
mantic augmentation of a psycholinguistically-
motivated syntactic formalism. In Proceed-
ings of the Fourth Annual Workshop on Cog-
nitive Modeling and Computational Linguistics
(CMCL). Association for Computational Lin-
guistics, Sofia, Bulgaria, pages 57–65.
Surdeanu, Mihai, Richard Johansson, Adam Mey-
ers, Lluis M`arquez, and Joakim Nivre. 2008.
The CoNLL-2008 shared task on joint pars-
ing of syntactic and semantic dependencies. In
Proceedings of the 12th Conference on Compu-
tational Natural Language Learning (CoNLL-
2008).
Witten, Ian H. and Timothy C. Bell. 1991. The
zero-frequency problem: estimating the proba-
bilities of novel events in adaptive text compres-
sion. Information Theory, IEEE Transactions
on 37(4):1085–1094.
Xia, Fei, Martha Palmer, and Aravind Joshi. 2000.
A uniform method of grammar extraction and
its applications. In Proceedings of the Joint
SIGDAT Conference on Empirical Methods in
</reference>
<page confidence="0.985193">
311
</page>
<reference confidence="0.9683765">
Natural Language Processing and Very Large
Corpora. pages 53–62.
</reference>
<page confidence="0.998559">
312
</page>
</variant>
</algorithm>
<algorithm name="ParsHed" version="110505">
<variant no="0" confidence="0.463971">
<title confidence="0.998004">Incremental Semantic Role Labeling with Tree Adjoining Grammar</title>
<author confidence="0.99993">Frank Vera</author>
<affiliation confidence="0.99267">Institute for Language, Cognition and School of Informatics, University of †: Cluster of Excellence Multimodal Computing and</affiliation>
<address confidence="0.539748">Saarland</address>
<email confidence="0.991907">vera@coli.uni-saarland.de</email>
<abstract confidence="0.994324">We introduce the task of incremental semantic role labeling (iSRL), in which semantic roles are assigned to incomplete input (sentence prefixes). iSRL is the semantic equivalent of incremental parsing, and is useful for language modeling, sentence completion, machine translation, and psycholinguistic modeling. We propose an iSRL system that combines an incremental TAG parser with a semantically enriched lexicon, a role propagation algorithm, and a cascade of classifiers. Our approach achieves an SRL Fscore of 78.38% on the standard CoNLL dataset. It substantially forms a strong baseline that combines gold-standard syntactic dependencies with heuristic role assignment, as well as a baseline based on Nivre’s incremental dependency parser.</abstract>
</variant>
</algorithm>
<algorithm name="ParsCit" version="110505">
<citationList>
<citation valid="true">
<authors>
<author>Niels Beuck</author>
<author>Arne Khn</author>
<author>Wolfgang Menzel</author>
</authors>
<title>Incremental parsing and the evaluation of partial dependency analyses.</title>
<date>2011</date>
<booktitle>In Proceedings of the 1st International Conference on Dependency Linguistics. Depling</booktitle>
<contexts>
<context position="29299" citStr="Beuck et al. (2011)" startWordPosition="4873" endWordPosition="4876"> 52.50 Table 3: Full-sentence combined SRL score This gives the baseline a considerable advantage especially in case of longer range dependencies. The third configuration (iSRL-Oracle), is identical to iSRL, but uses the gold standard roles for each PLTAG lexicon entry, and thus provides an upperbound for our methodology. Finally, we evaluated against Malt-Baseline, a variant of MajorityBaseline that uses the MaltParser of Nivre et al. (2007) to provide labeled syntactic dependencies MaltParser is a state-of-the-art shift-reduce dependency parser which uses an incremental algorithm. Following Beuck et al. (2011), we modified the parser to provide intermediate output at each word by emitting the current state of the dependency graph before each shift step. We trained MaltParser using the arc-eager algorithm (which outperformed the other parsing algorithms available with MaltParser) on the CoNLL dataset, achieving a labeled dependency accuracy of 89.66% on section 23. 6 Results Figures 6 and 7 show the results on the incremental SRL task. We plot the F1 for Unlabeled Prediction Score (UPS) and Combined Incremental SRL Score (CISS) per word, separately for sentences of lengths 10, 20, 30, and 40 words. </context>
</contexts>
<marker>Beuck, Khn, Menzel, 2011</marker>
<rawString>Beuck, Niels, Arne Khn, and Wolfgang Menzel. 2011. Incremental parsing and the evaluation of partial dependency analyses. In Proceedings of the 1st International Conference on Dependency Linguistics. Depling 2011.</rawString>
</citation>
<citation valid="true">
<authors>
<author>Anders Bj¨orkelund</author>
<author>Love Hafdell</author>
<author>Pierre Nugues</author>
</authors>
<title>Multilingual semantic role labeling.</title>
<date>2009</date>
<journal>CoNLL</journal>
<booktitle>In Proceedings of the Thirteenth Conference on Computational Natural Language Learning: Shared Task. Association for Computational Linguistics,</booktitle>
<volume>09</volume>
<pages>43--48</pages>
<location>Stroudsburg, PA, USA,</location>
<marker>Bj¨orkelund, Hafdell, Nugues, 2009</marker>
<rawString>Bj¨orkelund, Anders, Love Hafdell, and Pierre Nugues. 2009. Multilingual semantic role labeling. In Proceedings of the Thirteenth Conference on Computational Natural Language Learning: Shared Task. Association for Computational Linguistics, Stroudsburg, PA, USA, CoNLL ’09, pages 43–48.</rawString>
</citation>
<citation valid="true">
<authors>
<author>Xavier Carreras</author>
<author>Lluis M`arquez</author>
</authors>
<title>Introduction to the conll-2005 shared task: Semantic role labeling.</title>
<date>2005</date>
<journal>CONLL</journal>
<booktitle>In Proceedings of the Ninth Conference on Computational Natural Language Learning. Association for Computational Linguistics,</booktitle>
<volume>05</volume>
<pages>152--164</pages>
<location>Stroudsburg, PA, USA,</location>
<marker>Carreras, M`arquez, 2005</marker>
<rawString>Carreras, Xavier and Lluis M`arquez. 2005. Introduction to the conll-2005 shared task: Semantic role labeling. In Proceedings of the Ninth Conference on Computational Natural Language Learning. Association for Computational Linguistics, Stroudsburg, PA, USA, CONLL ’05, pages 152–164.</rawString>
</citation>
<citation valid="true">
<authors>
<author>David Chiang</author>
</authors>
<title>Statistical parsing with an automatically-extracted tree adjoining grammar.</title>
<date>2000</date>
<booktitle>In Proceedings of the 38th Annual Meeting on Association for Computational Linguistics.</booktitle>
<pages>456--463</pages>
<contexts>
<context position="12079" citStr="Chiang (2000)" startWordPosition="1972" endWordPosition="1973">rgument/modifier information from Propbank (Palmer et al., 2005). This makes it possible to decompose the Treebank trees into elementary trees as proposed by Xia et al. (2000). Prediction trees can be learned from the converted Treebank by calculating the connection path (Mazzei et al., 2007) at each word in a tree. Intuitively, a prediction tree for word wn contains the structure that is necessary to connect wn to the prefix tree w1 ...wn_1, but is not part of any of the elementary trees of w1 ...wn_1. Using this lexicon, a probabilistic model over PLTAG operations can be estimated following Chiang (2000). 4 Model 4.1 Problem Formulation In a typical semantic role labeling scenario, the goal is to first identify words that are predicates in the sentence and then identify and label all the arguments for each predicate. This translates into spotting specific words in a sentence that represent the predicate’s arguments, and assigning predefined semantic role labels to them. Note that in this work we focus on verb predicates only. The output of a semantic role labeler is a set of semantic dependency triples (l,a, p), with l E R,, and a, p E w, where R, is a set of semantic role labels denoting a s</context>
</contexts>
<marker>Chiang, 2000</marker>
<rawString>Chiang, David. 2000. Statistical parsing with an automatically-extracted tree adjoining grammar. In Proceedings of the 38th Annual Meeting on Association for Computational Linguistics. pages 456–463.</rawString>
</citation>
<citation valid="true">
<authors>
<author>Vera Demberg</author>
<author>Frank Keller</author>
<author>Alexander Koller</author>
</authors>
<title>Incremental, predictive parsing with psycholinguistically motivated treeadjoining grammar.</title>
<date>2013</date>
<journal>Computational Linguistics</journal>
<volume>39</volume>
<issue>4</issue>
<contexts>
<context position="4831" citStr="Demberg et al. (2013)" startWordPosition="739" endWordPosition="742">letions, in any real time application systems, such as dialog processing, and to incrementalize applications such as machine translation (e.g., in speech-tospeech MT). Crucially, any comprehensive model of human language understanding needs to combine an incremental parser with an incremental semantic processor (Pad´o et al., 2009; Keller, 2010). The present work takes inspiration from the psycholinguistic modeling literature by proposing an iSRL system that is built on top of a cognitively motivated incremental parser, viz., the Psycholinguistically Motivated Tree Adjoining Grammar parser of Demberg et al. (2013). This parser includes a predictive component, i.e., it predicts syntactic structure for upcoming input during incremental processing. This makes PLTAG particularly suitable for iSRL, allowing it to predict incomplete semantic roles as the input string unfolds. Competing approaches, such as iSRL based on an incremental dependency parser, do not share this advantage, as we will discuss in Section 4.3. 2 Related Work Most SRL systems to date conceptualize semantic role labeling as a supervised learning problem and rely on role-annotated data for model training. Existing models often implement a </context>
<context position="6748" citStr="Demberg et al., 2013" startWordPosition="1053" endWordPosition="1056">or the classifier to learn from. Moreover, as we will discuss in Section 4.4, such path information is not always available when the input is processed incrementally. There is previous SRL work employing Tree Adjoining Grammar, albeit in a non-incremental setting, as a means to reduce the sparsity of syntaxbased features. Liu and Sarkar (2007) extract a rich feature set from TAG derivations and demonstrate that this improves SRL performance. In contrast to incremental parsing, incremental semantic role labeling is a novel task. Our model builds on an incremental Tree Adjoining Grammar parser (Demberg et al., 2013) which predicts the syntactic structure of upcoming input. This allows us to perform incremental parsing and incremental SRL in tandem, exploiting the predictive component of the parser to assign (potentially incomplete) semantic roles on a word-by-word basis. Similar to work on incremental parsing that evaluates incomplete trees (Sangati and Keller, 2013), we evaluate the incomplete semantic structures produced by our model. 3 Psycholinguistically Motivated TAG Demberg et al. (2013) introduce Psycholinguistically Motivated Tree Adjoining Grammar (PLTAG), a grammar formalism that extends stand</context>
<context position="9897" citStr="Demberg et al. (2013)" startWordPosition="1578" endWordPosition="1581"> and (b) initial trees, (c) auxiliary tree, (d) prediction tree. (a) valid (b) invalid Figure 3: The current fringe (dashed line) indicates where valid substitutions can occur. Other substitutions result in an invalid prefix tree. of non-predictive elementary trees. An example of a PLTAG derivation is given in Figure 2. In step 1, a prediction tree is introduced through substitution, which then allows the adjunction of an adverb in step 2. Step 3 involves the verification of the marker introduced by the prediction tree against the elementary tree for open. In order to efficiently parse PLTAG, Demberg et al. (2013) introduce the concept of fringes. Fringes capture the fact that in an incremental derivation, a prefix tree can only be combined with an elementary tree at a limited set of nodes. For instance, the prefix tree in Figure 3 has two substitution nodes, for B and C. However, only substitution into B leads to a valid new prefix tree; if we substitute into C, we obtain the tree in Figure 3b, which is not a valid prefix tree (i.e., it represents a non-incremental derivation). The parsing algorithm proposed by Demberg et al. (2013) exploits fringes to tabulate intermediate results. It manipulates a c</context>
<context position="11358" citStr="Demberg et al. (2013)" startWordPosition="1845" endWordPosition="1848">es f such that the chart has entries in the cell (i, f ). For each such fringe, it needs to determine the elementary trees in the lexicon that can be combined with f using substitution or adjunction. In spite of the large size of a typical TAG lexicon, this can be done efficiently, as it only requires matching the current fringes. For each match, the parser then computes the new preFigure 4: Syntactic dependency graph with semantic role annotation and the accompanying semantic triples, for Banks refused to open today. fix trees and its new current fringe f&apos; and enters it into cell (i+ 1, f&apos;). Demberg et al. (2013) convert the Penn Treebank (Marcus et al., 1993) into TAG format by enriching it with head information and argument/modifier information from Propbank (Palmer et al., 2005). This makes it possible to decompose the Treebank trees into elementary trees as proposed by Xia et al. (2000). Prediction trees can be learned from the converted Treebank by calculating the connection path (Mazzei et al., 2007) at each word in a tree. Intuitively, a prediction tree for word wn contains the structure that is necessary to connect wn to the prefix tree w1 ...wn_1, but is not part of any of the elementary tree</context>
</contexts>
<marker>Demberg, Keller, Koller, 2013</marker>
<rawString>Demberg, Vera, Frank Keller, and Alexander Koller. 2013. Incremental, predictive parsing with psycholinguistically motivated treeadjoining grammar. Computational Linguistics 39(4):1025–1066.</rawString>
</citation>
<citation valid="true">
<authors>
<author>Rong-En Fan</author>
<author>Kai-Wei Chang</author>
<author>Cho-Jui Hsieh</author>
<author>Xiang-Rui Wang</author>
<author>Chih-Jen Lin</author>
</authors>
<title>Liblinear: A library for large linear classification.</title>
<date>2008</date>
<journal>Journal of Machine Learning Research</journal>
<volume>9</volume>
<pages>1874</pages>
<contexts>
<context position="21932" citStr="Fan et al. (2008)" startWordPosition="3664" endWordPosition="3667"> a whole, given bilexical and syntactic information. If the triple is identified as a good candidate, then we perform multi-class classification over role labels: we feed the same bilexical and syntactic information to a logistic classifier, and get a ranked list of labels. We then use this list to re-rank the existing ambiguous role labels in the semantic triple, and output the top scoring ones. The identifier is a binary L2-loss support vector classifier, and the role disambiguator an L2- regularized logistic regression classifier, both implemented using the efficient LIBLINEAR framework of Fan et al. (2008). The features used are based on Bj¨orkelund et al. (2009) and Liu and Sarkar (2007), and are listed in Table 2. The bilexical features are: predicate POS tag, predicate lemma, argument word form, argument POS tag, and position. The latter indicates the position of the argument relative to the predicate, i.e., before, on, or after. The syntactic features are: the predicate and argument elementary trees without the anchors (to avoid sparsity), the category of the integration point node on the prefix tree where the elementary tree of the argument attaches to, an alphabetically ordered set of the</context>
</contexts>
<marker>Fan, Chang, Hsieh, Wang, Lin, 2008</marker>
<rawString>Fan, Rong-En, Kai-Wei Chang, Cho-Jui Hsieh, Xiang-Rui Wang, and Chih-Jen Lin. 2008. Liblinear: A library for large linear classification. Journal of Machine Learning Research 9:1871– 1874.</rawString>
</citation>
<citation valid="false">
<authors>
<author>Jan Hajiˇc</author>
<author>Massimiliano Ciaramita</author>
<author>Richard Johansson</author>
<author>Daisuke Kawahara</author>
<author>Maria Ant`onia Marti</author>
<author>Lluis M`arquez</author>
<author>Adam Meyers</author>
<author>Joakim Nivre</author>
<author>Sebastian Pad´o</author>
<author>Jan ˇStˇep´anek</author>
<author>Pavel Straˇn´ak</author>
<author>Mihai Surdeanu</author>
<author>Nianwen Xue</author>
<author>Yi Zhang</author>
</authors>
<title>The CoNLL-2009 shared task: Syntactic and semantic dependencies in multiple languages.</title>
<date>2009</date>
<booktitle>In Proceedings of the 13th Conference on Computational Natural Language Learning (CoNLL-2009),</booktitle>
<pages>4--5</pages>
<location>Boulder, Colorado, USA.</location>
<marker>Hajiˇc, Ciaramita, Johansson, Kawahara, Marti, M`arquez, Meyers, Nivre, Pad´o, ˇStˇep´anek, Straˇn´ak, Surdeanu, Xue, Zhang, 2009</marker>
<rawString>Hajiˇc, Jan, Massimiliano Ciaramita, Richard Johansson, Daisuke Kawahara, Maria Ant`onia Marti, Lluis M`arquez, Adam Meyers, Joakim Nivre, Sebastian Pad´o, Jan ˇStˇep´anek, Pavel Straˇn´ak, Mihai Surdeanu, Nianwen Xue, and Yi Zhang. 2009. The CoNLL-2009 shared task: Syntactic and semantic dependencies in multiple languages. In Proceedings of the 13th Conference on Computational Natural Language Learning (CoNLL-2009), June 4-5. Boulder, Colorado, USA.</rawString>
</citation>
<citation valid="true">
<authors>
<author>Richard Johansson</author>
<author>Pierre Nugues</author>
</authors>
<title>Extended constituent-to-dependency conversion for english.</title>
<date>2007</date>
<booktitle>NODALIDA 2007 Proceedings. University of Tartu,</booktitle>
<pages>105--112</pages>
<editor>In Joakim Nivre, Heiki-Jaan Kalep, Kadri Muischnek, and Mare Koit, editors,</editor>
<marker>Johansson, Nugues, 2007</marker>
<rawString>Johansson, Richard and Pierre Nugues. 2007. Extended constituent-to-dependency conversion for english. In Joakim Nivre, Heiki-Jaan Kalep, Kadri Muischnek, and Mare Koit, editors, NODALIDA 2007 Proceedings. University of Tartu, pages 105–112.</rawString>
</citation>
<citation valid="true">
<authors>
<author>Aravind K Joshi</author>
<author>Yves Schabes</author>
</authors>
<title>Tree adjoining grammars and lexicalized grammars.</title>
<date>1992</date>
<booktitle>In Maurice Nivat and Andreas Podelski, editors, Tree Automata and Languages, North-Holland,</booktitle>
<pages>409--432</pages>
<location>Amsterdam,</location>
<contexts>
<context position="7381" citStr="Joshi and Schabes, 1992" startWordPosition="1147" endWordPosition="1150">edicts the syntactic structure of upcoming input. This allows us to perform incremental parsing and incremental SRL in tandem, exploiting the predictive component of the parser to assign (potentially incomplete) semantic roles on a word-by-word basis. Similar to work on incremental parsing that evaluates incomplete trees (Sangati and Keller, 2013), we evaluate the incomplete semantic structures produced by our model. 3 Psycholinguistically Motivated TAG Demberg et al. (2013) introduce Psycholinguistically Motivated Tree Adjoining Grammar (PLTAG), a grammar formalism that extends standard TAG (Joshi and Schabes, 1992) in order to enable incremental parsing. Standard TAG assumes a lexicon of elementary trees, each of which contains at least one lexical item as an anchor and at most one leaf node as a foot node, marked with A∗. All other leaves are marked with A↓ and are called substitution nodes. Elementary trees that contain a foot node are called auxiliary trees; those that do not are called initial trees. Examples for TAG elementary trees are given in Figure 1a–c. To derive a TAG parse for a sentence, we start with the elementary tree of the head of the sentence and integrate the elementary trees of the </context>
</contexts>
<marker>Joshi, Schabes, 1992</marker>
<rawString>Joshi, Aravind K. and Yves Schabes. 1992. Tree adjoining grammars and lexicalized grammars. In Maurice Nivat and Andreas Podelski, editors, Tree Automata and Languages, North-Holland, Amsterdam, pages 409–432.</rawString>
</citation>
<citation valid="true">
<authors>
<author>Frank Keller</author>
</authors>
<title>Cognitively plausible models of human language processing.</title>
<date>2010</date>
<booktitle>In Proceedings of the 48th Annual Meeting of the Association for Computational Linguistics, Companion Volume: Short Papers.</booktitle>
<pages>60--67</pages>
<location>Uppsala,</location>
<contexts>
<context position="4557" citStr="Keller, 2010" startWordPosition="699" endWordPosition="700">in sentence completion systems to 301 Proceedings of the 2014 Conference on Empirical Methods in Natural Language Processing (EMNLP), pages 301–312, October 25-29, 2014, Doha, Qatar. c�2014 Association for Computational Linguistics provide semantically informed completions, in any real time application systems, such as dialog processing, and to incrementalize applications such as machine translation (e.g., in speech-tospeech MT). Crucially, any comprehensive model of human language understanding needs to combine an incremental parser with an incremental semantic processor (Pad´o et al., 2009; Keller, 2010). The present work takes inspiration from the psycholinguistic modeling literature by proposing an iSRL system that is built on top of a cognitively motivated incremental parser, viz., the Psycholinguistically Motivated Tree Adjoining Grammar parser of Demberg et al. (2013). This parser includes a predictive component, i.e., it predicts syntactic structure for upcoming input during incremental processing. This makes PLTAG particularly suitable for iSRL, allowing it to predict incomplete semantic roles as the input string unfolds. Competing approaches, such as iSRL based on an incremental depen</context>
</contexts>
<marker>Keller, 2010</marker>
<rawString>Keller, Frank. 2010. Cognitively plausible models of human language processing. In Proceedings of the 48th Annual Meeting of the Association for Computational Linguistics, Companion Volume: Short Papers. Uppsala, pages 60–67.</rawString>
</citation>
<citation valid="true">
<authors>
<author>Joel Lang</author>
<author>Mirella Lapata</author>
</authors>
<title>Similaritydriven semantic role induction via graph partitioning.</title>
<date>2014</date>
<journal>Computational Linguistics Accepted</journal>
<pages>1--62</pages>
<note>To appear.</note>
<contexts>
<context position="28240" citStr="Lang and Lapata (2014)" startWordPosition="4714" endWordPosition="4717">ambiguous cases). It seems that the syntactic information held in the elementary trees discriminates well among different senses. 5.3 System Comparison We evaluated three configurations of our system. The first configuration (iSRL) uses all semantic roles for each PLTAG lexicon entry, applies the PLTAG parser, IRPA, and both classifiers to perform identification and disambiguation, as described in Section 4. The second one (MajorityBaseline), solves the problem of argument identification and role disambiguation without the classifiers. For the former we employ a set of heuristics according to Lang and Lapata (2014), that rely on gold syntactic dependency information, sourced from CoNLL input. For the latter, we choose the most frequent role given the gold standard dependency relation label for the particular argument. Note that dependencies have been produced in view of the whole sentence and not incrementally. 308 System Prec Rec F1 iSRL-Oracle 91.00 80.26 85.29 iSRL 81.48 75.51 78.38 Majority-Baseline 71.05 58.10 63.92 Malt-Baseline 60.90 46.14 52.50 Table 3: Full-sentence combined SRL score This gives the baseline a considerable advantage especially in case of longer range dependencies. The third con</context>
</contexts>
<marker>Lang, Lapata, 2014</marker>
<rawString>Lang, Joel and Mirella Lapata. 2014. Similaritydriven semantic role induction via graph partitioning. Computational Linguistics Accepted pages 1–62. To appear.</rawString>
</citation>
<citation valid="true">
<authors>
<author>Yudong Liu</author>
<author>Anoop Sarkar</author>
</authors>
<title>Experimental evaluation of LTAG-based features for semantic role labeling.</title>
<date>2007</date>
<booktitle>In Proceedings of the 2007 Joint Conference on Empirical Methods in Natural Language Processing and Computational Natural Language Learning (EMNLP-CoNLL).</booktitle>
<pages>590--599</pages>
<location>Prague, Czech Republic,</location>
<contexts>
<context position="6472" citStr="Liu and Sarkar (2007)" startWordPosition="1009" endWordPosition="1012">essing as the path from an argument to the predicate can be very informative but is often quite complicated, and depends on the syntactic formalism used. Many paths through the parse tree are likely to occur infrequently (or not at all), resulting in very sparse information for the classifier to learn from. Moreover, as we will discuss in Section 4.4, such path information is not always available when the input is processed incrementally. There is previous SRL work employing Tree Adjoining Grammar, albeit in a non-incremental setting, as a means to reduce the sparsity of syntaxbased features. Liu and Sarkar (2007) extract a rich feature set from TAG derivations and demonstrate that this improves SRL performance. In contrast to incremental parsing, incremental semantic role labeling is a novel task. Our model builds on an incremental Tree Adjoining Grammar parser (Demberg et al., 2013) which predicts the syntactic structure of upcoming input. This allows us to perform incremental parsing and incremental SRL in tandem, exploiting the predictive component of the parser to assign (potentially incomplete) semantic roles on a word-by-word basis. Similar to work on incremental parsing that evaluates incomplet</context>
<context position="22016" citStr="Liu and Sarkar (2007)" startWordPosition="3679" endWordPosition="3682"> as a good candidate, then we perform multi-class classification over role labels: we feed the same bilexical and syntactic information to a logistic classifier, and get a ranked list of labels. We then use this list to re-rank the existing ambiguous role labels in the semantic triple, and output the top scoring ones. The identifier is a binary L2-loss support vector classifier, and the role disambiguator an L2- regularized logistic regression classifier, both implemented using the efficient LIBLINEAR framework of Fan et al. (2008). The features used are based on Bj¨orkelund et al. (2009) and Liu and Sarkar (2007), and are listed in Table 2. The bilexical features are: predicate POS tag, predicate lemma, argument word form, argument POS tag, and position. The latter indicates the position of the argument relative to the predicate, i.e., before, on, or after. The syntactic features are: the predicate and argument elementary trees without the anchors (to avoid sparsity), the category of the integration point node on the prefix tree where the elementary tree of the argument attaches to, an alphabetically ordered set of the categories of the fringe nodes of the prefix tree after attaching the argument tree</context>
</contexts>
<marker>Liu, Sarkar, 2007</marker>
<rawString>Liu, Yudong and Anoop Sarkar. 2007. Experimental evaluation of LTAG-based features for semantic role labeling. In Proceedings of the 2007 Joint Conference on Empirical Methods in Natural Language Processing and Computational Natural Language Learning (EMNLP-CoNLL). Prague, Czech Republic, pages 590–599.</rawString>
</citation>
<citation valid="true">
<authors>
<author>Mitchell P Marcus</author>
<author>Mary Ann Marcinkiewicz</author>
<author>Beatrice Santorini</author>
</authors>
<title>Building a large annotated corpus of english: The penn treebank.</title>
<date>1993</date>
<journal>Computational Linguistics</journal>
<volume>19</volume>
<issue>2</issue>
<contexts>
<context position="11406" citStr="Marcus et al., 1993" startWordPosition="1854" endWordPosition="1857">(i, f ). For each such fringe, it needs to determine the elementary trees in the lexicon that can be combined with f using substitution or adjunction. In spite of the large size of a typical TAG lexicon, this can be done efficiently, as it only requires matching the current fringes. For each match, the parser then computes the new preFigure 4: Syntactic dependency graph with semantic role annotation and the accompanying semantic triples, for Banks refused to open today. fix trees and its new current fringe f&apos; and enters it into cell (i+ 1, f&apos;). Demberg et al. (2013) convert the Penn Treebank (Marcus et al., 1993) into TAG format by enriching it with head information and argument/modifier information from Propbank (Palmer et al., 2005). This makes it possible to decompose the Treebank trees into elementary trees as proposed by Xia et al. (2000). Prediction trees can be learned from the converted Treebank by calculating the connection path (Mazzei et al., 2007) at each word in a tree. Intuitively, a prediction tree for word wn contains the structure that is necessary to connect wn to the prefix tree w1 ...wn_1, but is not part of any of the elementary trees of w1 ...wn_1. Using this lexicon, a probabili</context>
</contexts>
<marker>Marcus, Marcinkiewicz, Santorini, 1993</marker>
<rawString>Marcus, Mitchell P., Mary Ann Marcinkiewicz, and Beatrice Santorini. 1993. Building a large annotated corpus of english: The penn treebank. Computational Linguistics 19(2):313–330.</rawString>
</citation>
<citation valid="true">
<authors>
<author>Lluis M`arquez</author>
<author>Xavier Carreras</author>
<author>Kenneth C Litkowski</author>
<author>Suzanne Stevenson</author>
</authors>
<title>Semantic Role Labeling: An Introduction to the Special Issue.</title>
<date>2008</date>
<journal>Computational Linguistics</journal>
<volume>34</volume>
<issue>2</issue>
<marker>M`arquez, Carreras, Litkowski, Stevenson, 2008</marker>
<rawString>M`arquez, Lluis, Xavier Carreras, Kenneth C. Litkowski, and Suzanne Stevenson. 2008. Semantic Role Labeling: An Introduction to the Special Issue. Computational Linguistics 34(2):145–159.</rawString>
</citation>
<citation valid="true">
<authors>
<author>Alessandro Mazzei</author>
<author>Vincenzo Lombardo</author>
<author>Patrick Sturt</author>
</authors>
<title>Dynamic TAG and lexical dependencies.</title>
<date>2007</date>
<journal>Research on Language and Computation</journal>
<pages>5--309</pages>
<contexts>
<context position="11759" citStr="Mazzei et al., 2007" startWordPosition="1912" endWordPosition="1915">ic dependency graph with semantic role annotation and the accompanying semantic triples, for Banks refused to open today. fix trees and its new current fringe f&apos; and enters it into cell (i+ 1, f&apos;). Demberg et al. (2013) convert the Penn Treebank (Marcus et al., 1993) into TAG format by enriching it with head information and argument/modifier information from Propbank (Palmer et al., 2005). This makes it possible to decompose the Treebank trees into elementary trees as proposed by Xia et al. (2000). Prediction trees can be learned from the converted Treebank by calculating the connection path (Mazzei et al., 2007) at each word in a tree. Intuitively, a prediction tree for word wn contains the structure that is necessary to connect wn to the prefix tree w1 ...wn_1, but is not part of any of the elementary trees of w1 ...wn_1. Using this lexicon, a probabilistic model over PLTAG operations can be estimated following Chiang (2000). 4 Model 4.1 Problem Formulation In a typical semantic role labeling scenario, the goal is to first identify words that are predicates in the sentence and then identify and label all the arguments for each predicate. This translates into spotting specific words in a sentence tha</context>
</contexts>
<marker>Mazzei, Lombardo, Sturt, 2007</marker>
<rawString>Mazzei, Alessandro, Vincenzo Lombardo, and Patrick Sturt. 2007. Dynamic TAG and lexical dependencies. Research on Language and Computation 5:309–332.</rawString>
</citation>
<citation valid="true">
<authors>
<author>Joakim Nivre</author>
<author>Johan Hall</author>
<author>Jens Nilsson</author>
</authors>
<title>Atanas Chanev, G¨ulsen Eryigit, Sandra K¨ubler, Svetoslav Marinov, and Erwin Marsi.</title>
<date>2007</date>
<journal>Natural Language Engineering</journal>
<pages>13--95</pages>
<contexts>
<context position="20070" citStr="Nivre et al., 2007" startWordPosition="3363" endWordPosition="3366"> role labels A0 and A2. This illustrated how the lexicalized verification tree disambiguates the semantic information stored in the prediction tree. Finally, trace t is set to the closest NP head that is below the same phrase subtree, in this case Banks. Note that Banks is part of two triples as shown in the last tree of Figure 5b: it is either an A0 or an A1 for refused and an A1 for open. We are able to create incomplete semantic triples after the prediction of the upcoming verb at step 2, as shown in Figure 5b. This is not possible using an incremental dependency parser such as MaltParser (Nivre et al., 2007) that lacks a predictive component. Table 1 illustrates this by comparing the output of IRPA for Figure 5b with the output of a baseline system that maps role labels onto the syntactic dependencies in Figure 4, generated incrementally by MaltParser (see Section 5.3 for a description of the MaltParser baseline). MaltParser has to wait for the verb open before outputting the relevant semantic triples. In contrast, IRPA outputs incomplete triples as soon as the information is available, and later on updates its decision. (MaltParser also incorrectly assigns A0 for the Banks–open pair.) 4.4 Argume</context>
<context position="29126" citStr="Nivre et al. (2007)" startWordPosition="4850" endWordPosition="4853">ole sentence and not incrementally. 308 System Prec Rec F1 iSRL-Oracle 91.00 80.26 85.29 iSRL 81.48 75.51 78.38 Majority-Baseline 71.05 58.10 63.92 Malt-Baseline 60.90 46.14 52.50 Table 3: Full-sentence combined SRL score This gives the baseline a considerable advantage especially in case of longer range dependencies. The third configuration (iSRL-Oracle), is identical to iSRL, but uses the gold standard roles for each PLTAG lexicon entry, and thus provides an upperbound for our methodology. Finally, we evaluated against Malt-Baseline, a variant of MajorityBaseline that uses the MaltParser of Nivre et al. (2007) to provide labeled syntactic dependencies MaltParser is a state-of-the-art shift-reduce dependency parser which uses an incremental algorithm. Following Beuck et al. (2011), we modified the parser to provide intermediate output at each word by emitting the current state of the dependency graph before each shift step. We trained MaltParser using the arc-eager algorithm (which outperformed the other parsing algorithms available with MaltParser) on the CoNLL dataset, achieving a labeled dependency accuracy of 89.66% on section 23. 6 Results Figures 6 and 7 show the results on the incremental SRL</context>
</contexts>
<marker>Nivre, Hall, Nilsson, 2007</marker>
<rawString>Nivre, Joakim, Johan Hall, Jens Nilsson, Atanas Chanev, G¨ulsen Eryigit, Sandra K¨ubler, Svetoslav Marinov, and Erwin Marsi. 2007. Maltparser: A language-independent system for data-driven dependency parsing. Natural Language Engineering 13:95–135.</rawString>
</citation>
<citation valid="true">
<authors>
<author>Ulrike Pad´o</author>
<author>Matthew W Crocker</author>
<author>Frank Keller</author>
</authors>
<title>A probabilistic model of semantic plausibility in sentence processing.</title>
<date>2009</date>
<journal>Cognitive Science</journal>
<volume>33</volume>
<issue>5</issue>
<marker>Pad´o, Crocker, Keller, 2009</marker>
<rawString>Pad´o, Ulrike, Matthew W. Crocker, and Frank Keller. 2009. A probabilistic model of semantic plausibility in sentence processing. Cognitive Science 33(5):794–838.</rawString>
</citation>
<citation valid="true">
<authors>
<author>Martha Palmer</author>
<author>Daniel Gildea</author>
<author>Paul Kingsbury</author>
</authors>
<title>The proposition bank: An annotated corpus of semantic roles.</title>
<date>2005</date>
<journal>Computational Linguistics</journal>
<volume>31</volume>
<issue>1</issue>
<contexts>
<context position="11530" citStr="Palmer et al., 2005" startWordPosition="1873" endWordPosition="1876">ubstitution or adjunction. In spite of the large size of a typical TAG lexicon, this can be done efficiently, as it only requires matching the current fringes. For each match, the parser then computes the new preFigure 4: Syntactic dependency graph with semantic role annotation and the accompanying semantic triples, for Banks refused to open today. fix trees and its new current fringe f&apos; and enters it into cell (i+ 1, f&apos;). Demberg et al. (2013) convert the Penn Treebank (Marcus et al., 1993) into TAG format by enriching it with head information and argument/modifier information from Propbank (Palmer et al., 2005). This makes it possible to decompose the Treebank trees into elementary trees as proposed by Xia et al. (2000). Prediction trees can be learned from the converted Treebank by calculating the connection path (Mazzei et al., 2007) at each word in a tree. Intuitively, a prediction tree for word wn contains the structure that is necessary to connect wn to the prefix tree w1 ...wn_1, but is not part of any of the elementary trees of w1 ...wn_1. Using this lexicon, a probabilistic model over PLTAG operations can be estimated following Chiang (2000). 4 Model 4.1 Problem Formulation In a typical sema</context>
</contexts>
<marker>Palmer, Gildea, Kingsbury, 2005</marker>
<rawString>Palmer, Martha, Daniel Gildea, and Paul Kingsbury. 2005. The proposition bank: An annotated corpus of semantic roles. Computational Linguistics 31(1):71–106.</rawString>
</citation>
<citation valid="true">
<authors>
<author>Martin J Pickering</author>
<author>Matthew J Traxler</author>
<author>Matthew W Crocker</author>
</authors>
<title>Ambiguity resolution in sentence processing: Evidence against frequency-based accounts.</title>
<date>2000</date>
<journal>Journal of Memory and Language</journal>
<volume>43</volume>
<issue>3</issue>
<contexts>
<context position="2075" citStr="Pickering et al., 2000" startWordPosition="310" endWordPosition="313">out of reach. When reaching the noun phrase her goals, the human language processor is faced with a semantic role ambiguity: her goals can either be the PATIENT of the verb realize, or it can be the THEME of a subsequent verb that has not been encountered yet. Experimental evidence shows that the human language processor initially prefers the PATIENT role, but switches its preference to the theme role when it reaches the subordinate verb were. Such semantic garden paths occur because human language processing occurs word-by-word, and are well attested in the psycholinguistic literature (e.g., Pickering et al., 2000). Computational systems for performing semantic role labeling (SRL), on the other hand, proceed non-incrementally. They require the whole sentence (typically together with its complete syntactic structure) as input and assign all semantic roles at once. The reason for this is that most features used by current SRL systems are defined globally, and cannot be computed on sentence prefixes. In this paper, we propose incremental SRL (iSRL) as a new computational task that mimics human semantic role assignment. The aim of an iSRL system is to determine semantic roles while the input unfolds: given </context>
</contexts>
<marker>Pickering, Traxler, Crocker, 2000</marker>
<rawString>Pickering, Martin J., Matthew J. Traxler, and Matthew W. Crocker. 2000. Ambiguity resolution in sentence processing: Evidence against frequency-based accounts. Journal of Memory and Language 43(3):447–475.</rawString>
</citation>
<citation valid="true">
<authors>
<author>Federico Sangati</author>
<author>Frank Keller</author>
</authors>
<title>Incremental tree substitution grammar for parsing and word prediction. Transactions of the Association for Computational Linguistics 1(May):111–124.</title>
<date>2013</date>
<contexts>
<context position="7106" citStr="Sangati and Keller, 2013" startWordPosition="1108" endWordPosition="1111">a rich feature set from TAG derivations and demonstrate that this improves SRL performance. In contrast to incremental parsing, incremental semantic role labeling is a novel task. Our model builds on an incremental Tree Adjoining Grammar parser (Demberg et al., 2013) which predicts the syntactic structure of upcoming input. This allows us to perform incremental parsing and incremental SRL in tandem, exploiting the predictive component of the parser to assign (potentially incomplete) semantic roles on a word-by-word basis. Similar to work on incremental parsing that evaluates incomplete trees (Sangati and Keller, 2013), we evaluate the incomplete semantic structures produced by our model. 3 Psycholinguistically Motivated TAG Demberg et al. (2013) introduce Psycholinguistically Motivated Tree Adjoining Grammar (PLTAG), a grammar formalism that extends standard TAG (Joshi and Schabes, 1992) in order to enable incremental parsing. Standard TAG assumes a lexicon of elementary trees, each of which contains at least one lexical item as an anchor and at most one leaf node as a foot node, marked with A∗. All other leaves are marked with A↓ and are called substitution nodes. Elementary trees that contain a foot node</context>
<context position="30098" citStr="Sangati and Keller 2013" startWordPosition="5006" endWordPosition="5009">he arc-eager algorithm (which outperformed the other parsing algorithms available with MaltParser) on the CoNLL dataset, achieving a labeled dependency accuracy of 89.66% on section 23. 6 Results Figures 6 and 7 show the results on the incremental SRL task. We plot the F1 for Unlabeled Prediction Score (UPS) and Combined Incremental SRL Score (CISS) per word, separately for sentences of lengths 10, 20, 30, and 40 words. The task gets harder with increasing sentence length, hence we can only meaningfully compare the average scores for sentence of the same length. (This approach was proposed by Sangati and Keller 2013 for evaluating the performance of incremental parsers.) The UPS results in Figure 6 clearly show that our system (iSRL) outperforms both baselines on unlabeled argument and predicate prediction, across all four sentence lengths. Furthermore, we note that the iSRL system achieves a nearconstant performance for all sentence prefixes. Our PLTAG-based prediction/verification architecture allows us to correctly predict incomplete semantic role triples, even at the beginning of the sentence. Both baselines perform worse than the iSRL system in general. Moreover, the MaltBaseline performs badly on t</context>
</contexts>
<marker>Sangati, Keller, 2013</marker>
<rawString>Sangati, Federico and Frank Keller. 2013. Incremental tree substitution grammar for parsing and word prediction. Transactions of the Association for Computational Linguistics 1(May):111–124.</rawString>
</citation>
<citation valid="true">
<authors>
<author>Asad Sayeed</author>
<author>Vera Demberg</author>
</authors>
<title>The semantic augmentation of a psycholinguisticallymotivated syntactic formalism.</title>
<date>2013</date>
<booktitle>In Proceedings of the Fourth Annual Workshop on Cognitive Modeling and Computational Linguistics (CMCL). Association for Computational Linguistics,</booktitle>
<pages>57--65</pages>
<location>Sofia, Bulgaria,</location>
<marker>Sayeed, Demberg, 2013</marker>
<rawString>Sayeed, Asad and Vera Demberg. 2013. The semantic augmentation of a psycholinguisticallymotivated syntactic formalism. In Proceedings of the Fourth Annual Workshop on Cognitive Modeling and Computational Linguistics (CMCL). Association for Computational Linguistics, Sofia, Bulgaria, pages 57–65.</rawString>
</citation>
<citation valid="true">
<authors>
<author>Mihai Surdeanu</author>
<author>Richard Johansson</author>
<author>Adam Meyers</author>
<author>Lluis M`arquez</author>
<author>Joakim Nivre</author>
</authors>
<title>The CoNLL-2008 shared task on joint parsing of syntactic and semantic dependencies.</title>
<date>2008</date>
<booktitle>In Proceedings of the 12th Conference on Computational Natural Language Learning (CoNLL2008).</booktitle>
<marker>Surdeanu, Johansson, Meyers, M`arquez, Nivre, 2008</marker>
<rawString>Surdeanu, Mihai, Richard Johansson, Adam Meyers, Lluis M`arquez, and Joakim Nivre. 2008. The CoNLL-2008 shared task on joint parsing of syntactic and semantic dependencies. In Proceedings of the 12th Conference on Computational Natural Language Learning (CoNLL2008).</rawString>
</citation>
<citation valid="true">
<authors>
<author>Ian H Witten</author>
<author>Timothy C Bell</author>
</authors>
<title>The zero-frequency problem: estimating the probabilities of novel events in adaptive text compression. Information Theory,</title>
<date>1991</date>
<journal>IEEE Transactions on</journal>
<volume>37</volume>
<issue>4</issue>
<marker>Witten, Bell, 1991</marker>
<rawString>Witten, Ian H. and Timothy C. Bell. 1991. The zero-frequency problem: estimating the probabilities of novel events in adaptive text compression. Information Theory, IEEE Transactions on 37(4):1085–1094.</rawString>
</citation>
<citation valid="true">
<authors>
<author>Fei Xia</author>
<author>Martha Palmer</author>
<author>Aravind Joshi</author>
</authors>
<title>A uniform method of grammar extraction and its applications.</title>
<date>2000</date>
<booktitle>In Proceedings of the Joint SIGDAT Conference on Empirical Methods in Natural Language Processing and Very Large Corpora.</booktitle>
<pages>53--62</pages>
<contexts>
<context position="11641" citStr="Xia et al. (2000)" startWordPosition="1892" endWordPosition="1895">t only requires matching the current fringes. For each match, the parser then computes the new preFigure 4: Syntactic dependency graph with semantic role annotation and the accompanying semantic triples, for Banks refused to open today. fix trees and its new current fringe f&apos; and enters it into cell (i+ 1, f&apos;). Demberg et al. (2013) convert the Penn Treebank (Marcus et al., 1993) into TAG format by enriching it with head information and argument/modifier information from Propbank (Palmer et al., 2005). This makes it possible to decompose the Treebank trees into elementary trees as proposed by Xia et al. (2000). Prediction trees can be learned from the converted Treebank by calculating the connection path (Mazzei et al., 2007) at each word in a tree. Intuitively, a prediction tree for word wn contains the structure that is necessary to connect wn to the prefix tree w1 ...wn_1, but is not part of any of the elementary trees of w1 ...wn_1. Using this lexicon, a probabilistic model over PLTAG operations can be estimated following Chiang (2000). 4 Model 4.1 Problem Formulation In a typical semantic role labeling scenario, the goal is to first identify words that are predicates in the sentence and then i</context>
</contexts>
<marker>Xia, Palmer, Joshi, 2000</marker>
<rawString>Xia, Fei, Martha Palmer, and Aravind Joshi. 2000. A uniform method of grammar extraction and its applications. In Proceedings of the Joint SIGDAT Conference on Empirical Methods in Natural Language Processing and Very Large Corpora. pages 53–62.</rawString>
</citation>
</citationList>
</algorithm>
</algorithms>
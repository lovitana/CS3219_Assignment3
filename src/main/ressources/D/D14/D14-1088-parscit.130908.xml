<?xml version="1.0" encoding="UTF-8"?>
<algorithms version="110505">
<algorithm name="SectLabel" version="110505">
<variant no="0" confidence="0.000000">
<title confidence="0.990835">
Taxonomy Construction Using Syntactic Contextual Evidence
</title>
<author confidence="0.986827">
Luu Anh Tuan #1, Jung-jae Kim #2, Ng See Kiong *3
</author>
<affiliation confidence="0.996511">
#School of Computer Engineering, Nanyang Technological University, Singapore
</affiliation>
<email confidence="0.884221">
1anhtuan001@e.ntu.edu.sg, 2jungjae.kim@ntu.edu.sg
</email>
<note confidence="0.69184">
*Institute for Infocomm Research, Agency for Science, Technology and Research, Singapore
</note>
<email confidence="0.698154">
3skng@i2r.a-star.edu.sg
</email>
<sectionHeader confidence="0.988413" genericHeader="abstract">
Abstract
</sectionHeader>
<bodyText confidence="0.9999238">
Taxonomies are the backbone of many
structured, semantic knowledge resources.
Recent works for extracting taxonomic
relations from text focused on collect-
ing lexical-syntactic patterns to extract the
taxonomic relations by matching the pat-
terns to text. These approaches, however,
often show low coverage due to the lack of
contextual analysis across sentences. To
address this issue, we propose a novel ap-
proach that collectively utilizes contextual
information of terms in syntactic struc-
tures such that if the set of contexts of
a term includes most of contexts of an-
other term, a subsumption relation be-
tween the two terms is inferred. We ap-
ply this method to the task of taxonomy
construction from scratch, where we intro-
duce another novel graph-based algorithm
for taxonomic structure induction. Our ex-
periment results show that the proposed
method is well complementary with previ-
ous methods of linguistic pattern matching
and significantly improves recall and thus
F-measure.
</bodyText>
<sectionHeader confidence="0.999134" genericHeader="keywords">
1 Introduction
</sectionHeader>
<bodyText confidence="0.999985254545455">
Taxonomies that are backbone of structured on-
tology knowledge have been found to be use-
ful for many areas such as question answering
(Harabagiu et al., 2003), document clustering
(Fodeh et al., 2011) and textual entailment (Gef-
fet and Dagan, 2005). There have been an in-
creasing number of hand-crafted, well-structured
taxonomies publicly available, including WordNet
(Miller, 1995), OpenCyc (Matuszek et al., 2006),
and Freebase (Bollacker et al., 2008). However,
the manual curation of those taxonomies is time-
consuming and human experts may miss relevant
terms. As such, there are still needs to extend ex-
isting taxonomies or even to construct new tax-
onomies from scratch.
The previous methods for identifying taxo-
nomic relations (i.e. is-a relations) from text can
be generally classified into two categories: statis-
tical and linguistic approaches. The former in-
cludes co-occurrence analysis (Budanitsky, 1999),
term subsumption (Fotzo and Gallinari, 2004) and
clustering (Wong et al., 2007). The main idea be-
hinds these techniques is that the terms that fre-
quently co-occur may have taxonomic relation-
ships. Such approaches, however, usually suffer
from low accuracy, though relatively high cover-
age, and heavily depend on the choice of feature
types and datasets. Most previous methods of the
linguistic approach, on the other hand, rely on the
lexical-syntactic patterns (e.g. A is a B, A such as
B) (Hearst, 1992). Those patterns can be manu-
ally created (Kozareva et al., 2008; Wentao et al.,
2012), chosen via automatic bootstrapping (Wid-
dows and Dorow, 2002; Girju et al., 2003) or iden-
tified from machine-learned classifiers (Navigli et
al., 2011). The pattern matching methods gen-
erally achieve high precision, but low coverage
due to the lack of contextual analysis across sen-
tences. In this paper, we introduce a novel statisti-
cal method and shows that when combined with a
pattern matching method, it shows significant per-
formance improvement.
The proposed statistical method, called syntac-
tic contextual subsumption (SCS), compares the
syntactic contexts of terms for the taxonomic re-
lation identification, instead of the usage of bag-
of-words model by the previous statistical meth-
ods. We observe that the terms in taxonomic rela-
tions may not occur in the same sentences, but in
similar syntactic structures of different sentences,
and that the contexts of a specific term are often
found in the contexts of a general term but not vice
versa. By context of a term, we mean the set of
words frequently have a particular syntactic rela-
tion (e.g. Subject-Verb-Object) with the term in a
</bodyText>
<page confidence="0.957013">
810
</page>
<note confidence="0.909598">
Proceedings of the 2014 Conference on Empirical Methods in Natural Language Processing (EMNLP), pages 810–819,
October 25-29, 2014, Doha, Qatar. c�2014 Association for Computational Linguistics
</note>
<bodyText confidence="0.999958716981132">
given corpus. Given two terms, the SCS method
collects from the Web pre-defined syntactic rela-
tions of each of the terms and checks if the syntac-
tic contexts of a term properly includes that of the
other term in order to determine their taxonomic
relation. The method scores each taxonomic rela-
tion candidate based on the two measures of Web-
based evidence and contextual set inclusion, and
as such, is able to find implicit subsumption rela-
tions between terms across sentences. The SCS
shows itself (Section 3.1) to be complementary to
linguistic pattern matching.
After the relation identification, the identified
taxonomic relations should be integrated into a
graph for the task of taxonomy construction from
scratch or associated with existing concepts of a
given taxonomy via is-a relations (Snow et al.,
2006). In this step of taxonomic structure con-
struction, there is a need for pruning incorrect
and redundant relations. Previous methods for the
pruning task (Kozareva and Hovy, 2010; Velardi et
al., 2012) treat the identified taxonomic relations
equally, and the pruning task is thus reduced to
finding the best trade-off between path length and
the connectivity of traversed nodes. This assump-
tion, however, is not always true due to the fact
that the identified taxonomic relations may have
different confidence values, and the relations with
high confidence values can be incorrectly elimi-
nated during the pruning process. We thus propose
a novel method for the taxonomy induction by uti-
lizing the evidence scores from the relation iden-
tification method and the topological properties of
the graph. We show that it can effectively prune
redundant edges and remove loops while preserv-
ing the correct edges of taxonomy.
We apply the proposed methods of taxonomic
relation identification and taxonomy induction to
the task of constructing a taxonomy from a given
text collection from scratch. The resultant system
consists of three modules: Term extraction and
filtering (Section 2.1), taxonomic relation iden-
tification (Section 2.2), and taxonomy induction
(Section 2.3). The outputs of the term extrac-
tion/filtering module are used as inputs of the tax-
onomic relation identification, such that the tax-
onomic relation identification module checks if
there is a taxonomic relation between each pair
of terms from the term extraction/filtering module.
The taxonomy induction module gets the identi-
fied taxonomic relation set as the input, and out-
puts the final optimal taxonomy by pruning redun-
dant and incorrect relations.
</bodyText>
<sectionHeader confidence="0.997691" genericHeader="introduction">
2 Methodology
</sectionHeader>
<subsectionHeader confidence="0.999848">
2.1 Term Extraction and Filtering
</subsectionHeader>
<bodyText confidence="0.999953434782609">
The first step to construct taxonomies is to col-
lect candidate terms from text documents in the
domain of interest. Like most of linguistic ap-
proaches, we use pre-defined linguistic filters to
extract candidate terms, including single-word
terms and multi-word terms which are noun or
noun phrases in sentences. These terms are
then preprocessed by removing determiners and
lemmatization.
The candidate terms collected are then filtered
to select the terms that are most relevant to the
domain of interest. Many statistical techniques
are developed for the filtering, such as TF-IDF,
domain relevance (DR), and domain consensus
(DC) (Navigli and Velardi, 2004). DR measures
the amount of information that a term t captures
within a domain of interest DZ, compared to other
contrasting domains (Dj), while DC measures the
distributed use of a term t across documents d in
a domain DZ. Since three measures have pros and
cons, and might be complementary to each other,
our term filtering method is thus the linear combi-
nation of them:
</bodyText>
<equation confidence="0.9976845">
T5(t, DZ) = α x TFIDF(t, DZ)
+ Q x DR(t, DZ) + &apos;y x DC(t, DZ) (1)
</equation>
<bodyText confidence="0.999924">
We experimented (see Section 3) with different
values of α, Q and &apos;y, and found that the method
shows the best performance when the values for α
and Q are 0.2 and 0.8 and the value for &apos;y is be-
tween 0.15 and 0.35, depending on the size of the
domain corpus.
</bodyText>
<subsectionHeader confidence="0.998785">
2.2 Taxonomic Relation Identification
</subsectionHeader>
<bodyText confidence="0.994633818181818">
In this section, we present three taxonomic rela-
tion identification methods which are adopted in
our system. First, two methods of string inclusion
with WordNet and lexical-syntactic pattern match-
ing, which were commonly used in the literature
will be introduced with some modifications. Then,
a novel syntactic contextual subsumption method
to find implicit relations between terms across sen-
tences by using contextual evidence from syntactic
structures and Web data will be proposed. Finally,
these three methods will be linearly combined to
</bodyText>
<page confidence="0.997141">
811
</page>
<table confidence="0.999671375">
Notation Meaning
t1 ≫ t2 t1 is a hypernym of t2
t1 ≈ t2 t1 semantically equals or is sim-
ilar to t2
t1 ≫WN t2 t1 is a direct or inherited hyper-
nym of t2 according to WordNet
t1 ≈WN t2 t1 and t2 belong to the same
synset of WordNet
</table>
<tableCaption confidence="0.999886">
Table 1: Notations
</tableCaption>
<bodyText confidence="0.99877725">
form an integrating solution for taxonomic rela-
tion identification. Given two terms t1 and t2, Ta-
ble 1 summarizes important notations used in this
paper.
</bodyText>
<subsubsectionHeader confidence="0.712247">
2.2.1 String Inclusion with WordNet (SIWN)
</subsubsectionHeader>
<bodyText confidence="0.999929130434783">
One simple way to check taxonomic relation is to
test string inclusion. For example, “terrorist orga-
nization” is a hypernym of “foreign terrorist orga-
nization”, as the former is a substring of the lat-
ter. We propose an algorithm to extend the string
inclusion test by using WordNet, which will be
named SIWN. Given a candidate general term tg
and a candidate specific term ts, the SIWN al-
gorithm examines tg from left to right (designat-
ing each word in tg to be examined as wg) to
check if there is any word (ws) in ts such that
wg ≈WN ws or wg ≫WN ws, and identifies
the taxonomic relation between two terms if ev-
ery word of tg has a corresponding word in ts
(with at least one ≫WN relation). For example,
consider two terms: “suicide attack” and “world
trade center self-destruction bombing”. Because
“attack” ≫WN “bombing” and “suicide” ≈WN
“self-destruction”, according to SIWN algorithm,
we conclude that “suicide attack” is the hypernym
of “world trade center self-destruction bombing”.
Given two terms t1 and t2, the evidence score
for SIWN algorithm is calculated as follows:
</bodyText>
<equation confidence="0.903902333333333">
{ 1 if t1 ≫ t2 via SIWN
5coreSIWN(t1, t2) = 0 otherwise
(2)
</equation>
<subsubsectionHeader confidence="0.781027">
2.2.2 Lexical-syntactic Pattern
</subsubsectionHeader>
<bodyText confidence="0.882902142857143">
Extending the ideas of Kozareva and Hovy (2010)
and Navigli et al. (2011), we propose a method
of extracting taxonomic relations by matching
lexical-syntactic patterns to the Web data.
Definition 1(Syntactic patterns). Given two terms
t1 and t2, Pat(t1, t2) is defined as the set of the
following patterns:
</bodyText>
<listItem confidence="0.9998434">
• “t1 such as t2”
• “t1, including t2”
• “t2 is [a|an] t1”
• “t2 is a [kind|type] of t1”
• “t2, [and|or] other t1”
</listItem>
<bodyText confidence="0.9730964">
, where t1 and t2 are replaced with actual terms
and [a|b] denotes a choice between a and b.
Given candidate general term t1 and candi-
date specific term t2, the lexical-syntactic pattern
(LSP) method works as follows:
</bodyText>
<listItem confidence="0.910470666666667">
1. Submit each phrase in Pat(t1, t2) to a Web
search engine as a query. The number of
the search results of the query is denoted as
WH(t1, t2).
2. Calculate the following evidence score:
5coreLSP(t1,
</listItem>
<equation confidence="0.8935">
log(WH(t1, t2))
t2) = 1 + log(WH(t2, t1))
(3)
</equation>
<listItem confidence="0.9442405">
3. If 5coreLSP(t1, t2) is greater than a thresh-
old value then t1 ≫ t2.
</listItem>
<bodyText confidence="0.9980422">
While most lexical-syntactic pattern meth-
ods in the literature only consider the value of
WH(t1, t2) in checking t1 ≫ t2 (Wentao et al.,
2012), we take into account both WH(t1, t2) and
WH(t2, t1). The intuition of formula (3) is that if
t1 is a hypernym of t2 then the size of WH(t1, t2)
will be much larger than that of WH(t2, t1),
which means the lexical-syntactic patterns are
more applicable for the ordered pair (t1, t2) than
(t2, t1).
</bodyText>
<subsectionHeader confidence="0.756033">
2.2.3 Syntactic Contextual Subsumption
</subsectionHeader>
<bodyText confidence="0.999974454545454">
The LSP method performs well in recognizing
the taxonomic relations between terms in the
sentences containing those pre-defined syntactic
patterns. This method, however, has a major
shortcoming: it cannot derive taxonomic relations
between two terms occurring in two different
sentences. We thus propose a novel syntactic
contextual subsumption (SCS) method which uti-
lizes contextual information of terms in syntactic
structure (i.e. Subject-Verb-Object in this study)
and Web data to infer implicit taxonomic relations
</bodyText>
<page confidence="0.986036">
812
</page>
<bodyText confidence="0.9999323">
between terms across sentences. Note that the
chosen syntactic structure Subject-Verb-Object
is identical to the definition of non-taxonomic
relations in the literature (Buitelaar et al., 2004),
where the Verb indicate non-taxonomic relations
between Subject and Object. In this subsection,
we first present the method to collect those
non-taxonomic relations. Then we present in
detail the ideas of the SCS method and how we
can use it to derive taxonomic relations in practice.
</bodyText>
<subsectionHeader confidence="0.619949">
A. Non-taxonomic Relation Identification
</subsectionHeader>
<bodyText confidence="0.997148235294118">
Following previous approaches to non-
taxonomic relation identification, e.g. (Ciaramita
et al., 2005), we use the Stanford parser (Klein
and Manning, 2003) to identify the syntactic
structures of sentences and extract triples of
(Subject, Verb, Object), where Subject and Object
are noun phrases.
We further consider the following issues: First,
if a term (or noun phrase) includes a preposition,
we remove the prepositional phrase. However, if
the headword of a term is a quantitative noun like
“lot”, “many” or “dozen” and it is modified by the
preposition “of”, we replace it with the headword
of the object of the preposition “of”. For example,
we can extract the triples (people, need, food)
and (people, like, snow) from the following sen-
tences, respectively:
</bodyText>
<listItem confidence="0.940934875">
• “People in poor countries need food”
• “A lot of people like snow”
Second, if the object of a verb is in a verb form,
we replace it with, if any, the object of the em-
bedded verb. For example, we can extract the
triple (soldier, attack, terrorist) from the fol-
lowing sentence:
• “The soldiers continue to attack terrorists”
</listItem>
<bodyText confidence="0.971068857142857">
Third, if a term has a coordinate structure with
a conjunction like “and” or “or”, we split it into all
coordinated noun phrases and duplicate the triple
by replacing the term with each of the coordinated
noun phrases. For example, we can extract the
triples of R(girl, like, dog) and R(girl, like, cat)
from the following sentence:
</bodyText>
<listItem confidence="0.984057">
• “The girl likes both dogs and cats”
</listItem>
<bodyText confidence="0.99789">
Given two terms t1, t2 and a non-taxonomic re-
lation r, some notations which will be used here-
after are shown below:
</bodyText>
<listItem confidence="0.8553088">
• R(t1, r, t2): t1, r, and t2 have a (Subject,
Verb, Object) triple.
• O(t1, t2): the set of relations r such that there
exists R(t1, r, t2) or R(t2, r, t1).
B. Syntactic Contextual Subsumption Method
</listItem>
<bodyText confidence="0.9975195">
The idea of the SCS method derived from the
following two observations.
</bodyText>
<construct confidence="0.903577">
Observation 1. Given three terms t1, t2, t3, and a
non-taxonomic relation r, if we have two triples
R(t1, r, t3) and R(t2, r, t3) (or R(t3, r, t1) and
R(t3, r, t2)), t1 and t2 may be in taxonomic rela-
tion.
</construct>
<bodyText confidence="0.885826">
For example, given two triples R(Al-Qaeda, at-
tack, American) and R(Terrorist group, attack,
American), a taxonomic relation Terrorist group
» Al-Qaeda can be induced. However, it is not
always guaranteed to induce a taxonomic rela-
tions from such a pair of triples, for example from
R(animal, eat, meat) and R(animal, eat, grass).
The second observation introduced hereafter will
provide more chance to infer taxonomic relation-
ship.
Definition 2 (Contextual set of a term). Given
a term t1 and a non-taxonomic relation r,
S(t1, r, “subj&amp;quot;) denotes the set of terms t2 such
that there exists triple R(t1, r, t2). Similarly,
S(t1, r, “obj&amp;quot;) is the set of terms t2 such that
there exists triple R(t2, r, t1).
</bodyText>
<construct confidence="0.794048285714286">
Observation 2. Given two terms t1, t2, and a non-
taxonomic relation r, if S(t1, r, “subj&amp;quot;) mostly
contains S(t2, r, “subj&amp;quot;) but not vice versa, then
most likely t1 is a hypernym of t2. Similarly, if
S(t1, r, “obj&amp;quot;) mostly contains S(t2, r, `obj&amp;quot;) but
not vice versa, then most likely t1 is a hypernym of
t2.
</construct>
<bodyText confidence="0.9721115">
For example, assume that S(animal, eat,
“subj”) = {grass, potato, mouse, insects, meat,
wild boar, deer, buffalo} and S(tiger, eat, “subj”)
= {meat, wild boar, deer, buffalo}. Since
S(animal, eat, “subj”) properly contains S(tiger,
eat, “subj”), we can induce animal » tiger.
Based on Observation 2, our strategy to infer
taxonomic relations is to first find the contextual
set of terms via the evidence of syntactic structures
and Web data, and then compute the score of the
set inclusion. The detail of the method is presented
hereafter.
</bodyText>
<page confidence="0.957886">
813
</page>
<equation confidence="0.579313">
C(t1, t2, rl, Fl)
</equation>
<bodyText confidence="0.963117083333333">
Definition 3. Given two terms t1, t2 and a non-
taxonomic relation r, C(t1, t2, r, “subj&amp;quot;) denotes
the number of terms t3 such that there exists
both triples R(t1, r, t3) and R(t2, r, t3). Simi-
larly, C(t1, t2, r, “obj&amp;quot;) is the number of terms
t3 such that there exists both relations R(t3, r, t1)
and R(t3, r, t2).
Given the pair of a candidate general term t1
and a candidate specific term t2, we extract their
non-taxonomic relations from corpora extracted
from the Web, and use them to determine the tax-
onomic relation between t1 and t2 as follows:
</bodyText>
<listItem confidence="0.9846915">
1. Find from a domain corpus the relation r and
type F such that:
</listItem>
<equation confidence="0.981130666666667">
C(t1, t2, r, F) = max
r′EO(t1,t2)
r′E{“subj°°,“obj&amp;quot;}
</equation>
<bodyText confidence="0.712544">
2. If type F is “subj”, collect the first 1,000
search results of the query “t1 r&amp;quot; using
the Google search engine, designated as
Corpusrt1. In the same way, construct
Corpusrt2 with the query “t2 r&amp;quot;. If F is “obj”,
two queries “r t1&amp;quot; and “r t2&amp;quot; are submitted
instead to collect Corpusrt1 and Corpusrt2,
respectively.
</bodyText>
<listItem confidence="0.998922">
3. Find the sets of S(t1, r, F) and S(t2, r, F)
from Corpusrt1 and Corpusrt2, respectively,
using the non-taxonomic relation identifica-
tion method above.
4. Calculate the following evidence score for
SCS method:
</listItem>
<equation confidence="0.996645125">
[ |S(t1, r, F) n S(t2, r, F)|
ScoreSCS = +
 |S(t2, r, F) |
)]
(1 − |S(t1, r, F) n S(t2, r, F)|
|S(t1, r, F)|
× log(|S(t1,r, F) |+ |S(t2,r, F)|)
(4)
</equation>
<bodyText confidence="0.999813125">
The basic idea of the contextual subsumption
score in our method is that if t1 is a hyper-
nym of t2 then the set S(t1, r, F) will mostly
contain S(t2, r, F) but not vice versa. The in-
tuition of formula (5) is inspired by Jaccard
similarity coefficient. We then multiply the
score with the log value of total size of two
sets to avoid the bias of small set inclusion.
</bodyText>
<listItem confidence="0.9928745">
5. If ScoreSCS(t1, t2) is greater than a thresh-
old value, then we have t1 ≫ t2.
</listItem>
<subsubsectionHeader confidence="0.799238">
2.2.4 Combined Method
</subsubsectionHeader>
<bodyText confidence="0.874704">
In our study, we linearly combine three methods
as follows:
</bodyText>
<listItem confidence="0.8031415">
1. For each ordered pair of terms (t1, t2) calcu-
late the total evidence score:
</listItem>
<equation confidence="0.8586925">
Score(t1, t2) = α × ScoreSIWN(t1, t2)
+ β × ScoreLSP(t1, t2)
+ γ × ScoreSCS(t1, t2)
(5)
</equation>
<listItem confidence="0.8446215">
2. If Score(t1, t2) is greater than a threshold
value, then we have t1 ≫ t2.
</listItem>
<bodyText confidence="0.999797666666667">
We experimented with various combinations of
values for α, β and γ, and found that the method
shows the best performance when the value of α is
0.5, β is between 0.35 and 0.45, and γ is between
0.15 and 0.25, depending on the domain corpus
size.
</bodyText>
<subsectionHeader confidence="0.990496">
2.3 Taxonomy Induction
</subsectionHeader>
<bodyText confidence="0.998595777777778">
The output of the taxonomic relation identifica-
tion module is a set of taxonomic relations T.
In this section, we will introduce a graph-based
algorithm (Algorithm 1) to convert this set into
an optimal tree-structured taxonomy, as well as
to eliminate incorrect and redundant relations.
Denote e(t1, t2) as an directed edge from t1 to t2,
the algorithm consists of three steps which will be
described hereafter with the corresponding lines
in Algorithm 1.
Step 1: Initial hypernym graph creation
(line 1 - 16) This step is to construct a connected
directed graph from the list of taxonomic rela-
tions. The idea is to add each taxonomic relation
t1 ≫ t2 as a directed edge from parent node
t1 to child node t2, and if t1 does not have any
hypernym term, t1 will become a child node of
ROOT node. The result of this step is a con-
nected graph containing all taxonomic relations
with the common ROOT node.
Step 2: Edge weighting (line 17) This step
is to calculate the weight of each edge in the
hypernym graph. Unlike the algorithm of Velardi
et al. (2012) and Kozareva and Hovy (2010)
where every taxonomic relation is treated equally,
we assume the confidence of each taxonomic
relation is different, depending on the amount of
</bodyText>
<page confidence="0.994657">
814
</page>
<bodyText confidence="0.436436">
Algorithm 1 Taxonomy Induction Algorithm
Input: T : the taxonomic relation set
Output: V : the vertex set of resultant taxonomy;
E: the edge set of resultant taxonomy;
</bodyText>
<listItem confidence="0.89688295">
1: Initialize V = {ROOT}, E = ∅;
2: for each taxonomic relation (t1 ≫ t2) E T do
3: E = E U {e(t1, t2)}
4: if t1 E̸ V then
5: V = V U {t1}
6: end if
7: if t2 E̸ V then
8: V=VU{t2}
9: end if
10: if ∄ e(t3, t1) E E with t3 ≠ ROOT then
11: E = E U {e(ROOT, t1)}
12: end if
13: if I e(ROOT, t2) E E then
14: E = E \ {e(ROOT, t2)}
15: end if
16: end for
17: edgeWeighting(V, E);
18: graphPruning(V, E);
evidence it has. Thus, the hypernym graph edges
will be weighted as follows:
</listItem>
<equation confidence="0.991282">
{ 1 if t1 = ROOT
w(e(t1, t2)) =
Score(t1, t2) otherwise
(6)
</equation>
<bodyText confidence="0.9999564">
Note that the Score value in formula (6) is de-
termined by the taxonomic relation identification
process described in Section 2.2.4.
Step 3: Graph pruning (line 18) The hy-
pernym graph generated in Step 1 is not an
optimal taxonomy as it may contain many redun-
dant edges or incorrect edges which together form
in a loop. In this step, we aim at producing an
optimal taxonomy by pruning the graph based
on our edge weighting strategy. A maximum
spanning tree algorithm, however, cannot be
applied as the graph is directed. For this purpose,
we apply Edmonds’ algorithm (Edmonds, 1967)
for finding a maximum optimum branching of a
weighted directed graph. Using this algorithm,
we can find a subset of the current edge set, which
is the optimized taxonomy where every non-root
node has in-degree 1 and the sum of the edge
weights is maximized. Figure 1 shows an example
of the taxonomy induction process.
</bodyText>
<sectionHeader confidence="0.990968" genericHeader="method">
3 Experiment Results
</sectionHeader>
<bodyText confidence="0.997547333333333">
We evaluated our methods for taxonomy construc-
tion against the following text collections of five
domains:
</bodyText>
<listItem confidence="0.9428354375">
• Artificial Intelligence (AI) domain: 4,119 pa-
pers extracted from the IJCAI proceedings
from 1969 to 2011 and the ACL archives
from year 1979 to 2010. The same dataset
used in the work of Velardi et al. (2012).
• Terrorism domain: 104 reports of the US
state department, titled “Patterns of Global
Terrorism (1991-2002)” 1. A report contains
about 1,500 words.
• Animals, Plants and Vehicles domains: Col-
lections of Web pages crawled by using
the bootstrapping algorithm described by
Kozareva et al. (2008). Navigli et al. (2011)
and Kozareva and Hovy (2010) used these
datasets to compare their outputs against
WordNet sub-hierarchies.
</listItem>
<bodyText confidence="0.999776571428571">
There are two experiments performed in this sec-
tion: 1) Evaluating the construction of new tax-
onomies for Terrorism and AI domains, and 2)
Comparing our results with the gold-standard
WordNet sub-hierarchies. Note that in the experi-
ments, the threshold value we used for ScoreLSP
is 1.9, ScoreSCS is 1.5 and Score is 2.1.
</bodyText>
<subsectionHeader confidence="0.9943235">
3.1 Constructing new taxonomies for AI and
Terrorism domains
</subsectionHeader>
<bodyText confidence="0.999869470588235">
Referential taxonomy structures such as WordNet
or OpenCyc are widely used in semantic analyt-
ics applications. However, their coverage is lim-
ited to common well-known areas, and many spe-
cific domains like Terrorism and AI are not well
covered in those structures. Therefore, an auto-
matic method which can induce taxonomies for
those specific domains from scratch can greatly
contribute to the process of knowledge discovery.
First, we applied our taxonomy construction
system to the AI domain corpus. We compared
the taxonomy constructed by our system with that
obtained by Velardi et al. (2012), and show the
comparison results in Table 2. Notice that in this
comparison, to be fair, we use the same set of
terms that was used in (Velardi et al., 2012). The
result shows that our approach can extract 9.8%
</bodyText>
<footnote confidence="0.989808">
1http://www.fas.org/irp/threat/terror.htm
</footnote>
<page confidence="0.997282">
815
</page>
<figureCaption confidence="0.7212742">
Figure 1: An example of taxonomy induction. (a) Initial weighted hypernym graph. (b) Final optimal
taxonomy, where we prune two redundant edges (group, International terrorist organization), (Militant
group, Hezbollah) and remove the loop by cutting an incorrect edge (Al-Qaeda, Terrorist organization).
more taxonomic relations and achieve 7% better
term coverage than Velardi’s approach.
</figureCaption>
<table confidence="0.999722666666667">
Our system Velardi’s system
#vertex 1839 1675
#edge 1838 1674
Average depth 6.2 6
Max depth 10 10
Term coverage 83% 76%
</table>
<tableCaption confidence="0.9018035">
Table 2: Comparison of our system with (Velardi
et al., 2012)
</tableCaption>
<bodyText confidence="0.999883285714286">
We also applied our system to the Terrorism
corpus. The proposed taxonomic relation identifi-
cation algorithm extracts a total of 976 taxonomic
relations, from which the taxonomy induction al-
gorithm builds the optimal taxonomy. The total
number of vertices in the taxonomy is 281, and the
total number of edges is 280. The average depth
of the trees is 3.1, with the maximum depth 6. In
addition, term coverage (the ratio of the number
of terms in the final optimal trees to the number
of terms obtained by the term suggestion/filtering
method) is 85%.
To judge the contribution of each of taxonomic
relation identification methods described in Sec-
tion 2.2 to the overall system, we alternately run
the system for the AI and Terrorism domains with
different combinations of the three methods (i.e.
SIWN, LSP, and SCS) as shown in Table 3. Note
that we employed only the first two modules of
term suggestion/filtering and taxonomic relation
identification except the last module of taxonomy
</bodyText>
<table confidence="0.999728714285714">
No. of extracted relations
Terrorism AI domain
SCS 484 1308
SIWN 301 984
LSP 527 1537
SIWN + LSP 711 2203
SCS + SIWN + LSP 976 3122
</table>
<tableCaption confidence="0.9742755">
Table 3: The number of taxonomic relations ex-
tracted by different methods.
</tableCaption>
<bodyText confidence="0.99801525">
induction for this experiment. Table 3 shows the
number of the taxonomic relations extracted by
each of the combinations. Since SIWN and LSP
are commonly used by previous taxonomic rela-
tion identification systems, we consider the com-
bination of SIWN + LSP as the baseline of the
experiment. The results in Table 3 show that the
three methods are all well complementary to each
other. In addition, the proposed SCS method can
contribute up to about 27% - 29% of all the iden-
tified taxonomic relations, which were not discov-
ered by the other two baseline methods.
</bodyText>
<table confidence="0.999692666666667">
Percentage of correct relations
Terrorism AI domain
SCS 91% 88%
SIWN 96% 91%
LSP 93% 93%
SCS + SIWN + LSP 92% 90%
</table>
<tableCaption confidence="0.9952655">
Table 4: Estimated precision of taxonomic relation
identification methods in 100 extracted relations.
</tableCaption>
<page confidence="0.958142">
816
</page>
<table confidence="0.999782857142857">
Animals domain Plants domain Vehicles domain
Our Kozareva Navigli Our Kozareva Navigli Our Kozareva Navigli
#Correct relations 2427 1643 N.A. 1243 905 N.A. 281 246 N.A.
Term coverage 96% N.A. 94% 98% N.A. 97% 97% N.A. 96%
Precision 95% 98% 97% 95% 97% 97% 93% 99% 91%
Recall 56% 38% 44% 53% 39% 38% 69% 60% 49%
F-measure 71% 55% 61% 68% 56% 55% 79% 75% 64%
</table>
<tableCaption confidence="0.958627">
Table 5: Comparison of (Navigli et al., 2011), (Kozareva and Hovy, 2010) and our system against Word-
Net in three domains: Animals, Plants and Vehicles.
</tableCaption>
<bodyText confidence="0.999991857142857">
We further evaluated the precision of each in-
dividual taxonomic relation identification method.
For AI and Terrorism domains, we again run the
system with each of the three methods and with all
together, and then randomly select 100 extracted
taxonomic relations each time. These selected tax-
onomic relations are then examined by two do-
main experts to check the correctness. The evalua-
tion results are given in Table 4. Note that only the
first two modules of term suggestion/filtering and
taxonomic relation identification are employed for
this experiment as well. The SIWN and LSP meth-
ods achieve high precision because they are based
on the gold-standard taxonomy hierarchy Word-
Net and on the well-defined patterns, respectively.
In contrast, the SCS method ambitiously looks
for terms pairs that share similar syntactic con-
texts across sentences, though the contextual ev-
idence is restricted to certain syntactic structures,
and thus has a slightly lower precision compared
to the other two methods.
In short, the SCS method is complementary to
the baseline methods, significantly improving the
coverage of the combined methods, when its pre-
cision is comparable to those of the baseline meth-
ods. We performed next experiments to show that
the SCS method overall has synergistic impact to
improve the F-measure of the combined methods.
</bodyText>
<subsectionHeader confidence="0.99965">
3.2 Evaluation against WordNet
</subsectionHeader>
<bodyText confidence="0.998957416666667">
In this experiment, we constructed taxonomies
for three domains Animals, Plants and Vehicles,
and then checked whether the identified relations
can be found in the WordNet, and which relations
in WordNet are not found by our method. Note
that in this comparison, to be fair, we changed our
algorithm to avoid using WordNet in identifying
taxonomic relations. Specifically, in the SIWN
algorithm, all operations of “AWN” are replaced
with normal string-matching comparison, and all
“≫WN” relations are falsified. The evaluation
uses the following measures:
</bodyText>
<subsubsectionHeader confidence="0.7636375">
Precision — #relations found in WordNet and by the method
#relations found by the method
Recall —_ #relations found in WordNet and by the method
#relations found in WordNet
</subsubsectionHeader>
<bodyText confidence="0.99998003030303">
We also compared our results with those ob-
tained by the approaches of Navigli et al. (2011)
and Kozareva and Hovy (2010), where they
also compared their resultant taxonomies against
WordNet. In this comparison, all the three ap-
proaches (i.e. ours, the two previous methods)
use the same corpora and term lists. The com-
parison results are given in Table 5. “N.A.”
value means that this parameter is not applicable to
the corresponding method. The results show that
our approach achieves better performance than the
other two approaches, in terms of both the num-
ber of correctly extracted taxonomic relations and
the term coverage. Our system has a slightly
lower precision than that of (Navigli et al., 2011)
and (Kozareva and Hovy, 2010) due to the SCS
method, but it significantly contributes to improve
the recall and eventually the F-measure over the
other two systems.
To judge the effectiveness of our proposed tax-
onomy induction algorithm described in Section
2.3, we compared it with the graph-based algo-
rithm of Velardi et al. (2012). Recall that in this al-
gorithm, they treat all taxonomic relations equally,
and the pruning task is reduced to finding the best
trade-off between path length and the connectiv-
ity of traversed nodes. For each of five domains
(i.e. Terrorism, AI, Animals, Plants and Vehicles),
we alternately run the two taxonomy induction
algorithms over the same taxonomic relation set
produced by our taxonomic relation identification
process. For Terrorism and AI domains, we ran-
domly pick up 100 edges in each resultant taxon-
</bodyText>
<page confidence="0.988203">
817
</page>
<bodyText confidence="0.999780714285714">
omy and ask two domain experts to judge for the
correctness. For Animals, Plants and Vehicles do-
mains, we check the correctness of the edges in re-
sultant taxonomies by comparing them against the
corresponding sub-hierarchies in WordNet. The
evaluation is given in Table 6. The results show
that the proposed taxonomy induction algorithm
can achieve better performance than the algorithm
of Velardi et al. (2012). This may be due to the fact
that our algorithm considers the scores of the iden-
tified taxonomic relations from the relation identi-
fication module, and thus is more precise in elim-
inating incorrect relations during the pruning pro-
cess.
</bodyText>
<table confidence="0.999738">
Percentage of correct edges
Our algorithm Velardi’s algorithm
Terrorism 94% 90%
AI 93% 88%
Animals 95% 93%
Plants 95% 92%
Vehicles 93% 92%
</table>
<tableCaption confidence="0.9727225">
Table 6: Comparison of our taxonomy induction
algorithms and that of Velardi et al. (2012).
</tableCaption>
<bodyText confidence="0.999907">
In addition, when comparing Tables 4 and 6, we
can find that the precision of taxonomic relations
after the pruning process is higher than that before
the pruning process, which proves that the pro-
posed taxonomy induction algorithm effectively
trims the incorrect relations of Terrorism and AI
taxonomies, leveraging the percentage of correct
relations 2% - 3% up.
For the SCS method, besides the triple Subject-
Verb-Object, we also explore other syntactic
structures like Noun-Preposition-Noun and Noun-
Adjective-Noun. For example, from the sentence
“I visited Microsoft in Washington”, the triple
(Microsoft, in, Washington) is extracted using
Noun-Preposition-Noun structure. Similarly, from
the sentence “Washington is a beautiful city”, the
triple (Washington, beautiful, city) is extracted us-
ing Noun-Adjective-Noun structure. We then use
the triples for the contextual subsumption method
described in Section 2.2.3, and test the method
against the Animals, Plants and Vehicles domains.
The results are then compared against WordNet
sub hierarchies. The experiment results in Table
7 show that the triples of Subject-Verb-Object give
the best performance compared to the other syn-
tactic structures. These can be explained as the
</bodyText>
<table confidence="0.998620615384616">
S-V-O N-P-N N-A-N
Animals domain
Precision 95% 68% 72%
Recall 56% 52% 47%
F-measure 71% 59% 57%
Plants domain
Precision 95% 63% 66%
Recall 53% 41% 43%
F-measure 68% 50% 52%
Vehicles domain
Precision 93% 59% 60%
Recall 69% 45% 48%
F-measure 79% 51% 53%
</table>
<tableCaption confidence="0.7723865">
Table 7: Comparison of three syntactic struc-
tures: S-V-O (Subject-Verb-Object), N-P-N
</tableCaption>
<bodyText confidence="0.965035777777778">
(Noun-Preposition-Noun) and N-A-N (Noun-
Adjective-Noun).
number of triples of two types Noun-Preposition-
Noun and Noun-Adjective-Noun are smaller than
that of Subject-Verb-Object, and the number of
Verb is much greater than number of Preposition
or Adjective.
All experiment results are available at
http://nlp.sce.ntu.edu.sg/wiki/projects/taxogen.
</bodyText>
<sectionHeader confidence="0.999375" genericHeader="conclusions">
4 Conclusion
</sectionHeader>
<bodyText confidence="0.999985214285714">
In this paper, we proposed a novel method of iden-
tifying taxonomic relations using contextual evi-
dence from syntactic structure and Web data. This
method is proved well complementary with pre-
vious method of linguistic pattern matching. We
also present a novel graph-based algorithm to in-
duce an optimal taxonomy from a given taxo-
nomic relation set. The experiment results show
that our system can generally achieve better per-
formance than the state-of-the-art methods. In
the future, we will apply the proposed taxon-
omy construction method to other domains such
as biomedicine and integrate it into other frame-
works such as ontology authoring.
</bodyText>
<sectionHeader confidence="0.999472" genericHeader="references">
References
</sectionHeader>
<reference confidence="0.9985055">
K. Bollacker, C. Evans, P. Paritosh, T. Sturge and J.
Taylor. 2008. Freebase: a collaboratively created
graph database for structuring human knowledge.
In proceedings of the ACM SIGMOD International
Conference on Management of Data, pp. 1247-1250.
A. Budanitsky. 1999. Lexical semantic relatedness
</reference>
<page confidence="0.980969">
818
</page>
<reference confidence="0.999281531914894">
and its application in natural language process-
ing. Technical Report CSRG-390, Computer Sys-
tems Research Group, University of Toronto.
P. Buitelaar, D. Olejnik and M. Sintek. 2004. A
Prot´eg´e Plug-in for Ontology Extraction from Text
Based on Linguistic Analysis. In proceedings of the
1st European Semantic Web Symposium, pp. 31-44.
M. Ciaramita, A. Gangemi, E. Ratsch, J. Saric and I.
Rojas. 2005. Unsupervised Learning of Semantic
Relations Between Concepts of a Molecular Biology
Ontology. In proceedings of the 19th International
Joint Conference on Artificial Intelligence, pp. 659-
664.
J. Edmonds. 1967. Optimum branchings. Journal of
Research of the National Bureau of Standards, 71,
pp. 233-240.
S. Fodeh, B. Punch and P. N. Tan. 2011. On Ontology-
driven Document Clustering Using Core Semantic
Features. Knowledge and information systems,
28(2), pp. 395-421.
H. N. Fotzo and P. Gallinari. 2004. Learning “Gen-
eralization/Specialization” Relations between Con-
cepts - Application for Automatically Building The-
matic Document Hierarchies. In proceedings of the
7th International Conference on Computer-Assisted
Information Retrieval.
M. Geffet and I. Dagan. 2005. The Distributional In-
clusion Hypotheses and Lexical Entailment. In pro-
ceedings of the 43rd Annual Meeting of the ACL,
pp. 107-114.
R. Girju, A. Badulescu, and D. Moldovan. 2003.
Learning Semantic Constraints for the Automatic
Discovery of Part-Whole Relations. In proceedings
of the NAACL, pp. 1-8.
S. M. Harabagiu, S. J. Maiorano and M. A. Pasca.
2003. Open-Domain Textual Question Answering
Techniques. Natural Language Engineering, 9(3):
pp. 1-38.
M. A. Hearst. 1992. Automatic Acquisition of Hy-
ponyms from Large Text Corpora. In proceedings
of the 14th Conference on Computational Linguis-
tics, pp. 539-545.
D. Klein and C. D. Manning. 2003. Accurate Unlexi-
calized Parsing. In proceedings of the 41st Annual
Meeting of the ACL, pp. 423-430.
Z. Kozareva, E. Riloff, and E. H. Hovy. 2008. Se-
mantic Class Learning from the Web with Hyponym
Pattern Linkage Graphs. In proceedings of the 46th
Annual Meeting of the ACL, pp. 1048-1056.
Z. Kozareva and E. Hovy. 2010. A Semi-supervised
Method to Learn and Construct Taxonomies Using
the Web. In proceedings of the Conference on Em-
pirical Methods in Natural Language Processing, pp.
1110-1118.
C. Matuszek, J. Cabral, M. J. Witbrock and J. DeO-
liveira. 2006. An Introduction to the Syntax and
Content of Cyc. In proceedings of the AAAI Spring
Symposium: Formalizing and Compiling Back-
ground Knowledge and Its Applications to Knowl-
edge Representation and Question Answering, pp.
44-49.
G. A. Miller. 1995. WordNet: a Lexical Database for
English. Communications of the ACM, 38(11), pp.
39-41.
R. Navigli and P. Velardi, 2004. Learning Domain
Ontologies from Document Warehouses and Dedi-
cated Web Sites. Computational Linguistics, 30(2),
pp. 151-179.
R. Navigli, P. Velardi and S. Faralli. 2011. A Graph-
based Algorithm for Inducing Lexical Taxonomies
from Scratch. In proceedings of the 20th Interna-
tional Joint Conference on Artificial Intelligence,
pp. 1872-1877.
R. Snow, D. Jurafsky and A. Y. Ng. 2006. Semantic
Taxonomy Induction from Heterogenous Evidence.
In proceedings of the 21st International Conference
on Computational Linguistics, pp. 801-808.
P. Velardi, S. Faralli and R. Navigli. 2012. Ontolearn
Reloaded: A Graph-based Algorithm for Taxonomy
Induction. Computational Linguistics, 39(3), pp.
665-707.
W. Wentao, L. Hongsong, W. Haixun, and Q. Zhu.
2012. Probase: A probabilistic taxonomy for text
understanding. In proceedings of the ACM SIG-
MOD International Conference on Management of
Data, pp. 481-492.
D. Widdows and B. Dorow. 2002. A Graph Model for
Unsupervised Lexical Acquisition. In proceedings
of the 19th International Conference on Computa-
tional Linguistics, pp. 1-7.
W. Wong, W. Liu and M. Bennamoun. 2007. Tree-
traversing ant algorithm for term clustering based
on featureless similarities. Data Mining and Knowl-
edge Discovery, 15(3), pp. 349-381.
</reference>
<page confidence="0.99892">
819
</page>
</variant>
</algorithm>
<algorithm name="ParsHed" version="110505">
<variant no="0" confidence="0.535145">
<title confidence="0.994056">Taxonomy Construction Using Syntactic Contextual Evidence</title>
<author confidence="0.997127">Anh Tuan Jung-jae Kim Ng See Kiong</author>
<affiliation confidence="0.8930665">of Computer Engineering, Nanyang Technological University, for Infocomm Research, Agency for Science, Technology and Research,</affiliation>
<abstract confidence="0.987740230769231">Taxonomies are the backbone of many structured, semantic knowledge resources. Recent works for extracting taxonomic relations from text focused on collecting lexical-syntactic patterns to extract the taxonomic relations by matching the patterns to text. These approaches, however, often show low coverage due to the lack of contextual analysis across sentences. To address this issue, we propose a novel approach that collectively utilizes contextual information of terms in syntactic structures such that if the set of contexts of a term includes most of contexts of another term, a subsumption relation between the two terms is inferred. We apply this method to the task of taxonomy construction from scratch, where we introduce another novel graph-based algorithm for taxonomic structure induction. Our experiment results show that the proposed method is well complementary with previous methods of linguistic pattern matching and significantly improves recall and thus F-measure.</abstract>
</variant>
</algorithm>
<algorithm name="ParsCit" version="110505">
<citationList>
<citation valid="true">
<authors>
<author>K Bollacker</author>
<author>C Evans</author>
<author>P Paritosh</author>
<author>T Sturge</author>
<author>J Taylor</author>
</authors>
<title>Freebase: a collaboratively created graph database for structuring human knowledge.</title>
<date>2008</date>
<booktitle>In proceedings of the ACM SIGMOD International Conference on Management of Data,</booktitle>
<pages>1247--1250</pages>
<contexts>
<context position="1810" citStr="Bollacker et al., 2008" startWordPosition="260" endWordPosition="263">that the proposed method is well complementary with previous methods of linguistic pattern matching and significantly improves recall and thus F-measure. 1 Introduction Taxonomies that are backbone of structured ontology knowledge have been found to be useful for many areas such as question answering (Harabagiu et al., 2003), document clustering (Fodeh et al., 2011) and textual entailment (Geffet and Dagan, 2005). There have been an increasing number of hand-crafted, well-structured taxonomies publicly available, including WordNet (Miller, 1995), OpenCyc (Matuszek et al., 2006), and Freebase (Bollacker et al., 2008). However, the manual curation of those taxonomies is timeconsuming and human experts may miss relevant terms. As such, there are still needs to extend existing taxonomies or even to construct new taxonomies from scratch. The previous methods for identifying taxonomic relations (i.e. is-a relations) from text can be generally classified into two categories: statistical and linguistic approaches. The former includes co-occurrence analysis (Budanitsky, 1999), term subsumption (Fotzo and Gallinari, 2004) and clustering (Wong et al., 2007). The main idea behinds these techniques is that the terms </context>
</contexts>
<marker>Bollacker, Evans, Paritosh, Sturge, Taylor, 2008</marker>
<rawString>K. Bollacker, C. Evans, P. Paritosh, T. Sturge and J. Taylor. 2008. Freebase: a collaboratively created graph database for structuring human knowledge. In proceedings of the ACM SIGMOD International Conference on Management of Data, pp. 1247-1250.</rawString>
</citation>
<citation valid="true">
<authors>
<author>A Budanitsky</author>
</authors>
<title>Lexical semantic relatedness and its application in natural language processing.</title>
<date>1999</date>
<tech>Technical Report CSRG-390,</tech>
<institution>Computer Systems Research Group, University of Toronto.</institution>
<contexts>
<context position="2270" citStr="Budanitsky, 1999" startWordPosition="332" endWordPosition="333">fted, well-structured taxonomies publicly available, including WordNet (Miller, 1995), OpenCyc (Matuszek et al., 2006), and Freebase (Bollacker et al., 2008). However, the manual curation of those taxonomies is timeconsuming and human experts may miss relevant terms. As such, there are still needs to extend existing taxonomies or even to construct new taxonomies from scratch. The previous methods for identifying taxonomic relations (i.e. is-a relations) from text can be generally classified into two categories: statistical and linguistic approaches. The former includes co-occurrence analysis (Budanitsky, 1999), term subsumption (Fotzo and Gallinari, 2004) and clustering (Wong et al., 2007). The main idea behinds these techniques is that the terms that frequently co-occur may have taxonomic relationships. Such approaches, however, usually suffer from low accuracy, though relatively high coverage, and heavily depend on the choice of feature types and datasets. Most previous methods of the linguistic approach, on the other hand, rely on the lexical-syntactic patterns (e.g. A is a B, A such as B) (Hearst, 1992). Those patterns can be manually created (Kozareva et al., 2008; Wentao et al., 2012), chosen</context>
</contexts>
<marker>Budanitsky, 1999</marker>
<rawString>A. Budanitsky. 1999. Lexical semantic relatedness and its application in natural language processing. Technical Report CSRG-390, Computer Systems Research Group, University of Toronto.</rawString>
</citation>
<citation valid="true">
<authors>
<author>P Buitelaar</author>
<author>D Olejnik</author>
<author>M Sintek</author>
</authors>
<title>A Prot´eg´e Plug-in for Ontology Extraction from Text Based on Linguistic Analysis.</title>
<date>2004</date>
<booktitle>In proceedings of the 1st European Semantic Web Symposium,</booktitle>
<pages>31--44</pages>
<contexts>
<context position="12455" citStr="Buitelaar et al., 2004" startWordPosition="2008" endWordPosition="2011">nces containing those pre-defined syntactic patterns. This method, however, has a major shortcoming: it cannot derive taxonomic relations between two terms occurring in two different sentences. We thus propose a novel syntactic contextual subsumption (SCS) method which utilizes contextual information of terms in syntactic structure (i.e. Subject-Verb-Object in this study) and Web data to infer implicit taxonomic relations 812 between terms across sentences. Note that the chosen syntactic structure Subject-Verb-Object is identical to the definition of non-taxonomic relations in the literature (Buitelaar et al., 2004), where the Verb indicate non-taxonomic relations between Subject and Object. In this subsection, we first present the method to collect those non-taxonomic relations. Then we present in detail the ideas of the SCS method and how we can use it to derive taxonomic relations in practice. A. Non-taxonomic Relation Identification Following previous approaches to nontaxonomic relation identification, e.g. (Ciaramita et al., 2005), we use the Stanford parser (Klein and Manning, 2003) to identify the syntactic structures of sentences and extract triples of (Subject, Verb, Object), where Subject and O</context>
</contexts>
<marker>Buitelaar, Olejnik, Sintek, 2004</marker>
<rawString>P. Buitelaar, D. Olejnik and M. Sintek. 2004. A Prot´eg´e Plug-in for Ontology Extraction from Text Based on Linguistic Analysis. In proceedings of the 1st European Semantic Web Symposium, pp. 31-44.</rawString>
</citation>
<citation valid="true">
<authors>
<author>M Ciaramita</author>
<author>A Gangemi</author>
<author>E Ratsch</author>
<author>J Saric</author>
<author>I Rojas</author>
</authors>
<title>Unsupervised Learning of Semantic Relations Between Concepts of a Molecular Biology Ontology.</title>
<date>2005</date>
<booktitle>In proceedings of the 19th International Joint Conference on Artificial Intelligence,</booktitle>
<pages>659--664</pages>
<contexts>
<context position="12883" citStr="Ciaramita et al., 2005" startWordPosition="2071" endWordPosition="2074">2 between terms across sentences. Note that the chosen syntactic structure Subject-Verb-Object is identical to the definition of non-taxonomic relations in the literature (Buitelaar et al., 2004), where the Verb indicate non-taxonomic relations between Subject and Object. In this subsection, we first present the method to collect those non-taxonomic relations. Then we present in detail the ideas of the SCS method and how we can use it to derive taxonomic relations in practice. A. Non-taxonomic Relation Identification Following previous approaches to nontaxonomic relation identification, e.g. (Ciaramita et al., 2005), we use the Stanford parser (Klein and Manning, 2003) to identify the syntactic structures of sentences and extract triples of (Subject, Verb, Object), where Subject and Object are noun phrases. We further consider the following issues: First, if a term (or noun phrase) includes a preposition, we remove the prepositional phrase. However, if the headword of a term is a quantitative noun like “lot”, “many” or “dozen” and it is modified by the preposition “of”, we replace it with the headword of the object of the preposition “of”. For example, we can extract the triples (people, need, food) and </context>
</contexts>
<marker>Ciaramita, Gangemi, Ratsch, Saric, Rojas, 2005</marker>
<rawString>M. Ciaramita, A. Gangemi, E. Ratsch, J. Saric and I. Rojas. 2005. Unsupervised Learning of Semantic Relations Between Concepts of a Molecular Biology Ontology. In proceedings of the 19th International Joint Conference on Artificial Intelligence, pp. 659-664.</rawString>
</citation>
<citation valid="true">
<authors>
<author>J Edmonds</author>
</authors>
<title>Optimum branchings.</title>
<date>1967</date>
<journal>Journal of Research of the National Bureau of Standards,</journal>
<volume>71</volume>
<pages>233--240</pages>
<contexts>
<context position="21314" citStr="Edmonds, 1967" startWordPosition="3609" endWordPosition="3610">)) = Score(t1, t2) otherwise (6) Note that the Score value in formula (6) is determined by the taxonomic relation identification process described in Section 2.2.4. Step 3: Graph pruning (line 18) The hypernym graph generated in Step 1 is not an optimal taxonomy as it may contain many redundant edges or incorrect edges which together form in a loop. In this step, we aim at producing an optimal taxonomy by pruning the graph based on our edge weighting strategy. A maximum spanning tree algorithm, however, cannot be applied as the graph is directed. For this purpose, we apply Edmonds’ algorithm (Edmonds, 1967) for finding a maximum optimum branching of a weighted directed graph. Using this algorithm, we can find a subset of the current edge set, which is the optimized taxonomy where every non-root node has in-degree 1 and the sum of the edge weights is maximized. Figure 1 shows an example of the taxonomy induction process. 3 Experiment Results We evaluated our methods for taxonomy construction against the following text collections of five domains: • Artificial Intelligence (AI) domain: 4,119 papers extracted from the IJCAI proceedings from 1969 to 2011 and the ACL archives from year 1979 to 2010. </context>
</contexts>
<marker>Edmonds, 1967</marker>
<rawString>J. Edmonds. 1967. Optimum branchings. Journal of Research of the National Bureau of Standards, 71, pp. 233-240.</rawString>
</citation>
<citation valid="true">
<authors>
<author>S Fodeh</author>
<author>B Punch</author>
<author>P N Tan</author>
</authors>
<title>On Ontologydriven Document Clustering Using Core Semantic Features. Knowledge and information systems,</title>
<date>2011</date>
<volume>28</volume>
<issue>2</issue>
<pages>395--421</pages>
<contexts>
<context position="1555" citStr="Fodeh et al., 2011" startWordPosition="224" endWordPosition="227">m, a subsumption relation between the two terms is inferred. We apply this method to the task of taxonomy construction from scratch, where we introduce another novel graph-based algorithm for taxonomic structure induction. Our experiment results show that the proposed method is well complementary with previous methods of linguistic pattern matching and significantly improves recall and thus F-measure. 1 Introduction Taxonomies that are backbone of structured ontology knowledge have been found to be useful for many areas such as question answering (Harabagiu et al., 2003), document clustering (Fodeh et al., 2011) and textual entailment (Geffet and Dagan, 2005). There have been an increasing number of hand-crafted, well-structured taxonomies publicly available, including WordNet (Miller, 1995), OpenCyc (Matuszek et al., 2006), and Freebase (Bollacker et al., 2008). However, the manual curation of those taxonomies is timeconsuming and human experts may miss relevant terms. As such, there are still needs to extend existing taxonomies or even to construct new taxonomies from scratch. The previous methods for identifying taxonomic relations (i.e. is-a relations) from text can be generally classified into t</context>
</contexts>
<marker>Fodeh, Punch, Tan, 2011</marker>
<rawString>S. Fodeh, B. Punch and P. N. Tan. 2011. On Ontologydriven Document Clustering Using Core Semantic Features. Knowledge and information systems, 28(2), pp. 395-421.</rawString>
</citation>
<citation valid="true">
<authors>
<author>H N Fotzo</author>
<author>P Gallinari</author>
</authors>
<title>Learning “Generalization/Specialization” Relations between Concepts - Application for Automatically Building Thematic Document Hierarchies.</title>
<date>2004</date>
<booktitle>In proceedings of the 7th International Conference on Computer-Assisted Information Retrieval.</booktitle>
<contexts>
<context position="2316" citStr="Fotzo and Gallinari, 2004" startWordPosition="336" endWordPosition="339">icly available, including WordNet (Miller, 1995), OpenCyc (Matuszek et al., 2006), and Freebase (Bollacker et al., 2008). However, the manual curation of those taxonomies is timeconsuming and human experts may miss relevant terms. As such, there are still needs to extend existing taxonomies or even to construct new taxonomies from scratch. The previous methods for identifying taxonomic relations (i.e. is-a relations) from text can be generally classified into two categories: statistical and linguistic approaches. The former includes co-occurrence analysis (Budanitsky, 1999), term subsumption (Fotzo and Gallinari, 2004) and clustering (Wong et al., 2007). The main idea behinds these techniques is that the terms that frequently co-occur may have taxonomic relationships. Such approaches, however, usually suffer from low accuracy, though relatively high coverage, and heavily depend on the choice of feature types and datasets. Most previous methods of the linguistic approach, on the other hand, rely on the lexical-syntactic patterns (e.g. A is a B, A such as B) (Hearst, 1992). Those patterns can be manually created (Kozareva et al., 2008; Wentao et al., 2012), chosen via automatic bootstrapping (Widdows and Doro</context>
</contexts>
<marker>Fotzo, Gallinari, 2004</marker>
<rawString>H. N. Fotzo and P. Gallinari. 2004. Learning “Generalization/Specialization” Relations between Concepts - Application for Automatically Building Thematic Document Hierarchies. In proceedings of the 7th International Conference on Computer-Assisted Information Retrieval.</rawString>
</citation>
<citation valid="true">
<authors>
<author>M Geffet</author>
<author>I Dagan</author>
</authors>
<title>The Distributional Inclusion Hypotheses and Lexical Entailment.</title>
<date>2005</date>
<booktitle>In proceedings of the 43rd Annual Meeting of the ACL,</booktitle>
<pages>107--114</pages>
<contexts>
<context position="1603" citStr="Geffet and Dagan, 2005" startWordPosition="231" endWordPosition="235">rms is inferred. We apply this method to the task of taxonomy construction from scratch, where we introduce another novel graph-based algorithm for taxonomic structure induction. Our experiment results show that the proposed method is well complementary with previous methods of linguistic pattern matching and significantly improves recall and thus F-measure. 1 Introduction Taxonomies that are backbone of structured ontology knowledge have been found to be useful for many areas such as question answering (Harabagiu et al., 2003), document clustering (Fodeh et al., 2011) and textual entailment (Geffet and Dagan, 2005). There have been an increasing number of hand-crafted, well-structured taxonomies publicly available, including WordNet (Miller, 1995), OpenCyc (Matuszek et al., 2006), and Freebase (Bollacker et al., 2008). However, the manual curation of those taxonomies is timeconsuming and human experts may miss relevant terms. As such, there are still needs to extend existing taxonomies or even to construct new taxonomies from scratch. The previous methods for identifying taxonomic relations (i.e. is-a relations) from text can be generally classified into two categories: statistical and linguistic approa</context>
</contexts>
<marker>Geffet, Dagan, 2005</marker>
<rawString>M. Geffet and I. Dagan. 2005. The Distributional Inclusion Hypotheses and Lexical Entailment. In proceedings of the 43rd Annual Meeting of the ACL, pp. 107-114.</rawString>
</citation>
<citation valid="true">
<authors>
<author>R Girju</author>
<author>A Badulescu</author>
<author>D Moldovan</author>
</authors>
<title>Learning Semantic Constraints for the Automatic Discovery of Part-Whole Relations.</title>
<date>2003</date>
<booktitle>In proceedings of the NAACL,</booktitle>
<pages>1--8</pages>
<contexts>
<context position="2944" citStr="Girju et al., 2003" startWordPosition="441" endWordPosition="444">tering (Wong et al., 2007). The main idea behinds these techniques is that the terms that frequently co-occur may have taxonomic relationships. Such approaches, however, usually suffer from low accuracy, though relatively high coverage, and heavily depend on the choice of feature types and datasets. Most previous methods of the linguistic approach, on the other hand, rely on the lexical-syntactic patterns (e.g. A is a B, A such as B) (Hearst, 1992). Those patterns can be manually created (Kozareva et al., 2008; Wentao et al., 2012), chosen via automatic bootstrapping (Widdows and Dorow, 2002; Girju et al., 2003) or identified from machine-learned classifiers (Navigli et al., 2011). The pattern matching methods generally achieve high precision, but low coverage due to the lack of contextual analysis across sentences. In this paper, we introduce a novel statistical method and shows that when combined with a pattern matching method, it shows significant performance improvement. The proposed statistical method, called syntactic contextual subsumption (SCS), compares the syntactic contexts of terms for the taxonomic relation identification, instead of the usage of bagof-words model by the previous statist</context>
</contexts>
<marker>Girju, Badulescu, Moldovan, 2003</marker>
<rawString>R. Girju, A. Badulescu, and D. Moldovan. 2003. Learning Semantic Constraints for the Automatic Discovery of Part-Whole Relations. In proceedings of the NAACL, pp. 1-8.</rawString>
</citation>
<citation valid="true">
<authors>
<author>S M Harabagiu</author>
<author>S J Maiorano</author>
<author>M A Pasca</author>
</authors>
<title>Open-Domain Textual Question Answering Techniques.</title>
<date>2003</date>
<journal>Natural Language Engineering,</journal>
<volume>9</volume>
<issue>3</issue>
<pages>1--38</pages>
<contexts>
<context position="1513" citStr="Harabagiu et al., 2003" startWordPosition="218" endWordPosition="221"> term includes most of contexts of another term, a subsumption relation between the two terms is inferred. We apply this method to the task of taxonomy construction from scratch, where we introduce another novel graph-based algorithm for taxonomic structure induction. Our experiment results show that the proposed method is well complementary with previous methods of linguistic pattern matching and significantly improves recall and thus F-measure. 1 Introduction Taxonomies that are backbone of structured ontology knowledge have been found to be useful for many areas such as question answering (Harabagiu et al., 2003), document clustering (Fodeh et al., 2011) and textual entailment (Geffet and Dagan, 2005). There have been an increasing number of hand-crafted, well-structured taxonomies publicly available, including WordNet (Miller, 1995), OpenCyc (Matuszek et al., 2006), and Freebase (Bollacker et al., 2008). However, the manual curation of those taxonomies is timeconsuming and human experts may miss relevant terms. As such, there are still needs to extend existing taxonomies or even to construct new taxonomies from scratch. The previous methods for identifying taxonomic relations (i.e. is-a relations) fr</context>
</contexts>
<marker>Harabagiu, Maiorano, Pasca, 2003</marker>
<rawString>S. M. Harabagiu, S. J. Maiorano and M. A. Pasca. 2003. Open-Domain Textual Question Answering Techniques. Natural Language Engineering, 9(3): pp. 1-38.</rawString>
</citation>
<citation valid="true">
<authors>
<author>M A Hearst</author>
</authors>
<title>Automatic Acquisition of Hyponyms from Large Text Corpora.</title>
<date>1992</date>
<booktitle>In proceedings of the 14th Conference on Computational Linguistics,</booktitle>
<pages>539--545</pages>
<contexts>
<context position="2777" citStr="Hearst, 1992" startWordPosition="415" endWordPosition="416">ries: statistical and linguistic approaches. The former includes co-occurrence analysis (Budanitsky, 1999), term subsumption (Fotzo and Gallinari, 2004) and clustering (Wong et al., 2007). The main idea behinds these techniques is that the terms that frequently co-occur may have taxonomic relationships. Such approaches, however, usually suffer from low accuracy, though relatively high coverage, and heavily depend on the choice of feature types and datasets. Most previous methods of the linguistic approach, on the other hand, rely on the lexical-syntactic patterns (e.g. A is a B, A such as B) (Hearst, 1992). Those patterns can be manually created (Kozareva et al., 2008; Wentao et al., 2012), chosen via automatic bootstrapping (Widdows and Dorow, 2002; Girju et al., 2003) or identified from machine-learned classifiers (Navigli et al., 2011). The pattern matching methods generally achieve high precision, but low coverage due to the lack of contextual analysis across sentences. In this paper, we introduce a novel statistical method and shows that when combined with a pattern matching method, it shows significant performance improvement. The proposed statistical method, called syntactic contextual s</context>
</contexts>
<marker>Hearst, 1992</marker>
<rawString>M. A. Hearst. 1992. Automatic Acquisition of Hyponyms from Large Text Corpora. In proceedings of the 14th Conference on Computational Linguistics, pp. 539-545.</rawString>
</citation>
<citation valid="true">
<authors>
<author>D Klein</author>
<author>C D Manning</author>
</authors>
<title>Accurate Unlexicalized Parsing.</title>
<date>2003</date>
<booktitle>In proceedings of the 41st Annual Meeting of the ACL,</booktitle>
<pages>423--430</pages>
<contexts>
<context position="12937" citStr="Klein and Manning, 2003" startWordPosition="2080" endWordPosition="2083">n syntactic structure Subject-Verb-Object is identical to the definition of non-taxonomic relations in the literature (Buitelaar et al., 2004), where the Verb indicate non-taxonomic relations between Subject and Object. In this subsection, we first present the method to collect those non-taxonomic relations. Then we present in detail the ideas of the SCS method and how we can use it to derive taxonomic relations in practice. A. Non-taxonomic Relation Identification Following previous approaches to nontaxonomic relation identification, e.g. (Ciaramita et al., 2005), we use the Stanford parser (Klein and Manning, 2003) to identify the syntactic structures of sentences and extract triples of (Subject, Verb, Object), where Subject and Object are noun phrases. We further consider the following issues: First, if a term (or noun phrase) includes a preposition, we remove the prepositional phrase. However, if the headword of a term is a quantitative noun like “lot”, “many” or “dozen” and it is modified by the preposition “of”, we replace it with the headword of the object of the preposition “of”. For example, we can extract the triples (people, need, food) and (people, like, snow) from the following sentences, res</context>
</contexts>
<marker>Klein, Manning, 2003</marker>
<rawString>D. Klein and C. D. Manning. 2003. Accurate Unlexicalized Parsing. In proceedings of the 41st Annual Meeting of the ACL, pp. 423-430.</rawString>
</citation>
<citation valid="true">
<authors>
<author>Z Kozareva</author>
<author>E Riloff</author>
<author>E H Hovy</author>
</authors>
<title>Semantic Class Learning from the Web with Hyponym Pattern Linkage Graphs.</title>
<date>2008</date>
<booktitle>In proceedings of the 46th Annual Meeting of the ACL,</booktitle>
<pages>1048--1056</pages>
<contexts>
<context position="2840" citStr="Kozareva et al., 2008" startWordPosition="424" endWordPosition="427"> includes co-occurrence analysis (Budanitsky, 1999), term subsumption (Fotzo and Gallinari, 2004) and clustering (Wong et al., 2007). The main idea behinds these techniques is that the terms that frequently co-occur may have taxonomic relationships. Such approaches, however, usually suffer from low accuracy, though relatively high coverage, and heavily depend on the choice of feature types and datasets. Most previous methods of the linguistic approach, on the other hand, rely on the lexical-syntactic patterns (e.g. A is a B, A such as B) (Hearst, 1992). Those patterns can be manually created (Kozareva et al., 2008; Wentao et al., 2012), chosen via automatic bootstrapping (Widdows and Dorow, 2002; Girju et al., 2003) or identified from machine-learned classifiers (Navigli et al., 2011). The pattern matching methods generally achieve high precision, but low coverage due to the lack of contextual analysis across sentences. In this paper, we introduce a novel statistical method and shows that when combined with a pattern matching method, it shows significant performance improvement. The proposed statistical method, called syntactic contextual subsumption (SCS), compares the syntactic contexts of terms for </context>
<context position="22269" citStr="Kozareva et al. (2008)" startWordPosition="3765" endWordPosition="3768">Results We evaluated our methods for taxonomy construction against the following text collections of five domains: • Artificial Intelligence (AI) domain: 4,119 papers extracted from the IJCAI proceedings from 1969 to 2011 and the ACL archives from year 1979 to 2010. The same dataset used in the work of Velardi et al. (2012). • Terrorism domain: 104 reports of the US state department, titled “Patterns of Global Terrorism (1991-2002)” 1. A report contains about 1,500 words. • Animals, Plants and Vehicles domains: Collections of Web pages crawled by using the bootstrapping algorithm described by Kozareva et al. (2008). Navigli et al. (2011) and Kozareva and Hovy (2010) used these datasets to compare their outputs against WordNet sub-hierarchies. There are two experiments performed in this section: 1) Evaluating the construction of new taxonomies for Terrorism and AI domains, and 2) Comparing our results with the gold-standard WordNet sub-hierarchies. Note that in the experiments, the threshold value we used for ScoreLSP is 1.9, ScoreSCS is 1.5 and Score is 2.1. 3.1 Constructing new taxonomies for AI and Terrorism domains Referential taxonomy structures such as WordNet or OpenCyc are widely used in semantic</context>
</contexts>
<marker>Kozareva, Riloff, Hovy, 2008</marker>
<rawString>Z. Kozareva, E. Riloff, and E. H. Hovy. 2008. Semantic Class Learning from the Web with Hyponym Pattern Linkage Graphs. In proceedings of the 46th Annual Meeting of the ACL, pp. 1048-1056.</rawString>
</citation>
<citation valid="true">
<authors>
<author>Z Kozareva</author>
<author>E Hovy</author>
</authors>
<title>A Semi-supervised Method to Learn and Construct Taxonomies Using the Web.</title>
<date>2010</date>
<booktitle>In proceedings of the Conference on Empirical Methods in Natural Language Processing,</booktitle>
<pages>1110--1118</pages>
<contexts>
<context position="5146" citStr="Kozareva and Hovy, 2010" startWordPosition="792" endWordPosition="795">l set inclusion, and as such, is able to find implicit subsumption relations between terms across sentences. The SCS shows itself (Section 3.1) to be complementary to linguistic pattern matching. After the relation identification, the identified taxonomic relations should be integrated into a graph for the task of taxonomy construction from scratch or associated with existing concepts of a given taxonomy via is-a relations (Snow et al., 2006). In this step of taxonomic structure construction, there is a need for pruning incorrect and redundant relations. Previous methods for the pruning task (Kozareva and Hovy, 2010; Velardi et al., 2012) treat the identified taxonomic relations equally, and the pruning task is thus reduced to finding the best trade-off between path length and the connectivity of traversed nodes. This assumption, however, is not always true due to the fact that the identified taxonomic relations may have different confidence values, and the relations with high confidence values can be incorrectly eliminated during the pruning process. We thus propose a novel method for the taxonomy induction by utilizing the evidence scores from the relation identification method and the topological prop</context>
<context position="10348" citStr="Kozareva and Hovy (2010)" startWordPosition="1658" endWordPosition="1661">two terms if every word of tg has a corresponding word in ts (with at least one ≫WN relation). For example, consider two terms: “suicide attack” and “world trade center self-destruction bombing”. Because “attack” ≫WN “bombing” and “suicide” ≈WN “self-destruction”, according to SIWN algorithm, we conclude that “suicide attack” is the hypernym of “world trade center self-destruction bombing”. Given two terms t1 and t2, the evidence score for SIWN algorithm is calculated as follows: { 1 if t1 ≫ t2 via SIWN 5coreSIWN(t1, t2) = 0 otherwise (2) 2.2.2 Lexical-syntactic Pattern Extending the ideas of Kozareva and Hovy (2010) and Navigli et al. (2011), we propose a method of extracting taxonomic relations by matching lexical-syntactic patterns to the Web data. Definition 1(Syntactic patterns). Given two terms t1 and t2, Pat(t1, t2) is defined as the set of the following patterns: • “t1 such as t2” • “t1, including t2” • “t2 is [a|an] t1” • “t2 is a [kind|type] of t1” • “t2, [and|or] other t1” , where t1 and t2 are replaced with actual terms and [a|b] denotes a choice between a and b. Given candidate general term t1 and candidate specific term t2, the lexical-syntactic pattern (LSP) method works as follows: 1. Subm</context>
<context position="19880" citStr="Kozareva and Hovy (2010)" startWordPosition="3329" endWordPosition="3332">tep 1: Initial hypernym graph creation (line 1 - 16) This step is to construct a connected directed graph from the list of taxonomic relations. The idea is to add each taxonomic relation t1 ≫ t2 as a directed edge from parent node t1 to child node t2, and if t1 does not have any hypernym term, t1 will become a child node of ROOT node. The result of this step is a connected graph containing all taxonomic relations with the common ROOT node. Step 2: Edge weighting (line 17) This step is to calculate the weight of each edge in the hypernym graph. Unlike the algorithm of Velardi et al. (2012) and Kozareva and Hovy (2010) where every taxonomic relation is treated equally, we assume the confidence of each taxonomic relation is different, depending on the amount of 814 Algorithm 1 Taxonomy Induction Algorithm Input: T : the taxonomic relation set Output: V : the vertex set of resultant taxonomy; E: the edge set of resultant taxonomy; 1: Initialize V = {ROOT}, E = ∅; 2: for each taxonomic relation (t1 ≫ t2) E T do 3: E = E U {e(t1, t2)} 4: if t1 E̸ V then 5: V = V U {t1} 6: end if 7: if t2 E̸ V then 8: V=VU{t2} 9: end if 10: if ∄ e(t3, t1) E E with t3 ≠ ROOT then 11: E = E U {e(ROOT, t1)} 12: end if 13: if I e(R</context>
<context position="22321" citStr="Kozareva and Hovy (2010)" startWordPosition="3774" endWordPosition="3777">truction against the following text collections of five domains: • Artificial Intelligence (AI) domain: 4,119 papers extracted from the IJCAI proceedings from 1969 to 2011 and the ACL archives from year 1979 to 2010. The same dataset used in the work of Velardi et al. (2012). • Terrorism domain: 104 reports of the US state department, titled “Patterns of Global Terrorism (1991-2002)” 1. A report contains about 1,500 words. • Animals, Plants and Vehicles domains: Collections of Web pages crawled by using the bootstrapping algorithm described by Kozareva et al. (2008). Navigli et al. (2011) and Kozareva and Hovy (2010) used these datasets to compare their outputs against WordNet sub-hierarchies. There are two experiments performed in this section: 1) Evaluating the construction of new taxonomies for Terrorism and AI domains, and 2) Comparing our results with the gold-standard WordNet sub-hierarchies. Note that in the experiments, the threshold value we used for ScoreLSP is 1.9, ScoreSCS is 1.5 and Score is 2.1. 3.1 Constructing new taxonomies for AI and Terrorism domains Referential taxonomy structures such as WordNet or OpenCyc are widely used in semantic analytics applications. However, their coverage is </context>
<context position="26615" citStr="Kozareva and Hovy, 2010" startWordPosition="4486" endWordPosition="4489">ations Terrorism AI domain SCS 91% 88% SIWN 96% 91% LSP 93% 93% SCS + SIWN + LSP 92% 90% Table 4: Estimated precision of taxonomic relation identification methods in 100 extracted relations. 816 Animals domain Plants domain Vehicles domain Our Kozareva Navigli Our Kozareva Navigli Our Kozareva Navigli #Correct relations 2427 1643 N.A. 1243 905 N.A. 281 246 N.A. Term coverage 96% N.A. 94% 98% N.A. 97% 97% N.A. 96% Precision 95% 98% 97% 95% 97% 97% 93% 99% 91% Recall 56% 38% 44% 53% 39% 38% 69% 60% 49% F-measure 71% 55% 61% 68% 56% 55% 79% 75% 64% Table 5: Comparison of (Navigli et al., 2011), (Kozareva and Hovy, 2010) and our system against WordNet in three domains: Animals, Plants and Vehicles. We further evaluated the precision of each individual taxonomic relation identification method. For AI and Terrorism domains, we again run the system with each of the three methods and with all together, and then randomly select 100 extracted taxonomic relations each time. These selected taxonomic relations are then examined by two domain experts to check the correctness. The evaluation results are given in Table 4. Note that only the first two modules of term suggestion/filtering and taxonomic relation identificat</context>
<context position="28914" citStr="Kozareva and Hovy (2010)" startWordPosition="4850" endWordPosition="4853">method. Note that in this comparison, to be fair, we changed our algorithm to avoid using WordNet in identifying taxonomic relations. Specifically, in the SIWN algorithm, all operations of “AWN” are replaced with normal string-matching comparison, and all “≫WN” relations are falsified. The evaluation uses the following measures: Precision — #relations found in WordNet and by the method #relations found by the method Recall —_ #relations found in WordNet and by the method #relations found in WordNet We also compared our results with those obtained by the approaches of Navigli et al. (2011) and Kozareva and Hovy (2010), where they also compared their resultant taxonomies against WordNet. In this comparison, all the three approaches (i.e. ours, the two previous methods) use the same corpora and term lists. The comparison results are given in Table 5. “N.A.” value means that this parameter is not applicable to the corresponding method. The results show that our approach achieves better performance than the other two approaches, in terms of both the number of correctly extracted taxonomic relations and the term coverage. Our system has a slightly lower precision than that of (Navigli et al., 2011) and (Kozarev</context>
</contexts>
<marker>Kozareva, Hovy, 2010</marker>
<rawString>Z. Kozareva and E. Hovy. 2010. A Semi-supervised Method to Learn and Construct Taxonomies Using the Web. In proceedings of the Conference on Empirical Methods in Natural Language Processing, pp. 1110-1118.</rawString>
</citation>
<citation valid="true">
<authors>
<author>C Matuszek</author>
<author>J Cabral</author>
<author>M J Witbrock</author>
<author>J DeOliveira</author>
</authors>
<title>An Introduction to the Syntax and Content of Cyc.</title>
<date>2006</date>
<booktitle>In proceedings of the AAAI Spring Symposium: Formalizing and Compiling Background Knowledge and Its Applications to Knowledge Representation and Question Answering,</booktitle>
<pages>44--49</pages>
<contexts>
<context position="1771" citStr="Matuszek et al., 2006" startWordPosition="254" endWordPosition="257">nduction. Our experiment results show that the proposed method is well complementary with previous methods of linguistic pattern matching and significantly improves recall and thus F-measure. 1 Introduction Taxonomies that are backbone of structured ontology knowledge have been found to be useful for many areas such as question answering (Harabagiu et al., 2003), document clustering (Fodeh et al., 2011) and textual entailment (Geffet and Dagan, 2005). There have been an increasing number of hand-crafted, well-structured taxonomies publicly available, including WordNet (Miller, 1995), OpenCyc (Matuszek et al., 2006), and Freebase (Bollacker et al., 2008). However, the manual curation of those taxonomies is timeconsuming and human experts may miss relevant terms. As such, there are still needs to extend existing taxonomies or even to construct new taxonomies from scratch. The previous methods for identifying taxonomic relations (i.e. is-a relations) from text can be generally classified into two categories: statistical and linguistic approaches. The former includes co-occurrence analysis (Budanitsky, 1999), term subsumption (Fotzo and Gallinari, 2004) and clustering (Wong et al., 2007). The main idea behi</context>
</contexts>
<marker>Matuszek, Cabral, Witbrock, DeOliveira, 2006</marker>
<rawString>C. Matuszek, J. Cabral, M. J. Witbrock and J. DeOliveira. 2006. An Introduction to the Syntax and Content of Cyc. In proceedings of the AAAI Spring Symposium: Formalizing and Compiling Background Knowledge and Its Applications to Knowledge Representation and Question Answering, pp. 44-49.</rawString>
</citation>
<citation valid="true">
<authors>
<author>G A Miller</author>
</authors>
<title>WordNet: a Lexical Database for English.</title>
<date>1995</date>
<journal>Communications of the ACM,</journal>
<volume>38</volume>
<issue>11</issue>
<pages>39--41</pages>
<contexts>
<context position="1738" citStr="Miller, 1995" startWordPosition="251" endWordPosition="252">or taxonomic structure induction. Our experiment results show that the proposed method is well complementary with previous methods of linguistic pattern matching and significantly improves recall and thus F-measure. 1 Introduction Taxonomies that are backbone of structured ontology knowledge have been found to be useful for many areas such as question answering (Harabagiu et al., 2003), document clustering (Fodeh et al., 2011) and textual entailment (Geffet and Dagan, 2005). There have been an increasing number of hand-crafted, well-structured taxonomies publicly available, including WordNet (Miller, 1995), OpenCyc (Matuszek et al., 2006), and Freebase (Bollacker et al., 2008). However, the manual curation of those taxonomies is timeconsuming and human experts may miss relevant terms. As such, there are still needs to extend existing taxonomies or even to construct new taxonomies from scratch. The previous methods for identifying taxonomic relations (i.e. is-a relations) from text can be generally classified into two categories: statistical and linguistic approaches. The former includes co-occurrence analysis (Budanitsky, 1999), term subsumption (Fotzo and Gallinari, 2004) and clustering (Wong </context>
</contexts>
<marker>Miller, 1995</marker>
<rawString>G. A. Miller. 1995. WordNet: a Lexical Database for English. Communications of the ACM, 38(11), pp. 39-41.</rawString>
</citation>
<citation valid="true">
<authors>
<author>R Navigli</author>
<author>P Velardi</author>
</authors>
<title>Learning Domain Ontologies from Document Warehouses and Dedicated Web Sites.</title>
<date>2004</date>
<journal>Computational Linguistics,</journal>
<volume>30</volume>
<issue>2</issue>
<pages>151--179</pages>
<contexts>
<context position="7396" citStr="Navigli and Velardi, 2004" startWordPosition="1139" endWordPosition="1142">mies is to collect candidate terms from text documents in the domain of interest. Like most of linguistic approaches, we use pre-defined linguistic filters to extract candidate terms, including single-word terms and multi-word terms which are noun or noun phrases in sentences. These terms are then preprocessed by removing determiners and lemmatization. The candidate terms collected are then filtered to select the terms that are most relevant to the domain of interest. Many statistical techniques are developed for the filtering, such as TF-IDF, domain relevance (DR), and domain consensus (DC) (Navigli and Velardi, 2004). DR measures the amount of information that a term t captures within a domain of interest DZ, compared to other contrasting domains (Dj), while DC measures the distributed use of a term t across documents d in a domain DZ. Since three measures have pros and cons, and might be complementary to each other, our term filtering method is thus the linear combination of them: T5(t, DZ) = α x TFIDF(t, DZ) + Q x DR(t, DZ) + &apos;y x DC(t, DZ) (1) We experimented (see Section 3) with different values of α, Q and &apos;y, and found that the method shows the best performance when the values for α and Q are 0.2 an</context>
</contexts>
<marker>Navigli, Velardi, 2004</marker>
<rawString>R. Navigli and P. Velardi, 2004. Learning Domain Ontologies from Document Warehouses and Dedicated Web Sites. Computational Linguistics, 30(2), pp. 151-179.</rawString>
</citation>
<citation valid="true">
<authors>
<author>R Navigli</author>
<author>P Velardi</author>
<author>S Faralli</author>
</authors>
<title>A Graphbased Algorithm for Inducing Lexical Taxonomies from Scratch.</title>
<date>2011</date>
<booktitle>In proceedings of the 20th International Joint Conference on Artificial Intelligence,</booktitle>
<pages>1872--1877</pages>
<contexts>
<context position="3014" citStr="Navigli et al., 2011" startWordPosition="451" endWordPosition="454">s that the terms that frequently co-occur may have taxonomic relationships. Such approaches, however, usually suffer from low accuracy, though relatively high coverage, and heavily depend on the choice of feature types and datasets. Most previous methods of the linguistic approach, on the other hand, rely on the lexical-syntactic patterns (e.g. A is a B, A such as B) (Hearst, 1992). Those patterns can be manually created (Kozareva et al., 2008; Wentao et al., 2012), chosen via automatic bootstrapping (Widdows and Dorow, 2002; Girju et al., 2003) or identified from machine-learned classifiers (Navigli et al., 2011). The pattern matching methods generally achieve high precision, but low coverage due to the lack of contextual analysis across sentences. In this paper, we introduce a novel statistical method and shows that when combined with a pattern matching method, it shows significant performance improvement. The proposed statistical method, called syntactic contextual subsumption (SCS), compares the syntactic contexts of terms for the taxonomic relation identification, instead of the usage of bagof-words model by the previous statistical methods. We observe that the terms in taxonomic relations may not</context>
<context position="10374" citStr="Navigli et al. (2011)" startWordPosition="1663" endWordPosition="1666"> has a corresponding word in ts (with at least one ≫WN relation). For example, consider two terms: “suicide attack” and “world trade center self-destruction bombing”. Because “attack” ≫WN “bombing” and “suicide” ≈WN “self-destruction”, according to SIWN algorithm, we conclude that “suicide attack” is the hypernym of “world trade center self-destruction bombing”. Given two terms t1 and t2, the evidence score for SIWN algorithm is calculated as follows: { 1 if t1 ≫ t2 via SIWN 5coreSIWN(t1, t2) = 0 otherwise (2) 2.2.2 Lexical-syntactic Pattern Extending the ideas of Kozareva and Hovy (2010) and Navigli et al. (2011), we propose a method of extracting taxonomic relations by matching lexical-syntactic patterns to the Web data. Definition 1(Syntactic patterns). Given two terms t1 and t2, Pat(t1, t2) is defined as the set of the following patterns: • “t1 such as t2” • “t1, including t2” • “t2 is [a|an] t1” • “t2 is a [kind|type] of t1” • “t2, [and|or] other t1” , where t1 and t2 are replaced with actual terms and [a|b] denotes a choice between a and b. Given candidate general term t1 and candidate specific term t2, the lexical-syntactic pattern (LSP) method works as follows: 1. Submit each phrase in Pat(t1, </context>
<context position="22292" citStr="Navigli et al. (2011)" startWordPosition="3769" endWordPosition="3772"> methods for taxonomy construction against the following text collections of five domains: • Artificial Intelligence (AI) domain: 4,119 papers extracted from the IJCAI proceedings from 1969 to 2011 and the ACL archives from year 1979 to 2010. The same dataset used in the work of Velardi et al. (2012). • Terrorism domain: 104 reports of the US state department, titled “Patterns of Global Terrorism (1991-2002)” 1. A report contains about 1,500 words. • Animals, Plants and Vehicles domains: Collections of Web pages crawled by using the bootstrapping algorithm described by Kozareva et al. (2008). Navigli et al. (2011) and Kozareva and Hovy (2010) used these datasets to compare their outputs against WordNet sub-hierarchies. There are two experiments performed in this section: 1) Evaluating the construction of new taxonomies for Terrorism and AI domains, and 2) Comparing our results with the gold-standard WordNet sub-hierarchies. Note that in the experiments, the threshold value we used for ScoreLSP is 1.9, ScoreSCS is 1.5 and Score is 2.1. 3.1 Constructing new taxonomies for AI and Terrorism domains Referential taxonomy structures such as WordNet or OpenCyc are widely used in semantic analytics applications</context>
<context position="26588" citStr="Navigli et al., 2011" startWordPosition="4482" endWordPosition="4485">ercentage of correct relations Terrorism AI domain SCS 91% 88% SIWN 96% 91% LSP 93% 93% SCS + SIWN + LSP 92% 90% Table 4: Estimated precision of taxonomic relation identification methods in 100 extracted relations. 816 Animals domain Plants domain Vehicles domain Our Kozareva Navigli Our Kozareva Navigli Our Kozareva Navigli #Correct relations 2427 1643 N.A. 1243 905 N.A. 281 246 N.A. Term coverage 96% N.A. 94% 98% N.A. 97% 97% N.A. 96% Precision 95% 98% 97% 95% 97% 97% 93% 99% 91% Recall 56% 38% 44% 53% 39% 38% 69% 60% 49% F-measure 71% 55% 61% 68% 56% 55% 79% 75% 64% Table 5: Comparison of (Navigli et al., 2011), (Kozareva and Hovy, 2010) and our system against WordNet in three domains: Animals, Plants and Vehicles. We further evaluated the precision of each individual taxonomic relation identification method. For AI and Terrorism domains, we again run the system with each of the three methods and with all together, and then randomly select 100 extracted taxonomic relations each time. These selected taxonomic relations are then examined by two domain experts to check the correctness. The evaluation results are given in Table 4. Note that only the first two modules of term suggestion/filtering and tax</context>
<context position="28885" citStr="Navigli et al. (2011)" startWordPosition="4845" endWordPosition="4848">dNet are not found by our method. Note that in this comparison, to be fair, we changed our algorithm to avoid using WordNet in identifying taxonomic relations. Specifically, in the SIWN algorithm, all operations of “AWN” are replaced with normal string-matching comparison, and all “≫WN” relations are falsified. The evaluation uses the following measures: Precision — #relations found in WordNet and by the method #relations found by the method Recall —_ #relations found in WordNet and by the method #relations found in WordNet We also compared our results with those obtained by the approaches of Navigli et al. (2011) and Kozareva and Hovy (2010), where they also compared their resultant taxonomies against WordNet. In this comparison, all the three approaches (i.e. ours, the two previous methods) use the same corpora and term lists. The comparison results are given in Table 5. “N.A.” value means that this parameter is not applicable to the corresponding method. The results show that our approach achieves better performance than the other two approaches, in terms of both the number of correctly extracted taxonomic relations and the term coverage. Our system has a slightly lower precision than that of (Navig</context>
</contexts>
<marker>Navigli, Velardi, Faralli, 2011</marker>
<rawString>R. Navigli, P. Velardi and S. Faralli. 2011. A Graphbased Algorithm for Inducing Lexical Taxonomies from Scratch. In proceedings of the 20th International Joint Conference on Artificial Intelligence, pp. 1872-1877.</rawString>
</citation>
<citation valid="true">
<authors>
<author>R Snow</author>
<author>D Jurafsky</author>
<author>A Y Ng</author>
</authors>
<title>Semantic Taxonomy Induction from Heterogenous Evidence.</title>
<date>2006</date>
<booktitle>In proceedings of the 21st International Conference on Computational Linguistics,</booktitle>
<pages>801--808</pages>
<contexts>
<context position="4969" citStr="Snow et al., 2006" startWordPosition="764" endWordPosition="767"> other term in order to determine their taxonomic relation. The method scores each taxonomic relation candidate based on the two measures of Webbased evidence and contextual set inclusion, and as such, is able to find implicit subsumption relations between terms across sentences. The SCS shows itself (Section 3.1) to be complementary to linguistic pattern matching. After the relation identification, the identified taxonomic relations should be integrated into a graph for the task of taxonomy construction from scratch or associated with existing concepts of a given taxonomy via is-a relations (Snow et al., 2006). In this step of taxonomic structure construction, there is a need for pruning incorrect and redundant relations. Previous methods for the pruning task (Kozareva and Hovy, 2010; Velardi et al., 2012) treat the identified taxonomic relations equally, and the pruning task is thus reduced to finding the best trade-off between path length and the connectivity of traversed nodes. This assumption, however, is not always true due to the fact that the identified taxonomic relations may have different confidence values, and the relations with high confidence values can be incorrectly eliminated during</context>
</contexts>
<marker>Snow, Jurafsky, Ng, 2006</marker>
<rawString>R. Snow, D. Jurafsky and A. Y. Ng. 2006. Semantic Taxonomy Induction from Heterogenous Evidence. In proceedings of the 21st International Conference on Computational Linguistics, pp. 801-808.</rawString>
</citation>
<citation valid="true">
<authors>
<author>P Velardi</author>
<author>S Faralli</author>
<author>R Navigli</author>
</authors>
<title>Ontolearn Reloaded: A Graph-based Algorithm for Taxonomy Induction.</title>
<date>2012</date>
<journal>Computational Linguistics,</journal>
<volume>39</volume>
<issue>3</issue>
<pages>665--707</pages>
<contexts>
<context position="5169" citStr="Velardi et al., 2012" startWordPosition="796" endWordPosition="799">uch, is able to find implicit subsumption relations between terms across sentences. The SCS shows itself (Section 3.1) to be complementary to linguistic pattern matching. After the relation identification, the identified taxonomic relations should be integrated into a graph for the task of taxonomy construction from scratch or associated with existing concepts of a given taxonomy via is-a relations (Snow et al., 2006). In this step of taxonomic structure construction, there is a need for pruning incorrect and redundant relations. Previous methods for the pruning task (Kozareva and Hovy, 2010; Velardi et al., 2012) treat the identified taxonomic relations equally, and the pruning task is thus reduced to finding the best trade-off between path length and the connectivity of traversed nodes. This assumption, however, is not always true due to the fact that the identified taxonomic relations may have different confidence values, and the relations with high confidence values can be incorrectly eliminated during the pruning process. We thus propose a novel method for the taxonomy induction by utilizing the evidence scores from the relation identification method and the topological properties of the graph. We</context>
<context position="19851" citStr="Velardi et al. (2012)" startWordPosition="3324" endWordPosition="3327">ng lines in Algorithm 1. Step 1: Initial hypernym graph creation (line 1 - 16) This step is to construct a connected directed graph from the list of taxonomic relations. The idea is to add each taxonomic relation t1 ≫ t2 as a directed edge from parent node t1 to child node t2, and if t1 does not have any hypernym term, t1 will become a child node of ROOT node. The result of this step is a connected graph containing all taxonomic relations with the common ROOT node. Step 2: Edge weighting (line 17) This step is to calculate the weight of each edge in the hypernym graph. Unlike the algorithm of Velardi et al. (2012) and Kozareva and Hovy (2010) where every taxonomic relation is treated equally, we assume the confidence of each taxonomic relation is different, depending on the amount of 814 Algorithm 1 Taxonomy Induction Algorithm Input: T : the taxonomic relation set Output: V : the vertex set of resultant taxonomy; E: the edge set of resultant taxonomy; 1: Initialize V = {ROOT}, E = ∅; 2: for each taxonomic relation (t1 ≫ t2) E T do 3: E = E U {e(t1, t2)} 4: if t1 E̸ V then 5: V = V U {t1} 6: end if 7: if t2 E̸ V then 8: V=VU{t2} 9: end if 10: if ∄ e(t3, t1) E E with t3 ≠ ROOT then 11: E = E U {e(ROOT,</context>
<context position="21972" citStr="Velardi et al. (2012)" startWordPosition="3719" endWordPosition="3722">ing of a weighted directed graph. Using this algorithm, we can find a subset of the current edge set, which is the optimized taxonomy where every non-root node has in-degree 1 and the sum of the edge weights is maximized. Figure 1 shows an example of the taxonomy induction process. 3 Experiment Results We evaluated our methods for taxonomy construction against the following text collections of five domains: • Artificial Intelligence (AI) domain: 4,119 papers extracted from the IJCAI proceedings from 1969 to 2011 and the ACL archives from year 1979 to 2010. The same dataset used in the work of Velardi et al. (2012). • Terrorism domain: 104 reports of the US state department, titled “Patterns of Global Terrorism (1991-2002)” 1. A report contains about 1,500 words. • Animals, Plants and Vehicles domains: Collections of Web pages crawled by using the bootstrapping algorithm described by Kozareva et al. (2008). Navigli et al. (2011) and Kozareva and Hovy (2010) used these datasets to compare their outputs against WordNet sub-hierarchies. There are two experiments performed in this section: 1) Evaluating the construction of new taxonomies for Terrorism and AI domains, and 2) Comparing our results with the go</context>
<context position="23378" citStr="Velardi et al. (2012)" startWordPosition="3941" endWordPosition="3944"> AI and Terrorism domains Referential taxonomy structures such as WordNet or OpenCyc are widely used in semantic analytics applications. However, their coverage is limited to common well-known areas, and many specific domains like Terrorism and AI are not well covered in those structures. Therefore, an automatic method which can induce taxonomies for those specific domains from scratch can greatly contribute to the process of knowledge discovery. First, we applied our taxonomy construction system to the AI domain corpus. We compared the taxonomy constructed by our system with that obtained by Velardi et al. (2012), and show the comparison results in Table 2. Notice that in this comparison, to be fair, we use the same set of terms that was used in (Velardi et al., 2012). The result shows that our approach can extract 9.8% 1http://www.fas.org/irp/threat/terror.htm 815 Figure 1: An example of taxonomy induction. (a) Initial weighted hypernym graph. (b) Final optimal taxonomy, where we prune two redundant edges (group, International terrorist organization), (Militant group, Hezbollah) and remove the loop by cutting an incorrect edge (Al-Qaeda, Terrorist organization). more taxonomic relations and achieve 7</context>
<context position="29835" citStr="Velardi et al. (2012)" startWordPosition="5001" endWordPosition="5004">corresponding method. The results show that our approach achieves better performance than the other two approaches, in terms of both the number of correctly extracted taxonomic relations and the term coverage. Our system has a slightly lower precision than that of (Navigli et al., 2011) and (Kozareva and Hovy, 2010) due to the SCS method, but it significantly contributes to improve the recall and eventually the F-measure over the other two systems. To judge the effectiveness of our proposed taxonomy induction algorithm described in Section 2.3, we compared it with the graph-based algorithm of Velardi et al. (2012). Recall that in this algorithm, they treat all taxonomic relations equally, and the pruning task is reduced to finding the best trade-off between path length and the connectivity of traversed nodes. For each of five domains (i.e. Terrorism, AI, Animals, Plants and Vehicles), we alternately run the two taxonomy induction algorithms over the same taxonomic relation set produced by our taxonomic relation identification process. For Terrorism and AI domains, we randomly pick up 100 edges in each resultant taxon817 omy and ask two domain experts to judge for the correctness. For Animals, Plants an</context>
<context position="31229" citStr="Velardi et al. (2012)" startWordPosition="5228" endWordPosition="5231">iven in Table 6. The results show that the proposed taxonomy induction algorithm can achieve better performance than the algorithm of Velardi et al. (2012). This may be due to the fact that our algorithm considers the scores of the identified taxonomic relations from the relation identification module, and thus is more precise in eliminating incorrect relations during the pruning process. Percentage of correct edges Our algorithm Velardi’s algorithm Terrorism 94% 90% AI 93% 88% Animals 95% 93% Plants 95% 92% Vehicles 93% 92% Table 6: Comparison of our taxonomy induction algorithms and that of Velardi et al. (2012). In addition, when comparing Tables 4 and 6, we can find that the precision of taxonomic relations after the pruning process is higher than that before the pruning process, which proves that the proposed taxonomy induction algorithm effectively trims the incorrect relations of Terrorism and AI taxonomies, leveraging the percentage of correct relations 2% - 3% up. For the SCS method, besides the triple SubjectVerb-Object, we also explore other syntactic structures like Noun-Preposition-Noun and NounAdjective-Noun. For example, from the sentence “I visited Microsoft in Washington”, the triple (</context>
</contexts>
<marker>Velardi, Faralli, Navigli, 2012</marker>
<rawString>P. Velardi, S. Faralli and R. Navigli. 2012. Ontolearn Reloaded: A Graph-based Algorithm for Taxonomy Induction. Computational Linguistics, 39(3), pp. 665-707.</rawString>
</citation>
<citation valid="true">
<authors>
<author>W Wentao</author>
<author>L Hongsong</author>
<author>W Haixun</author>
<author>Q Zhu</author>
</authors>
<title>Probase: A probabilistic taxonomy for text understanding.</title>
<date>2012</date>
<booktitle>In proceedings of the ACM SIGMOD International Conference on Management of Data,</booktitle>
<pages>481--492</pages>
<contexts>
<context position="2862" citStr="Wentao et al., 2012" startWordPosition="428" endWordPosition="431"> analysis (Budanitsky, 1999), term subsumption (Fotzo and Gallinari, 2004) and clustering (Wong et al., 2007). The main idea behinds these techniques is that the terms that frequently co-occur may have taxonomic relationships. Such approaches, however, usually suffer from low accuracy, though relatively high coverage, and heavily depend on the choice of feature types and datasets. Most previous methods of the linguistic approach, on the other hand, rely on the lexical-syntactic patterns (e.g. A is a B, A such as B) (Hearst, 1992). Those patterns can be manually created (Kozareva et al., 2008; Wentao et al., 2012), chosen via automatic bootstrapping (Widdows and Dorow, 2002; Girju et al., 2003) or identified from machine-learned classifiers (Navigli et al., 2011). The pattern matching methods generally achieve high precision, but low coverage due to the lack of contextual analysis across sentences. In this paper, we introduce a novel statistical method and shows that when combined with a pattern matching method, it shows significant performance improvement. The proposed statistical method, called syntactic contextual subsumption (SCS), compares the syntactic contexts of terms for the taxonomic relation</context>
<context position="11400" citStr="Wentao et al., 2012" startWordPosition="1848" endWordPosition="1851">] denotes a choice between a and b. Given candidate general term t1 and candidate specific term t2, the lexical-syntactic pattern (LSP) method works as follows: 1. Submit each phrase in Pat(t1, t2) to a Web search engine as a query. The number of the search results of the query is denoted as WH(t1, t2). 2. Calculate the following evidence score: 5coreLSP(t1, log(WH(t1, t2)) t2) = 1 + log(WH(t2, t1)) (3) 3. If 5coreLSP(t1, t2) is greater than a threshold value then t1 ≫ t2. While most lexical-syntactic pattern methods in the literature only consider the value of WH(t1, t2) in checking t1 ≫ t2 (Wentao et al., 2012), we take into account both WH(t1, t2) and WH(t2, t1). The intuition of formula (3) is that if t1 is a hypernym of t2 then the size of WH(t1, t2) will be much larger than that of WH(t2, t1), which means the lexical-syntactic patterns are more applicable for the ordered pair (t1, t2) than (t2, t1). 2.2.3 Syntactic Contextual Subsumption The LSP method performs well in recognizing the taxonomic relations between terms in the sentences containing those pre-defined syntactic patterns. This method, however, has a major shortcoming: it cannot derive taxonomic relations between two terms occurring in</context>
</contexts>
<marker>Wentao, Hongsong, Haixun, Zhu, 2012</marker>
<rawString>W. Wentao, L. Hongsong, W. Haixun, and Q. Zhu. 2012. Probase: A probabilistic taxonomy for text understanding. In proceedings of the ACM SIGMOD International Conference on Management of Data, pp. 481-492.</rawString>
</citation>
<citation valid="true">
<authors>
<author>D Widdows</author>
<author>B Dorow</author>
</authors>
<title>A Graph Model for Unsupervised Lexical Acquisition.</title>
<date>2002</date>
<booktitle>In proceedings of the 19th International Conference on Computational Linguistics,</booktitle>
<pages>1--7</pages>
<contexts>
<context position="2923" citStr="Widdows and Dorow, 2002" startWordPosition="436" endWordPosition="440">Gallinari, 2004) and clustering (Wong et al., 2007). The main idea behinds these techniques is that the terms that frequently co-occur may have taxonomic relationships. Such approaches, however, usually suffer from low accuracy, though relatively high coverage, and heavily depend on the choice of feature types and datasets. Most previous methods of the linguistic approach, on the other hand, rely on the lexical-syntactic patterns (e.g. A is a B, A such as B) (Hearst, 1992). Those patterns can be manually created (Kozareva et al., 2008; Wentao et al., 2012), chosen via automatic bootstrapping (Widdows and Dorow, 2002; Girju et al., 2003) or identified from machine-learned classifiers (Navigli et al., 2011). The pattern matching methods generally achieve high precision, but low coverage due to the lack of contextual analysis across sentences. In this paper, we introduce a novel statistical method and shows that when combined with a pattern matching method, it shows significant performance improvement. The proposed statistical method, called syntactic contextual subsumption (SCS), compares the syntactic contexts of terms for the taxonomic relation identification, instead of the usage of bagof-words model by</context>
</contexts>
<marker>Widdows, Dorow, 2002</marker>
<rawString>D. Widdows and B. Dorow. 2002. A Graph Model for Unsupervised Lexical Acquisition. In proceedings of the 19th International Conference on Computational Linguistics, pp. 1-7.</rawString>
</citation>
<citation valid="true">
<authors>
<author>W Wong</author>
<author>W Liu</author>
<author>M Bennamoun</author>
</authors>
<title>Treetraversing ant algorithm for term clustering based on featureless similarities.</title>
<date>2007</date>
<journal>Data Mining and Knowledge Discovery,</journal>
<volume>15</volume>
<issue>3</issue>
<pages>349--381</pages>
<contexts>
<context position="2351" citStr="Wong et al., 2007" startWordPosition="342" endWordPosition="345">1995), OpenCyc (Matuszek et al., 2006), and Freebase (Bollacker et al., 2008). However, the manual curation of those taxonomies is timeconsuming and human experts may miss relevant terms. As such, there are still needs to extend existing taxonomies or even to construct new taxonomies from scratch. The previous methods for identifying taxonomic relations (i.e. is-a relations) from text can be generally classified into two categories: statistical and linguistic approaches. The former includes co-occurrence analysis (Budanitsky, 1999), term subsumption (Fotzo and Gallinari, 2004) and clustering (Wong et al., 2007). The main idea behinds these techniques is that the terms that frequently co-occur may have taxonomic relationships. Such approaches, however, usually suffer from low accuracy, though relatively high coverage, and heavily depend on the choice of feature types and datasets. Most previous methods of the linguistic approach, on the other hand, rely on the lexical-syntactic patterns (e.g. A is a B, A such as B) (Hearst, 1992). Those patterns can be manually created (Kozareva et al., 2008; Wentao et al., 2012), chosen via automatic bootstrapping (Widdows and Dorow, 2002; Girju et al., 2003) or ide</context>
</contexts>
<marker>Wong, Liu, Bennamoun, 2007</marker>
<rawString>W. Wong, W. Liu and M. Bennamoun. 2007. Treetraversing ant algorithm for term clustering based on featureless similarities. Data Mining and Knowledge Discovery, 15(3), pp. 349-381.</rawString>
</citation>
</citationList>
</algorithm>
</algorithms>
<?xml version="1.0" encoding="UTF-8"?>
<algorithms version="110505">
<algorithm name="SectLabel" version="110505">
<variant no="0" confidence="0.000267">
<title confidence="0.998926">
Leveraging Effective Query Modeling Techniques
for Speech Recognition and Summarization
</title>
<author confidence="0.989156">
Kuan-Yu Chen*†, Shih-Hung Liu*, Berlin Chen#, Ea-Ee Jan+,
Hsin-Min Wang*, Wen-Lian Hsu*, and Hsin-Hsi Chen†
</author>
<affiliation confidence="0.992725333333333">
*Institute of Information Science, Academia Sinica, Taiwan
†National Taiwan University, Taiwan
#National Taiwan Normal University, Taiwan
</affiliation>
<note confidence="0.4883">
+IBM Thomas J. Watson Research Center, USA
{kychen, journey, whm, hsu}@iis.sinica.edu.tw,
</note>
<email confidence="0.724915">
berlin@ntnu.edu.tw, hhchen@csie.ntu.edu.tw, ejan@us.ibm.com
</email>
<sectionHeader confidence="0.981695" genericHeader="abstract">
Abstract
</sectionHeader>
<bodyText confidence="0.999990344827586">
Statistical language modeling (LM) that
purports to quantify the acceptability of a
given piece of text has long been an in-
teresting yet challenging research area. In
particular, language modeling for infor-
mation retrieval (IR) has enjoyed re-
markable empirical success; one emerg-
ing stream of the LM approach for IR is
to employ the pseudo-relevance feedback
process to enhance the representation of
an input query so as to improve retrieval
effectiveness. This paper presents a con-
tinuation of such a general line of re-
search and the main contribution is three-
fold. First, we propose a principled
framework which can unify the relation-
ships among several widely-used query
modeling formulations. Second, on top of
the successfully developed framework,
we propose an extended query modeling
formulation by incorporating critical que-
ry-specific information cues to guide the
model estimation. Third, we further adopt
and formalize such a framework to the
speech recognition and summarization
tasks. A series of empirical experiments
reveal the feasibility of such an LM
framework and the performance merits of
the deduced models on these two tasks.
</bodyText>
<sectionHeader confidence="0.99517" genericHeader="introduction">
1 Introduction
</sectionHeader>
<bodyText confidence="0.987845278481013">
Along with the rapidly growing popularity of the
Internet and the ubiquity of social web commu-
nications, tremendous volumes of multimedia
contents, such as broadcast radio and television
programs, digital libraries and so on, are made
available to the public. Research on multimedia
content understanding and organization has wit-
nessed a booming interest over the past decade.
By virtue of the developed techniques, a variety
of functionalities were created to help distill im-
portant content from multimedia collections, or
provide locations of important speech segments
in a video accompanied with their corresponding
transcripts, for users to listen to or to digest. Sta-
tistical language modeling (LM) (Jelinek, 1999;
Jurafsky and Martin, 2008; Zhai, 2008), which
manages to quantify the acceptability of a given
word sequence in a natural language or capture
the statistical characteristics of a given piece of
text, has been proved to offer both efficient and
effective modeling abilities in many practical
applications of natural language processing and
speech recognition (Ponte and Croft, 1998; Jelin-
ek, 1999; Huang, et al., 2001; Zhai and Lafferty,
2001a; Jurafsky and Martin, 2008; Furui et al.,
2012; Liu and Hakkani-Tur, 2011).
The LM approach was first introduced for the
information retrieval (IR) problems in the late
1990s, indicating very good potential, and was
subsequently extended in a wide array of follow-
up studies. One typical realization of the LM ap-
proach for IR is to access the degree of relevance
between a query and a document by computing
the likelihood of the query generated by the doc-
ument (usually referred to as the query-
likelihood approach) (Zhai, 2008; Baeza-Yates
and Ribeiro-Neto, 2011). A document is deemed
to be relevant to a given query if the correspond-
ing document model is more likely to generate
the query. On the other hand, the Kullback-
Leibler divergence measure (denoted by KLM
for short hereafter), which quantifies the degree
of relevance between a document and a query
from a more rigorous information-theoretic per-
spective, has been proposed (Lafferty and Zhai,
2001; Zhai and Lafferty, 2001b; Baeza-Yates and
Ribeiro-Neto, 2011). KLM not only can be
thought as a natural generalization of the query-
likelihood approach, but also has the additional
merit of being able to accommodate extra infor-
mation cues to improve the performance of doc-
ument ranking. For example, a main challenge
facing such a measure is that since a given query
usually consists of few words, the true infor-
mation need is hard to be inferred from the sur-
face statistics of a query. As such, one emerging
stream of thought for KLM is to employ the
1474
Proceedings of the 2014 Conference on Empirical Methods in Natural Language Processing (EMNLP), pages 1474–1480,
October 25-29, 2014, Doha, Qatar. c�2014 Association for Computational Linguistics
pseudo-relevance feedback process to construct
an enhanced query model (or representation) so
as to achieve better retrieval effectiveness (Hi-
emstra et al., 2004; Lv and Zhai, 2009; Carpineto
and Romano, 2012; Lee and Croft, 2013).
Following this line of research, the major con-
tribution of this paper is three-fold: 1) we ana-
lyze several widely-used query models and then
propose a principled framework to unify the rela-
tionships among them; 2) on top of the success-
fully developed query models, we propose an
extended modeling formulation by incorporating
additional query-specific information cues to
guide the model estimation; 3) we explore a nov-
el use of these query models by adapting them to
the speech recognition and summarization tasks.
As we will see, a series of experiments indeed
demonstrate the effectiveness of the proposed
models on these two tasks.
</bodyText>
<equation confidence="0.999611">
KL(Q||D)wVP(w|Q) logP(w|Q) (1)
P(w|D)
</equation>
<bodyText confidence="0.998652904761905">
KLM not only can be thought as a natural gener-
alization of the traditional query-likelihood ap-
proach (Yi and Allan, 2009; Baeza-Yates and
Ribeiro-Neto, 2011), but also has the additional
merit of being able to accommodate extra infor-
mation cues to improve the estimation of its
component models in a systematic way for better
document ranking (Zhai, 2008).
Due to that a query usually consists of only a
few words, the true query model P(w|Q) might
not be accurately estimated by the simple ML
estimator (Jelinek, 1991). There are several stud-
ies devoted to estimating a more accurate query
modeling, saying that it can be approached with
the pseudo-relevance feedback process (Lavren-
ko and Croft, 2001; Zhai and Lafferty, 2001b).
However, the success depends largely on the as-
sumption that the set of top-ranked documents,
DTop={D1,D2,...,Dr,...}, obtained from an initial
round of retrieval, are relevant and can be used to
estimate a more accurate query language model.
</bodyText>
<subsectionHeader confidence="0.997863">
2.2 Relevance Modeling
</subsectionHeader>
<bodyText confidence="0.999221583333333">
Under the notion of relevance modeling (RM,
often referred to as RM-1), each query Q is as-
sumed to be associated with an unknown rele-
vance class RQ, and documents that are relevant
to the semantic content expressed in query are
samples drawn from the relevance class RQ.
Since there is no prior knowledge about RQ, we
may use the top-ranked documents DTop to ap-
proximate the relevance class RQ. The corre-
sponding relevance model can be estimated using
the following equation (Lavrenko and Croft,
2001; Lavrenko, 2004):
</bodyText>
<subsectionHeader confidence="0.998514">
2.3 Simple Mixture Model
</subsectionHeader>
<bodyText confidence="0.998709290322581">
Another perspective of estimating an accurate
query model with the top-ranked documents is
the simple mixture model (SMM), which as-
sumes that words in DTop are drawn from a two-
component mixture model: 1) One component is
the query-specific topic model PSMM(w|Q), and 2)
the other is a generic background model
P(w|BG). By doing so, the SMM model
PSMM(w|Q) can be estimated by maximizing the
likelihood over all the top-ranked documents
(Zhai and Lafferty, 2001b; Tao and Zhai, 2006):
where is a pre-defined weighting parameter
used to control the degree of reliance between
PSMM(w|Q) and P(w|BG). This estimation will
enable more specific words to receive more
probability mass, thereby leading to a more dis-
criminative query model PSMM(w|Q).
Although the SMM modeling aims to extract
extra word usage cues for enhanced query mod-
eling, it may confront two intrinsic problems.
One is the extraction of word usage cues from
DTop is not guided by the original query. The oth-
er is that the mixing coefficient is fixed across
all top-ranked documents albeit that different
documents would potentially contribute different
amounts of word usage cues to the enhanced
query model. To mitigate these two problems,
the regularized simple mixture model has been
proposed and can be estimated by maximizing
the likelihood function (Tao and Zhai, 2006; Dil-
lon and Collins-Thompson, 2010)
</bodyText>
<equation confidence="0.9985575">
•P w Q
( |
(aDr • PRSMM(w  |Q) + (1- aDr) • P(w  |BG)�(w,Dr ),
(4)
</equation>
<bodyText confidence="0.9974225">
where y is a weighting factor indicating the con-
fidence on the prior information.
</bodyText>
<sectionHeader confidence="0.998353" genericHeader="method">
3 The Proposed Modeling Framework
</sectionHeader>
<subsectionHeader confidence="0.998733">
3.1 Fundamentals
</subsectionHeader>
<bodyText confidence="0.965053">
It is obvious that the major difference among the
</bodyText>
<equation confidence="0.78605525">
L = H PRSMM(w  |Q)1 )x
V
wE
2 Language Modeling Framework
</equation>
<subsectionHeader confidence="0.620808">
2.1 Kullback-Leibler Divergence Measure
</subsectionHeader>
<bodyText confidence="0.999806777777778">
A promising realization of the LM approach to
IR is the Kullback-Leibler divergence measure
(KLM), which determines the degree of rele-
vance between a document and a query from a
rigorous information-theoretic perspective. Two
different language models are involved in KLM:
one for the document and the other for the query.
The divergence of the document model with re-
spect to the query model is defined by
</bodyText>
<equation confidence="0.775619333333333">
H H
DrEDTop wEV
1475
</equation>
<bodyText confidence="0.985948545454545">
representative query models mentioned above is
how to capitalize on the set of top-ranked docu-
ments and the original query. Several subtle rela-
tionships can be deduced through the following
in-depth analysis. First, a direct inspiration of the
LM-based query reformulation framework can
be drawn from the celebrated Rocchio’s formula-
tion, while the former can be viewed as a proba-
bilistic counterpart of the latter (Robertson, 1990;
Ponte and Croft, 1998; Baeza-Yates and Ribeiro-
Neto, 2011). Second, after some mathematical
manipulation, the formulation of the RM model
(c.f. Eq. (2)) can be rewritten as
(5)
It becomes evident that the RM model is com-
posed by mixing a set of document models
P(w|Dr). As such, the RM model bears a close
resemblance to the Rocchio’s formulation. Fur-
thermore, based on Eq. (5), we can recast the
estimation of the RM model as an optimization
problem, and the likelihood (or objective) func-
tion is formulated as
</bodyText>
<equation confidence="0.991182285714286">
I
c(w,
L= ri EP(w |Dr)P(Dr |Q)
E
wmV Dr DTop
t. EP(Dr  |Q) = 1
DTop
</equation>
<bodyText confidence="0.999826142857143">
where the document models
are known
in advance; the conditional probability
of each document Dr is unknown and leave to be
estimated. Finally, a principled framework can
be obtained to unify all of these query models,
including RM
</bodyText>
<equation confidence="0.9080268">
Eq. (6)), SMM (c.f. Eq. (3))
an
P(w|Dr)
P(Dr|Q)
(c.f.
</equation>
<bodyText confidence="0.9732935">
d RSMM (c.f. Eq. (4))), by using a generalized
objective likelihood function:
</bodyText>
<equation confidence="0.984440833333333">
I
(w,Ei)
c
L=nn�EP(w|Mr)P(Mr) , (7)
wE V Ei
EE MrEM
</equation>
<bodyText confidence="0.999870666666667">
where E represents a set of observations which
we want to maximize their likelihood, and M
denotes a set of mixture components.
</bodyText>
<subsectionHeader confidence="0.999653">
3.2 Query-specific Mixture Modeling
</subsectionHeader>
<bodyText confidence="0.999948090909091">
The SMM model and the RSMM model are in-
tended to extract useful word usage cues from
DTop, which are not only relevant to the original
query Q but also external to those already cap-
tured by the generic background model. Howev-
er, we argue in this paper that the “generic in-
formation” should be carefully crafted for each
query due mainly to the fact that users’ infor-
mation needs may be very diverse from one an-
other. To crystallize the idea, a query-specific
background model PQ(w|BG) for each query Q
can be derived from DTop directly. Another con-
sideration is that since the original query model
P(w|Q) cannot be accurately estimated, it thus
may not necessarily be the best choice for use in
defining a conjugate Dirichlet prior for the en-
hanced query model to be estimated. We propose
to use the RM model as a prior to guide the esti-
mation of the enhanced query model. The en-
hanced query model is termed query-specific
mixture model (QMM), and its corresponding
training objective function can be expressed as
</bodyText>
<equation confidence="0.992402111111111">
(  |)
w Q X
( , )
w Dr .
L VPQMM (wQ)k
wEV �
II II (aDr•PQMM(w|Q)+(1-aDr)•PQ(w|BG)�
DrE DTop wEV
(8)
</equation>
<sectionHeader confidence="0.997603" genericHeader="method">
4 Applications
</sectionHeader>
<subsectionHeader confidence="0.999217">
4.1 Speech Recognition
</subsectionHeader>
<bodyText confidence="0.99999124">
Language modeling is a critical and integral
component in any large vocabulary continuous
speech recognition (LVCSR) system (Huang et
al., 2001; Jurafsky and Martin, 2008; Furui et al.,
2012). More concretely, the role of language
modeling in LVCSR can be interpreted as calcu-
lating the conditional probability P(w|H), in
which H is a search history, usually expressed as
a sequence of words H=h1, h2,..., hL, and w is
one of its possible immediately succeeding
words. Once the various aforementioned query
modeling methods are applied to speech recogni-
tion, for a search history H, we can conceptually
regard it as a query and each of its immediately
succeeding words w as a (single-word) document.
Then, we may leverage an IR procedure that
takes H as a query and poses it to a retrieval sys-
tem to obtain a set of top-ranked documents from
a contemporaneous (or in-domain) corpus. Final-
ly, the enhanced query model (that is P(w|H) in
speech recognition) can be estimated by RM,
SMM, RSMM or QMM, and further combined
with the background n-gram (e.g., trigram) lan-
guage model to form an adaptive language model
to guide the speech recognition process.
</bodyText>
<subsectionHeader confidence="0.988934">
4.2 Speech Summarization
</subsectionHeader>
<bodyText confidence="0.999780647058824">
On the other hand, extractive speech summariza-
tion aims at producing a concise summary by
selecting salient sentences or paragraphs from
the original spoken document according to a pre-
defined target summarization ratio (Carbonell
and Goldstein, 1998; Mani and Maybury, 1999;
Nenkova and McKeown, 2011; Liu and
Hakkani-Tur, 2011). Intuitively, this task could
be framed as an ad-hoc IR problem, where the
spoken document is treated as an information
need and each sentence of the document is re-
garded as a candidate information unit to be re-
trieved according to its relevance to the infor-
mation need. Therefore, KLM can be used to
quantify how close the document D and one of
its sentences S are: the closer the sentence model
P(w|S) to the document model P(w|D), the more
</bodyText>
<equation confidence="0.723654">
1
E
P( Mr)
s.t.
E
=
M
Mr
Q) (6)
,
s.
DrE
1476
</equation>
<bodyText confidence="0.9997155">
likely the sentence would be part of the summary.
Due to that each sentence S of a spoken docu-
ment D to be summarized usually consists of
only a few words, the corresponding sentence
model P(wjS) might not be appropriately esti-
mated by the ML estimation. To alleviate the
deficiency, we can leverage the merit of the
above query modeling techniques to estimate an
accurate sentence model for each sentence to
enhance the summarization performance.
</bodyText>
<sectionHeader confidence="0.998305" genericHeader="method">
5 Experimental Setup
</sectionHeader>
<bodyText confidence="0.99999402">
The speech corpus consists of about 196 hours of
Mandarin broadcast news collected by the Aca-
demia Sinica and the Public Television Service
Foundation of Taiwan between November 2001
and April 2003 (Wang et al., 2005), which is
publicly available and has been segmented into
separate stories and transcribed manually. Each
story contains the speech of one studio anchor, as
well as several field reporters and interviewees.
A subset of 25-hour speech data compiled during
November 2001 to December 2002 was used to
bootstrap the acoustic model training. The vo-
cabulary size is about 72 thousand words. The
background language model was estimated from
a background text corpus consisting of 170 mil-
lion Chinese characters collected from the Chi-
nese Gigaword Corpus released by LDC.
The dataset for use in the speech recognition
experiments is compiled by a subset of 3-hour
speech data from the corpus within 2003 (1.5
hours for development and 1.5 hours for test).
The contemporaneous (in-domain) text corpus
used for training the various LM adaptation
methods was collected between 2001 and 2003
from the corpus (excluding the test set), which
consists of one million Chinese characters of the
orthographic broadcast news transcripts. In this
paper, all the LM adaptation experiments were
performed in word graph rescoring. The associ-
ated word graphs of the speech data were built
beforehand with a typical LVCSR system (Ort-
manns et al., 1997; Young et al., 2006).
In addition, the summarization task also em-
ploys the same broadcast news corpus as well. A
subset of 205 broadcast news documents com-
piled between November 2001 and August 2002
was reserved for the summarization experiments
(185 for development and 20 for test). A subset
of about 100,000 text news documents, compiled
during the same period as the documents to be
summarized, was employed to estimate the relat-
ed summarization models compared in this paper.
We adopted three variants of the widely-used
ROUGE metric (i.e., ROUGE-1, ROGUE-2 and
ROUGE-L) for the assessment of summarization
performance (Lin, 2003). The summarization
ratio, defined as the ratio of the number of words
in the automatic (or manual) summary to that in
the reference transcript of a spoken document,
was set to 10% in this research.
</bodyText>
<sectionHeader confidence="0.996064" genericHeader="evaluation">
6 Experimental Results
</sectionHeader>
<bodyText confidence="0.99999649122807">
In the first part of experiments, we evaluate the
effectiveness of the various query models applied
to the speech recognition task. The correspond-
ing results with respect to different numbers of
top-ranked documents being used for estimating
their component models are shown in Table 1.
Also worth mentioning is that the baseline sys-
tem with the background trigram language model,
which was trained with the SRILM toolkit
(Stolcke, 2005) and Good-Turing smoothing
(Jelinek, 1999), results in a Chinese character
error rate (CER) of 20.08% on the test set. Con-
sulting Table 1 we notice two particularities. One
is that there is more fluctuation in the CER re-
sults of SMM than in those of RM. The reason
might be that, for SMM, the extraction of rele-
vance information from the top-ranked docu-
ments is conducted with no involvement of the
test utterance (i.e., the query; or its correspond-
ing search histories), as elaborated earlier in Sec-
tion 2. When too many feedback documents are
being used, there would be a concern for SMM
to be distracted from being able to appropriate
model the test utterance, which is probably
caused by some dominant distracting (or irrele-
vant) feedback documents. The other interesting
observation is that RSMM only achieves a com-
parable (even worse) result when compared to
SMM. A possible reason is that the prior con-
straint of the RSMM may contain too much
noisy information so as to bias the model estima-
tion. Furthermore, it is evident that the proposed
QMM is the best-performing method among all
the query models compared in the paper. Alt-
hough the improvements made by QMM are not
as pronounced as expected, we believe that
QMM has demonstrated its potential to be ap-
plied to other related applications. On the other
hand, we compare the various query models with
two well-practiced language models, namely the
cache model (Cache) (Kuhn and Mori, 1990;
Jelinek et al., 1991) and the latent Dirichlet allo-
cation (LDA) (Liu and Liu, 2007; Tam and
Schultz, 2005). The CER results of these two
models are also shown in Table 1, respectively.
For the cache model, bigram cache was used
since it can yield better results than the unigram
and trigram cache models in our experiments. It
is worthy to notice that the LDA model was
trained with the entire set of contemporaneous
text document collection (c.f. Section 4), while
all of the query models explored in the paper
were estimated based on a subset of the corpus
selected by an initial round of retrieval. The re-
sults reveal that most of these query models can
achieve superior performance over the two con-
ventional language models.
</bodyText>
<page confidence="0.558436">
1477
</page>
<bodyText confidence="0.999965066666667">
In the second part of experiments, we evaluate
the utilities of the various query models as ap-
plied to the speech summarization task. At the
outset, we assess the performance level of the
baseline KLM method by comparison with two
well-practiced unsupervised methods, viz. the
vector space model (VSM) (Gong and Liu, 2001),
and its extension, maximal marginal relevance
(MMR) (Carbonell and Goldstein, 1998). The
corresponding results are shown in Table 2 and
can be aligned with several related literature re-
views. By looking at the results, we find that
KLM outperforms VSM by a large margin, con-
firming the applicability of the language model-
ing framework for speech summarization. Fur-
thermore, MMR that presents an extension of
VSM performs on par with KLM for the text
summarization task (TD) and exhibits superior
performance over KLM for the speech summari-
zation task (SD). We now turn to evaluate the
effectiveness of the various query models (viz.
RM, SMM, RSMM and QMM) in conjunction
with the pseudo-relevance feedback process for
enhancing the sentence model involved in the
KLM method. The corresponding results are also
shown in Table 2. Two noteworthy observations
can be drawn from Table 2. One is that all these
query models can considerably improve the
summarization performance of the KLM method,
which corroborates the advantage of using them
for enhanced sentence representations. The other
is that QMM is the best-performing one among
all the formulations studied in this paper for both
the TD and SD cases.
Going one step further, we explore to use extra
prosodic features that are deemed complemen-
tary to the LM cue provided by QMM for speech
summarization. To this end, a support vector ma-
chine (SVM) based summarization model is
trained to integrate a set of 28 commonly-used
prosodic features (Liu and Hakkani-Tur, 2011)
for representing each spoken sentence, since
SVM is arguably one of the state-of-the-art su-
pervised methods that can make use of a diversi-
ty of indicative features for text or speech sum-
marization (Xie and Liu, 2010; Chen et al.,
2013). The sentence ranking scores derived by
QMM and SVM are in turn integrated through a
simple log-linear combination. The correspond-
ing results are shown in Table 2, demonstrating
consistent improvements with respect to all the
three variants of the ROUGE metric as compared
to that using either QMM or SVM in isolation.
We also investigate using SVM to additionally
integrate a richer set of lexical and relevance fea-
tures to complement QMM and further enhance
the summarization effectiveness. However, due
to space limitation, we omit the details here. As a
side note, there is a sizable gap between the TD
and SD cases, indicating room for further im-
</bodyText>
<tableCaption confidence="0.77094025">
Table 1. The speech recognition results (in CER
(%)) achieved by various language models along
with different numbers of latent topics/pseudo-
relevance feedback documents.
</tableCaption>
<table confidence="0.999364375">
16 32 64 128
Baseline 20.08
Cache 19.86
LDA 19.29 19.30 19.28 19.15
RM 19.26 19.26 19.26 19.26
SMM 19.19 19.00 19.14 19.10
RSMM 19.18 19.14 19.15 19.19
QMM 19.05 18.97 19.00 18.99
</table>
<tableCaption confidence="0.996113333333333">
Table 2. The summarization results (in F-scores)
achieved by various language models along with
text and spoken documents.
</tableCaption>
<table confidence="0.998073833333333">
Text Documents (TD) Spoken Documents (SD)
ROUGE-1 ROUGE-2 ROUGE-L ROUGE-1 ROUGE-2 ROUGE-L
VSM 0.347 0.228 0.290 0.342 0.189 0.287
MMR 0.407 0.294 0.358 0.381 0.226 0.331
KLM 0.411 0.298 0.361 0.364 0.210 0.307
RM 0.453 0.335 0.403 0.382 0.239 0.331
SMM 0.439 0.320 0.388 0.383 0.229 0.327
RSMM 0.472 0.365 0.423 0.381 0.235 0.329
QMM 0.486 0.382 0.435 0.395 0.256 0.349
SVM 0.441 0.334 0.396 0.370 0.222 0.326
QMM+ 0.492 0.395 0.448 0.398 0.261 0.358
SVM
</table>
<bodyText confidence="0.989009666666667">
provements. We may seek remedies, such as ro-
bust indexing schemes, to compensate for imper-
fect speech recognition.
</bodyText>
<sectionHeader confidence="0.804101" genericHeader="conclusions">
7 Conclusion and Outlook
</sectionHeader>
<bodyText confidence="0.9999674">
In this paper, we have presented a systematic and
thorough analysis of a few well-practiced query
models for IR and extended their novel applica-
bility to speech recognition and summarization in
a principled way. Furthermore, we have pro-
posed an extension of this research line by intro-
ducing query-specific mixture modeling; the util-
ities of the deduced model have been extensively
compared with several existing query models. As
to future work, we would like to investigate
jointly integrating proximity and other different
kinds of relevance and lexical/semantic infor-
mation cues into the process of feedback docu-
ment selection so as to improve the empirical
effectiveness of such query modeling.
</bodyText>
<sectionHeader confidence="0.955073" genericHeader="acknowledgments">
Acknowledgements
</sectionHeader>
<bodyText confidence="0.999722">
This research is supported in part by the “Aim
for the Top University Project” of National Tai-
wan Normal University (NTNU), sponsored by
the Ministry of Education, Taiwan, and by the
Ministry of Science and Technology, Taiwan,
under Grants MOST 103-2221-E-003-016-MY2,
NSC 101-2221-E-003-024-MY3, NSC 102-
2221-E-003-014-, NSC 101-2511-S-003-057-
MY3, NSC 101-2511-S-003-047-MY3 and NSC
103-2911-I-003-301.
</bodyText>
<page confidence="0.638561">
1478
</page>
<sectionHeader confidence="0.985334" genericHeader="references">
References
</sectionHeader>
<reference confidence="0.990494929347827">
Ricardo Baeza-Yates and Berthier Ribeiro-Neto.
2011. Modern information retrieval: the con-
cepts and technology behind search, ACM
Press.
David M. Blei, Andrew Y. Ng, and Michael I.
Jordan. 2003. Latent dirichlet allocation.
Journal of Machine Learning Research,
pp.993–1022.
David M. Blei and John Lafferty. 2009. Topic
models. In A. Srivastava and M. Sahami,
(eds.), Text Mining: Theory and Applications.
Taylor and Francis.
Jaime Carbonell and Jade Goldstein. 1998. The
use of MMR, diversitybased reranking for
reordering documents and producing sum-
maries. In Proc. SIGIR, pp. 335–336.
Claudio Carpineto and Giovanni Romano. 2012.
A survey of automatic query expansion in in-
formation retrieval. ACM Computing Surveys,
vol. 44, pp.1–56.
Stephane Clinchant and Eric Gaussier. 2013. A
theoretical analysis of pseudo-relevance
feedback models. In Proc. ICTIR.
Guihong Cao, Jian-Yun Nie, Jianfeng Gao, and
Stephen Robertson. 2008. Selecting good
expansion terms for pseudo-relevance feed-
back. In Proc. SIGIR, pp. 243–250.
Berlin Chen, Shih-Hsiang Lin, Yu-Mei Chang,
and Jia-Wen Liu. 2013. Extractive speech
summarization using evaluation metric-
related training criteria. Information Pro-
cessing &amp; Management, 49(1), pp. 1cess
Arthur P. Dempster, Nan M. Laird, and Donald
B. Rubin. 1977. Maximum likelihood from
incomplete data via the EM algorithm. Jour-
nal of Royal Statistical Society B, 39(1), pp.
1–38.
Joshua V. Dillon and Kevyn Collins-Thompson.
2010. A unified optimization framework for
robust pseudo-relevance feedback algorithms.
In Proc. CIKM, pp. 1069–1078.
Sadaoki Furui, Li Deng, Mark Gales, Hermann
Ney, and Keiichi Tokuda. 2012. Fundamen-
tal technologies in modern speech recogni-
tion. IEEE Signal Processing Magazine,
29(6), pp. 16–17.
Yihong Gong and Xin Liu. 2001. Generic text
summarization using relevance measure and
latent semantic analysis. In Proc. SIGIR, pp.
19–25.
Djoerd Hiemstra, Stephen Robertson, and Hugo
Zaragoza. 2004. Parsimonious language
models for information retrieval. In Proc.
SIGIR, pp. 178–185.
Thomas Hofmann. 1999. Probabilistic latent se-
mantic indexing. In Proc. SIGIR, pp. 50–57.
Thomas Hofmann. 2001. Unsupervised learning
by probabilistic latent semantic analysis.
Machine Learning, 42, pp. 177–196.
Xuedong Huang, Alex Acero, and Hsiao-Wuen
Hon. 2001. Spoken language processing: a
guide to theory, algorithm, and system de-
velopment. Prentice Hall PTR, Upper Saddle
River, NJ, USA.
Frederick Jelinek, Bernard Merialdo, Salim Rou-
kos, and M. Strauss. 1991. A dynamic lan-
guage model for speech recognition. In Proc.
the DARPA workshop on speech and natural
language, pp. 293–295.
Frederick Jelinek. 1999. Statistical methods for
speech recognition. MIT Press.
Daniel Jurafsky and James H. Martin. 2008.
Speech and language processing. Prentice
Hall PTR, Upper Saddle River, NJ, USA.
Roland Kuhn and Renato D. Mori. 1990. A
cache-based natural language model for
speech recognition. IEEE Transactions on
Pattern Analysis and Machine Intelligence,
12(6), pp. 570–583.
Solomon Kullback and Richard A. Leibler. 1951.
On information and sufficiency. The Annals
of Mathematical Statistics, 22(1), pp. 79–86.
Chin-Yew Lin. 2003. ROUGE: Recall-oriented
Understudy for Gisting Evaluation. Availa-
ble: http://haydn.isi.edu/ROUGE/.
Feifan Liu and Yang Liu. 2007. Unsupervised
language model adaptation incorporating
named entity information. In Proc. ACL, pp.
672–769.
Yang Liu and Dilek Hakkani-Tur. 2011. Speech
summarization. Chapter 13 in Spoken Lan-
guage Understanding: Systems for Extract-
ing Semantic Information from Speech, G.
Tur and R. D. Mori (Eds), New York: Wiley.
John Lafferty and Chengxiang Zhai. 2001. Doc-
ument language models, query models, and
risk minimization for information retrieval.
In Proc. SIGIR, pp. 111–119.
Victor Lavrenko and W. Bruce Croft. 2001. Rel-
evance-based language models. In Proc.
SIGIR, pp. 120–127.
Victor Lavrenko. 2004. A Generative Theory of
Relevance. PhD thesis, University of Massa-
chusetts, Amherst.
1479
Shasha Xie and Yang Liu. 2010. Improving su-
pervised learning for meeting summarization
using sampling and regression. Computer
Speech &amp; Language, 24(3), pp. 495–514.
Yuanhua Lv and Chengxiang Zhai. 2009. A
comparative study of methods for estimating
query language models with pseudo feed-
back. In Proc. CIKM, pp. 1895–1898.
Yuanhua Lv and Chengxiang Zhai. 2010. Posi-
tional relevance model for pseudo-relevance
feedback. In Proc. SIGIR, pp. 579–586.
Kyung Soon Lee, W. Bruce Croft, and James
Allan. 2008. A cluster-based resampling
method for pseudo-relevance feedback. In
Proc. SIGIR, pp. 235–242.
Kyung Soon Lee and W. Bruce Croft. 2013. A
deterministic resampling method using over-
lapping document clusters for pseudo-
relevance feedback. Inf. Process. Manage.
49(4), pp. 792–806.
Inderjeet Mani and Mark T. Maybury (Eds.).
1999. Advances in automatic text summari-
zation. Cambridge, MA: MIT Press.
Ani Nenkova and Kathleen McKeown. 2011.
Automatic summarization. Foundations and
Trends in Information Retrieval, 5(2–3), pp.
103–233.
Stefan Ortmanns, Hermann Ney, and Xavier Au-
bert. 1997. A word graph algorithm for large
vocabulary continuous speech recognition.
Computer Speech and Language, pp. 43–72.
Jay M. Ponte and W. Bruce Croft. 1998. A lan-
guage modeling approach to information re-
trieval. In Proc. SIGIR, pp. 275–281.
Stephen E. Robertson. 1990. On term selection
for query expansion. Journal of Documenta-
tion, 46(4), pp. 359–364.
Andreas Stolcke. 2005. SRILM - An extensible
language modeling toolkit. In Proc. INTER-
SPEECH, pp.901–904.
Tao Tao and Chengxiang Zhai. 2006. Regular-
ized estimation of mixture models for robust
pseudo-relevance feedback. In Proc. SIGIR,
pp. 162–169.
Yik-Cheung Tam and Tanja Schultz. 2005. Dy-
namic language model adaptation using vari-
ational Bayes inference. In Proc. INTER-
SPEECH, pp. 5–8.
Xuanhui Wang, Hui Fang, and Chengxiang Zhai.
2008. A study of methods for negative rele-
vance feedback. In Proc. SIGIR, pp. 219–226.
Hsin-Min Wang, Berlin Chen, Jen-Wei Kuo, and
Shih-Sian Cheng. 2005. MATBN: A Manda-
rin Chinese broadcast news corpus. Interna-
tional Journal of Computational Linguistics
&amp; Chinese Language Processing, 10(2), pp.
219–236.
Xing Yi and James Allan. 2009. A comparative
study of utilizing topic models for infor-
mation retrieval. In Proc. ECIR, pp. 29–41.
Steve Young, Dan Kershaw, Julian Odell, Dave
Ollason, Valtcho Valtchev, and Phil Wood-
land. 2006. The HTK book version 3.4.
Cambridge University Press.
Chengxiang Zhai and John Lafferty. 2001a. A
study of smoothing methods for language
models applied to ad hoc information re-
trieval. In Proc. SIGIR, pp. 334–342.
Chengxiang Zhai and John Lafferty. 2001b.
Model-based feedback in the language mod-
eling approach to information retrieval. In
Proc. CIKM, pp. 403–410.
Chengxiang Zhai. 2008. Statistical language
models for information retrieval: a critical
review. Foundations and Trends in Infor-
mation Retrieval, 2 (3), pp. 137–213.
Yi Zhang, Jamie Callan, and Thomas Minka.
2002. Novelty and redundancy detection in
adaptive filtering. In Proc. SIGIR, pp. 81–88.
</reference>
<page confidence="0.704846">
1480
</page>
</variant>
</algorithm>
<algorithm name="ParsHed" version="110505">
<variant no="0" confidence="0.530496">
<title confidence="0.892774666666667">Leveraging Effective Query Modeling for Speech Recognition and Summarization Shih-Hung Berlin Ea-Ee</title>
<author confidence="0.908479">Wen-Lian</author>
<author confidence="0.908479">Hsin-Hsi</author>
<affiliation confidence="0.974550333333333">of Information Science, Academia Sinica, Taiwan University, Taiwan Normal University,</affiliation>
<address confidence="0.980658">Thomas J. Watson Research Center, USA</address>
<email confidence="0.9610605">{kychen,journey,whm,berlin@ntnu.edu.tw,hhchen@csie.ntu.edu.tw,ejan@us.ibm.com</email>
<abstract confidence="0.9998454">Statistical language modeling (LM) that purports to quantify the acceptability of a given piece of text has long been an interesting yet challenging research area. In particular, language modeling for information retrieval (IR) has enjoyed remarkable empirical success; one emerging stream of the LM approach for IR is to employ the pseudo-relevance feedback process to enhance the representation of an input query so as to improve retrieval effectiveness. This paper presents a continuation of such a general line of research and the main contribution is threefold. First, we propose a principled framework which can unify the relationships among several widely-used query modeling formulations. Second, on top of the successfully developed framework, we propose an extended query modeling formulation by incorporating critical query-specific information cues to guide the model estimation. Third, we further adopt and formalize such a framework to the speech recognition and summarization tasks. A series of empirical experiments reveal the feasibility of such an LM framework and the performance merits of the deduced models on these two tasks.</abstract>
</variant>
</algorithm>
<algorithm name="ParsCit" version="110505">
<citationList>
<citation valid="true">
<authors>
<author>Ricardo Baeza-Yates</author>
<author>Berthier Ribeiro-Neto</author>
</authors>
<title>Modern information retrieval: the concepts and technology behind search,</title>
<date>2011</date>
<publisher>ACM Press.</publisher>
<contexts>
<context position="3381" citStr="Baeza-Yates and Ribeiro-Neto, 2011" startWordPosition="503" endWordPosition="506">roft, 1998; Jelinek, 1999; Huang, et al., 2001; Zhai and Lafferty, 2001a; Jurafsky and Martin, 2008; Furui et al., 2012; Liu and Hakkani-Tur, 2011). The LM approach was first introduced for the information retrieval (IR) problems in the late 1990s, indicating very good potential, and was subsequently extended in a wide array of followup studies. One typical realization of the LM approach for IR is to access the degree of relevance between a query and a document by computing the likelihood of the query generated by the document (usually referred to as the querylikelihood approach) (Zhai, 2008; Baeza-Yates and Ribeiro-Neto, 2011). A document is deemed to be relevant to a given query if the corresponding document model is more likely to generate the query. On the other hand, the KullbackLeibler divergence measure (denoted by KLM for short hereafter), which quantifies the degree of relevance between a document and a query from a more rigorous information-theoretic perspective, has been proposed (Lafferty and Zhai, 2001; Zhai and Lafferty, 2001b; Baeza-Yates and Ribeiro-Neto, 2011). KLM not only can be thought as a natural generalization of the querylikelihood approach, but also has the additional merit of being able to </context>
<context position="5596" citStr="Baeza-Yates and Ribeiro-Neto, 2011" startWordPosition="858" endWordPosition="861">among them; 2) on top of the successfully developed query models, we propose an extended modeling formulation by incorporating additional query-specific information cues to guide the model estimation; 3) we explore a novel use of these query models by adapting them to the speech recognition and summarization tasks. As we will see, a series of experiments indeed demonstrate the effectiveness of the proposed models on these two tasks. KL(Q||D)wVP(w|Q) logP(w|Q) (1) P(w|D) KLM not only can be thought as a natural generalization of the traditional query-likelihood approach (Yi and Allan, 2009; Baeza-Yates and Ribeiro-Neto, 2011), but also has the additional merit of being able to accommodate extra information cues to improve the estimation of its component models in a systematic way for better document ranking (Zhai, 2008). Due to that a query usually consists of only a few words, the true query model P(w|Q) might not be accurately estimated by the simple ML estimator (Jelinek, 1991). There are several studies devoted to estimating a more accurate query modeling, saying that it can be approached with the pseudo-relevance feedback process (Lavrenko and Croft, 2001; Zhai and Lafferty, 2001b). However, the success depen</context>
</contexts>
<marker>Baeza-Yates, Ribeiro-Neto, 2011</marker>
<rawString>Ricardo Baeza-Yates and Berthier Ribeiro-Neto. 2011. Modern information retrieval: the concepts and technology behind search, ACM Press.</rawString>
</citation>
<citation valid="true">
<authors>
<author>David M Blei</author>
<author>Andrew Y Ng</author>
<author>Michael I Jordan</author>
</authors>
<title>Latent dirichlet allocation.</title>
<date>2003</date>
<journal>Journal of Machine Learning Research,</journal>
<pages>993--1022</pages>
<marker>Blei, Ng, Jordan, 2003</marker>
<rawString>David M. Blei, Andrew Y. Ng, and Michael I. Jordan. 2003. Latent dirichlet allocation. Journal of Machine Learning Research, pp.993–1022.</rawString>
</citation>
<citation valid="true">
<authors>
<author>David M Blei</author>
<author>John Lafferty</author>
</authors>
<title>Topic models.</title>
<date>2009</date>
<booktitle>Text Mining: Theory and Applications.</booktitle>
<editor>In A. Srivastava and M. Sahami, (eds.),</editor>
<publisher>Taylor and Francis.</publisher>
<marker>Blei, Lafferty, 2009</marker>
<rawString>David M. Blei and John Lafferty. 2009. Topic models. In A. Srivastava and M. Sahami, (eds.), Text Mining: Theory and Applications. Taylor and Francis.</rawString>
</citation>
<citation valid="true">
<authors>
<author>Jaime Carbonell</author>
<author>Jade Goldstein</author>
</authors>
<title>The use of MMR, diversitybased reranking for reordering documents and producing summaries.</title>
<date>1998</date>
<booktitle>In Proc. SIGIR,</booktitle>
<pages>335--336</pages>
<contexts>
<context position="13295" citStr="Carbonell and Goldstein, 1998" startWordPosition="2148" endWordPosition="2151">o obtain a set of top-ranked documents from a contemporaneous (or in-domain) corpus. Finally, the enhanced query model (that is P(w|H) in speech recognition) can be estimated by RM, SMM, RSMM or QMM, and further combined with the background n-gram (e.g., trigram) language model to form an adaptive language model to guide the speech recognition process. 4.2 Speech Summarization On the other hand, extractive speech summarization aims at producing a concise summary by selecting salient sentences or paragraphs from the original spoken document according to a predefined target summarization ratio (Carbonell and Goldstein, 1998; Mani and Maybury, 1999; Nenkova and McKeown, 2011; Liu and Hakkani-Tur, 2011). Intuitively, this task could be framed as an ad-hoc IR problem, where the spoken document is treated as an information need and each sentence of the document is regarded as a candidate information unit to be retrieved according to its relevance to the information need. Therefore, KLM can be used to quantify how close the document D and one of its sentences S are: the closer the sentence model P(w|S) to the document model P(w|D), the more 1 E P( Mr) s.t. E = M Mr Q) (6) , s. DrE 1476 likely the sentence would be pa</context>
<context position="19663" citStr="Carbonell and Goldstein, 1998" startWordPosition="3213" endWordPosition="3216"> were estimated based on a subset of the corpus selected by an initial round of retrieval. The results reveal that most of these query models can achieve superior performance over the two conventional language models. 1477 In the second part of experiments, we evaluate the utilities of the various query models as applied to the speech summarization task. At the outset, we assess the performance level of the baseline KLM method by comparison with two well-practiced unsupervised methods, viz. the vector space model (VSM) (Gong and Liu, 2001), and its extension, maximal marginal relevance (MMR) (Carbonell and Goldstein, 1998). The corresponding results are shown in Table 2 and can be aligned with several related literature reviews. By looking at the results, we find that KLM outperforms VSM by a large margin, confirming the applicability of the language modeling framework for speech summarization. Furthermore, MMR that presents an extension of VSM performs on par with KLM for the text summarization task (TD) and exhibits superior performance over KLM for the speech summarization task (SD). We now turn to evaluate the effectiveness of the various query models (viz. RM, SMM, RSMM and QMM) in conjunction with the pse</context>
</contexts>
<marker>Carbonell, Goldstein, 1998</marker>
<rawString>Jaime Carbonell and Jade Goldstein. 1998. The use of MMR, diversitybased reranking for reordering documents and producing summaries. In Proc. SIGIR, pp. 335–336.</rawString>
</citation>
<citation valid="true">
<authors>
<author>Claudio Carpineto</author>
<author>Giovanni Romano</author>
</authors>
<title>A survey of automatic query expansion in information retrieval.</title>
<date>2012</date>
<journal>ACM Computing Surveys,</journal>
<volume>44</volume>
<pages>1--56</pages>
<contexts>
<context position="4738" citStr="Carpineto and Romano, 2012" startWordPosition="722" endWordPosition="725">ure is that since a given query usually consists of few words, the true information need is hard to be inferred from the surface statistics of a query. As such, one emerging stream of thought for KLM is to employ the 1474 Proceedings of the 2014 Conference on Empirical Methods in Natural Language Processing (EMNLP), pages 1474–1480, October 25-29, 2014, Doha, Qatar. c�2014 Association for Computational Linguistics pseudo-relevance feedback process to construct an enhanced query model (or representation) so as to achieve better retrieval effectiveness (Hiemstra et al., 2004; Lv and Zhai, 2009; Carpineto and Romano, 2012; Lee and Croft, 2013). Following this line of research, the major contribution of this paper is three-fold: 1) we analyze several widely-used query models and then propose a principled framework to unify the relationships among them; 2) on top of the successfully developed query models, we propose an extended modeling formulation by incorporating additional query-specific information cues to guide the model estimation; 3) we explore a novel use of these query models by adapting them to the speech recognition and summarization tasks. As we will see, a series of experiments indeed demonstrate t</context>
</contexts>
<marker>Carpineto, Romano, 2012</marker>
<rawString>Claudio Carpineto and Giovanni Romano. 2012. A survey of automatic query expansion in information retrieval. ACM Computing Surveys, vol. 44, pp.1–56.</rawString>
</citation>
<citation valid="true">
<authors>
<author>Stephane Clinchant</author>
<author>Eric Gaussier</author>
</authors>
<title>A theoretical analysis of pseudo-relevance feedback models.</title>
<date>2013</date>
<booktitle>In Proc. ICTIR.</booktitle>
<marker>Clinchant, Gaussier, 2013</marker>
<rawString>Stephane Clinchant and Eric Gaussier. 2013. A theoretical analysis of pseudo-relevance feedback models. In Proc. ICTIR.</rawString>
</citation>
<citation valid="true">
<authors>
<author>Guihong Cao</author>
<author>Jian-Yun Nie</author>
<author>Jianfeng Gao</author>
<author>Stephen Robertson</author>
</authors>
<title>Selecting good expansion terms for pseudo-relevance feedback.</title>
<date>2008</date>
<booktitle>In Proc. SIGIR,</booktitle>
<pages>243--250</pages>
<marker>Cao, Nie, Gao, Robertson, 2008</marker>
<rawString>Guihong Cao, Jian-Yun Nie, Jianfeng Gao, and Stephen Robertson. 2008. Selecting good expansion terms for pseudo-relevance feedback. In Proc. SIGIR, pp. 243–250.</rawString>
</citation>
<citation valid="true">
<authors>
<author>Berlin Chen</author>
<author>Shih-Hsiang Lin</author>
<author>Yu-Mei Chang</author>
<author>Jia-Wen Liu</author>
</authors>
<title>Extractive speech summarization using evaluation metricrelated training criteria.</title>
<date>2013</date>
<journal>Information Processing &amp; Management,</journal>
<volume>49</volume>
<issue>1</issue>
<pages>1</pages>
<contexts>
<context position="21339" citStr="Chen et al., 2013" startWordPosition="3491" endWordPosition="3494">formulations studied in this paper for both the TD and SD cases. Going one step further, we explore to use extra prosodic features that are deemed complementary to the LM cue provided by QMM for speech summarization. To this end, a support vector machine (SVM) based summarization model is trained to integrate a set of 28 commonly-used prosodic features (Liu and Hakkani-Tur, 2011) for representing each spoken sentence, since SVM is arguably one of the state-of-the-art supervised methods that can make use of a diversity of indicative features for text or speech summarization (Xie and Liu, 2010; Chen et al., 2013). The sentence ranking scores derived by QMM and SVM are in turn integrated through a simple log-linear combination. The corresponding results are shown in Table 2, demonstrating consistent improvements with respect to all the three variants of the ROUGE metric as compared to that using either QMM or SVM in isolation. We also investigate using SVM to additionally integrate a richer set of lexical and relevance features to complement QMM and further enhance the summarization effectiveness. However, due to space limitation, we omit the details here. As a side note, there is a sizable gap between</context>
</contexts>
<marker>Chen, Lin, Chang, Liu, 2013</marker>
<rawString>Berlin Chen, Shih-Hsiang Lin, Yu-Mei Chang, and Jia-Wen Liu. 2013. Extractive speech summarization using evaluation metricrelated training criteria. Information Processing &amp; Management, 49(1), pp. 1cess</rawString>
</citation>
<citation valid="true">
<authors>
<author>Arthur P Dempster</author>
<author>Nan M Laird</author>
<author>Donald B Rubin</author>
</authors>
<title>Maximum likelihood from incomplete data via the EM algorithm.</title>
<date>1977</date>
<journal>Journal of Royal Statistical Society B,</journal>
<volume>39</volume>
<issue>1</issue>
<pages>1--38</pages>
<marker>Dempster, Laird, Rubin, 1977</marker>
<rawString>Arthur P. Dempster, Nan M. Laird, and Donald B. Rubin. 1977. Maximum likelihood from incomplete data via the EM algorithm. Journal of Royal Statistical Society B, 39(1), pp. 1–38.</rawString>
</citation>
<citation valid="true">
<authors>
<author>Joshua V Dillon</author>
<author>Kevyn Collins-Thompson</author>
</authors>
<title>A unified optimization framework for robust pseudo-relevance feedback algorithms.</title>
<date>2010</date>
<booktitle>In Proc. CIKM,</booktitle>
<pages>1069--1078</pages>
<contexts>
<context position="8353" citStr="Dillon and Collins-Thompson, 2010" startWordPosition="1304" endWordPosition="1308">). Although the SMM modeling aims to extract extra word usage cues for enhanced query modeling, it may confront two intrinsic problems. One is the extraction of word usage cues from DTop is not guided by the original query. The other is that the mixing coefficient is fixed across all top-ranked documents albeit that different documents would potentially contribute different amounts of word usage cues to the enhanced query model. To mitigate these two problems, the regularized simple mixture model has been proposed and can be estimated by maximizing the likelihood function (Tao and Zhai, 2006; Dillon and Collins-Thompson, 2010) •P w Q ( | (aDr • PRSMM(w |Q) + (1- aDr) • P(w |BG)�(w,Dr ), (4) where y is a weighting factor indicating the confidence on the prior information. 3 The Proposed Modeling Framework 3.1 Fundamentals It is obvious that the major difference among the L = H PRSMM(w |Q)1 )x V wE 2 Language Modeling Framework 2.1 Kullback-Leibler Divergence Measure A promising realization of the LM approach to IR is the Kullback-Leibler divergence measure (KLM), which determines the degree of relevance between a document and a query from a rigorous information-theoretic perspective. Two different language models ar</context>
</contexts>
<marker>Dillon, Collins-Thompson, 2010</marker>
<rawString>Joshua V. Dillon and Kevyn Collins-Thompson. 2010. A unified optimization framework for robust pseudo-relevance feedback algorithms. In Proc. CIKM, pp. 1069–1078.</rawString>
</citation>
<citation valid="true">
<authors>
<author>Sadaoki Furui</author>
<author>Li Deng</author>
<author>Mark Gales</author>
<author>Hermann Ney</author>
<author>Keiichi Tokuda</author>
</authors>
<title>Fundamental technologies in modern speech recognition.</title>
<date>2012</date>
<journal>IEEE Signal Processing Magazine,</journal>
<volume>29</volume>
<issue>6</issue>
<pages>16--17</pages>
<contexts>
<context position="2865" citStr="Furui et al., 2012" startWordPosition="418" endWordPosition="421">ed with their corresponding transcripts, for users to listen to or to digest. Statistical language modeling (LM) (Jelinek, 1999; Jurafsky and Martin, 2008; Zhai, 2008), which manages to quantify the acceptability of a given word sequence in a natural language or capture the statistical characteristics of a given piece of text, has been proved to offer both efficient and effective modeling abilities in many practical applications of natural language processing and speech recognition (Ponte and Croft, 1998; Jelinek, 1999; Huang, et al., 2001; Zhai and Lafferty, 2001a; Jurafsky and Martin, 2008; Furui et al., 2012; Liu and Hakkani-Tur, 2011). The LM approach was first introduced for the information retrieval (IR) problems in the late 1990s, indicating very good potential, and was subsequently extended in a wide array of followup studies. One typical realization of the LM approach for IR is to access the degree of relevance between a query and a document by computing the likelihood of the query generated by the document (usually referred to as the querylikelihood approach) (Zhai, 2008; Baeza-Yates and Ribeiro-Neto, 2011). A document is deemed to be relevant to a given query if the corresponding document</context>
<context position="12059" citStr="Furui et al., 2012" startWordPosition="1945" endWordPosition="1948">t prior for the enhanced query model to be estimated. We propose to use the RM model as a prior to guide the estimation of the enhanced query model. The enhanced query model is termed query-specific mixture model (QMM), and its corresponding training objective function can be expressed as ( |) w Q X ( , ) w Dr . L VPQMM (wQ)k wEV � II II (aDr•PQMM(w|Q)+(1-aDr)•PQ(w|BG)� DrE DTop wEV (8) 4 Applications 4.1 Speech Recognition Language modeling is a critical and integral component in any large vocabulary continuous speech recognition (LVCSR) system (Huang et al., 2001; Jurafsky and Martin, 2008; Furui et al., 2012). More concretely, the role of language modeling in LVCSR can be interpreted as calculating the conditional probability P(w|H), in which H is a search history, usually expressed as a sequence of words H=h1, h2,..., hL, and w is one of its possible immediately succeeding words. Once the various aforementioned query modeling methods are applied to speech recognition, for a search history H, we can conceptually regard it as a query and each of its immediately succeeding words w as a (single-word) document. Then, we may leverage an IR procedure that takes H as a query and poses it to a retrieval s</context>
</contexts>
<marker>Furui, Deng, Gales, Ney, Tokuda, 2012</marker>
<rawString>Sadaoki Furui, Li Deng, Mark Gales, Hermann Ney, and Keiichi Tokuda. 2012. Fundamental technologies in modern speech recognition. IEEE Signal Processing Magazine, 29(6), pp. 16–17.</rawString>
</citation>
<citation valid="true">
<authors>
<author>Yihong Gong</author>
<author>Xin Liu</author>
</authors>
<title>Generic text summarization using relevance measure and latent semantic analysis.</title>
<date>2001</date>
<booktitle>In Proc. SIGIR,</booktitle>
<pages>pp.</pages>
<contexts>
<context position="19578" citStr="Gong and Liu, 2001" startWordPosition="3202" endWordPosition="3205">tion (c.f. Section 4), while all of the query models explored in the paper were estimated based on a subset of the corpus selected by an initial round of retrieval. The results reveal that most of these query models can achieve superior performance over the two conventional language models. 1477 In the second part of experiments, we evaluate the utilities of the various query models as applied to the speech summarization task. At the outset, we assess the performance level of the baseline KLM method by comparison with two well-practiced unsupervised methods, viz. the vector space model (VSM) (Gong and Liu, 2001), and its extension, maximal marginal relevance (MMR) (Carbonell and Goldstein, 1998). The corresponding results are shown in Table 2 and can be aligned with several related literature reviews. By looking at the results, we find that KLM outperforms VSM by a large margin, confirming the applicability of the language modeling framework for speech summarization. Furthermore, MMR that presents an extension of VSM performs on par with KLM for the text summarization task (TD) and exhibits superior performance over KLM for the speech summarization task (SD). We now turn to evaluate the effectiveness</context>
</contexts>
<marker>Gong, Liu, 2001</marker>
<rawString>Yihong Gong and Xin Liu. 2001. Generic text summarization using relevance measure and latent semantic analysis. In Proc. SIGIR, pp. 19–25.</rawString>
</citation>
<citation valid="true">
<authors>
<author>Djoerd Hiemstra</author>
<author>Stephen Robertson</author>
<author>Hugo Zaragoza</author>
</authors>
<title>Parsimonious language models for information retrieval.</title>
<date>2004</date>
<booktitle>In Proc. SIGIR,</booktitle>
<pages>178--185</pages>
<contexts>
<context position="4691" citStr="Hiemstra et al., 2004" startWordPosition="713" endWordPosition="717">ample, a main challenge facing such a measure is that since a given query usually consists of few words, the true information need is hard to be inferred from the surface statistics of a query. As such, one emerging stream of thought for KLM is to employ the 1474 Proceedings of the 2014 Conference on Empirical Methods in Natural Language Processing (EMNLP), pages 1474–1480, October 25-29, 2014, Doha, Qatar. c�2014 Association for Computational Linguistics pseudo-relevance feedback process to construct an enhanced query model (or representation) so as to achieve better retrieval effectiveness (Hiemstra et al., 2004; Lv and Zhai, 2009; Carpineto and Romano, 2012; Lee and Croft, 2013). Following this line of research, the major contribution of this paper is three-fold: 1) we analyze several widely-used query models and then propose a principled framework to unify the relationships among them; 2) on top of the successfully developed query models, we propose an extended modeling formulation by incorporating additional query-specific information cues to guide the model estimation; 3) we explore a novel use of these query models by adapting them to the speech recognition and summarization tasks. As we will se</context>
</contexts>
<marker>Hiemstra, Robertson, Zaragoza, 2004</marker>
<rawString>Djoerd Hiemstra, Stephen Robertson, and Hugo Zaragoza. 2004. Parsimonious language models for information retrieval. In Proc. SIGIR, pp. 178–185.</rawString>
</citation>
<citation valid="true">
<authors>
<author>Thomas Hofmann</author>
</authors>
<title>Probabilistic latent semantic indexing.</title>
<date>1999</date>
<booktitle>In Proc. SIGIR,</booktitle>
<pages>50--57</pages>
<marker>Hofmann, 1999</marker>
<rawString>Thomas Hofmann. 1999. Probabilistic latent semantic indexing. In Proc. SIGIR, pp. 50–57.</rawString>
</citation>
<citation valid="true">
<authors>
<author>Thomas Hofmann</author>
</authors>
<title>Unsupervised learning by probabilistic latent semantic analysis.</title>
<date>2001</date>
<booktitle>Machine Learning,</booktitle>
<volume>42</volume>
<pages>177--196</pages>
<marker>Hofmann, 2001</marker>
<rawString>Thomas Hofmann. 2001. Unsupervised learning by probabilistic latent semantic analysis. Machine Learning, 42, pp. 177–196.</rawString>
</citation>
<citation valid="true">
<authors>
<author>Xuedong Huang</author>
<author>Alex Acero</author>
<author>Hsiao-Wuen Hon</author>
</authors>
<title>Spoken language processing: a guide to theory, algorithm, and system development. Prentice Hall PTR, Upper Saddle River,</title>
<date>2001</date>
<location>NJ, USA.</location>
<contexts>
<context position="2792" citStr="Huang, et al., 2001" startWordPosition="406" endWordPosition="409">ns, or provide locations of important speech segments in a video accompanied with their corresponding transcripts, for users to listen to or to digest. Statistical language modeling (LM) (Jelinek, 1999; Jurafsky and Martin, 2008; Zhai, 2008), which manages to quantify the acceptability of a given word sequence in a natural language or capture the statistical characteristics of a given piece of text, has been proved to offer both efficient and effective modeling abilities in many practical applications of natural language processing and speech recognition (Ponte and Croft, 1998; Jelinek, 1999; Huang, et al., 2001; Zhai and Lafferty, 2001a; Jurafsky and Martin, 2008; Furui et al., 2012; Liu and Hakkani-Tur, 2011). The LM approach was first introduced for the information retrieval (IR) problems in the late 1990s, indicating very good potential, and was subsequently extended in a wide array of followup studies. One typical realization of the LM approach for IR is to access the degree of relevance between a query and a document by computing the likelihood of the query generated by the document (usually referred to as the querylikelihood approach) (Zhai, 2008; Baeza-Yates and Ribeiro-Neto, 2011). A documen</context>
<context position="12011" citStr="Huang et al., 2001" startWordPosition="1937" endWordPosition="1940">choice for use in defining a conjugate Dirichlet prior for the enhanced query model to be estimated. We propose to use the RM model as a prior to guide the estimation of the enhanced query model. The enhanced query model is termed query-specific mixture model (QMM), and its corresponding training objective function can be expressed as ( |) w Q X ( , ) w Dr . L VPQMM (wQ)k wEV � II II (aDr•PQMM(w|Q)+(1-aDr)•PQ(w|BG)� DrE DTop wEV (8) 4 Applications 4.1 Speech Recognition Language modeling is a critical and integral component in any large vocabulary continuous speech recognition (LVCSR) system (Huang et al., 2001; Jurafsky and Martin, 2008; Furui et al., 2012). More concretely, the role of language modeling in LVCSR can be interpreted as calculating the conditional probability P(w|H), in which H is a search history, usually expressed as a sequence of words H=h1, h2,..., hL, and w is one of its possible immediately succeeding words. Once the various aforementioned query modeling methods are applied to speech recognition, for a search history H, we can conceptually regard it as a query and each of its immediately succeeding words w as a (single-word) document. Then, we may leverage an IR procedure that </context>
</contexts>
<marker>Huang, Acero, Hon, 2001</marker>
<rawString>Xuedong Huang, Alex Acero, and Hsiao-Wuen Hon. 2001. Spoken language processing: a guide to theory, algorithm, and system development. Prentice Hall PTR, Upper Saddle River, NJ, USA.</rawString>
</citation>
<citation valid="true">
<authors>
<author>Frederick Jelinek</author>
<author>Bernard Merialdo</author>
<author>Salim Roukos</author>
<author>M Strauss</author>
</authors>
<title>A dynamic language model for speech recognition.</title>
<date>1991</date>
<booktitle>In Proc. the DARPA workshop on speech and natural language,</booktitle>
<pages>293--295</pages>
<contexts>
<context position="18543" citStr="Jelinek et al., 1991" startWordPosition="3025" endWordPosition="3028">n compared to SMM. A possible reason is that the prior constraint of the RSMM may contain too much noisy information so as to bias the model estimation. Furthermore, it is evident that the proposed QMM is the best-performing method among all the query models compared in the paper. Although the improvements made by QMM are not as pronounced as expected, we believe that QMM has demonstrated its potential to be applied to other related applications. On the other hand, we compare the various query models with two well-practiced language models, namely the cache model (Cache) (Kuhn and Mori, 1990; Jelinek et al., 1991) and the latent Dirichlet allocation (LDA) (Liu and Liu, 2007; Tam and Schultz, 2005). The CER results of these two models are also shown in Table 1, respectively. For the cache model, bigram cache was used since it can yield better results than the unigram and trigram cache models in our experiments. It is worthy to notice that the LDA model was trained with the entire set of contemporaneous text document collection (c.f. Section 4), while all of the query models explored in the paper were estimated based on a subset of the corpus selected by an initial round of retrieval. The results reveal </context>
</contexts>
<marker>Jelinek, Merialdo, Roukos, Strauss, 1991</marker>
<rawString>Frederick Jelinek, Bernard Merialdo, Salim Roukos, and M. Strauss. 1991. A dynamic language model for speech recognition. In Proc. the DARPA workshop on speech and natural language, pp. 293–295.</rawString>
</citation>
<citation valid="true">
<authors>
<author>Frederick Jelinek</author>
</authors>
<title>Statistical methods for speech recognition.</title>
<date>1999</date>
<publisher>MIT Press.</publisher>
<contexts>
<context position="2374" citStr="Jelinek, 1999" startWordPosition="343" endWordPosition="344">tremendous volumes of multimedia contents, such as broadcast radio and television programs, digital libraries and so on, are made available to the public. Research on multimedia content understanding and organization has witnessed a booming interest over the past decade. By virtue of the developed techniques, a variety of functionalities were created to help distill important content from multimedia collections, or provide locations of important speech segments in a video accompanied with their corresponding transcripts, for users to listen to or to digest. Statistical language modeling (LM) (Jelinek, 1999; Jurafsky and Martin, 2008; Zhai, 2008), which manages to quantify the acceptability of a given word sequence in a natural language or capture the statistical characteristics of a given piece of text, has been proved to offer both efficient and effective modeling abilities in many practical applications of natural language processing and speech recognition (Ponte and Croft, 1998; Jelinek, 1999; Huang, et al., 2001; Zhai and Lafferty, 2001a; Jurafsky and Martin, 2008; Furui et al., 2012; Liu and Hakkani-Tur, 2011). The LM approach was first introduced for the information retrieval (IR) problem</context>
<context position="17113" citStr="Jelinek, 1999" startWordPosition="2780" endWordPosition="2781"> (or manual) summary to that in the reference transcript of a spoken document, was set to 10% in this research. 6 Experimental Results In the first part of experiments, we evaluate the effectiveness of the various query models applied to the speech recognition task. The corresponding results with respect to different numbers of top-ranked documents being used for estimating their component models are shown in Table 1. Also worth mentioning is that the baseline system with the background trigram language model, which was trained with the SRILM toolkit (Stolcke, 2005) and Good-Turing smoothing (Jelinek, 1999), results in a Chinese character error rate (CER) of 20.08% on the test set. Consulting Table 1 we notice two particularities. One is that there is more fluctuation in the CER results of SMM than in those of RM. The reason might be that, for SMM, the extraction of relevance information from the top-ranked documents is conducted with no involvement of the test utterance (i.e., the query; or its corresponding search histories), as elaborated earlier in Section 2. When too many feedback documents are being used, there would be a concern for SMM to be distracted from being able to appropriate mode</context>
</contexts>
<marker>Jelinek, 1999</marker>
<rawString>Frederick Jelinek. 1999. Statistical methods for speech recognition. MIT Press.</rawString>
</citation>
<citation valid="true">
<authors>
<author>Daniel Jurafsky</author>
<author>James H Martin</author>
</authors>
<title>Speech and language processing. Prentice Hall PTR, Upper Saddle River,</title>
<date>2008</date>
<location>NJ, USA.</location>
<contexts>
<context position="2401" citStr="Jurafsky and Martin, 2008" startWordPosition="345" endWordPosition="348">mes of multimedia contents, such as broadcast radio and television programs, digital libraries and so on, are made available to the public. Research on multimedia content understanding and organization has witnessed a booming interest over the past decade. By virtue of the developed techniques, a variety of functionalities were created to help distill important content from multimedia collections, or provide locations of important speech segments in a video accompanied with their corresponding transcripts, for users to listen to or to digest. Statistical language modeling (LM) (Jelinek, 1999; Jurafsky and Martin, 2008; Zhai, 2008), which manages to quantify the acceptability of a given word sequence in a natural language or capture the statistical characteristics of a given piece of text, has been proved to offer both efficient and effective modeling abilities in many practical applications of natural language processing and speech recognition (Ponte and Croft, 1998; Jelinek, 1999; Huang, et al., 2001; Zhai and Lafferty, 2001a; Jurafsky and Martin, 2008; Furui et al., 2012; Liu and Hakkani-Tur, 2011). The LM approach was first introduced for the information retrieval (IR) problems in the late 1990s, indica</context>
<context position="12038" citStr="Jurafsky and Martin, 2008" startWordPosition="1941" endWordPosition="1944">fining a conjugate Dirichlet prior for the enhanced query model to be estimated. We propose to use the RM model as a prior to guide the estimation of the enhanced query model. The enhanced query model is termed query-specific mixture model (QMM), and its corresponding training objective function can be expressed as ( |) w Q X ( , ) w Dr . L VPQMM (wQ)k wEV � II II (aDr•PQMM(w|Q)+(1-aDr)•PQ(w|BG)� DrE DTop wEV (8) 4 Applications 4.1 Speech Recognition Language modeling is a critical and integral component in any large vocabulary continuous speech recognition (LVCSR) system (Huang et al., 2001; Jurafsky and Martin, 2008; Furui et al., 2012). More concretely, the role of language modeling in LVCSR can be interpreted as calculating the conditional probability P(w|H), in which H is a search history, usually expressed as a sequence of words H=h1, h2,..., hL, and w is one of its possible immediately succeeding words. Once the various aforementioned query modeling methods are applied to speech recognition, for a search history H, we can conceptually regard it as a query and each of its immediately succeeding words w as a (single-word) document. Then, we may leverage an IR procedure that takes H as a query and pose</context>
</contexts>
<marker>Jurafsky, Martin, 2008</marker>
<rawString>Daniel Jurafsky and James H. Martin. 2008. Speech and language processing. Prentice Hall PTR, Upper Saddle River, NJ, USA.</rawString>
</citation>
<citation valid="true">
<authors>
<author>Roland Kuhn</author>
<author>Renato D Mori</author>
</authors>
<title>A cache-based natural language model for speech recognition.</title>
<date>1990</date>
<journal>IEEE Transactions on Pattern Analysis and Machine Intelligence,</journal>
<volume>12</volume>
<issue>6</issue>
<pages>570--583</pages>
<contexts>
<context position="18520" citStr="Kuhn and Mori, 1990" startWordPosition="3021" endWordPosition="3024">ven worse) result when compared to SMM. A possible reason is that the prior constraint of the RSMM may contain too much noisy information so as to bias the model estimation. Furthermore, it is evident that the proposed QMM is the best-performing method among all the query models compared in the paper. Although the improvements made by QMM are not as pronounced as expected, we believe that QMM has demonstrated its potential to be applied to other related applications. On the other hand, we compare the various query models with two well-practiced language models, namely the cache model (Cache) (Kuhn and Mori, 1990; Jelinek et al., 1991) and the latent Dirichlet allocation (LDA) (Liu and Liu, 2007; Tam and Schultz, 2005). The CER results of these two models are also shown in Table 1, respectively. For the cache model, bigram cache was used since it can yield better results than the unigram and trigram cache models in our experiments. It is worthy to notice that the LDA model was trained with the entire set of contemporaneous text document collection (c.f. Section 4), while all of the query models explored in the paper were estimated based on a subset of the corpus selected by an initial round of retriev</context>
</contexts>
<marker>Kuhn, Mori, 1990</marker>
<rawString>Roland Kuhn and Renato D. Mori. 1990. A cache-based natural language model for speech recognition. IEEE Transactions on Pattern Analysis and Machine Intelligence, 12(6), pp. 570–583.</rawString>
</citation>
<citation valid="true">
<authors>
<author>Solomon Kullback</author>
<author>Richard A Leibler</author>
</authors>
<title>On information and sufficiency.</title>
<date>1951</date>
<journal>The Annals of Mathematical Statistics,</journal>
<volume>22</volume>
<issue>1</issue>
<pages>79--86</pages>
<marker>Kullback, Leibler, 1951</marker>
<rawString>Solomon Kullback and Richard A. Leibler. 1951. On information and sufficiency. The Annals of Mathematical Statistics, 22(1), pp. 79–86.</rawString>
</citation>
<citation valid="true">
<authors>
<author>Chin-Yew Lin</author>
</authors>
<title>ROUGE: Recall-oriented Understudy for Gisting Evaluation.</title>
<date>2003</date>
<location>Available: http://haydn.isi.edu/ROUGE/.</location>
<contexts>
<context position="16412" citStr="Lin, 2003" startWordPosition="2668" endWordPosition="2669">addition, the summarization task also employs the same broadcast news corpus as well. A subset of 205 broadcast news documents compiled between November 2001 and August 2002 was reserved for the summarization experiments (185 for development and 20 for test). A subset of about 100,000 text news documents, compiled during the same period as the documents to be summarized, was employed to estimate the related summarization models compared in this paper. We adopted three variants of the widely-used ROUGE metric (i.e., ROUGE-1, ROGUE-2 and ROUGE-L) for the assessment of summarization performance (Lin, 2003). The summarization ratio, defined as the ratio of the number of words in the automatic (or manual) summary to that in the reference transcript of a spoken document, was set to 10% in this research. 6 Experimental Results In the first part of experiments, we evaluate the effectiveness of the various query models applied to the speech recognition task. The corresponding results with respect to different numbers of top-ranked documents being used for estimating their component models are shown in Table 1. Also worth mentioning is that the baseline system with the background trigram language mode</context>
</contexts>
<marker>Lin, 2003</marker>
<rawString>Chin-Yew Lin. 2003. ROUGE: Recall-oriented Understudy for Gisting Evaluation. Available: http://haydn.isi.edu/ROUGE/.</rawString>
</citation>
<citation valid="true">
<authors>
<author>Feifan Liu</author>
<author>Yang Liu</author>
</authors>
<title>Unsupervised language model adaptation incorporating named entity information.</title>
<date>2007</date>
<booktitle>In Proc. ACL,</booktitle>
<pages>672--769</pages>
<contexts>
<context position="18604" citStr="Liu and Liu, 2007" startWordPosition="3036" endWordPosition="3039"> of the RSMM may contain too much noisy information so as to bias the model estimation. Furthermore, it is evident that the proposed QMM is the best-performing method among all the query models compared in the paper. Although the improvements made by QMM are not as pronounced as expected, we believe that QMM has demonstrated its potential to be applied to other related applications. On the other hand, we compare the various query models with two well-practiced language models, namely the cache model (Cache) (Kuhn and Mori, 1990; Jelinek et al., 1991) and the latent Dirichlet allocation (LDA) (Liu and Liu, 2007; Tam and Schultz, 2005). The CER results of these two models are also shown in Table 1, respectively. For the cache model, bigram cache was used since it can yield better results than the unigram and trigram cache models in our experiments. It is worthy to notice that the LDA model was trained with the entire set of contemporaneous text document collection (c.f. Section 4), while all of the query models explored in the paper were estimated based on a subset of the corpus selected by an initial round of retrieval. The results reveal that most of these query models can achieve superior performa</context>
</contexts>
<marker>Liu, Liu, 2007</marker>
<rawString>Feifan Liu and Yang Liu. 2007. Unsupervised language model adaptation incorporating named entity information. In Proc. ACL, pp. 672–769.</rawString>
</citation>
<citation valid="true">
<authors>
<author>Yang Liu</author>
<author>Dilek Hakkani-Tur</author>
</authors>
<title>Speech summarization. Chapter 13 in Spoken Language Understanding: Systems for Extracting Semantic Information from</title>
<date>2011</date>
<publisher>Wiley.</publisher>
<location>New York:</location>
<contexts>
<context position="2893" citStr="Liu and Hakkani-Tur, 2011" startWordPosition="422" endWordPosition="425">ponding transcripts, for users to listen to or to digest. Statistical language modeling (LM) (Jelinek, 1999; Jurafsky and Martin, 2008; Zhai, 2008), which manages to quantify the acceptability of a given word sequence in a natural language or capture the statistical characteristics of a given piece of text, has been proved to offer both efficient and effective modeling abilities in many practical applications of natural language processing and speech recognition (Ponte and Croft, 1998; Jelinek, 1999; Huang, et al., 2001; Zhai and Lafferty, 2001a; Jurafsky and Martin, 2008; Furui et al., 2012; Liu and Hakkani-Tur, 2011). The LM approach was first introduced for the information retrieval (IR) problems in the late 1990s, indicating very good potential, and was subsequently extended in a wide array of followup studies. One typical realization of the LM approach for IR is to access the degree of relevance between a query and a document by computing the likelihood of the query generated by the document (usually referred to as the querylikelihood approach) (Zhai, 2008; Baeza-Yates and Ribeiro-Neto, 2011). A document is deemed to be relevant to a given query if the corresponding document model is more likely to gen</context>
<context position="13374" citStr="Liu and Hakkani-Tur, 2011" startWordPosition="2160" endWordPosition="2163">s. Finally, the enhanced query model (that is P(w|H) in speech recognition) can be estimated by RM, SMM, RSMM or QMM, and further combined with the background n-gram (e.g., trigram) language model to form an adaptive language model to guide the speech recognition process. 4.2 Speech Summarization On the other hand, extractive speech summarization aims at producing a concise summary by selecting salient sentences or paragraphs from the original spoken document according to a predefined target summarization ratio (Carbonell and Goldstein, 1998; Mani and Maybury, 1999; Nenkova and McKeown, 2011; Liu and Hakkani-Tur, 2011). Intuitively, this task could be framed as an ad-hoc IR problem, where the spoken document is treated as an information need and each sentence of the document is regarded as a candidate information unit to be retrieved according to its relevance to the information need. Therefore, KLM can be used to quantify how close the document D and one of its sentences S are: the closer the sentence model P(w|S) to the document model P(w|D), the more 1 E P( Mr) s.t. E = M Mr Q) (6) , s. DrE 1476 likely the sentence would be part of the summary. Due to that each sentence S of a spoken document D to be sum</context>
<context position="21103" citStr="Liu and Hakkani-Tur, 2011" startWordPosition="3450" endWordPosition="3453">l these query models can considerably improve the summarization performance of the KLM method, which corroborates the advantage of using them for enhanced sentence representations. The other is that QMM is the best-performing one among all the formulations studied in this paper for both the TD and SD cases. Going one step further, we explore to use extra prosodic features that are deemed complementary to the LM cue provided by QMM for speech summarization. To this end, a support vector machine (SVM) based summarization model is trained to integrate a set of 28 commonly-used prosodic features (Liu and Hakkani-Tur, 2011) for representing each spoken sentence, since SVM is arguably one of the state-of-the-art supervised methods that can make use of a diversity of indicative features for text or speech summarization (Xie and Liu, 2010; Chen et al., 2013). The sentence ranking scores derived by QMM and SVM are in turn integrated through a simple log-linear combination. The corresponding results are shown in Table 2, demonstrating consistent improvements with respect to all the three variants of the ROUGE metric as compared to that using either QMM or SVM in isolation. We also investigate using SVM to additionall</context>
</contexts>
<marker>Liu, Hakkani-Tur, 2011</marker>
<rawString>Yang Liu and Dilek Hakkani-Tur. 2011. Speech summarization. Chapter 13 in Spoken Language Understanding: Systems for Extracting Semantic Information from Speech, G. Tur and R. D. Mori (Eds), New York: Wiley.</rawString>
</citation>
<citation valid="true">
<authors>
<author>John Lafferty</author>
<author>Chengxiang Zhai</author>
</authors>
<title>Document language models, query models, and risk minimization for information retrieval.</title>
<date>2001</date>
<booktitle>In Proc. SIGIR,</booktitle>
<pages>111--119</pages>
<contexts>
<context position="3776" citStr="Lafferty and Zhai, 2001" startWordPosition="568" endWordPosition="571">ess the degree of relevance between a query and a document by computing the likelihood of the query generated by the document (usually referred to as the querylikelihood approach) (Zhai, 2008; Baeza-Yates and Ribeiro-Neto, 2011). A document is deemed to be relevant to a given query if the corresponding document model is more likely to generate the query. On the other hand, the KullbackLeibler divergence measure (denoted by KLM for short hereafter), which quantifies the degree of relevance between a document and a query from a more rigorous information-theoretic perspective, has been proposed (Lafferty and Zhai, 2001; Zhai and Lafferty, 2001b; Baeza-Yates and Ribeiro-Neto, 2011). KLM not only can be thought as a natural generalization of the querylikelihood approach, but also has the additional merit of being able to accommodate extra information cues to improve the performance of document ranking. For example, a main challenge facing such a measure is that since a given query usually consists of few words, the true information need is hard to be inferred from the surface statistics of a query. As such, one emerging stream of thought for KLM is to employ the 1474 Proceedings of the 2014 Conference on Empi</context>
</contexts>
<marker>Lafferty, Zhai, 2001</marker>
<rawString>John Lafferty and Chengxiang Zhai. 2001. Document language models, query models, and risk minimization for information retrieval. In Proc. SIGIR, pp. 111–119.</rawString>
</citation>
<citation valid="true">
<authors>
<author>Victor Lavrenko</author>
<author>W Bruce Croft</author>
</authors>
<title>Relevance-based language models.</title>
<date>2001</date>
<booktitle>In Proc. SIGIR,</booktitle>
<pages>120--127</pages>
<contexts>
<context position="6141" citStr="Lavrenko and Croft, 2001" startWordPosition="948" endWordPosition="952">ry-likelihood approach (Yi and Allan, 2009; Baeza-Yates and Ribeiro-Neto, 2011), but also has the additional merit of being able to accommodate extra information cues to improve the estimation of its component models in a systematic way for better document ranking (Zhai, 2008). Due to that a query usually consists of only a few words, the true query model P(w|Q) might not be accurately estimated by the simple ML estimator (Jelinek, 1991). There are several studies devoted to estimating a more accurate query modeling, saying that it can be approached with the pseudo-relevance feedback process (Lavrenko and Croft, 2001; Zhai and Lafferty, 2001b). However, the success depends largely on the assumption that the set of top-ranked documents, DTop={D1,D2,...,Dr,...}, obtained from an initial round of retrieval, are relevant and can be used to estimate a more accurate query language model. 2.2 Relevance Modeling Under the notion of relevance modeling (RM, often referred to as RM-1), each query Q is assumed to be associated with an unknown relevance class RQ, and documents that are relevant to the semantic content expressed in query are samples drawn from the relevance class RQ. Since there is no prior knowledge a</context>
</contexts>
<marker>Lavrenko, Croft, 2001</marker>
<rawString>Victor Lavrenko and W. Bruce Croft. 2001. Relevance-based language models. In Proc. SIGIR, pp. 120–127.</rawString>
</citation>
<citation valid="true">
<authors>
<author>Victor Lavrenko</author>
</authors>
<title>A Generative Theory of Relevance.</title>
<date>2004</date>
<tech>PhD thesis,</tech>
<institution>University of Massachusetts,</institution>
<location>Amherst.</location>
<contexts>
<context position="6952" citStr="Lavrenko, 2004" startWordPosition="1083" endWordPosition="1084">relevant and can be used to estimate a more accurate query language model. 2.2 Relevance Modeling Under the notion of relevance modeling (RM, often referred to as RM-1), each query Q is assumed to be associated with an unknown relevance class RQ, and documents that are relevant to the semantic content expressed in query are samples drawn from the relevance class RQ. Since there is no prior knowledge about RQ, we may use the top-ranked documents DTop to approximate the relevance class RQ. The corresponding relevance model can be estimated using the following equation (Lavrenko and Croft, 2001; Lavrenko, 2004): 2.3 Simple Mixture Model Another perspective of estimating an accurate query model with the top-ranked documents is the simple mixture model (SMM), which assumes that words in DTop are drawn from a twocomponent mixture model: 1) One component is the query-specific topic model PSMM(w|Q), and 2) the other is a generic background model P(w|BG). By doing so, the SMM model PSMM(w|Q) can be estimated by maximizing the likelihood over all the top-ranked documents (Zhai and Lafferty, 2001b; Tao and Zhai, 2006): where is a pre-defined weighting parameter used to control the degree of reliance between</context>
</contexts>
<marker>Lavrenko, 2004</marker>
<rawString>Victor Lavrenko. 2004. A Generative Theory of Relevance. PhD thesis, University of Massachusetts, Amherst.</rawString>
</citation>
<citation valid="true">
<authors>
<author>Shasha Xie</author>
<author>Yang Liu</author>
</authors>
<title>Improving supervised learning for meeting summarization using sampling and regression.</title>
<date>2010</date>
<journal>Computer Speech &amp; Language,</journal>
<volume>24</volume>
<issue>3</issue>
<pages>495--514</pages>
<contexts>
<context position="21319" citStr="Xie and Liu, 2010" startWordPosition="3487" endWordPosition="3490"> one among all the formulations studied in this paper for both the TD and SD cases. Going one step further, we explore to use extra prosodic features that are deemed complementary to the LM cue provided by QMM for speech summarization. To this end, a support vector machine (SVM) based summarization model is trained to integrate a set of 28 commonly-used prosodic features (Liu and Hakkani-Tur, 2011) for representing each spoken sentence, since SVM is arguably one of the state-of-the-art supervised methods that can make use of a diversity of indicative features for text or speech summarization (Xie and Liu, 2010; Chen et al., 2013). The sentence ranking scores derived by QMM and SVM are in turn integrated through a simple log-linear combination. The corresponding results are shown in Table 2, demonstrating consistent improvements with respect to all the three variants of the ROUGE metric as compared to that using either QMM or SVM in isolation. We also investigate using SVM to additionally integrate a richer set of lexical and relevance features to complement QMM and further enhance the summarization effectiveness. However, due to space limitation, we omit the details here. As a side note, there is a</context>
</contexts>
<marker>Xie, Liu, 2010</marker>
<rawString>Shasha Xie and Yang Liu. 2010. Improving supervised learning for meeting summarization using sampling and regression. Computer Speech &amp; Language, 24(3), pp. 495–514.</rawString>
</citation>
<citation valid="true">
<authors>
<author>Yuanhua Lv</author>
<author>Chengxiang Zhai</author>
</authors>
<title>A comparative study of methods for estimating query language models with pseudo feedback.</title>
<date>2009</date>
<booktitle>In Proc. CIKM,</booktitle>
<pages>1895--1898</pages>
<contexts>
<context position="4710" citStr="Lv and Zhai, 2009" startWordPosition="718" endWordPosition="721"> facing such a measure is that since a given query usually consists of few words, the true information need is hard to be inferred from the surface statistics of a query. As such, one emerging stream of thought for KLM is to employ the 1474 Proceedings of the 2014 Conference on Empirical Methods in Natural Language Processing (EMNLP), pages 1474–1480, October 25-29, 2014, Doha, Qatar. c�2014 Association for Computational Linguistics pseudo-relevance feedback process to construct an enhanced query model (or representation) so as to achieve better retrieval effectiveness (Hiemstra et al., 2004; Lv and Zhai, 2009; Carpineto and Romano, 2012; Lee and Croft, 2013). Following this line of research, the major contribution of this paper is three-fold: 1) we analyze several widely-used query models and then propose a principled framework to unify the relationships among them; 2) on top of the successfully developed query models, we propose an extended modeling formulation by incorporating additional query-specific information cues to guide the model estimation; 3) we explore a novel use of these query models by adapting them to the speech recognition and summarization tasks. As we will see, a series of expe</context>
</contexts>
<marker>Lv, Zhai, 2009</marker>
<rawString>Yuanhua Lv and Chengxiang Zhai. 2009. A comparative study of methods for estimating query language models with pseudo feedback. In Proc. CIKM, pp. 1895–1898.</rawString>
</citation>
<citation valid="true">
<authors>
<author>Yuanhua Lv</author>
<author>Chengxiang Zhai</author>
</authors>
<title>Positional relevance model for pseudo-relevance feedback.</title>
<date>2010</date>
<booktitle>In Proc. SIGIR,</booktitle>
<pages>579--586</pages>
<marker>Lv, Zhai, 2010</marker>
<rawString>Yuanhua Lv and Chengxiang Zhai. 2010. Positional relevance model for pseudo-relevance feedback. In Proc. SIGIR, pp. 579–586.</rawString>
</citation>
<citation valid="true">
<authors>
<author>Kyung Soon Lee</author>
<author>W Bruce Croft</author>
<author>James Allan</author>
</authors>
<title>A cluster-based resampling method for pseudo-relevance feedback.</title>
<date>2008</date>
<booktitle>In Proc. SIGIR,</booktitle>
<pages>235--242</pages>
<marker>Lee, Croft, Allan, 2008</marker>
<rawString>Kyung Soon Lee, W. Bruce Croft, and James Allan. 2008. A cluster-based resampling method for pseudo-relevance feedback. In Proc. SIGIR, pp. 235–242.</rawString>
</citation>
<citation valid="true">
<authors>
<author>Kyung Soon Lee</author>
<author>W Bruce Croft</author>
</authors>
<title>A deterministic resampling method using overlapping document clusters for pseudorelevance feedback.</title>
<date>2013</date>
<journal>Inf. Process. Manage.</journal>
<volume>49</volume>
<issue>4</issue>
<pages>792--806</pages>
<contexts>
<context position="4760" citStr="Lee and Croft, 2013" startWordPosition="726" endWordPosition="729">ery usually consists of few words, the true information need is hard to be inferred from the surface statistics of a query. As such, one emerging stream of thought for KLM is to employ the 1474 Proceedings of the 2014 Conference on Empirical Methods in Natural Language Processing (EMNLP), pages 1474–1480, October 25-29, 2014, Doha, Qatar. c�2014 Association for Computational Linguistics pseudo-relevance feedback process to construct an enhanced query model (or representation) so as to achieve better retrieval effectiveness (Hiemstra et al., 2004; Lv and Zhai, 2009; Carpineto and Romano, 2012; Lee and Croft, 2013). Following this line of research, the major contribution of this paper is three-fold: 1) we analyze several widely-used query models and then propose a principled framework to unify the relationships among them; 2) on top of the successfully developed query models, we propose an extended modeling formulation by incorporating additional query-specific information cues to guide the model estimation; 3) we explore a novel use of these query models by adapting them to the speech recognition and summarization tasks. As we will see, a series of experiments indeed demonstrate the effectiveness of th</context>
</contexts>
<marker>Lee, Croft, 2013</marker>
<rawString>Kyung Soon Lee and W. Bruce Croft. 2013. A deterministic resampling method using overlapping document clusters for pseudorelevance feedback. Inf. Process. Manage. 49(4), pp. 792–806.</rawString>
</citation>
<citation valid="true">
<authors>
<author>Inderjeet Mani</author>
<author>Mark T Maybury</author>
</authors>
<title>Advances in automatic text summarization.</title>
<date>1999</date>
<publisher>MIT Press.</publisher>
<location>Cambridge, MA:</location>
<contexts>
<context position="13319" citStr="Mani and Maybury, 1999" startWordPosition="2152" endWordPosition="2155">cuments from a contemporaneous (or in-domain) corpus. Finally, the enhanced query model (that is P(w|H) in speech recognition) can be estimated by RM, SMM, RSMM or QMM, and further combined with the background n-gram (e.g., trigram) language model to form an adaptive language model to guide the speech recognition process. 4.2 Speech Summarization On the other hand, extractive speech summarization aims at producing a concise summary by selecting salient sentences or paragraphs from the original spoken document according to a predefined target summarization ratio (Carbonell and Goldstein, 1998; Mani and Maybury, 1999; Nenkova and McKeown, 2011; Liu and Hakkani-Tur, 2011). Intuitively, this task could be framed as an ad-hoc IR problem, where the spoken document is treated as an information need and each sentence of the document is regarded as a candidate information unit to be retrieved according to its relevance to the information need. Therefore, KLM can be used to quantify how close the document D and one of its sentences S are: the closer the sentence model P(w|S) to the document model P(w|D), the more 1 E P( Mr) s.t. E = M Mr Q) (6) , s. DrE 1476 likely the sentence would be part of the summary. Due t</context>
</contexts>
<marker>Mani, Maybury, 1999</marker>
<rawString>Inderjeet Mani and Mark T. Maybury (Eds.). 1999. Advances in automatic text summarization. Cambridge, MA: MIT Press.</rawString>
</citation>
<citation valid="true">
<authors>
<author>Ani Nenkova</author>
<author>Kathleen McKeown</author>
</authors>
<title>Automatic summarization. Foundations and Trends in</title>
<date>2011</date>
<journal>Information Retrieval,</journal>
<volume>5</volume>
<issue>2</issue>
<pages>103--233</pages>
<contexts>
<context position="13346" citStr="Nenkova and McKeown, 2011" startWordPosition="2156" endWordPosition="2159">aneous (or in-domain) corpus. Finally, the enhanced query model (that is P(w|H) in speech recognition) can be estimated by RM, SMM, RSMM or QMM, and further combined with the background n-gram (e.g., trigram) language model to form an adaptive language model to guide the speech recognition process. 4.2 Speech Summarization On the other hand, extractive speech summarization aims at producing a concise summary by selecting salient sentences or paragraphs from the original spoken document according to a predefined target summarization ratio (Carbonell and Goldstein, 1998; Mani and Maybury, 1999; Nenkova and McKeown, 2011; Liu and Hakkani-Tur, 2011). Intuitively, this task could be framed as an ad-hoc IR problem, where the spoken document is treated as an information need and each sentence of the document is regarded as a candidate information unit to be retrieved according to its relevance to the information need. Therefore, KLM can be used to quantify how close the document D and one of its sentences S are: the closer the sentence model P(w|S) to the document model P(w|D), the more 1 E P( Mr) s.t. E = M Mr Q) (6) , s. DrE 1476 likely the sentence would be part of the summary. Due to that each sentence S of a</context>
</contexts>
<marker>Nenkova, McKeown, 2011</marker>
<rawString>Ani Nenkova and Kathleen McKeown. 2011. Automatic summarization. Foundations and Trends in Information Retrieval, 5(2–3), pp. 103–233.</rawString>
</citation>
<citation valid="true">
<authors>
<author>Stefan Ortmanns</author>
<author>Hermann Ney</author>
<author>Xavier Aubert</author>
</authors>
<title>A word graph algorithm for large vocabulary continuous speech recognition.</title>
<date>1997</date>
<journal>Computer Speech and Language,</journal>
<pages>43--72</pages>
<contexts>
<context position="15776" citStr="Ortmanns et al., 1997" startWordPosition="2564" endWordPosition="2568">tion experiments is compiled by a subset of 3-hour speech data from the corpus within 2003 (1.5 hours for development and 1.5 hours for test). The contemporaneous (in-domain) text corpus used for training the various LM adaptation methods was collected between 2001 and 2003 from the corpus (excluding the test set), which consists of one million Chinese characters of the orthographic broadcast news transcripts. In this paper, all the LM adaptation experiments were performed in word graph rescoring. The associated word graphs of the speech data were built beforehand with a typical LVCSR system (Ortmanns et al., 1997; Young et al., 2006). In addition, the summarization task also employs the same broadcast news corpus as well. A subset of 205 broadcast news documents compiled between November 2001 and August 2002 was reserved for the summarization experiments (185 for development and 20 for test). A subset of about 100,000 text news documents, compiled during the same period as the documents to be summarized, was employed to estimate the related summarization models compared in this paper. We adopted three variants of the widely-used ROUGE metric (i.e., ROUGE-1, ROGUE-2 and ROUGE-L) for the assessment of s</context>
</contexts>
<marker>Ortmanns, Ney, Aubert, 1997</marker>
<rawString>Stefan Ortmanns, Hermann Ney, and Xavier Aubert. 1997. A word graph algorithm for large vocabulary continuous speech recognition. Computer Speech and Language, pp. 43–72.</rawString>
</citation>
<citation valid="true">
<authors>
<author>Jay M Ponte</author>
<author>W Bruce Croft</author>
</authors>
<title>A language modeling approach to information retrieval.</title>
<date>1998</date>
<booktitle>In Proc. SIGIR,</booktitle>
<pages>275--281</pages>
<contexts>
<context position="2756" citStr="Ponte and Croft, 1998" startWordPosition="399" endWordPosition="402">tant content from multimedia collections, or provide locations of important speech segments in a video accompanied with their corresponding transcripts, for users to listen to or to digest. Statistical language modeling (LM) (Jelinek, 1999; Jurafsky and Martin, 2008; Zhai, 2008), which manages to quantify the acceptability of a given word sequence in a natural language or capture the statistical characteristics of a given piece of text, has been proved to offer both efficient and effective modeling abilities in many practical applications of natural language processing and speech recognition (Ponte and Croft, 1998; Jelinek, 1999; Huang, et al., 2001; Zhai and Lafferty, 2001a; Jurafsky and Martin, 2008; Furui et al., 2012; Liu and Hakkani-Tur, 2011). The LM approach was first introduced for the information retrieval (IR) problems in the late 1990s, indicating very good potential, and was subsequently extended in a wide array of followup studies. One typical realization of the LM approach for IR is to access the degree of relevance between a query and a document by computing the likelihood of the query generated by the document (usually referred to as the querylikelihood approach) (Zhai, 2008; Baeza-Yate</context>
<context position="9580" citStr="Ponte and Croft, 1998" startWordPosition="1511" endWordPosition="1514">lved in KLM: one for the document and the other for the query. The divergence of the document model with respect to the query model is defined by H H DrEDTop wEV 1475 representative query models mentioned above is how to capitalize on the set of top-ranked documents and the original query. Several subtle relationships can be deduced through the following in-depth analysis. First, a direct inspiration of the LM-based query reformulation framework can be drawn from the celebrated Rocchio’s formulation, while the former can be viewed as a probabilistic counterpart of the latter (Robertson, 1990; Ponte and Croft, 1998; Baeza-Yates and RibeiroNeto, 2011). Second, after some mathematical manipulation, the formulation of the RM model (c.f. Eq. (2)) can be rewritten as (5) It becomes evident that the RM model is composed by mixing a set of document models P(w|Dr). As such, the RM model bears a close resemblance to the Rocchio’s formulation. Furthermore, based on Eq. (5), we can recast the estimation of the RM model as an optimization problem, and the likelihood (or objective) function is formulated as I c(w, L= ri EP(w |Dr)P(Dr |Q) E wmV Dr DTop t. EP(Dr |Q) = 1 DTop where the document models are known in adva</context>
</contexts>
<marker>Ponte, Croft, 1998</marker>
<rawString>Jay M. Ponte and W. Bruce Croft. 1998. A language modeling approach to information retrieval. In Proc. SIGIR, pp. 275–281.</rawString>
</citation>
<citation valid="true">
<authors>
<author>Stephen E Robertson</author>
</authors>
<title>On term selection for query expansion.</title>
<date>1990</date>
<journal>Journal of Documentation,</journal>
<volume>46</volume>
<issue>4</issue>
<pages>359--364</pages>
<contexts>
<context position="9557" citStr="Robertson, 1990" startWordPosition="1509" endWordPosition="1510">e models are involved in KLM: one for the document and the other for the query. The divergence of the document model with respect to the query model is defined by H H DrEDTop wEV 1475 representative query models mentioned above is how to capitalize on the set of top-ranked documents and the original query. Several subtle relationships can be deduced through the following in-depth analysis. First, a direct inspiration of the LM-based query reformulation framework can be drawn from the celebrated Rocchio’s formulation, while the former can be viewed as a probabilistic counterpart of the latter (Robertson, 1990; Ponte and Croft, 1998; Baeza-Yates and RibeiroNeto, 2011). Second, after some mathematical manipulation, the formulation of the RM model (c.f. Eq. (2)) can be rewritten as (5) It becomes evident that the RM model is composed by mixing a set of document models P(w|Dr). As such, the RM model bears a close resemblance to the Rocchio’s formulation. Furthermore, based on Eq. (5), we can recast the estimation of the RM model as an optimization problem, and the likelihood (or objective) function is formulated as I c(w, L= ri EP(w |Dr)P(Dr |Q) E wmV Dr DTop t. EP(Dr |Q) = 1 DTop where the document m</context>
</contexts>
<marker>Robertson, 1990</marker>
<rawString>Stephen E. Robertson. 1990. On term selection for query expansion. Journal of Documentation, 46(4), pp. 359–364.</rawString>
</citation>
<citation valid="true">
<authors>
<author>Andreas Stolcke</author>
</authors>
<title>SRILM - An extensible language modeling toolkit.</title>
<date>2005</date>
<booktitle>In Proc. INTERSPEECH,</booktitle>
<pages>901--904</pages>
<contexts>
<context position="17071" citStr="Stolcke, 2005" startWordPosition="2775" endWordPosition="2776">io of the number of words in the automatic (or manual) summary to that in the reference transcript of a spoken document, was set to 10% in this research. 6 Experimental Results In the first part of experiments, we evaluate the effectiveness of the various query models applied to the speech recognition task. The corresponding results with respect to different numbers of top-ranked documents being used for estimating their component models are shown in Table 1. Also worth mentioning is that the baseline system with the background trigram language model, which was trained with the SRILM toolkit (Stolcke, 2005) and Good-Turing smoothing (Jelinek, 1999), results in a Chinese character error rate (CER) of 20.08% on the test set. Consulting Table 1 we notice two particularities. One is that there is more fluctuation in the CER results of SMM than in those of RM. The reason might be that, for SMM, the extraction of relevance information from the top-ranked documents is conducted with no involvement of the test utterance (i.e., the query; or its corresponding search histories), as elaborated earlier in Section 2. When too many feedback documents are being used, there would be a concern for SMM to be dist</context>
</contexts>
<marker>Stolcke, 2005</marker>
<rawString>Andreas Stolcke. 2005. SRILM - An extensible language modeling toolkit. In Proc. INTERSPEECH, pp.901–904.</rawString>
</citation>
<citation valid="true">
<authors>
<author>Tao Tao</author>
<author>Chengxiang Zhai</author>
</authors>
<title>Regularized estimation of mixture models for robust pseudo-relevance feedback.</title>
<date>2006</date>
<booktitle>In Proc. SIGIR,</booktitle>
<pages>162--169</pages>
<contexts>
<context position="7461" citStr="Tao and Zhai, 2006" startWordPosition="1164" endWordPosition="1167">nding relevance model can be estimated using the following equation (Lavrenko and Croft, 2001; Lavrenko, 2004): 2.3 Simple Mixture Model Another perspective of estimating an accurate query model with the top-ranked documents is the simple mixture model (SMM), which assumes that words in DTop are drawn from a twocomponent mixture model: 1) One component is the query-specific topic model PSMM(w|Q), and 2) the other is a generic background model P(w|BG). By doing so, the SMM model PSMM(w|Q) can be estimated by maximizing the likelihood over all the top-ranked documents (Zhai and Lafferty, 2001b; Tao and Zhai, 2006): where is a pre-defined weighting parameter used to control the degree of reliance between PSMM(w|Q) and P(w|BG). This estimation will enable more specific words to receive more probability mass, thereby leading to a more discriminative query model PSMM(w|Q). Although the SMM modeling aims to extract extra word usage cues for enhanced query modeling, it may confront two intrinsic problems. One is the extraction of word usage cues from DTop is not guided by the original query. The other is that the mixing coefficient is fixed across all top-ranked documents albeit that different documents woul</context>
</contexts>
<marker>Tao, Zhai, 2006</marker>
<rawString>Tao Tao and Chengxiang Zhai. 2006. Regularized estimation of mixture models for robust pseudo-relevance feedback. In Proc. SIGIR, pp. 162–169.</rawString>
</citation>
<citation valid="true">
<authors>
<author>Yik-Cheung Tam</author>
<author>Tanja Schultz</author>
</authors>
<title>Dynamic language model adaptation using variational Bayes inference.</title>
<date>2005</date>
<booktitle>In Proc. INTERSPEECH,</booktitle>
<pages>5--8</pages>
<contexts>
<context position="18628" citStr="Tam and Schultz, 2005" startWordPosition="3040" endWordPosition="3043">ntain too much noisy information so as to bias the model estimation. Furthermore, it is evident that the proposed QMM is the best-performing method among all the query models compared in the paper. Although the improvements made by QMM are not as pronounced as expected, we believe that QMM has demonstrated its potential to be applied to other related applications. On the other hand, we compare the various query models with two well-practiced language models, namely the cache model (Cache) (Kuhn and Mori, 1990; Jelinek et al., 1991) and the latent Dirichlet allocation (LDA) (Liu and Liu, 2007; Tam and Schultz, 2005). The CER results of these two models are also shown in Table 1, respectively. For the cache model, bigram cache was used since it can yield better results than the unigram and trigram cache models in our experiments. It is worthy to notice that the LDA model was trained with the entire set of contemporaneous text document collection (c.f. Section 4), while all of the query models explored in the paper were estimated based on a subset of the corpus selected by an initial round of retrieval. The results reveal that most of these query models can achieve superior performance over the two convent</context>
</contexts>
<marker>Tam, Schultz, 2005</marker>
<rawString>Yik-Cheung Tam and Tanja Schultz. 2005. Dynamic language model adaptation using variational Bayes inference. In Proc. INTERSPEECH, pp. 5–8.</rawString>
</citation>
<citation valid="true">
<authors>
<author>Xuanhui Wang</author>
<author>Hui Fang</author>
<author>Chengxiang Zhai</author>
</authors>
<title>A study of methods for negative relevance feedback.</title>
<date>2008</date>
<booktitle>In Proc. SIGIR,</booktitle>
<pages>219--226</pages>
<marker>Wang, Fang, Zhai, 2008</marker>
<rawString>Xuanhui Wang, Hui Fang, and Chengxiang Zhai. 2008. A study of methods for negative relevance feedback. In Proc. SIGIR, pp. 219–226.</rawString>
</citation>
<citation valid="true">
<authors>
<author>Hsin-Min Wang</author>
<author>Berlin Chen</author>
<author>Jen-Wei Kuo</author>
<author>Shih-Sian Cheng</author>
</authors>
<title>MATBN: A Mandarin Chinese broadcast news corpus.</title>
<date>2005</date>
<journal>International Journal of Computational Linguistics &amp; Chinese Language Processing,</journal>
<volume>10</volume>
<issue>2</issue>
<pages>219--236</pages>
<contexts>
<context position="14550" citStr="Wang et al., 2005" startWordPosition="2369" endWordPosition="2372">entence S of a spoken document D to be summarized usually consists of only a few words, the corresponding sentence model P(wjS) might not be appropriately estimated by the ML estimation. To alleviate the deficiency, we can leverage the merit of the above query modeling techniques to estimate an accurate sentence model for each sentence to enhance the summarization performance. 5 Experimental Setup The speech corpus consists of about 196 hours of Mandarin broadcast news collected by the Academia Sinica and the Public Television Service Foundation of Taiwan between November 2001 and April 2003 (Wang et al., 2005), which is publicly available and has been segmented into separate stories and transcribed manually. Each story contains the speech of one studio anchor, as well as several field reporters and interviewees. A subset of 25-hour speech data compiled during November 2001 to December 2002 was used to bootstrap the acoustic model training. The vocabulary size is about 72 thousand words. The background language model was estimated from a background text corpus consisting of 170 million Chinese characters collected from the Chinese Gigaword Corpus released by LDC. The dataset for use in the speech re</context>
</contexts>
<marker>Wang, Chen, Kuo, Cheng, 2005</marker>
<rawString>Hsin-Min Wang, Berlin Chen, Jen-Wei Kuo, and Shih-Sian Cheng. 2005. MATBN: A Mandarin Chinese broadcast news corpus. International Journal of Computational Linguistics &amp; Chinese Language Processing, 10(2), pp. 219–236.</rawString>
</citation>
<citation valid="true">
<authors>
<author>Xing Yi</author>
<author>James Allan</author>
</authors>
<title>A comparative study of utilizing topic models for information retrieval.</title>
<date>2009</date>
<booktitle>In Proc. ECIR,</booktitle>
<pages>29--41</pages>
<contexts>
<context position="5559" citStr="Yi and Allan, 2009" startWordPosition="854" endWordPosition="857">y the relationships among them; 2) on top of the successfully developed query models, we propose an extended modeling formulation by incorporating additional query-specific information cues to guide the model estimation; 3) we explore a novel use of these query models by adapting them to the speech recognition and summarization tasks. As we will see, a series of experiments indeed demonstrate the effectiveness of the proposed models on these two tasks. KL(Q||D)wVP(w|Q) logP(w|Q) (1) P(w|D) KLM not only can be thought as a natural generalization of the traditional query-likelihood approach (Yi and Allan, 2009; Baeza-Yates and Ribeiro-Neto, 2011), but also has the additional merit of being able to accommodate extra information cues to improve the estimation of its component models in a systematic way for better document ranking (Zhai, 2008). Due to that a query usually consists of only a few words, the true query model P(w|Q) might not be accurately estimated by the simple ML estimator (Jelinek, 1991). There are several studies devoted to estimating a more accurate query modeling, saying that it can be approached with the pseudo-relevance feedback process (Lavrenko and Croft, 2001; Zhai and Laffert</context>
</contexts>
<marker>Yi, Allan, 2009</marker>
<rawString>Xing Yi and James Allan. 2009. A comparative study of utilizing topic models for information retrieval. In Proc. ECIR, pp. 29–41.</rawString>
</citation>
<citation valid="true">
<authors>
<author>Steve Young</author>
<author>Dan Kershaw</author>
<author>Julian Odell</author>
<author>Dave Ollason</author>
<author>Valtcho Valtchev</author>
<author>Phil Woodland</author>
</authors>
<title>The HTK book version 3.4.</title>
<date>2006</date>
<publisher>Cambridge University Press.</publisher>
<contexts>
<context position="15797" citStr="Young et al., 2006" startWordPosition="2569" endWordPosition="2572">piled by a subset of 3-hour speech data from the corpus within 2003 (1.5 hours for development and 1.5 hours for test). The contemporaneous (in-domain) text corpus used for training the various LM adaptation methods was collected between 2001 and 2003 from the corpus (excluding the test set), which consists of one million Chinese characters of the orthographic broadcast news transcripts. In this paper, all the LM adaptation experiments were performed in word graph rescoring. The associated word graphs of the speech data were built beforehand with a typical LVCSR system (Ortmanns et al., 1997; Young et al., 2006). In addition, the summarization task also employs the same broadcast news corpus as well. A subset of 205 broadcast news documents compiled between November 2001 and August 2002 was reserved for the summarization experiments (185 for development and 20 for test). A subset of about 100,000 text news documents, compiled during the same period as the documents to be summarized, was employed to estimate the related summarization models compared in this paper. We adopted three variants of the widely-used ROUGE metric (i.e., ROUGE-1, ROGUE-2 and ROUGE-L) for the assessment of summarization performa</context>
</contexts>
<marker>Young, Kershaw, Odell, Ollason, Valtchev, Woodland, 2006</marker>
<rawString>Steve Young, Dan Kershaw, Julian Odell, Dave Ollason, Valtcho Valtchev, and Phil Woodland. 2006. The HTK book version 3.4. Cambridge University Press.</rawString>
</citation>
<citation valid="true">
<authors>
<author>Chengxiang Zhai</author>
<author>John Lafferty</author>
</authors>
<title>A study of smoothing methods for language models applied to ad hoc information retrieval.</title>
<date>2001</date>
<booktitle>In Proc. SIGIR,</booktitle>
<pages>334--342</pages>
<contexts>
<context position="2817" citStr="Zhai and Lafferty, 2001" startWordPosition="410" endWordPosition="413">ons of important speech segments in a video accompanied with their corresponding transcripts, for users to listen to or to digest. Statistical language modeling (LM) (Jelinek, 1999; Jurafsky and Martin, 2008; Zhai, 2008), which manages to quantify the acceptability of a given word sequence in a natural language or capture the statistical characteristics of a given piece of text, has been proved to offer both efficient and effective modeling abilities in many practical applications of natural language processing and speech recognition (Ponte and Croft, 1998; Jelinek, 1999; Huang, et al., 2001; Zhai and Lafferty, 2001a; Jurafsky and Martin, 2008; Furui et al., 2012; Liu and Hakkani-Tur, 2011). The LM approach was first introduced for the information retrieval (IR) problems in the late 1990s, indicating very good potential, and was subsequently extended in a wide array of followup studies. One typical realization of the LM approach for IR is to access the degree of relevance between a query and a document by computing the likelihood of the query generated by the document (usually referred to as the querylikelihood approach) (Zhai, 2008; Baeza-Yates and Ribeiro-Neto, 2011). A document is deemed to be relevan</context>
<context position="6166" citStr="Zhai and Lafferty, 2001" startWordPosition="953" endWordPosition="956"> and Allan, 2009; Baeza-Yates and Ribeiro-Neto, 2011), but also has the additional merit of being able to accommodate extra information cues to improve the estimation of its component models in a systematic way for better document ranking (Zhai, 2008). Due to that a query usually consists of only a few words, the true query model P(w|Q) might not be accurately estimated by the simple ML estimator (Jelinek, 1991). There are several studies devoted to estimating a more accurate query modeling, saying that it can be approached with the pseudo-relevance feedback process (Lavrenko and Croft, 2001; Zhai and Lafferty, 2001b). However, the success depends largely on the assumption that the set of top-ranked documents, DTop={D1,D2,...,Dr,...}, obtained from an initial round of retrieval, are relevant and can be used to estimate a more accurate query language model. 2.2 Relevance Modeling Under the notion of relevance modeling (RM, often referred to as RM-1), each query Q is assumed to be associated with an unknown relevance class RQ, and documents that are relevant to the semantic content expressed in query are samples drawn from the relevance class RQ. Since there is no prior knowledge about RQ, we may use the t</context>
<context position="7439" citStr="Zhai and Lafferty, 2001" startWordPosition="1160" endWordPosition="1163">nce class RQ. The corresponding relevance model can be estimated using the following equation (Lavrenko and Croft, 2001; Lavrenko, 2004): 2.3 Simple Mixture Model Another perspective of estimating an accurate query model with the top-ranked documents is the simple mixture model (SMM), which assumes that words in DTop are drawn from a twocomponent mixture model: 1) One component is the query-specific topic model PSMM(w|Q), and 2) the other is a generic background model P(w|BG). By doing so, the SMM model PSMM(w|Q) can be estimated by maximizing the likelihood over all the top-ranked documents (Zhai and Lafferty, 2001b; Tao and Zhai, 2006): where is a pre-defined weighting parameter used to control the degree of reliance between PSMM(w|Q) and P(w|BG). This estimation will enable more specific words to receive more probability mass, thereby leading to a more discriminative query model PSMM(w|Q). Although the SMM modeling aims to extract extra word usage cues for enhanced query modeling, it may confront two intrinsic problems. One is the extraction of word usage cues from DTop is not guided by the original query. The other is that the mixing coefficient is fixed across all top-ranked documents albeit that di</context>
</contexts>
<marker>Zhai, Lafferty, 2001</marker>
<rawString>Chengxiang Zhai and John Lafferty. 2001a. A study of smoothing methods for language models applied to ad hoc information retrieval. In Proc. SIGIR, pp. 334–342.</rawString>
</citation>
<citation valid="true">
<authors>
<author>Chengxiang Zhai</author>
<author>John Lafferty</author>
</authors>
<title>Model-based feedback in the language modeling approach to information retrieval.</title>
<date>2001</date>
<booktitle>In Proc. CIKM,</booktitle>
<pages>403--410</pages>
<contexts>
<context position="2817" citStr="Zhai and Lafferty, 2001" startWordPosition="410" endWordPosition="413">ons of important speech segments in a video accompanied with their corresponding transcripts, for users to listen to or to digest. Statistical language modeling (LM) (Jelinek, 1999; Jurafsky and Martin, 2008; Zhai, 2008), which manages to quantify the acceptability of a given word sequence in a natural language or capture the statistical characteristics of a given piece of text, has been proved to offer both efficient and effective modeling abilities in many practical applications of natural language processing and speech recognition (Ponte and Croft, 1998; Jelinek, 1999; Huang, et al., 2001; Zhai and Lafferty, 2001a; Jurafsky and Martin, 2008; Furui et al., 2012; Liu and Hakkani-Tur, 2011). The LM approach was first introduced for the information retrieval (IR) problems in the late 1990s, indicating very good potential, and was subsequently extended in a wide array of followup studies. One typical realization of the LM approach for IR is to access the degree of relevance between a query and a document by computing the likelihood of the query generated by the document (usually referred to as the querylikelihood approach) (Zhai, 2008; Baeza-Yates and Ribeiro-Neto, 2011). A document is deemed to be relevan</context>
<context position="6166" citStr="Zhai and Lafferty, 2001" startWordPosition="953" endWordPosition="956"> and Allan, 2009; Baeza-Yates and Ribeiro-Neto, 2011), but also has the additional merit of being able to accommodate extra information cues to improve the estimation of its component models in a systematic way for better document ranking (Zhai, 2008). Due to that a query usually consists of only a few words, the true query model P(w|Q) might not be accurately estimated by the simple ML estimator (Jelinek, 1991). There are several studies devoted to estimating a more accurate query modeling, saying that it can be approached with the pseudo-relevance feedback process (Lavrenko and Croft, 2001; Zhai and Lafferty, 2001b). However, the success depends largely on the assumption that the set of top-ranked documents, DTop={D1,D2,...,Dr,...}, obtained from an initial round of retrieval, are relevant and can be used to estimate a more accurate query language model. 2.2 Relevance Modeling Under the notion of relevance modeling (RM, often referred to as RM-1), each query Q is assumed to be associated with an unknown relevance class RQ, and documents that are relevant to the semantic content expressed in query are samples drawn from the relevance class RQ. Since there is no prior knowledge about RQ, we may use the t</context>
<context position="7439" citStr="Zhai and Lafferty, 2001" startWordPosition="1160" endWordPosition="1163">nce class RQ. The corresponding relevance model can be estimated using the following equation (Lavrenko and Croft, 2001; Lavrenko, 2004): 2.3 Simple Mixture Model Another perspective of estimating an accurate query model with the top-ranked documents is the simple mixture model (SMM), which assumes that words in DTop are drawn from a twocomponent mixture model: 1) One component is the query-specific topic model PSMM(w|Q), and 2) the other is a generic background model P(w|BG). By doing so, the SMM model PSMM(w|Q) can be estimated by maximizing the likelihood over all the top-ranked documents (Zhai and Lafferty, 2001b; Tao and Zhai, 2006): where is a pre-defined weighting parameter used to control the degree of reliance between PSMM(w|Q) and P(w|BG). This estimation will enable more specific words to receive more probability mass, thereby leading to a more discriminative query model PSMM(w|Q). Although the SMM modeling aims to extract extra word usage cues for enhanced query modeling, it may confront two intrinsic problems. One is the extraction of word usage cues from DTop is not guided by the original query. The other is that the mixing coefficient is fixed across all top-ranked documents albeit that di</context>
</contexts>
<marker>Zhai, Lafferty, 2001</marker>
<rawString>Chengxiang Zhai and John Lafferty. 2001b. Model-based feedback in the language modeling approach to information retrieval. In Proc. CIKM, pp. 403–410.</rawString>
</citation>
<citation valid="true">
<authors>
<author>Chengxiang Zhai</author>
</authors>
<title>Statistical language models for information retrieval: a critical review. Foundations and Trends</title>
<date>2008</date>
<booktitle>in Information Retrieval,</booktitle>
<volume>2</volume>
<issue>3</issue>
<pages>137--213</pages>
<contexts>
<context position="2414" citStr="Zhai, 2008" startWordPosition="349" endWordPosition="350"> such as broadcast radio and television programs, digital libraries and so on, are made available to the public. Research on multimedia content understanding and organization has witnessed a booming interest over the past decade. By virtue of the developed techniques, a variety of functionalities were created to help distill important content from multimedia collections, or provide locations of important speech segments in a video accompanied with their corresponding transcripts, for users to listen to or to digest. Statistical language modeling (LM) (Jelinek, 1999; Jurafsky and Martin, 2008; Zhai, 2008), which manages to quantify the acceptability of a given word sequence in a natural language or capture the statistical characteristics of a given piece of text, has been proved to offer both efficient and effective modeling abilities in many practical applications of natural language processing and speech recognition (Ponte and Croft, 1998; Jelinek, 1999; Huang, et al., 2001; Zhai and Lafferty, 2001a; Jurafsky and Martin, 2008; Furui et al., 2012; Liu and Hakkani-Tur, 2011). The LM approach was first introduced for the information retrieval (IR) problems in the late 1990s, indicating very goo</context>
<context position="5794" citStr="Zhai, 2008" startWordPosition="893" endWordPosition="894"> use of these query models by adapting them to the speech recognition and summarization tasks. As we will see, a series of experiments indeed demonstrate the effectiveness of the proposed models on these two tasks. KL(Q||D)wVP(w|Q) logP(w|Q) (1) P(w|D) KLM not only can be thought as a natural generalization of the traditional query-likelihood approach (Yi and Allan, 2009; Baeza-Yates and Ribeiro-Neto, 2011), but also has the additional merit of being able to accommodate extra information cues to improve the estimation of its component models in a systematic way for better document ranking (Zhai, 2008). Due to that a query usually consists of only a few words, the true query model P(w|Q) might not be accurately estimated by the simple ML estimator (Jelinek, 1991). There are several studies devoted to estimating a more accurate query modeling, saying that it can be approached with the pseudo-relevance feedback process (Lavrenko and Croft, 2001; Zhai and Lafferty, 2001b). However, the success depends largely on the assumption that the set of top-ranked documents, DTop={D1,D2,...,Dr,...}, obtained from an initial round of retrieval, are relevant and can be used to estimate a more accurate quer</context>
</contexts>
<marker>Zhai, 2008</marker>
<rawString>Chengxiang Zhai. 2008. Statistical language models for information retrieval: a critical review. Foundations and Trends in Information Retrieval, 2 (3), pp. 137–213.</rawString>
</citation>
<citation valid="true">
<authors>
<author>Yi Zhang</author>
<author>Jamie Callan</author>
<author>Thomas Minka</author>
</authors>
<title>Novelty and redundancy detection in adaptive filtering.</title>
<date>2002</date>
<booktitle>In Proc. SIGIR,</booktitle>
<pages>81--88</pages>
<marker>Zhang, Callan, Minka, 2002</marker>
<rawString>Yi Zhang, Jamie Callan, and Thomas Minka. 2002. Novelty and redundancy detection in adaptive filtering. In Proc. SIGIR, pp. 81–88.</rawString>
</citation>
</citationList>
</algorithm>
</algorithms>
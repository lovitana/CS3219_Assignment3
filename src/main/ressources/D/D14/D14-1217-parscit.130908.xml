<?xml version="1.0" encoding="UTF-8"?>
<algorithms version="110505">
<algorithm name="SectLabel" version="110505">
<variant no="0" confidence="0.000000">
<title confidence="0.994748">
Learning Spatial Knowledge for Text to 3D Scene Generation
</title>
<author confidence="0.993624">
Angel X. Chang, Manolis Savva and Christopher D. Manning
</author>
<affiliation confidence="0.980917">
Stanford University
</affiliation>
<email confidence="0.995751">
{angelx,msavva,manning}@cs.stanford.edu
</email>
<sectionHeader confidence="0.993828" genericHeader="abstract">
Abstract
</sectionHeader>
<bodyText confidence="0.99997525">
We address the grounding of natural lan-
guage to concrete spatial constraints, and
inference of implicit pragmatics in 3D en-
vironments. We apply our approach to the
task of text-to-3D scene generation. We
present a representation for common sense
spatial knowledge and an approach to ex-
tract it from 3D scene data. In text-to-
3D scene generation, a user provides as in-
put natural language text from which we
extract explicit constraints on the objects
that should appear in the scene. The main
innovation of this work is to show how
to augment these explicit constraints with
learned spatial knowledge to infer missing
objects and likely layouts for the objects
in the scene. We demonstrate that spatial
knowledge is useful for interpreting natu-
ral language and show examples of learned
knowledge and generated 3D scenes.
</bodyText>
<sectionHeader confidence="0.998993" genericHeader="keywords">
1 Introduction
</sectionHeader>
<bodyText confidence="0.999136529411765">
To understand language, we need an understanding
of the world around us. Language describes the
world and provides symbols with which we rep-
resent meaning. Still, much knowledge about the
world is so obvious that it is rarely explicitly stated.
It is uncommon for people to state that chairs are
usually on the floor and upright, and that you usu-
ally eat a cake from a plate on a table. Knowledge
of such common facts provides the context within
which people communicate with language. There-
fore, to create practical systems that can interact
with the world and communicate with people, we
need to leverage such knowledge to interpret lan-
guage in context.
Spatial knowledge is an important aspect of the
world and is often not expressed explicitly in nat-
ural language. This is one of the biggest chal-
</bodyText>
<figureCaption confidence="0.886892">
Figure 1: Generated scene for “There is a room
</figureCaption>
<bodyText confidence="0.966770870967742">
with a chair and a computer.” Note that the system
infers the presence of a desk and that the computer
should be supported by the desk.
lenges in grounding language and enabling natu-
ral communication between people and intelligent
systems. For instance, if we want a robot that can
follow commands such as “bring me a piece of
cake”, it needs to be imparted with an understand-
ing of likely locations for the cake in the kitchen
and that the cake should be placed on a plate.
The pioneering WordsEye system (Coyne and
Sproat, 2001) addressed the text-to-3D task and is
an inspiration for our work. However, there are
many remaining gaps in this broad area. Among
them, there is a need for research into learning spa-
tial knowledge representations from data, and for
connecting them to language. Representing un-
stated facts is a challenging problem unaddressed
by prior work and the focus of our contribution.
This problem is a counterpart to the image descrip-
tion problem (Kulkarni et al., 2011; Mitchell et al.,
2012; Elliott and Keller, 2013), which has so far
remained largely unexplored by the community.
We present a representation for this form of spa-
tial knowledge that we learn from 3D scene data
and connect to natural language. We will show
how this representation is useful for grounding
language and for inferring unstated facts, i.e., the
pragmatics of language describing physical envi-
ronments. We demonstrate the use of this repre-
sentation in the task of text-to-3D scene genera-
</bodyText>
<page confidence="0.945345">
2028
</page>
<note confidence="0.951429">
Proceedings of the 2014 Conference on Empirical Methods in Natural Language Processing (EMNLP), pages 2028–2038,
October 25-29, 2014, Doha, Qatar. c�2014 Association for Computational Linguistics
</note>
<figure confidence="0.920080516129032">
Input Text a) Scene Template b) Geometric Scene c) 3D Scene
o0
room
supports(o0,o1) supports(o0,o2)
Room
color(red)
Parse
Infer
table
o4
plate
o1
supports(o1,o4)
supports(o4,o3)
right(o2,o1)
o2
chair
o3
cake
Ground
Layout
Table
Plate
Cake
Chair
Render
View
“There is a room with
a table and a cake.
There is a red chair to
the right of the table.”
</figure>
<figureCaption confidence="0.84516">
Figure 2: Overview of our spatial knowledge representation for text-to-3D scene generation. We parse
</figureCaption>
<bodyText confidence="0.999450433333333">
input text into a scene template and infer implicit spatial constraints from learned priors. We then ground
the template to a geometric scene, choose 3D models to instantiate and arrange them into a final 3D scene.
tion, where the input is natural language and the
desired output is a 3D scene.
We focus on the text-to-3D task to demonstrate
that extracting spatial knowledge is possible and
beneficial in a challenging scenario: one requiring
the grounding of natural language and inference of
rarely mentioned implicit pragmatics based on spa-
tial facts. Figure 1 illustrates some of the inference
challenges in generating 3D scenes from natural
language: the desk was not explicitly mentioned
in the input, but we need to infer that the computer
is likely to be supported by a desk rather than di-
rectly placed on the floor. Without this inference,
the user would need to be much more verbose with
text such as “There is a room with a chair, a com-
puter, and a desk. The computer is on the desk, and
the desk is on the floor. The chair is on the floor.”
Contributions We present a spatial knowledge
representation that can be learned from 3D scenes
and captures the statistics of what objects occur
in different scene types, and their spatial posi-
tions relative to each other. In addition, we model
spatial relations (left, on top of, etc.) and learn a
mapping between language and the geometric con-
straints that spatial terms imply. We show that
using our learned spatial knowledge representa-
tion, we can infer implicit constraints, and generate
plausible scenes from concise natural text input.
</bodyText>
<sectionHeader confidence="0.8049" genericHeader="introduction">
2 Task Definition and Overview
</sectionHeader>
<bodyText confidence="0.999119903225807">
We define text-to-scene generation as the task of
taking text that describes a scene as input, and gen-
erating a plausible 3D scene described by that text
as output. More concretely, based on the input
text, we select objects from a dataset of 3D models
and arrange them to generate output scenes.
The main challenge we address is in transform-
ing a scene template into a physically realizable 3D
scene. For this to be possible, the system must be
able to automatically specify the objects present
and their position and orientation with respect to
each other as constraints in 3D space. To do so, we
need to have a representation of scenes (§3). We
need good priors over the arrangements of objects
in scenes (§4) and we need to be able to ground
textual relations into spatial constraints (§5). We
break down our task as follows (see Figure 2):
Template Parsing (§6.1): Parse the textual de-
scription of a scene into a set of constraints on the
objects present and spatial relations between them.
Inference (§6.2): Expand this set of constraints by
accounting for implicit constraints not specified in
the text using learned spatial priors.
Grounding (§6.3): Given the constraints and pri-
ors on the spatial relations of objects, transform the
scene template into a geometric 3D scene with a set
of objects to be instantiated.
Scene Layout (§6.4): Arrange the objects and op-
timize their placement based on priors on the rel-
ative positions of objects and explicitly provided
spatial constraints.
</bodyText>
<sectionHeader confidence="0.984464" genericHeader="method">
3 Scene Representation
</sectionHeader>
<bodyText confidence="0.999854357142857">
To capture the objects present and their arrange-
ment, we represent scenes as graphs where nodes
are objects in the scene, and edges are semantic re-
lationships between the objects.
We represent the semantics of a scene using a
scene template and the geometric properties using
a geometric scene. One critical property which is
captured by our scene graph representation is that
of a static support hierarchy, i.e., the order in which
bigger objects physically support smaller ones: the
floor supports tables, which support plates, which
can support cakes. Static support and other con-
straints on relationships between objects are rep-
resented as edges in the scene graph.
</bodyText>
<page confidence="0.983916">
2029
</page>
<figure confidence="0.926164333333333">
Kitchen
0.0 0.1 0.2 0.3 0.4 0.5 0.6 0.7 0.8 0.9 1.0
p(Scene|Knife+Table)
</figure>
<figureCaption confidence="0.847496">
Figure 3: Probabilities of different scene types
given the presence of “knife” and “table”.
Figure 4: Probabilities of support for some most
likely child object categories given four different
parent object categories, from top left clockwise:
dining table, bookcase, room, desk.
</figureCaption>
<subsectionHeader confidence="0.999029">
3.1 Scene Template
</subsectionHeader>
<bodyText confidence="0.999968166666667">
A scene template T = (O, C, Cs) consists of a
set of object descriptions O = {oi, ... , on} and
constraints C = {ci, ... , ck} on the relationships
between the objects. A scene template also has a
scene type Cs.
Each object oi, has properties associated with
it such as category label, basic attributes such as
color and material, and number of occurrences in
the scene. For constraints, we focus on spatial re-
lations between objects, expressed as predicates of
the form supported_by(oi, oj) or left(oi, oj) where
oi and oj are recognized objects.1 Figure 2a shows
an example scene template. From the scene tem-
plate we instantiate concrete geometric 3D scenes.
To infer implicit constraints on objects and spa-
tial support we learn priors on object occurrences
in 3D scenes (§4.1) and their support hierarchies
(§4.2).
</bodyText>
<subsectionHeader confidence="0.999644">
3.2 Geometric Scene
</subsectionHeader>
<bodyText confidence="0.9999852">
We refer to the concrete geometric representation
of a scene as a “geometric scene”. It consists of
a set of 3D model instances – one for each ob-
ject – that capture the appearance of the object. A
transformation matrix that represents the position,
orientation, and scaling of the object in a scene is
also necessary to exactly position the object. We
generate a geometric scene from a scene template
by selecting appropriate models from a 3D model
database and determining transformations that op-
</bodyText>
<footnote confidence="0.98255">
1Our representation can also support other relationships
such as larger(oi, oj).
</footnote>
<bodyText confidence="0.99885725">
timize their layout to satisfy spatial constraints. To
inform geometric arrangement we learn priors on
the types of support surfaces (§4.2) and the relative
positions of objects (§4.4).
</bodyText>
<sectionHeader confidence="0.988393" genericHeader="method">
4 Spatial Knowledge
</sectionHeader>
<bodyText confidence="0.999870461538462">
Our model of spatial knowledge relies on the idea
of abstract scene types describing the occurrence
and arrangement of different categories of objects
within scenes of that type. For example, kitchens
typically contain kitchen counters on which plates
and cups are likely to be found. The type of scene
and category of objects condition the spatial rela-
tionships that can exist in a scene.
We learn spatial knowledge from 3D scene data,
basing our approach on that of Fisher et al. (2012)
and using their dataset of 133 small indoor scenes
created with 1723 Trimble 3D Warehouse mod-
els (Fisher et al., 2012).
</bodyText>
<subsectionHeader confidence="0.995289">
4.1 Object Occurrence Priors
</subsectionHeader>
<bodyText confidence="0.99995">
We learn priors for object occurrence in different
scene types (such as kitchens, offices, bedrooms).
</bodyText>
<equation confidence="0.822434">
Pocc(CoICs) = count (C(in j s)
count Cs
</equation>
<bodyText confidence="0.999948333333333">
This allows us to evaluate the probability of dif-
ferent scene types given lists of object occurring
in them (see Figure 3). For example given input of
the form “there is a knife on the table” then we are
likely to generate a scene with a dining table and
other related objects.
</bodyText>
<subsectionHeader confidence="0.997913">
4.2 Support Hierarchy Priors
</subsectionHeader>
<bodyText confidence="0.9992299">
We observe the static support relations of objects
in existing scenes to establish a prior over what ob-
jects go on top of what other objects. As an exam-
ple, by observing plates and forks on tables most
of the time, we establish that tables are more likely
to support plates and forks than chairs. We esti-
mate the probability of a parent category Cp sup-
porting a given child category Cc as a simple con-
ditional probability based on normalized observa-
tion counts.2
</bodyText>
<equation confidence="0.945743">
Psupport(CpICc) = count(Cc)
</equation>
<bodyText confidence="0.999982333333333">
We show a few of the priors we learn in Figure 4
as likelihoods of categories of child objects being
statically supported by a parent category object.
</bodyText>
<footnote confidence="0.7694285">
2The support hierarchy is explicitly modeled in the scene
dataset we use.
</footnote>
<figure confidence="0.516766">
Living Room Kitchen Counter Dining Table
count(Cc on Cp)
2030
</figure>
<figureCaption confidence="0.72607775">
Figure 5: Predicted positions using learned rela-
tive position priors for chair given desk (top left),
poster-room (top right), mouse-desk (bottom left),
keyboard-desk (bottom right).
</figureCaption>
<equation confidence="0.997456">
Relation P(relation)
V ol(AnB)
V ol(A)
1 -V ol(AnB)
V ol(A)
Vol(An left_of (B))
V ol(A)
Vol(An right_of (B))
V ol(A)
I(dist(A, B) &lt; tnear)
cos(front(A), c(B) − c(A))
</equation>
<tableCaption confidence="0.7153045">
Table 1: Definitions of spatial relation using
bounding boxes. Note: dist(A, B) is normalized
</tableCaption>
<bodyText confidence="0.823198">
against the maximum extent of the bounding box
of B. front(A) is the direction of the front vector
of A and c(A) is the centroid of A.
</bodyText>
<figure confidence="0.655532181818182">
Keyword Top Relations and Scores
inside(A,B)
outside(A,B)
left_of(A,B)
right_of(A,B)
near(A,B)
faces(A,B)
behind
adjacent
below
front
left
above
opposite
on
near
next
under
top
inside
right
beside
</figure>
<equation confidence="0.958088666666667">
(back_of, 0.46), (back_side, 0.33)
(front_side, 0.27), (outside, 0.26)
(below, 0.59), (lower_side, 0.38)
(front_of, 0.41), (front_side, 0.40)
(left_side, 0.44), (left_of, 0.43)
(above, 0.37), (near, 0.30)
(outside, 0.31), (next_to, 0.30)
(supported_by, 0.86), (on_top_of, 0.76)
(outside, 0.66), (near, 0.66)
(outside, 0.49), (near, 0.48)
(supports, 0.62), (below, 0.53)
(supported_by,0.65),(above,0.61)
(inside, 0.48), (supported_by, 0.35)
(right_of, 0.50), (lower_side, 0.38)
(outside, 0.45), (right_of, 0.45)
</equation>
<subsectionHeader confidence="0.614275">
4.3 Support Surface Priors
</subsectionHeader>
<bodyText confidence="0.987906533333333">
To identify which surfaces on parent objects sup-
port child objects, we first segment parent models
into planar surfaces using a simple region-growing
algorithm based on (Kalvin and Taylor, 1996). We
characterize support surfaces by the direction of
their normal vector, limited to the six canonical
directions: up, down, left, right, front, back. We
learn a probability of supporting surface normal di-
rection Sn given child object category Cc. For ex-
ample, posters are typically found on walls so their
support normal vectors are in the horizontal di-
rections. Any unobserved child categories are as-
sumed to have Psurf(Sn = up|Cc) = 1 since most
things rest on a horizontal surface (e.g., floor).
count(Cc on surface with Sn)
</bodyText>
<equation confidence="0.948241">
Psurf(Sn|Cc) = count(Cc)
</equation>
<subsectionHeader confidence="0.977792">
4.4 Relative Position Priors
</subsectionHeader>
<bodyText confidence="0.998904857142857">
We model the relative positions of objects based
on their object categories and current scene type:
i.e., the relative position of an object of category
Cobj is with respect to another object of category
Cref and for a scene type Cs. We condition on the
relationship R between the two objects, whether
they are siblings (R = Sibling) or child-parent
</bodyText>
<equation confidence="0.9733135">
(R = ChildParent).
Prelpos(x, y, 0|Cobj, Cref, Cs, R)
</equation>
<bodyText confidence="0.91611275">
When positioning objects, we restrict the search
space to points on the selected support surface.
The position x, y is the centroid of the target ob-
ject projected onto the support surface in the se-
mantic frame of the reference object. The 0 is the
angle between the front of the two objects. We rep-
resent these relative position and orientation pri-
ors by performing kernel density estimation on the
Table 2: Map of top keywords to spatial relations
(appropriate mappings in bold).
observed samples. Figure 5 shows predicted posi-
tions of objects using the learned priors.
</bodyText>
<sectionHeader confidence="0.996899" genericHeader="method">
5 Spatial Relations
</sectionHeader>
<bodyText confidence="0.999784571428571">
We define a set of formal spatial relations that we
map to natural language terms (§5.1). In addi-
tion, we collect annotations of spatial relation de-
scriptions from people, learn a mapping of spatial
keywords to our formal spatial relations, and train
a classifier that given two objects can predict the
likelihood of a spatial relation holding (§5.2).
</bodyText>
<subsectionHeader confidence="0.983949">
5.1 Predefined spatial relations
</subsectionHeader>
<bodyText confidence="0.999784625">
For spatial relations we use a set ofpredefined rela-
tions: left_of, right_of, above, below, front, back,
supported_by, supports, next_to, near, inside, out-
side, faces, left_side, right_side.3 These are mea-
sured using axis-aligned bounding boxes from the
viewer’s perspective; the involved bounding boxes
are compared to determine volume overlap or clos-
est distance (for proximity relations; see Table 1).
</bodyText>
<footnote confidence="0.732391333333333">
3We distinguish left_of(A,B) as A being left ofthe left edge
of the bounding box of B vs left_side(A,B) as A being left of
the centroid of B.
</footnote>
<page confidence="0.961682">
2031
</page>
<figure confidence="0.974424521739131">
Feature
# Description
delta(A, B)
dist(A, B)
overlap(A, f(B))
overlap(A, B)
support(A, B)
Delta position (x, y, z) between the centroids of A and B
Normalized distance (wrt B) between the centroids of A and B
Fraction of A inside left/right/front/back/top/bottom regions wrt B: V ol( V ol (A)
))
V ol(AnB) V ol(An B)
V ol(A) and Vol(B)
supported_by(A, B) and supports(A, B)
3
1
6
2
2
Table 3: Features for trained spatial relations predictor.
Above On
Left Right
Front Behind
</figure>
<figureCaption confidence="0.999699">
Figure 6: Our data collection task.
</figureCaption>
<bodyText confidence="0.998491666666667">
Since these spatial relations are resolved with re-
spect to the view of the scene, they correspond to
view-centric definitions of spatial concepts.
</bodyText>
<subsectionHeader confidence="0.99961">
5.2 Learning Spatial Relations
</subsectionHeader>
<bodyText confidence="0.994771">
We collect a set of text descriptions of spatial rela-
tionships between two objects in 3D scenes by run-
ning an experiment on Amazon Mechanical Turk.
We present a set of screenshots of scenes in our
dataset that highlight particular pairs of objects and
we ask people to fill in a spatial relationship of the
form “The __ is __ the __” (see Fig 6). We col-
lected a total of 609 annotations over 131 object
pairs in 17 scenes. We use this data to learn pri-
ors on view-centric spatial relation terms and their
concrete geometric interpretation.
For each response, we select one keyword from
the text based on length. We learn a mapping of
the top 15 keywords to our predefined set of spa-
tial relations. We use our predefined relations on
annotated spatial pairs of objects to create a binary
indicator vector that is set to 1 if the spatial relation
holds, or zero otherwise. We then create a simi-
lar vector for whether the keyword appeared in the
annotation for that spatial pair, and then compute
the cosine similarity of the two vectors to obtain
a score for mapping keywords to spatial relations.
Table 2 shows the obtained mapping. Using just
the top mapping, we are able to map 10 of the 15
Figure 7: High probability regions where the cen-
ter of another object would occur for some spatial
relations with respect to a table: above (top left),
on (top right), left (mid left), right (mid right), in
front (bottom left), behind (bottom right).
keywords to an appropriate spatial relation. The 5
keywords that are not well mapped are proximity
relations that are not well captured by our prede-
fined spatial relations.
Using the 15 keywords as our spatial relations,
we train a log linear binary classifier for each key-
word over features of the objects involved in that
spatial relation (see Table 3). We then use this
model to predict the likelihood of that spatial re-
lation in new scenes.
Figure 7 shows examples of predicted likeli-
hoods for different spatial relations with respect to
an anchor object in a scene. Note that the learned
spatial relations are much stricter than our prede-
fined relations. For instance, “above” is only used
to referred to the area directly above the table, not
to the region above and to the left or above and in
front (which our predefined classifier will all con-
sider to be above). In our results, we show we have
more accurate scenes using the trained spatial re-
lations than the predefined ones.
</bodyText>
<page confidence="0.951069">
2032
</page>
<figure confidence="0.612819666666667">
Dependency Pattern
Example Text
{tag:VBN}=verb &gt;nsubjpass {}=nsubj &gt;prep ({}=prep &gt;pobj {}=pobj)
</figure>
<equation confidence="0.966132266666666">
attribute(verb,pobj)(nsubj,pobj)
{}=dobj &gt;cop {} &gt;nsubj {}=nsubj
attribute(dobj)(nsubj,dobj)
The chair[nsubj] is made[verb] of[prep] wood[pobj].
material(chair,wood)
The chair[nsubj] is red[dobj].
color(chair,red)
{}=dobj &gt;cop {} &gt;nsubj {}=nsubj &gt;prep ({}=prep &gt;pobj {}=pobj)
spatial(dobj)(nsubj, pobj)
{}=nsubj &gt;advmod ({}=advmod &gt;prep ({}=prep &gt;pobj {}=pobj))
spatial(advmod)(nsubj, pobj)
The table[nsubj] is next[dobj] to[prep] the chair[pobj].
next_to(table,chair)
There is a table[nsubj] next[advmod] to[prep] a chair[pobj].
next_to(table,chair)
</equation>
<tableCaption confidence="0.992567">
Table 4: Example dependency patterns for extracting attributes and spatial relations.
</tableCaption>
<sectionHeader confidence="0.921145" genericHeader="method">
6 Text to Scene generation
</sectionHeader>
<bodyText confidence="0.9995295">
We generate 3D scenes from brief scene descrip-
tions using our learned priors.
</bodyText>
<subsectionHeader confidence="0.99542">
6.1 Scene Template Parsing
</subsectionHeader>
<bodyText confidence="0.99791754054054">
During scene template parsing we identify the
scene type, the objects present in the scene, their
attributes, and the relations between them. The
input text is first processed using the Stanford
CoreNLP pipeline (Manning et al., 2014). The
scene type is determined by matching the words
in the utterance against a list of known scene types
from the scene dataset.
To identify objects, we look for noun phrases
and use the head word as the category, filtering
with WordNet (Miller, 1995) to determine which
objects are visualizable (under the physical object
synset, excluding locations). We use the Stanford
coreference system to determine when the same
object is being referred to.
To identify properties of the objects, we extract
other adjectives and nouns in the noun phrase. We
also match dependency patterns such as “X is made
of Y” to extract additional attributes. Based on the
object category and attributes, and other words in
the noun phrase mentioning the object, we identify
a set of associated keywords to be used later for
querying the 3D model database.
Dependency patterns are also used to extract
spatial relations between objects (see Table 4 for
some example patterns). We use Semgrex patterns
to match the input text to dependencies (Cham-
bers et al., 2007). The attribute types are deter-
mined from a dictionary using the text express-
ing the attribute (e.g., attribute(red)=color, at-
tribute(round)=shape). Likewise, spatial relations
are looked up using the learned map of keywords
to spatial relations.
As an example, given the input “There is a room
with a desk and a red chair. The chair is to the left
of the desk.” we extract the following objects and
spatial relations:
</bodyText>
<table confidence="0.929486">
Objects category attributes keywords
o0 room color:red room
o1 desk desk
o2 chair chair, red
Relations: left(o2, o1)
</table>
<subsectionHeader confidence="0.983478">
6.2 Inferring Implicits
</subsectionHeader>
<bodyText confidence="0.999511541666666">
From the parsed scene template, we infer the pres-
ence of additional objects and support constraints.
We can optionally infer the presence of addi-
tional objects from object occurrences based on the
scene type. If the scene type is unknown, we use
the presence of known object categories to pre-
dict the most likely scene type by using Bayes’
rule on our object occurrence priors Po, to get
P(C3|{Co}) ∝ Po.({Co}|C3)P(C3). Once we
have a scene type C3, we sample Po,, to find ob-
jects that are likely to occur in the scene. We re-
strict sampling to the top n = 4 object categories.
We can also use the support hierarchy priors
P3,,pport to infer implicit objects. For instance, for
each object of we find the most likely supporting
object category and add it to our scene if not al-
ready present.
After inferring implicit objects, we infer the sup-
port constraints. Using the learned text to prede-
fined relation mapping from §5.2, we can map the
keywords “on” and “top” to the supported_by re-
lation. We infer the rest of the support hierarchy
by selecting for each object of the parent object oj
that maximizes P3,,pport(Coy|Coz).
</bodyText>
<subsectionHeader confidence="0.998475">
6.3 Grounding Objects
</subsectionHeader>
<bodyText confidence="0.9999118">
Once we determine from the input text what ob-
jects exist and their spatial relations, we select 3D
models matching the objects and their associated
properties. Each object in the scene template is
grounded by querying a 3D models database with
</bodyText>
<page confidence="0.959264">
2033
</page>
<figure confidence="0.996744333333333">
Input Text Basic +Support Hierarchy +Relative Positions
No Relations Predefined Relations Learned Relations
UPMA
There is a desk and
a keyboard and a
monitor.&amp;quot;
</figure>
<figureCaption confidence="0.649679625">
&amp;quot;There is a coffee table
and there is a lamp
behind the coffee table.
There is a chair in front of
the coffee table.&amp;quot;
Figure 8: Top Generated scenes for randomly placing objects on the floor (Basic), with inferred Support
Hierarchy, and with priors on Relative Positions. Bottom Generated scenes with no understanding of
spatial relations (No Relations), scoring using Predefined Relations and Learned Relations.
</figureCaption>
<bodyText confidence="0.99466525">
the appropriate category and keywords.
We use a 3D model dataset collected from
Google 3D Warehouse by prior work in scene syn-
thesis and containing about 12490 mostly indoor
objects (Fisher et al., 2012). These models have
text associated with them in the form of names and
tags. In addition, we semi-automatically annotated
models with object category labels (roughly 270
classes). We used model tags to set these labels,
and verified and augmented them manually.
In addition, we automatically rescale models so
that they have physically plausible sizes and orient
them so that they have a consistent up and front
direction (Savva et al., 2014). We then indexed all
models in a database that we query at run-time for
retrieval based on category and tag labels.
</bodyText>
<subsectionHeader confidence="0.993991">
6.4 Scene Layout
</subsectionHeader>
<bodyText confidence="0.9999756">
Once we have instantiated the objects in the scene
by selecting models, we aim to optimize an over-
all layout score L = AobjLobj + ArelLrel that is
a weighted sum of object arrangement Lobj score
and constraint satisfaction Lrel score:
</bodyText>
<equation confidence="0.98788925">
∑Lobj = ∑Psurf(S�|Coi) Prelpos(�)
oi ojEF(oi)
∑Lrel = Prel(Cz)
ci
</equation>
<bodyText confidence="0.972625">
where F(oz) are the sibling objects and parent ob-
ject of oz. We use Aobj = 0.25 and Arel = 0.75 for
the results we present.
We use a simple hill climbing strategy to find a
reasonable layout. We first initialize the positions
Figure 9: Generated scene for “There is a room
with a desk and a lamp. There is a chair to the
right of the desk.” The inferred scene hierarchy is
overlayed in the center.
of objects within the scene by traversing the sup-
port hierarchy in depth-first order, positioning the
children from largest to first and recursing. Child
nodes are positioned by first selecting a supporting
surface on a candidate parent object through sam-
pling of Psurf. After selecting a surface, we sam-
ple a position on the surface based on Prelpos. Fi-
nally, we check whether collisions exist with other
objects, rejecting layouts where collisions occur.
We iterate by randomly jittering and repositioning
objects. If there are any spatial constraints that are
not satisfied, we also remove and randomly repo-
sition the objects violating the constraints, and it-
erate to improve the layout. The resulting scene is
rendered and presented to the user.
</bodyText>
<sectionHeader confidence="0.99858" genericHeader="evaluation">
7 Results and Discussion
</sectionHeader>
<bodyText confidence="0.999736">
We show examples of generated scenes, and com-
pare against naive baselines to demonstrate learned
priors are essential for scene generation. We
</bodyText>
<page confidence="0.996672">
2034
</page>
<figureCaption confidence="0.996964333333333">
Figure 10: Generated scene for “There is a room
with a poster bed and a poster.”
Figure 11: Generated scene for “living room”.
</figureCaption>
<bodyText confidence="0.999778952380952">
also discuss interesting aspects of using spatial
knowledge in view-based object referent resolu-
tion (§7.2) and in disambiguating geometric inter-
pretations of “on” (§7.3).
Model Comparison Figure 8 shows a compari-
son of scenes generated by our model versus sev-
eral simpler baselines. The top row shows the im-
pact of modeling the support hierarchy and the rel-
ative positions in the layout of the scene. The bot-
tom row shows that the learned spatial relations
can give a more accurate layout than the naive
predefined spatial relations, since it captures prag-
matic implicatures of language, e.g., left is only
used for directly left and not top left or bottom
left (Vogel et al., 2013).
Figure 12: Left: chair is selected using “the chair
to the right of the table” or “the object to the right of
the table”. Chair is not selected for “the cup to the
right of the table”. Right: Different view results
in different chair being selected for the input “the
chair to the right of the table”.
</bodyText>
<subsectionHeader confidence="0.988949">
7.1 Generated Scenes
</subsectionHeader>
<bodyText confidence="0.9999039375">
Support Hierarchy Figure 9 shows a generated
scene along with the input text and support hier-
archy. Even though the spatial relation between
lamp and desk was not mentioned, we infer that the
lamp is supported by the top surface of the desk.
Disambiguation Figure 10 shows a generated
scene for the input “There is a room with a poster
bed and a poster”. Note that the system differen-
tiates between a “poster” and a “poster bed” – it
correctly selects and places the bed on the floor,
while the poster is placed on the wall.
Inferring objects for a scene type Figure 11
shows an example of inferring all the objects
present in a scene from the input “living room”.
Some of the placements are good, while others can
clearly be improved.
</bodyText>
<subsectionHeader confidence="0.989743">
7.2 View-centric object referent resolution
</subsectionHeader>
<bodyText confidence="0.999958214285714">
After a scene is generated, the user can refer to ob-
jects with their categories and with spatial relations
between them. Objects are disambiguated by both
category and view-centric spatial relations. We use
the WordNet hierarchy to resolve hyponym or hy-
pernym referents to objects in the scene. In Fig-
ure 12 (left), the user can select a chair to the right
of the table using the phrase “chair to the right of
the table” or “object to the right of the table”. The
user can then change their viewpoint by rotating
and moving around. Since spatial relations are re-
solved with respect to the current viewpoint, a dif-
ferent chair is selected for the same phrase from a
different viewpoint in the right screenshot.
</bodyText>
<subsectionHeader confidence="0.999026">
7.3 Disambiguating “on”
</subsectionHeader>
<bodyText confidence="0.9999546">
As shown in §5.2, the English preposition “on”,
when used as a spatial relation, corresponds
strongly to the supported_by relation. In our
trained model, the supported_by feature also has
a high positive weight for “on”.
Our model for supporting surfaces and hierar-
chy allows interpreting the placement of “A on
B” based on the categories of A and B. Fig-
ure 13 demonstrates four different interpretations
for “on”. Given the input “There is a cup on the
table” the system correctly places the cup on the
top surface of the table. In contrast, given “There
is a cup on the bookshelf”, the cup is placed on a
supporting surface of the bookshelf, but not nec-
essarily the top one which would be fairly high.
</bodyText>
<page confidence="0.988101">
2035
</page>
<figureCaption confidence="0.568772">
Figure 13: From top left clockwise: “There is a
</figureCaption>
<bodyText confidence="0.988414">
cup on the table”, “There is a cup on the book-
shelf”, “There is a poster on the wall”, “There is
a hat on the chair”. Note the different geometric
interpretations of “on”.
Given the input “There is a poster on the wall”, a
poster is pasted on the wall, while with the input
“There is a hat on the chair” the hat is placed on
the seat of the chair.
</bodyText>
<subsectionHeader confidence="0.99522">
7.4 Limitations
</subsectionHeader>
<bodyText confidence="0.999375826086957">
While the system shows promise, there are still
many challenges in text-to-scene generation. For
one, we did not address the difficulties of resolving
objects. A failure case of our system stems from
using a fixed set of categories to identify visualiz-
able objects. For example, the sense of “top” refer-
ring to a spinning top, and other uncommon object
types, are not handled by our system as concrete
objects. Furthermore, complex phrases including
object parts such as “there’s a coat on the seat of
the chair” are not handled. Figure 14 shows some
Figure 14: Left: A water bottle instead of wine
bottle is selected for “There is a bottle of wine on
the table in the kitchen”. In addition, the selected
table is inappropriate for a kitchen. Right: A floor
lamp is incorrectly selected for the input “There is
a lamp on the table”.
example cases where the context is important in
selecting an appropriate object and the difficulties
of interpreting noun phrases.
In addition, we rely on a few dependency pat-
terns for extracting spatial relations so robustness
to variations in spatial language is lacking. We
only handle binary spatial relations (e.g., “left”,
“behind”) ignoring more complex relations such as
“around the table” or “in the middle of the room”.
Though simple binary relations are some of the
most fundamental spatial expressions and a good
first step, handling more complex expressions will
do much to improve the system.
Another issue is that the interpretation of sen-
tences such as “the desk is covered with paper”,
which entails many pieces of paper placed on the
desk, is hard to resolve. With a more data-driven
approach we can hope to link such expressions to
concrete facts.
Finally, we use a traditional pipeline approach
for text processing, so errors in initial stages
can propagate downstream. Failures in depen-
dency parsing, part of speech tagging, or coref-
erence resolution can result in incorrect interpre-
tations of the input language. For example, in
the sentence “there is a desk with a chair in front
of it”, “it” is not identified as coreferent with
“desk” so we fail to extract the spatial relation
front_of(chair, desk).
</bodyText>
<sectionHeader confidence="0.999908" genericHeader="related work">
8 Related Work
</sectionHeader>
<bodyText confidence="0.999934666666667">
There is related prior work in the topics of mod-
eling spatial relations, generating 3D scenes from
text, and automatically laying out 3D scenes.
</bodyText>
<subsectionHeader confidence="0.998932">
8.1 Spatial knowledge and relations
</subsectionHeader>
<bodyText confidence="0.9999694">
Prior work that required modeling spatial knowl-
edge has defined representations specific to the
task addressed. Typically, such knowledge is man-
ually provided or crowdsourced – not learned from
data. For instance, WordsEye (Coyne et al., 2010)
uses a set of manually specified relations. The
NLP community has explored grounding text to
physical attributes and relations (Matuszek et al.,
2012; Krishnamurthy and Kollar, 2013), gener-
ating text for referring to objects (FitzGerald et
al., 2013) and connecting language to spatial re-
lationships (Vogel and Jurafsky, 2010; Golland et
al., 2010; Artzi and Zettlemoyer, 2013). Most
of this work focuses on learning a mapping from
text to formal representations, and does not model
</bodyText>
<page confidence="0.974083">
2036
</page>
<bodyText confidence="0.998012">
implicit spatial knowledge. Many priors on real
world spatial facts are typically unstated in text and
remain largely unaddressed.
</bodyText>
<subsectionHeader confidence="0.994184">
8.2 Text to Scene Systems
</subsectionHeader>
<bodyText confidence="0.999993379310345">
Early work on the SHRDLU system (Winograd,
1972) gives a good formalization of the linguis-
tic manipulation of objects in 3D scenes. By re-
stricting the discourse domain to a micro-world
with simple geometric shapes, the SHRDLU sys-
tem demonstrated parsing of natural language in-
put for manipulating scenes. However, generaliza-
tion to more complex objects and spatial relations
is still very hard to attain.
More recently, a pioneering text-to-3D scene
generation prototype system has been presented by
WordsEye (Coyne and Sproat, 2001). The authors
demonstrated the promise of text to scene genera-
tion systems but also pointed out some fundamen-
tal issues which restrict the success of their system:
much spatial knowledge is required which is hard
to obtain. As a result, users have to use unnatural
language (e.g., “the stool is 1 feet to the south of
the table”) to express their intent. Follow up work
has attempted to collect spatial knowledge through
crowd-sourcing (Coyne et al., 2012), but does not
address the learning of spatial priors.
We address the challenge of handling natural
language for scene generation, by learning spatial
knowledge from 3D scene data, and using it to in-
fer unstated implicit constraints. Our work is simi-
lar in spirit to recent work on generating 2D clipart
for sentences using probabilistic models learned
from data (Zitnick et al., 2013).
</bodyText>
<subsectionHeader confidence="0.996892">
8.3 Automatic Scene Layout
</subsectionHeader>
<bodyText confidence="0.999967363636364">
Work on scene layout has focused on determining
good furniture layouts by optimizing energy func-
tions that capture the quality of a proposed layout.
These energy functions are encoded from design
guidelines (Merrell et al., 2011) or learned from
scene data (Fisher et al., 2012). Knowledge of ob-
ject co-occurrences and spatial relations is repre-
sented by simple models such as mixtures of Gaus-
sians on pairwise object positions and orientations.
We leverage ideas from this work, but they do not
focus on linking spatial knowledge to language.
</bodyText>
<sectionHeader confidence="0.9876" genericHeader="conclusions">
9 Conclusion and Future Work
</sectionHeader>
<bodyText confidence="0.999994647058824">
We have demonstrated a representation of spatial
knowledge that can be learned from 3D scene data
and how it corresponds to natural language. We
also showed that spatial inference and grounding is
critical for achieving plausible results in the text-
to-3D scene generation task. Spatial knowledge is
critically useful not only in this task, but also in
other domains which require an understanding of
the pragmatics of physical environments.
We only presented a deterministic approach for
mapping input text to the parsed scene template.
An interesting avenue for future research is to
automatically learn how to parse text describing
scenes into formal representations by using more
advanced semantic parsing methods.
We can also improve the representation used for
spatial priors of objects in scenes. For instance, in
this paper we represented support surfaces by their
orientation. We can improve the representation by
modeling whether a surface is an interior or exte-
rior surface.
Another interesting line of future work would
be to explore the influence of object identity in de-
termining when people use ego-centric or object-
centric spatial reference models, and to improve
resolution of spatial terms that have different in-
terpretations (e.g., “the chair to the left of John” vs
“the chair to the left of the table”).
Finally, a promising line of research is to explore
using spatial priors for resolving ambiguities dur-
ing parsing. For example, the attachment of “next
to” in “Put a lamp on the table next to the book” can
be readily disambiguated with spatial priors such
as the ones we presented.
</bodyText>
<sectionHeader confidence="0.998226" genericHeader="acknowledgments">
Acknowledgments
</sectionHeader>
<bodyText confidence="0.999918818181818">
We thank the anonymous reviewers for their
thoughtful comments. We gratefully acknowl-
edge the support of the Defense Advanced Re-
search Projects Agency (DARPA) Deep Explo-
ration and Filtering of Text (DEFT) Program under
Air Force Research Laboratory (AFRL) contract
no. FA8750-13-2-0040. Any opinions, findings,
and conclusion or recommendations expressed in
this material are those of the authors and do not
necessarily reflect the view of the DARPA, AFRL,
or the US government.
</bodyText>
<sectionHeader confidence="0.999399" genericHeader="references">
References
</sectionHeader>
<reference confidence="0.9919135">
Yoav Artzi and Luke Zettlemoyer. 2013. Weakly su-
pervised learning of semantic parsers for mapping
instructions to actions. Transactions of the Associ-
ation for Computational Linguistics.
</reference>
<page confidence="0.840945">
2037
</page>
<reference confidence="0.99878652173913">
Nathanael Chambers, Daniel Cer, Trond Grenager,
David Hall, Chloe Kiddon, Bill MacCartney, Marie-
Catherine de Marneffe, Daniel Ramage, Eric Yeh,
and Christopher D. Manning. 2007. Learning align-
ments and leveraging natural logic. In Proceedings
of the ACL-PASCAL Workshop on Textual Entail-
ment and Paraphrasing.
Bob Coyne and Richard Sproat. 2001. WordsEye: an
automatic text-to-scene conversion system. In Pro-
ceedings of the 28th annual conference on Computer
graphics and interactive techniques.
Bob Coyne, Richard Sproat, and Julia Hirschberg.
2010. Spatial relations in text-to-scene conversion.
In Computational Models ofSpatial Language Inter-
pretation, Workshop at Spatial Cognition.
Bob Coyne, Alexander Klapheke, Masoud Rouhizadeh,
Richard Sproat, and Daniel Bauer. 2012. Annota-
tion tools and knowledge representation for a text-to-
scene system. Proceedings of COLING 2012: Tech-
nical Papers.
Desmond Elliott and Frank Keller. 2013. Image de-
scription using visual dependency representations.
In Proceedings of Empirical Methods in Natural
Language Processing (EMNLP).
Matthew Fisher, Daniel Ritchie, Manolis Savva,
Thomas Funkhouser, and Pat Hanrahan. 2012.
Example-based synthesis of 3D object arrangements.
ACM Transactions on Graphics (TOG).
Nicholas FitzGerald, Yoav Artzi, and Luke Zettle-
moyer. 2013. Learning distributions over logical
forms for referring expression generation. In Pro-
ceedings ofEmpirical Methods in Natural Language
Processing (EMNLP).
Dave Golland, Percy Liang, and Dan Klein. 2010.
A game-theoretic approach to generating spatial de-
scriptions. In Proceedings of Empirical Methods in
Natural Language Processing (EMNLP).
Alan D. Kalvin and Russell H. Taylor. 1996. Super-
faces: Polygonal mesh simplification with bounded
error. Computer Graphics and Applications, IEEE.
Jayant Krishnamurthy and Thomas Kollar. 2013.
Jointly learning to parse and perceive: Connecting
natural language to the physical world. Transactions
of the Association for Computational Linguistics.
Girish Kulkarni, Visruth Premraj, Sagnik Dhar, Siming
Li, Yejin Choi, Alexander C. Berg, and Tamara L.
Berg. 2011. Baby talk: Understanding and generat-
ing simple image descriptions. In Computer Vision
and Pattern Recognition (CVPR), 2011 IEEE Con-
ference on.
Christopher D. Manning, Mihai Surdeanu, John Bauer,
Jenny Finkel, Steven J. Bethard, and David Mc-
Closky. 2014. The Stanford CoreNLP natural lan-
guage processing toolkit. In Proceedings of the
52nd Annual Meeting of the Association for Com-
putational Linguistics: System Demonstrations.
Cynthia Matuszek, Nicholas Fitzgerald, Luke Zettle-
moyer, Liefeng Bo, and Dieter Fox. 2012. A joint
model of language and perception for grounded at-
tribute learning. In International Conference on Ma-
chine Learning (ICML).
Paul Merrell, Eric Schkufza, Zeyang Li, Maneesh
Agrawala, and Vladlen Koltun. 2011. Interactive
furniture layout using interior design guidelines. In
ACM Transactions on Graphics (TOG).
George A. Miller. 1995. WordNet: a lexical database
for English. Communications of the ACM.
Margaret Mitchell, Xufeng Han, Jesse Dodge, Alyssa
Mensch, Amit Goyal, Alex Berg, Kota Yamaguchi,
Tamara Berg, Karl Stratos, and Hal Daumé III. 2012.
Midge: Generating image descriptions from com-
puter vision detections. In Proceedings of the 13th
Conference of the European Chapter of the Associa-
tion for Computational Linguistics.
Manolis Savva, Angel X. Chang, Gilbert Bernstein,
Christopher D. Manning, and Pat Hanrahan. 2014.
On being the right scale: Sizing large collections of
3D models. Stanford University Technical Report
CSTR 2014-03.
Adam Vogel and Dan Jurafsky. 2010. Learning to fol-
low navigational directions. In Proceedings ofACL.
Adam Vogel, Christopher Potts, and Dan Jurafsky.
2013. Implicatures and nested beliefs in approxi-
mate Decentralized-POMDPs. In Proceedings ofthe
51st Annual Meeting of the Association for Compu-
tational Linguistics.
Terry Winograd. 1972. Understanding natural lan-
guage. Cognitive psychology.
C. Lawrence Zitnick, Devi Parikh, and Lucy Vander-
wende. 2013. Learning the visual interpretation
of sentences. In IEEE Intenational Conference on
Computer Vision (ICCV).
</reference>
<page confidence="0.994196">
2038
</page>
</variant>
</algorithm>
<algorithm name="ParsHed" version="110505">
<variant no="0" confidence="0.984650">
<title confidence="0.999853">Learning Spatial Knowledge for Text to 3D Scene Generation</title>
<author confidence="0.998757">Angel X Chang</author>
<author confidence="0.998757">Manolis Savva</author>
<author confidence="0.998757">D Christopher</author>
<affiliation confidence="0.999964">Stanford University</affiliation>
<email confidence="0.999064">angelx@cs.stanford.edu</email>
<email confidence="0.999064">msavva@cs.stanford.edu</email>
<email confidence="0.999064">manning@cs.stanford.edu</email>
<abstract confidence="0.999373714285714">We address the grounding of natural language to concrete spatial constraints, and inference of implicit pragmatics in 3D environments. We apply our approach to the task of text-to-3D scene generation. We present a representation for common sense spatial knowledge and an approach to extract it from 3D scene data. In text-to- 3D scene generation, a user provides as input natural language text from which we extract explicit constraints on the objects that should appear in the scene. The main innovation of this work is to show how to augment these explicit constraints with learned spatial knowledge to infer missing objects and likely layouts for the objects in the scene. We demonstrate that spatial knowledge is useful for interpreting natural language and show examples of learned knowledge and generated 3D scenes.</abstract>
</variant>
</algorithm>
<algorithm name="ParsCit" version="110505">
<citationList>
<citation valid="true">
<authors>
<author>Yoav Artzi</author>
<author>Luke Zettlemoyer</author>
</authors>
<title>Weakly supervised learning of semantic parsers for mapping instructions to actions. Transactions of the Association for Computational Linguistics.</title>
<date>2013</date>
<contexts>
<context position="32653" citStr="Artzi and Zettlemoyer, 2013" startWordPosition="5408" endWordPosition="5411">ations Prior work that required modeling spatial knowledge has defined representations specific to the task addressed. Typically, such knowledge is manually provided or crowdsourced – not learned from data. For instance, WordsEye (Coyne et al., 2010) uses a set of manually specified relations. The NLP community has explored grounding text to physical attributes and relations (Matuszek et al., 2012; Krishnamurthy and Kollar, 2013), generating text for referring to objects (FitzGerald et al., 2013) and connecting language to spatial relationships (Vogel and Jurafsky, 2010; Golland et al., 2010; Artzi and Zettlemoyer, 2013). Most of this work focuses on learning a mapping from text to formal representations, and does not model 2036 implicit spatial knowledge. Many priors on real world spatial facts are typically unstated in text and remain largely unaddressed. 8.2 Text to Scene Systems Early work on the SHRDLU system (Winograd, 1972) gives a good formalization of the linguistic manipulation of objects in 3D scenes. By restricting the discourse domain to a micro-world with simple geometric shapes, the SHRDLU system demonstrated parsing of natural language input for manipulating scenes. However, generalization to </context>
</contexts>
<marker>Artzi, Zettlemoyer, 2013</marker>
<rawString>Yoav Artzi and Luke Zettlemoyer. 2013. Weakly supervised learning of semantic parsers for mapping instructions to actions. Transactions of the Association for Computational Linguistics.</rawString>
</citation>
<citation valid="true">
<authors>
<author>Nathanael Chambers</author>
<author>Daniel Cer</author>
<author>Trond Grenager</author>
<author>David Hall</author>
<author>Chloe Kiddon</author>
<author>Bill MacCartney</author>
<author>MarieCatherine de Marneffe</author>
<author>Daniel Ramage</author>
<author>Eric Yeh</author>
<author>Christopher D Manning</author>
</authors>
<title>Learning alignments and leveraging natural logic.</title>
<date>2007</date>
<booktitle>In Proceedings of the ACL-PASCAL Workshop on Textual Entailment and Paraphrasing.</booktitle>
<marker>Chambers, Cer, Grenager, Hall, Kiddon, MacCartney, de Marneffe, Ramage, Yeh, Manning, 2007</marker>
<rawString>Nathanael Chambers, Daniel Cer, Trond Grenager, David Hall, Chloe Kiddon, Bill MacCartney, MarieCatherine de Marneffe, Daniel Ramage, Eric Yeh, and Christopher D. Manning. 2007. Learning alignments and leveraging natural logic. In Proceedings of the ACL-PASCAL Workshop on Textual Entailment and Paraphrasing.</rawString>
</citation>
<citation valid="true">
<authors>
<author>Bob Coyne</author>
<author>Richard Sproat</author>
</authors>
<title>WordsEye: an automatic text-to-scene conversion system.</title>
<date>2001</date>
<booktitle>In Proceedings of the 28th annual conference on Computer graphics and interactive techniques.</booktitle>
<contexts>
<context position="2397" citStr="Coyne and Sproat, 2001" startWordPosition="397" endWordPosition="400">ural language. This is one of the biggest chalFigure 1: Generated scene for “There is a room with a chair and a computer.” Note that the system infers the presence of a desk and that the computer should be supported by the desk. lenges in grounding language and enabling natural communication between people and intelligent systems. For instance, if we want a robot that can follow commands such as “bring me a piece of cake”, it needs to be imparted with an understanding of likely locations for the cake in the kitchen and that the cake should be placed on a plate. The pioneering WordsEye system (Coyne and Sproat, 2001) addressed the text-to-3D task and is an inspiration for our work. However, there are many remaining gaps in this broad area. Among them, there is a need for research into learning spatial knowledge representations from data, and for connecting them to language. Representing unstated facts is a challenging problem unaddressed by prior work and the focus of our contribution. This problem is a counterpart to the image description problem (Kulkarni et al., 2011; Mitchell et al., 2012; Elliott and Keller, 2013), which has so far remained largely unexplored by the community. We present a representa</context>
<context position="33454" citStr="Coyne and Sproat, 2001" startWordPosition="5534" endWordPosition="5537"> are typically unstated in text and remain largely unaddressed. 8.2 Text to Scene Systems Early work on the SHRDLU system (Winograd, 1972) gives a good formalization of the linguistic manipulation of objects in 3D scenes. By restricting the discourse domain to a micro-world with simple geometric shapes, the SHRDLU system demonstrated parsing of natural language input for manipulating scenes. However, generalization to more complex objects and spatial relations is still very hard to attain. More recently, a pioneering text-to-3D scene generation prototype system has been presented by WordsEye (Coyne and Sproat, 2001). The authors demonstrated the promise of text to scene generation systems but also pointed out some fundamental issues which restrict the success of their system: much spatial knowledge is required which is hard to obtain. As a result, users have to use unnatural language (e.g., “the stool is 1 feet to the south of the table”) to express their intent. Follow up work has attempted to collect spatial knowledge through crowd-sourcing (Coyne et al., 2012), but does not address the learning of spatial priors. We address the challenge of handling natural language for scene generation, by learning s</context>
</contexts>
<marker>Coyne, Sproat, 2001</marker>
<rawString>Bob Coyne and Richard Sproat. 2001. WordsEye: an automatic text-to-scene conversion system. In Proceedings of the 28th annual conference on Computer graphics and interactive techniques.</rawString>
</citation>
<citation valid="true">
<authors>
<author>Bob Coyne</author>
<author>Richard Sproat</author>
<author>Julia Hirschberg</author>
</authors>
<title>Spatial relations in text-to-scene conversion.</title>
<date>2010</date>
<booktitle>In Computational Models ofSpatial Language Interpretation, Workshop at Spatial Cognition.</booktitle>
<contexts>
<context position="32275" citStr="Coyne et al., 2010" startWordPosition="5351" endWordPosition="5354">, in the sentence “there is a desk with a chair in front of it”, “it” is not identified as coreferent with “desk” so we fail to extract the spatial relation front_of(chair, desk). 8 Related Work There is related prior work in the topics of modeling spatial relations, generating 3D scenes from text, and automatically laying out 3D scenes. 8.1 Spatial knowledge and relations Prior work that required modeling spatial knowledge has defined representations specific to the task addressed. Typically, such knowledge is manually provided or crowdsourced – not learned from data. For instance, WordsEye (Coyne et al., 2010) uses a set of manually specified relations. The NLP community has explored grounding text to physical attributes and relations (Matuszek et al., 2012; Krishnamurthy and Kollar, 2013), generating text for referring to objects (FitzGerald et al., 2013) and connecting language to spatial relationships (Vogel and Jurafsky, 2010; Golland et al., 2010; Artzi and Zettlemoyer, 2013). Most of this work focuses on learning a mapping from text to formal representations, and does not model 2036 implicit spatial knowledge. Many priors on real world spatial facts are typically unstated in text and remain l</context>
</contexts>
<marker>Coyne, Sproat, Hirschberg, 2010</marker>
<rawString>Bob Coyne, Richard Sproat, and Julia Hirschberg. 2010. Spatial relations in text-to-scene conversion. In Computational Models ofSpatial Language Interpretation, Workshop at Spatial Cognition.</rawString>
</citation>
<citation valid="true">
<authors>
<author>Bob Coyne</author>
<author>Alexander Klapheke</author>
<author>Masoud Rouhizadeh</author>
<author>Richard Sproat</author>
<author>Daniel Bauer</author>
</authors>
<title>Annotation tools and knowledge representation for a text-toscene system.</title>
<date>2012</date>
<booktitle>Proceedings of COLING 2012: Technical Papers.</booktitle>
<contexts>
<context position="33910" citStr="Coyne et al., 2012" startWordPosition="5611" endWordPosition="5614">ions is still very hard to attain. More recently, a pioneering text-to-3D scene generation prototype system has been presented by WordsEye (Coyne and Sproat, 2001). The authors demonstrated the promise of text to scene generation systems but also pointed out some fundamental issues which restrict the success of their system: much spatial knowledge is required which is hard to obtain. As a result, users have to use unnatural language (e.g., “the stool is 1 feet to the south of the table”) to express their intent. Follow up work has attempted to collect spatial knowledge through crowd-sourcing (Coyne et al., 2012), but does not address the learning of spatial priors. We address the challenge of handling natural language for scene generation, by learning spatial knowledge from 3D scene data, and using it to infer unstated implicit constraints. Our work is similar in spirit to recent work on generating 2D clipart for sentences using probabilistic models learned from data (Zitnick et al., 2013). 8.3 Automatic Scene Layout Work on scene layout has focused on determining good furniture layouts by optimizing energy functions that capture the quality of a proposed layout. These energy functions are encoded fr</context>
</contexts>
<marker>Coyne, Klapheke, Rouhizadeh, Sproat, Bauer, 2012</marker>
<rawString>Bob Coyne, Alexander Klapheke, Masoud Rouhizadeh, Richard Sproat, and Daniel Bauer. 2012. Annotation tools and knowledge representation for a text-toscene system. Proceedings of COLING 2012: Technical Papers.</rawString>
</citation>
<citation valid="true">
<authors>
<author>Desmond Elliott</author>
<author>Frank Keller</author>
</authors>
<title>Image description using visual dependency representations.</title>
<date>2013</date>
<booktitle>In Proceedings of Empirical Methods in Natural Language Processing (EMNLP).</booktitle>
<contexts>
<context position="2909" citStr="Elliott and Keller, 2013" startWordPosition="481" endWordPosition="484">he kitchen and that the cake should be placed on a plate. The pioneering WordsEye system (Coyne and Sproat, 2001) addressed the text-to-3D task and is an inspiration for our work. However, there are many remaining gaps in this broad area. Among them, there is a need for research into learning spatial knowledge representations from data, and for connecting them to language. Representing unstated facts is a challenging problem unaddressed by prior work and the focus of our contribution. This problem is a counterpart to the image description problem (Kulkarni et al., 2011; Mitchell et al., 2012; Elliott and Keller, 2013), which has so far remained largely unexplored by the community. We present a representation for this form of spatial knowledge that we learn from 3D scene data and connect to natural language. We will show how this representation is useful for grounding language and for inferring unstated facts, i.e., the pragmatics of language describing physical environments. We demonstrate the use of this representation in the task of text-to-3D scene genera2028 Proceedings of the 2014 Conference on Empirical Methods in Natural Language Processing (EMNLP), pages 2028–2038, October 25-29, 2014, Doha, Qatar.</context>
</contexts>
<marker>Elliott, Keller, 2013</marker>
<rawString>Desmond Elliott and Frank Keller. 2013. Image description using visual dependency representations. In Proceedings of Empirical Methods in Natural Language Processing (EMNLP).</rawString>
</citation>
<citation valid="true">
<authors>
<author>Matthew Fisher</author>
<author>Daniel Ritchie</author>
<author>Manolis Savva</author>
<author>Thomas Funkhouser</author>
<author>Pat Hanrahan</author>
</authors>
<title>Example-based synthesis of 3D object arrangements.</title>
<date>2012</date>
<journal>ACM Transactions on Graphics (TOG).</journal>
<contexts>
<context position="10309" citStr="Fisher et al. (2012)" startWordPosition="1702" endWordPosition="1705">rangement we learn priors on the types of support surfaces (§4.2) and the relative positions of objects (§4.4). 4 Spatial Knowledge Our model of spatial knowledge relies on the idea of abstract scene types describing the occurrence and arrangement of different categories of objects within scenes of that type. For example, kitchens typically contain kitchen counters on which plates and cups are likely to be found. The type of scene and category of objects condition the spatial relationships that can exist in a scene. We learn spatial knowledge from 3D scene data, basing our approach on that of Fisher et al. (2012) and using their dataset of 133 small indoor scenes created with 1723 Trimble 3D Warehouse models (Fisher et al., 2012). 4.1 Object Occurrence Priors We learn priors for object occurrence in different scene types (such as kitchens, offices, bedrooms). Pocc(CoICs) = count (C(in j s) count Cs This allows us to evaluate the probability of different scene types given lists of object occurring in them (see Figure 3). For example given input of the form “there is a knife on the table” then we are likely to generate a scene with a dining table and other related objects. 4.2 Support Hierarchy Priors W</context>
<context position="23709" citStr="Fisher et al., 2012" startWordPosition="3880" endWordPosition="3883">r.&amp;quot; &amp;quot;There is a coffee table and there is a lamp behind the coffee table. There is a chair in front of the coffee table.&amp;quot; Figure 8: Top Generated scenes for randomly placing objects on the floor (Basic), with inferred Support Hierarchy, and with priors on Relative Positions. Bottom Generated scenes with no understanding of spatial relations (No Relations), scoring using Predefined Relations and Learned Relations. the appropriate category and keywords. We use a 3D model dataset collected from Google 3D Warehouse by prior work in scene synthesis and containing about 12490 mostly indoor objects (Fisher et al., 2012). These models have text associated with them in the form of names and tags. In addition, we semi-automatically annotated models with object category labels (roughly 270 classes). We used model tags to set these labels, and verified and augmented them manually. In addition, we automatically rescale models so that they have physically plausible sizes and orient them so that they have a consistent up and front direction (Savva et al., 2014). We then indexed all models in a database that we query at run-time for retrieval based on category and tag labels. 6.4 Scene Layout Once we have instantiate</context>
<context position="34602" citStr="Fisher et al., 2012" startWordPosition="5723" endWordPosition="5726">allenge of handling natural language for scene generation, by learning spatial knowledge from 3D scene data, and using it to infer unstated implicit constraints. Our work is similar in spirit to recent work on generating 2D clipart for sentences using probabilistic models learned from data (Zitnick et al., 2013). 8.3 Automatic Scene Layout Work on scene layout has focused on determining good furniture layouts by optimizing energy functions that capture the quality of a proposed layout. These energy functions are encoded from design guidelines (Merrell et al., 2011) or learned from scene data (Fisher et al., 2012). Knowledge of object co-occurrences and spatial relations is represented by simple models such as mixtures of Gaussians on pairwise object positions and orientations. We leverage ideas from this work, but they do not focus on linking spatial knowledge to language. 9 Conclusion and Future Work We have demonstrated a representation of spatial knowledge that can be learned from 3D scene data and how it corresponds to natural language. We also showed that spatial inference and grounding is critical for achieving plausible results in the textto-3D scene generation task. Spatial knowledge is critic</context>
</contexts>
<marker>Fisher, Ritchie, Savva, Funkhouser, Hanrahan, 2012</marker>
<rawString>Matthew Fisher, Daniel Ritchie, Manolis Savva, Thomas Funkhouser, and Pat Hanrahan. 2012. Example-based synthesis of 3D object arrangements. ACM Transactions on Graphics (TOG).</rawString>
</citation>
<citation valid="true">
<authors>
<author>Nicholas FitzGerald</author>
<author>Yoav Artzi</author>
<author>Luke Zettlemoyer</author>
</authors>
<title>Learning distributions over logical forms for referring expression generation.</title>
<date>2013</date>
<booktitle>In Proceedings ofEmpirical Methods in Natural Language Processing (EMNLP).</booktitle>
<contexts>
<context position="32526" citStr="FitzGerald et al., 2013" startWordPosition="5389" endWordPosition="5392">ng spatial relations, generating 3D scenes from text, and automatically laying out 3D scenes. 8.1 Spatial knowledge and relations Prior work that required modeling spatial knowledge has defined representations specific to the task addressed. Typically, such knowledge is manually provided or crowdsourced – not learned from data. For instance, WordsEye (Coyne et al., 2010) uses a set of manually specified relations. The NLP community has explored grounding text to physical attributes and relations (Matuszek et al., 2012; Krishnamurthy and Kollar, 2013), generating text for referring to objects (FitzGerald et al., 2013) and connecting language to spatial relationships (Vogel and Jurafsky, 2010; Golland et al., 2010; Artzi and Zettlemoyer, 2013). Most of this work focuses on learning a mapping from text to formal representations, and does not model 2036 implicit spatial knowledge. Many priors on real world spatial facts are typically unstated in text and remain largely unaddressed. 8.2 Text to Scene Systems Early work on the SHRDLU system (Winograd, 1972) gives a good formalization of the linguistic manipulation of objects in 3D scenes. By restricting the discourse domain to a micro-world with simple geometri</context>
</contexts>
<marker>FitzGerald, Artzi, Zettlemoyer, 2013</marker>
<rawString>Nicholas FitzGerald, Yoav Artzi, and Luke Zettlemoyer. 2013. Learning distributions over logical forms for referring expression generation. In Proceedings ofEmpirical Methods in Natural Language Processing (EMNLP).</rawString>
</citation>
<citation valid="true">
<authors>
<author>Dave Golland</author>
<author>Percy Liang</author>
<author>Dan Klein</author>
</authors>
<title>A game-theoretic approach to generating spatial descriptions.</title>
<date>2010</date>
<booktitle>In Proceedings of Empirical Methods in Natural Language Processing (EMNLP).</booktitle>
<contexts>
<context position="32623" citStr="Golland et al., 2010" startWordPosition="5404" endWordPosition="5407">tial knowledge and relations Prior work that required modeling spatial knowledge has defined representations specific to the task addressed. Typically, such knowledge is manually provided or crowdsourced – not learned from data. For instance, WordsEye (Coyne et al., 2010) uses a set of manually specified relations. The NLP community has explored grounding text to physical attributes and relations (Matuszek et al., 2012; Krishnamurthy and Kollar, 2013), generating text for referring to objects (FitzGerald et al., 2013) and connecting language to spatial relationships (Vogel and Jurafsky, 2010; Golland et al., 2010; Artzi and Zettlemoyer, 2013). Most of this work focuses on learning a mapping from text to formal representations, and does not model 2036 implicit spatial knowledge. Many priors on real world spatial facts are typically unstated in text and remain largely unaddressed. 8.2 Text to Scene Systems Early work on the SHRDLU system (Winograd, 1972) gives a good formalization of the linguistic manipulation of objects in 3D scenes. By restricting the discourse domain to a micro-world with simple geometric shapes, the SHRDLU system demonstrated parsing of natural language input for manipulating scene</context>
</contexts>
<marker>Golland, Liang, Klein, 2010</marker>
<rawString>Dave Golland, Percy Liang, and Dan Klein. 2010. A game-theoretic approach to generating spatial descriptions. In Proceedings of Empirical Methods in Natural Language Processing (EMNLP).</rawString>
</citation>
<citation valid="true">
<authors>
<author>Alan D Kalvin</author>
<author>Russell H Taylor</author>
</authors>
<title>Superfaces: Polygonal mesh simplification with bounded error. Computer Graphics and Applications,</title>
<date>1996</date>
<publisher>IEEE.</publisher>
<contexts>
<context position="13193" citStr="Kalvin and Taylor, 1996" startWordPosition="2154" endWordPosition="2157">ont_of, 0.41), (front_side, 0.40) (left_side, 0.44), (left_of, 0.43) (above, 0.37), (near, 0.30) (outside, 0.31), (next_to, 0.30) (supported_by, 0.86), (on_top_of, 0.76) (outside, 0.66), (near, 0.66) (outside, 0.49), (near, 0.48) (supports, 0.62), (below, 0.53) (supported_by,0.65),(above,0.61) (inside, 0.48), (supported_by, 0.35) (right_of, 0.50), (lower_side, 0.38) (outside, 0.45), (right_of, 0.45) 4.3 Support Surface Priors To identify which surfaces on parent objects support child objects, we first segment parent models into planar surfaces using a simple region-growing algorithm based on (Kalvin and Taylor, 1996). We characterize support surfaces by the direction of their normal vector, limited to the six canonical directions: up, down, left, right, front, back. We learn a probability of supporting surface normal direction Sn given child object category Cc. For example, posters are typically found on walls so their support normal vectors are in the horizontal directions. Any unobserved child categories are assumed to have Psurf(Sn = up|Cc) = 1 since most things rest on a horizontal surface (e.g., floor). count(Cc on surface with Sn) Psurf(Sn|Cc) = count(Cc) 4.4 Relative Position Priors We model the re</context>
</contexts>
<marker>Kalvin, Taylor, 1996</marker>
<rawString>Alan D. Kalvin and Russell H. Taylor. 1996. Superfaces: Polygonal mesh simplification with bounded error. Computer Graphics and Applications, IEEE.</rawString>
</citation>
<citation valid="true">
<authors>
<author>Jayant Krishnamurthy</author>
<author>Thomas Kollar</author>
</authors>
<title>Jointly learning to parse and perceive: Connecting natural language to the physical world. Transactions of the Association for Computational Linguistics.</title>
<date>2013</date>
<contexts>
<context position="32458" citStr="Krishnamurthy and Kollar, 2013" startWordPosition="5378" endWordPosition="5381">, desk). 8 Related Work There is related prior work in the topics of modeling spatial relations, generating 3D scenes from text, and automatically laying out 3D scenes. 8.1 Spatial knowledge and relations Prior work that required modeling spatial knowledge has defined representations specific to the task addressed. Typically, such knowledge is manually provided or crowdsourced – not learned from data. For instance, WordsEye (Coyne et al., 2010) uses a set of manually specified relations. The NLP community has explored grounding text to physical attributes and relations (Matuszek et al., 2012; Krishnamurthy and Kollar, 2013), generating text for referring to objects (FitzGerald et al., 2013) and connecting language to spatial relationships (Vogel and Jurafsky, 2010; Golland et al., 2010; Artzi and Zettlemoyer, 2013). Most of this work focuses on learning a mapping from text to formal representations, and does not model 2036 implicit spatial knowledge. Many priors on real world spatial facts are typically unstated in text and remain largely unaddressed. 8.2 Text to Scene Systems Early work on the SHRDLU system (Winograd, 1972) gives a good formalization of the linguistic manipulation of objects in 3D scenes. By re</context>
</contexts>
<marker>Krishnamurthy, Kollar, 2013</marker>
<rawString>Jayant Krishnamurthy and Thomas Kollar. 2013. Jointly learning to parse and perceive: Connecting natural language to the physical world. Transactions of the Association for Computational Linguistics.</rawString>
</citation>
<citation valid="true">
<authors>
<author>Girish Kulkarni</author>
<author>Visruth Premraj</author>
<author>Sagnik Dhar</author>
<author>Siming Li</author>
<author>Yejin Choi</author>
<author>Alexander C Berg</author>
<author>Tamara L Berg</author>
</authors>
<title>Baby talk: Understanding and generating simple image descriptions.</title>
<date>2011</date>
<booktitle>In Computer Vision and Pattern Recognition (CVPR), 2011 IEEE Conference on.</booktitle>
<contexts>
<context position="2859" citStr="Kulkarni et al., 2011" startWordPosition="473" endWordPosition="476">standing of likely locations for the cake in the kitchen and that the cake should be placed on a plate. The pioneering WordsEye system (Coyne and Sproat, 2001) addressed the text-to-3D task and is an inspiration for our work. However, there are many remaining gaps in this broad area. Among them, there is a need for research into learning spatial knowledge representations from data, and for connecting them to language. Representing unstated facts is a challenging problem unaddressed by prior work and the focus of our contribution. This problem is a counterpart to the image description problem (Kulkarni et al., 2011; Mitchell et al., 2012; Elliott and Keller, 2013), which has so far remained largely unexplored by the community. We present a representation for this form of spatial knowledge that we learn from 3D scene data and connect to natural language. We will show how this representation is useful for grounding language and for inferring unstated facts, i.e., the pragmatics of language describing physical environments. We demonstrate the use of this representation in the task of text-to-3D scene genera2028 Proceedings of the 2014 Conference on Empirical Methods in Natural Language Processing (EMNLP), </context>
</contexts>
<marker>Kulkarni, Premraj, Dhar, Li, Choi, Berg, Berg, 2011</marker>
<rawString>Girish Kulkarni, Visruth Premraj, Sagnik Dhar, Siming Li, Yejin Choi, Alexander C. Berg, and Tamara L. Berg. 2011. Baby talk: Understanding and generating simple image descriptions. In Computer Vision and Pattern Recognition (CVPR), 2011 IEEE Conference on.</rawString>
</citation>
<citation valid="true">
<authors>
<author>Christopher D Manning</author>
<author>Mihai Surdeanu</author>
<author>John Bauer</author>
<author>Jenny Finkel</author>
<author>Steven J Bethard</author>
<author>David McClosky</author>
</authors>
<title>The Stanford CoreNLP natural language processing toolkit.</title>
<date>2014</date>
<booktitle>In Proceedings of the 52nd Annual Meeting of the Association for Computational Linguistics: System Demonstrations.</booktitle>
<contexts>
<context position="19935" citStr="Manning et al., 2014" startWordPosition="3251" endWordPosition="3254">j, pobj) The table[nsubj] is next[dobj] to[prep] the chair[pobj]. next_to(table,chair) There is a table[nsubj] next[advmod] to[prep] a chair[pobj]. next_to(table,chair) Table 4: Example dependency patterns for extracting attributes and spatial relations. 6 Text to Scene generation We generate 3D scenes from brief scene descriptions using our learned priors. 6.1 Scene Template Parsing During scene template parsing we identify the scene type, the objects present in the scene, their attributes, and the relations between them. The input text is first processed using the Stanford CoreNLP pipeline (Manning et al., 2014). The scene type is determined by matching the words in the utterance against a list of known scene types from the scene dataset. To identify objects, we look for noun phrases and use the head word as the category, filtering with WordNet (Miller, 1995) to determine which objects are visualizable (under the physical object synset, excluding locations). We use the Stanford coreference system to determine when the same object is being referred to. To identify properties of the objects, we extract other adjectives and nouns in the noun phrase. We also match dependency patterns such as “X is made o</context>
</contexts>
<marker>Manning, Surdeanu, Bauer, Finkel, Bethard, McClosky, 2014</marker>
<rawString>Christopher D. Manning, Mihai Surdeanu, John Bauer, Jenny Finkel, Steven J. Bethard, and David McClosky. 2014. The Stanford CoreNLP natural language processing toolkit. In Proceedings of the 52nd Annual Meeting of the Association for Computational Linguistics: System Demonstrations.</rawString>
</citation>
<citation valid="true">
<authors>
<author>Cynthia Matuszek</author>
<author>Nicholas Fitzgerald</author>
<author>Luke Zettlemoyer</author>
<author>Liefeng Bo</author>
<author>Dieter Fox</author>
</authors>
<title>A joint model of language and perception for grounded attribute learning.</title>
<date>2012</date>
<booktitle>In International Conference on Machine Learning (ICML).</booktitle>
<contexts>
<context position="32425" citStr="Matuszek et al., 2012" startWordPosition="5374" endWordPosition="5377">relation front_of(chair, desk). 8 Related Work There is related prior work in the topics of modeling spatial relations, generating 3D scenes from text, and automatically laying out 3D scenes. 8.1 Spatial knowledge and relations Prior work that required modeling spatial knowledge has defined representations specific to the task addressed. Typically, such knowledge is manually provided or crowdsourced – not learned from data. For instance, WordsEye (Coyne et al., 2010) uses a set of manually specified relations. The NLP community has explored grounding text to physical attributes and relations (Matuszek et al., 2012; Krishnamurthy and Kollar, 2013), generating text for referring to objects (FitzGerald et al., 2013) and connecting language to spatial relationships (Vogel and Jurafsky, 2010; Golland et al., 2010; Artzi and Zettlemoyer, 2013). Most of this work focuses on learning a mapping from text to formal representations, and does not model 2036 implicit spatial knowledge. Many priors on real world spatial facts are typically unstated in text and remain largely unaddressed. 8.2 Text to Scene Systems Early work on the SHRDLU system (Winograd, 1972) gives a good formalization of the linguistic manipulati</context>
</contexts>
<marker>Matuszek, Fitzgerald, Zettlemoyer, Bo, Fox, 2012</marker>
<rawString>Cynthia Matuszek, Nicholas Fitzgerald, Luke Zettlemoyer, Liefeng Bo, and Dieter Fox. 2012. A joint model of language and perception for grounded attribute learning. In International Conference on Machine Learning (ICML).</rawString>
</citation>
<citation valid="true">
<authors>
<author>Paul Merrell</author>
</authors>
<title>Eric Schkufza, Zeyang Li, Maneesh Agrawala, and Vladlen Koltun.</title>
<date>2011</date>
<journal>ACM Transactions on Graphics (TOG).</journal>
<marker>Merrell, 2011</marker>
<rawString>Paul Merrell, Eric Schkufza, Zeyang Li, Maneesh Agrawala, and Vladlen Koltun. 2011. Interactive furniture layout using interior design guidelines. In ACM Transactions on Graphics (TOG).</rawString>
</citation>
<citation valid="true">
<authors>
<author>George A Miller</author>
</authors>
<title>WordNet: a lexical database for English.</title>
<date>1995</date>
<journal>Communications of the ACM.</journal>
<contexts>
<context position="20187" citStr="Miller, 1995" startWordPosition="3297" endWordPosition="3298">t to Scene generation We generate 3D scenes from brief scene descriptions using our learned priors. 6.1 Scene Template Parsing During scene template parsing we identify the scene type, the objects present in the scene, their attributes, and the relations between them. The input text is first processed using the Stanford CoreNLP pipeline (Manning et al., 2014). The scene type is determined by matching the words in the utterance against a list of known scene types from the scene dataset. To identify objects, we look for noun phrases and use the head word as the category, filtering with WordNet (Miller, 1995) to determine which objects are visualizable (under the physical object synset, excluding locations). We use the Stanford coreference system to determine when the same object is being referred to. To identify properties of the objects, we extract other adjectives and nouns in the noun phrase. We also match dependency patterns such as “X is made of Y” to extract additional attributes. Based on the object category and attributes, and other words in the noun phrase mentioning the object, we identify a set of associated keywords to be used later for querying the 3D model database. Dependency patte</context>
</contexts>
<marker>Miller, 1995</marker>
<rawString>George A. Miller. 1995. WordNet: a lexical database for English. Communications of the ACM.</rawString>
</citation>
<citation valid="true">
<authors>
<author>Margaret Mitchell</author>
<author>Xufeng Han</author>
<author>Jesse Dodge</author>
<author>Alyssa Mensch</author>
<author>Amit Goyal</author>
</authors>
<title>Midge: Generating image descriptions from computer vision detections.</title>
<date>2012</date>
<booktitle>In Proceedings of the 13th Conference of the European Chapter of the Association for Computational Linguistics.</booktitle>
<location>Alex Berg, Kota Yamaguchi, Tamara Berg, Karl</location>
<contexts>
<context position="2882" citStr="Mitchell et al., 2012" startWordPosition="477" endWordPosition="480">tions for the cake in the kitchen and that the cake should be placed on a plate. The pioneering WordsEye system (Coyne and Sproat, 2001) addressed the text-to-3D task and is an inspiration for our work. However, there are many remaining gaps in this broad area. Among them, there is a need for research into learning spatial knowledge representations from data, and for connecting them to language. Representing unstated facts is a challenging problem unaddressed by prior work and the focus of our contribution. This problem is a counterpart to the image description problem (Kulkarni et al., 2011; Mitchell et al., 2012; Elliott and Keller, 2013), which has so far remained largely unexplored by the community. We present a representation for this form of spatial knowledge that we learn from 3D scene data and connect to natural language. We will show how this representation is useful for grounding language and for inferring unstated facts, i.e., the pragmatics of language describing physical environments. We demonstrate the use of this representation in the task of text-to-3D scene genera2028 Proceedings of the 2014 Conference on Empirical Methods in Natural Language Processing (EMNLP), pages 2028–2038, Octobe</context>
</contexts>
<marker>Mitchell, Han, Dodge, Mensch, Goyal, 2012</marker>
<rawString>Margaret Mitchell, Xufeng Han, Jesse Dodge, Alyssa Mensch, Amit Goyal, Alex Berg, Kota Yamaguchi, Tamara Berg, Karl Stratos, and Hal Daumé III. 2012. Midge: Generating image descriptions from computer vision detections. In Proceedings of the 13th Conference of the European Chapter of the Association for Computational Linguistics.</rawString>
</citation>
<citation valid="true">
<authors>
<author>Manolis Savva</author>
<author>Angel X Chang</author>
<author>Gilbert Bernstein</author>
<author>Christopher D Manning</author>
<author>Pat Hanrahan</author>
</authors>
<title>On being the right scale: Sizing large collections of 3D models.</title>
<date>2014</date>
<tech>Technical Report CSTR 2014-03.</tech>
<institution>Stanford University</institution>
<contexts>
<context position="24151" citStr="Savva et al., 2014" startWordPosition="3951" endWordPosition="3954">nd keywords. We use a 3D model dataset collected from Google 3D Warehouse by prior work in scene synthesis and containing about 12490 mostly indoor objects (Fisher et al., 2012). These models have text associated with them in the form of names and tags. In addition, we semi-automatically annotated models with object category labels (roughly 270 classes). We used model tags to set these labels, and verified and augmented them manually. In addition, we automatically rescale models so that they have physically plausible sizes and orient them so that they have a consistent up and front direction (Savva et al., 2014). We then indexed all models in a database that we query at run-time for retrieval based on category and tag labels. 6.4 Scene Layout Once we have instantiated the objects in the scene by selecting models, we aim to optimize an overall layout score L = AobjLobj + ArelLrel that is a weighted sum of object arrangement Lobj score and constraint satisfaction Lrel score: ∑Lobj = ∑Psurf(S�|Coi) Prelpos(�) oi ojEF(oi) ∑Lrel = Prel(Cz) ci where F(oz) are the sibling objects and parent object of oz. We use Aobj = 0.25 and Arel = 0.75 for the results we present. We use a simple hill climbing strategy to</context>
</contexts>
<marker>Savva, Chang, Bernstein, Manning, Hanrahan, 2014</marker>
<rawString>Manolis Savva, Angel X. Chang, Gilbert Bernstein, Christopher D. Manning, and Pat Hanrahan. 2014. On being the right scale: Sizing large collections of 3D models. Stanford University Technical Report CSTR 2014-03.</rawString>
</citation>
<citation valid="true">
<authors>
<author>Adam Vogel</author>
<author>Dan Jurafsky</author>
</authors>
<title>Learning to follow navigational directions.</title>
<date>2010</date>
<booktitle>In Proceedings ofACL.</booktitle>
<contexts>
<context position="32601" citStr="Vogel and Jurafsky, 2010" startWordPosition="5400" endWordPosition="5403">ing out 3D scenes. 8.1 Spatial knowledge and relations Prior work that required modeling spatial knowledge has defined representations specific to the task addressed. Typically, such knowledge is manually provided or crowdsourced – not learned from data. For instance, WordsEye (Coyne et al., 2010) uses a set of manually specified relations. The NLP community has explored grounding text to physical attributes and relations (Matuszek et al., 2012; Krishnamurthy and Kollar, 2013), generating text for referring to objects (FitzGerald et al., 2013) and connecting language to spatial relationships (Vogel and Jurafsky, 2010; Golland et al., 2010; Artzi and Zettlemoyer, 2013). Most of this work focuses on learning a mapping from text to formal representations, and does not model 2036 implicit spatial knowledge. Many priors on real world spatial facts are typically unstated in text and remain largely unaddressed. 8.2 Text to Scene Systems Early work on the SHRDLU system (Winograd, 1972) gives a good formalization of the linguistic manipulation of objects in 3D scenes. By restricting the discourse domain to a micro-world with simple geometric shapes, the SHRDLU system demonstrated parsing of natural language input </context>
</contexts>
<marker>Vogel, Jurafsky, 2010</marker>
<rawString>Adam Vogel and Dan Jurafsky. 2010. Learning to follow navigational directions. In Proceedings ofACL.</rawString>
</citation>
<citation valid="true">
<authors>
<author>Adam Vogel</author>
<author>Christopher Potts</author>
<author>Dan Jurafsky</author>
</authors>
<title>Implicatures and nested beliefs in approximate Decentralized-POMDPs.</title>
<date>2013</date>
<booktitle>In Proceedings ofthe 51st Annual Meeting of the Association for Computational Linguistics.</booktitle>
<contexts>
<context position="26717" citStr="Vogel et al., 2013" startWordPosition="4388" endWordPosition="4391">iew-based object referent resolution (§7.2) and in disambiguating geometric interpretations of “on” (§7.3). Model Comparison Figure 8 shows a comparison of scenes generated by our model versus several simpler baselines. The top row shows the impact of modeling the support hierarchy and the relative positions in the layout of the scene. The bottom row shows that the learned spatial relations can give a more accurate layout than the naive predefined spatial relations, since it captures pragmatic implicatures of language, e.g., left is only used for directly left and not top left or bottom left (Vogel et al., 2013). Figure 12: Left: chair is selected using “the chair to the right of the table” or “the object to the right of the table”. Chair is not selected for “the cup to the right of the table”. Right: Different view results in different chair being selected for the input “the chair to the right of the table”. 7.1 Generated Scenes Support Hierarchy Figure 9 shows a generated scene along with the input text and support hierarchy. Even though the spatial relation between lamp and desk was not mentioned, we infer that the lamp is supported by the top surface of the desk. Disambiguation Figure 10 shows a </context>
</contexts>
<marker>Vogel, Potts, Jurafsky, 2013</marker>
<rawString>Adam Vogel, Christopher Potts, and Dan Jurafsky. 2013. Implicatures and nested beliefs in approximate Decentralized-POMDPs. In Proceedings ofthe 51st Annual Meeting of the Association for Computational Linguistics.</rawString>
</citation>
<citation valid="true">
<authors>
<author>Terry Winograd</author>
</authors>
<title>Understanding natural language. Cognitive psychology.</title>
<date>1972</date>
<contexts>
<context position="32969" citStr="Winograd, 1972" startWordPosition="5461" endWordPosition="5462">unding text to physical attributes and relations (Matuszek et al., 2012; Krishnamurthy and Kollar, 2013), generating text for referring to objects (FitzGerald et al., 2013) and connecting language to spatial relationships (Vogel and Jurafsky, 2010; Golland et al., 2010; Artzi and Zettlemoyer, 2013). Most of this work focuses on learning a mapping from text to formal representations, and does not model 2036 implicit spatial knowledge. Many priors on real world spatial facts are typically unstated in text and remain largely unaddressed. 8.2 Text to Scene Systems Early work on the SHRDLU system (Winograd, 1972) gives a good formalization of the linguistic manipulation of objects in 3D scenes. By restricting the discourse domain to a micro-world with simple geometric shapes, the SHRDLU system demonstrated parsing of natural language input for manipulating scenes. However, generalization to more complex objects and spatial relations is still very hard to attain. More recently, a pioneering text-to-3D scene generation prototype system has been presented by WordsEye (Coyne and Sproat, 2001). The authors demonstrated the promise of text to scene generation systems but also pointed out some fundamental is</context>
</contexts>
<marker>Winograd, 1972</marker>
<rawString>Terry Winograd. 1972. Understanding natural language. Cognitive psychology.</rawString>
</citation>
<citation valid="true">
<authors>
<author>C Lawrence Zitnick</author>
<author>Devi Parikh</author>
<author>Lucy Vanderwende</author>
</authors>
<title>Learning the visual interpretation of sentences.</title>
<date>2013</date>
<booktitle>In IEEE Intenational Conference on Computer Vision (ICCV).</booktitle>
<contexts>
<context position="34295" citStr="Zitnick et al., 2013" startWordPosition="5674" endWordPosition="5677">in. As a result, users have to use unnatural language (e.g., “the stool is 1 feet to the south of the table”) to express their intent. Follow up work has attempted to collect spatial knowledge through crowd-sourcing (Coyne et al., 2012), but does not address the learning of spatial priors. We address the challenge of handling natural language for scene generation, by learning spatial knowledge from 3D scene data, and using it to infer unstated implicit constraints. Our work is similar in spirit to recent work on generating 2D clipart for sentences using probabilistic models learned from data (Zitnick et al., 2013). 8.3 Automatic Scene Layout Work on scene layout has focused on determining good furniture layouts by optimizing energy functions that capture the quality of a proposed layout. These energy functions are encoded from design guidelines (Merrell et al., 2011) or learned from scene data (Fisher et al., 2012). Knowledge of object co-occurrences and spatial relations is represented by simple models such as mixtures of Gaussians on pairwise object positions and orientations. We leverage ideas from this work, but they do not focus on linking spatial knowledge to language. 9 Conclusion and Future Wor</context>
</contexts>
<marker>Zitnick, Parikh, Vanderwende, 2013</marker>
<rawString>C. Lawrence Zitnick, Devi Parikh, and Lucy Vanderwende. 2013. Learning the visual interpretation of sentences. In IEEE Intenational Conference on Computer Vision (ICCV).</rawString>
</citation>
</citationList>
</algorithm>
</algorithms>
<?xml version="1.0" encoding="UTF-8"?>
<algorithms version="110505">
<algorithm name="SectLabel" version="110505">
<variant no="0" confidence="0.000357">
<title confidence="0.978962">
Joint Learning of Chinese Words, Terms and Keywords
</title>
<author confidence="0.999547">
Ziqiang Cao&apos; Sujian Li&apos; Heng Ji2
</author>
<affiliation confidence="0.995339">
&apos;Key Laboratory of Computational Linguistics, Peking University, MOE, China
2Computer Science Department, Rensselaer Polytechnic Institute, USA
</affiliation>
<email confidence="0.991036">
{ziqiangyeah, lisujian}@pku.edu.cn jih@rpi.edu
</email>
<sectionHeader confidence="0.997252" genericHeader="abstract">
Abstract
</sectionHeader>
<bodyText confidence="0.999915176470588">
Previous work often used a pipelined
framework where Chinese word segmen-
tation is followed by term extraction and
keyword extraction. Such framework suf-
fers from error propagation and is un-
able to leverage information in later mod-
ules for prior components. In this paper,
we propose a four-level Dirichlet Process
based model (DP-4) to jointly learn the
word distributions from the corpus, do-
main and document levels simultaneously.
Based on the DP-4 model, a sentence-wise
Gibbs sampler is adopted to obtain proper
segmentation results. Meanwhile, terms
and keywords are acquired in the sampling
process. Experimental results have shown
the effectiveness of our method.
</bodyText>
<sectionHeader confidence="0.999517" genericHeader="keywords">
1 Introduction
</sectionHeader>
<bodyText confidence="0.99993315">
For Chinese language which does not contain ex-
plicitly marked word boundaries, word segmenta-
tion (WS) is usually the first important step for
many Natural Language Processing (NLP) tasks
including term extraction (TE) and keyword ex-
traction (KE). Generally, Chinese terms and key-
words can be regarded as words which are repre-
sentative of one domain or one document respec-
tively. Previous work of TE and KE normally used
the pipelined approaches which first conducted
WS and then extracted important word sequences
as terms or keywords.
It is obvious that the pipelined approaches are
prone to suffer from error propagation and fail to
leverage information for word segmentation from
later stages. Here, we provide one example in the
disease domain, to demonstrate the common prob-
lems in current pipelined approaches and propose
the basic idea of our joint learning of words, terms
and keywords.
</bodyText>
<equation confidence="0.5291395">
Example: _k/1&apos;%&amp;Vi`)t(thrombocytopenia) n(with)
Aff (heparinoid) (have) )`,*,,(relation).
</equation>
<bodyText confidence="0.972865026315789">
This is a correctly segmented Chinese sen-
tence. The document containing the example sen-
tence mainly talks about the property of ”A1f
(heparinoid)” which can be regarded as one key-
word of the document. At the same time, the
word leu./J,,&amp;V�`Ih(thrombocytopenia) appears fre-
quently in the disease domain and can be treated
as a domain-specific term.
However, for such a simple sentence, current
segmentation tools perform poorly. The segmen-
tation result with the state-of-the-art Conditional
Random Fields (CRFs) approach (Zhao et al.,
2006) is as follows:
_k/1&apos;%&amp;(blood platelet) Ïi`(reduction) Ç(symptom)
nA(of same kind) (liver) 17(always) s,*,,(relation)
where is segmented into three com-
mon Chinese words and Alf A is mixed with its
neighbors.
In a text processing pipeline of WS, TE and
KE, it is obvious that imprecise WS results will
make the overall system performance unsatisfy-
ing. At the same time, we can hardly make use of
domain-level and document-level information col-
lected in TE and KE to promote the performance
of WS. Thus, one question comes to our minds:
can words, terms and keywords be jointly learned
with consideration of all the information from the
corpus, domain, and document levels?
Recently, the hierarchical Dirichlet process
(HDP) model has been used as a smoothed bigram
model to conduct word segmentation (Goldwater
et al., 2006; Goldwater et al., 2009). Meanwhile,
one strong point of the HDP based models is that
they can model the diversity and commonality in
multiple correlated corpora (Ren et al., 2008; Xu
et al., 2008; Zhang et al., 2010; Li et al., 2012;
Chang et al., 2014). Inspired by such existing
work, we propose a four-level DP based model,
</bodyText>
<page confidence="0.924818">
1774
</page>
<note confidence="0.6574685">
Proceedings of the 2014 Conference on Empirical Methods in Natural Language Processing (EMNLP), pages 1774–1778,
October 25-29, 2014, Doha, Qatar. c�2014 Association for Computational Linguistics
</note>
<figureCaption confidence="0.998079">
Figure 1: DP-4 Model
</figureCaption>
<sectionHeader confidence="0.361461" genericHeader="introduction">
 |VI
</sectionHeader>
<bodyText confidence="0.999773705882353">
named DP-4, to adapt to three levels: corpus, do-
main and document. In our model, various DPs
are designed to reflect the smoothed word distri-
butions in the whole corpus, different domains and
different documents. Same as the DP based seg-
mentation models, our model can be easily used
as a semi-supervised framework, through exerting
on the corpus level the word distributions learned
from the available segmentation results. Refer-
ring to the work of Mochihashi et al. (2009), we
conduct word segmentation using a sentence-wise
Gibbs sampler, which combines the Gibbs sam-
pling techniques with the dynamic programming
strategy. During the sampling process, the impor-
tance values of segmented words are measured in
domains and documents respectively, and words,
terms and keywords are jointly learned.
</bodyText>
<sectionHeader confidence="0.998476" genericHeader="method">
2 DP-4 Model
</sectionHeader>
<bodyText confidence="0.9999885625">
Goldwater et al. (2006) applied the HDP model on
the word segmentation task. In essence, Goldwa-
ter’s model can be viewed as a bigram language
model with a unigram back-off. With the lan-
guage model, word segmentation is implemented
by a character-based Gibbs sampler which repeat-
edly samples the possible word boundary posi-
tions between two neighboring words, conditioned
on the current values of all other words. How-
ever, Goldwater’s model can be deemed as mod-
eling the whole corpus only, and does not distin-
guish between domains and documents. To jointly
learn the word information from the corpus, do-
main and document levels, we extend Goldwater’s
model by adding two levels (domain level and doc-
ument level) of DPs, as illustrated in Figure 1.
</bodyText>
<subsectionHeader confidence="0.995993">
2.1 Model Description
</subsectionHeader>
<bodyText confidence="0.967842555555556">
M DPs (Hmw ;1 &lt; m &lt; M) are designed specif-
ically to word w to model the bigram distribu-
tions in each domain and these DPs share an
overall base measure Hw, which is drawn from
DP(α0, G1) and gives the bigram distribution for
the whole corpus. Assuming the mth domain in-
cludes Nm documents, we use Hmj
w (1 &lt; j &lt;
Nm) to model the bigram distribution of the ith
document in the domain. Usually, given a do-
main, the bigram distributions of different docu-
ments are not conditionally independent and simi-
lar documents exhibit similar bigram distributions.
Thus, the bigram distribution of one document is
generated according to both the bigram distribu-
tion of the domain and the bigram distributions
of other documents in the same domain. That is,
Hmj
</bodyText>
<equation confidence="0.946434">
w — g(α3, Hmw , Hm−j
w ) where Hm−j
</equation>
<bodyText confidence="0.948456714285714">
w repre-
sents the bigram distributions of the documents in
the mth domain except the jth document. Assum-
ing the jth document in the mth domain contains
Njm words, each word is drawn according to Hmj
w.
That is, wmj
</bodyText>
<equation confidence="0.9760775">
z — Hmj
w (1 &lt; i &lt; Njm). Thus, our
</equation>
<bodyText confidence="0.674496">
four-level DP model can be summarized formally
as follows:
</bodyText>
<equation confidence="0.99984675">
G1 — DP (α0, G0); Hw — DP (α1, G1)
Hwm — DP (α2, Hw) ; H. — g (α3, Hmw , H m−j )
w
|wz−1 = w — Hdw
</equation>
<bodyText confidence="0.999944428571429">
Here, we provide for our model the Chinese
Restaurant Process (CRP) metaphor, which can
create a partition of items into groups. In our
model, the word type of the previous word wz−1
corresponds to a restaurant and the current word
wz corresponds to a customer. Each domain is
analogous to a floor in a restaurant and a room de-
notes a document. Now, we can see that there are
|V  |restaurants and each restaurant consists of M
floors. The mth floor contains Nm rooms and each
room has an infinite number of tables with infinite
seating capacity. Customers enter a specific room
on a specific floor of one restaurant and seat them-
selves at a table with the label of a word type. Dif-
ferent from the standard HDP, each customer sits
at an occupied table with probability proportional
to both the numbers of customers already seated
there and the numbers of customers with the same
word type seated in the neighboring rooms, and at
an unoccupied table with probability proportional
to both the constant α3 and the probability that the
</bodyText>
<figure confidence="0.991911111111111">
0
,
2
3 Ir. ...Hmt iw-N-
Go
G,
M
mj
wz
</figure>
<page confidence="0.955633">
1775
</page>
<bodyText confidence="0.9981815">
customers with the same word type are seated on
the same floor.
</bodyText>
<subsectionHeader confidence="0.997178">
2.2 Model Inference
</subsectionHeader>
<bodyText confidence="0.999909">
It is important to build an accurate G0 which de-
termines the prior word distribution p0(w). Sim-
ilar to the work of Mochihashi et al. (2009), we
consider the dependence between characters and
calculate the prior distribution of a word wi using
the string frequency statistics (Krug, 1998):
</bodyText>
<equation confidence="0.992344666666667">
ns(wi)
p0(wi) = (1)
Pns(.)
</equation>
<bodyText confidence="0.9999805">
where ns(wi) counts the character string com-
posed of wi and the symbol “.” represents any
word in the vocabulary V .
Then, with the CRP metaphor, we can obtain the
expected word unigram and bigram distributions
on the corpus level according to G1 and Hw:
</bodyText>
<equation confidence="0.999859333333333">
n (wi) + α0p0 (wi)
p1 (wi) = (2)
Pn(.) + α0
nw (wi) + α1p1 (wi)
p2 (wi|wi−1 = w) = (3)
P nw (.) + α1
</equation>
<bodyText confidence="0.9999815">
where the subscript numbers indicate the corre-
sponding DP levels. n(wi) denotes the number of
wi and nw(wi) denotes the number of the bigram
&lt; w, wi &gt; occurring in the corpus. Next, we can
easily get the bigram distribution on the domain
level by extending to the third DP.
</bodyText>
<equation confidence="0.974197666666667">
nm w (wi) + α2p2(wi|wi−1)
pm 3 (wi|wi−1 = w) =
(4)
</equation>
<bodyText confidence="0.999907642857143">
where nmw (wi) is the number of the bigram &lt;
w, wi &gt; occurring in the mth domain.
To model the bigram distributions on the docu-
ment level, it is beneficial to consider the influence
of related documents in the same domain (Wan
and Xiao, 2008). Here, we only consider the in-
fluence from the K most similar documents with a
simple similarity metric s(d1, d2) which calculates
the Chinese character overlap ratio of two docu-
ments d1 and d2. Let djm denote the jth document
in the mth domain and djm[k](1 ≤ k ≤ K) the K
most similar documents. djm can be deemed to be
“lengthened” by djm[k](1 ≤ k ≤ K). Therefore,
we estimate the count of wi in djm as:
</bodyText>
<equation confidence="0.815043">
tdjwm (wi) = nwm (wi)+X
k
</equation>
<bodyText confidence="0.9430398">
where ndj w (wi) denotes the count of the bigram
m[k]
&lt; w, wi &gt; occurring in djm[k]. Next, we model
the bigram distribution in djm as a DP with the base
measure Hmw :
</bodyText>
<equation confidence="0.995684">
pdj w (wi) + α3pm 3 (wi|wi−1)
4 (wi|wi−1 = w) = tdj m
m
P tdj w (.) + α3
</equation>
<bodyText confidence="0.828756666666667">
m
With CRP, we can also easily estimate the un-
igram probabilities pm 3 (wi) and pdj
</bodyText>
<equation confidence="0.7103245">
4 (wi) respec-
m
</equation>
<bodyText confidence="0.99747075">
tively on the domain and document levels, through
combining all the restaurants.
To measure whether a word is eligible to be a
term, the score function THm(·) is defined as:
</bodyText>
<equation confidence="0.9904015">
THm(wi) = p3m (wi)
p1 (wi)
</equation>
<bodyText confidence="0.997324666666667">
This equation is inspired by the work of Nazar
(2011), which extracts terms with consideration of
both the frequency in the domain corpus and the
frequency in the general reference corpus. Similar
to Eq. 7, we define the function KHdjm(·) to judge
whether wi is an appropriate keyword.
</bodyText>
<equation confidence="0.9986385">
KHdjm(wi) = pdj
4 (wi)
m
p1(wi)
</equation>
<bodyText confidence="0.9554395">
During each sampling, we make use of Eqs. (7)
and (8) to identify the most possible terms and
keywords. Once a word is identified as a term
or keyword, it will drop out of the sampling pro-
cess in the following iterations. Its CRP explana-
tion is that some customers (terms and keywords)
find their proper tables and keep sitting there after-
wards.
</bodyText>
<subsectionHeader confidence="0.999429">
2.3 Sentence-wise Gibbs Sampler
</subsectionHeader>
<bodyText confidence="0.999935692307692">
The character-based Gibbs sampler for word seg-
mentation (Goldwater et al., 2006) is extremely
slow to converge, since there exists high correla-
tion between neighboring words. Here, we intro-
duce the sentence-wise Gibbs sampling technique
as well as efficient dynamic programming strat-
egy proposed by Mochihashi et al. (2009). The
basic idea is that we randomly select a sentence
in each sampling process and use the Viterbi al-
gorithm (Viterbi, 1967) to find the optimal seg-
mentation results according to the word distribu-
tions derived from other sentences. Different from
Mochihashi’s work, once terms or keywords are
</bodyText>
<equation confidence="0.651392">
P nmw (.) + α2
s(djm[k], dj m)ndj w (wi)
m[k]
(5)
</equation>
<page confidence="0.948589">
1776
</page>
<bodyText confidence="0.99737725">
identified, we do not consider them in the segmen-
tation process. Due to space limitation, the algo-
rithm is not detailed here and can be referred in
(Mochihashi et al., 2009).
</bodyText>
<sectionHeader confidence="0.999733" genericHeader="method">
3 Experiment
</sectionHeader>
<subsectionHeader confidence="0.999838">
3.1 Data and Setting
</subsectionHeader>
<bodyText confidence="0.999975965517241">
It is indeed difficult to find a standard evaluation
corpus for our joint tasks, especially in different
domains. As a result, we spent a lot of time to col-
lect and annotate a new corpus 1 composed of ten
domains (including Physics, Computer, Agricul-
ture, Sports, Disease, Environment, History, Art,
Politics and Economy) and each domain is com-
posed of 200 documents. On average each doc-
ument consists of about 4800 Chinese characters.
For these 2000 documents, three annotators have
manually checked the segmented words, terms and
keywords as the gold standard results for evalu-
ation. As we know, there exists a large amount
of manually-checked segmented text for the gen-
eral domain, which can be used as the training data
for further segmentation. As with other nonpara-
metric Bayesian models (Goldwater et al., 2006;
Mochihashi et al., 2009), our DP-4 model can be
easily amenable to semi-supervised learning by
imposing the word distributions of the segmented
text on the corpus level. The news texts pro-
vided by Peking University (named PKU corpus)2
is used as the training data. This corpus contains
about 1,870,000 Chinese characters and has been
manually segmented into words.
In our experiments, the concentration coeffi-
cient (α0) is finally set to 20 and the other three
(α1∼3) are set to 15. The parameter K which con-
trols the number of similar documents is set to 3.
</bodyText>
<subsectionHeader confidence="0.999828">
3.2 Performance Evaluation
</subsectionHeader>
<bodyText confidence="0.999898555555556">
The following baselines are implemented for com-
parison of segmentation results: (1) Forward max-
imum matching (FMM) algorithm with a vocab-
ulary compiled from the PKU corpus; (2) Re-
verse maximum matching (RMM) algorithm with
the compiled vocabulary; (3) Conditional Random
Fields (CRFs)3 based supervised algorithm trained
from the PKU corpus; (4) HDP based semi-
supervised algorithm (Goldwater et al., 2006) us-
</bodyText>
<footnote confidence="0.9671686">
1Nine domains are from http://www.datatang.
com/data/44139 and we add an extra Disease domain.
2http://icl.pku.edu.cn
3We adopt CRF++(http://crfpp.googlecode.
com/svn/trunk/doc/index.html)
</footnote>
<bodyText confidence="0.999942352941177">
ing the PKU corpus. The strength of Mochi-
hashi et al. (2009)’s NPYLM based segmentation
model is its speed due to the sentence-wise sam-
pling technique, and its performance is similar to
Goldwater et al. (2006)’s model. Thus, we do not
consider the NPYLM based model for compari-
son here. Then, the segmentation results of FMM,
RMM, CRF, and HDP methods are used respec-
tively for further extracting terms and keywords.
We use the mutual information to identify the can-
didate terms or keywords composed of more than
two segmented words. As for DP-4, this recogni-
tion process has been done implicitly during sam-
pling. To measure the candidate terms or key-
words, we refer to the metric in Nazar (2011) to
calculate their importance in some specific domain
or document.
The metrics of F1 and the out-of-vocabulary
Recall (OOV-R) are used to evaluate the segmenta-
tion results, referring to the gold standard results.
The second and third columns of Table 1 show the
F1 and OOV-R scores averaged on the 10 domains
for all the compared methods. Our method sig-
nificantly outperforms FMM, RMM and HDP ac-
cording to t-test (p-value ≤ 0.05). From the seg-
mentation results, we can see that the FMM and
RMM methods are highly dependent on the com-
piled vocabulary and their identified OOV words
are mainly the ones composed of a single Chinese
character. The HDP method is heavily influenced
by the segmented text, but it also exhibits the abil-
ity of learning new words. Our method only shows
a slight advantage over the CRF approach. We
check our segmentation results and find that the
performance of the DP-4 model is depressed by
the identified terms and keywords which may be
composed of more than two words in the gold
standard results, because the DP-4 model always
treats the term or keyword as a single word. For
example, in the gold standard, ”*-W�((Lingnan
Culture)” is segmented into two words ”WW” and
”k”, ”pn¥ã(data interface)” is segmented
into ”pn” and ”¥ã” and so on. In fact, our seg-
mentation results correctly treat ”dW�” and ”p
n¥ã” as words.
To evaluate the TE and KE performance, the top
50 (TE-50) and 100 (TE-100) accuracy are mea-
sured for the identified terms of one domain, while
the top 5 (KE-5) and 10 (KE-10) accuracy for the
keywords in one document, are shown in the right
four columns of Table 1. We can see that DP-
</bodyText>
<page confidence="0.977637">
1777
</page>
<bodyText confidence="0.997466888888889">
4 performs significantly better than all the other
methods in TE and KE results.
As for the ten domains, we find our approach
behaves much better than the other approaches on
the following three domains: Disease, Physics and
Computer. It is because the language of these
three domains is much different from that of the
general domain (PKU corpus), while the rest do-
mains are more similar to the general domain.
</bodyText>
<table confidence="0.999827833333333">
Method F1 OOV-R TE-50 TE-100 KE-5 KE-10
FMM 0.796 0.136 0.420 0.360 0.476 0.413
RMM 0.794 0.136 0.424 0.352 0.478 0.414
HDP 0.808 0.356 0.672 0.592 0.552 0.506
CRF 0.817 0.330 0.624 0.560 0.543 0.511
DP-4 0.821 0.374 0.704 0.640 0.571 0.545
</table>
<tableCaption confidence="0.955141">
Table 1: Comparison of WS, TE and KE Perfor-
mance (averaged on the 10 domains).
</tableCaption>
<sectionHeader confidence="0.999121" genericHeader="conclusions">
4 Conclusion
</sectionHeader>
<bodyText confidence="0.9999735">
This paper proposes a four-level DP based model
to construct the word distributions from the cor-
pus, domain and document levels simultaneously,
through which Chinese words, terms and key-
words can be learned jointly and effectively. In
the future, we plan to explore how to combine
more features such as part-of-speech tags into our
model.
</bodyText>
<sectionHeader confidence="0.99863" genericHeader="acknowledgments">
Acknowledgments
</sectionHeader>
<bodyText confidence="0.99986">
We thank the three anonymous reviewers for
their helpful comments. This work was par-
tially supported by National High Technology Re-
search and Development Program of China (No.
2012AA011101), National Key Basic Research
Program of China (No. 2014CB340504), Na-
tional Natural Science Foundation of China (No.
61273278), and National Key Technology R&amp;D
Program (No: 2011BAH10B04-03). The contact
author of this paper, according to the meaning
given to this role by Peking University, is Sujian
Li.
</bodyText>
<sectionHeader confidence="0.999643" genericHeader="references">
References
</sectionHeader>
<reference confidence="0.9998607">
Baobao Chang, Wenzhe Pei, and Miaohong Chen.
2014. Inducing word sense with automatically
learned hidden concepts. In Proceedings of COL-
ING 2014, pages 355–364, Dublin, Ireland, August.
Sharon Goldwater, Thomas L. Griffiths, and Mark
Johnson. 2006. Contextual dependencies in un-
supervised word segmentation. In Proceedings of
the 21st International Conference on Computational
Linguistics and the 44th annual meeting of the Asso-
ciation for Computational Linguistics, pages 673–
680.
Sharon Goldwater, Thomas L. Griffiths, and Mark
Johnson. 2009. A bayesian framework for word
segmentation: Exploring the effects of context.
Cognition, 112(1):21–54.
Manfred Krug. 1998. String frequency: A cognitive
motivating factor in coalescence, language process-
ing, and linguistic change. Journal of English Lin-
guistics, 26(4):286–320.
Jiwei Li, Sujian Li, Xun Wang, Ye Tian, and Baobao
Chang. 2012. Update summarization using a multi-
level hierarchical dirichlet process model. In Pro-
ceedings of Coling 2012, pages 1603–1618, Mum-
bai, India.
Daichi Mochihashi, Takeshi Yamada, and Naonori
Ueda. 2009. Bayesian unsupervised word segmen-
tation with nested pitman-yor language modeling.
In Proceedings of the Joint Conference of the 47th
Annual Meeting of the ACL and the 4th International
Joint Conference on Natural Language Processing
of the AFNLP: Volume 1-Volume 1, pages 100–108.
Rogelio Nazar. 2011. A statistical approach to term
extraction. IJES, International Journal of English
Studies, 11(2):159–182.
Lu Ren, David B. Dunson, and Lawrence Carin. 2008.
The dynamic hierarchical dirichlet process. In Pro-
ceedings of the 25th international conference on
Machine learning, pages 824–831.
Andrew J. Viterbi. 1967. Error bounds for convolu-
tional codes and an asymptotically optimum decod-
ing algorithm. IEEE Transactions on Information
Theory, pages 260–269.
Xiaojun Wan and Jianguo Xiao. 2008. Single
document keyphrase extraction using neighborhood
knowledge. In AAAI, volume 8, pages 855–860.
Tianbing Xu, Zhongfei Zhang, Philip S. Yu, and
Bo Long. 2008. Dirichlet process based evolution-
ary clustering. In ICDM’08, pages 648–657.
Jianwen Zhang, Yangqiu Song, Changshui Zhang, and
Shixia Liu. 2010. Evolutionary hierarchical dirich-
let processes for multiple correlated time-varying
corpora. In Proceedings of the 16th ACM SIGKDD
International Conference on Knowledge Discovery
and Data Mining, pages 1079–1088, New York, NY,
USA.
Hai Zhao, Chang-Ning Huang, and Mu Li. 2006. An
improved chinese word segmentation system with
conditional random field. In Proceedings of the Fifth
SIGHAN Workshop on Chinese Language Process-
ing, volume 1082117.
</reference>
<page confidence="0.993652">
1778
</page>
</variant>
</algorithm>
<algorithm name="ParsHed" version="110505">
<variant no="0" confidence="0.708154">
<title confidence="0.841333">Joint Learning of Chinese Words, Terms and Keywords</title>
<affiliation confidence="0.926113">Laboratory of Computational Linguistics, Peking University, MOE, Science Department, Rensselaer Polytechnic Institute, USA</affiliation>
<email confidence="0.999773">jih@rpi.edu</email>
<abstract confidence="0.999309944444444">Previous work often used a pipelined framework where Chinese word segmentation is followed by term extraction and keyword extraction. Such framework suffers from error propagation and is unable to leverage information in later modules for prior components. In this paper, we propose a four-level Dirichlet Process based model (DP-4) to jointly learn the word distributions from the corpus, domain and document levels simultaneously. Based on the DP-4 model, a sentence-wise Gibbs sampler is adopted to obtain proper segmentation results. Meanwhile, terms and keywords are acquired in the sampling process. Experimental results have shown the effectiveness of our method.</abstract>
</variant>
</algorithm>
<algorithm name="ParsCit" version="110505">
<citationList>
<citation valid="true">
<authors>
<author>Baobao Chang</author>
<author>Wenzhe Pei</author>
<author>Miaohong Chen</author>
</authors>
<title>Inducing word sense with automatically learned hidden concepts.</title>
<date>2014</date>
<booktitle>In Proceedings of COLING 2014,</booktitle>
<pages>355--364</pages>
<location>Dublin, Ireland,</location>
<contexts>
<context position="3573" citStr="Chang et al., 2014" startWordPosition="547" endWordPosition="550"> KE to promote the performance of WS. Thus, one question comes to our minds: can words, terms and keywords be jointly learned with consideration of all the information from the corpus, domain, and document levels? Recently, the hierarchical Dirichlet process (HDP) model has been used as a smoothed bigram model to conduct word segmentation (Goldwater et al., 2006; Goldwater et al., 2009). Meanwhile, one strong point of the HDP based models is that they can model the diversity and commonality in multiple correlated corpora (Ren et al., 2008; Xu et al., 2008; Zhang et al., 2010; Li et al., 2012; Chang et al., 2014). Inspired by such existing work, we propose a four-level DP based model, 1774 Proceedings of the 2014 Conference on Empirical Methods in Natural Language Processing (EMNLP), pages 1774–1778, October 25-29, 2014, Doha, Qatar. c�2014 Association for Computational Linguistics Figure 1: DP-4 Model |VI named DP-4, to adapt to three levels: corpus, domain and document. In our model, various DPs are designed to reflect the smoothed word distributions in the whole corpus, different domains and different documents. Same as the DP based segmentation models, our model can be easily used as a semi-superv</context>
</contexts>
<marker>Chang, Pei, Chen, 2014</marker>
<rawString>Baobao Chang, Wenzhe Pei, and Miaohong Chen. 2014. Inducing word sense with automatically learned hidden concepts. In Proceedings of COLING 2014, pages 355–364, Dublin, Ireland, August.</rawString>
</citation>
<citation valid="true">
<authors>
<author>Sharon Goldwater</author>
<author>Thomas L Griffiths</author>
<author>Mark Johnson</author>
</authors>
<title>Contextual dependencies in unsupervised word segmentation.</title>
<date>2006</date>
<booktitle>In Proceedings of the 21st International Conference on Computational Linguistics and the 44th annual meeting of the Association for Computational Linguistics,</booktitle>
<pages>673--680</pages>
<contexts>
<context position="3318" citStr="Goldwater et al., 2006" startWordPosition="501" endWordPosition="504">ghbors. In a text processing pipeline of WS, TE and KE, it is obvious that imprecise WS results will make the overall system performance unsatisfying. At the same time, we can hardly make use of domain-level and document-level information collected in TE and KE to promote the performance of WS. Thus, one question comes to our minds: can words, terms and keywords be jointly learned with consideration of all the information from the corpus, domain, and document levels? Recently, the hierarchical Dirichlet process (HDP) model has been used as a smoothed bigram model to conduct word segmentation (Goldwater et al., 2006; Goldwater et al., 2009). Meanwhile, one strong point of the HDP based models is that they can model the diversity and commonality in multiple correlated corpora (Ren et al., 2008; Xu et al., 2008; Zhang et al., 2010; Li et al., 2012; Chang et al., 2014). Inspired by such existing work, we propose a four-level DP based model, 1774 Proceedings of the 2014 Conference on Empirical Methods in Natural Language Processing (EMNLP), pages 1774–1778, October 25-29, 2014, Doha, Qatar. c�2014 Association for Computational Linguistics Figure 1: DP-4 Model |VI named DP-4, to adapt to three levels: corpus,</context>
<context position="4708" citStr="Goldwater et al. (2006)" startWordPosition="722" endWordPosition="725"> Same as the DP based segmentation models, our model can be easily used as a semi-supervised framework, through exerting on the corpus level the word distributions learned from the available segmentation results. Referring to the work of Mochihashi et al. (2009), we conduct word segmentation using a sentence-wise Gibbs sampler, which combines the Gibbs sampling techniques with the dynamic programming strategy. During the sampling process, the importance values of segmented words are measured in domains and documents respectively, and words, terms and keywords are jointly learned. 2 DP-4 Model Goldwater et al. (2006) applied the HDP model on the word segmentation task. In essence, Goldwater’s model can be viewed as a bigram language model with a unigram back-off. With the language model, word segmentation is implemented by a character-based Gibbs sampler which repeatedly samples the possible word boundary positions between two neighboring words, conditioned on the current values of all other words. However, Goldwater’s model can be deemed as modeling the whole corpus only, and does not distinguish between domains and documents. To jointly learn the word information from the corpus, domain and document lev</context>
<context position="10795" citStr="Goldwater et al., 2006" startWordPosition="1856" endWordPosition="1859">cy in the general reference corpus. Similar to Eq. 7, we define the function KHdjm(·) to judge whether wi is an appropriate keyword. KHdjm(wi) = pdj 4 (wi) m p1(wi) During each sampling, we make use of Eqs. (7) and (8) to identify the most possible terms and keywords. Once a word is identified as a term or keyword, it will drop out of the sampling process in the following iterations. Its CRP explanation is that some customers (terms and keywords) find their proper tables and keep sitting there afterwards. 2.3 Sentence-wise Gibbs Sampler The character-based Gibbs sampler for word segmentation (Goldwater et al., 2006) is extremely slow to converge, since there exists high correlation between neighboring words. Here, we introduce the sentence-wise Gibbs sampling technique as well as efficient dynamic programming strategy proposed by Mochihashi et al. (2009). The basic idea is that we randomly select a sentence in each sampling process and use the Viterbi algorithm (Viterbi, 1967) to find the optimal segmentation results according to the word distributions derived from other sentences. Different from Mochihashi’s work, once terms or keywords are P nmw (.) + α2 s(djm[k], dj m)ndj w (wi) m[k] (5) 1776 identifi</context>
<context position="12413" citStr="Goldwater et al., 2006" startWordPosition="2126" endWordPosition="2129">ed of ten domains (including Physics, Computer, Agriculture, Sports, Disease, Environment, History, Art, Politics and Economy) and each domain is composed of 200 documents. On average each document consists of about 4800 Chinese characters. For these 2000 documents, three annotators have manually checked the segmented words, terms and keywords as the gold standard results for evaluation. As we know, there exists a large amount of manually-checked segmented text for the general domain, which can be used as the training data for further segmentation. As with other nonparametric Bayesian models (Goldwater et al., 2006; Mochihashi et al., 2009), our DP-4 model can be easily amenable to semi-supervised learning by imposing the word distributions of the segmented text on the corpus level. The news texts provided by Peking University (named PKU corpus)2 is used as the training data. This corpus contains about 1,870,000 Chinese characters and has been manually segmented into words. In our experiments, the concentration coefficient (α0) is finally set to 20 and the other three (α1∼3) are set to 15. The parameter K which controls the number of similar documents is set to 3. 3.2 Performance Evaluation The followin</context>
<context position="13807" citStr="Goldwater et al. (2006)" startWordPosition="2340" endWordPosition="2343">e maximum matching (RMM) algorithm with the compiled vocabulary; (3) Conditional Random Fields (CRFs)3 based supervised algorithm trained from the PKU corpus; (4) HDP based semisupervised algorithm (Goldwater et al., 2006) us1Nine domains are from http://www.datatang. com/data/44139 and we add an extra Disease domain. 2http://icl.pku.edu.cn 3We adopt CRF++(http://crfpp.googlecode. com/svn/trunk/doc/index.html) ing the PKU corpus. The strength of Mochihashi et al. (2009)’s NPYLM based segmentation model is its speed due to the sentence-wise sampling technique, and its performance is similar to Goldwater et al. (2006)’s model. Thus, we do not consider the NPYLM based model for comparison here. Then, the segmentation results of FMM, RMM, CRF, and HDP methods are used respectively for further extracting terms and keywords. We use the mutual information to identify the candidate terms or keywords composed of more than two segmented words. As for DP-4, this recognition process has been done implicitly during sampling. To measure the candidate terms or keywords, we refer to the metric in Nazar (2011) to calculate their importance in some specific domain or document. The metrics of F1 and the out-of-vocabulary R</context>
</contexts>
<marker>Goldwater, Griffiths, Johnson, 2006</marker>
<rawString>Sharon Goldwater, Thomas L. Griffiths, and Mark Johnson. 2006. Contextual dependencies in unsupervised word segmentation. In Proceedings of the 21st International Conference on Computational Linguistics and the 44th annual meeting of the Association for Computational Linguistics, pages 673– 680.</rawString>
</citation>
<citation valid="true">
<authors>
<author>Sharon Goldwater</author>
<author>Thomas L Griffiths</author>
<author>Mark Johnson</author>
</authors>
<title>A bayesian framework for word segmentation: Exploring the effects of context.</title>
<date>2009</date>
<journal>Cognition,</journal>
<volume>112</volume>
<issue>1</issue>
<contexts>
<context position="3343" citStr="Goldwater et al., 2009" startWordPosition="505" endWordPosition="508">sing pipeline of WS, TE and KE, it is obvious that imprecise WS results will make the overall system performance unsatisfying. At the same time, we can hardly make use of domain-level and document-level information collected in TE and KE to promote the performance of WS. Thus, one question comes to our minds: can words, terms and keywords be jointly learned with consideration of all the information from the corpus, domain, and document levels? Recently, the hierarchical Dirichlet process (HDP) model has been used as a smoothed bigram model to conduct word segmentation (Goldwater et al., 2006; Goldwater et al., 2009). Meanwhile, one strong point of the HDP based models is that they can model the diversity and commonality in multiple correlated corpora (Ren et al., 2008; Xu et al., 2008; Zhang et al., 2010; Li et al., 2012; Chang et al., 2014). Inspired by such existing work, we propose a four-level DP based model, 1774 Proceedings of the 2014 Conference on Empirical Methods in Natural Language Processing (EMNLP), pages 1774–1778, October 25-29, 2014, Doha, Qatar. c�2014 Association for Computational Linguistics Figure 1: DP-4 Model |VI named DP-4, to adapt to three levels: corpus, domain and document. In </context>
</contexts>
<marker>Goldwater, Griffiths, Johnson, 2009</marker>
<rawString>Sharon Goldwater, Thomas L. Griffiths, and Mark Johnson. 2009. A bayesian framework for word segmentation: Exploring the effects of context. Cognition, 112(1):21–54.</rawString>
</citation>
<citation valid="true">
<authors>
<author>Manfred Krug</author>
</authors>
<title>String frequency: A cognitive motivating factor in coalescence, language processing, and linguistic change.</title>
<date>1998</date>
<journal>Journal of English Linguistics,</journal>
<volume>26</volume>
<issue>4</issue>
<contexts>
<context position="8094" citStr="Krug, 1998" startWordPosition="1340" endWordPosition="1341">he numbers of customers with the same word type seated in the neighboring rooms, and at an unoccupied table with probability proportional to both the constant α3 and the probability that the 0 , 2 3 Ir. ...Hmt iw-NGo G, M mj wz 1775 customers with the same word type are seated on the same floor. 2.2 Model Inference It is important to build an accurate G0 which determines the prior word distribution p0(w). Similar to the work of Mochihashi et al. (2009), we consider the dependence between characters and calculate the prior distribution of a word wi using the string frequency statistics (Krug, 1998): ns(wi) p0(wi) = (1) Pns(.) where ns(wi) counts the character string composed of wi and the symbol “.” represents any word in the vocabulary V . Then, with the CRP metaphor, we can obtain the expected word unigram and bigram distributions on the corpus level according to G1 and Hw: n (wi) + α0p0 (wi) p1 (wi) = (2) Pn(.) + α0 nw (wi) + α1p1 (wi) p2 (wi|wi−1 = w) = (3) P nw (.) + α1 where the subscript numbers indicate the corresponding DP levels. n(wi) denotes the number of wi and nw(wi) denotes the number of the bigram &lt; w, wi &gt; occurring in the corpus. Next, we can easily get the bigram dist</context>
</contexts>
<marker>Krug, 1998</marker>
<rawString>Manfred Krug. 1998. String frequency: A cognitive motivating factor in coalescence, language processing, and linguistic change. Journal of English Linguistics, 26(4):286–320.</rawString>
</citation>
<citation valid="true">
<authors>
<author>Jiwei Li</author>
<author>Sujian Li</author>
<author>Xun Wang</author>
<author>Ye Tian</author>
<author>Baobao Chang</author>
</authors>
<title>Update summarization using a multilevel hierarchical dirichlet process model.</title>
<date>2012</date>
<booktitle>In Proceedings of Coling 2012,</booktitle>
<pages>1603--1618</pages>
<location>Mumbai, India.</location>
<contexts>
<context position="3552" citStr="Li et al., 2012" startWordPosition="543" endWordPosition="546">llected in TE and KE to promote the performance of WS. Thus, one question comes to our minds: can words, terms and keywords be jointly learned with consideration of all the information from the corpus, domain, and document levels? Recently, the hierarchical Dirichlet process (HDP) model has been used as a smoothed bigram model to conduct word segmentation (Goldwater et al., 2006; Goldwater et al., 2009). Meanwhile, one strong point of the HDP based models is that they can model the diversity and commonality in multiple correlated corpora (Ren et al., 2008; Xu et al., 2008; Zhang et al., 2010; Li et al., 2012; Chang et al., 2014). Inspired by such existing work, we propose a four-level DP based model, 1774 Proceedings of the 2014 Conference on Empirical Methods in Natural Language Processing (EMNLP), pages 1774–1778, October 25-29, 2014, Doha, Qatar. c�2014 Association for Computational Linguistics Figure 1: DP-4 Model |VI named DP-4, to adapt to three levels: corpus, domain and document. In our model, various DPs are designed to reflect the smoothed word distributions in the whole corpus, different domains and different documents. Same as the DP based segmentation models, our model can be easily </context>
</contexts>
<marker>Li, Li, Wang, Tian, Chang, 2012</marker>
<rawString>Jiwei Li, Sujian Li, Xun Wang, Ye Tian, and Baobao Chang. 2012. Update summarization using a multilevel hierarchical dirichlet process model. In Proceedings of Coling 2012, pages 1603–1618, Mumbai, India.</rawString>
</citation>
<citation valid="true">
<authors>
<author>Daichi Mochihashi</author>
<author>Takeshi Yamada</author>
<author>Naonori Ueda</author>
</authors>
<title>Bayesian unsupervised word segmentation with nested pitman-yor language modeling.</title>
<date>2009</date>
<booktitle>In Proceedings of the Joint Conference of the 47th Annual Meeting of the ACL and the 4th International Joint Conference on Natural Language Processing of the AFNLP: Volume</booktitle>
<volume>1</volume>
<pages>100--108</pages>
<contexts>
<context position="4347" citStr="Mochihashi et al. (2009)" startWordPosition="668" endWordPosition="671">uage Processing (EMNLP), pages 1774–1778, October 25-29, 2014, Doha, Qatar. c�2014 Association for Computational Linguistics Figure 1: DP-4 Model |VI named DP-4, to adapt to three levels: corpus, domain and document. In our model, various DPs are designed to reflect the smoothed word distributions in the whole corpus, different domains and different documents. Same as the DP based segmentation models, our model can be easily used as a semi-supervised framework, through exerting on the corpus level the word distributions learned from the available segmentation results. Referring to the work of Mochihashi et al. (2009), we conduct word segmentation using a sentence-wise Gibbs sampler, which combines the Gibbs sampling techniques with the dynamic programming strategy. During the sampling process, the importance values of segmented words are measured in domains and documents respectively, and words, terms and keywords are jointly learned. 2 DP-4 Model Goldwater et al. (2006) applied the HDP model on the word segmentation task. In essence, Goldwater’s model can be viewed as a bigram language model with a unigram back-off. With the language model, word segmentation is implemented by a character-based Gibbs samp</context>
<context position="7946" citStr="Mochihashi et al. (2009)" startWordPosition="1316" endWordPosition="1319">ifferent from the standard HDP, each customer sits at an occupied table with probability proportional to both the numbers of customers already seated there and the numbers of customers with the same word type seated in the neighboring rooms, and at an unoccupied table with probability proportional to both the constant α3 and the probability that the 0 , 2 3 Ir. ...Hmt iw-NGo G, M mj wz 1775 customers with the same word type are seated on the same floor. 2.2 Model Inference It is important to build an accurate G0 which determines the prior word distribution p0(w). Similar to the work of Mochihashi et al. (2009), we consider the dependence between characters and calculate the prior distribution of a word wi using the string frequency statistics (Krug, 1998): ns(wi) p0(wi) = (1) Pns(.) where ns(wi) counts the character string composed of wi and the symbol “.” represents any word in the vocabulary V . Then, with the CRP metaphor, we can obtain the expected word unigram and bigram distributions on the corpus level according to G1 and Hw: n (wi) + α0p0 (wi) p1 (wi) = (2) Pn(.) + α0 nw (wi) + α1p1 (wi) p2 (wi|wi−1 = w) = (3) P nw (.) + α1 where the subscript numbers indicate the corresponding DP levels. n</context>
<context position="11038" citStr="Mochihashi et al. (2009)" startWordPosition="1893" endWordPosition="1896">ssible terms and keywords. Once a word is identified as a term or keyword, it will drop out of the sampling process in the following iterations. Its CRP explanation is that some customers (terms and keywords) find their proper tables and keep sitting there afterwards. 2.3 Sentence-wise Gibbs Sampler The character-based Gibbs sampler for word segmentation (Goldwater et al., 2006) is extremely slow to converge, since there exists high correlation between neighboring words. Here, we introduce the sentence-wise Gibbs sampling technique as well as efficient dynamic programming strategy proposed by Mochihashi et al. (2009). The basic idea is that we randomly select a sentence in each sampling process and use the Viterbi algorithm (Viterbi, 1967) to find the optimal segmentation results according to the word distributions derived from other sentences. Different from Mochihashi’s work, once terms or keywords are P nmw (.) + α2 s(djm[k], dj m)ndj w (wi) m[k] (5) 1776 identified, we do not consider them in the segmentation process. Due to space limitation, the algorithm is not detailed here and can be referred in (Mochihashi et al., 2009). 3 Experiment 3.1 Data and Setting It is indeed difficult to find a standard </context>
<context position="12439" citStr="Mochihashi et al., 2009" startWordPosition="2130" endWordPosition="2133">ding Physics, Computer, Agriculture, Sports, Disease, Environment, History, Art, Politics and Economy) and each domain is composed of 200 documents. On average each document consists of about 4800 Chinese characters. For these 2000 documents, three annotators have manually checked the segmented words, terms and keywords as the gold standard results for evaluation. As we know, there exists a large amount of manually-checked segmented text for the general domain, which can be used as the training data for further segmentation. As with other nonparametric Bayesian models (Goldwater et al., 2006; Mochihashi et al., 2009), our DP-4 model can be easily amenable to semi-supervised learning by imposing the word distributions of the segmented text on the corpus level. The news texts provided by Peking University (named PKU corpus)2 is used as the training data. This corpus contains about 1,870,000 Chinese characters and has been manually segmented into words. In our experiments, the concentration coefficient (α0) is finally set to 20 and the other three (α1∼3) are set to 15. The parameter K which controls the number of similar documents is set to 3. 3.2 Performance Evaluation The following baselines are implemente</context>
</contexts>
<marker>Mochihashi, Yamada, Ueda, 2009</marker>
<rawString>Daichi Mochihashi, Takeshi Yamada, and Naonori Ueda. 2009. Bayesian unsupervised word segmentation with nested pitman-yor language modeling. In Proceedings of the Joint Conference of the 47th Annual Meeting of the ACL and the 4th International Joint Conference on Natural Language Processing of the AFNLP: Volume 1-Volume 1, pages 100–108.</rawString>
</citation>
<citation valid="true">
<authors>
<author>Rogelio Nazar</author>
</authors>
<title>A statistical approach to term extraction.</title>
<date>2011</date>
<journal>IJES, International Journal of English Studies,</journal>
<volume>11</volume>
<issue>2</issue>
<contexts>
<context position="10072" citStr="Nazar (2011)" startWordPosition="1734" endWordPosition="1735">wi) = nwm (wi)+X k where ndj w (wi) denotes the count of the bigram m[k] &lt; w, wi &gt; occurring in djm[k]. Next, we model the bigram distribution in djm as a DP with the base measure Hmw : pdj w (wi) + α3pm 3 (wi|wi−1) 4 (wi|wi−1 = w) = tdj m m P tdj w (.) + α3 m With CRP, we can also easily estimate the unigram probabilities pm 3 (wi) and pdj 4 (wi) respecm tively on the domain and document levels, through combining all the restaurants. To measure whether a word is eligible to be a term, the score function THm(·) is defined as: THm(wi) = p3m (wi) p1 (wi) This equation is inspired by the work of Nazar (2011), which extracts terms with consideration of both the frequency in the domain corpus and the frequency in the general reference corpus. Similar to Eq. 7, we define the function KHdjm(·) to judge whether wi is an appropriate keyword. KHdjm(wi) = pdj 4 (wi) m p1(wi) During each sampling, we make use of Eqs. (7) and (8) to identify the most possible terms and keywords. Once a word is identified as a term or keyword, it will drop out of the sampling process in the following iterations. Its CRP explanation is that some customers (terms and keywords) find their proper tables and keep sitting there a</context>
<context position="14294" citStr="Nazar (2011)" startWordPosition="2427" endWordPosition="2428">ion model is its speed due to the sentence-wise sampling technique, and its performance is similar to Goldwater et al. (2006)’s model. Thus, we do not consider the NPYLM based model for comparison here. Then, the segmentation results of FMM, RMM, CRF, and HDP methods are used respectively for further extracting terms and keywords. We use the mutual information to identify the candidate terms or keywords composed of more than two segmented words. As for DP-4, this recognition process has been done implicitly during sampling. To measure the candidate terms or keywords, we refer to the metric in Nazar (2011) to calculate their importance in some specific domain or document. The metrics of F1 and the out-of-vocabulary Recall (OOV-R) are used to evaluate the segmentation results, referring to the gold standard results. The second and third columns of Table 1 show the F1 and OOV-R scores averaged on the 10 domains for all the compared methods. Our method significantly outperforms FMM, RMM and HDP according to t-test (p-value ≤ 0.05). From the segmentation results, we can see that the FMM and RMM methods are highly dependent on the compiled vocabulary and their identified OOV words are mainly the one</context>
</contexts>
<marker>Nazar, 2011</marker>
<rawString>Rogelio Nazar. 2011. A statistical approach to term extraction. IJES, International Journal of English Studies, 11(2):159–182.</rawString>
</citation>
<citation valid="true">
<authors>
<author>Lu Ren</author>
<author>David B Dunson</author>
<author>Lawrence Carin</author>
</authors>
<title>The dynamic hierarchical dirichlet process.</title>
<date>2008</date>
<booktitle>In Proceedings of the 25th international conference on Machine learning,</booktitle>
<pages>824--831</pages>
<contexts>
<context position="3498" citStr="Ren et al., 2008" startWordPosition="531" endWordPosition="534">e use of domain-level and document-level information collected in TE and KE to promote the performance of WS. Thus, one question comes to our minds: can words, terms and keywords be jointly learned with consideration of all the information from the corpus, domain, and document levels? Recently, the hierarchical Dirichlet process (HDP) model has been used as a smoothed bigram model to conduct word segmentation (Goldwater et al., 2006; Goldwater et al., 2009). Meanwhile, one strong point of the HDP based models is that they can model the diversity and commonality in multiple correlated corpora (Ren et al., 2008; Xu et al., 2008; Zhang et al., 2010; Li et al., 2012; Chang et al., 2014). Inspired by such existing work, we propose a four-level DP based model, 1774 Proceedings of the 2014 Conference on Empirical Methods in Natural Language Processing (EMNLP), pages 1774–1778, October 25-29, 2014, Doha, Qatar. c�2014 Association for Computational Linguistics Figure 1: DP-4 Model |VI named DP-4, to adapt to three levels: corpus, domain and document. In our model, various DPs are designed to reflect the smoothed word distributions in the whole corpus, different domains and different documents. Same as the </context>
</contexts>
<marker>Ren, Dunson, Carin, 2008</marker>
<rawString>Lu Ren, David B. Dunson, and Lawrence Carin. 2008. The dynamic hierarchical dirichlet process. In Proceedings of the 25th international conference on Machine learning, pages 824–831.</rawString>
</citation>
<citation valid="true">
<authors>
<author>Andrew J Viterbi</author>
</authors>
<title>Error bounds for convolutional codes and an asymptotically optimum decoding algorithm.</title>
<date>1967</date>
<journal>IEEE Transactions on Information Theory,</journal>
<pages>260--269</pages>
<contexts>
<context position="11163" citStr="Viterbi, 1967" startWordPosition="1917" endWordPosition="1918">erations. Its CRP explanation is that some customers (terms and keywords) find their proper tables and keep sitting there afterwards. 2.3 Sentence-wise Gibbs Sampler The character-based Gibbs sampler for word segmentation (Goldwater et al., 2006) is extremely slow to converge, since there exists high correlation between neighboring words. Here, we introduce the sentence-wise Gibbs sampling technique as well as efficient dynamic programming strategy proposed by Mochihashi et al. (2009). The basic idea is that we randomly select a sentence in each sampling process and use the Viterbi algorithm (Viterbi, 1967) to find the optimal segmentation results according to the word distributions derived from other sentences. Different from Mochihashi’s work, once terms or keywords are P nmw (.) + α2 s(djm[k], dj m)ndj w (wi) m[k] (5) 1776 identified, we do not consider them in the segmentation process. Due to space limitation, the algorithm is not detailed here and can be referred in (Mochihashi et al., 2009). 3 Experiment 3.1 Data and Setting It is indeed difficult to find a standard evaluation corpus for our joint tasks, especially in different domains. As a result, we spent a lot of time to collect and an</context>
</contexts>
<marker>Viterbi, 1967</marker>
<rawString>Andrew J. Viterbi. 1967. Error bounds for convolutional codes and an asymptotically optimum decoding algorithm. IEEE Transactions on Information Theory, pages 260–269.</rawString>
</citation>
<citation valid="true">
<authors>
<author>Xiaojun Wan</author>
<author>Jianguo Xiao</author>
</authors>
<title>Single document keyphrase extraction using neighborhood knowledge.</title>
<date>2008</date>
<booktitle>In AAAI,</booktitle>
<volume>8</volume>
<pages>855--860</pages>
<contexts>
<context position="9046" citStr="Wan and Xiao, 2008" startWordPosition="1525" endWordPosition="1528">i) + α1p1 (wi) p2 (wi|wi−1 = w) = (3) P nw (.) + α1 where the subscript numbers indicate the corresponding DP levels. n(wi) denotes the number of wi and nw(wi) denotes the number of the bigram &lt; w, wi &gt; occurring in the corpus. Next, we can easily get the bigram distribution on the domain level by extending to the third DP. nm w (wi) + α2p2(wi|wi−1) pm 3 (wi|wi−1 = w) = (4) where nmw (wi) is the number of the bigram &lt; w, wi &gt; occurring in the mth domain. To model the bigram distributions on the document level, it is beneficial to consider the influence of related documents in the same domain (Wan and Xiao, 2008). Here, we only consider the influence from the K most similar documents with a simple similarity metric s(d1, d2) which calculates the Chinese character overlap ratio of two documents d1 and d2. Let djm denote the jth document in the mth domain and djm[k](1 ≤ k ≤ K) the K most similar documents. djm can be deemed to be “lengthened” by djm[k](1 ≤ k ≤ K). Therefore, we estimate the count of wi in djm as: tdjwm (wi) = nwm (wi)+X k where ndj w (wi) denotes the count of the bigram m[k] &lt; w, wi &gt; occurring in djm[k]. Next, we model the bigram distribution in djm as a DP with the base measure Hmw : </context>
</contexts>
<marker>Wan, Xiao, 2008</marker>
<rawString>Xiaojun Wan and Jianguo Xiao. 2008. Single document keyphrase extraction using neighborhood knowledge. In AAAI, volume 8, pages 855–860.</rawString>
</citation>
<citation valid="true">
<authors>
<author>Tianbing Xu</author>
<author>Zhongfei Zhang</author>
<author>Philip S Yu</author>
<author>Bo Long</author>
</authors>
<title>Dirichlet process based evolutionary clustering.</title>
<date>2008</date>
<booktitle>In ICDM’08,</booktitle>
<pages>648--657</pages>
<contexts>
<context position="3515" citStr="Xu et al., 2008" startWordPosition="535" endWordPosition="538">vel and document-level information collected in TE and KE to promote the performance of WS. Thus, one question comes to our minds: can words, terms and keywords be jointly learned with consideration of all the information from the corpus, domain, and document levels? Recently, the hierarchical Dirichlet process (HDP) model has been used as a smoothed bigram model to conduct word segmentation (Goldwater et al., 2006; Goldwater et al., 2009). Meanwhile, one strong point of the HDP based models is that they can model the diversity and commonality in multiple correlated corpora (Ren et al., 2008; Xu et al., 2008; Zhang et al., 2010; Li et al., 2012; Chang et al., 2014). Inspired by such existing work, we propose a four-level DP based model, 1774 Proceedings of the 2014 Conference on Empirical Methods in Natural Language Processing (EMNLP), pages 1774–1778, October 25-29, 2014, Doha, Qatar. c�2014 Association for Computational Linguistics Figure 1: DP-4 Model |VI named DP-4, to adapt to three levels: corpus, domain and document. In our model, various DPs are designed to reflect the smoothed word distributions in the whole corpus, different domains and different documents. Same as the DP based segmenta</context>
</contexts>
<marker>Xu, Zhang, Yu, Long, 2008</marker>
<rawString>Tianbing Xu, Zhongfei Zhang, Philip S. Yu, and Bo Long. 2008. Dirichlet process based evolutionary clustering. In ICDM’08, pages 648–657.</rawString>
</citation>
<citation valid="true">
<authors>
<author>Jianwen Zhang</author>
<author>Yangqiu Song</author>
<author>Changshui Zhang</author>
<author>Shixia Liu</author>
</authors>
<title>Evolutionary hierarchical dirichlet processes for multiple correlated time-varying corpora.</title>
<date>2010</date>
<booktitle>In Proceedings of the 16th ACM SIGKDD International Conference on Knowledge Discovery and Data Mining,</booktitle>
<pages>1079--1088</pages>
<location>New York, NY, USA.</location>
<contexts>
<context position="3535" citStr="Zhang et al., 2010" startWordPosition="539" endWordPosition="542">level information collected in TE and KE to promote the performance of WS. Thus, one question comes to our minds: can words, terms and keywords be jointly learned with consideration of all the information from the corpus, domain, and document levels? Recently, the hierarchical Dirichlet process (HDP) model has been used as a smoothed bigram model to conduct word segmentation (Goldwater et al., 2006; Goldwater et al., 2009). Meanwhile, one strong point of the HDP based models is that they can model the diversity and commonality in multiple correlated corpora (Ren et al., 2008; Xu et al., 2008; Zhang et al., 2010; Li et al., 2012; Chang et al., 2014). Inspired by such existing work, we propose a four-level DP based model, 1774 Proceedings of the 2014 Conference on Empirical Methods in Natural Language Processing (EMNLP), pages 1774–1778, October 25-29, 2014, Doha, Qatar. c�2014 Association for Computational Linguistics Figure 1: DP-4 Model |VI named DP-4, to adapt to three levels: corpus, domain and document. In our model, various DPs are designed to reflect the smoothed word distributions in the whole corpus, different domains and different documents. Same as the DP based segmentation models, our mod</context>
</contexts>
<marker>Zhang, Song, Zhang, Liu, 2010</marker>
<rawString>Jianwen Zhang, Yangqiu Song, Changshui Zhang, and Shixia Liu. 2010. Evolutionary hierarchical dirichlet processes for multiple correlated time-varying corpora. In Proceedings of the 16th ACM SIGKDD International Conference on Knowledge Discovery and Data Mining, pages 1079–1088, New York, NY, USA.</rawString>
</citation>
<citation valid="true">
<authors>
<author>Hai Zhao</author>
<author>Chang-Ning Huang</author>
<author>Mu Li</author>
</authors>
<title>An improved chinese word segmentation system with conditional random field.</title>
<date>2006</date>
<booktitle>In Proceedings of the Fifth SIGHAN Workshop on Chinese Language Processing,</booktitle>
<volume>volume</volume>
<pages>1082117</pages>
<contexts>
<context position="2495" citStr="Zhao et al., 2006" startWordPosition="369" endWordPosition="372">/1&apos;%&amp;Vi`)t(thrombocytopenia) n(with) Aff (heparinoid) (have) )`,*,,(relation). This is a correctly segmented Chinese sentence. The document containing the example sentence mainly talks about the property of ”A1f (heparinoid)” which can be regarded as one keyword of the document. At the same time, the word leu./J,,&amp;V�`Ih(thrombocytopenia) appears frequently in the disease domain and can be treated as a domain-specific term. However, for such a simple sentence, current segmentation tools perform poorly. The segmentation result with the state-of-the-art Conditional Random Fields (CRFs) approach (Zhao et al., 2006) is as follows: _k/1&apos;%&amp;(blood platelet) Ïi`(reduction) Ç(symptom) nA(of same kind) (liver) 17(always) s,*,,(relation) where is segmented into three common Chinese words and Alf A is mixed with its neighbors. In a text processing pipeline of WS, TE and KE, it is obvious that imprecise WS results will make the overall system performance unsatisfying. At the same time, we can hardly make use of domain-level and document-level information collected in TE and KE to promote the performance of WS. Thus, one question comes to our minds: can words, terms and keywords be jointly learned with considerat</context>
</contexts>
<marker>Zhao, Huang, Li, 2006</marker>
<rawString>Hai Zhao, Chang-Ning Huang, and Mu Li. 2006. An improved chinese word segmentation system with conditional random field. In Proceedings of the Fifth SIGHAN Workshop on Chinese Language Processing, volume 1082117.</rawString>
</citation>
</citationList>
</algorithm>
</algorithms>
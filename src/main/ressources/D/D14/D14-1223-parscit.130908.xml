<?xml version="1.0" encoding="UTF-8"?>
<algorithms version="110505">
<algorithm name="SectLabel" version="110505">
<variant no="0" confidence="0.000000">
<title confidence="0.9967195">
Resolving Referring Expressions in Conversational Dialogs for
Natural User Interfaces
</title>
<author confidence="0.949319">
Asli Celikyilmaz, Zhaleh Feizollahi, Dilek Hakkani-Tur, Ruhi Sarikaya
</author>
<affiliation confidence="0.885919">
Microsoft
</affiliation>
<email confidence="0.9749285">
asli@ieee.org, zhalehf@microsoft.com
dilek@ieee.org, ruhi.sarikaya@microsoft.com
</email>
<sectionHeader confidence="0.998584" genericHeader="abstract">
Abstract
</sectionHeader>
<bodyText confidence="0.999924227272727">
Unlike traditional over-the-phone spoken
dialog systems (SDSs), modern dialog
systems tend to have visual rendering on
the device screen as an additional modal-
ity to communicate the system’s response
to the user. Visual display of the system’s
response not only changes human behav-
ior when interacting with devices, but also
creates new research areas in SDSs. On-
screen item identification and resolution
in utterances is one critical problem to
achieve a natural and accurate human-
machine communication. We pose the
problem as a classification task to cor-
rectly identify intended on-screen item(s)
from user utterances. Using syntactic, se-
mantic as well as context features from the
display screen, our model can resolve dif-
ferent types of referring expressions with
up to 90% accuracy. In the experiments we
also show that the proposed model is ro-
bust to domain and screen layout changes.
</bodyText>
<sectionHeader confidence="0.999472" genericHeader="introduction">
1 Introduction
</sectionHeader>
<bodyText confidence="0.999794916666667">
Todays natural user interfaces (NUI) for applica-
tions running on smart devices, e.g, phones (SIRI,
Cortana, GoogleNow), consoles (Amazon FireTV,
XBOX), tablet, etc., can handle not only simple
spoken commands, but also natural conversational
utterances. Unlike traditional over-the-phone spo-
ken dialog systems (SDSs), user hears and sees the
system’s response displayed on the screen as an
additional modality. Having visual access to the
system’s response and results changes human be-
havior when interacting with the machine, creating
new and challenging problems in SDS.
</bodyText>
<figure confidence="0.352411625">
[System]: How can i help you today ?
[User]: Find non-fiction books by Chomsky.
[System]: (Fetches the following books from database)
[User]: “show details for the oldest production” or
“details for the syntax book” or
“open the last one” or
“i want to see the one on linguistics” or
“bring me Jurafsky’s text book”
</figure>
<figureCaption confidence="0.249481666666667">
Table 1: A sample multi-turn dialog. A list of second turn
utterances referring to the last book (in bold) and a new search
query (highlighted) are shown.
</figureCaption>
<bodyText confidence="0.9990059">
Consider a sample dialog in Table 1 between a
user and a NUI in the books domain. After the sys-
tem displays results on the screen, the user may
choose one or more of the on-screen items with
natural language utterances as shown in Table 1.
Note that, there are multiple ways of referring to
the same item, (e.g. the last book)1. To achieve a
natural and accurate human to machine conversa-
tion, it is crucial to accurately identify and resolve
referring expressions in utterances. As important
as interpreting referring expressions (REs) is for
modern NUI designs, relatively few studies have
investigated withing the SDSs. Those that do fo-
cus on the impact of the input from multimodal
interfaces such as gesture for understanding (Bolt,
1980; Heck et al., 2013; Johnston et al., 2002),
touch for ASR error correction (Huggins-Daines
and Rudnicky, 2008), or cues from the screen
(Balchandran et al., 2008; Anastasiou et al., 2012).
Most of these systems are engineered for a specific
</bodyText>
<footnote confidence="0.939089333333333">
1An item could be anything from a list, e.g. restaurants,
games, contact list, organized in different lay-outs on the
screen.
</footnote>
<page confidence="0.911057">
2094
</page>
<note confidence="0.907506">
Proceedings of the 2014 Conference on Empirical Methods in Natural Language Processing (EMNLP), pages 2094–2104,
October 25-29, 2014, Doha, Qatar. c�2014 Association for Computational Linguistics
</note>
<bodyText confidence="0.99994838">
task, making it harder to generalize for different
domains or SDSs. In this paper, we investigate a
rather generic contextual model for resolving nat-
ural language REs for on-screen item selection to
improve conversational understanding.
Our model, which we call FIS (Flexible Item
Selection), is able to (1) detect if the user is re-
ferring to any item(s) on the screen, and (2) re-
solve REs to identify which items are referred to
and score each item. FIS is a learning based sys-
tem that uses information from pair of user utter-
ance and candidate items on the screen to model
association between them. We cast the task as a
classification problem to determine whether there
is a relation between the utterance and the item,
representing each instance in the training dataset
as relational features.
In a typical SDS, the spoken language under-
standing (SLU) engine maps user utterances into
meaning representation by identifying user’s in-
tent and token level semantic slots via a seman-
tic parser (Mori et al., 2008). The dialog man-
ager uses the SLU components to decide on the
correct system action. For on-screen item selec-
tion SLU alone may not be sufficient. To correctly
associate the user’s utterance with any of the on-
screen items one would need to resolve the rela-
tional information between the utterance and the
items. For instance, consider the dialog in Ta-
ble 1. SLU engine can provide signals to the di-
alog model about the selected item, e.g., that “lin-
guistics” is a book-genre or content, but may not
be enough to indicate which book the user is refer-
ring. FIS module provides additional information
for the dialog manager by augmenting SLU com-
ponents.
In §3, we provide details about our data as well
as data collection and annotation steps. In §4, we
present various syntactic and semantic features to
resolve different REs in utterances. In the exper-
iments (§6), we evaluate the individual impact of
each feature on the FIS model. We analyze the
performance of the FIS model per each type of
REs. Finally, we empirically investigate the ro-
bustness of the FIS model to domain and display
screen changes. When tested on a domain that
is unseen to the training data or on a device that
has a different NUI design, the performance only
slightly degrades proving its robustness to domain
and design changes.
</bodyText>
<sectionHeader confidence="0.999786" genericHeader="related work">
2 Related Work
</sectionHeader>
<bodyText confidence="0.99996762">
Although the problems of modern NUIs on smart
devices are fairly new, RE resolution in natural
language has been studied by many in NLP com-
munity.
Multimodal systems provide a natural and ef-
fective way for users to interact with computers
through multiple modalities such as speech, ges-
ture, and gaze. Since the first appearance of the
Put-That-There system (Bolt, 1980), a number of
multimodal systems have been built, among which
there are systems that combine speech, point-
ing (Neal, 1991), and gaze (Koons et al., 1993),
systems that engage users in an intelligent con-
versation (Gustafson et al., 2000). Earlier stud-
ies have shown that multimodal interfaces enable
users to interact with computers naturally and ef-
fectively (Schober and Clark, 1989; Oviatt et al.,
1997). Considered as part of the situated interac-
tive frameworks, many work focus on the prob-
lem of predicting how the user has resolved REs
that is generated by the system, e.g., (Clark and
Wilkes-Gibbs, ; Dale and Viethen, 2009; Giesel-
mann, 2004; Janarthanam and Lemon, 2010; Gol-
land et al., 2010). In this work, focusing on smart
devices, we investigate how the system resolves
the REs in user utterances to take the next correct
action.
In (Pfleger and J.Alexandersson, 2006) a refer-
ence resolution model is presented for a question-
answering system on a mobile, multi-modal inter-
face. Their system has several features to parse
the posed question and keep history of the dia-
log to resolve co-reference issues. Their question-
answering model uses gesture as features to re-
solve queries such as “what’s the name of that
[pointing gesture] player?”, but they do not re-
solve locational referrals such as “the middle one”
or “the second harry potter movie”. Others such as
(Funakoshi et al., 2012) resolve anaphoric (“it”)
or exophoric (“this one”) types of expressions in
user utterances to identify geometric objects. In
this paper, we study several types of REs to build
a natural and flexible interaction for the user.
(Heck et al., 2013) present an intent prediction
model enriched with gesture detector to help dis-
ambiguate between different user intents related to
the interface. In (Misu et al., 2014) a situated in-
car dialog model is presented to answer drivers’
spoken queries about their surroundings (no dis-
play screen). They integrate multi-modal inputs of
</bodyText>
<page confidence="0.98496">
2095
</page>
<bodyText confidence="0.9999685">
speech, geo-location and gaze. We investigate a
variety of REs for visual interfaces, and analyze
automatic resolution in a classification task intro-
ducing a wide range of syntactic, semantic and
contextual features. We look at how REs change
with screen layout comparing different devices.
To the best of our knowledge, our work is first to
analyze REs from these aspects.
</bodyText>
<sectionHeader confidence="0.996651" genericHeader="method">
3 Data
</sectionHeader>
<bodyText confidence="0.999946826086957">
Crowdsourcing services, such as Amazon Me-
chanical Turk or CrowdFlower, have been exten-
sively used for a variety of NLP tasks (Callison-
Burch and Dredze, 2010). Here we explain how
we collected the raw utterances from Crowd-
Flower platform (crowdflower.com).
For each HITApp (Human Intelligence Task
Application), we provide judges with a written ex-
planation about our Media App, a SDS built on a
device with a large screen which displays items in
a grid style layout, and what this particular sys-
tem would do, namely search for books, music,
tv and movies media result 2 Media App returns
results based on the user query using an already
implemented speech recognition, SLU and dialog
engines. For each HIT, the users are shown a dif-
ferent screenshot showing the Media App’s search
results after a first-turn query is issued (e.g., “find
non-fiction books by Chomsky” in Table 1). Users
are asked to provide five different second turn text
utterances for each screenshot. We launch several
hitapps each with a different prompt to cover dif-
ferent REs.
</bodyText>
<subsectionHeader confidence="0.999918">
3.1 HITApp Types and Data Collection
</subsectionHeader>
<bodyText confidence="0.999977166666667">
A grid of media items is shown to the user with
a red arrow pointing to the media result we want
them to refer to (see Fig. 1). They can ask to play
(an album or an audio book), select, or ask details
about the particular media item. Each item in each
grid layout becomes a different HIT or screenshot.
</bodyText>
<subsectionHeader confidence="0.971494">
3.1.1 Item Layout and Screen Type Variation
</subsectionHeader>
<bodyText confidence="0.999136">
The applications we consider have the following
rowxcolumn layouts: 1x6, 2x6 and 3x6, as
shown in Fig. 1 (columns vary depending on the
returned item size). By varying the layout, we ex-
pect the referent of the last and the bottom layer
items to change depending on how many rows, or
</bodyText>
<footnote confidence="0.759053">
2Please e-mail the first author to inquire about the
datasets.
</footnote>
<figure confidence="0.622883">
(c) Single row display. (d) Display forcing location
based referring expressions.
</figure>
<figureCaption confidence="0.989567">
Figure 1: Sketches of different HITApp Screens.
</figureCaption>
<bodyText confidence="0.97628927027027">
The red arrows point to the media we want the an-
notators to refer.
columns exist in the grid. In addition, phrases like
“middle”, or ”center” would not appear in the data
when there are only one or two rows. Also, we ex-
pect that the distribution of types of utterances to
vary. For example, in a grid of 1x6, “the second
one” makes sense, but not so much on a 2x6 grid.
We expect similar change based on the number of
columns.
We use two kinds of screenshots to collect ut-
terances with variations in REs. The first type of
screenshots are aimed to bias the users to refer to
items ’directly’ using (full/partial) titles or ’indi-
rectly’ using other descriptors, or meta informa-
tion such as year the movie is taken, or the au-
thor of the book. To collect utterances that indi-
rectly referred to items, we need to show screen
shots displaying system results with common ti-
tles, eventually forcing the user to use other de-
scriptors for disambiguation. For example, given
the first turn query “find harry potter movies”, the
system returns all the Harry Potter series, all of
which contain the words Harry Potter in the title.
The user can either refer in their utterance with the
series number, the subtitle (e.g. The prisoners of
Azkaban) or the location of the movie in the grid
or by date, e.g., “the new one”,
Because some media items have long titles, or
contain foreign names that are not easy to pro-
nounce, users may chose to refer these items by
their location on the display, such as “top right”,
“first album”, “the movie on the bottom left”, etc.
The second type of screen shots contains a tem-
plate for each layout with no actual media item
(Fig. 1(d)) which simply forces user to use loca-
tional references.
</bodyText>
<figure confidence="0.903007">
(a) A two row display. (b) A three row display.
</figure>
<page confidence="0.881863">
2096
</page>
<subsubsectionHeader confidence="0.555617">
3.1.2 Interface Design Variation
</subsubsectionHeader>
<bodyText confidence="0.999581793103448">
In order to test our model’s robustness to a
different screen display on a new device, we
employ an additional collection running another
application named places, designed for hand-
held devices. The places application can assist
users in finding local businesses (restaurants, ho-
tels, schools, etc.). and by nature of the de-
vice size can display fewer media items and ar-
ranges them in a list (one column). The num-
ber of items on the screen at any given time de-
pends on the size of the hand-held device screen.
The user can scroll
down to see the rest
of the results. Our
collection displays the
items in a 3, 4, and
5-rows per 1 column
layout as shown in
Fig. 2. We use the
same variations in
prompts as in §3.1. To
generate the HitApp
screens, we search
for nearby places, in
the top search engines
(Google, Bing) and
collect the results to
the first turn natural language search queries
(e.g.,“find me sushi restaurants near me”).
</bodyText>
<subsectionHeader confidence="0.999164">
3.2 Data Annotation
</subsectionHeader>
<bodyText confidence="0.9998058125">
We collect text utterances using our media and
places application. Using a similar HitApp we
labeled each utterance with a domain, intent and
segments in utterance with slot tags (see Table 2).
The annotation agreement, Kappa measure (Co-
hen, 1960) is around 85%. Since we are building a
relational model between utterances and each item
on the screen, we ask the annotators to label each
utterance-item as ’0’ or ’1’ indicating if the utter-
ance is referring to that item or not. ’1’ means
the item is the intended one. ’0’ indicates the item
is not intended one or the utterance is not refer-
ring to any item on the screen, e.g., new search
query. We also ask the annotators to label each
utterance whether they contain locational (spatial)
references.
</bodyText>
<table confidence="0.992197818181818">
Domain Intents (I) &amp; Slots
movie I: find-movie/director/actor,buy-ticket
Slots: name, mpaa-rating (g-rated), date,
books I: find-book, buy-book,
Slots: name, genre(thriller), author, publisher,
music I: find-album, find-song,
Slots: song-name, genre, album-type,...
tv I: find-tvseries/play/add-to-queue..
Slots: name, type(cartoon), show-time....
places I: find-place, select-item(first one)..
Slots: place-type, rating, nearby(closest)....
</table>
<tableCaption confidence="0.996324">
Table 2: A sample of intents and semantic slot tags
</tableCaption>
<bodyText confidence="0.636232666666667">
of utterance segments per domain. Examples for
some slots values are presented in parenthesis as
italicized.
</bodyText>
<subsectionHeader confidence="0.994323">
3.3 Types of Observed Referring Expressions
</subsectionHeader>
<bodyText confidence="0.998653952380952">
We observe four main categories of REs in the ut-
terances that are collected by varying the prompts
and HITApp screens in crowd-sourcing:
Explicit Referential : Explicit mentions of
whole or portions of the title of the item on the
screen, and no other descriptors, e.g.,“show me the
details of star wars six” (referring to the item with
title ”Star wars: Episode VI - Return of the Jedi”).
Implicit Referential : The user refers to the
item using distinguishing features other than the
title, such as the release or publishing date, writ-
ers, actors, image content (describing the item im-
age), genre, etc. “how about the one with Kevin
Spacey”.
Explicit Locational : The user refers to the
item using the grid design, e.g., “i want to pur-
chase the e-book on the bottom right corner”.
Implicit Locational : Locational references in
relation to other items on the screen, e.g., “the sec-
ond of Dan Brown’s book” (showing two of the
Dan Brown’s book on the same row).
</bodyText>
<sectionHeader confidence="0.97648" genericHeader="method">
4 Feature Extraction for FIS Model
</sectionHeader>
<bodyText confidence="0.9999545">
Here, provide descriptions of each set of features
of FIS model used to resolve each expression.
</bodyText>
<subsectionHeader confidence="0.997454">
4.1 Similarity Features (SIM)
</subsectionHeader>
<bodyText confidence="0.861368625">
Similarity features represent the lexical overlap
between the utterance and the item’s title (that
is displayed on the user’s screen) and are mainly
aimed to resolve explicit REs. We represent
each utterance ui and item-title tk as sequence of
words:
ui={wi(1), ... , wi(ni)}
tk={wk(1), ... , wk(mk)}
</bodyText>
<figureCaption confidence="0.945232166666667">
Figure 2: A HitApp
screen of places app. Items
returned by the system re-
garding the first-turn utter-
ance ”burger places near
me?”
</figureCaption>
<page confidence="0.614364">
2097
</page>
<figure confidence="0.968285571428572">
item bigrams &lt;bos&gt; call five guys and fries &lt;eos&gt;
&lt;bos&gt; five
five guys
guys burgers
burgers and
and fries
fries &lt;eos&gt;
</figure>
<tableCaption confidence="0.9904135">
Table 3: Bigram overlap between the item “five guys burg-
ers and fries” and utterance“five guys and fries”.
</tableCaption>
<bodyText confidence="0.9693005625">
where wi(j) and wk(j) are the jth word in the se-
quence. Since inflectional morphology may make
a word appear in an utterance in a different form
than what occurs in the official title, we use both
the word form as it appears in the utterance and in
the item title. For example, burger and burgers, or
woman and women are considered as four distinct
words and all included in the bag-of-words. Us-
ing this representation we calculate four different
similarity measures:
Jaccard Similarity: A common feature that
can represent the ratio of the intersection to
the union of unigrams. Consider, for instance,
ui=“call five guys and fries” and the item tk=“five
guys burgers and fries” in Fig 2. The Jaccard sim-
ilarity S(i,k) is:
</bodyText>
<equation confidence="0.554067">
S(i,k)=1- ( c(ri n rk)/c(ri U rk) )
</equation>
<bodyText confidence="0.962190379310345">
where the ri and rk are unigrams of ui and tk re-
spectively. c(ri n rk) is the number of common
words of ui and tk, c(ri U rk) is the total unigram
vocabulary size between them. In this case, the
S(i,k)=0.66.
Orthographic Distance: Orthographic dis-
tance represent similarity of two text and can be
as simple as an edit distance (Levenshtein dis-
tance) between their graphemes. The Levenshtein
distance (Levenshtein, 1965) counts the insertion,
deletion and substitution operations that are re-
quired to transform an utterance ui into item’s title
tk.
Word Order: This feature represents how sim-
ilar are the order of words in two text. Sentences
containing the same words but in different orders
may result in different meanings. We extend Jac-
card similarity by defining bigram word vectors ri
and rk and look for overlapping bigrams as in Ta-
ble 3. Among 6 bigrams between them, only 2
are overapping, hence the word-order similarity is
S(i,k)=0.33.
Word Vector: This feature is the cosine sim-
ilarity between the utterance ui and the item-
title tk that measures the cosine of the an-
gle between them. Here, we use the uni-
gram word counts to represent the word vec-
tors and the word vector similarity is defined as:
S(i,k)=(ri&apos;rk)/IIriII&apos; IIrkII.
</bodyText>
<subsectionHeader confidence="0.991801">
4.2 Knowledge Graph Features
</subsectionHeader>
<bodyText confidence="0.996288032258064">
This binary feature is used to represent overlap be-
tween utterance and the meta information about
the item and is mainly aimed to resolve implicit
REs.
First, we obtain the meta information about
the on-screen items using Freebase (Bollacker et
al., 2008), the knowledge graph that contains
knowledge about classes (books, movies, ...) and
their attributes (title, publisher, year-released, ...).
Knowledge is often represented as the attributes
of the instances, along with values for those prop-
erties. Once we obtain the attribute values of
the item from Freebase, we check if any attribute
overlaps with part of the utterance. For instance,
given an utterance “how about the one with Kevin
Spacey”, and the item-title “House of Cards”, the
knowledge graph attributes include year(2013),
cast(Kevin Spacey), director(James Foley),... We
turn the freebase feature ’on’ since the actor at-
tribute of that item is contained in the utterance.
We also consider partial matches, e.g., last name
of the actor attribute.
This feature is also used to resolve implicit REs,
with item descriptions, such as “the messenger boy
with bicycle” referring to the media item Ride Like
Hell, a movie about a bike messenger. The syn-
opsis feature in Freebase fires the freebase meta
feature as the synopsis includes the following pas-
sage: “... in which real ✿✿✿✿✿✿✿✿✿
messenger✿✿✿✿✿ boys are used
as stunts... ”.
</bodyText>
<subsectionHeader confidence="0.943675">
4.3 Semantic Location Labeler (SLL)
Feature
</subsectionHeader>
<bodyText confidence="0.999975153846154">
This feature set captures spatial cues in utterances
and is mainly aimed to resolve explicit locational
REs. Our goal is to capture the location indicating
tokens in utterances and then resolve the referred
location on the screen by using an indicator fea-
ture. We implement the SLL (Semantic Location
Labeler), a sequence labeling model to tag loca-
tional cues in utterances using Conditional Ran-
dom Fields (CRF) (Lafferty et al., 2001).
We sampled a set of locational utterances from
each domain to be used as training set. We
asked the annotators to label tokens with four
different semantic tags that indicate a location.
</bodyText>
<page confidence="0.970019">
2098
</page>
<bodyText confidence="0.989854026315789">
The semantic tags include row and column in-
dicator tags, referring to the position or pivotal
reference. For instance, in “second ✿✿✿✿✿✿ from the
top”, “second” is the column-position, and
✿✿✿
“top” is the row-pivot, indicating the pivotal
reference of the row in a multi-row grid dis-
play. Also in “✿✿✿✿third from the ✿✿✿last”, the “third”
is the column-position, and the “last” is the
column-pivot, the pivotal reference of the col-
umn in a multi-column grid display. The fourth
tag, row-position, is used when the specific
row is explicitly referred, such as in “the Harry
Potter movie in the ✿✿✿first row”.
To train our CRF-based SLL model we use
three types of features: the current word, window
words e.g., previous-word, next-word, etc., using
five-window around the current word, and syntac-
tic features from the part-of-speech (POS) tagger
using the Stanford’s parser (Klein and Manning,
2003).
Row Indicator Feature: This feature sets the
relationship between the n-gram in an utterance in-
dicated by the row-position or row-pivot
tag and the item’s row number on the screen. For
instance, given SSL output row-pivot(’top’)
and item’s location row=1, the value of the feature
is set to ’1’. If no row tag is found by SLL, this
feature is set to ’0’. We use regular expressions to
parse the numerical indicators, e.g., ’top’=’1’.
Column Indicator Feature: Similarly,
this feature indicates if a phrase in utterance
indicated by the column-position or
column-pivot tag matches the item’s col-
umn number on the screen. If SLL model tags
column-pivot(’on the left’), then using the
item’s column number(=1), the value of this
feature is set to ’1’.
</bodyText>
<subsectionHeader confidence="0.994075">
4.4 SLU Features
</subsectionHeader>
<bodyText confidence="0.999986774193548">
The SLU (Spoken Language Understanding) fea-
tures are used to resolve implicit and explicit REs.
For our dialog system, we build one SLU model
per each domain to extract two sets of semantic at-
tributes from utterances: user’s intent and seman-
tic slots based on a predefined semantic schema
(see examples in Table 2). We use the best in-
tent hypothesis as a categorical feature in our FIS
model. Although FIS is not an intent detection
model, the intent from SLU is an effective seman-
tic feature in resolving REs. Consider second turn
utterance such as “weather in seattle”, which is
a ’find’ intent that is a new search or not related
to any item on the screen. We map SLU intents
such as find-book or find-place, to more specific
ones, so that the intent feature would have values
such as find, filter, check-time, not specific to a
domain or device. The intent feature helps us to
identify if user’s utterance is related to any item
on the screen. We also use the best slot hypothesis
from the SLU slot model and search if there is full
overlap of any recognized slot value with either the
item-title or the item meta-information from free-
base. In addition, we include the longest slot value
n-gram match as an additional feature. We add
a binary feature per domain, indicating whether
there is a slot value match. Because we are us-
ing generic intents as categorical features instead
of specific intents, and a slot value match feature
instead of domain specific slot types as features,
our models are rather domain independent.
</bodyText>
<sectionHeader confidence="0.998874" genericHeader="method">
5 GBDT Classifier
</sectionHeader>
<bodyText confidence="0.999957222222222">
Among various classifier learning algorithms, we
choose the GBDT (gradient boosted decision tree)
(Friedman, 2001; Hastie et al., 2009), also known
as MART (Multiple Additive Regression Trees).
GBDT3 is an efficient algorithm which learns an
ensemble of trees. We find the main advantage
of the decision tree classifier as opposed to other
non-linear classifiers such as SVM (support vec-
tor machines) (Vapnik, 1995) or NN (neural net-
works) (Bishop, 1995) is the interpretability. De-
cision trees are ”white boxes” in the sense that per-
feature gain can be expressed by the magnitude
of their weights, while SVM or NN’s are gener-
ally black boxes, i.e. we cannot read the acquired
knowledge in a comprehensible way. Addition-
ally, decision trees can easily accept categorical
and continuous valued features. We also present
the results of the SVM models.
</bodyText>
<sectionHeader confidence="0.999674" genericHeader="conclusions">
6 Experiments
</sectionHeader>
<bodyText confidence="0.999814428571429">
We investigate several aspects of the SISI model
including its robustness in resolving REs for do-
main or device variability. We start with the details
of the data and model parameters.
We collect around 16K utterances in the me-
dia domains (movies, music, tv, and books) and
around 10K utterances in places (businesses and
</bodyText>
<footnote confidence="0.983234333333333">
3Treenet: http://www.salford-systems.com/products/
treenet is the implementation of the GBDT which is used in
this paper.
</footnote>
<page confidence="0.990043">
2099
</page>
<table confidence="0.999800545454546">
Feature Description Movies Tv Music Book Overall Media Places
GBDT SVM GBDT SVM GBDT SVM GBDT SVM GBDT SVM GBDT SVM
SLL 79.6 77.1 62.0 62.0 77.1 76.5 63.7 63.0 83.6 82.7 67.9 68.9
SIM 86.6 85.7 78.7 74.1 84.9 84.0 81.6 77.3 88.5 88.3 67.1 66.5
Knowledge Graph (KG) 81.0 82.0 64.8 65.6 86.3 85.4 77.8 77.9 84.4 84.1 76.5 76.5
SLU (Gold) 91.7 91.8 89.1 88.5 87.8 87.5 86.3 84.9 83.7 83.2 77.8 71.1
SLU (Pred.) 75.8 72.6 80.3 79.8 84.3 84.1 82.4 82.4 81.4 80.9 71.4 67.8
SIM+SLL 90.9 90.2 87.2 87.1 85.9 86.2 88.5 87.6 91.9 91.9 78.9 73.4
SIM+SLL+KG 91.7 91.3 89.9 89.1 89.1 87.7 91.4 90.3 93.0 92.7 85.9 82.3
SIM+SLL+KG+SLU(Gold) 96.2 95.01 95.2 95.09 90.3 89.9 94.6 94.0 93.7 93.2 86.3 84.3
SIM+SLL+KG+SLU(Pred.) 90.9 90.8 92.3 92.00 86.9 85.7 93.1 93.0 89.3 88.9 85.7 83.9
</table>
<tableCaption confidence="0.9784585">
Table 5: Performance of the FIS models on test data using different features. Acc:Accuracy,. SIM: sim-
ilarity features; SLU:Spoken Language Understanding features (intent and slot features); SLL:Semantic
Locational Labeler features; Gold: using true intent and slot values, Pred.: using predicted intent and
slot values from the SLU models.
</tableCaption>
<table confidence="0.998533">
Model: Movies TV Music Books Places
IntentAcc. 84.5% 87.4% 87.6% 98.1% 89.5%
Slot F-score 92.1F 89.4F 88.5F 86.6F 88.4F
</table>
<tableCaption confidence="0.997427">
Table 4: The performance of the SLU Engine’s
</tableCaption>
<bodyText confidence="0.978252592592592">
intent detection models in accuracy (Acc.) and slot
tagging models in F-Score on the test dataset.
locations) domain. We also construct additional
negative instances from utterance-item pairs us-
ing first turn non-selection queries, which mainly
indicate a new search or starting over. In total
we compile around 250K utterance-item pairs for
media domains and 150K utterance-item pairs for
the places domain.4 We randomly split each col-
lection into 60%-20%-20% parts to construct the
train/dev/test datasets. We use the dev set to tune
the regularization parameter for the GBDT and
SVM using LIBSVM (Chang and Lin, 2011) with
linear kernel.
We use the training dataset to build the SLU in-
tent and slot models for each domain. For the in-
tent model, we use the GBDT classifier with n-
gram and lexicon features. The lexicon entries are
obtained from Freebase and are used as indicator
variables, e.g., whether the utterance contains an
instance which exists in the lexicon. Similarly,
we train a semantic slot tagging model using CRF
method. We use n-gram features with up to five-
gram window, and lexicon features similar to the
intent models. Table 4 shows the accuracy and F-
score values of SLU models on the test data. The
slot and intent performance is consistent accroess
</bodyText>
<footnote confidence="0.974555666666667">
4In the final version of the paper, we will provide anno-
tated data sets on a web page, which is reserved due to blind
review.
</footnote>
<bodyText confidence="0.999331666666667">
domains. The books domain has only two intents
and hence we observe much better intent perfor-
mance compared to other domains.
</bodyText>
<subsectionHeader confidence="0.99982">
6.1 Impact of Individual FIS Features
</subsectionHeader>
<bodyText confidence="0.999975166666667">
In our first experiment, we investigate the impact
of individual feature sets on FIS model’s perfor-
mance. We train a set of FIS models on the entire
media dataset to investigate the per-feature gain on
the test dataset for each domain. We also train an-
other set of FIS models with the same feature sets,
this time on the places dataset and present the re-
sults on the places test set. Table 5 shows the re-
sults. We measure the performance starting with
individual feature sets, and then incrementally add
each feature set. Note that the SLU feature set
includes the categorical intent, binary slot-value
match and the longest slot value n-gram match
with the item’s title or meta information. The SLL
feature set includes two features indicating the row
and column (see §4.3).
As expected, larger gains in accuracy are ob-
served when features that resolve different REs
are used. Resolving locational cues in utter-
ances with SLL features considerably impacts the
performance when used together with similarity
(SIM) features. We see a positive impact on per-
formance as we add the knowledge graph features,
which are used to resolve implicit REs. Using
only the predicted SLU features in feature gener-
ation without golden values degrades the perfor-
mance. Although the results are not statistically
significant, the GBDT outperforms the SVM for
almost all models, except for a few models, where
the results are similar. However, the models which
</bodyText>
<page confidence="0.97883">
2100
</page>
<table confidence="0.95045625">
Places Domain ♦ All Media Domains
All Media Places
Utterance Type % Acc. % Acc.
All utterances 100% 93.7% 100% 86.3%
Direct/IndirectRE 81% 93.9% 73% 86.9%
Locational RE 19% 92.5% 28% 85.2%
Explicit RE 60% 94.3% 45% 88.4%
Implicit RE 21% 83.4% 28% 72.2%
</table>
<figure confidence="0.927216">
SLU-intent
SIM-WordVector
SIM-Levenstein
SLU-Slot
SIM-Jaccard
SLL
SIM-WordOrder
Knowledge-Graph
0.2 0.4 0.6 0.8 1
Feature Weights
</figure>
<figureCaption confidence="0.9982105">
Figure 3: A sample of normalized feature weights of the
GBDT FIS models across domains.
</figureCaption>
<bodyText confidence="0.999942913043478">
combine different features as apposed to individ-
ual feature set (the above the line models versusu
below the horizantil line models) are statistically
significant (based on the student t-test p¡0.01).
Next, we illustrate the significance of individual
features across domains as well as devices. Fig. 3
compares the normalized feature weights of me-
dia and places domains. Across domains there are
similar features with similar weight values such as
SLU-intent, some similarity features (SIM-) and
even spatial cue features (SLL). It is not surpris-
ing to observe that the places domain knowledge-
graph meta feature weights are noticeably larger
than all media model features. We think that this
is due to the way the REs are used when the de-
vice changes (places app is on a phone with a
smaller screen display). Especially, places appli-
cation users refer items related to restaurants, li-
braries, etc., not so much by their names, but more
so with implicit REs by using: the location (refer-
ring to the address: “call the one on 31 street”) or
cuisine (“Chinese”), or the star-rating (“with the
most stars”), etc.
</bodyText>
<subsectionHeader confidence="0.999595">
6.2 Resolution Across REs
</subsectionHeader>
<bodyText confidence="0.999823692307692">
We go on to analyze the performance of differ-
ent RE types. A particularly interesting set of er-
rors we found from the previous experiments are
those that involve implicit referrals. Table 6 shows
the distribution of different REs in the collected
datasets.
Some noticeable instances with false positives
for implicit locational REs include ambiguous
cases or item referrals with one of its facets that
require further resolution including comparison to
other items, e.g., “the nearest one”. Table 7 shows
further examples. As might be expected, the lo-
cational cues are less common compared to other
</bodyText>
<table confidence="0.9812895">
Explicit Locational RE 15% 75.2% 24% 86.2%
Implicit Locational RE 3% 56.6% 2% 56.7%
</table>
<tableCaption confidence="0.709345">
Table 6: Distribution of referring expressions
(RE) in the media (large screen like tv) and places
(handheld device like phone) corpus and the FIS
accuracies per RE type.
Utterance Displayed on screen
“the most rated restaurant” ***’s next to each item
“first thomas crown affair” original release (vs. remake)
“second one over” (incomplete row/col. information)
Table 7: Display screen as user utters.
</tableCaption>
<bodyText confidence="0.999928764705882">
expressions. We also confirm that the handheld
(places domain) users implicitly refer to the items
more commonly compared to media app, and use
the contextual information about the items such as
their location, address, star-rating, etc. The mod-
els are considerably better at resolving explicit re-
ferrals (both non-spatial and spatial) compared to
implicit ones. However, for locational referrals,
the difference between the accuracy of implicit
and explicit REs is significant (75.2% vs. 56.6%
in media and 86.2% vs. 56.7% in places). Al-
though not very common, we observe negative ex-
pressions, e.g., “the one with no reviews”, which
are harder for the FIS to resolve. They require
quantifying over every other item on the screen,
namely the context features, which we leave as a
future work.
</bodyText>
<subsectionHeader confidence="0.99989">
6.3 New Domains and Device Independence
</subsectionHeader>
<bodyText confidence="0.997170230769231">
In the series of experiments below, we empirically
investigate the FIS model’s robustness to when a
new domain or device is introduced.
Robustness to New Domains: So far we
trained media domain FIS models on utterances
from all domains. To investigate how FIS models
would behave when tested on a new domain, we
train additional models by leaving out utterances
from one domain and test on the left out domain.
We used GBDT with all the feature sets. To set
up an upper bound, we also train models on each
individual domain and test on the same domain.
Table 8 shows the performance of the FIS mod-
</bodyText>
<page confidence="0.98883">
2101
</page>
<table confidence="0.9025654">
Models tested on:
Model trained on: Movies TV Music Books
All domains 96.2% 95.2% 90.3% 94.6%
All other domains 94.6% 92.4% 89.7% %
Only *this domain 96.4% 96.8% 93.4% %
</table>
<tableCaption confidence="0.914453666666667">
Table 8: Accuracy of FIS models tested on domains that
are: seen at training time (all domains), unseen at training
time (all other domains) and trained on individual domains.
</tableCaption>
<figureCaption confidence="0.99200725">
Figure 4: The accuracy (y-axis) versus the percentage (%)
of in-domain utterances used in the training dataset. The
dashed vertical line indicates an optimum threshold for the
amount of in-domain data to be added to the training data.
</figureCaption>
<bodyText confidence="0.997519782608696">
els in accuracy on each media test domain. The
first row shows the results when all domains are
used at training time (same as in Table 5). The
second row represents models where one domain
is unseen at training time. We notice that the ac-
curacy, although degraded for movies and tv do-
mains, is in general not significantly effected by
the domain variations. We setup another experi-
ment, where we incrementally add utterances from
the domain that we are testing the model on. For
instance, we incrementally add random samples
from movies training utterances on the dataset that
does not contain movies utterances and test on all
movies test data. The charts in Fig. 4 show the
% improvement in accuracy as in-domain data is
incrementally added to the training dataset. The
results are interesting in that, using as low as 10-
20% in-domain data is sufficient to build a flexi-
ble item selection model given enough utterances
from other domains with varying REs.
Robustness to a New Device: The difference
between the vocabulary and language usage ob-
served in the data collected from the two devices
</bodyText>
<subsectionHeader confidence="0.754783">
Media
</subsectionHeader>
<bodyText confidence="0.944166">
“only the new movies” ; “second one on the left”
“show me the thriller song”; “by Lewis Milestone”
“the first harry potter book”
</bodyText>
<subsectionHeader confidence="0.250821">
Places
</subsectionHeader>
<bodyText confidence="0.282553333333333">
“directions to Les Schwab tire center”
“the closest one” ;“show me a map of ...”
“get hours of Peking restaurant”; “call Mike’s burgers”
</bodyText>
<tableCaption confidence="0.991389333333333">
Table 9: Sample of utterances collected from media and
places applications illustrating the differences in language us-
age.
</tableCaption>
<table confidence="0.99811875">
Trained on Tested on Media Tested on Places
Media 93.7 % 85.9%
Places 85.9% 86.3%
Media+Places 92.7% 85.8%
</table>
<tableCaption confidence="0.981854333333333">
Table 10: Accuracy of FIS models tested on two separate
devices (large screen media, and small screen places) that are
unseen at test time.
</tableCaption>
<bodyText confidence="0.999988333333333">
is mainly due to changes in: (i) the screen design
(places on phone has one column format wheres
the media app has multi-column layout); (ii) the
domain of the data. Table 9 shows some exam-
ples. Here, we add a little bit of complexity, and
train one FIS model using the training data col-
lected on one device and test the model on a dif-
ferent one, which is unseen at training time. Table
10 shows the comparisons for media and phone
interfaces. The results are interesting. The perfor-
mance of the places domain on phone does not get
affected when the models are trained on the media
data and tested on the phone device (86.3% down
to 85.9% which is statistically insignificant). But
when the data is trained on the places and tested
on the media, we see a rather larger degradation
on the performance (93.7% down to 85.9%). This
is due to the fact that the media display screens
are much complicated compared to phone result-
ing in a larger vocabulary with more variation in
REs compared to places domain.
</bodyText>
<subsectionHeader confidence="0.938084">
6.4 Conclusion
</subsectionHeader>
<bodyText confidence="0.9999448">
We presented a framework for identifying and rec-
ognizing referring expressions in user utterances
of human-machine conversations in natural user
interfaces. We use several on-screen cues to in-
terpret whether the user is referring to on-screen
items, and if so, which item is being referred to.
We investigate the effect of different set of fea-
tures on the FIS models performance. We also
show that our model is domain and device inde-
pendent which is very beneficial when new do-
</bodyText>
<figure confidence="0.9808435625">
0 50 100
96
95
94
93
20%
% of Movies instances in training data
0 50 100 0 50 100
% of TV instances in training data % of Music instances in training data
10%
90
89
15%
96
95
94
</figure>
<page confidence="0.999463">
2102
</page>
<bodyText confidence="0.999904833333333">
mains are added to the application to cover more
scenarios or when FIS is implemented on new de-
vices. As a future work, we would like to adapt our
model for different languages and include other
features from multi modality including gesture or
geo-location.
</bodyText>
<sectionHeader confidence="0.998344" genericHeader="references">
References
</sectionHeader>
<reference confidence="0.998482626262626">
Dimitr Anastasiou and Cui Jian and Desislava
Zhaekova. 2012. Speech and gesture interaction in
an ambient assited living lab. In. Proc. of the 1st
Workshop on Speech and Multimodal Interaction in
Assitive Environments at ACL’2012.
Rajesh Balchandran, and Mark E. Epstein, and Gerasi-
mos Potamianos, and Lsadislav Seredi. 2008. A
multi-modal spoken dialog system for interactive tv.
In. Proc. of the 10th International Conference on
Multimodal Interfaces.
Christopher M. Bishop. 1995. Neural networks for
Pattern recognition.
Kurt Bollacker and Colin Evans and Praveen Paritosh
and Ttim Sturge and Jamie Taylor. 2008. Free-
base: A collaboratively created graph database for
structuring human knowledge. In. Proc. of the 2008
International Conference on Management of Data
(SIGMOD-08).
Richard A. Bolt. 1980. Put-that-there: Voice and ges-
ture at the graphics interface. Computer Graphics.
Chris Callison-Burch and Mark Dredze. 2010. Cre-
ating speech and language data with amazons me-
chanical turk. In. Proc. of NAACL.
Chih-Chung Chang and Chih-Jen Lin. 2011. LIB-
SVM: A library for support vector machines. ACM
Transactions on Intelligent Systems and Technology,
2:27:1–27:27.
Herbert H. Clark and Deanna Wilkes-Gibbs. Referring
as colloborative processes.
Jacob Cohen. 1960. A coefficient of agreement
for nominal scales. Educational and Psychological
Measurement, 20.
Robert Dale and Jette Viethen. 2009. Referring ex-
pression generation through attribute-based heuris-
tics. In Proc. of the 12th European Workshop on
Natural Language Generation (ENLG).
Jerome H. Friedman. 2001. Greedy function approx-
imation: A gradient boosting machine. Annals of
Statistics, 2001.
Kotaro Funakoshi, Mikio Nakano, Takenobu Toku-
naga, and Ryu Iida. 2012. A unified probabilis-
tic approach to referring expressions. In Proc. of
the Special Interest Group on Discourse and Dialog
(SIGDIAL).
Petra Gieselmann. 2004. Reference resolution mecha-
nisms in dialogue management. In Proc. of the 8th
Workshop on the semantics and pragmatics of dia-
logues (CATALOG).
Dave Golland, Percy Liang, and Dan Klein. 2010.
A game-theoretic approach to generating spatial de-
scriptions. In Proceedings of the 2010 Conference
on Empirical Methods in Natural Language Pro-
cessing, pages 410–419, Cambridge, MA, October.
Association for Computational Linguistics.
Joakim Gustafson, Linda Bell, Jonas Beskow, Johan
Boye, Rolf Carlson, Jens Edlund, Bjorn Granstrom,
David House, and Mats Wiren. 2000. Adapt -
a multimodal conversational dialogue system in an
apartment domain. In Proc. of the 6th International
Conference on Spoken Language Processing (IC-
SLP), pages 134–137.
Trevor Hastie, Robert Tibshirani, and Jerome H. Fried-
man. 2009. The Elements of Statistical Learning
(2nd ed.) Chapter 10. Boosting and Additive Trees,
2009.
Larry Heck, Dilek Hakkani-Tur, Madhu Chinthakunta,
Gokhan Tur, Rukmini Iyer, Partha Parthasarathy,
Lisa Stifelman, Elizabeth Shriberg, and Ashley Fi-
dler. 2013. Multi-modal conversational search and
browse. In Proc. of the IEEE Workshop on Speech,
Language and Audio in Multimedia.
Dvaid Huggins-Daines and Alexander I. Rudnicky.
2008. Interactive asr error correction for touch-
screen devices. In Proc. of ACL, Demo session.
Srinivasan Janarthanam and Oliver Lemon. 2010.
Adaptive referring expression generation in spoken
dialog systems: Evaluation with real users. In Proc.
of SIGDIAL 2010: the 11th Annual Meeting of the
Special Interest Group on Discourse and Dialogue.
Mark Johnston, Srinivas Bangalore, Gunaranjan
Vasireddy, Amanda Stent, Patrick Ehlen, Marilyn
Walker, and Steve Whittaker and Preetam Maloor.
2002. Match: an architecture for multimodal dialog
systems. In. Proc. of the ACL.
Dan Klein and Christopher D. Manning. 2003. Accu-
rate unlexicalized parsing.
David B. Koons, Carlton J. Sparrell, and Kristinn R.
Thorisson. 1993. Integrating simultaneous input
from speech, gaze and hand gestures. In Proc. of
the In Maybury, M. (Ed.), Intelligent Multimedia In-
terfaces, pages 257–276.
John Lafferty, Andrew McCallum, and Fernando C.N.
Pereira. 2001. Conditional random fields: prob-
abilistic models for segmenting and labeling se-
quence data. In Proc. ICML.
Vlademir Levenshtein. 1965. Binary codes capa-
ble of correcting deletions, insertions and rever-
sals. In Proc. of the Doklady Akademii Nauk SSSR,
163:845–848.
</reference>
<page confidence="0.703704">
2103
</page>
<reference confidence="0.996623740740741">
Teruhisa Misu, Antoine Raux, Rakesh Gupta, and Ian
Lane. 2014. Situated language understanding at 25
miles per hour. In. Proc. of the SIGDIAL - Annual
Meeting on Discourse and Dialogue.
Renato De Mori, Frederic Bechet, Dilek Hakkani-Tur,
Michael McTear, Giuseppe Riccardi, and Gokhan
Tur. 2008. Spoken language understanding: A sur-
vey. IEEE Signal Processing Magazine, 25:50–58.
Joseph G. Neal. 1991. Intelligent multimedia inter-
face technology. In Proc. of the Intelligent User In-
terfaces: In Sullivan, J., and Tyler, S. (Eds.), pages
45–68.
Sharon Oviatt, Antonella DeAngeli, and Karen Khun.
1997. Integration and synchronization of input
modes during multimodal human-computer interac-
tion. In Proc. of the Human Factors in Computing
Systems: CHI, pages 415–422.
Nobert Pfleger and Jan Alexandersson. 2006. To-
wards resolving referring expressions by implicitly
activated referents in practical dialog systems. In.
Proc. of the 10th Workshop on the Semantics and
Pragmatics of Dialog (SemDial-10).
Michael F. Schober and Herbert H. Clark. 1989. Un-
derstanding by addressees and overhearers. In Proc.
of the Cognitive Psychology, pages 211–232.
Vlademrr Vapnik. 1995. The nature of statistical
learning theory.
</reference>
<page confidence="0.998464">
2104
</page>
</variant>
</algorithm>
<algorithm name="ParsHed" version="110505">
<variant no="0" confidence="0.835582">
<title confidence="0.9978745">Resolving Referring Expressions in Conversational Dialogs Natural User Interfaces</title>
<author confidence="0.959748">Asli Celikyilmaz</author>
<author confidence="0.959748">Zhaleh Feizollahi</author>
<author confidence="0.959748">Dilek Hakkani-Tur</author>
<author confidence="0.959748">Ruhi</author>
<email confidence="0.9401035">asli@ieee.org,dilek@ieee.org,ruhi.sarikaya@microsoft.com</email>
<abstract confidence="0.999157217391304">Unlike traditional over-the-phone spoken dialog systems (SDSs), modern dialog systems tend to have visual rendering on the device screen as an additional modality to communicate the system’s response to the user. Visual display of the system’s response not only changes human behavior when interacting with devices, but also creates new research areas in SDSs. Onscreen item identification and resolution in utterances is one critical problem to achieve a natural and accurate humanmachine communication. We pose the problem as a classification task to correctly identify intended on-screen item(s) from user utterances. Using syntactic, semantic as well as context features from the display screen, our model can resolve different types of referring expressions with up to 90% accuracy. In the experiments we also show that the proposed model is robust to domain and screen layout changes.</abstract>
</variant>
</algorithm>
<algorithm name="ParsCit" version="110505">
<citationList>
<citation valid="true">
<authors>
<author>Dimitr Anastasiou</author>
<author>Cui Jian</author>
<author>Desislava Zhaekova</author>
</authors>
<title>Speech and gesture interaction in an ambient assited living lab.</title>
<date>2012</date>
<booktitle>In. Proc. of the 1st Workshop on Speech and Multimodal Interaction in Assitive Environments at ACL’2012.</booktitle>
<contexts>
<context position="3136" citStr="Anastasiou et al., 2012" startWordPosition="484" endWordPosition="487">(e.g. the last book)1. To achieve a natural and accurate human to machine conversation, it is crucial to accurately identify and resolve referring expressions in utterances. As important as interpreting referring expressions (REs) is for modern NUI designs, relatively few studies have investigated withing the SDSs. Those that do focus on the impact of the input from multimodal interfaces such as gesture for understanding (Bolt, 1980; Heck et al., 2013; Johnston et al., 2002), touch for ASR error correction (Huggins-Daines and Rudnicky, 2008), or cues from the screen (Balchandran et al., 2008; Anastasiou et al., 2012). Most of these systems are engineered for a specific 1An item could be anything from a list, e.g. restaurants, games, contact list, organized in different lay-outs on the screen. 2094 Proceedings of the 2014 Conference on Empirical Methods in Natural Language Processing (EMNLP), pages 2094–2104, October 25-29, 2014, Doha, Qatar. c�2014 Association for Computational Linguistics task, making it harder to generalize for different domains or SDSs. In this paper, we investigate a rather generic contextual model for resolving natural language REs for on-screen item selection to improve conversation</context>
</contexts>
<marker>Anastasiou, Jian, Zhaekova, 2012</marker>
<rawString>Dimitr Anastasiou and Cui Jian and Desislava Zhaekova. 2012. Speech and gesture interaction in an ambient assited living lab. In. Proc. of the 1st Workshop on Speech and Multimodal Interaction in Assitive Environments at ACL’2012.</rawString>
</citation>
<citation valid="true">
<authors>
<author>Rajesh Balchandran</author>
<author>Mark E Epstein</author>
<author>Gerasimos Potamianos</author>
<author>Lsadislav Seredi</author>
</authors>
<title>A multi-modal spoken dialog system for interactive tv.</title>
<date>2008</date>
<booktitle>In. Proc. of the 10th International Conference on Multimodal Interfaces.</booktitle>
<contexts>
<context position="3110" citStr="Balchandran et al., 2008" startWordPosition="480" endWordPosition="483">ferring to the same item, (e.g. the last book)1. To achieve a natural and accurate human to machine conversation, it is crucial to accurately identify and resolve referring expressions in utterances. As important as interpreting referring expressions (REs) is for modern NUI designs, relatively few studies have investigated withing the SDSs. Those that do focus on the impact of the input from multimodal interfaces such as gesture for understanding (Bolt, 1980; Heck et al., 2013; Johnston et al., 2002), touch for ASR error correction (Huggins-Daines and Rudnicky, 2008), or cues from the screen (Balchandran et al., 2008; Anastasiou et al., 2012). Most of these systems are engineered for a specific 1An item could be anything from a list, e.g. restaurants, games, contact list, organized in different lay-outs on the screen. 2094 Proceedings of the 2014 Conference on Empirical Methods in Natural Language Processing (EMNLP), pages 2094–2104, October 25-29, 2014, Doha, Qatar. c�2014 Association for Computational Linguistics task, making it harder to generalize for different domains or SDSs. In this paper, we investigate a rather generic contextual model for resolving natural language REs for on-screen item selecti</context>
</contexts>
<marker>Balchandran, Epstein, Potamianos, Seredi, 2008</marker>
<rawString>Rajesh Balchandran, and Mark E. Epstein, and Gerasimos Potamianos, and Lsadislav Seredi. 2008. A multi-modal spoken dialog system for interactive tv. In. Proc. of the 10th International Conference on Multimodal Interfaces.</rawString>
</citation>
<citation valid="true">
<authors>
<author>Christopher M Bishop</author>
</authors>
<date>1995</date>
<booktitle>Neural networks for Pattern recognition.</booktitle>
<contexts>
<context position="24180" citStr="Bishop, 1995" startWordPosition="4018" endWordPosition="4019">pecific intents, and a slot value match feature instead of domain specific slot types as features, our models are rather domain independent. 5 GBDT Classifier Among various classifier learning algorithms, we choose the GBDT (gradient boosted decision tree) (Friedman, 2001; Hastie et al., 2009), also known as MART (Multiple Additive Regression Trees). GBDT3 is an efficient algorithm which learns an ensemble of trees. We find the main advantage of the decision tree classifier as opposed to other non-linear classifiers such as SVM (support vector machines) (Vapnik, 1995) or NN (neural networks) (Bishop, 1995) is the interpretability. Decision trees are ”white boxes” in the sense that perfeature gain can be expressed by the magnitude of their weights, while SVM or NN’s are generally black boxes, i.e. we cannot read the acquired knowledge in a comprehensible way. Additionally, decision trees can easily accept categorical and continuous valued features. We also present the results of the SVM models. 6 Experiments We investigate several aspects of the SISI model including its robustness in resolving REs for domain or device variability. We start with the details of the data and model parameters. We co</context>
</contexts>
<marker>Bishop, 1995</marker>
<rawString>Christopher M. Bishop. 1995. Neural networks for Pattern recognition.</rawString>
</citation>
<citation valid="true">
<authors>
<author>Kurt Bollacker</author>
<author>Colin Evans</author>
<author>Praveen Paritosh</author>
<author>Ttim Sturge</author>
<author>Jamie Taylor</author>
</authors>
<title>Freebase: A collaboratively created graph database for structuring human knowledge.</title>
<date>2008</date>
<booktitle>In. Proc. of the 2008 International Conference on Management of Data (SIGMOD-08).</booktitle>
<contexts>
<context position="18701" citStr="Bollacker et al., 2008" startWordPosition="3114" endWordPosition="3117">nce the word-order similarity is S(i,k)=0.33. Word Vector: This feature is the cosine similarity between the utterance ui and the itemtitle tk that measures the cosine of the angle between them. Here, we use the unigram word counts to represent the word vectors and the word vector similarity is defined as: S(i,k)=(ri&apos;rk)/IIriII&apos; IIrkII. 4.2 Knowledge Graph Features This binary feature is used to represent overlap between utterance and the meta information about the item and is mainly aimed to resolve implicit REs. First, we obtain the meta information about the on-screen items using Freebase (Bollacker et al., 2008), the knowledge graph that contains knowledge about classes (books, movies, ...) and their attributes (title, publisher, year-released, ...). Knowledge is often represented as the attributes of the instances, along with values for those properties. Once we obtain the attribute values of the item from Freebase, we check if any attribute overlaps with part of the utterance. For instance, given an utterance “how about the one with Kevin Spacey”, and the item-title “House of Cards”, the knowledge graph attributes include year(2013), cast(Kevin Spacey), director(James Foley),... We turn the freebas</context>
</contexts>
<marker>Bollacker, Evans, Paritosh, Sturge, Taylor, 2008</marker>
<rawString>Kurt Bollacker and Colin Evans and Praveen Paritosh and Ttim Sturge and Jamie Taylor. 2008. Freebase: A collaboratively created graph database for structuring human knowledge. In. Proc. of the 2008 International Conference on Management of Data (SIGMOD-08).</rawString>
</citation>
<citation valid="true">
<authors>
<author>Richard A Bolt</author>
</authors>
<title>Put-that-there: Voice and gesture at the graphics interface. Computer Graphics.</title>
<date>1980</date>
<contexts>
<context position="2948" citStr="Bolt, 1980" startWordPosition="456" endWordPosition="457">he user may choose one or more of the on-screen items with natural language utterances as shown in Table 1. Note that, there are multiple ways of referring to the same item, (e.g. the last book)1. To achieve a natural and accurate human to machine conversation, it is crucial to accurately identify and resolve referring expressions in utterances. As important as interpreting referring expressions (REs) is for modern NUI designs, relatively few studies have investigated withing the SDSs. Those that do focus on the impact of the input from multimodal interfaces such as gesture for understanding (Bolt, 1980; Heck et al., 2013; Johnston et al., 2002), touch for ASR error correction (Huggins-Daines and Rudnicky, 2008), or cues from the screen (Balchandran et al., 2008; Anastasiou et al., 2012). Most of these systems are engineered for a specific 1An item could be anything from a list, e.g. restaurants, games, contact list, organized in different lay-outs on the screen. 2094 Proceedings of the 2014 Conference on Empirical Methods in Natural Language Processing (EMNLP), pages 2094–2104, October 25-29, 2014, Doha, Qatar. c�2014 Association for Computational Linguistics task, making it harder to gener</context>
<context position="6219" citStr="Bolt, 1980" startWordPosition="1003" endWordPosition="1004">d display screen changes. When tested on a domain that is unseen to the training data or on a device that has a different NUI design, the performance only slightly degrades proving its robustness to domain and design changes. 2 Related Work Although the problems of modern NUIs on smart devices are fairly new, RE resolution in natural language has been studied by many in NLP community. Multimodal systems provide a natural and effective way for users to interact with computers through multiple modalities such as speech, gesture, and gaze. Since the first appearance of the Put-That-There system (Bolt, 1980), a number of multimodal systems have been built, among which there are systems that combine speech, pointing (Neal, 1991), and gaze (Koons et al., 1993), systems that engage users in an intelligent conversation (Gustafson et al., 2000). Earlier studies have shown that multimodal interfaces enable users to interact with computers naturally and effectively (Schober and Clark, 1989; Oviatt et al., 1997). Considered as part of the situated interactive frameworks, many work focus on the problem of predicting how the user has resolved REs that is generated by the system, e.g., (Clark and Wilkes-Gib</context>
</contexts>
<marker>Bolt, 1980</marker>
<rawString>Richard A. Bolt. 1980. Put-that-there: Voice and gesture at the graphics interface. Computer Graphics.</rawString>
</citation>
<citation valid="true">
<authors>
<author>Chris Callison-Burch</author>
<author>Mark Dredze</author>
</authors>
<title>Creating speech and language data with amazons mechanical turk.</title>
<date>2010</date>
<booktitle>In. Proc. of NAACL.</booktitle>
<marker>Callison-Burch, Dredze, 2010</marker>
<rawString>Chris Callison-Burch and Mark Dredze. 2010. Creating speech and language data with amazons mechanical turk. In. Proc. of NAACL.</rawString>
</citation>
<citation valid="true">
<authors>
<author>Chih-Chung Chang</author>
<author>Chih-Jen Lin</author>
</authors>
<title>LIBSVM: A library for support vector machines.</title>
<date>2011</date>
<journal>ACM Transactions on Intelligent Systems and Technology,</journal>
<pages>2--27</pages>
<contexts>
<context position="26937" citStr="Chang and Lin, 2011" startWordPosition="4467" endWordPosition="4470">intent detection models in accuracy (Acc.) and slot tagging models in F-Score on the test dataset. locations) domain. We also construct additional negative instances from utterance-item pairs using first turn non-selection queries, which mainly indicate a new search or starting over. In total we compile around 250K utterance-item pairs for media domains and 150K utterance-item pairs for the places domain.4 We randomly split each collection into 60%-20%-20% parts to construct the train/dev/test datasets. We use the dev set to tune the regularization parameter for the GBDT and SVM using LIBSVM (Chang and Lin, 2011) with linear kernel. We use the training dataset to build the SLU intent and slot models for each domain. For the intent model, we use the GBDT classifier with ngram and lexicon features. The lexicon entries are obtained from Freebase and are used as indicator variables, e.g., whether the utterance contains an instance which exists in the lexicon. Similarly, we train a semantic slot tagging model using CRF method. We use n-gram features with up to fivegram window, and lexicon features similar to the intent models. Table 4 shows the accuracy and Fscore values of SLU models on the test data. The</context>
</contexts>
<marker>Chang, Lin, 2011</marker>
<rawString>Chih-Chung Chang and Chih-Jen Lin. 2011. LIBSVM: A library for support vector machines. ACM Transactions on Intelligent Systems and Technology, 2:27:1–27:27.</rawString>
</citation>
<citation valid="false">
<authors>
<author>Herbert H Clark</author>
<author>Deanna Wilkes-Gibbs</author>
</authors>
<title>Referring as colloborative processes.</title>
<marker>Clark, Wilkes-Gibbs, </marker>
<rawString>Herbert H. Clark and Deanna Wilkes-Gibbs. Referring as colloborative processes.</rawString>
</citation>
<citation valid="true">
<authors>
<author>Jacob Cohen</author>
</authors>
<title>A coefficient of agreement for nominal scales.</title>
<date>1960</date>
<journal>Educational and Psychological Measurement,</journal>
<volume>20</volume>
<contexts>
<context position="13480" citStr="Cohen, 1960" startWordPosition="2251" endWordPosition="2253">ys the items in a 3, 4, and 5-rows per 1 column layout as shown in Fig. 2. We use the same variations in prompts as in §3.1. To generate the HitApp screens, we search for nearby places, in the top search engines (Google, Bing) and collect the results to the first turn natural language search queries (e.g.,“find me sushi restaurants near me”). 3.2 Data Annotation We collect text utterances using our media and places application. Using a similar HitApp we labeled each utterance with a domain, intent and segments in utterance with slot tags (see Table 2). The annotation agreement, Kappa measure (Cohen, 1960) is around 85%. Since we are building a relational model between utterances and each item on the screen, we ask the annotators to label each utterance-item as ’0’ or ’1’ indicating if the utterance is referring to that item or not. ’1’ means the item is the intended one. ’0’ indicates the item is not intended one or the utterance is not referring to any item on the screen, e.g., new search query. We also ask the annotators to label each utterance whether they contain locational (spatial) references. Domain Intents (I) &amp; Slots movie I: find-movie/director/actor,buy-ticket Slots: name, mpaa-rati</context>
</contexts>
<marker>Cohen, 1960</marker>
<rawString>Jacob Cohen. 1960. A coefficient of agreement for nominal scales. Educational and Psychological Measurement, 20.</rawString>
</citation>
<citation valid="true">
<authors>
<author>Robert Dale</author>
<author>Jette Viethen</author>
</authors>
<title>Referring expression generation through attribute-based heuristics.</title>
<date>2009</date>
<booktitle>In Proc. of the 12th European Workshop on Natural Language Generation (ENLG).</booktitle>
<contexts>
<context position="6847" citStr="Dale and Viethen, 2009" startWordPosition="1105" endWordPosition="1108">mber of multimodal systems have been built, among which there are systems that combine speech, pointing (Neal, 1991), and gaze (Koons et al., 1993), systems that engage users in an intelligent conversation (Gustafson et al., 2000). Earlier studies have shown that multimodal interfaces enable users to interact with computers naturally and effectively (Schober and Clark, 1989; Oviatt et al., 1997). Considered as part of the situated interactive frameworks, many work focus on the problem of predicting how the user has resolved REs that is generated by the system, e.g., (Clark and Wilkes-Gibbs, ; Dale and Viethen, 2009; Gieselmann, 2004; Janarthanam and Lemon, 2010; Golland et al., 2010). In this work, focusing on smart devices, we investigate how the system resolves the REs in user utterances to take the next correct action. In (Pfleger and J.Alexandersson, 2006) a reference resolution model is presented for a questionanswering system on a mobile, multi-modal interface. Their system has several features to parse the posed question and keep history of the dialog to resolve co-reference issues. Their questionanswering model uses gesture as features to resolve queries such as “what’s the name of that [pointin</context>
</contexts>
<marker>Dale, Viethen, 2009</marker>
<rawString>Robert Dale and Jette Viethen. 2009. Referring expression generation through attribute-based heuristics. In Proc. of the 12th European Workshop on Natural Language Generation (ENLG).</rawString>
</citation>
<citation valid="true">
<authors>
<author>Jerome H Friedman</author>
</authors>
<title>Greedy function approximation: A gradient boosting machine. Annals of Statistics,</title>
<date>2001</date>
<contexts>
<context position="23839" citStr="Friedman, 2001" startWordPosition="3963" endWordPosition="3964">f any recognized slot value with either the item-title or the item meta-information from freebase. In addition, we include the longest slot value n-gram match as an additional feature. We add a binary feature per domain, indicating whether there is a slot value match. Because we are using generic intents as categorical features instead of specific intents, and a slot value match feature instead of domain specific slot types as features, our models are rather domain independent. 5 GBDT Classifier Among various classifier learning algorithms, we choose the GBDT (gradient boosted decision tree) (Friedman, 2001; Hastie et al., 2009), also known as MART (Multiple Additive Regression Trees). GBDT3 is an efficient algorithm which learns an ensemble of trees. We find the main advantage of the decision tree classifier as opposed to other non-linear classifiers such as SVM (support vector machines) (Vapnik, 1995) or NN (neural networks) (Bishop, 1995) is the interpretability. Decision trees are ”white boxes” in the sense that perfeature gain can be expressed by the magnitude of their weights, while SVM or NN’s are generally black boxes, i.e. we cannot read the acquired knowledge in a comprehensible way. A</context>
</contexts>
<marker>Friedman, 2001</marker>
<rawString>Jerome H. Friedman. 2001. Greedy function approximation: A gradient boosting machine. Annals of Statistics, 2001.</rawString>
</citation>
<citation valid="true">
<authors>
<author>Kotaro Funakoshi</author>
<author>Mikio Nakano</author>
<author>Takenobu Tokunaga</author>
<author>Ryu Iida</author>
</authors>
<title>A unified probabilistic approach to referring expressions.</title>
<date>2012</date>
<booktitle>In Proc. of the Special Interest Group on Discourse and Dialog (SIGDIAL).</booktitle>
<contexts>
<context position="7613" citStr="Funakoshi et al., 2012" startWordPosition="1233" endWordPosition="1236">resolves the REs in user utterances to take the next correct action. In (Pfleger and J.Alexandersson, 2006) a reference resolution model is presented for a questionanswering system on a mobile, multi-modal interface. Their system has several features to parse the posed question and keep history of the dialog to resolve co-reference issues. Their questionanswering model uses gesture as features to resolve queries such as “what’s the name of that [pointing gesture] player?”, but they do not resolve locational referrals such as “the middle one” or “the second harry potter movie”. Others such as (Funakoshi et al., 2012) resolve anaphoric (“it”) or exophoric (“this one”) types of expressions in user utterances to identify geometric objects. In this paper, we study several types of REs to build a natural and flexible interaction for the user. (Heck et al., 2013) present an intent prediction model enriched with gesture detector to help disambiguate between different user intents related to the interface. In (Misu et al., 2014) a situated incar dialog model is presented to answer drivers’ spoken queries about their surroundings (no display screen). They integrate multi-modal inputs of 2095 speech, geo-location a</context>
</contexts>
<marker>Funakoshi, Nakano, Tokunaga, Iida, 2012</marker>
<rawString>Kotaro Funakoshi, Mikio Nakano, Takenobu Tokunaga, and Ryu Iida. 2012. A unified probabilistic approach to referring expressions. In Proc. of the Special Interest Group on Discourse and Dialog (SIGDIAL).</rawString>
</citation>
<citation valid="true">
<authors>
<author>Petra Gieselmann</author>
</authors>
<title>Reference resolution mechanisms in dialogue management.</title>
<date>2004</date>
<booktitle>In Proc. of the 8th Workshop on the semantics and pragmatics of dialogues (CATALOG).</booktitle>
<contexts>
<context position="6865" citStr="Gieselmann, 2004" startWordPosition="1109" endWordPosition="1111">ms have been built, among which there are systems that combine speech, pointing (Neal, 1991), and gaze (Koons et al., 1993), systems that engage users in an intelligent conversation (Gustafson et al., 2000). Earlier studies have shown that multimodal interfaces enable users to interact with computers naturally and effectively (Schober and Clark, 1989; Oviatt et al., 1997). Considered as part of the situated interactive frameworks, many work focus on the problem of predicting how the user has resolved REs that is generated by the system, e.g., (Clark and Wilkes-Gibbs, ; Dale and Viethen, 2009; Gieselmann, 2004; Janarthanam and Lemon, 2010; Golland et al., 2010). In this work, focusing on smart devices, we investigate how the system resolves the REs in user utterances to take the next correct action. In (Pfleger and J.Alexandersson, 2006) a reference resolution model is presented for a questionanswering system on a mobile, multi-modal interface. Their system has several features to parse the posed question and keep history of the dialog to resolve co-reference issues. Their questionanswering model uses gesture as features to resolve queries such as “what’s the name of that [pointing gesture] player?</context>
</contexts>
<marker>Gieselmann, 2004</marker>
<rawString>Petra Gieselmann. 2004. Reference resolution mechanisms in dialogue management. In Proc. of the 8th Workshop on the semantics and pragmatics of dialogues (CATALOG).</rawString>
</citation>
<citation valid="true">
<authors>
<author>Dave Golland</author>
<author>Percy Liang</author>
<author>Dan Klein</author>
</authors>
<title>A game-theoretic approach to generating spatial descriptions.</title>
<date>2010</date>
<booktitle>In Proceedings of the 2010 Conference on Empirical Methods in Natural Language Processing,</booktitle>
<pages>410--419</pages>
<publisher>Association</publisher>
<institution>for Computational Linguistics.</institution>
<location>Cambridge, MA,</location>
<contexts>
<context position="6917" citStr="Golland et al., 2010" startWordPosition="1116" endWordPosition="1120">ms that combine speech, pointing (Neal, 1991), and gaze (Koons et al., 1993), systems that engage users in an intelligent conversation (Gustafson et al., 2000). Earlier studies have shown that multimodal interfaces enable users to interact with computers naturally and effectively (Schober and Clark, 1989; Oviatt et al., 1997). Considered as part of the situated interactive frameworks, many work focus on the problem of predicting how the user has resolved REs that is generated by the system, e.g., (Clark and Wilkes-Gibbs, ; Dale and Viethen, 2009; Gieselmann, 2004; Janarthanam and Lemon, 2010; Golland et al., 2010). In this work, focusing on smart devices, we investigate how the system resolves the REs in user utterances to take the next correct action. In (Pfleger and J.Alexandersson, 2006) a reference resolution model is presented for a questionanswering system on a mobile, multi-modal interface. Their system has several features to parse the posed question and keep history of the dialog to resolve co-reference issues. Their questionanswering model uses gesture as features to resolve queries such as “what’s the name of that [pointing gesture] player?”, but they do not resolve locational referrals such</context>
</contexts>
<marker>Golland, Liang, Klein, 2010</marker>
<rawString>Dave Golland, Percy Liang, and Dan Klein. 2010. A game-theoretic approach to generating spatial descriptions. In Proceedings of the 2010 Conference on Empirical Methods in Natural Language Processing, pages 410–419, Cambridge, MA, October. Association for Computational Linguistics.</rawString>
</citation>
<citation valid="true">
<authors>
<author>Joakim Gustafson</author>
<author>Linda Bell</author>
<author>Jonas Beskow</author>
<author>Johan Boye</author>
<author>Rolf Carlson</author>
<author>Jens Edlund</author>
<author>Bjorn Granstrom</author>
<author>David House</author>
<author>Mats Wiren</author>
</authors>
<title>Adapt -a multimodal conversational dialogue system in an apartment domain.</title>
<date>2000</date>
<booktitle>In Proc. of the 6th International Conference on Spoken Language Processing (ICSLP),</booktitle>
<pages>134--137</pages>
<contexts>
<context position="6455" citStr="Gustafson et al., 2000" startWordPosition="1040" endWordPosition="1043">. 2 Related Work Although the problems of modern NUIs on smart devices are fairly new, RE resolution in natural language has been studied by many in NLP community. Multimodal systems provide a natural and effective way for users to interact with computers through multiple modalities such as speech, gesture, and gaze. Since the first appearance of the Put-That-There system (Bolt, 1980), a number of multimodal systems have been built, among which there are systems that combine speech, pointing (Neal, 1991), and gaze (Koons et al., 1993), systems that engage users in an intelligent conversation (Gustafson et al., 2000). Earlier studies have shown that multimodal interfaces enable users to interact with computers naturally and effectively (Schober and Clark, 1989; Oviatt et al., 1997). Considered as part of the situated interactive frameworks, many work focus on the problem of predicting how the user has resolved REs that is generated by the system, e.g., (Clark and Wilkes-Gibbs, ; Dale and Viethen, 2009; Gieselmann, 2004; Janarthanam and Lemon, 2010; Golland et al., 2010). In this work, focusing on smart devices, we investigate how the system resolves the REs in user utterances to take the next correct acti</context>
</contexts>
<marker>Gustafson, Bell, Beskow, Boye, Carlson, Edlund, Granstrom, House, Wiren, 2000</marker>
<rawString>Joakim Gustafson, Linda Bell, Jonas Beskow, Johan Boye, Rolf Carlson, Jens Edlund, Bjorn Granstrom, David House, and Mats Wiren. 2000. Adapt -a multimodal conversational dialogue system in an apartment domain. In Proc. of the 6th International Conference on Spoken Language Processing (ICSLP), pages 134–137.</rawString>
</citation>
<citation valid="true">
<authors>
<author>Trevor Hastie</author>
<author>Robert Tibshirani</author>
<author>Jerome H Friedman</author>
</authors>
<date>2009</date>
<booktitle>The Elements of Statistical Learning (2nd ed.) Chapter 10. Boosting and Additive Trees,</booktitle>
<contexts>
<context position="23861" citStr="Hastie et al., 2009" startWordPosition="3965" endWordPosition="3968"> slot value with either the item-title or the item meta-information from freebase. In addition, we include the longest slot value n-gram match as an additional feature. We add a binary feature per domain, indicating whether there is a slot value match. Because we are using generic intents as categorical features instead of specific intents, and a slot value match feature instead of domain specific slot types as features, our models are rather domain independent. 5 GBDT Classifier Among various classifier learning algorithms, we choose the GBDT (gradient boosted decision tree) (Friedman, 2001; Hastie et al., 2009), also known as MART (Multiple Additive Regression Trees). GBDT3 is an efficient algorithm which learns an ensemble of trees. We find the main advantage of the decision tree classifier as opposed to other non-linear classifiers such as SVM (support vector machines) (Vapnik, 1995) or NN (neural networks) (Bishop, 1995) is the interpretability. Decision trees are ”white boxes” in the sense that perfeature gain can be expressed by the magnitude of their weights, while SVM or NN’s are generally black boxes, i.e. we cannot read the acquired knowledge in a comprehensible way. Additionally, decision </context>
</contexts>
<marker>Hastie, Tibshirani, Friedman, 2009</marker>
<rawString>Trevor Hastie, Robert Tibshirani, and Jerome H. Friedman. 2009. The Elements of Statistical Learning (2nd ed.) Chapter 10. Boosting and Additive Trees, 2009.</rawString>
</citation>
<citation valid="true">
<authors>
<author>Larry Heck</author>
</authors>
<title>Dilek Hakkani-Tur, Madhu Chinthakunta, Gokhan Tur, Rukmini Iyer, Partha Parthasarathy, Lisa Stifelman, Elizabeth Shriberg, and Ashley Fidler.</title>
<date>2013</date>
<booktitle>In Proc. of the IEEE Workshop on Speech, Language and Audio in Multimedia.</booktitle>
<marker>Heck, 2013</marker>
<rawString>Larry Heck, Dilek Hakkani-Tur, Madhu Chinthakunta, Gokhan Tur, Rukmini Iyer, Partha Parthasarathy, Lisa Stifelman, Elizabeth Shriberg, and Ashley Fidler. 2013. Multi-modal conversational search and browse. In Proc. of the IEEE Workshop on Speech, Language and Audio in Multimedia.</rawString>
</citation>
<citation valid="true">
<authors>
<author>Dvaid Huggins-Daines</author>
<author>Alexander I Rudnicky</author>
</authors>
<title>Interactive asr error correction for touchscreen devices.</title>
<date>2008</date>
<booktitle>In Proc. of ACL, Demo session.</booktitle>
<contexts>
<context position="3059" citStr="Huggins-Daines and Rudnicky, 2008" startWordPosition="471" endWordPosition="474">as shown in Table 1. Note that, there are multiple ways of referring to the same item, (e.g. the last book)1. To achieve a natural and accurate human to machine conversation, it is crucial to accurately identify and resolve referring expressions in utterances. As important as interpreting referring expressions (REs) is for modern NUI designs, relatively few studies have investigated withing the SDSs. Those that do focus on the impact of the input from multimodal interfaces such as gesture for understanding (Bolt, 1980; Heck et al., 2013; Johnston et al., 2002), touch for ASR error correction (Huggins-Daines and Rudnicky, 2008), or cues from the screen (Balchandran et al., 2008; Anastasiou et al., 2012). Most of these systems are engineered for a specific 1An item could be anything from a list, e.g. restaurants, games, contact list, organized in different lay-outs on the screen. 2094 Proceedings of the 2014 Conference on Empirical Methods in Natural Language Processing (EMNLP), pages 2094–2104, October 25-29, 2014, Doha, Qatar. c�2014 Association for Computational Linguistics task, making it harder to generalize for different domains or SDSs. In this paper, we investigate a rather generic contextual model for resolv</context>
</contexts>
<marker>Huggins-Daines, Rudnicky, 2008</marker>
<rawString>Dvaid Huggins-Daines and Alexander I. Rudnicky. 2008. Interactive asr error correction for touchscreen devices. In Proc. of ACL, Demo session.</rawString>
</citation>
<citation valid="true">
<authors>
<author>Srinivasan Janarthanam</author>
<author>Oliver Lemon</author>
</authors>
<title>Adaptive referring expression generation in spoken dialog systems: Evaluation with real users.</title>
<date>2010</date>
<booktitle>In Proc. of SIGDIAL 2010: the 11th Annual Meeting of the Special Interest Group on Discourse and Dialogue.</booktitle>
<contexts>
<context position="6894" citStr="Janarthanam and Lemon, 2010" startWordPosition="1112" endWordPosition="1115">, among which there are systems that combine speech, pointing (Neal, 1991), and gaze (Koons et al., 1993), systems that engage users in an intelligent conversation (Gustafson et al., 2000). Earlier studies have shown that multimodal interfaces enable users to interact with computers naturally and effectively (Schober and Clark, 1989; Oviatt et al., 1997). Considered as part of the situated interactive frameworks, many work focus on the problem of predicting how the user has resolved REs that is generated by the system, e.g., (Clark and Wilkes-Gibbs, ; Dale and Viethen, 2009; Gieselmann, 2004; Janarthanam and Lemon, 2010; Golland et al., 2010). In this work, focusing on smart devices, we investigate how the system resolves the REs in user utterances to take the next correct action. In (Pfleger and J.Alexandersson, 2006) a reference resolution model is presented for a questionanswering system on a mobile, multi-modal interface. Their system has several features to parse the posed question and keep history of the dialog to resolve co-reference issues. Their questionanswering model uses gesture as features to resolve queries such as “what’s the name of that [pointing gesture] player?”, but they do not resolve lo</context>
</contexts>
<marker>Janarthanam, Lemon, 2010</marker>
<rawString>Srinivasan Janarthanam and Oliver Lemon. 2010. Adaptive referring expression generation in spoken dialog systems: Evaluation with real users. In Proc. of SIGDIAL 2010: the 11th Annual Meeting of the Special Interest Group on Discourse and Dialogue.</rawString>
</citation>
<citation valid="true">
<authors>
<author>Mark Johnston</author>
<author>Srinivas Bangalore</author>
<author>Gunaranjan Vasireddy</author>
<author>Amanda Stent</author>
<author>Patrick Ehlen</author>
<author>Marilyn Walker</author>
<author>Steve Whittaker</author>
<author>Preetam Maloor</author>
</authors>
<title>Match: an architecture for multimodal dialog systems.</title>
<date>2002</date>
<booktitle>In. Proc. of the ACL.</booktitle>
<contexts>
<context position="2991" citStr="Johnston et al., 2002" startWordPosition="462" endWordPosition="465">of the on-screen items with natural language utterances as shown in Table 1. Note that, there are multiple ways of referring to the same item, (e.g. the last book)1. To achieve a natural and accurate human to machine conversation, it is crucial to accurately identify and resolve referring expressions in utterances. As important as interpreting referring expressions (REs) is for modern NUI designs, relatively few studies have investigated withing the SDSs. Those that do focus on the impact of the input from multimodal interfaces such as gesture for understanding (Bolt, 1980; Heck et al., 2013; Johnston et al., 2002), touch for ASR error correction (Huggins-Daines and Rudnicky, 2008), or cues from the screen (Balchandran et al., 2008; Anastasiou et al., 2012). Most of these systems are engineered for a specific 1An item could be anything from a list, e.g. restaurants, games, contact list, organized in different lay-outs on the screen. 2094 Proceedings of the 2014 Conference on Empirical Methods in Natural Language Processing (EMNLP), pages 2094–2104, October 25-29, 2014, Doha, Qatar. c�2014 Association for Computational Linguistics task, making it harder to generalize for different domains or SDSs. In thi</context>
</contexts>
<marker>Johnston, Bangalore, Vasireddy, Stent, Ehlen, Walker, Whittaker, Maloor, 2002</marker>
<rawString>Mark Johnston, Srinivas Bangalore, Gunaranjan Vasireddy, Amanda Stent, Patrick Ehlen, Marilyn Walker, and Steve Whittaker and Preetam Maloor. 2002. Match: an architecture for multimodal dialog systems. In. Proc. of the ACL.</rawString>
</citation>
<citation valid="true">
<authors>
<author>Dan Klein</author>
<author>Christopher D Manning</author>
</authors>
<date>2003</date>
<note>Accurate unlexicalized parsing.</note>
<contexts>
<context position="21411" citStr="Klein and Manning, 2003" startWordPosition="3549" endWordPosition="3552"> grid display. Also in “✿✿✿✿third from the ✿✿✿last”, the “third” is the column-position, and the “last” is the column-pivot, the pivotal reference of the column in a multi-column grid display. The fourth tag, row-position, is used when the specific row is explicitly referred, such as in “the Harry Potter movie in the ✿✿✿first row”. To train our CRF-based SLL model we use three types of features: the current word, window words e.g., previous-word, next-word, etc., using five-window around the current word, and syntactic features from the part-of-speech (POS) tagger using the Stanford’s parser (Klein and Manning, 2003). Row Indicator Feature: This feature sets the relationship between the n-gram in an utterance indicated by the row-position or row-pivot tag and the item’s row number on the screen. For instance, given SSL output row-pivot(’top’) and item’s location row=1, the value of the feature is set to ’1’. If no row tag is found by SLL, this feature is set to ’0’. We use regular expressions to parse the numerical indicators, e.g., ’top’=’1’. Column Indicator Feature: Similarly, this feature indicates if a phrase in utterance indicated by the column-position or column-pivot tag matches the item’s column </context>
</contexts>
<marker>Klein, Manning, 2003</marker>
<rawString>Dan Klein and Christopher D. Manning. 2003. Accurate unlexicalized parsing.</rawString>
</citation>
<citation valid="true">
<authors>
<author>David B Koons</author>
<author>Carlton J Sparrell</author>
<author>Kristinn R Thorisson</author>
</authors>
<title>Integrating simultaneous input from speech, gaze and hand gestures.</title>
<date>1993</date>
<booktitle>In Proc. of the In</booktitle>
<pages>257--276</pages>
<contexts>
<context position="6372" citStr="Koons et al., 1993" startWordPosition="1027" endWordPosition="1030">ance only slightly degrades proving its robustness to domain and design changes. 2 Related Work Although the problems of modern NUIs on smart devices are fairly new, RE resolution in natural language has been studied by many in NLP community. Multimodal systems provide a natural and effective way for users to interact with computers through multiple modalities such as speech, gesture, and gaze. Since the first appearance of the Put-That-There system (Bolt, 1980), a number of multimodal systems have been built, among which there are systems that combine speech, pointing (Neal, 1991), and gaze (Koons et al., 1993), systems that engage users in an intelligent conversation (Gustafson et al., 2000). Earlier studies have shown that multimodal interfaces enable users to interact with computers naturally and effectively (Schober and Clark, 1989; Oviatt et al., 1997). Considered as part of the situated interactive frameworks, many work focus on the problem of predicting how the user has resolved REs that is generated by the system, e.g., (Clark and Wilkes-Gibbs, ; Dale and Viethen, 2009; Gieselmann, 2004; Janarthanam and Lemon, 2010; Golland et al., 2010). In this work, focusing on smart devices, we investiga</context>
</contexts>
<marker>Koons, Sparrell, Thorisson, 1993</marker>
<rawString>David B. Koons, Carlton J. Sparrell, and Kristinn R. Thorisson. 1993. Integrating simultaneous input from speech, gaze and hand gestures. In Proc. of the In Maybury, M. (Ed.), Intelligent Multimedia Interfaces, pages 257–276.</rawString>
</citation>
<citation valid="true">
<authors>
<author>John Lafferty</author>
<author>Andrew McCallum</author>
<author>Fernando C N Pereira</author>
</authors>
<title>Conditional random fields: probabilistic models for segmenting and labeling sequence data. In</title>
<date>2001</date>
<booktitle>Proc. ICML.</booktitle>
<contexts>
<context position="20318" citStr="Lafferty et al., 2001" startWordPosition="3371" endWordPosition="3374">reebase meta feature as the synopsis includes the following passage: “... in which real ✿✿✿✿✿✿✿✿✿ messenger✿✿✿✿✿ boys are used as stunts... ”. 4.3 Semantic Location Labeler (SLL) Feature This feature set captures spatial cues in utterances and is mainly aimed to resolve explicit locational REs. Our goal is to capture the location indicating tokens in utterances and then resolve the referred location on the screen by using an indicator feature. We implement the SLL (Semantic Location Labeler), a sequence labeling model to tag locational cues in utterances using Conditional Random Fields (CRF) (Lafferty et al., 2001). We sampled a set of locational utterances from each domain to be used as training set. We asked the annotators to label tokens with four different semantic tags that indicate a location. 2098 The semantic tags include row and column indicator tags, referring to the position or pivotal reference. For instance, in “second ✿✿✿✿✿✿ from the top”, “second” is the column-position, and ✿✿✿ “top” is the row-pivot, indicating the pivotal reference of the row in a multi-row grid display. Also in “✿✿✿✿third from the ✿✿✿last”, the “third” is the column-position, and the “last” is the column-pivot, the pi</context>
</contexts>
<marker>Lafferty, McCallum, Pereira, 2001</marker>
<rawString>John Lafferty, Andrew McCallum, and Fernando C.N. Pereira. 2001. Conditional random fields: probabilistic models for segmenting and labeling sequence data. In Proc. ICML.</rawString>
</citation>
<citation valid="true">
<authors>
<author>Vlademir Levenshtein</author>
</authors>
<title>Binary codes capable of correcting deletions, insertions and reversals.</title>
<date>1965</date>
<booktitle>In Proc. of the Doklady Akademii Nauk SSSR,</booktitle>
<pages>163--845</pages>
<contexts>
<context position="17597" citStr="Levenshtein, 1965" startWordPosition="2930" endWordPosition="2931">union of unigrams. Consider, for instance, ui=“call five guys and fries” and the item tk=“five guys burgers and fries” in Fig 2. The Jaccard similarity S(i,k) is: S(i,k)=1- ( c(ri n rk)/c(ri U rk) ) where the ri and rk are unigrams of ui and tk respectively. c(ri n rk) is the number of common words of ui and tk, c(ri U rk) is the total unigram vocabulary size between them. In this case, the S(i,k)=0.66. Orthographic Distance: Orthographic distance represent similarity of two text and can be as simple as an edit distance (Levenshtein distance) between their graphemes. The Levenshtein distance (Levenshtein, 1965) counts the insertion, deletion and substitution operations that are required to transform an utterance ui into item’s title tk. Word Order: This feature represents how similar are the order of words in two text. Sentences containing the same words but in different orders may result in different meanings. We extend Jaccard similarity by defining bigram word vectors ri and rk and look for overlapping bigrams as in Table 3. Among 6 bigrams between them, only 2 are overapping, hence the word-order similarity is S(i,k)=0.33. Word Vector: This feature is the cosine similarity between the utterance </context>
</contexts>
<marker>Levenshtein, 1965</marker>
<rawString>Vlademir Levenshtein. 1965. Binary codes capable of correcting deletions, insertions and reversals. In Proc. of the Doklady Akademii Nauk SSSR, 163:845–848.</rawString>
</citation>
<citation valid="true">
<authors>
<author>Teruhisa Misu</author>
<author>Antoine Raux</author>
<author>Rakesh Gupta</author>
<author>Ian Lane</author>
</authors>
<title>Situated language understanding at 25 miles per hour.</title>
<date>2014</date>
<booktitle>In. Proc. of the SIGDIAL - Annual Meeting on Discourse and Dialogue.</booktitle>
<contexts>
<context position="8025" citStr="Misu et al., 2014" startWordPosition="1299" endWordPosition="1302">uch as “what’s the name of that [pointing gesture] player?”, but they do not resolve locational referrals such as “the middle one” or “the second harry potter movie”. Others such as (Funakoshi et al., 2012) resolve anaphoric (“it”) or exophoric (“this one”) types of expressions in user utterances to identify geometric objects. In this paper, we study several types of REs to build a natural and flexible interaction for the user. (Heck et al., 2013) present an intent prediction model enriched with gesture detector to help disambiguate between different user intents related to the interface. In (Misu et al., 2014) a situated incar dialog model is presented to answer drivers’ spoken queries about their surroundings (no display screen). They integrate multi-modal inputs of 2095 speech, geo-location and gaze. We investigate a variety of REs for visual interfaces, and analyze automatic resolution in a classification task introducing a wide range of syntactic, semantic and contextual features. We look at how REs change with screen layout comparing different devices. To the best of our knowledge, our work is first to analyze REs from these aspects. 3 Data Crowdsourcing services, such as Amazon Mechanical Tur</context>
</contexts>
<marker>Misu, Raux, Gupta, Lane, 2014</marker>
<rawString>Teruhisa Misu, Antoine Raux, Rakesh Gupta, and Ian Lane. 2014. Situated language understanding at 25 miles per hour. In. Proc. of the SIGDIAL - Annual Meeting on Discourse and Dialogue.</rawString>
</citation>
<citation valid="true">
<authors>
<author>Renato De Mori</author>
<author>Frederic Bechet</author>
<author>Dilek Hakkani-Tur</author>
<author>Michael McTear</author>
<author>Giuseppe Riccardi</author>
<author>Gokhan Tur</author>
</authors>
<title>Spoken language understanding: A survey.</title>
<date>2008</date>
<journal>IEEE Signal Processing Magazine,</journal>
<pages>25--50</pages>
<marker>De Mori, Bechet, Hakkani-Tur, McTear, Riccardi, Tur, 2008</marker>
<rawString>Renato De Mori, Frederic Bechet, Dilek Hakkani-Tur, Michael McTear, Giuseppe Riccardi, and Gokhan Tur. 2008. Spoken language understanding: A survey. IEEE Signal Processing Magazine, 25:50–58.</rawString>
</citation>
<citation valid="true">
<authors>
<author>Joseph G Neal</author>
</authors>
<title>Intelligent multimedia interface technology.</title>
<date>1991</date>
<booktitle>In Proc. of the Intelligent User Interfaces: In</booktitle>
<pages>45--68</pages>
<contexts>
<context position="6341" citStr="Neal, 1991" startWordPosition="1023" endWordPosition="1024">NUI design, the performance only slightly degrades proving its robustness to domain and design changes. 2 Related Work Although the problems of modern NUIs on smart devices are fairly new, RE resolution in natural language has been studied by many in NLP community. Multimodal systems provide a natural and effective way for users to interact with computers through multiple modalities such as speech, gesture, and gaze. Since the first appearance of the Put-That-There system (Bolt, 1980), a number of multimodal systems have been built, among which there are systems that combine speech, pointing (Neal, 1991), and gaze (Koons et al., 1993), systems that engage users in an intelligent conversation (Gustafson et al., 2000). Earlier studies have shown that multimodal interfaces enable users to interact with computers naturally and effectively (Schober and Clark, 1989; Oviatt et al., 1997). Considered as part of the situated interactive frameworks, many work focus on the problem of predicting how the user has resolved REs that is generated by the system, e.g., (Clark and Wilkes-Gibbs, ; Dale and Viethen, 2009; Gieselmann, 2004; Janarthanam and Lemon, 2010; Golland et al., 2010). In this work, focusing</context>
</contexts>
<marker>Neal, 1991</marker>
<rawString>Joseph G. Neal. 1991. Intelligent multimedia interface technology. In Proc. of the Intelligent User Interfaces: In Sullivan, J., and Tyler, S. (Eds.), pages 45–68.</rawString>
</citation>
<citation valid="true">
<authors>
<author>Sharon Oviatt</author>
<author>Antonella DeAngeli</author>
<author>Karen Khun</author>
</authors>
<title>Integration and synchronization of input modes during multimodal human-computer interaction.</title>
<date>1997</date>
<booktitle>In Proc. of the Human Factors in Computing Systems: CHI,</booktitle>
<pages>415--422</pages>
<contexts>
<context position="6623" citStr="Oviatt et al., 1997" startWordPosition="1066" endWordPosition="1069">dal systems provide a natural and effective way for users to interact with computers through multiple modalities such as speech, gesture, and gaze. Since the first appearance of the Put-That-There system (Bolt, 1980), a number of multimodal systems have been built, among which there are systems that combine speech, pointing (Neal, 1991), and gaze (Koons et al., 1993), systems that engage users in an intelligent conversation (Gustafson et al., 2000). Earlier studies have shown that multimodal interfaces enable users to interact with computers naturally and effectively (Schober and Clark, 1989; Oviatt et al., 1997). Considered as part of the situated interactive frameworks, many work focus on the problem of predicting how the user has resolved REs that is generated by the system, e.g., (Clark and Wilkes-Gibbs, ; Dale and Viethen, 2009; Gieselmann, 2004; Janarthanam and Lemon, 2010; Golland et al., 2010). In this work, focusing on smart devices, we investigate how the system resolves the REs in user utterances to take the next correct action. In (Pfleger and J.Alexandersson, 2006) a reference resolution model is presented for a questionanswering system on a mobile, multi-modal interface. Their system has</context>
</contexts>
<marker>Oviatt, DeAngeli, Khun, 1997</marker>
<rawString>Sharon Oviatt, Antonella DeAngeli, and Karen Khun. 1997. Integration and synchronization of input modes during multimodal human-computer interaction. In Proc. of the Human Factors in Computing Systems: CHI, pages 415–422.</rawString>
</citation>
<citation valid="true">
<authors>
<author>Nobert Pfleger</author>
<author>Jan Alexandersson</author>
</authors>
<title>Towards resolving referring expressions by implicitly activated referents in practical dialog systems.</title>
<date>2006</date>
<booktitle>In. Proc. of the 10th Workshop on the Semantics and Pragmatics of Dialog (SemDial-10).</booktitle>
<marker>Pfleger, Alexandersson, 2006</marker>
<rawString>Nobert Pfleger and Jan Alexandersson. 2006. Towards resolving referring expressions by implicitly activated referents in practical dialog systems. In. Proc. of the 10th Workshop on the Semantics and Pragmatics of Dialog (SemDial-10).</rawString>
</citation>
<citation valid="true">
<authors>
<author>Michael F Schober</author>
<author>Herbert H Clark</author>
</authors>
<title>Understanding by addressees and overhearers.</title>
<date>1989</date>
<booktitle>In Proc. of the Cognitive Psychology,</booktitle>
<pages>211--232</pages>
<contexts>
<context position="6601" citStr="Schober and Clark, 1989" startWordPosition="1062" endWordPosition="1065">in NLP community. Multimodal systems provide a natural and effective way for users to interact with computers through multiple modalities such as speech, gesture, and gaze. Since the first appearance of the Put-That-There system (Bolt, 1980), a number of multimodal systems have been built, among which there are systems that combine speech, pointing (Neal, 1991), and gaze (Koons et al., 1993), systems that engage users in an intelligent conversation (Gustafson et al., 2000). Earlier studies have shown that multimodal interfaces enable users to interact with computers naturally and effectively (Schober and Clark, 1989; Oviatt et al., 1997). Considered as part of the situated interactive frameworks, many work focus on the problem of predicting how the user has resolved REs that is generated by the system, e.g., (Clark and Wilkes-Gibbs, ; Dale and Viethen, 2009; Gieselmann, 2004; Janarthanam and Lemon, 2010; Golland et al., 2010). In this work, focusing on smart devices, we investigate how the system resolves the REs in user utterances to take the next correct action. In (Pfleger and J.Alexandersson, 2006) a reference resolution model is presented for a questionanswering system on a mobile, multi-modal inter</context>
</contexts>
<marker>Schober, Clark, 1989</marker>
<rawString>Michael F. Schober and Herbert H. Clark. 1989. Understanding by addressees and overhearers. In Proc. of the Cognitive Psychology, pages 211–232.</rawString>
</citation>
<citation valid="true">
<authors>
<author>Vlademrr Vapnik</author>
</authors>
<title>The nature of statistical learning theory.</title>
<date>1995</date>
<contexts>
<context position="24141" citStr="Vapnik, 1995" startWordPosition="4011" endWordPosition="4012">ts as categorical features instead of specific intents, and a slot value match feature instead of domain specific slot types as features, our models are rather domain independent. 5 GBDT Classifier Among various classifier learning algorithms, we choose the GBDT (gradient boosted decision tree) (Friedman, 2001; Hastie et al., 2009), also known as MART (Multiple Additive Regression Trees). GBDT3 is an efficient algorithm which learns an ensemble of trees. We find the main advantage of the decision tree classifier as opposed to other non-linear classifiers such as SVM (support vector machines) (Vapnik, 1995) or NN (neural networks) (Bishop, 1995) is the interpretability. Decision trees are ”white boxes” in the sense that perfeature gain can be expressed by the magnitude of their weights, while SVM or NN’s are generally black boxes, i.e. we cannot read the acquired knowledge in a comprehensible way. Additionally, decision trees can easily accept categorical and continuous valued features. We also present the results of the SVM models. 6 Experiments We investigate several aspects of the SISI model including its robustness in resolving REs for domain or device variability. We start with the details </context>
</contexts>
<marker>Vapnik, 1995</marker>
<rawString>Vlademrr Vapnik. 1995. The nature of statistical learning theory.</rawString>
</citation>
</citationList>
</algorithm>
</algorithms>
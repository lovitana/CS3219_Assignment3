<?xml version="1.0" encoding="UTF-8"?>
<algorithms version="110505">
<algorithm name="SectLabel" version="110505">
<variant no="0" confidence="0.000011">
<title confidence="0.991166">
Recall Error Analysis for Coreference Resolution
</title>
<author confidence="0.976522">
Sebastian Martschat and Michael Strube
</author>
<affiliation confidence="0.807342">
Heidelberg Institute for Theoretical Studies gGmbH
</affiliation>
<address confidence="0.608222">
Schloss-Wolfsbrunnenweg 35, 69118 Heidelberg, Germany
</address>
<email confidence="0.641086">
(sebastian.martschat|michael.strube)@h-its.org
</email>
<sectionHeader confidence="0.979309" genericHeader="abstract">
Abstract
</sectionHeader>
<bodyText confidence="0.999989090909091">
We present a novel method for coreference
resolution error analysis which we apply
to perform a recall error analysis of four
state-of-the-art English coreference reso-
lution systems. Our analysis highlights
differences between the systems and iden-
tifies that the majority of recall errors for
nouns and names are shared by all sys-
tems. We characterize this set of com-
mon challenging errors in terms of a broad
range of lexical and semantic properties.
</bodyText>
<sectionHeader confidence="0.998796" genericHeader="introduction">
1 Introduction
</sectionHeader>
<bodyText confidence="0.999923708333334">
Coreference resolution is the task of determining
which mentions in a text refer to the same entity.
State-of-the-art approaches include both learning-
based (Fernandes et al., 2012; Bj¨orkelund and
Farkas, 2012; Durrett and Klein, 2013) and de-
terministic models (Lee et al., 2013; Martschat,
2013). These approaches achieve state-of-the-art
performance mainly relying on morphosyntactic
and lexical factors. However, consider the follow-
ing example.
In order to improving the added value
of oil products, the second phase project
of the Qinghai Petroleum Bureau’s
Ge’ermu oil refinery has been put into
production. This will further improve
the factory’s oil products structure.
Due to the lack of any string overlap, most
state-of-the-art systems will miss the link between
the factory and the Qinghai Petroleum Bureau’s
Ge’ermu oil refinery. The information that factory
is a hypernym of refinery, however, may be useful
to resolve such links.
The aim of this paper is to quantify and char-
acterize such recall errors made by state-of-the-
art coreference resolution systems. By doing so,
we provide a solid foundation for work on em-
ploying knowledge sources for improving recall
for coreference resolution (Ponzetto and Strube,
2006; Rahman and Ng, 2011; Ratinov and Roth,
2012; Bansal and Klein, 2012, inter alia). In par-
ticular, we make the following contributions:
We present a novel framework for coreference
resolution error analysis. This yields a formal
foundation for previous work on link-based error
analysis (Uryupina, 2008; Martschat, 2013) and
complements work on transformation-based error
analysis (Kummerfeld and Klein, 2013).
We apply the method proposed in this paper to
perform a recall error analysis of four state-of-
the-art systems, encompassing deterministic and
learning-based approaches. In particular, we iden-
tify and characterize a set of challenging errors
common to all systems, and discuss strengths and
weaknesses of each system regarding specific er-
ror types. We also present a brief precision error
analysis.
A toolkit which implements the framework pro-
posed in this paper is available for download.1
</bodyText>
<sectionHeader confidence="0.981803" genericHeader="method">
2 A Link-Based Analysis Framework
</sectionHeader>
<bodyText confidence="0.999849">
In this section we discuss challenges in corefer-
ence resolution error analysis and devise an error
analysis framework to overcome these challenges.
</bodyText>
<subsectionHeader confidence="0.95136">
2.1 Motivation
</subsectionHeader>
<bodyText confidence="0.999388444444444">
Suppose a document contains the entity BARACK
OBAMA, which is referenced by four mentions
in the following order: Obama, he, the president
and his. A typical output of a current system not
equipped with world knowledge will consist of
two entities: {Obama, he} and {the president, his}
Obviously, the system made a recall error. But,
due to the complex nature of the coreference reso-
lution task, it is not clear how to represent the re-
</bodyText>
<footnote confidence="0.991463">
1http://smartschat.de/software
</footnote>
<page confidence="0.846398">
2070
</page>
<note confidence="0.981136">
Proceedings of the 2014 Conference on Empirical Methods in Natural Language Processing (EMNLP), pages 2070–2081,
October 25-29, 2014, Doha, Qatar. c�2014 Association for Computational Linguistics
</note>
<figure confidence="0.5515">
the president
</figure>
<figureCaption confidence="0.9941495">
Figure 1: (a) a reference entity r, represented as a complete one-directional graph, (b) a set S of three
system entities, (c) the partition rS, (d) a spanning tree for r.
</figureCaption>
<figure confidence="0.98266096">
(c)
(d)
M1
M1
M4
M2
M4
M2
M3
M3
his
he
(a)
Obama
M1
M4
M2
M3
(b)
M1
M4
M2
n3
M3
n1 n2
</figure>
<bodyText confidence="0.97252875">
call error: is it missing the link between the pres-
ident and Obama? Can the error be attributed to
deficiencies in pronoun resolution?
Linguistically motivated error representations
would facilitate both understanding of current
challenges and make system development faster
and easier. The aim of this section is to devise
such representations.
</bodyText>
<subsectionHeader confidence="0.996099">
2.2 Formalizing Coreference Resolution
</subsectionHeader>
<bodyText confidence="0.999993">
To start with, we give a formal description of the
coreference resolution task following the termi-
nology used for the ACE (Mitchell et al., 2004)
and OntoNotes (Weischedel et al., 2013) projects.
A mention is a linguistic realization of a reference
to an entity. Two mentions corefer if they refer to
the same entity. Hence, coreference is reflexive,
symmetric and transitive, and therefore an equiva-
lence relation. The task of coreference resolution
is to predict equivalence classes of mentions in a
document according to the coreference relation.
In order to extract errors, we need to compare
the reference equivalence classes, given by the
annotation, with the system equivalence classes
obtained from system output. The key question
now is how we represent these equivalence classes
of mentions. Adapting common terminology, we
also refer to the equivalence classes as entities.
</bodyText>
<subsectionHeader confidence="0.997516">
2.3 Representing Entities
</subsectionHeader>
<bodyText confidence="0.999986459459459">
The most straightforward entity representation ig-
nores any structure and models an entity as a set
of mentions. This representation was utilized for
error analysis by Kummerfeld and Klein (2013),
who extract errors by transforming reference into
system entities. In this set-based representation,
we can only extract whether two mentions corefer
at all. More fine-grained information, for example
about antecedent information, is not accessible.
We therefore propose to employ a structured en-
tity representation, which explicitly models links
established by the coreference relation between
mentions. This leads to a link-based error repre-
sentation which formalizes the methods presented
in Uryupina (2008) and Martschat (2013).
We employ for representation a complete one-
directional graph. That is, we represent an en-
tity e over mentions {m1, ... , mn} as a graph
e = (N, A), where N = {m1,... , mn} and
A = {(mk, mj)  |k &gt; j}. The indices respect
the mention ordering. Mentions earlier in the text
have a lower index. An example graph for an en-
tity over four mentions m1, ... , m4 (such as the
BARACK OBAMA entity) is depicted in Figure 1a.
In this graph, we express all coreference relations
between all pairs of mentions.2
Using this representation, we can represent a set
of entities as a set of graphs. In particular, given a
document we consider the set of reference entities
R given by the annotation, and the set of system
entities S, given by the system output. In order to
extract errors, we compare the graphs in R with
the graphs in S.
In the following, we discuss how to compute re-
call errors for a reference entity r E R with respect
to the system entities S. For computing precision
errors, we just switch the roles of R and S.
</bodyText>
<subsectionHeader confidence="0.9982335">
2.4 Comparing Reference and System
Entities
</subsectionHeader>
<bodyText confidence="0.99983775">
As we represent entities as sets of links between
mentions, errors can be quantified as differences in
the links. For example, if an edge (representing a
link) from some reference entity r E R is missing
</bodyText>
<footnote confidence="0.99814625">
2We could also use an undirected instead of a one-
directional graph, but using a one-directional graph conve-
niently models sequential information, which simplifies no-
tation and the algorithms we will present.
</footnote>
<page confidence="0.996409">
2071
</page>
<bodyText confidence="0.9997055">
in all system entities in S, this is a recall error.
In order to formalize this, we employ the notion
of a partition of an entity. Let r E R be some ref-
erence entity, and let S be a set of system entities.
The partition of r by S, written rS, is obtained
by taking all edges in r that also appear in S. rS
consists of all connected components of r (we will
refer to these as subentities) that are also in S. All
edges in r that are not in rS are candidates for re-
call errors, as these were not in any entity in S.
Figure 1b shows a set S of three system entities:
two consist of two mentions, one of three men-
tions. In our running example, this corresponds
to the system output {Obama, he} and {the pres-
ident, his} plus some spurious mentions, which
are colored gray. The graph rS for our example
is shown in Figure 1c. The two edges correspond
to the correctly recognized links (he, Obama) and
(his, the president). All edges in r (Figure 1a)
missing from this graph are candidates for errors.
</bodyText>
<subsectionHeader confidence="0.998397">
2.5 Spanning Trees
</subsectionHeader>
<bodyText confidence="0.999978739130435">
However, taking all edges in r missing in rS as er-
rors leads to unintuitive results. In the BARACK
OBAMA example, this would lead to four errors
being extracted: (the president, Obama), (his,
Obama), (the president, he) and (his, he). But,
in order to correctly predict the BARACK OBAMA
entity, a coreference resolution system only needs
to predict three correct links, i.e. it has to provide a
spanning tree of the entity’s graph representation.
Therefore, to extract errors, we compute a span-
ning tree Tr of r, and take all edges in Tr that do
not appear in rS as errors. Figure 1d shows an ex-
ample spanning tree for the running example en-
tity r. The dashed edge, which corresponds to the
link (the president, Obama), does not appear in rS
and is therefore extracted as an error.
The strategies for computing a spanning tree
may differ for recall and precision errors. Hence,
our extraction algorithm is parametrized by two
procedures STrec(e, P) and STprec(e, P) which,
given an entity e and a set of entities P, output
a spanning tree Te of e. The whole algorithm for
error extraction is summarized in Algorithm 1.
</bodyText>
<sectionHeader confidence="0.935815" genericHeader="method">
3 Spanning Tree Algorithms
</sectionHeader>
<bodyText confidence="0.93827275">
In the last section we presented a framework for
link-based error analysis, which extracts errors by
comparing entity spanning trees to entity parti-
tions. Therefore we can accommodate different
Algorithm 1 Error Extraction from a Corpus
Input: A corpus C, algorithms STrec, STprec for
computing spanning trees.
function ERRORS(C, STrec, STprec)
</bodyText>
<equation confidence="0.702153833333333">
recall errors = [ ]
precision errors = [ ]
ford E C do
Let Rd be the reference entities and Sd
be the system entities of document d.
for r E Rd do
</equation>
<bodyText confidence="0.8991895">
Add all edges in STrec(r, Sd) not in
rSd to recall errors.
</bodyText>
<subsectionHeader confidence="0.683543">
for s E Sd do
</subsectionHeader>
<bodyText confidence="0.981785857142857">
Add all edges in STprec(s, Rd) not in
sRd to precision errors.
Output: recall errors, precision errors
notions of errors by varying the algorithm for com-
puting spanning trees. We now present some span-
ning tree algorithms for extracting recall and pre-
cision errors.
</bodyText>
<subsectionHeader confidence="0.999168">
3.1 Recall Errors
</subsectionHeader>
<bodyText confidence="0.999491777777778">
We first observe that for error extraction, the struc-
ture of the spanning trees of the subentities appear-
ing in rS does not play a role. Edges present in rS
are not candidates for errors, since they appear in
both the reference entity r and the system output
S. Therefore, it does not matter which edges from
the subentities are in the spanning tree.
Hence, to build the spanning tree, we first
choose arbitrary spanning trees for the subentities
in the partition. We choose the remaining edges
according to the spanning tree algorithm.
Having settled on this, we only have to decide
which edges to choose that connect the trees rep-
resenting the subentities. There are many possible
choices for this. For example, the graph in Fig-
ure 1c has four candidate edges which connect the
trees for the subentities.
We can reduce the number of candidate edges
by only considering the first mention (with respect
to textual order) in a subentity as the source of
an edge to be added. This makes sense since all
other mentions in that subentity were correctly re-
solved to be coreferent with some preceding men-
tion. We still have to decide on the target of the
edge. In Figure 1c, we have two choices for edges:
(m3, m1) and (m3, m2). We now present two
methods for choosing edges.
</bodyText>
<page confidence="0.965999">
2072
</page>
<bodyText confidence="0.999881827586207">
Choosing Edges by Distance. The most
straight-forward way to decide on an edge is to
take the edge with smallest mention distance
between source and target. This is the approach
taken by Martschat (2013).
Choosing Edges by Accessibility. However, the
distance-based approach may lead to unintuitive
results. Let us consider again the BARACK
OBAMA example from Figure 1. When choosing
edges by distance, we would extract the error (the
president, he). However, such links with a non-
pronominal anaphor and a pronominal antecedent
are difficult to process and considered unreliable
(Ng and Cardie, 2002; Bengtson and Roth, 2008).
On the other hand, the missed link (the president,
Obama) constitutes a well-defined hyponymy re-
lation which can be found in knowledge bases and
is easily interpretable by humans.
Uryupina (Uryupina, 2007; Uryupina, 2008)
presents a recall error analysis where she takes
the “intuitively easiest” missing link to analyze
(Uryupina, 2007, p. 196). How can we formal-
ize such an intuition? We will employ a no-
tion grounded in accessibility theory (Ariel, 1988).
Names and nouns refer to less accessible entities
than pronouns do. For such anaphors, we prefer
descriptive (name/nominal) antecedents. Inspired
by Ariel’s degrees of accessibility, we choose a
target for a given anaphor mi as follows:
</bodyText>
<listItem confidence="0.898295142857143">
• If mi is a pronoun, choose the closest preced-
ing mention.
• If mi is not a pronoun, choose the closest
preceding proper name. If no such mention
exists, choose the closest preceding common
noun. If no such mention exists, choose the
closest preceding mention.
</listItem>
<bodyText confidence="0.9962635">
Applied to the example from Figure 1, this algo-
rithm extracts the error (the president, Obama).3
</bodyText>
<subsectionHeader confidence="0.999706">
3.2 Precision Errors
</subsectionHeader>
<bodyText confidence="0.999829">
Virtually all approaches to coreference resolu-
tion obtain entities by outputting pairs of anaphor
and antecedent, subject to the constraint that one
anaphor has at most one antecedent.
We use this information to build spanning trees
for system entities: these spanning trees con-
sist of exactly the edges which correspond to
anaphor/antecedent pairs in the system output.
</bodyText>
<footnote confidence="0.481139333333333">
3A similar procedure was used by Ng and Cardie (2002)
to extract meaningful antecedents when training a corefer-
ence resolution system.
</footnote>
<sectionHeader confidence="0.771473" genericHeader="method">
4 Data and Systems
</sectionHeader>
<bodyText confidence="0.9999815">
We now discuss data and coreference resolution
systems which we will employ for our analysis.
</bodyText>
<subsectionHeader confidence="0.989629">
4.1 Data
</subsectionHeader>
<bodyText confidence="0.999979857142857">
We analyze the errors of the systems on the En-
glish development data of the CoNLL’12 shared
task on multilingual coreference resolution (Prad-
han et al., 2012). This corpus contains 343 docu-
ments, spanning seven genres: bible texts, broad-
cast conversation, broadcast news, magazine texts,
news wire, telephone conversations and web logs.
</bodyText>
<subsectionHeader confidence="0.985428">
4.2 Systems
</subsectionHeader>
<bodyText confidence="0.999908125">
State-of-the-art approaches to coreference resolu-
tion encompass various paradigms, ranging from
deterministic pairwise systems to learning-based
structured prediction models. Hence, we want to
conduct our analysis on a representative sample of
the state of the art, which should be publicly avail-
able. Therefore, we decided on two deterministic
and two learning-based systems:
</bodyText>
<listItem confidence="0.981590772727273">
• StanfordSieve4 (Lee et al., 2013) was the
winning system of the CoNLL’11 shared
task. It employs a multi-sieve approach by
making more confident decisions first.
• Multigraph5 (Martschat, 2013) is a deter-
ministic pairwise system which is based on
Martschat et al. (2012), the second-ranking
system in the English track of the CoNLL’12
shared task. It uses a subset of features as
hard constraints and chooses an antecedent
for a mention by summing up the remaining
boolean features.
• IMSCoref6 (Bj¨orkelund and Farkas, 2012)
ranked second overall in the CoNLL’12
shared task (third for English). It stacks mul-
tiple decoders and relies on a combination of
standard pairwise and lexicalized features.
• BerkeleyCoref7 (Durrett and Klein, 2013) is
a state-of-the-art system that uses mainly lex-
icalized features and a latent antecedent rank-
ing architecture. It outperforms Stanford-
Sieve and IMSCoref on the CoNLL’11 data.
</listItem>
<footnote confidence="0.985216333333333">
4Part of Stanford CoreNLP, available at http://nlp.
stanford.edu/software/corenlp.shtml. We
use version 3.4.
5http://smartschat.de/software
6http://www.ims.uni-stuttgart.de/
forschung/ressourcen/werkzeuge/IMSCoref.
en.html . We use the CoNLL 2012 system.
7http://nlp.cs.berkeley.edu/
berkeleycoref.shtml
</footnote>
<page confidence="0.914234">
2073
</page>
<table confidence="0.999299714285714">
System MUC B3 CEAF, Average
Fernandes et al. 69.46 57.83 54.43 60.57
Martschat 66.22 55.47 51.90 57.86
StanfordSieve 64.96 54.49 51.24 56.90
Multigraph 69.13 58.61 56.06 61.28
IMSCoref 67.15 55.19 50.94 57.76
BerkeleyCoref 70.27 59.29 56.11 61.89
</table>
<tableCaption confidence="0.999022">
Table 1: Comparison of the systems with Fernan-
</tableCaption>
<bodyText confidence="0.967482551724138">
des et al. (2012) and with Martschat (2013) on
CoNLL’12 English development data.
For Multigraph, we modified the system described
in Martschat (2013) slightly to allow for the in-
corporation of distance (similar to Cai and Strube
(2010)). Inspired by Lappin and Leass (1994), we
add salience weights for subjects and objects to
the model to improve third-person pronoun reso-
lution. We also extended the feature set by a sub-
string feature. Furthermore, motivated by Chen
and Ng (2012), we added a lexicalized feature for
non-pronominal mentions that were coreferent in
at least 50% of the cases in the training data.
StanfordSieve was run with its standard CoNLL
shared task settings. The learning-based sys-
tems were trained on the CoNLL’12 training data.
We trained IMSCoref with its standard settings,
and trained BerkeleyCoref with the final feature
set from Durrett and Klein (2013) for twenty it-
erations. We evaluate the systems on English
CoNLL’12 development data and compare it with
the winning system of the CoNLL’12 shared task
(Fernandes et al., 2012) and with Martschat (2013)
in Table 1, using the reference implementation v7
of the CoNLL scorer (Pradhan et al., 2014).
BerkeleyCoref performs best according to all
metrics, followed by Multigraph. StanfordSieve
is the worst performing system: the gap to Berke-
leyCoref is five points in average score.
</bodyText>
<subsectionHeader confidence="0.973621">
4.3 Discussion
</subsectionHeader>
<bodyText confidence="0.999968777777778">
Although we analyze recent systems on a recently
published coreference data set, we believe that the
results of our analysis will have implications for
coreference in general. The data set is the largest
and most genre-diverse coreference corpus so far.
The systems we investigate represent major di-
rections in coreference resolution model research,
and make use of large and diverse feature sets pro-
posed in the literature (Ng, 2010).
</bodyText>
<sectionHeader confidence="0.987536" genericHeader="method">
5 A Comparative Analysis
</sectionHeader>
<bodyText confidence="0.999986">
The coreference resolution systems presented in
the previous section are a representative sample of
the state of the art. Therefore, by analyzing the
errors they make, we can learn about remaining
challenges in coreference resolution and analyze
the qualitative differences between the systems.
The results of such an analysis will deepen our
understanding of coreference resolution and will
suggest promising directions for further research.
</bodyText>
<subsectionHeader confidence="0.971064">
5.1 Experimental Settings
</subsectionHeader>
<bodyText confidence="0.999884777777778">
Previous studies identified the presence of recall
errors as a main bottleneck for improving per-
formance (Raghunathan et al., 2010; Durrett and
Klein, 2013; Kummerfeld and Klein, 2013). This
is also evidenced by the CoNLL shared tasks on
coreference resolution (Pradhan et al., 2011; Prad-
han et al., 2012), where most competitive systems
had higher precision than recall. This indicates
that an analysis of recall errors helps to understand
and improve the state of the art. Hence, we focus
on analyzing recall errors, and complement this by
a brief analysis of precision errors.
We analyze errors of the four systems presented
in the previous section on the CoNLL’12 English
development data. To extract recall errors we em-
ploy the spanning tree algorithm which chooses
edges by accessibility. We obtain precision errors
from the pairwise output of the systems.
</bodyText>
<subsectionHeader confidence="0.997433">
5.2 A Recall Error Analysis of StanfordSieve
</subsectionHeader>
<bodyText confidence="0.99988775">
Since StanfordSieve is currently the most-widely
used coreference resolution system, it serves as a
good starting point for our analysis. Remember
that we represent each error as a pair of anaphor
and antecedent. For an initial analysis, we cate-
gorize each error by mention type, distinguishing
between proper name, common noun, pronoun,
demonstrative pronoun and verb.8
StanfordSieve makes 5245 recall errors. To put
this number into context, we compare it with the
maximum number of recall errors a system can
make. This count is obtained by extracting recall
errors from the output of a system that puts each
mention in its own entity, which yields 14609 er-
rors. In Table 2 we present a detailed analysis.
For each pair of mention type of anaphor and an-
</bodyText>
<footnote confidence="0.905164333333333">
8We obtain the type from the part-of-speech tag of the
mention’s head. Furthermore, we treat every mention whose
head has a NER label in the data as a proper name.
</footnote>
<page confidence="0.921475">
2074
</page>
<table confidence="0.9999520625">
Name Noun Pron. Dem. Verb
Name
Errors 1006 181 43 0 0
Maximum 3578 206 56 2 0
Noun
Errors 517 1127 46 14 91
Maximum 742 2063 51 14 91
Pron.
Errors 483 761 543 45 53
Maximum 1166 1535 4596 92 53
Dem.
Errors 23 86 41 31 117
Maximum 27 93 43 46 117
Verb
Errors 1 20 2 4 10
Maximum 1 20 2 4 11
</table>
<tableCaption confidence="0.99107">
Table 2: Number of StanfordSieve’s recall er-
</tableCaption>
<bodyText confidence="0.93324947826087">
rors according to mention type, compared to the
maximum possible number of errors. Rows are
anaphors, columns antecedents.
tecedent, the table displays the number of recall
errors and of maximum errors possible.
StanfordSieve gets almost none of the links in-
volving verbal or demonstrative mentions correct.
This is due to the system not attempting to handle
event coreference, and performing very poorly for
demonstratives. On the other hand, recall for pro-
noun resolution is quite good, at least when con-
sidering non-verbal antecedents. While Stanford-
Sieve makes 1885 recall errors when the anaphor
is a pronoun, it successfully resolves most of such
links present in the corpus. Finally, let us consider
the links involving only proper names and com-
mon nouns. In total, these amount to 6589 links
in the corpus (around 45% of all links). Stanford-
Sieve misses 2831 of these links. Pairs of proper
names seem to be easier to resolve than pairs of
common nouns. Links between a common noun
and a proper name are less frequent, but much
more difficult: most of the links are missing.
</bodyText>
<subsectionHeader confidence="0.997988">
5.3 Analysis of the Other Systems
</subsectionHeader>
<bodyText confidence="0.9998785">
In the previous section we identified various char-
acteristics of the errors made by StanfordSieve:
only (comparatively) few errors are made for pro-
noun resolution and name coreference, while other
types of nominal anaphora and coreference of
demonstrative/verbal mentions pose a challenge
for the system. Do the other systems in our study
also have these characteristics? In order to answer
</bodyText>
<table confidence="0.999805166666667">
Proportion
System Total Anaphor Pron. Name/Noun
StanfordSieve 5245 36% 54%
Multigraph 4630 32% 56%
IMSCoref 5220 32% 58%
BerkeleyCoref 4635 32% 56%
</table>
<tableCaption confidence="0.999809">
Table 3: Recall error numbers for all systems.
</tableCaption>
<bodyText confidence="0.999959791666667">
this question, we repeated the analysis for the three
other systems described in Section 4. We summa-
rize the results in Table 3. We only report num-
bers for pronoun resolution and name/noun coref-
erence, as all systems do not resolve verbal men-
tions and perform poorly for demonstratives.
StanfordSieve makes the most recall errors,
closely followed by IMSCoref. Multigraph
and BerkeleyCoref make around 600 errors less.
While the total number of errors differs between
the systems, the distributions are similar. In par-
ticular, around 55% of recall errors made involve
only proper names and common nouns. The num-
ber is a bit higher for IMSCoref. We conclude
that, despite variations in performance, both de-
terministic and learning-based state-of-the-art sys-
tems have similar weaknesses regarding recall.
The results displayed in Table 3 suggest vari-
ous opportunities for future research. In this pa-
per, we will focus on analyzing name/noun recall
errors, as these constitute a large fraction of all re-
call errors. Future work should address the pro-
noun resolution errors and a characterization of the
verbal/demonstrative errors.
</bodyText>
<subsectionHeader confidence="0.998241">
5.4 Analysis of the Name/Noun Recall Errors
</subsectionHeader>
<bodyText confidence="0.999931571428571">
We now turn towards a fine-grained analysis of the
name/noun recall errors.
Table 4 displays the number of such recall errors
made by each system, according to the mention
types of anaphor and antecedent. We are interested
in errors common to all systems, and in qualitative
differences of errors between the systems.
</bodyText>
<subsectionHeader confidence="0.574826">
5.4.1 Common Errors
</subsectionHeader>
<bodyText confidence="0.999888285714286">
Let us first analyze the errors common to all sys-
tems. Our analysis is driven by the question how
these can be characterized, and which knowledge
is missing to resolve such links. We discuss the
errors depending on the mention types of anaphor
and antecedent. The lower part of Table 4 displays
the number of common errors for each category.
</bodyText>
<page confidence="0.983722">
2075
</page>
<table confidence="0.999118555555555">
Description Number of Recall Errors (Anaphor-Antecedent)
Name-Name Noun-Name Name-Noun Noun-Noun
StanfordSieve 1006 517 181 1127
Multigraph 753 501 189 1152
IMSCoref 1082 500 188 1264
BerkeleyCoref 910 456 171 1072
Common errors 475 371 147 835
Correct boundaries idenfified 257 273 108 563
excl. IMSCoref 156 222 97 475
</table>
<tableCaption confidence="0.989036">
Table 4: Name/Noun recall errors for all systems.
</tableCaption>
<table confidence="0.999690142857143">
Common All
Type % Type %
ORG 25% PERSON 26%
PERSON 19% GPE 26%
GPE 16% ORG 20%
DATE 14% NONE 14%
NONE 9% DATE 6%
</table>
<tableCaption confidence="0.966154">
Table 5: Distribution of top five named entity
</tableCaption>
<bodyText confidence="0.991119677419355">
types of common name-name recall errors and all
possible name-name recall errors.
Furthermore, in order to assess the impact of
mention detection, the table shows the number of
common errors where boundaries for both men-
tions were identified correctly by some system.
We can see that boundary identification is a diffi-
cult problem, especially for proper name pairs: for
48% of such errors, no system found the correct
boundaries of both mentions participating in the
error. The number of errors where correct bound-
aries could be found drops significantly after ex-
cluding IMSCoref. This is due to the mention ex-
traction strategy of IMSCoref: the other systems
in our study discard the shorter mention when two
mentions have the same head, IMSCoref keeps
both mentions. Hence, the system is able to cor-
rectly identify some mentions even in the presence
of parsing or preprocessing errors. However, as
a result, IMSCoref has to process many spurious
mentions, which makes learning more difficult.
We conclude that mention detection still consti-
tutes a challenge. We now proceed to a detailed
analysis of errors common to all systems. In pass-
ing we will discuss difficulties in mention detec-
tion with regard to specific error types.
Errors between Pairs of Proper Names. The
systems share 475 recall errors between pairs of
proper names. In Table 5, we compare the distri-
bution of gold named entity types of these errors
with the distribution of gold named entity types of
all possible errors (obtained via a singleton sys-
tem). We see that especially difficult classes of
links are pairs with type ORG or DATE.
Let us now consider lexical features of the er-
rors.9 In 154 errors, the strings match completely,
but the correct resolution was mostly prevented by
annotation inconsistencies (e.g. China instead of
China’s) or propagated parsing and NER errors,
which lead to deficiencies in mention extraction.
For 217 errors, at least one token appears in both
mention strings, as in the “Cole” and the “USS
Cole”. This shows the insufficiency of the features
which hint to alias relations, may it be heuristics or
learned lexical similarities (for 109 of the 217 er-
rors, both mention boundaries were identified cor-
rectly by at least one system). Disambiguation
with respect to knowledge bases could provide a
principled way to identify name variations.
We classified the remaining 104 errors manu-
ally, see Table 6. For a couple of categories such
as identifying acronyms, spelling variations and
aliases, disambiguation could also help. Many er-
rors happen for date mentions, which suggests the
use of temporal tagging features.
Errors for Noun-Name Pairs. We now inves-
tigate the errors where the anaphor is a common
noun and the antecedent is a proper name. 371 er-
rors are common to all systems. The high fraction
of common errors shows that this is an especially
challenging category. We again start by investigat-
ing how the distribution of the named entity type
</bodyText>
<footnote confidence="0.997999">
9When computing these, we ignored case and ignored all
tokens with part-of-speech tag DT or POS.
</footnote>
<page confidence="0.984244">
2076
</page>
<table confidence="0.964957454545455">
Description Occ. Example
Acronyms 20 National Ice Hockey League and
NHL
Alias 24 Florida and the Sunshine State
Annotation 2 Annotation errors (pronoun as
name)
Context 2 Paula Coccoz and juror number ten
Date 29 1989 and last year’s
Metonymy 12 South Afria and Pretoria
Roles 8 Al Gore and the Vice President
Spelling 7 Hsiachuotzu and Hsiachuotsu
</table>
<tableCaption confidence="0.994466">
Table 6: Classification of common name-name re-
call errors without common tokens.
</tableCaption>
<table confidence="0.959722428571429">
Common All
Type % Type %
ORG 28% ORG 27%
PERSON 22% GPE 22%
GPE 19% PERSON 18%
NONE 7% NONE 11%
DATE 5% DATE 5%
</table>
<tableCaption confidence="0.971823">
Table 7: Distribution of top five named entity
</tableCaption>
<bodyText confidence="0.966115346153846">
types of common noun-name recall errors and all
possible noun-name recall errors.
of the antecedent differs when we compare com-
mon errors to all possible errors. The results are
shown in Table 7. Links with a proper name an-
tecedent of type PERSON are especially difficult.
They constitute 22% of the common errors, but
only 18% of all possible errors.
Most mentions are in a hyponymy relation, like
the prime minister and Mr. Papandreou. This con-
firms that harnessing such relations could improve
coreference resolution (Rahman and Ng, 2011;
Uryupina et al., 2011). For 65 of the errors (18%)
there is lexical overlap: the head of the anaphor is
contained in the proper name antecedent, as in the
entire park and the Ocean Park.
When categorizing all common errors accord-
ing to the head of the anaphor, we observe 204 dif-
ferent heads. 142 heads appear only once, but the
top ten heads make up 88 of the 371 errors. The
most frequent heads are company (15), group (12),
government, country and nation (each 9). This
suggests that even with few reliable hyponymy re-
lations recall could be significantly improved.
We observe similar trends when the anaphor is
a proper name and the antecedent is a noun.
</bodyText>
<table confidence="0.9938315">
Reference System
System Stanford MG IMS Berkeley
Stanford - 51 47 61
MG 17 - 42 60
IMS 26 54 - 54
Berkeley 12 42 25 -
</table>
<tableCaption confidence="0.997926">
Table 8: Comparison of noun-name recall errors.
</tableCaption>
<bodyText confidence="0.985511555555556">
Entries are errors made by the system in the row,
while the participating mentions are coreferent ac-
cording to the the system in the column.
Errors between Pairs of Common Nouns. 835
errors between pairs of common nouns are shared
by all systems. For 174 of these, the anaphor is
an indefinite noun phase, which makes resolution
a lot harder, since most coreference resolution sys-
tems classify these as non-anaphoric and therefore
do not attempt resolution.
For further analysis, we split all 835 errors in
two categories, distinguishing whether the head
matches between the mentions or not. In 341 cases
the heads match. For many of these cases, parsing
errors propagate and prevent the systems from rec-
ognizing the correct mention boundaries.
In order to get a better understanding of the er-
rors for nouns with different heads, we randomly
extracted 50 of the 494 pairs and investigated the
relation that holds between the heads. In 23 cases,
the heads were related via hyponymy. In 10 cases
they were synonyms. The remaining 17 cases
involve many different phenomena, for example
meronymy. This confirms findings from previous
research (Vieira and Poesio, 2000).
Hence, looking up lexical relations, especially
hyponymy, might be helpful to solve these cases.
</bodyText>
<subsubsectionHeader confidence="0.737634">
5.4.2 Differences between the Systems
</subsubsectionHeader>
<bodyText confidence="0.999026846153846">
In order to analyze differences between the sys-
tems, we compare the recall errors they make.
The information how recall errors differ between
systems will enable us to understand individual
strengths and weaknesses.
Exemplarily, we will have a look at the differ-
ences in the errors when the anaphor is a common
noun and the antecedent is a proper name. By sys-
tem design and by the total error numbers (Table
4) we expect the learning-based systems to have a
slight advantage over the deterministic systems.
In Table 8 we compare noun-name recall errors
made by each system. Entries are errors made by
</bodyText>
<page confidence="0.991071">
2077
</page>
<note confidence="0.514797">
Number and Proportion of Precision Errors (Anaphor-Antecedent)
</note>
<table confidence="0.99995">
Description Name-Name Noun-Name Name-Noun Noun-Noun
StanfordSieve 1038 31% 64 59% 65 72% 944 48%
Multigraph 1131 30% 76 51% 24 56% 743 42%
IMSCoref 834 26% 74 59% 46 64% 1050 54%
BerkeleyCoref 810 24% 191 67% 60 62% 1015 48%
Common errors 158 1 2 167
</table>
<tableCaption confidence="0.939374">
Table 9: Name/Noun precision errors for all systems. The percentages are the proportion of precision
errors with respect to all decision of the system in that category.
</tableCaption>
<bodyText confidence="0.999610435897436">
the system in the row, while the participating men-
tions are coreferent according to the the system in
the column. The numbers confirm our hypothesis,
but also show that the deterministic systems are
able to recover a few links missed by the learning-
based systems.
For example, BerkeleyCoref recovers 60 links
that could not be found by Multigraph, including
34 links without any common token, such as the
airline and Pan Am. Multigraph recovers only 42
links not found by BerkeleyCoref, 21 without any
common token. Qualitatively, StanfordSieve and
Multigraph are able to resolve a few links thanks
to their engineered substring match, such as the
judge and Dallas District Judge Jack Hampton.
We also conducted similar investigations for
common noun and proper name pairs. For com-
mon nouns, the trends are similar: the learning-
based systems have an advantage over the deter-
ministic systems. However, only few relations be-
tween nouns with different heads are learned –
compared to StanfordSieve, BerkeleyCoref recov-
ers only 11 such pairs, such as the man and an
expert in the law. Recall of the deterministic sys-
tems is further hampered by their strict checks for
modifier agreement, which they employ to keep
precision high. Both systems miss for example the
link from the anaphor the Milosevic regime to the
regime, since the nominal modifier of the anaphor
does not appear in the antecedent.
For proper names, Multigraph employs so-
phisticated alias heuristics which help to resolve
matches such as Marshall Ye Ting’s and his grand-
father Ye Ting. This explains the corresponding
low number in Table 4. The lexicalized features
of Multigraph, IMSCoref and BerkeleyCoref help
to learn aliases when there is no string match, es-
pecially for the bible part of the corpus (resolving
links such as Jesus and the Son of Man).
</bodyText>
<subsectionHeader confidence="0.996228">
5.5 Precision Errors
</subsectionHeader>
<bodyText confidence="0.999918476190476">
In the above analysis we identified common
name/noun recall errors and discussed strengths
and weaknesses of each system. Let us comple-
ment this analysis by a brief discussion of corre-
sponding precision errors.
Table 9 gives an overview. It displays the num-
ber of precision errors for each category, and the
proportion of these errors compared to all deci-
sions in that category. We can see some general
trends from this table: first, more decisions lead to
a higher proportion of errors. This shows the dif-
ficulty of balancing recall and precision. Second,
proper name coreference seems much easier than
common noun coreference. Coreference involving
different mention types is a lot harder – the sys-
tems only attempt few decisions, most of them are
wrong. This confirms findings from our recall er-
ror analysis. Third, the fraction of common errors
is very low, which indicates that precisions errors
stem from various sources, which are handled dif-
ferently by each system.
</bodyText>
<sectionHeader confidence="0.999902" genericHeader="method">
6 Related Work
</sectionHeader>
<bodyText confidence="0.999452642857143">
We now discuss related work in coreference res-
olution error analysis and in the related field of
coreference resolution evaluation metrics.
Error Analysis. While many papers on coref-
erence resolution briefly discuss errors made and
resolved by the system under consideration, only
few concentrate on error analysis. Uryupina
(2008) presents a manual error analysis on the
small MUC-7 test set; Martschat (2013) performs
an automatic coarse-grained error classification on
CoNLL data. By extending and formalizing the
approach of Martschat (2013), we are able to per-
form a large-scale investigation of recall errors
made by state-of-the-art systems.
</bodyText>
<page confidence="0.976316">
2078
</page>
<bodyText confidence="0.999980229166667">
Kummerfeld and Klein (2013) devise a method
to extract errors from transformations of reference
to system entities. They apply this method to a
variety of systems and aggregate errors over these
systems. By aggregating, they are not able to ana-
lyze differences. They furthermore focus on de-
scribing many different error classes, instead of
closely investigating particular phenomena.
Evaluation Metrics. We extract recall and pre-
cision errors. How does our error analysis frame-
work relate to coreference resolution evaluation
metrics, which quantify recall and precision er-
rors? We first observe a fundamental difference:
evaluation metrics deal with scoring coreference
chains, they provide no means of extracting recall
or precision errors. Therefore our analysis com-
plements insights obtained via evaluation metrics.
We follow Chen and Ng (2013) and distinguish
between linguistically agnostic metrics, which do
not employ linguistic information during scoring,
and linguistically informed metrics, which employ
linguistic information similar as we do when com-
puting spanning trees.
We limit the discussion of linguistically ag-
nostic metrics to the three most popular evalua-
tion metrics whose average constitutes the official
score in the CoNLL shared tasks on coreference
resolution: MUC (Vilain et al., 1995), B3 (Bagga
and Baldwin, 1998) and CEAFe (Luo, 2005).10
Our framework bears most similarities to the
MUC metric, as both are based on the same link-
based entity representation. In particular, when we
divide the number of errors extracted from an en-
tity by the size of a spanning tree for that entity, we
obtain a score linearly related11 to the MUC score
for that entity (recall for reference entities, preci-
sion for system entities). B3 and CEAFe are not
founded on a link-based structure. B3 computes
recall by computing the relative overlap of refer-
ence and system entity for each reference mention,
and then normalizes by the number of mentions.
CEAFe computes an optimal entity alignment with
respect to the relative overlap, and then normalizes
by the number of entities. As the metrics are not
link-based, they do not provide means to extract
link-based errors. We leave determining whether
the framework of these metrics exhibits a useful
notion of errors to future work.
</bodyText>
<footnote confidence="0.953004333333333">
10These are linguistically agnostic since they do not differ
between different mention or entity types when evaluating.
11via the transformation x → 1 − x
</footnote>
<bodyText confidence="0.9997325">
Recent work considered devising evaluation
metrics which take linguistic information into
account. Chen and Ng (2013) inject linguis-
tic knowledge into existing evaluation metrics by
weighting links in an entity representation graph.
Tuggener (2014) devises scoring algorithms tai-
lored for particular applications by redefining the
notion of a correct link. While both of these works
focus on scoring, they weight or explicitly define
links in the reference and system entities, thereby
they in principle allow error extraction. However,
the authors do not attempt this and it is not clear
whether the errors extracted that way are useful for
analysis and system development.
</bodyText>
<sectionHeader confidence="0.999574" genericHeader="conclusions">
7 Conclusions
</sectionHeader>
<bodyText confidence="0.99998475">
We presented a novel link-based framework for
coreference resolution error analysis, which ex-
tends and complements previous work. We ap-
plied the framework to analyze recall errors of four
state-of-the-art systems on a large English bench-
mark dataset. Concentrating on errors involving
only proper names and common nouns, we identi-
fied a core set of challenging errors common to all
systems in our study.
We characterized the common errors among a
broad range of properties. In particular, our anal-
ysis highlights and quantifies the usefulness of
world knowledge. Furthermore, by comparing the
recall errors made by each system, we identified
individual strengths and weaknesses. A brief pre-
cision error analysis highlighted the hardness of
resolving noun-name and noun-noun links.
The presented method and findings help to iden-
tify challenges in coreference resolution and to in-
vestigate ways to overcome these challenges.
</bodyText>
<sectionHeader confidence="0.997487" genericHeader="acknowledgments">
Acknowledgments
</sectionHeader>
<bodyText confidence="0.9997915">
This work has been funded by the Klaus Tschira
Foundation, Heidelberg, Germany. The first au-
thor has been supported by a HITS Ph.D. scholar-
ship.
</bodyText>
<sectionHeader confidence="0.998897" genericHeader="references">
References
</sectionHeader>
<reference confidence="0.991645142857143">
Mira Ariel. 1988. Referring and accessibility. Journal
of Linguistics, 24(1):65–87.
Amit Bagga and Breck Baldwin. 1998. Algorithms
for scoring coreference chains. In Proceedings
of the 1st International Conference on Language
Resources and Evaluation, Granada, Spain, 28–30
May 1998, pages 563–566.
</reference>
<page confidence="0.958704">
2079
</page>
<reference confidence="0.993692824074074">
Mohit Bansal and Dan Klein. 2012. Coreference se-
mantics from web features. In Proceedings of the
50th Annual Meeting of the Association for Compu-
tational Linguistics (Volume 1: Long Papers), Jeju
Island, Korea, 8–14 July 2012, pages 389–398.
Eric Bengtson and Dan Roth. 2008. Understanding
the value of features for coreference resolution. In
Proceedings of the 2008 Conference on Empirical
Methods in Natural Language Processing, Waikiki,
Honolulu, Hawaii, 25–27 October 2008, pages 294–
303.
Anders Bj¨orkelund and Rich´ard Farkas. 2012. Data-
driven multilingual coreference resolution using re-
solver stacking. In Proceedings of the Shared Task
of the 16th Conference on Computational Natural
Language Learning, Jeju Island, Korea, 12–14 July
2012, pages 49–55.
Jie Cai and Michael Strube. 2010. End-to-end coref-
erence resolution via hypergraph partitioning. In
Proceedings of the 23rd International Conference
on Computational Linguistics, Beijing, China, 23–
27 August 2010, pages 143–151.
Chen Chen and Vincent Ng. 2012. Combining the
best of two worlds: A hybrid approach to multilin-
gual coreference resolution. In Proceedings of the
Shared Task of the 16th Conference on Computa-
tional Natural Language Learning, Jeju Island, Ko-
rea, 12–14 July 2012, pages 56–63.
Chen Chen and Vincent Ng. 2013. Linguistically
aware coreference evaluation metrics. In Proceed-
ings of the 6th International Joint Conference on
Natural Language Processing, Nagoya, Japan, 14–
18 October 2013, pages 1366–1374.
Greg Durrett and Dan Klein. 2013. Easy victories
and uphill battles in coreference resolution. In Pro-
ceedings of the 2013 Conference on Empirical Meth-
ods in Natural Language Processing, Seattle, Wash.,
18–21 October 2013, pages 1971–1982.
Eraldo Fernandes, C´ıcero dos Santos, and Ruy Milidi´u.
2012. Latent structure perceptron with feature in-
duction for unrestricted coreference resolution. In
Proceedings of the Shared Task of the 16th Confer-
ence on Computational Natural Language Learning,
Jeju Island, Korea, 12–14 July 2012, pages 41–48.
Jonathan K. Kummerfeld and Dan Klein. 2013. Error-
driven analysis of challenges in coreference reso-
lution. In Proceedings of the 2013 Conference on
Empirical Methods in Natural Language Process-
ing, Seattle, Wash., 18–21 October 2013, pages 265–
277.
Shalom Lappin and Herbert J. Leass. 1994. An algo-
rithm for pronominal anaphora resolution. Compu-
tational Linguistics, 20(4):535–561.
Heeyoung Lee, Angel Chang, Yves Peirsman,
Nathanael Chambers, Mihai Surdeanu, and Dan Ju-
rafsky. 2013. Deterministic coreference resolu-
tion based on entity-centric, precision-ranked rules.
Computational Linguistics, 39(4):885–916.
Xiaoqiang Luo. 2005. On coreference resolution
performance metrics. In Proceedings of the Hu-
man Language Technology Conference and the 2005
Conference on Empirical Methods in Natural Lan-
guage Processing, Vancouver, B.C., Canada, 6–8
October 2005, pages 25–32.
Sebastian Martschat, Jie Cai, Samuel Broscheit, ´Eva
M´ujdricza-Maydt, and Michael Strube. 2012. A
multigraph model for coreference resolution. In
Proceedings of the Shared Task of the 16th Confer-
ence on Computational Natural Language Learning,
Jeju Island, Korea, 12–14 July 2012, pages 100–106.
Sebastian Martschat. 2013. Multigraph clustering for
unsupervised coreference resolution. In 51st Annual
Meeting of the Association for Computational Lin-
guistics: Proceedings of the Student Research Work-
shop, Sofia, Bulgaria, 5–7 August 2013, pages 81–
88.
Alexis Mitchell, Stephanie Strassel, Shudong Huang,
and Ramez Zakhary. 2004. ACE 2004 multilingual
training corpus. LDC2005T09, Philadelphia, Penn.:
Linguistic Data Consortium.
Vincent Ng and Claire Cardie. 2002. Improving ma-
chine learning approaches to coreference resolution.
In Proceedings of the 40th Annual Meeting of the As-
sociation for Computational Linguistics, Philadel-
phia, Penn., 7–12 July 2002, pages 104–111.
Vincent Ng. 2010. Supervised noun phrase corefer-
ence research: The first fifteen years. In Proceed-
ings of the 48th Annual Meeting of the Associa-
tion for Computational Linguistics, Uppsala, Swe-
den, 11–16 July 2010, pages 1396–1411.
Simone Paolo Ponzetto and Michael Strube. 2006.
Exploiting semantic role labeling, WordNet and
Wikipedia for coreference resolution. In Proceed-
ings of the Human Language Technology Confer-
ence of the North American Chapter of the Associa-
tion for Computational Linguistics, New York, N.Y.,
4–9 June 2006, pages 192–199.
Sameer Pradhan, Lance Ramshaw, Mitchell Marcus,
Martha Palmer, Ralph Weischedel, and Nianwen
Xue. 2011. CoNLL-2011 Shared Task: Modeling
unrestricted coreference in OntoNotes. In Proceed-
ings of the Shared Task of the 15th Conference on
Computational Natural Language Learning, Port-
land, Oreg., 23–24 June 2011, pages 1–27.
Sameer Pradhan, Alessandro Moschitti, Nianwen Xue,
Olga Uryupina, and Yuchen Zhang. 2012. CoNLL-
2012 Shared Task: Modeling multilingual unre-
stricted coreference in OntoNotes. In Proceedings
</reference>
<page confidence="0.774308">
2080
</page>
<reference confidence="0.999717272727273">
of the Shared Task of the 16th Conference on Com-
putational Natural Language Learning, Jeju Island,
Korea, 12–14 July 2012, pages 1–40.
Sameer Pradhan, Xiaoqiang Luo, Marta Recasens, Ed-
uard Hovy, Vincent Ng, and Michael Strube. 2014.
Scoring coreference partitions of predicted men-
tions: A reference implementation. In Proceed-
ings of the 52nd Annual Meeting of the Association
for Computational Linguistics (Volume 2: Short Pa-
pers), Baltimore, Md., 22–27 June 2014, pages 30–
35.
Karthik Raghunathan, Heeyoung Lee, Sudarshan Ran-
garajan, Nathanael Chambers, Mihai Surdeanu, Dan
Jurafsky, and Christopher Manning. 2010. A multi-
pass sieve for coreference resolution. In Proceed-
ings of the 2010 Conference on Empirical Methods
in Natural Language Processing, Cambridge, Mass.,
9–11 October 2010, pages 492–501.
Altaf Rahman and Vincent Ng. 2011. Coreference res-
olution with world knowledge. In Proceedings of
the 49th Annual Meeting of the Association for Com-
putational Linguistics (Volume 1: Long Papers),
Portland, Oreg., 19–24 June 2011, pages 814–824.
Lev Ratinov and Dan Roth. 2012. Learning-based
multi-sieve co-reference resolution with knowledge.
In Proceedings of the 2012 Conference on Empirical
Methods in Natural Language Processing and Nat-
ural Language Learning, Jeju Island, Korea, 12–14
July 2012, pages 1234–1244.
Don Tuggener. 2014. Coreference resolution evalua-
tion for higher level applications. In Proceedings of
the 14th Conference of the European Chapter of the
Association for Computational Linguistics, Gothen-
burg, Sweden, 26–30 April 2014, pages 231–235.
Olga Uryupina, Massimo Poesio, Claudio Giuliano,
and Kateryna Tymoshenko. 2011. Disambigua-
tion and filtering methods in using web knowledge
for coreference resolution. In Proceedings of the
24th International Florida Artificial Intelligence Re-
search Society Conference, Palm Beach, USA, 18–
20 May 2011, pages 317–322.
Olga Uryupina. 2007. Knowledge acquisition for
coreference resolution. Ph.D. thesis, Saarland Uni-
versity, Saarbr¨ucken, Germany.
Olga Uryupina. 2008. Error analysis for learning-
based coreference resolution. In Proceedings of
the 6th International Conference on Language Re-
sources and Evaluation, Marrakech, Morocco, 26
May – 1 June 2008, pages 1914–1919.
Renata Vieira and Massimo Poesio. 2000. An
empirically-based system for processing definite de-
scriptions. Computational Linguistics, 26(4):539–
593.
Marc Vilain, John Burger, John Aberdeen, Dennis Con-
nolly, and Lynette Hirschman. 1995. A model-
theoretic coreference scoring scheme. In Proceed-
ings of the 6th Message Understanding Conference
(MUC-6), pages 45–52, San Mateo, Cal. Morgan
Kaufmann.
Ralph Weischedel, Martha Palmer, Mitchell Marcus,
Eduard Hovy, Sameer Pradhan, Lance Ramshaw,
Nianwen Xue, Ann Taylor, Jeff Kaufman, Michelle
Franchini, Mohammed El-Bachouti, Robert Belvin,
and Ann Houston. 2013. OntoNotes release 5.0.
LDC2013T19, Philadelphia, Penn.: Linguistic Data
Consortium.
</reference>
<page confidence="0.993595">
2081
</page>
</variant>
</algorithm>
<algorithm name="ParsHed" version="110505">
<variant no="0" confidence="0.595335">
<title confidence="0.991453">Recall Error Analysis for Coreference Resolution</title>
<author confidence="0.661234">Martschat</author>
<affiliation confidence="0.746895">Heidelberg Institute for Theoretical Studies</affiliation>
<address confidence="0.717777">Schloss-Wolfsbrunnenweg 35, 69118 Heidelberg,</address>
<email confidence="0.998408">(sebastian.martschat|michael.strube)@h-its.org</email>
<abstract confidence="0.99975075">We present a novel method for coreference resolution error analysis which we apply to perform a recall error analysis of four state-of-the-art English coreference resolution systems. Our analysis highlights differences between the systems and identifies that the majority of recall errors for nouns and names are shared by all systems. We characterize this set of common challenging errors in terms of a broad range of lexical and semantic properties.</abstract>
</variant>
</algorithm>
<algorithm name="ParsCit" version="110505">
<citationList>
<citation valid="true">
<authors>
<author>Mira Ariel</author>
</authors>
<title>Referring and accessibility.</title>
<date>1988</date>
<journal>Journal of Linguistics,</journal>
<volume>24</volume>
<issue>1</issue>
<contexts>
<context position="12850" citStr="Ariel, 1988" startWordPosition="2124" endWordPosition="2125">ith a nonpronominal anaphor and a pronominal antecedent are difficult to process and considered unreliable (Ng and Cardie, 2002; Bengtson and Roth, 2008). On the other hand, the missed link (the president, Obama) constitutes a well-defined hyponymy relation which can be found in knowledge bases and is easily interpretable by humans. Uryupina (Uryupina, 2007; Uryupina, 2008) presents a recall error analysis where she takes the “intuitively easiest” missing link to analyze (Uryupina, 2007, p. 196). How can we formalize such an intuition? We will employ a notion grounded in accessibility theory (Ariel, 1988). Names and nouns refer to less accessible entities than pronouns do. For such anaphors, we prefer descriptive (name/nominal) antecedents. Inspired by Ariel’s degrees of accessibility, we choose a target for a given anaphor mi as follows: • If mi is a pronoun, choose the closest preceding mention. • If mi is not a pronoun, choose the closest preceding proper name. If no such mention exists, choose the closest preceding common noun. If no such mention exists, choose the closest preceding mention. Applied to the example from Figure 1, this algorithm extracts the error (the president, Obama).3 3.</context>
</contexts>
<marker>Ariel, 1988</marker>
<rawString>Mira Ariel. 1988. Referring and accessibility. Journal of Linguistics, 24(1):65–87.</rawString>
</citation>
<citation valid="true">
<authors>
<author>Amit Bagga</author>
<author>Breck Baldwin</author>
</authors>
<title>Algorithms for scoring coreference chains.</title>
<date>1998</date>
<booktitle>In Proceedings of the 1st International Conference on Language Resources and Evaluation,</booktitle>
<pages>563--566</pages>
<location>Granada,</location>
<contexts>
<context position="37148" citStr="Bagga and Baldwin, 1998" startWordPosition="6059" endWordPosition="6062">ll or precision errors. Therefore our analysis complements insights obtained via evaluation metrics. We follow Chen and Ng (2013) and distinguish between linguistically agnostic metrics, which do not employ linguistic information during scoring, and linguistically informed metrics, which employ linguistic information similar as we do when computing spanning trees. We limit the discussion of linguistically agnostic metrics to the three most popular evaluation metrics whose average constitutes the official score in the CoNLL shared tasks on coreference resolution: MUC (Vilain et al., 1995), B3 (Bagga and Baldwin, 1998) and CEAFe (Luo, 2005).10 Our framework bears most similarities to the MUC metric, as both are based on the same linkbased entity representation. In particular, when we divide the number of errors extracted from an entity by the size of a spanning tree for that entity, we obtain a score linearly related11 to the MUC score for that entity (recall for reference entities, precision for system entities). B3 and CEAFe are not founded on a link-based structure. B3 computes recall by computing the relative overlap of reference and system entity for each reference mention, and then normalizes by the n</context>
</contexts>
<marker>Bagga, Baldwin, 1998</marker>
<rawString>Amit Bagga and Breck Baldwin. 1998. Algorithms for scoring coreference chains. In Proceedings of the 1st International Conference on Language Resources and Evaluation, Granada, Spain, 28–30 May 1998, pages 563–566.</rawString>
</citation>
<citation valid="true">
<authors>
<author>Mohit Bansal</author>
<author>Dan Klein</author>
</authors>
<title>Coreference semantics from web features.</title>
<date>2012</date>
<booktitle>In Proceedings of the 50th Annual Meeting of the Association for Computational Linguistics (Volume 1: Long Papers), Jeju Island, Korea, 8–14</booktitle>
<pages>389--398</pages>
<contexts>
<context position="2015" citStr="Bansal and Klein, 2012" startWordPosition="295" endWordPosition="298">Due to the lack of any string overlap, most state-of-the-art systems will miss the link between the factory and the Qinghai Petroleum Bureau’s Ge’ermu oil refinery. The information that factory is a hypernym of refinery, however, may be useful to resolve such links. The aim of this paper is to quantify and characterize such recall errors made by state-of-theart coreference resolution systems. By doing so, we provide a solid foundation for work on employing knowledge sources for improving recall for coreference resolution (Ponzetto and Strube, 2006; Rahman and Ng, 2011; Ratinov and Roth, 2012; Bansal and Klein, 2012, inter alia). In particular, we make the following contributions: We present a novel framework for coreference resolution error analysis. This yields a formal foundation for previous work on link-based error analysis (Uryupina, 2008; Martschat, 2013) and complements work on transformation-based error analysis (Kummerfeld and Klein, 2013). We apply the method proposed in this paper to perform a recall error analysis of four state-ofthe-art systems, encompassing deterministic and learning-based approaches. In particular, we identify and characterize a set of challenging errors common to all sys</context>
</contexts>
<marker>Bansal, Klein, 2012</marker>
<rawString>Mohit Bansal and Dan Klein. 2012. Coreference semantics from web features. In Proceedings of the 50th Annual Meeting of the Association for Computational Linguistics (Volume 1: Long Papers), Jeju Island, Korea, 8–14 July 2012, pages 389–398.</rawString>
</citation>
<citation valid="true">
<authors>
<author>Eric Bengtson</author>
<author>Dan Roth</author>
</authors>
<title>Understanding the value of features for coreference resolution.</title>
<date>2008</date>
<booktitle>In Proceedings of the 2008 Conference on Empirical Methods in Natural Language Processing,</booktitle>
<pages>294--303</pages>
<location>Waikiki, Honolulu, Hawaii,</location>
<contexts>
<context position="12391" citStr="Bengtson and Roth, 2008" startWordPosition="2049" endWordPosition="2052">hoosing Edges by Distance. The most straight-forward way to decide on an edge is to take the edge with smallest mention distance between source and target. This is the approach taken by Martschat (2013). Choosing Edges by Accessibility. However, the distance-based approach may lead to unintuitive results. Let us consider again the BARACK OBAMA example from Figure 1. When choosing edges by distance, we would extract the error (the president, he). However, such links with a nonpronominal anaphor and a pronominal antecedent are difficult to process and considered unreliable (Ng and Cardie, 2002; Bengtson and Roth, 2008). On the other hand, the missed link (the president, Obama) constitutes a well-defined hyponymy relation which can be found in knowledge bases and is easily interpretable by humans. Uryupina (Uryupina, 2007; Uryupina, 2008) presents a recall error analysis where she takes the “intuitively easiest” missing link to analyze (Uryupina, 2007, p. 196). How can we formalize such an intuition? We will employ a notion grounded in accessibility theory (Ariel, 1988). Names and nouns refer to less accessible entities than pronouns do. For such anaphors, we prefer descriptive (name/nominal) antecedents. In</context>
</contexts>
<marker>Bengtson, Roth, 2008</marker>
<rawString>Eric Bengtson and Dan Roth. 2008. Understanding the value of features for coreference resolution. In Proceedings of the 2008 Conference on Empirical Methods in Natural Language Processing, Waikiki, Honolulu, Hawaii, 25–27 October 2008, pages 294– 303.</rawString>
</citation>
<citation valid="true">
<authors>
<author>Anders Bj¨orkelund</author>
<author>Rich´ard Farkas</author>
</authors>
<title>Datadriven multilingual coreference resolution using resolver stacking.</title>
<date>2012</date>
<booktitle>In Proceedings of the Shared Task of the 16th Conference on Computational Natural Language Learning,</booktitle>
<pages>49--55</pages>
<location>Jeju Island,</location>
<marker>Bj¨orkelund, Farkas, 2012</marker>
<rawString>Anders Bj¨orkelund and Rich´ard Farkas. 2012. Datadriven multilingual coreference resolution using resolver stacking. In Proceedings of the Shared Task of the 16th Conference on Computational Natural Language Learning, Jeju Island, Korea, 12–14 July 2012, pages 49–55.</rawString>
</citation>
<citation valid="true">
<authors>
<author>Jie Cai</author>
<author>Michael Strube</author>
</authors>
<title>End-to-end coreference resolution via hypergraph partitioning.</title>
<date>2010</date>
<booktitle>In Proceedings of the 23rd International Conference on Computational Linguistics,</booktitle>
<volume>23</volume>
<pages>143--151</pages>
<location>Beijing,</location>
<contexts>
<context position="16583" citStr="Cai and Strube (2010)" startWordPosition="2682" endWordPosition="2685">.html . We use the CoNLL 2012 system. 7http://nlp.cs.berkeley.edu/ berkeleycoref.shtml 2073 System MUC B3 CEAF, Average Fernandes et al. 69.46 57.83 54.43 60.57 Martschat 66.22 55.47 51.90 57.86 StanfordSieve 64.96 54.49 51.24 56.90 Multigraph 69.13 58.61 56.06 61.28 IMSCoref 67.15 55.19 50.94 57.76 BerkeleyCoref 70.27 59.29 56.11 61.89 Table 1: Comparison of the systems with Fernandes et al. (2012) and with Martschat (2013) on CoNLL’12 English development data. For Multigraph, we modified the system described in Martschat (2013) slightly to allow for the incorporation of distance (similar to Cai and Strube (2010)). Inspired by Lappin and Leass (1994), we add salience weights for subjects and objects to the model to improve third-person pronoun resolution. We also extended the feature set by a substring feature. Furthermore, motivated by Chen and Ng (2012), we added a lexicalized feature for non-pronominal mentions that were coreferent in at least 50% of the cases in the training data. StanfordSieve was run with its standard CoNLL shared task settings. The learning-based systems were trained on the CoNLL’12 training data. We trained IMSCoref with its standard settings, and trained BerkeleyCoref with th</context>
</contexts>
<marker>Cai, Strube, 2010</marker>
<rawString>Jie Cai and Michael Strube. 2010. End-to-end coreference resolution via hypergraph partitioning. In Proceedings of the 23rd International Conference on Computational Linguistics, Beijing, China, 23– 27 August 2010, pages 143–151.</rawString>
</citation>
<citation valid="true">
<authors>
<author>Chen Chen</author>
<author>Vincent Ng</author>
</authors>
<title>Combining the best of two worlds: A hybrid approach to multilingual coreference resolution.</title>
<date>2012</date>
<booktitle>In Proceedings of the Shared Task of the 16th Conference on Computational Natural Language Learning,</booktitle>
<pages>56--63</pages>
<location>Jeju Island,</location>
<contexts>
<context position="16830" citStr="Chen and Ng (2012)" startWordPosition="2723" endWordPosition="2726">58.61 56.06 61.28 IMSCoref 67.15 55.19 50.94 57.76 BerkeleyCoref 70.27 59.29 56.11 61.89 Table 1: Comparison of the systems with Fernandes et al. (2012) and with Martschat (2013) on CoNLL’12 English development data. For Multigraph, we modified the system described in Martschat (2013) slightly to allow for the incorporation of distance (similar to Cai and Strube (2010)). Inspired by Lappin and Leass (1994), we add salience weights for subjects and objects to the model to improve third-person pronoun resolution. We also extended the feature set by a substring feature. Furthermore, motivated by Chen and Ng (2012), we added a lexicalized feature for non-pronominal mentions that were coreferent in at least 50% of the cases in the training data. StanfordSieve was run with its standard CoNLL shared task settings. The learning-based systems were trained on the CoNLL’12 training data. We trained IMSCoref with its standard settings, and trained BerkeleyCoref with the final feature set from Durrett and Klein (2013) for twenty iterations. We evaluate the systems on English CoNLL’12 development data and compare it with the winning system of the CoNLL’12 shared task (Fernandes et al., 2012) and with Martschat (2</context>
</contexts>
<marker>Chen, Ng, 2012</marker>
<rawString>Chen Chen and Vincent Ng. 2012. Combining the best of two worlds: A hybrid approach to multilingual coreference resolution. In Proceedings of the Shared Task of the 16th Conference on Computational Natural Language Learning, Jeju Island, Korea, 12–14 July 2012, pages 56–63.</rawString>
</citation>
<citation valid="true">
<authors>
<author>Chen Chen</author>
<author>Vincent Ng</author>
</authors>
<title>Linguistically aware coreference evaluation metrics.</title>
<date>2013</date>
<booktitle>In Proceedings of the 6th International Joint Conference on Natural Language Processing,</booktitle>
<pages>1366--1374</pages>
<location>Nagoya,</location>
<contexts>
<context position="36653" citStr="Chen and Ng (2013)" startWordPosition="5987" endWordPosition="5990">ot able to analyze differences. They furthermore focus on describing many different error classes, instead of closely investigating particular phenomena. Evaluation Metrics. We extract recall and precision errors. How does our error analysis framework relate to coreference resolution evaluation metrics, which quantify recall and precision errors? We first observe a fundamental difference: evaluation metrics deal with scoring coreference chains, they provide no means of extracting recall or precision errors. Therefore our analysis complements insights obtained via evaluation metrics. We follow Chen and Ng (2013) and distinguish between linguistically agnostic metrics, which do not employ linguistic information during scoring, and linguistically informed metrics, which employ linguistic information similar as we do when computing spanning trees. We limit the discussion of linguistically agnostic metrics to the three most popular evaluation metrics whose average constitutes the official score in the CoNLL shared tasks on coreference resolution: MUC (Vilain et al., 1995), B3 (Bagga and Baldwin, 1998) and CEAFe (Luo, 2005).10 Our framework bears most similarities to the MUC metric, as both are based on t</context>
<context position="38369" citStr="Chen and Ng (2013)" startWordPosition="6259" endWordPosition="6262">er of mentions. CEAFe computes an optimal entity alignment with respect to the relative overlap, and then normalizes by the number of entities. As the metrics are not link-based, they do not provide means to extract link-based errors. We leave determining whether the framework of these metrics exhibits a useful notion of errors to future work. 10These are linguistically agnostic since they do not differ between different mention or entity types when evaluating. 11via the transformation x → 1 − x Recent work considered devising evaluation metrics which take linguistic information into account. Chen and Ng (2013) inject linguistic knowledge into existing evaluation metrics by weighting links in an entity representation graph. Tuggener (2014) devises scoring algorithms tailored for particular applications by redefining the notion of a correct link. While both of these works focus on scoring, they weight or explicitly define links in the reference and system entities, thereby they in principle allow error extraction. However, the authors do not attempt this and it is not clear whether the errors extracted that way are useful for analysis and system development. 7 Conclusions We presented a novel link-ba</context>
</contexts>
<marker>Chen, Ng, 2013</marker>
<rawString>Chen Chen and Vincent Ng. 2013. Linguistically aware coreference evaluation metrics. In Proceedings of the 6th International Joint Conference on Natural Language Processing, Nagoya, Japan, 14– 18 October 2013, pages 1366–1374.</rawString>
</citation>
<citation valid="true">
<authors>
<author>Greg Durrett</author>
<author>Dan Klein</author>
</authors>
<title>Easy victories and uphill battles in coreference resolution.</title>
<date>2013</date>
<booktitle>In Proceedings of the 2013 Conference on Empirical Methods in Natural Language Processing,</booktitle>
<pages>1971--1982</pages>
<location>Seattle, Wash.,</location>
<contexts>
<context position="951" citStr="Durrett and Klein, 2013" startWordPosition="130" endWordPosition="133">pply to perform a recall error analysis of four state-of-the-art English coreference resolution systems. Our analysis highlights differences between the systems and identifies that the majority of recall errors for nouns and names are shared by all systems. We characterize this set of common challenging errors in terms of a broad range of lexical and semantic properties. 1 Introduction Coreference resolution is the task of determining which mentions in a text refer to the same entity. State-of-the-art approaches include both learningbased (Fernandes et al., 2012; Bj¨orkelund and Farkas, 2012; Durrett and Klein, 2013) and deterministic models (Lee et al., 2013; Martschat, 2013). These approaches achieve state-of-the-art performance mainly relying on morphosyntactic and lexical factors. However, consider the following example. In order to improving the added value of oil products, the second phase project of the Qinghai Petroleum Bureau’s Ge’ermu oil refinery has been put into production. This will further improve the factory’s oil products structure. Due to the lack of any string overlap, most state-of-the-art systems will miss the link between the factory and the Qinghai Petroleum Bureau’s Ge’ermu oil ref</context>
<context position="15567" citStr="Durrett and Klein, 2013" startWordPosition="2546" endWordPosition="2549">h by making more confident decisions first. • Multigraph5 (Martschat, 2013) is a deterministic pairwise system which is based on Martschat et al. (2012), the second-ranking system in the English track of the CoNLL’12 shared task. It uses a subset of features as hard constraints and chooses an antecedent for a mention by summing up the remaining boolean features. • IMSCoref6 (Bj¨orkelund and Farkas, 2012) ranked second overall in the CoNLL’12 shared task (third for English). It stacks multiple decoders and relies on a combination of standard pairwise and lexicalized features. • BerkeleyCoref7 (Durrett and Klein, 2013) is a state-of-the-art system that uses mainly lexicalized features and a latent antecedent ranking architecture. It outperforms StanfordSieve and IMSCoref on the CoNLL’11 data. 4Part of Stanford CoreNLP, available at http://nlp. stanford.edu/software/corenlp.shtml. We use version 3.4. 5http://smartschat.de/software 6http://www.ims.uni-stuttgart.de/ forschung/ressourcen/werkzeuge/IMSCoref. en.html . We use the CoNLL 2012 system. 7http://nlp.cs.berkeley.edu/ berkeleycoref.shtml 2073 System MUC B3 CEAF, Average Fernandes et al. 69.46 57.83 54.43 60.57 Martschat 66.22 55.47 51.90 57.86 StanfordSi</context>
<context position="17232" citStr="Durrett and Klein (2013)" startWordPosition="2786" endWordPosition="2789"> Leass (1994), we add salience weights for subjects and objects to the model to improve third-person pronoun resolution. We also extended the feature set by a substring feature. Furthermore, motivated by Chen and Ng (2012), we added a lexicalized feature for non-pronominal mentions that were coreferent in at least 50% of the cases in the training data. StanfordSieve was run with its standard CoNLL shared task settings. The learning-based systems were trained on the CoNLL’12 training data. We trained IMSCoref with its standard settings, and trained BerkeleyCoref with the final feature set from Durrett and Klein (2013) for twenty iterations. We evaluate the systems on English CoNLL’12 development data and compare it with the winning system of the CoNLL’12 shared task (Fernandes et al., 2012) and with Martschat (2013) in Table 1, using the reference implementation v7 of the CoNLL scorer (Pradhan et al., 2014). BerkeleyCoref performs best according to all metrics, followed by Multigraph. StanfordSieve is the worst performing system: the gap to BerkeleyCoref is five points in average score. 4.3 Discussion Although we analyze recent systems on a recently published coreference data set, we believe that the resul</context>
<context position="18811" citStr="Durrett and Klein, 2013" startWordPosition="3028" endWordPosition="3031">he coreference resolution systems presented in the previous section are a representative sample of the state of the art. Therefore, by analyzing the errors they make, we can learn about remaining challenges in coreference resolution and analyze the qualitative differences between the systems. The results of such an analysis will deepen our understanding of coreference resolution and will suggest promising directions for further research. 5.1 Experimental Settings Previous studies identified the presence of recall errors as a main bottleneck for improving performance (Raghunathan et al., 2010; Durrett and Klein, 2013; Kummerfeld and Klein, 2013). This is also evidenced by the CoNLL shared tasks on coreference resolution (Pradhan et al., 2011; Pradhan et al., 2012), where most competitive systems had higher precision than recall. This indicates that an analysis of recall errors helps to understand and improve the state of the art. Hence, we focus on analyzing recall errors, and complement this by a brief analysis of precision errors. We analyze errors of the four systems presented in the previous section on the CoNLL’12 English development data. To extract recall errors we employ the spanning tree algorith</context>
</contexts>
<marker>Durrett, Klein, 2013</marker>
<rawString>Greg Durrett and Dan Klein. 2013. Easy victories and uphill battles in coreference resolution. In Proceedings of the 2013 Conference on Empirical Methods in Natural Language Processing, Seattle, Wash., 18–21 October 2013, pages 1971–1982.</rawString>
</citation>
<citation valid="true">
<authors>
<author>Eraldo Fernandes</author>
<author>C´ıcero dos Santos</author>
<author>Ruy Milidi´u</author>
</authors>
<title>Latent structure perceptron with feature induction for unrestricted coreference resolution.</title>
<date>2012</date>
<booktitle>In Proceedings of the Shared Task of the 16th Conference on Computational Natural Language Learning,</booktitle>
<pages>41--48</pages>
<location>Jeju Island,</location>
<marker>Fernandes, Santos, Milidi´u, 2012</marker>
<rawString>Eraldo Fernandes, C´ıcero dos Santos, and Ruy Milidi´u. 2012. Latent structure perceptron with feature induction for unrestricted coreference resolution. In Proceedings of the Shared Task of the 16th Conference on Computational Natural Language Learning, Jeju Island, Korea, 12–14 July 2012, pages 41–48.</rawString>
</citation>
<citation valid="true">
<authors>
<author>Jonathan K Kummerfeld</author>
<author>Dan Klein</author>
</authors>
<title>Errordriven analysis of challenges in coreference resolution.</title>
<date>2013</date>
<booktitle>In Proceedings of the 2013 Conference on Empirical Methods in Natural Language Processing,</booktitle>
<pages>265--277</pages>
<location>Seattle, Wash.,</location>
<contexts>
<context position="2355" citStr="Kummerfeld and Klein, 2013" startWordPosition="342" endWordPosition="345">rors made by state-of-theart coreference resolution systems. By doing so, we provide a solid foundation for work on employing knowledge sources for improving recall for coreference resolution (Ponzetto and Strube, 2006; Rahman and Ng, 2011; Ratinov and Roth, 2012; Bansal and Klein, 2012, inter alia). In particular, we make the following contributions: We present a novel framework for coreference resolution error analysis. This yields a formal foundation for previous work on link-based error analysis (Uryupina, 2008; Martschat, 2013) and complements work on transformation-based error analysis (Kummerfeld and Klein, 2013). We apply the method proposed in this paper to perform a recall error analysis of four state-ofthe-art systems, encompassing deterministic and learning-based approaches. In particular, we identify and characterize a set of challenging errors common to all systems, and discuss strengths and weaknesses of each system regarding specific error types. We also present a brief precision error analysis. A toolkit which implements the framework proposed in this paper is available for download.1 2 A Link-Based Analysis Framework In this section we discuss challenges in coreference resolution error anal</context>
<context position="5473" citStr="Kummerfeld and Klein (2013)" startWordPosition="834" endWordPosition="837">lasses of mentions in a document according to the coreference relation. In order to extract errors, we need to compare the reference equivalence classes, given by the annotation, with the system equivalence classes obtained from system output. The key question now is how we represent these equivalence classes of mentions. Adapting common terminology, we also refer to the equivalence classes as entities. 2.3 Representing Entities The most straightforward entity representation ignores any structure and models an entity as a set of mentions. This representation was utilized for error analysis by Kummerfeld and Klein (2013), who extract errors by transforming reference into system entities. In this set-based representation, we can only extract whether two mentions corefer at all. More fine-grained information, for example about antecedent information, is not accessible. We therefore propose to employ a structured entity representation, which explicitly models links established by the coreference relation between mentions. This leads to a link-based error representation which formalizes the methods presented in Uryupina (2008) and Martschat (2013). We employ for representation a complete onedirectional graph. Tha</context>
<context position="18840" citStr="Kummerfeld and Klein, 2013" startWordPosition="3032" endWordPosition="3035"> systems presented in the previous section are a representative sample of the state of the art. Therefore, by analyzing the errors they make, we can learn about remaining challenges in coreference resolution and analyze the qualitative differences between the systems. The results of such an analysis will deepen our understanding of coreference resolution and will suggest promising directions for further research. 5.1 Experimental Settings Previous studies identified the presence of recall errors as a main bottleneck for improving performance (Raghunathan et al., 2010; Durrett and Klein, 2013; Kummerfeld and Klein, 2013). This is also evidenced by the CoNLL shared tasks on coreference resolution (Pradhan et al., 2011; Pradhan et al., 2012), where most competitive systems had higher precision than recall. This indicates that an analysis of recall errors helps to understand and improve the state of the art. Hence, we focus on analyzing recall errors, and complement this by a brief analysis of precision errors. We analyze errors of the four systems presented in the previous section on the CoNLL’12 English development data. To extract recall errors we employ the spanning tree algorithm which chooses edges by acce</context>
<context position="35832" citStr="Kummerfeld and Klein (2013)" startWordPosition="5865" endWordPosition="5868">error analysis and in the related field of coreference resolution evaluation metrics. Error Analysis. While many papers on coreference resolution briefly discuss errors made and resolved by the system under consideration, only few concentrate on error analysis. Uryupina (2008) presents a manual error analysis on the small MUC-7 test set; Martschat (2013) performs an automatic coarse-grained error classification on CoNLL data. By extending and formalizing the approach of Martschat (2013), we are able to perform a large-scale investigation of recall errors made by state-of-the-art systems. 2078 Kummerfeld and Klein (2013) devise a method to extract errors from transformations of reference to system entities. They apply this method to a variety of systems and aggregate errors over these systems. By aggregating, they are not able to analyze differences. They furthermore focus on describing many different error classes, instead of closely investigating particular phenomena. Evaluation Metrics. We extract recall and precision errors. How does our error analysis framework relate to coreference resolution evaluation metrics, which quantify recall and precision errors? We first observe a fundamental difference: evalu</context>
</contexts>
<marker>Kummerfeld, Klein, 2013</marker>
<rawString>Jonathan K. Kummerfeld and Dan Klein. 2013. Errordriven analysis of challenges in coreference resolution. In Proceedings of the 2013 Conference on Empirical Methods in Natural Language Processing, Seattle, Wash., 18–21 October 2013, pages 265– 277.</rawString>
</citation>
<citation valid="true">
<authors>
<author>Shalom Lappin</author>
<author>Herbert J Leass</author>
</authors>
<title>An algorithm for pronominal anaphora resolution.</title>
<date>1994</date>
<journal>Computational Linguistics,</journal>
<volume>20</volume>
<issue>4</issue>
<contexts>
<context position="16621" citStr="Lappin and Leass (1994)" startWordPosition="2688" endWordPosition="2691">. 7http://nlp.cs.berkeley.edu/ berkeleycoref.shtml 2073 System MUC B3 CEAF, Average Fernandes et al. 69.46 57.83 54.43 60.57 Martschat 66.22 55.47 51.90 57.86 StanfordSieve 64.96 54.49 51.24 56.90 Multigraph 69.13 58.61 56.06 61.28 IMSCoref 67.15 55.19 50.94 57.76 BerkeleyCoref 70.27 59.29 56.11 61.89 Table 1: Comparison of the systems with Fernandes et al. (2012) and with Martschat (2013) on CoNLL’12 English development data. For Multigraph, we modified the system described in Martschat (2013) slightly to allow for the incorporation of distance (similar to Cai and Strube (2010)). Inspired by Lappin and Leass (1994), we add salience weights for subjects and objects to the model to improve third-person pronoun resolution. We also extended the feature set by a substring feature. Furthermore, motivated by Chen and Ng (2012), we added a lexicalized feature for non-pronominal mentions that were coreferent in at least 50% of the cases in the training data. StanfordSieve was run with its standard CoNLL shared task settings. The learning-based systems were trained on the CoNLL’12 training data. We trained IMSCoref with its standard settings, and trained BerkeleyCoref with the final feature set from Durrett and K</context>
</contexts>
<marker>Lappin, Leass, 1994</marker>
<rawString>Shalom Lappin and Herbert J. Leass. 1994. An algorithm for pronominal anaphora resolution. Computational Linguistics, 20(4):535–561.</rawString>
</citation>
<citation valid="true">
<authors>
<author>Heeyoung Lee</author>
<author>Angel Chang</author>
<author>Yves Peirsman</author>
<author>Nathanael Chambers</author>
<author>Mihai Surdeanu</author>
<author>Dan Jurafsky</author>
</authors>
<title>Deterministic coreference resolution based on entity-centric, precision-ranked rules.</title>
<date>2013</date>
<journal>Computational Linguistics,</journal>
<volume>39</volume>
<issue>4</issue>
<contexts>
<context position="994" citStr="Lee et al., 2013" startWordPosition="138" endWordPosition="141">te-of-the-art English coreference resolution systems. Our analysis highlights differences between the systems and identifies that the majority of recall errors for nouns and names are shared by all systems. We characterize this set of common challenging errors in terms of a broad range of lexical and semantic properties. 1 Introduction Coreference resolution is the task of determining which mentions in a text refer to the same entity. State-of-the-art approaches include both learningbased (Fernandes et al., 2012; Bj¨orkelund and Farkas, 2012; Durrett and Klein, 2013) and deterministic models (Lee et al., 2013; Martschat, 2013). These approaches achieve state-of-the-art performance mainly relying on morphosyntactic and lexical factors. However, consider the following example. In order to improving the added value of oil products, the second phase project of the Qinghai Petroleum Bureau’s Ge’ermu oil refinery has been put into production. This will further improve the factory’s oil products structure. Due to the lack of any string overlap, most state-of-the-art systems will miss the link between the factory and the Qinghai Petroleum Bureau’s Ge’ermu oil refinery. The information that factory is a hy</context>
<context position="14858" citStr="Lee et al., 2013" startWordPosition="2434" endWordPosition="2437"> et al., 2012). This corpus contains 343 documents, spanning seven genres: bible texts, broadcast conversation, broadcast news, magazine texts, news wire, telephone conversations and web logs. 4.2 Systems State-of-the-art approaches to coreference resolution encompass various paradigms, ranging from deterministic pairwise systems to learning-based structured prediction models. Hence, we want to conduct our analysis on a representative sample of the state of the art, which should be publicly available. Therefore, we decided on two deterministic and two learning-based systems: • StanfordSieve4 (Lee et al., 2013) was the winning system of the CoNLL’11 shared task. It employs a multi-sieve approach by making more confident decisions first. • Multigraph5 (Martschat, 2013) is a deterministic pairwise system which is based on Martschat et al. (2012), the second-ranking system in the English track of the CoNLL’12 shared task. It uses a subset of features as hard constraints and chooses an antecedent for a mention by summing up the remaining boolean features. • IMSCoref6 (Bj¨orkelund and Farkas, 2012) ranked second overall in the CoNLL’12 shared task (third for English). It stacks multiple decoders and reli</context>
</contexts>
<marker>Lee, Chang, Peirsman, Chambers, Surdeanu, Jurafsky, 2013</marker>
<rawString>Heeyoung Lee, Angel Chang, Yves Peirsman, Nathanael Chambers, Mihai Surdeanu, and Dan Jurafsky. 2013. Deterministic coreference resolution based on entity-centric, precision-ranked rules. Computational Linguistics, 39(4):885–916.</rawString>
</citation>
<citation valid="true">
<authors>
<author>Xiaoqiang Luo</author>
</authors>
<title>On coreference resolution performance metrics.</title>
<date>2005</date>
<booktitle>In Proceedings of the Human Language Technology Conference and the 2005 Conference on Empirical Methods in Natural Language Processing,</booktitle>
<pages>25--32</pages>
<location>Vancouver, B.C.,</location>
<contexts>
<context position="37170" citStr="Luo, 2005" startWordPosition="6065" endWordPosition="6066">r analysis complements insights obtained via evaluation metrics. We follow Chen and Ng (2013) and distinguish between linguistically agnostic metrics, which do not employ linguistic information during scoring, and linguistically informed metrics, which employ linguistic information similar as we do when computing spanning trees. We limit the discussion of linguistically agnostic metrics to the three most popular evaluation metrics whose average constitutes the official score in the CoNLL shared tasks on coreference resolution: MUC (Vilain et al., 1995), B3 (Bagga and Baldwin, 1998) and CEAFe (Luo, 2005).10 Our framework bears most similarities to the MUC metric, as both are based on the same linkbased entity representation. In particular, when we divide the number of errors extracted from an entity by the size of a spanning tree for that entity, we obtain a score linearly related11 to the MUC score for that entity (recall for reference entities, precision for system entities). B3 and CEAFe are not founded on a link-based structure. B3 computes recall by computing the relative overlap of reference and system entity for each reference mention, and then normalizes by the number of mentions. CEA</context>
</contexts>
<marker>Luo, 2005</marker>
<rawString>Xiaoqiang Luo. 2005. On coreference resolution performance metrics. In Proceedings of the Human Language Technology Conference and the 2005 Conference on Empirical Methods in Natural Language Processing, Vancouver, B.C., Canada, 6–8 October 2005, pages 25–32.</rawString>
</citation>
<citation valid="true">
<authors>
<author>Sebastian Martschat</author>
<author>Jie Cai</author>
<author>Samuel Broscheit</author>
<author>´Eva M´ujdricza-Maydt</author>
<author>Michael Strube</author>
</authors>
<title>A multigraph model for coreference resolution.</title>
<date>2012</date>
<booktitle>In Proceedings of the Shared Task of the 16th Conference on Computational Natural Language Learning,</booktitle>
<pages>100--106</pages>
<location>Jeju Island,</location>
<marker>Martschat, Cai, Broscheit, M´ujdricza-Maydt, Strube, 2012</marker>
<rawString>Sebastian Martschat, Jie Cai, Samuel Broscheit, ´Eva M´ujdricza-Maydt, and Michael Strube. 2012. A multigraph model for coreference resolution. In Proceedings of the Shared Task of the 16th Conference on Computational Natural Language Learning, Jeju Island, Korea, 12–14 July 2012, pages 100–106.</rawString>
</citation>
<citation valid="true">
<authors>
<author>Sebastian Martschat</author>
</authors>
<title>Multigraph clustering for unsupervised coreference resolution.</title>
<date>2013</date>
<booktitle>In 51st Annual Meeting of the Association for Computational Linguistics: Proceedings of the Student Research Workshop,</booktitle>
<pages>81--88</pages>
<location>Sofia, Bulgaria, 5–7</location>
<contexts>
<context position="1012" citStr="Martschat, 2013" startWordPosition="142" endWordPosition="143">ish coreference resolution systems. Our analysis highlights differences between the systems and identifies that the majority of recall errors for nouns and names are shared by all systems. We characterize this set of common challenging errors in terms of a broad range of lexical and semantic properties. 1 Introduction Coreference resolution is the task of determining which mentions in a text refer to the same entity. State-of-the-art approaches include both learningbased (Fernandes et al., 2012; Bj¨orkelund and Farkas, 2012; Durrett and Klein, 2013) and deterministic models (Lee et al., 2013; Martschat, 2013). These approaches achieve state-of-the-art performance mainly relying on morphosyntactic and lexical factors. However, consider the following example. In order to improving the added value of oil products, the second phase project of the Qinghai Petroleum Bureau’s Ge’ermu oil refinery has been put into production. This will further improve the factory’s oil products structure. Due to the lack of any string overlap, most state-of-the-art systems will miss the link between the factory and the Qinghai Petroleum Bureau’s Ge’ermu oil refinery. The information that factory is a hypernym of refinery</context>
<context position="2266" citStr="Martschat, 2013" startWordPosition="333" endWordPosition="334">ch links. The aim of this paper is to quantify and characterize such recall errors made by state-of-theart coreference resolution systems. By doing so, we provide a solid foundation for work on employing knowledge sources for improving recall for coreference resolution (Ponzetto and Strube, 2006; Rahman and Ng, 2011; Ratinov and Roth, 2012; Bansal and Klein, 2012, inter alia). In particular, we make the following contributions: We present a novel framework for coreference resolution error analysis. This yields a formal foundation for previous work on link-based error analysis (Uryupina, 2008; Martschat, 2013) and complements work on transformation-based error analysis (Kummerfeld and Klein, 2013). We apply the method proposed in this paper to perform a recall error analysis of four state-ofthe-art systems, encompassing deterministic and learning-based approaches. In particular, we identify and characterize a set of challenging errors common to all systems, and discuss strengths and weaknesses of each system regarding specific error types. We also present a brief precision error analysis. A toolkit which implements the framework proposed in this paper is available for download.1 2 A Link-Based Anal</context>
<context position="6006" citStr="Martschat (2013)" startWordPosition="910" endWordPosition="911"> This representation was utilized for error analysis by Kummerfeld and Klein (2013), who extract errors by transforming reference into system entities. In this set-based representation, we can only extract whether two mentions corefer at all. More fine-grained information, for example about antecedent information, is not accessible. We therefore propose to employ a structured entity representation, which explicitly models links established by the coreference relation between mentions. This leads to a link-based error representation which formalizes the methods presented in Uryupina (2008) and Martschat (2013). We employ for representation a complete onedirectional graph. That is, we represent an entity e over mentions {m1, ... , mn} as a graph e = (N, A), where N = {m1,... , mn} and A = {(mk, mj) |k &gt; j}. The indices respect the mention ordering. Mentions earlier in the text have a lower index. An example graph for an entity over four mentions m1, ... , m4 (such as the BARACK OBAMA entity) is depicted in Figure 1a. In this graph, we express all coreference relations between all pairs of mentions.2 Using this representation, we can represent a set of entities as a set of graphs. In particular, give</context>
<context position="11969" citStr="Martschat (2013)" startWordPosition="1987" endWordPosition="1988">sidering the first mention (with respect to textual order) in a subentity as the source of an edge to be added. This makes sense since all other mentions in that subentity were correctly resolved to be coreferent with some preceding mention. We still have to decide on the target of the edge. In Figure 1c, we have two choices for edges: (m3, m1) and (m3, m2). We now present two methods for choosing edges. 2072 Choosing Edges by Distance. The most straight-forward way to decide on an edge is to take the edge with smallest mention distance between source and target. This is the approach taken by Martschat (2013). Choosing Edges by Accessibility. However, the distance-based approach may lead to unintuitive results. Let us consider again the BARACK OBAMA example from Figure 1. When choosing edges by distance, we would extract the error (the president, he). However, such links with a nonpronominal anaphor and a pronominal antecedent are difficult to process and considered unreliable (Ng and Cardie, 2002; Bengtson and Roth, 2008). On the other hand, the missed link (the president, Obama) constitutes a well-defined hyponymy relation which can be found in knowledge bases and is easily interpretable by huma</context>
<context position="15018" citStr="Martschat, 2013" startWordPosition="2460" endWordPosition="2461">one conversations and web logs. 4.2 Systems State-of-the-art approaches to coreference resolution encompass various paradigms, ranging from deterministic pairwise systems to learning-based structured prediction models. Hence, we want to conduct our analysis on a representative sample of the state of the art, which should be publicly available. Therefore, we decided on two deterministic and two learning-based systems: • StanfordSieve4 (Lee et al., 2013) was the winning system of the CoNLL’11 shared task. It employs a multi-sieve approach by making more confident decisions first. • Multigraph5 (Martschat, 2013) is a deterministic pairwise system which is based on Martschat et al. (2012), the second-ranking system in the English track of the CoNLL’12 shared task. It uses a subset of features as hard constraints and chooses an antecedent for a mention by summing up the remaining boolean features. • IMSCoref6 (Bj¨orkelund and Farkas, 2012) ranked second overall in the CoNLL’12 shared task (third for English). It stacks multiple decoders and relies on a combination of standard pairwise and lexicalized features. • BerkeleyCoref7 (Durrett and Klein, 2013) is a state-of-the-art system that uses mainly lexi</context>
<context position="16390" citStr="Martschat (2013)" startWordPosition="2654" endWordPosition="2655">ailable at http://nlp. stanford.edu/software/corenlp.shtml. We use version 3.4. 5http://smartschat.de/software 6http://www.ims.uni-stuttgart.de/ forschung/ressourcen/werkzeuge/IMSCoref. en.html . We use the CoNLL 2012 system. 7http://nlp.cs.berkeley.edu/ berkeleycoref.shtml 2073 System MUC B3 CEAF, Average Fernandes et al. 69.46 57.83 54.43 60.57 Martschat 66.22 55.47 51.90 57.86 StanfordSieve 64.96 54.49 51.24 56.90 Multigraph 69.13 58.61 56.06 61.28 IMSCoref 67.15 55.19 50.94 57.76 BerkeleyCoref 70.27 59.29 56.11 61.89 Table 1: Comparison of the systems with Fernandes et al. (2012) and with Martschat (2013) on CoNLL’12 English development data. For Multigraph, we modified the system described in Martschat (2013) slightly to allow for the incorporation of distance (similar to Cai and Strube (2010)). Inspired by Lappin and Leass (1994), we add salience weights for subjects and objects to the model to improve third-person pronoun resolution. We also extended the feature set by a substring feature. Furthermore, motivated by Chen and Ng (2012), we added a lexicalized feature for non-pronominal mentions that were coreferent in at least 50% of the cases in the training data. StanfordSieve was run with </context>
<context position="35561" citStr="Martschat (2013)" startWordPosition="5828" endWordPosition="5829">m our recall error analysis. Third, the fraction of common errors is very low, which indicates that precisions errors stem from various sources, which are handled differently by each system. 6 Related Work We now discuss related work in coreference resolution error analysis and in the related field of coreference resolution evaluation metrics. Error Analysis. While many papers on coreference resolution briefly discuss errors made and resolved by the system under consideration, only few concentrate on error analysis. Uryupina (2008) presents a manual error analysis on the small MUC-7 test set; Martschat (2013) performs an automatic coarse-grained error classification on CoNLL data. By extending and formalizing the approach of Martschat (2013), we are able to perform a large-scale investigation of recall errors made by state-of-the-art systems. 2078 Kummerfeld and Klein (2013) devise a method to extract errors from transformations of reference to system entities. They apply this method to a variety of systems and aggregate errors over these systems. By aggregating, they are not able to analyze differences. They furthermore focus on describing many different error classes, instead of closely investig</context>
</contexts>
<marker>Martschat, 2013</marker>
<rawString>Sebastian Martschat. 2013. Multigraph clustering for unsupervised coreference resolution. In 51st Annual Meeting of the Association for Computational Linguistics: Proceedings of the Student Research Workshop, Sofia, Bulgaria, 5–7 August 2013, pages 81– 88.</rawString>
</citation>
<citation valid="true">
<authors>
<author>Alexis Mitchell</author>
<author>Stephanie Strassel</author>
<author>Shudong Huang</author>
<author>Ramez Zakhary</author>
</authors>
<title>multilingual training corpus. LDC2005T09,</title>
<date>2004</date>
<publisher>ACE</publisher>
<institution>Linguistic Data Consortium.</institution>
<location>Philadelphia, Penn.:</location>
<contexts>
<context position="4513" citStr="Mitchell et al., 2004" startWordPosition="688" endWordPosition="691">or r. (c) (d) M1 M1 M4 M2 M4 M2 M3 M3 his he (a) Obama M1 M4 M2 M3 (b) M1 M4 M2 n3 M3 n1 n2 call error: is it missing the link between the president and Obama? Can the error be attributed to deficiencies in pronoun resolution? Linguistically motivated error representations would facilitate both understanding of current challenges and make system development faster and easier. The aim of this section is to devise such representations. 2.2 Formalizing Coreference Resolution To start with, we give a formal description of the coreference resolution task following the terminology used for the ACE (Mitchell et al., 2004) and OntoNotes (Weischedel et al., 2013) projects. A mention is a linguistic realization of a reference to an entity. Two mentions corefer if they refer to the same entity. Hence, coreference is reflexive, symmetric and transitive, and therefore an equivalence relation. The task of coreference resolution is to predict equivalence classes of mentions in a document according to the coreference relation. In order to extract errors, we need to compare the reference equivalence classes, given by the annotation, with the system equivalence classes obtained from system output. The key question now is</context>
</contexts>
<marker>Mitchell, Strassel, Huang, Zakhary, 2004</marker>
<rawString>Alexis Mitchell, Stephanie Strassel, Shudong Huang, and Ramez Zakhary. 2004. ACE 2004 multilingual training corpus. LDC2005T09, Philadelphia, Penn.: Linguistic Data Consortium.</rawString>
</citation>
<citation valid="true">
<authors>
<author>Vincent Ng</author>
<author>Claire Cardie</author>
</authors>
<title>Improving machine learning approaches to coreference resolution.</title>
<date>2002</date>
<booktitle>In Proceedings of the 40th Annual Meeting of the Association for Computational Linguistics,</booktitle>
<pages>104--111</pages>
<location>Philadelphia, Penn., 7–12</location>
<contexts>
<context position="12365" citStr="Ng and Cardie, 2002" startWordPosition="2045" endWordPosition="2048">hoosing edges. 2072 Choosing Edges by Distance. The most straight-forward way to decide on an edge is to take the edge with smallest mention distance between source and target. This is the approach taken by Martschat (2013). Choosing Edges by Accessibility. However, the distance-based approach may lead to unintuitive results. Let us consider again the BARACK OBAMA example from Figure 1. When choosing edges by distance, we would extract the error (the president, he). However, such links with a nonpronominal anaphor and a pronominal antecedent are difficult to process and considered unreliable (Ng and Cardie, 2002; Bengtson and Roth, 2008). On the other hand, the missed link (the president, Obama) constitutes a well-defined hyponymy relation which can be found in knowledge bases and is easily interpretable by humans. Uryupina (Uryupina, 2007; Uryupina, 2008) presents a recall error analysis where she takes the “intuitively easiest” missing link to analyze (Uryupina, 2007, p. 196). How can we formalize such an intuition? We will employ a notion grounded in accessibility theory (Ariel, 1988). Names and nouns refer to less accessible entities than pronouns do. For such anaphors, we prefer descriptive (nam</context>
<context position="13893" citStr="Ng and Cardie (2002)" startWordPosition="2290" endWordPosition="2293">common noun. If no such mention exists, choose the closest preceding mention. Applied to the example from Figure 1, this algorithm extracts the error (the president, Obama).3 3.2 Precision Errors Virtually all approaches to coreference resolution obtain entities by outputting pairs of anaphor and antecedent, subject to the constraint that one anaphor has at most one antecedent. We use this information to build spanning trees for system entities: these spanning trees consist of exactly the edges which correspond to anaphor/antecedent pairs in the system output. 3A similar procedure was used by Ng and Cardie (2002) to extract meaningful antecedents when training a coreference resolution system. 4 Data and Systems We now discuss data and coreference resolution systems which we will employ for our analysis. 4.1 Data We analyze the errors of the systems on the English development data of the CoNLL’12 shared task on multilingual coreference resolution (Pradhan et al., 2012). This corpus contains 343 documents, spanning seven genres: bible texts, broadcast conversation, broadcast news, magazine texts, news wire, telephone conversations and web logs. 4.2 Systems State-of-the-art approaches to coreference reso</context>
</contexts>
<marker>Ng, Cardie, 2002</marker>
<rawString>Vincent Ng and Claire Cardie. 2002. Improving machine learning approaches to coreference resolution. In Proceedings of the 40th Annual Meeting of the Association for Computational Linguistics, Philadelphia, Penn., 7–12 July 2002, pages 104–111.</rawString>
</citation>
<citation valid="true">
<authors>
<author>Vincent Ng</author>
</authors>
<title>Supervised noun phrase coreference research: The first fifteen years.</title>
<date>2010</date>
<booktitle>In Proceedings of the 48th Annual Meeting of the Association for Computational Linguistics,</booktitle>
<pages>1396--1411</pages>
<location>Uppsala,</location>
<contexts>
<context position="18160" citStr="Ng, 2010" startWordPosition="2935" endWordPosition="2936"> best according to all metrics, followed by Multigraph. StanfordSieve is the worst performing system: the gap to BerkeleyCoref is five points in average score. 4.3 Discussion Although we analyze recent systems on a recently published coreference data set, we believe that the results of our analysis will have implications for coreference in general. The data set is the largest and most genre-diverse coreference corpus so far. The systems we investigate represent major directions in coreference resolution model research, and make use of large and diverse feature sets proposed in the literature (Ng, 2010). 5 A Comparative Analysis The coreference resolution systems presented in the previous section are a representative sample of the state of the art. Therefore, by analyzing the errors they make, we can learn about remaining challenges in coreference resolution and analyze the qualitative differences between the systems. The results of such an analysis will deepen our understanding of coreference resolution and will suggest promising directions for further research. 5.1 Experimental Settings Previous studies identified the presence of recall errors as a main bottleneck for improving performance</context>
</contexts>
<marker>Ng, 2010</marker>
<rawString>Vincent Ng. 2010. Supervised noun phrase coreference research: The first fifteen years. In Proceedings of the 48th Annual Meeting of the Association for Computational Linguistics, Uppsala, Sweden, 11–16 July 2010, pages 1396–1411.</rawString>
</citation>
<citation valid="true">
<authors>
<author>Simone Paolo Ponzetto</author>
<author>Michael Strube</author>
</authors>
<title>Exploiting semantic role labeling, WordNet and Wikipedia for coreference resolution.</title>
<date>2006</date>
<booktitle>In Proceedings of the Human Language Technology Conference of the North American Chapter of the Association for Computational Linguistics,</booktitle>
<pages>192--199</pages>
<location>New York, N.Y., 4–9</location>
<contexts>
<context position="1946" citStr="Ponzetto and Strube, 2006" startWordPosition="283" endWordPosition="286">uction. This will further improve the factory’s oil products structure. Due to the lack of any string overlap, most state-of-the-art systems will miss the link between the factory and the Qinghai Petroleum Bureau’s Ge’ermu oil refinery. The information that factory is a hypernym of refinery, however, may be useful to resolve such links. The aim of this paper is to quantify and characterize such recall errors made by state-of-theart coreference resolution systems. By doing so, we provide a solid foundation for work on employing knowledge sources for improving recall for coreference resolution (Ponzetto and Strube, 2006; Rahman and Ng, 2011; Ratinov and Roth, 2012; Bansal and Klein, 2012, inter alia). In particular, we make the following contributions: We present a novel framework for coreference resolution error analysis. This yields a formal foundation for previous work on link-based error analysis (Uryupina, 2008; Martschat, 2013) and complements work on transformation-based error analysis (Kummerfeld and Klein, 2013). We apply the method proposed in this paper to perform a recall error analysis of four state-ofthe-art systems, encompassing deterministic and learning-based approaches. In particular, we id</context>
</contexts>
<marker>Ponzetto, Strube, 2006</marker>
<rawString>Simone Paolo Ponzetto and Michael Strube. 2006. Exploiting semantic role labeling, WordNet and Wikipedia for coreference resolution. In Proceedings of the Human Language Technology Conference of the North American Chapter of the Association for Computational Linguistics, New York, N.Y., 4–9 June 2006, pages 192–199.</rawString>
</citation>
<citation valid="true">
<authors>
<author>Sameer Pradhan</author>
<author>Lance Ramshaw</author>
<author>Mitchell Marcus</author>
<author>Martha Palmer</author>
<author>Ralph Weischedel</author>
<author>Nianwen Xue</author>
</authors>
<title>CoNLL-2011 Shared Task: Modeling unrestricted coreference in OntoNotes.</title>
<date>2011</date>
<booktitle>In Proceedings of the Shared Task of the 15th Conference on Computational Natural Language Learning,</booktitle>
<pages>1--27</pages>
<location>Portland, Oreg., 23–24</location>
<contexts>
<context position="18938" citStr="Pradhan et al., 2011" startWordPosition="3048" endWordPosition="3051">, by analyzing the errors they make, we can learn about remaining challenges in coreference resolution and analyze the qualitative differences between the systems. The results of such an analysis will deepen our understanding of coreference resolution and will suggest promising directions for further research. 5.1 Experimental Settings Previous studies identified the presence of recall errors as a main bottleneck for improving performance (Raghunathan et al., 2010; Durrett and Klein, 2013; Kummerfeld and Klein, 2013). This is also evidenced by the CoNLL shared tasks on coreference resolution (Pradhan et al., 2011; Pradhan et al., 2012), where most competitive systems had higher precision than recall. This indicates that an analysis of recall errors helps to understand and improve the state of the art. Hence, we focus on analyzing recall errors, and complement this by a brief analysis of precision errors. We analyze errors of the four systems presented in the previous section on the CoNLL’12 English development data. To extract recall errors we employ the spanning tree algorithm which chooses edges by accessibility. We obtain precision errors from the pairwise output of the systems. 5.2 A Recall Error </context>
</contexts>
<marker>Pradhan, Ramshaw, Marcus, Palmer, Weischedel, Xue, 2011</marker>
<rawString>Sameer Pradhan, Lance Ramshaw, Mitchell Marcus, Martha Palmer, Ralph Weischedel, and Nianwen Xue. 2011. CoNLL-2011 Shared Task: Modeling unrestricted coreference in OntoNotes. In Proceedings of the Shared Task of the 15th Conference on Computational Natural Language Learning, Portland, Oreg., 23–24 June 2011, pages 1–27.</rawString>
</citation>
<citation valid="true">
<authors>
<author>Sameer Pradhan</author>
<author>Alessandro Moschitti</author>
<author>Nianwen Xue</author>
<author>Olga Uryupina</author>
<author>Yuchen Zhang</author>
</authors>
<title>CoNLL2012 Shared Task: Modeling multilingual unrestricted coreference in OntoNotes.</title>
<date>2012</date>
<booktitle>In Proceedings of the Shared Task of the 16th Conference on Computational Natural Language Learning,</booktitle>
<pages>1--40</pages>
<location>Jeju Island,</location>
<contexts>
<context position="14255" citStr="Pradhan et al., 2012" startWordPosition="2348" endWordPosition="2352">ost one antecedent. We use this information to build spanning trees for system entities: these spanning trees consist of exactly the edges which correspond to anaphor/antecedent pairs in the system output. 3A similar procedure was used by Ng and Cardie (2002) to extract meaningful antecedents when training a coreference resolution system. 4 Data and Systems We now discuss data and coreference resolution systems which we will employ for our analysis. 4.1 Data We analyze the errors of the systems on the English development data of the CoNLL’12 shared task on multilingual coreference resolution (Pradhan et al., 2012). This corpus contains 343 documents, spanning seven genres: bible texts, broadcast conversation, broadcast news, magazine texts, news wire, telephone conversations and web logs. 4.2 Systems State-of-the-art approaches to coreference resolution encompass various paradigms, ranging from deterministic pairwise systems to learning-based structured prediction models. Hence, we want to conduct our analysis on a representative sample of the state of the art, which should be publicly available. Therefore, we decided on two deterministic and two learning-based systems: • StanfordSieve4 (Lee et al., 20</context>
<context position="18961" citStr="Pradhan et al., 2012" startWordPosition="3052" endWordPosition="3056">ors they make, we can learn about remaining challenges in coreference resolution and analyze the qualitative differences between the systems. The results of such an analysis will deepen our understanding of coreference resolution and will suggest promising directions for further research. 5.1 Experimental Settings Previous studies identified the presence of recall errors as a main bottleneck for improving performance (Raghunathan et al., 2010; Durrett and Klein, 2013; Kummerfeld and Klein, 2013). This is also evidenced by the CoNLL shared tasks on coreference resolution (Pradhan et al., 2011; Pradhan et al., 2012), where most competitive systems had higher precision than recall. This indicates that an analysis of recall errors helps to understand and improve the state of the art. Hence, we focus on analyzing recall errors, and complement this by a brief analysis of precision errors. We analyze errors of the four systems presented in the previous section on the CoNLL’12 English development data. To extract recall errors we employ the spanning tree algorithm which chooses edges by accessibility. We obtain precision errors from the pairwise output of the systems. 5.2 A Recall Error Analysis of StanfordSie</context>
</contexts>
<marker>Pradhan, Moschitti, Xue, Uryupina, Zhang, 2012</marker>
<rawString>Sameer Pradhan, Alessandro Moschitti, Nianwen Xue, Olga Uryupina, and Yuchen Zhang. 2012. CoNLL2012 Shared Task: Modeling multilingual unrestricted coreference in OntoNotes. In Proceedings of the Shared Task of the 16th Conference on Computational Natural Language Learning, Jeju Island, Korea, 12–14 July 2012, pages 1–40.</rawString>
</citation>
<citation valid="true">
<authors>
<author>Sameer Pradhan</author>
<author>Xiaoqiang Luo</author>
<author>Marta Recasens</author>
<author>Eduard Hovy</author>
<author>Vincent Ng</author>
<author>Michael Strube</author>
</authors>
<title>Scoring coreference partitions of predicted mentions: A reference implementation.</title>
<date>2014</date>
<booktitle>In Proceedings of the 52nd Annual Meeting of the Association for Computational Linguistics (Volume 2: Short Papers),</booktitle>
<pages>30--35</pages>
<location>Baltimore, Md.,</location>
<contexts>
<context position="17527" citStr="Pradhan et al., 2014" startWordPosition="2835" endWordPosition="2838">erent in at least 50% of the cases in the training data. StanfordSieve was run with its standard CoNLL shared task settings. The learning-based systems were trained on the CoNLL’12 training data. We trained IMSCoref with its standard settings, and trained BerkeleyCoref with the final feature set from Durrett and Klein (2013) for twenty iterations. We evaluate the systems on English CoNLL’12 development data and compare it with the winning system of the CoNLL’12 shared task (Fernandes et al., 2012) and with Martschat (2013) in Table 1, using the reference implementation v7 of the CoNLL scorer (Pradhan et al., 2014). BerkeleyCoref performs best according to all metrics, followed by Multigraph. StanfordSieve is the worst performing system: the gap to BerkeleyCoref is five points in average score. 4.3 Discussion Although we analyze recent systems on a recently published coreference data set, we believe that the results of our analysis will have implications for coreference in general. The data set is the largest and most genre-diverse coreference corpus so far. The systems we investigate represent major directions in coreference resolution model research, and make use of large and diverse feature sets prop</context>
</contexts>
<marker>Pradhan, Luo, Recasens, Hovy, Ng, Strube, 2014</marker>
<rawString>Sameer Pradhan, Xiaoqiang Luo, Marta Recasens, Eduard Hovy, Vincent Ng, and Michael Strube. 2014. Scoring coreference partitions of predicted mentions: A reference implementation. In Proceedings of the 52nd Annual Meeting of the Association for Computational Linguistics (Volume 2: Short Papers), Baltimore, Md., 22–27 June 2014, pages 30– 35.</rawString>
</citation>
<citation valid="true">
<authors>
<author>Karthik Raghunathan</author>
<author>Heeyoung Lee</author>
<author>Sudarshan Rangarajan</author>
<author>Nathanael Chambers</author>
<author>Mihai Surdeanu</author>
<author>Dan Jurafsky</author>
<author>Christopher Manning</author>
</authors>
<title>A multipass sieve for coreference resolution.</title>
<date>2010</date>
<booktitle>In Proceedings of the 2010 Conference on Empirical Methods in Natural Language Processing,</booktitle>
<pages>492--501</pages>
<location>Cambridge, Mass., 9–11</location>
<contexts>
<context position="18786" citStr="Raghunathan et al., 2010" startWordPosition="3024" endWordPosition="3027">5 A Comparative Analysis The coreference resolution systems presented in the previous section are a representative sample of the state of the art. Therefore, by analyzing the errors they make, we can learn about remaining challenges in coreference resolution and analyze the qualitative differences between the systems. The results of such an analysis will deepen our understanding of coreference resolution and will suggest promising directions for further research. 5.1 Experimental Settings Previous studies identified the presence of recall errors as a main bottleneck for improving performance (Raghunathan et al., 2010; Durrett and Klein, 2013; Kummerfeld and Klein, 2013). This is also evidenced by the CoNLL shared tasks on coreference resolution (Pradhan et al., 2011; Pradhan et al., 2012), where most competitive systems had higher precision than recall. This indicates that an analysis of recall errors helps to understand and improve the state of the art. Hence, we focus on analyzing recall errors, and complement this by a brief analysis of precision errors. We analyze errors of the four systems presented in the previous section on the CoNLL’12 English development data. To extract recall errors we employ t</context>
</contexts>
<marker>Raghunathan, Lee, Rangarajan, Chambers, Surdeanu, Jurafsky, Manning, 2010</marker>
<rawString>Karthik Raghunathan, Heeyoung Lee, Sudarshan Rangarajan, Nathanael Chambers, Mihai Surdeanu, Dan Jurafsky, and Christopher Manning. 2010. A multipass sieve for coreference resolution. In Proceedings of the 2010 Conference on Empirical Methods in Natural Language Processing, Cambridge, Mass., 9–11 October 2010, pages 492–501.</rawString>
</citation>
<citation valid="true">
<authors>
<author>Altaf Rahman</author>
<author>Vincent Ng</author>
</authors>
<title>Coreference resolution with world knowledge.</title>
<date>2011</date>
<booktitle>In Proceedings of the 49th Annual Meeting of the Association for Computational Linguistics (Volume 1: Long Papers),</booktitle>
<pages>814--824</pages>
<location>Portland, Oreg.,</location>
<contexts>
<context position="1967" citStr="Rahman and Ng, 2011" startWordPosition="287" endWordPosition="290">mprove the factory’s oil products structure. Due to the lack of any string overlap, most state-of-the-art systems will miss the link between the factory and the Qinghai Petroleum Bureau’s Ge’ermu oil refinery. The information that factory is a hypernym of refinery, however, may be useful to resolve such links. The aim of this paper is to quantify and characterize such recall errors made by state-of-theart coreference resolution systems. By doing so, we provide a solid foundation for work on employing knowledge sources for improving recall for coreference resolution (Ponzetto and Strube, 2006; Rahman and Ng, 2011; Ratinov and Roth, 2012; Bansal and Klein, 2012, inter alia). In particular, we make the following contributions: We present a novel framework for coreference resolution error analysis. This yields a formal foundation for previous work on link-based error analysis (Uryupina, 2008; Martschat, 2013) and complements work on transformation-based error analysis (Kummerfeld and Klein, 2013). We apply the method proposed in this paper to perform a recall error analysis of four state-ofthe-art systems, encompassing deterministic and learning-based approaches. In particular, we identify and characteri</context>
<context position="29104" citStr="Rahman and Ng, 2011" startWordPosition="4750" endWordPosition="4753">ONE 7% NONE 11% DATE 5% DATE 5% Table 7: Distribution of top five named entity types of common noun-name recall errors and all possible noun-name recall errors. of the antecedent differs when we compare common errors to all possible errors. The results are shown in Table 7. Links with a proper name antecedent of type PERSON are especially difficult. They constitute 22% of the common errors, but only 18% of all possible errors. Most mentions are in a hyponymy relation, like the prime minister and Mr. Papandreou. This confirms that harnessing such relations could improve coreference resolution (Rahman and Ng, 2011; Uryupina et al., 2011). For 65 of the errors (18%) there is lexical overlap: the head of the anaphor is contained in the proper name antecedent, as in the entire park and the Ocean Park. When categorizing all common errors according to the head of the anaphor, we observe 204 different heads. 142 heads appear only once, but the top ten heads make up 88 of the 371 errors. The most frequent heads are company (15), group (12), government, country and nation (each 9). This suggests that even with few reliable hyponymy relations recall could be significantly improved. We observe similar trends whe</context>
</contexts>
<marker>Rahman, Ng, 2011</marker>
<rawString>Altaf Rahman and Vincent Ng. 2011. Coreference resolution with world knowledge. In Proceedings of the 49th Annual Meeting of the Association for Computational Linguistics (Volume 1: Long Papers), Portland, Oreg., 19–24 June 2011, pages 814–824.</rawString>
</citation>
<citation valid="true">
<authors>
<author>Lev Ratinov</author>
<author>Dan Roth</author>
</authors>
<title>Learning-based multi-sieve co-reference resolution with knowledge.</title>
<date>2012</date>
<booktitle>In Proceedings of the 2012 Conference on Empirical Methods in Natural Language Processing and Natural Language Learning,</booktitle>
<pages>1234--1244</pages>
<location>Jeju Island,</location>
<contexts>
<context position="1991" citStr="Ratinov and Roth, 2012" startWordPosition="291" endWordPosition="294">oil products structure. Due to the lack of any string overlap, most state-of-the-art systems will miss the link between the factory and the Qinghai Petroleum Bureau’s Ge’ermu oil refinery. The information that factory is a hypernym of refinery, however, may be useful to resolve such links. The aim of this paper is to quantify and characterize such recall errors made by state-of-theart coreference resolution systems. By doing so, we provide a solid foundation for work on employing knowledge sources for improving recall for coreference resolution (Ponzetto and Strube, 2006; Rahman and Ng, 2011; Ratinov and Roth, 2012; Bansal and Klein, 2012, inter alia). In particular, we make the following contributions: We present a novel framework for coreference resolution error analysis. This yields a formal foundation for previous work on link-based error analysis (Uryupina, 2008; Martschat, 2013) and complements work on transformation-based error analysis (Kummerfeld and Klein, 2013). We apply the method proposed in this paper to perform a recall error analysis of four state-ofthe-art systems, encompassing deterministic and learning-based approaches. In particular, we identify and characterize a set of challenging </context>
</contexts>
<marker>Ratinov, Roth, 2012</marker>
<rawString>Lev Ratinov and Dan Roth. 2012. Learning-based multi-sieve co-reference resolution with knowledge. In Proceedings of the 2012 Conference on Empirical Methods in Natural Language Processing and Natural Language Learning, Jeju Island, Korea, 12–14 July 2012, pages 1234–1244.</rawString>
</citation>
<citation valid="true">
<authors>
<author>Don Tuggener</author>
</authors>
<title>Coreference resolution evaluation for higher level applications.</title>
<date>2014</date>
<booktitle>In Proceedings of the 14th Conference of the European Chapter of the Association for Computational Linguistics,</booktitle>
<pages>231--235</pages>
<location>Gothenburg,</location>
<contexts>
<context position="38500" citStr="Tuggener (2014)" startWordPosition="6279" endWordPosition="6280">entities. As the metrics are not link-based, they do not provide means to extract link-based errors. We leave determining whether the framework of these metrics exhibits a useful notion of errors to future work. 10These are linguistically agnostic since they do not differ between different mention or entity types when evaluating. 11via the transformation x → 1 − x Recent work considered devising evaluation metrics which take linguistic information into account. Chen and Ng (2013) inject linguistic knowledge into existing evaluation metrics by weighting links in an entity representation graph. Tuggener (2014) devises scoring algorithms tailored for particular applications by redefining the notion of a correct link. While both of these works focus on scoring, they weight or explicitly define links in the reference and system entities, thereby they in principle allow error extraction. However, the authors do not attempt this and it is not clear whether the errors extracted that way are useful for analysis and system development. 7 Conclusions We presented a novel link-based framework for coreference resolution error analysis, which extends and complements previous work. We applied the framework to a</context>
</contexts>
<marker>Tuggener, 2014</marker>
<rawString>Don Tuggener. 2014. Coreference resolution evaluation for higher level applications. In Proceedings of the 14th Conference of the European Chapter of the Association for Computational Linguistics, Gothenburg, Sweden, 26–30 April 2014, pages 231–235.</rawString>
</citation>
<citation valid="true">
<authors>
<author>Olga Uryupina</author>
<author>Massimo Poesio</author>
<author>Claudio Giuliano</author>
<author>Kateryna Tymoshenko</author>
</authors>
<title>Disambiguation and filtering methods in using web knowledge for coreference resolution.</title>
<date>2011</date>
<booktitle>In Proceedings of the 24th International Florida Artificial Intelligence Research Society Conference,</booktitle>
<volume>18</volume>
<pages>317--322</pages>
<location>Palm Beach, USA,</location>
<contexts>
<context position="29128" citStr="Uryupina et al., 2011" startWordPosition="4754" endWordPosition="4757">5% DATE 5% Table 7: Distribution of top five named entity types of common noun-name recall errors and all possible noun-name recall errors. of the antecedent differs when we compare common errors to all possible errors. The results are shown in Table 7. Links with a proper name antecedent of type PERSON are especially difficult. They constitute 22% of the common errors, but only 18% of all possible errors. Most mentions are in a hyponymy relation, like the prime minister and Mr. Papandreou. This confirms that harnessing such relations could improve coreference resolution (Rahman and Ng, 2011; Uryupina et al., 2011). For 65 of the errors (18%) there is lexical overlap: the head of the anaphor is contained in the proper name antecedent, as in the entire park and the Ocean Park. When categorizing all common errors according to the head of the anaphor, we observe 204 different heads. 142 heads appear only once, but the top ten heads make up 88 of the 371 errors. The most frequent heads are company (15), group (12), government, country and nation (each 9). This suggests that even with few reliable hyponymy relations recall could be significantly improved. We observe similar trends when the anaphor is a prope</context>
</contexts>
<marker>Uryupina, Poesio, Giuliano, Tymoshenko, 2011</marker>
<rawString>Olga Uryupina, Massimo Poesio, Claudio Giuliano, and Kateryna Tymoshenko. 2011. Disambiguation and filtering methods in using web knowledge for coreference resolution. In Proceedings of the 24th International Florida Artificial Intelligence Research Society Conference, Palm Beach, USA, 18– 20 May 2011, pages 317–322.</rawString>
</citation>
<citation valid="true">
<authors>
<author>Olga Uryupina</author>
</authors>
<title>Knowledge acquisition for coreference resolution.</title>
<date>2007</date>
<tech>Ph.D. thesis,</tech>
<institution>Saarland University,</institution>
<location>Saarbr¨ucken, Germany.</location>
<contexts>
<context position="12597" citStr="Uryupina, 2007" startWordPosition="2083" endWordPosition="2084">es by Accessibility. However, the distance-based approach may lead to unintuitive results. Let us consider again the BARACK OBAMA example from Figure 1. When choosing edges by distance, we would extract the error (the president, he). However, such links with a nonpronominal anaphor and a pronominal antecedent are difficult to process and considered unreliable (Ng and Cardie, 2002; Bengtson and Roth, 2008). On the other hand, the missed link (the president, Obama) constitutes a well-defined hyponymy relation which can be found in knowledge bases and is easily interpretable by humans. Uryupina (Uryupina, 2007; Uryupina, 2008) presents a recall error analysis where she takes the “intuitively easiest” missing link to analyze (Uryupina, 2007, p. 196). How can we formalize such an intuition? We will employ a notion grounded in accessibility theory (Ariel, 1988). Names and nouns refer to less accessible entities than pronouns do. For such anaphors, we prefer descriptive (name/nominal) antecedents. Inspired by Ariel’s degrees of accessibility, we choose a target for a given anaphor mi as follows: • If mi is a pronoun, choose the closest preceding mention. • If mi is not a pronoun, choose the closest pre</context>
</contexts>
<marker>Uryupina, 2007</marker>
<rawString>Olga Uryupina. 2007. Knowledge acquisition for coreference resolution. Ph.D. thesis, Saarland University, Saarbr¨ucken, Germany.</rawString>
</citation>
<citation valid="true">
<authors>
<author>Olga Uryupina</author>
</authors>
<title>Error analysis for learningbased coreference resolution.</title>
<date>2008</date>
<booktitle>In Proceedings of the 6th International Conference on Language Resources and Evaluation,</booktitle>
<volume>1</volume>
<pages>1914--1919</pages>
<location>Marrakech,</location>
<contexts>
<context position="2248" citStr="Uryupina, 2008" startWordPosition="331" endWordPosition="332">ul to resolve such links. The aim of this paper is to quantify and characterize such recall errors made by state-of-theart coreference resolution systems. By doing so, we provide a solid foundation for work on employing knowledge sources for improving recall for coreference resolution (Ponzetto and Strube, 2006; Rahman and Ng, 2011; Ratinov and Roth, 2012; Bansal and Klein, 2012, inter alia). In particular, we make the following contributions: We present a novel framework for coreference resolution error analysis. This yields a formal foundation for previous work on link-based error analysis (Uryupina, 2008; Martschat, 2013) and complements work on transformation-based error analysis (Kummerfeld and Klein, 2013). We apply the method proposed in this paper to perform a recall error analysis of four state-ofthe-art systems, encompassing deterministic and learning-based approaches. In particular, we identify and characterize a set of challenging errors common to all systems, and discuss strengths and weaknesses of each system regarding specific error types. We also present a brief precision error analysis. A toolkit which implements the framework proposed in this paper is available for download.1 2</context>
<context position="5985" citStr="Uryupina (2008)" startWordPosition="907" endWordPosition="908">s a set of mentions. This representation was utilized for error analysis by Kummerfeld and Klein (2013), who extract errors by transforming reference into system entities. In this set-based representation, we can only extract whether two mentions corefer at all. More fine-grained information, for example about antecedent information, is not accessible. We therefore propose to employ a structured entity representation, which explicitly models links established by the coreference relation between mentions. This leads to a link-based error representation which formalizes the methods presented in Uryupina (2008) and Martschat (2013). We employ for representation a complete onedirectional graph. That is, we represent an entity e over mentions {m1, ... , mn} as a graph e = (N, A), where N = {m1,... , mn} and A = {(mk, mj) |k &gt; j}. The indices respect the mention ordering. Mentions earlier in the text have a lower index. An example graph for an entity over four mentions m1, ... , m4 (such as the BARACK OBAMA entity) is depicted in Figure 1a. In this graph, we express all coreference relations between all pairs of mentions.2 Using this representation, we can represent a set of entities as a set of graphs</context>
<context position="12614" citStr="Uryupina, 2008" startWordPosition="2085" endWordPosition="2086">ity. However, the distance-based approach may lead to unintuitive results. Let us consider again the BARACK OBAMA example from Figure 1. When choosing edges by distance, we would extract the error (the president, he). However, such links with a nonpronominal anaphor and a pronominal antecedent are difficult to process and considered unreliable (Ng and Cardie, 2002; Bengtson and Roth, 2008). On the other hand, the missed link (the president, Obama) constitutes a well-defined hyponymy relation which can be found in knowledge bases and is easily interpretable by humans. Uryupina (Uryupina, 2007; Uryupina, 2008) presents a recall error analysis where she takes the “intuitively easiest” missing link to analyze (Uryupina, 2007, p. 196). How can we formalize such an intuition? We will employ a notion grounded in accessibility theory (Ariel, 1988). Names and nouns refer to less accessible entities than pronouns do. For such anaphors, we prefer descriptive (name/nominal) antecedents. Inspired by Ariel’s degrees of accessibility, we choose a target for a given anaphor mi as follows: • If mi is a pronoun, choose the closest preceding mention. • If mi is not a pronoun, choose the closest preceding proper nam</context>
<context position="35482" citStr="Uryupina (2008)" startWordPosition="5815" endWordPosition="5816">only attempt few decisions, most of them are wrong. This confirms findings from our recall error analysis. Third, the fraction of common errors is very low, which indicates that precisions errors stem from various sources, which are handled differently by each system. 6 Related Work We now discuss related work in coreference resolution error analysis and in the related field of coreference resolution evaluation metrics. Error Analysis. While many papers on coreference resolution briefly discuss errors made and resolved by the system under consideration, only few concentrate on error analysis. Uryupina (2008) presents a manual error analysis on the small MUC-7 test set; Martschat (2013) performs an automatic coarse-grained error classification on CoNLL data. By extending and formalizing the approach of Martschat (2013), we are able to perform a large-scale investigation of recall errors made by state-of-the-art systems. 2078 Kummerfeld and Klein (2013) devise a method to extract errors from transformations of reference to system entities. They apply this method to a variety of systems and aggregate errors over these systems. By aggregating, they are not able to analyze differences. They furthermor</context>
</contexts>
<marker>Uryupina, 2008</marker>
<rawString>Olga Uryupina. 2008. Error analysis for learningbased coreference resolution. In Proceedings of the 6th International Conference on Language Resources and Evaluation, Marrakech, Morocco, 26 May – 1 June 2008, pages 1914–1919.</rawString>
</citation>
<citation valid="true">
<authors>
<author>Renata Vieira</author>
<author>Massimo Poesio</author>
</authors>
<title>An empirically-based system for processing definite descriptions.</title>
<date>2000</date>
<journal>Computational Linguistics,</journal>
<volume>26</volume>
<issue>4</issue>
<pages>593</pages>
<contexts>
<context position="31095" citStr="Vieira and Poesio, 2000" startWordPosition="5093" endWordPosition="5096">ether the head matches between the mentions or not. In 341 cases the heads match. For many of these cases, parsing errors propagate and prevent the systems from recognizing the correct mention boundaries. In order to get a better understanding of the errors for nouns with different heads, we randomly extracted 50 of the 494 pairs and investigated the relation that holds between the heads. In 23 cases, the heads were related via hyponymy. In 10 cases they were synonyms. The remaining 17 cases involve many different phenomena, for example meronymy. This confirms findings from previous research (Vieira and Poesio, 2000). Hence, looking up lexical relations, especially hyponymy, might be helpful to solve these cases. 5.4.2 Differences between the Systems In order to analyze differences between the systems, we compare the recall errors they make. The information how recall errors differ between systems will enable us to understand individual strengths and weaknesses. Exemplarily, we will have a look at the differences in the errors when the anaphor is a common noun and the antecedent is a proper name. By system design and by the total error numbers (Table 4) we expect the learning-based systems to have a sligh</context>
</contexts>
<marker>Vieira, Poesio, 2000</marker>
<rawString>Renata Vieira and Massimo Poesio. 2000. An empirically-based system for processing definite descriptions. Computational Linguistics, 26(4):539– 593.</rawString>
</citation>
<citation valid="true">
<authors>
<author>Marc Vilain</author>
<author>John Burger</author>
<author>John Aberdeen</author>
<author>Dennis Connolly</author>
<author>Lynette Hirschman</author>
</authors>
<title>A modeltheoretic coreference scoring scheme.</title>
<date>1995</date>
<booktitle>In Proceedings of the 6th Message Understanding Conference (MUC-6),</booktitle>
<pages>45--52</pages>
<publisher>Morgan Kaufmann.</publisher>
<location>San Mateo, Cal.</location>
<contexts>
<context position="37118" citStr="Vilain et al., 1995" startWordPosition="6054" endWordPosition="6057">o means of extracting recall or precision errors. Therefore our analysis complements insights obtained via evaluation metrics. We follow Chen and Ng (2013) and distinguish between linguistically agnostic metrics, which do not employ linguistic information during scoring, and linguistically informed metrics, which employ linguistic information similar as we do when computing spanning trees. We limit the discussion of linguistically agnostic metrics to the three most popular evaluation metrics whose average constitutes the official score in the CoNLL shared tasks on coreference resolution: MUC (Vilain et al., 1995), B3 (Bagga and Baldwin, 1998) and CEAFe (Luo, 2005).10 Our framework bears most similarities to the MUC metric, as both are based on the same linkbased entity representation. In particular, when we divide the number of errors extracted from an entity by the size of a spanning tree for that entity, we obtain a score linearly related11 to the MUC score for that entity (recall for reference entities, precision for system entities). B3 and CEAFe are not founded on a link-based structure. B3 computes recall by computing the relative overlap of reference and system entity for each reference mention</context>
</contexts>
<marker>Vilain, Burger, Aberdeen, Connolly, Hirschman, 1995</marker>
<rawString>Marc Vilain, John Burger, John Aberdeen, Dennis Connolly, and Lynette Hirschman. 1995. A modeltheoretic coreference scoring scheme. In Proceedings of the 6th Message Understanding Conference (MUC-6), pages 45–52, San Mateo, Cal. Morgan Kaufmann.</rawString>
</citation>
<citation valid="true">
<authors>
<author>Ralph Weischedel</author>
<author>Martha Palmer</author>
<author>Mitchell Marcus</author>
<author>Eduard Hovy</author>
<author>Sameer Pradhan</author>
<author>Lance Ramshaw</author>
</authors>
<date>2013</date>
<booktitle>OntoNotes release 5.0. LDC2013T19,</booktitle>
<institution>Linguistic Data Consortium.</institution>
<location>Nianwen Xue, Ann Taylor, Jeff Kaufman, Michelle Franchini, Mohammed El-Bachouti, Robert Belvin, and Ann Houston.</location>
<contexts>
<context position="4553" citStr="Weischedel et al., 2013" startWordPosition="694" endWordPosition="697">his he (a) Obama M1 M4 M2 M3 (b) M1 M4 M2 n3 M3 n1 n2 call error: is it missing the link between the president and Obama? Can the error be attributed to deficiencies in pronoun resolution? Linguistically motivated error representations would facilitate both understanding of current challenges and make system development faster and easier. The aim of this section is to devise such representations. 2.2 Formalizing Coreference Resolution To start with, we give a formal description of the coreference resolution task following the terminology used for the ACE (Mitchell et al., 2004) and OntoNotes (Weischedel et al., 2013) projects. A mention is a linguistic realization of a reference to an entity. Two mentions corefer if they refer to the same entity. Hence, coreference is reflexive, symmetric and transitive, and therefore an equivalence relation. The task of coreference resolution is to predict equivalence classes of mentions in a document according to the coreference relation. In order to extract errors, we need to compare the reference equivalence classes, given by the annotation, with the system equivalence classes obtained from system output. The key question now is how we represent these equivalence clas</context>
</contexts>
<marker>Weischedel, Palmer, Marcus, Hovy, Pradhan, Ramshaw, 2013</marker>
<rawString>Ralph Weischedel, Martha Palmer, Mitchell Marcus, Eduard Hovy, Sameer Pradhan, Lance Ramshaw, Nianwen Xue, Ann Taylor, Jeff Kaufman, Michelle Franchini, Mohammed El-Bachouti, Robert Belvin, and Ann Houston. 2013. OntoNotes release 5.0. LDC2013T19, Philadelphia, Penn.: Linguistic Data Consortium.</rawString>
</citation>
</citationList>
</algorithm>
</algorithms>
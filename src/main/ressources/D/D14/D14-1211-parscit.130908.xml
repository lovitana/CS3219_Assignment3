<?xml version="1.0" encoding="UTF-8"?>
<algorithms version="110505">
<algorithm name="SectLabel" version="110505">
<variant no="0" confidence="0.001110">
<title confidence="0.898657">
Gender and Power:
How Gender and Gender Environment Affect Manifestations of Power
</title>
<author confidence="0.995587">
Vinodkumar Prabhakaran
</author>
<affiliation confidence="0.9967195">
Dept. of Computer Science
Columbia University
</affiliation>
<address confidence="0.91252">
New York, NY, USA
</address>
<email confidence="0.999155">
vinod@cs.columbia.edu
</email>
<author confidence="0.975141">
Emily E. Reid
</author>
<affiliation confidence="0.9964915">
Dept. of Computer Science
Columbia University
</affiliation>
<address confidence="0.912478">
New York, NY, USA
</address>
<email confidence="0.998771">
eer2137@columbia.edu
</email>
<author confidence="0.889013">
Owen Rambow
</author>
<affiliation confidence="0.8341935">
CCLS
Columbia University
</affiliation>
<address confidence="0.909864">
New York, NY, USA
</address>
<email confidence="0.999274">
rambow@ccls.columbia.edu
</email>
<sectionHeader confidence="0.993912" genericHeader="abstract">
Abstract
</sectionHeader>
<bodyText confidence="0.999833434782609">
We investigate the interaction of power,
gender, and language use in the Enron
email corpus. We present a freely avail-
able extension to the Enron corpus, with
the gender of senders of 87% messages
reliably identified. Using this data, we
test two specific hypotheses drawn from
the sociolinguistic literature pertaining to
gender and power: women managers use
face-saving communicative strategies, and
women use language more explicitly than
men to create and maintain social rela-
tions. We introduce the notion of “gender
environment” to the computational study
of written conversations; we interpret this
notion as the gender makeup of an email
thread, and show that some manifestations
of power differ significantly between gen-
der environments. Finally, we show the
utility of gender information in the prob-
lem of automatically predicting the direc-
tion of power between pairs of participants
in email interactions.
</bodyText>
<sectionHeader confidence="0.999134" genericHeader="introduction">
1 Introduction
</sectionHeader>
<bodyText confidence="0.999810264150944">
It has long been observed that men and women
communicate differently in different contexts.
This phenomenon has been studied by sociolin-
guists, who typically rely on case studies or sur-
veys. The availability of large corpora of nat-
urally occurring social interactions has given us
the opportunity to study language use at a broader
level than before. In this paper, we use the Enron
Corpus of work-related emails to examine written
communication in a corporate setting. We inves-
tigate three factors that affect choices in commu-
nication: the writer’s gender, the gender of his or
her fellow discourse participants (what we call the
“gender environment”), and the relations of orga-
nizational power he or she has to the discourse par-
ticipants. We concentrate on modeling the writer’s
choices related to discourse structure, rather than
lexical choice. Specifically, our goal is to show
that gender, gender environment, and power all af-
fect individuals’ choices in complex ways, result-
ing in patterns in the discourse that reveal the un-
derlying factors.
This paper makes three major contributions.
First, we introduce an extension to the well-known
Enron corpus of emails: we semi-automatically
identify the sender’s gender of 87% of email mes-
sages in the corpus. This extension will be made
publicly available. Second, we use this enriched
version of the corpus to investigate the interaction
of hierarchical power and gender. We formalize
the notion of “gender environment”, which reflects
the gender makeup of the discourse participants
of a particular conversation. We study how gen-
der, power, and gender environment influence dis-
course participants’ choices in dialog. We inves-
tigate two specific hypotheses from the sociolin-
guistic literature, relating to face-saving use of lan-
guage, and to the use of language to strengthen so-
cial relations. This contribution does not exhaust
the possibilities of our corpus, but it shows how
social science can benefit from advanced natural
language processing techniques in analyzing cor-
pora, allowing social scientists to tackle corpora
such as the Enron corpus which cannot be exam-
ined in its entirety by hand. Third, we show that
the gender information in the enriched corpus can
be useful for computational tasks, specifically for
training a system that predicts the direction of hier-
archical power between participants in an interac-
tion. Our use of the gender-based features boosts
the accuracy of predicting the direction of power
between pairs of email interactants from 68.9% to
70.2% on an unseen test set.
</bodyText>
<page confidence="0.948239">
1965
</page>
<note confidence="0.899543">
Proceedings of the 2014 Conference on Empirical Methods in Natural Language Processing (EMNLP), pages 1965–1976,
October 25-29, 2014, Doha, Qatar. c�2014 Association for Computational Linguistics
</note>
<bodyText confidence="0.999173272727273">
The paper is structured as follows. We review
related work in Section 2. We present the Gender
Identified Enron Corpus (our first contribution) in
Section 3. Section 4 defines the problem of pre-
dicting power and the various dimensions of in-
teraction we analyze. We turn to our second con-
tribution, the analysis of the data, in Sections 5
and 6. Section 7 describes our third contribution,
the machine learning experiments using gender-
related features in the prediction of hierarchical
power. We then conclude and discuss future work.
</bodyText>
<sectionHeader confidence="0.999781" genericHeader="related work">
2 Related Work
</sectionHeader>
<bodyText confidence="0.998664080459771">
There is much sociolinguistic background related
to gender and language use, some of it specifically
related to language use in the work environment
(Kendall and Tannen, 1997; Holmes and Stubbe,
2003; Kendall, 2003; Herring, 2008). We do not
provide a full discussion of this work for lack of
space, but single out one paper which has partic-
ularly influenced our work. Holmes and Stubbe
(2003) provide two case studies that do not look
at the differences between male and female man-
agers’ communication, but at the difference be-
tween female managers’ communication in more
heavily female vs. more heavily male environ-
ments. They find that, while female managers tend
to break many stereotypes of “feminine” commu-
nication, they have different strategies in connect-
ing with employees and exhibiting power in the
two gender environments. This work has inspired
us to look at this phenomenon by including “Gen-
der Environment” in our study. By finding the ra-
tios of males to females on a thread, we can look at
whether indicators change within a more heavily
male or female thread. This notion of gender envi-
ronment is supported by an idea in recent Twitter-
based sociolinguistic research on gender identity
and lexical variation (Bamman et al., 2014). One
of the many insights from their work is that gen-
dered linguistic behavior is oriented by a number
of factors, one of which includes the speaker’s au-
dience. Their work looks at Twitter users whose
linguistic style fails to identify their gender in clas-
sification experiments, and finds that the linguis-
tic gender norms can be influenced by the style of
their interlocutors.
Within the NLP community, there has been
substantial research exploring language use and
power. A large number of these studies are per-
formed in the domain of organizational email
where the notion of power is well defined in terms
of organizational hierarchy. It is also aided by the
availability of the moderately large Enron email
corpus which captures email interactions in an or-
ganizational setting. Earlier approaches used sim-
ple lexical features alone (e.g. (Bramsen et al.,
2011; Gilbert, 2012)) as a means to predict power.
Later studies have used more complex linguistic
and structural features, such as formality (Peterson
et al., 2011), dialog acts (Prabhakaran and Ram-
bow, 2013), and thread structure (Prabhakaran and
Rambow, 2014). Our work is also on the Enron
email corpus, and our baseline features are derived
from some of this prior work. Researchers have
also studied power and influence in other genres
of interactions, such as online forums (Danescu-
Niculescu-Mizil et al., 2012; Biran et al., 2012),
multi-party chats (Strzalkowski et al., 2012) and
off-line interactions such as presidential debates
(Nguyen et al., 2013; Prabhakaran et al., 2013;
Prabhakaran et al., 2014).
There is also some work within the NLP field
on analyzing language use in relation to gender.
Mohammad and Yang (2011) analyzed the way
gender affects the expression of sentiments in text,
while we are interested in how gender relates to
manifestations of organizational power. For their
study, they assigned gender for the core employees
in the Enron email corpus based on whether the
first name of the person was easily gender iden-
tifiable or not. If the person had an unfamiliar
name or a name that could be of either gender,
they marked his/her gender as unknown and ex-
cluded them from their study.1 For example, the
gender of the employee Kay Mann was marked as
unknown in their gender assignment. However, in
our work, we manually research and determine the
gender of every core employee.
Researchers have also attempted to automati-
cally predict the gender of email senders using su-
pervised learning techniques based on linguistic
features (Corney et al., 2002; Cheng et al., 2011;
Deitrick et al., 2012), a task we do not address in
this paper. These studies use datasets that are rel-
atively smaller in size. Corney et al. (2002) use
around 4K emails from 325 gender identified au-
thors. Cheng et al. (2011) use around 9K emails
from 108 gender identified authors. Deitrick et al.
(2012) use around 18K emails from 144 gender
</bodyText>
<footnote confidence="0.9350885">
1http://www.saifmohammad.com/WebDocs/dir-email-
gender.txt
</footnote>
<page confidence="0.995494">
1966
</page>
<bodyText confidence="0.99975275">
identified authors. The dataset we offer is much
larger in size, with around 97K emails whose au-
thors are gender identified. We believe that our
resource will aid further research in this area.
</bodyText>
<sectionHeader confidence="0.99023" genericHeader="method">
3 Gender Identified Enron Corpus
</sectionHeader>
<subsectionHeader confidence="0.99386">
3.1 Enron Corpus
</subsectionHeader>
<bodyText confidence="0.999874128205128">
In our work, we use the version of Enron email
corpus released by Yeh and Harnly (2006). The
corpus contains emails from the mailboxes of 145
core employees who held top managerial positions
within Enron at the time of bankruptcy. Yeh and
Harnly (2006) preprocessed the corpus to combine
multiple email addresses belonging to the same
entity and identify each entity in the corpus with
a unique identifier. The corpus contains a total of
111,933 messages. This version of the corpus has
been enriched later by Agarwal et al. (2012) with
gold organizational power relations, manually de-
termined using information from Enron organiza-
tional charts. It includes relations of 1,518 em-
ployees and captures dominance relations between
13,724 pairs of them. This information enables us
to study the manifestations of power in these inter-
actions, in relation to gender.
In this version of the corpus, the thread structure
of email messages is reconstructed, with the miss-
ing messages restored from other emails in which
they were quoted. This allows us to go beyond
isolated messages and study the dialog structure
within email threads. There were 34,156 unique
discourse participants across all the email threads
present in the corpus. Manually determining the
gender of all the discourse participants in the cor-
pus is not feasible. Hence, we adopt a two-step
approach through which we reliably identify the
gender of a large majority of entities in the email
threads within the corpus. We manually deter-
mine the gender of the 145 core employees who
have a bigger representation in the corpus, and we
systemically determine the gender of the rest of
the discourse participants using the Social Secu-
rity Administration’s baby names database. We
adopt a conservative approach so that we assign
a gender only when the name of the participant
meets a very low ambiguity threshold.
</bodyText>
<subsectionHeader confidence="0.999782">
3.2 Manual Gender Assignment
</subsectionHeader>
<bodyText confidence="0.999811428571429">
We researched each of the 145 core employees us-
ing web search and found public records about
them or articles referring to them. In order to
make sure that the results are about the same per-
son we want, we added the word ‘enron’ to the
search queries. Within the public records returned
for each core employee, we looked for instances
in which they were being referred to either using a
gender revealing pronoun (he/him/his vs. she/her)
or using a gender revealing addressing form (Mr.
vs. Mrs./Ms./Miss). Since these employees held
top managerial positions within Enron at the time
of bankruptcy, it was fairly easy to find public
records or articles referring to them. For example,
the page we found for Kay Mann clearly identifies
her gender.2 We were able to correctly determine
the gender of each of the 145 core employees in
this manner. A benefit of manually determining
the gender of these core employees is that it en-
sures a high coverage of 100% confident gender
assignments in the corpus.
</bodyText>
<subsectionHeader confidence="0.999399">
3.3 Automatic Gender Assignment
</subsectionHeader>
<bodyText confidence="0.999992416666667">
As mentioned in Section 3.1, our corpus contains
a large number of discourse participants in addi-
tion to the 145 core employees for which we man-
ually identified the gender. To attempt to find
the gender of these other discourse participants,
we first determine their first names and then find
how ambiguous the names are by querying the So-
cial Security Administration’s (SSA) baby names
dataset. We first describe how we calculate an am-
biguity score for a name using the SSA dataset and
then describe how we use it to determine the gen-
der of discourse participants in our corpus.
</bodyText>
<subsectionHeader confidence="0.986295">
3.3.1 SSA Names and Gender Dataset
</subsectionHeader>
<bodyText confidence="0.996799928571429">
The US Social Security Administration maintains
a dataset of baby names, gender, and name count
for each year starting with the 1880s, for names
with at least five counts.3 We used this dataset
in order to determine the gender ambiguity of a
name. The Enron data set contains emails from
1998 to 2001. We estimate the common age range
for a large, corporate firm like Enron at 24-67,4 so
we used the SSA data from 1931-1977 to calculate
ambiguity scores for our purposes.
For each name n in the database, let mp(n)
and fp(n) denote the percentages of males and fe-
males with the name n. Then, we calculate the
ambiguity score AS(n) as 100−Jmp(n)−fp(n)J.
</bodyText>
<footnote confidence="0.99980875">
2http://www.prnewswire.com/news-releases/kay-mann-
joins-noble-as-general-counsel-57073687.html
3http://www.ssa.gov/oact/babynames/limits.html
4http://www.bls.gov/cps/demographics.htm
</footnote>
<page confidence="0.995192">
1967
</page>
<bodyText confidence="0.999873375">
The value of AS(n) varies between 0 and 100. A
name that is ‘perfectly unambiguous’ would have
an ambiguity score of 0, while a ‘perfectly am-
biguous’ name (i.e., 50%/50% split between gen-
ders) would have an ambiguity score of 100. We
assign the likely gender of the name to be the one
with the higher percentage, if the ambiguity score
is below a threshold AST.
</bodyText>
<equation confidence="0.960670333333333">
�M, if AS(n) &lt; AST and mp(n) &gt; fp(n)
G(n) = F, if AS(n) &lt; AST and mp(n) &lt; fp(n)
I, if AS(n) &gt; AST
</equation>
<bodyText confidence="0.999848">
Around 88% of the names in the SSA dataset
have AS(n) = 0. We choose a very conserva-
tive threshold of AST = 10 for our gender assign-
ments, which assigns gender to around 93% names
in the SSA dataset.5
</bodyText>
<subsectionHeader confidence="0.956715">
3.3.2 Identifying the First Name
</subsectionHeader>
<bodyText confidence="0.999948823529412">
Each discourse participant in our corpus has at
least one email address and zero or more names
associated with it. The name field is automatically
assembled by Yeh and Harnly (2006), where they
captured the different names from email headers,
which are populated from individual email clients
and do not follow a standard format. Not all dis-
course participants are human; some may refer to
organizational groups (e.g., HR Department) or
anonymous corporate email accounts (e.g., a web-
master account, do-not-reply address etc.). The
name field may sometimes be empty, contain mul-
tiple names, contain an email address, or show
other irregularities. Hence, it is nontrivial to deter-
mine the first name of our discourse participants.
We used the heuristics below to extract the most
likely first name for each discourse participant.
</bodyText>
<listItem confidence="0.851237722222222">
• If the name field contains two words, pick the
second or first word, depending on whether a
comma separates them or not.
• If the name field contains three words and a
comma, choose the second and third words
(a likely first and middle name, respectively).
If the name field contains three words but no
comma, choose the first and second words
(again, a likely first and middle name).
• If the name field contains an email address,
pick the portion from the beginning of the
string to a ‘.’,‘ ’ or ‘-’; if the email address
is in camel case, take portion from the begin-
ning of the string to the first upper case letter.
5In the corpus that will be released, we retain the AS(n)
of each name, so that the users of this resource can decide the
threshold that suit their needs.
• If the name field is empty, apply the above
</listItem>
<bodyText confidence="0.999905105263158">
rule to the email address field to pick a name.
The above heuristics create a list of candidate
names for each discourse participant which we
then query for an ambiguity score (Section 3.3.1)
and the likely gender. We find the candidate
name with the lowest ambiguity score that passes
the threshold and assign the associated gender to
the discourse participant. If none of the candi-
date names for a discourse participant passes the
threshold, we assign the gender to be ‘I’ (Indeter-
minate). We also assign the gender to be ‘I’, if
none of the candidate names is present in the SSA
dataset. This will occur if the name is a first name
that is not in the database (an unusual or interna-
tional name; e.g., Vladi), or if no true first name
was found (e.g., the name field was empty and the
email address was only a pseudonym). This will
also include most of the cases where the discourse
participant is not a human.
</bodyText>
<subsectionHeader confidence="0.909807">
3.3.3 Coverage and Accuracy
</subsectionHeader>
<bodyText confidence="0.999967611111111">
We evaluated the coverage and accuracy of our
gender assignment system on the manually as-
signed gender data of the 145 core people. We
obtained a coverage of 90.3%, i.e., for 14 of the
145 core people, the ambiguity score was higher
than the threshold. Of the 131 people the sys-
tem assigned a gender to, we obtained an accu-
racy of 89.3% in correctly identifying the gender.
We investigated the errors and found that all er-
rors were caused due to incorrectly identifying the
first name. These errors arise because the name
fields are automatically populated and sometimes
the core discourse participants’ name fields in-
clude their secretaries. While this is common for
people in higher managerial positions, we expect
this not to happen in the middle management and
below, to which most of the automatically gender-
assigned discourse participants belong.
</bodyText>
<subsectionHeader confidence="0.999888">
3.4 Corpus Statistics and Divisions
</subsectionHeader>
<bodyText confidence="0.9999964">
We apply the gender assignment system described
above to all discourse participants of all email
threads in the entire Enron corpus described in
Section 3.1. Table 1 shows the coverage of gen-
der assignment in our corpus at different lev-
els: unique discourse participants, messages and
threads. In Table 2, we show the male/female per-
centage split of all unique discourse participants,
as well as the split at the level of messages (i.e.,
messages sent by males vs. females).
</bodyText>
<page confidence="0.977695">
1968
</page>
<table confidence="0.999613125">
Count (%)
Total unique discourse participants 34,156
- gender identified 23,009 (67.3%)
Total messages 111,933
- senders gender identified 97,255 (86.9%)
Total threads 36,615
- all senders gender identified 26,015 (71.1%)
- all participants gender identified 18,030 (49.2%)
</table>
<tableCaption confidence="0.9615995">
Table 1: Coverage of Gender Identification at various level:
unique discourse participants, messages and threads
</tableCaption>
<table confidence="0.989469333333333">
Male Female
Unique Discourse Participants 66.1% 33.9%
Message Senders 58.2% 41.8%
</table>
<tableCaption confidence="0.991025333333333">
Table 2: Male/Female split across a) all unique participants
who were gender identified, b) all messages whose senders
were gender identified
</tableCaption>
<bodyText confidence="0.9996342">
We divide the entire corpus into Train, Dev and
Test sets at the thread level, through random sam-
pling, with a distribution of 50%, 25% and 25%
each. The number of threads and messages in each
subdivision is shown in Table 3.
</bodyText>
<table confidence="0.998569">
Total Train Dev Test
Threads 36,615 18,498 8,973 9,144
Messages 111,933 56,447 27,565 27,921
</table>
<tableCaption confidence="0.999935">
Table 3: Train/Test/Dev breakup of the entire corpus
</tableCaption>
<bodyText confidence="0.9999436">
We also create a sub-corpus of the threads called
All Participants Gender Identified (APGI), con-
taining the 18,030 threads for which the gender as-
signment system succeeded in assigning the gen-
ders of all participants, including senders and all
recipients (To and CC). For the analysis and ex-
periments presented in the rest of this paper, we
use 17,788 threads from this APGI subset, exclud-
ing the remaining 242 threads that were used for
previous manual annotation efforts.
</bodyText>
<sectionHeader confidence="0.917082" genericHeader="method">
4 Manifestations of Power
</sectionHeader>
<bodyText confidence="0.999961125">
We use the gender information of the participants
to investigate how the gender of the sender and
recipients affect the manifestations of hierarchical
power in interactions. In order to do this, we use
the interaction analysis framework from our prior
work (Prabhakaran and Rambow, 2014). In this
section, we give a brief overview of the problem
formulation and the structural features we used.
</bodyText>
<subsectionHeader confidence="0.994823">
4.1 Hierarchically Related Interacting Pairs
</subsectionHeader>
<bodyText confidence="0.9897548">
Let t denote an email thread and Mt denote the
set of all messages in t. Also, let Pt be the set
of all participants in t, i.e., the union of senders
and recipients (To and CC) of all messages in Mt.
We are interested in analyzing the power relations
between pairs of participants who interact within
a given email thread. Not every pair of partic-
ipants (p1, p2) E Pt x Pt interact with one an-
other within t. Let IMt(p1, p2) denote the set of
Interaction Messages — non-empty messages in
t in which either p1 is the sender and p2 is one
of the recipients or vice versa. We call the set
of (p1, p2) such that |IMt(p1, p2) |&gt; 0 the inter-
acting participant pairs of t (IPPt). For every
(p1, p2) E IPPt, we query the set of dominance
relations in the gold hierarchy and assign their hi-
erarchical power relation (HP(p1, p2)) to be su-
perior if p1 dominates p2, and subordinate if p2
dominates p1. We exclude pairs that do not exist
in the gold hierarchy from our analysis and call
the remaining set related interacting participant
pairs (RIPPt). Table 4 shows the total number
of pairs in IPPt and RIPPt from all the threads
in the APGI subset of our corpus and across Train,
Dev and Test sets.
</bodyText>
<table confidence="0.99969425">
Description Total Train Dev Test
# of threads 17,788 8,911 4,328 4,549
Et JIPPtj 74,523 36,528 18,540 19,455
Et IRIPPtl 4,649 2,260 1,080 1,309
</table>
<tableCaption confidence="0.6177704">
Table 4: Data Statistics
Row 1 presents the total number of threads in different
subsets of the corpus. Row 2 and 3 present the number of
interacting participant pairs (IPP) and related interacting
participant pairs (RIPP) in those subsets.
</tableCaption>
<subsectionHeader confidence="0.988413">
4.2 Structural Features
</subsectionHeader>
<bodyText confidence="0.989899733333333">
Now, we describe various features that capture
the structure of interaction between the pairs of
participants in a thread. Each feature f is ex-
tracted with respect to a person p over a refer-
ence set of messages M (denoted fMp). For a pair
(p1, p2), we extract 4 versions of each feature f :
f p1
IMt(p1,p2), f p2
IMt(p1,p2), fMtp1 and fMtp2. The first two
capture behavior of each person of the pair in in-
teractions between themselves, while the third and
fourth capture their overall behavior in the entire
thread. We group our features into three categories
— THRSTR, THRMETA and DIA. THRSTR cap-
tures the thread structure in terms of verbosity and
</bodyText>
<page confidence="0.987465">
1969
</page>
<bodyText confidence="0.999892725490196">
positional features of messages (e.g., how many
emails did a person send). THRMETA contain
email header meta-data based features that cap-
ture the thread structure (e.g., how many recipients
were there). Both sets of features do not perform
any NLP analysis on the the content of the emails.
DIA captures the pragmatics of the dialog and re-
quires a deeper analysis of the email content (e.g.,
did they issue any requests).
THRSTR: This feature set includes two kinds
of features — positional and verbosity. The po-
sitional features are a boolean feature to denote
whether p sent the first message (Initiate), and
the relative positions of p’s first and last messages
(FirstMsgPos and LastMsgPos) in M. The ver-
bosity features are p’s message count (MsgCount),
message ratio (MsgRatio), token count (Token-
Count), token ratio (TokenRato) and tokens per
message (TokenPerMsg), all calculated over M.
THRMETA: This feature set includes the av-
erage number of recipients (AvgRecipients) and
To recipients (AvgToRecipients) in emails sent by
p, the percentage of emails p received in which
he/she was in the To list (InToList%), boolean fea-
tures denoting whether p added or removed peo-
ple when responding to a message (AddPerson
and RemovePerson), average number of replies re-
ceived per message sent by p (ReplyRate) and av-
erage number of replies received from the other
person of the pair to messages where he/she was
a To recipient (ReplyRateWithinPair). ReplyRate-
WithinPair applies only to IMt(p1, p2).
DIA: We use dialog acts (DA) and overt dis-
plays of power (ODP) tags to model the struc-
ture of interactions within the message content.
We obtain DA and ODP tags using automatic tag-
gers trained on manual annotations. The DA tag-
ger (Omuya et al., 2013) obtained an accuracy of
92%. The ODP tagger (Prabhakaran et al., 2012)
obtained an accuracy of 96% and F-measure of
54%. The DA tagger labels each sentence to be
one of the 4 dialog acts: Request Action, Request
Information, Inform, and Conventional. The ODP
Tagger identifies sentences (mostly requests) that
express additional constraints on their addressee,
beyond those introduced by the dialog act. For
example, the sentence “Please come to my of-
fice right now” is considered as an ODP, while
“It would be great if you could come to my of-
fice now” is not, even though both issue the same
request. For more details on ODP, we refer the
</bodyText>
<table confidence="0.999918">
Feature Name Mean(fXIMt)|X = Msup
Fsub Fsup Msub
THRMETA
AvgRecipients*** 4.76 5.74 5.58 4.98
AvgToRecipients*** 3.63 4.73 3.84 3.80
InToList%. 0.83 0.86 0.84 0.83
ReplyRate*** 0.72 0.86 0.70 0.61
AddPerson 0.58 0.66 0.59 0.68
RemovePerson 0.55 0.60 0.54 0.65
THRSTR
Initiate 0.38 0.24 0.39 0.30
FirstMsgPos* 0.18 0.25 0.19 0.22
LastMsgPos** 0.34 0.33 0.34 0.39
MsgCount*** 0.92 0.61 0.93 0.91
MsgRatio*** 0.33 0.23 0.33 0.32
TokenCount 76.5 41.0 102.0 54.3
TokenRatio 0.38 0.23 0.40 0.27
TokenPerMsg*** 90.2 67.9 118.2 53.2
DIAPR
Conventional 0.55 0.43 0.64 0.56
Inform 3.50 1.96 4.51 2.53
ReqAction** 0.07 0.06 0.05 0.10
ReqInform 0.29 0.21 0.20 0.16
DanglingReq% 0.06 0.12 0.07 0.18
ODPCount*** 0.10 0.07 0.09 0.13
</table>
<tableCaption confidence="0.994597">
Table 5: ANOVA results and group means for Hierarchical
Power and Gender
Fsub: Female subordinates; Fsup: Female superiors;
Msub: Male subordinates; Msup: Male superiors;
* (p &lt; .05); ** (p &lt; .01); *** (p &lt; .001)
</tableCaption>
<bodyText confidence="0.999190125">
reader to (Prabhakaran et al., 2012). We use 5
features: ReqAction, ReqInform, Inform, Conven-
tional, and ODPCount to capture the number of
sentences in messages sent by p that have each of
these labels. We also use a feature to capture the
number of p’s messages with a request that did not
get a reply, i.e., dangling request percentage (Dan-
glingReq%), over all messages sent by p.
</bodyText>
<sectionHeader confidence="0.972463" genericHeader="evaluation">
5 Gender and Power
</sectionHeader>
<bodyText confidence="0.999969909090909">
In this subsection, we analyze the impact of gen-
der on the expression of power in email. We per-
form an ANOVA test on all features described in
Section 4.2 keeping both Hierarchical Power and
Gender as independent variables. We perform this
on the Train subset of the APGI subset of our cor-
pus. Table 5 shows the results for thread level ver-
sion of the features (we obtain similar significance
results at the interaction level as well). As can be
seen from the ANOVA results, the mean values of
many features differ significantly for the factorial
</bodyText>
<page confidence="0.906123">
1970
</page>
<figure confidence="0.9980962">
0.16
0.135
Subordinates Superiors Female Male Female Female Male Male
Subordinates Superiors Subordinates Superiors
0.12
0.08
0.04
0
0.091
0.114
0.086
0.113
0.096
0.072
0.086
</figure>
<figureCaption confidence="0.999864">
Figure 1: Mean values of ODPCounts in different groups: Subordinates vs. Superiors; Female vs. Male;
across all combinations of Hierarchical Power and Gender.
</figureCaption>
<bodyText confidence="0.998982428571429">
groups of Hierarchical Power and Gender. For ex-
ample, ReplyRate was highly significant; female
superiors obtain the highest reply rate.
It is crucial to note that ANOVA only deter-
mines that there is a significant difference between
groups, but does not tell which groups are signifi-
cantly different. In order to ascertain that, we must
use the Tukey’s HSD (Honest Significant Differ-
ence) Test. We do not describe the analysis of
all our features to that depth in this paper due to
space limitations. Instead, we investigate specific
hypotheses which we have derived from sociolin-
guistic literature. The first hypothesis we investi-
gate is:
</bodyText>
<listItem confidence="0.9927358">
• Hypothesis 1: Female superiors tend to use
“face-saving” strategies at work that include
conventionally polite requests and imperson-
alized directives, and that avoid imperatives
(Herring, 2008).
</listItem>
<bodyText confidence="0.998074333333333">
As a stand-in for a face-threatening communica-
tive strategy, we use our “Overt Display of Power”
feature (ODP). An ODP limits the addressee’s
range of possible responses, and thus threatens his
or her (negative) face.6 We thus reformulate our
hypothesis as follows: the use of ODP by superi-
ors changes when looking at the splits by gender,
with female superiors using fewer ODPs than male
superiors. We look further into the ANOVA anal-
ysis of the thread-level ODPCount treating Hierar-
chical Power and Gender as independent variables.
Figure 1 shows the mean values of ODP counts in
</bodyText>
<footnote confidence="0.8892135">
6For a discussion of the notion of “face”, see (Brown and
Levinson, 1987).
</footnote>
<bodyText confidence="0.99792494117647">
each group of participants. A summary of the re-
sults follows.
Hierarchical Power was significant. Subordi-
nates had an average of 0.091 ODP counts and Su-
periors had an average of 0.114 ODP counts. Gen-
der was also significant; Females had an average
of 0.086 ODP counts and Males had an average of
0.113 ODP counts. When looking at the factorial
groups of Hierarchical Power and Gender, how-
ever, several results were very highly significant.
The significantly different pairs of groups, as per
the Tukey’s HSD test, are Male Superiors/Male
Subordinates, Male Superiors/Female Superiors,
and Male Superiors/Female Subordinates. Male
Superiors used the most ODPs, with an average
of 0.135 counts. Somewhat surprisingly, Female
Superiors used the least of the entire group, with
an average of 0.072 counts. Among Subordinates,
Females actually used slightly more ODP, with an
average of 0.096 counts. Male Subordinates had
an average of 0.086 ODP counts. However, the
differences among these three groups (Female Su-
periors, Female Subordinates, and Male Subordi-
nates) are not significant.
The results confirm our hypothesis: female
superiors use fewer ODPs than male superiors.
However, we also see that among women, there
is no significant difference between superiors and
subordinates, and the difference between superi-
ors and subordinates in general (which is signif-
icant) is entirely due to men. This in fact shows
that a more specific (and more interesting) hypoth-
esis than our original hypothesis is validated: only
male superiors use more ODPs than subordinates.
</bodyText>
<page confidence="0.867716">
1971
6 Gender Environment and Power
</page>
<bodyText confidence="0.999916">
We now turn to gender environments and their re-
lation to the expression of power in written di-
alogs. We again start with a hypothesis based on
the sociolinguistic literature.
three groups by setting thresholds on these per-
centages. Finer-grained gender environments re-
sulted in partitions of the data with very few in-
stances, since most of our data involves fairly bal-
anced gender ratios. The three gender environ-
ments we use are the following:
</bodyText>
<listItem confidence="0.980902">
• Hypothesis 2: Women use language to cre-
ate and maintain social relations, for exam-
</listItem>
<bodyText confidence="0.7925105">
ple, they use more small talk (based on a re-
ported “stereotype” in (Holmes and Stubbe,
2003)).
We first define more formally what we mean by
“gender environment” (Section 6.1), and then in-
vestigate our hypothesis (Section 6.2).
</bodyText>
<subsectionHeader confidence="0.995297">
6.1 The Notion of “Gender Environment”
</subsectionHeader>
<bodyText confidence="0.999973129032258">
The notion of “gender environment” refers to the
gender composition of a group who are communi-
cating. In the sociolinguistic studies we have con-
sulted (Holmes and Stubbe, 2003; Herring, 2008),
the notion refers to a stable work group who in-
teract regularly. Since we are interested in study-
ing email conversations (threads), we adapt the
notion to refer to a single thread at a time. Fur-
thermore, we assume that a discourse participant
makes communicative decisions based on (among
other factors) his or her own gender, and based
on the genders of the people he or she is commu-
nicating with in a given conversation (i.e., email
thread). We therefore consider the “gender envi-
ronment” to be specific to each discourse partic-
ipant and to describe the other participants from
his or her point of view. Put differently, we use the
notion of “gender environment” to model a dis-
course participant’s (potential) audience in a con-
versation. For example, a conversation among five
women and one man looks like an all-female audi-
ence from the man’s point of view, but a majority-
female audience from the women’s points of view.
We define the gender environment of a dis-
course participant p in a thread t as follows. As
discussed, we assume that the gender environment
is a property of each discourse participant p in
thread t. We take the set of all discourse partic-
ipants of the thread t, Pt (see Section 4.1), and
exclude p from it: Pt \ {p}. We then calculate
the percentage of women in this set.7 We obtain
</bodyText>
<footnote confidence="0.9903125">
7We note that one could also define the notion of gender
environment at the level of individual emails: not all emails
in a thread involve the same set of participants. We leave this
to future work.
</footnote>
<listItem confidence="0.998768">
• Female Environment: if the percentage of
women in Pt \ {p} is above 66.7%.
• Mixed Environment: if the percentage of
women in Pt \ {p} is between 33.3% and
66.7%.
• Male Environment: if the percentage of
women in Pt \ {p} is below 33.3%
</listItem>
<bodyText confidence="0.533172">
Across all threads and discourse participants in
the threads, we have 791 female, 2087 mixed and
1642 male gender environments.
</bodyText>
<subsectionHeader confidence="0.9991945">
6.2 Gender Environment and Conventional
Dialog Acts
</subsectionHeader>
<bodyText confidence="0.999952827586207">
We now turn to testing Hypothesis 2. We have at
present no way of testing for “small talk” as op-
posed to work-related talk, so we instead test Hy-
pothesis 2 by asking how many conventional dia-
log acts a person performs. Conventional dialog
acts serve not to convey information or requests
(both of which would typically be work-related in
the Enron corpus), but to establish communication
(greetings) and to manage communication (sign-
offs); since communication is an important way of
creating and maintaining social relations, we can
say that conventional dialog acts serve the purpose
of easing conversations and thus of maintaining
social relations. Since this aspect of language is
specifically dependent on a group of people (it is
an inherently social function), we assume that the
relevant feature is not simply Gender, but Gender
Environment. Specifically, we make our Hypothe-
sis 2 more precise by saying that a higher number
of conventional dialog acts is used in Female En-
vironments. We use the thread level version of the
feature ConventionalCount.
Figure 2 shows the mean values of Conven-
tionalCount in each sub-group of participants.
Hierarchical Power was highly significant as
per ANOVA results. Subordinates use conven-
tional language more (0.60 counts) than Superiors
(0.52). Gender is a very highly significant vari-
able; Males use 0.60 counts on average, whereas
</bodyText>
<page confidence="0.979657">
1972
</page>
<figure confidence="0.997919454545455">
1.00
0.79
0.75
0.50
0.25
0.00
0.60
0.52
0.61
0.54 0.56
0.48
0.57
0.51
0.56 0.55
Superiors in
Mixed Env
Subordinates
in Male Env
Superiors in
Male Env
Subordinates Superiors Female Env Mixed Env Male Env Subordinates Superiors in Subordinates
in Female Env Female Env in Mixed Env
</figure>
<figureCaption confidence="0.9977525">
Figure 2: Mean values of Conventional Counts: Subordinates vs. Superiors; across all Gender
Environments; across all combinations of Hierarchical Power and Gender Environments.
</figureCaption>
<bodyText confidence="0.999977324324324">
Females use 0.50. This result is somewhat sur-
prising, but does not invalidate our Hypothesis 2,
since our hypothesis is not formulated in terms of
Gender, but in terms of Gender Environment. The
analysis of Gender Environment at first appears to
be a negative result: while the averages by Gender
Environment differ, the differences are not signif-
icant. However, the groups defined by both Hi-
erarchical Power and Gender Environment have
highly significant differences. Subordinates in Fe-
male Environments use the most conventional lan-
guage of all six groups, with an average of 0.79.
Superiors in Female Environments use the least,
with an average of 0.48. Mixed Environments and
Male Environments differ, but are more similar to
each other than to Female Environments. In fact,
in the Tukey HSD test, the only significant pairs
are exactly the set of subordinates in Female En-
vironments paired with each other group (Supe-
riors in Female Environments, and Subordinates
and Superiors in Mixed Environments and Male
Environments). That is, Subordinates in Female
environments use significantly more conventional
language than any other group, but the remaining
groups do not differ significantly from each other.
Our hypothesis is thus only partially verified:
while gender environment is a crucial aspect of the
use of conventional DAs, we also need to look at
the power status of the writer. In fact only sub-
ordinates in female environments use more con-
ventional DAs than any other group (as defined by
power status and gender environment). While our
hypothesis is not fully verified, we interpret the
results to mean that subordinates are more com-
fortable in female environments to use a style of
communication which includes more conventional
DAs than outside the female environments.
</bodyText>
<subsectionHeader confidence="0.469959">
7 Predicting Power in Participant Pairs
</subsectionHeader>
<bodyText confidence="0.999972416666667">
In this section, we use the formulation of
the power prediction problem presented in our
prior work (Prabhakaran and Rambow, 2014).
Given a thread t and a pair of participants
(p1i p2) ∈ RIPPt, we want to automatically de-
tect HP(p1i p2). We use the SVM-based su-
pervised learning system from (Prabhakaran and
Rambow, 2014) that can predict HP(p1 i p2) to
be either superior or subordinate based on the in-
teraction within a thread t for any pair of partici-
pants (p1i p2) ∈ RIPPt. The order of participants
in (p1i p2) is fixed such that p1 is the sender of
the first message in IMt(p1i p2). The power pre-
diction system is built using the ClearTK (Ogren
et al., 2008) wrapper for SVMLight (Joachims,
1999) package. It uses a quadratic kernel to cap-
ture feature-feature interactions, which is very im-
portant as we see in Section 5 and 6. We use the
Train, Dev and Test subsets of the APGI subset
of our corpus for our experiments. We use the re-
lated interacting participant pairs in threads from
the Train set to train our models and optimize our
performance on those from the Dev set. We report
results on both Dev and Test sets.
In addition to the features described in Sec-
tion 4.2, the power prediction system presented
in (Prabhakaran and Rambow, 2014) uses a lexi-
cal feature set (LEX) that captures word ngrams,
POS (part of speech) ngrams and mixed ngrams,
since lexical features have been established to be
very useful for power prediction. Mixed ngrams
are word ngrams where words belonging to open
classes are replaced with their POS tags. We add
two gender-based feature sets: GEN containing
the gender of both persons of the pair and ENV
containing the gender environment feature.
</bodyText>
<page confidence="0.97672">
1973
</page>
<bodyText confidence="0.999915843137255">
Table 6 presents the results obtained using vari-
ous feature combinations. We experimented using
all subsets of {LEX, THRSTR, THRMETA, DIA,
GEN, ENV } on the Dev set; we report the most
interesting results here. The majority baseline
(subordinate) obtains an accuracy of 55.8%. Us-
ing the gender-based features alone performs only
slightly better than the majority baseline. We use
the best performing feature subset from (Prab-
hakaran and Rambow, 2014) (LEX + THRMETA)
as another baseline, which obtains an accuracy
of 68.2%. Adding the GEN features improves
the performance to 70.6%. Further adding the
ENV features improves the performance, but only
marginally to 70.7% (our overall best result, an
improvment of 2.4% points). The best perform-
ing feature set without using LEX was the combi-
nation of DIA, THRMETA and GEN (67.3%). Re-
moving the gender features from this reduced the
performance to 64.6%. Similarly, the best per-
forming feature set which do not use the content
of emails at all was THRSTR + THRMETA + GEN
(66.6). Removing the gender features decreases
the accuracy by a larger margin (5.4% accuracy
reduction to 63.0).
We interpret the differences in absolute im-
provement as follows: the gender-based features
on their own are not very useful, and gain predic-
tive value only when paired with other features.
This is because the other features in fact make
quite different predictions depending on gender
and/or gender environment. However, the content
features (and in particular the lexical features) are
so powerful on their own that the relative contribu-
tion of the gender-based features decreases again.
Nonetheless, we take these results as validation of
the claim that gender-based features enhance the
value of other features in the task of predicting
power relations.
We performed another experiment where we
partitioned the data into two subsets according to
the gender of the first person of the pair and trained
two separate models to predict power. At test time,
we chose the appropriate model based on the gen-
der of the first person of the pair. However, this
did not improve the performance.
On our blind test set, the majority baseline ob-
tains an accuracy of 57.9% and the (Prabhakaran
and Rambow, 2014) baseline obtains an accuracy
of 68.9%. On adding the gender-based features,
the accuracy of the system improves to 70.2%.
</bodyText>
<table confidence="0.999719454545455">
Description Accuracy
Majority (Always Subordinate) 55.83
GEN 57.59
GEN + ENV 57.59
Baseline (LEX + THRMETA) 68.24
Baseline (LEX + THRMETA) + GEN 70.56
Baseline (LEX + THRMETA) + GEN + ENV 70.74
DIA + THRMETA + GEN 67.31
DIA + THRMETA 64.63
THRSTR + THRMETA + GEN 66.57
THRSTR + THRMETA 62.96
</table>
<tableCaption confidence="0.992878666666667">
Table 6: Accuracies on feature subsets (Dev set).
THRMETA: meta-data; THRSTR: structural; DIA: dialog-act;
GEN: gender; ENV: gender environment; LEX: ngrams;
</tableCaption>
<sectionHeader confidence="0.994219" genericHeader="conclusions">
8 Conclusion
</sectionHeader>
<bodyText confidence="0.999989708333333">
We presented a new, freely available resource: the
Gender Identified Enron Corpus, and explored the
relation between power, gender, and language us-
ing this resource. We also introduced the notion
of gender environment, and showed that the man-
ifestations of power differ significantly between
gender environments. We also showed that the
gender-related features helps in improving power
prediction. In future work, we will explore ma-
chine learning algorithms which capture the inter-
actions between features better than our SVM with
quadratic kernel.
We expect our corpus to be a rich resource for
social scientists interested in the effect of power
and gender on language use. We will investi-
gate several other sociolinguistic-inspired research
questions; for example, do the strategies managers
use for “effectiveness” of communication differ
based on gender environments?
While our findings pertain to the Enron data
set, we believe that the insights and techniques
from this study can be extended to other genres
in which there is an independent notion of hierar-
chical power, such as moderated online forums.
</bodyText>
<sectionHeader confidence="0.998296" genericHeader="acknowledgments">
Acknowledgments
</sectionHeader>
<bodyText confidence="0.9998405">
This paper is based upon work supported by the
DARPA DEFT Program. The views expressed are
those of the authors and do not reflect the official
policy or position of the Department of Defense
or the U.S. Government. We thank several anony-
mous reviewers for their constructive feedback.
</bodyText>
<page confidence="0.99587">
1974
</page>
<sectionHeader confidence="0.989833" genericHeader="references">
References
</sectionHeader>
<reference confidence="0.999359546296296">
Apoorv Agarwal, Adinoyi Omuya, Aaron Harnly, and
Owen Rambow. 2012. A comprehensive gold stan-
dard for the enron organizational hierarchy. In Pro-
ceedings of the 50th Annual Meeting of the Associa-
tion for Computational Linguistics (Volume 2: Short
Papers), pages 161–165, Jeju Island, Korea, July.
Association for Computational Linguistics.
David Bamman, Jacob Eisenstein, and Tyler Schnoe-
belen. 2014. Gender identity and lexical varia-
tion in social media. Journal of Sociolinguistics,
18(2):135–160.
Or Biran, Sara Rosenthal, Jacob Andreas, Kathleen
McKeown, and Owen Rambow. 2012. Detecting
influencers in written online conversations. In Pro-
ceedings of the Second Workshop on Language in
Social Media, pages 37–45, Montr´eal, Canada, June.
Association for Computational Linguistics.
Philip Bramsen, Martha Escobar-Molano, Ami Patel,
and Rafael Alonso. 2011. Extracting social power
relationships from natural language. In ACL, pages
773–782. The Association for Computational Lin-
guistics.
Penelope Brown and Stephen C. Levinson. 1987.
Politeness : Some Universals in Language Usage
(Studies in Interactional Sociolinguistics). Cam-
bridge University Press, February.
Na Cheng, R. Chandramouli, and K. P. Subbalakshmi.
2011. Author gender identification from text. Digit.
Investig., 8(1):78–88, July.
Malcolm Corney, Olivier de Vel, Alison Anderson, and
George Mohay. 2002. Gender-preferential text min-
ing of e-mail discourse. In Computer Security Ap-
plications Conference, 2002. Proceedings. 18th An-
nual, pages 282–289. IEEE.
Cristian Danescu-Niculescu-Mizil, Lillian Lee,
Bo Pang, and Jon Kleinberg. 2012. Echoes of
power: language effects and power differences in
social interaction. In Proceedings of the 21st in-
ternational conference on World Wide Web, WWW
’12, New York, NY, USA. ACM.
William Deitrick, Zachary Miller, Benjamin Valyou,
Brian Dickinson, Timothy Munson, and Wei Hu.
2012. Author gender prediction in an email stream
using neural networks. Journal of Intelligent Learn-
ing Systems &amp; Applications, 4(3).
Eric Gilbert. 2012. Phrases that signal workplace hier-
archy. In Proceedings of the ACM 2012 conference
on Computer Supported Cooperative Work, CSCW
’12, pages 1037–1046, New York, NY, USA. ACM.
Susan C Herring. 2008. Gender and power in on-
line communication. The handbook of language and
gender, page 202.
Janet Holmes and Maria Stubbe. 2003. feminine work-
places: stereotype and reality. The handbook of lan-
guage and gender, pages 572–599.
Thorsten Joachims. 1999. Making Large-Scale SVM
Learning Practical. In Bernhard Sch¨olkopf, Christo-
pher J.C. Burges, and A. Smola, editors, Advances
in Kernel Methods - Support Vector Learning, Cam-
bridge, MA, USA. MIT Press.
Shari Kendall and Deborah Tannen. 1997. Gender
and language in the workplace. In Gender and Dis-
course, pages 81–105. Sage, London.
Shari Kendall. 2003. Creating gendered demeanors
of authority at work and at home. The handbook of
language and gender, page 600.
Saif Mohammad and Tony Yang. 2011. Tracking sen-
timent in mail: How genders differ on emotional
axes. In Proceedings of the 2nd Workshop on Com-
putational Approaches to Subjectivity and Sentiment
Analysis (WASSA 2.011), pages 70–79, Portland,
Oregon, June. Association for Computational Lin-
guistics.
Viet-An Nguyen, Jordan Boyd-Graber, Philip Resnik,
Deborah A. Cai, Jennifer E. Midberry, and Yuanxin
Wang. 2013. Modeling topic control to detect in-
fluence in conversations using nonparametric topic
models. Machine Learning, pages 1–41.
Philip V. Ogren, Philipp G. Wetzler, and Steven
Bethard. 2008. ClearTK: A UIMA toolkit for sta-
tistical natural language processing. In Towards
Enhanced Interoperability for Large HLT Systems:
UIMA for NLP workshop at Language Resources
and Evaluation Conference (LREC).
Adinoyi Omuya, Vinodkumar Prabhakaran, and Owen
Rambow. 2013. Improving the quality of minor-
ity class identification in dialog act tagging. In Pro-
ceedings of the 2013 Conference of the North Amer-
ican Chapter of the Association for Computational
Linguistics: Human Language Technologies, pages
802–807, Atlanta, Georgia, June. Association for
Computational Linguistics.
Kelly Peterson, Matt Hohensee, and Fei Xia. 2011.
Email formality in the workplace: A case study
on the enron corpus. In Proceedings of the Work-
shop on Language in Social Media (LSM 2011),
pages 86–95, Portland, Oregon, June. Association
for Computational Linguistics.
Vinodkumar Prabhakaran and Owen Rambow. 2013.
Written dialog and social power: Manifestations of
different types of power in dialog behavior. In Pro-
ceedings of the IJCNLP, pages 216–224, Nagoya,
Japan, October. Asian Federation of Natural Lan-
guage Processing.
Vinodkumar Prabhakaran and Owen Rambow. 2014.
Predicting power relations between participants in
written dialog from a single thread. In Proceed-
ings of the 52nd Annual Meeting of the Association
</reference>
<page confidence="0.87077">
1975
</page>
<reference confidence="0.9980518">
for Computational Linguistics (Volume 2: Short Pa-
pers), pages 339–344, Baltimore, Maryland, June.
Association for Computational Linguistics.
Vinodkumar Prabhakaran, Owen Rambow, and Mona
Diab. 2012. Predicting Overt Display of Power in
Written Dialogs. In Human Language Technolo-
gies: The 2012 Annual Conference of the North
American Chapter of the Association for Computa-
tional Linguistics, Montreal, Canada, June. Associ-
ation for Computational Linguistics.
Vinodkumar Prabhakaran, Ajita John, and Dor´ee D.
Seligmann. 2013. Who had the upper hand? rank-
ing participants of interactions based on their rela-
tive power. In Proceedings of the IJCNLP, pages
365–373, Nagoya, Japan, October. Asian Federation
of Natural Language Processing.
Vinodkumar Prabhakaran, Ashima Arora, and Owen
Rambow. 2014. Power of confidence: How poll
scores impact topic dynamics in political debates.
In Proceedings of the ACL 2014 Workshop on Lan-
guage Technologies and Computational Social Sci-
ence, page 49, Baltimore, MD, USA, June. Associa-
tion for Computational Linguistics.
Tomek Strzalkowski, Samira Shaikh, Ting Liu,
George Aaron Broadwell, Jenny Stromer-Galley,
Sarah Taylor, Umit Boz, Veena Ravishankar, and
Xiaoai Ren. 2012. Modeling leadership and influ-
ence in multi-party online discourse. In Proceedings
of COLING, pages 2535–2552, Mumbai, India, De-
cember. The COLING 2012 Organizing Committee.
Jen-Yuan Yeh and Aaron Harnly. 2006. Email thread
reassembly using similarity matching. In CEAS
2006 - The Third Conference on Email and Anti-
Spam, July 27-28, 2006, Mountain View, California,
USA, Mountain View, California, USA, July.
</reference>
<page confidence="0.995085">
1976
</page>
</variant>
</algorithm>
<algorithm name="ParsHed" version="110505">
<variant no="0" confidence="0.488238">
<title confidence="0.995544">Gender and Power: How Gender and Gender Environment Affect Manifestations of Power</title>
<author confidence="0.990129">Vinodkumar Prabhakaran</author>
<affiliation confidence="0.9998905">Dept. of Computer Science Columbia University</affiliation>
<address confidence="0.999484">New York, NY, USA</address>
<email confidence="0.996613">vinod@cs.columbia.edu</email>
<author confidence="0.854873">E Emily</author>
<affiliation confidence="0.999913">Dept. of Computer</affiliation>
<address confidence="0.9034785">Columbia New York, NY, USA</address>
<email confidence="0.999543">eer2137@columbia.edu</email>
<author confidence="0.873356">Owen</author>
<affiliation confidence="0.790787">Columbia</affiliation>
<address confidence="0.99833">New York, NY, USA</address>
<email confidence="0.999862">rambow@ccls.columbia.edu</email>
<abstract confidence="0.99935825">We investigate the interaction of power, gender, and language use in the Enron email corpus. We present a freely available extension to the Enron corpus, with the gender of senders of 87% messages reliably identified. Using this data, we test two specific hypotheses drawn from the sociolinguistic literature pertaining to gender and power: women managers use face-saving communicative strategies, and women use language more explicitly than men to create and maintain social relations. We introduce the notion of “gender environment” to the computational study of written conversations; we interpret this notion as the gender makeup of an email thread, and show that some manifestations of power differ significantly between gender environments. Finally, we show the utility of gender information in the problem of automatically predicting the direction of power between pairs of participants in email interactions.</abstract>
</variant>
</algorithm>
<algorithm name="ParsCit" version="110505">
<citationList>
<citation valid="true">
<authors>
<author>Apoorv Agarwal</author>
<author>Adinoyi Omuya</author>
<author>Aaron Harnly</author>
<author>Owen Rambow</author>
</authors>
<title>A comprehensive gold standard for the enron organizational hierarchy.</title>
<date>2012</date>
<booktitle>In Proceedings of the 50th Annual Meeting of the Association for Computational Linguistics (Volume 2: Short Papers),</booktitle>
<pages>161--165</pages>
<institution>Jeju Island, Korea, July. Association for Computational Linguistics.</institution>
<contexts>
<context position="9608" citStr="Agarwal et al. (2012)" startWordPosition="1533" endWordPosition="1536">will aid further research in this area. 3 Gender Identified Enron Corpus 3.1 Enron Corpus In our work, we use the version of Enron email corpus released by Yeh and Harnly (2006). The corpus contains emails from the mailboxes of 145 core employees who held top managerial positions within Enron at the time of bankruptcy. Yeh and Harnly (2006) preprocessed the corpus to combine multiple email addresses belonging to the same entity and identify each entity in the corpus with a unique identifier. The corpus contains a total of 111,933 messages. This version of the corpus has been enriched later by Agarwal et al. (2012) with gold organizational power relations, manually determined using information from Enron organizational charts. It includes relations of 1,518 employees and captures dominance relations between 13,724 pairs of them. This information enables us to study the manifestations of power in these interactions, in relation to gender. In this version of the corpus, the thread structure of email messages is reconstructed, with the missing messages restored from other emails in which they were quoted. This allows us to go beyond isolated messages and study the dialog structure within email threads. The</context>
</contexts>
<marker>Agarwal, Omuya, Harnly, Rambow, 2012</marker>
<rawString>Apoorv Agarwal, Adinoyi Omuya, Aaron Harnly, and Owen Rambow. 2012. A comprehensive gold standard for the enron organizational hierarchy. In Proceedings of the 50th Annual Meeting of the Association for Computational Linguistics (Volume 2: Short Papers), pages 161–165, Jeju Island, Korea, July. Association for Computational Linguistics.</rawString>
</citation>
<citation valid="true">
<authors>
<author>David Bamman</author>
<author>Jacob Eisenstein</author>
<author>Tyler Schnoebelen</author>
</authors>
<title>Gender identity and lexical variation in social media.</title>
<date>2014</date>
<journal>Journal of Sociolinguistics,</journal>
<volume>18</volume>
<issue>2</issue>
<contexts>
<context position="5863" citStr="Bamman et al., 2014" startWordPosition="918" endWordPosition="921">hey find that, while female managers tend to break many stereotypes of “feminine” communication, they have different strategies in connecting with employees and exhibiting power in the two gender environments. This work has inspired us to look at this phenomenon by including “Gender Environment” in our study. By finding the ratios of males to females on a thread, we can look at whether indicators change within a more heavily male or female thread. This notion of gender environment is supported by an idea in recent Twitterbased sociolinguistic research on gender identity and lexical variation (Bamman et al., 2014). One of the many insights from their work is that gendered linguistic behavior is oriented by a number of factors, one of which includes the speaker’s audience. Their work looks at Twitter users whose linguistic style fails to identify their gender in classification experiments, and finds that the linguistic gender norms can be influenced by the style of their interlocutors. Within the NLP community, there has been substantial research exploring language use and power. A large number of these studies are performed in the domain of organizational email where the notion of power is well defined</context>
</contexts>
<marker>Bamman, Eisenstein, Schnoebelen, 2014</marker>
<rawString>David Bamman, Jacob Eisenstein, and Tyler Schnoebelen. 2014. Gender identity and lexical variation in social media. Journal of Sociolinguistics, 18(2):135–160.</rawString>
</citation>
<citation valid="true">
<authors>
<author>Or Biran</author>
<author>Sara Rosenthal</author>
<author>Jacob Andreas</author>
<author>Kathleen McKeown</author>
<author>Owen Rambow</author>
</authors>
<title>Detecting influencers in written online conversations.</title>
<date>2012</date>
<booktitle>In Proceedings of the Second Workshop on Language in Social Media,</booktitle>
<pages>37--45</pages>
<publisher>Association</publisher>
<institution>for Computational Linguistics.</institution>
<location>Montr´eal, Canada,</location>
<contexts>
<context position="7261" citStr="Biran et al., 2012" startWordPosition="1145" endWordPosition="1148">ng. Earlier approaches used simple lexical features alone (e.g. (Bramsen et al., 2011; Gilbert, 2012)) as a means to predict power. Later studies have used more complex linguistic and structural features, such as formality (Peterson et al., 2011), dialog acts (Prabhakaran and Rambow, 2013), and thread structure (Prabhakaran and Rambow, 2014). Our work is also on the Enron email corpus, and our baseline features are derived from some of this prior work. Researchers have also studied power and influence in other genres of interactions, such as online forums (DanescuNiculescu-Mizil et al., 2012; Biran et al., 2012), multi-party chats (Strzalkowski et al., 2012) and off-line interactions such as presidential debates (Nguyen et al., 2013; Prabhakaran et al., 2013; Prabhakaran et al., 2014). There is also some work within the NLP field on analyzing language use in relation to gender. Mohammad and Yang (2011) analyzed the way gender affects the expression of sentiments in text, while we are interested in how gender relates to manifestations of organizational power. For their study, they assigned gender for the core employees in the Enron email corpus based on whether the first name of the person was easily </context>
</contexts>
<marker>Biran, Rosenthal, Andreas, McKeown, Rambow, 2012</marker>
<rawString>Or Biran, Sara Rosenthal, Jacob Andreas, Kathleen McKeown, and Owen Rambow. 2012. Detecting influencers in written online conversations. In Proceedings of the Second Workshop on Language in Social Media, pages 37–45, Montr´eal, Canada, June. Association for Computational Linguistics.</rawString>
</citation>
<citation valid="true">
<authors>
<author>Philip Bramsen</author>
<author>Martha Escobar-Molano</author>
<author>Ami Patel</author>
<author>Rafael Alonso</author>
</authors>
<title>Extracting social power relationships from natural language. In</title>
<date>2011</date>
<booktitle>ACL,</booktitle>
<pages>773--782</pages>
<contexts>
<context position="6727" citStr="Bramsen et al., 2011" startWordPosition="1060" endWordPosition="1063">ender in classification experiments, and finds that the linguistic gender norms can be influenced by the style of their interlocutors. Within the NLP community, there has been substantial research exploring language use and power. A large number of these studies are performed in the domain of organizational email where the notion of power is well defined in terms of organizational hierarchy. It is also aided by the availability of the moderately large Enron email corpus which captures email interactions in an organizational setting. Earlier approaches used simple lexical features alone (e.g. (Bramsen et al., 2011; Gilbert, 2012)) as a means to predict power. Later studies have used more complex linguistic and structural features, such as formality (Peterson et al., 2011), dialog acts (Prabhakaran and Rambow, 2013), and thread structure (Prabhakaran and Rambow, 2014). Our work is also on the Enron email corpus, and our baseline features are derived from some of this prior work. Researchers have also studied power and influence in other genres of interactions, such as online forums (DanescuNiculescu-Mizil et al., 2012; Biran et al., 2012), multi-party chats (Strzalkowski et al., 2012) and off-line inter</context>
</contexts>
<marker>Bramsen, Escobar-Molano, Patel, Alonso, 2011</marker>
<rawString>Philip Bramsen, Martha Escobar-Molano, Ami Patel, and Rafael Alonso. 2011. Extracting social power relationships from natural language. In ACL, pages 773–782. The Association for Computational Linguistics.</rawString>
</citation>
<citation valid="true">
<authors>
<author>Penelope Brown</author>
<author>Stephen C Levinson</author>
</authors>
<title>Politeness : Some Universals in Language Usage (Studies in Interactional Sociolinguistics).</title>
<date>1987</date>
<publisher>Cambridge University Press,</publisher>
<contexts>
<context position="28391" citStr="Brown and Levinson, 1987" startWordPosition="4687" endWordPosition="4690">tening communicative strategy, we use our “Overt Display of Power” feature (ODP). An ODP limits the addressee’s range of possible responses, and thus threatens his or her (negative) face.6 We thus reformulate our hypothesis as follows: the use of ODP by superiors changes when looking at the splits by gender, with female superiors using fewer ODPs than male superiors. We look further into the ANOVA analysis of the thread-level ODPCount treating Hierarchical Power and Gender as independent variables. Figure 1 shows the mean values of ODP counts in 6For a discussion of the notion of “face”, see (Brown and Levinson, 1987). each group of participants. A summary of the results follows. Hierarchical Power was significant. Subordinates had an average of 0.091 ODP counts and Superiors had an average of 0.114 ODP counts. Gender was also significant; Females had an average of 0.086 ODP counts and Males had an average of 0.113 ODP counts. When looking at the factorial groups of Hierarchical Power and Gender, however, several results were very highly significant. The significantly different pairs of groups, as per the Tukey’s HSD test, are Male Superiors/Male Subordinates, Male Superiors/Female Superiors, and Male Supe</context>
</contexts>
<marker>Brown, Levinson, 1987</marker>
<rawString>Penelope Brown and Stephen C. Levinson. 1987. Politeness : Some Universals in Language Usage (Studies in Interactional Sociolinguistics). Cambridge University Press, February.</rawString>
</citation>
<citation valid="true">
<authors>
<author>Na Cheng</author>
<author>R Chandramouli</author>
<author>K P Subbalakshmi</author>
</authors>
<title>Author gender identification from text.</title>
<date>2011</date>
<journal>Digit. Investig.,</journal>
<volume>8</volume>
<issue>1</issue>
<contexts>
<context position="8425" citStr="Cheng et al., 2011" startWordPosition="1337" endWordPosition="1340">ed on whether the first name of the person was easily gender identifiable or not. If the person had an unfamiliar name or a name that could be of either gender, they marked his/her gender as unknown and excluded them from their study.1 For example, the gender of the employee Kay Mann was marked as unknown in their gender assignment. However, in our work, we manually research and determine the gender of every core employee. Researchers have also attempted to automatically predict the gender of email senders using supervised learning techniques based on linguistic features (Corney et al., 2002; Cheng et al., 2011; Deitrick et al., 2012), a task we do not address in this paper. These studies use datasets that are relatively smaller in size. Corney et al. (2002) use around 4K emails from 325 gender identified authors. Cheng et al. (2011) use around 9K emails from 108 gender identified authors. Deitrick et al. (2012) use around 18K emails from 144 gender 1http://www.saifmohammad.com/WebDocs/dir-emailgender.txt 1966 identified authors. The dataset we offer is much larger in size, with around 97K emails whose authors are gender identified. We believe that our resource will aid further research in this area</context>
</contexts>
<marker>Cheng, Chandramouli, Subbalakshmi, 2011</marker>
<rawString>Na Cheng, R. Chandramouli, and K. P. Subbalakshmi. 2011. Author gender identification from text. Digit. Investig., 8(1):78–88, July.</rawString>
</citation>
<citation valid="true">
<authors>
<author>Malcolm Corney</author>
<author>Olivier de Vel</author>
<author>Alison Anderson</author>
<author>George Mohay</author>
</authors>
<title>Gender-preferential text mining of e-mail discourse.</title>
<date>2002</date>
<booktitle>In Computer Security Applications Conference, 2002. Proceedings. 18th Annual,</booktitle>
<pages>282--289</pages>
<publisher>IEEE.</publisher>
<marker>Corney, de Vel, Anderson, Mohay, 2002</marker>
<rawString>Malcolm Corney, Olivier de Vel, Alison Anderson, and George Mohay. 2002. Gender-preferential text mining of e-mail discourse. In Computer Security Applications Conference, 2002. Proceedings. 18th Annual, pages 282–289. IEEE.</rawString>
</citation>
<citation valid="true">
<authors>
<author>Cristian Danescu-Niculescu-Mizil</author>
<author>Lillian Lee</author>
<author>Bo Pang</author>
<author>Jon Kleinberg</author>
</authors>
<title>Echoes of power: language effects and power differences in social interaction.</title>
<date>2012</date>
<booktitle>In Proceedings of the 21st international conference on World Wide Web, WWW ’12,</booktitle>
<publisher>ACM.</publisher>
<location>New York, NY, USA.</location>
<marker>Danescu-Niculescu-Mizil, Lee, Pang, Kleinberg, 2012</marker>
<rawString>Cristian Danescu-Niculescu-Mizil, Lillian Lee, Bo Pang, and Jon Kleinberg. 2012. Echoes of power: language effects and power differences in social interaction. In Proceedings of the 21st international conference on World Wide Web, WWW ’12, New York, NY, USA. ACM.</rawString>
</citation>
<citation valid="true">
<authors>
<author>William Deitrick</author>
<author>Zachary Miller</author>
<author>Benjamin Valyou</author>
<author>Brian Dickinson</author>
<author>Timothy Munson</author>
<author>Wei Hu</author>
</authors>
<title>Author gender prediction in an email stream using neural networks.</title>
<date>2012</date>
<journal>Journal of Intelligent Learning Systems &amp; Applications,</journal>
<volume>4</volume>
<issue>3</issue>
<contexts>
<context position="8449" citStr="Deitrick et al., 2012" startWordPosition="1341" endWordPosition="1344">rst name of the person was easily gender identifiable or not. If the person had an unfamiliar name or a name that could be of either gender, they marked his/her gender as unknown and excluded them from their study.1 For example, the gender of the employee Kay Mann was marked as unknown in their gender assignment. However, in our work, we manually research and determine the gender of every core employee. Researchers have also attempted to automatically predict the gender of email senders using supervised learning techniques based on linguistic features (Corney et al., 2002; Cheng et al., 2011; Deitrick et al., 2012), a task we do not address in this paper. These studies use datasets that are relatively smaller in size. Corney et al. (2002) use around 4K emails from 325 gender identified authors. Cheng et al. (2011) use around 9K emails from 108 gender identified authors. Deitrick et al. (2012) use around 18K emails from 144 gender 1http://www.saifmohammad.com/WebDocs/dir-emailgender.txt 1966 identified authors. The dataset we offer is much larger in size, with around 97K emails whose authors are gender identified. We believe that our resource will aid further research in this area. 3 Gender Identified En</context>
</contexts>
<marker>Deitrick, Miller, Valyou, Dickinson, Munson, Hu, 2012</marker>
<rawString>William Deitrick, Zachary Miller, Benjamin Valyou, Brian Dickinson, Timothy Munson, and Wei Hu. 2012. Author gender prediction in an email stream using neural networks. Journal of Intelligent Learning Systems &amp; Applications, 4(3).</rawString>
</citation>
<citation valid="true">
<authors>
<author>Eric Gilbert</author>
</authors>
<title>Phrases that signal workplace hierarchy.</title>
<date>2012</date>
<booktitle>In Proceedings of the ACM 2012 conference on Computer Supported Cooperative Work, CSCW ’12,</booktitle>
<pages>1037--1046</pages>
<publisher>ACM.</publisher>
<location>New York, NY, USA.</location>
<contexts>
<context position="6743" citStr="Gilbert, 2012" startWordPosition="1064" endWordPosition="1065">n experiments, and finds that the linguistic gender norms can be influenced by the style of their interlocutors. Within the NLP community, there has been substantial research exploring language use and power. A large number of these studies are performed in the domain of organizational email where the notion of power is well defined in terms of organizational hierarchy. It is also aided by the availability of the moderately large Enron email corpus which captures email interactions in an organizational setting. Earlier approaches used simple lexical features alone (e.g. (Bramsen et al., 2011; Gilbert, 2012)) as a means to predict power. Later studies have used more complex linguistic and structural features, such as formality (Peterson et al., 2011), dialog acts (Prabhakaran and Rambow, 2013), and thread structure (Prabhakaran and Rambow, 2014). Our work is also on the Enron email corpus, and our baseline features are derived from some of this prior work. Researchers have also studied power and influence in other genres of interactions, such as online forums (DanescuNiculescu-Mizil et al., 2012; Biran et al., 2012), multi-party chats (Strzalkowski et al., 2012) and off-line interactions such as </context>
</contexts>
<marker>Gilbert, 2012</marker>
<rawString>Eric Gilbert. 2012. Phrases that signal workplace hierarchy. In Proceedings of the ACM 2012 conference on Computer Supported Cooperative Work, CSCW ’12, pages 1037–1046, New York, NY, USA. ACM.</rawString>
</citation>
<citation valid="true">
<authors>
<author>Susan C Herring</author>
</authors>
<title>Gender and power in online communication. The handbook of language and gender,</title>
<date>2008</date>
<pages>202</pages>
<contexts>
<context position="4848" citStr="Herring, 2008" startWordPosition="749" endWordPosition="750"> 4 defines the problem of predicting power and the various dimensions of interaction we analyze. We turn to our second contribution, the analysis of the data, in Sections 5 and 6. Section 7 describes our third contribution, the machine learning experiments using genderrelated features in the prediction of hierarchical power. We then conclude and discuss future work. 2 Related Work There is much sociolinguistic background related to gender and language use, some of it specifically related to language use in the work environment (Kendall and Tannen, 1997; Holmes and Stubbe, 2003; Kendall, 2003; Herring, 2008). We do not provide a full discussion of this work for lack of space, but single out one paper which has particularly influenced our work. Holmes and Stubbe (2003) provide two case studies that do not look at the differences between male and female managers’ communication, but at the difference between female managers’ communication in more heavily female vs. more heavily male environments. They find that, while female managers tend to break many stereotypes of “feminine” communication, they have different strategies in connecting with employees and exhibiting power in the two gender environme</context>
<context position="27734" citStr="Herring, 2008" startWordPosition="4579" endWordPosition="4580">nce between groups, but does not tell which groups are significantly different. In order to ascertain that, we must use the Tukey’s HSD (Honest Significant Difference) Test. We do not describe the analysis of all our features to that depth in this paper due to space limitations. Instead, we investigate specific hypotheses which we have derived from sociolinguistic literature. The first hypothesis we investigate is: • Hypothesis 1: Female superiors tend to use “face-saving” strategies at work that include conventionally polite requests and impersonalized directives, and that avoid imperatives (Herring, 2008). As a stand-in for a face-threatening communicative strategy, we use our “Overt Display of Power” feature (ODP). An ODP limits the addressee’s range of possible responses, and thus threatens his or her (negative) face.6 We thus reformulate our hypothesis as follows: the use of ODP by superiors changes when looking at the splits by gender, with female superiors using fewer ODPs than male superiors. We look further into the ANOVA analysis of the thread-level ODPCount treating Hierarchical Power and Gender as independent variables. Figure 1 shows the mean values of ODP counts in 6For a discussio</context>
<context position="30981" citStr="Herring, 2008" startWordPosition="5104" endWordPosition="5105"> gender ratios. The three gender environments we use are the following: • Hypothesis 2: Women use language to create and maintain social relations, for example, they use more small talk (based on a reported “stereotype” in (Holmes and Stubbe, 2003)). We first define more formally what we mean by “gender environment” (Section 6.1), and then investigate our hypothesis (Section 6.2). 6.1 The Notion of “Gender Environment” The notion of “gender environment” refers to the gender composition of a group who are communicating. In the sociolinguistic studies we have consulted (Holmes and Stubbe, 2003; Herring, 2008), the notion refers to a stable work group who interact regularly. Since we are interested in studying email conversations (threads), we adapt the notion to refer to a single thread at a time. Furthermore, we assume that a discourse participant makes communicative decisions based on (among other factors) his or her own gender, and based on the genders of the people he or she is communicating with in a given conversation (i.e., email thread). We therefore consider the “gender environment” to be specific to each discourse participant and to describe the other participants from his or her point o</context>
</contexts>
<marker>Herring, 2008</marker>
<rawString>Susan C Herring. 2008. Gender and power in online communication. The handbook of language and gender, page 202.</rawString>
</citation>
<citation valid="true">
<authors>
<author>Janet Holmes</author>
<author>Maria Stubbe</author>
</authors>
<title>feminine workplaces: stereotype and reality. The handbook of language and gender,</title>
<date>2003</date>
<pages>572--599</pages>
<contexts>
<context position="4817" citStr="Holmes and Stubbe, 2003" startWordPosition="743" endWordPosition="746">irst contribution) in Section 3. Section 4 defines the problem of predicting power and the various dimensions of interaction we analyze. We turn to our second contribution, the analysis of the data, in Sections 5 and 6. Section 7 describes our third contribution, the machine learning experiments using genderrelated features in the prediction of hierarchical power. We then conclude and discuss future work. 2 Related Work There is much sociolinguistic background related to gender and language use, some of it specifically related to language use in the work environment (Kendall and Tannen, 1997; Holmes and Stubbe, 2003; Kendall, 2003; Herring, 2008). We do not provide a full discussion of this work for lack of space, but single out one paper which has particularly influenced our work. Holmes and Stubbe (2003) provide two case studies that do not look at the differences between male and female managers’ communication, but at the difference between female managers’ communication in more heavily female vs. more heavily male environments. They find that, while female managers tend to break many stereotypes of “feminine” communication, they have different strategies in connecting with employees and exhibiting po</context>
<context position="30615" citStr="Holmes and Stubbe, 2003" startWordPosition="5044" endWordPosition="5047">er We now turn to gender environments and their relation to the expression of power in written dialogs. We again start with a hypothesis based on the sociolinguistic literature. three groups by setting thresholds on these percentages. Finer-grained gender environments resulted in partitions of the data with very few instances, since most of our data involves fairly balanced gender ratios. The three gender environments we use are the following: • Hypothesis 2: Women use language to create and maintain social relations, for example, they use more small talk (based on a reported “stereotype” in (Holmes and Stubbe, 2003)). We first define more formally what we mean by “gender environment” (Section 6.1), and then investigate our hypothesis (Section 6.2). 6.1 The Notion of “Gender Environment” The notion of “gender environment” refers to the gender composition of a group who are communicating. In the sociolinguistic studies we have consulted (Holmes and Stubbe, 2003; Herring, 2008), the notion refers to a stable work group who interact regularly. Since we are interested in studying email conversations (threads), we adapt the notion to refer to a single thread at a time. Furthermore, we assume that a discourse p</context>
</contexts>
<marker>Holmes, Stubbe, 2003</marker>
<rawString>Janet Holmes and Maria Stubbe. 2003. feminine workplaces: stereotype and reality. The handbook of language and gender, pages 572–599.</rawString>
</citation>
<citation valid="true">
<authors>
<author>Thorsten Joachims</author>
</authors>
<title>Making Large-Scale SVM Learning Practical.</title>
<date>1999</date>
<booktitle>Advances in Kernel Methods - Support Vector Learning,</booktitle>
<editor>In Bernhard Sch¨olkopf, Christopher J.C. Burges, and A. Smola, editors,</editor>
<publisher>MIT Press.</publisher>
<location>Cambridge, MA, USA.</location>
<contexts>
<context position="37277" citStr="Joachims, 1999" startWordPosition="6158" endWordPosition="6159">r prior work (Prabhakaran and Rambow, 2014). Given a thread t and a pair of participants (p1i p2) ∈ RIPPt, we want to automatically detect HP(p1i p2). We use the SVM-based supervised learning system from (Prabhakaran and Rambow, 2014) that can predict HP(p1 i p2) to be either superior or subordinate based on the interaction within a thread t for any pair of participants (p1i p2) ∈ RIPPt. The order of participants in (p1i p2) is fixed such that p1 is the sender of the first message in IMt(p1i p2). The power prediction system is built using the ClearTK (Ogren et al., 2008) wrapper for SVMLight (Joachims, 1999) package. It uses a quadratic kernel to capture feature-feature interactions, which is very important as we see in Section 5 and 6. We use the Train, Dev and Test subsets of the APGI subset of our corpus for our experiments. We use the related interacting participant pairs in threads from the Train set to train our models and optimize our performance on those from the Dev set. We report results on both Dev and Test sets. In addition to the features described in Section 4.2, the power prediction system presented in (Prabhakaran and Rambow, 2014) uses a lexical feature set (LEX) that captures wo</context>
</contexts>
<marker>Joachims, 1999</marker>
<rawString>Thorsten Joachims. 1999. Making Large-Scale SVM Learning Practical. In Bernhard Sch¨olkopf, Christopher J.C. Burges, and A. Smola, editors, Advances in Kernel Methods - Support Vector Learning, Cambridge, MA, USA. MIT Press.</rawString>
</citation>
<citation valid="true">
<authors>
<author>Shari Kendall</author>
<author>Deborah Tannen</author>
</authors>
<title>Gender and language in the workplace.</title>
<date>1997</date>
<booktitle>In Gender and Discourse,</booktitle>
<pages>81--105</pages>
<location>Sage, London.</location>
<contexts>
<context position="4792" citStr="Kendall and Tannen, 1997" startWordPosition="739" endWordPosition="742">tified Enron Corpus (our first contribution) in Section 3. Section 4 defines the problem of predicting power and the various dimensions of interaction we analyze. We turn to our second contribution, the analysis of the data, in Sections 5 and 6. Section 7 describes our third contribution, the machine learning experiments using genderrelated features in the prediction of hierarchical power. We then conclude and discuss future work. 2 Related Work There is much sociolinguistic background related to gender and language use, some of it specifically related to language use in the work environment (Kendall and Tannen, 1997; Holmes and Stubbe, 2003; Kendall, 2003; Herring, 2008). We do not provide a full discussion of this work for lack of space, but single out one paper which has particularly influenced our work. Holmes and Stubbe (2003) provide two case studies that do not look at the differences between male and female managers’ communication, but at the difference between female managers’ communication in more heavily female vs. more heavily male environments. They find that, while female managers tend to break many stereotypes of “feminine” communication, they have different strategies in connecting with em</context>
</contexts>
<marker>Kendall, Tannen, 1997</marker>
<rawString>Shari Kendall and Deborah Tannen. 1997. Gender and language in the workplace. In Gender and Discourse, pages 81–105. Sage, London.</rawString>
</citation>
<citation valid="true">
<authors>
<author>Shari Kendall</author>
</authors>
<title>Creating gendered demeanors of authority at work and at home. The handbook of language and gender,</title>
<date>2003</date>
<pages>600</pages>
<contexts>
<context position="4832" citStr="Kendall, 2003" startWordPosition="747" endWordPosition="748">tion 3. Section 4 defines the problem of predicting power and the various dimensions of interaction we analyze. We turn to our second contribution, the analysis of the data, in Sections 5 and 6. Section 7 describes our third contribution, the machine learning experiments using genderrelated features in the prediction of hierarchical power. We then conclude and discuss future work. 2 Related Work There is much sociolinguistic background related to gender and language use, some of it specifically related to language use in the work environment (Kendall and Tannen, 1997; Holmes and Stubbe, 2003; Kendall, 2003; Herring, 2008). We do not provide a full discussion of this work for lack of space, but single out one paper which has particularly influenced our work. Holmes and Stubbe (2003) provide two case studies that do not look at the differences between male and female managers’ communication, but at the difference between female managers’ communication in more heavily female vs. more heavily male environments. They find that, while female managers tend to break many stereotypes of “feminine” communication, they have different strategies in connecting with employees and exhibiting power in the two </context>
</contexts>
<marker>Kendall, 2003</marker>
<rawString>Shari Kendall. 2003. Creating gendered demeanors of authority at work and at home. The handbook of language and gender, page 600.</rawString>
</citation>
<citation valid="true">
<authors>
<author>Saif Mohammad</author>
<author>Tony Yang</author>
</authors>
<title>Tracking sentiment in mail: How genders differ on emotional axes.</title>
<date>2011</date>
<booktitle>In Proceedings of the 2nd Workshop on Computational Approaches to Subjectivity and Sentiment Analysis (WASSA 2.011),</booktitle>
<pages>70--79</pages>
<publisher>Association</publisher>
<institution>for Computational Linguistics.</institution>
<location>Portland, Oregon,</location>
<contexts>
<context position="7557" citStr="Mohammad and Yang (2011)" startWordPosition="1191" endWordPosition="1194"> and thread structure (Prabhakaran and Rambow, 2014). Our work is also on the Enron email corpus, and our baseline features are derived from some of this prior work. Researchers have also studied power and influence in other genres of interactions, such as online forums (DanescuNiculescu-Mizil et al., 2012; Biran et al., 2012), multi-party chats (Strzalkowski et al., 2012) and off-line interactions such as presidential debates (Nguyen et al., 2013; Prabhakaran et al., 2013; Prabhakaran et al., 2014). There is also some work within the NLP field on analyzing language use in relation to gender. Mohammad and Yang (2011) analyzed the way gender affects the expression of sentiments in text, while we are interested in how gender relates to manifestations of organizational power. For their study, they assigned gender for the core employees in the Enron email corpus based on whether the first name of the person was easily gender identifiable or not. If the person had an unfamiliar name or a name that could be of either gender, they marked his/her gender as unknown and excluded them from their study.1 For example, the gender of the employee Kay Mann was marked as unknown in their gender assignment. However, in our</context>
</contexts>
<marker>Mohammad, Yang, 2011</marker>
<rawString>Saif Mohammad and Tony Yang. 2011. Tracking sentiment in mail: How genders differ on emotional axes. In Proceedings of the 2nd Workshop on Computational Approaches to Subjectivity and Sentiment Analysis (WASSA 2.011), pages 70–79, Portland, Oregon, June. Association for Computational Linguistics.</rawString>
</citation>
<citation valid="true">
<authors>
<author>Viet-An Nguyen</author>
<author>Jordan Boyd-Graber</author>
<author>Philip Resnik</author>
<author>Deborah A Cai</author>
<author>Jennifer E Midberry</author>
<author>Yuanxin Wang</author>
</authors>
<title>Modeling topic control to detect influence in conversations using nonparametric topic models.</title>
<date>2013</date>
<booktitle>Machine Learning,</booktitle>
<pages>1--41</pages>
<contexts>
<context position="7384" citStr="Nguyen et al., 2013" startWordPosition="1162" endWordPosition="1165">t power. Later studies have used more complex linguistic and structural features, such as formality (Peterson et al., 2011), dialog acts (Prabhakaran and Rambow, 2013), and thread structure (Prabhakaran and Rambow, 2014). Our work is also on the Enron email corpus, and our baseline features are derived from some of this prior work. Researchers have also studied power and influence in other genres of interactions, such as online forums (DanescuNiculescu-Mizil et al., 2012; Biran et al., 2012), multi-party chats (Strzalkowski et al., 2012) and off-line interactions such as presidential debates (Nguyen et al., 2013; Prabhakaran et al., 2013; Prabhakaran et al., 2014). There is also some work within the NLP field on analyzing language use in relation to gender. Mohammad and Yang (2011) analyzed the way gender affects the expression of sentiments in text, while we are interested in how gender relates to manifestations of organizational power. For their study, they assigned gender for the core employees in the Enron email corpus based on whether the first name of the person was easily gender identifiable or not. If the person had an unfamiliar name or a name that could be of either gender, they marked his/</context>
</contexts>
<marker>Nguyen, Boyd-Graber, Resnik, Cai, Midberry, Wang, 2013</marker>
<rawString>Viet-An Nguyen, Jordan Boyd-Graber, Philip Resnik, Deborah A. Cai, Jennifer E. Midberry, and Yuanxin Wang. 2013. Modeling topic control to detect influence in conversations using nonparametric topic models. Machine Learning, pages 1–41.</rawString>
</citation>
<citation valid="true">
<authors>
<author>Philip V Ogren</author>
<author>Philipp G Wetzler</author>
<author>Steven Bethard</author>
</authors>
<title>ClearTK: A UIMA toolkit for statistical natural language processing.</title>
<date>2008</date>
<booktitle>In Towards Enhanced Interoperability for Large HLT Systems: UIMA for NLP workshop at Language Resources and Evaluation Conference (LREC).</booktitle>
<contexts>
<context position="37239" citStr="Ogren et al., 2008" startWordPosition="6151" endWordPosition="6154">e power prediction problem presented in our prior work (Prabhakaran and Rambow, 2014). Given a thread t and a pair of participants (p1i p2) ∈ RIPPt, we want to automatically detect HP(p1i p2). We use the SVM-based supervised learning system from (Prabhakaran and Rambow, 2014) that can predict HP(p1 i p2) to be either superior or subordinate based on the interaction within a thread t for any pair of participants (p1i p2) ∈ RIPPt. The order of participants in (p1i p2) is fixed such that p1 is the sender of the first message in IMt(p1i p2). The power prediction system is built using the ClearTK (Ogren et al., 2008) wrapper for SVMLight (Joachims, 1999) package. It uses a quadratic kernel to capture feature-feature interactions, which is very important as we see in Section 5 and 6. We use the Train, Dev and Test subsets of the APGI subset of our corpus for our experiments. We use the related interacting participant pairs in threads from the Train set to train our models and optimize our performance on those from the Dev set. We report results on both Dev and Test sets. In addition to the features described in Section 4.2, the power prediction system presented in (Prabhakaran and Rambow, 2014) uses a lexi</context>
</contexts>
<marker>Ogren, Wetzler, Bethard, 2008</marker>
<rawString>Philip V. Ogren, Philipp G. Wetzler, and Steven Bethard. 2008. ClearTK: A UIMA toolkit for statistical natural language processing. In Towards Enhanced Interoperability for Large HLT Systems: UIMA for NLP workshop at Language Resources and Evaluation Conference (LREC).</rawString>
</citation>
<citation valid="true">
<authors>
<author>Adinoyi Omuya</author>
<author>Vinodkumar Prabhakaran</author>
<author>Owen Rambow</author>
</authors>
<title>Improving the quality of minority class identification in dialog act tagging.</title>
<date>2013</date>
<booktitle>In Proceedings of the 2013 Conference of the North American Chapter of the Association for Computational Linguistics: Human Language Technologies,</booktitle>
<pages>802--807</pages>
<publisher>Association</publisher>
<institution>for Computational Linguistics.</institution>
<location>Atlanta, Georgia,</location>
<contexts>
<context position="24040" citStr="Omuya et al., 2013" startWordPosition="3971" endWordPosition="3974">features denoting whether p added or removed people when responding to a message (AddPerson and RemovePerson), average number of replies received per message sent by p (ReplyRate) and average number of replies received from the other person of the pair to messages where he/she was a To recipient (ReplyRateWithinPair). ReplyRateWithinPair applies only to IMt(p1, p2). DIA: We use dialog acts (DA) and overt displays of power (ODP) tags to model the structure of interactions within the message content. We obtain DA and ODP tags using automatic taggers trained on manual annotations. The DA tagger (Omuya et al., 2013) obtained an accuracy of 92%. The ODP tagger (Prabhakaran et al., 2012) obtained an accuracy of 96% and F-measure of 54%. The DA tagger labels each sentence to be one of the 4 dialog acts: Request Action, Request Information, Inform, and Conventional. The ODP Tagger identifies sentences (mostly requests) that express additional constraints on their addressee, beyond those introduced by the dialog act. For example, the sentence “Please come to my office right now” is considered as an ODP, while “It would be great if you could come to my office now” is not, even though both issue the same reques</context>
</contexts>
<marker>Omuya, Prabhakaran, Rambow, 2013</marker>
<rawString>Adinoyi Omuya, Vinodkumar Prabhakaran, and Owen Rambow. 2013. Improving the quality of minority class identification in dialog act tagging. In Proceedings of the 2013 Conference of the North American Chapter of the Association for Computational Linguistics: Human Language Technologies, pages 802–807, Atlanta, Georgia, June. Association for Computational Linguistics.</rawString>
</citation>
<citation valid="true">
<authors>
<author>Kelly Peterson</author>
<author>Matt Hohensee</author>
<author>Fei Xia</author>
</authors>
<title>Email formality in the workplace: A case study on the enron corpus.</title>
<date>2011</date>
<booktitle>In Proceedings of the Workshop on Language in Social Media (LSM 2011),</booktitle>
<pages>86--95</pages>
<publisher>Association</publisher>
<institution>for Computational Linguistics.</institution>
<location>Portland, Oregon,</location>
<contexts>
<context position="6888" citStr="Peterson et al., 2011" startWordPosition="1085" endWordPosition="1088">, there has been substantial research exploring language use and power. A large number of these studies are performed in the domain of organizational email where the notion of power is well defined in terms of organizational hierarchy. It is also aided by the availability of the moderately large Enron email corpus which captures email interactions in an organizational setting. Earlier approaches used simple lexical features alone (e.g. (Bramsen et al., 2011; Gilbert, 2012)) as a means to predict power. Later studies have used more complex linguistic and structural features, such as formality (Peterson et al., 2011), dialog acts (Prabhakaran and Rambow, 2013), and thread structure (Prabhakaran and Rambow, 2014). Our work is also on the Enron email corpus, and our baseline features are derived from some of this prior work. Researchers have also studied power and influence in other genres of interactions, such as online forums (DanescuNiculescu-Mizil et al., 2012; Biran et al., 2012), multi-party chats (Strzalkowski et al., 2012) and off-line interactions such as presidential debates (Nguyen et al., 2013; Prabhakaran et al., 2013; Prabhakaran et al., 2014). There is also some work within the NLP field on a</context>
</contexts>
<marker>Peterson, Hohensee, Xia, 2011</marker>
<rawString>Kelly Peterson, Matt Hohensee, and Fei Xia. 2011. Email formality in the workplace: A case study on the enron corpus. In Proceedings of the Workshop on Language in Social Media (LSM 2011), pages 86–95, Portland, Oregon, June. Association for Computational Linguistics.</rawString>
</citation>
<citation valid="true">
<authors>
<author>Vinodkumar Prabhakaran</author>
<author>Owen Rambow</author>
</authors>
<title>Written dialog and social power: Manifestations of different types of power in dialog behavior.</title>
<date>2013</date>
<journal>Asian Federation of Natural Language Processing.</journal>
<booktitle>In Proceedings of the IJCNLP,</booktitle>
<pages>216--224</pages>
<location>Nagoya, Japan,</location>
<contexts>
<context position="6932" citStr="Prabhakaran and Rambow, 2013" startWordPosition="1091" endWordPosition="1095"> exploring language use and power. A large number of these studies are performed in the domain of organizational email where the notion of power is well defined in terms of organizational hierarchy. It is also aided by the availability of the moderately large Enron email corpus which captures email interactions in an organizational setting. Earlier approaches used simple lexical features alone (e.g. (Bramsen et al., 2011; Gilbert, 2012)) as a means to predict power. Later studies have used more complex linguistic and structural features, such as formality (Peterson et al., 2011), dialog acts (Prabhakaran and Rambow, 2013), and thread structure (Prabhakaran and Rambow, 2014). Our work is also on the Enron email corpus, and our baseline features are derived from some of this prior work. Researchers have also studied power and influence in other genres of interactions, such as online forums (DanescuNiculescu-Mizil et al., 2012; Biran et al., 2012), multi-party chats (Strzalkowski et al., 2012) and off-line interactions such as presidential debates (Nguyen et al., 2013; Prabhakaran et al., 2013; Prabhakaran et al., 2014). There is also some work within the NLP field on analyzing language use in relation to gender.</context>
</contexts>
<marker>Prabhakaran, Rambow, 2013</marker>
<rawString>Vinodkumar Prabhakaran and Owen Rambow. 2013. Written dialog and social power: Manifestations of different types of power in dialog behavior. In Proceedings of the IJCNLP, pages 216–224, Nagoya, Japan, October. Asian Federation of Natural Language Processing.</rawString>
</citation>
<citation valid="true">
<authors>
<author>Vinodkumar Prabhakaran</author>
<author>Owen Rambow</author>
</authors>
<title>Predicting power relations between participants in written dialog from a single thread.</title>
<date>2014</date>
<booktitle>In Proceedings of the 52nd Annual Meeting of the Association for Computational Linguistics (Volume 2: Short Papers),</booktitle>
<pages>339--344</pages>
<publisher>Association</publisher>
<institution>for Computational Linguistics.</institution>
<location>Baltimore, Maryland,</location>
<contexts>
<context position="6985" citStr="Prabhakaran and Rambow, 2014" startWordPosition="1099" endWordPosition="1102">these studies are performed in the domain of organizational email where the notion of power is well defined in terms of organizational hierarchy. It is also aided by the availability of the moderately large Enron email corpus which captures email interactions in an organizational setting. Earlier approaches used simple lexical features alone (e.g. (Bramsen et al., 2011; Gilbert, 2012)) as a means to predict power. Later studies have used more complex linguistic and structural features, such as formality (Peterson et al., 2011), dialog acts (Prabhakaran and Rambow, 2013), and thread structure (Prabhakaran and Rambow, 2014). Our work is also on the Enron email corpus, and our baseline features are derived from some of this prior work. Researchers have also studied power and influence in other genres of interactions, such as online forums (DanescuNiculescu-Mizil et al., 2012; Biran et al., 2012), multi-party chats (Strzalkowski et al., 2012) and off-line interactions such as presidential debates (Nguyen et al., 2013; Prabhakaran et al., 2013; Prabhakaran et al., 2014). There is also some work within the NLP field on analyzing language use in relation to gender. Mohammad and Yang (2011) analyzed the way gender aff</context>
<context position="19888" citStr="Prabhakaran and Rambow, 2014" startWordPosition="3251" endWordPosition="3254">tem succeeded in assigning the genders of all participants, including senders and all recipients (To and CC). For the analysis and experiments presented in the rest of this paper, we use 17,788 threads from this APGI subset, excluding the remaining 242 threads that were used for previous manual annotation efforts. 4 Manifestations of Power We use the gender information of the participants to investigate how the gender of the sender and recipients affect the manifestations of hierarchical power in interactions. In order to do this, we use the interaction analysis framework from our prior work (Prabhakaran and Rambow, 2014). In this section, we give a brief overview of the problem formulation and the structural features we used. 4.1 Hierarchically Related Interacting Pairs Let t denote an email thread and Mt denote the set of all messages in t. Also, let Pt be the set of all participants in t, i.e., the union of senders and recipients (To and CC) of all messages in Mt. We are interested in analyzing the power relations between pairs of participants who interact within a given email thread. Not every pair of participants (p1, p2) E Pt x Pt interact with one another within t. Let IMt(p1, p2) denote the set of Inte</context>
<context position="36705" citStr="Prabhakaran and Rambow, 2014" startWordPosition="6050" endWordPosition="6053">al DAs, we also need to look at the power status of the writer. In fact only subordinates in female environments use more conventional DAs than any other group (as defined by power status and gender environment). While our hypothesis is not fully verified, we interpret the results to mean that subordinates are more comfortable in female environments to use a style of communication which includes more conventional DAs than outside the female environments. 7 Predicting Power in Participant Pairs In this section, we use the formulation of the power prediction problem presented in our prior work (Prabhakaran and Rambow, 2014). Given a thread t and a pair of participants (p1i p2) ∈ RIPPt, we want to automatically detect HP(p1i p2). We use the SVM-based supervised learning system from (Prabhakaran and Rambow, 2014) that can predict HP(p1 i p2) to be either superior or subordinate based on the interaction within a thread t for any pair of participants (p1i p2) ∈ RIPPt. The order of participants in (p1i p2) is fixed such that p1 is the sender of the first message in IMt(p1i p2). The power prediction system is built using the ClearTK (Ogren et al., 2008) wrapper for SVMLight (Joachims, 1999) package. It uses a quadrati</context>
<context position="38718" citStr="Prabhakaran and Rambow, 2014" startWordPosition="6398" endWordPosition="6402"> replaced with their POS tags. We add two gender-based feature sets: GEN containing the gender of both persons of the pair and ENV containing the gender environment feature. 1973 Table 6 presents the results obtained using various feature combinations. We experimented using all subsets of {LEX, THRSTR, THRMETA, DIA, GEN, ENV } on the Dev set; we report the most interesting results here. The majority baseline (subordinate) obtains an accuracy of 55.8%. Using the gender-based features alone performs only slightly better than the majority baseline. We use the best performing feature subset from (Prabhakaran and Rambow, 2014) (LEX + THRMETA) as another baseline, which obtains an accuracy of 68.2%. Adding the GEN features improves the performance to 70.6%. Further adding the ENV features improves the performance, but only marginally to 70.7% (our overall best result, an improvment of 2.4% points). The best performing feature set without using LEX was the combination of DIA, THRMETA and GEN (67.3%). Removing the gender features from this reduced the performance to 64.6%. Similarly, the best performing feature set which do not use the content of emails at all was THRSTR + THRMETA + GEN (66.6). Removing the gender fea</context>
<context position="40500" citStr="Prabhakaran and Rambow, 2014" startWordPosition="6693" endWordPosition="6696">s decreases again. Nonetheless, we take these results as validation of the claim that gender-based features enhance the value of other features in the task of predicting power relations. We performed another experiment where we partitioned the data into two subsets according to the gender of the first person of the pair and trained two separate models to predict power. At test time, we chose the appropriate model based on the gender of the first person of the pair. However, this did not improve the performance. On our blind test set, the majority baseline obtains an accuracy of 57.9% and the (Prabhakaran and Rambow, 2014) baseline obtains an accuracy of 68.9%. On adding the gender-based features, the accuracy of the system improves to 70.2%. Description Accuracy Majority (Always Subordinate) 55.83 GEN 57.59 GEN + ENV 57.59 Baseline (LEX + THRMETA) 68.24 Baseline (LEX + THRMETA) + GEN 70.56 Baseline (LEX + THRMETA) + GEN + ENV 70.74 DIA + THRMETA + GEN 67.31 DIA + THRMETA 64.63 THRSTR + THRMETA + GEN 66.57 THRSTR + THRMETA 62.96 Table 6: Accuracies on feature subsets (Dev set). THRMETA: meta-data; THRSTR: structural; DIA: dialog-act; GEN: gender; ENV: gender environment; LEX: ngrams; 8 Conclusion We presented a</context>
</contexts>
<marker>Prabhakaran, Rambow, 2014</marker>
<rawString>Vinodkumar Prabhakaran and Owen Rambow. 2014. Predicting power relations between participants in written dialog from a single thread. In Proceedings of the 52nd Annual Meeting of the Association for Computational Linguistics (Volume 2: Short Papers), pages 339–344, Baltimore, Maryland, June. Association for Computational Linguistics.</rawString>
</citation>
<citation valid="true">
<authors>
<author>Vinodkumar Prabhakaran</author>
<author>Owen Rambow</author>
<author>Mona Diab</author>
</authors>
<title>Predicting Overt Display of Power in Written Dialogs.</title>
<date>2012</date>
<booktitle>In Human Language Technologies: The 2012 Annual Conference of the North American Chapter of the Association for Computational Linguistics,</booktitle>
<publisher>Association</publisher>
<institution>for Computational Linguistics.</institution>
<location>Montreal, Canada,</location>
<contexts>
<context position="24111" citStr="Prabhakaran et al., 2012" startWordPosition="3983" endWordPosition="3986">ng to a message (AddPerson and RemovePerson), average number of replies received per message sent by p (ReplyRate) and average number of replies received from the other person of the pair to messages where he/she was a To recipient (ReplyRateWithinPair). ReplyRateWithinPair applies only to IMt(p1, p2). DIA: We use dialog acts (DA) and overt displays of power (ODP) tags to model the structure of interactions within the message content. We obtain DA and ODP tags using automatic taggers trained on manual annotations. The DA tagger (Omuya et al., 2013) obtained an accuracy of 92%. The ODP tagger (Prabhakaran et al., 2012) obtained an accuracy of 96% and F-measure of 54%. The DA tagger labels each sentence to be one of the 4 dialog acts: Request Action, Request Information, Inform, and Conventional. The ODP Tagger identifies sentences (mostly requests) that express additional constraints on their addressee, beyond those introduced by the dialog act. For example, the sentence “Please come to my office right now” is considered as an ODP, while “It would be great if you could come to my office now” is not, even though both issue the same request. For more details on ODP, we refer the Feature Name Mean(fXIMt)|X = M</context>
<context position="25648" citStr="Prabhakaran et al., 2012" startWordPosition="4235" endWordPosition="4238">0.34 0.33 0.34 0.39 MsgCount*** 0.92 0.61 0.93 0.91 MsgRatio*** 0.33 0.23 0.33 0.32 TokenCount 76.5 41.0 102.0 54.3 TokenRatio 0.38 0.23 0.40 0.27 TokenPerMsg*** 90.2 67.9 118.2 53.2 DIAPR Conventional 0.55 0.43 0.64 0.56 Inform 3.50 1.96 4.51 2.53 ReqAction** 0.07 0.06 0.05 0.10 ReqInform 0.29 0.21 0.20 0.16 DanglingReq% 0.06 0.12 0.07 0.18 ODPCount*** 0.10 0.07 0.09 0.13 Table 5: ANOVA results and group means for Hierarchical Power and Gender Fsub: Female subordinates; Fsup: Female superiors; Msub: Male subordinates; Msup: Male superiors; * (p &lt; .05); ** (p &lt; .01); *** (p &lt; .001) reader to (Prabhakaran et al., 2012). We use 5 features: ReqAction, ReqInform, Inform, Conventional, and ODPCount to capture the number of sentences in messages sent by p that have each of these labels. We also use a feature to capture the number of p’s messages with a request that did not get a reply, i.e., dangling request percentage (DanglingReq%), over all messages sent by p. 5 Gender and Power In this subsection, we analyze the impact of gender on the expression of power in email. We perform an ANOVA test on all features described in Section 4.2 keeping both Hierarchical Power and Gender as independent variables. We perform</context>
</contexts>
<marker>Prabhakaran, Rambow, Diab, 2012</marker>
<rawString>Vinodkumar Prabhakaran, Owen Rambow, and Mona Diab. 2012. Predicting Overt Display of Power in Written Dialogs. In Human Language Technologies: The 2012 Annual Conference of the North American Chapter of the Association for Computational Linguistics, Montreal, Canada, June. Association for Computational Linguistics.</rawString>
</citation>
<citation valid="true">
<authors>
<author>Vinodkumar Prabhakaran</author>
<author>Ajita John</author>
<author>Dor´ee D Seligmann</author>
</authors>
<title>Who had the upper hand? ranking participants of interactions based on their relative power.</title>
<date>2013</date>
<journal>Asian Federation of Natural Language Processing.</journal>
<booktitle>In Proceedings of the IJCNLP,</booktitle>
<pages>365--373</pages>
<location>Nagoya, Japan,</location>
<contexts>
<context position="7410" citStr="Prabhakaran et al., 2013" startWordPosition="1166" endWordPosition="1169">s have used more complex linguistic and structural features, such as formality (Peterson et al., 2011), dialog acts (Prabhakaran and Rambow, 2013), and thread structure (Prabhakaran and Rambow, 2014). Our work is also on the Enron email corpus, and our baseline features are derived from some of this prior work. Researchers have also studied power and influence in other genres of interactions, such as online forums (DanescuNiculescu-Mizil et al., 2012; Biran et al., 2012), multi-party chats (Strzalkowski et al., 2012) and off-line interactions such as presidential debates (Nguyen et al., 2013; Prabhakaran et al., 2013; Prabhakaran et al., 2014). There is also some work within the NLP field on analyzing language use in relation to gender. Mohammad and Yang (2011) analyzed the way gender affects the expression of sentiments in text, while we are interested in how gender relates to manifestations of organizational power. For their study, they assigned gender for the core employees in the Enron email corpus based on whether the first name of the person was easily gender identifiable or not. If the person had an unfamiliar name or a name that could be of either gender, they marked his/her gender as unknown and </context>
</contexts>
<marker>Prabhakaran, John, Seligmann, 2013</marker>
<rawString>Vinodkumar Prabhakaran, Ajita John, and Dor´ee D. Seligmann. 2013. Who had the upper hand? ranking participants of interactions based on their relative power. In Proceedings of the IJCNLP, pages 365–373, Nagoya, Japan, October. Asian Federation of Natural Language Processing.</rawString>
</citation>
<citation valid="true">
<authors>
<author>Vinodkumar Prabhakaran</author>
<author>Ashima Arora</author>
<author>Owen Rambow</author>
</authors>
<title>Power of confidence: How poll scores impact topic dynamics in political debates.</title>
<date>2014</date>
<booktitle>In Proceedings of the ACL 2014 Workshop on Language Technologies and Computational Social Science, page 49,</booktitle>
<publisher>Association</publisher>
<institution>for Computational Linguistics.</institution>
<location>Baltimore, MD, USA,</location>
<contexts>
<context position="7437" citStr="Prabhakaran et al., 2014" startWordPosition="1170" endWordPosition="1173">inguistic and structural features, such as formality (Peterson et al., 2011), dialog acts (Prabhakaran and Rambow, 2013), and thread structure (Prabhakaran and Rambow, 2014). Our work is also on the Enron email corpus, and our baseline features are derived from some of this prior work. Researchers have also studied power and influence in other genres of interactions, such as online forums (DanescuNiculescu-Mizil et al., 2012; Biran et al., 2012), multi-party chats (Strzalkowski et al., 2012) and off-line interactions such as presidential debates (Nguyen et al., 2013; Prabhakaran et al., 2013; Prabhakaran et al., 2014). There is also some work within the NLP field on analyzing language use in relation to gender. Mohammad and Yang (2011) analyzed the way gender affects the expression of sentiments in text, while we are interested in how gender relates to manifestations of organizational power. For their study, they assigned gender for the core employees in the Enron email corpus based on whether the first name of the person was easily gender identifiable or not. If the person had an unfamiliar name or a name that could be of either gender, they marked his/her gender as unknown and excluded them from their st</context>
</contexts>
<marker>Prabhakaran, Arora, Rambow, 2014</marker>
<rawString>Vinodkumar Prabhakaran, Ashima Arora, and Owen Rambow. 2014. Power of confidence: How poll scores impact topic dynamics in political debates. In Proceedings of the ACL 2014 Workshop on Language Technologies and Computational Social Science, page 49, Baltimore, MD, USA, June. Association for Computational Linguistics.</rawString>
</citation>
<citation valid="true">
<authors>
<author>Tomek Strzalkowski</author>
<author>Samira Shaikh</author>
<author>Ting Liu</author>
<author>George Aaron Broadwell</author>
<author>Jenny Stromer-Galley</author>
<author>Sarah Taylor</author>
<author>Umit Boz</author>
<author>Veena Ravishankar</author>
<author>Xiaoai Ren</author>
</authors>
<title>Modeling leadership and influence in multi-party online discourse.</title>
<date>2012</date>
<booktitle>In Proceedings of COLING,</booktitle>
<pages>2535--2552</pages>
<location>Mumbai, India,</location>
<contexts>
<context position="7308" citStr="Strzalkowski et al., 2012" startWordPosition="1151" endWordPosition="1154">al features alone (e.g. (Bramsen et al., 2011; Gilbert, 2012)) as a means to predict power. Later studies have used more complex linguistic and structural features, such as formality (Peterson et al., 2011), dialog acts (Prabhakaran and Rambow, 2013), and thread structure (Prabhakaran and Rambow, 2014). Our work is also on the Enron email corpus, and our baseline features are derived from some of this prior work. Researchers have also studied power and influence in other genres of interactions, such as online forums (DanescuNiculescu-Mizil et al., 2012; Biran et al., 2012), multi-party chats (Strzalkowski et al., 2012) and off-line interactions such as presidential debates (Nguyen et al., 2013; Prabhakaran et al., 2013; Prabhakaran et al., 2014). There is also some work within the NLP field on analyzing language use in relation to gender. Mohammad and Yang (2011) analyzed the way gender affects the expression of sentiments in text, while we are interested in how gender relates to manifestations of organizational power. For their study, they assigned gender for the core employees in the Enron email corpus based on whether the first name of the person was easily gender identifiable or not. If the person had a</context>
</contexts>
<marker>Strzalkowski, Shaikh, Liu, Broadwell, Stromer-Galley, Taylor, Boz, Ravishankar, Ren, 2012</marker>
<rawString>Tomek Strzalkowski, Samira Shaikh, Ting Liu, George Aaron Broadwell, Jenny Stromer-Galley, Sarah Taylor, Umit Boz, Veena Ravishankar, and Xiaoai Ren. 2012. Modeling leadership and influence in multi-party online discourse. In Proceedings of COLING, pages 2535–2552, Mumbai, India, December. The COLING 2012 Organizing Committee.</rawString>
</citation>
<citation valid="true">
<authors>
<author>Jen-Yuan Yeh</author>
<author>Aaron Harnly</author>
</authors>
<title>Email thread reassembly using similarity matching.</title>
<date>2006</date>
<booktitle>In CEAS 2006 - The Third Conference on Email and AntiSpam,</booktitle>
<location>Mountain View, California, USA, Mountain View, California, USA,</location>
<contexts>
<context position="9164" citStr="Yeh and Harnly (2006)" startWordPosition="1460" endWordPosition="1463">er in size. Corney et al. (2002) use around 4K emails from 325 gender identified authors. Cheng et al. (2011) use around 9K emails from 108 gender identified authors. Deitrick et al. (2012) use around 18K emails from 144 gender 1http://www.saifmohammad.com/WebDocs/dir-emailgender.txt 1966 identified authors. The dataset we offer is much larger in size, with around 97K emails whose authors are gender identified. We believe that our resource will aid further research in this area. 3 Gender Identified Enron Corpus 3.1 Enron Corpus In our work, we use the version of Enron email corpus released by Yeh and Harnly (2006). The corpus contains emails from the mailboxes of 145 core employees who held top managerial positions within Enron at the time of bankruptcy. Yeh and Harnly (2006) preprocessed the corpus to combine multiple email addresses belonging to the same entity and identify each entity in the corpus with a unique identifier. The corpus contains a total of 111,933 messages. This version of the corpus has been enriched later by Agarwal et al. (2012) with gold organizational power relations, manually determined using information from Enron organizational charts. It includes relations of 1,518 employees </context>
<context position="14337" citStr="Yeh and Harnly (2006)" startWordPosition="2322" endWordPosition="2325">nder of the name to be the one with the higher percentage, if the ambiguity score is below a threshold AST. �M, if AS(n) &lt; AST and mp(n) &gt; fp(n) G(n) = F, if AS(n) &lt; AST and mp(n) &lt; fp(n) I, if AS(n) &gt; AST Around 88% of the names in the SSA dataset have AS(n) = 0. We choose a very conservative threshold of AST = 10 for our gender assignments, which assigns gender to around 93% names in the SSA dataset.5 3.3.2 Identifying the First Name Each discourse participant in our corpus has at least one email address and zero or more names associated with it. The name field is automatically assembled by Yeh and Harnly (2006), where they captured the different names from email headers, which are populated from individual email clients and do not follow a standard format. Not all discourse participants are human; some may refer to organizational groups (e.g., HR Department) or anonymous corporate email accounts (e.g., a webmaster account, do-not-reply address etc.). The name field may sometimes be empty, contain multiple names, contain an email address, or show other irregularities. Hence, it is nontrivial to determine the first name of our discourse participants. We used the heuristics below to extract the most li</context>
</contexts>
<marker>Yeh, Harnly, 2006</marker>
<rawString>Jen-Yuan Yeh and Aaron Harnly. 2006. Email thread reassembly using similarity matching. In CEAS 2006 - The Third Conference on Email and AntiSpam, July 27-28, 2006, Mountain View, California, USA, Mountain View, California, USA, July.</rawString>
</citation>
</citationList>
</algorithm>
</algorithms>
<?xml version="1.0" encoding="UTF-8"?>
<algorithms version="110505">
<algorithm name="SectLabel" version="110505">
<variant no="0" confidence="0.002403">
<title confidence="0.9989395">
Neural Network Based Bilingual Language Model Growing
for Statistical Machine Translation
</title>
<author confidence="0.997846">
Rui Wang&apos;,3,, Hai Zhao&apos;,3, Bao-Liang Lu&apos;,3, Masao Utiyama2 and Eiichro Sumita2
</author>
<affiliation confidence="0.989387">
&apos;Center for Brain-Like Computing and Machine Intelligence,
Department of Computer Science and Engineering,
Shanghai Jiao Tong University, Shanghai, 200240, China
2Multilingual Translation Laboratory, MASTAR Project,
National Institute of Information and Communications Technology,
3-5 Hikaridai, Keihanna Science City, Kyoto, 619-0289, Japan
3Key Laboratory of Shanghai Education Commission for Intelligent Interaction
and Cognitive Engineering, Shanghai Jiao Tong University, Shanghai, 200240, China
</affiliation>
<email confidence="0.972735">
wangrui.nlp@gmail.com, {zhaohai, blu}@cs.sjtu.edu.cn,
{mutiyama, eiichiro.sumita}@nict.go.jp
</email>
<sectionHeader confidence="0.995582" genericHeader="abstract">
Abstract
</sectionHeader>
<bodyText confidence="0.9999635">
Since larger n-gram Language Model
(LM) usually performs better in Statistical
Machine Translation (SMT), how to con-
struct efficient large LM is an important
topic in SMT. However, most of the ex-
isting LM growing methods need an extra
monolingual corpus, where additional LM
adaption technology is necessary. In this
paper, we propose a novel neural network
based bilingual LM growing method, only
using the bilingual parallel corpus in SMT.
The results show that our method can im-
prove both the perplexity score for LM e-
valuation and BLEU score for SMT, and
significantly outperforms the existing LM
growing methods without extra corpus.
</bodyText>
<sectionHeader confidence="0.998952" genericHeader="introduction">
1 Introduction
</sectionHeader>
<bodyText confidence="0.989994096153846">
‘Language Model (LM) Growing’ refers to adding
n-grams outside the corpus together with their
probabilities into the original LM. This operation
is useful as it can make LM perform better through
letting it become larger and larger, by only using a
small training corpus.
There are various methods for adding n-grams
selected by different criteria from a monolingual
corpus (Ristad and Thomas, 1995; Niesler and
Woodland, 1996; Siu and Ostendorf, 2000; Si-
ivola et al., 2007). However, all of these approach-
es need additional corpora. Meanwhile the extra
corpora from different domains will not result in
better LMs (Clarkson and Robinson, 1997; Iyer et
al., 1997; Bellegarda, 2004; Koehn and Schroeder,
*Part of this work was done as Rui Wang visited in NICT.
2007). In addition, it is very difficult or even im-
possible to collect an extra large corpus for some
special domains such as the TED corpus (Cettolo
et al., 2012) or for some rare languages. There-
fore, to improve the performance of LMs, without
assistance of extra corpus, is one of important re-
search topics in SMT.
Recently, Continues Space Language Model
(CSLM), especially Neural Network based Lan-
guage Model (NNLM) (Bengio et al., 2003;
Schwenk, 2007; Mikolov et al., 2010; Le et al.,
2011), is being actively used in SMT (Schwenk
et al., 2006; Son et al., 2010; Schwenk, 2010;
Schwenk et al., 2012; Son et al., 2012; Niehues
and Waibel, 2012). One of the main advantages
of CSLM is that it can more accurately predic-
t the probabilities of the n-grams, which are not in
the training corpus. However, in practice, CSLM-
s have not been widely used in the current SMT
systems, due to their too high computational cost.
Vaswani and colleagues (2013) propose a
method for reducing the training cost of CSLM
and apply it to SMT decoder. However, they do
not show their improvement for decoding speed,
and their method is still slower than the n-gram
LM. There are several other methods for attempt-
ing to implement neural network based LM or
translation model for SMT (Devlin et al., 2014;
Liu et al., 2014; Auli et al., 2013). However, the
decoding speed using n-gram LM is still state-of-
the-art one. Some approaches calculate the prob-
abilities of the n-grams n-grams before decoding,
and store them in the n-gram format (Wang et al.,
2013a; Arsoy et al., 2013; Arsoy et al., 2014). The
‘converted CSLM’ can be directly used in SMT.
Though more n-grams which are not in the train-
</bodyText>
<page confidence="0.984087">
189
</page>
<note confidence="0.9107185">
Proceedings of the 2014 Conference on Empirical Methods in Natural Language Processing (EMNLP), pages 189–195,
October 25-29, 2014, Doha, Qatar. c�2014 Association for Computational Linguistics
</note>
<bodyText confidence="0.999068611111111">
ing corpus can be generated by using some of
these ‘converting’ methods, these methods only
consider the monolingual information, and do not
take the bilingual information into account.
We observe that the translation output of a
phrase-based SMT system is concatenation of
phrases from the phrase table, whose probabilities
can be calculated by CSLM. Based on this obser-
vation, a novel neural network based bilingual LM
growing method is proposed using the ‘connecting
phrases’. The remainder of this paper is organized
as follows: In Section 2, we will review the exist-
ing CSLM converting methods. The new neural
network based bilingual LM growing method will
be proposed in Section 3. In Section 4, the exper-
iments will be conducted and the results will be
analyzed. We will conclude our work in Section
5.
</bodyText>
<sectionHeader confidence="0.990855" genericHeader="method">
2 Existing CSLM Converting Methods
</sectionHeader>
<bodyText confidence="0.996861428571429">
Traditional Backoff N-gram LMs (BNLMs) have
been widely used in many NLP tasks (Zhang and
Zhao, 2013; Jia and Zhao, 2014; Zhao et al., 2013;
Zhang et al., 2012; Xu and Zhao, 2012; Wang et
al., 2013b; Jia and Zhao, 2013; Wang et al., 2014).
Recently, CSLMs become popular because they
can obtain more accurate probability estimation.
</bodyText>
<subsectionHeader confidence="0.996145">
2.1 Continues Space Language Model
</subsectionHeader>
<bodyText confidence="0.999779958333333">
A CSLM implemented in a multi-layer neural net-
work contains four layers: the input layer projects
(first layer) all words in the context hi onto the
projection layer (second layer); the hidden layer
(third layer) and the output layer (fourth layer)
achieve the non-liner probability estimation and
calculate the LM probability P(wi|hi) for the giv-
en context (Schwenk, 2007).
CSLM is able to calculate the probabilities of
all words in the vocabulary of the corpus given the
context. However, due to too high computational
complexity, CSLM is mainly used to calculate the
probabilities of a subset of the whole vocabulary
(Schwenk, 2007). This subset is called a short-
list, which consists of the most frequent words in
the vocabulary. CSLM also calculates the sum of
the probabilities of all words not included in the
short-list by assigning a neuron with the help of
BNLM. The probabilities of other words not in the
short-list are obtained from an BNLM (Schwenk,
2007; Schwenk, 2010; Wang et al., 2013a).
Let wi and hi be the current word and history,
respectively. CSLM with a BNLM calculates the
probability P(wi|hi) of wi given hi, as follows:
</bodyText>
<equation confidence="0.925092666666667">
I ∑wEVwi |&apos;i) E)P (h) if wi E V0 (1)
0 Pc wh s i
Pb(wi|hi) otherwise
</equation>
<bodyText confidence="0.9998964">
where Vo is the short-list, P,(·) is the probabil-
ity calculated by CSLM, ∑wEU0 P,(w|hi) is the
summary of probabilities of the neuron for all the
words in the short-list, Pb(·) is the probability cal-
culated by the BNLM, and
</bodyText>
<equation confidence="0.981793">
Ps(hi) = E Pb(v|hi). (2)
vEVo
</equation>
<bodyText confidence="0.972038666666667">
We may regard that CSLM redistributes the
probability mass of all words in the short-list,
which is calculated by using the n-gram LM.
</bodyText>
<subsectionHeader confidence="0.998773">
2.2 Existing Converting Methods
</subsectionHeader>
<bodyText confidence="0.99689209375">
As baseline systems, our approach proposed in
(Wang et al., 2013a) only re-writes the probabil-
ities from CSLM into the BNLM, so it can only
conduct a convert LM with the same size as the o-
riginal one. The main difference between our pro-
posed method in this paper and our previous ap-
proach is that n-grams outside the corpus are gen-
erated firstly and the probabilities using CSLM are
calculated by using the same method as our previ-
ous approach. That is, the proposed new method
is the same as our previous one when no grown
n-grams are generated.
The method developed by Arsoy and colleagues
(Arsoy et al., 2013; Arsoy et al., 2014) adds al-
l the words in the short-list after the tail word of
the i-grams to construct the (i+1)-grams. For ex-
ample, if the i-gram is “I want”, then the (i+1)-
grams will be “I want *”, where “*” stands for any
word in the short list. Then the probabilities of
the (i+1)-grams are calculated using (i+1)-CSLM.
So a very large intermediate (i+1)-grams will have
to be grownl, and then be pruned into smaller
suitable size using an entropy-based LM pruning
method modified from (Stolcke, 1998). The (i+2)-
grams are grown using (i+1)-grams, recursively.
&apos;In practice, the probabilities of all the target/tail words
in the short list for the history i-grams can be calculated by
the neurons in the output layer at the same time, which will
save some time. According to our experiments, the time cost
for Arsoy’s growing method is around 4 times more than our
proposed method, if the LMs which are 10 times larger than
the original one are grown with other settings all the same.
</bodyText>
<equation confidence="0.990508">
P(wi|hi) =
</equation>
<page confidence="0.984281">
190
</page>
<sectionHeader confidence="0.990919" genericHeader="method">
3 Bilingual LM Growing
</sectionHeader>
<bodyText confidence="0.995176714285714">
The translation output of a phrase-based SMT sys-
tem can be regarded as a concatenation of phrases
in the phrase table (except unknown words). This
leads to the following procedure:
Step 1. All the n-grams included in the phrase
table should be maintained at first.
Step 2. The connecting phrases are defined in
the following way.
The wba is a target language phrase starting from
the a-th word ending with the b-th word, and Qwba-y
is a phrase including wba as a part of it, where Q and
-y represent any word sequence or none. An i-gram
phrase w+1wk++1 (1 G k G i − 1) is a connecting
phrase2, if :
</bodyText>
<listItem confidence="0.59045175">
(1) w+1 is the right (rear) part of one phrase Qw+1
in the phrase table, or
(2) wk+1 is the left (front) part of one phrase
wk+1-y in the phrase table.
</listItem>
<bodyText confidence="0.98137775">
After the probabilities are calculated using C-
SLM (Eqs.1 and 2), we combine the n-grams in
the phrase table from Step 1 and the connecting
phrases from Step 2.
</bodyText>
<subsectionHeader confidence="0.999619">
3.1 Ranking the Connecting Phrases
</subsectionHeader>
<bodyText confidence="0.999989142857143">
Since the size of connecting phrases is too huge
(usually more than one Terabyte), it is necessary
to decide the usefulness of connecting phrases for
SMT. The more useful connecting phrases can be
selected, by ranking the appearing probabilities of
the connecting phrases in SMT decoding.
Each line of a phrase table can be simplified
(without considering other unrelated scores in the
phrase table) as
where the P(e|f) means the translation probabili-
ty from f(source phrase) to e(target phrase),
which can be calculated using bilingual parallel
training data. In decoding, the probability of a tar-
get phrase e appearing in SMT should be
</bodyText>
<equation confidence="0.999501">
EPt(e) =
s P3(f) × P(e|f), (4)
</equation>
<bodyText confidence="0.99933764">
2We are aware that connecting phrases can be applied to
not only two phrases, but also three or more. However the ap-
pearing probabilities (which will be discussed in Eq. 5 of next
subsection) of connecting phrases are approximately estimat-
ed. To estimate and compare probabilities of longer phrases
in different lengths will lead to serious bias, and the experi-
ments also showed using more than two connecting phrases
did not perform well (not shown for limited space), so only
two connecting phrases are applied in this paper.
where the P3(f) means the appearing probability
of a source phrase, which can be calculated using
source language part in the bilingual training data.
Using Pt(e)3, we can select the connecting
phrases e with high appearing probabilities as
the n-grams to be added to the original n-
grams. These n-grams are called ‘grown n-
grams’. Namely, we build all the connecting
phrases at first, and then we use the appearing
probabilities of the connecting phrases to decide
which connecting phrases should be selected. For
an i-gram connecting phrase w+1wk++1, where w+ 1 is
part of Qw+1 and wk+1 is part of wk+1-y (the Qw+1
and wk+1-y are from the phrase table), the prob-
ability of the connecting phrases can be roughly
estimated as
</bodyText>
<equation confidence="0.990255">
Pcon(wk1wik+1) =
(5)
E Pt(wik+1&apos;Y))�
7
</equation>
<bodyText confidence="0.9998345">
A threshold for Pcon(w+1wk+1) is set, and only
the connecting phrases whose appearing probabil-
ities are higher than the threshold will be selected
as the grown n-grams.
</bodyText>
<subsectionHeader confidence="0.999855">
3.2 Calculating the Probabilities of Grown
N-grams Using CSLM
</subsectionHeader>
<bodyText confidence="0.99975885">
To our bilingual LM growing method, a 5-gram
LM and n-gram (n=2,3,4,5) CSLMs are built by
using the target language of the parallel corpus,
and the phrase table is learned from the parallel
corpus.
The probabilities of unigram in the original n-
gram LM will be maintained as they are. The
n-grams from the bilingual phrase table will be
grown by using the ‘connecting phrases’ method.
As the whole connecting phrases are too huge, we
use the ranking method to select the more useful
connecting phrases. The distribution of different
n-grams (n=2,3,4,5) of the grown LMs are set as
the same as the original LM.
The probabilities of the grown n-grams
(n=2,3,4,5) are calculated using the 2,3,4,5-
CSLM, respectively. If the tail (target) words of
the grown n-grams are not in the short-list of C-
SLM, the Pb(·) in Eq. 1 will be applied to calcu-
late their probabilities.
</bodyText>
<footnote confidence="0.979841">
3This Pt(e) hence provides more bilingual information,
in comparison with using monolingual target LMs only.
</footnote>
<figure confidence="0.938930166666667">
Ei − 1
k_1
Pt(,3wk1)×
E
(
a
</figure>
<page confidence="0.987055">
191
</page>
<bodyText confidence="0.9999296">
We combine the n-grams (n=1,2,3,4,5) togeth-
er and re-normalize the probabilities and backof-
f weights of the grown LM. Finally the original
BNLM and the grown LM are interpolated. The
entire process is illustrated in Figure 1.
</bodyText>
<figureCaption confidence="0.994665">
Figure 1: NN based bilingual LM growing.
</figureCaption>
<sectionHeader confidence="0.996491" genericHeader="method">
4 Experiments and Results
</sectionHeader>
<subsectionHeader confidence="0.961429">
4.1 Experiment Setting up
</subsectionHeader>
<bodyText confidence="0.9997945625">
The same setting up of the NTCIR-9 Chinese to
English translation baseline system (Goto et al.,
2011) was followed, only with various LMs to
compare them. The Moses phrase-based SMT
system was applied (Koehn et al., 2007), togeth-
er with GIZA++ (Och and Ney, 2003) for align-
ment and MERT (Och, 2003) for tuning on the de-
velopment data. Fourteen standard SMT features
were used: five translation model scores, one word
penalty score, seven distortion scores, and one LM
score. The translation performance was measured
by the case-insensitive BLEU on the tokenized test
data.
We used the patent data for the Chinese to En-
glish patent translation subtask from the NTCIR-9
patent translation task (Goto et al., 2011). The par-
allel training, development, and test data sets con-
sist of 1 million (M), 2,000, and 2,000 sentences,
respectively.
Using SRILM (Stolcke, 2002; Stolcke et al.,
2011), we trained a 5-gram LM with the interpo-
lated Kneser-Ney smoothing method using the 1M
English training sentences containing 42M words
without cutoff. The 2,3,4,5-CSLMs were trained
on the same 1M training sentences using CSLM
toolkit (Schwenk, 2007; Schwenk, 2010). The set-
tings for CSLMs were: input layer of the same
dimension as vocabulary size (456K), projection
layer of dimension 256 for each word, hidden lay-
er of dimension 384 and output layer (short-list) of
dimension 8192, which were recommended in the
CSLM toolkit and (Wang et al., 2013a)4.
</bodyText>
<footnote confidence="0.838073">
4Arsoy used around 55 M words as the corpus, including
</footnote>
<sectionHeader confidence="0.705342" genericHeader="evaluation">
4.2 Results
</sectionHeader>
<bodyText confidence="0.999892941176471">
The experiment results were divided into four
groups: the original BNLMs (BN), the CSLM
Re-ranking (RE), our previous converting (WA),
the Arsoy’s growing, and our growing methods.
For our bilingual LM growing method, 5 bilingual
grown LMs (BI-1 to 5) were conducted in increas-
ing sizes. For the method of Arsoy, 5 grown LMs
(AR-1 to 5) with similar size of BI-1 to 5 were also
conducted, respectively.
For the CSLM re-ranking, we used CSLM to
re-rank the 100-best lists of SMT. Our previous
converted LM, Arsoy’s grown LMs and bilingual
grown LMs were interpolated with the original
BNLMs, using default setting of SRILM5. To re-
duce the randomness of MERT, we used two meth-
ods for tuning the weights of different SMT fea-
tures, and two BLEU scores are corresponding to
these two methods. The BLEU-s indicated that the
same weights of the BNLM (BN) features were
used for all the SMT systems. The BLEU-i indi-
cated that the MERT was run independently by
three times and the average BLEU scores were
taken.
We also performed the paired bootstrap re-
sampling test (Koehn, 2004)6. Two thousands
samples were sampled for each significance test.
The marks at the right of the BLEU score indicated
whether the LMs were significantly better/worse
than the Arsoy’s grown LMs with the same IDs
for SMT (“++/−−”: significantly better/worse at
α = 0.01, “+/−”: α = 0.05, no mark: not signif-
icantly better/worse at α = 0.05).
From the results shown in Table 1, we can get
the following observations:
</bodyText>
<listItem confidence="0.999325777777778">
(1) Nearly all the bilingual grown LMs outper-
formed both BNLM and our previous converted
LM on PPL and BLEU. As the size of grown LM-
s is increased, the PPL always decreased and the
BLEU scores trended to increase. These indicated
that our proposed method can give better probabil-
ity estimation for LM and better performance for
SMT.
(2) In comparison with the grown LMs in Ar-
</listItem>
<bodyText confidence="0.739416">
84K words as vocabulary, and 20K words as short-list. In this
paper, we used the same setting as our previous work, which
covers 92.89% of the frequency of words in the training cor-
pus, for all the baselines and our method for fair comparison.
</bodyText>
<footnote confidence="0.8978518">
5In our previous work, we used the development data to
tune the weights of interpolation. In this paper, we used the
default 0.5 as the interpolation weights for fair comparison.
6We used the code available at http://www.ark.cs.
cmu.edu/MT
</footnote>
<figure confidence="0.999232230769231">
Grown LM
Corpus
Phrase Table
BNLM
Interpolate
Connecting
Phrases
Output
Grown n-grams
Grown n-grams
with Probabilities
Input
CSLM
</figure>
<page confidence="0.992931">
192
</page>
<tableCaption confidence="0.999818">
Table 1: Performance of the Grown LMs
</tableCaption>
<table confidence="0.999760714285714">
LMs n-grams PPL BLEU-s BLEU-i ALH
BN 73.9M 108.8 32.19 32.19 3.03
RE N/A 97.5 32.34 32.42 N/A
WA 73.9M 104.4 32.60 32.62 3.03
AR-1 217.6M 103.3 32.55 32.75 3.14
AR-2 323.8M 103.1 32.61 32.64 3.18
AR-3 458.5M 103.0 32.39 32.71 3.20
AR-4 565.6M 102.8 32.67 32.51 3.21
AR-5 712.2M 102.5 32.49 32.60 3.22
BI-1 223.5M 101.9 32.81+ 33.02+ 3.20
BI-2 343.6M 101.0 32.92+ 33.11++ 3.24
BI-3 464.5M 100.6 33.08++ 33.25++ 3.26
BI-4 571.0M 100.3 33.15++ 33.12++ 3.28
BI-5 705.5M 100.1 33.11++ 33.24++ 3.31
</table>
<bodyText confidence="0.997885666666667">
soy’s method, our grown LMs obtained better P-
PL and significantly better BLEU with the sim-
ilar size. Furthermore, the improvement of PPL
and BLEU of the existing methods became satu-
rated much more quickly than ours did, as the LMs
grew.
(3) The last column was the Average Length of
the n-grams Hit (ALH) in SMT decoding for dif-
ferent LMs using the following function
</bodyText>
<equation confidence="0.99693">
Pi−gram X i, (6)
</equation>
<bodyText confidence="0.999963857142857">
where the Pi−gram means the ratio of the i-grams
hit in SMT decoding. There were also positive
correlations between ALH, PPL and BLEUs. The
ALH of bilingual grown LM was longer than that
of the Arsoy’s grown LM of the similar size. In
another word, less back-off was used for our pro-
posed grown LMs in SMT decoding.
</bodyText>
<subsectionHeader confidence="0.997612">
4.3 Experiments on TED Corpus
</subsectionHeader>
<bodyText confidence="0.999962857142857">
The TED corpus is in special domain as discussed
in the introduction, where large extra monolingual
corpora are hard to find. In this subsection, we
conducted the SMT experiments on TED corpora
using our proposed LM growing method, to eval-
uate whether our method was adaptable to some
special domains.
We mainly followed the baselines of the IWSLT
2014 evaluation campaign7, only with a few mod-
ifications such as the LM toolkits and n-gram or-
der for constructing LMs. The Chinese (CN) to
English (EN) language pair was chosen, using de-
v2010 as development data and test2010 as evalu-
ation data. The same LM growing method was ap-
</bodyText>
<footnote confidence="0.86812">
7https://wit3.fbk.eu/
</footnote>
<bodyText confidence="0.610086">
plied on TED corpora as on NTCIR corpora. The
results were shown in Table 2.
</bodyText>
<tableCaption confidence="0.984169">
Table 2: CN-EN TED Experiments
</tableCaption>
<table confidence="0.9999335">
LMs n-grams PPL BLEU-s
BN 7.8M 87.1 12.41
WA 7.8M 85.3 12.73
BI-1 23.1M 79.2 12.92
BI-2 49.7M 78.3 13.16
BI-3 73.4M 77.6 13.24
</table>
<tableCaption confidence="0.874419">
Table 2 indicated that our proposed LM grow-
ing method improved both PPL and BLEU in com-
</tableCaption>
<bodyText confidence="0.62800775">
parison with both BNLM and our previous CSLM
converting method, so it was suitable for domain
adaptation, which is one of focuses of the current
SMT research.
</bodyText>
<sectionHeader confidence="0.994539" genericHeader="conclusions">
5 Conclusion
</sectionHeader>
<bodyText confidence="0.99971225">
In this paper, we have proposed a neural network
based bilingual LM growing method by using the
bilingual parallel corpus only for SMT. The results
show that our proposed method can improve both
LM and SMT performance, and outperforms the
existing LM growing methods significantly with-
out extra corpus. The connecting phrase-based
method can also be applied to LM adaptation.
</bodyText>
<sectionHeader confidence="0.987295" genericHeader="acknowledgments">
Acknowledgments
</sectionHeader>
<bodyText confidence="0.999756681818182">
We appreciate the helpful discussion with Dr.
Isao Goto and Zhongye Jia, and three anony-
mous reviewers for valuable comments and sug-
gestions on our paper. Rui Wang, Hai Zhao
and Bao-Liang Lu were partially supported by
the National Natural Science Foundation of Chi-
na (No. 60903119, No. 61170114, and No.
61272248), the National Basic Research Program
of China (No. 2013CB329401), the Science and
Technology Commission of Shanghai Municipali-
ty (No. 13511500200), the European Union Sev-
enth Framework Program (No. 247619), the Cai
Yuanpei Program (CSC fund 201304490199 and
201304490171), and the art and science interdis-
cipline funds of Shanghai Jiao Tong University
(A study on mobilization mechanism and alerting
threshold setting for online community, and media
image and psychology evaluation: a computation-
al intelligence approach). The corresponding au-
thor of this paper, according to the meaning given
to this role by Shanghai Jiao Tong University, is
Hai Zhao.
</bodyText>
<figure confidence="0.759237">
5
∑
i��
ALH =
</figure>
<page confidence="0.997535">
193
</page>
<sectionHeader confidence="0.95733" genericHeader="references">
References
</sectionHeader>
<reference confidence="0.995838890909091">
Ebru Arsoy, Stanley F. Chen, Bhuvana Ramabhadran,
and Abhinav Sethy. 2013. Converting neural net-
work language models into back-off language mod-
els for efficient decoding in automatic speech recog-
nition. In Proceedings of ICASSP-2013, Vancouver,
Canada, May. IEEE.
Ebru Arsoy, Stanley F. Chen, Bhuvana Ramabhadran,
and Abhinav Sethy. 2014. Converting neural net-
work language models into back-off language mod-
els for efficient decoding in automatic speech recog-
nition. IEEE/ACM Transactions on Audio, Speech,
and Language, 22(1):184–192.
Michael Auli, Michel Galley, Chris Quirk, and Geof-
frey Zweig. 2013. Joint language and translation
modeling with recurrent neural networks. In Pro-
cessings of EMNLP-2013, pages 1044–1054, Seat-
tle, Washington, USA, October. Association for
Computational Linguistics.
Jerome R Bellegarda. 2004. Statistical language mod-
el adaptation: review and perspectives. Speech
Communication, 42(1):93–108. Adaptation Meth-
ods for Speech Recognition.
Yoshua Bengio, R´ejean Ducharme, Pascal Vincent, and
Christian Janvin. 2003. A neural probabilistic lan-
guage model. Journal of Machine Learning Re-
search (JMLR), 3:1137–1155, March.
Mauro Cettolo, Christian Girardi, and Marcello Fed-
erico. 2012. Wit3: Web inventory of transcribed
and translated talks. In Proceedings of EAMT-2012,
pages 261–268, Trento, Italy, May.
Philip Clarkson and A.J. Robinson. 1997. Lan-
guage model adaptation using mixtures and an ex-
ponentially decaying cache. In Proceedings of
ICASSP-1997, volume 2, pages 799–802 vol.2, Mu-
nich,Germany.
Jacob Devlin, Rabih Zbib, Zhongqiang Huang, Thomas
Lamar, Richard Schwartz, and John Makhoul. 2014.
Fast and robust neural network joint models for sta-
tistical machine translation. In Proceedings of ACL-
2014, pages 1370–1380, Baltimore, Maryland, June.
Association for Computational Linguistics.
Isao Goto, Bin Lu, Ka Po Chow, Eiichiro Sumita, and
Benjamin K. Tsou. 2011. Overview of the paten-
t machine translation task at the NTCIR-9 work-
shop. In Proceedings of NTCIR-9 Workshop Meet-
ing, pages 559–578, Tokyo, Japan, December.
Rukmini Iyer, Mari Ostendorf, and Herbert Gish.
1997. Using out-of-domain data to improve in-
domain language models. Signal Processing Letter-
s, IEEE, 4(8):221–223.
Zhongye Jia and Hai Zhao. 2013. Kyss 1.0: a
framework for automatic evaluation of chinese input
method engines. In Proceedings of IJCNLP-2013,
pages 1195–1201, Nagoya, Japan, October. Asian
Federation of Natural Language Processing.
Zhongye Jia and Hai Zhao. 2014. A joint graph mod-
el for pinyin-to-chinese conversion with typo cor-
rection. In Proceedings of ACL-2014, pages 1512–
1523, Baltimore, Maryland, June. Association for
Computational Linguistics.
Philipp Koehn and Josh Schroeder. 2007. Experi-
ments in domain adaptation for statistical machine
translation. In Proceedings of ACL-2007 Workshop
on Statistical Machine Translation, pages 224–227,
Stroudsburg, PA, USA. Association for Computa-
tional Linguistics.
Philipp Koehn, Hieu Hoang, Alexandra Birch, Chris
Callison-Burch, Marcello Federico, Nicola Bertol-
di, Brooke Cowan, Wade Shen, Christine Moran,
Richard Zens, Chris Dyer, Ondrej Bojar, Alexan-
dra Constantin, and Evan Herbst. 2007. Moses:
Open source toolkit for statistical machine transla-
tion. In Proceedings of ACL-2007, pages 177–180,
Prague, Czech Republic, June. Association for Com-
putational Linguistics.
Philipp Koehn. 2004. Statistical significance tests
for machine translation evaluation. In Proceedings
of EMNLP-2004, pages 388–395, Barcelona, Spain,
July. Association for Computational Linguistics.
Hai-Son Le, Ilya Oparin, Alexandre Allauzen, J Gau-
vain, and Franc¸ois Yvon. 2011. Structured output
layer neural network language model. In Proceed-
ings of ICASSP-2011, pages 5524–5527, Prague,
Czech Republic, May. IEEE.
Shujie Liu, Nan Yang, Mu Li, and Ming Zhou. 2014.
A recursive recurrent neural network for statistical
machine translation. In Proceedings of ACL-2014,
pages 1491–1500, Baltimore, Maryland, June. As-
sociation for Computational Linguistics.
Tomas Mikolov, Martin Karafi´at, Lukas Burget, Jan
Cernock`y, and Sanjeev Khudanpur. 2010. Re-
current neural network based language model. In
Proceedings of INTERSPEECH-2010, pages 1045–
1048.
Jan Niehues and Alex Waibel. 2012. Continuous
space language models using restricted boltzman-
n machines. In Proceedings of IWSLT-2012, pages
311–318, Hong Kong.
Thomas Niesler and Phil Woodland. 1996. A variable-
length category-based n-gram language model. In
Proceedings of ICASSP-1996, volume 1, pages 164–
167 vol. 1.
Franz Josef Och and Hermann Ney. 2003. A sys-
tematic comparison of various statistical alignmen-
t models. Computational Linguistics, 29(1):19–51,
March.
Franz Josef Och. 2003. Minimum error rate training
in statistical machine translation. In Proceedings
of ACL-2003, pages 160–167, Sapporo, Japan, July.
Association for Computational Linguistics.
</reference>
<page confidence="0.99444">
194
</page>
<reference confidence="0.99941146">
Eric Sven Ristad and Robert G. Thomas. 1995. New
techniques for context modeling. In Proceedings
of ACL-1995, pages 220–227, Cambridge, Mas-
sachusetts. Association for Computational Linguis-
tics.
Holger Schwenk, Daniel Dchelotte, and Jean-Luc Gau-
vain. 2006. Continuous space language models for
statistical machine translation. In Proceedings of
COLING ACL-2006, pages 723–730, Sydney, Aus-
tralia, July. Association for Computational Linguis-
tics.
Holger Schwenk, Anthony Rousseau, and Mohammed
Attik. 2012. Large, pruned or continuous space
language models on a gpu for statistical machine
translation. In Proceedings of the NAACL-HLT 2012
Workshop: Will We Ever Really Replace the N-gram
Model? On the Future of Language Modeling for
HLT, WLM ’12, pages 11–19, Montreal, Canada,
June. Association for Computational Linguistics.
Holger Schwenk. 2007. Continuous space lan-
guage models. Computer Speech and Language,
21(3):492–518.
Holger Schwenk. 2010. Continuous-space language
models for statistical machine translation. The
Prague Bulletin of Mathematical Linguistics, pages
137–146.
Vesa Siivola, Teemu Hirsimki, and Sami Virpioja.
2007. On growing and pruning kneser-ney s-
moothed n-gram models. IEEE Transactions on Au-
dio, Speech, and Language, 15(5):1617–1624.
Manhung Siu and Mari Ostendorf. 2000. Variable n-
grams and extensions for conversational speech lan-
guage modeling. IEEE Transactions on Speech and
Audio, 8(1):63–75.
Le Hai Son, Alexandre Allauzen, Guillaume Wis-
niewski, and Franc¸ois Yvon. 2010. Training con-
tinuous space language models: some practical is-
sues. In Proceedings of EMNLP-2010, pages 778–
788, Cambridge, Massachusetts, October. Associa-
tion for Computational Linguistics.
Le Hai Son, Alexandre Allauzen, and Franc¸ois Yvon.
2012. Continuous space translation models with
neural networks. In Proceedings of NAACL HLT-
2012, pages 39–48, Montreal, Canada, June. Asso-
ciation for Computational Linguistics.
Andreas Stolcke, Jing Zheng, Wen Wang, and Vic-
tor Abrash. 2011. SRILM at sixteen: Update and
outlook. In Proceedings of INTERSPEECH 2011,
Waikoloa, HI, USA, December.
Andreas Stolcke. 1998. Entropy-based pruning of
backoff language models. In Proceedings of DARPA
Broadcast News Transcription and Understanding
Workshop, pages 270–274, Lansdowne, VA, USA.
Andreas Stolcke. 2002. Srilm-an extensible
language modeling toolkit. In Proceedings of
INTERSPEECH-2002, pages 257–286, Seattle, US-
A, November.
Ashish Vaswani, Yinggong Zhao, Victoria Fossum,
and David Chiang. 2013. Decoding with large-
scale neural language models improves translation.
In Proceedings of EMNLP-2013, pages 1387–1392,
Seattle, Washington, USA, October. Association for
Computational Linguistics.
Rui Wang, Masao Utiyama, Isao Goto, Eiichro Sumi-
ta, Hai Zhao, and Bao-Liang Lu. 2013a. Convert-
ing continuous-space language models into n-gram
language models for statistical machine translation.
In Proceedings of EMNLP-2013, pages 845–850,
Seattle, Washington, USA, October. Association for
Computational Linguistics.
Xiaolin Wang, Hai Zhao, and Bao-Liang Lu. 2013b.
Labeled alignment for recognizing textual entail-
ment. In Proceedings of IJCNLP-2013, pages 605–
613, Nagoya, Japan, October. Asian Federation of
Natural Language Processing.
Xiao-Lin Wang, Yang-Yang Chen, Hai Zhao, and Bao-
Liang Lu. 2014. Parallelized extreme learning ma-
chine ensemble based on minmax modular network.
Neurocomputing, 128(0):31 – 41.
Qiongkai Xu and Hai Zhao. 2012. Using deep lin-
guistic features for finding deceptive opinion spam.
In Proceedings of COLING-2012, pages 1341–1350,
Mumbai, India, December. The COLING 2012 Or-
ganizing Committee.
Jingyi Zhang and Hai Zhao. 2013. Improving function
word alignment with frequency and syntactic infor-
mation. In Proceedings of IJCAI-2013, pages 2211–
2217. AAAI Press.
Xiaotian Zhang, Hai Zhao, and Cong Hui. 2012.
A machine learning approach to convert CCGbank
to Penn treebank. In Proceedings of COLING-
2012, pages 535–542, Mumbai, India, December.
The COLING 2012 Organizing Committee.
Hai Zhao, Masao Utiyama, Eiichiro Sumita, and Bao-
Liang Lu. 2013. An empirical study on word
segmentation for chinese machine translation. In
Alexander Gelbukh, editor, Computational Linguis-
tics and Intelligent Text Processing, volume 7817 of
Lecture Notes in Computer Science, pages 248–263.
Springer Berlin Heidelberg.
</reference>
<page confidence="0.998941">
195
</page>
</variant>
</algorithm>
<algorithm name="ParsHed" version="110505">
<variant no="0" confidence="0.073368">
<title confidence="0.9974545">Neural Network Based Bilingual Language Model for Statistical Machine Translation</title>
<author confidence="0.7068945">Hai Bao-Liang Masao</author>
<author confidence="0.7068945">Eiichro for Brain-Like Computing</author>
<author confidence="0.7068945">Machine</author>
<affiliation confidence="0.995892">Department of Computer Science and</affiliation>
<address confidence="0.784071">Shanghai Jiao Tong University, Shanghai, 200240,</address>
<affiliation confidence="0.7041">Translation Laboratory, MASTAR National Institute of Information and Communications</affiliation>
<address confidence="0.86344">3-5 Hikaridai, Keihanna Science City, Kyoto, 619-0289,</address>
<affiliation confidence="0.770048">Laboratory of Shanghai Education Commission for Intelligent</affiliation>
<address confidence="0.491899">and Cognitive Engineering, Shanghai Jiao Tong University, Shanghai, 200240,</address>
<abstract confidence="0.999225058823529">larger Language Model (LM) usually performs better in Statistical Machine Translation (SMT), how to construct efficient large LM is an important topic in SMT. However, most of the existing LM growing methods need an extra monolingual corpus, where additional LM adaption technology is necessary. In this paper, we propose a novel neural network based bilingual LM growing method, only using the bilingual parallel corpus in SMT. The results show that our method can improve both the perplexity score for LM evaluation and BLEU score for SMT, and significantly outperforms the existing LM growing methods without extra corpus.</abstract>
</variant>
</algorithm>
<algorithm name="ParsCit" version="110505">
<citationList>
<citation valid="true">
<authors>
<author>Ebru Arsoy</author>
<author>Stanley F Chen</author>
<author>Bhuvana Ramabhadran</author>
<author>Abhinav Sethy</author>
</authors>
<title>Converting neural network language models into back-off language models for efficient decoding in automatic speech recognition.</title>
<date>2013</date>
<booktitle>In Proceedings of ICASSP-2013,</booktitle>
<publisher>IEEE.</publisher>
<location>Vancouver, Canada,</location>
<contexts>
<context position="3745" citStr="Arsoy et al., 2013" startWordPosition="583" endWordPosition="586">es (2013) propose a method for reducing the training cost of CSLM and apply it to SMT decoder. However, they do not show their improvement for decoding speed, and their method is still slower than the n-gram LM. There are several other methods for attempting to implement neural network based LM or translation model for SMT (Devlin et al., 2014; Liu et al., 2014; Auli et al., 2013). However, the decoding speed using n-gram LM is still state-ofthe-art one. Some approaches calculate the probabilities of the n-grams n-grams before decoding, and store them in the n-gram format (Wang et al., 2013a; Arsoy et al., 2013; Arsoy et al., 2014). The ‘converted CSLM’ can be directly used in SMT. Though more n-grams which are not in the train189 Proceedings of the 2014 Conference on Empirical Methods in Natural Language Processing (EMNLP), pages 189–195, October 25-29, 2014, Doha, Qatar. c�2014 Association for Computational Linguistics ing corpus can be generated by using some of these ‘converting’ methods, these methods only consider the monolingual information, and do not take the bilingual information into account. We observe that the translation output of a phrase-based SMT system is concatenation of phrases f</context>
<context position="7524" citStr="Arsoy et al., 2013" startWordPosition="1223" endWordPosition="1226">ethods As baseline systems, our approach proposed in (Wang et al., 2013a) only re-writes the probabilities from CSLM into the BNLM, so it can only conduct a convert LM with the same size as the original one. The main difference between our proposed method in this paper and our previous approach is that n-grams outside the corpus are generated firstly and the probabilities using CSLM are calculated by using the same method as our previous approach. That is, the proposed new method is the same as our previous one when no grown n-grams are generated. The method developed by Arsoy and colleagues (Arsoy et al., 2013; Arsoy et al., 2014) adds all the words in the short-list after the tail word of the i-grams to construct the (i+1)-grams. For example, if the i-gram is “I want”, then the (i+1)- grams will be “I want *”, where “*” stands for any word in the short list. Then the probabilities of the (i+1)-grams are calculated using (i+1)-CSLM. So a very large intermediate (i+1)-grams will have to be grownl, and then be pruned into smaller suitable size using an entropy-based LM pruning method modified from (Stolcke, 1998). The (i+2)- grams are grown using (i+1)-grams, recursively. &apos;In practice, the probabilit</context>
</contexts>
<marker>Arsoy, Chen, Ramabhadran, Sethy, 2013</marker>
<rawString>Ebru Arsoy, Stanley F. Chen, Bhuvana Ramabhadran, and Abhinav Sethy. 2013. Converting neural network language models into back-off language models for efficient decoding in automatic speech recognition. In Proceedings of ICASSP-2013, Vancouver, Canada, May. IEEE.</rawString>
</citation>
<citation valid="true">
<authors>
<author>Ebru Arsoy</author>
<author>Stanley F Chen</author>
<author>Bhuvana Ramabhadran</author>
<author>Abhinav Sethy</author>
</authors>
<title>Converting neural network language models into back-off language models for efficient decoding in automatic speech recognition.</title>
<date>2014</date>
<journal>IEEE/ACM Transactions on Audio, Speech, and Language,</journal>
<volume>22</volume>
<issue>1</issue>
<contexts>
<context position="3766" citStr="Arsoy et al., 2014" startWordPosition="587" endWordPosition="590">method for reducing the training cost of CSLM and apply it to SMT decoder. However, they do not show their improvement for decoding speed, and their method is still slower than the n-gram LM. There are several other methods for attempting to implement neural network based LM or translation model for SMT (Devlin et al., 2014; Liu et al., 2014; Auli et al., 2013). However, the decoding speed using n-gram LM is still state-ofthe-art one. Some approaches calculate the probabilities of the n-grams n-grams before decoding, and store them in the n-gram format (Wang et al., 2013a; Arsoy et al., 2013; Arsoy et al., 2014). The ‘converted CSLM’ can be directly used in SMT. Though more n-grams which are not in the train189 Proceedings of the 2014 Conference on Empirical Methods in Natural Language Processing (EMNLP), pages 189–195, October 25-29, 2014, Doha, Qatar. c�2014 Association for Computational Linguistics ing corpus can be generated by using some of these ‘converting’ methods, these methods only consider the monolingual information, and do not take the bilingual information into account. We observe that the translation output of a phrase-based SMT system is concatenation of phrases from the phrase table,</context>
<context position="7545" citStr="Arsoy et al., 2014" startWordPosition="1227" endWordPosition="1230">ystems, our approach proposed in (Wang et al., 2013a) only re-writes the probabilities from CSLM into the BNLM, so it can only conduct a convert LM with the same size as the original one. The main difference between our proposed method in this paper and our previous approach is that n-grams outside the corpus are generated firstly and the probabilities using CSLM are calculated by using the same method as our previous approach. That is, the proposed new method is the same as our previous one when no grown n-grams are generated. The method developed by Arsoy and colleagues (Arsoy et al., 2013; Arsoy et al., 2014) adds all the words in the short-list after the tail word of the i-grams to construct the (i+1)-grams. For example, if the i-gram is “I want”, then the (i+1)- grams will be “I want *”, where “*” stands for any word in the short list. Then the probabilities of the (i+1)-grams are calculated using (i+1)-CSLM. So a very large intermediate (i+1)-grams will have to be grownl, and then be pruned into smaller suitable size using an entropy-based LM pruning method modified from (Stolcke, 1998). The (i+2)- grams are grown using (i+1)-grams, recursively. &apos;In practice, the probabilities of all the target</context>
</contexts>
<marker>Arsoy, Chen, Ramabhadran, Sethy, 2014</marker>
<rawString>Ebru Arsoy, Stanley F. Chen, Bhuvana Ramabhadran, and Abhinav Sethy. 2014. Converting neural network language models into back-off language models for efficient decoding in automatic speech recognition. IEEE/ACM Transactions on Audio, Speech, and Language, 22(1):184–192.</rawString>
</citation>
<citation valid="true">
<authors>
<author>Michael Auli</author>
<author>Michel Galley</author>
<author>Chris Quirk</author>
<author>Geoffrey Zweig</author>
</authors>
<title>Joint language and translation modeling with recurrent neural networks.</title>
<date>2013</date>
<booktitle>In Processings of EMNLP-2013,</booktitle>
<pages>1044--1054</pages>
<publisher>Association</publisher>
<institution>for Computational Linguistics.</institution>
<location>Seattle, Washington, USA,</location>
<contexts>
<context position="3510" citStr="Auli et al., 2013" startWordPosition="544" endWordPosition="547">accurately predict the probabilities of the n-grams, which are not in the training corpus. However, in practice, CSLMs have not been widely used in the current SMT systems, due to their too high computational cost. Vaswani and colleagues (2013) propose a method for reducing the training cost of CSLM and apply it to SMT decoder. However, they do not show their improvement for decoding speed, and their method is still slower than the n-gram LM. There are several other methods for attempting to implement neural network based LM or translation model for SMT (Devlin et al., 2014; Liu et al., 2014; Auli et al., 2013). However, the decoding speed using n-gram LM is still state-ofthe-art one. Some approaches calculate the probabilities of the n-grams n-grams before decoding, and store them in the n-gram format (Wang et al., 2013a; Arsoy et al., 2013; Arsoy et al., 2014). The ‘converted CSLM’ can be directly used in SMT. Though more n-grams which are not in the train189 Proceedings of the 2014 Conference on Empirical Methods in Natural Language Processing (EMNLP), pages 189–195, October 25-29, 2014, Doha, Qatar. c�2014 Association for Computational Linguistics ing corpus can be generated by using some of the</context>
</contexts>
<marker>Auli, Galley, Quirk, Zweig, 2013</marker>
<rawString>Michael Auli, Michel Galley, Chris Quirk, and Geoffrey Zweig. 2013. Joint language and translation modeling with recurrent neural networks. In Processings of EMNLP-2013, pages 1044–1054, Seattle, Washington, USA, October. Association for Computational Linguistics.</rawString>
</citation>
<citation valid="true">
<authors>
<author>Jerome R Bellegarda</author>
</authors>
<title>Statistical language model adaptation: review and perspectives.</title>
<date>2004</date>
<journal>Speech Communication,</journal>
<volume>42</volume>
<issue>1</issue>
<contexts>
<context position="2106" citStr="Bellegarda, 2004" startWordPosition="298" endWordPosition="299">he corpus together with their probabilities into the original LM. This operation is useful as it can make LM perform better through letting it become larger and larger, by only using a small training corpus. There are various methods for adding n-grams selected by different criteria from a monolingual corpus (Ristad and Thomas, 1995; Niesler and Woodland, 1996; Siu and Ostendorf, 2000; Siivola et al., 2007). However, all of these approaches need additional corpora. Meanwhile the extra corpora from different domains will not result in better LMs (Clarkson and Robinson, 1997; Iyer et al., 1997; Bellegarda, 2004; Koehn and Schroeder, *Part of this work was done as Rui Wang visited in NICT. 2007). In addition, it is very difficult or even impossible to collect an extra large corpus for some special domains such as the TED corpus (Cettolo et al., 2012) or for some rare languages. Therefore, to improve the performance of LMs, without assistance of extra corpus, is one of important research topics in SMT. Recently, Continues Space Language Model (CSLM), especially Neural Network based Language Model (NNLM) (Bengio et al., 2003; Schwenk, 2007; Mikolov et al., 2010; Le et al., 2011), is being actively used</context>
</contexts>
<marker>Bellegarda, 2004</marker>
<rawString>Jerome R Bellegarda. 2004. Statistical language model adaptation: review and perspectives. Speech Communication, 42(1):93–108. Adaptation Methods for Speech Recognition.</rawString>
</citation>
<citation valid="true">
<authors>
<author>Yoshua Bengio</author>
<author>R´ejean Ducharme</author>
<author>Pascal Vincent</author>
<author>Christian Janvin</author>
</authors>
<title>A neural probabilistic language model.</title>
<date>2003</date>
<journal>Journal of Machine Learning Research (JMLR),</journal>
<pages>3--1137</pages>
<contexts>
<context position="2627" citStr="Bengio et al., 2003" startWordPosition="386" endWordPosition="389">ns will not result in better LMs (Clarkson and Robinson, 1997; Iyer et al., 1997; Bellegarda, 2004; Koehn and Schroeder, *Part of this work was done as Rui Wang visited in NICT. 2007). In addition, it is very difficult or even impossible to collect an extra large corpus for some special domains such as the TED corpus (Cettolo et al., 2012) or for some rare languages. Therefore, to improve the performance of LMs, without assistance of extra corpus, is one of important research topics in SMT. Recently, Continues Space Language Model (CSLM), especially Neural Network based Language Model (NNLM) (Bengio et al., 2003; Schwenk, 2007; Mikolov et al., 2010; Le et al., 2011), is being actively used in SMT (Schwenk et al., 2006; Son et al., 2010; Schwenk, 2010; Schwenk et al., 2012; Son et al., 2012; Niehues and Waibel, 2012). One of the main advantages of CSLM is that it can more accurately predict the probabilities of the n-grams, which are not in the training corpus. However, in practice, CSLMs have not been widely used in the current SMT systems, due to their too high computational cost. Vaswani and colleagues (2013) propose a method for reducing the training cost of CSLM and apply it to SMT decoder. Howev</context>
</contexts>
<marker>Bengio, Ducharme, Vincent, Janvin, 2003</marker>
<rawString>Yoshua Bengio, R´ejean Ducharme, Pascal Vincent, and Christian Janvin. 2003. A neural probabilistic language model. Journal of Machine Learning Research (JMLR), 3:1137–1155, March.</rawString>
</citation>
<citation valid="true">
<authors>
<author>Mauro Cettolo</author>
<author>Christian Girardi</author>
<author>Marcello Federico</author>
</authors>
<title>Wit3: Web inventory of transcribed and translated talks.</title>
<date>2012</date>
<booktitle>In Proceedings of EAMT-2012,</booktitle>
<pages>261--268</pages>
<location>Trento, Italy,</location>
<contexts>
<context position="2349" citStr="Cettolo et al., 2012" startWordPosition="341" endWordPosition="344">adding n-grams selected by different criteria from a monolingual corpus (Ristad and Thomas, 1995; Niesler and Woodland, 1996; Siu and Ostendorf, 2000; Siivola et al., 2007). However, all of these approaches need additional corpora. Meanwhile the extra corpora from different domains will not result in better LMs (Clarkson and Robinson, 1997; Iyer et al., 1997; Bellegarda, 2004; Koehn and Schroeder, *Part of this work was done as Rui Wang visited in NICT. 2007). In addition, it is very difficult or even impossible to collect an extra large corpus for some special domains such as the TED corpus (Cettolo et al., 2012) or for some rare languages. Therefore, to improve the performance of LMs, without assistance of extra corpus, is one of important research topics in SMT. Recently, Continues Space Language Model (CSLM), especially Neural Network based Language Model (NNLM) (Bengio et al., 2003; Schwenk, 2007; Mikolov et al., 2010; Le et al., 2011), is being actively used in SMT (Schwenk et al., 2006; Son et al., 2010; Schwenk, 2010; Schwenk et al., 2012; Son et al., 2012; Niehues and Waibel, 2012). One of the main advantages of CSLM is that it can more accurately predict the probabilities of the n-grams, whic</context>
</contexts>
<marker>Cettolo, Girardi, Federico, 2012</marker>
<rawString>Mauro Cettolo, Christian Girardi, and Marcello Federico. 2012. Wit3: Web inventory of transcribed and translated talks. In Proceedings of EAMT-2012, pages 261–268, Trento, Italy, May.</rawString>
</citation>
<citation valid="true">
<authors>
<author>Philip Clarkson</author>
<author>A J Robinson</author>
</authors>
<title>Language model adaptation using mixtures and an exponentially decaying cache.</title>
<date>1997</date>
<booktitle>In Proceedings of ICASSP-1997,</booktitle>
<volume>2</volume>
<pages>799--802</pages>
<location>Munich,Germany.</location>
<contexts>
<context position="2069" citStr="Clarkson and Robinson, 1997" startWordPosition="290" endWordPosition="293">(LM) Growing’ refers to adding n-grams outside the corpus together with their probabilities into the original LM. This operation is useful as it can make LM perform better through letting it become larger and larger, by only using a small training corpus. There are various methods for adding n-grams selected by different criteria from a monolingual corpus (Ristad and Thomas, 1995; Niesler and Woodland, 1996; Siu and Ostendorf, 2000; Siivola et al., 2007). However, all of these approaches need additional corpora. Meanwhile the extra corpora from different domains will not result in better LMs (Clarkson and Robinson, 1997; Iyer et al., 1997; Bellegarda, 2004; Koehn and Schroeder, *Part of this work was done as Rui Wang visited in NICT. 2007). In addition, it is very difficult or even impossible to collect an extra large corpus for some special domains such as the TED corpus (Cettolo et al., 2012) or for some rare languages. Therefore, to improve the performance of LMs, without assistance of extra corpus, is one of important research topics in SMT. Recently, Continues Space Language Model (CSLM), especially Neural Network based Language Model (NNLM) (Bengio et al., 2003; Schwenk, 2007; Mikolov et al., 2010; Le </context>
</contexts>
<marker>Clarkson, Robinson, 1997</marker>
<rawString>Philip Clarkson and A.J. Robinson. 1997. Language model adaptation using mixtures and an exponentially decaying cache. In Proceedings of ICASSP-1997, volume 2, pages 799–802 vol.2, Munich,Germany.</rawString>
</citation>
<citation valid="true">
<authors>
<author>Jacob Devlin</author>
<author>Rabih Zbib</author>
<author>Zhongqiang Huang</author>
<author>Thomas Lamar</author>
<author>Richard Schwartz</author>
<author>John Makhoul</author>
</authors>
<title>Fast and robust neural network joint models for statistical machine translation.</title>
<date>2014</date>
<booktitle>In Proceedings of ACL2014,</booktitle>
<pages>1370--1380</pages>
<publisher>Association</publisher>
<institution>for Computational Linguistics.</institution>
<location>Baltimore, Maryland,</location>
<contexts>
<context position="3472" citStr="Devlin et al., 2014" startWordPosition="536" endWordPosition="539">advantages of CSLM is that it can more accurately predict the probabilities of the n-grams, which are not in the training corpus. However, in practice, CSLMs have not been widely used in the current SMT systems, due to their too high computational cost. Vaswani and colleagues (2013) propose a method for reducing the training cost of CSLM and apply it to SMT decoder. However, they do not show their improvement for decoding speed, and their method is still slower than the n-gram LM. There are several other methods for attempting to implement neural network based LM or translation model for SMT (Devlin et al., 2014; Liu et al., 2014; Auli et al., 2013). However, the decoding speed using n-gram LM is still state-ofthe-art one. Some approaches calculate the probabilities of the n-grams n-grams before decoding, and store them in the n-gram format (Wang et al., 2013a; Arsoy et al., 2013; Arsoy et al., 2014). The ‘converted CSLM’ can be directly used in SMT. Though more n-grams which are not in the train189 Proceedings of the 2014 Conference on Empirical Methods in Natural Language Processing (EMNLP), pages 189–195, October 25-29, 2014, Doha, Qatar. c�2014 Association for Computational Linguistics ing corpus</context>
</contexts>
<marker>Devlin, Zbib, Huang, Lamar, Schwartz, Makhoul, 2014</marker>
<rawString>Jacob Devlin, Rabih Zbib, Zhongqiang Huang, Thomas Lamar, Richard Schwartz, and John Makhoul. 2014. Fast and robust neural network joint models for statistical machine translation. In Proceedings of ACL2014, pages 1370–1380, Baltimore, Maryland, June. Association for Computational Linguistics.</rawString>
</citation>
<citation valid="true">
<authors>
<author>Isao Goto</author>
<author>Bin Lu</author>
<author>Ka Po Chow</author>
<author>Eiichiro Sumita</author>
<author>Benjamin K Tsou</author>
</authors>
<title>Overview of the patent machine translation task at the NTCIR-9 workshop.</title>
<date>2011</date>
<booktitle>In Proceedings of NTCIR-9 Workshop Meeting,</booktitle>
<pages>559--578</pages>
<location>Tokyo, Japan,</location>
<contexts>
<context position="13122" citStr="Goto et al., 2011" startWordPosition="2186" endWordPosition="2189"> Eq. 1 will be applied to calculate their probabilities. 3This Pt(e) hence provides more bilingual information, in comparison with using monolingual target LMs only. Ei − 1 k_1 Pt(,3wk1)× E ( a 191 We combine the n-grams (n=1,2,3,4,5) together and re-normalize the probabilities and backoff weights of the grown LM. Finally the original BNLM and the grown LM are interpolated. The entire process is illustrated in Figure 1. Figure 1: NN based bilingual LM growing. 4 Experiments and Results 4.1 Experiment Setting up The same setting up of the NTCIR-9 Chinese to English translation baseline system (Goto et al., 2011) was followed, only with various LMs to compare them. The Moses phrase-based SMT system was applied (Koehn et al., 2007), together with GIZA++ (Och and Ney, 2003) for alignment and MERT (Och, 2003) for tuning on the development data. Fourteen standard SMT features were used: five translation model scores, one word penalty score, seven distortion scores, and one LM score. The translation performance was measured by the case-insensitive BLEU on the tokenized test data. We used the patent data for the Chinese to English patent translation subtask from the NTCIR-9 patent translation task (Goto et </context>
</contexts>
<marker>Goto, Lu, Chow, Sumita, Tsou, 2011</marker>
<rawString>Isao Goto, Bin Lu, Ka Po Chow, Eiichiro Sumita, and Benjamin K. Tsou. 2011. Overview of the patent machine translation task at the NTCIR-9 workshop. In Proceedings of NTCIR-9 Workshop Meeting, pages 559–578, Tokyo, Japan, December.</rawString>
</citation>
<citation valid="true">
<authors>
<author>Rukmini Iyer</author>
<author>Mari Ostendorf</author>
<author>Herbert Gish</author>
</authors>
<title>Using out-of-domain data to improve indomain language models.</title>
<date>1997</date>
<booktitle>Signal Processing Letters, IEEE,</booktitle>
<pages>4--8</pages>
<contexts>
<context position="2088" citStr="Iyer et al., 1997" startWordPosition="294" endWordPosition="297">g n-grams outside the corpus together with their probabilities into the original LM. This operation is useful as it can make LM perform better through letting it become larger and larger, by only using a small training corpus. There are various methods for adding n-grams selected by different criteria from a monolingual corpus (Ristad and Thomas, 1995; Niesler and Woodland, 1996; Siu and Ostendorf, 2000; Siivola et al., 2007). However, all of these approaches need additional corpora. Meanwhile the extra corpora from different domains will not result in better LMs (Clarkson and Robinson, 1997; Iyer et al., 1997; Bellegarda, 2004; Koehn and Schroeder, *Part of this work was done as Rui Wang visited in NICT. 2007). In addition, it is very difficult or even impossible to collect an extra large corpus for some special domains such as the TED corpus (Cettolo et al., 2012) or for some rare languages. Therefore, to improve the performance of LMs, without assistance of extra corpus, is one of important research topics in SMT. Recently, Continues Space Language Model (CSLM), especially Neural Network based Language Model (NNLM) (Bengio et al., 2003; Schwenk, 2007; Mikolov et al., 2010; Le et al., 2011), is b</context>
</contexts>
<marker>Iyer, Ostendorf, Gish, 1997</marker>
<rawString>Rukmini Iyer, Mari Ostendorf, and Herbert Gish. 1997. Using out-of-domain data to improve indomain language models. Signal Processing Letters, IEEE, 4(8):221–223.</rawString>
</citation>
<citation valid="true">
<authors>
<author>Zhongye Jia</author>
<author>Hai Zhao</author>
</authors>
<title>Kyss 1.0: a framework for automatic evaluation of chinese input method engines.</title>
<date>2013</date>
<journal>Asian Federation of Natural Language Processing.</journal>
<booktitle>In Proceedings of IJCNLP-2013,</booktitle>
<pages>1195--1201</pages>
<location>Nagoya, Japan,</location>
<contexts>
<context position="5125" citStr="Jia and Zhao, 2013" startWordPosition="810" endWordPosition="813">ed using the ‘connecting phrases’. The remainder of this paper is organized as follows: In Section 2, we will review the existing CSLM converting methods. The new neural network based bilingual LM growing method will be proposed in Section 3. In Section 4, the experiments will be conducted and the results will be analyzed. We will conclude our work in Section 5. 2 Existing CSLM Converting Methods Traditional Backoff N-gram LMs (BNLMs) have been widely used in many NLP tasks (Zhang and Zhao, 2013; Jia and Zhao, 2014; Zhao et al., 2013; Zhang et al., 2012; Xu and Zhao, 2012; Wang et al., 2013b; Jia and Zhao, 2013; Wang et al., 2014). Recently, CSLMs become popular because they can obtain more accurate probability estimation. 2.1 Continues Space Language Model A CSLM implemented in a multi-layer neural network contains four layers: the input layer projects (first layer) all words in the context hi onto the projection layer (second layer); the hidden layer (third layer) and the output layer (fourth layer) achieve the non-liner probability estimation and calculate the LM probability P(wi|hi) for the given context (Schwenk, 2007). CSLM is able to calculate the probabilities of all words in the vocabulary </context>
</contexts>
<marker>Jia, Zhao, 2013</marker>
<rawString>Zhongye Jia and Hai Zhao. 2013. Kyss 1.0: a framework for automatic evaluation of chinese input method engines. In Proceedings of IJCNLP-2013, pages 1195–1201, Nagoya, Japan, October. Asian Federation of Natural Language Processing.</rawString>
</citation>
<citation valid="true">
<authors>
<author>Zhongye Jia</author>
<author>Hai Zhao</author>
</authors>
<title>A joint graph model for pinyin-to-chinese conversion with typo correction.</title>
<date>2014</date>
<booktitle>In Proceedings of ACL-2014,</booktitle>
<pages>1512--1523</pages>
<publisher>Association</publisher>
<institution>for Computational Linguistics.</institution>
<location>Baltimore, Maryland,</location>
<contexts>
<context position="5027" citStr="Jia and Zhao, 2014" startWordPosition="790" endWordPosition="793">SLM. Based on this observation, a novel neural network based bilingual LM growing method is proposed using the ‘connecting phrases’. The remainder of this paper is organized as follows: In Section 2, we will review the existing CSLM converting methods. The new neural network based bilingual LM growing method will be proposed in Section 3. In Section 4, the experiments will be conducted and the results will be analyzed. We will conclude our work in Section 5. 2 Existing CSLM Converting Methods Traditional Backoff N-gram LMs (BNLMs) have been widely used in many NLP tasks (Zhang and Zhao, 2013; Jia and Zhao, 2014; Zhao et al., 2013; Zhang et al., 2012; Xu and Zhao, 2012; Wang et al., 2013b; Jia and Zhao, 2013; Wang et al., 2014). Recently, CSLMs become popular because they can obtain more accurate probability estimation. 2.1 Continues Space Language Model A CSLM implemented in a multi-layer neural network contains four layers: the input layer projects (first layer) all words in the context hi onto the projection layer (second layer); the hidden layer (third layer) and the output layer (fourth layer) achieve the non-liner probability estimation and calculate the LM probability P(wi|hi) for the given co</context>
</contexts>
<marker>Jia, Zhao, 2014</marker>
<rawString>Zhongye Jia and Hai Zhao. 2014. A joint graph model for pinyin-to-chinese conversion with typo correction. In Proceedings of ACL-2014, pages 1512– 1523, Baltimore, Maryland, June. Association for Computational Linguistics.</rawString>
</citation>
<citation valid="true">
<authors>
<author>Philipp Koehn</author>
<author>Josh Schroeder</author>
</authors>
<title>Experiments in domain adaptation for statistical machine translation.</title>
<date>2007</date>
<booktitle>In Proceedings of ACL-2007 Workshop on Statistical Machine Translation,</booktitle>
<pages>224--227</pages>
<publisher>Association</publisher>
<institution>for Computational Linguistics.</institution>
<location>Stroudsburg, PA, USA.</location>
<marker>Koehn, Schroeder, 2007</marker>
<rawString>Philipp Koehn and Josh Schroeder. 2007. Experiments in domain adaptation for statistical machine translation. In Proceedings of ACL-2007 Workshop on Statistical Machine Translation, pages 224–227, Stroudsburg, PA, USA. Association for Computational Linguistics.</rawString>
</citation>
<citation valid="false">
<authors>
<author>Philipp Koehn</author>
<author>Hieu Hoang</author>
<author>Alexandra Birch</author>
<author>Chris Callison-Burch</author>
<author>Marcello Federico</author>
<author>Nicola Bertoldi</author>
<author>Brooke Cowan</author>
<author>Wade Shen</author>
</authors>
<title>Moses: Open source toolkit for statistical machine translation.</title>
<date>2007</date>
<booktitle>In Proceedings of ACL-2007,</booktitle>
<pages>177--180</pages>
<publisher>Association</publisher>
<institution>for Computational Linguistics.</institution>
<location>Christine Moran, Richard Zens, Chris Dyer, Ondrej Bojar, Alexandra</location>
<contexts>
<context position="13242" citStr="Koehn et al., 2007" startWordPosition="2206" endWordPosition="2209">arison with using monolingual target LMs only. Ei − 1 k_1 Pt(,3wk1)× E ( a 191 We combine the n-grams (n=1,2,3,4,5) together and re-normalize the probabilities and backoff weights of the grown LM. Finally the original BNLM and the grown LM are interpolated. The entire process is illustrated in Figure 1. Figure 1: NN based bilingual LM growing. 4 Experiments and Results 4.1 Experiment Setting up The same setting up of the NTCIR-9 Chinese to English translation baseline system (Goto et al., 2011) was followed, only with various LMs to compare them. The Moses phrase-based SMT system was applied (Koehn et al., 2007), together with GIZA++ (Och and Ney, 2003) for alignment and MERT (Och, 2003) for tuning on the development data. Fourteen standard SMT features were used: five translation model scores, one word penalty score, seven distortion scores, and one LM score. The translation performance was measured by the case-insensitive BLEU on the tokenized test data. We used the patent data for the Chinese to English patent translation subtask from the NTCIR-9 patent translation task (Goto et al., 2011). The parallel training, development, and test data sets consist of 1 million (M), 2,000, and 2,000 sentences,</context>
</contexts>
<marker>Koehn, Hoang, Birch, Callison-Burch, Federico, Bertoldi, Cowan, Shen, 2007</marker>
<rawString>Philipp Koehn, Hieu Hoang, Alexandra Birch, Chris Callison-Burch, Marcello Federico, Nicola Bertoldi, Brooke Cowan, Wade Shen, Christine Moran, Richard Zens, Chris Dyer, Ondrej Bojar, Alexandra Constantin, and Evan Herbst. 2007. Moses: Open source toolkit for statistical machine translation. In Proceedings of ACL-2007, pages 177–180, Prague, Czech Republic, June. Association for Computational Linguistics.</rawString>
</citation>
<citation valid="true">
<authors>
<author>Philipp Koehn</author>
</authors>
<title>Statistical significance tests for machine translation evaluation.</title>
<date>2004</date>
<booktitle>In Proceedings of EMNLP-2004,</booktitle>
<pages>388--395</pages>
<location>Barcelona, Spain,</location>
<contexts>
<context position="15601" citStr="Koehn, 2004" startWordPosition="2603" endWordPosition="2604">best lists of SMT. Our previous converted LM, Arsoy’s grown LMs and bilingual grown LMs were interpolated with the original BNLMs, using default setting of SRILM5. To reduce the randomness of MERT, we used two methods for tuning the weights of different SMT features, and two BLEU scores are corresponding to these two methods. The BLEU-s indicated that the same weights of the BNLM (BN) features were used for all the SMT systems. The BLEU-i indicated that the MERT was run independently by three times and the average BLEU scores were taken. We also performed the paired bootstrap resampling test (Koehn, 2004)6. Two thousands samples were sampled for each significance test. The marks at the right of the BLEU score indicated whether the LMs were significantly better/worse than the Arsoy’s grown LMs with the same IDs for SMT (“++/−−”: significantly better/worse at α = 0.01, “+/−”: α = 0.05, no mark: not significantly better/worse at α = 0.05). From the results shown in Table 1, we can get the following observations: (1) Nearly all the bilingual grown LMs outperformed both BNLM and our previous converted LM on PPL and BLEU. As the size of grown LMs is increased, the PPL always decreased and the BLEU s</context>
</contexts>
<marker>Koehn, 2004</marker>
<rawString>Philipp Koehn. 2004. Statistical significance tests for machine translation evaluation. In Proceedings of EMNLP-2004, pages 388–395, Barcelona, Spain, July. Association for Computational Linguistics.</rawString>
</citation>
<citation valid="true">
<authors>
<author>Hai-Son Le</author>
<author>Ilya Oparin</author>
<author>Alexandre Allauzen</author>
<author>J Gauvain</author>
<author>Franc¸ois Yvon</author>
</authors>
<title>Structured output layer neural network language model.</title>
<date>2011</date>
<booktitle>In Proceedings of ICASSP-2011,</booktitle>
<pages>5524--5527</pages>
<publisher>IEEE.</publisher>
<location>Prague, Czech Republic,</location>
<contexts>
<context position="2682" citStr="Le et al., 2011" startWordPosition="396" endWordPosition="399">997; Iyer et al., 1997; Bellegarda, 2004; Koehn and Schroeder, *Part of this work was done as Rui Wang visited in NICT. 2007). In addition, it is very difficult or even impossible to collect an extra large corpus for some special domains such as the TED corpus (Cettolo et al., 2012) or for some rare languages. Therefore, to improve the performance of LMs, without assistance of extra corpus, is one of important research topics in SMT. Recently, Continues Space Language Model (CSLM), especially Neural Network based Language Model (NNLM) (Bengio et al., 2003; Schwenk, 2007; Mikolov et al., 2010; Le et al., 2011), is being actively used in SMT (Schwenk et al., 2006; Son et al., 2010; Schwenk, 2010; Schwenk et al., 2012; Son et al., 2012; Niehues and Waibel, 2012). One of the main advantages of CSLM is that it can more accurately predict the probabilities of the n-grams, which are not in the training corpus. However, in practice, CSLMs have not been widely used in the current SMT systems, due to their too high computational cost. Vaswani and colleagues (2013) propose a method for reducing the training cost of CSLM and apply it to SMT decoder. However, they do not show their improvement for decoding spe</context>
</contexts>
<marker>Le, Oparin, Allauzen, Gauvain, Yvon, 2011</marker>
<rawString>Hai-Son Le, Ilya Oparin, Alexandre Allauzen, J Gauvain, and Franc¸ois Yvon. 2011. Structured output layer neural network language model. In Proceedings of ICASSP-2011, pages 5524–5527, Prague, Czech Republic, May. IEEE.</rawString>
</citation>
<citation valid="true">
<authors>
<author>Shujie Liu</author>
<author>Nan Yang</author>
<author>Mu Li</author>
<author>Ming Zhou</author>
</authors>
<title>A recursive recurrent neural network for statistical machine translation.</title>
<date>2014</date>
<booktitle>In Proceedings of ACL-2014,</booktitle>
<pages>1491--1500</pages>
<publisher>Association</publisher>
<institution>for Computational Linguistics.</institution>
<location>Baltimore, Maryland,</location>
<contexts>
<context position="3490" citStr="Liu et al., 2014" startWordPosition="540" endWordPosition="543"> that it can more accurately predict the probabilities of the n-grams, which are not in the training corpus. However, in practice, CSLMs have not been widely used in the current SMT systems, due to their too high computational cost. Vaswani and colleagues (2013) propose a method for reducing the training cost of CSLM and apply it to SMT decoder. However, they do not show their improvement for decoding speed, and their method is still slower than the n-gram LM. There are several other methods for attempting to implement neural network based LM or translation model for SMT (Devlin et al., 2014; Liu et al., 2014; Auli et al., 2013). However, the decoding speed using n-gram LM is still state-ofthe-art one. Some approaches calculate the probabilities of the n-grams n-grams before decoding, and store them in the n-gram format (Wang et al., 2013a; Arsoy et al., 2013; Arsoy et al., 2014). The ‘converted CSLM’ can be directly used in SMT. Though more n-grams which are not in the train189 Proceedings of the 2014 Conference on Empirical Methods in Natural Language Processing (EMNLP), pages 189–195, October 25-29, 2014, Doha, Qatar. c�2014 Association for Computational Linguistics ing corpus can be generated </context>
</contexts>
<marker>Liu, Yang, Li, Zhou, 2014</marker>
<rawString>Shujie Liu, Nan Yang, Mu Li, and Ming Zhou. 2014. A recursive recurrent neural network for statistical machine translation. In Proceedings of ACL-2014, pages 1491–1500, Baltimore, Maryland, June. Association for Computational Linguistics.</rawString>
</citation>
<citation valid="true">
<authors>
<author>Tomas Mikolov</author>
<author>Martin Karafi´at</author>
<author>Lukas Burget</author>
<author>Jan Cernock`y</author>
<author>Sanjeev Khudanpur</author>
</authors>
<title>Recurrent neural network based language model.</title>
<date>2010</date>
<booktitle>In Proceedings of INTERSPEECH-2010,</booktitle>
<pages>1045--1048</pages>
<marker>Mikolov, Karafi´at, Burget, Cernock`y, Khudanpur, 2010</marker>
<rawString>Tomas Mikolov, Martin Karafi´at, Lukas Burget, Jan Cernock`y, and Sanjeev Khudanpur. 2010. Recurrent neural network based language model. In Proceedings of INTERSPEECH-2010, pages 1045– 1048.</rawString>
</citation>
<citation valid="true">
<authors>
<author>Jan Niehues</author>
<author>Alex Waibel</author>
</authors>
<title>Continuous space language models using restricted boltzmann machines.</title>
<date>2012</date>
<booktitle>In Proceedings of IWSLT-2012,</booktitle>
<pages>311--318</pages>
<location>Hong Kong.</location>
<contexts>
<context position="2835" citStr="Niehues and Waibel, 2012" startWordPosition="424" endWordPosition="427">s very difficult or even impossible to collect an extra large corpus for some special domains such as the TED corpus (Cettolo et al., 2012) or for some rare languages. Therefore, to improve the performance of LMs, without assistance of extra corpus, is one of important research topics in SMT. Recently, Continues Space Language Model (CSLM), especially Neural Network based Language Model (NNLM) (Bengio et al., 2003; Schwenk, 2007; Mikolov et al., 2010; Le et al., 2011), is being actively used in SMT (Schwenk et al., 2006; Son et al., 2010; Schwenk, 2010; Schwenk et al., 2012; Son et al., 2012; Niehues and Waibel, 2012). One of the main advantages of CSLM is that it can more accurately predict the probabilities of the n-grams, which are not in the training corpus. However, in practice, CSLMs have not been widely used in the current SMT systems, due to their too high computational cost. Vaswani and colleagues (2013) propose a method for reducing the training cost of CSLM and apply it to SMT decoder. However, they do not show their improvement for decoding speed, and their method is still slower than the n-gram LM. There are several other methods for attempting to implement neural network based LM or translati</context>
</contexts>
<marker>Niehues, Waibel, 2012</marker>
<rawString>Jan Niehues and Alex Waibel. 2012. Continuous space language models using restricted boltzmann machines. In Proceedings of IWSLT-2012, pages 311–318, Hong Kong.</rawString>
</citation>
<citation valid="true">
<authors>
<author>Thomas Niesler</author>
<author>Phil Woodland</author>
</authors>
<title>A variablelength category-based n-gram language model.</title>
<date>1996</date>
<booktitle>In Proceedings of ICASSP-1996,</booktitle>
<volume>1</volume>
<pages>pages</pages>
<contexts>
<context position="1852" citStr="Niesler and Woodland, 1996" startWordPosition="255" endWordPosition="258">show that our method can improve both the perplexity score for LM evaluation and BLEU score for SMT, and significantly outperforms the existing LM growing methods without extra corpus. 1 Introduction ‘Language Model (LM) Growing’ refers to adding n-grams outside the corpus together with their probabilities into the original LM. This operation is useful as it can make LM perform better through letting it become larger and larger, by only using a small training corpus. There are various methods for adding n-grams selected by different criteria from a monolingual corpus (Ristad and Thomas, 1995; Niesler and Woodland, 1996; Siu and Ostendorf, 2000; Siivola et al., 2007). However, all of these approaches need additional corpora. Meanwhile the extra corpora from different domains will not result in better LMs (Clarkson and Robinson, 1997; Iyer et al., 1997; Bellegarda, 2004; Koehn and Schroeder, *Part of this work was done as Rui Wang visited in NICT. 2007). In addition, it is very difficult or even impossible to collect an extra large corpus for some special domains such as the TED corpus (Cettolo et al., 2012) or for some rare languages. Therefore, to improve the performance of LMs, without assistance of extra </context>
</contexts>
<marker>Niesler, Woodland, 1996</marker>
<rawString>Thomas Niesler and Phil Woodland. 1996. A variablelength category-based n-gram language model. In Proceedings of ICASSP-1996, volume 1, pages 164– 167 vol. 1.</rawString>
</citation>
<citation valid="true">
<authors>
<author>Franz Josef Och</author>
<author>Hermann Ney</author>
</authors>
<title>A systematic comparison of various statistical alignment models.</title>
<date>2003</date>
<journal>Computational Linguistics,</journal>
<volume>29</volume>
<issue>1</issue>
<contexts>
<context position="13284" citStr="Och and Ney, 2003" startWordPosition="2214" endWordPosition="2217">ly. Ei − 1 k_1 Pt(,3wk1)× E ( a 191 We combine the n-grams (n=1,2,3,4,5) together and re-normalize the probabilities and backoff weights of the grown LM. Finally the original BNLM and the grown LM are interpolated. The entire process is illustrated in Figure 1. Figure 1: NN based bilingual LM growing. 4 Experiments and Results 4.1 Experiment Setting up The same setting up of the NTCIR-9 Chinese to English translation baseline system (Goto et al., 2011) was followed, only with various LMs to compare them. The Moses phrase-based SMT system was applied (Koehn et al., 2007), together with GIZA++ (Och and Ney, 2003) for alignment and MERT (Och, 2003) for tuning on the development data. Fourteen standard SMT features were used: five translation model scores, one word penalty score, seven distortion scores, and one LM score. The translation performance was measured by the case-insensitive BLEU on the tokenized test data. We used the patent data for the Chinese to English patent translation subtask from the NTCIR-9 patent translation task (Goto et al., 2011). The parallel training, development, and test data sets consist of 1 million (M), 2,000, and 2,000 sentences, respectively. Using SRILM (Stolcke, 2002;</context>
</contexts>
<marker>Och, Ney, 2003</marker>
<rawString>Franz Josef Och and Hermann Ney. 2003. A systematic comparison of various statistical alignment models. Computational Linguistics, 29(1):19–51, March.</rawString>
</citation>
<citation valid="true">
<authors>
<author>Franz Josef Och</author>
</authors>
<title>Minimum error rate training in statistical machine translation.</title>
<date>2003</date>
<booktitle>In Proceedings of ACL-2003,</booktitle>
<pages>160--167</pages>
<location>Sapporo, Japan,</location>
<contexts>
<context position="13319" citStr="Och, 2003" startWordPosition="2223" endWordPosition="2224">ine the n-grams (n=1,2,3,4,5) together and re-normalize the probabilities and backoff weights of the grown LM. Finally the original BNLM and the grown LM are interpolated. The entire process is illustrated in Figure 1. Figure 1: NN based bilingual LM growing. 4 Experiments and Results 4.1 Experiment Setting up The same setting up of the NTCIR-9 Chinese to English translation baseline system (Goto et al., 2011) was followed, only with various LMs to compare them. The Moses phrase-based SMT system was applied (Koehn et al., 2007), together with GIZA++ (Och and Ney, 2003) for alignment and MERT (Och, 2003) for tuning on the development data. Fourteen standard SMT features were used: five translation model scores, one word penalty score, seven distortion scores, and one LM score. The translation performance was measured by the case-insensitive BLEU on the tokenized test data. We used the patent data for the Chinese to English patent translation subtask from the NTCIR-9 patent translation task (Goto et al., 2011). The parallel training, development, and test data sets consist of 1 million (M), 2,000, and 2,000 sentences, respectively. Using SRILM (Stolcke, 2002; Stolcke et al., 2011), we trained </context>
</contexts>
<marker>Och, 2003</marker>
<rawString>Franz Josef Och. 2003. Minimum error rate training in statistical machine translation. In Proceedings of ACL-2003, pages 160–167, Sapporo, Japan, July.</rawString>
</citation>
<citation valid="true">
<authors>
<author>Eric Sven Ristad</author>
<author>Robert G Thomas</author>
</authors>
<title>New techniques for context modeling.</title>
<date>1995</date>
<booktitle>In Proceedings of ACL-1995,</booktitle>
<pages>220--227</pages>
<publisher>Association</publisher>
<institution>for Computational Linguistics.</institution>
<location>Cambridge, Massachusetts.</location>
<contexts>
<context position="1824" citStr="Ristad and Thomas, 1995" startWordPosition="251" endWordPosition="254">rpus in SMT. The results show that our method can improve both the perplexity score for LM evaluation and BLEU score for SMT, and significantly outperforms the existing LM growing methods without extra corpus. 1 Introduction ‘Language Model (LM) Growing’ refers to adding n-grams outside the corpus together with their probabilities into the original LM. This operation is useful as it can make LM perform better through letting it become larger and larger, by only using a small training corpus. There are various methods for adding n-grams selected by different criteria from a monolingual corpus (Ristad and Thomas, 1995; Niesler and Woodland, 1996; Siu and Ostendorf, 2000; Siivola et al., 2007). However, all of these approaches need additional corpora. Meanwhile the extra corpora from different domains will not result in better LMs (Clarkson and Robinson, 1997; Iyer et al., 1997; Bellegarda, 2004; Koehn and Schroeder, *Part of this work was done as Rui Wang visited in NICT. 2007). In addition, it is very difficult or even impossible to collect an extra large corpus for some special domains such as the TED corpus (Cettolo et al., 2012) or for some rare languages. Therefore, to improve the performance of LMs, </context>
</contexts>
<marker>Ristad, Thomas, 1995</marker>
<rawString>Eric Sven Ristad and Robert G. Thomas. 1995. New techniques for context modeling. In Proceedings of ACL-1995, pages 220–227, Cambridge, Massachusetts. Association for Computational Linguistics.</rawString>
</citation>
<citation valid="true">
<authors>
<author>Holger Schwenk</author>
<author>Daniel Dchelotte</author>
<author>Jean-Luc Gauvain</author>
</authors>
<title>Continuous space language models for statistical machine translation.</title>
<date>2006</date>
<booktitle>In Proceedings of COLING ACL-2006,</booktitle>
<pages>723--730</pages>
<publisher>Association</publisher>
<institution>for Computational Linguistics.</institution>
<location>Sydney, Australia,</location>
<contexts>
<context position="2735" citStr="Schwenk et al., 2006" startWordPosition="406" endWordPosition="409">nd Schroeder, *Part of this work was done as Rui Wang visited in NICT. 2007). In addition, it is very difficult or even impossible to collect an extra large corpus for some special domains such as the TED corpus (Cettolo et al., 2012) or for some rare languages. Therefore, to improve the performance of LMs, without assistance of extra corpus, is one of important research topics in SMT. Recently, Continues Space Language Model (CSLM), especially Neural Network based Language Model (NNLM) (Bengio et al., 2003; Schwenk, 2007; Mikolov et al., 2010; Le et al., 2011), is being actively used in SMT (Schwenk et al., 2006; Son et al., 2010; Schwenk, 2010; Schwenk et al., 2012; Son et al., 2012; Niehues and Waibel, 2012). One of the main advantages of CSLM is that it can more accurately predict the probabilities of the n-grams, which are not in the training corpus. However, in practice, CSLMs have not been widely used in the current SMT systems, due to their too high computational cost. Vaswani and colleagues (2013) propose a method for reducing the training cost of CSLM and apply it to SMT decoder. However, they do not show their improvement for decoding speed, and their method is still slower than the n-gram </context>
</contexts>
<marker>Schwenk, Dchelotte, Gauvain, 2006</marker>
<rawString>Holger Schwenk, Daniel Dchelotte, and Jean-Luc Gauvain. 2006. Continuous space language models for statistical machine translation. In Proceedings of COLING ACL-2006, pages 723–730, Sydney, Australia, July. Association for Computational Linguistics.</rawString>
</citation>
<citation valid="true">
<authors>
<author>Holger Schwenk</author>
<author>Anthony Rousseau</author>
<author>Mohammed Attik</author>
</authors>
<title>Large, pruned or continuous space language models on a gpu for statistical machine translation.</title>
<date>2012</date>
<booktitle>In Proceedings of the NAACL-HLT 2012 Workshop: Will We Ever Really Replace the N-gram Model? On the Future of Language Modeling for HLT, WLM ’12,</booktitle>
<pages>11--19</pages>
<publisher>Association</publisher>
<institution>for Computational Linguistics.</institution>
<location>Montreal, Canada,</location>
<contexts>
<context position="2790" citStr="Schwenk et al., 2012" startWordPosition="416" endWordPosition="419">isited in NICT. 2007). In addition, it is very difficult or even impossible to collect an extra large corpus for some special domains such as the TED corpus (Cettolo et al., 2012) or for some rare languages. Therefore, to improve the performance of LMs, without assistance of extra corpus, is one of important research topics in SMT. Recently, Continues Space Language Model (CSLM), especially Neural Network based Language Model (NNLM) (Bengio et al., 2003; Schwenk, 2007; Mikolov et al., 2010; Le et al., 2011), is being actively used in SMT (Schwenk et al., 2006; Son et al., 2010; Schwenk, 2010; Schwenk et al., 2012; Son et al., 2012; Niehues and Waibel, 2012). One of the main advantages of CSLM is that it can more accurately predict the probabilities of the n-grams, which are not in the training corpus. However, in practice, CSLMs have not been widely used in the current SMT systems, due to their too high computational cost. Vaswani and colleagues (2013) propose a method for reducing the training cost of CSLM and apply it to SMT decoder. However, they do not show their improvement for decoding speed, and their method is still slower than the n-gram LM. There are several other methods for attempting to i</context>
</contexts>
<marker>Schwenk, Rousseau, Attik, 2012</marker>
<rawString>Holger Schwenk, Anthony Rousseau, and Mohammed Attik. 2012. Large, pruned or continuous space language models on a gpu for statistical machine translation. In Proceedings of the NAACL-HLT 2012 Workshop: Will We Ever Really Replace the N-gram Model? On the Future of Language Modeling for HLT, WLM ’12, pages 11–19, Montreal, Canada, June. Association for Computational Linguistics.</rawString>
</citation>
<citation valid="true">
<authors>
<author>Holger Schwenk</author>
</authors>
<title>Continuous space language models.</title>
<date>2007</date>
<journal>Computer Speech and Language,</journal>
<volume>21</volume>
<issue>3</issue>
<contexts>
<context position="2642" citStr="Schwenk, 2007" startWordPosition="390" endWordPosition="391"> better LMs (Clarkson and Robinson, 1997; Iyer et al., 1997; Bellegarda, 2004; Koehn and Schroeder, *Part of this work was done as Rui Wang visited in NICT. 2007). In addition, it is very difficult or even impossible to collect an extra large corpus for some special domains such as the TED corpus (Cettolo et al., 2012) or for some rare languages. Therefore, to improve the performance of LMs, without assistance of extra corpus, is one of important research topics in SMT. Recently, Continues Space Language Model (CSLM), especially Neural Network based Language Model (NNLM) (Bengio et al., 2003; Schwenk, 2007; Mikolov et al., 2010; Le et al., 2011), is being actively used in SMT (Schwenk et al., 2006; Son et al., 2010; Schwenk, 2010; Schwenk et al., 2012; Son et al., 2012; Niehues and Waibel, 2012). One of the main advantages of CSLM is that it can more accurately predict the probabilities of the n-grams, which are not in the training corpus. However, in practice, CSLMs have not been widely used in the current SMT systems, due to their too high computational cost. Vaswani and colleagues (2013) propose a method for reducing the training cost of CSLM and apply it to SMT decoder. However, they do not</context>
<context position="5648" citStr="Schwenk, 2007" startWordPosition="892" endWordPosition="893">et al., 2013; Zhang et al., 2012; Xu and Zhao, 2012; Wang et al., 2013b; Jia and Zhao, 2013; Wang et al., 2014). Recently, CSLMs become popular because they can obtain more accurate probability estimation. 2.1 Continues Space Language Model A CSLM implemented in a multi-layer neural network contains four layers: the input layer projects (first layer) all words in the context hi onto the projection layer (second layer); the hidden layer (third layer) and the output layer (fourth layer) achieve the non-liner probability estimation and calculate the LM probability P(wi|hi) for the given context (Schwenk, 2007). CSLM is able to calculate the probabilities of all words in the vocabulary of the corpus given the context. However, due to too high computational complexity, CSLM is mainly used to calculate the probabilities of a subset of the whole vocabulary (Schwenk, 2007). This subset is called a shortlist, which consists of the most frequent words in the vocabulary. CSLM also calculates the sum of the probabilities of all words not included in the short-list by assigning a neuron with the help of BNLM. The probabilities of other words not in the short-list are obtained from an BNLM (Schwenk, 2007; Sch</context>
<context position="14156" citStr="Schwenk, 2007" startWordPosition="2355" endWordPosition="2356"> case-insensitive BLEU on the tokenized test data. We used the patent data for the Chinese to English patent translation subtask from the NTCIR-9 patent translation task (Goto et al., 2011). The parallel training, development, and test data sets consist of 1 million (M), 2,000, and 2,000 sentences, respectively. Using SRILM (Stolcke, 2002; Stolcke et al., 2011), we trained a 5-gram LM with the interpolated Kneser-Ney smoothing method using the 1M English training sentences containing 42M words without cutoff. The 2,3,4,5-CSLMs were trained on the same 1M training sentences using CSLM toolkit (Schwenk, 2007; Schwenk, 2010). The settings for CSLMs were: input layer of the same dimension as vocabulary size (456K), projection layer of dimension 256 for each word, hidden layer of dimension 384 and output layer (short-list) of dimension 8192, which were recommended in the CSLM toolkit and (Wang et al., 2013a)4. 4Arsoy used around 55 M words as the corpus, including 4.2 Results The experiment results were divided into four groups: the original BNLMs (BN), the CSLM Re-ranking (RE), our previous converting (WA), the Arsoy’s growing, and our growing methods. For our bilingual LM growing method, 5 bilingu</context>
</contexts>
<marker>Schwenk, 2007</marker>
<rawString>Holger Schwenk. 2007. Continuous space language models. Computer Speech and Language, 21(3):492–518.</rawString>
</citation>
<citation valid="true">
<authors>
<author>Holger Schwenk</author>
</authors>
<title>Continuous-space language models for statistical machine translation. The Prague Bulletin of Mathematical Linguistics,</title>
<date>2010</date>
<pages>137--146</pages>
<contexts>
<context position="2768" citStr="Schwenk, 2010" startWordPosition="414" endWordPosition="415">e as Rui Wang visited in NICT. 2007). In addition, it is very difficult or even impossible to collect an extra large corpus for some special domains such as the TED corpus (Cettolo et al., 2012) or for some rare languages. Therefore, to improve the performance of LMs, without assistance of extra corpus, is one of important research topics in SMT. Recently, Continues Space Language Model (CSLM), especially Neural Network based Language Model (NNLM) (Bengio et al., 2003; Schwenk, 2007; Mikolov et al., 2010; Le et al., 2011), is being actively used in SMT (Schwenk et al., 2006; Son et al., 2010; Schwenk, 2010; Schwenk et al., 2012; Son et al., 2012; Niehues and Waibel, 2012). One of the main advantages of CSLM is that it can more accurately predict the probabilities of the n-grams, which are not in the training corpus. However, in practice, CSLMs have not been widely used in the current SMT systems, due to their too high computational cost. Vaswani and colleagues (2013) propose a method for reducing the training cost of CSLM and apply it to SMT decoder. However, they do not show their improvement for decoding speed, and their method is still slower than the n-gram LM. There are several other metho</context>
<context position="6258" citStr="Schwenk, 2010" startWordPosition="995" endWordPosition="996">07). CSLM is able to calculate the probabilities of all words in the vocabulary of the corpus given the context. However, due to too high computational complexity, CSLM is mainly used to calculate the probabilities of a subset of the whole vocabulary (Schwenk, 2007). This subset is called a shortlist, which consists of the most frequent words in the vocabulary. CSLM also calculates the sum of the probabilities of all words not included in the short-list by assigning a neuron with the help of BNLM. The probabilities of other words not in the short-list are obtained from an BNLM (Schwenk, 2007; Schwenk, 2010; Wang et al., 2013a). Let wi and hi be the current word and history, respectively. CSLM with a BNLM calculates the probability P(wi|hi) of wi given hi, as follows: I ∑wEVwi |&apos;i) E)P (h) if wi E V0 (1) 0 Pc wh s i Pb(wi|hi) otherwise where Vo is the short-list, P,(·) is the probability calculated by CSLM, ∑wEU0 P,(w|hi) is the summary of probabilities of the neuron for all the words in the short-list, Pb(·) is the probability calculated by the BNLM, and Ps(hi) = E Pb(v|hi). (2) vEVo We may regard that CSLM redistributes the probability mass of all words in the short-list, which is calculated b</context>
<context position="14172" citStr="Schwenk, 2010" startWordPosition="2357" endWordPosition="2358">ve BLEU on the tokenized test data. We used the patent data for the Chinese to English patent translation subtask from the NTCIR-9 patent translation task (Goto et al., 2011). The parallel training, development, and test data sets consist of 1 million (M), 2,000, and 2,000 sentences, respectively. Using SRILM (Stolcke, 2002; Stolcke et al., 2011), we trained a 5-gram LM with the interpolated Kneser-Ney smoothing method using the 1M English training sentences containing 42M words without cutoff. The 2,3,4,5-CSLMs were trained on the same 1M training sentences using CSLM toolkit (Schwenk, 2007; Schwenk, 2010). The settings for CSLMs were: input layer of the same dimension as vocabulary size (456K), projection layer of dimension 256 for each word, hidden layer of dimension 384 and output layer (short-list) of dimension 8192, which were recommended in the CSLM toolkit and (Wang et al., 2013a)4. 4Arsoy used around 55 M words as the corpus, including 4.2 Results The experiment results were divided into four groups: the original BNLMs (BN), the CSLM Re-ranking (RE), our previous converting (WA), the Arsoy’s growing, and our growing methods. For our bilingual LM growing method, 5 bilingual grown LMs (BI</context>
</contexts>
<marker>Schwenk, 2010</marker>
<rawString>Holger Schwenk. 2010. Continuous-space language models for statistical machine translation. The Prague Bulletin of Mathematical Linguistics, pages 137–146.</rawString>
</citation>
<citation valid="true">
<authors>
<author>Vesa Siivola</author>
<author>Teemu Hirsimki</author>
<author>Sami Virpioja</author>
</authors>
<title>On growing and pruning kneser-ney smoothed n-gram models.</title>
<date>2007</date>
<journal>IEEE Transactions on Audio, Speech, and Language,</journal>
<volume>15</volume>
<issue>5</issue>
<contexts>
<context position="1900" citStr="Siivola et al., 2007" startWordPosition="263" endWordPosition="267">score for LM evaluation and BLEU score for SMT, and significantly outperforms the existing LM growing methods without extra corpus. 1 Introduction ‘Language Model (LM) Growing’ refers to adding n-grams outside the corpus together with their probabilities into the original LM. This operation is useful as it can make LM perform better through letting it become larger and larger, by only using a small training corpus. There are various methods for adding n-grams selected by different criteria from a monolingual corpus (Ristad and Thomas, 1995; Niesler and Woodland, 1996; Siu and Ostendorf, 2000; Siivola et al., 2007). However, all of these approaches need additional corpora. Meanwhile the extra corpora from different domains will not result in better LMs (Clarkson and Robinson, 1997; Iyer et al., 1997; Bellegarda, 2004; Koehn and Schroeder, *Part of this work was done as Rui Wang visited in NICT. 2007). In addition, it is very difficult or even impossible to collect an extra large corpus for some special domains such as the TED corpus (Cettolo et al., 2012) or for some rare languages. Therefore, to improve the performance of LMs, without assistance of extra corpus, is one of important research topics in S</context>
</contexts>
<marker>Siivola, Hirsimki, Virpioja, 2007</marker>
<rawString>Vesa Siivola, Teemu Hirsimki, and Sami Virpioja. 2007. On growing and pruning kneser-ney smoothed n-gram models. IEEE Transactions on Audio, Speech, and Language, 15(5):1617–1624.</rawString>
</citation>
<citation valid="true">
<authors>
<author>Manhung Siu</author>
<author>Mari Ostendorf</author>
</authors>
<title>Variable ngrams and extensions for conversational speech language modeling.</title>
<date>2000</date>
<journal>IEEE Transactions on Speech and Audio,</journal>
<volume>8</volume>
<issue>1</issue>
<contexts>
<context position="1877" citStr="Siu and Ostendorf, 2000" startWordPosition="259" endWordPosition="262">rove both the perplexity score for LM evaluation and BLEU score for SMT, and significantly outperforms the existing LM growing methods without extra corpus. 1 Introduction ‘Language Model (LM) Growing’ refers to adding n-grams outside the corpus together with their probabilities into the original LM. This operation is useful as it can make LM perform better through letting it become larger and larger, by only using a small training corpus. There are various methods for adding n-grams selected by different criteria from a monolingual corpus (Ristad and Thomas, 1995; Niesler and Woodland, 1996; Siu and Ostendorf, 2000; Siivola et al., 2007). However, all of these approaches need additional corpora. Meanwhile the extra corpora from different domains will not result in better LMs (Clarkson and Robinson, 1997; Iyer et al., 1997; Bellegarda, 2004; Koehn and Schroeder, *Part of this work was done as Rui Wang visited in NICT. 2007). In addition, it is very difficult or even impossible to collect an extra large corpus for some special domains such as the TED corpus (Cettolo et al., 2012) or for some rare languages. Therefore, to improve the performance of LMs, without assistance of extra corpus, is one of importa</context>
</contexts>
<marker>Siu, Ostendorf, 2000</marker>
<rawString>Manhung Siu and Mari Ostendorf. 2000. Variable ngrams and extensions for conversational speech language modeling. IEEE Transactions on Speech and Audio, 8(1):63–75.</rawString>
</citation>
<citation valid="true">
<authors>
<author>Le Hai Son</author>
<author>Alexandre Allauzen</author>
<author>Guillaume Wisniewski</author>
<author>Franc¸ois Yvon</author>
</authors>
<title>Training continuous space language models: some practical issues.</title>
<date>2010</date>
<booktitle>In Proceedings of EMNLP-2010,</booktitle>
<pages>778--788</pages>
<publisher>Association</publisher>
<institution>for Computational Linguistics.</institution>
<location>Cambridge, Massachusetts,</location>
<contexts>
<context position="2753" citStr="Son et al., 2010" startWordPosition="410" endWordPosition="413"> this work was done as Rui Wang visited in NICT. 2007). In addition, it is very difficult or even impossible to collect an extra large corpus for some special domains such as the TED corpus (Cettolo et al., 2012) or for some rare languages. Therefore, to improve the performance of LMs, without assistance of extra corpus, is one of important research topics in SMT. Recently, Continues Space Language Model (CSLM), especially Neural Network based Language Model (NNLM) (Bengio et al., 2003; Schwenk, 2007; Mikolov et al., 2010; Le et al., 2011), is being actively used in SMT (Schwenk et al., 2006; Son et al., 2010; Schwenk, 2010; Schwenk et al., 2012; Son et al., 2012; Niehues and Waibel, 2012). One of the main advantages of CSLM is that it can more accurately predict the probabilities of the n-grams, which are not in the training corpus. However, in practice, CSLMs have not been widely used in the current SMT systems, due to their too high computational cost. Vaswani and colleagues (2013) propose a method for reducing the training cost of CSLM and apply it to SMT decoder. However, they do not show their improvement for decoding speed, and their method is still slower than the n-gram LM. There are seve</context>
</contexts>
<marker>Son, Allauzen, Wisniewski, Yvon, 2010</marker>
<rawString>Le Hai Son, Alexandre Allauzen, Guillaume Wisniewski, and Franc¸ois Yvon. 2010. Training continuous space language models: some practical issues. In Proceedings of EMNLP-2010, pages 778– 788, Cambridge, Massachusetts, October. Association for Computational Linguistics.</rawString>
</citation>
<citation valid="true">
<authors>
<author>Le Hai Son</author>
<author>Alexandre Allauzen</author>
<author>Franc¸ois Yvon</author>
</authors>
<title>Continuous space translation models with neural networks.</title>
<date>2012</date>
<booktitle>In Proceedings of NAACL HLT2012,</booktitle>
<pages>39--48</pages>
<publisher>Association</publisher>
<institution>for Computational Linguistics.</institution>
<location>Montreal, Canada,</location>
<contexts>
<context position="2808" citStr="Son et al., 2012" startWordPosition="420" endWordPosition="423"> In addition, it is very difficult or even impossible to collect an extra large corpus for some special domains such as the TED corpus (Cettolo et al., 2012) or for some rare languages. Therefore, to improve the performance of LMs, without assistance of extra corpus, is one of important research topics in SMT. Recently, Continues Space Language Model (CSLM), especially Neural Network based Language Model (NNLM) (Bengio et al., 2003; Schwenk, 2007; Mikolov et al., 2010; Le et al., 2011), is being actively used in SMT (Schwenk et al., 2006; Son et al., 2010; Schwenk, 2010; Schwenk et al., 2012; Son et al., 2012; Niehues and Waibel, 2012). One of the main advantages of CSLM is that it can more accurately predict the probabilities of the n-grams, which are not in the training corpus. However, in practice, CSLMs have not been widely used in the current SMT systems, due to their too high computational cost. Vaswani and colleagues (2013) propose a method for reducing the training cost of CSLM and apply it to SMT decoder. However, they do not show their improvement for decoding speed, and their method is still slower than the n-gram LM. There are several other methods for attempting to implement neural ne</context>
</contexts>
<marker>Son, Allauzen, Yvon, 2012</marker>
<rawString>Le Hai Son, Alexandre Allauzen, and Franc¸ois Yvon. 2012. Continuous space translation models with neural networks. In Proceedings of NAACL HLT2012, pages 39–48, Montreal, Canada, June. Association for Computational Linguistics.</rawString>
</citation>
<citation valid="true">
<authors>
<author>Andreas Stolcke</author>
<author>Jing Zheng</author>
<author>Wen Wang</author>
<author>Victor Abrash</author>
</authors>
<title>SRILM at sixteen: Update and outlook.</title>
<date>2011</date>
<booktitle>In Proceedings of INTERSPEECH 2011,</booktitle>
<location>Waikoloa, HI, USA,</location>
<contexts>
<context position="13906" citStr="Stolcke et al., 2011" startWordPosition="2315" endWordPosition="2318">for alignment and MERT (Och, 2003) for tuning on the development data. Fourteen standard SMT features were used: five translation model scores, one word penalty score, seven distortion scores, and one LM score. The translation performance was measured by the case-insensitive BLEU on the tokenized test data. We used the patent data for the Chinese to English patent translation subtask from the NTCIR-9 patent translation task (Goto et al., 2011). The parallel training, development, and test data sets consist of 1 million (M), 2,000, and 2,000 sentences, respectively. Using SRILM (Stolcke, 2002; Stolcke et al., 2011), we trained a 5-gram LM with the interpolated Kneser-Ney smoothing method using the 1M English training sentences containing 42M words without cutoff. The 2,3,4,5-CSLMs were trained on the same 1M training sentences using CSLM toolkit (Schwenk, 2007; Schwenk, 2010). The settings for CSLMs were: input layer of the same dimension as vocabulary size (456K), projection layer of dimension 256 for each word, hidden layer of dimension 384 and output layer (short-list) of dimension 8192, which were recommended in the CSLM toolkit and (Wang et al., 2013a)4. 4Arsoy used around 55 M words as the corpus,</context>
</contexts>
<marker>Stolcke, Zheng, Wang, Abrash, 2011</marker>
<rawString>Andreas Stolcke, Jing Zheng, Wen Wang, and Victor Abrash. 2011. SRILM at sixteen: Update and outlook. In Proceedings of INTERSPEECH 2011, Waikoloa, HI, USA, December.</rawString>
</citation>
<citation valid="true">
<authors>
<author>Andreas Stolcke</author>
</authors>
<title>Entropy-based pruning of backoff language models.</title>
<date>1998</date>
<booktitle>In Proceedings of DARPA Broadcast News Transcription and Understanding Workshop,</booktitle>
<pages>270--274</pages>
<location>Lansdowne, VA, USA.</location>
<contexts>
<context position="8035" citStr="Stolcke, 1998" startWordPosition="1315" endWordPosition="1316">e when no grown n-grams are generated. The method developed by Arsoy and colleagues (Arsoy et al., 2013; Arsoy et al., 2014) adds all the words in the short-list after the tail word of the i-grams to construct the (i+1)-grams. For example, if the i-gram is “I want”, then the (i+1)- grams will be “I want *”, where “*” stands for any word in the short list. Then the probabilities of the (i+1)-grams are calculated using (i+1)-CSLM. So a very large intermediate (i+1)-grams will have to be grownl, and then be pruned into smaller suitable size using an entropy-based LM pruning method modified from (Stolcke, 1998). The (i+2)- grams are grown using (i+1)-grams, recursively. &apos;In practice, the probabilities of all the target/tail words in the short list for the history i-grams can be calculated by the neurons in the output layer at the same time, which will save some time. According to our experiments, the time cost for Arsoy’s growing method is around 4 times more than our proposed method, if the LMs which are 10 times larger than the original one are grown with other settings all the same. P(wi|hi) = 190 3 Bilingual LM Growing The translation output of a phrase-based SMT system can be regarded as a conc</context>
</contexts>
<marker>Stolcke, 1998</marker>
<rawString>Andreas Stolcke. 1998. Entropy-based pruning of backoff language models. In Proceedings of DARPA Broadcast News Transcription and Understanding Workshop, pages 270–274, Lansdowne, VA, USA.</rawString>
</citation>
<citation valid="true">
<authors>
<author>Andreas Stolcke</author>
</authors>
<title>Srilm-an extensible language modeling toolkit.</title>
<date>2002</date>
<booktitle>In Proceedings of INTERSPEECH-2002,</booktitle>
<pages>257--286</pages>
<location>Seattle, USA,</location>
<contexts>
<context position="13883" citStr="Stolcke, 2002" startWordPosition="2313" endWordPosition="2314">and Ney, 2003) for alignment and MERT (Och, 2003) for tuning on the development data. Fourteen standard SMT features were used: five translation model scores, one word penalty score, seven distortion scores, and one LM score. The translation performance was measured by the case-insensitive BLEU on the tokenized test data. We used the patent data for the Chinese to English patent translation subtask from the NTCIR-9 patent translation task (Goto et al., 2011). The parallel training, development, and test data sets consist of 1 million (M), 2,000, and 2,000 sentences, respectively. Using SRILM (Stolcke, 2002; Stolcke et al., 2011), we trained a 5-gram LM with the interpolated Kneser-Ney smoothing method using the 1M English training sentences containing 42M words without cutoff. The 2,3,4,5-CSLMs were trained on the same 1M training sentences using CSLM toolkit (Schwenk, 2007; Schwenk, 2010). The settings for CSLMs were: input layer of the same dimension as vocabulary size (456K), projection layer of dimension 256 for each word, hidden layer of dimension 384 and output layer (short-list) of dimension 8192, which were recommended in the CSLM toolkit and (Wang et al., 2013a)4. 4Arsoy used around 55</context>
</contexts>
<marker>Stolcke, 2002</marker>
<rawString>Andreas Stolcke. 2002. Srilm-an extensible language modeling toolkit. In Proceedings of INTERSPEECH-2002, pages 257–286, Seattle, USA, November.</rawString>
</citation>
<citation valid="true">
<authors>
<author>Ashish Vaswani</author>
<author>Yinggong Zhao</author>
<author>Victoria Fossum</author>
<author>David Chiang</author>
</authors>
<title>Decoding with largescale neural language models improves translation.</title>
<date>2013</date>
<booktitle>In Proceedings of EMNLP-2013,</booktitle>
<pages>1387--1392</pages>
<publisher>Association</publisher>
<institution>for Computational Linguistics.</institution>
<location>Seattle, Washington, USA,</location>
<marker>Vaswani, Zhao, Fossum, Chiang, 2013</marker>
<rawString>Ashish Vaswani, Yinggong Zhao, Victoria Fossum, and David Chiang. 2013. Decoding with largescale neural language models improves translation. In Proceedings of EMNLP-2013, pages 1387–1392, Seattle, Washington, USA, October. Association for Computational Linguistics.</rawString>
</citation>
<citation valid="true">
<authors>
<author>Rui Wang</author>
<author>Masao Utiyama</author>
<author>Isao Goto</author>
<author>Eiichro Sumita</author>
<author>Hai Zhao</author>
<author>Bao-Liang Lu</author>
</authors>
<title>Converting continuous-space language models into n-gram language models for statistical machine translation.</title>
<date>2013</date>
<booktitle>In Proceedings of EMNLP-2013,</booktitle>
<pages>845--850</pages>
<publisher>Association</publisher>
<institution>for Computational Linguistics.</institution>
<location>Seattle, Washington, USA,</location>
<contexts>
<context position="3724" citStr="Wang et al., 2013" startWordPosition="579" endWordPosition="582">Vaswani and colleagues (2013) propose a method for reducing the training cost of CSLM and apply it to SMT decoder. However, they do not show their improvement for decoding speed, and their method is still slower than the n-gram LM. There are several other methods for attempting to implement neural network based LM or translation model for SMT (Devlin et al., 2014; Liu et al., 2014; Auli et al., 2013). However, the decoding speed using n-gram LM is still state-ofthe-art one. Some approaches calculate the probabilities of the n-grams n-grams before decoding, and store them in the n-gram format (Wang et al., 2013a; Arsoy et al., 2013; Arsoy et al., 2014). The ‘converted CSLM’ can be directly used in SMT. Though more n-grams which are not in the train189 Proceedings of the 2014 Conference on Empirical Methods in Natural Language Processing (EMNLP), pages 189–195, October 25-29, 2014, Doha, Qatar. c�2014 Association for Computational Linguistics ing corpus can be generated by using some of these ‘converting’ methods, these methods only consider the monolingual information, and do not take the bilingual information into account. We observe that the translation output of a phrase-based SMT system is conca</context>
<context position="5104" citStr="Wang et al., 2013" startWordPosition="806" endWordPosition="809">ing method is proposed using the ‘connecting phrases’. The remainder of this paper is organized as follows: In Section 2, we will review the existing CSLM converting methods. The new neural network based bilingual LM growing method will be proposed in Section 3. In Section 4, the experiments will be conducted and the results will be analyzed. We will conclude our work in Section 5. 2 Existing CSLM Converting Methods Traditional Backoff N-gram LMs (BNLMs) have been widely used in many NLP tasks (Zhang and Zhao, 2013; Jia and Zhao, 2014; Zhao et al., 2013; Zhang et al., 2012; Xu and Zhao, 2012; Wang et al., 2013b; Jia and Zhao, 2013; Wang et al., 2014). Recently, CSLMs become popular because they can obtain more accurate probability estimation. 2.1 Continues Space Language Model A CSLM implemented in a multi-layer neural network contains four layers: the input layer projects (first layer) all words in the context hi onto the projection layer (second layer); the hidden layer (third layer) and the output layer (fourth layer) achieve the non-liner probability estimation and calculate the LM probability P(wi|hi) for the given context (Schwenk, 2007). CSLM is able to calculate the probabilities of all wor</context>
<context position="6977" citStr="Wang et al., 2013" startWordPosition="1123" endWordPosition="1126">alculates the probability P(wi|hi) of wi given hi, as follows: I ∑wEVwi |&apos;i) E)P (h) if wi E V0 (1) 0 Pc wh s i Pb(wi|hi) otherwise where Vo is the short-list, P,(·) is the probability calculated by CSLM, ∑wEU0 P,(w|hi) is the summary of probabilities of the neuron for all the words in the short-list, Pb(·) is the probability calculated by the BNLM, and Ps(hi) = E Pb(v|hi). (2) vEVo We may regard that CSLM redistributes the probability mass of all words in the short-list, which is calculated by using the n-gram LM. 2.2 Existing Converting Methods As baseline systems, our approach proposed in (Wang et al., 2013a) only re-writes the probabilities from CSLM into the BNLM, so it can only conduct a convert LM with the same size as the original one. The main difference between our proposed method in this paper and our previous approach is that n-grams outside the corpus are generated firstly and the probabilities using CSLM are calculated by using the same method as our previous approach. That is, the proposed new method is the same as our previous one when no grown n-grams are generated. The method developed by Arsoy and colleagues (Arsoy et al., 2013; Arsoy et al., 2014) adds all the words in the short</context>
<context position="14457" citStr="Wang et al., 2013" startWordPosition="2404" endWordPosition="2407">s, respectively. Using SRILM (Stolcke, 2002; Stolcke et al., 2011), we trained a 5-gram LM with the interpolated Kneser-Ney smoothing method using the 1M English training sentences containing 42M words without cutoff. The 2,3,4,5-CSLMs were trained on the same 1M training sentences using CSLM toolkit (Schwenk, 2007; Schwenk, 2010). The settings for CSLMs were: input layer of the same dimension as vocabulary size (456K), projection layer of dimension 256 for each word, hidden layer of dimension 384 and output layer (short-list) of dimension 8192, which were recommended in the CSLM toolkit and (Wang et al., 2013a)4. 4Arsoy used around 55 M words as the corpus, including 4.2 Results The experiment results were divided into four groups: the original BNLMs (BN), the CSLM Re-ranking (RE), our previous converting (WA), the Arsoy’s growing, and our growing methods. For our bilingual LM growing method, 5 bilingual grown LMs (BI-1 to 5) were conducted in increasing sizes. For the method of Arsoy, 5 grown LMs (AR-1 to 5) with similar size of BI-1 to 5 were also conducted, respectively. For the CSLM re-ranking, we used CSLM to re-rank the 100-best lists of SMT. Our previous converted LM, Arsoy’s grown LMs and </context>
</contexts>
<marker>Wang, Utiyama, Goto, Sumita, Zhao, Lu, 2013</marker>
<rawString>Rui Wang, Masao Utiyama, Isao Goto, Eiichro Sumita, Hai Zhao, and Bao-Liang Lu. 2013a. Converting continuous-space language models into n-gram language models for statistical machine translation. In Proceedings of EMNLP-2013, pages 845–850, Seattle, Washington, USA, October. Association for Computational Linguistics.</rawString>
</citation>
<citation valid="true">
<authors>
<author>Xiaolin Wang</author>
<author>Hai Zhao</author>
<author>Bao-Liang Lu</author>
</authors>
<title>Labeled alignment for recognizing textual entailment.</title>
<date>2013</date>
<journal>Asian Federation of Natural Language Processing.</journal>
<booktitle>In Proceedings of IJCNLP-2013,</booktitle>
<pages>605--613</pages>
<location>Nagoya, Japan,</location>
<contexts>
<context position="3724" citStr="Wang et al., 2013" startWordPosition="579" endWordPosition="582">Vaswani and colleagues (2013) propose a method for reducing the training cost of CSLM and apply it to SMT decoder. However, they do not show their improvement for decoding speed, and their method is still slower than the n-gram LM. There are several other methods for attempting to implement neural network based LM or translation model for SMT (Devlin et al., 2014; Liu et al., 2014; Auli et al., 2013). However, the decoding speed using n-gram LM is still state-ofthe-art one. Some approaches calculate the probabilities of the n-grams n-grams before decoding, and store them in the n-gram format (Wang et al., 2013a; Arsoy et al., 2013; Arsoy et al., 2014). The ‘converted CSLM’ can be directly used in SMT. Though more n-grams which are not in the train189 Proceedings of the 2014 Conference on Empirical Methods in Natural Language Processing (EMNLP), pages 189–195, October 25-29, 2014, Doha, Qatar. c�2014 Association for Computational Linguistics ing corpus can be generated by using some of these ‘converting’ methods, these methods only consider the monolingual information, and do not take the bilingual information into account. We observe that the translation output of a phrase-based SMT system is conca</context>
<context position="5104" citStr="Wang et al., 2013" startWordPosition="806" endWordPosition="809">ing method is proposed using the ‘connecting phrases’. The remainder of this paper is organized as follows: In Section 2, we will review the existing CSLM converting methods. The new neural network based bilingual LM growing method will be proposed in Section 3. In Section 4, the experiments will be conducted and the results will be analyzed. We will conclude our work in Section 5. 2 Existing CSLM Converting Methods Traditional Backoff N-gram LMs (BNLMs) have been widely used in many NLP tasks (Zhang and Zhao, 2013; Jia and Zhao, 2014; Zhao et al., 2013; Zhang et al., 2012; Xu and Zhao, 2012; Wang et al., 2013b; Jia and Zhao, 2013; Wang et al., 2014). Recently, CSLMs become popular because they can obtain more accurate probability estimation. 2.1 Continues Space Language Model A CSLM implemented in a multi-layer neural network contains four layers: the input layer projects (first layer) all words in the context hi onto the projection layer (second layer); the hidden layer (third layer) and the output layer (fourth layer) achieve the non-liner probability estimation and calculate the LM probability P(wi|hi) for the given context (Schwenk, 2007). CSLM is able to calculate the probabilities of all wor</context>
<context position="6977" citStr="Wang et al., 2013" startWordPosition="1123" endWordPosition="1126">alculates the probability P(wi|hi) of wi given hi, as follows: I ∑wEVwi |&apos;i) E)P (h) if wi E V0 (1) 0 Pc wh s i Pb(wi|hi) otherwise where Vo is the short-list, P,(·) is the probability calculated by CSLM, ∑wEU0 P,(w|hi) is the summary of probabilities of the neuron for all the words in the short-list, Pb(·) is the probability calculated by the BNLM, and Ps(hi) = E Pb(v|hi). (2) vEVo We may regard that CSLM redistributes the probability mass of all words in the short-list, which is calculated by using the n-gram LM. 2.2 Existing Converting Methods As baseline systems, our approach proposed in (Wang et al., 2013a) only re-writes the probabilities from CSLM into the BNLM, so it can only conduct a convert LM with the same size as the original one. The main difference between our proposed method in this paper and our previous approach is that n-grams outside the corpus are generated firstly and the probabilities using CSLM are calculated by using the same method as our previous approach. That is, the proposed new method is the same as our previous one when no grown n-grams are generated. The method developed by Arsoy and colleagues (Arsoy et al., 2013; Arsoy et al., 2014) adds all the words in the short</context>
<context position="14457" citStr="Wang et al., 2013" startWordPosition="2404" endWordPosition="2407">s, respectively. Using SRILM (Stolcke, 2002; Stolcke et al., 2011), we trained a 5-gram LM with the interpolated Kneser-Ney smoothing method using the 1M English training sentences containing 42M words without cutoff. The 2,3,4,5-CSLMs were trained on the same 1M training sentences using CSLM toolkit (Schwenk, 2007; Schwenk, 2010). The settings for CSLMs were: input layer of the same dimension as vocabulary size (456K), projection layer of dimension 256 for each word, hidden layer of dimension 384 and output layer (short-list) of dimension 8192, which were recommended in the CSLM toolkit and (Wang et al., 2013a)4. 4Arsoy used around 55 M words as the corpus, including 4.2 Results The experiment results were divided into four groups: the original BNLMs (BN), the CSLM Re-ranking (RE), our previous converting (WA), the Arsoy’s growing, and our growing methods. For our bilingual LM growing method, 5 bilingual grown LMs (BI-1 to 5) were conducted in increasing sizes. For the method of Arsoy, 5 grown LMs (AR-1 to 5) with similar size of BI-1 to 5 were also conducted, respectively. For the CSLM re-ranking, we used CSLM to re-rank the 100-best lists of SMT. Our previous converted LM, Arsoy’s grown LMs and </context>
</contexts>
<marker>Wang, Zhao, Lu, 2013</marker>
<rawString>Xiaolin Wang, Hai Zhao, and Bao-Liang Lu. 2013b. Labeled alignment for recognizing textual entailment. In Proceedings of IJCNLP-2013, pages 605– 613, Nagoya, Japan, October. Asian Federation of Natural Language Processing.</rawString>
</citation>
<citation valid="true">
<authors>
<author>Xiao-Lin Wang</author>
<author>Yang-Yang Chen</author>
<author>Hai Zhao</author>
<author>BaoLiang Lu</author>
</authors>
<title>Parallelized extreme learning machine ensemble based on minmax modular network.</title>
<date>2014</date>
<journal>Neurocomputing,</journal>
<volume>128</volume>
<issue>0</issue>
<contexts>
<context position="5145" citStr="Wang et al., 2014" startWordPosition="814" endWordPosition="817">ting phrases’. The remainder of this paper is organized as follows: In Section 2, we will review the existing CSLM converting methods. The new neural network based bilingual LM growing method will be proposed in Section 3. In Section 4, the experiments will be conducted and the results will be analyzed. We will conclude our work in Section 5. 2 Existing CSLM Converting Methods Traditional Backoff N-gram LMs (BNLMs) have been widely used in many NLP tasks (Zhang and Zhao, 2013; Jia and Zhao, 2014; Zhao et al., 2013; Zhang et al., 2012; Xu and Zhao, 2012; Wang et al., 2013b; Jia and Zhao, 2013; Wang et al., 2014). Recently, CSLMs become popular because they can obtain more accurate probability estimation. 2.1 Continues Space Language Model A CSLM implemented in a multi-layer neural network contains four layers: the input layer projects (first layer) all words in the context hi onto the projection layer (second layer); the hidden layer (third layer) and the output layer (fourth layer) achieve the non-liner probability estimation and calculate the LM probability P(wi|hi) for the given context (Schwenk, 2007). CSLM is able to calculate the probabilities of all words in the vocabulary of the corpus given </context>
</contexts>
<marker>Wang, Chen, Zhao, Lu, 2014</marker>
<rawString>Xiao-Lin Wang, Yang-Yang Chen, Hai Zhao, and BaoLiang Lu. 2014. Parallelized extreme learning machine ensemble based on minmax modular network. Neurocomputing, 128(0):31 – 41.</rawString>
</citation>
<citation valid="true">
<authors>
<author>Qiongkai Xu</author>
<author>Hai Zhao</author>
</authors>
<title>Using deep linguistic features for finding deceptive opinion spam.</title>
<date>2012</date>
<booktitle>In Proceedings of COLING-2012,</booktitle>
<pages>1341--1350</pages>
<location>Mumbai, India,</location>
<contexts>
<context position="5085" citStr="Xu and Zhao, 2012" startWordPosition="802" endWordPosition="805">d bilingual LM growing method is proposed using the ‘connecting phrases’. The remainder of this paper is organized as follows: In Section 2, we will review the existing CSLM converting methods. The new neural network based bilingual LM growing method will be proposed in Section 3. In Section 4, the experiments will be conducted and the results will be analyzed. We will conclude our work in Section 5. 2 Existing CSLM Converting Methods Traditional Backoff N-gram LMs (BNLMs) have been widely used in many NLP tasks (Zhang and Zhao, 2013; Jia and Zhao, 2014; Zhao et al., 2013; Zhang et al., 2012; Xu and Zhao, 2012; Wang et al., 2013b; Jia and Zhao, 2013; Wang et al., 2014). Recently, CSLMs become popular because they can obtain more accurate probability estimation. 2.1 Continues Space Language Model A CSLM implemented in a multi-layer neural network contains four layers: the input layer projects (first layer) all words in the context hi onto the projection layer (second layer); the hidden layer (third layer) and the output layer (fourth layer) achieve the non-liner probability estimation and calculate the LM probability P(wi|hi) for the given context (Schwenk, 2007). CSLM is able to calculate the proba</context>
</contexts>
<marker>Xu, Zhao, 2012</marker>
<rawString>Qiongkai Xu and Hai Zhao. 2012. Using deep linguistic features for finding deceptive opinion spam. In Proceedings of COLING-2012, pages 1341–1350, Mumbai, India, December. The COLING 2012 Organizing Committee.</rawString>
</citation>
<citation valid="true">
<authors>
<author>Jingyi Zhang</author>
<author>Hai Zhao</author>
</authors>
<title>Improving function word alignment with frequency and syntactic information.</title>
<date>2013</date>
<booktitle>In Proceedings of IJCAI-2013,</booktitle>
<pages>2211--2217</pages>
<publisher>AAAI Press.</publisher>
<contexts>
<context position="5007" citStr="Zhang and Zhao, 2013" startWordPosition="786" endWordPosition="789">can be calculated by CSLM. Based on this observation, a novel neural network based bilingual LM growing method is proposed using the ‘connecting phrases’. The remainder of this paper is organized as follows: In Section 2, we will review the existing CSLM converting methods. The new neural network based bilingual LM growing method will be proposed in Section 3. In Section 4, the experiments will be conducted and the results will be analyzed. We will conclude our work in Section 5. 2 Existing CSLM Converting Methods Traditional Backoff N-gram LMs (BNLMs) have been widely used in many NLP tasks (Zhang and Zhao, 2013; Jia and Zhao, 2014; Zhao et al., 2013; Zhang et al., 2012; Xu and Zhao, 2012; Wang et al., 2013b; Jia and Zhao, 2013; Wang et al., 2014). Recently, CSLMs become popular because they can obtain more accurate probability estimation. 2.1 Continues Space Language Model A CSLM implemented in a multi-layer neural network contains four layers: the input layer projects (first layer) all words in the context hi onto the projection layer (second layer); the hidden layer (third layer) and the output layer (fourth layer) achieve the non-liner probability estimation and calculate the LM probability P(wi|</context>
</contexts>
<marker>Zhang, Zhao, 2013</marker>
<rawString>Jingyi Zhang and Hai Zhao. 2013. Improving function word alignment with frequency and syntactic information. In Proceedings of IJCAI-2013, pages 2211– 2217. AAAI Press.</rawString>
</citation>
<citation valid="true">
<authors>
<author>Xiaotian Zhang</author>
<author>Hai Zhao</author>
<author>Cong Hui</author>
</authors>
<title>A machine learning approach to convert CCGbank to Penn treebank.</title>
<date>2012</date>
<booktitle>In Proceedings of COLING2012,</booktitle>
<pages>535--542</pages>
<location>Mumbai, India,</location>
<contexts>
<context position="5066" citStr="Zhang et al., 2012" startWordPosition="798" endWordPosition="801"> neural network based bilingual LM growing method is proposed using the ‘connecting phrases’. The remainder of this paper is organized as follows: In Section 2, we will review the existing CSLM converting methods. The new neural network based bilingual LM growing method will be proposed in Section 3. In Section 4, the experiments will be conducted and the results will be analyzed. We will conclude our work in Section 5. 2 Existing CSLM Converting Methods Traditional Backoff N-gram LMs (BNLMs) have been widely used in many NLP tasks (Zhang and Zhao, 2013; Jia and Zhao, 2014; Zhao et al., 2013; Zhang et al., 2012; Xu and Zhao, 2012; Wang et al., 2013b; Jia and Zhao, 2013; Wang et al., 2014). Recently, CSLMs become popular because they can obtain more accurate probability estimation. 2.1 Continues Space Language Model A CSLM implemented in a multi-layer neural network contains four layers: the input layer projects (first layer) all words in the context hi onto the projection layer (second layer); the hidden layer (third layer) and the output layer (fourth layer) achieve the non-liner probability estimation and calculate the LM probability P(wi|hi) for the given context (Schwenk, 2007). CSLM is able to </context>
</contexts>
<marker>Zhang, Zhao, Hui, 2012</marker>
<rawString>Xiaotian Zhang, Hai Zhao, and Cong Hui. 2012. A machine learning approach to convert CCGbank to Penn treebank. In Proceedings of COLING2012, pages 535–542, Mumbai, India, December. The COLING 2012 Organizing Committee.</rawString>
</citation>
<citation valid="true">
<authors>
<author>Hai Zhao</author>
</authors>
<title>Masao Utiyama, Eiichiro Sumita, and BaoLiang Lu.</title>
<date>2013</date>
<booktitle>Computational Linguistics and Intelligent Text Processing,</booktitle>
<volume>7817</volume>
<pages>248--263</pages>
<editor>In Alexander Gelbukh, editor,</editor>
<contexts>
<context position="5007" citStr="Zhao, 2013" startWordPosition="788" endWordPosition="789">culated by CSLM. Based on this observation, a novel neural network based bilingual LM growing method is proposed using the ‘connecting phrases’. The remainder of this paper is organized as follows: In Section 2, we will review the existing CSLM converting methods. The new neural network based bilingual LM growing method will be proposed in Section 3. In Section 4, the experiments will be conducted and the results will be analyzed. We will conclude our work in Section 5. 2 Existing CSLM Converting Methods Traditional Backoff N-gram LMs (BNLMs) have been widely used in many NLP tasks (Zhang and Zhao, 2013; Jia and Zhao, 2014; Zhao et al., 2013; Zhang et al., 2012; Xu and Zhao, 2012; Wang et al., 2013b; Jia and Zhao, 2013; Wang et al., 2014). Recently, CSLMs become popular because they can obtain more accurate probability estimation. 2.1 Continues Space Language Model A CSLM implemented in a multi-layer neural network contains four layers: the input layer projects (first layer) all words in the context hi onto the projection layer (second layer); the hidden layer (third layer) and the output layer (fourth layer) achieve the non-liner probability estimation and calculate the LM probability P(wi|</context>
</contexts>
<marker>Zhao, 2013</marker>
<rawString>Hai Zhao, Masao Utiyama, Eiichiro Sumita, and BaoLiang Lu. 2013. An empirical study on word segmentation for chinese machine translation. In Alexander Gelbukh, editor, Computational Linguistics and Intelligent Text Processing, volume 7817 of Lecture Notes in Computer Science, pages 248–263.</rawString>
</citation>
</citationList>
</algorithm>
</algorithms>
<?xml version="1.0" encoding="UTF-8"?>
<algorithms version="110505">
<algorithm name="SectLabel" version="110505">
<variant no="0" confidence="0.002787">
<title confidence="0.975833">
A Human Judgment Corpus and a Metric for Arabic MT Evaluation
</title>
<author confidence="0.879802">
Houda Bouamor, Hanan Alshikhabobakr, Behrang Mohit and Kemal Oflazer
</author>
<affiliation confidence="0.736991">
Carnegie Mellon University in Qatar
</affiliation>
<email confidence="0.995789">
{hbouamor,halshikh,behrang,ko}@cmu.edu
</email>
<sectionHeader confidence="0.997349" genericHeader="abstract">
Abstract
</sectionHeader>
<bodyText confidence="0.9999512">
We present a human judgments dataset
and an adapted metric for evaluation of
Arabic machine translation. Our medium-
scale dataset is the first of its kind for Ara-
bic with high annotation quality. We use
the dataset to adapt the BLEU score for
Arabic. Our score (AL-BLEU) provides
partial credits for stem and morphologi-
cal matchings of hypothesis and reference
words. We evaluate BLEU, METEOR and
AL-BLEU on our human judgments cor-
pus and show that AL-BLEU has the high-
est correlation with human judgments. We
are releasing the dataset and software to
the research community.
</bodyText>
<sectionHeader confidence="0.999515" genericHeader="introduction">
1 Introduction
</sectionHeader>
<bodyText confidence="0.999836631578947">
Evaluation of Machine Translation (MT) contin-
ues to be a challenging research problem. There
is an ongoing effort in finding simple and scal-
able metrics with rich linguistic analysis. A wide
range of metrics have been proposed and evaluated
mostly for European target languages (Callison-
Burch et al., 2011; Mach´aˇcek and Bojar, 2013).
These metrics are usually evaluated based on their
correlation with human judgments on a set of MT
output. While there has been growing interest in
building systems for translating into Arabic, the
evaluation of Arabic MT is still an under-studied
problem. Standard MT metrics such as BLEU (Pa-
pineni et al., 2002) or TER (Snover et al., 2006)
have been widely used for evaluating Arabic MT
(El Kholy and Habash, 2012). These metrics use
strict word and phrase matching between the MT
output and reference translations. For morpholog-
ically rich target languages such as Arabic, such
criteria are too simplistic and inadequate. In this
paper, we present: (a) the first human judgment
dataset for Arabic MT (b) the Arabic Language
BLEU (AL-BLEU), an extension of the BLEU
score for Arabic MT evaluation.
Our annotated dataset is composed of the output
of six MT systems with texts from a diverse set of
topics. A group of ten native Arabic speakers an-
notated this corpus with high-levels of inter- and
intra-annotator agreements. Our AL-BLEU met-
ric uses a rich set of morphological, syntactic and
lexical features to extend the evaluation beyond
the exact matching. We conduct different exper-
iments on the newly built dataset and demonstrate
that AL-BLEU shows a stronger average correla-
tion with human judgments than the BLEU and
METEOR scores. Our dataset and our AL-BLEU
metric provide useful testbeds for further research
on Arabic MT and its evaluation.1
</bodyText>
<sectionHeader confidence="0.999926" genericHeader="related work">
2 Related Work
</sectionHeader>
<bodyText confidence="0.9995563">
Several studies on MT evaluation have pointed out
the inadequacy of the standard n-gram based eval-
uation metrics for various languages (Callison-
Burch et al., 2006). For morphologically complex
languages and those without word delimiters, sev-
eral studies have attempted to improve upon them
and suggest more reliable metrics that correlate
better with human judgments (Denoual and Lep-
age, 2005; Homola et al., 2009).
A common approach to the problem of mor-
phologically complex words is to integrate some
linguistic knowledge in the metric. ME-
TEOR (Denkowski and Lavie, 2011), TER-
Plus (Snover et al., 2010) incorporate limited lin-
guistic resources. Popovi´c and Ney (2009) showed
that n-gram based evaluation metrics calculated on
POS sequences correlate well with human judg-
ments, and recently designed and evaluated MPF,
a BLEU-style metric based on morphemes and
POS tags (Popovi´c, 2011). In the same direc-
</bodyText>
<footnote confidence="0.996271333333333">
1The dataset and the software are available at:
http://nlp.qatar.cmu.edu/resources/
AL-BLEU
</footnote>
<page confidence="0.898715">
207
</page>
<bodyText confidence="0.970552421052632">
Proceedings of the 2014 Conference on Empirical Methods in Natural Language Processing (EMNLP), pages 207–213,
October 25-29, 2014, Doha, Qatar. c�2014 Association for Computational Linguistics
tion, Chen and Kuhn (2011) proposed AMBER,
a modified version of BLEU incorporating re-
call, extra penalties, and light linguistic knowl-
edge about English morphology. Liu et al. (2010)
propose TESLA-M, a variant of a metric based
on n-gram matching that utilizes light-weight lin-
guistic analysis including lemmatization, POS tag-
ging, and WordNet synonym relations. This met-
ric was then extended to TESLA-B to model
phrase synonyms by exploiting bilingual phrase
tables (Dahlmeier et al., 2011). Tantug et al.
(2008) presented BLEU+, a tool that implements
various extension to BLEU computation to allow
for a better evaluation of the translation perfor-
mance for Turkish.
To the best of our knowledge the only human
judgment dataset for Arabic MT is the small cor-
pus which was used to tune parameters of the ME-
TEOR metric for Arabic (Denkowski and Lavie,
2011). Due to the shortage of Arabic human judg-
ment dataset, studies on the performance of eval-
uation metrics have been constrained and limited.
A relevant effort in this area is the upper-bound es-
timation of BLEU and METEOR scores for Ara-
bic MT output (El Kholy and Habash, 2011). As
part of its extensive functionality, the AMEANA
system provides the upper-bound estimate by an
exhaustive matching of morphological and lexical
features between the hypothesis and the reference
translations. Our use of morphological and lex-
ical features overlaps with the AMEANA frame-
work. However, we extend our partial matching
to a supervised tuning framework for estimating
the value of partial credits. Moreover, our human
judgment dataset allows us to validate our frame-
work with a large-scale gold-standard data.
</bodyText>
<sectionHeader confidence="0.97522" genericHeader="method">
3 Human judgment dataset
</sectionHeader>
<bodyText confidence="0.999677">
We describe here our procedure for compiling a
diverse Arabic MT dataset and annotating it with
human judgments.
</bodyText>
<subsectionHeader confidence="0.998639">
3.1 Data and systems
</subsectionHeader>
<bodyText confidence="0.973545454545454">
We annotate a corpus composed of three datasets:
(1) the standard English-Arabic NIST 2005 cor-
pus, commonly used for MT evaluations and com-
posed of news stories. We use the first English
translation as the source and the single corre-
sponding Arabic sentence as the reference. (2) the
MEDAR corpus (Maegaard et al., 2010) that con-
sists of texts related to the climate change with
four Arabic reference translations. We only use
the first reference in this study. (3) a small dataset
of Wikipedia articles (WIKI) to extend our cor-
pus and metric evaluation to topics beyond the
commonly-used news topics. This sub-corpus
consists of our in-house Arabic translations of
seven English Wikipedia articles. The articles are:
Earl Francis Lloyd, Western Europe, Citizenship,
Marcus Garvey, Middle Age translation, Acadian,
NBA. The English articles which do not exist in
the Arabic Wikipedia were manually translated by
a bilingual linguist.
Table 1 gives an overview of these sub-corpora
characteristics.
</bodyText>
<table confidence="0.990524333333333">
NIST MEDAR WIKI
# of Documents 100 4 7
# of Sentences 1056 509 327
</table>
<tableCaption confidence="0.999934">
Table 1: Statistics on the datasets.
</tableCaption>
<bodyText confidence="0.999889333333333">
We use six state-of-the-art English-to-Arabic
MT systems. These include four research-oriented
phrase-based systems with various morphological
and syntactic features and different Arabic tok-
enization schemes and also two commercial off-
the-shelf systems.
</bodyText>
<subsectionHeader confidence="0.999982">
3.2 Annotation of human judgments
</subsectionHeader>
<bodyText confidence="0.999989695652174">
In order conduct a manual evaluation of the six
MT systems, we formulated it as a ranking prob-
lem. We adapt the framework used in the WMT
2011 shared task for evaluating MT metrics on
European language pairs (Callison-Burch et al.,
2011) for Arabic MT. We gather human ranking
judgments by asking ten annotators (each native
speaker of Arabic with English as a second lan-
guage) to assess the quality of the English-Arabic
systems, by ranking sentences relative to each
other, from the best to the worst (ties are allowed).
We use the Appraise toolkit (Federmann, 2012)
designed for manual MT evaluation. The tool dis-
plays to the annotator, the source sentence and
translations produced by various MT systems. The
annotators received initial training on the tool and
the task with ten sentences. They were presented
with a brief guideline indicating the purpose of the
task and the main criteria of MT output evaluation.
Each annotator was assigned to 22 ranking
tasks. Each task included ten screens. Each screen
involveed ranking translations of ten sentences. In
total, we collected 22, 000 rankings for 1892 sen-
</bodyText>
<page confidence="0.990125">
208
</page>
<bodyText confidence="0.999707076923077">
tences (22 tasksx10 screensx10 judges). In each
annotation screen, the annotator was shown the
source-language (English) sentences, as well as
five translations to be ranked. We did not provide
annotators with the reference to avoid any bias in
the annotation process. Each source sentence was
presented with its direct context. Rather than at-
tempting to get a complete ordering over the sys-
tems, we instead relied on random selection and a
reasonably large sample size to make the compar-
isons fair (Callison-Burch et al., 2011).
An example of a source sentence and its five
translations to be ranked is given in Table 2.
</bodyText>
<subsectionHeader confidence="0.999927">
3.3 Annotation quality and analysis
</subsectionHeader>
<bodyText confidence="0.999352666666667">
In order to ensure the validity of any evaluation
setup, a reasonable of inter- and intra-annotator
agreement rates in ranking should exist. To mea-
sure these agreements, we deliberately reassigned
10% of the tasks to second annotators. More-
over, we ensured that 10% of the screens are re-
displayed to the same annotator within the same
task. This procedure allowed us to collect reliable
quality control measure for our dataset.
</bodyText>
<table confidence="0.68546475">
ninter nintra
EN-AR 0.57 0.62
Average EN-EU 0.41 0.57
EN-CZ 0.40 0.54
</table>
<tableCaption confidence="0.772364">
Table 3: Inter- and intra-annotator agreement
</tableCaption>
<bodyText confidence="0.949779857142857">
scores for our annotation compared to the aver-
age scores for five English to five European lan-
guages and also English-Czech (Callison-Burch et
al., 2011).
We measured head-to-head pairwise agreement
among annotators using Cohen’s kappa (n) (Co-
hen, 1968), defined as follows:
</bodyText>
<equation confidence="0.997685">
P(A) − P(E)
1 − P(E)
</equation>
<bodyText confidence="0.999962">
where P(A) is the proportion of times annotators
agree and P(E) is the proportion of agreement by
chance.
Table 3 gives average values obtained for inter-
annotator and intra-annotator agreement and com-
pare our results to similar annotation efforts in
WMT-13 on different European languages. Here
we compare against the average agreement for En-
glish to five languages and also from English to
one morphologically rich language (Czech).4
Based on Landis and Koch (1977) n interpre-
tation, the ninter value (57%) and also compar-
ing our agreement scores with WMT-13 annota-
tions, we believe that we have reached a reliable
and consistent annotation quality.
</bodyText>
<sectionHeader confidence="0.996334" genericHeader="method">
4 AL-BLEU
</sectionHeader>
<bodyText confidence="0.9993997">
Despite its well-known shortcomings (Callison-
Burch et al., 2006), BLEU continues to be the
de-facto MT evaluation metric. BLEU uses an
exact n-gram matching criterion that is too strict
for a morphologically rich language like Arabic.
The system outputs in Table 2 are examples of
how BLEU heavily penalizes Arabic. Based on
BLEU, the best hypothesis is from Sys5 which has
three unigram and one bigram exact matches with
the reference. However, the sentence is the 4th
ranked by annotators. In contrast, the output of
Sys3 (ranked 1st by annotators) has only one ex-
act match, but several partial matches when mor-
phological and lexical information are taken into
consideration.
We propose the Arabic Language BLEU (AL-
BLEU) metric which extends BLEU to deal with
Arabic rich morphology. We extend the matching
to morphological, syntactic and lexical levels with
an optimized partial credit. AL-BLEU starts with
the exact matching of hypothesis tokens against
the reference tokens. Furthermore, it considers the
following: (a) morphological and syntactic feature
matching, (b) stem matching. Based on Arabic lin-
guistic intuition, we check the matching of a sub-
set of 5 morphological features: (i) POS tag, (ii)
gender (iii) number (iv) person (v) definiteness.
We use the MADA package (Habash et al., 2009)
to collect the stem and the morphological features
of the hypothesis and reference translation.
Figure 1 summarizes the function in which we
consider partial matching (m(th, tr)) of a hypoth-
esis token (th) and its associated reference token
(tr). Starting with the BLEU criterion, we first
check if the hypothesis token is same as the ref-
erence one and provide the full credit for it. If
the exact matching fails, we provide partial credit
for matching at the stem and morphological level.
The value of the partial credits are the sum of
the stem weight (ws) and the morphological fea-
</bodyText>
<footnote confidence="0.995280333333333">
4We compare against the agreement score for annotations
performed by WMT researchers which are higher than the
WMT annotations on Mechanical Turk.
</footnote>
<equation confidence="0.905013">
n =
</equation>
<page confidence="0.984341">
209
</page>
<table confidence="0.999775291666667">
Source France plans to attend ASEAN emergency summit.
Reference .�é�KPA¢Ë@ �àAJ�ƒB@ �éÔ�¯ Pñ ’k Ð �Q��ª�K A‚��Q�¯
frnsaA tEtzm HDwr qmp AaAlaAsyaAn AaAlTaAr}ip
Hypothesis Systems RankAnnot BLEU RankBLEU AL-BLEU RankAL−BLEU �Ì
aPA¢Ë@ àB@ éÔ�¯ P A iQ�¯ ¡¢ s
wtxTaTfrnsaA lHDwr qmp AaAl—syaAn AaAlTaAr}ip
� �
B@ �éÔ�¯ Pñ ’mÌ A‚��Q�¯ ¡¢ m~~
àAJ�ƒð
wtxTaTfrnsaA lHDwr qmp AaAlOasyaAn
�
àAJ�ƒCË �é�KPA¢Ë@ �éÒ�®Ë@ Pñ ’mÌ ¡¢ m~~
�A‚��Q�¯
frnsaA txTaT lHDwr AaAlqmp AaAlTaAr}ip lalOasyaAn
@ �éÔ�¯ Pñ ’mÌ A‚��Q�¯ ¡¢ k
�øP@ñ¢Ë@ �àAJ�ƒ�
xTaT frnsaA lHDwr qmp —syaAn AaAlTwaAri}
�øP@ñ¢Ë@ ¡¢ k �àAJ�ƒB@ �éÔ�¯ Pñ ’mÌ A‚��Q�¯
frnsaA lHDwr qmp AaAlaAsyaAn xTaT AaAlTwaAri}
Sys1 2 0.0047 2 0.4816 1
Sys2 3 0.0037 3 0.0840 3
Sys3 1 0.0043 4 0.0940 2
Sys4 5 0.0043 4 0.0604 5
Sys5 4 0.0178 1 0.0826 4
</table>
<tableCaption confidence="0.990104">
Table 2: Example of ranked MT outputs in our gold-standard dataset. The first two rows specify the
</tableCaption>
<bodyText confidence="0.95220075">
English input and the Arabic reference, respectively. The third row of the table lists the different MT
system as ranked by annotators, using BLEU scores (column 4) and AL-BLEU (column 6). The differ-
ent translation candidates are given here along with their associated Bucklwalter transliteration.3 This
example, shows clearly that AL-BLEU correlates better with human decision.
</bodyText>
<equation confidence="0.8522665">
1, if th = tr
5
ws + wfi otherwise
i=1
</equation>
<figureCaption confidence="0.999655">
Figure 1: Formulation of our partial matching.
</figureCaption>
<bodyText confidence="0.948005205882353">
ture weights (wfi). Each weight is included in
the partial score, if such matching exist (e.g., stem
match). In order to avoid over-crediting, we limit
the range of weights with a set of constraints.
Moreover, we use a development set to optimize
the weights towards improvement of correlation
with human judgments, using a hill-climbing al-
gorithm (Russell and Norvig, 2009). Figure 2 il-
lustrates these various samples of partial matching
highlighted in different colors.
SRC: Frane Plans To Attend ASEA
Figure 2: An MT example with exact matchings
(blue), stem and morphological matching (green),
stem only matching (red) and morphological-only
matching (pink).
Following the BLEU-style exact matching and
scoring of different n-grams, AL-BLEU updates
the n-gram scores with the partial credits from
non-exact matches. We use a minimum partial
credit for n-grams which have tokens with dif-
ferent matching score. The contribution of a
partially-matched n-gram is not 1 (as counted in
BLEU), but the minimum value that individual to-
kens within the bigram are credited. For exam-
ple, if a bigram is composed of a token with exact
matching and a token with stem matching, this bi-
gram receives a credit equal to a unigram with the
stem matching (a value less than 1). While par-
tial credits are added for various n-grams, the fi-
nal computation of the AL-BLEU is similar to the
original BLEU based on the geometric mean of the
different matched n-grams. We follow BLEU in
using a very small smoothing value to avoid zero
n-gram counts and zero score.
</bodyText>
<sectionHeader confidence="0.995371" genericHeader="evaluation">
5 Experiments and results
</sectionHeader>
<bodyText confidence="0.991597875">
An automatic evaluation metric is said to be suc-
cessful if it is shown to have high agreement with
human-performed evaluations (Soricut and Brill,
2004). We use Kendall’s tau τ (Kendall, 1938),
a coefficient to measure the correlation between
the system rankings and the human judgments at
the sentence level. Kendall’s tau τ is calculated as
follows:
τ = # of concordant pairs - # of discordant pairs
total pairs
where a concordant pair indicates two translations
of the same sentence for which the ranks obtained
from the manual ranking task and from the corre-
sponding metric scores agree (they disagree in a
discordant pair). The possible values of τ range
from -1 (all pairs are discordant) to 1 (all pairs
</bodyText>
<equation confidence="0.7281884">
m(th, tr) = {
نايسلأل ةئراطلا ةمقلا روضحل ططخت اسنرف
ةئراطلا نايسلاا ةمق روضح مزتعت اسنرف
HYP:
REF:
</equation>
<page confidence="0.673762">
210
</page>
<table confidence="0.999779666666667">
Dev Test
BLEU 0.3361 0.3162
METEOR 0.3331 0.3426
AL-BLEUMorph 0.3746 0.3535
AL-BLEULex 0.3732 0.3564
AL-BLEU 0.3759 0.3521
</table>
<tableCaption confidence="0.9516585">
Table 4: Comparison of the average Kendall’s r
correlation.
</tableCaption>
<bodyText confidence="0.999729461538462">
are concordant). Thus, an automatic evaluation
metric with a higher r value is making predic-
tions that are more similar to the human judgments
than an automatic evaluation metric with a lower
r. We calculate the r score for each sentence and
average the scores to reach the corpus-level cor-
relation. We conducted a set of experiments to
compare the correlation of AL-BLEU against the
state-of-the art MT evaluation metrics. For this we
use a subset of 900 sentences extracted from the
dataset described in Section 3.1. As mentioned
above, the stem and morphological features in AL-
BLEU are parameterized each by weights which
are used to calculate the partial credits. We op-
timize the value of each weight towards correla-
tion with human judgment by hill climbing with
100 random restarts using a development set of
600 sentences. The 300 remaining sentences (100
from each corpus) are kept for testing. The de-
velopment and test sets are composed of equal
portions of sentences from the three sub-corpora
(NIST, MEDAR, WIKI).
As baselines, we measured the correlation of
BLEU and METEOR with human judgments col-
lected for each sentence. We did not observe
a strong correlation with the Arabic-tuned ME-
TEOR. We conducted our experiments on the stan-
dard METEOR which was a stronger baseline than
its Arabic version. In order to avoid the zero n-
gram counts and artificially low BLEU scores, we
use a smoothed version of BLEU. We follow Liu
and Gildea (2005) to add a small value to both the
matched n-grams and the total number of n-grams
(epsilon value of 10−3). In order to reach an op-
timal ordering of partial matches, we conducted a
set of experiments in which we compared differ-
ent orders between the morphological and lexical
matchings to settle with the final order which was
presented in Figure 1.
Table 4 shows a comparison of the average cor-
relation with human judgments for BLEU, ME-
TEOR and AL-BLEU. AL-BLEU shows a strong
improvement against BLEU and a competitive im-
provement against METEOR both on the test and
development sets. The example in Table 2 shows
a sample case of such improvement. In the ex-
ample, the sentence ranked the highest by the an-
notator has only two exact matching with the ref-
erence translation (which results in a low BLEU
score). The stem and morphological matching of
AL-BLEU, gives a score and ranking much closer
to human judgments.
</bodyText>
<sectionHeader confidence="0.995475" genericHeader="conclusions">
6 Conclusion
</sectionHeader>
<bodyText confidence="0.999818285714286">
We presented AL-BLEU, our adaptation of BLEU
for the evaluation of machine translation into Ara-
bic. The metric uses morphological, syntactic and
lexical matching to go beyond exact token match-
ing. We also presented our annotated corpus of
human ranking judgments for evaluation of Ara-
bic MT. The size and diversity of the topics in
the corpus, along with its relatively high annota-
tion quality (measured by IAA scores) makes it
a useful resource for future research on Arabic
MT. Moreover, the strong performance of our AL-
BLEU metric is a positive indicator for future ex-
ploration of richer linguistic information in evalu-
ation of Arabic MT.
</bodyText>
<sectionHeader confidence="0.998982" genericHeader="acknowledgments">
7 Acknowledgements
</sectionHeader>
<bodyText confidence="0.999777">
We thank Michael Denkowski, Ahmed El Kholy,
Francisco Guzman, Nizar Habash, Alon Lavie,
Austin Matthews, Preslav Nakov for their com-
ments and help in creation of our dataset. We
also thank our team of annotators from CMU-
Qatar. This publication was made possible by
grants YSREP-1-018-1-004 and NPRP-09-1140-
1-177 from the Qatar National Research Fund (a
member of the Qatar Foundation). The statements
made herein are solely the responsibility of the au-
thors.
</bodyText>
<sectionHeader confidence="0.999324" genericHeader="references">
References
</sectionHeader>
<reference confidence="0.999018">
Chris Callison-Burch, Miles Osborne, and Philipp
Koehn. 2006. Re-evaluating the Role of BLEU in
Machine Translation Research. In Proceedings of
the 11th Conference of the European Chapter of the
Association for Computational Linguistics, Trento,
Italy.
Chris Callison-Burch, Philipp Koehn, Christof Monz,
and Omar Zaidan. 2011. Findings of the 2011
Workshop on Statistical Machine Translation. In
</reference>
<page confidence="0.994623">
211
</page>
<reference confidence="0.982404348623854">
Proceedings of the Sixth Workshop on Statistical
Machine Translation, Edinburgh, Scotland.
Boxing Chen and Roland Kuhn. 2011. AMBER: A
Modified BLEU, Enhanced Ranking Metric. In Pro-
ceedings of the Sixth Workshop on Statistical Ma-
chine Translation, pages 71–77, Edinburgh, Scot-
land.
Jacob Cohen. 1968. Weighted Kappa: Nominal Scale
Agreement Provision for Scaled Disagreement or
Partial Credit. Psychological bulletin, 70(4):213.
Daniel Dahlmeier, Chang Liu, and Hwee Tou Ng.
2011. TESLA at WMT 2011: Translation Eval-
uation and Tunable Metric. In Proceedings of the
Sixth Workshop on Statistical Machine Translation,
pages 78–84, Edinburgh, Scotland, July. Association
for Computational Linguistics.
Michael Denkowski and Alon Lavie. 2011. Meteor
1.3: Automatic Metric for Reliable Optimization
and Evaluation of Machine Translation Systems. In
Proceedings of the EMNLP 2011 Workshop on Sta-
tistical Machine Translation, Edinburgh, UK.
Etienne Denoual and Yves Lepage. 2005. BLEU in
Characters: Towards Automatic MT Evaluation in
Languages Without Word Delimiters. In Proceed-
ings of the Second International Joint Conference on
Natural Language Processing, Jeju Island, Republic
of Korea.
Ahmed El Kholy and Nizar Habash. 2011. Auto-
matic Error Analysis for Morphologically Rich Lan-
guages. In Proceedings of the MT Summit XIII,
pages 225–232, Xiamen, China.
Ahmed El Kholy and Nizar Habash. 2012. Ortho-
graphic and Morphological Processing for English-
Arabic Statistical Machine Translation. Machine
Translation, 26(1):25–45.
Christian Federmann. 2012. Appraise: an Open-
Source Toolkit for Manual Evaluation of MT Out-
put. The Prague Bulletin of Mathematical Linguis-
tics, 98(1):25–35.
N. Habash, O. Rambow, and R. Roth. 2009. Mada+
Tokan: A Toolkit for Arabic Tokenization, Diacriti-
zation, Morphological Disambiguation, POS Tag-
ging, Stemming and Lemmatization. In Proceed-
ings of the Second International Conference on Ara-
bic Language Resources and Tools (MEDAR), Cairo,
Egypt.
Petr Homola, Vladislav Kubo&amp;quot;n, and Pavel Pecina.
2009. A Simple Automatic MT Evaluation Metric.
In Proceedings of the Fourth Workshop on Statistical
Machine Translation, pages 33–36, Athens, Greece,
March. Association for Computational Linguistics.
Maurice G Kendall. 1938. A New Measure of Rank
Correlation. Biometrika.
J Richard Landis and Gary G Koch. 1977. The Mea-
surement of Observer Agreement for Categorical
Data. Biometrics, 33(1):159–174.
Ding Liu and Daniel Gildea. 2005. Syntactic Features
for Evaluation of Machine Translation. In Proceed-
ings of the ACL Workshop on Intrinsic and Extrin-
sic Evaluation Measures for Machine Translation
and/or Summarization, pages 25–32.
Chang Liu, Daniel Dahlmeier, and Hwee Tou Ng.
2010. TESLA: Translation Evaluation of Sentences
with Linear-Programming-Based Analysis. In Pro-
ceedings of the Joint Fifth Workshop on Statistical
Machine Translation and Metrics (MATR), pages
354–359.
Matou&amp;quot;s Mach´a&amp;quot;cek and Ond&amp;quot;rej Bojar. 2013. Results
of the WMT13 Metrics Shared Task. In Proceed-
ings of the Eighth Workshop on Statistical Machine
Translation, pages 45–51, Sofia, Bulgaria.
Bente Maegaard, Mohamed Attia, Khalid Choukri,
Olivier Hamon, Steven Krauwer, and Mustafa
Yaseen. 2010. Cooperation for Arabic Language
Resources and Tools–The MEDAR Project. In Pro-
ceedings of LREC, Valetta, Malta.
Kishore Papineni, Salim Roukos, Todd Ward, and Wei-
Jing Zhu. 2002. BLEU: A Method for Auto-
matic Evaluation of Machine Translation. In Pro-
ceedings of the 40th Annual Meeting on Associa-
tion for Computational Linguistics, pages 311–318,
Philadelphia, Pennsylvania.
Maja Popovi´c and Hermann Ney. 2009. Syntax-
oriented Evaluation Measures for Machine Trans-
lation Output. In Proceedings of the Fourth Work-
shop on Statistical Machine Translation, pages 29–
32, Athens, Greece.
Maja Popovi´c. 2011. Morphemes and POS Tags for
n-gram Based Evaluation Metrics. In Proceedings
of the Sixth Workshop on Statistical Machine Trans-
lation, pages 104–107, Edinburgh, Scotland.
Stuart Russell and Peter Norvig. 2009. Artificial Intel-
ligence: A Modern Approach. Prentice Hall Engle-
wood Cliffs.
Matthew Snover, Bonnie J. Dorr, Richard Schwartz,
Linnea Micciulla, and John Makhoul. 2006. A
Study of Translation Edit Rate with Targeted Hu-
man Annotation. In Proceedings of AMTA, Boston,
USA.
Matthew Snover, Nitin Madnani, Bonnie J. Dorr, and
Richard Schwartz. 2010. TER-Plus: Paraphrase,
Semantic, and Alignment Enhancements to Transla-
tion Edit Rate. Machine Translation, 23(2-3).
Radu Soricut and Eric Brill. 2004. A Unified Frame-
work For Automatic Evaluation Using 4-Gram Co-
occurrence Statistics. In Proceedings of the 42nd
Meeting of the Association for Computational Lin-
guistics (ACL’04), Main Volume, pages 613–620,
Barcelona, Spain, July.
</reference>
<page confidence="0.975894">
212
</page>
<reference confidence="0.9991156">
C¨uneyd Tantug, Kemal Oflazer, and Ilknur Durgar El-
Kahlout. 2008. BLEU+: a Tool for Fine-Grained
BLEU Computation. In Proceedings of the 6th
edition of the Language Resources and Evaluation
Conference, Marrakech, Morocco.
</reference>
<page confidence="0.999396">
213
</page>
</variant>
</algorithm>
<algorithm name="ParsHed" version="110505">
<variant no="0" confidence="0.768764">
<title confidence="0.999791">A Human Judgment Corpus and a Metric for Arabic MT Evaluation</title>
<author confidence="0.947586">Houda Bouamor</author>
<author confidence="0.947586">Hanan Alshikhabobakr</author>
<author confidence="0.947586">Behrang Mohit</author>
<author confidence="0.947586">Kemal</author>
<affiliation confidence="0.970575">Carnegie Mellon University in</affiliation>
<abstract confidence="0.98884525">We present a human judgments dataset and an adapted metric for evaluation of Arabic machine translation. Our mediumscale dataset is the first of its kind for Arabic with high annotation quality. We use the dataset to adapt the BLEU score for Arabic. Our score (AL-BLEU) provides partial credits for stem and morphological matchings of hypothesis and reference words. We evaluate BLEU, METEOR and AL-BLEU on our human judgments corpus and show that AL-BLEU has the highest correlation with human judgments. We are releasing the dataset and software to the research community.</abstract>
</variant>
</algorithm>
<algorithm name="ParsCit" version="110505">
<citationList>
<citation valid="true">
<authors>
<author>Chris Callison-Burch</author>
<author>Miles Osborne</author>
<author>Philipp Koehn</author>
</authors>
<title>Re-evaluating the Role of BLEU in Machine Translation Research.</title>
<date>2006</date>
<booktitle>In Proceedings of the 11th Conference of the European Chapter of the Association for Computational Linguistics,</booktitle>
<location>Trento, Italy.</location>
<marker>Callison-Burch, Osborne, Koehn, 2006</marker>
<rawString>Chris Callison-Burch, Miles Osborne, and Philipp Koehn. 2006. Re-evaluating the Role of BLEU in Machine Translation Research. In Proceedings of the 11th Conference of the European Chapter of the Association for Computational Linguistics, Trento, Italy.</rawString>
</citation>
<citation valid="true">
<authors>
<author>Chris Callison-Burch</author>
<author>Philipp Koehn</author>
<author>Christof Monz</author>
<author>Omar Zaidan</author>
</authors>
<date>2011</date>
<booktitle>Findings of the 2011 Workshop on Statistical Machine Translation. In Proceedings of the Sixth Workshop on Statistical Machine Translation,</booktitle>
<location>Edinburgh, Scotland.</location>
<contexts>
<context position="7255" citStr="Callison-Burch et al., 2011" startWordPosition="1144" endWordPosition="1147">IST MEDAR WIKI # of Documents 100 4 7 # of Sentences 1056 509 327 Table 1: Statistics on the datasets. We use six state-of-the-art English-to-Arabic MT systems. These include four research-oriented phrase-based systems with various morphological and syntactic features and different Arabic tokenization schemes and also two commercial offthe-shelf systems. 3.2 Annotation of human judgments In order conduct a manual evaluation of the six MT systems, we formulated it as a ranking problem. We adapt the framework used in the WMT 2011 shared task for evaluating MT metrics on European language pairs (Callison-Burch et al., 2011) for Arabic MT. We gather human ranking judgments by asking ten annotators (each native speaker of Arabic with English as a second language) to assess the quality of the English-Arabic systems, by ranking sentences relative to each other, from the best to the worst (ties are allowed). We use the Appraise toolkit (Federmann, 2012) designed for manual MT evaluation. The tool displays to the annotator, the source sentence and translations produced by various MT systems. The annotators received initial training on the tool and the task with ten sentences. They were presented with a brief guideline</context>
<context position="8664" citStr="Callison-Burch et al., 2011" startWordPosition="1373" endWordPosition="1376">veed ranking translations of ten sentences. In total, we collected 22, 000 rankings for 1892 sen208 tences (22 tasksx10 screensx10 judges). In each annotation screen, the annotator was shown the source-language (English) sentences, as well as five translations to be ranked. We did not provide annotators with the reference to avoid any bias in the annotation process. Each source sentence was presented with its direct context. Rather than attempting to get a complete ordering over the systems, we instead relied on random selection and a reasonably large sample size to make the comparisons fair (Callison-Burch et al., 2011). An example of a source sentence and its five translations to be ranked is given in Table 2. 3.3 Annotation quality and analysis In order to ensure the validity of any evaluation setup, a reasonable of inter- and intra-annotator agreement rates in ranking should exist. To measure these agreements, we deliberately reassigned 10% of the tasks to second annotators. Moreover, we ensured that 10% of the screens are redisplayed to the same annotator within the same task. This procedure allowed us to collect reliable quality control measure for our dataset. ninter nintra EN-AR 0.57 0.62 Average EN-E</context>
</contexts>
<marker>Callison-Burch, Koehn, Monz, Zaidan, 2011</marker>
<rawString>Chris Callison-Burch, Philipp Koehn, Christof Monz, and Omar Zaidan. 2011. Findings of the 2011 Workshop on Statistical Machine Translation. In Proceedings of the Sixth Workshop on Statistical Machine Translation, Edinburgh, Scotland.</rawString>
</citation>
<citation valid="true">
<authors>
<author>Boxing Chen</author>
<author>Roland Kuhn</author>
</authors>
<title>AMBER: A Modified BLEU, Enhanced Ranking Metric.</title>
<date>2011</date>
<booktitle>In Proceedings of the Sixth Workshop on Statistical Machine Translation,</booktitle>
<pages>71--77</pages>
<location>Edinburgh, Scotland.</location>
<contexts>
<context position="3837" citStr="Chen and Kuhn (2011)" startWordPosition="599" endWordPosition="602"> incorporate limited linguistic resources. Popovi´c and Ney (2009) showed that n-gram based evaluation metrics calculated on POS sequences correlate well with human judgments, and recently designed and evaluated MPF, a BLEU-style metric based on morphemes and POS tags (Popovi´c, 2011). In the same direc1The dataset and the software are available at: http://nlp.qatar.cmu.edu/resources/ AL-BLEU 207 Proceedings of the 2014 Conference on Empirical Methods in Natural Language Processing (EMNLP), pages 207–213, October 25-29, 2014, Doha, Qatar. c�2014 Association for Computational Linguistics tion, Chen and Kuhn (2011) proposed AMBER, a modified version of BLEU incorporating recall, extra penalties, and light linguistic knowledge about English morphology. Liu et al. (2010) propose TESLA-M, a variant of a metric based on n-gram matching that utilizes light-weight linguistic analysis including lemmatization, POS tagging, and WordNet synonym relations. This metric was then extended to TESLA-B to model phrase synonyms by exploiting bilingual phrase tables (Dahlmeier et al., 2011). Tantug et al. (2008) presented BLEU+, a tool that implements various extension to BLEU computation to allow for a better evaluation </context>
</contexts>
<marker>Chen, Kuhn, 2011</marker>
<rawString>Boxing Chen and Roland Kuhn. 2011. AMBER: A Modified BLEU, Enhanced Ranking Metric. In Proceedings of the Sixth Workshop on Statistical Machine Translation, pages 71–77, Edinburgh, Scotland.</rawString>
</citation>
<citation valid="true">
<authors>
<author>Jacob Cohen</author>
</authors>
<title>Weighted Kappa: Nominal Scale Agreement Provision for Scaled Disagreement or Partial Credit. Psychological bulletin,</title>
<date>1968</date>
<pages>70--4</pages>
<contexts>
<context position="9591" citStr="Cohen, 1968" startWordPosition="1524" endWordPosition="1526"> reassigned 10% of the tasks to second annotators. Moreover, we ensured that 10% of the screens are redisplayed to the same annotator within the same task. This procedure allowed us to collect reliable quality control measure for our dataset. ninter nintra EN-AR 0.57 0.62 Average EN-EU 0.41 0.57 EN-CZ 0.40 0.54 Table 3: Inter- and intra-annotator agreement scores for our annotation compared to the average scores for five English to five European languages and also English-Czech (Callison-Burch et al., 2011). We measured head-to-head pairwise agreement among annotators using Cohen’s kappa (n) (Cohen, 1968), defined as follows: P(A) − P(E) 1 − P(E) where P(A) is the proportion of times annotators agree and P(E) is the proportion of agreement by chance. Table 3 gives average values obtained for interannotator and intra-annotator agreement and compare our results to similar annotation efforts in WMT-13 on different European languages. Here we compare against the average agreement for English to five languages and also from English to one morphologically rich language (Czech).4 Based on Landis and Koch (1977) n interpretation, the ninter value (57%) and also comparing our agreement scores with WMT-</context>
</contexts>
<marker>Cohen, 1968</marker>
<rawString>Jacob Cohen. 1968. Weighted Kappa: Nominal Scale Agreement Provision for Scaled Disagreement or Partial Credit. Psychological bulletin, 70(4):213.</rawString>
</citation>
<citation valid="true">
<authors>
<author>Daniel Dahlmeier</author>
<author>Chang Liu</author>
<author>Hwee Tou Ng</author>
</authors>
<title>TESLA at WMT 2011: Translation Evaluation and Tunable Metric.</title>
<date>2011</date>
<booktitle>In Proceedings of the Sixth Workshop on Statistical Machine Translation,</booktitle>
<pages>78--84</pages>
<publisher>Association</publisher>
<institution>for Computational Linguistics.</institution>
<location>Edinburgh, Scotland,</location>
<contexts>
<context position="4303" citStr="Dahlmeier et al., 2011" startWordPosition="670" endWordPosition="673">ral Language Processing (EMNLP), pages 207–213, October 25-29, 2014, Doha, Qatar. c�2014 Association for Computational Linguistics tion, Chen and Kuhn (2011) proposed AMBER, a modified version of BLEU incorporating recall, extra penalties, and light linguistic knowledge about English morphology. Liu et al. (2010) propose TESLA-M, a variant of a metric based on n-gram matching that utilizes light-weight linguistic analysis including lemmatization, POS tagging, and WordNet synonym relations. This metric was then extended to TESLA-B to model phrase synonyms by exploiting bilingual phrase tables (Dahlmeier et al., 2011). Tantug et al. (2008) presented BLEU+, a tool that implements various extension to BLEU computation to allow for a better evaluation of the translation performance for Turkish. To the best of our knowledge the only human judgment dataset for Arabic MT is the small corpus which was used to tune parameters of the METEOR metric for Arabic (Denkowski and Lavie, 2011). Due to the shortage of Arabic human judgment dataset, studies on the performance of evaluation metrics have been constrained and limited. A relevant effort in this area is the upper-bound estimation of BLEU and METEOR scores for Ara</context>
</contexts>
<marker>Dahlmeier, Liu, Ng, 2011</marker>
<rawString>Daniel Dahlmeier, Chang Liu, and Hwee Tou Ng. 2011. TESLA at WMT 2011: Translation Evaluation and Tunable Metric. In Proceedings of the Sixth Workshop on Statistical Machine Translation, pages 78–84, Edinburgh, Scotland, July. Association for Computational Linguistics.</rawString>
</citation>
<citation valid="true">
<authors>
<author>Michael Denkowski</author>
<author>Alon Lavie</author>
</authors>
<title>Meteor 1.3: Automatic Metric for Reliable Optimization and Evaluation of Machine Translation Systems.</title>
<date>2011</date>
<booktitle>In Proceedings of the EMNLP 2011 Workshop on Statistical Machine Translation,</booktitle>
<location>Edinburgh, UK.</location>
<contexts>
<context position="3186" citStr="Denkowski and Lavie, 2011" startWordPosition="506" endWordPosition="509">rch on Arabic MT and its evaluation.1 2 Related Work Several studies on MT evaluation have pointed out the inadequacy of the standard n-gram based evaluation metrics for various languages (CallisonBurch et al., 2006). For morphologically complex languages and those without word delimiters, several studies have attempted to improve upon them and suggest more reliable metrics that correlate better with human judgments (Denoual and Lepage, 2005; Homola et al., 2009). A common approach to the problem of morphologically complex words is to integrate some linguistic knowledge in the metric. METEOR (Denkowski and Lavie, 2011), TERPlus (Snover et al., 2010) incorporate limited linguistic resources. Popovi´c and Ney (2009) showed that n-gram based evaluation metrics calculated on POS sequences correlate well with human judgments, and recently designed and evaluated MPF, a BLEU-style metric based on morphemes and POS tags (Popovi´c, 2011). In the same direc1The dataset and the software are available at: http://nlp.qatar.cmu.edu/resources/ AL-BLEU 207 Proceedings of the 2014 Conference on Empirical Methods in Natural Language Processing (EMNLP), pages 207–213, October 25-29, 2014, Doha, Qatar. c�2014 Association for C</context>
<context position="4669" citStr="Denkowski and Lavie, 2011" startWordPosition="734" endWordPosition="737">n-gram matching that utilizes light-weight linguistic analysis including lemmatization, POS tagging, and WordNet synonym relations. This metric was then extended to TESLA-B to model phrase synonyms by exploiting bilingual phrase tables (Dahlmeier et al., 2011). Tantug et al. (2008) presented BLEU+, a tool that implements various extension to BLEU computation to allow for a better evaluation of the translation performance for Turkish. To the best of our knowledge the only human judgment dataset for Arabic MT is the small corpus which was used to tune parameters of the METEOR metric for Arabic (Denkowski and Lavie, 2011). Due to the shortage of Arabic human judgment dataset, studies on the performance of evaluation metrics have been constrained and limited. A relevant effort in this area is the upper-bound estimation of BLEU and METEOR scores for Arabic MT output (El Kholy and Habash, 2011). As part of its extensive functionality, the AMEANA system provides the upper-bound estimate by an exhaustive matching of morphological and lexical features between the hypothesis and the reference translations. Our use of morphological and lexical features overlaps with the AMEANA framework. However, we extend our partial</context>
</contexts>
<marker>Denkowski, Lavie, 2011</marker>
<rawString>Michael Denkowski and Alon Lavie. 2011. Meteor 1.3: Automatic Metric for Reliable Optimization and Evaluation of Machine Translation Systems. In Proceedings of the EMNLP 2011 Workshop on Statistical Machine Translation, Edinburgh, UK.</rawString>
</citation>
<citation valid="true">
<authors>
<author>Etienne Denoual</author>
<author>Yves Lepage</author>
</authors>
<title>BLEU in Characters: Towards Automatic MT Evaluation in Languages Without Word Delimiters.</title>
<date>2005</date>
<booktitle>In Proceedings of the Second International Joint Conference on Natural Language Processing, Jeju Island,</booktitle>
<location>Republic of</location>
<contexts>
<context position="3005" citStr="Denoual and Lepage, 2005" startWordPosition="475" endWordPosition="479">hat AL-BLEU shows a stronger average correlation with human judgments than the BLEU and METEOR scores. Our dataset and our AL-BLEU metric provide useful testbeds for further research on Arabic MT and its evaluation.1 2 Related Work Several studies on MT evaluation have pointed out the inadequacy of the standard n-gram based evaluation metrics for various languages (CallisonBurch et al., 2006). For morphologically complex languages and those without word delimiters, several studies have attempted to improve upon them and suggest more reliable metrics that correlate better with human judgments (Denoual and Lepage, 2005; Homola et al., 2009). A common approach to the problem of morphologically complex words is to integrate some linguistic knowledge in the metric. METEOR (Denkowski and Lavie, 2011), TERPlus (Snover et al., 2010) incorporate limited linguistic resources. Popovi´c and Ney (2009) showed that n-gram based evaluation metrics calculated on POS sequences correlate well with human judgments, and recently designed and evaluated MPF, a BLEU-style metric based on morphemes and POS tags (Popovi´c, 2011). In the same direc1The dataset and the software are available at: http://nlp.qatar.cmu.edu/resources/ </context>
</contexts>
<marker>Denoual, Lepage, 2005</marker>
<rawString>Etienne Denoual and Yves Lepage. 2005. BLEU in Characters: Towards Automatic MT Evaluation in Languages Without Word Delimiters. In Proceedings of the Second International Joint Conference on Natural Language Processing, Jeju Island, Republic of Korea.</rawString>
</citation>
<citation valid="true">
<authors>
<author>Ahmed El Kholy</author>
<author>Nizar Habash</author>
</authors>
<title>Automatic Error Analysis for Morphologically Rich Languages.</title>
<date>2011</date>
<booktitle>In Proceedings of the MT Summit XIII,</booktitle>
<pages>225--232</pages>
<location>Xiamen, China.</location>
<marker>El Kholy, Habash, 2011</marker>
<rawString>Ahmed El Kholy and Nizar Habash. 2011. Automatic Error Analysis for Morphologically Rich Languages. In Proceedings of the MT Summit XIII, pages 225–232, Xiamen, China.</rawString>
</citation>
<citation valid="true">
<authors>
<author>Ahmed El Kholy</author>
<author>Nizar Habash</author>
</authors>
<date>2012</date>
<booktitle>Orthographic and Morphological Processing for EnglishArabic Statistical Machine Translation. Machine Translation,</booktitle>
<volume>26</volume>
<issue>1</issue>
<marker>El Kholy, Habash, 2012</marker>
<rawString>Ahmed El Kholy and Nizar Habash. 2012. Orthographic and Morphological Processing for EnglishArabic Statistical Machine Translation. Machine Translation, 26(1):25–45.</rawString>
</citation>
<citation valid="true">
<authors>
<author>Christian Federmann</author>
</authors>
<title>Appraise: an OpenSource Toolkit for</title>
<date>2012</date>
<journal>Manual Evaluation of MT Output. The Prague Bulletin of Mathematical Linguistics,</journal>
<volume>98</volume>
<issue>1</issue>
<contexts>
<context position="7586" citStr="Federmann, 2012" startWordPosition="1201" endWordPosition="1202">helf systems. 3.2 Annotation of human judgments In order conduct a manual evaluation of the six MT systems, we formulated it as a ranking problem. We adapt the framework used in the WMT 2011 shared task for evaluating MT metrics on European language pairs (Callison-Burch et al., 2011) for Arabic MT. We gather human ranking judgments by asking ten annotators (each native speaker of Arabic with English as a second language) to assess the quality of the English-Arabic systems, by ranking sentences relative to each other, from the best to the worst (ties are allowed). We use the Appraise toolkit (Federmann, 2012) designed for manual MT evaluation. The tool displays to the annotator, the source sentence and translations produced by various MT systems. The annotators received initial training on the tool and the task with ten sentences. They were presented with a brief guideline indicating the purpose of the task and the main criteria of MT output evaluation. Each annotator was assigned to 22 ranking tasks. Each task included ten screens. Each screen involveed ranking translations of ten sentences. In total, we collected 22, 000 rankings for 1892 sen208 tences (22 tasksx10 screensx10 judges). In each an</context>
</contexts>
<marker>Federmann, 2012</marker>
<rawString>Christian Federmann. 2012. Appraise: an OpenSource Toolkit for Manual Evaluation of MT Output. The Prague Bulletin of Mathematical Linguistics, 98(1):25–35.</rawString>
</citation>
<citation valid="true">
<authors>
<author>N Habash</author>
<author>O Rambow</author>
<author>R Roth</author>
</authors>
<title>Mada+ Tokan: A Toolkit for Arabic Tokenization, Diacritization, Morphological Disambiguation, POS Tagging, Stemming and Lemmatization.</title>
<date>2009</date>
<booktitle>In Proceedings of the Second International Conference on Arabic Language Resources and Tools (MEDAR),</booktitle>
<location>Cairo, Egypt.</location>
<contexts>
<context position="11599" citStr="Habash et al., 2009" startWordPosition="1846" endWordPosition="1849">pose the Arabic Language BLEU (ALBLEU) metric which extends BLEU to deal with Arabic rich morphology. We extend the matching to morphological, syntactic and lexical levels with an optimized partial credit. AL-BLEU starts with the exact matching of hypothesis tokens against the reference tokens. Furthermore, it considers the following: (a) morphological and syntactic feature matching, (b) stem matching. Based on Arabic linguistic intuition, we check the matching of a subset of 5 morphological features: (i) POS tag, (ii) gender (iii) number (iv) person (v) definiteness. We use the MADA package (Habash et al., 2009) to collect the stem and the morphological features of the hypothesis and reference translation. Figure 1 summarizes the function in which we consider partial matching (m(th, tr)) of a hypothesis token (th) and its associated reference token (tr). Starting with the BLEU criterion, we first check if the hypothesis token is same as the reference one and provide the full credit for it. If the exact matching fails, we provide partial credit for matching at the stem and morphological level. The value of the partial credits are the sum of the stem weight (ws) and the morphological fea4We compare aga</context>
</contexts>
<marker>Habash, Rambow, Roth, 2009</marker>
<rawString>N. Habash, O. Rambow, and R. Roth. 2009. Mada+ Tokan: A Toolkit for Arabic Tokenization, Diacritization, Morphological Disambiguation, POS Tagging, Stemming and Lemmatization. In Proceedings of the Second International Conference on Arabic Language Resources and Tools (MEDAR), Cairo, Egypt.</rawString>
</citation>
<citation valid="true">
<authors>
<author>Petr Homola</author>
<author>Vladislav Kubon</author>
<author>Pavel Pecina</author>
</authors>
<title>A Simple Automatic MT Evaluation Metric.</title>
<date>2009</date>
<booktitle>In Proceedings of the Fourth Workshop on Statistical Machine Translation,</booktitle>
<pages>33--36</pages>
<publisher>Association</publisher>
<institution>for Computational Linguistics.</institution>
<location>Athens, Greece,</location>
<contexts>
<context position="3027" citStr="Homola et al., 2009" startWordPosition="480" endWordPosition="483">er average correlation with human judgments than the BLEU and METEOR scores. Our dataset and our AL-BLEU metric provide useful testbeds for further research on Arabic MT and its evaluation.1 2 Related Work Several studies on MT evaluation have pointed out the inadequacy of the standard n-gram based evaluation metrics for various languages (CallisonBurch et al., 2006). For morphologically complex languages and those without word delimiters, several studies have attempted to improve upon them and suggest more reliable metrics that correlate better with human judgments (Denoual and Lepage, 2005; Homola et al., 2009). A common approach to the problem of morphologically complex words is to integrate some linguistic knowledge in the metric. METEOR (Denkowski and Lavie, 2011), TERPlus (Snover et al., 2010) incorporate limited linguistic resources. Popovi´c and Ney (2009) showed that n-gram based evaluation metrics calculated on POS sequences correlate well with human judgments, and recently designed and evaluated MPF, a BLEU-style metric based on morphemes and POS tags (Popovi´c, 2011). In the same direc1The dataset and the software are available at: http://nlp.qatar.cmu.edu/resources/ AL-BLEU 207 Proceeding</context>
</contexts>
<marker>Homola, Kubon, Pecina, 2009</marker>
<rawString>Petr Homola, Vladislav Kubo&amp;quot;n, and Pavel Pecina. 2009. A Simple Automatic MT Evaluation Metric. In Proceedings of the Fourth Workshop on Statistical Machine Translation, pages 33–36, Athens, Greece, March. Association for Computational Linguistics.</rawString>
</citation>
<citation valid="true">
<authors>
<author>Maurice G Kendall</author>
</authors>
<date>1938</date>
<journal>A New Measure of Rank Correlation. Biometrika.</journal>
<contexts>
<context position="15446" citStr="Kendall, 1938" startWordPosition="2487" endWordPosition="2488">with stem matching, this bigram receives a credit equal to a unigram with the stem matching (a value less than 1). While partial credits are added for various n-grams, the final computation of the AL-BLEU is similar to the original BLEU based on the geometric mean of the different matched n-grams. We follow BLEU in using a very small smoothing value to avoid zero n-gram counts and zero score. 5 Experiments and results An automatic evaluation metric is said to be successful if it is shown to have high agreement with human-performed evaluations (Soricut and Brill, 2004). We use Kendall’s tau τ (Kendall, 1938), a coefficient to measure the correlation between the system rankings and the human judgments at the sentence level. Kendall’s tau τ is calculated as follows: τ = # of concordant pairs - # of discordant pairs total pairs where a concordant pair indicates two translations of the same sentence for which the ranks obtained from the manual ranking task and from the corresponding metric scores agree (they disagree in a discordant pair). The possible values of τ range from -1 (all pairs are discordant) to 1 (all pairs m(th, tr) = { نايسلأل ةئراطلا ةمقلا روضحل ططخت اسنرف ةئراطلا نايسلاا ةمق روضح مزت</context>
</contexts>
<marker>Kendall, 1938</marker>
<rawString>Maurice G Kendall. 1938. A New Measure of Rank Correlation. Biometrika.</rawString>
</citation>
<citation valid="true">
<authors>
<author>J Richard Landis</author>
<author>Gary G Koch</author>
</authors>
<title>The Measurement of Observer Agreement for Categorical Data.</title>
<date>1977</date>
<journal>Biometrics,</journal>
<volume>33</volume>
<issue>1</issue>
<contexts>
<context position="10100" citStr="Landis and Koch (1977)" startWordPosition="1606" endWordPosition="1609">et al., 2011). We measured head-to-head pairwise agreement among annotators using Cohen’s kappa (n) (Cohen, 1968), defined as follows: P(A) − P(E) 1 − P(E) where P(A) is the proportion of times annotators agree and P(E) is the proportion of agreement by chance. Table 3 gives average values obtained for interannotator and intra-annotator agreement and compare our results to similar annotation efforts in WMT-13 on different European languages. Here we compare against the average agreement for English to five languages and also from English to one morphologically rich language (Czech).4 Based on Landis and Koch (1977) n interpretation, the ninter value (57%) and also comparing our agreement scores with WMT-13 annotations, we believe that we have reached a reliable and consistent annotation quality. 4 AL-BLEU Despite its well-known shortcomings (CallisonBurch et al., 2006), BLEU continues to be the de-facto MT evaluation metric. BLEU uses an exact n-gram matching criterion that is too strict for a morphologically rich language like Arabic. The system outputs in Table 2 are examples of how BLEU heavily penalizes Arabic. Based on BLEU, the best hypothesis is from Sys5 which has three unigram and one bigram ex</context>
</contexts>
<marker>Landis, Koch, 1977</marker>
<rawString>J Richard Landis and Gary G Koch. 1977. The Measurement of Observer Agreement for Categorical Data. Biometrics, 33(1):159–174.</rawString>
</citation>
<citation valid="true">
<authors>
<author>Ding Liu</author>
<author>Daniel Gildea</author>
</authors>
<title>Syntactic Features for Evaluation of Machine Translation.</title>
<date>2005</date>
<booktitle>In Proceedings of the ACL Workshop on Intrinsic and Extrinsic Evaluation Measures for Machine Translation and/or Summarization,</booktitle>
<pages>25--32</pages>
<contexts>
<context position="17704" citStr="Liu and Gildea (2005)" startWordPosition="2865" endWordPosition="2868">e 300 remaining sentences (100 from each corpus) are kept for testing. The development and test sets are composed of equal portions of sentences from the three sub-corpora (NIST, MEDAR, WIKI). As baselines, we measured the correlation of BLEU and METEOR with human judgments collected for each sentence. We did not observe a strong correlation with the Arabic-tuned METEOR. We conducted our experiments on the standard METEOR which was a stronger baseline than its Arabic version. In order to avoid the zero ngram counts and artificially low BLEU scores, we use a smoothed version of BLEU. We follow Liu and Gildea (2005) to add a small value to both the matched n-grams and the total number of n-grams (epsilon value of 10−3). In order to reach an optimal ordering of partial matches, we conducted a set of experiments in which we compared different orders between the morphological and lexical matchings to settle with the final order which was presented in Figure 1. Table 4 shows a comparison of the average correlation with human judgments for BLEU, METEOR and AL-BLEU. AL-BLEU shows a strong improvement against BLEU and a competitive improvement against METEOR both on the test and development sets. The example in</context>
</contexts>
<marker>Liu, Gildea, 2005</marker>
<rawString>Ding Liu and Daniel Gildea. 2005. Syntactic Features for Evaluation of Machine Translation. In Proceedings of the ACL Workshop on Intrinsic and Extrinsic Evaluation Measures for Machine Translation and/or Summarization, pages 25–32.</rawString>
</citation>
<citation valid="true">
<authors>
<author>Chang Liu</author>
<author>Daniel Dahlmeier</author>
<author>Hwee Tou Ng</author>
</authors>
<title>TESLA: Translation Evaluation of Sentences with Linear-Programming-Based Analysis.</title>
<date>2010</date>
<booktitle>In Proceedings of the Joint Fifth Workshop on Statistical Machine Translation and Metrics (MATR),</booktitle>
<pages>354--359</pages>
<contexts>
<context position="3994" citStr="Liu et al. (2010)" startWordPosition="623" endWordPosition="626">uman judgments, and recently designed and evaluated MPF, a BLEU-style metric based on morphemes and POS tags (Popovi´c, 2011). In the same direc1The dataset and the software are available at: http://nlp.qatar.cmu.edu/resources/ AL-BLEU 207 Proceedings of the 2014 Conference on Empirical Methods in Natural Language Processing (EMNLP), pages 207–213, October 25-29, 2014, Doha, Qatar. c�2014 Association for Computational Linguistics tion, Chen and Kuhn (2011) proposed AMBER, a modified version of BLEU incorporating recall, extra penalties, and light linguistic knowledge about English morphology. Liu et al. (2010) propose TESLA-M, a variant of a metric based on n-gram matching that utilizes light-weight linguistic analysis including lemmatization, POS tagging, and WordNet synonym relations. This metric was then extended to TESLA-B to model phrase synonyms by exploiting bilingual phrase tables (Dahlmeier et al., 2011). Tantug et al. (2008) presented BLEU+, a tool that implements various extension to BLEU computation to allow for a better evaluation of the translation performance for Turkish. To the best of our knowledge the only human judgment dataset for Arabic MT is the small corpus which was used to </context>
</contexts>
<marker>Liu, Dahlmeier, Ng, 2010</marker>
<rawString>Chang Liu, Daniel Dahlmeier, and Hwee Tou Ng. 2010. TESLA: Translation Evaluation of Sentences with Linear-Programming-Based Analysis. In Proceedings of the Joint Fifth Workshop on Statistical Machine Translation and Metrics (MATR), pages 354–359.</rawString>
</citation>
<citation valid="true">
<authors>
<author>Matous Mach´acek</author>
<author>Ondrej Bojar</author>
</authors>
<title>Results of the WMT13 Metrics Shared Task.</title>
<date>2013</date>
<booktitle>In Proceedings of the Eighth Workshop on Statistical Machine Translation,</booktitle>
<pages>45--51</pages>
<location>Sofia, Bulgaria.</location>
<marker>Mach´acek, Bojar, 2013</marker>
<rawString>Matou&amp;quot;s Mach´a&amp;quot;cek and Ond&amp;quot;rej Bojar. 2013. Results of the WMT13 Metrics Shared Task. In Proceedings of the Eighth Workshop on Statistical Machine Translation, pages 45–51, Sofia, Bulgaria.</rawString>
</citation>
<citation valid="true">
<authors>
<author>Bente Maegaard</author>
<author>Mohamed Attia</author>
<author>Khalid Choukri</author>
<author>Olivier Hamon</author>
<author>Steven Krauwer</author>
<author>Mustafa Yaseen</author>
</authors>
<title>Cooperation for Arabic Language Resources and Tools–The MEDAR Project.</title>
<date>2010</date>
<booktitle>In Proceedings of LREC, Valetta,</booktitle>
<contexts>
<context position="5948" citStr="Maegaard et al., 2010" startWordPosition="941" endWordPosition="944"> the value of partial credits. Moreover, our human judgment dataset allows us to validate our framework with a large-scale gold-standard data. 3 Human judgment dataset We describe here our procedure for compiling a diverse Arabic MT dataset and annotating it with human judgments. 3.1 Data and systems We annotate a corpus composed of three datasets: (1) the standard English-Arabic NIST 2005 corpus, commonly used for MT evaluations and composed of news stories. We use the first English translation as the source and the single corresponding Arabic sentence as the reference. (2) the MEDAR corpus (Maegaard et al., 2010) that consists of texts related to the climate change with four Arabic reference translations. We only use the first reference in this study. (3) a small dataset of Wikipedia articles (WIKI) to extend our corpus and metric evaluation to topics beyond the commonly-used news topics. This sub-corpus consists of our in-house Arabic translations of seven English Wikipedia articles. The articles are: Earl Francis Lloyd, Western Europe, Citizenship, Marcus Garvey, Middle Age translation, Acadian, NBA. The English articles which do not exist in the Arabic Wikipedia were manually translated by a biling</context>
</contexts>
<marker>Maegaard, Attia, Choukri, Hamon, Krauwer, Yaseen, 2010</marker>
<rawString>Bente Maegaard, Mohamed Attia, Khalid Choukri, Olivier Hamon, Steven Krauwer, and Mustafa Yaseen. 2010. Cooperation for Arabic Language Resources and Tools–The MEDAR Project. In Proceedings of LREC, Valetta, Malta.</rawString>
</citation>
<citation valid="true">
<authors>
<author>Kishore Papineni</author>
<author>Salim Roukos</author>
<author>Todd Ward</author>
<author>WeiJing Zhu</author>
</authors>
<title>BLEU: A Method for Automatic Evaluation of Machine Translation.</title>
<date>2002</date>
<booktitle>In Proceedings of the 40th Annual Meeting on Association for Computational Linguistics,</booktitle>
<pages>311--318</pages>
<location>Philadelphia, Pennsylvania.</location>
<contexts>
<context position="1454" citStr="Papineni et al., 2002" startWordPosition="225" endWordPosition="229">lation (MT) continues to be a challenging research problem. There is an ongoing effort in finding simple and scalable metrics with rich linguistic analysis. A wide range of metrics have been proposed and evaluated mostly for European target languages (CallisonBurch et al., 2011; Mach´aˇcek and Bojar, 2013). These metrics are usually evaluated based on their correlation with human judgments on a set of MT output. While there has been growing interest in building systems for translating into Arabic, the evaluation of Arabic MT is still an under-studied problem. Standard MT metrics such as BLEU (Papineni et al., 2002) or TER (Snover et al., 2006) have been widely used for evaluating Arabic MT (El Kholy and Habash, 2012). These metrics use strict word and phrase matching between the MT output and reference translations. For morphologically rich target languages such as Arabic, such criteria are too simplistic and inadequate. In this paper, we present: (a) the first human judgment dataset for Arabic MT (b) the Arabic Language BLEU (AL-BLEU), an extension of the BLEU score for Arabic MT evaluation. Our annotated dataset is composed of the output of six MT systems with texts from a diverse set of topics. A gro</context>
</contexts>
<marker>Papineni, Roukos, Ward, Zhu, 2002</marker>
<rawString>Kishore Papineni, Salim Roukos, Todd Ward, and WeiJing Zhu. 2002. BLEU: A Method for Automatic Evaluation of Machine Translation. In Proceedings of the 40th Annual Meeting on Association for Computational Linguistics, pages 311–318, Philadelphia, Pennsylvania.</rawString>
</citation>
<citation valid="true">
<authors>
<author>Maja Popovi´c</author>
<author>Hermann Ney</author>
</authors>
<title>Syntaxoriented Evaluation Measures for Machine Translation Output.</title>
<date>2009</date>
<booktitle>In Proceedings of the Fourth Workshop on Statistical Machine Translation,</booktitle>
<pages>29--32</pages>
<location>Athens, Greece.</location>
<marker>Popovi´c, Ney, 2009</marker>
<rawString>Maja Popovi´c and Hermann Ney. 2009. Syntaxoriented Evaluation Measures for Machine Translation Output. In Proceedings of the Fourth Workshop on Statistical Machine Translation, pages 29– 32, Athens, Greece.</rawString>
</citation>
<citation valid="true">
<authors>
<author>Maja Popovi´c</author>
</authors>
<title>Morphemes and POS Tags for n-gram Based Evaluation Metrics.</title>
<date>2011</date>
<booktitle>In Proceedings of the Sixth Workshop on Statistical Machine Translation,</booktitle>
<pages>104--107</pages>
<location>Edinburgh, Scotland.</location>
<marker>Popovi´c, 2011</marker>
<rawString>Maja Popovi´c. 2011. Morphemes and POS Tags for n-gram Based Evaluation Metrics. In Proceedings of the Sixth Workshop on Statistical Machine Translation, pages 104–107, Edinburgh, Scotland.</rawString>
</citation>
<citation valid="true">
<authors>
<author>Stuart Russell</author>
<author>Peter Norvig</author>
</authors>
<title>Artificial Intelligence: A Modern Approach.</title>
<date>2009</date>
<publisher>Prentice Hall</publisher>
<location>Englewood Cliffs.</location>
<contexts>
<context position="14058" citStr="Russell and Norvig, 2009" startWordPosition="2254" endWordPosition="2257"> candidates are given here along with their associated Bucklwalter transliteration.3 This example, shows clearly that AL-BLEU correlates better with human decision. 1, if th = tr 5 ws + wfi otherwise i=1 Figure 1: Formulation of our partial matching. ture weights (wfi). Each weight is included in the partial score, if such matching exist (e.g., stem match). In order to avoid over-crediting, we limit the range of weights with a set of constraints. Moreover, we use a development set to optimize the weights towards improvement of correlation with human judgments, using a hill-climbing algorithm (Russell and Norvig, 2009). Figure 2 illustrates these various samples of partial matching highlighted in different colors. SRC: Frane Plans To Attend ASEA Figure 2: An MT example with exact matchings (blue), stem and morphological matching (green), stem only matching (red) and morphological-only matching (pink). Following the BLEU-style exact matching and scoring of different n-grams, AL-BLEU updates the n-gram scores with the partial credits from non-exact matches. We use a minimum partial credit for n-grams which have tokens with different matching score. The contribution of a partially-matched n-gram is not 1 (as c</context>
</contexts>
<marker>Russell, Norvig, 2009</marker>
<rawString>Stuart Russell and Peter Norvig. 2009. Artificial Intelligence: A Modern Approach. Prentice Hall Englewood Cliffs.</rawString>
</citation>
<citation valid="true">
<authors>
<author>Matthew Snover</author>
<author>Bonnie J Dorr</author>
<author>Richard Schwartz</author>
<author>Linnea Micciulla</author>
<author>John Makhoul</author>
</authors>
<title>A Study of Translation Edit Rate with Targeted Human Annotation.</title>
<date>2006</date>
<booktitle>In Proceedings of AMTA,</booktitle>
<location>Boston, USA.</location>
<contexts>
<context position="1483" citStr="Snover et al., 2006" startWordPosition="232" endWordPosition="235">hallenging research problem. There is an ongoing effort in finding simple and scalable metrics with rich linguistic analysis. A wide range of metrics have been proposed and evaluated mostly for European target languages (CallisonBurch et al., 2011; Mach´aˇcek and Bojar, 2013). These metrics are usually evaluated based on their correlation with human judgments on a set of MT output. While there has been growing interest in building systems for translating into Arabic, the evaluation of Arabic MT is still an under-studied problem. Standard MT metrics such as BLEU (Papineni et al., 2002) or TER (Snover et al., 2006) have been widely used for evaluating Arabic MT (El Kholy and Habash, 2012). These metrics use strict word and phrase matching between the MT output and reference translations. For morphologically rich target languages such as Arabic, such criteria are too simplistic and inadequate. In this paper, we present: (a) the first human judgment dataset for Arabic MT (b) the Arabic Language BLEU (AL-BLEU), an extension of the BLEU score for Arabic MT evaluation. Our annotated dataset is composed of the output of six MT systems with texts from a diverse set of topics. A group of ten native Arabic speak</context>
</contexts>
<marker>Snover, Dorr, Schwartz, Micciulla, Makhoul, 2006</marker>
<rawString>Matthew Snover, Bonnie J. Dorr, Richard Schwartz, Linnea Micciulla, and John Makhoul. 2006. A Study of Translation Edit Rate with Targeted Human Annotation. In Proceedings of AMTA, Boston, USA.</rawString>
</citation>
<citation valid="true">
<authors>
<author>Matthew Snover</author>
<author>Nitin Madnani</author>
<author>Bonnie J Dorr</author>
<author>Richard Schwartz</author>
</authors>
<title>TER-Plus: Paraphrase, Semantic, and Alignment Enhancements to Translation Edit Rate.</title>
<date>2010</date>
<journal>Machine Translation,</journal>
<pages>23--2</pages>
<contexts>
<context position="3217" citStr="Snover et al., 2010" startWordPosition="512" endWordPosition="515"> 2 Related Work Several studies on MT evaluation have pointed out the inadequacy of the standard n-gram based evaluation metrics for various languages (CallisonBurch et al., 2006). For morphologically complex languages and those without word delimiters, several studies have attempted to improve upon them and suggest more reliable metrics that correlate better with human judgments (Denoual and Lepage, 2005; Homola et al., 2009). A common approach to the problem of morphologically complex words is to integrate some linguistic knowledge in the metric. METEOR (Denkowski and Lavie, 2011), TERPlus (Snover et al., 2010) incorporate limited linguistic resources. Popovi´c and Ney (2009) showed that n-gram based evaluation metrics calculated on POS sequences correlate well with human judgments, and recently designed and evaluated MPF, a BLEU-style metric based on morphemes and POS tags (Popovi´c, 2011). In the same direc1The dataset and the software are available at: http://nlp.qatar.cmu.edu/resources/ AL-BLEU 207 Proceedings of the 2014 Conference on Empirical Methods in Natural Language Processing (EMNLP), pages 207–213, October 25-29, 2014, Doha, Qatar. c�2014 Association for Computational Linguistics tion, </context>
</contexts>
<marker>Snover, Madnani, Dorr, Schwartz, 2010</marker>
<rawString>Matthew Snover, Nitin Madnani, Bonnie J. Dorr, and Richard Schwartz. 2010. TER-Plus: Paraphrase, Semantic, and Alignment Enhancements to Translation Edit Rate. Machine Translation, 23(2-3).</rawString>
</citation>
<citation valid="true">
<authors>
<author>Radu Soricut</author>
<author>Eric Brill</author>
</authors>
<title>A Unified Framework For Automatic Evaluation Using 4-Gram Cooccurrence Statistics.</title>
<date>2004</date>
<booktitle>In Proceedings of the 42nd Meeting of the Association for Computational Linguistics (ACL’04), Main Volume,</booktitle>
<pages>613--620</pages>
<location>Barcelona, Spain,</location>
<contexts>
<context position="15406" citStr="Soricut and Brill, 2004" startWordPosition="2478" endWordPosition="2481">mposed of a token with exact matching and a token with stem matching, this bigram receives a credit equal to a unigram with the stem matching (a value less than 1). While partial credits are added for various n-grams, the final computation of the AL-BLEU is similar to the original BLEU based on the geometric mean of the different matched n-grams. We follow BLEU in using a very small smoothing value to avoid zero n-gram counts and zero score. 5 Experiments and results An automatic evaluation metric is said to be successful if it is shown to have high agreement with human-performed evaluations (Soricut and Brill, 2004). We use Kendall’s tau τ (Kendall, 1938), a coefficient to measure the correlation between the system rankings and the human judgments at the sentence level. Kendall’s tau τ is calculated as follows: τ = # of concordant pairs - # of discordant pairs total pairs where a concordant pair indicates two translations of the same sentence for which the ranks obtained from the manual ranking task and from the corresponding metric scores agree (they disagree in a discordant pair). The possible values of τ range from -1 (all pairs are discordant) to 1 (all pairs m(th, tr) = { نايسلأل ةئراطلا ةمقلا روضحل</context>
</contexts>
<marker>Soricut, Brill, 2004</marker>
<rawString>Radu Soricut and Eric Brill. 2004. A Unified Framework For Automatic Evaluation Using 4-Gram Cooccurrence Statistics. In Proceedings of the 42nd Meeting of the Association for Computational Linguistics (ACL’04), Main Volume, pages 613–620, Barcelona, Spain, July.</rawString>
</citation>
<citation valid="true">
<authors>
<author>C¨uneyd Tantug</author>
</authors>
<title>Kemal Oflazer, and Ilknur Durgar ElKahlout.</title>
<date>2008</date>
<booktitle>In Proceedings of the 6th edition of the Language Resources and Evaluation Conference,</booktitle>
<location>Marrakech, Morocco.</location>
<marker>Tantug, 2008</marker>
<rawString>C¨uneyd Tantug, Kemal Oflazer, and Ilknur Durgar ElKahlout. 2008. BLEU+: a Tool for Fine-Grained BLEU Computation. In Proceedings of the 6th edition of the Language Resources and Evaluation Conference, Marrakech, Morocco.</rawString>
</citation>
</citationList>
</algorithm>
</algorithms>
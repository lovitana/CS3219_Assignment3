<?xml version="1.0" encoding="UTF-8"?>
<algorithms version="110505">
<algorithm name="SectLabel" version="110505">
<variant no="0" confidence="0.000005">
<title confidence="0.9974575">
Domain Adaptation for CRF-based Chinese Word Segmentation using
Free Annotations
</title>
<author confidence="0.994024">
Yijia Liu †‡, Yue Zhang †, Wanxiang Che ‡, Ting Liu ‡, Fan Wu ††Singapore University of Technology and Design
</author>
<affiliation confidence="0.9402605">
‡Research Center for Social Computing and Information Retrieval
Harbin Institute of Technology, China
</affiliation>
<email confidence="0.990946">
{yjliu,car,tliu}@ir.hit.edu.cn {yue zhang,fan wu}@sutd.edu.sg
</email>
<sectionHeader confidence="0.993687" genericHeader="abstract">
Abstract
</sectionHeader>
<bodyText confidence="0.999926947368421">
Supervised methods have been the domi-
nant approach for Chinese word segmen-
tation. The performance can drop signif-
icantly when the test domain is different
from the training domain. In this paper,
we study the problem of obtaining par-
tial annotation from freely available data
to help Chinese word segmentation on dif-
ferent domains. Different sources of free
annotations are transformed into a unified
form of partial annotation and a variant
CRF model is used to leverage both fully
and partially annotated data consistently.
Experimental results show that the Chi-
nese word segmentation model benefits
from free partially annotated data. On the
SIGHAN Bakeoff 2010 data, we achieve
results that are competitive to the best re-
ported in the literature.
</bodyText>
<sectionHeader confidence="0.998991" genericHeader="introduction">
1 Introduction
</sectionHeader>
<bodyText confidence="0.996772333333333">
Statistical Chinese word segmentation gains high
accuracies on newswire (Xue and Shen, 2003;
Zhang and Clark, 2007; Jiang et al., 2009; Zhao
et al., 2010; Sun and Xu, 2011). However, man-
ually annotated training data mostly come from
the news domain, and the performance can drop
severely when the test data shift from newswire
to blogs, computer forums and Internet literature
(Liu and Zhang, 2012).
Several methods have been proposed for solv-
ing the domain adaptation problem for segmenta-
tion, which include the traditional token- and type-
supervised methods (Song et al., 2012; Zhang et
al., 2014). While token-supervised methods rely
on manually annotated target-domain sentences,
type-supervised methods leverage manually as-
sembled domain-specific lexicons to improve
target-domain segmentation accuracies. Both
</bodyText>
<figureCaption confidence="0.64905775">
Figure 1: The segmentation problem, illustrated
using the sentence “浦东 (Pudong) 开发 (devel-
opment) 与 (and) 法制 (legal) 建设 (construc-
tion)”. Possible segmentation labels are drawn un-
</figureCaption>
<bodyText confidence="0.990469379310345">
der each character, where b, m, e, s stand for the
beginning, middle, end of a multi-character word,
and a single character word, respectively. The path
shows the correct segmentation by choosing one
label for each character.
methods are competitive given the same amount of
annotation effects (Garrette and Baldridge, 2012;
Zhang et al., 2014). However, obtaining manually
annotated data can be expensive.
On the other hand, there are free data which
contain limited but useful segmentation informa-
tion over the Internet, including large-scale un-
labeled data, domain-specific lexicons and semi-
annotated web pages such as Wikipedia. In the
last case, word-boundary information is contained
in hyperlinks and other markup annotations. Such
free data offer a useful alternative for improving
the segmentation performance, especially on do-
mains that are not identical to newswire, and for
which little annotation is available.
In this paper, we investigate techniques for
adopting freely available data to help improve the
performance on Chinese word segmentation. We
propose a simple but robust method for construct-
ing partial segmentation from different sources
of free data, including unlabeled data and the
Wikipedia. There has been work on making use
of both unlabeled data (Sun and Xu, 2011; Wang
et al., 2011) and Wikipedia (Jiang et al., 2013)
</bodyText>
<figure confidence="0.9940552">
浦 东 开 发 与 法 制 建 设
b b b b b b b b b
m m m m m m m m m
e e e e e e e e e
s s s s s s s s s
</figure>
<page confidence="0.978552">
864
</page>
<note confidence="0.9104205">
Proceedings of the 2014 Conference on Empirical Methods in Natural Language Processing (EMNLP), pages 864–874,
October 25-29, 2014, Doha, Qatar. c�2014 Association for Computational Linguistics
</note>
<bodyText confidence="0.999759833333333">
to improve segmentation. However, no empiri-
cal results have been reported on a unified ap-
proach to deal with different types of free data.
We use a conditional random fields (Lafferty et al.,
2001; Tsuboi et al., 2008) variant that can lever-
age the partial annotations obtained from different
sources of free annotation. Training is achieved by
a modification to the learning objective, incorpo-
rating partial annotation likelihood, so that a single
model can be trained consistently with a mixture
of full and partial annotation.
Experimental results show that our method of
using partially annotated data can consistently im-
proves cross-domain segmentation performance.
We obtain results which are competitive to the
best reported in the literature. Our segmentor
is freely released at https://github.com/
ExpResults/partial-crfsuite.
</bodyText>
<sectionHeader confidence="0.983113" genericHeader="method">
2 Obtaining Partially Annotated Data
</sectionHeader>
<bodyText confidence="0.999963935483871">
We model the Chinese word segmentation task as
a character sequence tagging problem, which is to
give each character in a sentence a word-boundary
tag (Xue and Shen, 2003). We adopt four tags, b,
m, e and s, which represent the beginning, middle,
end of a multi-character word, and a single char-
acter word, respectively. A manually segmented
sentence can be represented as a tag sequence, as
shown in Figure 1.
We investigate two major sources of freely-
available annotations: lexicons and natural anno-
tation, both with the help of unannotated data.
To make use of the first source of informa-
tion, we incorporate words from a lexicon into
unannotated sentences by matching of character
sequences, resulting in partially annotated sen-
tences, as shown in Figure 2a. In this example,
the word “狐岐山 (the Huqi Mountain)” in the
unannotated sentence matches an item in the lex-
icon. As a result, we obtain a partially-annotated
sentence, in which the segmentation ambiguity of
the characters “狐 (fox)”, “岐 (brandy road)” and
“山 (mountain)” are resolved (“狐” being the be-
ginning, “岐” being the middle and “山” being the
end of the same word). At the same time, the seg-
mentation ambiguity of the surrounding characters
“在 (at)” and “救 (save)” are reduced (“在” be-
ing either a single-character word or the end of
a multi-character word, and “救” being either a
single-character word or the beginning of a multi-
character word).
</bodyText>
<figure confidence="0.8815465">
(a) “在 (at) 狐岐山 (Huqi Mountain) 救 治 (save) 碧
瑶 (Biyao)”, where “狐岐山” matches a lexicon word.
(b) “如 (e.g.) 乳铁蛋白 (lysozyme) 、 溶菌酶 (lactoferrin)”,
where “乳铁蛋白” is a hyperlink.
</figure>
<figureCaption confidence="0.970704">
Figure 2: Examples of partially annotated data.
</figureCaption>
<bodyText confidence="0.99165245">
The paths show possible correct segmentations.
Natural annotation, which refers to word
boundaries that can be inferred from URLs, fonts
or colors on web pages, also result in partially-
annotated sentences. Taking a web page shown
in Figure 2b for example. It can be inferred from
the URL tags on “乳铁蛋白” that “乳” should be
either the beginning of a multi-character word or
a single-character word, and “白” should be either
the end a multi-character word or single-character
word. Similarly, possible tags of the surrounding
character “如” and “、” can also be inferred.
We turn both lexicons and natural annotation
into the same form of partial annotation with
same unresolved ambiguities, as shown in Figure
2, and use them together with available full anno-
tation (Figure 1) as the training data for the seg-
mentor. In this section, we describe in detail how
to obtain partially annotated sentences from each
resource, respectively.
</bodyText>
<subsectionHeader confidence="0.985276">
2.1 Lexicons
</subsectionHeader>
<bodyText confidence="0.999931444444444">
In this scenario, we assume that there are unla-
beled sentences along with a lexicon for the target
domain. We obtain partially segmented sentences
by extracting word boundaries from the unlabeled
sentences with the help of the lexicon. Previous
matching methods (Wu and Tseng, 1993; Wong
and Chan, 1996) for Chinese word segmentation
largely rely on the lexicons, and are generally con-
sidered being weak in ambiguity resolution (Gao
</bodyText>
<figure confidence="0.964485363636364">
在 狐 ,
歧 山 救 治 碧 瑶
b b b b b b b b b
m m m m m m m m m
e e e e e e e e e
s s s s s s s s s
如 乳 铁 蛋 白 、 溶 菌 酶
b b b b b b b b b
m m m m m m m m m
e e e e e e e e e
s s s s s s s s s
</figure>
<page confidence="0.555082">
865
</page>
<figure confidence="0.992706888888889">
People’s
Daily
*P] (saw) 4A (Hainan) A*A, (tourist industry) it�-A (full) *V (hope)
saw tourist industry in Hainan is full of hope
Wikipedia
�L-*(mainly) A_(is) A* (tourist) A (industry) (and) *k* (software) r&apos;A(industry)
mainly is tourist industry and software industry
(a) Case of incompatible annotation on “A游业(tourist industry)” between People’s Daily and Wikipedia.
(b) Similar subsequence “Vl(field)” is segmented differently under different domains in Wikipedia.
</figure>
<tableCaption confidence="0.992129">
Table 1: Examples natural annotation from Wikipedia. Underline marks annotated words.
</tableCaption>
<bodyText confidence="0.8800265">
((i i- -T (Shuo Wen Jie Zi, a book) R(segmented) i!(annotated) ))
the segmented and annotated version of Shuo Wen Jie Zi
</bodyText>
<equation confidence="0.72762125">
Literature
-* (each) -iEAX(record) �k(is) 4j&amp;quot;M(splitted) jJ(into) -TR (fields)
each record is splitted into several fields
Computer
</equation>
<bodyText confidence="0.999928071428571">
et al., 2005). But for obtaining the partial labeled
data with lexicon, the matching method can still be
a solution. Since we do not aim to recognize every
word from sentence, we can select a lexicon with
smaller coverage but less ambiguity to achieve rel-
atively precise matching result.
In this paper, we apply two matching schemes
to the same raw sentences to obtain partially an-
notated sentences. The first is a simple forward-
maximum matching (FMM) scheme, which is
very close to the forward maximum matching al-
gorithm of Wu and Tseng (1993) for Chinese word
segmentation. This scheme scans the input sen-
tence from left to right. At each position, it at-
tempts to find the longest subsequence of Chi-
nese characters that matches a lexicon entry. If
such an entry is found, the subsequence is tagged
with the corresponding tags, and its surrounding
characters are also constrained to a smaller set of
tags. If no subsequence is found in the lexicon, the
character is left with all the possible tags. Taking
the sentence in Figure 2a for example. When the
algorithm scans the second character, “VP”, and
finds the entry “VP,_t�W” in the lexicon, the sub-
sequence of characters is recognized as a word,
and tagged with b, m and e, respectively. At the
same time, the previous character “.fit.” can be in-
ferred as only end of a multi-character word (e) or
a single-character word (s). The second matching
scheme is backward maximum matching, which
can be treated as the application of FMM on the
reverse of unlabeled sentences using a lexicon of
reversed words.
To mitigate the errors resulting from one single
matching scheme, we combine the two matching
results by agreement. The basic idea is that if a
subsequence of sentence is recognized as word by
multiple matching results, it can be considered as a
more precise annotation. Our algorithm reads par-
tial segmentation by different methods and selects
the subsequences that are identified as word by all
methods as annotated words.
</bodyText>
<subsectionHeader confidence="0.99648">
2.2 Natural Annotation
</subsectionHeader>
<bodyText confidence="0.999981">
We use the Chinese Wikipedia for natural anno-
tation. Partially annotated sentences are readily
formed in Wikipedia by markup syntax, such as
URLs. However, some subtle issues exist if the
sentences are used directly. One problem is in-
compatibility of segmentation standards between
the annotated training data and Wikipedia. Jiang
et al. (2009) discuss this incompatibility problem
between two corpora — the CTB and the Peo-
ple’s Daily; the problem is even more severe on
Wikipedia because it can be edited by any user.
Table 1a shows a case of incompatible annota-
tion between the People’s Daily data and natural
annotation in Wikipedia, where the three charac-
ters “���” are segmented differently. Both can
be treated as correct, although they have different
segmentation granularities.
Another problem is the intrinsic ambiguity of
segmentation. The same character sequence can
be segmented into different words under differ-
ent contexts. If the training and test data contain
different contexts, the learned model can give in-
correct results on the test data. This is particu-
larly true across different domains. Table 1b gives
such an example, where the character sequence
“-TR” is segmented differently in two of our test
domains, but both cases exist in Wikipedia.
In summary, Wikipedia introduces both use-
ful information for domain adaptation and harm-
ful noise with negative effects on the model. To
</bodyText>
<page confidence="0.991327">
866
</page>
<bodyText confidence="0.999983166666667">
achieve better performance of domain adaptation
using Wikipedia, one intuitive approach is to se-
lect more domain-related data and less irrelevant
data to minimize the risks that result from incom-
patible annotation and domain difference.
To this end, we assume that there are some raw
sentences on the target domain, which can be used
to evaluate the relevance between Wikipedia and
target domain test data. We assume that URL-
tagged entries reflect the segmentation standards
of Wikipedia sentence, and use them to match
Wikipedia sentences with the raw target domain
data. If the character sequence of any URL-tagged
entry in a Wikipedia sentence matches the target
domain data, the Wikipedia sentence is selected
for training. Another advantage of such data se-
lection is that the training time consumption can
be reduced by reducing the size of training data.
</bodyText>
<sectionHeader confidence="0.999506" genericHeader="method">
3 CRF for Word Segmentation
</sectionHeader>
<bodyText confidence="0.998542833333333">
We follow the work of Zhao et al. (2010) and Sun
and Xu (2011), and adopt the Conditional Random
Fields (CRF) model (Lafferty et al., 2001) for the
sequence labeling problem of word segmentation.
Given an input characters sequence, the task is to
assign one segmentation label from {b, m, e, s} on
each character. Let x = (x1,x2, ..., xT) be the
sequence of characters in sentence whose length
is T, and y = (y1, y2, ..., yT) be the correspond-
ing label sequence, where yi E Y . The linear-
chain conditional random field for Chinese word
segmentation can be formalized as
</bodyText>
<equation confidence="0.982856">
∑T
1
p(y|x) = Z exp
t=1
</equation>
<bodyText confidence="0.999987">
where Ak are the model parameters, fk are the fea-
ture functions and Z is the probability normalizer.
</bodyText>
<equation confidence="0.913706">
Akfk(yt, yt−1, x) (2)
</equation>
<bodyText confidence="0.9996883">
We follow Sun and Xu (2011) and use the fea-
ture templates shown in Table 2 to model the seg-
mented task. For ith character in the sentence, the
n-gram features represent the surrounding charac-
ters of this character; Type categorizes the charac-
ter it into digit, punctuation, english and other;
Identical indicates whether the input character is
the same with its surrounding characters. This
feature captures repetition patterns such as “试
试 (try)” or “走走 (stroll)”.
</bodyText>
<figure confidence="0.938128181818182">
Type Template
unigram Cs (i − 3 &lt; s &lt; i + 3)
bigram CsCs+1 (i − 3 &lt; s &lt; i + 2)
CsCs+2 (i − 3 &lt; s &lt; i + 1)
type Type(Ci)
Type(Cs)T ype(Cs+1)
(i − 1 &lt; s &lt; i + 2)
identical Identical(Cs, Cs+1) (i − 3 &lt;
s &lt; i + 1)
Identical(Cs,Cs+2) (i − 3 &lt;
s &lt; i)
</figure>
<tableCaption confidence="0.965478">
Table 2: Feature templates for the ith character.
</tableCaption>
<bodyText confidence="0.9901695">
For fully-annotated training data, the learning
problem of conditional random fields is to maxi-
mize the log likelihood over all the training data
(Lafferty et al., 2001)
</bodyText>
<equation confidence="0.686567">
log p(y(n)|x(n))
</equation>
<bodyText confidence="0.980890931034483">
Here N is the number of training sentences. Both
the likelihood p(y(n)|x(n)) and its gradient can be
calculated by performing the forward-backward
algorithm (Baum and Petrie, 1966) on the se-
quence, and several optimization algorithm can be
adopted to learn parameters from data, including
L-BFGS (Liu and Nocedal, 1989) and SGD (Bot-
tou, 1991).
4 Training a CRF with partially
annotated data
For word segmentation with partially annotated
data, some characters in a sentence can have
a definite segmentation label, while some can
have multiple labels with ambiguities remain-
ing. Taking the partially annotated sentence
in Figure 2a for example, the corresponding
potential label sequence for “在狐岐山救” is
{(e, s), (b), (m), (e), (b, s)}, where the characters
“狐”, “岐” and “山” have fixed labels but for “在”
and “救”, some ambiguities exist. Note that the
full annotation in Figure 1 can be regarded as a
special case of partial annotation, where the num-
ber of potential labels for each character is one.
We follow Tsuboi et al. (2008) and model
marginal probabilities over partially annotated
data. Define the possible labels that correspond
to the partial annotation as L = (L1, L2,..., LT),
where each Li is a non-empty subset of Y that cor-
responds to the set of possible labels for xi. Let
</bodyText>
<equation confidence="0.999539416666667">
∑ Akfk(yt, yt−1, x) (1)
k
∑Z =
Y
∑
k
exp
∑T
t=1
∑N
n=1
L =
</equation>
<page confidence="0.942055">
867
</page>
<bodyText confidence="0.967181">
YL be the set of all possible label sequences where
VY E YL, yi E Li. The marginal probability of
YL can be modeled as
</bodyText>
<equation confidence="0.979137571428571">
1 ∑
p(YL|x) = Z
YEYL
Defining the unnormalized marginal probability as
T
�ZYL = exp � λkfk(yt, yt−1, X),
YEYL t=1 k
</equation>
<bodyText confidence="0.99922475">
and the normalizer Z being the same as Equation
2, the log marginal probability of YL over N par-
tially annotated training examples can be formal-
ized as
</bodyText>
<equation confidence="0.859455833333333">
N N
LYL = log p(YL|X) = (log ZYL − log Z)
n=1 n=1
The gradient of the likelihood can be written as
∂cYL
∂λk
</equation>
<bodyText confidence="0.999572571428571">
Both ZYL and its gradient are similar in form to
Z. By introducing a modification to the forward-
backward algorithm, ZYL and LYL can be calcu-
lated. Define the forward variable for partially an-
notated data αYL,t(j) = pYL(x(1,...,t), yt = j). A
modification on the forward algorithm can be for-
malized as
</bodyText>
<equation confidence="0.992373333333333">
{ 0 j E/ Lt
αYL,t(j) = ∑ Ψt(j, i, xt)αYL,t−1(i) j E Lt
iELt−1
</equation>
<bodyText confidence="0.989865666666667">
where Ψt(j, i, x) is a potential function that equals
Ek λkfk(yt = j, yt−1 = i, xt). Similarly, for the
backward variable βYL,t,
</bodyText>
<equation confidence="0.986234">
{ //�� 0 i E/Lt
NYL,t(i) = ∑9ELt+1 Ψt(j, i, xt+1)βYL,t+1(j) i E Lt
</equation>
<bodyText confidence="0.996801916666667">
ZYL can be calculated by αYL(T),
and pYL(y, y′|x) can be calculated by
αYL,t−1(y′)Ψt(y, y′, xt)βYL,t(y).
Note that if each element in YL is constrained
to one single label, the CRF model in Equation 3
degrades into Equation 1. So we can train a unified
model with both fully and partially annotated data.
We implement this CRF model based on a open
source toolkit CRFSuite.1 In our experiments, we
use the L-BFGS (Liu and Nocedal, 1989) algo-
rithm to learn parameters from both fully and par-
tially annotated data.
</bodyText>
<sectionHeader confidence="0.999548" genericHeader="method">
5 Experiments
</sectionHeader>
<bodyText confidence="0.999938861111111">
We perform our experiments on the domain adap-
tation test data from SIGHAN Bakeoff 2010 (Zhao
et al., 2010), adapting annotated training sentences
from People’s Daily (PD) (Yu et al., 2001) to
different test domains. The fully annotated data
is selected from the People’s Daily newspaper
in January of 1998, and the four test domains
from the SIGHAN Bakeoff 2010 include finance,
medicine, literature and computer. Sample seg-
mented data in the computer domain from this
bakeoff is used as development set. Statistics of
the data are shown in first half of Table 3. We
use wikidump201404192 for the Wikipedia data.
All the traditional Chinese pages in Wikipedia are
converted to simplified Chinese. After filtering
functional pages like redirection and removing du-
plication, 5.45 million sentences are reserved.
For comparison with related work on using a
lexicon to improve segmentation, another set of
test data is chosen for this setting. We use the Chi-
nese Treebank (CTB) as the source domain data,
and Zhuxian (a free Internet novel, also named as
“Jade dynasty”, referred to as ZX henceforth) as
the target domain data.3 The ZX data are written
in a different style from newswire, and contains
many out-of-vocabulary words. This setting has
been used by Liu and Zhang (2012) and Zhang et
al. (2014) for domain adaptation of segmentation
and POS-tagging. We use the standard training,
development and test split. Statistics of the test
data annotated by Zhang et al. (2014) are shown
in the second half of Table 3.
The data preparation method in Section 2 and
the CRF method in Section 4 are used for all
the experiments. Both recall of out-of-vocabulary
words (Roov) and F-score are used to evaluate the
</bodyText>
<footnote confidence="0.994292142857143">
1http://www.chokkan.org/software/
crfsuite/
2http://dumps.wikimedia.org/zhwiki/
20140419/
3Annotated target domain test data and lexicon are avail-
able from http://ir.hit.edu.cn/˜mszhang/
eacl14mszhang.zip.
</footnote>
<equation confidence="0.912780642857143">
λkfk(yt, yt−1, x) (3)
∑
k
exp
T
∑
t=1
fk(y, y�, x)p(y, y�|x)
=
N
∑
n=1
fk(yYL, yYL, x)pYL(yYL, y�YL|x)
T
</equation>
<figure confidence="0.794586666666667">
∑
t=1
∑
yYL ELt,
y′YL
ELt−1
�
N
∑
n=1
T
∑
t=1
∑
y,y′
</figure>
<page confidence="0.969244">
868
</page>
<table confidence="0.9987644">
CTB5 —* ZXPD —* SIGHAN Data set Train Development Test
PD Computer Finance Medicine Literature Computer
# sent. 19,056 1,000 560 1,308 670 1,329
# words 1,109,734 21,398 33,035 31,499 35,735 35,319
OOV 0.1766 0.0874 0.1102 0.0619 0.1522
Data set Train Development Test Unlabeled Wikipedia Unlabeled
CTB5 ZX
# sent. 18,086 788 1,394 32,023 5,456,151
# words 493,934 20,393 34,355
OOV 0.1377 0.1550
</table>
<tableCaption confidence="0.999888">
Table 3: Statistics of data used in this paper.
</tableCaption>
<bodyText confidence="0.9823195">
segmentation performance. There is a mixture of
Chinese characters, English words and numeric
expression in the test data from SIGHAN Bakeoff
2010. To test the influence of Wikipedia data on
Chinese word segmentation alone, we apply reg-
ular expressions to detect English words and nu-
meric expressions, so that they are marked as not
segmented. After performing this preprocessing
step, cleaned test input data are fed to the CRF
model to give a relatively strong baseline.
</bodyText>
<subsectionHeader confidence="0.955068">
5.1 Free Lexicons
5.1.1 Obtaining lexicons
</subsectionHeader>
<bodyText confidence="0.999968157894737">
For domain adaption from CTB to ZX, we use
a lexicon released by Zhang et al. (2014). The
lexicon is crawled from a online encyclopedia4,
and contains the names of 159 characters and ar-
tifacts in the Zhuxian novel. We follow Zhang et
al. (2014) and name it NR for convenience of fur-
ther discussion. The NR lexicon can be treated
as a strongly domain-related, high quality but rel-
atively small lexicon. It’s a typical example of
freely available lexicon over the Internet.
For domain adaptation from PD to medicine and
computer, we collect a list of page titles under
the corresponding categories in Wikipedia. For
medicine, entries under essential medicines, bi-
ological system and diseases are collected. For
computer, entries under computer network, Mi-
crosoft Windows and software widgets are se-
lected. These lexicons are typical freely available
lexicons that we can access to.
</bodyText>
<subsectionHeader confidence="0.780774">
5.1.2 Obtaining Unlabeled Sentences
</subsectionHeader>
<bodyText confidence="0.982404333333333">
For ZX, partially annotated sentences are obtained
using the NR lexicon and unlabeled ZX sentences
by applying the matching scheme described in
</bodyText>
<footnote confidence="0.837475">
4http://baike.baidu.com/view/18277.htm
</footnote>
<page confidence="0.481101">
90.4
</page>
<figure confidence="0.9948248">
F score on development 90.3
90.2
90.1
1 2 4 8 16 32
# of sentences * 1000
</figure>
<figureCaption confidence="0.979778">
Figure 3: F-score on the development data when
using different numbers of unlabeled data.
</figureCaption>
<bodyText confidence="0.944289944444444">
Section 2. The CTB5 training data and the par-
tially annotated data are mixed as the final train-
ing data. Different amounts of unlabeled data are
applied to the development test set, and results are
shown in Figure 3. From this figure we can see
that incorporating 16K sentences gives the high-
est accuracy, and adding more partial labeled data
does not change the accuracy significantly. So for
the ZX experiments, we choose the 16K sentences
as the unlabeled data.
For the medicine and computer experiments, we
selected domain-specific sentences by matching
with the domain-specific lexicons. About 46K out
of the 5.45 million wiki sentences contain subse-
quences in the medicine lexicon and 22K in the
case of the computer domain. We randomly se-
lect 16K sentences as the unlabeled data for each
domain, respectively.
</bodyText>
<subsectionHeader confidence="0.647671">
5.1.3 Final results
</subsectionHeader>
<bodyText confidence="0.999945333333333">
We incorporate the partially annotated data ob-
tained with the help of lexicon for each of the
test domain. For adaptation from CTB to ZX, we
trained our baseline model on the CTB5 training
data with the feature templates in Table 2. For
adaptation from PD to medicine and computer, we
</bodyText>
<page confidence="0.997258">
869
</page>
<table confidence="0.999643">
Domain ZX Medicine Computer
F Roov F Roov F Roov
Baseline 87.50 73.65 91.36 72.95 93.16 84.02
Baseline+Lexicon Feature 90.36 80.69 91.60 74.39 93.14 84.27
Baseline+PA (Lex) 90.63 84.88 91.68 74.99 93.47 85.63
Zhang et al. (2014) 88.34 - - - - -
</table>
<tableCaption confidence="0.9697435">
Table 4: Final result for adapting CTB to Zhuxian and adapting PD to the medicine and computer
domains, using partially annotated data (referred to as PA) obtained from unlabeled data and lexicons.
</tableCaption>
<bodyText confidence="0.988919019230769">
trained our baseline model on the PD training data
with the same feature template setting.
Previous research makes use of a lexicon by
adding lexicon features directly into a model (Sun
and Xu, 2011; Zhang et al., 2014), rather than
transforming them into partially annotated sen-
tences. To make a comparison, we follow Sun and
Xu (2011) and add three lexicon features to repre-
sent whether cz is located at the beginning, middle
or the end of a word in the lexicon, respectively.
For each test domain, the lexicon for the lexi-
con feature model consists of the most frequent
words in the source domain training data (about
6.7K for CTB5 and 8K for PD, respectively) and
the domain-specific lexicon we obtained in Sec-
tion 5.1.1.
The results are shown in Table 4, where the first
row shows the performance of the baseline mod-
els and the second row shows the performance
of the model incorporating lexicon feature. The
third row shows our method using partial anno-
tation. On the ZX test set, our method outper-
forms the baseline by more than 3 absolute per-
centage. The model with partially annotated data
performs better than the one with additional lexi-
con features. Similar conclusion is obtained when
adapting from PD to medicine and computer. By
incorporating the partially annotated data, the seg-
mentation of lexicon words, along with the con-
text, is learned.
We also compare our method with the work of
Zhang et al. (2014), who reported results only on
the ZX test data. We use the same lexicon settings.
Our method gives better result than Zhang et al.
(2014), showing that the combination of a lexicon
and unannotated sentence into partially annotated
data can lead to better performance than using a
dictionary alone in type-supervision. Given that
we only explore the use of free resource, combin-
ing a lexicon with unannotated sentences is a bet-
ter option than using the lexicon directly. Zhang
et al.’s concern, on the other hand, is to compare
Table 5: The performance of data selection on the
development set of the computer domain.
type- and token-annotation. Our partial annota-
tion can thus be treated as a compromise to obtain
some pseudo partial token-annotations when full
token annotations are unavailable. Another thing
to note is that the model of Zhang et al. (2014) is
a joint model for segmentation and POS-tagging,
which is generally considered stronger than a sin-
gle segmentation model.
</bodyText>
<subsectionHeader confidence="0.999918">
5.2 Free Natural Annotation
</subsectionHeader>
<bodyText confidence="0.999985">
When extracting word boundaries from Wikipedia
sentences, we ignore natural annotations on En-
glish words and digits because these words are rec-
ognized by the preprocessor. Following Jiang et
al. (2013), we also recognize a naturally annotated
two-character subsequence as a word.
</bodyText>
<subsectionHeader confidence="0.998384">
5.2.1 Effect of data selection
</subsectionHeader>
<bodyText confidence="0.9999542">
To make better use of more domain-specific data,
and to alleviate noise in partial annotation, we ap-
ply the selection method proposed in Section 2
to the Wikipedia data. On the computer domain
development test data, this selection method re-
sults in 9.4K computer-related sentences with par-
tial annotation. A model is trained with both the
PD training data and the partially annotated com-
puter domain Wikipedia data. For comparison, we
also trained a model with 160K randomly selected
Wikipedia sentences. The experimental result is
shown in Table 5. The model incorporating se-
lected data achieves better performance compared
to the model with randomly sampled data, demon-
strating that data selection is helpful to improving
</bodyText>
<figure confidence="0.996594615384615">
F
Com. Dev
Roov
Method
Baseline
Baseline+PA (Selected)
93.56
94.29
95.00
83.75
86.58
88.28
Baseline+PA (Random 160K)
</figure>
<page confidence="0.975799">
870
</page>
<table confidence="0.99911225">
Method Finance Medicine Literature Computer Avg-F
F Roov F Roov F Roov F Roov
Baseline 95.20 86.90 91.36 72.90 92.27 73.61 93.16 83.48 93.00
Baseline+PA (Ran- 95.16 87.60 92.41 78.13 92.17 75.30 93.91 83.48 93.41
dom 160K)
Baseline+PA 95.54 88.53 92.47 78.28 92.49 76.84 93.93 87.53 93.61
(Selected) +0.34 +1.11 +0.22 +0.77
Jiang et al. (2013) 93.16 93.34 93.53 91.19 92.80
</table>
<tableCaption confidence="0.6690855">
Table 6: Experimental results on the SIGHAN Bakeoff 2010 data.
the domain adaption accuracy.
</tableCaption>
<subsectionHeader confidence="0.861559">
5.2.2 Final Result
</subsectionHeader>
<bodyText confidence="0.999910545454545">
The final results on the four test domains are
shown in Table 6. From this table, we can see
that significant improvements are achieved with
the help of the partially annotated Wikipedia data,
when compared to the baseline. The models
trained with selected partial annotation perform
better than those trained with random partial an-
notation. Our F-scores are competitive to those re-
ported by Jiang et al. (2013). However, since their
model is trained on a different source domain, the
results are not directly comparable.
</bodyText>
<subsectionHeader confidence="0.987265">
5.2.3 Analysis
</subsectionHeader>
<bodyText confidence="0.9756055">
In this section, we study the effect of Wikipedia on
domain adaptation when no data selection is per-
formed, in order to analyze the effect of partially
annotated data. We randomly sample 10K, 20K,
40K, 80K and 160K sentences from the 5.45 mil-
lion Wikipedia sentences, and incorporate them
into the training process, respectively. Five models
are obtained adding the baseline, and we test their
performances on the four test domains. Figure 4
shows the results.
From the figure we can see that for the medicine
and computer domains, where the OOV rate is rel-
atively high, the F-score generally increases when
more data from Wikipedia are used. The trends
of F-score and OOV recall against the volume of
Wikipedia data are almost identical. However, for
the finance and literature domains, which have low
OOV rates, such a relation between data size and
accuracy is not witnessed. For the literature do-
main, even an opposite trends is shown.
We can draw the following conclusions: (1)
Natural annotation on Wikipedia data contributes
to the recognition of OOV words on domain adap-
tation; (2) target domains with more OOV words
benefit more from Wikipedia data. (3) along with
Table 7: Results by combining different sources of
free annotation.
the positive effect on OOV recognition, Wikipedia
data can also introduce noise, and hence data se-
lection can be useful.
</bodyText>
<subsectionHeader confidence="0.9973735">
5.3 Combining Lexicon and Natural
Annotation
</subsectionHeader>
<bodyText confidence="0.9999958">
To make the most use of free annotation, we com-
bine available free lexicon and natural annotation
resources by joining the partially annotated sen-
tences derived using each resource, training our
CRF model with these partially annotated sen-
tences and the fully annotated PD sentences. The
tests are performed on medicine and computer do-
mains. Table 7 shows the results, where further
improvements are made on both domains when the
two types of resources are combined.
</bodyText>
<sectionHeader confidence="0.999903" genericHeader="evaluation">
6 Related Work
</sectionHeader>
<bodyText confidence="0.9999615">
There has been a line of research on making use of
unlabeled data for word segmentation. Zhao and
Kit (2008) improve segmentation performance by
mutual information between characters, collected
from large unlabeled data; Li and Sun (2009) use
punctuation information in a large raw corpus to
learn a segmentation model, and achieve better
recognition of OOV words; Sun and Xu (2011) ex-
plore several statistical features derived from un-
labeled data to help improve character-based word
segmentation. These investigations mainly focus
on in-domain accuracies. Liu and Zhang (2012)
</bodyText>
<figure confidence="0.990209815789474">
Method
Baseline
Baseline+PA (Lex)
Baseline+PA (Natural)
Baseline+PA (Lex+Natural)
91.36
91.68
92.47
92.63
Med.
F
93.16
93.47
93.93
94.07
Com.
F
871
0.9525 0.8775
0.9522 0.8750
0.9519 0.8725
0.9516 0.8700
0 50 100 150
(a) Finance
0.9225 0.78
0.9200 0.77
0.9175 0.76
0.9150 0.75
0.74
0.73
0 50 100 150
(b) Medicine
0.9225 0.750 0.938 0.87
0.9222 0.745 0.936 0.86
0.9219 0.740 0.934 0.85
0.9216 0.735 0.84
0 50 100 150 0 50 100 150
(c) Literature (d) Computer
</figure>
<figureCaption confidence="0.9595145">
Figure 4: Performance of the model incorporating difference sizes of Wikipedia data. The solid line
represents the F-score and dashed line represents the recall of OOV words.
</figureCaption>
<bodyText confidence="0.999961367346939">
study domain adaptation using an unsupervised
self-training method. In contrast to their work,
we make use of not only unlabeled data, but also
leverage any free annotation to achieve better re-
sults for domain adaptation.
There has also been work on making use of a
dictionary and natural annotation for segmenta-
tion. Zhang et al. (2014) study type-supervised do-
main adaptation for Chinese segmentation. They
categorize domain difference into two types: dif-
ferent vocabulary and different POS distributions.
While the first type of difference can be effec-
tively resolved by using lexicon for each domain,
the second type of difference needs to be resolved
by using annotated sentences. They found that
given the same manual annotation time, a com-
bination of the lexicon and sentence is the most
effective. Jiang et al. (2013) use 160K Wikipedia
sentences to improves segmentation accuracies on
several domains. Both Zhang et al. (2014) and
Jiang et al. (2013) work on discriminative mod-
els using the structure perceptron (Collins, 2002),
although they study two different sources of infor-
mation. In contrast to their work, we unify both
types of information under the CRF framework.
CRF has been used for Chinese word segmenta-
tion (Tseng, 2005; Shi and Wang, 2007; Zhao and
Kit, 2008; Wang et al., 2011). However, most pre-
vious work train a CRF by using full annotation
only. In contrast, we study CRF based segmenta-
tion by using both full and partial annotation.
Several other variants of CRF model has been
proposed in the machine learning literature, such
as the generalized expectation method (Mann and
McCallum, 2008), which introduce knowledge by
incorporating a manually annotated feature dis-
tribution into the regularizer, and the JESS-CM
(Suzuki and Isozaki, 2008), which use a EM-like
method to iteratively optimize the parameter on
both the annotated data and unlabeled data. In
contrast, we directly incorporate the likelihood of
partial annotation into the objective function. The
work that is the most similar to ours is Tsuboi et
al. (2008), who modify the CRF learning objec-
tive for partial data. They focus on Japanese lexi-
cal analysis using manually collected partial data,
while we investigate the effect of partial annota-
tion from freely available sources for Chinese seg-
mentation.
</bodyText>
<sectionHeader confidence="0.998484" genericHeader="conclusions">
7 Conclusion
</sectionHeader>
<bodyText confidence="0.999947727272727">
In this paper, we investigated the problem of do-
main adaptation for word segmentation, by trans-
ferring various sources of free annotations into a
consistent form of partially annotated data and ap-
plying a variant of CRF that can be trained using
fully- and partially-annotated data simultaneously.
We performed a large set of experiments to study
the effectness of free data, finding that they are
useful for improving segmentation accuracy. Ex-
periments also show that proper data selection can
further benefit the model’s performance.
</bodyText>
<page confidence="0.995813">
872
</page>
<sectionHeader confidence="0.998325" genericHeader="acknowledgments">
Acknowledgments
</sectionHeader>
<bodyText confidence="0.999703222222222">
We thank the anonymous reviewers for their con-
structive comments. This work was supported
by the National Key Basic Research Program of
China via grant 2014CB340503 and the National
Natural Science Foundation of China (NSFC) via
grant 61133012 and 61370164, the Singapore
Ministry of Education (MOE) AcRF Tier 2 grant
T2MOE201301 and SRG ISTD 2012 038 from
Singapore University of Technology and Design.
</bodyText>
<sectionHeader confidence="0.99705" genericHeader="references">
References
</sectionHeader>
<reference confidence="0.998038864583334">
Leonard E Baum and Ted Petrie. 1966. Statistical
inference for probabilistic functions of finite state
markov chains. The annals of mathematical statis-
tics, pages 1554–1563.
L´eon Bottou. 1991. Stochastic gradient learning in
neural networks. In Proceedings of Neuro-Nimes
91, Nimes, France. EC2.
Michael Collins. 2002. Discriminative training meth-
ods for hidden markov models: Theory and experi-
ments with perceptron algorithms. In Proceedings
of the 2002 Conference on Empirical Methods in
Natural Language Processing, pages 1–8. Associ-
ation for Computational Linguistics, July.
Jianfeng Gao, Mu Li, Andi Wu, and Chang-Ning
Huang. 2005. Chinese word segmentation and
named entity recognition: A pragmatic approach.
Comput. Linguist., 31(4):531–574, December.
Dan Garrette and Jason Baldridge. 2012. Type-
supervised hidden markov models for part-of-speech
tagging with incomplete tag dictionaries. In Pro-
ceedings of the 2012 Joint Conference on Empirical
Methods in Natural Language Processing and Com-
putational Natural Language Learning, pages 821–
831, Jeju Island, Korea, July. Association for Com-
putational Linguistics.
Wenbin Jiang, Liang Huang, and Qun Liu. 2009. Au-
tomatic adaptation of annotation standards: Chinese
word segmentation and pos tagging – a case study.
In Proceedings of the Joint Conference of the 47th
Annual Meeting of the ACL and the 4th International
Joint Conference on Natural Language Processing
of the AFNLP, pages 522–530, Suntec, Singapore,
August. Association for Computational Linguistics.
Wenbin Jiang, Meng Sun, Yajuan L¨u, Yating Yang, and
Qun Liu. 2013. Discriminative learning with natu-
ral annotations: Word segmentation as a case study.
In Proceedings of the 51st Annual Meeting of the As-
sociation for Computational Linguistics (Volume 1:
Long Papers), pages 761–769, Sofia, Bulgaria, Au-
gust. Association for Computational Linguistics.
John D. Lafferty, Andrew McCallum, and Fernando
C. N. Pereira. 2001. Conditional random fields:
Probabilistic models for segmenting and labeling se-
quence data. In Proceedings of the Eighteenth Inter-
national Conference on Machine Learning, ICML
’01, pages 282–289, San Francisco, CA, USA. Mor-
gan Kaufmann Publishers Inc.
Zhongguo Li and Maosong Sun. 2009. Punctuation as
implicit annotations for chinese word segmentation.
Comput. Linguist., 35(4):505–512, December.
D. C. Liu and J. Nocedal. 1989. On the limited mem-
ory bfgs method for large scale optimization. Math.
Program., 45(3):503–528, December.
Yang Liu and Yue Zhang. 2012. Unsupervised domain
adaptation for joint segmentation and POS-tagging.
In Proceedings of COLING 2012: Posters, pages
745–754, Mumbai, India, December. The COLING
2012 Organizing Committee.
Gideon S. Mann and Andrew McCallum. 2008.
Generalized expectation criteria for semi-supervised
learning of conditional random fields. In Proceed-
ings of ACL-08: HLT, pages 870–878, Columbus,
Ohio, June. Association for Computational Linguis-
tics.
Yanxin Shi and Mengqiu Wang. 2007. A dual-layer
crfs based joint decoding method for cascaded seg-
mentation and labeling tasks. In Proceedings of the
20th International Joint Conference on Artifical In-
telligence, IJCAI’07, pages 1707–1712, San Fran-
cisco, CA, USA. Morgan Kaufmann Publishers Inc.
Yan Song, Prescott Klassen, Fei Xia, and Chunyu Kit.
2012. Entropy-based training data selection for do-
main adaptation. In Proceedings of COLING 2012:
Posters, pages 1191–1200, Mumbai, India, Decem-
ber. The COLING 2012 Organizing Committee.
Weiwei Sun and Jia Xu. 2011. Enhancing chinese
word segmentation using unlabeled data. In Pro-
ceedings of the 2011 Conference on Empirical Meth-
ods in Natural Language Processing, pages 970–
979, Edinburgh, Scotland, UK., July. Association for
Computational Linguistics.
Jun Suzuki and Hideki Isozaki. 2008. Semi-supervised
sequential labeling and segmentation using giga-
word scale unlabeled data. In Proceedings of ACL-
08: HLT, pages 665–673, Columbus, Ohio, June.
Association for Computational Linguistics.
Huihsin Tseng. 2005. A conditional random field word
segmenter. In In Fourth SIGHAN Workshop on Chi-
nese Language Processing.
Yuta Tsuboi, Hisashi Kashima, Shinsuke Mori, Hiroki
Oda, and Yuji Matsumoto. 2008. Training condi-
tional random fields using incomplete annotations.
In Proceedings of the 22nd International Conference
on Computational Linguistics (Coling 2008), pages
897–904, Manchester, UK, August. Coling 2008 Or-
ganizing Committee.
</reference>
<page confidence="0.989993">
873
</page>
<reference confidence="0.999783941176471">
Yiou Wang, Jun’ichi Kazama, Yoshimasa Tsuruoka,
Wenliang Chen, Yujie Zhang, and Kentaro Tori-
sawa. 2011. Improving chinese word segmentation
and pos tagging with semi-supervised methods using
large auto-analyzed data. In Proceedings of 5th In-
ternational Joint Conference on Natural Language
Processing, pages 309–317, Chiang Mai, Thailand,
November. Asian Federation of Natural Language
Processing.
Pak-kwong Wong and Chorkin Chan. 1996. Chinese
word segmentation based on maximum matching
and word binding force. In Proceedings of the 16th
Conference on Computational Linguistics - Volume
1, COLING ’96, pages 200–203, Stroudsburg, PA,
USA. Association for Computational Linguistics.
Zimin Wu and Gwyneth Tseng. 1993. Chinese text
segmentation for text retrieval: Achievements and
problems. J. Am. Soc. Inf. Sci., 44(9):532–542, Oc-
tober.
Nianwen Xue and Libin Shen. 2003. Chinese word
segmentation as lmr tagging. In Proceedings of the
Second SIGHAN Workshop on Chinese Language
Processing - Volume 17, SIGHAN ’03, pages 176–
179, Stroudsburg, PA, USA. Association for Com-
putational Linguistics.
Shiwen Yu, Jianming Lu, Xuefeng Zhu, Huiming
Duan, Shiyong Kang, Honglin Sun, Hui Wang,
Qiang Zhao, and Weidong Zhan. 2001. Processing
norms of modern chinese corpus. Technical report.
Yue Zhang and Stephen Clark. 2007. Chinese segmen-
tation with a word-based perceptron algorithm. In
Proceedings of the 45th Annual Meeting of the As-
sociation of Computational Linguistics, pages 840–
847, Prague, Czech Republic, June. Association for
Computational Linguistics.
Meishan Zhang, Yue Zhang, Wanxiang Che, and Ting
Liu. 2014. Type-supervised domain adaptation for
joint segmentation and pos-tagging. In Proceed-
ings of the 14th Conference of the European Chap-
ter of the Association for Computational Linguistics,
pages 588–597, Gothenburg, Sweden, April. Asso-
ciation for Computational Linguistics.
Hai Zhao and Chunyu Kit. 2008. An empirical com-
parison of goodness measures for unsupervised chi-
nese word segmentation with a unified framework.
In In: The Third International Joint Conference on
Natural Language Processing (IJCNLP-2008.
Hai Zhao, Chang-Ning Huang, Mu Li, and Bao-Liang
Lu. 2010. A unified character-based tagging frame-
work for chinese word segmentation. 9(2):5:1–5:32,
June.
</reference>
<page confidence="0.998761">
874
</page>
</variant>
</algorithm>
<algorithm name="ParsHed" version="110505">
<variant no="0" confidence="0.647878">
<title confidence="0.921875">Domain Adaptation for CRF-based Chinese Word Segmentation Free Annotations</title>
<author confidence="0.968375">Liu Yue Zhang Wanxiang Che Ting Liu Fan Wu University of Technology</author>
<affiliation confidence="0.999162">Center for Social Computing and Information Harbin Institute of Technology,</affiliation>
<email confidence="0.923271">zhang,fan</email>
<abstract confidence="0.9919175">Supervised methods have been the dominant approach for Chinese word segmentation. The performance can drop significantly when the test domain is different from the training domain. In this paper, we study the problem of obtaining partial annotation from freely available data to help Chinese word segmentation on different domains. Different sources of free annotations are transformed into a unified form of partial annotation and a variant CRF model is used to leverage both fully and partially annotated data consistently. Experimental results show that the Chinese word segmentation model benefits from free partially annotated data. On the SIGHAN Bakeoff 2010 data, we achieve results that are competitive to the best reported in the literature.</abstract>
</variant>
</algorithm>
<algorithm name="ParsCit" version="110505">
<citationList>
<citation valid="true">
<authors>
<author>Leonard E Baum</author>
<author>Ted Petrie</author>
</authors>
<title>Statistical inference for probabilistic functions of finite state markov chains. The annals of mathematical statistics,</title>
<date>1966</date>
<pages>1554--1563</pages>
<contexts>
<context position="14783" citStr="Baum and Petrie, 1966" startWordPosition="2477" endWordPosition="2480">+ 3) bigram CsCs+1 (i − 3 &lt; s &lt; i + 2) CsCs+2 (i − 3 &lt; s &lt; i + 1) type Type(Ci) Type(Cs)T ype(Cs+1) (i − 1 &lt; s &lt; i + 2) identical Identical(Cs, Cs+1) (i − 3 &lt; s &lt; i + 1) Identical(Cs,Cs+2) (i − 3 &lt; s &lt; i) Table 2: Feature templates for the ith character. For fully-annotated training data, the learning problem of conditional random fields is to maximize the log likelihood over all the training data (Lafferty et al., 2001) log p(y(n)|x(n)) Here N is the number of training sentences. Both the likelihood p(y(n)|x(n)) and its gradient can be calculated by performing the forward-backward algorithm (Baum and Petrie, 1966) on the sequence, and several optimization algorithm can be adopted to learn parameters from data, including L-BFGS (Liu and Nocedal, 1989) and SGD (Bottou, 1991). 4 Training a CRF with partially annotated data For word segmentation with partially annotated data, some characters in a sentence can have a definite segmentation label, while some can have multiple labels with ambiguities remaining. Taking the partially annotated sentence in Figure 2a for example, the corresponding potential label sequence for “在狐岐山救” is {(e, s), (b), (m), (e), (b, s)}, where the characters “狐”, “岐” and “山” have fi</context>
</contexts>
<marker>Baum, Petrie, 1966</marker>
<rawString>Leonard E Baum and Ted Petrie. 1966. Statistical inference for probabilistic functions of finite state markov chains. The annals of mathematical statistics, pages 1554–1563.</rawString>
</citation>
<citation valid="true">
<authors>
<author>L´eon Bottou</author>
</authors>
<title>Stochastic gradient learning in neural networks.</title>
<date>1991</date>
<booktitle>In Proceedings of Neuro-Nimes 91,</booktitle>
<location>Nimes,</location>
<contexts>
<context position="14945" citStr="Bottou, 1991" startWordPosition="2505" endWordPosition="2507">dentical(Cs,Cs+2) (i − 3 &lt; s &lt; i) Table 2: Feature templates for the ith character. For fully-annotated training data, the learning problem of conditional random fields is to maximize the log likelihood over all the training data (Lafferty et al., 2001) log p(y(n)|x(n)) Here N is the number of training sentences. Both the likelihood p(y(n)|x(n)) and its gradient can be calculated by performing the forward-backward algorithm (Baum and Petrie, 1966) on the sequence, and several optimization algorithm can be adopted to learn parameters from data, including L-BFGS (Liu and Nocedal, 1989) and SGD (Bottou, 1991). 4 Training a CRF with partially annotated data For word segmentation with partially annotated data, some characters in a sentence can have a definite segmentation label, while some can have multiple labels with ambiguities remaining. Taking the partially annotated sentence in Figure 2a for example, the corresponding potential label sequence for “在狐岐山救” is {(e, s), (b), (m), (e), (b, s)}, where the characters “狐”, “岐” and “山” have fixed labels but for “在” and “救”, some ambiguities exist. Note that the full annotation in Figure 1 can be regarded as a special case of partial annotation, where t</context>
</contexts>
<marker>Bottou, 1991</marker>
<rawString>L´eon Bottou. 1991. Stochastic gradient learning in neural networks. In Proceedings of Neuro-Nimes 91, Nimes, France. EC2.</rawString>
</citation>
<citation valid="true">
<authors>
<author>Michael Collins</author>
</authors>
<title>Discriminative training methods for hidden markov models: Theory and experiments with perceptron algorithms.</title>
<date>2002</date>
<booktitle>In Proceedings of the 2002 Conference on Empirical Methods in Natural Language Processing,</booktitle>
<pages>1--8</pages>
<publisher>Association</publisher>
<institution>for Computational Linguistics,</institution>
<contexts>
<context position="32160" citStr="Collins, 2002" startWordPosition="5373" endWordPosition="5374">main difference into two types: different vocabulary and different POS distributions. While the first type of difference can be effectively resolved by using lexicon for each domain, the second type of difference needs to be resolved by using annotated sentences. They found that given the same manual annotation time, a combination of the lexicon and sentence is the most effective. Jiang et al. (2013) use 160K Wikipedia sentences to improves segmentation accuracies on several domains. Both Zhang et al. (2014) and Jiang et al. (2013) work on discriminative models using the structure perceptron (Collins, 2002), although they study two different sources of information. In contrast to their work, we unify both types of information under the CRF framework. CRF has been used for Chinese word segmentation (Tseng, 2005; Shi and Wang, 2007; Zhao and Kit, 2008; Wang et al., 2011). However, most previous work train a CRF by using full annotation only. In contrast, we study CRF based segmentation by using both full and partial annotation. Several other variants of CRF model has been proposed in the machine learning literature, such as the generalized expectation method (Mann and McCallum, 2008), which introd</context>
</contexts>
<marker>Collins, 2002</marker>
<rawString>Michael Collins. 2002. Discriminative training methods for hidden markov models: Theory and experiments with perceptron algorithms. In Proceedings of the 2002 Conference on Empirical Methods in Natural Language Processing, pages 1–8. Association for Computational Linguistics, July.</rawString>
</citation>
<citation valid="true">
<authors>
<author>Jianfeng Gao</author>
<author>Mu Li</author>
<author>Andi Wu</author>
<author>Chang-Ning Huang</author>
</authors>
<title>Chinese word segmentation and named entity recognition: A pragmatic approach.</title>
<date>2005</date>
<journal>Comput. Linguist.,</journal>
<volume>31</volume>
<issue>4</issue>
<marker>Gao, Li, Wu, Huang, 2005</marker>
<rawString>Jianfeng Gao, Mu Li, Andi Wu, and Chang-Ning Huang. 2005. Chinese word segmentation and named entity recognition: A pragmatic approach. Comput. Linguist., 31(4):531–574, December.</rawString>
</citation>
<citation valid="true">
<authors>
<author>Dan Garrette</author>
<author>Jason Baldridge</author>
</authors>
<title>Typesupervised hidden markov models for part-of-speech tagging with incomplete tag dictionaries.</title>
<date>2012</date>
<booktitle>In Proceedings of the 2012 Joint Conference on Empirical Methods in Natural Language Processing and Computational Natural Language Learning,</booktitle>
<pages>821--831</pages>
<institution>Jeju Island, Korea, July. Association for Computational Linguistics.</institution>
<contexts>
<context position="2445" citStr="Garrette and Baldridge, 2012" startWordPosition="364" endWordPosition="367">-supervised methods leverage manually assembled domain-specific lexicons to improve target-domain segmentation accuracies. Both Figure 1: The segmentation problem, illustrated using the sentence “浦东 (Pudong) 开发 (development) 与 (and) 法制 (legal) 建设 (construction)”. Possible segmentation labels are drawn under each character, where b, m, e, s stand for the beginning, middle, end of a multi-character word, and a single character word, respectively. The path shows the correct segmentation by choosing one label for each character. methods are competitive given the same amount of annotation effects (Garrette and Baldridge, 2012; Zhang et al., 2014). However, obtaining manually annotated data can be expensive. On the other hand, there are free data which contain limited but useful segmentation information over the Internet, including large-scale unlabeled data, domain-specific lexicons and semiannotated web pages such as Wikipedia. In the last case, word-boundary information is contained in hyperlinks and other markup annotations. Such free data offer a useful alternative for improving the segmentation performance, especially on domains that are not identical to newswire, and for which little annotation is available.</context>
</contexts>
<marker>Garrette, Baldridge, 2012</marker>
<rawString>Dan Garrette and Jason Baldridge. 2012. Typesupervised hidden markov models for part-of-speech tagging with incomplete tag dictionaries. In Proceedings of the 2012 Joint Conference on Empirical Methods in Natural Language Processing and Computational Natural Language Learning, pages 821– 831, Jeju Island, Korea, July. Association for Computational Linguistics.</rawString>
</citation>
<citation valid="true">
<authors>
<author>Wenbin Jiang</author>
<author>Liang Huang</author>
<author>Qun Liu</author>
</authors>
<title>Automatic adaptation of annotation standards: Chinese word segmentation and pos tagging – a case study.</title>
<date>2009</date>
<booktitle>In Proceedings of the Joint Conference of the 47th Annual Meeting of the ACL and the 4th International Joint Conference on Natural Language Processing of the AFNLP,</booktitle>
<pages>522--530</pages>
<publisher>Association</publisher>
<institution>for Computational Linguistics.</institution>
<location>Suntec, Singapore,</location>
<contexts>
<context position="1264" citStr="Jiang et al., 2009" startWordPosition="187" endWordPosition="190">p Chinese word segmentation on different domains. Different sources of free annotations are transformed into a unified form of partial annotation and a variant CRF model is used to leverage both fully and partially annotated data consistently. Experimental results show that the Chinese word segmentation model benefits from free partially annotated data. On the SIGHAN Bakeoff 2010 data, we achieve results that are competitive to the best reported in the literature. 1 Introduction Statistical Chinese word segmentation gains high accuracies on newswire (Xue and Shen, 2003; Zhang and Clark, 2007; Jiang et al., 2009; Zhao et al., 2010; Sun and Xu, 2011). However, manually annotated training data mostly come from the news domain, and the performance can drop severely when the test data shift from newswire to blogs, computer forums and Internet literature (Liu and Zhang, 2012). Several methods have been proposed for solving the domain adaptation problem for segmentation, which include the traditional token- and typesupervised methods (Song et al., 2012; Zhang et al., 2014). While token-supervised methods rely on manually annotated target-domain sentences, type-supervised methods leverage manually assembled</context>
<context position="10987" citStr="Jiang et al. (2009)" startWordPosition="1822" endWordPosition="1825">cognized as word by multiple matching results, it can be considered as a more precise annotation. Our algorithm reads partial segmentation by different methods and selects the subsequences that are identified as word by all methods as annotated words. 2.2 Natural Annotation We use the Chinese Wikipedia for natural annotation. Partially annotated sentences are readily formed in Wikipedia by markup syntax, such as URLs. However, some subtle issues exist if the sentences are used directly. One problem is incompatibility of segmentation standards between the annotated training data and Wikipedia. Jiang et al. (2009) discuss this incompatibility problem between two corpora — the CTB and the People’s Daily; the problem is even more severe on Wikipedia because it can be edited by any user. Table 1a shows a case of incompatible annotation between the People’s Daily data and natural annotation in Wikipedia, where the three characters “���” are segmented differently. Both can be treated as correct, although they have different segmentation granularities. Another problem is the intrinsic ambiguity of segmentation. The same character sequence can be segmented into different words under different contexts. If the</context>
</contexts>
<marker>Jiang, Huang, Liu, 2009</marker>
<rawString>Wenbin Jiang, Liang Huang, and Qun Liu. 2009. Automatic adaptation of annotation standards: Chinese word segmentation and pos tagging – a case study. In Proceedings of the Joint Conference of the 47th Annual Meeting of the ACL and the 4th International Joint Conference on Natural Language Processing of the AFNLP, pages 522–530, Suntec, Singapore, August. Association for Computational Linguistics.</rawString>
</citation>
<citation valid="true">
<authors>
<author>Wenbin Jiang</author>
<author>Meng Sun</author>
<author>Yajuan L¨u</author>
<author>Yating Yang</author>
<author>Qun Liu</author>
</authors>
<title>Discriminative learning with natural annotations: Word segmentation as a case study.</title>
<date>2013</date>
<booktitle>In Proceedings of the 51st Annual Meeting of the Association for Computational Linguistics (Volume 1: Long Papers),</booktitle>
<pages>761--769</pages>
<publisher>Association</publisher>
<institution>for Computational Linguistics.</institution>
<location>Sofia, Bulgaria,</location>
<marker>Jiang, Sun, L¨u, Yang, Liu, 2013</marker>
<rawString>Wenbin Jiang, Meng Sun, Yajuan L¨u, Yating Yang, and Qun Liu. 2013. Discriminative learning with natural annotations: Word segmentation as a case study. In Proceedings of the 51st Annual Meeting of the Association for Computational Linguistics (Volume 1: Long Papers), pages 761–769, Sofia, Bulgaria, August. Association for Computational Linguistics.</rawString>
</citation>
<citation valid="true">
<authors>
<author>John D Lafferty</author>
<author>Andrew McCallum</author>
<author>Fernando C N Pereira</author>
</authors>
<title>Conditional random fields: Probabilistic models for segmenting and labeling sequence data.</title>
<date>2001</date>
<booktitle>In Proceedings of the Eighteenth International Conference on Machine Learning, ICML ’01,</booktitle>
<pages>282--289</pages>
<publisher>Morgan Kaufmann Publishers Inc.</publisher>
<location>San Francisco, CA, USA.</location>
<contexts>
<context position="3955" citStr="Lafferty et al., 2001" startWordPosition="627" endWordPosition="630">pedia. There has been work on making use of both unlabeled data (Sun and Xu, 2011; Wang et al., 2011) and Wikipedia (Jiang et al., 2013) 浦 东 开 发 与 法 制 建 设 b b b b b b b b b m m m m m m m m m e e e e e e e e e s s s s s s s s s 864 Proceedings of the 2014 Conference on Empirical Methods in Natural Language Processing (EMNLP), pages 864–874, October 25-29, 2014, Doha, Qatar. c�2014 Association for Computational Linguistics to improve segmentation. However, no empirical results have been reported on a unified approach to deal with different types of free data. We use a conditional random fields (Lafferty et al., 2001; Tsuboi et al., 2008) variant that can leverage the partial annotations obtained from different sources of free annotation. Training is achieved by a modification to the learning objective, incorporating partial annotation likelihood, so that a single model can be trained consistently with a mixture of full and partial annotation. Experimental results show that our method of using partially annotated data can consistently improves cross-domain segmentation performance. We obtain results which are competitive to the best reported in the literature. Our segmentor is freely released at https://g</context>
<context position="13077" citStr="Lafferty et al., 2001" startWordPosition="2162" endWordPosition="2165"> We assume that URLtagged entries reflect the segmentation standards of Wikipedia sentence, and use them to match Wikipedia sentences with the raw target domain data. If the character sequence of any URL-tagged entry in a Wikipedia sentence matches the target domain data, the Wikipedia sentence is selected for training. Another advantage of such data selection is that the training time consumption can be reduced by reducing the size of training data. 3 CRF for Word Segmentation We follow the work of Zhao et al. (2010) and Sun and Xu (2011), and adopt the Conditional Random Fields (CRF) model (Lafferty et al., 2001) for the sequence labeling problem of word segmentation. Given an input characters sequence, the task is to assign one segmentation label from {b, m, e, s} on each character. Let x = (x1,x2, ..., xT) be the sequence of characters in sentence whose length is T, and y = (y1, y2, ..., yT) be the corresponding label sequence, where yi E Y . The linearchain conditional random field for Chinese word segmentation can be formalized as ∑T 1 p(y|x) = Z exp t=1 where Ak are the model parameters, fk are the feature functions and Z is the probability normalizer. Akfk(yt, yt−1, x) (2) We follow Sun and Xu (</context>
<context position="14585" citStr="Lafferty et al., 2001" startWordPosition="2448" endWordPosition="2451">ates whether the input character is the same with its surrounding characters. This feature captures repetition patterns such as “试 试 (try)” or “走走 (stroll)”. Type Template unigram Cs (i − 3 &lt; s &lt; i + 3) bigram CsCs+1 (i − 3 &lt; s &lt; i + 2) CsCs+2 (i − 3 &lt; s &lt; i + 1) type Type(Ci) Type(Cs)T ype(Cs+1) (i − 1 &lt; s &lt; i + 2) identical Identical(Cs, Cs+1) (i − 3 &lt; s &lt; i + 1) Identical(Cs,Cs+2) (i − 3 &lt; s &lt; i) Table 2: Feature templates for the ith character. For fully-annotated training data, the learning problem of conditional random fields is to maximize the log likelihood over all the training data (Lafferty et al., 2001) log p(y(n)|x(n)) Here N is the number of training sentences. Both the likelihood p(y(n)|x(n)) and its gradient can be calculated by performing the forward-backward algorithm (Baum and Petrie, 1966) on the sequence, and several optimization algorithm can be adopted to learn parameters from data, including L-BFGS (Liu and Nocedal, 1989) and SGD (Bottou, 1991). 4 Training a CRF with partially annotated data For word segmentation with partially annotated data, some characters in a sentence can have a definite segmentation label, while some can have multiple labels with ambiguities remaining. Taki</context>
</contexts>
<marker>Lafferty, McCallum, Pereira, 2001</marker>
<rawString>John D. Lafferty, Andrew McCallum, and Fernando C. N. Pereira. 2001. Conditional random fields: Probabilistic models for segmenting and labeling sequence data. In Proceedings of the Eighteenth International Conference on Machine Learning, ICML ’01, pages 282–289, San Francisco, CA, USA. Morgan Kaufmann Publishers Inc.</rawString>
</citation>
<citation valid="true">
<authors>
<author>Zhongguo Li</author>
<author>Maosong Sun</author>
</authors>
<title>Punctuation as implicit annotations for chinese word segmentation.</title>
<date>2009</date>
<journal>Comput. Linguist.,</journal>
<volume>35</volume>
<issue>4</issue>
<contexts>
<context position="30151" citStr="Li and Sun (2009)" startWordPosition="5050" endWordPosition="5053">rces by joining the partially annotated sentences derived using each resource, training our CRF model with these partially annotated sentences and the fully annotated PD sentences. The tests are performed on medicine and computer domains. Table 7 shows the results, where further improvements are made on both domains when the two types of resources are combined. 6 Related Work There has been a line of research on making use of unlabeled data for word segmentation. Zhao and Kit (2008) improve segmentation performance by mutual information between characters, collected from large unlabeled data; Li and Sun (2009) use punctuation information in a large raw corpus to learn a segmentation model, and achieve better recognition of OOV words; Sun and Xu (2011) explore several statistical features derived from unlabeled data to help improve character-based word segmentation. These investigations mainly focus on in-domain accuracies. Liu and Zhang (2012) Method Baseline Baseline+PA (Lex) Baseline+PA (Natural) Baseline+PA (Lex+Natural) 91.36 91.68 92.47 92.63 Med. F 93.16 93.47 93.93 94.07 Com. F 871 0.9525 0.8775 0.9522 0.8750 0.9519 0.8725 0.9516 0.8700 0 50 100 150 (a) Finance 0.9225 0.78 0.9200 0.77 0.9175</context>
</contexts>
<marker>Li, Sun, 2009</marker>
<rawString>Zhongguo Li and Maosong Sun. 2009. Punctuation as implicit annotations for chinese word segmentation. Comput. Linguist., 35(4):505–512, December.</rawString>
</citation>
<citation valid="true">
<authors>
<author>D C Liu</author>
<author>J Nocedal</author>
</authors>
<title>On the limited memory bfgs method for large scale optimization.</title>
<date>1989</date>
<journal>Math. Program.,</journal>
<volume>45</volume>
<issue>3</issue>
<contexts>
<context position="14922" citStr="Liu and Nocedal, 1989" startWordPosition="2499" endWordPosition="2502">(Cs, Cs+1) (i − 3 &lt; s &lt; i + 1) Identical(Cs,Cs+2) (i − 3 &lt; s &lt; i) Table 2: Feature templates for the ith character. For fully-annotated training data, the learning problem of conditional random fields is to maximize the log likelihood over all the training data (Lafferty et al., 2001) log p(y(n)|x(n)) Here N is the number of training sentences. Both the likelihood p(y(n)|x(n)) and its gradient can be calculated by performing the forward-backward algorithm (Baum and Petrie, 1966) on the sequence, and several optimization algorithm can be adopted to learn parameters from data, including L-BFGS (Liu and Nocedal, 1989) and SGD (Bottou, 1991). 4 Training a CRF with partially annotated data For word segmentation with partially annotated data, some characters in a sentence can have a definite segmentation label, while some can have multiple labels with ambiguities remaining. Taking the partially annotated sentence in Figure 2a for example, the corresponding potential label sequence for “在狐岐山救” is {(e, s), (b), (m), (e), (b, s)}, where the characters “狐”, “岐” and “山” have fixed labels but for “在” and “救”, some ambiguities exist. Note that the full annotation in Figure 1 can be regarded as a special case of part</context>
<context position="17444" citStr="Liu and Nocedal, 1989" startWordPosition="2966" endWordPosition="2969">, i, x) is a potential function that equals Ek λkfk(yt = j, yt−1 = i, xt). Similarly, for the backward variable βYL,t, { //�� 0 i E/Lt NYL,t(i) = ∑9ELt+1 Ψt(j, i, xt+1)βYL,t+1(j) i E Lt ZYL can be calculated by αYL(T), and pYL(y, y′|x) can be calculated by αYL,t−1(y′)Ψt(y, y′, xt)βYL,t(y). Note that if each element in YL is constrained to one single label, the CRF model in Equation 3 degrades into Equation 1. So we can train a unified model with both fully and partially annotated data. We implement this CRF model based on a open source toolkit CRFSuite.1 In our experiments, we use the L-BFGS (Liu and Nocedal, 1989) algorithm to learn parameters from both fully and partially annotated data. 5 Experiments We perform our experiments on the domain adaptation test data from SIGHAN Bakeoff 2010 (Zhao et al., 2010), adapting annotated training sentences from People’s Daily (PD) (Yu et al., 2001) to different test domains. The fully annotated data is selected from the People’s Daily newspaper in January of 1998, and the four test domains from the SIGHAN Bakeoff 2010 include finance, medicine, literature and computer. Sample segmented data in the computer domain from this bakeoff is used as development set. Stat</context>
</contexts>
<marker>Liu, Nocedal, 1989</marker>
<rawString>D. C. Liu and J. Nocedal. 1989. On the limited memory bfgs method for large scale optimization. Math. Program., 45(3):503–528, December.</rawString>
</citation>
<citation valid="true">
<authors>
<author>Yang Liu</author>
<author>Yue Zhang</author>
</authors>
<title>Unsupervised domain adaptation for joint segmentation and POS-tagging.</title>
<date>2012</date>
<booktitle>In Proceedings of COLING 2012: Posters,</booktitle>
<pages>745--754</pages>
<location>Mumbai, India,</location>
<contexts>
<context position="1528" citStr="Liu and Zhang, 2012" startWordPosition="231" endWordPosition="234">ts show that the Chinese word segmentation model benefits from free partially annotated data. On the SIGHAN Bakeoff 2010 data, we achieve results that are competitive to the best reported in the literature. 1 Introduction Statistical Chinese word segmentation gains high accuracies on newswire (Xue and Shen, 2003; Zhang and Clark, 2007; Jiang et al., 2009; Zhao et al., 2010; Sun and Xu, 2011). However, manually annotated training data mostly come from the news domain, and the performance can drop severely when the test data shift from newswire to blogs, computer forums and Internet literature (Liu and Zhang, 2012). Several methods have been proposed for solving the domain adaptation problem for segmentation, which include the traditional token- and typesupervised methods (Song et al., 2012; Zhang et al., 2014). While token-supervised methods rely on manually annotated target-domain sentences, type-supervised methods leverage manually assembled domain-specific lexicons to improve target-domain segmentation accuracies. Both Figure 1: The segmentation problem, illustrated using the sentence “浦东 (Pudong) 开发 (development) 与 (and) 法制 (legal) 建设 (construction)”. Possible segmentation labels are drawn under ea</context>
<context position="18813" citStr="Liu and Zhang (2012)" startWordPosition="3190" endWordPosition="3193">a are converted to simplified Chinese. After filtering functional pages like redirection and removing duplication, 5.45 million sentences are reserved. For comparison with related work on using a lexicon to improve segmentation, another set of test data is chosen for this setting. We use the Chinese Treebank (CTB) as the source domain data, and Zhuxian (a free Internet novel, also named as “Jade dynasty”, referred to as ZX henceforth) as the target domain data.3 The ZX data are written in a different style from newswire, and contains many out-of-vocabulary words. This setting has been used by Liu and Zhang (2012) and Zhang et al. (2014) for domain adaptation of segmentation and POS-tagging. We use the standard training, development and test split. Statistics of the test data annotated by Zhang et al. (2014) are shown in the second half of Table 3. The data preparation method in Section 2 and the CRF method in Section 4 are used for all the experiments. Both recall of out-of-vocabulary words (Roov) and F-score are used to evaluate the 1http://www.chokkan.org/software/ crfsuite/ 2http://dumps.wikimedia.org/zhwiki/ 20140419/ 3Annotated target domain test data and lexicon are available from http://ir.hit.</context>
<context position="30491" citStr="Liu and Zhang (2012)" startWordPosition="5101" endWordPosition="5104">of resources are combined. 6 Related Work There has been a line of research on making use of unlabeled data for word segmentation. Zhao and Kit (2008) improve segmentation performance by mutual information between characters, collected from large unlabeled data; Li and Sun (2009) use punctuation information in a large raw corpus to learn a segmentation model, and achieve better recognition of OOV words; Sun and Xu (2011) explore several statistical features derived from unlabeled data to help improve character-based word segmentation. These investigations mainly focus on in-domain accuracies. Liu and Zhang (2012) Method Baseline Baseline+PA (Lex) Baseline+PA (Natural) Baseline+PA (Lex+Natural) 91.36 91.68 92.47 92.63 Med. F 93.16 93.47 93.93 94.07 Com. F 871 0.9525 0.8775 0.9522 0.8750 0.9519 0.8725 0.9516 0.8700 0 50 100 150 (a) Finance 0.9225 0.78 0.9200 0.77 0.9175 0.76 0.9150 0.75 0.74 0.73 0 50 100 150 (b) Medicine 0.9225 0.750 0.938 0.87 0.9222 0.745 0.936 0.86 0.9219 0.740 0.934 0.85 0.9216 0.735 0.84 0 50 100 150 0 50 100 150 (c) Literature (d) Computer Figure 4: Performance of the model incorporating difference sizes of Wikipedia data. The solid line represents the F-score and dashed line rep</context>
</contexts>
<marker>Liu, Zhang, 2012</marker>
<rawString>Yang Liu and Yue Zhang. 2012. Unsupervised domain adaptation for joint segmentation and POS-tagging. In Proceedings of COLING 2012: Posters, pages 745–754, Mumbai, India, December. The COLING 2012 Organizing Committee.</rawString>
</citation>
<citation valid="true">
<authors>
<author>Gideon S Mann</author>
<author>Andrew McCallum</author>
</authors>
<title>Generalized expectation criteria for semi-supervised learning of conditional random fields.</title>
<date>2008</date>
<booktitle>In Proceedings of ACL-08: HLT,</booktitle>
<pages>870--878</pages>
<publisher>Association</publisher>
<institution>for Computational Linguistics.</institution>
<location>Columbus, Ohio,</location>
<contexts>
<context position="32746" citStr="Mann and McCallum, 2008" startWordPosition="5470" endWordPosition="5473">e structure perceptron (Collins, 2002), although they study two different sources of information. In contrast to their work, we unify both types of information under the CRF framework. CRF has been used for Chinese word segmentation (Tseng, 2005; Shi and Wang, 2007; Zhao and Kit, 2008; Wang et al., 2011). However, most previous work train a CRF by using full annotation only. In contrast, we study CRF based segmentation by using both full and partial annotation. Several other variants of CRF model has been proposed in the machine learning literature, such as the generalized expectation method (Mann and McCallum, 2008), which introduce knowledge by incorporating a manually annotated feature distribution into the regularizer, and the JESS-CM (Suzuki and Isozaki, 2008), which use a EM-like method to iteratively optimize the parameter on both the annotated data and unlabeled data. In contrast, we directly incorporate the likelihood of partial annotation into the objective function. The work that is the most similar to ours is Tsuboi et al. (2008), who modify the CRF learning objective for partial data. They focus on Japanese lexical analysis using manually collected partial data, while we investigate the effec</context>
</contexts>
<marker>Mann, McCallum, 2008</marker>
<rawString>Gideon S. Mann and Andrew McCallum. 2008. Generalized expectation criteria for semi-supervised learning of conditional random fields. In Proceedings of ACL-08: HLT, pages 870–878, Columbus, Ohio, June. Association for Computational Linguistics.</rawString>
</citation>
<citation valid="true">
<authors>
<author>Yanxin Shi</author>
<author>Mengqiu Wang</author>
</authors>
<title>A dual-layer crfs based joint decoding method for cascaded segmentation and labeling tasks.</title>
<date>2007</date>
<booktitle>In Proceedings of the 20th International Joint Conference on Artifical Intelligence, IJCAI’07,</booktitle>
<pages>1707--1712</pages>
<publisher>Morgan Kaufmann Publishers Inc.</publisher>
<location>San Francisco, CA, USA.</location>
<contexts>
<context position="32387" citStr="Shi and Wang, 2007" startWordPosition="5410" endWordPosition="5413">e resolved by using annotated sentences. They found that given the same manual annotation time, a combination of the lexicon and sentence is the most effective. Jiang et al. (2013) use 160K Wikipedia sentences to improves segmentation accuracies on several domains. Both Zhang et al. (2014) and Jiang et al. (2013) work on discriminative models using the structure perceptron (Collins, 2002), although they study two different sources of information. In contrast to their work, we unify both types of information under the CRF framework. CRF has been used for Chinese word segmentation (Tseng, 2005; Shi and Wang, 2007; Zhao and Kit, 2008; Wang et al., 2011). However, most previous work train a CRF by using full annotation only. In contrast, we study CRF based segmentation by using both full and partial annotation. Several other variants of CRF model has been proposed in the machine learning literature, such as the generalized expectation method (Mann and McCallum, 2008), which introduce knowledge by incorporating a manually annotated feature distribution into the regularizer, and the JESS-CM (Suzuki and Isozaki, 2008), which use a EM-like method to iteratively optimize the parameter on both the annotated d</context>
</contexts>
<marker>Shi, Wang, 2007</marker>
<rawString>Yanxin Shi and Mengqiu Wang. 2007. A dual-layer crfs based joint decoding method for cascaded segmentation and labeling tasks. In Proceedings of the 20th International Joint Conference on Artifical Intelligence, IJCAI’07, pages 1707–1712, San Francisco, CA, USA. Morgan Kaufmann Publishers Inc.</rawString>
</citation>
<citation valid="true">
<authors>
<author>Yan Song</author>
<author>Prescott Klassen</author>
<author>Fei Xia</author>
<author>Chunyu Kit</author>
</authors>
<title>Entropy-based training data selection for domain adaptation.</title>
<date>2012</date>
<booktitle>In Proceedings of COLING 2012: Posters,</booktitle>
<pages>1191--1200</pages>
<location>Mumbai, India,</location>
<contexts>
<context position="1707" citStr="Song et al., 2012" startWordPosition="259" endWordPosition="262">orted in the literature. 1 Introduction Statistical Chinese word segmentation gains high accuracies on newswire (Xue and Shen, 2003; Zhang and Clark, 2007; Jiang et al., 2009; Zhao et al., 2010; Sun and Xu, 2011). However, manually annotated training data mostly come from the news domain, and the performance can drop severely when the test data shift from newswire to blogs, computer forums and Internet literature (Liu and Zhang, 2012). Several methods have been proposed for solving the domain adaptation problem for segmentation, which include the traditional token- and typesupervised methods (Song et al., 2012; Zhang et al., 2014). While token-supervised methods rely on manually annotated target-domain sentences, type-supervised methods leverage manually assembled domain-specific lexicons to improve target-domain segmentation accuracies. Both Figure 1: The segmentation problem, illustrated using the sentence “浦东 (Pudong) 开发 (development) 与 (and) 法制 (legal) 建设 (construction)”. Possible segmentation labels are drawn under each character, where b, m, e, s stand for the beginning, middle, end of a multi-character word, and a single character word, respectively. The path shows the correct segmentation b</context>
</contexts>
<marker>Song, Klassen, Xia, Kit, 2012</marker>
<rawString>Yan Song, Prescott Klassen, Fei Xia, and Chunyu Kit. 2012. Entropy-based training data selection for domain adaptation. In Proceedings of COLING 2012: Posters, pages 1191–1200, Mumbai, India, December. The COLING 2012 Organizing Committee.</rawString>
</citation>
<citation valid="true">
<authors>
<author>Weiwei Sun</author>
<author>Jia Xu</author>
</authors>
<title>Enhancing chinese word segmentation using unlabeled data.</title>
<date>2011</date>
<booktitle>In Proceedings of the 2011 Conference on Empirical Methods in Natural Language Processing,</booktitle>
<pages>970--979</pages>
<publisher>Association</publisher>
<institution>for Computational Linguistics.</institution>
<location>Edinburgh, Scotland, UK.,</location>
<contexts>
<context position="1302" citStr="Sun and Xu, 2011" startWordPosition="195" endWordPosition="198">t domains. Different sources of free annotations are transformed into a unified form of partial annotation and a variant CRF model is used to leverage both fully and partially annotated data consistently. Experimental results show that the Chinese word segmentation model benefits from free partially annotated data. On the SIGHAN Bakeoff 2010 data, we achieve results that are competitive to the best reported in the literature. 1 Introduction Statistical Chinese word segmentation gains high accuracies on newswire (Xue and Shen, 2003; Zhang and Clark, 2007; Jiang et al., 2009; Zhao et al., 2010; Sun and Xu, 2011). However, manually annotated training data mostly come from the news domain, and the performance can drop severely when the test data shift from newswire to blogs, computer forums and Internet literature (Liu and Zhang, 2012). Several methods have been proposed for solving the domain adaptation problem for segmentation, which include the traditional token- and typesupervised methods (Song et al., 2012; Zhang et al., 2014). While token-supervised methods rely on manually annotated target-domain sentences, type-supervised methods leverage manually assembled domain-specific lexicons to improve t</context>
<context position="3415" citStr="Sun and Xu, 2011" startWordPosition="512" endWordPosition="515">is contained in hyperlinks and other markup annotations. Such free data offer a useful alternative for improving the segmentation performance, especially on domains that are not identical to newswire, and for which little annotation is available. In this paper, we investigate techniques for adopting freely available data to help improve the performance on Chinese word segmentation. We propose a simple but robust method for constructing partial segmentation from different sources of free data, including unlabeled data and the Wikipedia. There has been work on making use of both unlabeled data (Sun and Xu, 2011; Wang et al., 2011) and Wikipedia (Jiang et al., 2013) 浦 东 开 发 与 法 制 建 设 b b b b b b b b b m m m m m m m m m e e e e e e e e e s s s s s s s s s 864 Proceedings of the 2014 Conference on Empirical Methods in Natural Language Processing (EMNLP), pages 864–874, October 25-29, 2014, Doha, Qatar. c�2014 Association for Computational Linguistics to improve segmentation. However, no empirical results have been reported on a unified approach to deal with different types of free data. We use a conditional random fields (Lafferty et al., 2001; Tsuboi et al., 2008) variant that can leverage the partial</context>
<context position="13000" citStr="Sun and Xu (2011)" startWordPosition="2150" endWordPosition="2153">to evaluate the relevance between Wikipedia and target domain test data. We assume that URLtagged entries reflect the segmentation standards of Wikipedia sentence, and use them to match Wikipedia sentences with the raw target domain data. If the character sequence of any URL-tagged entry in a Wikipedia sentence matches the target domain data, the Wikipedia sentence is selected for training. Another advantage of such data selection is that the training time consumption can be reduced by reducing the size of training data. 3 CRF for Word Segmentation We follow the work of Zhao et al. (2010) and Sun and Xu (2011), and adopt the Conditional Random Fields (CRF) model (Lafferty et al., 2001) for the sequence labeling problem of word segmentation. Given an input characters sequence, the task is to assign one segmentation label from {b, m, e, s} on each character. Let x = (x1,x2, ..., xT) be the sequence of characters in sentence whose length is T, and y = (y1, y2, ..., yT) be the corresponding label sequence, where yi E Y . The linearchain conditional random field for Chinese word segmentation can be formalized as ∑T 1 p(y|x) = Z exp t=1 where Ak are the model parameters, fk are the feature functions and </context>
<context position="23607" citStr="Sun and Xu, 2011" startWordPosition="3979" endWordPosition="3982">er F Roov F Roov F Roov Baseline 87.50 73.65 91.36 72.95 93.16 84.02 Baseline+Lexicon Feature 90.36 80.69 91.60 74.39 93.14 84.27 Baseline+PA (Lex) 90.63 84.88 91.68 74.99 93.47 85.63 Zhang et al. (2014) 88.34 - - - - - Table 4: Final result for adapting CTB to Zhuxian and adapting PD to the medicine and computer domains, using partially annotated data (referred to as PA) obtained from unlabeled data and lexicons. trained our baseline model on the PD training data with the same feature template setting. Previous research makes use of a lexicon by adding lexicon features directly into a model (Sun and Xu, 2011; Zhang et al., 2014), rather than transforming them into partially annotated sentences. To make a comparison, we follow Sun and Xu (2011) and add three lexicon features to represent whether cz is located at the beginning, middle or the end of a word in the lexicon, respectively. For each test domain, the lexicon for the lexicon feature model consists of the most frequent words in the source domain training data (about 6.7K for CTB5 and 8K for PD, respectively) and the domain-specific lexicon we obtained in Section 5.1.1. The results are shown in Table 4, where the first row shows the performa</context>
<context position="30295" citStr="Sun and Xu (2011)" startWordPosition="5074" endWordPosition="5077">nd the fully annotated PD sentences. The tests are performed on medicine and computer domains. Table 7 shows the results, where further improvements are made on both domains when the two types of resources are combined. 6 Related Work There has been a line of research on making use of unlabeled data for word segmentation. Zhao and Kit (2008) improve segmentation performance by mutual information between characters, collected from large unlabeled data; Li and Sun (2009) use punctuation information in a large raw corpus to learn a segmentation model, and achieve better recognition of OOV words; Sun and Xu (2011) explore several statistical features derived from unlabeled data to help improve character-based word segmentation. These investigations mainly focus on in-domain accuracies. Liu and Zhang (2012) Method Baseline Baseline+PA (Lex) Baseline+PA (Natural) Baseline+PA (Lex+Natural) 91.36 91.68 92.47 92.63 Med. F 93.16 93.47 93.93 94.07 Com. F 871 0.9525 0.8775 0.9522 0.8750 0.9519 0.8725 0.9516 0.8700 0 50 100 150 (a) Finance 0.9225 0.78 0.9200 0.77 0.9175 0.76 0.9150 0.75 0.74 0.73 0 50 100 150 (b) Medicine 0.9225 0.750 0.938 0.87 0.9222 0.745 0.936 0.86 0.9219 0.740 0.934 0.85 0.9216 0.735 0.84 </context>
</contexts>
<marker>Sun, Xu, 2011</marker>
<rawString>Weiwei Sun and Jia Xu. 2011. Enhancing chinese word segmentation using unlabeled data. In Proceedings of the 2011 Conference on Empirical Methods in Natural Language Processing, pages 970– 979, Edinburgh, Scotland, UK., July. Association for Computational Linguistics.</rawString>
</citation>
<citation valid="true">
<authors>
<author>Jun Suzuki</author>
<author>Hideki Isozaki</author>
</authors>
<title>Semi-supervised sequential labeling and segmentation using gigaword scale unlabeled data.</title>
<date>2008</date>
<booktitle>In Proceedings of ACL08: HLT,</booktitle>
<pages>665--673</pages>
<publisher>Association</publisher>
<institution>for Computational Linguistics.</institution>
<location>Columbus, Ohio,</location>
<contexts>
<context position="32897" citStr="Suzuki and Isozaki, 2008" startWordPosition="5491" endWordPosition="5494">formation under the CRF framework. CRF has been used for Chinese word segmentation (Tseng, 2005; Shi and Wang, 2007; Zhao and Kit, 2008; Wang et al., 2011). However, most previous work train a CRF by using full annotation only. In contrast, we study CRF based segmentation by using both full and partial annotation. Several other variants of CRF model has been proposed in the machine learning literature, such as the generalized expectation method (Mann and McCallum, 2008), which introduce knowledge by incorporating a manually annotated feature distribution into the regularizer, and the JESS-CM (Suzuki and Isozaki, 2008), which use a EM-like method to iteratively optimize the parameter on both the annotated data and unlabeled data. In contrast, we directly incorporate the likelihood of partial annotation into the objective function. The work that is the most similar to ours is Tsuboi et al. (2008), who modify the CRF learning objective for partial data. They focus on Japanese lexical analysis using manually collected partial data, while we investigate the effect of partial annotation from freely available sources for Chinese segmentation. 7 Conclusion In this paper, we investigated the problem of domain adapt</context>
</contexts>
<marker>Suzuki, Isozaki, 2008</marker>
<rawString>Jun Suzuki and Hideki Isozaki. 2008. Semi-supervised sequential labeling and segmentation using gigaword scale unlabeled data. In Proceedings of ACL08: HLT, pages 665–673, Columbus, Ohio, June. Association for Computational Linguistics.</rawString>
</citation>
<citation valid="true">
<authors>
<author>Huihsin Tseng</author>
</authors>
<title>A conditional random field word segmenter. In</title>
<date>2005</date>
<booktitle>In Fourth SIGHAN Workshop on Chinese Language Processing.</booktitle>
<contexts>
<context position="32367" citStr="Tseng, 2005" startWordPosition="5408" endWordPosition="5409">ce needs to be resolved by using annotated sentences. They found that given the same manual annotation time, a combination of the lexicon and sentence is the most effective. Jiang et al. (2013) use 160K Wikipedia sentences to improves segmentation accuracies on several domains. Both Zhang et al. (2014) and Jiang et al. (2013) work on discriminative models using the structure perceptron (Collins, 2002), although they study two different sources of information. In contrast to their work, we unify both types of information under the CRF framework. CRF has been used for Chinese word segmentation (Tseng, 2005; Shi and Wang, 2007; Zhao and Kit, 2008; Wang et al., 2011). However, most previous work train a CRF by using full annotation only. In contrast, we study CRF based segmentation by using both full and partial annotation. Several other variants of CRF model has been proposed in the machine learning literature, such as the generalized expectation method (Mann and McCallum, 2008), which introduce knowledge by incorporating a manually annotated feature distribution into the regularizer, and the JESS-CM (Suzuki and Isozaki, 2008), which use a EM-like method to iteratively optimize the parameter on </context>
</contexts>
<marker>Tseng, 2005</marker>
<rawString>Huihsin Tseng. 2005. A conditional random field word segmenter. In In Fourth SIGHAN Workshop on Chinese Language Processing.</rawString>
</citation>
<citation valid="true">
<authors>
<author>Yuta Tsuboi</author>
<author>Hisashi Kashima</author>
<author>Shinsuke Mori</author>
<author>Hiroki Oda</author>
<author>Yuji Matsumoto</author>
</authors>
<title>Training conditional random fields using incomplete annotations.</title>
<date>2008</date>
<journal>Organizing Committee.</journal>
<booktitle>In Proceedings of the 22nd International Conference on Computational Linguistics (Coling</booktitle>
<pages>897--904</pages>
<location>Manchester, UK,</location>
<contexts>
<context position="3977" citStr="Tsuboi et al., 2008" startWordPosition="631" endWordPosition="634">ork on making use of both unlabeled data (Sun and Xu, 2011; Wang et al., 2011) and Wikipedia (Jiang et al., 2013) 浦 东 开 发 与 法 制 建 设 b b b b b b b b b m m m m m m m m m e e e e e e e e e s s s s s s s s s 864 Proceedings of the 2014 Conference on Empirical Methods in Natural Language Processing (EMNLP), pages 864–874, October 25-29, 2014, Doha, Qatar. c�2014 Association for Computational Linguistics to improve segmentation. However, no empirical results have been reported on a unified approach to deal with different types of free data. We use a conditional random fields (Lafferty et al., 2001; Tsuboi et al., 2008) variant that can leverage the partial annotations obtained from different sources of free annotation. Training is achieved by a modification to the learning objective, incorporating partial annotation likelihood, so that a single model can be trained consistently with a mixture of full and partial annotation. Experimental results show that our method of using partially annotated data can consistently improves cross-domain segmentation performance. We obtain results which are competitive to the best reported in the literature. Our segmentor is freely released at https://github.com/ ExpResults/</context>
<context position="15632" citStr="Tsuboi et al. (2008)" startWordPosition="2619" endWordPosition="2622">ion with partially annotated data, some characters in a sentence can have a definite segmentation label, while some can have multiple labels with ambiguities remaining. Taking the partially annotated sentence in Figure 2a for example, the corresponding potential label sequence for “在狐岐山救” is {(e, s), (b), (m), (e), (b, s)}, where the characters “狐”, “岐” and “山” have fixed labels but for “在” and “救”, some ambiguities exist. Note that the full annotation in Figure 1 can be regarded as a special case of partial annotation, where the number of potential labels for each character is one. We follow Tsuboi et al. (2008) and model marginal probabilities over partially annotated data. Define the possible labels that correspond to the partial annotation as L = (L1, L2,..., LT), where each Li is a non-empty subset of Y that corresponds to the set of possible labels for xi. Let ∑ Akfk(yt, yt−1, x) (1) k ∑Z = Y ∑ k exp ∑T t=1 ∑N n=1 L = 867 YL be the set of all possible label sequences where VY E YL, yi E Li. The marginal probability of YL can be modeled as 1 ∑ p(YL|x) = Z YEYL Defining the unnormalized marginal probability as T �ZYL = exp � λkfk(yt, yt−1, X), YEYL t=1 k and the normalizer Z being the same as Equa</context>
<context position="33179" citStr="Tsuboi et al. (2008)" startWordPosition="5537" endWordPosition="5540">full and partial annotation. Several other variants of CRF model has been proposed in the machine learning literature, such as the generalized expectation method (Mann and McCallum, 2008), which introduce knowledge by incorporating a manually annotated feature distribution into the regularizer, and the JESS-CM (Suzuki and Isozaki, 2008), which use a EM-like method to iteratively optimize the parameter on both the annotated data and unlabeled data. In contrast, we directly incorporate the likelihood of partial annotation into the objective function. The work that is the most similar to ours is Tsuboi et al. (2008), who modify the CRF learning objective for partial data. They focus on Japanese lexical analysis using manually collected partial data, while we investigate the effect of partial annotation from freely available sources for Chinese segmentation. 7 Conclusion In this paper, we investigated the problem of domain adaptation for word segmentation, by transferring various sources of free annotations into a consistent form of partially annotated data and applying a variant of CRF that can be trained using fully- and partially-annotated data simultaneously. We performed a large set of experiments to</context>
</contexts>
<marker>Tsuboi, Kashima, Mori, Oda, Matsumoto, 2008</marker>
<rawString>Yuta Tsuboi, Hisashi Kashima, Shinsuke Mori, Hiroki Oda, and Yuji Matsumoto. 2008. Training conditional random fields using incomplete annotations. In Proceedings of the 22nd International Conference on Computational Linguistics (Coling 2008), pages 897–904, Manchester, UK, August. Coling 2008 Organizing Committee.</rawString>
</citation>
<citation valid="true">
<authors>
<author>Yiou Wang</author>
<author>Jun’ichi Kazama</author>
<author>Yoshimasa Tsuruoka</author>
<author>Wenliang Chen</author>
<author>Yujie Zhang</author>
<author>Kentaro Torisawa</author>
</authors>
<title>Improving chinese word segmentation and pos tagging with semi-supervised methods using large auto-analyzed data.</title>
<date>2011</date>
<journal>Asian Federation of Natural Language Processing.</journal>
<booktitle>In Proceedings of 5th International Joint Conference on Natural Language Processing,</booktitle>
<pages>309--317</pages>
<location>Chiang Mai, Thailand,</location>
<contexts>
<context position="3435" citStr="Wang et al., 2011" startWordPosition="516" endWordPosition="519">perlinks and other markup annotations. Such free data offer a useful alternative for improving the segmentation performance, especially on domains that are not identical to newswire, and for which little annotation is available. In this paper, we investigate techniques for adopting freely available data to help improve the performance on Chinese word segmentation. We propose a simple but robust method for constructing partial segmentation from different sources of free data, including unlabeled data and the Wikipedia. There has been work on making use of both unlabeled data (Sun and Xu, 2011; Wang et al., 2011) and Wikipedia (Jiang et al., 2013) 浦 东 开 发 与 法 制 建 设 b b b b b b b b b m m m m m m m m m e e e e e e e e e s s s s s s s s s 864 Proceedings of the 2014 Conference on Empirical Methods in Natural Language Processing (EMNLP), pages 864–874, October 25-29, 2014, Doha, Qatar. c�2014 Association for Computational Linguistics to improve segmentation. However, no empirical results have been reported on a unified approach to deal with different types of free data. We use a conditional random fields (Lafferty et al., 2001; Tsuboi et al., 2008) variant that can leverage the partial annotations obtaine</context>
<context position="32427" citStr="Wang et al., 2011" startWordPosition="5418" endWordPosition="5421"> They found that given the same manual annotation time, a combination of the lexicon and sentence is the most effective. Jiang et al. (2013) use 160K Wikipedia sentences to improves segmentation accuracies on several domains. Both Zhang et al. (2014) and Jiang et al. (2013) work on discriminative models using the structure perceptron (Collins, 2002), although they study two different sources of information. In contrast to their work, we unify both types of information under the CRF framework. CRF has been used for Chinese word segmentation (Tseng, 2005; Shi and Wang, 2007; Zhao and Kit, 2008; Wang et al., 2011). However, most previous work train a CRF by using full annotation only. In contrast, we study CRF based segmentation by using both full and partial annotation. Several other variants of CRF model has been proposed in the machine learning literature, such as the generalized expectation method (Mann and McCallum, 2008), which introduce knowledge by incorporating a manually annotated feature distribution into the regularizer, and the JESS-CM (Suzuki and Isozaki, 2008), which use a EM-like method to iteratively optimize the parameter on both the annotated data and unlabeled data. In contrast, we </context>
</contexts>
<marker>Wang, Kazama, Tsuruoka, Chen, Zhang, Torisawa, 2011</marker>
<rawString>Yiou Wang, Jun’ichi Kazama, Yoshimasa Tsuruoka, Wenliang Chen, Yujie Zhang, and Kentaro Torisawa. 2011. Improving chinese word segmentation and pos tagging with semi-supervised methods using large auto-analyzed data. In Proceedings of 5th International Joint Conference on Natural Language Processing, pages 309–317, Chiang Mai, Thailand, November. Asian Federation of Natural Language Processing.</rawString>
</citation>
<citation valid="true">
<authors>
<author>Pak-kwong Wong</author>
<author>Chorkin Chan</author>
</authors>
<title>Chinese word segmentation based on maximum matching and word binding force.</title>
<date>1996</date>
<booktitle>In Proceedings of the 16th Conference on Computational Linguistics - Volume 1, COLING ’96,</booktitle>
<pages>200--203</pages>
<publisher>Association</publisher>
<institution>for Computational Linguistics.</institution>
<location>Stroudsburg, PA, USA.</location>
<contexts>
<context position="7513" citStr="Wong and Chan, 1996" startWordPosition="1202" endWordPosition="1205">form of partial annotation with same unresolved ambiguities, as shown in Figure 2, and use them together with available full annotation (Figure 1) as the training data for the segmentor. In this section, we describe in detail how to obtain partially annotated sentences from each resource, respectively. 2.1 Lexicons In this scenario, we assume that there are unlabeled sentences along with a lexicon for the target domain. We obtain partially segmented sentences by extracting word boundaries from the unlabeled sentences with the help of the lexicon. Previous matching methods (Wu and Tseng, 1993; Wong and Chan, 1996) for Chinese word segmentation largely rely on the lexicons, and are generally considered being weak in ambiguity resolution (Gao 在 狐 , 歧 山 救 治 碧 瑶 b b b b b b b b b m m m m m m m m m e e e e e e e e e s s s s s s s s s 如 乳 铁 蛋 白 、 溶 菌 酶 b b b b b b b b b m m m m m m m m m e e e e e e e e e s s s s s s s s s 865 People’s Daily *P] (saw) 4A (Hainan) A*A, (tourist industry) it�-A (full) *V (hope) saw tourist industry in Hainan is full of hope Wikipedia �L-*(mainly) A_(is) A* (tourist) A (industry) (and) *k* (software) r&apos;A(industry) mainly is tourist industry and software industry (a) Case of inc</context>
</contexts>
<marker>Wong, Chan, 1996</marker>
<rawString>Pak-kwong Wong and Chorkin Chan. 1996. Chinese word segmentation based on maximum matching and word binding force. In Proceedings of the 16th Conference on Computational Linguistics - Volume 1, COLING ’96, pages 200–203, Stroudsburg, PA, USA. Association for Computational Linguistics.</rawString>
</citation>
<citation valid="true">
<authors>
<author>Zimin Wu</author>
<author>Gwyneth Tseng</author>
</authors>
<title>Chinese text segmentation for text retrieval: Achievements and problems.</title>
<date>1993</date>
<journal>J. Am. Soc. Inf. Sci.,</journal>
<volume>44</volume>
<issue>9</issue>
<contexts>
<context position="7491" citStr="Wu and Tseng, 1993" startWordPosition="1198" endWordPosition="1201">ation into the same form of partial annotation with same unresolved ambiguities, as shown in Figure 2, and use them together with available full annotation (Figure 1) as the training data for the segmentor. In this section, we describe in detail how to obtain partially annotated sentences from each resource, respectively. 2.1 Lexicons In this scenario, we assume that there are unlabeled sentences along with a lexicon for the target domain. We obtain partially segmented sentences by extracting word boundaries from the unlabeled sentences with the help of the lexicon. Previous matching methods (Wu and Tseng, 1993; Wong and Chan, 1996) for Chinese word segmentation largely rely on the lexicons, and are generally considered being weak in ambiguity resolution (Gao 在 狐 , 歧 山 救 治 碧 瑶 b b b b b b b b b m m m m m m m m m e e e e e e e e e s s s s s s s s s 如 乳 铁 蛋 白 、 溶 菌 酶 b b b b b b b b b m m m m m m m m m e e e e e e e e e s s s s s s s s s 865 People’s Daily *P] (saw) 4A (Hainan) A*A, (tourist industry) it�-A (full) *V (hope) saw tourist industry in Hainan is full of hope Wikipedia �L-*(mainly) A_(is) A* (tourist) A (industry) (and) *k* (software) r&apos;A(industry) mainly is tourist industry and software in</context>
<context position="9181" citStr="Wu and Tseng (1993)" startWordPosition="1522" endWordPosition="1525">jJ(into) -TR (fields) each record is splitted into several fields Computer et al., 2005). But for obtaining the partial labeled data with lexicon, the matching method can still be a solution. Since we do not aim to recognize every word from sentence, we can select a lexicon with smaller coverage but less ambiguity to achieve relatively precise matching result. In this paper, we apply two matching schemes to the same raw sentences to obtain partially annotated sentences. The first is a simple forwardmaximum matching (FMM) scheme, which is very close to the forward maximum matching algorithm of Wu and Tseng (1993) for Chinese word segmentation. This scheme scans the input sentence from left to right. At each position, it attempts to find the longest subsequence of Chinese characters that matches a lexicon entry. If such an entry is found, the subsequence is tagged with the corresponding tags, and its surrounding characters are also constrained to a smaller set of tags. If no subsequence is found in the lexicon, the character is left with all the possible tags. Taking the sentence in Figure 2a for example. When the algorithm scans the second character, “VP”, and finds the entry “VP,_t�W” in the lexicon,</context>
</contexts>
<marker>Wu, Tseng, 1993</marker>
<rawString>Zimin Wu and Gwyneth Tseng. 1993. Chinese text segmentation for text retrieval: Achievements and problems. J. Am. Soc. Inf. Sci., 44(9):532–542, October.</rawString>
</citation>
<citation valid="true">
<authors>
<author>Nianwen Xue</author>
<author>Libin Shen</author>
</authors>
<title>Chinese word segmentation as lmr tagging.</title>
<date>2003</date>
<booktitle>In Proceedings of the Second SIGHAN Workshop on Chinese Language Processing - Volume 17, SIGHAN ’03,</booktitle>
<pages>176--179</pages>
<publisher>Association</publisher>
<institution>for Computational Linguistics.</institution>
<location>Stroudsburg, PA, USA.</location>
<contexts>
<context position="1221" citStr="Xue and Shen, 2003" startWordPosition="179" endWordPosition="182">nnotation from freely available data to help Chinese word segmentation on different domains. Different sources of free annotations are transformed into a unified form of partial annotation and a variant CRF model is used to leverage both fully and partially annotated data consistently. Experimental results show that the Chinese word segmentation model benefits from free partially annotated data. On the SIGHAN Bakeoff 2010 data, we achieve results that are competitive to the best reported in the literature. 1 Introduction Statistical Chinese word segmentation gains high accuracies on newswire (Xue and Shen, 2003; Zhang and Clark, 2007; Jiang et al., 2009; Zhao et al., 2010; Sun and Xu, 2011). However, manually annotated training data mostly come from the news domain, and the performance can drop severely when the test data shift from newswire to blogs, computer forums and Internet literature (Liu and Zhang, 2012). Several methods have been proposed for solving the domain adaptation problem for segmentation, which include the traditional token- and typesupervised methods (Song et al., 2012; Zhang et al., 2014). While token-supervised methods rely on manually annotated target-domain sentences, type-sup</context>
<context position="4803" citStr="Xue and Shen, 2003" startWordPosition="751" endWordPosition="754">ihood, so that a single model can be trained consistently with a mixture of full and partial annotation. Experimental results show that our method of using partially annotated data can consistently improves cross-domain segmentation performance. We obtain results which are competitive to the best reported in the literature. Our segmentor is freely released at https://github.com/ ExpResults/partial-crfsuite. 2 Obtaining Partially Annotated Data We model the Chinese word segmentation task as a character sequence tagging problem, which is to give each character in a sentence a word-boundary tag (Xue and Shen, 2003). We adopt four tags, b, m, e and s, which represent the beginning, middle, end of a multi-character word, and a single character word, respectively. A manually segmented sentence can be represented as a tag sequence, as shown in Figure 1. We investigate two major sources of freelyavailable annotations: lexicons and natural annotation, both with the help of unannotated data. To make use of the first source of information, we incorporate words from a lexicon into unannotated sentences by matching of character sequences, resulting in partially annotated sentences, as shown in Figure 2a. In this </context>
</contexts>
<marker>Xue, Shen, 2003</marker>
<rawString>Nianwen Xue and Libin Shen. 2003. Chinese word segmentation as lmr tagging. In Proceedings of the Second SIGHAN Workshop on Chinese Language Processing - Volume 17, SIGHAN ’03, pages 176– 179, Stroudsburg, PA, USA. Association for Computational Linguistics.</rawString>
</citation>
<citation valid="true">
<authors>
<author>Shiwen Yu</author>
</authors>
<title>Jianming Lu, Xuefeng Zhu, Huiming Duan, Shiyong Kang, Honglin Sun, Hui Wang, Qiang Zhao, and Weidong Zhan.</title>
<date>2001</date>
<tech>Technical report.</tech>
<marker>Yu, 2001</marker>
<rawString>Shiwen Yu, Jianming Lu, Xuefeng Zhu, Huiming Duan, Shiyong Kang, Honglin Sun, Hui Wang, Qiang Zhao, and Weidong Zhan. 2001. Processing norms of modern chinese corpus. Technical report.</rawString>
</citation>
<citation valid="true">
<authors>
<author>Yue Zhang</author>
<author>Stephen Clark</author>
</authors>
<title>Chinese segmentation with a word-based perceptron algorithm.</title>
<date>2007</date>
<booktitle>In Proceedings of the 45th Annual Meeting of the Association of Computational Linguistics,</booktitle>
<pages>840--847</pages>
<publisher>Association</publisher>
<institution>for Computational Linguistics.</institution>
<location>Prague, Czech Republic,</location>
<contexts>
<context position="1244" citStr="Zhang and Clark, 2007" startWordPosition="183" endWordPosition="186">y available data to help Chinese word segmentation on different domains. Different sources of free annotations are transformed into a unified form of partial annotation and a variant CRF model is used to leverage both fully and partially annotated data consistently. Experimental results show that the Chinese word segmentation model benefits from free partially annotated data. On the SIGHAN Bakeoff 2010 data, we achieve results that are competitive to the best reported in the literature. 1 Introduction Statistical Chinese word segmentation gains high accuracies on newswire (Xue and Shen, 2003; Zhang and Clark, 2007; Jiang et al., 2009; Zhao et al., 2010; Sun and Xu, 2011). However, manually annotated training data mostly come from the news domain, and the performance can drop severely when the test data shift from newswire to blogs, computer forums and Internet literature (Liu and Zhang, 2012). Several methods have been proposed for solving the domain adaptation problem for segmentation, which include the traditional token- and typesupervised methods (Song et al., 2012; Zhang et al., 2014). While token-supervised methods rely on manually annotated target-domain sentences, type-supervised methods leverag</context>
</contexts>
<marker>Zhang, Clark, 2007</marker>
<rawString>Yue Zhang and Stephen Clark. 2007. Chinese segmentation with a word-based perceptron algorithm. In Proceedings of the 45th Annual Meeting of the Association of Computational Linguistics, pages 840– 847, Prague, Czech Republic, June. Association for Computational Linguistics.</rawString>
</citation>
<citation valid="true">
<authors>
<author>Meishan Zhang</author>
<author>Yue Zhang</author>
<author>Wanxiang Che</author>
<author>Ting Liu</author>
</authors>
<title>Type-supervised domain adaptation for joint segmentation and pos-tagging.</title>
<date>2014</date>
<booktitle>In Proceedings of the 14th Conference of the European Chapter of the Association for Computational Linguistics,</booktitle>
<pages>588--597</pages>
<publisher>Association</publisher>
<institution>for Computational Linguistics.</institution>
<location>Gothenburg, Sweden,</location>
<contexts>
<context position="1728" citStr="Zhang et al., 2014" startWordPosition="263" endWordPosition="266">ture. 1 Introduction Statistical Chinese word segmentation gains high accuracies on newswire (Xue and Shen, 2003; Zhang and Clark, 2007; Jiang et al., 2009; Zhao et al., 2010; Sun and Xu, 2011). However, manually annotated training data mostly come from the news domain, and the performance can drop severely when the test data shift from newswire to blogs, computer forums and Internet literature (Liu and Zhang, 2012). Several methods have been proposed for solving the domain adaptation problem for segmentation, which include the traditional token- and typesupervised methods (Song et al., 2012; Zhang et al., 2014). While token-supervised methods rely on manually annotated target-domain sentences, type-supervised methods leverage manually assembled domain-specific lexicons to improve target-domain segmentation accuracies. Both Figure 1: The segmentation problem, illustrated using the sentence “浦东 (Pudong) 开发 (development) 与 (and) 法制 (legal) 建设 (construction)”. Possible segmentation labels are drawn under each character, where b, m, e, s stand for the beginning, middle, end of a multi-character word, and a single character word, respectively. The path shows the correct segmentation by choosing one label </context>
<context position="18837" citStr="Zhang et al. (2014)" startWordPosition="3195" endWordPosition="3198">fied Chinese. After filtering functional pages like redirection and removing duplication, 5.45 million sentences are reserved. For comparison with related work on using a lexicon to improve segmentation, another set of test data is chosen for this setting. We use the Chinese Treebank (CTB) as the source domain data, and Zhuxian (a free Internet novel, also named as “Jade dynasty”, referred to as ZX henceforth) as the target domain data.3 The ZX data are written in a different style from newswire, and contains many out-of-vocabulary words. This setting has been used by Liu and Zhang (2012) and Zhang et al. (2014) for domain adaptation of segmentation and POS-tagging. We use the standard training, development and test split. Statistics of the test data annotated by Zhang et al. (2014) are shown in the second half of Table 3. The data preparation method in Section 2 and the CRF method in Section 4 are used for all the experiments. Both recall of out-of-vocabulary words (Roov) and F-score are used to evaluate the 1http://www.chokkan.org/software/ crfsuite/ 2http://dumps.wikimedia.org/zhwiki/ 20140419/ 3Annotated target domain test data and lexicon are available from http://ir.hit.edu.cn/˜mszhang/ eacl14m</context>
<context position="20656" citStr="Zhang et al. (2014)" startWordPosition="3492" endWordPosition="3495">sed in this paper. segmentation performance. There is a mixture of Chinese characters, English words and numeric expression in the test data from SIGHAN Bakeoff 2010. To test the influence of Wikipedia data on Chinese word segmentation alone, we apply regular expressions to detect English words and numeric expressions, so that they are marked as not segmented. After performing this preprocessing step, cleaned test input data are fed to the CRF model to give a relatively strong baseline. 5.1 Free Lexicons 5.1.1 Obtaining lexicons For domain adaption from CTB to ZX, we use a lexicon released by Zhang et al. (2014). The lexicon is crawled from a online encyclopedia4, and contains the names of 159 characters and artifacts in the Zhuxian novel. We follow Zhang et al. (2014) and name it NR for convenience of further discussion. The NR lexicon can be treated as a strongly domain-related, high quality but relatively small lexicon. It’s a typical example of freely available lexicon over the Internet. For domain adaptation from PD to medicine and computer, we collect a list of page titles under the corresponding categories in Wikipedia. For medicine, entries under essential medicines, biological system and dis</context>
<context position="23194" citStr="Zhang et al. (2014)" startWordPosition="3907" endWordPosition="3910">andomly select 16K sentences as the unlabeled data for each domain, respectively. 5.1.3 Final results We incorporate the partially annotated data obtained with the help of lexicon for each of the test domain. For adaptation from CTB to ZX, we trained our baseline model on the CTB5 training data with the feature templates in Table 2. For adaptation from PD to medicine and computer, we 869 Domain ZX Medicine Computer F Roov F Roov F Roov Baseline 87.50 73.65 91.36 72.95 93.16 84.02 Baseline+Lexicon Feature 90.36 80.69 91.60 74.39 93.14 84.27 Baseline+PA (Lex) 90.63 84.88 91.68 74.99 93.47 85.63 Zhang et al. (2014) 88.34 - - - - - Table 4: Final result for adapting CTB to Zhuxian and adapting PD to the medicine and computer domains, using partially annotated data (referred to as PA) obtained from unlabeled data and lexicons. trained our baseline model on the PD training data with the same feature template setting. Previous research makes use of a lexicon by adding lexicon features directly into a model (Sun and Xu, 2011; Zhang et al., 2014), rather than transforming them into partially annotated sentences. To make a comparison, we follow Sun and Xu (2011) and add three lexicon features to represent whet</context>
<context position="24831" citStr="Zhang et al. (2014)" startWordPosition="4191" endWordPosition="4194">f the baseline models and the second row shows the performance of the model incorporating lexicon feature. The third row shows our method using partial annotation. On the ZX test set, our method outperforms the baseline by more than 3 absolute percentage. The model with partially annotated data performs better than the one with additional lexicon features. Similar conclusion is obtained when adapting from PD to medicine and computer. By incorporating the partially annotated data, the segmentation of lexicon words, along with the context, is learned. We also compare our method with the work of Zhang et al. (2014), who reported results only on the ZX test data. We use the same lexicon settings. Our method gives better result than Zhang et al. (2014), showing that the combination of a lexicon and unannotated sentence into partially annotated data can lead to better performance than using a dictionary alone in type-supervision. Given that we only explore the use of free resource, combining a lexicon with unannotated sentences is a better option than using the lexicon directly. Zhang et al.’s concern, on the other hand, is to compare Table 5: The performance of data selection on the development set of the</context>
<context position="31461" citStr="Zhang et al. (2014)" startWordPosition="5262" endWordPosition="5265">0.936 0.86 0.9219 0.740 0.934 0.85 0.9216 0.735 0.84 0 50 100 150 0 50 100 150 (c) Literature (d) Computer Figure 4: Performance of the model incorporating difference sizes of Wikipedia data. The solid line represents the F-score and dashed line represents the recall of OOV words. study domain adaptation using an unsupervised self-training method. In contrast to their work, we make use of not only unlabeled data, but also leverage any free annotation to achieve better results for domain adaptation. There has also been work on making use of a dictionary and natural annotation for segmentation. Zhang et al. (2014) study type-supervised domain adaptation for Chinese segmentation. They categorize domain difference into two types: different vocabulary and different POS distributions. While the first type of difference can be effectively resolved by using lexicon for each domain, the second type of difference needs to be resolved by using annotated sentences. They found that given the same manual annotation time, a combination of the lexicon and sentence is the most effective. Jiang et al. (2013) use 160K Wikipedia sentences to improves segmentation accuracies on several domains. Both Zhang et al. (2014) a</context>
</contexts>
<marker>Zhang, Zhang, Che, Liu, 2014</marker>
<rawString>Meishan Zhang, Yue Zhang, Wanxiang Che, and Ting Liu. 2014. Type-supervised domain adaptation for joint segmentation and pos-tagging. In Proceedings of the 14th Conference of the European Chapter of the Association for Computational Linguistics, pages 588–597, Gothenburg, Sweden, April. Association for Computational Linguistics.</rawString>
</citation>
<citation valid="true">
<authors>
<author>Hai Zhao</author>
<author>Chunyu Kit</author>
</authors>
<title>An empirical comparison of goodness measures for unsupervised chinese word segmentation with a unified framework. In In:</title>
<date>2008</date>
<booktitle>The Third International Joint Conference on Natural Language Processing (IJCNLP-2008.</booktitle>
<contexts>
<context position="30021" citStr="Zhao and Kit (2008)" startWordPosition="5033" endWordPosition="5036">xicon and Natural Annotation To make the most use of free annotation, we combine available free lexicon and natural annotation resources by joining the partially annotated sentences derived using each resource, training our CRF model with these partially annotated sentences and the fully annotated PD sentences. The tests are performed on medicine and computer domains. Table 7 shows the results, where further improvements are made on both domains when the two types of resources are combined. 6 Related Work There has been a line of research on making use of unlabeled data for word segmentation. Zhao and Kit (2008) improve segmentation performance by mutual information between characters, collected from large unlabeled data; Li and Sun (2009) use punctuation information in a large raw corpus to learn a segmentation model, and achieve better recognition of OOV words; Sun and Xu (2011) explore several statistical features derived from unlabeled data to help improve character-based word segmentation. These investigations mainly focus on in-domain accuracies. Liu and Zhang (2012) Method Baseline Baseline+PA (Lex) Baseline+PA (Natural) Baseline+PA (Lex+Natural) 91.36 91.68 92.47 92.63 Med. F 93.16 93.47 93.9</context>
<context position="32407" citStr="Zhao and Kit, 2008" startWordPosition="5414" endWordPosition="5417">annotated sentences. They found that given the same manual annotation time, a combination of the lexicon and sentence is the most effective. Jiang et al. (2013) use 160K Wikipedia sentences to improves segmentation accuracies on several domains. Both Zhang et al. (2014) and Jiang et al. (2013) work on discriminative models using the structure perceptron (Collins, 2002), although they study two different sources of information. In contrast to their work, we unify both types of information under the CRF framework. CRF has been used for Chinese word segmentation (Tseng, 2005; Shi and Wang, 2007; Zhao and Kit, 2008; Wang et al., 2011). However, most previous work train a CRF by using full annotation only. In contrast, we study CRF based segmentation by using both full and partial annotation. Several other variants of CRF model has been proposed in the machine learning literature, such as the generalized expectation method (Mann and McCallum, 2008), which introduce knowledge by incorporating a manually annotated feature distribution into the regularizer, and the JESS-CM (Suzuki and Isozaki, 2008), which use a EM-like method to iteratively optimize the parameter on both the annotated data and unlabeled da</context>
</contexts>
<marker>Zhao, Kit, 2008</marker>
<rawString>Hai Zhao and Chunyu Kit. 2008. An empirical comparison of goodness measures for unsupervised chinese word segmentation with a unified framework. In In: The Third International Joint Conference on Natural Language Processing (IJCNLP-2008.</rawString>
</citation>
<citation valid="true">
<authors>
<author>Hai Zhao</author>
<author>Chang-Ning Huang</author>
<author>Mu Li</author>
<author>Bao-Liang Lu</author>
</authors>
<title>A unified character-based tagging framework for chinese word segmentation.</title>
<date>2010</date>
<tech>9(2):5:1–5:32,</tech>
<contexts>
<context position="1283" citStr="Zhao et al., 2010" startWordPosition="191" endWordPosition="194">ntation on different domains. Different sources of free annotations are transformed into a unified form of partial annotation and a variant CRF model is used to leverage both fully and partially annotated data consistently. Experimental results show that the Chinese word segmentation model benefits from free partially annotated data. On the SIGHAN Bakeoff 2010 data, we achieve results that are competitive to the best reported in the literature. 1 Introduction Statistical Chinese word segmentation gains high accuracies on newswire (Xue and Shen, 2003; Zhang and Clark, 2007; Jiang et al., 2009; Zhao et al., 2010; Sun and Xu, 2011). However, manually annotated training data mostly come from the news domain, and the performance can drop severely when the test data shift from newswire to blogs, computer forums and Internet literature (Liu and Zhang, 2012). Several methods have been proposed for solving the domain adaptation problem for segmentation, which include the traditional token- and typesupervised methods (Song et al., 2012; Zhang et al., 2014). While token-supervised methods rely on manually annotated target-domain sentences, type-supervised methods leverage manually assembled domain-specific le</context>
<context position="12978" citStr="Zhao et al. (2010)" startWordPosition="2145" endWordPosition="2148">ain, which can be used to evaluate the relevance between Wikipedia and target domain test data. We assume that URLtagged entries reflect the segmentation standards of Wikipedia sentence, and use them to match Wikipedia sentences with the raw target domain data. If the character sequence of any URL-tagged entry in a Wikipedia sentence matches the target domain data, the Wikipedia sentence is selected for training. Another advantage of such data selection is that the training time consumption can be reduced by reducing the size of training data. 3 CRF for Word Segmentation We follow the work of Zhao et al. (2010) and Sun and Xu (2011), and adopt the Conditional Random Fields (CRF) model (Lafferty et al., 2001) for the sequence labeling problem of word segmentation. Given an input characters sequence, the task is to assign one segmentation label from {b, m, e, s} on each character. Let x = (x1,x2, ..., xT) be the sequence of characters in sentence whose length is T, and y = (y1, y2, ..., yT) be the corresponding label sequence, where yi E Y . The linearchain conditional random field for Chinese word segmentation can be formalized as ∑T 1 p(y|x) = Z exp t=1 where Ak are the model parameters, fk are the </context>
<context position="17641" citStr="Zhao et al., 2010" startWordPosition="3000" endWordPosition="3003">ulated by αYL(T), and pYL(y, y′|x) can be calculated by αYL,t−1(y′)Ψt(y, y′, xt)βYL,t(y). Note that if each element in YL is constrained to one single label, the CRF model in Equation 3 degrades into Equation 1. So we can train a unified model with both fully and partially annotated data. We implement this CRF model based on a open source toolkit CRFSuite.1 In our experiments, we use the L-BFGS (Liu and Nocedal, 1989) algorithm to learn parameters from both fully and partially annotated data. 5 Experiments We perform our experiments on the domain adaptation test data from SIGHAN Bakeoff 2010 (Zhao et al., 2010), adapting annotated training sentences from People’s Daily (PD) (Yu et al., 2001) to different test domains. The fully annotated data is selected from the People’s Daily newspaper in January of 1998, and the four test domains from the SIGHAN Bakeoff 2010 include finance, medicine, literature and computer. Sample segmented data in the computer domain from this bakeoff is used as development set. Statistics of the data are shown in first half of Table 3. We use wikidump201404192 for the Wikipedia data. All the traditional Chinese pages in Wikipedia are converted to simplified Chinese. After fil</context>
</contexts>
<marker>Zhao, Huang, Li, Lu, 2010</marker>
<rawString>Hai Zhao, Chang-Ning Huang, Mu Li, and Bao-Liang Lu. 2010. A unified character-based tagging framework for chinese word segmentation. 9(2):5:1–5:32, June.</rawString>
</citation>
</citationList>
</algorithm>
</algorithms>
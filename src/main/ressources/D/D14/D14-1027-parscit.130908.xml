<?xml version="1.0" encoding="UTF-8"?>
<algorithms version="110505">
<algorithm name="SectLabel" version="110505">
<variant no="0" confidence="0.009623">
<title confidence="0.919202">
Learning to Differentiate Better from Worse Translations
</title>
<author confidence="0.862418">
Francisco Guzm´an Shafiq Joty Lluis M`arquez
Alessandro Moschitti Preslav Nakov Massimo Nicosia
</author>
<affiliation confidence="0.930491">
ALT Research Group
Qatar Computing Research Institute — Qatar Foundation
</affiliation>
<email confidence="0.998085">
{fguzman,sjoty,lmarquez,amoschitti,pnakov,mnicosia}@qf.org.qa
</email>
<sectionHeader confidence="0.99738" genericHeader="abstract">
Abstract
</sectionHeader>
<bodyText confidence="0.999953478260869">
We present a pairwise learning-to-rank
approach to machine translation evalua-
tion that learns to differentiate better from
worse translations in the context of a given
reference. We integrate several layers
of linguistic information encapsulated in
tree-based structures, making use of both
the reference and the system output simul-
taneously, thus bringing our ranking closer
to how humans evaluate translations. Most
importantly, instead of deciding upfront
which types of features are important, we
use the learning framework of preference
re-ranking kernels to learn the features au-
tomatically. The evaluation results show
that learning in the proposed framework
yields better correlation with humans than
computing the direct similarity over the
same type of structures. Also, we show
our structural kernel learning (SKL) can
be a general framework for MT evaluation,
in which syntactic and semantic informa-
tion can be naturally incorporated.
</bodyText>
<sectionHeader confidence="0.999471" genericHeader="introduction">
1 Introduction
</sectionHeader>
<bodyText confidence="0.999904962962963">
We have seen in recent years fast improvement
in the overall quality of machine translation (MT)
systems. This was only possible because of the
use of automatic metrics for MT evaluation, such
as BLEU (Papineni et al., 2002), which is the de-
facto standard; and more recently: TER (Snover et
al., 2006) and METEOR (Lavie and Denkowski,
2009), among other emerging MT evaluation met-
rics. These automatic metrics provide fast and in-
expensive means to compare the output of differ-
ent MT systems, without the need to ask for hu-
man judgments each time the MT system has been
changed.
As a result, this has enabled rapid develop-
ment in the field of statistical machine translation
(SMT), by allowing to train and tune systems as
well as to track progress in a way that highly cor-
relates with human judgments.
Today, MT evaluation is an active field of re-
search, and modern metrics perform analysis at
various levels, e.g., lexical (Papineni et al., 2002;
Snover et al., 2006), including synonymy and
paraphrasing (Lavie and Denkowski, 2009); syn-
tactic (Gim´enez and M`arquez, 2007; Popovi´c
and Ney, 2007; Liu and Gildea, 2005); semantic
(Gim´enez and M`arquez, 2007; Lo et al., 2012);
and discourse (Comelles et al., 2010; Wong and
Kit, 2012; Guzm´an et al., 2014; Joty et al., 2014).
Automatic MT evaluation metrics compare the
output of a system to one or more human ref-
erences in order to produce a similarity score.
The quality of such a metric is typically judged
in terms of correlation of the scores it produces
with scores given by human judges. As a result,
some evaluation metrics have been trained to re-
produce the scores assigned by humans as closely
as possible (Albrecht and Hwa, 2008). Unfortu-
nately, humans have a hard time assigning an ab-
solute score to a translation. Hence, direct hu-
man evaluation scores such as adequacy and flu-
ency, which were widely used in the past, are
now discontinued in favor of ranking-based eval-
uations, where judges are asked to rank the out-
put of 2 to 5 systems instead. It has been shown
that using such ranking-based assessments yields
much higher inter-annotator agreement (Callison-
Burch et al., 2007).
While evaluation metrics still produce numeri-
cal scores, in part because MT evaluation shared
tasks at NIST and WMT ask for it, there has also
been work on a ranking formulation of the MT
evaluation task for a given set of outputs. This
was shown to yield higher correlation with human
judgments (Duh, 2008; Song and Cohn, 2011).
</bodyText>
<page confidence="0.97546">
214
</page>
<bodyText confidence="0.984644735294118">
Proceedings of the 2014 Conference on Empirical Methods in Natural Language Processing (EMNLP), pages 214–220,
October 25-29, 2014, Doha, Qatar. c�2014 Association for Computational Linguistics
Learning automatic metrics in a pairwise set-
ting, i.e., learning to distinguish between two al-
ternative translations and to decide which of the
two is better (which is arguably one of the easiest
ways to produce a ranking), emulates closely how
human judges perform evaluation assessments in
reality. Instead of learning a similarity function
between a translation and the reference, they learn
how to differentiate a better from a worse trans-
lation given a corresponding reference. While the
pairwise setting does not provide an absolute qual-
ity scoring metric, it is useful for most evaluation
and MT development scenarios.
In this paper, we propose a pairwise learning
setting similar to that of Duh (2008), but we extend
it to a new level, both in terms of feature represen-
tation and learning framework. First, we integrate
several layers of linguistic information encapsu-
lated in tree-based structures; Duh (2008) only
used lexical and POS matches as features. Second,
we use information about both the reference and
two alternative translations simultaneously, thus
bringing our ranking closer to how humans rank
translations. Finally, instead of deciding upfront
which types of features between hypotheses and
references are important, we use a our structural
kernel learning (SKL) framework to generate and
select them automatically.
The structural kernel learning (SKL) framework
we propose consists in: (i) designing a struc-
tural representation, e.g., using syntactic and dis-
course trees of translation hypotheses and a refer-
ences; and (ii) applying structural kernels (Mos-
chitti, 2006; Moschitti, 2008), to such representa-
tions in order to automatically inject structural fea-
tures in the preference re-ranking algorithm. We
use this method with translation-reference pairs
to directly learn the features themselves, instead
of learning the importance of a predetermined set
of features. A similar learning framework has
been proven to be effective for question answer-
ing (Moschitti et al., 2007), and textual entailment
recognition (Zanzotto and Moschitti, 2006).
Our goals are twofold: (i) in the short term, to
demonstrate that structural kernel learning is suit-
able for this task, and can effectively learn to rank
hypotheses at the segment-level; and (ii) in the
long term, to show that this approach provides a
unified framework that allows to integrate several
layers of linguistic analysis and information and to
improve over the state-of-the-art.
Below we report the results of some initial ex-
periments using syntactic and discourse structures.
We show that learning in the proposed framework
yields better correlation with humans than apply-
ing the traditional translation–reference similarity
metrics using the same type of structures. We
also show that the contributions of syntax and dis-
course information are cumulative. Finally, de-
spite the limited information we use, we achieve
correlation at the segment level that outperforms
BLEU and other metrics at WMT12, e.g., our met-
ric would have been ranked higher in terms of cor-
relation with human judgments compared to TER,
NIST, and BLEU in the WMT12 Metrics shared
task (Callison-Burch et al., 2012).
</bodyText>
<sectionHeader confidence="0.910597" genericHeader="method">
2 Kernel-based Learning from Linguistic
Structures
</sectionHeader>
<bodyText confidence="0.99684215">
In our pairwise setting, each sentence s in
the source language is represented by a tuple
(t1, t2, r), where t1 and t2 are two alternative
translations and r is a reference translation. Our
goal is to develop a classifier of such tuples that
decides whether t1 is a better translation than t2
given the reference r.
Engineering features for deciding whether t1 is
a better translation than t2 is a difficult task. Thus,
we rely on the automatic feature extraction en-
abled by the SKL framework, and our task is re-
duced to choosing: (i) a meaningful structural rep-
resentation for (t1, t2, r), and (ii) a feature func-
tion φ,,tt that maps such structures to substruc-
tures, i.e., our feature space. Since the design
of φ,,tt is complex, we use tree kernels applied
to two simpler structural mappings φM(t1, r) and
φM(t2,r). The latter generate the tree representa-
tions for the translation-reference pairs (t1, r) and
(t2, r). The next section shows such mappings.
</bodyText>
<subsectionHeader confidence="0.987164">
2.1 Representations
</subsectionHeader>
<bodyText confidence="0.999688727272727">
To represent a translation-reference pair (t, r), we
adopt shallow syntactic trees combined with RST-
style discourse trees. Shallow trees have been
successfully used for question answering (Severyn
and Moschitti, 2012) and semantic textual sim-
ilarity (Severyn et al., 2013b); while discourse
information has proved useful in MT evaluation
(Guzm´an et al., 2014; Joty et al., 2014). Com-
bined shallow syntax and discourse trees worked
well for concept segmentation and labeling (Saleh
et al., 2014a).
</bodyText>
<page confidence="0.995468">
215
</page>
<figure confidence="0.971948">
a) Hypothesis
</figure>
<figureCaption confidence="0.999625">
Figure 1: Hypothesis and reference trees combining discourse, shallow syntax and POS.
</figureCaption>
<figure confidence="0.975006166666667">
DIS:ELABORATION
EDU:NUCLEUS EDU:SATELLITE-REL
VP NP-REL NP VP-REL o-REL o-REL
RB TO-REL VB-REL PRP-REL DT NN-REL TO-REL VB-REL .-REL &amp;quot;-REL
not to give them the time to think . &amp;quot;
Bag-of-words relations
to &amp;quot; give them no time to think . &amp;quot;
TO-REL VB-REL PRP-REL DT NN-REL TO-REL VB-REL .-REL &amp;quot;-REL
VP NP-REL NP VP-REL o-REL o-REL
EDU:NUCLEUS EDU:SATELLITE-REL
b) Reference DIS:ELABORATION
relation propagation direction
</figure>
<bodyText confidence="0.940564647058824">
Figure 1 shows two example trees combining
discourse, shallow syntax and POS: one for a
translation hypothesis (top) and the other one for
the reference (bottom). To build such structures,
we used the Stanford POS tagger (Toutanova et
al., 2003), the Illinois chunker (Punyakanok and
Roth, 2001), and the discourse parser1 of (Joty et
al., 2012; Joty et al., 2013).
The lexical items constitute the leaves of the
tree. The words are connected to their respec-
tive POS tags, which are in turn grouped into
chunks. Then, the chunks are grouped into el-
ementary discourse units (EDU), to which the
nuclearity status is attached (i.e., NUCLEUS or
SATELLITE). Finally, EDUs and higher-order dis-
course units are connected by discourse relations
(e.g., DIS:ELABORATION).
</bodyText>
<subsectionHeader confidence="0.996159">
2.2 Kernels-based modeling
</subsectionHeader>
<bodyText confidence="0.999917375">
In the SKL framework, the learning objects are
pairs of translations ht1, t2i. Our objective is to
automatically learn which pair features are impor-
tant, independently of the source sentence. We
achieve this by using kernel machines (KMs) over
two learning objects ht1, t2i, hti, t2i, along with
an explicit and structural representation of the
pairs (see Fig. 1).
</bodyText>
<footnote confidence="0.96584">
1The discourse parser can be downloaded from
http://alt.qcri.org/tools/
</footnote>
<bodyText confidence="0.9711735">
More specifically, KMs carry out learning using
the scalar product
</bodyText>
<equation confidence="0.773917">
Kmt(ht1, t2i, ht�1, t�2i) = φmt(t1, t2) · φmt(t�1, t�2),
</equation>
<bodyText confidence="0.999985125">
where φmt maps pairs into the feature space.
Considering that our task is to decide whether
t1 is better than t2, we can conveniently rep-
resent the vector for the pair in terms of the
difference between the two translation vectors,
i.e., φmt(t1, t2) = φK(t1) − φK(t2). We can
approximate Kmt with a preference kernel PK to
compute this difference in the kernel space K:
</bodyText>
<equation confidence="0.998837666666667">
PK(ht1, t2i, hti, t2i) (1)
= K(t1) − φK(t2)) · (φK(ti) − φK(t2))
= K(t1, ti) + K(t2, t2) − K(t1, t2) − K(t2, ti)
</equation>
<bodyText confidence="0.999428615384616">
The advantage of this is that now K(ti, t�j) =
φK(ti) · φK(t�j) is defined between two transla-
tions only, and not between two pairs of transla-
tions. This simplification enables us to map trans-
lations into simple trees, e.g., those in Figure 1,
and then to apply them tree kernels, e.g., the Par-
tial Tree Kernel (Moschitti, 2006), which carry out
a scalar product in the subtree space.
We can further enrich the representation φK, if
we consider all the information available to the
human judges when they are ranking translations.
That is, the two alternative translations along with
their corresponding reference.
</bodyText>
<page confidence="0.994818">
216
</page>
<bodyText confidence="0.990463020833333">
In particular, let r and r0 be the references for
the pairs (t1, t2) and (t01, t02), we can redefine all
the members of Eq. 1, e.g., K(t1, t01) becomes
K((t1, r), (t01, r0)) = PTK(OM(t1, r), OM(t01, r0))
+ PTK(OM(r, t1), OM(r0, t01)),
where OM maps a pair of texts to a single tree.
There are several options to produce the bitext-
to-tree mapping for OM. A simple approach is
to only use the tree corresponding to the first ar-
gument of OM. This leads to the basic model
K((t1,r),(t01,r0)) = PTK(OM(t1), OM(t01)) +
PTK(OM(r), OM(r0)), i.e., the sum of two tree
kernels applied to the trees constructed by OM (we
previously informally mentioned it).
However, this simple mapping may be ineffec-
tive since the trees within a pair, e.g., (t1, r), are
treated independently, and no meaningful features
connecting t1 and r can be derived from their
tree fragments. Therefore, we model OM(r, t1) by
using word-matching relations between t1 and r,
such that connections between words and con-
stituents of the two trees are established using
position-independent word matching. For exam-
ple, in Figure 1, the thin dashed arrows show the
links connecting the matching words between t1
and r. The propagation of these relations works
from the bottom up. Thus, if all children in a con-
stituent have a link, their parent is also linked.
The use of such connections is essential as it en-
ables the comparison of the structural properties
and relations between two translation-reference
pairs. For example, the tree fragment [ELABORA-
TION [SATELLITE]] from the translation is con-
nected to [ELABORATION [SATELLITE]] in the
reference, indicating a link between two entire dis-
course units (drawn with a thicker arrow), and pro-
viding some reliability to the translation2.
Note that the use of connections yields a graph
representation instead of a tree. This is problem-
atic as effective models for graph kernels, which
would be a natural fit to this problem, are not cur-
rently available for exploiting linguistic informa-
tion. Thus, we simply use K, as defined above,
where the mapping OM(t1, r) only produces a tree
for t1 annotated with the marker REL represent-
ing the connections to r. This marker is placed on
all node labels of the tree generated from t1 that
match labels from the tree generated from r.
</bodyText>
<footnote confidence="0.664807666666667">
2Note that a non-pairwise model, i.e., K(t1, r), could
also be used to match the structural information above, but
it would not learn to compare it to a second pair (t2, r).
</footnote>
<bodyText confidence="0.912215666666667">
In other words, we only consider the trees en-
riched by markers separately, and ignore the edges
connecting both trees.
</bodyText>
<sectionHeader confidence="0.997245" genericHeader="evaluation">
3 Experiments and Discussion
</sectionHeader>
<bodyText confidence="0.99597023255814">
We experimented with datasets of segment-level
human rankings of system outputs from the
WMT11 and the WMT12 Metrics shared tasks
(Callison-Burch et al., 2011; Callison-Burch et al.,
2012): we used the WMT11 dataset for training
and the WMT12 dataset for testing. We focused
on translating into English only, for which the
datasets can be split by source language: Czech
(cs), German (de), Spanish (es), and French (fr).
There were about 10,000 non-tied human judg-
ments per language pair per dataset. We scored
our pairwise system predictions with respect to
the WMT12 human judgments using the Kendall’s
Tau (T), which was official at WMT12.
Table 1 presents the T scores for all metric vari-
ants introduced in this paper: for the individual
language pairs and overall. The left-hand side of
the table shows the results when using as sim-
ilarity the direct kernel calculation between the
corresponding structures of the candidate transla-
tion and the reference3, e.g., as in (Guzm´an et al.,
2014; Joty et al., 2014). The right-hand side con-
tains the results for structured kernel learning.
We can make the following observations:
(i) The overall results for all SKL-trained metrics
are higher than the ones when applying direct sim-
ilarity, showing that learning tree structures is bet-
ter than just calculating similarity.
(ii) Regarding the linguistic representation, we see
that, when learning tree structures, syntactic and
discourse-based trees yield similar improvements
with a slight advantage for the former. More in-
terestingly, when both structures are put together
in a combined tree, the improvement is cumula-
tive and yields the best results by a sizable margin.
This provides positive evidence towards our goal
of a unified tree-based representation with multi-
ple layers of linguistic information.
(iii) Comparing to the best evaluation metrics
that participated in the WMT12 Metrics shared
task, we find that our approach is competitive and
would have been ranked among the top 3 partici-
pants.
</bodyText>
<footnote confidence="0.98921725">
3Applying tree kernels between the members of a pair to
generate one feature (for each different kernel function) has
become a standard practice in text similarity tasks (Severyn et
al., 2013b) and in question answering (Severyn et al., 2013a).
</footnote>
<page confidence="0.990916">
217
</page>
<table confidence="0.999772666666667">
Similarity Structured Kernel Learning
Structure cs-en de-en es-en fr-en all cs-en de-en es-en fr-en all
1 SYN 0.169 0.188 0.203 0.222 0.195 0.190 0.244 0.198 0.158 0.198
2 DIS 0.130 0.174 0.188 0.169 0.165 0.176 0.235 0.166 0.160 0.184
3 DIS+POS 0.135 0.186 0.190 0.178 0.172 0.167 0.232 0.202 0.133 0.183
4 DIS+SYN 0.156 0.205 0.206 0.203 0.192 0.210 0.251 0.240 0.223 0.231
</table>
<tableCaption confidence="0.999908">
Table 1: Kendall’s (τ) correlation with human judgements on WMT12 for each language pair.
</tableCaption>
<bodyText confidence="0.9992801">
Furthermore, our result (0.237) is ahead of the
correlation obtained by popular metrics such as
TER (0.217), NIST (0.214) and BLEU (0.185) at
WMT12. This is very encouraging and shows the
potential of our new proposal.
In this paper, we have presented only the first
exploratory results. Our approach can be easily
extended with richer linguistic structures and fur-
ther combined with some of the already existing
strong evaluation metrics.
</bodyText>
<table confidence="0.997704285714286">
Train cs-en de-en Testing fr-en all
es-en
1 cs-en 0.210 0.204 0.217 0.204 0.209
2 de-en 0.196 0.251 0.203 0.202 0.213
3 es-en 0.218 0.204 0.240 0.223 0.221
4 fr-en 0.203 0.218 0.224 0.223 0.217
5 all 0.231 0.258 0.226 0.232 0.237
</table>
<tableCaption confidence="0.981529">
Table 2: Kendall’s (τ) on WMT12 for cross-
</tableCaption>
<bodyText confidence="0.983071961538461">
language training with DIS+SYN.
Note that the results in Table 1 were for train-
ing on WMT11 and testing on WMT12 for each
language pair in isolation. Next, we study the im-
pact of the choice of training language pair. Ta-
ble 2 shows cross-language evaluation results for
DIS+SYN: lines 1-4 show results when training on
WMT11 for one language pair, and then testing for
each language pair of WMT12.
We can see that the overall differences in perfor-
mance (see the last column: all) when training on
different source languages are rather small, rang-
ing from 0.209 to 0.221, which suggests that our
approach is quite independent of the source lan-
guage used for training. Still, looking at individ-
ual test languages, we can see that for de-en and
es-en, it is best to train on the same language; this
also holds for fr-en, but there it is equally good
to train on es-en. Interestingly, training on es-en
improves a bit for cs-en.
These somewhat mixed results have motivated
us to try tuning on the full WMT11 dataset; as line
5 shows, this yielded improvements for all lan-
guage pairs except for es-en. Comparing to line
4 in Table 1, we see that the overall Tau improved
from 0.231 to 0.237.
</bodyText>
<sectionHeader confidence="0.993691" genericHeader="conclusions">
4 Conclusions and Future Work
</sectionHeader>
<bodyText confidence="0.999985205882353">
We have presented a pairwise learning-to-rank ap-
proach to MT evaluation, which learns to differen-
tiate good from bad translations in the context of
a given reference. We have integrated several lay-
ers of linguistic information (lexical, syntactic and
discourse) in tree-based structures, and we have
used the structured kernel learning to identify rel-
evant features and learn pairwise rankers.
The evaluation results have shown that learning
in the proposed SKL framework is possible, yield-
ing better correlation (Kendall’s τ) with human
judgments than computing the direct kernel sim-
ilarity between translation and reference, over the
same type of structures. We have also shown that
the contributions of syntax and discourse informa-
tion are cumulative, indicating that this learning
framework can be appropriate for the combination
of different sources of information. Finally, de-
spite the limited information we used, we achieved
better correlation at the segment level than BLEU
and other metrics in the WMT12 Metrics task.
In the future, we plan to work towards our long-
term goal, i.e., including more linguistic informa-
tion in the SKL framework and showing that this
can help. This would also include more semantic
information, e.g., in the form of Brown clusters or
using semantic similarity between the words com-
posing the structure calculated with latent seman-
tic analysis (Saleh et al., 2014b).
We further want to show that the proposed
framework is flexible and can include information
in the form of quality scores predicted by other
evaluation metrics, for which a vector of features
would be combined with the structured kernel.
</bodyText>
<sectionHeader confidence="0.991822" genericHeader="acknowledgments">
Acknowledgments
</sectionHeader>
<bodyText confidence="0.9033572">
This research is part of the Interactive sYstems
for Answer Search (Iyas) project, conducted by
the Arabic Language Technologies (ALT) group
at Qatar Computing Research Institute (QCRI)
within the Qatar Foundation.
</bodyText>
<page confidence="0.998033">
218
</page>
<sectionHeader confidence="0.984209" genericHeader="references">
References
</sectionHeader>
<reference confidence="0.998999540540541">
Joshua Albrecht and Rebecca Hwa. 2008. Regression
for machine translation evaluation at the sentence
level. Machine Translation, 22(1-2):1–27.
Chris Callison-Burch, Cameron Fordyce, Philipp
Koehn, Christof Monz, and Josh Schroeder. 2007.
(Meta-) evaluation of machine translation. In Pro-
ceedings of the Second Workshop on Statistical
Machine Translation, WMT ’07, pages 136–158,
Prague, Czech Republic.
Chris Callison-Burch, Philipp Koehn, Christof Monz,
and Omar Zaidan. 2011. Findings of the 2011
workshop on statistical machine translation. In Pro-
ceedings of the Sixth Workshop on Statistical Ma-
chine Translation, WMT ’11, pages 22–64, Edin-
burgh, Scotland, UK.
Chris Callison-Burch, Philipp Koehn, Christof Monz,
Matt Post, Radu Soricut, and Lucia Specia. 2012.
Findings of the 2012 workshop on statistical ma-
chine translation. In Proceedings of the Seventh
Workshop on Statistical Machine Translation, WMT
’12, pages 10–51, Montr´eal, Canada.
Elisabet Comelles, Jes´us Gim´enez, Llu´ıs M`arquez,
Irene Castell´on, and Victoria Arranz. 2010.
Document-level automatic MT evaluation based on
discourse representations. In Proceedings of the
Joint Fifth Workshop on Statistical Machine Trans-
lation and MetricsMATR, WMT ’10, pages 333–
338, Uppsala, Sweden.
Kevin Duh. 2008. Ranking vs. regression in machine
translation evaluation. In Proceedings of the Third
Workshop on Statistical Machine Translation, WMT
’08, pages 191–194, Columbus, Ohio, USA.
Jes´us Gim´enez and Llu´ıs M`arquez. 2007. Linguis-
tic features for automatic evaluation of heterogenous
MT systems. In Proceedings of the Second Work-
shop on Statistical Machine Translation, WMT ’07,
pages 256–264, Prague, Czech Republic.
Francisco Guzm´an, Shafiq Joty, Llu´ıs M`arquez, and
Preslav Nakov. 2014. Using discourse structure
improves machine translation evaluation. In Pro-
ceedings of 52nd Annual Meeting of the Association
for Computational Linguistics, ACL ’14, pages 687–
698, Baltimore, Maryland, USA.
Shafiq Joty, Giuseppe Carenini, and Raymond Ng.
2012. A Novel Discriminative Framework for
Sentence-Level Discourse Analysis. In Proceedings
of the 2012 Joint Conference on Empirical Methods
in Natural Language Processing and Computational
Natural Language Learning, EMNLP-CoNLL ’12,
pages 904–915, Jeju Island, Korea.
Shafiq Joty, Giuseppe Carenini, Raymond Ng, and
Yashar Mehdad. 2013. Combining Intra- and
Multi-sentential Rhetorical Parsing for Document-
level Discourse Analysis. In Proceedings of the
51st Annual Meeting of the Association for Compu-
tational Linguistics, ACL ’13, pages 486–496, Sofia,
Bulgaria.
Shafiq Joty, Francisco Guzm´an, Llu´ıs M`arquez, and
Preslav Nakov. 2014. DiscoTK: Using discourse
structure for machine translation evaluation. In Pro-
ceedings of the Ninth Workshop on Statistical Ma-
chine Translation, WMT ’14, pages 402–408, Balti-
more, Maryland, USA.
Alon Lavie and Michael Denkowski. 2009. The ME-
TEOR metric for automatic evaluation of machine
translation. Machine Translation, 23(2–3):105–115.
Ding Liu and Daniel Gildea. 2005. Syntactic fea-
tures for evaluation of machine translation. In Pro-
ceedings of the ACL Workshop on Intrinsic and Ex-
trinsic Evaluation Measures for Machine Transla-
tion and/or Summarization, pages 25–32, Ann Ar-
bor, Michigan, USA.
Chi-kiu Lo, Anand Karthik Tumuluru, and Dekai Wu.
2012. Fully automatic semantic MT evaluation. In
Proceedings of the Seventh Workshop on Statisti-
cal Machine Translation, WMT ’12, pages 243–252,
Montr´eal, Canada.
Alessandro Moschitti, Silvia Quarteroni, Roberto
Basili, and Suresh Manandhar. 2007. Exploiting
syntactic and shallow semantic kernels for ques-
tion answer classification. In Proceedings of the
45th Annual Meeting of the Association of Computa-
tional Linguistics, ACL ’07, pages 776–783, Prague,
Czech Republic.
Alessandro Moschitti. 2006. Efficient convolution ker-
nels for dependency and constituent syntactic trees.
In Proceedings of 17th European Conference on Ma-
chine Learning and the 10th European Conference
on Principles and Practice of Knowledge Discovery
in Databases, ECML/PKDD ’06, pages 318–329,
Berlin, Germany.
Alessandro Moschitti. 2008. Kernel methods, syn-
tax and semantics for relational text categorization.
In Proceedings of the 17th ACM Conference on In-
formation and Knowledge Management, CIKM ’08,
pages 253–262, Napa Valley, California, USA.
Kishore Papineni, Salim Roukos, Todd Ward, and Wei-
Jing Zhu. 2002. BLEU: a method for automatic
evaluation of machine translation. In Proceedings
of 40th Annual Meting of the Association for Com-
putational Linguistics, ACL ’02, pages 311–318,
Philadelphia, Pennsylvania, USA.
Maja Popovi´c and Hermann Ney. 2007. Word error
rates: Decomposition over POS classes and applica-
tions for error analysis. In Proceedings of the Sec-
ond Workshop on Statistical Machine Translation,
WMT ’07, pages 48–55, Prague, Czech Republic.
Vasin Punyakanok and Dan Roth. 2001. The use of
classifiers in sequential inference. In Advances in
Neural Information Processing Systems 14, NIPS
’01, pages 995–1001, Vancouver, Canada.
</reference>
<page confidence="0.986819">
219
</page>
<reference confidence="0.999889507936508">
Iman Saleh, Scott Cyphers, Jim Glass, Shafiq Joty,
Llu´ıs M`arquez, Alessandro Moschitti, and Preslav
Nakov. 2014a. A study of using syntactic and se-
mantic structures for concept segmentation and la-
beling. In Proceedings of the 25th International
Conference on Computational Linguistics, COLING
’14, pages 193–202, Dublin, Ireland.
Iman Saleh, Alessandro Moschitti, Preslav Nakov,
Llu´ıs M`arquez, and Shafiq Joty. 2014b. Semantic
kernels for semantic parsing. In Proceedings of the
Conference on Empirical Methods in Natural Lan-
guage Processing, EMNLP ’14, Doha, Qatar.
Aliaksei Severyn and Alessandro Moschitti. 2012.
Structural relationships for large-scale learning of
answer re-ranking. In Proceedings of the 35th Inter-
national ACM SIGIR Conference on Research and
Development in Information Retrieval, SIGIR ’12,
pages 741–750, Portland, Oregon, USA.
Aliaksei Severyn, Massimo Nicosia, and Alessandro
Moschitti. 2013a. Learning adaptable patterns for
passage reranking. In Proceedings of the Seven-
teenth Conference on Computational Natural Lan-
guage Learning, CoNLL ’13, pages 75–83, Sofia,
Bulgaria.
Aliaksei Severyn, Massimo Nicosia, and Alessandro
Moschitti. 2013b. Learning semantic textual sim-
ilarity with structural representations. In Proceed-
ings of the 51st Annual Meeting of the Association
for Computational Linguistics (Volume 2: Short Pa-
pers), ACL ’13, pages 714–718, Sofia, Bulgaria.
Matthew Snover, Bonnie Dorr, Richard Schwartz, Lin-
nea Micciulla, and John Makhoul. 2006. A study of
translation edit rate with targeted human annotation.
In Proceedings of the 7th Biennial Conference of the
Association for Machine Translation in the Ameri-
cas, AMTA ’06, Cambridge, Massachusetts, USA.
Xingyi Song and Trevor Cohn. 2011. Regression and
ranking based optimisation for sentence-level MT
evaluation. In Proceedings of the Sixth Workshop
on Statistical Machine Translation, WMT ’11, pages
123–129, Edinburgh, Scotland, UK.
Kristina Toutanova, Dan Klein, Christopher Manning,
and Yoram Singer. 2003. Feature-rich part-of-
speech tagging with a cyclic dependency network.
In Proceedings of the 2003 Conference of the North
American Chapter of the Association for Computa-
tional Linguistics on Human Language Technology
- Volume 1, HLT-NAACL ’03, pages 173–180, Ed-
monton, Canada.
Billy Wong and Chunyu Kit. 2012. Extending ma-
chine translation evaluation metrics with lexical co-
hesion to document level. In Proceedings of the
2012 Joint Conference on Empirical Methods in
Natural Language Processing and Computational
Natural Language Learning, EMNLP-CoNLL ’12,
pages 1060–1068, Jeju Island, Korea.
Fabio Massimo Zanzotto and Alessandro Moschitti.
2006. Automatic learning of textual entailments
with cross-pair similarities. In Proceedings of the
21st International Conference on Computational
Linguistics and 44th Annual Meeting of the Associ-
ation for Computational Linguistics, COLING-ACL
’06, pages 401–408, Sydney, Australia.
</reference>
<page confidence="0.997593">
220
</page>
</variant>
</algorithm>
<algorithm name="ParsHed" version="110505">
<variant no="0" confidence="0.606874">
<title confidence="0.99997">Learning to Differentiate Better from Worse Translations</title>
<author confidence="0.883562">Francisco Guzm´an Shafiq Joty Lluis Alessandro Moschitti Preslav Nakov Massimo</author>
<affiliation confidence="0.9079125">ALT Research Qatar Computing Research Institute — Qatar</affiliation>
<abstract confidence="0.998405666666667">We present a pairwise learning-to-rank approach to machine translation evaluathat learns to differentiate in the context of a given reference. We integrate several layers of linguistic information encapsulated in tree-based structures, making use of both the reference and the system output simultaneously, thus bringing our ranking closer to how humans evaluate translations. Most importantly, instead of deciding upfront which types of features are important, we use the learning framework of preference re-ranking kernels to learn the features automatically. The evaluation results show that learning in the proposed framework yields better correlation with humans than computing the direct similarity over the same type of structures. Also, we show our structural kernel learning (SKL) can be a general framework for MT evaluation, in which syntactic and semantic information can be naturally incorporated.</abstract>
</variant>
</algorithm>
<algorithm name="ParsCit" version="110505">
<citationList>
<citation valid="true">
<authors>
<author>Joshua Albrecht</author>
<author>Rebecca Hwa</author>
</authors>
<title>Regression for machine translation evaluation at the sentence level.</title>
<date>2008</date>
<journal>Machine Translation,</journal>
<pages>22--1</pages>
<contexts>
<context position="2952" citStr="Albrecht and Hwa, 2008" startWordPosition="458" endWordPosition="461">007; Popovi´c and Ney, 2007; Liu and Gildea, 2005); semantic (Gim´enez and M`arquez, 2007; Lo et al., 2012); and discourse (Comelles et al., 2010; Wong and Kit, 2012; Guzm´an et al., 2014; Joty et al., 2014). Automatic MT evaluation metrics compare the output of a system to one or more human references in order to produce a similarity score. The quality of such a metric is typically judged in terms of correlation of the scores it produces with scores given by human judges. As a result, some evaluation metrics have been trained to reproduce the scores assigned by humans as closely as possible (Albrecht and Hwa, 2008). Unfortunately, humans have a hard time assigning an absolute score to a translation. Hence, direct human evaluation scores such as adequacy and fluency, which were widely used in the past, are now discontinued in favor of ranking-based evaluations, where judges are asked to rank the output of 2 to 5 systems instead. It has been shown that using such ranking-based assessments yields much higher inter-annotator agreement (CallisonBurch et al., 2007). While evaluation metrics still produce numerical scores, in part because MT evaluation shared tasks at NIST and WMT ask for it, there has also be</context>
</contexts>
<marker>Albrecht, Hwa, 2008</marker>
<rawString>Joshua Albrecht and Rebecca Hwa. 2008. Regression for machine translation evaluation at the sentence level. Machine Translation, 22(1-2):1–27.</rawString>
</citation>
<citation valid="true">
<authors>
<author>Chris Callison-Burch</author>
<author>Cameron Fordyce</author>
<author>Philipp Koehn</author>
<author>Christof Monz</author>
<author>Josh Schroeder</author>
</authors>
<title>(Meta-) evaluation of machine translation.</title>
<date>2007</date>
<booktitle>In Proceedings of the Second Workshop on Statistical Machine Translation, WMT ’07,</booktitle>
<pages>136--158</pages>
<location>Prague, Czech Republic.</location>
<marker>Callison-Burch, Fordyce, Koehn, Monz, Schroeder, 2007</marker>
<rawString>Chris Callison-Burch, Cameron Fordyce, Philipp Koehn, Christof Monz, and Josh Schroeder. 2007. (Meta-) evaluation of machine translation. In Proceedings of the Second Workshop on Statistical Machine Translation, WMT ’07, pages 136–158, Prague, Czech Republic.</rawString>
</citation>
<citation valid="true">
<authors>
<author>Chris Callison-Burch</author>
<author>Philipp Koehn</author>
<author>Christof Monz</author>
<author>Omar Zaidan</author>
</authors>
<title>Findings of the 2011 workshop on statistical machine translation.</title>
<date>2011</date>
<booktitle>In Proceedings of the Sixth Workshop on Statistical Machine Translation, WMT ’11,</booktitle>
<pages>22--64</pages>
<location>Edinburgh, Scotland, UK.</location>
<contexts>
<context position="14375" citStr="Callison-Burch et al., 2011" startWordPosition="2314" endWordPosition="2317">ing the connections to r. This marker is placed on all node labels of the tree generated from t1 that match labels from the tree generated from r. 2Note that a non-pairwise model, i.e., K(t1, r), could also be used to match the structural information above, but it would not learn to compare it to a second pair (t2, r). In other words, we only consider the trees enriched by markers separately, and ignore the edges connecting both trees. 3 Experiments and Discussion We experimented with datasets of segment-level human rankings of system outputs from the WMT11 and the WMT12 Metrics shared tasks (Callison-Burch et al., 2011; Callison-Burch et al., 2012): we used the WMT11 dataset for training and the WMT12 dataset for testing. We focused on translating into English only, for which the datasets can be split by source language: Czech (cs), German (de), Spanish (es), and French (fr). There were about 10,000 non-tied human judgments per language pair per dataset. We scored our pairwise system predictions with respect to the WMT12 human judgments using the Kendall’s Tau (T), which was official at WMT12. Table 1 presents the T scores for all metric variants introduced in this paper: for the individual language pairs a</context>
</contexts>
<marker>Callison-Burch, Koehn, Monz, Zaidan, 2011</marker>
<rawString>Chris Callison-Burch, Philipp Koehn, Christof Monz, and Omar Zaidan. 2011. Findings of the 2011 workshop on statistical machine translation. In Proceedings of the Sixth Workshop on Statistical Machine Translation, WMT ’11, pages 22–64, Edinburgh, Scotland, UK.</rawString>
</citation>
<citation valid="true">
<authors>
<author>Chris Callison-Burch</author>
<author>Philipp Koehn</author>
<author>Christof Monz</author>
<author>Matt Post</author>
<author>Radu Soricut</author>
<author>Lucia Specia</author>
</authors>
<title>Findings of the 2012 workshop on statistical machine translation.</title>
<date>2012</date>
<booktitle>In Proceedings of the Seventh Workshop on Statistical Machine Translation, WMT ’12,</booktitle>
<pages>10--51</pages>
<location>Montr´eal, Canada.</location>
<contexts>
<context position="7108" citStr="Callison-Burch et al., 2012" startWordPosition="1109" endWordPosition="1112">course structures. We show that learning in the proposed framework yields better correlation with humans than applying the traditional translation–reference similarity metrics using the same type of structures. We also show that the contributions of syntax and discourse information are cumulative. Finally, despite the limited information we use, we achieve correlation at the segment level that outperforms BLEU and other metrics at WMT12, e.g., our metric would have been ranked higher in terms of correlation with human judgments compared to TER, NIST, and BLEU in the WMT12 Metrics shared task (Callison-Burch et al., 2012). 2 Kernel-based Learning from Linguistic Structures In our pairwise setting, each sentence s in the source language is represented by a tuple (t1, t2, r), where t1 and t2 are two alternative translations and r is a reference translation. Our goal is to develop a classifier of such tuples that decides whether t1 is a better translation than t2 given the reference r. Engineering features for deciding whether t1 is a better translation than t2 is a difficult task. Thus, we rely on the automatic feature extraction enabled by the SKL framework, and our task is reduced to choosing: (i) a meaningful</context>
<context position="14405" citStr="Callison-Burch et al., 2012" startWordPosition="2318" endWordPosition="2321">s marker is placed on all node labels of the tree generated from t1 that match labels from the tree generated from r. 2Note that a non-pairwise model, i.e., K(t1, r), could also be used to match the structural information above, but it would not learn to compare it to a second pair (t2, r). In other words, we only consider the trees enriched by markers separately, and ignore the edges connecting both trees. 3 Experiments and Discussion We experimented with datasets of segment-level human rankings of system outputs from the WMT11 and the WMT12 Metrics shared tasks (Callison-Burch et al., 2011; Callison-Burch et al., 2012): we used the WMT11 dataset for training and the WMT12 dataset for testing. We focused on translating into English only, for which the datasets can be split by source language: Czech (cs), German (de), Spanish (es), and French (fr). There were about 10,000 non-tied human judgments per language pair per dataset. We scored our pairwise system predictions with respect to the WMT12 human judgments using the Kendall’s Tau (T), which was official at WMT12. Table 1 presents the T scores for all metric variants introduced in this paper: for the individual language pairs and overall. The left-hand side</context>
</contexts>
<marker>Callison-Burch, Koehn, Monz, Post, Soricut, Specia, 2012</marker>
<rawString>Chris Callison-Burch, Philipp Koehn, Christof Monz, Matt Post, Radu Soricut, and Lucia Specia. 2012. Findings of the 2012 workshop on statistical machine translation. In Proceedings of the Seventh Workshop on Statistical Machine Translation, WMT ’12, pages 10–51, Montr´eal, Canada.</rawString>
</citation>
<citation valid="false">
<authors>
<author>Elisabet Comelles</author>
<author>Jes´us Gim´enez</author>
</authors>
<location>Llu´ıs M`arquez,</location>
<marker>Comelles, Gim´enez, </marker>
<rawString>Elisabet Comelles, Jes´us Gim´enez, Llu´ıs M`arquez,</rawString>
</citation>
<citation valid="true">
<authors>
<author>Irene Castell´on</author>
<author>Victoria Arranz</author>
</authors>
<title>Document-level automatic MT evaluation based on discourse representations.</title>
<date>2010</date>
<booktitle>In Proceedings of the Joint Fifth Workshop on Statistical Machine Translation and MetricsMATR, WMT ’10,</booktitle>
<pages>333--338</pages>
<location>Uppsala,</location>
<marker>Castell´on, Arranz, 2010</marker>
<rawString>Irene Castell´on, and Victoria Arranz. 2010. Document-level automatic MT evaluation based on discourse representations. In Proceedings of the Joint Fifth Workshop on Statistical Machine Translation and MetricsMATR, WMT ’10, pages 333– 338, Uppsala, Sweden.</rawString>
</citation>
<citation valid="true">
<authors>
<author>Kevin Duh</author>
</authors>
<title>Ranking vs. regression in machine translation evaluation.</title>
<date>2008</date>
<booktitle>In Proceedings of the Third Workshop on Statistical Machine Translation, WMT ’08,</booktitle>
<pages>191--194</pages>
<location>Columbus, Ohio, USA.</location>
<contexts>
<context position="3713" citStr="Duh, 2008" startWordPosition="591" endWordPosition="592">y, which were widely used in the past, are now discontinued in favor of ranking-based evaluations, where judges are asked to rank the output of 2 to 5 systems instead. It has been shown that using such ranking-based assessments yields much higher inter-annotator agreement (CallisonBurch et al., 2007). While evaluation metrics still produce numerical scores, in part because MT evaluation shared tasks at NIST and WMT ask for it, there has also been work on a ranking formulation of the MT evaluation task for a given set of outputs. This was shown to yield higher correlation with human judgments (Duh, 2008; Song and Cohn, 2011). 214 Proceedings of the 2014 Conference on Empirical Methods in Natural Language Processing (EMNLP), pages 214–220, October 25-29, 2014, Doha, Qatar. c�2014 Association for Computational Linguistics Learning automatic metrics in a pairwise setting, i.e., learning to distinguish between two alternative translations and to decide which of the two is better (which is arguably one of the easiest ways to produce a ranking), emulates closely how human judges perform evaluation assessments in reality. Instead of learning a similarity function between a translation and the refer</context>
</contexts>
<marker>Duh, 2008</marker>
<rawString>Kevin Duh. 2008. Ranking vs. regression in machine translation evaluation. In Proceedings of the Third Workshop on Statistical Machine Translation, WMT ’08, pages 191–194, Columbus, Ohio, USA.</rawString>
</citation>
<citation valid="true">
<authors>
<author>Jes´us Gim´enez</author>
<author>Llu´ıs M`arquez</author>
</authors>
<title>Linguistic features for automatic evaluation of heterogenous MT systems.</title>
<date>2007</date>
<booktitle>In Proceedings of the Second Workshop on Statistical Machine Translation, WMT ’07,</booktitle>
<pages>256--264</pages>
<location>Prague, Czech Republic.</location>
<marker>Gim´enez, M`arquez, 2007</marker>
<rawString>Jes´us Gim´enez and Llu´ıs M`arquez. 2007. Linguistic features for automatic evaluation of heterogenous MT systems. In Proceedings of the Second Workshop on Statistical Machine Translation, WMT ’07, pages 256–264, Prague, Czech Republic.</rawString>
</citation>
<citation valid="true">
<authors>
<author>Francisco Guzm´an</author>
<author>Shafiq Joty</author>
<author>Llu´ıs M`arquez</author>
<author>Preslav Nakov</author>
</authors>
<title>Using discourse structure improves machine translation evaluation.</title>
<date>2014</date>
<booktitle>In Proceedings of 52nd Annual Meeting of the Association for Computational Linguistics, ACL ’14,</booktitle>
<pages>687--698</pages>
<location>Baltimore, Maryland, USA.</location>
<marker>Guzm´an, Joty, M`arquez, Nakov, 2014</marker>
<rawString>Francisco Guzm´an, Shafiq Joty, Llu´ıs M`arquez, and Preslav Nakov. 2014. Using discourse structure improves machine translation evaluation. In Proceedings of 52nd Annual Meeting of the Association for Computational Linguistics, ACL ’14, pages 687– 698, Baltimore, Maryland, USA.</rawString>
</citation>
<citation valid="true">
<authors>
<author>Shafiq Joty</author>
<author>Giuseppe Carenini</author>
<author>Raymond Ng</author>
</authors>
<title>A Novel Discriminative Framework for Sentence-Level Discourse Analysis.</title>
<date>2012</date>
<booktitle>In Proceedings of the 2012 Joint Conference on Empirical Methods in Natural Language Processing and Computational Natural Language Learning, EMNLP-CoNLL ’12,</booktitle>
<pages>904--915</pages>
<location>Jeju Island,</location>
<contexts>
<context position="9503" citStr="Joty et al., 2012" startWordPosition="1493" endWordPosition="1496">o give them the time to think . &amp;quot; Bag-of-words relations to &amp;quot; give them no time to think . &amp;quot; TO-REL VB-REL PRP-REL DT NN-REL TO-REL VB-REL .-REL &amp;quot;-REL VP NP-REL NP VP-REL o-REL o-REL EDU:NUCLEUS EDU:SATELLITE-REL b) Reference DIS:ELABORATION relation propagation direction Figure 1 shows two example trees combining discourse, shallow syntax and POS: one for a translation hypothesis (top) and the other one for the reference (bottom). To build such structures, we used the Stanford POS tagger (Toutanova et al., 2003), the Illinois chunker (Punyakanok and Roth, 2001), and the discourse parser1 of (Joty et al., 2012; Joty et al., 2013). The lexical items constitute the leaves of the tree. The words are connected to their respective POS tags, which are in turn grouped into chunks. Then, the chunks are grouped into elementary discourse units (EDU), to which the nuclearity status is attached (i.e., NUCLEUS or SATELLITE). Finally, EDUs and higher-order discourse units are connected by discourse relations (e.g., DIS:ELABORATION). 2.2 Kernels-based modeling In the SKL framework, the learning objects are pairs of translations ht1, t2i. Our objective is to automatically learn which pair features are important, i</context>
</contexts>
<marker>Joty, Carenini, Ng, 2012</marker>
<rawString>Shafiq Joty, Giuseppe Carenini, and Raymond Ng. 2012. A Novel Discriminative Framework for Sentence-Level Discourse Analysis. In Proceedings of the 2012 Joint Conference on Empirical Methods in Natural Language Processing and Computational Natural Language Learning, EMNLP-CoNLL ’12, pages 904–915, Jeju Island, Korea.</rawString>
</citation>
<citation valid="true">
<authors>
<author>Shafiq Joty</author>
<author>Giuseppe Carenini</author>
<author>Raymond Ng</author>
<author>Yashar Mehdad</author>
</authors>
<title>Combining Intra- and Multi-sentential Rhetorical Parsing for Documentlevel Discourse Analysis.</title>
<date>2013</date>
<booktitle>In Proceedings of the 51st Annual Meeting of the Association for Computational Linguistics, ACL ’13,</booktitle>
<pages>486--496</pages>
<location>Sofia, Bulgaria.</location>
<contexts>
<context position="9523" citStr="Joty et al., 2013" startWordPosition="1497" endWordPosition="1500">e to think . &amp;quot; Bag-of-words relations to &amp;quot; give them no time to think . &amp;quot; TO-REL VB-REL PRP-REL DT NN-REL TO-REL VB-REL .-REL &amp;quot;-REL VP NP-REL NP VP-REL o-REL o-REL EDU:NUCLEUS EDU:SATELLITE-REL b) Reference DIS:ELABORATION relation propagation direction Figure 1 shows two example trees combining discourse, shallow syntax and POS: one for a translation hypothesis (top) and the other one for the reference (bottom). To build such structures, we used the Stanford POS tagger (Toutanova et al., 2003), the Illinois chunker (Punyakanok and Roth, 2001), and the discourse parser1 of (Joty et al., 2012; Joty et al., 2013). The lexical items constitute the leaves of the tree. The words are connected to their respective POS tags, which are in turn grouped into chunks. Then, the chunks are grouped into elementary discourse units (EDU), to which the nuclearity status is attached (i.e., NUCLEUS or SATELLITE). Finally, EDUs and higher-order discourse units are connected by discourse relations (e.g., DIS:ELABORATION). 2.2 Kernels-based modeling In the SKL framework, the learning objects are pairs of translations ht1, t2i. Our objective is to automatically learn which pair features are important, independently of the </context>
</contexts>
<marker>Joty, Carenini, Ng, Mehdad, 2013</marker>
<rawString>Shafiq Joty, Giuseppe Carenini, Raymond Ng, and Yashar Mehdad. 2013. Combining Intra- and Multi-sentential Rhetorical Parsing for Documentlevel Discourse Analysis. In Proceedings of the 51st Annual Meeting of the Association for Computational Linguistics, ACL ’13, pages 486–496, Sofia, Bulgaria.</rawString>
</citation>
<citation valid="true">
<authors>
<author>Shafiq Joty</author>
<author>Francisco Guzm´an</author>
<author>Llu´ıs M`arquez</author>
<author>Preslav Nakov</author>
</authors>
<title>DiscoTK: Using discourse structure for machine translation evaluation.</title>
<date>2014</date>
<booktitle>In Proceedings of the Ninth Workshop on Statistical Machine Translation, WMT ’14,</booktitle>
<pages>402--408</pages>
<location>Baltimore, Maryland, USA.</location>
<marker>Joty, Guzm´an, M`arquez, Nakov, 2014</marker>
<rawString>Shafiq Joty, Francisco Guzm´an, Llu´ıs M`arquez, and Preslav Nakov. 2014. DiscoTK: Using discourse structure for machine translation evaluation. In Proceedings of the Ninth Workshop on Statistical Machine Translation, WMT ’14, pages 402–408, Baltimore, Maryland, USA.</rawString>
</citation>
<citation valid="true">
<authors>
<author>Alon Lavie</author>
<author>Michael Denkowski</author>
</authors>
<title>The METEOR metric for automatic evaluation of machine translation.</title>
<date>2009</date>
<journal>Machine Translation,</journal>
<pages>23--2</pages>
<contexts>
<context position="1599" citStr="Lavie and Denkowski, 2009" startWordPosition="226" endWordPosition="229">ter correlation with humans than computing the direct similarity over the same type of structures. Also, we show our structural kernel learning (SKL) can be a general framework for MT evaluation, in which syntactic and semantic information can be naturally incorporated. 1 Introduction We have seen in recent years fast improvement in the overall quality of machine translation (MT) systems. This was only possible because of the use of automatic metrics for MT evaluation, such as BLEU (Papineni et al., 2002), which is the defacto standard; and more recently: TER (Snover et al., 2006) and METEOR (Lavie and Denkowski, 2009), among other emerging MT evaluation metrics. These automatic metrics provide fast and inexpensive means to compare the output of different MT systems, without the need to ask for human judgments each time the MT system has been changed. As a result, this has enabled rapid development in the field of statistical machine translation (SMT), by allowing to train and tune systems as well as to track progress in a way that highly correlates with human judgments. Today, MT evaluation is an active field of research, and modern metrics perform analysis at various levels, e.g., lexical (Papineni et al.</context>
</contexts>
<marker>Lavie, Denkowski, 2009</marker>
<rawString>Alon Lavie and Michael Denkowski. 2009. The METEOR metric for automatic evaluation of machine translation. Machine Translation, 23(2–3):105–115.</rawString>
</citation>
<citation valid="true">
<authors>
<author>Ding Liu</author>
<author>Daniel Gildea</author>
</authors>
<title>Syntactic features for evaluation of machine translation.</title>
<date>2005</date>
<booktitle>In Proceedings of the ACL Workshop on Intrinsic and Extrinsic Evaluation Measures for Machine Translation and/or Summarization,</booktitle>
<pages>25--32</pages>
<location>Ann Arbor, Michigan, USA.</location>
<contexts>
<context position="2379" citStr="Liu and Gildea, 2005" startWordPosition="358" endWordPosition="361"> the need to ask for human judgments each time the MT system has been changed. As a result, this has enabled rapid development in the field of statistical machine translation (SMT), by allowing to train and tune systems as well as to track progress in a way that highly correlates with human judgments. Today, MT evaluation is an active field of research, and modern metrics perform analysis at various levels, e.g., lexical (Papineni et al., 2002; Snover et al., 2006), including synonymy and paraphrasing (Lavie and Denkowski, 2009); syntactic (Gim´enez and M`arquez, 2007; Popovi´c and Ney, 2007; Liu and Gildea, 2005); semantic (Gim´enez and M`arquez, 2007; Lo et al., 2012); and discourse (Comelles et al., 2010; Wong and Kit, 2012; Guzm´an et al., 2014; Joty et al., 2014). Automatic MT evaluation metrics compare the output of a system to one or more human references in order to produce a similarity score. The quality of such a metric is typically judged in terms of correlation of the scores it produces with scores given by human judges. As a result, some evaluation metrics have been trained to reproduce the scores assigned by humans as closely as possible (Albrecht and Hwa, 2008). Unfortunately, humans hav</context>
</contexts>
<marker>Liu, Gildea, 2005</marker>
<rawString>Ding Liu and Daniel Gildea. 2005. Syntactic features for evaluation of machine translation. In Proceedings of the ACL Workshop on Intrinsic and Extrinsic Evaluation Measures for Machine Translation and/or Summarization, pages 25–32, Ann Arbor, Michigan, USA.</rawString>
</citation>
<citation valid="true">
<authors>
<author>Chi-kiu Lo</author>
<author>Anand Karthik Tumuluru</author>
<author>Dekai Wu</author>
</authors>
<title>Fully automatic semantic MT evaluation.</title>
<date>2012</date>
<booktitle>In Proceedings of the Seventh Workshop on Statistical Machine Translation, WMT ’12,</booktitle>
<pages>243--252</pages>
<location>Montr´eal, Canada.</location>
<contexts>
<context position="2436" citStr="Lo et al., 2012" startWordPosition="367" endWordPosition="370">as been changed. As a result, this has enabled rapid development in the field of statistical machine translation (SMT), by allowing to train and tune systems as well as to track progress in a way that highly correlates with human judgments. Today, MT evaluation is an active field of research, and modern metrics perform analysis at various levels, e.g., lexical (Papineni et al., 2002; Snover et al., 2006), including synonymy and paraphrasing (Lavie and Denkowski, 2009); syntactic (Gim´enez and M`arquez, 2007; Popovi´c and Ney, 2007; Liu and Gildea, 2005); semantic (Gim´enez and M`arquez, 2007; Lo et al., 2012); and discourse (Comelles et al., 2010; Wong and Kit, 2012; Guzm´an et al., 2014; Joty et al., 2014). Automatic MT evaluation metrics compare the output of a system to one or more human references in order to produce a similarity score. The quality of such a metric is typically judged in terms of correlation of the scores it produces with scores given by human judges. As a result, some evaluation metrics have been trained to reproduce the scores assigned by humans as closely as possible (Albrecht and Hwa, 2008). Unfortunately, humans have a hard time assigning an absolute score to a translatio</context>
</contexts>
<marker>Lo, Tumuluru, Wu, 2012</marker>
<rawString>Chi-kiu Lo, Anand Karthik Tumuluru, and Dekai Wu. 2012. Fully automatic semantic MT evaluation. In Proceedings of the Seventh Workshop on Statistical Machine Translation, WMT ’12, pages 243–252, Montr´eal, Canada.</rawString>
</citation>
<citation valid="true">
<authors>
<author>Alessandro Moschitti</author>
<author>Silvia Quarteroni</author>
<author>Roberto Basili</author>
<author>Suresh Manandhar</author>
</authors>
<title>Exploiting syntactic and shallow semantic kernels for question answer classification.</title>
<date>2007</date>
<booktitle>In Proceedings of the 45th Annual Meeting of the Association of Computational Linguistics, ACL ’07,</booktitle>
<pages>776--783</pages>
<location>Prague, Czech Republic.</location>
<contexts>
<context position="5941" citStr="Moschitti et al., 2007" startWordPosition="926" endWordPosition="929">ework we propose consists in: (i) designing a structural representation, e.g., using syntactic and discourse trees of translation hypotheses and a references; and (ii) applying structural kernels (Moschitti, 2006; Moschitti, 2008), to such representations in order to automatically inject structural features in the preference re-ranking algorithm. We use this method with translation-reference pairs to directly learn the features themselves, instead of learning the importance of a predetermined set of features. A similar learning framework has been proven to be effective for question answering (Moschitti et al., 2007), and textual entailment recognition (Zanzotto and Moschitti, 2006). Our goals are twofold: (i) in the short term, to demonstrate that structural kernel learning is suitable for this task, and can effectively learn to rank hypotheses at the segment-level; and (ii) in the long term, to show that this approach provides a unified framework that allows to integrate several layers of linguistic analysis and information and to improve over the state-of-the-art. Below we report the results of some initial experiments using syntactic and discourse structures. We show that learning in the proposed fram</context>
</contexts>
<marker>Moschitti, Quarteroni, Basili, Manandhar, 2007</marker>
<rawString>Alessandro Moschitti, Silvia Quarteroni, Roberto Basili, and Suresh Manandhar. 2007. Exploiting syntactic and shallow semantic kernels for question answer classification. In Proceedings of the 45th Annual Meeting of the Association of Computational Linguistics, ACL ’07, pages 776–783, Prague, Czech Republic.</rawString>
</citation>
<citation valid="true">
<authors>
<author>Alessandro Moschitti</author>
</authors>
<title>Efficient convolution kernels for dependency and constituent syntactic trees.</title>
<date>2006</date>
<booktitle>In Proceedings of 17th European Conference on Machine Learning and the 10th European Conference on Principles and Practice of Knowledge Discovery in Databases, ECML/PKDD ’06,</booktitle>
<pages>318--329</pages>
<location>Berlin, Germany.</location>
<contexts>
<context position="5530" citStr="Moschitti, 2006" startWordPosition="866" endWordPosition="868"> information about both the reference and two alternative translations simultaneously, thus bringing our ranking closer to how humans rank translations. Finally, instead of deciding upfront which types of features between hypotheses and references are important, we use a our structural kernel learning (SKL) framework to generate and select them automatically. The structural kernel learning (SKL) framework we propose consists in: (i) designing a structural representation, e.g., using syntactic and discourse trees of translation hypotheses and a references; and (ii) applying structural kernels (Moschitti, 2006; Moschitti, 2008), to such representations in order to automatically inject structural features in the preference re-ranking algorithm. We use this method with translation-reference pairs to directly learn the features themselves, instead of learning the importance of a predetermined set of features. A similar learning framework has been proven to be effective for question answering (Moschitti et al., 2007), and textual entailment recognition (Zanzotto and Moschitti, 2006). Our goals are twofold: (i) in the short term, to demonstrate that structural kernel learning is suitable for this task, </context>
<context position="11320" citStr="Moschitti, 2006" startWordPosition="1801" endWordPosition="1802">between the two translation vectors, i.e., φmt(t1, t2) = φK(t1) − φK(t2). We can approximate Kmt with a preference kernel PK to compute this difference in the kernel space K: PK(ht1, t2i, hti, t2i) (1) = K(t1) − φK(t2)) · (φK(ti) − φK(t2)) = K(t1, ti) + K(t2, t2) − K(t1, t2) − K(t2, ti) The advantage of this is that now K(ti, t�j) = φK(ti) · φK(t�j) is defined between two translations only, and not between two pairs of translations. This simplification enables us to map translations into simple trees, e.g., those in Figure 1, and then to apply them tree kernels, e.g., the Partial Tree Kernel (Moschitti, 2006), which carry out a scalar product in the subtree space. We can further enrich the representation φK, if we consider all the information available to the human judges when they are ranking translations. That is, the two alternative translations along with their corresponding reference. 216 In particular, let r and r0 be the references for the pairs (t1, t2) and (t01, t02), we can redefine all the members of Eq. 1, e.g., K(t1, t01) becomes K((t1, r), (t01, r0)) = PTK(OM(t1, r), OM(t01, r0)) + PTK(OM(r, t1), OM(r0, t01)), where OM maps a pair of texts to a single tree. There are several options </context>
</contexts>
<marker>Moschitti, 2006</marker>
<rawString>Alessandro Moschitti. 2006. Efficient convolution kernels for dependency and constituent syntactic trees. In Proceedings of 17th European Conference on Machine Learning and the 10th European Conference on Principles and Practice of Knowledge Discovery in Databases, ECML/PKDD ’06, pages 318–329, Berlin, Germany.</rawString>
</citation>
<citation valid="true">
<authors>
<author>Alessandro Moschitti</author>
</authors>
<title>Kernel methods, syntax and semantics for relational text categorization.</title>
<date>2008</date>
<booktitle>In Proceedings of the 17th ACM Conference on Information and Knowledge Management, CIKM ’08,</booktitle>
<pages>253--262</pages>
<location>Napa Valley, California, USA.</location>
<contexts>
<context position="5548" citStr="Moschitti, 2008" startWordPosition="869" endWordPosition="870">t both the reference and two alternative translations simultaneously, thus bringing our ranking closer to how humans rank translations. Finally, instead of deciding upfront which types of features between hypotheses and references are important, we use a our structural kernel learning (SKL) framework to generate and select them automatically. The structural kernel learning (SKL) framework we propose consists in: (i) designing a structural representation, e.g., using syntactic and discourse trees of translation hypotheses and a references; and (ii) applying structural kernels (Moschitti, 2006; Moschitti, 2008), to such representations in order to automatically inject structural features in the preference re-ranking algorithm. We use this method with translation-reference pairs to directly learn the features themselves, instead of learning the importance of a predetermined set of features. A similar learning framework has been proven to be effective for question answering (Moschitti et al., 2007), and textual entailment recognition (Zanzotto and Moschitti, 2006). Our goals are twofold: (i) in the short term, to demonstrate that structural kernel learning is suitable for this task, and can effectivel</context>
</contexts>
<marker>Moschitti, 2008</marker>
<rawString>Alessandro Moschitti. 2008. Kernel methods, syntax and semantics for relational text categorization. In Proceedings of the 17th ACM Conference on Information and Knowledge Management, CIKM ’08, pages 253–262, Napa Valley, California, USA.</rawString>
</citation>
<citation valid="true">
<authors>
<author>Kishore Papineni</author>
<author>Salim Roukos</author>
<author>Todd Ward</author>
<author>WeiJing Zhu</author>
</authors>
<title>BLEU: a method for automatic evaluation of machine translation.</title>
<date>2002</date>
<booktitle>In Proceedings of 40th Annual Meting of the Association for Computational Linguistics, ACL ’02,</booktitle>
<pages>311--318</pages>
<location>Philadelphia, Pennsylvania, USA.</location>
<contexts>
<context position="1483" citStr="Papineni et al., 2002" startWordPosition="206" endWordPosition="209">learn the features automatically. The evaluation results show that learning in the proposed framework yields better correlation with humans than computing the direct similarity over the same type of structures. Also, we show our structural kernel learning (SKL) can be a general framework for MT evaluation, in which syntactic and semantic information can be naturally incorporated. 1 Introduction We have seen in recent years fast improvement in the overall quality of machine translation (MT) systems. This was only possible because of the use of automatic metrics for MT evaluation, such as BLEU (Papineni et al., 2002), which is the defacto standard; and more recently: TER (Snover et al., 2006) and METEOR (Lavie and Denkowski, 2009), among other emerging MT evaluation metrics. These automatic metrics provide fast and inexpensive means to compare the output of different MT systems, without the need to ask for human judgments each time the MT system has been changed. As a result, this has enabled rapid development in the field of statistical machine translation (SMT), by allowing to train and tune systems as well as to track progress in a way that highly correlates with human judgments. Today, MT evaluation i</context>
</contexts>
<marker>Papineni, Roukos, Ward, Zhu, 2002</marker>
<rawString>Kishore Papineni, Salim Roukos, Todd Ward, and WeiJing Zhu. 2002. BLEU: a method for automatic evaluation of machine translation. In Proceedings of 40th Annual Meting of the Association for Computational Linguistics, ACL ’02, pages 311–318, Philadelphia, Pennsylvania, USA.</rawString>
</citation>
<citation valid="true">
<authors>
<author>Maja Popovi´c</author>
<author>Hermann Ney</author>
</authors>
<title>Word error rates: Decomposition over POS classes and applications for error analysis.</title>
<date>2007</date>
<booktitle>In Proceedings of the Second Workshop on Statistical Machine Translation, WMT ’07,</booktitle>
<pages>48--55</pages>
<location>Prague, Czech Republic.</location>
<marker>Popovi´c, Ney, 2007</marker>
<rawString>Maja Popovi´c and Hermann Ney. 2007. Word error rates: Decomposition over POS classes and applications for error analysis. In Proceedings of the Second Workshop on Statistical Machine Translation, WMT ’07, pages 48–55, Prague, Czech Republic.</rawString>
</citation>
<citation valid="true">
<authors>
<author>Vasin Punyakanok</author>
<author>Dan Roth</author>
</authors>
<title>The use of classifiers in sequential inference.</title>
<date>2001</date>
<booktitle>In Advances in Neural Information Processing Systems 14, NIPS ’01,</booktitle>
<pages>995--1001</pages>
<location>Vancouver, Canada.</location>
<contexts>
<context position="9454" citStr="Punyakanok and Roth, 2001" startWordPosition="1484" endWordPosition="1487">L VB-REL PRP-REL DT NN-REL TO-REL VB-REL .-REL &amp;quot;-REL not to give them the time to think . &amp;quot; Bag-of-words relations to &amp;quot; give them no time to think . &amp;quot; TO-REL VB-REL PRP-REL DT NN-REL TO-REL VB-REL .-REL &amp;quot;-REL VP NP-REL NP VP-REL o-REL o-REL EDU:NUCLEUS EDU:SATELLITE-REL b) Reference DIS:ELABORATION relation propagation direction Figure 1 shows two example trees combining discourse, shallow syntax and POS: one for a translation hypothesis (top) and the other one for the reference (bottom). To build such structures, we used the Stanford POS tagger (Toutanova et al., 2003), the Illinois chunker (Punyakanok and Roth, 2001), and the discourse parser1 of (Joty et al., 2012; Joty et al., 2013). The lexical items constitute the leaves of the tree. The words are connected to their respective POS tags, which are in turn grouped into chunks. Then, the chunks are grouped into elementary discourse units (EDU), to which the nuclearity status is attached (i.e., NUCLEUS or SATELLITE). Finally, EDUs and higher-order discourse units are connected by discourse relations (e.g., DIS:ELABORATION). 2.2 Kernels-based modeling In the SKL framework, the learning objects are pairs of translations ht1, t2i. Our objective is to automat</context>
</contexts>
<marker>Punyakanok, Roth, 2001</marker>
<rawString>Vasin Punyakanok and Dan Roth. 2001. The use of classifiers in sequential inference. In Advances in Neural Information Processing Systems 14, NIPS ’01, pages 995–1001, Vancouver, Canada.</rawString>
</citation>
<citation valid="true">
<authors>
<author>Iman Saleh</author>
<author>Scott Cyphers</author>
<author>Jim Glass</author>
<author>Shafiq Joty</author>
<author>Llu´ıs M`arquez</author>
<author>Alessandro Moschitti</author>
<author>Preslav Nakov</author>
</authors>
<title>A study of using syntactic and semantic structures for concept segmentation and labeling.</title>
<date>2014</date>
<booktitle>In Proceedings of the 25th International Conference on Computational Linguistics, COLING ’14,</booktitle>
<pages>193--202</pages>
<location>Dublin, Ireland.</location>
<marker>Saleh, Cyphers, Glass, Joty, M`arquez, Moschitti, Nakov, 2014</marker>
<rawString>Iman Saleh, Scott Cyphers, Jim Glass, Shafiq Joty, Llu´ıs M`arquez, Alessandro Moschitti, and Preslav Nakov. 2014a. A study of using syntactic and semantic structures for concept segmentation and labeling. In Proceedings of the 25th International Conference on Computational Linguistics, COLING ’14, pages 193–202, Dublin, Ireland.</rawString>
</citation>
<citation valid="true">
<authors>
<author>Iman Saleh</author>
<author>Alessandro Moschitti</author>
<author>Preslav Nakov</author>
<author>Llu´ıs M`arquez</author>
<author>Shafiq Joty</author>
</authors>
<title>Semantic kernels for semantic parsing.</title>
<date>2014</date>
<booktitle>In Proceedings of the Conference on Empirical Methods in Natural Language Processing, EMNLP ’14,</booktitle>
<location>Doha, Qatar.</location>
<marker>Saleh, Moschitti, Nakov, M`arquez, Joty, 2014</marker>
<rawString>Iman Saleh, Alessandro Moschitti, Preslav Nakov, Llu´ıs M`arquez, and Shafiq Joty. 2014b. Semantic kernels for semantic parsing. In Proceedings of the Conference on Empirical Methods in Natural Language Processing, EMNLP ’14, Doha, Qatar.</rawString>
</citation>
<citation valid="true">
<authors>
<author>Aliaksei Severyn</author>
<author>Alessandro Moschitti</author>
</authors>
<title>Structural relationships for large-scale learning of answer re-ranking.</title>
<date>2012</date>
<booktitle>In Proceedings of the 35th International ACM SIGIR Conference on Research and Development in Information Retrieval, SIGIR ’12,</booktitle>
<pages>741--750</pages>
<location>Portland, Oregon, USA.</location>
<contexts>
<context position="8357" citStr="Severyn and Moschitti, 2012" startWordPosition="1314" endWordPosition="1317">ation for (t1, t2, r), and (ii) a feature function φ,,tt that maps such structures to substructures, i.e., our feature space. Since the design of φ,,tt is complex, we use tree kernels applied to two simpler structural mappings φM(t1, r) and φM(t2,r). The latter generate the tree representations for the translation-reference pairs (t1, r) and (t2, r). The next section shows such mappings. 2.1 Representations To represent a translation-reference pair (t, r), we adopt shallow syntactic trees combined with RSTstyle discourse trees. Shallow trees have been successfully used for question answering (Severyn and Moschitti, 2012) and semantic textual similarity (Severyn et al., 2013b); while discourse information has proved useful in MT evaluation (Guzm´an et al., 2014; Joty et al., 2014). Combined shallow syntax and discourse trees worked well for concept segmentation and labeling (Saleh et al., 2014a). 215 a) Hypothesis Figure 1: Hypothesis and reference trees combining discourse, shallow syntax and POS. DIS:ELABORATION EDU:NUCLEUS EDU:SATELLITE-REL VP NP-REL NP VP-REL o-REL o-REL RB TO-REL VB-REL PRP-REL DT NN-REL TO-REL VB-REL .-REL &amp;quot;-REL not to give them the time to think . &amp;quot; Bag-of-words relations to &amp;quot; give them</context>
</contexts>
<marker>Severyn, Moschitti, 2012</marker>
<rawString>Aliaksei Severyn and Alessandro Moschitti. 2012. Structural relationships for large-scale learning of answer re-ranking. In Proceedings of the 35th International ACM SIGIR Conference on Research and Development in Information Retrieval, SIGIR ’12, pages 741–750, Portland, Oregon, USA.</rawString>
</citation>
<citation valid="true">
<authors>
<author>Aliaksei Severyn</author>
<author>Massimo Nicosia</author>
<author>Alessandro Moschitti</author>
</authors>
<title>Learning adaptable patterns for passage reranking.</title>
<date>2013</date>
<booktitle>In Proceedings of the Seventeenth Conference on Computational Natural Language Learning, CoNLL ’13,</booktitle>
<pages>75--83</pages>
<location>Sofia, Bulgaria.</location>
<contexts>
<context position="8411" citStr="Severyn et al., 2013" startWordPosition="1323" endWordPosition="1326">maps such structures to substructures, i.e., our feature space. Since the design of φ,,tt is complex, we use tree kernels applied to two simpler structural mappings φM(t1, r) and φM(t2,r). The latter generate the tree representations for the translation-reference pairs (t1, r) and (t2, r). The next section shows such mappings. 2.1 Representations To represent a translation-reference pair (t, r), we adopt shallow syntactic trees combined with RSTstyle discourse trees. Shallow trees have been successfully used for question answering (Severyn and Moschitti, 2012) and semantic textual similarity (Severyn et al., 2013b); while discourse information has proved useful in MT evaluation (Guzm´an et al., 2014; Joty et al., 2014). Combined shallow syntax and discourse trees worked well for concept segmentation and labeling (Saleh et al., 2014a). 215 a) Hypothesis Figure 1: Hypothesis and reference trees combining discourse, shallow syntax and POS. DIS:ELABORATION EDU:NUCLEUS EDU:SATELLITE-REL VP NP-REL NP VP-REL o-REL o-REL RB TO-REL VB-REL PRP-REL DT NN-REL TO-REL VB-REL .-REL &amp;quot;-REL not to give them the time to think . &amp;quot; Bag-of-words relations to &amp;quot; give them no time to think . &amp;quot; TO-REL VB-REL PRP-REL DT NN-REL </context>
<context position="16412" citStr="Severyn et al., 2013" startWordPosition="2639" endWordPosition="2642">a combined tree, the improvement is cumulative and yields the best results by a sizable margin. This provides positive evidence towards our goal of a unified tree-based representation with multiple layers of linguistic information. (iii) Comparing to the best evaluation metrics that participated in the WMT12 Metrics shared task, we find that our approach is competitive and would have been ranked among the top 3 participants. 3Applying tree kernels between the members of a pair to generate one feature (for each different kernel function) has become a standard practice in text similarity tasks (Severyn et al., 2013b) and in question answering (Severyn et al., 2013a). 217 Similarity Structured Kernel Learning Structure cs-en de-en es-en fr-en all cs-en de-en es-en fr-en all 1 SYN 0.169 0.188 0.203 0.222 0.195 0.190 0.244 0.198 0.158 0.198 2 DIS 0.130 0.174 0.188 0.169 0.165 0.176 0.235 0.166 0.160 0.184 3 DIS+POS 0.135 0.186 0.190 0.178 0.172 0.167 0.232 0.202 0.133 0.183 4 DIS+SYN 0.156 0.205 0.206 0.203 0.192 0.210 0.251 0.240 0.223 0.231 Table 1: Kendall’s (τ) correlation with human judgements on WMT12 for each language pair. Furthermore, our result (0.237) is ahead of the correlation obtained by popu</context>
</contexts>
<marker>Severyn, Nicosia, Moschitti, 2013</marker>
<rawString>Aliaksei Severyn, Massimo Nicosia, and Alessandro Moschitti. 2013a. Learning adaptable patterns for passage reranking. In Proceedings of the Seventeenth Conference on Computational Natural Language Learning, CoNLL ’13, pages 75–83, Sofia, Bulgaria.</rawString>
</citation>
<citation valid="true">
<authors>
<author>Aliaksei Severyn</author>
<author>Massimo Nicosia</author>
<author>Alessandro Moschitti</author>
</authors>
<title>Learning semantic textual similarity with structural representations.</title>
<date>2013</date>
<booktitle>In Proceedings of the 51st Annual Meeting of the Association for Computational Linguistics (Volume 2: Short Papers), ACL ’13,</booktitle>
<pages>714--718</pages>
<location>Sofia, Bulgaria.</location>
<contexts>
<context position="8411" citStr="Severyn et al., 2013" startWordPosition="1323" endWordPosition="1326">maps such structures to substructures, i.e., our feature space. Since the design of φ,,tt is complex, we use tree kernels applied to two simpler structural mappings φM(t1, r) and φM(t2,r). The latter generate the tree representations for the translation-reference pairs (t1, r) and (t2, r). The next section shows such mappings. 2.1 Representations To represent a translation-reference pair (t, r), we adopt shallow syntactic trees combined with RSTstyle discourse trees. Shallow trees have been successfully used for question answering (Severyn and Moschitti, 2012) and semantic textual similarity (Severyn et al., 2013b); while discourse information has proved useful in MT evaluation (Guzm´an et al., 2014; Joty et al., 2014). Combined shallow syntax and discourse trees worked well for concept segmentation and labeling (Saleh et al., 2014a). 215 a) Hypothesis Figure 1: Hypothesis and reference trees combining discourse, shallow syntax and POS. DIS:ELABORATION EDU:NUCLEUS EDU:SATELLITE-REL VP NP-REL NP VP-REL o-REL o-REL RB TO-REL VB-REL PRP-REL DT NN-REL TO-REL VB-REL .-REL &amp;quot;-REL not to give them the time to think . &amp;quot; Bag-of-words relations to &amp;quot; give them no time to think . &amp;quot; TO-REL VB-REL PRP-REL DT NN-REL </context>
<context position="16412" citStr="Severyn et al., 2013" startWordPosition="2639" endWordPosition="2642">a combined tree, the improvement is cumulative and yields the best results by a sizable margin. This provides positive evidence towards our goal of a unified tree-based representation with multiple layers of linguistic information. (iii) Comparing to the best evaluation metrics that participated in the WMT12 Metrics shared task, we find that our approach is competitive and would have been ranked among the top 3 participants. 3Applying tree kernels between the members of a pair to generate one feature (for each different kernel function) has become a standard practice in text similarity tasks (Severyn et al., 2013b) and in question answering (Severyn et al., 2013a). 217 Similarity Structured Kernel Learning Structure cs-en de-en es-en fr-en all cs-en de-en es-en fr-en all 1 SYN 0.169 0.188 0.203 0.222 0.195 0.190 0.244 0.198 0.158 0.198 2 DIS 0.130 0.174 0.188 0.169 0.165 0.176 0.235 0.166 0.160 0.184 3 DIS+POS 0.135 0.186 0.190 0.178 0.172 0.167 0.232 0.202 0.133 0.183 4 DIS+SYN 0.156 0.205 0.206 0.203 0.192 0.210 0.251 0.240 0.223 0.231 Table 1: Kendall’s (τ) correlation with human judgements on WMT12 for each language pair. Furthermore, our result (0.237) is ahead of the correlation obtained by popu</context>
</contexts>
<marker>Severyn, Nicosia, Moschitti, 2013</marker>
<rawString>Aliaksei Severyn, Massimo Nicosia, and Alessandro Moschitti. 2013b. Learning semantic textual similarity with structural representations. In Proceedings of the 51st Annual Meeting of the Association for Computational Linguistics (Volume 2: Short Papers), ACL ’13, pages 714–718, Sofia, Bulgaria.</rawString>
</citation>
<citation valid="true">
<authors>
<author>Matthew Snover</author>
<author>Bonnie Dorr</author>
<author>Richard Schwartz</author>
<author>Linnea Micciulla</author>
<author>John Makhoul</author>
</authors>
<title>A study of translation edit rate with targeted human annotation.</title>
<date>2006</date>
<booktitle>In Proceedings of the 7th Biennial Conference of the Association for Machine Translation in the Americas, AMTA ’06,</booktitle>
<location>Cambridge, Massachusetts, USA.</location>
<contexts>
<context position="1560" citStr="Snover et al., 2006" startWordPosition="220" endWordPosition="223">the proposed framework yields better correlation with humans than computing the direct similarity over the same type of structures. Also, we show our structural kernel learning (SKL) can be a general framework for MT evaluation, in which syntactic and semantic information can be naturally incorporated. 1 Introduction We have seen in recent years fast improvement in the overall quality of machine translation (MT) systems. This was only possible because of the use of automatic metrics for MT evaluation, such as BLEU (Papineni et al., 2002), which is the defacto standard; and more recently: TER (Snover et al., 2006) and METEOR (Lavie and Denkowski, 2009), among other emerging MT evaluation metrics. These automatic metrics provide fast and inexpensive means to compare the output of different MT systems, without the need to ask for human judgments each time the MT system has been changed. As a result, this has enabled rapid development in the field of statistical machine translation (SMT), by allowing to train and tune systems as well as to track progress in a way that highly correlates with human judgments. Today, MT evaluation is an active field of research, and modern metrics perform analysis at various</context>
</contexts>
<marker>Snover, Dorr, Schwartz, Micciulla, Makhoul, 2006</marker>
<rawString>Matthew Snover, Bonnie Dorr, Richard Schwartz, Linnea Micciulla, and John Makhoul. 2006. A study of translation edit rate with targeted human annotation. In Proceedings of the 7th Biennial Conference of the Association for Machine Translation in the Americas, AMTA ’06, Cambridge, Massachusetts, USA.</rawString>
</citation>
<citation valid="true">
<authors>
<author>Xingyi Song</author>
<author>Trevor Cohn</author>
</authors>
<title>Regression and ranking based optimisation for sentence-level MT evaluation.</title>
<date>2011</date>
<booktitle>In Proceedings of the Sixth Workshop on Statistical Machine Translation, WMT ’11,</booktitle>
<pages>123--129</pages>
<location>Edinburgh, Scotland, UK.</location>
<contexts>
<context position="3735" citStr="Song and Cohn, 2011" startWordPosition="593" endWordPosition="596">re widely used in the past, are now discontinued in favor of ranking-based evaluations, where judges are asked to rank the output of 2 to 5 systems instead. It has been shown that using such ranking-based assessments yields much higher inter-annotator agreement (CallisonBurch et al., 2007). While evaluation metrics still produce numerical scores, in part because MT evaluation shared tasks at NIST and WMT ask for it, there has also been work on a ranking formulation of the MT evaluation task for a given set of outputs. This was shown to yield higher correlation with human judgments (Duh, 2008; Song and Cohn, 2011). 214 Proceedings of the 2014 Conference on Empirical Methods in Natural Language Processing (EMNLP), pages 214–220, October 25-29, 2014, Doha, Qatar. c�2014 Association for Computational Linguistics Learning automatic metrics in a pairwise setting, i.e., learning to distinguish between two alternative translations and to decide which of the two is better (which is arguably one of the easiest ways to produce a ranking), emulates closely how human judges perform evaluation assessments in reality. Instead of learning a similarity function between a translation and the reference, they learn how t</context>
</contexts>
<marker>Song, Cohn, 2011</marker>
<rawString>Xingyi Song and Trevor Cohn. 2011. Regression and ranking based optimisation for sentence-level MT evaluation. In Proceedings of the Sixth Workshop on Statistical Machine Translation, WMT ’11, pages 123–129, Edinburgh, Scotland, UK.</rawString>
</citation>
<citation valid="true">
<authors>
<author>Kristina Toutanova</author>
<author>Dan Klein</author>
<author>Christopher Manning</author>
<author>Yoram Singer</author>
</authors>
<title>Feature-rich part-ofspeech tagging with a cyclic dependency network.</title>
<date>2003</date>
<booktitle>In Proceedings of the 2003 Conference of the North American Chapter of the Association for Computational Linguistics on Human Language Technology - Volume 1, HLT-NAACL ’03,</booktitle>
<pages>173--180</pages>
<location>Edmonton, Canada.</location>
<contexts>
<context position="9404" citStr="Toutanova et al., 2003" startWordPosition="1477" endWordPosition="1480">TE-REL VP NP-REL NP VP-REL o-REL o-REL RB TO-REL VB-REL PRP-REL DT NN-REL TO-REL VB-REL .-REL &amp;quot;-REL not to give them the time to think . &amp;quot; Bag-of-words relations to &amp;quot; give them no time to think . &amp;quot; TO-REL VB-REL PRP-REL DT NN-REL TO-REL VB-REL .-REL &amp;quot;-REL VP NP-REL NP VP-REL o-REL o-REL EDU:NUCLEUS EDU:SATELLITE-REL b) Reference DIS:ELABORATION relation propagation direction Figure 1 shows two example trees combining discourse, shallow syntax and POS: one for a translation hypothesis (top) and the other one for the reference (bottom). To build such structures, we used the Stanford POS tagger (Toutanova et al., 2003), the Illinois chunker (Punyakanok and Roth, 2001), and the discourse parser1 of (Joty et al., 2012; Joty et al., 2013). The lexical items constitute the leaves of the tree. The words are connected to their respective POS tags, which are in turn grouped into chunks. Then, the chunks are grouped into elementary discourse units (EDU), to which the nuclearity status is attached (i.e., NUCLEUS or SATELLITE). Finally, EDUs and higher-order discourse units are connected by discourse relations (e.g., DIS:ELABORATION). 2.2 Kernels-based modeling In the SKL framework, the learning objects are pairs of </context>
</contexts>
<marker>Toutanova, Klein, Manning, Singer, 2003</marker>
<rawString>Kristina Toutanova, Dan Klein, Christopher Manning, and Yoram Singer. 2003. Feature-rich part-ofspeech tagging with a cyclic dependency network. In Proceedings of the 2003 Conference of the North American Chapter of the Association for Computational Linguistics on Human Language Technology - Volume 1, HLT-NAACL ’03, pages 173–180, Edmonton, Canada.</rawString>
</citation>
<citation valid="true">
<authors>
<author>Billy Wong</author>
<author>Chunyu Kit</author>
</authors>
<title>Extending machine translation evaluation metrics with lexical cohesion to document level.</title>
<date>2012</date>
<booktitle>In Proceedings of the 2012 Joint Conference on Empirical Methods in Natural Language Processing and Computational Natural Language Learning, EMNLP-CoNLL ’12,</booktitle>
<pages>1060--1068</pages>
<location>Jeju Island,</location>
<contexts>
<context position="2494" citStr="Wong and Kit, 2012" startWordPosition="377" endWordPosition="380">elopment in the field of statistical machine translation (SMT), by allowing to train and tune systems as well as to track progress in a way that highly correlates with human judgments. Today, MT evaluation is an active field of research, and modern metrics perform analysis at various levels, e.g., lexical (Papineni et al., 2002; Snover et al., 2006), including synonymy and paraphrasing (Lavie and Denkowski, 2009); syntactic (Gim´enez and M`arquez, 2007; Popovi´c and Ney, 2007; Liu and Gildea, 2005); semantic (Gim´enez and M`arquez, 2007; Lo et al., 2012); and discourse (Comelles et al., 2010; Wong and Kit, 2012; Guzm´an et al., 2014; Joty et al., 2014). Automatic MT evaluation metrics compare the output of a system to one or more human references in order to produce a similarity score. The quality of such a metric is typically judged in terms of correlation of the scores it produces with scores given by human judges. As a result, some evaluation metrics have been trained to reproduce the scores assigned by humans as closely as possible (Albrecht and Hwa, 2008). Unfortunately, humans have a hard time assigning an absolute score to a translation. Hence, direct human evaluation scores such as adequacy </context>
</contexts>
<marker>Wong, Kit, 2012</marker>
<rawString>Billy Wong and Chunyu Kit. 2012. Extending machine translation evaluation metrics with lexical cohesion to document level. In Proceedings of the 2012 Joint Conference on Empirical Methods in Natural Language Processing and Computational Natural Language Learning, EMNLP-CoNLL ’12, pages 1060–1068, Jeju Island, Korea.</rawString>
</citation>
<citation valid="true">
<authors>
<author>Fabio Massimo Zanzotto</author>
<author>Alessandro Moschitti</author>
</authors>
<title>Automatic learning of textual entailments with cross-pair similarities.</title>
<date>2006</date>
<booktitle>In Proceedings of the 21st International Conference on Computational Linguistics and 44th Annual Meeting of the Association for Computational Linguistics, COLING-ACL ’06,</booktitle>
<pages>401--408</pages>
<location>Sydney, Australia.</location>
<contexts>
<context position="6008" citStr="Zanzotto and Moschitti, 2006" startWordPosition="934" endWordPosition="937">esentation, e.g., using syntactic and discourse trees of translation hypotheses and a references; and (ii) applying structural kernels (Moschitti, 2006; Moschitti, 2008), to such representations in order to automatically inject structural features in the preference re-ranking algorithm. We use this method with translation-reference pairs to directly learn the features themselves, instead of learning the importance of a predetermined set of features. A similar learning framework has been proven to be effective for question answering (Moschitti et al., 2007), and textual entailment recognition (Zanzotto and Moschitti, 2006). Our goals are twofold: (i) in the short term, to demonstrate that structural kernel learning is suitable for this task, and can effectively learn to rank hypotheses at the segment-level; and (ii) in the long term, to show that this approach provides a unified framework that allows to integrate several layers of linguistic analysis and information and to improve over the state-of-the-art. Below we report the results of some initial experiments using syntactic and discourse structures. We show that learning in the proposed framework yields better correlation with humans than applying the tradi</context>
</contexts>
<marker>Zanzotto, Moschitti, 2006</marker>
<rawString>Fabio Massimo Zanzotto and Alessandro Moschitti. 2006. Automatic learning of textual entailments with cross-pair similarities. In Proceedings of the 21st International Conference on Computational Linguistics and 44th Annual Meeting of the Association for Computational Linguistics, COLING-ACL ’06, pages 401–408, Sydney, Australia.</rawString>
</citation>
</citationList>
</algorithm>
</algorithms>
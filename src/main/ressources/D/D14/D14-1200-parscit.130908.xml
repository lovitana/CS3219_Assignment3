<?xml version="1.0" encoding="UTF-8"?>
<algorithms version="110505">
<algorithm name="SectLabel" version="110505">
<variant no="0" confidence="0.000000">
<title confidence="0.672942">
Modeling Joint Entity and Relation Extraction with Table Representation
</title>
<author confidence="0.882974">
Makoto Miwa and Yutaka Sasaki
</author>
<affiliation confidence="0.921471">
Toyota Technological Institute
</affiliation>
<address confidence="0.785628">
2-12-1 Hisakata, Tempaku-ku, Nagoya, 468-8511, Japan
</address>
<figure confidence="0.855395">
{makoto-miwa, yutaka.sasaki}@toyota-ti.ac.jp
Abstract
Live_in
Live_in Located_in
</figure>
<bodyText confidence="0.999617071428571">
This paper proposes a history-based struc-
tured learning approach that jointly ex-
tracts entities and relations in a sentence.
We introduce a novel simple and flexible
table representation of entities and rela-
tions. We investigate several feature set-
tings, search orders, and learning meth-
ods with inexact search on the table. The
experimental results demonstrate that a
joint learning approach significantly out-
performs a pipeline approach by incorpo-
rating global features and by selecting ap-
propriate learning methods and search or-
ders.
</bodyText>
<sectionHeader confidence="0.999394" genericHeader="abstract">
1 Introduction
</sectionHeader>
<bodyText confidence="0.998765782608696">
Extraction of entities and relations from texts has
been traditionally treated as a pipeline of two sep-
arate subtasks: entity recognition and relation ex-
traction. This separation makes the task easy to
deal with, but it ignores underlying dependencies
between and within subtasks. First, since entity
recognition is not affected by relation extraction,
errors in entity recognition are propagated to re-
lation extraction. Second, relation extraction is
often treated as a multi-class classification prob-
lem on pairs of entities, so dependencies between
pairs are ignored. Examples of these dependen-
cies are illustrated in Figure 1. For dependencies
between subtasks, a Live in relation requires PER
and LOC entities, and vice versa. For in-subtask
dependencies, the Live in relation between “Mrs.
Tsutayama” and “Japan” can be inferred from the
two other relations.
Figure 1 also shows that the task has a flexible
graph structure. This structure usually does not
cover all the words in a sentence differently from
other natural language processing (NLP) tasks
such as part-of-speech (POS) tagging and depen-
</bodyText>
<figure confidence="0.5088845">
Mrs. Tsuruyama is from Kumamoto Prefecture in Japan .
PER LOC LOC
</figure>
<figureCaption confidence="0.77406425">
Figure 1: An entity and relation example (Roth
and Yih, 2004). Person (PER) and location (LOC)
entities are connected by Live in and Located in
relations.
</figureCaption>
<bodyText confidence="0.998066074074074">
dency parsing, so local constraints are considered
to be more important in the task.
Joint learning approaches (Yang and Cardie,
2013; Singh et al., 2013) incorporate these de-
pendencies and local constraints in their models;
however most approaches are time-consuming and
employ complex structures consisting of multi-
ple models. Li and Ji (2014) recently proposed
a history-based structured learning approach that
is simpler and more computationally efficient than
other approaches. While this approach is promis-
ing, it still has a complexity in search and restricts
the search order partly due to its semi-Markov rep-
resentation, and thus the potential of the history-
based learning is not fully investigated.
In this paper, we introduce an entity and relation
table to address the difficulty in representing the
task. We propose a joint extraction of entities and
relations using a history-based structured learning
on the table. This table representation simplifies
the task into a table-filling problem, and makes
the task flexible enough to incorporate several en-
hancements that have not been addressed in the
previous history-based approach, such as search
orders in decoding, global features from relations
to entities, and several learning methods with in-
exact search.
</bodyText>
<sectionHeader confidence="0.992379" genericHeader="keywords">
2 Method
</sectionHeader>
<bodyText confidence="0.9871125">
In this section, we first introduce an entity and re-
lation table that is utilized to represent the whole
</bodyText>
<page confidence="0.942321">
1858
</page>
<note confidence="0.8988405">
Proceedings of the 2014 Conference on Empirical Methods in Natural Language Processing (EMNLP), pages 1858–1869,
October 25-29, 2014, Doha, Qatar. c�2014 Association for Computational Linguistics
</note>
<bodyText confidence="0.998949">
entity and relation structures in a sentence. We
then overview our model on the table. We finally
explain the decoding, learning, search order, and
features in our model.
</bodyText>
<subsectionHeader confidence="0.963616">
2.1 Entity and relation table
</subsectionHeader>
<bodyText confidence="0.99998696969697">
The task we address in this work is the extraction
of entities and their relations from a sentence. En-
tities are typed and may span multiple words. Re-
lations are typed and directed.
We use words to represent entities and relations.
We assume entities do not overlap. We employ
a BILOU (Begin, Inside, Last, Outside, Unit) en-
coding scheme that has been shown to outperform
the traditional BIO scheme (Ratinov and Roth,
2009), and we will show that this scheme induces
several label dependencies between words and be-
tween words and relations in §2.3.2. A label is
assigned to a word according to the relative posi-
tion to its corresponding entity and the type of the
entity. Relations are represented with their types
and directions. 1 denotes a non-relation pair, and
—* and +— denote left-to-right and right-to-left re-
lations, respectively. Relations are defined on not
entities but words, since entities are not always
given when relations are extracted. Relations on
entities are mapped to relations on the last words
of the entities.
Based on this representation, we propose an en-
tity and relation table that jointly represents en-
tities and relations in a sentence. Figure 2 illus-
trates an entity and relation table corresponding to
an example in Figure 1. We use only the lower tri-
angular part because the table is symmetric, so the
number of cells is n(n + 1)/2 when there are n
words in a sentence. With this entity and relation
table representation, the joint extraction problem
can be mapped to a table-filling problem in that
labels are assigned to cells in the table.
</bodyText>
<subsectionHeader confidence="0.996861">
2.2 Model
</subsectionHeader>
<bodyText confidence="0.9999952">
We tackle the table-filling problem by a history-
based structured learning approach that assigns la-
bels to cells one by one. This is mostly the same as
the traditional history-based model (Collins, 2002)
except for the table representation.
Let x be an input table, Y(x) be all possible
assignments to the table, and s(x, y) be a scoring
function that assesses the assignment of y E Y(x)
to x. With these definitions, we define our model
to predict the most probable assignment as fol-
</bodyText>
<equation confidence="0.947950333333333">
lows:
y* = arg max s(x, y) (1)
YEY(X)
</equation>
<bodyText confidence="0.999469">
This scoring function is a decomposable function,
and each decomposed function assesses the as-
signment of a label to a cell in the table.
</bodyText>
<equation confidence="0.997566">
s(x, y) = ∑ |X |s(x, y, 1, i) (2)
Z=1
</equation>
<bodyText confidence="0.9992045">
Here, i represents an index of a cell in the table,
which will be explained in §2.3.1. The decom-
posed function s(x, y,1, i) corresponds to the i-th
cell. The decomposed function is represented as a
linear model, i.e., an inner product of features and
their corresponding weights.
</bodyText>
<equation confidence="0.7439464">
s(x, y,1, i) = w·f(x, y,1, i) (3)
The scoring function are further divided into two
functions as follows:
s(x, y, 1, i) = slocal(x, y, i) + sglobal(x, y,1, i)
(4)
</equation>
<bodyText confidence="0.996971928571429">
Here, slocal(x, y, i) is a local scoring func-
tion that assesses the assignment to the i-th
cell without considering other assignments, and
sglobal(x, y,1, i) is a global scoring function that
assesses the assignment in the context of 1st to
(i − 1)-th assignments. This global scoring func-
tion represents the dependencies between entities,
between relations, and between entities and rela-
tions. Similarly, features f are divided into local
features flocal and global features fglobal, and they
are defined on its target cell and surrounding con-
texts. The features will be explained in §2.5. The
weights w can also be divided, but they are tuned
jointly in learning as shown in §2.4.
</bodyText>
<subsectionHeader confidence="0.99934">
2.3 Decoding
</subsectionHeader>
<bodyText confidence="0.999969615384615">
The scoring function s(x, y,1, i) in Equation (2)
uses all the preceding assignments and does not
rely on the Markov assumption, so we cannot em-
ploy dynamic programming.
We instead employ a beam search to find the
best assignment with the highest score (Collins
and Roark, 2004). The beam search assigns la-
bels to cells one by one with keeping the top K
best assignments when moving from a cell to the
next cell, and it returns the best assignment when
labels are assigned to all the cells. The pseudo
code for decoding with the beam search is shown
in Figure 3.
</bodyText>
<page confidence="0.976407">
1859
</page>
<table confidence="0.9860886">
Mrs. Tsutayama is from Kumamoto Prefecture in Japan .
Mrs. B-PER
Tsutayama 1 L-PER
is 1 1 O
from 1 1 1 O
Kumamoto 1 1 1 1 B-LOC
Prefecture 1 Live in—* 1 1 1 L-LOC
in 1 1 1 1 1 1 O
Japan 1 Live in—* 1 1 1 Located in—* 1 U-LOC
. 1 1 1 1 1 1 1 1 1
</table>
<figureCaption confidence="0.991389">
Figure 2: The entity and relation table for the example in Figure 1.
</figureCaption>
<listItem confidence="0.930712733333333">
INPUT: x: input table with no assignment,
K: beam size
OUTPUT: best assignment y* for x
1: b +— [x]
2: for i = 1 to jxj do
3: T +— 0
4: for k = 1 to jbj do
5: for a E A(i, b[k]) do
6: T +— T U append(a, b[k])
7: end for
8: end for
9: b +— top K tables from T using the scoring
function in Equation (2)
10: end for
11: return b[0]
</listItem>
<figureCaption confidence="0.91469225">
Figure 3: Decoding with the beam search. A(i, t)
returns possible assignments for i-th cell of a table
t, and append(a, t) returns a table t updated with
an assignment a.
</figureCaption>
<figure confidence="0.999246935185186">
4
2 5
1 3 6
A B C
6
3
1 2 4
A B C
6
5
4 2 1
A B C
5
3
(a) Up to
down, left to
right
4
5 2
6 3 1
A B C
(b) Up to
down, right
to left
6
4
1 2 3
A B C
(c) Right to
left, up to
down
6
5
3 2 1
A B C
5
4
C
1
A
C
B
A
4
6
1
2
5
B
C
3
A
C
B
A
3
5
6
4
B
2
C
1
A
1
2
4
B
3
5
C
6
A
C
B
A
3
6
1
2
5
B
C
4
A
B
C
A
4
5
6
B
2
3
C
1
C
A
B
A
B
C
A
6
5
4
B
3
</figure>
<page confidence="0.550966">
2
</page>
<bodyText confidence="0.987385">
We explain how to map the table to a sequence
We explain how to map the table to a sequence
(line 2 in Figure 3), and how to calculate possible
(line 2 in Figure 3), and how to calculate possible (d) Right to (e) Close- (f) Close-
</bodyText>
<figure confidence="0.78141675">
(f) Close-
(d) Right to
(e) Close-
left, down to
</figure>
<figureCaption confidence="0.571556">
assignments (line 6 in Figure 3) in the following left, down to first, left to first, right to
assignments (line 6 in Figure 3) in the following
</figureCaption>
<bodyText confidence="0.721834">
first, right to
first, left to
left
up
right
subsections. up right left
subsections.
</bodyText>
<subsubsectionHeader confidence="0.428327">
2.3.1 Table-to-sequence mapping
</subsubsectionHeader>
<figureCaption confidence="0.7159515">
2.3.1 Table-to-sequence mapping Figure 4: Static search orders.
Figure 4: Static search orders.
</figureCaption>
<bodyText confidence="0.996356970588235">
Cells in an input table are originally indexed in
Cells in an input table are originally indexed in
two dimensions. To apply our model in §2.2 to the
two dimensions. To apply our model in §2.2 to the
cells, we need to map the two-dimensional table
fine two mappings (Figures 4(a) and 4(b)) with the
cells, we need to map the two-dimensional table fine two mappings (Figures 4(a) and 4(b)) with the
to a one-dimensional sequence. This is equivalent
highest priority on the “up to down” order, which
to a one-dimensional sequence. This is equivalent highest priority on the “up to down” order, which
to defining a search order in the table, so we will
checks a sentence forwardly (from the beginning
to defining a search order in the table, so we will checks a sentence forwardly (from the beginning
use the terms “mapping” and “search order” inter-
of a sentence). Similarly, we also define two map-
use the terms “mapping” and “search order” inter- of a sentence). Similarly, we also define two map-
changeably.
pings (Figures 4(c) and 4(d)) with the highest pri-
changeably. pings (Figures 4(c) and 4(d)) with the highest pri-
Since it is infeasible to try all possible map-
ority on the “right to left” order, which check a
Since it is infeasible to try all possible map- ority on the “right to left” order, which check a
pings, we define six promising static mappings
sentence backwardly (from the end of a sentence).
pings, we define six promising static mappings sentence backwardly (from the end of a sentence).
(search orders) as shown in Figure 4. Note that the
From another point of view, entities are detected
(search orders) as shown in Figure 4. Note that the From another point of view, entities are detected
“left” and “right” directions in the captions cor-
before relations in Figures 4(b) and 4(c) whereas
“left” and “right” directions in the captions cor- before relations in Figures 4(b) and 4(c) whereas
respond to not word orders, but tables. We de-
the order in a sentence is prioritized in Figures 4(a)
respond to not word orders, but tables. We de- the order in a sentence is prioritized in Figures 4(a)
</bodyText>
<page confidence="0.995567">
1860
</page>
<tableCaption confidence="0.9450285">
Table 1: Label dependencies from relations to en-
tities. * indicates any type.
</tableCaption>
<table confidence="0.999678">
Label on wi Relations from/to wi
B-*, I-*, O 1
L-*, U-* *
Label on wi+1 Relations from/to wi
I-*, L-* 1
B-*, U-*, O *
</table>
<tableCaption confidence="0.822261">
Table 2: Label dependencies from entities to rela-
tions.
</tableCaption>
<bodyText confidence="0.999229166666667">
and 4(d). We further define two close-first map-
pings (Figures 4(e) and 4(f)) since entities are
easier to find than relations and close relations are
easier to find than distant relations.
We also investigate dynamic mappings (search
orders) with an easy-first policy (Goldberg and El-
hadad, 2010). Dynamic mappings are different
from the static mappings above, since we reorder
the cells before each decoding1. We evaluate the
cells using the local scoring function, and assign
indices to the cells so that the cells with higher
scores have higher priorities. In addition to this
naive easy-first policy, we define two other dy-
namic mappings that restricts the reordering by
combining the easy-first policy with one of the fol-
lowing two policies: entity-first (all entities are de-
tected before relations) and close-first (closer cells
are detected before distant cells) policies.
</bodyText>
<subsectionHeader confidence="0.820671">
2.3.2 Label dependencies
</subsectionHeader>
<bodyText confidence="0.999436538461539">
To avoid illegal assignments to a table, we have
to restrict the possible assignments to the cells ac-
cording to the preceding assignments. This restric-
tion can also reduce the computational costs.
We consider all the dependencies between cells
to allow the assignments of labels to the cells in
an arbitrary order. Our representation of entities
and relations in §2.1 induces the dependencies be-
tween entities and between entities and relations.
Tables 1-3 summarize these dependencies on the i-
th word wi in a sentence. We can further utilize de-
pendencies between entity types and relation types
if some entity types are involved in a limited num-
</bodyText>
<footnote confidence="0.9855385">
1It is also possible to reorder the cells during decoding,
but it greatly increases the computational costs.
</footnote>
<table confidence="0.99998325">
Label on wi_2 Possible labels on wi
B-TYPE B-*, I-TYPE, L-TYPE, O, U-*
I-TYPE B-*, I-TYPE, L-TYPE, O, U-*
L-TYPE B-*, I-*, L-*, O, U-*
O B-*, I-*, L-*, O, U-*
U-TYPE B-*, I-*, L-*, O, U-*
O/S B-*, I-*, L-*, O, U-*
Label on wi_1 Possible labels on wi
B-TYPE I-TYPE, L-TYPE
I-TYPE I-TYPE, L-TYPE
L-TYPE B-*, O, U-*
O B-*, O, U-*
U-TYPE B-*, O, U-*
O/S B-*, O, U-*
Label on wi+1 Possible labels on wi
B-TYPE L-*, O, U-*
I-TYPE B-TYPE, I-TYPE
L-TYPE B-TYPE, I-TYPE
O L-*, O, U-*
U-TYPE L-*, O, U-*
O/S L-*, O, U-*
Label on wi+2 Possible labels on wi
B-TYPE B-*, I-*, L-*, O, U-*
I-TYPE B-TYPE, I-TYPE, L-*, O, U-*
L-TYPE B-TYPE, I-TYPE, L-*, O, U-*
O B-*, I-*, L-*, O, U-*
U-TYPE B-*, I-*, L-*, O, U-*
O/S B-*, I-*, L-*, O, U-*
</table>
<tableCaption confidence="0.9017395">
Table 3: Label dependencies between entities.
TYPE represents an entity type, and O/S means
</tableCaption>
<bodyText confidence="0.9687146">
the word is outside of a sentence.
ber of relation types or vice versa. We note that
the dependencies between entity types and rela-
tion types include not only words participating in
relations but also their surrounding words. For ex-
ample, the label on wi_1 can restrict the types of
relations involving wi. We employ these type de-
pendencies in the evaluation, but we omit these de-
pendencies here since these dependencies are de-
pendent on the tasks.
</bodyText>
<subsectionHeader confidence="0.994466">
2.4 Learning
</subsectionHeader>
<bodyText confidence="0.999912">
The goal of learning is to minimize errors between
predicted assignments y* and gold assignments
ygold by tuning the weights w in the scoring func-
tion in Equation 3. We employ a margin-based
structured learning approach to tune the weights
w. The pseudo code is shown in Figure 5. This ap-
proach enhances the traditional structured percep-
</bodyText>
<figure confidence="0.661363666666667">
Condition
Possible labels on wi
Relation(s) on wi_1
Relation(s) on wi
B-*, O, U-*
L-*, U-*
</figure>
<page confidence="0.932444">
1861
</page>
<bodyText confidence="0.871990666666667">
INPUT: training sets D = {(xi, yi)}N i=1,
T: iterations
OUTPUT: weights w
</bodyText>
<listItem confidence="0.981408214285714">
1: w +— 0
2: for t = 1 to T do
3: for x, y E D do
4: y* +— best assignment for x using decod-
ing in Figure 3 with s′ in Equation (5)
5: if y* =� ygold then
6: m +— arg maxi{s′(x, ygold, 1, i)�
s′(x, y*, 1, i)}
7: w +— update(w, f(x, ygold, 1, m),
f(x, y*,1, m))
8: end if
9: end for
10: end for
11: return w
</listItem>
<figureCaption confidence="0.5999165">
Figure 5: Margin-based structured learn-
ing approach with a max-violation update.
update(w, f(x, ygold, 1, m), f(x, y*, 1, m))
depends on employed learning methods.
</figureCaption>
<bodyText confidence="0.9995815">
tron (Collins, 2002) in the following ways. Firstly,
we incorporate a margin ∆ into the scoring func-
tion as follows so that wrong assignments with
small differences from gold assignments are pe-
nalized (lines 4 and 6 in Figure 5) (Freund and
Schapire, 1999).
</bodyText>
<equation confidence="0.949423">
s′(x, y) = s(x, y) + ∆(y, ygold) (5)
</equation>
<bodyText confidence="0.999699333333333">
Similarly to the scoring function s, the margin ∆
is defined as a decomposable function using 0-1
loss as follows:
</bodyText>
<equation confidence="0.9366312">
∆(yi, ygold
i ),
{ 0 if yi = ygold
∆(yi, ygold
i ) = i (6) 1 otherwise
</equation>
<bodyText confidence="0.999944333333333">
Secondly, we update the weights w based on a
max-violation update rule following Huang et al.
(2012) (lines 6-7 in Figure 5). Finally, we em-
ploy not only perceptron (Collins, 2002) but also
AROW (Mejer and Crammer, 2010; Crammer et
al., 2013), AdaGrad (Duchi et al., 2011), and
DCD-SSVM (Chang and Yih, 2013) for learning
methods (line 7 in Figure 5.) We employ parame-
ter averaging except for DCD-SSVM. AROW and
AdaGrad store additional information for covari-
ance and feature counts respectively, and DCD-
SSVM keeps a working set and performs addi-
tional updates in each iteration. Due to space limi-
tations, we refer to the papers for the details of the
learning methods.
</bodyText>
<subsectionHeader confidence="0.654027">
2.5 Features
</subsectionHeader>
<bodyText confidence="0.9999625">
Here, we explain the local features flo,.al and the
global features fglobal introduced in §2.2.
</bodyText>
<subsectionHeader confidence="0.604639">
2.5.1 Local features
</subsectionHeader>
<bodyText confidence="0.9999676">
Our focus is not to exploit useful local features
for entities and relations, so we incorporate several
features from existing work to realize a reasonable
baseline. Table 4 summarizes the local features.
Local features for entities (or words) are similar
to the features used by Florian et al. (2003), but
some features are generalized and extended, and
gazetteer features are excluded. For relations (or
pairs of words), we employ and extend features in
Miwa et al. (2009).
</bodyText>
<subsectionHeader confidence="0.584441">
2.5.2 Global features
</subsectionHeader>
<bodyText confidence="0.99999755">
We design global features to represent dependen-
cies among entities and relations. Table 5 summa-
rizes the global features2. These global features
are activated when all the information is available
during decoding.
We incorporate label dependency features like
traditional sequential labeling for entities. Al-
though our model can include other non-local fea-
tures between entities (Ratinov and Roth, 2009),
we do not include them expecting that global fea-
tures on entities and relations can cover them. We
design three types of global features for relations.
These features are activated when all the partic-
ipating relations are not 1 (non-relations). Fea-
tures except for the “Crossing” category are simi-
lar to global relation features in Li and Ji (2014).
We further incorporate global features for both en-
tities and relations. These features are activated
when the relation label is not 1. These features
can act as a bridge between entities and relations.
</bodyText>
<sectionHeader confidence="0.999381" genericHeader="introduction">
3 Evaluation
</sectionHeader>
<bodyText confidence="0.9998355">
In this section, we first introduce the corpus and
evaluation metrics that we employed for evalua-
tion. We then show the performance on the train-
ing data set with explaining the parameters used
</bodyText>
<footnote confidence="0.547486666666667">
2We tried other “Entity+Relation” features to represent a
relation and both its participating entities, but they slightly
degraded the performance in our preliminary experiments.
</footnote>
<equation confidence="0.9985765">
∆(y,ygold) = ∑ |X|
i=1
</equation>
<page confidence="0.986757">
1862
</page>
<table confidence="0.475959538461539">
Target Category Features
Word Lexical Character n-grams (n=2,3,4)
(Entity) Attributes by parsers (base form, POS)
Word types (all-capitalized, initial-capitalized, all-digits, all-puncts, all-
digits-or-puncts)
Contextual Word n-grams (n=1,2,3) within a context window size of 2
Word pair Entity Entity lexical features of each word
(Relation) Contextual Word n-grams (n=1,2,3) within a context window size of 2
Shortest Walk features (word-dependency-word or dependency-word-
path dependency) on the shortest paths in parsers’ outputs
n-grams (n=2,3) of words and dependencies on the paths
n-grams (n=1,2) of token modifier-modifiee pairs on the paths
The length of the paths
</table>
<tableCaption confidence="0.979929">
Table 4: Local features.
</tableCaption>
<bodyText confidence="0.921779230769231">
Target Category Details
Entity Bigram Bigrams of labels
Combinations of two labels and their corresponding POS tags
Combinations of two labels and their corresponding words
Trigram Trigrams of labels
Combinations of three labels and each of their corresponding POS tags
Combinations of three labels and each of their corresponding words
Entity Combinations of a label and its corresponding entity
Relation Entity- Combinations of two relation labels that share a word (i.e., relations in
sharing same columns or same rows in a table)
Combinations of two relation labels and the shared word
Relation shortest path features between non-shared words, augmented by
a combination of relation labels and the shared word
</bodyText>
<tableCaption confidence="0.8270116">
Cyclic Combinations of three relation labels that make a cycle
Crossing Combinations of two relation labels that cross each other
Entity + Entity- Relation label and the label of its participating entity
Relation relation Relation label and the label and word of its participating entity
Table 5: Global features.
</tableCaption>
<bodyText confidence="0.9347685">
for the test set evaluation, and show the perfor-
mance on the test data set.
</bodyText>
<subsectionHeader confidence="0.999343">
3.1 Evaluation settings
</subsectionHeader>
<bodyText confidence="0.998905555555555">
We used an entity and relation recognition corpus
by Roth and Yih (2004)3. The corpus defines four
named entity types Location, Organization, Per-
son, and Other and five relation types Kill, Live In,
Located In, OrgBased In and Work For.
All the entities were words in the original cor-
pus because all the spaces in entities were replaced
with slashes. Previous systems (Roth and Yih,
2007; Kate and Mooney, 2010) used these word
</bodyText>
<footnote confidence="0.874394">
3conll04.corp at http://cogcomp.cs.illinois.
edu/page/resource_view/43
</footnote>
<bodyText confidence="0.999895466666667">
boundaries as they were, treated the boundaries as
given, and focused the entity classification prob-
lem alone. Differently from such systems, we re-
covered these spaces by replacing these slashes
with spaces to evaluate the entity boundary detec-
tion performance on this corpus. Due to this re-
placement and the inclusion of the boundary de-
tection problem, our task is more challenging than
the original task, and our results are not compara-
ble with those by the previous systems.
The corpus contains 1,441 sentences that con-
tain at least one relation. Instead of 5-fold cross
validation on the entire corpus by the previous sys-
tems, we split the data set into training (1,153 sen-
tences) and blind test (288 sentences) data sets and
</bodyText>
<page confidence="0.943243">
1863
</page>
<bodyText confidence="0.999454317073171">
developed the system on the training data set. We
tuned the hyper-parameters using a 5-fold cross
validation on the training data set, and evaluated
the performance on the test set.
We prepared a pipeline approach as a baseline.
We first trained an entity recognition model using
the local and global features, and then trained a
relation extraction model using the local features
and global features without global “Relation” fea-
tures in Table 5. We did not employ the global
“Relation” features in this baseline since it is com-
mon to treat relation extraction as a multi-class
classification problem.
We extracted features using the results from two
syntactic parsers Enju (Miyao and Tsujii, 2008)
and LRDEP (Sagae and Tsujii, 2007). We em-
ployed feature hashing (Weinberger et al., 2009)
and limited the feature space to 224. The num-
bers of features greatly varied for categories and
targets. They also caused biased predictions that
prefer entities to relations in our preliminary ex-
periments. We thus chose to re-scale the features
as follows. We normalized local features for each
feature category and then for each target. We also
normalized global features for each feature cate-
gory, but we did not normalize them for each target
since normalization was impossible during decod-
ing. We instead scaled the global features, and the
scaling factor was tuned by using the same 5-fold
cross validation above.
We used the F1 score on relations with entities
as our primary evaluation measure and used it for
tuning parameters. In this measure, a relation with
two entities is considered correct when the offsets
and types of the entities and the type of the relation
are all correct. We also evaluated the F1 scores for
entities and relations individually on the test data
set by checking their corresponding cells. An en-
tity is correct when the offset and type are correct,
and a relation is correct when the type is correct
and the last words of two entities are correct.
</bodyText>
<subsectionHeader confidence="0.999645">
3.2 Performance on Training Data Set
</subsectionHeader>
<bodyText confidence="0.999921915254238">
It is infeasible to investigate all the combinations
of the parameters, so we greedily searched for a
default parameter setting by using the evaluated
results on the training data set. The default pa-
rameter setting was the best setting except for the
beam size. We show learning curves on the train-
ing data set in Figure 6 when we varied each pa-
rameter from the default parameter setting. We
employed 5-fold cross validation. The default pa-
rameter setting used DCD-SSVM as the learning
method, entity-first, easy-first as the search order,
local and global features, and 8 as the beam size.
This section discusses how these parameters affect
the performance on the training data set and ex-
plains how the parameter setting was selected for
the test set.
Figure 6(a) compares the learning methods in-
troduced in §2.4. DCD-SSVM and AdaGrad per-
formed slightly better than perceptron, which has
often been employed in history-based structured
learning. AROW did not show comparable per-
formance to the others. We ran 100 iterations to
find the number of iterations that saturates learn-
ing curves. The large number of iterations took
time and the performance of DCD-SSVM almost
converged after 30 iterations, so we employed 50
iterations for other evaluation on the training data
set. AdaGrad got its highest performance more
quickly than other learning methods and AROW
converged slower than other methods, so we em-
ployed 10 for AdaGrad, 90 for AROW, and 50 it-
erations for other settings on the test data set.
The performance was improved by widening
the beam as in Figure 6(b), but the improvement
was gradually diminished as the beam size in-
creased. Since the wider beam requires more train-
ing and test time, we chose 8 for the beam size.
Figure 6(c) shows the effects of joint learning
as well as features explained in §2.5. We show the
performance of the pipeline approach (Pipeline)
introduced in §3.1, and the performance with lo-
cal features alone (Local), local and global fea-
tures without global “Relation” features in Table 5
(Local+global (−relation)) and all local and global
features (Local+global). We note that Pipeline
shows the learning curve of relation extraction in
the pipeline approach. Features in “Local+global
(−relation)” are the same as the features in the
pipeline approach, and the result shows that the
joint learning approach performed slightly better
than the pipeline approach. The incorporation
of global “Entity” and “Entity+Relation” features
improved the performance as is common with the
existing pipeline approaches, and relation-related
features further improved the performance.
Static search orders in §2.3.1 also affected the
performance as shown in Figure 6(d), although
search orders are not investigated in the joint en-
tity and relation extraction. Surprisingly, the gap
</bodyText>
<page confidence="0.991543">
1864
</page>
<figure confidence="0.998072333333333">
(a) Learning methods (b) Beam sizes
(c) Features and pipeline /joint approaches (d) Static search orders
(e) Dynamic search orders
</figure>
<figureCaption confidence="0.9791165">
Figure 6: Learning curves of entity and relation extraction on the training data set using 5-fold cross
validation.
</figureCaption>
<bodyText confidence="0.9855573125">
between the performances with the best order and
worst order was about 0.04 in an F1 score, which
is statistically significant, and the performance can
be worse than the pipeline approach in Figure 6(c).
This means improvement by joint learning can be
easily cancelled out if we do not carefully con-
sider search order. It is also surprising that the sec-
ond worst order (Figure 4(b)) is the most intuitive
“left-to-right” order, which is closest to the order
in Li and Ji (2014) among the six search orders.
Figure 6(e) shows the performance with dy-
namic search orders. Unfortunately, the easy-first
policy did not work well on this entity and relation
task, but, with the two enhancements, dynamic or-
ders performed as well as the best static order in
Figure 6(d). This shows that entities should be de-
</bodyText>
<page confidence="0.981986">
1865
</page>
<bodyText confidence="0.981962">
tected earlier than relations on this data set.
</bodyText>
<subsectionHeader confidence="0.993223">
3.3 Performance on Test Data Set
</subsectionHeader>
<bodyText confidence="0.999979236842105">
Table 6 summarizes the performance on the test
data set. We employed the default parameter set-
ting explained in §3.2, and compared parameters
by changing the parameters shown in the first col-
umn. We performed a statistical test using the ap-
proximate randomization method (Noreen, 1989)
on our primary measure (“Entity+Relation”). The
results are almost consistent with the results on the
training data set with a few exceptions.
Differently from the results on the training data
set, AdaGrad and AROW performed significantly
worse than perceptron and DCD-SSVM and they
performed slightly worse than the pipeline ap-
proach. This result shows that DCD-SSVM per-
forms well with inexact search and the selection of
learning methods can significantly affect the entity
and relation extraction performance.
The joint learning approach showed a signifi-
cant improvement over the pipeline approach with
relation-related global features, although the joint
learning approach alone did not show a signif-
icant improvement over the pipeline approach.
Unfortunately, no joint learning approach outper-
formed the pipeline approach in entity recognition.
This may be partly because hyper-parameters were
tuned to the primary measure. The results on the
pipeline approach also indicate that the better per-
formance on entity recognition does not necessar-
ily improve the relation extraction performance.
Search orders also affected the performance,
and the worst order (right to left, down to up) and
best order (close-first, left to right) were signifi-
cantly different. The performance of the worst or-
der was worse than that of the pipeline approach,
although the difference was not significant. These
results show that it is necessary to carefully select
the search order for the joint entity and relation
extraction task.
</bodyText>
<subsectionHeader confidence="0.997545">
3.4 Comparison with Other Systems
</subsectionHeader>
<bodyText confidence="0.999982">
To compare our model with the other sys-
tems (Roth and Yih, 2007; Kate and Mooney,
2010), we evaluated the performance of our model
when the entity boundaries were given. Differ-
ently from our setting in §3.1, we used the gold
entity boundaries encoded in the BILOU scheme
and assigned entity labels to the boundaries. We
performed 5-fold cross validation on the data set
following Roth and Yih (2007) although the split
was different from theirs since their splits were not
available. We employed the default parameter set-
ting in §3.2 for this comparison.
Table 7 shows the evaluation results. Although
we cannot directly compare the results, our model
performs better than the other models. Compared
to Table 6, Table 7 also shows that the inclusion
of entity boundary detection degrades the perfor-
mance about 0.09 in F-score.
</bodyText>
<sectionHeader confidence="0.999986" genericHeader="related work">
4 Related Work
</sectionHeader>
<bodyText confidence="0.999969861111111">
Search order in structured learning has been stud-
ied in several NLP tasks. Left-to-right and right-
to-left orderings have been often investigated in
sequential labeling tasks (Kudo and Matsumoto,
2001). Easy-first policy was firstly introduced
by Goldberg and Elhadad (2010) for dependency
parsing, and it was successfully employed in sev-
eral tasks, such as joint POS tagging and depen-
dency parsing (Ma et al., 2012) and co-reference
resolution (Stoyanov and Eisner, 2012). Search
order, however, has not been focused in relation
extraction tasks.
Named entity recognition (Florian et al., 2003;
Nadeau and Sekine, 2007) and relation extrac-
tion (Zelenko et al., 2003; Miwa et al., 2009)
have often been treated as separate tasks, but
there are some previous studies that treat enti-
ties and relations jointly in learning. Most stud-
ies built joint learning models upon individual
models for subtasks, such as Integer Linear Pro-
gramming (ILP) (Roth and Yih, 2007; Yang and
Cardie, 2013) and Card-Pyramid Parsing (Kate
and Mooney, 2010). Our approach does not re-
quire such individual models, and it also can de-
tect entity boundaries that these approaches except
for Yang and Cardie (2013) did not treat. Other
studies (Yu and Lam, 2010; Singh et al., 2013)
built global probabilistic graphical models. They
need to compute distributions over variables, but
our approach does not. Li and Ji (2014) proposed
an approach to jointly find entities and relations.
They incorporated a semi-Markov chain in repre-
senting entities and they defined two actions dur-
ing search, but our approach does not employ such
representation and actions, and thus it is more sim-
ple and flexible to investigate search orders.
</bodyText>
<sectionHeader confidence="0.999681" genericHeader="conclusions">
5 Conclusions
</sectionHeader>
<bodyText confidence="0.9861195">
In this paper, we proposed a history-based struc-
tured learning approach that jointly detects enti-
</bodyText>
<page confidence="0.962673">
1866
</page>
<table confidence="0.985025888888889">
Parameter Entity Relation Entity+Relation
Perceptron 0.809 / 0.809 / 0.809 0.760 / 0.547 / 0.636 0.731 / 0.527 / 0.612*
AdaGrad 0.801 / 0.790 / 0.795 0.732 / 0.486 / 0.584 0.716 / 0.476 / 0.572
AROW 0.810 / 0.802 / 0.806 0.797 / 0.468 / 0.590 0.758 / 0.445 / 0.561
DCD-SSVM† 0.812 / 0.802 / 0.807 0.783 / 0.524 / 0.628 0.760 / 0.509 / 0.610*
Pipeline 0.823 / 0.814 / 0.818 0.672 / 0.542 / 0.600 0.647 / 0.522 / 0.577
Local 0.819 / 0.812 / 0.815 0.844 / 0.399 / 0.542 0.812 / 0.384 / 0.522
Local + global (−relation) 0.809 / 0.799 / 0.804 0.784 / 0.481 / 0.596 0.747 / 0.458 / 0.568
Local + global† 0.812 / 0.802 / 0.807 0.783 / 0.524 / 0.628 0.760 / 0.509 / 0.610*
</table>
<figure confidence="0.984475">
(a) Up to down, left to right 0.824 / 0.801 / 0.813 0.821 / 0.433 / 0.567 0.787 / 0.415 / 0.543
(b) Up to down, right to left 0.828 / 0.808 / 0.818 0.850 / 0.461 / 0.597 0.822 / 0.445 / 0.578
(c) Right to left, up to down 0.823 / 0.799 / 0.811 0.826 / 0.448 / 0.581 0.789 / 0.427 / 0.554
(d) Right to left, down to up 0.811 / 0.784 / 0.797 0.774 / 0.445 / 0.565 0.739 / 0.425 / 0.540
</figure>
<table confidence="0.8731846">
(e) Close-first, left to right 0.821 / 0.806 / 0.813 0.807 / 0.522 / 0.634 0.780 / 0.504 / 0.612*
(f) Close-first, right to left 0.817 / 0.801 / 0.809 0.832 / 0.491 / 0.618 0.797 / 0.471 / 0.592
Easy-first 0.811 / 0.790 / 0.801 0.862 / 0.415 / 0.560 0.831 / 0.399 / 0.540
Entity-first, easy-first† 0.812 / 0.802 / 0.807 0.783 / 0.524 / 0.628 0.760 / 0.509 / 0.610*
Close-first, easy-first 0.816 / 0.803 / 0.810 0.796 / 0.486 / 0.603 0.767 / 0.468 / 0.581
</table>
<tableCaption confidence="0.712312666666667">
Table 6: Performance of entity and relation extraction on the test data set (precision / recall / F1 score).
The † denotes the default parameter setting in §3.2 and * represents a significant improvement over the
underlined “Pipeline” baseline (p&lt;0.05). Labels (a)-(f) correspond to those in Figure 4.
</tableCaption>
<table confidence="0.999178090909091">
Kate and Mooney (2010) Roth and Yih (2007) Entity-first, easy-first
Person 0.921 / 0.942 / 0.932 0.891 / 0.895 / 0.890 0.931 / 0.948 / 0.939
Location 0.908 / 0.942 / 0.924 0.897 / 0.887 / 0.891 0.922 / 0.939 / 0.930
Organization 0.905 / 0.887 / 0.895 0.895 / 0.720 / 0.792 0.903 / 0.896 / 0.899
All entities - - 0.924 / 0.924 / 0.924
Located In 0.675 / 0.567 / 0.583 0.539 / 0.557 / 0.513 0.821 / 0.549 / 0.654
Work For 0.735 / 0.683 / 0.707 0.720 / 0.423 / 0.531 0.886 / 0.642 / 0.743
OrgBased In 0.662 / 0.641 / 0.647 0.798 / 0.416 / 0.543 0.768 / 0.572 / 0.654
Live In 0.664 / 0.601 / 0.629 0.591 / 0.490 / 0.530 0.819 / 0.532 / 0.644
Kill 0.916 / 0.641 / 0.752 0.775 / 0.815 / 0.790 0.933 / 0.797 / 0.858
All relations - - 0.837 / 0.599 / 0.698
</table>
<tableCaption confidence="0.994739">
Table 7: Results of entity classification and relation extraction on the data set using the 5-fold cross
</tableCaption>
<bodyText confidence="0.9769745">
validation (precision/ recall / F1 score).
ties and relations. We introduced a novel entity
and relation table that jointly represents entities
and relations, and showed how the entity and re-
lation extraction task can be mapped to a simple
table-filling problem. We also investigated search
orders and learning methods that have been fixed
in previous research. Experimental results showed
that the joint learning approach outperforms the
pipeline approach and the appropriate selection of
learning methods and search orders is crucial to
produce a high performance on this task.
As future work, we plan to apply this approach
to other relation extraction tasks and explore more
suitable search orders for relation extraction tasks.
We also plan to investigate the potential of this ta-
ble representation in other tasks such as semantic
parsing and co-reference resolution.
</bodyText>
<sectionHeader confidence="0.998812" genericHeader="acknowledgments">
Acknowledgments
</sectionHeader>
<bodyText confidence="0.999049666666667">
We thank Yoshimasa Tsuruoka and Yusuke Miyao
for valuable discussions, and the anonymous re-
viewers for their insightful comments. This work
was supported by the TTI Start-Up Research
Support Program and the JSPS Grant-in-Aid for
Young Scientists (B) [grant number 25730129].
</bodyText>
<page confidence="0.993601">
1867
</page>
<sectionHeader confidence="0.99546" genericHeader="references">
References
</sectionHeader>
<reference confidence="0.995911411214953">
Ming-Wei Chang and Wen-Tau Yih. 2013. Dual coor-
dinate descent algorithms for efficient large margin
structured prediction. Transactions of the Associa-
tion for Computational Linguistics, 1:207–218.
Michael Collins and Brian Roark. 2004. Incremen-
tal parsing with the perceptron algorithm. In Pro-
ceedings of the 42nd Meeting of the Association for
Computational Linguistics (ACL’04), Main Volume,
pages 111–118, Barcelona, Spain, July.
Michael Collins. 2002. Discriminative training meth-
ods for hidden markov models: Theory and experi-
ments with perceptron algorithms. In Proceedings
of the 2002 Conference on Empirical Methods in
Natural Language Processing, pages 1–8. Associ-
ation for Computational Linguistics, July.
Koby Crammer, Alex Kulesza, and Mark Dredze.
2013. Adaptive regularization of weight vectors.
Machine learning, 91(2):155–187.
John Duchi, Elad Hazan, and Yoram Singer. 2011.
Adaptive subgradient methods for online learning
and stochastic optimization. The Journal of Ma-
chine Learning Research, 12:2121–2159.
Radu Florian, Abe Ittycheriah, Hongyan Jing, and
Tong Zhang. 2003. Named entity recognition
through classifier combination. In Walter Daele-
mans and Miles Osborne, editors, Proceedings of the
Seventh Conference on Natural Language Learning
at HLT-NAACL 2003, pages 168–171.
Yoav Freund and Robert E Schapire. 1999. Large
margin classification using the perceptron algorithm.
Machine learning, 37(3):277–296.
Yoav Goldberg and Michael Elhadad. 2010. An effi-
cient algorithm for easy-first non-directional depen-
dency parsing. In Human Language Technologies:
The 2010 Annual Conference of the North American
Chapter of the Association for Computational Lin-
guistics, pages 742–750, Los Angeles, California,
June. Association for Computational Linguistics.
Liang Huang, Suphan Fayong, and Yang Guo. 2012.
Structured perceptron with inexact search. In Pro-
ceedings of the 2012 Conference of the North Amer-
ican Chapter of the Association for Computational
Linguistics: Human Language Technologies, pages
142–151, Montr´eal, Canada, June. Association for
Computational Linguistics.
Rohit J. Kate and Raymond Mooney. 2010. Joint en-
tity and relation extraction using card-pyramid pars-
ing. In Proceedings of the Fourteenth Conference on
Computational Natural Language Learning, pages
203–212, Uppsala, Sweden, July. Association for
Computational Linguistics.
Taku Kudo and Yuji Matsumoto. 2001. Chunking with
support vector machines. In Proceedings of the Sec-
ond Meeting of the North American Chapter of the
Association for Computational Linguistics on Lan-
guage Technologies, Stroudsburg, PA, USA. Asso-
ciation for Computational Linguistics.
Qi Li and Heng Ji. 2014. Incremental joint extrac-
tion of entity mentions and relations. In Proceed-
ings of the 52nd Annual Meeting of the Association
for Computational Linguistics (Volume 1: Long Pa-
pers), pages 402–412, Baltimore, Maryland, June.
Association for Computational Linguistics.
Ji Ma, Tong Xiao, Jingbo Zhu, and Feiliang Ren.
2012. Easy-first Chinese POS tagging and depen-
dency parsing. In Proceedings of COLING 2012,
pages 1731–1746, Mumbai, India, December. The
COLING 2012 Organizing Committee.
Avihai Mejer and Koby Crammer. 2010. Confidence
in structured-prediction using confidence-weighted
models. In Proceedings of the 2010 Conference on
Empirical Methods in Natural Language Process-
ing, pages 971–981, Cambridge, MA, October. As-
sociation for Computational Linguistics.
Makoto Miwa, Rune Sætre, Yusuke Miyao, and
Jun’ichi Tsujii. 2009. A rich feature vector for
protein-protein interaction extraction from multiple
corpora. In Proceedings of the 2009 Conference on
Empirical Methods in Natural Language Process-
ing, pages 121–130, Singapore, August. Association
for Computational Linguistics.
Yusuke Miyao and Jun’ichi Tsujii. 2008. Feature for-
est models for probabilistic HPSG parsing. Compu-
tational Linguistics, 34(1):35–80, March.
David Nadeau and Satoshi Sekine. 2007. A sur-
vey of named entity recognition and classification.
Lingvisticae Investigationes, 30(1):3–26.
Eric W. Noreen. 1989. Computer-Intensive Methods
for Testing Hypotheses : An Introduction. Wiley-
Interscience, April.
Lev Ratinov and Dan Roth. 2009. Design chal-
lenges and misconceptions in named entity recog-
nition. In Proceedings of the Thirteenth Confer-
ence on Computational Natural Language Learning
(CoNLL-2009), pages 147–155, Boulder, Colorado,
June. Association for Computational Linguistics.
Dan Roth and Wen-Tau Yih. 2004. A linear program-
ming formulation for global inference in natural lan-
guage tasks. In Hwee Tou Ng and Ellen Riloff, ed-
itors, HLT-NAACL 2004 Workshop: Eighth Confer-
ence on Computational Natural Language Learning
(CoNLL-2004), pages 1–8, Boston, Massachusetts,
USA, May. Association for Computational Linguis-
tics.
Dan Roth and Wen-Tau Yih, 2007. Global Inference
for Entity and Relation Identification via a Linear
Programming Formulation. MIT Press.
</reference>
<page confidence="0.838555">
1868
</page>
<reference confidence="0.999338444444445">
Kenji Sagae and Jun’ichi Tsujii. 2007. Dependency
parsing and domain adaptation with LR models and
parser ensembles. In Proceedings of the CoNLL
Shared Task Session of EMNLP-CoNLL 2007, pages
1044–1050, Prague, Czech Republic, June. Associ-
ation for Computational Linguistics.
Sameer Singh, Sebastian Riedel, Brian Martin, Jiaping
Zheng, and Andrew McCallum. 2013. Joint infer-
ence of entities, relations, and coreference. In Pro-
ceedings of the 2013 workshop on Automated knowl-
edge base construction, pages 1–6. ACM.
Veselin Stoyanov and Jason Eisner. 2012. Easy-first
coreference resolution. In Proceedings of COLING
2012, pages 2519–2534, Mumbai, India, December.
The COLING 2012 Organizing Committee.
Kilian Weinberger, Anirban Dasgupta, John Langford,
Alex Smola, and Josh Attenberg. 2009. Feature
hashing for large scale multitask learning. In Pro-
ceedings of the 26th Annual International Confer-
ence on Machine Learning, ICML ’09, pages 1113–
1120, New York, NY, USA. ACM.
Bishan Yang and Claire Cardie. 2013. Joint infer-
ence for fine-grained opinion extraction. In Pro-
ceedings of the 51st Annual Meeting of the Associa-
tion for Computational Linguistics (Volume 1: Long
Papers), pages 1640–1649, Sofia, Bulgaria, August.
Association for Computational Linguistics.
Xiaofeng Yu and Wai Lam. 2010. Jointly identifying
entities and extracting relations in encyclopedia text
via a graphical model approach. In Coling 2010:
Posters, pages 1399–1407, Beijing, China, August.
Coling 2010 Organizing Committee.
Dmitry Zelenko, Chinatsu Aone, and Anthony
Richardella. 2003. Kernel methods for relation
extraction. The Journal of Machine Learning Re-
search, 3:1083–1106.
</reference>
<page confidence="0.996235">
1869
</page>
</variant>
</algorithm>
<algorithm name="ParsHed" version="110505">
<variant no="0" confidence="0.307232">
<title confidence="0.998684">Modeling Joint Entity and Relation Extraction with Table Representation</title>
<author confidence="0.506265">Miwa</author>
<affiliation confidence="0.682255">Toyota Technological</affiliation>
<address confidence="0.940671">2-12-1 Hisakata, Tempaku-ku, Nagoya, 468-8511,</address>
<abstract confidence="0.988023235294118">Live_in Live_in Located_in This paper proposes a history-based structured learning approach that jointly extracts entities and relations in a sentence. We introduce a novel simple and flexible table representation of entities and relations. We investigate several feature settings, search orders, and learning methods with inexact search on the table. The experimental results demonstrate that a joint learning approach significantly outperforms a pipeline approach by incorporating global features and by selecting appropriate learning methods and search orders.</abstract>
</variant>
</algorithm>
<algorithm name="ParsCit" version="110505">
<citationList>
<citation valid="true">
<authors>
<author>Ming-Wei Chang</author>
<author>Wen-Tau Yih</author>
</authors>
<title>Dual coordinate descent algorithms for efficient large margin structured prediction.</title>
<date>2013</date>
<journal>Transactions of the Association for Computational Linguistics,</journal>
<pages>1--207</pages>
<contexts>
<context position="16791" citStr="Chang and Yih, 2013" startWordPosition="2996" endWordPosition="2999"> gold assignments are penalized (lines 4 and 6 in Figure 5) (Freund and Schapire, 1999). s′(x, y) = s(x, y) + ∆(y, ygold) (5) Similarly to the scoring function s, the margin ∆ is defined as a decomposable function using 0-1 loss as follows: ∆(yi, ygold i ), { 0 if yi = ygold ∆(yi, ygold i ) = i (6) 1 otherwise Secondly, we update the weights w based on a max-violation update rule following Huang et al. (2012) (lines 6-7 in Figure 5). Finally, we employ not only perceptron (Collins, 2002) but also AROW (Mejer and Crammer, 2010; Crammer et al., 2013), AdaGrad (Duchi et al., 2011), and DCD-SSVM (Chang and Yih, 2013) for learning methods (line 7 in Figure 5.) We employ parameter averaging except for DCD-SSVM. AROW and AdaGrad store additional information for covariance and feature counts respectively, and DCDSSVM keeps a working set and performs additional updates in each iteration. Due to space limitations, we refer to the papers for the details of the learning methods. 2.5 Features Here, we explain the local features flo,.al and the global features fglobal introduced in §2.2. 2.5.1 Local features Our focus is not to exploit useful local features for entities and relations, so we incorporate several feat</context>
</contexts>
<marker>Chang, Yih, 2013</marker>
<rawString>Ming-Wei Chang and Wen-Tau Yih. 2013. Dual coordinate descent algorithms for efficient large margin structured prediction. Transactions of the Association for Computational Linguistics, 1:207–218.</rawString>
</citation>
<citation valid="true">
<authors>
<author>Michael Collins</author>
<author>Brian Roark</author>
</authors>
<title>Incremental parsing with the perceptron algorithm.</title>
<date>2004</date>
<booktitle>In Proceedings of the 42nd Meeting of the Association for Computational Linguistics (ACL’04), Main Volume,</booktitle>
<pages>111--118</pages>
<location>Barcelona, Spain,</location>
<contexts>
<context position="7638" citStr="Collins and Roark, 2004" startWordPosition="1233" endWordPosition="1236">lations, and between entities and relations. Similarly, features f are divided into local features flocal and global features fglobal, and they are defined on its target cell and surrounding contexts. The features will be explained in §2.5. The weights w can also be divided, but they are tuned jointly in learning as shown in §2.4. 2.3 Decoding The scoring function s(x, y,1, i) in Equation (2) uses all the preceding assignments and does not rely on the Markov assumption, so we cannot employ dynamic programming. We instead employ a beam search to find the best assignment with the highest score (Collins and Roark, 2004). The beam search assigns labels to cells one by one with keeping the top K best assignments when moving from a cell to the next cell, and it returns the best assignment when labels are assigned to all the cells. The pseudo code for decoding with the beam search is shown in Figure 3. 1859 Mrs. Tsutayama is from Kumamoto Prefecture in Japan . Mrs. B-PER Tsutayama 1 L-PER is 1 1 O from 1 1 1 O Kumamoto 1 1 1 1 B-LOC Prefecture 1 Live in—* 1 1 1 L-LOC in 1 1 1 1 1 1 O Japan 1 Live in—* 1 1 1 Located in—* 1 U-LOC . 1 1 1 1 1 1 1 1 1 Figure 2: The entity and relation table for the example in Figure</context>
</contexts>
<marker>Collins, Roark, 2004</marker>
<rawString>Michael Collins and Brian Roark. 2004. Incremental parsing with the perceptron algorithm. In Proceedings of the 42nd Meeting of the Association for Computational Linguistics (ACL’04), Main Volume, pages 111–118, Barcelona, Spain, July.</rawString>
</citation>
<citation valid="true">
<authors>
<author>Michael Collins</author>
</authors>
<title>Discriminative training methods for hidden markov models: Theory and experiments with perceptron algorithms.</title>
<date>2002</date>
<booktitle>In Proceedings of the 2002 Conference on Empirical Methods in Natural Language Processing,</booktitle>
<pages>1--8</pages>
<publisher>Association</publisher>
<institution>for Computational Linguistics,</institution>
<contexts>
<context position="5726" citStr="Collins, 2002" startWordPosition="897" endWordPosition="898">2 illustrates an entity and relation table corresponding to an example in Figure 1. We use only the lower triangular part because the table is symmetric, so the number of cells is n(n + 1)/2 when there are n words in a sentence. With this entity and relation table representation, the joint extraction problem can be mapped to a table-filling problem in that labels are assigned to cells in the table. 2.2 Model We tackle the table-filling problem by a historybased structured learning approach that assigns labels to cells one by one. This is mostly the same as the traditional history-based model (Collins, 2002) except for the table representation. Let x be an input table, Y(x) be all possible assignments to the table, and s(x, y) be a scoring function that assesses the assignment of y E Y(x) to x. With these definitions, we define our model to predict the most probable assignment as follows: y* = arg max s(x, y) (1) YEY(X) This scoring function is a decomposable function, and each decomposed function assesses the assignment of a label to a cell in the table. s(x, y) = ∑ |X |s(x, y, 1, i) (2) Z=1 Here, i represents an index of a cell in the table, which will be explained in §2.3.1. The decomposed fun</context>
<context position="16022" citStr="Collins, 2002" startWordPosition="2856" endWordPosition="2857">wi_1 Relation(s) on wi B-*, O, U-* L-*, U-* 1861 INPUT: training sets D = {(xi, yi)}N i=1, T: iterations OUTPUT: weights w 1: w +— 0 2: for t = 1 to T do 3: for x, y E D do 4: y* +— best assignment for x using decoding in Figure 3 with s′ in Equation (5) 5: if y* =� ygold then 6: m +— arg maxi{s′(x, ygold, 1, i)� s′(x, y*, 1, i)} 7: w +— update(w, f(x, ygold, 1, m), f(x, y*,1, m)) 8: end if 9: end for 10: end for 11: return w Figure 5: Margin-based structured learning approach with a max-violation update. update(w, f(x, ygold, 1, m), f(x, y*, 1, m)) depends on employed learning methods. tron (Collins, 2002) in the following ways. Firstly, we incorporate a margin ∆ into the scoring function as follows so that wrong assignments with small differences from gold assignments are penalized (lines 4 and 6 in Figure 5) (Freund and Schapire, 1999). s′(x, y) = s(x, y) + ∆(y, ygold) (5) Similarly to the scoring function s, the margin ∆ is defined as a decomposable function using 0-1 loss as follows: ∆(yi, ygold i ), { 0 if yi = ygold ∆(yi, ygold i ) = i (6) 1 otherwise Secondly, we update the weights w based on a max-violation update rule following Huang et al. (2012) (lines 6-7 in Figure 5). Finally, we e</context>
</contexts>
<marker>Collins, 2002</marker>
<rawString>Michael Collins. 2002. Discriminative training methods for hidden markov models: Theory and experiments with perceptron algorithms. In Proceedings of the 2002 Conference on Empirical Methods in Natural Language Processing, pages 1–8. Association for Computational Linguistics, July.</rawString>
</citation>
<citation valid="true">
<authors>
<author>Koby Crammer</author>
<author>Alex Kulesza</author>
<author>Mark Dredze</author>
</authors>
<title>Adaptive regularization of weight vectors.</title>
<date>2013</date>
<booktitle>Machine learning,</booktitle>
<pages>91--2</pages>
<contexts>
<context position="16725" citStr="Crammer et al., 2013" startWordPosition="2985" endWordPosition="2988">on as follows so that wrong assignments with small differences from gold assignments are penalized (lines 4 and 6 in Figure 5) (Freund and Schapire, 1999). s′(x, y) = s(x, y) + ∆(y, ygold) (5) Similarly to the scoring function s, the margin ∆ is defined as a decomposable function using 0-1 loss as follows: ∆(yi, ygold i ), { 0 if yi = ygold ∆(yi, ygold i ) = i (6) 1 otherwise Secondly, we update the weights w based on a max-violation update rule following Huang et al. (2012) (lines 6-7 in Figure 5). Finally, we employ not only perceptron (Collins, 2002) but also AROW (Mejer and Crammer, 2010; Crammer et al., 2013), AdaGrad (Duchi et al., 2011), and DCD-SSVM (Chang and Yih, 2013) for learning methods (line 7 in Figure 5.) We employ parameter averaging except for DCD-SSVM. AROW and AdaGrad store additional information for covariance and feature counts respectively, and DCDSSVM keeps a working set and performs additional updates in each iteration. Due to space limitations, we refer to the papers for the details of the learning methods. 2.5 Features Here, we explain the local features flo,.al and the global features fglobal introduced in §2.2. 2.5.1 Local features Our focus is not to exploit useful local f</context>
</contexts>
<marker>Crammer, Kulesza, Dredze, 2013</marker>
<rawString>Koby Crammer, Alex Kulesza, and Mark Dredze. 2013. Adaptive regularization of weight vectors. Machine learning, 91(2):155–187.</rawString>
</citation>
<citation valid="true">
<authors>
<author>John Duchi</author>
<author>Elad Hazan</author>
<author>Yoram Singer</author>
</authors>
<title>Adaptive subgradient methods for online learning and stochastic optimization.</title>
<date>2011</date>
<journal>The Journal of Machine Learning Research,</journal>
<pages>12--2121</pages>
<contexts>
<context position="16755" citStr="Duchi et al., 2011" startWordPosition="2990" endWordPosition="2993">gnments with small differences from gold assignments are penalized (lines 4 and 6 in Figure 5) (Freund and Schapire, 1999). s′(x, y) = s(x, y) + ∆(y, ygold) (5) Similarly to the scoring function s, the margin ∆ is defined as a decomposable function using 0-1 loss as follows: ∆(yi, ygold i ), { 0 if yi = ygold ∆(yi, ygold i ) = i (6) 1 otherwise Secondly, we update the weights w based on a max-violation update rule following Huang et al. (2012) (lines 6-7 in Figure 5). Finally, we employ not only perceptron (Collins, 2002) but also AROW (Mejer and Crammer, 2010; Crammer et al., 2013), AdaGrad (Duchi et al., 2011), and DCD-SSVM (Chang and Yih, 2013) for learning methods (line 7 in Figure 5.) We employ parameter averaging except for DCD-SSVM. AROW and AdaGrad store additional information for covariance and feature counts respectively, and DCDSSVM keeps a working set and performs additional updates in each iteration. Due to space limitations, we refer to the papers for the details of the learning methods. 2.5 Features Here, we explain the local features flo,.al and the global features fglobal introduced in §2.2. 2.5.1 Local features Our focus is not to exploit useful local features for entities and relat</context>
</contexts>
<marker>Duchi, Hazan, Singer, 2011</marker>
<rawString>John Duchi, Elad Hazan, and Yoram Singer. 2011. Adaptive subgradient methods for online learning and stochastic optimization. The Journal of Machine Learning Research, 12:2121–2159.</rawString>
</citation>
<citation valid="true">
<authors>
<author>Radu Florian</author>
<author>Abe Ittycheriah</author>
<author>Hongyan Jing</author>
<author>Tong Zhang</author>
</authors>
<title>Named entity recognition through classifier combination.</title>
<date>2003</date>
<booktitle>In Walter Daelemans and</booktitle>
<pages>168--171</pages>
<editor>Miles Osborne, editors,</editor>
<contexts>
<context position="17584" citStr="Florian et al. (2003)" startWordPosition="3126" endWordPosition="3129">ts respectively, and DCDSSVM keeps a working set and performs additional updates in each iteration. Due to space limitations, we refer to the papers for the details of the learning methods. 2.5 Features Here, we explain the local features flo,.al and the global features fglobal introduced in §2.2. 2.5.1 Local features Our focus is not to exploit useful local features for entities and relations, so we incorporate several features from existing work to realize a reasonable baseline. Table 4 summarizes the local features. Local features for entities (or words) are similar to the features used by Florian et al. (2003), but some features are generalized and extended, and gazetteer features are excluded. For relations (or pairs of words), we employ and extend features in Miwa et al. (2009). 2.5.2 Global features We design global features to represent dependencies among entities and relations. Table 5 summarizes the global features2. These global features are activated when all the information is available during decoding. We incorporate label dependency features like traditional sequential labeling for entities. Although our model can include other non-local features between entities (Ratinov and Roth, 2009)</context>
<context position="31442" citStr="Florian et al., 2003" startWordPosition="5337" endWordPosition="5340">in F-score. 4 Related Work Search order in structured learning has been studied in several NLP tasks. Left-to-right and rightto-left orderings have been often investigated in sequential labeling tasks (Kudo and Matsumoto, 2001). Easy-first policy was firstly introduced by Goldberg and Elhadad (2010) for dependency parsing, and it was successfully employed in several tasks, such as joint POS tagging and dependency parsing (Ma et al., 2012) and co-reference resolution (Stoyanov and Eisner, 2012). Search order, however, has not been focused in relation extraction tasks. Named entity recognition (Florian et al., 2003; Nadeau and Sekine, 2007) and relation extraction (Zelenko et al., 2003; Miwa et al., 2009) have often been treated as separate tasks, but there are some previous studies that treat entities and relations jointly in learning. Most studies built joint learning models upon individual models for subtasks, such as Integer Linear Programming (ILP) (Roth and Yih, 2007; Yang and Cardie, 2013) and Card-Pyramid Parsing (Kate and Mooney, 2010). Our approach does not require such individual models, and it also can detect entity boundaries that these approaches except for Yang and Cardie (2013) did not t</context>
</contexts>
<marker>Florian, Ittycheriah, Jing, Zhang, 2003</marker>
<rawString>Radu Florian, Abe Ittycheriah, Hongyan Jing, and Tong Zhang. 2003. Named entity recognition through classifier combination. In Walter Daelemans and Miles Osborne, editors, Proceedings of the Seventh Conference on Natural Language Learning at HLT-NAACL 2003, pages 168–171.</rawString>
</citation>
<citation valid="true">
<authors>
<author>Yoav Freund</author>
<author>Robert E Schapire</author>
</authors>
<title>Large margin classification using the perceptron algorithm.</title>
<date>1999</date>
<booktitle>Machine learning,</booktitle>
<pages>37--3</pages>
<contexts>
<context position="16258" citStr="Freund and Schapire, 1999" startWordPosition="2895" endWordPosition="2898">gure 3 with s′ in Equation (5) 5: if y* =� ygold then 6: m +— arg maxi{s′(x, ygold, 1, i)� s′(x, y*, 1, i)} 7: w +— update(w, f(x, ygold, 1, m), f(x, y*,1, m)) 8: end if 9: end for 10: end for 11: return w Figure 5: Margin-based structured learning approach with a max-violation update. update(w, f(x, ygold, 1, m), f(x, y*, 1, m)) depends on employed learning methods. tron (Collins, 2002) in the following ways. Firstly, we incorporate a margin ∆ into the scoring function as follows so that wrong assignments with small differences from gold assignments are penalized (lines 4 and 6 in Figure 5) (Freund and Schapire, 1999). s′(x, y) = s(x, y) + ∆(y, ygold) (5) Similarly to the scoring function s, the margin ∆ is defined as a decomposable function using 0-1 loss as follows: ∆(yi, ygold i ), { 0 if yi = ygold ∆(yi, ygold i ) = i (6) 1 otherwise Secondly, we update the weights w based on a max-violation update rule following Huang et al. (2012) (lines 6-7 in Figure 5). Finally, we employ not only perceptron (Collins, 2002) but also AROW (Mejer and Crammer, 2010; Crammer et al., 2013), AdaGrad (Duchi et al., 2011), and DCD-SSVM (Chang and Yih, 2013) for learning methods (line 7 in Figure 5.) We employ parameter ave</context>
</contexts>
<marker>Freund, Schapire, 1999</marker>
<rawString>Yoav Freund and Robert E Schapire. 1999. Large margin classification using the perceptron algorithm. Machine learning, 37(3):277–296.</rawString>
</citation>
<citation valid="true">
<authors>
<author>Yoav Goldberg</author>
<author>Michael Elhadad</author>
</authors>
<title>An efficient algorithm for easy-first non-directional dependency parsing.</title>
<date>2010</date>
<booktitle>In Human Language Technologies: The 2010 Annual Conference of the North American Chapter of the Association for Computational Linguistics,</booktitle>
<pages>742--750</pages>
<publisher>Association</publisher>
<institution>for Computational Linguistics.</institution>
<location>Los Angeles, California,</location>
<contexts>
<context position="12382" citStr="Goldberg and Elhadad, 2010" startWordPosition="2204" endWordPosition="2208">but tables. We de- the order in a sentence is prioritized in Figures 4(a) 1860 Table 1: Label dependencies from relations to entities. * indicates any type. Label on wi Relations from/to wi B-*, I-*, O 1 L-*, U-* * Label on wi+1 Relations from/to wi I-*, L-* 1 B-*, U-*, O * Table 2: Label dependencies from entities to relations. and 4(d). We further define two close-first mappings (Figures 4(e) and 4(f)) since entities are easier to find than relations and close relations are easier to find than distant relations. We also investigate dynamic mappings (search orders) with an easy-first policy (Goldberg and Elhadad, 2010). Dynamic mappings are different from the static mappings above, since we reorder the cells before each decoding1. We evaluate the cells using the local scoring function, and assign indices to the cells so that the cells with higher scores have higher priorities. In addition to this naive easy-first policy, we define two other dynamic mappings that restricts the reordering by combining the easy-first policy with one of the following two policies: entity-first (all entities are detected before relations) and close-first (closer cells are detected before distant cells) policies. 2.3.2 Label depe</context>
<context position="31122" citStr="Goldberg and Elhadad (2010)" startWordPosition="5287" endWordPosition="5290">mployed the default parameter setting in §3.2 for this comparison. Table 7 shows the evaluation results. Although we cannot directly compare the results, our model performs better than the other models. Compared to Table 6, Table 7 also shows that the inclusion of entity boundary detection degrades the performance about 0.09 in F-score. 4 Related Work Search order in structured learning has been studied in several NLP tasks. Left-to-right and rightto-left orderings have been often investigated in sequential labeling tasks (Kudo and Matsumoto, 2001). Easy-first policy was firstly introduced by Goldberg and Elhadad (2010) for dependency parsing, and it was successfully employed in several tasks, such as joint POS tagging and dependency parsing (Ma et al., 2012) and co-reference resolution (Stoyanov and Eisner, 2012). Search order, however, has not been focused in relation extraction tasks. Named entity recognition (Florian et al., 2003; Nadeau and Sekine, 2007) and relation extraction (Zelenko et al., 2003; Miwa et al., 2009) have often been treated as separate tasks, but there are some previous studies that treat entities and relations jointly in learning. Most studies built joint learning models upon individ</context>
</contexts>
<marker>Goldberg, Elhadad, 2010</marker>
<rawString>Yoav Goldberg and Michael Elhadad. 2010. An efficient algorithm for easy-first non-directional dependency parsing. In Human Language Technologies: The 2010 Annual Conference of the North American Chapter of the Association for Computational Linguistics, pages 742–750, Los Angeles, California, June. Association for Computational Linguistics.</rawString>
</citation>
<citation valid="true">
<authors>
<author>Liang Huang</author>
<author>Suphan Fayong</author>
<author>Yang Guo</author>
</authors>
<title>Structured perceptron with inexact search.</title>
<date>2012</date>
<booktitle>In Proceedings of the 2012 Conference of the North American Chapter of the Association for Computational Linguistics: Human Language Technologies,</booktitle>
<pages>142--151</pages>
<publisher>Association</publisher>
<institution>for Computational Linguistics.</institution>
<location>Montr´eal, Canada,</location>
<contexts>
<context position="16583" citStr="Huang et al. (2012)" startWordPosition="2960" endWordPosition="2963">depends on employed learning methods. tron (Collins, 2002) in the following ways. Firstly, we incorporate a margin ∆ into the scoring function as follows so that wrong assignments with small differences from gold assignments are penalized (lines 4 and 6 in Figure 5) (Freund and Schapire, 1999). s′(x, y) = s(x, y) + ∆(y, ygold) (5) Similarly to the scoring function s, the margin ∆ is defined as a decomposable function using 0-1 loss as follows: ∆(yi, ygold i ), { 0 if yi = ygold ∆(yi, ygold i ) = i (6) 1 otherwise Secondly, we update the weights w based on a max-violation update rule following Huang et al. (2012) (lines 6-7 in Figure 5). Finally, we employ not only perceptron (Collins, 2002) but also AROW (Mejer and Crammer, 2010; Crammer et al., 2013), AdaGrad (Duchi et al., 2011), and DCD-SSVM (Chang and Yih, 2013) for learning methods (line 7 in Figure 5.) We employ parameter averaging except for DCD-SSVM. AROW and AdaGrad store additional information for covariance and feature counts respectively, and DCDSSVM keeps a working set and performs additional updates in each iteration. Due to space limitations, we refer to the papers for the details of the learning methods. 2.5 Features Here, we explain </context>
</contexts>
<marker>Huang, Fayong, Guo, 2012</marker>
<rawString>Liang Huang, Suphan Fayong, and Yang Guo. 2012. Structured perceptron with inexact search. In Proceedings of the 2012 Conference of the North American Chapter of the Association for Computational Linguistics: Human Language Technologies, pages 142–151, Montr´eal, Canada, June. Association for Computational Linguistics.</rawString>
</citation>
<citation valid="true">
<authors>
<author>Rohit J Kate</author>
<author>Raymond Mooney</author>
</authors>
<title>Joint entity and relation extraction using card-pyramid parsing.</title>
<date>2010</date>
<booktitle>In Proceedings of the Fourteenth Conference on Computational Natural Language Learning,</booktitle>
<pages>203--212</pages>
<publisher>Association</publisher>
<institution>for Computational Linguistics.</institution>
<location>Uppsala, Sweden,</location>
<contexts>
<context position="21388" citStr="Kate and Mooney, 2010" startWordPosition="3716" endWordPosition="3719"> Relation relation Relation label and the label and word of its participating entity Table 5: Global features. for the test set evaluation, and show the performance on the test data set. 3.1 Evaluation settings We used an entity and relation recognition corpus by Roth and Yih (2004)3. The corpus defines four named entity types Location, Organization, Person, and Other and five relation types Kill, Live In, Located In, OrgBased In and Work For. All the entities were words in the original corpus because all the spaces in entities were replaced with slashes. Previous systems (Roth and Yih, 2007; Kate and Mooney, 2010) used these word 3conll04.corp at http://cogcomp.cs.illinois. edu/page/resource_view/43 boundaries as they were, treated the boundaries as given, and focused the entity classification problem alone. Differently from such systems, we recovered these spaces by replacing these slashes with spaces to evaluate the entity boundary detection performance on this corpus. Due to this replacement and the inclusion of the boundary detection problem, our task is more challenging than the original task, and our results are not comparable with those by the previous systems. The corpus contains 1,441 sentence</context>
<context position="30094" citStr="Kate and Mooney, 2010" startWordPosition="5123" endWordPosition="5126">y recognition does not necessarily improve the relation extraction performance. Search orders also affected the performance, and the worst order (right to left, down to up) and best order (close-first, left to right) were significantly different. The performance of the worst order was worse than that of the pipeline approach, although the difference was not significant. These results show that it is necessary to carefully select the search order for the joint entity and relation extraction task. 3.4 Comparison with Other Systems To compare our model with the other systems (Roth and Yih, 2007; Kate and Mooney, 2010), we evaluated the performance of our model when the entity boundaries were given. Differently from our setting in §3.1, we used the gold entity boundaries encoded in the BILOU scheme and assigned entity labels to the boundaries. We performed 5-fold cross validation on the data set following Roth and Yih (2007) although the split was different from theirs since their splits were not available. We employed the default parameter setting in §3.2 for this comparison. Table 7 shows the evaluation results. Although we cannot directly compare the results, our model performs better than the other mode</context>
<context position="31880" citStr="Kate and Mooney, 2010" startWordPosition="5409" endWordPosition="5412"> 2012) and co-reference resolution (Stoyanov and Eisner, 2012). Search order, however, has not been focused in relation extraction tasks. Named entity recognition (Florian et al., 2003; Nadeau and Sekine, 2007) and relation extraction (Zelenko et al., 2003; Miwa et al., 2009) have often been treated as separate tasks, but there are some previous studies that treat entities and relations jointly in learning. Most studies built joint learning models upon individual models for subtasks, such as Integer Linear Programming (ILP) (Roth and Yih, 2007; Yang and Cardie, 2013) and Card-Pyramid Parsing (Kate and Mooney, 2010). Our approach does not require such individual models, and it also can detect entity boundaries that these approaches except for Yang and Cardie (2013) did not treat. Other studies (Yu and Lam, 2010; Singh et al., 2013) built global probabilistic graphical models. They need to compute distributions over variables, but our approach does not. Li and Ji (2014) proposed an approach to jointly find entities and relations. They incorporated a semi-Markov chain in representing entities and they defined two actions during search, but our approach does not employ such representation and actions, and t</context>
<context position="34489" citStr="Kate and Mooney (2010)" startWordPosition="5903" endWordPosition="5906">/ 0.491 / 0.618 0.797 / 0.471 / 0.592 Easy-first 0.811 / 0.790 / 0.801 0.862 / 0.415 / 0.560 0.831 / 0.399 / 0.540 Entity-first, easy-first† 0.812 / 0.802 / 0.807 0.783 / 0.524 / 0.628 0.760 / 0.509 / 0.610* Close-first, easy-first 0.816 / 0.803 / 0.810 0.796 / 0.486 / 0.603 0.767 / 0.468 / 0.581 Table 6: Performance of entity and relation extraction on the test data set (precision / recall / F1 score). The † denotes the default parameter setting in §3.2 and * represents a significant improvement over the underlined “Pipeline” baseline (p&lt;0.05). Labels (a)-(f) correspond to those in Figure 4. Kate and Mooney (2010) Roth and Yih (2007) Entity-first, easy-first Person 0.921 / 0.942 / 0.932 0.891 / 0.895 / 0.890 0.931 / 0.948 / 0.939 Location 0.908 / 0.942 / 0.924 0.897 / 0.887 / 0.891 0.922 / 0.939 / 0.930 Organization 0.905 / 0.887 / 0.895 0.895 / 0.720 / 0.792 0.903 / 0.896 / 0.899 All entities - - 0.924 / 0.924 / 0.924 Located In 0.675 / 0.567 / 0.583 0.539 / 0.557 / 0.513 0.821 / 0.549 / 0.654 Work For 0.735 / 0.683 / 0.707 0.720 / 0.423 / 0.531 0.886 / 0.642 / 0.743 OrgBased In 0.662 / 0.641 / 0.647 0.798 / 0.416 / 0.543 0.768 / 0.572 / 0.654 Live In 0.664 / 0.601 / 0.629 0.591 / 0.490 / 0.530 0.819 </context>
</contexts>
<marker>Kate, Mooney, 2010</marker>
<rawString>Rohit J. Kate and Raymond Mooney. 2010. Joint entity and relation extraction using card-pyramid parsing. In Proceedings of the Fourteenth Conference on Computational Natural Language Learning, pages 203–212, Uppsala, Sweden, July. Association for Computational Linguistics.</rawString>
</citation>
<citation valid="true">
<authors>
<author>Taku Kudo</author>
<author>Yuji Matsumoto</author>
</authors>
<title>Chunking with support vector machines.</title>
<date>2001</date>
<booktitle>In Proceedings of the Second Meeting of the North American Chapter of the Association for Computational Linguistics on Language Technologies,</booktitle>
<publisher>Association</publisher>
<institution>for Computational Linguistics.</institution>
<location>Stroudsburg, PA, USA.</location>
<contexts>
<context position="31049" citStr="Kudo and Matsumoto, 2001" startWordPosition="5277" endWordPosition="5280">t was different from theirs since their splits were not available. We employed the default parameter setting in §3.2 for this comparison. Table 7 shows the evaluation results. Although we cannot directly compare the results, our model performs better than the other models. Compared to Table 6, Table 7 also shows that the inclusion of entity boundary detection degrades the performance about 0.09 in F-score. 4 Related Work Search order in structured learning has been studied in several NLP tasks. Left-to-right and rightto-left orderings have been often investigated in sequential labeling tasks (Kudo and Matsumoto, 2001). Easy-first policy was firstly introduced by Goldberg and Elhadad (2010) for dependency parsing, and it was successfully employed in several tasks, such as joint POS tagging and dependency parsing (Ma et al., 2012) and co-reference resolution (Stoyanov and Eisner, 2012). Search order, however, has not been focused in relation extraction tasks. Named entity recognition (Florian et al., 2003; Nadeau and Sekine, 2007) and relation extraction (Zelenko et al., 2003; Miwa et al., 2009) have often been treated as separate tasks, but there are some previous studies that treat entities and relations j</context>
</contexts>
<marker>Kudo, Matsumoto, 2001</marker>
<rawString>Taku Kudo and Yuji Matsumoto. 2001. Chunking with support vector machines. In Proceedings of the Second Meeting of the North American Chapter of the Association for Computational Linguistics on Language Technologies, Stroudsburg, PA, USA. Association for Computational Linguistics.</rawString>
</citation>
<citation valid="true">
<authors>
<author>Qi Li</author>
<author>Heng Ji</author>
</authors>
<title>Incremental joint extraction of entity mentions and relations.</title>
<date>2014</date>
<booktitle>In Proceedings of the 52nd Annual Meeting of the Association for Computational Linguistics (Volume 1: Long Papers),</booktitle>
<pages>402--412</pages>
<publisher>Association</publisher>
<institution>for Computational Linguistics.</institution>
<location>Baltimore, Maryland,</location>
<contexts>
<context position="2491" citStr="Li and Ji (2014)" startWordPosition="370" endWordPosition="373">sks such as part-of-speech (POS) tagging and depenMrs. Tsuruyama is from Kumamoto Prefecture in Japan . PER LOC LOC Figure 1: An entity and relation example (Roth and Yih, 2004). Person (PER) and location (LOC) entities are connected by Live in and Located in relations. dency parsing, so local constraints are considered to be more important in the task. Joint learning approaches (Yang and Cardie, 2013; Singh et al., 2013) incorporate these dependencies and local constraints in their models; however most approaches are time-consuming and employ complex structures consisting of multiple models. Li and Ji (2014) recently proposed a history-based structured learning approach that is simpler and more computationally efficient than other approaches. While this approach is promising, it still has a complexity in search and restricts the search order partly due to its semi-Markov representation, and thus the potential of the historybased learning is not fully investigated. In this paper, we introduce an entity and relation table to address the difficulty in representing the task. We propose a joint extraction of entities and relations using a history-based structured learning on the table. This table repr</context>
<context position="18534" citStr="Li and Ji (2014)" startWordPosition="3275" endWordPosition="3278">eatures are activated when all the information is available during decoding. We incorporate label dependency features like traditional sequential labeling for entities. Although our model can include other non-local features between entities (Ratinov and Roth, 2009), we do not include them expecting that global features on entities and relations can cover them. We design three types of global features for relations. These features are activated when all the participating relations are not 1 (non-relations). Features except for the “Crossing” category are similar to global relation features in Li and Ji (2014). We further incorporate global features for both entities and relations. These features are activated when the relation label is not 1. These features can act as a bridge between entities and relations. 3 Evaluation In this section, we first introduce the corpus and evaluation metrics that we employed for evaluation. We then show the performance on the training data set with explaining the parameters used 2We tried other “Entity+Relation” features to represent a relation and both its participating entities, but they slightly degraded the performance in our preliminary experiments. ∆(y,ygold) </context>
<context position="27760" citStr="Li and Ji (2014)" startWordPosition="4755" endWordPosition="4758"> Dynamic search orders Figure 6: Learning curves of entity and relation extraction on the training data set using 5-fold cross validation. between the performances with the best order and worst order was about 0.04 in an F1 score, which is statistically significant, and the performance can be worse than the pipeline approach in Figure 6(c). This means improvement by joint learning can be easily cancelled out if we do not carefully consider search order. It is also surprising that the second worst order (Figure 4(b)) is the most intuitive “left-to-right” order, which is closest to the order in Li and Ji (2014) among the six search orders. Figure 6(e) shows the performance with dynamic search orders. Unfortunately, the easy-first policy did not work well on this entity and relation task, but, with the two enhancements, dynamic orders performed as well as the best static order in Figure 6(d). This shows that entities should be de1865 tected earlier than relations on this data set. 3.3 Performance on Test Data Set Table 6 summarizes the performance on the test data set. We employed the default parameter setting explained in §3.2, and compared parameters by changing the parameters shown in the first co</context>
<context position="32240" citStr="Li and Ji (2014)" startWordPosition="5469" endWordPosition="5472"> entities and relations jointly in learning. Most studies built joint learning models upon individual models for subtasks, such as Integer Linear Programming (ILP) (Roth and Yih, 2007; Yang and Cardie, 2013) and Card-Pyramid Parsing (Kate and Mooney, 2010). Our approach does not require such individual models, and it also can detect entity boundaries that these approaches except for Yang and Cardie (2013) did not treat. Other studies (Yu and Lam, 2010; Singh et al., 2013) built global probabilistic graphical models. They need to compute distributions over variables, but our approach does not. Li and Ji (2014) proposed an approach to jointly find entities and relations. They incorporated a semi-Markov chain in representing entities and they defined two actions during search, but our approach does not employ such representation and actions, and thus it is more simple and flexible to investigate search orders. 5 Conclusions In this paper, we proposed a history-based structured learning approach that jointly detects enti1866 Parameter Entity Relation Entity+Relation Perceptron 0.809 / 0.809 / 0.809 0.760 / 0.547 / 0.636 0.731 / 0.527 / 0.612* AdaGrad 0.801 / 0.790 / 0.795 0.732 / 0.486 / 0.584 0.716 /</context>
</contexts>
<marker>Li, Ji, 2014</marker>
<rawString>Qi Li and Heng Ji. 2014. Incremental joint extraction of entity mentions and relations. In Proceedings of the 52nd Annual Meeting of the Association for Computational Linguistics (Volume 1: Long Papers), pages 402–412, Baltimore, Maryland, June. Association for Computational Linguistics.</rawString>
</citation>
<citation valid="true">
<authors>
<author>Ji Ma</author>
<author>Tong Xiao</author>
<author>Jingbo Zhu</author>
<author>Feiliang Ren</author>
</authors>
<title>Easy-first Chinese POS tagging and dependency parsing.</title>
<date>2012</date>
<booktitle>In Proceedings of COLING 2012,</booktitle>
<pages>1731--1746</pages>
<location>Mumbai, India,</location>
<contexts>
<context position="31264" citStr="Ma et al., 2012" startWordPosition="5312" endWordPosition="5315"> our model performs better than the other models. Compared to Table 6, Table 7 also shows that the inclusion of entity boundary detection degrades the performance about 0.09 in F-score. 4 Related Work Search order in structured learning has been studied in several NLP tasks. Left-to-right and rightto-left orderings have been often investigated in sequential labeling tasks (Kudo and Matsumoto, 2001). Easy-first policy was firstly introduced by Goldberg and Elhadad (2010) for dependency parsing, and it was successfully employed in several tasks, such as joint POS tagging and dependency parsing (Ma et al., 2012) and co-reference resolution (Stoyanov and Eisner, 2012). Search order, however, has not been focused in relation extraction tasks. Named entity recognition (Florian et al., 2003; Nadeau and Sekine, 2007) and relation extraction (Zelenko et al., 2003; Miwa et al., 2009) have often been treated as separate tasks, but there are some previous studies that treat entities and relations jointly in learning. Most studies built joint learning models upon individual models for subtasks, such as Integer Linear Programming (ILP) (Roth and Yih, 2007; Yang and Cardie, 2013) and Card-Pyramid Parsing (Kate a</context>
</contexts>
<marker>Ma, Xiao, Zhu, Ren, 2012</marker>
<rawString>Ji Ma, Tong Xiao, Jingbo Zhu, and Feiliang Ren. 2012. Easy-first Chinese POS tagging and dependency parsing. In Proceedings of COLING 2012, pages 1731–1746, Mumbai, India, December. The COLING 2012 Organizing Committee.</rawString>
</citation>
<citation valid="true">
<authors>
<author>Avihai Mejer</author>
<author>Koby Crammer</author>
</authors>
<title>Confidence in structured-prediction using confidence-weighted models.</title>
<date>2010</date>
<booktitle>In Proceedings of the 2010 Conference on Empirical Methods in Natural Language Processing,</booktitle>
<pages>971--981</pages>
<publisher>Association</publisher>
<institution>for Computational Linguistics.</institution>
<location>Cambridge, MA,</location>
<contexts>
<context position="16702" citStr="Mejer and Crammer, 2010" startWordPosition="2981" endWordPosition="2984">∆ into the scoring function as follows so that wrong assignments with small differences from gold assignments are penalized (lines 4 and 6 in Figure 5) (Freund and Schapire, 1999). s′(x, y) = s(x, y) + ∆(y, ygold) (5) Similarly to the scoring function s, the margin ∆ is defined as a decomposable function using 0-1 loss as follows: ∆(yi, ygold i ), { 0 if yi = ygold ∆(yi, ygold i ) = i (6) 1 otherwise Secondly, we update the weights w based on a max-violation update rule following Huang et al. (2012) (lines 6-7 in Figure 5). Finally, we employ not only perceptron (Collins, 2002) but also AROW (Mejer and Crammer, 2010; Crammer et al., 2013), AdaGrad (Duchi et al., 2011), and DCD-SSVM (Chang and Yih, 2013) for learning methods (line 7 in Figure 5.) We employ parameter averaging except for DCD-SSVM. AROW and AdaGrad store additional information for covariance and feature counts respectively, and DCDSSVM keeps a working set and performs additional updates in each iteration. Due to space limitations, we refer to the papers for the details of the learning methods. 2.5 Features Here, we explain the local features flo,.al and the global features fglobal introduced in §2.2. 2.5.1 Local features Our focus is not to</context>
</contexts>
<marker>Mejer, Crammer, 2010</marker>
<rawString>Avihai Mejer and Koby Crammer. 2010. Confidence in structured-prediction using confidence-weighted models. In Proceedings of the 2010 Conference on Empirical Methods in Natural Language Processing, pages 971–981, Cambridge, MA, October. Association for Computational Linguistics.</rawString>
</citation>
<citation valid="true">
<authors>
<author>Makoto Miwa</author>
<author>Rune Sætre</author>
<author>Yusuke Miyao</author>
<author>Jun’ichi Tsujii</author>
</authors>
<title>A rich feature vector for protein-protein interaction extraction from multiple corpora.</title>
<date>2009</date>
<booktitle>In Proceedings of the 2009 Conference on Empirical Methods in Natural Language Processing,</booktitle>
<pages>121--130</pages>
<institution>Singapore, August. Association for Computational Linguistics.</institution>
<contexts>
<context position="17757" citStr="Miwa et al. (2009)" startWordPosition="3154" endWordPosition="3157">ning methods. 2.5 Features Here, we explain the local features flo,.al and the global features fglobal introduced in §2.2. 2.5.1 Local features Our focus is not to exploit useful local features for entities and relations, so we incorporate several features from existing work to realize a reasonable baseline. Table 4 summarizes the local features. Local features for entities (or words) are similar to the features used by Florian et al. (2003), but some features are generalized and extended, and gazetteer features are excluded. For relations (or pairs of words), we employ and extend features in Miwa et al. (2009). 2.5.2 Global features We design global features to represent dependencies among entities and relations. Table 5 summarizes the global features2. These global features are activated when all the information is available during decoding. We incorporate label dependency features like traditional sequential labeling for entities. Although our model can include other non-local features between entities (Ratinov and Roth, 2009), we do not include them expecting that global features on entities and relations can cover them. We design three types of global features for relations. These features are </context>
<context position="31534" citStr="Miwa et al., 2009" startWordPosition="5353" endWordPosition="5356"> tasks. Left-to-right and rightto-left orderings have been often investigated in sequential labeling tasks (Kudo and Matsumoto, 2001). Easy-first policy was firstly introduced by Goldberg and Elhadad (2010) for dependency parsing, and it was successfully employed in several tasks, such as joint POS tagging and dependency parsing (Ma et al., 2012) and co-reference resolution (Stoyanov and Eisner, 2012). Search order, however, has not been focused in relation extraction tasks. Named entity recognition (Florian et al., 2003; Nadeau and Sekine, 2007) and relation extraction (Zelenko et al., 2003; Miwa et al., 2009) have often been treated as separate tasks, but there are some previous studies that treat entities and relations jointly in learning. Most studies built joint learning models upon individual models for subtasks, such as Integer Linear Programming (ILP) (Roth and Yih, 2007; Yang and Cardie, 2013) and Card-Pyramid Parsing (Kate and Mooney, 2010). Our approach does not require such individual models, and it also can detect entity boundaries that these approaches except for Yang and Cardie (2013) did not treat. Other studies (Yu and Lam, 2010; Singh et al., 2013) built global probabilistic graphi</context>
</contexts>
<marker>Miwa, Sætre, Miyao, Tsujii, 2009</marker>
<rawString>Makoto Miwa, Rune Sætre, Yusuke Miyao, and Jun’ichi Tsujii. 2009. A rich feature vector for protein-protein interaction extraction from multiple corpora. In Proceedings of the 2009 Conference on Empirical Methods in Natural Language Processing, pages 121–130, Singapore, August. Association for Computational Linguistics.</rawString>
</citation>
<citation valid="true">
<authors>
<author>Yusuke Miyao</author>
<author>Jun’ichi Tsujii</author>
</authors>
<title>Feature forest models for probabilistic HPSG parsing.</title>
<date>2008</date>
<journal>Computational Linguistics,</journal>
<volume>34</volume>
<issue>1</issue>
<contexts>
<context position="22910" citStr="Miyao and Tsujii, 2008" startWordPosition="3960" endWordPosition="3963"> using a 5-fold cross validation on the training data set, and evaluated the performance on the test set. We prepared a pipeline approach as a baseline. We first trained an entity recognition model using the local and global features, and then trained a relation extraction model using the local features and global features without global “Relation” features in Table 5. We did not employ the global “Relation” features in this baseline since it is common to treat relation extraction as a multi-class classification problem. We extracted features using the results from two syntactic parsers Enju (Miyao and Tsujii, 2008) and LRDEP (Sagae and Tsujii, 2007). We employed feature hashing (Weinberger et al., 2009) and limited the feature space to 224. The numbers of features greatly varied for categories and targets. They also caused biased predictions that prefer entities to relations in our preliminary experiments. We thus chose to re-scale the features as follows. We normalized local features for each feature category and then for each target. We also normalized global features for each feature category, but we did not normalize them for each target since normalization was impossible during decoding. We instead</context>
</contexts>
<marker>Miyao, Tsujii, 2008</marker>
<rawString>Yusuke Miyao and Jun’ichi Tsujii. 2008. Feature forest models for probabilistic HPSG parsing. Computational Linguistics, 34(1):35–80, March.</rawString>
</citation>
<citation valid="true">
<authors>
<author>David Nadeau</author>
<author>Satoshi Sekine</author>
</authors>
<title>A survey of named entity recognition and classification.</title>
<date>2007</date>
<journal>Lingvisticae Investigationes,</journal>
<volume>30</volume>
<issue>1</issue>
<contexts>
<context position="31468" citStr="Nadeau and Sekine, 2007" startWordPosition="5341" endWordPosition="5344">Work Search order in structured learning has been studied in several NLP tasks. Left-to-right and rightto-left orderings have been often investigated in sequential labeling tasks (Kudo and Matsumoto, 2001). Easy-first policy was firstly introduced by Goldberg and Elhadad (2010) for dependency parsing, and it was successfully employed in several tasks, such as joint POS tagging and dependency parsing (Ma et al., 2012) and co-reference resolution (Stoyanov and Eisner, 2012). Search order, however, has not been focused in relation extraction tasks. Named entity recognition (Florian et al., 2003; Nadeau and Sekine, 2007) and relation extraction (Zelenko et al., 2003; Miwa et al., 2009) have often been treated as separate tasks, but there are some previous studies that treat entities and relations jointly in learning. Most studies built joint learning models upon individual models for subtasks, such as Integer Linear Programming (ILP) (Roth and Yih, 2007; Yang and Cardie, 2013) and Card-Pyramid Parsing (Kate and Mooney, 2010). Our approach does not require such individual models, and it also can detect entity boundaries that these approaches except for Yang and Cardie (2013) did not treat. Other studies (Yu an</context>
</contexts>
<marker>Nadeau, Sekine, 2007</marker>
<rawString>David Nadeau and Satoshi Sekine. 2007. A survey of named entity recognition and classification. Lingvisticae Investigationes, 30(1):3–26.</rawString>
</citation>
<citation valid="true">
<authors>
<author>Eric W Noreen</author>
</authors>
<title>Computer-Intensive Methods for Testing Hypotheses : An Introduction.</title>
<date>1989</date>
<location>WileyInterscience,</location>
<contexts>
<context position="28455" citStr="Noreen, 1989" startWordPosition="4873" endWordPosition="4874">h orders. Unfortunately, the easy-first policy did not work well on this entity and relation task, but, with the two enhancements, dynamic orders performed as well as the best static order in Figure 6(d). This shows that entities should be de1865 tected earlier than relations on this data set. 3.3 Performance on Test Data Set Table 6 summarizes the performance on the test data set. We employed the default parameter setting explained in §3.2, and compared parameters by changing the parameters shown in the first column. We performed a statistical test using the approximate randomization method (Noreen, 1989) on our primary measure (“Entity+Relation”). The results are almost consistent with the results on the training data set with a few exceptions. Differently from the results on the training data set, AdaGrad and AROW performed significantly worse than perceptron and DCD-SSVM and they performed slightly worse than the pipeline approach. This result shows that DCD-SSVM performs well with inexact search and the selection of learning methods can significantly affect the entity and relation extraction performance. The joint learning approach showed a significant improvement over the pipeline approac</context>
</contexts>
<marker>Noreen, 1989</marker>
<rawString>Eric W. Noreen. 1989. Computer-Intensive Methods for Testing Hypotheses : An Introduction. WileyInterscience, April.</rawString>
</citation>
<citation valid="true">
<authors>
<author>Lev Ratinov</author>
<author>Dan Roth</author>
</authors>
<title>Design challenges and misconceptions in named entity recognition.</title>
<date>2009</date>
<booktitle>In Proceedings of the Thirteenth Conference on Computational Natural Language Learning (CoNLL-2009),</booktitle>
<pages>147--155</pages>
<publisher>Association</publisher>
<institution>for Computational Linguistics.</institution>
<location>Boulder, Colorado,</location>
<contexts>
<context position="4360" citStr="Ratinov and Roth, 2009" startWordPosition="664" endWordPosition="667">ntity and relation structures in a sentence. We then overview our model on the table. We finally explain the decoding, learning, search order, and features in our model. 2.1 Entity and relation table The task we address in this work is the extraction of entities and their relations from a sentence. Entities are typed and may span multiple words. Relations are typed and directed. We use words to represent entities and relations. We assume entities do not overlap. We employ a BILOU (Begin, Inside, Last, Outside, Unit) encoding scheme that has been shown to outperform the traditional BIO scheme (Ratinov and Roth, 2009), and we will show that this scheme induces several label dependencies between words and between words and relations in §2.3.2. A label is assigned to a word according to the relative position to its corresponding entity and the type of the entity. Relations are represented with their types and directions. 1 denotes a non-relation pair, and —* and +— denote left-to-right and right-to-left relations, respectively. Relations are defined on not entities but words, since entities are not always given when relations are extracted. Relations on entities are mapped to relations on the last words of t</context>
<context position="18184" citStr="Ratinov and Roth, 2009" startWordPosition="3216" endWordPosition="3219">y Florian et al. (2003), but some features are generalized and extended, and gazetteer features are excluded. For relations (or pairs of words), we employ and extend features in Miwa et al. (2009). 2.5.2 Global features We design global features to represent dependencies among entities and relations. Table 5 summarizes the global features2. These global features are activated when all the information is available during decoding. We incorporate label dependency features like traditional sequential labeling for entities. Although our model can include other non-local features between entities (Ratinov and Roth, 2009), we do not include them expecting that global features on entities and relations can cover them. We design three types of global features for relations. These features are activated when all the participating relations are not 1 (non-relations). Features except for the “Crossing” category are similar to global relation features in Li and Ji (2014). We further incorporate global features for both entities and relations. These features are activated when the relation label is not 1. These features can act as a bridge between entities and relations. 3 Evaluation In this section, we first introdu</context>
</contexts>
<marker>Ratinov, Roth, 2009</marker>
<rawString>Lev Ratinov and Dan Roth. 2009. Design challenges and misconceptions in named entity recognition. In Proceedings of the Thirteenth Conference on Computational Natural Language Learning (CoNLL-2009), pages 147–155, Boulder, Colorado, June. Association for Computational Linguistics.</rawString>
</citation>
<citation valid="true">
<authors>
<author>Dan Roth</author>
<author>Wen-Tau Yih</author>
</authors>
<title>A linear programming formulation for global inference in natural language tasks.</title>
<date>2004</date>
<booktitle>In Hwee Tou Ng and Ellen Riloff, editors, HLT-NAACL 2004 Workshop: Eighth Conference on Computational Natural Language Learning (CoNLL-2004),</booktitle>
<pages>1--8</pages>
<publisher>Association for Computational Linguistics.</publisher>
<location>Boston, Massachusetts, USA,</location>
<contexts>
<context position="2052" citStr="Roth and Yih, 2004" startWordPosition="302" endWordPosition="305">ted in Figure 1. For dependencies between subtasks, a Live in relation requires PER and LOC entities, and vice versa. For in-subtask dependencies, the Live in relation between “Mrs. Tsutayama” and “Japan” can be inferred from the two other relations. Figure 1 also shows that the task has a flexible graph structure. This structure usually does not cover all the words in a sentence differently from other natural language processing (NLP) tasks such as part-of-speech (POS) tagging and depenMrs. Tsuruyama is from Kumamoto Prefecture in Japan . PER LOC LOC Figure 1: An entity and relation example (Roth and Yih, 2004). Person (PER) and location (LOC) entities are connected by Live in and Located in relations. dency parsing, so local constraints are considered to be more important in the task. Joint learning approaches (Yang and Cardie, 2013; Singh et al., 2013) incorporate these dependencies and local constraints in their models; however most approaches are time-consuming and employ complex structures consisting of multiple models. Li and Ji (2014) recently proposed a history-based structured learning approach that is simpler and more computationally efficient than other approaches. While this approach is </context>
<context position="21049" citStr="Roth and Yih (2004)" startWordPosition="3659" endWordPosition="3662">red word Relation shortest path features between non-shared words, augmented by a combination of relation labels and the shared word Cyclic Combinations of three relation labels that make a cycle Crossing Combinations of two relation labels that cross each other Entity + Entity- Relation label and the label of its participating entity Relation relation Relation label and the label and word of its participating entity Table 5: Global features. for the test set evaluation, and show the performance on the test data set. 3.1 Evaluation settings We used an entity and relation recognition corpus by Roth and Yih (2004)3. The corpus defines four named entity types Location, Organization, Person, and Other and five relation types Kill, Live In, Located In, OrgBased In and Work For. All the entities were words in the original corpus because all the spaces in entities were replaced with slashes. Previous systems (Roth and Yih, 2007; Kate and Mooney, 2010) used these word 3conll04.corp at http://cogcomp.cs.illinois. edu/page/resource_view/43 boundaries as they were, treated the boundaries as given, and focused the entity classification problem alone. Differently from such systems, we recovered these spaces by re</context>
</contexts>
<marker>Roth, Yih, 2004</marker>
<rawString>Dan Roth and Wen-Tau Yih. 2004. A linear programming formulation for global inference in natural language tasks. In Hwee Tou Ng and Ellen Riloff, editors, HLT-NAACL 2004 Workshop: Eighth Conference on Computational Natural Language Learning (CoNLL-2004), pages 1–8, Boston, Massachusetts, USA, May. Association for Computational Linguistics.</rawString>
</citation>
<citation valid="true">
<authors>
<author>Dan Roth</author>
<author>Wen-Tau Yih</author>
</authors>
<title>Global Inference for Entity and Relation Identification via a Linear Programming Formulation.</title>
<date>2007</date>
<publisher>MIT Press.</publisher>
<contexts>
<context position="21364" citStr="Roth and Yih, 2007" startWordPosition="3712" endWordPosition="3715">participating entity Relation relation Relation label and the label and word of its participating entity Table 5: Global features. for the test set evaluation, and show the performance on the test data set. 3.1 Evaluation settings We used an entity and relation recognition corpus by Roth and Yih (2004)3. The corpus defines four named entity types Location, Organization, Person, and Other and five relation types Kill, Live In, Located In, OrgBased In and Work For. All the entities were words in the original corpus because all the spaces in entities were replaced with slashes. Previous systems (Roth and Yih, 2007; Kate and Mooney, 2010) used these word 3conll04.corp at http://cogcomp.cs.illinois. edu/page/resource_view/43 boundaries as they were, treated the boundaries as given, and focused the entity classification problem alone. Differently from such systems, we recovered these spaces by replacing these slashes with spaces to evaluate the entity boundary detection performance on this corpus. Due to this replacement and the inclusion of the boundary detection problem, our task is more challenging than the original task, and our results are not comparable with those by the previous systems. The corpus</context>
<context position="30070" citStr="Roth and Yih, 2007" startWordPosition="5119" endWordPosition="5122">performance on entity recognition does not necessarily improve the relation extraction performance. Search orders also affected the performance, and the worst order (right to left, down to up) and best order (close-first, left to right) were significantly different. The performance of the worst order was worse than that of the pipeline approach, although the difference was not significant. These results show that it is necessary to carefully select the search order for the joint entity and relation extraction task. 3.4 Comparison with Other Systems To compare our model with the other systems (Roth and Yih, 2007; Kate and Mooney, 2010), we evaluated the performance of our model when the entity boundaries were given. Differently from our setting in §3.1, we used the gold entity boundaries encoded in the BILOU scheme and assigned entity labels to the boundaries. We performed 5-fold cross validation on the data set following Roth and Yih (2007) although the split was different from theirs since their splits were not available. We employed the default parameter setting in §3.2 for this comparison. Table 7 shows the evaluation results. Although we cannot directly compare the results, our model performs be</context>
<context position="31807" citStr="Roth and Yih, 2007" startWordPosition="5398" endWordPosition="5401">l tasks, such as joint POS tagging and dependency parsing (Ma et al., 2012) and co-reference resolution (Stoyanov and Eisner, 2012). Search order, however, has not been focused in relation extraction tasks. Named entity recognition (Florian et al., 2003; Nadeau and Sekine, 2007) and relation extraction (Zelenko et al., 2003; Miwa et al., 2009) have often been treated as separate tasks, but there are some previous studies that treat entities and relations jointly in learning. Most studies built joint learning models upon individual models for subtasks, such as Integer Linear Programming (ILP) (Roth and Yih, 2007; Yang and Cardie, 2013) and Card-Pyramid Parsing (Kate and Mooney, 2010). Our approach does not require such individual models, and it also can detect entity boundaries that these approaches except for Yang and Cardie (2013) did not treat. Other studies (Yu and Lam, 2010; Singh et al., 2013) built global probabilistic graphical models. They need to compute distributions over variables, but our approach does not. Li and Ji (2014) proposed an approach to jointly find entities and relations. They incorporated a semi-Markov chain in representing entities and they defined two actions during search</context>
<context position="34509" citStr="Roth and Yih (2007)" startWordPosition="5907" endWordPosition="5910"> 0.471 / 0.592 Easy-first 0.811 / 0.790 / 0.801 0.862 / 0.415 / 0.560 0.831 / 0.399 / 0.540 Entity-first, easy-first† 0.812 / 0.802 / 0.807 0.783 / 0.524 / 0.628 0.760 / 0.509 / 0.610* Close-first, easy-first 0.816 / 0.803 / 0.810 0.796 / 0.486 / 0.603 0.767 / 0.468 / 0.581 Table 6: Performance of entity and relation extraction on the test data set (precision / recall / F1 score). The † denotes the default parameter setting in §3.2 and * represents a significant improvement over the underlined “Pipeline” baseline (p&lt;0.05). Labels (a)-(f) correspond to those in Figure 4. Kate and Mooney (2010) Roth and Yih (2007) Entity-first, easy-first Person 0.921 / 0.942 / 0.932 0.891 / 0.895 / 0.890 0.931 / 0.948 / 0.939 Location 0.908 / 0.942 / 0.924 0.897 / 0.887 / 0.891 0.922 / 0.939 / 0.930 Organization 0.905 / 0.887 / 0.895 0.895 / 0.720 / 0.792 0.903 / 0.896 / 0.899 All entities - - 0.924 / 0.924 / 0.924 Located In 0.675 / 0.567 / 0.583 0.539 / 0.557 / 0.513 0.821 / 0.549 / 0.654 Work For 0.735 / 0.683 / 0.707 0.720 / 0.423 / 0.531 0.886 / 0.642 / 0.743 OrgBased In 0.662 / 0.641 / 0.647 0.798 / 0.416 / 0.543 0.768 / 0.572 / 0.654 Live In 0.664 / 0.601 / 0.629 0.591 / 0.490 / 0.530 0.819 / 0.532 / 0.644 Kill</context>
</contexts>
<marker>Roth, Yih, 2007</marker>
<rawString>Dan Roth and Wen-Tau Yih, 2007. Global Inference for Entity and Relation Identification via a Linear Programming Formulation. MIT Press.</rawString>
</citation>
<citation valid="true">
<authors>
<author>Kenji Sagae</author>
<author>Jun’ichi Tsujii</author>
</authors>
<title>Dependency parsing and domain adaptation with LR models and parser ensembles.</title>
<date>2007</date>
<booktitle>In Proceedings of the CoNLL Shared Task Session of EMNLP-CoNLL 2007,</booktitle>
<pages>1044--1050</pages>
<publisher>Association</publisher>
<institution>for Computational Linguistics.</institution>
<location>Prague, Czech Republic,</location>
<contexts>
<context position="22945" citStr="Sagae and Tsujii, 2007" startWordPosition="3966" endWordPosition="3969"> the training data set, and evaluated the performance on the test set. We prepared a pipeline approach as a baseline. We first trained an entity recognition model using the local and global features, and then trained a relation extraction model using the local features and global features without global “Relation” features in Table 5. We did not employ the global “Relation” features in this baseline since it is common to treat relation extraction as a multi-class classification problem. We extracted features using the results from two syntactic parsers Enju (Miyao and Tsujii, 2008) and LRDEP (Sagae and Tsujii, 2007). We employed feature hashing (Weinberger et al., 2009) and limited the feature space to 224. The numbers of features greatly varied for categories and targets. They also caused biased predictions that prefer entities to relations in our preliminary experiments. We thus chose to re-scale the features as follows. We normalized local features for each feature category and then for each target. We also normalized global features for each feature category, but we did not normalize them for each target since normalization was impossible during decoding. We instead scaled the global features, and th</context>
</contexts>
<marker>Sagae, Tsujii, 2007</marker>
<rawString>Kenji Sagae and Jun’ichi Tsujii. 2007. Dependency parsing and domain adaptation with LR models and parser ensembles. In Proceedings of the CoNLL Shared Task Session of EMNLP-CoNLL 2007, pages 1044–1050, Prague, Czech Republic, June. Association for Computational Linguistics.</rawString>
</citation>
<citation valid="true">
<authors>
<author>Sameer Singh</author>
<author>Sebastian Riedel</author>
<author>Brian Martin</author>
<author>Jiaping Zheng</author>
<author>Andrew McCallum</author>
</authors>
<title>Joint inference of entities, relations, and coreference.</title>
<date>2013</date>
<booktitle>In Proceedings of the 2013 workshop on Automated knowledge base construction,</booktitle>
<pages>1--6</pages>
<publisher>ACM.</publisher>
<contexts>
<context position="2300" citStr="Singh et al., 2013" startWordPosition="342" endWordPosition="345">s. Figure 1 also shows that the task has a flexible graph structure. This structure usually does not cover all the words in a sentence differently from other natural language processing (NLP) tasks such as part-of-speech (POS) tagging and depenMrs. Tsuruyama is from Kumamoto Prefecture in Japan . PER LOC LOC Figure 1: An entity and relation example (Roth and Yih, 2004). Person (PER) and location (LOC) entities are connected by Live in and Located in relations. dency parsing, so local constraints are considered to be more important in the task. Joint learning approaches (Yang and Cardie, 2013; Singh et al., 2013) incorporate these dependencies and local constraints in their models; however most approaches are time-consuming and employ complex structures consisting of multiple models. Li and Ji (2014) recently proposed a history-based structured learning approach that is simpler and more computationally efficient than other approaches. While this approach is promising, it still has a complexity in search and restricts the search order partly due to its semi-Markov representation, and thus the potential of the historybased learning is not fully investigated. In this paper, we introduce an entity and rel</context>
<context position="32100" citStr="Singh et al., 2013" startWordPosition="5448" endWordPosition="5451"> extraction (Zelenko et al., 2003; Miwa et al., 2009) have often been treated as separate tasks, but there are some previous studies that treat entities and relations jointly in learning. Most studies built joint learning models upon individual models for subtasks, such as Integer Linear Programming (ILP) (Roth and Yih, 2007; Yang and Cardie, 2013) and Card-Pyramid Parsing (Kate and Mooney, 2010). Our approach does not require such individual models, and it also can detect entity boundaries that these approaches except for Yang and Cardie (2013) did not treat. Other studies (Yu and Lam, 2010; Singh et al., 2013) built global probabilistic graphical models. They need to compute distributions over variables, but our approach does not. Li and Ji (2014) proposed an approach to jointly find entities and relations. They incorporated a semi-Markov chain in representing entities and they defined two actions during search, but our approach does not employ such representation and actions, and thus it is more simple and flexible to investigate search orders. 5 Conclusions In this paper, we proposed a history-based structured learning approach that jointly detects enti1866 Parameter Entity Relation Entity+Relati</context>
</contexts>
<marker>Singh, Riedel, Martin, Zheng, McCallum, 2013</marker>
<rawString>Sameer Singh, Sebastian Riedel, Brian Martin, Jiaping Zheng, and Andrew McCallum. 2013. Joint inference of entities, relations, and coreference. In Proceedings of the 2013 workshop on Automated knowledge base construction, pages 1–6. ACM.</rawString>
</citation>
<citation valid="true">
<authors>
<author>Veselin Stoyanov</author>
<author>Jason Eisner</author>
</authors>
<title>Easy-first coreference resolution.</title>
<date>2012</date>
<booktitle>In Proceedings of COLING 2012,</booktitle>
<pages>2519--2534</pages>
<location>Mumbai, India,</location>
<contexts>
<context position="31320" citStr="Stoyanov and Eisner, 2012" startWordPosition="5319" endWordPosition="5322">ls. Compared to Table 6, Table 7 also shows that the inclusion of entity boundary detection degrades the performance about 0.09 in F-score. 4 Related Work Search order in structured learning has been studied in several NLP tasks. Left-to-right and rightto-left orderings have been often investigated in sequential labeling tasks (Kudo and Matsumoto, 2001). Easy-first policy was firstly introduced by Goldberg and Elhadad (2010) for dependency parsing, and it was successfully employed in several tasks, such as joint POS tagging and dependency parsing (Ma et al., 2012) and co-reference resolution (Stoyanov and Eisner, 2012). Search order, however, has not been focused in relation extraction tasks. Named entity recognition (Florian et al., 2003; Nadeau and Sekine, 2007) and relation extraction (Zelenko et al., 2003; Miwa et al., 2009) have often been treated as separate tasks, but there are some previous studies that treat entities and relations jointly in learning. Most studies built joint learning models upon individual models for subtasks, such as Integer Linear Programming (ILP) (Roth and Yih, 2007; Yang and Cardie, 2013) and Card-Pyramid Parsing (Kate and Mooney, 2010). Our approach does not require such ind</context>
</contexts>
<marker>Stoyanov, Eisner, 2012</marker>
<rawString>Veselin Stoyanov and Jason Eisner. 2012. Easy-first coreference resolution. In Proceedings of COLING 2012, pages 2519–2534, Mumbai, India, December. The COLING 2012 Organizing Committee.</rawString>
</citation>
<citation valid="true">
<authors>
<author>Kilian Weinberger</author>
<author>Anirban Dasgupta</author>
<author>John Langford</author>
<author>Alex Smola</author>
<author>Josh Attenberg</author>
</authors>
<title>Feature hashing for large scale multitask learning.</title>
<date>2009</date>
<booktitle>In Proceedings of the 26th Annual International Conference on Machine Learning, ICML ’09,</booktitle>
<pages>1113--1120</pages>
<publisher>ACM.</publisher>
<location>New York, NY, USA.</location>
<contexts>
<context position="23000" citStr="Weinberger et al., 2009" startWordPosition="3975" endWordPosition="3978">on the test set. We prepared a pipeline approach as a baseline. We first trained an entity recognition model using the local and global features, and then trained a relation extraction model using the local features and global features without global “Relation” features in Table 5. We did not employ the global “Relation” features in this baseline since it is common to treat relation extraction as a multi-class classification problem. We extracted features using the results from two syntactic parsers Enju (Miyao and Tsujii, 2008) and LRDEP (Sagae and Tsujii, 2007). We employed feature hashing (Weinberger et al., 2009) and limited the feature space to 224. The numbers of features greatly varied for categories and targets. They also caused biased predictions that prefer entities to relations in our preliminary experiments. We thus chose to re-scale the features as follows. We normalized local features for each feature category and then for each target. We also normalized global features for each feature category, but we did not normalize them for each target since normalization was impossible during decoding. We instead scaled the global features, and the scaling factor was tuned by using the same 5-fold cro</context>
</contexts>
<marker>Weinberger, Dasgupta, Langford, Smola, Attenberg, 2009</marker>
<rawString>Kilian Weinberger, Anirban Dasgupta, John Langford, Alex Smola, and Josh Attenberg. 2009. Feature hashing for large scale multitask learning. In Proceedings of the 26th Annual International Conference on Machine Learning, ICML ’09, pages 1113– 1120, New York, NY, USA. ACM.</rawString>
</citation>
<citation valid="true">
<authors>
<author>Bishan Yang</author>
<author>Claire Cardie</author>
</authors>
<title>Joint inference for fine-grained opinion extraction.</title>
<date>2013</date>
<booktitle>In Proceedings of the 51st Annual Meeting of the Association for Computational Linguistics (Volume 1: Long Papers),</booktitle>
<pages>1640--1649</pages>
<publisher>Association</publisher>
<institution>for Computational Linguistics.</institution>
<location>Sofia, Bulgaria,</location>
<contexts>
<context position="2279" citStr="Yang and Cardie, 2013" startWordPosition="338" endWordPosition="341"> the two other relations. Figure 1 also shows that the task has a flexible graph structure. This structure usually does not cover all the words in a sentence differently from other natural language processing (NLP) tasks such as part-of-speech (POS) tagging and depenMrs. Tsuruyama is from Kumamoto Prefecture in Japan . PER LOC LOC Figure 1: An entity and relation example (Roth and Yih, 2004). Person (PER) and location (LOC) entities are connected by Live in and Located in relations. dency parsing, so local constraints are considered to be more important in the task. Joint learning approaches (Yang and Cardie, 2013; Singh et al., 2013) incorporate these dependencies and local constraints in their models; however most approaches are time-consuming and employ complex structures consisting of multiple models. Li and Ji (2014) recently proposed a history-based structured learning approach that is simpler and more computationally efficient than other approaches. While this approach is promising, it still has a complexity in search and restricts the search order partly due to its semi-Markov representation, and thus the potential of the historybased learning is not fully investigated. In this paper, we introd</context>
<context position="31831" citStr="Yang and Cardie, 2013" startWordPosition="5402" endWordPosition="5405">nt POS tagging and dependency parsing (Ma et al., 2012) and co-reference resolution (Stoyanov and Eisner, 2012). Search order, however, has not been focused in relation extraction tasks. Named entity recognition (Florian et al., 2003; Nadeau and Sekine, 2007) and relation extraction (Zelenko et al., 2003; Miwa et al., 2009) have often been treated as separate tasks, but there are some previous studies that treat entities and relations jointly in learning. Most studies built joint learning models upon individual models for subtasks, such as Integer Linear Programming (ILP) (Roth and Yih, 2007; Yang and Cardie, 2013) and Card-Pyramid Parsing (Kate and Mooney, 2010). Our approach does not require such individual models, and it also can detect entity boundaries that these approaches except for Yang and Cardie (2013) did not treat. Other studies (Yu and Lam, 2010; Singh et al., 2013) built global probabilistic graphical models. They need to compute distributions over variables, but our approach does not. Li and Ji (2014) proposed an approach to jointly find entities and relations. They incorporated a semi-Markov chain in representing entities and they defined two actions during search, but our approach does </context>
</contexts>
<marker>Yang, Cardie, 2013</marker>
<rawString>Bishan Yang and Claire Cardie. 2013. Joint inference for fine-grained opinion extraction. In Proceedings of the 51st Annual Meeting of the Association for Computational Linguistics (Volume 1: Long Papers), pages 1640–1649, Sofia, Bulgaria, August. Association for Computational Linguistics.</rawString>
</citation>
<citation valid="true">
<authors>
<author>Xiaofeng Yu</author>
<author>Wai Lam</author>
</authors>
<title>Jointly identifying entities and extracting relations in encyclopedia text via a graphical model approach.</title>
<date>2010</date>
<journal>Organizing Committee.</journal>
<booktitle>In Coling 2010: Posters,</booktitle>
<pages>1399--1407</pages>
<location>Beijing, China,</location>
<contexts>
<context position="32079" citStr="Yu and Lam, 2010" startWordPosition="5444" endWordPosition="5447">2007) and relation extraction (Zelenko et al., 2003; Miwa et al., 2009) have often been treated as separate tasks, but there are some previous studies that treat entities and relations jointly in learning. Most studies built joint learning models upon individual models for subtasks, such as Integer Linear Programming (ILP) (Roth and Yih, 2007; Yang and Cardie, 2013) and Card-Pyramid Parsing (Kate and Mooney, 2010). Our approach does not require such individual models, and it also can detect entity boundaries that these approaches except for Yang and Cardie (2013) did not treat. Other studies (Yu and Lam, 2010; Singh et al., 2013) built global probabilistic graphical models. They need to compute distributions over variables, but our approach does not. Li and Ji (2014) proposed an approach to jointly find entities and relations. They incorporated a semi-Markov chain in representing entities and they defined two actions during search, but our approach does not employ such representation and actions, and thus it is more simple and flexible to investigate search orders. 5 Conclusions In this paper, we proposed a history-based structured learning approach that jointly detects enti1866 Parameter Entity R</context>
</contexts>
<marker>Yu, Lam, 2010</marker>
<rawString>Xiaofeng Yu and Wai Lam. 2010. Jointly identifying entities and extracting relations in encyclopedia text via a graphical model approach. In Coling 2010: Posters, pages 1399–1407, Beijing, China, August. Coling 2010 Organizing Committee.</rawString>
</citation>
<citation valid="true">
<authors>
<author>Dmitry Zelenko</author>
<author>Chinatsu Aone</author>
<author>Anthony Richardella</author>
</authors>
<title>Kernel methods for relation extraction.</title>
<date>2003</date>
<journal>The Journal of Machine Learning Research,</journal>
<pages>3--1083</pages>
<contexts>
<context position="31514" citStr="Zelenko et al., 2003" startWordPosition="5349" endWordPosition="5352">studied in several NLP tasks. Left-to-right and rightto-left orderings have been often investigated in sequential labeling tasks (Kudo and Matsumoto, 2001). Easy-first policy was firstly introduced by Goldberg and Elhadad (2010) for dependency parsing, and it was successfully employed in several tasks, such as joint POS tagging and dependency parsing (Ma et al., 2012) and co-reference resolution (Stoyanov and Eisner, 2012). Search order, however, has not been focused in relation extraction tasks. Named entity recognition (Florian et al., 2003; Nadeau and Sekine, 2007) and relation extraction (Zelenko et al., 2003; Miwa et al., 2009) have often been treated as separate tasks, but there are some previous studies that treat entities and relations jointly in learning. Most studies built joint learning models upon individual models for subtasks, such as Integer Linear Programming (ILP) (Roth and Yih, 2007; Yang and Cardie, 2013) and Card-Pyramid Parsing (Kate and Mooney, 2010). Our approach does not require such individual models, and it also can detect entity boundaries that these approaches except for Yang and Cardie (2013) did not treat. Other studies (Yu and Lam, 2010; Singh et al., 2013) built global </context>
</contexts>
<marker>Zelenko, Aone, Richardella, 2003</marker>
<rawString>Dmitry Zelenko, Chinatsu Aone, and Anthony Richardella. 2003. Kernel methods for relation extraction. The Journal of Machine Learning Research, 3:1083–1106.</rawString>
</citation>
</citationList>
</algorithm>
</algorithms>
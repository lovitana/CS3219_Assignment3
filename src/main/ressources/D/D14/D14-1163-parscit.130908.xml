<?xml version="1.0" encoding="UTF-8"?>
<algorithms version="110505">
<algorithm name="SectLabel" version="110505">
<variant no="0" confidence="0.000083">
<title confidence="0.9960925">
Jointly Learning Word Representations and Composition Functions
Using Predicate-Argument Structures
</title>
<author confidence="0.881314">
Kazuma Hashimoto†, Pontus Stenetorp†, Makoto Miwa‡, and Yoshimasa Tsuruoka††The University of Tokyo, 3-7-1 Hongo, Bunkyo-ku, Tokyo, Japan
</author>
<email confidence="0.91151">
{hassy,pontus,tsuruoka}@logos.t.u-tokyo.ac.jp
</email>
<affiliation confidence="0.596694">
‡Toyota Technological Institute, 2-12-1 Hisakata, Tempaku-ku, Nagoya, Japan
</affiliation>
<email confidence="0.819226">
makoto-miwa@toyota-ti.ac.jp
</email>
<sectionHeader confidence="0.98827" genericHeader="abstract">
Abstract
</sectionHeader>
<bodyText confidence="0.99999575">
We introduce a novel compositional lan-
guage model that works on Predicate-
Argument Structures (PASs). Our model
jointly learns word representations and
their composition functions using bag-
of-words and dependency-based con-
texts. Unlike previous word-sequence-
based models, our PAS-based model com-
poses arguments into predicates by using
the category information from the PAS.
This enables our model to capture long-
range dependencies between words and
to better handle constructs such as verb-
object and subject-verb-object relations.
We verify this experimentally using two
phrase similarity datasets and achieve re-
sults comparable to or higher than the pre-
vious best results. Our system achieves
these results without the need for pre-
trained word vectors and using a much
smaller training corpus; despite this, for
the subject-verb-object dataset our model
improves upon the state of the art by as
much as ∼10% in relative performance.
</bodyText>
<sectionHeader confidence="0.999133" genericHeader="keywords">
1 Introduction
</sectionHeader>
<bodyText confidence="0.999852604166667">
Studies on embedding single words in a vector
space have made notable successes in capturing
their syntactic and semantic properties (Turney
and Pantel, 2010). These embeddings have also
been found to be a useful component for Natural
Language Processing (NLP) systems; for exam-
ple, Turian et al. (2010) and Collobert et al. (2011)
demonstrated how low-dimensional word vectors
learned by Neural Network Language Models
(NNLMs) are beneficial for a wide range of NLP
tasks.
Recently, the main focus of research on vector
space representation is shifting from word repre-
sentations to phrase representations (Baroni and
Zamparelli, 2010; Grefenstette and Sadrzadeh,
2011; Mitchell and Lapata, 2010; Socher et al.,
2012). Combining the ideas of NNLMs and se-
mantic composition, Tsubaki et al. (2013) intro-
duced a novel NNLM incorporating verb-object
dependencies. More recently, Levy and Goldberg
(2014) presented a NNLM that integrated syntac-
tic dependencies. However, to the best of our
knowledge, there is no previous work on integrat-
ing a variety of syntactic and semantic dependen-
cies into NNLMs in order to learn composition
functions as well as word representations. The fol-
lowing question thus arises naturally:
Can a variety of dependencies be used to
jointly learn both stand-alone word vectors
and their compositions, embedding them in
the same vector space?
In this work, we bridge the gap between
purely context-based (Levy and Goldberg, 2014;
Mikolov et al., 2013b; Mnih and Kavukcuoglu,
2013) and compositional (Tsubaki et al., 2013)
NNLMs by using the flexible set of categories
from Predicate-Argument-Structures (PASs).
More specifically, we propose a Compositional
Log-Bilinear Language Model using PASs (PAS-
CLBLM), an overview of which is shown in
Figure 1. The model is trained by maximizing
the accuracy of predicting target words from their
bag-of-words and dependency-based context,
which provides information about selectional
preference. As shown in Figure 1 (b), one of the
advantages of the PAS-CLBLM is that the model
can treat not only word vectors but also composed
vectors as contexts. Since the composed vectors
</bodyText>
<page confidence="0.963249">
1544
</page>
<note confidence="0.991018">
Proceedings of the 2014 Conference on Empirical Methods in Natural Language Processing (EMNLP), pages 1544–1555,
October 25-29, 2014, Doha, Qatar. c�2014 Association for Computational Linguistics
</note>
<figureCaption confidence="0.9459562">
Figure 1: An overview of the proposed model: PAS-CLBLM. (a) The PAS-LBLM predicts target words
from their bag-of-words and dependency-based context words. (b) The PAS-CLBLM predicts target
words using not only context words but also composed vector representations derived from another level
of predicate-argument structures. Underlined words are target words and we only depict the bag-of-
words vector for the PAS-CLBLM.
</figureCaption>
<bodyText confidence="0.999892739130435">
are treated as input to the language model in
the same way as word vectors, these composed
vectors are expected to become similar to word
vectors for words with similar meanings.
Our empirical results demonstrate that the pro-
posed model has the ability to learn meaning-
ful representations for adjective-noun, noun-noun,
and (subject-) verb-object dependencies. On three
tasks of measuring the semantic similarity be-
tween short phrases (adjective-noun, noun-noun,
and verb-object), the learned composed vectors
achieve scores (Spearman’s rank correlation p)
comparable to or higher than those of previ-
ous models. On a task involving more complex
phrases (subject-verb-object), our learned com-
posed vectors achieve state-of-the-art performance
(p = 0.50) with a training corpus that is an order
of magnitude smaller than that used by previous
work (Tsubaki et al., 2013; Van de Cruys et al.,
2013). Moreover, the proposed model does not
require any pre-trained word vectors produced by
external models, but rather induces word vectors
jointly while training.
</bodyText>
<sectionHeader confidence="0.999833" genericHeader="introduction">
2 Related Work
</sectionHeader>
<bodyText confidence="0.996864363636364">
There is a large body of work on how to represent
the meaning of a word in a vector space. Distri-
butional approaches assume that the meaning of
a word is determined by the contexts in which it
appears (Firth, 1957). The context of a word is of-
ten defined as the words appearing in a window
of fixed-length (bag-of-words) and a simple ap-
proach is to treat the co-occurrence statistics of a
word w as a vector representation for w (Mitchell
and Lapata, 2008; Mitchell and Lapata, 2010); al-
ternatively, dependencies between words can be
used to define contexts (Goyal et al., 2013; Erk
and Pad´o, 2008; Thater et al., 2010).
In contrast to distributional representations,
NNLMs represent words in a low-dimensional
vector space (Bengio et al., 2003; Collobert et al.,
2011). Recently, Mikolov et al. (2013b) and Mnih
and Kavukcuoglu (2013) proposed highly scalable
models to learn high-dimensional word vectors.
Levy and Goldberg (2014) extended the model of
Mikolov et al. (2013b) by treating syntactic depen-
dencies as contexts.
Mitchell and Lapata (2008) investigated a vari-
ety of compositional operators to combine word
vectors into phrasal representations. Among these
operators, simple element-wise addition and mul-
tiplication are now widely used to represent short
phrases (Mitchell and Lapata, 2010; Blacoe and
Lapata, 2012). The obvious limitation with these
simple approaches is that information about word
order and syntactic relations is lost.
To incorporate syntactic information into com-
position functions, a variety of compositional
models have been proposed. These include recur-
sive neural networks using phrase-structure trees
(Socher et al., 2012; Socher et al., 2013b) and
models in which words have a specific form of
parameters according to their syntactic roles and
composition functions are syntactically dependent
on the relations of input words (Baroni and Zam-
parelli, 2010; Grefenstette and Sadrzadeh, 2011;
Hashimoto et al., 2013; Hermann and Blunsom,
2013; Socher et al., 2013a).
More recently, syntactic dependency-based
</bodyText>
<page confidence="0.988171">
1545
</page>
<bodyText confidence="0.999312">
compositional models have been proposed (Pa-
perno et al., 2014; Socher et al., 2014; Tsub-
aki et al., 2013). One of the advantages of these
models is that they are less restricted by word or-
der. Among these, Tsubaki et al. (2013) intro-
duced a novel compositional NNLM mainly fo-
cusing on verb-object dependencies and achieved
state-of-the-art performance for the task of mea-
suring the semantic similarity between subject-
verb-object phrases.
</bodyText>
<sectionHeader confidence="0.9192575" genericHeader="method">
3 PAS-CLBLM: A Compositional
Log-Bilinear Language Model Using
</sectionHeader>
<subsectionHeader confidence="0.682544">
Predicate-Argument Structures
</subsectionHeader>
<bodyText confidence="0.99997664">
In some recent studies on representing words as
vectors, word vectors are learned by solving word
prediction tasks (Mikolov et al., 2013a; Mnih and
Kavukcuoglu, 2013). More specifically, given tar-
get words and their context words, the basic idea
is to train classifiers to discriminate between each
target word and artificially induced negative tar-
get words. The feature vector of the classifiers are
calculated using the context word vectors whose
values are optimized during training. As a result,
vectors of words in similar contexts become simi-
lar to each other.
Following these studies, we propose a novel
model to jointly learn representations for words
and their compositions by training word predic-
tion classifiers using PASs. In this section, we
first describe the predicate-argument structures as
they serve as the basis of our model. We then
introduce a Log-Bilinear Language Model us-
ing Predicate-Argument Structures (PAS-LBLM)
to learn word representations using both bag-of-
words and dependency-based contexts. Finally,
we propose integrating compositions of words into
the model. Figure 1 (b) shows the overview of the
proposed model.
</bodyText>
<subsectionHeader confidence="0.999079">
3.1 Predicate-Argument Structures
</subsectionHeader>
<bodyText confidence="0.8911346">
Due to advances in deep parsing technologies,
syntactic parsers that can produce predicate-
argument structures are becoming accurate and
fast enough to be used for practical applications.
In this work, we use the probabilistic HPSG
parser Enju (Miyao and Tsujii, 2008) to obtain the
predicate-argument structures of individual sen-
tences. In its grammar, each word in a sentence
is treated as a predicate of a certain category with
zero or more arguments. Table 1 shows some ex-
Category predicate arg1 arg2
adj arg1 heavy rain
noun arg1 car accident
verb arg12 cause rain accident
prep arg12 at eat restaurant
</bodyText>
<tableCaption confidence="0.690506">
Table 1: Examples of predicates of different cate-
</tableCaption>
<bodyText confidence="0.99829836">
gories from the grammar of the Enju parser. arg1
and arg2 denote the first and second arguments.
amples of predicates of different categories.&apos; For
example, a predicate of the category verb arg12
expresses a verb with two arguments. A graph can
be constructed by connecting words in predicate-
argument structures in a sentence; in general, these
graphs are acyclic.
One of the merits of using predicate-argument
structures is that they can capture dependencies
between more than two words, while standard syn-
tactic dependency structures are limited to depen-
dencies between two words. For example, one of
the predicates in the phrase “heavy rain caused car
accidents” is the verb “cause”, and it has two ar-
guments (“rain” and “accident”). Furthermore, ex-
actly the same predicate-argument structure (pred-
icate: cause, first argument: rain, second argu-
ment: accident) is extracted from the passive form
of the above phrase: “car accidents were caused
by heavy rain”. This is helpful when capturing
semantic dependencies between predicates and ar-
guments, and in extracting facts or relations de-
scribed in a sentence, such as who did what to
whom.
</bodyText>
<subsectionHeader confidence="0.988328666666667">
3.2 A Log-Bilinear Language Model Using
Predicate-Argument Structures
3.2.1 PAS-based Word Prediction
</subsectionHeader>
<bodyText confidence="0.999328142857143">
The PAS-LBLM predicts a target word given its
PAS-based context. We assume that each word
w in the vocabulary d is represented with a d-
dimensional vector v(w). When a predicate of
category c is extracted from a sentence, the PAS-
LBLM computes the predicted d-dimensional vec-
tor p(wt) for the target word wt from its context
</bodyText>
<equation confidence="0.969478">
words w1, w2, ... , wm:
� m
p(wt) = f hz ⊙ v(wi) ,(1)
i=1
</equation>
<footnote confidence="0.992171">
&apos;The categories of the predicates in the Enju parser are
summarized at http://kmcs.nii.ac.jp/˜yusuke/
enju/enju-manual/enju-output-spec.html.
</footnote>
<page confidence="0.995817">
1546
</page>
<bodyText confidence="0.993013666666667">
where h E Rdx1 are category-specific weight
vectors and O denotes element-wise multiplica-
tion. f is a non-linearity function; in this work
we define f as tanh.
As an example following Figure 1 (a), when
the predicate “cause” is extracted with its first
and second arguments “rain” and “accident”, the
PAS-LBLM computes p(cause) E Rd following
Eq. (1):
</bodyText>
<equation confidence="0.99687225">
p(cause) = f(hverb arg12 O v(rain)+
arg1
hverb arg12 O v(accident)).
arg2
</equation>
<bodyText confidence="0.99753925">
In Eq. (2), the predicate is treated as the target
word, and its arguments are treated as the con-
text words. In the same way, an argument can be
treated as a target word:
</bodyText>
<equation confidence="0.99982225">
p(rain) = f(hverb arg12 O v(cause)+
hverb arg12 O v(accident)).
verb
arg2 (3)
</equation>
<bodyText confidence="0.999928904761905">
Relationship to previous work. If we omit the
the category-specific weight vectors hz in Eq. (1),
our model is similar to the CBOW model in
Mikolov et al. (2013a). CBOW predicts a tar-
get word given its surrounding bag-of-words con-
text, while our model uses its PAS-based context.
To incorporate the PAS information in our model
more efficiently, we use category-specific weight
vectors. Similarly, the vLBL model of Mnih and
Kavukcuoglu (2013) uses different weight vec-
tors depending on the position relative to the tar-
get word. As with previous neural network lan-
guage models (Collobert et al., 2011; Huang et al.,
2012), our model and vLBL can use weight ma-
trices rather than weight vectors. However, as dis-
cussed by Mnih and Teh (2012), using weight vec-
tors makes the training significantly faster than us-
ing weight matrices. Despite the simple formula-
tion of the element-wise operations, the category-
specific weight vectors efficiently propagate PAS-
based context information as explained next.
</bodyText>
<subsectionHeader confidence="0.929524">
3.2.2 Training Word Vectors
</subsectionHeader>
<bodyText confidence="0.999406">
To train the PAS-LBLM, we use a scoring function
to evaluate how well the target word wt fits the
given context:
</bodyText>
<equation confidence="0.996497">
s(wt,p(wt)) = v(wt)Tp(wt), (4)
</equation>
<bodyText confidence="0.999696222222222">
where v(wt) E Rdx1 is the scoring weight vector
for wt. Thus, the model parameters in the PAS-
LBLM are (V, V, H). V is the set of word vec-
tors v(w), and V is the set of scoring weight vec-
tors v(w). H is the set of the predicate-category-
specific weight vectors h,.
Based on the objective in the model of Collobert
et al. (2011), the model parameters are learned by
minimizing the following hinge loss:
</bodyText>
<equation confidence="0.992681666666667">
N
max(1 − s(wt,p(wt)) + s(wn,p(wt)), 0),
n=1
</equation>
<bodyText confidence="0.993785133333333">
(5)
where the negative sample wn is a randomly sam-
pled word other than wt, and N is the number
of negative samples. In our experiments we set
N = 1. Following Mikolov et al. (2013b), nega-
tive samples were drawn from the distribution over
unigrams that we raise to the power 0.75 and then
normalize to once again attain a probability distri-
bution. We minimize the loss function in Eq. (5)
using AdaGrad (Duchi et al., 2011). For further
training details, see Section 4.5.
Relationship to softmax regression models.
The model parameters can be learned by maximiz-
ing the log probability of the target word wt based
on the softmax function:
</bodyText>
<equation confidence="0.986526">
exp(s(wt,p(wt)))
p(wt|context) = (6)
V|
,i=1 exp(s(wi,p(wt)))
</equation>
<bodyText confidence="0.999975913043478">
This is equivalent to a softmax regression model.
However, when the vocabulary V is large, com-
puting the softmax function in Eq. (6) is compu-
tationally expensive. If we do not need probabil-
ity distributions over words, we are not necessar-
ily restricted to using the probabilistic expressions.
Recently, several methods have been proposed to
efficiently learn word representations rather than
accurate language models (Collobert et al., 2011;
Mikolov et al., 2013b; Mnih and Kavukcuoglu,
2013), and our objective follows the work of Col-
lobert et al. (2011). Mikolov et al. (2013b) and
Mnih and Kavukcuoglu (2013) trained their mod-
els using word-dependent scoring weight vectors
which are the arguments of our scoring function
in Eq. (4). During development we also trained
our model using the negative sampling technique
of Mikolov et al. (2013b); however, we did not ob-
serve any significant performance difference.
Intuition behind the PAS-LBLM. Here we
briefly explain how each class of the model pa-
rameters of the PAS-LBLM contributes to learning
word representations at each stochastic gradient
</bodyText>
<figure confidence="0.415943">
(2)
</figure>
<page confidence="0.963013">
1547
</page>
<bodyText confidence="0.999966071428571">
decent step. The category-specific weight vectors
provide the PAS information for context word vec-
tors which we would like to learn. During train-
ing, context word vectors having the same PAS-
based syntactic roles are updated similarly. The
word-dependent scoring weight vectors propagate
the information on which words should, or should
not, be predicted. In effect, context word vectors
making similar contributions to word predictions
are updated similarly. The non-linear function f
provides context words with information on the
other context words in the same PAS. In this way,
word vectors are expected to be learned efficiently
by the PAS-LBLM.
</bodyText>
<subsectionHeader confidence="0.998954">
3.3 Learning Composition Functions
</subsectionHeader>
<bodyText confidence="0.999990333333333">
As explained in Section 3.1, predicate-argument
structures inherently form graphs whose nodes are
words in a sentence. Using the graphs, we can in-
tegrate relationships between multiple predicate-
argument structures into our model.
When the context word wi in Eq. (1), excluding
predicate words, has another predicate-argument
of category c′ as a dependency, we replace v(wi)
with the vector produced by the composition func-
tion for the predicate category c′. For example,
as shown in Figure 1 (b), when the first argument
“rain” of the predicate “cause” is also the argu-
ment of the predicate “heavy”, we first compute
the d-dimensional composed vector representation
for “heavy” and “rain”:
</bodyText>
<equation confidence="0.728434">
gc′(v(heavy),v(rain)), (7)
</equation>
<bodyText confidence="0.999781647058824">
where c′ is the category adj arg1, and gc′ is a func-
tion to combine input vectors for the predicate-
category c′. We can use any composition func-
tion that produces a representation of the same
dimensionality as its inputs, such as element-
wise addition/multiplication (Mitchell and Lap-
ata, 2008) or neural networks (Socher et al.,
2012). We then replace v(rain) in Eq. (2) with
gc′(v(heavy), v(rain)). When the second argu-
ment “accident” in Eq. (2) is also the argument
of the predicate “car”, v(accident) is replaced
with gc′′(v(car), v(accident)). c″is the predi-
cate category noun arg1. These multiple relation-
ships of predicate-argument structures should pro-
vide richer context information. We refer to the
PAS-LBLM with composition functions as PAS-
CLBLM.
</bodyText>
<subsectionHeader confidence="0.994063">
3.4 Bag-of-Words Sensitive PAS-CLBLM
</subsectionHeader>
<bodyText confidence="0.999965166666667">
Both the PAS-LBLM and PAS-CLBLM can take
meaningful relationships between words into ac-
count. However, at times, the number of context
words can be limited and the ability of other mod-
els to take ten or more words from a fixed con-
text in a bag-of-words (BoW) fashion could com-
pensate for this sparseness. Huang et al. (2012)
combined local and global contexts in their neural
network language models, and motivated by their
work, we integrate bag-of-words vectors into our
models. Concretely, we add an additional input
term to Eq. (1):
</bodyText>
<equation confidence="0.993042">
M
p(wt) = f(i=1
� hci O v(wi) + hBoW O v(BoW) ,
</equation>
<bodyText confidence="0.930562428571429">
(8)
where hc�oW E Rdx1 are additional weight vec-
tors, and v(BoW) E Rdx1 is the average of the
word vectors in the same sentence. To construct
the v(BoW) for each sentence, we average the
word vectors of nouns and verbs in the same sen-
tence, excluding the target and context words.
</bodyText>
<sectionHeader confidence="0.997797" genericHeader="method">
4 Experimental Settings
</sectionHeader>
<subsectionHeader confidence="0.973569">
4.1 Training Corpus
</subsectionHeader>
<bodyText confidence="0.99995875">
We used the British National Corpus (BNC) as our
training corpus, extracted 6 million sentences from
the original BNC files, and parsed them using the
Enju parser described in Section 3.1.
</bodyText>
<subsectionHeader confidence="0.9712995">
4.2 Word Sense Disambiguation Using
Part-of-Speech Tags
</subsectionHeader>
<bodyText confidence="0.999993705882353">
In general, words can have multiple syntactic us-
ages. For example, the word cause can be a
noun or a verb depending on its context. Most
of the previous work on learning word vectors
ignores this ambiguity since word sense disam-
biguation could potentially be performed after the
word vectors have been trained (Huang et al.,
2012; Kartsaklis and Sadrzadeh, 2013). Some re-
cent work explicitly assigns an independent vec-
tor for each word usage according to its part-of-
speech (POS) tag (Hashimoto et al., 2013; Kart-
saklis and Sadrzadeh, 2013). Alternatively, Baroni
and Zamparelli (2010) assigned different forms of
parameters to adjectives and nouns.
In our experiments, we combined each word
with its corresponding POS tags. We used the
base-forms provided by the Enju parser rather than
</bodyText>
<page confidence="0.990184">
1548
</page>
<figureCaption confidence="0.998457">
Figure 2: Two PAS-CLBLM training samples.
</figureCaption>
<bodyText confidence="0.999718833333333">
the surface-forms, and used the first two charac-
ters of the POS tags. For example, VB, VBP,
VBZ, VBG, VBD, VBI were all mapped to VB.
This resulted in two kinds of cause: cause II and
cause VB and we used the 100,000 most frequent
lowercased word-POS pairs in the BNC.
</bodyText>
<subsectionHeader confidence="0.9978305">
4.3 Selection of Training Samples Based on
Categories of Predicates
</subsectionHeader>
<bodyText confidence="0.999108">
To train the PAS-LBLM and PAS-CLBLM, we
could use all predicate categories. However, our
preliminary experiments showed that these cate-
gories covered many training samples which are
not directly relevant to our experimental setting,
such as determiner-noun dependencies. We thus
manually selected the categories used in our ex-
periments. The selected predicates are listed in
Table 1: adj arg1, noun arg1, prep arg12, and
verb arg12. These categories should provide
meaningful information on selectional preference.
For example, the prep arg12 denotes prepositions
with two arguments, such as “eat at restaurant”
which means that the verb “eat” is related to the
noun “restaurant” by the preposition “at”. Prepo-
sitions are one of the predicates whose arguments
can be verbs, and thus prepositions are important
in training the composition functions for (subject-)
verb-object dependencies as described in the next
paragraph.
Another point we had to consider was how
to construct the training samples for the PAS-
CLBLM. We constructed compositional training
samples as explained in Section 3.3 when c′ was
adj arg1, noun arg1, or verb arg12. Figure 2
shows two examples in addition to the example
in Figure 1 (b). Using such training samples, the
PAS-CLBLM could, for example, recognize from
the two predicate-argument structures, “eat food”
and “eat at restaurant”, that eating foods is an ac-
tion that occurs at restaurants.
</bodyText>
<table confidence="0.9986992">
Model Composition Function
Addl v(w1) + v(w2)
Addnl tanh(v(w1) + v(w2))
Waddl mc�ds O v(w1) + mc��g1 O v(w2)
Waddnl tanh(mc�d�Ov(w1)+mc ��g1Ov(w2))
</table>
<tableCaption confidence="0.853653666666667">
Table 2: Composition functions used in this work.
The examples are shown as the adjective-noun de-
pendency between w1 =“heavy” and w2 =“rain”.
</tableCaption>
<subsectionHeader confidence="0.999831">
4.4 Selection of Composition Functions
</subsectionHeader>
<bodyText confidence="0.99999535">
As described in Section 3.3, we are free to se-
lect any composition functions in Eq. (7). To
maintain the fast training speed of the PAS-
LBLM, we avoid dense matrix-vector multiplica-
tion in our composition functions. In Table 2,
we list the composition functions used for the
PAS-CLBLM. Addl is element-wise addition and
Addnl is element-wise addition with the non-
linear function tanh. The subscripts l and nl de-
note the words linear and non-linear. Similarly,
Waddl is element-wise weighted addition and
Waddnl is element-wise weighted addition with
the non-linear function tanh. The weight vec-
tors mc� E Rd×1 in Table 2 are predicate-category-
specific parameters which are learned during train-
ing. We investigate the effects of the non-linear
function tanh for these composition functions.
In the formulations of the backpropagation algo-
rithm, non-linear functions allow the input vectors
to weakly interact with each other.
</bodyText>
<subsectionHeader confidence="0.849074">
4.5 Initialization and Optimization of Model
Parameters
</subsectionHeader>
<bodyText confidence="0.9999884375">
We assigned a 50-dimensional vector for each
word-POS pair described in Section 4.2 and ini-
tialized the vectors and the scoring weight vec-
tors using small random values. In part inspired
by the initialization method of the weight matrices
in Socher et al. (2013a), we initialized all values
in the compositional weight vectors of the Waddl
and Waddnl as 1.0. The context weight vectors
were initialized using small random values.
We minimized the loss function in Eq. (5) us-
ing mini-batch SGD and AdaGrad (Duchi et al.,
2011). Using AdaGrad, the SGD’s learning rate
is adapted independently for each model parame-
ter. This is helpful in training the PAS-LBLM and
PAS-CLBLM, as they have conditionally depen-
dent model parameters with varying frequencies.
</bodyText>
<page confidence="0.985172">
1549
</page>
<bodyText confidence="0.999933333333333">
The mini-batch size was 32 and the learning rate
was 0.05 for each experiment, and no regulariza-
tion was used. To verify the semantics captured by
the proposed models during training and to tune
the hyperparameters, we used the WordSim-3532
word similarity data set (Finkelstein et al., 2001).
</bodyText>
<sectionHeader confidence="0.924657" genericHeader="method">
5 Evaluation on Phrase Similarity Tasks
</sectionHeader>
<subsectionHeader confidence="0.982591">
5.1 Evaluation Settings
</subsectionHeader>
<bodyText confidence="0.999684722222222">
The learned models were evaluated on four tasks
of measuring the semantic similarity between
short phrases. We performed evaluation using the
three tasks (AN, NN, and VO) in the dataset3 pro-
vided by Mitchell and Lapata (2010), and the SVO
task in the dataset4 provided by Grefenstette and
Sadrzadeh (2011).
The datasets include pairs of short phrases ex-
tracted from the BNC. AN, NN, and VO con-
tain 108 phrase pairs of adjective-noun, noun-
noun, and verb-object. SVO contains 200 pairs of
subject-verb-object phrases. Each phrase pair has
multiple human-ratings: the higher the rating is,
the more semantically similar the phrases. For ex-
ample, the subject-verb-object phrase pair of “stu-
dent write name” and “student spell name” has a
high rating. The pair “people try door” and “peo-
ple judge door” has a low rating.
For evaluation we used the Spearman’s rank
correlation p between the human-ratings and the
cosine similarity between the composed vector
pairs. We mainly used non-averaged human-
ratings for each pair, and as described in Section
5.3, we also used averaged human-ratings for the
SVO task. Each phrase pair in the datasets was an-
notated by more than two annotators. In the case
of averaged human ratings, we averaged multiple
human-ratings for each phrase pair, and in the case
of non-averaged human-ratings, we treated each
human-rating as a separate annotation.
With the PAS-CLBLM, we represented each
phrase using the composition functions listed in
Table 2. When there was no composition present,
we represented the phrase using element-wise ad-
dition. For example, when we trained the PAS-
CLBLM with the composition function Waddnl,
</bodyText>
<footnote confidence="0.9978965">
2http://www.cs.technion.ac.il/˜gabr/
resources/data/wordsim353/
3http://homepages.inf.ed.ac.uk/
s0453356/share
4http://www.cs.ox.ac.uk/activities/
compdistmeaning/GS2011data.txt
</footnote>
<table confidence="0.998549416666667">
Model AN NN VO
PAS-CLBLM (Addl) 0.52 0.44 0.35
PAS-CLBLM (Addnl) 0.52 0.46 0.45
PAS-CLBLM (Waddl) 0.48 0.39 0.34
PAS-CLBLM (Waddnl) 0.48 0.40 0.39
PAS-LBLM 0.41 0.44 0.39
word2vec 0.52 0.48 0.42
BL w/ BNC 0.48 0.50 0.35
HB w/ BNC 0.41 0.44 0.34
KS w/ ukWaC n/a n/a 0.45
K w/ BNC n/a n/a 0.41
Human agreement 0.52 0.49 0.55
</table>
<tableCaption confidence="0.867615">
Table 3: Spearman’s rank correlation scores p for
the three tasks: AN, NN, and VO.
</tableCaption>
<bodyText confidence="0.999960357142857">
the composed vector for each phrase was com-
puted using the Waddnl function, and when we
trained the PAS-LBLM, we used the element-wise
addition function. To compute the composed vec-
tors using the Waddl and Waddnl functions, we
used the categories of the predicates adj arg1,
noun arg1, and verb arg12 listed in Table 1.
As a strong baseline, we trained the Skip-gram
model of Mikolov et al. (2013b) using the pub-
licly available word2vec5 software. We fed the
POS-tagged BNC into word2vec since our models
utilize POS tags and trained 50-dimensional word
vectors using word2vec. For each phrase we then
computed the representation using vector addition.
</bodyText>
<subsectionHeader confidence="0.998323">
5.2 AN, NN, and VO Tasks
</subsectionHeader>
<bodyText confidence="0.999864411764706">
Table 3 shows the correlation scores p for the AN,
NN, and VO tasks. Human agreement denotes the
inter-annotator agreement. The word2vec baseline
achieves unexpectedly high scores for these three
tasks. Previously these kinds of models (Mikolov
et al., 2013b; Mnih and Kavukcuoglu, 2013) have
mainly been evaluated for word analogy tasks and,
to date, there has been no work using these word
vectors for the task of measuring the semantic sim-
ilarity between phrases. However, this experimen-
tal result suggests that word2vec can serve as a
strong baseline for these kinds of tasks, in addi-
tion to word analogy tasks.
In Table 3, BL, HB, KS, and K denote the work
of Blacoe and Lapata (2012), Hermann and Blun-
som (2013), Kartsaklis and Sadrzadeh (2013), and
Kartsaklis et al. (2013) respectively. Among these,
</bodyText>
<footnote confidence="0.975074">
5https://code.google.com/p/word2vec/
</footnote>
<page confidence="0.957142">
1550
</page>
<table confidence="0.999223083333333">
Model Corpus Averaged Non-averaged
SVO-SVO SVO-V SVO-SVO SVO-V
PAS-CLBLM (Addl) 0.29 0.34 0.24 0.28
PAS-CLBLM (Addnl) 0.27 0.32 0.24 0.28
PAS-CLBLM (Waddl) BNC 0.25 0.26 0.21 0.23
PAS-CLBLM (Waddnl) 0.42 0.50 0.34 0.41
PAS-LBLM 0.21 0.06 0.18 0.08
word2vec BNC 0.12 0.32 0.12 0.28
Grefenstette and Sadrzadeh (2011) BNC n/a n/a 0.21 n/a
Tsubaki et al. (2013) ukWaC n/a 0.47 n/a n/a
Van de Cruys et al. (2013) ukWaC n/a n/a 0.32 0.37
Human agreement 0.75 0.62
</table>
<tableCaption confidence="0.8576465">
Table 4: Spearman’s rank correlation scores p for the SVO task. Averaged denotes the p calculated by
averaged human ratings, and Non-averaged denotes the p calculated by non-averaged human ratings.
</tableCaption>
<bodyText confidence="0.998958166666667">
only Kartsaklis and Sadrzadeh (2013) used the
ukWaC corpus (Baroni et al., 2009) which is an or-
der of magnitude larger than the BNC. As we can
see in Table 3, the PAS-CLBLM (Addnl) achieves
scores comparable to and higher than those of the
baseline and the previous state-of-the-art results.
In relation to these results, the Waddl and Waddnl
variants of the PAS-CLBLM do not achieve great
improvements in performance. This indicates that
simple word vector addition can be sufficient to
compose representations for phrases consisting of
word pairs.
</bodyText>
<subsectionHeader confidence="0.991058">
5.3 SVO Task
</subsectionHeader>
<bodyText confidence="0.999885482758621">
Table 4 shows the correlation scores p for the SVO
task. The scores p for this task are reported for
both averaged and non-averaged human ratings.
This is due to a disagreement in previous work
regarding which metric to use when reporting re-
sults. Hence, we report the scores for both settings
in Table 4. Another point we should consider is
that some previous work reported scores based on
the similarity between composed representations
(Grefenstette and Sadrzadeh, 2011; Van de Cruys
et al., 2013), and others reported scores based on
the similarity between composed representations
and word representations of landmark verbs from
the dataset (Tsubaki et al., 2013; Van de Cruys et
al., 2013). For completeness, we report the scores
for both settings: SVO-SVO and SVO-V in Table 4.
The results show that the weighted addition
model with the non-linear function tanh (PAS-
CLBLM (Waddnl)) is effective for the more com-
plex phrase task. While simple vector addition is
sufficient for phrases consisting of word pairs, it is
clear from our experimental results that they fall
short for more complex structures such as those
involved in the SVO task.
Our PAS-CLBLM (Waddnl) model outperforms
the previous state-of-the-art scores for the SVO
task as reported by Tsubaki et al. (2013) and
Van de Cruys et al. (2013). As such, there are three
key points that we would like to emphasize:
</bodyText>
<listItem confidence="0.999011333333333">
(1) the difference of the training corpus size,
(2) the necessity of the pre-trained word vectors,
(3) the modularity of deep learning models.
</listItem>
<bodyText confidence="0.9993961">
Tsubaki et al. (2013) and Van de Cruys et al.
(2013) used the ukWaC corpus. This means our
model works better, despite using a considerably
smaller corpora. It should also be noted that, like
us, Grefenstette and Sadrzadeh (2011) used the
BNC corpus.
The model of Tsubaki et al. (2013) is based on
neural network language models which use syn-
tactic dependencies between verbs and their ob-
jects. While their novel model, which incorpo-
rates the idea of co-compositionality, works well
with pre-trained word vectors produced by exter-
nal models, it is not clear whether the pre-trained
vectors are required to achieve high scores. In
contrast, we have achieved state-of-the-art results
without the use of pre-trained word vectors.
Despite our model’s scalability, we trained 50-
dimensional vector representations for words and
their composition functions and achieved high
scores using this low dimensional vector space.
</bodyText>
<page confidence="0.969984">
1551
</page>
<table confidence="0.997769666666667">
model d AN NN VO SVO
Addl 50 0.52 0.44 0.35 0.24
1000 0.51 0.51 0.43 0.31
Addnl 50 0.52 0.46 0.45 0.24
1000 0.51 0.50 0.45 0.31
Waddl 50 0.48 0.39 0.34 0.21
1000 0.50 0.49 0.43 0.32
Waddnl 50 0.48 0.40 0.39 0.34
1000 0.51 0.48 0.48 0.34
</table>
<tableCaption confidence="0.752941">
Table 5: Comparison of the PAS-CLBLM between
d = 50 and d = 1000.
</tableCaption>
<table confidence="0.999564888888889">
model BoW AN NN VO SVO
Addl w/ 0.52 0.44 0.35 0.24
w/o 0.48 0.46 0.38 0.23
Addnl w/ 0.52 0.46 0.45 0.24
w/o 0.50 0.47 0.41 0.15
Waddl w/ 0.48 0.39 0.34 0.21
w/o 0.47 0.39 0.38 0.21
Waddnl w/ 0.48 0.40 0.39 0.34
w/o 0.52 0.42 0.33 0.26
</table>
<tableCaption confidence="0.9446955">
Table 6: Scores of the PAS-CLBLM with and
without BoW contexts.
</tableCaption>
<bodyText confidence="0.999609888888889">
This maintains the possibility to incorporate re-
cently developed deep learning composition func-
tions into our models, such as recursive neural
tensor networks (Socher et al., 2013b) and co-
compositional neural networks (Tsubaki et al.,
2013). While such complex composition functions
slow down the training of compositional models,
richer information could be captured during train-
ing.
</bodyText>
<subsectionHeader confidence="0.997148">
5.4 Effects of the Dimensionality
</subsectionHeader>
<bodyText confidence="0.9999884">
To see how the dimensionality of the word vectors
affects the scores, we trained the PAS-CLBLM for
each setting using 1,000-dimensional word vectors
and set the learning rate to 0.01. Table 5 shows
the scores for all four tasks. Note that we only re-
port the scores for the setting non-averaged SVO-
SVO here. As shown in Table 5, the scores consis-
tently improved with a few exceptions. The scores
p = 0.51 for the NN task and p = 0.48 for the
VO task are the best results to date. However, the
score p = 0.34 for the SVO task did not improve
by increasing the dimensionality. This means that
simply increasing the dimensionality of the word
vectors does not necessarily lead to better results
for complex phrases.
</bodyText>
<subsectionHeader confidence="0.999437">
5.5 Effects of Bag-of-Words Contexts
</subsectionHeader>
<bodyText confidence="0.999985214285714">
Lastly, we trained the PAS-CLBLM without the
bag-of-words contexts described in Section 3.4
and used 50-dimensional word vectors. As can be
seen in Table 6, large score improvements were
observed only for the VO and SVO tasks by in-
cluding the bag-of-words contexts and the non-
linearity function. It is likely that the results de-
pend on how the bag-of-words contexts are con-
structed. However, we leave this line of analysis
as future work. Both adjective-noun and noun-
noun phrase are noun phrases, and (subject-) verb-
object phrases can be regarded as complete sen-
tences. Therefore, different kinds of context infor-
mation might be required for both groups.
</bodyText>
<sectionHeader confidence="0.9597225" genericHeader="method">
6 Qualitative Analysis on Composed
Vectors
</sectionHeader>
<bodyText confidence="0.998639258064516">
An open question that remains is to what ex-
tent composition affects the representations pro-
duced by our PAS-CLBLM model. To evalu-
ate this we assigned a vector for each composed
representation. For example, the adjective-noun
dependency “heavy rain” would be assigned an
independent vector. We added the most fre-
quent 100,000 adjective-noun, noun-noun, and
(subject-) verb-object tuples to the vocabulary and
the resulting vocabulary contained 400,000 to-
kens (100,000+3×100,000). A similar method
for treating frequent neighboring words as single
words was introduced by Mikolov et al. (2013b).
However, some dependencies, such as (subject-)
verb-object phrases, are not always captured when
considering only neighboring words.
Table 7 (Io composition) shows some examples
of predicate-argument dependencies with their
closest neighbors in the vector space according
to the cosine similarity. The table shows that the
learned vectors of multiple words capture seman-
tic similarity. For example, the vector of “heavy
rain” is close to the vectors of words which ex-
press the phenomena heavily raining. The vector
of “new york” captures the concept of a major city.
The vectors of (subject-) verb-object dependencies
also capture the semantic similarity, which is the
main difference to previous approaches, such as
that of Mikolov et al. (2013b), which only consider
neighboring words. These results suggest that the
PAS-CLBLM can learn meaningful composition
</bodyText>
<page confidence="0.97536">
1552
</page>
<table confidence="0.999800722222222">
Query No composition Composition
(AN) rain rain
heavy thunderstorm sunshine
rain downpour storm
blizzard drizzle
much rain chill
(AN) general manager executive
chief vice president director
executive executive director representative
project manager officer
managing director administrator
(NN) second war war
world plane crash world
war riot race
last war holocaust
great war warfare
(NN) oslo york
new paris toronto
york birmingham paris
moscow edinburgh
madrid glasgow
(VO) make order make
make carry survey allow
payment pay tax demand
pay produce
impose tax bring
(VO) achieve objective solve
solve bridge gap alleviate
problem improve quality overcome
deliver information resolve
encourage development circumvent
(SVO) hold meeting take
meeting event take place get
take end season win
place discussion take place put
do work gain
</table>
<tableCaption confidence="0.9947875">
Table 7: Nearest neighbor vectors for multiple
words. POS-tags are not shown for simplicity.
</tableCaption>
<bodyText confidence="0.84138795">
category predicate arg1 arg2
adj arg1 2.38 6.55 -
noun arg1 3.37 5.60 -
verb arg12 6.78 2.57 2.18
Table 8: L2-norms of the 50-dimensional weight
vectors of the composition function Waddnl.
not always be sufficient. This can be observed in
Table 3 and Table 4, which demonstrates that verb-
related tasks are more difficult than noun-phrase
tasks.
While No composition captures the seman-
tic similarity well using independent parameters,
there is the issue of data sparseness. As the size of
the vocabulary increases, the number of tuples of
word dependencies increases rapidly. In this ex-
periment, we used only the 300,000 most frequent
tuples. In contrast to this, the learned composi-
tion functions can capture similar information us-
ing only word vectors and a small set of predicate
categories.
</bodyText>
<sectionHeader confidence="0.992083" genericHeader="conclusions">
7 Conclusion and Future Work
</sectionHeader>
<bodyText confidence="0.999920928571429">
We have presented a compositional log-bilinear
language model using predicate-argument struc-
tures that incorporates both bag-of-words and
dependency-based contexts. In our experiments
the learned composed vectors achieve state-of-the-
art scores for the task of measuring the semantic
similarity between short phrases. For the subject-
verb-object phrase task, the result is achieved
without any pre-trained word vectors using a cor-
pus an order of magnitude smaller than that of the
previous state of the art. For future work, we will
investigate how our models and the resulting vec-
tor representations can be helpful for other unsu-
pervised and/or supervised tasks.
</bodyText>
<sectionHeader confidence="0.99764" genericHeader="acknowledgments">
Acknowledgments
</sectionHeader>
<bodyText confidence="0.999951823529412">
We thank the anonymous reviewers for their help-
ful comments and suggestions. This work was
supported by JSPS KAKENHI Grant Number
13F03041.
functions since the composition layers receive the
same error signal via backpropagation.
We then trained the PAS-CLBLM using Waddnl
to learn composition functions. Table 7 (Compo-
sition) shows the nearest neighbor words for each
composed vector, and as we can see, the learned
composition function emphasizes the head words
and captures some sort of semantic similarity. We
then inspected the L2-norms of the weight vectors
of the composition function. As shown in Table 8,
head words are strongly emphasized. Emphasiz-
ing head words is helpful in representing com-
posed meanings, but in the case of verbs it may
</bodyText>
<sectionHeader confidence="0.998581" genericHeader="references">
References
</sectionHeader>
<reference confidence="0.998904461538462">
Marco Baroni and Roberto Zamparelli. 2010. Nouns
are vectors, adjectives are matrices: Representing
adjective-noun constructions in semantic space. In
Proceedings of the 2010 Conference on Empirical
Methods in Natural Language Processing, pages
1183–1193.
Marco Baroni, Silvia Bernardini, Adriano Ferraresi,
and Eros Zanchetta. 2009. The WaCky Wide Web:
A Collection of Very Large Linguistically Processed
Web-Crawled Corpora. Language Resources and
Evaluation, 43(3):209–226.
Yoshua Bengio, R´ejean Ducharme, Pascal Vincent, and
Christian Jauvin. 2003. A Neural Probabilistic Lan-
</reference>
<page confidence="0.835136">
1553
</page>
<reference confidence="0.996797036697248">
guage Model. Journal of Machine Learning Re-
search, 3:1137–1155.
William Blacoe and Mirella Lapata. 2012. A Com-
parison of Vector-based Representations for Seman-
tic Composition. In Proceedings of the 2012 Joint
Conference on Empirical Methods in Natural Lan-
guage Processing and Computational Natural Lan-
guage Learning, pages 546–556.
Ronan Collobert, Jason Weston, L´eon Bottou, Michael
Karlen, Koray Kavukcuoglu, and Pavel Kuksa.
2011. Natural Language Processing (Almost) from
Scratch. Journal of Machine Learning Research,
12:2493–2537.
John Duchi, Elad Hazan, and Yoram Singer. 2011.
Adaptive Subgradient Methods for Online Learning
and Stochastic Optimization. Journal of Machine
Learning Research, 12:2121–2159.
Katrin Erk and Sebastian Pad´o. 2008. A Structured
Vector Space Model for Word Meaning in Context.
In Proceedings of the 2008 Conference on Empiri-
cal Methods in Natural Language Processing, pages
897–906.
Lev Finkelstein, Gabrilovich Evgenly, Matias Yossi,
Rivlin Ehud, Solan Zach, Wolfman Gadi, and Rup-
pin Eytan. 2001. Placing Search in Context: The
Concept Revisited. In Proceedings of the Tenth In-
ternational World Wide Web Conference.
John Rupert Firth. 1957. A synopsis of linguistic
theory 1930-55. In Studies in Linguistic Analysis,
pages 1–32.
Kartik Goyal, Sujay Kumar Jauhar, Huiying Li, Mrin-
maya Sachan, Shashank Srivastava, and Eduard
Hovy. 2013. A Structured Distributional Seman-
tic Model : Integrating Structure with Semantics. In
Proceedings of the Workshop on Continuous Vector
Space Models and their Compositionality, pages 20–
29.
Edward Grefenstette and Mehrnoosh Sadrzadeh. 2011.
Experimental Support for a Categorical Composi-
tional Distributional Model of Meaning. In Pro-
ceedings of the 2011 Conference on Empirical Meth-
ods in Natural Language Processing, pages 1394–
1404.
Kazuma Hashimoto, Makoto Miwa, Yoshimasa Tsu-
ruoka, and Takashi Chikayama. 2013. Simple Cus-
tomization of Recursive Neural Networks for Se-
mantic Relation Classification. In Proceedings of
the 2013 Conference on Empirical Methods in Nat-
ural Language Processing, pages 1372–1376.
Karl Moritz Hermann and Phil Blunsom. 2013. The
Role of Syntax in Vector Space Models of Composi-
tional Semantics. In Proceedings of the 51st Annual
Meeting of the Association for Computational Lin-
guistics (Volume 1: Long Papers), pages 894–904.
Eric Huang, Richard Socher, Christopher Manning,
and Andrew Ng. 2012. Improving Word Represen-
tations via Global Context and Multiple Word Proto-
types. In Proceedings of the 50th Annual Meeting of
the Association for Computational Linguistics (Vol-
ume 1: Long Papers), pages 873–882.
Dimitri Kartsaklis and Mehrnoosh Sadrzadeh. 2013.
Prior Disambiguation of Word Tensors for Con-
structing Sentence Vectors. In Proceedings of the
2013 Conference on Empirical Methods in Natural
Language Processing, pages 1590–1601.
Dimitri Kartsaklis, Mehrnoosh Sadrzadeh, and Stephen
Pulman. 2013. Separating Disambiguation from
Composition in Distributional Semantics. In Pro-
ceedings of 17th Conference on Natural Language
Learning (CoNLL), pages 114–123.
Omer Levy and Yoav Goldberg. 2014. Dependency-
Based Word Embeddings. In Proceedings of the
52nd Annual Meeting of the Association for Compu-
tational Linguistics (Volume 2: Short Papers), pages
302–308.
Tomas Mikolov, Kai Chen, Greg Corrado, and Jeffrey
Dean. 2013a. Efficient Estimation of Word Repre-
sentations in Vector Space. In Proceedings of Work-
shop at the International Conference on Learning
Representations.
Tomas Mikolov, Ilya Sutskever, Kai Chen, Greg S Cor-
rado, and Jeff Dean. 2013b. Distributed Represen-
tations of Words and Phrases and their Composition-
ality. In Advances in Neural Information Processing
Systems 26, pages 3111–3119.
Jeff Mitchell and Mirella Lapata. 2008. Vector-based
models of semantic composition. In Proceedings of
46th Annual Meeting of the Association for Com-
putational Linguistics: Human Language Technolo-
gies, pages 236–244.
Jeff Mitchell and Mirella Lapata. 2010. Composition
in Distributional Models of Semantics. Cognitive
Science, 34(8):1388–1439.
Yusuke Miyao and Jun’ichi Tsujii. 2008. Feature for-
est models for probabilistic HPSG parsing. Compu-
tational Linguistics, 34(1):35–80.
Andriy Mnih and Koray Kavukcuoglu. 2013. Learning
word embeddings efficiently with noise-contrastive
estimation. In Advances in Neural Information Pro-
cessing Systems 26, pages 2265–2273.
Andriy Mnih and Yee Whye Teh. 2012. A fast
and simple algorithm for training neural probabilis-
tic language models. In John Langford and Joelle
Pineau, editors, Proceedings of the 29th Interna-
tional Conference on Machine Learning (ICML-12),
ICML ’12, pages 1751–1758.
Denis Paperno, Nghia The Pham, and Marco Baroni.
2014. A practical and linguistically-motivated ap-
proach to compositional distributional semantics. In
</reference>
<page confidence="0.888324">
1554
</page>
<reference confidence="0.9991274">
Proceedings of the 52nd Annual Meeting of the As-
sociation for Computational Linguistics (Volume 1:
Long Papers), pages 90–99.
Richard Socher, Brody Huval, Christopher D. Man-
ning, and Andrew Y. Ng. 2012. Semantic Compo-
sitionality through Recursive Matrix-Vector Spaces.
In Proceedings of the 2012 Joint Conference on
Empirical Methods in Natural Language Process-
ing and Computational Natural Language Learning,
pages 1201–1211.
Richard Socher, John Bauer, Christopher D. Manning,
and Ng Andrew Y. 2013a. Parsing with Compo-
sitional Vector Grammars. In Proceedings of the
51st Annual Meeting of the Association for Compu-
tational Linguistics (Volume 1: Long Papers), pages
455–465.
Richard Socher, Alex Perelygin, Jean Wu, Jason
Chuang, Christopher D. Manning, Andrew Ng, and
Christopher Potts. 2013b. Recursive Deep Mod-
els for Semantic Compositionality Over a Sentiment
Treebank. In Proceedings of the 2013 Conference
on Empirical Methods in Natural Language Pro-
cessing, pages 1631–1642.
Richard Socher, Quoc V. Le, Christopher D. Manning,
and Andrew Y. Ng. 2014. Grounded Compositional
Semantics for Finding and Describing Images with
Sentences. Transactions ofthe Association for Com-
putational Linguistics, 2:207–218.
Stefan Thater, Hagen F¨urstenau, and Manfred Pinkal.
2010. Contextualizing Semantic Representations
Using Syntactically Enriched Vector Models. In
Proceedings of the 48th Annual Meeting of the As-
sociation for Computational Linguistics, pages 948–
957.
Masashi Tsubaki, Kevin Duh, Masashi Shimbo, and
Yuji Matsumoto. 2013. Modeling and Learning Se-
mantic Co-Compositionality through Prototype Pro-
jections and Neural Networks. In Proceedings of the
2013 Conference on Empirical Methods in Natural
Language Processing, pages 130–140.
Joseph Turian, Lev-Arie Ratinov, and Yoshua Bengio.
2010. Word Representations: A Simple and General
Method for Semi-Supervised Learning. In Proceed-
ings of the 48th Annual Meeting of the Association
for Computational Linguistics, pages 384–394.
Peter D. Turney and Patrick Pantel. 2010. From Fre-
quency to Meaning: Vector Space Models of Se-
mantics. Journal ofArtificial Intelligence Research,
37(1):141–188.
Tim Van de Cruys, Thierry Poibeau, and Anna Korho-
nen. 2013. A Tensor-based Factorization Model of
Semantic Compositionality. In Proceedings of the
2013 Conference of the North American Chapter of
the Association for Computational Linguistics: Hu-
man Language Technologies, pages 1142–1151.
</reference>
<page confidence="0.993905">
1555
</page>
</variant>
</algorithm>
<algorithm name="ParsHed" version="110505">
<variant no="0" confidence="0.382204">
<title confidence="0.999356">Jointly Learning Word Representations and Composition Using Predicate-Argument Structures</title>
<author confidence="0.787181">Pontus Makoto</author>
<author confidence="0.787181">Yoshimasa University of Tokyo</author>
<address confidence="0.579416">Technological Institute, 2-12-1 Hisakata, Tempaku-ku, Nagoya,</address>
<email confidence="0.989491">makoto-miwa@toyota-ti.ac.jp</email>
<abstract confidence="0.99966256">We introduce a novel compositional language model that works on Predicate- Argument Structures (PASs). Our model jointly learns word representations and their composition functions using bagof-words and dependency-based contexts. Unlike previous word-sequencebased models, our PAS-based model composes arguments into predicates by using the category information from the PAS. This enables our model to capture longrange dependencies between words and to better handle constructs such as verbobject and subject-verb-object relations. We verify this experimentally using two phrase similarity datasets and achieve results comparable to or higher than the previous best results. Our system achieves these results without the need for pretrained word vectors and using a much smaller training corpus; despite this, for the subject-verb-object dataset our model improves upon the state of the art by as as relative performance.</abstract>
</variant>
</algorithm>
<algorithm name="ParsCit" version="110505">
<citationList>
<citation valid="true">
<authors>
<author>Marco Baroni</author>
<author>Roberto Zamparelli</author>
</authors>
<title>Nouns are vectors, adjectives are matrices: Representing adjective-noun constructions in semantic space.</title>
<date>2010</date>
<booktitle>In Proceedings of the 2010 Conference on Empirical Methods in Natural Language Processing,</booktitle>
<pages>1183--1193</pages>
<contexts>
<context position="1980" citStr="Baroni and Zamparelli, 2010" startWordPosition="275" endWordPosition="278"> on embedding single words in a vector space have made notable successes in capturing their syntactic and semantic properties (Turney and Pantel, 2010). These embeddings have also been found to be a useful component for Natural Language Processing (NLP) systems; for example, Turian et al. (2010) and Collobert et al. (2011) demonstrated how low-dimensional word vectors learned by Neural Network Language Models (NNLMs) are beneficial for a wide range of NLP tasks. Recently, the main focus of research on vector space representation is shifting from word representations to phrase representations (Baroni and Zamparelli, 2010; Grefenstette and Sadrzadeh, 2011; Mitchell and Lapata, 2010; Socher et al., 2012). Combining the ideas of NNLMs and semantic composition, Tsubaki et al. (2013) introduced a novel NNLM incorporating verb-object dependencies. More recently, Levy and Goldberg (2014) presented a NNLM that integrated syntactic dependencies. However, to the best of our knowledge, there is no previous work on integrating a variety of syntactic and semantic dependencies into NNLMs in order to learn composition functions as well as word representations. The following question thus arises naturally: Can a variety of d</context>
<context position="7075" citStr="Baroni and Zamparelli, 2010" startWordPosition="1054" endWordPosition="1058"> phrases (Mitchell and Lapata, 2010; Blacoe and Lapata, 2012). The obvious limitation with these simple approaches is that information about word order and syntactic relations is lost. To incorporate syntactic information into composition functions, a variety of compositional models have been proposed. These include recursive neural networks using phrase-structure trees (Socher et al., 2012; Socher et al., 2013b) and models in which words have a specific form of parameters according to their syntactic roles and composition functions are syntactically dependent on the relations of input words (Baroni and Zamparelli, 2010; Grefenstette and Sadrzadeh, 2011; Hashimoto et al., 2013; Hermann and Blunsom, 2013; Socher et al., 2013a). More recently, syntactic dependency-based 1545 compositional models have been proposed (Paperno et al., 2014; Socher et al., 2014; Tsubaki et al., 2013). One of the advantages of these models is that they are less restricted by word order. Among these, Tsubaki et al. (2013) introduced a novel compositional NNLM mainly focusing on verb-object dependencies and achieved state-of-the-art performance for the task of measuring the semantic similarity between subjectverb-object phrases. 3 PAS</context>
<context position="19405" citStr="Baroni and Zamparelli (2010)" startWordPosition="3047" endWordPosition="3050">se Disambiguation Using Part-of-Speech Tags In general, words can have multiple syntactic usages. For example, the word cause can be a noun or a verb depending on its context. Most of the previous work on learning word vectors ignores this ambiguity since word sense disambiguation could potentially be performed after the word vectors have been trained (Huang et al., 2012; Kartsaklis and Sadrzadeh, 2013). Some recent work explicitly assigns an independent vector for each word usage according to its part-ofspeech (POS) tag (Hashimoto et al., 2013; Kartsaklis and Sadrzadeh, 2013). Alternatively, Baroni and Zamparelli (2010) assigned different forms of parameters to adjectives and nouns. In our experiments, we combined each word with its corresponding POS tags. We used the base-forms provided by the Enju parser rather than 1548 Figure 2: Two PAS-CLBLM training samples. the surface-forms, and used the first two characters of the POS tags. For example, VB, VBP, VBZ, VBG, VBD, VBI were all mapped to VB. This resulted in two kinds of cause: cause II and cause VB and we used the 100,000 most frequent lowercased word-POS pairs in the BNC. 4.3 Selection of Training Samples Based on Categories of Predicates To train the </context>
</contexts>
<marker>Baroni, Zamparelli, 2010</marker>
<rawString>Marco Baroni and Roberto Zamparelli. 2010. Nouns are vectors, adjectives are matrices: Representing adjective-noun constructions in semantic space. In Proceedings of the 2010 Conference on Empirical Methods in Natural Language Processing, pages 1183–1193.</rawString>
</citation>
<citation valid="true">
<authors>
<author>Marco Baroni</author>
<author>Silvia Bernardini</author>
<author>Adriano Ferraresi</author>
<author>Eros Zanchetta</author>
</authors>
<title>The WaCky Wide Web: A Collection of Very Large Linguistically Processed Web-Crawled Corpora. Language Resources and Evaluation,</title>
<date>2009</date>
<pages>43--3</pages>
<contexts>
<context position="28337" citStr="Baroni et al., 2009" startWordPosition="4468" endWordPosition="4471">) 0.27 0.32 0.24 0.28 PAS-CLBLM (Waddl) BNC 0.25 0.26 0.21 0.23 PAS-CLBLM (Waddnl) 0.42 0.50 0.34 0.41 PAS-LBLM 0.21 0.06 0.18 0.08 word2vec BNC 0.12 0.32 0.12 0.28 Grefenstette and Sadrzadeh (2011) BNC n/a n/a 0.21 n/a Tsubaki et al. (2013) ukWaC n/a 0.47 n/a n/a Van de Cruys et al. (2013) ukWaC n/a n/a 0.32 0.37 Human agreement 0.75 0.62 Table 4: Spearman’s rank correlation scores p for the SVO task. Averaged denotes the p calculated by averaged human ratings, and Non-averaged denotes the p calculated by non-averaged human ratings. only Kartsaklis and Sadrzadeh (2013) used the ukWaC corpus (Baroni et al., 2009) which is an order of magnitude larger than the BNC. As we can see in Table 3, the PAS-CLBLM (Addnl) achieves scores comparable to and higher than those of the baseline and the previous state-of-the-art results. In relation to these results, the Waddl and Waddnl variants of the PAS-CLBLM do not achieve great improvements in performance. This indicates that simple word vector addition can be sufficient to compose representations for phrases consisting of word pairs. 5.3 SVO Task Table 4 shows the correlation scores p for the SVO task. The scores p for this task are reported for both averaged an</context>
</contexts>
<marker>Baroni, Bernardini, Ferraresi, Zanchetta, 2009</marker>
<rawString>Marco Baroni, Silvia Bernardini, Adriano Ferraresi, and Eros Zanchetta. 2009. The WaCky Wide Web: A Collection of Very Large Linguistically Processed Web-Crawled Corpora. Language Resources and Evaluation, 43(3):209–226.</rawString>
</citation>
<citation valid="true">
<authors>
<author>Yoshua Bengio</author>
<author>R´ejean Ducharme</author>
<author>Pascal Vincent</author>
<author>Christian Jauvin</author>
</authors>
<title>A Neural Probabilistic Language Model.</title>
<date>2003</date>
<journal>Journal of Machine Learning Research,</journal>
<pages>3--1137</pages>
<contexts>
<context position="5926" citStr="Bengio et al., 2003" startWordPosition="886" endWordPosition="889">at the meaning of a word is determined by the contexts in which it appears (Firth, 1957). The context of a word is often defined as the words appearing in a window of fixed-length (bag-of-words) and a simple approach is to treat the co-occurrence statistics of a word w as a vector representation for w (Mitchell and Lapata, 2008; Mitchell and Lapata, 2010); alternatively, dependencies between words can be used to define contexts (Goyal et al., 2013; Erk and Pad´o, 2008; Thater et al., 2010). In contrast to distributional representations, NNLMs represent words in a low-dimensional vector space (Bengio et al., 2003; Collobert et al., 2011). Recently, Mikolov et al. (2013b) and Mnih and Kavukcuoglu (2013) proposed highly scalable models to learn high-dimensional word vectors. Levy and Goldberg (2014) extended the model of Mikolov et al. (2013b) by treating syntactic dependencies as contexts. Mitchell and Lapata (2008) investigated a variety of compositional operators to combine word vectors into phrasal representations. Among these operators, simple element-wise addition and multiplication are now widely used to represent short phrases (Mitchell and Lapata, 2010; Blacoe and Lapata, 2012). The obvious lim</context>
</contexts>
<marker>Bengio, Ducharme, Vincent, Jauvin, 2003</marker>
<rawString>Yoshua Bengio, R´ejean Ducharme, Pascal Vincent, and Christian Jauvin. 2003. A Neural Probabilistic Language Model. Journal of Machine Learning Research, 3:1137–1155.</rawString>
</citation>
<citation valid="true">
<authors>
<author>William Blacoe</author>
<author>Mirella Lapata</author>
</authors>
<title>A Comparison of Vector-based Representations for Semantic Composition.</title>
<date>2012</date>
<booktitle>In Proceedings of the 2012 Joint Conference on Empirical Methods in Natural Language Processing and Computational Natural Language Learning,</booktitle>
<pages>546--556</pages>
<contexts>
<context position="6509" citStr="Blacoe and Lapata, 2012" startWordPosition="971" endWordPosition="974">nsional vector space (Bengio et al., 2003; Collobert et al., 2011). Recently, Mikolov et al. (2013b) and Mnih and Kavukcuoglu (2013) proposed highly scalable models to learn high-dimensional word vectors. Levy and Goldberg (2014) extended the model of Mikolov et al. (2013b) by treating syntactic dependencies as contexts. Mitchell and Lapata (2008) investigated a variety of compositional operators to combine word vectors into phrasal representations. Among these operators, simple element-wise addition and multiplication are now widely used to represent short phrases (Mitchell and Lapata, 2010; Blacoe and Lapata, 2012). The obvious limitation with these simple approaches is that information about word order and syntactic relations is lost. To incorporate syntactic information into composition functions, a variety of compositional models have been proposed. These include recursive neural networks using phrase-structure trees (Socher et al., 2012; Socher et al., 2013b) and models in which words have a specific form of parameters according to their syntactic roles and composition functions are syntactically dependent on the relations of input words (Baroni and Zamparelli, 2010; Grefenstette and Sadrzadeh, 2011</context>
<context position="27440" citStr="Blacoe and Lapata (2012)" startWordPosition="4329" endWordPosition="4332">ks. Human agreement denotes the inter-annotator agreement. The word2vec baseline achieves unexpectedly high scores for these three tasks. Previously these kinds of models (Mikolov et al., 2013b; Mnih and Kavukcuoglu, 2013) have mainly been evaluated for word analogy tasks and, to date, there has been no work using these word vectors for the task of measuring the semantic similarity between phrases. However, this experimental result suggests that word2vec can serve as a strong baseline for these kinds of tasks, in addition to word analogy tasks. In Table 3, BL, HB, KS, and K denote the work of Blacoe and Lapata (2012), Hermann and Blunsom (2013), Kartsaklis and Sadrzadeh (2013), and Kartsaklis et al. (2013) respectively. Among these, 5https://code.google.com/p/word2vec/ 1550 Model Corpus Averaged Non-averaged SVO-SVO SVO-V SVO-SVO SVO-V PAS-CLBLM (Addl) 0.29 0.34 0.24 0.28 PAS-CLBLM (Addnl) 0.27 0.32 0.24 0.28 PAS-CLBLM (Waddl) BNC 0.25 0.26 0.21 0.23 PAS-CLBLM (Waddnl) 0.42 0.50 0.34 0.41 PAS-LBLM 0.21 0.06 0.18 0.08 word2vec BNC 0.12 0.32 0.12 0.28 Grefenstette and Sadrzadeh (2011) BNC n/a n/a 0.21 n/a Tsubaki et al. (2013) ukWaC n/a 0.47 n/a n/a Van de Cruys et al. (2013) ukWaC n/a n/a 0.32 0.37 Human a</context>
</contexts>
<marker>Blacoe, Lapata, 2012</marker>
<rawString>William Blacoe and Mirella Lapata. 2012. A Comparison of Vector-based Representations for Semantic Composition. In Proceedings of the 2012 Joint Conference on Empirical Methods in Natural Language Processing and Computational Natural Language Learning, pages 546–556.</rawString>
</citation>
<citation valid="true">
<authors>
<author>Ronan Collobert</author>
<author>Jason Weston</author>
<author>L´eon Bottou</author>
<author>Michael Karlen</author>
<author>Koray Kavukcuoglu</author>
<author>Pavel Kuksa</author>
</authors>
<title>Natural Language Processing (Almost) from Scratch.</title>
<date>2011</date>
<journal>Journal of Machine Learning Research,</journal>
<pages>12--2493</pages>
<contexts>
<context position="1677" citStr="Collobert et al. (2011)" startWordPosition="231" endWordPosition="234">us best results. Our system achieves these results without the need for pretrained word vectors and using a much smaller training corpus; despite this, for the subject-verb-object dataset our model improves upon the state of the art by as much as ∼10% in relative performance. 1 Introduction Studies on embedding single words in a vector space have made notable successes in capturing their syntactic and semantic properties (Turney and Pantel, 2010). These embeddings have also been found to be a useful component for Natural Language Processing (NLP) systems; for example, Turian et al. (2010) and Collobert et al. (2011) demonstrated how low-dimensional word vectors learned by Neural Network Language Models (NNLMs) are beneficial for a wide range of NLP tasks. Recently, the main focus of research on vector space representation is shifting from word representations to phrase representations (Baroni and Zamparelli, 2010; Grefenstette and Sadrzadeh, 2011; Mitchell and Lapata, 2010; Socher et al., 2012). Combining the ideas of NNLMs and semantic composition, Tsubaki et al. (2013) introduced a novel NNLM incorporating verb-object dependencies. More recently, Levy and Goldberg (2014) presented a NNLM that integrate</context>
<context position="5951" citStr="Collobert et al., 2011" startWordPosition="890" endWordPosition="893">ord is determined by the contexts in which it appears (Firth, 1957). The context of a word is often defined as the words appearing in a window of fixed-length (bag-of-words) and a simple approach is to treat the co-occurrence statistics of a word w as a vector representation for w (Mitchell and Lapata, 2008; Mitchell and Lapata, 2010); alternatively, dependencies between words can be used to define contexts (Goyal et al., 2013; Erk and Pad´o, 2008; Thater et al., 2010). In contrast to distributional representations, NNLMs represent words in a low-dimensional vector space (Bengio et al., 2003; Collobert et al., 2011). Recently, Mikolov et al. (2013b) and Mnih and Kavukcuoglu (2013) proposed highly scalable models to learn high-dimensional word vectors. Levy and Goldberg (2014) extended the model of Mikolov et al. (2013b) by treating syntactic dependencies as contexts. Mitchell and Lapata (2008) investigated a variety of compositional operators to combine word vectors into phrasal representations. Among these operators, simple element-wise addition and multiplication are now widely used to represent short phrases (Mitchell and Lapata, 2010; Blacoe and Lapata, 2012). The obvious limitation with these simple</context>
<context position="12649" citStr="Collobert et al., 2011" startWordPosition="1942" endWordPosition="1945">dent)). verb arg2 (3) Relationship to previous work. If we omit the the category-specific weight vectors hz in Eq. (1), our model is similar to the CBOW model in Mikolov et al. (2013a). CBOW predicts a target word given its surrounding bag-of-words context, while our model uses its PAS-based context. To incorporate the PAS information in our model more efficiently, we use category-specific weight vectors. Similarly, the vLBL model of Mnih and Kavukcuoglu (2013) uses different weight vectors depending on the position relative to the target word. As with previous neural network language models (Collobert et al., 2011; Huang et al., 2012), our model and vLBL can use weight matrices rather than weight vectors. However, as discussed by Mnih and Teh (2012), using weight vectors makes the training significantly faster than using weight matrices. Despite the simple formulation of the element-wise operations, the categoryspecific weight vectors efficiently propagate PASbased context information as explained next. 3.2.2 Training Word Vectors To train the PAS-LBLM, we use a scoring function to evaluate how well the target word wt fits the given context: s(wt,p(wt)) = v(wt)Tp(wt), (4) where v(wt) E Rdx1 is the scor</context>
<context position="14803" citStr="Collobert et al., 2011" startWordPosition="2306" endWordPosition="2309"> model parameters can be learned by maximizing the log probability of the target word wt based on the softmax function: exp(s(wt,p(wt))) p(wt|context) = (6) V| ,i=1 exp(s(wi,p(wt))) This is equivalent to a softmax regression model. However, when the vocabulary V is large, computing the softmax function in Eq. (6) is computationally expensive. If we do not need probability distributions over words, we are not necessarily restricted to using the probabilistic expressions. Recently, several methods have been proposed to efficiently learn word representations rather than accurate language models (Collobert et al., 2011; Mikolov et al., 2013b; Mnih and Kavukcuoglu, 2013), and our objective follows the work of Collobert et al. (2011). Mikolov et al. (2013b) and Mnih and Kavukcuoglu (2013) trained their models using word-dependent scoring weight vectors which are the arguments of our scoring function in Eq. (4). During development we also trained our model using the negative sampling technique of Mikolov et al. (2013b); however, we did not observe any significant performance difference. Intuition behind the PAS-LBLM. Here we briefly explain how each class of the model parameters of the PAS-LBLM contributes to </context>
</contexts>
<marker>Collobert, Weston, Bottou, Karlen, Kavukcuoglu, Kuksa, 2011</marker>
<rawString>Ronan Collobert, Jason Weston, L´eon Bottou, Michael Karlen, Koray Kavukcuoglu, and Pavel Kuksa. 2011. Natural Language Processing (Almost) from Scratch. Journal of Machine Learning Research, 12:2493–2537.</rawString>
</citation>
<citation valid="true">
<authors>
<author>John Duchi</author>
<author>Elad Hazan</author>
<author>Yoram Singer</author>
</authors>
<title>Adaptive Subgradient Methods for Online Learning and Stochastic Optimization.</title>
<date>2011</date>
<journal>Journal of Machine Learning Research,</journal>
<pages>12--2121</pages>
<contexts>
<context position="14086" citStr="Duchi et al., 2011" startWordPosition="2197" endWordPosition="2200">ht vectors h,. Based on the objective in the model of Collobert et al. (2011), the model parameters are learned by minimizing the following hinge loss: N max(1 − s(wt,p(wt)) + s(wn,p(wt)), 0), n=1 (5) where the negative sample wn is a randomly sampled word other than wt, and N is the number of negative samples. In our experiments we set N = 1. Following Mikolov et al. (2013b), negative samples were drawn from the distribution over unigrams that we raise to the power 0.75 and then normalize to once again attain a probability distribution. We minimize the loss function in Eq. (5) using AdaGrad (Duchi et al., 2011). For further training details, see Section 4.5. Relationship to softmax regression models. The model parameters can be learned by maximizing the log probability of the target word wt based on the softmax function: exp(s(wt,p(wt))) p(wt|context) = (6) V| ,i=1 exp(s(wi,p(wt))) This is equivalent to a softmax regression model. However, when the vocabulary V is large, computing the softmax function in Eq. (6) is computationally expensive. If we do not need probability distributions over words, we are not necessarily restricted to using the probabilistic expressions. Recently, several methods have</context>
<context position="23249" citStr="Duchi et al., 2011" startWordPosition="3659" endWordPosition="3662">s to weakly interact with each other. 4.5 Initialization and Optimization of Model Parameters We assigned a 50-dimensional vector for each word-POS pair described in Section 4.2 and initialized the vectors and the scoring weight vectors using small random values. In part inspired by the initialization method of the weight matrices in Socher et al. (2013a), we initialized all values in the compositional weight vectors of the Waddl and Waddnl as 1.0. The context weight vectors were initialized using small random values. We minimized the loss function in Eq. (5) using mini-batch SGD and AdaGrad (Duchi et al., 2011). Using AdaGrad, the SGD’s learning rate is adapted independently for each model parameter. This is helpful in training the PAS-LBLM and PAS-CLBLM, as they have conditionally dependent model parameters with varying frequencies. 1549 The mini-batch size was 32 and the learning rate was 0.05 for each experiment, and no regularization was used. To verify the semantics captured by the proposed models during training and to tune the hyperparameters, we used the WordSim-3532 word similarity data set (Finkelstein et al., 2001). 5 Evaluation on Phrase Similarity Tasks 5.1 Evaluation Settings The learn</context>
</contexts>
<marker>Duchi, Hazan, Singer, 2011</marker>
<rawString>John Duchi, Elad Hazan, and Yoram Singer. 2011. Adaptive Subgradient Methods for Online Learning and Stochastic Optimization. Journal of Machine Learning Research, 12:2121–2159.</rawString>
</citation>
<citation valid="true">
<authors>
<author>Katrin Erk</author>
<author>Sebastian Pad´o</author>
</authors>
<title>A Structured Vector Space Model for Word Meaning in Context.</title>
<date>2008</date>
<booktitle>In Proceedings of the 2008 Conference on Empirical Methods in Natural Language Processing,</booktitle>
<pages>897--906</pages>
<marker>Erk, Pad´o, 2008</marker>
<rawString>Katrin Erk and Sebastian Pad´o. 2008. A Structured Vector Space Model for Word Meaning in Context. In Proceedings of the 2008 Conference on Empirical Methods in Natural Language Processing, pages 897–906.</rawString>
</citation>
<citation valid="true">
<authors>
<author>Lev Finkelstein</author>
<author>Gabrilovich Evgenly</author>
<author>Matias Yossi</author>
<author>Rivlin Ehud</author>
<author>Solan Zach</author>
<author>Wolfman Gadi</author>
<author>Ruppin Eytan</author>
</authors>
<title>Placing Search in Context: The Concept Revisited.</title>
<date>2001</date>
<booktitle>In Proceedings of the Tenth International World Wide Web Conference.</booktitle>
<contexts>
<context position="23774" citStr="Finkelstein et al., 2001" startWordPosition="3742" endWordPosition="3745">ues. We minimized the loss function in Eq. (5) using mini-batch SGD and AdaGrad (Duchi et al., 2011). Using AdaGrad, the SGD’s learning rate is adapted independently for each model parameter. This is helpful in training the PAS-LBLM and PAS-CLBLM, as they have conditionally dependent model parameters with varying frequencies. 1549 The mini-batch size was 32 and the learning rate was 0.05 for each experiment, and no regularization was used. To verify the semantics captured by the proposed models during training and to tune the hyperparameters, we used the WordSim-3532 word similarity data set (Finkelstein et al., 2001). 5 Evaluation on Phrase Similarity Tasks 5.1 Evaluation Settings The learned models were evaluated on four tasks of measuring the semantic similarity between short phrases. We performed evaluation using the three tasks (AN, NN, and VO) in the dataset3 provided by Mitchell and Lapata (2010), and the SVO task in the dataset4 provided by Grefenstette and Sadrzadeh (2011). The datasets include pairs of short phrases extracted from the BNC. AN, NN, and VO contain 108 phrase pairs of adjective-noun, nounnoun, and verb-object. SVO contains 200 pairs of subject-verb-object phrases. Each phrase pair h</context>
</contexts>
<marker>Finkelstein, Evgenly, Yossi, Ehud, Zach, Gadi, Eytan, 2001</marker>
<rawString>Lev Finkelstein, Gabrilovich Evgenly, Matias Yossi, Rivlin Ehud, Solan Zach, Wolfman Gadi, and Ruppin Eytan. 2001. Placing Search in Context: The Concept Revisited. In Proceedings of the Tenth International World Wide Web Conference.</rawString>
</citation>
<citation valid="true">
<authors>
<author>John Rupert Firth</author>
</authors>
<title>A synopsis of linguistic theory 1930-55.</title>
<date>1957</date>
<booktitle>In Studies in Linguistic Analysis,</booktitle>
<pages>1--32</pages>
<contexts>
<context position="5395" citStr="Firth, 1957" startWordPosition="800" endWordPosition="801">, our learned composed vectors achieve state-of-the-art performance (p = 0.50) with a training corpus that is an order of magnitude smaller than that used by previous work (Tsubaki et al., 2013; Van de Cruys et al., 2013). Moreover, the proposed model does not require any pre-trained word vectors produced by external models, but rather induces word vectors jointly while training. 2 Related Work There is a large body of work on how to represent the meaning of a word in a vector space. Distributional approaches assume that the meaning of a word is determined by the contexts in which it appears (Firth, 1957). The context of a word is often defined as the words appearing in a window of fixed-length (bag-of-words) and a simple approach is to treat the co-occurrence statistics of a word w as a vector representation for w (Mitchell and Lapata, 2008; Mitchell and Lapata, 2010); alternatively, dependencies between words can be used to define contexts (Goyal et al., 2013; Erk and Pad´o, 2008; Thater et al., 2010). In contrast to distributional representations, NNLMs represent words in a low-dimensional vector space (Bengio et al., 2003; Collobert et al., 2011). Recently, Mikolov et al. (2013b) and Mnih </context>
</contexts>
<marker>Firth, 1957</marker>
<rawString>John Rupert Firth. 1957. A synopsis of linguistic theory 1930-55. In Studies in Linguistic Analysis, pages 1–32.</rawString>
</citation>
<citation valid="true">
<authors>
<author>Kartik Goyal</author>
<author>Sujay Kumar Jauhar</author>
<author>Huiying Li</author>
<author>Mrinmaya Sachan</author>
<author>Shashank Srivastava</author>
<author>Eduard Hovy</author>
</authors>
<title>A Structured Distributional Semantic Model : Integrating Structure with Semantics.</title>
<date>2013</date>
<booktitle>In Proceedings of the Workshop on Continuous Vector Space Models and their Compositionality,</booktitle>
<pages>20--29</pages>
<contexts>
<context position="5758" citStr="Goyal et al., 2013" startWordPosition="861" endWordPosition="864">s jointly while training. 2 Related Work There is a large body of work on how to represent the meaning of a word in a vector space. Distributional approaches assume that the meaning of a word is determined by the contexts in which it appears (Firth, 1957). The context of a word is often defined as the words appearing in a window of fixed-length (bag-of-words) and a simple approach is to treat the co-occurrence statistics of a word w as a vector representation for w (Mitchell and Lapata, 2008; Mitchell and Lapata, 2010); alternatively, dependencies between words can be used to define contexts (Goyal et al., 2013; Erk and Pad´o, 2008; Thater et al., 2010). In contrast to distributional representations, NNLMs represent words in a low-dimensional vector space (Bengio et al., 2003; Collobert et al., 2011). Recently, Mikolov et al. (2013b) and Mnih and Kavukcuoglu (2013) proposed highly scalable models to learn high-dimensional word vectors. Levy and Goldberg (2014) extended the model of Mikolov et al. (2013b) by treating syntactic dependencies as contexts. Mitchell and Lapata (2008) investigated a variety of compositional operators to combine word vectors into phrasal representations. Among these operato</context>
</contexts>
<marker>Goyal, Jauhar, Li, Sachan, Srivastava, Hovy, 2013</marker>
<rawString>Kartik Goyal, Sujay Kumar Jauhar, Huiying Li, Mrinmaya Sachan, Shashank Srivastava, and Eduard Hovy. 2013. A Structured Distributional Semantic Model : Integrating Structure with Semantics. In Proceedings of the Workshop on Continuous Vector Space Models and their Compositionality, pages 20– 29.</rawString>
</citation>
<citation valid="true">
<authors>
<author>Edward Grefenstette</author>
<author>Mehrnoosh Sadrzadeh</author>
</authors>
<title>Experimental Support for a Categorical Compositional Distributional Model of Meaning.</title>
<date>2011</date>
<booktitle>In Proceedings of the 2011 Conference on Empirical Methods in Natural Language Processing,</booktitle>
<pages>1394--1404</pages>
<contexts>
<context position="2014" citStr="Grefenstette and Sadrzadeh, 2011" startWordPosition="279" endWordPosition="282"> a vector space have made notable successes in capturing their syntactic and semantic properties (Turney and Pantel, 2010). These embeddings have also been found to be a useful component for Natural Language Processing (NLP) systems; for example, Turian et al. (2010) and Collobert et al. (2011) demonstrated how low-dimensional word vectors learned by Neural Network Language Models (NNLMs) are beneficial for a wide range of NLP tasks. Recently, the main focus of research on vector space representation is shifting from word representations to phrase representations (Baroni and Zamparelli, 2010; Grefenstette and Sadrzadeh, 2011; Mitchell and Lapata, 2010; Socher et al., 2012). Combining the ideas of NNLMs and semantic composition, Tsubaki et al. (2013) introduced a novel NNLM incorporating verb-object dependencies. More recently, Levy and Goldberg (2014) presented a NNLM that integrated syntactic dependencies. However, to the best of our knowledge, there is no previous work on integrating a variety of syntactic and semantic dependencies into NNLMs in order to learn composition functions as well as word representations. The following question thus arises naturally: Can a variety of dependencies be used to jointly lea</context>
<context position="7109" citStr="Grefenstette and Sadrzadeh, 2011" startWordPosition="1059" endWordPosition="1062">, 2010; Blacoe and Lapata, 2012). The obvious limitation with these simple approaches is that information about word order and syntactic relations is lost. To incorporate syntactic information into composition functions, a variety of compositional models have been proposed. These include recursive neural networks using phrase-structure trees (Socher et al., 2012; Socher et al., 2013b) and models in which words have a specific form of parameters according to their syntactic roles and composition functions are syntactically dependent on the relations of input words (Baroni and Zamparelli, 2010; Grefenstette and Sadrzadeh, 2011; Hashimoto et al., 2013; Hermann and Blunsom, 2013; Socher et al., 2013a). More recently, syntactic dependency-based 1545 compositional models have been proposed (Paperno et al., 2014; Socher et al., 2014; Tsubaki et al., 2013). One of the advantages of these models is that they are less restricted by word order. Among these, Tsubaki et al. (2013) introduced a novel compositional NNLM mainly focusing on verb-object dependencies and achieved state-of-the-art performance for the task of measuring the semantic similarity between subjectverb-object phrases. 3 PAS-CLBLM: A Compositional Log-Biline</context>
<context position="24145" citStr="Grefenstette and Sadrzadeh (2011)" startWordPosition="3801" endWordPosition="3804"> the learning rate was 0.05 for each experiment, and no regularization was used. To verify the semantics captured by the proposed models during training and to tune the hyperparameters, we used the WordSim-3532 word similarity data set (Finkelstein et al., 2001). 5 Evaluation on Phrase Similarity Tasks 5.1 Evaluation Settings The learned models were evaluated on four tasks of measuring the semantic similarity between short phrases. We performed evaluation using the three tasks (AN, NN, and VO) in the dataset3 provided by Mitchell and Lapata (2010), and the SVO task in the dataset4 provided by Grefenstette and Sadrzadeh (2011). The datasets include pairs of short phrases extracted from the BNC. AN, NN, and VO contain 108 phrase pairs of adjective-noun, nounnoun, and verb-object. SVO contains 200 pairs of subject-verb-object phrases. Each phrase pair has multiple human-ratings: the higher the rating is, the more semantically similar the phrases. For example, the subject-verb-object phrase pair of “student write name” and “student spell name” has a high rating. The pair “people try door” and “people judge door” has a low rating. For evaluation we used the Spearman’s rank correlation p between the human-ratings and th</context>
<context position="27915" citStr="Grefenstette and Sadrzadeh (2011)" startWordPosition="4396" endWordPosition="4399">ve as a strong baseline for these kinds of tasks, in addition to word analogy tasks. In Table 3, BL, HB, KS, and K denote the work of Blacoe and Lapata (2012), Hermann and Blunsom (2013), Kartsaklis and Sadrzadeh (2013), and Kartsaklis et al. (2013) respectively. Among these, 5https://code.google.com/p/word2vec/ 1550 Model Corpus Averaged Non-averaged SVO-SVO SVO-V SVO-SVO SVO-V PAS-CLBLM (Addl) 0.29 0.34 0.24 0.28 PAS-CLBLM (Addnl) 0.27 0.32 0.24 0.28 PAS-CLBLM (Waddl) BNC 0.25 0.26 0.21 0.23 PAS-CLBLM (Waddnl) 0.42 0.50 0.34 0.41 PAS-LBLM 0.21 0.06 0.18 0.08 word2vec BNC 0.12 0.32 0.12 0.28 Grefenstette and Sadrzadeh (2011) BNC n/a n/a 0.21 n/a Tsubaki et al. (2013) ukWaC n/a 0.47 n/a n/a Van de Cruys et al. (2013) ukWaC n/a n/a 0.32 0.37 Human agreement 0.75 0.62 Table 4: Spearman’s rank correlation scores p for the SVO task. Averaged denotes the p calculated by averaged human ratings, and Non-averaged denotes the p calculated by non-averaged human ratings. only Kartsaklis and Sadrzadeh (2013) used the ukWaC corpus (Baroni et al., 2009) which is an order of magnitude larger than the BNC. As we can see in Table 3, the PAS-CLBLM (Addnl) achieves scores comparable to and higher than those of the baseline and the p</context>
<context position="29292" citStr="Grefenstette and Sadrzadeh, 2011" startWordPosition="4625" endWordPosition="4628">in performance. This indicates that simple word vector addition can be sufficient to compose representations for phrases consisting of word pairs. 5.3 SVO Task Table 4 shows the correlation scores p for the SVO task. The scores p for this task are reported for both averaged and non-averaged human ratings. This is due to a disagreement in previous work regarding which metric to use when reporting results. Hence, we report the scores for both settings in Table 4. Another point we should consider is that some previous work reported scores based on the similarity between composed representations (Grefenstette and Sadrzadeh, 2011; Van de Cruys et al., 2013), and others reported scores based on the similarity between composed representations and word representations of landmark verbs from the dataset (Tsubaki et al., 2013; Van de Cruys et al., 2013). For completeness, we report the scores for both settings: SVO-SVO and SVO-V in Table 4. The results show that the weighted addition model with the non-linear function tanh (PASCLBLM (Waddnl)) is effective for the more complex phrase task. While simple vector addition is sufficient for phrases consisting of word pairs, it is clear from our experimental results that they fal</context>
<context position="30573" citStr="Grefenstette and Sadrzadeh (2011)" startWordPosition="4838" endWordPosition="4841">e involved in the SVO task. Our PAS-CLBLM (Waddnl) model outperforms the previous state-of-the-art scores for the SVO task as reported by Tsubaki et al. (2013) and Van de Cruys et al. (2013). As such, there are three key points that we would like to emphasize: (1) the difference of the training corpus size, (2) the necessity of the pre-trained word vectors, (3) the modularity of deep learning models. Tsubaki et al. (2013) and Van de Cruys et al. (2013) used the ukWaC corpus. This means our model works better, despite using a considerably smaller corpora. It should also be noted that, like us, Grefenstette and Sadrzadeh (2011) used the BNC corpus. The model of Tsubaki et al. (2013) is based on neural network language models which use syntactic dependencies between verbs and their objects. While their novel model, which incorporates the idea of co-compositionality, works well with pre-trained word vectors produced by external models, it is not clear whether the pre-trained vectors are required to achieve high scores. In contrast, we have achieved state-of-the-art results without the use of pre-trained word vectors. Despite our model’s scalability, we trained 50- dimensional vector representations for words and their</context>
</contexts>
<marker>Grefenstette, Sadrzadeh, 2011</marker>
<rawString>Edward Grefenstette and Mehrnoosh Sadrzadeh. 2011. Experimental Support for a Categorical Compositional Distributional Model of Meaning. In Proceedings of the 2011 Conference on Empirical Methods in Natural Language Processing, pages 1394– 1404.</rawString>
</citation>
<citation valid="true">
<authors>
<author>Kazuma Hashimoto</author>
<author>Makoto Miwa</author>
<author>Yoshimasa Tsuruoka</author>
<author>Takashi Chikayama</author>
</authors>
<title>Simple Customization of Recursive Neural Networks for Semantic Relation Classification.</title>
<date>2013</date>
<booktitle>In Proceedings of the 2013 Conference on Empirical Methods in Natural Language Processing,</booktitle>
<pages>1372--1376</pages>
<contexts>
<context position="7133" citStr="Hashimoto et al., 2013" startWordPosition="1063" endWordPosition="1066">The obvious limitation with these simple approaches is that information about word order and syntactic relations is lost. To incorporate syntactic information into composition functions, a variety of compositional models have been proposed. These include recursive neural networks using phrase-structure trees (Socher et al., 2012; Socher et al., 2013b) and models in which words have a specific form of parameters according to their syntactic roles and composition functions are syntactically dependent on the relations of input words (Baroni and Zamparelli, 2010; Grefenstette and Sadrzadeh, 2011; Hashimoto et al., 2013; Hermann and Blunsom, 2013; Socher et al., 2013a). More recently, syntactic dependency-based 1545 compositional models have been proposed (Paperno et al., 2014; Socher et al., 2014; Tsubaki et al., 2013). One of the advantages of these models is that they are less restricted by word order. Among these, Tsubaki et al. (2013) introduced a novel compositional NNLM mainly focusing on verb-object dependencies and achieved state-of-the-art performance for the task of measuring the semantic similarity between subjectverb-object phrases. 3 PAS-CLBLM: A Compositional Log-Bilinear Language Model Using </context>
<context position="19327" citStr="Hashimoto et al., 2013" startWordPosition="3037" endWordPosition="3040">parsed them using the Enju parser described in Section 3.1. 4.2 Word Sense Disambiguation Using Part-of-Speech Tags In general, words can have multiple syntactic usages. For example, the word cause can be a noun or a verb depending on its context. Most of the previous work on learning word vectors ignores this ambiguity since word sense disambiguation could potentially be performed after the word vectors have been trained (Huang et al., 2012; Kartsaklis and Sadrzadeh, 2013). Some recent work explicitly assigns an independent vector for each word usage according to its part-ofspeech (POS) tag (Hashimoto et al., 2013; Kartsaklis and Sadrzadeh, 2013). Alternatively, Baroni and Zamparelli (2010) assigned different forms of parameters to adjectives and nouns. In our experiments, we combined each word with its corresponding POS tags. We used the base-forms provided by the Enju parser rather than 1548 Figure 2: Two PAS-CLBLM training samples. the surface-forms, and used the first two characters of the POS tags. For example, VB, VBP, VBZ, VBG, VBD, VBI were all mapped to VB. This resulted in two kinds of cause: cause II and cause VB and we used the 100,000 most frequent lowercased word-POS pairs in the BNC. 4.3</context>
</contexts>
<marker>Hashimoto, Miwa, Tsuruoka, Chikayama, 2013</marker>
<rawString>Kazuma Hashimoto, Makoto Miwa, Yoshimasa Tsuruoka, and Takashi Chikayama. 2013. Simple Customization of Recursive Neural Networks for Semantic Relation Classification. In Proceedings of the 2013 Conference on Empirical Methods in Natural Language Processing, pages 1372–1376.</rawString>
</citation>
<citation valid="true">
<authors>
<author>Karl Moritz Hermann</author>
<author>Phil Blunsom</author>
</authors>
<title>The Role of Syntax in Vector Space Models of Compositional Semantics.</title>
<date>2013</date>
<booktitle>In Proceedings of the 51st Annual Meeting of the Association for Computational Linguistics (Volume 1: Long Papers),</booktitle>
<pages>894--904</pages>
<contexts>
<context position="7160" citStr="Hermann and Blunsom, 2013" startWordPosition="1067" endWordPosition="1070">ith these simple approaches is that information about word order and syntactic relations is lost. To incorporate syntactic information into composition functions, a variety of compositional models have been proposed. These include recursive neural networks using phrase-structure trees (Socher et al., 2012; Socher et al., 2013b) and models in which words have a specific form of parameters according to their syntactic roles and composition functions are syntactically dependent on the relations of input words (Baroni and Zamparelli, 2010; Grefenstette and Sadrzadeh, 2011; Hashimoto et al., 2013; Hermann and Blunsom, 2013; Socher et al., 2013a). More recently, syntactic dependency-based 1545 compositional models have been proposed (Paperno et al., 2014; Socher et al., 2014; Tsubaki et al., 2013). One of the advantages of these models is that they are less restricted by word order. Among these, Tsubaki et al. (2013) introduced a novel compositional NNLM mainly focusing on verb-object dependencies and achieved state-of-the-art performance for the task of measuring the semantic similarity between subjectverb-object phrases. 3 PAS-CLBLM: A Compositional Log-Bilinear Language Model Using Predicate-Argument Structur</context>
<context position="27468" citStr="Hermann and Blunsom (2013)" startWordPosition="4333" endWordPosition="4337">s the inter-annotator agreement. The word2vec baseline achieves unexpectedly high scores for these three tasks. Previously these kinds of models (Mikolov et al., 2013b; Mnih and Kavukcuoglu, 2013) have mainly been evaluated for word analogy tasks and, to date, there has been no work using these word vectors for the task of measuring the semantic similarity between phrases. However, this experimental result suggests that word2vec can serve as a strong baseline for these kinds of tasks, in addition to word analogy tasks. In Table 3, BL, HB, KS, and K denote the work of Blacoe and Lapata (2012), Hermann and Blunsom (2013), Kartsaklis and Sadrzadeh (2013), and Kartsaklis et al. (2013) respectively. Among these, 5https://code.google.com/p/word2vec/ 1550 Model Corpus Averaged Non-averaged SVO-SVO SVO-V SVO-SVO SVO-V PAS-CLBLM (Addl) 0.29 0.34 0.24 0.28 PAS-CLBLM (Addnl) 0.27 0.32 0.24 0.28 PAS-CLBLM (Waddl) BNC 0.25 0.26 0.21 0.23 PAS-CLBLM (Waddnl) 0.42 0.50 0.34 0.41 PAS-LBLM 0.21 0.06 0.18 0.08 word2vec BNC 0.12 0.32 0.12 0.28 Grefenstette and Sadrzadeh (2011) BNC n/a n/a 0.21 n/a Tsubaki et al. (2013) ukWaC n/a 0.47 n/a n/a Van de Cruys et al. (2013) ukWaC n/a n/a 0.32 0.37 Human agreement 0.75 0.62 Table 4: </context>
</contexts>
<marker>Hermann, Blunsom, 2013</marker>
<rawString>Karl Moritz Hermann and Phil Blunsom. 2013. The Role of Syntax in Vector Space Models of Compositional Semantics. In Proceedings of the 51st Annual Meeting of the Association for Computational Linguistics (Volume 1: Long Papers), pages 894–904.</rawString>
</citation>
<citation valid="true">
<authors>
<author>Eric Huang</author>
<author>Richard Socher</author>
<author>Christopher Manning</author>
<author>Andrew Ng</author>
</authors>
<title>Improving Word Representations via Global Context and Multiple Word Prototypes.</title>
<date>2012</date>
<booktitle>In Proceedings of the 50th Annual Meeting of the Association for Computational Linguistics (Volume 1: Long Papers),</booktitle>
<pages>873--882</pages>
<contexts>
<context position="12670" citStr="Huang et al., 2012" startWordPosition="1946" endWordPosition="1949">lationship to previous work. If we omit the the category-specific weight vectors hz in Eq. (1), our model is similar to the CBOW model in Mikolov et al. (2013a). CBOW predicts a target word given its surrounding bag-of-words context, while our model uses its PAS-based context. To incorporate the PAS information in our model more efficiently, we use category-specific weight vectors. Similarly, the vLBL model of Mnih and Kavukcuoglu (2013) uses different weight vectors depending on the position relative to the target word. As with previous neural network language models (Collobert et al., 2011; Huang et al., 2012), our model and vLBL can use weight matrices rather than weight vectors. However, as discussed by Mnih and Teh (2012), using weight vectors makes the training significantly faster than using weight matrices. Despite the simple formulation of the element-wise operations, the categoryspecific weight vectors efficiently propagate PASbased context information as explained next. 3.2.2 Training Word Vectors To train the PAS-LBLM, we use a scoring function to evaluate how well the target word wt fits the given context: s(wt,p(wt)) = v(wt)Tp(wt), (4) where v(wt) E Rdx1 is the scoring weight vector for</context>
<context position="17990" citStr="Huang et al. (2012)" startWordPosition="2809" endWordPosition="2812">ent) is replaced with gc′′(v(car), v(accident)). c″is the predicate category noun arg1. These multiple relationships of predicate-argument structures should provide richer context information. We refer to the PAS-LBLM with composition functions as PASCLBLM. 3.4 Bag-of-Words Sensitive PAS-CLBLM Both the PAS-LBLM and PAS-CLBLM can take meaningful relationships between words into account. However, at times, the number of context words can be limited and the ability of other models to take ten or more words from a fixed context in a bag-of-words (BoW) fashion could compensate for this sparseness. Huang et al. (2012) combined local and global contexts in their neural network language models, and motivated by their work, we integrate bag-of-words vectors into our models. Concretely, we add an additional input term to Eq. (1): M p(wt) = f(i=1 � hci O v(wi) + hBoW O v(BoW) , (8) where hc�oW E Rdx1 are additional weight vectors, and v(BoW) E Rdx1 is the average of the word vectors in the same sentence. To construct the v(BoW) for each sentence, we average the word vectors of nouns and verbs in the same sentence, excluding the target and context words. 4 Experimental Settings 4.1 Training Corpus We used the Br</context>
</contexts>
<marker>Huang, Socher, Manning, Ng, 2012</marker>
<rawString>Eric Huang, Richard Socher, Christopher Manning, and Andrew Ng. 2012. Improving Word Representations via Global Context and Multiple Word Prototypes. In Proceedings of the 50th Annual Meeting of the Association for Computational Linguistics (Volume 1: Long Papers), pages 873–882.</rawString>
</citation>
<citation valid="true">
<authors>
<author>Dimitri Kartsaklis</author>
<author>Mehrnoosh Sadrzadeh</author>
</authors>
<title>Prior Disambiguation of Word Tensors for Constructing Sentence Vectors.</title>
<date>2013</date>
<booktitle>In Proceedings of the 2013 Conference on Empirical Methods in Natural Language Processing,</booktitle>
<pages>1590--1601</pages>
<contexts>
<context position="19183" citStr="Kartsaklis and Sadrzadeh, 2013" startWordPosition="3012" endWordPosition="3015">ngs 4.1 Training Corpus We used the British National Corpus (BNC) as our training corpus, extracted 6 million sentences from the original BNC files, and parsed them using the Enju parser described in Section 3.1. 4.2 Word Sense Disambiguation Using Part-of-Speech Tags In general, words can have multiple syntactic usages. For example, the word cause can be a noun or a verb depending on its context. Most of the previous work on learning word vectors ignores this ambiguity since word sense disambiguation could potentially be performed after the word vectors have been trained (Huang et al., 2012; Kartsaklis and Sadrzadeh, 2013). Some recent work explicitly assigns an independent vector for each word usage according to its part-ofspeech (POS) tag (Hashimoto et al., 2013; Kartsaklis and Sadrzadeh, 2013). Alternatively, Baroni and Zamparelli (2010) assigned different forms of parameters to adjectives and nouns. In our experiments, we combined each word with its corresponding POS tags. We used the base-forms provided by the Enju parser rather than 1548 Figure 2: Two PAS-CLBLM training samples. the surface-forms, and used the first two characters of the POS tags. For example, VB, VBP, VBZ, VBG, VBD, VBI were all mapped t</context>
<context position="27501" citStr="Kartsaklis and Sadrzadeh (2013)" startWordPosition="4338" endWordPosition="4341">ent. The word2vec baseline achieves unexpectedly high scores for these three tasks. Previously these kinds of models (Mikolov et al., 2013b; Mnih and Kavukcuoglu, 2013) have mainly been evaluated for word analogy tasks and, to date, there has been no work using these word vectors for the task of measuring the semantic similarity between phrases. However, this experimental result suggests that word2vec can serve as a strong baseline for these kinds of tasks, in addition to word analogy tasks. In Table 3, BL, HB, KS, and K denote the work of Blacoe and Lapata (2012), Hermann and Blunsom (2013), Kartsaklis and Sadrzadeh (2013), and Kartsaklis et al. (2013) respectively. Among these, 5https://code.google.com/p/word2vec/ 1550 Model Corpus Averaged Non-averaged SVO-SVO SVO-V SVO-SVO SVO-V PAS-CLBLM (Addl) 0.29 0.34 0.24 0.28 PAS-CLBLM (Addnl) 0.27 0.32 0.24 0.28 PAS-CLBLM (Waddl) BNC 0.25 0.26 0.21 0.23 PAS-CLBLM (Waddnl) 0.42 0.50 0.34 0.41 PAS-LBLM 0.21 0.06 0.18 0.08 word2vec BNC 0.12 0.32 0.12 0.28 Grefenstette and Sadrzadeh (2011) BNC n/a n/a 0.21 n/a Tsubaki et al. (2013) ukWaC n/a 0.47 n/a n/a Van de Cruys et al. (2013) ukWaC n/a n/a 0.32 0.37 Human agreement 0.75 0.62 Table 4: Spearman’s rank correlation score</context>
</contexts>
<marker>Kartsaklis, Sadrzadeh, 2013</marker>
<rawString>Dimitri Kartsaklis and Mehrnoosh Sadrzadeh. 2013. Prior Disambiguation of Word Tensors for Constructing Sentence Vectors. In Proceedings of the 2013 Conference on Empirical Methods in Natural Language Processing, pages 1590–1601.</rawString>
</citation>
<citation valid="true">
<authors>
<author>Dimitri Kartsaklis</author>
<author>Mehrnoosh Sadrzadeh</author>
<author>Stephen Pulman</author>
</authors>
<title>Separating Disambiguation from Composition in Distributional Semantics.</title>
<date>2013</date>
<booktitle>In Proceedings of 17th Conference on Natural Language Learning (CoNLL),</booktitle>
<pages>114--123</pages>
<contexts>
<context position="27531" citStr="Kartsaklis et al. (2013)" startWordPosition="4343" endWordPosition="4346">nexpectedly high scores for these three tasks. Previously these kinds of models (Mikolov et al., 2013b; Mnih and Kavukcuoglu, 2013) have mainly been evaluated for word analogy tasks and, to date, there has been no work using these word vectors for the task of measuring the semantic similarity between phrases. However, this experimental result suggests that word2vec can serve as a strong baseline for these kinds of tasks, in addition to word analogy tasks. In Table 3, BL, HB, KS, and K denote the work of Blacoe and Lapata (2012), Hermann and Blunsom (2013), Kartsaklis and Sadrzadeh (2013), and Kartsaklis et al. (2013) respectively. Among these, 5https://code.google.com/p/word2vec/ 1550 Model Corpus Averaged Non-averaged SVO-SVO SVO-V SVO-SVO SVO-V PAS-CLBLM (Addl) 0.29 0.34 0.24 0.28 PAS-CLBLM (Addnl) 0.27 0.32 0.24 0.28 PAS-CLBLM (Waddl) BNC 0.25 0.26 0.21 0.23 PAS-CLBLM (Waddnl) 0.42 0.50 0.34 0.41 PAS-LBLM 0.21 0.06 0.18 0.08 word2vec BNC 0.12 0.32 0.12 0.28 Grefenstette and Sadrzadeh (2011) BNC n/a n/a 0.21 n/a Tsubaki et al. (2013) ukWaC n/a 0.47 n/a n/a Van de Cruys et al. (2013) ukWaC n/a n/a 0.32 0.37 Human agreement 0.75 0.62 Table 4: Spearman’s rank correlation scores p for the SVO task. Averaged</context>
</contexts>
<marker>Kartsaklis, Sadrzadeh, Pulman, 2013</marker>
<rawString>Dimitri Kartsaklis, Mehrnoosh Sadrzadeh, and Stephen Pulman. 2013. Separating Disambiguation from Composition in Distributional Semantics. In Proceedings of 17th Conference on Natural Language Learning (CoNLL), pages 114–123.</rawString>
</citation>
<citation valid="true">
<authors>
<author>Omer Levy</author>
<author>Yoav Goldberg</author>
</authors>
<title>DependencyBased Word Embeddings.</title>
<date>2014</date>
<booktitle>In Proceedings of the 52nd Annual Meeting of the Association for Computational Linguistics (Volume 2: Short Papers),</booktitle>
<pages>302--308</pages>
<contexts>
<context position="2245" citStr="Levy and Goldberg (2014)" startWordPosition="314" endWordPosition="317">xample, Turian et al. (2010) and Collobert et al. (2011) demonstrated how low-dimensional word vectors learned by Neural Network Language Models (NNLMs) are beneficial for a wide range of NLP tasks. Recently, the main focus of research on vector space representation is shifting from word representations to phrase representations (Baroni and Zamparelli, 2010; Grefenstette and Sadrzadeh, 2011; Mitchell and Lapata, 2010; Socher et al., 2012). Combining the ideas of NNLMs and semantic composition, Tsubaki et al. (2013) introduced a novel NNLM incorporating verb-object dependencies. More recently, Levy and Goldberg (2014) presented a NNLM that integrated syntactic dependencies. However, to the best of our knowledge, there is no previous work on integrating a variety of syntactic and semantic dependencies into NNLMs in order to learn composition functions as well as word representations. The following question thus arises naturally: Can a variety of dependencies be used to jointly learn both stand-alone word vectors and their compositions, embedding them in the same vector space? In this work, we bridge the gap between purely context-based (Levy and Goldberg, 2014; Mikolov et al., 2013b; Mnih and Kavukcuoglu, 2</context>
<context position="6114" citStr="Levy and Goldberg (2014)" startWordPosition="913" endWordPosition="916">g-of-words) and a simple approach is to treat the co-occurrence statistics of a word w as a vector representation for w (Mitchell and Lapata, 2008; Mitchell and Lapata, 2010); alternatively, dependencies between words can be used to define contexts (Goyal et al., 2013; Erk and Pad´o, 2008; Thater et al., 2010). In contrast to distributional representations, NNLMs represent words in a low-dimensional vector space (Bengio et al., 2003; Collobert et al., 2011). Recently, Mikolov et al. (2013b) and Mnih and Kavukcuoglu (2013) proposed highly scalable models to learn high-dimensional word vectors. Levy and Goldberg (2014) extended the model of Mikolov et al. (2013b) by treating syntactic dependencies as contexts. Mitchell and Lapata (2008) investigated a variety of compositional operators to combine word vectors into phrasal representations. Among these operators, simple element-wise addition and multiplication are now widely used to represent short phrases (Mitchell and Lapata, 2010; Blacoe and Lapata, 2012). The obvious limitation with these simple approaches is that information about word order and syntactic relations is lost. To incorporate syntactic information into composition functions, a variety of com</context>
</contexts>
<marker>Levy, Goldberg, 2014</marker>
<rawString>Omer Levy and Yoav Goldberg. 2014. DependencyBased Word Embeddings. In Proceedings of the 52nd Annual Meeting of the Association for Computational Linguistics (Volume 2: Short Papers), pages 302–308.</rawString>
</citation>
<citation valid="true">
<authors>
<author>Tomas Mikolov</author>
<author>Kai Chen</author>
<author>Greg Corrado</author>
<author>Jeffrey Dean</author>
</authors>
<title>Efficient Estimation of Word Representations in Vector Space.</title>
<date>2013</date>
<booktitle>In Proceedings of Workshop at the International Conference on Learning Representations.</booktitle>
<contexts>
<context position="2819" citStr="Mikolov et al., 2013" startWordPosition="407" endWordPosition="410">ncies. More recently, Levy and Goldberg (2014) presented a NNLM that integrated syntactic dependencies. However, to the best of our knowledge, there is no previous work on integrating a variety of syntactic and semantic dependencies into NNLMs in order to learn composition functions as well as word representations. The following question thus arises naturally: Can a variety of dependencies be used to jointly learn both stand-alone word vectors and their compositions, embedding them in the same vector space? In this work, we bridge the gap between purely context-based (Levy and Goldberg, 2014; Mikolov et al., 2013b; Mnih and Kavukcuoglu, 2013) and compositional (Tsubaki et al., 2013) NNLMs by using the flexible set of categories from Predicate-Argument-Structures (PASs). More specifically, we propose a Compositional Log-Bilinear Language Model using PASs (PASCLBLM), an overview of which is shown in Figure 1. The model is trained by maximizing the accuracy of predicting target words from their bag-of-words and dependency-based context, which provides information about selectional preference. As shown in Figure 1 (b), one of the advantages of the PAS-CLBLM is that the model can treat not only word vector</context>
<context position="5983" citStr="Mikolov et al. (2013" startWordPosition="895" endWordPosition="898">n which it appears (Firth, 1957). The context of a word is often defined as the words appearing in a window of fixed-length (bag-of-words) and a simple approach is to treat the co-occurrence statistics of a word w as a vector representation for w (Mitchell and Lapata, 2008; Mitchell and Lapata, 2010); alternatively, dependencies between words can be used to define contexts (Goyal et al., 2013; Erk and Pad´o, 2008; Thater et al., 2010). In contrast to distributional representations, NNLMs represent words in a low-dimensional vector space (Bengio et al., 2003; Collobert et al., 2011). Recently, Mikolov et al. (2013b) and Mnih and Kavukcuoglu (2013) proposed highly scalable models to learn high-dimensional word vectors. Levy and Goldberg (2014) extended the model of Mikolov et al. (2013b) by treating syntactic dependencies as contexts. Mitchell and Lapata (2008) investigated a variety of compositional operators to combine word vectors into phrasal representations. Among these operators, simple element-wise addition and multiplication are now widely used to represent short phrases (Mitchell and Lapata, 2010; Blacoe and Lapata, 2012). The obvious limitation with these simple approaches is that information </context>
<context position="7899" citStr="Mikolov et al., 2013" startWordPosition="1179" endWordPosition="1182">o et al., 2014; Socher et al., 2014; Tsubaki et al., 2013). One of the advantages of these models is that they are less restricted by word order. Among these, Tsubaki et al. (2013) introduced a novel compositional NNLM mainly focusing on verb-object dependencies and achieved state-of-the-art performance for the task of measuring the semantic similarity between subjectverb-object phrases. 3 PAS-CLBLM: A Compositional Log-Bilinear Language Model Using Predicate-Argument Structures In some recent studies on representing words as vectors, word vectors are learned by solving word prediction tasks (Mikolov et al., 2013a; Mnih and Kavukcuoglu, 2013). More specifically, given target words and their context words, the basic idea is to train classifiers to discriminate between each target word and artificially induced negative target words. The feature vector of the classifiers are calculated using the context word vectors whose values are optimized during training. As a result, vectors of words in similar contexts become similar to each other. Following these studies, we propose a novel model to jointly learn representations for words and their compositions by training word prediction classifiers using PASs. I</context>
<context position="12209" citStr="Mikolov et al. (2013" startWordPosition="1872" endWordPosition="1875">e “cause” is extracted with its first and second arguments “rain” and “accident”, the PAS-LBLM computes p(cause) E Rd following Eq. (1): p(cause) = f(hverb arg12 O v(rain)+ arg1 hverb arg12 O v(accident)). arg2 In Eq. (2), the predicate is treated as the target word, and its arguments are treated as the context words. In the same way, an argument can be treated as a target word: p(rain) = f(hverb arg12 O v(cause)+ hverb arg12 O v(accident)). verb arg2 (3) Relationship to previous work. If we omit the the category-specific weight vectors hz in Eq. (1), our model is similar to the CBOW model in Mikolov et al. (2013a). CBOW predicts a target word given its surrounding bag-of-words context, while our model uses its PAS-based context. To incorporate the PAS information in our model more efficiently, we use category-specific weight vectors. Similarly, the vLBL model of Mnih and Kavukcuoglu (2013) uses different weight vectors depending on the position relative to the target word. As with previous neural network language models (Collobert et al., 2011; Huang et al., 2012), our model and vLBL can use weight matrices rather than weight vectors. However, as discussed by Mnih and Teh (2012), using weight vectors</context>
<context position="13843" citStr="Mikolov et al. (2013" startWordPosition="2155" endWordPosition="2158">e v(wt) E Rdx1 is the scoring weight vector for wt. Thus, the model parameters in the PASLBLM are (V, V, H). V is the set of word vectors v(w), and V is the set of scoring weight vectors v(w). H is the set of the predicate-categoryspecific weight vectors h,. Based on the objective in the model of Collobert et al. (2011), the model parameters are learned by minimizing the following hinge loss: N max(1 − s(wt,p(wt)) + s(wn,p(wt)), 0), n=1 (5) where the negative sample wn is a randomly sampled word other than wt, and N is the number of negative samples. In our experiments we set N = 1. Following Mikolov et al. (2013b), negative samples were drawn from the distribution over unigrams that we raise to the power 0.75 and then normalize to once again attain a probability distribution. We minimize the loss function in Eq. (5) using AdaGrad (Duchi et al., 2011). For further training details, see Section 4.5. Relationship to softmax regression models. The model parameters can be learned by maximizing the log probability of the target word wt based on the softmax function: exp(s(wt,p(wt))) p(wt|context) = (6) V| ,i=1 exp(s(wi,p(wt))) This is equivalent to a softmax regression model. However, when the vocabulary V</context>
<context position="15206" citStr="Mikolov et al. (2013" startWordPosition="2372" endWordPosition="2375">re not necessarily restricted to using the probabilistic expressions. Recently, several methods have been proposed to efficiently learn word representations rather than accurate language models (Collobert et al., 2011; Mikolov et al., 2013b; Mnih and Kavukcuoglu, 2013), and our objective follows the work of Collobert et al. (2011). Mikolov et al. (2013b) and Mnih and Kavukcuoglu (2013) trained their models using word-dependent scoring weight vectors which are the arguments of our scoring function in Eq. (4). During development we also trained our model using the negative sampling technique of Mikolov et al. (2013b); however, we did not observe any significant performance difference. Intuition behind the PAS-LBLM. Here we briefly explain how each class of the model parameters of the PAS-LBLM contributes to learning word representations at each stochastic gradient (2) 1547 decent step. The category-specific weight vectors provide the PAS information for context word vectors which we would like to learn. During training, context word vectors having the same PASbased syntactic roles are updated similarly. The word-dependent scoring weight vectors propagate the information on which words should, or should </context>
<context position="26469" citStr="Mikolov et al. (2013" startWordPosition="4168" endWordPosition="4171">52 0.48 0.42 BL w/ BNC 0.48 0.50 0.35 HB w/ BNC 0.41 0.44 0.34 KS w/ ukWaC n/a n/a 0.45 K w/ BNC n/a n/a 0.41 Human agreement 0.52 0.49 0.55 Table 3: Spearman’s rank correlation scores p for the three tasks: AN, NN, and VO. the composed vector for each phrase was computed using the Waddnl function, and when we trained the PAS-LBLM, we used the element-wise addition function. To compute the composed vectors using the Waddl and Waddnl functions, we used the categories of the predicates adj arg1, noun arg1, and verb arg12 listed in Table 1. As a strong baseline, we trained the Skip-gram model of Mikolov et al. (2013b) using the publicly available word2vec5 software. We fed the POS-tagged BNC into word2vec since our models utilize POS tags and trained 50-dimensional word vectors using word2vec. For each phrase we then computed the representation using vector addition. 5.2 AN, NN, and VO Tasks Table 3 shows the correlation scores p for the AN, NN, and VO tasks. Human agreement denotes the inter-annotator agreement. The word2vec baseline achieves unexpectedly high scores for these three tasks. Previously these kinds of models (Mikolov et al., 2013b; Mnih and Kavukcuoglu, 2013) have mainly been evaluated for</context>
<context position="34325" citStr="Mikolov et al. (2013" startWordPosition="5466" endWordPosition="5469"> Analysis on Composed Vectors An open question that remains is to what extent composition affects the representations produced by our PAS-CLBLM model. To evaluate this we assigned a vector for each composed representation. For example, the adjective-noun dependency “heavy rain” would be assigned an independent vector. We added the most frequent 100,000 adjective-noun, noun-noun, and (subject-) verb-object tuples to the vocabulary and the resulting vocabulary contained 400,000 tokens (100,000+3×100,000). A similar method for treating frequent neighboring words as single words was introduced by Mikolov et al. (2013b). However, some dependencies, such as (subject-) verb-object phrases, are not always captured when considering only neighboring words. Table 7 (Io composition) shows some examples of predicate-argument dependencies with their closest neighbors in the vector space according to the cosine similarity. The table shows that the learned vectors of multiple words capture semantic similarity. For example, the vector of “heavy rain” is close to the vectors of words which express the phenomena heavily raining. The vector of “new york” captures the concept of a major city. The vectors of (subject-) ver</context>
</contexts>
<marker>Mikolov, Chen, Corrado, Dean, 2013</marker>
<rawString>Tomas Mikolov, Kai Chen, Greg Corrado, and Jeffrey Dean. 2013a. Efficient Estimation of Word Representations in Vector Space. In Proceedings of Workshop at the International Conference on Learning Representations.</rawString>
</citation>
<citation valid="false">
<authors>
<author>Tomas Mikolov</author>
<author>Ilya Sutskever</author>
<author>Kai Chen</author>
<author>Greg S Corrado</author>
<author>Jeff Dean</author>
</authors>
<booktitle>2013b. Distributed Representations of Words and Phrases and their Compositionality. In Advances in Neural Information Processing Systems 26,</booktitle>
<pages>3111--3119</pages>
<marker>Mikolov, Sutskever, Chen, Corrado, Dean, </marker>
<rawString>Tomas Mikolov, Ilya Sutskever, Kai Chen, Greg S Corrado, and Jeff Dean. 2013b. Distributed Representations of Words and Phrases and their Compositionality. In Advances in Neural Information Processing Systems 26, pages 3111–3119.</rawString>
</citation>
<citation valid="true">
<authors>
<author>Jeff Mitchell</author>
<author>Mirella Lapata</author>
</authors>
<title>Vector-based models of semantic composition.</title>
<date>2008</date>
<booktitle>In Proceedings of 46th Annual Meeting of the Association for Computational Linguistics: Human Language Technologies,</booktitle>
<pages>236--244</pages>
<contexts>
<context position="5636" citStr="Mitchell and Lapata, 2008" startWordPosition="842" endWordPosition="845">ver, the proposed model does not require any pre-trained word vectors produced by external models, but rather induces word vectors jointly while training. 2 Related Work There is a large body of work on how to represent the meaning of a word in a vector space. Distributional approaches assume that the meaning of a word is determined by the contexts in which it appears (Firth, 1957). The context of a word is often defined as the words appearing in a window of fixed-length (bag-of-words) and a simple approach is to treat the co-occurrence statistics of a word w as a vector representation for w (Mitchell and Lapata, 2008; Mitchell and Lapata, 2010); alternatively, dependencies between words can be used to define contexts (Goyal et al., 2013; Erk and Pad´o, 2008; Thater et al., 2010). In contrast to distributional representations, NNLMs represent words in a low-dimensional vector space (Bengio et al., 2003; Collobert et al., 2011). Recently, Mikolov et al. (2013b) and Mnih and Kavukcuoglu (2013) proposed highly scalable models to learn high-dimensional word vectors. Levy and Goldberg (2014) extended the model of Mikolov et al. (2013b) by treating syntactic dependencies as contexts. Mitchell and Lapata (2008) i</context>
<context position="17165" citStr="Mitchell and Lapata, 2008" startWordPosition="2675" endWordPosition="2679">vector produced by the composition function for the predicate category c′. For example, as shown in Figure 1 (b), when the first argument “rain” of the predicate “cause” is also the argument of the predicate “heavy”, we first compute the d-dimensional composed vector representation for “heavy” and “rain”: gc′(v(heavy),v(rain)), (7) where c′ is the category adj arg1, and gc′ is a function to combine input vectors for the predicatecategory c′. We can use any composition function that produces a representation of the same dimensionality as its inputs, such as elementwise addition/multiplication (Mitchell and Lapata, 2008) or neural networks (Socher et al., 2012). We then replace v(rain) in Eq. (2) with gc′(v(heavy), v(rain)). When the second argument “accident” in Eq. (2) is also the argument of the predicate “car”, v(accident) is replaced with gc′′(v(car), v(accident)). c″is the predicate category noun arg1. These multiple relationships of predicate-argument structures should provide richer context information. We refer to the PAS-LBLM with composition functions as PASCLBLM. 3.4 Bag-of-Words Sensitive PAS-CLBLM Both the PAS-LBLM and PAS-CLBLM can take meaningful relationships between words into account. Howev</context>
</contexts>
<marker>Mitchell, Lapata, 2008</marker>
<rawString>Jeff Mitchell and Mirella Lapata. 2008. Vector-based models of semantic composition. In Proceedings of 46th Annual Meeting of the Association for Computational Linguistics: Human Language Technologies, pages 236–244.</rawString>
</citation>
<citation valid="true">
<authors>
<author>Jeff Mitchell</author>
<author>Mirella Lapata</author>
</authors>
<title>Composition in Distributional Models of Semantics.</title>
<date>2010</date>
<journal>Cognitive Science,</journal>
<volume>34</volume>
<issue>8</issue>
<contexts>
<context position="2041" citStr="Mitchell and Lapata, 2010" startWordPosition="283" endWordPosition="286">successes in capturing their syntactic and semantic properties (Turney and Pantel, 2010). These embeddings have also been found to be a useful component for Natural Language Processing (NLP) systems; for example, Turian et al. (2010) and Collobert et al. (2011) demonstrated how low-dimensional word vectors learned by Neural Network Language Models (NNLMs) are beneficial for a wide range of NLP tasks. Recently, the main focus of research on vector space representation is shifting from word representations to phrase representations (Baroni and Zamparelli, 2010; Grefenstette and Sadrzadeh, 2011; Mitchell and Lapata, 2010; Socher et al., 2012). Combining the ideas of NNLMs and semantic composition, Tsubaki et al. (2013) introduced a novel NNLM incorporating verb-object dependencies. More recently, Levy and Goldberg (2014) presented a NNLM that integrated syntactic dependencies. However, to the best of our knowledge, there is no previous work on integrating a variety of syntactic and semantic dependencies into NNLMs in order to learn composition functions as well as word representations. The following question thus arises naturally: Can a variety of dependencies be used to jointly learn both stand-alone word ve</context>
<context position="5664" citStr="Mitchell and Lapata, 2010" startWordPosition="846" endWordPosition="849">s not require any pre-trained word vectors produced by external models, but rather induces word vectors jointly while training. 2 Related Work There is a large body of work on how to represent the meaning of a word in a vector space. Distributional approaches assume that the meaning of a word is determined by the contexts in which it appears (Firth, 1957). The context of a word is often defined as the words appearing in a window of fixed-length (bag-of-words) and a simple approach is to treat the co-occurrence statistics of a word w as a vector representation for w (Mitchell and Lapata, 2008; Mitchell and Lapata, 2010); alternatively, dependencies between words can be used to define contexts (Goyal et al., 2013; Erk and Pad´o, 2008; Thater et al., 2010). In contrast to distributional representations, NNLMs represent words in a low-dimensional vector space (Bengio et al., 2003; Collobert et al., 2011). Recently, Mikolov et al. (2013b) and Mnih and Kavukcuoglu (2013) proposed highly scalable models to learn high-dimensional word vectors. Levy and Goldberg (2014) extended the model of Mikolov et al. (2013b) by treating syntactic dependencies as contexts. Mitchell and Lapata (2008) investigated a variety of com</context>
<context position="24065" citStr="Mitchell and Lapata (2010)" startWordPosition="3788" endWordPosition="3791"> parameters with varying frequencies. 1549 The mini-batch size was 32 and the learning rate was 0.05 for each experiment, and no regularization was used. To verify the semantics captured by the proposed models during training and to tune the hyperparameters, we used the WordSim-3532 word similarity data set (Finkelstein et al., 2001). 5 Evaluation on Phrase Similarity Tasks 5.1 Evaluation Settings The learned models were evaluated on four tasks of measuring the semantic similarity between short phrases. We performed evaluation using the three tasks (AN, NN, and VO) in the dataset3 provided by Mitchell and Lapata (2010), and the SVO task in the dataset4 provided by Grefenstette and Sadrzadeh (2011). The datasets include pairs of short phrases extracted from the BNC. AN, NN, and VO contain 108 phrase pairs of adjective-noun, nounnoun, and verb-object. SVO contains 200 pairs of subject-verb-object phrases. Each phrase pair has multiple human-ratings: the higher the rating is, the more semantically similar the phrases. For example, the subject-verb-object phrase pair of “student write name” and “student spell name” has a high rating. The pair “people try door” and “people judge door” has a low rating. For evalu</context>
</contexts>
<marker>Mitchell, Lapata, 2010</marker>
<rawString>Jeff Mitchell and Mirella Lapata. 2010. Composition in Distributional Models of Semantics. Cognitive Science, 34(8):1388–1439.</rawString>
</citation>
<citation valid="true">
<authors>
<author>Yusuke Miyao</author>
<author>Jun’ichi Tsujii</author>
</authors>
<title>Feature forest models for probabilistic HPSG parsing.</title>
<date>2008</date>
<journal>Computational Linguistics,</journal>
<volume>34</volume>
<issue>1</issue>
<contexts>
<context position="9213" citStr="Miyao and Tsujii, 2008" startWordPosition="1377" endWordPosition="1380">sis of our model. We then introduce a Log-Bilinear Language Model using Predicate-Argument Structures (PAS-LBLM) to learn word representations using both bag-ofwords and dependency-based contexts. Finally, we propose integrating compositions of words into the model. Figure 1 (b) shows the overview of the proposed model. 3.1 Predicate-Argument Structures Due to advances in deep parsing technologies, syntactic parsers that can produce predicateargument structures are becoming accurate and fast enough to be used for practical applications. In this work, we use the probabilistic HPSG parser Enju (Miyao and Tsujii, 2008) to obtain the predicate-argument structures of individual sentences. In its grammar, each word in a sentence is treated as a predicate of a certain category with zero or more arguments. Table 1 shows some exCategory predicate arg1 arg2 adj arg1 heavy rain noun arg1 car accident verb arg12 cause rain accident prep arg12 at eat restaurant Table 1: Examples of predicates of different categories from the grammar of the Enju parser. arg1 and arg2 denote the first and second arguments. amples of predicates of different categories.&apos; For example, a predicate of the category verb arg12 expresses a ver</context>
</contexts>
<marker>Miyao, Tsujii, 2008</marker>
<rawString>Yusuke Miyao and Jun’ichi Tsujii. 2008. Feature forest models for probabilistic HPSG parsing. Computational Linguistics, 34(1):35–80.</rawString>
</citation>
<citation valid="true">
<authors>
<author>Andriy Mnih</author>
<author>Koray Kavukcuoglu</author>
</authors>
<title>Learning word embeddings efficiently with noise-contrastive estimation.</title>
<date>2013</date>
<booktitle>In Advances in Neural Information Processing Systems 26,</booktitle>
<pages>2265--2273</pages>
<contexts>
<context position="2849" citStr="Mnih and Kavukcuoglu, 2013" startWordPosition="411" endWordPosition="414">evy and Goldberg (2014) presented a NNLM that integrated syntactic dependencies. However, to the best of our knowledge, there is no previous work on integrating a variety of syntactic and semantic dependencies into NNLMs in order to learn composition functions as well as word representations. The following question thus arises naturally: Can a variety of dependencies be used to jointly learn both stand-alone word vectors and their compositions, embedding them in the same vector space? In this work, we bridge the gap between purely context-based (Levy and Goldberg, 2014; Mikolov et al., 2013b; Mnih and Kavukcuoglu, 2013) and compositional (Tsubaki et al., 2013) NNLMs by using the flexible set of categories from Predicate-Argument-Structures (PASs). More specifically, we propose a Compositional Log-Bilinear Language Model using PASs (PASCLBLM), an overview of which is shown in Figure 1. The model is trained by maximizing the accuracy of predicting target words from their bag-of-words and dependency-based context, which provides information about selectional preference. As shown in Figure 1 (b), one of the advantages of the PAS-CLBLM is that the model can treat not only word vectors but also composed vectors as</context>
<context position="6017" citStr="Mnih and Kavukcuoglu (2013)" startWordPosition="900" endWordPosition="903">1957). The context of a word is often defined as the words appearing in a window of fixed-length (bag-of-words) and a simple approach is to treat the co-occurrence statistics of a word w as a vector representation for w (Mitchell and Lapata, 2008; Mitchell and Lapata, 2010); alternatively, dependencies between words can be used to define contexts (Goyal et al., 2013; Erk and Pad´o, 2008; Thater et al., 2010). In contrast to distributional representations, NNLMs represent words in a low-dimensional vector space (Bengio et al., 2003; Collobert et al., 2011). Recently, Mikolov et al. (2013b) and Mnih and Kavukcuoglu (2013) proposed highly scalable models to learn high-dimensional word vectors. Levy and Goldberg (2014) extended the model of Mikolov et al. (2013b) by treating syntactic dependencies as contexts. Mitchell and Lapata (2008) investigated a variety of compositional operators to combine word vectors into phrasal representations. Among these operators, simple element-wise addition and multiplication are now widely used to represent short phrases (Mitchell and Lapata, 2010; Blacoe and Lapata, 2012). The obvious limitation with these simple approaches is that information about word order and syntactic rel</context>
<context position="7929" citStr="Mnih and Kavukcuoglu, 2013" startWordPosition="1183" endWordPosition="1186">et al., 2014; Tsubaki et al., 2013). One of the advantages of these models is that they are less restricted by word order. Among these, Tsubaki et al. (2013) introduced a novel compositional NNLM mainly focusing on verb-object dependencies and achieved state-of-the-art performance for the task of measuring the semantic similarity between subjectverb-object phrases. 3 PAS-CLBLM: A Compositional Log-Bilinear Language Model Using Predicate-Argument Structures In some recent studies on representing words as vectors, word vectors are learned by solving word prediction tasks (Mikolov et al., 2013a; Mnih and Kavukcuoglu, 2013). More specifically, given target words and their context words, the basic idea is to train classifiers to discriminate between each target word and artificially induced negative target words. The feature vector of the classifiers are calculated using the context word vectors whose values are optimized during training. As a result, vectors of words in similar contexts become similar to each other. Following these studies, we propose a novel model to jointly learn representations for words and their compositions by training word prediction classifiers using PASs. In this section, we first descr</context>
<context position="12492" citStr="Mnih and Kavukcuoglu (2013)" startWordPosition="1915" endWordPosition="1918">arguments are treated as the context words. In the same way, an argument can be treated as a target word: p(rain) = f(hverb arg12 O v(cause)+ hverb arg12 O v(accident)). verb arg2 (3) Relationship to previous work. If we omit the the category-specific weight vectors hz in Eq. (1), our model is similar to the CBOW model in Mikolov et al. (2013a). CBOW predicts a target word given its surrounding bag-of-words context, while our model uses its PAS-based context. To incorporate the PAS information in our model more efficiently, we use category-specific weight vectors. Similarly, the vLBL model of Mnih and Kavukcuoglu (2013) uses different weight vectors depending on the position relative to the target word. As with previous neural network language models (Collobert et al., 2011; Huang et al., 2012), our model and vLBL can use weight matrices rather than weight vectors. However, as discussed by Mnih and Teh (2012), using weight vectors makes the training significantly faster than using weight matrices. Despite the simple formulation of the element-wise operations, the categoryspecific weight vectors efficiently propagate PASbased context information as explained next. 3.2.2 Training Word Vectors To train the PAS-</context>
<context position="14855" citStr="Mnih and Kavukcuoglu, 2013" startWordPosition="2314" endWordPosition="2317">the log probability of the target word wt based on the softmax function: exp(s(wt,p(wt))) p(wt|context) = (6) V| ,i=1 exp(s(wi,p(wt))) This is equivalent to a softmax regression model. However, when the vocabulary V is large, computing the softmax function in Eq. (6) is computationally expensive. If we do not need probability distributions over words, we are not necessarily restricted to using the probabilistic expressions. Recently, several methods have been proposed to efficiently learn word representations rather than accurate language models (Collobert et al., 2011; Mikolov et al., 2013b; Mnih and Kavukcuoglu, 2013), and our objective follows the work of Collobert et al. (2011). Mikolov et al. (2013b) and Mnih and Kavukcuoglu (2013) trained their models using word-dependent scoring weight vectors which are the arguments of our scoring function in Eq. (4). During development we also trained our model using the negative sampling technique of Mikolov et al. (2013b); however, we did not observe any significant performance difference. Intuition behind the PAS-LBLM. Here we briefly explain how each class of the model parameters of the PAS-LBLM contributes to learning word representations at each stochastic gra</context>
<context position="27038" citStr="Mnih and Kavukcuoglu, 2013" startWordPosition="4256" endWordPosition="4259">ne, we trained the Skip-gram model of Mikolov et al. (2013b) using the publicly available word2vec5 software. We fed the POS-tagged BNC into word2vec since our models utilize POS tags and trained 50-dimensional word vectors using word2vec. For each phrase we then computed the representation using vector addition. 5.2 AN, NN, and VO Tasks Table 3 shows the correlation scores p for the AN, NN, and VO tasks. Human agreement denotes the inter-annotator agreement. The word2vec baseline achieves unexpectedly high scores for these three tasks. Previously these kinds of models (Mikolov et al., 2013b; Mnih and Kavukcuoglu, 2013) have mainly been evaluated for word analogy tasks and, to date, there has been no work using these word vectors for the task of measuring the semantic similarity between phrases. However, this experimental result suggests that word2vec can serve as a strong baseline for these kinds of tasks, in addition to word analogy tasks. In Table 3, BL, HB, KS, and K denote the work of Blacoe and Lapata (2012), Hermann and Blunsom (2013), Kartsaklis and Sadrzadeh (2013), and Kartsaklis et al. (2013) respectively. Among these, 5https://code.google.com/p/word2vec/ 1550 Model Corpus Averaged Non-averaged SV</context>
</contexts>
<marker>Mnih, Kavukcuoglu, 2013</marker>
<rawString>Andriy Mnih and Koray Kavukcuoglu. 2013. Learning word embeddings efficiently with noise-contrastive estimation. In Advances in Neural Information Processing Systems 26, pages 2265–2273.</rawString>
</citation>
<citation valid="true">
<authors>
<author>Andriy Mnih</author>
<author>Yee Whye Teh</author>
</authors>
<title>A fast and simple algorithm for training neural probabilistic language models.</title>
<date>2012</date>
<booktitle>In John Langford and Joelle Pineau, editors, Proceedings of the 29th International Conference on Machine Learning (ICML-12), ICML ’12,</booktitle>
<pages>1751--1758</pages>
<contexts>
<context position="12787" citStr="Mnih and Teh (2012)" startWordPosition="1968" endWordPosition="1971">to the CBOW model in Mikolov et al. (2013a). CBOW predicts a target word given its surrounding bag-of-words context, while our model uses its PAS-based context. To incorporate the PAS information in our model more efficiently, we use category-specific weight vectors. Similarly, the vLBL model of Mnih and Kavukcuoglu (2013) uses different weight vectors depending on the position relative to the target word. As with previous neural network language models (Collobert et al., 2011; Huang et al., 2012), our model and vLBL can use weight matrices rather than weight vectors. However, as discussed by Mnih and Teh (2012), using weight vectors makes the training significantly faster than using weight matrices. Despite the simple formulation of the element-wise operations, the categoryspecific weight vectors efficiently propagate PASbased context information as explained next. 3.2.2 Training Word Vectors To train the PAS-LBLM, we use a scoring function to evaluate how well the target word wt fits the given context: s(wt,p(wt)) = v(wt)Tp(wt), (4) where v(wt) E Rdx1 is the scoring weight vector for wt. Thus, the model parameters in the PASLBLM are (V, V, H). V is the set of word vectors v(w), and V is the set of </context>
</contexts>
<marker>Mnih, Teh, 2012</marker>
<rawString>Andriy Mnih and Yee Whye Teh. 2012. A fast and simple algorithm for training neural probabilistic language models. In John Langford and Joelle Pineau, editors, Proceedings of the 29th International Conference on Machine Learning (ICML-12), ICML ’12, pages 1751–1758.</rawString>
</citation>
<citation valid="true">
<authors>
<author>Denis Paperno</author>
<author>Nghia The Pham</author>
<author>Marco Baroni</author>
</authors>
<title>A practical and linguistically-motivated approach to compositional distributional semantics.</title>
<date>2014</date>
<booktitle>In Proceedings of the 52nd Annual Meeting of the Association for Computational Linguistics (Volume 1: Long Papers),</booktitle>
<pages>90--99</pages>
<contexts>
<context position="7293" citStr="Paperno et al., 2014" startWordPosition="1085" endWordPosition="1089">o composition functions, a variety of compositional models have been proposed. These include recursive neural networks using phrase-structure trees (Socher et al., 2012; Socher et al., 2013b) and models in which words have a specific form of parameters according to their syntactic roles and composition functions are syntactically dependent on the relations of input words (Baroni and Zamparelli, 2010; Grefenstette and Sadrzadeh, 2011; Hashimoto et al., 2013; Hermann and Blunsom, 2013; Socher et al., 2013a). More recently, syntactic dependency-based 1545 compositional models have been proposed (Paperno et al., 2014; Socher et al., 2014; Tsubaki et al., 2013). One of the advantages of these models is that they are less restricted by word order. Among these, Tsubaki et al. (2013) introduced a novel compositional NNLM mainly focusing on verb-object dependencies and achieved state-of-the-art performance for the task of measuring the semantic similarity between subjectverb-object phrases. 3 PAS-CLBLM: A Compositional Log-Bilinear Language Model Using Predicate-Argument Structures In some recent studies on representing words as vectors, word vectors are learned by solving word prediction tasks (Mikolov et al.</context>
</contexts>
<marker>Paperno, Pham, Baroni, 2014</marker>
<rawString>Denis Paperno, Nghia The Pham, and Marco Baroni. 2014. A practical and linguistically-motivated approach to compositional distributional semantics. In Proceedings of the 52nd Annual Meeting of the Association for Computational Linguistics (Volume 1: Long Papers), pages 90–99.</rawString>
</citation>
<citation valid="true">
<authors>
<author>Richard Socher</author>
<author>Brody Huval</author>
<author>Christopher D Manning</author>
<author>Andrew Y Ng</author>
</authors>
<title>Semantic Compositionality through Recursive Matrix-Vector Spaces.</title>
<date>2012</date>
<booktitle>In Proceedings of the 2012 Joint Conference on Empirical Methods in Natural Language Processing and Computational Natural Language Learning,</booktitle>
<pages>1201--1211</pages>
<contexts>
<context position="2063" citStr="Socher et al., 2012" startWordPosition="287" endWordPosition="290">r syntactic and semantic properties (Turney and Pantel, 2010). These embeddings have also been found to be a useful component for Natural Language Processing (NLP) systems; for example, Turian et al. (2010) and Collobert et al. (2011) demonstrated how low-dimensional word vectors learned by Neural Network Language Models (NNLMs) are beneficial for a wide range of NLP tasks. Recently, the main focus of research on vector space representation is shifting from word representations to phrase representations (Baroni and Zamparelli, 2010; Grefenstette and Sadrzadeh, 2011; Mitchell and Lapata, 2010; Socher et al., 2012). Combining the ideas of NNLMs and semantic composition, Tsubaki et al. (2013) introduced a novel NNLM incorporating verb-object dependencies. More recently, Levy and Goldberg (2014) presented a NNLM that integrated syntactic dependencies. However, to the best of our knowledge, there is no previous work on integrating a variety of syntactic and semantic dependencies into NNLMs in order to learn composition functions as well as word representations. The following question thus arises naturally: Can a variety of dependencies be used to jointly learn both stand-alone word vectors and their compos</context>
<context position="6841" citStr="Socher et al., 2012" startWordPosition="1018" endWordPosition="1021">apata (2008) investigated a variety of compositional operators to combine word vectors into phrasal representations. Among these operators, simple element-wise addition and multiplication are now widely used to represent short phrases (Mitchell and Lapata, 2010; Blacoe and Lapata, 2012). The obvious limitation with these simple approaches is that information about word order and syntactic relations is lost. To incorporate syntactic information into composition functions, a variety of compositional models have been proposed. These include recursive neural networks using phrase-structure trees (Socher et al., 2012; Socher et al., 2013b) and models in which words have a specific form of parameters according to their syntactic roles and composition functions are syntactically dependent on the relations of input words (Baroni and Zamparelli, 2010; Grefenstette and Sadrzadeh, 2011; Hashimoto et al., 2013; Hermann and Blunsom, 2013; Socher et al., 2013a). More recently, syntactic dependency-based 1545 compositional models have been proposed (Paperno et al., 2014; Socher et al., 2014; Tsubaki et al., 2013). One of the advantages of these models is that they are less restricted by word order. Among these, Tsu</context>
<context position="17206" citStr="Socher et al., 2012" startWordPosition="2683" endWordPosition="2686"> the predicate category c′. For example, as shown in Figure 1 (b), when the first argument “rain” of the predicate “cause” is also the argument of the predicate “heavy”, we first compute the d-dimensional composed vector representation for “heavy” and “rain”: gc′(v(heavy),v(rain)), (7) where c′ is the category adj arg1, and gc′ is a function to combine input vectors for the predicatecategory c′. We can use any composition function that produces a representation of the same dimensionality as its inputs, such as elementwise addition/multiplication (Mitchell and Lapata, 2008) or neural networks (Socher et al., 2012). We then replace v(rain) in Eq. (2) with gc′(v(heavy), v(rain)). When the second argument “accident” in Eq. (2) is also the argument of the predicate “car”, v(accident) is replaced with gc′′(v(car), v(accident)). c″is the predicate category noun arg1. These multiple relationships of predicate-argument structures should provide richer context information. We refer to the PAS-LBLM with composition functions as PASCLBLM. 3.4 Bag-of-Words Sensitive PAS-CLBLM Both the PAS-LBLM and PAS-CLBLM can take meaningful relationships between words into account. However, at times, the number of context words</context>
</contexts>
<marker>Socher, Huval, Manning, Ng, 2012</marker>
<rawString>Richard Socher, Brody Huval, Christopher D. Manning, and Andrew Y. Ng. 2012. Semantic Compositionality through Recursive Matrix-Vector Spaces. In Proceedings of the 2012 Joint Conference on Empirical Methods in Natural Language Processing and Computational Natural Language Learning, pages 1201–1211.</rawString>
</citation>
<citation valid="true">
<authors>
<author>Richard Socher</author>
<author>John Bauer</author>
<author>Christopher D Manning</author>
<author>Ng Andrew Y</author>
</authors>
<title>Parsing with Compositional Vector Grammars.</title>
<date>2013</date>
<booktitle>In Proceedings of the 51st Annual Meeting of the Association for Computational Linguistics (Volume 1: Long Papers),</booktitle>
<pages>455--465</pages>
<contexts>
<context position="6862" citStr="Socher et al., 2013" startWordPosition="1022" endWordPosition="1025">ated a variety of compositional operators to combine word vectors into phrasal representations. Among these operators, simple element-wise addition and multiplication are now widely used to represent short phrases (Mitchell and Lapata, 2010; Blacoe and Lapata, 2012). The obvious limitation with these simple approaches is that information about word order and syntactic relations is lost. To incorporate syntactic information into composition functions, a variety of compositional models have been proposed. These include recursive neural networks using phrase-structure trees (Socher et al., 2012; Socher et al., 2013b) and models in which words have a specific form of parameters according to their syntactic roles and composition functions are syntactically dependent on the relations of input words (Baroni and Zamparelli, 2010; Grefenstette and Sadrzadeh, 2011; Hashimoto et al., 2013; Hermann and Blunsom, 2013; Socher et al., 2013a). More recently, syntactic dependency-based 1545 compositional models have been proposed (Paperno et al., 2014; Socher et al., 2014; Tsubaki et al., 2013). One of the advantages of these models is that they are less restricted by word order. Among these, Tsubaki et al. (2013) in</context>
<context position="22985" citStr="Socher et al. (2013" startWordPosition="3615" endWordPosition="3618">e predicate-categoryspecific parameters which are learned during training. We investigate the effects of the non-linear function tanh for these composition functions. In the formulations of the backpropagation algorithm, non-linear functions allow the input vectors to weakly interact with each other. 4.5 Initialization and Optimization of Model Parameters We assigned a 50-dimensional vector for each word-POS pair described in Section 4.2 and initialized the vectors and the scoring weight vectors using small random values. In part inspired by the initialization method of the weight matrices in Socher et al. (2013a), we initialized all values in the compositional weight vectors of the Waddl and Waddnl as 1.0. The context weight vectors were initialized using small random values. We minimized the loss function in Eq. (5) using mini-batch SGD and AdaGrad (Duchi et al., 2011). Using AdaGrad, the SGD’s learning rate is adapted independently for each model parameter. This is helpful in training the PAS-LBLM and PAS-CLBLM, as they have conditionally dependent model parameters with varying frequencies. 1549 The mini-batch size was 32 and the learning rate was 0.05 for each experiment, and no regularization wa</context>
<context position="32048" citStr="Socher et al., 2013" startWordPosition="5096" endWordPosition="5099">0.50 0.49 0.43 0.32 Waddnl 50 0.48 0.40 0.39 0.34 1000 0.51 0.48 0.48 0.34 Table 5: Comparison of the PAS-CLBLM between d = 50 and d = 1000. model BoW AN NN VO SVO Addl w/ 0.52 0.44 0.35 0.24 w/o 0.48 0.46 0.38 0.23 Addnl w/ 0.52 0.46 0.45 0.24 w/o 0.50 0.47 0.41 0.15 Waddl w/ 0.48 0.39 0.34 0.21 w/o 0.47 0.39 0.38 0.21 Waddnl w/ 0.48 0.40 0.39 0.34 w/o 0.52 0.42 0.33 0.26 Table 6: Scores of the PAS-CLBLM with and without BoW contexts. This maintains the possibility to incorporate recently developed deep learning composition functions into our models, such as recursive neural tensor networks (Socher et al., 2013b) and cocompositional neural networks (Tsubaki et al., 2013). While such complex composition functions slow down the training of compositional models, richer information could be captured during training. 5.4 Effects of the Dimensionality To see how the dimensionality of the word vectors affects the scores, we trained the PAS-CLBLM for each setting using 1,000-dimensional word vectors and set the learning rate to 0.01. Table 5 shows the scores for all four tasks. Note that we only report the scores for the setting non-averaged SVOSVO here. As shown in Table 5, the scores consistently improved</context>
</contexts>
<marker>Socher, Bauer, Manning, Y, 2013</marker>
<rawString>Richard Socher, John Bauer, Christopher D. Manning, and Ng Andrew Y. 2013a. Parsing with Compositional Vector Grammars. In Proceedings of the 51st Annual Meeting of the Association for Computational Linguistics (Volume 1: Long Papers), pages 455–465.</rawString>
</citation>
<citation valid="true">
<authors>
<author>Richard Socher</author>
<author>Alex Perelygin</author>
<author>Jean Wu</author>
<author>Jason Chuang</author>
<author>Christopher D Manning</author>
<author>Andrew Ng</author>
<author>Christopher Potts</author>
</authors>
<title>Recursive Deep Models for Semantic Compositionality Over a Sentiment Treebank.</title>
<date>2013</date>
<booktitle>In Proceedings of the 2013 Conference on Empirical Methods in Natural Language Processing,</booktitle>
<pages>1631--1642</pages>
<contexts>
<context position="6862" citStr="Socher et al., 2013" startWordPosition="1022" endWordPosition="1025">ated a variety of compositional operators to combine word vectors into phrasal representations. Among these operators, simple element-wise addition and multiplication are now widely used to represent short phrases (Mitchell and Lapata, 2010; Blacoe and Lapata, 2012). The obvious limitation with these simple approaches is that information about word order and syntactic relations is lost. To incorporate syntactic information into composition functions, a variety of compositional models have been proposed. These include recursive neural networks using phrase-structure trees (Socher et al., 2012; Socher et al., 2013b) and models in which words have a specific form of parameters according to their syntactic roles and composition functions are syntactically dependent on the relations of input words (Baroni and Zamparelli, 2010; Grefenstette and Sadrzadeh, 2011; Hashimoto et al., 2013; Hermann and Blunsom, 2013; Socher et al., 2013a). More recently, syntactic dependency-based 1545 compositional models have been proposed (Paperno et al., 2014; Socher et al., 2014; Tsubaki et al., 2013). One of the advantages of these models is that they are less restricted by word order. Among these, Tsubaki et al. (2013) in</context>
<context position="22985" citStr="Socher et al. (2013" startWordPosition="3615" endWordPosition="3618">e predicate-categoryspecific parameters which are learned during training. We investigate the effects of the non-linear function tanh for these composition functions. In the formulations of the backpropagation algorithm, non-linear functions allow the input vectors to weakly interact with each other. 4.5 Initialization and Optimization of Model Parameters We assigned a 50-dimensional vector for each word-POS pair described in Section 4.2 and initialized the vectors and the scoring weight vectors using small random values. In part inspired by the initialization method of the weight matrices in Socher et al. (2013a), we initialized all values in the compositional weight vectors of the Waddl and Waddnl as 1.0. The context weight vectors were initialized using small random values. We minimized the loss function in Eq. (5) using mini-batch SGD and AdaGrad (Duchi et al., 2011). Using AdaGrad, the SGD’s learning rate is adapted independently for each model parameter. This is helpful in training the PAS-LBLM and PAS-CLBLM, as they have conditionally dependent model parameters with varying frequencies. 1549 The mini-batch size was 32 and the learning rate was 0.05 for each experiment, and no regularization wa</context>
<context position="32048" citStr="Socher et al., 2013" startWordPosition="5096" endWordPosition="5099">0.50 0.49 0.43 0.32 Waddnl 50 0.48 0.40 0.39 0.34 1000 0.51 0.48 0.48 0.34 Table 5: Comparison of the PAS-CLBLM between d = 50 and d = 1000. model BoW AN NN VO SVO Addl w/ 0.52 0.44 0.35 0.24 w/o 0.48 0.46 0.38 0.23 Addnl w/ 0.52 0.46 0.45 0.24 w/o 0.50 0.47 0.41 0.15 Waddl w/ 0.48 0.39 0.34 0.21 w/o 0.47 0.39 0.38 0.21 Waddnl w/ 0.48 0.40 0.39 0.34 w/o 0.52 0.42 0.33 0.26 Table 6: Scores of the PAS-CLBLM with and without BoW contexts. This maintains the possibility to incorporate recently developed deep learning composition functions into our models, such as recursive neural tensor networks (Socher et al., 2013b) and cocompositional neural networks (Tsubaki et al., 2013). While such complex composition functions slow down the training of compositional models, richer information could be captured during training. 5.4 Effects of the Dimensionality To see how the dimensionality of the word vectors affects the scores, we trained the PAS-CLBLM for each setting using 1,000-dimensional word vectors and set the learning rate to 0.01. Table 5 shows the scores for all four tasks. Note that we only report the scores for the setting non-averaged SVOSVO here. As shown in Table 5, the scores consistently improved</context>
</contexts>
<marker>Socher, Perelygin, Wu, Chuang, Manning, Ng, Potts, 2013</marker>
<rawString>Richard Socher, Alex Perelygin, Jean Wu, Jason Chuang, Christopher D. Manning, Andrew Ng, and Christopher Potts. 2013b. Recursive Deep Models for Semantic Compositionality Over a Sentiment Treebank. In Proceedings of the 2013 Conference on Empirical Methods in Natural Language Processing, pages 1631–1642.</rawString>
</citation>
<citation valid="true">
<authors>
<author>Richard Socher</author>
<author>Quoc V Le</author>
<author>Christopher D Manning</author>
<author>Andrew Y Ng</author>
</authors>
<title>Grounded Compositional Semantics for Finding and Describing Images with Sentences. Transactions ofthe Association for Computational Linguistics,</title>
<date>2014</date>
<pages>2--207</pages>
<contexts>
<context position="7314" citStr="Socher et al., 2014" startWordPosition="1090" endWordPosition="1093">s, a variety of compositional models have been proposed. These include recursive neural networks using phrase-structure trees (Socher et al., 2012; Socher et al., 2013b) and models in which words have a specific form of parameters according to their syntactic roles and composition functions are syntactically dependent on the relations of input words (Baroni and Zamparelli, 2010; Grefenstette and Sadrzadeh, 2011; Hashimoto et al., 2013; Hermann and Blunsom, 2013; Socher et al., 2013a). More recently, syntactic dependency-based 1545 compositional models have been proposed (Paperno et al., 2014; Socher et al., 2014; Tsubaki et al., 2013). One of the advantages of these models is that they are less restricted by word order. Among these, Tsubaki et al. (2013) introduced a novel compositional NNLM mainly focusing on verb-object dependencies and achieved state-of-the-art performance for the task of measuring the semantic similarity between subjectverb-object phrases. 3 PAS-CLBLM: A Compositional Log-Bilinear Language Model Using Predicate-Argument Structures In some recent studies on representing words as vectors, word vectors are learned by solving word prediction tasks (Mikolov et al., 2013a; Mnih and Kav</context>
</contexts>
<marker>Socher, Le, Manning, Ng, 2014</marker>
<rawString>Richard Socher, Quoc V. Le, Christopher D. Manning, and Andrew Y. Ng. 2014. Grounded Compositional Semantics for Finding and Describing Images with Sentences. Transactions ofthe Association for Computational Linguistics, 2:207–218.</rawString>
</citation>
<citation valid="true">
<authors>
<author>Stefan Thater</author>
<author>Hagen F¨urstenau</author>
<author>Manfred Pinkal</author>
</authors>
<title>Contextualizing Semantic Representations Using Syntactically Enriched Vector Models.</title>
<date>2010</date>
<booktitle>In Proceedings of the 48th Annual Meeting of the Association for Computational Linguistics,</booktitle>
<pages>948--957</pages>
<marker>Thater, F¨urstenau, Pinkal, 2010</marker>
<rawString>Stefan Thater, Hagen F¨urstenau, and Manfred Pinkal. 2010. Contextualizing Semantic Representations Using Syntactically Enriched Vector Models. In Proceedings of the 48th Annual Meeting of the Association for Computational Linguistics, pages 948– 957.</rawString>
</citation>
<citation valid="true">
<authors>
<author>Masashi Tsubaki</author>
<author>Kevin Duh</author>
<author>Masashi Shimbo</author>
<author>Yuji Matsumoto</author>
</authors>
<title>Modeling and Learning Semantic Co-Compositionality through Prototype Projections and Neural Networks.</title>
<date>2013</date>
<booktitle>In Proceedings of the 2013 Conference on Empirical Methods in Natural Language Processing,</booktitle>
<pages>130--140</pages>
<contexts>
<context position="2141" citStr="Tsubaki et al. (2013)" startWordPosition="300" endWordPosition="303">gs have also been found to be a useful component for Natural Language Processing (NLP) systems; for example, Turian et al. (2010) and Collobert et al. (2011) demonstrated how low-dimensional word vectors learned by Neural Network Language Models (NNLMs) are beneficial for a wide range of NLP tasks. Recently, the main focus of research on vector space representation is shifting from word representations to phrase representations (Baroni and Zamparelli, 2010; Grefenstette and Sadrzadeh, 2011; Mitchell and Lapata, 2010; Socher et al., 2012). Combining the ideas of NNLMs and semantic composition, Tsubaki et al. (2013) introduced a novel NNLM incorporating verb-object dependencies. More recently, Levy and Goldberg (2014) presented a NNLM that integrated syntactic dependencies. However, to the best of our knowledge, there is no previous work on integrating a variety of syntactic and semantic dependencies into NNLMs in order to learn composition functions as well as word representations. The following question thus arises naturally: Can a variety of dependencies be used to jointly learn both stand-alone word vectors and their compositions, embedding them in the same vector space? In this work, we bridge the g</context>
<context position="4976" citStr="Tsubaki et al., 2013" startWordPosition="725" endWordPosition="728">ility to learn meaningful representations for adjective-noun, noun-noun, and (subject-) verb-object dependencies. On three tasks of measuring the semantic similarity between short phrases (adjective-noun, noun-noun, and verb-object), the learned composed vectors achieve scores (Spearman’s rank correlation p) comparable to or higher than those of previous models. On a task involving more complex phrases (subject-verb-object), our learned composed vectors achieve state-of-the-art performance (p = 0.50) with a training corpus that is an order of magnitude smaller than that used by previous work (Tsubaki et al., 2013; Van de Cruys et al., 2013). Moreover, the proposed model does not require any pre-trained word vectors produced by external models, but rather induces word vectors jointly while training. 2 Related Work There is a large body of work on how to represent the meaning of a word in a vector space. Distributional approaches assume that the meaning of a word is determined by the contexts in which it appears (Firth, 1957). The context of a word is often defined as the words appearing in a window of fixed-length (bag-of-words) and a simple approach is to treat the co-occurrence statistics of a word w</context>
<context position="7337" citStr="Tsubaki et al., 2013" startWordPosition="1094" endWordPosition="1098">sitional models have been proposed. These include recursive neural networks using phrase-structure trees (Socher et al., 2012; Socher et al., 2013b) and models in which words have a specific form of parameters according to their syntactic roles and composition functions are syntactically dependent on the relations of input words (Baroni and Zamparelli, 2010; Grefenstette and Sadrzadeh, 2011; Hashimoto et al., 2013; Hermann and Blunsom, 2013; Socher et al., 2013a). More recently, syntactic dependency-based 1545 compositional models have been proposed (Paperno et al., 2014; Socher et al., 2014; Tsubaki et al., 2013). One of the advantages of these models is that they are less restricted by word order. Among these, Tsubaki et al. (2013) introduced a novel compositional NNLM mainly focusing on verb-object dependencies and achieved state-of-the-art performance for the task of measuring the semantic similarity between subjectverb-object phrases. 3 PAS-CLBLM: A Compositional Log-Bilinear Language Model Using Predicate-Argument Structures In some recent studies on representing words as vectors, word vectors are learned by solving word prediction tasks (Mikolov et al., 2013a; Mnih and Kavukcuoglu, 2013). More s</context>
<context position="27958" citStr="Tsubaki et al. (2013)" startWordPosition="4405" endWordPosition="4408">dition to word analogy tasks. In Table 3, BL, HB, KS, and K denote the work of Blacoe and Lapata (2012), Hermann and Blunsom (2013), Kartsaklis and Sadrzadeh (2013), and Kartsaklis et al. (2013) respectively. Among these, 5https://code.google.com/p/word2vec/ 1550 Model Corpus Averaged Non-averaged SVO-SVO SVO-V SVO-SVO SVO-V PAS-CLBLM (Addl) 0.29 0.34 0.24 0.28 PAS-CLBLM (Addnl) 0.27 0.32 0.24 0.28 PAS-CLBLM (Waddl) BNC 0.25 0.26 0.21 0.23 PAS-CLBLM (Waddnl) 0.42 0.50 0.34 0.41 PAS-LBLM 0.21 0.06 0.18 0.08 word2vec BNC 0.12 0.32 0.12 0.28 Grefenstette and Sadrzadeh (2011) BNC n/a n/a 0.21 n/a Tsubaki et al. (2013) ukWaC n/a 0.47 n/a n/a Van de Cruys et al. (2013) ukWaC n/a n/a 0.32 0.37 Human agreement 0.75 0.62 Table 4: Spearman’s rank correlation scores p for the SVO task. Averaged denotes the p calculated by averaged human ratings, and Non-averaged denotes the p calculated by non-averaged human ratings. only Kartsaklis and Sadrzadeh (2013) used the ukWaC corpus (Baroni et al., 2009) which is an order of magnitude larger than the BNC. As we can see in Table 3, the PAS-CLBLM (Addnl) achieves scores comparable to and higher than those of the baseline and the previous state-of-the-art results. In relati</context>
<context position="29487" citStr="Tsubaki et al., 2013" startWordPosition="4655" endWordPosition="4658">SVO task. The scores p for this task are reported for both averaged and non-averaged human ratings. This is due to a disagreement in previous work regarding which metric to use when reporting results. Hence, we report the scores for both settings in Table 4. Another point we should consider is that some previous work reported scores based on the similarity between composed representations (Grefenstette and Sadrzadeh, 2011; Van de Cruys et al., 2013), and others reported scores based on the similarity between composed representations and word representations of landmark verbs from the dataset (Tsubaki et al., 2013; Van de Cruys et al., 2013). For completeness, we report the scores for both settings: SVO-SVO and SVO-V in Table 4. The results show that the weighted addition model with the non-linear function tanh (PASCLBLM (Waddnl)) is effective for the more complex phrase task. While simple vector addition is sufficient for phrases consisting of word pairs, it is clear from our experimental results that they fall short for more complex structures such as those involved in the SVO task. Our PAS-CLBLM (Waddnl) model outperforms the previous state-of-the-art scores for the SVO task as reported by Tsubaki e</context>
<context position="32109" citStr="Tsubaki et al., 2013" startWordPosition="5105" endWordPosition="5108"> 0.48 0.48 0.34 Table 5: Comparison of the PAS-CLBLM between d = 50 and d = 1000. model BoW AN NN VO SVO Addl w/ 0.52 0.44 0.35 0.24 w/o 0.48 0.46 0.38 0.23 Addnl w/ 0.52 0.46 0.45 0.24 w/o 0.50 0.47 0.41 0.15 Waddl w/ 0.48 0.39 0.34 0.21 w/o 0.47 0.39 0.38 0.21 Waddnl w/ 0.48 0.40 0.39 0.34 w/o 0.52 0.42 0.33 0.26 Table 6: Scores of the PAS-CLBLM with and without BoW contexts. This maintains the possibility to incorporate recently developed deep learning composition functions into our models, such as recursive neural tensor networks (Socher et al., 2013b) and cocompositional neural networks (Tsubaki et al., 2013). While such complex composition functions slow down the training of compositional models, richer information could be captured during training. 5.4 Effects of the Dimensionality To see how the dimensionality of the word vectors affects the scores, we trained the PAS-CLBLM for each setting using 1,000-dimensional word vectors and set the learning rate to 0.01. Table 5 shows the scores for all four tasks. Note that we only report the scores for the setting non-averaged SVOSVO here. As shown in Table 5, the scores consistently improved with a few exceptions. The scores p = 0.51 for the NN task a</context>
</contexts>
<marker>Tsubaki, Duh, Shimbo, Matsumoto, 2013</marker>
<rawString>Masashi Tsubaki, Kevin Duh, Masashi Shimbo, and Yuji Matsumoto. 2013. Modeling and Learning Semantic Co-Compositionality through Prototype Projections and Neural Networks. In Proceedings of the 2013 Conference on Empirical Methods in Natural Language Processing, pages 130–140.</rawString>
</citation>
<citation valid="true">
<authors>
<author>Joseph Turian</author>
<author>Lev-Arie Ratinov</author>
<author>Yoshua Bengio</author>
</authors>
<title>Word Representations: A Simple and General Method for Semi-Supervised Learning.</title>
<date>2010</date>
<booktitle>In Proceedings of the 48th Annual Meeting of the Association for Computational Linguistics,</booktitle>
<pages>384--394</pages>
<contexts>
<context position="1649" citStr="Turian et al. (2010)" startWordPosition="226" endWordPosition="229">or higher than the previous best results. Our system achieves these results without the need for pretrained word vectors and using a much smaller training corpus; despite this, for the subject-verb-object dataset our model improves upon the state of the art by as much as ∼10% in relative performance. 1 Introduction Studies on embedding single words in a vector space have made notable successes in capturing their syntactic and semantic properties (Turney and Pantel, 2010). These embeddings have also been found to be a useful component for Natural Language Processing (NLP) systems; for example, Turian et al. (2010) and Collobert et al. (2011) demonstrated how low-dimensional word vectors learned by Neural Network Language Models (NNLMs) are beneficial for a wide range of NLP tasks. Recently, the main focus of research on vector space representation is shifting from word representations to phrase representations (Baroni and Zamparelli, 2010; Grefenstette and Sadrzadeh, 2011; Mitchell and Lapata, 2010; Socher et al., 2012). Combining the ideas of NNLMs and semantic composition, Tsubaki et al. (2013) introduced a novel NNLM incorporating verb-object dependencies. More recently, Levy and Goldberg (2014) pre</context>
</contexts>
<marker>Turian, Ratinov, Bengio, 2010</marker>
<rawString>Joseph Turian, Lev-Arie Ratinov, and Yoshua Bengio. 2010. Word Representations: A Simple and General Method for Semi-Supervised Learning. In Proceedings of the 48th Annual Meeting of the Association for Computational Linguistics, pages 384–394.</rawString>
</citation>
<citation valid="true">
<authors>
<author>Peter D Turney</author>
<author>Patrick Pantel</author>
</authors>
<title>From Frequency to Meaning: Vector Space Models of Semantics.</title>
<date>2010</date>
<journal>Journal ofArtificial Intelligence Research,</journal>
<volume>37</volume>
<issue>1</issue>
<contexts>
<context position="1504" citStr="Turney and Pantel, 2010" startWordPosition="202" endWordPosition="205">s verbobject and subject-verb-object relations. We verify this experimentally using two phrase similarity datasets and achieve results comparable to or higher than the previous best results. Our system achieves these results without the need for pretrained word vectors and using a much smaller training corpus; despite this, for the subject-verb-object dataset our model improves upon the state of the art by as much as ∼10% in relative performance. 1 Introduction Studies on embedding single words in a vector space have made notable successes in capturing their syntactic and semantic properties (Turney and Pantel, 2010). These embeddings have also been found to be a useful component for Natural Language Processing (NLP) systems; for example, Turian et al. (2010) and Collobert et al. (2011) demonstrated how low-dimensional word vectors learned by Neural Network Language Models (NNLMs) are beneficial for a wide range of NLP tasks. Recently, the main focus of research on vector space representation is shifting from word representations to phrase representations (Baroni and Zamparelli, 2010; Grefenstette and Sadrzadeh, 2011; Mitchell and Lapata, 2010; Socher et al., 2012). Combining the ideas of NNLMs and semant</context>
</contexts>
<marker>Turney, Pantel, 2010</marker>
<rawString>Peter D. Turney and Patrick Pantel. 2010. From Frequency to Meaning: Vector Space Models of Semantics. Journal ofArtificial Intelligence Research, 37(1):141–188.</rawString>
</citation>
<citation valid="true">
<authors>
<author>Tim Van de Cruys</author>
<author>Thierry Poibeau</author>
<author>Anna Korhonen</author>
</authors>
<title>A Tensor-based Factorization Model of Semantic Compositionality.</title>
<date>2013</date>
<booktitle>In Proceedings of the 2013 Conference of the North American Chapter of the Association for Computational Linguistics: Human Language Technologies,</booktitle>
<pages>1142--1151</pages>
<marker>Van de Cruys, Poibeau, Korhonen, 2013</marker>
<rawString>Tim Van de Cruys, Thierry Poibeau, and Anna Korhonen. 2013. A Tensor-based Factorization Model of Semantic Compositionality. In Proceedings of the 2013 Conference of the North American Chapter of the Association for Computational Linguistics: Human Language Technologies, pages 1142–1151.</rawString>
</citation>
</citationList>
</algorithm>
</algorithms>
<?xml version="1.0" encoding="UTF-8"?>
<algorithms version="110505">
<algorithm name="SectLabel" version="110505">
<variant no="0" confidence="0.000351">
<title confidence="0.6906745">
Unsupervised Word Alignment Using Frequency Constraint in Posterior
Regularized EM
</title>
<author confidence="0.9637">
Hidetaka Kamigaito1 2, Taro Watanabe2, Hiroya Takamura1, Manabu Okumura1
</author>
<affiliation confidence="0.958562333333333">
1Tokyo Institute of Technology, Precision and Intelligence Laboratory
4259 Nagatsuta-cho Midori-ku Yokohama, Japan
2National Institute of Information and Communication Technology
</affiliation>
<address confidence="0.773488">
3-5 Hikari-dai, Seika-cho, Soraku-gun, Kyoto, Japan
</address>
<sectionHeader confidence="0.951208" genericHeader="abstract">
Abstract
</sectionHeader>
<bodyText confidence="0.999962291666667">
Generative word alignment models, such
as IBM Models, are restricted to one-
to-many alignment, and cannot explicitly
represent many-to-many relationships in
a bilingual text. The problem is par-
tially solved either by introducing heuris-
tics or by agreement constraints such that
two directional word alignments agree
with each other. In this paper, we fo-
cus on the posterior regularization frame-
work (Ganchev et al., 2010) that can force
two directional word alignment models
to agree with each other during train-
ing, and propose new constraints that can
take into account the difference between
function words and content words. Ex-
perimental results on French-to-English
and Japanese-to-English alignment tasks
show statistically significant gains over the
previous posterior regularization baseline.
We also observed gains in Japanese-to-
English translation tasks, which prove the
effectiveness of our methods under gram-
matically different language pairs.
</bodyText>
<sectionHeader confidence="0.998881" genericHeader="introduction">
1 Introduction
</sectionHeader>
<bodyText confidence="0.999802109090909">
Word alignment is an important component in sta-
tistical machine translation (SMT). For instance
phrase-based SMT (Koehn et al., 2003) is based
on the concept of phrase pairs that are automat-
ically extracted from bilingual data and rely on
word alignment annotation. Similarly, the model
for hierarchical phrase-based SMT is built from
exhaustively extracted phrases that are, in turn,
heavily reliant on word alignment.
The Generative word alignment models, such as
the IBM Models (Brown et al., 1993) and HMM
(Vogel et al., 1996), are popular methods for au-
tomatically aligning bilingual texts, but are re-
stricted to represent one-to-many correspondence
of each word. To resolve this weakness, vari-
ous symmetrization methods are proposed. Och
and Ney (2003) and Koehn et al. (2003) propose
various heuristic methods to combine two direc-
tional models to represent many-to-many relation-
ships. As an alternative to heuristic methods, fil-
tering methods employ a threshold to control the
trade-off between precision and recall based on
a score estimated from the posterior probabili-
ties from two directional models. Matusov et al.
(2004) proposed arithmetic means of two mod-
els as a score for the filtering, whereas Liang et
al. (2006) reported better results using geometric
means. The joint training method (Liang et al.,
2006) enforces agreement between two directional
models. Posterior regularization (Ganchev et al.,
2010) is an alternative agreement method which
directly encodes agreement during training. DeN-
ero and Macherey (2011) and Chang et al. (2014)
also enforce agreement during decoding.
However, these agreement models do not take
into account the difference in language pairs,
which is crucial for linguistically different lan-
guage pairs, such as Japanese and English: al-
though content words may be aligned with each
other by introducing some agreement constraints,
function words are difficult to align.
We focus on the posterior regularization frame-
work and improve upon the previous work by
proposing new constraint functions that take into
account the difference in languages in terms of
content words and function words. In particular,
we differentiate between content words and func-
tion words by frequency in bilingual data, follow-
ing Setiawan et al. (2007).
Experimental results show that the proposed
methods achieved better alignment qualities on the
French-English Hansard data and the Japanese-
English Kyoto free translation task (KFTT) mea-
sured by AER and F-measure. In translation eval-
uations, we achieved statistically significant gains
</bodyText>
<page confidence="0.982357">
153
</page>
<bodyText confidence="0.599222666666667">
Proceedings of the 2014 Conference on Empirical Methods in Natural Language Processing (EMNLP), pages 153–158,
October 25-29, 2014, Doha, Qatar. c�2014 Association for Computational Linguistics
in BLEU scores in the NTCIR10.
</bodyText>
<sectionHeader confidence="0.9363455" genericHeader="method">
2 Statistical word alignment with
posterior regularization framework
</sectionHeader>
<bodyText confidence="0.998133181818182">
Given a bilingual sentence x = (xs, xt) where xs
and xt denote a source and target sentence, respec-
tively, the bilingual sentence is aligned by a many-
to-many alignment of y. We represent posterior
probabilities from two directional word alignment
models as →−p g(−→y |x) and ←−p g(←−y |x) with each ar-
row indicating a particular direction, and use θ to
denote the parameters of the models. For instance,
→− y is a subset of y for the alignment from xs to
xt under the model of p(xt, →−y |xs). In the case of
IBM Model 1, the model is represented as follows:
</bodyText>
<equation confidence="0.990839">
|xs1+ 1pt(xti|x&apos;i). (1)
</equation>
<bodyText confidence="0.917190666666667">
where we define the index of xt, xs as i, j(1 ≤
i ≤ |xt|,1 ≤ j ≤ |xs|) and the posterior probabil-
ity for the word pair (xtZ, xsj) is defined as follows:
</bodyText>
<equation confidence="0.982106">
i|xs
→− p (i, j|x) = pt(xt j) (2)
Ej0 pt(xti|xsj0).
</equation>
<bodyText confidence="0.999868">
Herein, we assume that the posterior probabil-
ity for wrong directional alignment is zero (i.e.,
→−p (←−y |x) = 0).1 Given the two directional mod-
els, Ganchev et al. defined a symmetric feature for
each target/source position pair, i, j as follows:
</bodyText>
<equation confidence="0.627736666666667">
+1 (−→y ⊂ y) ∩ (−→y i = j),
−1 (←−y ⊂ y) ∩ (←− y j = i),
0 otherwise.
</equation>
<bodyText confidence="0.999883142857143">
The feature assigns 1 for the subset of word align-
ment for →− y , but assigns −1 for ←−y . As a result,
if a word pair i, j is aligned with equal posterior
probabilities in two directions, the expectation of
the feature value will be zero. Ganchev et al. de-
fined a joint model that combines two directional
models using arithmetic means:
</bodyText>
<equation confidence="0.996500333333333">
1 →−
pθ(y|x) = p θ(y|x) + 1 ←− p θ(y|x). (4)
2 2
</equation>
<bodyText confidence="0.9994632">
Under the posterior regularization framework, we
instead use q that is derived by maximizing the fol-
lowing posterior probability parametrized by A for
each bilingual data x as follows (Ganchev et al.,
2010):
</bodyText>
<equation confidence="0.937725055555556">
→− p θ(−→ y |x) + ←− p θ(←− y |x)
qa(y|x) = (5)
2
exp{−λ · φ(x, y)}
·Z
1No alignment is represented by alignment into a special
token ”null”.
q(−→y |x)nθ.) + ←−q (←−y |x)nθ(x) ,
2Z
Z = 1(Z��q + Z��q ),
2 P g Pe
→−q (−→y |x) = 1
Z9
Z_-_I = E
�i
←− q (←− y |x) = 1 Z9
Z9 = �
�� y
</equation>
<bodyText confidence="0.999664428571428">
such that EQa[φZ,j(x, y)] = 0. In the E-step of
EM-algorithm, we employ qλ instead of pg to ac-
cumulate fractional counts for its use in the M-
step. A is efficiently estimated by the gradient as-
cent for each bilingual sentence x. Note that pos-
terior regularization is performed during parame-
ter estimation, and not during testing.
</bodyText>
<sectionHeader confidence="0.978796" genericHeader="method">
3 Posterior Regularization with
</sectionHeader>
<subsectionHeader confidence="0.510319">
Frequency Constraint
</subsectionHeader>
<bodyText confidence="0.999980608695652">
The symmetric constraint method represented in
Equation (3) assumes a strong one-to-one rela-
tion for any word, and does not take into account
the divergence in language pairs. For linguisti-
cally different language pairs, such as Japanese-
English, content words may be easily aligned one-
to-one, but function words are not always aligned
together. In addition, Japanese is a pro-drop lan-
guage which can easily violate the symmetric con-
straint when proper nouns in the English side have
to be aligned with a “null” word. In addition, low
frequency words may cause unreliable estimates
for adjusting the weighing parameters A.
In order to solve the problem, we improve
Ganchev’s symmetric constraint so that it can con-
sider the difference between content words and
function words in each language. In particular, we
follow the frequency-based idea of Setiawan et al.
(2007) that discriminates content words and func-
tion words by their frequencies. We propose con-
straint features that take into account the differ-
ence between content words and function words,
determined by a frequency threshold.
</bodyText>
<subsectionHeader confidence="0.999816">
3.1 Mismatching constraint
</subsectionHeader>
<bodyText confidence="0.9998115">
First, we propose a mismatching constraint that
penalizes word alignment between content words
and function words by decreasing the correspond-
ing posterior probabilities.
</bodyText>
<equation confidence="0.955731">
rIp(xt, →− y |xs) =
i
φi,j(x, y) = I
(3)
</equation>
<bodyText confidence="0.6747345">
→−p θ(−→y , x)exp{−λ · φ(x, y)},
→−p θ(−→y , x)exp{−λ · φ(x, y)},
←−p θ(←−y , x)exp{−λ · φ(x, y)},
←−p θ(←−y , x)exp{−λ · φ(x, y)},
</bodyText>
<page confidence="0.98411">
154
</page>
<equation confidence="0.91585775">
The constraint is represented as f2c (function to
content) constraint:
φf2c
i,j (x, y) = (6)
+1 (−→y ⊂ y) ∩ (−→y i = j) ∩ ((xti ∈ Ct ∩ xsj ∈ Fs)
∪(xti ∈ Ft ∩ xs j ∈ Cs)) ∩ (δi,j(x, y) &gt; 0),
0 (←− y ⊂ y) ∩ (←− y j = i) ∩ ((xt i ∈ Ct ∩ xs j ∈ Fs)
∪(xti ∈ Ft ∩ xs j ∈ Cs)) ∩ (δi,j(x, y) &gt; 0),
0 (−→ y ⊂ y) ∩ (−→ y i = j) ∩ ((xt i ∈ Ct ∩ xs j ∈ Fs)
∪(xti ∈ Ft ∩ xs j ∈ Cs)) ∩ (δi,j(x, y) &lt; 0),
−1 (←− y ⊂ y) ∩ (←− y j = i) ∩ ((xt i ∈ Ct ∩ xs j ∈ Fs)
∪(xti ∈ Ft ∩ xsj ∈ Cs)) ∩ (δi,j(x, y) &lt; 0).
</equation>
<bodyText confidence="0.999947105263158">
where δi,j(x,y) = →−p θ(i,j|x) − ←−p θ(i,j|x) is
the difference in the posterior probabilities be-
tween the source-to-target and the target-to-source
alignment. Cs and Ct represent content words in
the source sentence and target sentence, respec-
tively. Similarly, Fs and Ft are function words
in the source and target sentence, respectively. In-
tuitively, when there exists a mismatch in content
word and function word for a word pair (i, j), the
constraint function returns a non-zero value for
the model with the highest posterior probability.
When coupled with the constraint such that the ex-
pectation of the feature value is zero, the constraint
function decreases the posterior probability of the
highest direction and discourages agreement with
each other.
Note that when this constraint is not fired, we
fall back to the constraint function in Equation (3)
for each word pair.
</bodyText>
<subsectionHeader confidence="0.999834">
3.2 Matching constraint
</subsectionHeader>
<bodyText confidence="0.99998475">
In contrast to the mismatching constraint, our
second constraint function rewards alignment for
function to function word matching, namely f2f.
The f2f constraint function is defined as follows:
</bodyText>
<equation confidence="0.9960045">
φf2f
i,j (x, y) = (7)
+1 (−→y ⊂ y) ∩ (−→y i = j)∩
(xti ∈ Ft ∩ xs j ∈ Fs) ∩ (δi,j(x, y) &gt; 0),
0 (←− y ⊂ y) ∩ (←− y j = i)∩
(xti ∈ Ft ∩ xs j ∈ Fs) ∩ (δi,j(x, y) &gt; 0),
0 (−→y ⊂ y) ∩ (−→yi = j)∩
(xti ∈ Ft ∩ xs j ∈ Fs) ∩ (δi,j(x, y) &lt; 0),
−1 (←− y ⊂ y) ∩ (←− y j = i)∩
(xti ∈ Ft ∩ xsj ∈ Fs) ∩ (δi,j(x, y) &lt; 0).
</equation>
<bodyText confidence="0.999948538461539">
This constraint function returns a non-zero value
for a word pair (i, j) when they are function
words. As a result, the pair of function words
are encouraged to agree with each other, but not
other pairs. The content to content word matching
function c2c can be defined similarly by replac-
ing Fs and Ft by Cs and Ct, respectively. Like-
wise, the function to content word matching func-
tion f2c is defined by considering the matching
of content words and function words in two lan-
guages. As noted in the mismatch function, when
no constraint is fired, we fall back to Eq (3) for
each word pair.
</bodyText>
<sectionHeader confidence="0.999477" genericHeader="evaluation">
4 Experiment
</sectionHeader>
<subsectionHeader confidence="0.997871">
4.1 Experimental Setup
</subsectionHeader>
<bodyText confidence="0.986135516129032">
The data sets used in our experiments are the
French-English Hansard Corpus, and two data sets
for Japanese-English tasks: the Kyoto free trans-
lation task (KFTT) and NTCIR10. The Hansard
Corpus consists of parallel texts drawn from of-
ficial records of the proceedings of the Canadian
Parliament. The KFTT (Neubig, 2011) is derived
from Japanese Wikipedia articles related to Ky-
oto, which is professionally translated into En-
glish. NTCIR10 comes from patent data employed
in a machine translation shared task (Goto et al.,
2013). The statistics of these data are presented in
Table 1.
Sentences of over 40 words on both source and
target sides are removed for training alignment
models. We used a word alignment toolkit ci-
cada 2 for training the IBM Model 4 with our
proposed methods. Training is bootstrapped from
IBM Model 1, followed by HMM and IBM Model
4. When generating the final bidirectional word
alignment, we use a grow-diag-final heuristic for
the Japanese-English tasks and an intersection
heuristic in the French-English task, judged by
preliminary studies.
Following Bisazza and Federico (2012), we
automatically decide the threshold for word fre-
quency to discriminate between content words and
function words. Specifically, the threshold is de-
termined by the ratio of highly frequent words.
The threshold th is the maximum frequency that
satisfies the following equation:
</bodyText>
<equation confidence="0.963921">
E
w∈(freq(w)&gt;th) freq(w)
</equation>
<subsectionHeader confidence="0.992746">
4.2 Word alignment evaluation
</subsectionHeader>
<bodyText confidence="0.999507">
We measure the impact of our proposed meth-
ods on the quality of word alignment measured
</bodyText>
<footnote confidence="0.556813">
2https://github.com/tarowatanabe/cicada
</footnote>
<equation confidence="0.8099725">
⎧
⎨⎪⎪⎪⎪⎪⎪⎪
⎪⎪⎪⎪⎪⎪⎪⎩
⎧
⎨⎪⎪⎪⎪⎪⎪⎪
⎪⎪⎪⎪⎪⎪⎪⎩
E (8)
w∈all freq(w) &gt; r.
</equation>
<bodyText confidence="0.998582">
Here, we empirically set r = 0.5 by preliminary
studies. This method is based on the intuition that
content words and function words exist in a docu-
ment at a constant rate.
</bodyText>
<page confidence="0.999647">
155
</page>
<tableCaption confidence="0.999942">
Table 1: The statistics of the data sets
</tableCaption>
<table confidence="0.999300714285714">
hansard kftt NTCIR10
French English Japanese English Japanese English
train sentence 1.13M 329.88K 2.02M
word 23.3M 19.8M 6.08M 5.91M 53.4M 49.4M
vocabulary 78.1K 57.3K 114K 138K 114K 183K
dev sentence 1.17K 2K
word 26.8K 24.3K 73K 67.3K
vocabulary 4.51K 4.78K 4.38K 5.04K
test WA sentence 447 582
word 7.76K 7.02K 14.4K 12.6K
vocabulary 1,92K 1.69K 2.57K 2.65K
TR sentence 1.16K 8.6K
word 28.5K 26.7K 334K 310K
vocabulary 4.91K 4.57K 10.4K 12.7K
</table>
<figureCaption confidence="0.999909333333333">
Figure 1: Precision Recall graph in Hansard Figure 2: Precision Recall graph in KFTT
French-English
Figure 3: AER in Hansard French-English Figure 4: AER in KFTT
</figureCaption>
<page confidence="0.997271">
156
</page>
<tableCaption confidence="0.999007">
Table 2: Results of word alignment evaluation with the heuristics-based method (GDF)
</tableCaption>
<table confidence="0.4441965">
KFTT Hansard (French-English)
method precision recall AER F precision recall AER F
symmetric 0.4595 0.5942 48.18 0.5182 0.7029 0.8816 7.29 0.7822
f2f 0.4633 0.5997 47.73 0.5227 0.7042 0.8851 7.29 0.7844
c2c 0.4606 0.5964 48.02 0.5198 0.7001 0.8816 7.34 0.7804
f2c 0.4630 0.5998 47.74 0.5226 0.7037 0.8871 7.10 0.7848
</table>
<bodyText confidence="0.999748703703704">
by AER and F-measure (Och and Ney, 2003).
Since there exists no distinction for sure-possible
alignments in the KFTT data, we use only sure
alignment for our evaluation, both for the French-
English and the Japanese-English tasks. Table 2
summarizes our results.
The baseline method is symmetric constraint
(Ganchev et al., 2010) shown in Table 2. The num-
bers in bold and in italics indicate the best score
and the second best score, respectively. The dif-
ferences between f2f,f2c and baseline in KFTT are
statistically significant at p &lt; 0.05 using the sign-
test, but in hansard corpus, there exist no signifi-
cant differences between the baseline and the pro-
posed methods. In terms of F-measure, it is clear
that the f2f method is the most effective method
in KFTT, and both f2f and f2c methods exceed the
original posterior regularized model of Ganchev et
al. (2010).
We also compared these methods with filtering
methods (Liang et al., 2006), in addition to heuris-
tic methods. We plot precision/recall curves and
AER by varying the threshold between 0.1 and
0.9 with 0.1 increments. From Figures, it can be
seen that our proposed methods are superior to
the baseline in terms of both precision-recall and
AER.
</bodyText>
<subsectionHeader confidence="0.999512">
4.3 Translation evaluation
</subsectionHeader>
<bodyText confidence="0.9992838">
Next, we performed a translation evaluation, mea-
sured by BLEU (Papineni et al., 2002). We
compared the grow-diag-final and filtering method
(Liang et al., 2006) for creating phrase tables.
The threshold for the filtering factor was set to
0.1 which was the best setting in the word align-
ment experiment in section 4.2 under KFTT. From
the English side of the training data, we trained a
word using the 5-gram model with SRILM (Stol-
cke and others, 2002). “Moses” toolkit was used
as a decoder (Koehn et al., 2007) and the model
parameters were tuned by k-best MIRA (Cherry
and Foster, 2012). In order to avoid tuning insta-
bility, we evaluated the average of five runs (Hop-
kins and May, 2011). The results are summarized
</bodyText>
<tableCaption confidence="0.999596">
Table 3: Results of translation evaluation
</tableCaption>
<table confidence="0.996731333333333">
KFTT NTCIR10
GDF Filtered GDF Filtered
symmetric 19.06 19.28 28.3 29.71
f2f 19.15 19.17 28.36 29.74
c2c 19.26 19.02 28.36 29.92
f2c 18.91 19.20 28.36 29.67
</table>
<bodyText confidence="0.999966083333333">
in Table 3. Our proposed methods achieved large
gains in NTCIR10 task with the filtered method,
but observed no gain in the KFTT with the filtered
method. In NTCIR10 task with GDF, the gain in
BLEU was smaller than that of KFTT. We cal-
culate p-values and the difference between sym-
metric and c2c (the most effective proposed con-
straint) are lower than 0.05 in kftt with GDF and
NTCIR10 with filtered method. There seems to
be no clear tendency in the improved alignment
qualities and the translation qualities, as shown in
numerous previous studies (Ganchev et al., 2008).
</bodyText>
<sectionHeader confidence="0.998925" genericHeader="conclusions">
5 Conclusion
</sectionHeader>
<bodyText confidence="0.999940210526316">
In this paper, we proposed new constraint func-
tions under the posterior regularization frame-
work. Our constraint functions introduce a
fine-grained agreement constraint considering the
frequency of words, a assuming that the high
frequency words correspond to function words
whereas the less frequent words may be treated
as content words, based on the previous work of
Setiawan et al. (2007). Experiments on word
alignment tasks showed better alignment quali-
ties measured by F-measure and AER on both the
Hansard task and KFTT. We also observed large
gain in BLEU, 0.2 on average, when compared
with the previous posterior regularization method
under NTCIR10 task.
As our future work, we will investigate more
precise methods for deciding function words and
content words for better alignment and translation
qualities.
</bodyText>
<page confidence="0.9966">
157
</page>
<sectionHeader confidence="0.981809" genericHeader="references">
References
</sectionHeader>
<reference confidence="0.999577633663366">
Arianna Bisazza and Marcello Federico. 2012. Cutting
the long tail: Hybrid language models for translation
style adaptation. In Proceedings of the 13th Confer-
ence of the European Chapter of the Association for
Computational Linguistics, pages 439–448. Associ-
ation for Computational Linguistics.
Peter F Brown, Vincent J Della Pietra, Stephen A Della
Pietra, and Robert L Mercer. 1993. The mathemat-
ics of statistical machine translation: Parameter esti-
mation. Computational linguistics, 19(2):263–311.
Yin-Wen Chang, Alexander M. Rush, John DeNero,
and Michael Collins. 2014. A constrained viterbi
relaxation for bidirectional word alignment. In Pro-
ceedings of the 52nd Annual Meeting of the Associa-
tion for Computational Linguistics (Volume 1: Long
Papers), pages 1481–1490, Baltimore, Maryland,
June. Association for Computational Linguistics.
Colin Cherry and George Foster. 2012. Batch tun-
ing strategies for statistical machine translation. In
Proceedings of the 2012 Conference of the North
American Chapter of the Association for Computa-
tional Linguistics: Human Language Technologies,
pages 427–436. Association for Computational Lin-
guistics.
John DeNero and Klaus Macherey. 2011. Model-
based aligner combination using dual decomposi-
tion. In Proceedings of the 49th Annual Meeting of
the Association for Computational Linguistics: Hu-
man Language Technologies, pages 420–429, Port-
land, Oregon, USA, June. Association for Computa-
tional Linguistics.
Kuzman Ganchev, Jo˜ao V. Grac¸a, and Ben Taskar.
2008. Better alignments = better translations?
In Proceedings of ACL-08: HLT, pages 986–993,
Columbus, Ohio, June. Association for Computa-
tional Linguistics.
Kuzman Ganchev, Joao Grac¸a, Jennifer Gillenwater,
and Ben Taskar. 2010. Posterior regularization for
structured latent variable models. The Journal of
Machine Learning Research, 99:2001–2049.
Isao Goto, Ka Po Chow, Bin Lu, Eiichiro Sumita, and
Benjamin K Tsou. 2013. Overview of the patent
machine translation task at the ntcir-10 workshop.
In Proceedings of the 10th NTCIR Workshop Meet-
ing on Evaluation of Information Access Technolo-
gies: Information Retrieval, Question Answering
and Cross-Lingual Information Access, NTCIR-10.
Mark Hopkins and Jonathan May. 2011. Tuning as
ranking. In Proceedings of the 2011 Conference on
Empirical Methods in Natural Language Process-
ing, pages 1352–1362, Edinburgh, Scotland, UK.,
July. Association for Computational Linguistics.
Philipp Koehn, Franz Josef Och, and Daniel Marcu.
2003. Statistical phrase-based translation. In
Proceedings of the 2003 Conference of the North
American Chapter of the Association for Computa-
tional Linguistics on Human Language Technology-
Volume 1, pages 48–54. Association for Computa-
tional Linguistics.
Philipp Koehn, Hieu Hoang, Alexandra Birch, Chris
Callison-Burch, Marcello Federico, Nicola Bertoldi,
Brooke Cowan, Wade Shen, Christine Moran,
Richard Zens, et al. 2007. Moses: Open source
toolkit for statistical machine translation. In Pro-
ceedings of the 45th Annual Meeting of the ACL
on Interactive Poster and Demonstration Sessions,
pages 177–180. Association for Computational Lin-
guistics.
Percy Liang, Ben Taskar, and Dan Klein. 2006. Align-
ment by agreement. In Proceedings of the Human
Language Technology Conference of the NAACL,
Main Conference, pages 104–111, New York City,
USA, June. Association for Computational Linguis-
tics.
E. Matusov, R. Zens, and H. Ney. 2004. Symmetric
Word Alignments for Statistical Machine Transla-
tion. In Proceedings of COLING 2004, pages 219–
225, Geneva, Switzerland, August 23–27.
Graham Neubig. 2011. The Kyoto free translation
task. http://www.phontron.com/kftt.
Franz Josef Och and Hermann Ney. 2003. A sys-
tematic comparison of various statistical alignment
models. Computational linguistics, 29(1):19–51.
Kishore Papineni, Salim Roukos, Todd Ward, and Wei-
Jing Zhu. 2002. Bleu: a method for automatic
evaluation of machine translation. In Proceedings of
the 40th annual meeting on association for compu-
tational linguistics, pages 311–318. Association for
Computational Linguistics.
Hendra Setiawan, Min-Yen Kan, and Haizhou Li.
2007. Ordering phrases with function words. In
Proceedings of the 45th annual meeting on associ-
ation for computational linguistics, pages 712–719.
Association for Computational Linguistics.
Andreas Stolcke et al. 2002. Srilm-an extensible lan-
guage modeling toolkit. In INTERSPEECH.
Stephan Vogel, Hermann Ney, and Christoph Tillmann.
1996. Hmm-based word alignment in statistical
translation. In Proceedings of the 16th conference
on Computational linguistics-Volume 2, pages 836–
841. Association for Computational Linguistics.
</reference>
<page confidence="0.997057">
158
</page>
</variant>
</algorithm>
<algorithm name="ParsHed" version="110505">
<variant no="0" confidence="0.348331">
<title confidence="0.8732915">Unsupervised Word Alignment Using Frequency Constraint in Posterior Regularized EM</title>
<author confidence="0.995482">Taro Hiroya Manabu</author>
<affiliation confidence="0.998365">Institute of Technology, Precision and Intelligence</affiliation>
<address confidence="0.522737">4259 Nagatsuta-cho Midori-ku Yokohama,</address>
<affiliation confidence="0.982769">Institute of Information and Communication</affiliation>
<address confidence="0.935754">3-5 Hikari-dai, Seika-cho, Soraku-gun, Kyoto, Japan</address>
<abstract confidence="0.998789">Generative word alignment models, such as IBM Models, are restricted to oneto-many alignment, and cannot explicitly represent many-to-many relationships in a bilingual text. The problem is partially solved either by introducing heuristics or by agreement constraints such that two directional word alignments agree with each other. In this paper, we focus on the posterior regularization framework (Ganchev et al., 2010) that can force two directional word alignment models to agree with each other during training, and propose new constraints that can take into account the difference between function words and content words. Experimental results on French-to-English and Japanese-to-English alignment tasks show statistically significant gains over the previous posterior regularization baseline. We also observed gains in Japanese-to- English translation tasks, which prove the effectiveness of our methods under grammatically different language pairs.</abstract>
</variant>
</algorithm>
<algorithm name="ParsCit" version="110505">
<citationList>
<citation valid="true">
<authors>
<author>Arianna Bisazza</author>
<author>Marcello Federico</author>
</authors>
<title>Cutting the long tail: Hybrid language models for translation style adaptation.</title>
<date>2012</date>
<booktitle>In Proceedings of the 13th Conference of the European Chapter of the Association for Computational Linguistics,</booktitle>
<pages>439--448</pages>
<publisher>Association</publisher>
<institution>for Computational Linguistics.</institution>
<contexts>
<context position="11688" citStr="Bisazza and Federico (2012)" startWordPosition="2022" endWordPosition="2025">ne translation shared task (Goto et al., 2013). The statistics of these data are presented in Table 1. Sentences of over 40 words on both source and target sides are removed for training alignment models. We used a word alignment toolkit cicada 2 for training the IBM Model 4 with our proposed methods. Training is bootstrapped from IBM Model 1, followed by HMM and IBM Model 4. When generating the final bidirectional word alignment, we use a grow-diag-final heuristic for the Japanese-English tasks and an intersection heuristic in the French-English task, judged by preliminary studies. Following Bisazza and Federico (2012), we automatically decide the threshold for word frequency to discriminate between content words and function words. Specifically, the threshold is determined by the ratio of highly frequent words. The threshold th is the maximum frequency that satisfies the following equation: E w∈(freq(w)&gt;th) freq(w) 4.2 Word alignment evaluation We measure the impact of our proposed methods on the quality of word alignment measured 2https://github.com/tarowatanabe/cicada ⎧ ⎨⎪⎪⎪⎪⎪⎪⎪ ⎪⎪⎪⎪⎪⎪⎪⎩ ⎧ ⎨⎪⎪⎪⎪⎪⎪⎪ ⎪⎪⎪⎪⎪⎪⎪⎩ E (8) w∈all freq(w) &gt; r. Here, we empirically set r = 0.5 by preliminary studies. This method is b</context>
</contexts>
<marker>Bisazza, Federico, 2012</marker>
<rawString>Arianna Bisazza and Marcello Federico. 2012. Cutting the long tail: Hybrid language models for translation style adaptation. In Proceedings of the 13th Conference of the European Chapter of the Association for Computational Linguistics, pages 439–448. Association for Computational Linguistics.</rawString>
</citation>
<citation valid="true">
<authors>
<author>Peter F Brown</author>
<author>Vincent J Della Pietra</author>
<author>Stephen A Della Pietra</author>
<author>Robert L Mercer</author>
</authors>
<title>The mathematics of statistical machine translation: Parameter estimation.</title>
<date>1993</date>
<journal>Computational linguistics,</journal>
<volume>19</volume>
<issue>2</issue>
<contexts>
<context position="1867" citStr="Brown et al., 1993" startWordPosition="261" endWordPosition="264">tion tasks, which prove the effectiveness of our methods under grammatically different language pairs. 1 Introduction Word alignment is an important component in statistical machine translation (SMT). For instance phrase-based SMT (Koehn et al., 2003) is based on the concept of phrase pairs that are automatically extracted from bilingual data and rely on word alignment annotation. Similarly, the model for hierarchical phrase-based SMT is built from exhaustively extracted phrases that are, in turn, heavily reliant on word alignment. The Generative word alignment models, such as the IBM Models (Brown et al., 1993) and HMM (Vogel et al., 1996), are popular methods for automatically aligning bilingual texts, but are restricted to represent one-to-many correspondence of each word. To resolve this weakness, various symmetrization methods are proposed. Och and Ney (2003) and Koehn et al. (2003) propose various heuristic methods to combine two directional models to represent many-to-many relationships. As an alternative to heuristic methods, filtering methods employ a threshold to control the trade-off between precision and recall based on a score estimated from the posterior probabilities from two direction</context>
</contexts>
<marker>Brown, Pietra, Pietra, Mercer, 1993</marker>
<rawString>Peter F Brown, Vincent J Della Pietra, Stephen A Della Pietra, and Robert L Mercer. 1993. The mathematics of statistical machine translation: Parameter estimation. Computational linguistics, 19(2):263–311.</rawString>
</citation>
<citation valid="true">
<authors>
<author>Yin-Wen Chang</author>
<author>Alexander M Rush</author>
<author>John DeNero</author>
<author>Michael Collins</author>
</authors>
<title>A constrained viterbi relaxation for bidirectional word alignment.</title>
<date>2014</date>
<booktitle>In Proceedings of the 52nd Annual Meeting of the Association for Computational Linguistics (Volume 1: Long Papers),</booktitle>
<pages>1481--1490</pages>
<publisher>Association</publisher>
<institution>for Computational Linguistics.</institution>
<location>Baltimore, Maryland,</location>
<contexts>
<context position="2926" citStr="Chang et al. (2014)" startWordPosition="424" endWordPosition="427"> methods employ a threshold to control the trade-off between precision and recall based on a score estimated from the posterior probabilities from two directional models. Matusov et al. (2004) proposed arithmetic means of two models as a score for the filtering, whereas Liang et al. (2006) reported better results using geometric means. The joint training method (Liang et al., 2006) enforces agreement between two directional models. Posterior regularization (Ganchev et al., 2010) is an alternative agreement method which directly encodes agreement during training. DeNero and Macherey (2011) and Chang et al. (2014) also enforce agreement during decoding. However, these agreement models do not take into account the difference in language pairs, which is crucial for linguistically different language pairs, such as Japanese and English: although content words may be aligned with each other by introducing some agreement constraints, function words are difficult to align. We focus on the posterior regularization framework and improve upon the previous work by proposing new constraint functions that take into account the difference in languages in terms of content words and function words. In particular, we d</context>
</contexts>
<marker>Chang, Rush, DeNero, Collins, 2014</marker>
<rawString>Yin-Wen Chang, Alexander M. Rush, John DeNero, and Michael Collins. 2014. A constrained viterbi relaxation for bidirectional word alignment. In Proceedings of the 52nd Annual Meeting of the Association for Computational Linguistics (Volume 1: Long Papers), pages 1481–1490, Baltimore, Maryland, June. Association for Computational Linguistics.</rawString>
</citation>
<citation valid="true">
<authors>
<author>Colin Cherry</author>
<author>George Foster</author>
</authors>
<title>Batch tuning strategies for statistical machine translation.</title>
<date>2012</date>
<booktitle>In Proceedings of the 2012 Conference of the North American Chapter of the Association for Computational Linguistics: Human Language Technologies,</booktitle>
<pages>427--436</pages>
<publisher>Association</publisher>
<institution>for Computational Linguistics.</institution>
<contexts>
<context position="15273" citStr="Cherry and Foster, 2012" startWordPosition="2608" endWordPosition="2611">nd AER. 4.3 Translation evaluation Next, we performed a translation evaluation, measured by BLEU (Papineni et al., 2002). We compared the grow-diag-final and filtering method (Liang et al., 2006) for creating phrase tables. The threshold for the filtering factor was set to 0.1 which was the best setting in the word alignment experiment in section 4.2 under KFTT. From the English side of the training data, we trained a word using the 5-gram model with SRILM (Stolcke and others, 2002). “Moses” toolkit was used as a decoder (Koehn et al., 2007) and the model parameters were tuned by k-best MIRA (Cherry and Foster, 2012). In order to avoid tuning instability, we evaluated the average of five runs (Hopkins and May, 2011). The results are summarized Table 3: Results of translation evaluation KFTT NTCIR10 GDF Filtered GDF Filtered symmetric 19.06 19.28 28.3 29.71 f2f 19.15 19.17 28.36 29.74 c2c 19.26 19.02 28.36 29.92 f2c 18.91 19.20 28.36 29.67 in Table 3. Our proposed methods achieved large gains in NTCIR10 task with the filtered method, but observed no gain in the KFTT with the filtered method. In NTCIR10 task with GDF, the gain in BLEU was smaller than that of KFTT. We calculate p-values and the difference b</context>
</contexts>
<marker>Cherry, Foster, 2012</marker>
<rawString>Colin Cherry and George Foster. 2012. Batch tuning strategies for statistical machine translation. In Proceedings of the 2012 Conference of the North American Chapter of the Association for Computational Linguistics: Human Language Technologies, pages 427–436. Association for Computational Linguistics.</rawString>
</citation>
<citation valid="true">
<authors>
<author>John DeNero</author>
<author>Klaus Macherey</author>
</authors>
<title>Modelbased aligner combination using dual decomposition.</title>
<date>2011</date>
<booktitle>In Proceedings of the 49th Annual Meeting of the Association for Computational Linguistics: Human Language Technologies,</booktitle>
<pages>420--429</pages>
<publisher>Association</publisher>
<institution>for Computational Linguistics.</institution>
<location>Portland, Oregon, USA,</location>
<contexts>
<context position="2902" citStr="DeNero and Macherey (2011)" startWordPosition="418" endWordPosition="422">to heuristic methods, filtering methods employ a threshold to control the trade-off between precision and recall based on a score estimated from the posterior probabilities from two directional models. Matusov et al. (2004) proposed arithmetic means of two models as a score for the filtering, whereas Liang et al. (2006) reported better results using geometric means. The joint training method (Liang et al., 2006) enforces agreement between two directional models. Posterior regularization (Ganchev et al., 2010) is an alternative agreement method which directly encodes agreement during training. DeNero and Macherey (2011) and Chang et al. (2014) also enforce agreement during decoding. However, these agreement models do not take into account the difference in language pairs, which is crucial for linguistically different language pairs, such as Japanese and English: although content words may be aligned with each other by introducing some agreement constraints, function words are difficult to align. We focus on the posterior regularization framework and improve upon the previous work by proposing new constraint functions that take into account the difference in languages in terms of content words and function wo</context>
</contexts>
<marker>DeNero, Macherey, 2011</marker>
<rawString>John DeNero and Klaus Macherey. 2011. Modelbased aligner combination using dual decomposition. In Proceedings of the 49th Annual Meeting of the Association for Computational Linguistics: Human Language Technologies, pages 420–429, Portland, Oregon, USA, June. Association for Computational Linguistics.</rawString>
</citation>
<citation valid="true">
<authors>
<author>Kuzman Ganchev</author>
<author>Jo˜ao V Grac¸a</author>
<author>Ben Taskar</author>
</authors>
<title>Better alignments = better translations?</title>
<date>2008</date>
<booktitle>In Proceedings of ACL-08: HLT,</booktitle>
<pages>986--993</pages>
<publisher>Association</publisher>
<institution>for Computational Linguistics.</institution>
<location>Columbus, Ohio,</location>
<marker>Ganchev, Grac¸a, Taskar, 2008</marker>
<rawString>Kuzman Ganchev, Jo˜ao V. Grac¸a, and Ben Taskar. 2008. Better alignments = better translations? In Proceedings of ACL-08: HLT, pages 986–993, Columbus, Ohio, June. Association for Computational Linguistics.</rawString>
</citation>
<citation valid="true">
<authors>
<author>Kuzman Ganchev</author>
<author>Joao Grac¸a</author>
<author>Jennifer Gillenwater</author>
<author>Ben Taskar</author>
</authors>
<title>Posterior regularization for structured latent variable models.</title>
<date>2010</date>
<journal>The Journal of Machine Learning Research,</journal>
<pages>99--2001</pages>
<marker>Ganchev, Grac¸a, Gillenwater, Taskar, 2010</marker>
<rawString>Kuzman Ganchev, Joao Grac¸a, Jennifer Gillenwater, and Ben Taskar. 2010. Posterior regularization for structured latent variable models. The Journal of Machine Learning Research, 99:2001–2049.</rawString>
</citation>
<citation valid="true">
<authors>
<author>Isao Goto</author>
<author>Ka Po Chow</author>
<author>Bin Lu</author>
<author>Eiichiro Sumita</author>
<author>Benjamin K Tsou</author>
</authors>
<title>Overview of the patent machine translation task at the ntcir-10 workshop.</title>
<date>2013</date>
<booktitle>In Proceedings of the 10th NTCIR Workshop Meeting on Evaluation of Information Access Technologies: Information Retrieval, Question Answering and Cross-Lingual Information Access, NTCIR-10.</booktitle>
<contexts>
<context position="11107" citStr="Goto et al., 2013" startWordPosition="1929" endWordPosition="1932">t is fired, we fall back to Eq (3) for each word pair. 4 Experiment 4.1 Experimental Setup The data sets used in our experiments are the French-English Hansard Corpus, and two data sets for Japanese-English tasks: the Kyoto free translation task (KFTT) and NTCIR10. The Hansard Corpus consists of parallel texts drawn from official records of the proceedings of the Canadian Parliament. The KFTT (Neubig, 2011) is derived from Japanese Wikipedia articles related to Kyoto, which is professionally translated into English. NTCIR10 comes from patent data employed in a machine translation shared task (Goto et al., 2013). The statistics of these data are presented in Table 1. Sentences of over 40 words on both source and target sides are removed for training alignment models. We used a word alignment toolkit cicada 2 for training the IBM Model 4 with our proposed methods. Training is bootstrapped from IBM Model 1, followed by HMM and IBM Model 4. When generating the final bidirectional word alignment, we use a grow-diag-final heuristic for the Japanese-English tasks and an intersection heuristic in the French-English task, judged by preliminary studies. Following Bisazza and Federico (2012), we automatically </context>
</contexts>
<marker>Goto, Chow, Lu, Sumita, Tsou, 2013</marker>
<rawString>Isao Goto, Ka Po Chow, Bin Lu, Eiichiro Sumita, and Benjamin K Tsou. 2013. Overview of the patent machine translation task at the ntcir-10 workshop. In Proceedings of the 10th NTCIR Workshop Meeting on Evaluation of Information Access Technologies: Information Retrieval, Question Answering and Cross-Lingual Information Access, NTCIR-10.</rawString>
</citation>
<citation valid="true">
<authors>
<author>Mark Hopkins</author>
<author>Jonathan May</author>
</authors>
<title>Tuning as ranking.</title>
<date>2011</date>
<booktitle>In Proceedings of the 2011 Conference on Empirical Methods in Natural Language Processing,</booktitle>
<pages>1352--1362</pages>
<publisher>Association</publisher>
<institution>for Computational Linguistics.</institution>
<location>Edinburgh, Scotland, UK.,</location>
<contexts>
<context position="15374" citStr="Hopkins and May, 2011" startWordPosition="2626" endWordPosition="2630">eni et al., 2002). We compared the grow-diag-final and filtering method (Liang et al., 2006) for creating phrase tables. The threshold for the filtering factor was set to 0.1 which was the best setting in the word alignment experiment in section 4.2 under KFTT. From the English side of the training data, we trained a word using the 5-gram model with SRILM (Stolcke and others, 2002). “Moses” toolkit was used as a decoder (Koehn et al., 2007) and the model parameters were tuned by k-best MIRA (Cherry and Foster, 2012). In order to avoid tuning instability, we evaluated the average of five runs (Hopkins and May, 2011). The results are summarized Table 3: Results of translation evaluation KFTT NTCIR10 GDF Filtered GDF Filtered symmetric 19.06 19.28 28.3 29.71 f2f 19.15 19.17 28.36 29.74 c2c 19.26 19.02 28.36 29.92 f2c 18.91 19.20 28.36 29.67 in Table 3. Our proposed methods achieved large gains in NTCIR10 task with the filtered method, but observed no gain in the KFTT with the filtered method. In NTCIR10 task with GDF, the gain in BLEU was smaller than that of KFTT. We calculate p-values and the difference between symmetric and c2c (the most effective proposed constraint) are lower than 0.05 in kftt with GD</context>
</contexts>
<marker>Hopkins, May, 2011</marker>
<rawString>Mark Hopkins and Jonathan May. 2011. Tuning as ranking. In Proceedings of the 2011 Conference on Empirical Methods in Natural Language Processing, pages 1352–1362, Edinburgh, Scotland, UK., July. Association for Computational Linguistics.</rawString>
</citation>
<citation valid="true">
<authors>
<author>Philipp Koehn</author>
<author>Franz Josef Och</author>
<author>Daniel Marcu</author>
</authors>
<title>Statistical phrase-based translation.</title>
<date>2003</date>
<booktitle>In Proceedings of the 2003 Conference of the North</booktitle>
<contexts>
<context position="1499" citStr="Koehn et al., 2003" startWordPosition="203" endWordPosition="206">ith each other during training, and propose new constraints that can take into account the difference between function words and content words. Experimental results on French-to-English and Japanese-to-English alignment tasks show statistically significant gains over the previous posterior regularization baseline. We also observed gains in Japanese-toEnglish translation tasks, which prove the effectiveness of our methods under grammatically different language pairs. 1 Introduction Word alignment is an important component in statistical machine translation (SMT). For instance phrase-based SMT (Koehn et al., 2003) is based on the concept of phrase pairs that are automatically extracted from bilingual data and rely on word alignment annotation. Similarly, the model for hierarchical phrase-based SMT is built from exhaustively extracted phrases that are, in turn, heavily reliant on word alignment. The Generative word alignment models, such as the IBM Models (Brown et al., 1993) and HMM (Vogel et al., 1996), are popular methods for automatically aligning bilingual texts, but are restricted to represent one-to-many correspondence of each word. To resolve this weakness, various symmetrization methods are pro</context>
</contexts>
<marker>Koehn, Och, Marcu, 2003</marker>
<rawString>Philipp Koehn, Franz Josef Och, and Daniel Marcu. 2003. Statistical phrase-based translation. In Proceedings of the 2003 Conference of the North</rawString>
</citation>
<citation valid="false">
<journal>American Chapter of the Association for Computational Linguistics on Human Language TechnologyVolume</journal>
<volume>1</volume>
<pages>48--54</pages>
<publisher>Association</publisher>
<institution>for Computational Linguistics.</institution>
<marker></marker>
<rawString>American Chapter of the Association for Computational Linguistics on Human Language TechnologyVolume 1, pages 48–54. Association for Computational Linguistics.</rawString>
</citation>
<citation valid="true">
<authors>
<author>Philipp Koehn</author>
<author>Hieu Hoang</author>
<author>Alexandra Birch</author>
<author>Chris Callison-Burch</author>
<author>Marcello Federico</author>
<author>Nicola Bertoldi</author>
<author>Brooke Cowan</author>
<author>Wade Shen</author>
<author>Christine Moran</author>
<author>Richard Zens</author>
</authors>
<title>Moses: Open source toolkit for statistical machine translation.</title>
<date>2007</date>
<booktitle>In Proceedings of the 45th Annual Meeting of the ACL on Interactive Poster and Demonstration Sessions,</booktitle>
<pages>177--180</pages>
<publisher>Association</publisher>
<institution>for Computational Linguistics.</institution>
<contexts>
<context position="15196" citStr="Koehn et al., 2007" startWordPosition="2595" endWordPosition="2598">methods are superior to the baseline in terms of both precision-recall and AER. 4.3 Translation evaluation Next, we performed a translation evaluation, measured by BLEU (Papineni et al., 2002). We compared the grow-diag-final and filtering method (Liang et al., 2006) for creating phrase tables. The threshold for the filtering factor was set to 0.1 which was the best setting in the word alignment experiment in section 4.2 under KFTT. From the English side of the training data, we trained a word using the 5-gram model with SRILM (Stolcke and others, 2002). “Moses” toolkit was used as a decoder (Koehn et al., 2007) and the model parameters were tuned by k-best MIRA (Cherry and Foster, 2012). In order to avoid tuning instability, we evaluated the average of five runs (Hopkins and May, 2011). The results are summarized Table 3: Results of translation evaluation KFTT NTCIR10 GDF Filtered GDF Filtered symmetric 19.06 19.28 28.3 29.71 f2f 19.15 19.17 28.36 29.74 c2c 19.26 19.02 28.36 29.92 f2c 18.91 19.20 28.36 29.67 in Table 3. Our proposed methods achieved large gains in NTCIR10 task with the filtered method, but observed no gain in the KFTT with the filtered method. In NTCIR10 task with GDF, the gain in B</context>
</contexts>
<marker>Koehn, Hoang, Birch, Callison-Burch, Federico, Bertoldi, Cowan, Shen, Moran, Zens, 2007</marker>
<rawString>Philipp Koehn, Hieu Hoang, Alexandra Birch, Chris Callison-Burch, Marcello Federico, Nicola Bertoldi, Brooke Cowan, Wade Shen, Christine Moran, Richard Zens, et al. 2007. Moses: Open source toolkit for statistical machine translation. In Proceedings of the 45th Annual Meeting of the ACL on Interactive Poster and Demonstration Sessions, pages 177–180. Association for Computational Linguistics.</rawString>
</citation>
<citation valid="true">
<authors>
<author>Percy Liang</author>
<author>Ben Taskar</author>
<author>Dan Klein</author>
</authors>
<title>Alignment by agreement.</title>
<date>2006</date>
<booktitle>In Proceedings of the Human Language Technology Conference of the NAACL, Main Conference,</booktitle>
<pages>104--111</pages>
<publisher>Association</publisher>
<institution>for Computational Linguistics.</institution>
<location>New York City, USA,</location>
<contexts>
<context position="2597" citStr="Liang et al. (2006)" startWordPosition="377" endWordPosition="380">ed to represent one-to-many correspondence of each word. To resolve this weakness, various symmetrization methods are proposed. Och and Ney (2003) and Koehn et al. (2003) propose various heuristic methods to combine two directional models to represent many-to-many relationships. As an alternative to heuristic methods, filtering methods employ a threshold to control the trade-off between precision and recall based on a score estimated from the posterior probabilities from two directional models. Matusov et al. (2004) proposed arithmetic means of two models as a score for the filtering, whereas Liang et al. (2006) reported better results using geometric means. The joint training method (Liang et al., 2006) enforces agreement between two directional models. Posterior regularization (Ganchev et al., 2010) is an alternative agreement method which directly encodes agreement during training. DeNero and Macherey (2011) and Chang et al. (2014) also enforce agreement during decoding. However, these agreement models do not take into account the difference in language pairs, which is crucial for linguistically different language pairs, such as Japanese and English: although content words may be aligned with each</context>
<context position="14388" citStr="Liang et al., 2006" startWordPosition="2458" endWordPosition="2461"> 2010) shown in Table 2. The numbers in bold and in italics indicate the best score and the second best score, respectively. The differences between f2f,f2c and baseline in KFTT are statistically significant at p &lt; 0.05 using the signtest, but in hansard corpus, there exist no significant differences between the baseline and the proposed methods. In terms of F-measure, it is clear that the f2f method is the most effective method in KFTT, and both f2f and f2c methods exceed the original posterior regularized model of Ganchev et al. (2010). We also compared these methods with filtering methods (Liang et al., 2006), in addition to heuristic methods. We plot precision/recall curves and AER by varying the threshold between 0.1 and 0.9 with 0.1 increments. From Figures, it can be seen that our proposed methods are superior to the baseline in terms of both precision-recall and AER. 4.3 Translation evaluation Next, we performed a translation evaluation, measured by BLEU (Papineni et al., 2002). We compared the grow-diag-final and filtering method (Liang et al., 2006) for creating phrase tables. The threshold for the filtering factor was set to 0.1 which was the best setting in the word alignment experiment i</context>
</contexts>
<marker>Liang, Taskar, Klein, 2006</marker>
<rawString>Percy Liang, Ben Taskar, and Dan Klein. 2006. Alignment by agreement. In Proceedings of the Human Language Technology Conference of the NAACL, Main Conference, pages 104–111, New York City, USA, June. Association for Computational Linguistics.</rawString>
</citation>
<citation valid="true">
<authors>
<author>E Matusov</author>
<author>R Zens</author>
<author>H Ney</author>
</authors>
<title>Symmetric Word Alignments for Statistical Machine Translation.</title>
<date>2004</date>
<booktitle>In Proceedings of COLING 2004,</booktitle>
<pages>219--225</pages>
<location>Geneva, Switzerland,</location>
<contexts>
<context position="2499" citStr="Matusov et al. (2004)" startWordPosition="359" endWordPosition="362">ogel et al., 1996), are popular methods for automatically aligning bilingual texts, but are restricted to represent one-to-many correspondence of each word. To resolve this weakness, various symmetrization methods are proposed. Och and Ney (2003) and Koehn et al. (2003) propose various heuristic methods to combine two directional models to represent many-to-many relationships. As an alternative to heuristic methods, filtering methods employ a threshold to control the trade-off between precision and recall based on a score estimated from the posterior probabilities from two directional models. Matusov et al. (2004) proposed arithmetic means of two models as a score for the filtering, whereas Liang et al. (2006) reported better results using geometric means. The joint training method (Liang et al., 2006) enforces agreement between two directional models. Posterior regularization (Ganchev et al., 2010) is an alternative agreement method which directly encodes agreement during training. DeNero and Macherey (2011) and Chang et al. (2014) also enforce agreement during decoding. However, these agreement models do not take into account the difference in language pairs, which is crucial for linguistically diffe</context>
</contexts>
<marker>Matusov, Zens, Ney, 2004</marker>
<rawString>E. Matusov, R. Zens, and H. Ney. 2004. Symmetric Word Alignments for Statistical Machine Translation. In Proceedings of COLING 2004, pages 219– 225, Geneva, Switzerland, August 23–27.</rawString>
</citation>
<citation valid="true">
<authors>
<author>Graham Neubig</author>
</authors>
<title>The Kyoto free translation task.</title>
<date>2011</date>
<note>http://www.phontron.com/kftt.</note>
<contexts>
<context position="10899" citStr="Neubig, 2011" startWordPosition="1898" endWordPosition="1899">ewise, the function to content word matching function f2c is defined by considering the matching of content words and function words in two languages. As noted in the mismatch function, when no constraint is fired, we fall back to Eq (3) for each word pair. 4 Experiment 4.1 Experimental Setup The data sets used in our experiments are the French-English Hansard Corpus, and two data sets for Japanese-English tasks: the Kyoto free translation task (KFTT) and NTCIR10. The Hansard Corpus consists of parallel texts drawn from official records of the proceedings of the Canadian Parliament. The KFTT (Neubig, 2011) is derived from Japanese Wikipedia articles related to Kyoto, which is professionally translated into English. NTCIR10 comes from patent data employed in a machine translation shared task (Goto et al., 2013). The statistics of these data are presented in Table 1. Sentences of over 40 words on both source and target sides are removed for training alignment models. We used a word alignment toolkit cicada 2 for training the IBM Model 4 with our proposed methods. Training is bootstrapped from IBM Model 1, followed by HMM and IBM Model 4. When generating the final bidirectional word alignment, we </context>
</contexts>
<marker>Neubig, 2011</marker>
<rawString>Graham Neubig. 2011. The Kyoto free translation task. http://www.phontron.com/kftt.</rawString>
</citation>
<citation valid="true">
<authors>
<author>Franz Josef Och</author>
<author>Hermann Ney</author>
</authors>
<title>A systematic comparison of various statistical alignment models.</title>
<date>2003</date>
<journal>Computational linguistics,</journal>
<pages>29--1</pages>
<contexts>
<context position="2124" citStr="Och and Ney (2003)" startWordPosition="301" endWordPosition="304">ed on the concept of phrase pairs that are automatically extracted from bilingual data and rely on word alignment annotation. Similarly, the model for hierarchical phrase-based SMT is built from exhaustively extracted phrases that are, in turn, heavily reliant on word alignment. The Generative word alignment models, such as the IBM Models (Brown et al., 1993) and HMM (Vogel et al., 1996), are popular methods for automatically aligning bilingual texts, but are restricted to represent one-to-many correspondence of each word. To resolve this weakness, various symmetrization methods are proposed. Och and Ney (2003) and Koehn et al. (2003) propose various heuristic methods to combine two directional models to represent many-to-many relationships. As an alternative to heuristic methods, filtering methods employ a threshold to control the trade-off between precision and recall based on a score estimated from the posterior probabilities from two directional models. Matusov et al. (2004) proposed arithmetic means of two models as a score for the filtering, whereas Liang et al. (2006) reported better results using geometric means. The joint training method (Liang et al., 2006) enforces agreement between two d</context>
<context position="13488" citStr="Och and Ney, 2003" startWordPosition="2307" endWordPosition="2310">.4K 12.7K Figure 1: Precision Recall graph in Hansard Figure 2: Precision Recall graph in KFTT French-English Figure 3: AER in Hansard French-English Figure 4: AER in KFTT 156 Table 2: Results of word alignment evaluation with the heuristics-based method (GDF) KFTT Hansard (French-English) method precision recall AER F precision recall AER F symmetric 0.4595 0.5942 48.18 0.5182 0.7029 0.8816 7.29 0.7822 f2f 0.4633 0.5997 47.73 0.5227 0.7042 0.8851 7.29 0.7844 c2c 0.4606 0.5964 48.02 0.5198 0.7001 0.8816 7.34 0.7804 f2c 0.4630 0.5998 47.74 0.5226 0.7037 0.8871 7.10 0.7848 by AER and F-measure (Och and Ney, 2003). Since there exists no distinction for sure-possible alignments in the KFTT data, we use only sure alignment for our evaluation, both for the FrenchEnglish and the Japanese-English tasks. Table 2 summarizes our results. The baseline method is symmetric constraint (Ganchev et al., 2010) shown in Table 2. The numbers in bold and in italics indicate the best score and the second best score, respectively. The differences between f2f,f2c and baseline in KFTT are statistically significant at p &lt; 0.05 using the signtest, but in hansard corpus, there exist no significant differences between the basel</context>
</contexts>
<marker>Och, Ney, 2003</marker>
<rawString>Franz Josef Och and Hermann Ney. 2003. A systematic comparison of various statistical alignment models. Computational linguistics, 29(1):19–51.</rawString>
</citation>
<citation valid="true">
<authors>
<author>Kishore Papineni</author>
<author>Salim Roukos</author>
<author>Todd Ward</author>
<author>WeiJing Zhu</author>
</authors>
<title>Bleu: a method for automatic evaluation of machine translation.</title>
<date>2002</date>
<booktitle>In Proceedings of the 40th annual meeting on association for computational linguistics,</booktitle>
<pages>311--318</pages>
<publisher>Association</publisher>
<institution>for Computational Linguistics.</institution>
<contexts>
<context position="14769" citStr="Papineni et al., 2002" startWordPosition="2520" endWordPosition="2523">clear that the f2f method is the most effective method in KFTT, and both f2f and f2c methods exceed the original posterior regularized model of Ganchev et al. (2010). We also compared these methods with filtering methods (Liang et al., 2006), in addition to heuristic methods. We plot precision/recall curves and AER by varying the threshold between 0.1 and 0.9 with 0.1 increments. From Figures, it can be seen that our proposed methods are superior to the baseline in terms of both precision-recall and AER. 4.3 Translation evaluation Next, we performed a translation evaluation, measured by BLEU (Papineni et al., 2002). We compared the grow-diag-final and filtering method (Liang et al., 2006) for creating phrase tables. The threshold for the filtering factor was set to 0.1 which was the best setting in the word alignment experiment in section 4.2 under KFTT. From the English side of the training data, we trained a word using the 5-gram model with SRILM (Stolcke and others, 2002). “Moses” toolkit was used as a decoder (Koehn et al., 2007) and the model parameters were tuned by k-best MIRA (Cherry and Foster, 2012). In order to avoid tuning instability, we evaluated the average of five runs (Hopkins and May, </context>
</contexts>
<marker>Papineni, Roukos, Ward, Zhu, 2002</marker>
<rawString>Kishore Papineni, Salim Roukos, Todd Ward, and WeiJing Zhu. 2002. Bleu: a method for automatic evaluation of machine translation. In Proceedings of the 40th annual meeting on association for computational linguistics, pages 311–318. Association for Computational Linguistics.</rawString>
</citation>
<citation valid="true">
<authors>
<author>Hendra Setiawan</author>
<author>Min-Yen Kan</author>
<author>Haizhou Li</author>
</authors>
<title>Ordering phrases with function words.</title>
<date>2007</date>
<booktitle>In Proceedings of the 45th annual meeting on association for computational linguistics,</booktitle>
<pages>712--719</pages>
<publisher>Association</publisher>
<institution>for Computational Linguistics.</institution>
<contexts>
<context position="3644" citStr="Setiawan et al. (2007)" startWordPosition="535" endWordPosition="538">t the difference in language pairs, which is crucial for linguistically different language pairs, such as Japanese and English: although content words may be aligned with each other by introducing some agreement constraints, function words are difficult to align. We focus on the posterior regularization framework and improve upon the previous work by proposing new constraint functions that take into account the difference in languages in terms of content words and function words. In particular, we differentiate between content words and function words by frequency in bilingual data, following Setiawan et al. (2007). Experimental results show that the proposed methods achieved better alignment qualities on the French-English Hansard data and the JapaneseEnglish Kyoto free translation task (KFTT) measured by AER and F-measure. In translation evaluations, we achieved statistically significant gains 153 Proceedings of the 2014 Conference on Empirical Methods in Natural Language Processing (EMNLP), pages 153–158, October 25-29, 2014, Doha, Qatar. c�2014 Association for Computational Linguistics in BLEU scores in the NTCIR10. 2 Statistical word alignment with posterior regularization framework Given a bilingu</context>
<context position="7457" citStr="Setiawan et al. (2007)" startWordPosition="1217" endWordPosition="1220">content words may be easily aligned oneto-one, but function words are not always aligned together. In addition, Japanese is a pro-drop language which can easily violate the symmetric constraint when proper nouns in the English side have to be aligned with a “null” word. In addition, low frequency words may cause unreliable estimates for adjusting the weighing parameters A. In order to solve the problem, we improve Ganchev’s symmetric constraint so that it can consider the difference between content words and function words in each language. In particular, we follow the frequency-based idea of Setiawan et al. (2007) that discriminates content words and function words by their frequencies. We propose constraint features that take into account the difference between content words and function words, determined by a frequency threshold. 3.1 Mismatching constraint First, we propose a mismatching constraint that penalizes word alignment between content words and function words by decreasing the corresponding posterior probabilities. rIp(xt, →− y |xs) = i φi,j(x, y) = I (3) →−p θ(−→y , x)exp{−λ · φ(x, y)}, →−p θ(−→y , x)exp{−λ · φ(x, y)}, ←−p θ(←−y , x)exp{−λ · φ(x, y)}, ←−p θ(←−y , x)exp{−λ · φ(x, y)}, 154 Th</context>
</contexts>
<marker>Setiawan, Kan, Li, 2007</marker>
<rawString>Hendra Setiawan, Min-Yen Kan, and Haizhou Li. 2007. Ordering phrases with function words. In Proceedings of the 45th annual meeting on association for computational linguistics, pages 712–719. Association for Computational Linguistics.</rawString>
</citation>
<citation valid="true">
<authors>
<author>Andreas Stolcke</author>
</authors>
<title>Srilm-an extensible language modeling toolkit.</title>
<date>2002</date>
<booktitle>In INTERSPEECH.</booktitle>
<marker>Stolcke, 2002</marker>
<rawString>Andreas Stolcke et al. 2002. Srilm-an extensible language modeling toolkit. In INTERSPEECH.</rawString>
</citation>
<citation valid="true">
<authors>
<author>Stephan Vogel</author>
<author>Hermann Ney</author>
<author>Christoph Tillmann</author>
</authors>
<title>Hmm-based word alignment in statistical translation.</title>
<date>1996</date>
<booktitle>In Proceedings of the 16th conference on Computational linguistics-Volume 2,</booktitle>
<pages>836--841</pages>
<publisher>Association</publisher>
<institution>for Computational Linguistics.</institution>
<contexts>
<context position="1896" citStr="Vogel et al., 1996" startWordPosition="267" endWordPosition="270">ffectiveness of our methods under grammatically different language pairs. 1 Introduction Word alignment is an important component in statistical machine translation (SMT). For instance phrase-based SMT (Koehn et al., 2003) is based on the concept of phrase pairs that are automatically extracted from bilingual data and rely on word alignment annotation. Similarly, the model for hierarchical phrase-based SMT is built from exhaustively extracted phrases that are, in turn, heavily reliant on word alignment. The Generative word alignment models, such as the IBM Models (Brown et al., 1993) and HMM (Vogel et al., 1996), are popular methods for automatically aligning bilingual texts, but are restricted to represent one-to-many correspondence of each word. To resolve this weakness, various symmetrization methods are proposed. Och and Ney (2003) and Koehn et al. (2003) propose various heuristic methods to combine two directional models to represent many-to-many relationships. As an alternative to heuristic methods, filtering methods employ a threshold to control the trade-off between precision and recall based on a score estimated from the posterior probabilities from two directional models. Matusov et al. (20</context>
</contexts>
<marker>Vogel, Ney, Tillmann, 1996</marker>
<rawString>Stephan Vogel, Hermann Ney, and Christoph Tillmann. 1996. Hmm-based word alignment in statistical translation. In Proceedings of the 16th conference on Computational linguistics-Volume 2, pages 836– 841. Association for Computational Linguistics.</rawString>
</citation>
</citationList>
</algorithm>
</algorithms>
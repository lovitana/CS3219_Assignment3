<?xml version="1.0" encoding="UTF-8"?>
<algorithms version="110505">
<algorithm name="SectLabel" version="110505">
<variant no="0" confidence="0.000005">
<title confidence="0.999357">
Improving Pivot-Based Statistical Machine Translation by Pivoting
the Co-occurrence Count of Phrase Pairs
</title>
<author confidence="0.9940745">
Xiaoning Zhu1*, Zhongjun He2, Hua Wu2, Conghui Zhu1,
Haifeng Wang2, and Tiejun Zhao1
</author>
<affiliation confidence="0.998424">
Harbin Institute of Technology, Harbin, China1
</affiliation>
<email confidence="0.961755">
{xnzhu,chzhu,tjzhao}@mtlab.hit.edu.cn
</email>
<address confidence="0.66616">
Baidu Inc., Beijing, China2
</address>
<email confidence="0.996955">
{hezhongjun,wu_hua,wanghaifeng}@baidu.com
</email>
<sectionHeader confidence="0.99859" genericHeader="abstract">
Abstract
</sectionHeader>
<bodyText confidence="0.9999614375">
To overcome the scarceness of bilingual
corpora for some language pairs in ma-
chine translation, pivot-based SMT uses
pivot language as a &amp;quot;bridge&amp;quot; to generate
source-target translation from source-
pivot and pivot-target translation. One of
the key issues is to estimate the probabili-
ties for the generated phrase pairs. In this
paper, we present a novel approach to
calculate the translation probability by
pivoting the co-occurrence count of
source-pivot and pivot-target phrase pairs.
Experimental results on Europarl data
and web data show that our method leads
to significant improvements over the
baseline systems.
</bodyText>
<sectionHeader confidence="0.999514" genericHeader="keywords">
1 Introduction
</sectionHeader>
<bodyText confidence="0.987795218181818">
Statistical Machine Translation (SMT) relies on
large bilingual parallel data to produce high qual-
ity translation results. Unfortunately, for some
language pairs, large bilingual corpora are not
readily available. To alleviate the parallel data
scarceness, a conventional solution is to intro-
duce a “bridge” language (named pivot language)
to connect the source and target language (de
Gispert and Marino, 2006; Utiyama and Isahara,
2007; Wu and Wang, 2007; Bertoldi et al., 2008;
Paul et al., 2011; El Kholy et al., 2013; Zahabi et
al., 2013), where there are large amounts of
source-pivot and pivot-target parallel corpora.
Among various pivot-based approaches, the
triangulation method (Cohn and Lapata, 2007;
Wu and Wang, 2007) is a representative work in
* This work was done when the first author was visiting Baidu.
pivot-based machine translation. The approach
proposes to build a source-target phrase table by
merging the source-pivot and pivot-target phrase
table. One of the key issues in this method is to
estimate the translation probabilities for the gen-
erated source-target phrase pairs. Conventionally,
the probabilities are estimated by multiplying the
posterior probabilities of source-pivot and pivot-
target phrase pairs. However, it has been shown
that the generated probabilities are not accurate
enough (Cui et al., 2013). One possible reason
may lie in the non-uniformity of the probability
space. Through Figure 1. (a), we can see that the
probability distributions of source-pivot and piv-
ot-target language are calculated separately, and
the source-target probability distributions are
induced from the source-pivot and pivot-target
probability distributions. Because of the absence
of the pivot language (e.g., p2 is in source-pivot
probability space but not in pivot-target one), the
induced source-target probability distribution is
not complete, which will result in inaccurate
probabilities.
To solve this problem, we propose a novel ap-
proach that utilizes the co-occurrence count of
source-target phrase pairs to estimate phrase
translation probabilities more precisely. Different
from the triangulation method, which merges the
source-pivot and pivot-target phrase pairs after
training the translation model, we propose to
merge the source-pivot and pivot-target phrase
pairs immediately after the phrase extraction step,
and estimate the co-occurrence count of the
source-pivot-target phrase pairs. Finally, we
compute the translation probabilities according
to the estimated co-occurrence counts, using the
standard training method in phrase-based SMT
(Koehn et al., 2003). As Figure 1. (b) shows, the
</bodyText>
<page confidence="0.92904">
1665
</page>
<note confidence="0.949794">
Proceedings of the 2014 Conference on Empirical Methods in Natural Language Processing (EMNLP), pages 1665–1675,
October 25-29, 2014, Doha, Qatar. c�2014 Association for Computational Linguistics
</note>
<figure confidence="0.871429">
(a) the triangulation method (b) the co-occurrence count method
</figure>
<figureCaption confidence="0.995389">
Figure 1: An example of probability space evolution in pivot translation.
</figureCaption>
<figure confidence="0.772236">
(a) the triangulation method (b) the co-occurrence count method (c) the mixed model
</figure>
<figureCaption confidence="0.99749675">
Figure 2: Framework of the triangulation method, the co-occurrence count method and the mixed
model. The shaded box in (b) denotes difference between the co-occurrence count method and the
triangulation method. The shaded box in (c) denotes the difference between the interpolation model
and the mixed model.
</figureCaption>
<figure confidence="0.997570554054054">
Interpolate
Train
Interpolate
Standard
ST corpus
Large SP
corpus
Large PT
corpus
Standard
ST corpus
Large SP
corpus
Large PT
corpus
Standard
ST corpus
Large SP
corpus
Large PT
corpus
Phrase Extraction Phrase Extraction
Phrase Extraction
Train Train
Merge
Merge
ST phrase
pairs
SP model PT model
ST phrase
pairs
ST phrase
pairs
ST phrase
pairs
ST phrase
pairs
Train
Merge
Train
Train
Mix
ST direct
model
ST pivot
model
ST direct
model
ST pivot
model
ST mixed
pairs
ST interpolated
model
ST mixed
model
Phrase ExtractionPhrase Extraction
Phrase Extraction Phrase Extraction
SP phrase
pairs
PT phrase
pairs
Phrase Extraction
SP phrase
pairs
PT phrase
pairs
Phrase Extraction
SP phrase
pairs
PT phrase
pairs
ST interpolated
model
</figure>
<bodyText confidence="0.99915775">
source-target probability distributions are calcu-
lated in a complete probability space. Thus, it
will be more accurate than the traditional trian-
gulation method. Figure 2. (a) and (b) show the
difference between the triangulation method and
our co-occurrence count method.
Furthermore, it is common that a small stand-
ard bilingual corpus can be available between the
source and target language. The direct translation
model trained with the standard bilingual corpus
exceeds in translation performance, but its weak-
ness lies in low phrase coverage. However, the
pivot model has characteristics characters. Thus,
it is important to combine the direct and pivot
translation model to compensate mutually and
further improve the translation performance. To
deal with this problem, we propose a mixed
model by merging the phrase pairs extracted by
pivot-based method and the phrase pairs extract-
ed from the standard bilingual corpus. Note that,
this is different from the conventional interpola-
tion method, which interpolates the direct and
pivot translation model. See Figure 2. (b) and (c)
for further illustration.
</bodyText>
<page confidence="0.985636">
1666
</page>
<bodyText confidence="0.999868857142857">
The remainder of this paper is organized as
follows. In Section 2, we describe the related
work. We introduce the co-occurrence count
method in Section 3, and the mixed model in
Section 4. In Section 5 and Section 6, we de-
scribe and analyze the experiments. Section 7
gives a conclusion of the paper.
</bodyText>
<sectionHeader confidence="0.999875" genericHeader="introduction">
2 Related Work
</sectionHeader>
<bodyText confidence="0.999060673913044">
Several methods have been proposed for pivot-
based translation. Typically, they can be classi-
fied into 3 kinds as follows:
Transfer Method: The transfer method
(Utiyama and Isahara, 2007; Wang et al., 2008;
Costa-jussà et al., 2011) connects two translation
systems: a source-pivot MT system and a pivot-
target MT system. Given a source sentence, (1)
the source-pivot MT system translates it into the
pivot language, (2) and the pivot-target MT sys-
tem translates the pivot sentence into the target
sentence. During each step (source to pivot and
pivot to target), multiple translation outputs will
be generated, thus a minimum Bayes-risk system
combination method is often used to select the
optimal sentence (González-Rubio et al., 2011;
Duh et al., 2011). The problem with the transfer
method is that it needs to decode twice. On one
hand, the time cost is doubled; on the other hand,
the translation error of the source-pivot transla-
tion system will be transferred to the pivot-target
translation.
Synthetic Method: It aims to create a synthet-
ic source-target corpus by: (1) translate the pivot
part in source-pivot corpus into target language
with a pivot-target model; (2) translate the pivot
part in pivot-target corpus into source language
with a pivot-source model; (3) combine the
source sentences with translated target sentences
or/and combine the target sentences with trans-
lated source sentences (Utiyama et al., 2008; Wu
and Wang, 2009). However, it is difficult to
build a high quality translation system with a
corpus created by a machine translation system.
Triangulation Method: The triangulation
method obtains source-target phrase table by
merging source-pivot and pivot-target phrase
table entries with identical pivot language
phrases and multiplying corresponding posterior
probabilities (Wu and Wang, 2007; Cohn and
Lapata, 2007), which has been shown to work
better than the other pivot approaches (Utiyama
and Isahara, 2007). A problem of this approach is
that the probability space of the source-target
phrase pairs is non-uniformity due to the mis-
matching of the pivot phrase.
</bodyText>
<sectionHeader confidence="0.972335" genericHeader="method">
3 Our Approach
</sectionHeader>
<bodyText confidence="0.9999715">
In this section, we will introduce our method for
learning a source-target phrase translation model
with a pivot language as a bridge. We extract the
co-occurrence count of phrase pairs for each lan-
guage pair with a source-pivot and a pivot-target
corpus. Then we generate the source-target
phrase pairs with induced co-occurrence infor-
mation. Finally, we compute translation proba-
bilities using the standard phrase-based SMT
training method.
</bodyText>
<subsectionHeader confidence="0.999251">
3.1 Phrase Translation Probabilities
</subsectionHeader>
<bodyText confidence="0.988226625">
Following the standard phrase extraction method
(Koehn et al., 2003), we can extract phrase pairs
ሺݏ̅, ݌̅ሻ and ሺ݌̅, ݐ̅ሻ from the corresponding word-
aligned source-pivot and pivot-target training
corpus, where ݏ̅, ݌̅ and ݐ̅ denotes the phrase in
source, pivot and target language respectively.
Formally, given the co-occurrence count
ܿሺݏ̅, ݌̅ሻ and ܿሺ݌̅, ݐ̅ሻ, we can estimate ܿሺݏ̅, ݐ̅ሻ by
</bodyText>
<equation confidence="0.98107675">
Equation 1:
ܿሺݏ̅, ݐ̅ሻ ൌ ෍ ݃ሺܿሺݏ̅, ݌̅ሻ, ܿሺ݌̅, ݐ̅ሻሻ (1)
̅
௣
</equation>
<bodyText confidence="0.983404083333333">
where ݃ሺ∙ሻ is a function to merge the co-
occurrences count ܿሺݏ̅, ݌̅ሻ and ܿሺ݌̅, ݐ̅ሻ. We pro-
pose four calculation methods for function ݃ሺ∙ሻ.
Given the co-occurrence count ܿሺݏ̅, ݌̅ሻ and
ܿሺ݌̅, ݐ̅ሻ, we first need to induce the co-occurrence
count ܿሺݏ̅, ݌,ഥ ݐ̅ሻ. The ܿሺݏ̅, ݌,ഥ ݐ̅ሻ is counted when
the source phrase, pivot phrase and target phrase
occurred together, thus we can infer that
ܿሺݏ̅, ݌,ഥ ݐ̅ሻ is smaller than ܿሺݏ̅, ݌̅ሻ and ܿሺ݌̅, ݐ̅ሻ. In
this circumstance, we consider that ܿሺݏ̅, ݌,ഥ ݐ̅ሻ is
approximately equal to the minimum value of
ܿሺݏ̅, ݌̅ሻ and ܿሺ݌̅, ݐ̅ሻ, as shown in Equation 2.
</bodyText>
<equation confidence="0.969053333333333">
ܿሺݏ̅, ݌̅, ݐ̅ሻ ൎ ෍ minሺܿሺݏ̅, ݌̅ሻ, ܿሺ݌̅, ݐ̅ሻሻ (2)
̅
௣
</equation>
<bodyText confidence="0.9703065">
Because the co-occurrence count of source-
target phrase pairs needs the existence of pivot
phrase ݌̅ , we intuitively believe that the co-
occurrence count ܿሺݏ̅, ݐ̅ሻ is equal to the co-
occurrence count ܿሺݏ̅, ݌,ഥ ݐ̅ሻ. Under this assump-
tion, we can obtain the co-occurrence count
ܿሺݏ̅, ݐ̅ሻ as shown in Equation 3. Furthermore, to
testify our assumption, we also try the maximum
value (Equation 4) to infer the co-occurrence
count of ሺݏ̅, ݐ̅ሻ phrase pair.
</bodyText>
<page confidence="0.602873">
1667
</page>
<equation confidence="0.8533084">
ܿሺݏ̅, ݐ̅ሻ ൌ ෍ minሺܿሺݏ̅, ݌̅ሻ, ܿሺ݌̅, ݐ̅ሻሻ
ܿሺݏ̅, ݐ̅ሻ ̅ ܿሺ݌̅, ݐ̅ሻሻ
௣
ൌ ෍ maxሺܿሺݏ̅, ݌̅ሻ,
௣̅
</equation>
<bodyText confidence="0.999436222222222">
In addition, if source-pivot and pivot-target
parallel corpus greatly differ in quantities, then
the minimum function would likely just take the
counts from the smaller corpus. To deal with the
problem of the imbalance of the parallel corpora,
we also try the arithmetic mean (Equation 5) and
geometric mean (Equation 6) function to infer
the co-occurrence count of source-target phrase
pairs.
</bodyText>
<equation confidence="0.99387">
ܿሺݏ̅, ݐ̅ሻ ൌ ෍ሺܿሺݏ̅, ݌̅ሻ ൅ ܿሺ݌̅, ݐ̅ሻሻ/2 (5)
̅
௣
ܿሺݏ̅, ݐ̅ሻ ൌ ෍ ඥܿሺݏ̅, ݌̅ሻ ൈ ܿሺ݌̅, ݐ̅ሻ (6)
௣̅
</equation>
<bodyText confidence="0.998332">
When the co-occurrence count of source-target
language is calculated, we can estimate the
phrase translation probabilities with the follow-
ing Equation 7 and Equation 8.
</bodyText>
<equation confidence="0.9604914">
ܿሺݏ̅, ݐ̅ሻ
߶ሺݏ̅|ݐ̅ሻ ൌ (7)
∑௦̅ ܿሺݏ̅, ݐ̅ሻ
߮ሺݐ̅|ݏ̅ሻ ൌ ܿሺݏ̅,̅ݐ̅ሻ (8)
∑௧ ܿሺݏ, ݐ̅ሻ
</equation>
<subsectionHeader confidence="0.998991">
3.2 Lexical Weight
</subsectionHeader>
<bodyText confidence="0.997494666666667">
Given a phrase pair ሺݏ̅, ݐ̅ሻ and a word alignment
a between the source word positions ݅ ൌ 1, ⋯ , ݊
and the target word positions ݆ ൌ 0, ⋯ , ݉, the
lexical weight of phrase pair ሺݏ̅, ݐ̅ሻ can be calcu-
lated by the following Equation 9 (Koehn et al.,
2003).
The lexical translation probability distribution
߱ሺݏ|ݐሻ between source word s and target word t
can be estimated with Equation 10.
</bodyText>
<equation confidence="0.958329">
ܿሺݏ, ݐሻ
߱ሺݏ|ݐሻ ൌ (10)
∑௦ᇲ ܿሺݏᇱ, ݐሻ
</equation>
<bodyText confidence="0.99919525">
To compute the lexical weight for a phrase
pair ሺݏ̅, ݐ̅ሻ generated by ሺݏ̅, ݌̅ሻ and ሺ݌̅, ݐ̅ሻ, we need
the alignment information ܽ, which can be ob-
tained as Equation 11 shows.
</bodyText>
<equation confidence="0.733655">
ܽ ൌ ሼሺݏ, ݐሻ|∃݌: ሺݏ, ݌ሻ ∈ ܽଵ&amp;ሺ݌, ݐሻ ∈ ܽଶሽ (11)
</equation>
<bodyText confidence="0.996525333333333">
where ܽଵ and ܽଶ indicate the word alignment
information in the phrase pair ሺݏ̅, ݌̅ሻ and ሺ݌̅, ݐ̅ሻ
respectively.
</bodyText>
<sectionHeader confidence="0.98602" genericHeader="method">
4 Integrate with Direct Translation
</sectionHeader>
<bodyText confidence="0.9995943">
If a standard source-target bilingual corpus is
available, we can train a direct translation model.
Thus we can integrate the direct model and the
pivot model to obtain further improvements. We
propose a mixed model by merging the co-
occurrence count in direct translation and pivot
translation. Besides, we also employ an interpo-
lated model (Wu and Wang, 2007) by merging
the direct translation model and pivot translation
model using a linear interpolation.
</bodyText>
<subsectionHeader confidence="0.885108">
4.1 Mixed Model
</subsectionHeader>
<bodyText confidence="0.987274555555556">
Given ݊ pivot languages, the co-occurrence
count can be estimated using the method de-
scribed in Section 3.1. Then the co-occurrence
count and the lexical weight of the mixed model
can be estimated with the following Equation 12
and 13.
௡
ܿሺݏ, ݐሻ ൌ ෍ ܿ௜ሺݏ, ݐሻ
௜ୀ଴
௡
݌ఠሺݏ̅|ݐ̅, ܽሻ ൌ ෍ ߙ௜ ݌ఠ,௜ሺݏ̅|ݐ̅, ܽሻ
௜ୀ଴
where ܿ଴ሺݏ, ݐሻ and ݌ఠ,଴ሺݏ̅|ݐ̅, ܽሻ are the co-
occurrence count and lexical weight in the direct
translation model respectively. ܿ௜ሺݏ, ݐሻ and
݌ఠ,௜ሺݏ̅|ݐ̅, ܽሻ denote the co-occurrence count and
lexical weight in the pivot translation model. ߙ௜
is the interpolation coefficient, requiring
</bodyText>
<equation confidence="0.8883295">
௡
∑௜ୀ଴ ߙ௜ ൌ 1.
</equation>
<sectionHeader confidence="0.64327" genericHeader="method">
4.2 Interpolated Model
</sectionHeader>
<bodyText confidence="0.998613">
Following Wu and Wang (2007), the interpolated
model can be modelled with Equation 14.
</bodyText>
<equation confidence="0.945318333333333">
௡
߶ሺݏ̅|ݐ̅ሻ ൌ ෍ ߚ௜߶௜ሺݏ̅|ݐ̅ሻ (14)
௜ୀ଴
</equation>
<bodyText confidence="0.9971">
where ߶଴ሺݏ̅|ݐ̅ሻ is the phrase translation probabil-
ity in direct translation model; ߶௜ሺݏ̅|ݐ̅ሻ is the
phrase translation probability in pivot translation
model. The lexical weight is obtained with Equa-
tion 13. ߚ௜ is the interpolation coefficient, requir-
</bodyText>
<equation confidence="0.975412571428571">
ing ∑௡௜ୀ଴ ߚ௜ ൌ 1.
1
݌ఠሺݏ̅ |ݐ̅, ܽሻ ൌ ෑ|ሼ݆|ሺ݅,݆ሻ ∈ ܽሽ |෍ ߱ሺݏ௜|ݐ௝ሻ
௜ ଵ∀ሺ௜
௡
,௝ሻ∈௔
(9)
</equation>
<page confidence="0.921795">
1668
</page>
<table confidence="0.9999372">
Language Sentence Source Target
Pairs Pairs Words Words
de-en 1.9M 48.5M 50.9M
es-en 1.9M 54M 51.7M
fr-en 2M 58.1M 52.4M
</table>
<tableCaption confidence="0.996294">
Table 1: Training data of Europarl corpus
</tableCaption>
<table confidence="0.999859571428571">
System BLEU%
de-es de-fr es-de es-fr fr-de fr-es
Baseline 27.04 23.01 20.65 33.84 20.87 38.31
Minimum 27.93* 23.94* 21.52* 35.38* 21.48* 39.62*
Maximum 25.70 21.59 20.26 32.58 20.50 37.30
Arithmetic mean 26.01 22.24 20.13 33.38 20.37 37.37
Geometric mean 27.31 23.49* 21.10* 34.76* 21.15* 39.19*
</table>
<tableCaption confidence="0.9840275">
Table 2: Comparison of different merging methods on in-domain test set. * indicates the results are
significantly better than the baseline (p&lt;0.05).
</tableCaption>
<table confidence="0.999872428571429">
System BLEU%
de-es de-fr es-de es-fr fr-de fr-es
Baseline 15.34 13.52 11.47 21.99 12.19 25.00
Minimum 15.77* 14.08* 11.99* 23.90* 12.55* 27.05*
Maximum 13.41 11.83 10.17 20.48 10.83 22.75
Arithmetic mean 13.96 12.10 10.57 21.07 11.30 23.70
Geometric mean 15.09 13.30 11.52 23.32* 12.46* 26.22*
</table>
<tableCaption confidence="0.999907">
Table 3: Comparison of different merging methods on out-of-domain test set.
</tableCaption>
<sectionHeader confidence="0.994303" genericHeader="method">
5 Experiments on Europarl Corpus
</sectionHeader>
<bodyText confidence="0.999805578947368">
Our first experiment is carried out on Europarl1
corpus, which is a multi-lingual corpus including
21 European languages (Koehn, 2005). In our
work, we perform translations among French (fr),
German (de) and Spanish (es). Due to the rich-
ness of available language resources, we choose
English (en) as the pivot language. Table 1
summarized the statistics of training data. For the
language model, the same monolingual data ex-
tracted from the Europarl are used.
The word alignment is obtained by GIZA++
(Och and Ney, 2000) and the heuristics “grow-
diag-final” refinement rule (Koehn et al., 2003).
Our translation system is an in-house phrase-
based system analogous to Moses (Koehn et al.,
2007). The baseline system is the triangulation
method (Wu and Wang, 2007), including an in-
terpolated model which linearly interpolate the
direct and pivot translation model.
</bodyText>
<footnote confidence="0.866225">
1 http://www.statmt.org/europarl
</footnote>
<bodyText confidence="0.9998825">
We use WMT082 as our test data, which con-
tains 2000 in-domain sentences and 2051 out-of-
domain sentences with single reference. The
translation results are evaluated by case-
insensitive BLEU-4 metric (Papineni et al.,
2002). The statistical significance tests using
95% confidence interval are measured with
paired bootstrap resampling (Koehn, 2004).
</bodyText>
<sectionHeader confidence="0.693619" genericHeader="evaluation">
5.1 Results
</sectionHeader>
<bodyText confidence="0.999817909090909">
We compare 4 merging methods with the base-
line system. The results are shown in Table 2 and
Table 3. We find that the minimum method out-
performs the others, achieving significant im-
provements over the baseline on all translation
directions. The absolute improvements range
from 0.61 (fr-de) to 1.54 (es-fr) in BLEU% score
on in-domain test data, and range from 0.36 (fr-
de) to 2.05 (fr-es) in BLEU% score on out-of-
domain test data. This indicates that our method
is effective and robust in general.
</bodyText>
<footnote confidence="0.946923">
2 http://www.statmt.org/wmt08/shared-task.html
</footnote>
<page confidence="0.985551">
1669
</page>
<figure confidence="0.999161347222223">
29.5
29
BLEU%
28.5
28
27.5
27
26.5
direct
tri
co
tri+inter
co+inter
co+mix
direct
tri
co
tri+inter
co+inter
co+mix
direct
tri
co
tri+inter
co+inter
co+mix
direct
tri
co
tri+inter
co+inter
co+mix
BLEU% 25.5
BLEU% 25
BLEU% 24.5
24
23.5
23
22.5
37
36.5
36
35.5
35
34.5
34
33.5
41
40.5
40
39.5
39
38.5
38
37.5
(a) German-English-Spanish (b) German-English-French
direct
tri
co
tri+inter
co+inter
co+mix
22
BLEU%
21.5
21
20.5
20
19.5
22.5
(c) Spanish-English-German (d) Spanish-English-French
(e) French-English-German (f) French-English-Spanish
</figure>
<figureCaption confidence="0.998747">
Figure 3: Comparisons of pivot-based methods on different scales of source-target standard corpora.
(direct: direct model; tri: triangulation model; co: co-occurrence count model; tri+inter: triangulation
model interpolated with direct model ; co+inter: co-occurrence count model interpolated with direct
model; co+mix: mixed model). X-axis represents the scale of the standard training data.
</figureCaption>
<figure confidence="0.9830935">
direct
tri
co
tri+inter
co+inter
co+mix
22
21.5
BLEU%
21
20.5
20
19.5
22.5
</figure>
<bodyText confidence="0.999442555555556">
The geometric mean method also achieves im-
provement, but not as significant as the minimum
method. However, the maximum and the arith-
metic mean methods show a decrement in BLEU
scores. This reminds us that how to choose a
proper merging function for the co-occurrence
count is a key problem. In the future, we will
explore more sophisticated method to merge co-
occurrence count.
</bodyText>
<subsectionHeader confidence="0.995647">
5.2 Analysis
</subsectionHeader>
<bodyText confidence="0.999976866666667">
The pivot-based translation is suitable for the
scenario that there exists large amount of source-
pivot and pivot-target bilingual corpora and only
a little source-target bilingual data. Thus, we
randomly select 10K, 50K, 100K, 200K, 500K,
1M, 1.5M sentence pairs from the source-target
bilingual corpora to simulate the lack of source-
target data. With these corpora, we train several
direct translation models with different scales of
bilingual data. We interpolate each direct transla-
tion model with the pivot model (both triangula-
tion method and co-occurrence count method) to
obtain the interpolated model respectively. We
also mix the direct model and pivot model using
the method described in Section 4.1. Following
</bodyText>
<page confidence="0.977024">
1670
</page>
<bodyText confidence="0.9991649">
Wu and Wang (2007), we set α଴ = 0.9, αଵ = 0.1,
β଴ = 0.9 and βଵ = 0.1 empirically. The experi-
ments are carried out on 6 translation directions:
German-Spanish, German-French, Spanish-
German, Spanish-French, French-German and
French-Spanish. The results are shown in Figure
3. We only list the results on in-domain test sets.
The trend of the results on out-of domain test
sets is similar with in-domain test sets.
The results are explained as follows:
</bodyText>
<listItem confidence="0.9744335">
(1) Comparison of Pivot Translation and Di-
rect Translation
</listItem>
<bodyText confidence="0.99868025">
The pivot translation models are better than
the direct translation models trained on a small
source-target bilingual corpus. With the incre-
ment of source-target corpus, the direct model
first outperforms the triangulation model and
then outperforms the co-occurrence count model
consecutively.
Taking Spanish-English-French translation as
an example, the co-occurrence count model
achieves BLEU% scores of 35.38, which is close
to the direct translation model trained with 200K
source-target bilingual data. Compared with the
co-occurrence count model, the triangulation
model only achieves BLEU% scores of 33.84,
which is close to the direct translation model
trained with 50K source-target bilingual data.
</bodyText>
<listItem confidence="0.4939055">
(2) Comparison of Different Interpolated
Models
</listItem>
<bodyText confidence="0.999746703703704">
For the pivot model trained by triangulation
method and co-occurrence count method, we
interpolate them with the direct translation model
trained with different scales of bilingual data.
Figure 3 shows the translation results of the dif-
ferent interpolated models. For all the translation
directions, our co-occurrence count method in-
terpolated with the direct model is better than the
triangulation model interpolated with the direct
model.
The two interpolated model are all better than
the direct translation model. With the increment
of the source-target training corpus, the gap be-
comes smaller. This indicates that the pivot mod-
el and its affiliated interpolated model are suita-
ble for language pairs with small bilingual data.
Even if the scale of source-pivot and pivot-target
corpora is close to the scale of source-target bi-
lingual corpora, the pivot translation model can
help the direct translation model to improve the
translation performance. Take Spanish-English-
French translation as an issue, when the scale of
Spanish-French parallel data is 1.5M sentences
pairs, which is close to the Spanish-English and
English-French parallel data, the performance of
co+mix model is still outperforms the direct
translation model.
</bodyText>
<listItem confidence="0.780384">
(3) Comparison of Interpolated Model and
Mixed Model
</listItem>
<bodyText confidence="0.999921777777778">
When only a small source-target bilingual
corpus is available, the mix model outperforms
the interpolated model. With the increasing of
source-target corpus, the mix model is close to
the interpolated model or worse than the interpo-
lated model. This indicates that the mix model
has a better performance when the source-target
corpus is small which is close to the realistic sce-
nario.
</bodyText>
<subsectionHeader confidence="0.9033195">
5.3 Integrate the Co-occurrence Count
Model and Triangulation Model
</subsectionHeader>
<bodyText confidence="0.9999543125">
Experimental results in the previous section
show that, our co-occurrence count models gen-
erally outperform the baseline system. In this
section, we carry out experiments that integrates
co-occurrence count model into the triangulation
model.
For French-English-German translation, we
apply a linear interpolation method to integrate
the co-occurrence count model into triangulation
model following the method described in Section
4.2. We set α as the interpolation coefficient of
triangulation model and 1 െ α as the interpola-
tion coefficient of co-occurrence count model
respectively. The experiments take 9 values for
interpolation coefficient, from 0.1 to 0.9. The
results are shown in Figure 4.
</bodyText>
<figure confidence="0.9435275">
21.8
21.6
21.4
21.2
21
20.4
0.1 0.2 0.3 0.4 0.5 0.6 0.7 0.8 0.9
Interpolation Coefficient
integrated triangulation
co-occurrence
</figure>
<figureCaption confidence="0.809816">
Figure 4: Results of integrating the co-
occurrence count model and the triangulation
model.
</figureCaption>
<bodyText confidence="0.98504025">
When using interpolation coefficient ranging
from 0.2 to 0.7, the integrated models outperform
the triangulation and the co-occurrence count
model. However, for the other intervals, the inte-
</bodyText>
<figure confidence="0.581325333333333">
BLEU%
20.8
20.6
</figure>
<page confidence="0.792079">
1671
</page>
<table confidence="0.999928125">
Language Sentence Source Target
Pairs Pairs Words Words
zh-en-1 1M 18.1M 17.7M
zh-en-2 2M 36.2M 35.5M
zh-en-3 3M 54.2M 53.2M
zh-en-4 4M 72.3M 70.9M
zh-en-5 5M 90.4M 88.6M
en-jp 1M 9.2M 11.1M
</table>
<tableCaption confidence="0.996649">
Table 4: Training data of web corpus
</tableCaption>
<table confidence="0.999737285714286">
System BLEU%
zh-en-jp-1* zh-en-jp-2 zh-en-jp-3 zh-en-jp-4 zh-en-jp-5
Baseline 29.07 29.39 29.44 29.67 29.80
Minimum 31.13* 31.28* 31.43* 31.62* 32.02*
Maximum 28.88 29.01 29.12 29.37 29.59
Arithmetic mean 29.08 29.36 29.51 29.79 30.01
Geometric mean 30.77* 31.30* 31.75* 32.07* 32.34*
</table>
<tableCaption confidence="0.993683">
Table 5: Comparison of different merging methods on the imbalanced web data. ( zh-en-jp-1 means
</tableCaption>
<bodyText confidence="0.9212491">
the translation system is trained with zh-en-1 as source-pivot corpus and en-jp as pivot-target corpus,
and so on. )
grated models perform slightly lower than the
co-occurrence count model, but still show better
results than the triangulation model. The trend of
the curve infers that the integrated model synthe-
sizes the contributions of co-occurrence count
model and triangulation model. Additionally, it
also indicates that, the choice of the interpolation
coefficient affects the translation performances.
</bodyText>
<sectionHeader confidence="0.985331" genericHeader="evaluation">
6 Experiments on Web Data
</sectionHeader>
<bodyText confidence="0.999973142857143">
The experimental on Europarl is artificial, as the
training data for directly translating between
source and target language actually exists in the
original data sets. Thus, we conducted several
experiments on a more realistic scenario: trans-
lating Chinese (zh) to Japanese (jp) via English
(en) with web crawled data.
As mentioned in Section 3.1, the source-pivot
and pivot-target parallel corpora can be imbal-
anced in quantities. If one parallel corpus was
much larger than another, then minimum heuris-
tic function would likely just take the counts
from the smaller corpus.
In order to analyze this issue, we manually set
up imbalanced corpora. For source-pivot parallel
corpora, we randomly select 1M, 2M, 3M, 4M
and 5M Chinese-English sentence pairs. On the
other hand, we randomly select 1M English-
Japanese sentence pairs as pivot-target parallel
corpora. The training data of Chinese-English
and English-Japanese language pairs are summa-
rized in Table 4. For the Chinese-Japanese direct
corpus, we randomly select 5K, 10K, 20K, 30K,
40K, 50K, 60K, 70K, 80K, 90K and 100K sen-
tence pairs to simulate the lack of bilingual data.
We built a 1K in-house test set with four refer-
ences. For Japanese language model training, we
used the monolingual part of English-Japanese
corpus.
Table 5 shows the results of different co-
occurrence count merging methods. First, the
minimum method and the geometric mean meth-
od outperform the other two merging methods
and the baseline system with different training
corpus. When the scale of source-pivot and piv-
ot-target corpus is roughly balanced (zh-en-jp-1),
the minimum method achieves an absolute im-
provement of 2.06 percentages points on BLEU
over the baseline, which is also better than the
other merging methods. While, with the growth
of source-pivot corpus, the gap between source-
pivot corpus and pivot-target corpus becomes
bigger. In this circumstance, the geometric mean
method becomes better than the minimum meth-
od. Compared to the minimum method, the geo-
metric mean method considers both the source-
pivot and the pivot-target corpus, which may
lead to a better result in the case of imbalanced
training corpus.
</bodyText>
<page confidence="0.978452">
1672
</page>
<bodyText confidence="0.9939148125">
Furthermore, with the imbalanced corpus zh-
en-jp-5, we compared the translation perfor-
mance of our co-occurrence count model (with
geometric mean merging method), triangulation
model, interpolated model, mixed model and the
direct translation models. Figure 5 summarized
the results.
The co-occurrence count model can achieve an
absolute improvement of 2.54 percentages points
on BLEU over the baseline. The triangulation
method outperforms the direct translation when
only 5K sentence pairs are available. Meanwhile,
the number is 10K when using the co-occurrence
count method. The co-occurrence count models
interpolated with the direct model significantly
outperform the other models.
</bodyText>
<figureCaption confidence="0.613388">
Figure 5: Results on Chinese-Japanese Web Data.
X-axis represents the scale of the standard train-
ing data.
</figureCaption>
<bodyText confidence="0.999868428571428">
In this experiment, the training data contains
parallel sentences on various domains. And the
training corpora (Chinese-English and English-
Japanese) are typically very different, since they
are obtained on the web. It indicates that our co-
occurrence count method is robust in the realistic
scenario.
</bodyText>
<sectionHeader confidence="0.998918" genericHeader="conclusions">
7 Conclusion
</sectionHeader>
<bodyText confidence="0.999966375">
This paper proposed a novel approach for pivot-
based SMT by pivoting the co-occurrence count
of phrase pairs. Different from the triangulation
method merging the source-pivot and pivot-
target language after training the translation
model, our method merges the source-pivot and
pivot-target language after extracting the phrase
pairs, thus the computing for phrase translation
probabilities is under the uniform probability
space. The experimental results on Europarl data
and web data show significant improvements
over the baseline systems. We also proposed a
mixed model to combine the direct translation
and pivot translation, and the experimental re-
sults show that the mixed model has a better per-
formance when the source-target corpus is small
which is close to the realistic scenario.
A key problem in the approach is how to learn
the co-occurrence count. In this paper, we use the
minimum function on balanced corpora and the
geometric mean function on imbalanced corpora
to estimate the co-occurrence count intuitively.
In the future, we plan to explore more effective
approaches.
</bodyText>
<sectionHeader confidence="0.998089" genericHeader="acknowledgments">
Acknowledgments
</sectionHeader>
<bodyText confidence="0.999692125">
We would like to thank Yiming Cui for insight-
ful discussions, and three anonymous reviewers
for many invaluable comments and suggestions
to improve our paper. This work is supported by
National Natural Science Foundation of China
(61100093), and the State Key Development
Program for Basic Research of China (973 Pro-
gram, 2014CB340505).
</bodyText>
<sectionHeader confidence="0.995494" genericHeader="references">
Reference
</sectionHeader>
<reference confidence="0.996048666666667">
Nicola Bertoldi, Madalina Barbaiani, Marcello
Federico, and Roldano Cattoni. 2008. Phrase-
Based statistical machine translation with Piv-
ot Languages. In Proceedings of the 5th Inter-
national Workshop on Spoken Language
Translation (IWSLT), pages 143-149.
Trevor Cohn and Mirella Lapata. 2007. Machine
Translation by Triangulation: Make Effective
Use of Multi-Parallel Corpora. In Proceedings
of 45th Annual Meeting of the Association for
Computational Linguistics, pages 828-735.
Marta R. Costa-jussà, Carlos Henríquez, and Ra-
fael E. Banchs. 2011. Enhancing Scarce-
Resource Language Translation through Pivot
Combinations. In Proceedings of the 5th In-
ternational Joint Conference on Natural Lan-
guage Processing, pages 1361-1365.
Yiming Cui, Conghui Zhu, Xiaoning Zhu, Tiejun
Zhao and Dequan Zheng. 2013. Phrase Table
Combination Deficiency Analyses in Pivot-
based SMT. In Proceedings of 18th Interna-
tional Conference on Application of Natural
Language to Information Systems, pages 355-
358.
Adria de Gispert and Jose B. Marino. 2006.
Catalan-English statistical machine translation
without parallel corpus: bridging through
Spanish. In Proceedings of 5th International
Conference on Language Resources and Eval-
uation (LREC), pages 65-68.
</reference>
<figure confidence="0.906510214285714">
39
37
35
33
31
29
BLEU%
direct
tri
co-occur
tri+inter
co+inter
co+mix
5K 20K 40K 60K 80K 100K
</figure>
<page confidence="0.929851">
1673
</page>
<reference confidence="0.999576237623762">
Kevin Duh, Katsuhito Sudoh, Xianchao Wu,
Hajime Tsukada and Masaaki Nagata. 2011.
Generalized Minimum Bayes Risk System
Combination. In Proceedings of the 5th Inter-
national Joint Conference on Natural Lan-
guage Processing, pages 1356-1360.
Ahmed El Kholy, Nizar Habash, Gregor Leusch,
Evgeny Matusov and Hassan Sawaf. 2013.
Language Independent Connectivity Strength
Features for Phrase Pivot Statistical Machine
Translation. In Proceedings of the 51st Annual
Meeting of the Association for Computational
Linguistics, pages 412-418.
Ahmed El Kholy, Nizar Habash, Gregor Leusch,
Evgeny Matusov and Hassan Sawaf. 2013. Se-
lective Combination of Pivot and Direct Sta-
tistical Machine Translation Models. In Pro-
ceedings of the 6th International Joint Confer-
ence on Natural Language Processing, pages
1174-1180.
Jesús González-Rubio, Alfons Juan and Francis-
co Casacuberta. 2011. Minimum Bayes-risk
System Combination. In Proceedings of the
49th Annual Meeting of the Association for
Computational Linguistics, pages 1268-1277.
Philipp Koehn, Franz J. Och, and Daniel Marcu.
2003. Statistical Phrase-Based Translation. In
HLT-NAACL: Human Language Technology
Conference of the North American Chapter of
the Association for Computational Linguistics,
pages 127-133.
Philipp Koehn. 2004. Statistical significance
tests for machine translation evaluation. In
Proceedings of the 2004 Conference on Em-
pirical Methods in Natural Language Pro-
cessing (EMNLP), pages 388-395.
Philipp Koehn. 2005. Europarl: A Parallel Cor-
pus for Statistical Machine Translation. In
Proceedings of MT Summit X, pages 79-86.
Philipp Koehn, Hieu Hoang, Alexanda Birch,
Chris Callison-Burch, Marcello Federico, Ni-
cola Bertoldi, Brooke Cowan, Wade Shen,
Christine Moran, Richard Zens, Chris Dyer,
Ondrej Bojar, Alexandra Constantin, and Evan
Herbst. 2007. Moses: Open Source Toolkit for
Statistical Machine Translation. In Proceed-
ings of the 45th Annual Meeting of the Associ-
ation for Computational Linguistics, demon-
stration session, pages 177-180.
Philipp Koehn, Alexandra Birch, and Ralf Stein-
berger. 2009. 462 Machine Translation Sys-
tems for Europe. In Proceedings of the MT
Summit XII.
Gregor Leusch, Aurélien Max, Josep Maria
Crego and Hermann Ney. 2010. Multi-Pivot
Translation by System Combination. In Pro-
ceedings of the 7th International Workshop on
Spoken Language Translation, pages 299-306.
Franz Josef Och and Hermann Ney. 2000. A
comparison of alignment models for statistical
machine translation. In Proceedings of the
18th International Conference on Computa-
tional Linguistics, pages 1086-1090.
Michael Paul, Andrew Finch, Paul R. Dixon and
Eiichiro Sumita. 2011. Dialect Translation: In-
tegrating Bayesian Co-segmentation Models
with Pivot-based SMT. In Proceedings of the
2011 Conference on Empirical Methods in
Natural Language Processing, pages 1-9.
Michael Paul and Eiichiro Sumita. 2011. Trans-
lation Quality Indicators for Pivot-based Sta-
tistical MT. In Proceedings of the 5th Interna-
tional Joint Conference on Natural Language
Processing, pages 811-818.
Kishore Papineni, Salim Roukos, Todd Ward and
Wei-Jing Zhu. 2002. BLEU: a Method for Au-
tomatic Evaluation of Machine Translation. In
Proceedings of the 40th Annual Meeting of the
Association for Computation Linguistics, pag-
es 311-319.
Rie Tanaka, Yohei Murakami and Toru Ishida.
2009. Context-Based Approach for Pivot
Translation Services. In the Twenty-first In-
ternational Conference on Artificial Intelli-
gence, pages 1555-1561.
Jörg Tiedemann. 2012. Character-Based Pivot
Translation for Under-Resourced Languages
and Domains. In Proceedings of the 13th Con-
ference of the European Chapter of the Asso-
ciation for Computational Linguistics, pages
141-151.
Masatoshi Tsuchiya, Ayu Purwarianti, Toshiyu-
kiWakita and Seiichi Nakagawa. 2007. Ex-
panding Indonesian-Japanese Small Transla-
tion Dictionary Using a Pivot Language. In
Proceedings of the ACL 2007 Demo and Post-
er Sessions, pages 197-200.
Takashi Tsunakawa, Naoaki Okazaki and
Jun&apos;ichi Tsujii. 2010. Building a Bilingual
Lexicon Using Phrase-based Statistical Ma-
chine Translation via a Pivot Language. In
</reference>
<page confidence="0.866029">
1674
</page>
<reference confidence="0.99893075">
Proceedings of the 22th International Confer-
ence on Computational Linguistics (Coling),
pages 127-130.
Masao Utiyama and Hitoshi Isahara. 2007. A
Comparison of Pivot Methods for Phrase-
Based Statistical Machine Translation. In Pro-
ceedings of Human Language Technology: the
Conference of the North American Chapter of
the Association for Computational Linguistics,
pages 484-491.
Masao Utiyama, Andrew Finch, Hideo Okuma,
Michael Paul, Hailong Cao, Hirofumi Yama-
moto, Keiji Yasuda,and Eiichiro Sumita. 2008.
The NICT/ATR speech Translation System for
IWSLT 2008. In Proceedings of the Interna-
tional Workshop on Spoken Language Trans-
lation, pages 77-84.
Haifeng Wang, Hua Wu, Xiaoguang Hu, Zhanyi
Liu, Jianfeng Li, Dengjun Ren, and Zhengyu
Niu. 2008. The TCH Machine Translation
System for IWSLT 2008. In Proceedings of
the International Workshop on Spoken Lan-
guage Translation, pages 124-131.
Hua Wu and Haifeng Wang. 2007. Pivot Lan-
guage Approach for Phrase-Based Statistical
Machine Translation. In Proceedings of 45th
Annual Meeting of the Association for Compu-
tational Linguistics, pages 856-863.
Hua Wu and Haifeng Wang. 2009. Revisiting
Pivot Language Approach for Machine Trans-
lation. In Proceedings of the 47th Annual
Meeting of the Association for Computational
Linguistics and the 4th IJCNLP of the AFNLP,
pages 154-162.
Samira Tofighi Zahabi, Somayeh Bakhshaei and
Shahram Khadivi. Using Context Vectors in
Improving a Machine Translation System with
Bridge Language. In Proceedings of the 51st
Annual Meeting of the Association for Compu-
tational Linguistics, pages 318-322.
</reference>
<page confidence="0.991326">
1675
</page>
</variant>
</algorithm>
<algorithm name="ParsHed" version="110505">
<variant no="0" confidence="0.675357">
<title confidence="0.9954935">Improving Pivot-Based Statistical Machine Translation by the Co-occurrence Count of Phrase Pairs</title>
<author confidence="0.994888">Zhongjun Hua Conghui</author>
<affiliation confidence="0.9266595">and Tiejun Institute of Technology, Harbin,</affiliation>
<address confidence="0.802789">Inc., Beijing,</address>
<email confidence="0.999873">hezhongjun@baidu.com</email>
<email confidence="0.999873">wu_hua@baidu.com</email>
<email confidence="0.999873">wanghaifeng@baidu.com</email>
<abstract confidence="0.99972">To overcome the scarceness of bilingual corpora for some language pairs in machine translation, pivot-based SMT uses pivot language as a &amp;quot;bridge&amp;quot; to generate source-target translation from sourcepivot and pivot-target translation. One of the key issues is to estimate the probabilities for the generated phrase pairs. In this paper, we present a novel approach to calculate the translation probability by pivoting the co-occurrence count of source-pivot and pivot-target phrase pairs. Experimental results on Europarl data and web data show that our method leads to significant improvements over the baseline systems.</abstract>
</variant>
</algorithm>
<algorithm name="ParsCit" version="110505">
<citationList>
<citation valid="true">
<authors>
<author>Nicola Bertoldi</author>
<author>Madalina Barbaiani</author>
<author>Marcello Federico</author>
<author>Roldano Cattoni</author>
</authors>
<title>PhraseBased statistical machine translation with Pivot Languages.</title>
<date>2008</date>
<booktitle>In Proceedings of the 5th International Workshop on Spoken Language Translation (IWSLT),</booktitle>
<pages>143--149</pages>
<contexts>
<context position="1467" citStr="Bertoldi et al., 2008" startWordPosition="202" endWordPosition="205">mental results on Europarl data and web data show that our method leads to significant improvements over the baseline systems. 1 Introduction Statistical Machine Translation (SMT) relies on large bilingual parallel data to produce high quality translation results. Unfortunately, for some language pairs, large bilingual corpora are not readily available. To alleviate the parallel data scarceness, a conventional solution is to introduce a “bridge” language (named pivot language) to connect the source and target language (de Gispert and Marino, 2006; Utiyama and Isahara, 2007; Wu and Wang, 2007; Bertoldi et al., 2008; Paul et al., 2011; El Kholy et al., 2013; Zahabi et al., 2013), where there are large amounts of source-pivot and pivot-target parallel corpora. Among various pivot-based approaches, the triangulation method (Cohn and Lapata, 2007; Wu and Wang, 2007) is a representative work in * This work was done when the first author was visiting Baidu. pivot-based machine translation. The approach proposes to build a source-target phrase table by merging the source-pivot and pivot-target phrase table. One of the key issues in this method is to estimate the translation probabilities for the generated sour</context>
</contexts>
<marker>Bertoldi, Barbaiani, Federico, Cattoni, 2008</marker>
<rawString>Nicola Bertoldi, Madalina Barbaiani, Marcello Federico, and Roldano Cattoni. 2008. PhraseBased statistical machine translation with Pivot Languages. In Proceedings of the 5th International Workshop on Spoken Language Translation (IWSLT), pages 143-149.</rawString>
</citation>
<citation valid="true">
<authors>
<author>Trevor Cohn</author>
<author>Mirella Lapata</author>
</authors>
<title>Machine Translation by Triangulation: Make Effective Use of Multi-Parallel Corpora.</title>
<date>2007</date>
<booktitle>In Proceedings of 45th Annual Meeting of the Association for Computational Linguistics,</booktitle>
<pages>828--735</pages>
<contexts>
<context position="1699" citStr="Cohn and Lapata, 2007" startWordPosition="237" endWordPosition="240">igh quality translation results. Unfortunately, for some language pairs, large bilingual corpora are not readily available. To alleviate the parallel data scarceness, a conventional solution is to introduce a “bridge” language (named pivot language) to connect the source and target language (de Gispert and Marino, 2006; Utiyama and Isahara, 2007; Wu and Wang, 2007; Bertoldi et al., 2008; Paul et al., 2011; El Kholy et al., 2013; Zahabi et al., 2013), where there are large amounts of source-pivot and pivot-target parallel corpora. Among various pivot-based approaches, the triangulation method (Cohn and Lapata, 2007; Wu and Wang, 2007) is a representative work in * This work was done when the first author was visiting Baidu. pivot-based machine translation. The approach proposes to build a source-target phrase table by merging the source-pivot and pivot-target phrase table. One of the key issues in this method is to estimate the translation probabilities for the generated source-target phrase pairs. Conventionally, the probabilities are estimated by multiplying the posterior probabilities of source-pivot and pivottarget phrase pairs. However, it has been shown that the generated probabilities are not acc</context>
<context position="8366" citStr="Cohn and Lapata, 2007" startWordPosition="1248" endWordPosition="1251">source language with a pivot-source model; (3) combine the source sentences with translated target sentences or/and combine the target sentences with translated source sentences (Utiyama et al., 2008; Wu and Wang, 2009). However, it is difficult to build a high quality translation system with a corpus created by a machine translation system. Triangulation Method: The triangulation method obtains source-target phrase table by merging source-pivot and pivot-target phrase table entries with identical pivot language phrases and multiplying corresponding posterior probabilities (Wu and Wang, 2007; Cohn and Lapata, 2007), which has been shown to work better than the other pivot approaches (Utiyama and Isahara, 2007). A problem of this approach is that the probability space of the source-target phrase pairs is non-uniformity due to the mismatching of the pivot phrase. 3 Our Approach In this section, we will introduce our method for learning a source-target phrase translation model with a pivot language as a bridge. We extract the co-occurrence count of phrase pairs for each language pair with a source-pivot and a pivot-target corpus. Then we generate the source-target phrase pairs with induced co-occurrence in</context>
</contexts>
<marker>Cohn, Lapata, 2007</marker>
<rawString>Trevor Cohn and Mirella Lapata. 2007. Machine Translation by Triangulation: Make Effective Use of Multi-Parallel Corpora. In Proceedings of 45th Annual Meeting of the Association for Computational Linguistics, pages 828-735.</rawString>
</citation>
<citation valid="true">
<authors>
<author>Marta R Costa-jussà</author>
<author>Carlos Henríquez</author>
<author>Rafael E Banchs</author>
</authors>
<title>Enhancing ScarceResource Language Translation through Pivot Combinations.</title>
<date>2011</date>
<booktitle>In Proceedings of the 5th International Joint Conference on Natural Language Processing,</booktitle>
<pages>1361--1365</pages>
<contexts>
<context position="6746" citStr="Costa-jussà et al., 2011" startWordPosition="1000" endWordPosition="1003">t translation model. See Figure 2. (b) and (c) for further illustration. 1666 The remainder of this paper is organized as follows. In Section 2, we describe the related work. We introduce the co-occurrence count method in Section 3, and the mixed model in Section 4. In Section 5 and Section 6, we describe and analyze the experiments. Section 7 gives a conclusion of the paper. 2 Related Work Several methods have been proposed for pivotbased translation. Typically, they can be classified into 3 kinds as follows: Transfer Method: The transfer method (Utiyama and Isahara, 2007; Wang et al., 2008; Costa-jussà et al., 2011) connects two translation systems: a source-pivot MT system and a pivottarget MT system. Given a source sentence, (1) the source-pivot MT system translates it into the pivot language, (2) and the pivot-target MT system translates the pivot sentence into the target sentence. During each step (source to pivot and pivot to target), multiple translation outputs will be generated, thus a minimum Bayes-risk system combination method is often used to select the optimal sentence (González-Rubio et al., 2011; Duh et al., 2011). The problem with the transfer method is that it needs to decode twice. On o</context>
</contexts>
<marker>Costa-jussà, Henríquez, Banchs, 2011</marker>
<rawString>Marta R. Costa-jussà, Carlos Henríquez, and Rafael E. Banchs. 2011. Enhancing ScarceResource Language Translation through Pivot Combinations. In Proceedings of the 5th International Joint Conference on Natural Language Processing, pages 1361-1365.</rawString>
</citation>
<citation valid="true">
<authors>
<author>Yiming Cui</author>
<author>Conghui Zhu</author>
</authors>
<title>Xiaoning Zhu, Tiejun Zhao and Dequan Zheng.</title>
<date>2013</date>
<booktitle>In Proceedings of 18th International Conference on Application of Natural Language to Information Systems,</booktitle>
<pages>355--358</pages>
<marker>Cui, Zhu, 2013</marker>
<rawString>Yiming Cui, Conghui Zhu, Xiaoning Zhu, Tiejun Zhao and Dequan Zheng. 2013. Phrase Table Combination Deficiency Analyses in Pivotbased SMT. In Proceedings of 18th International Conference on Application of Natural Language to Information Systems, pages 355-358.</rawString>
</citation>
<citation valid="true">
<authors>
<author>Adria de Gispert</author>
<author>Jose B Marino</author>
</authors>
<title>Catalan-English statistical machine translation without parallel corpus: bridging through Spanish.</title>
<date>2006</date>
<booktitle>In Proceedings of 5th International Conference on Language Resources and Evaluation (LREC),</booktitle>
<pages>65--68</pages>
<marker>de Gispert, Marino, 2006</marker>
<rawString>Adria de Gispert and Jose B. Marino. 2006. Catalan-English statistical machine translation without parallel corpus: bridging through Spanish. In Proceedings of 5th International Conference on Language Resources and Evaluation (LREC), pages 65-68.</rawString>
</citation>
<citation valid="true">
<authors>
<author>Kevin Duh</author>
</authors>
<title>Katsuhito Sudoh, Xianchao Wu, Hajime Tsukada and</title>
<date>2011</date>
<booktitle>In Proceedings of the 5th International Joint Conference on Natural Language Processing,</booktitle>
<pages>1356--1360</pages>
<marker>Duh, 2011</marker>
<rawString>Kevin Duh, Katsuhito Sudoh, Xianchao Wu, Hajime Tsukada and Masaaki Nagata. 2011. Generalized Minimum Bayes Risk System Combination. In Proceedings of the 5th International Joint Conference on Natural Language Processing, pages 1356-1360.</rawString>
</citation>
<citation valid="true">
<authors>
<author>Ahmed El Kholy</author>
<author>Nizar Habash</author>
<author>Gregor Leusch</author>
</authors>
<title>Evgeny Matusov and Hassan Sawaf.</title>
<date>2013</date>
<booktitle>In Proceedings of the 51st Annual Meeting of the Association for Computational Linguistics,</booktitle>
<pages>412--418</pages>
<marker>El Kholy, Habash, Leusch, 2013</marker>
<rawString>Ahmed El Kholy, Nizar Habash, Gregor Leusch, Evgeny Matusov and Hassan Sawaf. 2013. Language Independent Connectivity Strength Features for Phrase Pivot Statistical Machine Translation. In Proceedings of the 51st Annual Meeting of the Association for Computational Linguistics, pages 412-418.</rawString>
</citation>
<citation valid="true">
<authors>
<author>Ahmed El Kholy</author>
<author>Nizar Habash</author>
<author>Gregor Leusch</author>
</authors>
<title>Evgeny Matusov and Hassan Sawaf.</title>
<date>2013</date>
<booktitle>In Proceedings of the 6th International Joint Conference on Natural Language Processing,</booktitle>
<pages>1174--1180</pages>
<marker>El Kholy, Habash, Leusch, 2013</marker>
<rawString>Ahmed El Kholy, Nizar Habash, Gregor Leusch, Evgeny Matusov and Hassan Sawaf. 2013. Selective Combination of Pivot and Direct Statistical Machine Translation Models. In Proceedings of the 6th International Joint Conference on Natural Language Processing, pages 1174-1180.</rawString>
</citation>
<citation valid="true">
<authors>
<author>Jesús González-Rubio</author>
<author>Alfons Juan</author>
<author>Francisco Casacuberta</author>
</authors>
<title>Minimum Bayes-risk System Combination.</title>
<date>2011</date>
<booktitle>In Proceedings of the 49th Annual Meeting of the Association for Computational Linguistics,</booktitle>
<pages>1268--1277</pages>
<contexts>
<context position="7250" citStr="González-Rubio et al., 2011" startWordPosition="1079" endWordPosition="1082">s as follows: Transfer Method: The transfer method (Utiyama and Isahara, 2007; Wang et al., 2008; Costa-jussà et al., 2011) connects two translation systems: a source-pivot MT system and a pivottarget MT system. Given a source sentence, (1) the source-pivot MT system translates it into the pivot language, (2) and the pivot-target MT system translates the pivot sentence into the target sentence. During each step (source to pivot and pivot to target), multiple translation outputs will be generated, thus a minimum Bayes-risk system combination method is often used to select the optimal sentence (González-Rubio et al., 2011; Duh et al., 2011). The problem with the transfer method is that it needs to decode twice. On one hand, the time cost is doubled; on the other hand, the translation error of the source-pivot translation system will be transferred to the pivot-target translation. Synthetic Method: It aims to create a synthetic source-target corpus by: (1) translate the pivot part in source-pivot corpus into target language with a pivot-target model; (2) translate the pivot part in pivot-target corpus into source language with a pivot-source model; (3) combine the source sentences with translated target sentenc</context>
</contexts>
<marker>González-Rubio, Juan, Casacuberta, 2011</marker>
<rawString>Jesús González-Rubio, Alfons Juan and Francisco Casacuberta. 2011. Minimum Bayes-risk System Combination. In Proceedings of the 49th Annual Meeting of the Association for Computational Linguistics, pages 1268-1277.</rawString>
</citation>
<citation valid="true">
<authors>
<author>Philipp Koehn</author>
<author>Franz J Och</author>
<author>Daniel Marcu</author>
</authors>
<title>Statistical Phrase-Based Translation.</title>
<date>2003</date>
<booktitle>In HLT-NAACL: Human Language Technology Conference of the North American Chapter of the Association for Computational Linguistics,</booktitle>
<pages>127--133</pages>
<contexts>
<context position="3593" citStr="Koehn et al., 2003" startWordPosition="507" endWordPosition="510">the co-occurrence count of source-target phrase pairs to estimate phrase translation probabilities more precisely. Different from the triangulation method, which merges the source-pivot and pivot-target phrase pairs after training the translation model, we propose to merge the source-pivot and pivot-target phrase pairs immediately after the phrase extraction step, and estimate the co-occurrence count of the source-pivot-target phrase pairs. Finally, we compute the translation probabilities according to the estimated co-occurrence counts, using the standard training method in phrase-based SMT (Koehn et al., 2003). As Figure 1. (b) shows, the 1665 Proceedings of the 2014 Conference on Empirical Methods in Natural Language Processing (EMNLP), pages 1665–1675, October 25-29, 2014, Doha, Qatar. c�2014 Association for Computational Linguistics (a) the triangulation method (b) the co-occurrence count method Figure 1: An example of probability space evolution in pivot translation. (a) the triangulation method (b) the co-occurrence count method (c) the mixed model Figure 2: Framework of the triangulation method, the co-occurrence count method and the mixed model. The shaded box in (b) denotes difference betwe</context>
<context position="9181" citStr="Koehn et al., 2003" startWordPosition="1374" endWordPosition="1377">-uniformity due to the mismatching of the pivot phrase. 3 Our Approach In this section, we will introduce our method for learning a source-target phrase translation model with a pivot language as a bridge. We extract the co-occurrence count of phrase pairs for each language pair with a source-pivot and a pivot-target corpus. Then we generate the source-target phrase pairs with induced co-occurrence information. Finally, we compute translation probabilities using the standard phrase-based SMT training method. 3.1 Phrase Translation Probabilities Following the standard phrase extraction method (Koehn et al., 2003), we can extract phrase pairs ሺݏ̅, ̅ሻ and ሺ̅, ݐ̅ሻ from the corresponding wordaligned source-pivot and pivot-target training corpus, where ݏ̅, ̅ and ݐ̅ denotes the phrase in source, pivot and target language respectively. Formally, given the co-occurrence count ܿሺݏ̅, ̅ሻ and ܿሺ̅, ݐ̅ሻ, we can estimate ܿሺݏ̅, ݐ̅ሻ by Equation 1: ܿሺݏ̅, ݐ̅ሻ ൌ  ݃ሺܿሺݏ̅, ̅ሻ, ܿሺ̅, ݐ̅ሻሻ (1) ̅  where ݃ሺ∙ሻ is a function to merge the cooccurrences count ܿሺݏ̅, ̅ሻ and ܿሺ̅, ݐ̅ሻ. We propose four calculation methods for function ݃ሺ∙ሻ. Given the co-occurrence count ܿሺݏ̅, ̅ሻ and ܿሺ̅, ݐ̅ሻ, we first need to induce the co-</context>
<context position="11737" citStr="Koehn et al., 2003" startWordPosition="1832" endWordPosition="1835">rase pairs. ܿሺݏ̅, ݐ̅ሻ ൌ ሺܿሺݏ̅, ̅ሻ  ܿሺ̅, ݐ̅ሻሻ/2 (5) ̅  ܿሺݏ̅, ݐ̅ሻ ൌ  ඥܿሺݏ̅, ̅ሻ ൈ ܿሺ̅, ݐ̅ሻ (6) ̅ When the co-occurrence count of source-target language is calculated, we can estimate the phrase translation probabilities with the following Equation 7 and Equation 8. ܿሺݏ̅, ݐ̅ሻ ߶ሺݏ̅|ݐ̅ሻ ൌ (7) ∑௦̅ ܿሺݏ̅, ݐ̅ሻ ߮ሺݐ̅|ݏ̅ሻ ൌ ܿሺݏ̅,̅ݐ̅ሻ (8) ∑௧ ܿሺݏ, ݐ̅ሻ 3.2 Lexical Weight Given a phrase pair ሺݏ̅, ݐ̅ሻ and a word alignment a between the source word positions ݅ ൌ 1, ⋯ , ݊ and the target word positions ݆ ൌ 0, ⋯ , ݉, the lexical weight of phrase pair ሺݏ̅, ݐ̅ሻ can be calculated by the following Equation 9 (Koehn et al., 2003). The lexical translation probability distribution ߱ሺݏ|ݐሻ between source word s and target word t can be estimated with Equation 10. ܿሺݏ, ݐሻ ߱ሺݏ|ݐሻ ൌ (10) ∑௦ᇲ ܿሺݏᇱ, ݐሻ To compute the lexical weight for a phrase pair ሺݏ̅, ݐ̅ሻ generated by ሺݏ̅, ̅ሻ and ሺ̅, ݐ̅ሻ, we need the alignment information ܽ, which can be obtained as Equation 11 shows. ܽ ൌ ሼሺݏ, ݐሻ|∃: ሺݏ, ሻ ∈ ܽଵ&amp;ሺ, ݐሻ ∈ ܽଶሽ (11) where ܽଵ and ܽଶ indicate the word alignment information in the phrase pair ሺݏ̅, ̅ሻ and ሺ̅, ݐ̅ሻ respectively. 4 Integrate with Direct Translation If a standard source-target bilingual corpus is available, we can</context>
<context position="15439" citStr="Koehn et al., 2003" startWordPosition="2443" endWordPosition="2446">ments on Europarl Corpus Our first experiment is carried out on Europarl1 corpus, which is a multi-lingual corpus including 21 European languages (Koehn, 2005). In our work, we perform translations among French (fr), German (de) and Spanish (es). Due to the richness of available language resources, we choose English (en) as the pivot language. Table 1 summarized the statistics of training data. For the language model, the same monolingual data extracted from the Europarl are used. The word alignment is obtained by GIZA++ (Och and Ney, 2000) and the heuristics “growdiag-final” refinement rule (Koehn et al., 2003). Our translation system is an in-house phrasebased system analogous to Moses (Koehn et al., 2007). The baseline system is the triangulation method (Wu and Wang, 2007), including an interpolated model which linearly interpolate the direct and pivot translation model. 1 http://www.statmt.org/europarl We use WMT082 as our test data, which contains 2000 in-domain sentences and 2051 out-ofdomain sentences with single reference. The translation results are evaluated by caseinsensitive BLEU-4 metric (Papineni et al., 2002). The statistical significance tests using 95% confidence interval are measure</context>
</contexts>
<marker>Koehn, Och, Marcu, 2003</marker>
<rawString>Philipp Koehn, Franz J. Och, and Daniel Marcu. 2003. Statistical Phrase-Based Translation. In HLT-NAACL: Human Language Technology Conference of the North American Chapter of the Association for Computational Linguistics, pages 127-133.</rawString>
</citation>
<citation valid="true">
<authors>
<author>Philipp Koehn</author>
</authors>
<title>Statistical significance tests for machine translation evaluation.</title>
<date>2004</date>
<booktitle>In Proceedings of the 2004 Conference on Empirical Methods in Natural Language Processing (EMNLP),</booktitle>
<pages>388--395</pages>
<contexts>
<context position="16087" citStr="Koehn, 2004" startWordPosition="2539" endWordPosition="2540">use phrasebased system analogous to Moses (Koehn et al., 2007). The baseline system is the triangulation method (Wu and Wang, 2007), including an interpolated model which linearly interpolate the direct and pivot translation model. 1 http://www.statmt.org/europarl We use WMT082 as our test data, which contains 2000 in-domain sentences and 2051 out-ofdomain sentences with single reference. The translation results are evaluated by caseinsensitive BLEU-4 metric (Papineni et al., 2002). The statistical significance tests using 95% confidence interval are measured with paired bootstrap resampling (Koehn, 2004). 5.1 Results We compare 4 merging methods with the baseline system. The results are shown in Table 2 and Table 3. We find that the minimum method outperforms the others, achieving significant improvements over the baseline on all translation directions. The absolute improvements range from 0.61 (fr-de) to 1.54 (es-fr) in BLEU% score on in-domain test data, and range from 0.36 (frde) to 2.05 (fr-es) in BLEU% score on out-ofdomain test data. This indicates that our method is effective and robust in general. 2 http://www.statmt.org/wmt08/shared-task.html 1669 29.5 29 BLEU% 28.5 28 27.5 27 26.5 d</context>
</contexts>
<marker>Koehn, 2004</marker>
<rawString>Philipp Koehn. 2004. Statistical significance tests for machine translation evaluation. In Proceedings of the 2004 Conference on Empirical Methods in Natural Language Processing (EMNLP), pages 388-395.</rawString>
</citation>
<citation valid="true">
<authors>
<author>Philipp Koehn</author>
</authors>
<title>Europarl: A Parallel Corpus for Statistical Machine Translation.</title>
<date>2005</date>
<booktitle>In Proceedings of MT Summit X,</booktitle>
<pages>79--86</pages>
<contexts>
<context position="14979" citStr="Koehn, 2005" startWordPosition="2369" endWordPosition="2370">indicates the results are significantly better than the baseline (p&lt;0.05). System BLEU% de-es de-fr es-de es-fr fr-de fr-es Baseline 15.34 13.52 11.47 21.99 12.19 25.00 Minimum 15.77* 14.08* 11.99* 23.90* 12.55* 27.05* Maximum 13.41 11.83 10.17 20.48 10.83 22.75 Arithmetic mean 13.96 12.10 10.57 21.07 11.30 23.70 Geometric mean 15.09 13.30 11.52 23.32* 12.46* 26.22* Table 3: Comparison of different merging methods on out-of-domain test set. 5 Experiments on Europarl Corpus Our first experiment is carried out on Europarl1 corpus, which is a multi-lingual corpus including 21 European languages (Koehn, 2005). In our work, we perform translations among French (fr), German (de) and Spanish (es). Due to the richness of available language resources, we choose English (en) as the pivot language. Table 1 summarized the statistics of training data. For the language model, the same monolingual data extracted from the Europarl are used. The word alignment is obtained by GIZA++ (Och and Ney, 2000) and the heuristics “growdiag-final” refinement rule (Koehn et al., 2003). Our translation system is an in-house phrasebased system analogous to Moses (Koehn et al., 2007). The baseline system is the triangulation</context>
</contexts>
<marker>Koehn, 2005</marker>
<rawString>Philipp Koehn. 2005. Europarl: A Parallel Corpus for Statistical Machine Translation. In Proceedings of MT Summit X, pages 79-86.</rawString>
</citation>
<citation valid="false">
<authors>
<author>Philipp Koehn</author>
<author>Hieu Hoang</author>
<author>Alexanda Birch</author>
<author>Chris Callison-Burch</author>
<author>Marcello Federico</author>
<author>Nicola Bertoldi</author>
<author>Brooke Cowan</author>
<author>Wade Shen</author>
<author>Christine Moran</author>
<author>Richard Zens</author>
</authors>
<title>Chris Dyer, Ondrej Bojar,</title>
<date>2007</date>
<booktitle>In Proceedings of the 45th Annual Meeting of the Association for Computational Linguistics, demonstration session,</booktitle>
<pages>177--180</pages>
<location>Alexandra</location>
<contexts>
<context position="15537" citStr="Koehn et al., 2007" startWordPosition="2459" endWordPosition="2462">-lingual corpus including 21 European languages (Koehn, 2005). In our work, we perform translations among French (fr), German (de) and Spanish (es). Due to the richness of available language resources, we choose English (en) as the pivot language. Table 1 summarized the statistics of training data. For the language model, the same monolingual data extracted from the Europarl are used. The word alignment is obtained by GIZA++ (Och and Ney, 2000) and the heuristics “growdiag-final” refinement rule (Koehn et al., 2003). Our translation system is an in-house phrasebased system analogous to Moses (Koehn et al., 2007). The baseline system is the triangulation method (Wu and Wang, 2007), including an interpolated model which linearly interpolate the direct and pivot translation model. 1 http://www.statmt.org/europarl We use WMT082 as our test data, which contains 2000 in-domain sentences and 2051 out-ofdomain sentences with single reference. The translation results are evaluated by caseinsensitive BLEU-4 metric (Papineni et al., 2002). The statistical significance tests using 95% confidence interval are measured with paired bootstrap resampling (Koehn, 2004). 5.1 Results We compare 4 merging methods with th</context>
</contexts>
<marker>Koehn, Hoang, Birch, Callison-Burch, Federico, Bertoldi, Cowan, Shen, Moran, Zens, 2007</marker>
<rawString>Philipp Koehn, Hieu Hoang, Alexanda Birch, Chris Callison-Burch, Marcello Federico, Nicola Bertoldi, Brooke Cowan, Wade Shen, Christine Moran, Richard Zens, Chris Dyer, Ondrej Bojar, Alexandra Constantin, and Evan Herbst. 2007. Moses: Open Source Toolkit for Statistical Machine Translation. In Proceedings of the 45th Annual Meeting of the Association for Computational Linguistics, demonstration session, pages 177-180.</rawString>
</citation>
<citation valid="true">
<authors>
<author>Philipp Koehn</author>
<author>Alexandra Birch</author>
<author>Ralf Steinberger</author>
</authors>
<date>2009</date>
<booktitle>462 Machine Translation Systems for Europe. In Proceedings of the MT Summit XII.</booktitle>
<marker>Koehn, Birch, Steinberger, 2009</marker>
<rawString>Philipp Koehn, Alexandra Birch, and Ralf Steinberger. 2009. 462 Machine Translation Systems for Europe. In Proceedings of the MT Summit XII.</rawString>
</citation>
<citation valid="true">
<authors>
<author>Gregor Leusch</author>
<author>Aurélien Max</author>
<author>Josep Maria Crego</author>
<author>Hermann Ney</author>
</authors>
<title>Multi-Pivot Translation by System Combination.</title>
<date>2010</date>
<booktitle>In Proceedings of the 7th International Workshop on Spoken Language Translation,</booktitle>
<pages>299--306</pages>
<marker>Leusch, Max, Crego, Ney, 2010</marker>
<rawString>Gregor Leusch, Aurélien Max, Josep Maria Crego and Hermann Ney. 2010. Multi-Pivot Translation by System Combination. In Proceedings of the 7th International Workshop on Spoken Language Translation, pages 299-306.</rawString>
</citation>
<citation valid="true">
<authors>
<author>Franz Josef Och</author>
<author>Hermann Ney</author>
</authors>
<title>A comparison of alignment models for statistical machine translation.</title>
<date>2000</date>
<booktitle>In Proceedings of the 18th International Conference on Computational Linguistics,</booktitle>
<pages>1086--1090</pages>
<contexts>
<context position="15366" citStr="Och and Ney, 2000" startWordPosition="2432" endWordPosition="2435">parison of different merging methods on out-of-domain test set. 5 Experiments on Europarl Corpus Our first experiment is carried out on Europarl1 corpus, which is a multi-lingual corpus including 21 European languages (Koehn, 2005). In our work, we perform translations among French (fr), German (de) and Spanish (es). Due to the richness of available language resources, we choose English (en) as the pivot language. Table 1 summarized the statistics of training data. For the language model, the same monolingual data extracted from the Europarl are used. The word alignment is obtained by GIZA++ (Och and Ney, 2000) and the heuristics “growdiag-final” refinement rule (Koehn et al., 2003). Our translation system is an in-house phrasebased system analogous to Moses (Koehn et al., 2007). The baseline system is the triangulation method (Wu and Wang, 2007), including an interpolated model which linearly interpolate the direct and pivot translation model. 1 http://www.statmt.org/europarl We use WMT082 as our test data, which contains 2000 in-domain sentences and 2051 out-ofdomain sentences with single reference. The translation results are evaluated by caseinsensitive BLEU-4 metric (Papineni et al., 2002). The</context>
</contexts>
<marker>Och, Ney, 2000</marker>
<rawString>Franz Josef Och and Hermann Ney. 2000. A comparison of alignment models for statistical machine translation. In Proceedings of the 18th International Conference on Computational Linguistics, pages 1086-1090.</rawString>
</citation>
<citation valid="true">
<authors>
<author>Michael Paul</author>
<author>Andrew Finch</author>
<author>Paul R Dixon</author>
<author>Eiichiro Sumita</author>
</authors>
<title>Dialect Translation: Integrating Bayesian Co-segmentation Models with Pivot-based SMT.</title>
<date>2011</date>
<booktitle>In Proceedings of the 2011 Conference on Empirical Methods in Natural Language Processing,</booktitle>
<pages>1--9</pages>
<contexts>
<context position="1486" citStr="Paul et al., 2011" startWordPosition="206" endWordPosition="209">arl data and web data show that our method leads to significant improvements over the baseline systems. 1 Introduction Statistical Machine Translation (SMT) relies on large bilingual parallel data to produce high quality translation results. Unfortunately, for some language pairs, large bilingual corpora are not readily available. To alleviate the parallel data scarceness, a conventional solution is to introduce a “bridge” language (named pivot language) to connect the source and target language (de Gispert and Marino, 2006; Utiyama and Isahara, 2007; Wu and Wang, 2007; Bertoldi et al., 2008; Paul et al., 2011; El Kholy et al., 2013; Zahabi et al., 2013), where there are large amounts of source-pivot and pivot-target parallel corpora. Among various pivot-based approaches, the triangulation method (Cohn and Lapata, 2007; Wu and Wang, 2007) is a representative work in * This work was done when the first author was visiting Baidu. pivot-based machine translation. The approach proposes to build a source-target phrase table by merging the source-pivot and pivot-target phrase table. One of the key issues in this method is to estimate the translation probabilities for the generated source-target phrase pa</context>
</contexts>
<marker>Paul, Finch, Dixon, Sumita, 2011</marker>
<rawString>Michael Paul, Andrew Finch, Paul R. Dixon and Eiichiro Sumita. 2011. Dialect Translation: Integrating Bayesian Co-segmentation Models with Pivot-based SMT. In Proceedings of the 2011 Conference on Empirical Methods in Natural Language Processing, pages 1-9.</rawString>
</citation>
<citation valid="true">
<authors>
<author>Michael Paul</author>
<author>Eiichiro Sumita</author>
</authors>
<title>Translation Quality Indicators for Pivot-based Statistical MT.</title>
<date>2011</date>
<booktitle>In Proceedings of the 5th International Joint Conference on Natural Language Processing,</booktitle>
<pages>811--818</pages>
<marker>Paul, Sumita, 2011</marker>
<rawString>Michael Paul and Eiichiro Sumita. 2011. Translation Quality Indicators for Pivot-based Statistical MT. In Proceedings of the 5th International Joint Conference on Natural Language Processing, pages 811-818.</rawString>
</citation>
<citation valid="true">
<authors>
<author>Kishore Papineni</author>
<author>Salim Roukos</author>
<author>Todd Ward</author>
<author>Wei-Jing Zhu</author>
</authors>
<title>BLEU: a Method for Automatic Evaluation of Machine Translation.</title>
<date>2002</date>
<booktitle>In Proceedings of the 40th Annual Meeting of the Association for Computation Linguistics,</booktitle>
<pages>311--319</pages>
<contexts>
<context position="15961" citStr="Papineni et al., 2002" startWordPosition="2521" endWordPosition="2524"> GIZA++ (Och and Ney, 2000) and the heuristics “growdiag-final” refinement rule (Koehn et al., 2003). Our translation system is an in-house phrasebased system analogous to Moses (Koehn et al., 2007). The baseline system is the triangulation method (Wu and Wang, 2007), including an interpolated model which linearly interpolate the direct and pivot translation model. 1 http://www.statmt.org/europarl We use WMT082 as our test data, which contains 2000 in-domain sentences and 2051 out-ofdomain sentences with single reference. The translation results are evaluated by caseinsensitive BLEU-4 metric (Papineni et al., 2002). The statistical significance tests using 95% confidence interval are measured with paired bootstrap resampling (Koehn, 2004). 5.1 Results We compare 4 merging methods with the baseline system. The results are shown in Table 2 and Table 3. We find that the minimum method outperforms the others, achieving significant improvements over the baseline on all translation directions. The absolute improvements range from 0.61 (fr-de) to 1.54 (es-fr) in BLEU% score on in-domain test data, and range from 0.36 (frde) to 2.05 (fr-es) in BLEU% score on out-ofdomain test data. This indicates that our metho</context>
</contexts>
<marker>Papineni, Roukos, Ward, Zhu, 2002</marker>
<rawString>Kishore Papineni, Salim Roukos, Todd Ward and Wei-Jing Zhu. 2002. BLEU: a Method for Automatic Evaluation of Machine Translation. In Proceedings of the 40th Annual Meeting of the Association for Computation Linguistics, pages 311-319.</rawString>
</citation>
<citation valid="true">
<authors>
<author>Rie Tanaka</author>
<author>Yohei Murakami</author>
<author>Toru Ishida</author>
</authors>
<title>Context-Based Approach for Pivot Translation Services.</title>
<date>2009</date>
<booktitle>In the Twenty-first International Conference on Artificial Intelligence,</booktitle>
<pages>1555--1561</pages>
<marker>Tanaka, Murakami, Ishida, 2009</marker>
<rawString>Rie Tanaka, Yohei Murakami and Toru Ishida. 2009. Context-Based Approach for Pivot Translation Services. In the Twenty-first International Conference on Artificial Intelligence, pages 1555-1561.</rawString>
</citation>
<citation valid="true">
<authors>
<author>Jörg Tiedemann</author>
</authors>
<title>Character-Based Pivot Translation for Under-Resourced Languages and Domains.</title>
<date>2012</date>
<booktitle>In Proceedings of the 13th Conference of the European Chapter of the Association for Computational Linguistics,</booktitle>
<pages>141--151</pages>
<marker>Tiedemann, 2012</marker>
<rawString>Jörg Tiedemann. 2012. Character-Based Pivot Translation for Under-Resourced Languages and Domains. In Proceedings of the 13th Conference of the European Chapter of the Association for Computational Linguistics, pages 141-151.</rawString>
</citation>
<citation valid="true">
<authors>
<author>Masatoshi Tsuchiya</author>
<author>Ayu Purwarianti</author>
<author>ToshiyukiWakita</author>
<author>Seiichi Nakagawa</author>
</authors>
<title>Expanding Indonesian-Japanese Small Translation Dictionary Using a Pivot Language.</title>
<date>2007</date>
<booktitle>In Proceedings of the ACL 2007 Demo and Poster Sessions,</booktitle>
<pages>197--200</pages>
<marker>Tsuchiya, Purwarianti, ToshiyukiWakita, Nakagawa, 2007</marker>
<rawString>Masatoshi Tsuchiya, Ayu Purwarianti, ToshiyukiWakita and Seiichi Nakagawa. 2007. Expanding Indonesian-Japanese Small Translation Dictionary Using a Pivot Language. In Proceedings of the ACL 2007 Demo and Poster Sessions, pages 197-200.</rawString>
</citation>
<citation valid="true">
<authors>
<author>Takashi Tsunakawa</author>
<author>Naoaki Okazaki</author>
<author>Jun&apos;ichi Tsujii</author>
</authors>
<title>Building a Bilingual Lexicon Using Phrase-based Statistical Machine Translation via a Pivot Language.</title>
<date>2010</date>
<booktitle>In Proceedings of the 22th International Conference on Computational Linguistics (Coling),</booktitle>
<pages>127--130</pages>
<marker>Tsunakawa, Okazaki, Tsujii, 2010</marker>
<rawString>Takashi Tsunakawa, Naoaki Okazaki and Jun&apos;ichi Tsujii. 2010. Building a Bilingual Lexicon Using Phrase-based Statistical Machine Translation via a Pivot Language. In Proceedings of the 22th International Conference on Computational Linguistics (Coling), pages 127-130.</rawString>
</citation>
<citation valid="true">
<authors>
<author>Masao Utiyama</author>
<author>Hitoshi Isahara</author>
</authors>
<title>A Comparison of Pivot Methods for PhraseBased Statistical Machine Translation.</title>
<date>2007</date>
<booktitle>In Proceedings of Human Language Technology: the Conference of the North American Chapter of the Association for Computational Linguistics,</booktitle>
<pages>484--491</pages>
<contexts>
<context position="1425" citStr="Utiyama and Isahara, 2007" startWordPosition="194" endWordPosition="197">ce-pivot and pivot-target phrase pairs. Experimental results on Europarl data and web data show that our method leads to significant improvements over the baseline systems. 1 Introduction Statistical Machine Translation (SMT) relies on large bilingual parallel data to produce high quality translation results. Unfortunately, for some language pairs, large bilingual corpora are not readily available. To alleviate the parallel data scarceness, a conventional solution is to introduce a “bridge” language (named pivot language) to connect the source and target language (de Gispert and Marino, 2006; Utiyama and Isahara, 2007; Wu and Wang, 2007; Bertoldi et al., 2008; Paul et al., 2011; El Kholy et al., 2013; Zahabi et al., 2013), where there are large amounts of source-pivot and pivot-target parallel corpora. Among various pivot-based approaches, the triangulation method (Cohn and Lapata, 2007; Wu and Wang, 2007) is a representative work in * This work was done when the first author was visiting Baidu. pivot-based machine translation. The approach proposes to build a source-target phrase table by merging the source-pivot and pivot-target phrase table. One of the key issues in this method is to estimate the transl</context>
<context position="6700" citStr="Utiyama and Isahara, 2007" startWordPosition="992" endWordPosition="995">method, which interpolates the direct and pivot translation model. See Figure 2. (b) and (c) for further illustration. 1666 The remainder of this paper is organized as follows. In Section 2, we describe the related work. We introduce the co-occurrence count method in Section 3, and the mixed model in Section 4. In Section 5 and Section 6, we describe and analyze the experiments. Section 7 gives a conclusion of the paper. 2 Related Work Several methods have been proposed for pivotbased translation. Typically, they can be classified into 3 kinds as follows: Transfer Method: The transfer method (Utiyama and Isahara, 2007; Wang et al., 2008; Costa-jussà et al., 2011) connects two translation systems: a source-pivot MT system and a pivottarget MT system. Given a source sentence, (1) the source-pivot MT system translates it into the pivot language, (2) and the pivot-target MT system translates the pivot sentence into the target sentence. During each step (source to pivot and pivot to target), multiple translation outputs will be generated, thus a minimum Bayes-risk system combination method is often used to select the optimal sentence (González-Rubio et al., 2011; Duh et al., 2011). The problem with the transfer</context>
<context position="8463" citStr="Utiyama and Isahara, 2007" startWordPosition="1264" endWordPosition="1267">arget sentences or/and combine the target sentences with translated source sentences (Utiyama et al., 2008; Wu and Wang, 2009). However, it is difficult to build a high quality translation system with a corpus created by a machine translation system. Triangulation Method: The triangulation method obtains source-target phrase table by merging source-pivot and pivot-target phrase table entries with identical pivot language phrases and multiplying corresponding posterior probabilities (Wu and Wang, 2007; Cohn and Lapata, 2007), which has been shown to work better than the other pivot approaches (Utiyama and Isahara, 2007). A problem of this approach is that the probability space of the source-target phrase pairs is non-uniformity due to the mismatching of the pivot phrase. 3 Our Approach In this section, we will introduce our method for learning a source-target phrase translation model with a pivot language as a bridge. We extract the co-occurrence count of phrase pairs for each language pair with a source-pivot and a pivot-target corpus. Then we generate the source-target phrase pairs with induced co-occurrence information. Finally, we compute translation probabilities using the standard phrase-based SMT trai</context>
</contexts>
<marker>Utiyama, Isahara, 2007</marker>
<rawString>Masao Utiyama and Hitoshi Isahara. 2007. A Comparison of Pivot Methods for PhraseBased Statistical Machine Translation. In Proceedings of Human Language Technology: the Conference of the North American Chapter of the Association for Computational Linguistics, pages 484-491.</rawString>
</citation>
<citation valid="true">
<authors>
<author>Masao Utiyama</author>
<author>Andrew Finch</author>
<author>Hideo Okuma</author>
<author>Michael Paul</author>
</authors>
<title>Hailong Cao, Hirofumi Yamamoto, Keiji Yasuda,and Eiichiro Sumita.</title>
<date>2008</date>
<booktitle>The NICT/ATR speech Translation System for IWSLT</booktitle>
<pages>77--84</pages>
<contexts>
<context position="7943" citStr="Utiyama et al., 2008" startWordPosition="1189" endWordPosition="1192">s to decode twice. On one hand, the time cost is doubled; on the other hand, the translation error of the source-pivot translation system will be transferred to the pivot-target translation. Synthetic Method: It aims to create a synthetic source-target corpus by: (1) translate the pivot part in source-pivot corpus into target language with a pivot-target model; (2) translate the pivot part in pivot-target corpus into source language with a pivot-source model; (3) combine the source sentences with translated target sentences or/and combine the target sentences with translated source sentences (Utiyama et al., 2008; Wu and Wang, 2009). However, it is difficult to build a high quality translation system with a corpus created by a machine translation system. Triangulation Method: The triangulation method obtains source-target phrase table by merging source-pivot and pivot-target phrase table entries with identical pivot language phrases and multiplying corresponding posterior probabilities (Wu and Wang, 2007; Cohn and Lapata, 2007), which has been shown to work better than the other pivot approaches (Utiyama and Isahara, 2007). A problem of this approach is that the probability space of the source-target </context>
</contexts>
<marker>Utiyama, Finch, Okuma, Paul, 2008</marker>
<rawString>Masao Utiyama, Andrew Finch, Hideo Okuma, Michael Paul, Hailong Cao, Hirofumi Yamamoto, Keiji Yasuda,and Eiichiro Sumita. 2008. The NICT/ATR speech Translation System for IWSLT 2008. In Proceedings of the International Workshop on Spoken Language Translation, pages 77-84.</rawString>
</citation>
<citation valid="true">
<authors>
<author>Haifeng Wang</author>
<author>Hua Wu</author>
<author>Xiaoguang Hu</author>
<author>Zhanyi Liu</author>
<author>Jianfeng Li</author>
<author>Dengjun Ren</author>
<author>Zhengyu Niu</author>
</authors>
<date>2008</date>
<booktitle>The TCH Machine Translation System for IWSLT</booktitle>
<pages>124--131</pages>
<contexts>
<context position="6719" citStr="Wang et al., 2008" startWordPosition="996" endWordPosition="999">the direct and pivot translation model. See Figure 2. (b) and (c) for further illustration. 1666 The remainder of this paper is organized as follows. In Section 2, we describe the related work. We introduce the co-occurrence count method in Section 3, and the mixed model in Section 4. In Section 5 and Section 6, we describe and analyze the experiments. Section 7 gives a conclusion of the paper. 2 Related Work Several methods have been proposed for pivotbased translation. Typically, they can be classified into 3 kinds as follows: Transfer Method: The transfer method (Utiyama and Isahara, 2007; Wang et al., 2008; Costa-jussà et al., 2011) connects two translation systems: a source-pivot MT system and a pivottarget MT system. Given a source sentence, (1) the source-pivot MT system translates it into the pivot language, (2) and the pivot-target MT system translates the pivot sentence into the target sentence. During each step (source to pivot and pivot to target), multiple translation outputs will be generated, thus a minimum Bayes-risk system combination method is often used to select the optimal sentence (González-Rubio et al., 2011; Duh et al., 2011). The problem with the transfer method is that it </context>
</contexts>
<marker>Wang, Wu, Hu, Liu, Li, Ren, Niu, 2008</marker>
<rawString>Haifeng Wang, Hua Wu, Xiaoguang Hu, Zhanyi Liu, Jianfeng Li, Dengjun Ren, and Zhengyu Niu. 2008. The TCH Machine Translation System for IWSLT 2008. In Proceedings of the International Workshop on Spoken Language Translation, pages 124-131.</rawString>
</citation>
<citation valid="true">
<authors>
<author>Hua Wu</author>
<author>Haifeng Wang</author>
</authors>
<title>Pivot Language Approach for Phrase-Based Statistical Machine Translation.</title>
<date>2007</date>
<booktitle>In Proceedings of 45th Annual Meeting of the Association for Computational Linguistics,</booktitle>
<pages>856--863</pages>
<contexts>
<context position="1444" citStr="Wu and Wang, 2007" startWordPosition="198" endWordPosition="201">hrase pairs. Experimental results on Europarl data and web data show that our method leads to significant improvements over the baseline systems. 1 Introduction Statistical Machine Translation (SMT) relies on large bilingual parallel data to produce high quality translation results. Unfortunately, for some language pairs, large bilingual corpora are not readily available. To alleviate the parallel data scarceness, a conventional solution is to introduce a “bridge” language (named pivot language) to connect the source and target language (de Gispert and Marino, 2006; Utiyama and Isahara, 2007; Wu and Wang, 2007; Bertoldi et al., 2008; Paul et al., 2011; El Kholy et al., 2013; Zahabi et al., 2013), where there are large amounts of source-pivot and pivot-target parallel corpora. Among various pivot-based approaches, the triangulation method (Cohn and Lapata, 2007; Wu and Wang, 2007) is a representative work in * This work was done when the first author was visiting Baidu. pivot-based machine translation. The approach proposes to build a source-target phrase table by merging the source-pivot and pivot-target phrase table. One of the key issues in this method is to estimate the translation probabilities</context>
<context position="8342" citStr="Wu and Wang, 2007" startWordPosition="1244" endWordPosition="1247">target corpus into source language with a pivot-source model; (3) combine the source sentences with translated target sentences or/and combine the target sentences with translated source sentences (Utiyama et al., 2008; Wu and Wang, 2009). However, it is difficult to build a high quality translation system with a corpus created by a machine translation system. Triangulation Method: The triangulation method obtains source-target phrase table by merging source-pivot and pivot-target phrase table entries with identical pivot language phrases and multiplying corresponding posterior probabilities (Wu and Wang, 2007; Cohn and Lapata, 2007), which has been shown to work better than the other pivot approaches (Utiyama and Isahara, 2007). A problem of this approach is that the probability space of the source-target phrase pairs is non-uniformity due to the mismatching of the pivot phrase. 3 Our Approach In this section, we will introduce our method for learning a source-target phrase translation model with a pivot language as a bridge. We extract the co-occurrence count of phrase pairs for each language pair with a source-pivot and a pivot-target corpus. Then we generate the source-target phrase pairs with </context>
<context position="12632" citStr="Wu and Wang, 2007" startWordPosition="1988" endWordPosition="1991">nment information ܽ, which can be obtained as Equation 11 shows. ܽ ൌ ሼሺݏ, ݐሻ|∃: ሺݏ, ሻ ∈ ܽଵ&amp;ሺ, ݐሻ ∈ ܽଶሽ (11) where ܽଵ and ܽଶ indicate the word alignment information in the phrase pair ሺݏ̅, ̅ሻ and ሺ̅, ݐ̅ሻ respectively. 4 Integrate with Direct Translation If a standard source-target bilingual corpus is available, we can train a direct translation model. Thus we can integrate the direct model and the pivot model to obtain further improvements. We propose a mixed model by merging the cooccurrence count in direct translation and pivot translation. Besides, we also employ an interpolated model (Wu and Wang, 2007) by merging the direct translation model and pivot translation model using a linear interpolation. 4.1 Mixed Model Given ݊ pivot languages, the co-occurrence count can be estimated using the method described in Section 3.1. Then the co-occurrence count and the lexical weight of the mixed model can be estimated with the following Equation 12 and 13.  ܿሺݏ, ݐሻ ൌ  ܿሺݏ, ݐሻ ୀ  ఠሺݏ̅|ݐ̅, ܽሻ ൌ  ߙ ఠ,ሺݏ̅|ݐ̅, ܽሻ ୀ where ܿሺݏ, ݐሻ and ఠ,ሺݏ̅|ݐ̅, ܽሻ are the cooccurrence count and lexical weight in the direct translation model respectively. ܿሺݏ, ݐሻ and ఠ,ሺݏ̅|ݐ̅, ܽሻ denote the co-occurrence co</context>
<context position="15606" citStr="Wu and Wang, 2007" startWordPosition="2470" endWordPosition="2473">work, we perform translations among French (fr), German (de) and Spanish (es). Due to the richness of available language resources, we choose English (en) as the pivot language. Table 1 summarized the statistics of training data. For the language model, the same monolingual data extracted from the Europarl are used. The word alignment is obtained by GIZA++ (Och and Ney, 2000) and the heuristics “growdiag-final” refinement rule (Koehn et al., 2003). Our translation system is an in-house phrasebased system analogous to Moses (Koehn et al., 2007). The baseline system is the triangulation method (Wu and Wang, 2007), including an interpolated model which linearly interpolate the direct and pivot translation model. 1 http://www.statmt.org/europarl We use WMT082 as our test data, which contains 2000 in-domain sentences and 2051 out-ofdomain sentences with single reference. The translation results are evaluated by caseinsensitive BLEU-4 metric (Papineni et al., 2002). The statistical significance tests using 95% confidence interval are measured with paired bootstrap resampling (Koehn, 2004). 5.1 Results We compare 4 merging methods with the baseline system. The results are shown in Table 2 and Table 3. We f</context>
<context position="18795" citStr="Wu and Wang (2007)" startWordPosition="2958" endWordPosition="2961">ual corpora and only a little source-target bilingual data. Thus, we randomly select 10K, 50K, 100K, 200K, 500K, 1M, 1.5M sentence pairs from the source-target bilingual corpora to simulate the lack of sourcetarget data. With these corpora, we train several direct translation models with different scales of bilingual data. We interpolate each direct translation model with the pivot model (both triangulation method and co-occurrence count method) to obtain the interpolated model respectively. We also mix the direct model and pivot model using the method described in Section 4.1. Following 1670 Wu and Wang (2007), we set α = 0.9, αଵ = 0.1, β = 0.9 and βଵ = 0.1 empirically. The experiments are carried out on 6 translation directions: German-Spanish, German-French, SpanishGerman, Spanish-French, French-German and French-Spanish. The results are shown in Figure 3. We only list the results on in-domain test sets. The trend of the results on out-of domain test sets is similar with in-domain test sets. The results are explained as follows: (1) Comparison of Pivot Translation and Direct Translation The pivot translation models are better than the direct translation models trained on a small source-target b</context>
</contexts>
<marker>Wu, Wang, 2007</marker>
<rawString>Hua Wu and Haifeng Wang. 2007. Pivot Language Approach for Phrase-Based Statistical Machine Translation. In Proceedings of 45th Annual Meeting of the Association for Computational Linguistics, pages 856-863.</rawString>
</citation>
<citation valid="true">
<authors>
<author>Hua Wu</author>
<author>Haifeng Wang</author>
</authors>
<title>Revisiting Pivot Language Approach for Machine Translation.</title>
<date>2009</date>
<booktitle>In Proceedings of the 47th Annual Meeting of the Association for Computational Linguistics and the 4th IJCNLP of the AFNLP,</booktitle>
<pages>154--162</pages>
<contexts>
<context position="7963" citStr="Wu and Wang, 2009" startWordPosition="1193" endWordPosition="1196">one hand, the time cost is doubled; on the other hand, the translation error of the source-pivot translation system will be transferred to the pivot-target translation. Synthetic Method: It aims to create a synthetic source-target corpus by: (1) translate the pivot part in source-pivot corpus into target language with a pivot-target model; (2) translate the pivot part in pivot-target corpus into source language with a pivot-source model; (3) combine the source sentences with translated target sentences or/and combine the target sentences with translated source sentences (Utiyama et al., 2008; Wu and Wang, 2009). However, it is difficult to build a high quality translation system with a corpus created by a machine translation system. Triangulation Method: The triangulation method obtains source-target phrase table by merging source-pivot and pivot-target phrase table entries with identical pivot language phrases and multiplying corresponding posterior probabilities (Wu and Wang, 2007; Cohn and Lapata, 2007), which has been shown to work better than the other pivot approaches (Utiyama and Isahara, 2007). A problem of this approach is that the probability space of the source-target phrase pairs is non-</context>
</contexts>
<marker>Wu, Wang, 2009</marker>
<rawString>Hua Wu and Haifeng Wang. 2009. Revisiting Pivot Language Approach for Machine Translation. In Proceedings of the 47th Annual Meeting of the Association for Computational Linguistics and the 4th IJCNLP of the AFNLP, pages 154-162.</rawString>
</citation>
<citation valid="false">
<authors>
<author>Samira Tofighi Zahabi</author>
</authors>
<title>Somayeh Bakhshaei and Shahram Khadivi. Using Context Vectors in Improving a Machine Translation System with Bridge Language.</title>
<booktitle>In Proceedings of the 51st Annual Meeting of the Association for Computational Linguistics,</booktitle>
<pages>318--322</pages>
<marker>Zahabi, </marker>
<rawString>Samira Tofighi Zahabi, Somayeh Bakhshaei and Shahram Khadivi. Using Context Vectors in Improving a Machine Translation System with Bridge Language. In Proceedings of the 51st Annual Meeting of the Association for Computational Linguistics, pages 318-322.</rawString>
</citation>
</citationList>
</algorithm>
</algorithms>
<?xml version="1.0" encoding="UTF-8"?>
<algorithms version="110505">
<algorithm name="SectLabel" version="110505">
<variant no="0" confidence="0.000000">
<bodyText confidence="0.761589">
Tailor knowledge graph for query understanding: linking intent topics by
propagation
</bodyText>
<note confidence="0.724801">
Shi Zhao Yan Zhang
</note>
<email confidence="0.752631">
z.s@pku.edu.cn zhy@cis.pku.edu.cn
</email>
<note confidence="0.945198">
Department of Machine Intelligence, Peking University, Beijing, China
Key Laboratory on Machine Perception, Ministry of Education, Beijing, China
</note>
<sectionHeader confidence="0.940508" genericHeader="abstract">
Abstract
</sectionHeader>
<bodyText confidence="0.999889304347826">
Knowledge graphs are recently used for
enriching query representations in an
entity-aware way for the rich facts or-
ganized around entities in it. How-
ever, few of the methods pay attention to
non-entity words and clicked websites in
queries, which also help conveying user
intent. In this paper, we tackle the prob-
lem of intent understanding with innova-
tively representing entity words, refiners
and clicked urls as intent topics in a uni-
fied knowledge graph based framework,
in a way to exploit and expand knowl-
edge graph which we call ‘tailor’. We
collaboratively exploit global knowledge
in knowledge graphs and local contexts in
query log to initialize intent representa-
tion, then propagate the enriched features
in a graph consisting of intent topics us-
ing an unsupervised algorithm. The ex-
periments prove intent topics with knowl-
edge graph enriched features significantly
enhance intent understanding.
</bodyText>
<sectionHeader confidence="0.992512" genericHeader="introduction">
1 Introduction
</sectionHeader>
<bodyText confidence="0.999959240740741">
Query understanding is the process of generating a
representation which characterizes a user’s search
intent (Croft et al., 2010), which is of vital im-
portance for information retrieval. However, users
are remarkably laconic in describing their infor-
mation needs due to anomalous state of knowledge
(Belkin et al., 1982), resulting in vague and under-
specified queries, which makes it especially dif-
ficult to understand and locate what they intended
for in mountains of web data. The problem is often
significantly compounded that people convey their
intent rather in a series of behaviors called a search
session than a single query, leaving a wealth of
clues including query reformulations, page visits,
dwell times, etc. What’s more, as entities are tak-
ing center stage (Yin and Shah, 2010), string-level
or phrase-level modeling of intent soon hits the
bottleneck, calling for an entity-aware perspective.
Knowledge repositories, better known as
knowledge graphs, such as Wikipedia, DBpedia
and Freebase, have been recently utilized for en-
hancing query understanding for the large amounts
of world knowledge they’ve harvested about en-
tities and facts. A widely accepted way to use
knowledge graph is tying queries with it by anno-
tating entities in them, also known as entity link-
ing.
However, information need is conveyed through
more than entities. Quite a few non-entity words,
aka refiners or modifiers, as well as many urls are
barely included in knowledge graph, while they
play an irreplaceable role in intent understand-
ing. For example, a user may query toyota, volvo
or just enter car soup, cars for sale and click
www.carsoup.com, which should be encoded in
a form that we could perceive their closeness in
intent. That’s why at-a-glance info cards about
merely recognized entity in the query are far from
enough and previous methods disregarding refin-
ers and urls are too limited to cover queries in ma-
jority.
We move one step further to tailor knowledge
graph for representing more than entity words. We
collect refiners and clicked urls along with en-
tity words and model intents they represent us-
ing knowledge graph based features. We use
Freebase1, one of the largest available knowledge
graph, in our work and our method can be easily
generalized to other knowledge repositories.
We put up an idea of intent topic which can
be query words or urls, whether mean an entity
or not, representing an atomic information need.
We identify them with intent features by exploit-
ing global knowledge in Freebase and local con-
</bodyText>
<footnote confidence="0.958841">
1http://www.freebase.com
</footnote>
<note confidence="0.817330666666667">
1070
Proceedings of the 2014 Conference on Empirical Methods in Natural Language Processing (EMNLP), pages 1070–1080,
October 25-29, 2014, Doha, Qatar. c�2014 Association for Computational Linguistics
</note>
<bodyText confidence="0.999295666666667">
texts in query sessions. Notice the new concept
here is distinguished from query intent or query
facet in previous literature for it is in a holistic
view, not specifically meaning subtopics around a
certain query.
Our intuitive observations as follows inspire us
to represent intent features with topics and do-
mains in knowledge graph and propagate the en-
riched features in the intent topic graph.
</bodyText>
<listItem confidence="0.971683">
1) Query words and urls within the same session
tend to indicate the same query intent.
2) Intent topics sharing similar query intent often
relate to similar topics in knowledge graph.
3) Knowledge graph domains sketch the query in-
tent briefly.
</listItem>
<bodyText confidence="0.995888181818182">
Observation 1 indicates domain coherency
within sessions is a good starting point to gener-
ate intent features, along with Observation 2 and 3
lay the basis of proximity that the propagation rely
on.
To the best of our knowledge, we’re the first to
represent intent behind entity words, refiners and
urls in a unified knowledge graph based frame-
work, in a way to exploit and expand knowledge
graph which we call ‘tailor’.
Our contributions include:
</bodyText>
<listItem confidence="0.932308181818182">
• An innovative and unified framework to rep-
resent intent topics, whether they can directly
link to an entity in knowledge graph or not.
• A novel algorithm to generate a specified in-
tent topic graph, which enables learning in-
tent features in an unsupervised propagation
method.
• With intent topic graph we can better under-
stand user intent conducting session-based
contextualization and potentially find highly-
related intent topic.
</listItem>
<bodyText confidence="0.999726625">
The rest of the paper is organized as follows. Sec-
tion 2 tells our methods to map queries to Freebase
and initialize intent features. Section 3 is about
how we model intent topics in a unified graph and
the propagation framework to learn intent features.
We provide experiments and analysis in Section 4.
Related work and conclusions are presented at the
end of the paper.
</bodyText>
<sectionHeader confidence="0.854552" genericHeader="method">
2 Labeling intent topic nodes with
Freebase-enriched features
</sectionHeader>
<bodyText confidence="0.999976791666667">
In Freebase, facts around a certain topic and multi-
faceted intents they reflect is more like a global
domain distribution, what facet do users exactly
intend for is difficult to locate until in a specified
context, namely a query session.
We take a line in query log as a query, exhibit-
ing an interaction with the search engine, includ-
ing query words and page clicks. And a sequence
of queries with a certain time interval constitute
a session, completely conveying an information
need.
In existing knowledge graph, only a small part
of urls are contained in views of web pages be-
yond number online. Even for query words, we
can merely get access to some of them, which we
call entity words and the rest refiners. To avoid
misunderstanding, the url intent topics in the fol-
lowing will specially refer to the clicks without di-
rectly matched concepts in knowledge graph, oth-
erwise they’ll be taken as entity intent topic.
In this section, we propose a framework of
knowledge graph enriched representation of in-
tent topics, the following propagation in Section3
bases on it.
</bodyText>
<subsectionHeader confidence="0.99127">
2.1 Freebase as a knowledge graph
</subsectionHeader>
<bodyText confidence="0.99692285">
Freebase has over 39 million concepts, aka top-
ics, about real-world entities like people, places
and things stored as nodes in a graph. They’re
linked to each other with annotated egdes named
as property. These edges actually represent facts.
There are over a billion such facts or relations that
make up the graph and they’re all available for
free. Properties are grouped into types, types are
grouped into domains, which gives a broad view
of knowledge in addtion to specific topics.
We can tap into Freebase through dump data or
API2. In our work, we retrieve related Freebase
topics with relevance scores for entity words via
Freebase search API, which is based on combina-
tion of topic’s inbound and outbound link counts
in Freebase and Wikipedia as well as a popularity
score computed by Google, and all the facts about
a given topic through Freebase topic API. We use
T = {t1, t2, ...tn}, D = {d1, d2, ...dn} to denote
all Freebase topics and domains used in our work.
</bodyText>
<subsectionHeader confidence="0.965673">
2.2 Enriching entities and queries with
Freebase
</subsectionHeader>
<bodyText confidence="0.99984675">
We represent a query’s candidate intent topics by
three sets, Eq, Rq, Cq, where Eq includes entity
words and clicks which have equivalents in Free-
base, Rq the refiner words and Cq the rest clicks.
</bodyText>
<footnote confidence="0.558928">
2http://developers.google.com/freebase/
</footnote>
<page confidence="0.585045">
1071
</page>
<bodyText confidence="0.999485666666667">
Global knowledge in Freebase can directly enrich
each e in Eq with Freebase topics represented in
vector te, for each candidate topic there’s a Free-
base domain distribution vector dt. As for the rest
in Rq and Cq, they can learn features in later prop-
agation process.
For any topic ti in te, the relevance of entity
words e and knowledge graph topic ti is estimated
as follows:
</bodyText>
<equation confidence="0.896795285714286">
te _ Relevance5core(e, ti) 1
i maxtj∈T Relevance5core(e,tj) ( )
And the domain vector dti for ti is:
dtij = pr(dj|ti) (2)
Pdk ∈D pr(dk|ti)
# of links of ti in domain dj
pr(dj  |ti) = # of all links in domain dj
</equation>
<bodyText confidence="0.911892666666667">
Then we’ll get a knowledge graph enriched in-
tent description of the query by combining that of
e, r, c.
</bodyText>
<equation confidence="0.9998586">
X X X tciwq(c)
tq = tei wq(e) + triwq(r)+
i r∈Rq c∈Cq
e∈Eq
wq(e) = NCountq(e)cp(e) (5)
</equation>
<bodyText confidence="0.999892083333333">
Here te tr tc correspond to the topic vector of each
entity, refiner and click respectively. The weight
indicates how dominant it is in conveying intent
in the query. It is in proportion to the normalized
count as well as each occurrence’s quality denoted
by cp(e). Such as for entity words in Equation (5),
the quality cp(e) can be estimated with the help of
entity linking methods, which describes the proba-
bility of e as a candidate reference. That for clicks
and refiners will be explained later.
The query’s domain feature can be calculated as
follows:
</bodyText>
<equation confidence="0.9975975">
P tj∈T dtj
i tq
dq j
i = P P (6)
tj∈T dtj k tq
dk∈D j
</equation>
<bodyText confidence="0.98783325">
It describes the probability of query q in domain
di, in which tqj can be calculated by Equation (4)
and dtj
i via facts around topic tj by Equation (2).
</bodyText>
<subsectionHeader confidence="0.6360905">
2.3 Contextualized intent depiction of
sessions
</subsectionHeader>
<bodyText confidence="0.999201272727273">
The aforesaid enriched features we get about
queries rely heavily on global knowledge in Free-
base, reflecting prior distribution in the feature
space. In this part, we derive a contextualized de-
scription of session intent in a local view by aggre-
gating all the global knowledge we get about the
session’s queries. The ambiguity of a single query
can be alleviated by looking at the dominant do-
main within the session.
The intent features ts and ds of session s can
be represented by computations on its query set
</bodyText>
<equation confidence="0.9866035">
Qs = {q1, q2, ...qn} with time-order decay.
tqαrank(q)
ts = Pq∈Qs i tqαrank(q) (7)
Ptj ∈T Pq∈Qs j
</equation>
<bodyText confidence="0.939918333333333">
where we put an exponential decay controlled by
decay factor α. We get domain feature the same
way as Equation (6).
We’ll put up an unsupervised method of learn-
ing knowledge graph based intent representation
of refiners and clicks in the following part.
3 Propagating intent features in the
intent topic graph
In this section, our idea is to characterize entities,
refiners and urls uniformly as intent topics, tailor-
ing knowledge graph to intent topic graph so as to
enrich representations by propagation.
</bodyText>
<subsectionHeader confidence="0.998358">
3.1 Modeling intent topic graph
</subsectionHeader>
<bodyText confidence="0.99992525">
As in last section, with ds featuring the context,
candidate intent topics in sessions can make intent
topic nodes now. We use the concept intent topic
to stress words with local contexts tell a specified
information need, thus making a node. Taking en-
tity word fl as an example, it can be recognized
as the topic Florida in Freebase, while the intent
behind it can hardly be mapped to a single intent
topic, such as travel domain in hollywood fl, ed-
ucation domain in community college in florida,
and florida department of health actually convey
intent in government domain.
So each intent topic node is identified with its
name string and Freebase-enriched intent features
t and d. They’re directly linked by co-occurring
in the same line in the query log and implicitly
related via intent features similarities, so that con-
stitute a large graph G =&lt; V, E, W &gt;, where
bw E W denotes an explicit edge weight and
bv E V an intent topic. With intent topics and
their relations modeled in a graph, we can better
understand the query space so as to find the in-
tended query faster. We realize it by aggregating
massive sessions.
</bodyText>
<page confidence="0.456881">
1072
</page>
<bodyText confidence="0.9975225">
The implicit intent similarity ISim of any node
pair n and v can be encoded as follows.
</bodyText>
<equation confidence="0.62042">
ISimn,v = βSSimn,v +γDSimn,v +ηT Simn,v
</equation>
<bodyText confidence="0.989584227272727">
(8)
where SSim denotes the names’ string similar-
ity, DSim the similarity of their domain feature
and T Sim the topic vector similarity, with β, γ
and η controlling the weight. The parameters may
vary due to different scenarios. We just provide
a framework of modeling nodes’ intent features,
which actually mirror their proximity in query in-
tent.
To put it in more details, we use jaccard sim-
ilarity for name shinglings and cosine similarity
for domain and topic vector. As query log in-
duced intent topic graph is of considerable large
size, the pair-wise similarity is computationally
prohibitive, hence we use Local Sensitive Hash
(Indyk and Motwani, 1998) for each similarity
metric so as to compute ISim just in candidate
set. We use random hyperplane based hash fam-
ily proposed in (Charikar, 2002) and set the hash
code dimension and hash table numbers empiri-
cally to ensure the number of nodes falling into
each bucket is relatively stable.
</bodyText>
<subsectionHeader confidence="0.999921">
3.2 Merging nodes
</subsectionHeader>
<bodyText confidence="0.999984294117647">
Although our idea of specifying intent topics by
context better models the multi-facets of queries,
it obviously also brings a sparse issue. For exam-
ple, in one session user query beep lyrics and click
www.lyricsandsongs.com, lyrics is tagged with the
song beep and the musician Pussycat Dolls, in an-
other scenario lyrics occurs with the song what
you know and url www.dapslyrics.com, intents be-
hind these two nodes are so similar that they
should come into one, otherwise connections be-
tween the two intent-coherent urls may be lost.
To avoid that, we conduct a merge process to
integrate nodes with exactly the same names and
contexts into one, combing linked nodes and intent
features together.
For a set of nearly duplicate nodes ω the cal-
culation of new node’s features can be written as:
</bodyText>
<equation confidence="0.9999615">
t = Eiw l tu (9)
dˆ = E Iwo du (10)
</equation>
<bodyText confidence="0.9468925">
In other words, we gather candidate nodes re-
trieved by LSH and then calculate ISim for them
with η setting to 0. Only node pairs with ISim
higher than a merge threshold θ can be seen as
duplicates. The merge process is summarized in
Algorithm 1.
</bodyText>
<figure confidence="0.818926625">
Algorithm 1: Merging similar nodes
Input: G =&lt; V, E, W &gt;, β, γ, η, θ
Output: G� =&lt; V�, E, W� &gt;
begin
Initialize Q +— ∅
for v E V do
Find dupset ωv with ISimβ,γ,η
if lu E V,ωu E Q and ωv n ωu =�∅
then
ωv +— ωv U ωu
Remove ωu from Q
Add ωv to Q
for ω E Q do
Merge nodes in ω into new node v�
Update G with replacing nodes in ω
with v�
</figure>
<subsectionHeader confidence="0.997452">
3.3 Label propagation
</subsectionHeader>
<bodyText confidence="0.984198375">
We utilize knowledge graph induced intent fea-
tures instead of manually labels as constraints to
conduct label propagation(Zhu and Ghahramani,
2002). The idea is that node labels are propa-
gated to nearby nodes via weighted edges until
convergence, as highly weighted edges indicate
high probability of sharing labels.
Nodes in our work have soft labels, where each
dimension of intent features denotes a label, such
as a topic or domain of knowledge graph. As de-
scribed in aforesaid observations, it is intuitively
reasonable to propagate on the basis of explicit
edges and implicit intent similarities. We illustrate
the propagation with topic feature, that of domain
feature is similar.
We use matrix Yt E R|V |∗|T |to denote the in-
tent topic graph’s initial topic feature labels, with
element Ytik indicating node vi’s relevance to tk,
wherer tk E T. Yt is initialized based on the
results of the feature enriching step in Section 2,
with no manually-labelled instances needed in our
model. As only part of nodes can directly map
to Freebase topics, those are initialized as labelled
nodes, then propagate t to their linked neighbors.
The number of unlabelled data is written as u,
while that of labelled data l and the total number
of nodes N.
1073
The transition matrix T indicates the impact of
nodes on each other. Note that here the wij can
be replaced by other similarity measures such as
ISim in Section 3.2.
</bodyText>
<equation confidence="0.823087166666667">
Tij =� ij (11)
E,
k=1 wkj
Let D denote an N ×N diagonal matrix with dii =
Ej Tij. Then we can get a normalized version of
transition matrix P = D−1T.
</equation>
<bodyText confidence="0.936955">
The normalized transition matrix can be split
into 4 sub-matrices.
</bodyText>
<equation confidence="0.839389857142857">
[Pll Plu]
P = (12)
Pul Puu
At each step, we propagate and clamp the labelled
data and repeat until Y converges, the propagation
step can be written as:
ˆYu = PuuYu + PulYl (13)
</equation>
<bodyText confidence="0.983427">
As is shown in (Zhu and Ghahramani, 2002; Zhu
et al., 2003) the solution to the propagation con-
verges to:
</bodyText>
<equation confidence="0.949194">
ˆYu = (I − Puu)−1PulYl (14)
</equation>
<subsectionHeader confidence="0.5642955">
3.4 The propagation framework for intent
features
</subsectionHeader>
<bodyText confidence="0.981656">
We carry the propagation in an iterative process
illustrated in Algorithm 2.
</bodyText>
<figure confidence="0.698878954545455">
Algorithm 2: Intent feature propagation
Input: G, Y t
l ,Y d
l
Output: ˆG, Yˆtu, Yˆud
Initialize Y t
l Y d with results of Section2
repeat
Merge similar nodes according to
Algorithm 1
Compute matrix P
repeat
Yˆt
u = PuuYu t + PulY t
l
until Convergence;
Recompute Pˆ with Yˆt
repeat ˆPuuYud + ˆPulY d
Yˆd l
u
until Convergence;
until no dups;
</figure>
<bodyText confidence="0.9985116875">
Since intent features include both domain vec-
tor and topic vector, we propagate them in an alter-
nating way. At first we label nodes as described in
Section 2, though missing refiners’ and some urls’
intent features, they are just used for initialization.
Then we propagate Freebase topic features based
on explicit edge weights, so that more nodes in
intent topic graph have topic features now. Then
fetching the learned topic features, we reinput it
into domain feature propagation, which means we
recalculate the transition matrix combining the im-
plicit learned TSim into edge weight, then prop-
agate domain vector of labelled nodes through the
graph. At each iteration, we first update Yt, then
input it to update Y d, therefore merge near dupli-
cate intent topics to update the whole graph.
</bodyText>
<sectionHeader confidence="0.999577" genericHeader="method">
4 Experiments
</sectionHeader>
<subsectionHeader confidence="0.985337">
4.1 Data preparation
4.1.1 Search logs
</subsectionHeader>
<bodyText confidence="0.964931666666667">
We use AOL search log data for experiments. It
includes 20 million web queries collected covering
500K users over three months in 2006.
</bodyText>
<tableCaption confidence="0.999438">
Table 1: The query set
</tableCaption>
<table confidence="0.99945875">
# of sessions 35140
# of queries 271127
# of users 21378
# of urls 63019
</table>
<bodyText confidence="0.999927952380952">
We preprocess the query log by keeping urls oc-
curring more than 3 times and queries with 2 to
40 characters, then extract sessions considering 25
minutes duration. While user session segmenta-
tion can be improved with more sophisticated al-
gorithms, this simple low-cost heuristic performs
adequately for our purposes. We then move on to
map queries to Freebase and empirically filter ses-
sions that are less entity-centric. We use an anno-
tation tool especially for short text (Ferragina and
Scaiella, 2012) called Tagme3 to recognize entities
and observe only 16% of all the queries are ex-
actly an entity itself, which means most of queries
do have refiner words to convey information need.
To ensure the precision of recognized entities, we
set a significant threshold and bottom line thresh-
old , queries should have at least one recognized
entity with a likelihood above significant level,
and those below bottom line are ignored. They
are 0.19 and 0.05 in our work, which may vary
with entity recognition method. The normalized
</bodyText>
<footnote confidence="0.560029">
3http://tagme.di.unipi.it/
</footnote>
<table confidence="0.995516">
1074
percentage Queryosetn Testbsessionasetg Freebaseutopics. Freebaseifactsh
Figure1:Unbalanced domai
ndistributions inFreebase comparingagainst query set.Onlydomain
onsaresh ow n.Table 2
:Exam ples oflabel
switht oppro porti rned fe atureI ledintentt
opicnodeswith lea
ntenttopic nodesO seAfterpro pa g ati 87),(p roject s,0.13
rigin alinF reeba onAnnota tiontr )(loca tion, 0. 13),(tr
avel.yahoo.com Yahoo! ave l,0.11),(or ganizati o
Travel(int ernet ,0.
n,0 .08), ..Yahoo! ingandrese rvatio n,0.6),( bo ok ,0.4)(
(business, 0.08). Travelofferstr avelg nservices.mapq uestww loc atio n,0.13 ),( orga
uides, book w.mapque st.co m niza tion,0.0
MapQuest(org anizatio
</table>
<bodyText confidence="0.926759833333334">
9),(travel , 0 .09) ,( automo tive ,0 .06) ...M
apQue stisa nAmeri -canfr eeonl ine web map- ping
service .l ikel ih ood isus edas tq( m).T henwed
ropse s-sions where tagge dentity w ordsweig htl esst
hanrefiners aswellasthe oneswith tooman yent itywo
rdssp otte d in dic ating dis perse int en ts.For
eac hr ecogn iz edent i t
et opicsw ithre le-vanceover 0.3 a re kept. Th e quer
y,onlyFreebas
yse tw efinallyg etisshown inTable1. 4.1.2Fre
ebase Toe n richquer yrepre se ntati ons,w ecol
lectasu b-s etof Free base inc ludingmore than 7mill
ionsfa cts a nd4m il lion st opics int ota l whichal
soc ontai n15 0thousan dt opicaleq ui valent web
sites,th ough le ssthan 3 % urlsinque ryset ar ecov er
ed.Thefacts andentit ie sinFr eeb as e isra th erun
-ba la nced acr ossdomai ns espe ciallyaga in stt
hatof recogi nizedentitie sin query setas shown
inFig -ure1.Thu sthe origi nal global knowledgewe
usea
bou tdom aindistrib uti onmay cause bias,which
ma kes tailo ringnec essary forinte ntunder sta
nd-ing.F orbot hgenera li tyandprec i sion, wekeep
mo stofFreebasedo mainsex cept seve ral extremei
n- com pl ete o nes, i ns te ad of retai nin ga smal
lnumbe rof representa tive d om a ins li kema nyre
searche rsd o (Liet al. ,20 13;Yu et al .,201 4;
Line t al.,2 01 2).But generalitycomes ata price th
labe
atsomedomai nsa reconf us ing andmixedus edwh ic
hwethe ncho os et ome rg e,likece
leb rities andpe ople,
perio dicalsan dbo oks,t
va ndbroadc ast, etc.Wefin all y keep5 0 of all76 do
mains. 4.2Int enttopi cg rap h 4. 2.1 Bu ildingth
egra ph We leverag ebothF reebase an dsear chsessio
nst oenrich inte nt topics. Wese t gto0. 9inc
alcula -tiono fsess ion’s i ntentf ea ture s.Af
terl abelingt hese ssion log ,wero ughlymak eagrap
hwit h335 206int enttopic node s, 119364o f
them h avebeen labe l led wi th Fr e eb ase top i cf
eatu re ,othe rs only h avedo main feature.T henwe
con ductame rgeproces sw ith 4 set t o0.7, c to0 .3an
doto0.75 in orde rtomer geno de s withd uplic
aten ame sand s imi larc ontex ts .Wefind 46659dupl
icatese tscov eri n g1407 68 nodes. Thenwe ignore
no deswithfew lin ksandrare nam estoreduc espar sit
y.Fina llywe’ veg otagr aph of209351int enttopic
sto initializ et hepr opag a tion,in
clu di ng78932 la-belle dno des.T heme rge a ndpro
pag a tionpro gressg etconvergedin lesst
han4 rounds.
ll furtherevaluate We’ thegraphwithcase study
essionintent understandingtask. andas
</bodyText>
<subsectionHeader confidence="0.30841">
4.2.2Case Study
</subsectionHeader>
<bodyText confidence="0.669967">
dmonstrate intent features re good interpre- We
tations for query intent, whether they’re in
lled
Section 2 or learned by propagation in Section 3.
We can see in Table 2 that as nodes’ original
</bodyText>
<page confidence="0.801381">
1075
</page>
<tableCaption confidence="0.998756">
Table 3: Examples of unlabelled intent topic nodes with learned feature
</tableCaption>
<table confidence="0.936857">
Intent topic node intent features Annotation Similarity nodes
www.bnm.com (The Hertz Corporation, 0.25), (South-
west Florida International Airport,
0.17), (Punta Gorda Airport, 0.13),
(Supercar, 0.09), (Sports car, 0.08)...
(aviation, 0.23), (business, 0.21), (lo-
cation, 0.14), (automotive, 0.11)...
www.mobtime.com (Software, 0.18), (Mobile phone,
0.11), (100% Totally Free Ringtones,
0.10), (Motorola, 0.09), (Free Cell,
0.08), (Verizon Wireless, 0.04)...
(computer, 0.23), (cvg, 0.21), (music,
0.19), (business, 0.11) ...
</table>
<bodyText confidence="0.961836914285714">
Online booking
of discount
rentals at ma-
jor airports,
worldwide.
MobTime Cell
Phone Manager
is a PC soft-
ware to manage
or sync mobile
phones.
www.arac.com
www.rentalcars.com
www.hertz.com
www.alamo.com
rent a car
cheap rental cars
cellphones.about.com
cell software
cell to pc
reviews of
cellphone wallpaper
types in Freebase are not proper for describing in-
tent, the intent features they get after propagation
tend to be more explainable, such as the travel
site often co-occurs with city names, tourist attrac-
tions, hotels and so on, thus indicating its intent in
travel and location domain.
Table 3 shows examples which have no equiv-
alents in Freebase. Although some of them may
be accessible in other ontologies, we only take
them as examples to show our propagation method
makes it possible to depict intents behind urls and
words in a knowledge graph based way while be-
yond the capacity of knowledge graph.
</bodyText>
<subsectionHeader confidence="0.9897155">
4.3 Session intent understanding task
4.3.1 Experiment Setup
</subsectionHeader>
<bodyText confidence="0.999897075">
The evaluation of query understanding has long
been a challenging task. To judge whether the con-
cepts in query are successfully recognized seems
too straightforward, and it can hardly be consid-
ered understanding the intent until the big idea
about what kind of topics users emphasis is cap-
tured, which can be briefly sketched by distribu-
tion across Freebase domains. Also it is difficult to
translate results of previous log analysis methods
into knowledge graph domain information, thus
hardly fit into our evaluation schema. We take
popularity-based method as baseline.
We have few choices but to tag ground truth our-
selves for intent understanding evaluation.
We randomly select 150 sessions as test set,
the domain distribution of which agrees with the
whole query set as shown in Figure 1. As mas-
tering meanings of all Freebase domains is too
challenging, we ask 5 accessors to describe each
session’s intent broadly with a few natural lan-
guage terms, then an expert familiar with Freebase
schema translates the words into matched Free-
base domains. Each test session is tagged by 2
accessors and 1 expert, we choose to use the tags
of the cases in which the accessors reached agree-
ment as the gold stantard. For example, if acces-
sors tag session intent as pictures, then experts can
translate it into Freebase visual art domain. Each
session has 1∼4 tags and 1.6 tags in average. The
tags cover 30 domains.
For each session, we derive the local intent do-
main vector ds following the method in Section 2.
Here we simply set quality function cp(r) to a con-
stant Ar for all refiners and cp(c) to A&apos; for all clicks,
we’ll dive into more specialized weighting method
in future work. Ar and A&apos; are parameters to control
impact of different kinds of intent topics. Based on
whether to exploit global intent features of non-
entity words, we compare four variations against
one baseline.
</bodyText>
<listItem confidence="0.995014083333333">
• Popularity-based (GP). We use domains’ fre-
quency in the query set as a baseline.
• Entity-based (E). We only use entity nodes’
original intent features without propagation.
• Entity+Clicks (EC). Both intent features of
entity words and clicks are used, controlled
with A&apos;.
• Entity+Refiners (ER). Intent features of en-
tity words and refiner words are used, refin-
ers’ impact is controlled by Ar.
• Entity+Clicks+Refiners (ECR). All intent
topics are combined, controlled by A&apos;, Ar.
</listItem>
<figure confidence="0.996274773584906">
1076
1 0.720
0.9 0.714
0.8 0.708
0.7 0.702
0.6 0.696
0.5 0.690
0.4 0.684
0.3 0.678
0.2 0.672
0.1
0
0 0.1 0.2 0.3 0.4 0.5 0.6 0.7 0.8 0.9 1
(a) MAP@5
1 0.756
0.9 0.750
0.8 0.744
0.7 0.738
0.6 0.732
0.5 0.726
0.4 0.720
0.3 0.714
0.2 0.708
0.1
0
0 0.1 0.2 0.3 0.4 0.5 0.6 0.7 0.8 0.9 1
(c) MAP@10
1 0.39
0.9 0.36
0.8 0.33
0.7 0.30
0.6 0.27
0.5 0.24
0.4 0.21
0.3 0.18
0.2
0.1
0
0 0.1 0.2 0.3 0.4 0.5 0.6 0.7 0.8 0.9 1
(b) GMAP@5
1 0.575
0.9 0.550
0.8 0.525
0.7 0.500
0.6 0.475
0.5 0.450
0.4 0.425
0.3 0.400
0.2 0.375
0.1
0
0 0.1 0.2 0.3 0.4 0.5 0.6 0.7 0.8 0.9 1
(d) GMAP@10
</figure>
<figureCaption confidence="0.872622666666667">
Figure 2: The impact of Arand Ac on ECR methods in four metrics, with vertical axis indicating Ar,
horizontal axis as Ac. The first column on the left denotes ER method, while the bottom row the EC
method.
</figureCaption>
<subsectionHeader confidence="0.673104">
4.3.2 Evaluation metrics
</subsectionHeader>
<bodyText confidence="0.999087125">
We use each approach to rank domains accord-
ing to its derived weight, then compare it with
golden standard set. It can be evaluated using
Mean Average Precision (MAP), Geometric MAP
and Precision@K. We use GMAP because it is
more robust to outliers than the arithmetic mean.
For test set of size N, the MAP and GMAP can
be calculated as follows:
</bodyText>
<table confidence="0.8000678">
1 N APi@k (15)
MAP@k = N i=1
� � N APi@k (16)
� H
GMAP@k = N � i=1
</table>
<subsubsectionHeader confidence="0.657781">
4.3.3 Results and analysis
</subsubsectionHeader>
<bodyText confidence="0.999933382352941">
We first study impact of parameters Ar and Ac,
which is shown in Figure 2.
It roughly demonstrates different combinations
of parameters’ impact on ECR methods, perfor-
mance is evaluated in four metrics, with deeper
color indicating better result.
Best results comes with a Ac larger than Ar in
all four subfigures. This trend seems more obvi-
ous in (d) where right part with larger Ac get better
results. Also, deeper colors around diagonal line
in (a) (c) indicate a more balanced combination
of refiners and urls are more likely to enhance in-
tent understanding. Thus we conclude clicks has a
weak advantage over refiners in improving the re-
sult, while combining both with proper parameters
can get the best result.
When comparing between MAP and GMAP, we
can see while GMAP stays a high value when am-
plifying the impact of clicks, MAP changes with
the variation of Ar for better or worse. As GMAP
is a more robust metric, we can then infer that in-
creasing weight of refiners could bring more out-
liers, implying refiners’ intent features are more
susceptible to noise.
Then we use ER with Ar = 0.5 as ERopt, EC
with Ac = 0.5 as ECopt and ECR with Ar =
0.2, Ac = 0.5 as ECRopt.
Figure 3 clearly shows the superior performance
of our model, especially at top positions. Table 4
shows the detailed comparisons between different
methods. We can see our knowledge graph based
intent representations perform well in session in-
tent understanding. And refiners’ and clicks’ in-
tent features which we learn by propagation con-
</bodyText>
<page confidence="0.755184">
1077
</page>
<figureCaption confidence="0.9692955">
Figure 3: Precision@K results for different ap-
proaches, by varying number of k
</figureCaption>
<tableCaption confidence="0.997855">
Table 4: Comparisons among different methods
</tableCaption>
<table confidence="0.981421285714286">
K=5 K=10
MAP GMAP MAP GMAP
GP 0.177 0.000 0.232 0.002
E 0.676 0.166 0.707 0.355
EC°pt 0.708 0.412 0.739 0.579
ER°pt 0.688 0.227 0.723 0.421
ECR°pt 0.722 0.412 0.756 0.594
</table>
<bodyText confidence="0.978022666666667">
tribute a lot to improve naive entity-based method,
which do validate an complment effect of their
learned intent features.
</bodyText>
<sectionHeader confidence="0.999912" genericHeader="evaluation">
5 Related Work
</sectionHeader>
<subsectionHeader confidence="0.975531">
5.1 Query intent understanding
</subsectionHeader>
<bodyText confidence="0.99991880952381">
Query intent or search intent has been studied in-
tensively from various views.
A popular paradigm is to label several intents
for each query, also called facets subgoals and
subtopics in the literature, manully or by min-
ing methods and then do classification (Hu et al.,
2009; Li et al., 2008) based on that. Manually in-
tent schemas range from 3 top level (Broder, 2002)
to fine-grained subcatogories (Rose and Levinson,
2004) and taxonomy (Yin and Shah, 2010). Intent
tasks in NTCIR-10 (Sakai et al., 2013) also pro-
vide subtopic pools made by accessors.
Another view of intent is more generic, min-
ing or learning search intents without any kind of
pre-defined intent category and clustering method
is often used. Methods including (Sadikov et al.,
2010; Yamamoto et al., 2012; Cheung and Li,
2012) cast intent as represented by a pattern or
template consisting of a sequence of semantic con-
cepts or lexical items. (Tan et al., 2012) encode
intent in language models, aware of long-lasting
interests. (Ren et al., 2014) uses an unsupervised
heterogeneous clustering. (Yin and Shah, 2010)
capture generic intents around a certain named en-
tities and model their relationships in a tree tax-
onomy and (Wang et al., 2009) mine broad latent
modifiers of intent aspect , which are similar to
our motivation, while we model more than intent
phrases, but intent topics. We do not split queries
into clusters or subtopics relevant to the original
query to indicate a intent, but link them in an graph
with intent feature similarity, weakly or strongly,
in a holistical view.
On the other hand, previous research can be
categorized by what kind of resources they rely
on. Quite an amount of work leverage query logs
(Jiang et al., 2013), including query reformula-
tions (Radlinski et al., 2010), click-through data
(Li et al., 2008). There are also works using spon-
sered data (Yamamoto et al., 2012) and interactive
data (Ruotsalo et al., 2013). The new trend of in-
tegrating knowledge graph will be discussed next.
</bodyText>
<subsectionHeader confidence="0.619312">
5.2 Knowledge graph on intent
understanding
</subsectionHeader>
<bodyText confidence="0.999952928571429">
Instead of summarizing queries into concepts by
clustering, recently there appears a tendency to use
concpets from knowledge graph resources. Some
researchers manage to build entity graph from
queries (Bordino et al., 2013a) (Bordino et al.,
2013b; Yu et al., 2014), some in a structure view,
interpret quries into knowledge base fit template
(Pound et al., 2012; Li et al., 2013). (Pantel et al.,
2012) models latent intent to mine entity type dis-
tributions. (Ren et al., 2014) utilizes knowledge
graph resources in a hetrogeneous view. (Lin et
al., 2012) also pays attention to refiners, but re-
stricted to limited domains, while our method is
more general.
</bodyText>
<sectionHeader confidence="0.998211" genericHeader="conclusions">
6 Conclusion
</sectionHeader>
<bodyText confidence="0.999730416666667">
In this paper, we tailor knowledge graph to rep-
resent query intent behind entity words, refiners
and clicked urls in a unified framework, taking
them as intent topic nodes connected in a large
graph. We manage to get a contextualized intent
depiction exploiting global knowledge in Free-
base, then propagate the feature to cover more in-
tent topics. We show in experiments the knowl-
edge graph enriched representation is reasonable
and explainable, and the intents feature of refiners
and clicks can better enhance intent understanding
than methods simply relying on entities.
</bodyText>
<figure confidence="0.985998764705882">
Precision@K
0.8
0.7
0.6
0.5
0.4
0.3
0.2
0.1
0
1 3 5 10 20 50 K
GL
E
ER
EC
ECR
1078
</figure>
<bodyText confidence="0.999914">
There are several directions for future work, in-
cluding using both types and domains in Free-
base schema, diving into refiners and looking for a
proper weighting method, developing a query rec-
ommendation framework based on the intent topic
graph and user interest modeling.
</bodyText>
<sectionHeader confidence="0.99547" genericHeader="acknowledgments">
Acknowledgments
</sectionHeader>
<bodyText confidence="0.9985248">
We sincerely thank all the anonymous reviewers
for their valuable comments, which have helped to
improve this paper greatly. This work is supported
by NSFC with Grant No.61370054, and 973 Pro-
gram with Grant No.2014CB340405.
</bodyText>
<sectionHeader confidence="0.981965" genericHeader="references">
References
</sectionHeader>
<reference confidence="0.998067700680272">
Nicholas J Belkin, Robert N Oddy, and Helen M
Brooks. 1982. Ask for information retrieval: Parti.
background and theory. Journal of documentation,
38(2):61–71.
Ilaria Bordino, Gianmarco De Francisci Morales, Ing-
mar Weber, and Francesco Bonchi. 2013a. From
machu picchu to rafting the urubamba river: antici-
pating information needs via the entity-query graph.
In Proceedings of the sixth ACM international con-
ference on Web search and data mining, pages 275–
284. ACM.
Ilaria Bordino, Yelena Mejova, and Mounia Lalmas.
2013b. Penguins in sweaters, or serendipitous entity
search on user-generated content. In Proceedings
of the 22nd ACM international conference on Con-
ference on information &amp; knowledge management,
pages 109–118. ACM.
Andrei Broder. 2002. A taxonomy of web search. SI-
GIR Forum, 36(2):3–10, September.
Moses S Charikar. 2002. Similarity estimation tech-
niques from rounding algorithms. In Proceedings of
the thiry-fourth annual ACM symposium on Theory
of computing, pages 380–388. ACM.
Jackie Chi Kit Cheung and Xiao Li. 2012. Sequence
clustering and labeling for unsupervised query intent
discovery. In Proceedings of the fifth ACM interna-
tional conference on Web search and data mining,
pages 383–392. ACM.
W Bruce Croft, Michael Bendersky, Hang Li, and
Gu Xu. 2010. Query representation and understand-
ing workshop. In SIGIR Forum, volume 44, pages
48–53.
Paolo Ferragina and Ugo Scaiella. 2012. Fast and
accurate annotation of short texts with wikipedia
pages. IEEE software, 29(1).
Jian Hu, Gang Wang, Fred Lochovsky, Jian-tao Sun,
and Zheng Chen. 2009. Understanding user’s query
intent with wikipedia. In Proceedings of the 18th
international conference on World wide web, pages
471–480. ACM.
Piotr Indyk and Rajeev Motwani. 1998. Approxi-
mate nearest neighbors: towards removing the curse
of dimensionality. In Proceedings of the thirtieth
annual ACM symposium on Theory of computing,
pages 604–613. ACM.
Daxin Jiang, Jian Pei, and Hang Li. 2013. Mining
search and browse logs for web search: A survey.
ACM Trans. Intell. Syst. Technol., 4(4):57:1–57:37,
October.
Xiao Li, Ye-Yi Wang, and Alex Acero. 2008. Learn-
ing query intent from regularized click graphs. In
Proceedings of the 31st Annual International ACM
SIGIR Conference on Research and Development in
Information Retrieval, SIGIR ’08, pages 339–346,
New York, NY, USA. ACM.
Yanen Li, Bo-June Paul Hsu, and ChengXiang Zhai.
2013. Unsupervised identification of synonymous
query intent templates for attribute intents. In Pro-
ceedings of the 22nd ACM international conference
on Conference on information &amp; knowledge
management, CIKM ’13, pages 2029–2038, New
York, NY, USA. ACM.
Thomas Lin, Patrick Pantel, Michael Gamon, Anitha
Kannan, and Ariel Fuxman. 2012. Active objects:
Actions for entity-centric search. In Proceedings
of the 21st international conference on World Wide
Web, pages 589–598. ACM.
Patrick Pantel, Thomas Lin, and Michael Gamon.
2012. Mining entity types from query logs via user
intent modeling. In Proceedings of the 50th An-
nual Meeting of the Association for Computational
Linguistics: Long Papers-Volume 1, pages 563–571.
Association for Computational Linguistics.
Jeffrey Pound, Alexander K Hudek, Ihab F Ilyas, and
Grant Weddell. 2012. Interpreting keyword queries
over web knowledge bases. In Proceedings of the
21st ACM international conference on Information
and knowledge management, pages 305–314. ACM.
Filip Radlinski, Martin Szummer, and Nick Craswell.
2010. Inferring query intent from reformulations
and clicks. In Proceedings of the 19th international
conference on World wide web, pages 1171–1172.
ACM.
Xiang Ren, Yujing Wang, Xiao Yu, Jun Yan, Zheng
Chen, and Jiawei Han. 2014. Heterogeneous graph-
based intent learning with queries, web pages and
wikipedia concepts. In Proceedings of the 7th ACM
International Conference on Web Search and Data
Mining, WSDM ’14, pages 23–32, New York, NY,
USA. ACM.
Daniel E. Rose and Danny Levinson. 2004. Un-
derstanding user goals in web search. In Proceed-
ings of the 13th International Conference on World
1079
Wide Web, WWW ’04, pages 13–19, New York, NY,
USA. ACM.
Tuukka Ruotsalo, Jaakko Peltonen, Manuel Eugster,
Dorota Głowacka, Ksenia Konyushkova, Kumari-
paba Athukorala, Ilkka Kosunen, Aki Reijonen,
Petri Myllym¨aki, Giulio Jacucci, et al. 2013. Di-
recting exploratory search with interactive intent
modeling. In Proceedings of the 22nd ACM interna-
tional conference on Conference on information &amp;
knowledge management, pages 1759–1764. ACM.
Eldar Sadikov, Jayant Madhavan, Lu Wang, and Alon
Halevy. 2010. Clustering query refinements by user
intent. In Proceedings of the 19th international con-
ference on World wide web, pages 841–850. ACM.
Tetsuya Sakai, Zhicheng Dou, Takehiro Yamamoto,
Yiqun Liu, Min Zhang, Ruihua Song, MP Kato, and
M Iwata. 2013. Overview of the ntcir-10 intent-2
task. Proceedings of NTCIR-10, pages 94–123.
Bin Tan, Yuanhua Lv, and ChengXiang Zhai. 2012.
Mining long-lasting exploratory user interests from
search history. In Proceedings of the 21st ACM in-
ternational conference on Information and knowl-
edge management, pages 1477–1481. ACM.
Xuanhui Wang, Deepayan Chakrabarti, and Kunal
Punera. 2009. Mining broad latent query aspects
from search sessions. In Proceedings of the 15th
ACM SIGKDD international conference on Knowl-
edge discovery and data mining, pages 867–876.
ACM.
Takehiro Yamamoto, Tetsuya Sakai, Mayu Iwata, Chen
Yu, Ji-Rong Wen, and Katsumi Tanaka. 2012. The
wisdom of advertisers: mining subgoals via query
clustering. In Proceedings of the 21st ACM inter-
national conference on Information and knowledge
management, pages 505–514. ACM.
Xiaoxin Yin and Sarthak Shah. 2010. Building taxon-
omy of web search intents for name entity queries.
In Proceedings of the 19th international conference
on World wide web, pages 1001–1010. ACM.
Xiao Yu, Hao Ma, Bo-June (Paul) Hsu, and Jiawei Han.
2014. On building entity recommender systems us-
ing user click log and freebase knowledge. In Pro-
ceedings of the 7th ACM International Conference
on Web Search and Data Mining, WSDM ’14, pages
263–272, New York, NY, USA. ACM.
Xiaojin Zhu and Zoubin Ghahramani. 2002. Learning
from labeled and unlabeled data with label propa-
gation. Technical report, Technical Report CMU-
CALD-02-107, Carnegie Mellon University.
Xiaojin Zhu, Zoubin Ghahramani, John Lafferty, et al.
2003. Semi-supervised learning using gaussian
fields and harmonic functions. In ICML, volume 3,
pages 912–919.
</reference>
<page confidence="0.778645">
1080
</page>
</variant>
</algorithm>
<algorithm name="ParsHed" version="110505">
<variant no="0" confidence="0.388746">
<title confidence="0.9082775">Tailor knowledge graph for query understanding: linking intent topics by propagation</title>
<author confidence="0.762356">Shi Zhao Yan Zhang z spku edu cn zhycis pku edu cn</author>
<affiliation confidence="0.999918">Department of Machine Intelligence, Peking University, Beijing,</affiliation>
<address confidence="0.89976">Key Laboratory on Machine Perception, Ministry of Education, Beijing, China</address>
<abstract confidence="0.999273166666667">Knowledge graphs are recently used for enriching query representations in an way for the rich facts oraround entities in it. ever, few of the methods pay attention to non-entity words and clicked websites in queries, which also help conveying user intent. In this paper, we tackle the problem of intent understanding with innovatively representing entity words, refiners clicked urls as topics a unified knowledge graph based framework, in a way to exploit and expand knowledge graph which we call ‘tailor’. We collaboratively exploit global knowledge in knowledge graphs and local contexts in query log to initialize intent representation, then propagate the enriched features in a graph consisting of intent topics using an unsupervised algorithm. The experiments prove intent topics with knowledge graph enriched features significantly enhance intent understanding.</abstract>
</variant>
</algorithm>
<algorithm name="ParsCit" version="110505">
<citationList>
<citation valid="true">
<authors>
<author>Nicholas J Belkin</author>
<author>Robert N Oddy</author>
<author>Helen M Brooks</author>
</authors>
<title>Ask for information retrieval: Parti. background and theory.</title>
<date>1982</date>
<journal>Journal of documentation,</journal>
<volume>38</volume>
<issue>2</issue>
<contexts>
<context position="1535" citStr="Belkin et al., 1982" startWordPosition="226" endWordPosition="229">ntexts in query log to initialize intent representation, then propagate the enriched features in a graph consisting of intent topics using an unsupervised algorithm. The experiments prove intent topics with knowledge graph enriched features significantly enhance intent understanding. 1 Introduction Query understanding is the process of generating a representation which characterizes a user’s search intent (Croft et al., 2010), which is of vital importance for information retrieval. However, users are remarkably laconic in describing their information needs due to anomalous state of knowledge (Belkin et al., 1982), resulting in vague and underspecified queries, which makes it especially difficult to understand and locate what they intended for in mountains of web data. The problem is often significantly compounded that people convey their intent rather in a series of behaviors called a search session than a single query, leaving a wealth of clues including query reformulations, page visits, dwell times, etc. What’s more, as entities are taking center stage (Yin and Shah, 2010), string-level or phrase-level modeling of intent soon hits the bottleneck, calling for an entity-aware perspective. Knowledge r</context>
</contexts>
<marker>Belkin, Oddy, Brooks, 1982</marker>
<rawString>Nicholas J Belkin, Robert N Oddy, and Helen M Brooks. 1982. Ask for information retrieval: Parti. background and theory. Journal of documentation, 38(2):61–71.</rawString>
</citation>
<citation valid="true">
<authors>
<author>Ilaria Bordino</author>
<author>Gianmarco De Francisci Morales</author>
<author>Ingmar Weber</author>
<author>Francesco Bonchi</author>
</authors>
<title>From machu picchu to rafting the urubamba river: anticipating information needs via the entity-query graph.</title>
<date>2013</date>
<booktitle>In Proceedings of the sixth ACM international conference on Web search and data mining,</booktitle>
<pages>275--284</pages>
<publisher>ACM.</publisher>
<contexts>
<context position="31985" citStr="Bordino et al., 2013" startWordPosition="5457" endWordPosition="5460">ey rely on. Quite an amount of work leverage query logs (Jiang et al., 2013), including query reformulations (Radlinski et al., 2010), click-through data (Li et al., 2008). There are also works using sponsered data (Yamamoto et al., 2012) and interactive data (Ruotsalo et al., 2013). The new trend of integrating knowledge graph will be discussed next. 5.2 Knowledge graph on intent understanding Instead of summarizing queries into concepts by clustering, recently there appears a tendency to use concpets from knowledge graph resources. Some researchers manage to build entity graph from queries (Bordino et al., 2013a) (Bordino et al., 2013b; Yu et al., 2014), some in a structure view, interpret quries into knowledge base fit template (Pound et al., 2012; Li et al., 2013). (Pantel et al., 2012) models latent intent to mine entity type distributions. (Ren et al., 2014) utilizes knowledge graph resources in a hetrogeneous view. (Lin et al., 2012) also pays attention to refiners, but restricted to limited domains, while our method is more general. 6 Conclusion In this paper, we tailor knowledge graph to represent query intent behind entity words, refiners and clicked urls in a unified framework, taking them </context>
</contexts>
<marker>Bordino, Morales, Weber, Bonchi, 2013</marker>
<rawString>Ilaria Bordino, Gianmarco De Francisci Morales, Ingmar Weber, and Francesco Bonchi. 2013a. From machu picchu to rafting the urubamba river: anticipating information needs via the entity-query graph. In Proceedings of the sixth ACM international conference on Web search and data mining, pages 275– 284. ACM.</rawString>
</citation>
<citation valid="true">
<authors>
<author>Ilaria Bordino</author>
<author>Yelena Mejova</author>
<author>Mounia Lalmas</author>
</authors>
<title>Penguins in sweaters, or serendipitous entity search on user-generated content.</title>
<date>2013</date>
<booktitle>In Proceedings of the 22nd ACM international conference on Conference on information &amp; knowledge management,</booktitle>
<pages>109--118</pages>
<publisher>ACM.</publisher>
<contexts>
<context position="31985" citStr="Bordino et al., 2013" startWordPosition="5457" endWordPosition="5460">ey rely on. Quite an amount of work leverage query logs (Jiang et al., 2013), including query reformulations (Radlinski et al., 2010), click-through data (Li et al., 2008). There are also works using sponsered data (Yamamoto et al., 2012) and interactive data (Ruotsalo et al., 2013). The new trend of integrating knowledge graph will be discussed next. 5.2 Knowledge graph on intent understanding Instead of summarizing queries into concepts by clustering, recently there appears a tendency to use concpets from knowledge graph resources. Some researchers manage to build entity graph from queries (Bordino et al., 2013a) (Bordino et al., 2013b; Yu et al., 2014), some in a structure view, interpret quries into knowledge base fit template (Pound et al., 2012; Li et al., 2013). (Pantel et al., 2012) models latent intent to mine entity type distributions. (Ren et al., 2014) utilizes knowledge graph resources in a hetrogeneous view. (Lin et al., 2012) also pays attention to refiners, but restricted to limited domains, while our method is more general. 6 Conclusion In this paper, we tailor knowledge graph to represent query intent behind entity words, refiners and clicked urls in a unified framework, taking them </context>
</contexts>
<marker>Bordino, Mejova, Lalmas, 2013</marker>
<rawString>Ilaria Bordino, Yelena Mejova, and Mounia Lalmas. 2013b. Penguins in sweaters, or serendipitous entity search on user-generated content. In Proceedings of the 22nd ACM international conference on Conference on information &amp; knowledge management, pages 109–118. ACM.</rawString>
</citation>
<citation valid="true">
<authors>
<author>Andrei Broder</author>
</authors>
<title>A taxonomy of web search.</title>
<date>2002</date>
<journal>SIGIR Forum,</journal>
<volume>36</volume>
<issue>2</issue>
<contexts>
<context position="30086" citStr="Broder, 2002" startWordPosition="5149" endWordPosition="5150">0.739 0.579 ER°pt 0.688 0.227 0.723 0.421 ECR°pt 0.722 0.412 0.756 0.594 tribute a lot to improve naive entity-based method, which do validate an complment effect of their learned intent features. 5 Related Work 5.1 Query intent understanding Query intent or search intent has been studied intensively from various views. A popular paradigm is to label several intents for each query, also called facets subgoals and subtopics in the literature, manully or by mining methods and then do classification (Hu et al., 2009; Li et al., 2008) based on that. Manually intent schemas range from 3 top level (Broder, 2002) to fine-grained subcatogories (Rose and Levinson, 2004) and taxonomy (Yin and Shah, 2010). Intent tasks in NTCIR-10 (Sakai et al., 2013) also provide subtopic pools made by accessors. Another view of intent is more generic, mining or learning search intents without any kind of pre-defined intent category and clustering method is often used. Methods including (Sadikov et al., 2010; Yamamoto et al., 2012; Cheung and Li, 2012) cast intent as represented by a pattern or template consisting of a sequence of semantic concepts or lexical items. (Tan et al., 2012) encode intent in language models, aw</context>
</contexts>
<marker>Broder, 2002</marker>
<rawString>Andrei Broder. 2002. A taxonomy of web search. SIGIR Forum, 36(2):3–10, September.</rawString>
</citation>
<citation valid="true">
<authors>
<author>Moses S Charikar</author>
</authors>
<title>Similarity estimation techniques from rounding algorithms.</title>
<date>2002</date>
<booktitle>In Proceedings of the thiry-fourth annual ACM symposium on Theory of computing,</booktitle>
<pages>380--388</pages>
<publisher>ACM.</publisher>
<contexts>
<context position="13109" citStr="Charikar, 2002" startWordPosition="2196" endWordPosition="2197">ers may vary due to different scenarios. We just provide a framework of modeling nodes’ intent features, which actually mirror their proximity in query intent. To put it in more details, we use jaccard similarity for name shinglings and cosine similarity for domain and topic vector. As query log induced intent topic graph is of considerable large size, the pair-wise similarity is computationally prohibitive, hence we use Local Sensitive Hash (Indyk and Motwani, 1998) for each similarity metric so as to compute ISim just in candidate set. We use random hyperplane based hash family proposed in (Charikar, 2002) and set the hash code dimension and hash table numbers empirically to ensure the number of nodes falling into each bucket is relatively stable. 3.2 Merging nodes Although our idea of specifying intent topics by context better models the multi-facets of queries, it obviously also brings a sparse issue. For example, in one session user query beep lyrics and click www.lyricsandsongs.com, lyrics is tagged with the song beep and the musician Pussycat Dolls, in another scenario lyrics occurs with the song what you know and url www.dapslyrics.com, intents behind these two nodes are so similar that t</context>
</contexts>
<marker>Charikar, 2002</marker>
<rawString>Moses S Charikar. 2002. Similarity estimation techniques from rounding algorithms. In Proceedings of the thiry-fourth annual ACM symposium on Theory of computing, pages 380–388. ACM.</rawString>
</citation>
<citation valid="true">
<authors>
<author>Jackie Chi Kit Cheung</author>
<author>Xiao Li</author>
</authors>
<title>Sequence clustering and labeling for unsupervised query intent discovery.</title>
<date>2012</date>
<booktitle>In Proceedings of the fifth ACM international conference on Web search and data mining,</booktitle>
<pages>383--392</pages>
<publisher>ACM.</publisher>
<contexts>
<context position="30514" citStr="Cheung and Li, 2012" startWordPosition="5216" endWordPosition="5219">opics in the literature, manully or by mining methods and then do classification (Hu et al., 2009; Li et al., 2008) based on that. Manually intent schemas range from 3 top level (Broder, 2002) to fine-grained subcatogories (Rose and Levinson, 2004) and taxonomy (Yin and Shah, 2010). Intent tasks in NTCIR-10 (Sakai et al., 2013) also provide subtopic pools made by accessors. Another view of intent is more generic, mining or learning search intents without any kind of pre-defined intent category and clustering method is often used. Methods including (Sadikov et al., 2010; Yamamoto et al., 2012; Cheung and Li, 2012) cast intent as represented by a pattern or template consisting of a sequence of semantic concepts or lexical items. (Tan et al., 2012) encode intent in language models, aware of long-lasting interests. (Ren et al., 2014) uses an unsupervised heterogeneous clustering. (Yin and Shah, 2010) capture generic intents around a certain named entities and model their relationships in a tree taxonomy and (Wang et al., 2009) mine broad latent modifiers of intent aspect , which are similar to our motivation, while we model more than intent phrases, but intent topics. We do not split queries into clusters</context>
</contexts>
<marker>Cheung, Li, 2012</marker>
<rawString>Jackie Chi Kit Cheung and Xiao Li. 2012. Sequence clustering and labeling for unsupervised query intent discovery. In Proceedings of the fifth ACM international conference on Web search and data mining, pages 383–392. ACM.</rawString>
</citation>
<citation valid="true">
<authors>
<author>W Bruce Croft</author>
<author>Michael Bendersky</author>
<author>Hang Li</author>
<author>Gu Xu</author>
</authors>
<title>Query representation and understanding workshop.</title>
<date>2010</date>
<booktitle>In SIGIR Forum,</booktitle>
<volume>44</volume>
<pages>48--53</pages>
<contexts>
<context position="1344" citStr="Croft et al., 2010" startWordPosition="196" endWordPosition="199">a unified knowledge graph based framework, in a way to exploit and expand knowledge graph which we call ‘tailor’. We collaboratively exploit global knowledge in knowledge graphs and local contexts in query log to initialize intent representation, then propagate the enriched features in a graph consisting of intent topics using an unsupervised algorithm. The experiments prove intent topics with knowledge graph enriched features significantly enhance intent understanding. 1 Introduction Query understanding is the process of generating a representation which characterizes a user’s search intent (Croft et al., 2010), which is of vital importance for information retrieval. However, users are remarkably laconic in describing their information needs due to anomalous state of knowledge (Belkin et al., 1982), resulting in vague and underspecified queries, which makes it especially difficult to understand and locate what they intended for in mountains of web data. The problem is often significantly compounded that people convey their intent rather in a series of behaviors called a search session than a single query, leaving a wealth of clues including query reformulations, page visits, dwell times, etc. What’s</context>
</contexts>
<marker>Croft, Bendersky, Li, Xu, 2010</marker>
<rawString>W Bruce Croft, Michael Bendersky, Hang Li, and Gu Xu. 2010. Query representation and understanding workshop. In SIGIR Forum, volume 44, pages 48–53.</rawString>
</citation>
<citation valid="true">
<authors>
<author>Paolo Ferragina</author>
<author>Ugo Scaiella</author>
</authors>
<title>Fast and accurate annotation of short texts with wikipedia pages.</title>
<date>2012</date>
<journal>IEEE software,</journal>
<volume>29</volume>
<issue>1</issue>
<contexts>
<context position="18701" citStr="Ferragina and Scaiella, 2012" startWordPosition="3197" endWordPosition="3200">s over three months in 2006. Table 1: The query set # of sessions 35140 # of queries 271127 # of users 21378 # of urls 63019 We preprocess the query log by keeping urls occurring more than 3 times and queries with 2 to 40 characters, then extract sessions considering 25 minutes duration. While user session segmentation can be improved with more sophisticated algorithms, this simple low-cost heuristic performs adequately for our purposes. We then move on to map queries to Freebase and empirically filter sessions that are less entity-centric. We use an annotation tool especially for short text (Ferragina and Scaiella, 2012) called Tagme3 to recognize entities and observe only 16% of all the queries are exactly an entity itself, which means most of queries do have refiner words to convey information need. To ensure the precision of recognized entities, we set a significant threshold and bottom line threshold , queries should have at least one recognized entity with a likelihood above significant level, and those below bottom line are ignored. They are 0.19 and 0.05 in our work, which may vary with entity recognition method. The normalized 3http://tagme.di.unipi.it/ 1074 percentage Queryosetn Testbsessionasetg Fre</context>
</contexts>
<marker>Ferragina, Scaiella, 2012</marker>
<rawString>Paolo Ferragina and Ugo Scaiella. 2012. Fast and accurate annotation of short texts with wikipedia pages. IEEE software, 29(1).</rawString>
</citation>
<citation valid="true">
<authors>
<author>Jian Hu</author>
<author>Gang Wang</author>
<author>Fred Lochovsky</author>
<author>Jian-tao Sun</author>
<author>Zheng Chen</author>
</authors>
<title>Understanding user’s query intent with wikipedia.</title>
<date>2009</date>
<booktitle>In Proceedings of the 18th international conference on World wide web,</booktitle>
<pages>471--480</pages>
<publisher>ACM.</publisher>
<contexts>
<context position="29991" citStr="Hu et al., 2009" startWordPosition="5129" endWordPosition="5132">=5 K=10 MAP GMAP MAP GMAP GP 0.177 0.000 0.232 0.002 E 0.676 0.166 0.707 0.355 EC°pt 0.708 0.412 0.739 0.579 ER°pt 0.688 0.227 0.723 0.421 ECR°pt 0.722 0.412 0.756 0.594 tribute a lot to improve naive entity-based method, which do validate an complment effect of their learned intent features. 5 Related Work 5.1 Query intent understanding Query intent or search intent has been studied intensively from various views. A popular paradigm is to label several intents for each query, also called facets subgoals and subtopics in the literature, manully or by mining methods and then do classification (Hu et al., 2009; Li et al., 2008) based on that. Manually intent schemas range from 3 top level (Broder, 2002) to fine-grained subcatogories (Rose and Levinson, 2004) and taxonomy (Yin and Shah, 2010). Intent tasks in NTCIR-10 (Sakai et al., 2013) also provide subtopic pools made by accessors. Another view of intent is more generic, mining or learning search intents without any kind of pre-defined intent category and clustering method is often used. Methods including (Sadikov et al., 2010; Yamamoto et al., 2012; Cheung and Li, 2012) cast intent as represented by a pattern or template consisting of a sequence</context>
</contexts>
<marker>Hu, Wang, Lochovsky, Sun, Chen, 2009</marker>
<rawString>Jian Hu, Gang Wang, Fred Lochovsky, Jian-tao Sun, and Zheng Chen. 2009. Understanding user’s query intent with wikipedia. In Proceedings of the 18th international conference on World wide web, pages 471–480. ACM.</rawString>
</citation>
<citation valid="true">
<authors>
<author>Piotr Indyk</author>
<author>Rajeev Motwani</author>
</authors>
<title>Approximate nearest neighbors: towards removing the curse of dimensionality.</title>
<date>1998</date>
<booktitle>In Proceedings of the thirtieth annual ACM symposium on Theory of computing,</booktitle>
<pages>604--613</pages>
<publisher>ACM.</publisher>
<contexts>
<context position="12965" citStr="Indyk and Motwani, 1998" startWordPosition="2169" endWordPosition="2172">string similarity, DSim the similarity of their domain feature and T Sim the topic vector similarity, with β, γ and η controlling the weight. The parameters may vary due to different scenarios. We just provide a framework of modeling nodes’ intent features, which actually mirror their proximity in query intent. To put it in more details, we use jaccard similarity for name shinglings and cosine similarity for domain and topic vector. As query log induced intent topic graph is of considerable large size, the pair-wise similarity is computationally prohibitive, hence we use Local Sensitive Hash (Indyk and Motwani, 1998) for each similarity metric so as to compute ISim just in candidate set. We use random hyperplane based hash family proposed in (Charikar, 2002) and set the hash code dimension and hash table numbers empirically to ensure the number of nodes falling into each bucket is relatively stable. 3.2 Merging nodes Although our idea of specifying intent topics by context better models the multi-facets of queries, it obviously also brings a sparse issue. For example, in one session user query beep lyrics and click www.lyricsandsongs.com, lyrics is tagged with the song beep and the musician Pussycat Dolls</context>
</contexts>
<marker>Indyk, Motwani, 1998</marker>
<rawString>Piotr Indyk and Rajeev Motwani. 1998. Approximate nearest neighbors: towards removing the curse of dimensionality. In Proceedings of the thirtieth annual ACM symposium on Theory of computing, pages 604–613. ACM.</rawString>
</citation>
<citation valid="true">
<authors>
<author>Daxin Jiang</author>
<author>Jian Pei</author>
<author>Hang Li</author>
</authors>
<title>Mining search and browse logs for web search: A survey.</title>
<date>2013</date>
<journal>ACM Trans. Intell. Syst. Technol.,</journal>
<volume>4</volume>
<issue>4</issue>
<contexts>
<context position="31441" citStr="Jiang et al., 2013" startWordPosition="5373" endWordPosition="5376">tents around a certain named entities and model their relationships in a tree taxonomy and (Wang et al., 2009) mine broad latent modifiers of intent aspect , which are similar to our motivation, while we model more than intent phrases, but intent topics. We do not split queries into clusters or subtopics relevant to the original query to indicate a intent, but link them in an graph with intent feature similarity, weakly or strongly, in a holistical view. On the other hand, previous research can be categorized by what kind of resources they rely on. Quite an amount of work leverage query logs (Jiang et al., 2013), including query reformulations (Radlinski et al., 2010), click-through data (Li et al., 2008). There are also works using sponsered data (Yamamoto et al., 2012) and interactive data (Ruotsalo et al., 2013). The new trend of integrating knowledge graph will be discussed next. 5.2 Knowledge graph on intent understanding Instead of summarizing queries into concepts by clustering, recently there appears a tendency to use concpets from knowledge graph resources. Some researchers manage to build entity graph from queries (Bordino et al., 2013a) (Bordino et al., 2013b; Yu et al., 2014), some in a s</context>
</contexts>
<marker>Jiang, Pei, Li, 2013</marker>
<rawString>Daxin Jiang, Jian Pei, and Hang Li. 2013. Mining search and browse logs for web search: A survey. ACM Trans. Intell. Syst. Technol., 4(4):57:1–57:37, October.</rawString>
</citation>
<citation valid="true">
<authors>
<author>Xiao Li</author>
<author>Ye-Yi Wang</author>
<author>Alex Acero</author>
</authors>
<title>Learning query intent from regularized click graphs.</title>
<date>2008</date>
<booktitle>In Proceedings of the 31st Annual International ACM SIGIR Conference on Research and Development in Information Retrieval, SIGIR ’08,</booktitle>
<pages>339--346</pages>
<publisher>ACM.</publisher>
<location>New York, NY, USA.</location>
<contexts>
<context position="30009" citStr="Li et al., 2008" startWordPosition="5133" endWordPosition="5136">MAP GMAP GP 0.177 0.000 0.232 0.002 E 0.676 0.166 0.707 0.355 EC°pt 0.708 0.412 0.739 0.579 ER°pt 0.688 0.227 0.723 0.421 ECR°pt 0.722 0.412 0.756 0.594 tribute a lot to improve naive entity-based method, which do validate an complment effect of their learned intent features. 5 Related Work 5.1 Query intent understanding Query intent or search intent has been studied intensively from various views. A popular paradigm is to label several intents for each query, also called facets subgoals and subtopics in the literature, manully or by mining methods and then do classification (Hu et al., 2009; Li et al., 2008) based on that. Manually intent schemas range from 3 top level (Broder, 2002) to fine-grained subcatogories (Rose and Levinson, 2004) and taxonomy (Yin and Shah, 2010). Intent tasks in NTCIR-10 (Sakai et al., 2013) also provide subtopic pools made by accessors. Another view of intent is more generic, mining or learning search intents without any kind of pre-defined intent category and clustering method is often used. Methods including (Sadikov et al., 2010; Yamamoto et al., 2012; Cheung and Li, 2012) cast intent as represented by a pattern or template consisting of a sequence of semantic conce</context>
<context position="31536" citStr="Li et al., 2008" startWordPosition="5387" endWordPosition="5390">t al., 2009) mine broad latent modifiers of intent aspect , which are similar to our motivation, while we model more than intent phrases, but intent topics. We do not split queries into clusters or subtopics relevant to the original query to indicate a intent, but link them in an graph with intent feature similarity, weakly or strongly, in a holistical view. On the other hand, previous research can be categorized by what kind of resources they rely on. Quite an amount of work leverage query logs (Jiang et al., 2013), including query reformulations (Radlinski et al., 2010), click-through data (Li et al., 2008). There are also works using sponsered data (Yamamoto et al., 2012) and interactive data (Ruotsalo et al., 2013). The new trend of integrating knowledge graph will be discussed next. 5.2 Knowledge graph on intent understanding Instead of summarizing queries into concepts by clustering, recently there appears a tendency to use concpets from knowledge graph resources. Some researchers manage to build entity graph from queries (Bordino et al., 2013a) (Bordino et al., 2013b; Yu et al., 2014), some in a structure view, interpret quries into knowledge base fit template (Pound et al., 2012; Li et al.</context>
</contexts>
<marker>Li, Wang, Acero, 2008</marker>
<rawString>Xiao Li, Ye-Yi Wang, and Alex Acero. 2008. Learning query intent from regularized click graphs. In Proceedings of the 31st Annual International ACM SIGIR Conference on Research and Development in Information Retrieval, SIGIR ’08, pages 339–346, New York, NY, USA. ACM.</rawString>
</citation>
<citation valid="true">
<authors>
<author>Yanen Li</author>
<author>Bo-June Paul Hsu</author>
<author>ChengXiang Zhai</author>
</authors>
<title>Unsupervised identification of synonymous query intent templates for attribute intents.</title>
<date>2013</date>
<booktitle>In Proceedings of the 22nd ACM international conference on Conference on information &amp;#38; knowledge management, CIKM ’13,</booktitle>
<pages>2029--2038</pages>
<publisher>ACM.</publisher>
<location>New York, NY, USA.</location>
<contexts>
<context position="32143" citStr="Li et al., 2013" startWordPosition="5485" endWordPosition="5488">l., 2008). There are also works using sponsered data (Yamamoto et al., 2012) and interactive data (Ruotsalo et al., 2013). The new trend of integrating knowledge graph will be discussed next. 5.2 Knowledge graph on intent understanding Instead of summarizing queries into concepts by clustering, recently there appears a tendency to use concpets from knowledge graph resources. Some researchers manage to build entity graph from queries (Bordino et al., 2013a) (Bordino et al., 2013b; Yu et al., 2014), some in a structure view, interpret quries into knowledge base fit template (Pound et al., 2012; Li et al., 2013). (Pantel et al., 2012) models latent intent to mine entity type distributions. (Ren et al., 2014) utilizes knowledge graph resources in a hetrogeneous view. (Lin et al., 2012) also pays attention to refiners, but restricted to limited domains, while our method is more general. 6 Conclusion In this paper, we tailor knowledge graph to represent query intent behind entity words, refiners and clicked urls in a unified framework, taking them as intent topic nodes connected in a large graph. We manage to get a contextualized intent depiction exploiting global knowledge in Freebase, then propagate t</context>
</contexts>
<marker>Li, Hsu, Zhai, 2013</marker>
<rawString>Yanen Li, Bo-June Paul Hsu, and ChengXiang Zhai. 2013. Unsupervised identification of synonymous query intent templates for attribute intents. In Proceedings of the 22nd ACM international conference on Conference on information &amp;#38; knowledge management, CIKM ’13, pages 2029–2038, New York, NY, USA. ACM.</rawString>
</citation>
<citation valid="true">
<authors>
<author>Thomas Lin</author>
<author>Patrick Pantel</author>
<author>Michael Gamon</author>
<author>Anitha Kannan</author>
<author>Ariel Fuxman</author>
</authors>
<title>Active objects: Actions for entity-centric search.</title>
<date>2012</date>
<booktitle>In Proceedings of the 21st international conference on World Wide Web,</booktitle>
<pages>589--598</pages>
<publisher>ACM.</publisher>
<contexts>
<context position="32319" citStr="Lin et al., 2012" startWordPosition="5514" endWordPosition="5517"> discussed next. 5.2 Knowledge graph on intent understanding Instead of summarizing queries into concepts by clustering, recently there appears a tendency to use concpets from knowledge graph resources. Some researchers manage to build entity graph from queries (Bordino et al., 2013a) (Bordino et al., 2013b; Yu et al., 2014), some in a structure view, interpret quries into knowledge base fit template (Pound et al., 2012; Li et al., 2013). (Pantel et al., 2012) models latent intent to mine entity type distributions. (Ren et al., 2014) utilizes knowledge graph resources in a hetrogeneous view. (Lin et al., 2012) also pays attention to refiners, but restricted to limited domains, while our method is more general. 6 Conclusion In this paper, we tailor knowledge graph to represent query intent behind entity words, refiners and clicked urls in a unified framework, taking them as intent topic nodes connected in a large graph. We manage to get a contextualized intent depiction exploiting global knowledge in Freebase, then propagate the feature to cover more intent topics. We show in experiments the knowledge graph enriched representation is reasonable and explainable, and the intents feature of refiners an</context>
</contexts>
<marker>Lin, Pantel, Gamon, Kannan, Fuxman, 2012</marker>
<rawString>Thomas Lin, Patrick Pantel, Michael Gamon, Anitha Kannan, and Ariel Fuxman. 2012. Active objects: Actions for entity-centric search. In Proceedings of the 21st international conference on World Wide Web, pages 589–598. ACM.</rawString>
</citation>
<citation valid="true">
<authors>
<author>Patrick Pantel</author>
<author>Thomas Lin</author>
<author>Michael Gamon</author>
</authors>
<title>Mining entity types from query logs via user intent modeling.</title>
<date>2012</date>
<booktitle>In Proceedings of the 50th Annual Meeting of the Association for Computational Linguistics: Long Papers-Volume 1,</booktitle>
<pages>563--571</pages>
<publisher>Association</publisher>
<institution>for Computational Linguistics.</institution>
<contexts>
<context position="32166" citStr="Pantel et al., 2012" startWordPosition="5489" endWordPosition="5492">e also works using sponsered data (Yamamoto et al., 2012) and interactive data (Ruotsalo et al., 2013). The new trend of integrating knowledge graph will be discussed next. 5.2 Knowledge graph on intent understanding Instead of summarizing queries into concepts by clustering, recently there appears a tendency to use concpets from knowledge graph resources. Some researchers manage to build entity graph from queries (Bordino et al., 2013a) (Bordino et al., 2013b; Yu et al., 2014), some in a structure view, interpret quries into knowledge base fit template (Pound et al., 2012; Li et al., 2013). (Pantel et al., 2012) models latent intent to mine entity type distributions. (Ren et al., 2014) utilizes knowledge graph resources in a hetrogeneous view. (Lin et al., 2012) also pays attention to refiners, but restricted to limited domains, while our method is more general. 6 Conclusion In this paper, we tailor knowledge graph to represent query intent behind entity words, refiners and clicked urls in a unified framework, taking them as intent topic nodes connected in a large graph. We manage to get a contextualized intent depiction exploiting global knowledge in Freebase, then propagate the feature to cover mor</context>
</contexts>
<marker>Pantel, Lin, Gamon, 2012</marker>
<rawString>Patrick Pantel, Thomas Lin, and Michael Gamon. 2012. Mining entity types from query logs via user intent modeling. In Proceedings of the 50th Annual Meeting of the Association for Computational Linguistics: Long Papers-Volume 1, pages 563–571. Association for Computational Linguistics.</rawString>
</citation>
<citation valid="true">
<authors>
<author>Jeffrey Pound</author>
<author>Alexander K Hudek</author>
<author>Ihab F Ilyas</author>
<author>Grant Weddell</author>
</authors>
<title>Interpreting keyword queries over web knowledge bases.</title>
<date>2012</date>
<booktitle>In Proceedings of the 21st ACM international conference on Information and knowledge management,</booktitle>
<pages>305--314</pages>
<publisher>ACM.</publisher>
<contexts>
<context position="32125" citStr="Pound et al., 2012" startWordPosition="5481" endWordPosition="5484">hrough data (Li et al., 2008). There are also works using sponsered data (Yamamoto et al., 2012) and interactive data (Ruotsalo et al., 2013). The new trend of integrating knowledge graph will be discussed next. 5.2 Knowledge graph on intent understanding Instead of summarizing queries into concepts by clustering, recently there appears a tendency to use concpets from knowledge graph resources. Some researchers manage to build entity graph from queries (Bordino et al., 2013a) (Bordino et al., 2013b; Yu et al., 2014), some in a structure view, interpret quries into knowledge base fit template (Pound et al., 2012; Li et al., 2013). (Pantel et al., 2012) models latent intent to mine entity type distributions. (Ren et al., 2014) utilizes knowledge graph resources in a hetrogeneous view. (Lin et al., 2012) also pays attention to refiners, but restricted to limited domains, while our method is more general. 6 Conclusion In this paper, we tailor knowledge graph to represent query intent behind entity words, refiners and clicked urls in a unified framework, taking them as intent topic nodes connected in a large graph. We manage to get a contextualized intent depiction exploiting global knowledge in Freebase</context>
</contexts>
<marker>Pound, Hudek, Ilyas, Weddell, 2012</marker>
<rawString>Jeffrey Pound, Alexander K Hudek, Ihab F Ilyas, and Grant Weddell. 2012. Interpreting keyword queries over web knowledge bases. In Proceedings of the 21st ACM international conference on Information and knowledge management, pages 305–314. ACM.</rawString>
</citation>
<citation valid="true">
<authors>
<author>Filip Radlinski</author>
<author>Martin Szummer</author>
<author>Nick Craswell</author>
</authors>
<title>Inferring query intent from reformulations and clicks.</title>
<date>2010</date>
<booktitle>In Proceedings of the 19th international conference on World wide web,</booktitle>
<pages>1171--1172</pages>
<publisher>ACM.</publisher>
<contexts>
<context position="31498" citStr="Radlinski et al., 2010" startWordPosition="5381" endWordPosition="5384"> relationships in a tree taxonomy and (Wang et al., 2009) mine broad latent modifiers of intent aspect , which are similar to our motivation, while we model more than intent phrases, but intent topics. We do not split queries into clusters or subtopics relevant to the original query to indicate a intent, but link them in an graph with intent feature similarity, weakly or strongly, in a holistical view. On the other hand, previous research can be categorized by what kind of resources they rely on. Quite an amount of work leverage query logs (Jiang et al., 2013), including query reformulations (Radlinski et al., 2010), click-through data (Li et al., 2008). There are also works using sponsered data (Yamamoto et al., 2012) and interactive data (Ruotsalo et al., 2013). The new trend of integrating knowledge graph will be discussed next. 5.2 Knowledge graph on intent understanding Instead of summarizing queries into concepts by clustering, recently there appears a tendency to use concpets from knowledge graph resources. Some researchers manage to build entity graph from queries (Bordino et al., 2013a) (Bordino et al., 2013b; Yu et al., 2014), some in a structure view, interpret quries into knowledge base fit t</context>
</contexts>
<marker>Radlinski, Szummer, Craswell, 2010</marker>
<rawString>Filip Radlinski, Martin Szummer, and Nick Craswell. 2010. Inferring query intent from reformulations and clicks. In Proceedings of the 19th international conference on World wide web, pages 1171–1172. ACM.</rawString>
</citation>
<citation valid="true">
<authors>
<author>Xiang Ren</author>
<author>Yujing Wang</author>
<author>Xiao Yu</author>
<author>Jun Yan</author>
<author>Zheng Chen</author>
<author>Jiawei Han</author>
</authors>
<title>Heterogeneous graphbased intent learning with queries, web pages and wikipedia concepts.</title>
<date>2014</date>
<booktitle>In Proceedings of the 7th ACM International Conference on Web Search and Data Mining, WSDM ’14,</booktitle>
<pages>23--32</pages>
<publisher>ACM.</publisher>
<location>New York, NY, USA.</location>
<contexts>
<context position="30735" citStr="Ren et al., 2014" startWordPosition="5253" endWordPosition="5256">Rose and Levinson, 2004) and taxonomy (Yin and Shah, 2010). Intent tasks in NTCIR-10 (Sakai et al., 2013) also provide subtopic pools made by accessors. Another view of intent is more generic, mining or learning search intents without any kind of pre-defined intent category and clustering method is often used. Methods including (Sadikov et al., 2010; Yamamoto et al., 2012; Cheung and Li, 2012) cast intent as represented by a pattern or template consisting of a sequence of semantic concepts or lexical items. (Tan et al., 2012) encode intent in language models, aware of long-lasting interests. (Ren et al., 2014) uses an unsupervised heterogeneous clustering. (Yin and Shah, 2010) capture generic intents around a certain named entities and model their relationships in a tree taxonomy and (Wang et al., 2009) mine broad latent modifiers of intent aspect , which are similar to our motivation, while we model more than intent phrases, but intent topics. We do not split queries into clusters or subtopics relevant to the original query to indicate a intent, but link them in an graph with intent feature similarity, weakly or strongly, in a holistical view. On the other hand, previous research can be categorize</context>
<context position="32241" citStr="Ren et al., 2014" startWordPosition="5502" endWordPosition="5505"> (Ruotsalo et al., 2013). The new trend of integrating knowledge graph will be discussed next. 5.2 Knowledge graph on intent understanding Instead of summarizing queries into concepts by clustering, recently there appears a tendency to use concpets from knowledge graph resources. Some researchers manage to build entity graph from queries (Bordino et al., 2013a) (Bordino et al., 2013b; Yu et al., 2014), some in a structure view, interpret quries into knowledge base fit template (Pound et al., 2012; Li et al., 2013). (Pantel et al., 2012) models latent intent to mine entity type distributions. (Ren et al., 2014) utilizes knowledge graph resources in a hetrogeneous view. (Lin et al., 2012) also pays attention to refiners, but restricted to limited domains, while our method is more general. 6 Conclusion In this paper, we tailor knowledge graph to represent query intent behind entity words, refiners and clicked urls in a unified framework, taking them as intent topic nodes connected in a large graph. We manage to get a contextualized intent depiction exploiting global knowledge in Freebase, then propagate the feature to cover more intent topics. We show in experiments the knowledge graph enriched repres</context>
</contexts>
<marker>Ren, Wang, Yu, Yan, Chen, Han, 2014</marker>
<rawString>Xiang Ren, Yujing Wang, Xiao Yu, Jun Yan, Zheng Chen, and Jiawei Han. 2014. Heterogeneous graphbased intent learning with queries, web pages and wikipedia concepts. In Proceedings of the 7th ACM International Conference on Web Search and Data Mining, WSDM ’14, pages 23–32, New York, NY, USA. ACM.</rawString>
</citation>
<citation valid="true">
<authors>
<author>Daniel E Rose</author>
<author>Danny Levinson</author>
</authors>
<title>Understanding user goals in web search.</title>
<date>2004</date>
<booktitle>In Proceedings of the 13th International Conference on World</booktitle>
<pages>1079</pages>
<contexts>
<context position="30142" citStr="Rose and Levinson, 2004" startWordPosition="5154" endWordPosition="5157">°pt 0.722 0.412 0.756 0.594 tribute a lot to improve naive entity-based method, which do validate an complment effect of their learned intent features. 5 Related Work 5.1 Query intent understanding Query intent or search intent has been studied intensively from various views. A popular paradigm is to label several intents for each query, also called facets subgoals and subtopics in the literature, manully or by mining methods and then do classification (Hu et al., 2009; Li et al., 2008) based on that. Manually intent schemas range from 3 top level (Broder, 2002) to fine-grained subcatogories (Rose and Levinson, 2004) and taxonomy (Yin and Shah, 2010). Intent tasks in NTCIR-10 (Sakai et al., 2013) also provide subtopic pools made by accessors. Another view of intent is more generic, mining or learning search intents without any kind of pre-defined intent category and clustering method is often used. Methods including (Sadikov et al., 2010; Yamamoto et al., 2012; Cheung and Li, 2012) cast intent as represented by a pattern or template consisting of a sequence of semantic concepts or lexical items. (Tan et al., 2012) encode intent in language models, aware of long-lasting interests. (Ren et al., 2014) uses a</context>
</contexts>
<marker>Rose, Levinson, 2004</marker>
<rawString>Daniel E. Rose and Danny Levinson. 2004. Understanding user goals in web search. In Proceedings of the 13th International Conference on World 1079</rawString>
</citation>
<citation valid="false">
<authors>
<author>Wide Web</author>
</authors>
<journal>WWW</journal>
<volume>04</volume>
<pages>13--19</pages>
<publisher>ACM.</publisher>
<location>New York, NY, USA.</location>
<marker>Web, </marker>
<rawString>Wide Web, WWW ’04, pages 13–19, New York, NY, USA. ACM.</rawString>
</citation>
<citation valid="true">
<authors>
<author>Tuukka Ruotsalo</author>
<author>Jaakko Peltonen</author>
<author>Manuel Eugster</author>
</authors>
<title>Dorota Głowacka, Ksenia Konyushkova, Kumaripaba Athukorala, Ilkka Kosunen,</title>
<date>2013</date>
<booktitle>In Proceedings of the 22nd ACM international conference on Conference on information &amp; knowledge management,</booktitle>
<pages>1759--1764</pages>
<publisher>ACM.</publisher>
<location>Aki</location>
<contexts>
<context position="31648" citStr="Ruotsalo et al., 2013" startWordPosition="5406" endWordPosition="5409">model more than intent phrases, but intent topics. We do not split queries into clusters or subtopics relevant to the original query to indicate a intent, but link them in an graph with intent feature similarity, weakly or strongly, in a holistical view. On the other hand, previous research can be categorized by what kind of resources they rely on. Quite an amount of work leverage query logs (Jiang et al., 2013), including query reformulations (Radlinski et al., 2010), click-through data (Li et al., 2008). There are also works using sponsered data (Yamamoto et al., 2012) and interactive data (Ruotsalo et al., 2013). The new trend of integrating knowledge graph will be discussed next. 5.2 Knowledge graph on intent understanding Instead of summarizing queries into concepts by clustering, recently there appears a tendency to use concpets from knowledge graph resources. Some researchers manage to build entity graph from queries (Bordino et al., 2013a) (Bordino et al., 2013b; Yu et al., 2014), some in a structure view, interpret quries into knowledge base fit template (Pound et al., 2012; Li et al., 2013). (Pantel et al., 2012) models latent intent to mine entity type distributions. (Ren et al., 2014) utiliz</context>
</contexts>
<marker>Ruotsalo, Peltonen, Eugster, 2013</marker>
<rawString>Tuukka Ruotsalo, Jaakko Peltonen, Manuel Eugster, Dorota Głowacka, Ksenia Konyushkova, Kumaripaba Athukorala, Ilkka Kosunen, Aki Reijonen, Petri Myllym¨aki, Giulio Jacucci, et al. 2013. Directing exploratory search with interactive intent modeling. In Proceedings of the 22nd ACM international conference on Conference on information &amp; knowledge management, pages 1759–1764. ACM.</rawString>
</citation>
<citation valid="true">
<authors>
<author>Eldar Sadikov</author>
<author>Jayant Madhavan</author>
<author>Lu Wang</author>
<author>Alon Halevy</author>
</authors>
<title>Clustering query refinements by user intent.</title>
<date>2010</date>
<booktitle>In Proceedings of the 19th international conference on World wide web,</booktitle>
<pages>841--850</pages>
<publisher>ACM.</publisher>
<contexts>
<context position="30469" citStr="Sadikov et al., 2010" startWordPosition="5208" endWordPosition="5211">h query, also called facets subgoals and subtopics in the literature, manully or by mining methods and then do classification (Hu et al., 2009; Li et al., 2008) based on that. Manually intent schemas range from 3 top level (Broder, 2002) to fine-grained subcatogories (Rose and Levinson, 2004) and taxonomy (Yin and Shah, 2010). Intent tasks in NTCIR-10 (Sakai et al., 2013) also provide subtopic pools made by accessors. Another view of intent is more generic, mining or learning search intents without any kind of pre-defined intent category and clustering method is often used. Methods including (Sadikov et al., 2010; Yamamoto et al., 2012; Cheung and Li, 2012) cast intent as represented by a pattern or template consisting of a sequence of semantic concepts or lexical items. (Tan et al., 2012) encode intent in language models, aware of long-lasting interests. (Ren et al., 2014) uses an unsupervised heterogeneous clustering. (Yin and Shah, 2010) capture generic intents around a certain named entities and model their relationships in a tree taxonomy and (Wang et al., 2009) mine broad latent modifiers of intent aspect , which are similar to our motivation, while we model more than intent phrases, but intent </context>
</contexts>
<marker>Sadikov, Madhavan, Wang, Halevy, 2010</marker>
<rawString>Eldar Sadikov, Jayant Madhavan, Lu Wang, and Alon Halevy. 2010. Clustering query refinements by user intent. In Proceedings of the 19th international conference on World wide web, pages 841–850. ACM.</rawString>
</citation>
<citation valid="true">
<authors>
<author>Tetsuya Sakai</author>
<author>Zhicheng Dou</author>
<author>Takehiro Yamamoto</author>
<author>Yiqun Liu</author>
<author>Min Zhang</author>
<author>Ruihua Song</author>
<author>MP Kato</author>
<author>M Iwata</author>
</authors>
<date>2013</date>
<booktitle>Overview of the ntcir-10 intent-2 task. Proceedings of NTCIR-10,</booktitle>
<pages>94--123</pages>
<contexts>
<context position="30223" citStr="Sakai et al., 2013" startWordPosition="5168" endWordPosition="5171">do validate an complment effect of their learned intent features. 5 Related Work 5.1 Query intent understanding Query intent or search intent has been studied intensively from various views. A popular paradigm is to label several intents for each query, also called facets subgoals and subtopics in the literature, manully or by mining methods and then do classification (Hu et al., 2009; Li et al., 2008) based on that. Manually intent schemas range from 3 top level (Broder, 2002) to fine-grained subcatogories (Rose and Levinson, 2004) and taxonomy (Yin and Shah, 2010). Intent tasks in NTCIR-10 (Sakai et al., 2013) also provide subtopic pools made by accessors. Another view of intent is more generic, mining or learning search intents without any kind of pre-defined intent category and clustering method is often used. Methods including (Sadikov et al., 2010; Yamamoto et al., 2012; Cheung and Li, 2012) cast intent as represented by a pattern or template consisting of a sequence of semantic concepts or lexical items. (Tan et al., 2012) encode intent in language models, aware of long-lasting interests. (Ren et al., 2014) uses an unsupervised heterogeneous clustering. (Yin and Shah, 2010) capture generic int</context>
</contexts>
<marker>Sakai, Dou, Yamamoto, Liu, Zhang, Song, Kato, Iwata, 2013</marker>
<rawString>Tetsuya Sakai, Zhicheng Dou, Takehiro Yamamoto, Yiqun Liu, Min Zhang, Ruihua Song, MP Kato, and M Iwata. 2013. Overview of the ntcir-10 intent-2 task. Proceedings of NTCIR-10, pages 94–123.</rawString>
</citation>
<citation valid="true">
<authors>
<author>Bin Tan</author>
<author>Yuanhua Lv</author>
<author>ChengXiang Zhai</author>
</authors>
<title>Mining long-lasting exploratory user interests from search history.</title>
<date>2012</date>
<booktitle>In Proceedings of the 21st ACM international conference on Information and knowledge management,</booktitle>
<pages>1477--1481</pages>
<publisher>ACM.</publisher>
<contexts>
<context position="30649" citStr="Tan et al., 2012" startWordPosition="5240" endWordPosition="5243">y intent schemas range from 3 top level (Broder, 2002) to fine-grained subcatogories (Rose and Levinson, 2004) and taxonomy (Yin and Shah, 2010). Intent tasks in NTCIR-10 (Sakai et al., 2013) also provide subtopic pools made by accessors. Another view of intent is more generic, mining or learning search intents without any kind of pre-defined intent category and clustering method is often used. Methods including (Sadikov et al., 2010; Yamamoto et al., 2012; Cheung and Li, 2012) cast intent as represented by a pattern or template consisting of a sequence of semantic concepts or lexical items. (Tan et al., 2012) encode intent in language models, aware of long-lasting interests. (Ren et al., 2014) uses an unsupervised heterogeneous clustering. (Yin and Shah, 2010) capture generic intents around a certain named entities and model their relationships in a tree taxonomy and (Wang et al., 2009) mine broad latent modifiers of intent aspect , which are similar to our motivation, while we model more than intent phrases, but intent topics. We do not split queries into clusters or subtopics relevant to the original query to indicate a intent, but link them in an graph with intent feature similarity, weakly or </context>
</contexts>
<marker>Tan, Lv, Zhai, 2012</marker>
<rawString>Bin Tan, Yuanhua Lv, and ChengXiang Zhai. 2012. Mining long-lasting exploratory user interests from search history. In Proceedings of the 21st ACM international conference on Information and knowledge management, pages 1477–1481. ACM.</rawString>
</citation>
<citation valid="true">
<authors>
<author>Xuanhui Wang</author>
<author>Deepayan Chakrabarti</author>
<author>Kunal Punera</author>
</authors>
<title>Mining broad latent query aspects from search sessions.</title>
<date>2009</date>
<booktitle>In Proceedings of the 15th ACM SIGKDD international conference on Knowledge discovery and data mining,</booktitle>
<pages>867--876</pages>
<publisher>ACM.</publisher>
<contexts>
<context position="30932" citStr="Wang et al., 2009" startWordPosition="5285" endWordPosition="5288">ing or learning search intents without any kind of pre-defined intent category and clustering method is often used. Methods including (Sadikov et al., 2010; Yamamoto et al., 2012; Cheung and Li, 2012) cast intent as represented by a pattern or template consisting of a sequence of semantic concepts or lexical items. (Tan et al., 2012) encode intent in language models, aware of long-lasting interests. (Ren et al., 2014) uses an unsupervised heterogeneous clustering. (Yin and Shah, 2010) capture generic intents around a certain named entities and model their relationships in a tree taxonomy and (Wang et al., 2009) mine broad latent modifiers of intent aspect , which are similar to our motivation, while we model more than intent phrases, but intent topics. We do not split queries into clusters or subtopics relevant to the original query to indicate a intent, but link them in an graph with intent feature similarity, weakly or strongly, in a holistical view. On the other hand, previous research can be categorized by what kind of resources they rely on. Quite an amount of work leverage query logs (Jiang et al., 2013), including query reformulations (Radlinski et al., 2010), click-through data (Li et al., 2</context>
</contexts>
<marker>Wang, Chakrabarti, Punera, 2009</marker>
<rawString>Xuanhui Wang, Deepayan Chakrabarti, and Kunal Punera. 2009. Mining broad latent query aspects from search sessions. In Proceedings of the 15th ACM SIGKDD international conference on Knowledge discovery and data mining, pages 867–876. ACM.</rawString>
</citation>
<citation valid="true">
<authors>
<author>Takehiro Yamamoto</author>
<author>Tetsuya Sakai</author>
<author>Mayu Iwata</author>
<author>Chen Yu</author>
<author>Ji-Rong Wen</author>
<author>Katsumi Tanaka</author>
</authors>
<title>The wisdom of advertisers: mining subgoals via query clustering.</title>
<date>2012</date>
<booktitle>In Proceedings of the 21st ACM international conference on Information and knowledge management,</booktitle>
<pages>505--514</pages>
<publisher>ACM.</publisher>
<contexts>
<context position="30492" citStr="Yamamoto et al., 2012" startWordPosition="5212" endWordPosition="5215">acets subgoals and subtopics in the literature, manully or by mining methods and then do classification (Hu et al., 2009; Li et al., 2008) based on that. Manually intent schemas range from 3 top level (Broder, 2002) to fine-grained subcatogories (Rose and Levinson, 2004) and taxonomy (Yin and Shah, 2010). Intent tasks in NTCIR-10 (Sakai et al., 2013) also provide subtopic pools made by accessors. Another view of intent is more generic, mining or learning search intents without any kind of pre-defined intent category and clustering method is often used. Methods including (Sadikov et al., 2010; Yamamoto et al., 2012; Cheung and Li, 2012) cast intent as represented by a pattern or template consisting of a sequence of semantic concepts or lexical items. (Tan et al., 2012) encode intent in language models, aware of long-lasting interests. (Ren et al., 2014) uses an unsupervised heterogeneous clustering. (Yin and Shah, 2010) capture generic intents around a certain named entities and model their relationships in a tree taxonomy and (Wang et al., 2009) mine broad latent modifiers of intent aspect , which are similar to our motivation, while we model more than intent phrases, but intent topics. We do not split</context>
</contexts>
<marker>Yamamoto, Sakai, Iwata, Yu, Wen, Tanaka, 2012</marker>
<rawString>Takehiro Yamamoto, Tetsuya Sakai, Mayu Iwata, Chen Yu, Ji-Rong Wen, and Katsumi Tanaka. 2012. The wisdom of advertisers: mining subgoals via query clustering. In Proceedings of the 21st ACM international conference on Information and knowledge management, pages 505–514. ACM.</rawString>
</citation>
<citation valid="true">
<authors>
<author>Xiaoxin Yin</author>
<author>Sarthak Shah</author>
</authors>
<title>Building taxonomy of web search intents for name entity queries.</title>
<date>2010</date>
<booktitle>In Proceedings of the 19th international conference on World wide web,</booktitle>
<pages>1001--1010</pages>
<publisher>ACM.</publisher>
<contexts>
<context position="2007" citStr="Yin and Shah, 2010" startWordPosition="303" endWordPosition="306">on retrieval. However, users are remarkably laconic in describing their information needs due to anomalous state of knowledge (Belkin et al., 1982), resulting in vague and underspecified queries, which makes it especially difficult to understand and locate what they intended for in mountains of web data. The problem is often significantly compounded that people convey their intent rather in a series of behaviors called a search session than a single query, leaving a wealth of clues including query reformulations, page visits, dwell times, etc. What’s more, as entities are taking center stage (Yin and Shah, 2010), string-level or phrase-level modeling of intent soon hits the bottleneck, calling for an entity-aware perspective. Knowledge repositories, better known as knowledge graphs, such as Wikipedia, DBpedia and Freebase, have been recently utilized for enhancing query understanding for the large amounts of world knowledge they’ve harvested about entities and facts. A widely accepted way to use knowledge graph is tying queries with it by annotating entities in them, also known as entity linking. However, information need is conveyed through more than entities. Quite a few non-entity words, aka refin</context>
<context position="30176" citStr="Yin and Shah, 2010" startWordPosition="5160" endWordPosition="5163">ot to improve naive entity-based method, which do validate an complment effect of their learned intent features. 5 Related Work 5.1 Query intent understanding Query intent or search intent has been studied intensively from various views. A popular paradigm is to label several intents for each query, also called facets subgoals and subtopics in the literature, manully or by mining methods and then do classification (Hu et al., 2009; Li et al., 2008) based on that. Manually intent schemas range from 3 top level (Broder, 2002) to fine-grained subcatogories (Rose and Levinson, 2004) and taxonomy (Yin and Shah, 2010). Intent tasks in NTCIR-10 (Sakai et al., 2013) also provide subtopic pools made by accessors. Another view of intent is more generic, mining or learning search intents without any kind of pre-defined intent category and clustering method is often used. Methods including (Sadikov et al., 2010; Yamamoto et al., 2012; Cheung and Li, 2012) cast intent as represented by a pattern or template consisting of a sequence of semantic concepts or lexical items. (Tan et al., 2012) encode intent in language models, aware of long-lasting interests. (Ren et al., 2014) uses an unsupervised heterogeneous clust</context>
</contexts>
<marker>Yin, Shah, 2010</marker>
<rawString>Xiaoxin Yin and Sarthak Shah. 2010. Building taxonomy of web search intents for name entity queries. In Proceedings of the 19th international conference on World wide web, pages 1001–1010. ACM.</rawString>
</citation>
<citation valid="true">
<authors>
<author>Xiao Yu</author>
<author>Hao Ma</author>
<author>Bo-June Hsu</author>
<author>Jiawei Han</author>
</authors>
<title>On building entity recommender systems using user click log and freebase knowledge.</title>
<date>2014</date>
<booktitle>In Proceedings of the 7th ACM International Conference on Web Search and Data Mining, WSDM ’14,</booktitle>
<pages>263--272</pages>
<publisher>ACM.</publisher>
<location>New York, NY, USA.</location>
<contexts>
<context position="32028" citStr="Yu et al., 2014" startWordPosition="5465" endWordPosition="5468">ery logs (Jiang et al., 2013), including query reformulations (Radlinski et al., 2010), click-through data (Li et al., 2008). There are also works using sponsered data (Yamamoto et al., 2012) and interactive data (Ruotsalo et al., 2013). The new trend of integrating knowledge graph will be discussed next. 5.2 Knowledge graph on intent understanding Instead of summarizing queries into concepts by clustering, recently there appears a tendency to use concpets from knowledge graph resources. Some researchers manage to build entity graph from queries (Bordino et al., 2013a) (Bordino et al., 2013b; Yu et al., 2014), some in a structure view, interpret quries into knowledge base fit template (Pound et al., 2012; Li et al., 2013). (Pantel et al., 2012) models latent intent to mine entity type distributions. (Ren et al., 2014) utilizes knowledge graph resources in a hetrogeneous view. (Lin et al., 2012) also pays attention to refiners, but restricted to limited domains, while our method is more general. 6 Conclusion In this paper, we tailor knowledge graph to represent query intent behind entity words, refiners and clicked urls in a unified framework, taking them as intent topic nodes connected in a large </context>
</contexts>
<marker>Yu, Ma, Hsu, Han, 2014</marker>
<rawString>Xiao Yu, Hao Ma, Bo-June (Paul) Hsu, and Jiawei Han. 2014. On building entity recommender systems using user click log and freebase knowledge. In Proceedings of the 7th ACM International Conference on Web Search and Data Mining, WSDM ’14, pages 263–272, New York, NY, USA. ACM.</rawString>
</citation>
<citation valid="true">
<authors>
<author>Xiaojin Zhu</author>
<author>Zoubin Ghahramani</author>
</authors>
<title>Learning from labeled and unlabeled data with label propagation.</title>
<date>2002</date>
<tech>Technical report, Technical Report CMUCALD-02-107,</tech>
<institution>Carnegie Mellon University.</institution>
<contexts>
<context position="14850" citStr="Zhu and Ghahramani, 2002" startWordPosition="2523" endWordPosition="2526">pairs with ISim higher than a merge threshold θ can be seen as duplicates. The merge process is summarized in Algorithm 1. Algorithm 1: Merging similar nodes Input: G =&lt; V, E, W &gt;, β, γ, η, θ Output: G� =&lt; V�, E, W� &gt; begin Initialize Q +— ∅ for v E V do Find dupset ωv with ISimβ,γ,η if lu E V,ωu E Q and ωv n ωu =�∅ then ωv +— ωv U ωu Remove ωu from Q Add ωv to Q for ω E Q do Merge nodes in ω into new node v� Update G with replacing nodes in ω with v� 3.3 Label propagation We utilize knowledge graph induced intent features instead of manually labels as constraints to conduct label propagation(Zhu and Ghahramani, 2002). The idea is that node labels are propagated to nearby nodes via weighted edges until convergence, as highly weighted edges indicate high probability of sharing labels. Nodes in our work have soft labels, where each dimension of intent features denotes a label, such as a topic or domain of knowledge graph. As described in aforesaid observations, it is intuitively reasonable to propagate on the basis of explicit edges and implicit intent similarities. We illustrate the propagation with topic feature, that of domain feature is similar. We use matrix Yt E R|V |∗|T |to denote the intent topic gra</context>
<context position="16555" citStr="Zhu and Ghahramani, 2002" startWordPosition="2827" endWordPosition="2830">number of nodes N. 1073 The transition matrix T indicates the impact of nodes on each other. Note that here the wij can be replaced by other similarity measures such as ISim in Section 3.2. Tij =� ij (11) E, k=1 wkj Let D denote an N ×N diagonal matrix with dii = Ej Tij. Then we can get a normalized version of transition matrix P = D−1T. The normalized transition matrix can be split into 4 sub-matrices. [Pll Plu] P = (12) Pul Puu At each step, we propagate and clamp the labelled data and repeat until Y converges, the propagation step can be written as: ˆYu = PuuYu + PulYl (13) As is shown in (Zhu and Ghahramani, 2002; Zhu et al., 2003) the solution to the propagation converges to: ˆYu = (I − Puu)−1PulYl (14) 3.4 The propagation framework for intent features We carry the propagation in an iterative process illustrated in Algorithm 2. Algorithm 2: Intent feature propagation Input: G, Y t l ,Y d l Output: ˆG, Yˆtu, Yˆud Initialize Y t l Y d with results of Section2 repeat Merge similar nodes according to Algorithm 1 Compute matrix P repeat Yˆt u = PuuYu t + PulY t l until Convergence; Recompute Pˆ with Yˆt repeat ˆPuuYud + ˆPulY d Yˆd l u until Convergence; until no dups; Since intent features include both d</context>
</contexts>
<marker>Zhu, Ghahramani, 2002</marker>
<rawString>Xiaojin Zhu and Zoubin Ghahramani. 2002. Learning from labeled and unlabeled data with label propagation. Technical report, Technical Report CMUCALD-02-107, Carnegie Mellon University.</rawString>
</citation>
<citation valid="true">
<authors>
<author>Xiaojin Zhu</author>
<author>Zoubin Ghahramani</author>
<author>John Lafferty</author>
</authors>
<title>Semi-supervised learning using gaussian fields and harmonic functions.</title>
<date>2003</date>
<booktitle>In ICML,</booktitle>
<volume>3</volume>
<pages>912--919</pages>
<contexts>
<context position="16574" citStr="Zhu et al., 2003" startWordPosition="2831" endWordPosition="2834">e transition matrix T indicates the impact of nodes on each other. Note that here the wij can be replaced by other similarity measures such as ISim in Section 3.2. Tij =� ij (11) E, k=1 wkj Let D denote an N ×N diagonal matrix with dii = Ej Tij. Then we can get a normalized version of transition matrix P = D−1T. The normalized transition matrix can be split into 4 sub-matrices. [Pll Plu] P = (12) Pul Puu At each step, we propagate and clamp the labelled data and repeat until Y converges, the propagation step can be written as: ˆYu = PuuYu + PulYl (13) As is shown in (Zhu and Ghahramani, 2002; Zhu et al., 2003) the solution to the propagation converges to: ˆYu = (I − Puu)−1PulYl (14) 3.4 The propagation framework for intent features We carry the propagation in an iterative process illustrated in Algorithm 2. Algorithm 2: Intent feature propagation Input: G, Y t l ,Y d l Output: ˆG, Yˆtu, Yˆud Initialize Y t l Y d with results of Section2 repeat Merge similar nodes according to Algorithm 1 Compute matrix P repeat Yˆt u = PuuYu t + PulY t l until Convergence; Recompute Pˆ with Yˆt repeat ˆPuuYud + ˆPulY d Yˆd l u until Convergence; until no dups; Since intent features include both domain vector and to</context>
</contexts>
<marker>Zhu, Ghahramani, Lafferty, 2003</marker>
<rawString>Xiaojin Zhu, Zoubin Ghahramani, John Lafferty, et al. 2003. Semi-supervised learning using gaussian fields and harmonic functions. In ICML, volume 3, pages 912–919.</rawString>
</citation>
</citationList>
</algorithm>
</algorithms>
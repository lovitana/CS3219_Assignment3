<?xml version="1.0" encoding="UTF-8"?>
<algorithms version="110505">
<algorithm name="SectLabel" version="110505">
<variant no="0" confidence="0.000000">
<title confidence="0.998899">
An I-vector Based Approach to Compact Multi-Granularity Topic Spaces
Representation of Textual Documents
</title>
<author confidence="0.808774">
Mohamed Morchid†, Mohamed Bouallegue†, Richard Dufour†,
</author>
<affiliation confidence="0.7352495">
Georges Linar`es†, Driss Matrouf† and Renato de Mori†‡†LIA, University of Avignon, France
‡McGill University, School of Computer Science, Montreal, Quebec, Canada
</affiliation>
<email confidence="0.7519215">
{firstname.lastname}@univ-avignon.fr
rdemori@cs.mcgill.ca
</email>
<sectionHeader confidence="0.997251" genericHeader="abstract">
Abstract
</sectionHeader>
<bodyText confidence="0.99996548">
Various studies highlighted that topic-
based approaches give a powerful spo-
ken content representation of documents.
Nonetheless, these documents may con-
tain more than one main theme, and their
automatic transcription inevitably contains
errors. In this study, we propose an orig-
inal and promising framework based on a
compact representation of a textual docu-
ment, to solve issues related to topic space
granularity. Firstly, various topic spaces
are estimated with different numbers of
classes from a Latent Dirichlet Allocation.
Then, this multiple topic space representa-
tion is compacted into an elementary seg-
ment, called c-vector, originally developed
in the context of speaker recognition. Ex-
periments are conducted on the DECODA
corpus of conversations. Results show the
effectiveness of the proposed multi-view
compact representation paradigm. Our
identification system reaches an accuracy
of 85%, with a significant gain of 9 points
compared to the baseline (best single topic
space configuration).
</bodyText>
<sectionHeader confidence="0.999457" genericHeader="keywords">
1 Introduction
</sectionHeader>
<bodyText confidence="0.991753115384615">
Automatic Speech Recognition (ASR) systems
frequently fail on noisy conditions and high Word
Error Rates (WER) make the analysis of the au-
tomatic transcriptions difficult. Speech analyt-
ics suffer from these transcription issues that may
be overcome by improving the ASR robustness
and/or the tolerance of speech analytic systems to
ASR errors. This paper proposes a new method
to improve the robustness of speech analytics by
combining a semantic multi-model approach and
a noise reduction technique based on the i-vector
paradigm.
This method is evaluated in the application
framework of the RATP call centre (Paris Public
Transportation Authority), focusing on the theme
identification task (Bechet et al., 2012).
Telephone conversations are a particular case
of human-human interaction whose automatic
processing raises problems, especially due to the
speech recognition step required to obtain the
transcription of the speech contents. First, the
speaker’s behavior may be unexpected and the
training/test mismatch may be very large. Second,
the speech signal may be strongly impacted by
various sources of variability: environment and
channel noises, acquisition devices, etc.
Telephone conversation issues
Topics are related to the reason why the customer
called. Various classes corresponding to the
main customer’s requests are considered (lost and
founds, traffic state, timelines, etc). In addition
to classical issues in such adverse conditions,
the topic-identification system should deal with
problems related to class proximity. For example,
a lost &amp; found request is related to itinerary
(where was the object lost?) or timeline (when?),
that could appear in most of the classes. In fact,
these conversations involve a relatively small
set of basic concepts related to transportation
issues. Figure 1 shows an example of a dialogue
which is manually labeled by the agent as an
issue related to an infraction. However, words
in bold suggest that this conversation could be
related to a transportation card. Thus, we assume
that a dialogue representation should be seen as
a multi-view problem to substantiate the claims
regarding the multi-theme representation of a
given dialogue.
On the other hand, multi-view approaches in-
troduce additional variability due to the diversity
of the views. This variability is also due to the
vocabulary used by both agent and customer
</bodyText>
<page confidence="0.990682">
443
</page>
<note confidence="0.9741645">
Proceedings of the 2014 Conference on Empirical Methods in Natural Language Processing (EMNLP), pages 443–454,
October 25-29, 2014, Doha, Qatar. c�2014 Association for Computational Linguistics
</note>
<figureCaption confidence="0.86894475">
Figure 1: Example of a dialogue from the DE-
CODA corpus labeled by the agent as an infraction
issue which contains more than one theme (infrac-
tion + transportation cards).
</figureCaption>
<bodyText confidence="0.99976675">
during a telephone conversation. Indeed, an
agent have to follow an predefined scenario of
conversation. Thus, the agent can find the main
reason for the call which corresponds to the theme.
</bodyText>
<subsectionHeader confidence="0.87825">
Proposed solutions
</subsectionHeader>
<bodyText confidence="0.99998276923077">
An efficient way to tackle both ASR robustness
and class ambiguity could be to map dialogues
into a topic space abstracting the ASR outputs.
Then, dialogue categorization is achieved in this
topic space. Numerous unsupervised methods for
topic-space estimation were proposed in the past.
Latent Dirichlet Allocation (LDA) (Blei et al.,
2003) has been largely used for speech analytics;
one of its main drawbacks is the tuning of the
model, that involves various meta-parameters
such as the number of classes (that determines
the model granularity), word distribution meth-
ods, temporal spans... If the decision process is
highly dependent on these features, the system’s
performance could be quite unstable.
Classically, this abstract representation involves
selecting the right number of classes composing
the topic space. This decision is crucial since
topic model perplexity, which expresses its qual-
ity, is highly dependent on this feature. Further-
more, the multi-theme context of the study (see
Figure 1) involves a more complex dialogue rep-
resentation. In this paper, we propose to deal with
these two drawbacks by using a compact represen-
tation from multiple topic spaces. This model is
based on a robust multi-view representation of the
textual documents.
A multi-view representation of a dialogue intro-
duces both a relevant variability needed to repre-
sent different contexts of the dialogue, and a noisy
variability related to topic space processing. Thus,
a topic-based representation of a dialogue is built
from the dialogue content itself. For this reason,
the mapping process of a dialogue into several
topic spaces generates a noisy variability related to
the difference between the dialogue and the con-
tent of each class. In the same way, the relevant
variability comes from the common content be-
tween the dialogue and the classes composing the
topic space.
We propose to reduce the noisy variability by
using a factor analysis technique, which was ini-
tially developed in the domain of speaker identifi-
cation. In this field, the factor analysis paradigm
is used as a decomposition model that enables to
separate the representation space into two sub-
spaces containing respectively useful and useless
information. The general Joint Factor Analysis
(JFA) paradigm (Kenny et al., 2008) considers
multiple variabilities that may be cross-dependent.
Therefore, JFA representation allows us to com-
pensate the variability within sessions of a same
speaker. This representation is an extension of the
GMM-UBM (Gaussian Mixture Model-Universal
Background Model) models (Reynolds and Rose,
1995). (Dehak et al., 2011) extract a compact
super-vector (called an i-vector) from the GMM
super-vector. The aim of the compression pro-
cess (i-vector extraction) is to represent the super-
vector variability in a low dimensional space. Al-
though this compact representation is widely used
in speaker recognition systems, this method has
not been used yet in the field of text classification.
In this paper, we propose to apply factor anal-
ysis to compensate noisy variabilities due to the
multiplication of LDA models. Furthermore, a
normalization approach to condition dialogue rep-
resentations (multi-model and i-vector) is pre-
sented. The two methods showed improvements
for speaker verification: within Class Covariance
Normalization (WCCN) (Dehak et al., 2011) and
Eigen Factor Radial (EFR) (Bousquet et al., 2011).
The latter includes length normalization (Garcia-
Romero and Espy-Wilson, 2011). Both methods
dilate the total variability space as a means of re-
ducing the within-class variability. In our multi-
model representation, the within class variability
is redefined according to both dialogue content
</bodyText>
<figure confidence="0.97687347368421">
Agent: Hello
Customer: Hello
Agent: Speaking ...
Customer: I call you because
I was fined today, but I still
have an imagine card
suitable for zone 1 [...] I forgot
to use my navigo card for
zone 2
Agent: You did not use
your navigo card, that is
why they give you a fine not
for a zone issue [...]
Customer: Thanks, bye
Agent: bye
Agent
Customer
Transportation
cards
</figure>
<page confidence="0.998954">
444
</page>
<bodyText confidence="0.999805">
(vocabulary) and topic space characteristics (word
distributions among the topics). Thus, the speaker
is represented by a theme, and the speaker session
is a set of topic-based representations (frames) of
a dialogue (session).
The paper is organized as follows. Section 2
presents previous related works. The dialogue rep-
resentation is described in Section 3. Section 4 in-
troduces the i-vector compact representation and
presents its application to text documents. Sec-
tions 5 and 6 report experiments and results. The
last section concludes and proposes some perspec-
tives.
</bodyText>
<sectionHeader confidence="0.999893" genericHeader="introduction">
2 Related work
</sectionHeader>
<bodyText confidence="0.999956975000001">
In the past, several approaches considered a
text document as a mixture of latent topics.
These methods, such as Latent Semantic Analysis
(LSA) (Deerwester et al., 1990; Bellegarda, 1997),
Probabilistic LSA (PLSA) (Hofmann, 1999) or
Latent Dirichlet Allocation (LDA) (Blei et al.,
2003), build a higher-level representation of the
document in a topic space. ¿ Document is
then considered as a bag-of-words (Salton, 1989)
where the word order is not taken into account.
These methods have demonstrated their perfor-
mance on various tasks, such as sentence (Belle-
garda, 2000) or keyword (Suzuki et al., 1998) ex-
traction.
In opposition to a multinomial mixture model,
LDA considers that a theme is associated to each
occurrence of a word composing the document,
rather than associate a topic to the complete doc-
ument. Therefore, a document can change topics
from a word to another one. However, word oc-
currences are connected by a latent variable which
controls the global match of the distribution of
the topics in the document. These latent topics
are characterized by a distribution of associated
word probabilities. PLSA and LDA models have
been shown to generally outperform LSA on IR
tasks (Hofmann, 2001). Moreover, LDA provides
a direct estimate of the relevance of a topic given
a word set. In this paper, probabilities of hidden
topic features, estimated with LDA, are considered
for possibly capturing word dependencies express-
ing the semantic contents of a given conversation.
Topic-based approaches involve defining a
number of topics composing the topic space. The
choice of the “right” number of topics is a crucial
step, especially when the documents may contain
multiple themes. Many studies have tried to find
a relevant method to deal with this issue. (Arun et
al., 2010) proposed to use a Singular Value De-
composition (SVD) to represent the separability
between the words contained in the vocabulary.
Then, if the singular values of the topic-word ma-
trix M equal the norm of the rows of M, this means
that the vocabulary is well separated among the
topics. This method has to be evaluated with the
Kullback-Liebler divergence metric for each topic
space. However, this process would be time con-
suming for thousands of representations of a dia-
logue.
(Teh et al., 2004) proposed the Hierarchical
Dirichlet Process (HDP) method to find the “right”
number of topics by assuming that the data has
a hierarchical structure. The HDP models were
then compared to the LDA ones on the same
dataset. (Zavitsanos et al., 2008) presented a
method to learn the right depth of an ontology de-
pending of the number of topics of LDA models.
The study presented by (Cao et al., 2009) is quite
similar to (Teh et al., 2004). The authors consider
the average correlation between pairs of topics at
each stage as the right number of topics.
All these methods assume that a document can
have only one representation since they consider
that finding the optimal topic model is the best so-
lution. Another solution would be to consider a set
of topic models to represent a document. Nonethe-
less, a multi-topic-based representation of a dia-
logue can involve a noisy variability due to the
mapping of a dialogue in each topic space. Indeed,
a dialogue does not share its content (i.e. words)
with each class composing the topic space. Thus,
a variability is added during the mapping pro-
cess. Another weakness of the multi-view repre-
sentation is the relation between classes in a topic
space. (Blei and Lafferty, 2006) show that classes
into a LDA topic space are correlated. More-
over, (Li and McCallum, 2006) consider a class
as a node of an acyclic graph and as a distribu-
tion over other classes contained in the same topic
space.
</bodyText>
<sectionHeader confidence="0.704712333333333" genericHeader="method">
3 Multi-view representation of automatic
dialogue transcriptions in a
homogeneous space
</sectionHeader>
<bodyText confidence="0.999903666666667">
The purpose of the considered application is the
identification of the major theme of a human-
human telephone conversation in the customer
</bodyText>
<page confidence="0.995862">
445
</page>
<bodyText confidence="0.997793786885246">
care service (CCS) of the RATP Paris transporta-
tion system. The approach considered in this pa-
per focuses on modeling the variability between
different dialogues expressing the same theme t.
For this purpose, it is important to select relevant
features that represent semantic contents for the
theme of a dialogue. An attractive set of features
for capturing possible semantically relevant word
dependencies is obtained with Latent Dirichlet Al-
location (LDA) (Blei et al., 2003), as described in
section 2.
Given a training set of conversations D, a hid-
den topic space is derived and a conversation d
is represented by its probability in each topic of
the hidden space. Estimation of these probabili-
ties is affected by a variability inherent to the es-
timation of the model parameters. If many hidden
spaces are considered and features are computed
for each hidden space, it is possible to model the
estimation variability together with the variability
of the linguistic expression of a theme by different
speakers in different real-life situations. Even if
the purpose of the application is theme identifica-
tion and a training corpus annotated with themes is
available, supervised LDA (Griffiths and Steyvers,
2004) is not suitable for the proposed approach.
LDA is used only for producing different feature
sets used involved in statistical variability models.
In order to estimate the parameters of differ-
ent hidden spaces, a set of discriminative words
V is constructed as described in (Morchid et al.,
2014a). Each theme t contains a set of specific
words. Note that the same word may appear in
several discriminative word sets. All the selected
words are then merged without repetition to form
V .
Several techniques, such as Variational Meth-
ods (Blei et al., 2003), Expectation-propagation
(Minka and Lafferty, 2002) or Gibbs Sam-
pling (Griffiths and Steyvers, 2004), have been
proposed for estimating the parameters describ-
ing a LDA hidden space. Gibbs Sampling is
a special case of Markov-chain Monte Carlo
(MCMC) (Geman and Geman, 1984) and gives
a simple algorithm for approximate inference in
high-dimensional models such as LDA (Heinrich,
2005). This overcomes the difficulty to directly
and exactly estimate parameters that maximize the
likelihood of the whole data collection defined as:
p(W |−→ α , →−β ) = 11w∈W p(−→w |−→ α , →−β ) for the whole
data collection W knowing the Dirichlet parame-
ters →−α and →− β .
Gibbs Sampling allows us both to estimate the
LDA parameters in order to represent a new dia-
logue d with the rth topic space of size n, and to
obtain a feature vector V zr
d of the topic representa-
tion of d. The jth feature V zj = P(zjr |d) (where
1 G j G n) is the probability of topic zrj to be
generated by the unseen dialogue d in the rth topic
space of size n (see Figure 2) and V w
</bodyText>
<figure confidence="0.952380785714286">
Conversations agen/customer
= P (w|zr
zr j )
j
is the vector representation of a word into r.
customer care service of t
Agent: Hello
Customer: Hello
Paris
Agent: Speaking ...
Customer: I call you because I
Agent: Heo
was fined today, but I still have an
Customer: Helo
Aget Speaking .imagine card suitable for zone 1
Customer: I call you because I[...] I forgot to use my navigo card
was fined oday, but I sill have anfor zone 2
imagine c
Agent: You did not use your
[...] I forgot to use my navgo ca
navigo card, that is why they give
fo zon
Agent: You did not use youryou a fine not for a zone issue [...]
navigo card that s why they give
Customer: Thanks, bye
you a fine not for a zone is
Agent: bye
Customer:
</figure>
<figureCaption confidence="0.9987615">
Figure 2: Example of a dialogue d mapped into a
topic space of size n.
</figureCaption>
<bodyText confidence="0.999885055555556">
In the LDA technique, topic zj, j is drawn
from a multinomial over θ which is drawn from
a Dirichlet distribution over →−α . Thus, a set of
p topic spaces are learned using LDA by varying
the number of topics n to obtain p topic spaces of
size n. The number of topics n varies from 10 to
3, 010. Thus, a set of 3, 000 topic spaces is esti-
mated. This is high enough to generate, for each
dialogue, many feature sets for estimating the pa-
rameters of a variability model.
The next process allows us to obtain a homo-
geneous representation of transcription d for the
rth topic space r. The feature vector V zm
d of
d is mapped to the common vocabulary space
V composed with a set of |V  |discriminative
words (Morchid et al., 2014a) of size 166, to ob-
tain a new feature vector Vd,rw = {P(w|d)r}w∈V
</bodyText>
<figure confidence="0.999557825">
P(z |d) ... P(z |d)
1 n
I
TOPIC 1
0.03682338236708009 card
P(wz w
P(w|z) w
3682338236708009 car
0.026680126910873955 month
2681269187395 mon
0.026007114700509565 navigo
026007114700509565 n
0.01615229304874531 old
0.015527353139121238 agency
01615229304874531 old
0.014229401019132776 euros
0155275339121238 en
0.013123738102105566 imagine
...
P(w|z) w
0.06946564885496183 card
P(w|z)
0.040458152717557 fine
0.0694656488549
0.016793893129770993 transport
004045801526717557
..
0.01603053435114504 woman
00167938931297709
0.01450381679389313 fined
0.013740458015267175 aïe
0.01603053435114
0.01297709923664221 infraction
001450381679389313
TOPI
C n
TO
ervine
Agent
Customer
</figure>
<page confidence="0.991879">
446
</page>
<bodyText confidence="0.9469495">
of size |V  |for the rth topic space r of size n where
the ith (0 ≤ i ≤ |V |) feature is:
</bodyText>
<equation confidence="0.981046625">
V wi
d,r = P(wi|d)
P(wi|zr j )P(zr j |d)
zr
Vwi × Vd
j
�−−→ −−→ �
=V wi
</equation>
<bodyText confidence="0.8280135">
zr , V zr
d
where h·, ·i is the inner product, S being the fre-
quency of the term wi in d, V wi zr j= P(wi|zj) and
Vzr
d = P(zj|d) evaluated using Gibbs Sampling
j
in the topic space r.
</bodyText>
<sectionHeader confidence="0.986348" genericHeader="method">
4 Compact multi-view representation
</sectionHeader>
<bodyText confidence="0.999930590909091">
In this section, an i-vector-based method to
represent automatic transcriptions is presented.
Initially introduced for speaker recognition, i-
vectors (Kenny et al., 2008) have become very
popular in the field of speech processing and re-
cent publications show that they are also reli-
able for language recognition (Martınez et al.,
2011) and speaker diarization (Franco-Pedroso et
al., 2010). I-vectors are an elegant way of re-
ducing the imput space dimensionality while re-
taining most of the relevant information. The
technique was originally inspired by the Joint
Factor Analysis framework (Kenny et al., 2007).
Hence, i-vectors convey the speaker characteris-
tics among other information such as transmission
channel, acoustic environment or phonetic content
of speech segments. The next sections describe
the i-vector extraction process, the application of
this compact representation to textual documents
(called c-vector), and the vector transformation
with the EFR method and the Mahalanobis met-
ric.
</bodyText>
<subsectionHeader confidence="0.984635">
4.1 Total variability space definition
</subsectionHeader>
<bodyText confidence="0.998860714285714">
I-vector extraction could be seen as a probabilistic
compression process that reduces the dimension-
ality of speech super-vectors according to a linear-
Gaussian model. The speech (of a given speech
recording) super-vector ms of concatenated GMM
means is projected in a low dimensionality space,
named Total Variability space, with:
</bodyText>
<equation confidence="0.999386">
m(h,s) = m + Tx(h,s) , (1)
</equation>
<bodyText confidence="0.999973444444444">
where m is the mean super-vector of the UBM1.
T is a low rank matrix (MD × R), where M is
the number of Gaussians in the UBM and D is the
cepstral feature size, which represents a basis of
the reduced total variability space. T is named To-
tal Variability matrix; the components of x(h,s) are
the total factors which represent the coordinates of
the speech recording in the reduced total variabil-
ity space called i-vector (i for identification).
</bodyText>
<subsectionHeader confidence="0.615743">
4.2 From i-vector speaker identification to
</subsectionHeader>
<bodyText confidence="0.980169181818182">
c-vector textual document classification
The proposed approach uses i-vectors to model
transcription representation through each topic
space in a homogeneous vocabulary space. These
short segments are considered as basic semantic-
based representation units. Indeed, vector Vdw rep-
resents a segment or a session of a transcription d.
In the following, (d, r) will indicate the dialogue
representation d in the topic space r. In our model,
the segment super-vector m(d,r) of a transcription
d knowing a topic space r is modeled:
</bodyText>
<equation confidence="0.99972">
m(d,r) = m + Tx(d,r) (2)
</equation>
<bodyText confidence="0.999409625">
where x(d,r) contains the coordinates of the topic-
based representation of the dialogue in the re-
duced total variability space called c-vector (c for
classification).
Let N(d,r) and X(d,r) be two vectors containing
the zero order and first order dialogue statistics re-
spectively. The statistics are estimated against the
UBM:
</bodyText>
<equation confidence="0.990323">
Nr[g] = � -yg(t); {X(d,r)}[g] = � -yg(t) · t
</equation>
<bodyText confidence="0.786947">
t∈r t∈(d,r)
where -yg(t) is the a posteriori probability of Gaus-
sian g for the observation t. In the equation,
�
t∈(d,r) represents the sum over all the frames be-
longing to the dialogue d.
Let X(d,r) be the state dependent statistics de-
fined as follows:
{X(d,r)}[g] ={X(d,r)}[g] − m[g] · � N(d,r)[g]
(d,r)
Let L(d,r) be a R × R matrix, and B(d,r) a vector
</bodyText>
<footnote confidence="0.9005435">
1The UBM is a GMM that represents all the possible ob-
servations.
</footnote>
<table confidence="0.9039882">
n
=
j=1
n
=
j=1
447
Algorithm 1: Estimation algorithm of T and
latent variable x.
For each dialogue d mapped into the topic
space r: x(d,r) ← 0, T ← random;
Estimate statistics: N(d,r), X(d,r) (eq.3);
for i = 1 to nb iterations do
for all d and r do
Center statistics: X(d,r) (eq.4);
Estimate L(d,r) and B(d,r) (eq.5);
Estimate x(d,r) (eq.6);
end
Estimate matrix T (eq. 7 and 8) ;
end
</table>
<bodyText confidence="0.669581">
of dimension R, both defined as:
</bodyText>
<equation confidence="0.9936812">
L(d,r) = I + � N(d,r)[9] · {T}t[g]· Σ−1[g]· {T}[g]
g∈UBM
�B(d,r) = {T}t[g] · Σ−1
g∈UBM g · {X(d,r)}[g],
(5)
</equation>
<bodyText confidence="0.96852">
By using L(d,r) and B(d,r), x(d,r) can be obtained
using the following equation:
</bodyText>
<equation confidence="0.872248428571428">
x(d,r) = L−1
(d,r) · B(d,r) (6)
The matrix T can be estimated line by line, with
{T}i[ being the ith line of {T}[g] then:
g]
Ti[g] = LU−1
g · RUig, (7)
</equation>
<bodyText confidence="0.99352">
where RUig and LUg are given by:
</bodyText>
<equation confidence="0.99379425">
L−1
(d,r) + x(d,r)xt(d,r) · N(d,r)[9]
{X(d,r)}[i]
[g] · x(d,r)
</equation>
<bodyText confidence="0.999875733333333">
Algorithm 1 presents the method adopted to es-
timate the multi-view variability dialogue matrix
with the above developments where the standard
likelihood function can be used to assess the con-
vergence. One can refer to (Matrouf et al., 2007)
to find out more about the implementation of the
factor analysis.
C-vector representation suffers from 3 raised c-
vector issues: (i) the c-vectors x of equation 2
have to be theoretically distributed among the nor-
mal distribution N (0, I), (ii) the “radial” effect
should be removed, and (iii) the full rank total
factor space should be used to apply discriminant
transformations. The next section presents a solu-
tion to these 3 problems.
</bodyText>
<subsectionHeader confidence="0.987265">
4.3 C-vector standardization
</subsectionHeader>
<bodyText confidence="0.997142">
A solution to standardize c-vectors has been de-
veloped in (Bousquet et al., 2011). The authors
proposed to apply transformations for training and
test transcription representations. The first step is
to evaluate the empirical mean x and covariance
matrix V of the training c-vector. Covariance ma-
trix V is decomposed by diagonalization into:
</bodyText>
<equation confidence="0.958906">
PDPT (9)
</equation>
<bodyText confidence="0.988264333333333">
where P is the eigenvector matrix of V and D is the
diagonal version of V. A training i-vector x(d,r) is
transformed in x0(d,r) as follows:
</bodyText>
<equation confidence="0.999640666666667">
x (d,r) = O
(10)
x(d,r) − x)TV−1(x(d,r) − x)
The numerator is equivalent by rotation to
1
V−2 (x(d,r) − x) and the Euclidean norm of x0(d,r)
</equation>
<bodyText confidence="0.999438777777778">
is equal to 1. The same transformation is applied
to the test c-vectors, using the training set parame-
ters x and mean covariance Vas estimations of the
test set of parameters.
Figure 3 shows the transformation steps: Fig-
ure 3-(a) is the original training set; Figure 3-
(b) shows the rotation applied to the initial train-
ing set around the principal axes of the total vari-
ability when PT is applied; Figure 3-(c) shows
</bodyText>
<equation confidence="0.60092">
1
</equation>
<bodyText confidence="0.984966666666667">
the standardization of c-vectors when D−2 is
applied; and finally, Figure 3-(d) shows the c-
vector x0 (d,r) on the surface area of the unit hyper- sphereafter a length normalization by a division
</bodyText>
<equation confidence="0.580991">
�of (x(d,r)− x)TV−1(x(d,r) − x).
</equation>
<sectionHeader confidence="0.999107" genericHeader="method">
5 Experimental Protocol
</sectionHeader>
<bodyText confidence="0.999904714285714">
The proposed c-vector representation of automatic
transcriptions is evaluated in the context of the
theme identification of a human-human telephone
conversation in the customer care service (CCS)
of the RATP Paris transportation system. The met-
ric used to identify of the best theme is the Maha-
lanobis metric.
</bodyText>
<subsectionHeader confidence="0.958366">
5.1 Theme identification task
</subsectionHeader>
<bodyText confidence="0.9999376">
The DECODA project corpus (Bechet et al., 2012)
was designed to perform experiments on the iden-
tification of conversation themes. It is composed
of 1,514 telephone conversations, corresponding
to about 74 hours of signal, split into a training
</bodyText>
<equation confidence="0.864476">
�LUg =
(d,r)
RUig �=
(d,r)
(8)
0 1D−2 PT (x(d r) − x)
</equation>
<page confidence="0.524449">
448
</page>
<figure confidence="0.961221056818182">
−2 −1 0 1 2
−2 −1 0 1 2
a.Inital: M
b.Rotation: Pt
● ●
● ●
●
● ●
● ● ●
●
● ●
●
●
●
●●
●●
●
●
●
●
●
●
●●
● ●
●●
● ●
●
●
● ● ●
●
●
●
●
● ● ●
● ●
● ● ● ● ● ●
●●
● ●
● ●●
●
●
●
●
●●
●
●
● ● ●
●
●
●
● ●
● ● ●
−3 −2 −1 0 1 2 3 −3 −2 −1 0 1 2
c.Standardization: D−1 2
●
●
●●
● ●●
● ● ●
●● ●●
●● ● ● ● ● ● ●
● ● ● ●
● ●
●
●
●●
●
●
●
●
●
●
●
●
●
●
d.Norm: (x − x)tV−1(x − x)
●
●
●
●
● ●
● ● ● ●
●
●
●
●
● ●
</figure>
<equation confidence="0.925061">
● ● ● ●
−2 −1 0 1 2
−2 −1 0 1 2
−3 −2 −1 0 1 2 3 −3 −2 −1 0 1 2 3
</equation>
<figureCaption confidence="0.998527">
Figure 3: Effect of the standardization with the EFR algorithm.
</figureCaption>
<bodyText confidence="0.999914851851852">
set (740 dialogues), a development set (175 dia-
logues) and a test set (327 dialogues), and manu-
ally annotated with 8 conversation themes: prob-
lems of itinerary, lost and found, time schedules,
transportation cards, state of the traffic, fares, in-
fractions and special offers.
An LDA model allowed us to elaborate 3,000
topics spaces by varying the number of topics from
10 to 3,010. A topic space having less than 10
topics is not suitable for a corpus of more than 700
dialogues (training set). For each theme {CZ}8Z=1,
a set of 50 specific words is identified. All the
selected words are then merged without repetition
to compose V , which is made of 166 words. The
topic spaces are made with the LDA Mallet Java
implementation2.
The LIA-Speeral ASR system (Linar`es et al.,
2007) is used for the experiments. Acoustic model
parameters were estimated from 150 hours of
speech in telephone conditions. The vocabulary
contains 5,782 words. A 3-gram language model
(LM) was obtained by adapting a basic LM with
the training set transcriptions. A “stop list” of 126
words3 was used to remove unnecessary words
(mainly function words), which results in a Word
Error Rate (WER) of 33.8% on the training, 45.2%
on the development, and 49.5% on the test. These
</bodyText>
<footnote confidence="0.99797">
2http://mallet.cs.umass.edu/
3http://code.google.com/p/stop-words/
</footnote>
<bodyText confidence="0.99893">
high WER are mainly due to speech disfluencies
and to adverse acoustic environments (for exam-
ple, calls from noisy streets with mobile phones).
</bodyText>
<subsectionHeader confidence="0.999395">
5.2 Mahalanobis metric
</subsectionHeader>
<bodyText confidence="0.9993634375">
Given a new observation x, the goal of the task is
to identify the theme belonging to x. Probabilistic
approaches ignore the process by which c-vectors
were extracted and they pretend instead they were
generated by a prescribed generative model. Once
a c-vector is obtained from a dialogue, its repre-
sentation mechanism is ignored and it is regarded
as an observation from a probabilistic generative
model. The Mahalanobis scoring metric assigns a
dialogue d with the most likely theme C. Given
a training dataset of dialogues, let W denote the
within dialogue covariance matrix defined by:
where Wk is the covariance matrix of the kth
theme Ck, nt is the number of utterances for the
theme Ck, n is the total number of dialogues, and
xk is the centroid (mean) of all dialogues xkZ of Ck.
</bodyText>
<equation confidence="0.974660454545454">
W = K nt Wk
k=1 n
~ ~ ~ ~t
xk Z − xk xk Z − xk (11)
1
n
=
nt
Z=0
K
k=1
</equation>
<page confidence="0.996382">
449
</page>
<bodyText confidence="0.99982575">
Each dialogue does not contribute to the co-
variance in an equivalent way. For this reason,
is introduced in equation 11. If ho-
moscedasticity (equality of the class covariances)
and Gaussian conditional density models are as-
sumed, a new observation x from the test dataset
can be assigned to the most likely theme CkBayes us-
ing the classifier based on the Bayes decision rule:
</bodyText>
<equation confidence="0.759829666666667">
CkBayes = arg mkax {N (x  |xk, W)}
= arg max�− (x − xk)t W−1 (x − xk) + akJ
k
</equation>
<bodyText confidence="0.9011779">
where W is the within theme covariance ma-
trix defined in eq. 11; N denotes the normal dis-
tribution and ak = log (P(Ck)). It is noted that,
with these assumptions, the Bayesian approach is
similar to Fisher’s geometric approach: x is as-
signed to the class of the nearest centroid, accord-
ing to the Mahalanobis metric (Xing et al., 2002)
of W−1:
� CkBayes = arg max−1 ||x − xk ||2 − 1 + ak J
k
</bodyText>
<sectionHeader confidence="0.984169" genericHeader="evaluation">
6 Experiments and results
</sectionHeader>
<bodyText confidence="0.998963340425532">
The proposed c-vector approach is applied to
the same classification task and corpus proposed
in (Morchid et al., 2014a; Morchid et al., 2014b;
Morchid et al., 2013) (state-of-the-art in text clas-
sification in (Morchid et al., 2014a)). Experiments
are conducted using the multiple topic spaces esti-
mated with an LDA approach. From these mul-
tiple topic spaces, a classical way is to find the
one that reaches the best performance. Figure 4
presents the theme classification performance ob-
tained on the development and test sets using vari-
ous topic-based representation configurations with
the EFR normalization algorithm (baseline).
For sake of comparison, experiments are con-
ducted using the automatic transcriptions only
(ASR) only. The conditions indicated by the ab-
breviations between parentheses are considered
for the development (Dev) and the test (Test) sets.
Only homogenous conditions (ASR for both
training and validations sets) are considered in this
study. Authors in (Morchid et al., 2014a) notice
that results collapse dramatically when heteroge-
nous conditions are employed (TRS or TRS+ASR
for training set and ASR for validation set).
First of all, we can see that this baseline ap-
proach reached a classification accuracy of 83%
and 76%, respectively on the development and the
test sets. However, we note that the classifica-
tion performance is rather unstable, and may com-
pletely change from a topic space configuration to
another. The gap between the lower and the higher
classification results is also important, with a dif-
ference of 25 points on the development set (the
same trend is observed on the test set). As a result,
finding the best topic space size seems crucial for
this classification task, particularly in the context
of highly imperfect automatic dialogue transcrip-
tions containing more than one theme.
The topic space that yields the best accuracy
with the baseline method (n = 15 topics) is pre-
sented in Figure 5. This figure presents each of the
15 topics and their 10 most representative words
(highest P(w|z)). Several topics contain more or
less the same representative words, such as topics
3, 6 and 9. This figure points out some interesting
topics that allow us to distinguish a theme from the
others. For example:
</bodyText>
<listItem confidence="0.9860266875">
• topics 2, 10 and 15 represent some words re-
lated to itinerary problems,
• the transportation cards theme is mostly rep-
resented in topic 4 and 15 (Imagine and Nav-
igo are names of transportation cards),
• the words which represent the time schedules
theme are contained in topic 5,6,7 and less in
topic 9,
• state of the traffic could be discussed with
words such as: departure, line, service, day.
These words and others are contained in topic
13,
• topics 4 and 12 are related to the infractions
theme with to words fine, pass, zone or ticket,
• but topic 12 could be related to theme fares
or special offers as well .
</listItem>
<bodyText confidence="0.8884886">
Table 1 presents results obtained with the pro-
posed c-vector approach coupled with the EFR al-
gorithm. We can firstly note that this compact rep-
resentation allows it to outperform the best topic
space configuration (baseline), with a gain of 9.4
points on the development data and of 9 points on
the test data. Moreover, if we consider the differ-
ent c-vector configurations with the development
the term nt
n
</bodyText>
<page confidence="0.78888">
450
</page>
<figure confidence="0.994097">
15 500 1500 2000 2497 3010
Items by varying the granularity 10 &lt; n &lt; 3010
15 500 825 1500 2000 3010
Items by varying the granularity 10 &lt; n &lt; 3010
</figure>
<figureCaption confidence="0.999978">
Figure 4: Theme classification accuracies using various topic-based representations with EFR normal-
</figureCaption>
<bodyText confidence="0.959085">
ization (baseline) on the development and test sets (X-coordinates start at 10 indeed, but to show the best
configuration point (15), the origine (10) has been removed).
</bodyText>
<tableCaption confidence="0.999203">
Table 1: Theme classification accuracy (%) with different c-vectors and GMM-UBM sizes.
</tableCaption>
<table confidence="0.999788571428571">
DEV TEST
c-vector Number of Gaussians in GMM-UBM
size
32 64 128 256 32 64 128 256
60 88.8 86.5 91.2 90.6 85.0 82.6 83.5 84.7
100 91.2 92.4 92.4 87.7 86.0 85.0 83.5 84.7
120 89.5 92.2 89.5 87.7 85.0 83.5 85.4 84.1
</table>
<tableCaption confidence="0.95398">
Table 2: Maximum (Max), minimum (Min) and Difference (Max − Min) theme classification accu-
racies (%) using the baseline and the proposed c-vector approaches.
</tableCaption>
<table confidence="0.97391575">
Max Min Difference
Method DEV TEST DEV TEST DEV TEST
baseline 83.3 76.0 58.6 56.8 14.7 20.8
c-vector 92.4 85.0 86.5 82.6 5.9 2.4
</table>
<figure confidence="0.982703176470588">
Max = 83.3
(a) Accuracy with the development set
Min = 58.6
Accuracy (%)
90
80
60
70
50
Max = 76.0
Min = 56.8
(b) Accuracy with the test set
Accuracy (%) 90
80
70
60
50
</figure>
<bodyText confidence="0.9924791">
and test sets, the gap between accuracies is much
smaller: classification accuracy does not go be-
low 82.6%, while it reached 56% for the worst
topic-based configuration. Indeed, as shown in Ta-
ble 2, the difference between the maximum and
the minimum theme classification accuracies is of
20% using the baseline approach while it is only
of 2.4% using the c-vector method.
We can conclude that this original c-vector ap-
proach allows one to better handle the variabilities
</bodyText>
<page confidence="0.995053">
451
</page>
<table confidence="0.812545055555556">
TOPIC 1 TOPIC 2 TOPIC 3 TOPIC 4 TOPIC 5
W P(W|Z) W P(W|Z) W P(W|Z) W P(W|Z) W P(W|Z)
line 0.028 bus 0.038 bus 0.024 card 0.040 line 0.029
bag 0.027 direction 0.027 hours 0.023 pass 0.032 know 0.027
metro 0.018 road 0.022 twenty 0.019 navigo 0.024 station 0.024
lost 0.017 stop 0.021 four 0.014 month 0.022 traffic 0.021
hours 0.015 sixty 0.018 minutes 0.014 euro 0.021 hour 0.017
name 0.013 five 0.017 onto 0.013 go 0.018 say 0.015
found 0.012 three 0.016 old 0.010 agency 0.016 level 0.014
thing 0.012 go 0.013 know 0.010 mail 0.012 time 0.014
object 0.011 hour 0.012 sunday 0.010 fine 0.010 today 0.012
instant 0.009 station 0.010 line 0.008 address 0.009 instant 0.011
TOPIC 6 TOPIC 7 TOPIC 8 TOPIC 9 TOPIC 10
W P(W|Z) W P(W|Z) W P(W|Z) W P(W|Z) W P(W|Z)
bus 0.030 ticket 0.026 saint 0.018 hour 0.041 station 0.041
hour 0.021 say 0.017 plus 0.017 four 0.039 saint 0.036
hundred 0.020 old 0.016 say 0.013 ten 0.037 direction 0.024
line 0.019 bus 0.015 road 0.013 bus 0.036 orly 0.020
ten 0.018 issue 0.015 level 0.012 hundred 0.024 take 0.015
old 0.016 never 0.014 station 0.011 miss 0.024 madame 0.015
mister 0.015 always 0.014 train 0.011 zero 0.022 metro 0.013
morning 0.015 time 0.013 city 0.010 line 0.020 line 0.012
lost 0.014 validate 0.012 four 0.010 five 0.018 north 0.012
bag 0.012 normal 0.011 far 0.009 six 0.017 bus 0.011
TOPIC 11 TOPIC 12 TOPIC 13 TOPIC 14 TOPIC 15
W P(W|Z) W P(W|Z) W P(W|Z) W P(W|Z) W P(W|Z)
madame 0.028 paris 0.025 service 0.034 bus 0.040 number 0.040
service 0.027 euro 0.025 old 0.027 direction 0.023 integral 0.030
address 0.022 zone 0.017 line 0.018 metro 0.022 card 0.024
mail 0.021 ticket 0.015 madame 0.017 line 0.017 agency 0.023
metro 0.020 fare 0.014 mister 0.014 stop 0.016 imagine 0.018
paris 0.019 card 0.014 ask 0.014 madame 0.015 subscription 0.018
old 0.018 buy 0.013 internet 0.013 saint 0.015 navigo 0.017
stop 0.018 station 0.013 departure 0.013 old 0.014 old 0.014
lac 0.016 noisy 0.010 day 0.012 road 0.014 eleven 0.013
dock 0.015 week 0.010 client 0.011 door 0.014 call 0.012
</table>
<figureCaption confidence="0.988325">
Figure 5: Topic space (15 topics) that obtains the best accuracy with the baseline system (see Fig. 4).
</figureCaption>
<bodyText confidence="0.99972225">
contained in dialogue conversations: in a classi-
fication context, better accuracy can be obtained
and the results can be more consistent when vary-
ing the c-vector size and the number of Gaussians.
</bodyText>
<sectionHeader confidence="0.999459" genericHeader="conclusions">
7 Conclusions
</sectionHeader>
<bodyText confidence="0.999973648648649">
This paper presents an original multi-view repre-
sentation of automatic speech dialogue transcrip-
tions, and a fusion process with the use of a factor
analysis method called i-vector. The first step of
the proposed method is to represent a dialogue in
multiple topic spaces of different sizes (i.e. num-
ber of topics). Then, a compact representation
of the dialogue from the multiple views is pro-
cessed to compensate the vocabulary and the vari-
ability of the topic-based representations. The ef-
fectiveness of the proposed approach is evaluated
in a classification task of theme dialogue identifi-
cation. Thus, the architecture of the system iden-
tifies conversation themes using the i-vector ap-
proach. This compact representation was initially
developed for speaker recognition and we showed
that it can be successfully applied to a text clas-
sification task. Indeed, this solution allowed the
system to obtain better classification accuracy than
with the use of the classical best topic space con-
figuration. In fact, we highlighted that this original
compact version of all topic-based representations
of dialogues, called c-vector in this work, coupled
with the EFR normalization algorithm, is a better
solution to deal with dialogue variabilities (high
word error rates, bad acoustic conditions, unusual
word vocabulary, etc). This promising compact
representation allows us to effectively solve both
the difficult choice of the right number of topics
and the multi-theme representation issue of partic-
ular textual documents. Finally, the classification
accuracy reached 85% with a gain of 9 points com-
pared to usual baseline (best topic space configu-
ration). In a future work, we plan to evaluate this
new representation of textual documents in other
information retrieval tasks, such as keyword ex-
traction or automatic summarization systems.
</bodyText>
<sectionHeader confidence="0.997751" genericHeader="acknowledgments">
Acknowledgements
</sectionHeader>
<bodyText confidence="0.9998095">
We thank the anonymous reviewers for their help-
ful comments. This work was funded by the
SUMACC and ContNomina projects supported by
the French National Research Agency (ANR) un-
der contracts ANR-10-CORD-007 and ANR-12-
BS02-0009.
</bodyText>
<sectionHeader confidence="0.998457" genericHeader="references">
References
</sectionHeader>
<reference confidence="0.999942866666667">
R. Arun, Venkatasubramaniyan Suresh, C.E.
Veni Madhavan, and Musti Narasimha Murty.
2010. On finding the natural number of topics
with latent dirichlet allocation: Some observations.
In Advances in Knowledge Discovery and Data
Mining, pages 391–402. Springer.
Frederic Bechet, Benjamin Maza, Nicolas Bigouroux,
Thierry Bazillon, Marc El-Beze, Renato De Mori,
and Eric Arbillot. 2012. Decoda: a call-
centre human-human spoken conversation corpus.
LREC’12.
Jerome R. Bellegarda. 1997. A latent semantic analy-
sis framework for large-span language modeling. In
Fifth European Conference on Speech Communica-
tion and Technology.
Jerome R. Bellegarda. 2000. Exploiting latent se-
mantic information in statistical language modeling.
Proceedings of the IEEE, 88(8):1279–1296.
David M. Blei and John Lafferty. 2006. Correlated
topic models. Advances in neural information pro-
cessing systems, 18:147.
David M. Blei, Andrew Y. Ng, and Michael I. Jordan.
2003. Latent dirichlet allocation. The Journal of
Machine Learning Research, 3:993–1022.
Pierre-Michel Bousquet, Driss Matrouf, and Jean-
Franc¸ois Bonastre. 2011. Intersession compensa-
tion and scoring methods in the i-vectors space for
speaker recognition. In Interspeech, pages 485–488.
Juan Cao, Tian Xia, Jintao Li, Yongdong Zhang, and
Sheng Tang. 2009. A density-based method for
adaptive lda model selection. Neurocomputing,
72(7):1775–1781.
Scott Deerwester, Susan T. Dumais, George W. Fur-
nas, Thomas K. Landauer, and Richard Harshman.
1990. Indexing by latent semantic analysis. Jour-
nal of the American society for information science,
41(6):391–407.
Najim Dehak, Patrick J. Kenny, R´eda Dehak, Pierre
Dumouchel, and Pierre Ouellet. 2011. Front-end
factor analysis for speaker verification. IEEE Trans-
actions on Audio, Speech, and Language Process-
ing, 19(4):788–798.
Javier Franco-Pedroso, Ignacio Lopez-Moreno, Doro-
teo T Toledano, and Joaquin Gonzalez-Rodriguez.
2010. Atvs-uam system description for the audio
segmentation and speaker diarization albayzin 2010
evaluation. In FALA VI Jornadas en Tecnologa del
Habla and II Iberian SLTech Workshop, pages 415–
418.
Daniel Garcia-Romero and Carol Y Espy-Wilson.
2011. Analysis of i-vector length normalization in
speaker recognition systems. In Interspeech, pages
249–252.
Stuart Geman and Donald Geman. 1984. Stochas-
tic relaxation, gibbs distributions, and the bayesian
restoration of images. IEEE Transactions on Pattern
Analysis and Machine Intelligence, (6):721–741.
Thomas L. Griffiths and Mark Steyvers. 2004. Find-
ing scientific topics. Proceedings of the National
academy of Sciences of the United States of Amer-
ica, 101(Suppl 1):5228–5235.
Gregor Heinrich. 2005. Parameter estimation
for text analysis. Web: http://www. arbylon.
net/publications/text-est. pdf.
Thomas Hofmann. 1999. Probabilistic latent semantic
analysis. In Proc. of Uncertainty in Artificial Intelli-
gence, UAI ’ 99, page 21. Citeseer.
Thomas Hofmann. 2001. Unsupervised learning
by probabilistic latent semantic analysis. Machine
Learning, 42(1):177–196.
Patrick Kenny, Gilles Boulianne, Pierre Ouellet, and
Pierre Dumouchel. 2007. Joint factor analysis ver-
sus eigenchannels in speaker recognition. IEEE
Transactions on Audio, Speech, and Language Pro-
cessing, 15(4):1435–1447.
Patrick Kenny, Pierre Ouellet, Najim Dehak, Vishwa
Gupta, and Pierre Dumouchel. 2008. A study of in-
terspeaker variability in speaker verification. IEEE
Transactions on Audio, Speech, and Language Pro-
cessing, 16(5):980–988.
Wei Li and Andrew McCallum. 2006. Pachinko allo-
cation: Dag-structured mixture models of topic cor-
relations.
Georges Linar`es, Pascal Noc´era, Dominique Massonie,
and Driss Matrouf. 2007. The lia speech recogni-
tion system: from 10xrt to 1xrt. In Text, Speech and
Dialogue, pages 302–308. Springer.
David Martınez, Oldrich Plchot, Luk´as Burget, On-
drej Glembek, and Pavel Matejka. 2011. Language
recognition in ivectors space. Interspeech, pages
861–864.
Driss Matrouf, Nicolas Scheffer, Benoit G.B. Fauve,
and Jean-Francois Bonastre. 2007. A straightfor-
ward and efficient implementation of the factor anal-
ysis model for speaker verification. In Interspeech,
pages 1242–1245.
Thomas Minka and John Lafferty. 2002. Expectation-
propagation for the generative aspect model. In
Proceedings of the Eighteenth conference on Uncer-
tainty in artificial intelligence, pages 352–359. Mor-
gan Kaufmann Publishers Inc.
Mohamed Morchid, Georges Linar`es, Marc El-Beze,
and Renato De Mori. 2013. Theme identification in
telephone service conversations using quaternions of
speech features. In Interspeech. ISCA.
</reference>
<page confidence="0.991271">
453
</page>
<reference confidence="0.9991352">
Mohamed Morchid, Richard Dufour, Pierre-Michel
Bousquet, Mohamed Bouallegue, Georges Linar`es,
and Renato De Mori. 2014a. Improving dialogue
classification using a topic space representation and
a gaussian classifier based on the decision rule. In
ICASSP. IEEE.
Mohamed Morchid, Richard Dufour, and Georges
Linar`es. 2014b. A LDA-Based Topic Classification
Approach from Highly Imperfect Automatic Tran-
scriptions. In LREC.
Douglas A. Reynolds and Richard C. Rose. 1995.
Robust text-independent speaker identification using
gaussian mixture speaker models. IEEE Transac-
tions on Speech and Audio Processing, 3(1):72–83.
Gerard Salton. 1989. Automatic text processing: the
transformation. Analysis and Retrieval of Informa-
tion by Computer.
Yoshimi Suzuki, Fumiyo Fukumoto, and Yoshihiro
Sekiguchi. 1998. Keyword extraction using term-
domain interdependence for dictation of radio news.
In 17th international conference on Computational
linguistics, volume 2, pages 1272–1276. ACL.
Yee Whye Teh, Michael I. Jordan, Matthew J. Beal,
and David M. Blei. 2004. Sharing clusters among
related groups: Hierarchical dirichlet processes. In
NIPS.
Eric P. Xing, Michael I. Jordan, Stuart Russell, and
Andrew Ng. 2002. Distance metric learning with
application to clustering with side-information. In
Advances in neural information processing systems,
pages 505–512.
Elias Zavitsanos, Sergios Petridis, Georgios Paliouras,
and George A. Vouros. 2008. Determining auto-
matically the size of learned ontologies. In ECAI,
volume 178, pages 775–776.
</reference>
<page confidence="0.999174">
454
</page>
</variant>
</algorithm>
<algorithm name="ParsHed" version="110505">
<variant no="0" confidence="0.875785">
<title confidence="0.9994945">An I-vector Based Approach to Compact Multi-Granularity Topic Spaces Representation of Textual Documents</title>
<author confidence="0.993039">Mohamed Richard</author>
<affiliation confidence="0.999458">Driss and Renato de University of Avignon,</affiliation>
<address confidence="0.934365">University, School of Computer Science, Montreal, Quebec,</address>
<email confidence="0.998827">rdemori@cs.mcgill.ca</email>
<abstract confidence="0.997701423076923">Various studies highlighted that topicbased approaches give a powerful spoken content representation of documents. Nonetheless, these documents may contain more than one main theme, and their automatic transcription inevitably contains errors. In this study, we propose an original and promising framework based on a compact representation of a textual document, to solve issues related to topic space granularity. Firstly, various topic spaces are estimated with different numbers of classes from a Latent Dirichlet Allocation. Then, this multiple topic space representation is compacted into an elementary segcalled originally developed in the context of speaker recognition. Experiments are conducted on the DECODA corpus of conversations. Results show the effectiveness of the proposed multi-view compact representation paradigm. Our identification system reaches an accuracy of 85%, with a significant gain of 9 points compared to the baseline (best single topic space configuration).</abstract>
</variant>
</algorithm>
<algorithm name="ParsCit" version="110505">
<citationList>
<citation valid="true">
<authors>
<author>R Arun</author>
<author>Venkatasubramaniyan Suresh</author>
<author>C E Veni Madhavan</author>
<author>Musti Narasimha Murty</author>
</authors>
<title>On finding the natural number of topics with latent dirichlet allocation: Some observations.</title>
<date>2010</date>
<booktitle>In Advances in Knowledge Discovery and Data Mining,</booktitle>
<pages>391--402</pages>
<publisher>Springer.</publisher>
<contexts>
<context position="10822" citStr="Arun et al., 2010" startWordPosition="1649" endWordPosition="1652">perform LSA on IR tasks (Hofmann, 2001). Moreover, LDA provides a direct estimate of the relevance of a topic given a word set. In this paper, probabilities of hidden topic features, estimated with LDA, are considered for possibly capturing word dependencies expressing the semantic contents of a given conversation. Topic-based approaches involve defining a number of topics composing the topic space. The choice of the “right” number of topics is a crucial step, especially when the documents may contain multiple themes. Many studies have tried to find a relevant method to deal with this issue. (Arun et al., 2010) proposed to use a Singular Value Decomposition (SVD) to represent the separability between the words contained in the vocabulary. Then, if the singular values of the topic-word matrix M equal the norm of the rows of M, this means that the vocabulary is well separated among the topics. This method has to be evaluated with the Kullback-Liebler divergence metric for each topic space. However, this process would be time consuming for thousands of representations of a dialogue. (Teh et al., 2004) proposed the Hierarchical Dirichlet Process (HDP) method to find the “right” number of topics by assum</context>
</contexts>
<marker>Arun, Suresh, Madhavan, Murty, 2010</marker>
<rawString>R. Arun, Venkatasubramaniyan Suresh, C.E. Veni Madhavan, and Musti Narasimha Murty. 2010. On finding the natural number of topics with latent dirichlet allocation: Some observations. In Advances in Knowledge Discovery and Data Mining, pages 391–402. Springer.</rawString>
</citation>
<citation valid="true">
<authors>
<author>Frederic Bechet</author>
<author>Benjamin Maza</author>
<author>Nicolas Bigouroux</author>
</authors>
<title>Thierry Bazillon, Marc El-Beze, Renato De Mori, and Eric Arbillot.</title>
<date>2012</date>
<pages>12</pages>
<contexts>
<context position="2126" citStr="Bechet et al., 2012" startWordPosition="297" endWordPosition="300">rror Rates (WER) make the analysis of the automatic transcriptions difficult. Speech analytics suffer from these transcription issues that may be overcome by improving the ASR robustness and/or the tolerance of speech analytic systems to ASR errors. This paper proposes a new method to improve the robustness of speech analytics by combining a semantic multi-model approach and a noise reduction technique based on the i-vector paradigm. This method is evaluated in the application framework of the RATP call centre (Paris Public Transportation Authority), focusing on the theme identification task (Bechet et al., 2012). Telephone conversations are a particular case of human-human interaction whose automatic processing raises problems, especially due to the speech recognition step required to obtain the transcription of the speech contents. First, the speaker’s behavior may be unexpected and the training/test mismatch may be very large. Second, the speech signal may be strongly impacted by various sources of variability: environment and channel noises, acquisition devices, etc. Telephone conversation issues Topics are related to the reason why the customer called. Various classes corresponding to the main cu</context>
<context position="24922" citStr="Bechet et al., 2012" startWordPosition="4050" endWordPosition="4053"> of c-vectors when D−2 is applied; and finally, Figure 3-(d) shows the cvector x0 (d,r) on the surface area of the unit hyper- sphereafter a length normalization by a division �of (x(d,r)− x)TV−1(x(d,r) − x). 5 Experimental Protocol The proposed c-vector representation of automatic transcriptions is evaluated in the context of the theme identification of a human-human telephone conversation in the customer care service (CCS) of the RATP Paris transportation system. The metric used to identify of the best theme is the Mahalanobis metric. 5.1 Theme identification task The DECODA project corpus (Bechet et al., 2012) was designed to perform experiments on the identification of conversation themes. It is composed of 1,514 telephone conversations, corresponding to about 74 hours of signal, split into a training �LUg = (d,r) RUig �= (d,r) (8) 0 1D−2 PT (x(d r) − x) 448 −2 −1 0 1 2 −2 −1 0 1 2 a.Inital: M b.Rotation: Pt ● ● ● ● ● ● ● ● ● ● ● ● ● ● ● ● ●● ●● ● ● ● ● ● ● ●● ● ● ●● ● ● ● ● ● ● ● ● ● ● ● ● ● ● ● ● ● ● ● ● ● ● ●● ● ● ● ●● ● ● ● ● ●● ● ● ● ● ● ● ● ● ● ● ● ● ● −3 −2 −1 0 1 2 3 −3 −2 −1 0 1 2 c.Standardization: D−1 2 ● ● ●● ● ●● ● ● ● ●● ●● ●● ● ● ● ● ● ● ● ● ● ● ● ● ● ● ●● ● ● ● ● ● ● ● ● ● ● d.Norm</context>
</contexts>
<marker>Bechet, Maza, Bigouroux, 2012</marker>
<rawString>Frederic Bechet, Benjamin Maza, Nicolas Bigouroux, Thierry Bazillon, Marc El-Beze, Renato De Mori, and Eric Arbillot. 2012. Decoda: a callcentre human-human spoken conversation corpus. LREC’12.</rawString>
</citation>
<citation valid="true">
<authors>
<author>Jerome R Bellegarda</author>
</authors>
<title>A latent semantic analysis framework for large-span language modeling. In</title>
<date>1997</date>
<booktitle>Fifth European Conference on Speech Communication and Technology.</booktitle>
<contexts>
<context position="9223" citStr="Bellegarda, 1997" startWordPosition="1393" endWordPosition="1394">set of topic-based representations (frames) of a dialogue (session). The paper is organized as follows. Section 2 presents previous related works. The dialogue representation is described in Section 3. Section 4 introduces the i-vector compact representation and presents its application to text documents. Sections 5 and 6 report experiments and results. The last section concludes and proposes some perspectives. 2 Related work In the past, several approaches considered a text document as a mixture of latent topics. These methods, such as Latent Semantic Analysis (LSA) (Deerwester et al., 1990; Bellegarda, 1997), Probabilistic LSA (PLSA) (Hofmann, 1999) or Latent Dirichlet Allocation (LDA) (Blei et al., 2003), build a higher-level representation of the document in a topic space. ¿ Document is then considered as a bag-of-words (Salton, 1989) where the word order is not taken into account. These methods have demonstrated their performance on various tasks, such as sentence (Bellegarda, 2000) or keyword (Suzuki et al., 1998) extraction. In opposition to a multinomial mixture model, LDA considers that a theme is associated to each occurrence of a word composing the document, rather than associate a topic</context>
</contexts>
<marker>Bellegarda, 1997</marker>
<rawString>Jerome R. Bellegarda. 1997. A latent semantic analysis framework for large-span language modeling. In Fifth European Conference on Speech Communication and Technology.</rawString>
</citation>
<citation valid="true">
<authors>
<author>Jerome R Bellegarda</author>
</authors>
<title>Exploiting latent semantic information in statistical language modeling.</title>
<date>2000</date>
<booktitle>Proceedings of the IEEE,</booktitle>
<pages>88--8</pages>
<contexts>
<context position="9608" citStr="Bellegarda, 2000" startWordPosition="1452" endWordPosition="1454">d proposes some perspectives. 2 Related work In the past, several approaches considered a text document as a mixture of latent topics. These methods, such as Latent Semantic Analysis (LSA) (Deerwester et al., 1990; Bellegarda, 1997), Probabilistic LSA (PLSA) (Hofmann, 1999) or Latent Dirichlet Allocation (LDA) (Blei et al., 2003), build a higher-level representation of the document in a topic space. ¿ Document is then considered as a bag-of-words (Salton, 1989) where the word order is not taken into account. These methods have demonstrated their performance on various tasks, such as sentence (Bellegarda, 2000) or keyword (Suzuki et al., 1998) extraction. In opposition to a multinomial mixture model, LDA considers that a theme is associated to each occurrence of a word composing the document, rather than associate a topic to the complete document. Therefore, a document can change topics from a word to another one. However, word occurrences are connected by a latent variable which controls the global match of the distribution of the topics in the document. These latent topics are characterized by a distribution of associated word probabilities. PLSA and LDA models have been shown to generally outperf</context>
</contexts>
<marker>Bellegarda, 2000</marker>
<rawString>Jerome R. Bellegarda. 2000. Exploiting latent semantic information in statistical language modeling. Proceedings of the IEEE, 88(8):1279–1296.</rawString>
</citation>
<citation valid="true">
<authors>
<author>David M Blei</author>
<author>John Lafferty</author>
</authors>
<title>Correlated topic models. Advances in neural information processing systems,</title>
<date>2006</date>
<pages>18--147</pages>
<contexts>
<context position="12541" citStr="Blei and Lafferty, 2006" startWordPosition="1944" endWordPosition="1947">ave only one representation since they consider that finding the optimal topic model is the best solution. Another solution would be to consider a set of topic models to represent a document. Nonetheless, a multi-topic-based representation of a dialogue can involve a noisy variability due to the mapping of a dialogue in each topic space. Indeed, a dialogue does not share its content (i.e. words) with each class composing the topic space. Thus, a variability is added during the mapping process. Another weakness of the multi-view representation is the relation between classes in a topic space. (Blei and Lafferty, 2006) show that classes into a LDA topic space are correlated. Moreover, (Li and McCallum, 2006) consider a class as a node of an acyclic graph and as a distribution over other classes contained in the same topic space. 3 Multi-view representation of automatic dialogue transcriptions in a homogeneous space The purpose of the considered application is the identification of the major theme of a humanhuman telephone conversation in the customer 445 care service (CCS) of the RATP Paris transportation system. The approach considered in this paper focuses on modeling the variability between different dia</context>
</contexts>
<marker>Blei, Lafferty, 2006</marker>
<rawString>David M. Blei and John Lafferty. 2006. Correlated topic models. Advances in neural information processing systems, 18:147.</rawString>
</citation>
<citation valid="true">
<authors>
<author>David M Blei</author>
<author>Andrew Y Ng</author>
<author>Michael I Jordan</author>
</authors>
<title>Latent dirichlet allocation.</title>
<date>2003</date>
<journal>The Journal of Machine Learning Research,</journal>
<pages>3--993</pages>
<contexts>
<context position="4716" citStr="Blei et al., 2003" startWordPosition="690" endWordPosition="693"> contains more than one theme (infraction + transportation cards). during a telephone conversation. Indeed, an agent have to follow an predefined scenario of conversation. Thus, the agent can find the main reason for the call which corresponds to the theme. Proposed solutions An efficient way to tackle both ASR robustness and class ambiguity could be to map dialogues into a topic space abstracting the ASR outputs. Then, dialogue categorization is achieved in this topic space. Numerous unsupervised methods for topic-space estimation were proposed in the past. Latent Dirichlet Allocation (LDA) (Blei et al., 2003) has been largely used for speech analytics; one of its main drawbacks is the tuning of the model, that involves various meta-parameters such as the number of classes (that determines the model granularity), word distribution methods, temporal spans... If the decision process is highly dependent on these features, the system’s performance could be quite unstable. Classically, this abstract representation involves selecting the right number of classes composing the topic space. This decision is crucial since topic model perplexity, which expresses its quality, is highly dependent on this featur</context>
<context position="9322" citStr="Blei et al., 2003" startWordPosition="1405" endWordPosition="1408">lows. Section 2 presents previous related works. The dialogue representation is described in Section 3. Section 4 introduces the i-vector compact representation and presents its application to text documents. Sections 5 and 6 report experiments and results. The last section concludes and proposes some perspectives. 2 Related work In the past, several approaches considered a text document as a mixture of latent topics. These methods, such as Latent Semantic Analysis (LSA) (Deerwester et al., 1990; Bellegarda, 1997), Probabilistic LSA (PLSA) (Hofmann, 1999) or Latent Dirichlet Allocation (LDA) (Blei et al., 2003), build a higher-level representation of the document in a topic space. ¿ Document is then considered as a bag-of-words (Salton, 1989) where the word order is not taken into account. These methods have demonstrated their performance on various tasks, such as sentence (Bellegarda, 2000) or keyword (Suzuki et al., 1998) extraction. In opposition to a multinomial mixture model, LDA considers that a theme is associated to each occurrence of a word composing the document, rather than associate a topic to the complete document. Therefore, a document can change topics from a word to another one. Howe</context>
<context position="13464" citStr="Blei et al., 2003" startWordPosition="2092" endWordPosition="2095">e purpose of the considered application is the identification of the major theme of a humanhuman telephone conversation in the customer 445 care service (CCS) of the RATP Paris transportation system. The approach considered in this paper focuses on modeling the variability between different dialogues expressing the same theme t. For this purpose, it is important to select relevant features that represent semantic contents for the theme of a dialogue. An attractive set of features for capturing possible semantically relevant word dependencies is obtained with Latent Dirichlet Allocation (LDA) (Blei et al., 2003), as described in section 2. Given a training set of conversations D, a hidden topic space is derived and a conversation d is represented by its probability in each topic of the hidden space. Estimation of these probabilities is affected by a variability inherent to the estimation of the model parameters. If many hidden spaces are considered and features are computed for each hidden space, it is possible to model the estimation variability together with the variability of the linguistic expression of a theme by different speakers in different real-life situations. Even if the purpose of the ap</context>
<context position="14756" citStr="Blei et al., 2003" startWordPosition="2305" endWordPosition="2308">mes is available, supervised LDA (Griffiths and Steyvers, 2004) is not suitable for the proposed approach. LDA is used only for producing different feature sets used involved in statistical variability models. In order to estimate the parameters of different hidden spaces, a set of discriminative words V is constructed as described in (Morchid et al., 2014a). Each theme t contains a set of specific words. Note that the same word may appear in several discriminative word sets. All the selected words are then merged without repetition to form V . Several techniques, such as Variational Methods (Blei et al., 2003), Expectation-propagation (Minka and Lafferty, 2002) or Gibbs Sampling (Griffiths and Steyvers, 2004), have been proposed for estimating the parameters describing a LDA hidden space. Gibbs Sampling is a special case of Markov-chain Monte Carlo (MCMC) (Geman and Geman, 1984) and gives a simple algorithm for approximate inference in high-dimensional models such as LDA (Heinrich, 2005). This overcomes the difficulty to directly and exactly estimate parameters that maximize the likelihood of the whole data collection defined as: p(W |−→ α , →−β ) = 11w∈W p(−→w |−→ α , →−β ) for the whole data coll</context>
</contexts>
<marker>Blei, Ng, Jordan, 2003</marker>
<rawString>David M. Blei, Andrew Y. Ng, and Michael I. Jordan. 2003. Latent dirichlet allocation. The Journal of Machine Learning Research, 3:993–1022.</rawString>
</citation>
<citation valid="true">
<authors>
<author>Pierre-Michel Bousquet</author>
<author>Driss Matrouf</author>
<author>JeanFranc¸ois Bonastre</author>
</authors>
<title>Intersession compensation and scoring methods in the i-vectors space for speaker recognition. In Interspeech,</title>
<date>2011</date>
<pages>485--488</pages>
<contexts>
<context position="7783" citStr="Bousquet et al., 2011" startWordPosition="1160" endWordPosition="1163">or variability in a low dimensional space. Although this compact representation is widely used in speaker recognition systems, this method has not been used yet in the field of text classification. In this paper, we propose to apply factor analysis to compensate noisy variabilities due to the multiplication of LDA models. Furthermore, a normalization approach to condition dialogue representations (multi-model and i-vector) is presented. The two methods showed improvements for speaker verification: within Class Covariance Normalization (WCCN) (Dehak et al., 2011) and Eigen Factor Radial (EFR) (Bousquet et al., 2011). The latter includes length normalization (GarciaRomero and Espy-Wilson, 2011). Both methods dilate the total variability space as a means of reducing the within-class variability. In our multimodel representation, the within class variability is redefined according to both dialogue content Agent: Hello Customer: Hello Agent: Speaking ... Customer: I call you because I was fined today, but I still have an imagine card suitable for zone 1 [...] I forgot to use my navigo card for zone 2 Agent: You did not use your navigo card, that is why they give you a fine not for a zone issue [...] Customer</context>
<context position="23311" citStr="Bousquet et al., 2011" startWordPosition="3779" endWordPosition="3782">e used to assess the convergence. One can refer to (Matrouf et al., 2007) to find out more about the implementation of the factor analysis. C-vector representation suffers from 3 raised cvector issues: (i) the c-vectors x of equation 2 have to be theoretically distributed among the normal distribution N (0, I), (ii) the “radial” effect should be removed, and (iii) the full rank total factor space should be used to apply discriminant transformations. The next section presents a solution to these 3 problems. 4.3 C-vector standardization A solution to standardize c-vectors has been developed in (Bousquet et al., 2011). The authors proposed to apply transformations for training and test transcription representations. The first step is to evaluate the empirical mean x and covariance matrix V of the training c-vector. Covariance matrix V is decomposed by diagonalization into: PDPT (9) where P is the eigenvector matrix of V and D is the diagonal version of V. A training i-vector x(d,r) is transformed in x0(d,r) as follows: x (d,r) = O (10) x(d,r) − x)TV−1(x(d,r) − x) The numerator is equivalent by rotation to 1 V−2 (x(d,r) − x) and the Euclidean norm of x0(d,r) is equal to 1. The same transformation is applied</context>
</contexts>
<marker>Bousquet, Matrouf, Bonastre, 2011</marker>
<rawString>Pierre-Michel Bousquet, Driss Matrouf, and JeanFranc¸ois Bonastre. 2011. Intersession compensation and scoring methods in the i-vectors space for speaker recognition. In Interspeech, pages 485–488.</rawString>
</citation>
<citation valid="true">
<authors>
<author>Juan Cao</author>
<author>Tian Xia</author>
<author>Jintao Li</author>
<author>Yongdong Zhang</author>
<author>Sheng Tang</author>
</authors>
<title>A density-based method for adaptive lda model selection.</title>
<date>2009</date>
<journal>Neurocomputing,</journal>
<volume>72</volume>
<issue>7</issue>
<contexts>
<context position="11716" citStr="Cao et al., 2009" startWordPosition="1804" endWordPosition="1807">e topics. This method has to be evaluated with the Kullback-Liebler divergence metric for each topic space. However, this process would be time consuming for thousands of representations of a dialogue. (Teh et al., 2004) proposed the Hierarchical Dirichlet Process (HDP) method to find the “right” number of topics by assuming that the data has a hierarchical structure. The HDP models were then compared to the LDA ones on the same dataset. (Zavitsanos et al., 2008) presented a method to learn the right depth of an ontology depending of the number of topics of LDA models. The study presented by (Cao et al., 2009) is quite similar to (Teh et al., 2004). The authors consider the average correlation between pairs of topics at each stage as the right number of topics. All these methods assume that a document can have only one representation since they consider that finding the optimal topic model is the best solution. Another solution would be to consider a set of topic models to represent a document. Nonetheless, a multi-topic-based representation of a dialogue can involve a noisy variability due to the mapping of a dialogue in each topic space. Indeed, a dialogue does not share its content (i.e. words) </context>
</contexts>
<marker>Cao, Xia, Li, Zhang, Tang, 2009</marker>
<rawString>Juan Cao, Tian Xia, Jintao Li, Yongdong Zhang, and Sheng Tang. 2009. A density-based method for adaptive lda model selection. Neurocomputing, 72(7):1775–1781.</rawString>
</citation>
<citation valid="true">
<authors>
<author>Scott Deerwester</author>
<author>Susan T Dumais</author>
<author>George W Furnas</author>
<author>Thomas K Landauer</author>
<author>Richard Harshman</author>
</authors>
<title>Indexing by latent semantic analysis.</title>
<date>1990</date>
<journal>Journal of the American society for information science,</journal>
<pages>41--6</pages>
<contexts>
<context position="9204" citStr="Deerwester et al., 1990" startWordPosition="1389" endWordPosition="1392">the speaker session is a set of topic-based representations (frames) of a dialogue (session). The paper is organized as follows. Section 2 presents previous related works. The dialogue representation is described in Section 3. Section 4 introduces the i-vector compact representation and presents its application to text documents. Sections 5 and 6 report experiments and results. The last section concludes and proposes some perspectives. 2 Related work In the past, several approaches considered a text document as a mixture of latent topics. These methods, such as Latent Semantic Analysis (LSA) (Deerwester et al., 1990; Bellegarda, 1997), Probabilistic LSA (PLSA) (Hofmann, 1999) or Latent Dirichlet Allocation (LDA) (Blei et al., 2003), build a higher-level representation of the document in a topic space. ¿ Document is then considered as a bag-of-words (Salton, 1989) where the word order is not taken into account. These methods have demonstrated their performance on various tasks, such as sentence (Bellegarda, 2000) or keyword (Suzuki et al., 1998) extraction. In opposition to a multinomial mixture model, LDA considers that a theme is associated to each occurrence of a word composing the document, rather tha</context>
</contexts>
<marker>Deerwester, Dumais, Furnas, Landauer, Harshman, 1990</marker>
<rawString>Scott Deerwester, Susan T. Dumais, George W. Furnas, Thomas K. Landauer, and Richard Harshman. 1990. Indexing by latent semantic analysis. Journal of the American society for information science, 41(6):391–407.</rawString>
</citation>
<citation valid="true">
<authors>
<author>Najim Dehak</author>
<author>Patrick J Kenny</author>
<author>R´eda Dehak</author>
<author>Pierre Dumouchel</author>
<author>Pierre Ouellet</author>
</authors>
<title>Front-end factor analysis for speaker verification.</title>
<date>2011</date>
<journal>IEEE Transactions on Audio, Speech, and Language Processing,</journal>
<volume>19</volume>
<issue>4</issue>
<contexts>
<context position="6995" citStr="Dehak et al., 2011" startWordPosition="1041" endWordPosition="1044">peaker identification. In this field, the factor analysis paradigm is used as a decomposition model that enables to separate the representation space into two subspaces containing respectively useful and useless information. The general Joint Factor Analysis (JFA) paradigm (Kenny et al., 2008) considers multiple variabilities that may be cross-dependent. Therefore, JFA representation allows us to compensate the variability within sessions of a same speaker. This representation is an extension of the GMM-UBM (Gaussian Mixture Model-Universal Background Model) models (Reynolds and Rose, 1995). (Dehak et al., 2011) extract a compact super-vector (called an i-vector) from the GMM super-vector. The aim of the compression process (i-vector extraction) is to represent the supervector variability in a low dimensional space. Although this compact representation is widely used in speaker recognition systems, this method has not been used yet in the field of text classification. In this paper, we propose to apply factor analysis to compensate noisy variabilities due to the multiplication of LDA models. Furthermore, a normalization approach to condition dialogue representations (multi-model and i-vector) is pres</context>
</contexts>
<marker>Dehak, Kenny, Dehak, Dumouchel, Ouellet, 2011</marker>
<rawString>Najim Dehak, Patrick J. Kenny, R´eda Dehak, Pierre Dumouchel, and Pierre Ouellet. 2011. Front-end factor analysis for speaker verification. IEEE Transactions on Audio, Speech, and Language Processing, 19(4):788–798.</rawString>
</citation>
<citation valid="true">
<authors>
<author>Javier Franco-Pedroso</author>
<author>Ignacio Lopez-Moreno</author>
<author>Doroteo T Toledano</author>
<author>Joaquin Gonzalez-Rodriguez</author>
</authors>
<title>Atvs-uam system description for the audio segmentation and speaker diarization albayzin 2010 evaluation.</title>
<date>2010</date>
<booktitle>In FALA VI Jornadas en Tecnologa del Habla and II Iberian SLTech Workshop,</booktitle>
<pages>415--418</pages>
<contexts>
<context position="18825" citStr="Franco-Pedroso et al., 2010" startWordPosition="3025" endWordPosition="3028">j �−−→ −−→ � =V wi zr , V zr d where h·, ·i is the inner product, S being the frequency of the term wi in d, V wi zr j= P(wi|zj) and Vzr d = P(zj|d) evaluated using Gibbs Sampling j in the topic space r. 4 Compact multi-view representation In this section, an i-vector-based method to represent automatic transcriptions is presented. Initially introduced for speaker recognition, ivectors (Kenny et al., 2008) have become very popular in the field of speech processing and recent publications show that they are also reliable for language recognition (Martınez et al., 2011) and speaker diarization (Franco-Pedroso et al., 2010). I-vectors are an elegant way of reducing the imput space dimensionality while retaining most of the relevant information. The technique was originally inspired by the Joint Factor Analysis framework (Kenny et al., 2007). Hence, i-vectors convey the speaker characteristics among other information such as transmission channel, acoustic environment or phonetic content of speech segments. The next sections describe the i-vector extraction process, the application of this compact representation to textual documents (called c-vector), and the vector transformation with the EFR method and the Mahal</context>
</contexts>
<marker>Franco-Pedroso, Lopez-Moreno, Toledano, Gonzalez-Rodriguez, 2010</marker>
<rawString>Javier Franco-Pedroso, Ignacio Lopez-Moreno, Doroteo T Toledano, and Joaquin Gonzalez-Rodriguez. 2010. Atvs-uam system description for the audio segmentation and speaker diarization albayzin 2010 evaluation. In FALA VI Jornadas en Tecnologa del Habla and II Iberian SLTech Workshop, pages 415– 418.</rawString>
</citation>
<citation valid="true">
<authors>
<author>Daniel Garcia-Romero</author>
<author>Carol Y Espy-Wilson</author>
</authors>
<title>Analysis of i-vector length normalization in speaker recognition systems.</title>
<date>2011</date>
<booktitle>In Interspeech,</booktitle>
<pages>249--252</pages>
<marker>Garcia-Romero, Espy-Wilson, 2011</marker>
<rawString>Daniel Garcia-Romero and Carol Y Espy-Wilson. 2011. Analysis of i-vector length normalization in speaker recognition systems. In Interspeech, pages 249–252.</rawString>
</citation>
<citation valid="true">
<authors>
<author>Stuart Geman</author>
<author>Donald Geman</author>
</authors>
<title>Stochastic relaxation, gibbs distributions, and the bayesian restoration of images.</title>
<date>1984</date>
<journal>IEEE Transactions on Pattern Analysis and Machine Intelligence,</journal>
<pages>6--721</pages>
<contexts>
<context position="15030" citStr="Geman and Geman, 1984" startWordPosition="2346" endWordPosition="2349">es, a set of discriminative words V is constructed as described in (Morchid et al., 2014a). Each theme t contains a set of specific words. Note that the same word may appear in several discriminative word sets. All the selected words are then merged without repetition to form V . Several techniques, such as Variational Methods (Blei et al., 2003), Expectation-propagation (Minka and Lafferty, 2002) or Gibbs Sampling (Griffiths and Steyvers, 2004), have been proposed for estimating the parameters describing a LDA hidden space. Gibbs Sampling is a special case of Markov-chain Monte Carlo (MCMC) (Geman and Geman, 1984) and gives a simple algorithm for approximate inference in high-dimensional models such as LDA (Heinrich, 2005). This overcomes the difficulty to directly and exactly estimate parameters that maximize the likelihood of the whole data collection defined as: p(W |−→ α , →−β ) = 11w∈W p(−→w |−→ α , →−β ) for the whole data collection W knowing the Dirichlet parameters →−α and →− β . Gibbs Sampling allows us both to estimate the LDA parameters in order to represent a new dialogue d with the rth topic space of size n, and to obtain a feature vector V zr d of the topic representation of d. The jth f</context>
</contexts>
<marker>Geman, Geman, 1984</marker>
<rawString>Stuart Geman and Donald Geman. 1984. Stochastic relaxation, gibbs distributions, and the bayesian restoration of images. IEEE Transactions on Pattern Analysis and Machine Intelligence, (6):721–741.</rawString>
</citation>
<citation valid="true">
<authors>
<author>Thomas L Griffiths</author>
<author>Mark Steyvers</author>
</authors>
<title>Finding scientific topics.</title>
<date>2004</date>
<booktitle>Proceedings of the National academy of Sciences of the United States of America, 101(Suppl</booktitle>
<pages>1--5228</pages>
<contexts>
<context position="14201" citStr="Griffiths and Steyvers, 2004" startWordPosition="2212" endWordPosition="2215">conversation d is represented by its probability in each topic of the hidden space. Estimation of these probabilities is affected by a variability inherent to the estimation of the model parameters. If many hidden spaces are considered and features are computed for each hidden space, it is possible to model the estimation variability together with the variability of the linguistic expression of a theme by different speakers in different real-life situations. Even if the purpose of the application is theme identification and a training corpus annotated with themes is available, supervised LDA (Griffiths and Steyvers, 2004) is not suitable for the proposed approach. LDA is used only for producing different feature sets used involved in statistical variability models. In order to estimate the parameters of different hidden spaces, a set of discriminative words V is constructed as described in (Morchid et al., 2014a). Each theme t contains a set of specific words. Note that the same word may appear in several discriminative word sets. All the selected words are then merged without repetition to form V . Several techniques, such as Variational Methods (Blei et al., 2003), Expectation-propagation (Minka and Lafferty</context>
</contexts>
<marker>Griffiths, Steyvers, 2004</marker>
<rawString>Thomas L. Griffiths and Mark Steyvers. 2004. Finding scientific topics. Proceedings of the National academy of Sciences of the United States of America, 101(Suppl 1):5228–5235.</rawString>
</citation>
<citation valid="true">
<authors>
<author>Gregor Heinrich</author>
</authors>
<title>Parameter estimation for text analysis.</title>
<date>2005</date>
<note>Web: http://www. arbylon. net/publications/text-est. pdf.</note>
<contexts>
<context position="15141" citStr="Heinrich, 2005" startWordPosition="2364" endWordPosition="2365">t of specific words. Note that the same word may appear in several discriminative word sets. All the selected words are then merged without repetition to form V . Several techniques, such as Variational Methods (Blei et al., 2003), Expectation-propagation (Minka and Lafferty, 2002) or Gibbs Sampling (Griffiths and Steyvers, 2004), have been proposed for estimating the parameters describing a LDA hidden space. Gibbs Sampling is a special case of Markov-chain Monte Carlo (MCMC) (Geman and Geman, 1984) and gives a simple algorithm for approximate inference in high-dimensional models such as LDA (Heinrich, 2005). This overcomes the difficulty to directly and exactly estimate parameters that maximize the likelihood of the whole data collection defined as: p(W |−→ α , →−β ) = 11w∈W p(−→w |−→ α , →−β ) for the whole data collection W knowing the Dirichlet parameters →−α and →− β . Gibbs Sampling allows us both to estimate the LDA parameters in order to represent a new dialogue d with the rth topic space of size n, and to obtain a feature vector V zr d of the topic representation of d. The jth feature V zj = P(zjr |d) (where 1 G j G n) is the probability of topic zrj to be generated by the unseen dialogu</context>
</contexts>
<marker>Heinrich, 2005</marker>
<rawString>Gregor Heinrich. 2005. Parameter estimation for text analysis. Web: http://www. arbylon. net/publications/text-est. pdf.</rawString>
</citation>
<citation valid="true">
<authors>
<author>Thomas Hofmann</author>
</authors>
<title>Probabilistic latent semantic analysis.</title>
<date>1999</date>
<booktitle>In Proc. of Uncertainty in Artificial Intelligence, UAI ’ 99,</booktitle>
<pages>21</pages>
<publisher>Citeseer.</publisher>
<contexts>
<context position="9265" citStr="Hofmann, 1999" startWordPosition="1398" endWordPosition="1399">f a dialogue (session). The paper is organized as follows. Section 2 presents previous related works. The dialogue representation is described in Section 3. Section 4 introduces the i-vector compact representation and presents its application to text documents. Sections 5 and 6 report experiments and results. The last section concludes and proposes some perspectives. 2 Related work In the past, several approaches considered a text document as a mixture of latent topics. These methods, such as Latent Semantic Analysis (LSA) (Deerwester et al., 1990; Bellegarda, 1997), Probabilistic LSA (PLSA) (Hofmann, 1999) or Latent Dirichlet Allocation (LDA) (Blei et al., 2003), build a higher-level representation of the document in a topic space. ¿ Document is then considered as a bag-of-words (Salton, 1989) where the word order is not taken into account. These methods have demonstrated their performance on various tasks, such as sentence (Bellegarda, 2000) or keyword (Suzuki et al., 1998) extraction. In opposition to a multinomial mixture model, LDA considers that a theme is associated to each occurrence of a word composing the document, rather than associate a topic to the complete document. Therefore, a do</context>
</contexts>
<marker>Hofmann, 1999</marker>
<rawString>Thomas Hofmann. 1999. Probabilistic latent semantic analysis. In Proc. of Uncertainty in Artificial Intelligence, UAI ’ 99, page 21. Citeseer.</rawString>
</citation>
<citation valid="true">
<authors>
<author>Thomas Hofmann</author>
</authors>
<title>Unsupervised learning by probabilistic latent semantic analysis.</title>
<date>2001</date>
<booktitle>Machine Learning,</booktitle>
<volume>42</volume>
<issue>1</issue>
<contexts>
<context position="10243" citStr="Hofmann, 2001" startWordPosition="1558" endWordPosition="1559">t al., 1998) extraction. In opposition to a multinomial mixture model, LDA considers that a theme is associated to each occurrence of a word composing the document, rather than associate a topic to the complete document. Therefore, a document can change topics from a word to another one. However, word occurrences are connected by a latent variable which controls the global match of the distribution of the topics in the document. These latent topics are characterized by a distribution of associated word probabilities. PLSA and LDA models have been shown to generally outperform LSA on IR tasks (Hofmann, 2001). Moreover, LDA provides a direct estimate of the relevance of a topic given a word set. In this paper, probabilities of hidden topic features, estimated with LDA, are considered for possibly capturing word dependencies expressing the semantic contents of a given conversation. Topic-based approaches involve defining a number of topics composing the topic space. The choice of the “right” number of topics is a crucial step, especially when the documents may contain multiple themes. Many studies have tried to find a relevant method to deal with this issue. (Arun et al., 2010) proposed to use a Si</context>
</contexts>
<marker>Hofmann, 2001</marker>
<rawString>Thomas Hofmann. 2001. Unsupervised learning by probabilistic latent semantic analysis. Machine Learning, 42(1):177–196.</rawString>
</citation>
<citation valid="true">
<authors>
<author>Patrick Kenny</author>
<author>Gilles Boulianne</author>
<author>Pierre Ouellet</author>
<author>Pierre Dumouchel</author>
</authors>
<title>Joint factor analysis versus eigenchannels in speaker recognition.</title>
<date>2007</date>
<journal>IEEE Transactions on Audio, Speech, and Language Processing,</journal>
<volume>15</volume>
<issue>4</issue>
<contexts>
<context position="19046" citStr="Kenny et al., 2007" startWordPosition="3060" endWordPosition="3063">sentation In this section, an i-vector-based method to represent automatic transcriptions is presented. Initially introduced for speaker recognition, ivectors (Kenny et al., 2008) have become very popular in the field of speech processing and recent publications show that they are also reliable for language recognition (Martınez et al., 2011) and speaker diarization (Franco-Pedroso et al., 2010). I-vectors are an elegant way of reducing the imput space dimensionality while retaining most of the relevant information. The technique was originally inspired by the Joint Factor Analysis framework (Kenny et al., 2007). Hence, i-vectors convey the speaker characteristics among other information such as transmission channel, acoustic environment or phonetic content of speech segments. The next sections describe the i-vector extraction process, the application of this compact representation to textual documents (called c-vector), and the vector transformation with the EFR method and the Mahalanobis metric. 4.1 Total variability space definition I-vector extraction could be seen as a probabilistic compression process that reduces the dimensionality of speech super-vectors according to a linearGaussian model. T</context>
</contexts>
<marker>Kenny, Boulianne, Ouellet, Dumouchel, 2007</marker>
<rawString>Patrick Kenny, Gilles Boulianne, Pierre Ouellet, and Pierre Dumouchel. 2007. Joint factor analysis versus eigenchannels in speaker recognition. IEEE Transactions on Audio, Speech, and Language Processing, 15(4):1435–1447.</rawString>
</citation>
<citation valid="true">
<authors>
<author>Patrick Kenny</author>
<author>Pierre Ouellet</author>
<author>Najim Dehak</author>
<author>Vishwa Gupta</author>
<author>Pierre Dumouchel</author>
</authors>
<title>A study of interspeaker variability in speaker verification.</title>
<date>2008</date>
<journal>IEEE Transactions on Audio, Speech, and Language Processing,</journal>
<volume>16</volume>
<issue>5</issue>
<contexts>
<context position="6670" citStr="Kenny et al., 2008" startWordPosition="996" endWordPosition="999">ference between the dialogue and the content of each class. In the same way, the relevant variability comes from the common content between the dialogue and the classes composing the topic space. We propose to reduce the noisy variability by using a factor analysis technique, which was initially developed in the domain of speaker identification. In this field, the factor analysis paradigm is used as a decomposition model that enables to separate the representation space into two subspaces containing respectively useful and useless information. The general Joint Factor Analysis (JFA) paradigm (Kenny et al., 2008) considers multiple variabilities that may be cross-dependent. Therefore, JFA representation allows us to compensate the variability within sessions of a same speaker. This representation is an extension of the GMM-UBM (Gaussian Mixture Model-Universal Background Model) models (Reynolds and Rose, 1995). (Dehak et al., 2011) extract a compact super-vector (called an i-vector) from the GMM super-vector. The aim of the compression process (i-vector extraction) is to represent the supervector variability in a low dimensional space. Although this compact representation is widely used in speaker rec</context>
<context position="18606" citStr="Kenny et al., 2008" startWordPosition="2990" endWordPosition="2993">infraction 001450381679389313 TOPI C n TO ervine Agent Customer 446 of size |V |for the rth topic space r of size n where the ith (0 ≤ i ≤ |V |) feature is: V wi d,r = P(wi|d) P(wi|zr j )P(zr j |d) zr Vwi × Vd j �−−→ −−→ � =V wi zr , V zr d where h·, ·i is the inner product, S being the frequency of the term wi in d, V wi zr j= P(wi|zj) and Vzr d = P(zj|d) evaluated using Gibbs Sampling j in the topic space r. 4 Compact multi-view representation In this section, an i-vector-based method to represent automatic transcriptions is presented. Initially introduced for speaker recognition, ivectors (Kenny et al., 2008) have become very popular in the field of speech processing and recent publications show that they are also reliable for language recognition (Martınez et al., 2011) and speaker diarization (Franco-Pedroso et al., 2010). I-vectors are an elegant way of reducing the imput space dimensionality while retaining most of the relevant information. The technique was originally inspired by the Joint Factor Analysis framework (Kenny et al., 2007). Hence, i-vectors convey the speaker characteristics among other information such as transmission channel, acoustic environment or phonetic content of speech s</context>
</contexts>
<marker>Kenny, Ouellet, Dehak, Gupta, Dumouchel, 2008</marker>
<rawString>Patrick Kenny, Pierre Ouellet, Najim Dehak, Vishwa Gupta, and Pierre Dumouchel. 2008. A study of interspeaker variability in speaker verification. IEEE Transactions on Audio, Speech, and Language Processing, 16(5):980–988.</rawString>
</citation>
<citation valid="true">
<authors>
<author>Wei Li</author>
<author>Andrew McCallum</author>
</authors>
<title>Pachinko allocation: Dag-structured mixture models of topic correlations.</title>
<date>2006</date>
<contexts>
<context position="12632" citStr="Li and McCallum, 2006" startWordPosition="1960" endWordPosition="1963">est solution. Another solution would be to consider a set of topic models to represent a document. Nonetheless, a multi-topic-based representation of a dialogue can involve a noisy variability due to the mapping of a dialogue in each topic space. Indeed, a dialogue does not share its content (i.e. words) with each class composing the topic space. Thus, a variability is added during the mapping process. Another weakness of the multi-view representation is the relation between classes in a topic space. (Blei and Lafferty, 2006) show that classes into a LDA topic space are correlated. Moreover, (Li and McCallum, 2006) consider a class as a node of an acyclic graph and as a distribution over other classes contained in the same topic space. 3 Multi-view representation of automatic dialogue transcriptions in a homogeneous space The purpose of the considered application is the identification of the major theme of a humanhuman telephone conversation in the customer 445 care service (CCS) of the RATP Paris transportation system. The approach considered in this paper focuses on modeling the variability between different dialogues expressing the same theme t. For this purpose, it is important to select relevant fe</context>
</contexts>
<marker>Li, McCallum, 2006</marker>
<rawString>Wei Li and Andrew McCallum. 2006. Pachinko allocation: Dag-structured mixture models of topic correlations.</rawString>
</citation>
<citation valid="true">
<authors>
<author>Georges Linar`es</author>
<author>Pascal Noc´era</author>
<author>Dominique Massonie</author>
<author>Driss Matrouf</author>
</authors>
<title>The lia speech recognition system: from 10xrt to 1xrt.</title>
<date>2007</date>
<booktitle>In Text, Speech and Dialogue,</booktitle>
<pages>302--308</pages>
<publisher>Springer.</publisher>
<marker>Linar`es, Noc´era, Massonie, Matrouf, 2007</marker>
<rawString>Georges Linar`es, Pascal Noc´era, Dominique Massonie, and Driss Matrouf. 2007. The lia speech recognition system: from 10xrt to 1xrt. In Text, Speech and Dialogue, pages 302–308. Springer.</rawString>
</citation>
<citation valid="true">
<authors>
<author>David Martınez</author>
<author>Oldrich Plchot</author>
<author>Luk´as Burget</author>
<author>Ondrej Glembek</author>
<author>Pavel Matejka</author>
</authors>
<title>Language recognition in ivectors space.</title>
<date>2011</date>
<journal>Interspeech,</journal>
<pages>861--864</pages>
<contexts>
<context position="18771" citStr="Martınez et al., 2011" startWordPosition="3018" endWordPosition="3021">d,r = P(wi|d) P(wi|zr j )P(zr j |d) zr Vwi × Vd j �−−→ −−→ � =V wi zr , V zr d where h·, ·i is the inner product, S being the frequency of the term wi in d, V wi zr j= P(wi|zj) and Vzr d = P(zj|d) evaluated using Gibbs Sampling j in the topic space r. 4 Compact multi-view representation In this section, an i-vector-based method to represent automatic transcriptions is presented. Initially introduced for speaker recognition, ivectors (Kenny et al., 2008) have become very popular in the field of speech processing and recent publications show that they are also reliable for language recognition (Martınez et al., 2011) and speaker diarization (Franco-Pedroso et al., 2010). I-vectors are an elegant way of reducing the imput space dimensionality while retaining most of the relevant information. The technique was originally inspired by the Joint Factor Analysis framework (Kenny et al., 2007). Hence, i-vectors convey the speaker characteristics among other information such as transmission channel, acoustic environment or phonetic content of speech segments. The next sections describe the i-vector extraction process, the application of this compact representation to textual documents (called c-vector), and the v</context>
</contexts>
<marker>Martınez, Plchot, Burget, Glembek, Matejka, 2011</marker>
<rawString>David Martınez, Oldrich Plchot, Luk´as Burget, Ondrej Glembek, and Pavel Matejka. 2011. Language recognition in ivectors space. Interspeech, pages 861–864.</rawString>
</citation>
<citation valid="true">
<authors>
<author>Driss Matrouf</author>
<author>Nicolas Scheffer</author>
<author>Benoit G B Fauve</author>
<author>Jean-Francois Bonastre</author>
</authors>
<title>A straightforward and efficient implementation of the factor analysis model for speaker verification. In Interspeech,</title>
<date>2007</date>
<pages>1242--1245</pages>
<contexts>
<context position="22762" citStr="Matrouf et al., 2007" startWordPosition="3689" endWordPosition="3692"> �B(d,r) = {T}t[g] · Σ−1 g∈UBM g · {X(d,r)}[g], (5) By using L(d,r) and B(d,r), x(d,r) can be obtained using the following equation: x(d,r) = L−1 (d,r) · B(d,r) (6) The matrix T can be estimated line by line, with {T}i[ being the ith line of {T}[g] then: g] Ti[g] = LU−1 g · RUig, (7) where RUig and LUg are given by: L−1 (d,r) + x(d,r)xt(d,r) · N(d,r)[9] {X(d,r)}[i] [g] · x(d,r) Algorithm 1 presents the method adopted to estimate the multi-view variability dialogue matrix with the above developments where the standard likelihood function can be used to assess the convergence. One can refer to (Matrouf et al., 2007) to find out more about the implementation of the factor analysis. C-vector representation suffers from 3 raised cvector issues: (i) the c-vectors x of equation 2 have to be theoretically distributed among the normal distribution N (0, I), (ii) the “radial” effect should be removed, and (iii) the full rank total factor space should be used to apply discriminant transformations. The next section presents a solution to these 3 problems. 4.3 C-vector standardization A solution to standardize c-vectors has been developed in (Bousquet et al., 2011). The authors proposed to apply transformations for</context>
</contexts>
<marker>Matrouf, Scheffer, Fauve, Bonastre, 2007</marker>
<rawString>Driss Matrouf, Nicolas Scheffer, Benoit G.B. Fauve, and Jean-Francois Bonastre. 2007. A straightforward and efficient implementation of the factor analysis model for speaker verification. In Interspeech, pages 1242–1245.</rawString>
</citation>
<citation valid="true">
<authors>
<author>Thomas Minka</author>
<author>John Lafferty</author>
</authors>
<title>Expectationpropagation for the generative aspect model.</title>
<date>2002</date>
<booktitle>In Proceedings of the Eighteenth conference on Uncertainty in artificial intelligence,</booktitle>
<pages>352--359</pages>
<publisher>Morgan Kaufmann Publishers Inc.</publisher>
<contexts>
<context position="14808" citStr="Minka and Lafferty, 2002" startWordPosition="2310" endWordPosition="2313">nd Steyvers, 2004) is not suitable for the proposed approach. LDA is used only for producing different feature sets used involved in statistical variability models. In order to estimate the parameters of different hidden spaces, a set of discriminative words V is constructed as described in (Morchid et al., 2014a). Each theme t contains a set of specific words. Note that the same word may appear in several discriminative word sets. All the selected words are then merged without repetition to form V . Several techniques, such as Variational Methods (Blei et al., 2003), Expectation-propagation (Minka and Lafferty, 2002) or Gibbs Sampling (Griffiths and Steyvers, 2004), have been proposed for estimating the parameters describing a LDA hidden space. Gibbs Sampling is a special case of Markov-chain Monte Carlo (MCMC) (Geman and Geman, 1984) and gives a simple algorithm for approximate inference in high-dimensional models such as LDA (Heinrich, 2005). This overcomes the difficulty to directly and exactly estimate parameters that maximize the likelihood of the whole data collection defined as: p(W |−→ α , →−β ) = 11w∈W p(−→w |−→ α , →−β ) for the whole data collection W knowing the Dirichlet parameters →−α and →−</context>
</contexts>
<marker>Minka, Lafferty, 2002</marker>
<rawString>Thomas Minka and John Lafferty. 2002. Expectationpropagation for the generative aspect model. In Proceedings of the Eighteenth conference on Uncertainty in artificial intelligence, pages 352–359. Morgan Kaufmann Publishers Inc.</rawString>
</citation>
<citation valid="true">
<authors>
<author>Mohamed Morchid</author>
<author>Georges Linar`es</author>
<author>Marc El-Beze</author>
<author>Renato De Mori</author>
</authors>
<title>Theme identification in telephone service conversations using quaternions of speech features.</title>
<date>2013</date>
<booktitle>In Interspeech. ISCA.</booktitle>
<marker>Morchid, Linar`es, El-Beze, De Mori, 2013</marker>
<rawString>Mohamed Morchid, Georges Linar`es, Marc El-Beze, and Renato De Mori. 2013. Theme identification in telephone service conversations using quaternions of speech features. In Interspeech. ISCA.</rawString>
</citation>
<citation valid="true">
<authors>
<author>Mohamed Morchid</author>
<author>Richard Dufour</author>
<author>Pierre-Michel Bousquet</author>
<author>Mohamed Bouallegue</author>
<author>Georges Linar`es</author>
<author>Renato De Mori</author>
</authors>
<title>Improving dialogue classification using a topic space representation and a gaussian classifier based on the decision rule.</title>
<date>2014</date>
<booktitle>In ICASSP.</booktitle>
<publisher>IEEE.</publisher>
<marker>Morchid, Dufour, Bousquet, Bouallegue, Linar`es, De Mori, 2014</marker>
<rawString>Mohamed Morchid, Richard Dufour, Pierre-Michel Bousquet, Mohamed Bouallegue, Georges Linar`es, and Renato De Mori. 2014a. Improving dialogue classification using a topic space representation and a gaussian classifier based on the decision rule. In ICASSP. IEEE.</rawString>
</citation>
<citation valid="true">
<authors>
<author>Mohamed Morchid</author>
<author>Richard Dufour</author>
<author>Georges Linar`es</author>
</authors>
<title>A LDA-Based Topic Classification Approach from Highly Imperfect Automatic Transcriptions.</title>
<date>2014</date>
<booktitle>In LREC.</booktitle>
<marker>Morchid, Dufour, Linar`es, 2014</marker>
<rawString>Mohamed Morchid, Richard Dufour, and Georges Linar`es. 2014b. A LDA-Based Topic Classification Approach from Highly Imperfect Automatic Transcriptions. In LREC.</rawString>
</citation>
<citation valid="true">
<authors>
<author>Douglas A Reynolds</author>
<author>Richard C Rose</author>
</authors>
<title>Robust text-independent speaker identification using gaussian mixture speaker models.</title>
<date>1995</date>
<journal>IEEE Transactions on Speech and Audio Processing,</journal>
<volume>3</volume>
<issue>1</issue>
<contexts>
<context position="6973" citStr="Reynolds and Rose, 1995" startWordPosition="1037" endWordPosition="1040">eveloped in the domain of speaker identification. In this field, the factor analysis paradigm is used as a decomposition model that enables to separate the representation space into two subspaces containing respectively useful and useless information. The general Joint Factor Analysis (JFA) paradigm (Kenny et al., 2008) considers multiple variabilities that may be cross-dependent. Therefore, JFA representation allows us to compensate the variability within sessions of a same speaker. This representation is an extension of the GMM-UBM (Gaussian Mixture Model-Universal Background Model) models (Reynolds and Rose, 1995). (Dehak et al., 2011) extract a compact super-vector (called an i-vector) from the GMM super-vector. The aim of the compression process (i-vector extraction) is to represent the supervector variability in a low dimensional space. Although this compact representation is widely used in speaker recognition systems, this method has not been used yet in the field of text classification. In this paper, we propose to apply factor analysis to compensate noisy variabilities due to the multiplication of LDA models. Furthermore, a normalization approach to condition dialogue representations (multi-model</context>
</contexts>
<marker>Reynolds, Rose, 1995</marker>
<rawString>Douglas A. Reynolds and Richard C. Rose. 1995. Robust text-independent speaker identification using gaussian mixture speaker models. IEEE Transactions on Speech and Audio Processing, 3(1):72–83.</rawString>
</citation>
<citation valid="true">
<authors>
<author>Gerard Salton</author>
</authors>
<title>Automatic text processing: the transformation. Analysis and Retrieval of Information by Computer.</title>
<date>1989</date>
<contexts>
<context position="9456" citStr="Salton, 1989" startWordPosition="1428" endWordPosition="1429">ompact representation and presents its application to text documents. Sections 5 and 6 report experiments and results. The last section concludes and proposes some perspectives. 2 Related work In the past, several approaches considered a text document as a mixture of latent topics. These methods, such as Latent Semantic Analysis (LSA) (Deerwester et al., 1990; Bellegarda, 1997), Probabilistic LSA (PLSA) (Hofmann, 1999) or Latent Dirichlet Allocation (LDA) (Blei et al., 2003), build a higher-level representation of the document in a topic space. ¿ Document is then considered as a bag-of-words (Salton, 1989) where the word order is not taken into account. These methods have demonstrated their performance on various tasks, such as sentence (Bellegarda, 2000) or keyword (Suzuki et al., 1998) extraction. In opposition to a multinomial mixture model, LDA considers that a theme is associated to each occurrence of a word composing the document, rather than associate a topic to the complete document. Therefore, a document can change topics from a word to another one. However, word occurrences are connected by a latent variable which controls the global match of the distribution of the topics in the docu</context>
</contexts>
<marker>Salton, 1989</marker>
<rawString>Gerard Salton. 1989. Automatic text processing: the transformation. Analysis and Retrieval of Information by Computer.</rawString>
</citation>
<citation valid="true">
<authors>
<author>Yoshimi Suzuki</author>
<author>Fumiyo Fukumoto</author>
<author>Yoshihiro Sekiguchi</author>
</authors>
<title>Keyword extraction using termdomain interdependence for dictation of radio news.</title>
<date>1998</date>
<booktitle>In 17th international conference on Computational linguistics,</booktitle>
<volume>2</volume>
<pages>1272--1276</pages>
<publisher>ACL.</publisher>
<contexts>
<context position="9641" citStr="Suzuki et al., 1998" startWordPosition="1457" endWordPosition="1460">2 Related work In the past, several approaches considered a text document as a mixture of latent topics. These methods, such as Latent Semantic Analysis (LSA) (Deerwester et al., 1990; Bellegarda, 1997), Probabilistic LSA (PLSA) (Hofmann, 1999) or Latent Dirichlet Allocation (LDA) (Blei et al., 2003), build a higher-level representation of the document in a topic space. ¿ Document is then considered as a bag-of-words (Salton, 1989) where the word order is not taken into account. These methods have demonstrated their performance on various tasks, such as sentence (Bellegarda, 2000) or keyword (Suzuki et al., 1998) extraction. In opposition to a multinomial mixture model, LDA considers that a theme is associated to each occurrence of a word composing the document, rather than associate a topic to the complete document. Therefore, a document can change topics from a word to another one. However, word occurrences are connected by a latent variable which controls the global match of the distribution of the topics in the document. These latent topics are characterized by a distribution of associated word probabilities. PLSA and LDA models have been shown to generally outperform LSA on IR tasks (Hofmann, 200</context>
</contexts>
<marker>Suzuki, Fukumoto, Sekiguchi, 1998</marker>
<rawString>Yoshimi Suzuki, Fumiyo Fukumoto, and Yoshihiro Sekiguchi. 1998. Keyword extraction using termdomain interdependence for dictation of radio news. In 17th international conference on Computational linguistics, volume 2, pages 1272–1276. ACL.</rawString>
</citation>
<citation valid="true">
<authors>
<author>Yee Whye Teh</author>
<author>Michael I Jordan</author>
<author>Matthew J Beal</author>
<author>David M Blei</author>
</authors>
<title>Sharing clusters among related groups: Hierarchical dirichlet processes.</title>
<date>2004</date>
<booktitle>In NIPS.</booktitle>
<contexts>
<context position="11319" citStr="Teh et al., 2004" startWordPosition="1734" endWordPosition="1737"> contain multiple themes. Many studies have tried to find a relevant method to deal with this issue. (Arun et al., 2010) proposed to use a Singular Value Decomposition (SVD) to represent the separability between the words contained in the vocabulary. Then, if the singular values of the topic-word matrix M equal the norm of the rows of M, this means that the vocabulary is well separated among the topics. This method has to be evaluated with the Kullback-Liebler divergence metric for each topic space. However, this process would be time consuming for thousands of representations of a dialogue. (Teh et al., 2004) proposed the Hierarchical Dirichlet Process (HDP) method to find the “right” number of topics by assuming that the data has a hierarchical structure. The HDP models were then compared to the LDA ones on the same dataset. (Zavitsanos et al., 2008) presented a method to learn the right depth of an ontology depending of the number of topics of LDA models. The study presented by (Cao et al., 2009) is quite similar to (Teh et al., 2004). The authors consider the average correlation between pairs of topics at each stage as the right number of topics. All these methods assume that a document can hav</context>
</contexts>
<marker>Teh, Jordan, Beal, Blei, 2004</marker>
<rawString>Yee Whye Teh, Michael I. Jordan, Matthew J. Beal, and David M. Blei. 2004. Sharing clusters among related groups: Hierarchical dirichlet processes. In NIPS.</rawString>
</citation>
<citation valid="true">
<authors>
<author>Eric P Xing</author>
<author>Michael I Jordan</author>
<author>Stuart Russell</author>
<author>Andrew Ng</author>
</authors>
<title>Distance metric learning with application to clustering with side-information. In Advances in neural information processing systems,</title>
<date>2002</date>
<pages>505--512</pages>
<contexts>
<context position="28846" citStr="Xing et al., 2002" startWordPosition="4845" endWordPosition="4848">ances) and Gaussian conditional density models are assumed, a new observation x from the test dataset can be assigned to the most likely theme CkBayes using the classifier based on the Bayes decision rule: CkBayes = arg mkax {N (x |xk, W)} = arg max�− (x − xk)t W−1 (x − xk) + akJ k where W is the within theme covariance matrix defined in eq. 11; N denotes the normal distribution and ak = log (P(Ck)). It is noted that, with these assumptions, the Bayesian approach is similar to Fisher’s geometric approach: x is assigned to the class of the nearest centroid, according to the Mahalanobis metric (Xing et al., 2002) of W−1: � CkBayes = arg max−1 ||x − xk ||2 − 1 + ak J k 6 Experiments and results The proposed c-vector approach is applied to the same classification task and corpus proposed in (Morchid et al., 2014a; Morchid et al., 2014b; Morchid et al., 2013) (state-of-the-art in text classification in (Morchid et al., 2014a)). Experiments are conducted using the multiple topic spaces estimated with an LDA approach. From these multiple topic spaces, a classical way is to find the one that reaches the best performance. Figure 4 presents the theme classification performance obtained on the development and </context>
</contexts>
<marker>Xing, Jordan, Russell, Ng, 2002</marker>
<rawString>Eric P. Xing, Michael I. Jordan, Stuart Russell, and Andrew Ng. 2002. Distance metric learning with application to clustering with side-information. In Advances in neural information processing systems, pages 505–512.</rawString>
</citation>
<citation valid="true">
<authors>
<author>Elias Zavitsanos</author>
<author>Sergios Petridis</author>
<author>Georgios Paliouras</author>
<author>George A Vouros</author>
</authors>
<title>Determining automatically the size of learned ontologies.</title>
<date>2008</date>
<booktitle>In ECAI,</booktitle>
<volume>178</volume>
<pages>775--776</pages>
<contexts>
<context position="11566" citStr="Zavitsanos et al., 2008" startWordPosition="1775" endWordPosition="1778">ocabulary. Then, if the singular values of the topic-word matrix M equal the norm of the rows of M, this means that the vocabulary is well separated among the topics. This method has to be evaluated with the Kullback-Liebler divergence metric for each topic space. However, this process would be time consuming for thousands of representations of a dialogue. (Teh et al., 2004) proposed the Hierarchical Dirichlet Process (HDP) method to find the “right” number of topics by assuming that the data has a hierarchical structure. The HDP models were then compared to the LDA ones on the same dataset. (Zavitsanos et al., 2008) presented a method to learn the right depth of an ontology depending of the number of topics of LDA models. The study presented by (Cao et al., 2009) is quite similar to (Teh et al., 2004). The authors consider the average correlation between pairs of topics at each stage as the right number of topics. All these methods assume that a document can have only one representation since they consider that finding the optimal topic model is the best solution. Another solution would be to consider a set of topic models to represent a document. Nonetheless, a multi-topic-based representation of a dial</context>
</contexts>
<marker>Zavitsanos, Petridis, Paliouras, Vouros, 2008</marker>
<rawString>Elias Zavitsanos, Sergios Petridis, Georgios Paliouras, and George A. Vouros. 2008. Determining automatically the size of learned ontologies. In ECAI, volume 178, pages 775–776.</rawString>
</citation>
</citationList>
</algorithm>
</algorithms>
<?xml version="1.0" encoding="UTF-8"?>
<algorithms version="110505">
<algorithm name="SectLabel" version="110505">
<variant no="0" confidence="0.000000">
<title confidence="0.50607">
ReferItGame: Referring to Objects in Photographs of Natural Scenes
</title>
<author confidence="0.975454">
Sahar Kazemzadeh&apos;* Vicente Ordonez&apos;* Mark Matten2 Tamara L. Berg&apos;
</author>
<affiliation confidence="0.9373385">
&apos;University of North Carolina at Chapel Hill, Chapel Hill, NC 27599, USA
2The Bishop’s School, San Diego, CA 92037, USA
</affiliation>
<email confidence="0.991779">
vicente@cs.unc.edu,tlberg@cs.unc.edu
</email>
<sectionHeader confidence="0.997343" genericHeader="abstract">
Abstract
</sectionHeader>
<bodyText confidence="0.999324588235294">
In this paper we introduce a new game
to crowd-source natural language referring
expressions. By designing a two player
game, we can both collect and verify refer-
ring expressions directly within the game.
To date, the game has produced a dataset
containing 130,525 expressions, referring
to 96,654 distinct objects, in 19,894 pho-
tographs of natural scenes. This dataset is
larger and more varied than previous REG
datasets and allows us to study referring
expressions in real-world scenes. We pro-
vide an in depth analysis of the resulting
dataset. Based on our findings, we design
a new optimization based model for gen-
erating referring expressions and perform
experimental evaluations on 3 test sets.
</bodyText>
<sectionHeader confidence="0.999517" genericHeader="keywords">
1 Introduction
</sectionHeader>
<bodyText confidence="0.989994032786885">
Much of everyday language and discourse con-
cerns the visual world around us, making under-
standing the relationship between objects in the
physical world and language describing those ob-
jects an important challenge problem for AI. From
robotics, to image search, to situated language
learning, and natural language grounding, there
are a number of research areas that would bene-
fit from a better understanding of how people refer
to physical entities in the world.
Recent advances in automatic computer vision
methods have started to make technologies for rec-
ognizing thousands of object categories a near re-
ality (Perronnin et al., 2012; Deng et al., 2012;
Deng et al., 2010; Krizhevsky et al., 2012). As a
result, there has been a spurt of recent work trying
to estimate higher level semantics, including ex-
citing efforts to automatically produce natural lan-
guage descriptions of images and video (Farhadi et
∗Indicates equal author contribution.
al., 2010; Kulkarni et al., 2011; Yang et al., 2011;
Ordonez et al., 2011; Kuznetsova et al., 2012;
Feng and Lapata, 2013). Common challenges en-
countered in these pursuits include the fact that
descriptions can be highly task dependent, open-
ended, and difficult to evaluate automatically.
Therefore, we look at the related, but more fo-
cused problem of referring expression generation
(REG). Previous work on REG has made signif-
icant progress toward understanding how people
generate expressions to refer to objects (a recent
survey of techniques is provided in Krahmer and
van Deemter (2012)). In this paper, we study the
relatively unexplored setting of how people refer
to objects in complex photographs of real-world
cluttered scenes. One initial stumbling block to
examining this scenario is lack of existing rele-
vant datasets, as previous collections for studying
REG have used relatively focused domains such
as graphics generated objects (van Deemter et al.,
2006; Viethen and Dale, 2008), crafts (Mitchell et
al., 2010), or small everyday (home and office) ob-
jects arrayed on a simple background (Mitchell et
al., 2013a; FitzGerald et al., 2013).
In this paper, we collect a new large-scale cor-
pus, currently containing 130,525 expressions, re-
ferring to 96,654 distinct objects, in 19,894 pho-
tographs of real world scenes. Some examples
from our dataset are shown in Figure 5. To con-
struct this corpus efficiently, we design a new two
player referring expression game (ReferItGame)
to crowd-source the data collection. Popular-
ized by efforts like the ESP game (von Ahn and
Dabbish, 2004) and Peekaboom (von Ahn et al.,
2006b), Human Computation based games can be
an effective way to engage users and collect large
amounts of data inexpensively. Two player games
can also automate verification of human provided
annotations.
Our resulting corpus is both more real-world
and much bigger than previous datasets, allowing
</bodyText>
<page confidence="0.952353">
787
</page>
<note confidence="0.910394">
Proceedings of the 2014 Conference on Empirical Methods in Natural Language Processing (EMNLP), pages 787–798,
October 25-29, 2014, Doha, Qatar. c�2014 Association for Computational Linguistics
</note>
<bodyText confidence="0.999272551724138">
us to examine referring expression generation in
a new setting at large scale. To understand and
quantify this new dataset, we perform an exten-
sive set of analyses. One significant difference
from previous work is that we study how refer-
ring expressions vary for different categories. We
find that an object’s category greatly influences the
types of attributes used in their referring expres-
sion (e.g. people use color words to describe cars
more often than mountains). Additionally, we find
that references to an object are sometimes made
with respect to other nearby objects, e.g. “the ball
to left of the man”. Interestingly, the types of ref-
erence objects (i.e. “the man”) used in referring
expressions is also biased toward some categories.
Finally, we find that the word used to refer to the
object category itself displays consistencies across
people. This notion is related to ideas of entry-
level categories from Psychology (Rosch, 1978).
Given these findings, we propose an optimiza-
tion model for generating referring expressions
that jointly selects which attributes to include in
the expression, and what attribute values to gener-
ate. This model incorporates both visual models
for selecting attribute-values and object category
specific priors. Experimental evaluations indicate
that our proposed model produces reasonable re-
sults for REG.
In summary, contributions of our paper include:
</bodyText>
<listItem confidence="0.952317583333333">
• A two player online game to collect and ver-
ify natural language referring expressions.
• A new large-scale dataset containing natural
language expressions referring to objects in
photographs of real world scenes.
• Analyses of the collected dataset, including
studying category-specific variations in refer-
ring expressions.
• An optimization based model to generate
referring expressions for objects in real-
world scenes with experimental evaluations
on three labeled test sets.
</listItem>
<bodyText confidence="0.999257666666667">
The rest of the paper is organized as follows.
First we outline related work from the vision and
language communities (§2). Then we describe our
online game for collecting referring expressions
(§3) and provide an analysis of our new Refer-
ItGame Dataset (§4). Finally, we present and eval-
uate our model for generating referring expres-
sions (§5) and discuss conclusions and future work
(§6).
</bodyText>
<sectionHeader confidence="0.999788" genericHeader="introduction">
2 Related Work
</sectionHeader>
<bodyText confidence="0.99831024">
Referring Expression Generation: There has
been a long history of research on understanding
how people generate referring expressions, dating
back to the 1970s (Winograd, 1972). One com-
mon approach is the Incremental Algorithm (Dale
and Reiter, 1995; Dale and Reiter, 2000) which
uses logical expressions for generation. Much
work in REG follows the Gricean maxims (Grice,
1975) which provide principles for how people
will behave in conversation.
Recently, there has been progress examining
other aspects of the referring expression prob-
lem such as understanding what types of attributes
are used (Mitchell et al., 2013a), modeling varia-
tions between speakers (Viethen and Dale, 2010;
Viethen et al., 2013; Van Deemter et al., 2012;
Mitchell et al., 2013b), incorporating visual classi-
fiers (Mitchell et al., 2011), producing algorithms
to refer to object sets (Ren et al., 2010; FitzGerald
et al., 2013), or examining impoverished percep-
tion REG (Fang et al., 2013). A good survey of
work in this area is provided in Krahmer and van
Deemter (2012). We build on past work, extending
models to generate attributes jointly in a category
specific framework.
Referring Expression Datasets: Some initial
datasets in REG used graphics engines to pro-
duce images of objects (van Deemter et al., 2006;
Viethen and Dale, 2008). Recently more realis-
tic datasets have been introduced, consisting of
craft objects like pipecleaners, ribbons, and feath-
ers (Mitchell et al., 2010), or everyday home
and office objects such as staplers, combs, or
rulers (Mitchell et al., 2013a), arrayed on a sim-
ple background. These datasets helped moved re-
ferring expression generation research into the do-
main of real world objects. We seek to further
these pursuits by constructing a dataset of natural
objects in photographs of the real world.
Image &amp; Video Description Generation: Re-
cent research on automatic image description has
followed two main directions. Retrieval based
methods (Aker and Gaizauskas, 2010; Farhadi et
al., 2010; Ordonez et al., 2011; Feng and Lap-
ata, 2010; Feng and Lapata, 2013) retrieve exist-
ing captions or phrases to describe a query image.
Bottom up methods (Kulkarni et al., 2011; Yang
et al., 2011; Yao et al., 2010) rely on visual classi-
fiers to first recognize image content and then con-
struct captions from scratch, perhaps with some
</bodyText>
<page confidence="0.997421">
788
</page>
<figureCaption confidence="0.989111">
Figure 1: An example game. Player 1 (left) sees an image with an object outlined in red (the man)
</figureCaption>
<bodyText confidence="0.975038035714286">
and provides a referring expression for the object (“man in red shirt on horse”). Player 2 (right) sees
the image and the expression from Player 1 and must localize the correct object by clicking on it (click
indicated by the red square). Elapsed time and current scores are also provided.
input from natural language statistics. Very re-
cently, these ideas have been extended to produce
descriptions for videos (Guadarrama et al., 2013;
Barbu et al., 2012). Like these methods, we gen-
erate descriptions for natural scenes, but focus on
referring to particular objects rather than provid-
ing an overall description of an image or video.
Human Computation Games: Games can be
a useful tool for collecting large amounts of la-
beled data quickly. Human Computation Games
were first introduced by Luis von Ahn in the ESP
game (von Ahn and Dabbish, 2004) for image la-
beling, and later extended to segment objects (von
Ahn et al., 2006b), collect common-sense knowl-
edge (von Ahn et al., 2006a), or disambiguate
words (Seemakurty et al., 2010). Recently, crowd
games have also been introduced into the com-
puter vision community for tasks like fine grained
category recognition (Deng et al., 2013). These
games can be released publicly on the web or
used on Mechanical Turk to enhance and encour-
age turker participation (Deng et al., 2013). In-
spired by the success of previous games, we cre-
ate a game to collect and verify natural language
expressions referring to objects in natural scenes.
</bodyText>
<sectionHeader confidence="0.9973365" genericHeader="method">
3 Referring Expression Game
(ReferItGame)
</sectionHeader>
<bodyText confidence="0.809155">
In this section we describe our referring expres-
sion game (ReferItGame∗), a simple two player
game where players alternate between generating
expressions referring to objects in images of nat-
ural scenes, and clicking on the locations of de-
scribed objects. An example game is shown in
Figure 1.
∗Available online at http://referitgame.com
</bodyText>
<subsectionHeader confidence="0.998573">
3.1 Game Play
</subsectionHeader>
<bodyText confidence="0.999670826086957">
Player 1: is shown an image with an object out-
lined in red and provided with a text box in which
to write a referring expression. Player 2: is shown
the same image and the referring expression writ-
ten by Player 1 and must click on the location of
the described object (note, Player 2 does not see
the object segmentation). If Player 2 clicks on
the correct object, then both players receive game
points and the Player 1 and Player 2 roles swap for
the next image. If Player 2 does not click on the
correct object then no points are received and the
players remain in their current roles.
This provides us with referring expressions for
our dataset and verification that the expressions
are valid since they led to correct object localiza-
tions. Expressions written for games where the
object was not correctly localized are kept and re-
leased with the dataset for future study, but are not
included in our final dataset analyses or statistics.
A game timer encourages players to write expres-
sions quickly, resulting in more natural expres-
sions. Also, IP addresses are filtered to prevent
people from simultaneously playing both roles.
</bodyText>
<subsectionHeader confidence="0.999733">
3.2 Playing Against the Computer
</subsectionHeader>
<bodyText confidence="0.998683727272727">
To promote engagement, we implement a single
player version of the game. When a player con-
nects, if there is another player online then the two
people are paired. If there are currently no other
available players, then the person plays a “canned”
game against the computer. If at any point another
person connects, the canned game ends and the
player is paired with the new person.
To implement canned games we seed the
game with 5000 pre-recorded referring expression
games (5 referring expressions and resulting clicks
</bodyText>
<page confidence="0.997204">
789
</page>
<bodyText confidence="0.999755375">
for each of 1000 objects) collected using Ama-
zon’s Mechanical Turk service. Implementing an
automated version of Player 1 is simple; we just
show the person one of the pre-collected referring
expressions and they click as usual.
Automating the role of Player 2 is a bit more
complicated. In this case, we compare the per-
son’s written expression against the pre-recorded
expressions for the same object. For this compar-
ison we use a parser to lemmatize the words in an
expression and then compute cosine similarity be-
tween expressions with a bag of words representa-
tion. Based on this measure the closest matching
expression is determined. If there is no similarity
between the newly generated expression and the
canned expressions, the expression is deemed in-
correct and a random click location (outside of the
object) is generated. If there is a successful match
with a previously generated expression, then the
canned click from the most similar pre-recorded
game is used. More complex similarities could be
used, but since we require real-time performance
in our game setting we use this simple implemen-
tation which works well for our expressions.
</bodyText>
<sectionHeader confidence="0.99509" genericHeader="method">
4 ReferItGame Dataset
</sectionHeader>
<bodyText confidence="0.999939666666667">
In this section we describe the ReferItGame
dataset†, including images and labels, processing
the dataset, and analysis of the collection.
</bodyText>
<subsectionHeader confidence="0.996785">
4.1 Images and Labels
</subsectionHeader>
<bodyText confidence="0.999992176470588">
We build our dataset of referring expressions
on top of the ImageCLEF IAPR image retrieval
dataset (Grubinger et al., 2006). This dataset is
a collection of 20,000 images available free of
charge without copyright restrictions, depicting a
variety of aspects of everyday life, from sports,
to animals, to cities, and landscapes. Crucial for
our purposes, the SAIAPR TC-12 expansion (Es-
calante et al., 2010) includes segmentations of
each image into regions indicating the locations of
constituent objects. 238 different object categories
are labeled, including animals, people, buildings,
objects, and background elements like grass or
sky. This provides us with information regarding
object category, object location, and object size, as
well as the location and categories of other objects
present in the same image.
</bodyText>
<footnote confidence="0.673539">
†Available at http://tamaraberg.com/referitgame
</footnote>
<subsectionHeader confidence="0.997842">
4.2 Collecting the Dataset
</subsectionHeader>
<bodyText confidence="0.999981351351351">
From the ImageCLEF dataset, we created a total
of over 100k distinct games (one per object labeled
in the dataset). For the games we imposed an or-
dering to allow for collecting the most interesting
expressions first. Initially we prioritized games
for objects in images with multiple objects of the
same category. Once these games were completed,
we prioritized ordering based on object category to
include a comprehensive range of objects. Finally,
after successfully collecting referring expressions
from the prioritized games, we posted games for
the remaining objects. In order to evaluate consis-
tency of expression generation across people, we
also include a probability of repeating previously
played games during collection.
To date, we have collected 130,525 successfully
completed games. This includes 10,431 canned
games (a person playing against the computer, not
including the initial seed set) and 120,094 real
games (two people playing). 96,654 distinct ob-
jects from 19,984 photographs are represented in
the dataset. This covers almost all of the objects
present in the IAPR corpus. The remaining ob-
jects from the collection were either too small or
too ambiguous to result in successful games.
For data collection, we posted the game online
for anyone on the web to play and encouraged par-
ticipation through social media and the survey sec-
tion of reddit. In this manner we collected over
4 thousand referring expressions over a period of
3 weeks. To speed up data collection, we also
posted the game on Mechanical Turk. Turkers
were paid upon completion of 10 correct games
(games where Player 2 clicks on the correct object
of interest). Turkers were pre-screened to have ap-
proval ratings above 80% and to be located in the
US for language consistency.
</bodyText>
<subsectionHeader confidence="0.999928">
4.3 Processing the Dataset
</subsectionHeader>
<bodyText confidence="0.999985272727273">
Because of the size of the dataset, hand annotation
of all referring expressions is prohibitive. There-
fore, similar to past work (FitzGerald et al., 2013),
we design an automatic method to pre-process the
expressions and extract object and attribute men-
tions. These automatically processed expressions
are used only for analysis and model training. We
also fully hand label portions of the dataset for
evaluation (§5.2).
By examining the expressions in the collected
dataset, we define a set of attributes with broad
</bodyText>
<page confidence="0.964382">
790
</page>
<figure confidence="0.738059833333333">
S ::=subject word
color word&apos; ::= rel(S, color word)color word &apos;=color word |
prep in(S, color word)color word &apos;=color word
size word&apos; ::= rel(S, size word)size word &apos;=size word
abs loc word&apos; ::= rel(S, abs loc word) abs loc word &apos;=abs loc word|
prep on(S, orientation word) ∧ ¬prep of(S, )abs loc word&apos;=on+orientation word
rel loc word&apos; ::= RL
RL ::= prep rel loc word(S, object word)RL=rel loc word |
prep on(S, orientation word) ∧ prep of(S, object word) RL=on orientation word|
prep to(S, orientation word) ∧ prep of(S, object word) RL=to orientation word|
prep at(S, orientation word) ∧ prep of(S, object word) RL=at orientation word
generic word&apos; ::= amod(S, generic word)
</figure>
<figureCaption confidence="0.998305">
Figure 2: Templates for parsing attributes from referring expressions (§4.3).
</figureCaption>
<bodyText confidence="0.999513">
coverage of the attribute types used in the re-
ferring expressions. We define the set of at-
tributes for a referring expression as a 7-tuple R =
{r1, r2, r3, r4, r5, r6, r7}:
</bodyText>
<listItem confidence="0.998906857142857">
• r1 is an entry-level category attribute,
• r2 is a color attribute,
• r3 is a size attribute,
• r4 is an absolute location attribute,
• r5 is a relative location relation attribute,
• r6 is a relative location object attribute,
• r7 is a generic attribute,
</listItem>
<bodyText confidence="0.996446913793104">
Color and size attributes refer to the object color
(e.g. “blue”) and object size (e.g. “tiny”) respec-
tively. Absolute location refers to the location of
the object in the image (e.g. “top of the image”).
Relative location relation and relative location ob-
ject attributes allow for referring expressions that
localize the object with respect to another object
in the picture (e.g. “the car to the left of the tree”).
Generic attributes cover all less frequently ob-
served attribute types (e.g. “wooden” or “round”).
The entry-level category attribute is related to
the concept of entry-level categories first proposed
by Psychologists in the 1970s (Rosch, 1978) and
recently explored in visual recognition (Ordonez
et al., 2013). The idea of entry-level categories is
that an object can belong to many different cate-
gories; an indigo bunting is an oscine, a bird, a
vertebrate, a chordate, and so on. But, a person
looking at a picture of one would probably call it
a bird (unless they are very familiar with ornithol-
ogy). Therefore, we include this attribute to cap-
ture how people name object categories in refer-
ring expressions.
Parsing the referring expressions: We parse
the expressions using the most recent version
of the StanfordCoreNLP parser (Socher et al.,
2013). We begin by traversing the parse tree in a
breadth-first manner and selecting the head noun
of the sentence to determine the object of the
referring expression, denoted as subject word.
We pre-define a dictionary of attribute-values
(color word, size word, abs location word,
rel location word) for each of the attributes
based on the observed data using a combination
of POS-tagging and manual labeling.
We then apply a template-based approach on the
collapsed dependency relations to recover the set
of attributes (the main template rules are shown
in Figure 2). The relationship rel indicates any
linguistic binary relationship between the subject
word S and another word, including the amod re-
lationship. Orientation word captures the words
like left, right, top and bottom. For generic word
we consider any modifier words other than those
captured by our other attributes (color, size, loca-
tion).
Using this template-based parser we can
for instance parse the following expression:
“Red flower on top of pedestal”. The first
rule would match the prep(S, color word)
relation, effectively recovering the attribute
color word&apos; as “red”. The second rule would
match the prep on(S, orientation word) n
prep of(S, object word) relations, recovering
rel loc word&apos; as “on top of ” and object word
as “pedestal”.
The accuracy of our parser based processing is
91%. This was evaluated on 4,500 expressions
</bodyText>
<page confidence="0.976605">
791
</page>
<figure confidence="0.999349826446281">
Plot A: Attribute Breakdown by Category Plot B: Most Frequently Used Relative
0 2000 4000 6000 Objects
0 200 400 600
None Color Size Absolute Location Relative Location Other
sky-blue
man
group-of-persons
ground
rock
cloud
grass
woman
trees
mountain
vegetation
wall
sky
window
building
ocean
sky-light
tree
car
person
house
floor
couple-of-persons
face-of-person
plant
hat
street
hill
fabric
bed
bottle
lamp
sand-beach
chair
door
child-boy
painting
palm
river
bicycle
people
shirt
side
guy
head
tree
man
corner
background
building
wall
woman
table
bed
water
hat
girl
mountain
person
sign
car
pic
lady
window
sky
foot
boat
foreground
jacket
ground
hand
bike
grou loud
p
horse
grass
rock
house
Plot C: Number of Attributes Per
Expression
Plot D: Object Locations for Abs-Loc
Attributes
41%
1
9%
2
50%
0
1
0.9
0.8
0.7
0.6
0.5
0.4
0.3
0.2
0.1
0
0 0.2 0.4 0.6 0.8 1
Right Left Bottom Middle Top Front
Plot E: Object Area vs Indicated Size Plot F: Attribute Use Frequencies
None Small Big Tall Little Tiny Large Short Huge Long
Overall Single Multiple
0.35
0.3
0.25
0.2
0.15
0.1
0.05
0
None
Size
Relative
Location
0 0.1 0.2 0.3 0.4 0.5 0.6
</figure>
<figureCaption confidence="0.999717">
Figure 3: Analyses of the ReferItGame Dataset. Plot A shows frequency and attribute occurrence for
</figureCaption>
<bodyText confidence="0.4718754">
common object categories. Plot B shows objects frequently used as reference points, ie “to the left of the
man”. Plot C shows frequencies of using 0, 1 or 2 attributes within the same expression. Plot D shows
object locations vs location words used. Plot E shows normalized object size vs size words used (bars
show lst through 3rd quartiles). Plot F shows the frequency of usage of each attribute type for images
containing either a single instance of the object category or multiple instances of the category.
</bodyText>
<page confidence="0.877415">
792
</page>
<figure confidence="0.999303272727273">
Color
Beige
Red
Yellow
Blue
Black
People
Man
Woman
Car Bottle Street
Objects
</figure>
<figureCaption confidence="0.7836514">
Figure 4: Left: Tag clouds showing entry-Level category words used in referring expressions to name
various object categories, with word size indicating frequency. For example, this indicates that “streets”
are often called “road”, sometimes “ground”, sometimes “roadway”, etc. Right: example objects pre-
dicted to portray some of our color attribute values. Note sometimes our color predictor is quite accurate,
and sometimes it makes mistakes (see the man in a red shirt predicted as “yellow”).
</figureCaption>
<bodyText confidence="0.875242">
that were manually parsed by a human annotator.
</bodyText>
<subsectionHeader confidence="0.98621">
4.4 Dataset Analysis
</subsectionHeader>
<bodyText confidence="0.99998636">
In the resulting dataset, we have a range of cov-
erage over objects. For 10,304 of the objects we
have 2 or more referring expressions while for the
rest of the objects we have collected only one ex-
pression. This creates a dataset that emphasizes
breadth while also containing enough data to study
speaker variation.
Multiple attribute analyses are provided in Fig-
ure 3. We find that most expressions use 0, 1, or
2 attributes (in addition to the entry-level attribute
object word), with very few expressions contain-
ing more than 2 attributes (frequencies are shown
in Fig 3c). We also examine what types of at-
tributes are used most frequently, according to ob-
ject category in Fig 3a, and when associated with
single or multiple occurrences of the same object
category in an image in Fig 3f. The frequency
of attribute usage in images containing multiple
objects of the same type increases for all types,
compared to single object occurrences. Perhaps
more interestingly, the use of different attributes is
highly category dependent. People use more at-
tribute words overall to describe some categories,
like “man”, “woman”, or “plant”, and the distribu-
tion of attribute types also varies by category. For
example, color attributes are used more frequently
for categories like “car” or “woman” than for cat-
egories like “sky” or “rock”.
We also examine which objects are most fre-
quently used as points of reference, e.g.,“the chair
next to the man” in Fig 3b. We observe that peo-
ple and some background categories like “tree” or
“wall” are often used to help localize objects in
referring expressions. Additionally, we provide
plots showing the relationship between object lo-
cation in the image and use of absolute location
words, Fig 3d, as well as size words vs object area,
Fig 3e.
Finally, we study entry-level category attribute-
values to understand how people name objects in
referring expressions. Tag clouds indicating the
frequencies of words used to name various ob-
ject categories are provided in Fig 4 (left). Ob-
jects like “street” are usually referred to as “road”,
but sometimes they are called “ground”, “road-
way”, etc. “Bottles” are usually called “bottle”,
but sometimes referred to as “coke” or “beer”. In-
terestingly, “man” is usually called “man” while
“woman” is most often called “person” in the re-
ferring expressions.
</bodyText>
<sectionHeader confidence="0.992638" genericHeader="method">
5 Generating Referring Expressions
</sectionHeader>
<bodyText confidence="0.999468">
In this section we describe our proposed genera-
tion model and provide experimental evaluations
on three test sets.
</bodyText>
<subsectionHeader confidence="0.826845">
5.1 Generation Model
</subsectionHeader>
<bodyText confidence="0.99820275">
Given an input tuple I = {P, 5}, where P is a
target object and 5 is a scene (image containing
multiple objects), our goal is to generate an output
referring expression, R. For instance, the repre-
sentation R for the referring expression: The big
old white cabin beside the tree would be R =
{cabin, white, big, ∅, beside, tree, old}.
To generate referring expressions we construct
vocabularies Vrz with candidate values for each at-
tribute rz E R, where attribute vocabulary Vrz con-
tains the set of words observed in our parsed refer-
ring expressions for attribute rz plus an additional
</bodyText>
<page confidence="0.989735">
793
</page>
<figure confidence="0.907861406779661">
picture on the wall
picture
picture
Door
white door middle
white door
white shirt man
white shirt on right
man on right
Baseline:[picture, white, ,
right, , , ]
Full: [picture, , , , prep_on,
wall, ]
Baseline:[door, white, ,
right, , , ]
Full:[door, white, , right, , , ]
Baseline:[window, white, ,
right, , , ]
Full:[window, brown, , right,
, , ]
Baseline:[man, white, , right,
, , ]
Full:[man, white, , right, , , ]
Baseline:[building, white, ,
right, , , ]
Full:[building, white, , right, ,
, ]
big gated window on right of
white section
black big window right
brown railings on right
building on right behind guys
blue right building
building on right
picture Baseline:[picture, white, , right, , ,
santa ]
the santa picture Full:[picture, , , , prep_on, plant, ]
right doorway Baseline:[door, , , right, prep_on,
right brown door person, ]
right door Full:[door, , , right, prep_above,
person, ]
with flag
window top 2nd left
2nd window top left
red guy left sitting
left bottom guy
red shirt lef
Baseline:[window, , , right,
prep_on, person, ]
Full:[window, , , left, prep_above,
door, ]
Baseline:[man, , , right, prep_on,
wall, ]
Full:[man, , , left, prep_in, woman,
]
Image Human Expressions Generated Expressions Image Human Expressions Generated Expressions
buildings Baseline:[building, white, , right, , ,
buildings ]
buildings Full:[building, brown, , middle, , ,
</figure>
<figureCaption confidence="0.994832">
Figure 5: Example results, including human generated expressions, baseline and full model generated
</figureCaption>
<bodyText confidence="0.900835866666667">
expressions. For some images the model does well at mimicking human expressions (left). For others it
does not generate the correct attributes (right).
ε value indicating that the attribute should be om-
mited from the referring expression entirely.
In this way, our framework can jointly deter-
mine which attributes to include in the expression
(e.g.,“size” and “color”) and what attribute values
to generate (e.g.,“small” and “blue”) from the list
of all possible values. We enforce a constraint to
always include an “entry-level category” attribute
(e.g. “boy”) so that we always generate a word
referring to the object.
We pose our problem as an optimization where
we map a tuple {P, S1 to a referring expression
R* as:
</bodyText>
<equation confidence="0.977309333333333">
R* = argmax E(R, P, S)
R (1)
s. t. fi(R) G bi
</equation>
<bodyText confidence="0.833512">
Where the objective function E is decomposed as:
</bodyText>
<equation confidence="0.981650125">
6
E(R, P, S) = α 1: φi(ri, P, S)
i=2
7
ψi(ri, type(P)) (2)
i=1
+1: ψi,j(ri, rj)
i&gt;j
</equation>
<bodyText confidence="0.999648421052632">
Where φi is the compatibility function between an
attribute-value for ri and the properties of the ob-
served scene S and object P (described in §5.1.1).
The terms ψi and ψi,j are unary and pairwise pri-
ors computed based on observed co-occurrence
statistics of attribute-values for ri with categories
(where type(P) denotes the type or category of an
object) and between pairs of attribute-values (de-
scribed in §5.1.2). Attributes r1 and r7 are mod-
eled only in the priors since we do not have visual
models for these attributes.
The constraints fi(R) G bi are restricted to be
linear constraints and are used to impose hard con-
straints on the solution. The first such constraint is
used to control the verbosity (length) of the gener-
ated referring expression using a constraint func-
tion that imposes a minimum attribute length re-
quirement by restricting the number of entries ri
that can take value ε in the solution.
</bodyText>
<equation confidence="0.976594">
1: 1[ri = ε] G 7 − γ(P, S) (3)
i
</equation>
<bodyText confidence="0.9997225">
Where 1[.] is the indicator function and γ(P, S) is
a term that allows us to change the length require-
ment based on the object and scene (so that images
with a larger number of objects of the same type
have a larger length requirement).
Finally we add hard constraints such that r5 = ε
`#=� r6 = ε, so that relative location and relative
object attributes are produced together.
</bodyText>
<subsubsectionHeader confidence="0.539822">
5.1.1 Content-based potentials
</subsubsectionHeader>
<bodyText confidence="0.971905555555556">
Potentials φi are defined for attributes r2 to r6.
Attribute r7 represents a variety of different at-
tributes, e.g. material or shape attributes, but
we lack sufficient data to train visual models for
these infrequent attribute terms. Therefore, we
model these attributes using only prior statistics-
based potentials (§5.1.2). Visual recognition mod-
els for recognizing entry-level object categories
+ β
</bodyText>
<page confidence="0.993477">
794
</page>
<bodyText confidence="0.986082">
in the scene S of the same category type as the
object P.
could also be incorporated for modeling r1, but we
leave this as future work.
</bodyText>
<equation confidence="0.633399">
Color attribute:
φ2(r2 = ck, P, S) = sim(histck, hist(P))
</equation>
<bodyText confidence="0.9972874">
Where hist(P) is the HSV color histogram of the
object P. We compute similarity sim using cosine
similarity, and histck is the mean histogram of all
objects in our training data that were referred to
with color attribute-value ck E Vr2.
</bodyText>
<subsectionHeader confidence="0.514458">
Size attribute:
</subsectionHeader>
<equation confidence="0.986358333333333">
φ3(r3 = sk, P, S) =
1 e
σsk �2π
</equation>
<bodyText confidence="0.9955595">
Where size(P) is the size of object P normalized
by image size. We model the probabilities of each
size word sk E Vr3 as a Gaussian learned on our
training set.
</bodyText>
<sectionHeader confidence="0.510835" genericHeader="method">
Absolute-location attribute:
</sectionHeader>
<equation confidence="0.99013">
φ6(r6 = ok, P, S) =
1[ok E objectsnear(location(P), S)]
</equation>
<bodyText confidence="0.99817225">
The above expression filters out potential relative
objects ok E Vrs that are not located in sufficient
proximity to object P or are not present in the im-
age at all.
</bodyText>
<subsubsectionHeader confidence="0.671481">
5.1.2 Prior statistics-based potentials
</subsubsectionHeader>
<bodyText confidence="0.9834622">
Prior statistics-based potentials are modeled for all
of the attributes r1 - r7. Note that these potentials
do not depend on specific attribute-values but only
on the given object category type(P).
Unary prior potentials ψi are defined as:
</bodyText>
<equation confidence="0.99164645">
1[(r(j)
i =� E) n (type(P(j)) = type(P))]
+
1[type(P(j)) = type(P)]
−(size(P)−µsk)2/2σ2 sk
ψi(ri, type(P)) =
 |D |
E
j=1
 |D |
E
j=1
1[r(j) i=,� 6]
|D |+ λ
 |D |
E
j=1
φ4(r4 = ak, P, S) =
1 e 2 (loc(P)−µak )T Σak
V/ (2π)n|Eak 1 −1(loc(P)−µak )
</equation>
<bodyText confidence="0.9847608">
Where loc(P) are the 2-dimensional coordi-
nates of the object P normalized to be E [0 − 1].
Parameters µak and Eak are estimated from
training data for each absolute location word
ak E Vr4.
</bodyText>
<sectionHeader confidence="0.36412" genericHeader="evaluation">
Relative-location and Relative object:
</sectionHeader>
<equation confidence="0.994225">
φ5(r5 = lk, P, S) =
1[lk = ε] · g(count(type(P), S))
</equation>
<bodyText confidence="0.999823315789474">
If there are a larger number of objects of the same
type in the image we find that the probability of us-
ing a relative-location-object increases (e.g., “the
car to the right of the man”). For images where P
was the only object of that category type, the prob-
ability of using a relative-location-object is 0.12.
This increases to 0.22 when there were two ob-
jects of the same type and further increases to 0.26
for additional objects of the same type. There-
fore, we model the probability of selecting rela-
tive location value lk E Vr5 as a function g, where
count(type(P), S) counts the number of objects
Where D = {P(j), S(j), R(j)} is our training
dataset and λ is a small additive smoothing term.
The two terms in the above expression represent
category-specific counts and global counts of the
number of times a given attribute ri was output in
a referring expression in training data. Pairwise
prior potentials ψi,j are defined as:
</bodyText>
<equation confidence="0.946106933333333">
ψ(1)
i,j (ri, rj)+ ψ(2)
5,6(r5, r6)
� (
1) ( ) C +1 if ri = rj = ε
i,jri , rj λ
o.w.
ψ(2)
5,6(r5 = a, r6 = b) =
 |D  |1[(r(t)
5 = a) n (r(t) 6= b)]
|D|
. The pairwise
|D|
potential ψ(1)
</equation>
<bodyText confidence="0.939644">
i,j captures the pairwise statistics of
how frequently people use pairs of attribute types.
</bodyText>
<figure confidence="0.9830433">
E �ψi,j(ri,rj) =
i&lt;j i&lt;j
E
t=1
where C =
1[(r(t)
i �=�)n (r(t)
j �=�)]
|D|E
t=1
</figure>
<page confidence="0.98954">
795
</page>
<table confidence="0.999265285714286">
SOURCE PREC(%) RECALL(%)
Baseline - A 27.92 43.27
Full Model - A 36.28 53.44
Baseline - B 29.87 50.57
Full Model - B 36.68 59.80
Baseline - C 28.85 37.41
Full Model - C 37.73 48.54
</table>
<tableCaption confidence="0.9767755">
Table 1: Baseline Model &amp; Full Model perfor-
mance on the three test sets (A,B,C).
</tableCaption>
<bodyText confidence="0.997865666666667">
For instance how frequently people use both color
and size attributes to refer to an object. The pair-
wise potential ψ(2)
i,j produces a cohesion score be-
tween relative-location words and relative-object
words based on global dataset statistics.
</bodyText>
<subsectionHeader confidence="0.922907">
5.2 Experiments
</subsectionHeader>
<bodyText confidence="0.999971969230769">
We implement the proposed model using commer-
cial binary integer linear programming software
(IBM ILOG CPLEX). This requires introducing
a set of indicator variables for each of our multi-
valued attributes and another set of indicator vari-
ables to model pairwise interactions between our
variables, as well as incorporating additional con-
sistency constraints between variables. Model pa-
rameters (α and β) are tuned on data randomly
sampled from the training set.
Test Sets: We evaluate our model on three test
sets, each containing 500 objects. For each ob-
ject in the test sets we collect 3 referring expres-
sions using the ReferItGame and manually label
the attributes mentioned in each expression. We
find human agreement to be 72.31% on our dataset
(where we measure agreement as mean match-
ing accuracy of attribute values for pairs of users
across images in our test sets). The three test
sets are created to evaluate different aspects of our
data.
Test Set A contains objects sampled randomly
from the entire dataset. This test set is meant to
closely resemble the full dataset distribution. The
goal of the other two test sets is to sample expres-
sions for “interesting” objects. We first identify
categories that are mainly related to background
content elements, e.g. “sky, ground, floor, sand,
sidewalk, etc”. We consider these categories to
be potentially less interesting for study than cat-
egories like people, animals, cars, etc. Test Set B
contains objects sampled from the most frequently
occurring object categories in the dataset, selected
to contain a balanced number of objects from each
category, excluding the less interesting categories.
Test Set C contains objects sampled from images
that contain at least 2 objects of the same category,
excluding the less interesting categories.
Results: Qualitative examples are shown in Fig 5
comparing our results to the human produced ex-
pressions. For some images (left) we do quite well
at predicting the correct attributes and values. For
others we do less well (right). We also show exam-
ple objects predicted for some color words in Fig 4
(right). We see that our model can fail in several
ways, such as generating the wrong attribute-value
due to inaccurate predictions by visual models or
selecting incorrect attributes to include in the gen-
erated expression.
Quantitative results: precision and recall mea-
sures for the 3 test sets are reported in Table 1,
including evaluation of a baseline version of our
model which incorporates only the prior potentials
(§5.1.2) without any content based estimates. We
see that our model performs reasonably on both
measures, and outperforms the baseline by a large
margin on all test sets, with highest performance
on the broadly sampled interesting category test
set. Note that our problem is somewhat differ-
ent than traditional REG where the input is often
attribute-value pairs and the task is to select which
pairs to include in the expression. Our goal is to
jointly select which attributes to include and what
values to predict from a list of all possible values
for the attribute.
</bodyText>
<sectionHeader confidence="0.999505" genericHeader="conclusions">
6 Conclusions &amp; Future Work
</sectionHeader>
<bodyText confidence="0.9986349">
In this paper we have introduced a new game to
crowd-source referring expressions for objects in
natural scenes. We have used this game to pro-
duce a new large-scale dataset with analysis. We
have also proposed an optimization based model
for REG and performed experimental evaluations.
Future work includes developing fully automatic
visual recognition methods for REG in real world
scenes, and incorporating linguistically inspired
models for entry-level category prediction.
</bodyText>
<sectionHeader confidence="0.998816" genericHeader="acknowledgments">
Acknowledgments
</sectionHeader>
<bodyText confidence="0.9726016">
This work was funded by NSF Awards #1417991
and #1444234. M.M. was supported by the Stony
Brook Simons Summer Research Program for
High School students. We also thank Alex Berg
for many helpful discussions.
</bodyText>
<page confidence="0.997275">
796
</page>
<sectionHeader confidence="0.99607" genericHeader="references">
References
</sectionHeader>
<reference confidence="0.999129009433963">
Ahmet Aker and Robert Gaizauskas. 2010. Generating
image descriptions using dependency relational pat-
terns. In Association for Computational Linguistics
(ACL).
Andrei Barbu, Alexander Bridge, Zachary Burchill,
Dan Coroian, Sven J. Dickinson, Sanja Fi-
dler, Aaron Michaux, Sam Mussman, Siddharth
Narayanaswamy, Dhaval Salvi, Lara Schmidt,
Jiangnan Shangguan, Jeffrey Mark Siskind, Jar-
rell W. Waggoner, Song Wang, Jinlian Wei, Yifan
Yin, and Zhiqi Zhang. 2012. Video in sentences
out. In Uncertainty in Artificial Intelligence (UAI).
Robert Dale and Ehud Reiter. 1995. Computational
interpretations of the gricean maxims in the gener-
ation of referring expressions. Cognitive Science
(CogSci), 19:233264.
Robert Dale and Ehud Reiter. 2000. Building natural
language generation systems. In Cambridge Univer-
sity Press.
Jia Deng, Alexander C. Berg, Kai Li, and Fei-Fei Li.
2010. What does classifying more than 10,000 im-
age categories tell us? In European Conference on
Computer Vision (ECCV).
Jia Deng, Alex Berg, Sanjeev Satheesh, Hao Su, Aditya
Khosla, and Fei-Fei Li. 2012. Large scale vi-
sual recognition challenge. In http://www.image-
net.org/challenges/LSVRC/2012/index.
Jia Deng, Jonathan Krause, and Li Fei-Fei. 2013. Fine-
grained crowdsourcing for fine-grained recognition.
In IEEE Conference on Computer Vision and Pattern
Recognition (CVPR).
Hugo Jair Escalante, Carlos A. Hernandez, Jesus A.
Gonzalez, A. Lopez-Lopez, Manuel Montes, Ed-
uardo F. Morales, L. Enrique Sucar, Luis Villasenor,
and Michael Grubinger. 2010. The segmented and
annotated iapr tc-12 benchmark. Computer Vision
and Image Understanding (CVIU).
Rui Fang, Changsong Liu, Lanbo She, and Joyce Chai.
2013. Towards situated dialogue: Revisiting refer-
ring expression generation. In Empirical Methods
on Natural Language Processing (EMNLP).
Ali Farhadi, Mohsen Hejrati, Mohammad Amin
Sadeghi, Peter Young, Cyrus Rashtchian, Julia
Hockenmaier, and David Forsyth. 2010. Every
picture tells a story: generating sentences for im-
ages. In European Conference on Computer Vision
(ECCV).
Yansong Feng and Mirella Lapata. 2010. How many
words is a picture worth? automatic caption genera-
tion for news images. In Association for Computa-
tional Linguistics (ACL).
Yansong Feng and Mirella Lapata. 2013. Automatic
caption generation for news images. IEEE Transac-
tions on Pattern Analysis and Machine Intelligence,
35(4):797–812.
Nicholas FitzGerald, Yoav Artzi, and Luke Zettle-
moyer. 2013. Learning distributions over logical
forms for referring expression generation. In Em-
pirical Methods on Natural Language Processing
(EMNLP).
H. Paul Grice. 1975. Logic and conversation. page
4158.
Michael Grubinger, Paul D. Clough, Henning Muller,
and Thomas Deselaers. 2006. The iapr benchmark:
A new evaluation resource for visual information
systems. In Proceedings of the International Work-
shop OntoImage (LREC).
Sergio Guadarrama, Niveda Krishnamoorthy, Girish
Malkarnenkar, Subhashini Venugopalan, Raymond
Mooney, Trevor Darrell, and Kate Saenko. 2013.
Youtube2text: Recognizing and describing arbitrary
activities using semantic hierarchies and zero-shot
recognition. In International Conference on Com-
puter Vision (ICCV).
Emiel Krahmer and Kees van Deemter. 2012. Compu-
tational generation of referring expressions: A sur-
vey. In Computational Linguistics, volume 38, page
173218.
Alex Krizhevsky, Ilya Sutskever, and Geoffrey Hinton.
2012. Imagenet classification with deep convolu-
tional neural networks. In Neural Information Pro-
cessing Systems (NIPS).
Girish Kulkarni, Visruth Premraj, Sagnik Dhar, Sim-
ing Li, Yejin Choi, Alexander C Berg, and Tamara L
Berg. 2011. Babytalk: Understanding and generat-
ing simple image descriptions. In IEEE Computer
Vision and Pattern Recognition (CVPR).
Polina Kuznetsova, Vicente Ordonez, Alex Berg,
Tamara L Berg, and Yejin Choi. 2012. Collective
generation of natural image descriptions. In Associ-
ation for Computational Linguistics (ACL).
Margaret Mitchell, Kees van Deemter, and Ehud Re-
iter. 2010. Natural reference to objects in a visual
domain. In International Natural Language Gener-
ation Conference (INLG).
Margaret Mitchell, Kees van Deemter, and Ehud Reiter.
2011. Two approaches for generating size modifiers.
In European Workshop on Natural Language Gener-
ation.
Margaret Mitchell, Ehud Reiter, and Kees van Deemter.
2013a. Typicality and object reference. In Cognitive
Science (CogSci).
Margaret Mitchell, Kees van Deemter, and Ehud Reiter.
2013b. Generating expressions that refer to visible
objects. In North American Chapter of the Associa-
tion for Computational Linguistics (NAACL).
</reference>
<page confidence="0.970125">
797
</page>
<reference confidence="0.99981552238806">
Vicente Ordonez, Girish Kulkarni, and Tamara L. Berg.
2011. Im2text: Describing images using 1 million
captioned photographs. In Neural Information Pro-
cessing Systems (NIPS).
Vicente Ordonez, Jia Deng, Yejin Choi, Alexander C.
Berg, and Tamara L. Berg. 2013. From large scale
image categorization to entry-level categories. In In-
ternational Conference on Computer Vision (ICCV).
Florent Perronnin, Zeynep Akata, Zaid Harchaoui, and
Cordelia Schmid. 2012. Towards good practice
in large-scale learning for image classification. In
Computer Vision and Pattern Recognition (CVPR).
Yuan Ren, Kees Van Deemter, and Jeff Z Pan. 2010.
Charting the potential of description logic for the
generation of referring expressions. In International
Natural Language Generation Conference (INLG).
Eleanor Rosch. 1978. Principles of categorization.
Cognition and Categorization, page 2748.
Nitin Seemakurty, Jonathan Chu, Luis von Ahn, and
Anthony Tomasic. 2010. Word sense disambigua-
tion via human computation. In Human Computa-
tion Workshop.
Richard Socher, John Bauer, Christopher D. Manning,
and Andrew Y. Ng. 2013. Parsing With Composi-
tional Vector Grammars. In Association for Compu-
tational Linguistics (ACL).
Kees van Deemter, Ielka van der Sluis, and Albert Gatt.
2006. Building a semantically transparent corpus
for the generation of referring expressions. In In-
ternational Conference on Natural Language Gen-
eration (INLG).
Kees Van Deemter, Albert Gatt, Roger PG van Gompel,
and Emiel Krahmer. 2012. Toward a computational
psycholinguistics of reference production. In Topics
in Cognitive Science, volume 4(2), page 166183.
Jette Viethen and Robert Dale. 2008. The use of spa-
tial relations in referring expression generation. In
International Natural Language Generation Confer-
ence (INLG).
Jette Viethen and Robert Dale. 2010. Speaker-
dependent variation in content selection for referring
expression generation. In Australasian Language
Technology Workshop.
Jette Viethen, Margaret Mitchell, and Emiel Krahmer.
2013. Graphs and spatial relations in the generation
of referring expressions. In European Workshop on
Natural Language Generation.
Luis von Ahn and Laura Dabbish. 2004. Labeling im-
ages with a computer game. In ACM Conf. on Hu-
man Factors in Computing Systems (CHI).
Luis von Ahn, Mihir Kedia, and Manuel Blum. 2006a.
Verbosity: A game for collecting common-sense
knowledge. In ACM Conference on Human Factors
in Computing Systems (CHI).
Luis von Ahn, Ruoran Liu, and Manuel Blum. 2006b.
Peekaboom: A game for locating objects in images.
In ACM Conference on Human Factors in Comput-
ing Systems (CHI).
Terry Winograd. 1972. Understanding natural lan-
guage. Cognitive Psychology, 3(1):1191.
Yezhou Yang, Ching Lik Teo, Hal Daume III, and Yian-
nis Aloimonos. 2011. Corpus-guided sentence gen-
eration of natural images. In Empirical Methods on
Natural Language Processing (EMNLP).
Benjamin Z. Yao, Xiong Yang, Liang Lin, Mun Wai
Lee, and Song-Chun Zhu. 2010. I2t: Image parsing
to text description. Proc. IEEE, 98(8).
</reference>
<page confidence="0.997056">
798
</page>
</variant>
</algorithm>
<algorithm name="ParsHed" version="110505">
<variant no="0" confidence="0.634118">
<title confidence="0.998731">ReferItGame: Referring to Objects in Photographs of Natural Scenes</title>
<author confidence="0.982966">Vicente Mark Tamara L</author>
<affiliation confidence="0.674098">of North Carolina at Chapel Hill, Chapel Hill, NC 27599,</affiliation>
<address confidence="0.984658">Bishop’s School, San Diego, CA 92037,</address>
<email confidence="0.9995">vicente@cs.unc.edu,tlberg@cs.unc.edu</email>
<abstract confidence="0.997661944444444">In this paper we introduce a new game to crowd-source natural language referring expressions. By designing a two player game, we can both collect and verify referring expressions directly within the game. To date, the game has produced a dataset containing 130,525 expressions, referring to 96,654 distinct objects, in 19,894 photographs of natural scenes. This dataset is larger and more varied than previous REG datasets and allows us to study referring expressions in real-world scenes. We provide an in depth analysis of the resulting dataset. Based on our findings, we design a new optimization based model for generating referring expressions and perform experimental evaluations on 3 test sets.</abstract>
</variant>
</algorithm>
<algorithm name="ParsCit" version="110505">
<citationList>
<citation valid="true">
<authors>
<author>Ahmet Aker</author>
<author>Robert Gaizauskas</author>
</authors>
<title>Generating image descriptions using dependency relational patterns.</title>
<date>2010</date>
<booktitle>In Association for Computational Linguistics (ACL).</booktitle>
<contexts>
<context position="8363" citStr="Aker and Gaizauskas, 2010" startWordPosition="1297" endWordPosition="1300">introduced, consisting of craft objects like pipecleaners, ribbons, and feathers (Mitchell et al., 2010), or everyday home and office objects such as staplers, combs, or rulers (Mitchell et al., 2013a), arrayed on a simple background. These datasets helped moved referring expression generation research into the domain of real world objects. We seek to further these pursuits by constructing a dataset of natural objects in photographs of the real world. Image &amp; Video Description Generation: Recent research on automatic image description has followed two main directions. Retrieval based methods (Aker and Gaizauskas, 2010; Farhadi et al., 2010; Ordonez et al., 2011; Feng and Lapata, 2010; Feng and Lapata, 2013) retrieve existing captions or phrases to describe a query image. Bottom up methods (Kulkarni et al., 2011; Yang et al., 2011; Yao et al., 2010) rely on visual classifiers to first recognize image content and then construct captions from scratch, perhaps with some 788 Figure 1: An example game. Player 1 (left) sees an image with an object outlined in red (the man) and provides a referring expression for the object (“man in red shirt on horse”). Player 2 (right) sees the image and the expression from Play</context>
</contexts>
<marker>Aker, Gaizauskas, 2010</marker>
<rawString>Ahmet Aker and Robert Gaizauskas. 2010. Generating image descriptions using dependency relational patterns. In Association for Computational Linguistics (ACL).</rawString>
</citation>
<citation valid="true">
<authors>
<author>Andrei Barbu</author>
<author>Alexander Bridge</author>
<author>Zachary Burchill</author>
<author>Dan Coroian</author>
<author>Sven J Dickinson</author>
<author>Sanja Fidler</author>
<author>Aaron Michaux</author>
<author>Sam Mussman</author>
<author>Siddharth Narayanaswamy</author>
<author>Dhaval Salvi</author>
<author>Lara Schmidt</author>
<author>Jiangnan Shangguan</author>
<author>Jeffrey Mark Siskind</author>
<author>Jarrell W Waggoner</author>
<author>Song Wang</author>
<author>Jinlian Wei</author>
<author>Yifan Yin</author>
<author>Zhiqi Zhang</author>
</authors>
<title>Video in sentences out.</title>
<date>2012</date>
<booktitle>In Uncertainty in Artificial Intelligence (UAI).</booktitle>
<contexts>
<context position="9277" citStr="Barbu et al., 2012" startWordPosition="1456" endWordPosition="1459"> and then construct captions from scratch, perhaps with some 788 Figure 1: An example game. Player 1 (left) sees an image with an object outlined in red (the man) and provides a referring expression for the object (“man in red shirt on horse”). Player 2 (right) sees the image and the expression from Player 1 and must localize the correct object by clicking on it (click indicated by the red square). Elapsed time and current scores are also provided. input from natural language statistics. Very recently, these ideas have been extended to produce descriptions for videos (Guadarrama et al., 2013; Barbu et al., 2012). Like these methods, we generate descriptions for natural scenes, but focus on referring to particular objects rather than providing an overall description of an image or video. Human Computation Games: Games can be a useful tool for collecting large amounts of labeled data quickly. Human Computation Games were first introduced by Luis von Ahn in the ESP game (von Ahn and Dabbish, 2004) for image labeling, and later extended to segment objects (von Ahn et al., 2006b), collect common-sense knowledge (von Ahn et al., 2006a), or disambiguate words (Seemakurty et al., 2010). Recently, crowd games</context>
</contexts>
<marker>Barbu, Bridge, Burchill, Coroian, Dickinson, Fidler, Michaux, Mussman, Narayanaswamy, Salvi, Schmidt, Shangguan, Siskind, Waggoner, Wang, Wei, Yin, Zhang, 2012</marker>
<rawString>Andrei Barbu, Alexander Bridge, Zachary Burchill, Dan Coroian, Sven J. Dickinson, Sanja Fidler, Aaron Michaux, Sam Mussman, Siddharth Narayanaswamy, Dhaval Salvi, Lara Schmidt, Jiangnan Shangguan, Jeffrey Mark Siskind, Jarrell W. Waggoner, Song Wang, Jinlian Wei, Yifan Yin, and Zhiqi Zhang. 2012. Video in sentences out. In Uncertainty in Artificial Intelligence (UAI).</rawString>
</citation>
<citation valid="true">
<authors>
<author>Robert Dale</author>
<author>Ehud Reiter</author>
</authors>
<title>Computational interpretations of the gricean maxims in the generation of referring expressions.</title>
<date>1995</date>
<journal>Cognitive Science (CogSci),</journal>
<pages>19--233264</pages>
<contexts>
<context position="6625" citStr="Dale and Reiter, 1995" startWordPosition="1023" endWordPosition="1026"> as follows. First we outline related work from the vision and language communities (§2). Then we describe our online game for collecting referring expressions (§3) and provide an analysis of our new ReferItGame Dataset (§4). Finally, we present and evaluate our model for generating referring expressions (§5) and discuss conclusions and future work (§6). 2 Related Work Referring Expression Generation: There has been a long history of research on understanding how people generate referring expressions, dating back to the 1970s (Winograd, 1972). One common approach is the Incremental Algorithm (Dale and Reiter, 1995; Dale and Reiter, 2000) which uses logical expressions for generation. Much work in REG follows the Gricean maxims (Grice, 1975) which provide principles for how people will behave in conversation. Recently, there has been progress examining other aspects of the referring expression problem such as understanding what types of attributes are used (Mitchell et al., 2013a), modeling variations between speakers (Viethen and Dale, 2010; Viethen et al., 2013; Van Deemter et al., 2012; Mitchell et al., 2013b), incorporating visual classifiers (Mitchell et al., 2011), producing algorithms to refer to</context>
</contexts>
<marker>Dale, Reiter, 1995</marker>
<rawString>Robert Dale and Ehud Reiter. 1995. Computational interpretations of the gricean maxims in the generation of referring expressions. Cognitive Science (CogSci), 19:233264.</rawString>
</citation>
<citation valid="true">
<authors>
<author>Robert Dale</author>
<author>Ehud Reiter</author>
</authors>
<title>Building natural language generation systems. In</title>
<date>2000</date>
<publisher>Cambridge University Press.</publisher>
<contexts>
<context position="6649" citStr="Dale and Reiter, 2000" startWordPosition="1027" endWordPosition="1030">utline related work from the vision and language communities (§2). Then we describe our online game for collecting referring expressions (§3) and provide an analysis of our new ReferItGame Dataset (§4). Finally, we present and evaluate our model for generating referring expressions (§5) and discuss conclusions and future work (§6). 2 Related Work Referring Expression Generation: There has been a long history of research on understanding how people generate referring expressions, dating back to the 1970s (Winograd, 1972). One common approach is the Incremental Algorithm (Dale and Reiter, 1995; Dale and Reiter, 2000) which uses logical expressions for generation. Much work in REG follows the Gricean maxims (Grice, 1975) which provide principles for how people will behave in conversation. Recently, there has been progress examining other aspects of the referring expression problem such as understanding what types of attributes are used (Mitchell et al., 2013a), modeling variations between speakers (Viethen and Dale, 2010; Viethen et al., 2013; Van Deemter et al., 2012; Mitchell et al., 2013b), incorporating visual classifiers (Mitchell et al., 2011), producing algorithms to refer to object sets (Ren et al.</context>
</contexts>
<marker>Dale, Reiter, 2000</marker>
<rawString>Robert Dale and Ehud Reiter. 2000. Building natural language generation systems. In Cambridge University Press.</rawString>
</citation>
<citation valid="true">
<authors>
<author>Jia Deng</author>
<author>Alexander C Berg</author>
<author>Kai Li</author>
<author>Fei-Fei Li</author>
</authors>
<title>What does classifying more than 10,000 image categories tell us?</title>
<date>2010</date>
<booktitle>In European Conference on Computer Vision (ECCV).</booktitle>
<contexts>
<context position="1690" citStr="Deng et al., 2010" startWordPosition="259" endWordPosition="262">al world around us, making understanding the relationship between objects in the physical world and language describing those objects an important challenge problem for AI. From robotics, to image search, to situated language learning, and natural language grounding, there are a number of research areas that would benefit from a better understanding of how people refer to physical entities in the world. Recent advances in automatic computer vision methods have started to make technologies for recognizing thousands of object categories a near reality (Perronnin et al., 2012; Deng et al., 2012; Deng et al., 2010; Krizhevsky et al., 2012). As a result, there has been a spurt of recent work trying to estimate higher level semantics, including exciting efforts to automatically produce natural language descriptions of images and video (Farhadi et ∗Indicates equal author contribution. al., 2010; Kulkarni et al., 2011; Yang et al., 2011; Ordonez et al., 2011; Kuznetsova et al., 2012; Feng and Lapata, 2013). Common challenges encountered in these pursuits include the fact that descriptions can be highly task dependent, openended, and difficult to evaluate automatically. Therefore, we look at the related, bu</context>
</contexts>
<marker>Deng, Berg, Li, Li, 2010</marker>
<rawString>Jia Deng, Alexander C. Berg, Kai Li, and Fei-Fei Li. 2010. What does classifying more than 10,000 image categories tell us? In European Conference on Computer Vision (ECCV).</rawString>
</citation>
<citation valid="true">
<authors>
<author>Jia Deng</author>
</authors>
<title>Alex Berg, Sanjeev Satheesh, Hao Su, Aditya Khosla, and Fei-Fei Li.</title>
<date>2012</date>
<booktitle>In http://www.imagenet.org/challenges/LSVRC/2012/index.</booktitle>
<marker>Deng, 2012</marker>
<rawString>Jia Deng, Alex Berg, Sanjeev Satheesh, Hao Su, Aditya Khosla, and Fei-Fei Li. 2012. Large scale visual recognition challenge. In http://www.imagenet.org/challenges/LSVRC/2012/index.</rawString>
</citation>
<citation valid="true">
<authors>
<author>Jia Deng</author>
<author>Jonathan Krause</author>
<author>Li Fei-Fei</author>
</authors>
<title>Finegrained crowdsourcing for fine-grained recognition.</title>
<date>2013</date>
<booktitle>In IEEE Conference on Computer Vision and Pattern Recognition (CVPR).</booktitle>
<contexts>
<context position="10007" citStr="Deng et al., 2013" startWordPosition="1577" endWordPosition="1580">rather than providing an overall description of an image or video. Human Computation Games: Games can be a useful tool for collecting large amounts of labeled data quickly. Human Computation Games were first introduced by Luis von Ahn in the ESP game (von Ahn and Dabbish, 2004) for image labeling, and later extended to segment objects (von Ahn et al., 2006b), collect common-sense knowledge (von Ahn et al., 2006a), or disambiguate words (Seemakurty et al., 2010). Recently, crowd games have also been introduced into the computer vision community for tasks like fine grained category recognition (Deng et al., 2013). These games can be released publicly on the web or used on Mechanical Turk to enhance and encourage turker participation (Deng et al., 2013). Inspired by the success of previous games, we create a game to collect and verify natural language expressions referring to objects in natural scenes. 3 Referring Expression Game (ReferItGame) In this section we describe our referring expression game (ReferItGame∗), a simple two player game where players alternate between generating expressions referring to objects in images of natural scenes, and clicking on the locations of described objects. An exam</context>
</contexts>
<marker>Deng, Krause, Fei-Fei, 2013</marker>
<rawString>Jia Deng, Jonathan Krause, and Li Fei-Fei. 2013. Finegrained crowdsourcing for fine-grained recognition. In IEEE Conference on Computer Vision and Pattern Recognition (CVPR).</rawString>
</citation>
<citation valid="true">
<authors>
<author>Hugo Jair Escalante</author>
<author>Carlos A Hernandez</author>
<author>Jesus A Gonzalez</author>
<author>A Lopez-Lopez</author>
<author>Manuel Montes</author>
<author>Eduardo F Morales</author>
<author>L Enrique Sucar</author>
<author>Luis Villasenor</author>
<author>Michael Grubinger</author>
</authors>
<title>The segmented and annotated iapr tc-12 benchmark. Computer Vision and Image Understanding (CVIU).</title>
<date>2010</date>
<contexts>
<context position="14127" citStr="Escalante et al., 2010" startWordPosition="2256" endWordPosition="2260">h works well for our expressions. 4 ReferItGame Dataset In this section we describe the ReferItGame dataset†, including images and labels, processing the dataset, and analysis of the collection. 4.1 Images and Labels We build our dataset of referring expressions on top of the ImageCLEF IAPR image retrieval dataset (Grubinger et al., 2006). This dataset is a collection of 20,000 images available free of charge without copyright restrictions, depicting a variety of aspects of everyday life, from sports, to animals, to cities, and landscapes. Crucial for our purposes, the SAIAPR TC-12 expansion (Escalante et al., 2010) includes segmentations of each image into regions indicating the locations of constituent objects. 238 different object categories are labeled, including animals, people, buildings, objects, and background elements like grass or sky. This provides us with information regarding object category, object location, and object size, as well as the location and categories of other objects present in the same image. †Available at http://tamaraberg.com/referitgame 4.2 Collecting the Dataset From the ImageCLEF dataset, we created a total of over 100k distinct games (one per object labeled in the datase</context>
</contexts>
<marker>Escalante, Hernandez, Gonzalez, Lopez-Lopez, Montes, Morales, Sucar, Villasenor, Grubinger, 2010</marker>
<rawString>Hugo Jair Escalante, Carlos A. Hernandez, Jesus A. Gonzalez, A. Lopez-Lopez, Manuel Montes, Eduardo F. Morales, L. Enrique Sucar, Luis Villasenor, and Michael Grubinger. 2010. The segmented and annotated iapr tc-12 benchmark. Computer Vision and Image Understanding (CVIU).</rawString>
</citation>
<citation valid="true">
<authors>
<author>Rui Fang</author>
<author>Changsong Liu</author>
<author>Lanbo She</author>
<author>Joyce Chai</author>
</authors>
<title>Towards situated dialogue: Revisiting referring expression generation.</title>
<date>2013</date>
<booktitle>In Empirical Methods on Natural Language Processing (EMNLP).</booktitle>
<contexts>
<context position="7343" citStr="Fang et al., 2013" startWordPosition="1135" endWordPosition="1138"> Gricean maxims (Grice, 1975) which provide principles for how people will behave in conversation. Recently, there has been progress examining other aspects of the referring expression problem such as understanding what types of attributes are used (Mitchell et al., 2013a), modeling variations between speakers (Viethen and Dale, 2010; Viethen et al., 2013; Van Deemter et al., 2012; Mitchell et al., 2013b), incorporating visual classifiers (Mitchell et al., 2011), producing algorithms to refer to object sets (Ren et al., 2010; FitzGerald et al., 2013), or examining impoverished perception REG (Fang et al., 2013). A good survey of work in this area is provided in Krahmer and van Deemter (2012). We build on past work, extending models to generate attributes jointly in a category specific framework. Referring Expression Datasets: Some initial datasets in REG used graphics engines to produce images of objects (van Deemter et al., 2006; Viethen and Dale, 2008). Recently more realistic datasets have been introduced, consisting of craft objects like pipecleaners, ribbons, and feathers (Mitchell et al., 2010), or everyday home and office objects such as staplers, combs, or rulers (Mitchell et al., 2013a), ar</context>
</contexts>
<marker>Fang, Liu, She, Chai, 2013</marker>
<rawString>Rui Fang, Changsong Liu, Lanbo She, and Joyce Chai. 2013. Towards situated dialogue: Revisiting referring expression generation. In Empirical Methods on Natural Language Processing (EMNLP).</rawString>
</citation>
<citation valid="true">
<authors>
<author>Ali Farhadi</author>
<author>Mohsen Hejrati</author>
<author>Mohammad Amin Sadeghi</author>
<author>Peter Young</author>
<author>Cyrus Rashtchian</author>
<author>Julia Hockenmaier</author>
<author>David Forsyth</author>
</authors>
<title>Every picture tells a story: generating sentences for images.</title>
<date>2010</date>
<booktitle>In European Conference on Computer Vision (ECCV).</booktitle>
<contexts>
<context position="8385" citStr="Farhadi et al., 2010" startWordPosition="1301" endWordPosition="1304">raft objects like pipecleaners, ribbons, and feathers (Mitchell et al., 2010), or everyday home and office objects such as staplers, combs, or rulers (Mitchell et al., 2013a), arrayed on a simple background. These datasets helped moved referring expression generation research into the domain of real world objects. We seek to further these pursuits by constructing a dataset of natural objects in photographs of the real world. Image &amp; Video Description Generation: Recent research on automatic image description has followed two main directions. Retrieval based methods (Aker and Gaizauskas, 2010; Farhadi et al., 2010; Ordonez et al., 2011; Feng and Lapata, 2010; Feng and Lapata, 2013) retrieve existing captions or phrases to describe a query image. Bottom up methods (Kulkarni et al., 2011; Yang et al., 2011; Yao et al., 2010) rely on visual classifiers to first recognize image content and then construct captions from scratch, perhaps with some 788 Figure 1: An example game. Player 1 (left) sees an image with an object outlined in red (the man) and provides a referring expression for the object (“man in red shirt on horse”). Player 2 (right) sees the image and the expression from Player 1 and must localize</context>
</contexts>
<marker>Farhadi, Hejrati, Sadeghi, Young, Rashtchian, Hockenmaier, Forsyth, 2010</marker>
<rawString>Ali Farhadi, Mohsen Hejrati, Mohammad Amin Sadeghi, Peter Young, Cyrus Rashtchian, Julia Hockenmaier, and David Forsyth. 2010. Every picture tells a story: generating sentences for images. In European Conference on Computer Vision (ECCV).</rawString>
</citation>
<citation valid="true">
<authors>
<author>Yansong Feng</author>
<author>Mirella Lapata</author>
</authors>
<title>How many words is a picture worth? automatic caption generation for news images.</title>
<date>2010</date>
<booktitle>In Association for Computational Linguistics (ACL).</booktitle>
<contexts>
<context position="8430" citStr="Feng and Lapata, 2010" startWordPosition="1309" endWordPosition="1313"> feathers (Mitchell et al., 2010), or everyday home and office objects such as staplers, combs, or rulers (Mitchell et al., 2013a), arrayed on a simple background. These datasets helped moved referring expression generation research into the domain of real world objects. We seek to further these pursuits by constructing a dataset of natural objects in photographs of the real world. Image &amp; Video Description Generation: Recent research on automatic image description has followed two main directions. Retrieval based methods (Aker and Gaizauskas, 2010; Farhadi et al., 2010; Ordonez et al., 2011; Feng and Lapata, 2010; Feng and Lapata, 2013) retrieve existing captions or phrases to describe a query image. Bottom up methods (Kulkarni et al., 2011; Yang et al., 2011; Yao et al., 2010) rely on visual classifiers to first recognize image content and then construct captions from scratch, perhaps with some 788 Figure 1: An example game. Player 1 (left) sees an image with an object outlined in red (the man) and provides a referring expression for the object (“man in red shirt on horse”). Player 2 (right) sees the image and the expression from Player 1 and must localize the correct object by clicking on it (click </context>
</contexts>
<marker>Feng, Lapata, 2010</marker>
<rawString>Yansong Feng and Mirella Lapata. 2010. How many words is a picture worth? automatic caption generation for news images. In Association for Computational Linguistics (ACL).</rawString>
</citation>
<citation valid="true">
<authors>
<author>Yansong Feng</author>
<author>Mirella Lapata</author>
</authors>
<title>Automatic caption generation for news images.</title>
<date>2013</date>
<journal>IEEE Transactions on Pattern Analysis and Machine Intelligence,</journal>
<volume>35</volume>
<issue>4</issue>
<contexts>
<context position="2086" citStr="Feng and Lapata, 2013" startWordPosition="323" endWordPosition="326">s in the world. Recent advances in automatic computer vision methods have started to make technologies for recognizing thousands of object categories a near reality (Perronnin et al., 2012; Deng et al., 2012; Deng et al., 2010; Krizhevsky et al., 2012). As a result, there has been a spurt of recent work trying to estimate higher level semantics, including exciting efforts to automatically produce natural language descriptions of images and video (Farhadi et ∗Indicates equal author contribution. al., 2010; Kulkarni et al., 2011; Yang et al., 2011; Ordonez et al., 2011; Kuznetsova et al., 2012; Feng and Lapata, 2013). Common challenges encountered in these pursuits include the fact that descriptions can be highly task dependent, openended, and difficult to evaluate automatically. Therefore, we look at the related, but more focused problem of referring expression generation (REG). Previous work on REG has made significant progress toward understanding how people generate expressions to refer to objects (a recent survey of techniques is provided in Krahmer and van Deemter (2012)). In this paper, we study the relatively unexplored setting of how people refer to objects in complex photographs of real-world cl</context>
<context position="8454" citStr="Feng and Lapata, 2013" startWordPosition="1314" endWordPosition="1317">al., 2010), or everyday home and office objects such as staplers, combs, or rulers (Mitchell et al., 2013a), arrayed on a simple background. These datasets helped moved referring expression generation research into the domain of real world objects. We seek to further these pursuits by constructing a dataset of natural objects in photographs of the real world. Image &amp; Video Description Generation: Recent research on automatic image description has followed two main directions. Retrieval based methods (Aker and Gaizauskas, 2010; Farhadi et al., 2010; Ordonez et al., 2011; Feng and Lapata, 2010; Feng and Lapata, 2013) retrieve existing captions or phrases to describe a query image. Bottom up methods (Kulkarni et al., 2011; Yang et al., 2011; Yao et al., 2010) rely on visual classifiers to first recognize image content and then construct captions from scratch, perhaps with some 788 Figure 1: An example game. Player 1 (left) sees an image with an object outlined in red (the man) and provides a referring expression for the object (“man in red shirt on horse”). Player 2 (right) sees the image and the expression from Player 1 and must localize the correct object by clicking on it (click indicated by the red squ</context>
</contexts>
<marker>Feng, Lapata, 2013</marker>
<rawString>Yansong Feng and Mirella Lapata. 2013. Automatic caption generation for news images. IEEE Transactions on Pattern Analysis and Machine Intelligence, 35(4):797–812.</rawString>
</citation>
<citation valid="true">
<authors>
<author>Nicholas FitzGerald</author>
<author>Yoav Artzi</author>
<author>Luke Zettlemoyer</author>
</authors>
<title>Learning distributions over logical forms for referring expression generation.</title>
<date>2013</date>
<booktitle>In Empirical Methods on Natural Language Processing (EMNLP).</booktitle>
<contexts>
<context position="3117" citStr="FitzGerald et al., 2013" startWordPosition="483" endWordPosition="486">ey of techniques is provided in Krahmer and van Deemter (2012)). In this paper, we study the relatively unexplored setting of how people refer to objects in complex photographs of real-world cluttered scenes. One initial stumbling block to examining this scenario is lack of existing relevant datasets, as previous collections for studying REG have used relatively focused domains such as graphics generated objects (van Deemter et al., 2006; Viethen and Dale, 2008), crafts (Mitchell et al., 2010), or small everyday (home and office) objects arrayed on a simple background (Mitchell et al., 2013a; FitzGerald et al., 2013). In this paper, we collect a new large-scale corpus, currently containing 130,525 expressions, referring to 96,654 distinct objects, in 19,894 photographs of real world scenes. Some examples from our dataset are shown in Figure 5. To construct this corpus efficiently, we design a new two player referring expression game (ReferItGame) to crowd-source the data collection. Popularized by efforts like the ESP game (von Ahn and Dabbish, 2004) and Peekaboom (von Ahn et al., 2006b), Human Computation based games can be an effective way to engage users and collect large amounts of data inexpensively.</context>
<context position="7281" citStr="FitzGerald et al., 2013" startWordPosition="1125" endWordPosition="1128">ses logical expressions for generation. Much work in REG follows the Gricean maxims (Grice, 1975) which provide principles for how people will behave in conversation. Recently, there has been progress examining other aspects of the referring expression problem such as understanding what types of attributes are used (Mitchell et al., 2013a), modeling variations between speakers (Viethen and Dale, 2010; Viethen et al., 2013; Van Deemter et al., 2012; Mitchell et al., 2013b), incorporating visual classifiers (Mitchell et al., 2011), producing algorithms to refer to object sets (Ren et al., 2010; FitzGerald et al., 2013), or examining impoverished perception REG (Fang et al., 2013). A good survey of work in this area is provided in Krahmer and van Deemter (2012). We build on past work, extending models to generate attributes jointly in a category specific framework. Referring Expression Datasets: Some initial datasets in REG used graphics engines to produce images of objects (van Deemter et al., 2006; Viethen and Dale, 2008). Recently more realistic datasets have been introduced, consisting of craft objects like pipecleaners, ribbons, and feathers (Mitchell et al., 2010), or everyday home and office objects s</context>
<context position="16566" citStr="FitzGerald et al., 2013" startWordPosition="2637" endWordPosition="2640">ial media and the survey section of reddit. In this manner we collected over 4 thousand referring expressions over a period of 3 weeks. To speed up data collection, we also posted the game on Mechanical Turk. Turkers were paid upon completion of 10 correct games (games where Player 2 clicks on the correct object of interest). Turkers were pre-screened to have approval ratings above 80% and to be located in the US for language consistency. 4.3 Processing the Dataset Because of the size of the dataset, hand annotation of all referring expressions is prohibitive. Therefore, similar to past work (FitzGerald et al., 2013), we design an automatic method to pre-process the expressions and extract object and attribute mentions. These automatically processed expressions are used only for analysis and model training. We also fully hand label portions of the dataset for evaluation (§5.2). By examining the expressions in the collected dataset, we define a set of attributes with broad 790 S ::=subject word color word&apos; ::= rel(S, color word)color word &apos;=color word | prep in(S, color word)color word &apos;=color word size word&apos; ::= rel(S, size word)size word &apos;=size word abs loc word&apos; ::= rel(S, abs loc word) abs loc word &apos;=a</context>
</contexts>
<marker>FitzGerald, Artzi, Zettlemoyer, 2013</marker>
<rawString>Nicholas FitzGerald, Yoav Artzi, and Luke Zettlemoyer. 2013. Learning distributions over logical forms for referring expression generation. In Empirical Methods on Natural Language Processing (EMNLP).</rawString>
</citation>
<citation valid="true">
<authors>
<author>H Paul Grice</author>
</authors>
<title>Logic and conversation.</title>
<date>1975</date>
<pages>4158</pages>
<contexts>
<context position="6754" citStr="Grice, 1975" startWordPosition="1045" endWordPosition="1046">referring expressions (§3) and provide an analysis of our new ReferItGame Dataset (§4). Finally, we present and evaluate our model for generating referring expressions (§5) and discuss conclusions and future work (§6). 2 Related Work Referring Expression Generation: There has been a long history of research on understanding how people generate referring expressions, dating back to the 1970s (Winograd, 1972). One common approach is the Incremental Algorithm (Dale and Reiter, 1995; Dale and Reiter, 2000) which uses logical expressions for generation. Much work in REG follows the Gricean maxims (Grice, 1975) which provide principles for how people will behave in conversation. Recently, there has been progress examining other aspects of the referring expression problem such as understanding what types of attributes are used (Mitchell et al., 2013a), modeling variations between speakers (Viethen and Dale, 2010; Viethen et al., 2013; Van Deemter et al., 2012; Mitchell et al., 2013b), incorporating visual classifiers (Mitchell et al., 2011), producing algorithms to refer to object sets (Ren et al., 2010; FitzGerald et al., 2013), or examining impoverished perception REG (Fang et al., 2013). A good su</context>
</contexts>
<marker>Grice, 1975</marker>
<rawString>H. Paul Grice. 1975. Logic and conversation. page 4158.</rawString>
</citation>
<citation valid="true">
<authors>
<author>Michael Grubinger</author>
<author>Paul D Clough</author>
<author>Henning Muller</author>
<author>Thomas Deselaers</author>
</authors>
<title>The iapr benchmark: A new evaluation resource for visual information systems.</title>
<date>2006</date>
<booktitle>In Proceedings of the International Workshop OntoImage (LREC).</booktitle>
<contexts>
<context position="13844" citStr="Grubinger et al., 2006" startWordPosition="2213" endWordPosition="2216">f there is a successful match with a previously generated expression, then the canned click from the most similar pre-recorded game is used. More complex similarities could be used, but since we require real-time performance in our game setting we use this simple implementation which works well for our expressions. 4 ReferItGame Dataset In this section we describe the ReferItGame dataset†, including images and labels, processing the dataset, and analysis of the collection. 4.1 Images and Labels We build our dataset of referring expressions on top of the ImageCLEF IAPR image retrieval dataset (Grubinger et al., 2006). This dataset is a collection of 20,000 images available free of charge without copyright restrictions, depicting a variety of aspects of everyday life, from sports, to animals, to cities, and landscapes. Crucial for our purposes, the SAIAPR TC-12 expansion (Escalante et al., 2010) includes segmentations of each image into regions indicating the locations of constituent objects. 238 different object categories are labeled, including animals, people, buildings, objects, and background elements like grass or sky. This provides us with information regarding object category, object location, and </context>
</contexts>
<marker>Grubinger, Clough, Muller, Deselaers, 2006</marker>
<rawString>Michael Grubinger, Paul D. Clough, Henning Muller, and Thomas Deselaers. 2006. The iapr benchmark: A new evaluation resource for visual information systems. In Proceedings of the International Workshop OntoImage (LREC).</rawString>
</citation>
<citation valid="true">
<authors>
<author>Sergio Guadarrama</author>
<author>Niveda Krishnamoorthy</author>
<author>Girish Malkarnenkar</author>
<author>Subhashini Venugopalan</author>
<author>Raymond Mooney</author>
<author>Trevor Darrell</author>
<author>Kate Saenko</author>
</authors>
<title>Youtube2text: Recognizing and describing arbitrary activities using semantic hierarchies and zero-shot recognition.</title>
<date>2013</date>
<booktitle>In International Conference on Computer Vision (ICCV).</booktitle>
<contexts>
<context position="9256" citStr="Guadarrama et al., 2013" startWordPosition="1452" endWordPosition="1455">t recognize image content and then construct captions from scratch, perhaps with some 788 Figure 1: An example game. Player 1 (left) sees an image with an object outlined in red (the man) and provides a referring expression for the object (“man in red shirt on horse”). Player 2 (right) sees the image and the expression from Player 1 and must localize the correct object by clicking on it (click indicated by the red square). Elapsed time and current scores are also provided. input from natural language statistics. Very recently, these ideas have been extended to produce descriptions for videos (Guadarrama et al., 2013; Barbu et al., 2012). Like these methods, we generate descriptions for natural scenes, but focus on referring to particular objects rather than providing an overall description of an image or video. Human Computation Games: Games can be a useful tool for collecting large amounts of labeled data quickly. Human Computation Games were first introduced by Luis von Ahn in the ESP game (von Ahn and Dabbish, 2004) for image labeling, and later extended to segment objects (von Ahn et al., 2006b), collect common-sense knowledge (von Ahn et al., 2006a), or disambiguate words (Seemakurty et al., 2010). </context>
</contexts>
<marker>Guadarrama, Krishnamoorthy, Malkarnenkar, Venugopalan, Mooney, Darrell, Saenko, 2013</marker>
<rawString>Sergio Guadarrama, Niveda Krishnamoorthy, Girish Malkarnenkar, Subhashini Venugopalan, Raymond Mooney, Trevor Darrell, and Kate Saenko. 2013. Youtube2text: Recognizing and describing arbitrary activities using semantic hierarchies and zero-shot recognition. In International Conference on Computer Vision (ICCV).</rawString>
</citation>
<citation valid="true">
<authors>
<author>Emiel Krahmer</author>
<author>Kees van Deemter</author>
</authors>
<title>Computational generation of referring expressions: A survey.</title>
<date>2012</date>
<booktitle>In Computational Linguistics,</booktitle>
<volume>38</volume>
<pages>173218</pages>
<marker>Krahmer, van Deemter, 2012</marker>
<rawString>Emiel Krahmer and Kees van Deemter. 2012. Computational generation of referring expressions: A survey. In Computational Linguistics, volume 38, page 173218.</rawString>
</citation>
<citation valid="true">
<authors>
<author>Alex Krizhevsky</author>
<author>Ilya Sutskever</author>
<author>Geoffrey Hinton</author>
</authors>
<title>Imagenet classification with deep convolutional neural networks.</title>
<date>2012</date>
<booktitle>In Neural Information Processing Systems (NIPS).</booktitle>
<contexts>
<context position="1716" citStr="Krizhevsky et al., 2012" startWordPosition="263" endWordPosition="266"> making understanding the relationship between objects in the physical world and language describing those objects an important challenge problem for AI. From robotics, to image search, to situated language learning, and natural language grounding, there are a number of research areas that would benefit from a better understanding of how people refer to physical entities in the world. Recent advances in automatic computer vision methods have started to make technologies for recognizing thousands of object categories a near reality (Perronnin et al., 2012; Deng et al., 2012; Deng et al., 2010; Krizhevsky et al., 2012). As a result, there has been a spurt of recent work trying to estimate higher level semantics, including exciting efforts to automatically produce natural language descriptions of images and video (Farhadi et ∗Indicates equal author contribution. al., 2010; Kulkarni et al., 2011; Yang et al., 2011; Ordonez et al., 2011; Kuznetsova et al., 2012; Feng and Lapata, 2013). Common challenges encountered in these pursuits include the fact that descriptions can be highly task dependent, openended, and difficult to evaluate automatically. Therefore, we look at the related, but more focused problem of </context>
</contexts>
<marker>Krizhevsky, Sutskever, Hinton, 2012</marker>
<rawString>Alex Krizhevsky, Ilya Sutskever, and Geoffrey Hinton. 2012. Imagenet classification with deep convolutional neural networks. In Neural Information Processing Systems (NIPS).</rawString>
</citation>
<citation valid="true">
<authors>
<author>Girish Kulkarni</author>
<author>Visruth Premraj</author>
<author>Sagnik Dhar</author>
<author>Siming Li</author>
<author>Yejin Choi</author>
<author>Alexander C Berg</author>
<author>Tamara L Berg</author>
</authors>
<title>Babytalk: Understanding and generating simple image descriptions.</title>
<date>2011</date>
<booktitle>In IEEE Computer Vision and Pattern Recognition (CVPR).</booktitle>
<contexts>
<context position="1996" citStr="Kulkarni et al., 2011" startWordPosition="307" endWordPosition="310">as that would benefit from a better understanding of how people refer to physical entities in the world. Recent advances in automatic computer vision methods have started to make technologies for recognizing thousands of object categories a near reality (Perronnin et al., 2012; Deng et al., 2012; Deng et al., 2010; Krizhevsky et al., 2012). As a result, there has been a spurt of recent work trying to estimate higher level semantics, including exciting efforts to automatically produce natural language descriptions of images and video (Farhadi et ∗Indicates equal author contribution. al., 2010; Kulkarni et al., 2011; Yang et al., 2011; Ordonez et al., 2011; Kuznetsova et al., 2012; Feng and Lapata, 2013). Common challenges encountered in these pursuits include the fact that descriptions can be highly task dependent, openended, and difficult to evaluate automatically. Therefore, we look at the related, but more focused problem of referring expression generation (REG). Previous work on REG has made significant progress toward understanding how people generate expressions to refer to objects (a recent survey of techniques is provided in Krahmer and van Deemter (2012)). In this paper, we study the relatively</context>
<context position="8560" citStr="Kulkarni et al., 2011" startWordPosition="1332" endWordPosition="1335">, arrayed on a simple background. These datasets helped moved referring expression generation research into the domain of real world objects. We seek to further these pursuits by constructing a dataset of natural objects in photographs of the real world. Image &amp; Video Description Generation: Recent research on automatic image description has followed two main directions. Retrieval based methods (Aker and Gaizauskas, 2010; Farhadi et al., 2010; Ordonez et al., 2011; Feng and Lapata, 2010; Feng and Lapata, 2013) retrieve existing captions or phrases to describe a query image. Bottom up methods (Kulkarni et al., 2011; Yang et al., 2011; Yao et al., 2010) rely on visual classifiers to first recognize image content and then construct captions from scratch, perhaps with some 788 Figure 1: An example game. Player 1 (left) sees an image with an object outlined in red (the man) and provides a referring expression for the object (“man in red shirt on horse”). Player 2 (right) sees the image and the expression from Player 1 and must localize the correct object by clicking on it (click indicated by the red square). Elapsed time and current scores are also provided. input from natural language statistics. Very rece</context>
</contexts>
<marker>Kulkarni, Premraj, Dhar, Li, Choi, Berg, Berg, 2011</marker>
<rawString>Girish Kulkarni, Visruth Premraj, Sagnik Dhar, Siming Li, Yejin Choi, Alexander C Berg, and Tamara L Berg. 2011. Babytalk: Understanding and generating simple image descriptions. In IEEE Computer Vision and Pattern Recognition (CVPR).</rawString>
</citation>
<citation valid="true">
<authors>
<author>Polina Kuznetsova</author>
<author>Vicente Ordonez</author>
<author>Alex Berg</author>
<author>Tamara L Berg</author>
<author>Yejin Choi</author>
</authors>
<title>Collective generation of natural image descriptions.</title>
<date>2012</date>
<booktitle>In Association for Computational Linguistics (ACL).</booktitle>
<contexts>
<context position="2062" citStr="Kuznetsova et al., 2012" startWordPosition="319" endWordPosition="322">refer to physical entities in the world. Recent advances in automatic computer vision methods have started to make technologies for recognizing thousands of object categories a near reality (Perronnin et al., 2012; Deng et al., 2012; Deng et al., 2010; Krizhevsky et al., 2012). As a result, there has been a spurt of recent work trying to estimate higher level semantics, including exciting efforts to automatically produce natural language descriptions of images and video (Farhadi et ∗Indicates equal author contribution. al., 2010; Kulkarni et al., 2011; Yang et al., 2011; Ordonez et al., 2011; Kuznetsova et al., 2012; Feng and Lapata, 2013). Common challenges encountered in these pursuits include the fact that descriptions can be highly task dependent, openended, and difficult to evaluate automatically. Therefore, we look at the related, but more focused problem of referring expression generation (REG). Previous work on REG has made significant progress toward understanding how people generate expressions to refer to objects (a recent survey of techniques is provided in Krahmer and van Deemter (2012)). In this paper, we study the relatively unexplored setting of how people refer to objects in complex phot</context>
</contexts>
<marker>Kuznetsova, Ordonez, Berg, Berg, Choi, 2012</marker>
<rawString>Polina Kuznetsova, Vicente Ordonez, Alex Berg, Tamara L Berg, and Yejin Choi. 2012. Collective generation of natural image descriptions. In Association for Computational Linguistics (ACL).</rawString>
</citation>
<citation valid="true">
<authors>
<author>Margaret Mitchell</author>
<author>Kees van Deemter</author>
<author>Ehud Reiter</author>
</authors>
<title>Natural reference to objects in a visual domain.</title>
<date>2010</date>
<booktitle>In International Natural Language Generation Conference (INLG).</booktitle>
<marker>Mitchell, van Deemter, Reiter, 2010</marker>
<rawString>Margaret Mitchell, Kees van Deemter, and Ehud Reiter. 2010. Natural reference to objects in a visual domain. In International Natural Language Generation Conference (INLG).</rawString>
</citation>
<citation valid="true">
<authors>
<author>Margaret Mitchell</author>
<author>Kees van Deemter</author>
<author>Ehud Reiter</author>
</authors>
<title>Two approaches for generating size modifiers.</title>
<date>2011</date>
<booktitle>In European Workshop on Natural Language Generation.</booktitle>
<marker>Mitchell, van Deemter, Reiter, 2011</marker>
<rawString>Margaret Mitchell, Kees van Deemter, and Ehud Reiter. 2011. Two approaches for generating size modifiers. In European Workshop on Natural Language Generation.</rawString>
</citation>
<citation valid="true">
<authors>
<author>Margaret Mitchell</author>
<author>Ehud Reiter</author>
<author>Kees van Deemter</author>
</authors>
<title>Typicality and object reference.</title>
<date>2013</date>
<booktitle>In Cognitive Science (CogSci).</booktitle>
<marker>Mitchell, Reiter, van Deemter, 2013</marker>
<rawString>Margaret Mitchell, Ehud Reiter, and Kees van Deemter. 2013a. Typicality and object reference. In Cognitive Science (CogSci).</rawString>
</citation>
<citation valid="true">
<authors>
<author>Margaret Mitchell</author>
<author>Kees van Deemter</author>
<author>Ehud Reiter</author>
</authors>
<title>Generating expressions that refer to visible objects.</title>
<date>2013</date>
<booktitle>In North American Chapter of the Association for Computational Linguistics (NAACL).</booktitle>
<marker>Mitchell, van Deemter, Reiter, 2013</marker>
<rawString>Margaret Mitchell, Kees van Deemter, and Ehud Reiter. 2013b. Generating expressions that refer to visible objects. In North American Chapter of the Association for Computational Linguistics (NAACL).</rawString>
</citation>
<citation valid="true">
<authors>
<author>Vicente Ordonez</author>
<author>Girish Kulkarni</author>
<author>Tamara L Berg</author>
</authors>
<title>Im2text: Describing images using 1 million captioned photographs.</title>
<date>2011</date>
<booktitle>In Neural Information Processing Systems (NIPS).</booktitle>
<contexts>
<context position="2037" citStr="Ordonez et al., 2011" startWordPosition="315" endWordPosition="318">tanding of how people refer to physical entities in the world. Recent advances in automatic computer vision methods have started to make technologies for recognizing thousands of object categories a near reality (Perronnin et al., 2012; Deng et al., 2012; Deng et al., 2010; Krizhevsky et al., 2012). As a result, there has been a spurt of recent work trying to estimate higher level semantics, including exciting efforts to automatically produce natural language descriptions of images and video (Farhadi et ∗Indicates equal author contribution. al., 2010; Kulkarni et al., 2011; Yang et al., 2011; Ordonez et al., 2011; Kuznetsova et al., 2012; Feng and Lapata, 2013). Common challenges encountered in these pursuits include the fact that descriptions can be highly task dependent, openended, and difficult to evaluate automatically. Therefore, we look at the related, but more focused problem of referring expression generation (REG). Previous work on REG has made significant progress toward understanding how people generate expressions to refer to objects (a recent survey of techniques is provided in Krahmer and van Deemter (2012)). In this paper, we study the relatively unexplored setting of how people refer t</context>
<context position="8407" citStr="Ordonez et al., 2011" startWordPosition="1305" endWordPosition="1308">cleaners, ribbons, and feathers (Mitchell et al., 2010), or everyday home and office objects such as staplers, combs, or rulers (Mitchell et al., 2013a), arrayed on a simple background. These datasets helped moved referring expression generation research into the domain of real world objects. We seek to further these pursuits by constructing a dataset of natural objects in photographs of the real world. Image &amp; Video Description Generation: Recent research on automatic image description has followed two main directions. Retrieval based methods (Aker and Gaizauskas, 2010; Farhadi et al., 2010; Ordonez et al., 2011; Feng and Lapata, 2010; Feng and Lapata, 2013) retrieve existing captions or phrases to describe a query image. Bottom up methods (Kulkarni et al., 2011; Yang et al., 2011; Yao et al., 2010) rely on visual classifiers to first recognize image content and then construct captions from scratch, perhaps with some 788 Figure 1: An example game. Player 1 (left) sees an image with an object outlined in red (the man) and provides a referring expression for the object (“man in red shirt on horse”). Player 2 (right) sees the image and the expression from Player 1 and must localize the correct object by</context>
</contexts>
<marker>Ordonez, Kulkarni, Berg, 2011</marker>
<rawString>Vicente Ordonez, Girish Kulkarni, and Tamara L. Berg. 2011. Im2text: Describing images using 1 million captioned photographs. In Neural Information Processing Systems (NIPS).</rawString>
</citation>
<citation valid="true">
<authors>
<author>Vicente Ordonez</author>
<author>Jia Deng</author>
<author>Yejin Choi</author>
<author>Alexander C Berg</author>
<author>Tamara L Berg</author>
</authors>
<title>From large scale image categorization to entry-level categories.</title>
<date>2013</date>
<booktitle>In International Conference on Computer Vision (ICCV).</booktitle>
<contexts>
<context position="18849" citStr="Ordonez et al., 2013" startWordPosition="3012" endWordPosition="3015">ly. Absolute location refers to the location of the object in the image (e.g. “top of the image”). Relative location relation and relative location object attributes allow for referring expressions that localize the object with respect to another object in the picture (e.g. “the car to the left of the tree”). Generic attributes cover all less frequently observed attribute types (e.g. “wooden” or “round”). The entry-level category attribute is related to the concept of entry-level categories first proposed by Psychologists in the 1970s (Rosch, 1978) and recently explored in visual recognition (Ordonez et al., 2013). The idea of entry-level categories is that an object can belong to many different categories; an indigo bunting is an oscine, a bird, a vertebrate, a chordate, and so on. But, a person looking at a picture of one would probably call it a bird (unless they are very familiar with ornithology). Therefore, we include this attribute to capture how people name object categories in referring expressions. Parsing the referring expressions: We parse the expressions using the most recent version of the StanfordCoreNLP parser (Socher et al., 2013). We begin by traversing the parse tree in a breadth-fir</context>
</contexts>
<marker>Ordonez, Deng, Choi, Berg, Berg, 2013</marker>
<rawString>Vicente Ordonez, Jia Deng, Yejin Choi, Alexander C. Berg, and Tamara L. Berg. 2013. From large scale image categorization to entry-level categories. In International Conference on Computer Vision (ICCV).</rawString>
</citation>
<citation valid="true">
<authors>
<author>Florent Perronnin</author>
<author>Zeynep Akata</author>
<author>Zaid Harchaoui</author>
<author>Cordelia Schmid</author>
</authors>
<title>Towards good practice in large-scale learning for image classification.</title>
<date>2012</date>
<booktitle>In Computer Vision and Pattern Recognition (CVPR).</booktitle>
<contexts>
<context position="1652" citStr="Perronnin et al., 2012" startWordPosition="251" endWordPosition="254">ay language and discourse concerns the visual world around us, making understanding the relationship between objects in the physical world and language describing those objects an important challenge problem for AI. From robotics, to image search, to situated language learning, and natural language grounding, there are a number of research areas that would benefit from a better understanding of how people refer to physical entities in the world. Recent advances in automatic computer vision methods have started to make technologies for recognizing thousands of object categories a near reality (Perronnin et al., 2012; Deng et al., 2012; Deng et al., 2010; Krizhevsky et al., 2012). As a result, there has been a spurt of recent work trying to estimate higher level semantics, including exciting efforts to automatically produce natural language descriptions of images and video (Farhadi et ∗Indicates equal author contribution. al., 2010; Kulkarni et al., 2011; Yang et al., 2011; Ordonez et al., 2011; Kuznetsova et al., 2012; Feng and Lapata, 2013). Common challenges encountered in these pursuits include the fact that descriptions can be highly task dependent, openended, and difficult to evaluate automatically.</context>
</contexts>
<marker>Perronnin, Akata, Harchaoui, Schmid, 2012</marker>
<rawString>Florent Perronnin, Zeynep Akata, Zaid Harchaoui, and Cordelia Schmid. 2012. Towards good practice in large-scale learning for image classification. In Computer Vision and Pattern Recognition (CVPR).</rawString>
</citation>
<citation valid="true">
<authors>
<author>Yuan Ren</author>
<author>Kees Van Deemter</author>
<author>Jeff Z Pan</author>
</authors>
<title>Charting the potential of description logic for the generation of referring expressions.</title>
<date>2010</date>
<booktitle>In International Natural Language Generation Conference (INLG).</booktitle>
<marker>Ren, Van Deemter, Pan, 2010</marker>
<rawString>Yuan Ren, Kees Van Deemter, and Jeff Z Pan. 2010. Charting the potential of description logic for the generation of referring expressions. In International Natural Language Generation Conference (INLG).</rawString>
</citation>
<citation valid="true">
<authors>
<author>Eleanor Rosch</author>
</authors>
<title>Principles of categorization. Cognition and Categorization,</title>
<date>1978</date>
<pages>2748</pages>
<contexts>
<context position="5035" citStr="Rosch, 1978" startWordPosition="787" endWordPosition="788">ces the types of attributes used in their referring expression (e.g. people use color words to describe cars more often than mountains). Additionally, we find that references to an object are sometimes made with respect to other nearby objects, e.g. “the ball to left of the man”. Interestingly, the types of reference objects (i.e. “the man”) used in referring expressions is also biased toward some categories. Finally, we find that the word used to refer to the object category itself displays consistencies across people. This notion is related to ideas of entrylevel categories from Psychology (Rosch, 1978). Given these findings, we propose an optimization model for generating referring expressions that jointly selects which attributes to include in the expression, and what attribute values to generate. This model incorporates both visual models for selecting attribute-values and object category specific priors. Experimental evaluations indicate that our proposed model produces reasonable results for REG. In summary, contributions of our paper include: • A two player online game to collect and verify natural language referring expressions. • A new large-scale dataset containing natural language </context>
<context position="18782" citStr="Rosch, 1978" startWordPosition="3004" endWordPosition="3005">lor (e.g. “blue”) and object size (e.g. “tiny”) respectively. Absolute location refers to the location of the object in the image (e.g. “top of the image”). Relative location relation and relative location object attributes allow for referring expressions that localize the object with respect to another object in the picture (e.g. “the car to the left of the tree”). Generic attributes cover all less frequently observed attribute types (e.g. “wooden” or “round”). The entry-level category attribute is related to the concept of entry-level categories first proposed by Psychologists in the 1970s (Rosch, 1978) and recently explored in visual recognition (Ordonez et al., 2013). The idea of entry-level categories is that an object can belong to many different categories; an indigo bunting is an oscine, a bird, a vertebrate, a chordate, and so on. But, a person looking at a picture of one would probably call it a bird (unless they are very familiar with ornithology). Therefore, we include this attribute to capture how people name object categories in referring expressions. Parsing the referring expressions: We parse the expressions using the most recent version of the StanfordCoreNLP parser (Socher et</context>
</contexts>
<marker>Rosch, 1978</marker>
<rawString>Eleanor Rosch. 1978. Principles of categorization. Cognition and Categorization, page 2748.</rawString>
</citation>
<citation valid="true">
<authors>
<author>Nitin Seemakurty</author>
<author>Jonathan Chu</author>
<author>Luis von Ahn</author>
<author>Anthony Tomasic</author>
</authors>
<title>Word sense disambiguation via human computation.</title>
<date>2010</date>
<booktitle>In Human Computation Workshop.</booktitle>
<marker>Seemakurty, Chu, von Ahn, Tomasic, 2010</marker>
<rawString>Nitin Seemakurty, Jonathan Chu, Luis von Ahn, and Anthony Tomasic. 2010. Word sense disambiguation via human computation. In Human Computation Workshop.</rawString>
</citation>
<citation valid="true">
<authors>
<author>Richard Socher</author>
<author>John Bauer</author>
<author>Christopher D Manning</author>
<author>Andrew Y Ng</author>
</authors>
<title>Parsing With Compositional Vector Grammars.</title>
<date>2013</date>
<booktitle>In Association for Computational Linguistics (ACL).</booktitle>
<contexts>
<context position="19393" citStr="Socher et al., 2013" startWordPosition="3104" endWordPosition="3107">ch, 1978) and recently explored in visual recognition (Ordonez et al., 2013). The idea of entry-level categories is that an object can belong to many different categories; an indigo bunting is an oscine, a bird, a vertebrate, a chordate, and so on. But, a person looking at a picture of one would probably call it a bird (unless they are very familiar with ornithology). Therefore, we include this attribute to capture how people name object categories in referring expressions. Parsing the referring expressions: We parse the expressions using the most recent version of the StanfordCoreNLP parser (Socher et al., 2013). We begin by traversing the parse tree in a breadth-first manner and selecting the head noun of the sentence to determine the object of the referring expression, denoted as subject word. We pre-define a dictionary of attribute-values (color word, size word, abs location word, rel location word) for each of the attributes based on the observed data using a combination of POS-tagging and manual labeling. We then apply a template-based approach on the collapsed dependency relations to recover the set of attributes (the main template rules are shown in Figure 2). The relationship rel indicates an</context>
</contexts>
<marker>Socher, Bauer, Manning, Ng, 2013</marker>
<rawString>Richard Socher, John Bauer, Christopher D. Manning, and Andrew Y. Ng. 2013. Parsing With Compositional Vector Grammars. In Association for Computational Linguistics (ACL).</rawString>
</citation>
<citation valid="true">
<authors>
<author>Kees van Deemter</author>
<author>Ielka van der Sluis</author>
<author>Albert Gatt</author>
</authors>
<title>Building a semantically transparent corpus for the generation of referring expressions.</title>
<date>2006</date>
<booktitle>In International Conference on Natural Language Generation (INLG).</booktitle>
<marker>van Deemter, van der Sluis, Gatt, 2006</marker>
<rawString>Kees van Deemter, Ielka van der Sluis, and Albert Gatt. 2006. Building a semantically transparent corpus for the generation of referring expressions. In International Conference on Natural Language Generation (INLG).</rawString>
</citation>
<citation valid="true">
<authors>
<author>Kees Van Deemter</author>
<author>Albert Gatt</author>
<author>Roger PG van Gompel</author>
<author>Emiel Krahmer</author>
</authors>
<title>Toward a computational psycholinguistics of reference production.</title>
<date>2012</date>
<booktitle>In Topics in Cognitive Science,</booktitle>
<volume>4</volume>
<issue>2</issue>
<pages>166183</pages>
<marker>Van Deemter, Gatt, van Gompel, Krahmer, 2012</marker>
<rawString>Kees Van Deemter, Albert Gatt, Roger PG van Gompel, and Emiel Krahmer. 2012. Toward a computational psycholinguistics of reference production. In Topics in Cognitive Science, volume 4(2), page 166183.</rawString>
</citation>
<citation valid="true">
<authors>
<author>Jette Viethen</author>
<author>Robert Dale</author>
</authors>
<title>The use of spatial relations in referring expression generation.</title>
<date>2008</date>
<booktitle>In International Natural Language Generation Conference (INLG).</booktitle>
<contexts>
<context position="2959" citStr="Viethen and Dale, 2008" startWordPosition="457" endWordPosition="460"> generation (REG). Previous work on REG has made significant progress toward understanding how people generate expressions to refer to objects (a recent survey of techniques is provided in Krahmer and van Deemter (2012)). In this paper, we study the relatively unexplored setting of how people refer to objects in complex photographs of real-world cluttered scenes. One initial stumbling block to examining this scenario is lack of existing relevant datasets, as previous collections for studying REG have used relatively focused domains such as graphics generated objects (van Deemter et al., 2006; Viethen and Dale, 2008), crafts (Mitchell et al., 2010), or small everyday (home and office) objects arrayed on a simple background (Mitchell et al., 2013a; FitzGerald et al., 2013). In this paper, we collect a new large-scale corpus, currently containing 130,525 expressions, referring to 96,654 distinct objects, in 19,894 photographs of real world scenes. Some examples from our dataset are shown in Figure 5. To construct this corpus efficiently, we design a new two player referring expression game (ReferItGame) to crowd-source the data collection. Popularized by efforts like the ESP game (von Ahn and Dabbish, 2004)</context>
<context position="7693" citStr="Viethen and Dale, 2008" startWordPosition="1193" endWordPosition="1196">et al., 2013; Van Deemter et al., 2012; Mitchell et al., 2013b), incorporating visual classifiers (Mitchell et al., 2011), producing algorithms to refer to object sets (Ren et al., 2010; FitzGerald et al., 2013), or examining impoverished perception REG (Fang et al., 2013). A good survey of work in this area is provided in Krahmer and van Deemter (2012). We build on past work, extending models to generate attributes jointly in a category specific framework. Referring Expression Datasets: Some initial datasets in REG used graphics engines to produce images of objects (van Deemter et al., 2006; Viethen and Dale, 2008). Recently more realistic datasets have been introduced, consisting of craft objects like pipecleaners, ribbons, and feathers (Mitchell et al., 2010), or everyday home and office objects such as staplers, combs, or rulers (Mitchell et al., 2013a), arrayed on a simple background. These datasets helped moved referring expression generation research into the domain of real world objects. We seek to further these pursuits by constructing a dataset of natural objects in photographs of the real world. Image &amp; Video Description Generation: Recent research on automatic image description has followed t</context>
</contexts>
<marker>Viethen, Dale, 2008</marker>
<rawString>Jette Viethen and Robert Dale. 2008. The use of spatial relations in referring expression generation. In International Natural Language Generation Conference (INLG).</rawString>
</citation>
<citation valid="true">
<authors>
<author>Jette Viethen</author>
<author>Robert Dale</author>
</authors>
<title>Speakerdependent variation in content selection for referring expression generation.</title>
<date>2010</date>
<booktitle>In Australasian Language Technology Workshop.</booktitle>
<contexts>
<context position="7060" citStr="Viethen and Dale, 2010" startWordPosition="1089" endWordPosition="1092"> of research on understanding how people generate referring expressions, dating back to the 1970s (Winograd, 1972). One common approach is the Incremental Algorithm (Dale and Reiter, 1995; Dale and Reiter, 2000) which uses logical expressions for generation. Much work in REG follows the Gricean maxims (Grice, 1975) which provide principles for how people will behave in conversation. Recently, there has been progress examining other aspects of the referring expression problem such as understanding what types of attributes are used (Mitchell et al., 2013a), modeling variations between speakers (Viethen and Dale, 2010; Viethen et al., 2013; Van Deemter et al., 2012; Mitchell et al., 2013b), incorporating visual classifiers (Mitchell et al., 2011), producing algorithms to refer to object sets (Ren et al., 2010; FitzGerald et al., 2013), or examining impoverished perception REG (Fang et al., 2013). A good survey of work in this area is provided in Krahmer and van Deemter (2012). We build on past work, extending models to generate attributes jointly in a category specific framework. Referring Expression Datasets: Some initial datasets in REG used graphics engines to produce images of objects (van Deemter et a</context>
</contexts>
<marker>Viethen, Dale, 2010</marker>
<rawString>Jette Viethen and Robert Dale. 2010. Speakerdependent variation in content selection for referring expression generation. In Australasian Language Technology Workshop.</rawString>
</citation>
<citation valid="true">
<authors>
<author>Jette Viethen</author>
<author>Margaret Mitchell</author>
<author>Emiel Krahmer</author>
</authors>
<title>Graphs and spatial relations in the generation of referring expressions.</title>
<date>2013</date>
<booktitle>In European Workshop on Natural Language Generation.</booktitle>
<contexts>
<context position="7082" citStr="Viethen et al., 2013" startWordPosition="1093" endWordPosition="1096">nding how people generate referring expressions, dating back to the 1970s (Winograd, 1972). One common approach is the Incremental Algorithm (Dale and Reiter, 1995; Dale and Reiter, 2000) which uses logical expressions for generation. Much work in REG follows the Gricean maxims (Grice, 1975) which provide principles for how people will behave in conversation. Recently, there has been progress examining other aspects of the referring expression problem such as understanding what types of attributes are used (Mitchell et al., 2013a), modeling variations between speakers (Viethen and Dale, 2010; Viethen et al., 2013; Van Deemter et al., 2012; Mitchell et al., 2013b), incorporating visual classifiers (Mitchell et al., 2011), producing algorithms to refer to object sets (Ren et al., 2010; FitzGerald et al., 2013), or examining impoverished perception REG (Fang et al., 2013). A good survey of work in this area is provided in Krahmer and van Deemter (2012). We build on past work, extending models to generate attributes jointly in a category specific framework. Referring Expression Datasets: Some initial datasets in REG used graphics engines to produce images of objects (van Deemter et al., 2006; Viethen and </context>
</contexts>
<marker>Viethen, Mitchell, Krahmer, 2013</marker>
<rawString>Jette Viethen, Margaret Mitchell, and Emiel Krahmer. 2013. Graphs and spatial relations in the generation of referring expressions. In European Workshop on Natural Language Generation.</rawString>
</citation>
<citation valid="true">
<authors>
<author>Luis von Ahn</author>
<author>Laura Dabbish</author>
</authors>
<title>Labeling images with a computer game.</title>
<date>2004</date>
<booktitle>In ACM Conf. on Human Factors in Computing Systems (CHI).</booktitle>
<marker>von Ahn, Dabbish, 2004</marker>
<rawString>Luis von Ahn and Laura Dabbish. 2004. Labeling images with a computer game. In ACM Conf. on Human Factors in Computing Systems (CHI).</rawString>
</citation>
<citation valid="true">
<authors>
<author>Luis von Ahn</author>
<author>Mihir Kedia</author>
<author>Manuel Blum</author>
</authors>
<title>Verbosity: A game for collecting common-sense knowledge.</title>
<date>2006</date>
<booktitle>In ACM Conference on Human Factors in Computing Systems (CHI).</booktitle>
<marker>von Ahn, Kedia, Blum, 2006</marker>
<rawString>Luis von Ahn, Mihir Kedia, and Manuel Blum. 2006a. Verbosity: A game for collecting common-sense knowledge. In ACM Conference on Human Factors in Computing Systems (CHI).</rawString>
</citation>
<citation valid="true">
<authors>
<author>Luis von Ahn</author>
<author>Ruoran Liu</author>
<author>Manuel Blum</author>
</authors>
<title>Peekaboom: A game for locating objects in images.</title>
<date>2006</date>
<booktitle>In ACM Conference on Human Factors in Computing Systems (CHI).</booktitle>
<marker>von Ahn, Liu, Blum, 2006</marker>
<rawString>Luis von Ahn, Ruoran Liu, and Manuel Blum. 2006b. Peekaboom: A game for locating objects in images. In ACM Conference on Human Factors in Computing Systems (CHI).</rawString>
</citation>
<citation valid="true">
<authors>
<author>Terry Winograd</author>
</authors>
<title>Understanding natural language.</title>
<date>1972</date>
<journal>Cognitive Psychology,</journal>
<volume>3</volume>
<issue>1</issue>
<contexts>
<context position="6552" citStr="Winograd, 1972" startWordPosition="1013" endWordPosition="1014">ions on three labeled test sets. The rest of the paper is organized as follows. First we outline related work from the vision and language communities (§2). Then we describe our online game for collecting referring expressions (§3) and provide an analysis of our new ReferItGame Dataset (§4). Finally, we present and evaluate our model for generating referring expressions (§5) and discuss conclusions and future work (§6). 2 Related Work Referring Expression Generation: There has been a long history of research on understanding how people generate referring expressions, dating back to the 1970s (Winograd, 1972). One common approach is the Incremental Algorithm (Dale and Reiter, 1995; Dale and Reiter, 2000) which uses logical expressions for generation. Much work in REG follows the Gricean maxims (Grice, 1975) which provide principles for how people will behave in conversation. Recently, there has been progress examining other aspects of the referring expression problem such as understanding what types of attributes are used (Mitchell et al., 2013a), modeling variations between speakers (Viethen and Dale, 2010; Viethen et al., 2013; Van Deemter et al., 2012; Mitchell et al., 2013b), incorporating vis</context>
</contexts>
<marker>Winograd, 1972</marker>
<rawString>Terry Winograd. 1972. Understanding natural language. Cognitive Psychology, 3(1):1191.</rawString>
</citation>
<citation valid="true">
<authors>
<author>Yezhou Yang</author>
</authors>
<title>Ching Lik Teo, Hal Daume III, and Yiannis Aloimonos.</title>
<date>2011</date>
<booktitle>In Empirical Methods on Natural Language Processing (EMNLP).</booktitle>
<marker>Yang, 2011</marker>
<rawString>Yezhou Yang, Ching Lik Teo, Hal Daume III, and Yiannis Aloimonos. 2011. Corpus-guided sentence generation of natural images. In Empirical Methods on Natural Language Processing (EMNLP).</rawString>
</citation>
<citation valid="true">
<authors>
<author>Benjamin Z Yao</author>
<author>Xiong Yang</author>
<author>Liang Lin</author>
<author>Mun Wai Lee</author>
<author>Song-Chun Zhu</author>
</authors>
<title>I2t: Image parsing to text description.</title>
<date>2010</date>
<booktitle>Proc. IEEE,</booktitle>
<volume>98</volume>
<issue>8</issue>
<contexts>
<context position="8598" citStr="Yao et al., 2010" startWordPosition="1340" endWordPosition="1343">tasets helped moved referring expression generation research into the domain of real world objects. We seek to further these pursuits by constructing a dataset of natural objects in photographs of the real world. Image &amp; Video Description Generation: Recent research on automatic image description has followed two main directions. Retrieval based methods (Aker and Gaizauskas, 2010; Farhadi et al., 2010; Ordonez et al., 2011; Feng and Lapata, 2010; Feng and Lapata, 2013) retrieve existing captions or phrases to describe a query image. Bottom up methods (Kulkarni et al., 2011; Yang et al., 2011; Yao et al., 2010) rely on visual classifiers to first recognize image content and then construct captions from scratch, perhaps with some 788 Figure 1: An example game. Player 1 (left) sees an image with an object outlined in red (the man) and provides a referring expression for the object (“man in red shirt on horse”). Player 2 (right) sees the image and the expression from Player 1 and must localize the correct object by clicking on it (click indicated by the red square). Elapsed time and current scores are also provided. input from natural language statistics. Very recently, these ideas have been extended t</context>
</contexts>
<marker>Yao, Yang, Lin, Lee, Zhu, 2010</marker>
<rawString>Benjamin Z. Yao, Xiong Yang, Liang Lin, Mun Wai Lee, and Song-Chun Zhu. 2010. I2t: Image parsing to text description. Proc. IEEE, 98(8).</rawString>
</citation>
</citationList>
</algorithm>
</algorithms>
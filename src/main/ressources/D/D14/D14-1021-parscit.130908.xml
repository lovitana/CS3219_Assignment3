<?xml version="1.0" encoding="UTF-8"?>
<algorithms version="110505">
<algorithm name="SectLabel" version="110505">
<variant no="0" confidence="0.003454">
<title confidence="0.987102">
Syntactic SMT Using a Discriminative Text Generation Model
</title>
<author confidence="0.956014">
Yue Zhang Kai Song* Linfeng Song*
</author>
<affiliation confidence="0.751167">
SUTD, Singapore NEU, China ICT/CAS, China
</affiliation>
<email confidence="0.624091">
yue zhang@sutd.edu.sg songkai.sk@alibaba-inc.com songlinfeng@ict.ac.cn
</email>
<author confidence="0.73978">
Jingbo Zhu Qun Liu
</author>
<affiliation confidence="0.679269">
NEU, China CNGL, Ireland and ICT/CAS, China
</affiliation>
<email confidence="0.989045">
zhujingbo@mail.neu.edu.cn qliu@computing.dcu.ie
</email>
<sectionHeader confidence="0.997264" genericHeader="abstract">
Abstract
</sectionHeader>
<bodyText confidence="0.999912857142857">
We study a novel architecture for syntactic
SMT. In contrast to the dominant approach
in the literature, the system does not rely
on translation rules, but treat translation
as an unconstrained target sentence gen-
eration task, using soft features to cap-
ture lexical and syntactic correspondences
between the source and target languages.
Target syntax features and bilingual trans-
lation features are trained consistently in
a discriminative model. Experiments us-
ing the IWSLT 2010 dataset show that the
system achieves BLEU comparable to the
state-of-the-art syntactic SMT systems.
</bodyText>
<sectionHeader confidence="0.999394" genericHeader="keywords">
1 Introduction
</sectionHeader>
<bodyText confidence="0.999038047619048">
Translation rules have been central to hierarchi-
cal phrase-based and syntactic statistical machine
translation (SMT) (Galley et al., 2004; Chiang,
2005; Liu et al., 2006; Quirk et al., 2005; Marcu et
al., 2006; Shen and Joshi, 2008; Xie et al., 2011).
They are attractive by capturing the recursiveness
of languages and syntactic correspondences be-
tween them. One important advantage of trans-
lation rules is that they allow efficient decoding
by treating MT as a statistical parsing task, trans-
forming a source sentence to its translation via re-
cursive rule application.
The efficiency takes root in the fact that target
word orders are encoded in translation rules. This
fact, however, also leads to rule explosion, noise
and coverage problems (Auli et al., 2009), which
can hurt translation quality. Flexibility of function
word usage, rich morphology and paraphrasing all
add to the difficulty of rule extraction. In addition,
restricting target word orders by hard translation
rules can also hurt output fluency.
</bodyText>
<note confidence="0.7776595">
∗* Work done while visiting Singapore University of
Technology and Design (SUTD)
</note>
<figureCaption confidence="0.999628">
Figure 1: Overall system architecture.
</figureCaption>
<bodyText confidence="0.9996325">
A potential solution to the problems above is to
treat translation as a generation task, represent-
ing syntactic correspondences using soft features.
Both adequacy and fluency can potentially be im-
proved by giving full flexibility to target synthe-
sis, and leaving all options to the statistical model.
The main challenge to this method is a signifi-
cant increase in the search space (Knight, 1999).
To this end, recent advances in tackling complex
search tasks for text generation offer some so-
lutions (White and Rajkumar, 2009; Zhang and
Clark, 2011).
In this short paper, we present a preliminary in-
vestigation on the possibility of building a syn-
tactic SMT system that does not use hard transla-
tion rules, by utilizing recent advances in statisti-
cal natural language generation (NLG). The over-
all architecture is shown in Figure 1. Translation
is performed by first parsing the source sentence,
then transferring source words and phrases to their
target equivalences, and finally synthesizing the
target output.
We choose dependency grammar for both the
source and the target syntax, and adapt the syntac-
tic text synthesis system of Zhang (2013), which
performs dependency-based linearization. The
linearization task for MT is different from the
monolingual task in that not all translation options
are used to build the output, and that bilingual cor-
respondences need to be taken into account dur-
</bodyText>
<page confidence="0.954384">
177
</page>
<bodyText confidence="0.919031692307692">
Proceedings of the 2014 Conference on Empirical Methods in Natural Language Processing (EMNLP), pages 177–182,
October 25-29, 2014, Doha, Qatar. c�2014 Association for Computational Linguistics
ing synthesis. The algorithms of Zhang (2013) are
modified to perform word selection as well as or-
dering, using two sets of features to control trans-
lation adequacy and fluency, respectively.
Preliminary experiments on the IWSLT1 2010
data show that the system gives BLEU compara-
ble to traditional tree-to-string and string-to-tree
translation systems. It demonstrates the feasibility
of leveraging statistical NLG techniques for SMT,
and the possibility of building a statistical transfer-
based MT system.
</bodyText>
<sectionHeader confidence="0.986517" genericHeader="introduction">
2 Approach
</sectionHeader>
<bodyText confidence="0.999773">
The main goal being proof of concept, we keep
the system simple by utilizing existing methods
for the main components, minimizing engineer-
ing efforts. Shown in Figure 1, the end-to-end
system consists of two main components: lexical
transfer and synthesis. The former provides can-
didate translations for (overlapping) source words
and phrases. Although lexicons and rules can
be used for this step, we take a simple statisti-
cal alignment-based approach. The latter searches
for a target translation by constructing dependency
trees bottom-up. The process can be viewed as
a syntax-based generation process from a bag of
overlapping translation options.
</bodyText>
<subsectionHeader confidence="0.998799">
2.1 Lexical transfer
</subsectionHeader>
<bodyText confidence="0.99722205">
We perform word alignment using IBM model 4
(Brown et al., 1993), and then extract phrase pairs
according to the alignment and automatically-
annotated target syntax. In particular, consistent
(Och et al., 1999) and cohesive (Fox, 2002) phrase
pairs are extracted from intersected alignments in
both directions: the target side must form a pro-
jective span, with a single root, and the source side
must be contiguous. A resulting phrase pair con-
sists of the source phrase, its target translation, as
well as the head position and head part-of-speech
(POS) of the target span, which are useful for tar-
get synthesis. We further restrict that neither the
source nor the target side of a valid phrase pair
contains over s words.
Given an input source sentence, the lexical
transfer unit finds all valid target translation op-
tions for overlapping source phrases up to size s,
and feeds them as inputs to the target synthesis de-
coder. The translation options with a probability
</bodyText>
<footnote confidence="0.99858">
1International Workshop on Spoken Language Transla-
tion, http://iwslt2010.fbk.eu
</footnote>
<bodyText confidence="0.993642">
below A · Pmax are filtered out, where Pmax is the
probability of the most probable translation. Here
the probability of a target translation is calculated
as the count of the translation divided by the count
of all translations of the source phrase.
</bodyText>
<subsectionHeader confidence="0.998213">
2.2 Synthesis
</subsectionHeader>
<bodyText confidence="0.999992272727273">
The synthesis module is based on the monolingual
text synthesis algorithm of Zhang (2013), which
constructs an ordered dependency tree given a bag
of words. In the bilingual setting, inputs to the al-
gorithm are translation options, which can be over-
lapping and mutually exclusive, and not necessar-
ily all of which are included in the output. As a
result, the decoder needs to perform word selec-
tion in addition to word ordering. Another differ-
ence between the bilingual and monolingual set-
tings is that the former requires translation ade-
quacy in addition to output fluency.
We largely rely on the monolingual system for
MT decoding. To deal with overlapping transla-
tion options, a source coverage vector is used to
impose mutual exclusiveness on input words and
phrases. Each element in the coverage vector is
a binary value that indicates whether a particular
source word has been translated in the correspond-
ing target hypothesis. For translation adequacy,
we use a set of bilingual features on top of the set
of monolingual features for text synthesis.
</bodyText>
<subsectionHeader confidence="0.876155">
2.2.1 Search
</subsectionHeader>
<bodyText confidence="0.999989428571428">
The search algorithm is the best-first algorithm of
Zhang (2013). Each search hypothesis is a par-
tial or full target-language dependency tree, and
hypotheses are constructed bottom-up from leaf
nodes, which are translation options. An agenda
is used to maintain a list of search hypothesis to
be expanded, and a chart is used to record a set
of accepted hypotheses. Initially empty, the chart
is a beam of size k · n, where n is the number
of source words and k is a positive integer. The
agenda is a priority queue, initialized with all leaf
hypotheses (i.e. translation options). At each step,
the highest-scored hypothesis e is popped off the
agenda, and expanded by combination with all hy-
potheses on the chart in all possible ways, with
the set of newly generated hypotheses e1, e2, ...eN
being put onto the agenda, and e being put onto
the chart. When two hypotheses are combined,
they can be put in two different orders, and in each
case different dependencies can be constructed be-
tween their head words, leading to different new
</bodyText>
<page confidence="0.890406">
178
</page>
<equation confidence="0.951276851851852">
dependency syntax
WORD(h) · POS(h) · NORM(size) ,
WORD(h) · NORM(size), POS(h) · NORM(size)
POS(h) · POS(m) · POS(b) · dir
POS(h) · POS(hl) · POS(m) · POS(mr) · dir (h &gt; m),
POS(h) · POS(hr) · POS(m) · POS(ml) · dir (h &lt; m)
WORD(h) · POS(m) · POS(ml) · dir,
WORD(h) · POS(m) · POS(mr) · dir
POS(h) · POS(m) · POS(m1) · dir,
POS(h) · POS(m1) · dir, POS(m) · POS(m1) · dir
WORD(h) · POS(m) · POS(m1) · POS(m2) · dir,
POS(h) · POS(m) · POS(m1) · POS(m2) · dir ,
...
dependency syntax for completed words
WORD(h) · POS(h) · WORD(hl) · POS(hl),
POS(h) · POS(hl),
WORD(h) · POS(h) · POS(hl),
POS(h) · WORD(hl) · POS(hl) ,
WORD(h) · POS(h) · WORD(hr) · POS(hr),
POS(h) · POS(hr),
...
surface string patterns (B—bordering index)
WORD(B − 1) · WORD(B), POS(B − 1) · POS(B),
WORD(B − 1) · POS(B), POS(B − 1) · WORD(B),
WORD(B − 1) · WORD(B) · WORD(B + 1),
WORD(B − 2) · WORD(B − 1) · WORD(B),
POS(B − 1) · POS(B) · POS(B + 1),
</equation>
<table confidence="0.9743025">
...
surface string patterns for complete sentences
WORD(0), WORD(0) · WORD(1),
WORD(size − 1),
WORD(size − 1) · WORD(size − 2),
POS(0), POS(0) · POS(1),
POS(0) · POS(1) · POS(2),
...
</table>
<tableCaption confidence="0.999051">
Table 1: Monolingual feature templates.
</tableCaption>
<bodyText confidence="0.99997175">
hypotheses. The decoder expands a fixed number
L hypotheses, and then takes the highest-scored
chart hypothesis that contains over Q · n words as
the output, where Q is a real number near 1.0.
</bodyText>
<subsubsectionHeader confidence="0.565166">
2.2.2 Model and training
</subsubsectionHeader>
<bodyText confidence="0.99982">
A scaled linear model is used by the decoder to
score search hypotheses:
</bodyText>
<equation confidence="0.935655">
Score(e) =
</equation>
<bodyText confidence="0.999841363636364">
where Φ(e) is the global feature vector of the hy-
pothesis e, θ� is the parameter vector of the model,
and |e |is the number of leaf nodes in e. The
scaling factor |e |is necessary because hypothe-
ses with different numbers of words are compared
with each other in the search process to capture
translation equivalence.
While the monolingual features of Zhang
(2013) are applied (example feature templates
from the system are shown in Table 1), an addi-
tional set of bilingual features is defined, shown
</bodyText>
<table confidence="0.975471">
phrase translation features
PHRASE(m) · PHRASE(t), P(trans),
bilingual syntactic features
</table>
<equation confidence="0.997698318181818">
POS(th) · POS(tm) · dir · LEN(path),
WORD(th) · POS(tm) · dir · LEN(path),
POS(th) · WORD(tm) · dir · LEN(path),
WORD(th) · WORD(tm) · dir · LEN(path),
WORD(sh) · WORD(sm) · dir · LEN(path),
WORD(sh) · WORD(th) · dir · LEN(path),
WORD(sm) · WORD(tm) · dir · LEN(path),
bilingual syntactic features (LEN(path) &lt; 3)
POS(th) · POS(tm) · dir · LABELS(path),
WORD(th) · POS(tm) · dir · LABELS(path),
POS(th) · WORD(tm) · dir · LABELS(path),
WORD(th) · WORD(tm) · dir · LABELS(path),
WORD(sh) · WORD(sm) · dir · LABELS(path),
WORD(sh) · WORD(th) · dir · LABELS(path),
WORD(sm) · WORD(tm) · dir · LABELS(path),
POS(th) · POS(tm) · dir · LABELSPOS(path),
WORD(th) · POS(tm) · dir · LABELSPOS(path),
POS(th) · WORD(tm) · dir · LABELSPOS(path),
WORD(th) · WORD(tm) · dir · LABELSPOS(path),
WORD(sh) · WORD(sm) · dir · LABELSPOS(path),
WORD(sh) · WORD(th) · dir · LABELSPOS(path),
WORD(sm) · WORD(tm) · dir · LABELSPOS(path),
</equation>
<tableCaption confidence="0.907875">
Table 2: Bilingual feature templates.
</tableCaption>
<bodyText confidence="0.999952033333333">
in Table 2. In the tables, s and t represent the
source and target, respectively; h and m repre-
sent the head and modifier in a dependency arc,
respectively; hl and hr represent the neighboring
words on the left and right of h, respectively; ml
and mr represent the neighboring words on the left
and right of m, respectively; m1 and m2 repre-
sent the closest and second closest sibling of m on
the side of h, respectively. dir represents the arc
direction (i.e. left or right); PHRASE represents
a lexical phrase; P(trans) represents the source-
to-target translation probability from the phrase-
table, used as a real-valued feature; path repre-
sents the shortest path in the source dependency
tree between the two nodes that correspond to the
target head and modifier, respectively; LEN(path)
represents the number of arcs on path, normalized
to bins of [5, 10, 20, 40+]; LABELS(path) repre-
sents the array of dependency arc labels on path;
LABELSPOS(path) represents the array of depen-
dency arc labels and source POS on path. In addi-
tion, a real-valued four-gram language model fea-
ture is also used, with four-grams extracted from
the surface boundary when two hypothesis are
combined.
We apply the discriminative learning algorithm
of Zhang (2013) to train the parameters B. The al-
gorithm requires training examples that consist of
full target derivations, with leaf nodes being input
translation options. However, the readily available
</bodyText>
<equation confidence="0.972699">
θ� · Φ(e)
|e |,
</equation>
<page confidence="0.987829">
179
</page>
<bodyText confidence="0.999952333333333">
training examples are automatically-parsed target
derivations, with leaf nodes being the reference
translation. As a result, we apply a search pro-
cedure to find a derivation process, through which
the target dependency tree is constructed from a
subset of input translation options. The search
procedure can be treated as a constrained decod-
ing process, where only the oracle tree and its sub
trees can be constructed. In case the set of transla-
tion options cannot lead to the oracle tree, we ig-
nore the training instance.2 Although the ignored
training sentence pairs cannot be utilized for train-
ing the discriminative synthesizer, they are never-
theless used for building the phrase table and train-
ing the language model.
</bodyText>
<sectionHeader confidence="0.999836" genericHeader="method">
3 Experiments
</sectionHeader>
<bodyText confidence="0.999850928571429">
We perform experiments on the IWSLT 2010
Chinese-English dataset, which consists of train-
ing sentence pairs from the dialog task (dialog)
and Basic Travel and Expression Corpus (BTEC).
The union of dialog and BTEC are taken as our
training set, which contains 30,033 sentence pairs.
For system tuning, we use the IWSLT 2004 test set
(also released as the second development test set
of IWSLT 2010), which contains 500 sentences.
For final test, we use the IWSLT 2003 test set (also
released as the first development test set of IWSLT
2010), which contains 506 sentences.
The Chinese sentences in the datasets are seg-
mented using NiuTrans3 (Xiao et al., 2012), while
POS-tagging of both English and Chinese is per-
formed using ZPar4 version 0.5 (Zhang and Clark,
2011). We train the English POS-tagger using the
WSJ sections of the Penn Treebank (Marcus et al.,
1993), turned into lower-case. For syntactic pars-
ing of both English and Chinese, we use the de-
fault models of ZPar 0.5.
We choose three baseline systems: a string-to-
tree (S2T) system, a tree-to-string (T2S) system
and a tree-to-tree (T2T) system (Koehn, 2010).
The Moses release 1.0 implementations of all
three systems are used, with default parameter set-
tings. IRSTLM5 release 5.80.03 (Federico et al.,
2008) is used to train a four-gram language models
</bodyText>
<footnote confidence="0.993358375">
2This led to the ignoring of over 40% of the training sen-
tence pairs. For future work, we will consider substitute or-
acles from reachable target derivations by using maximum
sentence level BLEU approximation (Nakov et al., 2012) or
METEOR (Denkowski and Lavie, 2011) as selection criteria.
3http://www.nlplab.com/NiuPlan/NiuTrans.ch.html
4http://sourceforge.net/projects/zpar/
5http://sourceforge.net/apps/mediawiki/irstlm
</footnote>
<table confidence="0.9742055">
System T2S S2T T2T OURS
BLEU 32.65 36.07 28.46 34.24
</table>
<tableCaption confidence="0.947171">
Table 3: Final results.
</tableCaption>
<table confidence="0.925506045454545">
SOURCE: A 现在 头痛 0 厉害 。
REF: I have a terrible headache .
OURS: now, I have a headache.
SOURCE: A 要 带 浴缸 0 双人A, 。
REF: I ’d like a twin room with a bath please .
OURS: a twin room, I ’ll find a room with a bath.
SOURCE: 请 -k, 日元 兑换 Vit, 美元 。
REF: can you change yen into dollars ?
OURS: please change yen into dollars.
SOURCE: 请 给 A 烤鸡 。
REF: roast chicken, please.
OURS: please have roast chicken.
SOURCE: 请 每 次 饭 后 吃 两 粒 。
REF: take two tablets after every meal.
OURS: please eat after each meal.
SOURCE: 请 结帐 。
REF: check, please.
OURS: I have to check - out, please.
SOURCE: 对 呀 那 是 本店 最 *+ 0 菜 啊 。
REF: yes , well, that ’s our specialty.
OURS: ah , the food that ’s right.
SOURCE: 空调 坏 了 。
</table>
<tableCaption confidence="0.702002666666667">
REF: my air conditioner is n’t working .
OURS: the air - conditioner does n’t work.
Table 4: Sample output sentences.
</tableCaption>
<bodyText confidence="0.99980116">
over the English training data, which is applied to
the baseline systems and our system. Kneser-Ney
smoothing is used to train the language model.
We use the tuning set to determine the optimal
number of training iterations. The translation op-
tion filter A is set to 0.1; the phrase size limit s is
set to 5 in order to verify the effectiveness of syn-
thesis; the number of expanded nodes L is set to
200; the chart factor k is set to 16 for a balance be-
tween efficiency and accuracy; the goal parameter
Q is set to 0.8.
The final scores of our system and the baselines
are shown in Table 3. Our system gives a BLEU
of 34.24, which is comparable to the baseline sys-
tems. Some example outputs are shown in Table 4.
Manual comparison does not show significant dif-
ferences in overall translation adequacy or fluency
between the outputs of the four systems. However,
an observation is that, while our system can pro-
duce more fluent outputs, the choice of translation
options can be more frequently incorrect. This
suggests that while the target synthesis component
is effective under the bilingual setting, a stronger
lexical selection component may be necessary for
better translation quality.
</bodyText>
<page confidence="0.997325">
180
</page>
<sectionHeader confidence="0.999871" genericHeader="method">
4 Related work
</sectionHeader>
<bodyText confidence="0.9996565">
As discussed in the introduction, our work is
closely related to previous studies on syntactic
MT, with the salient difference that we do not rely
on hard translation rules, but allow free target syn-
thesis. The contrast can be summarized as “trans-
lation by parsing” vs “translation by generation”.
There has been a line of research on genera-
tion for translation. Soricut and Marcu (2006) use
a form of weighted IDL-expressions (Nederhof
and Satta, 2004) for generation. Bangalore et al.
(2007) treats MT as a combination of global lex-
ical transfer and word ordering; their generation
component does not perform lexical selection, re-
lying on an n-gram language model to order target
words. Goto et al. (2012) use a monotonic phrase-
based system to perform target word selection, and
treats target ordering as a post-processing step.
More recently, Chen et al. (2014) translate source
dependencies arc-by-arc to generate pseudo target
dependencies, and generate the translation by re-
ordering of arcs. In contrast with these systems,
our system relies more heavily on a syntax-based
synthesis component, in order to study the useful-
ness of statistical NLG on SMT.
With respect to syntax-based word ordering,
Chang and Toutanova (2007) and He et al. (2009)
study a simplified word ordering problem by as-
suming that the un-ordered target dependency tree
is given. Wan et al. (2009) and Zhang and Clark
(2011) study the ordering of a bag of words, with-
out input syntax. Zhang et al. (2012), Zhang
(2013) and Song et al. (2014) further extended this
line of research by adding input syntax and allow-
ing joint inflection and ordering. de Gispert et al.
(2014) use a phrase-structure grammer for word
ordering. Our generation system is based on the
work of Zhang (2013), but further allows lexical
selection.
Our work is also in line with the work of Liang
et al. (2006), Blunsom et al. (2008), Flanigan et
al. (2013) and Yu et al. (2013) in that we build a
discriminative model for SMT.
</bodyText>
<sectionHeader confidence="0.996908" genericHeader="conclusions">
5 Conclusion
</sectionHeader>
<bodyText confidence="0.999948625">
We investigated a novel system for syntactic ma-
chine translation, treating MT as an unconstrained
generation task, solved by using a single discrim-
inative model with both monolingual syntax and
bilingual translation features. Syntactic corre-
spondence is captured by using soft features rather
than hard translation rules, which are used by most
syntax-based statistical methods in the literature.
Our results are preliminary in the sense that
the experiments were performed using a relatively
small dataset, and little engineering effort was
made on fine-tuning of parameters for the base-
line and proposed models. Our Python imple-
mentation gives the same level of BLEU scores
compared with baseline syntactic SMT systems,
but is an order of magnitude slower than Moses.
However, the results demonstrate the feasibility of
leveraging text generation techniques for machine
translation, directly connecting the two currently
rather separated research fields. The system is not
strongly dependent on the specific generation al-
gorithm, and one potential of the SMT architec-
ture is that it can directly benefit from advances in
statistical NLG technology.
</bodyText>
<sectionHeader confidence="0.981581" genericHeader="acknowledgments">
Acknowledgement
</sectionHeader>
<bodyText confidence="0.9999358">
The work has been supported by the Singa-
pore Ministration of Education Tier 2 project
T2MOE201301 and the startup grant SRG ISTD
2012 038 from SUTD. We thank the anonymous
reviewers for their constructive comments.
</bodyText>
<sectionHeader confidence="0.998935" genericHeader="references">
References
</sectionHeader>
<reference confidence="0.999359869565217">
Michael Auli, Adam Lopez, Hieu Hoang, and Philipp
Koehn. 2009. A systematic analysis of translation
model search spaces. In Proc. WMT, pages 224–
232.
Srinivas Bangalore, Patrick Haffner, and Stephan Kan-
thak. 2007. Statistical machine translation through
global lexical selection and sentence reconstruction.
In Proc. ACL, pages 152–159.
Phil Blunsom, Trevor Cohn, and Miles Osborne. 2008.
A discriminative latent variable model for statistical
machine translation. In Proc. ACL, pages 200–208.
Peter F. Brown, Stephen Della Pietra, Vincent J. Della
Pietra, and Robert L. Mercer. 1993. The mathe-
matics of statistical machine translation: Parameter
estimation. Computational Linguistics, 19(2):263–
311.
Pi-Chuan Chang and Kristina Toutanova. 2007. A dis-
criminative syntactic word order model for machine
translation. In Proc. ACL, pages 9–16.
Hongshen Chen, Jun Xie, Fandong Meng, Wenbin
Jiang, and Qun Liu. 2014. A dependency edge-
based transfer model for statistical machine transla-
tion. In Proc. COLING 2014, pages 1103–1113.
</reference>
<page confidence="0.996345">
181
</page>
<note confidence="0.754984">
Preslav Nakov, Francisco Guzman, and Stephan Vo-
gel. 2012. Optimizing for sentence-level BLEU+1
yields short translations. In Proc. Coling, pages
1979–1994.
David Chiang. 2005. A hierarchical phrase-based
model for statistical machine translation. In Proc.
ACL, pages 263–270.
</note>
<reference confidence="0.999137346938776">
Adri`a de Gispert, Marcus Tomalin, and Bill Byrne.
2014. Word ordering with phrase-based grammars.
In Proc. EACL, pages 259–268.
Michael Denkowski and Alon Lavie. 2011. Meteor
1.3: Automatic metric for reliable optimization and
evaluation of machine translation systems. In Proc.
WMT, pages 85–91.
Marcello Federico, Nicola Bertoldi, and Mauro Cet-
tolo. 2008. IRSTLM: an open source toolkit for
handling large scale language models. In Proc. In-
terspeech, pages 1618–1621.
Jeffrey Flanigan, Chris Dyer, and Jaime Carbonell.
2013. Large-scale discriminative training for statis-
tical machine translation using held-out line search.
In Proc. IAACL, pages 248–258.
Heidi Fox. 2002. Phrasal cohesion and statistical ma-
chine translation. In Proc. EMILP, pages 304–311.
Michel Galley, Mark Hopkins, Kevin Knight, and
Daniel Marcu. 2004. What’s in a translation rule?
In Proc. HLT-IAACL, pages 273–280.
Isao Goto, Masao Utiyama, and Eiichiro Sumita. 2012.
Post-ordering by parsing for Japanese-English sta-
tistical machine translation. In Proc. ACL, pages
311–316.
Wei He, Haifeng Wang, Yuqing Guo, and Ting Liu.
2009. Dependency based Chinese sentence realiza-
tion. In Proc. ACL/AFILP, pages 809–816.
Kevin Knight. 1999. Squibs and Discussions: Decod-
ing Complexity in Word-Replacement Translation
Models. Computational Linguistics, 25(4):607–
615.
Phillip Koehn. 2010. Statistical Machine Translation.
Cambridge University Press.
P. Liang, A. Bouchard-Cote, D. Klein, and B. Taskar.
2006. An end-to-end discriminative approach to
machine translation. In Proc. COLIIG/ACL, pages
761–768.
Yang Liu, Qun Liu, and Shouxun Lin. 2006. Tree-
to-string alignment template for statistical machine
translation. In Proc. COLIIG/ACL, pages 609–616.
Daniel Marcu, Wei Wang, Abdessamad Echihabi, and
Kevin Knight. 2006. SPMT: Statistical machine
translation with syntactified target language phrases.
In Proc. EMILP, pages 44–52.
Mitchell P. Marcus, Mary Ann Marcinkiewicz, and
Beatrice Santorini. 1993. Building a large anno-
tated corpus of English: The penn treebank. Com-
putational linguistics, 19(2):313–330.
Mark-Jan Nederhof and Giorgio Satta. 2004. Idl-
expressions: a formalism for representing and pars-
ing finite languages in natural language processing.
J. Artif. Intell. Res.(JAIR), 21:287–317.
Franz Josef Och, Christoph Tillmann, and Hermann
Ney. 1999. Improved alignment models for statis-
tical machine translation. In Proc. EMILP, pages
20–28.
Chris Quirk, Arul Menezes, and Colin Cherry. 2005.
Dependency treelet translation: Syntactically in-
formed phrasal smt. In Proc. ACL, pages 271–279.
Libin Shen and Aravind Joshi. 2008. LTAG depen-
dency parsing with bidirectional incremental con-
struction. In Proc. EMILP, pages 495–504.
Linfeng Song, Yue Zhang, Kai Song, and Qun Liu.
2014. Joint morphological generation and syntactic
linearization. In Proc. AAAI, pages 1522–1528.
Radu Soricut and Daniel Marcu. 2006. Stochastic lan-
guage generation using widl-expressions and its ap-
plication in machine translation and summarization.
In Proc. ACL, pages 1105–1112.
Stephen Wan, Mark Dras, Robert Dale, and C´ecile
Paris. 2009. Improving grammaticality in statisti-
cal sentence generation: Introducing a dependency
spanning tree algorithm with an argument satisfac-
tion model. In Proc. EACL, pages 852–860.
Michael White and Rajakrishnan Rajkumar. 2009.
Perceptron reranking for CCG realization. In Proc.
the EMILP, pages 410–419.
Tong Xiao, Jingbo Zhu, Hao Zhang, and Qiang Li.
2012. NiuTrans: An open source toolkit for phrase-
based and syntax-based machine translation. In
Proc. ACL Demos, pages 19–24.
Jun Xie, Haitao Mi, and Qun Liu. 2011. A novel
dependency-to-string model for statistical machine
translation. In Proc. EMILP, pages 216–226.
Heng Yu, Liang Huang, Haitao Mi, and Kai Zhao.
2013. Max-violation perceptron and forced decod-
ing for scalable MT training. In Proc. EMILP,
pages 1112–1123.
Yue Zhang and Stephen Clark. 2011. Syntax-based
grammaticality improvement using CCG and guided
search. In Proc. EMILP, pages 1147–1157.
Yue Zhang, Graeme Blackwood, and Stephen Clark.
2012. Syntax-based word ordering incorporating a
large-scale language model. In Proc. EACL, pages
736–746.
Yue Zhang. 2013. Partial-tree linearization: General-
ized word ordering for text synthesis. In Proc. IJ-
CAI, pages 2232–2238.
</reference>
<page confidence="0.998015">
182
</page>
</variant>
</algorithm>
<algorithm name="ParsHed" version="110505">
<variant no="0" confidence="0.130540">
<title confidence="0.996103">Syntactic SMT Using a Discriminative Text Generation Model</title>
<author confidence="0.990226">Zhang Kai Linfeng</author>
<affiliation confidence="0.465205333333333">SUTD, Singapore NEU, China ICT/CAS, China yue zhang@sutd.edu.sg songkai.sk@alibaba-inc.com songlinfeng@ict.ac.cn Jingbo Zhu Qun Liu</affiliation>
<address confidence="0.593909">NEU, China CNGL, Ireland and ICT/CAS, China</address>
<email confidence="0.666809">zhujingbo@mail.neu.edu.cnqliu@computing.dcu.ie</email>
<abstract confidence="0.999434">We study a novel architecture for syntactic SMT. In contrast to the dominant approach in the literature, the system does not rely on translation rules, but treat translation as an unconstrained target sentence generation task, using soft features to capture lexical and syntactic correspondences between the source and target languages. Target syntax features and bilingual translation features are trained consistently in a discriminative model. Experiments using the IWSLT 2010 dataset show that the system achieves BLEU comparable to the state-of-the-art syntactic SMT systems.</abstract>
</variant>
</algorithm>
<algorithm name="ParsCit" version="110505">
<citationList>
<citation valid="true">
<authors>
<author>Michael Auli</author>
<author>Adam Lopez</author>
<author>Hieu Hoang</author>
<author>Philipp Koehn</author>
</authors>
<title>A systematic analysis of translation model search spaces.</title>
<date>2009</date>
<booktitle>In Proc. WMT,</booktitle>
<pages>224--232</pages>
<contexts>
<context position="1686" citStr="Auli et al., 2009" startWordPosition="245" endWordPosition="248"> 2005; Liu et al., 2006; Quirk et al., 2005; Marcu et al., 2006; Shen and Joshi, 2008; Xie et al., 2011). They are attractive by capturing the recursiveness of languages and syntactic correspondences between them. One important advantage of translation rules is that they allow efficient decoding by treating MT as a statistical parsing task, transforming a source sentence to its translation via recursive rule application. The efficiency takes root in the fact that target word orders are encoded in translation rules. This fact, however, also leads to rule explosion, noise and coverage problems (Auli et al., 2009), which can hurt translation quality. Flexibility of function word usage, rich morphology and paraphrasing all add to the difficulty of rule extraction. In addition, restricting target word orders by hard translation rules can also hurt output fluency. ∗* Work done while visiting Singapore University of Technology and Design (SUTD) Figure 1: Overall system architecture. A potential solution to the problems above is to treat translation as a generation task, representing syntactic correspondences using soft features. Both adequacy and fluency can potentially be improved by giving full flexibili</context>
</contexts>
<marker>Auli, Lopez, Hoang, Koehn, 2009</marker>
<rawString>Michael Auli, Adam Lopez, Hieu Hoang, and Philipp Koehn. 2009. A systematic analysis of translation model search spaces. In Proc. WMT, pages 224– 232.</rawString>
</citation>
<citation valid="true">
<authors>
<author>Srinivas Bangalore</author>
<author>Patrick Haffner</author>
<author>Stephan Kanthak</author>
</authors>
<title>Statistical machine translation through global lexical selection and sentence reconstruction.</title>
<date>2007</date>
<booktitle>In Proc. ACL,</booktitle>
<pages>152--159</pages>
<contexts>
<context position="17779" citStr="Bangalore et al. (2007)" startWordPosition="2945" endWordPosition="2948">the bilingual setting, a stronger lexical selection component may be necessary for better translation quality. 180 4 Related work As discussed in the introduction, our work is closely related to previous studies on syntactic MT, with the salient difference that we do not rely on hard translation rules, but allow free target synthesis. The contrast can be summarized as “translation by parsing” vs “translation by generation”. There has been a line of research on generation for translation. Soricut and Marcu (2006) use a form of weighted IDL-expressions (Nederhof and Satta, 2004) for generation. Bangalore et al. (2007) treats MT as a combination of global lexical transfer and word ordering; their generation component does not perform lexical selection, relying on an n-gram language model to order target words. Goto et al. (2012) use a monotonic phrasebased system to perform target word selection, and treats target ordering as a post-processing step. More recently, Chen et al. (2014) translate source dependencies arc-by-arc to generate pseudo target dependencies, and generate the translation by reordering of arcs. In contrast with these systems, our system relies more heavily on a syntax-based synthesis comp</context>
</contexts>
<marker>Bangalore, Haffner, Kanthak, 2007</marker>
<rawString>Srinivas Bangalore, Patrick Haffner, and Stephan Kanthak. 2007. Statistical machine translation through global lexical selection and sentence reconstruction. In Proc. ACL, pages 152–159.</rawString>
</citation>
<citation valid="true">
<authors>
<author>Phil Blunsom</author>
<author>Trevor Cohn</author>
<author>Miles Osborne</author>
</authors>
<title>A discriminative latent variable model for statistical machine translation.</title>
<date>2008</date>
<booktitle>In Proc. ACL,</booktitle>
<pages>200--208</pages>
<contexts>
<context position="19169" citStr="Blunsom et al. (2008)" startWordPosition="3178" endWordPosition="3181">mplified word ordering problem by assuming that the un-ordered target dependency tree is given. Wan et al. (2009) and Zhang and Clark (2011) study the ordering of a bag of words, without input syntax. Zhang et al. (2012), Zhang (2013) and Song et al. (2014) further extended this line of research by adding input syntax and allowing joint inflection and ordering. de Gispert et al. (2014) use a phrase-structure grammer for word ordering. Our generation system is based on the work of Zhang (2013), but further allows lexical selection. Our work is also in line with the work of Liang et al. (2006), Blunsom et al. (2008), Flanigan et al. (2013) and Yu et al. (2013) in that we build a discriminative model for SMT. 5 Conclusion We investigated a novel system for syntactic machine translation, treating MT as an unconstrained generation task, solved by using a single discriminative model with both monolingual syntax and bilingual translation features. Syntactic correspondence is captured by using soft features rather than hard translation rules, which are used by most syntax-based statistical methods in the literature. Our results are preliminary in the sense that the experiments were performed using a relatively</context>
</contexts>
<marker>Blunsom, Cohn, Osborne, 2008</marker>
<rawString>Phil Blunsom, Trevor Cohn, and Miles Osborne. 2008. A discriminative latent variable model for statistical machine translation. In Proc. ACL, pages 200–208.</rawString>
</citation>
<citation valid="true">
<authors>
<author>Peter F Brown</author>
<author>Stephen Della Pietra</author>
<author>Vincent J Della Pietra</author>
<author>Robert L Mercer</author>
</authors>
<title>The mathematics of statistical machine translation: Parameter estimation.</title>
<date>1993</date>
<journal>Computational Linguistics,</journal>
<volume>19</volume>
<issue>2</issue>
<pages>311</pages>
<contexts>
<context position="4910" citStr="Brown et al., 1993" startWordPosition="740" endWordPosition="743">minimizing engineering efforts. Shown in Figure 1, the end-to-end system consists of two main components: lexical transfer and synthesis. The former provides candidate translations for (overlapping) source words and phrases. Although lexicons and rules can be used for this step, we take a simple statistical alignment-based approach. The latter searches for a target translation by constructing dependency trees bottom-up. The process can be viewed as a syntax-based generation process from a bag of overlapping translation options. 2.1 Lexical transfer We perform word alignment using IBM model 4 (Brown et al., 1993), and then extract phrase pairs according to the alignment and automaticallyannotated target syntax. In particular, consistent (Och et al., 1999) and cohesive (Fox, 2002) phrase pairs are extracted from intersected alignments in both directions: the target side must form a projective span, with a single root, and the source side must be contiguous. A resulting phrase pair consists of the source phrase, its target translation, as well as the head position and head part-of-speech (POS) of the target span, which are useful for target synthesis. We further restrict that neither the source nor the </context>
</contexts>
<marker>Brown, Pietra, Pietra, Mercer, 1993</marker>
<rawString>Peter F. Brown, Stephen Della Pietra, Vincent J. Della Pietra, and Robert L. Mercer. 1993. The mathematics of statistical machine translation: Parameter estimation. Computational Linguistics, 19(2):263– 311.</rawString>
</citation>
<citation valid="true">
<authors>
<author>Pi-Chuan Chang</author>
<author>Kristina Toutanova</author>
</authors>
<title>A discriminative syntactic word order model for machine translation.</title>
<date>2007</date>
<booktitle>In Proc. ACL,</booktitle>
<pages>9--16</pages>
<contexts>
<context position="18516" citStr="Chang and Toutanova (2007)" startWordPosition="3061" endWordPosition="3064">erform lexical selection, relying on an n-gram language model to order target words. Goto et al. (2012) use a monotonic phrasebased system to perform target word selection, and treats target ordering as a post-processing step. More recently, Chen et al. (2014) translate source dependencies arc-by-arc to generate pseudo target dependencies, and generate the translation by reordering of arcs. In contrast with these systems, our system relies more heavily on a syntax-based synthesis component, in order to study the usefulness of statistical NLG on SMT. With respect to syntax-based word ordering, Chang and Toutanova (2007) and He et al. (2009) study a simplified word ordering problem by assuming that the un-ordered target dependency tree is given. Wan et al. (2009) and Zhang and Clark (2011) study the ordering of a bag of words, without input syntax. Zhang et al. (2012), Zhang (2013) and Song et al. (2014) further extended this line of research by adding input syntax and allowing joint inflection and ordering. de Gispert et al. (2014) use a phrase-structure grammer for word ordering. Our generation system is based on the work of Zhang (2013), but further allows lexical selection. Our work is also in line with t</context>
</contexts>
<marker>Chang, Toutanova, 2007</marker>
<rawString>Pi-Chuan Chang and Kristina Toutanova. 2007. A discriminative syntactic word order model for machine translation. In Proc. ACL, pages 9–16.</rawString>
</citation>
<citation valid="true">
<authors>
<author>Hongshen Chen</author>
<author>Jun Xie</author>
<author>Fandong Meng</author>
<author>Wenbin Jiang</author>
<author>Qun Liu</author>
</authors>
<title>A dependency edgebased transfer model for statistical machine translation.</title>
<date>2014</date>
<booktitle>In Proc. COLING 2014,</booktitle>
<pages>1103--1113</pages>
<contexts>
<context position="18150" citStr="Chen et al. (2014)" startWordPosition="3006" endWordPosition="3009">slation by parsing” vs “translation by generation”. There has been a line of research on generation for translation. Soricut and Marcu (2006) use a form of weighted IDL-expressions (Nederhof and Satta, 2004) for generation. Bangalore et al. (2007) treats MT as a combination of global lexical transfer and word ordering; their generation component does not perform lexical selection, relying on an n-gram language model to order target words. Goto et al. (2012) use a monotonic phrasebased system to perform target word selection, and treats target ordering as a post-processing step. More recently, Chen et al. (2014) translate source dependencies arc-by-arc to generate pseudo target dependencies, and generate the translation by reordering of arcs. In contrast with these systems, our system relies more heavily on a syntax-based synthesis component, in order to study the usefulness of statistical NLG on SMT. With respect to syntax-based word ordering, Chang and Toutanova (2007) and He et al. (2009) study a simplified word ordering problem by assuming that the un-ordered target dependency tree is given. Wan et al. (2009) and Zhang and Clark (2011) study the ordering of a bag of words, without input syntax. Z</context>
</contexts>
<marker>Chen, Xie, Meng, Jiang, Liu, 2014</marker>
<rawString>Hongshen Chen, Jun Xie, Fandong Meng, Wenbin Jiang, and Qun Liu. 2014. A dependency edgebased transfer model for statistical machine translation. In Proc. COLING 2014, pages 1103–1113.</rawString>
</citation>
<citation valid="true">
<authors>
<author>Adri`a de Gispert</author>
<author>Marcus Tomalin</author>
<author>Bill Byrne</author>
</authors>
<title>Word ordering with phrase-based grammars.</title>
<date>2014</date>
<booktitle>In Proc. EACL,</booktitle>
<pages>259--268</pages>
<marker>de Gispert, Tomalin, Byrne, 2014</marker>
<rawString>Adri`a de Gispert, Marcus Tomalin, and Bill Byrne. 2014. Word ordering with phrase-based grammars. In Proc. EACL, pages 259–268.</rawString>
</citation>
<citation valid="true">
<authors>
<author>Michael Denkowski</author>
<author>Alon Lavie</author>
</authors>
<title>Meteor 1.3: Automatic metric for reliable optimization and evaluation of machine translation systems.</title>
<date>2011</date>
<booktitle>In Proc. WMT,</booktitle>
<pages>85--91</pages>
<contexts>
<context position="15031" citStr="Denkowski and Lavie, 2011" startWordPosition="2452" endWordPosition="2455">ult models of ZPar 0.5. We choose three baseline systems: a string-totree (S2T) system, a tree-to-string (T2S) system and a tree-to-tree (T2T) system (Koehn, 2010). The Moses release 1.0 implementations of all three systems are used, with default parameter settings. IRSTLM5 release 5.80.03 (Federico et al., 2008) is used to train a four-gram language models 2This led to the ignoring of over 40% of the training sentence pairs. For future work, we will consider substitute oracles from reachable target derivations by using maximum sentence level BLEU approximation (Nakov et al., 2012) or METEOR (Denkowski and Lavie, 2011) as selection criteria. 3http://www.nlplab.com/NiuPlan/NiuTrans.ch.html 4http://sourceforge.net/projects/zpar/ 5http://sourceforge.net/apps/mediawiki/irstlm System T2S S2T T2T OURS BLEU 32.65 36.07 28.46 34.24 Table 3: Final results. SOURCE: A 现在 头痛 0 厉害 。 REF: I have a terrible headache . OURS: now, I have a headache. SOURCE: A 要 带 浴缸 0 双人A, 。 REF: I ’d like a twin room with a bath please . OURS: a twin room, I ’ll find a room with a bath. SOURCE: 请 -k, 日元 兑换 Vit, 美元 。 REF: can you change yen into dollars ? OURS: please change yen into dollars. SOURCE: 请 给 A 烤鸡 。 REF: roast chicken, please. O</context>
</contexts>
<marker>Denkowski, Lavie, 2011</marker>
<rawString>Michael Denkowski and Alon Lavie. 2011. Meteor 1.3: Automatic metric for reliable optimization and evaluation of machine translation systems. In Proc. WMT, pages 85–91.</rawString>
</citation>
<citation valid="true">
<authors>
<author>Marcello Federico</author>
<author>Nicola Bertoldi</author>
<author>Mauro Cettolo</author>
</authors>
<title>IRSTLM: an open source toolkit for handling large scale language models.</title>
<date>2008</date>
<booktitle>In Proc. Interspeech,</booktitle>
<pages>1618--1621</pages>
<contexts>
<context position="14719" citStr="Federico et al., 2008" startWordPosition="2400" endWordPosition="2403">l., 2012), while POS-tagging of both English and Chinese is performed using ZPar4 version 0.5 (Zhang and Clark, 2011). We train the English POS-tagger using the WSJ sections of the Penn Treebank (Marcus et al., 1993), turned into lower-case. For syntactic parsing of both English and Chinese, we use the default models of ZPar 0.5. We choose three baseline systems: a string-totree (S2T) system, a tree-to-string (T2S) system and a tree-to-tree (T2T) system (Koehn, 2010). The Moses release 1.0 implementations of all three systems are used, with default parameter settings. IRSTLM5 release 5.80.03 (Federico et al., 2008) is used to train a four-gram language models 2This led to the ignoring of over 40% of the training sentence pairs. For future work, we will consider substitute oracles from reachable target derivations by using maximum sentence level BLEU approximation (Nakov et al., 2012) or METEOR (Denkowski and Lavie, 2011) as selection criteria. 3http://www.nlplab.com/NiuPlan/NiuTrans.ch.html 4http://sourceforge.net/projects/zpar/ 5http://sourceforge.net/apps/mediawiki/irstlm System T2S S2T T2T OURS BLEU 32.65 36.07 28.46 34.24 Table 3: Final results. SOURCE: A 现在 头痛 0 厉害 。 REF: I have a terrible headache</context>
</contexts>
<marker>Federico, Bertoldi, Cettolo, 2008</marker>
<rawString>Marcello Federico, Nicola Bertoldi, and Mauro Cettolo. 2008. IRSTLM: an open source toolkit for handling large scale language models. In Proc. Interspeech, pages 1618–1621.</rawString>
</citation>
<citation valid="true">
<authors>
<author>Jeffrey Flanigan</author>
<author>Chris Dyer</author>
<author>Jaime Carbonell</author>
</authors>
<title>Large-scale discriminative training for statistical machine translation using held-out line search.</title>
<date>2013</date>
<booktitle>In Proc. IAACL,</booktitle>
<pages>248--258</pages>
<contexts>
<context position="19193" citStr="Flanigan et al. (2013)" startWordPosition="3182" endWordPosition="3185">problem by assuming that the un-ordered target dependency tree is given. Wan et al. (2009) and Zhang and Clark (2011) study the ordering of a bag of words, without input syntax. Zhang et al. (2012), Zhang (2013) and Song et al. (2014) further extended this line of research by adding input syntax and allowing joint inflection and ordering. de Gispert et al. (2014) use a phrase-structure grammer for word ordering. Our generation system is based on the work of Zhang (2013), but further allows lexical selection. Our work is also in line with the work of Liang et al. (2006), Blunsom et al. (2008), Flanigan et al. (2013) and Yu et al. (2013) in that we build a discriminative model for SMT. 5 Conclusion We investigated a novel system for syntactic machine translation, treating MT as an unconstrained generation task, solved by using a single discriminative model with both monolingual syntax and bilingual translation features. Syntactic correspondence is captured by using soft features rather than hard translation rules, which are used by most syntax-based statistical methods in the literature. Our results are preliminary in the sense that the experiments were performed using a relatively small dataset, and litt</context>
</contexts>
<marker>Flanigan, Dyer, Carbonell, 2013</marker>
<rawString>Jeffrey Flanigan, Chris Dyer, and Jaime Carbonell. 2013. Large-scale discriminative training for statistical machine translation using held-out line search. In Proc. IAACL, pages 248–258.</rawString>
</citation>
<citation valid="true">
<authors>
<author>Heidi Fox</author>
</authors>
<title>Phrasal cohesion and statistical machine translation.</title>
<date>2002</date>
<booktitle>In Proc. EMILP,</booktitle>
<pages>304--311</pages>
<contexts>
<context position="5080" citStr="Fox, 2002" startWordPosition="767" endWordPosition="768">s for (overlapping) source words and phrases. Although lexicons and rules can be used for this step, we take a simple statistical alignment-based approach. The latter searches for a target translation by constructing dependency trees bottom-up. The process can be viewed as a syntax-based generation process from a bag of overlapping translation options. 2.1 Lexical transfer We perform word alignment using IBM model 4 (Brown et al., 1993), and then extract phrase pairs according to the alignment and automaticallyannotated target syntax. In particular, consistent (Och et al., 1999) and cohesive (Fox, 2002) phrase pairs are extracted from intersected alignments in both directions: the target side must form a projective span, with a single root, and the source side must be contiguous. A resulting phrase pair consists of the source phrase, its target translation, as well as the head position and head part-of-speech (POS) of the target span, which are useful for target synthesis. We further restrict that neither the source nor the target side of a valid phrase pair contains over s words. Given an input source sentence, the lexical transfer unit finds all valid target translation options for overlap</context>
</contexts>
<marker>Fox, 2002</marker>
<rawString>Heidi Fox. 2002. Phrasal cohesion and statistical machine translation. In Proc. EMILP, pages 304–311.</rawString>
</citation>
<citation valid="true">
<authors>
<author>Michel Galley</author>
<author>Mark Hopkins</author>
<author>Kevin Knight</author>
<author>Daniel Marcu</author>
</authors>
<title>What’s in a translation rule? In</title>
<date>2004</date>
<booktitle>Proc. HLT-IAACL,</booktitle>
<pages>273--280</pages>
<contexts>
<context position="1059" citStr="Galley et al., 2004" startWordPosition="142" endWordPosition="145">m does not rely on translation rules, but treat translation as an unconstrained target sentence generation task, using soft features to capture lexical and syntactic correspondences between the source and target languages. Target syntax features and bilingual translation features are trained consistently in a discriminative model. Experiments using the IWSLT 2010 dataset show that the system achieves BLEU comparable to the state-of-the-art syntactic SMT systems. 1 Introduction Translation rules have been central to hierarchical phrase-based and syntactic statistical machine translation (SMT) (Galley et al., 2004; Chiang, 2005; Liu et al., 2006; Quirk et al., 2005; Marcu et al., 2006; Shen and Joshi, 2008; Xie et al., 2011). They are attractive by capturing the recursiveness of languages and syntactic correspondences between them. One important advantage of translation rules is that they allow efficient decoding by treating MT as a statistical parsing task, transforming a source sentence to its translation via recursive rule application. The efficiency takes root in the fact that target word orders are encoded in translation rules. This fact, however, also leads to rule explosion, noise and coverage p</context>
</contexts>
<marker>Galley, Hopkins, Knight, Marcu, 2004</marker>
<rawString>Michel Galley, Mark Hopkins, Kevin Knight, and Daniel Marcu. 2004. What’s in a translation rule? In Proc. HLT-IAACL, pages 273–280.</rawString>
</citation>
<citation valid="true">
<authors>
<author>Isao Goto</author>
<author>Masao Utiyama</author>
<author>Eiichiro Sumita</author>
</authors>
<title>Post-ordering by parsing for Japanese-English statistical machine translation.</title>
<date>2012</date>
<booktitle>In Proc. ACL,</booktitle>
<pages>311--316</pages>
<contexts>
<context position="17993" citStr="Goto et al. (2012)" startWordPosition="2981" endWordPosition="2984">actic MT, with the salient difference that we do not rely on hard translation rules, but allow free target synthesis. The contrast can be summarized as “translation by parsing” vs “translation by generation”. There has been a line of research on generation for translation. Soricut and Marcu (2006) use a form of weighted IDL-expressions (Nederhof and Satta, 2004) for generation. Bangalore et al. (2007) treats MT as a combination of global lexical transfer and word ordering; their generation component does not perform lexical selection, relying on an n-gram language model to order target words. Goto et al. (2012) use a monotonic phrasebased system to perform target word selection, and treats target ordering as a post-processing step. More recently, Chen et al. (2014) translate source dependencies arc-by-arc to generate pseudo target dependencies, and generate the translation by reordering of arcs. In contrast with these systems, our system relies more heavily on a syntax-based synthesis component, in order to study the usefulness of statistical NLG on SMT. With respect to syntax-based word ordering, Chang and Toutanova (2007) and He et al. (2009) study a simplified word ordering problem by assuming th</context>
</contexts>
<marker>Goto, Utiyama, Sumita, 2012</marker>
<rawString>Isao Goto, Masao Utiyama, and Eiichiro Sumita. 2012. Post-ordering by parsing for Japanese-English statistical machine translation. In Proc. ACL, pages 311–316.</rawString>
</citation>
<citation valid="true">
<authors>
<author>Wei He</author>
<author>Haifeng Wang</author>
<author>Yuqing Guo</author>
<author>Ting Liu</author>
</authors>
<title>Dependency based Chinese sentence realization.</title>
<date>2009</date>
<booktitle>In Proc. ACL/AFILP,</booktitle>
<pages>809--816</pages>
<contexts>
<context position="18537" citStr="He et al. (2009)" startWordPosition="3066" endWordPosition="3069">ng on an n-gram language model to order target words. Goto et al. (2012) use a monotonic phrasebased system to perform target word selection, and treats target ordering as a post-processing step. More recently, Chen et al. (2014) translate source dependencies arc-by-arc to generate pseudo target dependencies, and generate the translation by reordering of arcs. In contrast with these systems, our system relies more heavily on a syntax-based synthesis component, in order to study the usefulness of statistical NLG on SMT. With respect to syntax-based word ordering, Chang and Toutanova (2007) and He et al. (2009) study a simplified word ordering problem by assuming that the un-ordered target dependency tree is given. Wan et al. (2009) and Zhang and Clark (2011) study the ordering of a bag of words, without input syntax. Zhang et al. (2012), Zhang (2013) and Song et al. (2014) further extended this line of research by adding input syntax and allowing joint inflection and ordering. de Gispert et al. (2014) use a phrase-structure grammer for word ordering. Our generation system is based on the work of Zhang (2013), but further allows lexical selection. Our work is also in line with the work of Liang et a</context>
</contexts>
<marker>He, Wang, Guo, Liu, 2009</marker>
<rawString>Wei He, Haifeng Wang, Yuqing Guo, and Ting Liu. 2009. Dependency based Chinese sentence realization. In Proc. ACL/AFILP, pages 809–816.</rawString>
</citation>
<citation valid="true">
<authors>
<author>Kevin Knight</author>
</authors>
<title>Squibs and Discussions: Decoding Complexity in Word-Replacement Translation Models.</title>
<date>1999</date>
<journal>Computational Linguistics,</journal>
<volume>25</volume>
<issue>4</issue>
<pages>615</pages>
<contexts>
<context position="2454" citStr="Knight, 1999" startWordPosition="365" endWordPosition="366"> addition, restricting target word orders by hard translation rules can also hurt output fluency. ∗* Work done while visiting Singapore University of Technology and Design (SUTD) Figure 1: Overall system architecture. A potential solution to the problems above is to treat translation as a generation task, representing syntactic correspondences using soft features. Both adequacy and fluency can potentially be improved by giving full flexibility to target synthesis, and leaving all options to the statistical model. The main challenge to this method is a significant increase in the search space (Knight, 1999). To this end, recent advances in tackling complex search tasks for text generation offer some solutions (White and Rajkumar, 2009; Zhang and Clark, 2011). In this short paper, we present a preliminary investigation on the possibility of building a syntactic SMT system that does not use hard translation rules, by utilizing recent advances in statistical natural language generation (NLG). The overall architecture is shown in Figure 1. Translation is performed by first parsing the source sentence, then transferring source words and phrases to their target equivalences, and finally synthesizing t</context>
</contexts>
<marker>Knight, 1999</marker>
<rawString>Kevin Knight. 1999. Squibs and Discussions: Decoding Complexity in Word-Replacement Translation Models. Computational Linguistics, 25(4):607– 615.</rawString>
</citation>
<citation valid="true">
<authors>
<author>Phillip Koehn</author>
</authors>
<title>Statistical Machine Translation.</title>
<date>2010</date>
<publisher>Cambridge University Press.</publisher>
<contexts>
<context position="14568" citStr="Koehn, 2010" startWordPosition="2379" endWordPosition="2380">lopment test set of IWSLT 2010), which contains 506 sentences. The Chinese sentences in the datasets are segmented using NiuTrans3 (Xiao et al., 2012), while POS-tagging of both English and Chinese is performed using ZPar4 version 0.5 (Zhang and Clark, 2011). We train the English POS-tagger using the WSJ sections of the Penn Treebank (Marcus et al., 1993), turned into lower-case. For syntactic parsing of both English and Chinese, we use the default models of ZPar 0.5. We choose three baseline systems: a string-totree (S2T) system, a tree-to-string (T2S) system and a tree-to-tree (T2T) system (Koehn, 2010). The Moses release 1.0 implementations of all three systems are used, with default parameter settings. IRSTLM5 release 5.80.03 (Federico et al., 2008) is used to train a four-gram language models 2This led to the ignoring of over 40% of the training sentence pairs. For future work, we will consider substitute oracles from reachable target derivations by using maximum sentence level BLEU approximation (Nakov et al., 2012) or METEOR (Denkowski and Lavie, 2011) as selection criteria. 3http://www.nlplab.com/NiuPlan/NiuTrans.ch.html 4http://sourceforge.net/projects/zpar/ 5http://sourceforge.net/ap</context>
</contexts>
<marker>Koehn, 2010</marker>
<rawString>Phillip Koehn. 2010. Statistical Machine Translation. Cambridge University Press.</rawString>
</citation>
<citation valid="true">
<authors>
<author>P Liang</author>
<author>A Bouchard-Cote</author>
<author>D Klein</author>
<author>B Taskar</author>
</authors>
<title>An end-to-end discriminative approach to machine translation.</title>
<date>2006</date>
<booktitle>In Proc. COLIIG/ACL,</booktitle>
<pages>761--768</pages>
<contexts>
<context position="19146" citStr="Liang et al. (2006)" startWordPosition="3174" endWordPosition="3177">al. (2009) study a simplified word ordering problem by assuming that the un-ordered target dependency tree is given. Wan et al. (2009) and Zhang and Clark (2011) study the ordering of a bag of words, without input syntax. Zhang et al. (2012), Zhang (2013) and Song et al. (2014) further extended this line of research by adding input syntax and allowing joint inflection and ordering. de Gispert et al. (2014) use a phrase-structure grammer for word ordering. Our generation system is based on the work of Zhang (2013), but further allows lexical selection. Our work is also in line with the work of Liang et al. (2006), Blunsom et al. (2008), Flanigan et al. (2013) and Yu et al. (2013) in that we build a discriminative model for SMT. 5 Conclusion We investigated a novel system for syntactic machine translation, treating MT as an unconstrained generation task, solved by using a single discriminative model with both monolingual syntax and bilingual translation features. Syntactic correspondence is captured by using soft features rather than hard translation rules, which are used by most syntax-based statistical methods in the literature. Our results are preliminary in the sense that the experiments were perfo</context>
</contexts>
<marker>Liang, Bouchard-Cote, Klein, Taskar, 2006</marker>
<rawString>P. Liang, A. Bouchard-Cote, D. Klein, and B. Taskar. 2006. An end-to-end discriminative approach to machine translation. In Proc. COLIIG/ACL, pages 761–768.</rawString>
</citation>
<citation valid="true">
<authors>
<author>Yang Liu</author>
<author>Qun Liu</author>
<author>Shouxun Lin</author>
</authors>
<title>Treeto-string alignment template for statistical machine translation.</title>
<date>2006</date>
<booktitle>In Proc. COLIIG/ACL,</booktitle>
<pages>609--616</pages>
<contexts>
<context position="1091" citStr="Liu et al., 2006" startWordPosition="148" endWordPosition="151">s, but treat translation as an unconstrained target sentence generation task, using soft features to capture lexical and syntactic correspondences between the source and target languages. Target syntax features and bilingual translation features are trained consistently in a discriminative model. Experiments using the IWSLT 2010 dataset show that the system achieves BLEU comparable to the state-of-the-art syntactic SMT systems. 1 Introduction Translation rules have been central to hierarchical phrase-based and syntactic statistical machine translation (SMT) (Galley et al., 2004; Chiang, 2005; Liu et al., 2006; Quirk et al., 2005; Marcu et al., 2006; Shen and Joshi, 2008; Xie et al., 2011). They are attractive by capturing the recursiveness of languages and syntactic correspondences between them. One important advantage of translation rules is that they allow efficient decoding by treating MT as a statistical parsing task, transforming a source sentence to its translation via recursive rule application. The efficiency takes root in the fact that target word orders are encoded in translation rules. This fact, however, also leads to rule explosion, noise and coverage problems (Auli et al., 2009), whi</context>
</contexts>
<marker>Liu, Liu, Lin, 2006</marker>
<rawString>Yang Liu, Qun Liu, and Shouxun Lin. 2006. Treeto-string alignment template for statistical machine translation. In Proc. COLIIG/ACL, pages 609–616.</rawString>
</citation>
<citation valid="true">
<authors>
<author>Daniel Marcu</author>
<author>Wei Wang</author>
<author>Abdessamad Echihabi</author>
<author>Kevin Knight</author>
</authors>
<title>SPMT: Statistical machine translation with syntactified target language phrases.</title>
<date>2006</date>
<booktitle>In Proc. EMILP,</booktitle>
<pages>44--52</pages>
<contexts>
<context position="1131" citStr="Marcu et al., 2006" startWordPosition="156" endWordPosition="159">rained target sentence generation task, using soft features to capture lexical and syntactic correspondences between the source and target languages. Target syntax features and bilingual translation features are trained consistently in a discriminative model. Experiments using the IWSLT 2010 dataset show that the system achieves BLEU comparable to the state-of-the-art syntactic SMT systems. 1 Introduction Translation rules have been central to hierarchical phrase-based and syntactic statistical machine translation (SMT) (Galley et al., 2004; Chiang, 2005; Liu et al., 2006; Quirk et al., 2005; Marcu et al., 2006; Shen and Joshi, 2008; Xie et al., 2011). They are attractive by capturing the recursiveness of languages and syntactic correspondences between them. One important advantage of translation rules is that they allow efficient decoding by treating MT as a statistical parsing task, transforming a source sentence to its translation via recursive rule application. The efficiency takes root in the fact that target word orders are encoded in translation rules. This fact, however, also leads to rule explosion, noise and coverage problems (Auli et al., 2009), which can hurt translation quality. Flexibi</context>
</contexts>
<marker>Marcu, Wang, Echihabi, Knight, 2006</marker>
<rawString>Daniel Marcu, Wei Wang, Abdessamad Echihabi, and Kevin Knight. 2006. SPMT: Statistical machine translation with syntactified target language phrases. In Proc. EMILP, pages 44–52.</rawString>
</citation>
<citation valid="true">
<authors>
<author>Mitchell P Marcus</author>
<author>Mary Ann Marcinkiewicz</author>
<author>Beatrice Santorini</author>
</authors>
<title>Building a large annotated corpus of English: The penn treebank.</title>
<date>1993</date>
<booktitle>Computational linguistics,</booktitle>
<pages>19--2</pages>
<contexts>
<context position="14313" citStr="Marcus et al., 1993" startWordPosition="2335" endWordPosition="2338">which contains 30,033 sentence pairs. For system tuning, we use the IWSLT 2004 test set (also released as the second development test set of IWSLT 2010), which contains 500 sentences. For final test, we use the IWSLT 2003 test set (also released as the first development test set of IWSLT 2010), which contains 506 sentences. The Chinese sentences in the datasets are segmented using NiuTrans3 (Xiao et al., 2012), while POS-tagging of both English and Chinese is performed using ZPar4 version 0.5 (Zhang and Clark, 2011). We train the English POS-tagger using the WSJ sections of the Penn Treebank (Marcus et al., 1993), turned into lower-case. For syntactic parsing of both English and Chinese, we use the default models of ZPar 0.5. We choose three baseline systems: a string-totree (S2T) system, a tree-to-string (T2S) system and a tree-to-tree (T2T) system (Koehn, 2010). The Moses release 1.0 implementations of all three systems are used, with default parameter settings. IRSTLM5 release 5.80.03 (Federico et al., 2008) is used to train a four-gram language models 2This led to the ignoring of over 40% of the training sentence pairs. For future work, we will consider substitute oracles from reachable target der</context>
</contexts>
<marker>Marcus, Marcinkiewicz, Santorini, 1993</marker>
<rawString>Mitchell P. Marcus, Mary Ann Marcinkiewicz, and Beatrice Santorini. 1993. Building a large annotated corpus of English: The penn treebank. Computational linguistics, 19(2):313–330.</rawString>
</citation>
<citation valid="true">
<authors>
<author>Mark-Jan Nederhof</author>
<author>Giorgio Satta</author>
</authors>
<title>Idlexpressions: a formalism for representing and parsing finite languages in natural language processing.</title>
<date>2004</date>
<journal>J. Artif. Intell. Res.(JAIR),</journal>
<pages>21--287</pages>
<contexts>
<context position="17739" citStr="Nederhof and Satta, 2004" startWordPosition="2939" endWordPosition="2942">et synthesis component is effective under the bilingual setting, a stronger lexical selection component may be necessary for better translation quality. 180 4 Related work As discussed in the introduction, our work is closely related to previous studies on syntactic MT, with the salient difference that we do not rely on hard translation rules, but allow free target synthesis. The contrast can be summarized as “translation by parsing” vs “translation by generation”. There has been a line of research on generation for translation. Soricut and Marcu (2006) use a form of weighted IDL-expressions (Nederhof and Satta, 2004) for generation. Bangalore et al. (2007) treats MT as a combination of global lexical transfer and word ordering; their generation component does not perform lexical selection, relying on an n-gram language model to order target words. Goto et al. (2012) use a monotonic phrasebased system to perform target word selection, and treats target ordering as a post-processing step. More recently, Chen et al. (2014) translate source dependencies arc-by-arc to generate pseudo target dependencies, and generate the translation by reordering of arcs. In contrast with these systems, our system relies more </context>
</contexts>
<marker>Nederhof, Satta, 2004</marker>
<rawString>Mark-Jan Nederhof and Giorgio Satta. 2004. Idlexpressions: a formalism for representing and parsing finite languages in natural language processing. J. Artif. Intell. Res.(JAIR), 21:287–317.</rawString>
</citation>
<citation valid="true">
<authors>
<author>Franz Josef Och</author>
<author>Christoph Tillmann</author>
<author>Hermann Ney</author>
</authors>
<title>Improved alignment models for statistical machine translation.</title>
<date>1999</date>
<booktitle>In Proc. EMILP,</booktitle>
<pages>20--28</pages>
<contexts>
<context position="5055" citStr="Och et al., 1999" startWordPosition="761" endWordPosition="764">r provides candidate translations for (overlapping) source words and phrases. Although lexicons and rules can be used for this step, we take a simple statistical alignment-based approach. The latter searches for a target translation by constructing dependency trees bottom-up. The process can be viewed as a syntax-based generation process from a bag of overlapping translation options. 2.1 Lexical transfer We perform word alignment using IBM model 4 (Brown et al., 1993), and then extract phrase pairs according to the alignment and automaticallyannotated target syntax. In particular, consistent (Och et al., 1999) and cohesive (Fox, 2002) phrase pairs are extracted from intersected alignments in both directions: the target side must form a projective span, with a single root, and the source side must be contiguous. A resulting phrase pair consists of the source phrase, its target translation, as well as the head position and head part-of-speech (POS) of the target span, which are useful for target synthesis. We further restrict that neither the source nor the target side of a valid phrase pair contains over s words. Given an input source sentence, the lexical transfer unit finds all valid target transl</context>
</contexts>
<marker>Och, Tillmann, Ney, 1999</marker>
<rawString>Franz Josef Och, Christoph Tillmann, and Hermann Ney. 1999. Improved alignment models for statistical machine translation. In Proc. EMILP, pages 20–28.</rawString>
</citation>
<citation valid="true">
<authors>
<author>Chris Quirk</author>
<author>Arul Menezes</author>
<author>Colin Cherry</author>
</authors>
<title>Dependency treelet translation: Syntactically informed phrasal smt.</title>
<date>2005</date>
<booktitle>In Proc. ACL,</booktitle>
<pages>271--279</pages>
<contexts>
<context position="1111" citStr="Quirk et al., 2005" startWordPosition="152" endWordPosition="155">lation as an unconstrained target sentence generation task, using soft features to capture lexical and syntactic correspondences between the source and target languages. Target syntax features and bilingual translation features are trained consistently in a discriminative model. Experiments using the IWSLT 2010 dataset show that the system achieves BLEU comparable to the state-of-the-art syntactic SMT systems. 1 Introduction Translation rules have been central to hierarchical phrase-based and syntactic statistical machine translation (SMT) (Galley et al., 2004; Chiang, 2005; Liu et al., 2006; Quirk et al., 2005; Marcu et al., 2006; Shen and Joshi, 2008; Xie et al., 2011). They are attractive by capturing the recursiveness of languages and syntactic correspondences between them. One important advantage of translation rules is that they allow efficient decoding by treating MT as a statistical parsing task, transforming a source sentence to its translation via recursive rule application. The efficiency takes root in the fact that target word orders are encoded in translation rules. This fact, however, also leads to rule explosion, noise and coverage problems (Auli et al., 2009), which can hurt translat</context>
</contexts>
<marker>Quirk, Menezes, Cherry, 2005</marker>
<rawString>Chris Quirk, Arul Menezes, and Colin Cherry. 2005. Dependency treelet translation: Syntactically informed phrasal smt. In Proc. ACL, pages 271–279.</rawString>
</citation>
<citation valid="true">
<authors>
<author>Libin Shen</author>
<author>Aravind Joshi</author>
</authors>
<title>LTAG dependency parsing with bidirectional incremental construction.</title>
<date>2008</date>
<booktitle>In Proc. EMILP,</booktitle>
<pages>495--504</pages>
<contexts>
<context position="1153" citStr="Shen and Joshi, 2008" startWordPosition="160" endWordPosition="163">ce generation task, using soft features to capture lexical and syntactic correspondences between the source and target languages. Target syntax features and bilingual translation features are trained consistently in a discriminative model. Experiments using the IWSLT 2010 dataset show that the system achieves BLEU comparable to the state-of-the-art syntactic SMT systems. 1 Introduction Translation rules have been central to hierarchical phrase-based and syntactic statistical machine translation (SMT) (Galley et al., 2004; Chiang, 2005; Liu et al., 2006; Quirk et al., 2005; Marcu et al., 2006; Shen and Joshi, 2008; Xie et al., 2011). They are attractive by capturing the recursiveness of languages and syntactic correspondences between them. One important advantage of translation rules is that they allow efficient decoding by treating MT as a statistical parsing task, transforming a source sentence to its translation via recursive rule application. The efficiency takes root in the fact that target word orders are encoded in translation rules. This fact, however, also leads to rule explosion, noise and coverage problems (Auli et al., 2009), which can hurt translation quality. Flexibility of function word </context>
</contexts>
<marker>Shen, Joshi, 2008</marker>
<rawString>Libin Shen and Aravind Joshi. 2008. LTAG dependency parsing with bidirectional incremental construction. In Proc. EMILP, pages 495–504.</rawString>
</citation>
<citation valid="true">
<authors>
<author>Linfeng Song</author>
<author>Yue Zhang</author>
<author>Kai Song</author>
<author>Qun Liu</author>
</authors>
<title>Joint morphological generation and syntactic linearization.</title>
<date>2014</date>
<booktitle>In Proc. AAAI,</booktitle>
<pages>1522--1528</pages>
<contexts>
<context position="18805" citStr="Song et al. (2014)" startWordPosition="3115" endWordPosition="3118">-arc to generate pseudo target dependencies, and generate the translation by reordering of arcs. In contrast with these systems, our system relies more heavily on a syntax-based synthesis component, in order to study the usefulness of statistical NLG on SMT. With respect to syntax-based word ordering, Chang and Toutanova (2007) and He et al. (2009) study a simplified word ordering problem by assuming that the un-ordered target dependency tree is given. Wan et al. (2009) and Zhang and Clark (2011) study the ordering of a bag of words, without input syntax. Zhang et al. (2012), Zhang (2013) and Song et al. (2014) further extended this line of research by adding input syntax and allowing joint inflection and ordering. de Gispert et al. (2014) use a phrase-structure grammer for word ordering. Our generation system is based on the work of Zhang (2013), but further allows lexical selection. Our work is also in line with the work of Liang et al. (2006), Blunsom et al. (2008), Flanigan et al. (2013) and Yu et al. (2013) in that we build a discriminative model for SMT. 5 Conclusion We investigated a novel system for syntactic machine translation, treating MT as an unconstrained generation task, solved by usi</context>
</contexts>
<marker>Song, Zhang, Song, Liu, 2014</marker>
<rawString>Linfeng Song, Yue Zhang, Kai Song, and Qun Liu. 2014. Joint morphological generation and syntactic linearization. In Proc. AAAI, pages 1522–1528.</rawString>
</citation>
<citation valid="true">
<authors>
<author>Radu Soricut</author>
<author>Daniel Marcu</author>
</authors>
<title>Stochastic language generation using widl-expressions and its application in machine translation and summarization.</title>
<date>2006</date>
<booktitle>In Proc. ACL,</booktitle>
<pages>1105--1112</pages>
<contexts>
<context position="17673" citStr="Soricut and Marcu (2006)" startWordPosition="2929" endWordPosition="2932">n be more frequently incorrect. This suggests that while the target synthesis component is effective under the bilingual setting, a stronger lexical selection component may be necessary for better translation quality. 180 4 Related work As discussed in the introduction, our work is closely related to previous studies on syntactic MT, with the salient difference that we do not rely on hard translation rules, but allow free target synthesis. The contrast can be summarized as “translation by parsing” vs “translation by generation”. There has been a line of research on generation for translation. Soricut and Marcu (2006) use a form of weighted IDL-expressions (Nederhof and Satta, 2004) for generation. Bangalore et al. (2007) treats MT as a combination of global lexical transfer and word ordering; their generation component does not perform lexical selection, relying on an n-gram language model to order target words. Goto et al. (2012) use a monotonic phrasebased system to perform target word selection, and treats target ordering as a post-processing step. More recently, Chen et al. (2014) translate source dependencies arc-by-arc to generate pseudo target dependencies, and generate the translation by reorderin</context>
</contexts>
<marker>Soricut, Marcu, 2006</marker>
<rawString>Radu Soricut and Daniel Marcu. 2006. Stochastic language generation using widl-expressions and its application in machine translation and summarization. In Proc. ACL, pages 1105–1112.</rawString>
</citation>
<citation valid="true">
<authors>
<author>Stephen Wan</author>
<author>Mark Dras</author>
<author>Robert Dale</author>
<author>C´ecile Paris</author>
</authors>
<title>Improving grammaticality in statistical sentence generation: Introducing a dependency spanning tree algorithm with an argument satisfaction model.</title>
<date>2009</date>
<booktitle>In Proc. EACL,</booktitle>
<pages>852--860</pages>
<contexts>
<context position="18661" citStr="Wan et al. (2009)" startWordPosition="3087" endWordPosition="3090">et word selection, and treats target ordering as a post-processing step. More recently, Chen et al. (2014) translate source dependencies arc-by-arc to generate pseudo target dependencies, and generate the translation by reordering of arcs. In contrast with these systems, our system relies more heavily on a syntax-based synthesis component, in order to study the usefulness of statistical NLG on SMT. With respect to syntax-based word ordering, Chang and Toutanova (2007) and He et al. (2009) study a simplified word ordering problem by assuming that the un-ordered target dependency tree is given. Wan et al. (2009) and Zhang and Clark (2011) study the ordering of a bag of words, without input syntax. Zhang et al. (2012), Zhang (2013) and Song et al. (2014) further extended this line of research by adding input syntax and allowing joint inflection and ordering. de Gispert et al. (2014) use a phrase-structure grammer for word ordering. Our generation system is based on the work of Zhang (2013), but further allows lexical selection. Our work is also in line with the work of Liang et al. (2006), Blunsom et al. (2008), Flanigan et al. (2013) and Yu et al. (2013) in that we build a discriminative model for SM</context>
</contexts>
<marker>Wan, Dras, Dale, Paris, 2009</marker>
<rawString>Stephen Wan, Mark Dras, Robert Dale, and C´ecile Paris. 2009. Improving grammaticality in statistical sentence generation: Introducing a dependency spanning tree algorithm with an argument satisfaction model. In Proc. EACL, pages 852–860.</rawString>
</citation>
<citation valid="true">
<authors>
<author>Michael White</author>
<author>Rajakrishnan Rajkumar</author>
</authors>
<title>Perceptron reranking for CCG realization.</title>
<date>2009</date>
<booktitle>In Proc. the EMILP,</booktitle>
<pages>410--419</pages>
<contexts>
<context position="2584" citStr="White and Rajkumar, 2009" startWordPosition="384" endWordPosition="387">siting Singapore University of Technology and Design (SUTD) Figure 1: Overall system architecture. A potential solution to the problems above is to treat translation as a generation task, representing syntactic correspondences using soft features. Both adequacy and fluency can potentially be improved by giving full flexibility to target synthesis, and leaving all options to the statistical model. The main challenge to this method is a significant increase in the search space (Knight, 1999). To this end, recent advances in tackling complex search tasks for text generation offer some solutions (White and Rajkumar, 2009; Zhang and Clark, 2011). In this short paper, we present a preliminary investigation on the possibility of building a syntactic SMT system that does not use hard translation rules, by utilizing recent advances in statistical natural language generation (NLG). The overall architecture is shown in Figure 1. Translation is performed by first parsing the source sentence, then transferring source words and phrases to their target equivalences, and finally synthesizing the target output. We choose dependency grammar for both the source and the target syntax, and adapt the syntactic text synthesis s</context>
</contexts>
<marker>White, Rajkumar, 2009</marker>
<rawString>Michael White and Rajakrishnan Rajkumar. 2009. Perceptron reranking for CCG realization. In Proc. the EMILP, pages 410–419.</rawString>
</citation>
<citation valid="true">
<authors>
<author>Tong Xiao</author>
<author>Jingbo Zhu</author>
<author>Hao Zhang</author>
<author>Qiang Li</author>
</authors>
<title>NiuTrans: An open source toolkit for phrasebased and syntax-based machine translation.</title>
<date>2012</date>
<booktitle>In Proc. ACL Demos,</booktitle>
<pages>pages</pages>
<contexts>
<context position="14106" citStr="Xiao et al., 2012" startWordPosition="2300" endWordPosition="2303"> Chinese-English dataset, which consists of training sentence pairs from the dialog task (dialog) and Basic Travel and Expression Corpus (BTEC). The union of dialog and BTEC are taken as our training set, which contains 30,033 sentence pairs. For system tuning, we use the IWSLT 2004 test set (also released as the second development test set of IWSLT 2010), which contains 500 sentences. For final test, we use the IWSLT 2003 test set (also released as the first development test set of IWSLT 2010), which contains 506 sentences. The Chinese sentences in the datasets are segmented using NiuTrans3 (Xiao et al., 2012), while POS-tagging of both English and Chinese is performed using ZPar4 version 0.5 (Zhang and Clark, 2011). We train the English POS-tagger using the WSJ sections of the Penn Treebank (Marcus et al., 1993), turned into lower-case. For syntactic parsing of both English and Chinese, we use the default models of ZPar 0.5. We choose three baseline systems: a string-totree (S2T) system, a tree-to-string (T2S) system and a tree-to-tree (T2T) system (Koehn, 2010). The Moses release 1.0 implementations of all three systems are used, with default parameter settings. IRSTLM5 release 5.80.03 (Federico </context>
</contexts>
<marker>Xiao, Zhu, Zhang, Li, 2012</marker>
<rawString>Tong Xiao, Jingbo Zhu, Hao Zhang, and Qiang Li. 2012. NiuTrans: An open source toolkit for phrasebased and syntax-based machine translation. In Proc. ACL Demos, pages 19–24.</rawString>
</citation>
<citation valid="true">
<authors>
<author>Jun Xie</author>
<author>Haitao Mi</author>
<author>Qun Liu</author>
</authors>
<title>A novel dependency-to-string model for statistical machine translation.</title>
<date>2011</date>
<booktitle>In Proc. EMILP,</booktitle>
<pages>216--226</pages>
<contexts>
<context position="1172" citStr="Xie et al., 2011" startWordPosition="164" endWordPosition="167">ing soft features to capture lexical and syntactic correspondences between the source and target languages. Target syntax features and bilingual translation features are trained consistently in a discriminative model. Experiments using the IWSLT 2010 dataset show that the system achieves BLEU comparable to the state-of-the-art syntactic SMT systems. 1 Introduction Translation rules have been central to hierarchical phrase-based and syntactic statistical machine translation (SMT) (Galley et al., 2004; Chiang, 2005; Liu et al., 2006; Quirk et al., 2005; Marcu et al., 2006; Shen and Joshi, 2008; Xie et al., 2011). They are attractive by capturing the recursiveness of languages and syntactic correspondences between them. One important advantage of translation rules is that they allow efficient decoding by treating MT as a statistical parsing task, transforming a source sentence to its translation via recursive rule application. The efficiency takes root in the fact that target word orders are encoded in translation rules. This fact, however, also leads to rule explosion, noise and coverage problems (Auli et al., 2009), which can hurt translation quality. Flexibility of function word usage, rich morphol</context>
</contexts>
<marker>Xie, Mi, Liu, 2011</marker>
<rawString>Jun Xie, Haitao Mi, and Qun Liu. 2011. A novel dependency-to-string model for statistical machine translation. In Proc. EMILP, pages 216–226.</rawString>
</citation>
<citation valid="true">
<authors>
<author>Heng Yu</author>
<author>Liang Huang</author>
<author>Haitao Mi</author>
<author>Kai Zhao</author>
</authors>
<title>Max-violation perceptron and forced decoding for scalable MT training.</title>
<date>2013</date>
<booktitle>In Proc. EMILP,</booktitle>
<pages>1112--1123</pages>
<contexts>
<context position="19214" citStr="Yu et al. (2013)" startWordPosition="3187" endWordPosition="3190">e un-ordered target dependency tree is given. Wan et al. (2009) and Zhang and Clark (2011) study the ordering of a bag of words, without input syntax. Zhang et al. (2012), Zhang (2013) and Song et al. (2014) further extended this line of research by adding input syntax and allowing joint inflection and ordering. de Gispert et al. (2014) use a phrase-structure grammer for word ordering. Our generation system is based on the work of Zhang (2013), but further allows lexical selection. Our work is also in line with the work of Liang et al. (2006), Blunsom et al. (2008), Flanigan et al. (2013) and Yu et al. (2013) in that we build a discriminative model for SMT. 5 Conclusion We investigated a novel system for syntactic machine translation, treating MT as an unconstrained generation task, solved by using a single discriminative model with both monolingual syntax and bilingual translation features. Syntactic correspondence is captured by using soft features rather than hard translation rules, which are used by most syntax-based statistical methods in the literature. Our results are preliminary in the sense that the experiments were performed using a relatively small dataset, and little engineering effort</context>
</contexts>
<marker>Yu, Huang, Mi, Zhao, 2013</marker>
<rawString>Heng Yu, Liang Huang, Haitao Mi, and Kai Zhao. 2013. Max-violation perceptron and forced decoding for scalable MT training. In Proc. EMILP, pages 1112–1123.</rawString>
</citation>
<citation valid="true">
<authors>
<author>Yue Zhang</author>
<author>Stephen Clark</author>
</authors>
<title>Syntax-based grammaticality improvement using CCG and guided search.</title>
<date>2011</date>
<booktitle>In Proc. EMILP,</booktitle>
<pages>1147--1157</pages>
<contexts>
<context position="2608" citStr="Zhang and Clark, 2011" startWordPosition="388" endWordPosition="391">y of Technology and Design (SUTD) Figure 1: Overall system architecture. A potential solution to the problems above is to treat translation as a generation task, representing syntactic correspondences using soft features. Both adequacy and fluency can potentially be improved by giving full flexibility to target synthesis, and leaving all options to the statistical model. The main challenge to this method is a significant increase in the search space (Knight, 1999). To this end, recent advances in tackling complex search tasks for text generation offer some solutions (White and Rajkumar, 2009; Zhang and Clark, 2011). In this short paper, we present a preliminary investigation on the possibility of building a syntactic SMT system that does not use hard translation rules, by utilizing recent advances in statistical natural language generation (NLG). The overall architecture is shown in Figure 1. Translation is performed by first parsing the source sentence, then transferring source words and phrases to their target equivalences, and finally synthesizing the target output. We choose dependency grammar for both the source and the target syntax, and adapt the syntactic text synthesis system of Zhang (2013), w</context>
<context position="14214" citStr="Zhang and Clark, 2011" startWordPosition="2318" endWordPosition="2321">sic Travel and Expression Corpus (BTEC). The union of dialog and BTEC are taken as our training set, which contains 30,033 sentence pairs. For system tuning, we use the IWSLT 2004 test set (also released as the second development test set of IWSLT 2010), which contains 500 sentences. For final test, we use the IWSLT 2003 test set (also released as the first development test set of IWSLT 2010), which contains 506 sentences. The Chinese sentences in the datasets are segmented using NiuTrans3 (Xiao et al., 2012), while POS-tagging of both English and Chinese is performed using ZPar4 version 0.5 (Zhang and Clark, 2011). We train the English POS-tagger using the WSJ sections of the Penn Treebank (Marcus et al., 1993), turned into lower-case. For syntactic parsing of both English and Chinese, we use the default models of ZPar 0.5. We choose three baseline systems: a string-totree (S2T) system, a tree-to-string (T2S) system and a tree-to-tree (T2T) system (Koehn, 2010). The Moses release 1.0 implementations of all three systems are used, with default parameter settings. IRSTLM5 release 5.80.03 (Federico et al., 2008) is used to train a four-gram language models 2This led to the ignoring of over 40% of the trai</context>
<context position="18688" citStr="Zhang and Clark (2011)" startWordPosition="3092" endWordPosition="3095"> treats target ordering as a post-processing step. More recently, Chen et al. (2014) translate source dependencies arc-by-arc to generate pseudo target dependencies, and generate the translation by reordering of arcs. In contrast with these systems, our system relies more heavily on a syntax-based synthesis component, in order to study the usefulness of statistical NLG on SMT. With respect to syntax-based word ordering, Chang and Toutanova (2007) and He et al. (2009) study a simplified word ordering problem by assuming that the un-ordered target dependency tree is given. Wan et al. (2009) and Zhang and Clark (2011) study the ordering of a bag of words, without input syntax. Zhang et al. (2012), Zhang (2013) and Song et al. (2014) further extended this line of research by adding input syntax and allowing joint inflection and ordering. de Gispert et al. (2014) use a phrase-structure grammer for word ordering. Our generation system is based on the work of Zhang (2013), but further allows lexical selection. Our work is also in line with the work of Liang et al. (2006), Blunsom et al. (2008), Flanigan et al. (2013) and Yu et al. (2013) in that we build a discriminative model for SMT. 5 Conclusion We investig</context>
</contexts>
<marker>Zhang, Clark, 2011</marker>
<rawString>Yue Zhang and Stephen Clark. 2011. Syntax-based grammaticality improvement using CCG and guided search. In Proc. EMILP, pages 1147–1157.</rawString>
</citation>
<citation valid="true">
<authors>
<author>Yue Zhang</author>
<author>Graeme Blackwood</author>
<author>Stephen Clark</author>
</authors>
<title>Syntax-based word ordering incorporating a large-scale language model.</title>
<date>2012</date>
<booktitle>In Proc. EACL,</booktitle>
<pages>736--746</pages>
<contexts>
<context position="18768" citStr="Zhang et al. (2012)" startWordPosition="3108" endWordPosition="3111">) translate source dependencies arc-by-arc to generate pseudo target dependencies, and generate the translation by reordering of arcs. In contrast with these systems, our system relies more heavily on a syntax-based synthesis component, in order to study the usefulness of statistical NLG on SMT. With respect to syntax-based word ordering, Chang and Toutanova (2007) and He et al. (2009) study a simplified word ordering problem by assuming that the un-ordered target dependency tree is given. Wan et al. (2009) and Zhang and Clark (2011) study the ordering of a bag of words, without input syntax. Zhang et al. (2012), Zhang (2013) and Song et al. (2014) further extended this line of research by adding input syntax and allowing joint inflection and ordering. de Gispert et al. (2014) use a phrase-structure grammer for word ordering. Our generation system is based on the work of Zhang (2013), but further allows lexical selection. Our work is also in line with the work of Liang et al. (2006), Blunsom et al. (2008), Flanigan et al. (2013) and Yu et al. (2013) in that we build a discriminative model for SMT. 5 Conclusion We investigated a novel system for syntactic machine translation, treating MT as an unconst</context>
</contexts>
<marker>Zhang, Blackwood, Clark, 2012</marker>
<rawString>Yue Zhang, Graeme Blackwood, and Stephen Clark. 2012. Syntax-based word ordering incorporating a large-scale language model. In Proc. EACL, pages 736–746.</rawString>
</citation>
<citation valid="true">
<authors>
<author>Yue Zhang</author>
</authors>
<title>Partial-tree linearization: Generalized word ordering for text synthesis.</title>
<date>2013</date>
<booktitle>In Proc. IJCAI,</booktitle>
<pages>2232--2238</pages>
<contexts>
<context position="3205" citStr="Zhang (2013)" startWordPosition="486" endWordPosition="487">nd Clark, 2011). In this short paper, we present a preliminary investigation on the possibility of building a syntactic SMT system that does not use hard translation rules, by utilizing recent advances in statistical natural language generation (NLG). The overall architecture is shown in Figure 1. Translation is performed by first parsing the source sentence, then transferring source words and phrases to their target equivalences, and finally synthesizing the target output. We choose dependency grammar for both the source and the target syntax, and adapt the syntactic text synthesis system of Zhang (2013), which performs dependency-based linearization. The linearization task for MT is different from the monolingual task in that not all translation options are used to build the output, and that bilingual correspondences need to be taken into account dur177 Proceedings of the 2014 Conference on Empirical Methods in Natural Language Processing (EMNLP), pages 177–182, October 25-29, 2014, Doha, Qatar. c�2014 Association for Computational Linguistics ing synthesis. The algorithms of Zhang (2013) are modified to perform word selection as well as ordering, using two sets of features to control transl</context>
<context position="6249" citStr="Zhang (2013)" startWordPosition="959" endWordPosition="960">alid target translation options for overlapping source phrases up to size s, and feeds them as inputs to the target synthesis decoder. The translation options with a probability 1International Workshop on Spoken Language Translation, http://iwslt2010.fbk.eu below A · Pmax are filtered out, where Pmax is the probability of the most probable translation. Here the probability of a target translation is calculated as the count of the translation divided by the count of all translations of the source phrase. 2.2 Synthesis The synthesis module is based on the monolingual text synthesis algorithm of Zhang (2013), which constructs an ordered dependency tree given a bag of words. In the bilingual setting, inputs to the algorithm are translation options, which can be overlapping and mutually exclusive, and not necessarily all of which are included in the output. As a result, the decoder needs to perform word selection in addition to word ordering. Another difference between the bilingual and monolingual settings is that the former requires translation adequacy in addition to output fluency. We largely rely on the monolingual system for MT decoding. To deal with overlapping translation options, a source </context>
<context position="10078" citStr="Zhang (2013)" startWordPosition="1641" endWordPosition="1642">heses, and then takes the highest-scored chart hypothesis that contains over Q · n words as the output, where Q is a real number near 1.0. 2.2.2 Model and training A scaled linear model is used by the decoder to score search hypotheses: Score(e) = where Φ(e) is the global feature vector of the hypothesis e, θ� is the parameter vector of the model, and |e |is the number of leaf nodes in e. The scaling factor |e |is necessary because hypotheses with different numbers of words are compared with each other in the search process to capture translation equivalence. While the monolingual features of Zhang (2013) are applied (example feature templates from the system are shown in Table 1), an additional set of bilingual features is defined, shown phrase translation features PHRASE(m) · PHRASE(t), P(trans), bilingual syntactic features POS(th) · POS(tm) · dir · LEN(path), WORD(th) · POS(tm) · dir · LEN(path), POS(th) · WORD(tm) · dir · LEN(path), WORD(th) · WORD(tm) · dir · LEN(path), WORD(sh) · WORD(sm) · dir · LEN(path), WORD(sh) · WORD(th) · dir · LEN(path), WORD(sm) · WORD(tm) · dir · LEN(path), bilingual syntactic features (LEN(path) &lt; 3) POS(th) · POS(tm) · dir · LABELS(path), WORD(th) · POS(tm) </context>
<context position="12501" citStr="Zhang (2013)" startWordPosition="2039" endWordPosition="2040">th represents the shortest path in the source dependency tree between the two nodes that correspond to the target head and modifier, respectively; LEN(path) represents the number of arcs on path, normalized to bins of [5, 10, 20, 40+]; LABELS(path) represents the array of dependency arc labels on path; LABELSPOS(path) represents the array of dependency arc labels and source POS on path. In addition, a real-valued four-gram language model feature is also used, with four-grams extracted from the surface boundary when two hypothesis are combined. We apply the discriminative learning algorithm of Zhang (2013) to train the parameters B. The algorithm requires training examples that consist of full target derivations, with leaf nodes being input translation options. However, the readily available θ� · Φ(e) |e |, 179 training examples are automatically-parsed target derivations, with leaf nodes being the reference translation. As a result, we apply a search procedure to find a derivation process, through which the target dependency tree is constructed from a subset of input translation options. The search procedure can be treated as a constrained decoding process, where only the oracle tree and its s</context>
<context position="18782" citStr="Zhang (2013)" startWordPosition="3112" endWordPosition="3113">pendencies arc-by-arc to generate pseudo target dependencies, and generate the translation by reordering of arcs. In contrast with these systems, our system relies more heavily on a syntax-based synthesis component, in order to study the usefulness of statistical NLG on SMT. With respect to syntax-based word ordering, Chang and Toutanova (2007) and He et al. (2009) study a simplified word ordering problem by assuming that the un-ordered target dependency tree is given. Wan et al. (2009) and Zhang and Clark (2011) study the ordering of a bag of words, without input syntax. Zhang et al. (2012), Zhang (2013) and Song et al. (2014) further extended this line of research by adding input syntax and allowing joint inflection and ordering. de Gispert et al. (2014) use a phrase-structure grammer for word ordering. Our generation system is based on the work of Zhang (2013), but further allows lexical selection. Our work is also in line with the work of Liang et al. (2006), Blunsom et al. (2008), Flanigan et al. (2013) and Yu et al. (2013) in that we build a discriminative model for SMT. 5 Conclusion We investigated a novel system for syntactic machine translation, treating MT as an unconstrained generat</context>
</contexts>
<marker>Zhang, 2013</marker>
<rawString>Yue Zhang. 2013. Partial-tree linearization: Generalized word ordering for text synthesis. In Proc. IJCAI, pages 2232–2238.</rawString>
</citation>
</citationList>
</algorithm>
</algorithms>
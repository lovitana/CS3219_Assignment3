<?xml version="1.0" encoding="UTF-8"?>
<algorithms version="110505">
<algorithm name="SectLabel" version="110505">
<variant no="0" confidence="0.000849">
<title confidence="0.9977425">
Analyzing Stemming Approaches for Turkish Multi-Document
Summarization
</title>
<author confidence="0.983421">
Muhammed Yavuz Nuzumlalı Arzucan ¨Ozg¨ur
</author>
<affiliation confidence="0.995663">
Department of Computer Engineering
Bo˘gazic¸i University
</affiliation>
<address confidence="0.614609">
TR-34342, Bebek, ˙Istanbul, Turkey
</address>
<email confidence="0.999251">
{yavuz.nuzumlali,arzucan.ozgur}@boun.edu.tr
</email>
<sectionHeader confidence="0.99481" genericHeader="abstract">
Abstract
</sectionHeader>
<bodyText confidence="0.999612571428572">
In this study, we analyzed the effects of ap-
plying different levels of stemming approaches
such as fixed-length word truncation and mor-
phological analysis for multi-document sum-
marization (MDS) on Turkish, which is an ag-
glutinative and morphologically rich language.
We constructed a manually annotated MDS
data set, and to our best knowledge, reported
the first results on Turkish MDS. Our results
show that a simple fixed-length word trun-
cation approach performs slightly better than
no stemming, whereas applying complex mor-
phological analysis does not improve Turkish
MDS.
</bodyText>
<sectionHeader confidence="0.998428" genericHeader="introduction">
1 Introduction
</sectionHeader>
<bodyText confidence="0.999984649122807">
Automatic text summarization has gained more impor-
tance with the enormous growth and easy availability of
the Internet. It is now possible to reach extensive and
continuously growing amount of resources. However,
this situation brings its own challenges such as finding
the relevant documents, and absorbing a large quan-
tity of relevant information (Gupta and Lehal, 2010).
The goal of multi-document summarization (MDS) is
to automatically create a summary of a set of docu-
ments about the same topic without losing the impor-
tant information. Several approaches for MDS have
been proposed in the last decade. However, most of
them have only been applied to a relatively small set
of languages, mostly English, and recently also to lan-
guages like Chinese, Romanian, Arabic, and Spanish
(Giannakopoulos, 2013).
Previous studies have shown that methods proposed
for languages like English do not generally work well
for morphologically rich languages like Finnish, Turk-
ish, and Czech, and additional methods considering the
morphological structures of these languages are needed
(Eryi˘git et al., 2008). For instance, Turkish is an ag-
glutinative language where root words can take many
derivational and inflectional affixes. This feature re-
sults in a very high number of different word surface
forms, and eventually leads to the data sparseness prob-
lem. Hakkani-T¨ur et al. (2000) analyzed the number of
unique terms for Turkish and English and showed that
the term count for Turkish is three times more than En-
glish for a corpus of 1M words.
There are only a few studies for text summariza-
tion on Turkish, all of which are about single-document
summarization (Altan, 2004; C¸ı˘gır et al., 2009; ¨Ozsoy
et al., 2010; G¨uran et al., 2010; G¨uran et al., 2011).
Some of these studies applied morphological analysis
methods, but none of them analyzed their effects in de-
tail.
To our best knowledge, this paper reports the first
multi-document summarization study for Turkish. We
used LexRank as the main summarization algorithm
(Erkan and Radev, 2004), applied and analyzed differ-
ent levels of stemming methods such as complex mor-
phological analysis and fixed-length word truncation.
We also created the first manually annotated MDS data
set for Turkish, which has been made publicly available
for future studies.
The rest of the paper is organized as follows. Sec-
tion 2 presents the related work on MDS, as well as
on the applications of morphological analysis on Turk-
ish for different Natural Language Processing (NLP)
and Information Retrieval (IR) problems. In Section
3, we provide a very brief introduction to the Turkish
morphology and present the stemming methods that we
evaluated. The details about the created data set and
our experimental setup are presented in Section 4. We
present and discuss the results in Section 5, and con-
clude in Section 6.
</bodyText>
<sectionHeader confidence="0.999765" genericHeader="related work">
2 Related Work
</sectionHeader>
<bodyText confidence="0.992802066666666">
A large number of methods have been proposed for
multi-document summarization in the last 10-15 years
(e.g. (Erkan and Radev, 2004; Shen and Li, 2010;
Christensen et al., 2013)). While most of these ap-
proaches have only been applied to English, summa-
rization data sets and systems for other languages like
Chinese, Romanian, and Arabic have also been pro-
posed in the recent years (Giannakopoulos, 2013).
Previous studies on automatic summarization for
Turkish only tackled the problem of single-document
summarization (SDS). Altan (2004) and C¸ı˘gır et al.
(2009) proposed feature-based approaches for Turk-
ish SDS, whereas ¨Ozsoy et al. (2010) and G¨uran et
al. (2010) used Latent Semantic Analysis (LSA) based
methods. G¨uran et al. (2011) applied non-negative ma-
</bodyText>
<page confidence="0.965633">
702
</page>
<note confidence="0.6050815">
Proceedings of the 2014 Conference on Empirical Methods in Natural Language Processing (EMNLP), pages 702–706,
October 25-29, 2014, Doha, Qatar. c�2014 Association for Computational Linguistics
</note>
<tableCaption confidence="0.831325">
Word Analysis
Table 1: Different word forms and their morphological an
alysis for the stem “g¨or” (to see). The derivational
</tableCaption>
<figure confidence="0.7304103">
boundaries are marked with (DB).
g¨oren (the one who sees) g¨or+en(DB)
g ¨or¨ulen (the one which is seen) g¨or+¨ul(DB)+en(DB)
g¨or¨us¸ (opinion) g¨or+¨us¸(DB)
g¨or¨us¸¨un (your opinion) g¨or+¨us¸(DB)+¨un
g¨or¨us¸ler (opinions) g¨or+¨us¸(DB)+ler
g¨or¨us¸me (negotiation) g¨or+¨us¸(DB)+me(DB)
¨or¨us¸melerin (of negotiations)
g¨or+¨us¸(DB)+me(DB)+ler+in
g
</figure>
<bodyText confidence="0.988627">
et al., 2008) and Text Categorization
and
2013). Can et al. (2008) showed that us-
ing fixed-length truncation methods perform similarly
to lemmatization-based stemming for information re-
trieval.
and
(2013) obtained better results
for text categorization with fixed-length word trunca-
tion rather than complex morphological analysis, but
the difference was not significant. For other morpho-
logically rich languages, there is a case study on Greek
by Galiotou et al. (2013). They applied different stem-
ming algorithms and showed that stemming on Greek
texts improves the summarization performan
</bodyText>
<equation confidence="0.8605324">
(Akkus¸
C¸akıcı,
Akkus¸
C¸akıcı
ce.
</equation>
<sectionHeader confidence="0.962259" genericHeader="method">
3 Methodology
</sectionHeader>
<bodyText confidence="0.751314">
guage.
</bodyText>
<subsectionHeader confidence="0.8865475">
3.1
Turkish Morphology
</subsectionHeader>
<bodyText confidence="0.99677901754386">
forms for the stem
(to see). All the words in the
table have the same root, but the different suffixes lead
to different surface forms which may have similar or
different meanings. When the surface forms of these
words are used in a summarization system, they will be
regarded as totally different words. However, if a mor-
phological analysis method is applied to the sentences
before giving them to the summarization system, words
with similar meanings can match during the sentence
“g¨or”
calculations.
703 3.2 Stemming Policies
In this section, we explain the different stemming meth-
ods that we investigated.
Raw: In this method, we take the surface forms of
words, without applying any stemming.
Root: This method takes the most simple unit of a
word, namely the root form. For example, in Table 1,
the words
and
have the same root
so they will match during sen-
tence similarity calculations.
Deriv: Using the Root method may oversimplify
words because some words that are derived from the
same root may have irrelevant meanings. In the above
example,
and
have different mean-
ings, but they have the same root
In order to solve
this oversimplification issue, we propose to preserve
derivational affixes, and only remove the inflectional
affixes from the words. In this method,
and
will not match because when we remove
only the inflectional affixes, they become
and
On the other hand, the words
and
will match because their Deriv forms are the
same, which is
Prefix: In Turkish, affixes almost always occur as
suffixes, not prefixes. Additionally, applying morpho-
logical analysis methods is a time consuming process,
and may become an overhead for online applications.
Therefore, afixed-length simplification method is also
tried, since itis both a fast method and can help match
similar words by taking the first n characters of words
which have lengths larger than n.
based method that achieves promising results for MDS.
In LexRank, first a sentence connectivity graph is con-
structed based on the cosine similarities between sen-
tences, and then the PageRank (Page et al., 1999) algo-
rithm is used to find the most importan
“g¨oren”,“g¨or¨us¸¨un”,
</bodyText>
<equation confidence="0.989705538461539">
“g¨or¨us¸melerin”
(g¨or),
“g¨or¨us¸ler”
“g¨oren”
(g¨or).
“g¨or¨us¸ler”
“g¨oren”
“g¨or¨us¸”
“g¨oren”.
“g¨or¨us¸ler”
“g¨or¨us¸¨un”
“g¨or¨us¸”.
t sentences.
</equation>
<bodyText confidence="0.9750205">
As the summarization algorithm, we used LexRank
(Erkan and Radev, 2004), which is a salient graph-
</bodyText>
<sectionHeader confidence="0.999865" genericHeader="method">
4 Experimental Setup
</sectionHeader>
<subsectionHeader confidence="0.999012">
4.1 Data Set
</subsectionHeader>
<bodyText confidence="0.995747128205128">
trix factorization (NMF) and used consecutive words
detection as a preprocessing step.
The effect of morphological analysis for Turkish
was analyzed in detail for Information Retrieval (Can
This section contains detailed information about the ap-
plication of different levels of morphological features
during the summarization process. Before diving into
the details, we provide a very brief description of the
morphological structure of the Turkish lan
Turkish is an agglutinative language with a productive
morphology. Root words can take one or more deriva-
tional and inflectional affixes; therefore, a root can be
seen in a large number of different word forms. An-
other issue is the morphological ambiguity, where a
word can have more than one morphological parse.
Table 1 shows an example list of different word
similarity
One of the greatest challenges for MDS studies in Turk-
ish is that there does not exist a manually annotated
data set. In this study, we have collected and manuall
y
annotated a Turkish MDS data set, which is publicly
available for future studies1.
In order to match the standards for MDS data sets,
we tried to follow the specifications of the DUC 2004
data set. Our data set consists of 21 clusters, each con-
sisting of around 10 documents. We selected 21 differ-
ent topics from different domains (e.g., politics, eco-
nomics, sports, social, daily, and technology), and se-
lected 10 documents on average for each topic. The
documents were obtained from the websites of various
news sources. The average number of words per doc-
ument is 337, and the average number of letters in a
word is 6.84.
For manual annotation, we divided the 21 clusters
into three groups and sent them to three annotators dif-
ferent from the authors. We required the human sum-
maries not to exceed 120 words for the summary of
each cluster.
</bodyText>
<subsectionHeader confidence="0.5958415">
4.2 Tools
4.2.1 Turkish Morphological Analysis
</subsectionHeader>
<bodyText confidence="0.999872222222222">
In order to perform different levels of morphological
analysis on documents, we used a two-level morpho-
logical analyzer (Oflazer, 1994) and a perceptron-based
morphological disambiguator (Sak et al., 2007), which
is trained with a corpus of about 750, 000 tokens from
news articles. The accuracy of the disambiguator has
been reported as 96% (Sak et al., 2007). The Root and
Deriv forms of words were generated from the disam-
biguator output.
</bodyText>
<subsubsectionHeader confidence="0.476269">
4.2.2 MEAD Summarization Toolkit
</subsubsectionHeader>
<bodyText confidence="0.999956521739131">
We used MEAD (Radev et al., 2004), which is an open-
source toolkit created for extractive MDS, in our exper-
iments. MEAD handles all the necessary processes to
generate a summary document (e.g., sentence ranking,
selection, re-ordering, and etc.).
We used the LexRank implementation that comes
with MEAD as a feature, together with the Cen-
troid and Position features (each feature is equally
weighted). We forced the generated summaries not to
exceed 120 words. However, we define the following
exception in order to preserve the readability and the
grammaticality of the generated summary. For a can-
didate sentence S having n words, if the absolute dif-
ference between the threshold (which is 120) and the
summary length including sentence S (say N,,,) is less
than the absolute difference between the threshold and
the summary length excluding sentence S (say N,,,,,),
and if N,,, is less than 132 (which is 120∗1.1), we allow
the summary to exceed the threshold and add sentence
S as the last summary sentence.
We used term frequency (tf) based cosine similarity
as the similarity measure during the sentence selection
step. We also required sentence length to be between
</bodyText>
<footnote confidence="0.921737666666667">
1The data set can be retrieved from the following github
repository: https://github.com/manuyavuz/
TurkishMDSDataSet_alpha
</footnote>
<bodyText confidence="0.972318428571429">
6 and 50 words (which we found empirically) in or-
der to increase the readability of the summaries. The
reason behind applying this filtering is that very short
sentences generally do not contain much information
to become a summary sentence, whereas very long sen-
tences decrease the readability and fill a significant per-
centage of the summary limit.
</bodyText>
<subsectionHeader confidence="0.688429">
4.2.3 ROUGE
</subsectionHeader>
<bodyText confidence="0.999888111111111">
For evaluation, we used ROUGE, which is a standard
metric for automated evaluation of summaries based
on n-gram co-occurrence. We used ROUGE-1 (based
on uni-grams), ROUGE-2 (based on bi-grams), and
ROUGE-W (based on longest common sub-sequence
weighted by length) in our experiments. Among these,
ROUGE-1 has been shown to agree with human judges
the most (Lin and Hovy, 2003), so we give importance
to it while interpreting the results.
</bodyText>
<sectionHeader confidence="0.991373" genericHeader="evaluation">
5 Evaluation and Results
</sectionHeader>
<bodyText confidence="0.999928461538461">
We ran MEAD with the proposed stemming policies
using different levels of cosine similarity threshold val-
ues to analyze the effect of the similarity threshold on
the summarization performance. After the sentences
are ranked using the LexRank method, the similarity
threshold is used to decide whether to include a sen-
tence to the summary. A sentence is not included to the
summary, if its similarity to a previously picked sen-
tence is larger than the similarity threshold.
In our preliminary experiments, we used the default
similarity threshold 0.7, which was found empirically
by the MEAD developers for English. However, it pro-
duced poor results on the Turkish data set.
</bodyText>
<table confidence="0.999892538461539">
Policy ROUGE-1 ROUGE-2 ROUGE-W
Prefix10 0.438 0.194 0.197
Prefix12 0.433 0.197 0.195
Prefix9 0.432 0.194 0.194
Prefix4 0.432 0.178 0.190
Prefix7 0.431 0.189 0.190
Prefix5 0.431 0.183 0.190
Prefix6 0.430 0.185 0.189
Raw 0.428 0.189 0.191
Deriv 0.428 0.178 0.188
Prefix8 0.427 0.187 0.188
Prefix11 0.427 0.190 0.193
Root 0.420 0.186 0.185
</table>
<tableCaption confidence="0.996335">
Table 2: Best scores for different policies
</tableCaption>
<bodyText confidence="0.9989373">
Figure 1 shows the F-1 scores for the ROUGE-1
metric for policies with different thresholds. After the
threshold exceeds 0.5, the performances for all poli-
cies start to decrease, so we don’t report the values
here to make the chart more readable. In general, Raw
and Prefix10 (taking the first 10 letters of the words)
achieve better performances with lower threshold val-
ues, whereas Root and Deriv operate better with rel-
atively higher threshold values. As we stated earlier,
in Turkish, words with similar meanings can occur in
</bodyText>
<page confidence="0.997329">
704
</page>
<figureCaption confidence="0.999126">
Figure 1: F-1 scores for different similarity threshold values
</figureCaption>
<figure confidence="0.993378833333333">
0.15 0.2 0.25 0.3 0.35 0.4 0.45 0.5
Similarity Thresholds
Raw Deriv Root Prefix 10
ROUGE-1 F-1 Scores
0.46
0.44
0.42
0.36
0.34
0.32
0.38
0.4
</figure>
<bodyText confidence="0.9989532">
text with different surface forms due to their inflec-
tions. Such words can not be matched during similar-
ity computation if morphological analysis is not per-
formed. Therefore, using higher similarity threshold
values cause very similar sentences to occur together
in the summaries, and eventually, result in poor scores.
Table 2 shows the best scores obtained by each pol-
icy. The Prefix policy generally outperforms the Raw
policy. The Prefix10 policy achieves the best ROUGE-
1 score. On the other hand, the policies that apply com-
plex morphological analysis (i.e. Root and Deriv) are
not able to outperform the simple Prefix and Raw poli-
cies. The Deriv policy performs similarly to the Raw
and Prefix policies, whereas the Root policy obtains the
lowest ROUGE-1 score.
</bodyText>
<subsectionHeader confidence="0.972728">
5.1 Discussion
</subsectionHeader>
<bodyText confidence="0.999973090909091">
The results show that using a simple fixed-length pre-
fix policy outperforms all other methods, and apply-
ing complex morphological analysis does not improve
Turkish MDS. The poor performance of the Root pol-
icy is somewhat expected due to the fact that, if we pre-
serve only the roots of the words, we lose the semantic
differences among the surface forms provided by the
derivational affixes. On the other hand, the reason be-
hind the observation that Deriv and Raw obtain similar
performances is not obvious.
In order to further analyze this observation, we
used an entropy based measure, which is calculated as
shown below, to quantify the homogeneity of the clus-
ters in the data set in terms of the variety of the surface
forms corresponding to the Deriv forms of each word
in the cluster. We first compute the entropy for each
Deriv form in a cluster. The entropy of a Deriv form
is lower, if it occurs with fewer different surface forms
in the cluster. The entropy of a cluster is computed by
summing the entropies of the Deriv forms in the clus-
ter and dividing the sum by the number of words in the
cluster (i.e. N).
</bodyText>
<equation confidence="0.9946315">
DDerivi _ {t  |t inflected from Deriv i}
H(Derivi) _ � p(t) log p(t)
t∈DDerivi
H(C) _ � H(Derivi)
N
i
</equation>
<bodyText confidence="0.9999619">
To compare with the data set clusters, we generated
random document clusters by randomly selecting 10
different clusters and then randomly selecting one doc-
ument from each selected cluster. The average entropy
value for the data set clusters and the random clusters
were 4.99 and 7.58, respectively. Due to this signifi-
cant difference, we can hypothesize that the documents
about the same topic show a more homogeneous struc-
ture. In other words, a Deriv form is usually seen in the
same surface form in a cluster of documents which are
about the same topic. Therefore, the Deriv policy and
the Raw policy achieve similar results for summarizing
documents about the same topic.
During evaluation, we ran ROUGE with the Deriv
versions of the human summaries and the system sum-
maries in order to match semantically similar words
having different surface forms. We also experimented
with ROUGE using the Raw versions, but the results
followed very similar patterns, so those results were not
reported.
</bodyText>
<sectionHeader confidence="0.999323" genericHeader="conclusions">
6 Conclusion
</sectionHeader>
<bodyText confidence="0.996344666666667">
In this paper, we reported the first steps for a multi-
document summarization system for Turkish. A manu-
ally annotated data set has been constructed from news
</bodyText>
<page confidence="0.994454">
705
</page>
<bodyText confidence="0.999889153846154">
articles, and made publicly available for future stud-
ies. We utilized the LexRank summarization algorithm,
and analyzed the effects of different stemming poli-
cies for Turkish MDS. Our results show that simple
fixed-length truncation methods with high limits (such
as taking the first 10 letters) improves summarization
scores. In contrast to our expectation, using morpho-
logical analysis does not enhance Turkish MDS, possi-
bly due to the homogeneousness of the documents in a
cluster to be summarized. As future work, we plan to
extend the data set with more clusters and more refer-
ence summaries, as well as to develop sentence com-
pression methods for Turkish MDS.
</bodyText>
<sectionHeader confidence="0.996874" genericHeader="acknowledgments">
Acknowledgments
</sectionHeader>
<bodyText confidence="0.9999642">
We would like to thank Ferhat Aydın for his contri-
butions during the data set corpus collection and an-
notation process. We would also like to thank Burak
Sivrikaya and Serkan Bugur for their help in generat-
ing the human summaries for the data set.
</bodyText>
<sectionHeader confidence="0.994924" genericHeader="references">
References
</sectionHeader>
<reference confidence="0.987306609195402">
Burak Kerim Akkus¸ and Ruket C¸akıcı. 2013. Catego-
rization of turkish news documents with morpholog-
ical analysis. In ACL (Student Research Workshop),
pages 1–8. The Association for Computer Linguis-
tics.
Zeynep Altan. 2004. A turkish automatic text sum-
marization system. In Proceedings of the IASTED
International Conference Artificial Intelligence and
Applications, pages 74–83.
Fazlı Can, Seyit Koc¸berber, Erman Balc¸ık, Cihan Kay-
nak, H Ca˘gdas¸ ¨Ocalan, and Onur M Vursavas¸. 2008.
Information retrieval on turkish texts. Journal of the
American Society for Information Science and Tech-
nology, 59(3):407–421.
Janara Christensen, Stephen Soderland Mausam, and
Oren Etzioni. 2013. Towards coherent multi-
document summarization. In Proceedings of
NAACL-HLT, pages 1163–1173.
G¨unes¸ Erkan and Dragomir R. Radev. 2004. Lex-
pagerank: Prestige in multi-document text summa-
rization. In EMNLP, pages 365–371. ACL.
G¨uls¸en Eryi˘git, Joakim Nivre, and Kemal Oflazer.
2008. Dependency parsing of turkish. Computa-
tional Linguistics, 34(3):357–389.
Eleni Galiotou, Nikitas Karanikolas, and Christodou-
los Tsoulloftas. 2013. On the effect of stemming al-
gorithms on extractive summarization: a case study.
In Panayiotis H. Ketikidis, Konstantinos G. Margari-
tis, Ioannis P. Vlahavas, Alexander Chatzigeorgiou,
George Eleftherakis, and Ioannis Stamelos, editors,
Panhellenic Conference on Informatics, pages 300–
304. ACM.
George Giannakopoulos. 2013. Multi-document mul-
tilingual summarization and evaluation tracks in acl
2013 multiling workshop. MultiLing 2013, page 20.
Vishal Gupta and Gurpreet Singh Lehal. 2010. A
survey of text summarization extractive techniques.
Journal of Emerging Technologies in Web Intelli-
gence, 2(3).
Aysun G¨uran, Eren Bekar, and S Akyokus¸. 2010.
A comparison of feature and semantic-based sum-
marization algorithms for turkish. In International
Symposium on Innovations in Intelligent Systems
and Applications. Citeseer.
A G¨uran, NG Bayazıt, and E Bekar. 2011. Au-
tomatic summarization of turkish documents using
non-negative matrix factorization. In Innovations in
Intelligent Systems and Applications (INISTA), 2011
International Symposium on, pages 480–484. IEEE.
Dilek Z. Hakkani-T¨ur, Kemal Oflazer, and G¨okhan T¨ur.
2000. Statistical morphological disambiguation for
agglutinative languages. In COLING, pages 285–
291. Morgan Kaufmann.
Chin-Yew Lin and Eduard H. Hovy. 2003. Au-
tomatic evaluation of summaries using n-gram co-
occurrence statistics. In HLT-NAACL.
Kemal Oflazer. 1994. Two-level description of turk-
ish morphology. Literary and linguistic computing,
9(2):137–148.
Lawrence Page, Sergey Brin, Rajeev Motwani, and
Terry Winograd. 1999. The pagerank citation rank-
ing: Bringing order to the web. Stanford InfoLab.
Dragomir Radev, Timothy Allison, Sasha Blair-
Goldensohn, John Blitzer, Arda Celebi, Stanko Dim-
itrov, Elliott Drabek, Ali Hakim, Wai Lam, Danyu
Liu, et al. 2004. Mead-a platform for multidocu-
ment multilingual text summarization. Proceedings
of the 4th International Conference on Language Re-
sources and Evaluation (LREC 2004).
Has¸im Sak, Tunga G¨ung¨or, and Murat Sarac¸lar. 2007.
Morphological disambiguation of turkish text with
perceptron algorithm. In Alexander F. Gelbukh, edi-
tor, CICLing, volume 4394 of Lecture Notes in Com-
puter Science, pages 107–118. Springer.
Chao Shen and Tao Li. 2010. Multi-document sum-
marization via the minimum dominating set. In Pro-
ceedings of the 23rd International Conference on
Computational Linguistics, pages 984–992. Associ-
ation for Computational Linguistics.
Celal C¸ı˘gır, M¨ucahid Kutlu, and ˙Ilyas C¸ic¸ekli. 2009.
Generic text summarization for turkish. In ISCIS,
pages 224–229. IEEE.
Makbule G¨ulc¸in ¨Ozsoy, ˙Ilyas C¸ic¸ekli, and Ferda Nur
Alpaslan. 2010. Text summarization of turkish texts
using latent semantic analysis. In Chu-Ren Huang
and Dan Jurafsky, editors, COLING, pages 869–876.
Tsinghua University Press.
</reference>
<page confidence="0.998564">
706
</page>
</variant>
</algorithm>
<algorithm name="ParsHed" version="110505">
<variant no="0" confidence="0.284171">
<title confidence="0.999122">Analyzing Stemming Approaches for Turkish Summarization</title>
<author confidence="0.995019">Yavuz Nuzumlalı Arzucan</author>
<affiliation confidence="0.7832085">Department of Computer Bebek,</affiliation>
<abstract confidence="0.966062666666667">In this study, we analyzed the effects of applying different levels of stemming approaches such as fixed-length word truncation and morphological analysis for multi-document summarization (MDS) on Turkish, which is an agglutinative and morphologically rich language. We constructed a manually annotated MDS data set, and to our best knowledge, reported the first results on Turkish MDS. Our results show that a simple fixed-length word truncation approach performs slightly better than no stemming, whereas applying complex morphological analysis does not improve Turkish MDS.</abstract>
</variant>
</algorithm>
<algorithm name="ParsCit" version="110505">
<citationList>
<citation valid="true">
<authors>
<author>Burak Kerim Akkus¸</author>
<author>Ruket C¸akıcı</author>
</authors>
<title>Categorization of turkish news documents with morphological analysis.</title>
<date>2013</date>
<booktitle>In ACL (Student Research Workshop),</booktitle>
<pages>1--8</pages>
<publisher>The Association</publisher>
<institution>for Computer Linguistics.</institution>
<marker>Akkus¸, C¸akıcı, 2013</marker>
<rawString>Burak Kerim Akkus¸ and Ruket C¸akıcı. 2013. Categorization of turkish news documents with morphological analysis. In ACL (Student Research Workshop), pages 1–8. The Association for Computer Linguistics.</rawString>
</citation>
<citation valid="true">
<authors>
<author>Zeynep Altan</author>
</authors>
<title>A turkish automatic text summarization system.</title>
<date>2004</date>
<booktitle>In Proceedings of the IASTED International Conference Artificial Intelligence and Applications,</booktitle>
<pages>74--83</pages>
<contexts>
<context position="2518" citStr="Altan, 2004" startWordPosition="378" endWordPosition="379">guages are needed (Eryi˘git et al., 2008). For instance, Turkish is an agglutinative language where root words can take many derivational and inflectional affixes. This feature results in a very high number of different word surface forms, and eventually leads to the data sparseness problem. Hakkani-T¨ur et al. (2000) analyzed the number of unique terms for Turkish and English and showed that the term count for Turkish is three times more than English for a corpus of 1M words. There are only a few studies for text summarization on Turkish, all of which are about single-document summarization (Altan, 2004; C¸ı˘gır et al., 2009; ¨Ozsoy et al., 2010; G¨uran et al., 2010; G¨uran et al., 2011). Some of these studies applied morphological analysis methods, but none of them analyzed their effects in detail. To our best knowledge, this paper reports the first multi-document summarization study for Turkish. We used LexRank as the main summarization algorithm (Erkan and Radev, 2004), applied and analyzed different levels of stemming methods such as complex morphological analysis and fixed-length word truncation. We also created the first manually annotated MDS data set for Turkish, which has been made </context>
<context position="4256" citStr="Altan (2004)" startWordPosition="657" endWordPosition="658">iscuss the results in Section 5, and conclude in Section 6. 2 Related Work A large number of methods have been proposed for multi-document summarization in the last 10-15 years (e.g. (Erkan and Radev, 2004; Shen and Li, 2010; Christensen et al., 2013)). While most of these approaches have only been applied to English, summarization data sets and systems for other languages like Chinese, Romanian, and Arabic have also been proposed in the recent years (Giannakopoulos, 2013). Previous studies on automatic summarization for Turkish only tackled the problem of single-document summarization (SDS). Altan (2004) and C¸ı˘gır et al. (2009) proposed feature-based approaches for Turkish SDS, whereas ¨Ozsoy et al. (2010) and G¨uran et al. (2010) used Latent Semantic Analysis (LSA) based methods. G¨uran et al. (2011) applied non-negative ma702 Proceedings of the 2014 Conference on Empirical Methods in Natural Language Processing (EMNLP), pages 702–706, October 25-29, 2014, Doha, Qatar. c�2014 Association for Computational Linguistics Word Analysis Table 1: Different word forms and their morphological an alysis for the stem “g¨or” (to see). The derivational boundaries are marked with (DB). g¨oren (the one w</context>
</contexts>
<marker>Altan, 2004</marker>
<rawString>Zeynep Altan. 2004. A turkish automatic text summarization system. In Proceedings of the IASTED International Conference Artificial Intelligence and Applications, pages 74–83.</rawString>
</citation>
<citation valid="true">
<authors>
<author>Fazlı Can</author>
<author>Seyit Koc¸berber</author>
<author>Erman Balc¸ık</author>
<author>Cihan Kaynak</author>
<author>H Ca˘gdas¸ ¨Ocalan</author>
<author>Onur M Vursavas¸</author>
</authors>
<title>Information retrieval on turkish texts.</title>
<date>2008</date>
<journal>Journal of the American Society for Information Science and Technology,</journal>
<volume>59</volume>
<issue>3</issue>
<marker>Can, Koc¸berber, Balc¸ık, Kaynak, ¨Ocalan, Vursavas¸, 2008</marker>
<rawString>Fazlı Can, Seyit Koc¸berber, Erman Balc¸ık, Cihan Kaynak, H Ca˘gdas¸ ¨Ocalan, and Onur M Vursavas¸. 2008. Information retrieval on turkish texts. Journal of the American Society for Information Science and Technology, 59(3):407–421.</rawString>
</citation>
<citation valid="true">
<authors>
<author>Janara Christensen</author>
<author>Stephen Soderland Mausam</author>
<author>Oren Etzioni</author>
</authors>
<title>Towards coherent multidocument summarization.</title>
<date>2013</date>
<booktitle>In Proceedings of NAACL-HLT,</booktitle>
<pages>1163--1173</pages>
<contexts>
<context position="3895" citStr="Christensen et al., 2013" startWordPosition="601" endWordPosition="604">ications of morphological analysis on Turkish for different Natural Language Processing (NLP) and Information Retrieval (IR) problems. In Section 3, we provide a very brief introduction to the Turkish morphology and present the stemming methods that we evaluated. The details about the created data set and our experimental setup are presented in Section 4. We present and discuss the results in Section 5, and conclude in Section 6. 2 Related Work A large number of methods have been proposed for multi-document summarization in the last 10-15 years (e.g. (Erkan and Radev, 2004; Shen and Li, 2010; Christensen et al., 2013)). While most of these approaches have only been applied to English, summarization data sets and systems for other languages like Chinese, Romanian, and Arabic have also been proposed in the recent years (Giannakopoulos, 2013). Previous studies on automatic summarization for Turkish only tackled the problem of single-document summarization (SDS). Altan (2004) and C¸ı˘gır et al. (2009) proposed feature-based approaches for Turkish SDS, whereas ¨Ozsoy et al. (2010) and G¨uran et al. (2010) used Latent Semantic Analysis (LSA) based methods. G¨uran et al. (2011) applied non-negative ma702 Proceedi</context>
</contexts>
<marker>Christensen, Mausam, Etzioni, 2013</marker>
<rawString>Janara Christensen, Stephen Soderland Mausam, and Oren Etzioni. 2013. Towards coherent multidocument summarization. In Proceedings of NAACL-HLT, pages 1163–1173.</rawString>
</citation>
<citation valid="true">
<authors>
<author>G¨unes¸ Erkan</author>
<author>Dragomir R Radev</author>
</authors>
<title>Lexpagerank: Prestige in multi-document text summarization.</title>
<date>2004</date>
<booktitle>In EMNLP,</booktitle>
<pages>365--371</pages>
<publisher>ACL.</publisher>
<contexts>
<context position="2894" citStr="Erkan and Radev, 2004" startWordPosition="436" endWordPosition="439">sh and English and showed that the term count for Turkish is three times more than English for a corpus of 1M words. There are only a few studies for text summarization on Turkish, all of which are about single-document summarization (Altan, 2004; C¸ı˘gır et al., 2009; ¨Ozsoy et al., 2010; G¨uran et al., 2010; G¨uran et al., 2011). Some of these studies applied morphological analysis methods, but none of them analyzed their effects in detail. To our best knowledge, this paper reports the first multi-document summarization study for Turkish. We used LexRank as the main summarization algorithm (Erkan and Radev, 2004), applied and analyzed different levels of stemming methods such as complex morphological analysis and fixed-length word truncation. We also created the first manually annotated MDS data set for Turkish, which has been made publicly available for future studies. The rest of the paper is organized as follows. Section 2 presents the related work on MDS, as well as on the applications of morphological analysis on Turkish for different Natural Language Processing (NLP) and Information Retrieval (IR) problems. In Section 3, we provide a very brief introduction to the Turkish morphology and present </context>
<context position="8206" citStr="Erkan and Radev, 2004" startWordPosition="1250" endWordPosition="1253">t method and can help match similar words by taking the first n characters of words which have lengths larger than n. based method that achieves promising results for MDS. In LexRank, first a sentence connectivity graph is constructed based on the cosine similarities between sentences, and then the PageRank (Page et al., 1999) algorithm is used to find the most importan “g¨oren”,“g¨or¨us¸¨un”, “g¨or¨us¸melerin” (g¨or), “g¨or¨us¸ler” “g¨oren” (g¨or). “g¨or¨us¸ler” “g¨oren” “g¨or¨us¸” “g¨oren”. “g¨or¨us¸ler” “g¨or¨us¸¨un” “g¨or¨us¸”. t sentences. As the summarization algorithm, we used LexRank (Erkan and Radev, 2004), which is a salient graph4 Experimental Setup 4.1 Data Set trix factorization (NMF) and used consecutive words detection as a preprocessing step. The effect of morphological analysis for Turkish was analyzed in detail for Information Retrieval (Can This section contains detailed information about the application of different levels of morphological features during the summarization process. Before diving into the details, we provide a very brief description of the morphological structure of the Turkish lan Turkish is an agglutinative language with a productive morphology. Root words can take </context>
</contexts>
<marker>Erkan, Radev, 2004</marker>
<rawString>G¨unes¸ Erkan and Dragomir R. Radev. 2004. Lexpagerank: Prestige in multi-document text summarization. In EMNLP, pages 365–371. ACL.</rawString>
</citation>
<citation valid="true">
<authors>
<author>G¨uls¸en Eryi˘git</author>
<author>Joakim Nivre</author>
<author>Kemal Oflazer</author>
</authors>
<title>Dependency parsing of turkish.</title>
<date>2008</date>
<journal>Computational Linguistics,</journal>
<volume>34</volume>
<issue>3</issue>
<marker>Eryi˘git, Nivre, Oflazer, 2008</marker>
<rawString>G¨uls¸en Eryi˘git, Joakim Nivre, and Kemal Oflazer. 2008. Dependency parsing of turkish. Computational Linguistics, 34(3):357–389.</rawString>
</citation>
<citation valid="true">
<authors>
<author>Eleni Galiotou</author>
</authors>
<title>Nikitas Karanikolas, and Christodoulos Tsoulloftas.</title>
<date>2013</date>
<booktitle>Panhellenic Conference on Informatics,</booktitle>
<pages>300--304</pages>
<editor>In Panayiotis H. Ketikidis, Konstantinos G. Margaritis, Ioannis P. Vlahavas, Alexander Chatzigeorgiou, George Eleftherakis, and Ioannis Stamelos, editors,</editor>
<publisher>ACM.</publisher>
<marker>Galiotou, 2013</marker>
<rawString>Eleni Galiotou, Nikitas Karanikolas, and Christodoulos Tsoulloftas. 2013. On the effect of stemming algorithms on extractive summarization: a case study. In Panayiotis H. Ketikidis, Konstantinos G. Margaritis, Ioannis P. Vlahavas, Alexander Chatzigeorgiou, George Eleftherakis, and Ioannis Stamelos, editors, Panhellenic Conference on Informatics, pages 300– 304. ACM.</rawString>
</citation>
<citation valid="true">
<authors>
<author>George Giannakopoulos</author>
</authors>
<title>Multi-document multilingual summarization and evaluation tracks in acl 2013 multiling workshop. MultiLing</title>
<date>2013</date>
<pages>20</pages>
<contexts>
<context position="1656" citStr="Giannakopoulos, 2013" startWordPosition="239" endWordPosition="240">unt of resources. However, this situation brings its own challenges such as finding the relevant documents, and absorbing a large quantity of relevant information (Gupta and Lehal, 2010). The goal of multi-document summarization (MDS) is to automatically create a summary of a set of documents about the same topic without losing the important information. Several approaches for MDS have been proposed in the last decade. However, most of them have only been applied to a relatively small set of languages, mostly English, and recently also to languages like Chinese, Romanian, Arabic, and Spanish (Giannakopoulos, 2013). Previous studies have shown that methods proposed for languages like English do not generally work well for morphologically rich languages like Finnish, Turkish, and Czech, and additional methods considering the morphological structures of these languages are needed (Eryi˘git et al., 2008). For instance, Turkish is an agglutinative language where root words can take many derivational and inflectional affixes. This feature results in a very high number of different word surface forms, and eventually leads to the data sparseness problem. Hakkani-T¨ur et al. (2000) analyzed the number of unique</context>
<context position="4121" citStr="Giannakopoulos, 2013" startWordPosition="640" endWordPosition="641">emming methods that we evaluated. The details about the created data set and our experimental setup are presented in Section 4. We present and discuss the results in Section 5, and conclude in Section 6. 2 Related Work A large number of methods have been proposed for multi-document summarization in the last 10-15 years (e.g. (Erkan and Radev, 2004; Shen and Li, 2010; Christensen et al., 2013)). While most of these approaches have only been applied to English, summarization data sets and systems for other languages like Chinese, Romanian, and Arabic have also been proposed in the recent years (Giannakopoulos, 2013). Previous studies on automatic summarization for Turkish only tackled the problem of single-document summarization (SDS). Altan (2004) and C¸ı˘gır et al. (2009) proposed feature-based approaches for Turkish SDS, whereas ¨Ozsoy et al. (2010) and G¨uran et al. (2010) used Latent Semantic Analysis (LSA) based methods. G¨uran et al. (2011) applied non-negative ma702 Proceedings of the 2014 Conference on Empirical Methods in Natural Language Processing (EMNLP), pages 702–706, October 25-29, 2014, Doha, Qatar. c�2014 Association for Computational Linguistics Word Analysis Table 1: Different word fo</context>
</contexts>
<marker>Giannakopoulos, 2013</marker>
<rawString>George Giannakopoulos. 2013. Multi-document multilingual summarization and evaluation tracks in acl 2013 multiling workshop. MultiLing 2013, page 20.</rawString>
</citation>
<citation valid="true">
<authors>
<author>Vishal Gupta</author>
<author>Gurpreet Singh Lehal</author>
</authors>
<title>A survey of text summarization extractive techniques.</title>
<date>2010</date>
<journal>Journal of Emerging Technologies in Web Intelligence,</journal>
<volume>2</volume>
<issue>3</issue>
<contexts>
<context position="1221" citStr="Gupta and Lehal, 2010" startWordPosition="167" endWordPosition="170">, reported the first results on Turkish MDS. Our results show that a simple fixed-length word truncation approach performs slightly better than no stemming, whereas applying complex morphological analysis does not improve Turkish MDS. 1 Introduction Automatic text summarization has gained more importance with the enormous growth and easy availability of the Internet. It is now possible to reach extensive and continuously growing amount of resources. However, this situation brings its own challenges such as finding the relevant documents, and absorbing a large quantity of relevant information (Gupta and Lehal, 2010). The goal of multi-document summarization (MDS) is to automatically create a summary of a set of documents about the same topic without losing the important information. Several approaches for MDS have been proposed in the last decade. However, most of them have only been applied to a relatively small set of languages, mostly English, and recently also to languages like Chinese, Romanian, Arabic, and Spanish (Giannakopoulos, 2013). Previous studies have shown that methods proposed for languages like English do not generally work well for morphologically rich languages like Finnish, Turkish, a</context>
</contexts>
<marker>Gupta, Lehal, 2010</marker>
<rawString>Vishal Gupta and Gurpreet Singh Lehal. 2010. A survey of text summarization extractive techniques. Journal of Emerging Technologies in Web Intelligence, 2(3).</rawString>
</citation>
<citation valid="true">
<authors>
<author>Aysun G¨uran</author>
<author>Eren Bekar</author>
<author>S Akyokus¸</author>
</authors>
<title>A comparison of feature and semantic-based summarization algorithms for turkish.</title>
<date>2010</date>
<booktitle>In International Symposium on Innovations in Intelligent Systems and Applications. Citeseer.</booktitle>
<marker>G¨uran, Bekar, Akyokus¸, 2010</marker>
<rawString>Aysun G¨uran, Eren Bekar, and S Akyokus¸. 2010. A comparison of feature and semantic-based summarization algorithms for turkish. In International Symposium on Innovations in Intelligent Systems and Applications. Citeseer.</rawString>
</citation>
<citation valid="true">
<authors>
<author>A G¨uran</author>
<author>NG Bayazıt</author>
<author>E Bekar</author>
</authors>
<title>Automatic summarization of turkish documents using non-negative matrix factorization.</title>
<date>2011</date>
<booktitle>In Innovations in Intelligent Systems and Applications (INISTA), 2011 International Symposium on,</booktitle>
<pages>480--484</pages>
<publisher>IEEE.</publisher>
<marker>G¨uran, Bayazıt, Bekar, 2011</marker>
<rawString>A G¨uran, NG Bayazıt, and E Bekar. 2011. Automatic summarization of turkish documents using non-negative matrix factorization. In Innovations in Intelligent Systems and Applications (INISTA), 2011 International Symposium on, pages 480–484. IEEE.</rawString>
</citation>
<citation valid="true">
<authors>
<author>Dilek Z Hakkani-T¨ur</author>
<author>Kemal Oflazer</author>
<author>G¨okhan T¨ur</author>
</authors>
<title>Statistical morphological disambiguation for agglutinative languages. In</title>
<date>2000</date>
<booktitle>COLING,</booktitle>
<pages>285--291</pages>
<publisher>Morgan Kaufmann.</publisher>
<marker>Hakkani-T¨ur, Oflazer, T¨ur, 2000</marker>
<rawString>Dilek Z. Hakkani-T¨ur, Kemal Oflazer, and G¨okhan T¨ur. 2000. Statistical morphological disambiguation for agglutinative languages. In COLING, pages 285– 291. Morgan Kaufmann.</rawString>
</citation>
<citation valid="true">
<authors>
<author>Chin-Yew Lin</author>
<author>Eduard H Hovy</author>
</authors>
<title>Automatic evaluation of summaries using n-gram cooccurrence statistics.</title>
<date>2003</date>
<booktitle>In HLT-NAACL.</booktitle>
<contexts>
<context position="12657" citStr="Lin and Hovy, 2003" startWordPosition="1972" endWordPosition="1975">lying this filtering is that very short sentences generally do not contain much information to become a summary sentence, whereas very long sentences decrease the readability and fill a significant percentage of the summary limit. 4.2.3 ROUGE For evaluation, we used ROUGE, which is a standard metric for automated evaluation of summaries based on n-gram co-occurrence. We used ROUGE-1 (based on uni-grams), ROUGE-2 (based on bi-grams), and ROUGE-W (based on longest common sub-sequence weighted by length) in our experiments. Among these, ROUGE-1 has been shown to agree with human judges the most (Lin and Hovy, 2003), so we give importance to it while interpreting the results. 5 Evaluation and Results We ran MEAD with the proposed stemming policies using different levels of cosine similarity threshold values to analyze the effect of the similarity threshold on the summarization performance. After the sentences are ranked using the LexRank method, the similarity threshold is used to decide whether to include a sentence to the summary. A sentence is not included to the summary, if its similarity to a previously picked sentence is larger than the similarity threshold. In our preliminary experiments, we used </context>
</contexts>
<marker>Lin, Hovy, 2003</marker>
<rawString>Chin-Yew Lin and Eduard H. Hovy. 2003. Automatic evaluation of summaries using n-gram cooccurrence statistics. In HLT-NAACL.</rawString>
</citation>
<citation valid="true">
<authors>
<author>Kemal Oflazer</author>
</authors>
<title>Two-level description of turkish morphology.</title>
<date>1994</date>
<booktitle>Literary and linguistic computing,</booktitle>
<pages>9--2</pages>
<contexts>
<context position="10283" citStr="Oflazer, 1994" startWordPosition="1595" endWordPosition="1596">10 documents on average for each topic. The documents were obtained from the websites of various news sources. The average number of words per document is 337, and the average number of letters in a word is 6.84. For manual annotation, we divided the 21 clusters into three groups and sent them to three annotators different from the authors. We required the human summaries not to exceed 120 words for the summary of each cluster. 4.2 Tools 4.2.1 Turkish Morphological Analysis In order to perform different levels of morphological analysis on documents, we used a two-level morphological analyzer (Oflazer, 1994) and a perceptron-based morphological disambiguator (Sak et al., 2007), which is trained with a corpus of about 750, 000 tokens from news articles. The accuracy of the disambiguator has been reported as 96% (Sak et al., 2007). The Root and Deriv forms of words were generated from the disambiguator output. 4.2.2 MEAD Summarization Toolkit We used MEAD (Radev et al., 2004), which is an opensource toolkit created for extractive MDS, in our experiments. MEAD handles all the necessary processes to generate a summary document (e.g., sentence ranking, selection, re-ordering, and etc.). We used the Le</context>
</contexts>
<marker>Oflazer, 1994</marker>
<rawString>Kemal Oflazer. 1994. Two-level description of turkish morphology. Literary and linguistic computing, 9(2):137–148.</rawString>
</citation>
<citation valid="true">
<authors>
<author>Lawrence Page</author>
<author>Sergey Brin</author>
<author>Rajeev Motwani</author>
<author>Terry Winograd</author>
</authors>
<title>The pagerank citation ranking: Bringing order to the web. Stanford InfoLab.</title>
<date>1999</date>
<contexts>
<context position="7912" citStr="Page et al., 1999" startWordPosition="1215" endWordPosition="1218">fix: In Turkish, affixes almost always occur as suffixes, not prefixes. Additionally, applying morphological analysis methods is a time consuming process, and may become an overhead for online applications. Therefore, afixed-length simplification method is also tried, since itis both a fast method and can help match similar words by taking the first n characters of words which have lengths larger than n. based method that achieves promising results for MDS. In LexRank, first a sentence connectivity graph is constructed based on the cosine similarities between sentences, and then the PageRank (Page et al., 1999) algorithm is used to find the most importan “g¨oren”,“g¨or¨us¸¨un”, “g¨or¨us¸melerin” (g¨or), “g¨or¨us¸ler” “g¨oren” (g¨or). “g¨or¨us¸ler” “g¨oren” “g¨or¨us¸” “g¨oren”. “g¨or¨us¸ler” “g¨or¨us¸¨un” “g¨or¨us¸”. t sentences. As the summarization algorithm, we used LexRank (Erkan and Radev, 2004), which is a salient graph4 Experimental Setup 4.1 Data Set trix factorization (NMF) and used consecutive words detection as a preprocessing step. The effect of morphological analysis for Turkish was analyzed in detail for Information Retrieval (Can This section contains detailed information about the app</context>
</contexts>
<marker>Page, Brin, Motwani, Winograd, 1999</marker>
<rawString>Lawrence Page, Sergey Brin, Rajeev Motwani, and Terry Winograd. 1999. The pagerank citation ranking: Bringing order to the web. Stanford InfoLab.</rawString>
</citation>
<citation valid="true">
<authors>
<author>Dragomir Radev</author>
<author>Timothy Allison</author>
<author>Sasha BlairGoldensohn</author>
<author>John Blitzer</author>
<author>Arda Celebi</author>
<author>Stanko Dimitrov</author>
<author>Elliott Drabek</author>
<author>Ali Hakim</author>
<author>Wai Lam</author>
<author>Danyu Liu</author>
</authors>
<title>Mead-a platform for multidocument multilingual text summarization.</title>
<date>2004</date>
<booktitle>Proceedings of the 4th International Conference on Language Resources and Evaluation (LREC</booktitle>
<contexts>
<context position="10656" citStr="Radev et al., 2004" startWordPosition="1655" endWordPosition="1658">maries not to exceed 120 words for the summary of each cluster. 4.2 Tools 4.2.1 Turkish Morphological Analysis In order to perform different levels of morphological analysis on documents, we used a two-level morphological analyzer (Oflazer, 1994) and a perceptron-based morphological disambiguator (Sak et al., 2007), which is trained with a corpus of about 750, 000 tokens from news articles. The accuracy of the disambiguator has been reported as 96% (Sak et al., 2007). The Root and Deriv forms of words were generated from the disambiguator output. 4.2.2 MEAD Summarization Toolkit We used MEAD (Radev et al., 2004), which is an opensource toolkit created for extractive MDS, in our experiments. MEAD handles all the necessary processes to generate a summary document (e.g., sentence ranking, selection, re-ordering, and etc.). We used the LexRank implementation that comes with MEAD as a feature, together with the Centroid and Position features (each feature is equally weighted). We forced the generated summaries not to exceed 120 words. However, we define the following exception in order to preserve the readability and the grammaticality of the generated summary. For a candidate sentence S having n words, i</context>
</contexts>
<marker>Radev, Allison, BlairGoldensohn, Blitzer, Celebi, Dimitrov, Drabek, Hakim, Lam, Liu, 2004</marker>
<rawString>Dragomir Radev, Timothy Allison, Sasha BlairGoldensohn, John Blitzer, Arda Celebi, Stanko Dimitrov, Elliott Drabek, Ali Hakim, Wai Lam, Danyu Liu, et al. 2004. Mead-a platform for multidocument multilingual text summarization. Proceedings of the 4th International Conference on Language Resources and Evaluation (LREC 2004).</rawString>
</citation>
<citation valid="true">
<authors>
<author>Has¸im Sak</author>
<author>Tunga G¨ung¨or</author>
<author>Murat Sarac¸lar</author>
</authors>
<title>Morphological disambiguation of turkish text with perceptron algorithm.</title>
<date>2007</date>
<booktitle>of Lecture Notes in Computer Science,</booktitle>
<volume>4394</volume>
<pages>107--118</pages>
<editor>In Alexander F. Gelbukh, editor, CICLing,</editor>
<publisher>Springer.</publisher>
<marker>Sak, G¨ung¨or, Sarac¸lar, 2007</marker>
<rawString>Has¸im Sak, Tunga G¨ung¨or, and Murat Sarac¸lar. 2007. Morphological disambiguation of turkish text with perceptron algorithm. In Alexander F. Gelbukh, editor, CICLing, volume 4394 of Lecture Notes in Computer Science, pages 107–118. Springer.</rawString>
</citation>
<citation valid="true">
<authors>
<author>Chao Shen</author>
<author>Tao Li</author>
</authors>
<title>Multi-document summarization via the minimum dominating set.</title>
<date>2010</date>
<booktitle>In Proceedings of the 23rd International Conference on Computational Linguistics,</booktitle>
<pages>984--992</pages>
<publisher>Association</publisher>
<institution>for Computational Linguistics.</institution>
<contexts>
<context position="3868" citStr="Shen and Li, 2010" startWordPosition="597" endWordPosition="600">well as on the applications of morphological analysis on Turkish for different Natural Language Processing (NLP) and Information Retrieval (IR) problems. In Section 3, we provide a very brief introduction to the Turkish morphology and present the stemming methods that we evaluated. The details about the created data set and our experimental setup are presented in Section 4. We present and discuss the results in Section 5, and conclude in Section 6. 2 Related Work A large number of methods have been proposed for multi-document summarization in the last 10-15 years (e.g. (Erkan and Radev, 2004; Shen and Li, 2010; Christensen et al., 2013)). While most of these approaches have only been applied to English, summarization data sets and systems for other languages like Chinese, Romanian, and Arabic have also been proposed in the recent years (Giannakopoulos, 2013). Previous studies on automatic summarization for Turkish only tackled the problem of single-document summarization (SDS). Altan (2004) and C¸ı˘gır et al. (2009) proposed feature-based approaches for Turkish SDS, whereas ¨Ozsoy et al. (2010) and G¨uran et al. (2010) used Latent Semantic Analysis (LSA) based methods. G¨uran et al. (2011) applied </context>
</contexts>
<marker>Shen, Li, 2010</marker>
<rawString>Chao Shen and Tao Li. 2010. Multi-document summarization via the minimum dominating set. In Proceedings of the 23rd International Conference on Computational Linguistics, pages 984–992. Association for Computational Linguistics.</rawString>
</citation>
<citation valid="true">
<authors>
<author>Celal C¸ı˘gır</author>
<author>M¨ucahid Kutlu</author>
<author>˙Ilyas C¸ic¸ekli</author>
</authors>
<title>Generic text summarization for turkish.</title>
<date>2009</date>
<booktitle>In ISCIS,</booktitle>
<pages>224--229</pages>
<publisher>IEEE.</publisher>
<marker>C¸ı˘gır, Kutlu, C¸ic¸ekli, 2009</marker>
<rawString>Celal C¸ı˘gır, M¨ucahid Kutlu, and ˙Ilyas C¸ic¸ekli. 2009. Generic text summarization for turkish. In ISCIS, pages 224–229. IEEE.</rawString>
</citation>
<citation valid="true">
<authors>
<author>Makbule G¨ulc¸in ¨Ozsoy</author>
</authors>
<title>Ilyas C¸ic¸ekli, and Ferda Nur Alpaslan.</title>
<date>2010</date>
<booktitle>In Chu-Ren Huang and</booktitle>
<pages>869--876</pages>
<editor>Dan Jurafsky, editors, COLING,</editor>
<publisher>Tsinghua University Press.</publisher>
<marker>¨Ozsoy, 2010</marker>
<rawString>Makbule G¨ulc¸in ¨Ozsoy, ˙Ilyas C¸ic¸ekli, and Ferda Nur Alpaslan. 2010. Text summarization of turkish texts using latent semantic analysis. In Chu-Ren Huang and Dan Jurafsky, editors, COLING, pages 869–876. Tsinghua University Press.</rawString>
</citation>
</citationList>
</algorithm>
</algorithms>
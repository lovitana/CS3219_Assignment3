<?xml version="1.0" encoding="UTF-8"?>
<algorithms version="110505">
<algorithm name="SectLabel" version="110505">
<variant no="0" confidence="0.001273">
<title confidence="0.998544">
A Graph-based Approach for Contextual Text Normalization
</title>
<author confidence="0.693514">
C¸ a˘gıl S¨onmez and Arzucan ¨Ozg¨ur
</author>
<affiliation confidence="0.993431">
Department of Computer Engineering
Bogazici University
</affiliation>
<address confidence="0.742446">
Bebek, 34342 Istanbul, Turkey
</address>
<email confidence="0.999432">
{cagil.ulusahin,arzucan.ozgur}@boun.edu.tr
</email>
<sectionHeader confidence="0.997397" genericHeader="abstract">
Abstract
</sectionHeader>
<bodyText confidence="0.9999533125">
The informal nature of social media text
renders it very difficult to be automati-
cally processed by natural language pro-
cessing tools. Text normalization, which
corresponds to restoring the non-standard
words to their canonical forms, provides a
solution to this challenge. We introduce an
unsupervised text normalization approach
that utilizes not only lexical, but also con-
textual and grammatical features of social
text. The contextual and grammatical fea-
tures are extracted from a word association
graph built by using a large unlabeled so-
cial media text corpus. The graph encodes
the relative positions of the words with re-
spect to each other, as well as their part-of-
speech tags. The lexical features are ob-
tained by using the longest common sub-
sequence ratio and edit distance measures
to encode the surface similarity among
words, and the double metaphone algo-
rithm to represent the phonetic similarity.
Unlike most of the recent approaches that
are based on generating normalization dic-
tionaries, the proposed approach performs
normalization by considering the context
of the non-standard words in the input text.
Our results show that it achieves state-of-
the-art F-score performance on standard
datasets. In addition, the system can be
tuned to achieve very high precision with-
out sacrificing much from recall.
</bodyText>
<sectionHeader confidence="0.99963" genericHeader="introduction">
1 Introduction
</sectionHeader>
<bodyText confidence="0.999963682926829">
Social text, which has been growing and evolving
steadily, has its own lexical and grammatical fea-
tures (Choudhury et al., 2007; Eisenstein, 2013).
lol meaning laughing out loud, xoxo meaning kiss-
ing, 4u meaning for you are among the most com-
monly used examples of this jargon. In addition,
these informal expressions in social text usually
take many different lexical forms when generated
by different individuals (Eisenstein, 2013). The
limited accuracies of the Speech-to-Text (STT)
tools in mobile devices, which are increasingly be-
ing used to post messages on social media plat-
forms, along with the scarcity of attention of
the users result in additional divergence of so-
cial text from more standard text such as from
the newswire domain. Tools such as spellchecker
and slang dictionaries have been shown to be in-
sufficient to cope with this challenge long time
ago (Sproat et al., 2001). In addition, most Nat-
ural Language Processing (NLP) tools including
named entity recognizers and dependency parsers
generally perform poorly on social text (Ritter et
al., 2010).
Text normalization is a preprocessing step to
restore non-standard words in text to their origi-
nal (canonical) forms to make use in NLP applica-
tions or more broadly to understand the digitized
text better (Han and Baldwin, 2011). For exam-
ple, talk 2 u later can be normalized as talk to you
later or similarly enormoooos, enrmss and enour-
mos can be normalized as enormous. Other exam-
ples of text messages from Twitter and their corre-
sponding normalized forms are shown in Table 1.
The non-standard words in text are referred to
as Out of Vocabulary (OOV) words. The nor-
malization task restores the OOV words to their
In Vocabulary (IV) forms. Social text is contin-
uously evolving with new words and named en-
tities that are not in the vocabularies of the sys-
tems (Hassan and Menezes, 2013). Therefore, not
every OOV word (e.g. iPhone, WikiLeaks or tok-
</bodyText>
<page confidence="0.991527">
313
</page>
<note confidence="0.929296">
Proceedings of the 2014 Conference on Empirical Methods in Natural Language Processing (EMNLP), pages 313–324,
October 25-29, 2014, Doha, Qatar. c�2014 Association for Computational Linguistics
</note>
<construct confidence="0.6026755">
Hav guts to say wat u desire.. Dnt beat behind da bush!!
And 1 mre thng no mre say y r people’s man!!
There r sm songs u don’t want 2 listen 2 yl walking cos
when u start dancing ppl won’t knw y.
</construct>
<bodyText confidence="0.98693825">
Have guts to say what you desire.. Don’t beat behind the bush!!
And one more thing no more say you are people’s man!!
There are some songs you don’t want to listen to while walking
because when you start dancing people won’t know why.
</bodyText>
<tableCaption confidence="0.983952">
Table 1: Sample tweets and their normalized forms.
</tableCaption>
<bodyText confidence="0.999636512820513">
enizing) should be considered for normalization.
The OOV tokens that should be considered for
normalization are referred to as ill-formed words.
Ill-formed words can be normalized to different
canonical words depending on the context of the
text. For example, let’s consider the two examples
in Table 1. “y” is normalized as “you” in the first
one and as “why” in the second one.
In this paper, we propose a graph-based text
normalization method that utilizes both contex-
tual and grammatical features of social text. The
contextual information of words is modeled by
a word association graph that is created from a
large social media text corpus. The graph repre-
sents the relative positions of the words in the so-
cial media text messages and their Part-of-Speech
(POS) tags. The lexical similarity features among
the words are modeled using the longest common
subsequence ratio and edit distance that encode
the surface similarity and the double metaphone
algorithm that encodes the phonetic similarity.
The proposed approach is unsupervised, which is
an important advantage over supervised systems,
given the continuously evolving language in the
social media domain. The same OOV word may
have different appropriate normalizations depend-
ing on the context of the input text message. Re-
cently proposed dictionary-based text normaliza-
tion systems perform dictionary look-up and al-
ways normalize the same OOV word to the same
IV word regardless of the context of the input text
(Han et al., 2012; Hassan and Menezes, 2013). On
the other hand, the proposed approach does not
only make use of the general context information
in a large corpus of social media text, but it also
makes use of the context of the OOV word in the
input text message. Thus, an OOV word can be
normalized to different IV words depending on the
context of the input text.
</bodyText>
<sectionHeader confidence="0.999953" genericHeader="related work">
2 Related Work
</sectionHeader>
<bodyText confidence="0.999868428571428">
Early work on text normalization mostly made
use of the noisy channel model. The first work
that had a significant performance improvement
over the previous research was by Brill and Moore
(2000). They proposed a novel noisy channel
model for spell checking based on string to string
edits. Their model depended on probabilistic mod-
eling of sub-string transformations.
Toutanova and Moore (2002) improved this ap-
proach by extending the error model with phonetic
similarities over words. Their approach is based
on learning rules to predict the pronunciation of a
single letter in the word depending on the neigh-
bouring letters in the word.
Choudhury et al. (2007) developed a super-
vised Hidden Markov Model based approach for
normalizing Short Message Service (SMS) texts.
They proposed a word for word decoding ap-
proach and used a dictionary based method to
normalize commonly used abbreviations and non-
standard usage (e.g. “howz” to “how are” or
“aint” to “are not”). Cook and Stevenson (2009)
extended this model by introducing an unsuper-
vised noisy channel model. Rather than using
one generic model for all word formations as
in (Choudhury et al., 2007), they used a mix-
ture model in which each different word formation
type is modeled explicitly.
The limitations of these methods were that they
did not consider contextual features and assumed
that tokens have unique normalizations. In the text
normalization task several OOV tokens are am-
biguous and without contextual information it is
not possible to build models that can disambiguate
transformations correctly.
Aw et al. (2006) proposed a phrase-based statis-
tical machine translation (MT) model for the text
normalization task. They defined the problem as
translating the SMS language to the English lan-
guage and based their model on two submodels:
a word based language model and a phrase based
lexical mapping model (channel model). Their
system also benefits from the input context and
they argue that the strength of their model is in
its ability to disambiguate mapping as in “2” →
“two” or “to”, and “w” → “with” or “who”. Mak-
ing use of the whole conversation, this is the clos-
est approach to ours in the sense of utilizing con-
textual sensitivity and coverage.
</bodyText>
<page confidence="0.998497">
314
</page>
<bodyText confidence="0.999763666666667">
Pennell and Liu (2011) on the other hand, pro-
posed a character level MT system, that is robust
to new abbreviations. In their two phased system,
a character level trained MT model is used to pro-
duce word hypotheses and a trigram LM is used to
choose a hypothesis that fits into the input context.
The MT based models are supervised models,
a drawback of which is that they require anno-
tated data. Annotated training data is not readily
available and is difficult to create especially for
the rapidly evolving social media text (Yang and
Eisenstein, 2013).
More recent approaches handled the text nor-
malization task by building normalization lexi-
cons. Han and Baldwin (2011) developed a two
phased model, where they only consider the ill-
formed OOV words for normalization. First, a
confusion set is generated using the lexical and
phonetic distance features. Later, the candidates
in the confusion set are ranked using a mixture
of dictionary look up, word similarity based on
lexical edit distance, phonemic edit distance, pre-
fix sub-string, suffix sub-string and longest com-
mon subsequence (LCS), as well as context sup-
port metrics. Chrupala (2014) on the other hand
achieved lower word error rates without using any
lexical resources.
Gouws et al. (2011) investigated the distinct
contributions of features that are highly depended
on user-centric information such as the geologi-
cal location of the users and the twitter client that
the tweet is received from. Using such user-based
contextual metrics they modelled the transforma-
tion distributions across populations.
Liu et al. (2012) proposed a broad coverage nor-
malization system, which integrates an extended
noisy channel model, that is based on enhanced
letter transformations, visual priming, string and
phonetic similarity. They try to improve the per-
formance of the top n normalization candidates by
integrating human perspective modeling.
Yang and Eisenstein (2013) introduced an unsu-
pervised log linear model for text normalization.
Their joint statistical approach uses local context
based on language modeling and surface similar-
ity. Along with dictionary based models, Yang and
Eisenstein’s model have obtained a significant im-
provement on the performance of text normaliza-
tion systems.
Another relevant study is conducted by Hassan
and Menezes (2013), who generated a normaliza-
tion lexicon using Markov random walks on a con-
textual similarity lattice that they created using 5-
gram sequences of words. The best normaliza-
tion candidates are chosen using the average hit-
ting time and lexical similarity features. Context
of a word in the center of a 5-gram sequence is de-
fined by the other words in the 5-gram. Even if one
word is not the same, the context is considered to
be different. This is a relatively conservative way
for modeling the prior contexts of words. In our
model, we filtered candidate words based on their
grammatical properties and let each neighbouring
token to contribute to the prior context of a word,
which leads to both a higher recall and a higher
precision.
</bodyText>
<sectionHeader confidence="0.999165" genericHeader="method">
3 Methodology
</sectionHeader>
<bodyText confidence="0.999924352941176">
In this paper, we propose a graph-based approach
that models both contextual and lexical similar-
ity features among an ill-formed OOV word and
candidate IV words. An input text is first prepro-
cessed by tokenizing and Part-Of-Speech (POS)
tagging. If the text contains an OOV word, the
normalization candidates are chosen by making
use of the contextual features, which are extracted
from a pre-generated directed word association
graph, as well as lexical similarity features. Lexi-
cal similarity features are based on edit distance,
longest common subsequence ratio, and double
metaphone distance. In addition, a slang dictio-
nary1 is used as an external resource to enrich
the normalization candidate set. The details of
the approach are explained in the following sub-
sections.
</bodyText>
<subsectionHeader confidence="0.999175">
3.1 Preprocessing
</subsectionHeader>
<bodyText confidence="0.998539214285714">
After tokenization, the next step in the pipeline
is POS tagging each token using a POS tagger
specifically designed for social media text. Unlike
the regular POS taggers designed for well-written
newswire-like text, social media POS taggers pro-
vide a broader set of tags specific to the peculiari-
ties of social text (Owoputi et al., 2013; Gimpel et
al., 2011). Using this extended set of tags we can
identify tokens such as discourse markers (e.g. rt
for retweets, cont. for a tweet whose content fol-
lows up in the coming tweet) or URLs. This en-
ables us to model better the context of the words in
social media text. A sample preprocessed sentence
is shown in Table 3.
</bodyText>
<footnote confidence="0.996511">
1http://www.noslang.com
</footnote>
<page confidence="0.998814">
315
</page>
<bodyText confidence="0.99961475">
As shown in Table 2, after preprocessing, each
token is assigned a POS tag with a confidence
score between 0 and 12. Later, we use these confi-
dence scores in calculating the edge weights in our
context graph. Note that even though the words w
and beatiful are misspelled, they are tagged cor-
rectly by the tagger, with lower confidence scores
though.
</bodyText>
<tableCaption confidence="0.97797">
Table 2: Sample POS tagger output
</tableCaption>
<subsectionHeader confidence="0.998964">
3.2 Graph construction
</subsectionHeader>
<bodyText confidence="0.9999364">
Contextual information of words is modeled
through a word association graph created by us-
ing a large corpus of social media text. The graph
encodes the relative positions of the POS tagged
words in the text with respect to each other. Af-
ter preprocessing, each text message in the corpus
is traversed in order to extract the nodes and the
edges of the graph. A node is defined with four
properties: id, oov, freq and tag. The token itself is
the id field. The freq property indicates the node’s
frequency count in the dataset. The oov field is set
to True if the token is an OOV word. Following the
prior work by Han and Baldwin, (2011) we used
the GNU Aspell dictionary (v0.60.6) to determine
whether a word is OOV or not. We also edited the
output of Aspell dictionary to accept letters other
than “a” and “i” as OOV words. A portion of the
graph that covers parts of the sample sentence in
Table 3 is shown in Figure 1.
In the created word association graph, each
node is a unique set of a token and its POS tag.
This helps us to identify the candidate IV words
for a given OOV word by considering not only
lexical and contextual similarity, but also gram-
matical similarity in terms of POS tags. For ex-
ample, if the token smile has been frequently seen
as a Noun or a Verb, and not in other forms in the
dataset (e.g. Table 4), this provides evidence that it
is not a good normalization candidate for an OOV
token that has been tagged as a Pronoun. On the
</bodyText>
<footnote confidence="0.639415">
2CMU Ark Tagger (v0.3.2)
</footnote>
<figureCaption confidence="0.981620333333333">
Figure 1: Portion of the word association graph
for part of the sample sentence in Table 3. (d: dis-
tance, w: edge weight).
</figureCaption>
<bodyText confidence="0.961179466666667">
other hand, smile can be a good candidate for a
Noun or a Verb OOV token, if it is lexically and
contextually similar to it.
node id freq oov tag
smile 3 False A
smile 3403 False N
smile 2796 False V
Table 4: The different nodes in the word associ-
ation graph representing the token smile tagged
with different POS tags.
An edge is created between two nodes in the
graph, if the corresponding word pair (i.e. to-
ken/POS pair) are contextually associated. Two
words are considered to be contextually associated
if they satisfy the following criteria:
</bodyText>
<listItem confidence="0.9825778">
• The two words co-occur within a maximum
word distance of tdistance in a text message
in the corpus.
• Each word has a minimum frequency of
tfrequency in the corpus.
</listItem>
<bodyText confidence="0.996568166666667">
The directionality of the edges is based on the
sequence of words in the text messages in the cor-
pus. In other words, an edge between two nodes
is directed from the earlier seen token towards
the later seen token in a message. For example,
Figure 2 shows the edges that would be derived
</bodyText>
<figure confidence="0.998991526315789">
Token
with
a
beautiful
smile
POS tag
Preposition
Determiner
Adjective
Noun
Tag confidence
0.9963
0.9980
0.9971
0.9712
w Preposition 0.7486
a Determiner 0.9920
beatiful Adjective 0.9733
smile Noun 0.9806
</figure>
<page confidence="0.516117">
316
</page>
<figure confidence="0.994787777777778">
Let’sL
starter
thisD
morningN
wP
aD
beatifulA
smileN
.C
</figure>
<tableCaption confidence="0.884625">
Table 3: Sample tokenized, POS tagged sentence (L: nominal+verbal, V: verb, D: determiner, N: noun,
P: Preposition, A: adjective, C: punctuation).
</tableCaption>
<bodyText confidence="0.999323176470588">
from a text including the phrase “with a beautiful
smile”. The direction (from,to) and the distance
together represent a unique triplet. For each pair
of nodes with a specific distance there is an edge
with a positive weight, if the two nodes are con-
textually associated. Each co-occurrence of two
contextually associated nodes increases the weight
of the edge between them with an average of the
nodes’ POS tag confidence scores in the text mes-
sage considered. If we are to expand the graph
with the example phrase “with a beautiful smile”,
the weight of the edge with distance 2 from the
node with|P to the node smile|N would increase by
(0.9963 + 0.9712)/2, since the confidence score
of the POS tag for the token with is 0.9963 and the
confidence score of the POS tag of the token smile
is 0.9712 as shown in Table 2.
</bodyText>
<figureCaption confidence="0.9948525">
Figure 2: Sample nodes and edges from the word
association graph.
</figureCaption>
<subsectionHeader confidence="0.998635">
3.3 Graph-based Contextual Similarity
</subsectionHeader>
<bodyText confidence="0.999978266666667">
Our graph-based contextual similarity method is
based on the assumption that an IV word that is
the canonical form of an OOV word appears in the
same context with the corresponding OOV word.
In other words, the two nodes in the graph share
several neighbors that co-occur within the same
distances to the corresponding two words in social
media text. We also assume that an OOV word and
its canonical form should have the same POS tag.
Given an input text for normalization, the next
step after preprocessing is finding the normaliza-
tion candidates for each OOV token in the input
text. For each ill-formed OOV token oi in the in-
put text, first the list of tokens that co-occur with
oi in the input text and their positional distances to
oi are extracted. This list is called the neighbor list
of token oi, i.e., NL(oi).
For each neighbor node nj in NL(oi), the word
association graph is traversed, and the edges from
or to the node nj are extracted. The resulting edge
list EL(oi) has edges in the form of (nj, ck) or (ck,
nj), where ck is a candidate canonical form of the
OOV word oi. Here the neighbor node nj can be
an OOV node, but the candidate node ck is chosen
among the IV nodes. The edges in EL(oi) are fil-
tered by the relative distance of nj to oi as given in
the NL(oi). Any edge between nj and ck, whose
distance is not the same as the distance between
nj and oi is removed.
In addition to distance based filtering, POS tag
based filtering is also performed on the edges in
EL(oi). Each candidate node should have the
same POS tag with the corresponding OOV token.
For the OOV token oi that has the POS tag Ti, all
the edges that include candidates with a tag other
than Ti are removed from the edge list EL(oi).
Figure 3 represents a portion from the graph
where the neighbors and candidates of the OOV
node “beatiful” are shown. In the sample sentence
in Table 3 there are two OOV tokens to be normal-
ized, o1 = w and o2 = beatiful. The neighbor
list of o2, NL(o2) includes n1 = w, n2 = a and
n3 = smile. For each neighbor in NL(o2), the can-
didate nodes (c1 = broken, c2 = nice, c3 = new,
c4 = beautiful, c5 = big, c6 = best, c7 = great)
are extracted. As shown in Figure 3, there are 11
lines representing the edges between the neighbors
of the OOV token and the candidate nodes. These
are representative edges in EL(o2). Each member
of the edge list has the same tag (A for Adjective)
as the OOV node “beatiful” and the same distance
to the corresponding neighbor node of the OOV
node.
Each edge in EL(oi) consists of a neighbor
node nj, a candidate node ck and an edge weight
edgeWeight(nj, ck). The edge weight represents
the likelihood or the strength of association be-
tween the neighbor node nj and the candidate
node ck. As described in the previous section the
edge weights are computed based on the frequency
</bodyText>
<figure confidence="0.998605473684211">
with
P
distance: 0
weight: 25011
distance:1
weight: 198
a
D
distance: 0
weight: 2918 distance:0
weight: 305
distance:2
weight: 89
beautiful
A
distance:1
weight: 322
smile
N
</figure>
<page confidence="0.737733">
317
</page>
<figureCaption confidence="0.997978">
Figure 3: A portion of the graph that includes
</figureCaption>
<bodyText confidence="0.978643717948718">
the OOV token “beatiful”, its neighbors and the
candidate nodes that each neighbor is connected
to. Thick lines show the edge list with relative
weights.
of co-occurrence of two tokens, as well as the con-
fidence scores of their POS tags.
The edge weights of the edges in EL(o2) are
shown in Figure 3. The edges that are connected to
the OOV neighbor “w” have smaller edge weights
such as 3, 5, and 26. On the other hand, the edges
that are connected to common words have higher
weights. For example, the weight of the edge be-
tween the nodes “a” and “new” is 24388. This
indicates that they are more common words, and
frequently co-occur in the same form (“a new”).
Although this edge weight metric is reasonable
for identifying the most likely canonical form for
the OOV word oi, it has the drawback of favoring
words with high frequencies like common words
or stop words. Therefore, to avoid overrated words
and get contextually related candidates, we nor-
malize the edge weight edgeWeight(nj, ck) with
the frequency of the candidate node ck as shown
in Equation 1.
Equation 1 provides a metric that captures con-
textual similarity based on binary associations. In
order to achieve a more comprehensive contex-
tual coverage, a contextual similarity feature is
built based on the sum of the binary association
scores of several neighbors. As shown in Equa-
tion 2, for a candidate node ck the total edge
weight score is the sum of the normalized edge
weight scores EWNorm(nj, ck), which are the
edge weights coming from the different neighbors
of the OOV token oi. We expect this contextual
similarity feature to favor and identify the candi-
dates which are (i) related to many neighbors, and
(ii) have a high association score with each neigh-
bor.
</bodyText>
<equation confidence="0.997898">
EW Norm(nj, ck) = edgeWeight(nj, ck)/freq(ck)
�EW Score(oi, ck) = EWNorm(nj, ck)
EL(oi)
</equation>
<bodyText confidence="0.999747260869565">
Our word association graph includes both OOV
and IV tokens, and our OOV detection depends
on the spellchecker which fails to identify some
OOV tokens that have the same spelling with an IV
word. In order to propose better canonical forms,
the frequencies of the normalization candidates in
the social media corpus have also been incorpo-
rated to the contextual similarity feature. Nodes
with higher frequencies lead to tokens that are in
their most likely grammatical forms.
The final contextual similarity of the token oi
and the candidate ck is the weighted sum of the
total edge weight score and the frequency score
of the candidate (see Equation 3). The frequency
score of the candidate is a real number between 0
and 1. It is proportional to the frequency of the
candidate with respect to the frequencies of the
other candidates in the corpus. Since the total edge
weight score is our primary contextual resource,
we may want to favor edge weight scores. We give
the frequency score a weight 0 &lt; Q &lt; 1 to be able
to limit its effect on the total contextual similarity
score.
</bodyText>
<equation confidence="0.994661">
contSimScore(oi, ck) = EW Score(oi, ck) (3)
+ β ∗ freqScore(ck)
</equation>
<bodyText confidence="0.99943325">
Hereby, we have the candidate list CL(oi) for the
OOV token oi that includes all the unique can-
didates in EL(oi) and their contextual similarity
scores calculated.
</bodyText>
<subsectionHeader confidence="0.986377">
3.4 Lexical Similarity
</subsectionHeader>
<bodyText confidence="0.9996742">
Following the prior work in (Han and Baldwin,
2011; Hassan and Menezes, 2013), our lexical
similarity features are based on edit distance (Lev-
enshtein, 1966), double metaphone (phonetic edit
distance) (Philips, 2000), and a similarity function
</bodyText>
<figure confidence="0.998733">
broken
A
c1
nice
A
Distance: 1
c2
3
n1
w
P
26
new big
A A
c3 c5
24388
2
Distance: 0
beautiful
A
2918
n2
c4
a
D smile
N
305
750
beatiful
A
125
o2
Distance: 0
53
best
A
c6
20
great
A
n3
c7
</figure>
<page confidence="0.996663">
318
</page>
<bodyText confidence="0.999476333333333">
(simCost) (Contractor et al., 2010) which is de-
fined as the ratio of the Longest Common Sub-
sequence Ratio (LCSR) (Melamed, 1999) of two
words and the Edit Distance (ED) between their
skeletons (Equations 4 and 5), where the skeleton
of a word is obtained by removing its vowels.
</bodyText>
<equation confidence="0.9943065">
LCSR(oj, ck) = LCS(oj, ck)/maxLength(oj, ck) (4)
simCost(oj, ck) = LCSR(oj, ck)/ED(oj, ck) (5)
</equation>
<bodyText confidence="0.999647615384616">
Following the tradition that is inspired
from (Kaufmann and Kalita, 2010), before
lexical similarity calculations, any repetitions of
characters three or more times in OOV tokens are
reduced to two (e.g. goooood is reduced to good).
Then, the edit distance, phonetic edit distance, and
simCost between each candidate in CL(oi) and
the OOV token oi are calculated. Edit distance
and phonetic edit distance are used to filter the
candidates. Any candidate in CL(oi) with an
edit distance greater than tedit and phonetic edit
distance greater than tphonetic to oi is removed
from the candidate list CL(oi).
</bodyText>
<equation confidence="0.9919535">
lexSimScore(oi, ck) = simCost(oi, ck) (6)
+ A ∗ editScore(oi, ck)
</equation>
<bodyText confidence="0.999962181818182">
For the remaining candidates, the total lexical
similarity score (Equation 6) is calculated using
simCost and edit distance score3. Similar to con-
textual similarity score, here we have one main
lexical similarity feature and one minor lexical
similarity feature. The major lexical similarity
feature is simCost, whereas the edit distance score
is the minor feature. We assigned a weight 0 &lt;
A &lt; 1 to the edit distance score to be able to lower
its contribution while calculating the total lexical
similarity score.
</bodyText>
<subsectionHeader confidence="0.934173">
3.5 External Score
</subsectionHeader>
<bodyText confidence="0.999760888888889">
Since some social media text messages are ex-
tremely short and contain several OOV words,
they do not provide sufficient context, i.e., IV
neighbors, to enable the extraction of good candi-
dates from the word association graph. Therefore,
we extended the candidate list obtained through
contextual similarity as described in the previous
section, by including all the tokens in the word as-
sociation graph that satisfy the edit distance and
</bodyText>
<footnote confidence="0.963757333333333">
3an approximate string comparison measure
(between 0.0 and 1.0) using the edit distance
https://sourceforge.net/projects/febrl/
</footnote>
<bodyText confidence="0.99960268">
phonetic edit distance criteria. We also incorpo-
rated candidates from external resources, in other
words from a slang dictionary and a transliteration
table of numbers and pronouns. If a candidate oc-
curs in the slang dictionary or in the transliteration
table as a correspondence to its OOV word, it is
assigned an external score of 1, otherwise it is as-
signed an external score of 0.
The transliterations were first used by (Gouws
et al., 2011). Besides the token and its transliter-
ation we also use its POS tag information, which
was not available in their system.
The external score favors the well known inter-
pretations of common OOV words. However, un-
like the dictionary based methodologies, our sys-
tem does not return the corresponding unabbrevi-
ated word in the slang dictionary or in the translit-
eration table directly. Only an external score gets
assigned and the candidate still needs to com-
pete with other candidates which may have higher
contextual similarities and one of those contextu-
ally more similar candidates may be returned as
the correct normalization instead of the candidate
found equivalent to the OOV word in the slang dic-
tionary (or in the transliteration table).
</bodyText>
<subsectionHeader confidence="0.995402">
3.6 Overall Scoring
</subsectionHeader>
<bodyText confidence="0.9999565">
As shown in Equation 7, the final score of a can-
didate IV token ck for an OOV token oi is the sum
of its lexical similarity score, contextual similarity
score and external score with respect to oi.
</bodyText>
<equation confidence="0.983958333333333">
candScore(oi, ck) = lexSimScore(oi, ck)
+ contSimScore(oi, ck) (7)
+ externalScore(oi, ck)
</equation>
<sectionHeader confidence="0.999565" genericHeader="evaluation">
4 Experiments
</sectionHeader>
<subsectionHeader confidence="0.888442">
4.1 Datasets
</subsectionHeader>
<bodyText confidence="0.994591166666667">
We used the LexNorm1.1 (LN) dataset (Han and
Baldwin, 2011) and Pennell and Liu (2014)’s tri-
gram dataset to evaluate our proposed approach.
LexNorm1.1 contains 549 tweets with 1184 manu-
ally annotated ill-formed OOV tokens. It has been
used by recent text normalization studies for eval-
uation, which enables us to directly compare our
performance results with results obtained by the
recent previous work (Han and Baldwin, 2011;
Pennell and Liu, 2011; Han et al., 2012; Liu et
al., 2012; Hassan and Menezes, 2013; Yang and
Eisenstein, 2013; Chrupala, 2014). The trigram
</bodyText>
<page confidence="0.998601">
319
</page>
<bodyText confidence="0.999955666666667">
dataset is an SMS-like corpus collected from twit-
ter status updates sent via SMS. The dataset does
not include the complete tweet text but trigrams
from tweets and one OOV word in each trigram
is annotated. In total 4661 twitter status messages
and 7769 tokens are annotated.
</bodyText>
<subsectionHeader confidence="0.998211">
4.2 Graph Generation
</subsectionHeader>
<bodyText confidence="0.99993275">
We used a large corpus of social media text to con-
struct our word association graph. We extracted
1.5 GB of English tweets from Stanford’s 476 mil-
lion Twitter Dataset (Yang and Leskovec, 2011).
The language identification of tweets was per-
formed by using the langid.py Python library (Lui
and Baldwin, 2012; Baldwin and Lui, 2010).
CMU Ark Tagger (v0.3.2), which is a social me-
dia specific POS tagger achieving an accuracy of
95% over social media text (Owoputi et al., 2013;
Gimpel et al., 2011), is used for tokenizing and
POS tagging the tweets. We used the twitter tagset
which includes some extra POS tags specific to so-
cial media including URLs and emoticons, Twit-
ter hashtags (#), and twitter at-mentions (@). We
made use of these social media specific tags to dis-
ambiguate some OOV tokens.
After tokenization, we removed the tokens that
were POS tagged as mention (e.g. @brendon),
discourse marker (e.g. RT), URL, email address,
emoticon, numeral, and punctuation. The remain-
ing tokens are used to build the word association
graph. After constructing the graph we only kept
the nodes with a frequency greater than 8. For
the performance related reasons, the relatedness
thresholds tdistance and tfrequency were chosen as
3 and 8, respectively. The resulting graph contains
105428 nodes and 46609603 edges.
</bodyText>
<subsectionHeader confidence="0.999726">
4.3 Candidate Set Generation
</subsectionHeader>
<bodyText confidence="0.999986">
While extending the candidate set with lexical fea-
tures we use tedit ≤ 2 ∨ tphonetic ≤ 1 to keep
up with the settings in (Han and Baldwin, 2011).
In other words, IV words that are within 2 char-
acter edit distance or 1 character edit distance of
a given OOV word under phonemic transcription
were chosen as lexical similarity candidates. The
values for the A and Q parameters in Equations 3
and 6 are set to 0.5. We did not tune these pa-
rameters for optimized performance. We selected
the value of 0.5 in order to give less weight (half
weight) to our minor contextual and lexical simi-
larity features compared to the major ones.
</bodyText>
<subsectionHeader confidence="0.997117">
4.4 Normalization Candidates
</subsectionHeader>
<bodyText confidence="0.999956111111111">
Most of the prior work assume perfect detection
of ill-formed words during test set decoding (Liu
et al., 2012; Han and Baldwin, 2011; Pennell and
Liu, 2011; Yang and Eisenstein, 2013). To be
able to compare our results with studies that do
not assume that ill-formed words have been pre-
identified (Chrupala, 2014; Hassan and Menezes,
2013; Han et al., 2012) we used our graph and
built a dictionary to identify the ill-formed words.
Following Han and Baldwin (2011) and Yang
and Eisenstein (2013), we created a dictionary by
choosing the nodes in our graph that have a fre-
quency property higher than 20. Filtering this dic-
tionary of 49657 words using GNU Aspell dictio-
nary (v0.60.6) we produced a set of 26773 “in-
vocabulary” (IV) words. In our second setup our
system does not attemp to normalize the words in
this set.
</bodyText>
<subsectionHeader confidence="0.852203">
4.5 Results and Analysis
</subsectionHeader>
<bodyText confidence="0.999844733333333">
In this paper we introduced a new contextual ap-
proach for text normalization. The lexical similar-
ity score described in Section 3.4 and the external
score described in Section 3.5 depend on the work
of Han and Baldwin (2011). With small changes
made to the previously proposed method we took
it as a baseline in our study.
As contextual layer we proposed two metrics
extracted from the word association graph. The
first one depends on the total edge weights be-
tween candidates and OOV neighbours, the sec-
ond one is based on the frequencies of the candi-
dates in the corpus.
As the evaluation metrics we used precision,
recall, and F-Measure. Precision calculates the
proportion of correctly normalized words among
the words for which we produced a normaliza-
tion. Recall shows the amount of correct nor-
malizations over the words that require normal-
ization (ill-formed OOV words). The main metric
that we consider while evaluating the performance
of our system is F-Measure which is the harmonic
mean of precision and recall.
We investigated the impact of lexSimScore and
externalScore seperately on both datasets (Ta-
ble 5). Using only lexSimScore the sys-
tem achieved an F-measure of 28.24% on the
LexNorm1.1 dataset and 38.70% on the Trigram
dataset, which shows that lexical similarity alone
is not enough for a good normalization system.
</bodyText>
<page confidence="0.993399">
320
</page>
<bodyText confidence="0.99926705882353">
However, the externalScore which is the layer that
is more aware of the Internet jargon, along with
some social text specific rule based transliterations
performs better than expected on both datasets.
Mixing these two layers we reach our baseline that
is adopted from (Han and Baldwin, 2011). This
baseline setup obtained an F-measure of 77.12%
on LexNorm1.1, which is slightly better than the
result (75.30%) reported by the original system
of Han and Baldwin (2011).
The results obtained by our proposed Contex-
tual Word Association Graph (CWA-Graph) sys-
tem on the LexNorm1.1 and trigram datasets, as
well as the results of recent studies that used the
same datasets for evaluation are presented in Ta-
ble 5. The ill-formed words are assumed to have
been pre-identified in advance.
</bodyText>
<table confidence="0.999766153846154">
Method Dataset Precision Recall F-measure
lexSimScore LN 28.28 28.20 28.24
externalScore LN 64.69 64.52 64.60
lexSimScore+externalScore LN 77.22 77.02 77.12
Han and Baldwin (2011) LN 75.30 75.30 75.30
Liu et al. (2012) LN 84.13 78.38 81.15
Yang and Eisenstein (2013) LN 82.09 82.09 82.09
CWA-Graph LN 85.50 79.22 82.24
lexSimScore Trigram 39.10 38.40 38.70
externalScore Trigram 44.20 43.30 43.80
lexSimScore+externalScore Trigram 65.50 64.20 64.80
Pennell and Liu (2011) Trigram 69.7 69.7 69.7
CWA-Graph Trigram 77.2 68.8 72.8
</table>
<tableCaption confidence="0.991527">
Table 5: Results obtained when ill-formed words
</tableCaption>
<bodyText confidence="0.984033">
are assumed to have been pre-identified in ad-
vance.
Our CWA-Graph approach achieves the best F-
measure (82.24%) and precision (85.50%) among
the recent previous studies. The high precision
value is obtained without compromising much
from recall (79.22%). Our recall is the second best
among others. The F-score (82.09%) obtained
by Yang and Eisenstein (2013)’s system is close
to ours and the second best F-score, which on the
other hand, has a lower precision.
Without any modification to our system or to
the parameters, we were able to improve the re-
sults obtained by Pennell and Liu (2011) on the
trigram SMS-like dataset. The trigram nature of
the dataset resulted in input texts which are (short
thus) very limited with regard to contextual infor-
mation. Nevertheless, our system achieved 72.8%
F-Measure using this contextual information even
though it is limited.
Along the systems (presented in Table 5) that
assume ill-formed tokens have been pre-identified
perfectly by an oracle, there are also systems that
are not based on this assumption but contain ill-
formed word identification components (Han et
al., 2012; Hassan and Menezes, 2013; Chrupala,
2014). We used the method described in Section
4.4 to identify the candidate tokens for normaliza-
tion. Table 6 shows our results compared with the
results of other systems that perform ill-formed
word detection prior to normalization. We could
label 1141 tokens correctly as ill-formed among
1184 ill-formed tokens. We achieved a word error
rate (WER) of 2.6%, where Chrupala (2014) re-
ported 4.8% and Han et al. (2012) reported 6.6%
WER on the Lexnorm1.1 dataset.
</bodyText>
<table confidence="0.9983325">
Method Dataset Precision Recall F-measure
Han et al. (2012) LN 70.00 17.90 28.50
Hassan and Menezes (2013) LN 85.37 56.40 69.93
CWA-Graph LN 85.87 76.52 80.92
</table>
<tableCaption confidence="0.9445645">
Table 6: Results obtained without assuming that
ill-formed words have been pre-identified.
</tableCaption>
<bodyText confidence="0.999810533333333">
As shown in Table 5 some systems have equal
precision and recall values (Yang and Eisenstein,
2013; Han and Baldwin, 2011; Pennell and Liu,
2011). Those systems normalize all ill-formed
words. On the other hand, our system does not
return a normalization, if there are no candidates
that are lexically similar, grammatically correct,
and contextually close enough. For this reason,
we managed to achieve a higher precision com-
pared to the other systems. Our system returns a
normalization candidate for an OOV word only if
it achieves a similarity score (contextual, lexical,
external, or some degree of each feature) above a
threshold value. The default threshold used in the
system is set equal to the maximum score that can
be obtained by lexical features. Thus, we only re-
trieve candidates that obtain a non-zero contextual
similarity score (conSimScore). The results shown
at Table 7 and Table 8 demonstrate that CWA-
Graph can obtain even higher values of precision
by increasing the percentage of contextual context
of candidates. It achieved 94.1% precision on the
LexNorm1.1 dataset, where the highest precision
reported at the same recall level is 85.37% (Hassan
and Menezes, 2013). The precision of the normal-
ization system can be set (e.g. as high, medium,
low) depending on the application where it will be
used.
Our motivation behind introducing the A and
Q parameters was to investigate the importance
</bodyText>
<page confidence="0.993885">
321
</page>
<table confidence="0.999741">
conSimScore &gt; Precision Recall F-measure
0 85.5 79.2 82.2
0.1 88.8 75.1 81.4
0.2 91.1 72.8 80.9
0.3 92.3 67.6 78.0
0.5 94.1 56.4 70.5
</table>
<tableCaption confidence="0.98776375">
Table 7: Comparison of results for different
threshold values on LexNorm1.1, the setup we
have used for our other experiments is shown in
bold.
</tableCaption>
<table confidence="0.988530714285714">
conSimScore &gt; Precision Recall F-measure
0 77.2 68.8 72.8
0.1 80.9 65.8 72.6
0.2 84.2 60.8 70.6
0.3 87.6 54.6 67.3
0.4 89.5 47.1 61.7
0.5 90.8 42.1 57.6
</table>
<tableCaption confidence="0.859131">
Table 8: Comparison of results for different
threshold values on trigram dataset, the setup we
</tableCaption>
<bodyText confidence="0.9545516875">
have used for our other experiments is shown in
bold.
of the minor features compared to our major fea-
tures (described in Sections 3.3 and 3.4). For the
experiments reported in Tables 5, 6, 7 and 8 we set
the A and Q values to 0.5. We did not tune these pa-
rameters for optimized performance. Rather, our
aim was to give less weight (half weight) to the
minor features compared to the major ones. To
analyze the effects of the lambda and beta param-
eters, we plotted the performance of the system on
the LexNorm1.1 data set by varying their values
(see Figure 4). It is shown that for A and Q values
greater than 0.3 the performance of the system is
quite robust. The F-score varies between 80.4%
and 82.9%.
</bodyText>
<figureCaption confidence="0.9494625">
Figure 4: The effect of A and Q on the system per-
formance.
</figureCaption>
<sectionHeader confidence="0.997366" genericHeader="conclusions">
5 Conclusion
</sectionHeader>
<bodyText confidence="0.999928422222222">
In this paper, we present an unsupervised graph-
based approach for contextual text normalization.
The task of normalization is highly dependent on
understanding and capturing the dynamics of the
informal nature of social text. Our word associ-
ation graph is built using a large unlabeled social
media corpus. It helps to derive contextual analy-
sis on both clean and noisy data.
It is important to emphasize the difference be-
tween corpus based contextual information and
contextual information of the input text (input con-
text). Most recent unsupervised systems for text
normalization only make use of corpus based con-
text information. However, this approach is led
by statistical information. In other words, it finds
which IV word the OOV word is commonly nor-
malized to, regardless of the context of the OOV
word in the input text message. A major strength
of our approach is that it utilizes both corpus based
contextual information and input based contextual
information. We use corpus based statistical infor-
mation to connect/associate the words in the con-
textual word association graph. On the other hand,
the neighbors of an OOV word in the input text
provide us input based context information. Using
input context to find normalizations helps us iden-
tify the correct normalization, even if it is not the
statistically dominant one.
We compared our approach with the recent
social media text normalization systems and
achieved state-of-the-art precision and F-measure
scores. We reported our results on two datasets.
The first one is the standard text normalization
dataset (Lexnorm1.1) derived from Twitter. Our
results on this dataset showed that our system can
serve as a high precision text normalization sys-
tem which is highly preferable as an NLP pre-
processing step. The second dataset we tested
our approach is a SMS-like trigram dataset. The
tests showed that the proposed system can perform
good on SMS data as well.
The system does not require a clean corpus or
an annotated corpus. The contextual word asso-
ciation graph can be built by using the publicly
available social media text.
</bodyText>
<sectionHeader confidence="0.999242" genericHeader="references">
References
</sectionHeader>
<reference confidence="0.9313195">
AiTi Aw, Min Zhang, Juan Xiao, and Jian Su. 2006. A
Phrase-based Statistical Model for SMS Text Nor-
</reference>
<page confidence="0.980892">
322
</page>
<reference confidence="0.999845345454546">
malization. Proceedings of the 21st International
Conference on Computational Linguistics and 44th
Annual Meeting of the Association for Computa-
tional Linguistics, pages 33–40.
Timothy Baldwin and Marco Lui. 2010. Language
Identification: The Long and the Short of the Matter.
Human Language Technologies: The 2010 Annual
Conference of the North American Chapter of the
Association for Computational Linguistics, pages
229–237.
Eric Brill and Robert C. Moore. 2000. An Improved
Error Model for Noisy Channel Spelling Correction.
Proceedings of the 38th Annual Meeting on Associa-
tion for Computational Linguistics, pages 286–293.
Monojit Choudhury, Rahul Saraf, Vijit Jain, Animesh
Mukherjee, Sudeshna Sarkar, and Anupam Basu.
2007. Investigation and Modeling of the Structure
of Texting Language. International Journal on Doc-
ument Analysis and Recognition, 10(3):157–174.
Grzegorz Chrupala. 2014. Normalizing tweets with
edit scripts and recurrent neural embeddings. Pro-
ceedings of the 52st Annual Meeting of the Associa-
tion for Computational Linguistics, pages 680–686.
Danish Contractor, Tanveer A. Faruquie, and
L. Venkata Subramaniam. 2010. Unsuper-
vised Cleansing of Noisy Text. Proceedings of the
23rd International Conference on Computational
Linguistics: Posters, pages 189–196.
Paul Cook and Suzanne Stevenson. 2009. An Un-
supervised Model for Text Message Normalization.
Proceedings of the Workshop on Computational Ap-
proaches to Linguistic Creativity, pages 71–78.
Jacob Eisenstein. 2013. What to Do About Bad Lan-
guage on the Internet. Proceedings of the North
American Chapter of the Association for Computa-
tional Linguistics : Human Language Technologies,
pages 359–369.
Kevin Gimpel, Nathan Schneider, Brendan O’Connor,
Dipanjan Das, Daniel Mills, Jacob Eisenstein,
Michael Heilman, Dani Yogatama, Jeffrey Flanigan,
and Noah A. Smith. 2011. Part-of-speech Tagging
for Twitter: Annotation, Features, and Experiments.
Proceedings of the 49th Annual Meeting of the Asso-
ciation for Computational Linguistics: Human Lan-
guage Technologies: Short Papers - Volume 2, pages
42–47.
Stephan Gouws, Donald Metzler, Congxing Cai, and
Eduard Hovy. 2011. Contextual Bearing on Lin-
guistic Variation in Social Media. Proceedings of
the Workshop on Languages in Social Media, pages
20–29.
Bo Han and Timothy Baldwin. 2011. Lexical Normal-
isation of Short Text Messages: Makn Sens a #Twit-
ter. Proceedings of the 49th Annual Meeting of the
Association for Computational Linguistics: Human
Language Technologies - Volume 1, pages 368–378.
Bo Han, Paul Cook, and Timothy Baldwin. 2012. Au-
tomatically constructing a normalisation dictionary
for microblogs. Proceedings of the 2012 Joint Con-
ference on Empirical Methods in Natural Language
Processing and Computational Natural Language
Learning, pages 421–432.
Hany Hassan and Arul Menezes. 2013. Social
Text Normalization Using Contextual Graph Ran-
dom Walks. Proceedings of the 51st Annual Meet-
ing of the Association for Computational Linguis-
tics, pages 1577–1586.
Max Kaufmann and Jugal Kalita. 2010. Syntactic Nor-
malization of Twitter Messages. Proceedings of the
8th International Conference on Natural Language
Processing, pages 149–158.
Vladimir Iosifovich Levenshtein. 1966. Binary Codes
Capable of Correcting Deletions, Insertions and Re-
versals. Soviet Physics Doklady, 10:707.
Fei Liu, Fuliang Weng, and Xiao Jiang. 2012. A
Broad-Coverage Normalization System for Social
Media Language. Proceedings of the 50th Annual
Meeting of the Association for Computational Lin-
guistics: Long Papers-Volume 1, pages 1035–1044.
Marco Lui and Timothy Baldwin. 2012. Langid.Py:
An Off-the-shelf Language Identification Tool. Pro-
ceedings of the 50th Annual Meeting of the Associa-
tion for Computational Linguistics: System Demon-
strations, pages 25–30.
I. Dan Melamed. 1999. Bitext Maps and Alignment
via Pattern Recognition. Computational Linguistics,
25(1):107–130.
Olutobi Owoputi, Brendan O’Connor, Chris Dyer,
Kevin Gimpel, Nathan Schneider, and Noah A.
Smith. 2013. Improved Part-of-Speech Tagging
for Online Conversational Text with Word Clusters.
Proceedings of the North American Chapter of the
Association for Computational Linguistics : Human
Language Technologies, pages 380–390.
Deana Pennell and Yang Liu. 2011. A Character-
Level Machine Translation Approach for Normal-
ization of SMS Abbreviations. Fifth International
Joint Conference on Natural Language Processing,
pages 974–982.
Deana Pennell and Yang Liu. 2014. Normalization
of informal text. Computer Speech &amp; Language,
28(1):256 – 277.
Lawrence Philips. 2000. The Double Meta-
phone Search Algorithm. C/C++ Users Journal,
18(6):38–43, June.
Alan Ritter, Colin Cherry, and Bill Dolan. 2010. Unsu-
pervised modeling of twitter conversations. Human
Language Technologies: The 2010 Annual Confer-
ence of the North American Chapter of the Associa-
tion for Computational Linguistics, pages 172–180.
</reference>
<page confidence="0.990796">
323
</page>
<reference confidence="0.99928305882353">
Richard Sproat, Alan W. Black, Stanley Chen, Shankar
Kumar, Mari Ostendorf, and Christopher Richards.
2001. Normalization of Non-Standard Words.
Computer Speech &amp; Language, 15(3):287–333.
Kristina Toutanova and Robert C. Moore. 2002. Pro-
nunciation Modeling for Improved Spelling Correc-
tion. Proceedings of the 40th Annual Meeting on As-
sociation for Computational Linguistics, pages 144–
151.
Yi Yang and Jacob Eisenstein. 2013. A Log-Linear
Model for Unsupervised Text Normalization. Pro-
ceedings of the Empirical Methods on Natural Lan-
guage Processing, pages 61–72.
Jaewon Yang and Jure Leskovec. 2011. Patterns of
Temporal Variation in Online Media. Proceedings
of the Forth International Conference on Web Search
and Web Data Mining, pages 177–186.
</reference>
<page confidence="0.99902">
324
</page>
</variant>
</algorithm>
<algorithm name="ParsHed" version="110505">
<variant no="0" confidence="0.557752">
<title confidence="0.999374">A Graph-based Approach for Contextual Text Normalization</title>
<author confidence="0.957151">a˘gıl S¨onmez</author>
<author confidence="0.957151">Arzucan</author>
<affiliation confidence="0.804278">Department of Computer Bogazici</affiliation>
<address confidence="0.809698">Bebek, 34342 Istanbul,</address>
<abstract confidence="0.998982787878788">The informal nature of social media text renders it very difficult to be automatically processed by natural language processing tools. Text normalization, which corresponds to restoring the non-standard words to their canonical forms, provides a solution to this challenge. We introduce an unsupervised text normalization approach that utilizes not only lexical, but also contextual and grammatical features of social text. The contextual and grammatical features are extracted from a word association graph built by using a large unlabeled social media text corpus. The graph encodes the relative positions of the words with respect to each other, as well as their part-ofspeech tags. The lexical features are obtained by using the longest common subsequence ratio and edit distance measures to encode the surface similarity among words, and the double metaphone algorithm to represent the phonetic similarity. Unlike most of the recent approaches that are based on generating normalization dictionaries, the proposed approach performs normalization by considering the context of the non-standard words in the input text. Our results show that it achieves state-ofthe-art F-score performance on standard datasets. In addition, the system can be tuned to achieve very high precision without sacrificing much from recall.</abstract>
</variant>
</algorithm>
<algorithm name="ParsCit" version="110505">
<citationList>
<citation valid="true">
<authors>
<author>AiTi Aw</author>
<author>Min Zhang</author>
<author>Juan Xiao</author>
<author>Jian Su</author>
</authors>
<title>A Phrase-based Statistical Model for SMS Text Normalization.</title>
<date>2006</date>
<booktitle>Proceedings of the 21st International Conference on Computational Linguistics and 44th Annual Meeting of the Association for Computational Linguistics,</booktitle>
<pages>33--40</pages>
<contexts>
<context position="7618" citStr="Aw et al. (2006)" startWordPosition="1235" endWordPosition="1238">k and Stevenson (2009) extended this model by introducing an unsupervised noisy channel model. Rather than using one generic model for all word formations as in (Choudhury et al., 2007), they used a mixture model in which each different word formation type is modeled explicitly. The limitations of these methods were that they did not consider contextual features and assumed that tokens have unique normalizations. In the text normalization task several OOV tokens are ambiguous and without contextual information it is not possible to build models that can disambiguate transformations correctly. Aw et al. (2006) proposed a phrase-based statistical machine translation (MT) model for the text normalization task. They defined the problem as translating the SMS language to the English language and based their model on two submodels: a word based language model and a phrase based lexical mapping model (channel model). Their system also benefits from the input context and they argue that the strength of their model is in its ability to disambiguate mapping as in “2” → “two” or “to”, and “w” → “with” or “who”. Making use of the whole conversation, this is the closest approach to ours in the sense of utilizi</context>
</contexts>
<marker>Aw, Zhang, Xiao, Su, 2006</marker>
<rawString>AiTi Aw, Min Zhang, Juan Xiao, and Jian Su. 2006. A Phrase-based Statistical Model for SMS Text Normalization. Proceedings of the 21st International Conference on Computational Linguistics and 44th Annual Meeting of the Association for Computational Linguistics, pages 33–40.</rawString>
</citation>
<citation valid="true">
<authors>
<author>Timothy Baldwin</author>
<author>Marco Lui</author>
</authors>
<title>Language Identification: The Long and the Short of the Matter. Human Language Technologies: The</title>
<date>2010</date>
<booktitle>Annual Conference of the North American Chapter of the Association for Computational Linguistics,</booktitle>
<pages>229--237</pages>
<contexts>
<context position="28695" citStr="Baldwin and Lui, 2010" startWordPosition="4856" endWordPosition="4859">et is an SMS-like corpus collected from twitter status updates sent via SMS. The dataset does not include the complete tweet text but trigrams from tweets and one OOV word in each trigram is annotated. In total 4661 twitter status messages and 7769 tokens are annotated. 4.2 Graph Generation We used a large corpus of social media text to construct our word association graph. We extracted 1.5 GB of English tweets from Stanford’s 476 million Twitter Dataset (Yang and Leskovec, 2011). The language identification of tweets was performed by using the langid.py Python library (Lui and Baldwin, 2012; Baldwin and Lui, 2010). CMU Ark Tagger (v0.3.2), which is a social media specific POS tagger achieving an accuracy of 95% over social media text (Owoputi et al., 2013; Gimpel et al., 2011), is used for tokenizing and POS tagging the tweets. We used the twitter tagset which includes some extra POS tags specific to social media including URLs and emoticons, Twitter hashtags (#), and twitter at-mentions (@). We made use of these social media specific tags to disambiguate some OOV tokens. After tokenization, we removed the tokens that were POS tagged as mention (e.g. @brendon), discourse marker (e.g. RT), URL, email ad</context>
</contexts>
<marker>Baldwin, Lui, 2010</marker>
<rawString>Timothy Baldwin and Marco Lui. 2010. Language Identification: The Long and the Short of the Matter. Human Language Technologies: The 2010 Annual Conference of the North American Chapter of the Association for Computational Linguistics, pages 229–237.</rawString>
</citation>
<citation valid="true">
<authors>
<author>Eric Brill</author>
<author>Robert C Moore</author>
</authors>
<title>An Improved Error Model for Noisy Channel Spelling Correction.</title>
<date>2000</date>
<booktitle>Proceedings of the 38th Annual Meeting on Association for Computational Linguistics,</booktitle>
<pages>286--293</pages>
<contexts>
<context position="6224" citStr="Brill and Moore (2000)" startWordPosition="1012" endWordPosition="1015"> regardless of the context of the input text (Han et al., 2012; Hassan and Menezes, 2013). On the other hand, the proposed approach does not only make use of the general context information in a large corpus of social media text, but it also makes use of the context of the OOV word in the input text message. Thus, an OOV word can be normalized to different IV words depending on the context of the input text. 2 Related Work Early work on text normalization mostly made use of the noisy channel model. The first work that had a significant performance improvement over the previous research was by Brill and Moore (2000). They proposed a novel noisy channel model for spell checking based on string to string edits. Their model depended on probabilistic modeling of sub-string transformations. Toutanova and Moore (2002) improved this approach by extending the error model with phonetic similarities over words. Their approach is based on learning rules to predict the pronunciation of a single letter in the word depending on the neighbouring letters in the word. Choudhury et al. (2007) developed a supervised Hidden Markov Model based approach for normalizing Short Message Service (SMS) texts. They proposed a word f</context>
</contexts>
<marker>Brill, Moore, 2000</marker>
<rawString>Eric Brill and Robert C. Moore. 2000. An Improved Error Model for Noisy Channel Spelling Correction. Proceedings of the 38th Annual Meeting on Association for Computational Linguistics, pages 286–293.</rawString>
</citation>
<citation valid="true">
<authors>
<author>Monojit Choudhury</author>
<author>Rahul Saraf</author>
<author>Vijit Jain</author>
<author>Animesh Mukherjee</author>
<author>Sudeshna Sarkar</author>
<author>Anupam Basu</author>
</authors>
<date>2007</date>
<booktitle>Investigation and Modeling of the Structure of Texting Language. International Journal on Document Analysis and Recognition,</booktitle>
<volume>10</volume>
<issue>3</issue>
<contexts>
<context position="1694" citStr="Choudhury et al., 2007" startWordPosition="251" endWordPosition="254">d the double metaphone algorithm to represent the phonetic similarity. Unlike most of the recent approaches that are based on generating normalization dictionaries, the proposed approach performs normalization by considering the context of the non-standard words in the input text. Our results show that it achieves state-ofthe-art F-score performance on standard datasets. In addition, the system can be tuned to achieve very high precision without sacrificing much from recall. 1 Introduction Social text, which has been growing and evolving steadily, has its own lexical and grammatical features (Choudhury et al., 2007; Eisenstein, 2013). lol meaning laughing out loud, xoxo meaning kissing, 4u meaning for you are among the most commonly used examples of this jargon. In addition, these informal expressions in social text usually take many different lexical forms when generated by different individuals (Eisenstein, 2013). The limited accuracies of the Speech-to-Text (STT) tools in mobile devices, which are increasingly being used to post messages on social media platforms, along with the scarcity of attention of the users result in additional divergence of social text from more standard text such as from the </context>
<context position="6692" citStr="Choudhury et al. (2007)" startWordPosition="1087" endWordPosition="1090">made use of the noisy channel model. The first work that had a significant performance improvement over the previous research was by Brill and Moore (2000). They proposed a novel noisy channel model for spell checking based on string to string edits. Their model depended on probabilistic modeling of sub-string transformations. Toutanova and Moore (2002) improved this approach by extending the error model with phonetic similarities over words. Their approach is based on learning rules to predict the pronunciation of a single letter in the word depending on the neighbouring letters in the word. Choudhury et al. (2007) developed a supervised Hidden Markov Model based approach for normalizing Short Message Service (SMS) texts. They proposed a word for word decoding approach and used a dictionary based method to normalize commonly used abbreviations and nonstandard usage (e.g. “howz” to “how are” or “aint” to “are not”). Cook and Stevenson (2009) extended this model by introducing an unsupervised noisy channel model. Rather than using one generic model for all word formations as in (Choudhury et al., 2007), they used a mixture model in which each different word formation type is modeled explicitly. The limita</context>
</contexts>
<marker>Choudhury, Saraf, Jain, Mukherjee, Sarkar, Basu, 2007</marker>
<rawString>Monojit Choudhury, Rahul Saraf, Vijit Jain, Animesh Mukherjee, Sudeshna Sarkar, and Anupam Basu. 2007. Investigation and Modeling of the Structure of Texting Language. International Journal on Document Analysis and Recognition, 10(3):157–174.</rawString>
</citation>
<citation valid="true">
<authors>
<author>Grzegorz Chrupala</author>
</authors>
<title>Normalizing tweets with edit scripts and recurrent neural embeddings.</title>
<date>2014</date>
<booktitle>Proceedings of the 52st Annual Meeting of the Association for Computational Linguistics,</booktitle>
<pages>680--686</pages>
<contexts>
<context position="9411" citStr="Chrupala (2014)" startWordPosition="1539" endWordPosition="1540">nd Eisenstein, 2013). More recent approaches handled the text normalization task by building normalization lexicons. Han and Baldwin (2011) developed a two phased model, where they only consider the illformed OOV words for normalization. First, a confusion set is generated using the lexical and phonetic distance features. Later, the candidates in the confusion set are ranked using a mixture of dictionary look up, word similarity based on lexical edit distance, phonemic edit distance, prefix sub-string, suffix sub-string and longest common subsequence (LCS), as well as context support metrics. Chrupala (2014) on the other hand achieved lower word error rates without using any lexical resources. Gouws et al. (2011) investigated the distinct contributions of features that are highly depended on user-centric information such as the geological location of the users and the twitter client that the tweet is received from. Using such user-based contextual metrics they modelled the transformation distributions across populations. Liu et al. (2012) proposed a broad coverage normalization system, which integrates an extended noisy channel model, that is based on enhanced letter transformations, visual primi</context>
<context position="28050" citStr="Chrupala, 2014" startWordPosition="4749" endWordPosition="4750"> ck) (7) + externalScore(oi, ck) 4 Experiments 4.1 Datasets We used the LexNorm1.1 (LN) dataset (Han and Baldwin, 2011) and Pennell and Liu (2014)’s trigram dataset to evaluate our proposed approach. LexNorm1.1 contains 549 tweets with 1184 manually annotated ill-formed OOV tokens. It has been used by recent text normalization studies for evaluation, which enables us to directly compare our performance results with results obtained by the recent previous work (Han and Baldwin, 2011; Pennell and Liu, 2011; Han et al., 2012; Liu et al., 2012; Hassan and Menezes, 2013; Yang and Eisenstein, 2013; Chrupala, 2014). The trigram 319 dataset is an SMS-like corpus collected from twitter status updates sent via SMS. The dataset does not include the complete tweet text but trigrams from tweets and one OOV word in each trigram is annotated. In total 4661 twitter status messages and 7769 tokens are annotated. 4.2 Graph Generation We used a large corpus of social media text to construct our word association graph. We extracted 1.5 GB of English tweets from Stanford’s 476 million Twitter Dataset (Yang and Leskovec, 2011). The language identification of tweets was performed by using the langid.py Python library (</context>
<context position="30678" citStr="Chrupala, 2014" startWordPosition="5195" endWordPosition="5196">A and Q parameters in Equations 3 and 6 are set to 0.5. We did not tune these parameters for optimized performance. We selected the value of 0.5 in order to give less weight (half weight) to our minor contextual and lexical similarity features compared to the major ones. 4.4 Normalization Candidates Most of the prior work assume perfect detection of ill-formed words during test set decoding (Liu et al., 2012; Han and Baldwin, 2011; Pennell and Liu, 2011; Yang and Eisenstein, 2013). To be able to compare our results with studies that do not assume that ill-formed words have been preidentified (Chrupala, 2014; Hassan and Menezes, 2013; Han et al., 2012) we used our graph and built a dictionary to identify the ill-formed words. Following Han and Baldwin (2011) and Yang and Eisenstein (2013), we created a dictionary by choosing the nodes in our graph that have a frequency property higher than 20. Filtering this dictionary of 49657 words using GNU Aspell dictionary (v0.60.6) we produced a set of 26773 “invocabulary” (IV) words. In our second setup our system does not attemp to normalize the words in this set. 4.5 Results and Analysis In this paper we introduced a new contextual approach for text norm</context>
<context position="35076" citStr="Chrupala, 2014" startWordPosition="5908" endWordPosition="5909"> results obtained by Pennell and Liu (2011) on the trigram SMS-like dataset. The trigram nature of the dataset resulted in input texts which are (short thus) very limited with regard to contextual information. Nevertheless, our system achieved 72.8% F-Measure using this contextual information even though it is limited. Along the systems (presented in Table 5) that assume ill-formed tokens have been pre-identified perfectly by an oracle, there are also systems that are not based on this assumption but contain illformed word identification components (Han et al., 2012; Hassan and Menezes, 2013; Chrupala, 2014). We used the method described in Section 4.4 to identify the candidate tokens for normalization. Table 6 shows our results compared with the results of other systems that perform ill-formed word detection prior to normalization. We could label 1141 tokens correctly as ill-formed among 1184 ill-formed tokens. We achieved a word error rate (WER) of 2.6%, where Chrupala (2014) reported 4.8% and Han et al. (2012) reported 6.6% WER on the Lexnorm1.1 dataset. Method Dataset Precision Recall F-measure Han et al. (2012) LN 70.00 17.90 28.50 Hassan and Menezes (2013) LN 85.37 56.40 69.93 CWA-Graph LN </context>
</contexts>
<marker>Chrupala, 2014</marker>
<rawString>Grzegorz Chrupala. 2014. Normalizing tweets with edit scripts and recurrent neural embeddings. Proceedings of the 52st Annual Meeting of the Association for Computational Linguistics, pages 680–686.</rawString>
</citation>
<citation valid="true">
<authors>
<author>Danish Contractor</author>
<author>Tanveer A Faruquie</author>
<author>L Venkata Subramaniam</author>
</authors>
<title>Unsupervised Cleansing of Noisy Text.</title>
<date>2010</date>
<booktitle>Proceedings of the 23rd International Conference on Computational Linguistics: Posters,</booktitle>
<pages>189--196</pages>
<contexts>
<context position="23861" citStr="Contractor et al., 2010" startWordPosition="4067" endWordPosition="4070">t CL(oi) for the OOV token oi that includes all the unique candidates in EL(oi) and their contextual similarity scores calculated. 3.4 Lexical Similarity Following the prior work in (Han and Baldwin, 2011; Hassan and Menezes, 2013), our lexical similarity features are based on edit distance (Levenshtein, 1966), double metaphone (phonetic edit distance) (Philips, 2000), and a similarity function broken A c1 nice A Distance: 1 c2 3 n1 w P 26 new big A A c3 c5 24388 2 Distance: 0 beautiful A 2918 n2 c4 a D smile N 305 750 beatiful A 125 o2 Distance: 0 53 best A c6 20 great A n3 c7 318 (simCost) (Contractor et al., 2010) which is defined as the ratio of the Longest Common Subsequence Ratio (LCSR) (Melamed, 1999) of two words and the Edit Distance (ED) between their skeletons (Equations 4 and 5), where the skeleton of a word is obtained by removing its vowels. LCSR(oj, ck) = LCS(oj, ck)/maxLength(oj, ck) (4) simCost(oj, ck) = LCSR(oj, ck)/ED(oj, ck) (5) Following the tradition that is inspired from (Kaufmann and Kalita, 2010), before lexical similarity calculations, any repetitions of characters three or more times in OOV tokens are reduced to two (e.g. goooood is reduced to good). Then, the edit distance, pho</context>
</contexts>
<marker>Contractor, Faruquie, Subramaniam, 2010</marker>
<rawString>Danish Contractor, Tanveer A. Faruquie, and L. Venkata Subramaniam. 2010. Unsupervised Cleansing of Noisy Text. Proceedings of the 23rd International Conference on Computational Linguistics: Posters, pages 189–196.</rawString>
</citation>
<citation valid="true">
<authors>
<author>Paul Cook</author>
<author>Suzanne Stevenson</author>
</authors>
<title>An Unsupervised Model for Text Message Normalization.</title>
<date>2009</date>
<booktitle>Proceedings of the Workshop on Computational Approaches to Linguistic Creativity,</booktitle>
<pages>71--78</pages>
<contexts>
<context position="7024" citStr="Cook and Stevenson (2009)" startWordPosition="1141" endWordPosition="1144">outanova and Moore (2002) improved this approach by extending the error model with phonetic similarities over words. Their approach is based on learning rules to predict the pronunciation of a single letter in the word depending on the neighbouring letters in the word. Choudhury et al. (2007) developed a supervised Hidden Markov Model based approach for normalizing Short Message Service (SMS) texts. They proposed a word for word decoding approach and used a dictionary based method to normalize commonly used abbreviations and nonstandard usage (e.g. “howz” to “how are” or “aint” to “are not”). Cook and Stevenson (2009) extended this model by introducing an unsupervised noisy channel model. Rather than using one generic model for all word formations as in (Choudhury et al., 2007), they used a mixture model in which each different word formation type is modeled explicitly. The limitations of these methods were that they did not consider contextual features and assumed that tokens have unique normalizations. In the text normalization task several OOV tokens are ambiguous and without contextual information it is not possible to build models that can disambiguate transformations correctly. Aw et al. (2006) propo</context>
</contexts>
<marker>Cook, Stevenson, 2009</marker>
<rawString>Paul Cook and Suzanne Stevenson. 2009. An Unsupervised Model for Text Message Normalization. Proceedings of the Workshop on Computational Approaches to Linguistic Creativity, pages 71–78.</rawString>
</citation>
<citation valid="true">
<authors>
<author>Jacob Eisenstein</author>
</authors>
<title>What to Do About Bad Language on the Internet.</title>
<date>2013</date>
<booktitle>Proceedings of the North American Chapter of the Association for Computational Linguistics : Human Language Technologies,</booktitle>
<pages>359--369</pages>
<contexts>
<context position="1713" citStr="Eisenstein, 2013" startWordPosition="255" endWordPosition="256">lgorithm to represent the phonetic similarity. Unlike most of the recent approaches that are based on generating normalization dictionaries, the proposed approach performs normalization by considering the context of the non-standard words in the input text. Our results show that it achieves state-ofthe-art F-score performance on standard datasets. In addition, the system can be tuned to achieve very high precision without sacrificing much from recall. 1 Introduction Social text, which has been growing and evolving steadily, has its own lexical and grammatical features (Choudhury et al., 2007; Eisenstein, 2013). lol meaning laughing out loud, xoxo meaning kissing, 4u meaning for you are among the most commonly used examples of this jargon. In addition, these informal expressions in social text usually take many different lexical forms when generated by different individuals (Eisenstein, 2013). The limited accuracies of the Speech-to-Text (STT) tools in mobile devices, which are increasingly being used to post messages on social media platforms, along with the scarcity of attention of the users result in additional divergence of social text from more standard text such as from the newswire domain. To</context>
<context position="8816" citStr="Eisenstein, 2013" startWordPosition="1446" endWordPosition="1447">he sense of utilizing contextual sensitivity and coverage. 314 Pennell and Liu (2011) on the other hand, proposed a character level MT system, that is robust to new abbreviations. In their two phased system, a character level trained MT model is used to produce word hypotheses and a trigram LM is used to choose a hypothesis that fits into the input context. The MT based models are supervised models, a drawback of which is that they require annotated data. Annotated training data is not readily available and is difficult to create especially for the rapidly evolving social media text (Yang and Eisenstein, 2013). More recent approaches handled the text normalization task by building normalization lexicons. Han and Baldwin (2011) developed a two phased model, where they only consider the illformed OOV words for normalization. First, a confusion set is generated using the lexical and phonetic distance features. Later, the candidates in the confusion set are ranked using a mixture of dictionary look up, word similarity based on lexical edit distance, phonemic edit distance, prefix sub-string, suffix sub-string and longest common subsequence (LCS), as well as context support metrics. Chrupala (2014) on t</context>
<context position="10190" citStr="Eisenstein (2013)" startWordPosition="1655" endWordPosition="1656">at are highly depended on user-centric information such as the geological location of the users and the twitter client that the tweet is received from. Using such user-based contextual metrics they modelled the transformation distributions across populations. Liu et al. (2012) proposed a broad coverage normalization system, which integrates an extended noisy channel model, that is based on enhanced letter transformations, visual priming, string and phonetic similarity. They try to improve the performance of the top n normalization candidates by integrating human perspective modeling. Yang and Eisenstein (2013) introduced an unsupervised log linear model for text normalization. Their joint statistical approach uses local context based on language modeling and surface similarity. Along with dictionary based models, Yang and Eisenstein’s model have obtained a significant improvement on the performance of text normalization systems. Another relevant study is conducted by Hassan and Menezes (2013), who generated a normalization lexicon using Markov random walks on a contextual similarity lattice that they created using 5- gram sequences of words. The best normalization candidates are chosen using the av</context>
<context position="28033" citStr="Eisenstein, 2013" startWordPosition="4747" endWordPosition="4748">+ contSimScore(oi, ck) (7) + externalScore(oi, ck) 4 Experiments 4.1 Datasets We used the LexNorm1.1 (LN) dataset (Han and Baldwin, 2011) and Pennell and Liu (2014)’s trigram dataset to evaluate our proposed approach. LexNorm1.1 contains 549 tweets with 1184 manually annotated ill-formed OOV tokens. It has been used by recent text normalization studies for evaluation, which enables us to directly compare our performance results with results obtained by the recent previous work (Han and Baldwin, 2011; Pennell and Liu, 2011; Han et al., 2012; Liu et al., 2012; Hassan and Menezes, 2013; Yang and Eisenstein, 2013; Chrupala, 2014). The trigram 319 dataset is an SMS-like corpus collected from twitter status updates sent via SMS. The dataset does not include the complete tweet text but trigrams from tweets and one OOV word in each trigram is annotated. In total 4661 twitter status messages and 7769 tokens are annotated. 4.2 Graph Generation We used a large corpus of social media text to construct our word association graph. We extracted 1.5 GB of English tweets from Stanford’s 476 million Twitter Dataset (Yang and Leskovec, 2011). The language identification of tweets was performed by using the langid.py</context>
<context position="30549" citStr="Eisenstein, 2013" startWordPosition="5173" endWordPosition="5174">ter edit distance of a given OOV word under phonemic transcription were chosen as lexical similarity candidates. The values for the A and Q parameters in Equations 3 and 6 are set to 0.5. We did not tune these parameters for optimized performance. We selected the value of 0.5 in order to give less weight (half weight) to our minor contextual and lexical similarity features compared to the major ones. 4.4 Normalization Candidates Most of the prior work assume perfect detection of ill-formed words during test set decoding (Liu et al., 2012; Han and Baldwin, 2011; Pennell and Liu, 2011; Yang and Eisenstein, 2013). To be able to compare our results with studies that do not assume that ill-formed words have been preidentified (Chrupala, 2014; Hassan and Menezes, 2013; Han et al., 2012) we used our graph and built a dictionary to identify the ill-formed words. Following Han and Baldwin (2011) and Yang and Eisenstein (2013), we created a dictionary by choosing the nodes in our graph that have a frequency property higher than 20. Filtering this dictionary of 49657 words using GNU Aspell dictionary (v0.60.6) we produced a set of 26773 “invocabulary” (IV) words. In our second setup our system does not attemp</context>
<context position="33602" citStr="Eisenstein (2013)" startWordPosition="5678" endWordPosition="5679">nal system of Han and Baldwin (2011). The results obtained by our proposed Contextual Word Association Graph (CWA-Graph) system on the LexNorm1.1 and trigram datasets, as well as the results of recent studies that used the same datasets for evaluation are presented in Table 5. The ill-formed words are assumed to have been pre-identified in advance. Method Dataset Precision Recall F-measure lexSimScore LN 28.28 28.20 28.24 externalScore LN 64.69 64.52 64.60 lexSimScore+externalScore LN 77.22 77.02 77.12 Han and Baldwin (2011) LN 75.30 75.30 75.30 Liu et al. (2012) LN 84.13 78.38 81.15 Yang and Eisenstein (2013) LN 82.09 82.09 82.09 CWA-Graph LN 85.50 79.22 82.24 lexSimScore Trigram 39.10 38.40 38.70 externalScore Trigram 44.20 43.30 43.80 lexSimScore+externalScore Trigram 65.50 64.20 64.80 Pennell and Liu (2011) Trigram 69.7 69.7 69.7 CWA-Graph Trigram 77.2 68.8 72.8 Table 5: Results obtained when ill-formed words are assumed to have been pre-identified in advance. Our CWA-Graph approach achieves the best Fmeasure (82.24%) and precision (85.50%) among the recent previous studies. The high precision value is obtained without compromising much from recall (79.22%). Our recall is the second best among </context>
<context position="35883" citStr="Eisenstein, 2013" startWordPosition="6038" endWordPosition="6039">word detection prior to normalization. We could label 1141 tokens correctly as ill-formed among 1184 ill-formed tokens. We achieved a word error rate (WER) of 2.6%, where Chrupala (2014) reported 4.8% and Han et al. (2012) reported 6.6% WER on the Lexnorm1.1 dataset. Method Dataset Precision Recall F-measure Han et al. (2012) LN 70.00 17.90 28.50 Hassan and Menezes (2013) LN 85.37 56.40 69.93 CWA-Graph LN 85.87 76.52 80.92 Table 6: Results obtained without assuming that ill-formed words have been pre-identified. As shown in Table 5 some systems have equal precision and recall values (Yang and Eisenstein, 2013; Han and Baldwin, 2011; Pennell and Liu, 2011). Those systems normalize all ill-formed words. On the other hand, our system does not return a normalization, if there are no candidates that are lexically similar, grammatically correct, and contextually close enough. For this reason, we managed to achieve a higher precision compared to the other systems. Our system returns a normalization candidate for an OOV word only if it achieves a similarity score (contextual, lexical, external, or some degree of each feature) above a threshold value. The default threshold used in the system is set equal t</context>
</contexts>
<marker>Eisenstein, 2013</marker>
<rawString>Jacob Eisenstein. 2013. What to Do About Bad Language on the Internet. Proceedings of the North American Chapter of the Association for Computational Linguistics : Human Language Technologies, pages 359–369.</rawString>
</citation>
<citation valid="false">
<authors>
<author>Kevin Gimpel</author>
<author>Nathan Schneider</author>
<author>Brendan O’Connor</author>
<author>Dipanjan Das</author>
<author>Daniel Mills</author>
<author>Jacob Eisenstein</author>
<author>Michael Heilman</author>
<author>Dani Yogatama</author>
<author>Jeffrey Flanigan</author>
<author>Noah A Smith</author>
</authors>
<title>Part-of-speech Tagging for Twitter: Annotation, Features, and Experiments.</title>
<date>2011</date>
<booktitle>Proceedings of the 49th Annual Meeting of the Association</booktitle>
<volume>2</volume>
<pages>42--47</pages>
<marker>Gimpel, Schneider, O’Connor, Das, Mills, Eisenstein, Heilman, Yogatama, Flanigan, Smith, 2011</marker>
<rawString>Kevin Gimpel, Nathan Schneider, Brendan O’Connor, Dipanjan Das, Daniel Mills, Jacob Eisenstein, Michael Heilman, Dani Yogatama, Jeffrey Flanigan, and Noah A. Smith. 2011. Part-of-speech Tagging for Twitter: Annotation, Features, and Experiments. Proceedings of the 49th Annual Meeting of the Association for Computational Linguistics: Human Language Technologies: Short Papers - Volume 2, pages 42–47.</rawString>
</citation>
<citation valid="true">
<authors>
<author>Stephan Gouws</author>
<author>Donald Metzler</author>
<author>Congxing Cai</author>
<author>Eduard Hovy</author>
</authors>
<date>2011</date>
<booktitle>Contextual Bearing on Linguistic Variation in Social Media. Proceedings of the Workshop on Languages in Social Media,</booktitle>
<pages>20--29</pages>
<contexts>
<context position="9518" citStr="Gouws et al. (2011)" startWordPosition="1555" endWordPosition="1558">ion lexicons. Han and Baldwin (2011) developed a two phased model, where they only consider the illformed OOV words for normalization. First, a confusion set is generated using the lexical and phonetic distance features. Later, the candidates in the confusion set are ranked using a mixture of dictionary look up, word similarity based on lexical edit distance, phonemic edit distance, prefix sub-string, suffix sub-string and longest common subsequence (LCS), as well as context support metrics. Chrupala (2014) on the other hand achieved lower word error rates without using any lexical resources. Gouws et al. (2011) investigated the distinct contributions of features that are highly depended on user-centric information such as the geological location of the users and the twitter client that the tweet is received from. Using such user-based contextual metrics they modelled the transformation distributions across populations. Liu et al. (2012) proposed a broad coverage normalization system, which integrates an extended noisy channel model, that is based on enhanced letter transformations, visual priming, string and phonetic similarity. They try to improve the performance of the top n normalization candidat</context>
<context position="26415" citStr="Gouws et al., 2011" startWordPosition="4478" endWordPosition="4481">tion graph that satisfy the edit distance and 3an approximate string comparison measure (between 0.0 and 1.0) using the edit distance https://sourceforge.net/projects/febrl/ phonetic edit distance criteria. We also incorporated candidates from external resources, in other words from a slang dictionary and a transliteration table of numbers and pronouns. If a candidate occurs in the slang dictionary or in the transliteration table as a correspondence to its OOV word, it is assigned an external score of 1, otherwise it is assigned an external score of 0. The transliterations were first used by (Gouws et al., 2011). Besides the token and its transliteration we also use its POS tag information, which was not available in their system. The external score favors the well known interpretations of common OOV words. However, unlike the dictionary based methodologies, our system does not return the corresponding unabbreviated word in the slang dictionary or in the transliteration table directly. Only an external score gets assigned and the candidate still needs to compete with other candidates which may have higher contextual similarities and one of those contextually more similar candidates may be returned as</context>
</contexts>
<marker>Gouws, Metzler, Cai, Hovy, 2011</marker>
<rawString>Stephan Gouws, Donald Metzler, Congxing Cai, and Eduard Hovy. 2011. Contextual Bearing on Linguistic Variation in Social Media. Proceedings of the Workshop on Languages in Social Media, pages 20–29.</rawString>
</citation>
<citation valid="true">
<authors>
<author>Bo Han</author>
<author>Timothy Baldwin</author>
</authors>
<title>Lexical Normalisation of Short Text Messages: Makn Sens a #Twitter.</title>
<date>2011</date>
<booktitle>Proceedings of the 49th Annual Meeting of the Association for Computational Linguistics: Human Language Technologies -</booktitle>
<volume>1</volume>
<pages>368--378</pages>
<contexts>
<context position="2868" citStr="Han and Baldwin, 2011" startWordPosition="440" endWordPosition="443">l text from more standard text such as from the newswire domain. Tools such as spellchecker and slang dictionaries have been shown to be insufficient to cope with this challenge long time ago (Sproat et al., 2001). In addition, most Natural Language Processing (NLP) tools including named entity recognizers and dependency parsers generally perform poorly on social text (Ritter et al., 2010). Text normalization is a preprocessing step to restore non-standard words in text to their original (canonical) forms to make use in NLP applications or more broadly to understand the digitized text better (Han and Baldwin, 2011). For example, talk 2 u later can be normalized as talk to you later or similarly enormoooos, enrmss and enourmos can be normalized as enormous. Other examples of text messages from Twitter and their corresponding normalized forms are shown in Table 1. The non-standard words in text are referred to as Out of Vocabulary (OOV) words. The normalization task restores the OOV words to their In Vocabulary (IV) forms. Social text is continuously evolving with new words and named entities that are not in the vocabularies of the systems (Hassan and Menezes, 2013). Therefore, not every OOV word (e.g. iP</context>
<context position="8935" citStr="Han and Baldwin (2011)" startWordPosition="1462" endWordPosition="1465">a character level MT system, that is robust to new abbreviations. In their two phased system, a character level trained MT model is used to produce word hypotheses and a trigram LM is used to choose a hypothesis that fits into the input context. The MT based models are supervised models, a drawback of which is that they require annotated data. Annotated training data is not readily available and is difficult to create especially for the rapidly evolving social media text (Yang and Eisenstein, 2013). More recent approaches handled the text normalization task by building normalization lexicons. Han and Baldwin (2011) developed a two phased model, where they only consider the illformed OOV words for normalization. First, a confusion set is generated using the lexical and phonetic distance features. Later, the candidates in the confusion set are ranked using a mixture of dictionary look up, word similarity based on lexical edit distance, phonemic edit distance, prefix sub-string, suffix sub-string and longest common subsequence (LCS), as well as context support metrics. Chrupala (2014) on the other hand achieved lower word error rates without using any lexical resources. Gouws et al. (2011) investigated the</context>
<context position="13865" citStr="Han and Baldwin, (2011)" startWordPosition="2272" endWordPosition="2275">formation of words is modeled through a word association graph created by using a large corpus of social media text. The graph encodes the relative positions of the POS tagged words in the text with respect to each other. After preprocessing, each text message in the corpus is traversed in order to extract the nodes and the edges of the graph. A node is defined with four properties: id, oov, freq and tag. The token itself is the id field. The freq property indicates the node’s frequency count in the dataset. The oov field is set to True if the token is an OOV word. Following the prior work by Han and Baldwin, (2011) we used the GNU Aspell dictionary (v0.60.6) to determine whether a word is OOV or not. We also edited the output of Aspell dictionary to accept letters other than “a” and “i” as OOV words. A portion of the graph that covers parts of the sample sentence in Table 3 is shown in Figure 1. In the created word association graph, each node is a unique set of a token and its POS tag. This helps us to identify the candidate IV words for a given OOV word by considering not only lexical and contextual similarity, but also grammatical similarity in terms of POS tags. For example, if the token smile has b</context>
<context position="23441" citStr="Han and Baldwin, 2011" startWordPosition="3985" endWordPosition="3988"> of the candidate with respect to the frequencies of the other candidates in the corpus. Since the total edge weight score is our primary contextual resource, we may want to favor edge weight scores. We give the frequency score a weight 0 &lt; Q &lt; 1 to be able to limit its effect on the total contextual similarity score. contSimScore(oi, ck) = EW Score(oi, ck) (3) + β ∗ freqScore(ck) Hereby, we have the candidate list CL(oi) for the OOV token oi that includes all the unique candidates in EL(oi) and their contextual similarity scores calculated. 3.4 Lexical Similarity Following the prior work in (Han and Baldwin, 2011; Hassan and Menezes, 2013), our lexical similarity features are based on edit distance (Levenshtein, 1966), double metaphone (phonetic edit distance) (Philips, 2000), and a similarity function broken A c1 nice A Distance: 1 c2 3 n1 w P 26 new big A A c3 c5 24388 2 Distance: 0 beautiful A 2918 n2 c4 a D smile N 305 750 beatiful A 125 o2 Distance: 0 53 best A c6 20 great A n3 c7 318 (simCost) (Contractor et al., 2010) which is defined as the ratio of the Longest Common Subsequence Ratio (LCSR) (Melamed, 1999) of two words and the Edit Distance (ED) between their skeletons (Equations 4 and 5), w</context>
<context position="27554" citStr="Han and Baldwin, 2011" startWordPosition="4667" endWordPosition="4670">milarities and one of those contextually more similar candidates may be returned as the correct normalization instead of the candidate found equivalent to the OOV word in the slang dictionary (or in the transliteration table). 3.6 Overall Scoring As shown in Equation 7, the final score of a candidate IV token ck for an OOV token oi is the sum of its lexical similarity score, contextual similarity score and external score with respect to oi. candScore(oi, ck) = lexSimScore(oi, ck) + contSimScore(oi, ck) (7) + externalScore(oi, ck) 4 Experiments 4.1 Datasets We used the LexNorm1.1 (LN) dataset (Han and Baldwin, 2011) and Pennell and Liu (2014)’s trigram dataset to evaluate our proposed approach. LexNorm1.1 contains 549 tweets with 1184 manually annotated ill-formed OOV tokens. It has been used by recent text normalization studies for evaluation, which enables us to directly compare our performance results with results obtained by the recent previous work (Han and Baldwin, 2011; Pennell and Liu, 2011; Han et al., 2012; Liu et al., 2012; Hassan and Menezes, 2013; Yang and Eisenstein, 2013; Chrupala, 2014). The trigram 319 dataset is an SMS-like corpus collected from twitter status updates sent via SMS. The </context>
<context position="29852" citStr="Han and Baldwin, 2011" startWordPosition="5050" endWordPosition="5053">ention (e.g. @brendon), discourse marker (e.g. RT), URL, email address, emoticon, numeral, and punctuation. The remaining tokens are used to build the word association graph. After constructing the graph we only kept the nodes with a frequency greater than 8. For the performance related reasons, the relatedness thresholds tdistance and tfrequency were chosen as 3 and 8, respectively. The resulting graph contains 105428 nodes and 46609603 edges. 4.3 Candidate Set Generation While extending the candidate set with lexical features we use tedit ≤ 2 ∨ tphonetic ≤ 1 to keep up with the settings in (Han and Baldwin, 2011). In other words, IV words that are within 2 character edit distance or 1 character edit distance of a given OOV word under phonemic transcription were chosen as lexical similarity candidates. The values for the A and Q parameters in Equations 3 and 6 are set to 0.5. We did not tune these parameters for optimized performance. We selected the value of 0.5 in order to give less weight (half weight) to our minor contextual and lexical similarity features compared to the major ones. 4.4 Normalization Candidates Most of the prior work assume perfect detection of ill-formed words during test set dec</context>
<context position="31435" citStr="Han and Baldwin (2011)" startWordPosition="5326" endWordPosition="5329">and Baldwin (2011) and Yang and Eisenstein (2013), we created a dictionary by choosing the nodes in our graph that have a frequency property higher than 20. Filtering this dictionary of 49657 words using GNU Aspell dictionary (v0.60.6) we produced a set of 26773 “invocabulary” (IV) words. In our second setup our system does not attemp to normalize the words in this set. 4.5 Results and Analysis In this paper we introduced a new contextual approach for text normalization. The lexical similarity score described in Section 3.4 and the external score described in Section 3.5 depend on the work of Han and Baldwin (2011). With small changes made to the previously proposed method we took it as a baseline in our study. As contextual layer we proposed two metrics extracted from the word association graph. The first one depends on the total edge weights between candidates and OOV neighbours, the second one is based on the frequencies of the candidates in the corpus. As the evaluation metrics we used precision, recall, and F-Measure. Precision calculates the proportion of correctly normalized words among the words for which we produced a normalization. Recall shows the amount of correct normalizations over the wor</context>
<context position="32845" citStr="Han and Baldwin, 2011" startWordPosition="5556" endWordPosition="5559">nd recall. We investigated the impact of lexSimScore and externalScore seperately on both datasets (Table 5). Using only lexSimScore the system achieved an F-measure of 28.24% on the LexNorm1.1 dataset and 38.70% on the Trigram dataset, which shows that lexical similarity alone is not enough for a good normalization system. 320 However, the externalScore which is the layer that is more aware of the Internet jargon, along with some social text specific rule based transliterations performs better than expected on both datasets. Mixing these two layers we reach our baseline that is adopted from (Han and Baldwin, 2011). This baseline setup obtained an F-measure of 77.12% on LexNorm1.1, which is slightly better than the result (75.30%) reported by the original system of Han and Baldwin (2011). The results obtained by our proposed Contextual Word Association Graph (CWA-Graph) system on the LexNorm1.1 and trigram datasets, as well as the results of recent studies that used the same datasets for evaluation are presented in Table 5. The ill-formed words are assumed to have been pre-identified in advance. Method Dataset Precision Recall F-measure lexSimScore LN 28.28 28.20 28.24 externalScore LN 64.69 64.52 64.60</context>
<context position="35906" citStr="Han and Baldwin, 2011" startWordPosition="6040" endWordPosition="6043">or to normalization. We could label 1141 tokens correctly as ill-formed among 1184 ill-formed tokens. We achieved a word error rate (WER) of 2.6%, where Chrupala (2014) reported 4.8% and Han et al. (2012) reported 6.6% WER on the Lexnorm1.1 dataset. Method Dataset Precision Recall F-measure Han et al. (2012) LN 70.00 17.90 28.50 Hassan and Menezes (2013) LN 85.37 56.40 69.93 CWA-Graph LN 85.87 76.52 80.92 Table 6: Results obtained without assuming that ill-formed words have been pre-identified. As shown in Table 5 some systems have equal precision and recall values (Yang and Eisenstein, 2013; Han and Baldwin, 2011; Pennell and Liu, 2011). Those systems normalize all ill-formed words. On the other hand, our system does not return a normalization, if there are no candidates that are lexically similar, grammatically correct, and contextually close enough. For this reason, we managed to achieve a higher precision compared to the other systems. Our system returns a normalization candidate for an OOV word only if it achieves a similarity score (contextual, lexical, external, or some degree of each feature) above a threshold value. The default threshold used in the system is set equal to the maximum score tha</context>
</contexts>
<marker>Han, Baldwin, 2011</marker>
<rawString>Bo Han and Timothy Baldwin. 2011. Lexical Normalisation of Short Text Messages: Makn Sens a #Twitter. Proceedings of the 49th Annual Meeting of the Association for Computational Linguistics: Human Language Technologies - Volume 1, pages 368–378.</rawString>
</citation>
<citation valid="true">
<authors>
<author>Bo Han</author>
<author>Paul Cook</author>
<author>Timothy Baldwin</author>
</authors>
<title>Automatically constructing a normalisation dictionary for microblogs.</title>
<date>2012</date>
<booktitle>Proceedings of the 2012 Joint Conference on Empirical Methods in Natural Language Processing and Computational Natural Language Learning,</booktitle>
<pages>421--432</pages>
<contexts>
<context position="5664" citStr="Han et al., 2012" startWordPosition="912" endWordPosition="915">and edit distance that encode the surface similarity and the double metaphone algorithm that encodes the phonetic similarity. The proposed approach is unsupervised, which is an important advantage over supervised systems, given the continuously evolving language in the social media domain. The same OOV word may have different appropriate normalizations depending on the context of the input text message. Recently proposed dictionary-based text normalization systems perform dictionary look-up and always normalize the same OOV word to the same IV word regardless of the context of the input text (Han et al., 2012; Hassan and Menezes, 2013). On the other hand, the proposed approach does not only make use of the general context information in a large corpus of social media text, but it also makes use of the context of the OOV word in the input text message. Thus, an OOV word can be normalized to different IV words depending on the context of the input text. 2 Related Work Early work on text normalization mostly made use of the noisy channel model. The first work that had a significant performance improvement over the previous research was by Brill and Moore (2000). They proposed a novel noisy channel mo</context>
<context position="27962" citStr="Han et al., 2012" startWordPosition="4733" endWordPosition="4736">rnal score with respect to oi. candScore(oi, ck) = lexSimScore(oi, ck) + contSimScore(oi, ck) (7) + externalScore(oi, ck) 4 Experiments 4.1 Datasets We used the LexNorm1.1 (LN) dataset (Han and Baldwin, 2011) and Pennell and Liu (2014)’s trigram dataset to evaluate our proposed approach. LexNorm1.1 contains 549 tweets with 1184 manually annotated ill-formed OOV tokens. It has been used by recent text normalization studies for evaluation, which enables us to directly compare our performance results with results obtained by the recent previous work (Han and Baldwin, 2011; Pennell and Liu, 2011; Han et al., 2012; Liu et al., 2012; Hassan and Menezes, 2013; Yang and Eisenstein, 2013; Chrupala, 2014). The trigram 319 dataset is an SMS-like corpus collected from twitter status updates sent via SMS. The dataset does not include the complete tweet text but trigrams from tweets and one OOV word in each trigram is annotated. In total 4661 twitter status messages and 7769 tokens are annotated. 4.2 Graph Generation We used a large corpus of social media text to construct our word association graph. We extracted 1.5 GB of English tweets from Stanford’s 476 million Twitter Dataset (Yang and Leskovec, 2011). The</context>
<context position="30723" citStr="Han et al., 2012" startWordPosition="5201" endWordPosition="5204">e set to 0.5. We did not tune these parameters for optimized performance. We selected the value of 0.5 in order to give less weight (half weight) to our minor contextual and lexical similarity features compared to the major ones. 4.4 Normalization Candidates Most of the prior work assume perfect detection of ill-formed words during test set decoding (Liu et al., 2012; Han and Baldwin, 2011; Pennell and Liu, 2011; Yang and Eisenstein, 2013). To be able to compare our results with studies that do not assume that ill-formed words have been preidentified (Chrupala, 2014; Hassan and Menezes, 2013; Han et al., 2012) we used our graph and built a dictionary to identify the ill-formed words. Following Han and Baldwin (2011) and Yang and Eisenstein (2013), we created a dictionary by choosing the nodes in our graph that have a frequency property higher than 20. Filtering this dictionary of 49657 words using GNU Aspell dictionary (v0.60.6) we produced a set of 26773 “invocabulary” (IV) words. In our second setup our system does not attemp to normalize the words in this set. 4.5 Results and Analysis In this paper we introduced a new contextual approach for text normalization. The lexical similarity score descr</context>
<context position="35033" citStr="Han et al., 2012" startWordPosition="5900" endWordPosition="5903"> the parameters, we were able to improve the results obtained by Pennell and Liu (2011) on the trigram SMS-like dataset. The trigram nature of the dataset resulted in input texts which are (short thus) very limited with regard to contextual information. Nevertheless, our system achieved 72.8% F-Measure using this contextual information even though it is limited. Along the systems (presented in Table 5) that assume ill-formed tokens have been pre-identified perfectly by an oracle, there are also systems that are not based on this assumption but contain illformed word identification components (Han et al., 2012; Hassan and Menezes, 2013; Chrupala, 2014). We used the method described in Section 4.4 to identify the candidate tokens for normalization. Table 6 shows our results compared with the results of other systems that perform ill-formed word detection prior to normalization. We could label 1141 tokens correctly as ill-formed among 1184 ill-formed tokens. We achieved a word error rate (WER) of 2.6%, where Chrupala (2014) reported 4.8% and Han et al. (2012) reported 6.6% WER on the Lexnorm1.1 dataset. Method Dataset Precision Recall F-measure Han et al. (2012) LN 70.00 17.90 28.50 Hassan and Meneze</context>
</contexts>
<marker>Han, Cook, Baldwin, 2012</marker>
<rawString>Bo Han, Paul Cook, and Timothy Baldwin. 2012. Automatically constructing a normalisation dictionary for microblogs. Proceedings of the 2012 Joint Conference on Empirical Methods in Natural Language Processing and Computational Natural Language Learning, pages 421–432.</rawString>
</citation>
<citation valid="true">
<authors>
<author>Hany Hassan</author>
<author>Arul Menezes</author>
</authors>
<title>Social Text Normalization Using Contextual Graph Random Walks.</title>
<date>2013</date>
<booktitle>Proceedings of the 51st Annual Meeting of the Association for Computational Linguistics,</booktitle>
<pages>1577--1586</pages>
<contexts>
<context position="3428" citStr="Hassan and Menezes, 2013" startWordPosition="541" endWordPosition="544">y to understand the digitized text better (Han and Baldwin, 2011). For example, talk 2 u later can be normalized as talk to you later or similarly enormoooos, enrmss and enourmos can be normalized as enormous. Other examples of text messages from Twitter and their corresponding normalized forms are shown in Table 1. The non-standard words in text are referred to as Out of Vocabulary (OOV) words. The normalization task restores the OOV words to their In Vocabulary (IV) forms. Social text is continuously evolving with new words and named entities that are not in the vocabularies of the systems (Hassan and Menezes, 2013). Therefore, not every OOV word (e.g. iPhone, WikiLeaks or tok313 Proceedings of the 2014 Conference on Empirical Methods in Natural Language Processing (EMNLP), pages 313–324, October 25-29, 2014, Doha, Qatar. c�2014 Association for Computational Linguistics Hav guts to say wat u desire.. Dnt beat behind da bush!! And 1 mre thng no mre say y r people’s man!! There r sm songs u don’t want 2 listen 2 yl walking cos when u start dancing ppl won’t knw y. Have guts to say what you desire.. Don’t beat behind the bush!! And one more thing no more say you are people’s man!! There are some songs you d</context>
<context position="5691" citStr="Hassan and Menezes, 2013" startWordPosition="916" endWordPosition="919">that encode the surface similarity and the double metaphone algorithm that encodes the phonetic similarity. The proposed approach is unsupervised, which is an important advantage over supervised systems, given the continuously evolving language in the social media domain. The same OOV word may have different appropriate normalizations depending on the context of the input text message. Recently proposed dictionary-based text normalization systems perform dictionary look-up and always normalize the same OOV word to the same IV word regardless of the context of the input text (Han et al., 2012; Hassan and Menezes, 2013). On the other hand, the proposed approach does not only make use of the general context information in a large corpus of social media text, but it also makes use of the context of the OOV word in the input text message. Thus, an OOV word can be normalized to different IV words depending on the context of the input text. 2 Related Work Early work on text normalization mostly made use of the noisy channel model. The first work that had a significant performance improvement over the previous research was by Brill and Moore (2000). They proposed a novel noisy channel model for spell checking base</context>
<context position="10580" citStr="Hassan and Menezes (2013)" startWordPosition="1711" endWordPosition="1714">s based on enhanced letter transformations, visual priming, string and phonetic similarity. They try to improve the performance of the top n normalization candidates by integrating human perspective modeling. Yang and Eisenstein (2013) introduced an unsupervised log linear model for text normalization. Their joint statistical approach uses local context based on language modeling and surface similarity. Along with dictionary based models, Yang and Eisenstein’s model have obtained a significant improvement on the performance of text normalization systems. Another relevant study is conducted by Hassan and Menezes (2013), who generated a normalization lexicon using Markov random walks on a contextual similarity lattice that they created using 5- gram sequences of words. The best normalization candidates are chosen using the average hitting time and lexical similarity features. Context of a word in the center of a 5-gram sequence is defined by the other words in the 5-gram. Even if one word is not the same, the context is considered to be different. This is a relatively conservative way for modeling the prior contexts of words. In our model, we filtered candidate words based on their grammatical properties and</context>
<context position="23468" citStr="Hassan and Menezes, 2013" startWordPosition="3989" endWordPosition="3992">respect to the frequencies of the other candidates in the corpus. Since the total edge weight score is our primary contextual resource, we may want to favor edge weight scores. We give the frequency score a weight 0 &lt; Q &lt; 1 to be able to limit its effect on the total contextual similarity score. contSimScore(oi, ck) = EW Score(oi, ck) (3) + β ∗ freqScore(ck) Hereby, we have the candidate list CL(oi) for the OOV token oi that includes all the unique candidates in EL(oi) and their contextual similarity scores calculated. 3.4 Lexical Similarity Following the prior work in (Han and Baldwin, 2011; Hassan and Menezes, 2013), our lexical similarity features are based on edit distance (Levenshtein, 1966), double metaphone (phonetic edit distance) (Philips, 2000), and a similarity function broken A c1 nice A Distance: 1 c2 3 n1 w P 26 new big A A c3 c5 24388 2 Distance: 0 beautiful A 2918 n2 c4 a D smile N 305 750 beatiful A 125 o2 Distance: 0 53 best A c6 20 great A n3 c7 318 (simCost) (Contractor et al., 2010) which is defined as the ratio of the Longest Common Subsequence Ratio (LCSR) (Melamed, 1999) of two words and the Edit Distance (ED) between their skeletons (Equations 4 and 5), where the skeleton of a word</context>
<context position="28006" citStr="Hassan and Menezes, 2013" startWordPosition="4741" endWordPosition="4744">core(oi, ck) = lexSimScore(oi, ck) + contSimScore(oi, ck) (7) + externalScore(oi, ck) 4 Experiments 4.1 Datasets We used the LexNorm1.1 (LN) dataset (Han and Baldwin, 2011) and Pennell and Liu (2014)’s trigram dataset to evaluate our proposed approach. LexNorm1.1 contains 549 tweets with 1184 manually annotated ill-formed OOV tokens. It has been used by recent text normalization studies for evaluation, which enables us to directly compare our performance results with results obtained by the recent previous work (Han and Baldwin, 2011; Pennell and Liu, 2011; Han et al., 2012; Liu et al., 2012; Hassan and Menezes, 2013; Yang and Eisenstein, 2013; Chrupala, 2014). The trigram 319 dataset is an SMS-like corpus collected from twitter status updates sent via SMS. The dataset does not include the complete tweet text but trigrams from tweets and one OOV word in each trigram is annotated. In total 4661 twitter status messages and 7769 tokens are annotated. 4.2 Graph Generation We used a large corpus of social media text to construct our word association graph. We extracted 1.5 GB of English tweets from Stanford’s 476 million Twitter Dataset (Yang and Leskovec, 2011). The language identification of tweets was perfo</context>
<context position="30704" citStr="Hassan and Menezes, 2013" startWordPosition="5197" endWordPosition="5200">rs in Equations 3 and 6 are set to 0.5. We did not tune these parameters for optimized performance. We selected the value of 0.5 in order to give less weight (half weight) to our minor contextual and lexical similarity features compared to the major ones. 4.4 Normalization Candidates Most of the prior work assume perfect detection of ill-formed words during test set decoding (Liu et al., 2012; Han and Baldwin, 2011; Pennell and Liu, 2011; Yang and Eisenstein, 2013). To be able to compare our results with studies that do not assume that ill-formed words have been preidentified (Chrupala, 2014; Hassan and Menezes, 2013; Han et al., 2012) we used our graph and built a dictionary to identify the ill-formed words. Following Han and Baldwin (2011) and Yang and Eisenstein (2013), we created a dictionary by choosing the nodes in our graph that have a frequency property higher than 20. Filtering this dictionary of 49657 words using GNU Aspell dictionary (v0.60.6) we produced a set of 26773 “invocabulary” (IV) words. In our second setup our system does not attemp to normalize the words in this set. 4.5 Results and Analysis In this paper we introduced a new contextual approach for text normalization. The lexical sim</context>
<context position="35059" citStr="Hassan and Menezes, 2013" startWordPosition="5904" endWordPosition="5907">e were able to improve the results obtained by Pennell and Liu (2011) on the trigram SMS-like dataset. The trigram nature of the dataset resulted in input texts which are (short thus) very limited with regard to contextual information. Nevertheless, our system achieved 72.8% F-Measure using this contextual information even though it is limited. Along the systems (presented in Table 5) that assume ill-formed tokens have been pre-identified perfectly by an oracle, there are also systems that are not based on this assumption but contain illformed word identification components (Han et al., 2012; Hassan and Menezes, 2013; Chrupala, 2014). We used the method described in Section 4.4 to identify the candidate tokens for normalization. Table 6 shows our results compared with the results of other systems that perform ill-formed word detection prior to normalization. We could label 1141 tokens correctly as ill-formed among 1184 ill-formed tokens. We achieved a word error rate (WER) of 2.6%, where Chrupala (2014) reported 4.8% and Han et al. (2012) reported 6.6% WER on the Lexnorm1.1 dataset. Method Dataset Precision Recall F-measure Han et al. (2012) LN 70.00 17.90 28.50 Hassan and Menezes (2013) LN 85.37 56.40 69</context>
<context position="36974" citStr="Hassan and Menezes, 2013" startWordPosition="6210" endWordPosition="6213">l, lexical, external, or some degree of each feature) above a threshold value. The default threshold used in the system is set equal to the maximum score that can be obtained by lexical features. Thus, we only retrieve candidates that obtain a non-zero contextual similarity score (conSimScore). The results shown at Table 7 and Table 8 demonstrate that CWAGraph can obtain even higher values of precision by increasing the percentage of contextual context of candidates. It achieved 94.1% precision on the LexNorm1.1 dataset, where the highest precision reported at the same recall level is 85.37% (Hassan and Menezes, 2013). The precision of the normalization system can be set (e.g. as high, medium, low) depending on the application where it will be used. Our motivation behind introducing the A and Q parameters was to investigate the importance 321 conSimScore &gt; Precision Recall F-measure 0 85.5 79.2 82.2 0.1 88.8 75.1 81.4 0.2 91.1 72.8 80.9 0.3 92.3 67.6 78.0 0.5 94.1 56.4 70.5 Table 7: Comparison of results for different threshold values on LexNorm1.1, the setup we have used for our other experiments is shown in bold. conSimScore &gt; Precision Recall F-measure 0 77.2 68.8 72.8 0.1 80.9 65.8 72.6 0.2 84.2 60.8 7</context>
</contexts>
<marker>Hassan, Menezes, 2013</marker>
<rawString>Hany Hassan and Arul Menezes. 2013. Social Text Normalization Using Contextual Graph Random Walks. Proceedings of the 51st Annual Meeting of the Association for Computational Linguistics, pages 1577–1586.</rawString>
</citation>
<citation valid="true">
<authors>
<author>Max Kaufmann</author>
<author>Jugal Kalita</author>
</authors>
<title>Syntactic Normalization of Twitter Messages.</title>
<date>2010</date>
<booktitle>Proceedings of the 8th International Conference on Natural Language Processing,</booktitle>
<pages>149--158</pages>
<contexts>
<context position="24273" citStr="Kaufmann and Kalita, 2010" startWordPosition="4136" endWordPosition="4139">nice A Distance: 1 c2 3 n1 w P 26 new big A A c3 c5 24388 2 Distance: 0 beautiful A 2918 n2 c4 a D smile N 305 750 beatiful A 125 o2 Distance: 0 53 best A c6 20 great A n3 c7 318 (simCost) (Contractor et al., 2010) which is defined as the ratio of the Longest Common Subsequence Ratio (LCSR) (Melamed, 1999) of two words and the Edit Distance (ED) between their skeletons (Equations 4 and 5), where the skeleton of a word is obtained by removing its vowels. LCSR(oj, ck) = LCS(oj, ck)/maxLength(oj, ck) (4) simCost(oj, ck) = LCSR(oj, ck)/ED(oj, ck) (5) Following the tradition that is inspired from (Kaufmann and Kalita, 2010), before lexical similarity calculations, any repetitions of characters three or more times in OOV tokens are reduced to two (e.g. goooood is reduced to good). Then, the edit distance, phonetic edit distance, and simCost between each candidate in CL(oi) and the OOV token oi are calculated. Edit distance and phonetic edit distance are used to filter the candidates. Any candidate in CL(oi) with an edit distance greater than tedit and phonetic edit distance greater than tphonetic to oi is removed from the candidate list CL(oi). lexSimScore(oi, ck) = simCost(oi, ck) (6) + A ∗ editScore(oi, ck) For</context>
</contexts>
<marker>Kaufmann, Kalita, 2010</marker>
<rawString>Max Kaufmann and Jugal Kalita. 2010. Syntactic Normalization of Twitter Messages. Proceedings of the 8th International Conference on Natural Language Processing, pages 149–158.</rawString>
</citation>
<citation valid="true">
<authors>
<author>Vladimir Iosifovich Levenshtein</author>
</authors>
<title>Binary Codes Capable of Correcting Deletions, Insertions and Reversals. Soviet Physics Doklady,</title>
<date>1966</date>
<pages>10--707</pages>
<contexts>
<context position="23548" citStr="Levenshtein, 1966" startWordPosition="4002" endWordPosition="4004">weight score is our primary contextual resource, we may want to favor edge weight scores. We give the frequency score a weight 0 &lt; Q &lt; 1 to be able to limit its effect on the total contextual similarity score. contSimScore(oi, ck) = EW Score(oi, ck) (3) + β ∗ freqScore(ck) Hereby, we have the candidate list CL(oi) for the OOV token oi that includes all the unique candidates in EL(oi) and their contextual similarity scores calculated. 3.4 Lexical Similarity Following the prior work in (Han and Baldwin, 2011; Hassan and Menezes, 2013), our lexical similarity features are based on edit distance (Levenshtein, 1966), double metaphone (phonetic edit distance) (Philips, 2000), and a similarity function broken A c1 nice A Distance: 1 c2 3 n1 w P 26 new big A A c3 c5 24388 2 Distance: 0 beautiful A 2918 n2 c4 a D smile N 305 750 beatiful A 125 o2 Distance: 0 53 best A c6 20 great A n3 c7 318 (simCost) (Contractor et al., 2010) which is defined as the ratio of the Longest Common Subsequence Ratio (LCSR) (Melamed, 1999) of two words and the Edit Distance (ED) between their skeletons (Equations 4 and 5), where the skeleton of a word is obtained by removing its vowels. LCSR(oj, ck) = LCS(oj, ck)/maxLength(oj, ck</context>
</contexts>
<marker>Levenshtein, 1966</marker>
<rawString>Vladimir Iosifovich Levenshtein. 1966. Binary Codes Capable of Correcting Deletions, Insertions and Reversals. Soviet Physics Doklady, 10:707.</rawString>
</citation>
<citation valid="true">
<authors>
<author>Fei Liu</author>
<author>Fuliang Weng</author>
<author>Xiao Jiang</author>
</authors>
<title>A Broad-Coverage Normalization System for Social Media Language.</title>
<date>2012</date>
<booktitle>Proceedings of the 50th Annual Meeting of the Association for Computational Linguistics: Long Papers-Volume</booktitle>
<volume>1</volume>
<pages>1035--1044</pages>
<contexts>
<context position="9850" citStr="Liu et al. (2012)" startWordPosition="1604" endWordPosition="1607">d on lexical edit distance, phonemic edit distance, prefix sub-string, suffix sub-string and longest common subsequence (LCS), as well as context support metrics. Chrupala (2014) on the other hand achieved lower word error rates without using any lexical resources. Gouws et al. (2011) investigated the distinct contributions of features that are highly depended on user-centric information such as the geological location of the users and the twitter client that the tweet is received from. Using such user-based contextual metrics they modelled the transformation distributions across populations. Liu et al. (2012) proposed a broad coverage normalization system, which integrates an extended noisy channel model, that is based on enhanced letter transformations, visual priming, string and phonetic similarity. They try to improve the performance of the top n normalization candidates by integrating human perspective modeling. Yang and Eisenstein (2013) introduced an unsupervised log linear model for text normalization. Their joint statistical approach uses local context based on language modeling and surface similarity. Along with dictionary based models, Yang and Eisenstein’s model have obtained a signific</context>
<context position="27980" citStr="Liu et al., 2012" startWordPosition="4737" endWordPosition="4740">spect to oi. candScore(oi, ck) = lexSimScore(oi, ck) + contSimScore(oi, ck) (7) + externalScore(oi, ck) 4 Experiments 4.1 Datasets We used the LexNorm1.1 (LN) dataset (Han and Baldwin, 2011) and Pennell and Liu (2014)’s trigram dataset to evaluate our proposed approach. LexNorm1.1 contains 549 tweets with 1184 manually annotated ill-formed OOV tokens. It has been used by recent text normalization studies for evaluation, which enables us to directly compare our performance results with results obtained by the recent previous work (Han and Baldwin, 2011; Pennell and Liu, 2011; Han et al., 2012; Liu et al., 2012; Hassan and Menezes, 2013; Yang and Eisenstein, 2013; Chrupala, 2014). The trigram 319 dataset is an SMS-like corpus collected from twitter status updates sent via SMS. The dataset does not include the complete tweet text but trigrams from tweets and one OOV word in each trigram is annotated. In total 4661 twitter status messages and 7769 tokens are annotated. 4.2 Graph Generation We used a large corpus of social media text to construct our word association graph. We extracted 1.5 GB of English tweets from Stanford’s 476 million Twitter Dataset (Yang and Leskovec, 2011). The language identifi</context>
<context position="30475" citStr="Liu et al., 2012" startWordPosition="5159" endWordPosition="5162">her words, IV words that are within 2 character edit distance or 1 character edit distance of a given OOV word under phonemic transcription were chosen as lexical similarity candidates. The values for the A and Q parameters in Equations 3 and 6 are set to 0.5. We did not tune these parameters for optimized performance. We selected the value of 0.5 in order to give less weight (half weight) to our minor contextual and lexical similarity features compared to the major ones. 4.4 Normalization Candidates Most of the prior work assume perfect detection of ill-formed words during test set decoding (Liu et al., 2012; Han and Baldwin, 2011; Pennell and Liu, 2011; Yang and Eisenstein, 2013). To be able to compare our results with studies that do not assume that ill-formed words have been preidentified (Chrupala, 2014; Hassan and Menezes, 2013; Han et al., 2012) we used our graph and built a dictionary to identify the ill-formed words. Following Han and Baldwin (2011) and Yang and Eisenstein (2013), we created a dictionary by choosing the nodes in our graph that have a frequency property higher than 20. Filtering this dictionary of 49657 words using GNU Aspell dictionary (v0.60.6) we produced a set of 26773</context>
<context position="33554" citStr="Liu et al. (2012)" startWordPosition="5668" endWordPosition="5671">r than the result (75.30%) reported by the original system of Han and Baldwin (2011). The results obtained by our proposed Contextual Word Association Graph (CWA-Graph) system on the LexNorm1.1 and trigram datasets, as well as the results of recent studies that used the same datasets for evaluation are presented in Table 5. The ill-formed words are assumed to have been pre-identified in advance. Method Dataset Precision Recall F-measure lexSimScore LN 28.28 28.20 28.24 externalScore LN 64.69 64.52 64.60 lexSimScore+externalScore LN 77.22 77.02 77.12 Han and Baldwin (2011) LN 75.30 75.30 75.30 Liu et al. (2012) LN 84.13 78.38 81.15 Yang and Eisenstein (2013) LN 82.09 82.09 82.09 CWA-Graph LN 85.50 79.22 82.24 lexSimScore Trigram 39.10 38.40 38.70 externalScore Trigram 44.20 43.30 43.80 lexSimScore+externalScore Trigram 65.50 64.20 64.80 Pennell and Liu (2011) Trigram 69.7 69.7 69.7 CWA-Graph Trigram 77.2 68.8 72.8 Table 5: Results obtained when ill-formed words are assumed to have been pre-identified in advance. Our CWA-Graph approach achieves the best Fmeasure (82.24%) and precision (85.50%) among the recent previous studies. The high precision value is obtained without compromising much from recal</context>
</contexts>
<marker>Liu, Weng, Jiang, 2012</marker>
<rawString>Fei Liu, Fuliang Weng, and Xiao Jiang. 2012. A Broad-Coverage Normalization System for Social Media Language. Proceedings of the 50th Annual Meeting of the Association for Computational Linguistics: Long Papers-Volume 1, pages 1035–1044.</rawString>
</citation>
<citation valid="true">
<authors>
<author>Marco Lui</author>
<author>Timothy Baldwin</author>
</authors>
<title>Langid.Py: An Off-the-shelf Language Identification Tool.</title>
<date>2012</date>
<booktitle>Proceedings of the 50th Annual Meeting of the Association for Computational Linguistics: System Demonstrations,</booktitle>
<pages>25--30</pages>
<contexts>
<context position="28671" citStr="Lui and Baldwin, 2012" startWordPosition="4852" endWordPosition="4855">. The trigram 319 dataset is an SMS-like corpus collected from twitter status updates sent via SMS. The dataset does not include the complete tweet text but trigrams from tweets and one OOV word in each trigram is annotated. In total 4661 twitter status messages and 7769 tokens are annotated. 4.2 Graph Generation We used a large corpus of social media text to construct our word association graph. We extracted 1.5 GB of English tweets from Stanford’s 476 million Twitter Dataset (Yang and Leskovec, 2011). The language identification of tweets was performed by using the langid.py Python library (Lui and Baldwin, 2012; Baldwin and Lui, 2010). CMU Ark Tagger (v0.3.2), which is a social media specific POS tagger achieving an accuracy of 95% over social media text (Owoputi et al., 2013; Gimpel et al., 2011), is used for tokenizing and POS tagging the tweets. We used the twitter tagset which includes some extra POS tags specific to social media including URLs and emoticons, Twitter hashtags (#), and twitter at-mentions (@). We made use of these social media specific tags to disambiguate some OOV tokens. After tokenization, we removed the tokens that were POS tagged as mention (e.g. @brendon), discourse marker </context>
</contexts>
<marker>Lui, Baldwin, 2012</marker>
<rawString>Marco Lui and Timothy Baldwin. 2012. Langid.Py: An Off-the-shelf Language Identification Tool. Proceedings of the 50th Annual Meeting of the Association for Computational Linguistics: System Demonstrations, pages 25–30.</rawString>
</citation>
<citation valid="true">
<authors>
<author>I Dan Melamed</author>
</authors>
<title>Bitext Maps and Alignment via Pattern Recognition.</title>
<date>1999</date>
<journal>Computational Linguistics,</journal>
<volume>25</volume>
<issue>1</issue>
<contexts>
<context position="23954" citStr="Melamed, 1999" startWordPosition="4086" endWordPosition="4087">milarity scores calculated. 3.4 Lexical Similarity Following the prior work in (Han and Baldwin, 2011; Hassan and Menezes, 2013), our lexical similarity features are based on edit distance (Levenshtein, 1966), double metaphone (phonetic edit distance) (Philips, 2000), and a similarity function broken A c1 nice A Distance: 1 c2 3 n1 w P 26 new big A A c3 c5 24388 2 Distance: 0 beautiful A 2918 n2 c4 a D smile N 305 750 beatiful A 125 o2 Distance: 0 53 best A c6 20 great A n3 c7 318 (simCost) (Contractor et al., 2010) which is defined as the ratio of the Longest Common Subsequence Ratio (LCSR) (Melamed, 1999) of two words and the Edit Distance (ED) between their skeletons (Equations 4 and 5), where the skeleton of a word is obtained by removing its vowels. LCSR(oj, ck) = LCS(oj, ck)/maxLength(oj, ck) (4) simCost(oj, ck) = LCSR(oj, ck)/ED(oj, ck) (5) Following the tradition that is inspired from (Kaufmann and Kalita, 2010), before lexical similarity calculations, any repetitions of characters three or more times in OOV tokens are reduced to two (e.g. goooood is reduced to good). Then, the edit distance, phonetic edit distance, and simCost between each candidate in CL(oi) and the OOV token oi are ca</context>
</contexts>
<marker>Melamed, 1999</marker>
<rawString>I. Dan Melamed. 1999. Bitext Maps and Alignment via Pattern Recognition. Computational Linguistics, 25(1):107–130.</rawString>
</citation>
<citation valid="true">
<authors>
<author>Olutobi Owoputi</author>
<author>Brendan O’Connor</author>
<author>Chris Dyer</author>
<author>Kevin Gimpel</author>
<author>Nathan Schneider</author>
<author>Noah A Smith</author>
</authors>
<title>Improved Part-of-Speech Tagging for Online Conversational Text with Word Clusters.</title>
<date>2013</date>
<booktitle>Proceedings of the North American Chapter of the Association for Computational Linguistics : Human Language Technologies,</booktitle>
<pages>380--390</pages>
<marker>Owoputi, O’Connor, Dyer, Gimpel, Schneider, Smith, 2013</marker>
<rawString>Olutobi Owoputi, Brendan O’Connor, Chris Dyer, Kevin Gimpel, Nathan Schneider, and Noah A. Smith. 2013. Improved Part-of-Speech Tagging for Online Conversational Text with Word Clusters. Proceedings of the North American Chapter of the Association for Computational Linguistics : Human Language Technologies, pages 380–390.</rawString>
</citation>
<citation valid="true">
<authors>
<author>Deana Pennell</author>
<author>Yang Liu</author>
</authors>
<title>A CharacterLevel Machine Translation Approach for Normalization of SMS Abbreviations.</title>
<date>2011</date>
<booktitle>Fifth International Joint Conference on Natural Language Processing,</booktitle>
<pages>974--982</pages>
<contexts>
<context position="8284" citStr="Pennell and Liu (2011)" startWordPosition="1351" endWordPosition="1354"> translation (MT) model for the text normalization task. They defined the problem as translating the SMS language to the English language and based their model on two submodels: a word based language model and a phrase based lexical mapping model (channel model). Their system also benefits from the input context and they argue that the strength of their model is in its ability to disambiguate mapping as in “2” → “two” or “to”, and “w” → “with” or “who”. Making use of the whole conversation, this is the closest approach to ours in the sense of utilizing contextual sensitivity and coverage. 314 Pennell and Liu (2011) on the other hand, proposed a character level MT system, that is robust to new abbreviations. In their two phased system, a character level trained MT model is used to produce word hypotheses and a trigram LM is used to choose a hypothesis that fits into the input context. The MT based models are supervised models, a drawback of which is that they require annotated data. Annotated training data is not readily available and is difficult to create especially for the rapidly evolving social media text (Yang and Eisenstein, 2013). More recent approaches handled the text normalization task by buil</context>
<context position="27944" citStr="Pennell and Liu, 2011" startWordPosition="4729" endWordPosition="4732">milarity score and external score with respect to oi. candScore(oi, ck) = lexSimScore(oi, ck) + contSimScore(oi, ck) (7) + externalScore(oi, ck) 4 Experiments 4.1 Datasets We used the LexNorm1.1 (LN) dataset (Han and Baldwin, 2011) and Pennell and Liu (2014)’s trigram dataset to evaluate our proposed approach. LexNorm1.1 contains 549 tweets with 1184 manually annotated ill-formed OOV tokens. It has been used by recent text normalization studies for evaluation, which enables us to directly compare our performance results with results obtained by the recent previous work (Han and Baldwin, 2011; Pennell and Liu, 2011; Han et al., 2012; Liu et al., 2012; Hassan and Menezes, 2013; Yang and Eisenstein, 2013; Chrupala, 2014). The trigram 319 dataset is an SMS-like corpus collected from twitter status updates sent via SMS. The dataset does not include the complete tweet text but trigrams from tweets and one OOV word in each trigram is annotated. In total 4661 twitter status messages and 7769 tokens are annotated. 4.2 Graph Generation We used a large corpus of social media text to construct our word association graph. We extracted 1.5 GB of English tweets from Stanford’s 476 million Twitter Dataset (Yang and Le</context>
<context position="30521" citStr="Pennell and Liu, 2011" startWordPosition="5167" endWordPosition="5170">racter edit distance or 1 character edit distance of a given OOV word under phonemic transcription were chosen as lexical similarity candidates. The values for the A and Q parameters in Equations 3 and 6 are set to 0.5. We did not tune these parameters for optimized performance. We selected the value of 0.5 in order to give less weight (half weight) to our minor contextual and lexical similarity features compared to the major ones. 4.4 Normalization Candidates Most of the prior work assume perfect detection of ill-formed words during test set decoding (Liu et al., 2012; Han and Baldwin, 2011; Pennell and Liu, 2011; Yang and Eisenstein, 2013). To be able to compare our results with studies that do not assume that ill-formed words have been preidentified (Chrupala, 2014; Hassan and Menezes, 2013; Han et al., 2012) we used our graph and built a dictionary to identify the ill-formed words. Following Han and Baldwin (2011) and Yang and Eisenstein (2013), we created a dictionary by choosing the nodes in our graph that have a frequency property higher than 20. Filtering this dictionary of 49657 words using GNU Aspell dictionary (v0.60.6) we produced a set of 26773 “invocabulary” (IV) words. In our second setu</context>
<context position="33807" citStr="Pennell and Liu (2011)" startWordPosition="5704" endWordPosition="5707"> studies that used the same datasets for evaluation are presented in Table 5. The ill-formed words are assumed to have been pre-identified in advance. Method Dataset Precision Recall F-measure lexSimScore LN 28.28 28.20 28.24 externalScore LN 64.69 64.52 64.60 lexSimScore+externalScore LN 77.22 77.02 77.12 Han and Baldwin (2011) LN 75.30 75.30 75.30 Liu et al. (2012) LN 84.13 78.38 81.15 Yang and Eisenstein (2013) LN 82.09 82.09 82.09 CWA-Graph LN 85.50 79.22 82.24 lexSimScore Trigram 39.10 38.40 38.70 externalScore Trigram 44.20 43.30 43.80 lexSimScore+externalScore Trigram 65.50 64.20 64.80 Pennell and Liu (2011) Trigram 69.7 69.7 69.7 CWA-Graph Trigram 77.2 68.8 72.8 Table 5: Results obtained when ill-formed words are assumed to have been pre-identified in advance. Our CWA-Graph approach achieves the best Fmeasure (82.24%) and precision (85.50%) among the recent previous studies. The high precision value is obtained without compromising much from recall (79.22%). Our recall is the second best among others. The F-score (82.09%) obtained by Yang and Eisenstein (2013)’s system is close to ours and the second best F-score, which on the other hand, has a lower precision. Without any modification to our sy</context>
<context position="35930" citStr="Pennell and Liu, 2011" startWordPosition="6044" endWordPosition="6047"> could label 1141 tokens correctly as ill-formed among 1184 ill-formed tokens. We achieved a word error rate (WER) of 2.6%, where Chrupala (2014) reported 4.8% and Han et al. (2012) reported 6.6% WER on the Lexnorm1.1 dataset. Method Dataset Precision Recall F-measure Han et al. (2012) LN 70.00 17.90 28.50 Hassan and Menezes (2013) LN 85.37 56.40 69.93 CWA-Graph LN 85.87 76.52 80.92 Table 6: Results obtained without assuming that ill-formed words have been pre-identified. As shown in Table 5 some systems have equal precision and recall values (Yang and Eisenstein, 2013; Han and Baldwin, 2011; Pennell and Liu, 2011). Those systems normalize all ill-formed words. On the other hand, our system does not return a normalization, if there are no candidates that are lexically similar, grammatically correct, and contextually close enough. For this reason, we managed to achieve a higher precision compared to the other systems. Our system returns a normalization candidate for an OOV word only if it achieves a similarity score (contextual, lexical, external, or some degree of each feature) above a threshold value. The default threshold used in the system is set equal to the maximum score that can be obtained by lex</context>
</contexts>
<marker>Pennell, Liu, 2011</marker>
<rawString>Deana Pennell and Yang Liu. 2011. A CharacterLevel Machine Translation Approach for Normalization of SMS Abbreviations. Fifth International Joint Conference on Natural Language Processing, pages 974–982.</rawString>
</citation>
<citation valid="true">
<authors>
<author>Deana Pennell</author>
<author>Yang Liu</author>
</authors>
<title>Normalization of informal text.</title>
<date>2014</date>
<journal>Computer Speech &amp; Language,</journal>
<volume>28</volume>
<issue>1</issue>
<pages>277</pages>
<contexts>
<context position="27581" citStr="Pennell and Liu (2014)" startWordPosition="4672" endWordPosition="4675"> contextually more similar candidates may be returned as the correct normalization instead of the candidate found equivalent to the OOV word in the slang dictionary (or in the transliteration table). 3.6 Overall Scoring As shown in Equation 7, the final score of a candidate IV token ck for an OOV token oi is the sum of its lexical similarity score, contextual similarity score and external score with respect to oi. candScore(oi, ck) = lexSimScore(oi, ck) + contSimScore(oi, ck) (7) + externalScore(oi, ck) 4 Experiments 4.1 Datasets We used the LexNorm1.1 (LN) dataset (Han and Baldwin, 2011) and Pennell and Liu (2014)’s trigram dataset to evaluate our proposed approach. LexNorm1.1 contains 549 tweets with 1184 manually annotated ill-formed OOV tokens. It has been used by recent text normalization studies for evaluation, which enables us to directly compare our performance results with results obtained by the recent previous work (Han and Baldwin, 2011; Pennell and Liu, 2011; Han et al., 2012; Liu et al., 2012; Hassan and Menezes, 2013; Yang and Eisenstein, 2013; Chrupala, 2014). The trigram 319 dataset is an SMS-like corpus collected from twitter status updates sent via SMS. The dataset does not include th</context>
</contexts>
<marker>Pennell, Liu, 2014</marker>
<rawString>Deana Pennell and Yang Liu. 2014. Normalization of informal text. Computer Speech &amp; Language, 28(1):256 – 277.</rawString>
</citation>
<citation valid="true">
<authors>
<author>Lawrence Philips</author>
</authors>
<title>The Double Metaphone Search Algorithm.</title>
<date>2000</date>
<journal>C/C++ Users Journal,</journal>
<volume>18</volume>
<issue>6</issue>
<contexts>
<context position="23607" citStr="Philips, 2000" startWordPosition="4010" endWordPosition="4011"> favor edge weight scores. We give the frequency score a weight 0 &lt; Q &lt; 1 to be able to limit its effect on the total contextual similarity score. contSimScore(oi, ck) = EW Score(oi, ck) (3) + β ∗ freqScore(ck) Hereby, we have the candidate list CL(oi) for the OOV token oi that includes all the unique candidates in EL(oi) and their contextual similarity scores calculated. 3.4 Lexical Similarity Following the prior work in (Han and Baldwin, 2011; Hassan and Menezes, 2013), our lexical similarity features are based on edit distance (Levenshtein, 1966), double metaphone (phonetic edit distance) (Philips, 2000), and a similarity function broken A c1 nice A Distance: 1 c2 3 n1 w P 26 new big A A c3 c5 24388 2 Distance: 0 beautiful A 2918 n2 c4 a D smile N 305 750 beatiful A 125 o2 Distance: 0 53 best A c6 20 great A n3 c7 318 (simCost) (Contractor et al., 2010) which is defined as the ratio of the Longest Common Subsequence Ratio (LCSR) (Melamed, 1999) of two words and the Edit Distance (ED) between their skeletons (Equations 4 and 5), where the skeleton of a word is obtained by removing its vowels. LCSR(oj, ck) = LCS(oj, ck)/maxLength(oj, ck) (4) simCost(oj, ck) = LCSR(oj, ck)/ED(oj, ck) (5) Followi</context>
</contexts>
<marker>Philips, 2000</marker>
<rawString>Lawrence Philips. 2000. The Double Metaphone Search Algorithm. C/C++ Users Journal, 18(6):38–43, June.</rawString>
</citation>
<citation valid="true">
<authors>
<author>Alan Ritter</author>
<author>Colin Cherry</author>
<author>Bill Dolan</author>
</authors>
<title>Unsupervised modeling of twitter conversations. Human Language Technologies: The</title>
<date>2010</date>
<booktitle>Annual Conference of the North American Chapter of the Association for Computational Linguistics,</booktitle>
<pages>172--180</pages>
<contexts>
<context position="2638" citStr="Ritter et al., 2010" startWordPosition="402" endWordPosition="405">racies of the Speech-to-Text (STT) tools in mobile devices, which are increasingly being used to post messages on social media platforms, along with the scarcity of attention of the users result in additional divergence of social text from more standard text such as from the newswire domain. Tools such as spellchecker and slang dictionaries have been shown to be insufficient to cope with this challenge long time ago (Sproat et al., 2001). In addition, most Natural Language Processing (NLP) tools including named entity recognizers and dependency parsers generally perform poorly on social text (Ritter et al., 2010). Text normalization is a preprocessing step to restore non-standard words in text to their original (canonical) forms to make use in NLP applications or more broadly to understand the digitized text better (Han and Baldwin, 2011). For example, talk 2 u later can be normalized as talk to you later or similarly enormoooos, enrmss and enourmos can be normalized as enormous. Other examples of text messages from Twitter and their corresponding normalized forms are shown in Table 1. The non-standard words in text are referred to as Out of Vocabulary (OOV) words. The normalization task restores the </context>
</contexts>
<marker>Ritter, Cherry, Dolan, 2010</marker>
<rawString>Alan Ritter, Colin Cherry, and Bill Dolan. 2010. Unsupervised modeling of twitter conversations. Human Language Technologies: The 2010 Annual Conference of the North American Chapter of the Association for Computational Linguistics, pages 172–180.</rawString>
</citation>
<citation valid="true">
<authors>
<author>Richard Sproat</author>
<author>Alan W Black</author>
<author>Stanley Chen</author>
<author>Shankar Kumar</author>
<author>Mari Ostendorf</author>
<author>Christopher Richards</author>
</authors>
<date>2001</date>
<journal>Normalization of Non-Standard Words. Computer Speech &amp; Language,</journal>
<volume>15</volume>
<issue>3</issue>
<contexts>
<context position="2459" citStr="Sproat et al., 2001" startWordPosition="376" endWordPosition="379">rgon. In addition, these informal expressions in social text usually take many different lexical forms when generated by different individuals (Eisenstein, 2013). The limited accuracies of the Speech-to-Text (STT) tools in mobile devices, which are increasingly being used to post messages on social media platforms, along with the scarcity of attention of the users result in additional divergence of social text from more standard text such as from the newswire domain. Tools such as spellchecker and slang dictionaries have been shown to be insufficient to cope with this challenge long time ago (Sproat et al., 2001). In addition, most Natural Language Processing (NLP) tools including named entity recognizers and dependency parsers generally perform poorly on social text (Ritter et al., 2010). Text normalization is a preprocessing step to restore non-standard words in text to their original (canonical) forms to make use in NLP applications or more broadly to understand the digitized text better (Han and Baldwin, 2011). For example, talk 2 u later can be normalized as talk to you later or similarly enormoooos, enrmss and enourmos can be normalized as enormous. Other examples of text messages from Twitter a</context>
</contexts>
<marker>Sproat, Black, Chen, Kumar, Ostendorf, Richards, 2001</marker>
<rawString>Richard Sproat, Alan W. Black, Stanley Chen, Shankar Kumar, Mari Ostendorf, and Christopher Richards. 2001. Normalization of Non-Standard Words. Computer Speech &amp; Language, 15(3):287–333.</rawString>
</citation>
<citation valid="true">
<authors>
<author>Kristina Toutanova</author>
<author>Robert C Moore</author>
</authors>
<title>Pronunciation Modeling for Improved Spelling Correction.</title>
<date>2002</date>
<booktitle>Proceedings of the 40th Annual Meeting on Association for Computational Linguistics,</booktitle>
<pages>144--151</pages>
<contexts>
<context position="6424" citStr="Toutanova and Moore (2002)" startWordPosition="1042" endWordPosition="1045">arge corpus of social media text, but it also makes use of the context of the OOV word in the input text message. Thus, an OOV word can be normalized to different IV words depending on the context of the input text. 2 Related Work Early work on text normalization mostly made use of the noisy channel model. The first work that had a significant performance improvement over the previous research was by Brill and Moore (2000). They proposed a novel noisy channel model for spell checking based on string to string edits. Their model depended on probabilistic modeling of sub-string transformations. Toutanova and Moore (2002) improved this approach by extending the error model with phonetic similarities over words. Their approach is based on learning rules to predict the pronunciation of a single letter in the word depending on the neighbouring letters in the word. Choudhury et al. (2007) developed a supervised Hidden Markov Model based approach for normalizing Short Message Service (SMS) texts. They proposed a word for word decoding approach and used a dictionary based method to normalize commonly used abbreviations and nonstandard usage (e.g. “howz” to “how are” or “aint” to “are not”). Cook and Stevenson (2009)</context>
</contexts>
<marker>Toutanova, Moore, 2002</marker>
<rawString>Kristina Toutanova and Robert C. Moore. 2002. Pronunciation Modeling for Improved Spelling Correction. Proceedings of the 40th Annual Meeting on Association for Computational Linguistics, pages 144– 151.</rawString>
</citation>
<citation valid="true">
<authors>
<author>Yi Yang</author>
<author>Jacob Eisenstein</author>
</authors>
<title>A Log-Linear Model for Unsupervised Text Normalization.</title>
<date>2013</date>
<booktitle>Proceedings of the Empirical Methods on Natural Language Processing,</booktitle>
<pages>61--72</pages>
<contexts>
<context position="8816" citStr="Yang and Eisenstein, 2013" startWordPosition="1444" endWordPosition="1447">ours in the sense of utilizing contextual sensitivity and coverage. 314 Pennell and Liu (2011) on the other hand, proposed a character level MT system, that is robust to new abbreviations. In their two phased system, a character level trained MT model is used to produce word hypotheses and a trigram LM is used to choose a hypothesis that fits into the input context. The MT based models are supervised models, a drawback of which is that they require annotated data. Annotated training data is not readily available and is difficult to create especially for the rapidly evolving social media text (Yang and Eisenstein, 2013). More recent approaches handled the text normalization task by building normalization lexicons. Han and Baldwin (2011) developed a two phased model, where they only consider the illformed OOV words for normalization. First, a confusion set is generated using the lexical and phonetic distance features. Later, the candidates in the confusion set are ranked using a mixture of dictionary look up, word similarity based on lexical edit distance, phonemic edit distance, prefix sub-string, suffix sub-string and longest common subsequence (LCS), as well as context support metrics. Chrupala (2014) on t</context>
<context position="10190" citStr="Yang and Eisenstein (2013)" startWordPosition="1653" endWordPosition="1656">atures that are highly depended on user-centric information such as the geological location of the users and the twitter client that the tweet is received from. Using such user-based contextual metrics they modelled the transformation distributions across populations. Liu et al. (2012) proposed a broad coverage normalization system, which integrates an extended noisy channel model, that is based on enhanced letter transformations, visual priming, string and phonetic similarity. They try to improve the performance of the top n normalization candidates by integrating human perspective modeling. Yang and Eisenstein (2013) introduced an unsupervised log linear model for text normalization. Their joint statistical approach uses local context based on language modeling and surface similarity. Along with dictionary based models, Yang and Eisenstein’s model have obtained a significant improvement on the performance of text normalization systems. Another relevant study is conducted by Hassan and Menezes (2013), who generated a normalization lexicon using Markov random walks on a contextual similarity lattice that they created using 5- gram sequences of words. The best normalization candidates are chosen using the av</context>
<context position="28033" citStr="Yang and Eisenstein, 2013" startWordPosition="4745" endWordPosition="4748">(oi, ck) + contSimScore(oi, ck) (7) + externalScore(oi, ck) 4 Experiments 4.1 Datasets We used the LexNorm1.1 (LN) dataset (Han and Baldwin, 2011) and Pennell and Liu (2014)’s trigram dataset to evaluate our proposed approach. LexNorm1.1 contains 549 tweets with 1184 manually annotated ill-formed OOV tokens. It has been used by recent text normalization studies for evaluation, which enables us to directly compare our performance results with results obtained by the recent previous work (Han and Baldwin, 2011; Pennell and Liu, 2011; Han et al., 2012; Liu et al., 2012; Hassan and Menezes, 2013; Yang and Eisenstein, 2013; Chrupala, 2014). The trigram 319 dataset is an SMS-like corpus collected from twitter status updates sent via SMS. The dataset does not include the complete tweet text but trigrams from tweets and one OOV word in each trigram is annotated. In total 4661 twitter status messages and 7769 tokens are annotated. 4.2 Graph Generation We used a large corpus of social media text to construct our word association graph. We extracted 1.5 GB of English tweets from Stanford’s 476 million Twitter Dataset (Yang and Leskovec, 2011). The language identification of tweets was performed by using the langid.py</context>
<context position="30549" citStr="Yang and Eisenstein, 2013" startWordPosition="5171" endWordPosition="5174"> 1 character edit distance of a given OOV word under phonemic transcription were chosen as lexical similarity candidates. The values for the A and Q parameters in Equations 3 and 6 are set to 0.5. We did not tune these parameters for optimized performance. We selected the value of 0.5 in order to give less weight (half weight) to our minor contextual and lexical similarity features compared to the major ones. 4.4 Normalization Candidates Most of the prior work assume perfect detection of ill-formed words during test set decoding (Liu et al., 2012; Han and Baldwin, 2011; Pennell and Liu, 2011; Yang and Eisenstein, 2013). To be able to compare our results with studies that do not assume that ill-formed words have been preidentified (Chrupala, 2014; Hassan and Menezes, 2013; Han et al., 2012) we used our graph and built a dictionary to identify the ill-formed words. Following Han and Baldwin (2011) and Yang and Eisenstein (2013), we created a dictionary by choosing the nodes in our graph that have a frequency property higher than 20. Filtering this dictionary of 49657 words using GNU Aspell dictionary (v0.60.6) we produced a set of 26773 “invocabulary” (IV) words. In our second setup our system does not attemp</context>
<context position="33602" citStr="Yang and Eisenstein (2013)" startWordPosition="5676" endWordPosition="5679">the original system of Han and Baldwin (2011). The results obtained by our proposed Contextual Word Association Graph (CWA-Graph) system on the LexNorm1.1 and trigram datasets, as well as the results of recent studies that used the same datasets for evaluation are presented in Table 5. The ill-formed words are assumed to have been pre-identified in advance. Method Dataset Precision Recall F-measure lexSimScore LN 28.28 28.20 28.24 externalScore LN 64.69 64.52 64.60 lexSimScore+externalScore LN 77.22 77.02 77.12 Han and Baldwin (2011) LN 75.30 75.30 75.30 Liu et al. (2012) LN 84.13 78.38 81.15 Yang and Eisenstein (2013) LN 82.09 82.09 82.09 CWA-Graph LN 85.50 79.22 82.24 lexSimScore Trigram 39.10 38.40 38.70 externalScore Trigram 44.20 43.30 43.80 lexSimScore+externalScore Trigram 65.50 64.20 64.80 Pennell and Liu (2011) Trigram 69.7 69.7 69.7 CWA-Graph Trigram 77.2 68.8 72.8 Table 5: Results obtained when ill-formed words are assumed to have been pre-identified in advance. Our CWA-Graph approach achieves the best Fmeasure (82.24%) and precision (85.50%) among the recent previous studies. The high precision value is obtained without compromising much from recall (79.22%). Our recall is the second best among </context>
<context position="35883" citStr="Yang and Eisenstein, 2013" startWordPosition="6036" endWordPosition="6039">l-formed word detection prior to normalization. We could label 1141 tokens correctly as ill-formed among 1184 ill-formed tokens. We achieved a word error rate (WER) of 2.6%, where Chrupala (2014) reported 4.8% and Han et al. (2012) reported 6.6% WER on the Lexnorm1.1 dataset. Method Dataset Precision Recall F-measure Han et al. (2012) LN 70.00 17.90 28.50 Hassan and Menezes (2013) LN 85.37 56.40 69.93 CWA-Graph LN 85.87 76.52 80.92 Table 6: Results obtained without assuming that ill-formed words have been pre-identified. As shown in Table 5 some systems have equal precision and recall values (Yang and Eisenstein, 2013; Han and Baldwin, 2011; Pennell and Liu, 2011). Those systems normalize all ill-formed words. On the other hand, our system does not return a normalization, if there are no candidates that are lexically similar, grammatically correct, and contextually close enough. For this reason, we managed to achieve a higher precision compared to the other systems. Our system returns a normalization candidate for an OOV word only if it achieves a similarity score (contextual, lexical, external, or some degree of each feature) above a threshold value. The default threshold used in the system is set equal t</context>
</contexts>
<marker>Yang, Eisenstein, 2013</marker>
<rawString>Yi Yang and Jacob Eisenstein. 2013. A Log-Linear Model for Unsupervised Text Normalization. Proceedings of the Empirical Methods on Natural Language Processing, pages 61–72.</rawString>
</citation>
<citation valid="true">
<authors>
<author>Jaewon Yang</author>
<author>Jure Leskovec</author>
</authors>
<title>Patterns of Temporal Variation in Online Media.</title>
<date>2011</date>
<booktitle>Proceedings of the Forth International Conference on Web Search and Web Data Mining,</booktitle>
<pages>177--186</pages>
<contexts>
<context position="28557" citStr="Yang and Leskovec, 2011" startWordPosition="4834" endWordPosition="4837">d Liu, 2011; Han et al., 2012; Liu et al., 2012; Hassan and Menezes, 2013; Yang and Eisenstein, 2013; Chrupala, 2014). The trigram 319 dataset is an SMS-like corpus collected from twitter status updates sent via SMS. The dataset does not include the complete tweet text but trigrams from tweets and one OOV word in each trigram is annotated. In total 4661 twitter status messages and 7769 tokens are annotated. 4.2 Graph Generation We used a large corpus of social media text to construct our word association graph. We extracted 1.5 GB of English tweets from Stanford’s 476 million Twitter Dataset (Yang and Leskovec, 2011). The language identification of tweets was performed by using the langid.py Python library (Lui and Baldwin, 2012; Baldwin and Lui, 2010). CMU Ark Tagger (v0.3.2), which is a social media specific POS tagger achieving an accuracy of 95% over social media text (Owoputi et al., 2013; Gimpel et al., 2011), is used for tokenizing and POS tagging the tweets. We used the twitter tagset which includes some extra POS tags specific to social media including URLs and emoticons, Twitter hashtags (#), and twitter at-mentions (@). We made use of these social media specific tags to disambiguate some OOV to</context>
</contexts>
<marker>Yang, Leskovec, 2011</marker>
<rawString>Jaewon Yang and Jure Leskovec. 2011. Patterns of Temporal Variation in Online Media. Proceedings of the Forth International Conference on Web Search and Web Data Mining, pages 177–186.</rawString>
</citation>
</citationList>
</algorithm>
</algorithms>
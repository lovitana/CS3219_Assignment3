<?xml version="1.0" encoding="UTF-8"?>
<algorithms version="110505">
<algorithm name="SectLabel" version="110505">
<variant no="0" confidence="0.000000">
<title confidence="0.993299">
An Iterative Link-based Method for Parallel Web Page Mining
</title>
<author confidence="0.998748">
Le Liu1, Yu Hong1, Jun Lu2, Jun Lang2, Heng Ji3, Jianmin Yao1
</author>
<affiliation confidence="0.906547333333333">
1School of Computer Science &amp; Technology, Soochow University, Suzhou, 215006, China
2Institute for Infocomm Research, Singapore, 138632
3Computer Science Department, Rensselaer Polytechnic Institute, Troy, NY 12180, USA
</affiliation>
<email confidence="0.463855">
giden@sina.cn,{tianxianer,lujun59,billlangjun}@gmail.com
jih@rpi.edu,jyao@suda.edu.cn
</email>
<sectionHeader confidence="0.45031" genericHeader="abstract">
Abstracts
</sectionHeader>
<bodyText confidence="0.99976934375">
Identifying parallel web pages from bi-
lingual web sites is a crucial step of bi-
lingual resource construction for cross-
lingual information processing. In this
paper, we propose a link-based approach
to distinguish parallel web pages from bi-
lingual web sites. Compared with the ex-
isting methods, which only employ the
internal translation similarity (such as
content-based similarity and page struc-
tural similarity), we hypothesize that the
external translation similarity is an effec-
tive feature to identify parallel web pages.
Within a bilingual web site, web pages
are interconnected by hyperlinks. The
basic idea of our method is that the trans-
lation similarity of two pages can be in-
ferred from their neighbor pages, which
can be adopted as an important source of
external similarity. Thus, the translation
similarity of page pairs will influence
each other. An iterative algorithm is de-
veloped to estimate the external transla-
tion similarity and the final translation
similarity. Both internal and external
similarity measures are combined in the
iterative algorithm. Experiments on six
bilingual websites demonstrate that our
method is effective and obtains signifi-
cant improvement (6.2% F-Score) over
the baseline which only utilizes internal
translation similarity.
</bodyText>
<sectionHeader confidence="0.992528" genericHeader="introduction">
1 Introduction
</sectionHeader>
<bodyText confidence="0.988512632653061">
Parallel corpora have played an important role in
multilingual Natural Language Processing, espe-
cially in Machine Translation (MT) and Cross-
lingual Information Retrieval(CLIR). However,
it’s time-consuming to build parallel corpora
manually. Some existing parallel corpora are
subject to subscription or license fee and thus not
freely available, while others are domain-specific.
Therefore, a lot of previous research has focused
on automatically mining parallel corpora from
the web.
In the past decade, there have been extensive
studies on parallel resource extraction from the
web (e.g., Chen and Nie, 2000; Resnik 2003;
Jiang et al., 2009) and many effective Web min-
ing systems have been developed such as
STRAND, PTMiner, BITS and WPDE. For most
of these mining systems, there is a typical paral-
lel resource mining strategy which involves three
steps: (1) locate the bilingual websites (2) identi-
fy parallel web pages from these bilingual web-
sites and (3) extract bilingual resources from the
parallel web pages.
In this paper, we focus on the step (2) which is
regarded as the core of the mining system
(Chunyu, 2007). Estimating the translation simi-
larity of two pages is the most basic and key
problem in this step. Previous approaches have
tried to tackle this problem by using the infor-
mation within the pages. For example, in the
STRAND and PTMiner system, a structural fil-
tering process that relies on the analysis of the
underlying HTML structure of pages is used to
determine a set of pair-specific structural values,
and then the values are used to decide whether
the pages are translations of one another. The
BITS system filters out bad pairs by using a large
bilingual dictionary to compute a content-based
similarity score and comparing the score with a
threshold. The WPDE system combines URL
similarity, structure similarity with content-based
similarity to discover and verify candidate paral-
lel page pairs. Some other features or rules such
as page size ratio, predefined hypertexts which
link to different language versions of a web page
are also used in most of these systems. Here, all
of the mining systems are simply using the in-
formation within the page in the process of find-
1216
</bodyText>
<note confidence="0.974688">
Proceedings of the 2014 Conference on Empirical Methods in Natural Language Processing (EMNLP), pages 1216–1224,
October 25-29, 2014, Doha, Qatar. c�2014 Association for Computational Linguistics
</note>
<bodyText confidence="0.988024484848485">
ing parallel web pages. In this paper, we attempt
to explore other information to identify parallel
web pages.
On the Internet, most web pages are linked by
hyperlinks. We argue that the translation similar-
ity of two pages depends on not only their inter-
nal information but also their neighbors. The
neighbors of a web page are a set of pages,
which link to the page. We find that the similari-
ty of neighbors can provide more reliable evi-
dence in estimating the translation similarity of
two pages.
The main issues are discussed in this paper as
follows:
 Can the neighbors of candidate page pairs
really contribute to estimating the translation
similarity?
 How to estimate the translation similarity of
candidate page pairs by using their neighbors?
Our method has the following advantages:
High performance
The external and internal information is com-
bined to verify parallel page pairs in our method,
while in previous mining systems, only internal
information was used. Experimental results show
that compared with existing parallel page pair
identification technologies, our method obtains
both higher precision and recall (6.2% and 6.3%
improvement than the baseline, respectively). In
addition, the external information used in our
method is a more effective feature than internal
features alone such as structural similarity and
content-based similarity.
</bodyText>
<subsectionHeader confidence="0.853299">
Language independent
</subsectionHeader>
<bodyText confidence="0.9999714">
In principle, our method is language inde-
pendent and can be easily ported to new lan-
guage pairs, except for the language-specific bi-
lingual lexicons. Our method takes full ad-
vantage of the link information that is language-
independent. For the bilingual lexicons in our
experiments, compared to previous methods, our
method does not need a big bilingual lexicon,
which is good news to less-resource language
pairs.
</bodyText>
<subsectionHeader confidence="0.625826">
Unsupervised and fewer parameters
</subsectionHeader>
<bodyText confidence="0.999877461538462">
In previous work, some parameters need to be
optimized. Due to the diversity of web page
styles, it is not trivial to obtain the best parame-
ters. Some previous researches(Resnik, 2003;
Zhang et al., 2006) attempt to optimize parame-
ters by employing machine learning method. In
contrast, in our method, only two parameters
need to be estimated. One parameter remains
stable for different style websites. Another pa-
rameter can be easily adjusted to achieve the best
performance. Therefore, our method can be used
in other websites with different styles, without
much effort to optimize these parameters.
</bodyText>
<sectionHeader confidence="0.999443" genericHeader="related work">
2 Related Work
</sectionHeader>
<bodyText confidence="0.9999746">
A large amount of literature has been published
on parallel resource mining from the web. Ac-
cording to the existing form of the parallel re-
source on the Internet, related work can be cate-
gorized as follows:
</bodyText>
<subsectionHeader confidence="0.761171">
Mining from bilingual websites
</subsectionHeader>
<bodyText confidence="0.994616282051282">
Most existing web mining systems aimed at
mining bilingual resource from the bilingual
websites, such as PTMiner (Nie et al., 1999),
STRAND (Resnik and Smith, 2003), BITS (Ma
and Liberman, 1999), PTI (Chen et al., 2004).
PTMiner uses search engines to pinpoint the
candidate sites that are likely to contain parallel
pages, and then uses the collected URLs as seeds
to further crawl each web site for more URLs.
Web page pairs are extracted based on manually
defined URL pattern matching, and further fil-
tered according to several criteria. STRAND us-
es a search engine to search for multilingual
websites and generated candidate page pairs
based on manually created substitution rules.
Then, it filters some candidate pairs by analyzing
the HTML pages. PTI crawls the web to fetch
(potentially parallel) candidate multilingual web
documents by using a web spider. To determine
the parallelism between potential document pairs,
a filename comparison module is used to check
filename resemblance, and a content analysis
module is used to measure the semantic similari-
ty. BITS was the first to obtain bilingual web-
sites by employing a language identification
module, and then for each bilingual website, it
extracts parallel pages based on their content.
Mining from bilingual web pages
Parallel/bilingual resources may exist not only
in two parallel monolingual web pages, but also
in single bilingual web pages. Jiang et al. (2009)
used an adaptive pattern-based method to mine
interesting bilingual data based on the observa-
tion that bilingual data usually appears collec-
tively following similar patterns. They found that
bilingual web pages are a promising source of
up-to-date bilingual terms/sentences which cover
many domains and application scenarios. In ad-
dition, Feng et al. (2010) proposed a new method
</bodyText>
<page confidence="0.551014">
1217
</page>
<bodyText confidence="0.999749">
to automatically acquire bilingual web pages
from the result pages of a search engine.
</bodyText>
<subsectionHeader confidence="0.699193">
Mining from comparable corpus
</subsectionHeader>
<bodyText confidence="0.999772285714286">
Several attempts have been made to extract
parallel resources from comparable corpora.
Zhao et al. (2002) proposed a robust, adaptive
approach for mining parallel sentences from a
bilingual comparable news collection. In their
method, sentence length models and lexicon-
based models were combined under a maximum
likelihood criterion. Smith et al. (2010) found
that Wikipedia contains a lot of comparable doc-
uments, and adopted a ranking model to select
parallel sentence pairs from comparable docu-
ments. Bharadwaj et al. (2011) used a SVM clas-
sifier with some new features to identify parallel
sentences from Wikipedia.
</bodyText>
<sectionHeader confidence="0.973107" genericHeader="method">
3 Iterative Link-based Parallel Web
</sectionHeader>
<subsectionHeader confidence="0.953643">
Pages Mining
</subsectionHeader>
<bodyText confidence="0.932963666666667">
As mentioned, the basic idea of our method is
that the similarity of two pages can be inferred
from their neighbors. This idea is illustrated in
</bodyText>
<figureCaption confidence="0.9523155">
Figure 1.
Figure 1 Illustration of the link-based method
</figureCaption>
<bodyText confidence="0.999993222222222">
In Figure 1, A, B, C, D and E are some pages
in the same language; while A’, B’, C’, D’ and E’
are some pages in another language. The solid
black arrows indicate the links between these
pages. For example, page A points to C, page B’
points to C’ and so on. Then the page set {A, B,
D, E} is called the neighbors of page C. Similar-
ly, the page set {A’, B’, D’, E’} contains the
neighbors of page C’. If the page pairs : &lt;A, A’&gt;,
&lt;B, B’&gt;, &lt;D, D’&gt; and &lt;E, E’&gt; have high transla-
tion similarities, then it can be inferred that page
C and C’ have a high probability to be a pair of
parallel pages. Every page has its own neighbors.
For each web page, our method views link-in and
link-out hyperlinks as the same. Thus, the linked
pages will influence each other in estimating the
translation similarity. For example, the similari-
ties of two pairs &lt;A, A’&gt; and &lt;C, C’&gt; will influ-
ence each other. It is an iterative process. We
will elaborate the process in the following sec-
tions.
Since our goal is to find parallel pages in a
specific website, the key task is to evaluate the
translation similarity of two pages (which are in
different languages) as accurately as possible.
The final similarity of two pages should depend
both on their internal similarity and external sim-
ilarity. The internal similarity means the similari-
ty estimated by using the information in the page
itself, such as the structure similarity and the
content-based similarity of the two pages. On the
other hand, the external similarity of two pages is
the similarity depending on their neighbors. The
final translation similarity is called the En-
hanced Translation Similarity (ETS). The ETS
of two pages can be calculated as follows:
</bodyText>
<equation confidence="0.994079666666667">
ETS ( e, c) = a • Sext( e, c) + ( 1 — a) •
S ( e,c) ,a [ 0,1] (1)
i .
</equation>
<bodyText confidence="0.966531875">
Where, Si.( e, c) is the internal translation simi-
larity of two pages: e and c; Sext ( e, c) represents
the external translation similarity of pages e and
c. ETS( e, c) indicates the final similarity of two
pages, which combines the internal with external
translation similarity.
In this paper, we conduct the experiments on
English-Chinese parallel page pair mining. How-
ever, our method is language-independent. Thus,
it can be applied to other language pairs by only
replacing a bilingual lexicon. The symbol e and c
always indicate an English page and a Chinese
page respectively in this paper. In the following
sections, we will describe how to calculate the
S ( e,c) and S xt( e,c) step by step.
i.
</bodyText>
<subsectionHeader confidence="0.997804">
3.1 Preprocessing
</subsectionHeader>
<bodyText confidence="0.999993">
The input of our method is a bilingual website.
This paper aims to find English/Chinese parallel
pages. So a 3-gram language model is used to
identify (or classify) the language of a certain
document. The performance of the language
identification module achieves 99.5% accuracy
through in-house testing. As a result, a set of
English pages and a set of Chinese pages are ob-
tained. In order to get the neighbors of a page,
for each bilingual website, two networks are con-
structed based on the hyperlinks, one for English
pages and another for Chinese pages.
</bodyText>
<subsectionHeader confidence="0.998096">
3.2 The Internal Translation Similarity
</subsectionHeader>
<bodyText confidence="0.999729">
Following Resnik and Smith (2003), three fea-
tures are used to evaluate the internal translation
similarity of two pages:
</bodyText>
<figure confidence="0.9932214">
A D
A’ D’
B
B’
?
C
C’
E
E’
1218
</figure>
<bodyText confidence="0.995786891891892">
The size ratio of two pages
The length ratio of two documents is the sim-
plest criterion for determining whether two doc-
uments are parallel or not. Parallel documents
tend to be similar in length. And it is reasonable
to assume that for text E in one language and text
F in another language, length(E) z C•length(F),
where C is a constant that depends on the lan-
guage pair. Here, the content length of a web
page is regarded as its length.
The structure similarity of two pages
The HTML tags describe and control a web
page’s structure. Therefore, the structure similar-
ity of two pages can be calculated by their
HTML tags. Here, the HTML tags of each page
are extracted (except the visual tags such as “B”,
“FONT”.) as a linear sequence. Then the struc-
ture similarity of two pages is computed by com-
paring their linearized sequences. In this paper,
the LCS algorithm (Dan, 1997) is adopted to find
the longest common sequences of the two HTML
tag sequences. The ratio of LCS length and the
average length of two HTML tag sequences are
used as the structure similarity of the two pages.
The content-based translation similarity of
two pages
The basic idea is that if two documents are
parallel, they will contain word pairs that are mu-
tual translations (Ma, 1999). So the percentage of
translation word pairs in the two pages can be
considered as the content-based similarity. The
translation words of two documents can be ex-
tracted by using a bilingual lexicon. Here, for
each word in English document, we will try to
find a corresponding word in Chinese document.
Finally, the internal translation similarity of
two pages is calculated as follows:
</bodyText>
<equation confidence="0.9990805">
S ( e,c) =fl•S b( e,c) +( fl) •
i
S ( e,c) ,fl [ 0, ] (2)
t t
</equation>
<bodyText confidence="0.9520385">
Where, Scb ( e, c) and Sstruct ( e, c) are the con-
tent-based and structural similarity of page a and
c respectively. In addition, the size ratio of two
pages is used to filter invalid page pairs.
</bodyText>
<subsectionHeader confidence="0.990963">
3.3 The External and Enhanced Transla-
tion Similarity
</subsectionHeader>
<bodyText confidence="0.999153">
As described above, the external translation
similarity of two pages depends on their neigh-
bors:
</bodyText>
<equation confidence="0.99668">
( e, c) = Sim(PG ( e) , PG ( c)) (3)
</equation>
<bodyText confidence="0.9034233125">
Where, PG(x), a set of pages, is the neighbors of
page x. Obviously, the similarity of two sets re-
lies on the similarity of the elements in the two
sets. Here, the elements are namely web pages.
So, Sext( e, c) equals to Sim( PG( e) , PG( c)), and
Sim(PG(e),PG(c)) depends on ETS( ei,cj)
( ei, cj belongs to PG ( e) , PG( c) , respectively)
and ETS( e, c) . According to Equation (1),
ETS( e, c) depends on Sin e, c) and Sext( e, c) .
Therefore, it is a process of iteration. ETS( e, c)
will converge after a certain number of iterations.
Thus, ETSi(e,c) is defined as the enhanced
similarity of page e and c after the i-th iteration,
and the same is for Sext( e, c) and Simi ( PG ( e),
PG( c)) . Simi( PG( e) , PG( c)) is computed by
the following algorithm:
</bodyText>
<figure confidence="0.8828312">
Algorithm 1: Estimating the external transla-
tion similarity
Input: PG(e),PG(c)
Output: Sext( e, c)
Procedure:
sumF 0
e_setF PG( e)
c_set F PG( c)
While e_set and c_set are both not empty:
&lt;x,y&gt;
F arg maxxce_set , (ETSi ( , ))
_ t
sum F sum + ETSi-1(x, y)
Remove x from e_set
Remove y from c_setSi ( e,c) =S mi( ( e) , ( c))
xt
= 2 • sum / ( lPG( e) l + lPG( c) l)
Algorithm 2 Estimating the enhanced transla-
tion similarity
Input: Pe, Pc, (the English and Chinese page set)
Output: ETS(e, c), e E Pe, c E Pc
Initialization: Set ETS(e, c) random value or
small value
Procedure:
LOOP:
</figure>
<equation confidence="0.795075285714286">
For each e in Pe :
For each c in Pc:
i
ETS ( e,c) = a • Sext( e, c)
+ ( 1 - a) •Si ( e, c)
Parameters normalization
UNTIL ETS( e, c) is stable
</equation>
<bodyText confidence="0.9424915">
Algorithm 1 tries to find the real parallel pairs
from PG( e) and PG( c). The similarity of PG( e)
and PG( c) is calculated based on the similarity
1219
values of these pairs. Finally, ETS(e, c) is calcu-
lated by the following algorithm 2.
In Algorithm 2, the input Pe and Pc are English
and Chinese page sets in a certain bilingual web-
site. We use algorithm 2 to estimate the en-
hanced translation similarity.
</bodyText>
<subsectionHeader confidence="0.957528">
3.4 Find the Parallel Page Pairs
</subsectionHeader>
<bodyText confidence="0.999596333333333">
At last, the enhanced translation similarity of
every pair is obtained, and the parallel page pairs
can be extracted in terms of these similarities:
</bodyText>
<figure confidence="0.842194307692308">
Algorithm 3 Finding parallel page pairs
Input:
ETS(x, y) , x E Pe, y E P,
MAX_P (or MIN _SIM)
Output: Parallel Page Pairs List :
Procedure:
LOOP:
(ETS(x, y))
Add &lt; x,y &gt; to PPL
Remove x from Pe
Remove y from Pc
UNTIL size of PPL &gt; MAX_P (or ETS(x, y) &lt;
MIN_SIM)
</figure>
<bodyText confidence="0.99909">
This algorithm is similar to Algorithm 1 in
each bilingual website. The input MAX_P is an
integer threshold which means that only top
MAX_P page pairs will be extracted in a certain
website. It needs to be noted that MAX_P is al-
ways less than I Pe I and I Pc j . While the input
MIN _SIM is another kind of threshold that is
used for extracting page pairs with high transla-
tion similarity.
</bodyText>
<sectionHeader confidence="0.985818" genericHeader="evaluation">
4 Experiments and Analysis
</sectionHeader>
<subsectionHeader confidence="0.965841">
4.1 Experimental setup
</subsectionHeader>
<bodyText confidence="0.999892583333333">
Our experiments focus on six bilingual websites.
Most of them are selected from HK government
websites. All the web pages were retrieved by
using a web site download tool: HTTrack1. We
notice that a small amount of pages doesn’t al-
ways contain valuable contents. So, we put a
threshold (100 bytes in our experiment) on the
web pages&apos; content to filter meaningless pages. In
order to evaluate our method, the bilingual page
pairs of each website are annotated by a human
annotator. Finally, we got 23109 pages and
11684 bilingual page pairs in total for testing.
</bodyText>
<equation confidence="0.603852">
1 http://www.httrack.com/
</equation>
<bodyText confidence="0.9995818125">
The basic information of these websites is listed
in Table 1.
It’s time-consuming to annotate whether two
pages is parallel or not. Note that if a website
contains I English pages and M Chinese pages,
an annotator has to label I*M page pairs. To the
best of our knowledge, there is no large scale and
public parallel page pair dataset with human an-
notation. So we try to build a reliable and large-
scale dataset.
In our experiments, URL similarity is used to
reduce the workload for annotation. For a certain
website, firstly, we obtain its URL pattern be-
tween English and Chinese pages manually. For
example, in the website “www.gov.hk”, the URL
pairs like:
</bodyText>
<equation confidence="0.6948925">
http://www.gov.hk/en/about/govdirectory/ (English)
http://www.gov.hk/sc/about/govdirectory/ (Chinese)
</equation>
<bodyText confidence="0.999721375">
The URL pairs always point to a pair of paral-
lel pages. So &lt;”/en/”,”/sc/”&gt; is considered as a
URL pattern that was used to find parallel pages.
For the other URLs that can’t match the pattern,
we have to label them by hand. The column “No
pattern pairs” in Table 1 shows that the number
of parallel page pairs which mismatch any pat-
terns.
</bodyText>
<tableCaption confidence="0.9609085">
Table 1 Number of pages and bilingual page pairs of
each websites
</tableCaption>
<note confidence="0.945920666666667">
Site ID En/Ch pages Total No pat- URL
pairs tern pairs
S1 1101/1098 1092 20 www.gov.hk
S2 501/497 487 7 www.customs.gov.hk
S3 995/775 768 12 www.sbc.edu.sg
S4 4085/3838 3648 4 www.swd.gov.hk
S5 660/637 637 0 www.landsd.gov.hk
S6 4733/4626 4615 8 www.td.gov.hk
total 12075/11471 11684 51
</note>
<bodyText confidence="0.966598375">
Each website listed in Table 1 has a URL pat-
tern for most parallel web pages. Some previous
researches used the URL similarity or patterns to
find parallel page pairs. However, due to the di-
versity of web page styles and website mainte-
nance mechanisms, bilingual websites adopt var-
ied naming schemes for parallel documents (Shi,
et al, 2006). The effect of URL pattern-based
mining always depends on the style of website.
In order to build a large dataset, the URL pattern
is not used in our method. Our method is able to
handle bilingual websites without URL pattern
rules.
In addition, an English-Chinese dictionary
with 64K words pairs is used in our experiments.
Algorithm 3 needs a threshold MAX_P or
1220
MIN-SIM. It is very hard to tune the MIN-SIM
because it varies a lot in different websites and
language pairs. However, Table 1 shows that the
number of parallel pages is smaller than that of
English and Chinese pages. Here, for each web-
site, the MAX_P is set to the number of Chinese
pages (which is always smaller than that of Eng-
lish pages). In this way, the precision will never
reach 100%, but it is more practical in a real ap-
plication. As a result, in some experiments, we
only report the F-score, and the precision and
recall can be calculated as follows:
2-NPairs
Where, NPairs for each website is listed in the
“Total pairs” column of Table 1.
</bodyText>
<subsectionHeader confidence="0.9159725">
4.2 Results and Analysis
Performance of the Baseline
</subsectionHeader>
<bodyText confidence="0.993906875">
Let’s start by presenting the performance of a
baseline method as follows. The baseline only
employs the internal translation similarity for
parallel web pages mining. Algorithm 3 is also
used to get the page pairs in baseline system.
Here, the input ETS(x, y) is replaced by
S (x,y). The parameter fl in Equation 2 is a
,,
discount factor. For different fl values, the per-
formance of baseline system on six websites is
shown in Figure 2. In the Figure 2, it shows that
when fl is set to 0.6, the baseline system achieves
the best performance. The precision, recall and
F-score are 85.84%, 87.55% and 86.69% respec-
tively. So in the following experiments, we al-
ways set  to 0.6.
</bodyText>
<figure confidence="0.974673909090909">
95
F-score Precision Recall
90
85
80
75
70
65
60
0 0.1 0.2 0.3 0.4 0.5 0.6 0.7 0.8 0.9 1

</figure>
<figureCaption confidence="0.9894915">
Figure 2 Performances of baseline system with differ-
ent fl value
</figureCaption>
<bodyText confidence="0.9928325">
Performance of Our Method
As described in Section 3, our method com-
bines the internal with external translation simi-
larity in estimating the final translation similarity
(i.e., ETS) of two pages. So, the discount factor
a in Equation (1) is important in our method.
Besides, as shown in Algorithm 2, the iterative
algorithm is used to calculate the similarity. Then,
one question is that how many iterations are re-
quired in our algorithm. Figure 3 shows the per-
formance of our method on each website. Its hor-
izontal axis represents the number of iterations
and the vertical axis represents the F-score. And
for each website, the F-scores with different a
(range from 0.2 to 0.8) are also reported in this
figure. From Figure 3, it is very easy to find that
the best iteration number is 3. For almost all the
websites, the performance of our method
achieves the maximal values and converges after
the third iteration. In addition, Figure 3 also indi-
cates that our method is robust for different web-
sites. In the following experiments, the iteration
number is set to 3.
Next, let’s turn to the discount factor a. Figure
4 reports the experimental results on the whole
dataset. Here, the horizontal axis represents the
discount factor a and the vertical axis represents
the F-score. a = 0 means that only the internal
similarity is used in the algorithm, so the F-score
equals to that in Figure 2 when fl = 0.6. On the
contrary, a = 1 means that only the external
similarity is used in the method, and the F-score
is 80.20%. The performance is lower than the
baseline system when only the external link in-
formation is used, but it is much better than the
performance of the content-based method and
structure-based method whose F-scores are 64.82%
and 64.0% respectively. Besides, it is shown
from Figure 4, the performance is improved sig-
nificantly when the internal and external similari-
ty measures are combined together. Furthermore,
it is somewhat surprising that the discount factor
a is not important as we previously expected. In
fact, if we discard the cases that a equals to 0 or
1, the difference between the maximum and min-
imum F-score will be 0.76% which is very small.
This finding indicates that the internal and exter-
nal similarity can easily be combined and we
don’t need to make many efforts to tune this pa-
rameter when our method is applied to other
websites. The reason of this phenomenon is that,
no matter how much weight (i.e., 1- a) was as-
signed to the internal similarity, the internal sim-
ilarity always provides a relatively good initial
</bodyText>
<equation confidence="0.8687728">
2-MAX P
(NPairs + MAX) (5)
(NPairs+ MAX-P) (4)
Performance(%)
1221
</equation>
<figureCaption confidence="0.980964333333333">
Figure 3 Experiment results of our method on each website
Figure 4 The F-scores of our method with different
the value of 
</figureCaption>
<bodyText confidence="0.98724005">
The weight of pages
The weight of the neighbor pages should also
be considered. For example, in the most websites,
it is very common that most of the web pages
contain a hyperlink which points to the homep-
age of the website. While in most of the Eng-
lish/Chinese websites, almost every English page
will link to the English homepage and each Chi-
nese page will point to Chinese homepage. The
English and Chinese homepages are probably
parallel, but they will be helpless to find parallel
web pages, because they are neighbors of almost
every page in the site. On the contrary, some-
times the parallel homepages have negative ef-
fects on finding parallel pages They will increase
the translation similarity of two pages which are
not indeed mutual translations. So it is necessary
to amend the Algorithm 1.
The weight of each page is calculated accord-
ing to its popularity:
</bodyText>
<equation confidence="0.981326666666667">
N+c
w(
p) =log Freq (P) + c (6)
</equation>
<bodyText confidence="0.877982833333333">
where w(p) indicates the weight of page p, N is
the number of all pages, Freq(p) is the number
of pages pointing to page p and c is a constant
for smoothing.
In this paper, the weights of pages are used in
two ways:
Weight 1: The 9th line of Algorithm 1 is
amended by the page weight as follows:
sum  sum + ETSi-1(x, y) • ( w(x) + w(y)) /2
Weight 2: The pages with low weight are re-
moved from the input of Algorithm 1.
The experiment results are shown in Table 2.
</bodyText>
<tableCaption confidence="0.994052">
Table 2 The effect of page weight
</tableCaption>
<table confidence="0.8491395">
Type No Weight Weight 1 Weight 2
F-score (%) 92.91 92.78 92.75
</table>
<bodyText confidence="0.964830583333333">
Surprisingly, no big differences are found after
the introduction of the page weight. The side ef-
fect of popular pages is not so large in our meth-
od. In the neighbor pages of a certain page, the
popular pages are the minority. Besides, the iter-
ative process makes our method more stable and
robust.
The impact of the size of bilingual lexicon
The baseline system mainly combines the con-
tent-based similarity with structure similarity.
iterative direction. In the following experiments,
the parameter  is set to 0.6.
</bodyText>
<figure confidence="0.977311391304348">
95
92.40
92.83
92.83
92.67
92.15
F-score(%)
91
93
89
87
85
92.61
6.69
92.42
92.91
92.78
81 80.20
79
0 0.1 0.2 0.3 0.4 0.5 0.6 0.7 0.8 0.9 1

83
1222
</figure>
<bodyText confidence="0.99982212">
And two kinds of similarity measures are also
used in our method. As Ma and Liberman (1999)
pointed out, not all translators create translated
pages that look like the original page which
means that the structure similarity does not al-
ways work well. Compared to the structure simi-
larity, the content-based is more reliable and has
wider applicability. Furthermore, the bilingual
lexicon is the only information that relates to the
language pairs, and other features (such as struc-
ture and link information) are all language inde-
pendent. So, it’s important to investigate the ef-
fect of lexicon size in our method. We test the
performance of our method with different size of
the bilingual dictionary. The experiment results
are shown in Figure 5. In this figure, the horizon-
tal axis represents the bilingual lexicon size and
the vertical axis represents the F-score. With the
decline of the lexicon size, the performances of
both the baseline method and our method are
decreased. However, we can find that the descent
rate of our method is smaller than that of the
baseline. It indicates that our method does not
need a big bilingual lexicon which is good news
for the low-resource language pairs.
</bodyText>
<subsectionHeader confidence="0.95596">
Lexicon Size
</subsectionHeader>
<bodyText confidence="0.994049684210526">
Figure 5 The impact of the size of bilingual lexicon
Error analysis
Errors occur when the two pages are similar in
terms of structure, content and their neighbors.
For example, Figure 6 illustrates a typical web
page structure. There are 5 parts in the web page:
U, L, M, R and B. Part M always contains the
main content of this page. While part U, L, R and
B always contain some hyperlinks such as “home”
in part U and “About us” in part B. Links in L
and R sometimes relate to the content of the page.
For such a kind of non-parallel page pairs, let’s
assume that the two pages have the same struc-
ture (as shown in Figure 6). In addition, their
content part M is very short and contains the
same or related topics. As a result, the links in
other 4 parts are likely to be similar. In this case,
our method is likely to regard the two pages as
parallel.
</bodyText>
<figure confidence="0.995343333333333">
L U R
M
B
</figure>
<figureCaption confidence="0.995586">
Figure 6 A typical web page structure
</figureCaption>
<bodyText confidence="0.999973307692308">
There are about 920 errors when our system
obtains its best performance. By carefully inves-
tigating the error page pairs, we find that more
than 90% errors fall into the category discussed
above. The websites used in our experiments
mainly come from Hong Kong government web-
sites. Some government departments regularly
publish quarterly or monthly work reports on one
issue through their websites. These reports look
very similar except the publish date and some
data in them. The other 10% errors happen be-
cause of the particularity of the web pages, e.g.
very short pages, broken pages and so on.
</bodyText>
<sectionHeader confidence="0.99355" genericHeader="conclusions">
5 Conclusions and Future Work
</sectionHeader>
<bodyText confidence="0.999924576923077">
Parallel corpora are valuable resources for a lot
of NLP research problems and applications, such
as MT and CLIR. This paper introduces an effi-
cient and effective solution to bilingual language
processing. We first explore how to extract paral-
lel page pairs in bilingual websites with link in-
formation between web pages. Firstly, we hy-
pothesize that the translation similarity of pages
should be based on both internal and external
translation similarity. Secondly, a novel iterative
method is proposed to verify parallel page pairs.
Experimental results show that our method is
much more effective than the baseline system
with 6.2% improvement on F-Score. Further-
more, our method has some significant contribu-
tions. For example, compared to previous work,
our method does not depend on bilingual lexi-
cons, and the parameters in our method have lit-
tle effect on the final performance. These fea-
tures improve the applicability of our method.
In the future work, we will study some method
on extracting parallel resource from existing par-
allel page pairs, which are challenging tasks due
to the diversity of page structures and styles. Be-
sides, we will evaluate the effectiveness of our
mined data on MT or other applications.
</bodyText>
<figure confidence="0.992758">
Baseline Our Method
64K 32K 16K 8K 4K 2K 1K
94
92
F-score (%)
90
88
86
84
82
80
78
1223
</figure>
<sectionHeader confidence="0.987504" genericHeader="acknowledgments">
Acknowledgments
</sectionHeader>
<bodyText confidence="0.990527769230769">
This research work has been sponsored by Na-
tional Natural Science Foundation of China
(Grants No.61373097 and No.61272259), one
National Natural Science Foundation of Jiangsu
Province (Grants No.BK2011282), one Major
Project of College Natural Science Foundation of
Jiangsu Province (Grants No.11KJA520003) and
one National Science Foundation of Suzhou City
(Grants No.SH201212).
The corresponding author of this paper, ac-
cording to the meaning given to this role by
School of computer science and technology at
Soochow University, is Yu Hong
</bodyText>
<sectionHeader confidence="0.934421" genericHeader="references">
Reference
</sectionHeader>
<reference confidence="0.999636194444444">
Chen, Jiang and Jianyun Nie. 2000. Automatic con-
struction of parallel English-Chinese corpus for
cross-language information retrieval. Proceedings
of the sixth conference on Applied Natural Lan-
guage Processing, 21–28.
Resnik, Philip and Noah A. Smith. 2003. The Web as
a Parallel Corpus. Meeting of the Association for
Computational Linguistics 29(3). 349–380.
Kit, Chunyu and Jessica Yee Ha Ng. 2007. An Intelli-
gent Web Agent to Mine Bilingual Parallel Pages
via Automatic Discovery of URL Pairing Patterns.
Web Intelligence and Intelligent Agent Technology
Workshops, 526–529.
Zhang, Ying, Ke Wu, Jianfeng Gao and Phil Vines.
2006. Automatic Acquisition of Chinese-English
Parallel Corpus from the Web. Joint Proceedings of
the Association for Computational Linguistics and
the International Conference on Computational
Linguistics, 420–431.
Nie, Jianyun, Michel Simard, Pierre Isabelle and
Richard Durand. 1999. Cross-language information
retrieval based on parallel texts and automatic min-
ing of parallel texts from the Web. Proceedings of
the 22nd annual international ACM SIGIR confer-
ence on Research and development in information
retrieval, 74–81.
Ma, Xiaoyi and Mark Y. Liberman. 1999. BITS: A
Method for Bilingual Text Search over the Web.
Machine Translation Summit VII.
Chen, Jisong, Rowena Chau and Chung-Hsing Yeh.
2004. Discovering Parallel Text from the World
Wide Web. The Australasian Workshop on Data
Mining and Web Intelligence, vol. 32, 157–161.
Dunedin, New Zealand.
Jiang, Long, Shiquan Yang, Ming Zhou, Xiaohua Liu
and Qingsheng Zhu. 2009. Mining Bilingual Data
from the Web with Adaptively Learnt Patterns.
Proceedings of the Joint Conference of the 47th
Annual Meeting of the ACL and the 4th Interna-
tional Joint Conference on Natural Language Pro-
cessing of the AFNLP, vol. 2, 870–878.
Yanhui Feng, Yu Hong, Zhenxiang Yan, Jianmin
Yao and Qiaoming Zhu. 2010. A novel method for
bilingual web page acquisition from search engine
web records. Proceedings of the 23rd International
Conference on Computational Linguistics: Posters,
294–302.
Zhao, Bing and Stephan Vogel. 2002. Adaptive Paral-
lel Sentences Mining from Web Bilingual News
Collection. IEEE International Conference on Data
Mining, 745–748.
Smith, Jason R., Chris Quirk and Kristina Toutanova.
2010. Extracting parallel sentences from compara-
ble corpora using document level alignment. Hu-
man Language Technologies: The 2010 Annual
Conference of the North American Chapter of the
Association for Computational Linguistics, 403–
411.
Bharadwaj, Rohit G. and Vasudeva Varma. 2011.
Language independent identification of parallel
sentences using wikipedia. Proceedings of the 20th
International Conference Companion on World
Wide Web, 11–12. Hyderabad, India.
Gusfield, Dan. 1997. Algorithms on Strings, Trees
and Sequences: Computerss Science and Computa-
tional Biology. Cambridge University Press
Shi, Lei, Cheng Niu, Ming Zhou and Jianfeng Gao.
2006. A DOM Tree Alignment Model for Mining
Parallel Data from the Web. Proceedings of the
21st International Conference on Computational
Linguistics and the 44th annual meeting of the As-
sociation for Computational Linguistics, 489–496.
</reference>
<page confidence="0.810762">
1224
</page>
</variant>
</algorithm>
<algorithm name="ParsHed" version="110505">
<variant no="0" confidence="0.290625">
<title confidence="0.997526">An Iterative Link-based Method for Parallel Web Page Mining</title>
<author confidence="0.997174">Yu Jun Jun Heng Jianmin</author>
<address confidence="0.610629666666667">of Computer Science &amp; Technology, Soochow University, Suzhou, 215006, for Infocomm Research, Singapore, 138632 Science Department, Rensselaer Polytechnic Institute, Troy, NY 12180,</address>
<email confidence="0.763252">jih@rpi.edu,jyao@suda.edu.cn</email>
<abstract confidence="0.999473848484849">Abstracts Identifying parallel web pages from bilingual web sites is a crucial step of bilingual resource construction for crosslingual information processing. In this paper, we propose a link-based approach to distinguish parallel web pages from bilingual web sites. Compared with the existing methods, which only employ the internal translation similarity (such as content-based similarity and page structural similarity), we hypothesize that the external translation similarity is an effective feature to identify parallel web pages. Within a bilingual web site, web pages are interconnected by hyperlinks. The basic idea of our method is that the translation similarity of two pages can be inferred from their neighbor pages, which can be adopted as an important source of external similarity. Thus, the translation similarity of page pairs will influence each other. An iterative algorithm is developed to estimate the external translation similarity and the final translation similarity. Both internal and external similarity measures are combined in the iterative algorithm. Experiments on six bilingual websites demonstrate that our method is effective and obtains significant improvement (6.2% F-Score) over the baseline which only utilizes internal translation similarity.</abstract>
</variant>
</algorithm>
<algorithm name="ParsCit" version="110505">
<citationList>
<citation valid="true">
<authors>
<author>Jiang Chen</author>
<author>Jianyun Nie</author>
</authors>
<title>Automatic construction of parallel English-Chinese corpus for cross-language information retrieval.</title>
<date>2000</date>
<booktitle>Proceedings of the sixth conference on Applied Natural Language Processing,</booktitle>
<pages>21--28</pages>
<contexts>
<context position="2336" citStr="Chen and Nie, 2000" startWordPosition="330" endWordPosition="333">duction Parallel corpora have played an important role in multilingual Natural Language Processing, especially in Machine Translation (MT) and Crosslingual Information Retrieval(CLIR). However, it’s time-consuming to build parallel corpora manually. Some existing parallel corpora are subject to subscription or license fee and thus not freely available, while others are domain-specific. Therefore, a lot of previous research has focused on automatically mining parallel corpora from the web. In the past decade, there have been extensive studies on parallel resource extraction from the web (e.g., Chen and Nie, 2000; Resnik 2003; Jiang et al., 2009) and many effective Web mining systems have been developed such as STRAND, PTMiner, BITS and WPDE. For most of these mining systems, there is a typical parallel resource mining strategy which involves three steps: (1) locate the bilingual websites (2) identify parallel web pages from these bilingual websites and (3) extract bilingual resources from the parallel web pages. In this paper, we focus on the step (2) which is regarded as the core of the mining system (Chunyu, 2007). Estimating the translation similarity of two pages is the most basic and key problem</context>
</contexts>
<marker>Chen, Nie, 2000</marker>
<rawString>Chen, Jiang and Jianyun Nie. 2000. Automatic construction of parallel English-Chinese corpus for cross-language information retrieval. Proceedings of the sixth conference on Applied Natural Language Processing, 21–28.</rawString>
</citation>
<citation valid="true">
<authors>
<author>Philip Resnik</author>
<author>Noah A Smith</author>
</authors>
<title>The Web as a Parallel Corpus.</title>
<date>2003</date>
<journal>Meeting of the Association for Computational Linguistics</journal>
<volume>29</volume>
<issue>3</issue>
<pages>349--380</pages>
<contexts>
<context position="6987" citStr="Resnik and Smith, 2003" startWordPosition="1080" endWordPosition="1083">sites. Another parameter can be easily adjusted to achieve the best performance. Therefore, our method can be used in other websites with different styles, without much effort to optimize these parameters. 2 Related Work A large amount of literature has been published on parallel resource mining from the web. According to the existing form of the parallel resource on the Internet, related work can be categorized as follows: Mining from bilingual websites Most existing web mining systems aimed at mining bilingual resource from the bilingual websites, such as PTMiner (Nie et al., 1999), STRAND (Resnik and Smith, 2003), BITS (Ma and Liberman, 1999), PTI (Chen et al., 2004). PTMiner uses search engines to pinpoint the candidate sites that are likely to contain parallel pages, and then uses the collected URLs as seeds to further crawl each web site for more URLs. Web page pairs are extracted based on manually defined URL pattern matching, and further filtered according to several criteria. STRAND uses a search engine to search for multilingual websites and generated candidate page pairs based on manually created substitution rules. Then, it filters some candidate pairs by analyzing the HTML pages. PTI crawls </context>
<context position="12751" citStr="Resnik and Smith (2003)" startWordPosition="2056" endWordPosition="2059">t of our method is a bilingual website. This paper aims to find English/Chinese parallel pages. So a 3-gram language model is used to identify (or classify) the language of a certain document. The performance of the language identification module achieves 99.5% accuracy through in-house testing. As a result, a set of English pages and a set of Chinese pages are obtained. In order to get the neighbors of a page, for each bilingual website, two networks are constructed based on the hyperlinks, one for English pages and another for Chinese pages. 3.2 The Internal Translation Similarity Following Resnik and Smith (2003), three features are used to evaluate the internal translation similarity of two pages: A D A’ D’ B B’ ? C C’ E E’ 1218 The size ratio of two pages The length ratio of two documents is the simplest criterion for determining whether two documents are parallel or not. Parallel documents tend to be similar in length. And it is reasonable to assume that for text E in one language and text F in another language, length(E) z C•length(F), where C is a constant that depends on the language pair. Here, the content length of a web page is regarded as its length. The structure similarity of two pages The</context>
</contexts>
<marker>Resnik, Smith, 2003</marker>
<rawString>Resnik, Philip and Noah A. Smith. 2003. The Web as a Parallel Corpus. Meeting of the Association for Computational Linguistics 29(3). 349–380.</rawString>
</citation>
<citation valid="true">
<authors>
<author>Chunyu Kit</author>
<author>Jessica Yee Ha Ng</author>
</authors>
<title>An Intelligent Web Agent to Mine Bilingual Parallel Pages via Automatic Discovery of URL Pairing Patterns. Web Intelligence and Intelligent Agent Technology Workshops,</title>
<date>2007</date>
<pages>526--529</pages>
<marker>Kit, Ng, 2007</marker>
<rawString>Kit, Chunyu and Jessica Yee Ha Ng. 2007. An Intelligent Web Agent to Mine Bilingual Parallel Pages via Automatic Discovery of URL Pairing Patterns. Web Intelligence and Intelligent Agent Technology Workshops, 526–529.</rawString>
</citation>
<citation valid="true">
<authors>
<author>Ying Zhang</author>
<author>Ke Wu</author>
<author>Jianfeng Gao</author>
<author>Phil Vines</author>
</authors>
<date>2006</date>
<booktitle>Automatic Acquisition of Chinese-English Parallel Corpus from the Web. Joint Proceedings of the Association for Computational Linguistics and the International Conference on Computational Linguistics,</booktitle>
<pages>420--431</pages>
<contexts>
<context position="6172" citStr="Zhang et al., 2006" startWordPosition="949" endWordPosition="952">age independent and can be easily ported to new language pairs, except for the language-specific bilingual lexicons. Our method takes full advantage of the link information that is languageindependent. For the bilingual lexicons in our experiments, compared to previous methods, our method does not need a big bilingual lexicon, which is good news to less-resource language pairs. Unsupervised and fewer parameters In previous work, some parameters need to be optimized. Due to the diversity of web page styles, it is not trivial to obtain the best parameters. Some previous researches(Resnik, 2003; Zhang et al., 2006) attempt to optimize parameters by employing machine learning method. In contrast, in our method, only two parameters need to be estimated. One parameter remains stable for different style websites. Another parameter can be easily adjusted to achieve the best performance. Therefore, our method can be used in other websites with different styles, without much effort to optimize these parameters. 2 Related Work A large amount of literature has been published on parallel resource mining from the web. According to the existing form of the parallel resource on the Internet, related work can be cate</context>
</contexts>
<marker>Zhang, Wu, Gao, Vines, 2006</marker>
<rawString>Zhang, Ying, Ke Wu, Jianfeng Gao and Phil Vines. 2006. Automatic Acquisition of Chinese-English Parallel Corpus from the Web. Joint Proceedings of the Association for Computational Linguistics and the International Conference on Computational Linguistics, 420–431.</rawString>
</citation>
<citation valid="true">
<authors>
<author>Jianyun Nie</author>
<author>Michel Simard</author>
<author>Pierre Isabelle</author>
<author>Richard Durand</author>
</authors>
<title>Cross-language information retrieval based on parallel texts and automatic mining of parallel texts from the Web.</title>
<date>1999</date>
<booktitle>Proceedings of the 22nd annual international ACM SIGIR conference on Research and development in information retrieval,</booktitle>
<pages>74--81</pages>
<contexts>
<context position="6954" citStr="Nie et al., 1999" startWordPosition="1075" endWordPosition="1078">ble for different style websites. Another parameter can be easily adjusted to achieve the best performance. Therefore, our method can be used in other websites with different styles, without much effort to optimize these parameters. 2 Related Work A large amount of literature has been published on parallel resource mining from the web. According to the existing form of the parallel resource on the Internet, related work can be categorized as follows: Mining from bilingual websites Most existing web mining systems aimed at mining bilingual resource from the bilingual websites, such as PTMiner (Nie et al., 1999), STRAND (Resnik and Smith, 2003), BITS (Ma and Liberman, 1999), PTI (Chen et al., 2004). PTMiner uses search engines to pinpoint the candidate sites that are likely to contain parallel pages, and then uses the collected URLs as seeds to further crawl each web site for more URLs. Web page pairs are extracted based on manually defined URL pattern matching, and further filtered according to several criteria. STRAND uses a search engine to search for multilingual websites and generated candidate page pairs based on manually created substitution rules. Then, it filters some candidate pairs by anal</context>
</contexts>
<marker>Nie, Simard, Isabelle, Durand, 1999</marker>
<rawString>Nie, Jianyun, Michel Simard, Pierre Isabelle and Richard Durand. 1999. Cross-language information retrieval based on parallel texts and automatic mining of parallel texts from the Web. Proceedings of the 22nd annual international ACM SIGIR conference on Research and development in information retrieval, 74–81.</rawString>
</citation>
<citation valid="true">
<authors>
<author>Xiaoyi Ma</author>
<author>Mark Y Liberman</author>
</authors>
<title>BITS: A Method for Bilingual Text Search over the Web. Machine Translation Summit VII.</title>
<date>1999</date>
<contexts>
<context position="7017" citStr="Ma and Liberman, 1999" startWordPosition="1085" endWordPosition="1088"> easily adjusted to achieve the best performance. Therefore, our method can be used in other websites with different styles, without much effort to optimize these parameters. 2 Related Work A large amount of literature has been published on parallel resource mining from the web. According to the existing form of the parallel resource on the Internet, related work can be categorized as follows: Mining from bilingual websites Most existing web mining systems aimed at mining bilingual resource from the bilingual websites, such as PTMiner (Nie et al., 1999), STRAND (Resnik and Smith, 2003), BITS (Ma and Liberman, 1999), PTI (Chen et al., 2004). PTMiner uses search engines to pinpoint the candidate sites that are likely to contain parallel pages, and then uses the collected URLs as seeds to further crawl each web site for more URLs. Web page pairs are extracted based on manually defined URL pattern matching, and further filtered according to several criteria. STRAND uses a search engine to search for multilingual websites and generated candidate page pairs based on manually created substitution rules. Then, it filters some candidate pairs by analyzing the HTML pages. PTI crawls the web to fetch (potentially </context>
<context position="27008" citStr="Ma and Liberman (1999)" startWordPosition="4649" endWordPosition="4652">n our method. In the neighbor pages of a certain page, the popular pages are the minority. Besides, the iterative process makes our method more stable and robust. The impact of the size of bilingual lexicon The baseline system mainly combines the content-based similarity with structure similarity. iterative direction. In the following experiments, the parameter  is set to 0.6. 95 92.40 92.83 92.83 92.67 92.15 F-score(%) 91 93 89 87 85 92.61 6.69 92.42 92.91 92.78 81 80.20 79 0 0.1 0.2 0.3 0.4 0.5 0.6 0.7 0.8 0.9 1  83 1222 And two kinds of similarity measures are also used in our method. As Ma and Liberman (1999) pointed out, not all translators create translated pages that look like the original page which means that the structure similarity does not always work well. Compared to the structure similarity, the content-based is more reliable and has wider applicability. Furthermore, the bilingual lexicon is the only information that relates to the language pairs, and other features (such as structure and link information) are all language independent. So, it’s important to investigate the effect of lexicon size in our method. We test the performance of our method with different size of the bilingual di</context>
</contexts>
<marker>Ma, Liberman, 1999</marker>
<rawString>Ma, Xiaoyi and Mark Y. Liberman. 1999. BITS: A Method for Bilingual Text Search over the Web. Machine Translation Summit VII.</rawString>
</citation>
<citation valid="true">
<authors>
<author>Jisong Chen</author>
<author>Rowena Chau</author>
<author>Chung-Hsing Yeh</author>
</authors>
<title>Discovering Parallel Text from the World Wide Web. The Australasian</title>
<date>2004</date>
<booktitle>Workshop on Data Mining and Web Intelligence,</booktitle>
<volume>32</volume>
<pages>157--161</pages>
<location>Dunedin, New Zealand.</location>
<contexts>
<context position="7042" citStr="Chen et al., 2004" startWordPosition="1090" endWordPosition="1093">he best performance. Therefore, our method can be used in other websites with different styles, without much effort to optimize these parameters. 2 Related Work A large amount of literature has been published on parallel resource mining from the web. According to the existing form of the parallel resource on the Internet, related work can be categorized as follows: Mining from bilingual websites Most existing web mining systems aimed at mining bilingual resource from the bilingual websites, such as PTMiner (Nie et al., 1999), STRAND (Resnik and Smith, 2003), BITS (Ma and Liberman, 1999), PTI (Chen et al., 2004). PTMiner uses search engines to pinpoint the candidate sites that are likely to contain parallel pages, and then uses the collected URLs as seeds to further crawl each web site for more URLs. Web page pairs are extracted based on manually defined URL pattern matching, and further filtered according to several criteria. STRAND uses a search engine to search for multilingual websites and generated candidate page pairs based on manually created substitution rules. Then, it filters some candidate pairs by analyzing the HTML pages. PTI crawls the web to fetch (potentially parallel) candidate multi</context>
</contexts>
<marker>Chen, Chau, Yeh, 2004</marker>
<rawString>Chen, Jisong, Rowena Chau and Chung-Hsing Yeh. 2004. Discovering Parallel Text from the World Wide Web. The Australasian Workshop on Data Mining and Web Intelligence, vol. 32, 157–161. Dunedin, New Zealand.</rawString>
</citation>
<citation valid="true">
<authors>
<author>Long Jiang</author>
<author>Shiquan Yang</author>
<author>Ming Zhou</author>
<author>Xiaohua Liu</author>
<author>Qingsheng Zhu</author>
</authors>
<title>Mining Bilingual Data from the Web with Adaptively Learnt Patterns.</title>
<date>2009</date>
<booktitle>Proceedings of the Joint Conference of the 47th Annual Meeting of the ACL and the 4th International Joint Conference on Natural Language Processing of the AFNLP,</booktitle>
<volume>2</volume>
<pages>870--878</pages>
<contexts>
<context position="2370" citStr="Jiang et al., 2009" startWordPosition="336" endWordPosition="339">yed an important role in multilingual Natural Language Processing, especially in Machine Translation (MT) and Crosslingual Information Retrieval(CLIR). However, it’s time-consuming to build parallel corpora manually. Some existing parallel corpora are subject to subscription or license fee and thus not freely available, while others are domain-specific. Therefore, a lot of previous research has focused on automatically mining parallel corpora from the web. In the past decade, there have been extensive studies on parallel resource extraction from the web (e.g., Chen and Nie, 2000; Resnik 2003; Jiang et al., 2009) and many effective Web mining systems have been developed such as STRAND, PTMiner, BITS and WPDE. For most of these mining systems, there is a typical parallel resource mining strategy which involves three steps: (1) locate the bilingual websites (2) identify parallel web pages from these bilingual websites and (3) extract bilingual resources from the parallel web pages. In this paper, we focus on the step (2) which is regarded as the core of the mining system (Chunyu, 2007). Estimating the translation similarity of two pages is the most basic and key problem in this step. Previous approaches</context>
<context position="8253" citStr="Jiang et al. (2009)" startWordPosition="1281" endWordPosition="1284">te multilingual web documents by using a web spider. To determine the parallelism between potential document pairs, a filename comparison module is used to check filename resemblance, and a content analysis module is used to measure the semantic similarity. BITS was the first to obtain bilingual websites by employing a language identification module, and then for each bilingual website, it extracts parallel pages based on their content. Mining from bilingual web pages Parallel/bilingual resources may exist not only in two parallel monolingual web pages, but also in single bilingual web pages. Jiang et al. (2009) used an adaptive pattern-based method to mine interesting bilingual data based on the observation that bilingual data usually appears collectively following similar patterns. They found that bilingual web pages are a promising source of up-to-date bilingual terms/sentences which cover many domains and application scenarios. In addition, Feng et al. (2010) proposed a new method 1217 to automatically acquire bilingual web pages from the result pages of a search engine. Mining from comparable corpus Several attempts have been made to extract parallel resources from comparable corpora. Zhao et al</context>
</contexts>
<marker>Jiang, Yang, Zhou, Liu, Zhu, 2009</marker>
<rawString>Jiang, Long, Shiquan Yang, Ming Zhou, Xiaohua Liu and Qingsheng Zhu. 2009. Mining Bilingual Data from the Web with Adaptively Learnt Patterns. Proceedings of the Joint Conference of the 47th Annual Meeting of the ACL and the 4th International Joint Conference on Natural Language Processing of the AFNLP, vol. 2, 870–878.</rawString>
</citation>
<citation valid="true">
<authors>
<author>Yanhui Feng</author>
<author>Yu Hong</author>
<author>Zhenxiang Yan</author>
<author>Jianmin Yao</author>
<author>Qiaoming Zhu</author>
</authors>
<title>A novel method for bilingual web page acquisition from search engine web records.</title>
<date>2010</date>
<booktitle>Proceedings of the 23rd International Conference on Computational Linguistics: Posters,</booktitle>
<pages>294--302</pages>
<contexts>
<context position="8611" citStr="Feng et al. (2010)" startWordPosition="1334" endWordPosition="1337">en for each bilingual website, it extracts parallel pages based on their content. Mining from bilingual web pages Parallel/bilingual resources may exist not only in two parallel monolingual web pages, but also in single bilingual web pages. Jiang et al. (2009) used an adaptive pattern-based method to mine interesting bilingual data based on the observation that bilingual data usually appears collectively following similar patterns. They found that bilingual web pages are a promising source of up-to-date bilingual terms/sentences which cover many domains and application scenarios. In addition, Feng et al. (2010) proposed a new method 1217 to automatically acquire bilingual web pages from the result pages of a search engine. Mining from comparable corpus Several attempts have been made to extract parallel resources from comparable corpora. Zhao et al. (2002) proposed a robust, adaptive approach for mining parallel sentences from a bilingual comparable news collection. In their method, sentence length models and lexiconbased models were combined under a maximum likelihood criterion. Smith et al. (2010) found that Wikipedia contains a lot of comparable documents, and adopted a ranking model to select pa</context>
</contexts>
<marker>Feng, Hong, Yan, Yao, Zhu, 2010</marker>
<rawString>Yanhui Feng, Yu Hong, Zhenxiang Yan, Jianmin Yao and Qiaoming Zhu. 2010. A novel method for bilingual web page acquisition from search engine web records. Proceedings of the 23rd International Conference on Computational Linguistics: Posters, 294–302.</rawString>
</citation>
<citation valid="true">
<authors>
<author>Bing Zhao</author>
<author>Stephan Vogel</author>
</authors>
<title>Adaptive Parallel Sentences Mining from Web Bilingual News Collection.</title>
<date>2002</date>
<booktitle>IEEE International Conference on Data Mining,</booktitle>
<pages>745--748</pages>
<marker>Zhao, Vogel, 2002</marker>
<rawString>Zhao, Bing and Stephan Vogel. 2002. Adaptive Parallel Sentences Mining from Web Bilingual News Collection. IEEE International Conference on Data Mining, 745–748.</rawString>
</citation>
<citation valid="true">
<authors>
<author>Jason R Smith</author>
<author>Chris Quirk</author>
<author>Kristina Toutanova</author>
</authors>
<title>Extracting parallel sentences from comparable corpora using document level alignment. Human Language Technologies: The</title>
<date>2010</date>
<booktitle>Annual Conference of the North American Chapter of the Association for Computational Linguistics,</booktitle>
<volume>403</volume>
<pages>411</pages>
<contexts>
<context position="9109" citStr="Smith et al. (2010)" startWordPosition="1409" endWordPosition="1412"> up-to-date bilingual terms/sentences which cover many domains and application scenarios. In addition, Feng et al. (2010) proposed a new method 1217 to automatically acquire bilingual web pages from the result pages of a search engine. Mining from comparable corpus Several attempts have been made to extract parallel resources from comparable corpora. Zhao et al. (2002) proposed a robust, adaptive approach for mining parallel sentences from a bilingual comparable news collection. In their method, sentence length models and lexiconbased models were combined under a maximum likelihood criterion. Smith et al. (2010) found that Wikipedia contains a lot of comparable documents, and adopted a ranking model to select parallel sentence pairs from comparable documents. Bharadwaj et al. (2011) used a SVM classifier with some new features to identify parallel sentences from Wikipedia. 3 Iterative Link-based Parallel Web Pages Mining As mentioned, the basic idea of our method is that the similarity of two pages can be inferred from their neighbors. This idea is illustrated in Figure 1. Figure 1 Illustration of the link-based method In Figure 1, A, B, C, D and E are some pages in the same language; while A’, B’, C</context>
</contexts>
<marker>Smith, Quirk, Toutanova, 2010</marker>
<rawString>Smith, Jason R., Chris Quirk and Kristina Toutanova. 2010. Extracting parallel sentences from comparable corpora using document level alignment. Human Language Technologies: The 2010 Annual Conference of the North American Chapter of the Association for Computational Linguistics, 403– 411.</rawString>
</citation>
<citation valid="true">
<authors>
<author>Rohit G Bharadwaj</author>
<author>Vasudeva Varma</author>
</authors>
<title>Language independent identification of parallel sentences using wikipedia.</title>
<date>2011</date>
<booktitle>Proceedings of the 20th International Conference Companion on World Wide Web,</booktitle>
<pages>11--12</pages>
<location>Hyderabad, India.</location>
<marker>Bharadwaj, Varma, 2011</marker>
<rawString>Bharadwaj, Rohit G. and Vasudeva Varma. 2011. Language independent identification of parallel sentences using wikipedia. Proceedings of the 20th International Conference Companion on World Wide Web, 11–12. Hyderabad, India.</rawString>
</citation>
<citation valid="true">
<authors>
<author>Dan Gusfield</author>
</authors>
<title>Algorithms on Strings, Trees and Sequences: Computerss Science and Computational Biology.</title>
<date>1997</date>
<publisher>Cambridge University Press</publisher>
<marker>Gusfield, 1997</marker>
<rawString>Gusfield, Dan. 1997. Algorithms on Strings, Trees and Sequences: Computerss Science and Computational Biology. Cambridge University Press</rawString>
</citation>
<citation valid="true">
<authors>
<author>Lei Shi</author>
<author>Cheng Niu</author>
<author>Ming Zhou</author>
<author>Jianfeng Gao</author>
</authors>
<title>A DOM Tree Alignment Model for Mining Parallel Data from the Web.</title>
<date>2006</date>
<booktitle>Proceedings of the 21st International Conference on Computational Linguistics and the 44th annual meeting of the Association for Computational Linguistics,</booktitle>
<pages>489--496</pages>
<contexts>
<context position="20124" citStr="Shi, et al, 2006" startWordPosition="3412" endWordPosition="3415">Ch pages Total No pat- URL pairs tern pairs S1 1101/1098 1092 20 www.gov.hk S2 501/497 487 7 www.customs.gov.hk S3 995/775 768 12 www.sbc.edu.sg S4 4085/3838 3648 4 www.swd.gov.hk S5 660/637 637 0 www.landsd.gov.hk S6 4733/4626 4615 8 www.td.gov.hk total 12075/11471 11684 51 Each website listed in Table 1 has a URL pattern for most parallel web pages. Some previous researches used the URL similarity or patterns to find parallel page pairs. However, due to the diversity of web page styles and website maintenance mechanisms, bilingual websites adopt varied naming schemes for parallel documents (Shi, et al, 2006). The effect of URL pattern-based mining always depends on the style of website. In order to build a large dataset, the URL pattern is not used in our method. Our method is able to handle bilingual websites without URL pattern rules. In addition, an English-Chinese dictionary with 64K words pairs is used in our experiments. Algorithm 3 needs a threshold MAX_P or 1220 MIN-SIM. It is very hard to tune the MIN-SIM because it varies a lot in different websites and language pairs. However, Table 1 shows that the number of parallel pages is smaller than that of English and Chinese pages. Here, for e</context>
</contexts>
<marker>Shi, Niu, Zhou, Gao, 2006</marker>
<rawString>Shi, Lei, Cheng Niu, Ming Zhou and Jianfeng Gao. 2006. A DOM Tree Alignment Model for Mining Parallel Data from the Web. Proceedings of the 21st International Conference on Computational Linguistics and the 44th annual meeting of the Association for Computational Linguistics, 489–496.</rawString>
</citation>
</citationList>
</algorithm>
</algorithms>
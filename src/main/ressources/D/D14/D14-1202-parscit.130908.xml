<?xml version="1.0" encoding="UTF-8"?>
<algorithms version="110505">
<algorithm name="SectLabel" version="110505">
<variant no="0" confidence="0.000000">
<title confidence="0.9967995">
Coarse-grained Candidate Generation and Fine-grained Re-ranking for
Chinese Abbreviation Prediction
</title>
<author confidence="0.997341">
Longkai Zhang Houfeng Wang Xu Sun
</author>
<affiliation confidence="0.9820295">
Key Laboratory of Computational Linguistics (Peking University)
Ministry of Education, China
</affiliation>
<email confidence="0.991832">
zhlongk@qq.com, wanghf@pku.edu.cn, xusun@pku.edu.cn
</email>
<sectionHeader confidence="0.993911" genericHeader="abstract">
Abstract
</sectionHeader>
<bodyText confidence="0.99960752">
Correctly predicting abbreviations given
the full forms is important in many natu-
ral language processing systems. In this
paper we propose a two-stage method to
find the corresponding abbreviation given
its full form. We first use the contextual
information given a large corpus to get ab-
breviation candidates for each full form
and get a coarse-grained ranking through
graph random walk. This coarse-grained
rank list fixes the search space inside the
top-ranked candidates. Then we use a sim-
ilarity sensitive re-ranking strategy which
can utilize the features of the candidates
to give a fine-grained re-ranking and se-
lect the final result. Our method achieves
good results and outperforms the state-of-
the-art systems. One advantage of our
method is that it only needs weak super-
vision and can get competitive results with
fewer training data. The candidate genera-
tion and coarse-grained ranking is totally
unsupervised. The re-ranking phase can
use a very small amount of training data
to get a reasonably good result.
</bodyText>
<sectionHeader confidence="0.999131" genericHeader="keywords">
1 Introduction
</sectionHeader>
<bodyText confidence="0.999941462962963">
Abbreviation Prediction is defined as finding the
meaningful short subsequence of characters given
the original fully expanded form. As an example,
“HMM” is the abbreviation for the correspond-
ing full form “Hidden Markov Model”. While
the existence of abbreviations is a common lin-
guistic phenomenon, it causes many problems like
spelling variation (Nenadi´c et al., 2002). The dif-
ferent writing manners make it difficult to identify
the terms conveying the same concept, which will
hurt the performance of many applications, such
as information retrieval (IR) systems and machine
translation (MT) systems.
Previous works mainly treat the Chinese ab-
breviation generation task as a sequence labeling
problem, which gives each character a label to in-
dicate whether the given character in the full form
should be kept in the abbreviation or not. These
methods show acceptable results. However they
rely heavily on the character-based features, which
means it needs lots of training data to learn the
weights of these context features. The perfor-
mance is good on some test sets that are similar to
the training data, however, when it moves to an un-
seen context, this method may fail. This is always
true in real application contexts like the social me-
dia where there are tremendous new abbreviations
burst out every day.
A more intuitive way is to find the full-
abbreviation pairs directly from a large text cor-
pus. A good source of texts is the news texts. In
a news text, the full forms are often mentioned
first. Then in the rest of the news its corresponding
abbreviation is mentioned as an alternative. The
co-occurrence of the full form and the abbrevia-
tion makes it easier for us to mine the abbreviation
pairs from the large amount of news texts. There-
fore, given a long full form, we can generate its
abbreviation candidates from the given corpus, in-
stead of doing the character tagging job.
For the abbreviation prediction task, the candi-
date abbreviation must be a sub-sequence of the
given full form. An intuitive way is to select
all the sub-sequences in the corpus as the can-
didates. This will generate large numbers of ir-
relevant candidates. Instead, we use a contextual
graph random walk method, which can utilize the
contextual information through the graph, to select
a coarse grained list of candidates given the full
form. We only select the top-ranked candidates to
reduce the search space. On the other hand, the
candidate generation process can only use limited
contextual information to give a coarse-grained
ranked list of candidates. During generation, can-
</bodyText>
<page confidence="0.935112">
1881
</page>
<note confidence="0.8979655">
Proceedings of the 2014 Conference on Empirical Methods in Natural Language Processing (EMNLP), pages 1881–1890,
October 25-29, 2014, Doha, Qatar. c�2014 Association for Computational Linguistics
</note>
<bodyText confidence="0.999814863636364">
didate level features cannot be included. There-
fore we propose a similarity sensitive re-ranking
method to give a fine-grained ranked list. We then
select the final result based on the rank of each
candidate.
The contribution of our work is two folds.
Firstly we propose an improved method for abbre-
viation generation. Compared to previous work,
our method can perform well with less training
data. This is an advantage in the context of so-
cial media. Secondly, we build a new abbreviation
corpus and make it publicly available for future re-
search on this topic.
The paper is structured as follows. Section 1
gives the introduction. In section 2 we describe
the abbreviation task. In section 3 we describe
the candidate generation part and in section 4 we
describe the re-ranking part. Experiments are de-
scribed in section 5. We also give a detailed anal-
ysis of the results in section 5. In section 6 related
works are introduced, and the paper is concluded
in the last section.
</bodyText>
<sectionHeader confidence="0.902675" genericHeader="introduction">
2 Chinese Abbreviation Prediction
System
</sectionHeader>
<bodyText confidence="0.999320125">
Chinese Abbreviation Prediction is the task of
selecting representative characters from the long
full form1. Previous works mainly use the se-
quence labeling strategies, which views the full
form as a character sequence and give each char-
acter an extra label ‘Keep’ or ‘Skip’ to indicate
whether the current character should be kept in
the abbreviation. An example is shown in Table
1. The sequence labeling method assumes that
the character context information is crucial to de-
cide the keep or skip of a character. However,
we can give many counterexamples. An exam-
ple is “IL 3i&apos;*”(Peking University) and “�
It&amp;quot;&apos;*”(Tsinghua University), whose abbrevia-
tions correspond to “IL &apos;” and ‘M It&amp;quot;’ respec-
tively. Although sharing a similar character con-
text, the third character ‘&apos;’ is kept in the first case
and is skipped in the second case.
We believe that a better way is to extract these
abbreviation-full pairs from a natural text corpus
where the full form and its abbreviation co-exist.
Therefore we propose a two stage method. The
first stage generates a list of candidates given a
large corpus. To reduce the search space, we adopt
</bodyText>
<footnote confidence="0.8587425">
1Details of the difference between English and Chinese
abbreviation prediction can be found in Zhang et al. (2012).
</footnote>
<table confidence="0.950858333333333">
Full form ff / &apos;
Status Skip Keep Keep Skip
Result / &apos;
</table>
<tableCaption confidence="0.7867145">
Table 1: The abbreviation “/&apos;” of the full form
“�/&apos;*” (Hong Kong University)
</tableCaption>
<bodyText confidence="0.999125">
graph random walk to give a coarse-grained rank-
ing and select the top-ranked ones as the can-
didates. Then we use a similarity sensitive re-
ranking method to decide the final result. Detailed
description of the two parts is shown in the follow-
ing sections.
</bodyText>
<sectionHeader confidence="0.988097" genericHeader="method">
3 Candidate Generation through Graph
Random Walk
</sectionHeader>
<subsectionHeader confidence="0.9772385">
3.1 Candidate Generation and Graph
Representation
</subsectionHeader>
<bodyText confidence="0.99985906060606">
Chinese abbreviations are sub-sequences of the
full form. We use a brute force method to select
all strings in a given news article that is the sub-
sequence of the full form. The brute force method
is not time consuming compared to using more
complex data structures like trie tree, because in
a given news article there are a limited number of
sub-strings which meet the sub-sequence criteria
for abbreviations. When generating abbreviation
candidates for a given full form, we require the
full form should appear in the given news article
at least once. This is a coarse filter to indicate that
the given news article is related to the full form and
therefore the candidates generated are potentially
meaningful.
The main motivation of the candidate genera-
tion stage in our approach is that the full form and
its abbreviation tend to share similar context in a
given corpus. To be more detailed, given a word
context window w, the words that appear in the
context window of the full form tend to be sim-
ilar to those words in the context window of the
abbreviations.
We use a bipartite graph G(Vword, Vcontext, E)
to represent this phenomena. We build bipartite
graphs for each full form individually. For a given
full form vfull, we first extract all its candidate
abbreviations VC. We have two kinds of nodes
in the bipartite graph: the word nodes and the
context nodes. We construct the word nodes as
Vword = VC ∪ {vfull}, which is the node set of
the full form and all the candidates. We construct
the context nodes Vcontext as the words that appear
</bodyText>
<page confidence="0.991673">
1882
</page>
<bodyText confidence="0.977855708333334">
in a fixed window of Vword. To reduce the size of
the graph, we make two extra assumptions: 1) We
only consider the nouns and verbs in the context
and 2) context words should appear in the vocab-
ulary for more than a predefined threshold (i.e. 5
times). Because G is bipartite graph, the edges E
only connect word node and context nodes. We
use the number of co-occurrence of the candidate
and the context word as the weight of each edge
and then form the weight matrix W. Details of the
bipartite graph construction algorithm are shown
in Table 2. An example bipartite graph is shown
in figure 1.
Figure 1: An example of the bipartite graph rep-
resentation. The full form is “*%)C*”(Hong
Kong University), which is the first node on the
left. The three candidates are “%)C”, “*%”,
“)C *”, which are the nodes on the left. The
context words in this example are “���”(Tsui
Lap-chee, the headmaster of Hong Kong Uni-
versity), “fixt”(Enrollment), “#�”(Hold), “�
ah”(Enact), “�-ft�”(Subway), which are the nodes
on the right. The edge weight is the co-occurrence
of the left word and the right word.
</bodyText>
<subsectionHeader confidence="0.995021">
3.2 Coarse-grained Ranking Using Random
Walks
</subsectionHeader>
<bodyText confidence="0.995675310344828">
We perform Markov Random Walk on the con-
structed bipartite graph to give a coarse-grained
ranked list of all candidates. In random walk, a
walker starts from the full form source node S
(in later steps, vi) and randomly walks to another
node vj with a transition probability pij. In ran-
dom walk we assume the walker do the walking n
times and finally stops at a final node. When the
walking is done, we can get the probability of each
node that the walker stops in the end. Because
the destination of each step is selected based on
transition probabilities, the word node that shares
more similar contexts are more likely to be the fi-
nal stop. The random walk method we use is sim-
ilar to those defined in Norris (1998); Zhu et al.
(2003); Sproat et al. (2006); Hassan and Menezes
(2013); Li et al. (2013).
The transition probability pij is calculated us-
ing the weights in the weight matrix W and then
normalized with respect to the source node vi with
the formula pij = w.j
l w.l . When the graph ran-
dom walk is done, we get a list of coarse-ranked
candidates, each with a confidence score derived
from the context information. By performing the
graph random walk, we reduce the search space
from exponential to the top-ranked ones. Now we
only need to select the final result from the candi-
dates, which we will describe in the next section.
</bodyText>
<sectionHeader confidence="0.993431" genericHeader="method">
4 Candidate Re-ranking
</sectionHeader>
<bodyText confidence="0.999983">
Although the coarse-grained ranked list can serve
as a basic reference, it can only use limited in-
formation like co-occurrence. We still need a re-
ranking process to decide the final result. The rea-
son is that we cannot get any candidate-specific
features when the candidate is not fully gener-
ated. Features such as the length of a candidate are
proved to be useful to rank the candidates by pre-
vious work. In this section we describe our second
stage for abbreviation generation, which we use a
similarity sensitive re-ranking method to find the
final result.
</bodyText>
<subsectionHeader confidence="0.999485">
4.1 Similarity Sensitive Re-ranking
</subsectionHeader>
<bodyText confidence="0.999995181818182">
The basic idea behind our similarity sensitive re-
ranking model is that we penalize the mistakes
based on the similarity of the candidate and the
reference. If the model wrongly selects a less sim-
ilar candidate as the result, then we will attach a
large penalty to this mistake. If the model wrongly
chooses a candidate but the candidate is similar to
the reference, we slightly penalize this mistake.
The similarity between a candidate and the ref-
erence is measured through character similarity,
which we will describe later.
</bodyText>
<page confidence="0.932525">
1883
</page>
<bodyText confidence="0.707981166666667">
Input: the full form vfull, news corpus U
Output: bipartite graph G(Vword, Vcontext, E)
Candidate vector Vc = 0, Vcontext = 0
for each document d in U
if d contains vfull
add all words w in the window of vfull into Vcontext
</bodyText>
<equation confidence="0.914915818181818">
for each n-gram s in d
if s is a sub-sequence of vfull
add s into Vc
add all word w in the window of s into Vcontext
end if
end for
end if
end for ff
Vword = Vc U lvfull1
for each word vi in Vword
for each word vj in Vcontext
</equation>
<bodyText confidence="0.811423">
calculate edge weight in E based on co-occurrence
</bodyText>
<listItem confidence="0.766981">
end for
end for
</listItem>
<table confidence="0.167612">
Return G(Vword, Vcontext, E)
</table>
<tableCaption confidence="0.786225">
Table 2: Algorithm for constructing bipartite graphs
</tableCaption>
<figure confidence="0.945109333333333">
We first give some notation of the re-ranking c ,
phase. P=
a
</figure>
<listItem confidence="0.368877">
1. f(x, y) is a scoring function for a given com-
bination of x and y, where x is the original full
form and y is an abbreviation candidate. For a
given full form xi with K candidates, we assume
its corresponding K candidates are y1i ,y2i ,...,yKi .
2. evaluation function s(x, y) is used to mea-
sure the similarity of the candidate to the refer-
ence, where x is the original full form and y is one
abbreviation candidate. We require that s(x, y)
should be in [0, 1] and s(x, y) = 1 if and only if y
is the reference.
</listItem>
<bodyText confidence="0.9997999">
One choice for s(x,y) may be the indicator
function. However, indicator function returns zero
for all false candidates. In the abbreviation predic-
tion task, some false candidates are much closer to
the reference than the rest. Considering this, we
use a Longest Common Subsequence(LCS) based
criterion to calculate s(x, y). Suppose the length
of a candidate is a, the length of the reference is b
and the length of their LCS is c, then we can define
precision P and recall R as:
</bodyText>
<equation confidence="0.9969058">
c
R =b,
2 * P * R
F=
P + R
</equation>
<bodyText confidence="0.994476">
It is easy to see that F is a suitable s(x, y).
Therefore we can use the F-score as the value for
s(x, y).
</bodyText>
<listItem confidence="0.9838722">
3. O(x, y) is a feature function which returns a
m dimension feature vector. m is the number of
features in the re-ranking.
4. w� is a weight vector with dimension m.
wT O(x, y) is the score after re-ranking. The candi-
</listItem>
<bodyText confidence="0.8807475">
date with the highest score will be our final result.
Given these notations, we can now describe our
re-ranking algorithm. Suppose we have the train-
ing set X = {x1, x2, ..., xn1. We should find the
weight vector w� that can minimize the loss func-
tion:
</bodyText>
<equation confidence="0.9598556">
(1)
Loss(w) =
((s(xi, y1i )
− s(xi, yji ))
* I(wT O(xi, yji ) � wT O(xi, y1i )))
(2)
n
i=1
k
j=1
</equation>
<page confidence="0.956133">
1884
</page>
<bodyText confidence="0.992760777777778">
I(x) is the indicator function. It equals to 1
if and only if x ≥ 0. I(j) = 1 means that the
candidate which is less ‘similar’ to the reference
is ranked higher than the reference. Intuitively,
Loss(w) is the weighted sum of all the wrongly
ranked candidates.
It is difficult to optimize Loss(w) because
Loss(w) is discontinuous. We make a relaxation
here2:
</bodyText>
<listItem confidence="0.793865777777778">
2. The character uni-grams and bi-grams in
the candidate. This kind of feature cannot
be used in the traditional character tagging
methods.
3. The language model score of the candi-
date. In our experiment, we train a bi-gram
language model using Laplace smoothing on
the Chinese Gigaword Data3.
(3)
</listItem>
<bodyText confidence="0.9971545">
From the equations above we can see that
2L(w) is the upper bound of our loss function
Loss(w). Therefore we can optimize L(w) to ap-
proximate Loss(4
We can use optimization methods like gradient
descent to get the U� that minimize the loss func-
tion. Because L is not convex, it may go into a lo-
cal minimum. In our experiment we held out 10%
data as the develop set and try random initializa-
tion to decide the initial w.
</bodyText>
<sectionHeader confidence="0.614369" genericHeader="method">
4.2 Features for Re-ranking
</sectionHeader>
<bodyText confidence="0.978242571428572">
One advantage of the re-ranking phase is that it
can now use features related to candidates. There-
fore, we can use a variety of features. We list them
as follows.
1. The coarse-grained ranking score from the
graph random walk phase. From the de-
scription of the previous section we know that
this score is the probability a ‘walker’ ‘walk’
from the full form node to the current candi-
date. This is a coarse-grained score because
it can only use the information of words in-
side the window. However, it is still informa-
tive because in the re-ranking phase we can-
not collect this information directly.
</bodyText>
<footnote confidence="0.924847">
2To prove this we need the following two inequalities: 1)
</footnote>
<equation confidence="0.7917985">
when x &gt; 0, I(x) G 1+e s and 2) s(xi, yz) — s(xi, yz) &gt;
0. — — —
</equation>
<listItem confidence="0.872391903225807">
4. The length of the candidate. Intuitively,
abbreviations tend to be short. Therefore
length can be an important feature for the re-
ranking.
5. The degree of ambiguity of the candidate.
We first define the degree of ambiguity di of a
character ci as the number of identical words
that contain the character. We then define the
degree of ambiguity of the candidate as the
sum of all di in the candidates. We need a dic-
tionary to extract this feature. We collect all
words in the PKU data of the second Interna-
tional Chinese Word Segmentation Bakeoff4.
6. Whether the candidate is in a word dictio-
nary. We use the PKU dictionary in feature
5.
7. Whether all bi-grams are in a word dictio-
nary. We use the PKU dictionary in feature
5.
8. Adjacent Variety(AV) of the candidate. We
define the left AV of the candidate as the
probability that in a corpus the character in
front of the candidate is a character in the
full form. For example if we consider the full
form “I L¬&apos;*”(Peking University) and the
candidate “¬&apos;”, then the left AV of “¬&apos;”
is the probability that the character preced-
ing “¬&apos;” is ‘I L’ or ‘¬’ or ‘&apos;’ or ‘*’ in
a corpus. We can similarly define the right
AV, with respect to characters follow the can-
didate.
</listItem>
<bodyText confidence="0.9990002">
The AV feature is very useful because in some
cases a substring of the full form may have a con-
fusingly high frequency. In the example of “I L¬
&apos;*”(Peking University), an article in the corpus
may mention “I L¬&apos;*”(Peking University) and
</bodyText>
<footnote confidence="0.999384333333333">
3http://www.ldc.upenn.edu/Catalog/
catalogEntry.jsp?catalogId=LDC2003T09
4http://www.sighan.org/bakeoff2005/
</footnote>
<equation confidence="0.986205263157895">
L(w) = n k ((s(xi,y1i ) − s(xi,yji ))
i=1 j=1
((s(xi,y1i ) − s(xi, yji ))
∗ I(�UTO(xi,yj i ) ≥ wTO(xi,y1i )))
1
=
2
Loss(w)
1
≤
2
n
i=1
k
j=1
∗
1 + e−
IVT (φ(xi,yji )−φ(xi,y1i )) )
1
</equation>
<page confidence="0.953857">
1885
</page>
<bodyText confidence="0.999820692307692">
“,,)i,r)C*”(Tokyo University) at the same time.
Then the substring “�)C*” may be included in
the candidate generation phase for “��)C*”
with a high frequency. Because the left AV of “�
)C*” is high, the re-ranker can easily detect this
false candidate.
In practice, all the features need to be scaled in
order to speed up training. There are many ways
to scale features. We use the most intuitive scal-
ing method. For a feature value x, we scale it as
(x−mean)/(max−min). Note that for language
model score and the score of random walk phase,
we scale based on their log value.
</bodyText>
<sectionHeader confidence="0.999869" genericHeader="evaluation">
5 Experiments
</sectionHeader>
<subsectionHeader confidence="0.995886">
5.1 Dataset and Evaluation metrics
</subsectionHeader>
<bodyText confidence="0.999986923076923">
For the dataset, we collect 3210 abbreviation pairs
from the Chinese Gigaword corpus. The abbre-
viation pairs include noun phrases, organization
names and some other types. The Chinese Gi-
gaword corpus contains news texts from the year
1992 to 2007. We only collect those pairs whose
full form and corresponding abbreviation appear
in the same article for at least one time. For full
forms with more than one reasonable reference,
we keep the most frequently used one as its refer-
ence. We use 80% abbreviation pairs as the train-
ing data and the rest as the testing data.
We use the top-K accuracy as the evaluation
metrics. The top-K accuracy is widely used as the
measurement in previous work (Tsuruoka et al.,
2005; Sun et al., 2008, 2009; Zhang et al., 2012). It
measures what percentage of the reference abbre-
viations are found if we take the top k candidate
abbreviations from all the results. In our experi-
ment, we compare the top-5 accuracy with base-
lines. We choose the top-10 candidates from the
graph random walk are considered in re-ranking
phase and the measurement used is top-1 accuracy
because the final aim of the algorithm is to detect
the exact abbreviation, rather than a list of candi-
dates.
</bodyText>
<subsectionHeader confidence="0.997753">
5.2 Candidate List
</subsectionHeader>
<bodyText confidence="0.998665">
Table 3 shows examples of the candidates. In our
algorithm we further reduce the search space to
only incorporate 10 candidates from the candidate
generation phase.
</bodyText>
<table confidence="0.968565833333333">
K Top-K Accuracy
1 6.84%
2 19.35%
3 49.01%
4 63.70%
5 73.60%
</table>
<tableCaption confidence="0.9849285">
Table 4: Top-5 accuracy of the candidate genera-
tion phase
</tableCaption>
<subsectionHeader confidence="0.999725">
5.3 Comparison with baselines
</subsectionHeader>
<bodyText confidence="0.99995805">
We first show the top-5 accuracy of the candidate
generation phase Table 4. We can see that, just
like the case of using other feature alone, using
the score of random walk alone is far from enough.
However, the first 5 candidates contain most of the
correct answers. We use the top-5 candidates plus
another 5 candidates in the re-ranking phase.
We choose the character tagging method as the
baseline method. The character tagging strategy
is widely used in the abbreviation generation task
(Tsuruoka et al., 2005; Sun et al., 2008, 2009;
Zhang et al., 2012). We choose the ‘SK’ labeling
strategy which is used in Sun et al. (2009); Zhang
et al. (2012). The ‘SK’ labeling strategy gives each
character a label in the character sequence, with
‘S’ represents ’Skip’ and ‘K’ represents ‘Keep’.
Same with Zhang et al. (2012), we use the Con-
ditional Random Fields (CRFs) model in the se-
quence labeling process.
The baseline method mainly uses the charac-
ter context information to generate the candidate
abbreviation. To be fair we use the same fea-
ture set in Sun et al. (2009); Zhang et al. (2012).
One drawback of the sequence labeling method is
that it relies heavily on the character context in
the full form. With the number of new abbrevi-
ations grows rapidly (especially in social media
like Facebook or twitter), it is impossible to let the
model ‘remember’ all the character contexts. Our
method is different from theirs, we use a more in-
tuitive way which finds the list of candidates di-
rectly from a natural corpus.
Table 5 shows the comparison of the top-5 accu-
racy. We can see that our method outperforms the
baseline methods. The baseline model performs
well when using character features (Column 3).
However, it performs poorly without the charac-
ter features (Column 2). In contrast, without the
character features, our method (Column 4) works
much better than the sequence labeling method.
</bodyText>
<page confidence="0.978095">
1886
</page>
<table confidence="0.999824352941176">
Full form Reference Generated Candidates #Enum #Now
ýE?»*,, (Depart- ý?*,, ý?*,,,?»*,,,ýE?»,ý? 30 7
ment of International »,ý?,?»,ýE
Politics)
à8fhý (Non- à8ý 8 ý,à 8,8 f,8 ý �,à 8 62 13
nuclear Countries) ý,f h ý,à 8 ý *,8 f h
ý,à8fh,8fhý�,8f
h,ýXfh
l VÒÁ (Drug traf- l Ò VÒÁ,l ÒÁ,l V,ÒÁ,l Ò 14 5
ficking)
•_ÏNTaÑU¡ •Ñlø a ¡,• Ñ,• Ñ l ø,• _ l 16382 20
W4Plø (Yangtze ø,•_ÏN,TaÑU,ÏNÑ
Joint River Economic U,ÏNT�,•_T�ÑU,•
Development Inc.) _ÑUlø,•_ÏNT ÏN
T�ÑU,•_ÏNT�ÑU,¡
�4Plø,4Plø,•_,l
ø,¡�,Ta,ÏN
</table>
<tableCaption confidence="0.7532955">
Table 3: Generated Candidates. #Enum is the number of candidates generated by enumerating all possi-
ble candidates. #Now is the number of candidates generated by our method.
</tableCaption>
<bodyText confidence="0.984410333333333">
When we add character features, our method (Col-
umn 5) still outperforms the sequence labeling
method.
</bodyText>
<table confidence="0.999103166666667">
K CRF-char Our-char CRF Our
1 38.00% 48.60% 53.27% 55.61%
2 38.16% 70.87% 65.89% 73.10%
3 39.41% 81.78% 72.43% 81.96%
4 55.30% 87.54% 78.97% 87.57%
5 62.31% 89.25% 81.78% 89.27%
</table>
<tableCaption confidence="0.994006">
Table 5: Comparison of the baseline method and
</tableCaption>
<bodyText confidence="0.974012666666667">
our method. CRF-char (‘-’ means minus) is the
baseline method without character features. CRF
is the baseline method. Our-char is our method
without character features. We define character
features as the features that consider the charac-
ters from the original full form as their parts.
</bodyText>
<subsectionHeader confidence="0.999329">
5.4 Performance with less training data
</subsectionHeader>
<bodyText confidence="0.998931133333333">
One advantage of our method is that it only
requires weak supervision. The baseline
method needs plenty of manually collected
full-abbreviation pairs to learn a good model.
In our method, the candidate generation and
coarse-grained ranking is totally unsupervised.
The re-ranking phase needs training instances
to decide the parameters. However we can use
a very small amount of training data to get a
reasonably good model. Figure 2 shows the result
of using different size of training data. We can
see that the performance of the baseline methods
drops rapidly when there are less training data.
In contrast, when using less training data, our
method does not suffer that much.
</bodyText>
<figureCaption confidence="0.543447666666667">
Figure 2: Top-1 accuracy when changing the size
of training data. For example, “50%” means “us-
ing 50% of all the training data”.
</figureCaption>
<subsectionHeader confidence="0.999">
5.5 Comparison with previous work
</subsectionHeader>
<bodyText confidence="0.999967222222222">
We compare our method with the method in the
previous work DPLVM+GI in Sun et al. (2009),
which outperforms Tsuruoka et al. (2005); Sun
et al. (2008). We also compare our method with
the web-based method CRF+WEB in Zhang et al.
(2012). Because the comparison is performed on
different corpora, we run the two methods on our
data. Table 6 shows the top-1 accuracy. We
can see that our method outperforms the previous
</bodyText>
<page confidence="0.988973">
1887
</page>
<table confidence="0.8921614">
methods.
System Top-K Accuracy
DPLVM+GI 53.29%
CRF+WEB 54.02%
Our method 55.61%
</table>
<tableCaption confidence="0.98895">
Table 6: Comparison with previous work. The
search results of CRF+WEB is based on March 9,
2014 version of the Baidu search engine.
</tableCaption>
<subsectionHeader confidence="0.961077">
5.6 Error Analysis
</subsectionHeader>
<bodyText confidence="0.99993">
We perform cross-validation to find the errors and
list the two major errors below:
</bodyText>
<listItem confidence="0.813756739130435">
1. Some full forms may correspond to more
than one acceptable abbreviation. In this
case, our method may choose the one that is
indeed used as the full form’s abbreviation in
news texts, but not the same as the standard
reference abbreviations. The reason for this
phenomenon may lie in the fact that the veri-
fication data we use is news text, which tends
to be formal. Therefore when a reference is
often used colloquially, our method may miss
it. We can relieve this by changing the corpus
we use.
2. Our method may provide biased information
when handling location sensitive phrases.
Not only our system, the system of Sun et al.
(2009); Zhang et al. (2012) also shows this
phenomenon. An example is the case of “�
A ftPH,9” (Democracy League of Hong
Kong). Because most of the news is about
news in mainland China, it is hard for the
model to tell the difference between the ref-
erence “A PH,9” and a false candidate “�
9”(Democracy League of China).
</listItem>
<bodyText confidence="0.999871">
Another ambiguity is “%Af211&amp;quot;-)C*”(Tsinghua
University), which has two abbreviations “%Af
)C” and “%Af211&amp;quot;-”. This happens because the
full form itself is ambiguous. Word sense dis-
ambiguation can be performed first to handle
this kind of problem.
</bodyText>
<sectionHeader confidence="0.999919" genericHeader="related work">
6 Related Work
</sectionHeader>
<bodyText confidence="0.999942690909091">
Abbreviation generation has been studied during
recent years. At first, some approaches maintain
a database of abbreviations and their correspond-
ing “full form” pairs. The major problem of pure
database-building approach is obvious. It is im-
possible to cover all abbreviations, and the build-
ing process is quite laborious. To find these pairs
automatically, a powerful approach is to find the
reference for a full form given the context, which
is referred to as “abbreviation generation”.
There is research on using heuristic rules
for generating abbreviations Barrett and Grems
(1960); Bourne and Ford (1961); Taghva and
Gilbreth (1999); Park and Byrd (2001); Wren et al.
(2002); Hearst (2003). Most of them achieved
high performance. However, hand-crafted rules
are time consuming to create, and it is not easy to
transfer the knowledge of rules from one language
to another.
Recent studies of abbreviation generation have
focused on the use of machine learning tech-
niques. Sun et al. (2008) proposed an SVM ap-
proach. Tsuruoka et al. (2005); Sun et al. (2009)
formalized the process of abbreviation generation
as a sequence labeling problem. The drawback of
the sequence labeling strategies is that they rely
heavily on the character features. This kind of
method cannot fit the need for abbreviation gen-
eration in social media texts where the amount of
abbreviations grows fast.
Besides these pure statistical approaches, there
are also many approaches using Web as a corpus
in machine learning approaches for generating ab-
breviations. Adar (2004) proposed methods to de-
tect such pairs from biomedical documents. Jain
et al. (2007) used web search results as well as
search logs to find and rank abbreviates full pairs,
which show good result. The disadvantage is that
search log data is only available in a search en-
gine backend. The ordinary approaches do not
have access to search engine internals. Zhang et al.
(2012) used web search engine information to re-
rank the candidate abbreviations generated by sta-
tistical approaches. Compared to their approaches,
our method only uses a fixed corpus, instead of us-
ing collective information, which varies from time
to time.
Some of the previous work that relate to ab-
breviations focuses on the task of “abbreviation
disambiguation”, which aims to find the correct
abbreviation-full pairs. In these works, machine
learning approaches are commonly used (Park and
Byrd, 2001; HaCohen-Kerner et al., 2008; Yu
et al., 2006; Ao and Takagi, 2005). We focus on
another aspect. We want to find the abbreviation
</bodyText>
<page confidence="0.978279">
1888
</page>
<bodyText confidence="0.9999775">
given the full form. Besides, Sun et al. (2013) also
works on abbreviation prediction but focuses on
the negative full form problem, which is a little
different from our work.
One related research field is text normalization,
with many outstanding works (Sproat et al., 2001;
Aw et al., 2006; Hassan and Menezes, 2013; Ling
et al., 2013; Yang and Eisenstein, 2013). While
the two tasks share similarities, abbreviation pre-
diction has its identical characteristics, like the
sub-sequence assumption. This results in different
methods to tackle the two different problems.
</bodyText>
<sectionHeader confidence="0.998373" genericHeader="conclusions">
7 Conclusion
</sectionHeader>
<bodyText confidence="0.999894333333333">
In this paper, we propose a unified framework for
Chinese abbreviation generation. Our approach
contains two stages: candidate generation and
re-ranking. Given a long term, we first gener-
ate a list of abbreviation candidates using the co-
occurrence information. We give a coarse-grained
rank using graph random walk to reduce the search
space. After we get the candidate lists, we can use
the features related to the candidates. We use a
similarity sensitive re-rank method to get the final
abbreviation. Experiments show that our method
outperforms the previous systems.
</bodyText>
<sectionHeader confidence="0.997474" genericHeader="acknowledgments">
Acknowledgments
</sectionHeader>
<bodyText confidence="0.9997575">
This research was partly supported by Na-
tional Natural Science Foundation of China
(No.61370117,61333018,61300063),Major
National Social Science Fund of
China(No.12&amp;ZD227), National High Tech-
nology Research and Development Program of
China (863 Program) (No. 2012AA011101), and
Doctoral Fund of Ministry of Education of China
(No. 20130001120004). The contact author of
this paper, according to the meaning given to
this role by Key Laboratory of Computational
Linguistics, Ministry of Education, School of
Electronics Engineering and Computer Science,
Peking University, is Houfeng Wang. We thank
Ke Wu for part of our work is inspired by his
previous work at KLCL.
</bodyText>
<sectionHeader confidence="0.980329" genericHeader="references">
References
</sectionHeader>
<bodyText confidence="0.602325684210526">
Adar, E. (2004). Sarad: A simple and ro-
bust abbreviation dictionary. Bioinformatics,
20(4):527–533.
Ao, H. and Takagi, T. (2005). Alice: an algorithm
to extract abbreviations from medline. Journal
of the American Medical Informatics Associa-
tion, 12(5):576–586.
Aw, A., Zhang, M., Xiao, J., and Su, J. (2006). A
phrase-based statistical model for sms text nor-
malization. In Proceedings of the COLING/ACL
on Main conference poster sessions, pages 33–
40. Association for Computational Linguistics.
Barrett, J. and Grems, M. (1960). Abbreviating
words systematically. Communications of the
ACM, 3(5):323–324.
Bourne, C. and Ford, D. (1961). A study of
methods for systematically abbreviating english
words and names. Journal of the ACM (JACM),
8(4):538–552.
</bodyText>
<reference confidence="0.994294514285714">
HaCohen-Kerner, Y., Kass, A., and Peretz, A.
(2008). Combined one sense disambiguation
of abbreviations. In Proceedings of the 46th
Annual Meeting of the Association for Compu-
tational Linguistics on Human Language Tech-
nologies: Short Papers, pages 61–64. Associa-
tion for Computational Linguistics.
Hassan, H. and Menezes, A. (2013). Social text
normalization using contextual graph random
walks. In Proceedings of the 51st Annual Meet-
ing of the Association for Computational Lin-
guistics (Volume 1: Long Papers), pages 1577–
1586, Sofia, Bulgaria. Association for Compu-
tational Linguistics.
Hearst, M. S. (2003). A simple algorithm for
identifying abbreviation definitions in biomed-
ical text.
Jain, A., Cucerzan, S., and Azzam, S. (2007).
Acronym-expansion recognition and ranking on
the web. In Information Reuse and Integration,
2007. IRI 2007. IEEE International Conference
on, pages 209–214. IEEE.
Li, J., Ott, M., and Cardie, C. (2013). Identify-
ing manipulated offerings on review portals. In
EMNLP, pages 1933–1942.
Ling, W., Dyer, C., Black, A. W., and Trancoso, I.
(2013). Paraphrasing 4 microblog normaliza-
tion. In Proceedings of the 2013 Conference
on Empirical Methods in Natural Language
Processing, pages 73–84, Seattle, Washington,
USA. Association for Computational Linguis-
tics.
Nenadi´c, G., Spasi´c, I., and Ananiadou, S. (2002).
Automatic acronym acquisition and term varia-
tion management within domain-specific texts.
</reference>
<page confidence="0.894986">
1889
</page>
<reference confidence="0.84861224691358">
In Third International Conference on Language
Resources and Evaluation (LREC2002), pages
2155–2162.
Norris, J. R. (1998). Markov chains. Number
2008. Cambridge university press.
Park, Y. and Byrd, R. (2001). Hybrid text mining
for finding abbreviations and their definitions.
In Proceedings of the 2001 conference on em-
pirical methods in natural language processing,
pages 126–133.
Sproat, R., Black, A. W., Chen, S., Kumar, S.,
Ostendorf, M., and Richards, C. (2001). Nor-
malization of non-standard words. Computer
Speech &amp; Language, 15(3):287–333.
Sproat, R., Tao, T., and Zhai, C. (2006). Named
entity transliteration with comparable corpora.
In Proceedings of the 21st International Confer-
ence on Computational Linguistics and the 44th
annual meeting of the Association for Computa-
tional Linguistics, pages 73–80. Association for
Computational Linguistics.
Sun, X., Li, W., Meng, F., and Wang, H. (2013).
Generalized abbreviation prediction with nega-
tive full forms and its application on improv-
ing chinese web search. In Proceedings of the
Sixth International Joint Conference on Natural
Language Processing, pages 641–647, Nagoya,
Japan. Asian Federation of Natural Language
Processing.
Sun, X., Okazaki, N., and Tsujii, J. (2009). Ro-
bust approach to abbreviating terms: A discrim-
inative latent variable model with global infor-
mation. In Proceedings of the Joint Confer-
ence of the 47th Annual Meeting of the ACL and
the 4th International Joint Conference on Nat-
ural Language Processing of the AFNLP: Vol-
ume 2-Volume 2, pages 905–913. Association
for Computational Linguistics.
Sun, X., Wang, H., and Wang, B. (2008). Pre-
dicting chinese abbreviations from definitions:
An empirical learning approach using support
vector regression. Journal of Computer Science
and Technology, 23(4):602–611.
Taghva, K. and Gilbreth, J. (1999). Recognizing
acronyms and their definitions. International
Journal on Document Analysis and Recogni-
tion, 1(4):191–198.
Tsuruoka, Y., Ananiadou, S., and Tsujii, J. (2005).
A machine learning approach to acronym gen-
eration. In Proceedings of the ACL-ISMB Work-
shop on Linking Biological Literature, Ontolo-
gies and Databases: Mining Biological Seman-
tics, pages 25–31. Association for Computa-
tional Linguistics.
Wren, J., Garner, H., et al. (2002). Heuristics
for identification of acronym-definition patterns
within text: towards an automated construc-
tion of comprehensive acronym-definition dic-
tionaries. Methods of information in medicine,
41(5):426–434.
Yang, Y. and Eisenstein, J. (2013). A log-linear
model for unsupervised text normalization. In
Proceedings of the 2013 Conference on Empir-
ical Methods in Natural Language Processing,
pages 61–72, Seattle, Washington, USA. Asso-
ciation for Computational Linguistics.
Yu, H., Kim, W., Hatzivassiloglou, V., and Wilbur,
J. (2006). A large scale, corpus-based approach
for automatically disambiguating biomedical
abbreviations. ACM Transactions on Informa-
tion Systems (TOIS), 24(3):380–404.
Zhang, L., Li, S., Wang, H., Sun, N., and Meng,
X. (2012). Constructing Chinese abbreviation
dictionary: A stacked approach. In Proceedings
of COLING 2012, pages 3055–3070, Mumbai,
India. The COLING 2012 Organizing Commit-
tee.
Zhu, X., Ghahramani, Z., Lafferty, J., et al. (2003).
Semi-supervised learning using gaussian fields
and harmonic functions. In ICML, volume 3,
pages 912–919.
</reference>
<page confidence="0.990437">
1890
</page>
</variant>
</algorithm>
<algorithm name="ParsHed" version="110505">
<variant no="0" confidence="0.804435">
<title confidence="0.997215">Coarse-grained Candidate Generation and Fine-grained Re-ranking Chinese Abbreviation Prediction</title>
<author confidence="0.998808">Longkai Zhang Houfeng Wang Xu</author>
<affiliation confidence="0.980471">Key Laboratory of Computational Linguistics (Peking Ministry of Education,</affiliation>
<email confidence="0.885944">zhlongk@qq.com,wanghf@pku.edu.cn,xusun@pku.edu.cn</email>
<abstract confidence="0.998073538461538">Correctly predicting abbreviations given the full forms is important in many natural language processing systems. In this paper we propose a two-stage method to find the corresponding abbreviation given its full form. We first use the contextual information given a large corpus to get abbreviation candidates for each full form and get a coarse-grained ranking through graph random walk. This coarse-grained rank list fixes the search space inside the top-ranked candidates. Then we use a similarity sensitive re-ranking strategy which can utilize the features of the candidates to give a fine-grained re-ranking and select the final result. Our method achieves good results and outperforms the state-ofthe-art systems. One advantage of our method is that it only needs weak supervision and can get competitive results with fewer training data. The candidate generation and coarse-grained ranking is totally unsupervised. The re-ranking phase can use a very small amount of training data to get a reasonably good result.</abstract>
</variant>
</algorithm>
<algorithm name="ParsCit" version="110505">
<citationList>
<citation valid="true">
<authors>
<author>Y HaCohen-Kerner</author>
<author>A Kass</author>
<author>A Peretz</author>
</authors>
<title>Combined one sense disambiguation of abbreviations.</title>
<date>2008</date>
<booktitle>In Proceedings of the 46th Annual Meeting of the Association for Computational Linguistics on Human Language Technologies: Short Papers,</booktitle>
<pages>61--64</pages>
<publisher>Association</publisher>
<institution>for Computational Linguistics.</institution>
<contexts>
<context position="28631" citStr="HaCohen-Kerner et al., 2008" startWordPosition="4912" endWordPosition="4915"> backend. The ordinary approaches do not have access to search engine internals. Zhang et al. (2012) used web search engine information to rerank the candidate abbreviations generated by statistical approaches. Compared to their approaches, our method only uses a fixed corpus, instead of using collective information, which varies from time to time. Some of the previous work that relate to abbreviations focuses on the task of “abbreviation disambiguation”, which aims to find the correct abbreviation-full pairs. In these works, machine learning approaches are commonly used (Park and Byrd, 2001; HaCohen-Kerner et al., 2008; Yu et al., 2006; Ao and Takagi, 2005). We focus on another aspect. We want to find the abbreviation 1888 given the full form. Besides, Sun et al. (2013) also works on abbreviation prediction but focuses on the negative full form problem, which is a little different from our work. One related research field is text normalization, with many outstanding works (Sproat et al., 2001; Aw et al., 2006; Hassan and Menezes, 2013; Ling et al., 2013; Yang and Eisenstein, 2013). While the two tasks share similarities, abbreviation prediction has its identical characteristics, like the sub-sequence assump</context>
</contexts>
<marker>HaCohen-Kerner, Kass, Peretz, 2008</marker>
<rawString>HaCohen-Kerner, Y., Kass, A., and Peretz, A. (2008). Combined one sense disambiguation of abbreviations. In Proceedings of the 46th Annual Meeting of the Association for Computational Linguistics on Human Language Technologies: Short Papers, pages 61–64. Association for Computational Linguistics.</rawString>
</citation>
<citation valid="true">
<authors>
<author>H Hassan</author>
<author>A Menezes</author>
</authors>
<title>Social text normalization using contextual graph random walks.</title>
<date>2013</date>
<booktitle>In Proceedings of the 51st Annual Meeting of the Association for Computational Linguistics (Volume 1: Long Papers),</booktitle>
<pages>1577--1586</pages>
<institution>Sofia, Bulgaria. Association for Computational Linguistics.</institution>
<contexts>
<context position="10343" citStr="Hassan and Menezes (2013)" startWordPosition="1724" endWordPosition="1727"> full form source node S (in later steps, vi) and randomly walks to another node vj with a transition probability pij. In random walk we assume the walker do the walking n times and finally stops at a final node. When the walking is done, we can get the probability of each node that the walker stops in the end. Because the destination of each step is selected based on transition probabilities, the word node that shares more similar contexts are more likely to be the final stop. The random walk method we use is similar to those defined in Norris (1998); Zhu et al. (2003); Sproat et al. (2006); Hassan and Menezes (2013); Li et al. (2013). The transition probability pij is calculated using the weights in the weight matrix W and then normalized with respect to the source node vi with the formula pij = w.j l w.l . When the graph random walk is done, we get a list of coarse-ranked candidates, each with a confidence score derived from the context information. By performing the graph random walk, we reduce the search space from exponential to the top-ranked ones. Now we only need to select the final result from the candidates, which we will describe in the next section. 4 Candidate Re-ranking Although the coarse-g</context>
<context position="29055" citStr="Hassan and Menezes, 2013" startWordPosition="4985" endWordPosition="4988">f “abbreviation disambiguation”, which aims to find the correct abbreviation-full pairs. In these works, machine learning approaches are commonly used (Park and Byrd, 2001; HaCohen-Kerner et al., 2008; Yu et al., 2006; Ao and Takagi, 2005). We focus on another aspect. We want to find the abbreviation 1888 given the full form. Besides, Sun et al. (2013) also works on abbreviation prediction but focuses on the negative full form problem, which is a little different from our work. One related research field is text normalization, with many outstanding works (Sproat et al., 2001; Aw et al., 2006; Hassan and Menezes, 2013; Ling et al., 2013; Yang and Eisenstein, 2013). While the two tasks share similarities, abbreviation prediction has its identical characteristics, like the sub-sequence assumption. This results in different methods to tackle the two different problems. 7 Conclusion In this paper, we propose a unified framework for Chinese abbreviation generation. Our approach contains two stages: candidate generation and re-ranking. Given a long term, we first generate a list of abbreviation candidates using the cooccurrence information. We give a coarse-grained rank using graph random walk to reduce the sear</context>
</contexts>
<marker>Hassan, Menezes, 2013</marker>
<rawString>Hassan, H. and Menezes, A. (2013). Social text normalization using contextual graph random walks. In Proceedings of the 51st Annual Meeting of the Association for Computational Linguistics (Volume 1: Long Papers), pages 1577– 1586, Sofia, Bulgaria. Association for Computational Linguistics.</rawString>
</citation>
<citation valid="true">
<authors>
<author>M S Hearst</author>
</authors>
<title>A simple algorithm for identifying abbreviation definitions in biomedical text.</title>
<date>2003</date>
<contexts>
<context position="26874" citStr="Hearst (2003)" startWordPosition="4631" endWordPosition="4632">oaches maintain a database of abbreviations and their corresponding “full form” pairs. The major problem of pure database-building approach is obvious. It is impossible to cover all abbreviations, and the building process is quite laborious. To find these pairs automatically, a powerful approach is to find the reference for a full form given the context, which is referred to as “abbreviation generation”. There is research on using heuristic rules for generating abbreviations Barrett and Grems (1960); Bourne and Ford (1961); Taghva and Gilbreth (1999); Park and Byrd (2001); Wren et al. (2002); Hearst (2003). Most of them achieved high performance. However, hand-crafted rules are time consuming to create, and it is not easy to transfer the knowledge of rules from one language to another. Recent studies of abbreviation generation have focused on the use of machine learning techniques. Sun et al. (2008) proposed an SVM approach. Tsuruoka et al. (2005); Sun et al. (2009) formalized the process of abbreviation generation as a sequence labeling problem. The drawback of the sequence labeling strategies is that they rely heavily on the character features. This kind of method cannot fit the need for abbr</context>
</contexts>
<marker>Hearst, 2003</marker>
<rawString>Hearst, M. S. (2003). A simple algorithm for identifying abbreviation definitions in biomedical text.</rawString>
</citation>
<citation valid="true">
<authors>
<author>A Jain</author>
<author>S Cucerzan</author>
<author>S Azzam</author>
</authors>
<title>Acronym-expansion recognition and ranking on the web.</title>
<date>2007</date>
<booktitle>In Information Reuse and Integration,</booktitle>
<pages>209--214</pages>
<publisher>IEEE.</publisher>
<contexts>
<context position="27814" citStr="Jain et al. (2007)" startWordPosition="4780" endWordPosition="4783">ch. Tsuruoka et al. (2005); Sun et al. (2009) formalized the process of abbreviation generation as a sequence labeling problem. The drawback of the sequence labeling strategies is that they rely heavily on the character features. This kind of method cannot fit the need for abbreviation generation in social media texts where the amount of abbreviations grows fast. Besides these pure statistical approaches, there are also many approaches using Web as a corpus in machine learning approaches for generating abbreviations. Adar (2004) proposed methods to detect such pairs from biomedical documents. Jain et al. (2007) used web search results as well as search logs to find and rank abbreviates full pairs, which show good result. The disadvantage is that search log data is only available in a search engine backend. The ordinary approaches do not have access to search engine internals. Zhang et al. (2012) used web search engine information to rerank the candidate abbreviations generated by statistical approaches. Compared to their approaches, our method only uses a fixed corpus, instead of using collective information, which varies from time to time. Some of the previous work that relate to abbreviations focu</context>
</contexts>
<marker>Jain, Cucerzan, Azzam, 2007</marker>
<rawString>Jain, A., Cucerzan, S., and Azzam, S. (2007). Acronym-expansion recognition and ranking on the web. In Information Reuse and Integration, 2007. IRI 2007. IEEE International Conference on, pages 209–214. IEEE.</rawString>
</citation>
<citation valid="true">
<authors>
<author>J Li</author>
<author>M Ott</author>
<author>C Cardie</author>
</authors>
<title>Identifying manipulated offerings on review portals.</title>
<date>2013</date>
<booktitle>In EMNLP,</booktitle>
<pages>1933--1942</pages>
<contexts>
<context position="10361" citStr="Li et al. (2013)" startWordPosition="1728" endWordPosition="1731">n later steps, vi) and randomly walks to another node vj with a transition probability pij. In random walk we assume the walker do the walking n times and finally stops at a final node. When the walking is done, we can get the probability of each node that the walker stops in the end. Because the destination of each step is selected based on transition probabilities, the word node that shares more similar contexts are more likely to be the final stop. The random walk method we use is similar to those defined in Norris (1998); Zhu et al. (2003); Sproat et al. (2006); Hassan and Menezes (2013); Li et al. (2013). The transition probability pij is calculated using the weights in the weight matrix W and then normalized with respect to the source node vi with the formula pij = w.j l w.l . When the graph random walk is done, we get a list of coarse-ranked candidates, each with a confidence score derived from the context information. By performing the graph random walk, we reduce the search space from exponential to the top-ranked ones. Now we only need to select the final result from the candidates, which we will describe in the next section. 4 Candidate Re-ranking Although the coarse-grained ranked list</context>
</contexts>
<marker>Li, Ott, Cardie, 2013</marker>
<rawString>Li, J., Ott, M., and Cardie, C. (2013). Identifying manipulated offerings on review portals. In EMNLP, pages 1933–1942.</rawString>
</citation>
<citation valid="true">
<authors>
<author>W Ling</author>
<author>C Dyer</author>
<author>A W Black</author>
<author>I Trancoso</author>
</authors>
<title>Paraphrasing 4 microblog normalization.</title>
<date>2013</date>
<booktitle>In Proceedings of the 2013 Conference on Empirical Methods in Natural Language Processing,</booktitle>
<pages>73--84</pages>
<publisher>Association</publisher>
<institution>for Computational Linguistics.</institution>
<location>Seattle, Washington, USA.</location>
<contexts>
<context position="29074" citStr="Ling et al., 2013" startWordPosition="4989" endWordPosition="4992">tion”, which aims to find the correct abbreviation-full pairs. In these works, machine learning approaches are commonly used (Park and Byrd, 2001; HaCohen-Kerner et al., 2008; Yu et al., 2006; Ao and Takagi, 2005). We focus on another aspect. We want to find the abbreviation 1888 given the full form. Besides, Sun et al. (2013) also works on abbreviation prediction but focuses on the negative full form problem, which is a little different from our work. One related research field is text normalization, with many outstanding works (Sproat et al., 2001; Aw et al., 2006; Hassan and Menezes, 2013; Ling et al., 2013; Yang and Eisenstein, 2013). While the two tasks share similarities, abbreviation prediction has its identical characteristics, like the sub-sequence assumption. This results in different methods to tackle the two different problems. 7 Conclusion In this paper, we propose a unified framework for Chinese abbreviation generation. Our approach contains two stages: candidate generation and re-ranking. Given a long term, we first generate a list of abbreviation candidates using the cooccurrence information. We give a coarse-grained rank using graph random walk to reduce the search space. After we </context>
</contexts>
<marker>Ling, Dyer, Black, Trancoso, 2013</marker>
<rawString>Ling, W., Dyer, C., Black, A. W., and Trancoso, I. (2013). Paraphrasing 4 microblog normalization. In Proceedings of the 2013 Conference on Empirical Methods in Natural Language Processing, pages 73–84, Seattle, Washington, USA. Association for Computational Linguistics.</rawString>
</citation>
<citation valid="true">
<authors>
<author>G Nenadi´c</author>
<author>I Spasi´c</author>
<author>S Ananiadou</author>
</authors>
<title>Automatic acronym acquisition and term variation management within domain-specific texts.</title>
<date>2002</date>
<marker>Nenadi´c, Spasi´c, Ananiadou, 2002</marker>
<rawString>Nenadi´c, G., Spasi´c, I., and Ananiadou, S. (2002). Automatic acronym acquisition and term variation management within domain-specific texts.</rawString>
</citation>
<citation valid="false">
<booktitle>In Third International Conference on Language Resources and Evaluation (LREC2002),</booktitle>
<pages>2155--2162</pages>
<marker></marker>
<rawString>In Third International Conference on Language Resources and Evaluation (LREC2002), pages 2155–2162.</rawString>
</citation>
<citation valid="true">
<authors>
<author>J R Norris</author>
</authors>
<title>Markov chains. Number</title>
<date>1998</date>
<note>Cambridge university press.</note>
<contexts>
<context position="10275" citStr="Norris (1998)" startWordPosition="1714" endWordPosition="1715">all candidates. In random walk, a walker starts from the full form source node S (in later steps, vi) and randomly walks to another node vj with a transition probability pij. In random walk we assume the walker do the walking n times and finally stops at a final node. When the walking is done, we can get the probability of each node that the walker stops in the end. Because the destination of each step is selected based on transition probabilities, the word node that shares more similar contexts are more likely to be the final stop. The random walk method we use is similar to those defined in Norris (1998); Zhu et al. (2003); Sproat et al. (2006); Hassan and Menezes (2013); Li et al. (2013). The transition probability pij is calculated using the weights in the weight matrix W and then normalized with respect to the source node vi with the formula pij = w.j l w.l . When the graph random walk is done, we get a list of coarse-ranked candidates, each with a confidence score derived from the context information. By performing the graph random walk, we reduce the search space from exponential to the top-ranked ones. Now we only need to select the final result from the candidates, which we will descri</context>
</contexts>
<marker>Norris, 1998</marker>
<rawString>Norris, J. R. (1998). Markov chains. Number 2008. Cambridge university press.</rawString>
</citation>
<citation valid="true">
<authors>
<author>Y Park</author>
<author>R Byrd</author>
</authors>
<title>Hybrid text mining for finding abbreviations and their definitions.</title>
<date>2001</date>
<booktitle>In Proceedings of the 2001 conference on empirical methods in natural language processing,</booktitle>
<pages>126--133</pages>
<contexts>
<context position="26839" citStr="Park and Byrd (2001)" startWordPosition="4623" endWordPosition="4626">d during recent years. At first, some approaches maintain a database of abbreviations and their corresponding “full form” pairs. The major problem of pure database-building approach is obvious. It is impossible to cover all abbreviations, and the building process is quite laborious. To find these pairs automatically, a powerful approach is to find the reference for a full form given the context, which is referred to as “abbreviation generation”. There is research on using heuristic rules for generating abbreviations Barrett and Grems (1960); Bourne and Ford (1961); Taghva and Gilbreth (1999); Park and Byrd (2001); Wren et al. (2002); Hearst (2003). Most of them achieved high performance. However, hand-crafted rules are time consuming to create, and it is not easy to transfer the knowledge of rules from one language to another. Recent studies of abbreviation generation have focused on the use of machine learning techniques. Sun et al. (2008) proposed an SVM approach. Tsuruoka et al. (2005); Sun et al. (2009) formalized the process of abbreviation generation as a sequence labeling problem. The drawback of the sequence labeling strategies is that they rely heavily on the character features. This kind of </context>
<context position="28602" citStr="Park and Byrd, 2001" startWordPosition="4908" endWordPosition="4911">le in a search engine backend. The ordinary approaches do not have access to search engine internals. Zhang et al. (2012) used web search engine information to rerank the candidate abbreviations generated by statistical approaches. Compared to their approaches, our method only uses a fixed corpus, instead of using collective information, which varies from time to time. Some of the previous work that relate to abbreviations focuses on the task of “abbreviation disambiguation”, which aims to find the correct abbreviation-full pairs. In these works, machine learning approaches are commonly used (Park and Byrd, 2001; HaCohen-Kerner et al., 2008; Yu et al., 2006; Ao and Takagi, 2005). We focus on another aspect. We want to find the abbreviation 1888 given the full form. Besides, Sun et al. (2013) also works on abbreviation prediction but focuses on the negative full form problem, which is a little different from our work. One related research field is text normalization, with many outstanding works (Sproat et al., 2001; Aw et al., 2006; Hassan and Menezes, 2013; Ling et al., 2013; Yang and Eisenstein, 2013). While the two tasks share similarities, abbreviation prediction has its identical characteristics,</context>
</contexts>
<marker>Park, Byrd, 2001</marker>
<rawString>Park, Y. and Byrd, R. (2001). Hybrid text mining for finding abbreviations and their definitions. In Proceedings of the 2001 conference on empirical methods in natural language processing, pages 126–133.</rawString>
</citation>
<citation valid="true">
<authors>
<author>R Sproat</author>
<author>A W Black</author>
<author>S Chen</author>
<author>S Kumar</author>
<author>M Ostendorf</author>
<author>C Richards</author>
</authors>
<title>Normalization of non-standard words.</title>
<date>2001</date>
<journal>Computer Speech &amp; Language,</journal>
<volume>15</volume>
<issue>3</issue>
<contexts>
<context position="29012" citStr="Sproat et al., 2001" startWordPosition="4977" endWordPosition="4980">to abbreviations focuses on the task of “abbreviation disambiguation”, which aims to find the correct abbreviation-full pairs. In these works, machine learning approaches are commonly used (Park and Byrd, 2001; HaCohen-Kerner et al., 2008; Yu et al., 2006; Ao and Takagi, 2005). We focus on another aspect. We want to find the abbreviation 1888 given the full form. Besides, Sun et al. (2013) also works on abbreviation prediction but focuses on the negative full form problem, which is a little different from our work. One related research field is text normalization, with many outstanding works (Sproat et al., 2001; Aw et al., 2006; Hassan and Menezes, 2013; Ling et al., 2013; Yang and Eisenstein, 2013). While the two tasks share similarities, abbreviation prediction has its identical characteristics, like the sub-sequence assumption. This results in different methods to tackle the two different problems. 7 Conclusion In this paper, we propose a unified framework for Chinese abbreviation generation. Our approach contains two stages: candidate generation and re-ranking. Given a long term, we first generate a list of abbreviation candidates using the cooccurrence information. We give a coarse-grained rank</context>
</contexts>
<marker>Sproat, Black, Chen, Kumar, Ostendorf, Richards, 2001</marker>
<rawString>Sproat, R., Black, A. W., Chen, S., Kumar, S., Ostendorf, M., and Richards, C. (2001). Normalization of non-standard words. Computer Speech &amp; Language, 15(3):287–333.</rawString>
</citation>
<citation valid="true">
<authors>
<author>R Sproat</author>
<author>T Tao</author>
<author>C Zhai</author>
</authors>
<title>Named entity transliteration with comparable corpora.</title>
<date>2006</date>
<booktitle>In Proceedings of the 21st International Conference on Computational Linguistics and the 44th annual meeting of the Association for Computational Linguistics,</booktitle>
<pages>73--80</pages>
<publisher>Association</publisher>
<institution>for Computational Linguistics.</institution>
<contexts>
<context position="10316" citStr="Sproat et al. (2006)" startWordPosition="1720" endWordPosition="1723">walker starts from the full form source node S (in later steps, vi) and randomly walks to another node vj with a transition probability pij. In random walk we assume the walker do the walking n times and finally stops at a final node. When the walking is done, we can get the probability of each node that the walker stops in the end. Because the destination of each step is selected based on transition probabilities, the word node that shares more similar contexts are more likely to be the final stop. The random walk method we use is similar to those defined in Norris (1998); Zhu et al. (2003); Sproat et al. (2006); Hassan and Menezes (2013); Li et al. (2013). The transition probability pij is calculated using the weights in the weight matrix W and then normalized with respect to the source node vi with the formula pij = w.j l w.l . When the graph random walk is done, we get a list of coarse-ranked candidates, each with a confidence score derived from the context information. By performing the graph random walk, we reduce the search space from exponential to the top-ranked ones. Now we only need to select the final result from the candidates, which we will describe in the next section. 4 Candidate Re-ra</context>
</contexts>
<marker>Sproat, Tao, Zhai, 2006</marker>
<rawString>Sproat, R., Tao, T., and Zhai, C. (2006). Named entity transliteration with comparable corpora. In Proceedings of the 21st International Conference on Computational Linguistics and the 44th annual meeting of the Association for Computational Linguistics, pages 73–80. Association for Computational Linguistics.</rawString>
</citation>
<citation valid="true">
<authors>
<author>X Sun</author>
<author>W Li</author>
<author>F Meng</author>
<author>H Wang</author>
</authors>
<title>Generalized abbreviation prediction with negative full forms and its application on improving chinese web search.</title>
<date>2013</date>
<journal>Asian Federation of Natural Language Processing.</journal>
<booktitle>In Proceedings of the Sixth International Joint Conference on Natural Language Processing,</booktitle>
<pages>641--647</pages>
<location>Nagoya,</location>
<contexts>
<context position="28785" citStr="Sun et al. (2013)" startWordPosition="4941" endWordPosition="4944">eviations generated by statistical approaches. Compared to their approaches, our method only uses a fixed corpus, instead of using collective information, which varies from time to time. Some of the previous work that relate to abbreviations focuses on the task of “abbreviation disambiguation”, which aims to find the correct abbreviation-full pairs. In these works, machine learning approaches are commonly used (Park and Byrd, 2001; HaCohen-Kerner et al., 2008; Yu et al., 2006; Ao and Takagi, 2005). We focus on another aspect. We want to find the abbreviation 1888 given the full form. Besides, Sun et al. (2013) also works on abbreviation prediction but focuses on the negative full form problem, which is a little different from our work. One related research field is text normalization, with many outstanding works (Sproat et al., 2001; Aw et al., 2006; Hassan and Menezes, 2013; Ling et al., 2013; Yang and Eisenstein, 2013). While the two tasks share similarities, abbreviation prediction has its identical characteristics, like the sub-sequence assumption. This results in different methods to tackle the two different problems. 7 Conclusion In this paper, we propose a unified framework for Chinese abbre</context>
</contexts>
<marker>Sun, Li, Meng, Wang, 2013</marker>
<rawString>Sun, X., Li, W., Meng, F., and Wang, H. (2013). Generalized abbreviation prediction with negative full forms and its application on improving chinese web search. In Proceedings of the Sixth International Joint Conference on Natural Language Processing, pages 641–647, Nagoya, Japan. Asian Federation of Natural Language Processing.</rawString>
</citation>
<citation valid="true">
<authors>
<author>X Sun</author>
<author>N Okazaki</author>
<author>J Tsujii</author>
</authors>
<title>Robust approach to abbreviating terms: A discriminative latent variable model with global information.</title>
<date>2009</date>
<booktitle>In Proceedings of the Joint Conference of the 47th Annual Meeting of the ACL and the 4th International Joint Conference on Natural Language Processing of the AFNLP: Volume</booktitle>
<volume>2</volume>
<pages>905--913</pages>
<publisher>Association</publisher>
<institution>for Computational Linguistics.</institution>
<contexts>
<context position="20789" citStr="Sun et al. (2009)" startWordPosition="3620" endWordPosition="3623">he top-5 accuracy of the candidate generation phase Table 4. We can see that, just like the case of using other feature alone, using the score of random walk alone is far from enough. However, the first 5 candidates contain most of the correct answers. We use the top-5 candidates plus another 5 candidates in the re-ranking phase. We choose the character tagging method as the baseline method. The character tagging strategy is widely used in the abbreviation generation task (Tsuruoka et al., 2005; Sun et al., 2008, 2009; Zhang et al., 2012). We choose the ‘SK’ labeling strategy which is used in Sun et al. (2009); Zhang et al. (2012). The ‘SK’ labeling strategy gives each character a label in the character sequence, with ‘S’ represents ’Skip’ and ‘K’ represents ‘Keep’. Same with Zhang et al. (2012), we use the Conditional Random Fields (CRFs) model in the sequence labeling process. The baseline method mainly uses the character context information to generate the candidate abbreviation. To be fair we use the same feature set in Sun et al. (2009); Zhang et al. (2012). One drawback of the sequence labeling method is that it relies heavily on the character context in the full form. With the number of new </context>
<context position="24316" citStr="Sun et al. (2009)" startWordPosition="4203" endWordPosition="4206">decide the parameters. However we can use a very small amount of training data to get a reasonably good model. Figure 2 shows the result of using different size of training data. We can see that the performance of the baseline methods drops rapidly when there are less training data. In contrast, when using less training data, our method does not suffer that much. Figure 2: Top-1 accuracy when changing the size of training data. For example, “50%” means “using 50% of all the training data”. 5.5 Comparison with previous work We compare our method with the method in the previous work DPLVM+GI in Sun et al. (2009), which outperforms Tsuruoka et al. (2005); Sun et al. (2008). We also compare our method with the web-based method CRF+WEB in Zhang et al. (2012). Because the comparison is performed on different corpora, we run the two methods on our data. Table 6 shows the top-1 accuracy. We can see that our method outperforms the previous 1887 methods. System Top-K Accuracy DPLVM+GI 53.29% CRF+WEB 54.02% Our method 55.61% Table 6: Comparison with previous work. The search results of CRF+WEB is based on March 9, 2014 version of the Baidu search engine. 5.6 Error Analysis We perform cross-validation to find </context>
<context position="25603" citStr="Sun et al. (2009)" startWordPosition="4424" endWordPosition="4427"> correspond to more than one acceptable abbreviation. In this case, our method may choose the one that is indeed used as the full form’s abbreviation in news texts, but not the same as the standard reference abbreviations. The reason for this phenomenon may lie in the fact that the verification data we use is news text, which tends to be formal. Therefore when a reference is often used colloquially, our method may miss it. We can relieve this by changing the corpus we use. 2. Our method may provide biased information when handling location sensitive phrases. Not only our system, the system of Sun et al. (2009); Zhang et al. (2012) also shows this phenomenon. An example is the case of “� A ftPH,9” (Democracy League of Hong Kong). Because most of the news is about news in mainland China, it is hard for the model to tell the difference between the reference “A PH,9” and a false candidate “� 9”(Democracy League of China). Another ambiguity is “%Af211&amp;quot;-)C*”(Tsinghua University), which has two abbreviations “%Af )C” and “%Af211&amp;quot;-”. This happens because the full form itself is ambiguous. Word sense disambiguation can be performed first to handle this kind of problem. 6 Related Work Abbreviation generation</context>
<context position="27241" citStr="Sun et al. (2009)" startWordPosition="4691" endWordPosition="4694">is referred to as “abbreviation generation”. There is research on using heuristic rules for generating abbreviations Barrett and Grems (1960); Bourne and Ford (1961); Taghva and Gilbreth (1999); Park and Byrd (2001); Wren et al. (2002); Hearst (2003). Most of them achieved high performance. However, hand-crafted rules are time consuming to create, and it is not easy to transfer the knowledge of rules from one language to another. Recent studies of abbreviation generation have focused on the use of machine learning techniques. Sun et al. (2008) proposed an SVM approach. Tsuruoka et al. (2005); Sun et al. (2009) formalized the process of abbreviation generation as a sequence labeling problem. The drawback of the sequence labeling strategies is that they rely heavily on the character features. This kind of method cannot fit the need for abbreviation generation in social media texts where the amount of abbreviations grows fast. Besides these pure statistical approaches, there are also many approaches using Web as a corpus in machine learning approaches for generating abbreviations. Adar (2004) proposed methods to detect such pairs from biomedical documents. Jain et al. (2007) used web search results as</context>
</contexts>
<marker>Sun, Okazaki, Tsujii, 2009</marker>
<rawString>Sun, X., Okazaki, N., and Tsujii, J. (2009). Robust approach to abbreviating terms: A discriminative latent variable model with global information. In Proceedings of the Joint Conference of the 47th Annual Meeting of the ACL and the 4th International Joint Conference on Natural Language Processing of the AFNLP: Volume 2-Volume 2, pages 905–913. Association for Computational Linguistics.</rawString>
</citation>
<citation valid="true">
<authors>
<author>X Sun</author>
<author>H Wang</author>
<author>B Wang</author>
</authors>
<title>Predicting chinese abbreviations from definitions: An empirical learning approach using support vector regression.</title>
<date>2008</date>
<journal>Journal of Computer Science and Technology,</journal>
<volume>23</volume>
<issue>4</issue>
<contexts>
<context position="19347" citStr="Sun et al., 2008" startWordPosition="3372" endWordPosition="3375"> noun phrases, organization names and some other types. The Chinese Gigaword corpus contains news texts from the year 1992 to 2007. We only collect those pairs whose full form and corresponding abbreviation appear in the same article for at least one time. For full forms with more than one reasonable reference, we keep the most frequently used one as its reference. We use 80% abbreviation pairs as the training data and the rest as the testing data. We use the top-K accuracy as the evaluation metrics. The top-K accuracy is widely used as the measurement in previous work (Tsuruoka et al., 2005; Sun et al., 2008, 2009; Zhang et al., 2012). It measures what percentage of the reference abbreviations are found if we take the top k candidate abbreviations from all the results. In our experiment, we compare the top-5 accuracy with baselines. We choose the top-10 candidates from the graph random walk are considered in re-ranking phase and the measurement used is top-1 accuracy because the final aim of the algorithm is to detect the exact abbreviation, rather than a list of candidates. 5.2 Candidate List Table 3 shows examples of the candidates. In our algorithm we further reduce the search space to only in</context>
<context position="20689" citStr="Sun et al., 2008" startWordPosition="3601" endWordPosition="3604">e 4: Top-5 accuracy of the candidate generation phase 5.3 Comparison with baselines We first show the top-5 accuracy of the candidate generation phase Table 4. We can see that, just like the case of using other feature alone, using the score of random walk alone is far from enough. However, the first 5 candidates contain most of the correct answers. We use the top-5 candidates plus another 5 candidates in the re-ranking phase. We choose the character tagging method as the baseline method. The character tagging strategy is widely used in the abbreviation generation task (Tsuruoka et al., 2005; Sun et al., 2008, 2009; Zhang et al., 2012). We choose the ‘SK’ labeling strategy which is used in Sun et al. (2009); Zhang et al. (2012). The ‘SK’ labeling strategy gives each character a label in the character sequence, with ‘S’ represents ’Skip’ and ‘K’ represents ‘Keep’. Same with Zhang et al. (2012), we use the Conditional Random Fields (CRFs) model in the sequence labeling process. The baseline method mainly uses the character context information to generate the candidate abbreviation. To be fair we use the same feature set in Sun et al. (2009); Zhang et al. (2012). One drawback of the sequence labeling</context>
<context position="24377" citStr="Sun et al. (2008)" startWordPosition="4213" endWordPosition="4216"> of training data to get a reasonably good model. Figure 2 shows the result of using different size of training data. We can see that the performance of the baseline methods drops rapidly when there are less training data. In contrast, when using less training data, our method does not suffer that much. Figure 2: Top-1 accuracy when changing the size of training data. For example, “50%” means “using 50% of all the training data”. 5.5 Comparison with previous work We compare our method with the method in the previous work DPLVM+GI in Sun et al. (2009), which outperforms Tsuruoka et al. (2005); Sun et al. (2008). We also compare our method with the web-based method CRF+WEB in Zhang et al. (2012). Because the comparison is performed on different corpora, we run the two methods on our data. Table 6 shows the top-1 accuracy. We can see that our method outperforms the previous 1887 methods. System Top-K Accuracy DPLVM+GI 53.29% CRF+WEB 54.02% Our method 55.61% Table 6: Comparison with previous work. The search results of CRF+WEB is based on March 9, 2014 version of the Baidu search engine. 5.6 Error Analysis We perform cross-validation to find the errors and list the two major errors below: 1. Some full </context>
<context position="27173" citStr="Sun et al. (2008)" startWordPosition="4678" endWordPosition="4681">h is to find the reference for a full form given the context, which is referred to as “abbreviation generation”. There is research on using heuristic rules for generating abbreviations Barrett and Grems (1960); Bourne and Ford (1961); Taghva and Gilbreth (1999); Park and Byrd (2001); Wren et al. (2002); Hearst (2003). Most of them achieved high performance. However, hand-crafted rules are time consuming to create, and it is not easy to transfer the knowledge of rules from one language to another. Recent studies of abbreviation generation have focused on the use of machine learning techniques. Sun et al. (2008) proposed an SVM approach. Tsuruoka et al. (2005); Sun et al. (2009) formalized the process of abbreviation generation as a sequence labeling problem. The drawback of the sequence labeling strategies is that they rely heavily on the character features. This kind of method cannot fit the need for abbreviation generation in social media texts where the amount of abbreviations grows fast. Besides these pure statistical approaches, there are also many approaches using Web as a corpus in machine learning approaches for generating abbreviations. Adar (2004) proposed methods to detect such pairs from</context>
</contexts>
<marker>Sun, Wang, Wang, 2008</marker>
<rawString>Sun, X., Wang, H., and Wang, B. (2008). Predicting chinese abbreviations from definitions: An empirical learning approach using support vector regression. Journal of Computer Science and Technology, 23(4):602–611.</rawString>
</citation>
<citation valid="true">
<authors>
<author>K Taghva</author>
<author>J Gilbreth</author>
</authors>
<title>Recognizing acronyms and their definitions.</title>
<date>1999</date>
<journal>International Journal on Document Analysis and Recognition,</journal>
<volume>1</volume>
<issue>4</issue>
<contexts>
<context position="26817" citStr="Taghva and Gilbreth (1999)" startWordPosition="4619" endWordPosition="4622">n generation has been studied during recent years. At first, some approaches maintain a database of abbreviations and their corresponding “full form” pairs. The major problem of pure database-building approach is obvious. It is impossible to cover all abbreviations, and the building process is quite laborious. To find these pairs automatically, a powerful approach is to find the reference for a full form given the context, which is referred to as “abbreviation generation”. There is research on using heuristic rules for generating abbreviations Barrett and Grems (1960); Bourne and Ford (1961); Taghva and Gilbreth (1999); Park and Byrd (2001); Wren et al. (2002); Hearst (2003). Most of them achieved high performance. However, hand-crafted rules are time consuming to create, and it is not easy to transfer the knowledge of rules from one language to another. Recent studies of abbreviation generation have focused on the use of machine learning techniques. Sun et al. (2008) proposed an SVM approach. Tsuruoka et al. (2005); Sun et al. (2009) formalized the process of abbreviation generation as a sequence labeling problem. The drawback of the sequence labeling strategies is that they rely heavily on the character f</context>
</contexts>
<marker>Taghva, Gilbreth, 1999</marker>
<rawString>Taghva, K. and Gilbreth, J. (1999). Recognizing acronyms and their definitions. International Journal on Document Analysis and Recognition, 1(4):191–198.</rawString>
</citation>
<citation valid="true">
<authors>
<author>Y Tsuruoka</author>
<author>S Ananiadou</author>
<author>J Tsujii</author>
</authors>
<title>A machine learning approach to acronym generation.</title>
<date>2005</date>
<booktitle>In Proceedings of the ACL-ISMB Workshop on Linking Biological Literature, Ontologies and Databases: Mining Biological Semantics,</booktitle>
<pages>25--31</pages>
<publisher>Association</publisher>
<institution>for Computational Linguistics.</institution>
<contexts>
<context position="19329" citStr="Tsuruoka et al., 2005" startWordPosition="3368" endWordPosition="3371">reviation pairs include noun phrases, organization names and some other types. The Chinese Gigaword corpus contains news texts from the year 1992 to 2007. We only collect those pairs whose full form and corresponding abbreviation appear in the same article for at least one time. For full forms with more than one reasonable reference, we keep the most frequently used one as its reference. We use 80% abbreviation pairs as the training data and the rest as the testing data. We use the top-K accuracy as the evaluation metrics. The top-K accuracy is widely used as the measurement in previous work (Tsuruoka et al., 2005; Sun et al., 2008, 2009; Zhang et al., 2012). It measures what percentage of the reference abbreviations are found if we take the top k candidate abbreviations from all the results. In our experiment, we compare the top-5 accuracy with baselines. We choose the top-10 candidates from the graph random walk are considered in re-ranking phase and the measurement used is top-1 accuracy because the final aim of the algorithm is to detect the exact abbreviation, rather than a list of candidates. 5.2 Candidate List Table 3 shows examples of the candidates. In our algorithm we further reduce the searc</context>
<context position="20671" citStr="Tsuruoka et al., 2005" startWordPosition="3597" endWordPosition="3600"> 4 63.70% 5 73.60% Table 4: Top-5 accuracy of the candidate generation phase 5.3 Comparison with baselines We first show the top-5 accuracy of the candidate generation phase Table 4. We can see that, just like the case of using other feature alone, using the score of random walk alone is far from enough. However, the first 5 candidates contain most of the correct answers. We use the top-5 candidates plus another 5 candidates in the re-ranking phase. We choose the character tagging method as the baseline method. The character tagging strategy is widely used in the abbreviation generation task (Tsuruoka et al., 2005; Sun et al., 2008, 2009; Zhang et al., 2012). We choose the ‘SK’ labeling strategy which is used in Sun et al. (2009); Zhang et al. (2012). The ‘SK’ labeling strategy gives each character a label in the character sequence, with ‘S’ represents ’Skip’ and ‘K’ represents ‘Keep’. Same with Zhang et al. (2012), we use the Conditional Random Fields (CRFs) model in the sequence labeling process. The baseline method mainly uses the character context information to generate the candidate abbreviation. To be fair we use the same feature set in Sun et al. (2009); Zhang et al. (2012). One drawback of the</context>
<context position="24358" citStr="Tsuruoka et al. (2005)" startWordPosition="4209" endWordPosition="4212"> use a very small amount of training data to get a reasonably good model. Figure 2 shows the result of using different size of training data. We can see that the performance of the baseline methods drops rapidly when there are less training data. In contrast, when using less training data, our method does not suffer that much. Figure 2: Top-1 accuracy when changing the size of training data. For example, “50%” means “using 50% of all the training data”. 5.5 Comparison with previous work We compare our method with the method in the previous work DPLVM+GI in Sun et al. (2009), which outperforms Tsuruoka et al. (2005); Sun et al. (2008). We also compare our method with the web-based method CRF+WEB in Zhang et al. (2012). Because the comparison is performed on different corpora, we run the two methods on our data. Table 6 shows the top-1 accuracy. We can see that our method outperforms the previous 1887 methods. System Top-K Accuracy DPLVM+GI 53.29% CRF+WEB 54.02% Our method 55.61% Table 6: Comparison with previous work. The search results of CRF+WEB is based on March 9, 2014 version of the Baidu search engine. 5.6 Error Analysis We perform cross-validation to find the errors and list the two major errors b</context>
<context position="27222" citStr="Tsuruoka et al. (2005)" startWordPosition="4687" endWordPosition="4690">iven the context, which is referred to as “abbreviation generation”. There is research on using heuristic rules for generating abbreviations Barrett and Grems (1960); Bourne and Ford (1961); Taghva and Gilbreth (1999); Park and Byrd (2001); Wren et al. (2002); Hearst (2003). Most of them achieved high performance. However, hand-crafted rules are time consuming to create, and it is not easy to transfer the knowledge of rules from one language to another. Recent studies of abbreviation generation have focused on the use of machine learning techniques. Sun et al. (2008) proposed an SVM approach. Tsuruoka et al. (2005); Sun et al. (2009) formalized the process of abbreviation generation as a sequence labeling problem. The drawback of the sequence labeling strategies is that they rely heavily on the character features. This kind of method cannot fit the need for abbreviation generation in social media texts where the amount of abbreviations grows fast. Besides these pure statistical approaches, there are also many approaches using Web as a corpus in machine learning approaches for generating abbreviations. Adar (2004) proposed methods to detect such pairs from biomedical documents. Jain et al. (2007) used we</context>
</contexts>
<marker>Tsuruoka, Ananiadou, Tsujii, 2005</marker>
<rawString>Tsuruoka, Y., Ananiadou, S., and Tsujii, J. (2005). A machine learning approach to acronym generation. In Proceedings of the ACL-ISMB Workshop on Linking Biological Literature, Ontologies and Databases: Mining Biological Semantics, pages 25–31. Association for Computational Linguistics.</rawString>
</citation>
<citation valid="true">
<authors>
<author>J Wren</author>
<author>H Garner</author>
</authors>
<title>Heuristics for identification of acronym-definition patterns within text: towards an automated construction of comprehensive acronym-definition dictionaries. Methods of information in medicine,</title>
<date>2002</date>
<pages>41--5</pages>
<marker>Wren, Garner, 2002</marker>
<rawString>Wren, J., Garner, H., et al. (2002). Heuristics for identification of acronym-definition patterns within text: towards an automated construction of comprehensive acronym-definition dictionaries. Methods of information in medicine, 41(5):426–434.</rawString>
</citation>
<citation valid="true">
<authors>
<author>Y Yang</author>
<author>J Eisenstein</author>
</authors>
<title>A log-linear model for unsupervised text normalization.</title>
<date>2013</date>
<booktitle>In Proceedings of the 2013 Conference on Empirical Methods in Natural Language Processing,</booktitle>
<pages>61--72</pages>
<publisher>Association</publisher>
<institution>for Computational Linguistics.</institution>
<location>Seattle, Washington, USA.</location>
<contexts>
<context position="29102" citStr="Yang and Eisenstein, 2013" startWordPosition="4993" endWordPosition="4996">o find the correct abbreviation-full pairs. In these works, machine learning approaches are commonly used (Park and Byrd, 2001; HaCohen-Kerner et al., 2008; Yu et al., 2006; Ao and Takagi, 2005). We focus on another aspect. We want to find the abbreviation 1888 given the full form. Besides, Sun et al. (2013) also works on abbreviation prediction but focuses on the negative full form problem, which is a little different from our work. One related research field is text normalization, with many outstanding works (Sproat et al., 2001; Aw et al., 2006; Hassan and Menezes, 2013; Ling et al., 2013; Yang and Eisenstein, 2013). While the two tasks share similarities, abbreviation prediction has its identical characteristics, like the sub-sequence assumption. This results in different methods to tackle the two different problems. 7 Conclusion In this paper, we propose a unified framework for Chinese abbreviation generation. Our approach contains two stages: candidate generation and re-ranking. Given a long term, we first generate a list of abbreviation candidates using the cooccurrence information. We give a coarse-grained rank using graph random walk to reduce the search space. After we get the candidate lists, we </context>
</contexts>
<marker>Yang, Eisenstein, 2013</marker>
<rawString>Yang, Y. and Eisenstein, J. (2013). A log-linear model for unsupervised text normalization. In Proceedings of the 2013 Conference on Empirical Methods in Natural Language Processing, pages 61–72, Seattle, Washington, USA. Association for Computational Linguistics.</rawString>
</citation>
<citation valid="true">
<authors>
<author>H Yu</author>
<author>W Kim</author>
<author>V Hatzivassiloglou</author>
<author>J Wilbur</author>
</authors>
<title>A large scale, corpus-based approach for automatically disambiguating biomedical abbreviations.</title>
<date>2006</date>
<journal>ACM Transactions on Information Systems (TOIS),</journal>
<volume>24</volume>
<issue>3</issue>
<contexts>
<context position="28648" citStr="Yu et al., 2006" startWordPosition="4916" endWordPosition="4919">ches do not have access to search engine internals. Zhang et al. (2012) used web search engine information to rerank the candidate abbreviations generated by statistical approaches. Compared to their approaches, our method only uses a fixed corpus, instead of using collective information, which varies from time to time. Some of the previous work that relate to abbreviations focuses on the task of “abbreviation disambiguation”, which aims to find the correct abbreviation-full pairs. In these works, machine learning approaches are commonly used (Park and Byrd, 2001; HaCohen-Kerner et al., 2008; Yu et al., 2006; Ao and Takagi, 2005). We focus on another aspect. We want to find the abbreviation 1888 given the full form. Besides, Sun et al. (2013) also works on abbreviation prediction but focuses on the negative full form problem, which is a little different from our work. One related research field is text normalization, with many outstanding works (Sproat et al., 2001; Aw et al., 2006; Hassan and Menezes, 2013; Ling et al., 2013; Yang and Eisenstein, 2013). While the two tasks share similarities, abbreviation prediction has its identical characteristics, like the sub-sequence assumption. This result</context>
</contexts>
<marker>Yu, Kim, Hatzivassiloglou, Wilbur, 2006</marker>
<rawString>Yu, H., Kim, W., Hatzivassiloglou, V., and Wilbur, J. (2006). A large scale, corpus-based approach for automatically disambiguating biomedical abbreviations. ACM Transactions on Information Systems (TOIS), 24(3):380–404.</rawString>
</citation>
<citation valid="true">
<authors>
<author>L Zhang</author>
<author>S Li</author>
<author>H Wang</author>
<author>N Sun</author>
<author>X Meng</author>
</authors>
<title>Constructing Chinese abbreviation dictionary: A stacked approach.</title>
<date>2012</date>
<booktitle>In Proceedings of COLING 2012,</booktitle>
<pages>3055--3070</pages>
<location>Mumbai,</location>
<contexts>
<context position="6370" citStr="Zhang et al. (2012)" startWordPosition="1023" endWordPosition="1026">ersity), whose abbreviations correspond to “IL &apos;” and ‘M It&amp;quot;’ respectively. Although sharing a similar character context, the third character ‘&apos;’ is kept in the first case and is skipped in the second case. We believe that a better way is to extract these abbreviation-full pairs from a natural text corpus where the full form and its abbreviation co-exist. Therefore we propose a two stage method. The first stage generates a list of candidates given a large corpus. To reduce the search space, we adopt 1Details of the difference between English and Chinese abbreviation prediction can be found in Zhang et al. (2012). Full form ff / &apos; Status Skip Keep Keep Skip Result / &apos; Table 1: The abbreviation “/&apos;” of the full form “�/&apos;*” (Hong Kong University) graph random walk to give a coarse-grained ranking and select the top-ranked ones as the candidates. Then we use a similarity sensitive reranking method to decide the final result. Detailed description of the two parts is shown in the following sections. 3 Candidate Generation through Graph Random Walk 3.1 Candidate Generation and Graph Representation Chinese abbreviations are sub-sequences of the full form. We use a brute force method to select all strings in </context>
<context position="19374" citStr="Zhang et al., 2012" startWordPosition="3377" endWordPosition="3380">ion names and some other types. The Chinese Gigaword corpus contains news texts from the year 1992 to 2007. We only collect those pairs whose full form and corresponding abbreviation appear in the same article for at least one time. For full forms with more than one reasonable reference, we keep the most frequently used one as its reference. We use 80% abbreviation pairs as the training data and the rest as the testing data. We use the top-K accuracy as the evaluation metrics. The top-K accuracy is widely used as the measurement in previous work (Tsuruoka et al., 2005; Sun et al., 2008, 2009; Zhang et al., 2012). It measures what percentage of the reference abbreviations are found if we take the top k candidate abbreviations from all the results. In our experiment, we compare the top-5 accuracy with baselines. We choose the top-10 candidates from the graph random walk are considered in re-ranking phase and the measurement used is top-1 accuracy because the final aim of the algorithm is to detect the exact abbreviation, rather than a list of candidates. 5.2 Candidate List Table 3 shows examples of the candidates. In our algorithm we further reduce the search space to only incorporate 10 candidates fro</context>
<context position="20716" citStr="Zhang et al., 2012" startWordPosition="3606" endWordPosition="3609">he candidate generation phase 5.3 Comparison with baselines We first show the top-5 accuracy of the candidate generation phase Table 4. We can see that, just like the case of using other feature alone, using the score of random walk alone is far from enough. However, the first 5 candidates contain most of the correct answers. We use the top-5 candidates plus another 5 candidates in the re-ranking phase. We choose the character tagging method as the baseline method. The character tagging strategy is widely used in the abbreviation generation task (Tsuruoka et al., 2005; Sun et al., 2008, 2009; Zhang et al., 2012). We choose the ‘SK’ labeling strategy which is used in Sun et al. (2009); Zhang et al. (2012). The ‘SK’ labeling strategy gives each character a label in the character sequence, with ‘S’ represents ’Skip’ and ‘K’ represents ‘Keep’. Same with Zhang et al. (2012), we use the Conditional Random Fields (CRFs) model in the sequence labeling process. The baseline method mainly uses the character context information to generate the candidate abbreviation. To be fair we use the same feature set in Sun et al. (2009); Zhang et al. (2012). One drawback of the sequence labeling method is that it relies h</context>
<context position="24462" citStr="Zhang et al. (2012)" startWordPosition="4228" endWordPosition="4231">g different size of training data. We can see that the performance of the baseline methods drops rapidly when there are less training data. In contrast, when using less training data, our method does not suffer that much. Figure 2: Top-1 accuracy when changing the size of training data. For example, “50%” means “using 50% of all the training data”. 5.5 Comparison with previous work We compare our method with the method in the previous work DPLVM+GI in Sun et al. (2009), which outperforms Tsuruoka et al. (2005); Sun et al. (2008). We also compare our method with the web-based method CRF+WEB in Zhang et al. (2012). Because the comparison is performed on different corpora, we run the two methods on our data. Table 6 shows the top-1 accuracy. We can see that our method outperforms the previous 1887 methods. System Top-K Accuracy DPLVM+GI 53.29% CRF+WEB 54.02% Our method 55.61% Table 6: Comparison with previous work. The search results of CRF+WEB is based on March 9, 2014 version of the Baidu search engine. 5.6 Error Analysis We perform cross-validation to find the errors and list the two major errors below: 1. Some full forms may correspond to more than one acceptable abbreviation. In this case, our meth</context>
<context position="28104" citStr="Zhang et al. (2012)" startWordPosition="4831" endWordPosition="4834">neration in social media texts where the amount of abbreviations grows fast. Besides these pure statistical approaches, there are also many approaches using Web as a corpus in machine learning approaches for generating abbreviations. Adar (2004) proposed methods to detect such pairs from biomedical documents. Jain et al. (2007) used web search results as well as search logs to find and rank abbreviates full pairs, which show good result. The disadvantage is that search log data is only available in a search engine backend. The ordinary approaches do not have access to search engine internals. Zhang et al. (2012) used web search engine information to rerank the candidate abbreviations generated by statistical approaches. Compared to their approaches, our method only uses a fixed corpus, instead of using collective information, which varies from time to time. Some of the previous work that relate to abbreviations focuses on the task of “abbreviation disambiguation”, which aims to find the correct abbreviation-full pairs. In these works, machine learning approaches are commonly used (Park and Byrd, 2001; HaCohen-Kerner et al., 2008; Yu et al., 2006; Ao and Takagi, 2005). We focus on another aspect. We w</context>
</contexts>
<marker>Zhang, Li, Wang, Sun, Meng, 2012</marker>
<rawString>Zhang, L., Li, S., Wang, H., Sun, N., and Meng, X. (2012). Constructing Chinese abbreviation dictionary: A stacked approach. In Proceedings of COLING 2012, pages 3055–3070, Mumbai, India. The COLING 2012 Organizing Committee.</rawString>
</citation>
<citation valid="true">
<authors>
<author>X Zhu</author>
<author>Z Ghahramani</author>
<author>J Lafferty</author>
</authors>
<title>Semi-supervised learning using gaussian fields and harmonic functions.</title>
<date>2003</date>
<booktitle>In ICML,</booktitle>
<volume>3</volume>
<pages>912--919</pages>
<contexts>
<context position="10294" citStr="Zhu et al. (2003)" startWordPosition="1716" endWordPosition="1719"> In random walk, a walker starts from the full form source node S (in later steps, vi) and randomly walks to another node vj with a transition probability pij. In random walk we assume the walker do the walking n times and finally stops at a final node. When the walking is done, we can get the probability of each node that the walker stops in the end. Because the destination of each step is selected based on transition probabilities, the word node that shares more similar contexts are more likely to be the final stop. The random walk method we use is similar to those defined in Norris (1998); Zhu et al. (2003); Sproat et al. (2006); Hassan and Menezes (2013); Li et al. (2013). The transition probability pij is calculated using the weights in the weight matrix W and then normalized with respect to the source node vi with the formula pij = w.j l w.l . When the graph random walk is done, we get a list of coarse-ranked candidates, each with a confidence score derived from the context information. By performing the graph random walk, we reduce the search space from exponential to the top-ranked ones. Now we only need to select the final result from the candidates, which we will describe in the next sect</context>
</contexts>
<marker>Zhu, Ghahramani, Lafferty, 2003</marker>
<rawString>Zhu, X., Ghahramani, Z., Lafferty, J., et al. (2003). Semi-supervised learning using gaussian fields and harmonic functions. In ICML, volume 3, pages 912–919.</rawString>
</citation>
</citationList>
</algorithm>
</algorithms>
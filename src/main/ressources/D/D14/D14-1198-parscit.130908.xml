<?xml version="1.0" encoding="UTF-8"?>
<algorithms version="110505">
<algorithm name="SectLabel" version="110505">
<variant no="0" confidence="0.000661">
<title confidence="0.994657">
Constructing Information Networks Using One Single Model
</title>
<author confidence="0.998529">
Qi Li† Heng Ji† Yu Hong†$ Sujian Li¶
</author>
<affiliation confidence="0.990983666666667">
†Computer Science Department, Rensselaer Polytechnic Institute, USA
$School of Computer Science and Technology, Soochow University, China
¶Key Laboratory of Computational Linguistics, Peking University, MOE, China
</affiliation>
<email confidence="0.992969">
†{liq7,hongy2,jih}@rpi.edu, ¶lisujian@pku.edu.cn
</email>
<sectionHeader confidence="0.9973" genericHeader="abstract">
Abstract
</sectionHeader>
<bodyText confidence="0.9996406875">
In this paper, we propose a new frame-
work that unifies the output of three infor-
mation extraction (IE) tasks - entity men-
tions, relations and events as an informa-
tion network representation, and extracts
all of them using one single joint model
based on structured prediction. This novel
formulation allows different parts of the
information network fully interact with
each other. For example, many rela-
tions can now be considered as the re-
sultant states of events. Our approach
achieves substantial improvements over
traditional pipelined approaches, and sig-
nificantly advances state-of-the-art end-to-
end event argument extraction.
</bodyText>
<sectionHeader confidence="0.999516" genericHeader="keywords">
1 Introduction
</sectionHeader>
<bodyText confidence="0.999714454545455">
Information extraction (IE) aims to discover entity
mentions, relations and events from unstructured
texts, and these three subtasks are closely inter-
dependent: entity mentions are core components
of relations and events, and the extraction of rela-
tions and events can help to accurately recognize
entity mentions. In addition, the theory of eventu-
alities (D¨olling, 2011) suggested that relations can
be viewed as states that events start from and result
in. Therefore, it is intuitive but challenging to ex-
tract all of them simultaneously in a single model.
Some recent research attempted to jointly model
multiple IE subtasks (e.g., (Roth and Yih, 2007;
Riedel and McCallum, 2011; Yang and Cardie,
2013; Riedel et al., 2009; Singh et al., 2013; Li et
al., 2013; Li and Ji, 2014)). For example, Roth and
Yih (2007) conducted joint inference over entity
mentions and relations; Our previous work jointly
extracted event triggers and arguments (Li et al.,
2013), and entity mentions and relations (Li and
Ji, 2014). However, a single model that can ex-
tract all of them has never been studied so far.
</bodyText>
<figure confidence="0.749696666666667">
Physical
Asif Mohammed Hanif detonated explosives in Tel Aviv
x: x1 x2 x3 x4 x5 x6 x7 x8
</figure>
<figureCaption confidence="0.763079333333333">
Figure 1: Information Network Representation.
Information nodes are denoted by rectangles. Ar-
rows represent information arcs.
</figureCaption>
<bodyText confidence="0.999545333333333">
For the first time, we uniformly represent the IE
output from each sentence as an information net-
work, where entity mentions and event triggers are
nodes, relations and event-argument links are arcs.
We apply a structured perceptron framework with
a segment-based beam-search algorithm to con-
struct the information networks (Collins, 2002; Li
et al., 2013; Li and Ji, 2014). In addition to the per-
ceptron update, we also apply k-best MIRA (Mc-
Donald et al., 2005), which refines the perceptron
update in three aspects: it is flexible in using var-
ious loss functions, it is a large-margin approach,
and it can use mulitple candidate structures to tune
feature weights.
In an information network, we can capture the
interactions among multiple nodes by learning
joint features during training. In addition to the
cross-component dependencies studied in (Li et
al., 2013; Li and Ji, 2014), we are able to cap-
ture interactions between relations and events. For
example, in Figure 1, if we know that the Person
mention “Asif Mohammed Hanif ” is an Attacker
of the Attack event triggered by “detonated”, and
the Weapon mention “explosives” is an Instrument,
we can infer that there exists an Agent-Artifact
relation between them. Similarly we can infer
the Physical relation between “Asif Mohammed
Hanif” and “Tel Aviv”.
However, in practice many useful interactions
are missing during testing because of the data spar-
</bodyText>
<figure confidence="0.996769666666667">
V:
Person
Attacker
Agent-Artifact
Attack
Instrument
Weapon
Place
Geopolitical Entity
</figure>
<page confidence="0.951248">
1846
</page>
<bodyText confidence="0.972527846153846">
Proceedings of the 2014 Conference on Empirical Methods in Natural Language Processing (EMNLP), pages 1846–1851,
October 25-29, 2014, Doha, Qatar. c�2014 Association for Computational Linguistics
sity problem of event triggers. We observe that
21.5% of event triggers appear fewer than twice in
the ACE’051 training data. By using only lexical
and syntactic features we are not able to discover
the corresponding nodes and their connections. To
tackle this problem, we use FrameNet (Baker and
Sato, 2003) to generalize event triggers so that
semantically similar triggers are clustered in the
same frame.
The following sections will elaborate the de-
tailed implementation of our new framework.
</bodyText>
<sectionHeader confidence="0.986515" genericHeader="introduction">
2 Approach
</sectionHeader>
<bodyText confidence="0.997334882352941">
We uniformly represent the IE output from each
sentence as an information network y = (V, E).
Each node vi E V is represented as a triple
(ui, vi, ti) of start index ui, end index vi, and node
type ti. A node can be an entity mention or an
event trigger. A particular type of node is 1 (nei-
ther entity mention nor event trigger), whose max-
imal length is always 1. Similarly, each infor-
mation arc ej E E is represented as (uj, vj, rj),
where uj and vj are the end offsets of the nodes,
and rj is the arc type. For instance, in Fig-
ure 1, the event trigger “detonated” is represented
as (4, 4, Attack), the entity mention “Asif Mo-
hammed Hanif” is represented as (1, 3, Person),
and their argument arc is (4, 3, Attacker). Our
goal is to extract the whole information network y
for a given sentence x.
</bodyText>
<subsectionHeader confidence="0.994787">
2.1 Decoding Algorithm
</subsectionHeader>
<bodyText confidence="0.999890666666667">
Our joint decoding algorithm is based on ex-
tending the segment-based algorithm described in
our previous work (Li and Ji, 2014). Let x =
(x1, ..., xm) be the input sentence. The decoder
performs two types of actions at each token xi
from left to right:
</bodyText>
<listItem confidence="0.993389571428571">
• NODEACTION(i, j): appends a new node
(j, i, t) ending at the i-th token, where i − dt &lt;
j &lt; i, and dt is the maximal length of type-t
nodes in training data.
• ARCACTION(i, j): for each j &lt; i, incremen-
tally creates a new arc between the nodes ending
at the j-th and i-th tokens respectively: (i, j, r).
</listItem>
<bodyText confidence="0.978295">
After each action, the top-k hypotheses are se-
lected according to their features f(x, y&apos;) and
</bodyText>
<footnote confidence="0.776459">
1http://www.itl.nist.gov/iad/mig//tests/ace
</footnote>
<equation confidence="0.965157666666667">
weights w:
bestk f(x, y&apos;) • w
y&apos;Ebuffer
</equation>
<bodyText confidence="0.999967214285714">
Since a relation can only occur between a pair of
entity mentions, an argument arc can only occur
between an entity mention and an event trigger,
and each edge must obey certain entity type con-
straints, during the search we prune invalid AR-
CACTIONs by checking the types of the nodes
ending at the j-th and the i-th tokens. Finally, the
top hypothesis in the beam is returned as the final
prediction. The upper-bound time complexity of
the decoding algorithm is O(d • b • m2), where d
is the maximum size of nodes, b is the beam size,
and m is the sentence length. The actual execution
time is much shorter, especially when entity type
constraints are applied.
</bodyText>
<subsectionHeader confidence="0.994497">
2.2 Parameter Estimation
</subsectionHeader>
<bodyText confidence="0.999442333333333">
For each training instance (x, y), the structured
perceptron algorithm seeks the assignment with
the highest model score:
</bodyText>
<equation confidence="0.9953465">
z = argmax f(x, y&apos;) • w
y&apos;EY(x)
</equation>
<bodyText confidence="0.879279">
and then updates the feature weights by using:
</bodyText>
<equation confidence="0.928091">
wnew = w + f(x, y) − f(x, z)
</equation>
<bodyText confidence="0.9999723125">
We relax the exact inference problem by the afore-
mentioned beam-search procedure. The stan-
dard perceptron will cause invalid updates be-
cause of inexact search. Therefore we apply early-
update (Collins and Roark, 2004), an instance of
violation-fixing methods (Huang et al., 2012). In
the rest of this paper, we override y and z to denote
prefixes of structures.
In addition to the simple perceptron update, we
also apply k-best MIRA (McDonald et al., 2005),
an online large-margin learning algorithm. During
each update, it keeps the norm of the change to
feature weights w as small as possible, and forces
the margin between y and the k-best candidate z
greater or equal to their loss L(y, z). It is formu-
lated as a quadratic programming problem:
</bodyText>
<equation confidence="0.824316">
min 11wnew − w11
s.t. wnewf(x, y) − wnewf(x, z) &gt; L(y, z)
bz E bestk(x, w)
</equation>
<bodyText confidence="0.997927">
We employ the following three loss functions
for comparison:
</bodyText>
<page confidence="0.97644">
1847
</page>
<table confidence="0.998801333333333">
Freq. Relation Type Event Type Arg-1 Arg-2 Example
159 Physical Transport Artifact Destination He(arg-1) was escorted(trigger) into Iraq(arg-2).
46 Physical Attack Target Place Many people(arg-1) were in the cafe(arg-2) during the blast(trigger).
42 Agent-Artifact Attack Attacker Instrument Terrorists(arg-1) might use(trigger) the devices(arg-2) as weapons.
41 Physical Transport Artifact Origin The truck(arg-1) was carrying(trigger) Syrians fleeing the war in Iraq(arg-2).
33 Physical Meet Entity Place They(arg-1) have reunited(trigger) with their friends in Norfolk(arg-2).
32 Physical Die Victim Place Two Marines(arg-1) were killed(trigger) in the fighting in Kut(arg-2).
28 Physical Attack Attacker Place Protesters(arg-1) have been clashing(trigger) with police in Tehran(arg-2).
26 ORG-Affiliation End-Position Person Entity NBC(arg-2) is terminating(trigger) freelance reporter Peter Arnett(arg-1).
</table>
<tableCaption confidence="0.999218">
Table 1: Frequent overlapping relation and event types in the training set.
</tableCaption>
<listItem confidence="0.599299">
• The first one is F1 loss:
</listItem>
<equation confidence="0.807102">
|y |+ |z|
</equation>
<bodyText confidence="0.999520333333333">
When counting the numbers, we treat each node
and arc as a single unit. For example, in Fig-
ure 1,|y |= 6.
</bodyText>
<listItem confidence="0.7813032">
• The second one is 0-1 loss:
It does not discriminate the extent to which z
deviates from y.
• The third loss function counts the difference be-
tween y and z:
</listItem>
<equation confidence="0.746544">
L3(y, z) = |y |+ |z |− 2 · |y ∩ z|
</equation>
<bodyText confidence="0.992222">
Similar to F1 loss function, it penalizes both
missing and false-positive units. The difference
is that it is sensitive to the size of y and z.
</bodyText>
<subsectionHeader confidence="0.998962">
2.3 Joint Relation-Event Features
</subsectionHeader>
<bodyText confidence="0.999746352941177">
By extracting three core IE components in a joint
search space, we can utilize joint features over
multiple components in addition to factorized fea-
tures in pipelined approaches. In addition to the
features as described in (Li et al., 2013; Li and
Ji, 2014), we can make use of joint features be-
tween relations and events, given the fact that
relations are often ending or starting states of
events (D¨olling, 2011). Table 1 shows the most
frequent overlapping relation and event types in
our training data. In each partial structure y&apos; dur-
ing the search, if both arguments of a relation par-
ticipate in an event, we compose the correspond-
ing argument roles and relation type as a joint fea-
ture for y&apos;. For example, for the structure in Fig-
ure 1, we obtain the following joint relation-event
features:
</bodyText>
<table confidence="0.918411833333333">
Agent-Artifact
Attacker Instrument
Split Sentences Mentions Relations Triggers Arguments
Train 7.2k 25.7k 4.8k 2.8k 4.5k
Dev 1.7k 6.3k 1.2k 0.7k 1.1k
Test 1.5k 5.3k 1.1k 0.6k 1.0k
</table>
<tableCaption confidence="0.995067">
Table 2: Data set
</tableCaption>
<figure confidence="0.993161">
00 20 40 60 80 100
Number of instances
</figure>
<figureCaption confidence="0.999831">
Figure 2: Distribution of triggers and their frames.
</figureCaption>
<subsectionHeader confidence="0.992416">
2.4 Semantic Frame Features
</subsectionHeader>
<bodyText confidence="0.989341333333333">
One major challenge of constructing information
networks is the data sparsity problem in extract-
ing event triggers. For instance, in the sen-
tence: “Others were mutilated beyond recogni-
tion.” The Injure trigger “mutilated” does not oc-
cur in our training data. But there are some sim-
ilar words such as “stab” and “smash”. We uti-
lize FrameNet (Baker and Sato, 2003) to solve
this problem. FrameNet is a lexical resource for
semantic frames. Each frame characterizes a ba-
sic type of semantic concept, and contains a num-
ber of words (lexical units) that evoke the frame.
Many frames are highly related with ACE events.
For example, the frame “Cause harm” is closely
related with Injure event and contains 68 lexical
units such as “stab”, “smash” and “mutilate”.
Figure 2 compares the distributions of trigger
words and their frame IDs in the training data. We
can clearly see that the trigger word distribution
suffers from the long-tail problem, while Frames
reduce the number of triggers which occur only
</bodyText>
<figure confidence="0.875335941176471">
�
1 y =6 z
L2(y, z) =
0 y = z
Trigger Words
Frame IDs
2 ·|y ∩ z|
L1(y, z) = 1 −
Physical
Attacker Place
Frequency 14
12
10
8
6
4
2
</figure>
<page confidence="0.965576">
1848
</page>
<table confidence="0.997428777777778">
Methods Entity Mention (%) Relation (%) Event Trigger (%) Event Argument (%)
P R F1 P R F1 P R F1 P R F1
Pipelined Baseline 83.6 75.7 79.5 68.5 41.4 51.6 71.2 58.7 64.4 64.8 24.6 35.7
Pipeline + Li et al. (2013) N/A 74.5 56.9 64.5 67.5 31.6 43.1
Li and Ji (2014) 85.2 76.9 80.8 68.9 41.9 52.1 N/A
Joint w/ Avg. Perceptron 85.1 77.3 81.0 70.5 41.2 52.0 67.9 62.8 65.3 64.7 35.3 45.6
Joint w/ MIRA w/ F1 Loss 83.1 75.3 79.0 65.5 39.4 49.2 59.6 63.5 61.5 60.6 38.9 47.4
Joint w/ MIRA w/ 0-1 Loss 84.2 76.1 80.0 65.4 41.8 51.0 65.6 61.0 63.2 60.5 39.6 47.9
Joint w/ MIRA w/ L3 Loss 85.3 76.5 80.7 70.8 42.1 52.8 70.3 60.9 65.2 66.4 36.1 46.8
</table>
<tableCaption confidence="0.999939">
Table 3: Overall performance on test set.
</tableCaption>
<bodyText confidence="0.999923375">
once in the training data from 100 to 60 and al-
leviate the sparsity problem. For each token, we
exploit the frames that contain the combination of
its lemma and POS tag as features. For the above
example, “Cause harm” will be a feature for “mu-
tilated”. We only consider tokens that appear in
at most 2 frames, and omit the frames that occur
fewer than 20 times in our training data.
</bodyText>
<sectionHeader confidence="0.999928" genericHeader="method">
3 Experiments
</sectionHeader>
<subsectionHeader confidence="0.999849">
3.1 Data and Evaluation
</subsectionHeader>
<bodyText confidence="0.999569">
We use ACE’05 corpus to evaluate our method
with the same data split as in (Li and Ji, 2014). Ta-
ble 2 summarizes the statistics of the data set. We
report the performance of extracting entity men-
tions, relations, event triggers and arguments sep-
arately using the standard F1 measures as defined
in (Ji and Grishman, 2008; Chan and Roth, 2011):
</bodyText>
<listItem confidence="0.999144">
• An entity mention is correct if its entity type (7
in total) and head offsets are correct.
• A relation is correct if its type (6 in total) and the
head offsets of its two arguments are correct.
• An event trigger is correct if its event subtype
(33 in total) and offsets are correct.
• An argument link is correct if its event subtype,
offsets and role match those of any of the refer-
ence argument mentions.
</listItem>
<bodyText confidence="0.82378425">
In this paper we focus on entity arguments while
disregard values and time expressions because
they can be most effectively extracted by hand-
crafted patterns (Chang and Manning, 2012).
</bodyText>
<subsectionHeader confidence="0.969602">
3.2 Results
</subsectionHeader>
<bodyText confidence="0.999985523809524">
Based on the results of our development set, we
trained all models with 21 iterations and chose the
beam size to be 8. For the k-best MIRA updates,
we set k as 3. Table 3 compares the overall perfor-
mance of our approaches and baseline methods.
Our joint model with perceptron update out-
performs the state-of-the-art pipelined approach
in (Li et al., 2013; Li and Ji, 2014), and further
improves the joint event extraction system in (Li
et al., 2013) (p &lt; 0.05 for entity mention extrac-
tion, and p &lt; 0.01 for other subtasks, accord-
ing to Wilcoxon Signed RankTest). For the k-
best MIRA update, the L3 loss function achieved
better performance than F1 loss and 0-1 loss on
all sub-tasks except event argument extraction. It
also significantly outperforms perceptron update
on relation extraction and event argument extrac-
tion (p &lt; 0.01). It is particularly encouraging to
see the end output of an IE system (event argu-
ments) has made significant progress (12.2% ab-
solute gain over traditional pipelined approach).
</bodyText>
<subsectionHeader confidence="0.9899015">
3.3 Discussions
3.3.1 Feature Study
</subsectionHeader>
<table confidence="0.999623363636364">
Rank Feature Weight
1 Frame=Killing Die 0.80
2 Frame=Travel Transport 0.61
3 Physical(Artifact, Destination) 0.60
4 w1=“home” Transport 0.59
5 Frame=Arriving Transport 0.54
6 ORG-AFF(Person, Entity) 0.48
7 Lemma=charge Charge-Indict 0.45
8 Lemma=birth Be-Born 0.44
9 Physical(Artifact,Origin) 0.44
10 Frame=Cause harm Injure 0.43
</table>
<tableCaption confidence="0.999627">
Table 4: Top Features about Event Triggers.
</tableCaption>
<bodyText confidence="0.784036428571429">
Table 4 lists the weights of the most significant
features about event triggers. The 3rd, 6th, and
9th rows are joint relation-event features. For in-
stance, Physical(Artifact, Destination) means the
arguments of a Physical relation participate in a
Transport event as Artifact and Destination. We
can see that both the joint relation-event features
</bodyText>
<page confidence="0.992172">
1849
</page>
<bodyText confidence="0.999987875">
and FrameNet based features are of vital impor-
tance to event trigger labeling. We tested the im-
pact of each type of features by excluding them in
the experiments of “MIRA w/ L3 loss”. We found
that FrameNet based features provided 0.8% and
2.2% F1 gains for event trigger and argument la-
beling respectively. Joint relation-event features
also provided 0.6% F1 gain for relation extraction.
</bodyText>
<subsectionHeader confidence="0.97892">
3.3.2 Remaining Challenges
</subsectionHeader>
<bodyText confidence="0.997834285714286">
Event trigger labeling remains a major bottleneck.
In addition to the sparsity problem, the remain-
ing errors suggest to incorporate external world
knowledge. For example, some words act as trig-
gers for some certain types of events only when
they appear together with some particular argu-
ments:
</bodyText>
<listItem confidence="0.595359">
• “Williams picked up the child again and this
</listItem>
<bodyText confidence="0.981047333333333">
time, threwAttack her out the window.”
The word “threw” is used as an Attack event
trigger because the Victim argument is a “child”.
</bodyText>
<listItem confidence="0.964735666666667">
• “Ellison to spend $10.3 billion to getmerge Org
his company.” The common word “get” is
tagged as a trigger of Merge Org, because its
object is “company”.
• “We believe that the likelihood of them
usingAttack those weapons goes up.”
</listItem>
<bodyText confidence="0.968963090909091">
The word “using” is used as an Attack event
trigger because the Instrument argument is
“weapons”.
Another challenge is to distinguish physical and
non-physical events. For example, in the sentence:
• “we are paying great attention to their ability to
defendAttack on the ground.”,
our system fails to extract “defend” as an Attack
trigger. In the training data, “defend” appears mul-
tiple times, but none of them is tagged as Attack.
For instance, in the sentence:
</bodyText>
<listItem confidence="0.845266">
• “North Korea could do everything to defend it-
self. ”
</listItem>
<bodyText confidence="0.999764571428572">
“defend” is not an Attack trigger since it does not
relate to physical actions in a war. This challenge
calls for deeper understanding of the contexts.
Finally, some pronouns are used to refer to ac-
tual events. Event coreference is necessary to rec-
ognize them correctly. For example, in the follow-
ing two sentences from the same document:
</bodyText>
<listItem confidence="0.98760525">
• “It’s important that people all over the world
know that we don’t believe in the warAttack.”,
• “Nobody questions whether thisAttack is right
or not.”
</listItem>
<bodyText confidence="0.999305333333333">
“this” refers to “war” in its preceding contexts.
Without event coreference resolution, it is difficult
to tag it as an Attack event trigger.
</bodyText>
<sectionHeader confidence="0.999555" genericHeader="conclusions">
4 Conclusions
</sectionHeader>
<bodyText confidence="0.9999891875">
We presented the first joint model that effectively
extracts entity mentions, relations and events
based on a unified representation: information
networks. Experiment results on ACE’05 cor-
pus demonstrate that our approach outperforms
pipelined method, and improves event-argument
performance significantly over the state-of-the-art.
In addition to the joint relation-event features, we
demonstrated positive impact of using FrameNet
to handle the sparsity problem in event trigger la-
beling.
Although our primary focus in this paper is in-
formation extraction in the ACE paradigm, we be-
lieve that our framework is general to improve
other tightly coupled extraction tasks by capturing
the inter-dependencies in the joint search space.
</bodyText>
<sectionHeader confidence="0.998385" genericHeader="acknowledgments">
Acknowledgments
</sectionHeader>
<bodyText confidence="0.999981470588235">
We thank the three anonymous reviewers for their
insightful comments. This work was supported by
the U.S. Army Research Laboratory under Coop-
erative Agreement No. W911NF-09-2-0053 (NS-
CTA), U.S. NSF CAREER Award under Grant
IIS-0953149, U.S. DARPA Award No. FA8750-
13-2-0041 in the Deep Exploration and Filtering
of Text (DEFT) Program, IBM Faculty Award,
Google Research Award, Disney Research Award
and RPI faculty start-up grant. The views and con-
clusions contained in this document are those of
the authors and should not be interpreted as rep-
resenting the official policies, either expressed or
implied, of the U.S. Government. The U.S. Gov-
ernment is authorized to reproduce and distribute
reprints for Government purposes notwithstanding
any copyright notation here on.
</bodyText>
<sectionHeader confidence="0.999638" genericHeader="references">
References
</sectionHeader>
<reference confidence="0.99902">
Collin F. Baker and Hiroaki Sato. 2003. The framenet
data and software. In Proc. ACL, pages 161–164.
Yee Seng Chan and Dan Roth. 2011. Exploiting
syntactico-semantic structures for relation extrac-
tion. In Proc. ACL, pages 551–560.
</reference>
<page confidence="0.773159">
1850
</page>
<reference confidence="0.999743369565218">
Angel X. Chang and Christopher Manning. 2012. Su-
time: A library for recognizing and normalizing time
expressions. In Proc. LREC, pages 3735–3740.
Michael Collins and Brian Roark. 2004. Incremen-
tal parsing with the perceptron algorithm. In Proc.
ACL, pages 111–118.
Michael Collins. 2002. Discriminative training meth-
ods for hidden markov models: Theory and experi-
ments with perceptron algorithms. In Proc. EMNLP,
pages 1–8.
Johannes D¨olling. 2011. Aspectual coercion and even-
tuality structure. pages 189–226.
Liang Huang, Suphan Fayong, and Yang Guo. 2012.
Structured perceptron with inexact search. In Proc.
HLT-NAACL, pages 142–151.
Heng Ji and Ralph Grishman. 2008. Refining event ex-
traction through cross-document inference. In Proc.
ACL.
Qi Li and Heng Ji. 2014. Incremental joint extraction
of entity mentions and relations. In Proc. ACL.
Qi Li, Heng Ji, and Liang Huang. 2013. Joint event
extraction via structured prediction with global fea-
tures. In Proc. ACL, pages 73–82.
Ryan McDonald, Koby Crammer, and Fernando
Pereira. 2005. Online large-margin training of de-
pendency parsers. In Proc. ACL, pages 91–98.
Sebastian Riedel and Andrew McCallum. 2011. Fast
and robust joint models for biomedical event extrac-
tion. In Proc. EMNLP.
Sebastian Riedel, Hong-Woo Chun, Toshihisa Takagi,
and Jun’ichi Tsujii. 2009. A markov logic ap-
proach to bio-molecular event extraction. In Proc.
the Workshop on Current Trends in Biomedical Nat-
ural Language Processing: Shared Task.
Dan Roth and Wen-tau Yih. 2007. Global inference
for entity and relation identification via a lin- ear
programming formulation. In Introduction to Sta-
tistical Relational Learning. MIT.
Sameer Singh, Sebastian Riedel, Brian Martin, Jiap-
ing Zheng, and Andrew McCallum. 2013. Joint
inference of entities, relations, and coreference. In
Proc. CIKM Workshop on Automated Knowledge
Base Construction.
Bishan Yang and Claire Cardie. 2013. Joint inference
for fine-grained opinion extraction. In Proc. ACL,
pages 1640–1649.
</reference>
<page confidence="0.993453">
1851
</page>
</variant>
</algorithm>
<algorithm name="ParsHed" version="110505">
<variant no="0" confidence="0.964661">
<title confidence="0.999928">Constructing Information Networks Using One Single Model</title>
<author confidence="0.99694">Heng Yu Sujian</author>
<affiliation confidence="0.999077333333333">Science Department, Rensselaer Polytechnic Institute, of Computer Science and Technology, Soochow University, Laboratory of Computational Linguistics, Peking University, MOE,</affiliation>
<abstract confidence="0.998235352941176">In this paper, we propose a new framework that unifies the output of three information extraction (IE) tasks entity mentions, relations and events as an information network representation, and extracts all of them using one single joint model based on structured prediction. This novel formulation allows different parts of the information network fully interact with each other. For example, many relations can now be considered as the resultant states of events. Our approach achieves substantial improvements over traditional pipelined approaches, and significantly advances state-of-the-art end-toend event argument extraction.</abstract>
</variant>
</algorithm>
<algorithm name="ParsCit" version="110505">
<citationList>
<citation valid="true">
<authors>
<author>Collin F Baker</author>
<author>Hiroaki Sato</author>
</authors>
<title>The framenet data and software.</title>
<date>2003</date>
<booktitle>In Proc. ACL,</booktitle>
<pages>161--164</pages>
<contexts>
<context position="4335" citStr="Baker and Sato, 2003" startWordPosition="666" endWordPosition="669">ng because of the data sparV: Person Attacker Agent-Artifact Attack Instrument Weapon Place Geopolitical Entity 1846 Proceedings of the 2014 Conference on Empirical Methods in Natural Language Processing (EMNLP), pages 1846–1851, October 25-29, 2014, Doha, Qatar. c�2014 Association for Computational Linguistics sity problem of event triggers. We observe that 21.5% of event triggers appear fewer than twice in the ACE’051 training data. By using only lexical and syntactic features we are not able to discover the corresponding nodes and their connections. To tackle this problem, we use FrameNet (Baker and Sato, 2003) to generalize event triggers so that semantically similar triggers are clustered in the same frame. The following sections will elaborate the detailed implementation of our new framework. 2 Approach We uniformly represent the IE output from each sentence as an information network y = (V, E). Each node vi E V is represented as a triple (ui, vi, ti) of start index ui, end index vi, and node type ti. A node can be an entity mention or an event trigger. A particular type of node is 1 (neither entity mention nor event trigger), whose maximal length is always 1. Similarly, each information arc ej E</context>
<context position="10880" citStr="Baker and Sato, 2003" startWordPosition="1788" endWordPosition="1791"> Relations Triggers Arguments Train 7.2k 25.7k 4.8k 2.8k 4.5k Dev 1.7k 6.3k 1.2k 0.7k 1.1k Test 1.5k 5.3k 1.1k 0.6k 1.0k Table 2: Data set 00 20 40 60 80 100 Number of instances Figure 2: Distribution of triggers and their frames. 2.4 Semantic Frame Features One major challenge of constructing information networks is the data sparsity problem in extracting event triggers. For instance, in the sentence: “Others were mutilated beyond recognition.” The Injure trigger “mutilated” does not occur in our training data. But there are some similar words such as “stab” and “smash”. We utilize FrameNet (Baker and Sato, 2003) to solve this problem. FrameNet is a lexical resource for semantic frames. Each frame characterizes a basic type of semantic concept, and contains a number of words (lexical units) that evoke the frame. Many frames are highly related with ACE events. For example, the frame “Cause harm” is closely related with Injure event and contains 68 lexical units such as “stab”, “smash” and “mutilate”. Figure 2 compares the distributions of trigger words and their frame IDs in the training data. We can clearly see that the trigger word distribution suffers from the long-tail problem, while Frames reduce </context>
</contexts>
<marker>Baker, Sato, 2003</marker>
<rawString>Collin F. Baker and Hiroaki Sato. 2003. The framenet data and software. In Proc. ACL, pages 161–164.</rawString>
</citation>
<citation valid="true">
<authors>
<author>Yee Seng Chan</author>
<author>Dan Roth</author>
</authors>
<title>Exploiting syntactico-semantic structures for relation extraction.</title>
<date>2011</date>
<booktitle>In Proc. ACL,</booktitle>
<pages>551--560</pages>
<contexts>
<context position="13099" citStr="Chan and Roth, 2011" startWordPosition="2209" endWordPosition="2212"> of its lemma and POS tag as features. For the above example, “Cause harm” will be a feature for “mutilated”. We only consider tokens that appear in at most 2 frames, and omit the frames that occur fewer than 20 times in our training data. 3 Experiments 3.1 Data and Evaluation We use ACE’05 corpus to evaluate our method with the same data split as in (Li and Ji, 2014). Table 2 summarizes the statistics of the data set. We report the performance of extracting entity mentions, relations, event triggers and arguments separately using the standard F1 measures as defined in (Ji and Grishman, 2008; Chan and Roth, 2011): • An entity mention is correct if its entity type (7 in total) and head offsets are correct. • A relation is correct if its type (6 in total) and the head offsets of its two arguments are correct. • An event trigger is correct if its event subtype (33 in total) and offsets are correct. • An argument link is correct if its event subtype, offsets and role match those of any of the reference argument mentions. In this paper we focus on entity arguments while disregard values and time expressions because they can be most effectively extracted by handcrafted patterns (Chang and Manning, 2012). 3.</context>
</contexts>
<marker>Chan, Roth, 2011</marker>
<rawString>Yee Seng Chan and Dan Roth. 2011. Exploiting syntactico-semantic structures for relation extraction. In Proc. ACL, pages 551–560.</rawString>
</citation>
<citation valid="true">
<authors>
<author>Angel X Chang</author>
<author>Christopher Manning</author>
</authors>
<title>Sutime: A library for recognizing and normalizing time expressions.</title>
<date>2012</date>
<booktitle>In Proc. LREC,</booktitle>
<pages>3735--3740</pages>
<contexts>
<context position="13695" citStr="Chang and Manning, 2012" startWordPosition="2317" endWordPosition="2320">, 2008; Chan and Roth, 2011): • An entity mention is correct if its entity type (7 in total) and head offsets are correct. • A relation is correct if its type (6 in total) and the head offsets of its two arguments are correct. • An event trigger is correct if its event subtype (33 in total) and offsets are correct. • An argument link is correct if its event subtype, offsets and role match those of any of the reference argument mentions. In this paper we focus on entity arguments while disregard values and time expressions because they can be most effectively extracted by handcrafted patterns (Chang and Manning, 2012). 3.2 Results Based on the results of our development set, we trained all models with 21 iterations and chose the beam size to be 8. For the k-best MIRA updates, we set k as 3. Table 3 compares the overall performance of our approaches and baseline methods. Our joint model with perceptron update outperforms the state-of-the-art pipelined approach in (Li et al., 2013; Li and Ji, 2014), and further improves the joint event extraction system in (Li et al., 2013) (p &lt; 0.05 for entity mention extraction, and p &lt; 0.01 for other subtasks, according to Wilcoxon Signed RankTest). For the kbest MIRA upd</context>
</contexts>
<marker>Chang, Manning, 2012</marker>
<rawString>Angel X. Chang and Christopher Manning. 2012. Sutime: A library for recognizing and normalizing time expressions. In Proc. LREC, pages 3735–3740.</rawString>
</citation>
<citation valid="true">
<authors>
<author>Michael Collins</author>
<author>Brian Roark</author>
</authors>
<title>Incremental parsing with the perceptron algorithm.</title>
<date>2004</date>
<booktitle>In Proc. ACL,</booktitle>
<pages>111--118</pages>
<contexts>
<context position="7224" citStr="Collins and Roark, 2004" startWordPosition="1188" endWordPosition="1191">nodes, b is the beam size, and m is the sentence length. The actual execution time is much shorter, especially when entity type constraints are applied. 2.2 Parameter Estimation For each training instance (x, y), the structured perceptron algorithm seeks the assignment with the highest model score: z = argmax f(x, y&apos;) • w y&apos;EY(x) and then updates the feature weights by using: wnew = w + f(x, y) − f(x, z) We relax the exact inference problem by the aforementioned beam-search procedure. The standard perceptron will cause invalid updates because of inexact search. Therefore we apply earlyupdate (Collins and Roark, 2004), an instance of violation-fixing methods (Huang et al., 2012). In the rest of this paper, we override y and z to denote prefixes of structures. In addition to the simple perceptron update, we also apply k-best MIRA (McDonald et al., 2005), an online large-margin learning algorithm. During each update, it keeps the norm of the change to feature weights w as small as possible, and forces the margin between y and the k-best candidate z greater or equal to their loss L(y, z). It is formulated as a quadratic programming problem: min 11wnew − w11 s.t. wnewf(x, y) − wnewf(x, z) &gt; L(y, z) bz E bestk(</context>
</contexts>
<marker>Collins, Roark, 2004</marker>
<rawString>Michael Collins and Brian Roark. 2004. Incremental parsing with the perceptron algorithm. In Proc. ACL, pages 111–118.</rawString>
</citation>
<citation valid="true">
<authors>
<author>Michael Collins</author>
</authors>
<title>Discriminative training methods for hidden markov models: Theory and experiments with perceptron algorithms.</title>
<date>2002</date>
<booktitle>In Proc. EMNLP,</booktitle>
<pages>1--8</pages>
<contexts>
<context position="2668" citStr="Collins, 2002" startWordPosition="404" endWordPosition="405">that can extract all of them has never been studied so far. Physical Asif Mohammed Hanif detonated explosives in Tel Aviv x: x1 x2 x3 x4 x5 x6 x7 x8 Figure 1: Information Network Representation. Information nodes are denoted by rectangles. Arrows represent information arcs. For the first time, we uniformly represent the IE output from each sentence as an information network, where entity mentions and event triggers are nodes, relations and event-argument links are arcs. We apply a structured perceptron framework with a segment-based beam-search algorithm to construct the information networks (Collins, 2002; Li et al., 2013; Li and Ji, 2014). In addition to the perceptron update, we also apply k-best MIRA (McDonald et al., 2005), which refines the perceptron update in three aspects: it is flexible in using various loss functions, it is a large-margin approach, and it can use mulitple candidate structures to tune feature weights. In an information network, we can capture the interactions among multiple nodes by learning joint features during training. In addition to the cross-component dependencies studied in (Li et al., 2013; Li and Ji, 2014), we are able to capture interactions between relation</context>
</contexts>
<marker>Collins, 2002</marker>
<rawString>Michael Collins. 2002. Discriminative training methods for hidden markov models: Theory and experiments with perceptron algorithms. In Proc. EMNLP, pages 1–8.</rawString>
</citation>
<citation valid="true">
<authors>
<author>Johannes D¨olling</author>
</authors>
<title>Aspectual coercion and eventuality structure.</title>
<date>2011</date>
<pages>189--226</pages>
<marker>D¨olling, 2011</marker>
<rawString>Johannes D¨olling. 2011. Aspectual coercion and eventuality structure. pages 189–226.</rawString>
</citation>
<citation valid="true">
<authors>
<author>Liang Huang</author>
<author>Suphan Fayong</author>
<author>Yang Guo</author>
</authors>
<title>Structured perceptron with inexact search.</title>
<date>2012</date>
<booktitle>In Proc. HLT-NAACL,</booktitle>
<pages>142--151</pages>
<contexts>
<context position="7286" citStr="Huang et al., 2012" startWordPosition="1197" endWordPosition="1200"> execution time is much shorter, especially when entity type constraints are applied. 2.2 Parameter Estimation For each training instance (x, y), the structured perceptron algorithm seeks the assignment with the highest model score: z = argmax f(x, y&apos;) • w y&apos;EY(x) and then updates the feature weights by using: wnew = w + f(x, y) − f(x, z) We relax the exact inference problem by the aforementioned beam-search procedure. The standard perceptron will cause invalid updates because of inexact search. Therefore we apply earlyupdate (Collins and Roark, 2004), an instance of violation-fixing methods (Huang et al., 2012). In the rest of this paper, we override y and z to denote prefixes of structures. In addition to the simple perceptron update, we also apply k-best MIRA (McDonald et al., 2005), an online large-margin learning algorithm. During each update, it keeps the norm of the change to feature weights w as small as possible, and forces the margin between y and the k-best candidate z greater or equal to their loss L(y, z). It is formulated as a quadratic programming problem: min 11wnew − w11 s.t. wnewf(x, y) − wnewf(x, z) &gt; L(y, z) bz E bestk(x, w) We employ the following three loss functions for compari</context>
</contexts>
<marker>Huang, Fayong, Guo, 2012</marker>
<rawString>Liang Huang, Suphan Fayong, and Yang Guo. 2012. Structured perceptron with inexact search. In Proc. HLT-NAACL, pages 142–151.</rawString>
</citation>
<citation valid="true">
<authors>
<author>Heng Ji</author>
<author>Ralph Grishman</author>
</authors>
<title>Refining event extraction through cross-document inference.</title>
<date>2008</date>
<booktitle>In Proc. ACL.</booktitle>
<contexts>
<context position="13077" citStr="Ji and Grishman, 2008" startWordPosition="2205" endWordPosition="2208">contain the combination of its lemma and POS tag as features. For the above example, “Cause harm” will be a feature for “mutilated”. We only consider tokens that appear in at most 2 frames, and omit the frames that occur fewer than 20 times in our training data. 3 Experiments 3.1 Data and Evaluation We use ACE’05 corpus to evaluate our method with the same data split as in (Li and Ji, 2014). Table 2 summarizes the statistics of the data set. We report the performance of extracting entity mentions, relations, event triggers and arguments separately using the standard F1 measures as defined in (Ji and Grishman, 2008; Chan and Roth, 2011): • An entity mention is correct if its entity type (7 in total) and head offsets are correct. • A relation is correct if its type (6 in total) and the head offsets of its two arguments are correct. • An event trigger is correct if its event subtype (33 in total) and offsets are correct. • An argument link is correct if its event subtype, offsets and role match those of any of the reference argument mentions. In this paper we focus on entity arguments while disregard values and time expressions because they can be most effectively extracted by handcrafted patterns (Chang </context>
</contexts>
<marker>Ji, Grishman, 2008</marker>
<rawString>Heng Ji and Ralph Grishman. 2008. Refining event extraction through cross-document inference. In Proc. ACL.</rawString>
</citation>
<citation valid="true">
<authors>
<author>Qi Li</author>
<author>Heng Ji</author>
</authors>
<title>Incremental joint extraction of entity mentions and relations.</title>
<date>2014</date>
<booktitle>In Proc. ACL.</booktitle>
<contexts>
<context position="1796" citStr="Li and Ji, 2014" startWordPosition="264" endWordPosition="267">ty mentions are core components of relations and events, and the extraction of relations and events can help to accurately recognize entity mentions. In addition, the theory of eventualities (D¨olling, 2011) suggested that relations can be viewed as states that events start from and result in. Therefore, it is intuitive but challenging to extract all of them simultaneously in a single model. Some recent research attempted to jointly model multiple IE subtasks (e.g., (Roth and Yih, 2007; Riedel and McCallum, 2011; Yang and Cardie, 2013; Riedel et al., 2009; Singh et al., 2013; Li et al., 2013; Li and Ji, 2014)). For example, Roth and Yih (2007) conducted joint inference over entity mentions and relations; Our previous work jointly extracted event triggers and arguments (Li et al., 2013), and entity mentions and relations (Li and Ji, 2014). However, a single model that can extract all of them has never been studied so far. Physical Asif Mohammed Hanif detonated explosives in Tel Aviv x: x1 x2 x3 x4 x5 x6 x7 x8 Figure 1: Information Network Representation. Information nodes are denoted by rectangles. Arrows represent information arcs. For the first time, we uniformly represent the IE output from each</context>
<context position="3214" citStr="Li and Ji, 2014" startWordPosition="494" endWordPosition="497">-search algorithm to construct the information networks (Collins, 2002; Li et al., 2013; Li and Ji, 2014). In addition to the perceptron update, we also apply k-best MIRA (McDonald et al., 2005), which refines the perceptron update in three aspects: it is flexible in using various loss functions, it is a large-margin approach, and it can use mulitple candidate structures to tune feature weights. In an information network, we can capture the interactions among multiple nodes by learning joint features during training. In addition to the cross-component dependencies studied in (Li et al., 2013; Li and Ji, 2014), we are able to capture interactions between relations and events. For example, in Figure 1, if we know that the Person mention “Asif Mohammed Hanif ” is an Attacker of the Attack event triggered by “detonated”, and the Weapon mention “explosives” is an Instrument, we can infer that there exists an Agent-Artifact relation between them. Similarly we can infer the Physical relation between “Asif Mohammed Hanif” and “Tel Aviv”. However, in practice many useful interactions are missing during testing because of the data sparV: Person Attacker Agent-Artifact Attack Instrument Weapon Place Geopolit</context>
<context position="5483" citStr="Li and Ji, 2014" startWordPosition="876" endWordPosition="879">ose maximal length is always 1. Similarly, each information arc ej E E is represented as (uj, vj, rj), where uj and vj are the end offsets of the nodes, and rj is the arc type. For instance, in Figure 1, the event trigger “detonated” is represented as (4, 4, Attack), the entity mention “Asif Mohammed Hanif” is represented as (1, 3, Person), and their argument arc is (4, 3, Attacker). Our goal is to extract the whole information network y for a given sentence x. 2.1 Decoding Algorithm Our joint decoding algorithm is based on extending the segment-based algorithm described in our previous work (Li and Ji, 2014). Let x = (x1, ..., xm) be the input sentence. The decoder performs two types of actions at each token xi from left to right: • NODEACTION(i, j): appends a new node (j, i, t) ending at the i-th token, where i − dt &lt; j &lt; i, and dt is the maximal length of type-t nodes in training data. • ARCACTION(i, j): for each j &lt; i, incrementally creates a new arc between the nodes ending at the j-th and i-th tokens respectively: (i, j, r). After each action, the top-k hypotheses are selected according to their features f(x, y&apos;) and 1http://www.itl.nist.gov/iad/mig//tests/ace weights w: bestk f(x, y&apos;) • w y</context>
<context position="9656" citStr="Li and Ji, 2014" startWordPosition="1580" endWordPosition="1583">one is 0-1 loss: It does not discriminate the extent to which z deviates from y. • The third loss function counts the difference between y and z: L3(y, z) = |y |+ |z |− 2 · |y ∩ z| Similar to F1 loss function, it penalizes both missing and false-positive units. The difference is that it is sensitive to the size of y and z. 2.3 Joint Relation-Event Features By extracting three core IE components in a joint search space, we can utilize joint features over multiple components in addition to factorized features in pipelined approaches. In addition to the features as described in (Li et al., 2013; Li and Ji, 2014), we can make use of joint features between relations and events, given the fact that relations are often ending or starting states of events (D¨olling, 2011). Table 1 shows the most frequent overlapping relation and event types in our training data. In each partial structure y&apos; during the search, if both arguments of a relation participate in an event, we compose the corresponding argument roles and relation type as a joint feature for y&apos;. For example, for the structure in Figure 1, we obtain the following joint relation-event features: Agent-Artifact Attacker Instrument Split Sentences Menti</context>
<context position="11918" citStr="Li and Ji (2014)" startWordPosition="1986" endWordPosition="1989">ibutions of trigger words and their frame IDs in the training data. We can clearly see that the trigger word distribution suffers from the long-tail problem, while Frames reduce the number of triggers which occur only � 1 y =6 z L2(y, z) = 0 y = z Trigger Words Frame IDs 2 ·|y ∩ z| L1(y, z) = 1 − Physical Attacker Place Frequency 14 12 10 8 6 4 2 1848 Methods Entity Mention (%) Relation (%) Event Trigger (%) Event Argument (%) P R F1 P R F1 P R F1 P R F1 Pipelined Baseline 83.6 75.7 79.5 68.5 41.4 51.6 71.2 58.7 64.4 64.8 24.6 35.7 Pipeline + Li et al. (2013) N/A 74.5 56.9 64.5 67.5 31.6 43.1 Li and Ji (2014) 85.2 76.9 80.8 68.9 41.9 52.1 N/A Joint w/ Avg. Perceptron 85.1 77.3 81.0 70.5 41.2 52.0 67.9 62.8 65.3 64.7 35.3 45.6 Joint w/ MIRA w/ F1 Loss 83.1 75.3 79.0 65.5 39.4 49.2 59.6 63.5 61.5 60.6 38.9 47.4 Joint w/ MIRA w/ 0-1 Loss 84.2 76.1 80.0 65.4 41.8 51.0 65.6 61.0 63.2 60.5 39.6 47.9 Joint w/ MIRA w/ L3 Loss 85.3 76.5 80.7 70.8 42.1 52.8 70.3 60.9 65.2 66.4 36.1 46.8 Table 3: Overall performance on test set. once in the training data from 100 to 60 and alleviate the sparsity problem. For each token, we exploit the frames that contain the combination of its lemma and POS tag as features. </context>
<context position="14081" citStr="Li and Ji, 2014" startWordPosition="2386" endWordPosition="2389">ose of any of the reference argument mentions. In this paper we focus on entity arguments while disregard values and time expressions because they can be most effectively extracted by handcrafted patterns (Chang and Manning, 2012). 3.2 Results Based on the results of our development set, we trained all models with 21 iterations and chose the beam size to be 8. For the k-best MIRA updates, we set k as 3. Table 3 compares the overall performance of our approaches and baseline methods. Our joint model with perceptron update outperforms the state-of-the-art pipelined approach in (Li et al., 2013; Li and Ji, 2014), and further improves the joint event extraction system in (Li et al., 2013) (p &lt; 0.05 for entity mention extraction, and p &lt; 0.01 for other subtasks, according to Wilcoxon Signed RankTest). For the kbest MIRA update, the L3 loss function achieved better performance than F1 loss and 0-1 loss on all sub-tasks except event argument extraction. It also significantly outperforms perceptron update on relation extraction and event argument extraction (p &lt; 0.01). It is particularly encouraging to see the end output of an IE system (event arguments) has made significant progress (12.2% absolute gain </context>
</contexts>
<marker>Li, Ji, 2014</marker>
<rawString>Qi Li and Heng Ji. 2014. Incremental joint extraction of entity mentions and relations. In Proc. ACL.</rawString>
</citation>
<citation valid="true">
<authors>
<author>Qi Li</author>
<author>Heng Ji</author>
<author>Liang Huang</author>
</authors>
<title>Joint event extraction via structured prediction with global features.</title>
<date>2013</date>
<booktitle>In Proc. ACL,</booktitle>
<pages>73--82</pages>
<contexts>
<context position="1778" citStr="Li et al., 2013" startWordPosition="260" endWordPosition="263">erdependent: entity mentions are core components of relations and events, and the extraction of relations and events can help to accurately recognize entity mentions. In addition, the theory of eventualities (D¨olling, 2011) suggested that relations can be viewed as states that events start from and result in. Therefore, it is intuitive but challenging to extract all of them simultaneously in a single model. Some recent research attempted to jointly model multiple IE subtasks (e.g., (Roth and Yih, 2007; Riedel and McCallum, 2011; Yang and Cardie, 2013; Riedel et al., 2009; Singh et al., 2013; Li et al., 2013; Li and Ji, 2014)). For example, Roth and Yih (2007) conducted joint inference over entity mentions and relations; Our previous work jointly extracted event triggers and arguments (Li et al., 2013), and entity mentions and relations (Li and Ji, 2014). However, a single model that can extract all of them has never been studied so far. Physical Asif Mohammed Hanif detonated explosives in Tel Aviv x: x1 x2 x3 x4 x5 x6 x7 x8 Figure 1: Information Network Representation. Information nodes are denoted by rectangles. Arrows represent information arcs. For the first time, we uniformly represent the I</context>
<context position="3196" citStr="Li et al., 2013" startWordPosition="490" endWordPosition="493">egment-based beam-search algorithm to construct the information networks (Collins, 2002; Li et al., 2013; Li and Ji, 2014). In addition to the perceptron update, we also apply k-best MIRA (McDonald et al., 2005), which refines the perceptron update in three aspects: it is flexible in using various loss functions, it is a large-margin approach, and it can use mulitple candidate structures to tune feature weights. In an information network, we can capture the interactions among multiple nodes by learning joint features during training. In addition to the cross-component dependencies studied in (Li et al., 2013; Li and Ji, 2014), we are able to capture interactions between relations and events. For example, in Figure 1, if we know that the Person mention “Asif Mohammed Hanif ” is an Attacker of the Attack event triggered by “detonated”, and the Weapon mention “explosives” is an Instrument, we can infer that there exists an Agent-Artifact relation between them. Similarly we can infer the Physical relation between “Asif Mohammed Hanif” and “Tel Aviv”. However, in practice many useful interactions are missing during testing because of the data sparV: Person Attacker Agent-Artifact Attack Instrument Wea</context>
<context position="9638" citStr="Li et al., 2013" startWordPosition="1576" endWordPosition="1579"> 6. • The second one is 0-1 loss: It does not discriminate the extent to which z deviates from y. • The third loss function counts the difference between y and z: L3(y, z) = |y |+ |z |− 2 · |y ∩ z| Similar to F1 loss function, it penalizes both missing and false-positive units. The difference is that it is sensitive to the size of y and z. 2.3 Joint Relation-Event Features By extracting three core IE components in a joint search space, we can utilize joint features over multiple components in addition to factorized features in pipelined approaches. In addition to the features as described in (Li et al., 2013; Li and Ji, 2014), we can make use of joint features between relations and events, given the fact that relations are often ending or starting states of events (D¨olling, 2011). Table 1 shows the most frequent overlapping relation and event types in our training data. In each partial structure y&apos; during the search, if both arguments of a relation participate in an event, we compose the corresponding argument roles and relation type as a joint feature for y&apos;. For example, for the structure in Figure 1, we obtain the following joint relation-event features: Agent-Artifact Attacker Instrument Spl</context>
<context position="11867" citStr="Li et al. (2013)" startWordPosition="1975" endWordPosition="1978">“smash” and “mutilate”. Figure 2 compares the distributions of trigger words and their frame IDs in the training data. We can clearly see that the trigger word distribution suffers from the long-tail problem, while Frames reduce the number of triggers which occur only � 1 y =6 z L2(y, z) = 0 y = z Trigger Words Frame IDs 2 ·|y ∩ z| L1(y, z) = 1 − Physical Attacker Place Frequency 14 12 10 8 6 4 2 1848 Methods Entity Mention (%) Relation (%) Event Trigger (%) Event Argument (%) P R F1 P R F1 P R F1 P R F1 Pipelined Baseline 83.6 75.7 79.5 68.5 41.4 51.6 71.2 58.7 64.4 64.8 24.6 35.7 Pipeline + Li et al. (2013) N/A 74.5 56.9 64.5 67.5 31.6 43.1 Li and Ji (2014) 85.2 76.9 80.8 68.9 41.9 52.1 N/A Joint w/ Avg. Perceptron 85.1 77.3 81.0 70.5 41.2 52.0 67.9 62.8 65.3 64.7 35.3 45.6 Joint w/ MIRA w/ F1 Loss 83.1 75.3 79.0 65.5 39.4 49.2 59.6 63.5 61.5 60.6 38.9 47.4 Joint w/ MIRA w/ 0-1 Loss 84.2 76.1 80.0 65.4 41.8 51.0 65.6 61.0 63.2 60.5 39.6 47.9 Joint w/ MIRA w/ L3 Loss 85.3 76.5 80.7 70.8 42.1 52.8 70.3 60.9 65.2 66.4 36.1 46.8 Table 3: Overall performance on test set. once in the training data from 100 to 60 and alleviate the sparsity problem. For each token, we exploit the frames that contain the</context>
<context position="14063" citStr="Li et al., 2013" startWordPosition="2382" endWordPosition="2385">and role match those of any of the reference argument mentions. In this paper we focus on entity arguments while disregard values and time expressions because they can be most effectively extracted by handcrafted patterns (Chang and Manning, 2012). 3.2 Results Based on the results of our development set, we trained all models with 21 iterations and chose the beam size to be 8. For the k-best MIRA updates, we set k as 3. Table 3 compares the overall performance of our approaches and baseline methods. Our joint model with perceptron update outperforms the state-of-the-art pipelined approach in (Li et al., 2013; Li and Ji, 2014), and further improves the joint event extraction system in (Li et al., 2013) (p &lt; 0.05 for entity mention extraction, and p &lt; 0.01 for other subtasks, according to Wilcoxon Signed RankTest). For the kbest MIRA update, the L3 loss function achieved better performance than F1 loss and 0-1 loss on all sub-tasks except event argument extraction. It also significantly outperforms perceptron update on relation extraction and event argument extraction (p &lt; 0.01). It is particularly encouraging to see the end output of an IE system (event arguments) has made significant progress (12</context>
</contexts>
<marker>Li, Ji, Huang, 2013</marker>
<rawString>Qi Li, Heng Ji, and Liang Huang. 2013. Joint event extraction via structured prediction with global features. In Proc. ACL, pages 73–82.</rawString>
</citation>
<citation valid="true">
<authors>
<author>Ryan McDonald</author>
<author>Koby Crammer</author>
<author>Fernando Pereira</author>
</authors>
<title>Online large-margin training of dependency parsers.</title>
<date>2005</date>
<booktitle>In Proc. ACL,</booktitle>
<pages>91--98</pages>
<contexts>
<context position="2792" citStr="McDonald et al., 2005" startWordPosition="426" endWordPosition="430">l Aviv x: x1 x2 x3 x4 x5 x6 x7 x8 Figure 1: Information Network Representation. Information nodes are denoted by rectangles. Arrows represent information arcs. For the first time, we uniformly represent the IE output from each sentence as an information network, where entity mentions and event triggers are nodes, relations and event-argument links are arcs. We apply a structured perceptron framework with a segment-based beam-search algorithm to construct the information networks (Collins, 2002; Li et al., 2013; Li and Ji, 2014). In addition to the perceptron update, we also apply k-best MIRA (McDonald et al., 2005), which refines the perceptron update in three aspects: it is flexible in using various loss functions, it is a large-margin approach, and it can use mulitple candidate structures to tune feature weights. In an information network, we can capture the interactions among multiple nodes by learning joint features during training. In addition to the cross-component dependencies studied in (Li et al., 2013; Li and Ji, 2014), we are able to capture interactions between relations and events. For example, in Figure 1, if we know that the Person mention “Asif Mohammed Hanif ” is an Attacker of the Atta</context>
<context position="7463" citStr="McDonald et al., 2005" startWordPosition="1229" endWordPosition="1232">gorithm seeks the assignment with the highest model score: z = argmax f(x, y&apos;) • w y&apos;EY(x) and then updates the feature weights by using: wnew = w + f(x, y) − f(x, z) We relax the exact inference problem by the aforementioned beam-search procedure. The standard perceptron will cause invalid updates because of inexact search. Therefore we apply earlyupdate (Collins and Roark, 2004), an instance of violation-fixing methods (Huang et al., 2012). In the rest of this paper, we override y and z to denote prefixes of structures. In addition to the simple perceptron update, we also apply k-best MIRA (McDonald et al., 2005), an online large-margin learning algorithm. During each update, it keeps the norm of the change to feature weights w as small as possible, and forces the margin between y and the k-best candidate z greater or equal to their loss L(y, z). It is formulated as a quadratic programming problem: min 11wnew − w11 s.t. wnewf(x, y) − wnewf(x, z) &gt; L(y, z) bz E bestk(x, w) We employ the following three loss functions for comparison: 1847 Freq. Relation Type Event Type Arg-1 Arg-2 Example 159 Physical Transport Artifact Destination He(arg-1) was escorted(trigger) into Iraq(arg-2). 46 Physical Attack Tar</context>
</contexts>
<marker>McDonald, Crammer, Pereira, 2005</marker>
<rawString>Ryan McDonald, Koby Crammer, and Fernando Pereira. 2005. Online large-margin training of dependency parsers. In Proc. ACL, pages 91–98.</rawString>
</citation>
<citation valid="true">
<authors>
<author>Sebastian Riedel</author>
<author>Andrew McCallum</author>
</authors>
<title>Fast and robust joint models for biomedical event extraction.</title>
<date>2011</date>
<booktitle>In Proc. EMNLP.</booktitle>
<contexts>
<context position="1697" citStr="Riedel and McCallum, 2011" startWordPosition="244" endWordPosition="247">ons, relations and events from unstructured texts, and these three subtasks are closely interdependent: entity mentions are core components of relations and events, and the extraction of relations and events can help to accurately recognize entity mentions. In addition, the theory of eventualities (D¨olling, 2011) suggested that relations can be viewed as states that events start from and result in. Therefore, it is intuitive but challenging to extract all of them simultaneously in a single model. Some recent research attempted to jointly model multiple IE subtasks (e.g., (Roth and Yih, 2007; Riedel and McCallum, 2011; Yang and Cardie, 2013; Riedel et al., 2009; Singh et al., 2013; Li et al., 2013; Li and Ji, 2014)). For example, Roth and Yih (2007) conducted joint inference over entity mentions and relations; Our previous work jointly extracted event triggers and arguments (Li et al., 2013), and entity mentions and relations (Li and Ji, 2014). However, a single model that can extract all of them has never been studied so far. Physical Asif Mohammed Hanif detonated explosives in Tel Aviv x: x1 x2 x3 x4 x5 x6 x7 x8 Figure 1: Information Network Representation. Information nodes are denoted by rectangles. Ar</context>
</contexts>
<marker>Riedel, McCallum, 2011</marker>
<rawString>Sebastian Riedel and Andrew McCallum. 2011. Fast and robust joint models for biomedical event extraction. In Proc. EMNLP.</rawString>
</citation>
<citation valid="true">
<authors>
<author>Sebastian Riedel</author>
<author>Hong-Woo Chun</author>
<author>Toshihisa Takagi</author>
<author>Jun’ichi Tsujii</author>
</authors>
<title>A markov logic approach to bio-molecular event extraction.</title>
<date>2009</date>
<booktitle>In Proc. the Workshop on Current Trends in Biomedical Natural Language Processing: Shared Task.</booktitle>
<contexts>
<context position="1741" citStr="Riedel et al., 2009" startWordPosition="252" endWordPosition="255"> and these three subtasks are closely interdependent: entity mentions are core components of relations and events, and the extraction of relations and events can help to accurately recognize entity mentions. In addition, the theory of eventualities (D¨olling, 2011) suggested that relations can be viewed as states that events start from and result in. Therefore, it is intuitive but challenging to extract all of them simultaneously in a single model. Some recent research attempted to jointly model multiple IE subtasks (e.g., (Roth and Yih, 2007; Riedel and McCallum, 2011; Yang and Cardie, 2013; Riedel et al., 2009; Singh et al., 2013; Li et al., 2013; Li and Ji, 2014)). For example, Roth and Yih (2007) conducted joint inference over entity mentions and relations; Our previous work jointly extracted event triggers and arguments (Li et al., 2013), and entity mentions and relations (Li and Ji, 2014). However, a single model that can extract all of them has never been studied so far. Physical Asif Mohammed Hanif detonated explosives in Tel Aviv x: x1 x2 x3 x4 x5 x6 x7 x8 Figure 1: Information Network Representation. Information nodes are denoted by rectangles. Arrows represent information arcs. For the fir</context>
</contexts>
<marker>Riedel, Chun, Takagi, Tsujii, 2009</marker>
<rawString>Sebastian Riedel, Hong-Woo Chun, Toshihisa Takagi, and Jun’ichi Tsujii. 2009. A markov logic approach to bio-molecular event extraction. In Proc. the Workshop on Current Trends in Biomedical Natural Language Processing: Shared Task.</rawString>
</citation>
<citation valid="true">
<authors>
<author>Dan Roth</author>
<author>Wen-tau Yih</author>
</authors>
<title>Global inference for entity and relation identification via a lin- ear programming formulation. In Introduction to Statistical Relational Learning.</title>
<date>2007</date>
<publisher>MIT.</publisher>
<contexts>
<context position="1670" citStr="Roth and Yih, 2007" startWordPosition="240" endWordPosition="243">iscover entity mentions, relations and events from unstructured texts, and these three subtasks are closely interdependent: entity mentions are core components of relations and events, and the extraction of relations and events can help to accurately recognize entity mentions. In addition, the theory of eventualities (D¨olling, 2011) suggested that relations can be viewed as states that events start from and result in. Therefore, it is intuitive but challenging to extract all of them simultaneously in a single model. Some recent research attempted to jointly model multiple IE subtasks (e.g., (Roth and Yih, 2007; Riedel and McCallum, 2011; Yang and Cardie, 2013; Riedel et al., 2009; Singh et al., 2013; Li et al., 2013; Li and Ji, 2014)). For example, Roth and Yih (2007) conducted joint inference over entity mentions and relations; Our previous work jointly extracted event triggers and arguments (Li et al., 2013), and entity mentions and relations (Li and Ji, 2014). However, a single model that can extract all of them has never been studied so far. Physical Asif Mohammed Hanif detonated explosives in Tel Aviv x: x1 x2 x3 x4 x5 x6 x7 x8 Figure 1: Information Network Representation. Information nodes ar</context>
</contexts>
<marker>Roth, Yih, 2007</marker>
<rawString>Dan Roth and Wen-tau Yih. 2007. Global inference for entity and relation identification via a lin- ear programming formulation. In Introduction to Statistical Relational Learning. MIT.</rawString>
</citation>
<citation valid="true">
<authors>
<author>Sameer Singh</author>
<author>Sebastian Riedel</author>
<author>Brian Martin</author>
<author>Jiaping Zheng</author>
<author>Andrew McCallum</author>
</authors>
<title>Joint inference of entities, relations, and coreference.</title>
<date>2013</date>
<booktitle>In Proc. CIKM Workshop on Automated Knowledge Base Construction.</booktitle>
<contexts>
<context position="1761" citStr="Singh et al., 2013" startWordPosition="256" endWordPosition="259">asks are closely interdependent: entity mentions are core components of relations and events, and the extraction of relations and events can help to accurately recognize entity mentions. In addition, the theory of eventualities (D¨olling, 2011) suggested that relations can be viewed as states that events start from and result in. Therefore, it is intuitive but challenging to extract all of them simultaneously in a single model. Some recent research attempted to jointly model multiple IE subtasks (e.g., (Roth and Yih, 2007; Riedel and McCallum, 2011; Yang and Cardie, 2013; Riedel et al., 2009; Singh et al., 2013; Li et al., 2013; Li and Ji, 2014)). For example, Roth and Yih (2007) conducted joint inference over entity mentions and relations; Our previous work jointly extracted event triggers and arguments (Li et al., 2013), and entity mentions and relations (Li and Ji, 2014). However, a single model that can extract all of them has never been studied so far. Physical Asif Mohammed Hanif detonated explosives in Tel Aviv x: x1 x2 x3 x4 x5 x6 x7 x8 Figure 1: Information Network Representation. Information nodes are denoted by rectangles. Arrows represent information arcs. For the first time, we uniforml</context>
</contexts>
<marker>Singh, Riedel, Martin, Zheng, McCallum, 2013</marker>
<rawString>Sameer Singh, Sebastian Riedel, Brian Martin, Jiaping Zheng, and Andrew McCallum. 2013. Joint inference of entities, relations, and coreference. In Proc. CIKM Workshop on Automated Knowledge Base Construction.</rawString>
</citation>
<citation valid="true">
<authors>
<author>Bishan Yang</author>
<author>Claire Cardie</author>
</authors>
<title>Joint inference for fine-grained opinion extraction.</title>
<date>2013</date>
<booktitle>In Proc. ACL,</booktitle>
<pages>1640--1649</pages>
<contexts>
<context position="1720" citStr="Yang and Cardie, 2013" startWordPosition="248" endWordPosition="251">rom unstructured texts, and these three subtasks are closely interdependent: entity mentions are core components of relations and events, and the extraction of relations and events can help to accurately recognize entity mentions. In addition, the theory of eventualities (D¨olling, 2011) suggested that relations can be viewed as states that events start from and result in. Therefore, it is intuitive but challenging to extract all of them simultaneously in a single model. Some recent research attempted to jointly model multiple IE subtasks (e.g., (Roth and Yih, 2007; Riedel and McCallum, 2011; Yang and Cardie, 2013; Riedel et al., 2009; Singh et al., 2013; Li et al., 2013; Li and Ji, 2014)). For example, Roth and Yih (2007) conducted joint inference over entity mentions and relations; Our previous work jointly extracted event triggers and arguments (Li et al., 2013), and entity mentions and relations (Li and Ji, 2014). However, a single model that can extract all of them has never been studied so far. Physical Asif Mohammed Hanif detonated explosives in Tel Aviv x: x1 x2 x3 x4 x5 x6 x7 x8 Figure 1: Information Network Representation. Information nodes are denoted by rectangles. Arrows represent informat</context>
</contexts>
<marker>Yang, Cardie, 2013</marker>
<rawString>Bishan Yang and Claire Cardie. 2013. Joint inference for fine-grained opinion extraction. In Proc. ACL, pages 1640–1649.</rawString>
</citation>
</citationList>
</algorithm>
</algorithms>
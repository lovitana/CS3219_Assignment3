<?xml version="1.0" encoding="UTF-8"?>
<algorithms version="110505">
<algorithm name="SectLabel" version="110505">
<variant no="0" confidence="0.000071">
<title confidence="0.983009">
Correcting Keyboard Layout Errors and Homoglyphs in Queries
</title>
<author confidence="0.87595">
Derek Barnes Mahesh Joshi Hassan Sawaf
</author>
<email confidence="0.86529">
debarnes@ebay.com mahesh.joshi@ebay.com hsawaf@ebay.com
</email>
<note confidence="0.634364">
eBay Inc., 2065 Hamilton Ave, San Jose, CA, 95125, USA
</note>
<sectionHeader confidence="0.975262" genericHeader="abstract">
Abstract
</sectionHeader>
<bodyText confidence="0.999735636363636">
Keyboard layout errors and homoglyphs
in cross-language queries impact our abil-
ity to correctly interpret user informa-
tion needs and offer relevant results.
We present a machine learning approach
to correcting these errors, based largely
on character-level n-gram features. We
demonstrate superior performance over
rule-based methods, as well as a signif-
icant reduction in the number of queries
that yield null search results.
</bodyText>
<sectionHeader confidence="0.9988" genericHeader="keywords">
1 Introduction
</sectionHeader>
<bodyText confidence="0.999954409090909">
The success of an eCommerce site depends on
how well users are connected with products and
services of interest. Users typically communi-
cate their desires through search queries; however,
queries are often incomplete and contain errors,
which impact the quantity and quality of search
results.
New challenges arise for search engines in
cross-border eCommerce. In this paper, we fo-
cus on two cross-linguistic phenomena that make
interpreting queries difficult: (i) Homoglyphs:
(Miller, 2013): Tokens such as “case” (underlined
letters Cyrillic), in which users mix characters
from different character sets that are visually simi-
lar or identical. For instance, English and Russian
alphabets share homoglyphs such as c, a, e, o, y,
k, etc. Although the letters are visually similar or
in some cases identical, the underlying character
codes are different. (ii) Keyboard Layout Errors
(KLEs): (Baytin et al., 2013): When switching
one’s keyboard between language modes, users at
times enter terms in the wrong character set. For
instance, “yexon W30B” may appear to be a Rus-
sian query. While “yexon” is the Russian word
for “case”, “W30B” is actually the user’s attempt
to enter the characters “ipad” while leaving their
keyboard in Russian language mode. Queries con-
taining KLEs or homoglyphs are unlikely to pro-
duce any search results, unless the intended ASCII
sequences can be recovered. In a test set sam-
pled from Russian/English queries with null (i.e.
empty) search results (see Section 3.1), we found
approximately 7.8% contained at least one KLE or
homoglyph.
In this paper, we present a machine learning
approach to identifying and correcting query to-
kens containing homoglyphs and KLEs. We show
that the proposed method offers superior accuracy
over rule-based methods, as well as significant im-
provement in search recall. Although we focus our
results on Russian/English queries, the techniques
(particularly for KLEs) can be applied to other lan-
guage pairs that use different character sets, such
as Korean-English and Thai-English.
</bodyText>
<sectionHeader confidence="0.99709" genericHeader="introduction">
2 Methodology
</sectionHeader>
<bodyText confidence="0.980849095238095">
In cross-border trade at eBay, multilingual queries
are translated into the inventory’s source language
prior to search. A key application of this, and
the focus of this paper, is the translation of Rus-
sian queries into English, in order to provide Rus-
sian users a more convenient interface to English-
based inventory in North America. The presence
of KLEs and homoglyphs in multilingual queries,
however, leads to poor query translations, which in
turn increases the incidence of null search results.
We have found that null search results correlate
with users exiting our site.
In this work, we seek to correct for KLEs and
homoglyphs, thereby improving query translation,
reducing the incidence of null search results, and
increasing user engagement. Prior to translation
and search, we preprocess multilingual queries
by identifying and transforming KLEs and homo-
glyphs as follows (we use the query “yexon W30B
2 new” as a running example):
(a) Tag Tokens: label each query token
</bodyText>
<page confidence="0.965215">
621
</page>
<bodyText confidence="0.94726275">
Proceedings of the 2014 Conference on Empirical Methods in Natural Language Processing (EMNLP), pages 621–626,
October 25-29, 2014, Doha, Qatar. c�2014 Association for Computational Linguistics
with one of the following semantically moti-
vated classes, which identify the user’s informa-
tion need: (i) E: a token intended as an English
search term; (ii) R: a Cyrillic token intended as a
Russian search term; (iii) K: A KLE, e.g. “w30B”
for the term “ipad”. A token intended as an En-
glish search term, but at least partially entered in
the Russian keyboard layout; (iv) H: A Russian
homoglyph for an English term, e.g. “Bmw” (un-
derlined letters Cyrillic). Employs visually sim-
ilar letters from the Cyrillic character set when
spelling an intended English term; (v) A: Ambigu-
ous tokens, consisting of numbers and punctuation
characters with equivalent codes that can be en-
tered in both Russian and English keyboard lay-
outs. Given the above classes, our example query
“yexon w30B 2 new” should be tagged as “R K A
E”.
</bodyText>
<listItem confidence="0.948968076923077">
(b) Transform Queries: Apply a deterministic
mapping to transform KLE and homoglyph tokens
from Cyrillic to ASCII characters. For KLEs the
transformation maps between characters that share
the same location in Russian and English keyboard
layouts (e.g. 0 → a, bi → s). For homoglyphs the
transformation maps between a smaller set of vi-
sually similar characters (e.g. e → e, m → m). Our
example query would be transformed into “yexon
ipad 2 new”.
(c) Translate and Search: Translate the trans-
formed query (into “case ipad 2 new” for our ex-
ample), and dispatch it to the search engine.
</listItem>
<bodyText confidence="0.999899">
In this paper, we formulate the token-level tag-
ging task as a standard multiclass classification
problem (each token is labeled independently), as
well as a sequence labeling problem (a first order
conditional Markov model). In order to provide
end-to-end results, we preprocess queries by de-
terministically transforming into ASCII the tokens
tagged by our model as KLEs or homoglyphs. We
conclude by presenting an evaluation of the impact
of this transformation on search.
</bodyText>
<subsectionHeader confidence="0.748931">
2.1 Features
</subsectionHeader>
<bodyText confidence="0.998135666666667">
Our classification and sequence models share a
common set of features grouped into the follow-
ing categories:
</bodyText>
<subsectionHeader confidence="0.469487">
2.1.1 Language Model Features
</subsectionHeader>
<bodyText confidence="0.997543685714286">
A series of 5-gram, character-level language mod-
els (LMs) capture the structure of different types
of words. Intuitively, valid Russian terms will
have high probability in Russian LMs. In contrast,
KLEs or homoglyph tokens, despite appearing on
the surface to be Russian terms, will generally
have low probability in the LMs trained on valid
Russian words. Once mapped into ASCII (see
Section 2 above), however, these tokens tend to
have higher probability in the English LMs. LMs
are trained on the following corpora:
English and Russian Vocabulary: based on
a collection of open source, parallel En-
glish/Russian corpora (∼50M words in all).
English Brands: built from a curated list of 35K
English brand names, which often have distinctive
linguistic properties compared with common En-
glish words (Lowrey et al., 2013).
Russian Transliterations: built from a col-
lection of Russian transliterations of proper
names from Wikipedia (the Russian portion of
guessed-names.ru-en made available as a
part of WMT 20131).
For every input token, each of the above LMs
fires a real-valued feature — the negated log-
probability of the token in the given language
model. Additionally, for tokens containing Cyril-
lic characters, we consider the token’s KLE and
homoglyph ASCII mappings, where available. For
each mapping, a real-valued feature fires corre-
sponding to the negated log-probability of the
mapped token in the English and Brands LMs.
Lastly, an equivalent set of LM features fires for
the two preceding and following tokens around the
current token, if applicable.
</bodyText>
<subsubsectionHeader confidence="0.653898">
2.1.2 Token Features
</subsubsectionHeader>
<bodyText confidence="0.999976428571429">
We include several features commonly used in
token-level tagging problems, such as case and
shape features, token class (such as letters-only,
digits-only), position of the token within the query,
and token length. In addition, we include fea-
tures indicating the presence of characters from
the ASCII and/or Cyrillic character sets.
</bodyText>
<subsectionHeader confidence="0.635925">
2.1.3 Dictionary Features
</subsectionHeader>
<bodyText confidence="0.998675375">
We incorporate a set of features that indicate
whether a given lowercased query token is a mem-
ber of one of the lexicons described below.
UNIX: The English dictionary shipped with Cen-
tOS, including ∼480K entries, used as a lexicon
of common English words.
BRANDS: An expanded version of the curated list
of brand names used for LM features. Includes
</bodyText>
<footnote confidence="0.977901">
1www.statmt.org/wmt13/
translation-task.html#download
</footnote>
<page confidence="0.990033">
622
</page>
<listItem confidence="0.88432">
—58K brands.
PRODUCT TITLES: A lexicon of over 1.6M en-
tries extracted from a collection of 10M product
titles from eBay’s North American inventory.
QUERY LOGS: A larger, in-domain collection of
</listItem>
<bodyText confidence="0.991060666666667">
approximately 5M entries extracted from —100M
English search queries on eBay.
Dictionary features fire for Cyrillic tokens when
the KLE and/or homoglyph-mapped version of the
token appears in the above lexicons. Dictionary
features are binary for the Unix and Brands dictio-
naries, and weighted by relative frequency of the
entry for the Product Titles and Query Logs dic-
tionaries.
</bodyText>
<sectionHeader confidence="0.999764" genericHeader="background">
3 Experiments
</sectionHeader>
<subsectionHeader confidence="0.876375">
3.1 Datasets
</subsectionHeader>
<bodyText confidence="0.991406045454545">
The following datasets were used for training and
evaluating the baseline (see Section 3.2 below) and
our proposed systems:
Training Set: A training set of 6472 human-
labeled query examples (17,239 tokens).
In-Domain Query Test Set: A set of 2500 Rus-
sian/English queries (8,357 tokens) randomly se-
lected from queries with null search results. By
focusing on queries with null results, we empha-
size the presence of KLEs and homoglyphs, which
occur in 7.8% of queries in our test set.
Queries were labeled by a team of Russian lan-
guage specialists. The test set was also indepen-
dently reviewed, which resulted in the correction
of labels for 8 out of the 8,357 query tokens.
Although our test set is representative of the
types of problematic queries targeted by our
model, our training data was not sampled using the
same methodology. We expect that the differences
in distributions between training and test sets, if
anything, make the results reported in Section 3.3
somewhat pessimistic2.
</bodyText>
<subsectionHeader confidence="0.998903">
3.2 Dictionary Baseline
</subsectionHeader>
<bodyText confidence="0.9999718">
We implemented a rule-based baseline system em-
ploying the dictionaries described in Section 2.1.3.
In this system, each token was assigned a class
k E {E, R, K, H, Al using a set of rules: a token
among a list of 101 Russian stopwords3 is tagged
</bodyText>
<footnote confidence="0.9707535">
2As expected, cross-validation experiments on the train-
ing data (for parameter tuning) yielded results slightly higher
than the results reported in Section 3.3, which use a held-out
test set
3Taken from the Russian Analyzer packaged with Lucene
— see lucene.apache.org.
</footnote>
<listItem confidence="0.717173166666667">
as R. A token containing only ASCII characters is
labeled as A if all characters are common to En-
glish and Russian keyboards (i.e. numbers and
some punctuation), otherwise E. For tokens con-
taining Cyrillic characters, KLE and homoglyph-
mapped versions are searched in our dictionaries.
</listItem>
<bodyText confidence="0.986134333333333">
If found, K or H are assigned. If both mapped ver-
sions are found in the dictionaries, then either K
or H is assigned probabilistically4. In cases where
neither mapped version is found in the dictionary,
the token assigned is either R or A, depending on
whether it consists of purely Cyrillic characters, or
a mix of Cyrillic and ASCII, respectively.
Note that the above tagging rules allow tokens
with classes E and A to be identified with perfect
accuracy. As a result, we omit these classes from
all results reported in this work. We also note
that this simplification applies because we have
restricted our attention to the Russian —* English
direction. In the bidirectional case, ASCII tokens
could represent either English tokens or KLEs (i.e.
a Russian term entered in the English keyboard
layout). We leave the joint treatment of the bidi-
rectional case to future work.
</bodyText>
<table confidence="0.9996445">
Tag Prec Recall F1
K .528 .924 .672
H .347 .510 .413
R .996 .967 .982
</table>
<tableCaption confidence="0.992822">
Table 1: Baseline results on the test set, using
</tableCaption>
<bodyText confidence="0.997679">
UNIX, BRANDS, and the PRODUCT TITLES dic-
tionaries.
We experimented with different combinations
of dictionaries, and found the best combination to
be UNIX, BRANDS, and PRODUCT TITLES dic-
tionaries (see Table 1). We observed a sharp de-
crease in precision when incorporating the QUERY
LOGS dictionary, likely due to noise in the user-
generated content.
Error analysis suggests that shorter words are
the most problematic for the baseline system5.
Shorter Cyrillic tokens, when transformed from
Cyrillic to ASCII using KLE or homoglyph map-
pings, have a higher probability of spuriously
mapping to valid English acronyms, model IDs,
or short words. For instance, Russian car brand
“ваз” maps across keyboard layouts to “dfp”
</bodyText>
<footnote confidence="0.946338">
4We experimented with selecting K or H based on a prior
computed from training data; however, results were lower
than those reported, which use random selection.
5Stopwords are particularly problematic, and hence ex-
cluded from consideration as KLEs or homoglyphs.
</footnote>
<page confidence="0.7370695">
,
623
</page>
<table confidence="0.999315625">
Tag Classification Sequence
P R F1 P R F1
K .925 .944 .935 .915 .934 .925
LR H .708 .667 .687 .686 .686 .686
R .996 .997 .996 .997 .996 .997
K .926 .949 .937 .935 .949 .942
RF H .732 .588 .652 .750 .588 .659
R .997 .997 .997 .996 .998 .997
</table>
<tableCaption confidence="0.977483">
Table 2: Classification and sequence tagging re-
sults on the test set
</tableCaption>
<bodyText confidence="0.975402133333333">
a commonly used acronym in product titles for
“Digital Flat Panel”. Russian words “мyKm” and
“рyK” similarly map by chance to English words
“verb” and “her”.
A related problem occurs with product model
IDs, and highlights the limits of treating query to-
kens independently. Consider Cyrillic query “БМВ
e46”. The first token is a Russian transliteration
for the BMW brand. The second token, “e46”,
has three possible interpretations: i) as a Russian
token; ii) a homoglyph for ASCII “e46”; or iii)
a KLE for “t46”. It is difficult to discriminate
between these options without considering token
context, and in this case having some prior knowl-
edge that e46 is a BMW model.
</bodyText>
<subsectionHeader confidence="0.995311">
3.3 Machine Learning Models
</subsectionHeader>
<bodyText confidence="0.978039740740741">
We trained linear classification models using lo-
gistic regression (LR)6, and non-linear models us-
ing random forests (RFs), using implementations
from the Scikit-learn package (Pedregosa et al.,
2011). Sequence models are implemented as first
order conditional Markov models by applying a
beam search (k = 3) on top of the LR and RF
classifiers. The LR and RF models were tuned us-
ing 5-fold cross-validation results, with models se-
lected based on the mean F1 score across R, K, and
H tags.
Table 2 shows the token-level results on our in-
domain test set. As with the baseline, we focus the
model on disambiguating between classes R, K and
H. Each of the reported models performs signifi-
cantly better than the baseline (on each tag), with
statistical significance evaluated using McNemar’s
test. The differences between LR and RF mod-
els, as well as sequence and classification variants,
however, are not statistically significant. Each of
the machine learning models achieves a query-
level accuracy score of roughly 98% (the LR se-
6Although CRFs are state-of-the-art for many tagging
problems, in our experiments they yielded results slightly
lower than LR or RF models.
quence model achieved the lowest with 97.78%,
the RF sequence model the highest with 97.90%).
Our feature ablation experiments show that
the majority of predictive power comes from the
character-level LM features. Dropping LM fea-
tures results in a significant reduction in perfor-
mance (F1 scores .878 and .638 for the RF Se-
quence model on classes K and H). These results
are still significantly above the baseline, suggest-
ing that token and dictionary features are by them-
selves good predictors. However, we do not see
a similar performance reduction when dropping
these feature groups.
We experimented with lexical features, which
are commonly used in token-level tagging prob-
lems. Results, however, were slightly lower than
the results reported in this section. We suspect the
issue is one of overfitting, due to the limited size of
our training data, and general sparsity associated
with lexical features. Continuous word presenta-
tions (Mikolov et al., 2013), noted as future work,
may offer improved generalization.
Error analysis for our machine learning mod-
els suggests patterns similar to those reported in
Section 3.2. Although errors are significantly less
frequent than in our dictionary baseline, shorter
words still present the most difficulty. We note
as future work the use of word-level LM scores
to target errors with shorter words.
</bodyText>
<subsectionHeader confidence="0.999073">
3.4 Search Results
</subsectionHeader>
<bodyText confidence="0.99972395">
Recall that we translate multilingual queries into
English prior to search. KLEs and homoglyphs
in queries result in poor query translations, often
leading to null search results.
To evaluate the impact of KLE and homoglyph
correction, we consider a set of 100k randomly se-
lected Russian/English queries. We consider the
subset of queries that the RF or baseline models
predict as containing a KLE or homoglyph. Next,
we translate into English both the original query,
as well as a transformed version of it, with KLEs
and homoglyphs replaced with their ASCII map-
pings. Lastly, we execute independent searches
using original and transformed query translations.
Table 3 provides details on search results for
original and transformed queries. The baseline
model transforms over 12.6% of the 100k queries.
Of those, 24.3% yield search results where the un-
modified queries had null search results (i.e. Null
-* Non-null). In 20.9% of the cases, however, the
</bodyText>
<page confidence="0.997686">
624
</page>
<bodyText confidence="0.9999142">
transformations are destructive (i.e. Non-null —*
Null), and yield null results where the unmodified
query produced results.
Compared with the baseline, the RF model
transforms only 7.4% of the 100k queries; a frac-
tion that is roughly in line with the 7.8% of queries
in our test set that contain KLEs or homoglyphs.
In over 42% of the cases (versus 24.3% for the
baseline), the transformed query generates search
results where the original query yields none. Only
4.81% of the transformations using the RF model
are destructive; a fraction significantly lower than
the baseline.
Note that we distinguish here only between
queries that produce null results, and those that do
not. We do not include queries for which original
and transformed queries both produce (potentially
differing) search results. Evaluating these cases
requires deeper insight into the relevance of search
results, which is left as future work.
</bodyText>
<table confidence="0.99899625">
Baseline RF model
#Transformed 12,661 7,364
Null --+ Non-Null 3,078 (24.3%) 3,142 (42.7%)
Non-Null --+ Null 2,651 (20.9%) 354 (4.81%)
</table>
<tableCaption confidence="0.995282">
Table 3: Impact of KLE and homoglyph correction
</tableCaption>
<bodyText confidence="0.406585">
on search results for 100k queries
</bodyText>
<sectionHeader confidence="0.999943" genericHeader="related work">
4 Related Work
</sectionHeader>
<bodyText confidence="0.999937368421053">
Baytin et al. (2013) first refer to keyboard lay-
out errors in their work. However, their focus is
on predicting the performance of spell-correction,
not on fixing KLEs observed in their data. To
our knowledge, our work is the first to introduce
this problem and to propose a machine learning
solution. Since our task is a token-level tagging
problem, it is very similar to the part-of-speech
(POS) tagging task (Ratnaparkhi, 1996), only with
a very small set of candidate tags. We chose
a supervised machine learning approach in order
to achieve maximum precision. However, this
problem can also be approached in an unsuper-
vised setting, similar to the method Whitelaw et al.
(2009) use for spelling correction. In that setup,
the goal would be to directly choose the correct
transformation for an ill-formed KLE or homo-
glyph, instead of a tagging step followed by a de-
terministic mapping to ASCII.
</bodyText>
<sectionHeader confidence="0.995155" genericHeader="conclusions">
5 Conclusions and Future Work
</sectionHeader>
<bodyText confidence="0.999996">
We investigate two kinds of errors in search
queries: keyboard layout errors (KLEs) and ho-
moglyphs. Applying machine learning methods,
we are able to accurately identify a user’s intended
query, in spite of the presence of KLEs and ho-
moglyphs. The proposed models are based largely
on compact, character-level language models. The
proposed techniques, when applied to multilingual
queries prior to translation and search, offer signif-
icant gains in search results.
In the future, we plan to focus on additional fea-
tures to improve KLE and homoglyph discrimina-
tion for shorter words and acronyms. Although
lexical features did not prove useful for this work,
presumably due to data sparsity and overfitting
issues, we intend to explore the application of
continuous word representations (Mikolov et al.,
2013). Compared with lexical features, we expect
continuous representations to be less susceptible
to overfitting, and to generalize better to unknown
words. For instance, using continuous word rep-
resentations, Turian et al. (2010) show significant
gains for a named entity recognition task.
We also intend on exploring the use of features
from in-domain, word-level LMs. Word-level fea-
tures are expected to be particularly useful in the
case of spurious mappings (e.g. “ваз” vs. “dfp”
from Section 3.2), where context from surround-
ing tokens in a query can often help in resolving
ambiguity. Word-level features may also be useful
in re-ranking translated queries prior to search, in
order to reduce the incidence of erroneous query
transformations generated through our methods.
Finally, our future work will explore KLE and ho-
moglyph correction bidirectionally, as opposed to
the unidirectional approach explored in this work.
</bodyText>
<sectionHeader confidence="0.998361" genericHeader="acknowledgments">
Acknowledgments
</sectionHeader>
<bodyText confidence="0.999951285714286">
We would like to thank Jean-David Ruvini, Mike
Dillinger, Saˇsa Hasan, Irina Borisova and the
anonymous reviewers for their valuable feedback.
We also thank our Russian language special-
ists Tanya Badeka, Tatiana Kontsevich and Olga
Pospelova for their support in labeling and review-
ing datasets.
</bodyText>
<sectionHeader confidence="0.999043" genericHeader="references">
References
</sectionHeader>
<reference confidence="0.9672735">
Alexey Baytin, Irina Galinskaya, Marina Panina, and
Pavel Serdyukov. 2013. Speller performance pre-
</reference>
<page confidence="0.98562">
625
</page>
<reference confidence="0.999421216216216">
diction for query autocorrection. In Proceedings
of the 22nd ACM International Conference on Con-
ference on Information &amp; Knowledge Management,
pages 1821–1824.
Tina M. Lowrey, Larry J. Shrum, and Tony M. Du-
bitsky. 2013. The Relation Between Brand-name
Linguistic Characteristics and Brand-name Memory.
Journal of Advertising, 32(3):7–17.
Tomas Mikolov, Kai Chen, Greg Corrado, and Jeffrey
Dean. 2013. Efficient estimation of word represen-
tations in vector space. CoRR, abs/1301.3781.
Tristan Miller. 2013. Russian–English Homoglyphs,
Homographs, and Homographic Translations. Word
Ways: The Journal of Recreational Linguistics,
46(3):165–168.
Fabian Pedregosa, Ga¨el Varoquaux, Alexandre Gram-
fort, Vincent Michel, Bertrand Thirion, Olivier
Grisel, Mathieu Blondel, Peter Prettenhofer, Ron
Weiss, Vincent Dubourg, Jake Vanderplas, Alexan-
dre Passos, David Cournapeau, Matthieu Brucher,
Matthieu Perrot, and Edouard Duchesnay. 2011.
Scikit-learn: Machine Learning in Python. Journal
of Machine Learning Research, 12:2825–2830.
Adwait Ratnaparkhi. 1996. A Maximum Entropy
Model for Part–of–Speech Tagging. In Proceedings
of the Conference on Empirical Methods in Natural
Language Processing, pages 133–142.
Joseph Turian, Lev Ratinov, and Yoshua Bengio. 2010.
Word representations: A simple and general method
for semi-supervised learning. In Proceedings of
ACL, pages 384–394.
Casey Whitelaw, Ben Hutchinson, Grace Y. Chung,
and Ged Ellis. 2009. Using the Web for Lan-
guage Independent Spellchecking and Autocorrec-
tion. In Proceedings of the 2009 Conference on Em-
pirical Methods in Natural Language Processing,
pages 890–899.
</reference>
<page confidence="0.998807">
626
</page>
</variant>
</algorithm>
<algorithm name="ParsHed" version="110505">
<variant no="0" confidence="0.754812">
<title confidence="0.999927">Correcting Keyboard Layout Errors and Homoglyphs in Queries</title>
<author confidence="0.997606">Derek Barnes Mahesh Joshi Hassan Sawaf</author>
<email confidence="0.989533">debarnes@ebay.commahesh.joshi@ebay.comhsawaf@ebay.com</email>
<address confidence="0.769917">eBay Inc., 2065 Hamilton Ave, San Jose, CA, 95125, USA</address>
<abstract confidence="0.999486083333333">Keyboard layout errors and homoglyphs in cross-language queries impact our ability to correctly interpret user information needs and offer relevant results. We present a machine learning approach to correcting these errors, based largely character-level features. We demonstrate superior performance over rule-based methods, as well as a significant reduction in the number of queries that yield null search results.</abstract>
</variant>
</algorithm>
<algorithm name="ParsCit" version="110505">
<citationList>
<citation valid="true">
<authors>
<author>Alexey Baytin</author>
<author>Irina Galinskaya</author>
<author>Marina Panina</author>
<author>Pavel Serdyukov</author>
</authors>
<title>Speller performance prediction for query autocorrection.</title>
<date>2013</date>
<booktitle>In Proceedings of the 22nd ACM International Conference on Conference on Information &amp; Knowledge Management,</booktitle>
<pages>1821--1824</pages>
<contexts>
<context position="1572" citStr="Baytin et al., 2013" startWordPosition="228" endWordPosition="231">s. New challenges arise for search engines in cross-border eCommerce. In this paper, we focus on two cross-linguistic phenomena that make interpreting queries difficult: (i) Homoglyphs: (Miller, 2013): Tokens such as “case” (underlined letters Cyrillic), in which users mix characters from different character sets that are visually similar or identical. For instance, English and Russian alphabets share homoglyphs such as c, a, e, o, y, k, etc. Although the letters are visually similar or in some cases identical, the underlying character codes are different. (ii) Keyboard Layout Errors (KLEs): (Baytin et al., 2013): When switching one’s keyboard between language modes, users at times enter terms in the wrong character set. For instance, “yexon W30B” may appear to be a Russian query. While “yexon” is the Russian word for “case”, “W30B” is actually the user’s attempt to enter the characters “ipad” while leaving their keyboard in Russian language mode. Queries containing KLEs or homoglyphs are unlikely to produce any search results, unless the intended ASCII sequences can be recovered. In a test set sampled from Russian/English queries with null (i.e. empty) search results (see Section 3.1), we found appro</context>
<context position="18354" citStr="Baytin et al. (2013)" startWordPosition="2944" endWordPosition="2947">ntly lower than the baseline. Note that we distinguish here only between queries that produce null results, and those that do not. We do not include queries for which original and transformed queries both produce (potentially differing) search results. Evaluating these cases requires deeper insight into the relevance of search results, which is left as future work. Baseline RF model #Transformed 12,661 7,364 Null --+ Non-Null 3,078 (24.3%) 3,142 (42.7%) Non-Null --+ Null 2,651 (20.9%) 354 (4.81%) Table 3: Impact of KLE and homoglyph correction on search results for 100k queries 4 Related Work Baytin et al. (2013) first refer to keyboard layout errors in their work. However, their focus is on predicting the performance of spell-correction, not on fixing KLEs observed in their data. To our knowledge, our work is the first to introduce this problem and to propose a machine learning solution. Since our task is a token-level tagging problem, it is very similar to the part-of-speech (POS) tagging task (Ratnaparkhi, 1996), only with a very small set of candidate tags. We chose a supervised machine learning approach in order to achieve maximum precision. However, this problem can also be approached in an unsu</context>
</contexts>
<marker>Baytin, Galinskaya, Panina, Serdyukov, 2013</marker>
<rawString>Alexey Baytin, Irina Galinskaya, Marina Panina, and Pavel Serdyukov. 2013. Speller performance prediction for query autocorrection. In Proceedings of the 22nd ACM International Conference on Conference on Information &amp; Knowledge Management, pages 1821–1824.</rawString>
</citation>
<citation valid="true">
<authors>
<author>Tina M Lowrey</author>
<author>Larry J Shrum</author>
<author>Tony M Dubitsky</author>
</authors>
<title>The Relation Between Brand-name Linguistic Characteristics and Brand-name Memory.</title>
<date>2013</date>
<journal>Journal of Advertising,</journal>
<volume>32</volume>
<issue>3</issue>
<contexts>
<context position="6749" citStr="Lowrey et al., 2013" startWordPosition="1061" endWordPosition="1064">Es or homoglyph tokens, despite appearing on the surface to be Russian terms, will generally have low probability in the LMs trained on valid Russian words. Once mapped into ASCII (see Section 2 above), however, these tokens tend to have higher probability in the English LMs. LMs are trained on the following corpora: English and Russian Vocabulary: based on a collection of open source, parallel English/Russian corpora (∼50M words in all). English Brands: built from a curated list of 35K English brand names, which often have distinctive linguistic properties compared with common English words (Lowrey et al., 2013). Russian Transliterations: built from a collection of Russian transliterations of proper names from Wikipedia (the Russian portion of guessed-names.ru-en made available as a part of WMT 20131). For every input token, each of the above LMs fires a real-valued feature — the negated logprobability of the token in the given language model. Additionally, for tokens containing Cyrillic characters, we consider the token’s KLE and homoglyph ASCII mappings, where available. For each mapping, a real-valued feature fires corresponding to the negated log-probability of the mapped token in the English and</context>
</contexts>
<marker>Lowrey, Shrum, Dubitsky, 2013</marker>
<rawString>Tina M. Lowrey, Larry J. Shrum, and Tony M. Dubitsky. 2013. The Relation Between Brand-name Linguistic Characteristics and Brand-name Memory. Journal of Advertising, 32(3):7–17.</rawString>
</citation>
<citation valid="true">
<authors>
<author>Tomas Mikolov</author>
<author>Kai Chen</author>
<author>Greg Corrado</author>
<author>Jeffrey Dean</author>
</authors>
<title>Efficient estimation of word representations in vector space.</title>
<date>2013</date>
<location>CoRR, abs/1301.3781.</location>
<contexts>
<context position="15818" citStr="Mikolov et al., 2013" startWordPosition="2541" endWordPosition="2544">lasses K and H). These results are still significantly above the baseline, suggesting that token and dictionary features are by themselves good predictors. However, we do not see a similar performance reduction when dropping these feature groups. We experimented with lexical features, which are commonly used in token-level tagging problems. Results, however, were slightly lower than the results reported in this section. We suspect the issue is one of overfitting, due to the limited size of our training data, and general sparsity associated with lexical features. Continuous word presentations (Mikolov et al., 2013), noted as future work, may offer improved generalization. Error analysis for our machine learning models suggests patterns similar to those reported in Section 3.2. Although errors are significantly less frequent than in our dictionary baseline, shorter words still present the most difficulty. We note as future work the use of word-level LM scores to target errors with shorter words. 3.4 Search Results Recall that we translate multilingual queries into English prior to search. KLEs and homoglyphs in queries result in poor query translations, often leading to null search results. To evaluate t</context>
<context position="20071" citStr="Mikolov et al., 2013" startWordPosition="3221" endWordPosition="3224"> query, in spite of the presence of KLEs and homoglyphs. The proposed models are based largely on compact, character-level language models. The proposed techniques, when applied to multilingual queries prior to translation and search, offer significant gains in search results. In the future, we plan to focus on additional features to improve KLE and homoglyph discrimination for shorter words and acronyms. Although lexical features did not prove useful for this work, presumably due to data sparsity and overfitting issues, we intend to explore the application of continuous word representations (Mikolov et al., 2013). Compared with lexical features, we expect continuous representations to be less susceptible to overfitting, and to generalize better to unknown words. For instance, using continuous word representations, Turian et al. (2010) show significant gains for a named entity recognition task. We also intend on exploring the use of features from in-domain, word-level LMs. Word-level features are expected to be particularly useful in the case of spurious mappings (e.g. “ваз” vs. “dfp” from Section 3.2), where context from surrounding tokens in a query can often help in resolving ambiguity. Word-level f</context>
</contexts>
<marker>Mikolov, Chen, Corrado, Dean, 2013</marker>
<rawString>Tomas Mikolov, Kai Chen, Greg Corrado, and Jeffrey Dean. 2013. Efficient estimation of word representations in vector space. CoRR, abs/1301.3781.</rawString>
</citation>
<citation valid="true">
<authors>
<author>Tristan Miller</author>
</authors>
<title>Russian–English Homoglyphs, Homographs, and Homographic Translations. Word Ways:</title>
<date>2013</date>
<journal>The Journal of Recreational Linguistics,</journal>
<volume>46</volume>
<issue>3</issue>
<contexts>
<context position="1152" citStr="Miller, 2013" startWordPosition="164" endWordPosition="165">based methods, as well as a significant reduction in the number of queries that yield null search results. 1 Introduction The success of an eCommerce site depends on how well users are connected with products and services of interest. Users typically communicate their desires through search queries; however, queries are often incomplete and contain errors, which impact the quantity and quality of search results. New challenges arise for search engines in cross-border eCommerce. In this paper, we focus on two cross-linguistic phenomena that make interpreting queries difficult: (i) Homoglyphs: (Miller, 2013): Tokens such as “case” (underlined letters Cyrillic), in which users mix characters from different character sets that are visually similar or identical. For instance, English and Russian alphabets share homoglyphs such as c, a, e, o, y, k, etc. Although the letters are visually similar or in some cases identical, the underlying character codes are different. (ii) Keyboard Layout Errors (KLEs): (Baytin et al., 2013): When switching one’s keyboard between language modes, users at times enter terms in the wrong character set. For instance, “yexon W30B” may appear to be a Russian query. While “y</context>
</contexts>
<marker>Miller, 2013</marker>
<rawString>Tristan Miller. 2013. Russian–English Homoglyphs, Homographs, and Homographic Translations. Word Ways: The Journal of Recreational Linguistics, 46(3):165–168.</rawString>
</citation>
<citation valid="true">
<authors>
<author>Fabian Pedregosa</author>
<author>Alexandre Gramfort Ga¨el Varoquaux</author>
<author>Vincent Michel</author>
<author>Bertrand Thirion</author>
<author>Olivier Grisel</author>
<author>Mathieu Blondel</author>
<author>Peter Prettenhofer</author>
</authors>
<title>Scikit-learn: Machine Learning in Python.</title>
<date>2011</date>
<journal>Journal of Machine Learning Research,</journal>
<pages>12--2825</pages>
<location>Ron Weiss, Vincent Dubourg, Jake Vanderplas, Alexandre Passos, David</location>
<marker>Pedregosa, Ga¨el Varoquaux, Michel, Thirion, Grisel, Blondel, Prettenhofer, 2011</marker>
<rawString>Fabian Pedregosa, Ga¨el Varoquaux, Alexandre Gramfort, Vincent Michel, Bertrand Thirion, Olivier Grisel, Mathieu Blondel, Peter Prettenhofer, Ron Weiss, Vincent Dubourg, Jake Vanderplas, Alexandre Passos, David Cournapeau, Matthieu Brucher, Matthieu Perrot, and Edouard Duchesnay. 2011. Scikit-learn: Machine Learning in Python. Journal of Machine Learning Research, 12:2825–2830.</rawString>
</citation>
<citation valid="true">
<authors>
<author>Adwait Ratnaparkhi</author>
</authors>
<title>A Maximum Entropy Model for Part–of–Speech Tagging.</title>
<date>1996</date>
<booktitle>In Proceedings of the Conference on Empirical Methods in Natural Language Processing,</booktitle>
<pages>133--142</pages>
<contexts>
<context position="18764" citStr="Ratnaparkhi, 1996" startWordPosition="3013" endWordPosition="3014">Null --+ Non-Null 3,078 (24.3%) 3,142 (42.7%) Non-Null --+ Null 2,651 (20.9%) 354 (4.81%) Table 3: Impact of KLE and homoglyph correction on search results for 100k queries 4 Related Work Baytin et al. (2013) first refer to keyboard layout errors in their work. However, their focus is on predicting the performance of spell-correction, not on fixing KLEs observed in their data. To our knowledge, our work is the first to introduce this problem and to propose a machine learning solution. Since our task is a token-level tagging problem, it is very similar to the part-of-speech (POS) tagging task (Ratnaparkhi, 1996), only with a very small set of candidate tags. We chose a supervised machine learning approach in order to achieve maximum precision. However, this problem can also be approached in an unsupervised setting, similar to the method Whitelaw et al. (2009) use for spelling correction. In that setup, the goal would be to directly choose the correct transformation for an ill-formed KLE or homoglyph, instead of a tagging step followed by a deterministic mapping to ASCII. 5 Conclusions and Future Work We investigate two kinds of errors in search queries: keyboard layout errors (KLEs) and homoglyphs. A</context>
</contexts>
<marker>Ratnaparkhi, 1996</marker>
<rawString>Adwait Ratnaparkhi. 1996. A Maximum Entropy Model for Part–of–Speech Tagging. In Proceedings of the Conference on Empirical Methods in Natural Language Processing, pages 133–142.</rawString>
</citation>
<citation valid="true">
<authors>
<author>Joseph Turian</author>
<author>Lev Ratinov</author>
<author>Yoshua Bengio</author>
</authors>
<title>Word representations: A simple and general method for semi-supervised learning.</title>
<date>2010</date>
<booktitle>In Proceedings of ACL,</booktitle>
<pages>384--394</pages>
<contexts>
<context position="20297" citStr="Turian et al. (2010)" startWordPosition="3253" endWordPosition="3256">search, offer significant gains in search results. In the future, we plan to focus on additional features to improve KLE and homoglyph discrimination for shorter words and acronyms. Although lexical features did not prove useful for this work, presumably due to data sparsity and overfitting issues, we intend to explore the application of continuous word representations (Mikolov et al., 2013). Compared with lexical features, we expect continuous representations to be less susceptible to overfitting, and to generalize better to unknown words. For instance, using continuous word representations, Turian et al. (2010) show significant gains for a named entity recognition task. We also intend on exploring the use of features from in-domain, word-level LMs. Word-level features are expected to be particularly useful in the case of spurious mappings (e.g. “ваз” vs. “dfp” from Section 3.2), where context from surrounding tokens in a query can often help in resolving ambiguity. Word-level features may also be useful in re-ranking translated queries prior to search, in order to reduce the incidence of erroneous query transformations generated through our methods. Finally, our future work will explore KLE and homo</context>
</contexts>
<marker>Turian, Ratinov, Bengio, 2010</marker>
<rawString>Joseph Turian, Lev Ratinov, and Yoshua Bengio. 2010. Word representations: A simple and general method for semi-supervised learning. In Proceedings of ACL, pages 384–394.</rawString>
</citation>
<citation valid="true">
<authors>
<author>Casey Whitelaw</author>
<author>Ben Hutchinson</author>
<author>Grace Y Chung</author>
<author>Ged Ellis</author>
</authors>
<title>Using the Web for Language Independent Spellchecking and Autocorrection.</title>
<date>2009</date>
<booktitle>In Proceedings of the 2009 Conference on Empirical Methods in Natural Language Processing,</booktitle>
<pages>890--899</pages>
<contexts>
<context position="19016" citStr="Whitelaw et al. (2009)" startWordPosition="3053" endWordPosition="3056">n their work. However, their focus is on predicting the performance of spell-correction, not on fixing KLEs observed in their data. To our knowledge, our work is the first to introduce this problem and to propose a machine learning solution. Since our task is a token-level tagging problem, it is very similar to the part-of-speech (POS) tagging task (Ratnaparkhi, 1996), only with a very small set of candidate tags. We chose a supervised machine learning approach in order to achieve maximum precision. However, this problem can also be approached in an unsupervised setting, similar to the method Whitelaw et al. (2009) use for spelling correction. In that setup, the goal would be to directly choose the correct transformation for an ill-formed KLE or homoglyph, instead of a tagging step followed by a deterministic mapping to ASCII. 5 Conclusions and Future Work We investigate two kinds of errors in search queries: keyboard layout errors (KLEs) and homoglyphs. Applying machine learning methods, we are able to accurately identify a user’s intended query, in spite of the presence of KLEs and homoglyphs. The proposed models are based largely on compact, character-level language models. The proposed techniques, w</context>
</contexts>
<marker>Whitelaw, Hutchinson, Chung, Ellis, 2009</marker>
<rawString>Casey Whitelaw, Ben Hutchinson, Grace Y. Chung, and Ged Ellis. 2009. Using the Web for Language Independent Spellchecking and Autocorrection. In Proceedings of the 2009 Conference on Empirical Methods in Natural Language Processing, pages 890–899.</rawString>
</citation>
</citationList>
</algorithm>
</algorithms>
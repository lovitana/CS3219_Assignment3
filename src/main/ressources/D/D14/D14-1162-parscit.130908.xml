<?xml version="1.0" encoding="UTF-8"?>
<algorithms version="110505">
<algorithm name="SectLabel" version="110505">
<variant no="0" confidence="0.000014">
<title confidence="0.995486">
GloVe: Global Vectors for Word Representation
</title>
<author confidence="0.997307">
Jeffrey Pennington, Richard Socher, Christopher D. Manning
</author>
<affiliation confidence="0.994674">
Computer Science Department, Stanford University, Stanford, CA 94305
</affiliation>
<email confidence="0.967016">
jpennin@stanford.edu, richard@socher.org, manning@stanford.edu
</email>
<sectionHeader confidence="0.994614" genericHeader="abstract">
Abstract
</sectionHeader>
<bodyText confidence="0.999929166666667">
Recent methods for learning vector space
representations of words have succeeded
in capturing fine-grained semantic and
syntactic regularities using vector arith-
metic, but the origin of these regularities
has remained opaque. We analyze and
make explicit the model properties needed
for such regularities to emerge in word
vectors. The result is a new global log-
bilinear regression model that combines
the advantages of the two major model
families in the literature: global matrix
factorization and local context window
methods. Our model efficiently leverages
statistical information by training only on
the nonzero elements in a word-word co-
occurrence matrix, rather than on the en-
tire sparse matrix or on individual context
windows in a large corpus. The model pro-
duces a vector space with meaningful sub-
structure, as evidenced by its performance
of 75% on a recent word analogy task. It
also outperforms related models on simi-
larity tasks and named entity recognition.
</bodyText>
<sectionHeader confidence="0.998882" genericHeader="introduction">
1 Introduction
</sectionHeader>
<bodyText confidence="0.999755090909091">
Semantic vector space models of language repre-
sent each word with a real-valued vector. These
vectors can be used as features in a variety of ap-
plications, such as information retrieval (Manning
et al., 2008), document classification (Sebastiani,
2002), question answering (Tellex et al., 2003),
named entity recognition (Turian et al., 2010), and
parsing (Socher et al., 2013).
Most word vector methods rely on the distance
or angle between pairs of word vectors as the pri-
mary method for evaluating the intrinsic quality
of such a set of word representations. Recently,
Mikolov et al. (2013c) introduced a new evalua-
tion scheme based on word analogies that probes
the finer structure of the word vector space by ex-
amining not the scalar distance between word vec-
tors, but rather their various dimensions of dif-
ference. For example, the analogy “king is to
queen as man is to woman” should be encoded
in the vector space by the vector equation king −
queen = man − woman. This evaluation scheme
favors models that produce dimensions of mean-
ing, thereby capturing the multi-clustering idea of
distributed representations (Bengio, 2009).
The two main model families for learning word
vectors are: 1) global matrix factorization meth-
ods, such as latent semantic analysis (LSA) (Deer-
wester et al., 1990) and 2) local context window
methods, such as the skip-gram model of Mikolov
et al. (2013c). Currently, both families suffer sig-
nificant drawbacks. While methods like LSA ef-
ficiently leverage statistical information, they do
relatively poorly on the word analogy task, indi-
cating a sub-optimal vector space structure. Meth-
ods like skip-gram may do better on the analogy
task, but they poorly utilize the statistics of the cor-
pus since they train on separate local context win-
dows instead of on global co-occurrence counts.
In this work, we analyze the model properties
necessary to produce linear directions of meaning
and argue that global log-bilinear regression mod-
els are appropriate for doing so. We propose a spe-
cific weighted least squares model that trains on
global word-word co-occurrence counts and thus
makes efficient use of statistics. The model pro-
duces a word vector space with meaningful sub-
structure, as evidenced by its state-of-the-art per-
formance of 75% accuracy on the word analogy
dataset. We also demonstrate that our methods
outperform other current methods on several word
similarity tasks, and also on a common named en-
tity recognition (NER) benchmark.
We provide the source code for the model as
well as trained word vectors at http://nlp.
stanford.edu/projects/glove/.
</bodyText>
<page confidence="0.951295">
1532
</page>
<note confidence="0.9680895">
Proceedings of the 2014 Conference on Empirical Methods in Natural Language Processing (EMNLP), pages 1532–1543,
October 25-29, 2014, Doha, Qatar. c�2014 Association for Computational Linguistics
</note>
<sectionHeader confidence="0.996852" genericHeader="related work">
2 Related Work
</sectionHeader>
<bodyText confidence="0.9974842625">
Matrix Factorization Methods. Matrix factor-
ization methods for generating low-dimensional
word representations have roots stretching as far
back as LSA. These methods utilize low-rank ap-
proximations to decompose large matrices that
capture statistical information about a corpus. The
particular type of information captured by such
matrices varies by application. In LSA, the ma-
trices are of “term-document” type, i.e., the rows
correspond to words or terms, and the columns
correspond to different documents in the corpus.
In contrast, the Hyperspace Analogue to Language
(HAL) (Lund and Burgess, 1996), for example,
utilizes matrices of “term-term” type, i.e., the rows
and columns correspond to words and the entries
correspond to the number of times a given word
occurs in the context of another given word.
A main problem with HAL and related meth-
ods is that the most frequent words contribute a
disproportionate amount to the similarity measure:
the number of times two words co-occur with the
or and, for example, will have a large effect on
their similarity despite conveying relatively little
about their semantic relatedness. A number of
techniques exist that addresses this shortcoming of
HAL, such as the COALS method (Rohde et al.,
2006), in which the co-occurrence matrix is first
transformed by an entropy- or correlation-based
normalization. An advantage of this type of trans-
formation is that the raw co-occurrence counts,
which for a reasonably sized corpus might span
8 or 9 orders of magnitude, are compressed so as
to be distributed more evenly in a smaller inter-
val. A variety of newer models also pursue this
approach, including a study (Bullinaria and Levy,
2007) that indicates that positive pointwise mu-
tual information (PPMI) is a good transformation.
More recently, a square root type transformation
in the form of Hellinger PCA (HPCA) (Lebret and
Collobert, 2014) has been suggested as an effec-
tive way of learning word representations.
Shallow Window-Based Methods. Another
approach is to learn word representations that aid
in making predictions within local context win-
dows. For example, Bengio et al. (2003) intro-
duced a model that learns word vector representa-
tions as part of a simple neural network architec-
ture for language modeling. Collobert and Weston
(2008) decoupled the word vector training from
the downstream training objectives, which paved
the way for Collobert et al. (2011) to use the full
context of a word for learning the word represen-
tations, rather than just the preceding context as is
the case with language models.
Recently, the importance of the full neural net-
work structure for learning useful word repre-
sentations has been called into question. The
skip-gram and continuous bag-of-words (CBOW)
models of Mikolov et al. (2013a) propose a sim-
ple single-layer architecture based on the inner
product between two word vectors. Mnih and
Kavukcuoglu (2013) also proposed closely-related
vector log-bilinear models, vLBL and ivLBL, and
Levy et al. (2014) proposed explicit word embed-
dings based on a PPMI metric.
In the skip-gram and ivLBL models, the objec-
tive is to predict a word’s context given the word
itself, whereas the objective in the CBOW and
vLBL models is to predict a word given its con-
text. Through evaluation on a word analogy task,
these models demonstrated the capacity to learn
linguistic patterns as linear relationships between
the word vectors.
Unlike the matrix factorization methods, the
shallow window-based methods suffer from the
disadvantage that they do not operate directly on
the co-occurrence statistics of the corpus. Instead,
these models scan context windows across the en-
tire corpus, which fails to take advantage of the
vast amount of repetition in the data.
</bodyText>
<sectionHeader confidence="0.992778" genericHeader="method">
3 The GloVe Model
</sectionHeader>
<bodyText confidence="0.99996552631579">
The statistics of word occurrences in a corpus is
the primary source of information available to all
unsupervised methods for learning word represen-
tations, and although many such methods now ex-
ist, the question still remains as to how meaning
is generated from these statistics, and how the re-
sulting word vectors might represent that meaning.
In this section, we shed some light on this ques-
tion. We use our insights to construct a new model
for word representation which we call GloVe, for
Global Vectors, because the global corpus statis-
tics are captured directly by the model.
First we establish some notation. Let the matrix
of word-word co-occurrence counts be denoted by
X, whose entries Xi.i tabulate the number of times
word j occurs in the context of word i. Let Xi =
Ek Xik be the number of times any word appears
in the context of word i. Finally, let Pi.i = P(j|i) =
Xi.i/Xi be the probability that word j appear in the
</bodyText>
<page confidence="0.996684">
1533
</page>
<tableCaption confidence="0.76811">
Table 1: Co-occurrence probabilities for target words ice and steam with selected context words from a 6
billion token corpus. Only in the ratio does noise from non-discriminative words like water and fashion
cancel out, so that large values (much greater than 1) correlate well with properties specific to ice, and
small values (much less than 1) correlate well with properties specific of steam.
</tableCaption>
<equation confidence="0.826723">
Probability and Ratio k = solid k = gas k = water k =fashion
P(k|ice) 1.9 x 10−4 6.6 x 10−5 3.0 x 10−3 1.7 x 10−5
P(k|steam) 2.2 x 10−5 7.8 x 10−4 2.2 x 10−3 1.8 x 10−5
P(k|ice)/P(k|steam) 8.9 8.5 x 10−2 1.36 0.96
</equation>
<bodyText confidence="0.997758333333334">
context of word i.
We begin with a simple example that showcases
how certain aspects of meaning can be extracted
directly from co-occurrence probabilities. Con-
sider two words i and j that exhibit a particular as-
pect of interest; for concreteness, suppose we are
interested in the concept of thermodynamic phase,
for which we might take i = ice and j = steam.
The relationship of these words can be examined
by studying the ratio of their co-occurrence prob-
abilities with various probe words, k. For words
k related to ice but not steam, say k = solid, we
expect the ratio Pik/Pjk will be large. Similarly,
for words k related to steam but not ice, say k =
gas, the ratio should be small. For words k like
water or fashion, that are either related to both ice
and steam, or to neither, the ratio should be close
to one. Table 1 shows these probabilities and their
ratios for a large corpus, and the numbers confirm
these expectations. Compared to the raw probabil-
ities, the ratio is better able to distinguish relevant
words (solid and gas) from irrelevant words (water
and fashion) and it is also better able to discrimi-
nate between the two relevant words.
The above argument suggests that the appropri-
ate starting point for word vector learning should
be with ratios of co-occurrence probabilities rather
than the probabilities themselves. Noting that the
ratio Pik/Pjk depends on three words i, j, and k,
the most general model takes the form,
</bodyText>
<equation confidence="0.985944333333333">
Pik
F(wi,wj, ˜wk ) = , (1)
Pjk
</equation>
<bodyText confidence="0.9984284">
where w E Rd are word vectors and w˜ E Rd
are separate context word vectors whose role will
be discussed in Section 4.2. In this equation, the
right-hand side is extracted from the corpus, and
F may depend on some as-of-yet unspecified pa-
rameters. The number of possibilities for F is vast,
but by enforcing a few desiderata we can select a
unique choice. First, we would like F to encode
the information present the ratio Pik/Pjk in the
word vector space. Since vector spaces are inher-
ently linear structures, the most natural way to do
this is with vector differences. With this aim, we
can restrict our consideration to those functions F
that depend only on the difference of the two target
words, modifying Eqn. (1) to,
</bodyText>
<equation confidence="0.957824">
Pik .(2)
Pjk
</equation>
<bodyText confidence="0.997122">
Next, we note that the arguments of F in Eqn. (2)
are vectors while the right-hand side is a scalar.
While F could be taken to be a complicated func-
tion parameterized by, e.g., a neural network, do-
ing so would obfuscate the linear structure we are
trying to capture. To avoid this issue, we can first
take the dot product of the arguments,
</bodyText>
<equation confidence="0.94953">
( ) = Pik
F (wi − wj)T ˜wk , (3)
Pjk
</equation>
<bodyText confidence="0.999700363636364">
which prevents F from mixing the vector dimen-
sions in undesirable ways. Next, note that for
word-word co-occurrence matrices, the distinction
between a word and a context word is arbitrary and
that we are free to exchange the two roles. To do so
consistently, we must not only exchange w H w˜
but also X H XT. Our final model should be in-
variant under this relabeling, but Eqn. (3) is not.
However, the symmetry can be restored in two
steps. First, we require that F be a homomorphism
between the groups (R,+) and (R,0, x), i.e.,
</bodyText>
<equation confidence="0.9943612">
F(wTj ˜wk) , (4)
which, by Eqn. (3), is solved by,
F(wT i ˜wk) = Pik = Xik . (5)
Xi
The solution to Eqn. (4) is F = exp, or,
wTi ˜wk = log(Pik) = log(Xik) − log(Xi) . (6)
F(wi − wj, ˜wk) =
( )
F (wi − wj)T ˜wk =
F(wTi ˜wk)
</equation>
<page confidence="0.957753">
1534
</page>
<bodyText confidence="0.9997665">
Next, we note that Eqn. (6) would exhibit the ex-
change symmetry if not for the log(Xi) on the
right-hand side. However, this term is indepen-
dent of k so it can be absorbed into a bias bi for
wi. Finally, adding an additional bias ˜bk for ˜wk
restores the symmetry,
</bodyText>
<equation confidence="0.8548535">
wTi ˜wk + bi + ˜bk = log(Xik) . (7)
Eqn. (7) is a drastic simplification over Eqn. (1),
</equation>
<bodyText confidence="0.9916353">
but it is actually ill-defined since the logarithm di-
verges whenever its argument is zero. One reso-
lution to this issue is to include an additive shift
in the logarithm, log(Xik) → log(1 + Xik), which
maintains the sparsity of X while avoiding the di-
vergences. The idea of factorizing the log of the
co-occurrence matrix is closely related to LSA and
we will use the resulting model as a baseline in
our experiments. A main drawback to this model
is that it weighs all co-occurrences equally, even
those that happen rarely or never. Such rare co-
occurrences are noisy and carry less information
than the more frequent ones — yet even just the
zero entries account for 75–95% of the data in X,
depending on the vocabulary size and corpus.
We propose a new weighted least squares re-
gression model that addresses these problems.
Casting Eqn. (7) as a least squares problem and
introducing a weighting function f (Xi j) into the
cost function gives us the model
</bodyText>
<equation confidence="0.974716">
V
J = Z f (Xij) (wTi˜wj + bi +˜bj − log Xij)2
i, j=1
(8)
</equation>
<bodyText confidence="0.999874">
where V is the size of the vocabulary. The weight-
ing function should obey the following properties:
</bodyText>
<listItem confidence="0.99323375">
1. f (0) = 0. If f is viewed as a continuous
function, it should vanish as x → 0 fast
enough that the limx→0 f (x) log2 x is finite.
2. f (x) should be non-decreasing so that rare
co-occurrences are not overweighted.
3. f (x) should be relatively small for large val-
ues of x, so that frequent co-occurrences are
not overweighted.
</listItem>
<bodyText confidence="0.8736102">
Of course a large number of functions satisfy these
properties, but one class of functions that we found
to work well can be parameterized as,
PX) __ 1 (x/xmax)α if x &lt; xmax 9
f 1` 1 otherwise . ( )
</bodyText>
<figureCaption confidence="0.994925">
Figure 1: Weighting function f with α = 3/4.
</figureCaption>
<bodyText confidence="0.999911125">
The performance of the model depends weakly on
the cutoff, which we fix to xmax = 100 for all our
experiments. We found that α = 3/4 gives a mod-
est improvement over a linear version with α = 1.
Although we offer only empirical motivation for
choosing the value 3/4, it is interesting that a sim-
ilar fractional power scaling was found to give the
best performance in (Mikolov et al., 2013a).
</bodyText>
<subsectionHeader confidence="0.996369">
3.1 Relationship to Other Models
</subsectionHeader>
<bodyText confidence="0.999973461538461">
Because all unsupervised methods for learning
word vectors are ultimately based on the occur-
rence statistics of a corpus, there should be com-
monalities between the models. Nevertheless, cer-
tain models remain somewhat opaque in this re-
gard, particularly the recent window-based meth-
ods like skip-gram and ivLBL. Therefore, in this
subsection we show how these models are related
to our proposed model, as defined in Eqn. (8).
The starting point for the skip-gram or ivLBL
methods is a model Qij for the probability that
word j appears in the context of word i. For con-
creteness, let us assume that Qij is a softmax,
</bodyText>
<equation confidence="0.961133">
exp(wTi ˜wj)
EVk=1 exp(wTi˜wk)
</equation>
<bodyText confidence="0.999962">
Most of the details of these models are irrelevant
for our purposes, aside from the the fact that they
attempt to maximize the log probability as a con-
text window scans over the corpus. Training pro-
ceeds in an on-line, stochastic fashion, but the im-
plied global objective function can be written as,
</bodyText>
<equation confidence="0.925307666666667">
J = − � log Qij . (11)
i∈corpus
j ∈context(i)
</equation>
<bodyText confidence="0.999949">
Evaluating the normalization factor of the soft-
max for each term in this sum is costly. To al-
low for efficient training, the skip-gram and ivLBL
models introduce approximations to Qij. How-
ever, the sum in Eqn. (11) can be evaluated much
</bodyText>
<figure confidence="0.923276875">
1.0
0.8
0.6
0.4
0.2
0.0
Qij =
. (10)
</figure>
<page confidence="0.944139">
1535
</page>
<bodyText confidence="0.8855685">
more efficiently if we first group together those
terms that have the same values for i and j,
Xij log Qij , (12)
squared error of the logarithms of Pˆ and Qˆ instead,
</bodyText>
<equation confidence="0.988444142857143">
)2
Xi ( log ˆPij − log ˆQij
)2 . (15)
Xi (wT i ˜wj − log Xij
V
Z
j=1
J = −
ZV
i=1
ZJˆ =
i, j
Z=
i, j
</equation>
<bodyText confidence="0.976788382352941">
where we have used the fact that the number of
like terms is given by the co-occurrence matrix X.
Recalling our notation for Xi = Ik Xik and
Pij = Xij/Xi, we can rewrite J as,
XiH(Pi,Qi),
(13)
where H(Pi,Qi) is the cross entropy of the dis-
tributions Pi and Qi, which we define in analogy
to Xi. As a weighted sum of cross-entropy error,
this objective bears some formal resemblance to
the weighted least squares objective of Eqn. (8).
In fact, it is possible to optimize Eqn. (13) directly
as opposed to the on-line training methods used in
the skip-gram and ivLBL models. One could inter-
pret this objective as a “global skip-gram” model,
and it might be interesting to investigate further.
On the other hand, Eqn. (13) exhibits a number of
undesirable properties that ought to be addressed
before adopting it as a model for learning word
vectors.
To begin, cross entropy error is just one among
many possible distance measures between prob-
ability distributions, and it has the unfortunate
property that distributions with long tails are of-
ten modeled poorly with too much weight given
to the unlikely events. Furthermore, for the mea-
sure to be bounded it requires that the model dis-
tribution Q be properly normalized. This presents
a computational bottleneck owing to the sum over
the whole vocabulary in Eqn. (10), and it would be
desirable to consider a different distance measure
that did not require this property of Q. A natural
choice would be a least squares objective in which
normalization factors in Q and P are discarded,
</bodyText>
<equation confidence="0.9594265">
ZJˆ = )2
i, j Xi ( ˆPij − ˆQij (14)
</equation>
<bodyText confidence="0.999585">
where ˆPij = Xij and ˆQij = exp(wTi ˜wj) are the
unnormalized distributions. At this stage another
problem emerges, namely that Xij often takes very
large values, which can complicate the optimiza-
tion. An effective remedy is to minimize the
Finally, we observe that while the weighting factor
Xi is preordained by the on-line training method
inherent to the skip-gram and ivLBL models, it is
by no means guaranteed to be optimal. In fact,
Mikolov et al. (2013a) observe that performance
can be increased by filtering the data so as to re-
duce the effective value of the weighting factor for
frequent words. With this in mind, we introduce
a more general weighting function, which we are
free to take to depend on the context word as well.
The result is,
f (Xij)(wTi ˜wj − log Xij)2 , (16)
which is equivalent1 to the cost function of
Eqn. (8), which we derived previously.
</bodyText>
<subsectionHeader confidence="0.999685">
3.2 Complexity of the model
</subsectionHeader>
<bodyText confidence="0.997355041666667">
As can be seen from Eqn. (8) and the explicit form
of the weighting function f (X), the computational
complexity of the model depends on the number of
nonzero elements in the matrix X. As this num-
ber is always less than the total number of en-
tries of the matrix, the model scales no worse than
O(|V |2). At first glance this might seem like a sub-
stantial improvement over the shallow window-
based approaches, which scale with the corpus
size, |C|. However, typical vocabularies have hun-
dreds of thousands of words, so that |V |2 can be in
the hundreds of billions, which is actually much
larger than most corpora. For this reason it is im-
portant to determine whether a tighter bound can
be placed on the number of nonzero elements of
X.
In order to make any concrete statements about
the number of nonzero elements in X, it is neces-
sary to make some assumptions about the distribu-
tion of word co-occurrences. In particular, we will
assume that the number of co-occurrences of word
i with word j, Xij, can be modeled as a power-law
function of the frequency rank of that word pair,
rij:
</bodyText>
<equation confidence="0.885872444444444">
k. (17)
(rij)α
1We could also include bias terms in Eqn. (16).
J = − ZV Xi V Pij log Qij = ZV
i=1 Z i=1
j=1
ZJˆ =
i, j
Xij =
</equation>
<page confidence="0.831852">
1536
</page>
<bodyText confidence="0.986232916666667">
The total number of words in the corpus is pro-
portional to the sum over all elements of the co-
occurrence matrix X,
= kH|X|,α , (18)
where we have rewritten the last sum in terms of
the generalized harmonic number Hn,m. The up-
per limit of the sum, |X|, is the maximum fre-
quency rank, which coincides with the number of
nonzero elements in the matrix X. This number is
also equal to the maximum value of r in Eqn. (17)
such that Xij ≥ 1, i.e., |X  |= k1/α. Therefore we
can write Eqn. (18) as,
</bodyText>
<equation confidence="0.952877">
|C |∼ |X|α H|X|,α . (19)
</equation>
<bodyText confidence="0.999502">
We are interested in how |X  |is related to |C |when
both numbers are large; therefore we are free to
expand the right hand side of the equation for large
|X |. For this purpose we use the expansion of gen-
eralized harmonic numbers (Apostol, 1976),
</bodyText>
<equation confidence="0.999306">
+ ζ(s) + O(x−s) if s &gt; 0, s # 1,
(20)
giving,
|C |∼ 1|X|α + ζ(α) |X|α + O(1) , (21)
</equation>
<bodyText confidence="0.99996075">
where ζ(s) is the Riemann zeta function. In the
limit that X is large, only one of the two terms on
the right hand side of Eqn. (21) will be relevant,
and which term that is depends on whether α &gt; 1,
</bodyText>
<equation confidence="0.992471666666667">
� O(|C|) if α &lt; 1,
|X |= (22)
O(|C|1/α) if α &gt; 1.
</equation>
<bodyText confidence="0.999785142857143">
For the corpora studied in this article, we observe
that Xij is well-modeled by Eqn. (17) with α =
1.25. In this case we have that |X |= O(|C|0-8).
Therefore we conclude that the complexity of the
model is much better than the worst case O(V2),
and in fact it does somewhat better than the on-line
window-based methods which scale like O(|C|).
</bodyText>
<sectionHeader confidence="0.999854" genericHeader="method">
4 Experiments
</sectionHeader>
<subsectionHeader confidence="0.783603">
4.1 Evaluation methods
</subsectionHeader>
<bodyText confidence="0.991436692307692">
We conduct experiments on the word analogy
task of Mikolov et al. (2013a), a variety of word
similarity tasks, as described in (Luong et al.,
2013), and on the CoNLL-2003 shared benchmark
Table 2: Results on the word analogy task, given
as percent accuracy. Underlined scores are best
within groups of similarly-sized models; bold
scores are best overall. HPCA vectors are publicly
available2; (i)vLBL results are from (Mnih et al.,
2013); skip-gram (SG) and CBOW results are
from (Mikolov et al., 2013a,b); we trained SG†
and CBOW† using the word2vec tool3. See text
for details and a description of the SVD models.
</bodyText>
<table confidence="0.999681789473684">
Model Dim. Size Sem. Syn. Tot.
ivLBL 100 1.5B 55.9 50.1 53.2
HPCA 100 1.6B 4.2 16.4 10.8
GloVe 100 1.6B 67.5 54.3 60.3
SG 300 1B 61 61 61
CBOW 300 1.6B 16.1 52.6 36.1
vLBL 300 1.5B 54.2 64.8 60.0
ivLBL 300 1.5B 65.2 63.0 64.0
GloVe 300 1.6B 80.8 61.5 70.3
SVD 300 6B 6.3 8.1 7.3
SVD-S 300 6B 36.7 46.6 42.1
SVD-L 300 6B 56.6 63.0 60.1
CBOW† 300 6B 63.6 67.4 65.7
SG† 300 6B 73.0 66.0 69.1
GloVe 300 6B 77.4 67.0 71.7
CBOW 1000 6B 57.3 68.9 63.7
SG 1000 6B 66.1 65.1 65.6
SVD-L 300 42B 38.4 58.2 49.2
GloVe 300 42B 81.9 69.3 75.0
</table>
<bodyText confidence="0.974499388888889">
dataset for NER (Tjong Kim Sang and De Meul-
der, 2003).
Word analogies. The word analogy task con-
sists of questions like, “a is to b as c is to ?”
The dataset contains 19,544 such questions, di-
vided into a semantic subset and a syntactic sub-
set. The semantic questions are typically analogies
about people or places, like “Athens is to Greece
as Berlin is to ?”. The syntactic questions are
typically analogies about verb tenses or forms of
adjectives, for example “dance is to dancing as fly
is to ?”. To correctly answer the question, the
model should uniquely identify the missing term,
with only an exact correspondence counted as a
correct match. We answer the question “a is to b
as c is to ?” by finding the word d whose repre-
sentation wd is closest to wb − wa + wc according
to the cosine similarity.4
</bodyText>
<footnote confidence="0.9732085">
2http://lebret.ch/words/
3http://code.google.com/p/word2vec/
4Levy et al. (2014) introduce a multiplicative analogy
evaluation, 3COSMUL, and report an accuracy of 68.24% on
</footnote>
<equation confidence="0.9425477">
�
|C |∼
ij
k
Xij =
rα
|�X|
r=1
x1−s
Hx,s = 1 − s
</equation>
<page confidence="0.947449">
1537
</page>
<figure confidence="0.998904973684211">
Accuracy [%]
70
Accuracy [%]
70
45
45
Semantic
Syntactic
Overall
200 100 200 300 400 500 600
Vector Dimension
(a) Symmetric context
65
60
55
50
Semantic
Syntactic
Overall
402 4 6 8 10
Window Size
(b) Symmetric context
65
60
55
50
Semantic
Syntactic
Overall
402 4 6 8 10
Window Size
(c) Asymmetric context
Accuracy [%] 80
70
60
50
40
30
</figure>
<figureCaption confidence="0.978926">
Figure 2: Accuracy on the analogy task as function of vector size and window size/type. All models are
trained on the 6 billion token corpus. In (a), the window size is 10. In (b) and (c), the vector size is 100.
</figureCaption>
<bodyText confidence="0.998776551724138">
Word similarity. While the analogy task is our
primary focus since it tests for interesting vector
space substructures, we also evaluate our model on
a variety of word similarity tasks in Table 3. These
include WordSim-353 (Finkelstein et al., 2001),
MC (Miller and Charles, 1991), RG (Rubenstein
and Goodenough, 1965), SCWS (Huang et al.,
2012), and RW (Luong et al., 2013).
Named entity recognition. The CoNLL-2003
English benchmark dataset for NER is a collec-
tion of documents from Reuters newswire articles,
annotated with four entity types: person, location,
organization, and miscellaneous. We train mod-
els on CoNLL-03 training data on test on three
datasets: 1) ConLL-03 testing data, 2) ACE Phase
2 (2001-02) and ACE-2003 data, and 3) MUC7
Formal Run test set. We adopt the BIO2 annota-
tion standard, as well as all the preprocessing steps
described in (Wang and Manning, 2013). We use a
comprehensive set of discrete features that comes
with the standard distribution of the Stanford NER
model (Finkel et al., 2005). A total of 437,905
discrete features were generated for the CoNLL-
2003 training dataset. In addition, 50-dimensional
vectors for each word of a five-word context are
added and used as continuous features. With these
features as input, we trained a conditional random
field (CRF) with exactly the same setup as the
CRFjoin model of (Wang and Manning, 2013).
</bodyText>
<subsectionHeader confidence="0.980644">
4.2 Corpora and training details
</subsectionHeader>
<bodyText confidence="0.998571595238095">
We trained our model on five corpora of varying
sizes: a 2010 Wikipedia dump with 1 billion to-
kens; a 2014 Wikipedia dump with 1.6 billion to-
kens; Gigaword 5 which has 4.3 billion tokens; the
combination Gigaword5 + Wikipedia2014, which
the analogy task. This number is evaluated on a subset of the
dataset so it is not included in Table 2. 3COSMUL performed
worse than cosine similarity in almost all of our experiments.
has 6 billion tokens; and on 42 billion tokens of
web data, from Common Crawl5. We tokenize
and lowercase each corpus with the Stanford to-
kenizer, build a vocabulary of the 400,000 most
frequent words6, and then construct a matrix of co-
occurrence counts X. In constructing X, we must
choose how large the context window should be
and whether to distinguish left context from right
context. We explore the effect of these choices be-
low. In all cases we use a decreasing weighting
function, so that word pairs that are d words apart
contribute 1/d to the total count. This is one way
to account for the fact that very distant word pairs
are expected to contain less relevant information
about the words’ relationship to one another.
For all our experiments, we set xmax = 100,
α = 3/4, and train the model using AdaGrad
(Duchi et al., 2011), stochastically sampling non-
zero elements from X, with initial learning rate of
0.05. We run 50 iterations for vectors smaller than
300 dimensions, and 100 iterations otherwise (see
Section 4.6 for more details about the convergence
rate). Unless otherwise noted, we use a context of
ten words to the left and ten words to the right.
The model generates two sets of word vectors,
W and ˜W. When X is symmetric, W and W˜ are
equivalent and differ only as a result of their ran-
dom initializations; the two sets of vectors should
perform equivalently. On the other hand, there is
evidence that for certain types of neural networks,
training multiple instances of the network and then
combining the results can help reduce overfitting
and noise and generally improve results (Ciresan
et al., 2012). With this in mind, we choose to use
</bodyText>
<footnote confidence="0.998792166666667">
5To demonstrate the scalability of the model, we also
trained it on a much larger sixth corpus, containing 840 bil-
lion tokens of web data, but in this case we did not lowercase
the vocabulary, so the results are not directly comparable.
6For the model trained on Common Crawl data, we use a
larger vocabulary of about 2 million words.
</footnote>
<page confidence="0.996438">
1538
</page>
<bodyText confidence="0.999571571428571">
the sum W + W˜ as our word vectors. Doing so typ-
ically gives a small boost in performance, with the
biggest increase in the semantic analogy task.
We compare with the published results of a va-
riety of state-of-the-art models, as well as with
our own results produced using the word2vec
tool and with several baselines using SVDs. With
word2vec, we train the skip-gram (SG†) and
continuous bag-of-words (CBOW†) models on the
6 billion token corpus (Wikipedia 2014 + Giga-
word 5) with a vocabulary of the top 400,000 most
frequent words and a context window size of 10.
We used 10 negative samples, which we show in
Section 4.6 to be a good choice for this corpus.
For the SVD baselines, we generate a truncated
matrix Xtrunc which retains the information of how
frequently each word occurs with only the top
10,000 most frequent words. This step is typi-
cal of many matrix-factorization-based methods as
the extra columns can contribute a disproportion-
ate number of zero entries and the methods are
otherwise computationally expensive.
The singular vectors of this matrix constitute
the baseline “SVD”. We also evaluate two related
baselines: “SVD-S” in which we take the SVD of
√Xtrunc, and “SVD-L” in which we take the SVD
of log(1+Xtrunc). Both methods help compress the
otherwise large range of values in X.7
</bodyText>
<subsectionHeader confidence="0.926576">
4.3 Results
</subsectionHeader>
<bodyText confidence="0.9999246875">
We present results on the word analogy task in Ta-
ble 2. The GloVe model performs significantly
better than the other baselines, often with smaller
vector sizes and smaller corpora. Our results us-
ing the word2vec tool are somewhat better than
most of the previously published results. This is
due to a number of factors, including our choice to
use negative sampling (which typically works bet-
ter than the hierarchical softmax), the number of
negative samples, and the choice of the corpus.
We demonstrate that the model can easily be
trained on a large 42 billion token corpus, with a
substantial corresponding performance boost. We
note that increasing the corpus size does not guar-
antee improved results for other models, as can be
seen by the decreased performance of the SVD-
</bodyText>
<footnote confidence="0.984286">
7We also investigated several other weighting schemes for
transforming X; what we report here performed best. Many
weighting schemes like PPMI destroy the sparsity of X and
therefore cannot feasibly be used with large vocabularies.
With smaller vocabularies, these information-theoretic trans-
formations do indeed work well on word similarity measures,
but they perform very poorly on the word analogy task.
</footnote>
<tableCaption confidence="0.93809925">
Table 3: Spearman rank correlation on word simi-
larity tasks. All vectors are 300-dimensional. The
CBOW∗ vectors are from the word2vec website
and differ in that they contain phrase vectors.
</tableCaption>
<table confidence="0.9998137">
Model Size WS353 MC RG SCWS RW
SVD 6B 35.3 35.1 42.5 38.3 25.6
SVD-S 6B 56.5 71.5 71.0 53.6 34.7
SVD-L 6B 65.7 72.7 75.1 56.5 37.0
CBOW† 6B 57.2 65.6 68.2 57.0 32.5
SG† 6B 62.8 65.2 69.7 58.1 37.2
GloVe 6B 65.8 72.7 77.8 53.9 38.1
SVD-L 42B 74.0 76.4 74.1 58.3 39.9
GloVe 42B 75.9 83.6 82.9 59.6 47.8
CBOW∗ 100B 68.4 79.6 75.4 59.4 45.5
</table>
<bodyText confidence="0.989574852941177">
L model on this larger corpus. The fact that this
basic SVD model does not scale well to large cor-
pora lends further evidence to the necessity of the
type of weighting scheme proposed in our model.
Table 3 shows results on five different word
similarity datasets. A similarity score is obtained
from the word vectors by first normalizing each
feature across the vocabulary and then calculat-
ing the cosine similarity. We compute Spearman’s
rank correlation coefficient between this score and
the human judgments. CBOW∗ denotes the vec-
tors available on the word2vec website that are
trained with word and phrase vectors on 100B
words of news data. GloVe outperforms it while
using a corpus less than half the size.
Table 4 shows results on the NER task with the
CRF-based model. The L-BFGS training termi-
nates when no improvement has been achieved on
the dev set for 25 iterations. Otherwise all config-
urations are identical to those used by Wang and
Manning (2013). The model labeled Discrete is
the baseline using a comprehensive set of discrete
features that comes with the standard distribution
of the Stanford NER model, but with no word vec-
tor features. In addition to the HPCA and SVD
models discussed previously, we also compare to
the models of Huang et al. (2012) (HSMN) and
Collobert and Weston (2008) (CW). We trained
the CBOW model using the word2vec tool8.
The GloVe model outperforms all other methods
on all evaluation metrics, except for the CoNLL
test set, on which the HPCA method does slightly
better. We conclude that the GloVe vectors are
useful in downstream NLP tasks, as was first
</bodyText>
<footnote confidence="0.8904">
8We use the same parameters as above, except in this case
we found 5 negative samples to work slightly better than 10.
</footnote>
<page confidence="0.980697">
1539
</page>
<note confidence="0.416558">
Accuracy [%]
</note>
<tableCaption confidence="0.797049">
Table 4: F1 score on NER task with 50d vectors.
</tableCaption>
<bodyText confidence="0.763904666666667">
Discrete is the baseline without word vectors. We
use publicly-available vectors for HPCA, HSMN,
and CW. See text for details.
</bodyText>
<table confidence="0.9994514">
Model Dev Test ACE MUC7
Discrete 91.0 85.4 77.4 73.4
SVD 90.8 85.7 77.3 73.7
SVD-S 91.0 85.5 77.6 74.3
SVD-L 90.5 84.8 73.6 71.5
HPCA 92.6 88.7 81.7 80.7
HSMN 90.5 85.7 78.7 74.7
CW 92.2 87.4 81.7 80.2
CBOW 93.1 88.2 82.2 81.1
GloVe 93.2 88.3 82.9 82.2
</table>
<bodyText confidence="0.760809">
shown for neural vectors in (Turian et al., 2010).
</bodyText>
<subsectionHeader confidence="0.9966355">
4.4 Model Analysis: Vector Length and
Context Size
</subsectionHeader>
<bodyText confidence="0.999971823529412">
In Fig. 2, we show the results of experiments that
vary vector length and context window. A context
window that extends to the left and right of a tar-
get word will be called symmetric, and one which
extends only to the left will be called asymmet-
ric. In (a), we observe diminishing returns for vec-
tors larger than about 200 dimensions. In (b) and
(c), we examine the effect of varying the window
size for symmetric and asymmetric context win-
dows. Performance is better on the syntactic sub-
task for small and asymmetric context windows,
which aligns with the intuition that syntactic infor-
mation is mostly drawn from the immediate con-
text and can depend strongly on word order. Se-
mantic information, on the other hand, is more fre-
quently non-local, and more of it is captured with
larger window sizes.
</bodyText>
<subsectionHeader confidence="0.998998">
4.5 Model Analysis: Corpus Size
</subsectionHeader>
<bodyText confidence="0.999986142857143">
In Fig. 3, we show performance on the word anal-
ogy task for 300-dimensional vectors trained on
different corpora. On the syntactic subtask, there
is a monotonic increase in performance as the cor-
pus size increases. This is to be expected since
larger corpora typically produce better statistics.
Interestingly, the same trend is not true for the se-
mantic subtask, where the models trained on the
smaller Wikipedia corpora do better than those
trained on the larger Gigaword corpus. This is
likely due to the large number of city- and country-
based analogies in the analogy dataset and the fact
that Wikipedia has fairly comprehensive articles
for most such locations. Moreover, Wikipedia’s
</bodyText>
<figure confidence="0.955396375">
85
80
75
70
65
60
55
50
</figure>
<figureCaption confidence="0.9980655">
Figure 3: Accuracy on the analogy task for 300-
dimensional vectors trained on different corpora.
</figureCaption>
<bodyText confidence="0.993871333333333">
entries are updated to assimilate new knowledge,
whereas Gigaword is a fixed news repository with
outdated and possibly incorrect information.
</bodyText>
<subsectionHeader confidence="0.991706">
4.6 Model Analysis: Run-time
</subsectionHeader>
<bodyText confidence="0.999982176470588">
The total run-time is split between populating X
and training the model. The former depends on
many factors, including window size, vocabulary
size, and corpus size. Though we did not do so,
this step could easily be parallelized across mul-
tiple machines (see, e.g., Lebret and Collobert
(2014) for some benchmarks). Using a single
thread of a dual 2.1GHz Intel Xeon E5-2658 ma-
chine, populating X with a 10 word symmetric
context window, a 400,000 word vocabulary, and
a 6 billion token corpus takes about 85 minutes.
Given X, the time it takes to train the model de-
pends on the vector size and the number of itera-
tions. For 300-dimensional vectors with the above
settings (and using all 32 cores of the above ma-
chine), a single iteration takes 14 minutes. See
Fig. 4 for a plot of the learning curve.
</bodyText>
<subsectionHeader confidence="0.793898">
4.7 Model Analysis: Comparison with
word2vec
</subsectionHeader>
<bodyText confidence="0.999971">
A rigorous quantitative comparison of GloVe with
word2vec is complicated by the existence of
many parameters that have a strong effect on per-
formance. We control for the main sources of vari-
ation that we identified in Sections 4.4 and 4.5 by
setting the vector length, context window size, cor-
pus, and vocabulary size to the configuration men-
tioned in the previous subsection.
The most important remaining variable to con-
trol for is training time. For GloVe, the rele-
vant parameter is the number of training iterations.
For word2vec, the obvious choice would be the
number of training epochs. Unfortunately, the
code is currently designed for only a single epoch:
</bodyText>
<figure confidence="0.997318459459459">
Semantic
Syntactic Overall
Wiki2010 Wiki2014 Gigaword5 Gigaword5 + Common Crawl
1B tokens 1.6B tokens 4.3B tokens Wiki2014 42B tokens
6B tokens
1540
Accuracy [%]
Accuracy [%]
72
70
68
66
64
62
60
Training Time (hrs)
1 2 3 4 5 6
Iterations (GloVe)
5 10 15 20 25
GloVe
CBOW
72
70
68
66
64
62
60
Training Time (hrs)
3 6 9 12 15 18 21 24
GloVe
Skip-Gram
20 40 60 80 100
Iterations (GloVe)
1357 10 15 20 25 30 40 50 1 2 3 4 5 6 7 10 12 15 20
Negative Samples (CBOW) Negative Samples (Skip-Gram)
(a) GloVe vs CBOW (b) GloVe vs Skip-Gram
</figure>
<figureCaption confidence="0.996647">
Figure 4: Overall accuracy on the word analogy task as a function of training time, which is governed by
the number of iterations for GloVe and by the number of negative samples for CBOW (a) and skip-gram
(b). In all cases, we train 300-dimensional vectors on the same 6B token corpus (Wikipedia 2014 +
Gigaword 5) with the same 400,000 word vocabulary, and use a symmetric context window of size 10.
</figureCaption>
<bodyText confidence="0.99986304">
it specifies a learning schedule specific to a single
pass through the data, making a modification for
multiple passes a non-trivial task. Another choice
is to vary the number of negative samples. Adding
negative samples effectively increases the number
of training words seen by the model, so in some
ways it is analogous to extra epochs.
We set any unspecified parameters to their de-
fault values, assuming that they are close to opti-
mal, though we acknowledge that this simplifica-
tion should be relaxed in a more thorough analysis.
In Fig. 4, we plot the overall performance on
the analogy task as a function of training time.
The two x-axes at the bottom indicate the corre-
sponding number of training iterations for GloVe
and negative samples for word2vec. We note
that word2vec’s performance actually decreases
if the number of negative samples increases be-
yond about 10. Presumably this is because the
negative sampling method does not approximate
the target probability distribution well.9
For the same corpus, vocabulary, window size,
and training time, GloVe consistently outperforms
word2vec. It achieves better results faster, and
also obtains the best results irrespective of speed.
</bodyText>
<sectionHeader confidence="0.999183" genericHeader="conclusions">
5 Conclusion
</sectionHeader>
<bodyText confidence="0.991297961538462">
Recently, considerable attention has been focused
on the question of whether distributional word
representations are best learned from count-based
9In contrast, noise-contrastive estimation is an approxi-
mation which improves with more negative samples. In Ta-
ble 1 of (Mnih et al., 2013), accuracy on the analogy task is a
non-decreasing function of the number of negative samples.
methods or from prediction-based methods. Cur-
rently, prediction-based models garner substantial
support; for example, Baroni et al. (2014) argue
that these models perform better across a range of
tasks. In this work we argue that the two classes
of methods are not dramatically different at a fun-
damental level since they both probe the under-
lying co-occurrence statistics of the corpus, but
the efficiency with which the count-based meth-
ods capture global statistics can be advantageous.
We construct a model that utilizes this main ben-
efit of count data while simultaneously capturing
the meaningful linear substructures prevalent in
recent log-bilinear prediction-based methods like
word2vec. The result, GloVe, is a new global
log-bilinear regression model for the unsupervised
learning of word representations that outperforms
other models on word analogy, word similarity,
and named entity recognition tasks.
</bodyText>
<sectionHeader confidence="0.998237" genericHeader="acknowledgments">
Acknowledgments
</sectionHeader>
<bodyText confidence="0.999957076923077">
We thank the anonymous reviewers for their valu-
able comments. Stanford University gratefully
acknowledges the support of the Defense Threat
Reduction Agency (DTRA) under Air Force Re-
search Laboratory (AFRL) contract no. FA8650-
10-C-7020 and the Defense Advanced Research
Projects Agency (DARPA) Deep Exploration and
Filtering of Text (DEFT) Program under AFRL
contract no. FA8750-13-2-0040. Any opinions,
findings, and conclusion or recommendations ex-
pressed in this material are those of the authors and
do not necessarily reflect the view of the DTRA,
AFRL, DEFT, or the US government.
</bodyText>
<page confidence="0.990767">
1541
</page>
<sectionHeader confidence="0.993746" genericHeader="references">
References
</sectionHeader>
<reference confidence="0.99907047311828">
Tom M. Apostol. 1976. Introduction to Analytic
Number Theory. Introduction to Analytic Num-
ber Theory.
Marco Baroni, Georgiana Dinu, and Germ´an
Kruszewski. 2014. Don’t count, predict! A
systematic comparison of context-counting vs.
context-predicting semantic vectors. In ACL.
Yoshua Bengio. 2009. Learning deep architectures
for AI. Foundations and Trends in Machine
Learning.
Yoshua Bengio, R´ejean Ducharme, Pascal Vin-
cent, and Christian Janvin. 2003. A neural prob-
abilistic language model. JMLR, 3:1137–1155.
John A. Bullinaria and Joseph P. Levy. 2007. Ex-
tracting semantic representations from word co-
occurrence statistics: A computational study.
Behavior Research Methods, 39(3):510–526.
Dan C. Ciresan, Alessandro Giusti, Luca M. Gam-
bardella, and J¨urgen Schmidhuber. 2012. Deep
neural networks segment neuronal membranes
in electron microscopy images. In NIPS, pages
2852–2860.
Ronan Collobert and Jason Weston. 2008. A uni-
fied architecture for natural language process-
ing: deep neural networks with multitask learn-
ing. In Proceedings of ICML, pages 160–167.
Ronan Collobert, Jason Weston, L´eon Bottou,
Michael Karlen, Koray Kavukcuoglu, and Pavel
Kuksa. 2011. Natural Language Processing (Al-
most) from Scratch. JMLR, 12:2493–2537.
Scott Deerwester, Susan T. Dumais, George W.
Furnas, Thomas K. Landauer, and Richard
Harshman. 1990. Indexing by latent semantic
analysis. Journal of the American Society for
Information Science, 41.
John Duchi, Elad Hazan, and Yoram Singer. 2011.
Adaptive subgradient methods for online learn-
ing and stochastic optimization. JMLR, 12.
Lev Finkelstein, Evgenly Gabrilovich, Yossi Ma-
tias, Ehud Rivlin, Zach Solan, Gadi Wolfman,
and Eytan Ruppin. 2001. Placing search in con-
text: The concept revisited. In Proceedings
of the 10th international conference on World
Wide Web, pages 406–414. ACM.
Eric H. Huang, Richard Socher, Christopher D.
Manning, and Andrew Y. Ng. 2012. Improving
Word Representations via Global Context and
Multiple Word Prototypes. In ACL.
R´emi Lebret and Ronan Collobert. 2014. Word
embeddings through Hellinger PCA. In EACL.
Omer Levy, Yoav Goldberg, and Israel Ramat-
Gan. 2014. Linguistic regularities in sparse and
explicit word representations. CoNLL-2014.
Kevin Lund and Curt Burgess. 1996. Producing
high-dimensional semantic spaces from lexical
co-occurrence. Behavior Research Methods, In-
strumentation, and Computers, 28:203–208.
Minh-Thang Luong, Richard Socher, and Christo-
pher D Manning. 2013. Better word represen-
tations with recursive neural networks for mor-
phology. CoNLL-2013.
Tomas Mikolov, Kai Chen, Greg Corrado, and Jef-
frey Dean. 2013a. Efficient Estimation of Word
Representations in Vector Space. In ICLR Work-
shop Papers.
Tomas Mikolov, Ilya Sutskever, Kai Chen, Greg
Corrado, and Jeffrey Dean. 2013b. Distributed
representations of words and phrases and their
compositionality. In NIPS, pages 3111–3119.
Tomas Mikolov, Wen tau Yih, and Geoffrey
Zweig. 2013c. Linguistic regularities in con-
tinuous space word representations. In HLT-
NAACL.
George A. Miller and Walter G. Charles. 1991.
Contextual correlates of semantic similarity.
Language and cognitive processes, 6(1):1–28.
Andriy Mnih and Koray Kavukcuoglu. 2013.
Learning word embeddings efficiently with
noise-contrastive estimation. In NIPS.
Douglas L. T. Rohde, Laura M. Gonnerman,
and David C. Plaut. 2006. An improved
model of semantic similarity based on lexical
co-occurence. Communications of the ACM,
8:627–633.
Herbert Rubenstein and John B. Goodenough.
1965. Contextual correlates of synonymy. Com-
munications of the ACM, 8(10):627–633.
Fabrizio Sebastiani. 2002. Machine learning in au-
tomated text categorization. ACM Computing
Surveys, 34:1–47.
Richard Socher, John Bauer, Christopher D. Man-
ning, and Andrew Y. Ng. 2013. Parsing With
Compositional Vector Grammars. In ACL.
</reference>
<page confidence="0.881047">
1542
</page>
<reference confidence="0.997621105263158">
Stefanie Tellex, Boris Katz, Jimmy Lin, Aaron
Fernandes, and Gregory Marton. 2003. Quanti-
tative evaluation of passage retrieval algorithms
for question answering. In Proceedings of the
SIGIR Conference on Research and Develop-
ment in Informaion Retrieval.
Erik F. Tjong Kim Sang and Fien De Meul-
der. 2003. Introduction to the CoNLL-2003
shared task: Language-independent named en-
tity recognition. In CoNLL-2003.
Joseph Turian, Lev Ratinov, and Yoshua Bengio.
2010. Word representations: a simple and gen-
eral method for semi-supervised learning. In
Proceedings ofACL, pages 384–394.
Mengqiu Wang and Christopher D. Manning.
2013. Effect of non-linear deep architecture in
sequence labeling. In Proceedings of the 6th
International Joint Conference on Natural Lan-
guage Processing (IJCNLP).
</reference>
<page confidence="0.982512">
1543
</page>
</variant>
</algorithm>
<algorithm name="ParsHed" version="110505">
<variant no="0" confidence="0.553399">
<title confidence="0.999532">GloVe: Global Vectors for Word Representation</title>
<author confidence="0.99706">Jeffrey Pennington</author>
<author confidence="0.99706">Richard Socher</author>
<author confidence="0.99706">D Christopher</author>
<affiliation confidence="0.568618">Computer Science Department, Stanford University, Stanford, CA 94305</affiliation>
<email confidence="0.999492">jpennin@stanford.edu,richard@socher.org,manning@stanford.edu</email>
<abstract confidence="0.99898416">Recent methods for learning vector space representations of words have succeeded in capturing fine-grained semantic and syntactic regularities using vector arithmetic, but the origin of these regularities has remained opaque. We analyze and make explicit the model properties needed for such regularities to emerge in word vectors. The result is a new global logbilinear regression model that combines the advantages of the two major model families in the literature: global matrix factorization and local context window methods. Our model efficiently leverages statistical information by training only on the nonzero elements in a word-word cooccurrence matrix, rather than on the entire sparse matrix or on individual context windows in a large corpus. The model produces a vector space with meaningful substructure, as evidenced by its performance of 75% on a recent word analogy task. It also outperforms related models on similarity tasks and named entity recognition.</abstract>
</variant>
</algorithm>
<algorithm name="ParsCit" version="110505">
<citationList>
<citation valid="true">
<authors>
<author>Tom M Apostol</author>
</authors>
<title>Introduction to Analytic Number Theory. Introduction to Analytic Number Theory.</title>
<date>1976</date>
<contexts>
<context position="21168" citStr="Apostol, 1976" startWordPosition="3697" endWordPosition="3698">ewritten the last sum in terms of the generalized harmonic number Hn,m. The upper limit of the sum, |X|, is the maximum frequency rank, which coincides with the number of nonzero elements in the matrix X. This number is also equal to the maximum value of r in Eqn. (17) such that Xij ≥ 1, i.e., |X |= k1/α. Therefore we can write Eqn. (18) as, |C |∼ |X|α H|X|,α . (19) We are interested in how |X |is related to |C |when both numbers are large; therefore we are free to expand the right hand side of the equation for large |X |. For this purpose we use the expansion of generalized harmonic numbers (Apostol, 1976), + ζ(s) + O(x−s) if s &gt; 0, s # 1, (20) giving, |C |∼ 1|X|α + ζ(α) |X|α + O(1) , (21) where ζ(s) is the Riemann zeta function. In the limit that X is large, only one of the two terms on the right hand side of Eqn. (21) will be relevant, and which term that is depends on whether α &gt; 1, � O(|C|) if α &lt; 1, |X |= (22) O(|C|1/α) if α &gt; 1. For the corpora studied in this article, we observe that Xij is well-modeled by Eqn. (17) with α = 1.25. In this case we have that |X |= O(|C|0-8). Therefore we conclude that the complexity of the model is much better than the worst case O(V2), and in fact it does</context>
</contexts>
<marker>Apostol, 1976</marker>
<rawString>Tom M. Apostol. 1976. Introduction to Analytic Number Theory. Introduction to Analytic Number Theory.</rawString>
</citation>
<citation valid="true">
<authors>
<author>Marco Baroni</author>
<author>Georgiana Dinu</author>
<author>Germ´an Kruszewski</author>
</authors>
<title>Don’t count, predict! A systematic comparison of context-counting vs. context-predicting semantic vectors.</title>
<date>2014</date>
<booktitle>In ACL.</booktitle>
<contexts>
<context position="39752" citStr="Baroni et al. (2014)" startWordPosition="6929" endWordPosition="6932">chieves better results faster, and also obtains the best results irrespective of speed. 5 Conclusion Recently, considerable attention has been focused on the question of whether distributional word representations are best learned from count-based 9In contrast, noise-contrastive estimation is an approximation which improves with more negative samples. In Table 1 of (Mnih et al., 2013), accuracy on the analogy task is a non-decreasing function of the number of negative samples. methods or from prediction-based methods. Currently, prediction-based models garner substantial support; for example, Baroni et al. (2014) argue that these models perform better across a range of tasks. In this work we argue that the two classes of methods are not dramatically different at a fundamental level since they both probe the underlying co-occurrence statistics of the corpus, but the efficiency with which the count-based methods capture global statistics can be advantageous. We construct a model that utilizes this main benefit of count data while simultaneously capturing the meaningful linear substructures prevalent in recent log-bilinear prediction-based methods like word2vec. The result, GloVe, is a new global log-bil</context>
</contexts>
<marker>Baroni, Dinu, Kruszewski, 2014</marker>
<rawString>Marco Baroni, Georgiana Dinu, and Germ´an Kruszewski. 2014. Don’t count, predict! A systematic comparison of context-counting vs. context-predicting semantic vectors. In ACL.</rawString>
</citation>
<citation valid="true">
<authors>
<author>Yoshua Bengio</author>
</authors>
<title>Learning deep architectures for AI. Foundations and Trends</title>
<date>2009</date>
<booktitle>in Machine Learning.</booktitle>
<contexts>
<context position="2370" citStr="Bengio, 2009" startWordPosition="363" endWordPosition="364"> of such a set of word representations. Recently, Mikolov et al. (2013c) introduced a new evaluation scheme based on word analogies that probes the finer structure of the word vector space by examining not the scalar distance between word vectors, but rather their various dimensions of difference. For example, the analogy “king is to queen as man is to woman” should be encoded in the vector space by the vector equation king − queen = man − woman. This evaluation scheme favors models that produce dimensions of meaning, thereby capturing the multi-clustering idea of distributed representations (Bengio, 2009). The two main model families for learning word vectors are: 1) global matrix factorization methods, such as latent semantic analysis (LSA) (Deerwester et al., 1990) and 2) local context window methods, such as the skip-gram model of Mikolov et al. (2013c). Currently, both families suffer significant drawbacks. While methods like LSA efficiently leverage statistical information, they do relatively poorly on the word analogy task, indicating a sub-optimal vector space structure. Methods like skip-gram may do better on the analogy task, but they poorly utilize the statistics of the corpus since </context>
</contexts>
<marker>Bengio, 2009</marker>
<rawString>Yoshua Bengio. 2009. Learning deep architectures for AI. Foundations and Trends in Machine Learning.</rawString>
</citation>
<citation valid="true">
<authors>
<author>Yoshua Bengio</author>
<author>R´ejean Ducharme</author>
<author>Pascal Vincent</author>
<author>Christian Janvin</author>
</authors>
<title>A neural probabilistic language model.</title>
<date>2003</date>
<journal>JMLR,</journal>
<pages>3--1137</pages>
<contexts>
<context position="6189" citStr="Bengio et al. (2003)" startWordPosition="958" endWordPosition="961">ompressed so as to be distributed more evenly in a smaller interval. A variety of newer models also pursue this approach, including a study (Bullinaria and Levy, 2007) that indicates that positive pointwise mutual information (PPMI) is a good transformation. More recently, a square root type transformation in the form of Hellinger PCA (HPCA) (Lebret and Collobert, 2014) has been suggested as an effective way of learning word representations. Shallow Window-Based Methods. Another approach is to learn word representations that aid in making predictions within local context windows. For example, Bengio et al. (2003) introduced a model that learns word vector representations as part of a simple neural network architecture for language modeling. Collobert and Weston (2008) decoupled the word vector training from the downstream training objectives, which paved the way for Collobert et al. (2011) to use the full context of a word for learning the word representations, rather than just the preceding context as is the case with language models. Recently, the importance of the full neural network structure for learning useful word representations has been called into question. The skip-gram and continuous bag-o</context>
</contexts>
<marker>Bengio, Ducharme, Vincent, Janvin, 2003</marker>
<rawString>Yoshua Bengio, R´ejean Ducharme, Pascal Vincent, and Christian Janvin. 2003. A neural probabilistic language model. JMLR, 3:1137–1155.</rawString>
</citation>
<citation valid="true">
<authors>
<author>John A Bullinaria</author>
<author>Joseph P Levy</author>
</authors>
<title>Extracting semantic representations from word cooccurrence statistics: A computational study.</title>
<date>2007</date>
<journal>Behavior Research Methods,</journal>
<volume>39</volume>
<issue>3</issue>
<contexts>
<context position="5736" citStr="Bullinaria and Levy, 2007" startWordPosition="889" endWordPosition="892">rity despite conveying relatively little about their semantic relatedness. A number of techniques exist that addresses this shortcoming of HAL, such as the COALS method (Rohde et al., 2006), in which the co-occurrence matrix is first transformed by an entropy- or correlation-based normalization. An advantage of this type of transformation is that the raw co-occurrence counts, which for a reasonably sized corpus might span 8 or 9 orders of magnitude, are compressed so as to be distributed more evenly in a smaller interval. A variety of newer models also pursue this approach, including a study (Bullinaria and Levy, 2007) that indicates that positive pointwise mutual information (PPMI) is a good transformation. More recently, a square root type transformation in the form of Hellinger PCA (HPCA) (Lebret and Collobert, 2014) has been suggested as an effective way of learning word representations. Shallow Window-Based Methods. Another approach is to learn word representations that aid in making predictions within local context windows. For example, Bengio et al. (2003) introduced a model that learns word vector representations as part of a simple neural network architecture for language modeling. Collobert and We</context>
</contexts>
<marker>Bullinaria, Levy, 2007</marker>
<rawString>John A. Bullinaria and Joseph P. Levy. 2007. Extracting semantic representations from word cooccurrence statistics: A computational study. Behavior Research Methods, 39(3):510–526.</rawString>
</citation>
<citation valid="true">
<authors>
<author>Dan C Ciresan</author>
<author>Alessandro Giusti</author>
<author>Luca M Gambardella</author>
<author>J¨urgen Schmidhuber</author>
</authors>
<title>Deep neural networks segment neuronal membranes in electron microscopy images.</title>
<date>2012</date>
<booktitle>In NIPS,</booktitle>
<pages>2852--2860</pages>
<contexts>
<context position="28084" citStr="Ciresan et al., 2012" startWordPosition="4936" endWordPosition="4939">wise (see Section 4.6 for more details about the convergence rate). Unless otherwise noted, we use a context of ten words to the left and ten words to the right. The model generates two sets of word vectors, W and ˜W. When X is symmetric, W and W˜ are equivalent and differ only as a result of their random initializations; the two sets of vectors should perform equivalently. On the other hand, there is evidence that for certain types of neural networks, training multiple instances of the network and then combining the results can help reduce overfitting and noise and generally improve results (Ciresan et al., 2012). With this in mind, we choose to use 5To demonstrate the scalability of the model, we also trained it on a much larger sixth corpus, containing 840 billion tokens of web data, but in this case we did not lowercase the vocabulary, so the results are not directly comparable. 6For the model trained on Common Crawl data, we use a larger vocabulary of about 2 million words. 1538 the sum W + W˜ as our word vectors. Doing so typically gives a small boost in performance, with the biggest increase in the semantic analogy task. We compare with the published results of a variety of state-of-the-art mode</context>
</contexts>
<marker>Ciresan, Giusti, Gambardella, Schmidhuber, 2012</marker>
<rawString>Dan C. Ciresan, Alessandro Giusti, Luca M. Gambardella, and J¨urgen Schmidhuber. 2012. Deep neural networks segment neuronal membranes in electron microscopy images. In NIPS, pages 2852–2860.</rawString>
</citation>
<citation valid="true">
<authors>
<author>Ronan Collobert</author>
<author>Jason Weston</author>
</authors>
<title>A unified architecture for natural language processing: deep neural networks with multitask learning.</title>
<date>2008</date>
<booktitle>In Proceedings of ICML,</booktitle>
<pages>160--167</pages>
<contexts>
<context position="6347" citStr="Collobert and Weston (2008)" startWordPosition="984" endWordPosition="987"> and Levy, 2007) that indicates that positive pointwise mutual information (PPMI) is a good transformation. More recently, a square root type transformation in the form of Hellinger PCA (HPCA) (Lebret and Collobert, 2014) has been suggested as an effective way of learning word representations. Shallow Window-Based Methods. Another approach is to learn word representations that aid in making predictions within local context windows. For example, Bengio et al. (2003) introduced a model that learns word vector representations as part of a simple neural network architecture for language modeling. Collobert and Weston (2008) decoupled the word vector training from the downstream training objectives, which paved the way for Collobert et al. (2011) to use the full context of a word for learning the word representations, rather than just the preceding context as is the case with language models. Recently, the importance of the full neural network structure for learning useful word representations has been called into question. The skip-gram and continuous bag-of-words (CBOW) models of Mikolov et al. (2013a) propose a simple single-layer architecture based on the inner product between two word vectors. Mnih and Kavuk</context>
<context position="32806" citStr="Collobert and Weston (2008)" startWordPosition="5744" endWordPosition="5747">rpus less than half the size. Table 4 shows results on the NER task with the CRF-based model. The L-BFGS training terminates when no improvement has been achieved on the dev set for 25 iterations. Otherwise all configurations are identical to those used by Wang and Manning (2013). The model labeled Discrete is the baseline using a comprehensive set of discrete features that comes with the standard distribution of the Stanford NER model, but with no word vector features. In addition to the HPCA and SVD models discussed previously, we also compare to the models of Huang et al. (2012) (HSMN) and Collobert and Weston (2008) (CW). We trained the CBOW model using the word2vec tool8. The GloVe model outperforms all other methods on all evaluation metrics, except for the CoNLL test set, on which the HPCA method does slightly better. We conclude that the GloVe vectors are useful in downstream NLP tasks, as was first 8We use the same parameters as above, except in this case we found 5 negative samples to work slightly better than 10. 1539 Accuracy [%] Table 4: F1 score on NER task with 50d vectors. Discrete is the baseline without word vectors. We use publicly-available vectors for HPCA, HSMN, and CW. See text for det</context>
</contexts>
<marker>Collobert, Weston, 2008</marker>
<rawString>Ronan Collobert and Jason Weston. 2008. A unified architecture for natural language processing: deep neural networks with multitask learning. In Proceedings of ICML, pages 160–167.</rawString>
</citation>
<citation valid="true">
<authors>
<author>Ronan Collobert</author>
<author>Jason Weston</author>
<author>L´eon Bottou</author>
<author>Michael Karlen</author>
<author>Koray Kavukcuoglu</author>
<author>Pavel Kuksa</author>
</authors>
<date>2011</date>
<booktitle>Natural Language Processing (Almost) from Scratch. JMLR,</booktitle>
<pages>12--2493</pages>
<contexts>
<context position="6471" citStr="Collobert et al. (2011)" startWordPosition="1003" endWordPosition="1006">are root type transformation in the form of Hellinger PCA (HPCA) (Lebret and Collobert, 2014) has been suggested as an effective way of learning word representations. Shallow Window-Based Methods. Another approach is to learn word representations that aid in making predictions within local context windows. For example, Bengio et al. (2003) introduced a model that learns word vector representations as part of a simple neural network architecture for language modeling. Collobert and Weston (2008) decoupled the word vector training from the downstream training objectives, which paved the way for Collobert et al. (2011) to use the full context of a word for learning the word representations, rather than just the preceding context as is the case with language models. Recently, the importance of the full neural network structure for learning useful word representations has been called into question. The skip-gram and continuous bag-of-words (CBOW) models of Mikolov et al. (2013a) propose a simple single-layer architecture based on the inner product between two word vectors. Mnih and Kavukcuoglu (2013) also proposed closely-related vector log-bilinear models, vLBL and ivLBL, and Levy et al. (2014) proposed expl</context>
</contexts>
<marker>Collobert, Weston, Bottou, Karlen, Kavukcuoglu, Kuksa, 2011</marker>
<rawString>Ronan Collobert, Jason Weston, L´eon Bottou, Michael Karlen, Koray Kavukcuoglu, and Pavel Kuksa. 2011. Natural Language Processing (Almost) from Scratch. JMLR, 12:2493–2537.</rawString>
</citation>
<citation valid="true">
<authors>
<author>Scott Deerwester</author>
<author>Susan T Dumais</author>
<author>George W Furnas</author>
<author>Thomas K Landauer</author>
<author>Richard Harshman</author>
</authors>
<title>Indexing by latent semantic analysis.</title>
<date>1990</date>
<journal>Journal of the American Society for Information Science,</journal>
<volume>41</volume>
<contexts>
<context position="2535" citStr="Deerwester et al., 1990" startWordPosition="387" endWordPosition="391">structure of the word vector space by examining not the scalar distance between word vectors, but rather their various dimensions of difference. For example, the analogy “king is to queen as man is to woman” should be encoded in the vector space by the vector equation king − queen = man − woman. This evaluation scheme favors models that produce dimensions of meaning, thereby capturing the multi-clustering idea of distributed representations (Bengio, 2009). The two main model families for learning word vectors are: 1) global matrix factorization methods, such as latent semantic analysis (LSA) (Deerwester et al., 1990) and 2) local context window methods, such as the skip-gram model of Mikolov et al. (2013c). Currently, both families suffer significant drawbacks. While methods like LSA efficiently leverage statistical information, they do relatively poorly on the word analogy task, indicating a sub-optimal vector space structure. Methods like skip-gram may do better on the analogy task, but they poorly utilize the statistics of the corpus since they train on separate local context windows instead of on global co-occurrence counts. In this work, we analyze the model properties necessary to produce linear dir</context>
</contexts>
<marker>Deerwester, Dumais, Furnas, Landauer, Harshman, 1990</marker>
<rawString>Scott Deerwester, Susan T. Dumais, George W. Furnas, Thomas K. Landauer, and Richard Harshman. 1990. Indexing by latent semantic analysis. Journal of the American Society for Information Science, 41.</rawString>
</citation>
<citation valid="true">
<authors>
<author>John Duchi</author>
<author>Elad Hazan</author>
<author>Yoram Singer</author>
</authors>
<title>Adaptive subgradient methods for online learning and stochastic optimization.</title>
<date>2011</date>
<journal>JMLR,</journal>
<volume>12</volume>
<contexts>
<context position="27290" citStr="Duchi et al., 2011" startWordPosition="4802" endWordPosition="4805">ct a matrix of cooccurrence counts X. In constructing X, we must choose how large the context window should be and whether to distinguish left context from right context. We explore the effect of these choices below. In all cases we use a decreasing weighting function, so that word pairs that are d words apart contribute 1/d to the total count. This is one way to account for the fact that very distant word pairs are expected to contain less relevant information about the words’ relationship to one another. For all our experiments, we set xmax = 100, α = 3/4, and train the model using AdaGrad (Duchi et al., 2011), stochastically sampling nonzero elements from X, with initial learning rate of 0.05. We run 50 iterations for vectors smaller than 300 dimensions, and 100 iterations otherwise (see Section 4.6 for more details about the convergence rate). Unless otherwise noted, we use a context of ten words to the left and ten words to the right. The model generates two sets of word vectors, W and ˜W. When X is symmetric, W and W˜ are equivalent and differ only as a result of their random initializations; the two sets of vectors should perform equivalently. On the other hand, there is evidence that for cert</context>
</contexts>
<marker>Duchi, Hazan, Singer, 2011</marker>
<rawString>John Duchi, Elad Hazan, and Yoram Singer. 2011. Adaptive subgradient methods for online learning and stochastic optimization. JMLR, 12.</rawString>
</citation>
<citation valid="true">
<authors>
<author>Lev Finkelstein</author>
<author>Evgenly Gabrilovich</author>
<author>Yossi Matias</author>
<author>Ehud Rivlin</author>
<author>Zach Solan</author>
<author>Gadi Wolfman</author>
<author>Eytan Ruppin</author>
</authors>
<title>Placing search in context: The concept revisited.</title>
<date>2001</date>
<booktitle>In Proceedings of the 10th international conference on World Wide Web,</booktitle>
<pages>406--414</pages>
<publisher>ACM.</publisher>
<contexts>
<context position="24865" citStr="Finkelstein et al., 2001" startWordPosition="4389" endWordPosition="4392">2 4 6 8 10 Window Size (b) Symmetric context 65 60 55 50 Semantic Syntactic Overall 402 4 6 8 10 Window Size (c) Asymmetric context Accuracy [%] 80 70 60 50 40 30 Figure 2: Accuracy on the analogy task as function of vector size and window size/type. All models are trained on the 6 billion token corpus. In (a), the window size is 10. In (b) and (c), the vector size is 100. Word similarity. While the analogy task is our primary focus since it tests for interesting vector space substructures, we also evaluate our model on a variety of word similarity tasks in Table 3. These include WordSim-353 (Finkelstein et al., 2001), MC (Miller and Charles, 1991), RG (Rubenstein and Goodenough, 1965), SCWS (Huang et al., 2012), and RW (Luong et al., 2013). Named entity recognition. The CoNLL-2003 English benchmark dataset for NER is a collection of documents from Reuters newswire articles, annotated with four entity types: person, location, organization, and miscellaneous. We train models on CoNLL-03 training data on test on three datasets: 1) ConLL-03 testing data, 2) ACE Phase 2 (2001-02) and ACE-2003 data, and 3) MUC7 Formal Run test set. We adopt the BIO2 annotation standard, as well as all the preprocessing steps de</context>
</contexts>
<marker>Finkelstein, Gabrilovich, Matias, Rivlin, Solan, Wolfman, Ruppin, 2001</marker>
<rawString>Lev Finkelstein, Evgenly Gabrilovich, Yossi Matias, Ehud Rivlin, Zach Solan, Gadi Wolfman, and Eytan Ruppin. 2001. Placing search in context: The concept revisited. In Proceedings of the 10th international conference on World Wide Web, pages 406–414. ACM.</rawString>
</citation>
<citation valid="true">
<authors>
<author>Eric H Huang</author>
<author>Richard Socher</author>
<author>Christopher D Manning</author>
<author>Andrew Y Ng</author>
</authors>
<title>Improving Word Representations via Global Context and Multiple Word Prototypes.</title>
<date>2012</date>
<booktitle>In ACL.</booktitle>
<contexts>
<context position="24961" citStr="Huang et al., 2012" startWordPosition="4404" endWordPosition="4407">w Size (c) Asymmetric context Accuracy [%] 80 70 60 50 40 30 Figure 2: Accuracy on the analogy task as function of vector size and window size/type. All models are trained on the 6 billion token corpus. In (a), the window size is 10. In (b) and (c), the vector size is 100. Word similarity. While the analogy task is our primary focus since it tests for interesting vector space substructures, we also evaluate our model on a variety of word similarity tasks in Table 3. These include WordSim-353 (Finkelstein et al., 2001), MC (Miller and Charles, 1991), RG (Rubenstein and Goodenough, 1965), SCWS (Huang et al., 2012), and RW (Luong et al., 2013). Named entity recognition. The CoNLL-2003 English benchmark dataset for NER is a collection of documents from Reuters newswire articles, annotated with four entity types: person, location, organization, and miscellaneous. We train models on CoNLL-03 training data on test on three datasets: 1) ConLL-03 testing data, 2) ACE Phase 2 (2001-02) and ACE-2003 data, and 3) MUC7 Formal Run test set. We adopt the BIO2 annotation standard, as well as all the preprocessing steps described in (Wang and Manning, 2013). We use a comprehensive set of discrete features that comes </context>
<context position="32767" citStr="Huang et al. (2012)" startWordPosition="5738" endWordPosition="5741">outperforms it while using a corpus less than half the size. Table 4 shows results on the NER task with the CRF-based model. The L-BFGS training terminates when no improvement has been achieved on the dev set for 25 iterations. Otherwise all configurations are identical to those used by Wang and Manning (2013). The model labeled Discrete is the baseline using a comprehensive set of discrete features that comes with the standard distribution of the Stanford NER model, but with no word vector features. In addition to the HPCA and SVD models discussed previously, we also compare to the models of Huang et al. (2012) (HSMN) and Collobert and Weston (2008) (CW). We trained the CBOW model using the word2vec tool8. The GloVe model outperforms all other methods on all evaluation metrics, except for the CoNLL test set, on which the HPCA method does slightly better. We conclude that the GloVe vectors are useful in downstream NLP tasks, as was first 8We use the same parameters as above, except in this case we found 5 negative samples to work slightly better than 10. 1539 Accuracy [%] Table 4: F1 score on NER task with 50d vectors. Discrete is the baseline without word vectors. We use publicly-available vectors f</context>
</contexts>
<marker>Huang, Socher, Manning, Ng, 2012</marker>
<rawString>Eric H. Huang, Richard Socher, Christopher D. Manning, and Andrew Y. Ng. 2012. Improving Word Representations via Global Context and Multiple Word Prototypes. In ACL.</rawString>
</citation>
<citation valid="true">
<authors>
<author>R´emi Lebret</author>
<author>Ronan Collobert</author>
</authors>
<title>Word embeddings through Hellinger PCA.</title>
<date>2014</date>
<booktitle>In EACL.</booktitle>
<contexts>
<context position="5941" citStr="Lebret and Collobert, 2014" startWordPosition="920" endWordPosition="923">co-occurrence matrix is first transformed by an entropy- or correlation-based normalization. An advantage of this type of transformation is that the raw co-occurrence counts, which for a reasonably sized corpus might span 8 or 9 orders of magnitude, are compressed so as to be distributed more evenly in a smaller interval. A variety of newer models also pursue this approach, including a study (Bullinaria and Levy, 2007) that indicates that positive pointwise mutual information (PPMI) is a good transformation. More recently, a square root type transformation in the form of Hellinger PCA (HPCA) (Lebret and Collobert, 2014) has been suggested as an effective way of learning word representations. Shallow Window-Based Methods. Another approach is to learn word representations that aid in making predictions within local context windows. For example, Bengio et al. (2003) introduced a model that learns word vector representations as part of a simple neural network architecture for language modeling. Collobert and Weston (2008) decoupled the word vector training from the downstream training objectives, which paved the way for Collobert et al. (2011) to use the full context of a word for learning the word representatio</context>
<context position="35877" citStr="Lebret and Collobert (2014)" startWordPosition="6270" endWordPosition="6273"> such locations. Moreover, Wikipedia’s 85 80 75 70 65 60 55 50 Figure 3: Accuracy on the analogy task for 300- dimensional vectors trained on different corpora. entries are updated to assimilate new knowledge, whereas Gigaword is a fixed news repository with outdated and possibly incorrect information. 4.6 Model Analysis: Run-time The total run-time is split between populating X and training the model. The former depends on many factors, including window size, vocabulary size, and corpus size. Though we did not do so, this step could easily be parallelized across multiple machines (see, e.g., Lebret and Collobert (2014) for some benchmarks). Using a single thread of a dual 2.1GHz Intel Xeon E5-2658 machine, populating X with a 10 word symmetric context window, a 400,000 word vocabulary, and a 6 billion token corpus takes about 85 minutes. Given X, the time it takes to train the model depends on the vector size and the number of iterations. For 300-dimensional vectors with the above settings (and using all 32 cores of the above machine), a single iteration takes 14 minutes. See Fig. 4 for a plot of the learning curve. 4.7 Model Analysis: Comparison with word2vec A rigorous quantitative comparison of GloVe wit</context>
</contexts>
<marker>Lebret, Collobert, 2014</marker>
<rawString>R´emi Lebret and Ronan Collobert. 2014. Word embeddings through Hellinger PCA. In EACL.</rawString>
</citation>
<citation valid="true">
<authors>
<author>Omer Levy</author>
<author>Yoav Goldberg</author>
<author>Israel RamatGan</author>
</authors>
<title>Linguistic regularities in sparse and explicit word representations.</title>
<date>2014</date>
<journal>CoNLL-2014.</journal>
<contexts>
<context position="7057" citStr="Levy et al. (2014)" startWordPosition="1096" endWordPosition="1099"> way for Collobert et al. (2011) to use the full context of a word for learning the word representations, rather than just the preceding context as is the case with language models. Recently, the importance of the full neural network structure for learning useful word representations has been called into question. The skip-gram and continuous bag-of-words (CBOW) models of Mikolov et al. (2013a) propose a simple single-layer architecture based on the inner product between two word vectors. Mnih and Kavukcuoglu (2013) also proposed closely-related vector log-bilinear models, vLBL and ivLBL, and Levy et al. (2014) proposed explicit word embeddings based on a PPMI metric. In the skip-gram and ivLBL models, the objective is to predict a word’s context given the word itself, whereas the objective in the CBOW and vLBL models is to predict a word given its context. Through evaluation on a word analogy task, these models demonstrated the capacity to learn linguistic patterns as linear relationships between the word vectors. Unlike the matrix factorization methods, the shallow window-based methods suffer from the disadvantage that they do not operate directly on the co-occurrence statistics of the corpus. Ins</context>
<context position="23920" citStr="Levy et al. (2014)" startWordPosition="4213" endWordPosition="4216"> typically analogies about people or places, like “Athens is to Greece as Berlin is to ?”. The syntactic questions are typically analogies about verb tenses or forms of adjectives, for example “dance is to dancing as fly is to ?”. To correctly answer the question, the model should uniquely identify the missing term, with only an exact correspondence counted as a correct match. We answer the question “a is to b as c is to ?” by finding the word d whose representation wd is closest to wb − wa + wc according to the cosine similarity.4 2http://lebret.ch/words/ 3http://code.google.com/p/word2vec/ 4Levy et al. (2014) introduce a multiplicative analogy evaluation, 3COSMUL, and report an accuracy of 68.24% on � |C |∼ ij k Xij = rα |�X| r=1 x1−s Hx,s = 1 − s 1537 Accuracy [%] 70 Accuracy [%] 70 45 45 Semantic Syntactic Overall 200 100 200 300 400 500 600 Vector Dimension (a) Symmetric context 65 60 55 50 Semantic Syntactic Overall 402 4 6 8 10 Window Size (b) Symmetric context 65 60 55 50 Semantic Syntactic Overall 402 4 6 8 10 Window Size (c) Asymmetric context Accuracy [%] 80 70 60 50 40 30 Figure 2: Accuracy on the analogy task as function of vector size and window size/type. All models are trained on the</context>
</contexts>
<marker>Levy, Goldberg, RamatGan, 2014</marker>
<rawString>Omer Levy, Yoav Goldberg, and Israel RamatGan. 2014. Linguistic regularities in sparse and explicit word representations. CoNLL-2014.</rawString>
</citation>
<citation valid="true">
<authors>
<author>Kevin Lund</author>
<author>Curt Burgess</author>
</authors>
<title>Producing high-dimensional semantic spaces from lexical co-occurrence.</title>
<date>1996</date>
<journal>Behavior Research Methods, Instrumentation, and Computers,</journal>
<pages>28--203</pages>
<contexts>
<context position="4652" citStr="Lund and Burgess, 1996" startWordPosition="710" endWordPosition="713">tics 2 Related Work Matrix Factorization Methods. Matrix factorization methods for generating low-dimensional word representations have roots stretching as far back as LSA. These methods utilize low-rank approximations to decompose large matrices that capture statistical information about a corpus. The particular type of information captured by such matrices varies by application. In LSA, the matrices are of “term-document” type, i.e., the rows correspond to words or terms, and the columns correspond to different documents in the corpus. In contrast, the Hyperspace Analogue to Language (HAL) (Lund and Burgess, 1996), for example, utilizes matrices of “term-term” type, i.e., the rows and columns correspond to words and the entries correspond to the number of times a given word occurs in the context of another given word. A main problem with HAL and related methods is that the most frequent words contribute a disproportionate amount to the similarity measure: the number of times two words co-occur with the or and, for example, will have a large effect on their similarity despite conveying relatively little about their semantic relatedness. A number of techniques exist that addresses this shortcoming of HAL</context>
</contexts>
<marker>Lund, Burgess, 1996</marker>
<rawString>Kevin Lund and Curt Burgess. 1996. Producing high-dimensional semantic spaces from lexical co-occurrence. Behavior Research Methods, Instrumentation, and Computers, 28:203–208.</rawString>
</citation>
<citation valid="true">
<authors>
<author>Minh-Thang Luong</author>
<author>Richard Socher</author>
<author>Christopher D Manning</author>
</authors>
<title>Better word representations with recursive neural networks for morphology.</title>
<date>2013</date>
<tech>CoNLL-2013.</tech>
<contexts>
<context position="22032" citStr="Luong et al., 2013" startWordPosition="3867" endWordPosition="3870">h term that is depends on whether α &gt; 1, � O(|C|) if α &lt; 1, |X |= (22) O(|C|1/α) if α &gt; 1. For the corpora studied in this article, we observe that Xij is well-modeled by Eqn. (17) with α = 1.25. In this case we have that |X |= O(|C|0-8). Therefore we conclude that the complexity of the model is much better than the worst case O(V2), and in fact it does somewhat better than the on-line window-based methods which scale like O(|C|). 4 Experiments 4.1 Evaluation methods We conduct experiments on the word analogy task of Mikolov et al. (2013a), a variety of word similarity tasks, as described in (Luong et al., 2013), and on the CoNLL-2003 shared benchmark Table 2: Results on the word analogy task, given as percent accuracy. Underlined scores are best within groups of similarly-sized models; bold scores are best overall. HPCA vectors are publicly available2; (i)vLBL results are from (Mnih et al., 2013); skip-gram (SG) and CBOW results are from (Mikolov et al., 2013a,b); we trained SG† and CBOW† using the word2vec tool3. See text for details and a description of the SVD models. Model Dim. Size Sem. Syn. Tot. ivLBL 100 1.5B 55.9 50.1 53.2 HPCA 100 1.6B 4.2 16.4 10.8 GloVe 100 1.6B 67.5 54.3 60.3 SG 300 1B 6</context>
<context position="24990" citStr="Luong et al., 2013" startWordPosition="4410" endWordPosition="4413"> Accuracy [%] 80 70 60 50 40 30 Figure 2: Accuracy on the analogy task as function of vector size and window size/type. All models are trained on the 6 billion token corpus. In (a), the window size is 10. In (b) and (c), the vector size is 100. Word similarity. While the analogy task is our primary focus since it tests for interesting vector space substructures, we also evaluate our model on a variety of word similarity tasks in Table 3. These include WordSim-353 (Finkelstein et al., 2001), MC (Miller and Charles, 1991), RG (Rubenstein and Goodenough, 1965), SCWS (Huang et al., 2012), and RW (Luong et al., 2013). Named entity recognition. The CoNLL-2003 English benchmark dataset for NER is a collection of documents from Reuters newswire articles, annotated with four entity types: person, location, organization, and miscellaneous. We train models on CoNLL-03 training data on test on three datasets: 1) ConLL-03 testing data, 2) ACE Phase 2 (2001-02) and ACE-2003 data, and 3) MUC7 Formal Run test set. We adopt the BIO2 annotation standard, as well as all the preprocessing steps described in (Wang and Manning, 2013). We use a comprehensive set of discrete features that comes with the standard distributio</context>
</contexts>
<marker>Luong, Socher, Manning, 2013</marker>
<rawString>Minh-Thang Luong, Richard Socher, and Christopher D Manning. 2013. Better word representations with recursive neural networks for morphology. CoNLL-2013.</rawString>
</citation>
<citation valid="true">
<authors>
<author>Tomas Mikolov</author>
<author>Kai Chen</author>
<author>Greg Corrado</author>
<author>Jeffrey Dean</author>
</authors>
<title>Efficient Estimation of Word Representations in Vector Space.</title>
<date>2013</date>
<booktitle>In ICLR Workshop Papers.</booktitle>
<contexts>
<context position="1827" citStr="Mikolov et al. (2013" startWordPosition="270" endWordPosition="273">recognition. 1 Introduction Semantic vector space models of language represent each word with a real-valued vector. These vectors can be used as features in a variety of applications, such as information retrieval (Manning et al., 2008), document classification (Sebastiani, 2002), question answering (Tellex et al., 2003), named entity recognition (Turian et al., 2010), and parsing (Socher et al., 2013). Most word vector methods rely on the distance or angle between pairs of word vectors as the primary method for evaluating the intrinsic quality of such a set of word representations. Recently, Mikolov et al. (2013c) introduced a new evaluation scheme based on word analogies that probes the finer structure of the word vector space by examining not the scalar distance between word vectors, but rather their various dimensions of difference. For example, the analogy “king is to queen as man is to woman” should be encoded in the vector space by the vector equation king − queen = man − woman. This evaluation scheme favors models that produce dimensions of meaning, thereby capturing the multi-clustering idea of distributed representations (Bengio, 2009). The two main model families for learning word vectors a</context>
<context position="6834" citStr="Mikolov et al. (2013" startWordPosition="1063" endWordPosition="1066">learns word vector representations as part of a simple neural network architecture for language modeling. Collobert and Weston (2008) decoupled the word vector training from the downstream training objectives, which paved the way for Collobert et al. (2011) to use the full context of a word for learning the word representations, rather than just the preceding context as is the case with language models. Recently, the importance of the full neural network structure for learning useful word representations has been called into question. The skip-gram and continuous bag-of-words (CBOW) models of Mikolov et al. (2013a) propose a simple single-layer architecture based on the inner product between two word vectors. Mnih and Kavukcuoglu (2013) also proposed closely-related vector log-bilinear models, vLBL and ivLBL, and Levy et al. (2014) proposed explicit word embeddings based on a PPMI metric. In the skip-gram and ivLBL models, the objective is to predict a word’s context given the word itself, whereas the objective in the CBOW and vLBL models is to predict a word given its context. Through evaluation on a word analogy task, these models demonstrated the capacity to learn linguistic patterns as linear rela</context>
<context position="15145" citStr="Mikolov et al., 2013" startWordPosition="2581" endWordPosition="2584"> large number of functions satisfy these properties, but one class of functions that we found to work well can be parameterized as, PX) __ 1 (x/xmax)α if x &lt; xmax 9 f 1` 1 otherwise . ( ) Figure 1: Weighting function f with α = 3/4. The performance of the model depends weakly on the cutoff, which we fix to xmax = 100 for all our experiments. We found that α = 3/4 gives a modest improvement over a linear version with α = 1. Although we offer only empirical motivation for choosing the value 3/4, it is interesting that a similar fractional power scaling was found to give the best performance in (Mikolov et al., 2013a). 3.1 Relationship to Other Models Because all unsupervised methods for learning word vectors are ultimately based on the occurrence statistics of a corpus, there should be commonalities between the models. Nevertheless, certain models remain somewhat opaque in this regard, particularly the recent window-based methods like skip-gram and ivLBL. Therefore, in this subsection we show how these models are related to our proposed model, as defined in Eqn. (8). The starting point for the skip-gram or ivLBL methods is a model Qij for the probability that word j appears in the context of word i. For</context>
<context position="18751" citStr="Mikolov et al. (2013" startWordPosition="3232" endWordPosition="3235"> property of Q. A natural choice would be a least squares objective in which normalization factors in Q and P are discarded, ZJˆ = )2 i, j Xi ( ˆPij − ˆQij (14) where ˆPij = Xij and ˆQij = exp(wTi ˜wj) are the unnormalized distributions. At this stage another problem emerges, namely that Xij often takes very large values, which can complicate the optimization. An effective remedy is to minimize the Finally, we observe that while the weighting factor Xi is preordained by the on-line training method inherent to the skip-gram and ivLBL models, it is by no means guaranteed to be optimal. In fact, Mikolov et al. (2013a) observe that performance can be increased by filtering the data so as to reduce the effective value of the weighting factor for frequent words. With this in mind, we introduce a more general weighting function, which we are free to take to depend on the context word as well. The result is, f (Xij)(wTi ˜wj − log Xij)2 , (16) which is equivalent1 to the cost function of Eqn. (8), which we derived previously. 3.2 Complexity of the model As can be seen from Eqn. (8) and the explicit form of the weighting function f (X), the computational complexity of the model depends on the number of nonzero </context>
<context position="21956" citStr="Mikolov et al. (2013" startWordPosition="3854" endWordPosition="3857"> the two terms on the right hand side of Eqn. (21) will be relevant, and which term that is depends on whether α &gt; 1, � O(|C|) if α &lt; 1, |X |= (22) O(|C|1/α) if α &gt; 1. For the corpora studied in this article, we observe that Xij is well-modeled by Eqn. (17) with α = 1.25. In this case we have that |X |= O(|C|0-8). Therefore we conclude that the complexity of the model is much better than the worst case O(V2), and in fact it does somewhat better than the on-line window-based methods which scale like O(|C|). 4 Experiments 4.1 Evaluation methods We conduct experiments on the word analogy task of Mikolov et al. (2013a), a variety of word similarity tasks, as described in (Luong et al., 2013), and on the CoNLL-2003 shared benchmark Table 2: Results on the word analogy task, given as percent accuracy. Underlined scores are best within groups of similarly-sized models; bold scores are best overall. HPCA vectors are publicly available2; (i)vLBL results are from (Mnih et al., 2013); skip-gram (SG) and CBOW results are from (Mikolov et al., 2013a,b); we trained SG† and CBOW† using the word2vec tool3. See text for details and a description of the SVD models. Model Dim. Size Sem. Syn. Tot. ivLBL 100 1.5B 55.9 50.</context>
</contexts>
<marker>Mikolov, Chen, Corrado, Dean, 2013</marker>
<rawString>Tomas Mikolov, Kai Chen, Greg Corrado, and Jeffrey Dean. 2013a. Efficient Estimation of Word Representations in Vector Space. In ICLR Workshop Papers.</rawString>
</citation>
<citation valid="true">
<authors>
<author>Tomas Mikolov</author>
<author>Ilya Sutskever</author>
<author>Kai Chen</author>
<author>Greg Corrado</author>
<author>Jeffrey Dean</author>
</authors>
<title>Distributed representations of words and phrases and their compositionality.</title>
<date>2013</date>
<booktitle>In NIPS,</booktitle>
<pages>3111--3119</pages>
<contexts>
<context position="1827" citStr="Mikolov et al. (2013" startWordPosition="270" endWordPosition="273">recognition. 1 Introduction Semantic vector space models of language represent each word with a real-valued vector. These vectors can be used as features in a variety of applications, such as information retrieval (Manning et al., 2008), document classification (Sebastiani, 2002), question answering (Tellex et al., 2003), named entity recognition (Turian et al., 2010), and parsing (Socher et al., 2013). Most word vector methods rely on the distance or angle between pairs of word vectors as the primary method for evaluating the intrinsic quality of such a set of word representations. Recently, Mikolov et al. (2013c) introduced a new evaluation scheme based on word analogies that probes the finer structure of the word vector space by examining not the scalar distance between word vectors, but rather their various dimensions of difference. For example, the analogy “king is to queen as man is to woman” should be encoded in the vector space by the vector equation king − queen = man − woman. This evaluation scheme favors models that produce dimensions of meaning, thereby capturing the multi-clustering idea of distributed representations (Bengio, 2009). The two main model families for learning word vectors a</context>
<context position="6834" citStr="Mikolov et al. (2013" startWordPosition="1063" endWordPosition="1066">learns word vector representations as part of a simple neural network architecture for language modeling. Collobert and Weston (2008) decoupled the word vector training from the downstream training objectives, which paved the way for Collobert et al. (2011) to use the full context of a word for learning the word representations, rather than just the preceding context as is the case with language models. Recently, the importance of the full neural network structure for learning useful word representations has been called into question. The skip-gram and continuous bag-of-words (CBOW) models of Mikolov et al. (2013a) propose a simple single-layer architecture based on the inner product between two word vectors. Mnih and Kavukcuoglu (2013) also proposed closely-related vector log-bilinear models, vLBL and ivLBL, and Levy et al. (2014) proposed explicit word embeddings based on a PPMI metric. In the skip-gram and ivLBL models, the objective is to predict a word’s context given the word itself, whereas the objective in the CBOW and vLBL models is to predict a word given its context. Through evaluation on a word analogy task, these models demonstrated the capacity to learn linguistic patterns as linear rela</context>
<context position="15145" citStr="Mikolov et al., 2013" startWordPosition="2581" endWordPosition="2584"> large number of functions satisfy these properties, but one class of functions that we found to work well can be parameterized as, PX) __ 1 (x/xmax)α if x &lt; xmax 9 f 1` 1 otherwise . ( ) Figure 1: Weighting function f with α = 3/4. The performance of the model depends weakly on the cutoff, which we fix to xmax = 100 for all our experiments. We found that α = 3/4 gives a modest improvement over a linear version with α = 1. Although we offer only empirical motivation for choosing the value 3/4, it is interesting that a similar fractional power scaling was found to give the best performance in (Mikolov et al., 2013a). 3.1 Relationship to Other Models Because all unsupervised methods for learning word vectors are ultimately based on the occurrence statistics of a corpus, there should be commonalities between the models. Nevertheless, certain models remain somewhat opaque in this regard, particularly the recent window-based methods like skip-gram and ivLBL. Therefore, in this subsection we show how these models are related to our proposed model, as defined in Eqn. (8). The starting point for the skip-gram or ivLBL methods is a model Qij for the probability that word j appears in the context of word i. For</context>
<context position="18751" citStr="Mikolov et al. (2013" startWordPosition="3232" endWordPosition="3235"> property of Q. A natural choice would be a least squares objective in which normalization factors in Q and P are discarded, ZJˆ = )2 i, j Xi ( ˆPij − ˆQij (14) where ˆPij = Xij and ˆQij = exp(wTi ˜wj) are the unnormalized distributions. At this stage another problem emerges, namely that Xij often takes very large values, which can complicate the optimization. An effective remedy is to minimize the Finally, we observe that while the weighting factor Xi is preordained by the on-line training method inherent to the skip-gram and ivLBL models, it is by no means guaranteed to be optimal. In fact, Mikolov et al. (2013a) observe that performance can be increased by filtering the data so as to reduce the effective value of the weighting factor for frequent words. With this in mind, we introduce a more general weighting function, which we are free to take to depend on the context word as well. The result is, f (Xij)(wTi ˜wj − log Xij)2 , (16) which is equivalent1 to the cost function of Eqn. (8), which we derived previously. 3.2 Complexity of the model As can be seen from Eqn. (8) and the explicit form of the weighting function f (X), the computational complexity of the model depends on the number of nonzero </context>
<context position="21956" citStr="Mikolov et al. (2013" startWordPosition="3854" endWordPosition="3857"> the two terms on the right hand side of Eqn. (21) will be relevant, and which term that is depends on whether α &gt; 1, � O(|C|) if α &lt; 1, |X |= (22) O(|C|1/α) if α &gt; 1. For the corpora studied in this article, we observe that Xij is well-modeled by Eqn. (17) with α = 1.25. In this case we have that |X |= O(|C|0-8). Therefore we conclude that the complexity of the model is much better than the worst case O(V2), and in fact it does somewhat better than the on-line window-based methods which scale like O(|C|). 4 Experiments 4.1 Evaluation methods We conduct experiments on the word analogy task of Mikolov et al. (2013a), a variety of word similarity tasks, as described in (Luong et al., 2013), and on the CoNLL-2003 shared benchmark Table 2: Results on the word analogy task, given as percent accuracy. Underlined scores are best within groups of similarly-sized models; bold scores are best overall. HPCA vectors are publicly available2; (i)vLBL results are from (Mnih et al., 2013); skip-gram (SG) and CBOW results are from (Mikolov et al., 2013a,b); we trained SG† and CBOW† using the word2vec tool3. See text for details and a description of the SVD models. Model Dim. Size Sem. Syn. Tot. ivLBL 100 1.5B 55.9 50.</context>
</contexts>
<marker>Mikolov, Sutskever, Chen, Corrado, Dean, 2013</marker>
<rawString>Tomas Mikolov, Ilya Sutskever, Kai Chen, Greg Corrado, and Jeffrey Dean. 2013b. Distributed representations of words and phrases and their compositionality. In NIPS, pages 3111–3119.</rawString>
</citation>
<citation valid="true">
<authors>
<author>Tomas Mikolov</author>
<author>Wen tau Yih</author>
<author>Geoffrey Zweig</author>
</authors>
<title>Linguistic regularities in continuous space word representations.</title>
<date>2013</date>
<booktitle>In HLTNAACL.</booktitle>
<contexts>
<context position="1827" citStr="Mikolov et al. (2013" startWordPosition="270" endWordPosition="273">recognition. 1 Introduction Semantic vector space models of language represent each word with a real-valued vector. These vectors can be used as features in a variety of applications, such as information retrieval (Manning et al., 2008), document classification (Sebastiani, 2002), question answering (Tellex et al., 2003), named entity recognition (Turian et al., 2010), and parsing (Socher et al., 2013). Most word vector methods rely on the distance or angle between pairs of word vectors as the primary method for evaluating the intrinsic quality of such a set of word representations. Recently, Mikolov et al. (2013c) introduced a new evaluation scheme based on word analogies that probes the finer structure of the word vector space by examining not the scalar distance between word vectors, but rather their various dimensions of difference. For example, the analogy “king is to queen as man is to woman” should be encoded in the vector space by the vector equation king − queen = man − woman. This evaluation scheme favors models that produce dimensions of meaning, thereby capturing the multi-clustering idea of distributed representations (Bengio, 2009). The two main model families for learning word vectors a</context>
<context position="6834" citStr="Mikolov et al. (2013" startWordPosition="1063" endWordPosition="1066">learns word vector representations as part of a simple neural network architecture for language modeling. Collobert and Weston (2008) decoupled the word vector training from the downstream training objectives, which paved the way for Collobert et al. (2011) to use the full context of a word for learning the word representations, rather than just the preceding context as is the case with language models. Recently, the importance of the full neural network structure for learning useful word representations has been called into question. The skip-gram and continuous bag-of-words (CBOW) models of Mikolov et al. (2013a) propose a simple single-layer architecture based on the inner product between two word vectors. Mnih and Kavukcuoglu (2013) also proposed closely-related vector log-bilinear models, vLBL and ivLBL, and Levy et al. (2014) proposed explicit word embeddings based on a PPMI metric. In the skip-gram and ivLBL models, the objective is to predict a word’s context given the word itself, whereas the objective in the CBOW and vLBL models is to predict a word given its context. Through evaluation on a word analogy task, these models demonstrated the capacity to learn linguistic patterns as linear rela</context>
<context position="15145" citStr="Mikolov et al., 2013" startWordPosition="2581" endWordPosition="2584"> large number of functions satisfy these properties, but one class of functions that we found to work well can be parameterized as, PX) __ 1 (x/xmax)α if x &lt; xmax 9 f 1` 1 otherwise . ( ) Figure 1: Weighting function f with α = 3/4. The performance of the model depends weakly on the cutoff, which we fix to xmax = 100 for all our experiments. We found that α = 3/4 gives a modest improvement over a linear version with α = 1. Although we offer only empirical motivation for choosing the value 3/4, it is interesting that a similar fractional power scaling was found to give the best performance in (Mikolov et al., 2013a). 3.1 Relationship to Other Models Because all unsupervised methods for learning word vectors are ultimately based on the occurrence statistics of a corpus, there should be commonalities between the models. Nevertheless, certain models remain somewhat opaque in this regard, particularly the recent window-based methods like skip-gram and ivLBL. Therefore, in this subsection we show how these models are related to our proposed model, as defined in Eqn. (8). The starting point for the skip-gram or ivLBL methods is a model Qij for the probability that word j appears in the context of word i. For</context>
<context position="18751" citStr="Mikolov et al. (2013" startWordPosition="3232" endWordPosition="3235"> property of Q. A natural choice would be a least squares objective in which normalization factors in Q and P are discarded, ZJˆ = )2 i, j Xi ( ˆPij − ˆQij (14) where ˆPij = Xij and ˆQij = exp(wTi ˜wj) are the unnormalized distributions. At this stage another problem emerges, namely that Xij often takes very large values, which can complicate the optimization. An effective remedy is to minimize the Finally, we observe that while the weighting factor Xi is preordained by the on-line training method inherent to the skip-gram and ivLBL models, it is by no means guaranteed to be optimal. In fact, Mikolov et al. (2013a) observe that performance can be increased by filtering the data so as to reduce the effective value of the weighting factor for frequent words. With this in mind, we introduce a more general weighting function, which we are free to take to depend on the context word as well. The result is, f (Xij)(wTi ˜wj − log Xij)2 , (16) which is equivalent1 to the cost function of Eqn. (8), which we derived previously. 3.2 Complexity of the model As can be seen from Eqn. (8) and the explicit form of the weighting function f (X), the computational complexity of the model depends on the number of nonzero </context>
<context position="21956" citStr="Mikolov et al. (2013" startWordPosition="3854" endWordPosition="3857"> the two terms on the right hand side of Eqn. (21) will be relevant, and which term that is depends on whether α &gt; 1, � O(|C|) if α &lt; 1, |X |= (22) O(|C|1/α) if α &gt; 1. For the corpora studied in this article, we observe that Xij is well-modeled by Eqn. (17) with α = 1.25. In this case we have that |X |= O(|C|0-8). Therefore we conclude that the complexity of the model is much better than the worst case O(V2), and in fact it does somewhat better than the on-line window-based methods which scale like O(|C|). 4 Experiments 4.1 Evaluation methods We conduct experiments on the word analogy task of Mikolov et al. (2013a), a variety of word similarity tasks, as described in (Luong et al., 2013), and on the CoNLL-2003 shared benchmark Table 2: Results on the word analogy task, given as percent accuracy. Underlined scores are best within groups of similarly-sized models; bold scores are best overall. HPCA vectors are publicly available2; (i)vLBL results are from (Mnih et al., 2013); skip-gram (SG) and CBOW results are from (Mikolov et al., 2013a,b); we trained SG† and CBOW† using the word2vec tool3. See text for details and a description of the SVD models. Model Dim. Size Sem. Syn. Tot. ivLBL 100 1.5B 55.9 50.</context>
</contexts>
<marker>Mikolov, Yih, Zweig, 2013</marker>
<rawString>Tomas Mikolov, Wen tau Yih, and Geoffrey Zweig. 2013c. Linguistic regularities in continuous space word representations. In HLTNAACL.</rawString>
</citation>
<citation valid="true">
<authors>
<author>George A Miller</author>
<author>Walter G Charles</author>
</authors>
<title>Contextual correlates of semantic similarity. Language and cognitive processes,</title>
<date>1991</date>
<pages>6--1</pages>
<contexts>
<context position="24896" citStr="Miller and Charles, 1991" startWordPosition="4394" endWordPosition="4397">etric context 65 60 55 50 Semantic Syntactic Overall 402 4 6 8 10 Window Size (c) Asymmetric context Accuracy [%] 80 70 60 50 40 30 Figure 2: Accuracy on the analogy task as function of vector size and window size/type. All models are trained on the 6 billion token corpus. In (a), the window size is 10. In (b) and (c), the vector size is 100. Word similarity. While the analogy task is our primary focus since it tests for interesting vector space substructures, we also evaluate our model on a variety of word similarity tasks in Table 3. These include WordSim-353 (Finkelstein et al., 2001), MC (Miller and Charles, 1991), RG (Rubenstein and Goodenough, 1965), SCWS (Huang et al., 2012), and RW (Luong et al., 2013). Named entity recognition. The CoNLL-2003 English benchmark dataset for NER is a collection of documents from Reuters newswire articles, annotated with four entity types: person, location, organization, and miscellaneous. We train models on CoNLL-03 training data on test on three datasets: 1) ConLL-03 testing data, 2) ACE Phase 2 (2001-02) and ACE-2003 data, and 3) MUC7 Formal Run test set. We adopt the BIO2 annotation standard, as well as all the preprocessing steps described in (Wang and Manning, 2</context>
</contexts>
<marker>Miller, Charles, 1991</marker>
<rawString>George A. Miller and Walter G. Charles. 1991. Contextual correlates of semantic similarity. Language and cognitive processes, 6(1):1–28.</rawString>
</citation>
<citation valid="true">
<authors>
<author>Andriy Mnih</author>
<author>Koray Kavukcuoglu</author>
</authors>
<title>Learning word embeddings efficiently with noise-contrastive estimation.</title>
<date>2013</date>
<booktitle>In NIPS.</booktitle>
<contexts>
<context position="6960" citStr="Mnih and Kavukcuoglu (2013)" startWordPosition="1082" endWordPosition="1085"> Weston (2008) decoupled the word vector training from the downstream training objectives, which paved the way for Collobert et al. (2011) to use the full context of a word for learning the word representations, rather than just the preceding context as is the case with language models. Recently, the importance of the full neural network structure for learning useful word representations has been called into question. The skip-gram and continuous bag-of-words (CBOW) models of Mikolov et al. (2013a) propose a simple single-layer architecture based on the inner product between two word vectors. Mnih and Kavukcuoglu (2013) also proposed closely-related vector log-bilinear models, vLBL and ivLBL, and Levy et al. (2014) proposed explicit word embeddings based on a PPMI metric. In the skip-gram and ivLBL models, the objective is to predict a word’s context given the word itself, whereas the objective in the CBOW and vLBL models is to predict a word given its context. Through evaluation on a word analogy task, these models demonstrated the capacity to learn linguistic patterns as linear relationships between the word vectors. Unlike the matrix factorization methods, the shallow window-based methods suffer from the </context>
</contexts>
<marker>Mnih, Kavukcuoglu, 2013</marker>
<rawString>Andriy Mnih and Koray Kavukcuoglu. 2013. Learning word embeddings efficiently with noise-contrastive estimation. In NIPS.</rawString>
</citation>
<citation valid="true">
<authors>
<author>Douglas L T Rohde</author>
<author>Laura M Gonnerman</author>
<author>David C Plaut</author>
</authors>
<title>An improved model of semantic similarity based on lexical co-occurence.</title>
<date>2006</date>
<journal>Communications of the ACM,</journal>
<pages>8--627</pages>
<contexts>
<context position="5299" citStr="Rohde et al., 2006" startWordPosition="817" endWordPosition="820">ices of “term-term” type, i.e., the rows and columns correspond to words and the entries correspond to the number of times a given word occurs in the context of another given word. A main problem with HAL and related methods is that the most frequent words contribute a disproportionate amount to the similarity measure: the number of times two words co-occur with the or and, for example, will have a large effect on their similarity despite conveying relatively little about their semantic relatedness. A number of techniques exist that addresses this shortcoming of HAL, such as the COALS method (Rohde et al., 2006), in which the co-occurrence matrix is first transformed by an entropy- or correlation-based normalization. An advantage of this type of transformation is that the raw co-occurrence counts, which for a reasonably sized corpus might span 8 or 9 orders of magnitude, are compressed so as to be distributed more evenly in a smaller interval. A variety of newer models also pursue this approach, including a study (Bullinaria and Levy, 2007) that indicates that positive pointwise mutual information (PPMI) is a good transformation. More recently, a square root type transformation in the form of Helling</context>
</contexts>
<marker>Rohde, Gonnerman, Plaut, 2006</marker>
<rawString>Douglas L. T. Rohde, Laura M. Gonnerman, and David C. Plaut. 2006. An improved model of semantic similarity based on lexical co-occurence. Communications of the ACM, 8:627–633.</rawString>
</citation>
<citation valid="true">
<authors>
<author>Herbert Rubenstein</author>
<author>John B Goodenough</author>
</authors>
<title>Contextual correlates of synonymy.</title>
<date>1965</date>
<journal>Communications of the ACM,</journal>
<volume>8</volume>
<issue>10</issue>
<contexts>
<context position="24934" citStr="Rubenstein and Goodenough, 1965" startWordPosition="4399" endWordPosition="4402">tic Syntactic Overall 402 4 6 8 10 Window Size (c) Asymmetric context Accuracy [%] 80 70 60 50 40 30 Figure 2: Accuracy on the analogy task as function of vector size and window size/type. All models are trained on the 6 billion token corpus. In (a), the window size is 10. In (b) and (c), the vector size is 100. Word similarity. While the analogy task is our primary focus since it tests for interesting vector space substructures, we also evaluate our model on a variety of word similarity tasks in Table 3. These include WordSim-353 (Finkelstein et al., 2001), MC (Miller and Charles, 1991), RG (Rubenstein and Goodenough, 1965), SCWS (Huang et al., 2012), and RW (Luong et al., 2013). Named entity recognition. The CoNLL-2003 English benchmark dataset for NER is a collection of documents from Reuters newswire articles, annotated with four entity types: person, location, organization, and miscellaneous. We train models on CoNLL-03 training data on test on three datasets: 1) ConLL-03 testing data, 2) ACE Phase 2 (2001-02) and ACE-2003 data, and 3) MUC7 Formal Run test set. We adopt the BIO2 annotation standard, as well as all the preprocessing steps described in (Wang and Manning, 2013). We use a comprehensive set of di</context>
</contexts>
<marker>Rubenstein, Goodenough, 1965</marker>
<rawString>Herbert Rubenstein and John B. Goodenough. 1965. Contextual correlates of synonymy. Communications of the ACM, 8(10):627–633.</rawString>
</citation>
<citation valid="true">
<authors>
<author>Fabrizio Sebastiani</author>
</authors>
<title>Machine learning in automated text categorization.</title>
<date>2002</date>
<journal>ACM Computing Surveys,</journal>
<pages>34--1</pages>
<contexts>
<context position="1487" citStr="Sebastiani, 2002" startWordPosition="216" endWordPosition="217">ts in a word-word cooccurrence matrix, rather than on the entire sparse matrix or on individual context windows in a large corpus. The model produces a vector space with meaningful substructure, as evidenced by its performance of 75% on a recent word analogy task. It also outperforms related models on similarity tasks and named entity recognition. 1 Introduction Semantic vector space models of language represent each word with a real-valued vector. These vectors can be used as features in a variety of applications, such as information retrieval (Manning et al., 2008), document classification (Sebastiani, 2002), question answering (Tellex et al., 2003), named entity recognition (Turian et al., 2010), and parsing (Socher et al., 2013). Most word vector methods rely on the distance or angle between pairs of word vectors as the primary method for evaluating the intrinsic quality of such a set of word representations. Recently, Mikolov et al. (2013c) introduced a new evaluation scheme based on word analogies that probes the finer structure of the word vector space by examining not the scalar distance between word vectors, but rather their various dimensions of difference. For example, the analogy “king </context>
</contexts>
<marker>Sebastiani, 2002</marker>
<rawString>Fabrizio Sebastiani. 2002. Machine learning in automated text categorization. ACM Computing Surveys, 34:1–47.</rawString>
</citation>
<citation valid="true">
<authors>
<author>Richard Socher</author>
<author>John Bauer</author>
<author>Christopher D Manning</author>
<author>Andrew Y Ng</author>
</authors>
<title>Parsing With Compositional Vector Grammars.</title>
<date>2013</date>
<booktitle>In ACL.</booktitle>
<contexts>
<context position="1612" citStr="Socher et al., 2013" startWordPosition="233" endWordPosition="236"> corpus. The model produces a vector space with meaningful substructure, as evidenced by its performance of 75% on a recent word analogy task. It also outperforms related models on similarity tasks and named entity recognition. 1 Introduction Semantic vector space models of language represent each word with a real-valued vector. These vectors can be used as features in a variety of applications, such as information retrieval (Manning et al., 2008), document classification (Sebastiani, 2002), question answering (Tellex et al., 2003), named entity recognition (Turian et al., 2010), and parsing (Socher et al., 2013). Most word vector methods rely on the distance or angle between pairs of word vectors as the primary method for evaluating the intrinsic quality of such a set of word representations. Recently, Mikolov et al. (2013c) introduced a new evaluation scheme based on word analogies that probes the finer structure of the word vector space by examining not the scalar distance between word vectors, but rather their various dimensions of difference. For example, the analogy “king is to queen as man is to woman” should be encoded in the vector space by the vector equation king − queen = man − woman. This</context>
</contexts>
<marker>Socher, Bauer, Manning, Ng, 2013</marker>
<rawString>Richard Socher, John Bauer, Christopher D. Manning, and Andrew Y. Ng. 2013. Parsing With Compositional Vector Grammars. In ACL.</rawString>
</citation>
<citation valid="true">
<authors>
<author>Stefanie Tellex</author>
<author>Boris Katz</author>
<author>Jimmy Lin</author>
<author>Aaron Fernandes</author>
<author>Gregory Marton</author>
</authors>
<title>Quantitative evaluation of passage retrieval algorithms for question answering.</title>
<date>2003</date>
<booktitle>In Proceedings of the SIGIR Conference on Research and Development in Informaion Retrieval.</booktitle>
<contexts>
<context position="1529" citStr="Tellex et al., 2003" startWordPosition="220" endWordPosition="223">rather than on the entire sparse matrix or on individual context windows in a large corpus. The model produces a vector space with meaningful substructure, as evidenced by its performance of 75% on a recent word analogy task. It also outperforms related models on similarity tasks and named entity recognition. 1 Introduction Semantic vector space models of language represent each word with a real-valued vector. These vectors can be used as features in a variety of applications, such as information retrieval (Manning et al., 2008), document classification (Sebastiani, 2002), question answering (Tellex et al., 2003), named entity recognition (Turian et al., 2010), and parsing (Socher et al., 2013). Most word vector methods rely on the distance or angle between pairs of word vectors as the primary method for evaluating the intrinsic quality of such a set of word representations. Recently, Mikolov et al. (2013c) introduced a new evaluation scheme based on word analogies that probes the finer structure of the word vector space by examining not the scalar distance between word vectors, but rather their various dimensions of difference. For example, the analogy “king is to queen as man is to woman” should be </context>
</contexts>
<marker>Tellex, Katz, Lin, Fernandes, Marton, 2003</marker>
<rawString>Stefanie Tellex, Boris Katz, Jimmy Lin, Aaron Fernandes, and Gregory Marton. 2003. Quantitative evaluation of passage retrieval algorithms for question answering. In Proceedings of the SIGIR Conference on Research and Development in Informaion Retrieval.</rawString>
</citation>
<citation valid="true">
<authors>
<author>F Erik</author>
</authors>
<title>Tjong Kim Sang and Fien De Meulder.</title>
<date>2003</date>
<booktitle>In CoNLL-2003.</booktitle>
<marker>Erik, 2003</marker>
<rawString>Erik F. Tjong Kim Sang and Fien De Meulder. 2003. Introduction to the CoNLL-2003 shared task: Language-independent named entity recognition. In CoNLL-2003.</rawString>
</citation>
<citation valid="true">
<authors>
<author>Joseph Turian</author>
<author>Lev Ratinov</author>
<author>Yoshua Bengio</author>
</authors>
<title>Word representations: a simple and general method for semi-supervised learning.</title>
<date>2010</date>
<booktitle>In Proceedings ofACL,</booktitle>
<pages>384--394</pages>
<contexts>
<context position="1577" citStr="Turian et al., 2010" startWordPosition="227" endWordPosition="230">dividual context windows in a large corpus. The model produces a vector space with meaningful substructure, as evidenced by its performance of 75% on a recent word analogy task. It also outperforms related models on similarity tasks and named entity recognition. 1 Introduction Semantic vector space models of language represent each word with a real-valued vector. These vectors can be used as features in a variety of applications, such as information retrieval (Manning et al., 2008), document classification (Sebastiani, 2002), question answering (Tellex et al., 2003), named entity recognition (Turian et al., 2010), and parsing (Socher et al., 2013). Most word vector methods rely on the distance or angle between pairs of word vectors as the primary method for evaluating the intrinsic quality of such a set of word representations. Recently, Mikolov et al. (2013c) introduced a new evaluation scheme based on word analogies that probes the finer structure of the word vector space by examining not the scalar distance between word vectors, but rather their various dimensions of difference. For example, the analogy “king is to queen as man is to woman” should be encoded in the vector space by the vector equati</context>
<context position="33714" citStr="Turian et al., 2010" startWordPosition="5908" endWordPosition="5911">first 8We use the same parameters as above, except in this case we found 5 negative samples to work slightly better than 10. 1539 Accuracy [%] Table 4: F1 score on NER task with 50d vectors. Discrete is the baseline without word vectors. We use publicly-available vectors for HPCA, HSMN, and CW. See text for details. Model Dev Test ACE MUC7 Discrete 91.0 85.4 77.4 73.4 SVD 90.8 85.7 77.3 73.7 SVD-S 91.0 85.5 77.6 74.3 SVD-L 90.5 84.8 73.6 71.5 HPCA 92.6 88.7 81.7 80.7 HSMN 90.5 85.7 78.7 74.7 CW 92.2 87.4 81.7 80.2 CBOW 93.1 88.2 82.2 81.1 GloVe 93.2 88.3 82.9 82.2 shown for neural vectors in (Turian et al., 2010). 4.4 Model Analysis: Vector Length and Context Size In Fig. 2, we show the results of experiments that vary vector length and context window. A context window that extends to the left and right of a target word will be called symmetric, and one which extends only to the left will be called asymmetric. In (a), we observe diminishing returns for vectors larger than about 200 dimensions. In (b) and (c), we examine the effect of varying the window size for symmetric and asymmetric context windows. Performance is better on the syntactic subtask for small and asymmetric context windows, which align</context>
</contexts>
<marker>Turian, Ratinov, Bengio, 2010</marker>
<rawString>Joseph Turian, Lev Ratinov, and Yoshua Bengio. 2010. Word representations: a simple and general method for semi-supervised learning. In Proceedings ofACL, pages 384–394.</rawString>
</citation>
<citation valid="true">
<authors>
<author>Mengqiu Wang</author>
<author>Christopher D Manning</author>
</authors>
<title>Effect of non-linear deep architecture in sequence labeling.</title>
<date>2013</date>
<booktitle>In Proceedings of the 6th International Joint Conference on Natural Language Processing (IJCNLP).</booktitle>
<contexts>
<context position="25500" citStr="Wang and Manning, 2013" startWordPosition="4492" endWordPosition="4495"> and Charles, 1991), RG (Rubenstein and Goodenough, 1965), SCWS (Huang et al., 2012), and RW (Luong et al., 2013). Named entity recognition. The CoNLL-2003 English benchmark dataset for NER is a collection of documents from Reuters newswire articles, annotated with four entity types: person, location, organization, and miscellaneous. We train models on CoNLL-03 training data on test on three datasets: 1) ConLL-03 testing data, 2) ACE Phase 2 (2001-02) and ACE-2003 data, and 3) MUC7 Formal Run test set. We adopt the BIO2 annotation standard, as well as all the preprocessing steps described in (Wang and Manning, 2013). We use a comprehensive set of discrete features that comes with the standard distribution of the Stanford NER model (Finkel et al., 2005). A total of 437,905 discrete features were generated for the CoNLL2003 training dataset. In addition, 50-dimensional vectors for each word of a five-word context are added and used as continuous features. With these features as input, we trained a conditional random field (CRF) with exactly the same setup as the CRFjoin model of (Wang and Manning, 2013). 4.2 Corpora and training details We trained our model on five corpora of varying sizes: a 2010 Wikipedi</context>
<context position="32459" citStr="Wang and Manning (2013)" startWordPosition="5685" endWordPosition="5688">ch feature across the vocabulary and then calculating the cosine similarity. We compute Spearman’s rank correlation coefficient between this score and the human judgments. CBOW∗ denotes the vectors available on the word2vec website that are trained with word and phrase vectors on 100B words of news data. GloVe outperforms it while using a corpus less than half the size. Table 4 shows results on the NER task with the CRF-based model. The L-BFGS training terminates when no improvement has been achieved on the dev set for 25 iterations. Otherwise all configurations are identical to those used by Wang and Manning (2013). The model labeled Discrete is the baseline using a comprehensive set of discrete features that comes with the standard distribution of the Stanford NER model, but with no word vector features. In addition to the HPCA and SVD models discussed previously, we also compare to the models of Huang et al. (2012) (HSMN) and Collobert and Weston (2008) (CW). We trained the CBOW model using the word2vec tool8. The GloVe model outperforms all other methods on all evaluation metrics, except for the CoNLL test set, on which the HPCA method does slightly better. We conclude that the GloVe vectors are usef</context>
</contexts>
<marker>Wang, Manning, 2013</marker>
<rawString>Mengqiu Wang and Christopher D. Manning. 2013. Effect of non-linear deep architecture in sequence labeling. In Proceedings of the 6th International Joint Conference on Natural Language Processing (IJCNLP).</rawString>
</citation>
</citationList>
</algorithm>
</algorithms>
<?xml version="1.0" encoding="UTF-8"?>
<algorithms version="110505">
<algorithm name="SectLabel" version="110505">
<variant no="0" confidence="0.000042">
<title confidence="0.986068">
Identifying Argumentative Discourse Structures in Persuasive Essays
</title>
<author confidence="0.802001">
Christian Stab† and Iryna Gurevych†‡
</author>
<affiliation confidence="0.8173495">
†Ubiquitous Knowledge Processing Lab (UKP-TUDA)
Department of Computer Science, Technische Universit¨at Darmstadt
‡Ubiquitous Knowledge Processing Lab (UKP-DIPF)
German Institute for Educational Research
</affiliation>
<email confidence="0.889062">
www.ukp.tu-darmstadt.de
</email>
<sectionHeader confidence="0.991767" genericHeader="abstract">
Abstract
</sectionHeader>
<bodyText confidence="0.99994415">
In this paper, we present a novel ap-
proach for identifying argumentative dis-
course structures in persuasive essays. The
structure of argumentation consists of sev-
eral components (i.e. claims and premises)
that are connected with argumentative re-
lations. We consider this task in two
consecutive steps. First, we identify the
components of arguments using multiclass
classification. Second, we classify a pair
of argument components as either support
or non-support for identifying the struc-
ture of argumentative discourse. For both
tasks, we evaluate several classifiers and
propose novel feature sets including struc-
tural, lexical, syntactic and contextual fea-
tures. In our experiments, we obtain a
macro F1-score of 0.726 for identifying
argument components and 0.722 for argu-
mentative relations.
</bodyText>
<sectionHeader confidence="0.998992" genericHeader="keywords">
1 Introduction
</sectionHeader>
<bodyText confidence="0.999085056603774">
Argumentation is a crucial aspect of writing skills
acquisition. The ability of formulating persuasive
arguments is not only the foundation for convinc-
ing an audience of novel ideas but also plays a ma-
jor role in general decision making and analyzing
different stances. However, current writing sup-
port is limited to feedback about spelling, gram-
mar, or stylistic properties and there is currently no
system that provides feedback about written argu-
mentation. By integrating argumentation mining
in writing environments, students will be able to
inspect their texts for plausibility and to improve
the quality of their argumentation.
An argument consists of several components. It
includes a claim that is supported or attacked by at
least one premise. The claim is the central compo-
nent of an argument. It is a controversial statement
that should not be accepted by the reader without
additional support.1 The premise underpins the
validity of the claim. It is a reason given by an
author for persuading readers of the claim. Argu-
mentative relations model the discourse structure
of arguments. They indicate which argument com-
ponents are related and constitute the structure of
argumentative discourse. For example, the argu-
ment in the following paragraph contains four ar-
gument components: one claim (in bold face) and
three premises (underlined).
“(1) Museums and art galleries provide
a better understanding about arts than
Internet. (2) In most museums and art
galleries, detailed descriptions in terms
of the background, history and author
are provided. (3) Seeing an artwork on-
line is not the same as watching it with
our own eyes, as (4) the picture online
does not show the texture or three-di-
mensional structure of the art, which is
important to study.”
In this example, the premises (2) and (3) sup-
port the claim (1) whereas premise (4) is a support
for premise (3). Thus, this example includes three
argumentative support relations holding between
the components (2,1), (3,1) and (4,3) signaling that
the source component is a justification of the target
component. This illustrates two important proper-
ties of argumentative discourse structures. First,
argumentative relations are often implicit (not in-
dicated by discourse markers; e.g. the relation
holding between (2) and (1)). Indeed, Marcu and
Echihabi (2002) found that only 26% of the ev-
idence relations in the RST Discourse Treebank
(Carlson et al., 2001) include discourse markers.
</bodyText>
<footnote confidence="0.8020774">
1We use the term claim synonymously to conclusion.
In our definition the differentiation between claims and
premises does not indicate the validity of the statements but
signals which components include the gist of an argument
and which are given by the author as justification.
</footnote>
<page confidence="0.990509">
46
</page>
<note confidence="0.9108085">
Proceedings of the 2014 Conference on Empirical Methods in Natural Language Processing (EMNLP), pages 46–56,
October 25-29, 2014, Doha, Qatar. c�2014 Association for Computational Linguistics
</note>
<bodyText confidence="0.999966584615385">
Second, in contrast to Rhetorical Structure Theory
(RST) (Mann and Thompson, 1987), argumenta-
tive relations also hold between non-adjacent sen-
tences/clauses. For instance, in the corpus com-
piled by Stab and Gurevych (2014) only 37% of
the premises appear adjacent to a claim. There-
fore, existing approaches of discourse analysis,
e.g. based on RST, do not meet the require-
ments of argumentative discourse structure iden-
tification, since they only consider discourse re-
lations between adjacent sentences/clauses (Peld-
szus and Stede, 2013). In addition, there are no
distinct argumentative relations included in com-
mon approaches like RST or the Penn Discourse
Treebank (PDTB) (Prasad et al., 2008), since they
are focused on identifying general discourse struc-
tures (cp. section 2.2).
Most of the existing argumentation mining
methods focus solely on the identification of ar-
gument components. However, identifying argu-
mentative discourse structures is an important task
(Sergeant, 2013) in particular for providing feed-
back about argumentation. First, argumentative
discourse structures are essential for evaluating the
quality of an argument, since it is not possible
to examine how well a claim is justified without
knowing which premises belong to it. Second,
methods that recognize if a statement supports a
given claim enable the collection of additional ev-
idence from other sources. Third, the structure of
argumentation is needed for recommending better
arrangements of argument components and mean-
ingful usage of discourse markers. Both foster ar-
gument comprehension and recall (Britt and Lar-
son, 2003) and thus increase the argumentation
quality. To the best of our knowledge, there is
currently only one approach that aims at identi-
fying argumentative discourse structures proposed
by Mochales-Palau and Moens (2009). However,
it relies on a manually created context-free gram-
mar (CFG) and is tailored to the legal domain,
which follows a standardized argumentation style.
Therefore, it is likely that it will not achieve ac-
ceptable accuracy when applied to more general
texts in which discourse markers are missing or
even misleadingly used (e.g. student texts).
In this work, we present a novel approach
for identifying argumentative discourse structures
which includes two consecutive steps. In the first
step, we focus on the identification of argument
components using a multiclass classification ap-
proach. In the second step, we identify argumen-
tative relations by classifying a pair of argument
components as either support or non-support. In
particular, the contributions of this work are the
following: First, we introduce a novel approach
for identifying argumentative discourse structures.
Contrary to previous approaches, our approach
is capable of identifying argumentative discourse
structures even if discourse markers are missing or
misleadingly used. Second, we present two novel
feature sets for identifying argument components
as well as argumentative relations. Third, we eval-
uate several classifiers and feature groups for iden-
tifying the best system for both tasks.
</bodyText>
<sectionHeader confidence="0.999934" genericHeader="introduction">
2 Related Work
</sectionHeader>
<subsectionHeader confidence="0.99929">
2.1 Argumentation Mining
</subsectionHeader>
<bodyText confidence="0.999609852941176">
Previous research on argumentation mining spans
several subtasks, including (1) the separation of
argumentative from non-argumentative text units
(Moens et al., 2007; Florou et al., 2013), (2)
the classification of argument components or
argumentation schemes (Rooney et al., 2012;
Mochales-Palau and Moens, 2009; Teufel, 1999;
Feng and Hirst, 2011), and (3) the identification
of argumentation structures (Mochales-Palau and
Moens, 2009; Wyner et al., 2010).
The separation of argumentative from non-
argumentative text units is usually considered as
a binary classification task and constitutes one of
the first steps in an argumentation mining pipeline.
Moens et al. (2007) propose an approach for iden-
tifying argumentative sentences in the Araucaria
corpus (Reed et al., 2008). The argument an-
notations in Araucaria are based on a domain-
independent argumentation theory proposed by
Walton (1996). In their experiments, they ob-
tain the best accuracy (73.75%) using a combi-
nation of word pairs, text statistics, verbs, and a
list of keywords indicative for argumentative dis-
course. Florou et al. (2013) report a similar ap-
proach. They classify text segments crawled with
a focused crawler as either containing an argu-
ment or not. Their approach is based on several
discourse markers and features extracted from the
tense and mood of verbs. They report an F1-score
of 0.764 for their best performing system.
One of the first approaches focusing on the
identification of argument components is Argu-
mentative Zoning proposed by Teufel (1999). The
underlying assumption of this work is that argu-
</bodyText>
<page confidence="0.998734">
47
</page>
<bodyText confidence="0.9999854">
ment components extracted from a scientific arti-
cle provide a good summary of its content. Each
sentence is classified as one of seven rhetorical
roles including claim, result or purpose. The ap-
proach obtained an F1-score of 0.462 using struc-
tural, lexical and syntactic features. Rooney et
al. (2013) also focus on the identification of ar-
gument components but in contrast to the work of
Teufel (1999) their scheme is not tailored to a par-
ticular genre. In their experiments, they identify
claims, premises and non-argumentative text units
in the Araucaria corpus and report an overall ac-
curacy of 65%. Feng and Hirst (2011) also use
the Araucaria corpus for their experiments but fo-
cus on the identification of argumentation schemes
(Walton, 1996), which are templates for forms of
arguments (e.g. argument from example or argu-
ment from consequence). Since their approach is
based on features extracted from mutual informa-
tion of claims and premises, it requires that the ar-
gument components are reliably identified in ad-
vance. In their experiments, they achieve an accu-
racy between 62.9% and 97.9% depending on the
particular scheme and the classification setup.
In contrast to all approaches mentioned above,
the work presented in this paper focuses be-
sides the separation of argumentative from non-
argumentative text units and the classification of
argument components on the extraction of the ar-
gumentative discourse structure to identify which
components of the argument belong together for
achieving a more fine-grained and detailed analy-
sis of argumentation. We are only aware of one ap-
proach (Mochales-Palau and Moens, 2009; Wyner
et al., 2010) that also focuses on the identifica-
tion of argumentative discourse structures. How-
ever, this approach is based on a manually created
CFG that is tailored to documents from the legal
domain, which follow a standardized argumenta-
tion style. Therefore, it does not accommodate ill-
formatted arguments (Wyner et al., 2010), which
are likely in argumentative writing support. In ad-
dition, the approach relies on discourse markers
and is therefore not applicable for identifying im-
plicit argumentative discourse structures.
</bodyText>
<subsectionHeader confidence="0.997693">
2.2 Discourse Relations
</subsectionHeader>
<bodyText confidence="0.981857363636364">
Identifying argumentative discourse structures is
closely related to discourse analysis. As illustrated
2Calculated from the precision and recall scores provided
for individual rhetorical roles in (Teufel, 1999, p. 225).
in the initial example, the identification of argu-
mentative relations postulates the identification of
implicit as well as non-adjacent discourse rela-
tions. Marcu and Echihabi (2002) present the first
approach focused on identifying implicit discourse
relations. They exploit several discourse mark-
ers (e.g. ‘because’ or ‘but’) for collecting large
amounts of training data. For their experiments
they remove the discourse markers and discover
that word pair features are indicative for implicit
discourse relations. Depending on the utilized cor-
pus, they obtain accuracies between 64% and 75%
for identifying a cause-explanation-evidence rela-
tion (the most similar relation of their work com-
pared to argumentative relations).
With the release of the PDTB, the identifica-
tion of discourse relations gained a lot of interest
in the research community. The PDTB includes
implicit as well as explicit discourse relations of
different types, and there are multiple approaches
aiming at automatically identifying implicit rela-
tions. Pitler et al. (2009) experiment with polarity
tags, verb classes, length of verb phrases, modal-
ity, context and lexical features and found that
word pairs with non-zero Information Gain yield
best results. Lin et al. (2009) show that beside
lexical features, production rules collected from
parse trees yield good results, whereas Louis et
al. (2010) found that features based on named-
entities do not perform as well as lexical features.
However, current approaches to discourse analy-
sis like the RST or the PDTB are designed to ana-
lyze general discourse structures, and thus include
a large set of generic discourse relations, whereas
only a subset of those relations is relevant for ar-
gumentative discourse analysis. For instance, the
argumentation scheme proposed by Peldszus and
Stede (2013) includes three argumentative rela-
tions (support, attack and counter-attack), whereas
Stab and Gurevych (2014) propose a scheme in-
cluding only two relations (support and attack).
The difference between argumentative relations
and those included in general tagsets like RST and
PDTB is best illustrated by the work of Biran and
Rambow (2011), which is to the best of our knowl-
edge the only work that focuses on the identifica-
tion of argumentative relations. They argue that
existing definitions of discourse relations are only
relevant as a building block for identifying argu-
mentative discourse and that existing approaches
do not contain a single relation that corresponds to
</bodyText>
<page confidence="0.995459">
48
</page>
<bodyText confidence="0.999932421052632">
a distinct argumentative relation. Therefore, they
consider a set of 12 discourse relations from the
RST Discourse Treebank (Carlson et al., 2001) as
a single argumentative relation in order to identify
justifications for a given claim. They first extract
a set of lexical indicators for each relation from
the RST Discourse Treebank and create a word
pair resource using the English Wikipedia. In their
experiments, they use the extracted word pairs as
features and obtain an F1-score of up to 0.51 using
two different corpora. Although the approach con-
siders non-adjacent relations, it is limited to the
identification of relations between premises and
claims and requires that claims are known in ad-
vance. In addition, the combination of several
general relations to a single argumentative relation
might lead to consistency problems and to noisy
corpora (e.g. not each instance of a contrast rela-
tion is relevant for argumentative discourse).
</bodyText>
<sectionHeader confidence="0.997777" genericHeader="method">
3 Data
</sectionHeader>
<bodyText confidence="0.99999432">
For our experiments, we use a corpus of per-
suasive essays compiled by Stab and Gurevych
(2014). This corpus contains annotations of ar-
gument components at the clause-level as well
as argumentative relations. In particular, it in-
cludes annotations of major claims, claims and
premises, which are connected with argumentative
support and attack relations. Argumentative rela-
tions are directed (there is a specified source and
target component of each relation) and can hold
between a premise and another premise, a premise
and a (major-) claim, or a claim and a major claim.
Except for the last one, an argumentative relation
does not cross paragraph boundaries.
Three raters annotated the corpus with an inter-
annotator agreement of αU = 0.72 (Krippendorff,
2004) for argument components and α = 0.81 for
argumentative relations. In total, the corpus com-
prises 90 essays including 1,673 sentences. Since
it only contains a low number of attack relations,
we focus in this work solely on the identification
of argument components and argumentative sup-
port relations. However, the proposed approach
can also be applied to identify attack relations in
future work.
</bodyText>
<sectionHeader confidence="0.994243" genericHeader="method">
4 Identifying Argument Components
</sectionHeader>
<bodyText confidence="0.992513638888889">
We consider the identification of argument com-
ponents as a multiclass classification task. Each
clause in the corpus is either classified as major
claim, claim, premise or non-argumentative. So
this task includes besides the classification of ar-
gument components also the separation of argu-
mentative and non-argumentative text units. We
label each sentence that does not contain an ar-
gument component as class ‘none’. Since many
argument components cover an entire sentence
(30%), this is not an exclusive feature of this class.
In total, the corpus contains 1,879 instances.
Table 1 shows the class distribution among the
instances. The corpus includes 90 major claims
(each essay contains exactly one), 429 claims and
1,033 premises. This proportion between claims
and premises is common in argumentation since
claims are usually supported by several premises
for establishing a stable standpoint.
Table 1: Class distribution among the instances.
The corpus contains 1552 argument components
and 327 non-argumentative instances.
For our experiments, we randomly split the data
into a 80% training set and a 20% test set with
the same class distribution and determine the best
performing system using 10-fold cross-validation
on the training set only. In our experiments, we
use several classifiers (see section 4.2) from the
Weka data mining software (Hall et al., 2009).
For preprocessing the corpus, we use the Stanford
POS-Tagger (Toutanova et al., 2003) and Parser
(Klein and Manning, 2003) included in the DKPro
Framework (Gurevych et al., 2007). After these
steps, we use the DKPro-TC text classification
framework (Daxenberger et al., 2014) for extract-
ing the features described in the following section.
</bodyText>
<subsectionHeader confidence="0.925467">
4.1 Features
</subsectionHeader>
<bodyText confidence="0.999899083333333">
Structural features: We define structural features
based on token statistics, the location and punc-
tuations of the argument component and its cov-
ering sentence. Since Biran and Rambow (2011)
found that premises are longer on the average than
other sentences, we add the number of tokens of
the argument component and its covering sentence
to our feature set. In addition, we define the num-
ber of tokens preceding and following an argument
component in the covering sentence, the token ra-
tio between covering sentence and argument com-
ponent, and a Boolean feature that indicates if the
</bodyText>
<figure confidence="0.988514125">
MajorClaim
Claim
Premise
None
90 (4.8%)
429 (22.8%)
1,033 (55%)
327 (17.4%)
</figure>
<page confidence="0.997439">
49
</page>
<bodyText confidence="0.999798405940594">
argument component covers all tokens of its cov-
ering sentence as token statistics features.
For exploiting the structural properties of per-
suasive essays, we define a set of location-based
features. First, we define four Boolean features
that indicate if the argument component is present
in the introduction or conclusion of an essay and
if it is present in the first or the last sentence of
a paragraph. Second, we add the position of the
covering sentence in the essay as a numeric fea-
ture. Since major claims are always present in the
introduction or conclusion of an essay and para-
graphs frequently begin or conclude with a claim,
we expect that these features are good indicators
for classifying (major-) claims.
Further, we define structural features based on
the punctuation: the number of punctuation marks
of the covering sentence and the argument compo-
nent, the punctuation marks preceding and follow-
ing an argument component in its covering sen-
tence and a Boolean feature that indicates if the
sentence closes with a question mark.
Lexical features: We define n-grams, verbs,
adverbs and modals as lexical features. We con-
sider all n-grams of length 1-3 as a Boolean feature
and extract them from the argument component in-
cluding preceding tokens in the sentence that are
not covered by another argument component. So,
the n-gram features include discourse markers that
indicate certain argument components but which
are not included in the actual annotation of argu-
ment components.
Verbs and adverbs play an important role for
identifying argument components. For instance,
certain verbs like ‘believe’, ‘think’ or ‘agree’ of-
ten signal stance expressions which indicate the
presence of a major claim and adverbs like ‘also’,
‘often’ or ‘really’ emphasize the importance of a
premise. We model both verbs and adverbs as
Boolean features.
Modal verbs like ‘should’ and ‘could’ are fre-
quently used in argumentative discourse to signal
the degree of certainty when expressing a claim.
We use the POS tags generated during preprocess-
ing to identify modals and define a Boolean fea-
ture which indicates if an argument component
contains a modal verb.
Syntactic features: To capture syntactic prop-
erties of argument components, we define features
extracted from parse trees. We adopt two features
proposed by (Mochales-Palau and Moens, 2009):
the number of sub-clauses included in the covering
sentence and the depth of the parse tree. In addi-
tion, we extract production rules from the parse
tree as proposed by Lin et al. (2009) to capture
syntactic characteristics of an argument compo-
nent. The production rules are collected for each
function tag (e.g. VP, NN, S, etc.) in the sub-
tree of an argument component. The feature set
includes e.g. rules like V P → VBG, NP or
PP → IN, NP. We model each production rule
as a Boolean feature and set it to true if it appears
in the subtree of an argument component.
Since premises often refer to previous events
and claims are usually in present tense, we capture
the tense of the main verb of an argument compo-
nent as proposed by Mochales-Palau and Moens
(2009) and define a feature that indicates if an ar-
gument component is in the past or present tense.
Indicators: Discourse markers often indicate
the components of an argument. For example,
claims are frequently introduced with ‘therefore’,
‘thus’ or ‘consequently’, whereas premises con-
tain markers like ‘because’, ‘reason’ or ‘further-
more’. We collected a list of discourse markers
from the Penn Discourse Treebank 2.0 Annotation
Manual (Prasad et al., 2007) and removed markers
that do not indicate argumentative discourse (e.g.
markers which indicate temporal discourse). In to-
tal, we collected 55 discourse markers and model
each as a Boolean feature set to true if the particu-
lar marker precedes the argumentative component.
In addition, we define five Boolean features
which denote a reference to the first person in the
covering sentence of an argument component: ‘I’,
‘me’, ‘my’, ‘mine’, and ‘myself’. An additional
Boolean feature indicates if one of them is present
in the covering sentence. We expect that those fea-
tures are good indicators of the major claim, since
it is often introduced with expressions referring to
the personal stance of the author.
Contextual features: The context plays a ma-
jor role for identifying argument components. For
instance, a premise can only be classified as such,
if there is a corresponding claim. Therefore, we
define the following features each extracted from
the sentence preceding and following the covering
sentence of an argument component: the number
of punctuations, the number of tokens, the number
of sub-clauses and a Boolean feature indicating the
presence of modal verbs.
</bodyText>
<page confidence="0.985203">
50
</page>
<sectionHeader confidence="0.52005" genericHeader="method">
4.2 Results and Analysis
</sectionHeader>
<bodyText confidence="0.999982944444444">
For identifying the best performing system, we
conducted several experiments on the training set
using stratified 10-fold cross-validation. We de-
termine the evaluation scores by accumulating the
confusion matrices of each fold into one confusion
matrix, since it is the less biased method for evalu-
ating cross-validation studies (Forman and Scholz,
2010). In a comparison of several classifiers (Sup-
port Vector Machine, Naive Bayes, C4.5 Decision
Tree and Random Forest), we found that each of
the classifiers significantly outperforms a majority
baseline (McNemar Test (McNemar, 1947) with
p = 0.05) and that a Support Vector Machine
(SVM) achieves the best results using 100 top fea-
tures ranked by Information Gain.3 It achieves an
accuracy of 77.3% on the test set and outperforms
the majority baseline with respect to overall accu-
racy as well as F1-score (table 2).
</bodyText>
<table confidence="0.999859555555556">
Baseline Human SVM
Accuracy 0.55 0.877 0.773
Macro F1 0.177 0.871 0.726
Macro Precision 0.137 0.864 0.773
Macro Recall 0.25 0.879 0.684
F1 MajorClaim 0 0.916 0.625
F1 Claim 0 0.841 0.538
F1 Premise 0.709 0.911 0.826
F1 None 0 0.812 0.884
</table>
<tableCaption confidence="0.992451">
Table 2: Results of an SVM for argument com-
</tableCaption>
<bodyText confidence="0.951830285714286">
ponent classification on the test set compared to a
majority baseline and human performance.
The upper bound for this task constitutes the
human performance which we determine by com-
paring each annotator to the gold standard. Since
the boundaries of an argument component in the
gold standard can differ from the boundaries iden-
tified by a human annotator (the annotation task
included the identification of argument component
boundaries), we label each argument component
of the gold standard with the class of the maximum
overlapping annotation of a human annotator for
determining the human performance. We obtain a
challenging upper bound of 87.7% (accuracy) by
averaging the scores of all three annotators on the
test set (table 2). So, our system achieves 88.1%
of human performance (accuracy).
Feature influence: In subsequent experiments,
we evaluate each of the defined feature groups on
the entire data set using 10-fold cross-validation to
3Although the Naive Bayes classifier achieves lowest ac-
curacy, it exhibits a slightly higher recall compared to SVM.
find out which features perform best for identify-
ing argument components. As assumed, structural
features perform well for distinguishing claims
and premises in persuasive essays. They also yield
high results for separating argumentative from
non-argumentative text units (table 3).
</bodyText>
<table confidence="0.9906265">
Feature group MajorClaim Claim Premise None
Structural 0.477 0.419 0.781 0.897
Lexical 0.317 0.401 0.753 0.275
Syntactic 0.094 0.292 0.654 0.427
Indicators 0.286 0.265 0.730 0
Contextual 0 0 0.709 0
</table>
<tableCaption confidence="0.777643">
Table 3: F1-scores for individual feature groups
and classes (SVM with 10-fold cross-validation on
the entire data set)
</tableCaption>
<bodyText confidence="0.999787571428571">
Interestingly, the defined indicators are not
useful for separating argumentative from non-
argumentative text units though they are helpful
for classifying argument components. A reason
for this could be that not each occurrence of an
indicator distinctly signals argument components,
since their sense is often ambiguous (Prasad et
al., 2008). For example ‘since’ indicates temporal
properties as well as justifications, whereas ‘be-
cause’ also indicates causal links. Syntactic fea-
tures also contribute to the identification of argu-
ment components. They achieve an F1-score of
0.292 for claims and 0.654 for premises and also
contribute to the separation of argumentative from
non-argumentative text units. Contextual features
do not perform well. However, they increase the
accuracy by 0.7% in combination with other fea-
tures. Nevertheless, this difference is not signifi-
cant (p = 0.05).
Error analysis: The system performs well for
separating argumentative and non-argumentative
text units as well as for identifying premises.
However, the identification of claims and major
claims yields lower performance. The confusion
matrix (table 4) reveals that the most common er-
ror is between claims and premises. In total, 193
claims are incorrectly classified as premise. In
a manual assessment, we observed that many of
these errors occur if the claim is present in the first
paragraph sentence and exhibits preceding indica-
tors like ‘~rst(ly)’ or ‘second(ly)’ which are also
frequently used to enumerate premises. In these
cases, the author introduces the claim of the argu-
ment as support for the major claim and thus its
characteristic is similar to a premise. To prevent
</bodyText>
<page confidence="0.996799">
51
</page>
<bodyText confidence="0.999359666666667">
on the training set. We use the same preprocessing
pipeline as described in section 4 and DKPro-TC
for extracting the features described below.
this type of error, it might help to define features
representing the location of indicators or to disam-
biguate the function of indicators.
</bodyText>
<table confidence="0.903558166666667">
Predicted
MC Cl Pr No
MC 38 34 18 0
Cl 19 210 193 7
Pr 6 104 904 19
No 0 12 23 292
</table>
<tableCaption confidence="0.997712">
Table 4: Confusion matrix (SVM) for argument
</tableCaption>
<bodyText confidence="0.918737">
component classification (MC = Major Claim; Cl
= Claim; Pr = Premise; No = None)
We also observed, that some of the misclassified
claims cover an entire sentence and don’t include
indicators. For example, it is even difficult for hu-
mans to classify the sentences ‘Competition helps
in improvement and evolution’ as a claim without
knowing the intention of the author. For prevent-
ing these errors, it might help to include more so-
phisticated contextual features.
</bodyText>
<sectionHeader confidence="0.990475" genericHeader="method">
5 Identifying Argumentative Relations
</sectionHeader>
<bodyText confidence="0.9998079375">
We consider the identification of argumentative re-
lations as a binary classification task of argument
component pairs and classify each pair as either
support or non-support. For identifying argumen-
tative relations, all possible combinations of argu-
ment components have to be tested. Since this re-
sults in a heavily skewed class distribution, we ex-
tract all possible combinations of argument com-
ponents from each paragraph of an essay.4 So, we
omit argumentative relations between claims and
major claims which are the only relations in the
corpus that cross paragraph boundaries, but ob-
tain a better distribution between true (support)
and false (non-support) instances. In total, we ob-
tain 6,330 pairs, of which 15.6% are support and
84.4% are non-support relations (table 5).
</bodyText>
<subsectionHeader confidence="0.747122">
Support Non-support
</subsectionHeader>
<bodyText confidence="0.716130285714286">
989 (15.6%) 5341 (84.4%)
Table 5: Class distribution of argument component
pairs
Equivalent to the identification of argument
components, we randomly split the data in a 80%
training and a 20% test set and determine the best
performing system using 10-fold cross-validation
</bodyText>
<footnote confidence="0.836875">
4Only 4.6% of 28,434 possible pairs are true instances
(support), if all combinations are considered.
</footnote>
<subsectionHeader confidence="0.82911">
5.1 Features
</subsectionHeader>
<bodyText confidence="0.985118936170213">
Structural features: We define structural fea-
tures for each pair based on the source and tar-
get components, and on the mutual information of
both. Three numeric features are based on token
statistics. Two features represent the number of
tokens of the source and target components and
the third one represents the absolute difference in
the number of tokens. Three additional numeric
features count the number of punctuation marks
of the source and target components as well as
the absolute difference between both. We extract
both types of features solely from the clause an-
notated as argument component and do not con-
sider the covering sentence. In addition, we de-
fine nine structural features based on the position
of both argument components: two of them repre-
sent the position of the covering sentences in the
essay, four Boolean features indicate if the argu-
ment components are present in the first or last
sentence of a paragraph, one Boolean feature for
representing if the target component occurs before
the source component, the sentence distance be-
tween the covering sentences, and a Boolean fea-
ture which indicates if both argument components
are in the same sentence.
Lexical features: We define lexical features
based on word pairs, first words and modals. It
has been shown in previous work that word pairs
are effective for identifying implicit discourse re-
lations (Marcu and Echihabi, 2002). We define
each pair of words between the source and target
components as a Boolean feature and investigate
word pairs containing stop words as well as stop
word filtered word pairs.
In addition, we adopt the first word features
proposed by Pitler et al. (2009). We extract the
first word either from the argument component
or from non-annotated tokens preceding the ar-
gument component in the covering sentence if
present. So, the first word of an argument com-
ponent is either the first word of the sentence con-
taining the argument component, the first word
following a preceding argument component in the
same sentence or the first word of the actual ar-
gument component if it commences the sentence
or directly follows another argument component.
Actual
</bodyText>
<page confidence="0.987389">
52
</page>
<bodyText confidence="0.999977457142857">
So, we ensure that the first word of an argument
component includes important discourse markers
which are not included in the annotation. We de-
fine each first word of the source and target com-
ponents as a Boolean feature and also add the pairs
of first words to our feature set.
Further, we define a Boolean feature for the
source as well as for the target component that
indicates if they contain a modal verb and a nu-
merical feature that counts the number of common
terms of the two argument components.
Syntactic features: For capturing syntactic
properties, we extract production rules from the
source and target components. Equivalent to the
features extracted for the argument component
classification (section 4.1), we model each rule as
a Boolean feature which is true if the correspond-
ing argument component includes the rule.
Indicators: We use the same list of discourse
markers introduced above (section 4.1) as indi-
cator features. For each indicator we define a
Boolean feature for the source as well as for the
target component of the pair and set it to true if
it is present in the argument component or in its
preceding tokens.
Predicted type: The argumentative type (major
claim, claim or premise) of the source and target
components is a strong indicator for identifying ar-
gumentative relations. For example, there are no
argumentative relations from claims to premises.
Thus, if the type of the argument component is
reliably identified many potential pairs can be ex-
cluded. Therefore, we define two features that rep-
resent the argumentative type of the source and tar-
get components identified in the first experiment.
</bodyText>
<subsectionHeader confidence="0.988013">
5.2 Results and Analysis
</subsectionHeader>
<bodyText confidence="0.999942185185185">
The comparison of several classifiers reveals that
an SVM achieves the best results. In our exper-
iments, all classifiers except the C4.5 Decision
Tree significantly outperform a majority baseline
which classifies all pairs as non-support (p =
0.05). We also conducted several experiments
using word pair features only and found in con-
trast to Pitler et al. (2009) that limiting the num-
ber of word pairs decreases the performance. In
particular, we compared the top 100, 250, 500,
1000, 2500, 5000 word pairs ranked by Informa-
tion Gain, non-zero Information Gain word pairs
and non-filtered word pairs. The results show
that non-filtered word pairs perform best (macro
F1-score of 0.68). Our experiments also reveal
that filtering stop words containing word pairs de-
creases the macro F1-score to 0.60. We obtain the
best results using an SVM without any feature se-
lection method. Due to the class imbalance, the
SVM only slightly outperforms the accuracy of a
majority baseline on the test set (table 6). How-
ever, the macro F1-score is more appropriate for
evaluating the performance if the data is imbal-
anced since it assigns equal weight to the classes
and not to the instances. The SVM achieves a
macro F1-score of 0.722 and also outperforms the
baseline with respect to the majority class.
</bodyText>
<table confidence="0.999692285714286">
Baseline Human SVM
Accuracy 0.843 0.954 0.863
Macro F1 0.458 0.908 0.722
Macro Precision 0.422 0.937 0.739
Macro Recall 0.5 0.881 0.705
F1 Support 0 0.838 0.519
F1 Non-Support 0.915 0.973 0.92
</table>
<tableCaption confidence="0.987583">
Table 6: Results of an SVM for classifying argu-
</tableCaption>
<bodyText confidence="0.984529586206897">
mentative relations on the test set compared to a
majority baseline and human performance.
We determined the upper bound constituted by
the human performance by comparing the annota-
tions of all three annotators to the gold standard.
The scores in table 6 are the average scores of all
three annotators. Our system achieves 90.5% of
human performance (accuracy).
Feature influence: A comparison of the de-
fined feature groups using 10-fold cross-validation
on the entire data set shows that lexical features
perform best. They achieve an F1-score of 0.427
for support and 0.911 for non-support pairs (ta-
ble 7). The syntactic features also perform well
followed by the indicators. It turned out that struc-
tural features are not effective for identifying argu-
mentative relations though they are the most effec-
tive features for identifying argument components
(cp. section 4.2). However, when omitted from
the entire feature set the performance significantly
decreases by 0.018 macro F1-score (p = 0.05).
Interestingly, the predicted types from our first
experiment are not effective at all. Although the
argumentative type of the target component ex-
hibits the highest Information Gain in each fold
compared to all other features, the predicted type
does not yield a significant difference when com-
bined with all other features (p = 0.05). It only
improves the macro F1-score by 0.001 when in-
</bodyText>
<page confidence="0.997559">
53
</page>
<bodyText confidence="0.607609">
cluded in the entire feature set.
</bodyText>
<table confidence="0.9984215">
Feature group Support Non-Support
Structural 0 0.915
Lexical 0.427 0.911
Syntactic 0.305 0.911
Indicators 0.159 0.916
Predicted types 0 0.915
</table>
<tableCaption confidence="0.961959">
Table 7: F1-scores for individual feature groups
using an SVM and the entire data set
</tableCaption>
<bodyText confidence="0.99912852173913">
Error analysis: For identifying frequent er-
ror patterns, we manually investigated the mis-
takes of the classifier. Although our system identi-
fies 97.5% of the non-support pairs from claim to
premise correctly, there are still some false posi-
tives that could be prevented if the argument com-
ponents had been classified more accurately. For
instance, there are 18 non-support relations from
claim to another claim, 32 from claim to premise,
5 from major claim to premise and 4 from major
claim to claim among the false positives. How-
ever, the larger amount of errors is due to not iden-
tified support relations (false negatives). We found
that some errors might be related to missing con-
textual information and unresolved coreferences.
For instance, it might help to replace ‘It’ with ‘Ex-
ercising’ for classifying the pair ‘It helps relieve
tension and stress’ → ‘Exercising improves self-
esteem and confidence’ as support relation or to in-
clude contextual information for the premise ‘This
can have detrimental effects on health’ support-
ing the claim ‘There are some serious problems
springing from modern technology’.
</bodyText>
<sectionHeader confidence="0.999795" genericHeader="method">
6 Discussion
</sectionHeader>
<bodyText confidence="0.999994557692308">
In our experiments, we have investigated the clas-
sification of argument components as well as the
identification of argumentative relations for recog-
nizing argumentative discourse structures in per-
suasive essays. Both tasks are closely related and
we assume that sharing mutual information be-
tween both tasks might be a promising direction
for future research. On the one hand, knowing the
type of argument components is a strong indica-
tor for identifying argumentative relations and on
the other hand, it is likely that information about
the argumentative structure facilitates the identi-
fication of argument components. However, our
experiments revealed that the current accuracy for
identifying argument components is not sufficient
for increasing the performance of argumentative
relation identification. Nevertheless, we obtain
almost human performance when including the
types of argument components of the gold stan-
dard (macro F1-score &gt;0.85) in our argument re-
lation identification experiment and when includ-
ing the number of incoming and outgoing support
relations for each argument component in our first
experiment (macro F1-score &gt;0.9). Therefore, it
can be assumed, that if the identification of argu-
ment components can be improved, the identifica-
tion of argumentative relations will achieve better
results and vice versa.
The results also show that the distinction be-
tween claims and premises is the major challenge
for identifying argument components. It turned
out that structural features are the most effective
ones for this task. However, some of those features
are unique to persuasive essays, and it is an open
question if there are general structural properties
of arguments which can be exploited for separat-
ing claims from premises.
Our experiments show that discourse markers
yield only low accuracies. Using only our defined
indicator features, we obtain an F1-score of 0.265
for identifying claims, whereas Mochales-Palau
and Moens (2009) achieve 0.673 for the same task
in legal documents using a CFG. This confirms our
initial assumption that approaches relying on dis-
course markers are not applicable for identifying
argumentative discourse structures in documents
which do not follow a standardized form. In ad-
dition, it shows that discourse markers are either
frequently missing or misleadingly used in student
texts and that there is a need for argumentative
writing support systems that assist students in em-
ploying discourse markers correctly.
</bodyText>
<sectionHeader confidence="0.991261" genericHeader="conclusions">
7 Conclusion and Future Work
</sectionHeader>
<bodyText confidence="0.999760692307692">
We presented a novel approach for identifying ar-
gumentative discourse structures in persuasive es-
says. Previous approaches on argument recog-
nition suffer from several limitations: Existing
approaches focus either solely on the identifica-
tion of argument components or rely on manu-
ally created rules which are not able to identify
implicit argumentative discourse structures. Our
approach is the first step towards computational
argument analysis in the educational domain and
enables the identification of implicit argumenta-
tive discourse structures. The presented approach
achieves 88.1% of human performance for identi-
</bodyText>
<page confidence="0.993939">
54
</page>
<bodyText confidence="0.999696666666667">
fying argument components and 90.5% for identi-
fying argumentative relations.
For future work, we plan to extend our stud-
ies to larger corpora, to integrate our classifiers in
writing environments, and to investigate their ef-
fectiveness for supporting students.
</bodyText>
<sectionHeader confidence="0.990279" genericHeader="acknowledgments">
Acknowledgements
</sectionHeader>
<bodyText confidence="0.9997755">
This work has been supported by the Volk-
swagen Foundation as part of the Lichtenberg-
Professorship Program under grant No. I/82806.
We thank Krish Perumal and Piyush Paliwal for
their valuable contributions and we thank the
anonymous reviewers for their helpful comments.
</bodyText>
<sectionHeader confidence="0.998673" genericHeader="references">
References
</sectionHeader>
<reference confidence="0.999227580645161">
Or Biran and Owen Rambow. 2011. Identifying jus-
tifications in written dialogs by classifying text as
argumentative. International Journal of Semantic
Computing, 05(04):363–381.
M. Anne Britt and Aaron A. Larson. 2003. Construct-
ing representations of arguments. Journal of Mem-
ory and Language, 48(4):794 – 810.
Lynn Carlson, Daniel Marcu, and Mary Ellen
Okurowski. 2001. Building a discourse-tagged cor-
pus in the framework of rhetorical structure theory.
In Proceedings of the Second SIGdial Workshop on
Discourse and Dialogue - Volume 16, SIGDIAL ’01,
pages 1–10, Aalborg, Denmark.
Johannes Daxenberger, Oliver Ferschke, Iryna
Gurevych, and Torsten Zesch. 2014. DKPro TC:
A Java-based framework for supervised learning
experiments on textual data. In Proceedings of
the 52nd Annual Meeting of the Association for
Computational Linguistics. System Demonstrations,
pages 61–66, Baltimore, MD, USA.
Vanessa Wei Feng and Graeme Hirst. 2011. Classi-
fying arguments by scheme. In Proceedings of the
49th Annual Meeting of the Association for Com-
putational Linguistics: Human Language Technolo-
gies - Volume 1, HLT ’11, pages 987–996, Portland,
OR, USA.
Eirini Florou, Stasinos Konstantopoulos, Antonis
Koukourikos, and Pythagoras Karampiperis. 2013.
Argument extraction for supporting public policy
formulation. In Proceedings of the 7th Workshop
on Language Technology for Cultural Heritage, So-
cial Sciences, and Humanities, pages 49–54, Sofia,
Bulgaria.
George Forman and Martin Scholz. 2010. Apples-to-
apples in cross-validation studies: Pitfalls in clas-
sifier performance measurement. SIGKDD Explor.
Newsl., 12(1):49–57.
Iryna Gurevych, Max M¨uhlh¨auser, Christof Mueller,
Juergen Steimle, Markus Weimer, and Torsten
Zesch. 2007. Darmstadt Knowledge Processing
Repository based on UIMA. In Proceedings of the
First Workshop on Unstructured Information Man-
agement Architecture at Biannual Conference of
the Society for Computational Linguistics and Lan-
guage Technology, Tuebingen, Germany.
Mark Hall, Eibe Frank, Geoffrey Holmes, Bernhard
Pfahringer, Peter Reutemann, and Ian H. Witten.
2009. The weka data mining software: An update.
SIGKDDExplor. Newsl., 11(1):10–18.
Dan Klein and Christopher D. Manning. 2003. Ac-
curate unlexicalized parsing. In Proceedings of the
41st Annual Meeting on Association for Computa-
tional Linguistics - Volume 1, ACL ’03, pages 423–
430, Sapporo, Japan.
Klaus Krippendorff. 2004. Measuring the Reliability
of Qualitative Text Analysis Data. Quality &amp; Quan-
tity, 38(6):787–800.
Ziheng Lin, Min-Yen Kan, and Hwee Tou Ng. 2009.
Recognizing implicit discourse relations in the Penn
Discourse Treebank. In Proceedings of the 2009
Conference on Empirical Methods in Natural Lan-
guage Processing: Volume 1, EMNLP ’09, pages
343–351, Stroudsburg, PA, USA.
Annie Louis, Aravind Joshi, Rashmi Prasad, and Ani
Nenkova. 2010. Using entity features to classify
implicit discourse relations. In Proceedings of the
11th Annual Meeting of the Special Interest Group
on Discourse and Dialogue, SIGDIAL ’10, pages
59–62, Stroudsburg, PA, USA.
William C. Mann and Sandra A. Thompson. 1987.
Rhetorical structure theory: A theory of text orga-
nization. Technical Report ISI/RS-87-190, Informa-
tion Sciences Institute.
Daniel Marcu and Abdessamad Echihabi. 2002. An
unsupervised approach to recognizing discourse re-
lations. In Proceedings of the 40th Annual Meeting
on Association for Computational Linguistics, ACL
’02, pages 368–375.
Quinn McNemar. 1947. Note on the sampling error
of the difference between correlated proportions or
percentages. Psychometrika, 12(2):153–157.
Raquel Mochales-Palau and Marie-Francine Moens.
2009. Argumentation mining: The detection, classi-
fication and structure of arguments in text. In Pro-
ceedings of the 12th International Conference on Ar-
tificial Intelligence and Law, ICAIL ’09, pages 98–
107, New York, NY, USA. ACM.
Marie-Francine Moens, Erik Boiy, Raquel Mochales
Palau, and Chris Reed. 2007. Automatic detection
of arguments in legal texts. In Proceedings of the
11th International Conference on Artificial Intelli-
gence and Law, ICAIL ’07, pages 225–230, Stan-
ford, California.
</reference>
<page confidence="0.978149">
55
</page>
<reference confidence="0.999705079365079">
Andreas Peldszus and Manfred Stede. 2013. From
Argument Diagrams to Argumentation Mining in
Texts: A Survey. International Journal of Cogni-
tive Informatics and Natural Intelligence (IJCINI),
7(1):1–31.
Emily Pitler, Annie Louis, and Ani Nenkova. 2009.
Automatic sense prediction for implicit discourse re-
lations in text. In Proceedings of the Joint Confer-
ence of the 47th Annual Meeting of the ACL and the
4th International Joint Conference on Natural Lan-
guage Processing of the AFNLP, ACL ’09, pages
683–691, Suntec, Singapore. Association for Com-
putational Linguistics.
Rashmi Prasad, Eleni Miltsakaki, Nikhil Dinesh, Alan
Lee, Aravind Joshi, Livio Robaldo, and Bonnie L.
Webber. 2007. The Penn Discourse Treebank 2.0
annotation manual. Technical report, Institute for
Research in Cognitive Science, University of Penn-
sylvania.
Rashmi Prasad, Nikhil Dinesh, Alan Lee, Eleni Milt-
sakaki, Livio Robaldo, Aravind Joshi, and Bonnie
Webber. 2008. The Penn Discourse Treebank 2.0.
In Proceedings of the Sixth International Conference
on Language Resources and Evaluation (LREC’08),
Marrakech, Morocco.
Chris Reed, Raquel Mochales-Palau, Glenn Rowe, and
Marie-Francine Moens. 2008. Language resources
for studying argument. In Proceedings of the Sixth
International Conference on Language Resources
and Evaluation, LREC ’08, pages 2613–2618, Mar-
rakech, Morocco.
Niall Rooney, Hui Wang, and Fiona Browne. 2012.
Applying kernel methods to argumentation min-
ing. In Proceedings of the Twenty-Fifth Interna-
tional Florida Artificial Intelligence Research So-
ciety Conference, FLAIRS ’12, pages 272–275,
Marco Island, FL, USA.
Alan Sergeant. 2013. Automatic argumentation ex-
traction. In Proceedings of the 10th European Se-
mantic Web Conference, ESWC ’13, pages 656–660,
Montpellier, France.
Christian Stab and Iryna Gurevych. 2014. Annotat-
ing argument components and relations in persua-
sive essays. In Proceedings of the 25th International
Conference on Computational Linguistics (COLING
2014), pages 1501–1510, Dublin, Ireland, August.
Simone Teufel. 1999. Argumentative Zoning: Infor-
mation Extraction from Scientific Text. Ph.D. thesis,
University of Edinburgh.
Kristina Toutanova, Dan Klein, Christopher D. Man-
ning, and Yoram Singer. 2003. Feature-rich part-of-
speech tagging with a cyclic dependency network.
In Proceedings of the 2003 Conference of the North
American Chapter of the Association for Computa-
tional Linguistics on Human Language Technology,
NAACL ’03, pages 173–180, Edmonton, Canada.
Douglas N Walton. 1996. Argumentation schemes for
presumptive reasoning. Routledge.
Adam Wyner, Raquel Mochales Palau, Marie-Francine
Moens, and David Milward. 2010. Approaches to
text mining arguments from legal cases. In Semantic
Processing of Legal Texts, volume 6036 of Lecture
Notes in Computer Science, pages 60–79.
</reference>
<page confidence="0.998412">
56
</page>
</variant>
</algorithm>
<algorithm name="ParsHed" version="110505">
<variant no="0" confidence="0.363494">
<title confidence="0.736995">Identifying Argumentative Discourse Structures in Persuasive Essays Iryna</title>
<author confidence="0.472317">Knowledge Processing Lab</author>
<affiliation confidence="0.998244666666667">Department of Computer Science, Technische Universit¨at Knowledge Processing Lab German Institute for Educational</affiliation>
<email confidence="0.98284">www.ukp.tu-darmstadt.de</email>
<abstract confidence="0.997920857142857">In this paper, we present a novel approach for identifying argumentative discourse structures in persuasive essays. The structure of argumentation consists of several components (i.e. claims and premises) that are connected with argumentative relations. We consider this task in two consecutive steps. First, we identify the components of arguments using multiclass classification. Second, we classify a pair of argument components as either support or non-support for identifying the structure of argumentative discourse. For both tasks, we evaluate several classifiers and propose novel feature sets including structural, lexical, syntactic and contextual features. In our experiments, we obtain a F1-score of for identifying components and for argumentative relations.</abstract>
</variant>
</algorithm>
<algorithm name="ParsCit" version="110505">
<citationList>
<citation valid="true">
<authors>
<author>Or Biran</author>
<author>Owen Rambow</author>
</authors>
<title>Identifying justifications in written dialogs by classifying text as argumentative.</title>
<date>2011</date>
<journal>International Journal of Semantic Computing,</journal>
<volume>05</volume>
<issue>04</issue>
<contexts>
<context position="13395" citStr="Biran and Rambow (2011)" startWordPosition="2028" endWordPosition="2031"> are designed to analyze general discourse structures, and thus include a large set of generic discourse relations, whereas only a subset of those relations is relevant for argumentative discourse analysis. For instance, the argumentation scheme proposed by Peldszus and Stede (2013) includes three argumentative relations (support, attack and counter-attack), whereas Stab and Gurevych (2014) propose a scheme including only two relations (support and attack). The difference between argumentative relations and those included in general tagsets like RST and PDTB is best illustrated by the work of Biran and Rambow (2011), which is to the best of our knowledge the only work that focuses on the identification of argumentative relations. They argue that existing definitions of discourse relations are only relevant as a building block for identifying argumentative discourse and that existing approaches do not contain a single relation that corresponds to 48 a distinct argumentative relation. Therefore, they consider a set of 12 discourse relations from the RST Discourse Treebank (Carlson et al., 2001) as a single argumentative relation in order to identify justifications for a given claim. They first extract a se</context>
<context position="17800" citStr="Biran and Rambow (2011)" startWordPosition="2716" endWordPosition="2719">fiers (see section 4.2) from the Weka data mining software (Hall et al., 2009). For preprocessing the corpus, we use the Stanford POS-Tagger (Toutanova et al., 2003) and Parser (Klein and Manning, 2003) included in the DKPro Framework (Gurevych et al., 2007). After these steps, we use the DKPro-TC text classification framework (Daxenberger et al., 2014) for extracting the features described in the following section. 4.1 Features Structural features: We define structural features based on token statistics, the location and punctuations of the argument component and its covering sentence. Since Biran and Rambow (2011) found that premises are longer on the average than other sentences, we add the number of tokens of the argument component and its covering sentence to our feature set. In addition, we define the number of tokens preceding and following an argument component in the covering sentence, the token ratio between covering sentence and argument component, and a Boolean feature that indicates if the MajorClaim Claim Premise None 90 (4.8%) 429 (22.8%) 1,033 (55%) 327 (17.4%) 49 argument component covers all tokens of its covering sentence as token statistics features. For exploiting the structural prop</context>
</contexts>
<marker>Biran, Rambow, 2011</marker>
<rawString>Or Biran and Owen Rambow. 2011. Identifying justifications in written dialogs by classifying text as argumentative. International Journal of Semantic Computing, 05(04):363–381.</rawString>
</citation>
<citation valid="true">
<authors>
<author>M Anne Britt</author>
<author>Aaron A Larson</author>
</authors>
<title>Constructing representations of arguments.</title>
<date>2003</date>
<journal>Journal of Memory and Language,</journal>
<volume>48</volume>
<issue>4</issue>
<pages>810</pages>
<contexts>
<context position="5694" citStr="Britt and Larson, 2003" startWordPosition="852" endWordPosition="856">in particular for providing feedback about argumentation. First, argumentative discourse structures are essential for evaluating the quality of an argument, since it is not possible to examine how well a claim is justified without knowing which premises belong to it. Second, methods that recognize if a statement supports a given claim enable the collection of additional evidence from other sources. Third, the structure of argumentation is needed for recommending better arrangements of argument components and meaningful usage of discourse markers. Both foster argument comprehension and recall (Britt and Larson, 2003) and thus increase the argumentation quality. To the best of our knowledge, there is currently only one approach that aims at identifying argumentative discourse structures proposed by Mochales-Palau and Moens (2009). However, it relies on a manually created context-free grammar (CFG) and is tailored to the legal domain, which follows a standardized argumentation style. Therefore, it is likely that it will not achieve acceptable accuracy when applied to more general texts in which discourse markers are missing or even misleadingly used (e.g. student texts). In this work, we present a novel app</context>
</contexts>
<marker>Britt, Larson, 2003</marker>
<rawString>M. Anne Britt and Aaron A. Larson. 2003. Constructing representations of arguments. Journal of Memory and Language, 48(4):794 – 810.</rawString>
</citation>
<citation valid="true">
<authors>
<author>Lynn Carlson</author>
<author>Daniel Marcu</author>
<author>Mary Ellen Okurowski</author>
</authors>
<title>Building a discourse-tagged corpus in the framework of rhetorical structure theory.</title>
<date>2001</date>
<booktitle>In Proceedings of the Second SIGdial Workshop on Discourse and Dialogue - Volume 16, SIGDIAL ’01,</booktitle>
<pages>1--10</pages>
<location>Aalborg, Denmark.</location>
<contexts>
<context position="3582" citStr="Carlson et al., 2001" startWordPosition="538" endWordPosition="541"> support the claim (1) whereas premise (4) is a support for premise (3). Thus, this example includes three argumentative support relations holding between the components (2,1), (3,1) and (4,3) signaling that the source component is a justification of the target component. This illustrates two important properties of argumentative discourse structures. First, argumentative relations are often implicit (not indicated by discourse markers; e.g. the relation holding between (2) and (1)). Indeed, Marcu and Echihabi (2002) found that only 26% of the evidence relations in the RST Discourse Treebank (Carlson et al., 2001) include discourse markers. 1We use the term claim synonymously to conclusion. In our definition the differentiation between claims and premises does not indicate the validity of the statements but signals which components include the gist of an argument and which are given by the author as justification. 46 Proceedings of the 2014 Conference on Empirical Methods in Natural Language Processing (EMNLP), pages 46–56, October 25-29, 2014, Doha, Qatar. c�2014 Association for Computational Linguistics Second, in contrast to Rhetorical Structure Theory (RST) (Mann and Thompson, 1987), argumentative </context>
<context position="13881" citStr="Carlson et al., 2001" startWordPosition="2105" endWordPosition="2108">umentative relations and those included in general tagsets like RST and PDTB is best illustrated by the work of Biran and Rambow (2011), which is to the best of our knowledge the only work that focuses on the identification of argumentative relations. They argue that existing definitions of discourse relations are only relevant as a building block for identifying argumentative discourse and that existing approaches do not contain a single relation that corresponds to 48 a distinct argumentative relation. Therefore, they consider a set of 12 discourse relations from the RST Discourse Treebank (Carlson et al., 2001) as a single argumentative relation in order to identify justifications for a given claim. They first extract a set of lexical indicators for each relation from the RST Discourse Treebank and create a word pair resource using the English Wikipedia. In their experiments, they use the extracted word pairs as features and obtain an F1-score of up to 0.51 using two different corpora. Although the approach considers non-adjacent relations, it is limited to the identification of relations between premises and claims and requires that claims are known in advance. In addition, the combination of sever</context>
</contexts>
<marker>Carlson, Marcu, Okurowski, 2001</marker>
<rawString>Lynn Carlson, Daniel Marcu, and Mary Ellen Okurowski. 2001. Building a discourse-tagged corpus in the framework of rhetorical structure theory. In Proceedings of the Second SIGdial Workshop on Discourse and Dialogue - Volume 16, SIGDIAL ’01, pages 1–10, Aalborg, Denmark.</rawString>
</citation>
<citation valid="true">
<authors>
<author>Johannes Daxenberger</author>
<author>Oliver Ferschke</author>
<author>Iryna Gurevych</author>
<author>Torsten Zesch</author>
</authors>
<title>DKPro TC: A Java-based framework for supervised learning experiments on textual data.</title>
<date>2014</date>
<booktitle>In Proceedings of the 52nd Annual Meeting of the Association for Computational Linguistics. System Demonstrations,</booktitle>
<pages>61--66</pages>
<location>Baltimore, MD, USA.</location>
<contexts>
<context position="17532" citStr="Daxenberger et al., 2014" startWordPosition="2675" endWordPosition="2678">ances. For our experiments, we randomly split the data into a 80% training set and a 20% test set with the same class distribution and determine the best performing system using 10-fold cross-validation on the training set only. In our experiments, we use several classifiers (see section 4.2) from the Weka data mining software (Hall et al., 2009). For preprocessing the corpus, we use the Stanford POS-Tagger (Toutanova et al., 2003) and Parser (Klein and Manning, 2003) included in the DKPro Framework (Gurevych et al., 2007). After these steps, we use the DKPro-TC text classification framework (Daxenberger et al., 2014) for extracting the features described in the following section. 4.1 Features Structural features: We define structural features based on token statistics, the location and punctuations of the argument component and its covering sentence. Since Biran and Rambow (2011) found that premises are longer on the average than other sentences, we add the number of tokens of the argument component and its covering sentence to our feature set. In addition, we define the number of tokens preceding and following an argument component in the covering sentence, the token ratio between covering sentence and a</context>
</contexts>
<marker>Daxenberger, Ferschke, Gurevych, Zesch, 2014</marker>
<rawString>Johannes Daxenberger, Oliver Ferschke, Iryna Gurevych, and Torsten Zesch. 2014. DKPro TC: A Java-based framework for supervised learning experiments on textual data. In Proceedings of the 52nd Annual Meeting of the Association for Computational Linguistics. System Demonstrations, pages 61–66, Baltimore, MD, USA.</rawString>
</citation>
<citation valid="true">
<authors>
<author>Vanessa Wei Feng</author>
<author>Graeme Hirst</author>
</authors>
<title>Classifying arguments by scheme.</title>
<date>2011</date>
<booktitle>In Proceedings of the 49th Annual Meeting of the Association for Computational Linguistics: Human Language Technologies - Volume 1, HLT ’11,</booktitle>
<pages>987--996</pages>
<location>Portland, OR, USA.</location>
<contexts>
<context position="7571" citStr="Feng and Hirst, 2011" startWordPosition="1129" endWordPosition="1132">ingly used. Second, we present two novel feature sets for identifying argument components as well as argumentative relations. Third, we evaluate several classifiers and feature groups for identifying the best system for both tasks. 2 Related Work 2.1 Argumentation Mining Previous research on argumentation mining spans several subtasks, including (1) the separation of argumentative from non-argumentative text units (Moens et al., 2007; Florou et al., 2013), (2) the classification of argument components or argumentation schemes (Rooney et al., 2012; Mochales-Palau and Moens, 2009; Teufel, 1999; Feng and Hirst, 2011), and (3) the identification of argumentation structures (Mochales-Palau and Moens, 2009; Wyner et al., 2010). The separation of argumentative from nonargumentative text units is usually considered as a binary classification task and constitutes one of the first steps in an argumentation mining pipeline. Moens et al. (2007) propose an approach for identifying argumentative sentences in the Araucaria corpus (Reed et al., 2008). The argument annotations in Araucaria are based on a domainindependent argumentation theory proposed by Walton (1996). In their experiments, they obtain the best accurac</context>
<context position="9441" citStr="Feng and Hirst (2011)" startWordPosition="1430" endWordPosition="1433"> components extracted from a scientific article provide a good summary of its content. Each sentence is classified as one of seven rhetorical roles including claim, result or purpose. The approach obtained an F1-score of 0.462 using structural, lexical and syntactic features. Rooney et al. (2013) also focus on the identification of argument components but in contrast to the work of Teufel (1999) their scheme is not tailored to a particular genre. In their experiments, they identify claims, premises and non-argumentative text units in the Araucaria corpus and report an overall accuracy of 65%. Feng and Hirst (2011) also use the Araucaria corpus for their experiments but focus on the identification of argumentation schemes (Walton, 1996), which are templates for forms of arguments (e.g. argument from example or argument from consequence). Since their approach is based on features extracted from mutual information of claims and premises, it requires that the argument components are reliably identified in advance. In their experiments, they achieve an accuracy between 62.9% and 97.9% depending on the particular scheme and the classification setup. In contrast to all approaches mentioned above, the work pre</context>
</contexts>
<marker>Feng, Hirst, 2011</marker>
<rawString>Vanessa Wei Feng and Graeme Hirst. 2011. Classifying arguments by scheme. In Proceedings of the 49th Annual Meeting of the Association for Computational Linguistics: Human Language Technologies - Volume 1, HLT ’11, pages 987–996, Portland, OR, USA.</rawString>
</citation>
<citation valid="true">
<authors>
<author>Eirini Florou</author>
<author>Stasinos Konstantopoulos</author>
<author>Antonis Koukourikos</author>
<author>Pythagoras Karampiperis</author>
</authors>
<title>Argument extraction for supporting public policy formulation.</title>
<date>2013</date>
<booktitle>In Proceedings of the 7th Workshop on Language Technology for Cultural Heritage, Social Sciences, and Humanities,</booktitle>
<pages>49--54</pages>
<location>Sofia, Bulgaria.</location>
<contexts>
<context position="7409" citStr="Florou et al., 2013" startWordPosition="1106" endWordPosition="1109">ures. Contrary to previous approaches, our approach is capable of identifying argumentative discourse structures even if discourse markers are missing or misleadingly used. Second, we present two novel feature sets for identifying argument components as well as argumentative relations. Third, we evaluate several classifiers and feature groups for identifying the best system for both tasks. 2 Related Work 2.1 Argumentation Mining Previous research on argumentation mining spans several subtasks, including (1) the separation of argumentative from non-argumentative text units (Moens et al., 2007; Florou et al., 2013), (2) the classification of argument components or argumentation schemes (Rooney et al., 2012; Mochales-Palau and Moens, 2009; Teufel, 1999; Feng and Hirst, 2011), and (3) the identification of argumentation structures (Mochales-Palau and Moens, 2009; Wyner et al., 2010). The separation of argumentative from nonargumentative text units is usually considered as a binary classification task and constitutes one of the first steps in an argumentation mining pipeline. Moens et al. (2007) propose an approach for identifying argumentative sentences in the Araucaria corpus (Reed et al., 2008). The arg</context>
</contexts>
<marker>Florou, Konstantopoulos, Koukourikos, Karampiperis, 2013</marker>
<rawString>Eirini Florou, Stasinos Konstantopoulos, Antonis Koukourikos, and Pythagoras Karampiperis. 2013. Argument extraction for supporting public policy formulation. In Proceedings of the 7th Workshop on Language Technology for Cultural Heritage, Social Sciences, and Humanities, pages 49–54, Sofia, Bulgaria.</rawString>
</citation>
<citation valid="true">
<authors>
<author>George Forman</author>
<author>Martin Scholz</author>
</authors>
<title>Apples-toapples in cross-validation studies: Pitfalls in classifier performance measurement.</title>
<date>2010</date>
<journal>SIGKDD Explor. Newsl.,</journal>
<volume>12</volume>
<issue>1</issue>
<contexts>
<context position="23395" citStr="Forman and Scholz, 2010" startWordPosition="3624" endWordPosition="3627">ach extracted from the sentence preceding and following the covering sentence of an argument component: the number of punctuations, the number of tokens, the number of sub-clauses and a Boolean feature indicating the presence of modal verbs. 50 4.2 Results and Analysis For identifying the best performing system, we conducted several experiments on the training set using stratified 10-fold cross-validation. We determine the evaluation scores by accumulating the confusion matrices of each fold into one confusion matrix, since it is the less biased method for evaluating cross-validation studies (Forman and Scholz, 2010). In a comparison of several classifiers (Support Vector Machine, Naive Bayes, C4.5 Decision Tree and Random Forest), we found that each of the classifiers significantly outperforms a majority baseline (McNemar Test (McNemar, 1947) with p = 0.05) and that a Support Vector Machine (SVM) achieves the best results using 100 top features ranked by Information Gain.3 It achieves an accuracy of 77.3% on the test set and outperforms the majority baseline with respect to overall accuracy as well as F1-score (table 2). Baseline Human SVM Accuracy 0.55 0.877 0.773 Macro F1 0.177 0.871 0.726 Macro Precis</context>
</contexts>
<marker>Forman, Scholz, 2010</marker>
<rawString>George Forman and Martin Scholz. 2010. Apples-toapples in cross-validation studies: Pitfalls in classifier performance measurement. SIGKDD Explor. Newsl., 12(1):49–57.</rawString>
</citation>
<citation valid="true">
<authors>
<author>Iryna Gurevych</author>
<author>Max M¨uhlh¨auser</author>
<author>Christof Mueller</author>
<author>Juergen Steimle</author>
<author>Markus Weimer</author>
<author>Torsten Zesch</author>
</authors>
<title>Darmstadt Knowledge Processing Repository based on UIMA.</title>
<date>2007</date>
<booktitle>In Proceedings of the First Workshop on Unstructured Information Management Architecture at Biannual Conference of the Society for Computational Linguistics and Language Technology,</booktitle>
<location>Tuebingen, Germany.</location>
<marker>Gurevych, M¨uhlh¨auser, Mueller, Steimle, Weimer, Zesch, 2007</marker>
<rawString>Iryna Gurevych, Max M¨uhlh¨auser, Christof Mueller, Juergen Steimle, Markus Weimer, and Torsten Zesch. 2007. Darmstadt Knowledge Processing Repository based on UIMA. In Proceedings of the First Workshop on Unstructured Information Management Architecture at Biannual Conference of the Society for Computational Linguistics and Language Technology, Tuebingen, Germany.</rawString>
</citation>
<citation valid="true">
<authors>
<author>Mark Hall</author>
<author>Eibe Frank</author>
<author>Geoffrey Holmes</author>
<author>Bernhard Pfahringer</author>
<author>Peter Reutemann</author>
<author>Ian H Witten</author>
</authors>
<title>The weka data mining software: An update.</title>
<date>2009</date>
<journal>SIGKDDExplor. Newsl.,</journal>
<volume>11</volume>
<issue>1</issue>
<contexts>
<context position="17255" citStr="Hall et al., 2009" startWordPosition="2633" endWordPosition="2636">tween claims and premises is common in argumentation since claims are usually supported by several premises for establishing a stable standpoint. Table 1: Class distribution among the instances. The corpus contains 1552 argument components and 327 non-argumentative instances. For our experiments, we randomly split the data into a 80% training set and a 20% test set with the same class distribution and determine the best performing system using 10-fold cross-validation on the training set only. In our experiments, we use several classifiers (see section 4.2) from the Weka data mining software (Hall et al., 2009). For preprocessing the corpus, we use the Stanford POS-Tagger (Toutanova et al., 2003) and Parser (Klein and Manning, 2003) included in the DKPro Framework (Gurevych et al., 2007). After these steps, we use the DKPro-TC text classification framework (Daxenberger et al., 2014) for extracting the features described in the following section. 4.1 Features Structural features: We define structural features based on token statistics, the location and punctuations of the argument component and its covering sentence. Since Biran and Rambow (2011) found that premises are longer on the average than oth</context>
</contexts>
<marker>Hall, Frank, Holmes, Pfahringer, Reutemann, Witten, 2009</marker>
<rawString>Mark Hall, Eibe Frank, Geoffrey Holmes, Bernhard Pfahringer, Peter Reutemann, and Ian H. Witten. 2009. The weka data mining software: An update. SIGKDDExplor. Newsl., 11(1):10–18.</rawString>
</citation>
<citation valid="true">
<authors>
<author>Dan Klein</author>
<author>Christopher D Manning</author>
</authors>
<title>Accurate unlexicalized parsing.</title>
<date>2003</date>
<booktitle>In Proceedings of the 41st Annual Meeting on Association for Computational Linguistics - Volume 1, ACL ’03,</booktitle>
<pages>423--430</pages>
<location>Sapporo, Japan.</location>
<contexts>
<context position="17379" citStr="Klein and Manning, 2003" startWordPosition="2652" endWordPosition="2655">ishing a stable standpoint. Table 1: Class distribution among the instances. The corpus contains 1552 argument components and 327 non-argumentative instances. For our experiments, we randomly split the data into a 80% training set and a 20% test set with the same class distribution and determine the best performing system using 10-fold cross-validation on the training set only. In our experiments, we use several classifiers (see section 4.2) from the Weka data mining software (Hall et al., 2009). For preprocessing the corpus, we use the Stanford POS-Tagger (Toutanova et al., 2003) and Parser (Klein and Manning, 2003) included in the DKPro Framework (Gurevych et al., 2007). After these steps, we use the DKPro-TC text classification framework (Daxenberger et al., 2014) for extracting the features described in the following section. 4.1 Features Structural features: We define structural features based on token statistics, the location and punctuations of the argument component and its covering sentence. Since Biran and Rambow (2011) found that premises are longer on the average than other sentences, we add the number of tokens of the argument component and its covering sentence to our feature set. In additio</context>
</contexts>
<marker>Klein, Manning, 2003</marker>
<rawString>Dan Klein and Christopher D. Manning. 2003. Accurate unlexicalized parsing. In Proceedings of the 41st Annual Meeting on Association for Computational Linguistics - Volume 1, ACL ’03, pages 423– 430, Sapporo, Japan.</rawString>
</citation>
<citation valid="true">
<authors>
<author>Klaus Krippendorff</author>
</authors>
<title>Measuring the Reliability of Qualitative Text Analysis Data.</title>
<date>2004</date>
<journal>Quality &amp; Quantity,</journal>
<volume>38</volume>
<issue>6</issue>
<contexts>
<context position="15450" citStr="Krippendorff, 2004" startWordPosition="2357" endWordPosition="2358">omponents at the clause-level as well as argumentative relations. In particular, it includes annotations of major claims, claims and premises, which are connected with argumentative support and attack relations. Argumentative relations are directed (there is a specified source and target component of each relation) and can hold between a premise and another premise, a premise and a (major-) claim, or a claim and a major claim. Except for the last one, an argumentative relation does not cross paragraph boundaries. Three raters annotated the corpus with an interannotator agreement of αU = 0.72 (Krippendorff, 2004) for argument components and α = 0.81 for argumentative relations. In total, the corpus comprises 90 essays including 1,673 sentences. Since it only contains a low number of attack relations, we focus in this work solely on the identification of argument components and argumentative support relations. However, the proposed approach can also be applied to identify attack relations in future work. 4 Identifying Argument Components We consider the identification of argument components as a multiclass classification task. Each clause in the corpus is either classified as major claim, claim, premis</context>
</contexts>
<marker>Krippendorff, 2004</marker>
<rawString>Klaus Krippendorff. 2004. Measuring the Reliability of Qualitative Text Analysis Data. Quality &amp; Quantity, 38(6):787–800.</rawString>
</citation>
<citation valid="true">
<authors>
<author>Ziheng Lin</author>
<author>Min-Yen Kan</author>
<author>Hwee Tou Ng</author>
</authors>
<title>Recognizing implicit discourse relations in the Penn Discourse Treebank.</title>
<date>2009</date>
<booktitle>In Proceedings of the 2009 Conference on Empirical Methods in Natural Language Processing: Volume 1, EMNLP ’09,</booktitle>
<pages>343--351</pages>
<location>Stroudsburg, PA, USA.</location>
<contexts>
<context position="12483" citStr="Lin et al. (2009)" startWordPosition="1887" endWordPosition="1890">ence relation (the most similar relation of their work compared to argumentative relations). With the release of the PDTB, the identification of discourse relations gained a lot of interest in the research community. The PDTB includes implicit as well as explicit discourse relations of different types, and there are multiple approaches aiming at automatically identifying implicit relations. Pitler et al. (2009) experiment with polarity tags, verb classes, length of verb phrases, modality, context and lexical features and found that word pairs with non-zero Information Gain yield best results. Lin et al. (2009) show that beside lexical features, production rules collected from parse trees yield good results, whereas Louis et al. (2010) found that features based on namedentities do not perform as well as lexical features. However, current approaches to discourse analysis like the RST or the PDTB are designed to analyze general discourse structures, and thus include a large set of generic discourse relations, whereas only a subset of those relations is relevant for argumentative discourse analysis. For instance, the argumentation scheme proposed by Peldszus and Stede (2013) includes three argumentativ</context>
<context position="20808" citStr="Lin et al. (2009)" startWordPosition="3206" endWordPosition="3209">ntative discourse to signal the degree of certainty when expressing a claim. We use the POS tags generated during preprocessing to identify modals and define a Boolean feature which indicates if an argument component contains a modal verb. Syntactic features: To capture syntactic properties of argument components, we define features extracted from parse trees. We adopt two features proposed by (Mochales-Palau and Moens, 2009): the number of sub-clauses included in the covering sentence and the depth of the parse tree. In addition, we extract production rules from the parse tree as proposed by Lin et al. (2009) to capture syntactic characteristics of an argument component. The production rules are collected for each function tag (e.g. VP, NN, S, etc.) in the subtree of an argument component. The feature set includes e.g. rules like V P → VBG, NP or PP → IN, NP. We model each production rule as a Boolean feature and set it to true if it appears in the subtree of an argument component. Since premises often refer to previous events and claims are usually in present tense, we capture the tense of the main verb of an argument component as proposed by Mochales-Palau and Moens (2009) and define a feature t</context>
</contexts>
<marker>Lin, Kan, Ng, 2009</marker>
<rawString>Ziheng Lin, Min-Yen Kan, and Hwee Tou Ng. 2009. Recognizing implicit discourse relations in the Penn Discourse Treebank. In Proceedings of the 2009 Conference on Empirical Methods in Natural Language Processing: Volume 1, EMNLP ’09, pages 343–351, Stroudsburg, PA, USA.</rawString>
</citation>
<citation valid="true">
<authors>
<author>Annie Louis</author>
<author>Aravind Joshi</author>
<author>Rashmi Prasad</author>
<author>Ani Nenkova</author>
</authors>
<title>Using entity features to classify implicit discourse relations.</title>
<date>2010</date>
<booktitle>In Proceedings of the 11th Annual Meeting of the Special Interest Group on Discourse and Dialogue, SIGDIAL ’10,</booktitle>
<pages>59--62</pages>
<location>Stroudsburg, PA, USA.</location>
<contexts>
<context position="12610" citStr="Louis et al. (2010)" startWordPosition="1906" endWordPosition="1909">e identification of discourse relations gained a lot of interest in the research community. The PDTB includes implicit as well as explicit discourse relations of different types, and there are multiple approaches aiming at automatically identifying implicit relations. Pitler et al. (2009) experiment with polarity tags, verb classes, length of verb phrases, modality, context and lexical features and found that word pairs with non-zero Information Gain yield best results. Lin et al. (2009) show that beside lexical features, production rules collected from parse trees yield good results, whereas Louis et al. (2010) found that features based on namedentities do not perform as well as lexical features. However, current approaches to discourse analysis like the RST or the PDTB are designed to analyze general discourse structures, and thus include a large set of generic discourse relations, whereas only a subset of those relations is relevant for argumentative discourse analysis. For instance, the argumentation scheme proposed by Peldszus and Stede (2013) includes three argumentative relations (support, attack and counter-attack), whereas Stab and Gurevych (2014) propose a scheme including only two relation</context>
</contexts>
<marker>Louis, Joshi, Prasad, Nenkova, 2010</marker>
<rawString>Annie Louis, Aravind Joshi, Rashmi Prasad, and Ani Nenkova. 2010. Using entity features to classify implicit discourse relations. In Proceedings of the 11th Annual Meeting of the Special Interest Group on Discourse and Dialogue, SIGDIAL ’10, pages 59–62, Stroudsburg, PA, USA.</rawString>
</citation>
<citation valid="true">
<authors>
<author>William C Mann</author>
<author>Sandra A Thompson</author>
</authors>
<title>Rhetorical structure theory: A theory of text organization.</title>
<date>1987</date>
<tech>Technical Report ISI/RS-87-190,</tech>
<institution>Information Sciences Institute.</institution>
<contexts>
<context position="4166" citStr="Mann and Thompson, 1987" startWordPosition="623" endWordPosition="626">iscourse Treebank (Carlson et al., 2001) include discourse markers. 1We use the term claim synonymously to conclusion. In our definition the differentiation between claims and premises does not indicate the validity of the statements but signals which components include the gist of an argument and which are given by the author as justification. 46 Proceedings of the 2014 Conference on Empirical Methods in Natural Language Processing (EMNLP), pages 46–56, October 25-29, 2014, Doha, Qatar. c�2014 Association for Computational Linguistics Second, in contrast to Rhetorical Structure Theory (RST) (Mann and Thompson, 1987), argumentative relations also hold between non-adjacent sentences/clauses. For instance, in the corpus compiled by Stab and Gurevych (2014) only 37% of the premises appear adjacent to a claim. Therefore, existing approaches of discourse analysis, e.g. based on RST, do not meet the requirements of argumentative discourse structure identification, since they only consider discourse relations between adjacent sentences/clauses (Peldszus and Stede, 2013). In addition, there are no distinct argumentative relations included in common approaches like RST or the Penn Discourse Treebank (PDTB) (Prasad</context>
</contexts>
<marker>Mann, Thompson, 1987</marker>
<rawString>William C. Mann and Sandra A. Thompson. 1987. Rhetorical structure theory: A theory of text organization. Technical Report ISI/RS-87-190, Information Sciences Institute.</rawString>
</citation>
<citation valid="true">
<authors>
<author>Daniel Marcu</author>
<author>Abdessamad Echihabi</author>
</authors>
<title>An unsupervised approach to recognizing discourse relations.</title>
<date>2002</date>
<booktitle>In Proceedings of the 40th Annual Meeting on Association for Computational Linguistics, ACL ’02,</booktitle>
<pages>368--375</pages>
<contexts>
<context position="3483" citStr="Marcu and Echihabi (2002)" startWordPosition="520" endWordPosition="523">mensional structure of the art, which is important to study.” In this example, the premises (2) and (3) support the claim (1) whereas premise (4) is a support for premise (3). Thus, this example includes three argumentative support relations holding between the components (2,1), (3,1) and (4,3) signaling that the source component is a justification of the target component. This illustrates two important properties of argumentative discourse structures. First, argumentative relations are often implicit (not indicated by discourse markers; e.g. the relation holding between (2) and (1)). Indeed, Marcu and Echihabi (2002) found that only 26% of the evidence relations in the RST Discourse Treebank (Carlson et al., 2001) include discourse markers. 1We use the term claim synonymously to conclusion. In our definition the differentiation between claims and premises does not indicate the validity of the statements but signals which components include the gist of an argument and which are given by the author as justification. 46 Proceedings of the 2014 Conference on Empirical Methods in Natural Language Processing (EMNLP), pages 46–56, October 25-29, 2014, Doha, Qatar. c�2014 Association for Computational Linguistics</context>
<context position="11414" citStr="Marcu and Echihabi (2002)" startWordPosition="1727" endWordPosition="1730">which are likely in argumentative writing support. In addition, the approach relies on discourse markers and is therefore not applicable for identifying implicit argumentative discourse structures. 2.2 Discourse Relations Identifying argumentative discourse structures is closely related to discourse analysis. As illustrated 2Calculated from the precision and recall scores provided for individual rhetorical roles in (Teufel, 1999, p. 225). in the initial example, the identification of argumentative relations postulates the identification of implicit as well as non-adjacent discourse relations. Marcu and Echihabi (2002) present the first approach focused on identifying implicit discourse relations. They exploit several discourse markers (e.g. ‘because’ or ‘but’) for collecting large amounts of training data. For their experiments they remove the discourse markers and discover that word pair features are indicative for implicit discourse relations. Depending on the utilized corpus, they obtain accuracies between 64% and 75% for identifying a cause-explanation-evidence relation (the most similar relation of their work compared to argumentative relations). With the release of the PDTB, the identification of dis</context>
<context position="31043" citStr="Marcu and Echihabi, 2002" startWordPosition="4842" endWordPosition="4845">of the covering sentences in the essay, four Boolean features indicate if the argument components are present in the first or last sentence of a paragraph, one Boolean feature for representing if the target component occurs before the source component, the sentence distance between the covering sentences, and a Boolean feature which indicates if both argument components are in the same sentence. Lexical features: We define lexical features based on word pairs, first words and modals. It has been shown in previous work that word pairs are effective for identifying implicit discourse relations (Marcu and Echihabi, 2002). We define each pair of words between the source and target components as a Boolean feature and investigate word pairs containing stop words as well as stop word filtered word pairs. In addition, we adopt the first word features proposed by Pitler et al. (2009). We extract the first word either from the argument component or from non-annotated tokens preceding the argument component in the covering sentence if present. So, the first word of an argument component is either the first word of the sentence containing the argument component, the first word following a preceding argument component </context>
</contexts>
<marker>Marcu, Echihabi, 2002</marker>
<rawString>Daniel Marcu and Abdessamad Echihabi. 2002. An unsupervised approach to recognizing discourse relations. In Proceedings of the 40th Annual Meeting on Association for Computational Linguistics, ACL ’02, pages 368–375.</rawString>
</citation>
<citation valid="true">
<authors>
<author>Quinn McNemar</author>
</authors>
<title>Note on the sampling error of the difference between correlated proportions or percentages.</title>
<date>1947</date>
<journal>Psychometrika,</journal>
<volume>12</volume>
<issue>2</issue>
<contexts>
<context position="23626" citStr="McNemar, 1947" startWordPosition="3660" endWordPosition="3661"> 50 4.2 Results and Analysis For identifying the best performing system, we conducted several experiments on the training set using stratified 10-fold cross-validation. We determine the evaluation scores by accumulating the confusion matrices of each fold into one confusion matrix, since it is the less biased method for evaluating cross-validation studies (Forman and Scholz, 2010). In a comparison of several classifiers (Support Vector Machine, Naive Bayes, C4.5 Decision Tree and Random Forest), we found that each of the classifiers significantly outperforms a majority baseline (McNemar Test (McNemar, 1947) with p = 0.05) and that a Support Vector Machine (SVM) achieves the best results using 100 top features ranked by Information Gain.3 It achieves an accuracy of 77.3% on the test set and outperforms the majority baseline with respect to overall accuracy as well as F1-score (table 2). Baseline Human SVM Accuracy 0.55 0.877 0.773 Macro F1 0.177 0.871 0.726 Macro Precision 0.137 0.864 0.773 Macro Recall 0.25 0.879 0.684 F1 MajorClaim 0 0.916 0.625 F1 Claim 0 0.841 0.538 F1 Premise 0.709 0.911 0.826 F1 None 0 0.812 0.884 Table 2: Results of an SVM for argument component classification on the test </context>
</contexts>
<marker>McNemar, 1947</marker>
<rawString>Quinn McNemar. 1947. Note on the sampling error of the difference between correlated proportions or percentages. Psychometrika, 12(2):153–157.</rawString>
</citation>
<citation valid="true">
<authors>
<author>Raquel Mochales-Palau</author>
<author>Marie-Francine Moens</author>
</authors>
<title>Argumentation mining: The detection, classification and structure of arguments in text.</title>
<date>2009</date>
<booktitle>In Proceedings of the 12th International Conference on Artificial Intelligence and Law, ICAIL ’09,</booktitle>
<pages>98--107</pages>
<publisher>ACM.</publisher>
<location>New York, NY, USA.</location>
<contexts>
<context position="5910" citStr="Mochales-Palau and Moens (2009)" startWordPosition="885" endWordPosition="888">m is justified without knowing which premises belong to it. Second, methods that recognize if a statement supports a given claim enable the collection of additional evidence from other sources. Third, the structure of argumentation is needed for recommending better arrangements of argument components and meaningful usage of discourse markers. Both foster argument comprehension and recall (Britt and Larson, 2003) and thus increase the argumentation quality. To the best of our knowledge, there is currently only one approach that aims at identifying argumentative discourse structures proposed by Mochales-Palau and Moens (2009). However, it relies on a manually created context-free grammar (CFG) and is tailored to the legal domain, which follows a standardized argumentation style. Therefore, it is likely that it will not achieve acceptable accuracy when applied to more general texts in which discourse markers are missing or even misleadingly used (e.g. student texts). In this work, we present a novel approach for identifying argumentative discourse structures which includes two consecutive steps. In the first step, we focus on the identification of argument components using a multiclass classification approach. In t</context>
<context position="7534" citStr="Mochales-Palau and Moens, 2009" startWordPosition="1123" endWordPosition="1126">en if discourse markers are missing or misleadingly used. Second, we present two novel feature sets for identifying argument components as well as argumentative relations. Third, we evaluate several classifiers and feature groups for identifying the best system for both tasks. 2 Related Work 2.1 Argumentation Mining Previous research on argumentation mining spans several subtasks, including (1) the separation of argumentative from non-argumentative text units (Moens et al., 2007; Florou et al., 2013), (2) the classification of argument components or argumentation schemes (Rooney et al., 2012; Mochales-Palau and Moens, 2009; Teufel, 1999; Feng and Hirst, 2011), and (3) the identification of argumentation structures (Mochales-Palau and Moens, 2009; Wyner et al., 2010). The separation of argumentative from nonargumentative text units is usually considered as a binary classification task and constitutes one of the first steps in an argumentation mining pipeline. Moens et al. (2007) propose an approach for identifying argumentative sentences in the Araucaria corpus (Reed et al., 2008). The argument annotations in Araucaria are based on a domainindependent argumentation theory proposed by Walton (1996). In their expe</context>
<context position="10448" citStr="Mochales-Palau and Moens, 2009" startWordPosition="1587" endWordPosition="1590">ably identified in advance. In their experiments, they achieve an accuracy between 62.9% and 97.9% depending on the particular scheme and the classification setup. In contrast to all approaches mentioned above, the work presented in this paper focuses besides the separation of argumentative from nonargumentative text units and the classification of argument components on the extraction of the argumentative discourse structure to identify which components of the argument belong together for achieving a more fine-grained and detailed analysis of argumentation. We are only aware of one approach (Mochales-Palau and Moens, 2009; Wyner et al., 2010) that also focuses on the identification of argumentative discourse structures. However, this approach is based on a manually created CFG that is tailored to documents from the legal domain, which follow a standardized argumentation style. Therefore, it does not accommodate illformatted arguments (Wyner et al., 2010), which are likely in argumentative writing support. In addition, the approach relies on discourse markers and is therefore not applicable for identifying implicit argumentative discourse structures. 2.2 Discourse Relations Identifying argumentative discourse s</context>
<context position="20620" citStr="Mochales-Palau and Moens, 2009" startWordPosition="3172" endWordPosition="3175">d adverbs like ‘also’, ‘often’ or ‘really’ emphasize the importance of a premise. We model both verbs and adverbs as Boolean features. Modal verbs like ‘should’ and ‘could’ are frequently used in argumentative discourse to signal the degree of certainty when expressing a claim. We use the POS tags generated during preprocessing to identify modals and define a Boolean feature which indicates if an argument component contains a modal verb. Syntactic features: To capture syntactic properties of argument components, we define features extracted from parse trees. We adopt two features proposed by (Mochales-Palau and Moens, 2009): the number of sub-clauses included in the covering sentence and the depth of the parse tree. In addition, we extract production rules from the parse tree as proposed by Lin et al. (2009) to capture syntactic characteristics of an argument component. The production rules are collected for each function tag (e.g. VP, NN, S, etc.) in the subtree of an argument component. The feature set includes e.g. rules like V P → VBG, NP or PP → IN, NP. We model each production rule as a Boolean feature and set it to true if it appears in the subtree of an argument component. Since premises often refer to p</context>
<context position="39743" citStr="Mochales-Palau and Moens (2009)" startWordPosition="6241" endWordPosition="6244">. The results also show that the distinction between claims and premises is the major challenge for identifying argument components. It turned out that structural features are the most effective ones for this task. However, some of those features are unique to persuasive essays, and it is an open question if there are general structural properties of arguments which can be exploited for separating claims from premises. Our experiments show that discourse markers yield only low accuracies. Using only our defined indicator features, we obtain an F1-score of 0.265 for identifying claims, whereas Mochales-Palau and Moens (2009) achieve 0.673 for the same task in legal documents using a CFG. This confirms our initial assumption that approaches relying on discourse markers are not applicable for identifying argumentative discourse structures in documents which do not follow a standardized form. In addition, it shows that discourse markers are either frequently missing or misleadingly used in student texts and that there is a need for argumentative writing support systems that assist students in employing discourse markers correctly. 7 Conclusion and Future Work We presented a novel approach for identifying argumentati</context>
</contexts>
<marker>Mochales-Palau, Moens, 2009</marker>
<rawString>Raquel Mochales-Palau and Marie-Francine Moens. 2009. Argumentation mining: The detection, classification and structure of arguments in text. In Proceedings of the 12th International Conference on Artificial Intelligence and Law, ICAIL ’09, pages 98– 107, New York, NY, USA. ACM.</rawString>
</citation>
<citation valid="true">
<authors>
<author>Marie-Francine Moens</author>
<author>Erik Boiy</author>
<author>Raquel Mochales Palau</author>
<author>Chris Reed</author>
</authors>
<title>Automatic detection of arguments in legal texts.</title>
<date>2007</date>
<booktitle>In Proceedings of the 11th International Conference on Artificial Intelligence and Law, ICAIL ’07,</booktitle>
<pages>225--230</pages>
<location>Stanford, California.</location>
<contexts>
<context position="7387" citStr="Moens et al., 2007" startWordPosition="1102" endWordPosition="1105">ive discourse structures. Contrary to previous approaches, our approach is capable of identifying argumentative discourse structures even if discourse markers are missing or misleadingly used. Second, we present two novel feature sets for identifying argument components as well as argumentative relations. Third, we evaluate several classifiers and feature groups for identifying the best system for both tasks. 2 Related Work 2.1 Argumentation Mining Previous research on argumentation mining spans several subtasks, including (1) the separation of argumentative from non-argumentative text units (Moens et al., 2007; Florou et al., 2013), (2) the classification of argument components or argumentation schemes (Rooney et al., 2012; Mochales-Palau and Moens, 2009; Teufel, 1999; Feng and Hirst, 2011), and (3) the identification of argumentation structures (Mochales-Palau and Moens, 2009; Wyner et al., 2010). The separation of argumentative from nonargumentative text units is usually considered as a binary classification task and constitutes one of the first steps in an argumentation mining pipeline. Moens et al. (2007) propose an approach for identifying argumentative sentences in the Araucaria corpus (Reed </context>
</contexts>
<marker>Moens, Boiy, Palau, Reed, 2007</marker>
<rawString>Marie-Francine Moens, Erik Boiy, Raquel Mochales Palau, and Chris Reed. 2007. Automatic detection of arguments in legal texts. In Proceedings of the 11th International Conference on Artificial Intelligence and Law, ICAIL ’07, pages 225–230, Stanford, California.</rawString>
</citation>
<citation valid="true">
<authors>
<author>Andreas Peldszus</author>
<author>Manfred Stede</author>
</authors>
<title>From Argument Diagrams to Argumentation Mining in Texts: A Survey.</title>
<date>2013</date>
<journal>International Journal of Cognitive Informatics and Natural Intelligence (IJCINI),</journal>
<volume>7</volume>
<issue>1</issue>
<contexts>
<context position="4621" citStr="Peldszus and Stede, 2013" startWordPosition="691" endWordPosition="695">–56, October 25-29, 2014, Doha, Qatar. c�2014 Association for Computational Linguistics Second, in contrast to Rhetorical Structure Theory (RST) (Mann and Thompson, 1987), argumentative relations also hold between non-adjacent sentences/clauses. For instance, in the corpus compiled by Stab and Gurevych (2014) only 37% of the premises appear adjacent to a claim. Therefore, existing approaches of discourse analysis, e.g. based on RST, do not meet the requirements of argumentative discourse structure identification, since they only consider discourse relations between adjacent sentences/clauses (Peldszus and Stede, 2013). In addition, there are no distinct argumentative relations included in common approaches like RST or the Penn Discourse Treebank (PDTB) (Prasad et al., 2008), since they are focused on identifying general discourse structures (cp. section 2.2). Most of the existing argumentation mining methods focus solely on the identification of argument components. However, identifying argumentative discourse structures is an important task (Sergeant, 2013) in particular for providing feedback about argumentation. First, argumentative discourse structures are essential for evaluating the quality of an arg</context>
<context position="13055" citStr="Peldszus and Stede (2013)" startWordPosition="1977" endWordPosition="1980">nformation Gain yield best results. Lin et al. (2009) show that beside lexical features, production rules collected from parse trees yield good results, whereas Louis et al. (2010) found that features based on namedentities do not perform as well as lexical features. However, current approaches to discourse analysis like the RST or the PDTB are designed to analyze general discourse structures, and thus include a large set of generic discourse relations, whereas only a subset of those relations is relevant for argumentative discourse analysis. For instance, the argumentation scheme proposed by Peldszus and Stede (2013) includes three argumentative relations (support, attack and counter-attack), whereas Stab and Gurevych (2014) propose a scheme including only two relations (support and attack). The difference between argumentative relations and those included in general tagsets like RST and PDTB is best illustrated by the work of Biran and Rambow (2011), which is to the best of our knowledge the only work that focuses on the identification of argumentative relations. They argue that existing definitions of discourse relations are only relevant as a building block for identifying argumentative discourse and t</context>
</contexts>
<marker>Peldszus, Stede, 2013</marker>
<rawString>Andreas Peldszus and Manfred Stede. 2013. From Argument Diagrams to Argumentation Mining in Texts: A Survey. International Journal of Cognitive Informatics and Natural Intelligence (IJCINI), 7(1):1–31.</rawString>
</citation>
<citation valid="true">
<authors>
<author>Emily Pitler</author>
<author>Annie Louis</author>
<author>Ani Nenkova</author>
</authors>
<title>Automatic sense prediction for implicit discourse relations in text.</title>
<date>2009</date>
<booktitle>In Proceedings of the Joint Conference of the 47th Annual Meeting of the ACL and the 4th International Joint Conference on Natural Language Processing of the AFNLP, ACL ’09,</booktitle>
<pages>683--691</pages>
<institution>Suntec, Singapore. Association for Computational Linguistics.</institution>
<contexts>
<context position="12280" citStr="Pitler et al. (2009)" startWordPosition="1855" endWordPosition="1858">rs and discover that word pair features are indicative for implicit discourse relations. Depending on the utilized corpus, they obtain accuracies between 64% and 75% for identifying a cause-explanation-evidence relation (the most similar relation of their work compared to argumentative relations). With the release of the PDTB, the identification of discourse relations gained a lot of interest in the research community. The PDTB includes implicit as well as explicit discourse relations of different types, and there are multiple approaches aiming at automatically identifying implicit relations. Pitler et al. (2009) experiment with polarity tags, verb classes, length of verb phrases, modality, context and lexical features and found that word pairs with non-zero Information Gain yield best results. Lin et al. (2009) show that beside lexical features, production rules collected from parse trees yield good results, whereas Louis et al. (2010) found that features based on namedentities do not perform as well as lexical features. However, current approaches to discourse analysis like the RST or the PDTB are designed to analyze general discourse structures, and thus include a large set of generic discourse rel</context>
<context position="31305" citStr="Pitler et al. (2009)" startWordPosition="4887" endWordPosition="4890">istance between the covering sentences, and a Boolean feature which indicates if both argument components are in the same sentence. Lexical features: We define lexical features based on word pairs, first words and modals. It has been shown in previous work that word pairs are effective for identifying implicit discourse relations (Marcu and Echihabi, 2002). We define each pair of words between the source and target components as a Boolean feature and investigate word pairs containing stop words as well as stop word filtered word pairs. In addition, we adopt the first word features proposed by Pitler et al. (2009). We extract the first word either from the argument component or from non-annotated tokens preceding the argument component in the covering sentence if present. So, the first word of an argument component is either the first word of the sentence containing the argument component, the first word following a preceding argument component in the same sentence or the first word of the actual argument component if it commences the sentence or directly follows another argument component. Actual 52 So, we ensure that the first word of an argument component includes important discourse markers which a</context>
<context position="33828" citStr="Pitler et al. (2009)" startWordPosition="5304" endWordPosition="5307"> of the argument component is reliably identified many potential pairs can be excluded. Therefore, we define two features that represent the argumentative type of the source and target components identified in the first experiment. 5.2 Results and Analysis The comparison of several classifiers reveals that an SVM achieves the best results. In our experiments, all classifiers except the C4.5 Decision Tree significantly outperform a majority baseline which classifies all pairs as non-support (p = 0.05). We also conducted several experiments using word pair features only and found in contrast to Pitler et al. (2009) that limiting the number of word pairs decreases the performance. In particular, we compared the top 100, 250, 500, 1000, 2500, 5000 word pairs ranked by Information Gain, non-zero Information Gain word pairs and non-filtered word pairs. The results show that non-filtered word pairs perform best (macro F1-score of 0.68). Our experiments also reveal that filtering stop words containing word pairs decreases the macro F1-score to 0.60. We obtain the best results using an SVM without any feature selection method. Due to the class imbalance, the SVM only slightly outperforms the accuracy of a majo</context>
</contexts>
<marker>Pitler, Louis, Nenkova, 2009</marker>
<rawString>Emily Pitler, Annie Louis, and Ani Nenkova. 2009. Automatic sense prediction for implicit discourse relations in text. In Proceedings of the Joint Conference of the 47th Annual Meeting of the ACL and the 4th International Joint Conference on Natural Language Processing of the AFNLP, ACL ’09, pages 683–691, Suntec, Singapore. Association for Computational Linguistics.</rawString>
</citation>
<citation valid="true">
<authors>
<author>Rashmi Prasad</author>
<author>Eleni Miltsakaki</author>
<author>Nikhil Dinesh</author>
<author>Alan Lee</author>
<author>Aravind Joshi</author>
<author>Livio Robaldo</author>
<author>Bonnie L Webber</author>
</authors>
<title>The Penn Discourse Treebank 2.0 annotation manual.</title>
<date>2007</date>
<tech>Technical report,</tech>
<institution>Institute for Research in Cognitive Science, University of Pennsylvania.</institution>
<contexts>
<context position="21839" citStr="Prasad et al., 2007" startWordPosition="3381" endWordPosition="3384">previous events and claims are usually in present tense, we capture the tense of the main verb of an argument component as proposed by Mochales-Palau and Moens (2009) and define a feature that indicates if an argument component is in the past or present tense. Indicators: Discourse markers often indicate the components of an argument. For example, claims are frequently introduced with ‘therefore’, ‘thus’ or ‘consequently’, whereas premises contain markers like ‘because’, ‘reason’ or ‘furthermore’. We collected a list of discourse markers from the Penn Discourse Treebank 2.0 Annotation Manual (Prasad et al., 2007) and removed markers that do not indicate argumentative discourse (e.g. markers which indicate temporal discourse). In total, we collected 55 discourse markers and model each as a Boolean feature set to true if the particular marker precedes the argumentative component. In addition, we define five Boolean features which denote a reference to the first person in the covering sentence of an argument component: ‘I’, ‘me’, ‘my’, ‘mine’, and ‘myself’. An additional Boolean feature indicates if one of them is present in the covering sentence. We expect that those features are good indicators of the </context>
</contexts>
<marker>Prasad, Miltsakaki, Dinesh, Lee, Joshi, Robaldo, Webber, 2007</marker>
<rawString>Rashmi Prasad, Eleni Miltsakaki, Nikhil Dinesh, Alan Lee, Aravind Joshi, Livio Robaldo, and Bonnie L. Webber. 2007. The Penn Discourse Treebank 2.0 annotation manual. Technical report, Institute for Research in Cognitive Science, University of Pennsylvania.</rawString>
</citation>
<citation valid="true">
<authors>
<author>Rashmi Prasad</author>
<author>Nikhil Dinesh</author>
<author>Alan Lee</author>
<author>Eleni Miltsakaki</author>
<author>Livio Robaldo</author>
<author>Aravind Joshi</author>
<author>Bonnie Webber</author>
</authors>
<title>The Penn Discourse Treebank 2.0.</title>
<date>2008</date>
<booktitle>In Proceedings of the Sixth International Conference on Language Resources and Evaluation (LREC’08),</booktitle>
<location>Marrakech, Morocco.</location>
<contexts>
<context position="4780" citStr="Prasad et al., 2008" startWordPosition="717" endWordPosition="720"> 1987), argumentative relations also hold between non-adjacent sentences/clauses. For instance, in the corpus compiled by Stab and Gurevych (2014) only 37% of the premises appear adjacent to a claim. Therefore, existing approaches of discourse analysis, e.g. based on RST, do not meet the requirements of argumentative discourse structure identification, since they only consider discourse relations between adjacent sentences/clauses (Peldszus and Stede, 2013). In addition, there are no distinct argumentative relations included in common approaches like RST or the Penn Discourse Treebank (PDTB) (Prasad et al., 2008), since they are focused on identifying general discourse structures (cp. section 2.2). Most of the existing argumentation mining methods focus solely on the identification of argument components. However, identifying argumentative discourse structures is an important task (Sergeant, 2013) in particular for providing feedback about argumentation. First, argumentative discourse structures are essential for evaluating the quality of an argument, since it is not possible to examine how well a claim is justified without knowing which premises belong to it. Second, methods that recognize if a state</context>
<context position="26204" citStr="Prasad et al., 2008" startWordPosition="4064" endWordPosition="4067">ne Structural 0.477 0.419 0.781 0.897 Lexical 0.317 0.401 0.753 0.275 Syntactic 0.094 0.292 0.654 0.427 Indicators 0.286 0.265 0.730 0 Contextual 0 0 0.709 0 Table 3: F1-scores for individual feature groups and classes (SVM with 10-fold cross-validation on the entire data set) Interestingly, the defined indicators are not useful for separating argumentative from nonargumentative text units though they are helpful for classifying argument components. A reason for this could be that not each occurrence of an indicator distinctly signals argument components, since their sense is often ambiguous (Prasad et al., 2008). For example ‘since’ indicates temporal properties as well as justifications, whereas ‘because’ also indicates causal links. Syntactic features also contribute to the identification of argument components. They achieve an F1-score of 0.292 for claims and 0.654 for premises and also contribute to the separation of argumentative from non-argumentative text units. Contextual features do not perform well. However, they increase the accuracy by 0.7% in combination with other features. Nevertheless, this difference is not significant (p = 0.05). Error analysis: The system performs well for separati</context>
</contexts>
<marker>Prasad, Dinesh, Lee, Miltsakaki, Robaldo, Joshi, Webber, 2008</marker>
<rawString>Rashmi Prasad, Nikhil Dinesh, Alan Lee, Eleni Miltsakaki, Livio Robaldo, Aravind Joshi, and Bonnie Webber. 2008. The Penn Discourse Treebank 2.0. In Proceedings of the Sixth International Conference on Language Resources and Evaluation (LREC’08), Marrakech, Morocco.</rawString>
</citation>
<citation valid="true">
<authors>
<author>Chris Reed</author>
<author>Raquel Mochales-Palau</author>
<author>Glenn Rowe</author>
<author>Marie-Francine Moens</author>
</authors>
<title>Language resources for studying argument.</title>
<date>2008</date>
<booktitle>In Proceedings of the Sixth International Conference on Language Resources and Evaluation, LREC ’08,</booktitle>
<pages>2613--2618</pages>
<location>Marrakech, Morocco.</location>
<contexts>
<context position="8000" citStr="Reed et al., 2008" startWordPosition="1193" endWordPosition="1196"> 2007; Florou et al., 2013), (2) the classification of argument components or argumentation schemes (Rooney et al., 2012; Mochales-Palau and Moens, 2009; Teufel, 1999; Feng and Hirst, 2011), and (3) the identification of argumentation structures (Mochales-Palau and Moens, 2009; Wyner et al., 2010). The separation of argumentative from nonargumentative text units is usually considered as a binary classification task and constitutes one of the first steps in an argumentation mining pipeline. Moens et al. (2007) propose an approach for identifying argumentative sentences in the Araucaria corpus (Reed et al., 2008). The argument annotations in Araucaria are based on a domainindependent argumentation theory proposed by Walton (1996). In their experiments, they obtain the best accuracy (73.75%) using a combination of word pairs, text statistics, verbs, and a list of keywords indicative for argumentative discourse. Florou et al. (2013) report a similar approach. They classify text segments crawled with a focused crawler as either containing an argument or not. Their approach is based on several discourse markers and features extracted from the tense and mood of verbs. They report an F1-score of 0.764 for t</context>
</contexts>
<marker>Reed, Mochales-Palau, Rowe, Moens, 2008</marker>
<rawString>Chris Reed, Raquel Mochales-Palau, Glenn Rowe, and Marie-Francine Moens. 2008. Language resources for studying argument. In Proceedings of the Sixth International Conference on Language Resources and Evaluation, LREC ’08, pages 2613–2618, Marrakech, Morocco.</rawString>
</citation>
<citation valid="true">
<authors>
<author>Niall Rooney</author>
<author>Hui Wang</author>
<author>Fiona Browne</author>
</authors>
<title>Applying kernel methods to argumentation mining.</title>
<date>2012</date>
<booktitle>In Proceedings of the Twenty-Fifth International Florida Artificial Intelligence Research Society Conference, FLAIRS ’12,</booktitle>
<pages>272--275</pages>
<location>Marco Island, FL, USA.</location>
<contexts>
<context position="7502" citStr="Rooney et al., 2012" startWordPosition="1119" endWordPosition="1122">scourse structures even if discourse markers are missing or misleadingly used. Second, we present two novel feature sets for identifying argument components as well as argumentative relations. Third, we evaluate several classifiers and feature groups for identifying the best system for both tasks. 2 Related Work 2.1 Argumentation Mining Previous research on argumentation mining spans several subtasks, including (1) the separation of argumentative from non-argumentative text units (Moens et al., 2007; Florou et al., 2013), (2) the classification of argument components or argumentation schemes (Rooney et al., 2012; Mochales-Palau and Moens, 2009; Teufel, 1999; Feng and Hirst, 2011), and (3) the identification of argumentation structures (Mochales-Palau and Moens, 2009; Wyner et al., 2010). The separation of argumentative from nonargumentative text units is usually considered as a binary classification task and constitutes one of the first steps in an argumentation mining pipeline. Moens et al. (2007) propose an approach for identifying argumentative sentences in the Araucaria corpus (Reed et al., 2008). The argument annotations in Araucaria are based on a domainindependent argumentation theory proposed</context>
</contexts>
<marker>Rooney, Wang, Browne, 2012</marker>
<rawString>Niall Rooney, Hui Wang, and Fiona Browne. 2012. Applying kernel methods to argumentation mining. In Proceedings of the Twenty-Fifth International Florida Artificial Intelligence Research Society Conference, FLAIRS ’12, pages 272–275, Marco Island, FL, USA.</rawString>
</citation>
<citation valid="true">
<authors>
<author>Alan Sergeant</author>
</authors>
<title>Automatic argumentation extraction.</title>
<date>2013</date>
<booktitle>In Proceedings of the 10th European Semantic Web Conference, ESWC ’13,</booktitle>
<pages>656--660</pages>
<location>Montpellier, France.</location>
<contexts>
<context position="5070" citStr="Sergeant, 2013" startWordPosition="760" endWordPosition="761">ements of argumentative discourse structure identification, since they only consider discourse relations between adjacent sentences/clauses (Peldszus and Stede, 2013). In addition, there are no distinct argumentative relations included in common approaches like RST or the Penn Discourse Treebank (PDTB) (Prasad et al., 2008), since they are focused on identifying general discourse structures (cp. section 2.2). Most of the existing argumentation mining methods focus solely on the identification of argument components. However, identifying argumentative discourse structures is an important task (Sergeant, 2013) in particular for providing feedback about argumentation. First, argumentative discourse structures are essential for evaluating the quality of an argument, since it is not possible to examine how well a claim is justified without knowing which premises belong to it. Second, methods that recognize if a statement supports a given claim enable the collection of additional evidence from other sources. Third, the structure of argumentation is needed for recommending better arrangements of argument components and meaningful usage of discourse markers. Both foster argument comprehension and recall </context>
</contexts>
<marker>Sergeant, 2013</marker>
<rawString>Alan Sergeant. 2013. Automatic argumentation extraction. In Proceedings of the 10th European Semantic Web Conference, ESWC ’13, pages 656–660, Montpellier, France.</rawString>
</citation>
<citation valid="true">
<authors>
<author>Christian Stab</author>
<author>Iryna Gurevych</author>
</authors>
<title>Annotating argument components and relations in persuasive essays.</title>
<date>2014</date>
<booktitle>In Proceedings of the 25th International Conference on Computational Linguistics (COLING 2014),</booktitle>
<pages>1501--1510</pages>
<location>Dublin, Ireland,</location>
<contexts>
<context position="4306" citStr="Stab and Gurevych (2014)" startWordPosition="644" endWordPosition="647"> differentiation between claims and premises does not indicate the validity of the statements but signals which components include the gist of an argument and which are given by the author as justification. 46 Proceedings of the 2014 Conference on Empirical Methods in Natural Language Processing (EMNLP), pages 46–56, October 25-29, 2014, Doha, Qatar. c�2014 Association for Computational Linguistics Second, in contrast to Rhetorical Structure Theory (RST) (Mann and Thompson, 1987), argumentative relations also hold between non-adjacent sentences/clauses. For instance, in the corpus compiled by Stab and Gurevych (2014) only 37% of the premises appear adjacent to a claim. Therefore, existing approaches of discourse analysis, e.g. based on RST, do not meet the requirements of argumentative discourse structure identification, since they only consider discourse relations between adjacent sentences/clauses (Peldszus and Stede, 2013). In addition, there are no distinct argumentative relations included in common approaches like RST or the Penn Discourse Treebank (PDTB) (Prasad et al., 2008), since they are focused on identifying general discourse structures (cp. section 2.2). Most of the existing argumentation min</context>
<context position="13165" citStr="Stab and Gurevych (2014)" startWordPosition="1991" endWordPosition="1994">cted from parse trees yield good results, whereas Louis et al. (2010) found that features based on namedentities do not perform as well as lexical features. However, current approaches to discourse analysis like the RST or the PDTB are designed to analyze general discourse structures, and thus include a large set of generic discourse relations, whereas only a subset of those relations is relevant for argumentative discourse analysis. For instance, the argumentation scheme proposed by Peldszus and Stede (2013) includes three argumentative relations (support, attack and counter-attack), whereas Stab and Gurevych (2014) propose a scheme including only two relations (support and attack). The difference between argumentative relations and those included in general tagsets like RST and PDTB is best illustrated by the work of Biran and Rambow (2011), which is to the best of our knowledge the only work that focuses on the identification of argumentative relations. They argue that existing definitions of discourse relations are only relevant as a building block for identifying argumentative discourse and that existing approaches do not contain a single relation that corresponds to 48 a distinct argumentative relat</context>
<context position="14783" citStr="Stab and Gurevych (2014)" startWordPosition="2251" endWordPosition="2254">e the extracted word pairs as features and obtain an F1-score of up to 0.51 using two different corpora. Although the approach considers non-adjacent relations, it is limited to the identification of relations between premises and claims and requires that claims are known in advance. In addition, the combination of several general relations to a single argumentative relation might lead to consistency problems and to noisy corpora (e.g. not each instance of a contrast relation is relevant for argumentative discourse). 3 Data For our experiments, we use a corpus of persuasive essays compiled by Stab and Gurevych (2014). This corpus contains annotations of argument components at the clause-level as well as argumentative relations. In particular, it includes annotations of major claims, claims and premises, which are connected with argumentative support and attack relations. Argumentative relations are directed (there is a specified source and target component of each relation) and can hold between a premise and another premise, a premise and a (major-) claim, or a claim and a major claim. Except for the last one, an argumentative relation does not cross paragraph boundaries. Three raters annotated the corpus</context>
</contexts>
<marker>Stab, Gurevych, 2014</marker>
<rawString>Christian Stab and Iryna Gurevych. 2014. Annotating argument components and relations in persuasive essays. In Proceedings of the 25th International Conference on Computational Linguistics (COLING 2014), pages 1501–1510, Dublin, Ireland, August.</rawString>
</citation>
<citation valid="true">
<authors>
<author>Simone Teufel</author>
</authors>
<title>Argumentative Zoning: Information Extraction from Scientific Text.</title>
<date>1999</date>
<tech>Ph.D. thesis,</tech>
<institution>University of Edinburgh.</institution>
<contexts>
<context position="7548" citStr="Teufel, 1999" startWordPosition="1127" endWordPosition="1128">ing or misleadingly used. Second, we present two novel feature sets for identifying argument components as well as argumentative relations. Third, we evaluate several classifiers and feature groups for identifying the best system for both tasks. 2 Related Work 2.1 Argumentation Mining Previous research on argumentation mining spans several subtasks, including (1) the separation of argumentative from non-argumentative text units (Moens et al., 2007; Florou et al., 2013), (2) the classification of argument components or argumentation schemes (Rooney et al., 2012; Mochales-Palau and Moens, 2009; Teufel, 1999; Feng and Hirst, 2011), and (3) the identification of argumentation structures (Mochales-Palau and Moens, 2009; Wyner et al., 2010). The separation of argumentative from nonargumentative text units is usually considered as a binary classification task and constitutes one of the first steps in an argumentation mining pipeline. Moens et al. (2007) propose an approach for identifying argumentative sentences in the Araucaria corpus (Reed et al., 2008). The argument annotations in Araucaria are based on a domainindependent argumentation theory proposed by Walton (1996). In their experiments, they </context>
<context position="9218" citStr="Teufel (1999)" startWordPosition="1395" endWordPosition="1396">best performing system. One of the first approaches focusing on the identification of argument components is Argumentative Zoning proposed by Teufel (1999). The underlying assumption of this work is that argu47 ment components extracted from a scientific article provide a good summary of its content. Each sentence is classified as one of seven rhetorical roles including claim, result or purpose. The approach obtained an F1-score of 0.462 using structural, lexical and syntactic features. Rooney et al. (2013) also focus on the identification of argument components but in contrast to the work of Teufel (1999) their scheme is not tailored to a particular genre. In their experiments, they identify claims, premises and non-argumentative text units in the Araucaria corpus and report an overall accuracy of 65%. Feng and Hirst (2011) also use the Araucaria corpus for their experiments but focus on the identification of argumentation schemes (Walton, 1996), which are templates for forms of arguments (e.g. argument from example or argument from consequence). Since their approach is based on features extracted from mutual information of claims and premises, it requires that the argument components are reli</context>
<context position="11221" citStr="Teufel, 1999" startWordPosition="1701" endWordPosition="1702">at is tailored to documents from the legal domain, which follow a standardized argumentation style. Therefore, it does not accommodate illformatted arguments (Wyner et al., 2010), which are likely in argumentative writing support. In addition, the approach relies on discourse markers and is therefore not applicable for identifying implicit argumentative discourse structures. 2.2 Discourse Relations Identifying argumentative discourse structures is closely related to discourse analysis. As illustrated 2Calculated from the precision and recall scores provided for individual rhetorical roles in (Teufel, 1999, p. 225). in the initial example, the identification of argumentative relations postulates the identification of implicit as well as non-adjacent discourse relations. Marcu and Echihabi (2002) present the first approach focused on identifying implicit discourse relations. They exploit several discourse markers (e.g. ‘because’ or ‘but’) for collecting large amounts of training data. For their experiments they remove the discourse markers and discover that word pair features are indicative for implicit discourse relations. Depending on the utilized corpus, they obtain accuracies between 64% and</context>
</contexts>
<marker>Teufel, 1999</marker>
<rawString>Simone Teufel. 1999. Argumentative Zoning: Information Extraction from Scientific Text. Ph.D. thesis, University of Edinburgh.</rawString>
</citation>
<citation valid="true">
<authors>
<author>Kristina Toutanova</author>
<author>Dan Klein</author>
<author>Christopher D Manning</author>
<author>Yoram Singer</author>
</authors>
<title>Feature-rich part-ofspeech tagging with a cyclic dependency network.</title>
<date>2003</date>
<booktitle>In Proceedings of the 2003 Conference of the North American Chapter of the Association for Computational Linguistics on Human Language Technology, NAACL ’03,</booktitle>
<pages>173--180</pages>
<location>Edmonton, Canada.</location>
<contexts>
<context position="17342" citStr="Toutanova et al., 2003" startWordPosition="2646" endWordPosition="2649">orted by several premises for establishing a stable standpoint. Table 1: Class distribution among the instances. The corpus contains 1552 argument components and 327 non-argumentative instances. For our experiments, we randomly split the data into a 80% training set and a 20% test set with the same class distribution and determine the best performing system using 10-fold cross-validation on the training set only. In our experiments, we use several classifiers (see section 4.2) from the Weka data mining software (Hall et al., 2009). For preprocessing the corpus, we use the Stanford POS-Tagger (Toutanova et al., 2003) and Parser (Klein and Manning, 2003) included in the DKPro Framework (Gurevych et al., 2007). After these steps, we use the DKPro-TC text classification framework (Daxenberger et al., 2014) for extracting the features described in the following section. 4.1 Features Structural features: We define structural features based on token statistics, the location and punctuations of the argument component and its covering sentence. Since Biran and Rambow (2011) found that premises are longer on the average than other sentences, we add the number of tokens of the argument component and its covering se</context>
</contexts>
<marker>Toutanova, Klein, Manning, Singer, 2003</marker>
<rawString>Kristina Toutanova, Dan Klein, Christopher D. Manning, and Yoram Singer. 2003. Feature-rich part-ofspeech tagging with a cyclic dependency network. In Proceedings of the 2003 Conference of the North American Chapter of the Association for Computational Linguistics on Human Language Technology, NAACL ’03, pages 173–180, Edmonton, Canada.</rawString>
</citation>
<citation valid="true">
<authors>
<author>Douglas N Walton</author>
</authors>
<title>Argumentation schemes for presumptive reasoning.</title>
<date>1996</date>
<publisher>Routledge.</publisher>
<contexts>
<context position="8119" citStr="Walton (1996)" startWordPosition="1213" endWordPosition="1214">chales-Palau and Moens, 2009; Teufel, 1999; Feng and Hirst, 2011), and (3) the identification of argumentation structures (Mochales-Palau and Moens, 2009; Wyner et al., 2010). The separation of argumentative from nonargumentative text units is usually considered as a binary classification task and constitutes one of the first steps in an argumentation mining pipeline. Moens et al. (2007) propose an approach for identifying argumentative sentences in the Araucaria corpus (Reed et al., 2008). The argument annotations in Araucaria are based on a domainindependent argumentation theory proposed by Walton (1996). In their experiments, they obtain the best accuracy (73.75%) using a combination of word pairs, text statistics, verbs, and a list of keywords indicative for argumentative discourse. Florou et al. (2013) report a similar approach. They classify text segments crawled with a focused crawler as either containing an argument or not. Their approach is based on several discourse markers and features extracted from the tense and mood of verbs. They report an F1-score of 0.764 for their best performing system. One of the first approaches focusing on the identification of argument components is Argum</context>
<context position="9565" citStr="Walton, 1996" startWordPosition="1451" endWordPosition="1452">hetorical roles including claim, result or purpose. The approach obtained an F1-score of 0.462 using structural, lexical and syntactic features. Rooney et al. (2013) also focus on the identification of argument components but in contrast to the work of Teufel (1999) their scheme is not tailored to a particular genre. In their experiments, they identify claims, premises and non-argumentative text units in the Araucaria corpus and report an overall accuracy of 65%. Feng and Hirst (2011) also use the Araucaria corpus for their experiments but focus on the identification of argumentation schemes (Walton, 1996), which are templates for forms of arguments (e.g. argument from example or argument from consequence). Since their approach is based on features extracted from mutual information of claims and premises, it requires that the argument components are reliably identified in advance. In their experiments, they achieve an accuracy between 62.9% and 97.9% depending on the particular scheme and the classification setup. In contrast to all approaches mentioned above, the work presented in this paper focuses besides the separation of argumentative from nonargumentative text units and the classification</context>
</contexts>
<marker>Walton, 1996</marker>
<rawString>Douglas N Walton. 1996. Argumentation schemes for presumptive reasoning. Routledge.</rawString>
</citation>
<citation valid="true">
<authors>
<author>Adam Wyner</author>
<author>Raquel Mochales Palau</author>
<author>Marie-Francine Moens</author>
<author>David Milward</author>
</authors>
<title>Approaches to text mining arguments from legal cases.</title>
<date>2010</date>
<booktitle>In Semantic Processing of Legal Texts,</booktitle>
<volume>6036</volume>
<pages>60--79</pages>
<contexts>
<context position="7680" citStr="Wyner et al., 2010" startWordPosition="1144" endWordPosition="1147">ive relations. Third, we evaluate several classifiers and feature groups for identifying the best system for both tasks. 2 Related Work 2.1 Argumentation Mining Previous research on argumentation mining spans several subtasks, including (1) the separation of argumentative from non-argumentative text units (Moens et al., 2007; Florou et al., 2013), (2) the classification of argument components or argumentation schemes (Rooney et al., 2012; Mochales-Palau and Moens, 2009; Teufel, 1999; Feng and Hirst, 2011), and (3) the identification of argumentation structures (Mochales-Palau and Moens, 2009; Wyner et al., 2010). The separation of argumentative from nonargumentative text units is usually considered as a binary classification task and constitutes one of the first steps in an argumentation mining pipeline. Moens et al. (2007) propose an approach for identifying argumentative sentences in the Araucaria corpus (Reed et al., 2008). The argument annotations in Araucaria are based on a domainindependent argumentation theory proposed by Walton (1996). In their experiments, they obtain the best accuracy (73.75%) using a combination of word pairs, text statistics, verbs, and a list of keywords indicative for a</context>
<context position="10469" citStr="Wyner et al., 2010" startWordPosition="1591" endWordPosition="1594">heir experiments, they achieve an accuracy between 62.9% and 97.9% depending on the particular scheme and the classification setup. In contrast to all approaches mentioned above, the work presented in this paper focuses besides the separation of argumentative from nonargumentative text units and the classification of argument components on the extraction of the argumentative discourse structure to identify which components of the argument belong together for achieving a more fine-grained and detailed analysis of argumentation. We are only aware of one approach (Mochales-Palau and Moens, 2009; Wyner et al., 2010) that also focuses on the identification of argumentative discourse structures. However, this approach is based on a manually created CFG that is tailored to documents from the legal domain, which follow a standardized argumentation style. Therefore, it does not accommodate illformatted arguments (Wyner et al., 2010), which are likely in argumentative writing support. In addition, the approach relies on discourse markers and is therefore not applicable for identifying implicit argumentative discourse structures. 2.2 Discourse Relations Identifying argumentative discourse structures is closely </context>
</contexts>
<marker>Wyner, Palau, Moens, Milward, 2010</marker>
<rawString>Adam Wyner, Raquel Mochales Palau, Marie-Francine Moens, and David Milward. 2010. Approaches to text mining arguments from legal cases. In Semantic Processing of Legal Texts, volume 6036 of Lecture Notes in Computer Science, pages 60–79.</rawString>
</citation>
</citationList>
</algorithm>
</algorithms>
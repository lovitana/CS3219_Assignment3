<?xml version="1.0" encoding="UTF-8"?>
<algorithms version="110505">
<algorithm name="SectLabel" version="110505">
<variant no="0" confidence="0.000006">
<title confidence="0.9905555">
Policy Learning for Domain Selection in an Extensible Multi-domain
Spoken Dialogue System
</title>
<author confidence="0.99069">
Zhuoran Wang
</author>
<affiliation confidence="0.849747">
Mathematical &amp; Computer Sciences
Heriot-Watt University
Edinburgh, UK
</affiliation>
<email confidence="0.996756">
zhuoran.wang@hw.ac.uk
</email>
<author confidence="0.5655905">
Hongliang Chen, Guanchun Wang
Hao Tian, Hua Wu†, Haifeng Wang
</author>
<affiliation confidence="0.570266">
Baidu Inc., Beijing, P. R. China
</affiliation>
<email confidence="0.949218">
SurnameForename@baidu.com
†wu hua@baidu.com
</email>
<sectionHeader confidence="0.99695" genericHeader="abstract">
Abstract
</sectionHeader>
<bodyText confidence="0.999715947368421">
This paper proposes a Markov Decision
Process and reinforcement learning based
approach for domain selection in a multi-
domain Spoken Dialogue System built on
a distributed architecture. In the proposed
framework, the domain selection prob-
lem is treated as sequential planning in-
stead of classification, such that confir-
mation and clarification interaction mech-
anisms are supported. In addition, it is
shown that by using a model parameter ty-
ing trick, the extensibility of the system
can be preserved, where dialogue com-
ponents in new domains can be easily
plugged in, without re-training the domain
selection policy. The experimental results
based on human subjects suggest that the
proposed model marginally outperforms a
non-trivial baseline.
</bodyText>
<sectionHeader confidence="0.999517" genericHeader="keywords">
1 Introduction
</sectionHeader>
<bodyText confidence="0.999805694915254">
Due to growing demand for natural human-
machine interaction, over the last decade Spo-
ken Dialogue Systems (SDS) have been increas-
ingly deployed in various commercial applications
ranging from traditional call centre automation
(e.g. AT&amp;T “Lets Go!” bus information sys-
tem (Williams et al., 2010)) to mobile personal
assistants and knowledge navigators (e.g. Ap-
ple’s Siri R�, Google NowTM, Microsoft Cortana,
etc.) or voice interaction for smart household ap-
pliance control (e.g. Samsung Evolution Kit for
Smart TVs). Furthermore, latest progress in open-
vocabulary Automatic Speech Recognition (ASR)
is pushing SDS from traditional single-domain in-
formation systems towards more complex multi-
domain speech applications, of which typical ex-
amples are those voice assistant mobile applica-
tions.
Recent advances in SDS have shown that sta-
tistical approaches to dialogue management can
result in marginal improvement in both the nat-
uralness and the task success rate for domain-
specific dialogues (Lemon and Pietquin, 2012;
Young et al., 2013). State-of-the-art statistical
SDS treat the dialogue problem as a sequential
decision making process, and employ established
planning models, such as Markov Decision Pro-
cesses (MDPs) (Singh et al., 2002) or Partially Ob-
servable Markov Decision Processes (POMDPs)
(Thomson and Young, 2010; Young et al., 2010;
Williams and Young, 2007), in conjunction with
reinforcement learning techniques (Jurˇc´ıˇcek et al.,
2011; Jurˇc´ıˇcek et al., 2012; Gaˇsi´c et al., 2013a)
to seek optimal dialogue policies that maximise
long-term expected (discounted) rewards and are
robust to ASR errors.
However, to the best of our knowledge, most of
the existing multi-domain SDS in public use are
rule-based (e.g. (Gruber et al., 2012; Mirkovic
and Cavedon, 2006)). The application of statistical
models in multi-domain dialogue systems is still
preliminary. Komatani et al. (2006) and Nakano
et al. (2011) utilised a distributed architecture (Lin
et al., 1999) to integrate expert dialogue systems in
different domains into a unified framework, where
a central controller trained as a data-driven clas-
sifier selects a domain expert at each turn to ad-
dress user’s query. Alternatively, Hakkani-T¨ur et
al. (2012) adopted the well-known Information
State mechanism (Traum and Larsson, 2003) to
construct a multi-domain SDS and proposed a dis-
criminative classification model for more accurate
state updates. More recently, Gaˇsi´c et al. (2013b)
proposed that by a simple expansion of the kernel
function in Gaussian Process (GP) reinforcement
learning (Engel et al., 2005; Gaˇsi´c et al., 2013a),
one can adapt pre-trained dialogue policies to han-
dle unseen slots for SDS in extended domains.
In this paper, we use a voice assistant applica-
</bodyText>
<page confidence="0.986877">
57
</page>
<note confidence="0.957288">
Proceedings of the 2014 Conference on Empirical Methods in Natural Language Processing (EMNLP), pages 57–67,
October 25-29, 2014, Doha, Qatar. c�2014 Association for Computational Linguistics
</note>
<figure confidence="0.411411">
Mobile Devices
</figure>
<figureCaption confidence="0.999788">
Figure 1: The distributed architecture of the voice assistant system (a simplified illustration).
</figureCaption>
<figure confidence="0.99900747826087">
Flight Ticket
Booking
Train Ticket
Booking
Hotel
Booking
SLU
Domain Expert
(Travel Info)
NLG
text
User Intention
Identifier
speech
Domain Expert
(Restaurant Search)
ASR
SLU
NLG
text, clicks
query,
intention label,
confidence
TTS
User Interface Manager
Web Page
Rendering
Central Controller
SLU
Domain Expert
(Movie Search)
NLG
etc.
SLU
Domain Expert
(etc.)
Service
Ranker
NLG
Weather
Report
Web
Search
etc.
QA
Out-of-domain Services
</figure>
<bodyText confidence="0.999575375">
tion (similar to Apple’s Siri but in Chinese lan-
guage) as an example to demonstrate a novel
MDP-based approach for central interaction man-
agement in a complex multi-domain dialogue sys-
tem. The voice assistant employs a distributed ar-
chitecture similar to (Lin et al., 1999; Komatani et
al., 2006; Nakano et al., 2011), and handles mixed
interactions of multi-turn dialogues across differ-
ent domains and single-turn queries powered by
a collection of information access services (such
as web search, Question Answering (QA), etc.).
In our system, the dialogues in each domain are
managed by an individual domain expert SDS, and
the single-turn services are used to handle those
so-called out-of-domain requests. We use fea-
turised representations to summarise the current
dialogue states in each domain (see Section 3 for
more details), and let the central controller (the
MDP model) choose one of the following system
actions at each turn: (1) addressing user’s query
based on a domain expert, (2) treating it as an
out-of-domain request, (3) asking user to confirm
whether he/she wants to continue a domain ex-
pert’s dialogue or to switch to out-of-domain ser-
vices, and (4) clarifying user’s intention between
two domains. The Gaussian Process Temporal
Difference (GPTD) algorithm (Engel et al., 2005;
Gaˇsi´c et al., 2013a) is adopted here for policy op-
timisation based on human subjects, where a pa-
rameter tying trick is applied to preserve the ex-
tensibility of the system, such that new domain
experts (dialogue systems) can be flexibly plugged
in without the need of re-training the central con-
troller.
Comparing to the previous classification-based
methods (Komatani et al., 2006; Nakano et al.,
2011), the proposed approach not only has the
advantage of action selection in consideration of
long-term rewards, it can also yield more robust
policies that allow clarifications and confirmations
to mitigate ASR and Spoken Language Under-
standing (SLU) errors. Our human evaluation re-
sults show that the proposed system with a trained
MDP policy achieves significantly better natural-
ness in domain switching tasks than a non-trivial
baseline with a hand-crafted policy.
The remainder of this paper is organised as
follows. Section 2 defines the terminology used
throughout the paper. Section 3 briefly overviews
the distributed architecture of our system. The
MDP model and the policy optimisation algorithm
are introduced in Section 4 and Section 5, respec-
tively. After this, experimental settings and eval-
uation results are described in Section 6. Finally,
we discuss some possible improvements in Sec-
tion 7 and conclude ourselves in Section 8.
</bodyText>
<sectionHeader confidence="0.9984" genericHeader="introduction">
2 Terminology
</sectionHeader>
<bodyText confidence="0.9996245">
A voice assistant application provides a unified
speech interface to a collection of individual infor-
mation access systems. It aims to collect and sat-
isfy user requests in an interactive manner, where
</bodyText>
<page confidence="0.996869">
58
</page>
<bodyText confidence="0.99992776">
different types of interactions can be involved.
Here we focus ourselves on two interaction scenar-
ios, i.e. task-oriented (multi-turn) dialogues and
single-turn queries.
According to user intentions, the dialogue inter-
actions in our voice assistant system can further be
categorised into different domains, of which each
is handled by a separate dialogue manager, namely
a domain expert. Example domains include travel
information, restaurant search, etc. In addition,
some domains in our system can be further de-
composed into sub-domains, e.g. the travel in-
formation domain consists of three sub-domains:
flight ticket booking, train ticket booking and hotel
reservation. We use an integrated domain expert to
address queries in all its sub-domains, so that rel-
evant information can be shared across those sub-
domains to allow intelligent induction in the dia-
logue flow.
For convenience of future reference, we call
those single-turn information access systems out-
of-domain services or simply services for short.
The services integrated in our system include web
search, semantic search, QA, system command ex-
ecution, weather report, chat-bot, and many more.
</bodyText>
<sectionHeader confidence="0.974067" genericHeader="method">
3 System Architecture
</sectionHeader>
<bodyText confidence="0.999986254545455">
The voice assistant system introduced in this pa-
per is built on a distributed architecture (Lin et al.,
1999), as shown in Figure 1, where the dialogue
flow is processed as follows. Firstly, a user’s query
(either an ASR utterance or directly typed in text)
is passed to a user intention identifier, which la-
bels the raw query with a list of intention hypothe-
ses with confidence scores. Here an intention label
could be either a domain name or a service name.
After this, the central controller distributes the raw
query together with its intention labels and confi-
dence scores to all the domain experts and the ser-
vice modules, which will attempt to process the
query and return their results to the central con-
troller.
The domain experts in the current implementa-
tion of our system are all rule-based SDS follow-
ing the RavenClaw framework proposed in (Bo-
hus and Rudnicky, 2009). When receiving a query,
a domain expert will use its own SLU module to
parse the utterance or text input and try to update
its dialogue state in consideration of both the SLU
output and the intention labels. If the dialogue
state in the domain expert can be updated given
the query, it will return its output, internal ses-
sion record and a confidence score to the central
controller, where the output can be either a natu-
ral language utterance realised by its Natural Lan-
guage Generation (NLG) module or a set of data
records obtained from its database (if a database
search operation is triggered), or both. If the do-
main expert cannot update its state using the cur-
rent query, it will just return an empty result with
a low confidence score. Similar procedures ap-
ply to those out-of-domain services as well, but
there are no session records or confidence scores
returned. Finally, given all the returned informa-
tion, the central controller chooses, according to
its policy, the module (either a domain expert or a
service) whose results will be provided to the user.
When the central controller decides to pass a
domain expert’s output to the user, we regard the
domain expert as being activated. Also note here,
the updated state of a domain expert in a turn will
not be physically stored, unless the domain expert
is activated in that turn. This is a necessary mech-
anism to prevent an inactive domain expert being
misled by ambiguous queries in other domains.
In addition, we use a well-engineered priority
ranker to rank the services based on the num-
bers of results they returned as well as some prior
knowledge about the quality of their data sources.
When the central controller decides to show user
the results from an out-of-domain service, it will
choose the top one from the ranked list.
</bodyText>
<sectionHeader confidence="0.8728395" genericHeader="method">
4 MDP Modelling of the Central Control
Process
</sectionHeader>
<bodyText confidence="0.999985352941176">
The main focus of this paper is to seek a policy for
robustly switching the control flow among those
domain experts and services (the service ranker in
practice) during a dialogue, where the user may
have multiple or compound goals (e.g. booking a
flight ticket, booking a restaurant in the destina-
tion city and checking the weather report of the
departure or destination city).
In order to make the system robust to ASR er-
rors or ambiguous queries, the central controller
should also have basic dialogue abilities for confir-
mation and clarification purposes. Here we define
the confirmation as an action of asking whether a
user wants to continue the dialogue in a certain do-
main. If the system receives a negative response at
this point, it will switch to out-of-domain services.
On the other hand, the clarification action is de-
</bodyText>
<page confidence="0.993334">
59
</page>
<bodyText confidence="0.999983259259259">
fined between domains, in which case, the system
will explicitly ask the user to choose between two
domain candidates before continuing the dialogue.
Due to the confirmation and clarification mech-
anisms defined above, the central controller be-
comes a sequential decision maker that must take
the overall smoothness of the dialogue into ac-
count. Therefore, we propose an MDP-based ap-
proach for learning an optimal central control pol-
icy in this section.
The potential state space of our MDP is huge,
which in principle consists of the combinations of
all possible situations of the domain experts and
the out-of-domain services, therefore function ap-
proximation techniques must be employed to en-
able tractable computations. However, when de-
veloping such a complex application as the voice
assistant here, one also needs to take the extensi-
bility of the system into account, so that new do-
main experts can be easily integrated into the sys-
tem without major re-training or re-engineering of
the existing components. Essentially, it requires
the state featurisation and the central control pol-
icy learnt here to be independent of the number of
domain experts. In Section 4.3, we show that such
a property can be achieved by a parameter tying
trick in the definition of the MDP.
</bodyText>
<subsectionHeader confidence="0.994214">
4.1 MDP Preliminaries
</subsectionHeader>
<bodyText confidence="0.998438323529412">
Let PX denote the set of probability distributions
over a set X. An MDP is defined as a five tuple
(5, A, T, R, -y), where the components are defined
as follows. 5 and A are the sets of system states
and actions, respectively. T : 5 x A —* PS is the
transition function, and T (s&apos;|s, a) defines the con-
ditional probability of the system transiting from
state s E 5 to state s&apos; E 5 after taking action
a E A. R : 5 x A —* PR is the reward function
with R(s, a) specifying the distribution of the im-
mediate rewards for the system taking action a at
state s. In addition, 0 &lt; -y &lt; 1 is the discount
factor on the summed sequence of rewards.
A finite-horizon MDP operates as follows. The
system occupies a state s and takes an action a,
which then will make it transit to a next state s&apos; —
T (·|s, a) and receive a reward r — R(s, a). This
process repeats until a terminal state is reached.
For a given policy 7r : 5 —* A, the value
function Vπ is defined to be the expected cumula-
tive reward, as V π(s0) = E [Ent=0 -ytrt|st,π(st)],
where s0 is the starting state and n is the plan-
ning horizon. The aim of policy optimisation is
to seek an optimal policy 7r* that maximises the
value function. If T and R are given, in conjunc-
tion with a Q-function, the optimal value V * can
be expressed by recursive equations as Q(s, a) =
R(s, a) + -y Es&apos;ES T(s&apos;|s,a)V *(s&apos;) and V *(s) =
maxaEA Q(s, a) (here we assume R(s, a) is de-
terministic), which can be solved by dynamic pro-
gramming (Bellman, 1957). For problems with
unknown T or R, such as dialogue systems, the
Q-values are usually estimated via reinforcement
learning (Sutton and Barto, 1998).
</bodyText>
<subsectionHeader confidence="0.996647">
4.2 Problem Definition
</subsectionHeader>
<bodyText confidence="0.999608818181818">
Let D denote the set of the domain experts in our
voice assistant system, and sd be the current di-
alogue state of domain expert d E D at a certain
timestamp. We also define so as an abstract state to
describe the current status of those out-of-domain
services. Then mathematically we can represent
the central control process as an MDP, where its
state s is a joint set of the states of all the domain
experts and the services, as s = {sd}dED U {so}.
Four types of system actions are defined as fol-
lows.
</bodyText>
<listItem confidence="0.9896265">
• present(d): presenting the output of do-
main expert d to user;
• present ood(null): presenting the re-
sults of the top-ranked out-of-domain service
given by the service ranker;
• confirm(d): confirming whether user
wants to continue with domain expert d (or
to switch to out-of-domain services);
• clarify(d,d&apos;): asking user to clarify
his/her intention between domains d and d&apos;.
</listItem>
<bodyText confidence="0.998596666666667">
For convenience of notations, we use a(x) to
denote a system action of our MDP, where a E
{present, present ood, confirm, clarify},
x E {d, null, (d, d&apos;)}d,d&apos;ED,d74d&apos;, x = null
only applies to present ood, and x = (d,d&apos;)
only applies to clarify actions.
</bodyText>
<subsectionHeader confidence="0.998822">
4.3 Function Approximation
</subsectionHeader>
<bodyText confidence="0.99964775">
Function approximation is a commonly used tech-
nique to estimate the Q-values when the state
space of the MDP is huge. Concretely, in our case,
we assume that:
</bodyText>
<equation confidence="0.602965">
Q(s, a(x)) = f(O(s, a(x)); θ) (1)
</equation>
<page confidence="0.94904">
60
</page>
<bodyText confidence="0.9998526">
where 0 : S x A → RK is a feature function
that maps a state-action pair to an K-dimensional
feature vector, and f : RK → R is a function of
0(s, a(x)) parameterised by B. A frequent choice
of f is the linear function, as:
</bodyText>
<equation confidence="0.596262">
Q(s, a(x)) = BT0(s, a(x)) (2)
</equation>
<bodyText confidence="0.999987357142857">
After this, the policy optimisation problem be-
comes learning the parameter B to approximate the
Q-values based on example dialogue trajectories.
However, a crucial problem with the standard
formulation in Eq. (2) is that the feature function
0 is defined over the entire state and action spaces.
In this case, when a new domain expert is inte-
grated into the system, both the state space and the
action space will be changed, therefore one will
have to re-define the feature function and conse-
quentially re-train the model. In order to achieve
an extensible system, we make some simplifica-
tion assumptions and decompose the feature func-
tion as follows. Firstly, we let:
</bodyText>
<equation confidence="0.9981944">
0(s, a(x)) = 0a(sx) (3)
0pr(sd) if a(x) =present(d)
0ood(so) if a(x) =present ood()
0cf(sd) if a(x) =confirm(d)
0cl(sd, sdI) if a(x) =clarify(d,d�)
</equation>
<bodyText confidence="0.998326">
where the feature function is reduced to only de-
pend on the state of the action’s operand, instead
of the entire system state. Then, we make those ac-
tions a(x) that have a same action type (a) but op-
erate different domain experts (x) share the same
parameter, i.e.:
</bodyText>
<equation confidence="0.829306">
Q(s, a(x)) = BTa 0a(sx) (4)
</equation>
<bodyText confidence="0.999473">
This decomposition and parameter tying trick pre-
serves the extensibility of the system, because both
BTa and 0a are independent of x, when there is a
new domain expert ˜d, we can directly substitute
its state sd˜ into Eq. (3) and (4) to compute its cor-
responding Q-values.
</bodyText>
<subsectionHeader confidence="0.904671">
4.4 Features
</subsectionHeader>
<bodyText confidence="0.990001142857143">
Based on the problem formulation in Eq. (3) and
(4), we shall only select high-level summary fea-
tures to sketch the dialogue state and dialogue his-
tory of each domain expert, which must be ap-
plicable to all domain experts, regardless of their
domain-specific characteristics or implementation
differences. Suppose that the dialogue states of the
</bodyText>
<table confidence="0.999783066666667">
# Feature Range
1 the number of unfilled {0, ... , M}
required slots of a domain
expert
2 the number of filled required {0, ... , M}
slots of a domain expert
3 the number of filled optional {0, ... , L}
slots of a domain expert
4 whether a domain expert has {0,1}
executed a database search
5 the confidence score [0,1.2]
returned by a domain expert
6 the total number of turns that Z+
a domain expert has been
activated during a dialogue
7 [0, 1]
e−ta where ta denotes the
relative turn of a domain
expert being last activated,
or 0 if not applicable
8 [0, 1]
e−tc where tc denotes the
relative turn of a domain
expert being last confirmed,
or 0 if not applicable
9 [0,1.2N]
the summed confidence
score from the user intention
identifier of a query being
for out-of-domain services
</table>
<tableCaption confidence="0.999542">
Table 1: A list of all features used in our model.
</tableCaption>
<bodyText confidence="0.980384666666667">
M and L respectively denote the maximum num-
bers of required and optional slots for the domain
experts. N is the maximum number of hypotheses
that the intention identifier can return. Z+ stands
for the non-negative integer set.
domain experts can be represented as slot-value
pairs1, and for each domain there are required slots
and optional slots, where all required slots must
be filled before the domain expert can execute a
database search operation. The features investi-
gated in the proposed framework are listed in Ta-
ble 1.
Detailed featurisation in Eq. (3) is explained
as follows. For 0pr, we choose the first 8 fea-
tures plus a bias dimension that is always set to
</bodyText>
<footnote confidence="0.940238375">
1This is a rather general assumption. Informally speak-
ing, for most task-oriented SDS, one can extract a slot-value
representation from their dialogue models, of which exam-
ples include the RavenClaw architecture (Bohus and Rud-
nicky, 2009), the Information State dialogue engine (Traum
and Larsson, 2003), MDP-SDS (Singh et al., 2002) or
POMDP-SDS (Thomson and Young, 2010; Young et al.,
2010; Williams and Young, 2007).
</footnote>
<note confidence="0.22878">
{
</note>
<page confidence="0.991059">
61
</page>
<bodyText confidence="0.999819625">
-1. Whilst, feature #9 plus a bias is used to de-
fine 0ood. All the features are used in 0, as to
do a confirmation, one needs to consider the joint
situation in and out of the domain. Finally, the
feature function for a clarification action between
two domains d and d&apos; is defined as 0.,(sd, sd&apos;) =
exp{—|0p (sd) — 0p.(sd&apos;)|1, where we use  |� |
to denote the element-wise absolute of a vector
operand. The intuition here is that the more dis-
tinguishable the (featurised) states of two domain
experts are, the less we tend to clarify them.
For those domain experts that have multiple
sub-domains with different numbers of required
and optional slots, the feature extraction procedure
only applies to the latest active sub-domain.
In addition, note that, the confidence scores pro-
vided by the user intention identifier are only used
as features for out-of-domain services. This is be-
cause in the current version of our system, the con-
fidence estimation of the intention identifier for
domain-dependent dialogue queries is less reliable
due to the lack of context information. In contrast,
the confidence scores returned by the domain ex-
perts will be more informative at this point.
</bodyText>
<sectionHeader confidence="0.992995" genericHeader="method">
5 Policy Learning with GPTD
</sectionHeader>
<bodyText confidence="0.999096815384616">
In traditional statistical SDS, dialogue policies are
usually trained using reinforcement learning based
on simulated dialogue trajectories (Schatzmann
et al., 2007; Keizer et al., 2010; Thomson and
Young, 2010; Young et al., 2010). Although the
evaluation of the simulators themselves could be
an arguable issue, there are various advantages,
e.g. hundreds of thousands of data examples can
be easily generated for training and initial policy
evaluation purposes, and different reinforcement
learning models can be compared without incur-
ring notable extra costs.
However, for more complex multi-domain SDS,
particularly a voice assistant application like ours
that aims at handling very complicated (ideally
open-domain) dialogue scenarios, it would be dif-
ficult to develop a proper simulator that can rea-
sonably mimic real human behaviours. There-
fore, in this work, we learn the central control
policy directly with human subjects, for which
the following properties of the learning algorithm
are required. Firstly and most importantly, the
learner must be sample-efficient as the data collec-
tion procedure is costly. Secondly, the algorithm
should support batch reinforcement learning. This
is because when using function approximation, the
learning process may not strictly converge, and the
quality of the sequence of generated policies tends
to oscillate after a certain number of improving
steps at the beginning (Bertsekas and Tsitsiklis,
1996). If online reinforcement learning is used,
we will be unable to evaluate the generated policy
after each update, and hence will not know which
policy to keep for the final evaluation. Therefore,
we do a batch policy update and iterate the learn-
ing process for a number of batches, such that the
data collection phase in a new iteration yields an
evaluation of the policy obtained from the previ-
ous iteration at the same time.
To fulfill the above two requirements, the Gaus-
sian Process Temporal Difference (GPTD) algo-
rithm (Engel et al., 2005) is a proper choice, due to
its sample efficiency (Fard et al., 2011) and batch
learning ability (Engel et al., 2005), as well as its
previous success in dialogue policy learning with
human subjects (Gaˇsi´c et al., 2013a). Note that,
GPTD can also admit recursive (online) compu-
tations, but here we focus ourselves on the batch
version.
A Gaussian Process (GP) is a generative model
of Bayesian inference that can be used for func-
tion regression, and has the superiority of obtain-
ing good posterior estimates with just a few obser-
vations (Rasmussen and Williams, 2006). GPTD
models the Q-function as a zero mean GP which
defines correlations in different parts of the fea-
turised state and action spaces through a kernel
function κ, as:
Q(s, a(x)) — !9P(0, κ((sx, a), (sx, a))) (5)
Given a sequence of t state-action pairs Xt =
[(s0, a0(x0)), ... , (st, at(xt))] from a collection
of dialogues and their corresponding immedi-
ate rewards rt = [r0, ... , rt], the posterior of
Q(s, a(x)) for an arbitrary new state-action pair
(s, a(x)) can be computed as:
</bodyText>
<equation confidence="0.991790555555556">
Q(s, a(x))|Xt,rt
— N (¯Q(s, a(x)), cov (s, a(x))) (6)
¯Q(s, a(x)) = kt(sx, a)THTt G 1
t rt (7)
cov (s, a(x)) = κ((sx, a), (sx, a))
— kt(sx, a)THTt G 1
t Htkt(sx, a) (8)
Gt = HtKtHTt + σ2HtHT (9)
t
</equation>
<page confidence="0.959667">
62
</page>
<bodyText confidence="0.9939591">
where Kt is the Gram matrix with elements
Kt(i,j) = K((sixi,ai),(sjxj,aj)), kt(sx,a) =
[K((sixi, ai), (sx, a))]ti=0 is a vector, and σ is a
hyperparameter specifying the diagonal covari-
ance values of the zero-mean Gaussian noise. In
addition, we use cov (s, a(x)) to denote (for short)
the self-covariance cov (s, a(x), s, a(x)).
In our case, as different feature functions Oa are
defined for different action types, the kernel func-
tion is defined to be:
</bodyText>
<equation confidence="0.943553">
K((sx, a), (s&apos;x1, a&apos;)) = [a = a&apos;]Ka(sx, s&apos;x1) (11)
</equation>
<bodyText confidence="0.999099764705882">
where [·] is an indicator function and Ka is the ker-
nel function defined corresponding to the feature
function Oa.
Given a state, a most straightforward policy is
to select the action that corresponds to the max-
imum mean Q-value estimated by the GP. How-
ever, since the objective is to learn the Q-function
associated with the optimal policy by interacting
directly with users, the policy must exhibit some
form of stochastic behaviour in order to explore
alternatives during the process of learning. In this
work, the strategy employed for the exploration-
exploitation trade-off is that, during exploration,
actions are chosen according to the variance of
the GP estimate for the Q-function, and during
exploitation, actions are chosen according to the
mean. That is:
</bodyText>
<equation confidence="0.999231">
π(s) = arg maxa(x) Q(s, a(x)) : w.p. 1 − c
{ arg maxa(x) cov (s, a(x)) : w.p. c
(12)
</equation>
<bodyText confidence="0.999948727272727">
where 0 &lt; c &lt; 1 is a pre-defined exploration rate,
and will be exponentially reduced at each batch
iteration during our learning process.
Note that, in practice, not all the actions are
valid at every possible state. For example, if a do-
main expert d has never been activated during a
dialogue and can neither process the user’s current
query, the actions with an operand d will be re-
garded as invalid at this state. When executing the
policy, we only consider those valid actions for a
given state.
</bodyText>
<figureCaption confidence="0.862083375">
Score Interpretation
5 The domain selections are totally
correct, and the entire dialogue flow
is fluent.
4 The domain selections are totally
correct, but the dialogue flow is
slightly redundant.
3 There are accidental domain
selections errors, or the dialogue
flow is perceptually redundant.
2 There are frequent domain selections
errors, or the dialogue flow is
intolerably redundant.
1 Most domain selections are
incorrect, or the dialogue is
incompletable.
</figureCaption>
<tableCaption confidence="0.986979">
Table 2: The scoring standard in our experiments.
</tableCaption>
<sectionHeader confidence="0.997139" genericHeader="method">
6 Experimental Results
</sectionHeader>
<subsectionHeader confidence="0.983073">
6.1 Training
</subsectionHeader>
<bodyText confidence="0.999982956521739">
We use the batch version of GPTD as described
in Section 5 to learn the central control policy
with human subjects. There are three domain ex-
perts available in our current system, but during
the training only two domains are used, which are
the travel information domain and the restaurant
search domain. We reserve a movie search domain
for evaluating the generalisation property of the
learnt policy (see Section 6.2). The learning pro-
cess started from a hand-crafted policy. Then 15
experienced users2 volunteered to contribute dia-
logue examples with multiple or compound goals
(see Figure 4 for an instance), from whom we
collected around 50∼70 dialogues per day for 5
days3. After each dialogue, the users were asked
to score the system from 5 to 1 according to a scor-
ing standard shown in Table 2. The scores were
taken as the (delayed) rewards to train the GPTD
model, where we set the rewards for intermediate
turns to 0. The working policy was updated daily
based on the data obtained up to that day. The
data collected on the first day was used for pre-
liminary experiments to choose the hyperparame-
</bodyText>
<footnote confidence="0.98681775">
2Overall user satisfactions may rely on various aspects of
the entire system, e.g. the data source quality of the services,
the performance of each domain expert, etc. It will be diffi-
cult to make non-experienced users to score the central con-
troller isolatedly.
3Not all the users participated the experiments everyday.
There were 311 valid dialogues received in total, with an av-
erage length of 9 turns.
</footnote>
<figure confidence="0.984769785714286">
1 −&apos;y ··· 0 0
0 1 ··· 0 0
...
..
. .. ..
. .
0 ··· 0 1 −&apos;y
...
⎡
⎢ ⎢ ⎢ ⎣
Ht =
⎤
⎦⎥⎥⎥
(10)
</figure>
<page confidence="0.927811">
63
</page>
<figureCaption confidence="0.988986">
Figure 2: Average scores and standard deviations
during policy iteration.
</figureCaption>
<figure confidence="0.99979">
0.9
0.88
0.86
0.84
0.82
0.8
0.78
0.76
0.74
0.72
0.7
</figure>
<figureCaption confidence="0.9833065">
Figure 3: Domain selection accuracies during pol-
icy iteration.
</figureCaption>
<bodyText confidence="0.999968833333333">
ters of the model, such as the kernel function, the
kernel parameters (if applicable), and Q and -y in
the GPTD model. We initially experimented with
linear, polynomial and Gaussian kernels, with dif-
ferent configurations of Q and -y values, as well
as kernel parameter values. It was found that
the linear kernel in combination with Q = 5 and
-y = 0.99 works more appropriate than the other
settings. This configuration was then fixed for the
rest iterations.
The learning process was iterated for 4 days af-
ter the first one. On each day, we computed the
mean and standard deviation of the user scores
as an evaluation of the policy learnt on the pre-
vious day. The learning curve is illustrated in Fig-
ure 2. Note here, as we were actually executing a
stochastic policy according to Eq. (12), to calcu-
late the values in Figure 2 we ignored those dia-
logues that contain any actions selected due to the
exploration. We also manually labelled the cor-
rectness of domain selection at every turn of the
dialogues. The domain selection accuracies of the
obtained policy sequence are shown in Figure 3,
where similarly, those exploration actions as well
</bodyText>
<table confidence="0.9985695">
Scenario Policy p-value
Baseline GPTD
4.5±0.8 4.2±0.8 0.387
3.4±0.9 4.2±0.8 0.018
4.1±1.0 4.3±1.0 0.0821
3.9±1.1 4.5±0.8 0.0440
</table>
<tableCaption confidence="0.999208">
Table 3: Paired comparison experiments between
</tableCaption>
<bodyText confidence="0.9654771875">
the system with a trained GPTD policy and the
rule-based baseline.
as the clarification and confirmation actions were
excluded from the calculations. Although the do-
main selection accuracy is not the target that our
learning algorithm aims to optimise, it reflects the
quality of the learnt policies from a different angle
of view.
It can be found in Figure 2 that the best policy
was obtained in the third iteration, and after that
the policy quality oscillated. The same finding is
indicated in Figure 3 as well, when we use the do-
main selection accuracy as the evaluation metric.
Therefore, we kept the policy corresponding to the
peak point of the learning curve for the compari-
son experiments below.
</bodyText>
<subsectionHeader confidence="0.999738">
6.2 Comparison Experiments
</subsectionHeader>
<bodyText confidence="0.999939958333333">
We conducted paired comparison experiments in
four scenarios to compare between the system
with the GPTD-learnt central control policy and a
non-trivial baseline. The baseline is a publicly de-
ployed version of the voice assistant application.
The central control policy of the baseline system is
handcrafted, which has a separate list of semantic
matching rules for each domain to enable domain
switching.
The first two scenarios aim to test the perfor-
mance of the two systems on (i) switching between
a domain expert and out-of-domain services, and
(ii) switching between two domain experts, where
only the two training domains (travel information
and restaurant search) were considered. Scenar-
ios (iii) and (iv) are similar to scenarios (i) and (ii)
respectively, but at this time, the users were re-
quired to carry out the tests surrounding the movie
search domain (which is addressed by a new do-
main expert not used in the training phase). There
were 13 users who participated this experiment.
In each scenario, every user was required to test
the two systems with an identical goal and similar
queries. After each test, the users were asked to
</bodyText>
<page confidence="0.8596992">
5
4
3
2
64
</page>
<bodyText confidence="0.999896142857143">
score the two systems separately according to the
scoring standard in Table 2.
The average scores received by the two systems
are shown in Table 3, where we also compute the
statistical significance (the p-values) of the results
based on paired t-tests. It can be found that the
learnt policy works significantly better than the
rule-based policy in scenarios (ii) and (iv), but in
scenarios (i) and (iii) the differences between two
systems are statistically insignificant. Moreover,
the learnt policy preserves the extensibility of the
entire system as expected, of which strong evi-
dences are given by the results in scenarios (iii)
and (iv).
</bodyText>
<subsectionHeader confidence="0.999534">
6.3 Policy Analysis
</subsectionHeader>
<bodyText confidence="0.999961085714286">
To better understand the policy learnt by the
GPTD model, we look into the obtained weight
vectors, as shown in Table 4. It can be found that
confidence score (#5) is an informative feature for
all the system actions, while the relative turn of a
domain being last activated (#7) is an additional
strong evidence for a confirmation decision. In
addition, the similarity between the dialogue com-
pletion status (#1 &amp; #2) of two ambiguous domain
experts and the relative turns of them being last
confirmed (#8) tend to be extra dominating fea-
tures for clarification decisions, besides the close-
ness of the confidence scores returned by the two
domain experts.
A less noticeable but important phenomenon is
observed for feature #6, i.e. the total number of
active turns of a domain expert during a dialogue.
Concretely, feature #6 has a small negative effect
on presentation actions but a small positive con-
tribution to confirmation actions. Such weights
could correspond to the discount factor’s penalty
to long dialogues in the value function. How-
ever, it implies an unexpected effect in extreme
cases, which we explain in detail as follows. Al-
though the absolute weights for feature #6 are tiny
for both presentation and confirmation actions, the
feature value will grow linearly during a dialogue.
Therefore, when a dialogue in a certain domain
last rather long, there tend to be very frequent con-
firmations. A possible solution to this problem
could be either ignoring feature #6 or twisting it to
some nonlinear function, such that its value stops
increasing at a certain threshold point. In addition,
to cover sufficient amount of those “extreme” ex-
amples in the training phase could also be an alter-
</bodyText>
<table confidence="0.9997135">
# Feature Weights
present confirm clarify
1 0.09 0.02 0.60 present ood
2 0.20 0.29 0.53
3 0.18 0.29 0.16
4 -0.10 0.16 0.25
5 0.75 0.57 0.54
6 -0.02 0.11 0.13
7 0.25 1.19 0.36
8 -0.22 -0.19 0.69
9 – 0.20 – 0.47
Bias -1.79 – – -2.42
</table>
<tableCaption confidence="0.995263">
Table 4: Feature weights learnt by GPTD. See Ta-
</tableCaption>
<bodyText confidence="0.860399">
ble 1 for the meanings of the features.
native solution, as our current training set contains
very few examples that exhibit extraordinary long
domain persistence.
</bodyText>
<sectionHeader confidence="0.999478" genericHeader="method">
7 Further Discussions
</sectionHeader>
<bodyText confidence="0.999988366666667">
The proposed approach is a rather general frame-
work to learn extensible central control policies
for multi-domain SDS based on distributed archi-
tectures. It does not rely on any internal represen-
tations of those individual domain experts, as long
as a unified featurisation of their dialogue states
can be achieved.
However, from the entire system point of view,
the current implementation is still preliminary.
Particularly, the confirmation and clarification
mechanisms are isolated, for which the surface re-
alisations sometimes may sound stiff. This phe-
nomenon explains one of the reasons that make
the proposed system slightly less preferred by the
users than the baseline in scenario (i), when the
interaction flows are relative simple. A possi-
ble improvement here could be associating the
confirmation and clarification actions in the cen-
tral controller to the error handling mechanisms
within each domain expert, and letting domain ex-
perts generate their own utterances on receiving a
confirmation/clarification request from the central
controller.
Online reinforcement learning with real user
cases will be another undoubted direction of fur-
ther improvement of our system. The key chal-
lenge here is to automatically estimate user’s satis-
factions, which will be transformed to the rewards
for the reinforcement learners. Strong feedbacks
such as clicks or actual order placements can be
</bodyText>
<page confidence="0.998729">
65
</page>
<bodyText confidence="0.99742875">
collected. But to regress user’s true satisfaction,
other environment features must also be taken into
account. Practical solutions are still an open issue
at this stage, and are left to our future work.
</bodyText>
<sectionHeader confidence="0.997798" genericHeader="conclusions">
8 Conclusion
</sectionHeader>
<bodyText confidence="0.999957615384616">
In this paper, we introduce an MDP framework
for learning domain selection policies in a com-
plex multi-domain SDS. Standard problem for-
mulation is modified with tied model parameters,
so that the entire system is extensible and new
domain experts can be easily integrated without
re-training the policy. This expectation is con-
firmed by empirical experiments with human sub-
jects, where the proposed system marginally beats
a non-trivial baseline and demonstrates proper ex-
tensibility. Several possible improvements are dis-
cussed, which will be the central arc of our future
research.
</bodyText>
<sectionHeader confidence="0.996921" genericHeader="acknowledgments">
Acknowledgements
</sectionHeader>
<bodyText confidence="0.999959625">
The research in this paper is supported by China’s
973 Programme no. 2014CB340505. The first au-
thor is partially funded by the EC FP7 programme
under grant agreement no. 287615 (PARLANCE)
and a SICSA PECE grant. The authors would also
like to thank Qiaoqiao She, Duo Cai and the HCI-
APP group at Baidu for volunteering to participate
in the human subject experiments.
</bodyText>
<sectionHeader confidence="0.999153" genericHeader="references">
References
</sectionHeader>
<reference confidence="0.994838111111111">
Richard Ernest Bellman. 1957. Dynamic Program-
ming. Princeton University Press, Princeton, NJ.
Dimitri P. Bertsekas and John N. Tsitsiklis. 1996.
Neuro-Dynamic Programming. Athena Scientific,
Belmont, MA.
Dan Bohus and Alexander I. Rudnicky. 2009. The
RavenClaw dialog management framework: Archi-
tecture and systems. Computer Speech and Lan-
guage, 23(3):332–361.
Yaakov Engel, Shie Mannor, and Ron Meir. 2005. Re-
inforcement learning with Gaussian processes. In
Proceedings of the 22nd International Conference
on Machine Learning (ICML), pages 201–208.
Mahdi Milani Fard, Joelle Pineau, and Csaba
Szepesv´ari. 2011. PAC-Bayesian policy evaluation
for reinforcement learning. In Proceedings of the
27th Conference on Uncertainty in Artificial Intelli-
gence (UAI), pages 195–202.
</reference>
<table confidence="0.962879375">
USER/ASR: 去青岛的机票 (Flight ticket to Qingdao.)
SYSTEM: 好的,到青岛的飞机票。请问您什么时候离开?
自哪儿出发啊?
(Okay, flight ticket to Qingdao. When are you
departing, and where are you departing from?)
[Travel Info. Domain Expert]
USER/ASR: 明天出发 (I’m leaving tomorrow.)
SYSTEM: 好的,明天到青岛的飞机票。您预计由哪儿出发啊?
(Okay, flight ticket to Qingdao, leaving
tomorrow. Where are you departing from?)
[Travel Info. Domain Expert]
USER/ASR: 北京 (Beijing)
SYSTEM: [Results Displayed - Travel Info. Domain Expert]
USER: 青岛有好吃的四川菜吗
(Are there any good Sichuan cuisines in Qingdao?)
ASR: 听到有好吃的酸的川菜吗
</table>
<reference confidence="0.812376722222222">
(ungrammatical utterance, partially with
“delicious” and “Sichuan food”.)
SYSTEM: 您要继续订机票吗?
(Do you want to continue with flight ticket booking?)
[Confirmation - Central Controller]
USER: 不是 (No)
SYSTEM: [Web Search Service: “听到有好吃的酸的川菜吗” ]
USER/ASR: 明天去青岛 (I’m going to Qingdao tomorrow.)
SYSTEM: [Results Displayed - Travel Info. Domain Expert]
USER/ASR: 青岛机场附近的餐厅 (Restaurants near Qingdao Airport)
SYSTEM: 为您找到以下餐厅
(I found the following restaurants for you.)
[Results Displayed - Restaurant Search Domain Expert]
USER/ASR: 青岛明天会下雨吗? (Will it rain in Qingdao tomorrow?)
SYSTEM: 青岛明天阵雨,温度16℃至20℃,东风5-6级
(Qingdao is rainy tomorrow. The temperature is 16 to 20
degree centigrade with moderate east wind.)
[Results Displayed - Weather Report Service]
</reference>
<figureCaption confidence="0.706006333333333">
Figure 4: An example dialogue containing multi-
ple user goals. The icons stand for graphical user
interface based nonverbal outputs.
</figureCaption>
<reference confidence="0.99882775">
Milica Gaˇsi´c, Catherine Breslin, Matthew Henderson,
Dongho Kim, Martin Szummer, Blaise Thomson,
Pirros Tsiakoulis, and Steve Young. 2013a. On-
line policy optimisation of Bayesian spoken dia-
logue systems via human interaction. In Proceed-
ings of the IEEE International Conference on Acous-
tics, Speech and Signal Processing (ICASSP), pages
8367–8371.
Milica Gaˇsi´c, Catherine Breslin, Matthew Hender-
son, Dongho Kim, Martin Szummer, Blaise Thom-
son, Pirros Tsiakoulis, and Steve Young. 2013b.
POMDP-based dialogue manager adaptation to ex-
tended domains. In Proceedings of the 14th annual
SIGdial Meeting on Discourse and Dialogue, pages
214–222.
Thomas Robert Gruber, Adam John Cheyer, Dag
</reference>
<page confidence="0.854431">
66
</page>
<reference confidence="0.99975534">
Kittlaus, Didier Rene Guzzoni, Christopher Dean
Brigham, Richard Donald Giuli, Marcello Bastea-
Forte, and Harry Joseph Saddler. 2012. Intelligent
automated assistant. United States Patent No. US
20120245944 A1.
Dilek Z. Hakkani-T¨ur, Gokhan T¨ur, Larry P. Heck,
Ashley Fidler, and Asli C¸elikyilmaz. 2012. A dis-
criminative classification-based approach to infor-
mation state updates for a multi-domain dialog sys-
tem. In Proceedings of the 13th Annual Conference
of the International Speech Communication Associ-
ation (INTERSPEECH).
Filip Jur&amp;quot;c´ı&amp;quot;cek, Blaise Thomson, and Steve Young.
2011. Natural actor and belief critic: Reinforcement
algorithm for learning parameters of dialogue sys-
tems modelled as POMDPs. ACM Transactions on
Speech and Language Processing, 7(3):6:1–6:25.
Filip Jur&amp;quot;c´ı&amp;quot;cek, Blaise Thomson, and Steve Young.
2012. Reinforcement learning for parameter esti-
mation in statistical spoken dialogue systems. Com-
puter Speech &amp; Language, 26(3):168–192.
Simon Keizer, Milica Ga&amp;quot;si´c, Filip Jur&amp;quot;c´ı&amp;quot;cek, Franc¸ois
Mairesse, Blaise Thomson, Kai Yu, and Steve
Young. 2010. Parameter estimation for agenda-
based user simulation. In Proceedings of the 11th
annual SIGdial Meeting on Discourse and Dialogue,
pages 116–123.
Kazunori Komatani, Naoyuki Kanda, Mikio Nakano,
Kazuhiro Nakadai, Hiroshi Tsujino, Tetsuya Ogata,
and Hiroshi G. Okuno. 2006. Multi-domain spo-
ken dialogue system with extensibility and robust-
ness against speech recognition errors. In Proceed-
ings of the 7th SIGdial Workshop on Discourse and
Dialogue, pages 9–17.
Oliver Lemon and Olivier Pietquin, editors. 2012.
Data-Driven Methods for Adaptive Spoken Dia-
logue Systems: Computational Learning for Conver-
sationalInterfaces. Springer.
Bor-shen Lin, Hsin-min Wang, and Lin-Shan Lee.
1999. A distributed architecture for cooperative
spoken dialogue agents with coherent dialogue state
and history. In Proceedings of the IEEE Automatic
Speech Recognition and Understanding Workshop
(ASRU).
Danilo Mirkovic and Lawrence Cavedon. 2006. Di-
alogue management using scripts. United States
Patent No. US 20060271351 A1.
Mikio Nakano, Shun Sato, Kazunori Komatani, Kyoko
Matsuyama, Kotaro Funakoshi, and Hiroshi G.
Okuno. 2011. A two-stage domain selection frame-
work for extensible multi-domain spoken dialogue
systems. In Proceedings of the 12th annual SIGdial
Meeting on Discourse and Dialogue, pages 18–29.
Carl Edward Rasmussen and Christopher K. I.
Williams, editors. 2006. Gaussian Processes for
Machine Learning. MIT Press.
Jost Schatzmann, Blaise Thomson, Karl Weilhammer,
Hui Ye, and Steve Young. 2007. Agenda-based
user simulation for bootstrapping a POMDP dia-
logue system. In Proceedings of Human Language
Technologies 2007: The Conference of the North
American Chapter of the Association for Compu-
tational Linguistics; Companion Volume, Short Pa-
pers, pages 149–152.
Satinder Singh, Diane Litman, Michael Kearns, and
Marilyn Walker. 2002. Optimizing dialogue man-
agement with reinforcement learning: Experiments
with the NJFun system. Journal of Artificial Intelli-
gence Research, 16(1):105–133.
Richard S. Sutton and Andrew G. Barto. 1998. Re-
inforcement Learning: An Introduction. MIT Press,
Cambridge, MA.
Blaise Thomson and Steve Young. 2010. Bayesian
update of dialogue state: A POMDP framework for
spoken dialogue systems. Computer Speech and
Language, 24(4):562–588.
David R. Traum and Staffan Larsson. 2003. The In-
formation State approach to dialogue management.
In Jan van Kuppevelt and Ronnie W. Smith, editors,
Current and New Directions in Discourse and Dia-
logue, pages 325–353. Springer.
Jason D. Williams and Steve Young. 2007. Partially
observable Markov decision processes for spoken
dialog systems. Computer Speech and Language,
21(2):393–422.
Jason D. Williams, Iker Arizmendi, and Alistair
Conkie. 2010. Demonstration of AT&amp;T “Let’s Go”:
A production-grade statistical spoken dialog system.
In Proceedings of the 3rd IEEE Workshop on Spoken
Language Technology (SLT).
Steve Young, Milica Ga&amp;quot;si´c, Simon Keizer, Franc¸ois
Mairesse, Jost Schatzmann, Blaise Thomson, and
Kai Yu. 2010. The Hidden Information State model:
a practical framework for POMDP-based spoken di-
alogue management. Computer Speech and Lan-
guage, 24(2):150–174.
Steve Young, Milica Ga&amp;quot;si´c, Blaise Thomson, and Ja-
son D. Williams. 2013. POMDP-based statistical
spoken dialogue systems: a review. Proceedings of
the IEEE, PP(99):1–20.
</reference>
<page confidence="0.999506">
67
</page>
</variant>
</algorithm>
<algorithm name="ParsHed" version="110505">
<variant no="0" confidence="0.249621">
<title confidence="0.998688">Policy Learning for Domain Selection in an Extensible Spoken Dialogue System</title>
<author confidence="0.940804">Zhuoran</author>
<affiliation confidence="0.808752">Mathematical &amp; Computer Heriot-Watt Edinburgh,</affiliation>
<email confidence="0.998273">zhuoran.wang@hw.ac.uk</email>
<author confidence="0.9504">Hongliang Chen</author>
<author confidence="0.9504">Guanchun</author>
<affiliation confidence="0.473663">Baidu Inc., Beijing, P. R.</affiliation>
<email confidence="0.997623">hua@baidu.com</email>
<abstract confidence="0.99789615">This paper proposes a Markov Decision Process and reinforcement learning based approach for domain selection in a multidomain Spoken Dialogue System built on a distributed architecture. In the proposed framework, the domain selection problem is treated as sequential planning instead of classification, such that confirmation and clarification interaction mechanisms are supported. In addition, it is shown that by using a model parameter tying trick, the extensibility of the system can be preserved, where dialogue components in new domains can be easily plugged in, without re-training the domain selection policy. The experimental results based on human subjects suggest that the proposed model marginally outperforms a non-trivial baseline.</abstract>
</variant>
</algorithm>
<algorithm name="ParsCit" version="110505">
<citationList>
<citation valid="true">
<authors>
<author>Richard Ernest Bellman</author>
</authors>
<title>Dynamic Programming.</title>
<date>1957</date>
<publisher>Princeton University Press,</publisher>
<location>Princeton, NJ.</location>
<contexts>
<context position="14993" citStr="Bellman, 1957" startWordPosition="2432" endWordPosition="2433">te is reached. For a given policy 7r : 5 —* A, the value function Vπ is defined to be the expected cumulative reward, as V π(s0) = E [Ent=0 -ytrt|st,π(st)], where s0 is the starting state and n is the planning horizon. The aim of policy optimisation is to seek an optimal policy 7r* that maximises the value function. If T and R are given, in conjunction with a Q-function, the optimal value V * can be expressed by recursive equations as Q(s, a) = R(s, a) + -y Es&apos;ES T(s&apos;|s,a)V *(s&apos;) and V *(s) = maxaEA Q(s, a) (here we assume R(s, a) is deterministic), which can be solved by dynamic programming (Bellman, 1957). For problems with unknown T or R, such as dialogue systems, the Q-values are usually estimated via reinforcement learning (Sutton and Barto, 1998). 4.2 Problem Definition Let D denote the set of the domain experts in our voice assistant system, and sd be the current dialogue state of domain expert d E D at a certain timestamp. We also define so as an abstract state to describe the current status of those out-of-domain services. Then mathematically we can represent the central control process as an MDP, where its state s is a joint set of the states of all the domain experts and the services,</context>
</contexts>
<marker>Bellman, 1957</marker>
<rawString>Richard Ernest Bellman. 1957. Dynamic Programming. Princeton University Press, Princeton, NJ.</rawString>
</citation>
<citation valid="true">
<authors>
<author>Dimitri P Bertsekas</author>
<author>John N Tsitsiklis</author>
</authors>
<title>Neuro-Dynamic Programming. Athena Scientific,</title>
<date>1996</date>
<location>Belmont, MA.</location>
<contexts>
<context position="23114" citStr="Bertsekas and Tsitsiklis, 1996" startWordPosition="3802" endWordPosition="3805">eal human behaviours. Therefore, in this work, we learn the central control policy directly with human subjects, for which the following properties of the learning algorithm are required. Firstly and most importantly, the learner must be sample-efficient as the data collection procedure is costly. Secondly, the algorithm should support batch reinforcement learning. This is because when using function approximation, the learning process may not strictly converge, and the quality of the sequence of generated policies tends to oscillate after a certain number of improving steps at the beginning (Bertsekas and Tsitsiklis, 1996). If online reinforcement learning is used, we will be unable to evaluate the generated policy after each update, and hence will not know which policy to keep for the final evaluation. Therefore, we do a batch policy update and iterate the learning process for a number of batches, such that the data collection phase in a new iteration yields an evaluation of the policy obtained from the previous iteration at the same time. To fulfill the above two requirements, the Gaussian Process Temporal Difference (GPTD) algorithm (Engel et al., 2005) is a proper choice, due to its sample efficiency (Fard </context>
</contexts>
<marker>Bertsekas, Tsitsiklis, 1996</marker>
<rawString>Dimitri P. Bertsekas and John N. Tsitsiklis. 1996. Neuro-Dynamic Programming. Athena Scientific, Belmont, MA.</rawString>
</citation>
<citation valid="true">
<authors>
<author>Dan Bohus</author>
<author>Alexander I Rudnicky</author>
</authors>
<title>The RavenClaw dialog management framework: Architecture and systems.</title>
<date>2009</date>
<journal>Computer Speech and Language,</journal>
<volume>23</volume>
<issue>3</issue>
<contexts>
<context position="9529" citStr="Bohus and Rudnicky, 2009" startWordPosition="1456" endWordPosition="1460">xt) is passed to a user intention identifier, which labels the raw query with a list of intention hypotheses with confidence scores. Here an intention label could be either a domain name or a service name. After this, the central controller distributes the raw query together with its intention labels and confidence scores to all the domain experts and the service modules, which will attempt to process the query and return their results to the central controller. The domain experts in the current implementation of our system are all rule-based SDS following the RavenClaw framework proposed in (Bohus and Rudnicky, 2009). When receiving a query, a domain expert will use its own SLU module to parse the utterance or text input and try to update its dialogue state in consideration of both the SLU output and the intention labels. If the dialogue state in the domain expert can be updated given the query, it will return its output, internal session record and a confidence score to the central controller, where the output can be either a natural language utterance realised by its Natural Language Generation (NLG) module or a set of data records obtained from its database (if a database search operation is triggered)</context>
<context position="20267" citStr="Bohus and Rudnicky, 2009" startWordPosition="3353" endWordPosition="3357">d for each domain there are required slots and optional slots, where all required slots must be filled before the domain expert can execute a database search operation. The features investigated in the proposed framework are listed in Table 1. Detailed featurisation in Eq. (3) is explained as follows. For 0pr, we choose the first 8 features plus a bias dimension that is always set to 1This is a rather general assumption. Informally speaking, for most task-oriented SDS, one can extract a slot-value representation from their dialogue models, of which examples include the RavenClaw architecture (Bohus and Rudnicky, 2009), the Information State dialogue engine (Traum and Larsson, 2003), MDP-SDS (Singh et al., 2002) or POMDP-SDS (Thomson and Young, 2010; Young et al., 2010; Williams and Young, 2007). { 61 -1. Whilst, feature #9 plus a bias is used to define 0ood. All the features are used in 0, as to do a confirmation, one needs to consider the joint situation in and out of the domain. Finally, the feature function for a clarification action between two domains d and d&apos; is defined as 0.,(sd, sd&apos;) = exp{—|0p (sd) — 0p.(sd&apos;)|1, where we use |� | to denote the element-wise absolute of a vector operand. The intuiti</context>
</contexts>
<marker>Bohus, Rudnicky, 2009</marker>
<rawString>Dan Bohus and Alexander I. Rudnicky. 2009. The RavenClaw dialog management framework: Architecture and systems. Computer Speech and Language, 23(3):332–361.</rawString>
</citation>
<citation valid="true">
<authors>
<author>Yaakov Engel</author>
<author>Shie Mannor</author>
<author>Ron Meir</author>
</authors>
<title>Reinforcement learning with Gaussian processes.</title>
<date>2005</date>
<booktitle>In Proceedings of the 22nd International Conference on Machine Learning (ICML),</booktitle>
<pages>201--208</pages>
<contexts>
<context position="3693" citStr="Engel et al., 2005" startWordPosition="542" endWordPosition="545">l., 1999) to integrate expert dialogue systems in different domains into a unified framework, where a central controller trained as a data-driven classifier selects a domain expert at each turn to address user’s query. Alternatively, Hakkani-T¨ur et al. (2012) adopted the well-known Information State mechanism (Traum and Larsson, 2003) to construct a multi-domain SDS and proposed a discriminative classification model for more accurate state updates. More recently, Gaˇsi´c et al. (2013b) proposed that by a simple expansion of the kernel function in Gaussian Process (GP) reinforcement learning (Engel et al., 2005; Gaˇsi´c et al., 2013a), one can adapt pre-trained dialogue policies to handle unseen slots for SDS in extended domains. In this paper, we use a voice assistant applica57 Proceedings of the 2014 Conference on Empirical Methods in Natural Language Processing (EMNLP), pages 57–67, October 25-29, 2014, Doha, Qatar. c�2014 Association for Computational Linguistics Mobile Devices Figure 1: The distributed architecture of the voice assistant system (a simplified illustration). Flight Ticket Booking Train Ticket Booking Hotel Booking SLU Domain Expert (Travel Info) NLG text User Intention Identifier</context>
<context position="5902" citStr="Engel et al., 2005" startWordPosition="881" endWordPosition="884">alled out-of-domain requests. We use featurised representations to summarise the current dialogue states in each domain (see Section 3 for more details), and let the central controller (the MDP model) choose one of the following system actions at each turn: (1) addressing user’s query based on a domain expert, (2) treating it as an out-of-domain request, (3) asking user to confirm whether he/she wants to continue a domain expert’s dialogue or to switch to out-of-domain services, and (4) clarifying user’s intention between two domains. The Gaussian Process Temporal Difference (GPTD) algorithm (Engel et al., 2005; Gaˇsi´c et al., 2013a) is adopted here for policy optimisation based on human subjects, where a parameter tying trick is applied to preserve the extensibility of the system, such that new domain experts (dialogue systems) can be flexibly plugged in without the need of re-training the central controller. Comparing to the previous classification-based methods (Komatani et al., 2006; Nakano et al., 2011), the proposed approach not only has the advantage of action selection in consideration of long-term rewards, it can also yield more robust policies that allow clarifications and confirmations t</context>
<context position="23658" citStr="Engel et al., 2005" startWordPosition="3896" endWordPosition="3899">number of improving steps at the beginning (Bertsekas and Tsitsiklis, 1996). If online reinforcement learning is used, we will be unable to evaluate the generated policy after each update, and hence will not know which policy to keep for the final evaluation. Therefore, we do a batch policy update and iterate the learning process for a number of batches, such that the data collection phase in a new iteration yields an evaluation of the policy obtained from the previous iteration at the same time. To fulfill the above two requirements, the Gaussian Process Temporal Difference (GPTD) algorithm (Engel et al., 2005) is a proper choice, due to its sample efficiency (Fard et al., 2011) and batch learning ability (Engel et al., 2005), as well as its previous success in dialogue policy learning with human subjects (Gaˇsi´c et al., 2013a). Note that, GPTD can also admit recursive (online) computations, but here we focus ourselves on the batch version. A Gaussian Process (GP) is a generative model of Bayesian inference that can be used for function regression, and has the superiority of obtaining good posterior estimates with just a few observations (Rasmussen and Williams, 2006). GPTD models the Q-function as</context>
</contexts>
<marker>Engel, Mannor, Meir, 2005</marker>
<rawString>Yaakov Engel, Shie Mannor, and Ron Meir. 2005. Reinforcement learning with Gaussian processes. In Proceedings of the 22nd International Conference on Machine Learning (ICML), pages 201–208.</rawString>
</citation>
<citation valid="true">
<authors>
<author>Mahdi Milani Fard</author>
<author>Joelle Pineau</author>
<author>Csaba Szepesv´ari</author>
</authors>
<title>PAC-Bayesian policy evaluation for reinforcement learning.</title>
<date>2011</date>
<booktitle>In Proceedings of the 27th Conference on Uncertainty in Artificial Intelligence (UAI),</booktitle>
<pages>195--202</pages>
<marker>Fard, Pineau, Szepesv´ari, 2011</marker>
<rawString>Mahdi Milani Fard, Joelle Pineau, and Csaba Szepesv´ari. 2011. PAC-Bayesian policy evaluation for reinforcement learning. In Proceedings of the 27th Conference on Uncertainty in Artificial Intelligence (UAI), pages 195–202.</rawString>
</citation>
<citation valid="false">
<title>partially with “delicious” and “Sichuan food”.) SYSTEM: 您要继续订机票吗? (Do you want to continue with flight ticket booking?) [Confirmation - Central Controller] USER: 不是 (No) SYSTEM: [Web Search Service: “听到有好吃的酸的川菜吗” ] USER/ASR: 明天去青岛 (I’m going to Qingdao tomorrow.) SYSTEM: [Results Displayed - Travel Info. Domain Expert] USER/ASR: 青岛机场附近的餐厅 (Restaurants near Qingdao Airport) SYSTEM: 为您找到以下餐厅 (I found the following restaurants for you.) [Results Displayed - Restaurant Search Domain Expert] USER/ASR: 青岛明天会下雨吗? (Will it rain in Qingdao tomorrow?) SYSTEM: 青岛明天阵雨,温度16℃至20℃,东风5-6级 (Qingdao is rainy tomorrow. The temperature is 16 to 20 degree centigrade with moderate east wind.) [Results Displayed - Weather Report Service]</title>
<marker></marker>
<rawString>(ungrammatical utterance, partially with “delicious” and “Sichuan food”.) SYSTEM: 您要继续订机票吗? (Do you want to continue with flight ticket booking?) [Confirmation - Central Controller] USER: 不是 (No) SYSTEM: [Web Search Service: “听到有好吃的酸的川菜吗” ] USER/ASR: 明天去青岛 (I’m going to Qingdao tomorrow.) SYSTEM: [Results Displayed - Travel Info. Domain Expert] USER/ASR: 青岛机场附近的餐厅 (Restaurants near Qingdao Airport) SYSTEM: 为您找到以下餐厅 (I found the following restaurants for you.) [Results Displayed - Restaurant Search Domain Expert] USER/ASR: 青岛明天会下雨吗? (Will it rain in Qingdao tomorrow?) SYSTEM: 青岛明天阵雨,温度16℃至20℃,东风5-6级 (Qingdao is rainy tomorrow. The temperature is 16 to 20 degree centigrade with moderate east wind.) [Results Displayed - Weather Report Service]</rawString>
</citation>
<citation valid="false">
<authors>
<author>Milica Gaˇsi´c</author>
<author>Catherine Breslin</author>
<author>Matthew Henderson</author>
<author>Dongho Kim</author>
<author>Martin Szummer</author>
</authors>
<title>Blaise Thomson, Pirros Tsiakoulis, and Steve Young. 2013a. Online policy optimisation of Bayesian spoken dialogue systems via human interaction.</title>
<booktitle>In Proceedings of the IEEE International Conference on Acoustics, Speech and Signal Processing (ICASSP),</booktitle>
<pages>8367--8371</pages>
<marker>Gaˇsi´c, Breslin, Henderson, Kim, Szummer, </marker>
<rawString>Milica Gaˇsi´c, Catherine Breslin, Matthew Henderson, Dongho Kim, Martin Szummer, Blaise Thomson, Pirros Tsiakoulis, and Steve Young. 2013a. Online policy optimisation of Bayesian spoken dialogue systems via human interaction. In Proceedings of the IEEE International Conference on Acoustics, Speech and Signal Processing (ICASSP), pages 8367–8371.</rawString>
</citation>
<citation valid="false">
<authors>
<author>Milica Gaˇsi´c</author>
<author>Catherine Breslin</author>
<author>Matthew Henderson</author>
<author>Dongho Kim</author>
<author>Martin Szummer</author>
</authors>
<title>Blaise Thomson, Pirros Tsiakoulis, and Steve Young. 2013b. POMDP-based dialogue manager adaptation to extended domains.</title>
<booktitle>In Proceedings of the 14th annual SIGdial Meeting on Discourse and Dialogue,</booktitle>
<pages>214--222</pages>
<marker>Gaˇsi´c, Breslin, Henderson, Kim, Szummer, </marker>
<rawString>Milica Gaˇsi´c, Catherine Breslin, Matthew Henderson, Dongho Kim, Martin Szummer, Blaise Thomson, Pirros Tsiakoulis, and Steve Young. 2013b. POMDP-based dialogue manager adaptation to extended domains. In Proceedings of the 14th annual SIGdial Meeting on Discourse and Dialogue, pages 214–222.</rawString>
</citation>
<citation valid="true">
<authors>
<author>Thomas Robert Gruber</author>
<author>Adam John Cheyer</author>
<author>Dag Kittlaus</author>
<author>Didier Rene Guzzoni</author>
<author>Christopher Dean Brigham</author>
<author>Richard Donald Giuli</author>
<author>Marcello BasteaForte</author>
<author>Harry Joseph Saddler</author>
</authors>
<title>Intelligent automated assistant. United States Patent No.</title>
<date>2012</date>
<journal>US</journal>
<volume>20120245944</volume>
<pages>1</pages>
<contexts>
<context position="2857" citStr="Gruber et al., 2012" startWordPosition="417" endWordPosition="420">ess, and employ established planning models, such as Markov Decision Processes (MDPs) (Singh et al., 2002) or Partially Observable Markov Decision Processes (POMDPs) (Thomson and Young, 2010; Young et al., 2010; Williams and Young, 2007), in conjunction with reinforcement learning techniques (Jurˇc´ıˇcek et al., 2011; Jurˇc´ıˇcek et al., 2012; Gaˇsi´c et al., 2013a) to seek optimal dialogue policies that maximise long-term expected (discounted) rewards and are robust to ASR errors. However, to the best of our knowledge, most of the existing multi-domain SDS in public use are rule-based (e.g. (Gruber et al., 2012; Mirkovic and Cavedon, 2006)). The application of statistical models in multi-domain dialogue systems is still preliminary. Komatani et al. (2006) and Nakano et al. (2011) utilised a distributed architecture (Lin et al., 1999) to integrate expert dialogue systems in different domains into a unified framework, where a central controller trained as a data-driven classifier selects a domain expert at each turn to address user’s query. Alternatively, Hakkani-T¨ur et al. (2012) adopted the well-known Information State mechanism (Traum and Larsson, 2003) to construct a multi-domain SDS and proposed</context>
</contexts>
<marker>Gruber, Cheyer, Kittlaus, Guzzoni, Brigham, Giuli, BasteaForte, Saddler, 2012</marker>
<rawString>Thomas Robert Gruber, Adam John Cheyer, Dag Kittlaus, Didier Rene Guzzoni, Christopher Dean Brigham, Richard Donald Giuli, Marcello BasteaForte, and Harry Joseph Saddler. 2012. Intelligent automated assistant. United States Patent No. US 20120245944 A1.</rawString>
</citation>
<citation valid="true">
<authors>
<author>Dilek Z Hakkani-T¨ur</author>
<author>Gokhan T¨ur</author>
<author>Larry P Heck</author>
<author>Ashley Fidler</author>
<author>Asli C¸elikyilmaz</author>
</authors>
<title>A discriminative classification-based approach to information state updates for a multi-domain dialog system.</title>
<date>2012</date>
<booktitle>In Proceedings of the 13th Annual Conference of the International Speech Communication Association (INTERSPEECH).</booktitle>
<marker>Hakkani-T¨ur, T¨ur, Heck, Fidler, C¸elikyilmaz, 2012</marker>
<rawString>Dilek Z. Hakkani-T¨ur, Gokhan T¨ur, Larry P. Heck, Ashley Fidler, and Asli C¸elikyilmaz. 2012. A discriminative classification-based approach to information state updates for a multi-domain dialog system. In Proceedings of the 13th Annual Conference of the International Speech Communication Association (INTERSPEECH).</rawString>
</citation>
<citation valid="true">
<authors>
<author>Filip Jurc´ıcek</author>
<author>Blaise Thomson</author>
<author>Steve Young</author>
</authors>
<title>Natural actor and belief critic: Reinforcement algorithm for learning parameters of dialogue systems modelled as POMDPs.</title>
<date>2011</date>
<journal>ACM Transactions on Speech and Language Processing,</journal>
<volume>7</volume>
<issue>3</issue>
<marker>Jurc´ıcek, Thomson, Young, 2011</marker>
<rawString>Filip Jur&amp;quot;c´ı&amp;quot;cek, Blaise Thomson, and Steve Young. 2011. Natural actor and belief critic: Reinforcement algorithm for learning parameters of dialogue systems modelled as POMDPs. ACM Transactions on Speech and Language Processing, 7(3):6:1–6:25.</rawString>
</citation>
<citation valid="true">
<authors>
<author>Filip Jurc´ıcek</author>
<author>Blaise Thomson</author>
<author>Steve Young</author>
</authors>
<title>Reinforcement learning for parameter estimation in statistical spoken dialogue systems.</title>
<date>2012</date>
<journal>Computer Speech &amp; Language,</journal>
<volume>26</volume>
<issue>3</issue>
<marker>Jurc´ıcek, Thomson, Young, 2012</marker>
<rawString>Filip Jur&amp;quot;c´ı&amp;quot;cek, Blaise Thomson, and Steve Young. 2012. Reinforcement learning for parameter estimation in statistical spoken dialogue systems. Computer Speech &amp; Language, 26(3):168–192.</rawString>
</citation>
<citation valid="true">
<authors>
<author>Simon Keizer</author>
<author>Milica Gasi´c</author>
<author>Filip Jurc´ıcek</author>
<author>Franc¸ois Mairesse</author>
<author>Blaise Thomson</author>
<author>Kai Yu</author>
<author>Steve Young</author>
</authors>
<title>Parameter estimation for agendabased user simulation.</title>
<date>2010</date>
<booktitle>In Proceedings of the 11th annual SIGdial Meeting on Discourse and Dialogue,</booktitle>
<pages>116--123</pages>
<marker>Keizer, Gasi´c, Jurc´ıcek, Mairesse, Thomson, Yu, Young, 2010</marker>
<rawString>Simon Keizer, Milica Ga&amp;quot;si´c, Filip Jur&amp;quot;c´ı&amp;quot;cek, Franc¸ois Mairesse, Blaise Thomson, Kai Yu, and Steve Young. 2010. Parameter estimation for agendabased user simulation. In Proceedings of the 11th annual SIGdial Meeting on Discourse and Dialogue, pages 116–123.</rawString>
</citation>
<citation valid="true">
<authors>
<author>Kazunori Komatani</author>
<author>Naoyuki Kanda</author>
<author>Mikio Nakano</author>
<author>Kazuhiro Nakadai</author>
<author>Hiroshi Tsujino</author>
<author>Tetsuya Ogata</author>
<author>Hiroshi G Okuno</author>
</authors>
<title>Multi-domain spoken dialogue system with extensibility and robustness against speech recognition errors.</title>
<date>2006</date>
<booktitle>In Proceedings of the 7th SIGdial Workshop on Discourse and Dialogue,</booktitle>
<pages>9--17</pages>
<contexts>
<context position="3004" citStr="Komatani et al. (2006)" startWordPosition="437" endWordPosition="440">on Processes (POMDPs) (Thomson and Young, 2010; Young et al., 2010; Williams and Young, 2007), in conjunction with reinforcement learning techniques (Jurˇc´ıˇcek et al., 2011; Jurˇc´ıˇcek et al., 2012; Gaˇsi´c et al., 2013a) to seek optimal dialogue policies that maximise long-term expected (discounted) rewards and are robust to ASR errors. However, to the best of our knowledge, most of the existing multi-domain SDS in public use are rule-based (e.g. (Gruber et al., 2012; Mirkovic and Cavedon, 2006)). The application of statistical models in multi-domain dialogue systems is still preliminary. Komatani et al. (2006) and Nakano et al. (2011) utilised a distributed architecture (Lin et al., 1999) to integrate expert dialogue systems in different domains into a unified framework, where a central controller trained as a data-driven classifier selects a domain expert at each turn to address user’s query. Alternatively, Hakkani-T¨ur et al. (2012) adopted the well-known Information State mechanism (Traum and Larsson, 2003) to construct a multi-domain SDS and proposed a discriminative classification model for more accurate state updates. More recently, Gaˇsi´c et al. (2013b) proposed that by a simple expansion o</context>
<context position="4898" citStr="Komatani et al., 2006" startWordPosition="724" endWordPosition="727">ntion Identifier speech Domain Expert (Restaurant Search) ASR SLU NLG text, clicks query, intention label, confidence TTS User Interface Manager Web Page Rendering Central Controller SLU Domain Expert (Movie Search) NLG etc. SLU Domain Expert (etc.) Service Ranker NLG Weather Report Web Search etc. QA Out-of-domain Services tion (similar to Apple’s Siri but in Chinese language) as an example to demonstrate a novel MDP-based approach for central interaction management in a complex multi-domain dialogue system. The voice assistant employs a distributed architecture similar to (Lin et al., 1999; Komatani et al., 2006; Nakano et al., 2011), and handles mixed interactions of multi-turn dialogues across different domains and single-turn queries powered by a collection of information access services (such as web search, Question Answering (QA), etc.). In our system, the dialogues in each domain are managed by an individual domain expert SDS, and the single-turn services are used to handle those so-called out-of-domain requests. We use featurised representations to summarise the current dialogue states in each domain (see Section 3 for more details), and let the central controller (the MDP model) choose one of</context>
<context position="6286" citStr="Komatani et al., 2006" startWordPosition="943" endWordPosition="946">rm whether he/she wants to continue a domain expert’s dialogue or to switch to out-of-domain services, and (4) clarifying user’s intention between two domains. The Gaussian Process Temporal Difference (GPTD) algorithm (Engel et al., 2005; Gaˇsi´c et al., 2013a) is adopted here for policy optimisation based on human subjects, where a parameter tying trick is applied to preserve the extensibility of the system, such that new domain experts (dialogue systems) can be flexibly plugged in without the need of re-training the central controller. Comparing to the previous classification-based methods (Komatani et al., 2006; Nakano et al., 2011), the proposed approach not only has the advantage of action selection in consideration of long-term rewards, it can also yield more robust policies that allow clarifications and confirmations to mitigate ASR and Spoken Language Understanding (SLU) errors. Our human evaluation results show that the proposed system with a trained MDP policy achieves significantly better naturalness in domain switching tasks than a non-trivial baseline with a hand-crafted policy. The remainder of this paper is organised as follows. Section 2 defines the terminology used throughout the paper</context>
</contexts>
<marker>Komatani, Kanda, Nakano, Nakadai, Tsujino, Ogata, Okuno, 2006</marker>
<rawString>Kazunori Komatani, Naoyuki Kanda, Mikio Nakano, Kazuhiro Nakadai, Hiroshi Tsujino, Tetsuya Ogata, and Hiroshi G. Okuno. 2006. Multi-domain spoken dialogue system with extensibility and robustness against speech recognition errors. In Proceedings of the 7th SIGdial Workshop on Discourse and Dialogue, pages 9–17.</rawString>
</citation>
<citation valid="true">
<authors>
<author>Oliver Lemon</author>
<author>Olivier Pietquin</author>
<author>editors</author>
</authors>
<date>2012</date>
<booktitle>Data-Driven Methods for Adaptive Spoken Dialogue Systems: Computational Learning for ConversationalInterfaces.</booktitle>
<publisher>Springer.</publisher>
<marker>Lemon, Pietquin, editors, 2012</marker>
<rawString>Oliver Lemon and Olivier Pietquin, editors. 2012. Data-Driven Methods for Adaptive Spoken Dialogue Systems: Computational Learning for ConversationalInterfaces. Springer.</rawString>
</citation>
<citation valid="true">
<authors>
<author>Bor-shen Lin</author>
<author>Hsin-min Wang</author>
<author>Lin-Shan Lee</author>
</authors>
<title>A distributed architecture for cooperative spoken dialogue agents with coherent dialogue state and history.</title>
<date>1999</date>
<booktitle>In Proceedings of the IEEE Automatic Speech Recognition and Understanding Workshop (ASRU).</booktitle>
<contexts>
<context position="3084" citStr="Lin et al., 1999" startWordPosition="450" endWordPosition="453">g, 2007), in conjunction with reinforcement learning techniques (Jurˇc´ıˇcek et al., 2011; Jurˇc´ıˇcek et al., 2012; Gaˇsi´c et al., 2013a) to seek optimal dialogue policies that maximise long-term expected (discounted) rewards and are robust to ASR errors. However, to the best of our knowledge, most of the existing multi-domain SDS in public use are rule-based (e.g. (Gruber et al., 2012; Mirkovic and Cavedon, 2006)). The application of statistical models in multi-domain dialogue systems is still preliminary. Komatani et al. (2006) and Nakano et al. (2011) utilised a distributed architecture (Lin et al., 1999) to integrate expert dialogue systems in different domains into a unified framework, where a central controller trained as a data-driven classifier selects a domain expert at each turn to address user’s query. Alternatively, Hakkani-T¨ur et al. (2012) adopted the well-known Information State mechanism (Traum and Larsson, 2003) to construct a multi-domain SDS and proposed a discriminative classification model for more accurate state updates. More recently, Gaˇsi´c et al. (2013b) proposed that by a simple expansion of the kernel function in Gaussian Process (GP) reinforcement learning (Engel et </context>
<context position="4875" citStr="Lin et al., 1999" startWordPosition="720" endWordPosition="723">NLG text User Intention Identifier speech Domain Expert (Restaurant Search) ASR SLU NLG text, clicks query, intention label, confidence TTS User Interface Manager Web Page Rendering Central Controller SLU Domain Expert (Movie Search) NLG etc. SLU Domain Expert (etc.) Service Ranker NLG Weather Report Web Search etc. QA Out-of-domain Services tion (similar to Apple’s Siri but in Chinese language) as an example to demonstrate a novel MDP-based approach for central interaction management in a complex multi-domain dialogue system. The voice assistant employs a distributed architecture similar to (Lin et al., 1999; Komatani et al., 2006; Nakano et al., 2011), and handles mixed interactions of multi-turn dialogues across different domains and single-turn queries powered by a collection of information access services (such as web search, Question Answering (QA), etc.). In our system, the dialogues in each domain are managed by an individual domain expert SDS, and the single-turn services are used to handle those so-called out-of-domain requests. We use featurised representations to summarise the current dialogue states in each domain (see Section 3 for more details), and let the central controller (the M</context>
<context position="8759" citStr="Lin et al., 1999" startWordPosition="1323" endWordPosition="1326">We use an integrated domain expert to address queries in all its sub-domains, so that relevant information can be shared across those subdomains to allow intelligent induction in the dialogue flow. For convenience of future reference, we call those single-turn information access systems outof-domain services or simply services for short. The services integrated in our system include web search, semantic search, QA, system command execution, weather report, chat-bot, and many more. 3 System Architecture The voice assistant system introduced in this paper is built on a distributed architecture (Lin et al., 1999), as shown in Figure 1, where the dialogue flow is processed as follows. Firstly, a user’s query (either an ASR utterance or directly typed in text) is passed to a user intention identifier, which labels the raw query with a list of intention hypotheses with confidence scores. Here an intention label could be either a domain name or a service name. After this, the central controller distributes the raw query together with its intention labels and confidence scores to all the domain experts and the service modules, which will attempt to process the query and return their results to the central </context>
</contexts>
<marker>Lin, Wang, Lee, 1999</marker>
<rawString>Bor-shen Lin, Hsin-min Wang, and Lin-Shan Lee. 1999. A distributed architecture for cooperative spoken dialogue agents with coherent dialogue state and history. In Proceedings of the IEEE Automatic Speech Recognition and Understanding Workshop (ASRU).</rawString>
</citation>
<citation valid="true">
<authors>
<author>Danilo Mirkovic</author>
<author>Lawrence Cavedon</author>
</authors>
<title>Dialogue management using scripts. United States Patent No.</title>
<date>2006</date>
<journal>US</journal>
<volume>20060271351</volume>
<pages>1</pages>
<contexts>
<context position="2886" citStr="Mirkovic and Cavedon, 2006" startWordPosition="421" endWordPosition="424">lished planning models, such as Markov Decision Processes (MDPs) (Singh et al., 2002) or Partially Observable Markov Decision Processes (POMDPs) (Thomson and Young, 2010; Young et al., 2010; Williams and Young, 2007), in conjunction with reinforcement learning techniques (Jurˇc´ıˇcek et al., 2011; Jurˇc´ıˇcek et al., 2012; Gaˇsi´c et al., 2013a) to seek optimal dialogue policies that maximise long-term expected (discounted) rewards and are robust to ASR errors. However, to the best of our knowledge, most of the existing multi-domain SDS in public use are rule-based (e.g. (Gruber et al., 2012; Mirkovic and Cavedon, 2006)). The application of statistical models in multi-domain dialogue systems is still preliminary. Komatani et al. (2006) and Nakano et al. (2011) utilised a distributed architecture (Lin et al., 1999) to integrate expert dialogue systems in different domains into a unified framework, where a central controller trained as a data-driven classifier selects a domain expert at each turn to address user’s query. Alternatively, Hakkani-T¨ur et al. (2012) adopted the well-known Information State mechanism (Traum and Larsson, 2003) to construct a multi-domain SDS and proposed a discriminative classificat</context>
</contexts>
<marker>Mirkovic, Cavedon, 2006</marker>
<rawString>Danilo Mirkovic and Lawrence Cavedon. 2006. Dialogue management using scripts. United States Patent No. US 20060271351 A1.</rawString>
</citation>
<citation valid="true">
<authors>
<author>Mikio Nakano</author>
<author>Shun Sato</author>
<author>Kazunori Komatani</author>
<author>Kyoko Matsuyama</author>
<author>Kotaro Funakoshi</author>
<author>Hiroshi G Okuno</author>
</authors>
<title>A two-stage domain selection framework for extensible multi-domain spoken dialogue systems.</title>
<date>2011</date>
<booktitle>In Proceedings of the 12th annual SIGdial Meeting on Discourse and Dialogue,</booktitle>
<pages>18--29</pages>
<contexts>
<context position="3029" citStr="Nakano et al. (2011)" startWordPosition="442" endWordPosition="445">son and Young, 2010; Young et al., 2010; Williams and Young, 2007), in conjunction with reinforcement learning techniques (Jurˇc´ıˇcek et al., 2011; Jurˇc´ıˇcek et al., 2012; Gaˇsi´c et al., 2013a) to seek optimal dialogue policies that maximise long-term expected (discounted) rewards and are robust to ASR errors. However, to the best of our knowledge, most of the existing multi-domain SDS in public use are rule-based (e.g. (Gruber et al., 2012; Mirkovic and Cavedon, 2006)). The application of statistical models in multi-domain dialogue systems is still preliminary. Komatani et al. (2006) and Nakano et al. (2011) utilised a distributed architecture (Lin et al., 1999) to integrate expert dialogue systems in different domains into a unified framework, where a central controller trained as a data-driven classifier selects a domain expert at each turn to address user’s query. Alternatively, Hakkani-T¨ur et al. (2012) adopted the well-known Information State mechanism (Traum and Larsson, 2003) to construct a multi-domain SDS and proposed a discriminative classification model for more accurate state updates. More recently, Gaˇsi´c et al. (2013b) proposed that by a simple expansion of the kernel function in </context>
<context position="4920" citStr="Nakano et al., 2011" startWordPosition="728" endWordPosition="731"> Domain Expert (Restaurant Search) ASR SLU NLG text, clicks query, intention label, confidence TTS User Interface Manager Web Page Rendering Central Controller SLU Domain Expert (Movie Search) NLG etc. SLU Domain Expert (etc.) Service Ranker NLG Weather Report Web Search etc. QA Out-of-domain Services tion (similar to Apple’s Siri but in Chinese language) as an example to demonstrate a novel MDP-based approach for central interaction management in a complex multi-domain dialogue system. The voice assistant employs a distributed architecture similar to (Lin et al., 1999; Komatani et al., 2006; Nakano et al., 2011), and handles mixed interactions of multi-turn dialogues across different domains and single-turn queries powered by a collection of information access services (such as web search, Question Answering (QA), etc.). In our system, the dialogues in each domain are managed by an individual domain expert SDS, and the single-turn services are used to handle those so-called out-of-domain requests. We use featurised representations to summarise the current dialogue states in each domain (see Section 3 for more details), and let the central controller (the MDP model) choose one of the following system </context>
<context position="6308" citStr="Nakano et al., 2011" startWordPosition="947" endWordPosition="950"> to continue a domain expert’s dialogue or to switch to out-of-domain services, and (4) clarifying user’s intention between two domains. The Gaussian Process Temporal Difference (GPTD) algorithm (Engel et al., 2005; Gaˇsi´c et al., 2013a) is adopted here for policy optimisation based on human subjects, where a parameter tying trick is applied to preserve the extensibility of the system, such that new domain experts (dialogue systems) can be flexibly plugged in without the need of re-training the central controller. Comparing to the previous classification-based methods (Komatani et al., 2006; Nakano et al., 2011), the proposed approach not only has the advantage of action selection in consideration of long-term rewards, it can also yield more robust policies that allow clarifications and confirmations to mitigate ASR and Spoken Language Understanding (SLU) errors. Our human evaluation results show that the proposed system with a trained MDP policy achieves significantly better naturalness in domain switching tasks than a non-trivial baseline with a hand-crafted policy. The remainder of this paper is organised as follows. Section 2 defines the terminology used throughout the paper. Section 3 briefly ov</context>
</contexts>
<marker>Nakano, Sato, Komatani, Matsuyama, Funakoshi, Okuno, 2011</marker>
<rawString>Mikio Nakano, Shun Sato, Kazunori Komatani, Kyoko Matsuyama, Kotaro Funakoshi, and Hiroshi G. Okuno. 2011. A two-stage domain selection framework for extensible multi-domain spoken dialogue systems. In Proceedings of the 12th annual SIGdial Meeting on Discourse and Dialogue, pages 18–29.</rawString>
</citation>
<citation valid="true">
<date>2006</date>
<booktitle>Gaussian Processes for Machine Learning.</booktitle>
<editor>Carl Edward Rasmussen and Christopher K. I. Williams, editors.</editor>
<publisher>MIT Press.</publisher>
<contexts>
<context position="3004" citStr="(2006)" startWordPosition="440" endWordPosition="440">MDPs) (Thomson and Young, 2010; Young et al., 2010; Williams and Young, 2007), in conjunction with reinforcement learning techniques (Jurˇc´ıˇcek et al., 2011; Jurˇc´ıˇcek et al., 2012; Gaˇsi´c et al., 2013a) to seek optimal dialogue policies that maximise long-term expected (discounted) rewards and are robust to ASR errors. However, to the best of our knowledge, most of the existing multi-domain SDS in public use are rule-based (e.g. (Gruber et al., 2012; Mirkovic and Cavedon, 2006)). The application of statistical models in multi-domain dialogue systems is still preliminary. Komatani et al. (2006) and Nakano et al. (2011) utilised a distributed architecture (Lin et al., 1999) to integrate expert dialogue systems in different domains into a unified framework, where a central controller trained as a data-driven classifier selects a domain expert at each turn to address user’s query. Alternatively, Hakkani-T¨ur et al. (2012) adopted the well-known Information State mechanism (Traum and Larsson, 2003) to construct a multi-domain SDS and proposed a discriminative classification model for more accurate state updates. More recently, Gaˇsi´c et al. (2013b) proposed that by a simple expansion o</context>
</contexts>
<marker>2006</marker>
<rawString>Carl Edward Rasmussen and Christopher K. I. Williams, editors. 2006. Gaussian Processes for Machine Learning. MIT Press.</rawString>
</citation>
<citation valid="true">
<authors>
<author>Jost Schatzmann</author>
<author>Blaise Thomson</author>
<author>Karl Weilhammer</author>
<author>Hui Ye</author>
<author>Steve Young</author>
</authors>
<title>Agenda-based user simulation for bootstrapping a POMDP dialogue system.</title>
<date>2007</date>
<booktitle>In Proceedings of Human Language Technologies 2007: The Conference of the North American Chapter of the Association for Computational Linguistics; Companion Volume, Short Papers,</booktitle>
<pages>149--152</pages>
<contexts>
<context position="21826" citStr="Schatzmann et al., 2007" startWordPosition="3610" endWordPosition="3613">at, the confidence scores provided by the user intention identifier are only used as features for out-of-domain services. This is because in the current version of our system, the confidence estimation of the intention identifier for domain-dependent dialogue queries is less reliable due to the lack of context information. In contrast, the confidence scores returned by the domain experts will be more informative at this point. 5 Policy Learning with GPTD In traditional statistical SDS, dialogue policies are usually trained using reinforcement learning based on simulated dialogue trajectories (Schatzmann et al., 2007; Keizer et al., 2010; Thomson and Young, 2010; Young et al., 2010). Although the evaluation of the simulators themselves could be an arguable issue, there are various advantages, e.g. hundreds of thousands of data examples can be easily generated for training and initial policy evaluation purposes, and different reinforcement learning models can be compared without incurring notable extra costs. However, for more complex multi-domain SDS, particularly a voice assistant application like ours that aims at handling very complicated (ideally open-domain) dialogue scenarios, it would be difficult </context>
</contexts>
<marker>Schatzmann, Thomson, Weilhammer, Ye, Young, 2007</marker>
<rawString>Jost Schatzmann, Blaise Thomson, Karl Weilhammer, Hui Ye, and Steve Young. 2007. Agenda-based user simulation for bootstrapping a POMDP dialogue system. In Proceedings of Human Language Technologies 2007: The Conference of the North American Chapter of the Association for Computational Linguistics; Companion Volume, Short Papers, pages 149–152.</rawString>
</citation>
<citation valid="true">
<authors>
<author>Satinder Singh</author>
<author>Diane Litman</author>
<author>Michael Kearns</author>
<author>Marilyn Walker</author>
</authors>
<title>Optimizing dialogue management with reinforcement learning: Experiments with the NJFun system.</title>
<date>2002</date>
<journal>Journal of Artificial Intelligence Research,</journal>
<volume>16</volume>
<issue>1</issue>
<contexts>
<context position="2344" citStr="Singh et al., 2002" startWordPosition="339" endWordPosition="342">ditional single-domain information systems towards more complex multidomain speech applications, of which typical examples are those voice assistant mobile applications. Recent advances in SDS have shown that statistical approaches to dialogue management can result in marginal improvement in both the naturalness and the task success rate for domainspecific dialogues (Lemon and Pietquin, 2012; Young et al., 2013). State-of-the-art statistical SDS treat the dialogue problem as a sequential decision making process, and employ established planning models, such as Markov Decision Processes (MDPs) (Singh et al., 2002) or Partially Observable Markov Decision Processes (POMDPs) (Thomson and Young, 2010; Young et al., 2010; Williams and Young, 2007), in conjunction with reinforcement learning techniques (Jurˇc´ıˇcek et al., 2011; Jurˇc´ıˇcek et al., 2012; Gaˇsi´c et al., 2013a) to seek optimal dialogue policies that maximise long-term expected (discounted) rewards and are robust to ASR errors. However, to the best of our knowledge, most of the existing multi-domain SDS in public use are rule-based (e.g. (Gruber et al., 2012; Mirkovic and Cavedon, 2006)). The application of statistical models in multi-domain d</context>
<context position="20362" citStr="Singh et al., 2002" startWordPosition="3368" endWordPosition="3371">d before the domain expert can execute a database search operation. The features investigated in the proposed framework are listed in Table 1. Detailed featurisation in Eq. (3) is explained as follows. For 0pr, we choose the first 8 features plus a bias dimension that is always set to 1This is a rather general assumption. Informally speaking, for most task-oriented SDS, one can extract a slot-value representation from their dialogue models, of which examples include the RavenClaw architecture (Bohus and Rudnicky, 2009), the Information State dialogue engine (Traum and Larsson, 2003), MDP-SDS (Singh et al., 2002) or POMDP-SDS (Thomson and Young, 2010; Young et al., 2010; Williams and Young, 2007). { 61 -1. Whilst, feature #9 plus a bias is used to define 0ood. All the features are used in 0, as to do a confirmation, one needs to consider the joint situation in and out of the domain. Finally, the feature function for a clarification action between two domains d and d&apos; is defined as 0.,(sd, sd&apos;) = exp{—|0p (sd) — 0p.(sd&apos;)|1, where we use |� | to denote the element-wise absolute of a vector operand. The intuition here is that the more distinguishable the (featurised) states of two domain experts are, the</context>
</contexts>
<marker>Singh, Litman, Kearns, Walker, 2002</marker>
<rawString>Satinder Singh, Diane Litman, Michael Kearns, and Marilyn Walker. 2002. Optimizing dialogue management with reinforcement learning: Experiments with the NJFun system. Journal of Artificial Intelligence Research, 16(1):105–133.</rawString>
</citation>
<citation valid="true">
<authors>
<author>Richard S Sutton</author>
<author>Andrew G Barto</author>
</authors>
<title>Reinforcement Learning: An Introduction.</title>
<date>1998</date>
<publisher>MIT Press,</publisher>
<location>Cambridge, MA.</location>
<contexts>
<context position="15141" citStr="Sutton and Barto, 1998" startWordPosition="2453" endWordPosition="2456"> -ytrt|st,π(st)], where s0 is the starting state and n is the planning horizon. The aim of policy optimisation is to seek an optimal policy 7r* that maximises the value function. If T and R are given, in conjunction with a Q-function, the optimal value V * can be expressed by recursive equations as Q(s, a) = R(s, a) + -y Es&apos;ES T(s&apos;|s,a)V *(s&apos;) and V *(s) = maxaEA Q(s, a) (here we assume R(s, a) is deterministic), which can be solved by dynamic programming (Bellman, 1957). For problems with unknown T or R, such as dialogue systems, the Q-values are usually estimated via reinforcement learning (Sutton and Barto, 1998). 4.2 Problem Definition Let D denote the set of the domain experts in our voice assistant system, and sd be the current dialogue state of domain expert d E D at a certain timestamp. We also define so as an abstract state to describe the current status of those out-of-domain services. Then mathematically we can represent the central control process as an MDP, where its state s is a joint set of the states of all the domain experts and the services, as s = {sd}dED U {so}. Four types of system actions are defined as follows. • present(d): presenting the output of domain expert d to user; • prese</context>
</contexts>
<marker>Sutton, Barto, 1998</marker>
<rawString>Richard S. Sutton and Andrew G. Barto. 1998. Reinforcement Learning: An Introduction. MIT Press, Cambridge, MA.</rawString>
</citation>
<citation valid="true">
<authors>
<author>Blaise Thomson</author>
<author>Steve Young</author>
</authors>
<title>Bayesian update of dialogue state: A POMDP framework for spoken dialogue systems.</title>
<date>2010</date>
<journal>Computer Speech and Language,</journal>
<volume>24</volume>
<issue>4</issue>
<contexts>
<context position="2428" citStr="Thomson and Young, 2010" startWordPosition="351" endWordPosition="354">ch applications, of which typical examples are those voice assistant mobile applications. Recent advances in SDS have shown that statistical approaches to dialogue management can result in marginal improvement in both the naturalness and the task success rate for domainspecific dialogues (Lemon and Pietquin, 2012; Young et al., 2013). State-of-the-art statistical SDS treat the dialogue problem as a sequential decision making process, and employ established planning models, such as Markov Decision Processes (MDPs) (Singh et al., 2002) or Partially Observable Markov Decision Processes (POMDPs) (Thomson and Young, 2010; Young et al., 2010; Williams and Young, 2007), in conjunction with reinforcement learning techniques (Jurˇc´ıˇcek et al., 2011; Jurˇc´ıˇcek et al., 2012; Gaˇsi´c et al., 2013a) to seek optimal dialogue policies that maximise long-term expected (discounted) rewards and are robust to ASR errors. However, to the best of our knowledge, most of the existing multi-domain SDS in public use are rule-based (e.g. (Gruber et al., 2012; Mirkovic and Cavedon, 2006)). The application of statistical models in multi-domain dialogue systems is still preliminary. Komatani et al. (2006) and Nakano et al. (2011</context>
<context position="20400" citStr="Thomson and Young, 2010" startWordPosition="3374" endWordPosition="3377">cute a database search operation. The features investigated in the proposed framework are listed in Table 1. Detailed featurisation in Eq. (3) is explained as follows. For 0pr, we choose the first 8 features plus a bias dimension that is always set to 1This is a rather general assumption. Informally speaking, for most task-oriented SDS, one can extract a slot-value representation from their dialogue models, of which examples include the RavenClaw architecture (Bohus and Rudnicky, 2009), the Information State dialogue engine (Traum and Larsson, 2003), MDP-SDS (Singh et al., 2002) or POMDP-SDS (Thomson and Young, 2010; Young et al., 2010; Williams and Young, 2007). { 61 -1. Whilst, feature #9 plus a bias is used to define 0ood. All the features are used in 0, as to do a confirmation, one needs to consider the joint situation in and out of the domain. Finally, the feature function for a clarification action between two domains d and d&apos; is defined as 0.,(sd, sd&apos;) = exp{—|0p (sd) — 0p.(sd&apos;)|1, where we use |� | to denote the element-wise absolute of a vector operand. The intuition here is that the more distinguishable the (featurised) states of two domain experts are, the less we tend to clarify them. For tho</context>
<context position="21872" citStr="Thomson and Young, 2010" startWordPosition="3618" endWordPosition="3621"> intention identifier are only used as features for out-of-domain services. This is because in the current version of our system, the confidence estimation of the intention identifier for domain-dependent dialogue queries is less reliable due to the lack of context information. In contrast, the confidence scores returned by the domain experts will be more informative at this point. 5 Policy Learning with GPTD In traditional statistical SDS, dialogue policies are usually trained using reinforcement learning based on simulated dialogue trajectories (Schatzmann et al., 2007; Keizer et al., 2010; Thomson and Young, 2010; Young et al., 2010). Although the evaluation of the simulators themselves could be an arguable issue, there are various advantages, e.g. hundreds of thousands of data examples can be easily generated for training and initial policy evaluation purposes, and different reinforcement learning models can be compared without incurring notable extra costs. However, for more complex multi-domain SDS, particularly a voice assistant application like ours that aims at handling very complicated (ideally open-domain) dialogue scenarios, it would be difficult to develop a proper simulator that can reasona</context>
</contexts>
<marker>Thomson, Young, 2010</marker>
<rawString>Blaise Thomson and Steve Young. 2010. Bayesian update of dialogue state: A POMDP framework for spoken dialogue systems. Computer Speech and Language, 24(4):562–588.</rawString>
</citation>
<citation valid="true">
<authors>
<author>David R Traum</author>
<author>Staffan Larsson</author>
</authors>
<title>The Information State approach to dialogue management.</title>
<date>2003</date>
<booktitle>Current and New Directions in Discourse and Dialogue,</booktitle>
<pages>325--353</pages>
<editor>In Jan van Kuppevelt and Ronnie W. Smith, editors,</editor>
<publisher>Springer.</publisher>
<contexts>
<context position="3412" citStr="Traum and Larsson, 2003" startWordPosition="499" endWordPosition="502">ti-domain SDS in public use are rule-based (e.g. (Gruber et al., 2012; Mirkovic and Cavedon, 2006)). The application of statistical models in multi-domain dialogue systems is still preliminary. Komatani et al. (2006) and Nakano et al. (2011) utilised a distributed architecture (Lin et al., 1999) to integrate expert dialogue systems in different domains into a unified framework, where a central controller trained as a data-driven classifier selects a domain expert at each turn to address user’s query. Alternatively, Hakkani-T¨ur et al. (2012) adopted the well-known Information State mechanism (Traum and Larsson, 2003) to construct a multi-domain SDS and proposed a discriminative classification model for more accurate state updates. More recently, Gaˇsi´c et al. (2013b) proposed that by a simple expansion of the kernel function in Gaussian Process (GP) reinforcement learning (Engel et al., 2005; Gaˇsi´c et al., 2013a), one can adapt pre-trained dialogue policies to handle unseen slots for SDS in extended domains. In this paper, we use a voice assistant applica57 Proceedings of the 2014 Conference on Empirical Methods in Natural Language Processing (EMNLP), pages 57–67, October 25-29, 2014, Doha, Qatar. c�20</context>
<context position="20332" citStr="Traum and Larsson, 2003" startWordPosition="3363" endWordPosition="3366">re all required slots must be filled before the domain expert can execute a database search operation. The features investigated in the proposed framework are listed in Table 1. Detailed featurisation in Eq. (3) is explained as follows. For 0pr, we choose the first 8 features plus a bias dimension that is always set to 1This is a rather general assumption. Informally speaking, for most task-oriented SDS, one can extract a slot-value representation from their dialogue models, of which examples include the RavenClaw architecture (Bohus and Rudnicky, 2009), the Information State dialogue engine (Traum and Larsson, 2003), MDP-SDS (Singh et al., 2002) or POMDP-SDS (Thomson and Young, 2010; Young et al., 2010; Williams and Young, 2007). { 61 -1. Whilst, feature #9 plus a bias is used to define 0ood. All the features are used in 0, as to do a confirmation, one needs to consider the joint situation in and out of the domain. Finally, the feature function for a clarification action between two domains d and d&apos; is defined as 0.,(sd, sd&apos;) = exp{—|0p (sd) — 0p.(sd&apos;)|1, where we use |� | to denote the element-wise absolute of a vector operand. The intuition here is that the more distinguishable the (featurised) states </context>
</contexts>
<marker>Traum, Larsson, 2003</marker>
<rawString>David R. Traum and Staffan Larsson. 2003. The Information State approach to dialogue management. In Jan van Kuppevelt and Ronnie W. Smith, editors, Current and New Directions in Discourse and Dialogue, pages 325–353. Springer.</rawString>
</citation>
<citation valid="true">
<authors>
<author>Jason D Williams</author>
<author>Steve Young</author>
</authors>
<title>Partially observable Markov decision processes for spoken dialog systems.</title>
<date>2007</date>
<journal>Computer Speech and Language,</journal>
<volume>21</volume>
<issue>2</issue>
<contexts>
<context position="2475" citStr="Williams and Young, 2007" startWordPosition="359" endWordPosition="362">e those voice assistant mobile applications. Recent advances in SDS have shown that statistical approaches to dialogue management can result in marginal improvement in both the naturalness and the task success rate for domainspecific dialogues (Lemon and Pietquin, 2012; Young et al., 2013). State-of-the-art statistical SDS treat the dialogue problem as a sequential decision making process, and employ established planning models, such as Markov Decision Processes (MDPs) (Singh et al., 2002) or Partially Observable Markov Decision Processes (POMDPs) (Thomson and Young, 2010; Young et al., 2010; Williams and Young, 2007), in conjunction with reinforcement learning techniques (Jurˇc´ıˇcek et al., 2011; Jurˇc´ıˇcek et al., 2012; Gaˇsi´c et al., 2013a) to seek optimal dialogue policies that maximise long-term expected (discounted) rewards and are robust to ASR errors. However, to the best of our knowledge, most of the existing multi-domain SDS in public use are rule-based (e.g. (Gruber et al., 2012; Mirkovic and Cavedon, 2006)). The application of statistical models in multi-domain dialogue systems is still preliminary. Komatani et al. (2006) and Nakano et al. (2011) utilised a distributed architecture (Lin et a</context>
<context position="20447" citStr="Williams and Young, 2007" startWordPosition="3382" endWordPosition="3385">s investigated in the proposed framework are listed in Table 1. Detailed featurisation in Eq. (3) is explained as follows. For 0pr, we choose the first 8 features plus a bias dimension that is always set to 1This is a rather general assumption. Informally speaking, for most task-oriented SDS, one can extract a slot-value representation from their dialogue models, of which examples include the RavenClaw architecture (Bohus and Rudnicky, 2009), the Information State dialogue engine (Traum and Larsson, 2003), MDP-SDS (Singh et al., 2002) or POMDP-SDS (Thomson and Young, 2010; Young et al., 2010; Williams and Young, 2007). { 61 -1. Whilst, feature #9 plus a bias is used to define 0ood. All the features are used in 0, as to do a confirmation, one needs to consider the joint situation in and out of the domain. Finally, the feature function for a clarification action between two domains d and d&apos; is defined as 0.,(sd, sd&apos;) = exp{—|0p (sd) — 0p.(sd&apos;)|1, where we use |� | to denote the element-wise absolute of a vector operand. The intuition here is that the more distinguishable the (featurised) states of two domain experts are, the less we tend to clarify them. For those domain experts that have multiple sub-domain</context>
</contexts>
<marker>Williams, Young, 2007</marker>
<rawString>Jason D. Williams and Steve Young. 2007. Partially observable Markov decision processes for spoken dialog systems. Computer Speech and Language, 21(2):393–422.</rawString>
</citation>
<citation valid="true">
<authors>
<author>Jason D Williams</author>
<author>Iker Arizmendi</author>
<author>Alistair Conkie</author>
</authors>
<title>Demonstration of AT&amp;T “Let’s Go”: A production-grade statistical spoken dialog system.</title>
<date>2010</date>
<booktitle>In Proceedings of the 3rd IEEE Workshop on Spoken Language Technology (SLT).</booktitle>
<contexts>
<context position="1398" citStr="Williams et al., 2010" startWordPosition="199" endWordPosition="202">meter tying trick, the extensibility of the system can be preserved, where dialogue components in new domains can be easily plugged in, without re-training the domain selection policy. The experimental results based on human subjects suggest that the proposed model marginally outperforms a non-trivial baseline. 1 Introduction Due to growing demand for natural humanmachine interaction, over the last decade Spoken Dialogue Systems (SDS) have been increasingly deployed in various commercial applications ranging from traditional call centre automation (e.g. AT&amp;T “Lets Go!” bus information system (Williams et al., 2010)) to mobile personal assistants and knowledge navigators (e.g. Apple’s Siri R�, Google NowTM, Microsoft Cortana, etc.) or voice interaction for smart household appliance control (e.g. Samsung Evolution Kit for Smart TVs). Furthermore, latest progress in openvocabulary Automatic Speech Recognition (ASR) is pushing SDS from traditional single-domain information systems towards more complex multidomain speech applications, of which typical examples are those voice assistant mobile applications. Recent advances in SDS have shown that statistical approaches to dialogue management can result in marg</context>
</contexts>
<marker>Williams, Arizmendi, Conkie, 2010</marker>
<rawString>Jason D. Williams, Iker Arizmendi, and Alistair Conkie. 2010. Demonstration of AT&amp;T “Let’s Go”: A production-grade statistical spoken dialog system. In Proceedings of the 3rd IEEE Workshop on Spoken Language Technology (SLT).</rawString>
</citation>
<citation valid="true">
<authors>
<author>Steve Young</author>
<author>Milica Gasi´c</author>
<author>Simon Keizer</author>
<author>Franc¸ois Mairesse</author>
<author>Jost Schatzmann</author>
<author>Blaise Thomson</author>
<author>Kai Yu</author>
</authors>
<title>The Hidden Information State model: a practical framework for POMDP-based spoken dialogue management.</title>
<date>2010</date>
<journal>Computer Speech and Language,</journal>
<volume>24</volume>
<issue>2</issue>
<marker>Young, Gasi´c, Keizer, Mairesse, Schatzmann, Thomson, Yu, 2010</marker>
<rawString>Steve Young, Milica Ga&amp;quot;si´c, Simon Keizer, Franc¸ois Mairesse, Jost Schatzmann, Blaise Thomson, and Kai Yu. 2010. The Hidden Information State model: a practical framework for POMDP-based spoken dialogue management. Computer Speech and Language, 24(2):150–174.</rawString>
</citation>
<citation valid="true">
<authors>
<author>Steve Young</author>
<author>Milica Gasi´c</author>
<author>Blaise Thomson</author>
<author>Jason D Williams</author>
</authors>
<title>POMDP-based statistical spoken dialogue systems: a review.</title>
<date>2013</date>
<booktitle>Proceedings of the IEEE,</booktitle>
<pages>99--1</pages>
<marker>Young, Gasi´c, Thomson, Williams, 2013</marker>
<rawString>Steve Young, Milica Ga&amp;quot;si´c, Blaise Thomson, and Jason D. Williams. 2013. POMDP-based statistical spoken dialogue systems: a review. Proceedings of the IEEE, PP(99):1–20.</rawString>
</citation>
</citationList>
</algorithm>
</algorithms>
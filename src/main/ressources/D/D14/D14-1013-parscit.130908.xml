<?xml version="1.0" encoding="UTF-8"?>
<algorithms version="110505">
<algorithm name="SectLabel" version="110505">
<variant no="0" confidence="0.005958">
<title confidence="0.980133">
Combining Punctuation and Disfluency Prediction: An Empirical Study
</title>
<author confidence="0.999401">
Xuancong Wang&apos;,3 Khe Chai Sim&apos; Hwee Tou Ng&apos;,&apos;
</author>
<affiliation confidence="0.997136333333333">
&apos;NUS Graduate School for Integrative Sciences and Engineering
&apos;Department of Computer Science, National University of Singapore
3Human Language Technology, Institute for Infocomm Research, Singapore
</affiliation>
<email confidence="0.98902">
xuancong84@gmail.com, {simkc, nght}@comp.nus.edu.sg
</email>
<sectionHeader confidence="0.994593" genericHeader="abstract">
Abstract
</sectionHeader>
<bodyText confidence="0.999945454545454">
Punctuation prediction and disfluency pre-
diction can improve downstream natural
language processing tasks such as ma-
chine translation and information extrac-
tion. Combining the two tasks can poten-
tially improve the efficiency of the over-
all pipeline system and reduce error prop-
agation. In this work1, we compare var-
ious methods for combining punctuation
prediction (PU) and disfluency prediction
(DF) on the Switchboard corpus. We com-
pare an isolated prediction approach with
a cascade approach, a rescoring approach,
and three joint model approaches. For
the cascade approach, we show that the
soft cascade method is better than the hard
cascade method. We also use the cas-
cade models to generate an n-best list, use
the bi-directional cascade models to per-
form rescoring, and compare that with the
results of the cascade models. For the
joint model approach, we compare mixed-
label Linear-chain Conditional Random
Field (LCRF), cross-product LCRF and 2-
layer Factorial Conditional Random Field
(FCRF) with soft-cascade LCRF. Our re-
sults show that the various methods link-
ing the two tasks are not significantly dif-
ferent from one another, although they
perform better than the isolated predic-
tion method by 0.5–1.5% in the F1 score.
Moreover, the clique order of features also
shows a marked difference.
</bodyText>
<sectionHeader confidence="0.999143" genericHeader="introduction">
1 Introduction
</sectionHeader>
<bodyText confidence="0.992597">
The raw output from automatic speech recogni-
tion (ASR) systems does not have sentence bound-
</bodyText>
<footnote confidence="0.987359666666667">
1The research reported in this paper was carried out as
part of the PhD thesis research of Xuancong Wang at the NUS
Graduate School for Integrated Sciences and Engineering.
</footnote>
<bodyText confidence="0.999799243902439">
aries or punctuation symbols. Spontaneous speech
also contains a significant proportion of disflu-
ency. Researchers have shown that splitting input
sequences into sentences and adding in punctua-
tion symbols improve machine translation (Favre
et al., 2008; Lu and Ng, 2010). Moreover, dis-
fluencies in speech also introduce noise in down-
stream tasks like machine translation and informa-
tion extraction (Wang et al., 2010). Thus, punc-
tuation prediction (PU) and disfluency prediction
(DF) are two important post-processing tasks for
automatic speech recognition because they im-
prove not only the readability of ASR output, but
also the performance of downstream Natural Lan-
guage Processing (NLP) tasks.
The task of punctuation prediction is to insert
punctuation symbols into conversational speech
texts. Punctuation prediction on long, unseg-
mented texts also achieves the purpose of sentence
boundary prediction, because sentence boundaries
are identified by sentence-end punctuation sym-
bols: periods, question marks, and exclamation
marks. Consider the following example,
How do you feel about the Viet Nam War ? Yeah,
I saw that as well.
The question mark splits the sequence into two
sentences. This paper deals with this task which is
more challenging than that on text that has already
been split into sentences.
The task of disfluency prediction is to identify
word tokens that are spoken incorrectly due to
speech disfluency. There are two main types of
disfluencies: filler words and edit words. Filler
words mainly include filled pauses (e.g., ‘uh’,
‘um’) and discourse markers (e.g., “I mean”, “you
know”). As they are insertions in spontaneous
speech to indicate pauses or mark boundaries in
discourse, they do not convey useful content in-
formation. Edit words are words that are spoken
wrongly and then corrected by the speaker. For
example, consider the following utterance:
</bodyText>
<page confidence="0.97183">
121
</page>
<note confidence="0.8738864">
Proceedings of the 2014 Conference on Empirical Methods in Natural Language Processing (EMNLP), pages 121–130,
October 25-29, 2014, Doha, Qatar. c�2014 Association for Computational Linguistics
Edit Filler Repair 2 Previous Work
z } |{ z } |{ z }|{
I want a flight to Boston uh I mean to Denver
</note>
<bodyText confidence="0.9996515">
The phrase “to Boston” forms the edit region to be
replaced by “to Denver”. The words “uh I mean”
are filler words that serve to cue the listener about
the error and subsequent corrections.
The motivation of combining the two tasks can
be illustrated by the following two utterances:
</bodyText>
<equation confidence="0.8874565">
I am uh I am not going with you.
I am sorry. I am not going with you.
</equation>
<bodyText confidence="0.999844529411765">
Notice that the bi-gram “I am” is repeated in
both sentences. For the first utterance, if punctu-
ation prediction is performed first, it might break
the utterance both before and after “uh” so that the
second-stage disfluency prediction will treat the
whole utterance as three sentences, and thus may
not be able to detect any disfluency because each
one of the three sentences is legitimate on its own.
On the other hand, for the second utterance, if dis-
fluency prediction is performed first, it might mark
“I am sorry” as disfluent in the first place and re-
move it before passing into the second-stage punc-
tuation prediction. Therefore, no matter which
task is performed first, certain utterances can al-
ways cause confusion.
There are many ways to combine the two tasks.
For example, we can perform one task first fol-
lowed by another, which is called the cascade ap-
proach. We can also mix the labels, or take the
cross-product of the labels, or use joint prediction
models. In this paper, we study the mutual influ-
ence between the two tasks and compare a variety
of common state-of-the-art joint prediction tech-
niques on this joint task.
In Section 2, we briefly introduce previous work
on the two tasks. In Section 3, we describe our
baseline system which performs punctuation and
disfluency prediction separately (i.e., in isolation).
In Section 4, we compare the soft cascade ap-
proach with the hard cascade approach. We also
examine the effect of task order, i.e., performing
which task first benefits more. In Section 5, we
compare the cascade approach with bi-directional
n-best rescoring. In Section 6, we compare the 2-
layer Factorial CRF (Sutton et al., 2007) with the
cross-product LCRF (Ng and Low, 2004), mixed-
label LCRF (Stolcke et al., 1998), the cascade ap-
proach, and the baseline isolated prediction. Sec-
tion 7 gives a summary of our overall findings.
Section 8 gives the conclusion.
There were many works on punctuation prediction
or disfluency prediction as an isolated task. For
punctuation prediction, Huang and Zweig (2002)
used maximum entropy model; Christensen et al.
(2001) used finite state and multi-layer perceptron
method; Liu et al. (2005) used conditional ran-
dom fields; Lu and Ng (2010) proposed using dy-
namic conditional random fields for joint sentence
boundary type and punctuation prediction; Wang
et al. (2012) has added prosodic features for the
dynamic conditional random field approach and
Zhang et al. (2013) used transition-based parsing.
For disfluency prediction, Shriberg et al. (1997)
uses purely prosodic features to perform the task.
Johnson and Charniak (2004) proposed a TAG-
based (Tree-Adjoining Grammar) noisy channel
model. Maskey et al. (2006) proposed a phrase-
level machine translation approach for this task.
Georgila (2009) used integer linear programming
(ILP) which can incorporate local and global con-
straints. Zwarts and Johnson (2011) has inves-
tigated the effect of using extra language mod-
els as features in the reranking stage. Qian and
Liu (2013) proposed using weighted Max-margin
Markov Networks (M3N) to balance precision and
recall to further improve the F1-score. Wang et al.
(2014) proposed a beam-search decoder which in-
tegrates M3N and achieved further improvements.
There were also some works that addressed both
tasks. Liu et al. (2006) and Baron et al. (1998)
carried out sentence unit (SU) and disfluency pre-
diction as separate tasks. The difference between
SU prediction and punctuation prediction is only
in the non-sentence-end punctuation symbols such
as commas. Stolcke et al. (1998) mixed sen-
tence boundary labels with disfluency labels so
that they do not predict punctuation on disfluent
tokens. Kim (2004) performed joint SU and In-
terruption Point (IP) prediction, deriving edit and
filler word regions from predicted IPs using a rule-
based system as a separate step.
In this paper, we treat punctuation prediction
and disfluency prediction as a joint prediction task,
and compare various state-of-the-art joint predic-
tion methods on this task.
</bodyText>
<page confidence="0.99822">
122
</page>
<sectionHeader confidence="0.919332" genericHeader="method">
3 The Baseline System
</sectionHeader>
<subsectionHeader confidence="0.993858">
3.1 Experimental Setup
</subsectionHeader>
<bodyText confidence="0.999508090909091">
We use the Switchboard corpus (LDC99T42) in
our experiment with the same train/develop/test
split as (Qian and Liu, 2013) and (Johnson and
Charniak, 2004). The corpus statistics are shown
in Table 1. Since the proportion of exclamation
marks and incomplete SU boundaries is too small,
we convert all exclamation marks to periods and
remove all incomplete SU boundaries (treat as no
punctuation). In the Switchboard corpus, the ut-
terances of each speaker have already been seg-
mented into short sentences when used in (Qian
and Liu, 2013; Johnson and Charniak, 2004). In
our work, we concatenate the utterances of each
speaker to form one long sequence of words for
use as the input to punctuation prediction and dis-
fluency prediction. This form of input where,
utterances are not pre-segmented into short sen-
tences, better reflects the real-world scenarios and
provides a more realistic test setting for punctu-
ation and disfluency prediction. Punctuation pre-
diction also gives rise to sentence segmentation in
this setting.
</bodyText>
<table confidence="0.999215363636364">
Data set train develop test
# of tokens 1.3M 85.9K 65.5K
# of sentences 173.7K 10.1K 7.9K
# of sequences* 1854 174 134
# of edit words 63.6K 4.7K 3.7K
# of filler words 137.1K 9.6K 7.3K
# of Commas 52.7K 1.8K 2.1K
# of Periods 97.6K 6.5K 4.5K
# of Questions 6.8K 363 407
# of Exclamations 67 4 1
# of Incomplete 189 2 0
</table>
<tableCaption confidence="0.724225666666667">
Table 1: Corpus statistics for all the experiments.
*: each conversation produces two long/sentence-
joined sequences, one from each speaker.
</tableCaption>
<bodyText confidence="0.98470447826087">
Our baseline system uses M3N (Taskar et al.,
2004), one M3N for punctuation prediction and
the other for disfluency prediction. We use the
same set of punctuation and disfluency labels (as
shown in Table 2) throughout this paper. To com-
pare the various isolated, cascade, and joint pre-
diction models, we use the same feature templates
for both tasks as listed in Table 3. Since some of
the feature templates require predicted filler labels
and part-of-speech (POS) tags, we have trained a
POS tagger and a filler predictor both using CRF
(i.e., using the same approach as that in Qian and
Liu (2013)). The same predicted POS tags and
fillers are used for feature extraction in all the
experiments in this paper for a fair comparison.
The degradation on disfluency prediction due to
the concatenation of utterances of each speaker
is shown in Table 4. The pause duration fea-
tures are extracted by running forced alignment
on the corresponding Switchboard speech corpus
(LDC97S62).
Table 2: Labels for punctuation prediction and dis-
fluency prediction.
</bodyText>
<subsectionHeader confidence="0.985028">
3.2 Features
</subsectionHeader>
<bodyText confidence="0.999881846153846">
We use the standard NLP features such as
F(w_1w0=‘so that’), i.e., the word tokens at
the previous and current node position are
‘so’ and ‘that’ respectively. Each feature is
associated with a clique order. For example,
since the clique order of this feature template
is 2 (see Table 3), its feature functions can be
f(w_1w0=‘so that’, y0=‘F’, y_1=‘O’, t). The
example has a value of 1 only when the words
at node t − 1 and t are ‘so that’, and the labels
at node t and t − 1 are ‘F’ and ‘O’ respectively.
The maximum length of the y history is called the
clique order of the feature (in this feature func-
tion, it is 2 since only y0 and y_1 are covered).
The feature templates are listed in Table 3. wz
refers to the word at the ith position relative to
the current node; window size is the maximum
span of words centered at the current word that
the template covers, e.g., w_1w0 with a window
size of 9 means w_4w_3, w_3w_2, ..., w3w4; pz
refers to the POS tag at the ith position relative
to the current node; wz—j refers to any word
from the ith position to the jth position relative to
the current node, this template can capture word
pairs which can potentially indicate a repair, e.g.,
“was ... is ...”, the speaker may have spoken any
</bodyText>
<figure confidence="0.998809952380952">
comma
Task
Disfluency
prediction
Comma
Label
E
F
O
Meaning
edit word
filler word
otherwise / fluent
Punctuation
prediction
Period
QMark
None
full-stop
question mark
no punctuation
</figure>
<page confidence="0.995101">
123
</page>
<bodyText confidence="0.999974142857143">
word(s) in between and it is very difficult for the
standard n-gram features to capture all possible
variations; wi,=,4F refers to the ith non-filler word
with respect to the current position, this template
can extract n-gram features skipping filler words;
the multi-pair comparison function I(a, b, c, ...)
indicates whether each pair (a and b, b and c, and
so on) is identical, for example, if a = b =� c = d,
it will output “101” (‘1’ for being equal, ‘0’
for being unequal), this feature template can
capture consecutive word/POS repetitions which
can further improve upon the standard repetition
features; and ngram-score features are the nat-
ural logarithm of the following 8 probabilities:
</bodyText>
<equation confidence="0.989524166666667">
P(w−3, w−2, w−1, w0), P(w0|w−3, w−2, w−1),
P(w−3, w−2, w−1), P((/s)|w−3, w−2, w−1),
P(w−3), P(w−2), P(w−1) and P(w0) (where
“(/s)” denotes sentence-end).
Feature Template
w0
w−1w0
w−2w−1w0
p0
p−1p0
p−2p−1p0
w0w−6-1, w0w1-6
I(wi, wj)
I(wi, wj, wi+1, wj+1)
I(wi, wj)(wi if wi=wj)
I(pi, pj)
I(pi, pj, pi+1, pj+1)
I(pi, pj)(pi if pi=pj)
p−1w0
w−1p0
w−2,=,4Fw−1,=,4F
w−3,�Fw−2,�Fw−1,�F
p−2,�Fp−1,�F
p−3,�Fp−2,�Fp−1,�F
</equation>
<bodyText confidence="0.967701357142857">
ngram-score features
pause duration before w0
pause duration after w0
transitions
Table 3: Feature templates for disfluency predic-
tion, or punctuation prediction, or joint prediction
for all the experiments in this paper.
The performance of the system can be fur-
ther improved by adding additional prosodic fea-
tures (Savova and Bachenko, 2003; Shriberg et al.,
1998; Christensen et al., 2001) apart from pause
durations. However, since in this work we focus
on model-level comparison, we do not use other
prosodic features for simplicity.
</bodyText>
<subsectionHeader confidence="0.960845">
3.3 Evaluation and Results
</subsectionHeader>
<table confidence="0.996345421052631">
Experiment F1 F1
(PU) (DF)
Short sentences, with preci- N.A. 84.7
sion/recall balancing, clique or-
der of features up to 3, and la-
bels {E,F,O}
Short sentences, with preci- N.A. 84.3
sion/recall balancing, clique or-
der of features up to 3, and la-
bels {E,O}
Join utterances into long sen- 71.1 79.2
tences
Join utterances into long sen- 71.1 78.2
tences + remove precision/recall
balancing
Join utterances into long sen- 68.5 76.4
tences + remove precision/recall
balancing + reduce clique order
of all features
</table>
<tableCaption confidence="0.876975">
Table 4: Baseline results showing the degradation
</tableCaption>
<bodyText confidence="0.998848260869565">
by joining utterances into long sentences, remov-
ing precision/recall balancing, and reducing the
clique order of features. All models are trained
using M3N.
We use the standard F1 score as our evaluation
metric and this is similar to that in Qian and Liu
(2013). For training, we set the frequency prun-
ing threshold to 5 to control the number of pa-
rameters. The regularization parameter is tuned
on the development set. Since the toolkits used
to run different experiments have slightly differ-
ent limitations, in order to make fair comparisons
across different toolkits, we do not use weighting
to balance precision and recall when training M3N
and we have reduced the clique order of transi-
tion features to two and all the other features to
one in some of our experiments. Since the per-
formance of filler word prediction on this dataset
is already very high, (&gt;97%), we only focus on
the F1 score of edit word prediction in this pa-
per when reporting the performance of disfluency
prediction. Table 4 shows our baseline results.
Our preliminary study shows the following gen-
</bodyText>
<figure confidence="0.99592556">
Window
Size
Clique
Order
1
2
2
1
2
2
1
2
2
2
3
3
3
2
2
2
2
2
2
3
3
3
3
9
9
9
9
9
9
1
21
21
21
21
21
21
5
5
1
1
1
1
1
1
1
1
</figure>
<page confidence="0.988085">
124
</page>
<bodyText confidence="0.997811333333333">
eral trends: (i) for disfluency prediction: joining
utterances into long sentences will cause a 5–6%
drop in F1 score; removing precision/recall bal-
ance in M3N will cause about 1% drop in F1 score;
and reducing the clique order in Table 3 will cause
about 1–2% drop in F1 score; and (ii) for punctua-
tion prediction: removing precision/recall balance
in M3N will cause negligible drop in F1 score; and
reducing clique order will cause about 2–3% drop
in F1 score. Conventionally, the degradation from
reducing the clique orders can be mostly compen-
sated by using the BIES (Begin, Inside, End, and
Single) labeling scheme. In this work, for con-
sistency and comparability across various experi-
ments, we will stick to the same set of labels in
</bodyText>
<tableCaption confidence="0.64379">
Table 2.
</tableCaption>
<sectionHeader confidence="0.991499" genericHeader="method">
4 The Cascade Approach
</sectionHeader>
<bodyText confidence="0.9996705">
Instead of decomposing the joint prediction of
punctuation and disfluency into two independent
tasks, the cascade approach considers one task to
be conditionally dependent on the other task such
that the predictions are performed in sequence,
where the results from the first step is used in the
second step. In this paper, we compare two types
of cascade: hard cascade versus soft cascade.
</bodyText>
<subsectionHeader confidence="0.997624">
4.1 Hard Cascade
</subsectionHeader>
<bodyText confidence="0.999988475">
For the hard cascade, we use the output from the
first step to modify the input sequence before ex-
tracting features for the second step. For PU—*DF
(PUnctuation prediction followed by DisFluency
prediction), we split the input sequence into sen-
tences according to the sentence-end punctuation
symbols predicted by the first step, and then per-
form the DF prediction on the short/sentence-split
sequences in the second step. For DF—*PU, we
remove the edit and filler words predicted by the
first step, and then predict the punctuations using
the cleaned-up input sequence. The hard cascade
method may be helpful because the disfluency pre-
diction on short/sentence-split sequences is better
than on long/sentence-joined sequences (see the
second and third rows in Table 4). On the other
hand, the punctuation prediction on fluent text is
more accurate than that on non-fluent text based
on our preliminary study.
For this experiment, four models are trained
using M3N without balancing precision/recall.
For the first step, two models are trained on
long/sentence-joined sequences with disfluent to-
kens - one for PU prediction and the other for DF
prediction. These are simply the isolated base-
line systems. For the second step, the DF predic-
tion model is trained on the short/sentence-split se-
quences with disfluent tokens while the PU predic-
tion model is trained on the long/sentence-joined
sequences with disfluent tokens removed. Note
that in the second step of DF—*PU, punctuation la-
bels are predicted only for the fluent tokens since
the disfluent tokens predicted by the first step has
already been removed. Therefore, during evalua-
tion, if the first step makes a false positive by pre-
dicting a fluent token as an edit or filler, we set its
punctuation label to the neutral label, None. All
the four models are trained using the same feature
templates as shown in Table 3. The regularization
parameter is tuned on the development set.
</bodyText>
<subsectionHeader confidence="0.991783">
4.2 Soft Cascade
</subsectionHeader>
<bodyText confidence="0.998783">
For the soft cascade method, we use the labels pre-
dicted from the first step as additional features for
the second step. For PU—*DF, we model the joint
probability as:
</bodyText>
<equation confidence="0.94583575">
P(DF, PUJx) = P(PUJx) x P(DFJPU, x) (1)
Likewise, we model the joint probability for
DF—*PU as:
P(DF, PUJx) = P(DFJx) x P(PUJDF, x) (2)
</equation>
<bodyText confidence="0.999967545454545">
For this experiment, four models are trained us-
ing M3N without balancing precision/recall. As
with the case of hard cascade, the two models
used in the first step are simply the isolated base-
line systems. For the second step, in addition to
the feature templates in Table 3, we also pass on
the labels (at the previous, current and next posi-
tion) predicted by the first step as three third-order-
clique features. We also tune the regularization pa-
rameter on the development set to obtain the best
model.
</bodyText>
<subsectionHeader confidence="0.998461">
4.3 Experimental Results
</subsectionHeader>
<bodyText confidence="0.999947777777778">
Table 5 compares the performance of the hard
and soft cascade methods with the isolated base-
line systems. In addition, we have also included
the results of using the true labels in place of
the labels predicted by the first step to indicate
the upper-bound performance of the cascade ap-
proaches. The results show that both the hard and
soft cascade methods outperform the baseline sys-
tems, with the latter giving a better performance
</bodyText>
<page confidence="0.996102">
125
</page>
<table confidence="0.993220375">
Experiment F1 for PU F1 for DF
isolated baseline 71.1 78.2
hard cascade 71.2 79.1
hard cascade 72.6 83.5
(using true labels)
soft cascade 71.6 79.6
soft cascade 72.1 82.7
(using true labels)
</table>
<tableCaption confidence="0.99505">
Table 5: Performance comparison between the
</tableCaption>
<bodyText confidence="0.993796659574468">
hard cascade method and the soft cascade method
with respect to the baseline isolated prediction.
All models are trained using M3N without balanc-
ing precision and recall.
(statistical significance at p=0.01). However, hard
cascade has a higher upper-bound than soft cas-
cade. This observation can be explained as fol-
lows.
For hard cascade, the input sequence is modi-
fied prior to feature extraction. Therefore, many
of the features generated by the feature templates
given in Table 3 will be affected by these modi-
fications. So, provided that the modifications are
based on the correct information, the resulting fea-
tures will not contain unwanted artefacts caused
by the absence of the sentence boundary informa-
tion for the presence of disfluencies. For exam-
ple, in “do you do you feel that it was worthy”,
the punctuation prediction system tends to insert a
sentence-end punctuation after the first “do you”
because the speaker restarts the sentence.
If the disfluency was correctly predicted in the
first step, then the hard cascade method would
have removed the first “do you” and eliminated
the confusion. Similarly, in “I ’m sorry. I ’m not
going with you tomorrow . ”, the first “I ’m” is
likely to be incorrectly detected as disfluent tokens
since consecutive repetitions are a strong indica-
tion of disfluency. In the case of hard cascade,
PU—*DF, the input sequence would have been split
into sentences and the repetition feature would not
be activated. However, since the hard cascade
method has a greater influence on the features for
the second step, it is also more sensitive to the pre-
diction errors from the first step.
Another observation from Table 5 is that the
improvement of the soft cascade over the isolate
baseline is much larger on DF (1.4% absolute)
than that on PU (only 0.5% absolute). The same
holds true for the hard cascade, despite the fact
that there are more DF labels than PU labels in this
corpus (see Table 1) and the first step prediction is
more accurate on DF than on PU. This suggests
that their mutual influence is not symmetrical, in
the way that the output from punctuation predic-
tion provides more useful information for disflu-
ency prediction than the other way round.
</bodyText>
<sectionHeader confidence="0.967324" genericHeader="method">
5 The Rescoring Approach
</sectionHeader>
<bodyText confidence="0.999942294117647">
In Section 4, we have described that the two tasks
can be cascaded in either order, i.e., PU—*DF and
DF—*PU. However, the performance of the sec-
ond step greatly depends on that of the first step.
In order to reduce sensitivity to the errors made
in the first step, one simple approach is to prop-
agate multiple hypotheses from the first step to
the second step to obtain a list of joint hypothe-
ses (with both the DF and PU labels). We then
rerank these hypotheses based on the joint proba-
bility and pick the best. We call this the rescoring
approach. From (1) and (2), the joint probabilities
can be expressed in terms of the probabilities gen-
erated by four models: P(PU|x), P(DF|PU, x),
P(DF|x), and P(PU|DF, x). We can combine the
four models to form the following joint probability
function for rescoring:
</bodyText>
<equation confidence="0.9207765">
P(DF, PU|x) = P(DF|x)α1 x P(PU|DF, x)α2
x P(PU|x)β1 x P(DF|PU, x)β2
</equation>
<bodyText confidence="0.999959380952381">
where α1, α2, 01, and 02 are used to weight
the relative importance between (1) and (2); and
between the first and second steps. In practice,
the probabilities are computed in the log domain
where the above expression becomes a weighted
sum of the log probabilities. A similar rescoring
approach using two models is described in Shi and
Wang (2007).
The experimental framework is shown in Fig-
ure 1. For PU—*DF, we first use P(PU|x) to gen-
erate an n-best list. Then, for each hypothesis in
the n-best list, we use P(DF|PU, x) to obtain an-
other n-best list. So we have n2-best joint hy-
potheses. We do the same for DF—*PU to ob-
tain another n2-best joint hypotheses. We rescore
the 2n2-best list using the four models. The four
weights α1, α2, 01, and 02 are tuned to opti-
mize the overall F1 score on the development set.
We used the MERT (minimum-error-rate training,
(Och, 2003)) algorithm to tune the weights. We
also vary the size of n.
</bodyText>
<page confidence="0.992422">
126
</page>
<figure confidence="0.999088576923077">
2 n-best 2 n2-best
...
...
P(DF|PU,x)
PU-hypo-1
PU-hypo-n
P(PU|DF,x)
DF-hypo-1
...
...
DF-hypo-n
Input
Sequence
P(PU|x)
P(DF|x)
joint-hypo-1
joint-hypo-2
...
joint-hypo-1
joint-hypo-2
...
Rescore using:
a1 ∙ log𝑃 PU|x
+a2 ∙ log𝑃 DF|PU, x
+/31 ∙ log𝑃 DF|x
+/32 ∙ log𝑃 PU|DF, x
</figure>
<figureCaption confidence="0.9972675">
Figure 1: Illustration of the rescoring pipeline framework using the four M3N models used in the soft-
cascade method: P(PU|x), P(DF|PU, x), P(DF|x) and P(PU|DF, x)
</figureCaption>
<bodyText confidence="0.9999885">
The results shown in Table 6 suggest that the
rescoring method does not improve over the soft-
cascade baseline. This can be due to the fact that
we are using the same four models for the soft-
cascade and the rescoring methods. It may be
possible that the information contained in the two
models for the soft-cascade PU→DF mostly over-
laps with the information contained in the other
two models for the soft-cascade DF→PU since all
the four models are trained using the same fea-
tures. Thus, no additional information is gained
by combining the four models.
</bodyText>
<sectionHeader confidence="0.968089" genericHeader="method">
6 The Joint Approach
</sectionHeader>
<bodyText confidence="0.999918351851852">
In this section, we compare 2-layer FCRF (Lu and
Ng, 2010) with mixed-label LCRF (Stolcke et al.,
1998) and cross-product LCRF on the joint predic-
tion task. For the 2-layer FCRF, we use punctua-
tion labels for the first layer and disfluency labels
for the second layer (see Table 2). For the mixed-
label LCRF, we split the neutral label {O} into
{Comma, Period, QMark, None} so that we have
six labels in total, {E, F, Comma, Period, QMark,
None}. In this approach, disfluent tokens do not
have punctuation labels because in real applica-
tions, if we just want to get the cleaned-up/fluent
text with punctuations, we do not need to predict
punctuations on disfluent tokens as they will be
removed during the clean-up process. Since this
approach does not predict punctuation labels on
disfluent tokens, its punctuation F1 score is only
evaluated on those fluent tokens. For the cross-
product LCRF, we compose each of the three dis-
fluency labels with the four punctuation labels to
get 12 PU-DF-joint labels (Ng and Low, 2004).
Figure 2 shows a comparison of these three models
in the joint prediction of punctuation and disflu-
ency. All the LCRF and FCRF models are trained
using the GRMM toolkit (Sutton, 2006). We use
the same feature templates (Table 3) to generate
all the features for the toolkit. However, to reduce
the training time, we have set clique order to 2 for
the transitions and 1 for all other features. We tune
the Gaussian prior variance on the development set
for all the experiments to obtain the best model for
testing.
Table 7 shows the comparison of results. On
DF alone, the improvement of the cross-product
LCRF over the mixed-label LCRF, and the im-
provement of the mixed-label LCRF over the
isolated baseline are not statistically significant.
However, if we test the statistical significance on
the overall performance of both PU and DF, both
the 2-layer FCRF and the cross-product LCRF
perform better than the mixed-label LCRF. And
we also obtain the same conclusion as Stolcke
et al. (1998) that mixed-label LCRF performs
better than isolated prediction. However, for the
comparison between the 2-layer FCRF and the
cross-product LCRF, although the 2-layer FCRF
performs better than the cross-product LCRF on
disfluency prediction, it does worse on punctua-
tion prediction. Overall, the two methods perform
about the same, their difference is not statistically
significant. In addition, both the 2-layer FCRF
and the cross-product LCRF slightly outperform
the soft cascade method (statistical significance at
p=0.04).
</bodyText>
<page confidence="0.990327">
127
</page>
<table confidence="0.998747533333333">
Experiment F1 for PU F1 for DF
isolated baseline 71.1 78.2
soft-cascade 71.6 79.6
rescore n=1 71.5 (72.5) 79.3 (81.1)
rescore n=2 71.2 (73.0) 79.3 (81.8)
rescore n=3 71.2 (73.3) 79.9 (82.6)
rescore n=4 71.2 (73.6) 79.8 (82.8)
rescore n=5 71.2 (73.9) 79.4 (83.3)
rescore n=6 71.1 (74.0) 79.6 (83.5)
rescore n=8 71.2 (74.2) 79.8 (84.0)
rescore n=10 * 71.2 (74.4) 79.8 (84.3)
rescore n=12 71.1 (74.5) 79.7 (84.6)
rescore n=15 71.2 (74.8) 79.8 (84.9)
rescore n=18 71.1 (74.9) 79.7 (85.1)
rescore n=25 70.7 (75.2) 79.3 (85.5)
</table>
<tableCaption confidence="0.988363">
Table 6: Performance comparison between the
</tableCaption>
<bodyText confidence="0.998008">
rescoring method and the soft-cascade method
with respect to the baseline isolated prediction.
The rescoring is done on 2n2 hypotheses. All
models are trained using M3N without balancing
precision and recall. Figures in the bracket are the
oracle F1 scores of the 2n2 hypotheses. *:on the
development set, the best overall result is obtained
at n = 10.
</bodyText>
<figureCaption confidence="0.9635426">
Figure 2: Illustration using (a) mixed-label LCRF;
(b) cross-product LCRF; and (c) 2-layer FCRF, for
joint punctuation (PU) and disfluency (DF) predic-
tion. Shaded nodes are observations and unshaded
nodes are variables to be predicted.
</figureCaption>
<figure confidence="0.999755704918033">
E
E
E
F
O
O
O
O
Mixed-
label
LCRF
None
O
None
O
None
E
None
E
None
F
None
O
Comma
F
Period
O
Comma
E
Cross-
product
LCRF
2-layer
FCRF
None None Comma None Comma None None None Period
E E E F F O O O Period
x1 x2 x3 x4 x5 x6 x7 x8 x9
x1 x2 x3 x4 x5 x6 x7 x8 x9
x1 x2 x3 x4 x5 x6 x7 x8 x9
E E E F F O O O O
edit filler
it was n’t , you know , it was never announced .
it
was
n’t
you
know
it
was
never announced
Ref:
Token:
None Comma None Comma
Period
None
None
None
None
PU:
DF:
F
</figure>
<sectionHeader confidence="0.9865" genericHeader="method">
7 Discussion
</sectionHeader>
<bodyText confidence="0.999840333333333">
In this section, we will summarise our observa-
tions based on the empirical studies that we have
conducted in this paper.
Firstly, punctuation prediction and disfluency
prediction do influence each other. The output
from one task does provide useful information that
can improve the other task. All the approaches
studied in this work, which link the two tasks
together, perform better than their corresponding
</bodyText>
<table confidence="0.947271">
Experiment F1 for PU F1 for DF
isolated baseline 68.7 77.0
soft cascade 69.0 77.5
mixed-label LCRF 69.0 77.2
cross-product LCRF 69.9 77.3
2-layer FCRF 69.2 77.8
</table>
<tableCaption confidence="0.979475">
Table 7: Performance comparison among 2-
</tableCaption>
<bodyText confidence="0.993517826086956">
layer FCRF, mixed-label LCRF and cross-product
LCRF, with respect to the soft-cascade and the iso-
lated prediction baseline. All models are trained
using GRMM (Sutton, 2006), with reduced clique
orders.
isolated prediction baseline.
Secondly, as compared to the soft cascade, the
hard cascade passes more information from the
first step into the second step, and thus is much
more sensitive to errors in the first step. In prac-
tice, unless the first step has very high accuracy,
soft cascade is expected to do better than hard cas-
cade.
Thirdly, if we train a model using a fine-grained
label set but test it on the same coarse-grained la-
bel set, we are very likely to get improvement. For
example:
• The edit word F1 for mixed edit and filler pre-
diction using {E, F, O} is better than that for
edit prediction using {E, O} (see the second
and third rows in Table 4). This is because the
former actually splits the O in the latter into
F and O. Thus, it has a finer label granularity.
</bodyText>
<listItem confidence="0.8958466">
• Disfluency prediction using mixed-label
LCRF (using label set {E, F, Comma, Pe-
riod, Question, None}) performs better than
that using isolated LCRF (using label set {E,
F, O}) (see the second and fourth rows in
</listItem>
<tableCaption confidence="0.508754">
Table 7). This is because the former dis-
</tableCaption>
<page confidence="0.996338">
128
</page>
<bodyText confidence="0.995599208333333">
tinguishes between different punctuations for
fluent tokens and thus has a finer label granu-
larity.
• Both the cross-product LCRF and 2-layer
FCRF perform better than mixed-label LCRF
because the former two distinguish between
different punctuations for edit, filler and flu-
ent tokens while the latter distinguishes be-
tween different punctuations only for fluent
tokens. Thus, the former has a much finer la-
bel granularity.
From the above comparisons, we can see that
increasing the label granularity can greatly im-
prove the accuracy of a model. However, this
may also increase the model complexity dramat-
ically, especially when higher clique order is used.
Although the joint approach (2-layer FCRF and
cross-product LCRF) are better than the soft-
cascade approach, they cannot be easily scaled up
to using higher order cliques, which greatly limits
their potential. In practice, the soft cascade ap-
proach offers a simpler and more efficient way to
achieve a joint prediction of punctuations and dis-
fluencies.
</bodyText>
<sectionHeader confidence="0.997388" genericHeader="conclusions">
8 Conclusion
</sectionHeader>
<bodyText confidence="0.999985107142857">
In general, punctuation prediction and disfluency
prediction can improve downstream NLP tasks.
Combining the two tasks can potentially improve
the efficiency of the overall framework and mini-
mize error propagation. In this work, we have car-
ried out an empirical study on the various methods
for combining the two tasks. Our results show that
the various methods linking the two tasks perform
better than the isolated prediction. This means
that punctuation prediction and disfluency predic-
tion do influence each other, and the prediction
outcome in one task can provide useful informa-
tion that helps to improve the other task. Specifi-
cally, we compare the cascade models and the joint
prediction models. For the cascade approach, we
show that soft cascade is less sensitive to predic-
tion errors in the first step, and thus performs bet-
ter than hard cascade. For joint model approach,
we show that, when clique order of one is used, all
the three joint model approaches perform signifi-
cantly better than the isolated prediction baseline.
Moreover, the 2-layer FCRF and the cross-product
LCRF perform slightly better than the mix-label
LCRF and the soft-cascade approach, suggesting
that modelling at a finer label granularity is po-
tentially beneficial. However, the soft cascade ap-
proach is more efficient than the joint approach
when a higher clique order is used.
</bodyText>
<sectionHeader confidence="0.997134" genericHeader="acknowledgments">
Acknowledgments
</sectionHeader>
<bodyText confidence="0.99918325">
This research is supported by the Singapore Na-
tional Research Foundation under its International
Research Centre @ Singapore Funding Initiative
and administered by the IDM Programme Office.
</bodyText>
<sectionHeader confidence="0.998891" genericHeader="references">
References
</sectionHeader>
<reference confidence="0.999769">
Don Baron, Elizabeth Shriberg, and Andreas Stolcke.
2002. Automatic punctuation and disfluency detec-
tion in multi-party meetings using prosodic and lex-
ical cues. In Proc. of ICSLP.
Heidi Christensen, Yoshihiko Gotoh, and Steve Re-
nals. 2001. Punctuation annotation using statisti-
cal prosody models. In ISCA Tutorial and Research
Workshop (ITRW) on Prosody in Speech Recognition
and Understanding.
Benoit Favre, Ralph Grishman, Dustin Hillard, Heng
Ji, Dilek Hakkani-Tur, and Mari Ostendorf. 2008.
Punctuating speech for information extraction. In
Proc. of ICASSP.
Kallirroi Georgila. 2009. Using integer linear pro-
gramming for detecting speech disfluencies. In
Proc. of NAACL.
Jing Huang and Geoffrey Zweig. 2002. Maximum en-
tropy model for punctuation annotation from speech.
In Proc. of INTERSPEECH.
Mark Johnson and Eugene Charniak. 2004. A TAG-
based noisy-channel model of speech repairs. In
Proc. of ACL.
Joungbum Kim. 2004. Automatic detection of sen-
tence boundaries, disfluencies, and conversational
fillers in spontaneous speech. Master dissertation of
University of Washington.
Yang Liu, Andreas Stolcke, Elizabeth Shriberg, and
Mary Harper. 2005. Using conditional random
fields for sentence boundary detection in speech. In
Proc. of ACL.
Yang Liu, Elizabeth Shriberg, Andreas Stolcke, Dustin
Hillard, Mari Ostendorf, and Mary Harper. 2006.
Enriching speech recognition with automatic detec-
tion of sentence boundaries and disfluencies. IEEE
Transactions on Audio, Speech, and Language Pro-
cessing, 14(5):1526–1540.
Wei Lu and Hwee Tou Ng. 2010. Better punctuation
prediction with dynamic conditional random fields.
In Proc. of EMNLP.
</reference>
<page confidence="0.984044">
129
</page>
<reference confidence="0.997537754098361">
Sameer Maskey, Bowen Zhou, and Yuqing Gao. 2006.
A phrase-level machine translation approach for dis-
fluency detection using weighted finite state trans-
ducers. In Proc. of INTERSPEECH.
Hwee Tou Ng and Jin Kiat Low. 2004. Chi-
nese part-of-speech tagging: One-at-a-time or all-at-
once? Word-based or character-based? In Proc. of
EMNLP.
Franz Josef Och. 2003. Minimum error rate training in
statistical machine translation. In Proc. ofACL.
Xian Qian and Yang Liu. 2013. Disfluency detec-
tion using multi-step stacked learning. In Proc. of
NAACL.
Guergana Savova, Joan Bachenko. 2003. Prosodic fea-
tures of four types of disfluencies. In ISCA Tuto-
rial and Research Workshop on Disfiuency in Spon-
taneous Speech.
Yanxin Shi and Mengqiu Wang. 2007. A dual-layer
CRFs based joint decoding method for cascaded seg-
mentation and labeling tasks. In Proc. of IJCAI.
Elizabeth Shriberg, Rebecca Bates and Andreas Stol-
cke. 1997. A prosody-only decision-tree model for
disfluency detection. In Proc. of Eurospeech.
Elizabeth Shriberg, Andreas Stolcke, Daniel Jurafsky,
Noah Coccaro, Marie Meteer, Rebecca Bates, Paul
Taylor, Klaus Ries, Rachel Martin, and Carol Van
Ess-Dykema. 1998. Can prosody aid the auto-
matic classification of dialog acts in conversational
speech? In Language and speech 41, no. 3-4: 443-
492.
Andreas Stolcke, Elizabeth Shriberg, Rebecca A.
Bates, Mari Ostendorf, Dilek Hakkani, Madelaine
Plauche, Gokhan Tur, and Yu Lu. 1998. Auto-
matic detection of sentence boundaries and disfluen-
cies based on recognized words. In Proc. of ICSLP.
Charles Sutton. 2006. GRMM: GRaphical Models in
Mallet. http://mallet.cs.umass.edu/grmm/
Charles Sutton, Andrew McCallum, and Khashayar
Rohanimanesh. 2007. Dynamic conditional random
fields: factorized probabilistic models for labeling
and segmenting sequence data. In Journal of Ma-
chine Learning Research, 8: 693–723.
Ben Taskar, Carlos Guestrin, and Daphne Koller. 2004.
Max-margin Markov networks. In Proc. of NIPS.
Wen Wang, Gokhan Tur, Jing Zheng, and Necip Fazil
Ayan. 2010. Automatic disfluency removal for im-
proving spoken language translation. In Proc. of
ICASSP.
Xuancong Wang, Hwee Tou Ng, and Khe Chai Sim.
2012. Dynamic conditional random fields for joint
sentence boundary and punctuation prediction. In
Proc. of Interspeech.
Xuancong Wang, Hwee Tou Ng, and Khe Chai Sim.
2014. A beam-search decoder for disfluency detec-
tion. In Proc. of COLING.
Dongdong Zhang, Shuangzhi Wu, Nan Yang, and Mu
Li. 2013. Punctuation prediction with transition-
based parsing. In Proc. of ACL.
Simon Zwarts and Mark Johnson. 2011. The impact
of language models and loss functions on repair dis-
fluency detection. In Proc. ofACL.
</reference>
<page confidence="0.997609">
130
</page>
</variant>
</algorithm>
<algorithm name="ParsHed" version="110505">
<variant no="0" confidence="0.883122">
<title confidence="0.999343">Combining Punctuation and Disfluency Prediction: An Empirical Study</title>
<author confidence="0.983531">Khe Chai Tou</author>
<affiliation confidence="0.975861666666667">Graduate School for Integrative Sciences and of Computer Science, National University of Language Technology, Institute for Infocomm Research,</affiliation>
<abstract confidence="0.996886558823529">Punctuation prediction and disfluency prediction can improve downstream natural language processing tasks such as machine translation and information extraction. Combining the two tasks can potentially improve the efficiency of the overall pipeline system and reduce error prop- In this we compare various methods for combining punctuation prediction (PU) and disfluency prediction (DF) on the Switchboard corpus. We compare an isolated prediction approach with a cascade approach, a rescoring approach, and three joint model approaches. For the cascade approach, we show that the soft cascade method is better than the hard cascade method. We also use the cascade models to generate an n-best list, use the bi-directional cascade models to perform rescoring, and compare that with the results of the cascade models. For the joint model approach, we compare mixedlabel Linear-chain Conditional Random Field (LCRF), cross-product LCRF and 2layer Factorial Conditional Random Field (FCRF) with soft-cascade LCRF. Our results show that the various methods linking the two tasks are not significantly different from one another, although they perform better than the isolated prediction method by 0.5–1.5% in the F1 score. Moreover, the clique order of features also shows a marked difference.</abstract>
</variant>
</algorithm>
<algorithm name="ParsCit" version="110505">
<citationList>
<citation valid="true">
<authors>
<author>Don Baron</author>
<author>Elizabeth Shriberg</author>
<author>Andreas Stolcke</author>
</authors>
<title>Automatic punctuation and disfluency detection in multi-party meetings using prosodic and lexical cues.</title>
<date>2002</date>
<booktitle>In Proc. of ICSLP.</booktitle>
<marker>Baron, Shriberg, Stolcke, 2002</marker>
<rawString>Don Baron, Elizabeth Shriberg, and Andreas Stolcke. 2002. Automatic punctuation and disfluency detection in multi-party meetings using prosodic and lexical cues. In Proc. of ICSLP.</rawString>
</citation>
<citation valid="true">
<authors>
<author>Heidi Christensen</author>
<author>Yoshihiko Gotoh</author>
<author>Steve Renals</author>
</authors>
<title>Punctuation annotation using statistical prosody models.</title>
<date>2001</date>
<booktitle>In ISCA Tutorial and Research Workshop (ITRW) on Prosody in Speech Recognition and Understanding.</booktitle>
<contexts>
<context position="6596" citStr="Christensen et al. (2001)" startWordPosition="1050" endWordPosition="1053">rming which task first benefits more. In Section 5, we compare the cascade approach with bi-directional n-best rescoring. In Section 6, we compare the 2- layer Factorial CRF (Sutton et al., 2007) with the cross-product LCRF (Ng and Low, 2004), mixedlabel LCRF (Stolcke et al., 1998), the cascade approach, and the baseline isolated prediction. Section 7 gives a summary of our overall findings. Section 8 gives the conclusion. There were many works on punctuation prediction or disfluency prediction as an isolated task. For punctuation prediction, Huang and Zweig (2002) used maximum entropy model; Christensen et al. (2001) used finite state and multi-layer perceptron method; Liu et al. (2005) used conditional random fields; Lu and Ng (2010) proposed using dynamic conditional random fields for joint sentence boundary type and punctuation prediction; Wang et al. (2012) has added prosodic features for the dynamic conditional random field approach and Zhang et al. (2013) used transition-based parsing. For disfluency prediction, Shriberg et al. (1997) uses purely prosodic features to perform the task. Johnson and Charniak (2004) proposed a TAGbased (Tree-Adjoining Grammar) noisy channel model. Maskey et al. (2006) p</context>
<context position="14049" citStr="Christensen et al., 2001" startWordPosition="2275" endWordPosition="2278"> w−2w−1w0 p0 p−1p0 p−2p−1p0 w0w−6-1, w0w1-6 I(wi, wj) I(wi, wj, wi+1, wj+1) I(wi, wj)(wi if wi=wj) I(pi, pj) I(pi, pj, pi+1, pj+1) I(pi, pj)(pi if pi=pj) p−1w0 w−1p0 w−2,=,4Fw−1,=,4F w−3,�Fw−2,�Fw−1,�F p−2,�Fp−1,�F p−3,�Fp−2,�Fp−1,�F ngram-score features pause duration before w0 pause duration after w0 transitions Table 3: Feature templates for disfluency prediction, or punctuation prediction, or joint prediction for all the experiments in this paper. The performance of the system can be further improved by adding additional prosodic features (Savova and Bachenko, 2003; Shriberg et al., 1998; Christensen et al., 2001) apart from pause durations. However, since in this work we focus on model-level comparison, we do not use other prosodic features for simplicity. 3.3 Evaluation and Results Experiment F1 F1 (PU) (DF) Short sentences, with preci- N.A. 84.7 sion/recall balancing, clique order of features up to 3, and labels {E,F,O} Short sentences, with preci- N.A. 84.3 sion/recall balancing, clique order of features up to 3, and labels {E,O} Join utterances into long sen- 71.1 79.2 tences Join utterances into long sen- 71.1 78.2 tences + remove precision/recall balancing Join utterances into long sen- 68.5 76.</context>
</contexts>
<marker>Christensen, Gotoh, Renals, 2001</marker>
<rawString>Heidi Christensen, Yoshihiko Gotoh, and Steve Renals. 2001. Punctuation annotation using statistical prosody models. In ISCA Tutorial and Research Workshop (ITRW) on Prosody in Speech Recognition and Understanding.</rawString>
</citation>
<citation valid="true">
<authors>
<author>Benoit Favre</author>
<author>Ralph Grishman</author>
<author>Dustin Hillard</author>
<author>Heng Ji</author>
<author>Dilek Hakkani-Tur</author>
<author>Mari Ostendorf</author>
</authors>
<title>Punctuating speech for information extraction.</title>
<date>2008</date>
<booktitle>In Proc. of ICASSP.</booktitle>
<contexts>
<context position="2212" citStr="Favre et al., 2008" startWordPosition="329" endWordPosition="332"> the F1 score. Moreover, the clique order of features also shows a marked difference. 1 Introduction The raw output from automatic speech recognition (ASR) systems does not have sentence bound1The research reported in this paper was carried out as part of the PhD thesis research of Xuancong Wang at the NUS Graduate School for Integrated Sciences and Engineering. aries or punctuation symbols. Spontaneous speech also contains a significant proportion of disfluency. Researchers have shown that splitting input sequences into sentences and adding in punctuation symbols improve machine translation (Favre et al., 2008; Lu and Ng, 2010). Moreover, disfluencies in speech also introduce noise in downstream tasks like machine translation and information extraction (Wang et al., 2010). Thus, punctuation prediction (PU) and disfluency prediction (DF) are two important post-processing tasks for automatic speech recognition because they improve not only the readability of ASR output, but also the performance of downstream Natural Language Processing (NLP) tasks. The task of punctuation prediction is to insert punctuation symbols into conversational speech texts. Punctuation prediction on long, unsegmented texts al</context>
</contexts>
<marker>Favre, Grishman, Hillard, Ji, Hakkani-Tur, Ostendorf, 2008</marker>
<rawString>Benoit Favre, Ralph Grishman, Dustin Hillard, Heng Ji, Dilek Hakkani-Tur, and Mari Ostendorf. 2008. Punctuating speech for information extraction. In Proc. of ICASSP.</rawString>
</citation>
<citation valid="true">
<authors>
<author>Kallirroi Georgila</author>
</authors>
<title>Using integer linear programming for detecting speech disfluencies.</title>
<date>2009</date>
<booktitle>In Proc. of NAACL.</booktitle>
<contexts>
<context position="7277" citStr="Georgila (2009)" startWordPosition="1154" endWordPosition="1155">2005) used conditional random fields; Lu and Ng (2010) proposed using dynamic conditional random fields for joint sentence boundary type and punctuation prediction; Wang et al. (2012) has added prosodic features for the dynamic conditional random field approach and Zhang et al. (2013) used transition-based parsing. For disfluency prediction, Shriberg et al. (1997) uses purely prosodic features to perform the task. Johnson and Charniak (2004) proposed a TAGbased (Tree-Adjoining Grammar) noisy channel model. Maskey et al. (2006) proposed a phraselevel machine translation approach for this task. Georgila (2009) used integer linear programming (ILP) which can incorporate local and global constraints. Zwarts and Johnson (2011) has investigated the effect of using extra language models as features in the reranking stage. Qian and Liu (2013) proposed using weighted Max-margin Markov Networks (M3N) to balance precision and recall to further improve the F1-score. Wang et al. (2014) proposed a beam-search decoder which integrates M3N and achieved further improvements. There were also some works that addressed both tasks. Liu et al. (2006) and Baron et al. (1998) carried out sentence unit (SU) and disfluenc</context>
</contexts>
<marker>Georgila, 2009</marker>
<rawString>Kallirroi Georgila. 2009. Using integer linear programming for detecting speech disfluencies. In Proc. of NAACL.</rawString>
</citation>
<citation valid="true">
<authors>
<author>Jing Huang</author>
<author>Geoffrey Zweig</author>
</authors>
<title>Maximum entropy model for punctuation annotation from speech.</title>
<date>2002</date>
<booktitle>In Proc. of INTERSPEECH.</booktitle>
<contexts>
<context position="6542" citStr="Huang and Zweig (2002)" startWordPosition="1042" endWordPosition="1045"> also examine the effect of task order, i.e., performing which task first benefits more. In Section 5, we compare the cascade approach with bi-directional n-best rescoring. In Section 6, we compare the 2- layer Factorial CRF (Sutton et al., 2007) with the cross-product LCRF (Ng and Low, 2004), mixedlabel LCRF (Stolcke et al., 1998), the cascade approach, and the baseline isolated prediction. Section 7 gives a summary of our overall findings. Section 8 gives the conclusion. There were many works on punctuation prediction or disfluency prediction as an isolated task. For punctuation prediction, Huang and Zweig (2002) used maximum entropy model; Christensen et al. (2001) used finite state and multi-layer perceptron method; Liu et al. (2005) used conditional random fields; Lu and Ng (2010) proposed using dynamic conditional random fields for joint sentence boundary type and punctuation prediction; Wang et al. (2012) has added prosodic features for the dynamic conditional random field approach and Zhang et al. (2013) used transition-based parsing. For disfluency prediction, Shriberg et al. (1997) uses purely prosodic features to perform the task. Johnson and Charniak (2004) proposed a TAGbased (Tree-Adjoinin</context>
</contexts>
<marker>Huang, Zweig, 2002</marker>
<rawString>Jing Huang and Geoffrey Zweig. 2002. Maximum entropy model for punctuation annotation from speech. In Proc. of INTERSPEECH.</rawString>
</citation>
<citation valid="true">
<authors>
<author>Mark Johnson</author>
<author>Eugene Charniak</author>
</authors>
<title>A TAGbased noisy-channel model of speech repairs.</title>
<date>2004</date>
<booktitle>In Proc. of ACL.</booktitle>
<contexts>
<context position="7107" citStr="Johnson and Charniak (2004)" startWordPosition="1127" endWordPosition="1130">lated task. For punctuation prediction, Huang and Zweig (2002) used maximum entropy model; Christensen et al. (2001) used finite state and multi-layer perceptron method; Liu et al. (2005) used conditional random fields; Lu and Ng (2010) proposed using dynamic conditional random fields for joint sentence boundary type and punctuation prediction; Wang et al. (2012) has added prosodic features for the dynamic conditional random field approach and Zhang et al. (2013) used transition-based parsing. For disfluency prediction, Shriberg et al. (1997) uses purely prosodic features to perform the task. Johnson and Charniak (2004) proposed a TAGbased (Tree-Adjoining Grammar) noisy channel model. Maskey et al. (2006) proposed a phraselevel machine translation approach for this task. Georgila (2009) used integer linear programming (ILP) which can incorporate local and global constraints. Zwarts and Johnson (2011) has investigated the effect of using extra language models as features in the reranking stage. Qian and Liu (2013) proposed using weighted Max-margin Markov Networks (M3N) to balance precision and recall to further improve the F1-score. Wang et al. (2014) proposed a beam-search decoder which integrates M3N and a</context>
<context position="8730" citStr="Johnson and Charniak, 2004" startWordPosition="1380" endWordPosition="1383">luency labels so that they do not predict punctuation on disfluent tokens. Kim (2004) performed joint SU and Interruption Point (IP) prediction, deriving edit and filler word regions from predicted IPs using a rulebased system as a separate step. In this paper, we treat punctuation prediction and disfluency prediction as a joint prediction task, and compare various state-of-the-art joint prediction methods on this task. 122 3 The Baseline System 3.1 Experimental Setup We use the Switchboard corpus (LDC99T42) in our experiment with the same train/develop/test split as (Qian and Liu, 2013) and (Johnson and Charniak, 2004). The corpus statistics are shown in Table 1. Since the proportion of exclamation marks and incomplete SU boundaries is too small, we convert all exclamation marks to periods and remove all incomplete SU boundaries (treat as no punctuation). In the Switchboard corpus, the utterances of each speaker have already been segmented into short sentences when used in (Qian and Liu, 2013; Johnson and Charniak, 2004). In our work, we concatenate the utterances of each speaker to form one long sequence of words for use as the input to punctuation prediction and disfluency prediction. This form of input w</context>
</contexts>
<marker>Johnson, Charniak, 2004</marker>
<rawString>Mark Johnson and Eugene Charniak. 2004. A TAGbased noisy-channel model of speech repairs. In Proc. of ACL.</rawString>
</citation>
<citation valid="true">
<authors>
<author>Joungbum Kim</author>
</authors>
<title>Automatic detection of sentence boundaries, disfluencies, and conversational fillers in spontaneous speech. Master dissertation of</title>
<date>2004</date>
<institution>University of Washington.</institution>
<contexts>
<context position="8188" citStr="Kim (2004)" startWordPosition="1297" endWordPosition="1298">recision and recall to further improve the F1-score. Wang et al. (2014) proposed a beam-search decoder which integrates M3N and achieved further improvements. There were also some works that addressed both tasks. Liu et al. (2006) and Baron et al. (1998) carried out sentence unit (SU) and disfluency prediction as separate tasks. The difference between SU prediction and punctuation prediction is only in the non-sentence-end punctuation symbols such as commas. Stolcke et al. (1998) mixed sentence boundary labels with disfluency labels so that they do not predict punctuation on disfluent tokens. Kim (2004) performed joint SU and Interruption Point (IP) prediction, deriving edit and filler word regions from predicted IPs using a rulebased system as a separate step. In this paper, we treat punctuation prediction and disfluency prediction as a joint prediction task, and compare various state-of-the-art joint prediction methods on this task. 122 3 The Baseline System 3.1 Experimental Setup We use the Switchboard corpus (LDC99T42) in our experiment with the same train/develop/test split as (Qian and Liu, 2013) and (Johnson and Charniak, 2004). The corpus statistics are shown in Table 1. Since the pr</context>
</contexts>
<marker>Kim, 2004</marker>
<rawString>Joungbum Kim. 2004. Automatic detection of sentence boundaries, disfluencies, and conversational fillers in spontaneous speech. Master dissertation of University of Washington.</rawString>
</citation>
<citation valid="true">
<authors>
<author>Yang Liu</author>
<author>Andreas Stolcke</author>
<author>Elizabeth Shriberg</author>
<author>Mary Harper</author>
</authors>
<title>Using conditional random fields for sentence boundary detection in speech.</title>
<date>2005</date>
<booktitle>In Proc. of ACL.</booktitle>
<contexts>
<context position="6667" citStr="Liu et al. (2005)" startWordPosition="1061" endWordPosition="1064">oach with bi-directional n-best rescoring. In Section 6, we compare the 2- layer Factorial CRF (Sutton et al., 2007) with the cross-product LCRF (Ng and Low, 2004), mixedlabel LCRF (Stolcke et al., 1998), the cascade approach, and the baseline isolated prediction. Section 7 gives a summary of our overall findings. Section 8 gives the conclusion. There were many works on punctuation prediction or disfluency prediction as an isolated task. For punctuation prediction, Huang and Zweig (2002) used maximum entropy model; Christensen et al. (2001) used finite state and multi-layer perceptron method; Liu et al. (2005) used conditional random fields; Lu and Ng (2010) proposed using dynamic conditional random fields for joint sentence boundary type and punctuation prediction; Wang et al. (2012) has added prosodic features for the dynamic conditional random field approach and Zhang et al. (2013) used transition-based parsing. For disfluency prediction, Shriberg et al. (1997) uses purely prosodic features to perform the task. Johnson and Charniak (2004) proposed a TAGbased (Tree-Adjoining Grammar) noisy channel model. Maskey et al. (2006) proposed a phraselevel machine translation approach for this task. Georg</context>
</contexts>
<marker>Liu, Stolcke, Shriberg, Harper, 2005</marker>
<rawString>Yang Liu, Andreas Stolcke, Elizabeth Shriberg, and Mary Harper. 2005. Using conditional random fields for sentence boundary detection in speech. In Proc. of ACL.</rawString>
</citation>
<citation valid="true">
<authors>
<author>Yang Liu</author>
<author>Elizabeth Shriberg</author>
<author>Andreas Stolcke</author>
<author>Dustin Hillard</author>
<author>Mari Ostendorf</author>
<author>Mary Harper</author>
</authors>
<title>Enriching speech recognition with automatic detection of sentence boundaries and disfluencies.</title>
<date>2006</date>
<journal>IEEE Transactions on Audio, Speech, and Language Processing,</journal>
<volume>14</volume>
<issue>5</issue>
<contexts>
<context position="7808" citStr="Liu et al. (2006)" startWordPosition="1236" endWordPosition="1239">06) proposed a phraselevel machine translation approach for this task. Georgila (2009) used integer linear programming (ILP) which can incorporate local and global constraints. Zwarts and Johnson (2011) has investigated the effect of using extra language models as features in the reranking stage. Qian and Liu (2013) proposed using weighted Max-margin Markov Networks (M3N) to balance precision and recall to further improve the F1-score. Wang et al. (2014) proposed a beam-search decoder which integrates M3N and achieved further improvements. There were also some works that addressed both tasks. Liu et al. (2006) and Baron et al. (1998) carried out sentence unit (SU) and disfluency prediction as separate tasks. The difference between SU prediction and punctuation prediction is only in the non-sentence-end punctuation symbols such as commas. Stolcke et al. (1998) mixed sentence boundary labels with disfluency labels so that they do not predict punctuation on disfluent tokens. Kim (2004) performed joint SU and Interruption Point (IP) prediction, deriving edit and filler word regions from predicted IPs using a rulebased system as a separate step. In this paper, we treat punctuation prediction and disflue</context>
</contexts>
<marker>Liu, Shriberg, Stolcke, Hillard, Ostendorf, Harper, 2006</marker>
<rawString>Yang Liu, Elizabeth Shriberg, Andreas Stolcke, Dustin Hillard, Mari Ostendorf, and Mary Harper. 2006. Enriching speech recognition with automatic detection of sentence boundaries and disfluencies. IEEE Transactions on Audio, Speech, and Language Processing, 14(5):1526–1540.</rawString>
</citation>
<citation valid="true">
<authors>
<author>Wei Lu</author>
<author>Hwee Tou Ng</author>
</authors>
<title>Better punctuation prediction with dynamic conditional random fields.</title>
<date>2010</date>
<booktitle>In Proc. of EMNLP.</booktitle>
<contexts>
<context position="2230" citStr="Lu and Ng, 2010" startWordPosition="333" endWordPosition="336">ver, the clique order of features also shows a marked difference. 1 Introduction The raw output from automatic speech recognition (ASR) systems does not have sentence bound1The research reported in this paper was carried out as part of the PhD thesis research of Xuancong Wang at the NUS Graduate School for Integrated Sciences and Engineering. aries or punctuation symbols. Spontaneous speech also contains a significant proportion of disfluency. Researchers have shown that splitting input sequences into sentences and adding in punctuation symbols improve machine translation (Favre et al., 2008; Lu and Ng, 2010). Moreover, disfluencies in speech also introduce noise in downstream tasks like machine translation and information extraction (Wang et al., 2010). Thus, punctuation prediction (PU) and disfluency prediction (DF) are two important post-processing tasks for automatic speech recognition because they improve not only the readability of ASR output, but also the performance of downstream Natural Language Processing (NLP) tasks. The task of punctuation prediction is to insert punctuation symbols into conversational speech texts. Punctuation prediction on long, unsegmented texts also achieves the pu</context>
<context position="6716" citStr="Lu and Ng (2010)" startWordPosition="1070" endWordPosition="1073">ion 6, we compare the 2- layer Factorial CRF (Sutton et al., 2007) with the cross-product LCRF (Ng and Low, 2004), mixedlabel LCRF (Stolcke et al., 1998), the cascade approach, and the baseline isolated prediction. Section 7 gives a summary of our overall findings. Section 8 gives the conclusion. There were many works on punctuation prediction or disfluency prediction as an isolated task. For punctuation prediction, Huang and Zweig (2002) used maximum entropy model; Christensen et al. (2001) used finite state and multi-layer perceptron method; Liu et al. (2005) used conditional random fields; Lu and Ng (2010) proposed using dynamic conditional random fields for joint sentence boundary type and punctuation prediction; Wang et al. (2012) has added prosodic features for the dynamic conditional random field approach and Zhang et al. (2013) used transition-based parsing. For disfluency prediction, Shriberg et al. (1997) uses purely prosodic features to perform the task. Johnson and Charniak (2004) proposed a TAGbased (Tree-Adjoining Grammar) noisy channel model. Maskey et al. (2006) proposed a phraselevel machine translation approach for this task. Georgila (2009) used integer linear programming (ILP) </context>
<context position="25749" citStr="Lu and Ng, 2010" startWordPosition="4285" endWordPosition="4288">n in Table 6 suggest that the rescoring method does not improve over the softcascade baseline. This can be due to the fact that we are using the same four models for the softcascade and the rescoring methods. It may be possible that the information contained in the two models for the soft-cascade PU→DF mostly overlaps with the information contained in the other two models for the soft-cascade DF→PU since all the four models are trained using the same features. Thus, no additional information is gained by combining the four models. 6 The Joint Approach In this section, we compare 2-layer FCRF (Lu and Ng, 2010) with mixed-label LCRF (Stolcke et al., 1998) and cross-product LCRF on the joint prediction task. For the 2-layer FCRF, we use punctuation labels for the first layer and disfluency labels for the second layer (see Table 2). For the mixedlabel LCRF, we split the neutral label {O} into {Comma, Period, QMark, None} so that we have six labels in total, {E, F, Comma, Period, QMark, None}. In this approach, disfluent tokens do not have punctuation labels because in real applications, if we just want to get the cleaned-up/fluent text with punctuations, we do not need to predict punctuations on disfl</context>
</contexts>
<marker>Lu, Ng, 2010</marker>
<rawString>Wei Lu and Hwee Tou Ng. 2010. Better punctuation prediction with dynamic conditional random fields. In Proc. of EMNLP.</rawString>
</citation>
<citation valid="true">
<authors>
<author>Sameer Maskey</author>
<author>Bowen Zhou</author>
<author>Yuqing Gao</author>
</authors>
<title>A phrase-level machine translation approach for disfluency detection using weighted finite state transducers.</title>
<date>2006</date>
<booktitle>In Proc. of INTERSPEECH.</booktitle>
<contexts>
<context position="7194" citStr="Maskey et al. (2006)" startWordPosition="1140" endWordPosition="1143">istensen et al. (2001) used finite state and multi-layer perceptron method; Liu et al. (2005) used conditional random fields; Lu and Ng (2010) proposed using dynamic conditional random fields for joint sentence boundary type and punctuation prediction; Wang et al. (2012) has added prosodic features for the dynamic conditional random field approach and Zhang et al. (2013) used transition-based parsing. For disfluency prediction, Shriberg et al. (1997) uses purely prosodic features to perform the task. Johnson and Charniak (2004) proposed a TAGbased (Tree-Adjoining Grammar) noisy channel model. Maskey et al. (2006) proposed a phraselevel machine translation approach for this task. Georgila (2009) used integer linear programming (ILP) which can incorporate local and global constraints. Zwarts and Johnson (2011) has investigated the effect of using extra language models as features in the reranking stage. Qian and Liu (2013) proposed using weighted Max-margin Markov Networks (M3N) to balance precision and recall to further improve the F1-score. Wang et al. (2014) proposed a beam-search decoder which integrates M3N and achieved further improvements. There were also some works that addressed both tasks. Liu</context>
</contexts>
<marker>Maskey, Zhou, Gao, 2006</marker>
<rawString>Sameer Maskey, Bowen Zhou, and Yuqing Gao. 2006. A phrase-level machine translation approach for disfluency detection using weighted finite state transducers. In Proc. of INTERSPEECH.</rawString>
</citation>
<citation valid="true">
<authors>
<author>Hwee Tou Ng</author>
<author>Jin Kiat Low</author>
</authors>
<title>Chinese part-of-speech tagging: One-at-a-time or all-atonce? Word-based or character-based?</title>
<date>2004</date>
<booktitle>In Proc. of EMNLP.</booktitle>
<contexts>
<context position="6213" citStr="Ng and Low, 2004" startWordPosition="990" endWordPosition="993">ediction techniques on this joint task. In Section 2, we briefly introduce previous work on the two tasks. In Section 3, we describe our baseline system which performs punctuation and disfluency prediction separately (i.e., in isolation). In Section 4, we compare the soft cascade approach with the hard cascade approach. We also examine the effect of task order, i.e., performing which task first benefits more. In Section 5, we compare the cascade approach with bi-directional n-best rescoring. In Section 6, we compare the 2- layer Factorial CRF (Sutton et al., 2007) with the cross-product LCRF (Ng and Low, 2004), mixedlabel LCRF (Stolcke et al., 1998), the cascade approach, and the baseline isolated prediction. Section 7 gives a summary of our overall findings. Section 8 gives the conclusion. There were many works on punctuation prediction or disfluency prediction as an isolated task. For punctuation prediction, Huang and Zweig (2002) used maximum entropy model; Christensen et al. (2001) used finite state and multi-layer perceptron method; Liu et al. (2005) used conditional random fields; Lu and Ng (2010) proposed using dynamic conditional random fields for joint sentence boundary type and punctuatio</context>
<context position="26712" citStr="Ng and Low, 2004" startWordPosition="4449" endWordPosition="4452">otal, {E, F, Comma, Period, QMark, None}. In this approach, disfluent tokens do not have punctuation labels because in real applications, if we just want to get the cleaned-up/fluent text with punctuations, we do not need to predict punctuations on disfluent tokens as they will be removed during the clean-up process. Since this approach does not predict punctuation labels on disfluent tokens, its punctuation F1 score is only evaluated on those fluent tokens. For the crossproduct LCRF, we compose each of the three disfluency labels with the four punctuation labels to get 12 PU-DF-joint labels (Ng and Low, 2004). Figure 2 shows a comparison of these three models in the joint prediction of punctuation and disfluency. All the LCRF and FCRF models are trained using the GRMM toolkit (Sutton, 2006). We use the same feature templates (Table 3) to generate all the features for the toolkit. However, to reduce the training time, we have set clique order to 2 for the transitions and 1 for all other features. We tune the Gaussian prior variance on the development set for all the experiments to obtain the best model for testing. Table 7 shows the comparison of results. On DF alone, the improvement of the cross-p</context>
</contexts>
<marker>Ng, Low, 2004</marker>
<rawString>Hwee Tou Ng and Jin Kiat Low. 2004. Chinese part-of-speech tagging: One-at-a-time or all-atonce? Word-based or character-based? In Proc. of EMNLP.</rawString>
</citation>
<citation valid="true">
<authors>
<author>Franz Josef Och</author>
</authors>
<title>Minimum error rate training in statistical machine translation.</title>
<date>2003</date>
<booktitle>In Proc. ofACL.</booktitle>
<contexts>
<context position="24613" citStr="Och, 2003" startWordPosition="4092" endWordPosition="4093"> A similar rescoring approach using two models is described in Shi and Wang (2007). The experimental framework is shown in Figure 1. For PU—*DF, we first use P(PU|x) to generate an n-best list. Then, for each hypothesis in the n-best list, we use P(DF|PU, x) to obtain another n-best list. So we have n2-best joint hypotheses. We do the same for DF—*PU to obtain another n2-best joint hypotheses. We rescore the 2n2-best list using the four models. The four weights α1, α2, 01, and 02 are tuned to optimize the overall F1 score on the development set. We used the MERT (minimum-error-rate training, (Och, 2003)) algorithm to tune the weights. We also vary the size of n. 126 2 n-best 2 n2-best ... ... P(DF|PU,x) PU-hypo-1 PU-hypo-n P(PU|DF,x) DF-hypo-1 ... ... DF-hypo-n Input Sequence P(PU|x) P(DF|x) joint-hypo-1 joint-hypo-2 ... joint-hypo-1 joint-hypo-2 ... Rescore using: a1 ∙ log𝑃 PU|x +a2 ∙ log𝑃 DF|PU, x +/31 ∙ log𝑃 DF|x +/32 ∙ log𝑃 PU|DF, x Figure 1: Illustration of the rescoring pipeline framework using the four M3N models used in the softcascade method: P(PU|x), P(DF|PU, x), P(DF|x) and P(PU|DF, x) The results shown in Table 6 suggest that the rescoring method does not improve over the softcas</context>
</contexts>
<marker>Och, 2003</marker>
<rawString>Franz Josef Och. 2003. Minimum error rate training in statistical machine translation. In Proc. ofACL.</rawString>
</citation>
<citation valid="true">
<authors>
<author>Xian Qian</author>
<author>Yang Liu</author>
</authors>
<title>Disfluency detection using multi-step stacked learning.</title>
<date>2013</date>
<booktitle>In Proc. of NAACL.</booktitle>
<contexts>
<context position="7508" citStr="Qian and Liu (2013)" startWordPosition="1190" endWordPosition="1193"> conditional random field approach and Zhang et al. (2013) used transition-based parsing. For disfluency prediction, Shriberg et al. (1997) uses purely prosodic features to perform the task. Johnson and Charniak (2004) proposed a TAGbased (Tree-Adjoining Grammar) noisy channel model. Maskey et al. (2006) proposed a phraselevel machine translation approach for this task. Georgila (2009) used integer linear programming (ILP) which can incorporate local and global constraints. Zwarts and Johnson (2011) has investigated the effect of using extra language models as features in the reranking stage. Qian and Liu (2013) proposed using weighted Max-margin Markov Networks (M3N) to balance precision and recall to further improve the F1-score. Wang et al. (2014) proposed a beam-search decoder which integrates M3N and achieved further improvements. There were also some works that addressed both tasks. Liu et al. (2006) and Baron et al. (1998) carried out sentence unit (SU) and disfluency prediction as separate tasks. The difference between SU prediction and punctuation prediction is only in the non-sentence-end punctuation symbols such as commas. Stolcke et al. (1998) mixed sentence boundary labels with disfluenc</context>
<context position="9111" citStr="Qian and Liu, 2013" startWordPosition="1443" endWordPosition="1446"> prediction methods on this task. 122 3 The Baseline System 3.1 Experimental Setup We use the Switchboard corpus (LDC99T42) in our experiment with the same train/develop/test split as (Qian and Liu, 2013) and (Johnson and Charniak, 2004). The corpus statistics are shown in Table 1. Since the proportion of exclamation marks and incomplete SU boundaries is too small, we convert all exclamation marks to periods and remove all incomplete SU boundaries (treat as no punctuation). In the Switchboard corpus, the utterances of each speaker have already been segmented into short sentences when used in (Qian and Liu, 2013; Johnson and Charniak, 2004). In our work, we concatenate the utterances of each speaker to form one long sequence of words for use as the input to punctuation prediction and disfluency prediction. This form of input where, utterances are not pre-segmented into short sentences, better reflects the real-world scenarios and provides a more realistic test setting for punctuation and disfluency prediction. Punctuation prediction also gives rise to sentence segmentation in this setting. Data set train develop test # of tokens 1.3M 85.9K 65.5K # of sentences 173.7K 10.1K 7.9K # of sequences* 1854 1</context>
<context position="10658" citStr="Qian and Liu (2013)" startWordPosition="1709" endWordPosition="1712"> from each speaker. Our baseline system uses M3N (Taskar et al., 2004), one M3N for punctuation prediction and the other for disfluency prediction. We use the same set of punctuation and disfluency labels (as shown in Table 2) throughout this paper. To compare the various isolated, cascade, and joint prediction models, we use the same feature templates for both tasks as listed in Table 3. Since some of the feature templates require predicted filler labels and part-of-speech (POS) tags, we have trained a POS tagger and a filler predictor both using CRF (i.e., using the same approach as that in Qian and Liu (2013)). The same predicted POS tags and fillers are used for feature extraction in all the experiments in this paper for a fair comparison. The degradation on disfluency prediction due to the concatenation of utterances of each speaker is shown in Table 4. The pause duration features are extracted by running forced alignment on the corresponding Switchboard speech corpus (LDC97S62). Table 2: Labels for punctuation prediction and disfluency prediction. 3.2 Features We use the standard NLP features such as F(w_1w0=‘so that’), i.e., the word tokens at the previous and current node position are ‘so’ an</context>
<context position="15043" citStr="Qian and Liu (2013)" startWordPosition="2437" endWordPosition="2440">g, clique order of features up to 3, and labels {E,O} Join utterances into long sen- 71.1 79.2 tences Join utterances into long sen- 71.1 78.2 tences + remove precision/recall balancing Join utterances into long sen- 68.5 76.4 tences + remove precision/recall balancing + reduce clique order of all features Table 4: Baseline results showing the degradation by joining utterances into long sentences, removing precision/recall balancing, and reducing the clique order of features. All models are trained using M3N. We use the standard F1 score as our evaluation metric and this is similar to that in Qian and Liu (2013). For training, we set the frequency pruning threshold to 5 to control the number of parameters. The regularization parameter is tuned on the development set. Since the toolkits used to run different experiments have slightly different limitations, in order to make fair comparisons across different toolkits, we do not use weighting to balance precision and recall when training M3N and we have reduced the clique order of transition features to two and all the other features to one in some of our experiments. Since the performance of filler word prediction on this dataset is already very high, (</context>
</contexts>
<marker>Qian, Liu, 2013</marker>
<rawString>Xian Qian and Yang Liu. 2013. Disfluency detection using multi-step stacked learning. In Proc. of NAACL.</rawString>
</citation>
<citation valid="true">
<authors>
<author>Guergana Savova</author>
<author>Joan Bachenko</author>
</authors>
<title>Prosodic features of four types of disfluencies.</title>
<date>2003</date>
<booktitle>In ISCA Tutorial and Research Workshop on Disfiuency in Spontaneous Speech.</booktitle>
<contexts>
<context position="13999" citStr="Savova and Bachenko, 2003" startWordPosition="2267" endWordPosition="2270">” denotes sentence-end). Feature Template w0 w−1w0 w−2w−1w0 p0 p−1p0 p−2p−1p0 w0w−6-1, w0w1-6 I(wi, wj) I(wi, wj, wi+1, wj+1) I(wi, wj)(wi if wi=wj) I(pi, pj) I(pi, pj, pi+1, pj+1) I(pi, pj)(pi if pi=pj) p−1w0 w−1p0 w−2,=,4Fw−1,=,4F w−3,�Fw−2,�Fw−1,�F p−2,�Fp−1,�F p−3,�Fp−2,�Fp−1,�F ngram-score features pause duration before w0 pause duration after w0 transitions Table 3: Feature templates for disfluency prediction, or punctuation prediction, or joint prediction for all the experiments in this paper. The performance of the system can be further improved by adding additional prosodic features (Savova and Bachenko, 2003; Shriberg et al., 1998; Christensen et al., 2001) apart from pause durations. However, since in this work we focus on model-level comparison, we do not use other prosodic features for simplicity. 3.3 Evaluation and Results Experiment F1 F1 (PU) (DF) Short sentences, with preci- N.A. 84.7 sion/recall balancing, clique order of features up to 3, and labels {E,F,O} Short sentences, with preci- N.A. 84.3 sion/recall balancing, clique order of features up to 3, and labels {E,O} Join utterances into long sen- 71.1 79.2 tences Join utterances into long sen- 71.1 78.2 tences + remove precision/recall</context>
</contexts>
<marker>Savova, Bachenko, 2003</marker>
<rawString>Guergana Savova, Joan Bachenko. 2003. Prosodic features of four types of disfluencies. In ISCA Tutorial and Research Workshop on Disfiuency in Spontaneous Speech.</rawString>
</citation>
<citation valid="true">
<authors>
<author>Yanxin Shi</author>
<author>Mengqiu Wang</author>
</authors>
<title>A dual-layer CRFs based joint decoding method for cascaded segmentation and labeling tasks.</title>
<date>2007</date>
<booktitle>In Proc. of IJCAI.</booktitle>
<contexts>
<context position="24085" citStr="Shi and Wang (2007)" startWordPosition="3993" endWordPosition="3996">sed in terms of the probabilities generated by four models: P(PU|x), P(DF|PU, x), P(DF|x), and P(PU|DF, x). We can combine the four models to form the following joint probability function for rescoring: P(DF, PU|x) = P(DF|x)α1 x P(PU|DF, x)α2 x P(PU|x)β1 x P(DF|PU, x)β2 where α1, α2, 01, and 02 are used to weight the relative importance between (1) and (2); and between the first and second steps. In practice, the probabilities are computed in the log domain where the above expression becomes a weighted sum of the log probabilities. A similar rescoring approach using two models is described in Shi and Wang (2007). The experimental framework is shown in Figure 1. For PU—*DF, we first use P(PU|x) to generate an n-best list. Then, for each hypothesis in the n-best list, we use P(DF|PU, x) to obtain another n-best list. So we have n2-best joint hypotheses. We do the same for DF—*PU to obtain another n2-best joint hypotheses. We rescore the 2n2-best list using the four models. The four weights α1, α2, 01, and 02 are tuned to optimize the overall F1 score on the development set. We used the MERT (minimum-error-rate training, (Och, 2003)) algorithm to tune the weights. We also vary the size of n. 126 2 n-bes</context>
</contexts>
<marker>Shi, Wang, 2007</marker>
<rawString>Yanxin Shi and Mengqiu Wang. 2007. A dual-layer CRFs based joint decoding method for cascaded segmentation and labeling tasks. In Proc. of IJCAI.</rawString>
</citation>
<citation valid="true">
<authors>
<author>Elizabeth Shriberg</author>
<author>Rebecca Bates</author>
<author>Andreas Stolcke</author>
</authors>
<title>A prosody-only decision-tree model for disfluency detection.</title>
<date>1997</date>
<booktitle>In Proc. of Eurospeech.</booktitle>
<contexts>
<context position="7028" citStr="Shriberg et al. (1997)" startWordPosition="1115" endWordPosition="1118">re many works on punctuation prediction or disfluency prediction as an isolated task. For punctuation prediction, Huang and Zweig (2002) used maximum entropy model; Christensen et al. (2001) used finite state and multi-layer perceptron method; Liu et al. (2005) used conditional random fields; Lu and Ng (2010) proposed using dynamic conditional random fields for joint sentence boundary type and punctuation prediction; Wang et al. (2012) has added prosodic features for the dynamic conditional random field approach and Zhang et al. (2013) used transition-based parsing. For disfluency prediction, Shriberg et al. (1997) uses purely prosodic features to perform the task. Johnson and Charniak (2004) proposed a TAGbased (Tree-Adjoining Grammar) noisy channel model. Maskey et al. (2006) proposed a phraselevel machine translation approach for this task. Georgila (2009) used integer linear programming (ILP) which can incorporate local and global constraints. Zwarts and Johnson (2011) has investigated the effect of using extra language models as features in the reranking stage. Qian and Liu (2013) proposed using weighted Max-margin Markov Networks (M3N) to balance precision and recall to further improve the F1-scor</context>
</contexts>
<marker>Shriberg, Bates, Stolcke, 1997</marker>
<rawString>Elizabeth Shriberg, Rebecca Bates and Andreas Stolcke. 1997. A prosody-only decision-tree model for disfluency detection. In Proc. of Eurospeech.</rawString>
</citation>
<citation valid="true">
<authors>
<author>Elizabeth Shriberg</author>
<author>Andreas Stolcke</author>
<author>Daniel Jurafsky</author>
<author>Noah Coccaro</author>
<author>Marie Meteer</author>
<author>Rebecca Bates</author>
<author>Paul Taylor</author>
<author>Klaus Ries</author>
<author>Rachel Martin</author>
<author>Carol Van Ess-Dykema</author>
</authors>
<title>Can prosody aid the automatic classification of dialog acts in conversational speech?</title>
<date>1998</date>
<booktitle>In Language and speech 41,</booktitle>
<pages>3--4</pages>
<marker>Shriberg, Stolcke, Jurafsky, Coccaro, Meteer, Bates, Taylor, Ries, Martin, Van Ess-Dykema, 1998</marker>
<rawString>Elizabeth Shriberg, Andreas Stolcke, Daniel Jurafsky, Noah Coccaro, Marie Meteer, Rebecca Bates, Paul Taylor, Klaus Ries, Rachel Martin, and Carol Van Ess-Dykema. 1998. Can prosody aid the automatic classification of dialog acts in conversational speech? In Language and speech 41, no. 3-4: 443-492.</rawString>
</citation>
<citation valid="true">
<authors>
<author>Andreas Stolcke</author>
<author>Elizabeth Shriberg</author>
<author>Rebecca A Bates</author>
<author>Mari Ostendorf</author>
<author>Dilek Hakkani</author>
<author>Madelaine Plauche</author>
<author>Gokhan Tur</author>
<author>Yu Lu</author>
</authors>
<title>Automatic detection of sentence boundaries and disfluencies based on recognized words.</title>
<date>1998</date>
<booktitle>In Proc. of ICSLP.</booktitle>
<contexts>
<context position="6253" citStr="Stolcke et al., 1998" startWordPosition="997" endWordPosition="1000">sk. In Section 2, we briefly introduce previous work on the two tasks. In Section 3, we describe our baseline system which performs punctuation and disfluency prediction separately (i.e., in isolation). In Section 4, we compare the soft cascade approach with the hard cascade approach. We also examine the effect of task order, i.e., performing which task first benefits more. In Section 5, we compare the cascade approach with bi-directional n-best rescoring. In Section 6, we compare the 2- layer Factorial CRF (Sutton et al., 2007) with the cross-product LCRF (Ng and Low, 2004), mixedlabel LCRF (Stolcke et al., 1998), the cascade approach, and the baseline isolated prediction. Section 7 gives a summary of our overall findings. Section 8 gives the conclusion. There were many works on punctuation prediction or disfluency prediction as an isolated task. For punctuation prediction, Huang and Zweig (2002) used maximum entropy model; Christensen et al. (2001) used finite state and multi-layer perceptron method; Liu et al. (2005) used conditional random fields; Lu and Ng (2010) proposed using dynamic conditional random fields for joint sentence boundary type and punctuation prediction; Wang et al. (2012) has add</context>
<context position="8062" citStr="Stolcke et al. (1998)" startWordPosition="1275" endWordPosition="1278">uage models as features in the reranking stage. Qian and Liu (2013) proposed using weighted Max-margin Markov Networks (M3N) to balance precision and recall to further improve the F1-score. Wang et al. (2014) proposed a beam-search decoder which integrates M3N and achieved further improvements. There were also some works that addressed both tasks. Liu et al. (2006) and Baron et al. (1998) carried out sentence unit (SU) and disfluency prediction as separate tasks. The difference between SU prediction and punctuation prediction is only in the non-sentence-end punctuation symbols such as commas. Stolcke et al. (1998) mixed sentence boundary labels with disfluency labels so that they do not predict punctuation on disfluent tokens. Kim (2004) performed joint SU and Interruption Point (IP) prediction, deriving edit and filler word regions from predicted IPs using a rulebased system as a separate step. In this paper, we treat punctuation prediction and disfluency prediction as a joint prediction task, and compare various state-of-the-art joint prediction methods on this task. 122 3 The Baseline System 3.1 Experimental Setup We use the Switchboard corpus (LDC99T42) in our experiment with the same train/develop</context>
<context position="25794" citStr="Stolcke et al., 1998" startWordPosition="4292" endWordPosition="4295">method does not improve over the softcascade baseline. This can be due to the fact that we are using the same four models for the softcascade and the rescoring methods. It may be possible that the information contained in the two models for the soft-cascade PU→DF mostly overlaps with the information contained in the other two models for the soft-cascade DF→PU since all the four models are trained using the same features. Thus, no additional information is gained by combining the four models. 6 The Joint Approach In this section, we compare 2-layer FCRF (Lu and Ng, 2010) with mixed-label LCRF (Stolcke et al., 1998) and cross-product LCRF on the joint prediction task. For the 2-layer FCRF, we use punctuation labels for the first layer and disfluency labels for the second layer (see Table 2). For the mixedlabel LCRF, we split the neutral label {O} into {Comma, Period, QMark, None} so that we have six labels in total, {E, F, Comma, Period, QMark, None}. In this approach, disfluent tokens do not have punctuation labels because in real applications, if we just want to get the cleaned-up/fluent text with punctuations, we do not need to predict punctuations on disfluent tokens as they will be removed during th</context>
<context position="27706" citStr="Stolcke et al. (1998)" startWordPosition="4618" endWordPosition="4621">r all other features. We tune the Gaussian prior variance on the development set for all the experiments to obtain the best model for testing. Table 7 shows the comparison of results. On DF alone, the improvement of the cross-product LCRF over the mixed-label LCRF, and the improvement of the mixed-label LCRF over the isolated baseline are not statistically significant. However, if we test the statistical significance on the overall performance of both PU and DF, both the 2-layer FCRF and the cross-product LCRF perform better than the mixed-label LCRF. And we also obtain the same conclusion as Stolcke et al. (1998) that mixed-label LCRF performs better than isolated prediction. However, for the comparison between the 2-layer FCRF and the cross-product LCRF, although the 2-layer FCRF performs better than the cross-product LCRF on disfluency prediction, it does worse on punctuation prediction. Overall, the two methods perform about the same, their difference is not statistically significant. In addition, both the 2-layer FCRF and the cross-product LCRF slightly outperform the soft cascade method (statistical significance at p=0.04). 127 Experiment F1 for PU F1 for DF isolated baseline 71.1 78.2 soft-casca</context>
</contexts>
<marker>Stolcke, Shriberg, Bates, Ostendorf, Hakkani, Plauche, Tur, Lu, 1998</marker>
<rawString>Andreas Stolcke, Elizabeth Shriberg, Rebecca A. Bates, Mari Ostendorf, Dilek Hakkani, Madelaine Plauche, Gokhan Tur, and Yu Lu. 1998. Automatic detection of sentence boundaries and disfluencies based on recognized words. In Proc. of ICSLP.</rawString>
</citation>
<citation valid="true">
<authors>
<author>Charles Sutton</author>
</authors>
<date>2006</date>
<note>GRMM: GRaphical Models in Mallet. http://mallet.cs.umass.edu/grmm/</note>
<contexts>
<context position="26897" citStr="Sutton, 2006" startWordPosition="4483" endWordPosition="4484">ith punctuations, we do not need to predict punctuations on disfluent tokens as they will be removed during the clean-up process. Since this approach does not predict punctuation labels on disfluent tokens, its punctuation F1 score is only evaluated on those fluent tokens. For the crossproduct LCRF, we compose each of the three disfluency labels with the four punctuation labels to get 12 PU-DF-joint labels (Ng and Low, 2004). Figure 2 shows a comparison of these three models in the joint prediction of punctuation and disfluency. All the LCRF and FCRF models are trained using the GRMM toolkit (Sutton, 2006). We use the same feature templates (Table 3) to generate all the features for the toolkit. However, to reduce the training time, we have set clique order to 2 for the transitions and 1 for all other features. We tune the Gaussian prior variance on the development set for all the experiments to obtain the best model for testing. Table 7 shows the comparison of results. On DF alone, the improvement of the cross-product LCRF over the mixed-label LCRF, and the improvement of the mixed-label LCRF over the isolated baseline are not statistically significant. However, if we test the statistical sign</context>
<context position="30664" citStr="Sutton, 2006" startWordPosition="5131" endWordPosition="5132">diction do influence each other. The output from one task does provide useful information that can improve the other task. All the approaches studied in this work, which link the two tasks together, perform better than their corresponding Experiment F1 for PU F1 for DF isolated baseline 68.7 77.0 soft cascade 69.0 77.5 mixed-label LCRF 69.0 77.2 cross-product LCRF 69.9 77.3 2-layer FCRF 69.2 77.8 Table 7: Performance comparison among 2- layer FCRF, mixed-label LCRF and cross-product LCRF, with respect to the soft-cascade and the isolated prediction baseline. All models are trained using GRMM (Sutton, 2006), with reduced clique orders. isolated prediction baseline. Secondly, as compared to the soft cascade, the hard cascade passes more information from the first step into the second step, and thus is much more sensitive to errors in the first step. In practice, unless the first step has very high accuracy, soft cascade is expected to do better than hard cascade. Thirdly, if we train a model using a fine-grained label set but test it on the same coarse-grained label set, we are very likely to get improvement. For example: • The edit word F1 for mixed edit and filler prediction using {E, F, O} is </context>
</contexts>
<marker>Sutton, 2006</marker>
<rawString>Charles Sutton. 2006. GRMM: GRaphical Models in Mallet. http://mallet.cs.umass.edu/grmm/</rawString>
</citation>
<citation valid="true">
<authors>
<author>Charles Sutton</author>
<author>Andrew McCallum</author>
<author>Khashayar Rohanimanesh</author>
</authors>
<title>Dynamic conditional random fields: factorized probabilistic models for labeling and segmenting sequence data.</title>
<date>2007</date>
<journal>In Journal of Machine Learning Research,</journal>
<volume>8</volume>
<pages>693--723</pages>
<contexts>
<context position="6166" citStr="Sutton et al., 2007" startWordPosition="982" endWordPosition="985">pare a variety of common state-of-the-art joint prediction techniques on this joint task. In Section 2, we briefly introduce previous work on the two tasks. In Section 3, we describe our baseline system which performs punctuation and disfluency prediction separately (i.e., in isolation). In Section 4, we compare the soft cascade approach with the hard cascade approach. We also examine the effect of task order, i.e., performing which task first benefits more. In Section 5, we compare the cascade approach with bi-directional n-best rescoring. In Section 6, we compare the 2- layer Factorial CRF (Sutton et al., 2007) with the cross-product LCRF (Ng and Low, 2004), mixedlabel LCRF (Stolcke et al., 1998), the cascade approach, and the baseline isolated prediction. Section 7 gives a summary of our overall findings. Section 8 gives the conclusion. There were many works on punctuation prediction or disfluency prediction as an isolated task. For punctuation prediction, Huang and Zweig (2002) used maximum entropy model; Christensen et al. (2001) used finite state and multi-layer perceptron method; Liu et al. (2005) used conditional random fields; Lu and Ng (2010) proposed using dynamic conditional random fields </context>
</contexts>
<marker>Sutton, McCallum, Rohanimanesh, 2007</marker>
<rawString>Charles Sutton, Andrew McCallum, and Khashayar Rohanimanesh. 2007. Dynamic conditional random fields: factorized probabilistic models for labeling and segmenting sequence data. In Journal of Machine Learning Research, 8: 693–723.</rawString>
</citation>
<citation valid="true">
<authors>
<author>Ben Taskar</author>
<author>Carlos Guestrin</author>
<author>Daphne Koller</author>
</authors>
<title>Max-margin Markov networks.</title>
<date>2004</date>
<booktitle>In Proc. of NIPS.</booktitle>
<contexts>
<context position="10109" citStr="Taskar et al., 2004" startWordPosition="1615" endWordPosition="1618">n and disfluency prediction. Punctuation prediction also gives rise to sentence segmentation in this setting. Data set train develop test # of tokens 1.3M 85.9K 65.5K # of sentences 173.7K 10.1K 7.9K # of sequences* 1854 174 134 # of edit words 63.6K 4.7K 3.7K # of filler words 137.1K 9.6K 7.3K # of Commas 52.7K 1.8K 2.1K # of Periods 97.6K 6.5K 4.5K # of Questions 6.8K 363 407 # of Exclamations 67 4 1 # of Incomplete 189 2 0 Table 1: Corpus statistics for all the experiments. *: each conversation produces two long/sentencejoined sequences, one from each speaker. Our baseline system uses M3N (Taskar et al., 2004), one M3N for punctuation prediction and the other for disfluency prediction. We use the same set of punctuation and disfluency labels (as shown in Table 2) throughout this paper. To compare the various isolated, cascade, and joint prediction models, we use the same feature templates for both tasks as listed in Table 3. Since some of the feature templates require predicted filler labels and part-of-speech (POS) tags, we have trained a POS tagger and a filler predictor both using CRF (i.e., using the same approach as that in Qian and Liu (2013)). The same predicted POS tags and fillers are used</context>
</contexts>
<marker>Taskar, Guestrin, Koller, 2004</marker>
<rawString>Ben Taskar, Carlos Guestrin, and Daphne Koller. 2004. Max-margin Markov networks. In Proc. of NIPS.</rawString>
</citation>
<citation valid="true">
<authors>
<author>Wen Wang</author>
</authors>
<title>Gokhan Tur, Jing Zheng, and Necip Fazil Ayan.</title>
<date>2010</date>
<booktitle>In Proc. of ICASSP.</booktitle>
<marker>Wang, 2010</marker>
<rawString>Wen Wang, Gokhan Tur, Jing Zheng, and Necip Fazil Ayan. 2010. Automatic disfluency removal for improving spoken language translation. In Proc. of ICASSP.</rawString>
</citation>
<citation valid="true">
<authors>
<author>Xuancong Wang</author>
<author>Hwee Tou Ng</author>
<author>Khe Chai Sim</author>
</authors>
<title>Dynamic conditional random fields for joint sentence boundary and punctuation prediction.</title>
<date>2012</date>
<booktitle>In Proc. of Interspeech.</booktitle>
<contexts>
<context position="6845" citStr="Wang et al. (2012)" startWordPosition="1089" endWordPosition="1092">CRF (Stolcke et al., 1998), the cascade approach, and the baseline isolated prediction. Section 7 gives a summary of our overall findings. Section 8 gives the conclusion. There were many works on punctuation prediction or disfluency prediction as an isolated task. For punctuation prediction, Huang and Zweig (2002) used maximum entropy model; Christensen et al. (2001) used finite state and multi-layer perceptron method; Liu et al. (2005) used conditional random fields; Lu and Ng (2010) proposed using dynamic conditional random fields for joint sentence boundary type and punctuation prediction; Wang et al. (2012) has added prosodic features for the dynamic conditional random field approach and Zhang et al. (2013) used transition-based parsing. For disfluency prediction, Shriberg et al. (1997) uses purely prosodic features to perform the task. Johnson and Charniak (2004) proposed a TAGbased (Tree-Adjoining Grammar) noisy channel model. Maskey et al. (2006) proposed a phraselevel machine translation approach for this task. Georgila (2009) used integer linear programming (ILP) which can incorporate local and global constraints. Zwarts and Johnson (2011) has investigated the effect of using extra language</context>
</contexts>
<marker>Wang, Ng, Sim, 2012</marker>
<rawString>Xuancong Wang, Hwee Tou Ng, and Khe Chai Sim. 2012. Dynamic conditional random fields for joint sentence boundary and punctuation prediction. In Proc. of Interspeech.</rawString>
</citation>
<citation valid="true">
<authors>
<author>Xuancong Wang</author>
<author>Hwee Tou Ng</author>
<author>Khe Chai Sim</author>
</authors>
<title>A beam-search decoder for disfluency detection.</title>
<date>2014</date>
<booktitle>In Proc. of COLING.</booktitle>
<contexts>
<context position="7649" citStr="Wang et al. (2014)" startWordPosition="1211" endWordPosition="1214">es purely prosodic features to perform the task. Johnson and Charniak (2004) proposed a TAGbased (Tree-Adjoining Grammar) noisy channel model. Maskey et al. (2006) proposed a phraselevel machine translation approach for this task. Georgila (2009) used integer linear programming (ILP) which can incorporate local and global constraints. Zwarts and Johnson (2011) has investigated the effect of using extra language models as features in the reranking stage. Qian and Liu (2013) proposed using weighted Max-margin Markov Networks (M3N) to balance precision and recall to further improve the F1-score. Wang et al. (2014) proposed a beam-search decoder which integrates M3N and achieved further improvements. There were also some works that addressed both tasks. Liu et al. (2006) and Baron et al. (1998) carried out sentence unit (SU) and disfluency prediction as separate tasks. The difference between SU prediction and punctuation prediction is only in the non-sentence-end punctuation symbols such as commas. Stolcke et al. (1998) mixed sentence boundary labels with disfluency labels so that they do not predict punctuation on disfluent tokens. Kim (2004) performed joint SU and Interruption Point (IP) prediction, d</context>
</contexts>
<marker>Wang, Ng, Sim, 2014</marker>
<rawString>Xuancong Wang, Hwee Tou Ng, and Khe Chai Sim. 2014. A beam-search decoder for disfluency detection. In Proc. of COLING.</rawString>
</citation>
<citation valid="true">
<authors>
<author>Dongdong Zhang</author>
<author>Shuangzhi Wu</author>
<author>Nan Yang</author>
<author>Mu Li</author>
</authors>
<title>Punctuation prediction with transitionbased parsing.</title>
<date>2013</date>
<booktitle>In Proc. of ACL.</booktitle>
<contexts>
<context position="6947" citStr="Zhang et al. (2013)" startWordPosition="1105" endWordPosition="1108">es a summary of our overall findings. Section 8 gives the conclusion. There were many works on punctuation prediction or disfluency prediction as an isolated task. For punctuation prediction, Huang and Zweig (2002) used maximum entropy model; Christensen et al. (2001) used finite state and multi-layer perceptron method; Liu et al. (2005) used conditional random fields; Lu and Ng (2010) proposed using dynamic conditional random fields for joint sentence boundary type and punctuation prediction; Wang et al. (2012) has added prosodic features for the dynamic conditional random field approach and Zhang et al. (2013) used transition-based parsing. For disfluency prediction, Shriberg et al. (1997) uses purely prosodic features to perform the task. Johnson and Charniak (2004) proposed a TAGbased (Tree-Adjoining Grammar) noisy channel model. Maskey et al. (2006) proposed a phraselevel machine translation approach for this task. Georgila (2009) used integer linear programming (ILP) which can incorporate local and global constraints. Zwarts and Johnson (2011) has investigated the effect of using extra language models as features in the reranking stage. Qian and Liu (2013) proposed using weighted Max-margin Mar</context>
</contexts>
<marker>Zhang, Wu, Yang, Li, 2013</marker>
<rawString>Dongdong Zhang, Shuangzhi Wu, Nan Yang, and Mu Li. 2013. Punctuation prediction with transitionbased parsing. In Proc. of ACL.</rawString>
</citation>
<citation valid="true">
<authors>
<author>Simon Zwarts</author>
<author>Mark Johnson</author>
</authors>
<title>The impact of language models and loss functions on repair disfluency detection.</title>
<date>2011</date>
<booktitle>In Proc. ofACL.</booktitle>
<contexts>
<context position="7393" citStr="Zwarts and Johnson (2011)" startWordPosition="1169" endWordPosition="1172">r joint sentence boundary type and punctuation prediction; Wang et al. (2012) has added prosodic features for the dynamic conditional random field approach and Zhang et al. (2013) used transition-based parsing. For disfluency prediction, Shriberg et al. (1997) uses purely prosodic features to perform the task. Johnson and Charniak (2004) proposed a TAGbased (Tree-Adjoining Grammar) noisy channel model. Maskey et al. (2006) proposed a phraselevel machine translation approach for this task. Georgila (2009) used integer linear programming (ILP) which can incorporate local and global constraints. Zwarts and Johnson (2011) has investigated the effect of using extra language models as features in the reranking stage. Qian and Liu (2013) proposed using weighted Max-margin Markov Networks (M3N) to balance precision and recall to further improve the F1-score. Wang et al. (2014) proposed a beam-search decoder which integrates M3N and achieved further improvements. There were also some works that addressed both tasks. Liu et al. (2006) and Baron et al. (1998) carried out sentence unit (SU) and disfluency prediction as separate tasks. The difference between SU prediction and punctuation prediction is only in the non-s</context>
</contexts>
<marker>Zwarts, Johnson, 2011</marker>
<rawString>Simon Zwarts and Mark Johnson. 2011. The impact of language models and loss functions on repair disfluency detection. In Proc. ofACL.</rawString>
</citation>
</citationList>
</algorithm>
</algorithms>
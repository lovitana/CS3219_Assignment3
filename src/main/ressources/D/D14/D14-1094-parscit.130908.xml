<?xml version="1.0" encoding="UTF-8"?>
<algorithms version="110505">
<algorithm name="SectLabel" version="110505">
<variant no="0" confidence="0.000010">
<title confidence="0.987747">
Balanced Korean Word Spacing with Structural SVM
</title>
<author confidence="0.996372">
Changki Lee* Edward Choi Hyunki Kim
</author>
<affiliation confidence="0.772973">
*Kangwon National University, Chuncheon-si, Gangwondo, 200-701, Korea
Electronics and Telecommunications Research Institute, Daejeon, 305-350, Korea
</affiliation>
<email confidence="0.975283">
leeck@kangwon.ac.kr mp2893@gmail.com hkk@etri.re.kr
</email>
<sectionHeader confidence="0.982532" genericHeader="abstract">
Abstract
</sectionHeader>
<figureCaption confidence="0.5848106875">
Most studies on statistical Korean word spac-
ing do not utilize the information provided by
the input sentence and assume that it was
completely concatenated. This makes the word
spacer ignore the correct spaced parts of the
input sentence and erroneously alter them. To
overcome such limit, this paper proposes a
structural SVM-based Korean word spacing
method that can utilize the space information
of the input sentence. The experiment on sen-
tences with 10% spacing errors showed that
our method achieved 96.81% F-score, while
the basic structural SVM method only
achieved 92.53% F-score. The more the input
sentence was correctly spaced, the more accu-
rately our method performed.
</figureCaption>
<sectionHeader confidence="0.997186" genericHeader="introduction">
1 Introduction
</sectionHeader>
<bodyText confidence="0.99996616">
Automatic word spacing is a task to decide
boundaries between words, which is frequently
used for correcting spacing errors of text mes-
sages, Tweets, or Internet comments before using
them in information retrieval applications (Lee
and Kim, 2012). It is also often used in post-
processing optical character recognition (OCR)
or voice recognition (Lee et al., 2007). Except
for some Asian languages such as Chinese, Japa-
nese and Thai, most languages have explicit
word spacing that improves the readability of the
text and helps readers better understand the
meaning of it. Korean especially has a tricky
word spacing system and users often make mis-
takes, which makes automatic word spacing an
interesting and essential task.
In order to easily acquire the training data,
most studies on statistical Korean word spacing
assume that well-spaced raw text (e.g. newspaper
articles) is perfectly spaced and use it for training
(Lee and Kim, 2012; Lee and Kim, 2013; Lee et
al., 2007; Shim, 2011). This approach, however,
cannot observe incorrect spacing since the as-
sumption makes the training data devoid of nega-
tive example. Consequently, word spacers cannot
use the spacing information given by the user,
and erroneously alter the correctly spaced parts
of the sentence. To utilize the user-given spacing
information, a corpus of input sentences and their
correctly spaced version is necessary. Construct-
ing such corpus, however, requires much time
and resource.
In this paper, to resolve such issue, we propose
a structural SVM-based Korean word spacing
model that can utilize the word spacing infor-
mation given by the user. We name the proposed
model “Balanced Word Spacing Model
(BWSM)”. Our approach trains a basic structural
SVM-based Korean word spacing model as in
(Lee and Kim, 2013), and tries to obtain the sen-
tence which achieves the maximum score for the
basic model while minimally altering the input
sentence.
In the following section, we discuss related
studies. In Section 3, the proposed method and
its relation to Karush-Kuhn-Tucker (KKT) con-
dition are explained. The experiment and discus-
sion is presented in Section 4. Finally, in Section
5, the conclusion and future work for this study
is given.
</bodyText>
<sectionHeader confidence="0.999693" genericHeader="related work">
2 Related Work
</sectionHeader>
<bodyText confidence="0.999906">
There are two common approaches to Korean
word spacing: rule-based approach and statistical
approach. In rule-based approach, it is not easy
to construct rules and maintain them. Further-
more, it requires morphological analysis to apply
rule-based approach, which slows down the pro-
cess. Recent studies, therefore, mostly focus on
the statistical approach.
Most statistical approaches use well-spaced
raw corpus as training data (e.g. newspaper arti-
cles) assuming that they are perfectly spaced.
This is to avoid the expensive job of constructing
new training data. Lee et al. (2007) treated the
word spacing task as a sequence labeling prob-
lem on the input sentence which is a sequence of
syllables. They proposed a method based on
Hidden Markov Model (HMM). Shim (2011)
also considered the word spacing task as a se-
quence labeling problem and proposed a method
using Conditional Random Field (CRF) (Lafferty
et al., 2001), which is a well-known powerful
model for sequence labeling tasks. Lee and Kim
</bodyText>
<page confidence="0.973962">
875
</page>
<bodyText confidence="0.966194909090909">
Proceedings of the 2014 Conference on Empirical Methods in Natural Language Processing (EMNLP), pages 875–879,
October 25-29, 2014, Doha, Qatar. c�2014 Association for Computational Linguistics
(2013) tried to solve the sequence labeling prob-
lem using structural SVM (Tsochantaridis et al.,
2004; Joachims et al., 2009; Lee and Jang 2010;
Shalev-Shwartz et al., 2011).
The studies above (Lee and Kim, 2013; Lee et
al., 2007; Shim, 2011), however, do not take ad-
vantage of the spacing information provided by
the user, and often erroneously alter the correctly
spaced part of the sentence. Lee et al. (2007)
tries to resolve this issue by combining an HMM
model with an additional confidence model con-
structed from another corpus. Given an input
sentence, they first apply the basic HMM model
to obtain a candidate sentence. For every differ-
ent word spacing between the input sentence and
the candidate sentence, they calculate and com-
pare the confidence using the confidence model,
and whichever gets the higher confidence is used.
The spacing accuracy was improved from 97.52%
to 97.64%1.
This study is similar to (Lee et al., 2007) in
that it utilizes the spacing information given by
the user. But unlike (Lee et al., 2007), BWSM
uses structural SVM as the basic model and do
not require an additional confidence model. Fur-
thermore, while Lee et al. (2007) compares the
spacing confidence for each syllable to obtain the
final outcome, BWSM considers the whole sen-
tence when altering its spacing, enabling it to
achieve higher improvement on performance
(from 92.53% F-score to 96.81% F-score).
</bodyText>
<sectionHeader confidence="0.982052" genericHeader="method">
3 Balanced Word Spacing Model
</sectionHeader>
<bodyText confidence="0.995592090909091">
Like previous studies, the proposed model treats
the Korean word spacing task as a sequence la-
beling problem. The label consists of B and I,
which are assigned to each syllable of the sen-
tence. Assuming that x = &lt;x1, x2, ..., xT&gt; is a se-
quence of total T syllables of the input sentence
and y = &lt;y1, y2, ..., yT&gt; is a sequence of labels
for each syllable, an example could be given as
follows2:
Input: ah/beo/ji/ga bang/eh deul/eo/ga/sin/da
(Father entered the room)
</bodyText>
<equation confidence="0.6224385">
x = &lt;ah, beo, ji, ga, bang, eh, deul, eo, ga, sin, da&gt;
y = &lt;B, I, I, I, B, I, B, I, I, I, I&gt;
</equation>
<figureCaption confidence="0.997769">
Figure 1: An example of word spacing.
</figureCaption>
<bodyText confidence="0.99809">
In order to utilize the spacing information pro-
vided by the user, we propose a new model, the
</bodyText>
<footnote confidence="0.8616565">
1 Accuracy was calculated based on syllables.
2 Slashes are used for distinguishing between syllables.
</footnote>
<listItem confidence="0.721482">
Balanced Word Spacing Model that adheres to
the following principles:
1. The model must obtain the most likely se-
quence of labels(y*), while minimally altering
the user-given sequence of labels (𝐲input).
2. We assume that it costs α per syllable to
change the spacing of the original sentence, in
order to keep the original spacing information
as much as possible.
</listItem>
<bodyText confidence="0.974073">
Mathematically formulating the above princi-
ples would give us the following equation:
</bodyText>
<equation confidence="0.989582">
𝐲∗ = argmax score 𝐱, 𝐲 − 𝛼 ∙ 𝐿 𝐲input, 𝐲 (1)
</equation>
<bodyText confidence="0.9763789">
In Equation 1, score 𝐱, 𝐲 calculates how com-
patible the sequence of label y is with the input
sentence x. It is calculated by a basic word spac-
ing model as in (Lee and Kim, 2013).
𝐿 𝐲input, 𝐲 counts the number of different la-
bels between the user-given sequence of labels
𝐲input and an arbitrary sequence of labels 𝐲. 𝐲∗
of Equation 1 can be obtained by setting the gra-
dient of score 𝐱, 𝐲 − 𝛼 ∙ 𝐿 𝐲input, 𝐲 to 0,
which is equivalent to the following equation:
</bodyText>
<equation confidence="0.962451">
∇score 𝐱, 𝐲∗ = 𝛼 ∙ ∇𝐿 𝐲inrut• 𝐲∗ (2)
</equation>
<bodyText confidence="0.9999086">
In order to view the proposed model in a dif-
ferent perspective, we consider BWSM in terms
of Karush-Kuhn-Tucker (KKT) condition. KKT
condition is a technique for solving optimization
problems with inequality constraints. It is a gen-
eralized version of Lagrange multipliers, which
is a technique for solving optimization problems
with equality constraints. Converting the afore-
mentioned principles to a constrained optimiza-
tion problem gives:
</bodyText>
<equation confidence="0.7094695">
Maximize: score 𝐱, 𝐲
subject to 𝐿 𝐲inrut• 𝐲 ≤ 𝑏 (3)
</equation>
<bodyText confidence="0.999294375">
Equation 3 tries to obtain y that maximizes
score 𝐱, 𝐲 , namely the score of the basic model,
while maintaining 𝐿 𝐲input, 𝐲 below b, which is
equivalent to altering the word spacing of the
input sentence less than or equal to b times. To
solve this constrained optimization problem, we
apply KKT condition and define a new Lagran-
gian function as follows:
</bodyText>
<equation confidence="0.951162">
Λ 𝐱, 𝐲, 𝛼 =score 𝐱, 𝐲 − 𝛼 𝐿 𝐲nnput• 𝐲 − 𝑏 (4)
</equation>
<page confidence="0.972277">
876
</page>
<bodyText confidence="0.76529">
Setting the gradient of the Equation 4 to zero,
</bodyText>
<equation confidence="0.365569">
namely ∇Λ 𝐱, 𝐲, 𝛼 = 0, we get the following
necessary conditions:
Stationarity: ∇score 𝐱, 𝐲∗ = 𝛼∗ ∇𝐿 𝐲input, 𝐲∗
Primal feasibility: 𝐿 𝐲i, put, 𝐲∗ ≤ 𝑏
Dual feasibility: 𝛼∗ ≥ 0
Complementary slackness: 𝛼∗ 𝐿 𝐲input, 𝐲∗ − 𝑏 = 0 (5)
</equation>
<bodyText confidence="0.97323208">
Comparing Equation 1 with Equation 4 reveals
that they are the same except the constant b. And
𝐲∗ which satisfies the conditions of Equation 5,
and hence the solution to Equation 4, is also the
same as 𝐲∗ which satisfies Equation 2, and hence
the solution to Equation 1.
For the basic word spacing model, we use
margin rescaled version of structural SVM as
Lee and Kim (2013). The objective function of
structural SVM is as follows:
min𝒘,, 2 ∥ 𝐰 ∥2 + n n i, 𝑠. 𝑡. ∀ 𝑖, 𝜉i 0∀𝑖, ∀𝐲 ∈ Y\𝐲i: 𝐰T𝛿Ψ 𝐱i, 𝐲 ≥ 𝐿 𝐲i, 𝐲 − 𝜉i
where 𝛿Ψ 𝐱i, 𝐲 = Ψ 𝐱i, 𝐲i − Ψ 𝐱i, 𝐲 (6)
In Equation 6, 𝐱i, 𝐲i represents the i-th se-
quence of syllables and its correct spacing labels.
𝐿 𝐲i, 𝐲 is a loss function that counts the number
of different labels between the correct labels 𝐲i
and the predicted sequence of labels 𝐲. Ψ 𝐱, 𝐲 is
a typical feature vector function. The features
used for the basic word spacing model are the
same features used in (Lee and Kim, 2013).
Since structural SVM was used for the basic
word spacing model, the score function of Equa-
tion 1 becomes score 𝐱, 𝐲 = 𝐰TΨ 𝐱, 𝐲 .
We propose two approaches for implementing
Equation 1.
</bodyText>
<listItem confidence="0.926828666666667">
1. N-best re-ranking: N-best sequences of spac-
ing labels are obtained using the basic struc-
tural SVM model. For each of the sequence,
</listItem>
<equation confidence="0.688542">
𝛼∗𝐿 𝐲input, 𝐲∗ is calculated and subtracted
</equation>
<bodyText confidence="0.824679">
from score 𝐱, 𝐲 . The result of the subtrac-
tion is used to re-rank the sequences, and the
one with the highest rank is chosen.
2. Modified Viterbi search: Viterbi search algo-
rithm, which is used in the basic word spacing
model to solve 𝐲∗ = argmax 𝐰TΨ 𝐱, 𝐲 , is
modified to solve 𝐲∗ = argmax 𝐰TΨ 𝐱, 𝐲 −
</bodyText>
<equation confidence="0.8215505">
𝛼∙ 𝐿 𝐲input, 𝐲 . Both Ψ 𝐱, 𝐲 and
𝐿 𝐲input, 𝐲 can be calculated syllable by syl-
</equation>
<bodyText confidence="0.999981818181818">
lable, which makes it easy to modify Viterbi
search algorithm.
The first approach seems straightforward and
easy, but it would take a long time to obtain N-
best sequences of labels. Furthermore, the correct
label sequence might not be in those N-best se-
quences, hence degrading the overall perfor-
mance. The second approach is fast since it does
not calculate N-best sequences, and unlike the
first approach, will always consider the correct
label sequence as a candidate.
</bodyText>
<sectionHeader confidence="0.998876" genericHeader="method">
4 Experiment
</sectionHeader>
<bodyText confidence="0.999596888888889">
In order to compare the performance of BWSM
with HMM-based Korean word spacing and
structural SVM-based Korean word spacing, we
use Sejong raw corpus (Kang and Kim, 2004) as
train data and ETRI POS tagging corpus as test
data3. Pegasos-struct algorithm from (Lee and
Kim, 2013) was used to train the basic structural
SVM-based model. The optimal value for the
tradeoff variable C of structural SVM was found
after conducting several experiments4.
The rate of word spacing error varies depend-
ing on the corpus. Newspaper articles rarely have
word spacing errors but text messages or Tweets
frequently contain word spacing errors. To re-
flect such variety, we randomly insert spacing
errors into the test set to produce various test sets
with spacing error rate 0%, 10%, 20%, 35%,
50%, 60%, and 70%5.
</bodyText>
<figureCaption confidence="0.89880275">
Figure 2: Word-based F-score of N-best re-
ranking approach.
Figure 2 shows the relation between 𝛼(x-axis)
and word-based F-score6(y-axis) of N-best re-
</figureCaption>
<footnote confidence="0.9443841">
3 The number of words for the training set and test set are 26
million and 290,000 respectively.
4 We experimented with 10, 100, 1000, 10000, 100000 and
1000000, the optimal value being 100000.
5 We altered the input to the system and retained the origi-
nal gold standard’s space unit.
6 Word-based F-score = 2*Precword*Recallword / (Precword +
Recallword),
Precword = (# of correctly spaced words) / (the total number
of words produced by the system),
</footnote>
<page confidence="0.996165">
877
</page>
<bodyText confidence="0.999829545454546">
ranking approach using test sets with different
spacing error rate. When 𝛼 = 0, BWSM be-
comes a normal structural SVM-based model. As
𝛼 increases, F-score also increases for a while
but decreases afterward. And F-score increases
more when using test sets with low error rate. It
is worth noticing that when using the test set
with 0% error rate, as 𝛼 increases, F-score con-
verges to 98%. The reason it does not reach 100%
is that the correct label sequence is sometimes
not included in the N-best sequences.
</bodyText>
<figureCaption confidence="0.99807">
Figure 3: Word-based F-score of modified
Viterbi search.
</figureCaption>
<bodyText confidence="0.999443615384615">
Figure 3 shows the relation between 𝛼(x-axis)
and word-based F-score(y-axis) of modified
Viterbi search approach using test sets with dif-
ferent spacing error rate. The graphs are similar
to Figure 2, but F-score reaches higher values
compared to N-best re-ranking approach. Notice
that, when using the test set with 0% error rate,
F-score becomes 100% as 𝛼 surpasses 3. This is
because, unlike N-best re-ranking approach,
modified Viterbi search approach considers all
possible sequences as candidates.
From Figure 2 and 3, it can be seen that
BWSM, which takes into consideration the spac-
ing information provided by the user, can im-
prove performance significantly. It is also appar-
ent that modified Viterbi search approach outper-
forms N-best re-ranking approach. The optimal
value for 𝛼 varies as test sets with different error
rate are used. It is natural that, for test sets with
low error rate, the optimal value of 𝛼 increases,
thus forcing the model to more utilize the user-
given spacing information. It is difficult to auto-
matically obtain the optimal 𝛼 for an arbitrary
input sentence. Therefore we set 𝛼 to 1, which,
according to Figure 3, is more or less the optimal
value for most of the test sets.
</bodyText>
<footnote confidence="0.2565155">
Recallword = (# of correctly spaced words) / (the total num-
ber of words in the test data)
</footnote>
<table confidence="0.850784125">
Syllable Word
Model based based
precision precision
HMM (Lee et al., 2007) 98.44 90.31
S-SVM (Lee and Kim, 2013) 99.01 92.53
Modified Viterbi (error rate 10%) 99.64 96.81
Modified Viterbi (error rate 20%) 99.55 96.21
Modified Viterbi (error rate 35%) 99.35 95.01
</table>
<tableCaption confidence="0.775041">
Table 1: Precision of BWSM and previous
studies
</tableCaption>
<bodyText confidence="0.999779428571429">
With 𝛼 set to 1, and using modified Viterbi
search algorithm, the performance of BWSM is
shown in Table 1 with other previous studies
(Lee and Kim, 2013; Lee et al., 2007). Table 1
shows that BWSM gives superior performance
than other studies that do not utilize user-given
spacing information.
</bodyText>
<figureCaption confidence="0.998218">
Figure 4: Word-based F-score of modified
Viterbi search on Tweets.
</figureCaption>
<bodyText confidence="0.999822230769231">
We also collected Tweets from Twitter and
tested modified Viterbi algorithm on them. Fig-
ure 4 shows the relation between 𝛼 (x-axis) and
word-based F-score (y-axis). The raw Tweets
showed word-based F-score of approximate 91%,
and the basic structural SVM model (𝛼 = 0)
showed somewhat inferior 88%. Modified
Viterbi algorithm showed the similar behavior as
Figure 3, showing 93.2~93.4% word-based F-
score when 𝛼 was set to 0.5~1. Figure 4 shows
that BWSM is effective not only on text with
randomly inserted spacing errors, but also on
actual data, Tweets.
</bodyText>
<sectionHeader confidence="0.99926" genericHeader="conclusions">
5 Conclusion
</sectionHeader>
<bodyText confidence="0.998733375">
In this paper, we proposed BWSM, a new struc-
tural SVM-based Korean word spacing model
that utilizes user-given spacing information.
BWSM can obtain the most likely sequence of
spacing labels while minimally altering the word
spacing of the input sentence. Experiments on
test sets with various error rate showed that
BWSM significantly improved word-based F-
</bodyText>
<page confidence="0.993671">
878
</page>
<bodyText confidence="0.999957625">
score, from 95.47% to 98.39% in case of the test
set with 10% error rate.
For future work, there are two interesting di-
rections. First is to improve BWSM so that it can
automatically obtain the optimal value of 𝛼 for
an arbitrary sentence. This will require a training
set consisting of text with actual human spacing
errors and its corrected version. Second is to ap-
ply BWSM to other interesting problems such as
named entity recognition (NER). Newspaper ar-
ticles often use certain symbols such as quotation
marks or brackets around the titles of movies,
songs and books. Such symbols can be viewed as
user-given input, which BWSM will try to re-
spect as much as possible while trying to find the
most likely named entities.
</bodyText>
<sectionHeader confidence="0.998931" genericHeader="acknowledgments">
Acknowledgments
</sectionHeader>
<bodyText confidence="0.9984705">
This work was supported by the IT R&amp;D pro-
gram of MSIP/KEIT (10044577, Development of
Knowledge Evolutionary WiseQA Platform
Technology for Human Knowledge Augmented
Services). We would like to thank the anony-
mous reviewers for their comments.
</bodyText>
<sectionHeader confidence="0.999047" genericHeader="references">
References
</sectionHeader>
<reference confidence="0.99987143902439">
Thorsten Joachims, Thomas Finley, and Chun-Nam
John Yu. 2009. Cutting-plane training of structural
SVMs. Machine Learning, Vol. 77, No. 1.
Beom-mo Kang and Hunggyu Kim. 2004. Sejong
Korean Corpora in the making. In Proceedings of
the LREC, 1747-1750.
John Lafferty, Andrew McCallum and Fernando Pe-
reira. 2001. Conditional random fields: Probabilis-
tic models for segmenting and labeling sequence
data. In Proceedings of the ICML, 282-289.
Changki Lee and Myung-Gil Jang. 2010. A Modified
Fixed-threshold SMO for 1-Slack Structural SVM.
ETRI Journal, Vol.32, No.1, 120-128.
Changki Lee and Hyunki Kim. 2012. Automatic Ko-
rean word spacing using structural SVM. In Pro-
ceedings of the KCC, 270-272.
Changki Lee and Hyunki Kim. 2013. Automatic Ko-
rean word spacing using Pegasos algorithm. Infor-
mation Processing and Management, Vol. 49, No.
1, 370-379.
Do-Gil Lee, Hae-Chang Rim and Dongsuk Yook.
2007. Automatic word spacing using probabilistic
models based on character n-grams. Intelligent Sys-
tems IEEE, Vol. 22, No. 1, 28-35.
Seung-Wook Lee, Hae-Chang Rim and So-Young
Park. 2007. A new approach for Korean word spac-
ing incorporating confidence value of user’s input.
In Proceedings of the ALPIT, 92-97.
Kwang-Sup Shim. 2011. Automatic word spacing
based on Conditional Random Fields. Korean
Journal of Cognitive Science, Vol. 22, No. 2, 217-
233.
Shai Shalev-Shwartz, Yoram Singer, and Nathan Sre-
bro. 2004. Pegasos: Primal estimated sub-gradient
solver for SVM. Mathematical Programming, Vol.
127, No. 1.
Ioannis Tsochantaridis, Thomas Hofmann, Thorsten
Joachims and Yasemin Altun. 2004. Support vector
machine learning for interdependent and structured
output spaces. In Proceedings of the ICML, 104-
111.
</reference>
<page confidence="0.99896">
879
</page>
</variant>
</algorithm>
<algorithm name="ParsHed" version="110505">
<variant no="0" confidence="0.659480">
<title confidence="0.998299">Balanced Korean Word Spacing with Structural SVM</title>
<author confidence="0.999374">Edward Choi Hyunki Kim</author>
<affiliation confidence="0.898673">National University, Chuncheon-si, Gangwondo, 200-701,</affiliation>
<address confidence="0.837714">Electronics and Telecommunications Research Institute, Daejeon, 305-350, Korea</address>
<email confidence="0.774196">leeck@kangwon.ac.krmp2893@gmail.comhkk@etri.re.kr</email>
<abstract confidence="0.997518058823529">Most studies on statistical Korean word spacing do not utilize the information provided by the input sentence and assume that it was completely concatenated. This makes the word spacer ignore the correct spaced parts of the input sentence and erroneously alter them. To overcome such limit, this paper proposes a structural SVM-based Korean word spacing method that can utilize the space information of the input sentence. The experiment on sentences with 10% spacing errors showed that our method achieved 96.81% F-score, while the basic structural SVM method only achieved 92.53% F-score. The more the input sentence was correctly spaced, the more accurately our method performed.</abstract>
</variant>
</algorithm>
<algorithm name="ParsCit" version="110505">
<citationList>
<citation valid="true">
<authors>
<author>Thorsten Joachims</author>
<author>Thomas Finley</author>
<author>Chun-Nam John Yu</author>
</authors>
<title>Cutting-plane training of structural SVMs.</title>
<date>2009</date>
<booktitle>Machine Learning,</booktitle>
<volume>77</volume>
<contexts>
<context position="4547" citStr="Joachims et al., 2009" startWordPosition="701" endWordPosition="704">y proposed a method based on Hidden Markov Model (HMM). Shim (2011) also considered the word spacing task as a sequence labeling problem and proposed a method using Conditional Random Field (CRF) (Lafferty et al., 2001), which is a well-known powerful model for sequence labeling tasks. Lee and Kim 875 Proceedings of the 2014 Conference on Empirical Methods in Natural Language Processing (EMNLP), pages 875–879, October 25-29, 2014, Doha, Qatar. c�2014 Association for Computational Linguistics (2013) tried to solve the sequence labeling problem using structural SVM (Tsochantaridis et al., 2004; Joachims et al., 2009; Lee and Jang 2010; Shalev-Shwartz et al., 2011). The studies above (Lee and Kim, 2013; Lee et al., 2007; Shim, 2011), however, do not take advantage of the spacing information provided by the user, and often erroneously alter the correctly spaced part of the sentence. Lee et al. (2007) tries to resolve this issue by combining an HMM model with an additional confidence model constructed from another corpus. Given an input sentence, they first apply the basic HMM model to obtain a candidate sentence. For every different word spacing between the input sentence and the candidate sentence, they c</context>
</contexts>
<marker>Joachims, Finley, Yu, 2009</marker>
<rawString>Thorsten Joachims, Thomas Finley, and Chun-Nam John Yu. 2009. Cutting-plane training of structural SVMs. Machine Learning, Vol. 77, No. 1.</rawString>
</citation>
<citation valid="true">
<authors>
<author>Beom-mo Kang</author>
<author>Hunggyu Kim</author>
</authors>
<title>Sejong Korean Corpora in the making.</title>
<date>2004</date>
<booktitle>In Proceedings of the LREC,</booktitle>
<pages>1747--1750</pages>
<contexts>
<context position="11154" citStr="Kang and Kim, 2004" startWordPosition="1911" endWordPosition="1914">y Viterbi search algorithm. The first approach seems straightforward and easy, but it would take a long time to obtain Nbest sequences of labels. Furthermore, the correct label sequence might not be in those N-best sequences, hence degrading the overall performance. The second approach is fast since it does not calculate N-best sequences, and unlike the first approach, will always consider the correct label sequence as a candidate. 4 Experiment In order to compare the performance of BWSM with HMM-based Korean word spacing and structural SVM-based Korean word spacing, we use Sejong raw corpus (Kang and Kim, 2004) as train data and ETRI POS tagging corpus as test data3. Pegasos-struct algorithm from (Lee and Kim, 2013) was used to train the basic structural SVM-based model. The optimal value for the tradeoff variable C of structural SVM was found after conducting several experiments4. The rate of word spacing error varies depending on the corpus. Newspaper articles rarely have word spacing errors but text messages or Tweets frequently contain word spacing errors. To reflect such variety, we randomly insert spacing errors into the test set to produce various test sets with spacing error rate 0%, 10%, 20</context>
</contexts>
<marker>Kang, Kim, 2004</marker>
<rawString>Beom-mo Kang and Hunggyu Kim. 2004. Sejong Korean Corpora in the making. In Proceedings of the LREC, 1747-1750.</rawString>
</citation>
<citation valid="true">
<authors>
<author>John Lafferty</author>
<author>Andrew McCallum</author>
<author>Fernando Pereira</author>
</authors>
<title>Conditional random fields: Probabilistic models for segmenting and labeling sequence data.</title>
<date>2001</date>
<booktitle>In Proceedings of the ICML,</booktitle>
<pages>282--289</pages>
<contexts>
<context position="4145" citStr="Lafferty et al., 2001" startWordPosition="642" endWordPosition="645">s, therefore, mostly focus on the statistical approach. Most statistical approaches use well-spaced raw corpus as training data (e.g. newspaper articles) assuming that they are perfectly spaced. This is to avoid the expensive job of constructing new training data. Lee et al. (2007) treated the word spacing task as a sequence labeling problem on the input sentence which is a sequence of syllables. They proposed a method based on Hidden Markov Model (HMM). Shim (2011) also considered the word spacing task as a sequence labeling problem and proposed a method using Conditional Random Field (CRF) (Lafferty et al., 2001), which is a well-known powerful model for sequence labeling tasks. Lee and Kim 875 Proceedings of the 2014 Conference on Empirical Methods in Natural Language Processing (EMNLP), pages 875–879, October 25-29, 2014, Doha, Qatar. c�2014 Association for Computational Linguistics (2013) tried to solve the sequence labeling problem using structural SVM (Tsochantaridis et al., 2004; Joachims et al., 2009; Lee and Jang 2010; Shalev-Shwartz et al., 2011). The studies above (Lee and Kim, 2013; Lee et al., 2007; Shim, 2011), however, do not take advantage of the spacing information provided by the user</context>
</contexts>
<marker>Lafferty, McCallum, Pereira, 2001</marker>
<rawString>John Lafferty, Andrew McCallum and Fernando Pereira. 2001. Conditional random fields: Probabilistic models for segmenting and labeling sequence data. In Proceedings of the ICML, 282-289.</rawString>
</citation>
<citation valid="true">
<authors>
<author>Changki Lee</author>
<author>Myung-Gil Jang</author>
</authors>
<title>A Modified Fixed-threshold SMO for 1-Slack Structural SVM.</title>
<date>2010</date>
<journal>ETRI Journal,</journal>
<volume>32</volume>
<pages>120--128</pages>
<contexts>
<context position="4566" citStr="Lee and Jang 2010" startWordPosition="705" endWordPosition="708">ed on Hidden Markov Model (HMM). Shim (2011) also considered the word spacing task as a sequence labeling problem and proposed a method using Conditional Random Field (CRF) (Lafferty et al., 2001), which is a well-known powerful model for sequence labeling tasks. Lee and Kim 875 Proceedings of the 2014 Conference on Empirical Methods in Natural Language Processing (EMNLP), pages 875–879, October 25-29, 2014, Doha, Qatar. c�2014 Association for Computational Linguistics (2013) tried to solve the sequence labeling problem using structural SVM (Tsochantaridis et al., 2004; Joachims et al., 2009; Lee and Jang 2010; Shalev-Shwartz et al., 2011). The studies above (Lee and Kim, 2013; Lee et al., 2007; Shim, 2011), however, do not take advantage of the spacing information provided by the user, and often erroneously alter the correctly spaced part of the sentence. Lee et al. (2007) tries to resolve this issue by combining an HMM model with an additional confidence model constructed from another corpus. Given an input sentence, they first apply the basic HMM model to obtain a candidate sentence. For every different word spacing between the input sentence and the candidate sentence, they calculate and compar</context>
</contexts>
<marker>Lee, Jang, 2010</marker>
<rawString>Changki Lee and Myung-Gil Jang. 2010. A Modified Fixed-threshold SMO for 1-Slack Structural SVM. ETRI Journal, Vol.32, No.1, 120-128.</rawString>
</citation>
<citation valid="true">
<authors>
<author>Changki Lee</author>
<author>Hyunki Kim</author>
</authors>
<title>Automatic Korean word spacing using structural SVM.</title>
<date>2012</date>
<booktitle>In Proceedings of the KCC,</booktitle>
<pages>270--272</pages>
<contexts>
<context position="1239" citStr="Lee and Kim, 2012" startWordPosition="176" endWordPosition="179">M-based Korean word spacing method that can utilize the space information of the input sentence. The experiment on sentences with 10% spacing errors showed that our method achieved 96.81% F-score, while the basic structural SVM method only achieved 92.53% F-score. The more the input sentence was correctly spaced, the more accurately our method performed. 1 Introduction Automatic word spacing is a task to decide boundaries between words, which is frequently used for correcting spacing errors of text messages, Tweets, or Internet comments before using them in information retrieval applications (Lee and Kim, 2012). It is also often used in postprocessing optical character recognition (OCR) or voice recognition (Lee et al., 2007). Except for some Asian languages such as Chinese, Japanese and Thai, most languages have explicit word spacing that improves the readability of the text and helps readers better understand the meaning of it. Korean especially has a tricky word spacing system and users often make mistakes, which makes automatic word spacing an interesting and essential task. In order to easily acquire the training data, most studies on statistical Korean word spacing assume that well-spaced raw </context>
</contexts>
<marker>Lee, Kim, 2012</marker>
<rawString>Changki Lee and Hyunki Kim. 2012. Automatic Korean word spacing using structural SVM. In Proceedings of the KCC, 270-272.</rawString>
</citation>
<citation valid="true">
<authors>
<author>Changki Lee</author>
<author>Hyunki Kim</author>
</authors>
<title>Automatic Korean word spacing using Pegasos algorithm.</title>
<date>2013</date>
<journal>Information Processing and Management,</journal>
<volume>49</volume>
<pages>370--379</pages>
<contexts>
<context position="1951" citStr="Lee and Kim, 2013" startWordPosition="292" endWordPosition="295">ion (Lee et al., 2007). Except for some Asian languages such as Chinese, Japanese and Thai, most languages have explicit word spacing that improves the readability of the text and helps readers better understand the meaning of it. Korean especially has a tricky word spacing system and users often make mistakes, which makes automatic word spacing an interesting and essential task. In order to easily acquire the training data, most studies on statistical Korean word spacing assume that well-spaced raw text (e.g. newspaper articles) is perfectly spaced and use it for training (Lee and Kim, 2012; Lee and Kim, 2013; Lee et al., 2007; Shim, 2011). This approach, however, cannot observe incorrect spacing since the assumption makes the training data devoid of negative example. Consequently, word spacers cannot use the spacing information given by the user, and erroneously alter the correctly spaced parts of the sentence. To utilize the user-given spacing information, a corpus of input sentences and their correctly spaced version is necessary. Constructing such corpus, however, requires much time and resource. In this paper, to resolve such issue, we propose a structural SVM-based Korean word spacing model </context>
<context position="4634" citStr="Lee and Kim, 2013" startWordPosition="716" endWordPosition="719">d spacing task as a sequence labeling problem and proposed a method using Conditional Random Field (CRF) (Lafferty et al., 2001), which is a well-known powerful model for sequence labeling tasks. Lee and Kim 875 Proceedings of the 2014 Conference on Empirical Methods in Natural Language Processing (EMNLP), pages 875–879, October 25-29, 2014, Doha, Qatar. c�2014 Association for Computational Linguistics (2013) tried to solve the sequence labeling problem using structural SVM (Tsochantaridis et al., 2004; Joachims et al., 2009; Lee and Jang 2010; Shalev-Shwartz et al., 2011). The studies above (Lee and Kim, 2013; Lee et al., 2007; Shim, 2011), however, do not take advantage of the spacing information provided by the user, and often erroneously alter the correctly spaced part of the sentence. Lee et al. (2007) tries to resolve this issue by combining an HMM model with an additional confidence model constructed from another corpus. Given an input sentence, they first apply the basic HMM model to obtain a candidate sentence. For every different word spacing between the input sentence and the candidate sentence, they calculate and compare the confidence using the confidence model, and whichever gets the </context>
<context position="7326" citStr="Lee and Kim, 2013" startWordPosition="1192" endWordPosition="1195">g principles: 1. The model must obtain the most likely sequence of labels(y*), while minimally altering the user-given sequence of labels (𝐲input). 2. We assume that it costs α per syllable to change the spacing of the original sentence, in order to keep the original spacing information as much as possible. Mathematically formulating the above principles would give us the following equation: 𝐲∗ = argmax score 𝐱, 𝐲 − 𝛼 ∙ 𝐿 𝐲input, 𝐲 (1) In Equation 1, score 𝐱, 𝐲 calculates how compatible the sequence of label y is with the input sentence x. It is calculated by a basic word spacing model as in (Lee and Kim, 2013). 𝐿 𝐲input, 𝐲 counts the number of different labels between the user-given sequence of labels 𝐲input and an arbitrary sequence of labels 𝐲. 𝐲∗ of Equation 1 can be obtained by setting the gradient of score 𝐱, 𝐲 − 𝛼 ∙ 𝐿 𝐲input, 𝐲 to 0, which is equivalent to the following equation: ∇score 𝐱, 𝐲∗ = 𝛼 ∙ ∇𝐿 𝐲inrut• 𝐲∗ (2) In order to view the proposed model in a different perspective, we consider BWSM in terms of Karush-Kuhn-Tucker (KKT) condition. KKT condition is a technique for solving optimization problems with inequality constraints. It is a generalized version of Lagrange multipliers, which i</context>
<context position="9195" citStr="Lee and Kim (2013)" startWordPosition="1537" endWordPosition="1540">ion 4 to zero, namely ∇Λ 𝐱, 𝐲, 𝛼 = 0, we get the following necessary conditions: Stationarity: ∇score 𝐱, 𝐲∗ = 𝛼∗ ∇𝐿 𝐲input, 𝐲∗ Primal feasibility: 𝐿 𝐲i, put, 𝐲∗ ≤ 𝑏 Dual feasibility: 𝛼∗ ≥ 0 Complementary slackness: 𝛼∗ 𝐿 𝐲input, 𝐲∗ − 𝑏 = 0 (5) Comparing Equation 1 with Equation 4 reveals that they are the same except the constant b. And 𝐲∗ which satisfies the conditions of Equation 5, and hence the solution to Equation 4, is also the same as 𝐲∗ which satisfies Equation 2, and hence the solution to Equation 1. For the basic word spacing model, we use margin rescaled version of structural SVM as Lee and Kim (2013). The objective function of structural SVM is as follows: min𝒘,, 2 ∥ 𝐰 ∥2 + n n i, 𝑠. 𝑡. ∀ 𝑖, 𝜉i 0∀𝑖, ∀𝐲 ∈ Y\𝐲i: 𝐰T𝛿Ψ 𝐱i, 𝐲 ≥ 𝐿 𝐲i, 𝐲 − 𝜉i where 𝛿Ψ 𝐱i, 𝐲 = Ψ 𝐱i, 𝐲i − Ψ 𝐱i, 𝐲 (6) In Equation 6, 𝐱i, 𝐲i represents the i-th sequence of syllables and its correct spacing labels. 𝐿 𝐲i, 𝐲 is a loss function that counts the number of different labels between the correct labels 𝐲i and the predicted sequence of labels 𝐲. Ψ 𝐱, 𝐲 is a typical feature vector function. The features used for the basic word spacing model are the same features used in (Lee and Kim, 2013). Since structural SVM was used for the </context>
<context position="11261" citStr="Lee and Kim, 2013" startWordPosition="1929" endWordPosition="1932"> to obtain Nbest sequences of labels. Furthermore, the correct label sequence might not be in those N-best sequences, hence degrading the overall performance. The second approach is fast since it does not calculate N-best sequences, and unlike the first approach, will always consider the correct label sequence as a candidate. 4 Experiment In order to compare the performance of BWSM with HMM-based Korean word spacing and structural SVM-based Korean word spacing, we use Sejong raw corpus (Kang and Kim, 2004) as train data and ETRI POS tagging corpus as test data3. Pegasos-struct algorithm from (Lee and Kim, 2013) was used to train the basic structural SVM-based model. The optimal value for the tradeoff variable C of structural SVM was found after conducting several experiments4. The rate of word spacing error varies depending on the corpus. Newspaper articles rarely have word spacing errors but text messages or Tweets frequently contain word spacing errors. To reflect such variety, we randomly insert spacing errors into the test set to produce various test sets with spacing error rate 0%, 10%, 20%, 35%, 50%, 60%, and 70%5. Figure 2: Word-based F-score of N-best reranking approach. Figure 2 shows the r</context>
<context position="14363" citStr="Lee and Kim, 2013" startWordPosition="2451" endWordPosition="2454"> as test sets with different error rate are used. It is natural that, for test sets with low error rate, the optimal value of 𝛼 increases, thus forcing the model to more utilize the usergiven spacing information. It is difficult to automatically obtain the optimal 𝛼 for an arbitrary input sentence. Therefore we set 𝛼 to 1, which, according to Figure 3, is more or less the optimal value for most of the test sets. Recallword = (# of correctly spaced words) / (the total number of words in the test data) Syllable Word Model based based precision precision HMM (Lee et al., 2007) 98.44 90.31 S-SVM (Lee and Kim, 2013) 99.01 92.53 Modified Viterbi (error rate 10%) 99.64 96.81 Modified Viterbi (error rate 20%) 99.55 96.21 Modified Viterbi (error rate 35%) 99.35 95.01 Table 1: Precision of BWSM and previous studies With 𝛼 set to 1, and using modified Viterbi search algorithm, the performance of BWSM is shown in Table 1 with other previous studies (Lee and Kim, 2013; Lee et al., 2007). Table 1 shows that BWSM gives superior performance than other studies that do not utilize user-given spacing information. Figure 4: Word-based F-score of modified Viterbi search on Tweets. We also collected Tweets from Twitter a</context>
</contexts>
<marker>Lee, Kim, 2013</marker>
<rawString>Changki Lee and Hyunki Kim. 2013. Automatic Korean word spacing using Pegasos algorithm. Information Processing and Management, Vol. 49, No. 1, 370-379.</rawString>
</citation>
<citation valid="true">
<authors>
<author>Do-Gil Lee</author>
</authors>
<title>Hae-Chang Rim and Dongsuk Yook.</title>
<date>2007</date>
<booktitle>Intelligent Systems IEEE,</booktitle>
<volume>22</volume>
<pages>28--35</pages>
<marker>Lee, 2007</marker>
<rawString>Do-Gil Lee, Hae-Chang Rim and Dongsuk Yook. 2007. Automatic word spacing using probabilistic models based on character n-grams. Intelligent Systems IEEE, Vol. 22, No. 1, 28-35.</rawString>
</citation>
<citation valid="true">
<authors>
<author>Seung-Wook Lee</author>
<author>Hae-Chang Rim</author>
<author>So-Young Park</author>
</authors>
<title>A new approach for Korean word spacing incorporating confidence value of user’s input.</title>
<date>2007</date>
<booktitle>In Proceedings of the ALPIT,</booktitle>
<pages>92--97</pages>
<contexts>
<context position="1356" citStr="Lee et al., 2007" startWordPosition="195" endWordPosition="198">tences with 10% spacing errors showed that our method achieved 96.81% F-score, while the basic structural SVM method only achieved 92.53% F-score. The more the input sentence was correctly spaced, the more accurately our method performed. 1 Introduction Automatic word spacing is a task to decide boundaries between words, which is frequently used for correcting spacing errors of text messages, Tweets, or Internet comments before using them in information retrieval applications (Lee and Kim, 2012). It is also often used in postprocessing optical character recognition (OCR) or voice recognition (Lee et al., 2007). Except for some Asian languages such as Chinese, Japanese and Thai, most languages have explicit word spacing that improves the readability of the text and helps readers better understand the meaning of it. Korean especially has a tricky word spacing system and users often make mistakes, which makes automatic word spacing an interesting and essential task. In order to easily acquire the training data, most studies on statistical Korean word spacing assume that well-spaced raw text (e.g. newspaper articles) is perfectly spaced and use it for training (Lee and Kim, 2012; Lee and Kim, 2013; Lee</context>
<context position="3805" citStr="Lee et al. (2007)" startWordPosition="584" endWordPosition="587">his study is given. 2 Related Work There are two common approaches to Korean word spacing: rule-based approach and statistical approach. In rule-based approach, it is not easy to construct rules and maintain them. Furthermore, it requires morphological analysis to apply rule-based approach, which slows down the process. Recent studies, therefore, mostly focus on the statistical approach. Most statistical approaches use well-spaced raw corpus as training data (e.g. newspaper articles) assuming that they are perfectly spaced. This is to avoid the expensive job of constructing new training data. Lee et al. (2007) treated the word spacing task as a sequence labeling problem on the input sentence which is a sequence of syllables. They proposed a method based on Hidden Markov Model (HMM). Shim (2011) also considered the word spacing task as a sequence labeling problem and proposed a method using Conditional Random Field (CRF) (Lafferty et al., 2001), which is a well-known powerful model for sequence labeling tasks. Lee and Kim 875 Proceedings of the 2014 Conference on Empirical Methods in Natural Language Processing (EMNLP), pages 875–879, October 25-29, 2014, Doha, Qatar. c�2014 Association for Computat</context>
<context position="5362" citStr="Lee et al., 2007" startWordPosition="838" endWordPosition="841">, and often erroneously alter the correctly spaced part of the sentence. Lee et al. (2007) tries to resolve this issue by combining an HMM model with an additional confidence model constructed from another corpus. Given an input sentence, they first apply the basic HMM model to obtain a candidate sentence. For every different word spacing between the input sentence and the candidate sentence, they calculate and compare the confidence using the confidence model, and whichever gets the higher confidence is used. The spacing accuracy was improved from 97.52% to 97.64%1. This study is similar to (Lee et al., 2007) in that it utilizes the spacing information given by the user. But unlike (Lee et al., 2007), BWSM uses structural SVM as the basic model and do not require an additional confidence model. Furthermore, while Lee et al. (2007) compares the spacing confidence for each syllable to obtain the final outcome, BWSM considers the whole sentence when altering its spacing, enabling it to achieve higher improvement on performance (from 92.53% F-score to 96.81% F-score). 3 Balanced Word Spacing Model Like previous studies, the proposed model treats the Korean word spacing task as a sequence labeling prob</context>
<context position="14325" citStr="Lee et al., 2007" startWordPosition="2444" endWordPosition="2447">roach. The optimal value for 𝛼 varies as test sets with different error rate are used. It is natural that, for test sets with low error rate, the optimal value of 𝛼 increases, thus forcing the model to more utilize the usergiven spacing information. It is difficult to automatically obtain the optimal 𝛼 for an arbitrary input sentence. Therefore we set 𝛼 to 1, which, according to Figure 3, is more or less the optimal value for most of the test sets. Recallword = (# of correctly spaced words) / (the total number of words in the test data) Syllable Word Model based based precision precision HMM (Lee et al., 2007) 98.44 90.31 S-SVM (Lee and Kim, 2013) 99.01 92.53 Modified Viterbi (error rate 10%) 99.64 96.81 Modified Viterbi (error rate 20%) 99.55 96.21 Modified Viterbi (error rate 35%) 99.35 95.01 Table 1: Precision of BWSM and previous studies With 𝛼 set to 1, and using modified Viterbi search algorithm, the performance of BWSM is shown in Table 1 with other previous studies (Lee and Kim, 2013; Lee et al., 2007). Table 1 shows that BWSM gives superior performance than other studies that do not utilize user-given spacing information. Figure 4: Word-based F-score of modified Viterbi search on Tweets. W</context>
</contexts>
<marker>Lee, Rim, Park, 2007</marker>
<rawString>Seung-Wook Lee, Hae-Chang Rim and So-Young Park. 2007. A new approach for Korean word spacing incorporating confidence value of user’s input. In Proceedings of the ALPIT, 92-97.</rawString>
</citation>
<citation valid="true">
<authors>
<author>Kwang-Sup Shim</author>
</authors>
<title>Automatic word spacing based on Conditional Random Fields.</title>
<date>2011</date>
<journal>Korean Journal of Cognitive Science,</journal>
<volume>22</volume>
<pages>217--233</pages>
<contexts>
<context position="1982" citStr="Shim, 2011" startWordPosition="300" endWordPosition="301">me Asian languages such as Chinese, Japanese and Thai, most languages have explicit word spacing that improves the readability of the text and helps readers better understand the meaning of it. Korean especially has a tricky word spacing system and users often make mistakes, which makes automatic word spacing an interesting and essential task. In order to easily acquire the training data, most studies on statistical Korean word spacing assume that well-spaced raw text (e.g. newspaper articles) is perfectly spaced and use it for training (Lee and Kim, 2012; Lee and Kim, 2013; Lee et al., 2007; Shim, 2011). This approach, however, cannot observe incorrect spacing since the assumption makes the training data devoid of negative example. Consequently, word spacers cannot use the spacing information given by the user, and erroneously alter the correctly spaced parts of the sentence. To utilize the user-given spacing information, a corpus of input sentences and their correctly spaced version is necessary. Constructing such corpus, however, requires much time and resource. In this paper, to resolve such issue, we propose a structural SVM-based Korean word spacing model that can utilize the word spaci</context>
<context position="3993" citStr="Shim (2011)" startWordPosition="619" endWordPosition="620"> and maintain them. Furthermore, it requires morphological analysis to apply rule-based approach, which slows down the process. Recent studies, therefore, mostly focus on the statistical approach. Most statistical approaches use well-spaced raw corpus as training data (e.g. newspaper articles) assuming that they are perfectly spaced. This is to avoid the expensive job of constructing new training data. Lee et al. (2007) treated the word spacing task as a sequence labeling problem on the input sentence which is a sequence of syllables. They proposed a method based on Hidden Markov Model (HMM). Shim (2011) also considered the word spacing task as a sequence labeling problem and proposed a method using Conditional Random Field (CRF) (Lafferty et al., 2001), which is a well-known powerful model for sequence labeling tasks. Lee and Kim 875 Proceedings of the 2014 Conference on Empirical Methods in Natural Language Processing (EMNLP), pages 875–879, October 25-29, 2014, Doha, Qatar. c�2014 Association for Computational Linguistics (2013) tried to solve the sequence labeling problem using structural SVM (Tsochantaridis et al., 2004; Joachims et al., 2009; Lee and Jang 2010; Shalev-Shwartz et al., 20</context>
</contexts>
<marker>Shim, 2011</marker>
<rawString>Kwang-Sup Shim. 2011. Automatic word spacing based on Conditional Random Fields. Korean Journal of Cognitive Science, Vol. 22, No. 2, 217-233.</rawString>
</citation>
<citation valid="true">
<authors>
<author>Shai Shalev-Shwartz</author>
<author>Yoram Singer</author>
<author>Nathan Srebro</author>
</authors>
<title>Pegasos: Primal estimated sub-gradient solver for SVM.</title>
<date>2004</date>
<journal>Mathematical Programming,</journal>
<volume>127</volume>
<marker>Shalev-Shwartz, Singer, Srebro, 2004</marker>
<rawString>Shai Shalev-Shwartz, Yoram Singer, and Nathan Srebro. 2004. Pegasos: Primal estimated sub-gradient solver for SVM. Mathematical Programming, Vol. 127, No. 1.</rawString>
</citation>
<citation valid="true">
<authors>
<author>Ioannis Tsochantaridis</author>
<author>Thomas Hofmann</author>
<author>Thorsten Joachims</author>
<author>Yasemin Altun</author>
</authors>
<title>Support vector machine learning for interdependent and structured output spaces.</title>
<date>2004</date>
<booktitle>In Proceedings of the ICML,</booktitle>
<pages>104--111</pages>
<contexts>
<context position="4524" citStr="Tsochantaridis et al., 2004" startWordPosition="697" endWordPosition="700"> a sequence of syllables. They proposed a method based on Hidden Markov Model (HMM). Shim (2011) also considered the word spacing task as a sequence labeling problem and proposed a method using Conditional Random Field (CRF) (Lafferty et al., 2001), which is a well-known powerful model for sequence labeling tasks. Lee and Kim 875 Proceedings of the 2014 Conference on Empirical Methods in Natural Language Processing (EMNLP), pages 875–879, October 25-29, 2014, Doha, Qatar. c�2014 Association for Computational Linguistics (2013) tried to solve the sequence labeling problem using structural SVM (Tsochantaridis et al., 2004; Joachims et al., 2009; Lee and Jang 2010; Shalev-Shwartz et al., 2011). The studies above (Lee and Kim, 2013; Lee et al., 2007; Shim, 2011), however, do not take advantage of the spacing information provided by the user, and often erroneously alter the correctly spaced part of the sentence. Lee et al. (2007) tries to resolve this issue by combining an HMM model with an additional confidence model constructed from another corpus. Given an input sentence, they first apply the basic HMM model to obtain a candidate sentence. For every different word spacing between the input sentence and the can</context>
</contexts>
<marker>Tsochantaridis, Hofmann, Joachims, Altun, 2004</marker>
<rawString>Ioannis Tsochantaridis, Thomas Hofmann, Thorsten Joachims and Yasemin Altun. 2004. Support vector machine learning for interdependent and structured output spaces. In Proceedings of the ICML, 104-111.</rawString>
</citation>
</citationList>
</algorithm>
</algorithms>
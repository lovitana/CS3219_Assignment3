<?xml version="1.0" encoding="UTF-8"?>
<algorithms version="110505">
<algorithm name="SectLabel" version="110505">
<variant no="0" confidence="0.000022">
<title confidence="0.995435">
Translation Modeling with Bidirectional Recurrent Neural Networks
</title>
<author confidence="0.999819">
Martin Sundermeyer1, Tamer Alkhouli1, Joern Wuebker1, and Hermann Ney1,2
</author>
<affiliation confidence="0.929398">
1Human Language Technology and Pattern Recognition Group
RWTH Aachen University, Aachen, Germany
2Spoken Language Processing Group
</affiliation>
<address confidence="0.58217">
Univ. Paris-Sud, France and LIMSI/CNRS, Orsay, France
</address>
<email confidence="0.996792">
{surname}@cs.rwth-aachen.de
</email>
<sectionHeader confidence="0.997357" genericHeader="abstract">
Abstract
</sectionHeader>
<bodyText confidence="0.99978975">
This work presents two different trans-
lation models using recurrent neural net-
works. The first one is a word-based ap-
proach using word alignments. Second,
we present phrase-based translation mod-
els that are more consistent with phrase-
based decoding. Moreover, we introduce
bidirectional recurrent neural models to
the problem of machine translation, allow-
ing us to use the full source sentence in our
models, which is also of theoretical inter-
est. We demonstrate that our translation
models are capable of improving strong
baselines already including recurrent neu-
ral language models on three tasks:
IWSLT 2013 German—*English, BOLT
Arabic—*English and Chinese—*English.
We obtain gains up to 1.6% BLEU
and 1.7% TER by rescoring 1000-best
lists.
</bodyText>
<sectionHeader confidence="0.999472" genericHeader="introduction">
1 Introduction
</sectionHeader>
<bodyText confidence="0.999961788461538">
Neural network models have recently experienced
unprecedented attention in research on statistical
machine translation (SMT). Several groups have
reported strong improvements over state-of-the-art
baselines using feedforward neural network-based
language models (Schwenk et al., 2006; Vaswani
et al., 2013), as well as translation models (Le et
al., 2012; Schwenk, 2012; Devlin et al., 2014).
Different from the feedforward design, recurrent
neural networks (RNNs) have the advantage of be-
ing able to take into account an unbounded his-
tory of previous observations. In theory, this en-
ables them to model long-distance dependencies
of arbitrary length. However, while previous work
on translation modeling with recurrent neural net-
works shows its effectiveness on standard base-
lines, so far no notable gains have been presented
on top of recurrent language models (Auli et al.,
2013; Kalchbrenner and Blunsom, 2013; Hu et al.,
2014).
In this work, we present two novel approaches
to recurrent neural translation modeling: word-
based and phrase-based. The word-based ap-
proach assumes one-to-one aligned source and
target sentences. We evaluate different ways of
resolving alignment ambiguities to obtain such
alignments. The phrase-based RNN approach is
more closely tied to the underlying translation
paradigm. It models actual phrasal translation
probabilities while avoiding sparsity issues by us-
ing single words as input and output units. Fur-
thermore, in addition to the unidirectional formu-
lation, we are the first to propose a bidirectional
architecture which can take the full source sen-
tence into account for all predictions. Our ex-
periments show that these models can improve
state-of-the-art baselines containing a recurrent
language model on three tasks. For our compet-
itive IWSLT 2013 German—*English system, we
observe gains of up to 1.6% BLEU and 1.7% TER.
Improvements are also demonstrated on top of our
evaluation systems for BOLT Arabic—*English
and Chinese—*English, which also include recur-
rent neural language models.
The rest of this paper is structured as follows. In
Section 2 we review related work and in Section 3
an overview of long short-term memory (LSTM)
neural networks, a special type of recurrent neural
networks we make use of in this work, is given.
Section 4 describes our novel translation models.
Finally, experiments are presented in Section 5
and we conclude with Section 6.
</bodyText>
<page confidence="0.990424">
14
</page>
<note confidence="0.9385255">
Proceedings of the 2014 Conference on Empirical Methods in Natural Language Processing (EMNLP), pages 14–25,
October 25-29, 2014, Doha, Qatar. c�2014 Association for Computational Linguistics
</note>
<sectionHeader confidence="0.99917" genericHeader="related work">
2 Related Work
</sectionHeader>
<bodyText confidence="0.999935253521127">
In this Section we contrast previous work to ours,
where we design RNNs to model bilingual depen-
dencies, which are applied to rerank n-best lists
after decoding.
To the best of our knowledge, the earliest at-
tempts to apply neural networks in machine trans-
lation (MT) are presented in (Casta˜no et al.,
1997; Casta˜no and Casacuberta, 1997; Casta˜no
and Casacuberta, 1999), where they were used for
example-based MT.
Recently, Le et al. (2012) presented translation
models using an output layer with classes and
a shortlist for rescoring using feedforward net-
works. They compare between word-factored and
tuple-factored n-gram models, obtaining their best
results using the word-factored approach, which is
less amenable to data sparsity issues. Both of our
word-based and phrase-based models eventually
work on the word level. Kalchbrenner and Blun-
som (2013) use recurrent neural networks with
full source sentence representations. The continu-
ous representations are obtained by applying a se-
quence of convolutions, and the result is fed into
the hidden layer of a recurrent language model.
Rescoring results indicate no improvements over
the state of the art. Auli et al. (2013) also in-
clude source sentence representations built either
using Latent Semantic Analysis or by concatenat-
ing word embeddings. This approach produced
no notable gain over systems using a recurrent
language model. On the other hand, our pro-
posed bidirectional models include the full source
sentence relying on recurrency, yielding improve-
ments over competitive baselines already includ-
ing a recurrent language model.
RNNs were also used with minimum translation
units (Hu et al., 2014), which are phrase pairs un-
dergoing certain constraints. At the input layer,
each of the source and target phrases are mod-
eled as a bag of words, while the output phrase
is predicted word-by-word assuming conditional
independence. The approach seeks to alleviate
data sparsity problems that would arise if phrases
were to be uniquely distinguished. Our proposed
phrase-based models maintain word order within
phrases, but the phrases are processed in a word-
pair manner, while the phrase boundaries remain
implicitly encoded in the way the words are pre-
sented to the network. Schwenk (2012) proposed
a feedforward network that predicts phrases of a
fixed maximum length, such that all phrase words
are predicted at once. The prediction is condi-
tioned on the source phrase. Since our phrase-
based model predicts one word at a time, it does
not assume any phrase length. Moreover, our
model’s predictions go beyond phrase boundaries
and cover unbounded history and future contexts.
Using neural networks during decoding re-
quires tackling the costly output normalization
step. Vaswani et al. (2013) avoid this step by
training feedforward neural language models us-
ing noise contrastive estimation, while Devlin et
al. (2014) augment the training objective function
to produce approximately normalized scores di-
rectly. The latter work makes use of translation
and joint models, and pre-computes the first hid-
den layer beforehand, resulting in large speedups.
They report major improvements over strong base-
lines. The speedups achieved by both works al-
lowed to integrate feedforward neural networks
into the decoder.
</bodyText>
<sectionHeader confidence="0.997708" genericHeader="method">
3 LSTM Recurrent Neural Networks
</sectionHeader>
<bodyText confidence="0.999942821428571">
Our work is based on recurrent neural networks.
In related fields like e. g. language modeling, this
type of neural network has been shown to perform
considerably better than standard feedforward ar-
chitectures (Mikolov et al., 2011; Arisoy et al.,
2012; Sundermeyer et al., 2013; Liu et al., 2014).
Most commonly, recurrent neural networks are
trained with stochastic gradient descent (SGD),
where the gradient of the training criterion is com-
puted with the backpropagation through time al-
gorithm (Rumelhart et al., 1986; Werbos, 1990;
Williams and Zipser, 1995). However, the combi-
nation of RNN networks with conventional back-
propagation training leads to conceptual difficul-
ties which are known as the vanishing (or explod-
ing) gradient problem, described e. g. in (Bengio
et al., 1994). To remedy this problem, in (Hochre-
iter and Schmidhuber, 1997) it was suggested to
modify the architecture of a standard RNN in such
a way that vanishing and exploding gradients are
avoided during backpropagation. In particular, no
modification of the training algorithm is necessary.
The resulting architecture is referred to as long
short-term memory (LSTM) neural network.
Bidirectional recurrent neural networks
(BRNNs) were first proposed in (Schuster and
Paliwal, 1997) and applied to speech recognition
tasks. They have been since applied to different
</bodyText>
<page confidence="0.962942">
15
</page>
<figure confidence="0.999637181818182">
.
incredibly
Eunaligned
this
know
,
example
for
,
Surfers
(b) One-to-one alignment
</figure>
<figureCaption confidence="0.9838035">
Figure 1: Example sentence from the German→English IWSLT data. The one-to-one alignment is
created by introducing caligned and cunaligned tokens.
</figureCaption>
<bodyText confidence="0.999690833333334">
tasks like parsing (Henderson, 2004) and spoken
language understanding (Mesnil et al., 2013).
Bidirectional long short-term memory (BLSTM)
networks are BRNNs using LSTM hidden layers
(Graves and Schmidhuber, 2005). This work
introduces BLSTMs to the problem of machine
translation, allowing powerful models that employ
unlimited history and future information to make
predictions.
While the proposed models do not make any as-
sumptions about the type of RNN used, all of our
experiments make use of recurrent LSTM neural
networks, where we include later LSTM exten-
sions proposed in (Gers et al., 2000; Gers et al.,
2003). The cross-entropy error criterion is used
for training. Further details on LSTM neural net-
works can be found in (Graves and Schmidhuber,
2005; Sundermeyer et al., 2012).
</bodyText>
<sectionHeader confidence="0.993105" genericHeader="method">
4 Translation Modeling with RNNs
</sectionHeader>
<bodyText confidence="0.9999195">
In the following we describe our word- and
phrase-based translation models in detail. We also
show how bidirectional RNNs can enable such
models to include full source information.
</bodyText>
<subsectionHeader confidence="0.997667">
4.1 Resolving Alignment Ambiguities
</subsectionHeader>
<bodyText confidence="0.999683625">
Our word-based recurrent models are only de-
fined for one-to-one-aligned source-target sen-
tence pairs. In this work, we always evaluate the
model in the order of the target sentence. How-
ever, we experiment with several different ways
to resolve ambiguities due to unaligned or mul-
tiply aligned words. To that end, we introduce
two additional tokens, Ealigned and Eunaligned. Un-
</bodyText>
<table confidence="0.9976665">
dev test
BLEU TER BLEU TER
33.5 45.8 30.9 48.4
34.2 45.3 31.8 47.7
34.4 44.8 31.7 47.4
34.5 45.0 31.9 47.5
34.5 44.6 31.9 47.0
34.6 44.5 32.0 47.1
</table>
<tableCaption confidence="0.998801">
Table 1: Comparison of including different sets
</tableCaption>
<bodyText confidence="0.944429761904762">
of c tokens into the one-to-one alignment on the
IWSLT 2013 German→English task using the uni-
directional RNN translation model.
aligned words are either removed or aligned to an
extra Eunaligned token on the opposite side. If an
Eunaligned is introduced on the target side, its posi-
tion is determined by the aligned source word that
is closest to the unaligned source word in question,
preferring left to right. To resolve one-to-many
alignments, we use an IBM-1 translation table to
decide for one of the alignment connections to be
kept. The remaining words are also either deleted
or aligned to additionally introduced caligned to-
kens on the opposite side. Fig. 1 shows an ex-
ample sentence from the IWSLT data, where all c
tokens are introduced.
In a short experiment, we evaluated 5 differ-
ent setups with our unidirectional RNN translation
model (cf. next Section): without any c tokens,
without cunaligned, source identity, target identity
and using all c tokens. Source identity means we
</bodyText>
<figure confidence="0.978345588235294">
baseline
w/o E
w/o Eunaligned
source identity
target identity
all c
16
.
incredibly
this
know
,
example
for
,
Surfers
(a) Original
</figure>
<bodyText confidence="0.9988158">
introduce no c tokens on source side, but all on
target side. Target identity is defined analogously.
The results can be found in Tab. 1. We use the
setup with all c tokens in all following experi-
ments, which showed the best BLEU performance.
</bodyText>
<subsectionHeader confidence="0.985459">
4.2 Word-based RNN Models
</subsectionHeader>
<bodyText confidence="0.999889">
Given a pair of source sequence fI1 = f1 ... fI
and target sequence eI1 = e1 ... eI, where we as-
sume a direct correspondence between fi and ei,
we define the posterior translation probability by
factorizing on the target words:
</bodyText>
<equation confidence="0.999392">
p(ei|ei−1
1 ,fI 1 ) (1)
p(ei|ei−1 1 , fi+d
1 ) (2)
p(ei|fi+d
1 ). (3)
</equation>
<bodyText confidence="0.999974266666667">
We denote the formulation (1) as the bidirectional
joint model (BJM). This model can be simplified
by several independence assumptions. First, we
drop the dependency on the future source infor-
mation, receiving what we denote as the unidirec-
tional joint model (JM) in (2). Here, d ∈ N0 is
a delay parameter, which is set to d = 0 for all
experiments, except for the comparative results re-
ported in Fig. 7. Finally, assuming conditional in-
dependence from the previous target sequence, we
receive the unidirectional translation model (TM)
in (3). Analogously, we can define a bidirectional
translation model (BTM) by keeping the depen-
dency on the full source sentence fI1, but dropping
the previous target sequence ei−1
</bodyText>
<equation confidence="0.999719">
1 :
p(eI1|fI1) ≈ YI p(ei|fI1). (4)
i=1
</equation>
<bodyText confidence="0.999707166666667">
Fig. 2 shows the dependencies of the word-
based neural translation and joint models. The
alignment points are traversed in target order and
at each time step one target word is predicted.
The pure translation model (TM) takes only source
words as input, while the joint model (JM) takes
the preceding target words as an additional input.
A delay of d &gt; 0 is implemented by shifting the
target sequence by d time steps and filling the first
d target positions and the last d source positions
with a dedicated Cpadding symbol. The RNN archi-
tecture for the unidirectional word-based models
</bodyText>
<subsectionHeader confidence="0.811545">
all models bidirectional
</subsectionHeader>
<bodyText confidence="0.91667625">
Figure 2: Dependencies modeled within the word-
based RNN models when predicting the target
word ’know’. Directly processed information is
depicted with solid rectangles, and information
available through recurrent connections is marked
with dashed rectangles.
is illustrated in Fig. 3, which corresponds to the
following set of equations:
</bodyText>
<equation confidence="0.989883666666667">
yi = A1 ˆfi + A2ˆei−1
zi = ξ(yi; A3, yi−1
1 )
pc(ei)|ei−1
1 , fi� = ϕc(ei)(A4zi)
1
p(ei|c(ei), ei−1
1 , fi � = ϕei(Ac(ei)zi)
1 �
·
p(ei|ei−1
1 , fi 1) = pei|c(ei), ei−1
1 ,fi 1
pc(ei)|ei−1 �
1 , fi 1
</equation>
<bodyText confidence="0.96113580952381">
Here, by ˆfi and ˆei−1 we denote the one-hot en-
coded vector representations of the source and
target words fi and ei−1. The outgoing activa-
tion values of the projection layer and the LSTM
layer are yi and zi, respectively. The matrices Aj
contain the weights of the neural network layers.
By ξ(· ; A3, yi−1
1 ) we denote the LSTM formalism
that we plug in at the third layer. As the LSTM
layer is recurrent, we explicitly include the de-
pendence on the previous layer activations yi−1
1 .
Finally, ϕ is the widely-used softmax function to
obtain normalized probabilities, and c denotes a
word class mapping from any target word to its
unique word class. For the bidirectional model,
the equations can be defined analogously.
Due to the use of word classes, the output
layer consists of two parts. The class probabil-
ity pc(ei)|ei−1
1 , fi � is computed first, and then
</bodyText>
<page confidence="0.565014">
1
</page>
<figure confidence="0.979465909090909">
.
incredibly
Eunaligned
this
know
,
joint model
example
for
,
Surfers
</figure>
<equation confidence="0.942252833333333">
p(eI1|fI1) = YI
i=1
≈ YI
i=1
≈ YI
i=1
</equation>
<page confidence="0.946061">
17
</page>
<figure confidence="0.99555275">
.
.
p(c(ei)Iei-1 fi+d) p(e.le(e.) ei−1 fi+d)
a 1 , � a a, l , � 1
output layer
LSTM layer
projection layer
input layer
</figure>
<figureCaption confidence="0.767921666666667">
Figure 3: Architecture of a recurrent unidirec-
tional translation model. By including the dashed
parts, a joint model is obtained.
</figureCaption>
<bodyText confidence="0.994937230769231">
the word probability p(ei|c(ei), ei−1
1 , f1i) is ob-
tained given the word class. This trick helps avoid-
ing the otherwise computationally expensive nor-
malization sum, which would be carried out over
all words in the target vocabulary. In a class-
factorized output layer where each word belongs
to a single class, the normalization is carried out
over all classes, whose number is typically much
less than the vocabulary size. The other normal-
ization sum needed to produce the word probabil-
ity is limited to the words belonging to the same
class (Goodman, 2001; Morin and Bengio, 2005).
</bodyText>
<subsectionHeader confidence="0.998157">
4.3 Phrase-based RNN Models
</subsectionHeader>
<bodyText confidence="0.97362625">
One of the conceptual disadvantages of word-
based modeling as introduced in the previous sec-
tion is that there is a mismatch between train-
ing and testing conditions: During neural network
training, the vocabulary has to be extended by ad-
ditional c tokens, and a one-to-one alignment is
used which does not reflect the situation in decod-
ing. In phrase-based machine translation, more
complex alignments in terms of multiple words
on both the source and the target sides are used,
which allow the decoder to make use of richer
short-distance dependencies and are crucial for the
performance of the resulting system.
From this perspective, it seems interesting to
standardize the alignments used in decoding, and
in training the neural network. However, it is dif-
ficult to use the phrases themselves as the vocab-
ulary of the RNN. Usually, the huge number of
potential phrases in comparison to the relatively
small amount of training data makes the learn-
ing of continuous phrase representations difficult
Surfers , for example , know this incredibly
Surfer zum Beispiel kennen das zur Genüge
Figure 4: Example phrase alignment for a sen-
tence from the IWSLT training data.
due to data sparsity. This is confirmed by results
presented in (Le et al., 2012), which show that a
word-factored translation model outperforms the
phrase-factored version. Therefore, in this work
we continue relying on source and target word vo-
cabularies for building our phrase representations.
However, we no longer use a direct correspon-
dence between a source and a target word, as en-
forced in our word-based models.
Fig. 4 shows an example phrase alignment,
where a sequence of source words ˜fi is directly
mapped to a sequence of target words ˜ei for 1 ≤
i ≤ ˜I. By ˜I, we denote the number of phrases in
the alignment. We decompose the target sentence
posterior probability in the following way:
</bodyText>
<equation confidence="0.992989">
p(˜ei|˜ei−1
1 , f˜1I˜ ) (5)
p(˜ei|˜ei−1
1 , ˜fi 1) (6)
</equation>
<bodyText confidence="0.986502684210526">
where the joint model in Eq. 5 would correspond
to a bidirectional RNN, and Eq. 6 only requires a
unidirectional RNN. By leaving out the condition-
ing on the target side, we obtain a phrase-based
translation model.
As there is no one-to-one correspondence be-
tween the words within a phrase, the basic idea of
our phrase-based approach is to let the neural net-
work learn the dependencies itself, and present the
full source side of the phrase to the network be-
fore letting it predict target side words. Then the
probability for the target side of a phrase can be
computed, in case of Eq. 6, by:
p((˜ei)j |(˜ei)i−1, ˜e1 i 1, ˜fi1),
and analogously for the case of Eq. 5. Here, (˜ei)j
denotes the j-th word of the i-th aligned target
phrase.
We feed the source side of a phrase into the neu-
ral network one word at a time. Only when the
</bodyText>
<figure confidence="0.906092111111111">
fi+d ei−1
class layer
p(eI1|fJ1 ) = HI˜
i=1
HI˜
≈
i=1
p(˜ei|˜ei−1
1 , f˜ 1 I˜ ) =
|˜ei|
H
j=1
18
Surfers ε , for example , know this ε incredibly .
output layer
LSTM layer
projection layer
input layer
</figure>
<figureCaption confidence="0.999167">
Figure 5: A recurrent phrase-based joint translation model, unfolded over time. Source words are printed
</figureCaption>
<bodyText confidence="0.99197946">
in normal face, while target words are printed in bold face. Dashed lines indicate phrases from the
example sentence. For brevity, we omit the precise handling of sentence begin and end tokens.
presentation of the source side is finished we start
estimating probabilities for the target side. There-
fore, we do not let the neural network learn a target
distribution until the very last source word is con-
sidered. In this way, we break up the conventional
RNN training scheme where an input sample is di-
rectly followed by its corresponding teacher sig-
nal. Similarly, the presentation of the source side
of the next phrase only starts after the prediction
of the current target side is completed.
To this end, we introduce a no-operation token,
denoted by ε, which is not part of the vocabulary
(which means it cannot be input to or predicted by
the RNN). When the ε token occurs as input, it in-
dicates that no input needs to be processed by the
RNN. When the ε token occurs as a teacher signal
for the RNN, the output layer distribution is ig-
nored, and does not even have to be computed. In
both cases, all the other layers are still processed
during forward and backward passes such that the
RNN state can be advanced even without addi-
tional input or output.
Fig. 5 depicts the evaluation of a phrase-based
joint model for the example alignment from Fig. 4.
For a source phrase ˜fz, we include (|˜ez|−1) many ε
symbols at the end of the phrase. Conversely, for
a target phrase ˜ez, we include ( |˜fz |− 1) many ε
symbols at the beginning of the phrase.
E. g., in the figure, the second dashed rectan-
gle from the left depicts the training of the English
phrase “, for example ,” and its German transla-
tion “zum Beispiel”. At the input layer, we feed in
the source words one at a time, while we present
ε tokens at the target side input layer and the out-
put layer (with the exception of the very first time
step, where we still have the last target word from
the previous phrase as input instead of ε). With
the last word of the source phrase “Beispiel” being
presented to the network, the full source phrase is
stored in the hidden layer, and the neural network
is then trained to predict the target phrase words
at the output layer. Subsequently, the source input
is ε, and the target input is the most recent target
side history word.
To obtain a phrase-aligned training sequence for
the phrase-based RNN models, we force-align the
training data with the application of leave-one-out
as described in (Wuebker et al., 2010).
</bodyText>
<subsectionHeader confidence="0.99102">
4.4 Bidirectional RNN Architecture
</subsectionHeader>
<bodyText confidence="0.999980666666667">
While the unidirectional RNNs include an un-
bounded sentence history, they are still limited in
the number of future source words they include.
Bidirectional models provide a flexible means to
also include an unbounded future context, which,
unlike the delayed unidirectional models, require
no tuning to determine the amount of delay.
Fig. 6 illustrates the bidirectional model archi-
tecture, which is an extension of the unidirectional
model of Fig. 3. First, an additional recurrent
hidden layer is added in parallel to the existing
one. This layer will be referred to as the back-
ward layer, since it processes information in back-
ward time direction. This hidden layer receives
source word input only, while target words in the
case of a joint model are fed to the forward layer
as in the unidirectional case. Due to the backward
recurrency, the backward layer will make the in-
formation fi available when predicting the target
word ez, while the forward layer takes care of the
source history fz1. Jointly, the forward and back-
ward branches include the full source sentence fi1,
as indicated in Fig. 2. Fig. 6 shows the “deep”
variant of the bidirectional model, where the for-
</bodyText>
<page confidence="0.991865">
19
</page>
<figure confidence="0.977173857142857">
p(c(ei)|ei−1
1 , fi) p(ei|c(ei), ei 1, fi)
output layer
2nd LSTM layer
1st LSTM layer
projection layer
input layer
</figure>
<figureCaption confidence="0.938453">
Figure 6: Architecture of a recurrent bidirectional
translation model. By (+) and (−), we indicate
</figureCaption>
<bodyText confidence="0.964492823529412">
a processing in forward and backward time direc-
tions, respectively. The inclusion of the dashed
parts leads to a bidirectional joint model. One
source projection matrix is used for the forward
and backward branches.
ward and backward layers converge into a hidden
layer. A shallow variant can be obtained if the
parallel layers converge into the output layer di-
rectly1.
Due to the full dependence on the source se-
quence, evaluating bidirectional networks requires
computing the forward pass of the forward and
backward layers for the full sequence, before be-
ing able to evaluate the next layers. In the back-
ward pass of backpropagation, the forward and
backward recurrent layers are processed in de-
creasing and increasing time order, respectively.
</bodyText>
<sectionHeader confidence="0.999438" genericHeader="evaluation">
5 Experiments
</sectionHeader>
<subsectionHeader confidence="0.973255">
5.1 Setup
</subsectionHeader>
<bodyText confidence="0.9998618">
All translation experiments are performed with the
Jane toolkit (Vilar et al., 2010; Wuebker et al.,
2012). The largest part of our experiments is car-
ried out on the IWSLT 2013 German—*English
shared translation task.2 The baseline system is
trained on all available bilingual data, 4.3M sen-
tence pairs in total, and uses a 4-gram LM with
modified Kneser-Ney smoothing (Kneser and Ney,
1995; Chen and Goodman, 1998), trained with
the SRILM toolkit (Stolcke, 2002). As additional
</bodyText>
<footnote confidence="0.9939375">
1In our implementation, the forward and backward layers
converge into an intermediate identity layer, and the aggre-
gate is weighted and fed to the next layer.
2http://www.iwslt2013.org
</footnote>
<bodyText confidence="0.999758543478261">
data sources for the LM we selected parts of the
Shuffled News and LDC English Gigaword cor-
pora based on cross-entropy difference (Moore
and Lewis, 2010), resulting in a total of 1.7 bil-
lion running words for LM training. The state-of-
the-art baseline is a standard phrase-based SMT
system (Koehn et al., 2003) tuned with MERT
(Och, 2003). It contains a hierarchical reorder-
ing model (Galley and Manning, 2008) and a 7-
gram word cluster language model (Wuebker et
al., 2013). Here, we also compare against a feed-
forward joint model as described by Devlin et al.
(2014), with a source window of 11 words and a
target history of three words, which we denote as
BBN-JM. Instead of POS tags, we predict word
classes trained with mkcls. We use a shortlist
of size 16K and 1000 classes for the remaining
words. All neural networks are trained on the TED
portion of the data (138K segments) and are ap-
plied in a rescoring step on 1000-best lists.
To confirm our results, we run additional
experiments on the Arabic—*English and
Chinese—*English tasks of the DARPA BOLT
project. In both cases, the neural network models
are added on top of our most competitive eval-
uation system. On Chinese—*English, we use a
hierarchical phrase-based system trained on 3.7M
segments with 22 dense features, including an ad-
vanced orientation model (Huck et al., 2013). For
the neural network training, we selected a subset
of 9M running words. The Arabic—*English
system is a standard phrase-based decoder trained
on 6.6M segments, using 17 dense features. The
neural network training was performed using a
selection amounting to 15.5M running words.
For both tasks we apply the neural networks by
rescoring 1000-best lists and evaluate results on
two data sets from the ’discussion forum’ domain,
test1 and test2. The sizes of the data sets
for the Arabic—*English system are: 1219 (dev),
1510 (test1), and 1137 (test2) segments, and
for the Chinese—*English system are: 5074 (dev),
1844 (test1), and 1124 (test2) segments. All
results are measured in case-insensitive BLEU [%]
(Papineni et al., 2002) and TER [%] (Snover et al.,
2006) on a single reference.
</bodyText>
<subsectionHeader confidence="0.824352">
5.2 Results
</subsectionHeader>
<bodyText confidence="0.998603333333333">
Our results on the IWSLT German—*English task
are summarized in Tab. 2. At this point, we
do not include a recurrent neural network lan-
</bodyText>
<figure confidence="0.9900824">
fi ei−1
class layer
(−)
(+)
(+)
</figure>
<page confidence="0.846804">
20
</page>
<table confidence="0.997003666666667">
dev test
BLEU TER BLEU TER
baseline 33.5 45.8 30.9 48.4
TM 34.6 44.5 32.0 47.1
JM 34.7 44.7 31.8 47.4
BTM 34.7 44.9 32.3 47.0
BTM (deep) 34.8 44.3 32.5 46.7
BJM 34.7 44.5 32.1 47.0
BJM (deep) 34.9 44.1 32.2 46.6
PTM 34.3 44.9 32.1 47.5
PJM 34.3 45.0 32.0 47.5
PJM (10-best) 34.4 44.8 32.0 47.3
PJM (deep) 34.6 44.7 32.0 47.6
PBJM (deep) 34.8 44.9 31.9 47.5
BBN-JM 34.4 44.9 31.9 47.6
</table>
<tableCaption confidence="0.6887415">
Table 2: Results for the IWSLT 2013
German→English task with different RNN
models. T: translation, J: joint, B: bidirectional,
P: phrase-based.
</tableCaption>
<bodyText confidence="0.995572830769231">
guage model yet. Here, the delay parameter d
from Equations 2 and 3 is set to zero. We ob-
serve that for all recurrent translation models, we
achieve substantial improvements over the base-
line on the test data, ranging from 0.9 BLEU
up to 1.6 BLEU. These results are also consistent
with the improvements in terms of TER, where we
achieve reductions by 0.8 TER up to 1.8 TER.
These numbers can be directly compared to the
case of feedforward neural network-based transla-
tion modeling as proposed in (Devlin et al., 2014)
which we include in the very last row of the table.
Nearly all of our recurrent models outperform the
feedforward approach, where the RNN model per-
forming best on the dev data is better on test
by 0.3 BLEU and 1.0 TER.
Interestingly, for the recurrent word-based mod-
els, on the test data it can be seen that TMs per-
form better than JMs, even though TMs do not
take advantage of the target side history words.
However, exploiting this extra information does
not always need to result in a better model, as the
target side words are only derived from the given
source side, which is available to both TMs and
JMs. On the other hand, including future source
words in a bidirectional model clearly improves
the performance further. By adding another LSTM
Figure 7: BLEU scores on the IWSLT test set
with different delays for the unidirectional RNN-
TM and the bidirectional RNN-BTM.
layer that combines forward and backward time
directions (indicated as ‘deep’ in the table), we ob-
tain our overall best model.
In Fig. 7 we compare the word-based bidirec-
tional TM with a unidirectional TM that uses dif-
ferent time delays d = 0, ... , 4. For a delay d =
2, the same performance is obtained as with the
bidirectional model, but this comes at the price of
tuning the delay parameter.
In comparison to the unidirectional word-based
models, phrase-based models perform similarly.
In the tables, we include those phrase-based vari-
ants which perform best on the dev data, where
phrase-based JMs always are at least as good or
better than the corresponding TMs in terms of
BLEU. Therefore, we mainly report JM results
for the phrase-based networks. A phrase-based
model can also be trained on multiple variants for
the phrase alignment. For our experiments, we
tested 10-best alignments against the single best
alignment, which resulted in a small improvement
of 0.2 TER on both dev and test. We did not ob-
serve consistent gains by using an additional hid-
den layer or bidirectional models. To some ex-
tent, future information is already considered in
unidirectional phrase-based models by feeding the
complete source side before predicting the target
side.
Tab. 3 shows different model combination re-
sults for the IWSLT task, where a recurrent lan-
guage model is included in the baseline. Adding
a deep bidirectional TM or JM to the recur-
rent language model improves the RNN-LM base-
line by 1.2 BLEU or 1.1 BLEU, respectively. A
phrase-based model substantially improves over
</bodyText>
<page confidence="0.996779">
21
</page>
<table confidence="0.999495">
dev TER1%] eval11 test
BLEU1%] BLEU1%] TER1%] BLEU1%] TER1%]
baseline (w/ RNN-LM) 34.3 44.8 36.4 42.9 31.5 47.8
BTM (deep) 34.9 43.7 37.6 41.5 32.7 46.1
BJM (deep) 35.0 44.4 37.4 41.9 32.6 46.5
PBJM (deep) 34.8 44.6 36.9 42.6 32.3 47.2
4 RNN models 35.2 43.4 38.0 41.2 32.7 46.0
</table>
<tableCaption confidence="0.994921">
Table 3: Results for the IWSLT 2013 German-*English task with different RNN models. All results
</tableCaption>
<bodyText confidence="0.984732076923077">
include a recurrent language model. T: translation, J: joint, B: bidirectional, P: phrase-based.
the RNN-LM baseline, but performs not as good
as its word-based counterparts. By adding four
different translation models, including models in
reverse word order and reverse translation direc-
tion, we are able to improve these numbers even
further. However, especially on the test data, the
gains from model combination saturate quickly.
Apart from the IWSLT track, we also ana-
lyze the performance of our translation models on
the BOLT Chinese-*English and Arabic-*English
translation tasks. Due to the large amount of train-
ing data, we concentrate on models of high perfor-
mance in the IWSLT experiments. The results can
be found in Tab. 4 and 5. In both cases, we see
consistent improvements over the recurrent neural
network language model baseline, improving the
Arabic-*English system by 0.6 BLEU and 0.5 TER
on test1. This can be compared to the rescoring
results for the same task reported by (Devlin et al.,
2014), where they achieved 0.3 BLEU, despite the
fact that they used multiple references for scoring,
whereas in our experiments we rely on a single
reference only. The models are also able to im-
prove the Chinese-*English system by 0.5 BLEU
and 0.5 TER on test2.
</bodyText>
<subsectionHeader confidence="0.999629">
5.3 Analysis
</subsectionHeader>
<bodyText confidence="0.999852636363636">
To investigate whether bidirectional models ben-
efit from future source information, we compare
the single-best output of a system reranked with a
unidirectional model to the output reranked with
a bidirectional model. We choose the models
to be translation models in both cases, as they
predict target words independent of previous
predictions, given the source information (cf. Eqs.
(3, 4)). This makes it easier to detect the effect
of including future source information or the lack
thereof. The examples are taken from the IWSLT
</bodyText>
<table confidence="0.997366125">
test1 test2
BLEU TER BLEU TER
baseline 25.2 57.4 26.8 57.3
BTM (deep) 25.6 56.6 26.8 56.7
BJM (deep) 25.9 56.9 27.4 56.7
RNN-LM 25.6 57.1 27.5 56.7
+ BTM (deep) 25.9 56.7 27.3 56.8
+ BJM (deep) 26.2 56.6 27.9 56.5
</table>
<tableCaption confidence="0.950335">
Table 4: Results for the BOLT Arabic-*English
</tableCaption>
<bodyText confidence="0.990572111111111">
task with different RNN models. The “+” sign in
the last two rows indicates that either of the corre-
sponding deep models (BTM and BJM) are added
to the baseline including the recurrent language
model (i.e. they are not applied at the same time).
T: translation, J: joint, B: bidirectional.
task, where we include the one-to-one source
information, reordered according to the target
side.
</bodyText>
<tableCaption confidence="0.974445375">
source: nicht so wie ich
reference: not like me
Hypothesis 1:
1-to-1 source: so ich a nicht wie
1-to-1 target: so I do n’t like
Hypothesis 2:
1-to-1 source: nicht so wie ich
1-to-1 target: not a like me
</tableCaption>
<bodyText confidence="0.9999315">
In this example, the German phrase “so wie”
translates to “like” in English. The bidirectional
model prefers hypothesis 2, making use of the
future word “wie” when translating the German
word “so” to c, because it has future insight that
this move will pay off later when translating
</bodyText>
<page confidence="0.99465">
22
</page>
<table confidence="0.999850857142857">
BLEU TER BLEU TER
baseline 18.3 63.6 16.7 63.0
BTM (deep) 18.7 63.3 17.1 62.6
BJM (deep) 18.5 63.1 17.2 62.3
RNN-LM 18.8 63.3 17.2 62.8
+ BTM (deep) 18.9 63.1 17.7 62.3
+ BJM (deep) 18.8 63.3 17.5 62.5
</table>
<tableCaption confidence="0.965832">
Table 5: Results for the BOLT Chinese→English
</tableCaption>
<bodyText confidence="0.982381153846154">
task with different RNN models. The “+” sign in
the last two rows indicates that either of the corre-
sponding deep models (BTM and BJM) are added
to the baseline including the recurrent language
model (i.e. they are not applied at the same time).
T: translation, B: bidirectional.
the rest of the sentence. This information is
not available to the unidirectional model, which
prefers hypothesis 1 instead.
source: das taten wir dann auch und verschafften uns
so eine Zeit lang einen Wettbewerbs Vorteil .
reference: and we actually did that and it gave us a
competitive advantage for a while.
</bodyText>
<tableCaption confidence="0.6614345">
Hypothesis 1:
1-to-1 source: das e e e wir dann auch taten und
verschafften uns so eine Zeit lang einen Wettbewerbs
Vorteil .
1-to-1 target: that ’s just what we e e did and gave us e
a time , a competitive advantage .
Hypothesis 2:
1-to-1 source: das e e e wir dann auch taten und
verschafften uns so einen Wettbewerbs Vorteil a eine
Zeit lang .
1-to-1 target: that ’s just what we e e did and gave us e
a competitive advantage for a e while.
</tableCaption>
<bodyText confidence="0.9999818">
Here, the German phrase “eine Zeit lang” trans-
lates to “for a while” in English. Bidirectional
scoring favors hypothesis 2, while unidirectional
scoring favors hypothesis 1. It seems that the uni-
directional model translates “Zeit” to “time” as the
object of the verb “give” in hypothesis 1, being
blind to the remaining part “lang” of the phrase
which changes the meaning. The bidirectional
model, to its advantage, has the full source infor-
mation, allowing it to make the correct prediction.
</bodyText>
<sectionHeader confidence="0.999143" genericHeader="conclusions">
6 Conclusion
</sectionHeader>
<bodyText confidence="0.9999826875">
We developed word- and phrase-based RNN trans-
lation models. The former is simple and performs
well in practice, while the latter is more consistent
with the phrase-based paradigm. The approach in-
herently evades data sparsity problems as it works
on words in its lowest level of processing. Our
experiments show the models are able to achieve
notable improvements over baselines containing a
recurrent LM.
In addition, and for the first time in statistical
machine translation, we proposed a bidirectional
neural architecture that allows modeling past and
future dependencies of any length. Besides its
good performance in practice, the bidirectional ar-
chitecture is of theoretical interest as it allows the
exact modeling of posterior probabilities.
</bodyText>
<sectionHeader confidence="0.999157" genericHeader="acknowledgments">
Acknowledgments
</sectionHeader>
<bodyText confidence="0.999932533333333">
This material is partially based upon work sup-
ported by the DARPA BOLT project under Con-
tract No. HR0011- 12-C-0015. Any opinions,
findings and conclusions or recommendations ex-
pressed in this material are those of the authors and
do not necessarily reflect the views of DARPA.
The research leading to these results has also re-
ceived funding from the European Union Sev-
enth Framework Programme (FP7/2007-2013) un-
der grant agreements no 287658 and no 287755.
Experiments were performed with computing re-
sources granted by JARA-HPC from RWTH
Aachen University under project ‘jara0085’. We
would like to thank Jan-Thorsten Peter for provid-
ing the BBN-JM system.
</bodyText>
<sectionHeader confidence="0.998998" genericHeader="references">
References
</sectionHeader>
<reference confidence="0.9960732">
Ebru Arisoy, Tara N. Sainath, Brian Kingsbury, and
Bhuvana Ramabhadran. 2012. Deep neural network
language models. In Proceedings of the NAACL-
HLT 2012 Workshop: Will We Ever Really Replace
the N-gram Model? On the Future of Language
Modeling for HLT, pages 20–28. Association for
Computational Linguistics.
Michael Auli, Michel Galley, Chris Quirk, and Geof-
frey Zweig. 2013. Joint Language and Translation
Modeling with Recurrent Neural Networks. In Con-
ference on Empirical Methods in Natural Language
Processing, pages 1044–1054, Seattle, USA, Octo-
ber.
Yoshua Bengio, Patrice Simard, and Paolo Frasconi.
1994. Learning long-term dependencies with gra-
</reference>
<page confidence="0.988704">
23
</page>
<reference confidence="0.999009445454546">
dient descent is difficult. Neural Networks, IEEE
Transactions on, 5(2):157–166.
Maria Asunci´on Casta˜no and Francisco Casacuberta.
1997. A connectionist approach to machine trans-
lation. In 5th International Conference on Speech
Communication and Technology (EUROSPEECH-
97), Rhodes, Greece.
Maria Asunci´on Casta˜no and Francisco Casacuberta.
1999. Text-to-text machine translation using the
RECONTRA connectionist model. In Lecture Notes
in Computer Science (IWANN 99), volume 1607,
pages 683–692, Alicante, Spain.
Maria Asunci´on Casta˜no, Francisco Casacuberta, and
Enrique Vidal. 1997. Machine translation using
neural networks and finite-state models. In 7th In-
ternational Conference on Theoretical and Method-
ological Issues in Machine Translation. TMI’97,
pages 160–167, Santa Fe, USA.
Stanley F. Chen and Joshua Goodman. 1998. An
Empirical Study of Smoothing Techniques for Lan-
guage Modeling. Technical Report TR-10-98, Com-
puter Science Group, Harvard University, Cam-
bridge, MA, August.
Jacob Devlin, Rabih Zbib, Zhongqiang Huang, Thomas
Lamar, Richard Schwartz, and John Makhoul. 2014.
Fast and Robust Neural Network Joint Models for
Statistical Machine Translation. In 52nd Annual
Meeting of the Association for Computational Lin-
guistics, page to appear, Baltimore, MD, USA, June.
Michel Galley and Christopher D. Manning. 2008.
A simple and effective hierarchical phrase reorder-
ing model. In Proceedings of the Conference on
Empirical Methods in Natural Language Process-
ing, EMNLP ’08, pages 848–856, Stroudsburg, PA,
USA. Association for Computational Linguistics.
Felix A. Gers, J¨urgen Schmidhuber, and Fred Cum-
mins. 2000. Learning to forget: Contin-
ual prediction with LSTM. Neural computation,
12(10):2451–2471.
Felix A. Gers, Nicol N. Schraudolph, and J¨urgen
Schmidhuber. 2003. Learning precise timing with
lstm recurrent networks. The Journal of Machine
Learning Research, 3:115–143.
Joshua Goodman. 2001. Classes for fast maximum
entropy training. In Acoustics, Speech, and Signal
Processing, 2001. Proceedings.(ICASSP’01). 2001
IEEE International Conference on, volume 1, pages
561–564. IEEE.
Alex Graves and J¨urgen Schmidhuber. 2005. Frame-
wise phoneme classification with bidirectional
LSTM and other neural network architectures. Neu-
ral Networks, 18(5):602–610.
James Henderson. 2004. Discriminative training of a
neural network statistical parser. In Proceedings of
the 42nd Annual Meeting on Association for Compu-
tational Linguistics, page 95. Association for Com-
putational Linguistics.
Sepp Hochreiter and J¨urgen Schmidhuber. 1997.
Long short-term memory. Neural computation,
9(8):1735–1780.
Yuening Hu, Michael Auli, Qin Gao, and Jianfeng Gao.
2014. Minimum translation modeling with recur-
rent neural networks. In Proceedings of the 14th
Conference of the European Chapter of the Associ-
ation for Computational Linguistics, pages 20–29,
Gothenburg, Sweden, April. Association for Com-
putational Linguistics.
Matthias Huck, Joern Wuebker, Felix Rietig, and Her-
mann Ney. 2013. A phrase orientation model for hi-
erarchical machine translation. In ACL 2013 Eighth
Workshop on Statistical Machine Translation, pages
452–463, Sofia, Bulgaria, August.
Nal Kalchbrenner and Phil Blunsom. 2013. Recurrent
continuous translation models. In Proceedings of
the 2013 Conference on Empirical Methods in Natu-
ral Language Processing, pages 1700–1709, Seattle,
Washington, USA, October. Association for Compu-
tational Linguistics.
Reinhard Kneser and Hermann Ney. 1995. Improved
backing-off for M-gram language modeling. In Pro-
ceedings of the International Conference on Acous-
tics, Speech, and Signal Processingw, volume 1,
pages 181–184, May.
Philipp Koehn, Franz Josef Och, and Daniel Marcu.
2003. Statistical Phrase-Based Translation. In Pro-
ceedings of the 2003 Meeting of the North Ameri-
can chapter of the Association for Computational
Linguistics (NAACL-03), pages 127–133, Edmon-
ton, Alberta.
Hai Son Le, Alexandre Allauzen, and Franc¸ois Yvon.
2012. Continuous Space Translation Models with
Neural Networks. In Conference of the North Amer-
ican Chapter of the Association for Computational
Linguistics: Human Language Technologies, pages
39–48, Montreal, Canada, June.
Xunying Liu, Yongqiang Wang, Xie Chen, Mark J. F.
Gales, and Phil C. Woodland. 2014. Efficient lattice
rescoring using recurrent neural network language
models. In Acoustics, Speech and Signal Processing
(ICASSP), 2014 IEEE International Conference on,
pages 4941–4945. IEEE.
Gr´egoire Mesnil, Xiaodong He, Li Deng, and Yoshua
Bengio. 2013. Investigation of recurrent-neural-
network architectures and learning methods for spo-
ken language understanding. In Interspeech, pages
3771–3775.
Tomas Mikolov, Stefan Kombrink, Lukas Burget,
JH Cernocky, and Sanjeev Khudanpur. 2011.
Extensions of recurrent neural network language
model. In Acoustics, Speech and Signal Processing
</reference>
<page confidence="0.975013">
24
</page>
<reference confidence="0.999726772277228">
(ICASSP), 2011 IEEE International Conference on,
pages 5528–5531. IEEE.
Robert C. Moore and William Lewis. 2010. Intelligent
Selection of Language Model Training Data. In ACL
(Short Papers), pages 220–224, Uppsala, Sweden,
July.
Frederic Morin and Yoshua Bengio. 2005. Hierarchi-
cal probabilistic neural network language model. In
Proceedings of the international workshop on artifi-
cial intelligence and statistics, pages 246–252.
Franz Josef Och. 2003. Minimum Error Rate Training
in Statistical Machine Translation. In Proc. of the
41th Annual Meeting of the Association for Compu-
tational Linguistics (ACL), pages 160–167, Sapporo,
Japan, July.
Kishore Papineni, Salim Roukos, Todd Ward, and Wei-
Jing Zhu. 2002. Bleu: a Method for Automatic
Evaluation of Machine Translation. In Proceed-
ings of the 41st Annual Meeting of the Associa-
tion for Computational Linguistics, pages 311–318,
Philadelphia, Pennsylvania, USA, July.
David E. Rumelhart, Geoffrey E. Hinton, and Ronald J.
Williams. 1986. Learning Internal Representations
by Error Propagation. In: J. L. McClelland, D. E.
Rumelhart, and The PDP Research Group: “Paral-
lel Distributed Processing, Volume 1: Foundations”.
The MIT Press.
Mike Schuster and Kuldip K. Paliwal. 1997. Bidirec-
tional recurrent neural networks. IEEE Transactions
on Signal Processing, 45(11):2673–2681.
Holger Schwenk, Daniel D´echelotte, and Jean-Luc
Gauvain. 2006. Continuous Space Language Mod-
els for Statistical Machine Translation. In Proceed-
ings of the COLING/ACL 2006 Main Conference
Poster Sessions, pages 723–730, Sydney, Australia,
July.
Holger Schwenk. 2012. Continuous Space Translation
Models for Phrase-Based Statistical Machine Trans-
lation. In 25th International Conference on Compu-
tational Linguistics (COLING), pages 1071–1080,
Mumbai, India, December.
Matthew Snover, Bonnie Dorr, Richard Schwartz, Lin-
nea Micciulla, and John Makhoul. 2006. A Study of
Translation Edit Rate with Targeted Human Annota-
tion. In Proceedings of the 7th Conference of the As-
sociation for Machine Translation in the Americas,
pages 223–231, Cambridge, Massachusetts, USA,
August.
Andreas Stolcke. 2002. SRILM – An Extensible Lan-
guage Modeling Toolkit. In Proc. of the Int. Conf.
on Speech and Language Processing (ICSLP), vol-
ume 2, pages 901–904, Denver, CO, September.
Martin Sundermeyer, Ralf Schl¨uter, and Hermann Ney.
2012. LSTM neural networks for language model-
ing. In Interspeech, Portland, OR, USA, September.
Martin Sundermeyer, Ilya Oparin, Jean-Luc Gauvain,
Ben Freiberg, Ralf Schl¨uter, and Hermann Ney.
2013. Comparison of feedforward and recurrent
neural network language models. In IEEE Interna-
tional Conference on Acoustics, Speech, and Signal
Processing, pages 8430–8434, Vancouver, Canada,
May.
Ashish Vaswani, Yinggong Zhao, Victoria Fossum,
and David Chiang. 2013. Decoding with large-
scale neural language models improves translation.
In Proceedings of the 2013 Conference on Em-
pirical Methods in Natural Language Processing,
pages 1387–1392, Seattle, Washington, USA, Oc-
tober. Association for Computational Linguistics.
David Vilar, Daniel Stein, Matthias Huck, and Her-
mann Ney. 2010. Jane: Open source hierarchi-
cal translation, extended with reordering and lexi-
con models. In ACL 2010 Joint Fifth Workshop on
Statistical Machine Translation and Metrics MATR,
pages 262–270, Uppsala, Sweden, July.
Paul J. Werbos. 1990. Backpropagation through time:
what it does and how to do it. Proceedings of the
IEEE, 78(10):1550–1560.
Ronald J. Williams and David Zipser. 1995. Gradient-
Based Learning Algorithms for Recurrent Net-
works and Their Computational Complexity. In:
Yves Chauvain and David E. Rumelhart: “Back-
Propagation: Theory, Architectures and Applica-
tions”. Lawrence Erlbaum Publishers.
Joern Wuebker, Arne Mauser, and Hermann Ney.
2010. Training phrase translation models with
leaving-one-out. In Proceedings of the 48th Annual
Meeting of the Assoc. for Computational Linguistics,
pages 475–484, Uppsala, Sweden, July.
Joern Wuebker, Matthias Huck, Stephan Peitz, Malte
Nuhn, Markus Freitag, Jan-Thorsten Peter, Saab
Mansour, and Hermann Ney. 2012. Jane 2: Open
source phrase-based and hierarchical statistical ma-
chine translation. In International Conference on
Computational Linguistics, pages 483–491, Mum-
bai, India, December.
Joern Wuebker, Stephan Peitz, Felix Rietig, and Her-
mann Ney. 2013. Improving statistical machine
translation with word class models. In Conference
on Empirical Methods in Natural Language Pro-
cessing, pages 1377–1381, Seattle, USA, October.
</reference>
<page confidence="0.998729">
25
</page>
</variant>
</algorithm>
<algorithm name="ParsHed" version="110505">
<variant no="0" confidence="0.464867">
<title confidence="0.999873">Translation Modeling with Bidirectional Recurrent Neural Networks</title>
<author confidence="0.989333">Tamer Joern</author>
<author confidence="0.989333">Hermann</author>
<affiliation confidence="0.9731205">Language Technology and Pattern Recognition RWTH Aachen University, Aachen,</affiliation>
<address confidence="0.671042">Language Processing Univ. Paris-Sud, France and LIMSI/CNRS, Orsay,</address>
<abstract confidence="0.998343571428571">This work presents two different translation models using recurrent neural networks. The first one is a word-based approach using word alignments. Second, we present phrase-based translation models that are more consistent with phrasebased decoding. Moreover, we introduce bidirectional recurrent neural models to the problem of machine translation, allowing us to use the full source sentence in our models, which is also of theoretical interest. We demonstrate that our translation models are capable of improving strong baselines already including recurrent neural language models on three tasks: 2013 BOLT and obtain gains up to 1.6% 1.7% rescoring lists.</abstract>
</variant>
</algorithm>
<algorithm name="ParsCit" version="110505">
<citationList>
<citation valid="true">
<authors>
<author>Ebru Arisoy</author>
<author>Tara N Sainath</author>
<author>Brian Kingsbury</author>
<author>Bhuvana Ramabhadran</author>
</authors>
<title>Deep neural network language models.</title>
<date>2012</date>
<booktitle>In Proceedings of the NAACLHLT</booktitle>
<contexts>
<context position="7304" citStr="Arisoy et al., 2012" startWordPosition="1104" endWordPosition="1107">ximately normalized scores directly. The latter work makes use of translation and joint models, and pre-computes the first hidden layer beforehand, resulting in large speedups. They report major improvements over strong baselines. The speedups achieved by both works allowed to integrate feedforward neural networks into the decoder. 3 LSTM Recurrent Neural Networks Our work is based on recurrent neural networks. In related fields like e. g. language modeling, this type of neural network has been shown to perform considerably better than standard feedforward architectures (Mikolov et al., 2011; Arisoy et al., 2012; Sundermeyer et al., 2013; Liu et al., 2014). Most commonly, recurrent neural networks are trained with stochastic gradient descent (SGD), where the gradient of the training criterion is computed with the backpropagation through time algorithm (Rumelhart et al., 1986; Werbos, 1990; Williams and Zipser, 1995). However, the combination of RNN networks with conventional backpropagation training leads to conceptual difficulties which are known as the vanishing (or exploding) gradient problem, described e. g. in (Bengio et al., 1994). To remedy this problem, in (Hochreiter and Schmidhuber, 1997) i</context>
</contexts>
<marker>Arisoy, Sainath, Kingsbury, Ramabhadran, 2012</marker>
<rawString>Ebru Arisoy, Tara N. Sainath, Brian Kingsbury, and Bhuvana Ramabhadran. 2012. Deep neural network language models. In Proceedings of the NAACLHLT 2012 Workshop: Will We Ever Really Replace the N-gram Model? On the Future of Language Modeling for HLT, pages 20–28. Association for Computational Linguistics.</rawString>
</citation>
<citation valid="true">
<authors>
<author>Michael Auli</author>
<author>Michel Galley</author>
<author>Chris Quirk</author>
<author>Geoffrey Zweig</author>
</authors>
<title>Joint Language and Translation Modeling with Recurrent Neural Networks.</title>
<date>2013</date>
<booktitle>In Conference on Empirical Methods in Natural Language Processing,</booktitle>
<pages>1044--1054</pages>
<location>Seattle, USA,</location>
<contexts>
<context position="2003" citStr="Auli et al., 2013" startWordPosition="283" endWordPosition="286">chwenk et al., 2006; Vaswani et al., 2013), as well as translation models (Le et al., 2012; Schwenk, 2012; Devlin et al., 2014). Different from the feedforward design, recurrent neural networks (RNNs) have the advantage of being able to take into account an unbounded history of previous observations. In theory, this enables them to model long-distance dependencies of arbitrary length. However, while previous work on translation modeling with recurrent neural networks shows its effectiveness on standard baselines, so far no notable gains have been presented on top of recurrent language models (Auli et al., 2013; Kalchbrenner and Blunsom, 2013; Hu et al., 2014). In this work, we present two novel approaches to recurrent neural translation modeling: wordbased and phrase-based. The word-based approach assumes one-to-one aligned source and target sentences. We evaluate different ways of resolving alignment ambiguities to obtain such alignments. The phrase-based RNN approach is more closely tied to the underlying translation paradigm. It models actual phrasal translation probabilities while avoiding sparsity issues by using single words as input and output units. Furthermore, in addition to the unidirect</context>
<context position="4924" citStr="Auli et al. (2013)" startWordPosition="734" endWordPosition="737">ks. They compare between word-factored and tuple-factored n-gram models, obtaining their best results using the word-factored approach, which is less amenable to data sparsity issues. Both of our word-based and phrase-based models eventually work on the word level. Kalchbrenner and Blunsom (2013) use recurrent neural networks with full source sentence representations. The continuous representations are obtained by applying a sequence of convolutions, and the result is fed into the hidden layer of a recurrent language model. Rescoring results indicate no improvements over the state of the art. Auli et al. (2013) also include source sentence representations built either using Latent Semantic Analysis or by concatenating word embeddings. This approach produced no notable gain over systems using a recurrent language model. On the other hand, our proposed bidirectional models include the full source sentence relying on recurrency, yielding improvements over competitive baselines already including a recurrent language model. RNNs were also used with minimum translation units (Hu et al., 2014), which are phrase pairs undergoing certain constraints. At the input layer, each of the source and target phrases </context>
</contexts>
<marker>Auli, Galley, Quirk, Zweig, 2013</marker>
<rawString>Michael Auli, Michel Galley, Chris Quirk, and Geoffrey Zweig. 2013. Joint Language and Translation Modeling with Recurrent Neural Networks. In Conference on Empirical Methods in Natural Language Processing, pages 1044–1054, Seattle, USA, October.</rawString>
</citation>
<citation valid="true">
<authors>
<author>Yoshua Bengio</author>
<author>Patrice Simard</author>
<author>Paolo Frasconi</author>
</authors>
<title>Learning long-term dependencies with gradient descent is difficult. Neural Networks,</title>
<date>1994</date>
<journal>IEEE Transactions on,</journal>
<volume>5</volume>
<issue>2</issue>
<contexts>
<context position="7839" citStr="Bengio et al., 1994" startWordPosition="1187" endWordPosition="1190">er than standard feedforward architectures (Mikolov et al., 2011; Arisoy et al., 2012; Sundermeyer et al., 2013; Liu et al., 2014). Most commonly, recurrent neural networks are trained with stochastic gradient descent (SGD), where the gradient of the training criterion is computed with the backpropagation through time algorithm (Rumelhart et al., 1986; Werbos, 1990; Williams and Zipser, 1995). However, the combination of RNN networks with conventional backpropagation training leads to conceptual difficulties which are known as the vanishing (or exploding) gradient problem, described e. g. in (Bengio et al., 1994). To remedy this problem, in (Hochreiter and Schmidhuber, 1997) it was suggested to modify the architecture of a standard RNN in such a way that vanishing and exploding gradients are avoided during backpropagation. In particular, no modification of the training algorithm is necessary. The resulting architecture is referred to as long short-term memory (LSTM) neural network. Bidirectional recurrent neural networks (BRNNs) were first proposed in (Schuster and Paliwal, 1997) and applied to speech recognition tasks. They have been since applied to different 15 . incredibly Eunaligned this know , e</context>
</contexts>
<marker>Bengio, Simard, Frasconi, 1994</marker>
<rawString>Yoshua Bengio, Patrice Simard, and Paolo Frasconi. 1994. Learning long-term dependencies with gradient descent is difficult. Neural Networks, IEEE Transactions on, 5(2):157–166.</rawString>
</citation>
<citation valid="true">
<authors>
<author>Maria Asunci´on Casta˜no</author>
<author>Francisco Casacuberta</author>
</authors>
<title>A connectionist approach to machine translation.</title>
<date>1997</date>
<booktitle>In 5th International Conference on Speech Communication and Technology (EUROSPEECH97),</booktitle>
<location>Rhodes, Greece.</location>
<marker>Casta˜no, Casacuberta, 1997</marker>
<rawString>Maria Asunci´on Casta˜no and Francisco Casacuberta. 1997. A connectionist approach to machine translation. In 5th International Conference on Speech Communication and Technology (EUROSPEECH97), Rhodes, Greece.</rawString>
</citation>
<citation valid="true">
<authors>
<author>Maria Asunci´on Casta˜no</author>
<author>Francisco Casacuberta</author>
</authors>
<title>Text-to-text machine translation using the RECONTRA connectionist model.</title>
<date>1999</date>
<booktitle>In Lecture Notes in Computer Science (IWANN 99),</booktitle>
<volume>1607</volume>
<pages>683--692</pages>
<location>Alicante,</location>
<marker>Casta˜no, Casacuberta, 1999</marker>
<rawString>Maria Asunci´on Casta˜no and Francisco Casacuberta. 1999. Text-to-text machine translation using the RECONTRA connectionist model. In Lecture Notes in Computer Science (IWANN 99), volume 1607, pages 683–692, Alicante, Spain.</rawString>
</citation>
<citation valid="true">
<authors>
<author>Maria Asunci´on Casta˜no</author>
<author>Francisco Casacuberta</author>
<author>Enrique Vidal</author>
</authors>
<title>Machine translation using neural networks and finite-state models.</title>
<date>1997</date>
<booktitle>In 7th International Conference on Theoretical and Methodological Issues in Machine Translation. TMI’97,</booktitle>
<pages>160--167</pages>
<location>Santa Fe, USA.</location>
<marker>Casta˜no, Casacuberta, Vidal, 1997</marker>
<rawString>Maria Asunci´on Casta˜no, Francisco Casacuberta, and Enrique Vidal. 1997. Machine translation using neural networks and finite-state models. In 7th International Conference on Theoretical and Methodological Issues in Machine Translation. TMI’97, pages 160–167, Santa Fe, USA.</rawString>
</citation>
<citation valid="true">
<authors>
<author>Stanley F Chen</author>
<author>Joshua Goodman</author>
</authors>
<title>An Empirical Study of Smoothing Techniques for Language Modeling.</title>
<date>1998</date>
<tech>Technical Report TR-10-98,</tech>
<institution>Computer Science Group, Harvard University,</institution>
<location>Cambridge, MA,</location>
<contexts>
<context position="23885" citStr="Chen and Goodman, 1998" startWordPosition="3926" endWordPosition="3929">le to evaluate the next layers. In the backward pass of backpropagation, the forward and backward recurrent layers are processed in decreasing and increasing time order, respectively. 5 Experiments 5.1 Setup All translation experiments are performed with the Jane toolkit (Vilar et al., 2010; Wuebker et al., 2012). The largest part of our experiments is carried out on the IWSLT 2013 German—*English shared translation task.2 The baseline system is trained on all available bilingual data, 4.3M sentence pairs in total, and uses a 4-gram LM with modified Kneser-Ney smoothing (Kneser and Ney, 1995; Chen and Goodman, 1998), trained with the SRILM toolkit (Stolcke, 2002). As additional 1In our implementation, the forward and backward layers converge into an intermediate identity layer, and the aggregate is weighted and fed to the next layer. 2http://www.iwslt2013.org data sources for the LM we selected parts of the Shuffled News and LDC English Gigaword corpora based on cross-entropy difference (Moore and Lewis, 2010), resulting in a total of 1.7 billion running words for LM training. The state-ofthe-art baseline is a standard phrase-based SMT system (Koehn et al., 2003) tuned with MERT (Och, 2003). It contains </context>
</contexts>
<marker>Chen, Goodman, 1998</marker>
<rawString>Stanley F. Chen and Joshua Goodman. 1998. An Empirical Study of Smoothing Techniques for Language Modeling. Technical Report TR-10-98, Computer Science Group, Harvard University, Cambridge, MA, August.</rawString>
</citation>
<citation valid="true">
<authors>
<author>Jacob Devlin</author>
<author>Rabih Zbib</author>
<author>Zhongqiang Huang</author>
<author>Thomas Lamar</author>
<author>Richard Schwartz</author>
<author>John Makhoul</author>
</authors>
<title>Fast and Robust Neural Network Joint Models for Statistical Machine Translation.</title>
<date>2014</date>
<booktitle>In 52nd Annual Meeting of the Association for Computational Linguistics,</booktitle>
<pages>page</pages>
<location>Baltimore, MD, USA,</location>
<contexts>
<context position="1513" citStr="Devlin et al., 2014" startWordPosition="206" endWordPosition="209"> already including recurrent neural language models on three tasks: IWSLT 2013 German—*English, BOLT Arabic—*English and Chinese—*English. We obtain gains up to 1.6% BLEU and 1.7% TER by rescoring 1000-best lists. 1 Introduction Neural network models have recently experienced unprecedented attention in research on statistical machine translation (SMT). Several groups have reported strong improvements over state-of-the-art baselines using feedforward neural network-based language models (Schwenk et al., 2006; Vaswani et al., 2013), as well as translation models (Le et al., 2012; Schwenk, 2012; Devlin et al., 2014). Different from the feedforward design, recurrent neural networks (RNNs) have the advantage of being able to take into account an unbounded history of previous observations. In theory, this enables them to model long-distance dependencies of arbitrary length. However, while previous work on translation modeling with recurrent neural networks shows its effectiveness on standard baselines, so far no notable gains have been presented on top of recurrent language models (Auli et al., 2013; Kalchbrenner and Blunsom, 2013; Hu et al., 2014). In this work, we present two novel approaches to recurrent</context>
<context position="6628" citStr="Devlin et al. (2014)" startWordPosition="1000" endWordPosition="1003">a feedforward network that predicts phrases of a fixed maximum length, such that all phrase words are predicted at once. The prediction is conditioned on the source phrase. Since our phrasebased model predicts one word at a time, it does not assume any phrase length. Moreover, our model’s predictions go beyond phrase boundaries and cover unbounded history and future contexts. Using neural networks during decoding requires tackling the costly output normalization step. Vaswani et al. (2013) avoid this step by training feedforward neural language models using noise contrastive estimation, while Devlin et al. (2014) augment the training objective function to produce approximately normalized scores directly. The latter work makes use of translation and joint models, and pre-computes the first hidden layer beforehand, resulting in large speedups. They report major improvements over strong baselines. The speedups achieved by both works allowed to integrate feedforward neural networks into the decoder. 3 LSTM Recurrent Neural Networks Our work is based on recurrent neural networks. In related fields like e. g. language modeling, this type of neural network has been shown to perform considerably better than s</context>
<context position="24702" citStr="Devlin et al. (2014)" startWordPosition="4060" endWordPosition="4063">nd fed to the next layer. 2http://www.iwslt2013.org data sources for the LM we selected parts of the Shuffled News and LDC English Gigaword corpora based on cross-entropy difference (Moore and Lewis, 2010), resulting in a total of 1.7 billion running words for LM training. The state-ofthe-art baseline is a standard phrase-based SMT system (Koehn et al., 2003) tuned with MERT (Och, 2003). It contains a hierarchical reordering model (Galley and Manning, 2008) and a 7- gram word cluster language model (Wuebker et al., 2013). Here, we also compare against a feedforward joint model as described by Devlin et al. (2014), with a source window of 11 words and a target history of three words, which we denote as BBN-JM. Instead of POS tags, we predict word classes trained with mkcls. We use a shortlist of size 16K and 1000 classes for the remaining words. All neural networks are trained on the TED portion of the data (138K segments) and are applied in a rescoring step on 1000-best lists. To confirm our results, we run additional experiments on the Arabic—*English and Chinese—*English tasks of the DARPA BOLT project. In both cases, the neural network models are added on top of our most competitive evaluation syst</context>
<context position="27499" citStr="Devlin et al., 2014" startWordPosition="4541" endWordPosition="4544">English task with different RNN models. T: translation, J: joint, B: bidirectional, P: phrase-based. guage model yet. Here, the delay parameter d from Equations 2 and 3 is set to zero. We observe that for all recurrent translation models, we achieve substantial improvements over the baseline on the test data, ranging from 0.9 BLEU up to 1.6 BLEU. These results are also consistent with the improvements in terms of TER, where we achieve reductions by 0.8 TER up to 1.8 TER. These numbers can be directly compared to the case of feedforward neural network-based translation modeling as proposed in (Devlin et al., 2014) which we include in the very last row of the table. Nearly all of our recurrent models outperform the feedforward approach, where the RNN model performing best on the dev data is better on test by 0.3 BLEU and 1.0 TER. Interestingly, for the recurrent word-based models, on the test data it can be seen that TMs perform better than JMs, even though TMs do not take advantage of the target side history words. However, exploiting this extra information does not always need to result in a better model, as the target side words are only derived from the given source side, which is available to both </context>
<context position="31352" citStr="Devlin et al., 2014" startWordPosition="5191" endWordPosition="5194">m model combination saturate quickly. Apart from the IWSLT track, we also analyze the performance of our translation models on the BOLT Chinese-*English and Arabic-*English translation tasks. Due to the large amount of training data, we concentrate on models of high performance in the IWSLT experiments. The results can be found in Tab. 4 and 5. In both cases, we see consistent improvements over the recurrent neural network language model baseline, improving the Arabic-*English system by 0.6 BLEU and 0.5 TER on test1. This can be compared to the rescoring results for the same task reported by (Devlin et al., 2014), where they achieved 0.3 BLEU, despite the fact that they used multiple references for scoring, whereas in our experiments we rely on a single reference only. The models are also able to improve the Chinese-*English system by 0.5 BLEU and 0.5 TER on test2. 5.3 Analysis To investigate whether bidirectional models benefit from future source information, we compare the single-best output of a system reranked with a unidirectional model to the output reranked with a bidirectional model. We choose the models to be translation models in both cases, as they predict target words independent of previo</context>
</contexts>
<marker>Devlin, Zbib, Huang, Lamar, Schwartz, Makhoul, 2014</marker>
<rawString>Jacob Devlin, Rabih Zbib, Zhongqiang Huang, Thomas Lamar, Richard Schwartz, and John Makhoul. 2014. Fast and Robust Neural Network Joint Models for Statistical Machine Translation. In 52nd Annual Meeting of the Association for Computational Linguistics, page to appear, Baltimore, MD, USA, June.</rawString>
</citation>
<citation valid="true">
<authors>
<author>Michel Galley</author>
<author>Christopher D Manning</author>
</authors>
<title>A simple and effective hierarchical phrase reordering model.</title>
<date>2008</date>
<booktitle>In Proceedings of the Conference on Empirical Methods in Natural Language Processing, EMNLP ’08,</booktitle>
<pages>848--856</pages>
<publisher>Association</publisher>
<institution>for Computational Linguistics.</institution>
<location>Stroudsburg, PA, USA.</location>
<contexts>
<context position="24543" citStr="Galley and Manning, 2008" startWordPosition="4031" endWordPosition="4034">(Stolcke, 2002). As additional 1In our implementation, the forward and backward layers converge into an intermediate identity layer, and the aggregate is weighted and fed to the next layer. 2http://www.iwslt2013.org data sources for the LM we selected parts of the Shuffled News and LDC English Gigaword corpora based on cross-entropy difference (Moore and Lewis, 2010), resulting in a total of 1.7 billion running words for LM training. The state-ofthe-art baseline is a standard phrase-based SMT system (Koehn et al., 2003) tuned with MERT (Och, 2003). It contains a hierarchical reordering model (Galley and Manning, 2008) and a 7- gram word cluster language model (Wuebker et al., 2013). Here, we also compare against a feedforward joint model as described by Devlin et al. (2014), with a source window of 11 words and a target history of three words, which we denote as BBN-JM. Instead of POS tags, we predict word classes trained with mkcls. We use a shortlist of size 16K and 1000 classes for the remaining words. All neural networks are trained on the TED portion of the data (138K segments) and are applied in a rescoring step on 1000-best lists. To confirm our results, we run additional experiments on the Arabic—*</context>
</contexts>
<marker>Galley, Manning, 2008</marker>
<rawString>Michel Galley and Christopher D. Manning. 2008. A simple and effective hierarchical phrase reordering model. In Proceedings of the Conference on Empirical Methods in Natural Language Processing, EMNLP ’08, pages 848–856, Stroudsburg, PA, USA. Association for Computational Linguistics.</rawString>
</citation>
<citation valid="true">
<authors>
<author>Felix A Gers</author>
<author>J¨urgen Schmidhuber</author>
<author>Fred Cummins</author>
</authors>
<title>Learning to forget: Continual prediction with LSTM.</title>
<date>2000</date>
<booktitle>Neural computation,</booktitle>
<pages>12--10</pages>
<contexts>
<context position="9230" citStr="Gers et al., 2000" startWordPosition="1396" endWordPosition="1399">unaligned tokens. tasks like parsing (Henderson, 2004) and spoken language understanding (Mesnil et al., 2013). Bidirectional long short-term memory (BLSTM) networks are BRNNs using LSTM hidden layers (Graves and Schmidhuber, 2005). This work introduces BLSTMs to the problem of machine translation, allowing powerful models that employ unlimited history and future information to make predictions. While the proposed models do not make any assumptions about the type of RNN used, all of our experiments make use of recurrent LSTM neural networks, where we include later LSTM extensions proposed in (Gers et al., 2000; Gers et al., 2003). The cross-entropy error criterion is used for training. Further details on LSTM neural networks can be found in (Graves and Schmidhuber, 2005; Sundermeyer et al., 2012). 4 Translation Modeling with RNNs In the following we describe our word- and phrase-based translation models in detail. We also show how bidirectional RNNs can enable such models to include full source information. 4.1 Resolving Alignment Ambiguities Our word-based recurrent models are only defined for one-to-one-aligned source-target sentence pairs. In this work, we always evaluate the model in the order </context>
</contexts>
<marker>Gers, Schmidhuber, Cummins, 2000</marker>
<rawString>Felix A. Gers, J¨urgen Schmidhuber, and Fred Cummins. 2000. Learning to forget: Continual prediction with LSTM. Neural computation, 12(10):2451–2471.</rawString>
</citation>
<citation valid="true">
<authors>
<author>Felix A Gers</author>
<author>Nicol N Schraudolph</author>
<author>J¨urgen Schmidhuber</author>
</authors>
<title>Learning precise timing with lstm recurrent networks.</title>
<date>2003</date>
<journal>The Journal of Machine Learning Research,</journal>
<pages>3--115</pages>
<contexts>
<context position="9250" citStr="Gers et al., 2003" startWordPosition="1400" endWordPosition="1403">asks like parsing (Henderson, 2004) and spoken language understanding (Mesnil et al., 2013). Bidirectional long short-term memory (BLSTM) networks are BRNNs using LSTM hidden layers (Graves and Schmidhuber, 2005). This work introduces BLSTMs to the problem of machine translation, allowing powerful models that employ unlimited history and future information to make predictions. While the proposed models do not make any assumptions about the type of RNN used, all of our experiments make use of recurrent LSTM neural networks, where we include later LSTM extensions proposed in (Gers et al., 2000; Gers et al., 2003). The cross-entropy error criterion is used for training. Further details on LSTM neural networks can be found in (Graves and Schmidhuber, 2005; Sundermeyer et al., 2012). 4 Translation Modeling with RNNs In the following we describe our word- and phrase-based translation models in detail. We also show how bidirectional RNNs can enable such models to include full source information. 4.1 Resolving Alignment Ambiguities Our word-based recurrent models are only defined for one-to-one-aligned source-target sentence pairs. In this work, we always evaluate the model in the order of the target senten</context>
</contexts>
<marker>Gers, Schraudolph, Schmidhuber, 2003</marker>
<rawString>Felix A. Gers, Nicol N. Schraudolph, and J¨urgen Schmidhuber. 2003. Learning precise timing with lstm recurrent networks. The Journal of Machine Learning Research, 3:115–143.</rawString>
</citation>
<citation valid="true">
<authors>
<author>Joshua Goodman</author>
</authors>
<title>Classes for fast maximum entropy training.</title>
<date>2001</date>
<booktitle>In Acoustics, Speech, and Signal Processing,</booktitle>
<volume>1</volume>
<pages>561--564</pages>
<publisher>IEEE.</publisher>
<contexts>
<context position="15626" citStr="Goodman, 2001" startWordPosition="2503" endWordPosition="2504"> including the dashed parts, a joint model is obtained. the word probability p(ei|c(ei), ei−1 1 , f1i) is obtained given the word class. This trick helps avoiding the otherwise computationally expensive normalization sum, which would be carried out over all words in the target vocabulary. In a classfactorized output layer where each word belongs to a single class, the normalization is carried out over all classes, whose number is typically much less than the vocabulary size. The other normalization sum needed to produce the word probability is limited to the words belonging to the same class (Goodman, 2001; Morin and Bengio, 2005). 4.3 Phrase-based RNN Models One of the conceptual disadvantages of wordbased modeling as introduced in the previous section is that there is a mismatch between training and testing conditions: During neural network training, the vocabulary has to be extended by additional c tokens, and a one-to-one alignment is used which does not reflect the situation in decoding. In phrase-based machine translation, more complex alignments in terms of multiple words on both the source and the target sides are used, which allow the decoder to make use of richer short-distance depend</context>
</contexts>
<marker>Goodman, 2001</marker>
<rawString>Joshua Goodman. 2001. Classes for fast maximum entropy training. In Acoustics, Speech, and Signal Processing, 2001. Proceedings.(ICASSP’01). 2001 IEEE International Conference on, volume 1, pages 561–564. IEEE.</rawString>
</citation>
<citation valid="true">
<authors>
<author>Alex Graves</author>
<author>J¨urgen Schmidhuber</author>
</authors>
<title>Framewise phoneme classification with bidirectional LSTM and other neural network architectures.</title>
<date>2005</date>
<journal>Neural Networks,</journal>
<volume>18</volume>
<issue>5</issue>
<contexts>
<context position="8844" citStr="Graves and Schmidhuber, 2005" startWordPosition="1333" endWordPosition="1336"> Bidirectional recurrent neural networks (BRNNs) were first proposed in (Schuster and Paliwal, 1997) and applied to speech recognition tasks. They have been since applied to different 15 . incredibly Eunaligned this know , example for , Surfers (b) One-to-one alignment Figure 1: Example sentence from the German→English IWSLT data. The one-to-one alignment is created by introducing caligned and cunaligned tokens. tasks like parsing (Henderson, 2004) and spoken language understanding (Mesnil et al., 2013). Bidirectional long short-term memory (BLSTM) networks are BRNNs using LSTM hidden layers (Graves and Schmidhuber, 2005). This work introduces BLSTMs to the problem of machine translation, allowing powerful models that employ unlimited history and future information to make predictions. While the proposed models do not make any assumptions about the type of RNN used, all of our experiments make use of recurrent LSTM neural networks, where we include later LSTM extensions proposed in (Gers et al., 2000; Gers et al., 2003). The cross-entropy error criterion is used for training. Further details on LSTM neural networks can be found in (Graves and Schmidhuber, 2005; Sundermeyer et al., 2012). 4 Translation Modeling</context>
</contexts>
<marker>Graves, Schmidhuber, 2005</marker>
<rawString>Alex Graves and J¨urgen Schmidhuber. 2005. Framewise phoneme classification with bidirectional LSTM and other neural network architectures. Neural Networks, 18(5):602–610.</rawString>
</citation>
<citation valid="true">
<authors>
<author>James Henderson</author>
</authors>
<title>Discriminative training of a neural network statistical parser.</title>
<date>2004</date>
<booktitle>In Proceedings of the 42nd Annual Meeting on Association for Computational Linguistics,</booktitle>
<pages>page</pages>
<contexts>
<context position="8667" citStr="Henderson, 2004" startWordPosition="1311" endWordPosition="1312">n. In particular, no modification of the training algorithm is necessary. The resulting architecture is referred to as long short-term memory (LSTM) neural network. Bidirectional recurrent neural networks (BRNNs) were first proposed in (Schuster and Paliwal, 1997) and applied to speech recognition tasks. They have been since applied to different 15 . incredibly Eunaligned this know , example for , Surfers (b) One-to-one alignment Figure 1: Example sentence from the German→English IWSLT data. The one-to-one alignment is created by introducing caligned and cunaligned tokens. tasks like parsing (Henderson, 2004) and spoken language understanding (Mesnil et al., 2013). Bidirectional long short-term memory (BLSTM) networks are BRNNs using LSTM hidden layers (Graves and Schmidhuber, 2005). This work introduces BLSTMs to the problem of machine translation, allowing powerful models that employ unlimited history and future information to make predictions. While the proposed models do not make any assumptions about the type of RNN used, all of our experiments make use of recurrent LSTM neural networks, where we include later LSTM extensions proposed in (Gers et al., 2000; Gers et al., 2003). The cross-entro</context>
</contexts>
<marker>Henderson, 2004</marker>
<rawString>James Henderson. 2004. Discriminative training of a neural network statistical parser. In Proceedings of the 42nd Annual Meeting on Association for Computational Linguistics, page 95. Association for Computational Linguistics.</rawString>
</citation>
<citation valid="true">
<authors>
<author>Sepp Hochreiter</author>
<author>J¨urgen Schmidhuber</author>
</authors>
<title>Long short-term memory.</title>
<date>1997</date>
<booktitle>Neural computation,</booktitle>
<pages>9--8</pages>
<contexts>
<context position="7902" citStr="Hochreiter and Schmidhuber, 1997" startWordPosition="1196" endWordPosition="1200">v et al., 2011; Arisoy et al., 2012; Sundermeyer et al., 2013; Liu et al., 2014). Most commonly, recurrent neural networks are trained with stochastic gradient descent (SGD), where the gradient of the training criterion is computed with the backpropagation through time algorithm (Rumelhart et al., 1986; Werbos, 1990; Williams and Zipser, 1995). However, the combination of RNN networks with conventional backpropagation training leads to conceptual difficulties which are known as the vanishing (or exploding) gradient problem, described e. g. in (Bengio et al., 1994). To remedy this problem, in (Hochreiter and Schmidhuber, 1997) it was suggested to modify the architecture of a standard RNN in such a way that vanishing and exploding gradients are avoided during backpropagation. In particular, no modification of the training algorithm is necessary. The resulting architecture is referred to as long short-term memory (LSTM) neural network. Bidirectional recurrent neural networks (BRNNs) were first proposed in (Schuster and Paliwal, 1997) and applied to speech recognition tasks. They have been since applied to different 15 . incredibly Eunaligned this know , example for , Surfers (b) One-to-one alignment Figure 1: Example</context>
</contexts>
<marker>Hochreiter, Schmidhuber, 1997</marker>
<rawString>Sepp Hochreiter and J¨urgen Schmidhuber. 1997. Long short-term memory. Neural computation, 9(8):1735–1780.</rawString>
</citation>
<citation valid="true">
<authors>
<author>Yuening Hu</author>
<author>Michael Auli</author>
<author>Qin Gao</author>
<author>Jianfeng Gao</author>
</authors>
<title>Minimum translation modeling with recurrent neural networks.</title>
<date>2014</date>
<booktitle>In Proceedings of the 14th Conference of the European Chapter of the Association for Computational Linguistics,</booktitle>
<pages>pages</pages>
<publisher>Association</publisher>
<institution>for Computational Linguistics.</institution>
<location>Gothenburg, Sweden,</location>
<contexts>
<context position="2053" citStr="Hu et al., 2014" startWordPosition="291" endWordPosition="294"> as translation models (Le et al., 2012; Schwenk, 2012; Devlin et al., 2014). Different from the feedforward design, recurrent neural networks (RNNs) have the advantage of being able to take into account an unbounded history of previous observations. In theory, this enables them to model long-distance dependencies of arbitrary length. However, while previous work on translation modeling with recurrent neural networks shows its effectiveness on standard baselines, so far no notable gains have been presented on top of recurrent language models (Auli et al., 2013; Kalchbrenner and Blunsom, 2013; Hu et al., 2014). In this work, we present two novel approaches to recurrent neural translation modeling: wordbased and phrase-based. The word-based approach assumes one-to-one aligned source and target sentences. We evaluate different ways of resolving alignment ambiguities to obtain such alignments. The phrase-based RNN approach is more closely tied to the underlying translation paradigm. It models actual phrasal translation probabilities while avoiding sparsity issues by using single words as input and output units. Furthermore, in addition to the unidirectional formulation, we are the first to propose a b</context>
<context position="5409" citStr="Hu et al., 2014" startWordPosition="807" endWordPosition="810">idden layer of a recurrent language model. Rescoring results indicate no improvements over the state of the art. Auli et al. (2013) also include source sentence representations built either using Latent Semantic Analysis or by concatenating word embeddings. This approach produced no notable gain over systems using a recurrent language model. On the other hand, our proposed bidirectional models include the full source sentence relying on recurrency, yielding improvements over competitive baselines already including a recurrent language model. RNNs were also used with minimum translation units (Hu et al., 2014), which are phrase pairs undergoing certain constraints. At the input layer, each of the source and target phrases are modeled as a bag of words, while the output phrase is predicted word-by-word assuming conditional independence. The approach seeks to alleviate data sparsity problems that would arise if phrases were to be uniquely distinguished. Our proposed phrase-based models maintain word order within phrases, but the phrases are processed in a wordpair manner, while the phrase boundaries remain implicitly encoded in the way the words are presented to the network. Schwenk (2012) proposed a</context>
</contexts>
<marker>Hu, Auli, Gao, Gao, 2014</marker>
<rawString>Yuening Hu, Michael Auli, Qin Gao, and Jianfeng Gao. 2014. Minimum translation modeling with recurrent neural networks. In Proceedings of the 14th Conference of the European Chapter of the Association for Computational Linguistics, pages 20–29, Gothenburg, Sweden, April. Association for Computational Linguistics.</rawString>
</citation>
<citation valid="true">
<authors>
<author>Matthias Huck</author>
<author>Joern Wuebker</author>
<author>Felix Rietig</author>
<author>Hermann Ney</author>
</authors>
<title>A phrase orientation model for hierarchical machine translation.</title>
<date>2013</date>
<booktitle>In ACL 2013 Eighth Workshop on Statistical Machine Translation,</booktitle>
<pages>452--463</pages>
<location>Sofia, Bulgaria,</location>
<contexts>
<context position="25477" citStr="Huck et al., 2013" startWordPosition="4192" endWordPosition="4195">mkcls. We use a shortlist of size 16K and 1000 classes for the remaining words. All neural networks are trained on the TED portion of the data (138K segments) and are applied in a rescoring step on 1000-best lists. To confirm our results, we run additional experiments on the Arabic—*English and Chinese—*English tasks of the DARPA BOLT project. In both cases, the neural network models are added on top of our most competitive evaluation system. On Chinese—*English, we use a hierarchical phrase-based system trained on 3.7M segments with 22 dense features, including an advanced orientation model (Huck et al., 2013). For the neural network training, we selected a subset of 9M running words. The Arabic—*English system is a standard phrase-based decoder trained on 6.6M segments, using 17 dense features. The neural network training was performed using a selection amounting to 15.5M running words. For both tasks we apply the neural networks by rescoring 1000-best lists and evaluate results on two data sets from the ’discussion forum’ domain, test1 and test2. The sizes of the data sets for the Arabic—*English system are: 1219 (dev), 1510 (test1), and 1137 (test2) segments, and for the Chinese—*English system </context>
</contexts>
<marker>Huck, Wuebker, Rietig, Ney, 2013</marker>
<rawString>Matthias Huck, Joern Wuebker, Felix Rietig, and Hermann Ney. 2013. A phrase orientation model for hierarchical machine translation. In ACL 2013 Eighth Workshop on Statistical Machine Translation, pages 452–463, Sofia, Bulgaria, August.</rawString>
</citation>
<citation valid="true">
<authors>
<author>Nal Kalchbrenner</author>
<author>Phil Blunsom</author>
</authors>
<title>Recurrent continuous translation models.</title>
<date>2013</date>
<booktitle>In Proceedings of the 2013 Conference on Empirical Methods in Natural Language Processing,</booktitle>
<pages>1700--1709</pages>
<publisher>Association</publisher>
<institution>for Computational Linguistics.</institution>
<location>Seattle, Washington, USA,</location>
<contexts>
<context position="2035" citStr="Kalchbrenner and Blunsom, 2013" startWordPosition="287" endWordPosition="290">; Vaswani et al., 2013), as well as translation models (Le et al., 2012; Schwenk, 2012; Devlin et al., 2014). Different from the feedforward design, recurrent neural networks (RNNs) have the advantage of being able to take into account an unbounded history of previous observations. In theory, this enables them to model long-distance dependencies of arbitrary length. However, while previous work on translation modeling with recurrent neural networks shows its effectiveness on standard baselines, so far no notable gains have been presented on top of recurrent language models (Auli et al., 2013; Kalchbrenner and Blunsom, 2013; Hu et al., 2014). In this work, we present two novel approaches to recurrent neural translation modeling: wordbased and phrase-based. The word-based approach assumes one-to-one aligned source and target sentences. We evaluate different ways of resolving alignment ambiguities to obtain such alignments. The phrase-based RNN approach is more closely tied to the underlying translation paradigm. It models actual phrasal translation probabilities while avoiding sparsity issues by using single words as input and output units. Furthermore, in addition to the unidirectional formulation, we are the fi</context>
<context position="4603" citStr="Kalchbrenner and Blunsom (2013)" startWordPosition="682" endWordPosition="686">al networks in machine translation (MT) are presented in (Casta˜no et al., 1997; Casta˜no and Casacuberta, 1997; Casta˜no and Casacuberta, 1999), where they were used for example-based MT. Recently, Le et al. (2012) presented translation models using an output layer with classes and a shortlist for rescoring using feedforward networks. They compare between word-factored and tuple-factored n-gram models, obtaining their best results using the word-factored approach, which is less amenable to data sparsity issues. Both of our word-based and phrase-based models eventually work on the word level. Kalchbrenner and Blunsom (2013) use recurrent neural networks with full source sentence representations. The continuous representations are obtained by applying a sequence of convolutions, and the result is fed into the hidden layer of a recurrent language model. Rescoring results indicate no improvements over the state of the art. Auli et al. (2013) also include source sentence representations built either using Latent Semantic Analysis or by concatenating word embeddings. This approach produced no notable gain over systems using a recurrent language model. On the other hand, our proposed bidirectional models include the f</context>
</contexts>
<marker>Kalchbrenner, Blunsom, 2013</marker>
<rawString>Nal Kalchbrenner and Phil Blunsom. 2013. Recurrent continuous translation models. In Proceedings of the 2013 Conference on Empirical Methods in Natural Language Processing, pages 1700–1709, Seattle, Washington, USA, October. Association for Computational Linguistics.</rawString>
</citation>
<citation valid="true">
<authors>
<author>Reinhard Kneser</author>
<author>Hermann Ney</author>
</authors>
<title>Improved backing-off for M-gram language modeling.</title>
<date>1995</date>
<booktitle>In Proceedings of the International Conference on Acoustics, Speech, and Signal Processingw,</booktitle>
<volume>1</volume>
<pages>181--184</pages>
<contexts>
<context position="23860" citStr="Kneser and Ney, 1995" startWordPosition="3922" endWordPosition="3925">uence, before being able to evaluate the next layers. In the backward pass of backpropagation, the forward and backward recurrent layers are processed in decreasing and increasing time order, respectively. 5 Experiments 5.1 Setup All translation experiments are performed with the Jane toolkit (Vilar et al., 2010; Wuebker et al., 2012). The largest part of our experiments is carried out on the IWSLT 2013 German—*English shared translation task.2 The baseline system is trained on all available bilingual data, 4.3M sentence pairs in total, and uses a 4-gram LM with modified Kneser-Ney smoothing (Kneser and Ney, 1995; Chen and Goodman, 1998), trained with the SRILM toolkit (Stolcke, 2002). As additional 1In our implementation, the forward and backward layers converge into an intermediate identity layer, and the aggregate is weighted and fed to the next layer. 2http://www.iwslt2013.org data sources for the LM we selected parts of the Shuffled News and LDC English Gigaword corpora based on cross-entropy difference (Moore and Lewis, 2010), resulting in a total of 1.7 billion running words for LM training. The state-ofthe-art baseline is a standard phrase-based SMT system (Koehn et al., 2003) tuned with MERT </context>
</contexts>
<marker>Kneser, Ney, 1995</marker>
<rawString>Reinhard Kneser and Hermann Ney. 1995. Improved backing-off for M-gram language modeling. In Proceedings of the International Conference on Acoustics, Speech, and Signal Processingw, volume 1, pages 181–184, May.</rawString>
</citation>
<citation valid="true">
<authors>
<author>Philipp Koehn</author>
<author>Franz Josef Och</author>
<author>Daniel Marcu</author>
</authors>
<title>Statistical Phrase-Based Translation.</title>
<date>2003</date>
<booktitle>In Proceedings of the 2003 Meeting of the North American chapter of the Association for Computational Linguistics (NAACL-03),</booktitle>
<pages>127--133</pages>
<location>Edmonton, Alberta.</location>
<contexts>
<context position="24443" citStr="Koehn et al., 2003" startWordPosition="4015" endWordPosition="4018">-Ney smoothing (Kneser and Ney, 1995; Chen and Goodman, 1998), trained with the SRILM toolkit (Stolcke, 2002). As additional 1In our implementation, the forward and backward layers converge into an intermediate identity layer, and the aggregate is weighted and fed to the next layer. 2http://www.iwslt2013.org data sources for the LM we selected parts of the Shuffled News and LDC English Gigaword corpora based on cross-entropy difference (Moore and Lewis, 2010), resulting in a total of 1.7 billion running words for LM training. The state-ofthe-art baseline is a standard phrase-based SMT system (Koehn et al., 2003) tuned with MERT (Och, 2003). It contains a hierarchical reordering model (Galley and Manning, 2008) and a 7- gram word cluster language model (Wuebker et al., 2013). Here, we also compare against a feedforward joint model as described by Devlin et al. (2014), with a source window of 11 words and a target history of three words, which we denote as BBN-JM. Instead of POS tags, we predict word classes trained with mkcls. We use a shortlist of size 16K and 1000 classes for the remaining words. All neural networks are trained on the TED portion of the data (138K segments) and are applied in a resc</context>
</contexts>
<marker>Koehn, Och, Marcu, 2003</marker>
<rawString>Philipp Koehn, Franz Josef Och, and Daniel Marcu. 2003. Statistical Phrase-Based Translation. In Proceedings of the 2003 Meeting of the North American chapter of the Association for Computational Linguistics (NAACL-03), pages 127–133, Edmonton, Alberta.</rawString>
</citation>
<citation valid="true">
<authors>
<author>Hai Son Le</author>
<author>Alexandre Allauzen</author>
<author>Franc¸ois Yvon</author>
</authors>
<title>Continuous Space Translation Models with Neural Networks.</title>
<date>2012</date>
<booktitle>In Conference of the North American Chapter of the Association for Computational Linguistics: Human Language Technologies,</booktitle>
<pages>39--48</pages>
<location>Montreal, Canada,</location>
<contexts>
<context position="1476" citStr="Le et al., 2012" startWordPosition="200" endWordPosition="203">le of improving strong baselines already including recurrent neural language models on three tasks: IWSLT 2013 German—*English, BOLT Arabic—*English and Chinese—*English. We obtain gains up to 1.6% BLEU and 1.7% TER by rescoring 1000-best lists. 1 Introduction Neural network models have recently experienced unprecedented attention in research on statistical machine translation (SMT). Several groups have reported strong improvements over state-of-the-art baselines using feedforward neural network-based language models (Schwenk et al., 2006; Vaswani et al., 2013), as well as translation models (Le et al., 2012; Schwenk, 2012; Devlin et al., 2014). Different from the feedforward design, recurrent neural networks (RNNs) have the advantage of being able to take into account an unbounded history of previous observations. In theory, this enables them to model long-distance dependencies of arbitrary length. However, while previous work on translation modeling with recurrent neural networks shows its effectiveness on standard baselines, so far no notable gains have been presented on top of recurrent language models (Auli et al., 2013; Kalchbrenner and Blunsom, 2013; Hu et al., 2014). In this work, we pres</context>
<context position="4187" citStr="Le et al. (2012)" startWordPosition="623" endWordPosition="626">e on Empirical Methods in Natural Language Processing (EMNLP), pages 14–25, October 25-29, 2014, Doha, Qatar. c�2014 Association for Computational Linguistics 2 Related Work In this Section we contrast previous work to ours, where we design RNNs to model bilingual dependencies, which are applied to rerank n-best lists after decoding. To the best of our knowledge, the earliest attempts to apply neural networks in machine translation (MT) are presented in (Casta˜no et al., 1997; Casta˜no and Casacuberta, 1997; Casta˜no and Casacuberta, 1999), where they were used for example-based MT. Recently, Le et al. (2012) presented translation models using an output layer with classes and a shortlist for rescoring using feedforward networks. They compare between word-factored and tuple-factored n-gram models, obtaining their best results using the word-factored approach, which is less amenable to data sparsity issues. Both of our word-based and phrase-based models eventually work on the word level. Kalchbrenner and Blunsom (2013) use recurrent neural networks with full source sentence representations. The continuous representations are obtained by applying a sequence of convolutions, and the result is fed into</context>
<context position="16929" citStr="Le et al., 2012" startWordPosition="2716" endWordPosition="2719">ve, it seems interesting to standardize the alignments used in decoding, and in training the neural network. However, it is difficult to use the phrases themselves as the vocabulary of the RNN. Usually, the huge number of potential phrases in comparison to the relatively small amount of training data makes the learning of continuous phrase representations difficult Surfers , for example , know this incredibly Surfer zum Beispiel kennen das zur Genüge Figure 4: Example phrase alignment for a sentence from the IWSLT training data. due to data sparsity. This is confirmed by results presented in (Le et al., 2012), which show that a word-factored translation model outperforms the phrase-factored version. Therefore, in this work we continue relying on source and target word vocabularies for building our phrase representations. However, we no longer use a direct correspondence between a source and a target word, as enforced in our word-based models. Fig. 4 shows an example phrase alignment, where a sequence of source words ˜fi is directly mapped to a sequence of target words ˜ei for 1 ≤ i ≤ ˜I. By ˜I, we denote the number of phrases in the alignment. We decompose the target sentence posterior probability</context>
</contexts>
<marker>Le, Allauzen, Yvon, 2012</marker>
<rawString>Hai Son Le, Alexandre Allauzen, and Franc¸ois Yvon. 2012. Continuous Space Translation Models with Neural Networks. In Conference of the North American Chapter of the Association for Computational Linguistics: Human Language Technologies, pages 39–48, Montreal, Canada, June.</rawString>
</citation>
<citation valid="true">
<authors>
<author>Xunying Liu</author>
<author>Yongqiang Wang</author>
<author>Xie Chen</author>
<author>Mark J F Gales</author>
<author>Phil C Woodland</author>
</authors>
<title>Efficient lattice rescoring using recurrent neural network language models.</title>
<date>2014</date>
<booktitle>In Acoustics, Speech and Signal Processing (ICASSP), 2014 IEEE International Conference on,</booktitle>
<pages>4941--4945</pages>
<publisher>IEEE.</publisher>
<contexts>
<context position="7349" citStr="Liu et al., 2014" startWordPosition="1112" endWordPosition="1115"> work makes use of translation and joint models, and pre-computes the first hidden layer beforehand, resulting in large speedups. They report major improvements over strong baselines. The speedups achieved by both works allowed to integrate feedforward neural networks into the decoder. 3 LSTM Recurrent Neural Networks Our work is based on recurrent neural networks. In related fields like e. g. language modeling, this type of neural network has been shown to perform considerably better than standard feedforward architectures (Mikolov et al., 2011; Arisoy et al., 2012; Sundermeyer et al., 2013; Liu et al., 2014). Most commonly, recurrent neural networks are trained with stochastic gradient descent (SGD), where the gradient of the training criterion is computed with the backpropagation through time algorithm (Rumelhart et al., 1986; Werbos, 1990; Williams and Zipser, 1995). However, the combination of RNN networks with conventional backpropagation training leads to conceptual difficulties which are known as the vanishing (or exploding) gradient problem, described e. g. in (Bengio et al., 1994). To remedy this problem, in (Hochreiter and Schmidhuber, 1997) it was suggested to modify the architecture of</context>
</contexts>
<marker>Liu, Wang, Chen, Gales, Woodland, 2014</marker>
<rawString>Xunying Liu, Yongqiang Wang, Xie Chen, Mark J. F. Gales, and Phil C. Woodland. 2014. Efficient lattice rescoring using recurrent neural network language models. In Acoustics, Speech and Signal Processing (ICASSP), 2014 IEEE International Conference on, pages 4941–4945. IEEE.</rawString>
</citation>
<citation valid="true">
<authors>
<author>Gr´egoire Mesnil</author>
<author>Xiaodong He</author>
<author>Li Deng</author>
<author>Yoshua Bengio</author>
</authors>
<title>Investigation of recurrent-neuralnetwork architectures and learning methods for spoken language understanding.</title>
<date>2013</date>
<booktitle>In Interspeech,</booktitle>
<pages>3771--3775</pages>
<contexts>
<context position="8723" citStr="Mesnil et al., 2013" startWordPosition="1317" endWordPosition="1320">gorithm is necessary. The resulting architecture is referred to as long short-term memory (LSTM) neural network. Bidirectional recurrent neural networks (BRNNs) were first proposed in (Schuster and Paliwal, 1997) and applied to speech recognition tasks. They have been since applied to different 15 . incredibly Eunaligned this know , example for , Surfers (b) One-to-one alignment Figure 1: Example sentence from the German→English IWSLT data. The one-to-one alignment is created by introducing caligned and cunaligned tokens. tasks like parsing (Henderson, 2004) and spoken language understanding (Mesnil et al., 2013). Bidirectional long short-term memory (BLSTM) networks are BRNNs using LSTM hidden layers (Graves and Schmidhuber, 2005). This work introduces BLSTMs to the problem of machine translation, allowing powerful models that employ unlimited history and future information to make predictions. While the proposed models do not make any assumptions about the type of RNN used, all of our experiments make use of recurrent LSTM neural networks, where we include later LSTM extensions proposed in (Gers et al., 2000; Gers et al., 2003). The cross-entropy error criterion is used for training. Further details</context>
</contexts>
<marker>Mesnil, He, Deng, Bengio, 2013</marker>
<rawString>Gr´egoire Mesnil, Xiaodong He, Li Deng, and Yoshua Bengio. 2013. Investigation of recurrent-neuralnetwork architectures and learning methods for spoken language understanding. In Interspeech, pages 3771–3775.</rawString>
</citation>
<citation valid="true">
<authors>
<author>Tomas Mikolov</author>
<author>Stefan Kombrink</author>
<author>Lukas Burget</author>
<author>JH Cernocky</author>
<author>Sanjeev Khudanpur</author>
</authors>
<title>Extensions of recurrent neural network language model.</title>
<date>2011</date>
<booktitle>In Acoustics, Speech and Signal Processing (ICASSP), 2011 IEEE International Conference on,</booktitle>
<pages>5528--5531</pages>
<publisher>IEEE.</publisher>
<contexts>
<context position="7283" citStr="Mikolov et al., 2011" startWordPosition="1100" endWordPosition="1103">ction to produce approximately normalized scores directly. The latter work makes use of translation and joint models, and pre-computes the first hidden layer beforehand, resulting in large speedups. They report major improvements over strong baselines. The speedups achieved by both works allowed to integrate feedforward neural networks into the decoder. 3 LSTM Recurrent Neural Networks Our work is based on recurrent neural networks. In related fields like e. g. language modeling, this type of neural network has been shown to perform considerably better than standard feedforward architectures (Mikolov et al., 2011; Arisoy et al., 2012; Sundermeyer et al., 2013; Liu et al., 2014). Most commonly, recurrent neural networks are trained with stochastic gradient descent (SGD), where the gradient of the training criterion is computed with the backpropagation through time algorithm (Rumelhart et al., 1986; Werbos, 1990; Williams and Zipser, 1995). However, the combination of RNN networks with conventional backpropagation training leads to conceptual difficulties which are known as the vanishing (or exploding) gradient problem, described e. g. in (Bengio et al., 1994). To remedy this problem, in (Hochreiter and</context>
</contexts>
<marker>Mikolov, Kombrink, Burget, Cernocky, Khudanpur, 2011</marker>
<rawString>Tomas Mikolov, Stefan Kombrink, Lukas Burget, JH Cernocky, and Sanjeev Khudanpur. 2011. Extensions of recurrent neural network language model. In Acoustics, Speech and Signal Processing (ICASSP), 2011 IEEE International Conference on, pages 5528–5531. IEEE.</rawString>
</citation>
<citation valid="true">
<authors>
<author>Robert C Moore</author>
<author>William Lewis</author>
</authors>
<title>Intelligent Selection of Language Model Training Data.</title>
<date>2010</date>
<booktitle>In ACL (Short Papers),</booktitle>
<pages>220--224</pages>
<location>Uppsala, Sweden,</location>
<contexts>
<context position="24287" citStr="Moore and Lewis, 2010" startWordPosition="3988" endWordPosition="3991">ared translation task.2 The baseline system is trained on all available bilingual data, 4.3M sentence pairs in total, and uses a 4-gram LM with modified Kneser-Ney smoothing (Kneser and Ney, 1995; Chen and Goodman, 1998), trained with the SRILM toolkit (Stolcke, 2002). As additional 1In our implementation, the forward and backward layers converge into an intermediate identity layer, and the aggregate is weighted and fed to the next layer. 2http://www.iwslt2013.org data sources for the LM we selected parts of the Shuffled News and LDC English Gigaword corpora based on cross-entropy difference (Moore and Lewis, 2010), resulting in a total of 1.7 billion running words for LM training. The state-ofthe-art baseline is a standard phrase-based SMT system (Koehn et al., 2003) tuned with MERT (Och, 2003). It contains a hierarchical reordering model (Galley and Manning, 2008) and a 7- gram word cluster language model (Wuebker et al., 2013). Here, we also compare against a feedforward joint model as described by Devlin et al. (2014), with a source window of 11 words and a target history of three words, which we denote as BBN-JM. Instead of POS tags, we predict word classes trained with mkcls. We use a shortlist of</context>
</contexts>
<marker>Moore, Lewis, 2010</marker>
<rawString>Robert C. Moore and William Lewis. 2010. Intelligent Selection of Language Model Training Data. In ACL (Short Papers), pages 220–224, Uppsala, Sweden, July.</rawString>
</citation>
<citation valid="true">
<authors>
<author>Frederic Morin</author>
<author>Yoshua Bengio</author>
</authors>
<title>Hierarchical probabilistic neural network language model.</title>
<date>2005</date>
<booktitle>In Proceedings of the international workshop on artificial intelligence and statistics,</booktitle>
<pages>246--252</pages>
<contexts>
<context position="15651" citStr="Morin and Bengio, 2005" startWordPosition="2505" endWordPosition="2508">dashed parts, a joint model is obtained. the word probability p(ei|c(ei), ei−1 1 , f1i) is obtained given the word class. This trick helps avoiding the otherwise computationally expensive normalization sum, which would be carried out over all words in the target vocabulary. In a classfactorized output layer where each word belongs to a single class, the normalization is carried out over all classes, whose number is typically much less than the vocabulary size. The other normalization sum needed to produce the word probability is limited to the words belonging to the same class (Goodman, 2001; Morin and Bengio, 2005). 4.3 Phrase-based RNN Models One of the conceptual disadvantages of wordbased modeling as introduced in the previous section is that there is a mismatch between training and testing conditions: During neural network training, the vocabulary has to be extended by additional c tokens, and a one-to-one alignment is used which does not reflect the situation in decoding. In phrase-based machine translation, more complex alignments in terms of multiple words on both the source and the target sides are used, which allow the decoder to make use of richer short-distance dependencies and are crucial fo</context>
</contexts>
<marker>Morin, Bengio, 2005</marker>
<rawString>Frederic Morin and Yoshua Bengio. 2005. Hierarchical probabilistic neural network language model. In Proceedings of the international workshop on artificial intelligence and statistics, pages 246–252.</rawString>
</citation>
<citation valid="true">
<authors>
<author>Franz Josef Och</author>
</authors>
<title>Minimum Error Rate Training in Statistical Machine Translation.</title>
<date>2003</date>
<booktitle>In Proc. of the 41th Annual Meeting of the Association for Computational Linguistics (ACL),</booktitle>
<pages>160--167</pages>
<location>Sapporo, Japan,</location>
<contexts>
<context position="24471" citStr="Och, 2003" startWordPosition="4022" endWordPosition="4023"> Chen and Goodman, 1998), trained with the SRILM toolkit (Stolcke, 2002). As additional 1In our implementation, the forward and backward layers converge into an intermediate identity layer, and the aggregate is weighted and fed to the next layer. 2http://www.iwslt2013.org data sources for the LM we selected parts of the Shuffled News and LDC English Gigaword corpora based on cross-entropy difference (Moore and Lewis, 2010), resulting in a total of 1.7 billion running words for LM training. The state-ofthe-art baseline is a standard phrase-based SMT system (Koehn et al., 2003) tuned with MERT (Och, 2003). It contains a hierarchical reordering model (Galley and Manning, 2008) and a 7- gram word cluster language model (Wuebker et al., 2013). Here, we also compare against a feedforward joint model as described by Devlin et al. (2014), with a source window of 11 words and a target history of three words, which we denote as BBN-JM. Instead of POS tags, we predict word classes trained with mkcls. We use a shortlist of size 16K and 1000 classes for the remaining words. All neural networks are trained on the TED portion of the data (138K segments) and are applied in a rescoring step on 1000-best list</context>
</contexts>
<marker>Och, 2003</marker>
<rawString>Franz Josef Och. 2003. Minimum Error Rate Training in Statistical Machine Translation. In Proc. of the 41th Annual Meeting of the Association for Computational Linguistics (ACL), pages 160–167, Sapporo, Japan, July.</rawString>
</citation>
<citation valid="true">
<authors>
<author>Kishore Papineni</author>
<author>Salim Roukos</author>
<author>Todd Ward</author>
<author>WeiJing Zhu</author>
</authors>
<title>Bleu: a Method for Automatic Evaluation of Machine Translation.</title>
<date>2002</date>
<booktitle>In Proceedings of the 41st Annual Meeting of the Association for Computational Linguistics,</booktitle>
<pages>311--318</pages>
<location>Philadelphia, Pennsylvania, USA,</location>
<contexts>
<context position="26212" citStr="Papineni et al., 2002" startWordPosition="4307" endWordPosition="4310">dard phrase-based decoder trained on 6.6M segments, using 17 dense features. The neural network training was performed using a selection amounting to 15.5M running words. For both tasks we apply the neural networks by rescoring 1000-best lists and evaluate results on two data sets from the ’discussion forum’ domain, test1 and test2. The sizes of the data sets for the Arabic—*English system are: 1219 (dev), 1510 (test1), and 1137 (test2) segments, and for the Chinese—*English system are: 5074 (dev), 1844 (test1), and 1124 (test2) segments. All results are measured in case-insensitive BLEU [%] (Papineni et al., 2002) and TER [%] (Snover et al., 2006) on a single reference. 5.2 Results Our results on the IWSLT German—*English task are summarized in Tab. 2. At this point, we do not include a recurrent neural network lanfi ei−1 class layer (−) (+) (+) 20 dev test BLEU TER BLEU TER baseline 33.5 45.8 30.9 48.4 TM 34.6 44.5 32.0 47.1 JM 34.7 44.7 31.8 47.4 BTM 34.7 44.9 32.3 47.0 BTM (deep) 34.8 44.3 32.5 46.7 BJM 34.7 44.5 32.1 47.0 BJM (deep) 34.9 44.1 32.2 46.6 PTM 34.3 44.9 32.1 47.5 PJM 34.3 45.0 32.0 47.5 PJM (10-best) 34.4 44.8 32.0 47.3 PJM (deep) 34.6 44.7 32.0 47.6 PBJM (deep) 34.8 44.9 31.9 47.5 BBN</context>
</contexts>
<marker>Papineni, Roukos, Ward, Zhu, 2002</marker>
<rawString>Kishore Papineni, Salim Roukos, Todd Ward, and WeiJing Zhu. 2002. Bleu: a Method for Automatic Evaluation of Machine Translation. In Proceedings of the 41st Annual Meeting of the Association for Computational Linguistics, pages 311–318, Philadelphia, Pennsylvania, USA, July.</rawString>
</citation>
<citation valid="true">
<authors>
<author>David E Rumelhart</author>
<author>Geoffrey E Hinton</author>
<author>Ronald J Williams</author>
</authors>
<title>Learning Internal Representations by Error Propagation. In:</title>
<date>1986</date>
<booktitle>and The PDP Research Group: “Parallel Distributed Processing,</booktitle>
<volume>1</volume>
<pages>Foundations”.</pages>
<publisher>The MIT Press.</publisher>
<contexts>
<context position="7572" citStr="Rumelhart et al., 1986" startWordPosition="1145" endWordPosition="1148">s allowed to integrate feedforward neural networks into the decoder. 3 LSTM Recurrent Neural Networks Our work is based on recurrent neural networks. In related fields like e. g. language modeling, this type of neural network has been shown to perform considerably better than standard feedforward architectures (Mikolov et al., 2011; Arisoy et al., 2012; Sundermeyer et al., 2013; Liu et al., 2014). Most commonly, recurrent neural networks are trained with stochastic gradient descent (SGD), where the gradient of the training criterion is computed with the backpropagation through time algorithm (Rumelhart et al., 1986; Werbos, 1990; Williams and Zipser, 1995). However, the combination of RNN networks with conventional backpropagation training leads to conceptual difficulties which are known as the vanishing (or exploding) gradient problem, described e. g. in (Bengio et al., 1994). To remedy this problem, in (Hochreiter and Schmidhuber, 1997) it was suggested to modify the architecture of a standard RNN in such a way that vanishing and exploding gradients are avoided during backpropagation. In particular, no modification of the training algorithm is necessary. The resulting architecture is referred to as lo</context>
</contexts>
<marker>Rumelhart, Hinton, Williams, 1986</marker>
<rawString>David E. Rumelhart, Geoffrey E. Hinton, and Ronald J. Williams. 1986. Learning Internal Representations by Error Propagation. In: J. L. McClelland, D. E. Rumelhart, and The PDP Research Group: “Parallel Distributed Processing, Volume 1: Foundations”. The MIT Press.</rawString>
</citation>
<citation valid="true">
<authors>
<author>Mike Schuster</author>
<author>Kuldip K Paliwal</author>
</authors>
<title>Bidirectional recurrent neural networks.</title>
<date>1997</date>
<journal>IEEE Transactions on Signal Processing,</journal>
<volume>45</volume>
<issue>11</issue>
<contexts>
<context position="8315" citStr="Schuster and Paliwal, 1997" startWordPosition="1257" endWordPosition="1260">on training leads to conceptual difficulties which are known as the vanishing (or exploding) gradient problem, described e. g. in (Bengio et al., 1994). To remedy this problem, in (Hochreiter and Schmidhuber, 1997) it was suggested to modify the architecture of a standard RNN in such a way that vanishing and exploding gradients are avoided during backpropagation. In particular, no modification of the training algorithm is necessary. The resulting architecture is referred to as long short-term memory (LSTM) neural network. Bidirectional recurrent neural networks (BRNNs) were first proposed in (Schuster and Paliwal, 1997) and applied to speech recognition tasks. They have been since applied to different 15 . incredibly Eunaligned this know , example for , Surfers (b) One-to-one alignment Figure 1: Example sentence from the German→English IWSLT data. The one-to-one alignment is created by introducing caligned and cunaligned tokens. tasks like parsing (Henderson, 2004) and spoken language understanding (Mesnil et al., 2013). Bidirectional long short-term memory (BLSTM) networks are BRNNs using LSTM hidden layers (Graves and Schmidhuber, 2005). This work introduces BLSTMs to the problem of machine translation, al</context>
</contexts>
<marker>Schuster, Paliwal, 1997</marker>
<rawString>Mike Schuster and Kuldip K. Paliwal. 1997. Bidirectional recurrent neural networks. IEEE Transactions on Signal Processing, 45(11):2673–2681.</rawString>
</citation>
<citation valid="true">
<authors>
<author>Holger Schwenk</author>
<author>Daniel D´echelotte</author>
<author>Jean-Luc Gauvain</author>
</authors>
<title>Continuous Space Language Models for Statistical Machine Translation.</title>
<date>2006</date>
<booktitle>In Proceedings of the COLING/ACL 2006 Main Conference Poster Sessions,</booktitle>
<pages>723--730</pages>
<location>Sydney, Australia,</location>
<marker>Schwenk, D´echelotte, Gauvain, 2006</marker>
<rawString>Holger Schwenk, Daniel D´echelotte, and Jean-Luc Gauvain. 2006. Continuous Space Language Models for Statistical Machine Translation. In Proceedings of the COLING/ACL 2006 Main Conference Poster Sessions, pages 723–730, Sydney, Australia, July.</rawString>
</citation>
<citation valid="true">
<authors>
<author>Holger Schwenk</author>
</authors>
<title>Continuous Space Translation Models for Phrase-Based Statistical Machine Translation.</title>
<date>2012</date>
<booktitle>In 25th International Conference on Computational Linguistics (COLING),</booktitle>
<pages>1071--1080</pages>
<location>Mumbai, India,</location>
<contexts>
<context position="1491" citStr="Schwenk, 2012" startWordPosition="204" endWordPosition="205">trong baselines already including recurrent neural language models on three tasks: IWSLT 2013 German—*English, BOLT Arabic—*English and Chinese—*English. We obtain gains up to 1.6% BLEU and 1.7% TER by rescoring 1000-best lists. 1 Introduction Neural network models have recently experienced unprecedented attention in research on statistical machine translation (SMT). Several groups have reported strong improvements over state-of-the-art baselines using feedforward neural network-based language models (Schwenk et al., 2006; Vaswani et al., 2013), as well as translation models (Le et al., 2012; Schwenk, 2012; Devlin et al., 2014). Different from the feedforward design, recurrent neural networks (RNNs) have the advantage of being able to take into account an unbounded history of previous observations. In theory, this enables them to model long-distance dependencies of arbitrary length. However, while previous work on translation modeling with recurrent neural networks shows its effectiveness on standard baselines, so far no notable gains have been presented on top of recurrent language models (Auli et al., 2013; Kalchbrenner and Blunsom, 2013; Hu et al., 2014). In this work, we present two novel a</context>
<context position="5998" citStr="Schwenk (2012)" startWordPosition="903" endWordPosition="904">n units (Hu et al., 2014), which are phrase pairs undergoing certain constraints. At the input layer, each of the source and target phrases are modeled as a bag of words, while the output phrase is predicted word-by-word assuming conditional independence. The approach seeks to alleviate data sparsity problems that would arise if phrases were to be uniquely distinguished. Our proposed phrase-based models maintain word order within phrases, but the phrases are processed in a wordpair manner, while the phrase boundaries remain implicitly encoded in the way the words are presented to the network. Schwenk (2012) proposed a feedforward network that predicts phrases of a fixed maximum length, such that all phrase words are predicted at once. The prediction is conditioned on the source phrase. Since our phrasebased model predicts one word at a time, it does not assume any phrase length. Moreover, our model’s predictions go beyond phrase boundaries and cover unbounded history and future contexts. Using neural networks during decoding requires tackling the costly output normalization step. Vaswani et al. (2013) avoid this step by training feedforward neural language models using noise contrastive estimati</context>
</contexts>
<marker>Schwenk, 2012</marker>
<rawString>Holger Schwenk. 2012. Continuous Space Translation Models for Phrase-Based Statistical Machine Translation. In 25th International Conference on Computational Linguistics (COLING), pages 1071–1080, Mumbai, India, December.</rawString>
</citation>
<citation valid="true">
<authors>
<author>Matthew Snover</author>
<author>Bonnie Dorr</author>
<author>Richard Schwartz</author>
<author>Linnea Micciulla</author>
<author>John Makhoul</author>
</authors>
<title>A Study of Translation Edit Rate with Targeted Human Annotation.</title>
<date>2006</date>
<booktitle>In Proceedings of the 7th Conference of the Association for Machine Translation in the Americas,</booktitle>
<pages>223--231</pages>
<location>Cambridge, Massachusetts, USA,</location>
<contexts>
<context position="26246" citStr="Snover et al., 2006" startWordPosition="4314" endWordPosition="4317"> 6.6M segments, using 17 dense features. The neural network training was performed using a selection amounting to 15.5M running words. For both tasks we apply the neural networks by rescoring 1000-best lists and evaluate results on two data sets from the ’discussion forum’ domain, test1 and test2. The sizes of the data sets for the Arabic—*English system are: 1219 (dev), 1510 (test1), and 1137 (test2) segments, and for the Chinese—*English system are: 5074 (dev), 1844 (test1), and 1124 (test2) segments. All results are measured in case-insensitive BLEU [%] (Papineni et al., 2002) and TER [%] (Snover et al., 2006) on a single reference. 5.2 Results Our results on the IWSLT German—*English task are summarized in Tab. 2. At this point, we do not include a recurrent neural network lanfi ei−1 class layer (−) (+) (+) 20 dev test BLEU TER BLEU TER baseline 33.5 45.8 30.9 48.4 TM 34.6 44.5 32.0 47.1 JM 34.7 44.7 31.8 47.4 BTM 34.7 44.9 32.3 47.0 BTM (deep) 34.8 44.3 32.5 46.7 BJM 34.7 44.5 32.1 47.0 BJM (deep) 34.9 44.1 32.2 46.6 PTM 34.3 44.9 32.1 47.5 PJM 34.3 45.0 32.0 47.5 PJM (10-best) 34.4 44.8 32.0 47.3 PJM (deep) 34.6 44.7 32.0 47.6 PBJM (deep) 34.8 44.9 31.9 47.5 BBN-JM 34.4 44.9 31.9 47.6 Table 2: R</context>
</contexts>
<marker>Snover, Dorr, Schwartz, Micciulla, Makhoul, 2006</marker>
<rawString>Matthew Snover, Bonnie Dorr, Richard Schwartz, Linnea Micciulla, and John Makhoul. 2006. A Study of Translation Edit Rate with Targeted Human Annotation. In Proceedings of the 7th Conference of the Association for Machine Translation in the Americas, pages 223–231, Cambridge, Massachusetts, USA, August.</rawString>
</citation>
<citation valid="true">
<authors>
<author>Andreas Stolcke</author>
</authors>
<title>SRILM – An Extensible Language Modeling Toolkit.</title>
<date>2002</date>
<booktitle>In Proc. of the Int. Conf. on Speech and Language Processing (ICSLP),</booktitle>
<volume>2</volume>
<pages>901--904</pages>
<location>Denver, CO,</location>
<contexts>
<context position="23933" citStr="Stolcke, 2002" startWordPosition="3935" endWordPosition="3936">ackpropagation, the forward and backward recurrent layers are processed in decreasing and increasing time order, respectively. 5 Experiments 5.1 Setup All translation experiments are performed with the Jane toolkit (Vilar et al., 2010; Wuebker et al., 2012). The largest part of our experiments is carried out on the IWSLT 2013 German—*English shared translation task.2 The baseline system is trained on all available bilingual data, 4.3M sentence pairs in total, and uses a 4-gram LM with modified Kneser-Ney smoothing (Kneser and Ney, 1995; Chen and Goodman, 1998), trained with the SRILM toolkit (Stolcke, 2002). As additional 1In our implementation, the forward and backward layers converge into an intermediate identity layer, and the aggregate is weighted and fed to the next layer. 2http://www.iwslt2013.org data sources for the LM we selected parts of the Shuffled News and LDC English Gigaword corpora based on cross-entropy difference (Moore and Lewis, 2010), resulting in a total of 1.7 billion running words for LM training. The state-ofthe-art baseline is a standard phrase-based SMT system (Koehn et al., 2003) tuned with MERT (Och, 2003). It contains a hierarchical reordering model (Galley and Mann</context>
</contexts>
<marker>Stolcke, 2002</marker>
<rawString>Andreas Stolcke. 2002. SRILM – An Extensible Language Modeling Toolkit. In Proc. of the Int. Conf. on Speech and Language Processing (ICSLP), volume 2, pages 901–904, Denver, CO, September.</rawString>
</citation>
<citation valid="true">
<authors>
<author>Martin Sundermeyer</author>
<author>Ralf Schl¨uter</author>
<author>Hermann Ney</author>
</authors>
<title>LSTM neural networks for language modeling.</title>
<date>2012</date>
<booktitle>In Interspeech,</booktitle>
<location>Portland, OR, USA,</location>
<marker>Sundermeyer, Schl¨uter, Ney, 2012</marker>
<rawString>Martin Sundermeyer, Ralf Schl¨uter, and Hermann Ney. 2012. LSTM neural networks for language modeling. In Interspeech, Portland, OR, USA, September.</rawString>
</citation>
<citation valid="true">
<authors>
<author>Martin Sundermeyer</author>
<author>Ilya Oparin</author>
<author>Jean-Luc Gauvain</author>
<author>Ben Freiberg</author>
<author>Ralf Schl¨uter</author>
<author>Hermann Ney</author>
</authors>
<title>Comparison of feedforward and recurrent neural network language models.</title>
<date>2013</date>
<booktitle>In IEEE International Conference on Acoustics, Speech, and Signal Processing,</booktitle>
<pages>8430--8434</pages>
<location>Vancouver, Canada,</location>
<marker>Sundermeyer, Oparin, Gauvain, Freiberg, Schl¨uter, Ney, 2013</marker>
<rawString>Martin Sundermeyer, Ilya Oparin, Jean-Luc Gauvain, Ben Freiberg, Ralf Schl¨uter, and Hermann Ney. 2013. Comparison of feedforward and recurrent neural network language models. In IEEE International Conference on Acoustics, Speech, and Signal Processing, pages 8430–8434, Vancouver, Canada, May.</rawString>
</citation>
<citation valid="true">
<authors>
<author>Ashish Vaswani</author>
<author>Yinggong Zhao</author>
<author>Victoria Fossum</author>
<author>David Chiang</author>
</authors>
<title>Decoding with largescale neural language models improves translation.</title>
<date>2013</date>
<booktitle>In Proceedings of the 2013 Conference on Empirical Methods in Natural Language Processing,</booktitle>
<pages>1387--1392</pages>
<publisher>Association</publisher>
<institution>for Computational Linguistics.</institution>
<location>Seattle, Washington, USA,</location>
<contexts>
<context position="1428" citStr="Vaswani et al., 2013" startWordPosition="191" endWordPosition="194">. We demonstrate that our translation models are capable of improving strong baselines already including recurrent neural language models on three tasks: IWSLT 2013 German—*English, BOLT Arabic—*English and Chinese—*English. We obtain gains up to 1.6% BLEU and 1.7% TER by rescoring 1000-best lists. 1 Introduction Neural network models have recently experienced unprecedented attention in research on statistical machine translation (SMT). Several groups have reported strong improvements over state-of-the-art baselines using feedforward neural network-based language models (Schwenk et al., 2006; Vaswani et al., 2013), as well as translation models (Le et al., 2012; Schwenk, 2012; Devlin et al., 2014). Different from the feedforward design, recurrent neural networks (RNNs) have the advantage of being able to take into account an unbounded history of previous observations. In theory, this enables them to model long-distance dependencies of arbitrary length. However, while previous work on translation modeling with recurrent neural networks shows its effectiveness on standard baselines, so far no notable gains have been presented on top of recurrent language models (Auli et al., 2013; Kalchbrenner and Blunso</context>
<context position="6502" citStr="Vaswani et al. (2013)" startWordPosition="981" endWordPosition="984">ile the phrase boundaries remain implicitly encoded in the way the words are presented to the network. Schwenk (2012) proposed a feedforward network that predicts phrases of a fixed maximum length, such that all phrase words are predicted at once. The prediction is conditioned on the source phrase. Since our phrasebased model predicts one word at a time, it does not assume any phrase length. Moreover, our model’s predictions go beyond phrase boundaries and cover unbounded history and future contexts. Using neural networks during decoding requires tackling the costly output normalization step. Vaswani et al. (2013) avoid this step by training feedforward neural language models using noise contrastive estimation, while Devlin et al. (2014) augment the training objective function to produce approximately normalized scores directly. The latter work makes use of translation and joint models, and pre-computes the first hidden layer beforehand, resulting in large speedups. They report major improvements over strong baselines. The speedups achieved by both works allowed to integrate feedforward neural networks into the decoder. 3 LSTM Recurrent Neural Networks Our work is based on recurrent neural networks. In</context>
</contexts>
<marker>Vaswani, Zhao, Fossum, Chiang, 2013</marker>
<rawString>Ashish Vaswani, Yinggong Zhao, Victoria Fossum, and David Chiang. 2013. Decoding with largescale neural language models improves translation. In Proceedings of the 2013 Conference on Empirical Methods in Natural Language Processing, pages 1387–1392, Seattle, Washington, USA, October. Association for Computational Linguistics.</rawString>
</citation>
<citation valid="true">
<authors>
<author>David Vilar</author>
<author>Daniel Stein</author>
<author>Matthias Huck</author>
<author>Hermann Ney</author>
</authors>
<title>Jane: Open source hierarchical translation, extended with reordering and lexicon models.</title>
<date>2010</date>
<booktitle>In ACL 2010 Joint Fifth Workshop on Statistical Machine Translation and Metrics MATR,</booktitle>
<pages>262--270</pages>
<location>Uppsala, Sweden,</location>
<contexts>
<context position="23553" citStr="Vilar et al., 2010" startWordPosition="3871" endWordPosition="3874">ers converge into a hidden layer. A shallow variant can be obtained if the parallel layers converge into the output layer directly1. Due to the full dependence on the source sequence, evaluating bidirectional networks requires computing the forward pass of the forward and backward layers for the full sequence, before being able to evaluate the next layers. In the backward pass of backpropagation, the forward and backward recurrent layers are processed in decreasing and increasing time order, respectively. 5 Experiments 5.1 Setup All translation experiments are performed with the Jane toolkit (Vilar et al., 2010; Wuebker et al., 2012). The largest part of our experiments is carried out on the IWSLT 2013 German—*English shared translation task.2 The baseline system is trained on all available bilingual data, 4.3M sentence pairs in total, and uses a 4-gram LM with modified Kneser-Ney smoothing (Kneser and Ney, 1995; Chen and Goodman, 1998), trained with the SRILM toolkit (Stolcke, 2002). As additional 1In our implementation, the forward and backward layers converge into an intermediate identity layer, and the aggregate is weighted and fed to the next layer. 2http://www.iwslt2013.org data sources for th</context>
</contexts>
<marker>Vilar, Stein, Huck, Ney, 2010</marker>
<rawString>David Vilar, Daniel Stein, Matthias Huck, and Hermann Ney. 2010. Jane: Open source hierarchical translation, extended with reordering and lexicon models. In ACL 2010 Joint Fifth Workshop on Statistical Machine Translation and Metrics MATR, pages 262–270, Uppsala, Sweden, July.</rawString>
</citation>
<citation valid="true">
<authors>
<author>Paul J Werbos</author>
</authors>
<title>Backpropagation through time: what it does and how to do it.</title>
<date>1990</date>
<booktitle>Proceedings of the IEEE,</booktitle>
<pages>78--10</pages>
<contexts>
<context position="7586" citStr="Werbos, 1990" startWordPosition="1149" endWordPosition="1150">eedforward neural networks into the decoder. 3 LSTM Recurrent Neural Networks Our work is based on recurrent neural networks. In related fields like e. g. language modeling, this type of neural network has been shown to perform considerably better than standard feedforward architectures (Mikolov et al., 2011; Arisoy et al., 2012; Sundermeyer et al., 2013; Liu et al., 2014). Most commonly, recurrent neural networks are trained with stochastic gradient descent (SGD), where the gradient of the training criterion is computed with the backpropagation through time algorithm (Rumelhart et al., 1986; Werbos, 1990; Williams and Zipser, 1995). However, the combination of RNN networks with conventional backpropagation training leads to conceptual difficulties which are known as the vanishing (or exploding) gradient problem, described e. g. in (Bengio et al., 1994). To remedy this problem, in (Hochreiter and Schmidhuber, 1997) it was suggested to modify the architecture of a standard RNN in such a way that vanishing and exploding gradients are avoided during backpropagation. In particular, no modification of the training algorithm is necessary. The resulting architecture is referred to as long short-term </context>
</contexts>
<marker>Werbos, 1990</marker>
<rawString>Paul J. Werbos. 1990. Backpropagation through time: what it does and how to do it. Proceedings of the IEEE, 78(10):1550–1560.</rawString>
</citation>
<citation valid="true">
<authors>
<author>Ronald J Williams</author>
<author>David Zipser</author>
</authors>
<title>GradientBased Learning Algorithms for Recurrent Networks and Their Computational Complexity. In: Yves Chauvain and</title>
<date>1995</date>
<contexts>
<context position="7614" citStr="Williams and Zipser, 1995" startWordPosition="1151" endWordPosition="1154">ral networks into the decoder. 3 LSTM Recurrent Neural Networks Our work is based on recurrent neural networks. In related fields like e. g. language modeling, this type of neural network has been shown to perform considerably better than standard feedforward architectures (Mikolov et al., 2011; Arisoy et al., 2012; Sundermeyer et al., 2013; Liu et al., 2014). Most commonly, recurrent neural networks are trained with stochastic gradient descent (SGD), where the gradient of the training criterion is computed with the backpropagation through time algorithm (Rumelhart et al., 1986; Werbos, 1990; Williams and Zipser, 1995). However, the combination of RNN networks with conventional backpropagation training leads to conceptual difficulties which are known as the vanishing (or exploding) gradient problem, described e. g. in (Bengio et al., 1994). To remedy this problem, in (Hochreiter and Schmidhuber, 1997) it was suggested to modify the architecture of a standard RNN in such a way that vanishing and exploding gradients are avoided during backpropagation. In particular, no modification of the training algorithm is necessary. The resulting architecture is referred to as long short-term memory (LSTM) neural network</context>
</contexts>
<marker>Williams, Zipser, 1995</marker>
<rawString>Ronald J. Williams and David Zipser. 1995. GradientBased Learning Algorithms for Recurrent Networks and Their Computational Complexity. In: Yves Chauvain and David E. Rumelhart: “BackPropagation: Theory, Architectures and Applications”. Lawrence Erlbaum Publishers.</rawString>
</citation>
<citation valid="true">
<authors>
<author>Joern Wuebker</author>
<author>Arne Mauser</author>
<author>Hermann Ney</author>
</authors>
<title>Training phrase translation models with leaving-one-out.</title>
<date>2010</date>
<booktitle>In Proceedings of the 48th Annual Meeting of the Assoc. for Computational Linguistics,</booktitle>
<pages>475--484</pages>
<location>Uppsala, Sweden,</location>
<contexts>
<context position="21267" citStr="Wuebker et al., 2010" startWordPosition="3498" endWordPosition="3501">me step, where we still have the last target word from the previous phrase as input instead of ε). With the last word of the source phrase “Beispiel” being presented to the network, the full source phrase is stored in the hidden layer, and the neural network is then trained to predict the target phrase words at the output layer. Subsequently, the source input is ε, and the target input is the most recent target side history word. To obtain a phrase-aligned training sequence for the phrase-based RNN models, we force-align the training data with the application of leave-one-out as described in (Wuebker et al., 2010). 4.4 Bidirectional RNN Architecture While the unidirectional RNNs include an unbounded sentence history, they are still limited in the number of future source words they include. Bidirectional models provide a flexible means to also include an unbounded future context, which, unlike the delayed unidirectional models, require no tuning to determine the amount of delay. Fig. 6 illustrates the bidirectional model architecture, which is an extension of the unidirectional model of Fig. 3. First, an additional recurrent hidden layer is added in parallel to the existing one. This layer will be refer</context>
</contexts>
<marker>Wuebker, Mauser, Ney, 2010</marker>
<rawString>Joern Wuebker, Arne Mauser, and Hermann Ney. 2010. Training phrase translation models with leaving-one-out. In Proceedings of the 48th Annual Meeting of the Assoc. for Computational Linguistics, pages 475–484, Uppsala, Sweden, July.</rawString>
</citation>
<citation valid="true">
<authors>
<author>Joern Wuebker</author>
<author>Matthias Huck</author>
<author>Stephan Peitz</author>
<author>Malte Nuhn</author>
<author>Markus Freitag</author>
<author>Jan-Thorsten Peter</author>
<author>Saab Mansour</author>
<author>Hermann Ney</author>
</authors>
<title>Jane 2: Open source phrase-based and hierarchical statistical machine translation.</title>
<date>2012</date>
<booktitle>In International Conference on Computational Linguistics,</booktitle>
<pages>483--491</pages>
<location>Mumbai, India,</location>
<contexts>
<context position="23576" citStr="Wuebker et al., 2012" startWordPosition="3875" endWordPosition="3878">hidden layer. A shallow variant can be obtained if the parallel layers converge into the output layer directly1. Due to the full dependence on the source sequence, evaluating bidirectional networks requires computing the forward pass of the forward and backward layers for the full sequence, before being able to evaluate the next layers. In the backward pass of backpropagation, the forward and backward recurrent layers are processed in decreasing and increasing time order, respectively. 5 Experiments 5.1 Setup All translation experiments are performed with the Jane toolkit (Vilar et al., 2010; Wuebker et al., 2012). The largest part of our experiments is carried out on the IWSLT 2013 German—*English shared translation task.2 The baseline system is trained on all available bilingual data, 4.3M sentence pairs in total, and uses a 4-gram LM with modified Kneser-Ney smoothing (Kneser and Ney, 1995; Chen and Goodman, 1998), trained with the SRILM toolkit (Stolcke, 2002). As additional 1In our implementation, the forward and backward layers converge into an intermediate identity layer, and the aggregate is weighted and fed to the next layer. 2http://www.iwslt2013.org data sources for the LM we selected parts </context>
</contexts>
<marker>Wuebker, Huck, Peitz, Nuhn, Freitag, Peter, Mansour, Ney, 2012</marker>
<rawString>Joern Wuebker, Matthias Huck, Stephan Peitz, Malte Nuhn, Markus Freitag, Jan-Thorsten Peter, Saab Mansour, and Hermann Ney. 2012. Jane 2: Open source phrase-based and hierarchical statistical machine translation. In International Conference on Computational Linguistics, pages 483–491, Mumbai, India, December.</rawString>
</citation>
<citation valid="true">
<authors>
<author>Joern Wuebker</author>
<author>Stephan Peitz</author>
<author>Felix Rietig</author>
<author>Hermann Ney</author>
</authors>
<title>Improving statistical machine translation with word class models.</title>
<date>2013</date>
<booktitle>In Conference on Empirical Methods in Natural Language Processing,</booktitle>
<pages>1377--1381</pages>
<location>Seattle, USA,</location>
<contexts>
<context position="24608" citStr="Wuebker et al., 2013" startWordPosition="4043" endWordPosition="4046">d backward layers converge into an intermediate identity layer, and the aggregate is weighted and fed to the next layer. 2http://www.iwslt2013.org data sources for the LM we selected parts of the Shuffled News and LDC English Gigaword corpora based on cross-entropy difference (Moore and Lewis, 2010), resulting in a total of 1.7 billion running words for LM training. The state-ofthe-art baseline is a standard phrase-based SMT system (Koehn et al., 2003) tuned with MERT (Och, 2003). It contains a hierarchical reordering model (Galley and Manning, 2008) and a 7- gram word cluster language model (Wuebker et al., 2013). Here, we also compare against a feedforward joint model as described by Devlin et al. (2014), with a source window of 11 words and a target history of three words, which we denote as BBN-JM. Instead of POS tags, we predict word classes trained with mkcls. We use a shortlist of size 16K and 1000 classes for the remaining words. All neural networks are trained on the TED portion of the data (138K segments) and are applied in a rescoring step on 1000-best lists. To confirm our results, we run additional experiments on the Arabic—*English and Chinese—*English tasks of the DARPA BOLT project. In </context>
</contexts>
<marker>Wuebker, Peitz, Rietig, Ney, 2013</marker>
<rawString>Joern Wuebker, Stephan Peitz, Felix Rietig, and Hermann Ney. 2013. Improving statistical machine translation with word class models. In Conference on Empirical Methods in Natural Language Processing, pages 1377–1381, Seattle, USA, October.</rawString>
</citation>
</citationList>
</algorithm>
</algorithms>
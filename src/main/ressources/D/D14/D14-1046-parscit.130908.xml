<?xml version="1.0" encoding="UTF-8"?>
<algorithms version="110505">
<algorithm name="SectLabel" version="110505">
<variant no="0" confidence="0.000721">
<title confidence="0.993122">
Automatic Domain Assignment for Word Sense Alignment
</title>
<author confidence="0.864054">
Tommaso Caselli Carlo Strapparava
</author>
<affiliation confidence="0.486167">
TrentoRISE /Via Sommarive, 18 FBK /Via Sommarive, 18
</affiliation>
<address confidence="0.826959">
38123 Povo, Italy 38123 Povo, Italy
</address>
<email confidence="0.993444">
t.caselli@gmail.com strappa@fbk.eu
</email>
<sectionHeader confidence="0.993733" genericHeader="abstract">
Abstract
</sectionHeader>
<bodyText confidence="0.9999675625">
This paper reports on the development of a hy-
brid and simple method based on a machine
learning classifier (Naive Bayes), Word Sense
Disambiguation and rules, for the automatic
assignment of WordNet Domains to nominal
entries of a lexicographic dictionary, the Senso
Comune De Mauro Lexicon. The system ob-
tained an F1 score of 0.58, with a Precision
of 0.70. We further used the automatically as-
signed domains to filter out word sense align-
ments between MultiWordNet and Senso Co-
mune. This has led to an improvement in the
quality of the sense alignments showing the
validity of the approach for domain assign-
ment and the importance of domain informa-
tion for achieving good sense alignments.
</bodyText>
<sectionHeader confidence="0.976945" genericHeader="categories and subject descriptors">
1 Introduction and Problem Statement
</sectionHeader>
<bodyText confidence="0.999926408163266">
Lexical knowledge, i.e. how words are used and ex-
press meaning, plays a key role in Natural Language
Processing. Lexical knowledge is available in many
different forms, ranging from unstructured terminolo-
gies (i.e. word list), to full fledged computational lexica
and ontologies (e.g. WordNet (Fellbaum, 1998)). The
process of creation of lexical resources is costly both
in terms of money and time. To overcome these lim-
its, semi-automatic approaches have been developed
(e.g. MultiWordNet (Pianta et al., 2002)) with differ-
ent levels of success. Furthermore, important informa-
tion is scattered in different resources and difficult to
use. Semantic interoperability between resources could
represent a viable solution to allow reusability and de-
velop more robust and powerful resources. Word sense
alignment (WSA) qualifies as the preliminary require-
ment for achieving this goal (Matuschek and Gurevych,
2013).
WSA aims at creating lists of pairs of senses from
two, or more, (lexical-semantic) resources which de-
note the same meaning. Different approaches to WSA
have been proposed and they all share some common
elements, namely: i.) the extensive use of sense de-
scriptions of the words (e.g. WordNet glosses); and ii.)
the extension of the basic sense descriptions with addi-
tional information such as hypernyms, synonyms and
domain or category labels.
The purpose of this work is two folded: first, we exper-
iment on the automatic assignment of domain labels to
sense descriptions, and then, evaluate the impact of this
information for improving an existing sense aligned
dataset for nouns. Previous works has demonstrated
that domain labels are a good feature for obtaining high
quality alignments of entries (Navigli, 2006; Toral et
al., 2009; Navigli and Ponzetto, 2012). The Word-
Net (WN) Domains (Magnini and Cavaglia, 2000; Ben-
tivogli et al., 2004) have been selected as reference do-
main labels. We will use as candidate lexico-semantic
resources to be aligned two Italian lexica, namely, Mul-
tiWordNet (MWN) and the Senso Comune De Mauro
Lexicon (SCDM) (Vetere et al., 2011).
The two resources differ in terms of modelization: the
former, MWN, is an Italian version of WN obtained
through the “expand model” (Vossen, 1996) and per-
fectly aligned to Princeton WN 1.6, while the latter,
SCDM, is a machine readable dictionary obtained from
a paper-based reference lexicographic dictionary, De
Mauro GRADIT. Major issues for WSA of the lexica
concern the following aspects:
</bodyText>
<listItem confidence="0.909422">
• SCMD has no structure of word senses (i.e. no
taxonomy, no synonymy relations, no distinction
between core senses and subsenses for polyse-
mous entries) unlike MWN;
• SCDM has no domain or category labels associ-
ated to senses (with the exception of specific ter-
minological entries) unlike MWN;
• the Italian section of MWN has only 2,481 glosses
in Italian over 28,517 synsets for nouns (i.e.
8.7%).
</listItem>
<bodyText confidence="0.999805125">
The remainder of this paper is organized as follows:
Section 2 will report on the methodology and exper-
iments implemented for the automatic assignment of
the WN Domains to the SCDM entries. Section 3 will
describe the dataset used for the evaluation of the WSA
experiments and the use of the WN Domains for filter-
ing the sense alignments. Finally, Section 4 illustrates
conclusion and future work.
</bodyText>
<sectionHeader confidence="0.817426" genericHeader="method">
2 Methodology and Experiments
</sectionHeader>
<bodyText confidence="0.706885">
The WN Domains consist of a set of 166 hierarchically
organized labels which have been associated to each
</bodyText>
<page confidence="0.949956">
414
</page>
<note confidence="0.5129765">
Proceedings of the 2014 Conference on Empirical Methods in Natural Language Processing (EMNLP), pages 414–418,
October 25-29, 2014, Doha, Qatar. c�2014 Association for Computational Linguistics
</note>
<table confidence="0.9998938">
Classifiers P R F1 10-Fold F1
NaiveBayeslemma 0.77 0.58 0.66 0.66
MaxEntlemma 0.70 0.49 0.58 0.63
NaiveBayeswsd 0.77 0.58 0.66 0.69
MaxEntwsd 0.74 0.54 0.62 0.67
</table>
<tableCaption confidence="0.9961">
Table 2: Results for the Naive Bayes and Maximum Entropy binary classifiers.
</tableCaption>
<bodyText confidence="0.998233631578947">
synset1 and express a subject field label (e.g. SPORT,
MEDICINE). A special label, FACTOTUM, has been
used for those synsets which can appear in almost all
subject fields.
The identification of a domain label to the nominal en-
tries in the SCDM Lexicon is based the “One Domain
per Discourse” (ODD) hypothesis applied to the sense
descriptions. We have used a reduced set of domains
labels (45 normalized domains) following (Magnini et
al., 2001).
To assign the WN domain label to the SCDM entries,
we have developed a hybrid method: first a binary clas-
sifier is applied to the SCDM sense descriptions to dis-
criminate between two domain values, FACTOTUM
and OTHER, where the OTHER value includes all re-
maining 44 normalized domains. After this, all entries
classified with the OTHER value are analyzed by a rule
based system and associated with a specific domain la-
bel (i.e. SPORT, MEDICINE, FOOD ... ).
</bodyText>
<subsectionHeader confidence="0.975486">
2.1 Classifier and feature selection
</subsectionHeader>
<bodyText confidence="0.9996669">
We have developed a training set by manually align-
ing noun senses between the two lexica. The sense
alignment allows us to associate all the information of a
synset to a corresponding entry in the SCDM lexicon,
including the WN Domain label. Concerning the test
set, we have used an existing dataset of aligned noun
pairs as in (Caselli et al., 2014). We report in Table 1
the figures for the training and test sets. Multiple align-
ments with the same domain label have been excluded
from the training set.
</bodyText>
<table confidence="0.990303714285714">
Characteristics Training Set Test Set
# lemmas 131 46
# of aligned pairs 369 166
# of SCDM senses 747 216
# of MWN synsets 675 229
# SCDM with
WN Domain label 350 118
</table>
<tableCaption confidence="0.999877">
Table 1: Training and test sets for the classifier.
</tableCaption>
<bodyText confidence="0.9998538">
In order for the classifier to predict the binary do-
main labels (FACTOTUM and OTHER), each sense
description of the SCDM Lexicon has been repre-
sented by means of a two-dimensional feature vector
(e.g. for training data: BINARY DOMAIN LABEL
</bodyText>
<footnote confidence="0.910057">
1The full set of labels and hierarchy is available at
http://wndomains.fbk.eu/hierarchy.html
</footnote>
<bodyText confidence="0.550068">
GENERIC:val SPECIFIC:val). Feature values have
been obtained through two strategies:
</bodyText>
<listItem confidence="0.942990533333333">
• lemma label: we extract all normalized domain
labels associated to each sense of each lemma in
the sense description from MWN. The value of
the feature GENERIC corresponds to the sum of
the FACTOTUM labels. The value of the fea-
ture SPECIFIC corresponds to the sum of all other
specific domain labels (e.g. MEDICINE, SPORT
etc.) after they have been collapsed into a single
value (i.e. NOT-FACTOTUM).
• word sense label: for each sense description, we
have first performed Word Sense Disambiguation
by means of an adapted version to Italian of the
UKB package2 (Agirre et al., 2010; Agirre et al.,
2014)3. Only the highest ranked synset, and as-
sociated WN Domain(s), was retained as good.
</listItem>
<bodyText confidence="0.990432571428571">
Similarly to the lemma label strategy, the sum of
the domain label FACTOTUM is assigned to the
feature GENERIC, while the sum of all other do-
main labels collapsed into the single value NOT-
FACTOTUM is assigned to the feature SPECIFIC.
We experimented with two classifiers: Naive Bayes
and Maximum Entropy as implemented in the MAL-
LET package (McCallum, 2002). We illustrate the re-
sults in Table 2. The classifiers have been evaluated
with respect to standard Precision (P), Recall (R) and
F1 against the test set. Ten-fold cross validation has
been performed on the training set as well. Classifiers
trained with the first strategy will be associated with the
label lemma, while those trained with the second strat-
egy with the label wsd.
Both classifiers obtains good results with respect to
the test data in terms of Precision and Recall. The
Naive Bayes classifier outperforms the Maximum En-
tropy one in both training approaches, suggesting better
generalization capabilities even in presence of a small
training set and basic features. The role of WSD has
a positive impact, namely for the Maximum Entropy
classifier (Precision +4 points, Recall +5 points with
respect to the lemma label). Although such a positive
effect of the WSD does not emerge for the Naive Bayes
classifier with respect to the test set, we can still ob-
serve an improvement over the ten-fold cross valida-
tion (F1= 0.69 vs. F1=0.66). We finally selected the
</bodyText>
<footnote confidence="0.999552666666667">
2Available at http://ixa2.si.ehu.es/ukb/
3We used the WN Multilingual Central Repository as
knowledge base and the MWN entries as dictionary
</footnote>
<page confidence="0.996085">
415
</page>
<bodyText confidence="0.983005">
predictions of Naive Bayeswsd classifier as input to the
rule-based system as it provides the highest scores.
</bodyText>
<subsectionHeader confidence="0.910257">
2.2 Rules for WN Domain assignment
</subsectionHeader>
<bodyText confidence="0.99955">
The rule based classifier for final WN Domain assign-
ment works as follows:
</bodyText>
<listItem confidence="0.788520307692308">
• lemmatized and word sense disambiguated lem-
mas in the sense descriptions are associated with
the corresponding WN Domains from MWN;
• frequency counts on the WN Domain labels is ap-
plied; the most frequent WN Domain is assigned
as the correct WN Domain of the nominal entry;
• in case two or more WN Domains have same fre-
quency, the following assignment strategy is ap-
plied: if the frequency scores of the WN Do-
mains is equal to 1, the value FACTOTUM is se-
lected; on the contrary, if the frequency score is
higher than 1, all WN Domain labels are retained
as good.
</listItem>
<bodyText confidence="0.999859694444445">
We report the results on final domain assignment
in Table 3. The final system, NaiveBayes+Rules, has
been compared to two baselines. Both baselines ap-
ply frequency counts over the WN Domains labels
of the lemmas of the sense descriptions for the en-
tire set of the 45 normalized domain values, including
the FACTOTUM label, as explained in Section2. The
Baselinelemma assigns the domain by taking into ac-
count every WN Domain associated to each lemma. On
the other hand, the Baselinewsd selects only the WN
Domain of sense disambiguated lemmas. WSD for the
second baseline has been performed by applying the
same method described in Section 2.1. The results of
both baselines have high values for Precision (0.58 for
Baselinelemma, 0.70 for Baselinewsd). We consider
this as a further support to the validity of the ODD hy-
pothesis which seems to hold even for text descriptions
like dictionary glosses which normally use generic lex-
ical items to illustrate word senses. It is also interesting
to notice that WSD on its own has a positive impact in
Baselinewsd system for the assignment of specific do-
main labels (F1=0.53).
The hybrid system performs better than both base-
lines in terms of F1 scores (F1=0.58 vs. F1=0.45 for
Baselinelemma vs. F1=0.53 for Baselinewsd). How-
ever, both the hybrid system and the Baselinewsd ob-
tain the same Precision. To better evaluate the per-
formance of our hybrid approach, we computed the
paired t-test. The results of the hybrid system are sta-
tistically significant with respect to the Baselinelemma
(p &lt; 0.05) and for Recall only when compared to the
Baselinewsd.
To further analyze the difference between the hybrid
system and the Baselinewsd, we performed an error
analysis on their outputs. We have identified that the
hybrid system is more accurate in the prediction of the
</bodyText>
<table confidence="0.99975175">
System P R F1
NaiveBayeswsd+Rules 0.70† 0.50†* 0.58†
Baselinelemma 0.58 0.36 0.45
Baselinewsd 0.70 0.43 0.53
</table>
<tableCaption confidence="0.7883245">
Table 3: Results of WN Domain Assignment over the
SDCM entries. Statistical significance of the Naive-
Bayes+Rules system has been marked with a † for the
Baselinelemma and with a * for the Baselinewsd
</tableCaption>
<bodyText confidence="0.999132857142857">
FACTOTUM class with respect to the baseline. In par-
ticular, the accuracy of the hybrid system on this class
is 79% while that of the baseline is only 65%. In addi-
tion to this, the hybrid system provides better results in
terms of Recall (R=0.50 vs. R=0.43). Although compa-
rable, the hybrid system provides more accurate results
with respect to the baseline.
</bodyText>
<sectionHeader confidence="0.988707" genericHeader="method">
3 Domain Filtering for WSA
</sectionHeader>
<bodyText confidence="0.9963352">
This section reports on the experiments for improving
existing WSA for nouns between SDCM and MWN. In
this work we have used the same dataset and alignment
methods as in (Caselli et al., 2014), shortly described
here:
</bodyText>
<listItem confidence="0.954172071428571">
• Lexical Match: for each word w and for each
sense s in the given resources R E {MWN,
SCDM}, we constructed a sense descriptions
dR(s) as a bag of words in Italian. The alignment
is based on counting the number of overlapping
tokens between the two strings, normalized by the
length of the strings;
• Cosine Similarity: we used the Personalized Page
Rank (PPR) algorithm (Agirre et al., 2010) with
WN 3.0 as knowledge base extended with the
“Princeton Annotated Gloss Corpus”. Once the
PPR vector pairs are obtained, the alignment is
obtained on the basis of the cosine score for each
pair4.
</listItem>
<bodyText confidence="0.999958461538462">
The dataset consists of 166 pairs of aligned senses
from MWN and SCDM for 46 nominal lemmas
(see also column “Test set” in Table 1). Overall,
SCDM covers 53.71The main difference with respect
to (Caselli et al., 2014) is that the proposed alignments
have been additionally filtered on the basis of the output
of the WN domain system (NaiveBayeswsd+Rules). In
particular, for each aligned pair which was considered
as good in (Caselli et al., 2014), we have applied a fur-
ther filtering based on the WN domain system results
as follows: if two senses are aligned but do not have
the same domain, they are excluded from the WSA re-
sults, otherwise they are retained. Table 4 illustrates
</bodyText>
<footnote confidence="0.998783">
4The vectors for the SCDM entries were obtained by, first,
applying Google Translate API to get the English translations
and, then, PPR over WN 3.0.
</footnote>
<page confidence="0.992694">
416
</page>
<table confidence="0.999965714285714">
System P R F1
LexicalMatch 0.76 (0.69) 0.27 (0.44) 0.40 (0.55)
Cosine noThreshold 0.27 (0.12) 0.47 (0.94) 0.35 (0.21)
Cosine &gt; 0.1 0.77 (0.52) 0.21 (0.32) 0.33 (0.40)
Cosine &gt; 0.2 0.87 (0.77) 0.14 (0.21) 0.24 (0.33)
LexicalMatch+Cosine &gt; 0.1 0.73 (na) 0.40 (na) 0.51 (na)
LexicalMatch+Cosine &gt; 0.2 0.77 (0.67) 0.37 (0.61) 0.50 (0.64)
</table>
<tableCaption confidence="0.998909">
Table 4: Results for WSA of nouns with domain filtering.
</tableCaption>
<bodyText confidence="0.9996857">
the results of the WSA approaches with domain fil-
ters. We report in brackets the results from (Caselli et
al., 2014). The filtering based on WN Domains has a
big impact on Precision and contributes to increase the
quality of the aligned senses. Although, in general, we
have a downgrading of the performance with respect to
Recall, the increase in Precision will reduce the man-
ual post-processing effort to fully aligned the two re-
sources5. Furthermore, it is interesting to notice that,
when merging together the results of the pre-filtered
alignments from the two alignment approaches (Lex-
icalMatch+Cosine &gt; 0.1 and LexicalMatch+Cosine &gt;
0.2), we still have a very high Precision (&gt; 0.70) and an
increase in Recall (&gt; 0.40) with respect to the results of
each approach. Finally, we want to point out that what
was reported as the best alignment results in (Caselli
et al., 2014), namely LexicalMatch+Cosine &gt; 0.2, can
be obtained, at least for Precision, with a lower filtering
cut-off threshold on the Cosine Similarity approach (i.e
cut-off threshold at or higher than 0.1)
</bodyText>
<sectionHeader confidence="0.993515" genericHeader="conclusions">
4 Conclusions and Future Work
</sectionHeader>
<bodyText confidence="0.999979454545455">
This work describes a hybrid approach based on a
Naive Bayes classifier, Word Sense Disambiguation
and rules for assigning WN Domains to nominal sense
descriptions of a lexicographic dictionary, the Senso
Comune De Mauro Lexicon. The assignment of do-
main labels has been used to improve WSA results on
nouns between the Senso Comune Lexicon and Mul-
tiWordNet. The results support some observations,
namely: i.) domain filtering plays an important role
in WSA, namely as a strategy to exclude wrong align-
ments (false positives) and improve the quality of the
aligned pairs; ii.) the method we have proposed is a vi-
able approach for automatically enriching existing lex-
ical resources in a reliable way; and iii.) the ODD hy-
pothesis also apply to sense descriptions.
An advantage of our approach is its simplicity. We have
used features based on frequency counts and obtained
good results, with a Precision of 0.70 for automatic WN
Domain assignment. Nevertheless, an important role
is played by Word Sense Disambiguation. The use of
domain labels obtained from sense disambiguated lem-
mas improves both the results of the classifier and those
</bodyText>
<footnote confidence="0.972443666666667">
5The F1 of 0.64 in (Caselli et al., 2014) is obtained with a
Precision of 0.67, suggesting that some alignments are false
positives
</footnote>
<bodyText confidence="0.999495555555556">
of the rules. The absence of statistical significance with
respect to the Baselinewsd is not to be considered as a
negative result. As the error analysis has showed, the
classifier mostly contributes to the identification of the
FACTOTUM value, which tends to be overestimated
even with sense disambiguated lemmas, and to Recall.
We are planning to extend this work to include do-
main clusters to improve the domain assignment re-
sults, namely in terms of Recall.
</bodyText>
<sectionHeader confidence="0.99856" genericHeader="acknowledgments">
Acknowledgments
</sectionHeader>
<bodyText confidence="0.999458333333333">
One of the author wants to thank Vrije Univerisiteit
Amsterdam for sponsoring the attendance to the
EMNLP conference.
</bodyText>
<sectionHeader confidence="0.995445" genericHeader="references">
References
</sectionHeader>
<reference confidence="0.952124">
Eneko Agirre, Montse Cuadros, German Rigau, and
Aitor Soroa. 2010. Exploring knowledge bases
for similarity. In Nicoletta Calzolari (Conference
Chair), Khalid Choukri, Bente Maegaard, Joseph
Mariani, Jan Odijk, Stelios Piperidis, Mike Ros-
ner, and Daniel Tapias, editors, Proceedings of the
Seventh International Conference on Language Re-
sources and Evaluation (LREC’10), Valletta, Malta,
may. European Language Resources Association
(ELRA).
Eneko Agirre, Oier L´opez de Lacalle, and Aitor Soroa.
2014. Random walks for knowledge-based word
sense disambiguation. Computational Linguistics,
40(1):57–84.
Luisa Bentivogli, Pamela Forner, Bernardo Magnini,
and Emanuele Pianta. 2004. Revising the wordnet
domains hierarchy: semantics, coverage and balanc-
ing. In Proceedings of the Workshop on Multilin-
gual Linguistic Resources, pages 101–108. Associa-
tion for Computational Linguistics.
Tommaso Caselli, Carlo Strapparava, Laure Vieu, and
Guido Vetere. 2014. Aligning an italianwordnet
with a lexicographic dictionary: Coping with limited
data. In Proceedings of the Seventh Global WordNet
Conference, pages 290–298.
Christiane Fellbaum. 1998. WordNet: An Electronic
Lexical Database (Language, Speech, and Commu-
nication). MIT Press.
</reference>
<page confidence="0.994232">
417
</page>
<reference confidence="0.966293974358975">
Bernardo Magnini and Gabriela Cavaglia. 2000. Inte-
grating subject field codes into wordnet. In Proceed-
ings of the conference on International Language
Resources and Evaluation (LREC 2000).
Bernardo Magnini, Carlo Strapparava, Giovanni Pez-
zulo, and Alfio Gliozzo. 2001. Using domain in-
formation for word sense disambiguation. In The
Proceedings of the Second International Workshop
on Evaluating Word Sense Disambiguation Systems,
pages 111–114. Association for Computational Lin-
guistics.
Michael Matuschek and Iryna Gurevych. 2013.
Dijkstra-wsa: A graph-based approach to word sense
alignment. Transactions of the Association for Com-
putational Linguistics (TACL), 2:to appear.
Andrew Kachites McCallum. 2002. Mallet: A ma-
chine learning for language toolkit.
Rada Mihalcea. 2007. Using Wikipedia for automatic
word sense disambiguation. In Human Language
Technologies 2007: The Conference of the North
American Chapter of the Association for Computa-
tional Linguistics; Proceedings of the Main Confer-
ence, Rochester, New York.
Roberto Navigli and Simone Paolo Ponzetto. 2012.
BabelNet: The automatic construction, evaluation
and application of a wide-coverage multilingual se-
mantic network. Artificial Intelligence, 193:217–
250.
Roberto Navigli. 2006. Meaningful clustering of
senses helps boost word sense disambiguation per-
formance. In Proceedings of the 44th Annual Meet-
ing of the Association for Computational Linguis-
tics joint with the 21st International Conference on
Computational Linguistics (COLING-ACL), Sydney,
Australia.
Elisabeth Niemann and Iryna Gurevych. 2011. The
peoples web meets linguistic knowledge: Automatic
sense alignment of Wikipedia and WordNet. In
Proceedings of the 9th International Conference on
Computational Semantics, pages 205–214, Singa-
pore, January.
Emanuele Pianta, Luisa Bentivogli, and Cristian Gi-
rardi. 2002. MultiWordNet: developing an aligned
multilingual database. In First International Confer-
ence on Global WordNet, Mysore, India.
German Rigau and Agirre Eneko. 1995. Disambiguat-
ing bilingual nominal entries against WordNet. In
Proceedings of workshop The Computational Lexi-
con, 7th European Summer School in Logic, Lan-
guage and Information, Barcelona, Spain.
Adriana Roventini, Nilda Ruimy, Rita Marinelli,
Marisa Ulivieri, and Michele Mammini. 2007.
Mapping concrete entities from PAROLE-SIMPLE-
CLIPS to ItalWordNet: Methodology and results. In
Proceedings of the 45th Annual Meeting of the As-
sociation for Computational Linguistics Companion
Volume Proceedings of the Demo and Poster Ses-
sions, Prague, Czech Republic, June.
Maria Ruiz-Casado, Enrique Alfonseca, and Pablo
Castells. 2005. Automatic assignment of Wikipedia
encyclopedic entries to WordNet synsets. In Pro-
ceedings of the Third international conference on
Advances in Web Intelligence, AWIC’05, Berlin,
Heidelberg. Springer-Verlag.
Antonio Toral, Oscar Ferr´andez, Eneko Aguirre, and
Rafael Munoz. 2009. A study on linking and disam-
biguating wikipedia categories to wordnet using text
similarity. Proceedings of the International Confer-
ence on Recent Advances in Natural Language Pro-
cessing (RANLP 2009).
Guido Vetere, Alessandro Oltramari, Isabella Chiari,
Elisabetta Jezek, Laure Vieu, and Fabio Massimo
Zanzotto. 2011. Senso Comune, an open knowl-
edge base for italian. JTraitement Automatique des
Langues, 53(3):217–243.
Piek Vossen. 1996. Right or wrong: Combining lexi-
cal resources in the eurowordnet project. In Euralex,
volume 96, pages 715–728. Citeseer.
</reference>
<page confidence="0.993059">
418
</page>
</variant>
</algorithm>
<algorithm name="ParsHed" version="110505">
<variant no="0" confidence="0.856176">
<title confidence="0.999708">Automatic Domain Assignment for Word Sense Alignment</title>
<author confidence="0.995363">Tommaso Caselli Carlo Strapparava</author>
<address confidence="0.945132">TrentoRISE /Via Sommarive, 18 FBK /Via Sommarive, 18 38123 Povo, Italy 38123 Povo, Italy</address>
<email confidence="0.991335">t.caselli@gmail.comstrappa@fbk.eu</email>
<abstract confidence="0.998163176470588">This paper reports on the development of a hybrid and simple method based on a machine learning classifier (Naive Bayes), Word Sense Disambiguation and rules, for the automatic assignment of WordNet Domains to nominal entries of a lexicographic dictionary, the Senso Comune De Mauro Lexicon. The system obtained an F1 score of 0.58, with a Precision of 0.70. We further used the automatically assigned domains to filter out word sense alignments between MultiWordNet and Senso Comune. This has led to an improvement in the quality of the sense alignments showing the validity of the approach for domain assignment and the importance of domain information for achieving good sense alignments.</abstract>
</variant>
</algorithm>
<algorithm name="ParsCit" version="110505">
<citationList>
<citation valid="false">
<authors>
<author>Eneko Agirre</author>
<author>Montse Cuadros</author>
<author>German Rigau</author>
<author>Aitor Soroa</author>
</authors>
<title>Exploring knowledge bases for similarity.</title>
<date>2010</date>
<booktitle>Proceedings of the Seventh International Conference on Language Resources and Evaluation (LREC’10),</booktitle>
<editor>In Nicoletta Calzolari (Conference Chair), Khalid Choukri, Bente Maegaard, Joseph Mariani, Jan Odijk, Stelios Piperidis, Mike Rosner, and Daniel Tapias, editors,</editor>
<location>Valletta, Malta,</location>
<contexts>
<context position="7464" citStr="Agirre et al., 2010" startWordPosition="1205" endWordPosition="1208">een obtained through two strategies: • lemma label: we extract all normalized domain labels associated to each sense of each lemma in the sense description from MWN. The value of the feature GENERIC corresponds to the sum of the FACTOTUM labels. The value of the feature SPECIFIC corresponds to the sum of all other specific domain labels (e.g. MEDICINE, SPORT etc.) after they have been collapsed into a single value (i.e. NOT-FACTOTUM). • word sense label: for each sense description, we have first performed Word Sense Disambiguation by means of an adapted version to Italian of the UKB package2 (Agirre et al., 2010; Agirre et al., 2014)3. Only the highest ranked synset, and associated WN Domain(s), was retained as good. Similarly to the lemma label strategy, the sum of the domain label FACTOTUM is assigned to the feature GENERIC, while the sum of all other domain labels collapsed into the single value NOTFACTOTUM is assigned to the feature SPECIFIC. We experimented with two classifiers: Naive Bayes and Maximum Entropy as implemented in the MALLET package (McCallum, 2002). We illustrate the results in Table 2. The classifiers have been evaluated with respect to standard Precision (P), Recall (R) and F1 a</context>
<context position="13051" citStr="Agirre et al., 2010" startWordPosition="2148" endWordPosition="2151"> for WSA This section reports on the experiments for improving existing WSA for nouns between SDCM and MWN. In this work we have used the same dataset and alignment methods as in (Caselli et al., 2014), shortly described here: • Lexical Match: for each word w and for each sense s in the given resources R E {MWN, SCDM}, we constructed a sense descriptions dR(s) as a bag of words in Italian. The alignment is based on counting the number of overlapping tokens between the two strings, normalized by the length of the strings; • Cosine Similarity: we used the Personalized Page Rank (PPR) algorithm (Agirre et al., 2010) with WN 3.0 as knowledge base extended with the “Princeton Annotated Gloss Corpus”. Once the PPR vector pairs are obtained, the alignment is obtained on the basis of the cosine score for each pair4. The dataset consists of 166 pairs of aligned senses from MWN and SCDM for 46 nominal lemmas (see also column “Test set” in Table 1). Overall, SCDM covers 53.71The main difference with respect to (Caselli et al., 2014) is that the proposed alignments have been additionally filtered on the basis of the output of the WN domain system (NaiveBayeswsd+Rules). In particular, for each aligned pair which w</context>
</contexts>
<marker>Agirre, Cuadros, Rigau, Soroa, 2010</marker>
<rawString>Eneko Agirre, Montse Cuadros, German Rigau, and Aitor Soroa. 2010. Exploring knowledge bases for similarity. In Nicoletta Calzolari (Conference Chair), Khalid Choukri, Bente Maegaard, Joseph Mariani, Jan Odijk, Stelios Piperidis, Mike Rosner, and Daniel Tapias, editors, Proceedings of the Seventh International Conference on Language Resources and Evaluation (LREC’10), Valletta, Malta, may. European Language Resources Association (ELRA).</rawString>
</citation>
<citation valid="true">
<authors>
<author>Eneko Agirre</author>
</authors>
<title>Oier L´opez de Lacalle, and Aitor Soroa.</title>
<date>2014</date>
<journal>Computational Linguistics,</journal>
<volume>40</volume>
<issue>1</issue>
<marker>Agirre, 2014</marker>
<rawString>Eneko Agirre, Oier L´opez de Lacalle, and Aitor Soroa. 2014. Random walks for knowledge-based word sense disambiguation. Computational Linguistics, 40(1):57–84.</rawString>
</citation>
<citation valid="true">
<authors>
<author>Luisa Bentivogli</author>
<author>Pamela Forner</author>
<author>Bernardo Magnini</author>
<author>Emanuele Pianta</author>
</authors>
<title>Revising the wordnet domains hierarchy: semantics, coverage and balancing.</title>
<date>2004</date>
<booktitle>In Proceedings of the Workshop on Multilingual Linguistic Resources,</booktitle>
<pages>101--108</pages>
<publisher>Association</publisher>
<institution>for Computational Linguistics.</institution>
<contexts>
<context position="2805" citStr="Bentivogli et al., 2004" startWordPosition="432" endWordPosition="436">sion of the basic sense descriptions with additional information such as hypernyms, synonyms and domain or category labels. The purpose of this work is two folded: first, we experiment on the automatic assignment of domain labels to sense descriptions, and then, evaluate the impact of this information for improving an existing sense aligned dataset for nouns. Previous works has demonstrated that domain labels are a good feature for obtaining high quality alignments of entries (Navigli, 2006; Toral et al., 2009; Navigli and Ponzetto, 2012). The WordNet (WN) Domains (Magnini and Cavaglia, 2000; Bentivogli et al., 2004) have been selected as reference domain labels. We will use as candidate lexico-semantic resources to be aligned two Italian lexica, namely, MultiWordNet (MWN) and the Senso Comune De Mauro Lexicon (SCDM) (Vetere et al., 2011). The two resources differ in terms of modelization: the former, MWN, is an Italian version of WN obtained through the “expand model” (Vossen, 1996) and perfectly aligned to Princeton WN 1.6, while the latter, SCDM, is a machine readable dictionary obtained from a paper-based reference lexicographic dictionary, De Mauro GRADIT. Major issues for WSA of the lexica concern t</context>
</contexts>
<marker>Bentivogli, Forner, Magnini, Pianta, 2004</marker>
<rawString>Luisa Bentivogli, Pamela Forner, Bernardo Magnini, and Emanuele Pianta. 2004. Revising the wordnet domains hierarchy: semantics, coverage and balancing. In Proceedings of the Workshop on Multilingual Linguistic Resources, pages 101–108. Association for Computational Linguistics.</rawString>
</citation>
<citation valid="true">
<authors>
<author>Tommaso Caselli</author>
<author>Carlo Strapparava</author>
<author>Laure Vieu</author>
<author>Guido Vetere</author>
</authors>
<title>Aligning an italianwordnet with a lexicographic dictionary: Coping with limited data.</title>
<date>2014</date>
<booktitle>In Proceedings of the Seventh Global WordNet Conference,</booktitle>
<pages>290--298</pages>
<contexts>
<context position="6089" citStr="Caselli et al., 2014" startWordPosition="973" endWordPosition="976"> the OTHER value includes all remaining 44 normalized domains. After this, all entries classified with the OTHER value are analyzed by a rule based system and associated with a specific domain label (i.e. SPORT, MEDICINE, FOOD ... ). 2.1 Classifier and feature selection We have developed a training set by manually aligning noun senses between the two lexica. The sense alignment allows us to associate all the information of a synset to a corresponding entry in the SCDM lexicon, including the WN Domain label. Concerning the test set, we have used an existing dataset of aligned noun pairs as in (Caselli et al., 2014). We report in Table 1 the figures for the training and test sets. Multiple alignments with the same domain label have been excluded from the training set. Characteristics Training Set Test Set # lemmas 131 46 # of aligned pairs 369 166 # of SCDM senses 747 216 # of MWN synsets 675 229 # SCDM with WN Domain label 350 118 Table 1: Training and test sets for the classifier. In order for the classifier to predict the binary domain labels (FACTOTUM and OTHER), each sense description of the SCDM Lexicon has been represented by means of a two-dimensional feature vector (e.g. for training data: BINAR</context>
<context position="12632" citStr="Caselli et al., 2014" startWordPosition="2075" endWordPosition="2078">elinelemma and with a * for the Baselinewsd FACTOTUM class with respect to the baseline. In particular, the accuracy of the hybrid system on this class is 79% while that of the baseline is only 65%. In addition to this, the hybrid system provides better results in terms of Recall (R=0.50 vs. R=0.43). Although comparable, the hybrid system provides more accurate results with respect to the baseline. 3 Domain Filtering for WSA This section reports on the experiments for improving existing WSA for nouns between SDCM and MWN. In this work we have used the same dataset and alignment methods as in (Caselli et al., 2014), shortly described here: • Lexical Match: for each word w and for each sense s in the given resources R E {MWN, SCDM}, we constructed a sense descriptions dR(s) as a bag of words in Italian. The alignment is based on counting the number of overlapping tokens between the two strings, normalized by the length of the strings; • Cosine Similarity: we used the Personalized Page Rank (PPR) algorithm (Agirre et al., 2010) with WN 3.0 as knowledge base extended with the “Princeton Annotated Gloss Corpus”. Once the PPR vector pairs are obtained, the alignment is obtained on the basis of the cosine sco</context>
<context position="14594" citStr="Caselli et al., 2014" startWordPosition="2412" endWordPosition="2415">obtained by, first, applying Google Translate API to get the English translations and, then, PPR over WN 3.0. 416 System P R F1 LexicalMatch 0.76 (0.69) 0.27 (0.44) 0.40 (0.55) Cosine noThreshold 0.27 (0.12) 0.47 (0.94) 0.35 (0.21) Cosine &gt; 0.1 0.77 (0.52) 0.21 (0.32) 0.33 (0.40) Cosine &gt; 0.2 0.87 (0.77) 0.14 (0.21) 0.24 (0.33) LexicalMatch+Cosine &gt; 0.1 0.73 (na) 0.40 (na) 0.51 (na) LexicalMatch+Cosine &gt; 0.2 0.77 (0.67) 0.37 (0.61) 0.50 (0.64) Table 4: Results for WSA of nouns with domain filtering. the results of the WSA approaches with domain filters. We report in brackets the results from (Caselli et al., 2014). The filtering based on WN Domains has a big impact on Precision and contributes to increase the quality of the aligned senses. Although, in general, we have a downgrading of the performance with respect to Recall, the increase in Precision will reduce the manual post-processing effort to fully aligned the two resources5. Furthermore, it is interesting to notice that, when merging together the results of the pre-filtered alignments from the two alignment approaches (LexicalMatch+Cosine &gt; 0.1 and LexicalMatch+Cosine &gt; 0.2), we still have a very high Precision (&gt; 0.70) and an increase in Recall</context>
<context position="16766" citStr="Caselli et al., 2014" startWordPosition="2768" endWordPosition="2771">igned pairs; ii.) the method we have proposed is a viable approach for automatically enriching existing lexical resources in a reliable way; and iii.) the ODD hypothesis also apply to sense descriptions. An advantage of our approach is its simplicity. We have used features based on frequency counts and obtained good results, with a Precision of 0.70 for automatic WN Domain assignment. Nevertheless, an important role is played by Word Sense Disambiguation. The use of domain labels obtained from sense disambiguated lemmas improves both the results of the classifier and those 5The F1 of 0.64 in (Caselli et al., 2014) is obtained with a Precision of 0.67, suggesting that some alignments are false positives of the rules. The absence of statistical significance with respect to the Baselinewsd is not to be considered as a negative result. As the error analysis has showed, the classifier mostly contributes to the identification of the FACTOTUM value, which tends to be overestimated even with sense disambiguated lemmas, and to Recall. We are planning to extend this work to include domain clusters to improve the domain assignment results, namely in terms of Recall. Acknowledgments One of the author wants to than</context>
</contexts>
<marker>Caselli, Strapparava, Vieu, Vetere, 2014</marker>
<rawString>Tommaso Caselli, Carlo Strapparava, Laure Vieu, and Guido Vetere. 2014. Aligning an italianwordnet with a lexicographic dictionary: Coping with limited data. In Proceedings of the Seventh Global WordNet Conference, pages 290–298.</rawString>
</citation>
<citation valid="true">
<authors>
<author>Christiane Fellbaum</author>
</authors>
<title>WordNet: An Electronic Lexical Database (Language, Speech, and Communication).</title>
<date>1998</date>
<publisher>MIT Press.</publisher>
<contexts>
<context position="1258" citStr="Fellbaum, 1998" startWordPosition="193" endWordPosition="194"> word sense alignments between MultiWordNet and Senso Comune. This has led to an improvement in the quality of the sense alignments showing the validity of the approach for domain assignment and the importance of domain information for achieving good sense alignments. 1 Introduction and Problem Statement Lexical knowledge, i.e. how words are used and express meaning, plays a key role in Natural Language Processing. Lexical knowledge is available in many different forms, ranging from unstructured terminologies (i.e. word list), to full fledged computational lexica and ontologies (e.g. WordNet (Fellbaum, 1998)). The process of creation of lexical resources is costly both in terms of money and time. To overcome these limits, semi-automatic approaches have been developed (e.g. MultiWordNet (Pianta et al., 2002)) with different levels of success. Furthermore, important information is scattered in different resources and difficult to use. Semantic interoperability between resources could represent a viable solution to allow reusability and develop more robust and powerful resources. Word sense alignment (WSA) qualifies as the preliminary requirement for achieving this goal (Matuschek and Gurevych, 2013</context>
</contexts>
<marker>Fellbaum, 1998</marker>
<rawString>Christiane Fellbaum. 1998. WordNet: An Electronic Lexical Database (Language, Speech, and Communication). MIT Press.</rawString>
</citation>
<citation valid="true">
<authors>
<author>Bernardo Magnini</author>
<author>Gabriela Cavaglia</author>
</authors>
<title>Integrating subject field codes into wordnet.</title>
<date>2000</date>
<booktitle>In Proceedings of the conference on International Language Resources and Evaluation (LREC</booktitle>
<contexts>
<context position="2779" citStr="Magnini and Cavaglia, 2000" startWordPosition="428" endWordPosition="431">glosses); and ii.) the extension of the basic sense descriptions with additional information such as hypernyms, synonyms and domain or category labels. The purpose of this work is two folded: first, we experiment on the automatic assignment of domain labels to sense descriptions, and then, evaluate the impact of this information for improving an existing sense aligned dataset for nouns. Previous works has demonstrated that domain labels are a good feature for obtaining high quality alignments of entries (Navigli, 2006; Toral et al., 2009; Navigli and Ponzetto, 2012). The WordNet (WN) Domains (Magnini and Cavaglia, 2000; Bentivogli et al., 2004) have been selected as reference domain labels. We will use as candidate lexico-semantic resources to be aligned two Italian lexica, namely, MultiWordNet (MWN) and the Senso Comune De Mauro Lexicon (SCDM) (Vetere et al., 2011). The two resources differ in terms of modelization: the former, MWN, is an Italian version of WN obtained through the “expand model” (Vossen, 1996) and perfectly aligned to Princeton WN 1.6, while the latter, SCDM, is a machine readable dictionary obtained from a paper-based reference lexicographic dictionary, De Mauro GRADIT. Major issues for W</context>
</contexts>
<marker>Magnini, Cavaglia, 2000</marker>
<rawString>Bernardo Magnini and Gabriela Cavaglia. 2000. Integrating subject field codes into wordnet. In Proceedings of the conference on International Language Resources and Evaluation (LREC 2000).</rawString>
</citation>
<citation valid="true">
<authors>
<author>Bernardo Magnini</author>
<author>Carlo Strapparava</author>
<author>Giovanni Pezzulo</author>
<author>Alfio Gliozzo</author>
</authors>
<title>Using domain information for word sense disambiguation.</title>
<date>2001</date>
<booktitle>In The Proceedings of the Second International Workshop on Evaluating Word Sense Disambiguation Systems,</booktitle>
<pages>111--114</pages>
<publisher>Association</publisher>
<institution>for Computational Linguistics.</institution>
<contexts>
<context position="5244" citStr="Magnini et al., 2001" startWordPosition="827" endWordPosition="830">66 MaxEntlemma 0.70 0.49 0.58 0.63 NaiveBayeswsd 0.77 0.58 0.66 0.69 MaxEntwsd 0.74 0.54 0.62 0.67 Table 2: Results for the Naive Bayes and Maximum Entropy binary classifiers. synset1 and express a subject field label (e.g. SPORT, MEDICINE). A special label, FACTOTUM, has been used for those synsets which can appear in almost all subject fields. The identification of a domain label to the nominal entries in the SCDM Lexicon is based the “One Domain per Discourse” (ODD) hypothesis applied to the sense descriptions. We have used a reduced set of domains labels (45 normalized domains) following (Magnini et al., 2001). To assign the WN domain label to the SCDM entries, we have developed a hybrid method: first a binary classifier is applied to the SCDM sense descriptions to discriminate between two domain values, FACTOTUM and OTHER, where the OTHER value includes all remaining 44 normalized domains. After this, all entries classified with the OTHER value are analyzed by a rule based system and associated with a specific domain label (i.e. SPORT, MEDICINE, FOOD ... ). 2.1 Classifier and feature selection We have developed a training set by manually aligning noun senses between the two lexica. The sense align</context>
</contexts>
<marker>Magnini, Strapparava, Pezzulo, Gliozzo, 2001</marker>
<rawString>Bernardo Magnini, Carlo Strapparava, Giovanni Pezzulo, and Alfio Gliozzo. 2001. Using domain information for word sense disambiguation. In The Proceedings of the Second International Workshop on Evaluating Word Sense Disambiguation Systems, pages 111–114. Association for Computational Linguistics.</rawString>
</citation>
<citation valid="true">
<authors>
<author>Michael Matuschek</author>
<author>Iryna Gurevych</author>
</authors>
<title>Dijkstra-wsa: A graph-based approach to word sense alignment.</title>
<date>2013</date>
<journal>Transactions of the Association for Computational Linguistics (TACL),</journal>
<note>2:to appear.</note>
<contexts>
<context position="1859" citStr="Matuschek and Gurevych, 2013" startWordPosition="280" endWordPosition="283">.g. WordNet (Fellbaum, 1998)). The process of creation of lexical resources is costly both in terms of money and time. To overcome these limits, semi-automatic approaches have been developed (e.g. MultiWordNet (Pianta et al., 2002)) with different levels of success. Furthermore, important information is scattered in different resources and difficult to use. Semantic interoperability between resources could represent a viable solution to allow reusability and develop more robust and powerful resources. Word sense alignment (WSA) qualifies as the preliminary requirement for achieving this goal (Matuschek and Gurevych, 2013). WSA aims at creating lists of pairs of senses from two, or more, (lexical-semantic) resources which denote the same meaning. Different approaches to WSA have been proposed and they all share some common elements, namely: i.) the extensive use of sense descriptions of the words (e.g. WordNet glosses); and ii.) the extension of the basic sense descriptions with additional information such as hypernyms, synonyms and domain or category labels. The purpose of this work is two folded: first, we experiment on the automatic assignment of domain labels to sense descriptions, and then, evaluate the im</context>
</contexts>
<marker>Matuschek, Gurevych, 2013</marker>
<rawString>Michael Matuschek and Iryna Gurevych. 2013. Dijkstra-wsa: A graph-based approach to word sense alignment. Transactions of the Association for Computational Linguistics (TACL), 2:to appear.</rawString>
</citation>
<citation valid="true">
<authors>
<author>Andrew Kachites McCallum</author>
</authors>
<title>Mallet: A machine learning for language toolkit.</title>
<date>2002</date>
<contexts>
<context position="7929" citStr="McCallum, 2002" startWordPosition="1285" endWordPosition="1286">ense description, we have first performed Word Sense Disambiguation by means of an adapted version to Italian of the UKB package2 (Agirre et al., 2010; Agirre et al., 2014)3. Only the highest ranked synset, and associated WN Domain(s), was retained as good. Similarly to the lemma label strategy, the sum of the domain label FACTOTUM is assigned to the feature GENERIC, while the sum of all other domain labels collapsed into the single value NOTFACTOTUM is assigned to the feature SPECIFIC. We experimented with two classifiers: Naive Bayes and Maximum Entropy as implemented in the MALLET package (McCallum, 2002). We illustrate the results in Table 2. The classifiers have been evaluated with respect to standard Precision (P), Recall (R) and F1 against the test set. Ten-fold cross validation has been performed on the training set as well. Classifiers trained with the first strategy will be associated with the label lemma, while those trained with the second strategy with the label wsd. Both classifiers obtains good results with respect to the test data in terms of Precision and Recall. The Naive Bayes classifier outperforms the Maximum Entropy one in both training approaches, suggesting better generali</context>
</contexts>
<marker>McCallum, 2002</marker>
<rawString>Andrew Kachites McCallum. 2002. Mallet: A machine learning for language toolkit.</rawString>
</citation>
<citation valid="true">
<authors>
<author>Rada Mihalcea</author>
</authors>
<title>Using Wikipedia for automatic word sense disambiguation.</title>
<date>2007</date>
<booktitle>In Human Language Technologies 2007: The Conference of the North American Chapter of the Association for Computational Linguistics; Proceedings of the Main Conference,</booktitle>
<location>Rochester, New York.</location>
<marker>Mihalcea, 2007</marker>
<rawString>Rada Mihalcea. 2007. Using Wikipedia for automatic word sense disambiguation. In Human Language Technologies 2007: The Conference of the North American Chapter of the Association for Computational Linguistics; Proceedings of the Main Conference, Rochester, New York.</rawString>
</citation>
<citation valid="true">
<authors>
<author>Roberto Navigli</author>
<author>Simone Paolo Ponzetto</author>
</authors>
<title>BabelNet: The automatic construction, evaluation and application of a wide-coverage multilingual semantic network.</title>
<date>2012</date>
<journal>Artificial Intelligence,</journal>
<volume>193</volume>
<pages>250</pages>
<contexts>
<context position="2725" citStr="Navigli and Ponzetto, 2012" startWordPosition="419" endWordPosition="422">e use of sense descriptions of the words (e.g. WordNet glosses); and ii.) the extension of the basic sense descriptions with additional information such as hypernyms, synonyms and domain or category labels. The purpose of this work is two folded: first, we experiment on the automatic assignment of domain labels to sense descriptions, and then, evaluate the impact of this information for improving an existing sense aligned dataset for nouns. Previous works has demonstrated that domain labels are a good feature for obtaining high quality alignments of entries (Navigli, 2006; Toral et al., 2009; Navigli and Ponzetto, 2012). The WordNet (WN) Domains (Magnini and Cavaglia, 2000; Bentivogli et al., 2004) have been selected as reference domain labels. We will use as candidate lexico-semantic resources to be aligned two Italian lexica, namely, MultiWordNet (MWN) and the Senso Comune De Mauro Lexicon (SCDM) (Vetere et al., 2011). The two resources differ in terms of modelization: the former, MWN, is an Italian version of WN obtained through the “expand model” (Vossen, 1996) and perfectly aligned to Princeton WN 1.6, while the latter, SCDM, is a machine readable dictionary obtained from a paper-based reference lexicog</context>
</contexts>
<marker>Navigli, Ponzetto, 2012</marker>
<rawString>Roberto Navigli and Simone Paolo Ponzetto. 2012. BabelNet: The automatic construction, evaluation and application of a wide-coverage multilingual semantic network. Artificial Intelligence, 193:217– 250.</rawString>
</citation>
<citation valid="true">
<authors>
<author>Roberto Navigli</author>
</authors>
<title>Meaningful clustering of senses helps boost word sense disambiguation performance.</title>
<date>2006</date>
<booktitle>In Proceedings of the 44th Annual Meeting of the Association for Computational Linguistics joint with the 21st International Conference on Computational Linguistics (COLING-ACL),</booktitle>
<location>Sydney, Australia.</location>
<contexts>
<context position="2676" citStr="Navigli, 2006" startWordPosition="413" endWordPosition="414"> elements, namely: i.) the extensive use of sense descriptions of the words (e.g. WordNet glosses); and ii.) the extension of the basic sense descriptions with additional information such as hypernyms, synonyms and domain or category labels. The purpose of this work is two folded: first, we experiment on the automatic assignment of domain labels to sense descriptions, and then, evaluate the impact of this information for improving an existing sense aligned dataset for nouns. Previous works has demonstrated that domain labels are a good feature for obtaining high quality alignments of entries (Navigli, 2006; Toral et al., 2009; Navigli and Ponzetto, 2012). The WordNet (WN) Domains (Magnini and Cavaglia, 2000; Bentivogli et al., 2004) have been selected as reference domain labels. We will use as candidate lexico-semantic resources to be aligned two Italian lexica, namely, MultiWordNet (MWN) and the Senso Comune De Mauro Lexicon (SCDM) (Vetere et al., 2011). The two resources differ in terms of modelization: the former, MWN, is an Italian version of WN obtained through the “expand model” (Vossen, 1996) and perfectly aligned to Princeton WN 1.6, while the latter, SCDM, is a machine readable diction</context>
</contexts>
<marker>Navigli, 2006</marker>
<rawString>Roberto Navigli. 2006. Meaningful clustering of senses helps boost word sense disambiguation performance. In Proceedings of the 44th Annual Meeting of the Association for Computational Linguistics joint with the 21st International Conference on Computational Linguistics (COLING-ACL), Sydney, Australia.</rawString>
</citation>
<citation valid="true">
<authors>
<author>Elisabeth Niemann</author>
<author>Iryna Gurevych</author>
</authors>
<title>The peoples web meets linguistic knowledge: Automatic sense alignment of Wikipedia and WordNet.</title>
<date>2011</date>
<booktitle>In Proceedings of the 9th International Conference on Computational Semantics,</booktitle>
<pages>205--214</pages>
<marker>Niemann, Gurevych, 2011</marker>
<rawString>Elisabeth Niemann and Iryna Gurevych. 2011. The peoples web meets linguistic knowledge: Automatic sense alignment of Wikipedia and WordNet. In Proceedings of the 9th International Conference on Computational Semantics, pages 205–214, Singapore, January.</rawString>
</citation>
<citation valid="true">
<authors>
<author>Emanuele Pianta</author>
<author>Luisa Bentivogli</author>
<author>Cristian Girardi</author>
</authors>
<title>MultiWordNet: developing an aligned multilingual database.</title>
<date>2002</date>
<booktitle>In First International Conference on Global WordNet, Mysore,</booktitle>
<contexts>
<context position="1461" citStr="Pianta et al., 2002" startWordPosition="223" endWordPosition="226"> importance of domain information for achieving good sense alignments. 1 Introduction and Problem Statement Lexical knowledge, i.e. how words are used and express meaning, plays a key role in Natural Language Processing. Lexical knowledge is available in many different forms, ranging from unstructured terminologies (i.e. word list), to full fledged computational lexica and ontologies (e.g. WordNet (Fellbaum, 1998)). The process of creation of lexical resources is costly both in terms of money and time. To overcome these limits, semi-automatic approaches have been developed (e.g. MultiWordNet (Pianta et al., 2002)) with different levels of success. Furthermore, important information is scattered in different resources and difficult to use. Semantic interoperability between resources could represent a viable solution to allow reusability and develop more robust and powerful resources. Word sense alignment (WSA) qualifies as the preliminary requirement for achieving this goal (Matuschek and Gurevych, 2013). WSA aims at creating lists of pairs of senses from two, or more, (lexical-semantic) resources which denote the same meaning. Different approaches to WSA have been proposed and they all share some comm</context>
</contexts>
<marker>Pianta, Bentivogli, Girardi, 2002</marker>
<rawString>Emanuele Pianta, Luisa Bentivogli, and Cristian Girardi. 2002. MultiWordNet: developing an aligned multilingual database. In First International Conference on Global WordNet, Mysore, India.</rawString>
</citation>
<citation valid="true">
<authors>
<author>German Rigau</author>
<author>Agirre Eneko</author>
</authors>
<title>Disambiguating bilingual nominal entries against WordNet.</title>
<date>1995</date>
<booktitle>In Proceedings of workshop The Computational Lexicon, 7th European Summer School in Logic, Language and Information,</booktitle>
<location>Barcelona,</location>
<marker>Rigau, Eneko, 1995</marker>
<rawString>German Rigau and Agirre Eneko. 1995. Disambiguating bilingual nominal entries against WordNet. In Proceedings of workshop The Computational Lexicon, 7th European Summer School in Logic, Language and Information, Barcelona, Spain.</rawString>
</citation>
<citation valid="true">
<authors>
<author>Adriana Roventini</author>
<author>Nilda Ruimy</author>
<author>Rita Marinelli</author>
<author>Marisa Ulivieri</author>
<author>Michele Mammini</author>
</authors>
<title>Mapping concrete entities from PAROLE-SIMPLECLIPS to ItalWordNet: Methodology and results.</title>
<date>2007</date>
<booktitle>In Proceedings of the 45th Annual Meeting of the Association for Computational Linguistics Companion Volume Proceedings of the Demo and Poster Sessions,</booktitle>
<location>Prague, Czech Republic,</location>
<marker>Roventini, Ruimy, Marinelli, Ulivieri, Mammini, 2007</marker>
<rawString>Adriana Roventini, Nilda Ruimy, Rita Marinelli, Marisa Ulivieri, and Michele Mammini. 2007. Mapping concrete entities from PAROLE-SIMPLECLIPS to ItalWordNet: Methodology and results. In Proceedings of the 45th Annual Meeting of the Association for Computational Linguistics Companion Volume Proceedings of the Demo and Poster Sessions, Prague, Czech Republic, June.</rawString>
</citation>
<citation valid="true">
<authors>
<author>Maria Ruiz-Casado</author>
<author>Enrique Alfonseca</author>
<author>Pablo Castells</author>
</authors>
<title>Automatic assignment of Wikipedia encyclopedic entries to WordNet synsets.</title>
<date>2005</date>
<booktitle>In Proceedings of the Third international conference on Advances in Web Intelligence, AWIC’05,</booktitle>
<publisher>Springer-Verlag.</publisher>
<location>Berlin, Heidelberg.</location>
<marker>Ruiz-Casado, Alfonseca, Castells, 2005</marker>
<rawString>Maria Ruiz-Casado, Enrique Alfonseca, and Pablo Castells. 2005. Automatic assignment of Wikipedia encyclopedic entries to WordNet synsets. In Proceedings of the Third international conference on Advances in Web Intelligence, AWIC’05, Berlin, Heidelberg. Springer-Verlag.</rawString>
</citation>
<citation valid="true">
<authors>
<author>Antonio Toral</author>
<author>Oscar Ferr´andez</author>
<author>Eneko Aguirre</author>
<author>Rafael Munoz</author>
</authors>
<title>A study on linking and disambiguating wikipedia categories to wordnet using text similarity.</title>
<date>2009</date>
<booktitle>Proceedings of the International Conference on Recent Advances in Natural Language Processing (RANLP</booktitle>
<marker>Toral, Ferr´andez, Aguirre, Munoz, 2009</marker>
<rawString>Antonio Toral, Oscar Ferr´andez, Eneko Aguirre, and Rafael Munoz. 2009. A study on linking and disambiguating wikipedia categories to wordnet using text similarity. Proceedings of the International Conference on Recent Advances in Natural Language Processing (RANLP 2009).</rawString>
</citation>
<citation valid="true">
<authors>
<author>Guido Vetere</author>
<author>Alessandro Oltramari</author>
<author>Isabella Chiari</author>
<author>Elisabetta Jezek</author>
<author>Laure Vieu</author>
<author>Fabio Massimo Zanzotto</author>
</authors>
<title>Senso Comune, an open knowledge base for italian. JTraitement Automatique des Langues,</title>
<date>2011</date>
<pages>53--3</pages>
<contexts>
<context position="3031" citStr="Vetere et al., 2011" startWordPosition="470" endWordPosition="473">to sense descriptions, and then, evaluate the impact of this information for improving an existing sense aligned dataset for nouns. Previous works has demonstrated that domain labels are a good feature for obtaining high quality alignments of entries (Navigli, 2006; Toral et al., 2009; Navigli and Ponzetto, 2012). The WordNet (WN) Domains (Magnini and Cavaglia, 2000; Bentivogli et al., 2004) have been selected as reference domain labels. We will use as candidate lexico-semantic resources to be aligned two Italian lexica, namely, MultiWordNet (MWN) and the Senso Comune De Mauro Lexicon (SCDM) (Vetere et al., 2011). The two resources differ in terms of modelization: the former, MWN, is an Italian version of WN obtained through the “expand model” (Vossen, 1996) and perfectly aligned to Princeton WN 1.6, while the latter, SCDM, is a machine readable dictionary obtained from a paper-based reference lexicographic dictionary, De Mauro GRADIT. Major issues for WSA of the lexica concern the following aspects: • SCMD has no structure of word senses (i.e. no taxonomy, no synonymy relations, no distinction between core senses and subsenses for polysemous entries) unlike MWN; • SCDM has no domain or category label</context>
</contexts>
<marker>Vetere, Oltramari, Chiari, Jezek, Vieu, Zanzotto, 2011</marker>
<rawString>Guido Vetere, Alessandro Oltramari, Isabella Chiari, Elisabetta Jezek, Laure Vieu, and Fabio Massimo Zanzotto. 2011. Senso Comune, an open knowledge base for italian. JTraitement Automatique des Langues, 53(3):217–243.</rawString>
</citation>
<citation valid="true">
<authors>
<author>Piek Vossen</author>
</authors>
<title>Right or wrong: Combining lexical resources in the eurowordnet project.</title>
<date>1996</date>
<booktitle>In Euralex,</booktitle>
<volume>96</volume>
<pages>715--728</pages>
<publisher>Citeseer.</publisher>
<contexts>
<context position="3179" citStr="Vossen, 1996" startWordPosition="496" endWordPosition="497">nstrated that domain labels are a good feature for obtaining high quality alignments of entries (Navigli, 2006; Toral et al., 2009; Navigli and Ponzetto, 2012). The WordNet (WN) Domains (Magnini and Cavaglia, 2000; Bentivogli et al., 2004) have been selected as reference domain labels. We will use as candidate lexico-semantic resources to be aligned two Italian lexica, namely, MultiWordNet (MWN) and the Senso Comune De Mauro Lexicon (SCDM) (Vetere et al., 2011). The two resources differ in terms of modelization: the former, MWN, is an Italian version of WN obtained through the “expand model” (Vossen, 1996) and perfectly aligned to Princeton WN 1.6, while the latter, SCDM, is a machine readable dictionary obtained from a paper-based reference lexicographic dictionary, De Mauro GRADIT. Major issues for WSA of the lexica concern the following aspects: • SCMD has no structure of word senses (i.e. no taxonomy, no synonymy relations, no distinction between core senses and subsenses for polysemous entries) unlike MWN; • SCDM has no domain or category labels associated to senses (with the exception of specific terminological entries) unlike MWN; • the Italian section of MWN has only 2,481 glosses in It</context>
</contexts>
<marker>Vossen, 1996</marker>
<rawString>Piek Vossen. 1996. Right or wrong: Combining lexical resources in the eurowordnet project. In Euralex, volume 96, pages 715–728. Citeseer.</rawString>
</citation>
</citationList>
</algorithm>
</algorithms>
<?xml version="1.0" encoding="UTF-8"?>
<algorithms version="110505">
<algorithm name="SectLabel" version="110505">
<variant no="0" confidence="0.000269">
<title confidence="0.978324">
Learning Hierarchical Translation Spans
</title>
<author confidence="0.989284">
Jingyi Zhang&apos;,2, Masao Utiyama3, Eiichro Sumita3, Hai Zhao&apos;,2
</author>
<affiliation confidence="0.93675">
&apos;Center for Brain-Like Computing and Machine Intelligence, Department of Computer
Science and Engineering, Shanghai Jiao Tong Unviersity, Shanghai, 200240, China
2Key Laboratory of Shanghai Education Commission for Intelligent Interaction
and Cognitive Engineering, Shanghai Jiao Tong Unviersity, Shanghai, 200240, China
3National Institute of Information and Communications Technology
3-5Hikaridai, Keihanna Science City, Kyoto, 619-0289, Japan
</affiliation>
<email confidence="0.959371">
zhangjingyizz@gmail.com, mutiyama/eiichiro.sumita@nict.go.jp,
zhaohai@cs.sjtu.edu.cn
</email>
<sectionHeader confidence="0.998543" genericHeader="abstract">
Abstract
</sectionHeader>
<bodyText confidence="0.9999325">
We propose a simple and effective ap-
proach to learn translation spans for
the hierarchical phrase-based translation
model. Our model evaluates if a source
span should be covered by translation
rules during decoding, which is integrated
into the translation system as soft con-
straints. Compared to syntactic con-
straints, our model is directly acquired
from an aligned parallel corpus and does
not require parsers. Rich source side
contextual features and advanced machine
learning methods were utilized for this
learning task. The proposed approach was
evaluated on NTCIR-9 Chinese-English
and Japanese-English translation tasks and
showed significant improvement over the
baseline system.
</bodyText>
<sectionHeader confidence="0.999517" genericHeader="introduction">
1 Introduction
</sectionHeader>
<bodyText confidence="0.9999346875">
The hierarchical phrase-based (HPB) translation
model (Chiang, 2005) has been widely adopted in
statistical machine translation (SMT) tasks. The
HPB translation rules based on the synchronous
context free grammar (SCFG) are simple and pow-
erful.
One drawback of the HPB model is the appli-
cations of translation rules to the input sentence
are highly ambiguous. For example, a rule whose
English side is “X1 by X2” can be applied to any
word sequence that has “by” in them. In Figure 1,
this rule can be applied to the whole sentence as
well as to “experiment by tomorrow”.
In order to tackle rule application ambiguities,
a few previous works used syntax trees. Chi-
ang (2005) utilized a syntactic feature in the HPB
</bodyText>
<figure confidence="0.4457125">
I will finish this experiment by tomorrow
- � ;0-: HJJ)Q ZU 512GIA iZk/l` �R
</figure>
<figureCaption confidence="0.999622">
Figure 1: A translation example.
</figureCaption>
<bodyText confidence="0.999561166666667">
model, which represents if the source span cov-
ered by a translation rule is a syntactic constituent.
However, the experimental results showed this
feature gave no significant improvement. Instead
of using the undifferentiated constituency feature,
(Marton and Resnik, 2008) defined different soft
syntactic features for different constituent types
and obtained substantial performance improve-
ment. Later, (Mylonakis and Sima’an, 2011) in-
troduced joint probability synchronous grammars
to integrate flexible linguistic information. (Liu
et al., 2011) proposed the soft syntactic constraint
model based on discriminative classifiers for each
constituent type and integrated all of them into the
translation model. (Cui et al., 2010) focused on
hierarchical rule selection using many features in-
cluding syntax constituents.
These works have demonstrated the benefits of
using syntactic features in the HPB model. How-
ever, high quality syntax parsers are not always
easily obtained for many languages. Without this
problem, word alignment constraints can also be
used to guide the application of the rules.
Suppose that we want to translate the English
sentence into the Chinese sentence in Figure 1, a
translation rule can be applied to the source span
“finish this experiment by tomorrow”. Nonethe-
less, if a rule is applied to “experiment by”, then
the Chinese translation can not be correctly ob-
tained, because the target span projected from “ex-
</bodyText>
<page confidence="0.983408">
183
</page>
<bodyText confidence="0.976697133333333">
Proceedings of the 2014 Conference on Empirical Methods in Natural Language Processing (EMNLP), pages 183–188,
October 25-29, 2014, Doha, Qatar. c�2014 Association for Computational Linguistics
periment by” contains words projected from the
source words outside “experiment by”.
In general, a translation rule projects one con-
tinuous source word sequence (source span) into
one continuous target word sequence. Meanwhile,
the word alignment links between the source and
target sentence define the source spans where
translation rules are applicable. In this paper, we
call a source span that can be covered by a trans-
lation rule without violating word alignment links
a translation span.
Translation spans that have been correctly iden-
tified can guide translation rules to function prop-
erly, thus (Xiong et al., 2010) attempted to use
extra machine learning approaches to determine
boundaries of translation spans. They used two
separate classifiers to learn the beginning and end-
ing boundaries of translation spans, respectively.
A source word is marked as beginning (ending)
boundary if it is the first (last) word of a translation
span. However, a source span whose first and last
words are both boundaries is not always a transla-
tion span. In Figure 1, “I” is a beginning boundary
since it is the first word of translation span “I will”
and “experiment” is an ending boundary since it is
the last word of translation span “finish this exper-
iment” , but “I will finish this experiment” is not a
translation span. This happens because the trans-
lation spans are nested or hierarchical. Note that
(He et al., 2010) also learned phrase boundaries to
constrain decoding, but their approach identified
boundaries only for monotone translation.
In this paper, taking fully into account that
translation spans being nested, we propose an
approach to learn hierarchical translation spans
directly from an aligned parallel corpus that
makes more accurate identification over transla-
tion spans.
The rest of the paper is structured as follows:
In Section 2, we briefly review the HPB transla-
tion model. Section 3 describes our approach. We
describe experiments in Section 4 and conclude in
Section 5.
</bodyText>
<sectionHeader confidence="0.948998" genericHeader="method">
2 Hierarchical Phrase-based Translation
</sectionHeader>
<bodyText confidence="0.949387833333333">
Chiang’s HPB model is based on a weighted
SCFG. A translation rule is like: X —* (-y, α, —),
where X is a nonterminal, -y and α are source and
target strings of terminals and nonterminals, and —
is a one-to-one correspondence between nontermi-
nals in -y and α. The weight of each rule is:
</bodyText>
<equation confidence="0.983611">
�w (X → hy, α, ∼i) =
t
</equation>
<bodyText confidence="0.984791125">
where ht are the features defined on the rules.
Rewriting begins with a pair of linked start sym-
bols and ends when there is no nonterminal left.
Let D be a derivation of the grammar, f (D) and
e (D) be the source and target strings generated
by D. D consists of a set of triples (r, i, j), each
of which stands for applying a rule r on a span
f (D)ji . The weight of D is calculated as:
</bodyText>
<equation confidence="0.983978666666667">
w (D) = � w (r) × Plm(e)λ&amp;quot;— × exp (−Awp |e|)
(r,i,j)∈D
(2)
</equation>
<bodyText confidence="0.999882333333333">
where w (r) is the weight of rule r, the last two
terms represent the language model and word
penalty, respectively.
</bodyText>
<sectionHeader confidence="0.970047" genericHeader="method">
3 Learning Translation Spans
</sectionHeader>
<bodyText confidence="0.9998895">
We will describe how to learn translation spans in
this section.
</bodyText>
<subsectionHeader confidence="0.991782">
3.1 Our Model
</subsectionHeader>
<bodyText confidence="0.974061913043478">
We make a series of binary classifiers
{C1, C2, C3, ...J to learn if a source span
f (D)ji should be covered by translation rules dur-
ing translation. Ck is trained and tested on source
spans whose lengths are k, i.e., k = j — i + 1.1
Ck learns the probability
Pk (v|f (D) , i, j) (3)
where v E 10, 11, v = 1 represents a rule is ap-
plied on f (D)ji, otherwise v = 0.
Training instances for these classifiers are ex-
tracted from an aligned parallel corpus according
to Algorithm 1. For example, “I will” and “will
finish” are respectively extracted as positive and
negative instances in Figure 1.
Note that our model in Equation 3 only uses
the source sentence f (D) in the condition. This
means that the probabilities can be calculated be-
fore translation. Therefore, the predicted prob-
abilities can be integrated into the decoder con-
veniently as soft constraints and no extra time is
added during decoding. This enables us to use
rich source contextual features and various ma-
chine learning methods for this learning task.
</bodyText>
<footnote confidence="0.892883833333333">
1We indeed can utilize just one classifier for all source
spans. However, it will be difficult to design features for such
a classifier unless only boundary word features are adopted.
On the contrary, we can fully take advantage of rich informa-
tion about inside words as we turn to the fixed span length
approach.
</footnote>
<equation confidence="0.912265">
ht(X → hy, α, ∼i)λ` (1)
</equation>
<page confidence="0.992949">
184
</page>
<subsectionHeader confidence="0.987118">
3.2 Integration into the decoder
</subsectionHeader>
<bodyText confidence="0.9935255">
It is straightforward to integrate our model into
Equation 2. It is extended as
</bodyText>
<equation confidence="0.99836425">
w (D) = rl w (r) x Plm(e)alm x exp (−Awp |e|)
(r,i,j)∈D
x Pk(v = 1|f (D) , i, j)ak
(4)
</equation>
<bodyText confidence="0.93518925">
where λk is the weight for Ck.
During decoding, the decoder looks up the
probabilities Pk calculated and stored before de-
coding.
Algorithm 1 Extract training instances.
Input: A pair of parallel sentence fn1 and em1 with
word alignments A.
Output: Training examples for {C1, C2, C3, ...}.
</bodyText>
<listItem confidence="0.964366">
1: for i = 1 to n do
2: for j = i to n do
</listItem>
<figure confidence="0.764423125">
3: if Ieqp,1 &lt; p &lt; q &lt; m
&amp; I(k,t) EA,i&lt;k&lt; j,p&lt;t&lt;q
&amp;b(k,t)EA,i&lt;k&lt;j—p&lt;t&lt;q
then
fj
4: i is a positive instance for Cj−i+1
5: else
fj
</figure>
<listItem confidence="0.5783505">
6: i is a negative instance for Cj−i+1
7: end if
8: end for
9: end for
</listItem>
<subsectionHeader confidence="0.973751">
3.3 Classifiers
</subsectionHeader>
<bodyText confidence="0.930350833333333">
We compare two machine learning methods for
learning a series of binary classifiers.
For the first method, each Ck is individually
learned using the maximum entropy (ME) ap-
proach (Berger et al., 1996):
exp (Zt µtht (v, f (D) , i, j))
</bodyText>
<equation confidence="0.7412465">
Pk (v|f (D) , i, j) = Zv/ exp (Zt µtht (v&apos;, f (D) , i, j))
(5)
</equation>
<bodyText confidence="0.99883865">
where ht is a feature function and µt is weight
of ht. We use rich source contextual fea-
tures: unigram, bigram and trigram of the phrase
[fi−3, ..., fj+3].
As the second method, these classification tasks
are learned in the continuous space using feed-
forward neural networks (NNs). Each Ck has
the similar structure with the NN language model
(Vaswani et al., 2013). The inputs to the NN are
indices of the words: [fi−3, ..., fj+3]. Each source
word is projected into an N dimensional vector.
The output layer has two output neurons, whose
values correspond to Pk (v = 0|f (D) , i, j) and
Pk (v = 1|f (D) , i, j).
For both ME and NN approaches, words that
occur only once or never occur in the training
corpus are treated as a special word “UNK” (un-
known) during classifier training and predicting,
which can reduce training time and make the clas-
sifier training more smooth.
</bodyText>
<sectionHeader confidence="0.999405" genericHeader="method">
4 Experiment
</sectionHeader>
<bodyText confidence="0.999422">
We evaluated the effectiveness of the proposed ap-
proach for Chinese-to-English (CE) and Japanese-
to-English (JE) translation tasks. The datasets of-
ficially provided for the patent machine translation
task at NTCIR-9 (Goto et al., 2011) were used in
our experiments. The detailed training set statis-
tics are given in Table 1. The development and test
</bodyText>
<table confidence="0.996635">
SOURCE TARGET
#Sents 954k
CE #Words 37.2M 40.4M
#Vocab 288k 504k
#Sents 3.14M
JE #Words 118M 104M
#Vocab 150k 273k
</table>
<tableCaption confidence="0.999831">
Table 1: Data sets.
</tableCaption>
<bodyText confidence="0.99996547826087">
sets were both provided for CE task while only the
test set was provided for JE task. Therefore, we
used the sentences from the NTCIR-8 JE test set
as the development set. Word segmentation was
done by BaseSeg (Zhao et al., 2006; Zhao and Kit,
2008; Zhao et al., 2010; Zhao and Kit, 2011; Zhao
et al., 2013) for Chinese and Mecab 2 for Japanese.
To learn the classifiers for each translation task,
the training set and development set were put to-
gether to obtain symmetric word alignment us-
ing GIZA++ (Och and Ney, 2003) and the grow-
diag-final-and heuristic (Koehn et al., 2003). The
source span instances extracted from the aligned
training and development sets were used as the
training and validation data for the classifiers.
The toolkit Wapiti (Lavergne et al., 2010) was
adopted to train ME classifiers using the classi-
cal quasi-newton optimization algorithm with lim-
ited memory. The NNs are trained by the toolkit
NPLM (Vaswani et al., 2013). We chose “recti-
fier” as the activation function and the logarithmic
loss function for NNs. The number of epochs was
set to 20. Other parameters were set to default
</bodyText>
<footnote confidence="0.969345">
2http://sourceforge.net/projects/mecab/files/
</footnote>
<page confidence="0.993274">
185
</page>
<table confidence="0.999566785714286">
Span CE JE
length
Rate ME NN Rate ME NN
P N P N P N P N
1 2.67 0.93 0.63 0.93 0.64 1.08 0.85 0.79 0.86 0.80
2 1.37 0.83 0.70 0.82 0.75 0.73 0.69 0.84 0.71 0.87
3 0.86 0.70 0.80 0.73 0.83 0.52 0.56 0.89 0.63 0.90
4 0.62 0.57 0.81 0.67 0.88 0.36 0.48 0.93 0.54 0.93
5 0.48 0.52 0.90 0.61 0.91 0.26 0.30 0.96 0.47 0.95
6 0.40 0.47 0.91 0.58 0.92 0.20 0.25 0.97 0.41 0.96
7 0.34 0.40 0.93 0.53 0.93 0.16 0.14 0.98 0.33 0.97
8 0.28 0.35 0.94 0.46 0.94 0.13 0 1 0.32 0.97
9 0.22 0.28 0.96 0.37 0.96 0.10 0 1 0.25 0.98
10 0.15 0.21 0.97 0.28 0.97 0.08 0 1 0.23 0.99
</table>
<tableCaption confidence="0.9077405">
Table 2: Classification accuracies. The Rate column represents ratio of positive instances to negative
instances; the P and N columns give classification accuracies for positive and negative instances.
</tableCaption>
<bodyText confidence="0.977296813953488">
values. The training time of one classifier on a
12-core 3.47GHz Xeon X5690 machine was 0.5h
(2.5h) using ME (NN) approach for CE task; 1h
(4h) using ME (NN) approach for JE task.
The classification results are shown in Table 2.
Instead of the undifferentiated classification accu-
racy, we present separate classification accuracies
for positive and negative instances. The big differ-
ence between classification accuracies for positive
and negative instances was caused by the unbal-
anced rate of positive and negative instances in the
training corpus. For example, if there are more
positive training instances, then the classifier will
tend to classify new instances as positive and the
classification accuracy for positive instances will
be higher. In our classification tasks, there are less
positive instances for longer span lengths.
Since the word order difference of JE task is
much more significant than that of CE task, there
are more negative Japanese translation span in-
stances than Chinese. In JE tasks, the ME classi-
fiers C8, C9 and C10 predicted all new instances to
be negative due to the heavily unbalanced instance
distribution.
As shown in Table 2, NN outperformed ME ap-
proach for our classification tasks. As the span
length growing, the advantage of NN became
more significant. Since the classification accura-
cies deceased to be quite low for source spans with
more than 10 words, only {C1, ..., C10} were inte-
grated into the HPB translation system.
For each translation task, the recent version
of Moses HPB decoder (Koehn et al., 2007)
with the training scripts was used as the base-
line (Base). We used the default parameters for
Moses, and a 5-gram language model was trained
on the target side of the training corpus by IRST
LM Toolkit 3 with improved Kneser-Ney smooth-
ing. {C1,..., C10} were integrated into the base-
line with different weights, which were tuned by
MERT (Och, 2003) together with other feature
weights (language model, word penalty,...) under
the log-linear framework (Och and Ney, 2002).
</bodyText>
<table confidence="0.9998184">
Method TER BLEU-n n-gram precisions
4 1 2 3 4
Base 49.39- - 33.07- - 69.9/40.7/25.8/16.9
CE BLM 48.60 33.93 70.0/41.4/26.6/17.6
ME 49.02- 33.63- 70.0/41.2/26.3/17.4
NN 48.09++ 34.35++ 70.1/41.9/27.0/18.0
Base 57.39- - 30.13- - 67.1/38.3/23.0/14.0
JE BLM 56.79 30.81 67.7/38.9/23.6/14.5
ME 56.48 31.01 67.6/39.0/23.8/14.7
NN 55.96++ 31.77++ 67.8/39.7/24.6/15.4
</table>
<tableCaption confidence="0.998343">
Table 3: Translation results. The symbol ++ (- -)
</tableCaption>
<bodyText confidence="0.997094473684211">
represents a significant difference at the p &lt; 0.01
level and - represents a significant difference at the
p &lt; 0.05 level against the BLM.
We compare our method with the baseline and
the boundary learning method (BLM) (Xiong et
al., 2010) based on Maximum Entropy Markov
Models with Markov order 2. Table 3 reports
BLEU (Papineni et al., 2002) and TER (Snover
et al., 2006) scores. Significance tests are con-
ducted using bootstrap sampling (Koehn, 2004).
Our ME classifiers achieve comparable translation
improvement with the BLM and NN classifiers en-
hance translation system significantly compared to
others. Table 3 also shows that the relative gain
was higher for higher n-grams, which is reason-
able since the higher n-grams have higher ambi-
guities in the translation rule application.
It is true that because of multiple parallel sen-
tences, a source span can be applied with transla-
</bodyText>
<footnote confidence="0.969727">
3http://hlt.fbk.eu/en/irstlm
</footnote>
<page confidence="0.99758">
186
</page>
<bodyText confidence="0.99946">
tion rules in one sentence pair but not in another
sentence pair. So we used the probability score
as a feature in the decoding. That is, we did not
use classification results directly but use the prob-
ability score for softly constraining the decoding
process.
</bodyText>
<sectionHeader confidence="0.993931" genericHeader="conclusions">
5 Conclusion
</sectionHeader>
<bodyText confidence="0.999974545454545">
We have proposed a simple and effective transla-
tion span learning model for HPB translation. Our
model is learned from aligned parallel corpora and
predicts translation spans for source sentence be-
fore translating, which is integrated into the trans-
lation system conveniently as soft constraints. We
compared ME and NN approaches for this learn-
ing task. The results showed that NN classifiers on
the continuous space model achieved both higher
classification accuracies and better translation per-
formance with acceptable training times.
</bodyText>
<sectionHeader confidence="0.994349" genericHeader="acknowledgments">
Acknowledgments
</sectionHeader>
<bodyText confidence="0.999913571428572">
Hai Zhao were partially supported by CSC fund
(201304490199), the National Natural Science
Foundation of China (Grant No.60903119, Grant
No.61170114, and Grant No.61272248), the Na-
tional Basic Research Program of China (Grant
No.2013CB329401), the Science and Technol-
ogy Commission of Shanghai Municipality (Grant
No.13511500200), the European Union Seventh
Framework Program (Grant No.247619), and the
art and science interdiscipline funds of Shang-
hai Jiao Tong University, a study on mobilization
mechanism and alerting threshold setting for on-
line community, and media image and psychology
evaluation: a computational intelligence approach.
</bodyText>
<sectionHeader confidence="0.998996" genericHeader="references">
References
</sectionHeader>
<reference confidence="0.999687680555556">
Adam Berger, Vincent Della Pietra, and Stephen Della
Pietra. 1996. A maximum entropy approach to nat-
ural language processing. Computational linguis-
tics, 22(1):39–71.
David Chiang. 2005. A hierarchical phrase-based
model for statistical machine translation. In Pro-
ceedings of the 43rd Annual Meeting on Association
for Computational Linguistics, pages 263–270. As-
sociation for Computational Linguistics.
Lei Cui, Dongdong Zhang, Mu Li, Ming Zhou, and
Tiejun Zhao. 2010. A joint rule selection model
for hierarchical phrase-based translation. In Pro-
ceedings of the ACL 2010 Conference Short Papers,
pages 6–11. Association for Computational Linguis-
tics.
Isao Goto, Bin Lu, Ka Po Chow, Eiichiro Sumita, and
Benjamin K Tsou. 2011. Overview of the patent
machine translation task at the ntcir-9 workshop. In
Proceedings of NTCIR, volume 9, pages 559–578.
Zhongjun He, Yao Meng, and Hao Yu. 2010. Learn-
ing phrase boundaries for hierarchical phrase-based
translation. In Proceedings of the 23rd International
Conference on Computational Linguistics: Posters,
pages 383–390. Association for Computational Lin-
guistics.
Philipp Koehn, Franz Josef Och, and Daniel Marcu.
2003. Statistical phrase-based translation. In
Proceedings of the 2003 Conference of the North
American Chapter of the Association for Computa-
tional Linguistics on Human Language Technology-
Volume 1, pages 48–54. Association for Computa-
tional Linguistics.
Philipp Koehn, Hieu Hoang, Alexandra Birch, Chris
Callison-Burch, Marcello Federico, Nicola Bertoldi,
Brooke Cowan, Wade Shen, Christine Moran,
Richard Zens, et al. 2007. Moses: Open source
toolkit for statistical machine translation. In Pro-
ceedings of the 45th Annual Meeting of the ACL
on Interactive Poster and Demonstration Sessions,
pages 177–180. Association for Computational Lin-
guistics.
Philipp Koehn. 2004. Statistical significance tests for
machine translation evaluation. In EMNLP, pages
388–395.
Thomas Lavergne, Olivier Capp´e, and Franc¸ois Yvon.
2010. Practical very large scale crfs. In Proceed-
ings of the 48th Annual Meeting of the Association
for Computational Linguistics, pages 504–513, Up-
psala, Sweden, July. Association for Computational
Linguistics.
Lemao Liu, Tiejun Zhao, Chao Wang, and Hailong
Cao. 2011. A unified and discriminative soft syn-
tactic constraint model for hierarchical phrase-based
translation. In the Thirteenth Machine Translation
Summit, pages 253–260. Asia-Pacific Association
for Machine Translation.
Yuval Marton and Philip Resnik. 2008. Soft syntac-
tic constraints for hierarchical phrased-based trans-
lation. In ACL, pages 1003–1011.
Markos Mylonakis and Khalil Sima’an. 2011. Learn-
ing hierarchical translation structure with linguis-
tic annotations. In Proceedings of the 49th Annual
Meeting of the Association for Computational Lin-
guistics: Human Language Technologies-Volume 1,
pages 642–652. Association for Computational Lin-
guistics.
Franz Josef Och and Hermann Ney. 2002. Discrimina-
tive training and maximum entropy models for sta-
tistical machine translation. In Proceedings of the
40th Annual Meeting on Association for Computa-
tional Linguistics, pages 295–302. Association for
Computational Linguistics.
</reference>
<page confidence="0.978565">
187
</page>
<reference confidence="0.999794714285715">
Franz Josef Och and Hermann Ney. 2003. A sys-
tematic comparison of various statistical alignment
models. Computational linguistics, 29(1):19–51.
Franz Josef Och. 2003. Minimum error rate training
in statistical machine translation. In Proceedings of
the 41st Annual Meeting on Association for Compu-
tational Linguistics-Volume 1, pages 160–167. As-
sociation for Computational Linguistics.
Kishore Papineni, Salim Roukos, Todd Ward, and Wei-
Jing Zhu. 2002. BLEU: a method for automatic
evaluation of machine translation. In Proceedings of
the 40th annual meeting on association for compu-
tational linguistics, pages 311–318. Association for
Computational Linguistics.
Matthew Snover, Bonnie Dorr, Richard Schwartz, Lin-
nea Micciulla, and John Makhoul. 2006. A study of
translation edit rate with targeted human annotation.
In Proceedings of association for machine transla-
tion in the Americas, pages 223–231.
Ashish Vaswani, Yinggong Zhao, Victoria Fossum,
and David Chiang. 2013. Decoding with large-
scale neural language models improves translation.
In Proceedings of the 2013 Conference on Em-
pirical Methods in Natural Language Processing,
pages 1387–1392, Seattle, Washington, USA, Oc-
tober. Association for Computational Linguistics.
Deyi Xiong, Min Zhang, and Haizhou Li. 2010.
Learning translation boundaries for phrase-based
decoding. In Human Language Technologies: The
2010 Annual Conference of the North American
Chapter of the Association for Computational Lin-
guistics, pages 136–144. Association for Computa-
tional Linguistics.
Hai Zhao and Chunyu Kit. 2008. Exploiting unla-
beled text with different unsupervised segmentation
criteria for chinese word segmentation. Research in
Computing Science, 33:93–104.
Hai Zhao and Chunyu Kit. 2011. Integrating unsu-
pervised and supervised word segmentation: The
role of goodness measures. Information Sciences,
181(1):163–183.
Hai Zhao, Chang-Ning Huang, and Mu Li. 2006. An
improved chinese word segmentation system with
conditional random field. In Proceedings of the Fifth
SIGHAN Workshop on Chinese Language Process-
ing, pages 162–165. Sydney: July.
Hai Zhao, Chang-Ning Huang, Mu Li, and Bao-Liang
Lu. 2010. A unified character-based tagging frame-
work for chinese word segmentation. ACM Trans-
actions on Asian Language Information Processing
(TALIP), 9(2):5.
Hai Zhao, Masao Utiyama, Eiichiro Sumita, and Bao-
Liang Lu. 2013. An empirical study on word seg-
mentation for chinese machine translation. In Com-
putational Linguistics and Intelligent Text Process-
ing, pages 248–263. Springer.
</reference>
<page confidence="0.997527">
188
</page>
</variant>
</algorithm>
<algorithm name="ParsHed" version="110505">
<variant no="0" confidence="0.324448">
<title confidence="0.999971">Learning Hierarchical Translation Spans</title>
<author confidence="0.913958">Masao Eiichro Hai</author>
<affiliation confidence="0.6702048">for Brain-Like Computing and Machine Intelligence, Department of Science and Engineering, Shanghai Jiao Tong Unviersity, Shanghai, 200240, Laboratory of Shanghai Education Commission for Intelligent and Cognitive Engineering, Shanghai Jiao Tong Unviersity, Shanghai, 200240, Institute of Information and Communications</affiliation>
<address confidence="0.84423">3-5Hikaridai, Keihanna Science City, Kyoto, 619-0289,</address>
<email confidence="0.919181">zhangjingyizz@gmail.com,zhaohai@cs.sjtu.edu.cn</email>
<abstract confidence="0.999468368421053">We propose a simple and effective approach to learn translation spans for the hierarchical phrase-based translation model. Our model evaluates if a source span should be covered by translation rules during decoding, which is integrated the translation system as soft con- Compared to syntactic straints, our model is directly acquired from an aligned parallel corpus and does not require parsers. Rich source side contextual features and advanced machine learning methods were utilized for this learning task. The proposed approach was evaluated on NTCIR-9 Chinese-English and Japanese-English translation tasks and showed significant improvement over the baseline system.</abstract>
</variant>
</algorithm>
<algorithm name="ParsCit" version="110505">
<citationList>
<citation valid="true">
<authors>
<author>Adam Berger</author>
<author>Vincent Della Pietra</author>
<author>Stephen Della Pietra</author>
</authors>
<title>A maximum entropy approach to natural language processing.</title>
<date>1996</date>
<journal>Computational linguistics,</journal>
<pages>22--1</pages>
<contexts>
<context position="9144" citStr="Berger et al., 1996" startWordPosition="1504" endWordPosition="1507"> Algorithm 1 Extract training instances. Input: A pair of parallel sentence fn1 and em1 with word alignments A. Output: Training examples for {C1, C2, C3, ...}. 1: for i = 1 to n do 2: for j = i to n do 3: if Ieqp,1 &lt; p &lt; q &lt; m &amp; I(k,t) EA,i&lt;k&lt; j,p&lt;t&lt;q &amp;b(k,t)EA,i&lt;k&lt;j—p&lt;t&lt;q then fj 4: i is a positive instance for Cj−i+1 5: else fj 6: i is a negative instance for Cj−i+1 7: end if 8: end for 9: end for 3.3 Classifiers We compare two machine learning methods for learning a series of binary classifiers. For the first method, each Ck is individually learned using the maximum entropy (ME) approach (Berger et al., 1996): exp (Zt µtht (v, f (D) , i, j)) Pk (v|f (D) , i, j) = Zv/ exp (Zt µtht (v&apos;, f (D) , i, j)) (5) where ht is a feature function and µt is weight of ht. We use rich source contextual features: unigram, bigram and trigram of the phrase [fi−3, ..., fj+3]. As the second method, these classification tasks are learned in the continuous space using feedforward neural networks (NNs). Each Ck has the similar structure with the NN language model (Vaswani et al., 2013). The inputs to the NN are indices of the words: [fi−3, ..., fj+3]. Each source word is projected into an N dimensional vector. The output</context>
</contexts>
<marker>Berger, Pietra, Pietra, 1996</marker>
<rawString>Adam Berger, Vincent Della Pietra, and Stephen Della Pietra. 1996. A maximum entropy approach to natural language processing. Computational linguistics, 22(1):39–71.</rawString>
</citation>
<citation valid="true">
<authors>
<author>David Chiang</author>
</authors>
<title>A hierarchical phrase-based model for statistical machine translation.</title>
<date>2005</date>
<booktitle>In Proceedings of the 43rd Annual Meeting on Association for Computational Linguistics,</booktitle>
<pages>263--270</pages>
<publisher>Association</publisher>
<institution>for Computational Linguistics.</institution>
<contexts>
<context position="1414" citStr="Chiang, 2005" startWordPosition="177" endWordPosition="178"> should be covered by translation rules during decoding, which is integrated into the translation system as soft constraints. Compared to syntactic constraints, our model is directly acquired from an aligned parallel corpus and does not require parsers. Rich source side contextual features and advanced machine learning methods were utilized for this learning task. The proposed approach was evaluated on NTCIR-9 Chinese-English and Japanese-English translation tasks and showed significant improvement over the baseline system. 1 Introduction The hierarchical phrase-based (HPB) translation model (Chiang, 2005) has been widely adopted in statistical machine translation (SMT) tasks. The HPB translation rules based on the synchronous context free grammar (SCFG) are simple and powerful. One drawback of the HPB model is the applications of translation rules to the input sentence are highly ambiguous. For example, a rule whose English side is “X1 by X2” can be applied to any word sequence that has “by” in them. In Figure 1, this rule can be applied to the whole sentence as well as to “experiment by tomorrow”. In order to tackle rule application ambiguities, a few previous works used syntax trees. Chiang </context>
</contexts>
<marker>Chiang, 2005</marker>
<rawString>David Chiang. 2005. A hierarchical phrase-based model for statistical machine translation. In Proceedings of the 43rd Annual Meeting on Association for Computational Linguistics, pages 263–270. Association for Computational Linguistics.</rawString>
</citation>
<citation valid="true">
<authors>
<author>Lei Cui</author>
<author>Dongdong Zhang</author>
<author>Mu Li</author>
<author>Ming Zhou</author>
<author>Tiejun Zhao</author>
</authors>
<title>A joint rule selection model for hierarchical phrase-based translation.</title>
<date>2010</date>
<booktitle>In Proceedings of the ACL 2010 Conference Short Papers,</booktitle>
<pages>6--11</pages>
<publisher>Association</publisher>
<institution>for Computational Linguistics.</institution>
<contexts>
<context position="2901" citStr="Cui et al., 2010" startWordPosition="409" endWordPosition="412"> experimental results showed this feature gave no significant improvement. Instead of using the undifferentiated constituency feature, (Marton and Resnik, 2008) defined different soft syntactic features for different constituent types and obtained substantial performance improvement. Later, (Mylonakis and Sima’an, 2011) introduced joint probability synchronous grammars to integrate flexible linguistic information. (Liu et al., 2011) proposed the soft syntactic constraint model based on discriminative classifiers for each constituent type and integrated all of them into the translation model. (Cui et al., 2010) focused on hierarchical rule selection using many features including syntax constituents. These works have demonstrated the benefits of using syntactic features in the HPB model. However, high quality syntax parsers are not always easily obtained for many languages. Without this problem, word alignment constraints can also be used to guide the application of the rules. Suppose that we want to translate the English sentence into the Chinese sentence in Figure 1, a translation rule can be applied to the source span “finish this experiment by tomorrow”. Nonetheless, if a rule is applied to “expe</context>
</contexts>
<marker>Cui, Zhang, Li, Zhou, Zhao, 2010</marker>
<rawString>Lei Cui, Dongdong Zhang, Mu Li, Ming Zhou, and Tiejun Zhao. 2010. A joint rule selection model for hierarchical phrase-based translation. In Proceedings of the ACL 2010 Conference Short Papers, pages 6–11. Association for Computational Linguistics.</rawString>
</citation>
<citation valid="true">
<authors>
<author>Isao Goto</author>
<author>Bin Lu</author>
<author>Ka Po Chow</author>
<author>Eiichiro Sumita</author>
<author>Benjamin K Tsou</author>
</authors>
<title>Overview of the patent machine translation task at the ntcir-9 workshop.</title>
<date>2011</date>
<booktitle>In Proceedings of NTCIR,</booktitle>
<volume>9</volume>
<pages>559--578</pages>
<contexts>
<context position="10364" citStr="Goto et al., 2011" startWordPosition="1727" endWordPosition="1730">ayer has two output neurons, whose values correspond to Pk (v = 0|f (D) , i, j) and Pk (v = 1|f (D) , i, j). For both ME and NN approaches, words that occur only once or never occur in the training corpus are treated as a special word “UNK” (unknown) during classifier training and predicting, which can reduce training time and make the classifier training more smooth. 4 Experiment We evaluated the effectiveness of the proposed approach for Chinese-to-English (CE) and Japaneseto-English (JE) translation tasks. The datasets officially provided for the patent machine translation task at NTCIR-9 (Goto et al., 2011) were used in our experiments. The detailed training set statistics are given in Table 1. The development and test SOURCE TARGET #Sents 954k CE #Words 37.2M 40.4M #Vocab 288k 504k #Sents 3.14M JE #Words 118M 104M #Vocab 150k 273k Table 1: Data sets. sets were both provided for CE task while only the test set was provided for JE task. Therefore, we used the sentences from the NTCIR-8 JE test set as the development set. Word segmentation was done by BaseSeg (Zhao et al., 2006; Zhao and Kit, 2008; Zhao et al., 2010; Zhao and Kit, 2011; Zhao et al., 2013) for Chinese and Mecab 2 for Japanese. To l</context>
</contexts>
<marker>Goto, Lu, Chow, Sumita, Tsou, 2011</marker>
<rawString>Isao Goto, Bin Lu, Ka Po Chow, Eiichiro Sumita, and Benjamin K Tsou. 2011. Overview of the patent machine translation task at the ntcir-9 workshop. In Proceedings of NTCIR, volume 9, pages 559–578.</rawString>
</citation>
<citation valid="true">
<authors>
<author>Zhongjun He</author>
<author>Yao Meng</author>
<author>Hao Yu</author>
</authors>
<title>Learning phrase boundaries for hierarchical phrase-based translation.</title>
<date>2010</date>
<booktitle>In Proceedings of the 23rd International Conference on Computational Linguistics: Posters,</booktitle>
<pages>383--390</pages>
<publisher>Association</publisher>
<institution>for Computational Linguistics.</institution>
<contexts>
<context position="5233" citStr="He et al., 2010" startWordPosition="784" endWordPosition="787">translation spans, respectively. A source word is marked as beginning (ending) boundary if it is the first (last) word of a translation span. However, a source span whose first and last words are both boundaries is not always a translation span. In Figure 1, “I” is a beginning boundary since it is the first word of translation span “I will” and “experiment” is an ending boundary since it is the last word of translation span “finish this experiment” , but “I will finish this experiment” is not a translation span. This happens because the translation spans are nested or hierarchical. Note that (He et al., 2010) also learned phrase boundaries to constrain decoding, but their approach identified boundaries only for monotone translation. In this paper, taking fully into account that translation spans being nested, we propose an approach to learn hierarchical translation spans directly from an aligned parallel corpus that makes more accurate identification over translation spans. The rest of the paper is structured as follows: In Section 2, we briefly review the HPB translation model. Section 3 describes our approach. We describe experiments in Section 4 and conclude in Section 5. 2 Hierarchical Phrase-</context>
</contexts>
<marker>He, Meng, Yu, 2010</marker>
<rawString>Zhongjun He, Yao Meng, and Hao Yu. 2010. Learning phrase boundaries for hierarchical phrase-based translation. In Proceedings of the 23rd International Conference on Computational Linguistics: Posters, pages 383–390. Association for Computational Linguistics.</rawString>
</citation>
<citation valid="true">
<authors>
<author>Philipp Koehn</author>
<author>Franz Josef Och</author>
<author>Daniel Marcu</author>
</authors>
<title>Statistical phrase-based translation.</title>
<date>2003</date>
<booktitle>In Proceedings of the 2003 Conference of the North American Chapter of the Association for Computational Linguistics on Human Language TechnologyVolume 1,</booktitle>
<pages>48--54</pages>
<publisher>Association</publisher>
<institution>for Computational Linguistics.</institution>
<contexts>
<context position="11192" citStr="Koehn et al., 2003" startWordPosition="1875" endWordPosition="1878">04M #Vocab 150k 273k Table 1: Data sets. sets were both provided for CE task while only the test set was provided for JE task. Therefore, we used the sentences from the NTCIR-8 JE test set as the development set. Word segmentation was done by BaseSeg (Zhao et al., 2006; Zhao and Kit, 2008; Zhao et al., 2010; Zhao and Kit, 2011; Zhao et al., 2013) for Chinese and Mecab 2 for Japanese. To learn the classifiers for each translation task, the training set and development set were put together to obtain symmetric word alignment using GIZA++ (Och and Ney, 2003) and the growdiag-final-and heuristic (Koehn et al., 2003). The source span instances extracted from the aligned training and development sets were used as the training and validation data for the classifiers. The toolkit Wapiti (Lavergne et al., 2010) was adopted to train ME classifiers using the classical quasi-newton optimization algorithm with limited memory. The NNs are trained by the toolkit NPLM (Vaswani et al., 2013). We chose “rectifier” as the activation function and the logarithmic loss function for NNs. The number of epochs was set to 20. Other parameters were set to default 2http://sourceforge.net/projects/mecab/files/ 185 Span CE JE len</context>
</contexts>
<marker>Koehn, Och, Marcu, 2003</marker>
<rawString>Philipp Koehn, Franz Josef Och, and Daniel Marcu. 2003. Statistical phrase-based translation. In Proceedings of the 2003 Conference of the North American Chapter of the Association for Computational Linguistics on Human Language TechnologyVolume 1, pages 48–54. Association for Computational Linguistics.</rawString>
</citation>
<citation valid="true">
<authors>
<author>Philipp Koehn</author>
<author>Hieu Hoang</author>
<author>Alexandra Birch</author>
<author>Chris Callison-Burch</author>
<author>Marcello Federico</author>
<author>Nicola Bertoldi</author>
<author>Brooke Cowan</author>
<author>Wade Shen</author>
<author>Christine Moran</author>
<author>Richard Zens</author>
</authors>
<title>Moses: Open source toolkit for statistical machine translation.</title>
<date>2007</date>
<booktitle>In Proceedings of the 45th Annual Meeting of the ACL on Interactive Poster and Demonstration Sessions,</booktitle>
<pages>177--180</pages>
<publisher>Association</publisher>
<institution>for Computational Linguistics.</institution>
<contexts>
<context position="14095" citStr="Koehn et al., 2007" startWordPosition="2374" endWordPosition="2377"> more negative Japanese translation span instances than Chinese. In JE tasks, the ME classifiers C8, C9 and C10 predicted all new instances to be negative due to the heavily unbalanced instance distribution. As shown in Table 2, NN outperformed ME approach for our classification tasks. As the span length growing, the advantage of NN became more significant. Since the classification accuracies deceased to be quite low for source spans with more than 10 words, only {C1, ..., C10} were integrated into the HPB translation system. For each translation task, the recent version of Moses HPB decoder (Koehn et al., 2007) with the training scripts was used as the baseline (Base). We used the default parameters for Moses, and a 5-gram language model was trained on the target side of the training corpus by IRST LM Toolkit 3 with improved Kneser-Ney smoothing. {C1,..., C10} were integrated into the baseline with different weights, which were tuned by MERT (Och, 2003) together with other feature weights (language model, word penalty,...) under the log-linear framework (Och and Ney, 2002). Method TER BLEU-n n-gram precisions 4 1 2 3 4 Base 49.39- - 33.07- - 69.9/40.7/25.8/16.9 CE BLM 48.60 33.93 70.0/41.4/26.6/17.6</context>
</contexts>
<marker>Koehn, Hoang, Birch, Callison-Burch, Federico, Bertoldi, Cowan, Shen, Moran, Zens, 2007</marker>
<rawString>Philipp Koehn, Hieu Hoang, Alexandra Birch, Chris Callison-Burch, Marcello Federico, Nicola Bertoldi, Brooke Cowan, Wade Shen, Christine Moran, Richard Zens, et al. 2007. Moses: Open source toolkit for statistical machine translation. In Proceedings of the 45th Annual Meeting of the ACL on Interactive Poster and Demonstration Sessions, pages 177–180. Association for Computational Linguistics.</rawString>
</citation>
<citation valid="true">
<authors>
<author>Philipp Koehn</author>
</authors>
<title>Statistical significance tests for machine translation evaluation.</title>
<date>2004</date>
<booktitle>In EMNLP,</booktitle>
<pages>388--395</pages>
<contexts>
<context position="15431" citStr="Koehn, 2004" startWordPosition="2588" endWordPosition="2589">LM 56.79 30.81 67.7/38.9/23.6/14.5 ME 56.48 31.01 67.6/39.0/23.8/14.7 NN 55.96++ 31.77++ 67.8/39.7/24.6/15.4 Table 3: Translation results. The symbol ++ (- -) represents a significant difference at the p &lt; 0.01 level and - represents a significant difference at the p &lt; 0.05 level against the BLM. We compare our method with the baseline and the boundary learning method (BLM) (Xiong et al., 2010) based on Maximum Entropy Markov Models with Markov order 2. Table 3 reports BLEU (Papineni et al., 2002) and TER (Snover et al., 2006) scores. Significance tests are conducted using bootstrap sampling (Koehn, 2004). Our ME classifiers achieve comparable translation improvement with the BLM and NN classifiers enhance translation system significantly compared to others. Table 3 also shows that the relative gain was higher for higher n-grams, which is reasonable since the higher n-grams have higher ambiguities in the translation rule application. It is true that because of multiple parallel sentences, a source span can be applied with transla3http://hlt.fbk.eu/en/irstlm 186 tion rules in one sentence pair but not in another sentence pair. So we used the probability score as a feature in the decoding. That </context>
</contexts>
<marker>Koehn, 2004</marker>
<rawString>Philipp Koehn. 2004. Statistical significance tests for machine translation evaluation. In EMNLP, pages 388–395.</rawString>
</citation>
<citation valid="true">
<authors>
<author>Thomas Lavergne</author>
<author>Olivier Capp´e</author>
<author>Franc¸ois Yvon</author>
</authors>
<title>Practical very large scale crfs.</title>
<date>2010</date>
<booktitle>In Proceedings of the 48th Annual Meeting of the Association for Computational Linguistics,</booktitle>
<pages>504--513</pages>
<publisher>Association</publisher>
<institution>for Computational Linguistics.</institution>
<location>Uppsala, Sweden,</location>
<marker>Lavergne, Capp´e, Yvon, 2010</marker>
<rawString>Thomas Lavergne, Olivier Capp´e, and Franc¸ois Yvon. 2010. Practical very large scale crfs. In Proceedings of the 48th Annual Meeting of the Association for Computational Linguistics, pages 504–513, Uppsala, Sweden, July. Association for Computational Linguistics.</rawString>
</citation>
<citation valid="true">
<authors>
<author>Lemao Liu</author>
<author>Tiejun Zhao</author>
<author>Chao Wang</author>
<author>Hailong Cao</author>
</authors>
<title>A unified and discriminative soft syntactic constraint model for hierarchical phrase-based translation.</title>
<date>2011</date>
<booktitle>In the Thirteenth Machine Translation Summit,</booktitle>
<pages>253--260</pages>
<contexts>
<context position="2720" citStr="Liu et al., 2011" startWordPosition="382" endWordPosition="385">- � ;0-: HJJ)Q ZU 512GIA iZk/l` �R Figure 1: A translation example. model, which represents if the source span covered by a translation rule is a syntactic constituent. However, the experimental results showed this feature gave no significant improvement. Instead of using the undifferentiated constituency feature, (Marton and Resnik, 2008) defined different soft syntactic features for different constituent types and obtained substantial performance improvement. Later, (Mylonakis and Sima’an, 2011) introduced joint probability synchronous grammars to integrate flexible linguistic information. (Liu et al., 2011) proposed the soft syntactic constraint model based on discriminative classifiers for each constituent type and integrated all of them into the translation model. (Cui et al., 2010) focused on hierarchical rule selection using many features including syntax constituents. These works have demonstrated the benefits of using syntactic features in the HPB model. However, high quality syntax parsers are not always easily obtained for many languages. Without this problem, word alignment constraints can also be used to guide the application of the rules. Suppose that we want to translate the English </context>
</contexts>
<marker>Liu, Zhao, Wang, Cao, 2011</marker>
<rawString>Lemao Liu, Tiejun Zhao, Chao Wang, and Hailong Cao. 2011. A unified and discriminative soft syntactic constraint model for hierarchical phrase-based translation. In the Thirteenth Machine Translation Summit, pages 253–260. Asia-Pacific Association for Machine Translation.</rawString>
</citation>
<citation valid="true">
<authors>
<author>Yuval Marton</author>
<author>Philip Resnik</author>
</authors>
<title>Soft syntactic constraints for hierarchical phrased-based translation.</title>
<date>2008</date>
<booktitle>In ACL,</booktitle>
<pages>1003--1011</pages>
<contexts>
<context position="2444" citStr="Marton and Resnik, 2008" startWordPosition="347" endWordPosition="350"> Figure 1, this rule can be applied to the whole sentence as well as to “experiment by tomorrow”. In order to tackle rule application ambiguities, a few previous works used syntax trees. Chiang (2005) utilized a syntactic feature in the HPB I will finish this experiment by tomorrow - � ;0-: HJJ)Q ZU 512GIA iZk/l` �R Figure 1: A translation example. model, which represents if the source span covered by a translation rule is a syntactic constituent. However, the experimental results showed this feature gave no significant improvement. Instead of using the undifferentiated constituency feature, (Marton and Resnik, 2008) defined different soft syntactic features for different constituent types and obtained substantial performance improvement. Later, (Mylonakis and Sima’an, 2011) introduced joint probability synchronous grammars to integrate flexible linguistic information. (Liu et al., 2011) proposed the soft syntactic constraint model based on discriminative classifiers for each constituent type and integrated all of them into the translation model. (Cui et al., 2010) focused on hierarchical rule selection using many features including syntax constituents. These works have demonstrated the benefits of using </context>
</contexts>
<marker>Marton, Resnik, 2008</marker>
<rawString>Yuval Marton and Philip Resnik. 2008. Soft syntactic constraints for hierarchical phrased-based translation. In ACL, pages 1003–1011.</rawString>
</citation>
<citation valid="true">
<authors>
<author>Markos Mylonakis</author>
<author>Khalil Sima’an</author>
</authors>
<title>Learning hierarchical translation structure with linguistic annotations.</title>
<date>2011</date>
<booktitle>In Proceedings of the 49th Annual Meeting of the Association for Computational Linguistics: Human Language Technologies-Volume 1,</booktitle>
<pages>642--652</pages>
<publisher>Association</publisher>
<institution>for Computational Linguistics.</institution>
<marker>Mylonakis, Sima’an, 2011</marker>
<rawString>Markos Mylonakis and Khalil Sima’an. 2011. Learning hierarchical translation structure with linguistic annotations. In Proceedings of the 49th Annual Meeting of the Association for Computational Linguistics: Human Language Technologies-Volume 1, pages 642–652. Association for Computational Linguistics.</rawString>
</citation>
<citation valid="true">
<authors>
<author>Franz Josef Och</author>
<author>Hermann Ney</author>
</authors>
<title>Discriminative training and maximum entropy models for statistical machine translation.</title>
<date>2002</date>
<booktitle>In Proceedings of the 40th Annual Meeting on Association for Computational Linguistics,</booktitle>
<pages>295--302</pages>
<publisher>Association</publisher>
<institution>for Computational Linguistics.</institution>
<contexts>
<context position="14566" citStr="Och and Ney, 2002" startWordPosition="2452" endWordPosition="2455"> ..., C10} were integrated into the HPB translation system. For each translation task, the recent version of Moses HPB decoder (Koehn et al., 2007) with the training scripts was used as the baseline (Base). We used the default parameters for Moses, and a 5-gram language model was trained on the target side of the training corpus by IRST LM Toolkit 3 with improved Kneser-Ney smoothing. {C1,..., C10} were integrated into the baseline with different weights, which were tuned by MERT (Och, 2003) together with other feature weights (language model, word penalty,...) under the log-linear framework (Och and Ney, 2002). Method TER BLEU-n n-gram precisions 4 1 2 3 4 Base 49.39- - 33.07- - 69.9/40.7/25.8/16.9 CE BLM 48.60 33.93 70.0/41.4/26.6/17.6 ME 49.02- 33.63- 70.0/41.2/26.3/17.4 NN 48.09++ 34.35++ 70.1/41.9/27.0/18.0 Base 57.39- - 30.13- - 67.1/38.3/23.0/14.0 JE BLM 56.79 30.81 67.7/38.9/23.6/14.5 ME 56.48 31.01 67.6/39.0/23.8/14.7 NN 55.96++ 31.77++ 67.8/39.7/24.6/15.4 Table 3: Translation results. The symbol ++ (- -) represents a significant difference at the p &lt; 0.01 level and - represents a significant difference at the p &lt; 0.05 level against the BLM. We compare our method with the baseline and the b</context>
</contexts>
<marker>Och, Ney, 2002</marker>
<rawString>Franz Josef Och and Hermann Ney. 2002. Discriminative training and maximum entropy models for statistical machine translation. In Proceedings of the 40th Annual Meeting on Association for Computational Linguistics, pages 295–302. Association for Computational Linguistics.</rawString>
</citation>
<citation valid="true">
<authors>
<author>Franz Josef Och</author>
<author>Hermann Ney</author>
</authors>
<title>A systematic comparison of various statistical alignment models.</title>
<date>2003</date>
<journal>Computational linguistics,</journal>
<pages>29--1</pages>
<contexts>
<context position="11134" citStr="Och and Ney, 2003" startWordPosition="1866" endWordPosition="1869">7.2M 40.4M #Vocab 288k 504k #Sents 3.14M JE #Words 118M 104M #Vocab 150k 273k Table 1: Data sets. sets were both provided for CE task while only the test set was provided for JE task. Therefore, we used the sentences from the NTCIR-8 JE test set as the development set. Word segmentation was done by BaseSeg (Zhao et al., 2006; Zhao and Kit, 2008; Zhao et al., 2010; Zhao and Kit, 2011; Zhao et al., 2013) for Chinese and Mecab 2 for Japanese. To learn the classifiers for each translation task, the training set and development set were put together to obtain symmetric word alignment using GIZA++ (Och and Ney, 2003) and the growdiag-final-and heuristic (Koehn et al., 2003). The source span instances extracted from the aligned training and development sets were used as the training and validation data for the classifiers. The toolkit Wapiti (Lavergne et al., 2010) was adopted to train ME classifiers using the classical quasi-newton optimization algorithm with limited memory. The NNs are trained by the toolkit NPLM (Vaswani et al., 2013). We chose “rectifier” as the activation function and the logarithmic loss function for NNs. The number of epochs was set to 20. Other parameters were set to default 2http:</context>
</contexts>
<marker>Och, Ney, 2003</marker>
<rawString>Franz Josef Och and Hermann Ney. 2003. A systematic comparison of various statistical alignment models. Computational linguistics, 29(1):19–51.</rawString>
</citation>
<citation valid="true">
<authors>
<author>Franz Josef Och</author>
</authors>
<title>Minimum error rate training in statistical machine translation.</title>
<date>2003</date>
<booktitle>In Proceedings of the 41st Annual Meeting on Association for Computational Linguistics-Volume 1,</booktitle>
<pages>160--167</pages>
<publisher>Association</publisher>
<institution>for Computational Linguistics.</institution>
<contexts>
<context position="14444" citStr="Och, 2003" startWordPosition="2437" endWordPosition="2438">. Since the classification accuracies deceased to be quite low for source spans with more than 10 words, only {C1, ..., C10} were integrated into the HPB translation system. For each translation task, the recent version of Moses HPB decoder (Koehn et al., 2007) with the training scripts was used as the baseline (Base). We used the default parameters for Moses, and a 5-gram language model was trained on the target side of the training corpus by IRST LM Toolkit 3 with improved Kneser-Ney smoothing. {C1,..., C10} were integrated into the baseline with different weights, which were tuned by MERT (Och, 2003) together with other feature weights (language model, word penalty,...) under the log-linear framework (Och and Ney, 2002). Method TER BLEU-n n-gram precisions 4 1 2 3 4 Base 49.39- - 33.07- - 69.9/40.7/25.8/16.9 CE BLM 48.60 33.93 70.0/41.4/26.6/17.6 ME 49.02- 33.63- 70.0/41.2/26.3/17.4 NN 48.09++ 34.35++ 70.1/41.9/27.0/18.0 Base 57.39- - 30.13- - 67.1/38.3/23.0/14.0 JE BLM 56.79 30.81 67.7/38.9/23.6/14.5 ME 56.48 31.01 67.6/39.0/23.8/14.7 NN 55.96++ 31.77++ 67.8/39.7/24.6/15.4 Table 3: Translation results. The symbol ++ (- -) represents a significant difference at the p &lt; 0.01 level and - re</context>
</contexts>
<marker>Och, 2003</marker>
<rawString>Franz Josef Och. 2003. Minimum error rate training in statistical machine translation. In Proceedings of the 41st Annual Meeting on Association for Computational Linguistics-Volume 1, pages 160–167. Association for Computational Linguistics.</rawString>
</citation>
<citation valid="true">
<authors>
<author>Kishore Papineni</author>
<author>Salim Roukos</author>
<author>Todd Ward</author>
<author>WeiJing Zhu</author>
</authors>
<title>BLEU: a method for automatic evaluation of machine translation.</title>
<date>2002</date>
<booktitle>In Proceedings of the 40th annual meeting on association for computational linguistics,</booktitle>
<pages>311--318</pages>
<publisher>Association</publisher>
<institution>for Computational Linguistics.</institution>
<contexts>
<context position="15321" citStr="Papineni et al., 2002" startWordPosition="2569" endWordPosition="2572">49.02- 33.63- 70.0/41.2/26.3/17.4 NN 48.09++ 34.35++ 70.1/41.9/27.0/18.0 Base 57.39- - 30.13- - 67.1/38.3/23.0/14.0 JE BLM 56.79 30.81 67.7/38.9/23.6/14.5 ME 56.48 31.01 67.6/39.0/23.8/14.7 NN 55.96++ 31.77++ 67.8/39.7/24.6/15.4 Table 3: Translation results. The symbol ++ (- -) represents a significant difference at the p &lt; 0.01 level and - represents a significant difference at the p &lt; 0.05 level against the BLM. We compare our method with the baseline and the boundary learning method (BLM) (Xiong et al., 2010) based on Maximum Entropy Markov Models with Markov order 2. Table 3 reports BLEU (Papineni et al., 2002) and TER (Snover et al., 2006) scores. Significance tests are conducted using bootstrap sampling (Koehn, 2004). Our ME classifiers achieve comparable translation improvement with the BLM and NN classifiers enhance translation system significantly compared to others. Table 3 also shows that the relative gain was higher for higher n-grams, which is reasonable since the higher n-grams have higher ambiguities in the translation rule application. It is true that because of multiple parallel sentences, a source span can be applied with transla3http://hlt.fbk.eu/en/irstlm 186 tion rules in one senten</context>
</contexts>
<marker>Papineni, Roukos, Ward, Zhu, 2002</marker>
<rawString>Kishore Papineni, Salim Roukos, Todd Ward, and WeiJing Zhu. 2002. BLEU: a method for automatic evaluation of machine translation. In Proceedings of the 40th annual meeting on association for computational linguistics, pages 311–318. Association for Computational Linguistics.</rawString>
</citation>
<citation valid="true">
<authors>
<author>Matthew Snover</author>
<author>Bonnie Dorr</author>
<author>Richard Schwartz</author>
<author>Linnea Micciulla</author>
<author>John Makhoul</author>
</authors>
<title>A study of translation edit rate with targeted human annotation.</title>
<date>2006</date>
<booktitle>In Proceedings of association for machine translation in the Americas,</booktitle>
<pages>223--231</pages>
<contexts>
<context position="15351" citStr="Snover et al., 2006" startWordPosition="2575" endWordPosition="2578">4 NN 48.09++ 34.35++ 70.1/41.9/27.0/18.0 Base 57.39- - 30.13- - 67.1/38.3/23.0/14.0 JE BLM 56.79 30.81 67.7/38.9/23.6/14.5 ME 56.48 31.01 67.6/39.0/23.8/14.7 NN 55.96++ 31.77++ 67.8/39.7/24.6/15.4 Table 3: Translation results. The symbol ++ (- -) represents a significant difference at the p &lt; 0.01 level and - represents a significant difference at the p &lt; 0.05 level against the BLM. We compare our method with the baseline and the boundary learning method (BLM) (Xiong et al., 2010) based on Maximum Entropy Markov Models with Markov order 2. Table 3 reports BLEU (Papineni et al., 2002) and TER (Snover et al., 2006) scores. Significance tests are conducted using bootstrap sampling (Koehn, 2004). Our ME classifiers achieve comparable translation improvement with the BLM and NN classifiers enhance translation system significantly compared to others. Table 3 also shows that the relative gain was higher for higher n-grams, which is reasonable since the higher n-grams have higher ambiguities in the translation rule application. It is true that because of multiple parallel sentences, a source span can be applied with transla3http://hlt.fbk.eu/en/irstlm 186 tion rules in one sentence pair but not in another sen</context>
</contexts>
<marker>Snover, Dorr, Schwartz, Micciulla, Makhoul, 2006</marker>
<rawString>Matthew Snover, Bonnie Dorr, Richard Schwartz, Linnea Micciulla, and John Makhoul. 2006. A study of translation edit rate with targeted human annotation. In Proceedings of association for machine translation in the Americas, pages 223–231.</rawString>
</citation>
<citation valid="true">
<authors>
<author>Ashish Vaswani</author>
<author>Yinggong Zhao</author>
<author>Victoria Fossum</author>
<author>David Chiang</author>
</authors>
<title>Decoding with largescale neural language models improves translation.</title>
<date>2013</date>
<booktitle>In Proceedings of the 2013 Conference on Empirical Methods in Natural Language Processing,</booktitle>
<pages>1387--1392</pages>
<publisher>Association</publisher>
<institution>for Computational Linguistics.</institution>
<location>Seattle, Washington, USA,</location>
<contexts>
<context position="9606" citStr="Vaswani et al., 2013" startWordPosition="1594" endWordPosition="1597">or learning a series of binary classifiers. For the first method, each Ck is individually learned using the maximum entropy (ME) approach (Berger et al., 1996): exp (Zt µtht (v, f (D) , i, j)) Pk (v|f (D) , i, j) = Zv/ exp (Zt µtht (v&apos;, f (D) , i, j)) (5) where ht is a feature function and µt is weight of ht. We use rich source contextual features: unigram, bigram and trigram of the phrase [fi−3, ..., fj+3]. As the second method, these classification tasks are learned in the continuous space using feedforward neural networks (NNs). Each Ck has the similar structure with the NN language model (Vaswani et al., 2013). The inputs to the NN are indices of the words: [fi−3, ..., fj+3]. Each source word is projected into an N dimensional vector. The output layer has two output neurons, whose values correspond to Pk (v = 0|f (D) , i, j) and Pk (v = 1|f (D) , i, j). For both ME and NN approaches, words that occur only once or never occur in the training corpus are treated as a special word “UNK” (unknown) during classifier training and predicting, which can reduce training time and make the classifier training more smooth. 4 Experiment We evaluated the effectiveness of the proposed approach for Chinese-to-Engli</context>
<context position="11562" citStr="Vaswani et al., 2013" startWordPosition="1934" endWordPosition="1937">ab 2 for Japanese. To learn the classifiers for each translation task, the training set and development set were put together to obtain symmetric word alignment using GIZA++ (Och and Ney, 2003) and the growdiag-final-and heuristic (Koehn et al., 2003). The source span instances extracted from the aligned training and development sets were used as the training and validation data for the classifiers. The toolkit Wapiti (Lavergne et al., 2010) was adopted to train ME classifiers using the classical quasi-newton optimization algorithm with limited memory. The NNs are trained by the toolkit NPLM (Vaswani et al., 2013). We chose “rectifier” as the activation function and the logarithmic loss function for NNs. The number of epochs was set to 20. Other parameters were set to default 2http://sourceforge.net/projects/mecab/files/ 185 Span CE JE length Rate ME NN Rate ME NN P N P N P N P N 1 2.67 0.93 0.63 0.93 0.64 1.08 0.85 0.79 0.86 0.80 2 1.37 0.83 0.70 0.82 0.75 0.73 0.69 0.84 0.71 0.87 3 0.86 0.70 0.80 0.73 0.83 0.52 0.56 0.89 0.63 0.90 4 0.62 0.57 0.81 0.67 0.88 0.36 0.48 0.93 0.54 0.93 5 0.48 0.52 0.90 0.61 0.91 0.26 0.30 0.96 0.47 0.95 6 0.40 0.47 0.91 0.58 0.92 0.20 0.25 0.97 0.41 0.96 7 0.34 0.40 0.93</context>
</contexts>
<marker>Vaswani, Zhao, Fossum, Chiang, 2013</marker>
<rawString>Ashish Vaswani, Yinggong Zhao, Victoria Fossum, and David Chiang. 2013. Decoding with largescale neural language models improves translation. In Proceedings of the 2013 Conference on Empirical Methods in Natural Language Processing, pages 1387–1392, Seattle, Washington, USA, October. Association for Computational Linguistics.</rawString>
</citation>
<citation valid="true">
<authors>
<author>Deyi Xiong</author>
<author>Min Zhang</author>
<author>Haizhou Li</author>
</authors>
<title>Learning translation boundaries for phrase-based decoding. In Human Language Technologies: The</title>
<date>2010</date>
<booktitle>Annual Conference of the North American Chapter of the Association for Computational Linguistics,</booktitle>
<pages>136--144</pages>
<publisher>Association</publisher>
<institution>for Computational Linguistics.</institution>
<contexts>
<context position="4436" citStr="Xiong et al., 2010" startWordPosition="648" endWordPosition="651">ment by” contains words projected from the source words outside “experiment by”. In general, a translation rule projects one continuous source word sequence (source span) into one continuous target word sequence. Meanwhile, the word alignment links between the source and target sentence define the source spans where translation rules are applicable. In this paper, we call a source span that can be covered by a translation rule without violating word alignment links a translation span. Translation spans that have been correctly identified can guide translation rules to function properly, thus (Xiong et al., 2010) attempted to use extra machine learning approaches to determine boundaries of translation spans. They used two separate classifiers to learn the beginning and ending boundaries of translation spans, respectively. A source word is marked as beginning (ending) boundary if it is the first (last) word of a translation span. However, a source span whose first and last words are both boundaries is not always a translation span. In Figure 1, “I” is a beginning boundary since it is the first word of translation span “I will” and “experiment” is an ending boundary since it is the last word of translat</context>
<context position="15216" citStr="Xiong et al., 2010" startWordPosition="2551" endWordPosition="2554">isions 4 1 2 3 4 Base 49.39- - 33.07- - 69.9/40.7/25.8/16.9 CE BLM 48.60 33.93 70.0/41.4/26.6/17.6 ME 49.02- 33.63- 70.0/41.2/26.3/17.4 NN 48.09++ 34.35++ 70.1/41.9/27.0/18.0 Base 57.39- - 30.13- - 67.1/38.3/23.0/14.0 JE BLM 56.79 30.81 67.7/38.9/23.6/14.5 ME 56.48 31.01 67.6/39.0/23.8/14.7 NN 55.96++ 31.77++ 67.8/39.7/24.6/15.4 Table 3: Translation results. The symbol ++ (- -) represents a significant difference at the p &lt; 0.01 level and - represents a significant difference at the p &lt; 0.05 level against the BLM. We compare our method with the baseline and the boundary learning method (BLM) (Xiong et al., 2010) based on Maximum Entropy Markov Models with Markov order 2. Table 3 reports BLEU (Papineni et al., 2002) and TER (Snover et al., 2006) scores. Significance tests are conducted using bootstrap sampling (Koehn, 2004). Our ME classifiers achieve comparable translation improvement with the BLM and NN classifiers enhance translation system significantly compared to others. Table 3 also shows that the relative gain was higher for higher n-grams, which is reasonable since the higher n-grams have higher ambiguities in the translation rule application. It is true that because of multiple parallel sent</context>
</contexts>
<marker>Xiong, Zhang, Li, 2010</marker>
<rawString>Deyi Xiong, Min Zhang, and Haizhou Li. 2010. Learning translation boundaries for phrase-based decoding. In Human Language Technologies: The 2010 Annual Conference of the North American Chapter of the Association for Computational Linguistics, pages 136–144. Association for Computational Linguistics.</rawString>
</citation>
<citation valid="true">
<authors>
<author>Hai Zhao</author>
<author>Chunyu Kit</author>
</authors>
<title>Exploiting unlabeled text with different unsupervised segmentation criteria for chinese word segmentation.</title>
<date>2008</date>
<journal>Research in Computing Science,</journal>
<pages>33--93</pages>
<contexts>
<context position="10862" citStr="Zhao and Kit, 2008" startWordPosition="1817" endWordPosition="1820">anslation tasks. The datasets officially provided for the patent machine translation task at NTCIR-9 (Goto et al., 2011) were used in our experiments. The detailed training set statistics are given in Table 1. The development and test SOURCE TARGET #Sents 954k CE #Words 37.2M 40.4M #Vocab 288k 504k #Sents 3.14M JE #Words 118M 104M #Vocab 150k 273k Table 1: Data sets. sets were both provided for CE task while only the test set was provided for JE task. Therefore, we used the sentences from the NTCIR-8 JE test set as the development set. Word segmentation was done by BaseSeg (Zhao et al., 2006; Zhao and Kit, 2008; Zhao et al., 2010; Zhao and Kit, 2011; Zhao et al., 2013) for Chinese and Mecab 2 for Japanese. To learn the classifiers for each translation task, the training set and development set were put together to obtain symmetric word alignment using GIZA++ (Och and Ney, 2003) and the growdiag-final-and heuristic (Koehn et al., 2003). The source span instances extracted from the aligned training and development sets were used as the training and validation data for the classifiers. The toolkit Wapiti (Lavergne et al., 2010) was adopted to train ME classifiers using the classical quasi-newton optimi</context>
</contexts>
<marker>Zhao, Kit, 2008</marker>
<rawString>Hai Zhao and Chunyu Kit. 2008. Exploiting unlabeled text with different unsupervised segmentation criteria for chinese word segmentation. Research in Computing Science, 33:93–104.</rawString>
</citation>
<citation valid="true">
<authors>
<author>Hai Zhao</author>
<author>Chunyu Kit</author>
</authors>
<title>Integrating unsupervised and supervised word segmentation: The role of goodness measures.</title>
<date>2011</date>
<journal>Information Sciences,</journal>
<volume>181</volume>
<issue>1</issue>
<contexts>
<context position="10901" citStr="Zhao and Kit, 2011" startWordPosition="1825" endWordPosition="1828">y provided for the patent machine translation task at NTCIR-9 (Goto et al., 2011) were used in our experiments. The detailed training set statistics are given in Table 1. The development and test SOURCE TARGET #Sents 954k CE #Words 37.2M 40.4M #Vocab 288k 504k #Sents 3.14M JE #Words 118M 104M #Vocab 150k 273k Table 1: Data sets. sets were both provided for CE task while only the test set was provided for JE task. Therefore, we used the sentences from the NTCIR-8 JE test set as the development set. Word segmentation was done by BaseSeg (Zhao et al., 2006; Zhao and Kit, 2008; Zhao et al., 2010; Zhao and Kit, 2011; Zhao et al., 2013) for Chinese and Mecab 2 for Japanese. To learn the classifiers for each translation task, the training set and development set were put together to obtain symmetric word alignment using GIZA++ (Och and Ney, 2003) and the growdiag-final-and heuristic (Koehn et al., 2003). The source span instances extracted from the aligned training and development sets were used as the training and validation data for the classifiers. The toolkit Wapiti (Lavergne et al., 2010) was adopted to train ME classifiers using the classical quasi-newton optimization algorithm with limited memory. T</context>
</contexts>
<marker>Zhao, Kit, 2011</marker>
<rawString>Hai Zhao and Chunyu Kit. 2011. Integrating unsupervised and supervised word segmentation: The role of goodness measures. Information Sciences, 181(1):163–183.</rawString>
</citation>
<citation valid="true">
<authors>
<author>Hai Zhao</author>
<author>Chang-Ning Huang</author>
<author>Mu Li</author>
</authors>
<title>An improved chinese word segmentation system with conditional random field.</title>
<date>2006</date>
<booktitle>In Proceedings of the Fifth SIGHAN Workshop on Chinese Language Processing,</booktitle>
<pages>162--165</pages>
<location>Sydney:</location>
<contexts>
<context position="10842" citStr="Zhao et al., 2006" startWordPosition="1813" endWordPosition="1816">eto-English (JE) translation tasks. The datasets officially provided for the patent machine translation task at NTCIR-9 (Goto et al., 2011) were used in our experiments. The detailed training set statistics are given in Table 1. The development and test SOURCE TARGET #Sents 954k CE #Words 37.2M 40.4M #Vocab 288k 504k #Sents 3.14M JE #Words 118M 104M #Vocab 150k 273k Table 1: Data sets. sets were both provided for CE task while only the test set was provided for JE task. Therefore, we used the sentences from the NTCIR-8 JE test set as the development set. Word segmentation was done by BaseSeg (Zhao et al., 2006; Zhao and Kit, 2008; Zhao et al., 2010; Zhao and Kit, 2011; Zhao et al., 2013) for Chinese and Mecab 2 for Japanese. To learn the classifiers for each translation task, the training set and development set were put together to obtain symmetric word alignment using GIZA++ (Och and Ney, 2003) and the growdiag-final-and heuristic (Koehn et al., 2003). The source span instances extracted from the aligned training and development sets were used as the training and validation data for the classifiers. The toolkit Wapiti (Lavergne et al., 2010) was adopted to train ME classifiers using the classical</context>
</contexts>
<marker>Zhao, Huang, Li, 2006</marker>
<rawString>Hai Zhao, Chang-Ning Huang, and Mu Li. 2006. An improved chinese word segmentation system with conditional random field. In Proceedings of the Fifth SIGHAN Workshop on Chinese Language Processing, pages 162–165. Sydney: July.</rawString>
</citation>
<citation valid="true">
<authors>
<author>Hai Zhao</author>
<author>Chang-Ning Huang</author>
<author>Mu Li</author>
<author>Bao-Liang Lu</author>
</authors>
<title>A unified character-based tagging framework for chinese word segmentation.</title>
<date>2010</date>
<journal>ACM Transactions on Asian Language Information Processing (TALIP),</journal>
<volume>9</volume>
<issue>2</issue>
<contexts>
<context position="10881" citStr="Zhao et al., 2010" startWordPosition="1821" endWordPosition="1824"> datasets officially provided for the patent machine translation task at NTCIR-9 (Goto et al., 2011) were used in our experiments. The detailed training set statistics are given in Table 1. The development and test SOURCE TARGET #Sents 954k CE #Words 37.2M 40.4M #Vocab 288k 504k #Sents 3.14M JE #Words 118M 104M #Vocab 150k 273k Table 1: Data sets. sets were both provided for CE task while only the test set was provided for JE task. Therefore, we used the sentences from the NTCIR-8 JE test set as the development set. Word segmentation was done by BaseSeg (Zhao et al., 2006; Zhao and Kit, 2008; Zhao et al., 2010; Zhao and Kit, 2011; Zhao et al., 2013) for Chinese and Mecab 2 for Japanese. To learn the classifiers for each translation task, the training set and development set were put together to obtain symmetric word alignment using GIZA++ (Och and Ney, 2003) and the growdiag-final-and heuristic (Koehn et al., 2003). The source span instances extracted from the aligned training and development sets were used as the training and validation data for the classifiers. The toolkit Wapiti (Lavergne et al., 2010) was adopted to train ME classifiers using the classical quasi-newton optimization algorithm wi</context>
</contexts>
<marker>Zhao, Huang, Li, Lu, 2010</marker>
<rawString>Hai Zhao, Chang-Ning Huang, Mu Li, and Bao-Liang Lu. 2010. A unified character-based tagging framework for chinese word segmentation. ACM Transactions on Asian Language Information Processing (TALIP), 9(2):5.</rawString>
</citation>
<citation valid="true">
<authors>
<author>Hai Zhao</author>
<author>Masao Utiyama</author>
<author>Eiichiro Sumita</author>
<author>BaoLiang Lu</author>
</authors>
<title>An empirical study on word segmentation for chinese machine translation.</title>
<date>2013</date>
<booktitle>In Computational Linguistics and Intelligent Text Processing,</booktitle>
<pages>248--263</pages>
<publisher>Springer.</publisher>
<contexts>
<context position="10921" citStr="Zhao et al., 2013" startWordPosition="1829" endWordPosition="1832">atent machine translation task at NTCIR-9 (Goto et al., 2011) were used in our experiments. The detailed training set statistics are given in Table 1. The development and test SOURCE TARGET #Sents 954k CE #Words 37.2M 40.4M #Vocab 288k 504k #Sents 3.14M JE #Words 118M 104M #Vocab 150k 273k Table 1: Data sets. sets were both provided for CE task while only the test set was provided for JE task. Therefore, we used the sentences from the NTCIR-8 JE test set as the development set. Word segmentation was done by BaseSeg (Zhao et al., 2006; Zhao and Kit, 2008; Zhao et al., 2010; Zhao and Kit, 2011; Zhao et al., 2013) for Chinese and Mecab 2 for Japanese. To learn the classifiers for each translation task, the training set and development set were put together to obtain symmetric word alignment using GIZA++ (Och and Ney, 2003) and the growdiag-final-and heuristic (Koehn et al., 2003). The source span instances extracted from the aligned training and development sets were used as the training and validation data for the classifiers. The toolkit Wapiti (Lavergne et al., 2010) was adopted to train ME classifiers using the classical quasi-newton optimization algorithm with limited memory. The NNs are trained b</context>
</contexts>
<marker>Zhao, Utiyama, Sumita, Lu, 2013</marker>
<rawString>Hai Zhao, Masao Utiyama, Eiichiro Sumita, and BaoLiang Lu. 2013. An empirical study on word segmentation for chinese machine translation. In Computational Linguistics and Intelligent Text Processing, pages 248–263. Springer.</rawString>
</citation>
</citationList>
</algorithm>
</algorithms>
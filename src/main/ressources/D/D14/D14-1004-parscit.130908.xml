<?xml version="1.0" encoding="UTF-8"?>
<algorithms version="110505">
<algorithm name="SectLabel" version="110505">
<variant no="0" confidence="0.000060">
<title confidence="0.996094">
A Neural Network Approach to Selectional Preference Acquisition
</title>
<author confidence="0.863951">
Tim Van de Cruys
</author>
<affiliation confidence="0.585285">
IRIT &amp; CNRS
</affiliation>
<address confidence="0.757666">
Toulouse, France
</address>
<email confidence="0.962914">
tim.vandecruys@irit.fr
</email>
<sectionHeader confidence="0.99269" genericHeader="abstract">
Abstract
</sectionHeader>
<bodyText confidence="0.999873611111111">
This paper investigates the use of neural
networks for the acquisition of selectional
preferences. Inspired by recent advances
of neural network models for NLP applica-
tions, we propose a neural network model
that learns to discriminate between felici-
tous and infelicitous arguments for a par-
ticular predicate. The model is entirely un-
supervised – preferences are learned from
unannotated corpus data. We propose two
neural network architectures: one that han-
dles standard two-way selectional prefer-
ences and one that is able to deal with
multi-way selectional preferences. The
model’s performance is evaluated on a
pseudo-disambiguation task, on which it
is shown to achieve state of the art perfor-
mance.
</bodyText>
<sectionHeader confidence="0.998984" genericHeader="introduction">
1 Introduction
</sectionHeader>
<bodyText confidence="0.998786666666667">
Predicates often have a semantically motivated pref-
erence for particular arguments. Compare for ex-
ample the sentences in (1) and (2).
</bodyText>
<listItem confidence="0.9785855">
(1) The vocalist sings a ballad.
(2) The exception sings a tomato.
</listItem>
<bodyText confidence="0.999963094339623">
Most language users would have no problems ac-
cepting the first sentence as well-formed: a vocalist
can be expected to sing, and a ballad is something
that can be sung. The same language users, how-
ever, would likely consider the second sentence to
be ill-formed: an exception is not supposed to sing,
nor is a tomato something that is typically sung.
Within the field of natural language processing,
this inclination of predicates to select for particular
arguments is known as selectional preference.
The automatic acquisition of selectional prefer-
ences has been a popular research subject within
the field of natural language processing. An auto-
matically acquired selectional preference resource
is a versatile tool for numerous NLP applications,
such as semantic role labeling (Gildea and Jurafsky,
2002), word sense disambiguation (McCarthy and
Carroll, 2003), and metaphor processing (Shutova
et al., 2013).
Models for selectional preference need to ade-
quately deal with the consequences of Zipf’s law:
language is inherently sparse, and the majority of
language utterances occur very infrequently. As
a consequence, models that are based on corpus
data need to properly generalize beyond the mere
co-occurrence frequencies of sparse corpus data,
taking into account the semantic similarity of both
predicates and arguments. Researchers have come
up with various approaches to this generalization
step. Earlier approaches to selectional preference
acquisition mostly rely on hand-crafted resources
such as WordNet (Resnik, 1996; Li and Abe, 1998;
Clark and Weir, 2001), while later approaches tend
to take advantage of unsupervised learning machin-
ery, such as latent variable models (Rooth et al.,
1999; O´ S´eaghdha, 2010) and distributional simi-
larity metrics (Erk, 2007; Pad´o et al., 2007).
This paper investigates the use of neural net-
works for the acquisition of selectional preferences.
Inspired by recent advances of neural network mod-
els for NLP applications (Collobert and Weston,
2008; Mikolov et al., 2013), we propose a neural
network model that learns to discriminate between
felicitous and infelicitous arguments for a particu-
lar predicate. The model is entirely unsupervised –
preferences are learned from unannotated corpus
data. Positive training instances are constructed
from attested corpus data, while negative instances
are constructed from randomly corrupted instances.
We propose two neural network architectures: one
that handles standard two-way selectional prefer-
ences and one that is able to deal with multi-way
selectional preferences, where the interaction be-
</bodyText>
<page confidence="0.964556">
26
</page>
<note confidence="0.9094365">
Proceedings of the 2014 Conference on Empirical Methods in Natural Language Processing (EMNLP), pages 26–35,
October 25-29, 2014, Doha, Qatar. c�2014 Association for Computational Linguistics
</note>
<bodyText confidence="0.999933791666667">
tween multiple verb arguments is taken into ac-
count. The model’s performance is evaluated on a
pseudo-disambiguation task, on which it is shown
to achieve state of the art performance.
The contributions of this paper are twofold. First
of all, we apply and evaluate a neural network ap-
proach to the problem of standard (two-way) se-
lectional preference acquisition. Selectional pref-
erence acquisition using neural networks has not
yet been explored in the literature. Secondly, we
propose a novel network architecture and training
objective for the acquisition of multi-way selec-
tional preferences, where the interaction between
a verb and its various arguments is captured at the
same time.
The remainder of this paper is as follows. Sec-
tion 2 first discusses related work with respect to se-
lectional preference acquisition and neural network
modeling. Section 3 describes our neural network
architecture and its training procedure. Section 4
evaluates the model’s performance, comparing it
to other existing models for selectional preference
acquisition. Finally, section 5 concludes and indi-
cates a number of avenues for future work.
</bodyText>
<sectionHeader confidence="0.999909" genericHeader="related work">
2 Related Work
</sectionHeader>
<subsectionHeader confidence="0.999735">
2.1 Selectional preferences
</subsectionHeader>
<bodyText confidence="0.999996222222222">
One of the first approaches to the automatic induc-
tion of selectional preferences from corpora was
the one by Resnik (1996). Resnik (1996) relies
on WordNet synsets in order to generate gener-
alized noun clusters. The selectional preference
strength of a specific verb v in a particular relation
is calculated by computing the Kullback-Leibler
divergence between the cluster distribution of the
verb and the prior cluster distribution.
</bodyText>
<equation confidence="0.9976215">
SR(v) = Ec p(c|v)log p(c|v) (1)
p(c)
</equation>
<bodyText confidence="0.9996402">
where c stands for a noun cluster, and R stands for a
given predicate-argument relation. The selectional
association of a particular noun cluster is then the
contribution of that cluster to the verb’s preference
strength.
</bodyText>
<equation confidence="0.999594">
AR(v,c) = p(c|v)log p(c|v)
SR(v) p(c) (2)
</equation>
<bodyText confidence="0.999882083333334">
The model’s generalization relies entirely on Word-
Net, and there is no generalization among the verbs.
Other researchers have equally relied on Word-
Net in order to generalize over arguments. Li and
Abe (1998) use the principle of Minimum Descrip-
tion Length in order to find a suitable generalization
level within the lexical WordNet hierarchy. A same
intuition is used by Clark and Weir (2001), but they
use hypothesis testing instead to find the appro-
priate level of generalization. A recent approach
that makes use of WordNet (in combination with
Bayesian modeling) is the one by O´ S´eaghdha and
Korhonen (2012).
Most researchers, however, acknowledge the
shortcomings of hand-crafted resources, and fo-
cus on the acquisition of selectional preferences
from corpus data. Rooth et al. (1999) propose an
Expectation-Maximization (EM) clustering algo-
rithm for selectional preference acquisition based
on a probabilistic latent variable model. The idea
is that both predicate v and argument o are gen-
erated from a latent variable c, where the latent
variables represent clusters of tight verb-argument
interactions.
</bodyText>
<equation confidence="0.997724">
p(v,o) = Ep(c,v,o) = E p(c)p(v|c)p(o|c) (3)
c∈C c∈C
</equation>
<bodyText confidence="0.999918333333333">
The use of latent variables allows the model to
generalize to predicate-argument tuples that have
not been seen during training. The latent variable
distribution – and the probabilities of predicates
and argument given the latent variables – are au-
tomatically induced from data using EM. We will
compare against their model for evaluation pur-
poses.
Erk (2007) and Erk et al. (2010) describe a
method that uses corpus-driven distributional simi-
larity metrics for the induction of selectional pref-
erences. The key idea is that a predicate-argument
tuple (v,o) is felicitous if the predicate v appears
in the training corpus with arguments o0 that are
similar to o, i.e.
</bodyText>
<equation confidence="0.934452">
· sim(o,o0) (4)
</equation>
<bodyText confidence="0.999814888888889">
where Ov represents the set of arguments that have
been attested with predicate v, wt(·) represents an
appropriate weighting function (such as the fre-
quency of the (v,o0) tuple), and Z is a normaliza-
tion factor. We equally compare to their model for
evaluation purposes.
Bergsma et al. (2008) present a discriminative
approach to selectional preference acquisition. Pos-
itive examples are taken from observed predicate-
</bodyText>
<equation confidence="0.9709755">
S(v,o) = E wt(v,o0)
o0∈Ov Z(v)
</equation>
<page confidence="0.983224">
27
</page>
<bodyText confidence="0.999959285714286">
argument pairs, while negative examples are con-
structed from unobserved combinations. An SVM
classifier is used to distinguish the positive from the
negative instances. The training procedure used in
their model is based on an intuition that is similar
to ours, although it is implemented using different
techniques.
A number of researchers presented models that
are based on the framework of topic modeling. O´
S´eaghdha (2010) describes three models for selec-
tional preference induction based on Latent Dirich-
let Allocation, which model the selectional pref-
erence of a predicate and a single argument. Rit-
ter et al. (2010) equally present a selectional pref-
erence model based on topic modeling, but they
tackle multi-way selectional preferences (of transi-
tive predicates, which take two arguments) instead.
Finally, in previous work (Van de Cruys, 2009)
we presented a model for multi-way selectional
preference induction based on tensor factorization.
Three-way co-occurrences of subjects, verbs, and
objects are represented as a three-way tensor (the
generalization of a matrix), and a latent factoriza-
tion model is applied in order to generalize to
unseen instances. We will compare our neural
network based-approach for multi-way selectional
preference acquisition to this tensor-based factor-
ization model.
</bodyText>
<subsectionHeader confidence="0.997724">
2.2 Neural networks
</subsectionHeader>
<bodyText confidence="0.9999773">
In the last few years, neural networks have become
increasingly popular in NLP applications. In partic-
ular, neural language models (Bengio et al., 2003;
Mnih and Hinton, 2007; Collobert and Weston,
2008) have demonstrated impressive performance
at the task of language modeling. By incorporating
distributed representations for words that model
their similarity, neural language models are able
to overcome the problem of data sparseness that
standard n-gram models are confronted with. Also
related to our work is the approach by Tsubaki et
al. (2013), who successfully use a neural network
to model co-compositionality.
Our model for selectional preference acquisition
uses a network architecture that is similar to the
abovementioned models. Its training objective is
also similar to the ranking-loss training objective
proposed by Collobert and Weston (2008), but we
present a novel, modified version in order to deal
with multi-way selectional preferences.
</bodyText>
<sectionHeader confidence="0.999384" genericHeader="method">
3 Methodology
</sectionHeader>
<subsectionHeader confidence="0.998706">
3.1 Neural network architecture
</subsectionHeader>
<bodyText confidence="0.99999425">
Our model computes the score for a predicate i
and an argument j as follows. First, the selectional
preference tuple (i, j) is represented as the concate-
nation of the vectors vi and oj, i.e.
</bodyText>
<equation confidence="0.981394">
x = [vi,oj] (5)
</equation>
<bodyText confidence="0.999682583333333">
Vectors vi and oj are extracted from two embedding
matrices, V ∈ RN×I (the predicate matrix, where I
represents the number of elements in the predicate
vocabulary) and O ∈ RN×J (the argument matrix,
where J represents the number of elements in the
argument vocabulary). N is a parameter setting of
the model, representing the vector size of the em-
beddings. Matrices V and O will be automatically
learned during training.
Vector x then serves as input vector to our neural
network. We use a feed-forward neural network
architecture with one hidden layer:
</bodyText>
<equation confidence="0.9999435">
a1 = f(W1x+b1) (6)
y = W2a1 (7)
</equation>
<bodyText confidence="0.999991888888889">
where x ∈ R2N is our input vector, a1 ∈ RH repre-
sents the activation of the hidden layer with H hid-
den nodes, W1 ∈ RH×2N and W2 ∈ R1×H respec-
tively represent the first and second layer weights,
b1 represents the first layer’s bias, f (·) represents
the element-wise activation function tanh, and y is
our final selectional preference score. The left-hand
picture of figure 1 gives a graphical representation
of our standard neural network architecture.
</bodyText>
<subsectionHeader confidence="0.999796">
3.2 Training the network
</subsectionHeader>
<bodyText confidence="0.9999486875">
A proper estimation of a neural network’s param-
eters requires a large amount of training data. To
be able to use non-annotated corpus data for train-
ing, we use the method proposed by Collobert and
Weston (2008). The authors present a method for
training a neural network language model from un-
labeled data by corrupting actual attested n-grams
with a random word. They then define a ranking-
type cost function, which allows the network to
learn to discriminate between good and bad word
sequences. We adopt the same method for our se-
lectional preference model as follows.
Let (i, j) be our proper, attested predicate-
argument tuple. The goal of our model is to dis-
criminate the correct tuple (i, j) from other, non-
attested tuples (i, j0), in which the correct predicate
</bodyText>
<page confidence="0.993289">
28
</page>
<figure confidence="0.9971693">
i
j
x
y
W2
V
O
a1
W1
i
k
V
S
O
j
x
W1
a1
W2
y
</figure>
<figureCaption confidence="0.998966">
Figure 1: Neural network architectures for selectional preference acquisition. The left-hand picture shows
</figureCaption>
<bodyText confidence="0.966706111111111">
the architecture for two-way selectional preferences, the right-hand picture shows the architecture for
three-way selectional preferences. In both cases, vector x is constructed from the appropriate predicate
and argument vectors from the embedding matrices, and fed forward through the network to yield a
preference score y.
j has been replaced with a random predicate j0. We
require the score for the correct tuple to be larger
than the score for the corrupt tuple by a margin
of one. For one tuple (i, j), this corresponds to
minimizing the objective function in (8)
</bodyText>
<equation confidence="0.947185">
∑ max(0,1−g[(i, j)] +g[(i, j0)]) (8)
j0∈J
</equation>
<bodyText confidence="0.999964666666667">
where J represents the predicate vocabulary, and
g[·] represents our neural network scoring function
presented in the previous section.
In line with Collobert and Weston (2008), the
gradient of the objective function is sampled by
randomly picking one corrupt argument j0 from the
argument vocabulary for each attested predicate-
argument tuple (i, j). The derivative of the cost
with respect to the model’s parameters (weight ma-
trices W1 and W2, bias vector b1, and embedding
matrices V and O) is computed, and the appropriate
parameters are updated through backpropagation.
</bodyText>
<subsectionHeader confidence="0.999559">
3.3 Multi-way selectional preferences
</subsectionHeader>
<bodyText confidence="0.961465352941176">
The model presented in the previous section is
only able to deal with two-way selectional pref-
erences. In this section, we present an extension of
the model that is able to handle multi-way selec-
tional preferences.1
1We exemplify the model using three-way selectional pref-
erences for transitive predicates, but the model can be straight-
forwardly generalized to other multi-way selectional prefer-
ences.
In order to model the selectional preference of a
transitive verb for its subject and direct object, we
start out in a similar fashion to the two-way case.
Instead of having only one embedding matrix, we
now have two embedding matrices S ∈ RN×J and
O ∈ RN×x, representing the two different argument
slots of a transitive predicate. Our input vector can
now be represented as
</bodyText>
<equation confidence="0.997025">
x = (Vi,sj,ok) (9)
</equation>
<bodyText confidence="0.999837157894737">
Note that x ∈ R3N and W1 ∈ RH×3N. The rest of
our neural network architecture stays exactly the
same. The right-hand picture of figure 1 presents a
graphical representation.
For the multi-way case, we present an adapted
version of the training objective. Given an attested
subject-verb-object tuple (i, j,k), the goal of our
network is now to discriminate this correct tuple
from other, corrupted tuples (i, j,k0), (i, j0,k) and
(i, j0,k0), where the correct arguments have been
replaced by random subjects j0 and random objects
k0. Note that we do not only want the network
to learn the infelicity of tuples in which both the
subject and object slot are corrupted; we also want
our network to learn the infelicity of tuples in which
either the subject or object slot is corrupt, while the
other slot contains the correct, attested argument.
This leads us to the objective function represented
in (10).
</bodyText>
<page confidence="0.993542">
29
</page>
<bodyText confidence="0.999964">
As in the two-way case, the gradient of the objec-
tive function is sampled by randomly picking one
corrupted subject j0 and one corrupted object k0 for
each tuple (i, j,k). All of the model’s parameters
are again updated through backpropagation.
</bodyText>
<sectionHeader confidence="0.999846" genericHeader="method">
4 Evaluation
</sectionHeader>
<subsectionHeader confidence="0.997218">
4.1 Implementational details
</subsectionHeader>
<bodyText confidence="0.999952451612903">
We evaluate our neural network approach to se-
lectional preference acquisition using verb-object
tuples for the two-way model, and subject-verb-
object tuples for the multi-way model.
Our model has been applied to English, using the
UKWaC corpus (Baroni et al., 2009), which covers
about 2 billion words of web text. The corpus
has been part of speech tagged and lemmatized
with Stanford Part-Of-Speech Tagger (Toutanova
et al., 2003), and parsed with MaltParser (Nivre
et al., 2006), so that dependency tuples could be
extracted.
For the two-way model, we select all verbs and
objects that appear within a predicate-argument re-
lation with a frequency of at least 50. This gives
us a total of about 7K verbs and 30K objects. For
the multi-way model, we select the 2K most fre-
quent verbs, together with the 10K most frequent
subjects and the 10K most frequent objects (that
appear within a transitive frame).
All words are converted to lowercase. We use
the lemmatized forms, and only keep those forms
that contain alphabetic characters. Furthermore,
we require each tuple to appear at least three times
in the corpus.
We set N, the size of our embedding matrices, to
50, and H, the number of units in the hidden layer,
to 100. Following Huang et al. (2012), we use
mini-batch L-BFGS (Liu and Nocedal, 1989) with
1000 pairs of good and corrupt tuples per batch for
training, and train for 10 epochs.
</bodyText>
<subsectionHeader confidence="0.9317515">
4.2 Evaluation Setup
4.2.1 Task
</subsectionHeader>
<bodyText confidence="0.999398380952381">
Our models are quantitatively evaluated using a
pseudo-disambiguation task (Rooth et al., 1999),
which bears some resemblance to our training pro-
cedure. The task provides an adequate test of the
generalization capabilities of our models. For the
two-way case, the task is to judge which object (o
or o0) is more likely for a particular verb v, where
(v,o) is a tuple attested in the corpus, and o0 is a di-
rect object randomly drawn from the object vocab-
ulary. The tuple is considered correct if the model
prefers the attested tuple (v,o) over (v,o0). For the
three-way case, the task is to judge which subject
(s or s0) and direct object (o or o0) are more likely
for a particular verb v, where (v,s,o) is the attested
tuple, and s0 and o0 are a random subject and object
drawn from their respective vocabularies. The tu-
ple is considered correct if the model prefers the
attested tuple (v,s,o) over the alternatives (v,s,o0),
(v,s0,o), and (v,s0,o0). Tables 1 and 2 respectively
show a number of examples from the two-way and
three-way pseudo-disambiguation task.
</bodyText>
<figure confidence="0.640063">
v o o0
perform play geometry
buy wine renaissance
read introduction peanut
</figure>
<tableCaption confidence="0.8925995">
Table 1: Pseudo-disambiguation examples for two-
way verb-object tuples
</tableCaption>
<table confidence="0.71823025">
v s o s0 o0
win team game diversity egg
publish government document grid priest
develop company software breakfast landlord
</table>
<tableCaption confidence="0.9246955">
Table 2: Pseudo-disambiguation examples for
three-way subject-verb-object tuples
</tableCaption>
<bodyText confidence="0.998352454545454">
The models are evaluated using 10-fold cross
validation. All tuples from our corpus are randomly
divided into 10 equal parts. Next, for each fold, 9
parts are used for training, and the remaining part
is used for testing. In order to properly test the
generalization capability of our models, we make
sure that all instances of a particular tuple appear in
one part only. This way, we make sure that tuples
used for testing are never seen during training.
For the two-way model, our corpus consists of
about 70M tuple instances (1.9M types), so in each
</bodyText>
<equation confidence="0.527047">
∑ max(0,1−g[(i, j,k)]+g[(i, j,k0)])
k0∈K
+ ∑ max(0,1−g[(i, j,k)]+g[(i, j0,k)])
j0∈J
+ ∑ max(0,1−g[(i, j,k)]+g[(i, j0,k0)]) (10)
j0∈J
k0∈K
</equation>
<page confidence="0.982647">
30
</page>
<bodyText confidence="0.999926363636364">
fold, about 63M tuple instances are used for train-
ing and about 7M (190K types) are used for testing.
For the three-way model, our corpus consists of
about 5,5M tuple instances (750K types), so in
each fold, about 5M tuples are used for training
and about 500K (75K types) are used for testing.
Note that our training procedure is instance-based,
while our evaluation is type-based: during training,
the neural network sees a tuple as many times as it
appears in the training set, while for testing each
individual tuple is only evaluated once.
</bodyText>
<subsectionHeader confidence="0.536505">
4.2.2 Comparison models
</subsectionHeader>
<bodyText confidence="0.9999958">
We compare our neural network model to a number
of other models for selectional preference acquisi-
tion.
For the two-way case, we compare our model
to the EM-based clustering technique presented
by Rooth et al. (1999),2 and to Erk et al.’s (2010)
similarity-based model. For Rooth et al.’s model,
we set the number of latent factors to 50. Us-
ing a larger number of latent factors does not in-
crease performance. For Erk et al.’s model, we
create a dependency-based similarity model from
the UKWaC corpus using our 30K direct objects
as instances and 100K dependency relations as
features. The resulting matrix is weighted using
pointwise mutual information (Church and Hanks,
1990). Similarity values are computed using cosine.
Furthermore, we use a sampling procedure in the
testing phase: we sample 5000 predicate-argument
pairs for each fold, as testing Erk et al.’s model on
the complete test sets proved prohibitively expen-
sive.
For the three-way case, we compare our model
to the tensor factorization model we developed in
previous work (Van de Cruys, 2009). We set the
number of latent factors to 300.3
</bodyText>
<sectionHeader confidence="0.6137355" genericHeader="method">
4.3 Results
4.3.1 Two-way model
</sectionHeader>
<bodyText confidence="0.99462475">
Table 3 compares the results of our neural network
architecture for two-way selectional preferences to
the results of Rooth et al.’s (1999) model and Erk
et al.’s (2010) model.
</bodyText>
<footnote confidence="0.998164625">
2Our own implementation of Rooth et al.’s (1999) al-
gorithm is based on non-negative matrix factorization (Lee
and Seung, 2000). Non-negative matrix factorization with
Kullback-Leibler divergence has been shown to minimize the
same objective function as EM (Li and Ding, 2006).
3The best scoring model presented by Van de Cruys (2009)
also uses 300 latent factors; using more factors does not im-
prove the results.
</footnote>
<table confidence="0.805379">
model accuracy (g ± 6)
Rooth et al. (1999) .720 ± .002
Erk et al. (2010) .887 ± .004
2-way neural network .880 ± .001
</table>
<tableCaption confidence="0.996182">
Table 3: Comparison of model results for two-way
</tableCaption>
<bodyText confidence="0.984295458333333">
selectional preference acquisition – mean accuracy
and standard deviations of 10-fold cross-validation
results
The results indicate that our neural network ap-
proach outperforms Rooth et al.’s (1999) method
by a large margin (16%). Clearly, the neural net-
work architecture is able to model selectional pref-
erences more profoundly than Rooth et al.’s latent
variable approach. The difference between the
models is highly statistically significant (paired
t-test, p &lt; .01), as the standard deviations already
indicate.
Erk et al.’s model reaches a slightly better score
than our model, and this result is also statistically
significant (paired t-test, p &lt; .01). However, Erk et
al.’s model does not provide full coverage, whereas
the other two models are able to compute scores
for all pairs in the test set. In addition, Erk et al.’s
model is much more expensive to compute. Our
model computes selectional preference scores for
the test set in a matter of seconds, whereas for
Erk et al.’s model, we ended up sampling from
the test set, as computing preference values for the
complete test set proved prohibitively expensive.
</bodyText>
<sectionHeader confidence="0.385805" genericHeader="method">
4.3.2 Three-way model
</sectionHeader>
<bodyText confidence="0.949059071428572">
Table 4 compares the results of our neural network
architecture for three-way selectional preference
acquisition to the results of the tensor-based factor-
ization method (Van de Cruys, 2009).
model accuracy (g ± 6)
Van de Cruys (2009) .874 ± .001
3-way neural network .889 ± .001
Table 4: Comparison of model results for three-way
selectional preference acquisition – mean accuracy
and standard deviations of 10-fold cross-validation
results
The results indicate that the neural network ap-
proach slightly outperforms the tensor-based factor-
ization method. Again the model difference is sta-
</bodyText>
<page confidence="0.99972">
31
</page>
<bodyText confidence="0.9998088">
tistically significant (paired t-test, p &lt; 0.01). Using
our adapted training objective, the neural network
is clearly able to learn a rich model of three-way
selectional preferences, reaching state of the art
performance.
</bodyText>
<subsectionHeader confidence="0.995663">
4.4 Examples
</subsectionHeader>
<bodyText confidence="0.999413307692308">
We conclude our results section by briefly present-
ing a number of examples that illustrate the kind
of semantics present in our models. Similar to neu-
ral language models, the predicate and argument
embedding matrices of our neural network con-
tain distributed word representations, that capture
the similarity of predicates and arguments to other
words.
Tables 5 and 6 contain a number of nearest neigh-
bour similarity examples for predicate and argu-
ments from our two-way neural network model.
The nearest neighbours were calculated using stan-
dard cosine similarity.
</bodyText>
<table confidence="0.9985712">
DRINK PROGRAM INTERVIEW FLOOD
SIP RECOMPILE RECRUIT INUNDATE
BREW UNDELETE PERSUADE RAVAGE
MINCE CODE INSTRUCT SUBMERGE
FRY IMPORT PESTER COLONIZE
</table>
<tableCaption confidence="0.9542864">
Table 5: Nearest neighbours of 4 verbs, calculated
using the distributed word representations of em-
bedding matrix V from our two-way neural net-
work model
Table 5 indicates that the network is effectively
</tableCaption>
<bodyText confidence="0.9903">
able to capture a semantics for verbs. The first
column – verbs similar to DRINK – all have to do
with food consumption. The second column con-
tains verbs related to computer programming. The
third column is related to human communication;
and the fourth column seems to illustrate the net-
work’s comprehension of FLOOD having to do with
invasion and water.
</bodyText>
<table confidence="0.682655">
PAPER RASPBERRY SECRETARY DESIGNER
BOOK COURGETTE PRESIDENT PLANNER
JOURNAL LATTE MANAGER PAINTER
ARTICLE LEMONADE POLICE SPECIALIST
CODE OATMEAL EDITOR SPEAKER
</table>
<tableCaption confidence="0.689968">
Table 6: Nearest neighbours of 4 direct objects, cal-
</tableCaption>
<bodyText confidence="0.988438428571429">
culated using the distributed word representations
of embedding matrix O from our two way neural
network model
Similarly, table 6 shows the network’s ability to
capture the meaning of nouns that appear as direct
objects to the verbs. Column one contains things
that can be read. Column two contains things that
can be consumed. Column three seems to hint at
supervising professions, while column four seems
to capture creative professions.
A similar kind of semantics is present in the em-
bedding matrices of the three-way neural network
model. Tables 7, 8, and 9 again illustrate this using
word similarity calculations.
</bodyText>
<table confidence="0.9981232">
SEARCH DIMINISH CONFIGURE PROSECUTE
CLICK LESSEN AUTOMATE CRITICISE
BROWSE DISTORT SCROLL URGE
SCROLL HEIGHTEN PROGRAM DEPLORE
UPLOAD DEGRADE INSTALL CONDEMN
</table>
<tableCaption confidence="0.936337">
Table 7: Nearest neighbours of 4 verbs, calculated
using the distributed word representations of em-
bedding matrix V from our three-way neural net-
work model
Table 7 shows the network’s verb semantics for
</tableCaption>
<bodyText confidence="0.7775648">
the three-way case. The first column is related to
internet usage, the second column contains verbs
of scalar change, column three is again related to
computer usage, and column four seems to capture
‘mending’ verbs.
</bodyText>
<table confidence="0.9786972">
FLOWER COLLEGE PRESIDENT SONG
FISH UNIVERSITY BUSH FILM
BIRD INSTITUTE BLAIR ALBUM
SUN DEPARTMENT MP PLAY
TREE CENTRE CHAIRMAN MUSIC
</table>
<tableCaption confidence="0.983516">
Table 8: Nearest neighbours of 4 subjects, calcu-
</tableCaption>
<bodyText confidence="0.9136358">
lated using the distributed word representations of
embedding matrix S from our three way neural
network model
Table 8 illustrates the semantics for the subject
slot of our three-way model. The first column cap-
tures nature terms, the second column contains
university-related terms, the third column contains
politicians/government terms, and the fourth col-
umn contains art expressions.
Finally, table 9 demonstrates the semantics of
our three-way model’s object slot. Column one
generally contains housing terms, column two con-
tains various locations, column three contains din-
ing occasions, and column four contains textual
expressions.
</bodyText>
<page confidence="0.997677">
32
</page>
<table confidence="0.9987534">
WALL PARK LUNCH THESIS
FLOOR STUDIO DINNER QUESTIONNAIRE
CEILING VILLAGE MEAL DISSERTATION
ROOF HALL BUFFET PERIODICAL
METRE MUSEUM BREAKFAST DISCOURSE
</table>
<tableCaption confidence="0.978859">
Table 9: Nearest neighbours of 4 direct objects, cal-
</tableCaption>
<bodyText confidence="0.991751684210526">
culated using the distributed word representations
of embedding matrix O from our three way neural
network model
Note that the embeddings for the subject and
the object slot is different, although they mostly
contain the same words. This allows the model to
capture specific semantic characteristics for words
given their argument position. Virus, for example,
is in subject position more similar to active words
like animal, whereas in object position, it is more
similar to passive words like cell, device. Similarly,
mouse in subject position tends to be similar to
words like animal, rat whereas in object position it
is similar to words like web, browser.
These examples, although anecdotal, illustrate
that our neural network model is able to capture a
rich semantics for predicates and arguments, which
subsequently allows the network to make accurate
predictions with regard to selectional preference.
</bodyText>
<sectionHeader confidence="0.960362" genericHeader="conclusions">
5 Conclusion and future work
</sectionHeader>
<bodyText confidence="0.999986361702128">
In this paper, we presented a neural network ap-
proach to the acquisition of selectional preferences.
Inspired by recent work on neural language models,
we proposed a neural network model that learns
to discriminate between felicitous and infelicitous
arguments for a particular predicate. The model is
entirely unsupervised, as preferences are learned
from unannotated corpus data. Positive training
instances are constructed from attested corpus data,
while negative instances are constructed from ran-
domly corrupted instances. Using designated net-
work architectures, we are able to handle stan-
dard two-way selectional preferences as well as
multi-way selectional preferences. A quantitative
evaluation on a pseudo-disambiguation task shows
that our models achieve state of the art perfor-
mance. The results for our two-way neural network
are on a par with Erk et al.’s (2010) similarity-
based approach, while our three-way neural net-
work slightly outperforms the tensor-based factor-
ization model (Van de Cruys, 2009) for multi-way
selectional preference induction.
We conclude with a number of issues for future
work. First of all, we would like to investigate how
our neural network approach might be improved by
incorporating information from other sources. In
particular, we think of initializing our embedding
matrices with distributed representations that come
from a large-scale neural language model (Mikolov
et al., 2013). We also want to further investigate
the advantages and disadvantages of having dif-
ferent embedding matrices for different argument
positions in our multi-way neural network. In our
results section, we demonstrated that such an ap-
proach allows for more flexibility, but it also adds
a certain level of redundancy. We want to inves-
tigate the benefit of our approach, compared to a
model that shares the distributed word representa-
tion among different argument positions. Finally,
we want to investigate more advanced neural net-
work architectures for the acquisition of selectional
preferences. In particular, neural tensor networks
(Yu et al., 2013) have recently demonstrated im-
pressive results in related fields like speech recogni-
tion, and might provide the necessary machinery to
model multi-way selectional preferences in a more
profound way.
</bodyText>
<sectionHeader confidence="0.998099" genericHeader="references">
References
</sectionHeader>
<reference confidence="0.992343346153846">
Marco Baroni, Silvia Bernardini, Adriano Ferraresi,
and Eros Zanchetta. 2009. The wacky wide web: A
collection of very large linguistically processed web-
crawled corpora. Language Resources and Evalua-
tion, 43(3):209–226.
Yoshua Bengio, R´ejean Ducharme, Pascal Vincent, and
Christian Jauvin. 2003. A neural probabilistic lan-
guage model. Journal of Machine Learning Re-
search, 3:1137–1155.
Shane Bergsma, Dekang Lin, and Randy Goebel. 2008.
Discriminative learning of selectional preference
from unlabeled text. In Proceedings of the Con-
ference on Empirical Methods in Natural Language
Processing, pages 59–68. Association for Computa-
tional Linguistics.
Kenneth Ward Church and Patrick Hanks. 1990. Word
association norms, mutual information &amp; lexicogra-
phy. Computational Linguistics, 16(1):22–29.
Stephen Clark and David Weir. 2001. Class-based
probability estimation using a semantic hierarchy.
In Proceedings of the second meeting of the North
American Chapter of the Association for Computa-
tional Linguistics on Language technologies, pages
95–102. Association for Computational Linguistics.
Ronan Collobert and Jason Weston. 2008. A unified
architecture for natural language processing: Deep
</reference>
<page confidence="0.9955">
33
</page>
<reference confidence="0.994375252336449">
neural networks with multitask learning. In Pro-
ceedings of the 25th international conference on Ma-
chine learning, pages 160–167. ACM.
Katrin Erk, Sebastian Pad´o, and Ulrike Pad´o. 2010. A
flexible, corpus-driven model of regular and inverse
selectional preferences. Computational Linguistics,
36(4):723–763.
Katrin Erk. 2007. A simple, similarity-based model
for selectional preferences. In Proceedings of the
45th Annual Meeting of the Association of Compu-
tational Linguistics, pages 216–223, Prague, Czech
Republic, June. Association for Computational Lin-
guistics.
Daniel Gildea and Daniel Jurafsky. 2002. Automatic
labeling of semantic roles. Computational linguis-
tics, 28(3):245–288.
Eric H. Huang, Richard Socher, Christopher D. Man-
ning, and Andrew Y. Ng. 2012. Improving word
representations via global context and multiple word
prototypes. In Annual Meeting of the Association
for Computational Linguistics (ACL).
Daniel D. Lee and H. Sebastian Seung. 2000. Al-
gorithms for non-negative matrix factorization. In
Advances in Neural Information Processing Systems
13, pages 556–562.
Hang Li and Naoki Abe. 1998. Generalizing case
frames using a thesaurus and the MDL principle.
Computational linguistics, 24(2):217–244.
Tao Li and Chris Ding. 2006. The relationships among
various nonnegative matrix factorization methods
for clustering. In Data Mining, 2006. ICDM’06.
Sixth International Conference on, pages 362–371.
IEEE.
Dong C. Liu and Jorge Nocedal. 1989. On the limited
memory BFGS method for large scale optimization.
Mathematical programming, 45(1-3):503–528.
Diana McCarthy and John Carroll. 2003. Disam-
biguating nouns, verbs, and adjectives using auto-
matically acquired selectional preferences. Compu-
tational Linguistics, 29(4):639–654.
Tomas Mikolov, Kai Chen, Greg Corrado, and Jeffrey
Dean. 2013. Efficient estimation of word represen-
tations in vector space. In ICLR 2013.
Andriy Mnih and Geoffrey Hinton. 2007. Three new
graphical models for statistical language modelling.
In Proceedings of the 24th international conference
on Machine learning, pages 641–648. ACM.
Joakim Nivre, Johan Hall, and Jens Nilsson. 2006.
Maltparser: A data-driven parser-generator for de-
pendency parsing. In Proceedings of LREC-2006,
pages 2216–2219.
Diarmuid O´ S´eaghdha and Anna Korhonen. 2012.
Modelling selectional preferences in a lexical hier-
archy. In Proceedings of the First Joint Conference
on Lexical and Computational Semantics-Volume 1:
Proceedings of the main conference and the shared
task, and Volume 2: Proceedings of the Sixth Inter-
national Workshop on Semantic Evaluation, pages
170–179. Association for Computational Linguis-
tics.
Diarmuid O´ S´eaghdha. 2010. Latent variable mod-
els of selectional preference. In Proceedings of the
48th Annual Meeting of the Association for Compu-
tational Linguistics, pages 435–444. Association for
Computational Linguistics.
Sebastian Pad´o, Ulrike Pad´o, and Katrin Erk. 2007.
Flexible, corpus-based modelling of human plausi-
bility judgements. In Proceedings of the 2007 Joint
Conference on Empirical Methods in Natural Lan-
guage Processing and Computational Natural Lan-
guage Learning (EMNLP-CoNLL), pages 400–409,
Prague, Czech Republic, June. Association for Com-
putational Linguistics.
Philip Resnik. 1996. Selectional constraints: An
information-theoretic model and its computational
realization. Cognition, 61:127–159, November.
Alan Ritter, Mausam, and Oren Etzioni. 2010. A la-
tent dirichlet allocation method for selectional pref-
erences. In Proceedings of the 48th Annual Meet-
ing of the Association for Computational Linguistics,
pages 424–434, Uppsala, Sweden, July. Association
for Computational Linguistics.
Mats Rooth, Stefan Riezler, Detlef Prescher, Glenn Car-
roll, and Franz Beil. 1999. Inducing a semanti-
cally annotated lexicon via em-based clustering. In
Proceedings of the 37th annual meeting of the As-
sociation for Computational Linguistics on Compu-
tational Linguistics, pages 104–111. Association for
Computational Linguistics.
Ekaterina Shutova, Simone Teufel, and Anna Korho-
nen. 2013. Statistical metaphor processing. Compu-
tational Linguistics, 39(2):301–353.
Kristina Toutanova, Dan Klein, Christopher Manning,
and Yoram Singer. 2003. Feature-rich part-of-
speech tagging with a cyclic dependency network.
In Proceedings of HLT-NAACL 2003, pages 252–
259.
Masashi Tsubaki, Kevin Duh, Masashi Shimbo, and
Yuji Matsumoto. 2013. Modeling and learning se-
mantic co-compositionality through prototype pro-
jections and neural networks. In Proceedings of
the 2013 Conference on Empirical Methods in Nat-
ural Language Processing, pages 130–140, Seattle,
Washington, USA, October. Association for Compu-
tational Linguistics.
Tim Van de Cruys. 2009. A non-negative tensor fac-
torization model for selectional preference induction.
</reference>
<page confidence="0.990687">
34
</page>
<reference confidence="0.996779">
In Proceedings of the Workshop on Geometrical
Models of Natural Language Semantics, pages 83–
90, Athens, Greece, March. Association for Compu-
tational Linguistics.
Dong Yu, Li Deng, and Frank Seide. 2013. The
deep tensor neural network with applications to
large vocabulary speech recognition. IEEE Transac-
tions on Audio, Speech, and Language Processing,
21(2):388–396.
</reference>
<page confidence="0.999318">
35
</page>
</variant>
</algorithm>
<algorithm name="ParsHed" version="110505">
<variant no="0" confidence="0.803274">
<title confidence="0.999932">A Neural Network Approach to Selectional Preference Acquisition</title>
<author confidence="0.999995">Tim Van_de</author>
<affiliation confidence="0.999115">IRIT &amp;</affiliation>
<address confidence="0.847527">Toulouse,</address>
<email confidence="0.998505">tim.vandecruys@irit.fr</email>
<abstract confidence="0.997274894736842">This paper investigates the use of neural networks for the acquisition of selectional preferences. Inspired by recent advances neural network models for applications, we propose a neural network model that learns to discriminate between felicitous and infelicitous arguments for a particular predicate. The model is entirely unsupervised – preferences are learned from unannotated corpus data. We propose two neural network architectures: one that handles standard two-way selectional preferences and one that is able to deal with multi-way selectional preferences. The model’s performance is evaluated on a pseudo-disambiguation task, on which it is shown to achieve state of the art performance.</abstract>
</variant>
</algorithm>
<algorithm name="ParsCit" version="110505">
<citationList>
<citation valid="true">
<authors>
<author>Marco Baroni</author>
<author>Silvia Bernardini</author>
<author>Adriano Ferraresi</author>
<author>Eros Zanchetta</author>
</authors>
<title>The wacky wide web: A collection of very large linguistically processed webcrawled corpora. Language Resources and Evaluation,</title>
<date>2009</date>
<pages>43--3</pages>
<contexts>
<context position="16081" citStr="Baroni et al., 2009" startWordPosition="2534" endWordPosition="2537">ent. This leads us to the objective function represented in (10). 29 As in the two-way case, the gradient of the objective function is sampled by randomly picking one corrupted subject j0 and one corrupted object k0 for each tuple (i, j,k). All of the model’s parameters are again updated through backpropagation. 4 Evaluation 4.1 Implementational details We evaluate our neural network approach to selectional preference acquisition using verb-object tuples for the two-way model, and subject-verbobject tuples for the multi-way model. Our model has been applied to English, using the UKWaC corpus (Baroni et al., 2009), which covers about 2 billion words of web text. The corpus has been part of speech tagged and lemmatized with Stanford Part-Of-Speech Tagger (Toutanova et al., 2003), and parsed with MaltParser (Nivre et al., 2006), so that dependency tuples could be extracted. For the two-way model, we select all verbs and objects that appear within a predicate-argument relation with a frequency of at least 50. This gives us a total of about 7K verbs and 30K objects. For the multi-way model, we select the 2K most frequent verbs, together with the 10K most frequent subjects and the 10K most frequent objects </context>
</contexts>
<marker>Baroni, Bernardini, Ferraresi, Zanchetta, 2009</marker>
<rawString>Marco Baroni, Silvia Bernardini, Adriano Ferraresi, and Eros Zanchetta. 2009. The wacky wide web: A collection of very large linguistically processed webcrawled corpora. Language Resources and Evaluation, 43(3):209–226.</rawString>
</citation>
<citation valid="true">
<authors>
<author>Yoshua Bengio</author>
<author>R´ejean Ducharme</author>
<author>Pascal Vincent</author>
<author>Christian Jauvin</author>
</authors>
<title>A neural probabilistic language model.</title>
<date>2003</date>
<journal>Journal of Machine Learning Research,</journal>
<pages>3--1137</pages>
<contexts>
<context position="9552" citStr="Bengio et al., 2003" startWordPosition="1456" endWordPosition="1459">ted a model for multi-way selectional preference induction based on tensor factorization. Three-way co-occurrences of subjects, verbs, and objects are represented as a three-way tensor (the generalization of a matrix), and a latent factorization model is applied in order to generalize to unseen instances. We will compare our neural network based-approach for multi-way selectional preference acquisition to this tensor-based factorization model. 2.2 Neural networks In the last few years, neural networks have become increasingly popular in NLP applications. In particular, neural language models (Bengio et al., 2003; Mnih and Hinton, 2007; Collobert and Weston, 2008) have demonstrated impressive performance at the task of language modeling. By incorporating distributed representations for words that model their similarity, neural language models are able to overcome the problem of data sparseness that standard n-gram models are confronted with. Also related to our work is the approach by Tsubaki et al. (2013), who successfully use a neural network to model co-compositionality. Our model for selectional preference acquisition uses a network architecture that is similar to the abovementioned models. Its tr</context>
</contexts>
<marker>Bengio, Ducharme, Vincent, Jauvin, 2003</marker>
<rawString>Yoshua Bengio, R´ejean Ducharme, Pascal Vincent, and Christian Jauvin. 2003. A neural probabilistic language model. Journal of Machine Learning Research, 3:1137–1155.</rawString>
</citation>
<citation valid="true">
<authors>
<author>Shane Bergsma</author>
<author>Dekang Lin</author>
<author>Randy Goebel</author>
</authors>
<title>Discriminative learning of selectional preference from unlabeled text.</title>
<date>2008</date>
<booktitle>In Proceedings of the Conference on Empirical Methods in Natural Language Processing,</booktitle>
<pages>59--68</pages>
<publisher>Association</publisher>
<institution>for Computational Linguistics.</institution>
<contexts>
<context position="7909" citStr="Bergsma et al. (2008)" startWordPosition="1210" endWordPosition="1213">007) and Erk et al. (2010) describe a method that uses corpus-driven distributional similarity metrics for the induction of selectional preferences. The key idea is that a predicate-argument tuple (v,o) is felicitous if the predicate v appears in the training corpus with arguments o0 that are similar to o, i.e. · sim(o,o0) (4) where Ov represents the set of arguments that have been attested with predicate v, wt(·) represents an appropriate weighting function (such as the frequency of the (v,o0) tuple), and Z is a normalization factor. We equally compare to their model for evaluation purposes. Bergsma et al. (2008) present a discriminative approach to selectional preference acquisition. Positive examples are taken from observed predicateS(v,o) = E wt(v,o0) o0∈Ov Z(v) 27 argument pairs, while negative examples are constructed from unobserved combinations. An SVM classifier is used to distinguish the positive from the negative instances. The training procedure used in their model is based on an intuition that is similar to ours, although it is implemented using different techniques. A number of researchers presented models that are based on the framework of topic modeling. O´ S´eaghdha (2010) describes th</context>
</contexts>
<marker>Bergsma, Lin, Goebel, 2008</marker>
<rawString>Shane Bergsma, Dekang Lin, and Randy Goebel. 2008. Discriminative learning of selectional preference from unlabeled text. In Proceedings of the Conference on Empirical Methods in Natural Language Processing, pages 59–68. Association for Computational Linguistics.</rawString>
</citation>
<citation valid="true">
<authors>
<author>Kenneth Ward Church</author>
<author>Patrick Hanks</author>
</authors>
<title>Word association norms, mutual information &amp; lexicography.</title>
<date>1990</date>
<journal>Computational Linguistics,</journal>
<volume>16</volume>
<issue>1</issue>
<contexts>
<context position="20601" citStr="Church and Hanks, 1990" startWordPosition="3290" endWordPosition="3293"> models for selectional preference acquisition. For the two-way case, we compare our model to the EM-based clustering technique presented by Rooth et al. (1999),2 and to Erk et al.’s (2010) similarity-based model. For Rooth et al.’s model, we set the number of latent factors to 50. Using a larger number of latent factors does not increase performance. For Erk et al.’s model, we create a dependency-based similarity model from the UKWaC corpus using our 30K direct objects as instances and 100K dependency relations as features. The resulting matrix is weighted using pointwise mutual information (Church and Hanks, 1990). Similarity values are computed using cosine. Furthermore, we use a sampling procedure in the testing phase: we sample 5000 predicate-argument pairs for each fold, as testing Erk et al.’s model on the complete test sets proved prohibitively expensive. For the three-way case, we compare our model to the tensor factorization model we developed in previous work (Van de Cruys, 2009). We set the number of latent factors to 300.3 4.3 Results 4.3.1 Two-way model Table 3 compares the results of our neural network architecture for two-way selectional preferences to the results of Rooth et al.’s (1999)</context>
</contexts>
<marker>Church, Hanks, 1990</marker>
<rawString>Kenneth Ward Church and Patrick Hanks. 1990. Word association norms, mutual information &amp; lexicography. Computational Linguistics, 16(1):22–29.</rawString>
</citation>
<citation valid="true">
<authors>
<author>Stephen Clark</author>
<author>David Weir</author>
</authors>
<title>Class-based probability estimation using a semantic hierarchy.</title>
<date>2001</date>
<booktitle>In Proceedings of the second meeting of the North American Chapter of the Association for Computational Linguistics on Language technologies,</booktitle>
<pages>95--102</pages>
<publisher>Association</publisher>
<institution>for Computational Linguistics.</institution>
<contexts>
<context position="2635" citStr="Clark and Weir, 2001" startWordPosition="396" endWordPosition="399">to adequately deal with the consequences of Zipf’s law: language is inherently sparse, and the majority of language utterances occur very infrequently. As a consequence, models that are based on corpus data need to properly generalize beyond the mere co-occurrence frequencies of sparse corpus data, taking into account the semantic similarity of both predicates and arguments. Researchers have come up with various approaches to this generalization step. Earlier approaches to selectional preference acquisition mostly rely on hand-crafted resources such as WordNet (Resnik, 1996; Li and Abe, 1998; Clark and Weir, 2001), while later approaches tend to take advantage of unsupervised learning machinery, such as latent variable models (Rooth et al., 1999; O´ S´eaghdha, 2010) and distributional similarity metrics (Erk, 2007; Pad´o et al., 2007). This paper investigates the use of neural networks for the acquisition of selectional preferences. Inspired by recent advances of neural network models for NLP applications (Collobert and Weston, 2008; Mikolov et al., 2013), we propose a neural network model that learns to discriminate between felicitous and infelicitous arguments for a particular predicate. The model is</context>
<context position="6160" citStr="Clark and Weir (2001)" startWordPosition="934" endWordPosition="937">nd R stands for a given predicate-argument relation. The selectional association of a particular noun cluster is then the contribution of that cluster to the verb’s preference strength. AR(v,c) = p(c|v)log p(c|v) SR(v) p(c) (2) The model’s generalization relies entirely on WordNet, and there is no generalization among the verbs. Other researchers have equally relied on WordNet in order to generalize over arguments. Li and Abe (1998) use the principle of Minimum Description Length in order to find a suitable generalization level within the lexical WordNet hierarchy. A same intuition is used by Clark and Weir (2001), but they use hypothesis testing instead to find the appropriate level of generalization. A recent approach that makes use of WordNet (in combination with Bayesian modeling) is the one by O´ S´eaghdha and Korhonen (2012). Most researchers, however, acknowledge the shortcomings of hand-crafted resources, and focus on the acquisition of selectional preferences from corpus data. Rooth et al. (1999) propose an Expectation-Maximization (EM) clustering algorithm for selectional preference acquisition based on a probabilistic latent variable model. The idea is that both predicate v and argument o ar</context>
</contexts>
<marker>Clark, Weir, 2001</marker>
<rawString>Stephen Clark and David Weir. 2001. Class-based probability estimation using a semantic hierarchy. In Proceedings of the second meeting of the North American Chapter of the Association for Computational Linguistics on Language technologies, pages 95–102. Association for Computational Linguistics.</rawString>
</citation>
<citation valid="true">
<authors>
<author>Ronan Collobert</author>
<author>Jason Weston</author>
</authors>
<title>A unified architecture for natural language processing: Deep neural networks with multitask learning.</title>
<date>2008</date>
<booktitle>In Proceedings of the 25th international conference on Machine learning,</booktitle>
<pages>160--167</pages>
<publisher>ACM.</publisher>
<contexts>
<context position="3062" citStr="Collobert and Weston, 2008" startWordPosition="462" endWordPosition="465">ches to this generalization step. Earlier approaches to selectional preference acquisition mostly rely on hand-crafted resources such as WordNet (Resnik, 1996; Li and Abe, 1998; Clark and Weir, 2001), while later approaches tend to take advantage of unsupervised learning machinery, such as latent variable models (Rooth et al., 1999; O´ S´eaghdha, 2010) and distributional similarity metrics (Erk, 2007; Pad´o et al., 2007). This paper investigates the use of neural networks for the acquisition of selectional preferences. Inspired by recent advances of neural network models for NLP applications (Collobert and Weston, 2008; Mikolov et al., 2013), we propose a neural network model that learns to discriminate between felicitous and infelicitous arguments for a particular predicate. The model is entirely unsupervised – preferences are learned from unannotated corpus data. Positive training instances are constructed from attested corpus data, while negative instances are constructed from randomly corrupted instances. We propose two neural network architectures: one that handles standard two-way selectional preferences and one that is able to deal with multi-way selectional preferences, where the interaction be26 Pr</context>
<context position="9604" citStr="Collobert and Weston, 2008" startWordPosition="1464" endWordPosition="1467">ence induction based on tensor factorization. Three-way co-occurrences of subjects, verbs, and objects are represented as a three-way tensor (the generalization of a matrix), and a latent factorization model is applied in order to generalize to unseen instances. We will compare our neural network based-approach for multi-way selectional preference acquisition to this tensor-based factorization model. 2.2 Neural networks In the last few years, neural networks have become increasingly popular in NLP applications. In particular, neural language models (Bengio et al., 2003; Mnih and Hinton, 2007; Collobert and Weston, 2008) have demonstrated impressive performance at the task of language modeling. By incorporating distributed representations for words that model their similarity, neural language models are able to overcome the problem of data sparseness that standard n-gram models are confronted with. Also related to our work is the approach by Tsubaki et al. (2013), who successfully use a neural network to model co-compositionality. Our model for selectional preference acquisition uses a network architecture that is similar to the abovementioned models. Its training objective is also similar to the ranking-loss</context>
<context position="11890" citStr="Collobert and Weston (2008)" startWordPosition="1839" endWordPosition="1842">ivation of the hidden layer with H hidden nodes, W1 ∈ RH×2N and W2 ∈ R1×H respectively represent the first and second layer weights, b1 represents the first layer’s bias, f (·) represents the element-wise activation function tanh, and y is our final selectional preference score. The left-hand picture of figure 1 gives a graphical representation of our standard neural network architecture. 3.2 Training the network A proper estimation of a neural network’s parameters requires a large amount of training data. To be able to use non-annotated corpus data for training, we use the method proposed by Collobert and Weston (2008). The authors present a method for training a neural network language model from unlabeled data by corrupting actual attested n-grams with a random word. They then define a rankingtype cost function, which allows the network to learn to discriminate between good and bad word sequences. We adopt the same method for our selectional preference model as follows. Let (i, j) be our proper, attested predicateargument tuple. The goal of our model is to discriminate the correct tuple (i, j) from other, nonattested tuples (i, j0), in which the correct predicate 28 i j x y W2 V O a1 W1 i k V S O j x W1 a</context>
<context position="13392" citStr="Collobert and Weston (2008)" startWordPosition="2094" endWordPosition="2097">x is constructed from the appropriate predicate and argument vectors from the embedding matrices, and fed forward through the network to yield a preference score y. j has been replaced with a random predicate j0. We require the score for the correct tuple to be larger than the score for the corrupt tuple by a margin of one. For one tuple (i, j), this corresponds to minimizing the objective function in (8) ∑ max(0,1−g[(i, j)] +g[(i, j0)]) (8) j0∈J where J represents the predicate vocabulary, and g[·] represents our neural network scoring function presented in the previous section. In line with Collobert and Weston (2008), the gradient of the objective function is sampled by randomly picking one corrupt argument j0 from the argument vocabulary for each attested predicateargument tuple (i, j). The derivative of the cost with respect to the model’s parameters (weight matrices W1 and W2, bias vector b1, and embedding matrices V and O) is computed, and the appropriate parameters are updated through backpropagation. 3.3 Multi-way selectional preferences The model presented in the previous section is only able to deal with two-way selectional preferences. In this section, we present an extension of the model that is</context>
</contexts>
<marker>Collobert, Weston, 2008</marker>
<rawString>Ronan Collobert and Jason Weston. 2008. A unified architecture for natural language processing: Deep neural networks with multitask learning. In Proceedings of the 25th international conference on Machine learning, pages 160–167. ACM.</rawString>
</citation>
<citation valid="true">
<authors>
<author>Katrin Erk</author>
<author>Sebastian Pad´o</author>
<author>Ulrike Pad´o</author>
</authors>
<title>A flexible, corpus-driven model of regular and inverse selectional preferences.</title>
<date>2010</date>
<journal>Computational Linguistics,</journal>
<volume>36</volume>
<issue>4</issue>
<marker>Erk, Pad´o, Pad´o, 2010</marker>
<rawString>Katrin Erk, Sebastian Pad´o, and Ulrike Pad´o. 2010. A flexible, corpus-driven model of regular and inverse selectional preferences. Computational Linguistics, 36(4):723–763.</rawString>
</citation>
<citation valid="true">
<authors>
<author>Katrin Erk</author>
</authors>
<title>A simple, similarity-based model for selectional preferences.</title>
<date>2007</date>
<booktitle>In Proceedings of the 45th Annual Meeting of the Association of Computational Linguistics,</booktitle>
<pages>216--223</pages>
<publisher>Association</publisher>
<institution>for Computational Linguistics.</institution>
<location>Prague, Czech Republic,</location>
<contexts>
<context position="2839" citStr="Erk, 2007" startWordPosition="429" endWordPosition="430">roperly generalize beyond the mere co-occurrence frequencies of sparse corpus data, taking into account the semantic similarity of both predicates and arguments. Researchers have come up with various approaches to this generalization step. Earlier approaches to selectional preference acquisition mostly rely on hand-crafted resources such as WordNet (Resnik, 1996; Li and Abe, 1998; Clark and Weir, 2001), while later approaches tend to take advantage of unsupervised learning machinery, such as latent variable models (Rooth et al., 1999; O´ S´eaghdha, 2010) and distributional similarity metrics (Erk, 2007; Pad´o et al., 2007). This paper investigates the use of neural networks for the acquisition of selectional preferences. Inspired by recent advances of neural network models for NLP applications (Collobert and Weston, 2008; Mikolov et al., 2013), we propose a neural network model that learns to discriminate between felicitous and infelicitous arguments for a particular predicate. The model is entirely unsupervised – preferences are learned from unannotated corpus data. Positive training instances are constructed from attested corpus data, while negative instances are constructed from randomly</context>
<context position="7292" citStr="Erk (2007)" startWordPosition="1109" endWordPosition="1110">tic latent variable model. The idea is that both predicate v and argument o are generated from a latent variable c, where the latent variables represent clusters of tight verb-argument interactions. p(v,o) = Ep(c,v,o) = E p(c)p(v|c)p(o|c) (3) c∈C c∈C The use of latent variables allows the model to generalize to predicate-argument tuples that have not been seen during training. The latent variable distribution – and the probabilities of predicates and argument given the latent variables – are automatically induced from data using EM. We will compare against their model for evaluation purposes. Erk (2007) and Erk et al. (2010) describe a method that uses corpus-driven distributional similarity metrics for the induction of selectional preferences. The key idea is that a predicate-argument tuple (v,o) is felicitous if the predicate v appears in the training corpus with arguments o0 that are similar to o, i.e. · sim(o,o0) (4) where Ov represents the set of arguments that have been attested with predicate v, wt(·) represents an appropriate weighting function (such as the frequency of the (v,o0) tuple), and Z is a normalization factor. We equally compare to their model for evaluation purposes. Berg</context>
</contexts>
<marker>Erk, 2007</marker>
<rawString>Katrin Erk. 2007. A simple, similarity-based model for selectional preferences. In Proceedings of the 45th Annual Meeting of the Association of Computational Linguistics, pages 216–223, Prague, Czech Republic, June. Association for Computational Linguistics.</rawString>
</citation>
<citation valid="true">
<authors>
<author>Daniel Gildea</author>
<author>Daniel Jurafsky</author>
</authors>
<title>Automatic labeling of semantic roles.</title>
<date>2002</date>
<journal>Computational linguistics,</journal>
<pages>28--3</pages>
<contexts>
<context position="1869" citStr="Gildea and Jurafsky, 2002" startWordPosition="285" endWordPosition="288">guage users, however, would likely consider the second sentence to be ill-formed: an exception is not supposed to sing, nor is a tomato something that is typically sung. Within the field of natural language processing, this inclination of predicates to select for particular arguments is known as selectional preference. The automatic acquisition of selectional preferences has been a popular research subject within the field of natural language processing. An automatically acquired selectional preference resource is a versatile tool for numerous NLP applications, such as semantic role labeling (Gildea and Jurafsky, 2002), word sense disambiguation (McCarthy and Carroll, 2003), and metaphor processing (Shutova et al., 2013). Models for selectional preference need to adequately deal with the consequences of Zipf’s law: language is inherently sparse, and the majority of language utterances occur very infrequently. As a consequence, models that are based on corpus data need to properly generalize beyond the mere co-occurrence frequencies of sparse corpus data, taking into account the semantic similarity of both predicates and arguments. Researchers have come up with various approaches to this generalization step.</context>
</contexts>
<marker>Gildea, Jurafsky, 2002</marker>
<rawString>Daniel Gildea and Daniel Jurafsky. 2002. Automatic labeling of semantic roles. Computational linguistics, 28(3):245–288.</rawString>
</citation>
<citation valid="true">
<authors>
<author>Eric H Huang</author>
<author>Richard Socher</author>
<author>Christopher D Manning</author>
<author>Andrew Y Ng</author>
</authors>
<title>Improving word representations via global context and multiple word prototypes.</title>
<date>2012</date>
<booktitle>In Annual Meeting of the Association for Computational Linguistics (ACL).</booktitle>
<contexts>
<context position="17070" citStr="Huang et al. (2012)" startWordPosition="2705" endWordPosition="2708">th a frequency of at least 50. This gives us a total of about 7K verbs and 30K objects. For the multi-way model, we select the 2K most frequent verbs, together with the 10K most frequent subjects and the 10K most frequent objects (that appear within a transitive frame). All words are converted to lowercase. We use the lemmatized forms, and only keep those forms that contain alphabetic characters. Furthermore, we require each tuple to appear at least three times in the corpus. We set N, the size of our embedding matrices, to 50, and H, the number of units in the hidden layer, to 100. Following Huang et al. (2012), we use mini-batch L-BFGS (Liu and Nocedal, 1989) with 1000 pairs of good and corrupt tuples per batch for training, and train for 10 epochs. 4.2 Evaluation Setup 4.2.1 Task Our models are quantitatively evaluated using a pseudo-disambiguation task (Rooth et al., 1999), which bears some resemblance to our training procedure. The task provides an adequate test of the generalization capabilities of our models. For the two-way case, the task is to judge which object (o or o0) is more likely for a particular verb v, where (v,o) is a tuple attested in the corpus, and o0 is a direct object randomly</context>
</contexts>
<marker>Huang, Socher, Manning, Ng, 2012</marker>
<rawString>Eric H. Huang, Richard Socher, Christopher D. Manning, and Andrew Y. Ng. 2012. Improving word representations via global context and multiple word prototypes. In Annual Meeting of the Association for Computational Linguistics (ACL).</rawString>
</citation>
<citation valid="true">
<authors>
<author>Daniel D Lee</author>
<author>H Sebastian Seung</author>
</authors>
<title>Algorithms for non-negative matrix factorization.</title>
<date>2000</date>
<booktitle>In Advances in Neural Information Processing Systems 13,</booktitle>
<pages>556--562</pages>
<contexts>
<context position="21365" citStr="Lee and Seung, 2000" startWordPosition="3414" endWordPosition="3417">pairs for each fold, as testing Erk et al.’s model on the complete test sets proved prohibitively expensive. For the three-way case, we compare our model to the tensor factorization model we developed in previous work (Van de Cruys, 2009). We set the number of latent factors to 300.3 4.3 Results 4.3.1 Two-way model Table 3 compares the results of our neural network architecture for two-way selectional preferences to the results of Rooth et al.’s (1999) model and Erk et al.’s (2010) model. 2Our own implementation of Rooth et al.’s (1999) algorithm is based on non-negative matrix factorization (Lee and Seung, 2000). Non-negative matrix factorization with Kullback-Leibler divergence has been shown to minimize the same objective function as EM (Li and Ding, 2006). 3The best scoring model presented by Van de Cruys (2009) also uses 300 latent factors; using more factors does not improve the results. model accuracy (g ± 6) Rooth et al. (1999) .720 ± .002 Erk et al. (2010) .887 ± .004 2-way neural network .880 ± .001 Table 3: Comparison of model results for two-way selectional preference acquisition – mean accuracy and standard deviations of 10-fold cross-validation results The results indicate that our neura</context>
</contexts>
<marker>Lee, Seung, 2000</marker>
<rawString>Daniel D. Lee and H. Sebastian Seung. 2000. Algorithms for non-negative matrix factorization. In Advances in Neural Information Processing Systems 13, pages 556–562.</rawString>
</citation>
<citation valid="true">
<authors>
<author>Hang Li</author>
<author>Naoki Abe</author>
</authors>
<title>Generalizing case frames using a thesaurus and the MDL principle.</title>
<date>1998</date>
<journal>Computational linguistics,</journal>
<pages>24--2</pages>
<contexts>
<context position="2612" citStr="Li and Abe, 1998" startWordPosition="392" endWordPosition="395">l preference need to adequately deal with the consequences of Zipf’s law: language is inherently sparse, and the majority of language utterances occur very infrequently. As a consequence, models that are based on corpus data need to properly generalize beyond the mere co-occurrence frequencies of sparse corpus data, taking into account the semantic similarity of both predicates and arguments. Researchers have come up with various approaches to this generalization step. Earlier approaches to selectional preference acquisition mostly rely on hand-crafted resources such as WordNet (Resnik, 1996; Li and Abe, 1998; Clark and Weir, 2001), while later approaches tend to take advantage of unsupervised learning machinery, such as latent variable models (Rooth et al., 1999; O´ S´eaghdha, 2010) and distributional similarity metrics (Erk, 2007; Pad´o et al., 2007). This paper investigates the use of neural networks for the acquisition of selectional preferences. Inspired by recent advances of neural network models for NLP applications (Collobert and Weston, 2008; Mikolov et al., 2013), we propose a neural network model that learns to discriminate between felicitous and infelicitous arguments for a particular </context>
<context position="5975" citStr="Li and Abe (1998)" startWordPosition="903" endWordPosition="906">ullback-Leibler divergence between the cluster distribution of the verb and the prior cluster distribution. SR(v) = Ec p(c|v)log p(c|v) (1) p(c) where c stands for a noun cluster, and R stands for a given predicate-argument relation. The selectional association of a particular noun cluster is then the contribution of that cluster to the verb’s preference strength. AR(v,c) = p(c|v)log p(c|v) SR(v) p(c) (2) The model’s generalization relies entirely on WordNet, and there is no generalization among the verbs. Other researchers have equally relied on WordNet in order to generalize over arguments. Li and Abe (1998) use the principle of Minimum Description Length in order to find a suitable generalization level within the lexical WordNet hierarchy. A same intuition is used by Clark and Weir (2001), but they use hypothesis testing instead to find the appropriate level of generalization. A recent approach that makes use of WordNet (in combination with Bayesian modeling) is the one by O´ S´eaghdha and Korhonen (2012). Most researchers, however, acknowledge the shortcomings of hand-crafted resources, and focus on the acquisition of selectional preferences from corpus data. Rooth et al. (1999) propose an Expe</context>
</contexts>
<marker>Li, Abe, 1998</marker>
<rawString>Hang Li and Naoki Abe. 1998. Generalizing case frames using a thesaurus and the MDL principle. Computational linguistics, 24(2):217–244.</rawString>
</citation>
<citation valid="true">
<authors>
<author>Tao Li</author>
<author>Chris Ding</author>
</authors>
<title>The relationships among various nonnegative matrix factorization methods for clustering.</title>
<date>2006</date>
<booktitle>In Data Mining, 2006. ICDM’06. Sixth International Conference on,</booktitle>
<pages>362--371</pages>
<publisher>IEEE.</publisher>
<contexts>
<context position="21514" citStr="Li and Ding, 2006" startWordPosition="3435" endWordPosition="3438">el to the tensor factorization model we developed in previous work (Van de Cruys, 2009). We set the number of latent factors to 300.3 4.3 Results 4.3.1 Two-way model Table 3 compares the results of our neural network architecture for two-way selectional preferences to the results of Rooth et al.’s (1999) model and Erk et al.’s (2010) model. 2Our own implementation of Rooth et al.’s (1999) algorithm is based on non-negative matrix factorization (Lee and Seung, 2000). Non-negative matrix factorization with Kullback-Leibler divergence has been shown to minimize the same objective function as EM (Li and Ding, 2006). 3The best scoring model presented by Van de Cruys (2009) also uses 300 latent factors; using more factors does not improve the results. model accuracy (g ± 6) Rooth et al. (1999) .720 ± .002 Erk et al. (2010) .887 ± .004 2-way neural network .880 ± .001 Table 3: Comparison of model results for two-way selectional preference acquisition – mean accuracy and standard deviations of 10-fold cross-validation results The results indicate that our neural network approach outperforms Rooth et al.’s (1999) method by a large margin (16%). Clearly, the neural network architecture is able to model select</context>
</contexts>
<marker>Li, Ding, 2006</marker>
<rawString>Tao Li and Chris Ding. 2006. The relationships among various nonnegative matrix factorization methods for clustering. In Data Mining, 2006. ICDM’06. Sixth International Conference on, pages 362–371. IEEE.</rawString>
</citation>
<citation valid="true">
<authors>
<author>Dong C Liu</author>
<author>Jorge Nocedal</author>
</authors>
<title>On the limited memory BFGS method for large scale optimization.</title>
<date>1989</date>
<booktitle>Mathematical programming,</booktitle>
<pages>45--1</pages>
<contexts>
<context position="17120" citStr="Liu and Nocedal, 1989" startWordPosition="2713" endWordPosition="2716">total of about 7K verbs and 30K objects. For the multi-way model, we select the 2K most frequent verbs, together with the 10K most frequent subjects and the 10K most frequent objects (that appear within a transitive frame). All words are converted to lowercase. We use the lemmatized forms, and only keep those forms that contain alphabetic characters. Furthermore, we require each tuple to appear at least three times in the corpus. We set N, the size of our embedding matrices, to 50, and H, the number of units in the hidden layer, to 100. Following Huang et al. (2012), we use mini-batch L-BFGS (Liu and Nocedal, 1989) with 1000 pairs of good and corrupt tuples per batch for training, and train for 10 epochs. 4.2 Evaluation Setup 4.2.1 Task Our models are quantitatively evaluated using a pseudo-disambiguation task (Rooth et al., 1999), which bears some resemblance to our training procedure. The task provides an adequate test of the generalization capabilities of our models. For the two-way case, the task is to judge which object (o or o0) is more likely for a particular verb v, where (v,o) is a tuple attested in the corpus, and o0 is a direct object randomly drawn from the object vocabulary. The tuple is co</context>
</contexts>
<marker>Liu, Nocedal, 1989</marker>
<rawString>Dong C. Liu and Jorge Nocedal. 1989. On the limited memory BFGS method for large scale optimization. Mathematical programming, 45(1-3):503–528.</rawString>
</citation>
<citation valid="true">
<authors>
<author>Diana McCarthy</author>
<author>John Carroll</author>
</authors>
<title>Disambiguating nouns, verbs, and adjectives using automatically acquired selectional preferences.</title>
<date>2003</date>
<journal>Computational Linguistics,</journal>
<volume>29</volume>
<issue>4</issue>
<contexts>
<context position="1925" citStr="McCarthy and Carroll, 2003" startWordPosition="292" endWordPosition="295">sentence to be ill-formed: an exception is not supposed to sing, nor is a tomato something that is typically sung. Within the field of natural language processing, this inclination of predicates to select for particular arguments is known as selectional preference. The automatic acquisition of selectional preferences has been a popular research subject within the field of natural language processing. An automatically acquired selectional preference resource is a versatile tool for numerous NLP applications, such as semantic role labeling (Gildea and Jurafsky, 2002), word sense disambiguation (McCarthy and Carroll, 2003), and metaphor processing (Shutova et al., 2013). Models for selectional preference need to adequately deal with the consequences of Zipf’s law: language is inherently sparse, and the majority of language utterances occur very infrequently. As a consequence, models that are based on corpus data need to properly generalize beyond the mere co-occurrence frequencies of sparse corpus data, taking into account the semantic similarity of both predicates and arguments. Researchers have come up with various approaches to this generalization step. Earlier approaches to selectional preference acquisitio</context>
</contexts>
<marker>McCarthy, Carroll, 2003</marker>
<rawString>Diana McCarthy and John Carroll. 2003. Disambiguating nouns, verbs, and adjectives using automatically acquired selectional preferences. Computational Linguistics, 29(4):639–654.</rawString>
</citation>
<citation valid="true">
<authors>
<author>Tomas Mikolov</author>
<author>Kai Chen</author>
<author>Greg Corrado</author>
<author>Jeffrey Dean</author>
</authors>
<title>Efficient estimation of word representations in vector space.</title>
<date>2013</date>
<booktitle>In ICLR</booktitle>
<contexts>
<context position="3085" citStr="Mikolov et al., 2013" startWordPosition="466" endWordPosition="469">step. Earlier approaches to selectional preference acquisition mostly rely on hand-crafted resources such as WordNet (Resnik, 1996; Li and Abe, 1998; Clark and Weir, 2001), while later approaches tend to take advantage of unsupervised learning machinery, such as latent variable models (Rooth et al., 1999; O´ S´eaghdha, 2010) and distributional similarity metrics (Erk, 2007; Pad´o et al., 2007). This paper investigates the use of neural networks for the acquisition of selectional preferences. Inspired by recent advances of neural network models for NLP applications (Collobert and Weston, 2008; Mikolov et al., 2013), we propose a neural network model that learns to discriminate between felicitous and infelicitous arguments for a particular predicate. The model is entirely unsupervised – preferences are learned from unannotated corpus data. Positive training instances are constructed from attested corpus data, while negative instances are constructed from randomly corrupted instances. We propose two neural network architectures: one that handles standard two-way selectional preferences and one that is able to deal with multi-way selectional preferences, where the interaction be26 Proceedings of the 2014 C</context>
<context position="29871" citStr="Mikolov et al., 2013" startWordPosition="4727" endWordPosition="4730">for our two-way neural network are on a par with Erk et al.’s (2010) similaritybased approach, while our three-way neural network slightly outperforms the tensor-based factorization model (Van de Cruys, 2009) for multi-way selectional preference induction. We conclude with a number of issues for future work. First of all, we would like to investigate how our neural network approach might be improved by incorporating information from other sources. In particular, we think of initializing our embedding matrices with distributed representations that come from a large-scale neural language model (Mikolov et al., 2013). We also want to further investigate the advantages and disadvantages of having different embedding matrices for different argument positions in our multi-way neural network. In our results section, we demonstrated that such an approach allows for more flexibility, but it also adds a certain level of redundancy. We want to investigate the benefit of our approach, compared to a model that shares the distributed word representation among different argument positions. Finally, we want to investigate more advanced neural network architectures for the acquisition of selectional preferences. In par</context>
</contexts>
<marker>Mikolov, Chen, Corrado, Dean, 2013</marker>
<rawString>Tomas Mikolov, Kai Chen, Greg Corrado, and Jeffrey Dean. 2013. Efficient estimation of word representations in vector space. In ICLR 2013.</rawString>
</citation>
<citation valid="true">
<authors>
<author>Andriy Mnih</author>
<author>Geoffrey Hinton</author>
</authors>
<title>Three new graphical models for statistical language modelling.</title>
<date>2007</date>
<booktitle>In Proceedings of the 24th international conference on Machine learning,</booktitle>
<pages>641--648</pages>
<publisher>ACM.</publisher>
<contexts>
<context position="9575" citStr="Mnih and Hinton, 2007" startWordPosition="1460" endWordPosition="1463">-way selectional preference induction based on tensor factorization. Three-way co-occurrences of subjects, verbs, and objects are represented as a three-way tensor (the generalization of a matrix), and a latent factorization model is applied in order to generalize to unseen instances. We will compare our neural network based-approach for multi-way selectional preference acquisition to this tensor-based factorization model. 2.2 Neural networks In the last few years, neural networks have become increasingly popular in NLP applications. In particular, neural language models (Bengio et al., 2003; Mnih and Hinton, 2007; Collobert and Weston, 2008) have demonstrated impressive performance at the task of language modeling. By incorporating distributed representations for words that model their similarity, neural language models are able to overcome the problem of data sparseness that standard n-gram models are confronted with. Also related to our work is the approach by Tsubaki et al. (2013), who successfully use a neural network to model co-compositionality. Our model for selectional preference acquisition uses a network architecture that is similar to the abovementioned models. Its training objective is als</context>
</contexts>
<marker>Mnih, Hinton, 2007</marker>
<rawString>Andriy Mnih and Geoffrey Hinton. 2007. Three new graphical models for statistical language modelling. In Proceedings of the 24th international conference on Machine learning, pages 641–648. ACM.</rawString>
</citation>
<citation valid="true">
<authors>
<author>Joakim Nivre</author>
<author>Johan Hall</author>
<author>Jens Nilsson</author>
</authors>
<title>Maltparser: A data-driven parser-generator for dependency parsing.</title>
<date>2006</date>
<booktitle>In Proceedings of LREC-2006,</booktitle>
<pages>2216--2219</pages>
<contexts>
<context position="16297" citStr="Nivre et al., 2006" startWordPosition="2569" endWordPosition="2572">or each tuple (i, j,k). All of the model’s parameters are again updated through backpropagation. 4 Evaluation 4.1 Implementational details We evaluate our neural network approach to selectional preference acquisition using verb-object tuples for the two-way model, and subject-verbobject tuples for the multi-way model. Our model has been applied to English, using the UKWaC corpus (Baroni et al., 2009), which covers about 2 billion words of web text. The corpus has been part of speech tagged and lemmatized with Stanford Part-Of-Speech Tagger (Toutanova et al., 2003), and parsed with MaltParser (Nivre et al., 2006), so that dependency tuples could be extracted. For the two-way model, we select all verbs and objects that appear within a predicate-argument relation with a frequency of at least 50. This gives us a total of about 7K verbs and 30K objects. For the multi-way model, we select the 2K most frequent verbs, together with the 10K most frequent subjects and the 10K most frequent objects (that appear within a transitive frame). All words are converted to lowercase. We use the lemmatized forms, and only keep those forms that contain alphabetic characters. Furthermore, we require each tuple to appear a</context>
</contexts>
<marker>Nivre, Hall, Nilsson, 2006</marker>
<rawString>Joakim Nivre, Johan Hall, and Jens Nilsson. 2006. Maltparser: A data-driven parser-generator for dependency parsing. In Proceedings of LREC-2006, pages 2216–2219.</rawString>
</citation>
<citation valid="true">
<authors>
<author>Diarmuid O´ S´eaghdha</author>
<author>Anna Korhonen</author>
</authors>
<title>Modelling selectional preferences in a lexical hierarchy.</title>
<date>2012</date>
<booktitle>In Proceedings of the First Joint Conference on Lexical and Computational Semantics-Volume 1: Proceedings of the main conference and the shared task, and Volume 2: Proceedings of the Sixth International Workshop on Semantic Evaluation,</booktitle>
<pages>170--179</pages>
<publisher>Association</publisher>
<institution>for Computational Linguistics.</institution>
<marker>S´eaghdha, Korhonen, 2012</marker>
<rawString>Diarmuid O´ S´eaghdha and Anna Korhonen. 2012. Modelling selectional preferences in a lexical hierarchy. In Proceedings of the First Joint Conference on Lexical and Computational Semantics-Volume 1: Proceedings of the main conference and the shared task, and Volume 2: Proceedings of the Sixth International Workshop on Semantic Evaluation, pages 170–179. Association for Computational Linguistics.</rawString>
</citation>
<citation valid="true">
<authors>
<author>Diarmuid O´ S´eaghdha</author>
</authors>
<title>Latent variable models of selectional preference.</title>
<date>2010</date>
<booktitle>In Proceedings of the 48th Annual Meeting of the Association for Computational Linguistics,</booktitle>
<pages>435--444</pages>
<publisher>Association</publisher>
<institution>for Computational Linguistics.</institution>
<marker>S´eaghdha, 2010</marker>
<rawString>Diarmuid O´ S´eaghdha. 2010. Latent variable models of selectional preference. In Proceedings of the 48th Annual Meeting of the Association for Computational Linguistics, pages 435–444. Association for Computational Linguistics.</rawString>
</citation>
<citation valid="true">
<authors>
<author>Sebastian Pad´o</author>
<author>Ulrike Pad´o</author>
<author>Katrin Erk</author>
</authors>
<title>Flexible, corpus-based modelling of human plausibility judgements.</title>
<date>2007</date>
<booktitle>In Proceedings of the 2007 Joint Conference on Empirical Methods in Natural Language Processing and Computational Natural Language Learning (EMNLP-CoNLL),</booktitle>
<pages>400--409</pages>
<publisher>Association</publisher>
<institution>for Computational Linguistics.</institution>
<location>Prague, Czech Republic,</location>
<marker>Pad´o, Pad´o, Erk, 2007</marker>
<rawString>Sebastian Pad´o, Ulrike Pad´o, and Katrin Erk. 2007. Flexible, corpus-based modelling of human plausibility judgements. In Proceedings of the 2007 Joint Conference on Empirical Methods in Natural Language Processing and Computational Natural Language Learning (EMNLP-CoNLL), pages 400–409, Prague, Czech Republic, June. Association for Computational Linguistics.</rawString>
</citation>
<citation valid="true">
<authors>
<author>Philip Resnik</author>
</authors>
<title>Selectional constraints: An information-theoretic model and its computational realization.</title>
<date>1996</date>
<journal>Cognition,</journal>
<pages>61--127</pages>
<contexts>
<context position="2594" citStr="Resnik, 1996" startWordPosition="390" endWordPosition="391">for selectional preference need to adequately deal with the consequences of Zipf’s law: language is inherently sparse, and the majority of language utterances occur very infrequently. As a consequence, models that are based on corpus data need to properly generalize beyond the mere co-occurrence frequencies of sparse corpus data, taking into account the semantic similarity of both predicates and arguments. Researchers have come up with various approaches to this generalization step. Earlier approaches to selectional preference acquisition mostly rely on hand-crafted resources such as WordNet (Resnik, 1996; Li and Abe, 1998; Clark and Weir, 2001), while later approaches tend to take advantage of unsupervised learning machinery, such as latent variable models (Rooth et al., 1999; O´ S´eaghdha, 2010) and distributional similarity metrics (Erk, 2007; Pad´o et al., 2007). This paper investigates the use of neural networks for the acquisition of selectional preferences. Inspired by recent advances of neural network models for NLP applications (Collobert and Weston, 2008; Mikolov et al., 2013), we propose a neural network model that learns to discriminate between felicitous and infelicitous arguments</context>
<context position="5154" citStr="Resnik (1996)" startWordPosition="776" endWordPosition="777">. The remainder of this paper is as follows. Section 2 first discusses related work with respect to selectional preference acquisition and neural network modeling. Section 3 describes our neural network architecture and its training procedure. Section 4 evaluates the model’s performance, comparing it to other existing models for selectional preference acquisition. Finally, section 5 concludes and indicates a number of avenues for future work. 2 Related Work 2.1 Selectional preferences One of the first approaches to the automatic induction of selectional preferences from corpora was the one by Resnik (1996). Resnik (1996) relies on WordNet synsets in order to generate generalized noun clusters. The selectional preference strength of a specific verb v in a particular relation is calculated by computing the Kullback-Leibler divergence between the cluster distribution of the verb and the prior cluster distribution. SR(v) = Ec p(c|v)log p(c|v) (1) p(c) where c stands for a noun cluster, and R stands for a given predicate-argument relation. The selectional association of a particular noun cluster is then the contribution of that cluster to the verb’s preference strength. AR(v,c) = p(c|v)log p(c|v) SR</context>
</contexts>
<marker>Resnik, 1996</marker>
<rawString>Philip Resnik. 1996. Selectional constraints: An information-theoretic model and its computational realization. Cognition, 61:127–159, November.</rawString>
</citation>
<citation valid="true">
<authors>
<author>Alan Ritter</author>
<author>Mausam</author>
<author>Oren Etzioni</author>
</authors>
<title>A latent dirichlet allocation method for selectional preferences.</title>
<date>2010</date>
<booktitle>In Proceedings of the 48th Annual Meeting of the Association for Computational Linguistics,</booktitle>
<pages>424--434</pages>
<publisher>Association</publisher>
<institution>for Computational Linguistics.</institution>
<location>Uppsala, Sweden,</location>
<contexts>
<context position="8692" citStr="Ritter et al. (2010)" startWordPosition="1330" endWordPosition="1334">ment pairs, while negative examples are constructed from unobserved combinations. An SVM classifier is used to distinguish the positive from the negative instances. The training procedure used in their model is based on an intuition that is similar to ours, although it is implemented using different techniques. A number of researchers presented models that are based on the framework of topic modeling. O´ S´eaghdha (2010) describes three models for selectional preference induction based on Latent Dirichlet Allocation, which model the selectional preference of a predicate and a single argument. Ritter et al. (2010) equally present a selectional preference model based on topic modeling, but they tackle multi-way selectional preferences (of transitive predicates, which take two arguments) instead. Finally, in previous work (Van de Cruys, 2009) we presented a model for multi-way selectional preference induction based on tensor factorization. Three-way co-occurrences of subjects, verbs, and objects are represented as a three-way tensor (the generalization of a matrix), and a latent factorization model is applied in order to generalize to unseen instances. We will compare our neural network based-approach fo</context>
</contexts>
<marker>Ritter, Mausam, Etzioni, 2010</marker>
<rawString>Alan Ritter, Mausam, and Oren Etzioni. 2010. A latent dirichlet allocation method for selectional preferences. In Proceedings of the 48th Annual Meeting of the Association for Computational Linguistics, pages 424–434, Uppsala, Sweden, July. Association for Computational Linguistics.</rawString>
</citation>
<citation valid="true">
<authors>
<author>Mats Rooth</author>
<author>Stefan Riezler</author>
<author>Detlef Prescher</author>
<author>Glenn Carroll</author>
<author>Franz Beil</author>
</authors>
<title>Inducing a semantically annotated lexicon via em-based clustering.</title>
<date>1999</date>
<booktitle>In Proceedings of the 37th annual meeting of the Association for Computational Linguistics on Computational Linguistics,</booktitle>
<pages>104--111</pages>
<publisher>Association</publisher>
<institution>for Computational Linguistics.</institution>
<contexts>
<context position="2769" citStr="Rooth et al., 1999" startWordPosition="417" endWordPosition="420"> infrequently. As a consequence, models that are based on corpus data need to properly generalize beyond the mere co-occurrence frequencies of sparse corpus data, taking into account the semantic similarity of both predicates and arguments. Researchers have come up with various approaches to this generalization step. Earlier approaches to selectional preference acquisition mostly rely on hand-crafted resources such as WordNet (Resnik, 1996; Li and Abe, 1998; Clark and Weir, 2001), while later approaches tend to take advantage of unsupervised learning machinery, such as latent variable models (Rooth et al., 1999; O´ S´eaghdha, 2010) and distributional similarity metrics (Erk, 2007; Pad´o et al., 2007). This paper investigates the use of neural networks for the acquisition of selectional preferences. Inspired by recent advances of neural network models for NLP applications (Collobert and Weston, 2008; Mikolov et al., 2013), we propose a neural network model that learns to discriminate between felicitous and infelicitous arguments for a particular predicate. The model is entirely unsupervised – preferences are learned from unannotated corpus data. Positive training instances are constructed from attest</context>
<context position="6559" citStr="Rooth et al. (1999)" startWordPosition="995" endWordPosition="998">e over arguments. Li and Abe (1998) use the principle of Minimum Description Length in order to find a suitable generalization level within the lexical WordNet hierarchy. A same intuition is used by Clark and Weir (2001), but they use hypothesis testing instead to find the appropriate level of generalization. A recent approach that makes use of WordNet (in combination with Bayesian modeling) is the one by O´ S´eaghdha and Korhonen (2012). Most researchers, however, acknowledge the shortcomings of hand-crafted resources, and focus on the acquisition of selectional preferences from corpus data. Rooth et al. (1999) propose an Expectation-Maximization (EM) clustering algorithm for selectional preference acquisition based on a probabilistic latent variable model. The idea is that both predicate v and argument o are generated from a latent variable c, where the latent variables represent clusters of tight verb-argument interactions. p(v,o) = Ep(c,v,o) = E p(c)p(v|c)p(o|c) (3) c∈C c∈C The use of latent variables allows the model to generalize to predicate-argument tuples that have not been seen during training. The latent variable distribution – and the probabilities of predicates and argument given the lat</context>
<context position="17340" citStr="Rooth et al., 1999" startWordPosition="2748" endWordPosition="2751"> All words are converted to lowercase. We use the lemmatized forms, and only keep those forms that contain alphabetic characters. Furthermore, we require each tuple to appear at least three times in the corpus. We set N, the size of our embedding matrices, to 50, and H, the number of units in the hidden layer, to 100. Following Huang et al. (2012), we use mini-batch L-BFGS (Liu and Nocedal, 1989) with 1000 pairs of good and corrupt tuples per batch for training, and train for 10 epochs. 4.2 Evaluation Setup 4.2.1 Task Our models are quantitatively evaluated using a pseudo-disambiguation task (Rooth et al., 1999), which bears some resemblance to our training procedure. The task provides an adequate test of the generalization capabilities of our models. For the two-way case, the task is to judge which object (o or o0) is more likely for a particular verb v, where (v,o) is a tuple attested in the corpus, and o0 is a direct object randomly drawn from the object vocabulary. The tuple is considered correct if the model prefers the attested tuple (v,o) over (v,o0). For the three-way case, the task is to judge which subject (s or s0) and direct object (o or o0) are more likely for a particular verb v, where </context>
<context position="20138" citStr="Rooth et al. (1999)" startWordPosition="3214" endWordPosition="3217"> instances (750K types), so in each fold, about 5M tuples are used for training and about 500K (75K types) are used for testing. Note that our training procedure is instance-based, while our evaluation is type-based: during training, the neural network sees a tuple as many times as it appears in the training set, while for testing each individual tuple is only evaluated once. 4.2.2 Comparison models We compare our neural network model to a number of other models for selectional preference acquisition. For the two-way case, we compare our model to the EM-based clustering technique presented by Rooth et al. (1999),2 and to Erk et al.’s (2010) similarity-based model. For Rooth et al.’s model, we set the number of latent factors to 50. Using a larger number of latent factors does not increase performance. For Erk et al.’s model, we create a dependency-based similarity model from the UKWaC corpus using our 30K direct objects as instances and 100K dependency relations as features. The resulting matrix is weighted using pointwise mutual information (Church and Hanks, 1990). Similarity values are computed using cosine. Furthermore, we use a sampling procedure in the testing phase: we sample 5000 predicate-ar</context>
<context position="21694" citStr="Rooth et al. (1999)" startWordPosition="3468" endWordPosition="3471">res the results of our neural network architecture for two-way selectional preferences to the results of Rooth et al.’s (1999) model and Erk et al.’s (2010) model. 2Our own implementation of Rooth et al.’s (1999) algorithm is based on non-negative matrix factorization (Lee and Seung, 2000). Non-negative matrix factorization with Kullback-Leibler divergence has been shown to minimize the same objective function as EM (Li and Ding, 2006). 3The best scoring model presented by Van de Cruys (2009) also uses 300 latent factors; using more factors does not improve the results. model accuracy (g ± 6) Rooth et al. (1999) .720 ± .002 Erk et al. (2010) .887 ± .004 2-way neural network .880 ± .001 Table 3: Comparison of model results for two-way selectional preference acquisition – mean accuracy and standard deviations of 10-fold cross-validation results The results indicate that our neural network approach outperforms Rooth et al.’s (1999) method by a large margin (16%). Clearly, the neural network architecture is able to model selectional preferences more profoundly than Rooth et al.’s latent variable approach. The difference between the models is highly statistically significant (paired t-test, p &lt; .01), as t</context>
</contexts>
<marker>Rooth, Riezler, Prescher, Carroll, Beil, 1999</marker>
<rawString>Mats Rooth, Stefan Riezler, Detlef Prescher, Glenn Carroll, and Franz Beil. 1999. Inducing a semantically annotated lexicon via em-based clustering. In Proceedings of the 37th annual meeting of the Association for Computational Linguistics on Computational Linguistics, pages 104–111. Association for Computational Linguistics.</rawString>
</citation>
<citation valid="true">
<authors>
<author>Ekaterina Shutova</author>
<author>Simone Teufel</author>
<author>Anna Korhonen</author>
</authors>
<title>Statistical metaphor processing.</title>
<date>2013</date>
<journal>Computational Linguistics,</journal>
<volume>39</volume>
<issue>2</issue>
<contexts>
<context position="1973" citStr="Shutova et al., 2013" startWordPosition="299" endWordPosition="302">d to sing, nor is a tomato something that is typically sung. Within the field of natural language processing, this inclination of predicates to select for particular arguments is known as selectional preference. The automatic acquisition of selectional preferences has been a popular research subject within the field of natural language processing. An automatically acquired selectional preference resource is a versatile tool for numerous NLP applications, such as semantic role labeling (Gildea and Jurafsky, 2002), word sense disambiguation (McCarthy and Carroll, 2003), and metaphor processing (Shutova et al., 2013). Models for selectional preference need to adequately deal with the consequences of Zipf’s law: language is inherently sparse, and the majority of language utterances occur very infrequently. As a consequence, models that are based on corpus data need to properly generalize beyond the mere co-occurrence frequencies of sparse corpus data, taking into account the semantic similarity of both predicates and arguments. Researchers have come up with various approaches to this generalization step. Earlier approaches to selectional preference acquisition mostly rely on hand-crafted resources such as </context>
</contexts>
<marker>Shutova, Teufel, Korhonen, 2013</marker>
<rawString>Ekaterina Shutova, Simone Teufel, and Anna Korhonen. 2013. Statistical metaphor processing. Computational Linguistics, 39(2):301–353.</rawString>
</citation>
<citation valid="true">
<authors>
<author>Kristina Toutanova</author>
<author>Dan Klein</author>
<author>Christopher Manning</author>
<author>Yoram Singer</author>
</authors>
<title>Feature-rich part-ofspeech tagging with a cyclic dependency network.</title>
<date>2003</date>
<booktitle>In Proceedings of HLT-NAACL 2003,</booktitle>
<pages>252--259</pages>
<contexts>
<context position="16248" citStr="Toutanova et al., 2003" startWordPosition="2561" endWordPosition="2564">ne corrupted subject j0 and one corrupted object k0 for each tuple (i, j,k). All of the model’s parameters are again updated through backpropagation. 4 Evaluation 4.1 Implementational details We evaluate our neural network approach to selectional preference acquisition using verb-object tuples for the two-way model, and subject-verbobject tuples for the multi-way model. Our model has been applied to English, using the UKWaC corpus (Baroni et al., 2009), which covers about 2 billion words of web text. The corpus has been part of speech tagged and lemmatized with Stanford Part-Of-Speech Tagger (Toutanova et al., 2003), and parsed with MaltParser (Nivre et al., 2006), so that dependency tuples could be extracted. For the two-way model, we select all verbs and objects that appear within a predicate-argument relation with a frequency of at least 50. This gives us a total of about 7K verbs and 30K objects. For the multi-way model, we select the 2K most frequent verbs, together with the 10K most frequent subjects and the 10K most frequent objects (that appear within a transitive frame). All words are converted to lowercase. We use the lemmatized forms, and only keep those forms that contain alphabetic character</context>
</contexts>
<marker>Toutanova, Klein, Manning, Singer, 2003</marker>
<rawString>Kristina Toutanova, Dan Klein, Christopher Manning, and Yoram Singer. 2003. Feature-rich part-ofspeech tagging with a cyclic dependency network. In Proceedings of HLT-NAACL 2003, pages 252– 259.</rawString>
</citation>
<citation valid="true">
<authors>
<author>Masashi Tsubaki</author>
<author>Kevin Duh</author>
<author>Masashi Shimbo</author>
<author>Yuji Matsumoto</author>
</authors>
<title>Modeling and learning semantic co-compositionality through prototype projections and neural networks.</title>
<date>2013</date>
<booktitle>In Proceedings of the 2013 Conference on Empirical Methods in Natural Language Processing,</booktitle>
<pages>130--140</pages>
<publisher>Association</publisher>
<institution>for Computational Linguistics.</institution>
<location>Seattle, Washington, USA,</location>
<contexts>
<context position="9953" citStr="Tsubaki et al. (2013)" startWordPosition="1516" endWordPosition="1519">sition to this tensor-based factorization model. 2.2 Neural networks In the last few years, neural networks have become increasingly popular in NLP applications. In particular, neural language models (Bengio et al., 2003; Mnih and Hinton, 2007; Collobert and Weston, 2008) have demonstrated impressive performance at the task of language modeling. By incorporating distributed representations for words that model their similarity, neural language models are able to overcome the problem of data sparseness that standard n-gram models are confronted with. Also related to our work is the approach by Tsubaki et al. (2013), who successfully use a neural network to model co-compositionality. Our model for selectional preference acquisition uses a network architecture that is similar to the abovementioned models. Its training objective is also similar to the ranking-loss training objective proposed by Collobert and Weston (2008), but we present a novel, modified version in order to deal with multi-way selectional preferences. 3 Methodology 3.1 Neural network architecture Our model computes the score for a predicate i and an argument j as follows. First, the selectional preference tuple (i, j) is represented as th</context>
</contexts>
<marker>Tsubaki, Duh, Shimbo, Matsumoto, 2013</marker>
<rawString>Masashi Tsubaki, Kevin Duh, Masashi Shimbo, and Yuji Matsumoto. 2013. Modeling and learning semantic co-compositionality through prototype projections and neural networks. In Proceedings of the 2013 Conference on Empirical Methods in Natural Language Processing, pages 130–140, Seattle, Washington, USA, October. Association for Computational Linguistics.</rawString>
</citation>
<citation valid="true">
<authors>
<author>Tim Van de Cruys</author>
</authors>
<title>A non-negative tensor factorization model for selectional preference induction.</title>
<date>2009</date>
<marker>Van de Cruys, 2009</marker>
<rawString>Tim Van de Cruys. 2009. A non-negative tensor factorization model for selectional preference induction.</rawString>
</citation>
<citation valid="true">
<date></date>
<booktitle>In Proceedings of the Workshop on Geometrical Models of Natural Language Semantics,</booktitle>
<pages>83--90</pages>
<publisher>Association</publisher>
<institution>for Computational Linguistics.</institution>
<location>Athens, Greece,</location>
<marker></marker>
<rawString>In Proceedings of the Workshop on Geometrical Models of Natural Language Semantics, pages 83– 90, Athens, Greece, March. Association for Computational Linguistics.</rawString>
</citation>
<citation valid="true">
<authors>
<author>Dong Yu</author>
<author>Li Deng</author>
<author>Frank Seide</author>
</authors>
<title>The deep tensor neural network with applications to large vocabulary speech recognition.</title>
<date>2013</date>
<journal>IEEE Transactions on Audio, Speech, and Language Processing,</journal>
<volume>21</volume>
<issue>2</issue>
<marker>Yu, Deng, Seide, 2013</marker>
<rawString>Dong Yu, Li Deng, and Frank Seide. 2013. The deep tensor neural network with applications to large vocabulary speech recognition. IEEE Transactions on Audio, Speech, and Language Processing, 21(2):388–396.</rawString>
</citation>
</citationList>
</algorithm>
</algorithms>
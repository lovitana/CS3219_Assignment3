<?xml version="1.0" encoding="UTF-8"?>
<algorithms version="110505">
<algorithm name="SectLabel" version="110505">
<variant no="0" confidence="0.000002">
<title confidence="0.98908">
Clustering Aspect-related Phrases by Leveraging Sentiment Distribution
Consistency
</title>
<author confidence="0.998438">
Li Zhao, Minlie Huang, Haiqiang Chen*, Junjun Cheng*, Xiaoyan Zhu
</author>
<affiliation confidence="0.99029775">
State Key Laboratory of Intelligent Technology and Systems
National Laboratory for Information Science and Technology
Dept. of Computer Science and Technology, Tsinghua University, Beijing, PR China
*China Information Technology Security Evaluation Center
</affiliation>
<email confidence="0.994316">
zhaoli19881113@126.com aihuang@tsinghua.edu.cn
</email>
<sectionHeader confidence="0.993757" genericHeader="abstract">
Abstract
</sectionHeader>
<bodyText confidence="0.999862380952381">
Clustering aspect-related phrases in terms
of product’s property is a precursor pro-
cess to aspect-level sentiment analysis
which is a central task in sentiment analy-
sis. Most of existing methods for address-
ing this problem are context-based models
which assume that domain synonymous
phrases share similar co-occurrence con-
texts. In this paper, we explore a novel
idea, sentiment distribution consistency,
which states that different phrases (e.g.
“price”, “money”, “worth”, and “cost”) of
the same aspect tend to have consistent
sentiment distribution. Through formal-
izing sentiment distribution consistency as
soft constraint, we propose a novel unsu-
pervised model in the framework of Poste-
rior Regularization (PR) to cluster aspect-
related phrases. Experiments demonstrate
that our approach outperforms baselines
remarkably.
</bodyText>
<sectionHeader confidence="0.998994" genericHeader="introduction">
1 Introduction
</sectionHeader>
<bodyText confidence="0.999903371428572">
Aspect-level sentiment analysis has become a cen-
tral task in sentiment analysis because it can ag-
gregate various opinions according to a product’s
properties, and provide much detailed, complete,
and in-depth summaries of a large number of re-
views. Aspect finding and clustering, a precursor
process of aspect-level sentiment analysis, has at-
tracted more and more attentions (Mukherjee and
Liu, 2012; Chen et al., 2013; Zhai et al., 2011a;
Zhai et al., 2010).
Aspect finding and clustering has never been a
trivial task. People often use different words or
phrases to refer to the same product property (also
called product aspect or feature in the literature).
Some terms are lexically dissimilar while seman-
tically close, which makes the task more challeng-
ing. For example, “price”, “money” , “worth” and
“cost” all refer to the aspect “price” in reviews.
In order to present aspect-specific summaries of
opinions, we first of all, have to cluster different
aspect-related phrases. It is expensive and time-
consuming to manually group hundreds of aspect-
related phrases. In this paper, we assume that the
aspect phrases have been extracted in advance and
we keep focused on clustering domain synony-
mous aspect-related phrases.
Existing studies addressing this problem are
mainly based on the assumption that different
phrases of the same aspect should have similar co-
occurrence contexts. In addition to the traditional
assumption, we develop a new angle to address the
problem, which is based on sentiment distribution
consistency assumption that different phrases of
the same aspect should have consistent sentiment
distribution, which will be detailed soon later.
</bodyText>
<figureCaption confidence="0.999515">
Figure 1: A semi-structured Review.
</figureCaption>
<bodyText confidence="0.9998338">
This new angle is inspired by this simple obser-
vation (as illustrated in Fig. 1): two phrases within
the same cluster are not likely to be simultaneously
placed in Pros and Cons of the same review. A
straightforward way to use this information is to
formulate cannot-link knowledge in clustering al-
gorithms (Chen et al., 2013; Zhai et al., 2011b).
However, we have a particularly different manner
to leverage the knowledge.
Due to the availability of large-scale semi-
structured customer reviews (as exemplified in
Fig. 1) that are supported by many web sites,
we can easily get the estimation of sentiment dis-
tribution for each aspect phrase by simply count-
ing how many times a phrase appears in Pros and
</bodyText>
<page confidence="0.945067">
1614
</page>
<note confidence="0.906185">
Proceedings of the 2014 Conference on Empirical Methods in Natural Language Processing (EMNLP), pages 1614–1623,
October 25-29, 2014, Doha, Qatar. c�2014 Association for Computational Linguistics
</note>
<bodyText confidence="0.986608">
Cons respectively. As illustrated in Fig. 2, we
can see that the estimated sentiment distribution
of a phrase is close to that of its aspect. The
above observation suggests the sentiment distri-
bution consistency assumption: different phrases
of the same aspect tend to have the same senti-
ment distribution, or to have statistically close
distributions. This assumption is also verified by
our data: for most (above 91.3%) phrase with rela-
tively reliable estimation (whose occurrence &gt;50),
the KL-divergence between the sentiment distri-
bution of a phrase and that of its corresponding
aspect is less than 0.05.
</bodyText>
<figureCaption confidence="0.543108333333333">
Figure 2: The sentiment distribution of aspect
“battery” and its related-phrases on nokia 5130
with a large amount of reviews.
</figureCaption>
<bodyText confidence="0.9970815">
It is worth noting that, the sentiment distribution
of a phrase can be estimated accurately only when
we obtain a sufficient number of reviews. When
the number of reviews is limited, however, the es-
timated sentiment distribution for each phrase is
unreliable (as shown in Fig. 3). A key issue,
arisen here, is how to formulate this assumption in
a statistically robust manner. The proposed model
should be robust when only a limited number of
reviews are available.
</bodyText>
<figureCaption confidence="0.550018">
Figure 3: The sentiment distribution of aspect
</figureCaption>
<bodyText confidence="0.959912">
“battery” and its related-phrases on nokia 3110c
with a small mumber of reviews.
To deal with this issue, we model sentiment dis-
tribution consistency as soft constraint, integrated
into a probabilistic model that maximizes the data
likelihood. We design the constraint to work in
the following way: when we have sufficient ob-
servations, the constraint becomes tighter, which
plays a more important role in the learning pro-
cess; when we have limited observations, the con-
straint becomes very loose so that it will have less
effect on the model.
In this paper, we propose a novel unsupervised
model, Sentiment Distribution Consistency Reg-
ularized Multinomial Naive Bayes (SDC-MNB).
The context part is modeled by Multinomial Naive
Bayes in which aspect is treated as latent variable,
and Sentiment distribution consistency is encoded
as soft constraint within the framework of Poste-
rior Regularization (PR) (Graca et al., 2008). The
main contributions of this paper are summarized
as follows:
</bodyText>
<listItem confidence="0.9888763">
• We study the problem of clustering phrases
by integrating both context information
and sentiment distribution of aspect-related
phrases.
• We explore a novel concept, sentiment distri-
bution consistency(SDC), and model it as soft
constraint to guide the clustering process.
• Experiments show that our model outper-
forms the state-of-art approaches for aspect
clustering.
</listItem>
<bodyText confidence="0.9997834">
The rest of this paper is organized as follows.
We introduce the SDC-MNB model in Section 2.
We present experiment results in Section 3. In
Section 4, we survey related work. We summarize
the work in Section 5.
</bodyText>
<sectionHeader confidence="0.9255885" genericHeader="method">
2 Sentiment Distribution Consistency
Regularized Multinomial Naive Bayes
</sectionHeader>
<bodyText confidence="0.99975275">
In this section, we firstly introduce our assumption
sentiment distribution consistency formally and
show how to model the above assumption as soft
constraint, which we term SDC-constraint. Sec-
ondly, we show how to combine SDC-constraint
with the probabilistic context model. Finally, we
present the details for context and sentiment ex-
traction.
</bodyText>
<subsectionHeader confidence="0.988582">
2.1 Sentiment Distribution Consistency
</subsectionHeader>
<bodyText confidence="0.999841333333333">
We define aspect as a set of phrases that refer to
the same property of a product and each phrase is
termed aspect-related phrase (or aspect phrase in
short). For example, the aspect “battery” contains
aspect phrases such as “battery”, “battery life”,
“power”, and so on.
</bodyText>
<page confidence="0.983126">
1615
</page>
<table confidence="0.996912173913043">
F the aspect phrase set
fj the jth aspect phrase
yj the aspect for aspect phrase fj
A the aspect set
ai the ith aspect
D the set of context documents
dj the context document of fj
V the word vocabulary
wt the tth word in vocabulary V
wdj,k the kth word in dj
Ntj the number of times word wt occurs in dj
P the product set
pk the kth product
uik the sentiment distribution parameter
ˆSjk of aspect ai on pk
njk the estimated sentiment distribution parameter
ˆ�jk of phrase fj on pk
the occurrence times of aspect phrase fj on pk
the sample standard deviation
B the model parameters
pe(ai|dj) the posterior distribution of ai given dj
4(yj = ai) the projected posterior distribution
of ai given dj
</table>
<tableCaption confidence="0.999868">
Table 1: Notations
</tableCaption>
<bodyText confidence="0.999791142857143">
Let us consider the sentiment distribution on a
certain aspect ai. In a large review dataset, as-
pect ai could receive many comments from differ-
ent reviewers. For each comment, we assume that
people either praise or complain about the aspect.
So each comment on the aspect can be seen as a
Bernoulli trial, where the aspect receives positive
comments with probability pai1. We introduce a
random variable Xai to denote the sentiment on
aspect ai, where Xai = 1 means that aspect ai
receives positive comments, Xai = 0 means that
aspect ai receives negative comments. Obviously,
the sentiment on aspect ai follows the Bernoulli
distribution,
</bodyText>
<equation confidence="0.99855475">
Pr(Xai) = pXai
ai * (1 − pai)1−Xai � Xai E f0� 11. (1)
Or in short,
Xai — Bernoulli(pai)
</equation>
<bodyText confidence="0.998774333333333">
Let us see the case for aspect phrase fj, where
fj E aspect ai. Similarly, each comment on an as-
pect phrase fj can also be seen as a Bernoulli trial.
We introduce a random variable Xfj to denote the
sentiment on aspect phrase fj, where Xfj = 1
means that aspect fj receives positive comments,
Xfj = 0 means that aspect fj receives negative
comments. As just discussed, we assume that each
aspect phrase follows the same distribution with
</bodyText>
<footnote confidence="0.7351435">
1positive comment means that an aspect term is observed
in Pros of a review.
</footnote>
<bodyText confidence="0.6868555">
the corresponding aspect. This leads to the fol-
lowing formal description:
</bodyText>
<listItem confidence="0.9990386">
• Sentiment Distribution Consistency : The
sentiment distribution of aspect phrase is the
same as that of the corresponding aspect.
Formally, for all aspect phrase fj E aspect
ai, Xfj — Bernoulli(pai).
</listItem>
<subsectionHeader confidence="0.9931945">
2.2 Sentiment Distribution Consistency
Constraint
</subsectionHeader>
<bodyText confidence="0.999661714285714">
Assuming the sentiment distribution of aspect ai is
given in advance, we need to judge whether an as-
pect phrase fj belongs to the aspect ai with limited
observations for fj. Let’s consider the example in
Fig. 4. For aspect phrase 3, we have no definite
answer due to the limited number of observations.
For aspect phrase 1, it seems that the sentiment
distribution is consistent with that of the left as-
pect. However, we can not say that the phrase be-
longs to the aspect because the distribution may
be the same for two different aspects. For aspect
phrase 2, we are confident that its sentiment dis-
tribution is different from that of the left aspect,
given sufficient observations.
</bodyText>
<figureCaption confidence="0.9726235">
Figure 4: Sentiment distribution of an aspect, and
observations on aspect phrases.
</figureCaption>
<bodyText confidence="0.999832882352941">
To be concise, we judge an aspect phrase
doesn’t belong to certain aspect only when we are
confident that they follow different sentiment dis-
tributions.
Inspired by the intuition, we conduct interval
parameter estimation for parameter pfj(sentiment
distribution for phrase fj) with limited observa-
tions, and thus get a confidence interval for pfj.
If pai(sentiment distribution for aspect ai) is not
in the confidence interval of pfj, we then are con-
fident that they follow different distributions. In
other words, if aspect phrase fj E aspect ai, we
are confident that pai is in the confidence interval
of pfj.
More formally, we use uik to denote the senti-
ment distribution parameter of aspect ai on prod-
uct pk, and assume that uik is given in advance.
</bodyText>
<page confidence="0.947417">
1616
</page>
<bodyText confidence="0.937658956521739">
We want to know whether the sentiment distribu-
tion on aspect phrase fj is the same as that of as-
pect ai on product pk given a limited number of
observations (samples). It’s straightforward to cal-
culate the confidence interval for parameter sjk in
the Bernoulli distribution function. Let the sam-
ple mean of njk samples be sjk, and the sample
standard deviation be Qjk. Since the sample size
is small here, we use the Student-t distribution to
calculate the confidence interval. According to our
assumption, we are confident that uik is in the con-
fidence interval if fj E ai.
√njk , ∀fj ∈ ai, ∀k. (2)
�σjk
where we look for t-table to find C corresponding
to a certain confidence level(such as 95%) with the
freedom of njk − 1. For simplicity, we represent
the above confidence interval by Pik − djk, sjk +
djk�, where djk = C �σjk
�njk .
We introduce an indicator variable zij to repre-
sent whether the aspect phrase fj belongs to aspect
ai, as follows:
</bodyText>
<equation confidence="0.9840575">
�
1 ; if fj ∈ ai
zji = (3)
0 ; otherwise
</equation>
<bodyText confidence="0.574724">
This leads to our SDC-constraint function.
</bodyText>
<equation confidence="0.952114">
ϕ = zji|uik − �sjk |≤ djk, ∀i, j, k (4)
</equation>
<bodyText confidence="0.999988523809524">
SDC-constraint are flexible for modeling Senti-
ment Distribution Consistency. The more obser-
vations we have, the smaller djk is. For frequent
aspect phrase, the constraint can be very informa-
tive because it can filter unrelated aspects for as-
pect phrase fj. The less observations we have,
the larger djk is. For rare aspect phrases, the con-
straint can be very loose, and will not have much
effect on the clustering process for aspect phrase
fj. In this way, the model can work very robustly.
SDC-constraints are data-driven constraints.
Usually we have many reviews about hundreds of
products in our dataset. For each aspect phrase,
there are |A |* |P |constraints (the number of as-
pects times the number of product). With thou-
sands of constraints about which aspect it is not
likely to belong to, the model learns to which as-
pect a phrase fj should be assigned. Although
most constraints may be loose because of the lim-
ited observations, SDC-constraint can still play an
important role in the learning process.
</bodyText>
<subsectionHeader confidence="0.8614935">
2.3 Sentiment Distribution Consistency
Regularized Multinomial Naive Bayes
</subsectionHeader>
<bodyText confidence="0.979388090909091">
(SDC-MNB)
In this section, we present our probabilistic model
which employs both context information and sen-
timent distribution.
First of all, we extract a context document d
for each aspect phrase, which will be described in
Section 2.5. In other word, a phrase is represented
by its context document. Assuming that the doc-
uments in D are independent and identically dis-
tributed, the probability of generating D is then
given by:
</bodyText>
<equation confidence="0.992853">
pθ(dj, yj) (5)
</equation>
<bodyText confidence="0.999931384615385">
where yj is a latent variable indicating the aspect
label for aspect phrase fj, and B is the model pa-
rameter.
In our problem, we are actually more inter-
ested in the posterior distribution over aspect,
i.e., pθ(yj|dj). Once the learned parameter B is
obtained, we can get our clustering result from
pθ(yj|dj), by assigning aspect ai with the largest
posterior to phrase fj. We can also enforce SDC-
constraint in expectation(on posterior pθ). We use
q(Y ) to denote the valid posterior distribution that
satisfy our SDC-constraint, and Q to denote the
valid posterior distribution space, as follows:
</bodyText>
<equation confidence="0.999398">
Q = {q(Y ) : Eq[zji|uik − �sjk|1 ≤ djk, ∀i, j, k}. (6)
</equation>
<bodyText confidence="0.999319555555556">
Since posterior plays such an important role in
joining the context model and SDC-constraint, we
formulate our problem in the framework of Poste-
rior Regularization (PR). PR is an efficient frame-
work to inject constraints on the posteriors of la-
tent variables. Instead of restricting pθ directly,
which might not be feasible, PR penalizes the dis-
tance of pθ to the constraint set Q. The posterior-
regularized objective is termed as follows:
</bodyText>
<equation confidence="0.7384865">
max{log pθ(D) − min
θ q∈Q
</equation>
<bodyText confidence="0.999952777777778">
By trading off the data likelihood of the ob-
served context documents (as defined in the first
term), and the KL divergence of the posteriors
to the valid posterior subspace defined by SDC-
constraint (as defined in the second term), the ob-
jective encourages models with both desired pos-
terior distribution and data likelihood. In essence,
the model attempts to maximize data likelihood of
context subject (softly) to SDC-constraint.
</bodyText>
<equation confidence="0.995255214285714">
sjk − C
�σjk ≤ uik ≤ sjk + C
√njk
|D|
ri
j=1
pθ(dj) =
pθ(D) =
|D|
ri
j=1
E
yj ∈A
KL(q(Y )||pθ(Y |D))} (7)
</equation>
<page confidence="0.959797">
1617
</page>
<subsectionHeader confidence="0.951898">
2.3.1 Multinomial Naive Bayes
</subsectionHeader>
<bodyText confidence="0.999992571428571">
In spirit to (Zhai et al., 2011a), we use Multino-
mial Naive Bayes (MNB) to model the context
document. Let wdj,k denotes the kth word in doc-
ument dj, where each word is from the vocabulary
V = {w1, w2,..., w|V |}. For each aspect phrase
fj, the probability of its latent aspect being ai and
generating context document di is
</bodyText>
<equation confidence="0.98626">
|dj|
pg(dj, yj = ai) = p(ai) ∏ p(wdj,k|ai) (8)
k=1
</equation>
<bodyText confidence="0.999935444444444">
where p(ai) and p(wdj,k|ai) are parameters of this
model. Each word wdj,k is conditionally indepen-
dent of all other words given the aspect ai.
Although MNB has been used in existing work
for aspect clustering, all of the studies used it in
a semi-supervised manner, with labeled data or
pseudo-labeled data. In contrast, MNB proposed
here is used in an unsupervised manner for aspect-
related phrases clustering.
</bodyText>
<subsectionHeader confidence="0.964317">
2.3.2 SDC-constraint
</subsectionHeader>
<bodyText confidence="0.9999185">
As mentioned above, the constraint posterior set Q
is defined by
</bodyText>
<equation confidence="0.999671">
Q = {q(Y ) &apos; q(yj = ai)|uik − �sjk |≤ djk, ∀i, j, k}. (9)
</equation>
<bodyText confidence="0.9998108">
We can see that Q denotes a set of linear con-
straints on the projected posterior distribution q.
Note that we do not directly observe uik, the sen-
timent distribution of aspect ai on product pk. For
aspect phrase fj that belongs to aspect ai, we es-
timate uik by counting all sentiment samples. We
use the posterior pg(ai|dj) to approximately rep-
resent how likely phrase fj belongs to aspect ai.
where pg(ai|dj) is short for pg(yj = ai|dj), the
probability that aspect phrase fj belongs to ai
given the context document dj. We estimate uik in
this way because observations for aspect are rela-
tively sufficient for a reliable estimation since ob-
servations for an aspect are aggregated from those
for all phrases belonging to that aspect.
</bodyText>
<subsectionHeader confidence="0.994433">
2.4 The Optimization Algorithm
</subsectionHeader>
<bodyText confidence="0.993004142857143">
The optimization algorithm for the objective (see
Eq. 7) is an EM-like two-stage iterative algorithm.
In E-step, we first calculate the posterior distri-
bution pg(ai|dj), then project it onto the valid pos-
terior distribution space Q. Given the parameters
B, the posterior distribution can be calculated by
Eq. 11.
</bodyText>
<equation confidence="0.9998404">
pθ(ai|dj) = p(ai) ∏|dj|
k=1 p(wdj,k|ai)
∑|A |(11)
r=1 p(ar) ∏|dj|
k=1 p(wdj ,k|ar)
</equation>
<bodyText confidence="0.99942225">
We use the above posterior distribution to update
the sentiment parameter for each aspect by Eq. 10.
The projected posterior distribution q is calculated
by
</bodyText>
<equation confidence="0.985848">
q = argmin KL(q(Y )||pθ(Y |D)) (12)
q∈Q
</equation>
<bodyText confidence="0.998033">
For each instance, there are |A |* |P |constraints.
However, we can prune a large number of useless
constraints derived from limited observations. All
constraints with djk &gt; 1 can be pruned, due to
the fact that the parameter uik, sok is within [0,1],
and the difference can not be larger than 1. This
optimization problem in Eq. 12 is easily solved via
the dual form by the projected gradient algorithm
(Boyd and Vandenberghe, 2004):
</bodyText>
<equation confidence="0.998154227272727">
( ∑|A |∑|P |
max −
λ≥0
i
k
u
k
3jk
a
i
|
i
−
|}−ϵ∥
∥ l
/(13)
bution q by
1
pθ (ai  |dj)ex p { −∑
 |P |
λ ik |u ik − §jk |} (14)
k=1
</equation>
<bodyText confidence="0.998939555555556">
where Z is the normalization factor. Note that sen-
timent distribution consistency is actually modeled
as instance-level constraint here, which makes it
very efficient to solve.
In M-step, the projected posteriors q(Y ) are
then used to compute sufficient statistics and up-
date the models parameters B. Given the projected
posteriors q(Y ), the parameters can be updated by
Eq. 15,16.
</bodyText>
<equation confidence="0.999563142857143">
p(ai) = 1 +
∑|D|
|A |+ |D|
j=1 q(yj = ai) (15)
∑ |P |
k=1
X
</equation>
<bodyText confidence="0.9998075">
where E controls the slack size for constraint. After
solving the above optimization problem and ob-
taining the optimal A, we can calculate the pro-
jected posterior distri
</bodyText>
<equation confidence="0.9995735">
q(yj = ai) = Z
1 +
</equation>
<bodyText confidence="0.9991558">
where
is the number of times that the word wt
occurs in document
The parameters are initialized randomly, and we
repeat E-step an
</bodyText>
<equation confidence="0.957287761904762">
∑|D|
j=1 Ntiq(yj = ai)
p(wt|ai) = |V  |+ ∑|V  |∑|D |(16)
j=1 Nmjq(yj = ai)
m=1
Ntj
dj.
d M-step until convergence.
|D|
uik = ∑|D|
j=1 njkpθ(ai|dj)j=1
∑
1
njkpθ(ai|dj)�sjk (10)
=1 k=1
,\
ikdjk−
log
pθ(ai|dj)exp{−
∑ |A|
i=1
</equation>
<page confidence="0.98288">
1618
</page>
<subsectionHeader confidence="0.94613">
2.5 Data Extraction
2.5.1 Context Extraction
</subsectionHeader>
<bodyText confidence="0.995201761904762">
In order to extract the context document d for each
aspect phrase, we follow the approach in Zhai et
al. (2011a). For each aspect phrase, we generate
its context document by aggregating the surround-
ing texts of the phrase in all reviews. The preced-
ing and following t words of a phrase are taken as
the context where we set t = 3 in this paper. Stop-
words and other aspect phrases are removed. For
example, the following review contains two aspect
phrases, ”screen” and ”picture”,
The LCD screen gives clear picture.
For ”screen”, the surrounding texts are {the,
LCD, gives, clear, picture}. We remove stop-
words ”the”, and the aspect term ”picture”, and
the resultant context of ”screen” in this review is
context(screen) ={LCD, screen, gives, clear}.
Similarly, the context of ”picture” in this review is
context(picture) ={gives, clear}.
By aggregating the contexts of all the reviews
that contain aspect phrase fj, we obtain the cor-
responding context document dj.
</bodyText>
<subsectionHeader confidence="0.940618">
2.5.2 Sentiment Extraction
</subsectionHeader>
<bodyText confidence="0.999996307692308">
Since we use semi-structured reviews, we ob-
tain the estimated sentiment distribution by sim-
ply counting how many times each aspect phrase
appears in Pros and Cons reviews for each prod-
uct respectively. So for each aspect phrase fj, let
n+jk denotes the times that fj appears in Pros of
all reviews for product pk, and let n�jk denotes the
times that fj appears in Cons of all reviews for
product pk. So the total number of occurrence of a
phrase is njk = n+jk + n�k. We have samples like
(1,1,1,0,0) where 1 means a phrase occurs in Pros
of a review, and 0 in Cons. Given a sequence of
such observations, the sample mean is easily com-
</bodyText>
<equation confidence="0.855633333333333">
n+
jk
puted as sjk = k+nlY,� . And the sample standard
n+j
deviation is Qjk = 1V, .
njk�1
</equation>
<sectionHeader confidence="0.999834" genericHeader="method">
3 Experiments
</sectionHeader>
<subsectionHeader confidence="0.999587">
3.1 Data Preparation
</subsectionHeader>
<bodyText confidence="0.998525555555556">
The details of our review corpus are given
in Table 2. This corpus contains semi-
structured customer reviews from four do-
mains: Camera, Cellphone, Laptop, and MP3.
These reviews were crawled from the following
web sites: www.amazon.cn, www.360buy.com,
www.newegg.com.cn, and www.zol.com. The as-
pect label of each aspect phrases is annotated by
human curators.
</bodyText>
<table confidence="0.99928">
Camera Cellphone Laptop MP3
#Products 449 694 702 329
#Reviews 101,235 579,402 102,439 129,471
#Aspect Phrases 236 230 238 166
#Aspect 12 10 14 8
</table>
<tableCaption confidence="0.97732">
Table 2: Statistics of the review corpus. # denotes
the size.
</tableCaption>
<subsectionHeader confidence="0.998303">
3.2 Evaluation Measures
</subsectionHeader>
<bodyText confidence="0.994779285714286">
We adapt three measures Purity, Entropy, and
Rand Index for performance evaluation. These
measures have been commonly used to evaluate
clustering algorithms.
Given a data set DS, suppose its gold-standard
partition is G = {g1, ..., gj, ..., gk}, where k
is the number of clusters. A clustering algo-
rithm partitions DS into k disjoint subsets, say
DS1, DS2, ..., DSk.
Entropy: For each resulting cluster, we can mea-
sure its entropy using Eq. 17, where Pi(gj) is the
proportion of data points of class gj in DSi. The
entropy of the entire clustering result is calculated
by Eq. 18.
</bodyText>
<equation confidence="0.9957532">
k
entropy(DSi) E Pi(gj)log2Pi(gj) (17)
j=1
|DSi|
|DS |entropy(DSi) (18)
</equation>
<bodyText confidence="0.9960568">
Purity: Purity measures the extent that a cluster
contains only data from one gold-standard parti-
tion. The cluster purity is computed with Eq. 19.
The total purity of the whole clustering result (all
clusters) is computed with Eq. 20.
</bodyText>
<equation confidence="0.997824">
purity(DSi) = max Pi(gj) (19)
j
|DSi|
|DS |purity(DSi) (20)
</equation>
<bodyText confidence="0.999559428571429">
RI: The Rand Index(RI) penalizes both false posi-
tive and false negative decisions during clustering.
Let TP (True Positive) denotes the number of pairs
of elements that are in the same set in DS and in
the same set in G. TN (True Negative) denotes
number of pairs of elements that are in different
sets in DS and in different sets in G. FP (False
</bodyText>
<equation confidence="0.995494666666667">
k
entropy(DS) =
i=1
k
purity(DS) =
i=1
</equation>
<page confidence="0.97972">
1619
</page>
<table confidence="0.999647571428571">
Camera Cellphone Laptop MP3
P RI E P RI E P RI E P RI E
Kmeans 43.48% 83.52% 2.098 48.91% 84.80% 1.792 43.46% 87.11% 2.211 40.00% 70.98% 2.047
L-EM 54.89% 87.07% 1.690 51.96% 86.64% 1.456 48.94% 84.53% 2.039 44.24% 75.37% 1.990
LDA 36.84% 83.28% 2.426 48.65% 85.33% 1.833 35.02% 83.53% 2.660 36.12% 76.08% 2.296
Constraint-LDA 43.30% 86.01% 2.216 47.89% 86.04% 1.974 32.35% 84.86% 2.676 50.70% 81.42% 1.924
SDC-MNB 56.42% 88.16% 1.725 67.95% 90.62% 1.266 55.52% 90.72% 1.780 58.06% 83.57% 1.578
</table>
<tableCaption confidence="0.791268">
Table 3: Comparison to unsupervised baselines. (P is short for purity, E for entropy, and RI for random
index.)
</tableCaption>
<bodyText confidence="0.996219333333333">
Positive) denotes number of pairs of elements in
S that are in the same set in D5 and in different
sets in G. FN (False Negative) denotes number of
pairs of elements that are in different sets in D5
and in the same set in G. The Rand Index(RI) is
computed with Eq. 21.
</bodyText>
<equation confidence="0.998178">
TP + TN
RI(DS) = (21)
TP + TN + FP + FN
</equation>
<subsectionHeader confidence="0.95154">
3.3 Evaluation Results
3.3.1 Comparison to unsupervised baselines
</subsectionHeader>
<bodyText confidence="0.999428333333333">
We compared our approach with several existing
unsupervised methods. Some of the methods aug-
mented unsupervised models by incorporating lex-
ical similarity and other domain knowledge. All
of them are context-based models.2 We list these
models as follows.
</bodyText>
<listItem confidence="0.715478176470588">
• Kmeans: Kmeans is the most popular cluster-
ing algorithm. Here we use the context distri-
butional similarity (cosine similarity) as the
similarity measure.
• L-EM: This is a state-of-the-art unsupervised
method for clustering aspect phrases (Zhai et
al., 2011a). L-EM employed lexical knowl-
edge to provide a better initialization for EM.
• LDA: LDA is a popular topic model(Blei et
al., 2003). Given a set of documents, it out-
puts groups of terms of different topics. In
our case, each aspect phrase is processed as a
term. 3 Each sentence in a review is consid-
ered as a document. Each aspect is consid-
ered as a topic. In LDA, a term may belong
to more than one topic/group, but we take the
topic/group with the maximum probability.
</listItem>
<footnote confidence="0.61474275">
2In our method, we collect context document for each
aspect phrase. This process is conducted for L-EM and K-
means. But for LDA and Constraint-LDA, we take each sen-
tence of reviews as a document. This setting for the LDA
baselines is adapted from previous work.
3Each aspect phrase is pre-processed as a single word
(e.g., “battery life” is treated as battery-life). Other words
are normally used in LDA.
</footnote>
<listItem confidence="0.767128">
• Constraint-LDA: Constraint-LDA (Zhai et
al., 2011b) is a state-of-the-art LDA-based
method that incorporates must-link and
cannot-link constraints for this task. We set
</listItem>
<bodyText confidence="0.890135583333333">
the damping factor A = 0.3 and relaxation
factor q = 0.9, as suggested in the original
reference.
For all methods that depend on the random ini-
tiation, we use the average results of 10 runs as the
final result. For all LDA-based models, we choose
α = 50/T, Q = 0.1, and run 1000 iterations.
Experiment results are shown in Table 3. We
can see that our approach almost outperforms all
unsupervised baseline methods by a large margin
on all domains. In addition, we have the following
observations:
</bodyText>
<listItem confidence="0.9919354">
• LDA and Kmeans perform poorly due to the
fact that the two methods do not use any prior
knowledge. It is also shown that only using
the context distributional information is not
sufficient for clustering aspect phrases.
• Constraint-LDA and L-EM that utilize prior
knowledge perform better. We can see that
Constraint-LDA outperforms LDA in terms
of RI (Rand Index) on all domains. L-EM
achieves the best results against the baselines.
This demonstrates the effectiveness to incor-
porate prior knowledge.
• SDC-MNB produces the optimal results
among all models for clustering. Methods
that use must-links and cannot-links may suf-
fer from noisy links. For L-EM, we find
that it is sensitive to noisy must-links. As
L-EM assumes that must-link is transitive,
several noisy must-links may totally misla-
bel the softly annotated data. For Constraint-
LDA, it is more robust than L-EM, because
it doesn’t assume the transitivity of must-
link. However, it only promotes the RI (Rand
Index) consistently by leveraging pair-wise
prior knowledge, but sometimes it hurts the
</listItem>
<page confidence="0.985647">
1620
</page>
<bodyText confidence="0.899501666666667">
performance with respect to purity or en-
tropy. Our method is consistently better on
almost all domains, which shows the advan-
tages of the proposed model.
• SDC-MNB is remarkably better than base-
lines, particularly for the cellphone domain.
We argue that this is because we have the
largest number of reviews for each product
in the cellphone domain. The larger dataset
gives us more observations on each phrase,
so that we obtain more reliable estimation of
model parameters.
</bodyText>
<subsectionHeader confidence="0.927481">
3.3.2 Comparison to supervised baselines
</subsectionHeader>
<bodyText confidence="0.999469">
We further compare our methods with two super-
vised models. For each supervised model, we
provide a proportion of manually labeled data for
training, which is randomly selected from gold-
standard annotations. However, we didn’t use any
labeled data for our approach.
</bodyText>
<listItem confidence="0.961510333333333">
• MNB: The labeled seeds are used to train a
MNB classifier to classify all unlabeled as-
pect phrases into different classes.
• L-Kmeans: In L-Kmeans, the clusters of the
labeled seeds are fixed at the initiation and
remain unchanged during iteration.
</listItem>
<table confidence="0.999360125">
Purity RI Entropy
MNB-5% 53.21% 85.77% 1.854
MNB-10% 59.55% 86.70% 1.656
MNB-15% 66.06% 88.39% 1.449
L-Kmeans-10% 53.54% 86.15% 1.745
L-Kmeans-15% 57.00% 86.89% 1.643
L-Kmeans-20% 60.97% 87.63% 1.528
SDC-MNB 59.49% 88.26% 1.580
</table>
<tableCaption confidence="0.9946965">
Table 4: Comparison to supervised baselines.
MNB-5% means MNB with 5% labeled data.
</tableCaption>
<bodyText confidence="0.999810727272727">
We experiment with several settings: taking
5%, 10% and 15% of the manually labeled aspect
phrases for training, and the remainder as unla-
beled data. Experiment results is shown in Table
4 (the results are averaged over 4 domains). We
can see that our unsupervised approach is roughly
as good as the supervised MNB with 10% labeled
data. Our unsupervised approach is also slightly
better than L-Kmeans with 15% labeled data. This
result further demonstrates the effectiveness of our
model.
</bodyText>
<subsectionHeader confidence="0.991639">
3.3.3 Influence of parameters
</subsectionHeader>
<bodyText confidence="0.997209571428571">
We vary the confidence level from 90% to 99.9%
to see how it impacts on the performance of SDC-
MNB. The results are presented in Fig. 5 (the re-
sults are averaged over 4 domains). We can see
that the performance of clustering is fairly stable
when changing the confidence level, which im-
plies the robustness of our model.
</bodyText>
<figureCaption confidence="0.7213575">
Figure 5: Influence of the confidence level on
SDC-MNB.
</figureCaption>
<subsectionHeader confidence="0.993537">
3.3.4 Analysis of SDC-constraint
</subsectionHeader>
<bodyText confidence="0.999959846153846">
As mentioned in Section 2.2, SDC-constraint is
dependent on the number of observations. More
observations we get, more informative the con-
straint is, which means the constraint is tighter and
djk (see Eq.4) is smaller. For all k, we count how
many djk is less than 0.2 (and 1) on average for
each aspect phrase fj. djk is calculated with a
confidence level of 99%. The statistics of con-
straints is given in Table 5. We can see that the
cellphone domain has the most informative and
largest constraint set, that may explain why SDC-
MNB achieves the largest purity gain(over L-EM)
in cellphone domain.
</bodyText>
<table confidence="0.9977906">
#(djk &lt; 0.2) #(0.2 &lt; djk &lt; 1) purity gain
Camera 3.02 8.78 1.53%
Cellphone 17.29 30.5 15.99%
Laptop 4.6 13.22 6.58%
MP3MP4 6.1 10.7 13.82%
</table>
<tableCaption confidence="0.999406">
Table 5: Constraint statistics on different domains.
</tableCaption>
<sectionHeader confidence="0.999793" genericHeader="evaluation">
4 Related Work
</sectionHeader>
<bodyText confidence="0.999916888888889">
Our work is related to two important research
topics: aspect-level sentiment analysis, and
constraint-driven learning. For aspect-level senti-
ment analysis, aspect extraction and clustering are
key tasks. For constraint-driven learning, a variety
of frameworks and models for sentiment analysis
have been studied extensively.
There have been many studies on clustering
aspect-related phrases. Most existing studies are
</bodyText>
<page confidence="0.971011">
1621
</page>
<bodyText confidence="0.999972294871795">
based on context information. Some works also
encoded lexical similarity and synonyms as prior
knowledge. Carenini et al. (2005) proposed a
method that was based on several similarity met-
rics involving string similarity, synonyms, and lex-
ical distances defined with WordNet. Guo et al.
(2009) proposed a multi-level latent semantic as-
sociation model to capture expression-level and
context-level topic structure. Zhai et al. (2010)
proposed an EM-based semi-supervised learning
method to group aspect expressions into user-
specified aspects. They employed lexical knowl-
edge to provide a better initialization for EM. In
Zhai et al. (2011a), an EM-based unsupervised
version was proposed. The so-called L-EM model
first generated softly labeled data by grouping fea-
ture expressions that share words in common, and
then merged the groups by lexical similarity. Zhai
et al. (2011b) proposed a LDA-based method
that incorporates must-link and cannot-link con-
straints.
Another line of work aimed to extract and clus-
ter aspect words simultaneously using topic mod-
eling. Titov and McDonald (2008) proposed the
multi-grain topic models to discover global and
local aspects. Branavan et al. (2008) proposed
a method which first clustered the key-phrases
in Pros and Cons into some aspect categories
based on distributional similarity, then built a topic
model modeling the topics or aspects. Zhao et al.
(2010) proposed the MaxEnt-LDA (a Maximum
Entropy and LDA combination) hybrid model to
jointly discover both aspect words and aspect-
specific opinion words, which can leverage syn-
tactic features to separate aspects and sentiment
words. Mukherjee and Liu (2012) proposed a
semi-supervised topic model which used user-
provided seeds to discover aspects. Chen et al.
(2013) proposed a knowledge-based topic model
to incorporate must-link and cannot-link informa-
tion. Their model can adjust topic numbers auto-
matically by leveraging cannot-link.
Our work is also related to general constraint-
driven(or knowledge-driven) learning models.
Several general frameworks have been proposed to
fully utilize various prior knowledge in learning.
Constraint-driven learning (Chang et al., 2008)
(CODL) is an EM-like algorithm that incorpo-
rates per-instance constraints into semi-supervised
learning. Posterior regularization (Graca et al.,
2007) (PR) is a modified EM algorithm in which
the E-step is replaced by the projection of the
model posterior distribution onto the set of dis-
tributions that satisfy auxiliary expectation con-
straints. Generalized expectation criteria (Druck
et al., 2008) (GE) is a framework for incorporating
preferences about model expectations into param-
eter estimation objective functions. Liang et al.
(2009) developed a Bayesian decision-theoretic
framework to learn an exponential family model
using general measurements on the unlabeled data.
In this paper, we model our problem in the frame-
work of posterior regularization.
Many works promoted the performance of sen-
timent analysis by incorporating prior knowledge
as weak supervision. Li and Zhang (2009) in-
jected lexical prior knowledge to non-negative ma-
trix tri-factorization. Shen and Li (2011) further
extended the matrix factorization framework to
model dual supervision from both document and
word labels. Vikas Sindhwani (2008) proposed a
general framework for incorporating lexical infor-
mation as well as unlabeled data within standard
regularized least squares for sentiment prediction
tasks. Fang (2013)proposed a structural learning
model with a handful set of aspect signature terms
that are encoded as weak supervision to extract la-
tent sentiment explanations.
</bodyText>
<sectionHeader confidence="0.999568" genericHeader="conclusions">
5 Conclusions
</sectionHeader>
<bodyText confidence="0.999923083333333">
Aspect finding and clustering is an important task
for aspect-level sentiment analysis. In order to
cluster aspect-related phrases, this paper has ex-
plored a novel concept, sentiment distribution con-
sistency. We formalize the concept as soft con-
straint, integrate the constraint with a context-
based probabilistic model, and solve the problem
in the posterior regularization framework. The
proposed model is also designed to be robust with
both sufficient and insufficient observations. Ex-
periments show that our approach outperforms
state-of-the-art baselines consistently.
</bodyText>
<sectionHeader confidence="0.998844" genericHeader="acknowledgments">
Acknowledgments
</sectionHeader>
<bodyText confidence="0.999451428571429">
This work was partly supported by the following
grants from: the National Basic Research Program
(973 Program) under grant No.2012CB316301
and 2013CB329403, the National Science Foun-
dation of China project under grant No.61332007
and No. 61272227, and the Beijing Higher Educa-
tion Young Elite Teacher Project.
</bodyText>
<page confidence="0.992199">
1622
</page>
<sectionHeader confidence="0.995866" genericHeader="references">
References
</sectionHeader>
<reference confidence="0.999940486486486">
David M. Blei, Andrew Y. Ng, and Michael I. Jordan.
2003. Latent dirichlet allocation. J. Mach. Learn.
Res., 3:993–1022, March.
Stephen Boyd and Lieven Vandenberghe. 2004. Con-
vex Optimization. Cambridge University Press, New
York, NY, USA.
S. R. K. Branavan, Harr Chen, Jacob Eisenstein, and
Regina Barzilay. 2008. Learning document-level
semantic properties from free-text annotations. In
Proceedings of the Association for Computational
Linguistics (ACL).
Giuseppe Carenini, Raymond T. Ng, and Ed Zwart.
2005. Extracting knowledge from evaluative text.
In Proceedings of the 3rd International Conference
on Knowledge Capture, K-CAP ’05, pages 11–18,
New York, NY, USA. ACM.
Ming-Wei Chang, Lev Ratinov, Nicholas Rizzolo, and
Dan Roth. 2008. Learning and inference with
constraints. In Proceedings of the 23rd National
Conference on Artificial Intelligence - Volume 3,
AAAI’08, pages 1513–1518. AAAI Press.
Zhiyuan Chen, Arjun Mukherjee, Bing Liu, Meichun
Hsu, Mal Castellanos, and Riddhiman Ghosh. 2013.
Exploiting domain knowledge in aspect extraction.
In EMNLP, pages 1655–1667. ACL.
Gregory Druck, Gideon Mann, and Andrew McCal-
lum. 2008. Learning from labeled features using
generalized expectation criteria. In Proceedings of
the 31st Annual International ACM SIGIR Confer-
ence on Research and Development in Information
Retrieval, SIGIR ’08, pages 595–602, New York,
NY, USA. ACM.
Lei Fang, Minlie Huang, and Xiaoyan Zhu. 2013. Ex-
ploring weakly supervised latent sentiment expla-
nations for aspect-level review analysis. In Qi He,
Arun Iyengar, Wolfgang Nejdl, Jian Pei, and Rajeev
Rastogi, editors, CIKM, pages 1057–1066. ACM.
Joao V. Graca, Lf Inesc-id, Kuzman Ganchev, Ben
Taskar, Joo V. Graa, L F Inesc-id, Kuzman Ganchev,
and Ben Taskar. 2007. Expectation maximization
and posterior constraints. In In Advances in NIPS,
pages 569–576.
Honglei Guo, Huijia Zhu, Zhili Guo, XiaoXun Zhang,
and Zhong Su. 2009. Product feature categorization
with multilevel latent semantic association. In Pro-
ceedings of the 18th ACM Conference on Informa-
tion and Knowledge Management, CIKM ’09, pages
1087–1096, New York, NY, USA. ACM.
Tao Li, Yi Zhang, and Vikas Sindhwani. 2009. A non-
negative matrix tri-factorization approach to senti-
ment classification with lexical prior knowledge. In
Proceedings of the Joint Conference of the 47th An-
nual Meeting of the ACL and the 4th International
Joint Conference on Natural Language Processing
of theAFNLP: Volume 1 - Volume 1, ACL ’09, pages
244–252, Stroudsburg, PA, USA. Association for
Computational Linguistics.
Percy Liang, Michael I. Jordan, and Dan Klein. 2009.
Learning from measurements in exponential fami-
lies. In Proceedings of the 26th Annual Interna-
tional Conference on Machine Learning, ICML ’09,
pages 641–648, New York, NY, USA. ACM.
Arjun Mukherjee and Bing Liu. 2012. Aspect extrac-
tion through semi-supervised modeling. In Proceed-
ings of the 50th Annual Meeting of the Association
for Computational Linguistics: Long Papers - Vol-
ume 1, ACL ’12, pages 339–348, Stroudsburg, PA,
USA. Association for Computational Linguistics.
Chao Shen and Tao Li. 2011. A non-negative matrix
factorization based approach for active dual super-
vision from document and word labels. In Proceed-
ings of the Conference on Empirical Methods in Nat-
ural Language Processing, EMNLP ’11, pages 949–
958, Stroudsburg, PA, USA. Association for Com-
putational Linguistics.
Vikas Sindhwani and Prem Melville. 2008.
Document-word co-regularization for semi-
supervised sentiment analysis. In ICDM, pages
1025–1030. IEEE Computer Society.
Ivan Titov and Ryan McDonald. 2008. Modeling on-
line reviews with multi-grain topic models. In Pro-
ceedings of the 17th International Conference on
World Wide Web, WWW ’08, pages 111–120, New
York, NY, USA. ACM.
Zhongwu Zhai, Bing Liu, Hua Xu, and Peifa Jia. 2010.
Grouping product features using semi-supervised
learning with soft-constraints. In Proceedings of
the 23rd International Conference on Computa-
tional Linguistics, COLING ’10, pages 1272–1280,
Stroudsburg, PA, USA. Association for Computa-
tional Linguistics.
Zhongwu Zhai, Bing Liu, Hua Xu, and Peifa Jia.
2011a. Clustering product features for opinion min-
ing. In Proceedings of the Fourth ACM Interna-
tional Conference on Web Search and Data Mining,
WSDM ’11, pages 347–354, New York, NY, USA.
ACM.
Zhongwu Zhai, Bing Liu, Hua Xu, and Peifa Jia.
2011b. Constrained lda for grouping product fea-
tures in opinion mining. In Proceedings of the 15th
Pacific-Asia Conference on Advances in Knowl-
edge Discovery and Data Mining - Volume Part
I, PAKDD’11, pages 448–459, Berlin, Heidelberg.
Springer-Verlag.
Wayne X. Zhao, Jing Jiang, Hongfei Yan, and Xiaom-
ing Li. 2010. Jointly modeling aspects and opin-
ions with a MaxEnt-LDA hybrid. In Proceedings of
the 2010 Conference on Empirical Methods in Nat-
ural Language Processing, EMNLP ’10, pages 56–
65, Stroudsburg, PA, USA. Association for Compu-
tational Linguistics.
</reference>
<page confidence="0.955274">
1623
</page>
</variant>
</algorithm>
<algorithm name="ParsHed" version="110505">
<variant no="0" confidence="0.317586">
<title confidence="0.983552">Clustering Aspect-related Phrases by Leveraging Sentiment Distribution Consistency</title>
<author confidence="0.994973">Li Zhao</author>
<author confidence="0.994973">Minlie Huang</author>
<author confidence="0.994973">Haiqiang Chen</author>
<author confidence="0.994973">Junjun Cheng</author>
<author confidence="0.994973">Xiaoyan</author>
<affiliation confidence="0.92184325">State Key Laboratory of Intelligent Technology and National Laboratory for Information Science and Dept. of Computer Science and Technology, Tsinghua University, Beijing, PR *China Information Technology Security Evaluation</affiliation>
<note confidence="0.520503">zhaoli19881113@126.com aihuang@tsinghua.edu.cn</note>
<abstract confidence="0.996114545454546">Clustering aspect-related phrases in terms of product’s property is a precursor process to aspect-level sentiment analysis which is a central task in sentiment analysis. Most of existing methods for addressing this problem are context-based models which assume that domain synonymous phrases share similar co-occurrence contexts. In this paper, we explore a novel distribution which states that different phrases (e.g. “price”, “money”, “worth”, and “cost”) of the same aspect tend to have consistent sentiment distribution. Through formaldistribution consistency soft constraint, we propose a novel unsupervised model in the framework of Posterior Regularization (PR) to cluster aspectrelated phrases. Experiments demonstrate that our approach outperforms baselines remarkably.</abstract>
</variant>
</algorithm>
<algorithm name="ParsCit" version="110505">
<citationList>
<citation valid="true">
<authors>
<author>David M Blei</author>
<author>Andrew Y Ng</author>
<author>Michael I Jordan</author>
</authors>
<title>Latent dirichlet allocation.</title>
<date>2003</date>
<journal>J. Mach. Learn. Res.,</journal>
<pages>3--993</pages>
<contexts>
<context position="24776" citStr="Blei et al., 2003" startWordPosition="4159" endWordPosition="4162"> several existing unsupervised methods. Some of the methods augmented unsupervised models by incorporating lexical similarity and other domain knowledge. All of them are context-based models.2 We list these models as follows. • Kmeans: Kmeans is the most popular clustering algorithm. Here we use the context distributional similarity (cosine similarity) as the similarity measure. • L-EM: This is a state-of-the-art unsupervised method for clustering aspect phrases (Zhai et al., 2011a). L-EM employed lexical knowledge to provide a better initialization for EM. • LDA: LDA is a popular topic model(Blei et al., 2003). Given a set of documents, it outputs groups of terms of different topics. In our case, each aspect phrase is processed as a term. 3 Each sentence in a review is considered as a document. Each aspect is considered as a topic. In LDA, a term may belong to more than one topic/group, but we take the topic/group with the maximum probability. 2In our method, we collect context document for each aspect phrase. This process is conducted for L-EM and Kmeans. But for LDA and Constraint-LDA, we take each sentence of reviews as a document. This setting for the LDA baselines is adapted from previous work</context>
</contexts>
<marker>Blei, Ng, Jordan, 2003</marker>
<rawString>David M. Blei, Andrew Y. Ng, and Michael I. Jordan. 2003. Latent dirichlet allocation. J. Mach. Learn. Res., 3:993–1022, March.</rawString>
</citation>
<citation valid="true">
<authors>
<author>Stephen Boyd</author>
<author>Lieven Vandenberghe</author>
</authors>
<title>Convex Optimization.</title>
<date>2004</date>
<publisher>Cambridge University Press,</publisher>
<location>New York, NY, USA.</location>
<contexts>
<context position="18301" citStr="Boyd and Vandenberghe, 2004" startWordPosition="3021" endWordPosition="3024"> use the above posterior distribution to update the sentiment parameter for each aspect by Eq. 10. The projected posterior distribution q is calculated by q = argmin KL(q(Y )||pθ(Y |D)) (12) q∈Q For each instance, there are |A |* |P |constraints. However, we can prune a large number of useless constraints derived from limited observations. All constraints with djk &gt; 1 can be pruned, due to the fact that the parameter uik, sok is within [0,1], and the difference can not be larger than 1. This optimization problem in Eq. 12 is easily solved via the dual form by the projected gradient algorithm (Boyd and Vandenberghe, 2004): ( ∑|A |∑|P | max − λ≥0 i k u k 3jk a i | i − |}−ϵ∥ ∥ l /(13) bution q by 1 pθ (ai |dj)ex p { −∑ |P | λ ik |u ik − §jk |} (14) k=1 where Z is the normalization factor. Note that sentiment distribution consistency is actually modeled as instance-level constraint here, which makes it very efficient to solve. In M-step, the projected posteriors q(Y ) are then used to compute sufficient statistics and update the models parameters B. Given the projected posteriors q(Y ), the parameters can be updated by Eq. 15,16. p(ai) = 1 + ∑|D| |A |+ |D| j=1 q(yj = ai) (15) ∑ |P | k=1 X where E controls the sla</context>
</contexts>
<marker>Boyd, Vandenberghe, 2004</marker>
<rawString>Stephen Boyd and Lieven Vandenberghe. 2004. Convex Optimization. Cambridge University Press, New York, NY, USA.</rawString>
</citation>
<citation valid="true">
<authors>
<author>S R K Branavan</author>
<author>Harr Chen</author>
<author>Jacob Eisenstein</author>
<author>Regina Barzilay</author>
</authors>
<title>Learning document-level semantic properties from free-text annotations.</title>
<date>2008</date>
<booktitle>In Proceedings of the Association for Computational Linguistics (ACL).</booktitle>
<contexts>
<context position="31946" citStr="Branavan et al. (2008)" startWordPosition="5321" endWordPosition="5324">owledge to provide a better initialization for EM. In Zhai et al. (2011a), an EM-based unsupervised version was proposed. The so-called L-EM model first generated softly labeled data by grouping feature expressions that share words in common, and then merged the groups by lexical similarity. Zhai et al. (2011b) proposed a LDA-based method that incorporates must-link and cannot-link constraints. Another line of work aimed to extract and cluster aspect words simultaneously using topic modeling. Titov and McDonald (2008) proposed the multi-grain topic models to discover global and local aspects. Branavan et al. (2008) proposed a method which first clustered the key-phrases in Pros and Cons into some aspect categories based on distributional similarity, then built a topic model modeling the topics or aspects. Zhao et al. (2010) proposed the MaxEnt-LDA (a Maximum Entropy and LDA combination) hybrid model to jointly discover both aspect words and aspectspecific opinion words, which can leverage syntactic features to separate aspects and sentiment words. Mukherjee and Liu (2012) proposed a semi-supervised topic model which used userprovided seeds to discover aspects. Chen et al. (2013) proposed a knowledge-bas</context>
</contexts>
<marker>Branavan, Chen, Eisenstein, Barzilay, 2008</marker>
<rawString>S. R. K. Branavan, Harr Chen, Jacob Eisenstein, and Regina Barzilay. 2008. Learning document-level semantic properties from free-text annotations. In Proceedings of the Association for Computational Linguistics (ACL).</rawString>
</citation>
<citation valid="true">
<authors>
<author>Giuseppe Carenini</author>
<author>Raymond T Ng</author>
<author>Ed Zwart</author>
</authors>
<title>Extracting knowledge from evaluative text.</title>
<date>2005</date>
<booktitle>In Proceedings of the 3rd International Conference on Knowledge Capture, K-CAP ’05,</booktitle>
<pages>11--18</pages>
<publisher>ACM.</publisher>
<location>New York, NY, USA.</location>
<contexts>
<context position="30887" citStr="Carenini et al. (2005)" startWordPosition="5161" endWordPosition="5164">2% Table 5: Constraint statistics on different domains. 4 Related Work Our work is related to two important research topics: aspect-level sentiment analysis, and constraint-driven learning. For aspect-level sentiment analysis, aspect extraction and clustering are key tasks. For constraint-driven learning, a variety of frameworks and models for sentiment analysis have been studied extensively. There have been many studies on clustering aspect-related phrases. Most existing studies are 1621 based on context information. Some works also encoded lexical similarity and synonyms as prior knowledge. Carenini et al. (2005) proposed a method that was based on several similarity metrics involving string similarity, synonyms, and lexical distances defined with WordNet. Guo et al. (2009) proposed a multi-level latent semantic association model to capture expression-level and context-level topic structure. Zhai et al. (2010) proposed an EM-based semi-supervised learning method to group aspect expressions into userspecified aspects. They employed lexical knowledge to provide a better initialization for EM. In Zhai et al. (2011a), an EM-based unsupervised version was proposed. The so-called L-EM model first generated </context>
</contexts>
<marker>Carenini, Ng, Zwart, 2005</marker>
<rawString>Giuseppe Carenini, Raymond T. Ng, and Ed Zwart. 2005. Extracting knowledge from evaluative text. In Proceedings of the 3rd International Conference on Knowledge Capture, K-CAP ’05, pages 11–18, New York, NY, USA. ACM.</rawString>
</citation>
<citation valid="true">
<authors>
<author>Ming-Wei Chang</author>
<author>Lev Ratinov</author>
<author>Nicholas Rizzolo</author>
<author>Dan Roth</author>
</authors>
<title>Learning and inference with constraints.</title>
<date>2008</date>
<booktitle>In Proceedings of the 23rd National Conference on Artificial Intelligence - Volume 3, AAAI’08,</booktitle>
<pages>1513--1518</pages>
<publisher>AAAI Press.</publisher>
<contexts>
<context position="32931" citStr="Chang et al., 2008" startWordPosition="5465" endWordPosition="5468">ch can leverage syntactic features to separate aspects and sentiment words. Mukherjee and Liu (2012) proposed a semi-supervised topic model which used userprovided seeds to discover aspects. Chen et al. (2013) proposed a knowledge-based topic model to incorporate must-link and cannot-link information. Their model can adjust topic numbers automatically by leveraging cannot-link. Our work is also related to general constraintdriven(or knowledge-driven) learning models. Several general frameworks have been proposed to fully utilize various prior knowledge in learning. Constraint-driven learning (Chang et al., 2008) (CODL) is an EM-like algorithm that incorporates per-instance constraints into semi-supervised learning. Posterior regularization (Graca et al., 2007) (PR) is a modified EM algorithm in which the E-step is replaced by the projection of the model posterior distribution onto the set of distributions that satisfy auxiliary expectation constraints. Generalized expectation criteria (Druck et al., 2008) (GE) is a framework for incorporating preferences about model expectations into parameter estimation objective functions. Liang et al. (2009) developed a Bayesian decision-theoretic framework to lea</context>
</contexts>
<marker>Chang, Ratinov, Rizzolo, Roth, 2008</marker>
<rawString>Ming-Wei Chang, Lev Ratinov, Nicholas Rizzolo, and Dan Roth. 2008. Learning and inference with constraints. In Proceedings of the 23rd National Conference on Artificial Intelligence - Volume 3, AAAI’08, pages 1513–1518. AAAI Press.</rawString>
</citation>
<citation valid="true">
<authors>
<author>Zhiyuan Chen</author>
<author>Arjun Mukherjee</author>
<author>Bing Liu</author>
<author>Meichun Hsu</author>
<author>Mal Castellanos</author>
<author>Riddhiman Ghosh</author>
</authors>
<title>Exploiting domain knowledge in aspect extraction.</title>
<date>2013</date>
<booktitle>In EMNLP,</booktitle>
<pages>1655--1667</pages>
<publisher>ACL.</publisher>
<contexts>
<context position="1721" citStr="Chen et al., 2013" startWordPosition="234" endWordPosition="237">ovel unsupervised model in the framework of Posterior Regularization (PR) to cluster aspectrelated phrases. Experiments demonstrate that our approach outperforms baselines remarkably. 1 Introduction Aspect-level sentiment analysis has become a central task in sentiment analysis because it can aggregate various opinions according to a product’s properties, and provide much detailed, complete, and in-depth summaries of a large number of reviews. Aspect finding and clustering, a precursor process of aspect-level sentiment analysis, has attracted more and more attentions (Mukherjee and Liu, 2012; Chen et al., 2013; Zhai et al., 2011a; Zhai et al., 2010). Aspect finding and clustering has never been a trivial task. People often use different words or phrases to refer to the same product property (also called product aspect or feature in the literature). Some terms are lexically dissimilar while semantically close, which makes the task more challenging. For example, “price”, “money” , “worth” and “cost” all refer to the aspect “price” in reviews. In order to present aspect-specific summaries of opinions, we first of all, have to cluster different aspect-related phrases. It is expensive and timeconsuming </context>
<context position="3330" citStr="Chen et al., 2013" startWordPosition="490" endWordPosition="493">ditional assumption, we develop a new angle to address the problem, which is based on sentiment distribution consistency assumption that different phrases of the same aspect should have consistent sentiment distribution, which will be detailed soon later. Figure 1: A semi-structured Review. This new angle is inspired by this simple observation (as illustrated in Fig. 1): two phrases within the same cluster are not likely to be simultaneously placed in Pros and Cons of the same review. A straightforward way to use this information is to formulate cannot-link knowledge in clustering algorithms (Chen et al., 2013; Zhai et al., 2011b). However, we have a particularly different manner to leverage the knowledge. Due to the availability of large-scale semistructured customer reviews (as exemplified in Fig. 1) that are supported by many web sites, we can easily get the estimation of sentiment distribution for each aspect phrase by simply counting how many times a phrase appears in Pros and 1614 Proceedings of the 2014 Conference on Empirical Methods in Natural Language Processing (EMNLP), pages 1614–1623, October 25-29, 2014, Doha, Qatar. c�2014 Association for Computational Linguistics Cons respectively. </context>
<context position="32521" citStr="Chen et al. (2013)" startWordPosition="5410" endWordPosition="5413">l and local aspects. Branavan et al. (2008) proposed a method which first clustered the key-phrases in Pros and Cons into some aspect categories based on distributional similarity, then built a topic model modeling the topics or aspects. Zhao et al. (2010) proposed the MaxEnt-LDA (a Maximum Entropy and LDA combination) hybrid model to jointly discover both aspect words and aspectspecific opinion words, which can leverage syntactic features to separate aspects and sentiment words. Mukherjee and Liu (2012) proposed a semi-supervised topic model which used userprovided seeds to discover aspects. Chen et al. (2013) proposed a knowledge-based topic model to incorporate must-link and cannot-link information. Their model can adjust topic numbers automatically by leveraging cannot-link. Our work is also related to general constraintdriven(or knowledge-driven) learning models. Several general frameworks have been proposed to fully utilize various prior knowledge in learning. Constraint-driven learning (Chang et al., 2008) (CODL) is an EM-like algorithm that incorporates per-instance constraints into semi-supervised learning. Posterior regularization (Graca et al., 2007) (PR) is a modified EM algorithm in whi</context>
</contexts>
<marker>Chen, Mukherjee, Liu, Hsu, Castellanos, Ghosh, 2013</marker>
<rawString>Zhiyuan Chen, Arjun Mukherjee, Bing Liu, Meichun Hsu, Mal Castellanos, and Riddhiman Ghosh. 2013. Exploiting domain knowledge in aspect extraction. In EMNLP, pages 1655–1667. ACL.</rawString>
</citation>
<citation valid="true">
<authors>
<author>Gregory Druck</author>
<author>Gideon Mann</author>
<author>Andrew McCallum</author>
</authors>
<title>Learning from labeled features using generalized expectation criteria.</title>
<date>2008</date>
<booktitle>In Proceedings of the 31st Annual International ACM SIGIR Conference on Research and Development in Information Retrieval, SIGIR ’08,</booktitle>
<pages>595--602</pages>
<publisher>ACM.</publisher>
<location>New York, NY, USA.</location>
<contexts>
<context position="33332" citStr="Druck et al., 2008" startWordPosition="5523" endWordPosition="5526">ated to general constraintdriven(or knowledge-driven) learning models. Several general frameworks have been proposed to fully utilize various prior knowledge in learning. Constraint-driven learning (Chang et al., 2008) (CODL) is an EM-like algorithm that incorporates per-instance constraints into semi-supervised learning. Posterior regularization (Graca et al., 2007) (PR) is a modified EM algorithm in which the E-step is replaced by the projection of the model posterior distribution onto the set of distributions that satisfy auxiliary expectation constraints. Generalized expectation criteria (Druck et al., 2008) (GE) is a framework for incorporating preferences about model expectations into parameter estimation objective functions. Liang et al. (2009) developed a Bayesian decision-theoretic framework to learn an exponential family model using general measurements on the unlabeled data. In this paper, we model our problem in the framework of posterior regularization. Many works promoted the performance of sentiment analysis by incorporating prior knowledge as weak supervision. Li and Zhang (2009) injected lexical prior knowledge to non-negative matrix tri-factorization. Shen and Li (2011) further exte</context>
</contexts>
<marker>Druck, Mann, McCallum, 2008</marker>
<rawString>Gregory Druck, Gideon Mann, and Andrew McCallum. 2008. Learning from labeled features using generalized expectation criteria. In Proceedings of the 31st Annual International ACM SIGIR Conference on Research and Development in Information Retrieval, SIGIR ’08, pages 595–602, New York, NY, USA. ACM.</rawString>
</citation>
<citation valid="true">
<authors>
<author>Lei Fang</author>
<author>Minlie Huang</author>
<author>Xiaoyan Zhu</author>
</authors>
<title>Exploring weakly supervised latent sentiment explanations for aspect-level review analysis.</title>
<date>2013</date>
<pages>1057--1066</pages>
<editor>In Qi He, Arun Iyengar, Wolfgang Nejdl, Jian Pei, and Rajeev Rastogi, editors, CIKM,</editor>
<publisher>ACM.</publisher>
<marker>Fang, Huang, Zhu, 2013</marker>
<rawString>Lei Fang, Minlie Huang, and Xiaoyan Zhu. 2013. Exploring weakly supervised latent sentiment explanations for aspect-level review analysis. In Qi He, Arun Iyengar, Wolfgang Nejdl, Jian Pei, and Rajeev Rastogi, editors, CIKM, pages 1057–1066. ACM.</rawString>
</citation>
<citation valid="true">
<authors>
<author>Joao V Graca</author>
<author>Lf Inesc-id</author>
<author>Kuzman Ganchev</author>
<author>Ben Taskar</author>
<author>Joo V Graa</author>
<author>L F Inesc-id</author>
<author>Kuzman Ganchev</author>
<author>Ben Taskar</author>
</authors>
<title>Expectation maximization and posterior constraints. In</title>
<date>2007</date>
<booktitle>In Advances in NIPS,</booktitle>
<pages>569--576</pages>
<contexts>
<context position="33082" citStr="Graca et al., 2007" startWordPosition="5484" endWordPosition="5487">userprovided seeds to discover aspects. Chen et al. (2013) proposed a knowledge-based topic model to incorporate must-link and cannot-link information. Their model can adjust topic numbers automatically by leveraging cannot-link. Our work is also related to general constraintdriven(or knowledge-driven) learning models. Several general frameworks have been proposed to fully utilize various prior knowledge in learning. Constraint-driven learning (Chang et al., 2008) (CODL) is an EM-like algorithm that incorporates per-instance constraints into semi-supervised learning. Posterior regularization (Graca et al., 2007) (PR) is a modified EM algorithm in which the E-step is replaced by the projection of the model posterior distribution onto the set of distributions that satisfy auxiliary expectation constraints. Generalized expectation criteria (Druck et al., 2008) (GE) is a framework for incorporating preferences about model expectations into parameter estimation objective functions. Liang et al. (2009) developed a Bayesian decision-theoretic framework to learn an exponential family model using general measurements on the unlabeled data. In this paper, we model our problem in the framework of posterior regu</context>
</contexts>
<marker>Graca, Inesc-id, Ganchev, Taskar, Graa, Inesc-id, Ganchev, Taskar, 2007</marker>
<rawString>Joao V. Graca, Lf Inesc-id, Kuzman Ganchev, Ben Taskar, Joo V. Graa, L F Inesc-id, Kuzman Ganchev, and Ben Taskar. 2007. Expectation maximization and posterior constraints. In In Advances in NIPS, pages 569–576.</rawString>
</citation>
<citation valid="true">
<authors>
<author>Honglei Guo</author>
<author>Huijia Zhu</author>
<author>Zhili Guo</author>
<author>XiaoXun Zhang</author>
<author>Zhong Su</author>
</authors>
<title>Product feature categorization with multilevel latent semantic association.</title>
<date>2009</date>
<booktitle>In Proceedings of the 18th ACM Conference on Information and Knowledge Management, CIKM ’09,</booktitle>
<pages>1087--1096</pages>
<publisher>ACM.</publisher>
<location>New York, NY, USA.</location>
<contexts>
<context position="31051" citStr="Guo et al. (2009)" startWordPosition="5187" endWordPosition="5190">int-driven learning. For aspect-level sentiment analysis, aspect extraction and clustering are key tasks. For constraint-driven learning, a variety of frameworks and models for sentiment analysis have been studied extensively. There have been many studies on clustering aspect-related phrases. Most existing studies are 1621 based on context information. Some works also encoded lexical similarity and synonyms as prior knowledge. Carenini et al. (2005) proposed a method that was based on several similarity metrics involving string similarity, synonyms, and lexical distances defined with WordNet. Guo et al. (2009) proposed a multi-level latent semantic association model to capture expression-level and context-level topic structure. Zhai et al. (2010) proposed an EM-based semi-supervised learning method to group aspect expressions into userspecified aspects. They employed lexical knowledge to provide a better initialization for EM. In Zhai et al. (2011a), an EM-based unsupervised version was proposed. The so-called L-EM model first generated softly labeled data by grouping feature expressions that share words in common, and then merged the groups by lexical similarity. Zhai et al. (2011b) proposed a LDA</context>
</contexts>
<marker>Guo, Zhu, Guo, Zhang, Su, 2009</marker>
<rawString>Honglei Guo, Huijia Zhu, Zhili Guo, XiaoXun Zhang, and Zhong Su. 2009. Product feature categorization with multilevel latent semantic association. In Proceedings of the 18th ACM Conference on Information and Knowledge Management, CIKM ’09, pages 1087–1096, New York, NY, USA. ACM.</rawString>
</citation>
<citation valid="false">
<authors>
<author>Tao Li</author>
<author>Yi Zhang</author>
<author>Vikas Sindhwani</author>
</authors>
<title>A nonnegative matrix tri-factorization approach to sentiment classification with lexical prior knowledge.</title>
<date>2009</date>
<booktitle>In Proceedings of the Joint Conference of the 47th Annual Meeting of the ACL and the 4th International Joint Conference on Natural Language Processing of theAFNLP: Volume 1 - Volume 1, ACL ’09,</booktitle>
<pages>244--252</pages>
<publisher>Association</publisher>
<institution>for Computational Linguistics.</institution>
<location>Stroudsburg, PA, USA.</location>
<marker>Li, Zhang, Sindhwani, 2009</marker>
<rawString>Tao Li, Yi Zhang, and Vikas Sindhwani. 2009. A nonnegative matrix tri-factorization approach to sentiment classification with lexical prior knowledge. In Proceedings of the Joint Conference of the 47th Annual Meeting of the ACL and the 4th International Joint Conference on Natural Language Processing of theAFNLP: Volume 1 - Volume 1, ACL ’09, pages 244–252, Stroudsburg, PA, USA. Association for Computational Linguistics.</rawString>
</citation>
<citation valid="true">
<authors>
<author>Percy Liang</author>
<author>Michael I Jordan</author>
<author>Dan Klein</author>
</authors>
<title>Learning from measurements in exponential families.</title>
<date>2009</date>
<booktitle>In Proceedings of the 26th Annual International Conference on Machine Learning, ICML ’09,</booktitle>
<pages>641--648</pages>
<publisher>ACM.</publisher>
<location>New York, NY, USA.</location>
<contexts>
<context position="33474" citStr="Liang et al. (2009)" startWordPosition="5543" endWordPosition="5546">prior knowledge in learning. Constraint-driven learning (Chang et al., 2008) (CODL) is an EM-like algorithm that incorporates per-instance constraints into semi-supervised learning. Posterior regularization (Graca et al., 2007) (PR) is a modified EM algorithm in which the E-step is replaced by the projection of the model posterior distribution onto the set of distributions that satisfy auxiliary expectation constraints. Generalized expectation criteria (Druck et al., 2008) (GE) is a framework for incorporating preferences about model expectations into parameter estimation objective functions. Liang et al. (2009) developed a Bayesian decision-theoretic framework to learn an exponential family model using general measurements on the unlabeled data. In this paper, we model our problem in the framework of posterior regularization. Many works promoted the performance of sentiment analysis by incorporating prior knowledge as weak supervision. Li and Zhang (2009) injected lexical prior knowledge to non-negative matrix tri-factorization. Shen and Li (2011) further extended the matrix factorization framework to model dual supervision from both document and word labels. Vikas Sindhwani (2008) proposed a genera</context>
</contexts>
<marker>Liang, Jordan, Klein, 2009</marker>
<rawString>Percy Liang, Michael I. Jordan, and Dan Klein. 2009. Learning from measurements in exponential families. In Proceedings of the 26th Annual International Conference on Machine Learning, ICML ’09, pages 641–648, New York, NY, USA. ACM.</rawString>
</citation>
<citation valid="true">
<authors>
<author>Arjun Mukherjee</author>
<author>Bing Liu</author>
</authors>
<title>Aspect extraction through semi-supervised modeling.</title>
<date>2012</date>
<booktitle>In Proceedings of the 50th Annual Meeting of the Association for Computational Linguistics: Long Papers - Volume 1, ACL ’12,</booktitle>
<pages>339--348</pages>
<publisher>Association</publisher>
<institution>for Computational Linguistics.</institution>
<location>Stroudsburg, PA, USA.</location>
<contexts>
<context position="1702" citStr="Mukherjee and Liu, 2012" startWordPosition="230" endWordPosition="233">onstraint, we propose a novel unsupervised model in the framework of Posterior Regularization (PR) to cluster aspectrelated phrases. Experiments demonstrate that our approach outperforms baselines remarkably. 1 Introduction Aspect-level sentiment analysis has become a central task in sentiment analysis because it can aggregate various opinions according to a product’s properties, and provide much detailed, complete, and in-depth summaries of a large number of reviews. Aspect finding and clustering, a precursor process of aspect-level sentiment analysis, has attracted more and more attentions (Mukherjee and Liu, 2012; Chen et al., 2013; Zhai et al., 2011a; Zhai et al., 2010). Aspect finding and clustering has never been a trivial task. People often use different words or phrases to refer to the same product property (also called product aspect or feature in the literature). Some terms are lexically dissimilar while semantically close, which makes the task more challenging. For example, “price”, “money” , “worth” and “cost” all refer to the aspect “price” in reviews. In order to present aspect-specific summaries of opinions, we first of all, have to cluster different aspect-related phrases. It is expensive</context>
<context position="32412" citStr="Mukherjee and Liu (2012)" startWordPosition="5393" endWordPosition="5396">ultaneously using topic modeling. Titov and McDonald (2008) proposed the multi-grain topic models to discover global and local aspects. Branavan et al. (2008) proposed a method which first clustered the key-phrases in Pros and Cons into some aspect categories based on distributional similarity, then built a topic model modeling the topics or aspects. Zhao et al. (2010) proposed the MaxEnt-LDA (a Maximum Entropy and LDA combination) hybrid model to jointly discover both aspect words and aspectspecific opinion words, which can leverage syntactic features to separate aspects and sentiment words. Mukherjee and Liu (2012) proposed a semi-supervised topic model which used userprovided seeds to discover aspects. Chen et al. (2013) proposed a knowledge-based topic model to incorporate must-link and cannot-link information. Their model can adjust topic numbers automatically by leveraging cannot-link. Our work is also related to general constraintdriven(or knowledge-driven) learning models. Several general frameworks have been proposed to fully utilize various prior knowledge in learning. Constraint-driven learning (Chang et al., 2008) (CODL) is an EM-like algorithm that incorporates per-instance constraints into s</context>
</contexts>
<marker>Mukherjee, Liu, 2012</marker>
<rawString>Arjun Mukherjee and Bing Liu. 2012. Aspect extraction through semi-supervised modeling. In Proceedings of the 50th Annual Meeting of the Association for Computational Linguistics: Long Papers - Volume 1, ACL ’12, pages 339–348, Stroudsburg, PA, USA. Association for Computational Linguistics.</rawString>
</citation>
<citation valid="true">
<authors>
<author>Chao Shen</author>
<author>Tao Li</author>
</authors>
<title>A non-negative matrix factorization based approach for active dual supervision from document and word labels.</title>
<date>2011</date>
<booktitle>In Proceedings of the Conference on Empirical Methods in Natural Language Processing, EMNLP ’11,</booktitle>
<pages>949--958</pages>
<publisher>Association</publisher>
<institution>for Computational Linguistics.</institution>
<location>Stroudsburg, PA, USA.</location>
<contexts>
<context position="33919" citStr="Shen and Li (2011)" startWordPosition="5609" endWordPosition="5612">n criteria (Druck et al., 2008) (GE) is a framework for incorporating preferences about model expectations into parameter estimation objective functions. Liang et al. (2009) developed a Bayesian decision-theoretic framework to learn an exponential family model using general measurements on the unlabeled data. In this paper, we model our problem in the framework of posterior regularization. Many works promoted the performance of sentiment analysis by incorporating prior knowledge as weak supervision. Li and Zhang (2009) injected lexical prior knowledge to non-negative matrix tri-factorization. Shen and Li (2011) further extended the matrix factorization framework to model dual supervision from both document and word labels. Vikas Sindhwani (2008) proposed a general framework for incorporating lexical information as well as unlabeled data within standard regularized least squares for sentiment prediction tasks. Fang (2013)proposed a structural learning model with a handful set of aspect signature terms that are encoded as weak supervision to extract latent sentiment explanations. 5 Conclusions Aspect finding and clustering is an important task for aspect-level sentiment analysis. In order to cluster a</context>
</contexts>
<marker>Shen, Li, 2011</marker>
<rawString>Chao Shen and Tao Li. 2011. A non-negative matrix factorization based approach for active dual supervision from document and word labels. In Proceedings of the Conference on Empirical Methods in Natural Language Processing, EMNLP ’11, pages 949– 958, Stroudsburg, PA, USA. Association for Computational Linguistics.</rawString>
</citation>
<citation valid="true">
<authors>
<author>Vikas Sindhwani</author>
<author>Prem Melville</author>
</authors>
<title>Document-word co-regularization for semisupervised sentiment analysis.</title>
<date>2008</date>
<booktitle>In ICDM,</booktitle>
<pages>1025--1030</pages>
<publisher>IEEE Computer Society.</publisher>
<marker>Sindhwani, Melville, 2008</marker>
<rawString>Vikas Sindhwani and Prem Melville. 2008. Document-word co-regularization for semisupervised sentiment analysis. In ICDM, pages 1025–1030. IEEE Computer Society.</rawString>
</citation>
<citation valid="true">
<authors>
<author>Ivan Titov</author>
<author>Ryan McDonald</author>
</authors>
<title>Modeling online reviews with multi-grain topic models.</title>
<date>2008</date>
<booktitle>In Proceedings of the 17th International Conference on World Wide Web, WWW ’08,</booktitle>
<pages>111--120</pages>
<publisher>ACM.</publisher>
<location>New York, NY, USA.</location>
<contexts>
<context position="31847" citStr="Titov and McDonald (2008)" startWordPosition="5306" endWordPosition="5309">vised learning method to group aspect expressions into userspecified aspects. They employed lexical knowledge to provide a better initialization for EM. In Zhai et al. (2011a), an EM-based unsupervised version was proposed. The so-called L-EM model first generated softly labeled data by grouping feature expressions that share words in common, and then merged the groups by lexical similarity. Zhai et al. (2011b) proposed a LDA-based method that incorporates must-link and cannot-link constraints. Another line of work aimed to extract and cluster aspect words simultaneously using topic modeling. Titov and McDonald (2008) proposed the multi-grain topic models to discover global and local aspects. Branavan et al. (2008) proposed a method which first clustered the key-phrases in Pros and Cons into some aspect categories based on distributional similarity, then built a topic model modeling the topics or aspects. Zhao et al. (2010) proposed the MaxEnt-LDA (a Maximum Entropy and LDA combination) hybrid model to jointly discover both aspect words and aspectspecific opinion words, which can leverage syntactic features to separate aspects and sentiment words. Mukherjee and Liu (2012) proposed a semi-supervised topic m</context>
</contexts>
<marker>Titov, McDonald, 2008</marker>
<rawString>Ivan Titov and Ryan McDonald. 2008. Modeling online reviews with multi-grain topic models. In Proceedings of the 17th International Conference on World Wide Web, WWW ’08, pages 111–120, New York, NY, USA. ACM.</rawString>
</citation>
<citation valid="true">
<authors>
<author>Zhongwu Zhai</author>
<author>Bing Liu</author>
<author>Hua Xu</author>
<author>Peifa Jia</author>
</authors>
<title>Grouping product features using semi-supervised learning with soft-constraints.</title>
<date>2010</date>
<booktitle>In Proceedings of the 23rd International Conference on Computational Linguistics, COLING ’10,</booktitle>
<pages>1272--1280</pages>
<publisher>Association</publisher>
<institution>for Computational Linguistics.</institution>
<location>Stroudsburg, PA, USA.</location>
<contexts>
<context position="1761" citStr="Zhai et al., 2010" startWordPosition="242" endWordPosition="245">k of Posterior Regularization (PR) to cluster aspectrelated phrases. Experiments demonstrate that our approach outperforms baselines remarkably. 1 Introduction Aspect-level sentiment analysis has become a central task in sentiment analysis because it can aggregate various opinions according to a product’s properties, and provide much detailed, complete, and in-depth summaries of a large number of reviews. Aspect finding and clustering, a precursor process of aspect-level sentiment analysis, has attracted more and more attentions (Mukherjee and Liu, 2012; Chen et al., 2013; Zhai et al., 2011a; Zhai et al., 2010). Aspect finding and clustering has never been a trivial task. People often use different words or phrases to refer to the same product property (also called product aspect or feature in the literature). Some terms are lexically dissimilar while semantically close, which makes the task more challenging. For example, “price”, “money” , “worth” and “cost” all refer to the aspect “price” in reviews. In order to present aspect-specific summaries of opinions, we first of all, have to cluster different aspect-related phrases. It is expensive and timeconsuming to manually group hundreds of aspectrela</context>
<context position="31190" citStr="Zhai et al. (2010)" startWordPosition="5206" endWordPosition="5209">a variety of frameworks and models for sentiment analysis have been studied extensively. There have been many studies on clustering aspect-related phrases. Most existing studies are 1621 based on context information. Some works also encoded lexical similarity and synonyms as prior knowledge. Carenini et al. (2005) proposed a method that was based on several similarity metrics involving string similarity, synonyms, and lexical distances defined with WordNet. Guo et al. (2009) proposed a multi-level latent semantic association model to capture expression-level and context-level topic structure. Zhai et al. (2010) proposed an EM-based semi-supervised learning method to group aspect expressions into userspecified aspects. They employed lexical knowledge to provide a better initialization for EM. In Zhai et al. (2011a), an EM-based unsupervised version was proposed. The so-called L-EM model first generated softly labeled data by grouping feature expressions that share words in common, and then merged the groups by lexical similarity. Zhai et al. (2011b) proposed a LDA-based method that incorporates must-link and cannot-link constraints. Another line of work aimed to extract and cluster aspect words simul</context>
</contexts>
<marker>Zhai, Liu, Xu, Jia, 2010</marker>
<rawString>Zhongwu Zhai, Bing Liu, Hua Xu, and Peifa Jia. 2010. Grouping product features using semi-supervised learning with soft-constraints. In Proceedings of the 23rd International Conference on Computational Linguistics, COLING ’10, pages 1272–1280, Stroudsburg, PA, USA. Association for Computational Linguistics.</rawString>
</citation>
<citation valid="true">
<authors>
<author>Zhongwu Zhai</author>
<author>Bing Liu</author>
<author>Hua Xu</author>
<author>Peifa Jia</author>
</authors>
<title>Clustering product features for opinion mining.</title>
<date>2011</date>
<booktitle>In Proceedings of the Fourth ACM International Conference on Web Search and Data Mining, WSDM ’11,</booktitle>
<pages>347--354</pages>
<publisher>ACM.</publisher>
<location>New York, NY, USA.</location>
<contexts>
<context position="1740" citStr="Zhai et al., 2011" startWordPosition="238" endWordPosition="241">odel in the framework of Posterior Regularization (PR) to cluster aspectrelated phrases. Experiments demonstrate that our approach outperforms baselines remarkably. 1 Introduction Aspect-level sentiment analysis has become a central task in sentiment analysis because it can aggregate various opinions according to a product’s properties, and provide much detailed, complete, and in-depth summaries of a large number of reviews. Aspect finding and clustering, a precursor process of aspect-level sentiment analysis, has attracted more and more attentions (Mukherjee and Liu, 2012; Chen et al., 2013; Zhai et al., 2011a; Zhai et al., 2010). Aspect finding and clustering has never been a trivial task. People often use different words or phrases to refer to the same product property (also called product aspect or feature in the literature). Some terms are lexically dissimilar while semantically close, which makes the task more challenging. For example, “price”, “money” , “worth” and “cost” all refer to the aspect “price” in reviews. In order to present aspect-specific summaries of opinions, we first of all, have to cluster different aspect-related phrases. It is expensive and timeconsuming to manually group h</context>
<context position="3349" citStr="Zhai et al., 2011" startWordPosition="494" endWordPosition="497">, we develop a new angle to address the problem, which is based on sentiment distribution consistency assumption that different phrases of the same aspect should have consistent sentiment distribution, which will be detailed soon later. Figure 1: A semi-structured Review. This new angle is inspired by this simple observation (as illustrated in Fig. 1): two phrases within the same cluster are not likely to be simultaneously placed in Pros and Cons of the same review. A straightforward way to use this information is to formulate cannot-link knowledge in clustering algorithms (Chen et al., 2013; Zhai et al., 2011b). However, we have a particularly different manner to leverage the knowledge. Due to the availability of large-scale semistructured customer reviews (as exemplified in Fig. 1) that are supported by many web sites, we can easily get the estimation of sentiment distribution for each aspect phrase by simply counting how many times a phrase appears in Pros and 1614 Proceedings of the 2014 Conference on Empirical Methods in Natural Language Processing (EMNLP), pages 1614–1623, October 25-29, 2014, Doha, Qatar. c�2014 Association for Computational Linguistics Cons respectively. As illustrated in F</context>
<context position="15609" citStr="Zhai et al., 2011" startWordPosition="2557" endWordPosition="2560">) − min θ q∈Q By trading off the data likelihood of the observed context documents (as defined in the first term), and the KL divergence of the posteriors to the valid posterior subspace defined by SDCconstraint (as defined in the second term), the objective encourages models with both desired posterior distribution and data likelihood. In essence, the model attempts to maximize data likelihood of context subject (softly) to SDC-constraint. sjk − C �σjk ≤ uik ≤ sjk + C √njk |D| ri j=1 pθ(dj) = pθ(D) = |D| ri j=1 E yj ∈A KL(q(Y )||pθ(Y |D))} (7) 1617 2.3.1 Multinomial Naive Bayes In spirit to (Zhai et al., 2011a), we use Multinomial Naive Bayes (MNB) to model the context document. Let wdj,k denotes the kth word in document dj, where each word is from the vocabulary V = {w1, w2,..., w|V |}. For each aspect phrase fj, the probability of its latent aspect being ai and generating context document di is |dj| pg(dj, yj = ai) = p(ai) ∏ p(wdj,k|ai) (8) k=1 where p(ai) and p(wdj,k|ai) are parameters of this model. Each word wdj,k is conditionally independent of all other words given the aspect ai. Although MNB has been used in existing work for aspect clustering, all of the studies used it in a semi-supervis</context>
<context position="19576" citStr="Zhai et al. (2011" startWordPosition="3270" endWordPosition="3273">on problem and obtaining the optimal A, we can calculate the projected posterior distri q(yj = ai) = Z 1 + where is the number of times that the word wt occurs in document The parameters are initialized randomly, and we repeat E-step an ∑|D| j=1 Ntiq(yj = ai) p(wt|ai) = |V |+ ∑|V |∑|D |(16) j=1 Nmjq(yj = ai) m=1 Ntj dj. d M-step until convergence. |D| uik = ∑|D| j=1 njkpθ(ai|dj)j=1 ∑ 1 njkpθ(ai|dj)�sjk (10) =1 k=1 ,\ ikdjk− log pθ(ai|dj)exp{− ∑ |A| i=1 1618 2.5 Data Extraction 2.5.1 Context Extraction In order to extract the context document d for each aspect phrase, we follow the approach in Zhai et al. (2011a). For each aspect phrase, we generate its context document by aggregating the surrounding texts of the phrase in all reviews. The preceding and following t words of a phrase are taken as the context where we set t = 3 in this paper. Stopwords and other aspect phrases are removed. For example, the following review contains two aspect phrases, ”screen” and ”picture”, The LCD screen gives clear picture. For ”screen”, the surrounding texts are {the, LCD, gives, clear, picture}. We remove stopwords ”the”, and the aspect term ”picture”, and the resultant context of ”screen” in this review is conte</context>
<context position="24643" citStr="Zhai et al., 2011" startWordPosition="4136" endWordPosition="4139">+ TN RI(DS) = (21) TP + TN + FP + FN 3.3 Evaluation Results 3.3.1 Comparison to unsupervised baselines We compared our approach with several existing unsupervised methods. Some of the methods augmented unsupervised models by incorporating lexical similarity and other domain knowledge. All of them are context-based models.2 We list these models as follows. • Kmeans: Kmeans is the most popular clustering algorithm. Here we use the context distributional similarity (cosine similarity) as the similarity measure. • L-EM: This is a state-of-the-art unsupervised method for clustering aspect phrases (Zhai et al., 2011a). L-EM employed lexical knowledge to provide a better initialization for EM. • LDA: LDA is a popular topic model(Blei et al., 2003). Given a set of documents, it outputs groups of terms of different topics. In our case, each aspect phrase is processed as a term. 3 Each sentence in a review is considered as a document. Each aspect is considered as a topic. In LDA, a term may belong to more than one topic/group, but we take the topic/group with the maximum probability. 2In our method, we collect context document for each aspect phrase. This process is conducted for L-EM and Kmeans. But for LDA</context>
<context position="31395" citStr="Zhai et al. (2011" startWordPosition="5237" endWordPosition="5240">formation. Some works also encoded lexical similarity and synonyms as prior knowledge. Carenini et al. (2005) proposed a method that was based on several similarity metrics involving string similarity, synonyms, and lexical distances defined with WordNet. Guo et al. (2009) proposed a multi-level latent semantic association model to capture expression-level and context-level topic structure. Zhai et al. (2010) proposed an EM-based semi-supervised learning method to group aspect expressions into userspecified aspects. They employed lexical knowledge to provide a better initialization for EM. In Zhai et al. (2011a), an EM-based unsupervised version was proposed. The so-called L-EM model first generated softly labeled data by grouping feature expressions that share words in common, and then merged the groups by lexical similarity. Zhai et al. (2011b) proposed a LDA-based method that incorporates must-link and cannot-link constraints. Another line of work aimed to extract and cluster aspect words simultaneously using topic modeling. Titov and McDonald (2008) proposed the multi-grain topic models to discover global and local aspects. Branavan et al. (2008) proposed a method which first clustered the key-</context>
</contexts>
<marker>Zhai, Liu, Xu, Jia, 2011</marker>
<rawString>Zhongwu Zhai, Bing Liu, Hua Xu, and Peifa Jia. 2011a. Clustering product features for opinion mining. In Proceedings of the Fourth ACM International Conference on Web Search and Data Mining, WSDM ’11, pages 347–354, New York, NY, USA. ACM.</rawString>
</citation>
<citation valid="true">
<authors>
<author>Zhongwu Zhai</author>
<author>Bing Liu</author>
<author>Hua Xu</author>
<author>Peifa Jia</author>
</authors>
<title>Constrained lda for grouping product features in opinion mining.</title>
<date>2011</date>
<booktitle>In Proceedings of the 15th Pacific-Asia Conference on Advances in Knowledge Discovery and Data Mining - Volume Part I, PAKDD’11,</booktitle>
<pages>448--459</pages>
<publisher>Springer-Verlag.</publisher>
<location>Berlin, Heidelberg.</location>
<contexts>
<context position="1740" citStr="Zhai et al., 2011" startWordPosition="238" endWordPosition="241">odel in the framework of Posterior Regularization (PR) to cluster aspectrelated phrases. Experiments demonstrate that our approach outperforms baselines remarkably. 1 Introduction Aspect-level sentiment analysis has become a central task in sentiment analysis because it can aggregate various opinions according to a product’s properties, and provide much detailed, complete, and in-depth summaries of a large number of reviews. Aspect finding and clustering, a precursor process of aspect-level sentiment analysis, has attracted more and more attentions (Mukherjee and Liu, 2012; Chen et al., 2013; Zhai et al., 2011a; Zhai et al., 2010). Aspect finding and clustering has never been a trivial task. People often use different words or phrases to refer to the same product property (also called product aspect or feature in the literature). Some terms are lexically dissimilar while semantically close, which makes the task more challenging. For example, “price”, “money” , “worth” and “cost” all refer to the aspect “price” in reviews. In order to present aspect-specific summaries of opinions, we first of all, have to cluster different aspect-related phrases. It is expensive and timeconsuming to manually group h</context>
<context position="3349" citStr="Zhai et al., 2011" startWordPosition="494" endWordPosition="497">, we develop a new angle to address the problem, which is based on sentiment distribution consistency assumption that different phrases of the same aspect should have consistent sentiment distribution, which will be detailed soon later. Figure 1: A semi-structured Review. This new angle is inspired by this simple observation (as illustrated in Fig. 1): two phrases within the same cluster are not likely to be simultaneously placed in Pros and Cons of the same review. A straightforward way to use this information is to formulate cannot-link knowledge in clustering algorithms (Chen et al., 2013; Zhai et al., 2011b). However, we have a particularly different manner to leverage the knowledge. Due to the availability of large-scale semistructured customer reviews (as exemplified in Fig. 1) that are supported by many web sites, we can easily get the estimation of sentiment distribution for each aspect phrase by simply counting how many times a phrase appears in Pros and 1614 Proceedings of the 2014 Conference on Empirical Methods in Natural Language Processing (EMNLP), pages 1614–1623, October 25-29, 2014, Doha, Qatar. c�2014 Association for Computational Linguistics Cons respectively. As illustrated in F</context>
<context position="15609" citStr="Zhai et al., 2011" startWordPosition="2557" endWordPosition="2560">) − min θ q∈Q By trading off the data likelihood of the observed context documents (as defined in the first term), and the KL divergence of the posteriors to the valid posterior subspace defined by SDCconstraint (as defined in the second term), the objective encourages models with both desired posterior distribution and data likelihood. In essence, the model attempts to maximize data likelihood of context subject (softly) to SDC-constraint. sjk − C �σjk ≤ uik ≤ sjk + C √njk |D| ri j=1 pθ(dj) = pθ(D) = |D| ri j=1 E yj ∈A KL(q(Y )||pθ(Y |D))} (7) 1617 2.3.1 Multinomial Naive Bayes In spirit to (Zhai et al., 2011a), we use Multinomial Naive Bayes (MNB) to model the context document. Let wdj,k denotes the kth word in document dj, where each word is from the vocabulary V = {w1, w2,..., w|V |}. For each aspect phrase fj, the probability of its latent aspect being ai and generating context document di is |dj| pg(dj, yj = ai) = p(ai) ∏ p(wdj,k|ai) (8) k=1 where p(ai) and p(wdj,k|ai) are parameters of this model. Each word wdj,k is conditionally independent of all other words given the aspect ai. Although MNB has been used in existing work for aspect clustering, all of the studies used it in a semi-supervis</context>
<context position="19576" citStr="Zhai et al. (2011" startWordPosition="3270" endWordPosition="3273">on problem and obtaining the optimal A, we can calculate the projected posterior distri q(yj = ai) = Z 1 + where is the number of times that the word wt occurs in document The parameters are initialized randomly, and we repeat E-step an ∑|D| j=1 Ntiq(yj = ai) p(wt|ai) = |V |+ ∑|V |∑|D |(16) j=1 Nmjq(yj = ai) m=1 Ntj dj. d M-step until convergence. |D| uik = ∑|D| j=1 njkpθ(ai|dj)j=1 ∑ 1 njkpθ(ai|dj)�sjk (10) =1 k=1 ,\ ikdjk− log pθ(ai|dj)exp{− ∑ |A| i=1 1618 2.5 Data Extraction 2.5.1 Context Extraction In order to extract the context document d for each aspect phrase, we follow the approach in Zhai et al. (2011a). For each aspect phrase, we generate its context document by aggregating the surrounding texts of the phrase in all reviews. The preceding and following t words of a phrase are taken as the context where we set t = 3 in this paper. Stopwords and other aspect phrases are removed. For example, the following review contains two aspect phrases, ”screen” and ”picture”, The LCD screen gives clear picture. For ”screen”, the surrounding texts are {the, LCD, gives, clear, picture}. We remove stopwords ”the”, and the aspect term ”picture”, and the resultant context of ”screen” in this review is conte</context>
<context position="24643" citStr="Zhai et al., 2011" startWordPosition="4136" endWordPosition="4139">+ TN RI(DS) = (21) TP + TN + FP + FN 3.3 Evaluation Results 3.3.1 Comparison to unsupervised baselines We compared our approach with several existing unsupervised methods. Some of the methods augmented unsupervised models by incorporating lexical similarity and other domain knowledge. All of them are context-based models.2 We list these models as follows. • Kmeans: Kmeans is the most popular clustering algorithm. Here we use the context distributional similarity (cosine similarity) as the similarity measure. • L-EM: This is a state-of-the-art unsupervised method for clustering aspect phrases (Zhai et al., 2011a). L-EM employed lexical knowledge to provide a better initialization for EM. • LDA: LDA is a popular topic model(Blei et al., 2003). Given a set of documents, it outputs groups of terms of different topics. In our case, each aspect phrase is processed as a term. 3 Each sentence in a review is considered as a document. Each aspect is considered as a topic. In LDA, a term may belong to more than one topic/group, but we take the topic/group with the maximum probability. 2In our method, we collect context document for each aspect phrase. This process is conducted for L-EM and Kmeans. But for LDA</context>
<context position="31395" citStr="Zhai et al. (2011" startWordPosition="5237" endWordPosition="5240">formation. Some works also encoded lexical similarity and synonyms as prior knowledge. Carenini et al. (2005) proposed a method that was based on several similarity metrics involving string similarity, synonyms, and lexical distances defined with WordNet. Guo et al. (2009) proposed a multi-level latent semantic association model to capture expression-level and context-level topic structure. Zhai et al. (2010) proposed an EM-based semi-supervised learning method to group aspect expressions into userspecified aspects. They employed lexical knowledge to provide a better initialization for EM. In Zhai et al. (2011a), an EM-based unsupervised version was proposed. The so-called L-EM model first generated softly labeled data by grouping feature expressions that share words in common, and then merged the groups by lexical similarity. Zhai et al. (2011b) proposed a LDA-based method that incorporates must-link and cannot-link constraints. Another line of work aimed to extract and cluster aspect words simultaneously using topic modeling. Titov and McDonald (2008) proposed the multi-grain topic models to discover global and local aspects. Branavan et al. (2008) proposed a method which first clustered the key-</context>
</contexts>
<marker>Zhai, Liu, Xu, Jia, 2011</marker>
<rawString>Zhongwu Zhai, Bing Liu, Hua Xu, and Peifa Jia. 2011b. Constrained lda for grouping product features in opinion mining. In Proceedings of the 15th Pacific-Asia Conference on Advances in Knowledge Discovery and Data Mining - Volume Part I, PAKDD’11, pages 448–459, Berlin, Heidelberg. Springer-Verlag.</rawString>
</citation>
<citation valid="true">
<authors>
<author>Wayne X Zhao</author>
<author>Jing Jiang</author>
<author>Hongfei Yan</author>
<author>Xiaoming Li</author>
</authors>
<title>Jointly modeling aspects and opinions with a MaxEnt-LDA hybrid.</title>
<date>2010</date>
<booktitle>In Proceedings of the 2010 Conference on Empirical Methods in Natural Language Processing, EMNLP ’10,</booktitle>
<pages>56--65</pages>
<publisher>Association</publisher>
<institution>for Computational Linguistics.</institution>
<location>Stroudsburg, PA, USA.</location>
<contexts>
<context position="32159" citStr="Zhao et al. (2010)" startWordPosition="5355" endWordPosition="5358">at share words in common, and then merged the groups by lexical similarity. Zhai et al. (2011b) proposed a LDA-based method that incorporates must-link and cannot-link constraints. Another line of work aimed to extract and cluster aspect words simultaneously using topic modeling. Titov and McDonald (2008) proposed the multi-grain topic models to discover global and local aspects. Branavan et al. (2008) proposed a method which first clustered the key-phrases in Pros and Cons into some aspect categories based on distributional similarity, then built a topic model modeling the topics or aspects. Zhao et al. (2010) proposed the MaxEnt-LDA (a Maximum Entropy and LDA combination) hybrid model to jointly discover both aspect words and aspectspecific opinion words, which can leverage syntactic features to separate aspects and sentiment words. Mukherjee and Liu (2012) proposed a semi-supervised topic model which used userprovided seeds to discover aspects. Chen et al. (2013) proposed a knowledge-based topic model to incorporate must-link and cannot-link information. Their model can adjust topic numbers automatically by leveraging cannot-link. Our work is also related to general constraintdriven(or knowledge-</context>
</contexts>
<marker>Zhao, Jiang, Yan, Li, 2010</marker>
<rawString>Wayne X. Zhao, Jing Jiang, Hongfei Yan, and Xiaoming Li. 2010. Jointly modeling aspects and opinions with a MaxEnt-LDA hybrid. In Proceedings of the 2010 Conference on Empirical Methods in Natural Language Processing, EMNLP ’10, pages 56– 65, Stroudsburg, PA, USA. Association for Computational Linguistics.</rawString>
</citation>
</citationList>
</algorithm>
</algorithms>
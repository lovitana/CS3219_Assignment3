<?xml version="1.0" encoding="UTF-8"?>
<algorithms version="110505">
<algorithm name="SectLabel" version="110505">
<variant no="0" confidence="0.000000">
<title confidence="0.984076">
Greed is Good if Randomized: New Inference for Dependency Parsing
</title>
<author confidence="0.999487">
Yuan Zhang∗, Tao Lei∗, Regina Barzilay, and Tommi Jaaliliola
</author>
<affiliation confidence="0.998365">
Computer Science and Artificial Intelligence Laboratory
Massachusetts Institute of Technology
</affiliation>
<email confidence="0.988113">
{yuanzh, taolei, regina, tommi}@csail.mit.edu
</email>
<sectionHeader confidence="0.99358" genericHeader="abstract">
Abstract
</sectionHeader>
<bodyText confidence="0.999559285714286">
Dependency parsing with high-order fea-
tures results in a provably hard decoding
problem. A lot of work has gone into
developing powerful optimization meth-
ods for solving these combinatorial prob-
lems. In contrast, we explore, analyze, and
demonstrate that a substantially simpler
randomized greedy inference algorithm al-
ready suffices for near optimal parsing: a)
we analytically quantify the number of lo-
cal optima that the greedy method has to
overcome in the context of first-order pars-
ing; b) we show that, as a decoding algo-
rithm, the greedy method surpasses dual
decomposition in second-order parsing; c)
we empirically demonstrate that our ap-
proach with up to third-order and global
features outperforms the state-of-the-art
dual decomposition and MCMC sampling
methods when evaluated on 14 languages
of non-projective CoNLL datasets.1
</bodyText>
<sectionHeader confidence="0.99899" genericHeader="keywords">
1 Introduction
</sectionHeader>
<bodyText confidence="0.999795428571429">
Dependency parsing is typically guided by param-
eterized scoring functions that involve rich fea-
tures exerting refined control over the choice of
parse trees. As a consequence, finding the high-
est scoring parse tree is a provably hard combina-
torial inference problem (McDonald and Pereira,
2006). Much of the recent work on parsing has
focused on solving these problems using powerful
optimization techniques. In this paper, we follow a
different strategy, arguing that a much simpler in-
ference strategy suffices. In fact, we demonstrate
that a randomized greedy method of inference sur-
passes the state-of-the-art performance in depen-
dency parsing.
</bodyText>
<note confidence="0.89175">
∗Both authors contributed equally.
</note>
<footnote confidence="0.993797">
1Our code is available at https://github.com/
taolei87/RBGParser.
</footnote>
<bodyText confidence="0.999945243902439">
Our choice of a randomized greedy algorithm
for parsing follows from a successful track record
of such methods in other hard combinatorial prob-
lems. These conceptually simple and intuitive
algorithms have delivered competitive approxi-
mations across a broad class of NP-hard prob-
lems ranging from set cover (Hochbaum, 1982) to
MAX-SAT (Resende et al., 1997). Their success
is predicated on the observation that most realiza-
tions of problems are much easier to solve than the
worst-cases. A simpler algorithm will therefore
suffice in typical cases. Evidence is accumulating
that parsing problems may exhibit similar proper-
ties. For instance, methods such as dual decom-
position offer certificates of optimality when the
highest scoring tree is found. Across languages,
dual decomposition has shown to lead to a cer-
tificate of optimality for the vast majority of the
sentences (Koo et al., 2010; Martins et al., 2011).
These remarkable results suggest that, as a com-
binatorial problem, parsing appears simpler than
its broader complexity class would suggest. In-
deed, we show that a simpler inference algorithm
already suffices for superior results.
In this paper, we introduce a randomized greedy
algorithm that can be easily used with any rich
scoring function. Starting with an initial tree
drawn uniformly at random, the algorithm makes
only local myopic changes to the parse tree in an
attempt to climb the objective function. While a
single run of the hill-climbing algorithm may in-
deed get stuck in a locally optimal solution, mul-
tiple random restarts can help to overcome this
problem. The same algorithm is used both for
learning the parameters of the scoring function as
well as for parsing test sentences.
The success of a randomized greedy algorithm
is tied to the number of local maxima in the search
space. When the number is small, only a few
restarts will suffice for the greedy algorithm to
find the highest scoring parse. We provide an al-
</bodyText>
<page confidence="0.904045">
1013
</page>
<note confidence="0.898882">
Proceedings of the 2014 Conference on Empirical Methods in Natural Language Processing (EMNLP), pages 1013–1024,
October 25-29, 2014, Doha, Qatar. c�2014 Association for Computational Linguistics
</note>
<bodyText confidence="0.999976479166667">
gorithm for explicitly counting the number of lo-
cal optima in the context of first-order parsing,
and demonstrate that the number is typically quite
small. Indeed, we find that a first-order parser
trained with exact inference or using our random-
ized greedy algorithm delivers basically the same
performance.
We hypothesize that parsing with high-order
scoring functions exhibits similar properties. The
main rationale is that, even in the presence of high-
order features, the resulting scoring function re-
mains first-order dominant. The performance of
a simple arc-factored first-order parser is only a
few percentage points behind higher-order parsers.
The higher-order features in the scoring function
offer additional refinement but only a few changes
above and beyond the first-order result. As a
consequence, most of the arc choices are already
determined by a much simpler, polynomial time
parser.
We use dual decomposition to show that the
greedy method indeed succeeds as an inference al-
gorithm even with higher-order scoring functions.
In fact, with second-order features, regardless of
which method was used for training, the random-
ized greedy method outperforms dual decomposi-
tion by finding higher scoring trees. For the sen-
tences that dual decomposition is optimal (obtains
a certificate), the greedy method finds the same
solution in over 99% of the cases. Our simple
inference algorithm is therefore likely to scale to
higher-order parsing and we demonstrate empiri-
cally that this is indeed so.
We validate our claim by evaluating the method
on the CoNLL dependency benchmark that com-
prises treebanks from 14 languages. Aver-
aged across all languages, our method out-
performs state-of-the-art parsers, including Tur-
boParser (Martins et al., 2013) and our earlier
sampling-based parser (Zhang et al., 2014). On
seven languages, we report the best published re-
sults. The method is not sensitive to initialization.
In fact, drawing the initial tree uniformly at ran-
dom results in the same performance as when ini-
tialized from a trained first-order distribution. In
contrast, sufficient randomization of the starting
point is critical. Only a small number of restarts
suffices for finding (near) optimal parse trees.
</bodyText>
<sectionHeader confidence="0.99975" genericHeader="introduction">
2 Related Work
</sectionHeader>
<subsectionHeader confidence="0.924372">
Finding Optimal Structure in Parsing The use
</subsectionHeader>
<bodyText confidence="0.989989583333333">
of rich-scoring functions in dependency parsing
inevitably leads to the challenging combinatorial
problem of finding the maximizing parse. In fact,
McDonald and Pereira (2006) demonstrated that
the task is provably NP-hard for non-projective
second-order parsing. Not surprisingly, approx-
imate inference has been at the center of pars-
ing research. Examples of these approaches in-
clude easy-first parsing (Goldberg and Elhadad,
2010), inexact search (Johansson and Nugues,
2007; Zhang and Clark, 2008; Huang et al., 2012;
Zhang et al., 2013), partial dynamic program-
ming (Huang and Sagae, 2010) and dual decom-
position (Koo et al., 2010; Martins et al., 2011).
Our work is most closely related to the MCMC
sampling-based approaches (Nakagawa, 2007;
Zhang et al., 2014). In our earlier work, we devel-
oped a method that learns to take guided stochas-
tic steps towards a high-scoring parse (Zhang et
al., 2014). In the heart of that technique are so-
phisticated samplers for traversing the space of
trees. In this paper, we demonstrate that a sub-
stantially simpler approach that starts from a tree
drawn from the uniform distribution and uses hill-
climbing for parameter updates achieves similar or
higher performance.
Another related greedy inference method has
been used for non-projective dependency pars-
ing (McDonald and Pereira, 2006). This method
relies on hill-climbing to convert the highest scor-
ing projective tree into its non-projective approxi-
mation. Our experiments demonstrate that when
hill-climbing is employed as a primary learning
mechanism for high-order parsing, it exhibits dif-
ferent properties: the distribution for initialization
does not play a major role in the final outcome,
while the use of restarts contributes significantly
to the quality of the resulting tree.
Greedy Approximations for NP-hard Problems
There is an expansive body of research on greedy
approximations for NP-hard problems. Examples
of NP-hard problems with successful greedy ap-
proximations include the traveling saleman prob-
lem problem (Held and Karp, 1970; Rego et
al., 2011), the MAX-SAT problem (Mitchell et
al., 1992; Resende et al., 1997) and vertex
cover (Hochbaum, 1982). While some greedy
methods have poor worst-case complexity, many
</bodyText>
<page confidence="0.993103">
1014
</page>
<bodyText confidence="0.999955588235294">
of them work remarkably well in practice. Despite
the apparent simplicity of these algorithms, un-
derstanding their properties is challenging: often
their “theoretical analyses are negative and incon-
clusive” (Amenta and Ziegler, 1999; Spielman and
Teng, 2001). Identifying conditions under which
approximations are provably optimal is an active
area of research in computer science theory (Du-
mitrescu and T´oth, 2013; Jonsson et al., 2013).
In NLP, randomized and greedy approximations
have been successfully used across multiple ap-
plications, including machine translation and lan-
guage modeling (Brown et al., 1993; Ravi and
Knight, 2010; Daum´e III et al., 2009; Moore and
Quirk, 2008; Deoras et al., 2011). In this paper,
we study the properties of these approximations in
the context of dependency parsing.
</bodyText>
<sectionHeader confidence="0.998695" genericHeader="method">
3 Method
</sectionHeader>
<subsectionHeader confidence="0.998763">
3.1 Preliminaries
</subsectionHeader>
<bodyText confidence="0.999835333333333">
Let x be a sentence and T (x) be the set of possi-
ble dependency trees over the words in x. We use
y ∈ T (x) to denote a dependency tree for x, and
y(m) to specify the head (parent) of the modifier
word indexed by m in tree y. We also use m to
denote the indexed word when there is no ambi-
guity. In addition, we define T (y, m) as the set
of “neighboring trees” of y obtained by changing
only the head of the modifier, i.e. y(m).
The dependency trees are scored according to
S(x, y) = θ · φ(x, y), where θ is a vector of pa-
rameters and φ(x, y) is a sparse feature vector rep-
resentation of tree y for sentence x. In this work,
φ(x, y) will include up to third-order features as
well as a range of global features commonly used
in re-ranking methods (Collins, 2000; Charniak
and Johnson, 2005; Huang, 2008).
The parameters θ in the scoring function are
estimated on the basis of a training set D =
{(ˆxi, ˆyi)}Ni=1 of sentences ˆxiand the correspond-
ing gold (target) trees ˆyi. We adopt a max-margin
framework for this learning problem. Specifically,
we aim to find parameter values that score the gold
target trees higher than others:
</bodyText>
<equation confidence="0.983363">
∀i ∈ {1, · · · , N}, y ∈ T (ˆxi),
S(ˆxi, ˆyi) ≥ S(ˆxi, y) + kˆyi − yk1 − ξi
</equation>
<bodyText confidence="0.999660111111111">
where ξi ≥ 0 is the slack variable (non-zero values
are penalized against) and kˆyi − yk1 is the ham-
ming distance between the gold tree ˆyi and a can-
didate parse y.
In an online learning setup, parameters are up-
dated successively after each sentence. Each up-
date still requires us to find the “strongest viola-
tion”, i.e., a candidate tree y˜ that scores higher
than the gold tree ˆyi:
</bodyText>
<equation confidence="0.99933">
y˜ = arg max {S(ˆxi, y) + ky − ˆyik1}
yET (ˆxi)
</equation>
<bodyText confidence="0.9999509">
The parameters are then revised so as to select
against the offending ˜y. Instead of a standard
parameter update based on y˜ as in perceptron,
stochastic gradient descent, or passive-aggressive
updates, our implementation follows Lei et al.
(2014) where the first-order parameters are broken
up into a tensor. Each tensor component is updated
successively in combination with the parameters
corresponding to MST features (McDonald et al.,
2005) and higher-order features (when included).2
</bodyText>
<subsectionHeader confidence="0.996943">
3.2 Algorithm
</subsectionHeader>
<bodyText confidence="0.996995">
During training and testing, the key combinatorial
problem we must solve is that of decoding, i.e.,
finding the highest scoring tree y˜ ∈ T (x) for each
sentence x (or ˆxi). In our notation,
</bodyText>
<equation confidence="0.9997645">
y˜ = arg max {θ · φ(ˆxi, y) + ky − ˆyik1} (train)
yET (ˆxi)
y˜ = arg max {θ · φ(x, y)} (test)
yET (x)
</equation>
<bodyText confidence="0.999864">
While the decoding problem with feature sets sim-
ilar to ours has been shown to be NP-hard, many
approximation algorithms work remarkably well.
We commence with a motivating example.
Locality and Parsing One possible reason for
why greedy or other approximation algorithms
work well for dependency parsing is that typical
sentences and therefore the learned scoring func-
tions S(x, y) = θ · φ(x, y) are primarily “lo-
cal”. By this we mean that head-modifier deci-
sions could be made largely without considering
the surrounding structure (the context). For exam-
ple, in English an adjective and a determiner are
typically attached to the following noun.
We demonstrate the degree of locality in de-
pendency parsing by comparing a first-order tree-
based parser to the parser that predicts each head
word independently of others. Note that the in-
dependent prediction of dependency arcs does not
necessarily give rise to a tree. The parameters of
</bodyText>
<footnote confidence="0.956245">
2We refer the readers to Lei et al. (2014) for more details
about the tensor scoring function and the online update.
</footnote>
<page confidence="0.93686">
1015
</page>
<table confidence="0.999629">
Dataset Indp. Pred Tree Pred
Slovene 83.7 84.2
Arabic 79.0 79.2
Japanese 93.4 93.7
English 91.6 91.9
Average 86.9 87.3
</table>
<tableCaption confidence="0.726717">
Table 1: Head attachment accuracy of a first-order
local classifier (left) and a first-order structural
prediction model (right). The two types of mod-
els are trained using the same set of features.
</tableCaption>
<bodyText confidence="0.798703">
Input: parameter θ, sentence x
Output: dependency tree y˜
</bodyText>
<listItem confidence="0.9970563">
1: Randomly initialize tree y(0);
2: t = 0;
3: repeat
4: list = bottom-up node list of y(t);
5: for each word m in list do
6: y(t+1) = arg maxy∈T (y(t),m) S(x, y);
7: t = t + 1;
8: end for
9: until no change in this iteration
10: return y˜ = y(t);
</listItem>
<figureCaption confidence="0.91822">
Figure 1: A randomized hill-climbing algorithm
for dependency parsing.
</figureCaption>
<bodyText confidence="0.999798384615385">
the two parsers, the independent prediction and
a tree-based parser, are trained separately with
the corresponding decoding algorithm but with the
same feature set.
Table 1 shows that the accuracy of the inde-
pendent prediction ranges from 79% to 93% on
four CoNLL datasets. The results are on par with
the first-order structured prediction model. This
experiment reinforces the conclusion in Liang et
al. (2008), where a local classifier was shown
to achieve comparable accuracy to a sequential
model (e.g. CRF) in POS tagging and named-
entity recognition.
</bodyText>
<subsectionHeader confidence="0.860162">
Hill-Climbing with Random Restarts We
</subsectionHeader>
<bodyText confidence="0.999502425">
build here on the motivating example and explore
greedy algorithms as generalizations of purely lo-
cal decoding. Greedy algorithms break the decod-
ing problem into a sequence of simple local steps,
each required to improve the solution. In our case,
simple local steps correspond to choosing the head
for each modifier word.
We begin with a tree y(0), which can be a sam-
ple drawn uniformly from T (x) (Wilson, 1996).
Our greedy algorithm then updates y(t) to a bet-
ter tree y(t+1) by revising the head of one modifier
word while maintaining the constraint that the re-
sulting structure is a tree. The modifiers are con-
sidered in the bottom-up order relative to the cur-
rent tree (the word furthest from the root is consid-
ered first). We provide an analysis to motivate this
bottom-up update strategy in Section 4.1. The al-
gorithm continues until the score can no longer be
improved by changing the head of a single word.
The resulting tree represents a locally optimal pre-
diction relative to a single-arc greedy algorithm.
Figure 1 gives the algorithm in pseudo-code.
There are many possible variations of the sim-
ple randomized greedy hill-climbing algorithm.
First, the Wilson sampling algorithm (Wilson,
1996) can be naturally extended to obtain i.i.d.
samples from any first-order distributions. There-
fore, we could initialize the tree y(0) with a tree
from a first-order parser, or draw the initial tree
from a first-order distribution other than uniform.
However, perhaps surprisingly, as we demon-
strate later, little is lost with uniform initializa-
tion. Second, since a single run of randomized
hill-climbing is relatively cheap and runs are in-
dependent to each other, it is easy to execute mul-
tiple runs independently in parallel. The final pre-
dicted tree is then simply the highest scoring tree
across the multiple runs. We demonstrate that only
a small number of parallel runs are necessary for
near optimal prediction.
</bodyText>
<sectionHeader confidence="0.997973" genericHeader="method">
4 Analysis
</sectionHeader>
<subsectionHeader confidence="0.997104">
4.1 First-Order Parsing
</subsectionHeader>
<bodyText confidence="0.999980142857143">
We provide here a firmer basis for why the ran-
domized greedy algorithm can be expected to
work. While the focus of the rest of the paper
is on higher-order parsing, we limit ourselves in
this subsection to first-order parsing. The reasons
for this are threefold. First, a simple greedy algo-
rithm is already not guaranteed a priori to work in
the context of a first-order scoring function. The
conclusions from this analysis are therefore likely
to carry over to higher-order parsing scenarios as
well. Second, a first-order arc-factored scoring
provides us an easy way to ascertain when the ran-
domized greedy algorithm indeed found the high-
est scoring tree. Finally, we are able to count the
</bodyText>
<page confidence="0.964706">
1016
</page>
<table confidence="0.999767333333333">
Dataset Average Len. # of local optima at percentile fraction of finding global optima (%)
50% 70% 90% 0 &lt;Len.≤ 15 Len.&gt; 15
Turkish 12.1 1 1 2 100 100
Slovene 15.9 2 20 3647 100 98.1
English 24.0 21 121 2443 100 99.3
Arabic 36.8 2 35 &gt;10000 100 99.1
</table>
<tableCaption confidence="0.99673">
Table 2: The left part of the table shows the local optimum statistics of the first-order model. The
</tableCaption>
<bodyText confidence="0.983450317073171">
sentences are sorted by the number of local optima. Columns 3 to 5 show the number of local optima of
a sentence at different percentile of the sorted list. For example, on English 50% of the sentences have
no more than 21 local optimum trees. The right part shows the fraction of finding global optima using
300 uniform restarts for each sentence.
number of locally optimal solutions for a greedy
algorithm in the context of first-order parsing and
can therefore relate this property to the success
rates of the algorithm.
Reachability We begin by highlighting a basic
property of trees, namely that single arc changes
suffice for transforming any tree to any other tree
in a small number of steps while maintaining that
each intermediate structure is also a tree. In this
sense, a target tree is reachable from any start-
ing point using only single arc changes. More
formally, let y be any starting tree and y&apos; the de-
sired target. Let m1, m2, · · · , mn be the bottom-
up list of words (modifiers) corresponding to tree
y, where m1 is the word furthest from the root.
We can simply change each head y(mi) to that of
y&apos;(mi) in this order i = 1, ... , n. The bottom-up
order guarantees that no cycle is introduced with
respect to the remaining (yet unmodified) nodes of
y. The fact that y&apos; is a valid tree implies no cycle
will appear with respect to the already modified
nodes.
Note that, according to this property, any tree
is reachable from any starting point using only k
modifications, where k is the number of head dif-
ferences, i.e. k = |{m : y(m) =6 y&apos;(m)}|. The
result also suggests that it may be helpful to per-
form the greedy steps in the bottom-up order, a
suggestion that we follow in our implementation.
Broadly speaking, we have established that
the greedy algorithm is not inherently limited by
virtue of its basic steps. Of course, it is a differ-
ent question whether the scoring function supports
such local changes towards the correct target tree.
Locally Optimal Trees While greedy algo-
rithms are notoriously prone to getting stuck in
locally optimal solutions, we establish here that
</bodyText>
<equation confidence="0.900175">
Function CountOptima(G = hV, Ei)
V = {w0, w1, · · · , wn} where w0 is the
root
E = {eij ∈ R} are the arc scores
Return: the number of local optima
</equation>
<listItem confidence="0.965712357142857">
1: Let y(0) = ∅ and y(i) = arg maxj eji;
2: if y is a tree (no cycle) then return 1;
3: Find a cycle C ⊂ V in y;
4: count = 0;
// contract the cycle
5: create a vertex w*;
6: ∀j ∈/ C : e*j = maxkEC ekj;
7: for each vertex wi ∈ C do
8: ∀j ∈/ C : ej* = eji;
9: V &apos; = V ∪ {w*} \ C;
10: E&apos; = E ∪ {e*j, ej*  |∀j ∈/ C}
11: count += CountOptima(G&apos; = hV &apos;, E&apos;i);
12: end for
13: return count;
</listItem>
<figureCaption confidence="0.919147">
Figure 2: A recursive algorithm for counting lo-
</figureCaption>
<bodyText confidence="0.980010428571429">
cal optima for a sentence with words w1, · · · , wn
(first-order parsing). The algorithm resembles the
Chu-Liu-Edmonds algorithm for finding the max-
imum directed spanning tree (Chu and Liu, 1965).
decoding with learned scoring functions involves
only a small number of local optima. In our case,
a local optimum corresponds to a tree y where no
single change of head y(m) results in a higher
scoring tree. Clearly, the highest scoring tree is
also a local optimum in this sense. If there were
many such local optima, finding the one with the
highest score would be challenging for a greedy
algorithm, even with randomization.
We begin with a worst case analysis and estab-
</bodyText>
<page confidence="0.939677">
1017
</page>
<table confidence="0.999944333333333">
Dataset Trained with Hill-Climbing (HC) Trained with Dual Decomposition (DD)
%Cert (DD) SDD &gt;SHC SDD =SHC SDD GSHC %Cert (DD) SDD &gt;SHC SDD =SHC SDD GSHC
Turkish 98.7 0.0 99.8 0.2 98.7 0.0 100.0 0.0
Slovene 94.5 0.0 98.7 1.3 92.3 0.2 99.0 0.8
English 94.5 0.3 98.7 1.0 94.6 0.5 98.7 0.8
Arabic 78.8 3.4 93.9 2.7 75.3 4.7 88.4 6.9
</table>
<tableCaption confidence="0.996394">
Table 3: Decoding quality comparison between hill-climbing (HC) and dual decomposition (DD). Mod-
</tableCaption>
<bodyText confidence="0.733539666666667">
els are trained either with HC (left) or DD (right). sHC denotes the score of the tree retrieved by HC
and sDD gives the analogous score for DD. The columns show the percentage of all test sentences for
which one method succeeds in finding a higher or the same score. “Cert” column gives the percentage
of sentences for which DD finds a certificate.
lish a tight upper bound on the number of local
optima for a first-order scoring function.
</bodyText>
<construct confidence="0.9892316">
Theorem 1 For any first-order scoring function
thatfactorizes into the sum of arc scores S(x, y) =
E Sarc(y(m), m): (a) the number of locally op-
timal trees is at most 2n−1 for n words; (b) this
upper bound is tight.3
</construct>
<bodyText confidence="0.98693835">
While the number of possible dependency trees
is (n + 1)n−1 (Cayley’s formula), the number of
local optima is at most 2n−1. This is still too many
for longer sentences, suggesting that, in the worst
case, a randomized greedy algorithm is unlikely to
find the highest scoring tree. However, the scor-
ing functions we learn for dependency parsing are
considerably easier.
Average Case Analysis In contrast to the worst-
case analysis above, we will count here the actual
number of local optima per sentence for a first-
order scoring function learned from data with the
randomized greedy algorithm. Figure 2 provides
pseudo-code for our counting algorithm. The al-
gorithm is derived by tailoring the proof of Theo-
rem 1 to each sentence.
Table 2 shows the empirical number of locally
optimal trees estimated by our algorithm across 4
different languages. Decoding with trained scor-
ing functions in the average case is clearly sub-
stantially easier than the worst case. For exam-
ple, on the English test set more than 70% of the
sentences have at most 121 locally optimal trees.
Since the average sentence length is 24, the dis-
crepancy between the typical number (e.g., 121)
and the worst case (224−1) is substantial. As a re-
sult, only a small number of restarts is likely to
suffice for finding optimal trees in practice.
Optimal Decoding We can easily verify
whether the randomized greedy algorithm indeed
3A proof sketch is given in Appendix.
succeeds in finding the highest scoring trees with
a learned first-order scoring function. We have
established above that there are typically only a
small number of locally optimal trees. We would
therefore expect the algorithm to work. We show
the results in the second part of Table 2. For short
sentences of length up to 15, our method finds the
global optimum for all the test sentences. Success
rates remain high even for longer test sentences.
</bodyText>
<subsectionHeader confidence="0.868487">
4.2 Higher-Order Parsing
</subsectionHeader>
<bodyText confidence="0.999987379310345">
Exact decoding with high-order features is known
to be provably hard (McDonald et al., 2005). We
begin our analysis here with a second-order (sib-
ling/grandparent) model, and compare our ran-
domized hill-climbing (HC) method to dual de-
composition (DD), re-implementing Koo et al.
(2010). Table 3 compares decoding quality for the
two methods across four languages. Overall, in
97.8% of the sentences, HC obtains the same score
as DD, in 1.3% of the cases HC finds a higher
scoring tree, and in 0.9% of cases DD results in
a better tree. The results follow the same pattern
regardless of which method was used to train the
scoring function. The average rate of certificates
for DD was 92%. In over 99% of these sentences,
HC reaches the same optimum.
We expect that these observations about the suc-
cess of HC carry over to other high-order parsing
models for several reasons. First, a large num-
ber of arcs are pruned in the initial stage, con-
siderably reducing the search space and minimiz-
ing the number of possible locally optimal trees.
Second, many dependencies can be determined
already with independent arc prediction (see our
motivating example above), predictions that are
readily achieved with a greedy algorithm. Finally,
high-order features represent smaller refinements,
i.e., suggest only a few changes above and be-
yond the dominant first-order scores. Greedy al-
</bodyText>
<page confidence="0.986591">
1018
</page>
<bodyText confidence="0.999945384615385">
gorithms are therefore likely to be able to leverage
at least some of this potential. We demonstrate be-
low that this is indeed so.
Our methods are trained within the max-margin
framework. As a result, we are expected to find
the highest scoring competing tree for each train-
ing sentence (the “strongest violation”). One may
question therefore whether possible sub-optimal
decoding for some training sentences (finding “a
violation” rather than the “strongest violation”)
impacts the learned parser. To this end, Huang et
al. (2012) have established that weaker violations
do suffice for separable training sets.
</bodyText>
<sectionHeader confidence="0.999134" genericHeader="method">
5 Experimental Setup
</sectionHeader>
<bodyText confidence="0.99918784375">
Dataset and Evaluation Measures We evalu-
ate our model on CoNLL dependency treebanks
for 14 different languages (Buchholz and Marsi,
2006; Surdeanu et al., 2008), using standard train-
ing and testing splits. We use part-of-speech tags
and the morphological information provided in the
corpus. Following standard practice, we use Unla-
beled Attachment Score (UAS) excluding punctu-
ation (Koo et al., 2010; Martins et al., 2013) as the
evaluation metric in all our experiments.
Baselines We compare our model with the Tur-
boParser (Martins et al., 2013) and our earlier
sampling-based parser (Zhang et al., 2014). For
both parsers, we directly compare with the re-
cent published results on the CoNLL datasets.
We also compare our parser against the best pub-
lished results for the individual languages in our
datasets. This comparison set includes four ad-
ditional parsers: Martins et al. (2011), Koo et al.
(2010), Zhang et al. (2013) and our tensor-based
parser (Lei et al., 2014).
Features We use the same feature templates as
in our prior work (Zhang et al., 2014; Lei et al.,
2014)4. Figure 3 shows the first- to third-order
feature templates that we use in our model. For
the global features we use right-branching, coor-
dination, PP attachment, span length, neighbors,
valency and non-projective arcs features.
Implementation Details Following standard
practices, we train our model using the passive-
aggressive online learning algorithm (MIRA)
and parameter averaging (Crammer et al., 2006;
</bodyText>
<footnote confidence="0.920046">
4We refer the readers to Zhang et al. (2014) and Lei et al.
(2014) for the detailed definition of each feature template.
</footnote>
<figure confidence="0.444631">
arc consecutive sibling grandparent
</figure>
<figureCaption confidence="0.992387">
Figure 3: First- to third-order features.
Figure 4: Absolute UAS improvement of our full
</figureCaption>
<bodyText confidence="0.951836">
model over the first-order model. Sentences in the
test set are divided into 2 groups based on their
lengths.
Collins, 2002). By default we use an adaptive
strategy for running the hill-climbing algorithm
– for a given sentence we repeatedly run the al-
gorithm in parallel5 until the best tree does not
change for K = 300 consecutive restarts. For
each restart, by default we initialize the tree y(0)
by sampling from the first-order distribution us-
ing the current learned parameter values (and first-
order scores). We train our first-order and third-
order model for 10 epochs and our full model for
20 epochs for all languages, and report the average
performance across three independent runs.
</bodyText>
<sectionHeader confidence="0.999957" genericHeader="method">
6 Results
</sectionHeader>
<bodyText confidence="0.997242833333333">
Comparison with the Baselines Table 4 sum-
marizes the results of our model, along with the
state-of-the-art baselines. On average across 14
languages, our full model with the tensor com-
ponent outperforms both TurboParser and the
sampling-based parser. The direct comparison
</bodyText>
<figure confidence="0.98054128">
5We use 8 threads in all the experiments.
tri-siblings grand-grandparent
outer-sibling-grandchild inner-sibling-grandchild
h m s
g h m
grand-sibling
g h m s
h m
head bigram
h&apos; h m m+1
gg
h m s t
g h m
h m gc s h s m gc
−1
−2
4
5
3
2
1
0
Arabic Slovene English Chinese German
Len ≤ 15
Len &gt; 15
</figure>
<page confidence="0.978901">
1019
</page>
<table confidence="0.999827722222223">
Our Model Exact 1st Turbo Sampling Best Published
(MA13) (ZL14)
1st 3rd Fullw/o tensor Full
Arabic 78.98 79.95 79.38 80.24 79.22 79.64 80.12 81.12 (MS11)
Bulgarian 92.15 93.38 93.69 93.72 92.24 93.10 93.30 94.02 (ZH13)
Chinese 91.20 93.00 92.76 93.04 91.17 89.98 92.63 92.68 (LX14)
Czech 87.65 90.11 90.34 90.77 87.82 90.32 91.04 91.04 (ZL14)
Danish 90.50 91.43 91.66 91.86 90.56 91.48 91.80 92.00 (ZH13)
Dutch 84.49 86.43 87.04 87.39 84.79 86.19 86.47 86.47 (ZL14)
English 91.85 93.01 93.20 93.25 91.94 93.22 92.94 93.22 (MA13)
German 90.52 91.91 92.64 92.67 90.54 92.41 92.07 92.41 (MA13)
Japanese 93.78 93.80 93.35 93.56 93.74 93.52 93.42 93.74 (LX14)
Portuguese 91.12 92.07 92.60 92.36 91.16 92.69 92.41 93.03 (KR10)
Slovene 84.29 86.48 87.06 86.72 84.15 86.01 86.82 86.95 (MS11)
Spanish 85.52 87.87 88.17 88.75 85.59 85.59 88.24 88.24 (ZL14)
Swedish 89.89 91.17 91.35 91.08 89.78 91.14 90.71 91.62 (ZH13)
Turkish 76.57 76.80 76.13 76.68 76.40 76.90 77.21 77.55 (KR10)
Average 87.75 89.10 89.24 89.44 87.79 88.72 89.23 89.58
</table>
<tableCaption confidence="0.589615">
Table 4: Results of our model and several state-of-the-art systems. “Best Published UAS” includes the
most accurate parsers among Martins et al. (2011), Martins et al. (2013), Koo et al. (2010), Zhang et
al. (2013), Lei et al. (2014) and Zhang et al. (2014). For the third-order model, we use the feature set
of TurboParser (Martins et al., 2013). The full model combines features of our sampling-based parser
(Zhang et al., 2014) and tensor features (Lei et al., 2014).
</tableCaption>
<table confidence="0.9999235">
Dataset MAP-1st Uniform Rnd-1st
UAS Init. UAS Init. UAS Init.
Slovene 85.2 80.1 86.7 13.7 86.7 34.2
Arabic 78.8 75.1 79.7 12.4 80.2 32.8
English 91.1 82.0 93.3 39.6 93.3 55.6
Chinese 87.2 75.3 93.2 36.8 93.0 54.5
Dutch 84.8 79.5 87.0 26.9 87.4 45.6
Average 85.4 78.4 88.0 25.9 88.1 44.5
</table>
<tableCaption confidence="0.997701">
Table 5: Comparison between different initializa-
</tableCaption>
<bodyText confidence="0.988571716981132">
tion strategies: (a) MAP-1st: only the MAP tree
of the first-order score; (b) Uniform: random trees
are sampled from the uniform distribution; and
(c) Rnd-1st: random trees are sampled from the
first-order distribution. For each method, the table
shows the average accuracy of the initial tree and
the final parsing accuracy.
with TurboParser is achieved by restricting our
model to third order features which still outper-
forms TurboParser (89.10% vs 88.72%). To com-
pare against the sampling-based parser, we em-
ploy our model without the tensor component. The
two models achieve a similar average performance
(89.24% and 89.23% respectively). Since relative
parsing performance depends on a target language,
we also include comparison with the best pub-
lished results. The model achieves the best pub-
lished results for seven languages.
Another noteworthy comparison concerns first-
order parsers. As Table 4 shows, the exact and ap-
proximate versions of the first-order parser deliver
almost identical performance.
Impact of High-Order Features Table 4 shows
that the model can effectively utilize high-order
features. Comparing the average performance of
the model variants, we see that the accuracy on
the benchmark languages consistently improves
when higher-order features are added. This char-
acteristic of the randomized greedy parser is in
line with findings about other state-of-the-art high-
order parsers (Martins et al., 2013; Zhang et al.,
2014). Figure 4 breaks down these gains based
on the sentence length. As expected, on most lan-
guages high-order features are particularly helpful
when parsing longer sentences.
Impact of Initialization and Restarts Table 5
shows the impact of initialization on the model
performance for several languages. We consider
three strategies: the MAP estimate of the first-
order score from the model, uniform sampling and
sampling from the first-order distribution. The ac-
curacy of initial trees varies greatly, ranging from
78.4% for the MAP estimate to 25.9% and 44.5%
for the latter randomized strategies. However, the
resulting parsing accuracy is not determined by
the initial accuracy. In fact, the two sampling
strategies result in almost identical parsing perfor-
mance. While the first-order MAP estimate gives
the best initial guess, the overall parsing accuracy
of this method lags behind. This result demon-
strates the importance of restarts – in contrast to
the randomized strategies, the MAP initialization
performs only a single run of hill-climbing.
</bodyText>
<page confidence="0.957132">
1020
</page>
<table confidence="0.995545">
Length ≤ 15 Length &gt; 15
Slovene 100 98.11
English 100 99.12
</table>
<tableCaption confidence="0.863606666666667">
Table 6: Fractions (%) of the sentences that find
the best solution among 3,000 restarts within the
first 300 restarts.
</tableCaption>
<figure confidence="0.999031">
0 200 400 600 800 1000
# Restarts
(a) Slovene
0 200 400 600 800 1000
# Restarts
(b) English
</figure>
<figureCaption confidence="0.9244235">
Figure 5: Convergence analysis on Slovene and
English datasets. The graph shows the normalized
</figureCaption>
<bodyText confidence="0.943500869565217">
score of the output tree as a function of the number
of restarts. The score of each sentence is normal-
ized by the highest score obtained for this sentence
after 3,000 restarts. We only show the curves up to
1,000 restarts because they all reach convergence
after around 500 restarts.
Convergence Properties Figure 5 shows the
score of the trees retrieved by our full model with
respect to the number of restarts, for short and long
sentences in English and Slovene. To facilitate the
comparison, we normalize the score of each sen-
tence by the maximal score obtained for this sen-
tence after 3,000 restarts. Overall, most sentences
converge quickly. This view is also supported by
Table 6 which shows the fraction of the sentences
that converge within the first 300 restarts. We can
see that all the short sentences (length up to 15)
reach convergence within the allocated restarts.
Perhaps surprisingly, more than 98% of the long
sentences also converge within 300 restarts.
Decoding Speed As the number of restarts im-
pacts the parsing accuracy, we can trade perfor-
mance for speed. Figure 6 shows that the model
</bodyText>
<figure confidence="0.9997015">
(a) Slovene
2 4 6 8 10
Sec/Tok x 10−3
(b) English
</figure>
<figureCaption confidence="0.526784833333333">
Figure 6: Trade-off between performance and
speed on Slovene and English datasets. The graph
shows the accuracy as a function of decoding
speed measured in second per token. Variations in
decoding speed is achieved by changing the num-
ber of restarts.
</figureCaption>
<bodyText confidence="0.999816777777778">
achieves high performance with acceptable pars-
ing speed. While various system implementation
issues such as programming language and com-
putational platform complicate a direct compari-
son with other parsing systems, our model deliv-
ers parsing time roughly comparable to other state-
of-the-art graph-based systems (for example, Tur-
boParser and MST parser) and the sampling-based
parser.
</bodyText>
<sectionHeader confidence="0.993936" genericHeader="method">
7 Conclusions
</sectionHeader>
<bodyText confidence="0.999951866666667">
We have shown that a simple, generally appli-
cable randomized greedy algorithm for inference
suffices to deliver state-of-the-art parsing perfor-
mance. We argued that the effectiveness of such
greedy algorithms is contingent on having a small
number of local optima in the scoring function. By
algorithmically counting the number of locally op-
timal solutions in the context of first-order parsing,
we show that this number is indeed quite small.
Moreover, we show that, as a decoding algorithm,
the greedy method surpasses dual decomposition
in second-order parsing. Finally, we empirically
demonstrate that our approach with up to third-
order and global features outperforms the state-of-
the-art parsers when evaluated on 14 languages of
</bodyText>
<figure confidence="0.9941032">
Score
0.998
0.996
0.994
1
len≤15
len&gt;15
Score
0.998
0.996
0.994
1
len≤15
len&gt;15
2 4 6 8 10 12 14
Sec/Tok x 10−3
3rd−order Model
Full Model
88
86
UAS
84
82
3rd−order Model
Full Model
94
92
UAS
90
88
</figure>
<page confidence="0.891135">
1021
</page>
<bodyText confidence="0.346656">
non-projective CoNLL datasets.
</bodyText>
<sectionHeader confidence="0.917131" genericHeader="conclusions">
Appendix
</sectionHeader>
<bodyText confidence="0.9998245">
We provide here a more detailed justification for
the counting algorithm in Figure 2 and, by exten-
sion, a proof sketch of Theorem 1. The bullets
below follow the operation of the algorithm.
</bodyText>
<listItem confidence="0.934167333333333">
• Whenever independent selection of the heads
results in a valid tree, there is only 1 opti-
mum (Lines 1&amp;2 of the algorithm). Other-
wise there must be a cycle C in y (Line 3 of
the algorithm)
• We claim that any locally optimal tree y&apos; of
</listItem>
<bodyText confidence="0.898133555555556">
the graph G = (V, E) must contain |C |− 1
arcs of the cycle C C_ V . This can be shown
by contradiction. If y&apos; contains less than
|C |− 1 arcs of C, then (a) we can construct
a tree y&apos;&apos; that contains |C |− 1 arcs; (b) the
heads in y&apos;&apos; are strictly better than those in
y&apos; over the unused part of the cycle; (c) by
reachability, there is a path y&apos; — y&apos;&apos; so y&apos;
cannot be a local optimum.
</bodyText>
<listItem confidence="0.87203525">
• Any locally optimal tree in G must select an
arc in C and reassign it. The rest of the |C|−1
arcs will then result in a chain.
• By contracting cycle C we obtain a new
graph G&apos; of size |G |− |C |+ 1 (Lines 5-11
of the algorithm). Easy to verify that (not
shown): any local optimum in G&apos; is a local
optimum in G and vice versa.
</listItem>
<bodyText confidence="0.994088666666667">
The theorem follows as a corollary of these
steps. To see this, let F(Gm) be the number of
local optima in the graph of size m:
</bodyText>
<equation confidence="0.94415175">
�
F(Gm) &lt; max F(G(i)m−c+1)
CCV (G)
i
</equation>
<bodyText confidence="0.94169925">
where G(i)m−c+1 is the graph (of size m − c + 1)
created by selecting the ith arc in cycle C and con-
tracting Gm accordingly, and c = |C |is the size
of the cycle. Define Fˆ(m) as the upper bound of
F(Gm) for any graph of size m. By the above
formula, we know that
Fˆ(m − c + 1) x c
By solving for Fˆ (m) we get Fˆ (m) &lt; 2m−2. Since
m = n+1 for a sentence with n words, the upper-
bound of local optima is 2n−1.
To show the tightness, for any n &gt; 0, create
the graph Gn+1 with arc scores eij = eji = i for
any 0 &lt; i &lt; j &lt; n. Note that wn — wn−1 —
wn forms the circle C of size 2, it can be shown
by induction on n and F (Gn+1) that F (Gn+1) =
F(Gn) x 2 = 2n−1.
</bodyText>
<sectionHeader confidence="0.997413" genericHeader="acknowledgments">
Acknowledgments
</sectionHeader>
<bodyText confidence="0.99996925">
This research is developed in collaboration with
the Arabic Language Technologies (ALT) group
at Qatar Computing Research Institute (QCRI)
within the IYAS project. The authors acknowl-
edge the support of the U.S. Army Research Of-
fice under grant number W911NF-10-1-0533, and
of the DARPA BOLT program. We thank the MIT
NLP group and the ACL reviewers for their com-
ments. Any opinions, findings, conclusions, or
recommendations expressed in this paper are those
of the authors, and do not necessarily reflect the
views of the funding organizations.
</bodyText>
<sectionHeader confidence="0.997848" genericHeader="references">
References
</sectionHeader>
<reference confidence="0.963014466666667">
Nina Amenta and G¨unter Ziegler, 1999. Deformed
Products and Maximal Shadows of Polytopes. Con-
temporary Mathematics. American Mathematics So-
ciety.
Peter F. Brown, Vincent J Della Pietra, Stephen A Della
Pietra, and Robert L. Mercer. 1993. The mathemat-
ics of statistical machine translation: Parameter esti-
mation. Computational linguistics, 19(2):263–311.
Sabine Buchholz and Erwin Marsi. 2006. CoNLL-X
shared task on multilingual dependency parsing. In
Proceedings of the Tenth Conference on Computa-
tional Natural Language Learning, CoNLL-X ’06.
Association for Computational Linguistics.
Eugene Charniak and Mark Johnson. 2005. Coarse-
to-fine n-best parsing and maxent discriminative
reranking. In Proceedings of the 43rd Annual Meet-
ing on Association for Computational Linguistics,
pages 173–180. Association for Computational Lin-
guistics.
Yoeng-Jin Chu and Tseng-Hong Liu. 1965. On the
shortest arborescence of a directed graph. Scientia
Sinica, 14(10):1396.
Michael Collins. 2000. Discriminative reranking for
natural language parsing. In Proceedings of the
Seventeenth International Conference on Machine
Learning, ICML ’00, pages 175–182.
Michael Collins. 2002. Discriminative training meth-
ods for hidden markov models: Theory and exper-
iments with perceptron algorithms. In Proceedings
of the Conference on Empirical Methods in Natural
</reference>
<figure confidence="0.766728">
Fˆ(m) &lt; max
2&lt;c&lt;m
</figure>
<page confidence="0.976565">
1022
</page>
<reference confidence="0.996817816326531">
Language Processing - Volume 10, EMNLP ’02. As-
sociation for Computational Linguistics.
Koby Crammer, Ofer Dekel, Joseph Keshet, Shai
Shalev-Shwartz, and Yoram Singer. 2006. Online
passive-aggressive algorithms. The Journal of Ma-
chine Learning Research.
Hal Daum´e III, John Langford, and Daniel Marcu.
2009. Search-based structured prediction. Machine
learning, 75(3):297–325.
Anoop Deoras, Tom´aˇs Mikolov, and Kenneth Church.
2011. A fast re-scoring strategy to capture long dis-
tance dependencies. In Proceedings of the Confer-
ence on Empirical Methods in Natural Language
Processing (EMNLP), pages 1116–1127. Associa-
tion for Computational Linguistics.
Adrian Dumitrescu and Csaba D T´oth. 2013. The trav-
eling salesman problem for lines, balls and planes.
In SODA, pages 828–843. SIAM.
Yoav Goldberg and Michael Elhadad. 2010. An effi-
cient algorithm for easy-first non-directional depen-
dency parsing. In Human Language Technologies:
The 2010 Annual Conference of the North American
Chapter of the Association for Computational Lin-
guistics, pages 742–750. Association for Computa-
tional Linguistics.
Michael Held and Richard M. Karp. 1970. The
traveling-salesman problem and minimum spanning
trees. Operations Research, 18(6):1138–1162.
Dorit S. Hochbaum. 1982. Approximation algo-
rithms for the set covering and vertex cover prob-
lems. SIAM Journal on Computing, 11(3):555–556.
Liang Huang and Kenji Sagae. 2010. Dynamic pro-
gramming for linear-time incremental parsing. In
Proceedings of the 48th Annual Meeting of the Asso-
ciation for Computational Linguistics, pages 1077–
1086. Association for Computational Linguistics.
Liang Huang, Suphan Fayong, and Yang Guo. 2012.
Structured perceptron with inexact search. In Pro-
ceedings of the 2012 Conference of the North Amer-
ican Chapter of the Association for Computational
Linguistics: Human Language Technologies, pages
142–151. Association for Computational Linguis-
tics.
Liang Huang. 2008. Forest reranking: Discriminative
parsing with non-local features. In ACL, pages 586–
594.
Richard Johansson and Pierre Nugues. 2007. Incre-
mental dependency parsing using online learning. In
EMNLP-CoNLL, pages 1134–1138.
Peter Jonsson, Victor Lagerkvist, Gustav Nordh, and
Bruno Zanuttini. 2013. Complexity of sat problems,
clone theory and the exponential time hypothesis. In
SODA, pages 1264–1277. SIAM.
Terry Koo, Alexander M. Rush, Michael Collins,
Tommi Jaakkola, and David Sontag. 2010. Dual
decomposition for parsing with non-projective head
automata. In Proceedings of the 2010 Conference on
Empirical Methods in Natural Language Process-
ing. Association for Computational Linguistics.
Tao Lei, Yu Xin, Yuan Zhang, Regina Barzilay, and
Tommi Jaakkola. 2014. Low-rank tensors for scor-
ing dependency structures. In Proceedings of the
52th Annual Meeting of the Association for Compu-
tational Linguistics. Association for Computational
Linguistics.
Percy Liang, Hal Daum´e III, and Dan Klein. 2008.
Structure compilation: trading structure for features.
In Proceedings of the 25th international conference
on Machine learning, pages 592–599. ACM.
Andr´e F. T. Martins, Noah A. Smith, Pedro M. Q.
Aguiar, and M´ario A. T. Figueiredo. 2011. Dual de-
composition with many overlapping components. In
Proceedings of the Conference on Empirical Meth-
ods in Natural Language Processing, EMNLP ’11.
Association for Computational Linguistics.
Andr´e F. T. Martins, Miguel B. Almeida, and Noah A.
Smith. 2013. Turning on the turbo: Fast third-order
non-projective turbo parsers. In Proceedings of the
51th Annual Meeting of the Association for Compu-
tational Linguistics. Association for Computational
Linguistics.
Ryan McDonald and Fernando Pereira. 2006. Online
learning of approximate dependency parsing algo-
rithms. In EACL.
Ryan McDonald, Koby Crammer, and Fernando
Pereira. 2005. Online large-margin training of de-
pendency parsers. In Proceedings of the 43rd An-
nual Meeting of the Association for Computational
Linguistics (ACL’05).
David Mitchell, Bart Selman, and Hector Levesque.
1992. Hard and easy distributions of sat problems.
In AAAI, volume 92, pages 459–465. Citeseer.
Robert C. Moore and Chris Quirk. 2008. Random
restarts in minimum error rate training for statis-
tical machine translation. In Proceedings of the
22nd International Conference on Computational
Linguistics-Volume 1, pages 585–592. Association
for Computational Linguistics.
Tetsuji Nakagawa. 2007. Multilingual dependency
parsing using global features. In EMNLP-CoNLL,
pages 952–956.
Sujith Ravi and Kevin Knight. 2010. Does giza++
make search errors? Computational Linguistics,
36(3):295–302.
C´esar Rego, Dorabela Gamboa, Fred Glover, and Colin
Osterman. 2011. Traveling salesman problem
heuristics: leading methods, implementations and
latest advances. European Journal of Operational
Research, 211(3):427–441.
1023
Mauricio G. C. Resende, L. S. Pitsoulis, and P. M.
Pardalos. 1997. Approximate solution of weighted
max-sat problems using grasp. Satisfiability prob-
lems, 35:393–405.
Daniel Spielman and Shang-Hua Teng. 2001.
Smoothed analysis of algorithms: Why the simplex
algorithm usually takes polynomial time. In Pro-
ceedings of the thirty-third annual ACM symposium
on Theory of computing, pages 296–305. ACM.
Mihai Surdeanu, Richard Johansson, Adam Meyers,
Llu´ıs M`arquez, and Joakim Nivre. 2008. The
CoNLL-2008 shared task on joint parsing of syn-
tactic and semantic dependencies. In Proceedings
of the Twelfth Conference on Computational Natu-
ral Language Learning, CoNLL ’08. Association for
Computational Linguistics.
David B. Wilson. 1996. Generating random spanning
trees more quickly than the cover time. In Proceed-
ings of the twenty-eighth annual ACM symposium on
Theory of computing, pages 296–303. ACM.
Yue Zhang and Stephen Clark. 2008. A tale of
two parsers: investigating and combining graph-
based and transition-based dependency parsing us-
ing beam-search. In Proceedings of the Conference
on Empirical Methods in Natural Language Pro-
cessing, pages 562–571. Association for Computa-
tional Linguistics.
Hao Zhang, Liang Zhao, Kai Huang, and Ryan Mc-
Donald. 2013. Online learning for inexact hyper-
graph search. In Proceedings of EMNLP.
Yuan Zhang, Tao Lei, Regina Barzilay, Tommi
Jaakkola, and Amir Globerson. 2014. Steps to ex-
cellence: Simple inference with refined scoring of
dependency trees. In Proceedings of the 52th An-
nual Meeting of the Association for Computational
Linguistics. Association for Computational Linguis-
tics.
</reference>
<page confidence="0.995412">
1024
</page>
</variant>
</algorithm>
<algorithm name="ParsHed" version="110505">
<variant no="0" confidence="0.368638">
<title confidence="0.999138">Greed is Good if Randomized: New Inference for Dependency Parsing</title>
<author confidence="0.984117">Tao Regina Barzilay</author>
<author confidence="0.984117">Tommi</author>
<affiliation confidence="0.977789">Computer Science and Artificial Intelligence Massachusetts Institute of</affiliation>
<email confidence="0.453349">taolei,regina,</email>
<abstract confidence="0.999915142857143">Dependency parsing with high-order features results in a provably hard decoding problem. A lot of work has gone into developing powerful optimization methods for solving these combinatorial problems. In contrast, we explore, analyze, and demonstrate that a substantially simpler randomized greedy inference algorithm already suffices for near optimal parsing: a) we analytically quantify the number of local optima that the greedy method has to overcome in the context of first-order parsing; b) we show that, as a decoding algorithm, the greedy method surpasses dual decomposition in second-order parsing; c) we empirically demonstrate that our approach with up to third-order and global features outperforms the state-of-the-art dual decomposition and MCMC sampling methods when evaluated on 14 languages</abstract>
<intro confidence="0.870346">non-projective CoNLL</intro>
</variant>
</algorithm>
<algorithm name="ParsCit" version="110505">
<citationList>
<citation valid="true">
<authors>
<author>Nina Amenta</author>
<author>G¨unter Ziegler</author>
</authors>
<title>Deformed Products and Maximal Shadows of Polytopes. Contemporary Mathematics.</title>
<date>1999</date>
<publisher>American Mathematics Society.</publisher>
<contexts>
<context position="8791" citStr="Amenta and Ziegler, 1999" startWordPosition="1343" endWordPosition="1346">ve body of research on greedy approximations for NP-hard problems. Examples of NP-hard problems with successful greedy approximations include the traveling saleman problem problem (Held and Karp, 1970; Rego et al., 2011), the MAX-SAT problem (Mitchell et al., 1992; Resende et al., 1997) and vertex cover (Hochbaum, 1982). While some greedy methods have poor worst-case complexity, many 1014 of them work remarkably well in practice. Despite the apparent simplicity of these algorithms, understanding their properties is challenging: often their “theoretical analyses are negative and inconclusive” (Amenta and Ziegler, 1999; Spielman and Teng, 2001). Identifying conditions under which approximations are provably optimal is an active area of research in computer science theory (Dumitrescu and T´oth, 2013; Jonsson et al., 2013). In NLP, randomized and greedy approximations have been successfully used across multiple applications, including machine translation and language modeling (Brown et al., 1993; Ravi and Knight, 2010; Daum´e III et al., 2009; Moore and Quirk, 2008; Deoras et al., 2011). In this paper, we study the properties of these approximations in the context of dependency parsing. 3 Method 3.1 Prelimina</context>
</contexts>
<marker>Amenta, Ziegler, 1999</marker>
<rawString>Nina Amenta and G¨unter Ziegler, 1999. Deformed Products and Maximal Shadows of Polytopes. Contemporary Mathematics. American Mathematics Society.</rawString>
</citation>
<citation valid="true">
<authors>
<author>Peter F Brown</author>
<author>Vincent J Della Pietra</author>
<author>Stephen A Della Pietra</author>
<author>Robert L Mercer</author>
</authors>
<title>The mathematics of statistical machine translation: Parameter estimation.</title>
<date>1993</date>
<journal>Computational linguistics,</journal>
<volume>19</volume>
<issue>2</issue>
<contexts>
<context position="9173" citStr="Brown et al., 1993" startWordPosition="1399" endWordPosition="1402">014 of them work remarkably well in practice. Despite the apparent simplicity of these algorithms, understanding their properties is challenging: often their “theoretical analyses are negative and inconclusive” (Amenta and Ziegler, 1999; Spielman and Teng, 2001). Identifying conditions under which approximations are provably optimal is an active area of research in computer science theory (Dumitrescu and T´oth, 2013; Jonsson et al., 2013). In NLP, randomized and greedy approximations have been successfully used across multiple applications, including machine translation and language modeling (Brown et al., 1993; Ravi and Knight, 2010; Daum´e III et al., 2009; Moore and Quirk, 2008; Deoras et al., 2011). In this paper, we study the properties of these approximations in the context of dependency parsing. 3 Method 3.1 Preliminaries Let x be a sentence and T (x) be the set of possible dependency trees over the words in x. We use y ∈ T (x) to denote a dependency tree for x, and y(m) to specify the head (parent) of the modifier word indexed by m in tree y. We also use m to denote the indexed word when there is no ambiguity. In addition, we define T (y, m) as the set of “neighboring trees” of y obtained by</context>
</contexts>
<marker>Brown, Pietra, Pietra, Mercer, 1993</marker>
<rawString>Peter F. Brown, Vincent J Della Pietra, Stephen A Della Pietra, and Robert L. Mercer. 1993. The mathematics of statistical machine translation: Parameter estimation. Computational linguistics, 19(2):263–311.</rawString>
</citation>
<citation valid="true">
<authors>
<author>Sabine Buchholz</author>
<author>Erwin Marsi</author>
</authors>
<title>CoNLL-X shared task on multilingual dependency parsing.</title>
<date>2006</date>
<booktitle>In Proceedings of the Tenth Conference on Computational Natural Language Learning, CoNLL-X ’06. Association for Computational Linguistics.</booktitle>
<contexts>
<context position="25679" citStr="Buchholz and Marsi, 2006" startWordPosition="4293" endWordPosition="4296">e trained within the max-margin framework. As a result, we are expected to find the highest scoring competing tree for each training sentence (the “strongest violation”). One may question therefore whether possible sub-optimal decoding for some training sentences (finding “a violation” rather than the “strongest violation”) impacts the learned parser. To this end, Huang et al. (2012) have established that weaker violations do suffice for separable training sets. 5 Experimental Setup Dataset and Evaluation Measures We evaluate our model on CoNLL dependency treebanks for 14 different languages (Buchholz and Marsi, 2006; Surdeanu et al., 2008), using standard training and testing splits. We use part-of-speech tags and the morphological information provided in the corpus. Following standard practice, we use Unlabeled Attachment Score (UAS) excluding punctuation (Koo et al., 2010; Martins et al., 2013) as the evaluation metric in all our experiments. Baselines We compare our model with the TurboParser (Martins et al., 2013) and our earlier sampling-based parser (Zhang et al., 2014). For both parsers, we directly compare with the recent published results on the CoNLL datasets. We also compare our parser against</context>
</contexts>
<marker>Buchholz, Marsi, 2006</marker>
<rawString>Sabine Buchholz and Erwin Marsi. 2006. CoNLL-X shared task on multilingual dependency parsing. In Proceedings of the Tenth Conference on Computational Natural Language Learning, CoNLL-X ’06. Association for Computational Linguistics.</rawString>
</citation>
<citation valid="true">
<authors>
<author>Eugene Charniak</author>
<author>Mark Johnson</author>
</authors>
<title>Coarseto-fine n-best parsing and maxent discriminative reranking.</title>
<date>2005</date>
<booktitle>In Proceedings of the 43rd Annual Meeting on Association for Computational Linguistics,</booktitle>
<pages>173--180</pages>
<publisher>Association</publisher>
<institution>for Computational Linguistics.</institution>
<contexts>
<context position="10185" citStr="Charniak and Johnson, 2005" startWordPosition="1596" endWordPosition="1599">the head (parent) of the modifier word indexed by m in tree y. We also use m to denote the indexed word when there is no ambiguity. In addition, we define T (y, m) as the set of “neighboring trees” of y obtained by changing only the head of the modifier, i.e. y(m). The dependency trees are scored according to S(x, y) = θ · φ(x, y), where θ is a vector of parameters and φ(x, y) is a sparse feature vector representation of tree y for sentence x. In this work, φ(x, y) will include up to third-order features as well as a range of global features commonly used in re-ranking methods (Collins, 2000; Charniak and Johnson, 2005; Huang, 2008). The parameters θ in the scoring function are estimated on the basis of a training set D = {(ˆxi, ˆyi)}Ni=1 of sentences ˆxiand the corresponding gold (target) trees ˆyi. We adopt a max-margin framework for this learning problem. Specifically, we aim to find parameter values that score the gold target trees higher than others: ∀i ∈ {1, · · · , N}, y ∈ T (ˆxi), S(ˆxi, ˆyi) ≥ S(ˆxi, y) + kˆyi − yk1 − ξi where ξi ≥ 0 is the slack variable (non-zero values are penalized against) and kˆyi − yk1 is the hamming distance between the gold tree ˆyi and a candidate parse y. In an online le</context>
</contexts>
<marker>Charniak, Johnson, 2005</marker>
<rawString>Eugene Charniak and Mark Johnson. 2005. Coarseto-fine n-best parsing and maxent discriminative reranking. In Proceedings of the 43rd Annual Meeting on Association for Computational Linguistics, pages 173–180. Association for Computational Linguistics.</rawString>
</citation>
<citation valid="true">
<authors>
<author>Yoeng-Jin Chu</author>
<author>Tseng-Hong Liu</author>
</authors>
<title>On the shortest arborescence of a directed graph.</title>
<date>1965</date>
<journal>Scientia Sinica,</journal>
<volume>14</volume>
<issue>10</issue>
<contexts>
<context position="20061" citStr="Chu and Liu, 1965" startWordPosition="3348" endWordPosition="3351">(0) = ∅ and y(i) = arg maxj eji; 2: if y is a tree (no cycle) then return 1; 3: Find a cycle C ⊂ V in y; 4: count = 0; // contract the cycle 5: create a vertex w*; 6: ∀j ∈/ C : e*j = maxkEC ekj; 7: for each vertex wi ∈ C do 8: ∀j ∈/ C : ej* = eji; 9: V &apos; = V ∪ {w*} \ C; 10: E&apos; = E ∪ {e*j, ej* |∀j ∈/ C} 11: count += CountOptima(G&apos; = hV &apos;, E&apos;i); 12: end for 13: return count; Figure 2: A recursive algorithm for counting local optima for a sentence with words w1, · · · , wn (first-order parsing). The algorithm resembles the Chu-Liu-Edmonds algorithm for finding the maximum directed spanning tree (Chu and Liu, 1965). decoding with learned scoring functions involves only a small number of local optima. In our case, a local optimum corresponds to a tree y where no single change of head y(m) results in a higher scoring tree. Clearly, the highest scoring tree is also a local optimum in this sense. If there were many such local optima, finding the one with the highest score would be challenging for a greedy algorithm, even with randomization. We begin with a worst case analysis and estab1017 Dataset Trained with Hill-Climbing (HC) Trained with Dual Decomposition (DD) %Cert (DD) SDD &gt;SHC SDD =SHC SDD GSHC %Cer</context>
</contexts>
<marker>Chu, Liu, 1965</marker>
<rawString>Yoeng-Jin Chu and Tseng-Hong Liu. 1965. On the shortest arborescence of a directed graph. Scientia Sinica, 14(10):1396.</rawString>
</citation>
<citation valid="true">
<authors>
<author>Michael Collins</author>
</authors>
<title>Discriminative reranking for natural language parsing.</title>
<date>2000</date>
<booktitle>In Proceedings of the Seventeenth International Conference on Machine Learning, ICML ’00,</booktitle>
<pages>175--182</pages>
<contexts>
<context position="10157" citStr="Collins, 2000" startWordPosition="1594" endWordPosition="1595">(m) to specify the head (parent) of the modifier word indexed by m in tree y. We also use m to denote the indexed word when there is no ambiguity. In addition, we define T (y, m) as the set of “neighboring trees” of y obtained by changing only the head of the modifier, i.e. y(m). The dependency trees are scored according to S(x, y) = θ · φ(x, y), where θ is a vector of parameters and φ(x, y) is a sparse feature vector representation of tree y for sentence x. In this work, φ(x, y) will include up to third-order features as well as a range of global features commonly used in re-ranking methods (Collins, 2000; Charniak and Johnson, 2005; Huang, 2008). The parameters θ in the scoring function are estimated on the basis of a training set D = {(ˆxi, ˆyi)}Ni=1 of sentences ˆxiand the corresponding gold (target) trees ˆyi. We adopt a max-margin framework for this learning problem. Specifically, we aim to find parameter values that score the gold target trees higher than others: ∀i ∈ {1, · · · , N}, y ∈ T (ˆxi), S(ˆxi, ˆyi) ≥ S(ˆxi, y) + kˆyi − yk1 − ξi where ξi ≥ 0 is the slack variable (non-zero values are penalized against) and kˆyi − yk1 is the hamming distance between the gold tree ˆyi and a candid</context>
</contexts>
<marker>Collins, 2000</marker>
<rawString>Michael Collins. 2000. Discriminative reranking for natural language parsing. In Proceedings of the Seventeenth International Conference on Machine Learning, ICML ’00, pages 175–182.</rawString>
</citation>
<citation valid="true">
<authors>
<author>Michael Collins</author>
</authors>
<title>Discriminative training methods for hidden markov models: Theory and experiments with perceptron algorithms.</title>
<date>2002</date>
<booktitle>In Proceedings of the Conference on Empirical Methods in Natural Language Processing - Volume 10, EMNLP ’02. Association for Computational Linguistics.</booktitle>
<contexts>
<context position="27400" citStr="Collins, 2002" startWordPosition="4570" endWordPosition="4571">an length, neighbors, valency and non-projective arcs features. Implementation Details Following standard practices, we train our model using the passiveaggressive online learning algorithm (MIRA) and parameter averaging (Crammer et al., 2006; 4We refer the readers to Zhang et al. (2014) and Lei et al. (2014) for the detailed definition of each feature template. arc consecutive sibling grandparent Figure 3: First- to third-order features. Figure 4: Absolute UAS improvement of our full model over the first-order model. Sentences in the test set are divided into 2 groups based on their lengths. Collins, 2002). By default we use an adaptive strategy for running the hill-climbing algorithm – for a given sentence we repeatedly run the algorithm in parallel5 until the best tree does not change for K = 300 consecutive restarts. For each restart, by default we initialize the tree y(0) by sampling from the first-order distribution using the current learned parameter values (and firstorder scores). We train our first-order and thirdorder model for 10 epochs and our full model for 20 epochs for all languages, and report the average performance across three independent runs. 6 Results Comparison with the Ba</context>
</contexts>
<marker>Collins, 2002</marker>
<rawString>Michael Collins. 2002. Discriminative training methods for hidden markov models: Theory and experiments with perceptron algorithms. In Proceedings of the Conference on Empirical Methods in Natural Language Processing - Volume 10, EMNLP ’02. Association for Computational Linguistics.</rawString>
</citation>
<citation valid="true">
<authors>
<author>Koby Crammer</author>
<author>Ofer Dekel</author>
<author>Joseph Keshet</author>
<author>Shai Shalev-Shwartz</author>
<author>Yoram Singer</author>
</authors>
<title>Online passive-aggressive algorithms.</title>
<date>2006</date>
<journal>The Journal of Machine Learning Research.</journal>
<contexts>
<context position="27028" citStr="Crammer et al., 2006" startWordPosition="4507" endWordPosition="4510">artins et al. (2011), Koo et al. (2010), Zhang et al. (2013) and our tensor-based parser (Lei et al., 2014). Features We use the same feature templates as in our prior work (Zhang et al., 2014; Lei et al., 2014)4. Figure 3 shows the first- to third-order feature templates that we use in our model. For the global features we use right-branching, coordination, PP attachment, span length, neighbors, valency and non-projective arcs features. Implementation Details Following standard practices, we train our model using the passiveaggressive online learning algorithm (MIRA) and parameter averaging (Crammer et al., 2006; 4We refer the readers to Zhang et al. (2014) and Lei et al. (2014) for the detailed definition of each feature template. arc consecutive sibling grandparent Figure 3: First- to third-order features. Figure 4: Absolute UAS improvement of our full model over the first-order model. Sentences in the test set are divided into 2 groups based on their lengths. Collins, 2002). By default we use an adaptive strategy for running the hill-climbing algorithm – for a given sentence we repeatedly run the algorithm in parallel5 until the best tree does not change for K = 300 consecutive restarts. For each </context>
</contexts>
<marker>Crammer, Dekel, Keshet, Shalev-Shwartz, Singer, 2006</marker>
<rawString>Koby Crammer, Ofer Dekel, Joseph Keshet, Shai Shalev-Shwartz, and Yoram Singer. 2006. Online passive-aggressive algorithms. The Journal of Machine Learning Research.</rawString>
</citation>
<citation valid="true">
<authors>
<author>Hal Daum´e John Langford</author>
<author>Daniel Marcu</author>
</authors>
<title>Search-based structured prediction.</title>
<date>2009</date>
<booktitle>Machine learning,</booktitle>
<pages>75--3</pages>
<marker>Langford, Marcu, 2009</marker>
<rawString>Hal Daum´e III, John Langford, and Daniel Marcu. 2009. Search-based structured prediction. Machine learning, 75(3):297–325.</rawString>
</citation>
<citation valid="true">
<authors>
<author>Anoop Deoras</author>
<author>Tom´aˇs Mikolov</author>
<author>Kenneth Church</author>
</authors>
<title>A fast re-scoring strategy to capture long distance dependencies.</title>
<date>2011</date>
<booktitle>In Proceedings of the Conference on Empirical Methods in Natural Language Processing (EMNLP),</booktitle>
<pages>1116--1127</pages>
<publisher>Association</publisher>
<institution>for Computational Linguistics.</institution>
<contexts>
<context position="9266" citStr="Deoras et al., 2011" startWordPosition="1416" endWordPosition="1419">rithms, understanding their properties is challenging: often their “theoretical analyses are negative and inconclusive” (Amenta and Ziegler, 1999; Spielman and Teng, 2001). Identifying conditions under which approximations are provably optimal is an active area of research in computer science theory (Dumitrescu and T´oth, 2013; Jonsson et al., 2013). In NLP, randomized and greedy approximations have been successfully used across multiple applications, including machine translation and language modeling (Brown et al., 1993; Ravi and Knight, 2010; Daum´e III et al., 2009; Moore and Quirk, 2008; Deoras et al., 2011). In this paper, we study the properties of these approximations in the context of dependency parsing. 3 Method 3.1 Preliminaries Let x be a sentence and T (x) be the set of possible dependency trees over the words in x. We use y ∈ T (x) to denote a dependency tree for x, and y(m) to specify the head (parent) of the modifier word indexed by m in tree y. We also use m to denote the indexed word when there is no ambiguity. In addition, we define T (y, m) as the set of “neighboring trees” of y obtained by changing only the head of the modifier, i.e. y(m). The dependency trees are scored according</context>
</contexts>
<marker>Deoras, Mikolov, Church, 2011</marker>
<rawString>Anoop Deoras, Tom´aˇs Mikolov, and Kenneth Church. 2011. A fast re-scoring strategy to capture long distance dependencies. In Proceedings of the Conference on Empirical Methods in Natural Language Processing (EMNLP), pages 1116–1127. Association for Computational Linguistics.</rawString>
</citation>
<citation valid="true">
<authors>
<author>Adrian Dumitrescu</author>
<author>Csaba D T´oth</author>
</authors>
<title>The traveling salesman problem for lines, balls and planes.</title>
<date>2013</date>
<booktitle>In SODA,</booktitle>
<pages>828--843</pages>
<publisher>SIAM.</publisher>
<marker>Dumitrescu, T´oth, 2013</marker>
<rawString>Adrian Dumitrescu and Csaba D T´oth. 2013. The traveling salesman problem for lines, balls and planes. In SODA, pages 828–843. SIAM.</rawString>
</citation>
<citation valid="true">
<authors>
<author>Yoav Goldberg</author>
<author>Michael Elhadad</author>
</authors>
<title>An efficient algorithm for easy-first non-directional dependency parsing.</title>
<date>2010</date>
<booktitle>In Human Language Technologies: The 2010 Annual Conference of the North American Chapter of the Association for Computational Linguistics,</booktitle>
<pages>742--750</pages>
<publisher>Association</publisher>
<institution>for Computational Linguistics.</institution>
<contexts>
<context position="6753" citStr="Goldberg and Elhadad, 2010" startWordPosition="1029" endWordPosition="1032">ient randomization of the starting point is critical. Only a small number of restarts suffices for finding (near) optimal parse trees. 2 Related Work Finding Optimal Structure in Parsing The use of rich-scoring functions in dependency parsing inevitably leads to the challenging combinatorial problem of finding the maximizing parse. In fact, McDonald and Pereira (2006) demonstrated that the task is provably NP-hard for non-projective second-order parsing. Not surprisingly, approximate inference has been at the center of parsing research. Examples of these approaches include easy-first parsing (Goldberg and Elhadad, 2010), inexact search (Johansson and Nugues, 2007; Zhang and Clark, 2008; Huang et al., 2012; Zhang et al., 2013), partial dynamic programming (Huang and Sagae, 2010) and dual decomposition (Koo et al., 2010; Martins et al., 2011). Our work is most closely related to the MCMC sampling-based approaches (Nakagawa, 2007; Zhang et al., 2014). In our earlier work, we developed a method that learns to take guided stochastic steps towards a high-scoring parse (Zhang et al., 2014). In the heart of that technique are sophisticated samplers for traversing the space of trees. In this paper, we demonstrate tha</context>
</contexts>
<marker>Goldberg, Elhadad, 2010</marker>
<rawString>Yoav Goldberg and Michael Elhadad. 2010. An efficient algorithm for easy-first non-directional dependency parsing. In Human Language Technologies: The 2010 Annual Conference of the North American Chapter of the Association for Computational Linguistics, pages 742–750. Association for Computational Linguistics.</rawString>
</citation>
<citation valid="true">
<authors>
<author>Michael Held</author>
<author>Richard M Karp</author>
</authors>
<title>The traveling-salesman problem and minimum spanning trees.</title>
<date>1970</date>
<journal>Operations Research,</journal>
<volume>18</volume>
<issue>6</issue>
<contexts>
<context position="8367" citStr="Held and Karp, 1970" startWordPosition="1280" endWordPosition="1283">n-projective approximation. Our experiments demonstrate that when hill-climbing is employed as a primary learning mechanism for high-order parsing, it exhibits different properties: the distribution for initialization does not play a major role in the final outcome, while the use of restarts contributes significantly to the quality of the resulting tree. Greedy Approximations for NP-hard Problems There is an expansive body of research on greedy approximations for NP-hard problems. Examples of NP-hard problems with successful greedy approximations include the traveling saleman problem problem (Held and Karp, 1970; Rego et al., 2011), the MAX-SAT problem (Mitchell et al., 1992; Resende et al., 1997) and vertex cover (Hochbaum, 1982). While some greedy methods have poor worst-case complexity, many 1014 of them work remarkably well in practice. Despite the apparent simplicity of these algorithms, understanding their properties is challenging: often their “theoretical analyses are negative and inconclusive” (Amenta and Ziegler, 1999; Spielman and Teng, 2001). Identifying conditions under which approximations are provably optimal is an active area of research in computer science theory (Dumitrescu and T´ot</context>
</contexts>
<marker>Held, Karp, 1970</marker>
<rawString>Michael Held and Richard M. Karp. 1970. The traveling-salesman problem and minimum spanning trees. Operations Research, 18(6):1138–1162.</rawString>
</citation>
<citation valid="true">
<authors>
<author>Dorit S Hochbaum</author>
</authors>
<title>Approximation algorithms for the set covering and vertex cover problems.</title>
<date>1982</date>
<journal>SIAM Journal on Computing,</journal>
<volume>11</volume>
<issue>3</issue>
<contexts>
<context position="2204" citStr="Hochbaum, 1982" startWordPosition="322" endWordPosition="323">arguing that a much simpler inference strategy suffices. In fact, we demonstrate that a randomized greedy method of inference surpasses the state-of-the-art performance in dependency parsing. ∗Both authors contributed equally. 1Our code is available at https://github.com/ taolei87/RBGParser. Our choice of a randomized greedy algorithm for parsing follows from a successful track record of such methods in other hard combinatorial problems. These conceptually simple and intuitive algorithms have delivered competitive approximations across a broad class of NP-hard problems ranging from set cover (Hochbaum, 1982) to MAX-SAT (Resende et al., 1997). Their success is predicated on the observation that most realizations of problems are much easier to solve than the worst-cases. A simpler algorithm will therefore suffice in typical cases. Evidence is accumulating that parsing problems may exhibit similar properties. For instance, methods such as dual decomposition offer certificates of optimality when the highest scoring tree is found. Across languages, dual decomposition has shown to lead to a certificate of optimality for the vast majority of the sentences (Koo et al., 2010; Martins et al., 2011). These </context>
<context position="8488" citStr="Hochbaum, 1982" startWordPosition="1302" endWordPosition="1303">or high-order parsing, it exhibits different properties: the distribution for initialization does not play a major role in the final outcome, while the use of restarts contributes significantly to the quality of the resulting tree. Greedy Approximations for NP-hard Problems There is an expansive body of research on greedy approximations for NP-hard problems. Examples of NP-hard problems with successful greedy approximations include the traveling saleman problem problem (Held and Karp, 1970; Rego et al., 2011), the MAX-SAT problem (Mitchell et al., 1992; Resende et al., 1997) and vertex cover (Hochbaum, 1982). While some greedy methods have poor worst-case complexity, many 1014 of them work remarkably well in practice. Despite the apparent simplicity of these algorithms, understanding their properties is challenging: often their “theoretical analyses are negative and inconclusive” (Amenta and Ziegler, 1999; Spielman and Teng, 2001). Identifying conditions under which approximations are provably optimal is an active area of research in computer science theory (Dumitrescu and T´oth, 2013; Jonsson et al., 2013). In NLP, randomized and greedy approximations have been successfully used across multiple </context>
</contexts>
<marker>Hochbaum, 1982</marker>
<rawString>Dorit S. Hochbaum. 1982. Approximation algorithms for the set covering and vertex cover problems. SIAM Journal on Computing, 11(3):555–556.</rawString>
</citation>
<citation valid="true">
<authors>
<author>Liang Huang</author>
<author>Kenji Sagae</author>
</authors>
<title>Dynamic programming for linear-time incremental parsing.</title>
<date>2010</date>
<booktitle>In Proceedings of the 48th Annual Meeting of the Association for Computational Linguistics,</booktitle>
<pages>1077--1086</pages>
<institution>Association for Computational Linguistics.</institution>
<contexts>
<context position="6914" citStr="Huang and Sagae, 2010" startWordPosition="1055" endWordPosition="1058">Structure in Parsing The use of rich-scoring functions in dependency parsing inevitably leads to the challenging combinatorial problem of finding the maximizing parse. In fact, McDonald and Pereira (2006) demonstrated that the task is provably NP-hard for non-projective second-order parsing. Not surprisingly, approximate inference has been at the center of parsing research. Examples of these approaches include easy-first parsing (Goldberg and Elhadad, 2010), inexact search (Johansson and Nugues, 2007; Zhang and Clark, 2008; Huang et al., 2012; Zhang et al., 2013), partial dynamic programming (Huang and Sagae, 2010) and dual decomposition (Koo et al., 2010; Martins et al., 2011). Our work is most closely related to the MCMC sampling-based approaches (Nakagawa, 2007; Zhang et al., 2014). In our earlier work, we developed a method that learns to take guided stochastic steps towards a high-scoring parse (Zhang et al., 2014). In the heart of that technique are sophisticated samplers for traversing the space of trees. In this paper, we demonstrate that a substantially simpler approach that starts from a tree drawn from the uniform distribution and uses hillclimbing for parameter updates achieves similar or hi</context>
</contexts>
<marker>Huang, Sagae, 2010</marker>
<rawString>Liang Huang and Kenji Sagae. 2010. Dynamic programming for linear-time incremental parsing. In Proceedings of the 48th Annual Meeting of the Association for Computational Linguistics, pages 1077– 1086. Association for Computational Linguistics.</rawString>
</citation>
<citation valid="true">
<authors>
<author>Liang Huang</author>
<author>Suphan Fayong</author>
<author>Yang Guo</author>
</authors>
<title>Structured perceptron with inexact search.</title>
<date>2012</date>
<booktitle>In Proceedings of the 2012 Conference of the North American Chapter of the Association for Computational Linguistics: Human Language Technologies,</booktitle>
<pages>142--151</pages>
<publisher>Association</publisher>
<institution>for Computational Linguistics.</institution>
<contexts>
<context position="6840" citStr="Huang et al., 2012" startWordPosition="1043" endWordPosition="1046">or finding (near) optimal parse trees. 2 Related Work Finding Optimal Structure in Parsing The use of rich-scoring functions in dependency parsing inevitably leads to the challenging combinatorial problem of finding the maximizing parse. In fact, McDonald and Pereira (2006) demonstrated that the task is provably NP-hard for non-projective second-order parsing. Not surprisingly, approximate inference has been at the center of parsing research. Examples of these approaches include easy-first parsing (Goldberg and Elhadad, 2010), inexact search (Johansson and Nugues, 2007; Zhang and Clark, 2008; Huang et al., 2012; Zhang et al., 2013), partial dynamic programming (Huang and Sagae, 2010) and dual decomposition (Koo et al., 2010; Martins et al., 2011). Our work is most closely related to the MCMC sampling-based approaches (Nakagawa, 2007; Zhang et al., 2014). In our earlier work, we developed a method that learns to take guided stochastic steps towards a high-scoring parse (Zhang et al., 2014). In the heart of that technique are sophisticated samplers for traversing the space of trees. In this paper, we demonstrate that a substantially simpler approach that starts from a tree drawn from the uniform distr</context>
<context position="25441" citStr="Huang et al. (2012)" startWordPosition="4258" endWordPosition="4261">gest only a few changes above and beyond the dominant first-order scores. Greedy al1018 gorithms are therefore likely to be able to leverage at least some of this potential. We demonstrate below that this is indeed so. Our methods are trained within the max-margin framework. As a result, we are expected to find the highest scoring competing tree for each training sentence (the “strongest violation”). One may question therefore whether possible sub-optimal decoding for some training sentences (finding “a violation” rather than the “strongest violation”) impacts the learned parser. To this end, Huang et al. (2012) have established that weaker violations do suffice for separable training sets. 5 Experimental Setup Dataset and Evaluation Measures We evaluate our model on CoNLL dependency treebanks for 14 different languages (Buchholz and Marsi, 2006; Surdeanu et al., 2008), using standard training and testing splits. We use part-of-speech tags and the morphological information provided in the corpus. Following standard practice, we use Unlabeled Attachment Score (UAS) excluding punctuation (Koo et al., 2010; Martins et al., 2013) as the evaluation metric in all our experiments. Baselines We compare our m</context>
</contexts>
<marker>Huang, Fayong, Guo, 2012</marker>
<rawString>Liang Huang, Suphan Fayong, and Yang Guo. 2012. Structured perceptron with inexact search. In Proceedings of the 2012 Conference of the North American Chapter of the Association for Computational Linguistics: Human Language Technologies, pages 142–151. Association for Computational Linguistics.</rawString>
</citation>
<citation valid="true">
<authors>
<author>Liang Huang</author>
</authors>
<title>Forest reranking: Discriminative parsing with non-local features.</title>
<date>2008</date>
<booktitle>In ACL,</booktitle>
<pages>586--594</pages>
<contexts>
<context position="10199" citStr="Huang, 2008" startWordPosition="1600" endWordPosition="1601">ifier word indexed by m in tree y. We also use m to denote the indexed word when there is no ambiguity. In addition, we define T (y, m) as the set of “neighboring trees” of y obtained by changing only the head of the modifier, i.e. y(m). The dependency trees are scored according to S(x, y) = θ · φ(x, y), where θ is a vector of parameters and φ(x, y) is a sparse feature vector representation of tree y for sentence x. In this work, φ(x, y) will include up to third-order features as well as a range of global features commonly used in re-ranking methods (Collins, 2000; Charniak and Johnson, 2005; Huang, 2008). The parameters θ in the scoring function are estimated on the basis of a training set D = {(ˆxi, ˆyi)}Ni=1 of sentences ˆxiand the corresponding gold (target) trees ˆyi. We adopt a max-margin framework for this learning problem. Specifically, we aim to find parameter values that score the gold target trees higher than others: ∀i ∈ {1, · · · , N}, y ∈ T (ˆxi), S(ˆxi, ˆyi) ≥ S(ˆxi, y) + kˆyi − yk1 − ξi where ξi ≥ 0 is the slack variable (non-zero values are penalized against) and kˆyi − yk1 is the hamming distance between the gold tree ˆyi and a candidate parse y. In an online learning setup, </context>
</contexts>
<marker>Huang, 2008</marker>
<rawString>Liang Huang. 2008. Forest reranking: Discriminative parsing with non-local features. In ACL, pages 586– 594.</rawString>
</citation>
<citation valid="true">
<authors>
<author>Richard Johansson</author>
<author>Pierre Nugues</author>
</authors>
<title>Incremental dependency parsing using online learning.</title>
<date>2007</date>
<booktitle>In EMNLP-CoNLL,</booktitle>
<pages>1134--1138</pages>
<contexts>
<context position="6797" citStr="Johansson and Nugues, 2007" startWordPosition="1035" endWordPosition="1038">ritical. Only a small number of restarts suffices for finding (near) optimal parse trees. 2 Related Work Finding Optimal Structure in Parsing The use of rich-scoring functions in dependency parsing inevitably leads to the challenging combinatorial problem of finding the maximizing parse. In fact, McDonald and Pereira (2006) demonstrated that the task is provably NP-hard for non-projective second-order parsing. Not surprisingly, approximate inference has been at the center of parsing research. Examples of these approaches include easy-first parsing (Goldberg and Elhadad, 2010), inexact search (Johansson and Nugues, 2007; Zhang and Clark, 2008; Huang et al., 2012; Zhang et al., 2013), partial dynamic programming (Huang and Sagae, 2010) and dual decomposition (Koo et al., 2010; Martins et al., 2011). Our work is most closely related to the MCMC sampling-based approaches (Nakagawa, 2007; Zhang et al., 2014). In our earlier work, we developed a method that learns to take guided stochastic steps towards a high-scoring parse (Zhang et al., 2014). In the heart of that technique are sophisticated samplers for traversing the space of trees. In this paper, we demonstrate that a substantially simpler approach that star</context>
</contexts>
<marker>Johansson, Nugues, 2007</marker>
<rawString>Richard Johansson and Pierre Nugues. 2007. Incremental dependency parsing using online learning. In EMNLP-CoNLL, pages 1134–1138.</rawString>
</citation>
<citation valid="true">
<authors>
<author>Peter Jonsson</author>
<author>Victor Lagerkvist</author>
<author>Gustav Nordh</author>
<author>Bruno Zanuttini</author>
</authors>
<title>Complexity of sat problems, clone theory and the exponential time hypothesis.</title>
<date>2013</date>
<booktitle>In SODA,</booktitle>
<pages>1264--1277</pages>
<publisher>SIAM.</publisher>
<contexts>
<context position="8997" citStr="Jonsson et al., 2013" startWordPosition="1374" endWordPosition="1377"> al., 2011), the MAX-SAT problem (Mitchell et al., 1992; Resende et al., 1997) and vertex cover (Hochbaum, 1982). While some greedy methods have poor worst-case complexity, many 1014 of them work remarkably well in practice. Despite the apparent simplicity of these algorithms, understanding their properties is challenging: often their “theoretical analyses are negative and inconclusive” (Amenta and Ziegler, 1999; Spielman and Teng, 2001). Identifying conditions under which approximations are provably optimal is an active area of research in computer science theory (Dumitrescu and T´oth, 2013; Jonsson et al., 2013). In NLP, randomized and greedy approximations have been successfully used across multiple applications, including machine translation and language modeling (Brown et al., 1993; Ravi and Knight, 2010; Daum´e III et al., 2009; Moore and Quirk, 2008; Deoras et al., 2011). In this paper, we study the properties of these approximations in the context of dependency parsing. 3 Method 3.1 Preliminaries Let x be a sentence and T (x) be the set of possible dependency trees over the words in x. We use y ∈ T (x) to denote a dependency tree for x, and y(m) to specify the head (parent) of the modifier word</context>
</contexts>
<marker>Jonsson, Lagerkvist, Nordh, Zanuttini, 2013</marker>
<rawString>Peter Jonsson, Victor Lagerkvist, Gustav Nordh, and Bruno Zanuttini. 2013. Complexity of sat problems, clone theory and the exponential time hypothesis. In SODA, pages 1264–1277. SIAM.</rawString>
</citation>
<citation valid="true">
<authors>
<author>Terry Koo</author>
<author>Alexander M Rush</author>
<author>Michael Collins</author>
<author>Tommi Jaakkola</author>
<author>David Sontag</author>
</authors>
<title>Dual decomposition for parsing with non-projective head automata.</title>
<date>2010</date>
<booktitle>In Proceedings of the 2010 Conference on Empirical Methods in Natural Language Processing. Association for Computational Linguistics.</booktitle>
<contexts>
<context position="2773" citStr="Koo et al., 2010" startWordPosition="411" endWordPosition="414">roblems ranging from set cover (Hochbaum, 1982) to MAX-SAT (Resende et al., 1997). Their success is predicated on the observation that most realizations of problems are much easier to solve than the worst-cases. A simpler algorithm will therefore suffice in typical cases. Evidence is accumulating that parsing problems may exhibit similar properties. For instance, methods such as dual decomposition offer certificates of optimality when the highest scoring tree is found. Across languages, dual decomposition has shown to lead to a certificate of optimality for the vast majority of the sentences (Koo et al., 2010; Martins et al., 2011). These remarkable results suggest that, as a combinatorial problem, parsing appears simpler than its broader complexity class would suggest. Indeed, we show that a simpler inference algorithm already suffices for superior results. In this paper, we introduce a randomized greedy algorithm that can be easily used with any rich scoring function. Starting with an initial tree drawn uniformly at random, the algorithm makes only local myopic changes to the parse tree in an attempt to climb the objective function. While a single run of the hill-climbing algorithm may indeed ge</context>
<context position="6955" citStr="Koo et al., 2010" startWordPosition="1063" endWordPosition="1066">nctions in dependency parsing inevitably leads to the challenging combinatorial problem of finding the maximizing parse. In fact, McDonald and Pereira (2006) demonstrated that the task is provably NP-hard for non-projective second-order parsing. Not surprisingly, approximate inference has been at the center of parsing research. Examples of these approaches include easy-first parsing (Goldberg and Elhadad, 2010), inexact search (Johansson and Nugues, 2007; Zhang and Clark, 2008; Huang et al., 2012; Zhang et al., 2013), partial dynamic programming (Huang and Sagae, 2010) and dual decomposition (Koo et al., 2010; Martins et al., 2011). Our work is most closely related to the MCMC sampling-based approaches (Nakagawa, 2007; Zhang et al., 2014). In our earlier work, we developed a method that learns to take guided stochastic steps towards a high-scoring parse (Zhang et al., 2014). In the heart of that technique are sophisticated samplers for traversing the space of trees. In this paper, we demonstrate that a substantially simpler approach that starts from a tree drawn from the uniform distribution and uses hillclimbing for parameter updates achieves similar or higher performance. Another related greedy </context>
<context position="23818" citStr="Koo et al. (2010)" startWordPosition="3992" endWordPosition="3995">only a small number of locally optimal trees. We would therefore expect the algorithm to work. We show the results in the second part of Table 2. For short sentences of length up to 15, our method finds the global optimum for all the test sentences. Success rates remain high even for longer test sentences. 4.2 Higher-Order Parsing Exact decoding with high-order features is known to be provably hard (McDonald et al., 2005). We begin our analysis here with a second-order (sibling/grandparent) model, and compare our randomized hill-climbing (HC) method to dual decomposition (DD), re-implementing Koo et al. (2010). Table 3 compares decoding quality for the two methods across four languages. Overall, in 97.8% of the sentences, HC obtains the same score as DD, in 1.3% of the cases HC finds a higher scoring tree, and in 0.9% of cases DD results in a better tree. The results follow the same pattern regardless of which method was used to train the scoring function. The average rate of certificates for DD was 92%. In over 99% of these sentences, HC reaches the same optimum. We expect that these observations about the success of HC carry over to other high-order parsing models for several reasons. First, a la</context>
<context position="25942" citStr="Koo et al., 2010" startWordPosition="4333" endWordPosition="4336">g “a violation” rather than the “strongest violation”) impacts the learned parser. To this end, Huang et al. (2012) have established that weaker violations do suffice for separable training sets. 5 Experimental Setup Dataset and Evaluation Measures We evaluate our model on CoNLL dependency treebanks for 14 different languages (Buchholz and Marsi, 2006; Surdeanu et al., 2008), using standard training and testing splits. We use part-of-speech tags and the morphological information provided in the corpus. Following standard practice, we use Unlabeled Attachment Score (UAS) excluding punctuation (Koo et al., 2010; Martins et al., 2013) as the evaluation metric in all our experiments. Baselines We compare our model with the TurboParser (Martins et al., 2013) and our earlier sampling-based parser (Zhang et al., 2014). For both parsers, we directly compare with the recent published results on the CoNLL datasets. We also compare our parser against the best published results for the individual languages in our datasets. This comparison set includes four additional parsers: Martins et al. (2011), Koo et al. (2010), Zhang et al. (2013) and our tensor-based parser (Lei et al., 2014). Features We use the same </context>
<context position="29771" citStr="Koo et al. (2010)" startWordPosition="4968" endWordPosition="4971"> 93.80 93.35 93.56 93.74 93.52 93.42 93.74 (LX14) Portuguese 91.12 92.07 92.60 92.36 91.16 92.69 92.41 93.03 (KR10) Slovene 84.29 86.48 87.06 86.72 84.15 86.01 86.82 86.95 (MS11) Spanish 85.52 87.87 88.17 88.75 85.59 85.59 88.24 88.24 (ZL14) Swedish 89.89 91.17 91.35 91.08 89.78 91.14 90.71 91.62 (ZH13) Turkish 76.57 76.80 76.13 76.68 76.40 76.90 77.21 77.55 (KR10) Average 87.75 89.10 89.24 89.44 87.79 88.72 89.23 89.58 Table 4: Results of our model and several state-of-the-art systems. “Best Published UAS” includes the most accurate parsers among Martins et al. (2011), Martins et al. (2013), Koo et al. (2010), Zhang et al. (2013), Lei et al. (2014) and Zhang et al. (2014). For the third-order model, we use the feature set of TurboParser (Martins et al., 2013). The full model combines features of our sampling-based parser (Zhang et al., 2014) and tensor features (Lei et al., 2014). Dataset MAP-1st Uniform Rnd-1st UAS Init. UAS Init. UAS Init. Slovene 85.2 80.1 86.7 13.7 86.7 34.2 Arabic 78.8 75.1 79.7 12.4 80.2 32.8 English 91.1 82.0 93.3 39.6 93.3 55.6 Chinese 87.2 75.3 93.2 36.8 93.0 54.5 Dutch 84.8 79.5 87.0 26.9 87.4 45.6 Average 85.4 78.4 88.0 25.9 88.1 44.5 Table 5: Comparison between differe</context>
</contexts>
<marker>Koo, Rush, Collins, Jaakkola, Sontag, 2010</marker>
<rawString>Terry Koo, Alexander M. Rush, Michael Collins, Tommi Jaakkola, and David Sontag. 2010. Dual decomposition for parsing with non-projective head automata. In Proceedings of the 2010 Conference on Empirical Methods in Natural Language Processing. Association for Computational Linguistics.</rawString>
</citation>
<citation valid="true">
<authors>
<author>Tao Lei</author>
<author>Yu Xin</author>
<author>Yuan Zhang</author>
<author>Regina Barzilay</author>
<author>Tommi Jaakkola</author>
</authors>
<title>Low-rank tensors for scoring dependency structures.</title>
<date>2014</date>
<booktitle>In Proceedings of the 52th Annual Meeting of the Association for Computational Linguistics. Association for Computational Linguistics.</booktitle>
<contexts>
<context position="11285" citStr="Lei et al. (2014)" startWordPosition="1797" endWordPosition="1800">ed against) and kˆyi − yk1 is the hamming distance between the gold tree ˆyi and a candidate parse y. In an online learning setup, parameters are updated successively after each sentence. Each update still requires us to find the “strongest violation”, i.e., a candidate tree y˜ that scores higher than the gold tree ˆyi: y˜ = arg max {S(ˆxi, y) + ky − ˆyik1} yET (ˆxi) The parameters are then revised so as to select against the offending ˜y. Instead of a standard parameter update based on y˜ as in perceptron, stochastic gradient descent, or passive-aggressive updates, our implementation follows Lei et al. (2014) where the first-order parameters are broken up into a tensor. Each tensor component is updated successively in combination with the parameters corresponding to MST features (McDonald et al., 2005) and higher-order features (when included).2 3.2 Algorithm During training and testing, the key combinatorial problem we must solve is that of decoding, i.e., finding the highest scoring tree y˜ ∈ T (x) for each sentence x (or ˆxi). In our notation, y˜ = arg max {θ · φ(ˆxi, y) + ky − ˆyik1} (train) yET (ˆxi) y˜ = arg max {θ · φ(x, y)} (test) yET (x) While the decoding problem with feature sets simila</context>
<context position="12812" citStr="Lei et al. (2014)" startWordPosition="2055" endWordPosition="2058">ing functions S(x, y) = θ · φ(x, y) are primarily “local”. By this we mean that head-modifier decisions could be made largely without considering the surrounding structure (the context). For example, in English an adjective and a determiner are typically attached to the following noun. We demonstrate the degree of locality in dependency parsing by comparing a first-order treebased parser to the parser that predicts each head word independently of others. Note that the independent prediction of dependency arcs does not necessarily give rise to a tree. The parameters of 2We refer the readers to Lei et al. (2014) for more details about the tensor scoring function and the online update. 1015 Dataset Indp. Pred Tree Pred Slovene 83.7 84.2 Arabic 79.0 79.2 Japanese 93.4 93.7 English 91.6 91.9 Average 86.9 87.3 Table 1: Head attachment accuracy of a first-order local classifier (left) and a first-order structural prediction model (right). The two types of models are trained using the same set of features. Input: parameter θ, sentence x Output: dependency tree y˜ 1: Randomly initialize tree y(0); 2: t = 0; 3: repeat 4: list = bottom-up node list of y(t); 5: for each word m in list do 6: y(t+1) = arg maxy∈T</context>
<context position="26515" citStr="Lei et al., 2014" startWordPosition="4429" endWordPosition="4432">UAS) excluding punctuation (Koo et al., 2010; Martins et al., 2013) as the evaluation metric in all our experiments. Baselines We compare our model with the TurboParser (Martins et al., 2013) and our earlier sampling-based parser (Zhang et al., 2014). For both parsers, we directly compare with the recent published results on the CoNLL datasets. We also compare our parser against the best published results for the individual languages in our datasets. This comparison set includes four additional parsers: Martins et al. (2011), Koo et al. (2010), Zhang et al. (2013) and our tensor-based parser (Lei et al., 2014). Features We use the same feature templates as in our prior work (Zhang et al., 2014; Lei et al., 2014)4. Figure 3 shows the first- to third-order feature templates that we use in our model. For the global features we use right-branching, coordination, PP attachment, span length, neighbors, valency and non-projective arcs features. Implementation Details Following standard practices, we train our model using the passiveaggressive online learning algorithm (MIRA) and parameter averaging (Crammer et al., 2006; 4We refer the readers to Zhang et al. (2014) and Lei et al. (2014) for the detailed d</context>
<context position="29811" citStr="Lei et al. (2014)" startWordPosition="4976" endWordPosition="4979">74 (LX14) Portuguese 91.12 92.07 92.60 92.36 91.16 92.69 92.41 93.03 (KR10) Slovene 84.29 86.48 87.06 86.72 84.15 86.01 86.82 86.95 (MS11) Spanish 85.52 87.87 88.17 88.75 85.59 85.59 88.24 88.24 (ZL14) Swedish 89.89 91.17 91.35 91.08 89.78 91.14 90.71 91.62 (ZH13) Turkish 76.57 76.80 76.13 76.68 76.40 76.90 77.21 77.55 (KR10) Average 87.75 89.10 89.24 89.44 87.79 88.72 89.23 89.58 Table 4: Results of our model and several state-of-the-art systems. “Best Published UAS” includes the most accurate parsers among Martins et al. (2011), Martins et al. (2013), Koo et al. (2010), Zhang et al. (2013), Lei et al. (2014) and Zhang et al. (2014). For the third-order model, we use the feature set of TurboParser (Martins et al., 2013). The full model combines features of our sampling-based parser (Zhang et al., 2014) and tensor features (Lei et al., 2014). Dataset MAP-1st Uniform Rnd-1st UAS Init. UAS Init. UAS Init. Slovene 85.2 80.1 86.7 13.7 86.7 34.2 Arabic 78.8 75.1 79.7 12.4 80.2 32.8 English 91.1 82.0 93.3 39.6 93.3 55.6 Chinese 87.2 75.3 93.2 36.8 93.0 54.5 Dutch 84.8 79.5 87.0 26.9 87.4 45.6 Average 85.4 78.4 88.0 25.9 88.1 44.5 Table 5: Comparison between different initialization strategies: (a) MAP-1s</context>
</contexts>
<marker>Lei, Xin, Zhang, Barzilay, Jaakkola, 2014</marker>
<rawString>Tao Lei, Yu Xin, Yuan Zhang, Regina Barzilay, and Tommi Jaakkola. 2014. Low-rank tensors for scoring dependency structures. In Proceedings of the 52th Annual Meeting of the Association for Computational Linguistics. Association for Computational Linguistics.</rawString>
</citation>
<citation valid="true">
<authors>
<author>Percy Liang</author>
<author>Hal Daum´e</author>
<author>Dan Klein</author>
</authors>
<title>Structure compilation: trading structure for features.</title>
<date>2008</date>
<booktitle>In Proceedings of the 25th international conference on Machine learning,</booktitle>
<pages>592--599</pages>
<publisher>ACM.</publisher>
<marker>Liang, Daum´e, Klein, 2008</marker>
<rawString>Percy Liang, Hal Daum´e III, and Dan Klein. 2008. Structure compilation: trading structure for features. In Proceedings of the 25th international conference on Machine learning, pages 592–599. ACM.</rawString>
</citation>
<citation valid="true">
<authors>
<author>Andr´e F T Martins</author>
<author>Noah A Smith</author>
<author>Pedro M Q Aguiar</author>
<author>M´ario A T Figueiredo</author>
</authors>
<title>Dual decomposition with many overlapping components.</title>
<date>2011</date>
<booktitle>In Proceedings of the Conference on Empirical Methods in Natural Language Processing, EMNLP ’11. Association for Computational Linguistics.</booktitle>
<contexts>
<context position="2796" citStr="Martins et al., 2011" startWordPosition="415" endWordPosition="418">om set cover (Hochbaum, 1982) to MAX-SAT (Resende et al., 1997). Their success is predicated on the observation that most realizations of problems are much easier to solve than the worst-cases. A simpler algorithm will therefore suffice in typical cases. Evidence is accumulating that parsing problems may exhibit similar properties. For instance, methods such as dual decomposition offer certificates of optimality when the highest scoring tree is found. Across languages, dual decomposition has shown to lead to a certificate of optimality for the vast majority of the sentences (Koo et al., 2010; Martins et al., 2011). These remarkable results suggest that, as a combinatorial problem, parsing appears simpler than its broader complexity class would suggest. Indeed, we show that a simpler inference algorithm already suffices for superior results. In this paper, we introduce a randomized greedy algorithm that can be easily used with any rich scoring function. Starting with an initial tree drawn uniformly at random, the algorithm makes only local myopic changes to the parse tree in an attempt to climb the objective function. While a single run of the hill-climbing algorithm may indeed get stuck in a locally op</context>
<context position="6978" citStr="Martins et al., 2011" startWordPosition="1067" endWordPosition="1070">ncy parsing inevitably leads to the challenging combinatorial problem of finding the maximizing parse. In fact, McDonald and Pereira (2006) demonstrated that the task is provably NP-hard for non-projective second-order parsing. Not surprisingly, approximate inference has been at the center of parsing research. Examples of these approaches include easy-first parsing (Goldberg and Elhadad, 2010), inexact search (Johansson and Nugues, 2007; Zhang and Clark, 2008; Huang et al., 2012; Zhang et al., 2013), partial dynamic programming (Huang and Sagae, 2010) and dual decomposition (Koo et al., 2010; Martins et al., 2011). Our work is most closely related to the MCMC sampling-based approaches (Nakagawa, 2007; Zhang et al., 2014). In our earlier work, we developed a method that learns to take guided stochastic steps towards a high-scoring parse (Zhang et al., 2014). In the heart of that technique are sophisticated samplers for traversing the space of trees. In this paper, we demonstrate that a substantially simpler approach that starts from a tree drawn from the uniform distribution and uses hillclimbing for parameter updates achieves similar or higher performance. Another related greedy inference method has be</context>
<context position="26428" citStr="Martins et al. (2011)" startWordPosition="4413" endWordPosition="4416">on provided in the corpus. Following standard practice, we use Unlabeled Attachment Score (UAS) excluding punctuation (Koo et al., 2010; Martins et al., 2013) as the evaluation metric in all our experiments. Baselines We compare our model with the TurboParser (Martins et al., 2013) and our earlier sampling-based parser (Zhang et al., 2014). For both parsers, we directly compare with the recent published results on the CoNLL datasets. We also compare our parser against the best published results for the individual languages in our datasets. This comparison set includes four additional parsers: Martins et al. (2011), Koo et al. (2010), Zhang et al. (2013) and our tensor-based parser (Lei et al., 2014). Features We use the same feature templates as in our prior work (Zhang et al., 2014; Lei et al., 2014)4. Figure 3 shows the first- to third-order feature templates that we use in our model. For the global features we use right-branching, coordination, PP attachment, span length, neighbors, valency and non-projective arcs features. Implementation Details Following standard practices, we train our model using the passiveaggressive online learning algorithm (MIRA) and parameter averaging (Crammer et al., 2006</context>
<context position="29729" citStr="Martins et al. (2011)" startWordPosition="4960" endWordPosition="4963"> 90.54 92.41 92.07 92.41 (MA13) Japanese 93.78 93.80 93.35 93.56 93.74 93.52 93.42 93.74 (LX14) Portuguese 91.12 92.07 92.60 92.36 91.16 92.69 92.41 93.03 (KR10) Slovene 84.29 86.48 87.06 86.72 84.15 86.01 86.82 86.95 (MS11) Spanish 85.52 87.87 88.17 88.75 85.59 85.59 88.24 88.24 (ZL14) Swedish 89.89 91.17 91.35 91.08 89.78 91.14 90.71 91.62 (ZH13) Turkish 76.57 76.80 76.13 76.68 76.40 76.90 77.21 77.55 (KR10) Average 87.75 89.10 89.24 89.44 87.79 88.72 89.23 89.58 Table 4: Results of our model and several state-of-the-art systems. “Best Published UAS” includes the most accurate parsers among Martins et al. (2011), Martins et al. (2013), Koo et al. (2010), Zhang et al. (2013), Lei et al. (2014) and Zhang et al. (2014). For the third-order model, we use the feature set of TurboParser (Martins et al., 2013). The full model combines features of our sampling-based parser (Zhang et al., 2014) and tensor features (Lei et al., 2014). Dataset MAP-1st Uniform Rnd-1st UAS Init. UAS Init. UAS Init. Slovene 85.2 80.1 86.7 13.7 86.7 34.2 Arabic 78.8 75.1 79.7 12.4 80.2 32.8 English 91.1 82.0 93.3 39.6 93.3 55.6 Chinese 87.2 75.3 93.2 36.8 93.0 54.5 Dutch 84.8 79.5 87.0 26.9 87.4 45.6 Average 85.4 78.4 88.0 25.9 88.</context>
</contexts>
<marker>Martins, Smith, Aguiar, Figueiredo, 2011</marker>
<rawString>Andr´e F. T. Martins, Noah A. Smith, Pedro M. Q. Aguiar, and M´ario A. T. Figueiredo. 2011. Dual decomposition with many overlapping components. In Proceedings of the Conference on Empirical Methods in Natural Language Processing, EMNLP ’11. Association for Computational Linguistics.</rawString>
</citation>
<citation valid="true">
<authors>
<author>Andr´e F T Martins</author>
<author>Miguel B Almeida</author>
<author>Noah A Smith</author>
</authors>
<title>Turning on the turbo: Fast third-order non-projective turbo parsers.</title>
<date>2013</date>
<booktitle>In Proceedings of the 51th Annual Meeting of the Association for Computational Linguistics. Association for Computational Linguistics.</booktitle>
<contexts>
<context position="5794" citStr="Martins et al., 2013" startWordPosition="885" endWordPosition="888">ndomized greedy method outperforms dual decomposition by finding higher scoring trees. For the sentences that dual decomposition is optimal (obtains a certificate), the greedy method finds the same solution in over 99% of the cases. Our simple inference algorithm is therefore likely to scale to higher-order parsing and we demonstrate empirically that this is indeed so. We validate our claim by evaluating the method on the CoNLL dependency benchmark that comprises treebanks from 14 languages. Averaged across all languages, our method outperforms state-of-the-art parsers, including TurboParser (Martins et al., 2013) and our earlier sampling-based parser (Zhang et al., 2014). On seven languages, we report the best published results. The method is not sensitive to initialization. In fact, drawing the initial tree uniformly at random results in the same performance as when initialized from a trained first-order distribution. In contrast, sufficient randomization of the starting point is critical. Only a small number of restarts suffices for finding (near) optimal parse trees. 2 Related Work Finding Optimal Structure in Parsing The use of rich-scoring functions in dependency parsing inevitably leads to the c</context>
<context position="25965" citStr="Martins et al., 2013" startWordPosition="4337" endWordPosition="4340">ther than the “strongest violation”) impacts the learned parser. To this end, Huang et al. (2012) have established that weaker violations do suffice for separable training sets. 5 Experimental Setup Dataset and Evaluation Measures We evaluate our model on CoNLL dependency treebanks for 14 different languages (Buchholz and Marsi, 2006; Surdeanu et al., 2008), using standard training and testing splits. We use part-of-speech tags and the morphological information provided in the corpus. Following standard practice, we use Unlabeled Attachment Score (UAS) excluding punctuation (Koo et al., 2010; Martins et al., 2013) as the evaluation metric in all our experiments. Baselines We compare our model with the TurboParser (Martins et al., 2013) and our earlier sampling-based parser (Zhang et al., 2014). For both parsers, we directly compare with the recent published results on the CoNLL datasets. We also compare our parser against the best published results for the individual languages in our datasets. This comparison set includes four additional parsers: Martins et al. (2011), Koo et al. (2010), Zhang et al. (2013) and our tensor-based parser (Lei et al., 2014). Features We use the same feature templates as in</context>
<context position="29752" citStr="Martins et al. (2013)" startWordPosition="4964" endWordPosition="4967">1 (MA13) Japanese 93.78 93.80 93.35 93.56 93.74 93.52 93.42 93.74 (LX14) Portuguese 91.12 92.07 92.60 92.36 91.16 92.69 92.41 93.03 (KR10) Slovene 84.29 86.48 87.06 86.72 84.15 86.01 86.82 86.95 (MS11) Spanish 85.52 87.87 88.17 88.75 85.59 85.59 88.24 88.24 (ZL14) Swedish 89.89 91.17 91.35 91.08 89.78 91.14 90.71 91.62 (ZH13) Turkish 76.57 76.80 76.13 76.68 76.40 76.90 77.21 77.55 (KR10) Average 87.75 89.10 89.24 89.44 87.79 88.72 89.23 89.58 Table 4: Results of our model and several state-of-the-art systems. “Best Published UAS” includes the most accurate parsers among Martins et al. (2011), Martins et al. (2013), Koo et al. (2010), Zhang et al. (2013), Lei et al. (2014) and Zhang et al. (2014). For the third-order model, we use the feature set of TurboParser (Martins et al., 2013). The full model combines features of our sampling-based parser (Zhang et al., 2014) and tensor features (Lei et al., 2014). Dataset MAP-1st Uniform Rnd-1st UAS Init. UAS Init. UAS Init. Slovene 85.2 80.1 86.7 13.7 86.7 34.2 Arabic 78.8 75.1 79.7 12.4 80.2 32.8 English 91.1 82.0 93.3 39.6 93.3 55.6 Chinese 87.2 75.3 93.2 36.8 93.0 54.5 Dutch 84.8 79.5 87.0 26.9 87.4 45.6 Average 85.4 78.4 88.0 25.9 88.1 44.5 Table 5: Compari</context>
<context position="31813" citStr="Martins et al., 2013" startWordPosition="5290" endWordPosition="5293">ults for seven languages. Another noteworthy comparison concerns firstorder parsers. As Table 4 shows, the exact and approximate versions of the first-order parser deliver almost identical performance. Impact of High-Order Features Table 4 shows that the model can effectively utilize high-order features. Comparing the average performance of the model variants, we see that the accuracy on the benchmark languages consistently improves when higher-order features are added. This characteristic of the randomized greedy parser is in line with findings about other state-of-the-art highorder parsers (Martins et al., 2013; Zhang et al., 2014). Figure 4 breaks down these gains based on the sentence length. As expected, on most languages high-order features are particularly helpful when parsing longer sentences. Impact of Initialization and Restarts Table 5 shows the impact of initialization on the model performance for several languages. We consider three strategies: the MAP estimate of the firstorder score from the model, uniform sampling and sampling from the first-order distribution. The accuracy of initial trees varies greatly, ranging from 78.4% for the MAP estimate to 25.9% and 44.5% for the latter random</context>
</contexts>
<marker>Martins, Almeida, Smith, 2013</marker>
<rawString>Andr´e F. T. Martins, Miguel B. Almeida, and Noah A. Smith. 2013. Turning on the turbo: Fast third-order non-projective turbo parsers. In Proceedings of the 51th Annual Meeting of the Association for Computational Linguistics. Association for Computational Linguistics.</rawString>
</citation>
<citation valid="true">
<authors>
<author>Ryan McDonald</author>
<author>Fernando Pereira</author>
</authors>
<title>Online learning of approximate dependency parsing algorithms.</title>
<date>2006</date>
<booktitle>In EACL.</booktitle>
<contexts>
<context position="1427" citStr="McDonald and Pereira, 2006" startWordPosition="206" endWordPosition="209">algorithm, the greedy method surpasses dual decomposition in second-order parsing; c) we empirically demonstrate that our approach with up to third-order and global features outperforms the state-of-the-art dual decomposition and MCMC sampling methods when evaluated on 14 languages of non-projective CoNLL datasets.1 1 Introduction Dependency parsing is typically guided by parameterized scoring functions that involve rich features exerting refined control over the choice of parse trees. As a consequence, finding the highest scoring parse tree is a provably hard combinatorial inference problem (McDonald and Pereira, 2006). Much of the recent work on parsing has focused on solving these problems using powerful optimization techniques. In this paper, we follow a different strategy, arguing that a much simpler inference strategy suffices. In fact, we demonstrate that a randomized greedy method of inference surpasses the state-of-the-art performance in dependency parsing. ∗Both authors contributed equally. 1Our code is available at https://github.com/ taolei87/RBGParser. Our choice of a randomized greedy algorithm for parsing follows from a successful track record of such methods in other hard combinatorial proble</context>
<context position="6496" citStr="McDonald and Pereira (2006)" startWordPosition="992" endWordPosition="995">ages, we report the best published results. The method is not sensitive to initialization. In fact, drawing the initial tree uniformly at random results in the same performance as when initialized from a trained first-order distribution. In contrast, sufficient randomization of the starting point is critical. Only a small number of restarts suffices for finding (near) optimal parse trees. 2 Related Work Finding Optimal Structure in Parsing The use of rich-scoring functions in dependency parsing inevitably leads to the challenging combinatorial problem of finding the maximizing parse. In fact, McDonald and Pereira (2006) demonstrated that the task is provably NP-hard for non-projective second-order parsing. Not surprisingly, approximate inference has been at the center of parsing research. Examples of these approaches include easy-first parsing (Goldberg and Elhadad, 2010), inexact search (Johansson and Nugues, 2007; Zhang and Clark, 2008; Huang et al., 2012; Zhang et al., 2013), partial dynamic programming (Huang and Sagae, 2010) and dual decomposition (Koo et al., 2010; Martins et al., 2011). Our work is most closely related to the MCMC sampling-based approaches (Nakagawa, 2007; Zhang et al., 2014). In our </context>
</contexts>
<marker>McDonald, Pereira, 2006</marker>
<rawString>Ryan McDonald and Fernando Pereira. 2006. Online learning of approximate dependency parsing algorithms. In EACL.</rawString>
</citation>
<citation valid="true">
<authors>
<author>Ryan McDonald</author>
<author>Koby Crammer</author>
<author>Fernando Pereira</author>
</authors>
<title>Online large-margin training of dependency parsers.</title>
<date>2005</date>
<booktitle>In Proceedings of the 43rd Annual Meeting of the Association for Computational Linguistics (ACL’05).</booktitle>
<contexts>
<context position="11482" citStr="McDonald et al., 2005" startWordPosition="1826" endWordPosition="1829"> update still requires us to find the “strongest violation”, i.e., a candidate tree y˜ that scores higher than the gold tree ˆyi: y˜ = arg max {S(ˆxi, y) + ky − ˆyik1} yET (ˆxi) The parameters are then revised so as to select against the offending ˜y. Instead of a standard parameter update based on y˜ as in perceptron, stochastic gradient descent, or passive-aggressive updates, our implementation follows Lei et al. (2014) where the first-order parameters are broken up into a tensor. Each tensor component is updated successively in combination with the parameters corresponding to MST features (McDonald et al., 2005) and higher-order features (when included).2 3.2 Algorithm During training and testing, the key combinatorial problem we must solve is that of decoding, i.e., finding the highest scoring tree y˜ ∈ T (x) for each sentence x (or ˆxi). In our notation, y˜ = arg max {θ · φ(ˆxi, y) + ky − ˆyik1} (train) yET (ˆxi) y˜ = arg max {θ · φ(x, y)} (test) yET (x) While the decoding problem with feature sets similar to ours has been shown to be NP-hard, many approximation algorithms work remarkably well. We commence with a motivating example. Locality and Parsing One possible reason for why greedy or other a</context>
<context position="23626" citStr="McDonald et al., 2005" startWordPosition="3963" endWordPosition="3966">algorithm indeed 3A proof sketch is given in Appendix. succeeds in finding the highest scoring trees with a learned first-order scoring function. We have established above that there are typically only a small number of locally optimal trees. We would therefore expect the algorithm to work. We show the results in the second part of Table 2. For short sentences of length up to 15, our method finds the global optimum for all the test sentences. Success rates remain high even for longer test sentences. 4.2 Higher-Order Parsing Exact decoding with high-order features is known to be provably hard (McDonald et al., 2005). We begin our analysis here with a second-order (sibling/grandparent) model, and compare our randomized hill-climbing (HC) method to dual decomposition (DD), re-implementing Koo et al. (2010). Table 3 compares decoding quality for the two methods across four languages. Overall, in 97.8% of the sentences, HC obtains the same score as DD, in 1.3% of the cases HC finds a higher scoring tree, and in 0.9% of cases DD results in a better tree. The results follow the same pattern regardless of which method was used to train the scoring function. The average rate of certificates for DD was 92%. In ov</context>
</contexts>
<marker>McDonald, Crammer, Pereira, 2005</marker>
<rawString>Ryan McDonald, Koby Crammer, and Fernando Pereira. 2005. Online large-margin training of dependency parsers. In Proceedings of the 43rd Annual Meeting of the Association for Computational Linguistics (ACL’05).</rawString>
</citation>
<citation valid="true">
<authors>
<author>David Mitchell</author>
<author>Bart Selman</author>
<author>Hector Levesque</author>
</authors>
<title>Hard and easy distributions of sat problems.</title>
<date>1992</date>
<booktitle>In AAAI,</booktitle>
<volume>92</volume>
<pages>459--465</pages>
<publisher>Citeseer.</publisher>
<contexts>
<context position="8431" citStr="Mitchell et al., 1992" startWordPosition="1291" endWordPosition="1294">hen hill-climbing is employed as a primary learning mechanism for high-order parsing, it exhibits different properties: the distribution for initialization does not play a major role in the final outcome, while the use of restarts contributes significantly to the quality of the resulting tree. Greedy Approximations for NP-hard Problems There is an expansive body of research on greedy approximations for NP-hard problems. Examples of NP-hard problems with successful greedy approximations include the traveling saleman problem problem (Held and Karp, 1970; Rego et al., 2011), the MAX-SAT problem (Mitchell et al., 1992; Resende et al., 1997) and vertex cover (Hochbaum, 1982). While some greedy methods have poor worst-case complexity, many 1014 of them work remarkably well in practice. Despite the apparent simplicity of these algorithms, understanding their properties is challenging: often their “theoretical analyses are negative and inconclusive” (Amenta and Ziegler, 1999; Spielman and Teng, 2001). Identifying conditions under which approximations are provably optimal is an active area of research in computer science theory (Dumitrescu and T´oth, 2013; Jonsson et al., 2013). In NLP, randomized and greedy ap</context>
</contexts>
<marker>Mitchell, Selman, Levesque, 1992</marker>
<rawString>David Mitchell, Bart Selman, and Hector Levesque. 1992. Hard and easy distributions of sat problems. In AAAI, volume 92, pages 459–465. Citeseer.</rawString>
</citation>
<citation valid="true">
<authors>
<author>Robert C Moore</author>
<author>Chris Quirk</author>
</authors>
<title>Random restarts in minimum error rate training for statistical machine translation.</title>
<date>2008</date>
<booktitle>In Proceedings of the 22nd International Conference on Computational Linguistics-Volume 1,</booktitle>
<pages>585--592</pages>
<publisher>Association</publisher>
<institution>for Computational Linguistics.</institution>
<contexts>
<context position="9244" citStr="Moore and Quirk, 2008" startWordPosition="1412" endWordPosition="1415">implicity of these algorithms, understanding their properties is challenging: often their “theoretical analyses are negative and inconclusive” (Amenta and Ziegler, 1999; Spielman and Teng, 2001). Identifying conditions under which approximations are provably optimal is an active area of research in computer science theory (Dumitrescu and T´oth, 2013; Jonsson et al., 2013). In NLP, randomized and greedy approximations have been successfully used across multiple applications, including machine translation and language modeling (Brown et al., 1993; Ravi and Knight, 2010; Daum´e III et al., 2009; Moore and Quirk, 2008; Deoras et al., 2011). In this paper, we study the properties of these approximations in the context of dependency parsing. 3 Method 3.1 Preliminaries Let x be a sentence and T (x) be the set of possible dependency trees over the words in x. We use y ∈ T (x) to denote a dependency tree for x, and y(m) to specify the head (parent) of the modifier word indexed by m in tree y. We also use m to denote the indexed word when there is no ambiguity. In addition, we define T (y, m) as the set of “neighboring trees” of y obtained by changing only the head of the modifier, i.e. y(m). The dependency tree</context>
</contexts>
<marker>Moore, Quirk, 2008</marker>
<rawString>Robert C. Moore and Chris Quirk. 2008. Random restarts in minimum error rate training for statistical machine translation. In Proceedings of the 22nd International Conference on Computational Linguistics-Volume 1, pages 585–592. Association for Computational Linguistics.</rawString>
</citation>
<citation valid="true">
<authors>
<author>Tetsuji Nakagawa</author>
</authors>
<title>Multilingual dependency parsing using global features.</title>
<date>2007</date>
<booktitle>In EMNLP-CoNLL,</booktitle>
<pages>952--956</pages>
<contexts>
<context position="7066" citStr="Nakagawa, 2007" startWordPosition="1082" endWordPosition="1083"> parse. In fact, McDonald and Pereira (2006) demonstrated that the task is provably NP-hard for non-projective second-order parsing. Not surprisingly, approximate inference has been at the center of parsing research. Examples of these approaches include easy-first parsing (Goldberg and Elhadad, 2010), inexact search (Johansson and Nugues, 2007; Zhang and Clark, 2008; Huang et al., 2012; Zhang et al., 2013), partial dynamic programming (Huang and Sagae, 2010) and dual decomposition (Koo et al., 2010; Martins et al., 2011). Our work is most closely related to the MCMC sampling-based approaches (Nakagawa, 2007; Zhang et al., 2014). In our earlier work, we developed a method that learns to take guided stochastic steps towards a high-scoring parse (Zhang et al., 2014). In the heart of that technique are sophisticated samplers for traversing the space of trees. In this paper, we demonstrate that a substantially simpler approach that starts from a tree drawn from the uniform distribution and uses hillclimbing for parameter updates achieves similar or higher performance. Another related greedy inference method has been used for non-projective dependency parsing (McDonald and Pereira, 2006). This method </context>
</contexts>
<marker>Nakagawa, 2007</marker>
<rawString>Tetsuji Nakagawa. 2007. Multilingual dependency parsing using global features. In EMNLP-CoNLL, pages 952–956.</rawString>
</citation>
<citation valid="true">
<authors>
<author>Sujith Ravi</author>
<author>Kevin Knight</author>
</authors>
<title>Does giza++ make search errors?</title>
<date>2010</date>
<journal>Computational Linguistics,</journal>
<volume>36</volume>
<issue>3</issue>
<contexts>
<context position="9196" citStr="Ravi and Knight, 2010" startWordPosition="1403" endWordPosition="1406">arkably well in practice. Despite the apparent simplicity of these algorithms, understanding their properties is challenging: often their “theoretical analyses are negative and inconclusive” (Amenta and Ziegler, 1999; Spielman and Teng, 2001). Identifying conditions under which approximations are provably optimal is an active area of research in computer science theory (Dumitrescu and T´oth, 2013; Jonsson et al., 2013). In NLP, randomized and greedy approximations have been successfully used across multiple applications, including machine translation and language modeling (Brown et al., 1993; Ravi and Knight, 2010; Daum´e III et al., 2009; Moore and Quirk, 2008; Deoras et al., 2011). In this paper, we study the properties of these approximations in the context of dependency parsing. 3 Method 3.1 Preliminaries Let x be a sentence and T (x) be the set of possible dependency trees over the words in x. We use y ∈ T (x) to denote a dependency tree for x, and y(m) to specify the head (parent) of the modifier word indexed by m in tree y. We also use m to denote the indexed word when there is no ambiguity. In addition, we define T (y, m) as the set of “neighboring trees” of y obtained by changing only the head</context>
</contexts>
<marker>Ravi, Knight, 2010</marker>
<rawString>Sujith Ravi and Kevin Knight. 2010. Does giza++ make search errors? Computational Linguistics, 36(3):295–302.</rawString>
</citation>
<citation valid="true">
<authors>
<author>C´esar Rego</author>
<author>Dorabela Gamboa</author>
<author>Fred Glover</author>
<author>Colin Osterman</author>
</authors>
<title>Traveling salesman problem heuristics: leading methods, implementations and latest advances.</title>
<date>2011</date>
<journal>European Journal of Operational Research,</journal>
<volume>211</volume>
<issue>3</issue>
<contexts>
<context position="8387" citStr="Rego et al., 2011" startWordPosition="1284" endWordPosition="1287">ation. Our experiments demonstrate that when hill-climbing is employed as a primary learning mechanism for high-order parsing, it exhibits different properties: the distribution for initialization does not play a major role in the final outcome, while the use of restarts contributes significantly to the quality of the resulting tree. Greedy Approximations for NP-hard Problems There is an expansive body of research on greedy approximations for NP-hard problems. Examples of NP-hard problems with successful greedy approximations include the traveling saleman problem problem (Held and Karp, 1970; Rego et al., 2011), the MAX-SAT problem (Mitchell et al., 1992; Resende et al., 1997) and vertex cover (Hochbaum, 1982). While some greedy methods have poor worst-case complexity, many 1014 of them work remarkably well in practice. Despite the apparent simplicity of these algorithms, understanding their properties is challenging: often their “theoretical analyses are negative and inconclusive” (Amenta and Ziegler, 1999; Spielman and Teng, 2001). Identifying conditions under which approximations are provably optimal is an active area of research in computer science theory (Dumitrescu and T´oth, 2013; Jonsson et </context>
</contexts>
<marker>Rego, Gamboa, Glover, Osterman, 2011</marker>
<rawString>C´esar Rego, Dorabela Gamboa, Fred Glover, and Colin Osterman. 2011. Traveling salesman problem heuristics: leading methods, implementations and latest advances. European Journal of Operational Research, 211(3):427–441.</rawString>
</citation>
<citation valid="true">
<authors>
<author>Mauricio G C Resende</author>
<author>L S Pitsoulis</author>
<author>P M Pardalos</author>
</authors>
<title>Approximate solution of weighted max-sat problems using grasp. Satisfiability problems,</title>
<date>1997</date>
<pages>35--393</pages>
<contexts>
<context position="2238" citStr="Resende et al., 1997" startWordPosition="326" endWordPosition="329">inference strategy suffices. In fact, we demonstrate that a randomized greedy method of inference surpasses the state-of-the-art performance in dependency parsing. ∗Both authors contributed equally. 1Our code is available at https://github.com/ taolei87/RBGParser. Our choice of a randomized greedy algorithm for parsing follows from a successful track record of such methods in other hard combinatorial problems. These conceptually simple and intuitive algorithms have delivered competitive approximations across a broad class of NP-hard problems ranging from set cover (Hochbaum, 1982) to MAX-SAT (Resende et al., 1997). Their success is predicated on the observation that most realizations of problems are much easier to solve than the worst-cases. A simpler algorithm will therefore suffice in typical cases. Evidence is accumulating that parsing problems may exhibit similar properties. For instance, methods such as dual decomposition offer certificates of optimality when the highest scoring tree is found. Across languages, dual decomposition has shown to lead to a certificate of optimality for the vast majority of the sentences (Koo et al., 2010; Martins et al., 2011). These remarkable results suggest that, a</context>
<context position="8454" citStr="Resende et al., 1997" startWordPosition="1295" endWordPosition="1298">ployed as a primary learning mechanism for high-order parsing, it exhibits different properties: the distribution for initialization does not play a major role in the final outcome, while the use of restarts contributes significantly to the quality of the resulting tree. Greedy Approximations for NP-hard Problems There is an expansive body of research on greedy approximations for NP-hard problems. Examples of NP-hard problems with successful greedy approximations include the traveling saleman problem problem (Held and Karp, 1970; Rego et al., 2011), the MAX-SAT problem (Mitchell et al., 1992; Resende et al., 1997) and vertex cover (Hochbaum, 1982). While some greedy methods have poor worst-case complexity, many 1014 of them work remarkably well in practice. Despite the apparent simplicity of these algorithms, understanding their properties is challenging: often their “theoretical analyses are negative and inconclusive” (Amenta and Ziegler, 1999; Spielman and Teng, 2001). Identifying conditions under which approximations are provably optimal is an active area of research in computer science theory (Dumitrescu and T´oth, 2013; Jonsson et al., 2013). In NLP, randomized and greedy approximations have been </context>
</contexts>
<marker>Resende, Pitsoulis, Pardalos, 1997</marker>
<rawString>Mauricio G. C. Resende, L. S. Pitsoulis, and P. M. Pardalos. 1997. Approximate solution of weighted max-sat problems using grasp. Satisfiability problems, 35:393–405.</rawString>
</citation>
<citation valid="true">
<authors>
<author>Daniel Spielman</author>
<author>Shang-Hua Teng</author>
</authors>
<title>Smoothed analysis of algorithms: Why the simplex algorithm usually takes polynomial time.</title>
<date>2001</date>
<booktitle>In Proceedings of the thirty-third annual ACM symposium on Theory of computing,</booktitle>
<pages>296--305</pages>
<publisher>ACM.</publisher>
<contexts>
<context position="8817" citStr="Spielman and Teng, 2001" startWordPosition="1347" endWordPosition="1350">edy approximations for NP-hard problems. Examples of NP-hard problems with successful greedy approximations include the traveling saleman problem problem (Held and Karp, 1970; Rego et al., 2011), the MAX-SAT problem (Mitchell et al., 1992; Resende et al., 1997) and vertex cover (Hochbaum, 1982). While some greedy methods have poor worst-case complexity, many 1014 of them work remarkably well in practice. Despite the apparent simplicity of these algorithms, understanding their properties is challenging: often their “theoretical analyses are negative and inconclusive” (Amenta and Ziegler, 1999; Spielman and Teng, 2001). Identifying conditions under which approximations are provably optimal is an active area of research in computer science theory (Dumitrescu and T´oth, 2013; Jonsson et al., 2013). In NLP, randomized and greedy approximations have been successfully used across multiple applications, including machine translation and language modeling (Brown et al., 1993; Ravi and Knight, 2010; Daum´e III et al., 2009; Moore and Quirk, 2008; Deoras et al., 2011). In this paper, we study the properties of these approximations in the context of dependency parsing. 3 Method 3.1 Preliminaries Let x be a sentence a</context>
</contexts>
<marker>Spielman, Teng, 2001</marker>
<rawString>Daniel Spielman and Shang-Hua Teng. 2001. Smoothed analysis of algorithms: Why the simplex algorithm usually takes polynomial time. In Proceedings of the thirty-third annual ACM symposium on Theory of computing, pages 296–305. ACM.</rawString>
</citation>
<citation valid="true">
<authors>
<author>Mihai Surdeanu</author>
<author>Richard Johansson</author>
<author>Adam Meyers</author>
<author>Llu´ıs M`arquez</author>
<author>Joakim Nivre</author>
</authors>
<title>The CoNLL-2008 shared task on joint parsing of syntactic and semantic dependencies.</title>
<date>2008</date>
<booktitle>In Proceedings of the Twelfth Conference on Computational Natural Language Learning, CoNLL ’08. Association for Computational Linguistics.</booktitle>
<marker>Surdeanu, Johansson, Meyers, M`arquez, Nivre, 2008</marker>
<rawString>Mihai Surdeanu, Richard Johansson, Adam Meyers, Llu´ıs M`arquez, and Joakim Nivre. 2008. The CoNLL-2008 shared task on joint parsing of syntactic and semantic dependencies. In Proceedings of the Twelfth Conference on Computational Natural Language Learning, CoNLL ’08. Association for Computational Linguistics.</rawString>
</citation>
<citation valid="true">
<authors>
<author>David B Wilson</author>
</authors>
<title>Generating random spanning trees more quickly than the cover time.</title>
<date>1996</date>
<booktitle>In Proceedings of the twenty-eighth annual ACM symposium on Theory of computing,</booktitle>
<pages>296--303</pages>
<publisher>ACM.</publisher>
<contexts>
<context position="14593" citStr="Wilson, 1996" startWordPosition="2360" endWordPosition="2361">ang et al. (2008), where a local classifier was shown to achieve comparable accuracy to a sequential model (e.g. CRF) in POS tagging and namedentity recognition. Hill-Climbing with Random Restarts We build here on the motivating example and explore greedy algorithms as generalizations of purely local decoding. Greedy algorithms break the decoding problem into a sequence of simple local steps, each required to improve the solution. In our case, simple local steps correspond to choosing the head for each modifier word. We begin with a tree y(0), which can be a sample drawn uniformly from T (x) (Wilson, 1996). Our greedy algorithm then updates y(t) to a better tree y(t+1) by revising the head of one modifier word while maintaining the constraint that the resulting structure is a tree. The modifiers are considered in the bottom-up order relative to the current tree (the word furthest from the root is considered first). We provide an analysis to motivate this bottom-up update strategy in Section 4.1. The algorithm continues until the score can no longer be improved by changing the head of a single word. The resulting tree represents a locally optimal prediction relative to a single-arc greedy algori</context>
</contexts>
<marker>Wilson, 1996</marker>
<rawString>David B. Wilson. 1996. Generating random spanning trees more quickly than the cover time. In Proceedings of the twenty-eighth annual ACM symposium on Theory of computing, pages 296–303. ACM.</rawString>
</citation>
<citation valid="true">
<authors>
<author>Yue Zhang</author>
<author>Stephen Clark</author>
</authors>
<title>A tale of two parsers: investigating and combining graphbased and transition-based dependency parsing using beam-search.</title>
<date>2008</date>
<booktitle>In Proceedings of the Conference on Empirical Methods in Natural Language Processing,</booktitle>
<pages>562--571</pages>
<publisher>Association</publisher>
<institution>for Computational Linguistics.</institution>
<contexts>
<context position="6820" citStr="Zhang and Clark, 2008" startWordPosition="1039" endWordPosition="1042"> of restarts suffices for finding (near) optimal parse trees. 2 Related Work Finding Optimal Structure in Parsing The use of rich-scoring functions in dependency parsing inevitably leads to the challenging combinatorial problem of finding the maximizing parse. In fact, McDonald and Pereira (2006) demonstrated that the task is provably NP-hard for non-projective second-order parsing. Not surprisingly, approximate inference has been at the center of parsing research. Examples of these approaches include easy-first parsing (Goldberg and Elhadad, 2010), inexact search (Johansson and Nugues, 2007; Zhang and Clark, 2008; Huang et al., 2012; Zhang et al., 2013), partial dynamic programming (Huang and Sagae, 2010) and dual decomposition (Koo et al., 2010; Martins et al., 2011). Our work is most closely related to the MCMC sampling-based approaches (Nakagawa, 2007; Zhang et al., 2014). In our earlier work, we developed a method that learns to take guided stochastic steps towards a high-scoring parse (Zhang et al., 2014). In the heart of that technique are sophisticated samplers for traversing the space of trees. In this paper, we demonstrate that a substantially simpler approach that starts from a tree drawn fr</context>
</contexts>
<marker>Zhang, Clark, 2008</marker>
<rawString>Yue Zhang and Stephen Clark. 2008. A tale of two parsers: investigating and combining graphbased and transition-based dependency parsing using beam-search. In Proceedings of the Conference on Empirical Methods in Natural Language Processing, pages 562–571. Association for Computational Linguistics.</rawString>
</citation>
<citation valid="true">
<authors>
<author>Hao Zhang</author>
<author>Liang Zhao</author>
<author>Kai Huang</author>
<author>Ryan McDonald</author>
</authors>
<title>Online learning for inexact hypergraph search.</title>
<date>2013</date>
<booktitle>In Proceedings of EMNLP.</booktitle>
<contexts>
<context position="6861" citStr="Zhang et al., 2013" startWordPosition="1047" endWordPosition="1050">timal parse trees. 2 Related Work Finding Optimal Structure in Parsing The use of rich-scoring functions in dependency parsing inevitably leads to the challenging combinatorial problem of finding the maximizing parse. In fact, McDonald and Pereira (2006) demonstrated that the task is provably NP-hard for non-projective second-order parsing. Not surprisingly, approximate inference has been at the center of parsing research. Examples of these approaches include easy-first parsing (Goldberg and Elhadad, 2010), inexact search (Johansson and Nugues, 2007; Zhang and Clark, 2008; Huang et al., 2012; Zhang et al., 2013), partial dynamic programming (Huang and Sagae, 2010) and dual decomposition (Koo et al., 2010; Martins et al., 2011). Our work is most closely related to the MCMC sampling-based approaches (Nakagawa, 2007; Zhang et al., 2014). In our earlier work, we developed a method that learns to take guided stochastic steps towards a high-scoring parse (Zhang et al., 2014). In the heart of that technique are sophisticated samplers for traversing the space of trees. In this paper, we demonstrate that a substantially simpler approach that starts from a tree drawn from the uniform distribution and uses hill</context>
<context position="26468" citStr="Zhang et al. (2013)" startWordPosition="4421" endWordPosition="4424">ard practice, we use Unlabeled Attachment Score (UAS) excluding punctuation (Koo et al., 2010; Martins et al., 2013) as the evaluation metric in all our experiments. Baselines We compare our model with the TurboParser (Martins et al., 2013) and our earlier sampling-based parser (Zhang et al., 2014). For both parsers, we directly compare with the recent published results on the CoNLL datasets. We also compare our parser against the best published results for the individual languages in our datasets. This comparison set includes four additional parsers: Martins et al. (2011), Koo et al. (2010), Zhang et al. (2013) and our tensor-based parser (Lei et al., 2014). Features We use the same feature templates as in our prior work (Zhang et al., 2014; Lei et al., 2014)4. Figure 3 shows the first- to third-order feature templates that we use in our model. For the global features we use right-branching, coordination, PP attachment, span length, neighbors, valency and non-projective arcs features. Implementation Details Following standard practices, we train our model using the passiveaggressive online learning algorithm (MIRA) and parameter averaging (Crammer et al., 2006; 4We refer the readers to Zhang et al. </context>
<context position="29792" citStr="Zhang et al. (2013)" startWordPosition="4972" endWordPosition="4975">93.74 93.52 93.42 93.74 (LX14) Portuguese 91.12 92.07 92.60 92.36 91.16 92.69 92.41 93.03 (KR10) Slovene 84.29 86.48 87.06 86.72 84.15 86.01 86.82 86.95 (MS11) Spanish 85.52 87.87 88.17 88.75 85.59 85.59 88.24 88.24 (ZL14) Swedish 89.89 91.17 91.35 91.08 89.78 91.14 90.71 91.62 (ZH13) Turkish 76.57 76.80 76.13 76.68 76.40 76.90 77.21 77.55 (KR10) Average 87.75 89.10 89.24 89.44 87.79 88.72 89.23 89.58 Table 4: Results of our model and several state-of-the-art systems. “Best Published UAS” includes the most accurate parsers among Martins et al. (2011), Martins et al. (2013), Koo et al. (2010), Zhang et al. (2013), Lei et al. (2014) and Zhang et al. (2014). For the third-order model, we use the feature set of TurboParser (Martins et al., 2013). The full model combines features of our sampling-based parser (Zhang et al., 2014) and tensor features (Lei et al., 2014). Dataset MAP-1st Uniform Rnd-1st UAS Init. UAS Init. UAS Init. Slovene 85.2 80.1 86.7 13.7 86.7 34.2 Arabic 78.8 75.1 79.7 12.4 80.2 32.8 English 91.1 82.0 93.3 39.6 93.3 55.6 Chinese 87.2 75.3 93.2 36.8 93.0 54.5 Dutch 84.8 79.5 87.0 26.9 87.4 45.6 Average 85.4 78.4 88.0 25.9 88.1 44.5 Table 5: Comparison between different initialization str</context>
</contexts>
<marker>Zhang, Zhao, Huang, McDonald, 2013</marker>
<rawString>Hao Zhang, Liang Zhao, Kai Huang, and Ryan McDonald. 2013. Online learning for inexact hypergraph search. In Proceedings of EMNLP.</rawString>
</citation>
<citation valid="true">
<authors>
<author>Yuan Zhang</author>
<author>Tao Lei</author>
<author>Regina Barzilay</author>
<author>Tommi Jaakkola</author>
<author>Amir Globerson</author>
</authors>
<title>Steps to excellence: Simple inference with refined scoring of dependency trees.</title>
<date>2014</date>
<booktitle>In Proceedings of the 52th Annual Meeting of the Association for Computational Linguistics. Association for Computational Linguistics.</booktitle>
<contexts>
<context position="5853" citStr="Zhang et al., 2014" startWordPosition="894" endWordPosition="897">ing higher scoring trees. For the sentences that dual decomposition is optimal (obtains a certificate), the greedy method finds the same solution in over 99% of the cases. Our simple inference algorithm is therefore likely to scale to higher-order parsing and we demonstrate empirically that this is indeed so. We validate our claim by evaluating the method on the CoNLL dependency benchmark that comprises treebanks from 14 languages. Averaged across all languages, our method outperforms state-of-the-art parsers, including TurboParser (Martins et al., 2013) and our earlier sampling-based parser (Zhang et al., 2014). On seven languages, we report the best published results. The method is not sensitive to initialization. In fact, drawing the initial tree uniformly at random results in the same performance as when initialized from a trained first-order distribution. In contrast, sufficient randomization of the starting point is critical. Only a small number of restarts suffices for finding (near) optimal parse trees. 2 Related Work Finding Optimal Structure in Parsing The use of rich-scoring functions in dependency parsing inevitably leads to the challenging combinatorial problem of finding the maximizing </context>
<context position="7087" citStr="Zhang et al., 2014" startWordPosition="1084" endWordPosition="1087"> McDonald and Pereira (2006) demonstrated that the task is provably NP-hard for non-projective second-order parsing. Not surprisingly, approximate inference has been at the center of parsing research. Examples of these approaches include easy-first parsing (Goldberg and Elhadad, 2010), inexact search (Johansson and Nugues, 2007; Zhang and Clark, 2008; Huang et al., 2012; Zhang et al., 2013), partial dynamic programming (Huang and Sagae, 2010) and dual decomposition (Koo et al., 2010; Martins et al., 2011). Our work is most closely related to the MCMC sampling-based approaches (Nakagawa, 2007; Zhang et al., 2014). In our earlier work, we developed a method that learns to take guided stochastic steps towards a high-scoring parse (Zhang et al., 2014). In the heart of that technique are sophisticated samplers for traversing the space of trees. In this paper, we demonstrate that a substantially simpler approach that starts from a tree drawn from the uniform distribution and uses hillclimbing for parameter updates achieves similar or higher performance. Another related greedy inference method has been used for non-projective dependency parsing (McDonald and Pereira, 2006). This method relies on hill-climbi</context>
<context position="26148" citStr="Zhang et al., 2014" startWordPosition="4367" endWordPosition="4370">imental Setup Dataset and Evaluation Measures We evaluate our model on CoNLL dependency treebanks for 14 different languages (Buchholz and Marsi, 2006; Surdeanu et al., 2008), using standard training and testing splits. We use part-of-speech tags and the morphological information provided in the corpus. Following standard practice, we use Unlabeled Attachment Score (UAS) excluding punctuation (Koo et al., 2010; Martins et al., 2013) as the evaluation metric in all our experiments. Baselines We compare our model with the TurboParser (Martins et al., 2013) and our earlier sampling-based parser (Zhang et al., 2014). For both parsers, we directly compare with the recent published results on the CoNLL datasets. We also compare our parser against the best published results for the individual languages in our datasets. This comparison set includes four additional parsers: Martins et al. (2011), Koo et al. (2010), Zhang et al. (2013) and our tensor-based parser (Lei et al., 2014). Features We use the same feature templates as in our prior work (Zhang et al., 2014; Lei et al., 2014)4. Figure 3 shows the first- to third-order feature templates that we use in our model. For the global features we use right-bran</context>
<context position="29835" citStr="Zhang et al. (2014)" startWordPosition="4981" endWordPosition="4984">1.12 92.07 92.60 92.36 91.16 92.69 92.41 93.03 (KR10) Slovene 84.29 86.48 87.06 86.72 84.15 86.01 86.82 86.95 (MS11) Spanish 85.52 87.87 88.17 88.75 85.59 85.59 88.24 88.24 (ZL14) Swedish 89.89 91.17 91.35 91.08 89.78 91.14 90.71 91.62 (ZH13) Turkish 76.57 76.80 76.13 76.68 76.40 76.90 77.21 77.55 (KR10) Average 87.75 89.10 89.24 89.44 87.79 88.72 89.23 89.58 Table 4: Results of our model and several state-of-the-art systems. “Best Published UAS” includes the most accurate parsers among Martins et al. (2011), Martins et al. (2013), Koo et al. (2010), Zhang et al. (2013), Lei et al. (2014) and Zhang et al. (2014). For the third-order model, we use the feature set of TurboParser (Martins et al., 2013). The full model combines features of our sampling-based parser (Zhang et al., 2014) and tensor features (Lei et al., 2014). Dataset MAP-1st Uniform Rnd-1st UAS Init. UAS Init. UAS Init. Slovene 85.2 80.1 86.7 13.7 86.7 34.2 Arabic 78.8 75.1 79.7 12.4 80.2 32.8 English 91.1 82.0 93.3 39.6 93.3 55.6 Chinese 87.2 75.3 93.2 36.8 93.0 54.5 Dutch 84.8 79.5 87.0 26.9 87.4 45.6 Average 85.4 78.4 88.0 25.9 88.1 44.5 Table 5: Comparison between different initialization strategies: (a) MAP-1st: only the MAP tree of </context>
<context position="31834" citStr="Zhang et al., 2014" startWordPosition="5294" endWordPosition="5297">es. Another noteworthy comparison concerns firstorder parsers. As Table 4 shows, the exact and approximate versions of the first-order parser deliver almost identical performance. Impact of High-Order Features Table 4 shows that the model can effectively utilize high-order features. Comparing the average performance of the model variants, we see that the accuracy on the benchmark languages consistently improves when higher-order features are added. This characteristic of the randomized greedy parser is in line with findings about other state-of-the-art highorder parsers (Martins et al., 2013; Zhang et al., 2014). Figure 4 breaks down these gains based on the sentence length. As expected, on most languages high-order features are particularly helpful when parsing longer sentences. Impact of Initialization and Restarts Table 5 shows the impact of initialization on the model performance for several languages. We consider three strategies: the MAP estimate of the firstorder score from the model, uniform sampling and sampling from the first-order distribution. The accuracy of initial trees varies greatly, ranging from 78.4% for the MAP estimate to 25.9% and 44.5% for the latter randomized strategies. Howe</context>
</contexts>
<marker>Zhang, Lei, Barzilay, Jaakkola, Globerson, 2014</marker>
<rawString>Yuan Zhang, Tao Lei, Regina Barzilay, Tommi Jaakkola, and Amir Globerson. 2014. Steps to excellence: Simple inference with refined scoring of dependency trees. In Proceedings of the 52th Annual Meeting of the Association for Computational Linguistics. Association for Computational Linguistics.</rawString>
</citation>
</citationList>
</algorithm>
</algorithms>
<?xml version="1.0" encoding="UTF-8"?>
<algorithms version="110505">
<algorithm name="SectLabel" version="110505">
<variant no="0" confidence="0.003708">
<title confidence="0.996704">
Multi-Resolution Language Grounding with Weak Supervision
</title>
<author confidence="0.998275">
R. Koncel-Kedziorski, Hannaneh Hajishirzi, and Ali Farhadi
</author>
<affiliation confidence="0.998756">
University of Washington
</affiliation>
<email confidence="0.998847">
{kedzior,hannaneh,farhadi}@washington.edu
</email>
<sectionHeader confidence="0.997388" genericHeader="abstract">
Abstract
</sectionHeader>
<bodyText confidence="0.99997405882353">
Language is given meaning through its
correspondence with a world representa-
tion. This correspondence can be at mul-
tiple levels of granularity or resolutions.
In this paper, we introduce an approach
to multi-resolution language grounding in
the extremely challenging domain of pro-
fessional soccer commentaries. We define
and optimize a factored objective function
that allows us to leverage discourse struc-
ture and the compositional nature of both
language and game events. We show that
finer resolution grounding helps coarser
resolution grounding, and vice versa. Our
method results in an F1 improvement of
more than 48% versus the previous state
of the art for fine-resolution grounding1.
</bodyText>
<sectionHeader confidence="0.999517" genericHeader="keywords">
1 Introduction
</sectionHeader>
<bodyText confidence="0.999946733333333">
Language is inextricable from its context. A hu-
man language user interprets an utterance in the
context of, among other things, their perception of
the world. Grounded language acquisition algo-
rithms imitate this setup: language is given mean-
ing through its correspondence with a rich world
representation. A solution to the acquisition prob-
lem must resolve several ambiguities: the seg-
mentation of the text into meaningful units (spans
of words that refer to events); determining which
events are being referenced; and finding the proper
alignment of events to these units.
Historically, language grounding was only pos-
sible over simple controlled domains and rigidly
structured language. Current research in grounded
</bodyText>
<footnote confidence="0.961625">
1Source code and data are available at http://ssli.
ee.washington.edu/tial/projects/multires/
</footnote>
<figureCaption confidence="0.9984064">
Figure 1: An example of the multiple resolutions at which
soccer commentaries refer to events: The utterance level
alignments are shown in the black dashed boxes. The first
utterance can be further broken into the fragment-level align-
ments shown; the second cannot be decomposed further.
</figureCaption>
<bodyText confidence="0.9995682">
language acquisition is moving into real-world en-
vironments (Yu and Siskind, 2013). Grounding
sports commentaries in game events is a specific
instance of this problem that has attracted attention
(Liang et al., 2009; Snyder and Barzilay, 2007;
Hajishirzi et al., 2012), in part because of the com-
plexity of both the language and the world repre-
sentation involved.
The language employed in soccer commentaries
is difficult to ground due to its dense informa-
tion structure, novel vocabulary and word senses,
and colorful, non-traditional syntax. These chal-
lenges conspire to foil most language processing
techniques including automated parsers and word-
sense disambiguation systems.
In addition to the structural problems presented
by the language of soccer commentaries, the prob-
lem of reference is further complicated by the fact
that for game events (and other real-world phe-
nomena) there is no standardized meaningful lin-
guistic unit. Utterances ranging from a single
word to multiple sentences can be used to refer to a
single event. For example, in Figure 1 the first four
words of commentary (I) refer to a single event, as
does the entirety of (II).
</bodyText>
<page confidence="0.980289">
386
</page>
<note confidence="0.974035">
Proceedings of the 2014 Conference on Empirical Methods in Natural Language Processing (EMNLP), pages 386–396,
October 25-29, 2014, Doha, Qatar. c�2014 Association for Computational Linguistics
</note>
<figureCaption confidence="0.916039">
Figure 2: An example of the different levels of granularity present in the soccer data. The dashed boxes on the left denote ut-
terances made by the commentators. Solid boxes denote fragments that cannot be decomposed into finer resolution alignments.
The table on the right is a portion of the detailed listing of game events.
</figureCaption>
<bodyText confidence="0.9999849">
Turning our attention to Figure 2, sometimes a
fragment refers to a combination of events and no
further decomposition is available, such as the first
fragment of commentary (I). Moreover, it is some-
times desirable to construct a complex of events
by determining all the events corresponding to a
particular collection of words. For instance, we
would want to be able to align the whole of (I)
with all the events in the corresponding dashed
box. This suggests studying language grounding
at multiple levels of granularity (resolutions).
We use resolution to describe the continuum of
meaningful units which exist in human language2.
These resolutions interact in a complicated way,
with clues from different resolutions sometimes
combining to produce an effect and sometimes
negating one another. With enough training data,
one could hope to learn the details of the interac-
tions of various resolutions. However, the expense
of producing or obtaining supervised training data
at multiple resolutions is prohibitive.
To address all these complications, we in-
troduce weakly-supervised multi-resolution lan-
guage grounding. Our method makes use of a
factorized objective function which allows us to
model the complex interplay of resolutions. Our
language model takes advantage of the discourse
structure of the commentaries, making it robust
enough to handle the unique language of the soc-
cer domain. Finally, our method relies only on
</bodyText>
<footnote confidence="0.98589175">
2Though it is tempting to discritize meaning in text, Chafe
(1988) shows that readers imbue text with meaningful intona-
tional patterns drawn from the potentially continuous space of
auditory signals.
</footnote>
<bodyText confidence="0.997557866666666">
loose temporal co-occurrence of events and utter-
ances as supervision and does not require expen-
sive annotated training data.
To test our method we augment the Profes-
sional Soccer Commentary Dataset (Hajishirzi et
al., 2012) with fragment-level event alignment an-
notations. This dataset is composed of commen-
taries for soccer matches paired with event logs
produced by Opta Sportsdata and includes human
annotated gold alignments3. We achieve an F1 im-
provement of over 48% on fragment-level align-
ment versus a previous state-of-the-art. We are
also able to leverage the interplay of fragment- and
utterance- level alignments to improve the previ-
ous state-of-the-art utterance-alignment system.
</bodyText>
<sectionHeader confidence="0.997973" genericHeader="introduction">
2 Challenges
</sectionHeader>
<bodyText confidence="0.997936866666667">
Syntactic Limitations: Syntax is used to struc-
ture the information provided by an utterance, and
so it seems intuitive that syntactic relations could
be leveraged in this task. For example, consider
utterance (III) in Figure 2. The multi-resolution
grounding of (III) would provide a segmentation
of the utterance – or a division of the utterance into
the fragments which refer to separate events. In
(III), there is an obvious syntactic correlate to the
correct segmentation: each verb phrase within the
conjunction headed by “and” identifies a separate
event. Parsing (III) to an event-based semantics
like that of Davidson (1967), one could associate
each verb in an utterance with a game event and
achieve the desired segmentation.
</bodyText>
<footnote confidence="0.992232">
3Our updated dataset is available at http://ssli.
ee.washington.edu/tial/projects/multires/
</footnote>
<page confidence="0.998211">
387
</page>
<bodyText confidence="0.998076881355932">
Unfortunately, there is a preponderance of ex-
amples such as (II) in Figure 2, where 4 verbs
are used to describe a single “miss” event. (II) il-
lustrates just one of the many difficulties of using
syntactic information – elsewhere, events are ref-
erenced without an explicit verb whatsoever (such
as the use of the phrase “into the books” to refer to
a foul event). What is needed instead is a language
model that is powerful enough to proscribe some
structure yet robust enough to allow the world rep-
resentation to determine which pieces of language
are referring to which referent or set of referents.
Complex Interplay between Resolutions:
Language refers at a variety of resolutions, and
the relationship between nested reference scopes
is complex. A single or few words can indicate
entities or properties; full phrases are often needed
to denote an action; complex events like a missed
shot may take up to several phrases of narration
to properly describe. A soccer commentator
does not encode every detail necessary for proper
alignment and segmentation into their utterances,
but rather only enough to make clear to another
with similar world knowledge what is meant.
A language grounding method is at a severe
disadvantage when faced with such implicit
information.
Instead, a successful method can make heavy
use of the limited lexical, phrasal, and discourse
structural cues provided in an utterance, as the dif-
ferent resolutions rely on these different contex-
tual clues to meaning. At finer resolutions one can
rely more on the lexical meanings of the words;
at medium resolutions, compositionality can be
leveraged; at coarser resolutions, discourse fea-
tures come into play. These cues interact in a com-
plicated way, providing additional challenge.
Consider again Figure 2. In (III), the tempo-
ral discourse marker “and” marks the division be-
tween the fragments referring to each event. In
(I) the same word (used again as a temporal dis-
course marker) is used to elaborate on the single
“foul” event being described in the second frag-
ment. A human (with sufficient understanding of
soccer) knows that, despite being separated by the
discourse marker, the phrases “bring him down”
and “set piece” both refer to the foul. A language
grounding algorithm that can model the interac-
tion between such word-level and utterance-level
cues can successfully segment both (I) and (III).
Supervision: For language grounding generally,
and multi-resolution grounding specifically, su-
pervised training data is expensive to produce.
Also, the various grounding domains of interest
are highly independent of one another (Liang et
al., 2009). In the face of these issues, the ideal
correspondence between language and world rep-
resentation would be learned with as little supervi-
sion as possible.
</bodyText>
<sectionHeader confidence="0.989401" genericHeader="method">
3 Problem Definition
</sectionHeader>
<bodyText confidence="0.999818175">
We define the problem of multi-resolution lan-
guage grounding as follows: Given a temporal
evolution of a world state (a sequence of events)
and an overlapping natural language text (a se-
quence of utterances), we want to learn the best
correspondences between the language and the
world at different levels of granularity (Figure 2).
To set up notations, for each utterance repre-
sented as a set of words W = {w1, w2,. . . , wn},
we want a segmentation which expresses the re-
lationship of the words to the events which they
describe.
Let S denote a set of all possible segmentations
of W. Then S = {S|S is a segmentation of W }.
A segmentation S is in turn a set of non-
overlapping fragments (S = {sz}), where each
fragment is a consecutive sequence of words from
the utterance W. For example, for utterance (III)
from Figure 2, one possible (incorrect) segmenta-
tion is S = {s1, s2, s3} for s1 ={Chamakh rises
highest}, s2 ={and aims a header}, and s3 = {to-
wards goal which is narrowly wide}.
An alignment consists of a segmentation S and
a mapping E from fragments of S to the set of all
events E. For example, the segmentation S could
be mapped as E = {(s1, e2), (s2, e3), (s3, e1)},
with e1 being an Aerial Challenge, e2 being a
missed attempt on goal, and e3 being an out of
bounds penalty. Let E = S x E denote the set
of all possible alignments.
As we show in Figure 2, events are composed
of the various attributes Time, Type, Pass Events,
Outcome, and Player. For example, the aerial
event in Figure 2 has the attributes and values
type:aerial, outcome:successful, pass events:head
pass, and player:Chamakh.
Finally, we denote the values for the attributes
of each ej as eaj, where a ranges over the different
attributes of events as represented in the data.
We define the multi-resolution grounding of W
</bodyText>
<page confidence="0.970189">
388
</page>
<bodyText confidence="0.5371985">
into ]E as the best segmentation S and alignment
E that maximize the joint probability distribution:
</bodyText>
<equation confidence="0.95023">
arg max P(S, EDW) (1)
SES,EEE
</equation>
<bodyText confidence="0.999741463414634">
This optimization4 can be accomplished
through the use of supervised learning. However,
training data is expensive and tedious to produce
for the grounding problem, especially at multiple
resolutions. Additionally, the complexity of the
language in this domain would result in very
sparse associations.
Yet if we knew some of the correct fine-
resolution alignments, we could use that informa-
tion to produce good coarse resolution alignments,
and vice versa. Therefore, we formulate a fac-
torized form of the above objective which allows
us to learn features specific to aligning at the ut-
terance, fragment, and attribute resolutions. Our
method can be optimized with only weak super-
vision (loose temporal alignments between utter-
ances and a set of events occurring within a win-
dow of the utterance time).
We can evaluate such a correspondence in sev-
eral ways. For each utterance, can we predict the
correct events to which this utterance refers? This
is the problem of utterance-level alignment.
We can also evaluate based on events: for each
event, can we identify the minimal text span(s)
which refers to this event? We want a tight corre-
spondence because loose, overlapping alignments
are not semantically satisfying. However, we do
not want to under associate: human language
makes reference at a variety of levels (the word
level, the phrase level, the utterance level, and be-
yond). It is important to correctly identify all and
only the words which correspond to a given event.
This is the fragment-level alignment problem. We
show that good fragment-level alignments will im-
prove utterance-level alignment, and vice versa.
Since events are composed of their attributes,
we can imagine a very fine resolution grounding of
individual words to individual attributes. In fact,
our solution involves producing such a grounding
and composing the fragment- and utterance-level
alignments therefrom.
</bodyText>
<sectionHeader confidence="0.993178" genericHeader="method">
4 Our Method
</sectionHeader>
<bodyText confidence="0.999989285714286">
We have formulated the grounding problem as an
optimization of the joint probability distribution
P(S, EDW), which returns the best segmentation
and accompanying event alignments given an ut-
terance W. Optimizing this function in the do-
main of real world language, however, is a diffi-
cult problem. Utterances are long here, and there
are many events which could be grounded to each.
Furthermore, the cardinality of the set of possible
segmentations is combinatorially large.
Therefore we decompose Equation 1 using the
factor graph depicted in Figure 3. We write the
joint probability distribution as a product of the
following two potential functions:
</bodyText>
<equation confidence="0.949187">
P(S, EDW) def 1
Z 11 Ψalign(E, s) ∗ Φseg(s, W)
SES
</equation>
<bodyText confidence="0.997672307692308">
(2)
where Ψalign is a function for scoring the align-
ment E for fragment s and Φseg scores how good
a fragment s is for the utterance W, and Z is for
normalization.
To optimize Equation 2 it is not practical to
search the space of possible S, E combinations
(this space is combinatorially large). However, we
can optimize the factored form using dynamic pro-
gramming. We first describe how to find values
for each of the potentials in sections 4.1 and 4.2.
In section 4.3 we describe the dynamic program-
ming approach to optimization.
</bodyText>
<subsectionHeader confidence="0.996956">
4.1 Event Alignments Given Segmentation
</subsectionHeader>
<bodyText confidence="0.836088666666667">
The potential function Ψalign(E, s) takes as inputs
a fragment s from segmentation S and a candidate
alignment E for S and returns a score for E with
4As this and future equations are conditioned on the set
of all events lE, we omit this variable from the equations for
notational simplicity.
</bodyText>
<figure confidence="0.996955">
w1 w2 w3 w4 w5
{w1,w2,w3}
{w4,w5}
{w1,w2}
S1 S2 Sn
{w3,w4,w5}
E1 E2 E3 E4
Chamakh raises highest and aims...
pass aerial miss out
</figure>
<figureCaption confidence="0.951913333333333">
Figure 3: Factor graph for P(S, EDW). Here the wi are the
words of utterance W, Sj are the possible segmentations of
W, and Ek are different events.
</figureCaption>
<figure confidence="0.99382975">
{w3,w4,w5}
W
S
E
</figure>
<page confidence="0.993854">
389
</page>
<bodyText confidence="0.999287">
regards to s. It is here that we produce the multi-
resolution alignments; s can vary in size from a
single word to a whole utterance. ψalign decom-
poses as the following:
</bodyText>
<equation confidence="0.972143">
Ψalign(E, s) = Ψprior(E) * Ψaffinity(s, E) (3)
</equation>
<bodyText confidence="0.998970263157895">
where the priors (Ψprior) are confidence scores for
an alignment E with the whole utterance as given
by Hajishirzi et al. (2012), which fits an exemplar
SVM to each utterance/event pair. An exemplar
SVM is an SVM fit with one positive and many
negative instances, allowing us to define an ex-
ample by what it is not (Malisiewicz et al., 2011;
Shrivastava et al., 2011).
Ψaffinity scores the affinity between a fragment
s and the event ej to which it is aligned. We use
the term affinity as a measure of the goodness of
an alignment. Intuitively, a fragment s will have
a higher affinity for an event ej if s describes that
event well. Formally, the affinity between s and ej
amounts to a product of the affinity between each
word wi E s and ej. Since ej is defined by a col-
lection of attributes, we can compose a score for
wi with ej from the affinity between wi and each
attribute a of ej.
</bodyText>
<equation confidence="0.993722333333333">
11 Ψaffinity(s, E) = ψatr.(wi, ej)
wi∈s,ej∈E
ψ(wi, eja) (4)
</equation>
<bodyText confidence="0.999975977272727">
where ej is the event to which s is aligned in align-
ment E, ψatr.(wi, ej) is the affinity between wi and
event ej, and ψ(wi, eaj) is the affinity between wi
and attribute a of ej.
In order to determine the affinity of a word and
an event attribute, we create attribute:value clas-
sifiers – one for each attribute:value pair that oc-
curs in any event. For example, for goals we create
a type:goal classifier, and for unsuccessful events
we create an outcome:unsuccessful classifier.
For the categorical attributes Type, Outcome,
and Pass Events, we fit a linear SVM (Fan et al.,
2008) using the utterance-level alignments pro-
vided by Ψprior (the exemplar SVMs) to deter-
mine the positive and negative examples. For in-
stance, we use all the utterances which are aligned
with an event whose type value is “pass” as posi-
tive examples for our type:pass classifier, and all
other utterances as negative examples.
The weight assigned to each dimension in a
linear SVM describes the relative importance of
that dimension in the classification process. The
dimensions of our attribute:value SVMs are the
words of the corpus, normalized for case and mi-
nus punctuation and stop words. Therefore, the
affinity of a word wi and the attribute:value ea j is
the weight of the dimension corresponding to wi
in the ea j attribute:value classifier. Following oth-
ers (Liang et al., 2009; Kate and Mooney, 2007),
we use string matches to determine the affinity be-
tween a word and the Player attribute.
In order to make comparisons between the im-
portance of a word in the decision process for dif-
ferent classifiers, we normalize the weight vectors
for each. These attribute:value classifiers produce
our finest resolution alignments, allowing us to de-
fine a correspondence between a single word and
a single attribute of any event.
By considering ej in terms of its attributes, we
are able to compose a score for ej with fragment
s. This is a kind of double-sided compositional
semantics, where both the meaningful signs (s)
and their extensions (ej) are composed of finer-
resolution atomic parts (wi and eaj, respectively).
</bodyText>
<subsectionHeader confidence="0.996713">
4.2 Segmentations Given Utterances
</subsectionHeader>
<bodyText confidence="0.999922166666667">
The potential function Φseg(s, W) from Equation
2 returns a score for a fragment within an utter-
ance. A segmentation can be thought of as the
collection of bigrams (wi, wi+1) where wi is the
last word of a fragment which is being used to de-
scribe one event and wi+1 is the first word of a
fragment being used to describe a different event.
We will refer to such bigrams as splitpoints.
The function Φseg should favor fragments that
begin and end at good splitpoint and whose inter-
mediate bigrams are bad splitpoints. We formalize
this as follows:
</bodyText>
<equation confidence="0.975815666666667">
Φseg(s, W) a φ(wk−1, wk) * φ(wk+m, wk+m+1)
l l j 01
H O(wk+j , wk+j+1)
</equation>
<bodyText confidence="0.997630333333333">
where fragment s is a span of m consecutive words
{wk, ..., wk+m} from W, and φ is a score for how
good of a splitpoint (wi, wi+1) would make (ex-
plained below).
Ideally, φ will be a classifier which can tell
us if a given bigram is a good splitpoint for the
utterance W. However, ours being an attempt
at weakly-supervised learning, we have no la-
beled examples of correct splitpoints from which
</bodyText>
<figure confidence="0.988116">
11 =
wi∈s,ej∈E
max
a
</figure>
<page confidence="0.982449">
390
</page>
<bodyText confidence="0.999059612244898">
to work. Instead, we employ linguistic knowledge
to create a proxy of labels. We will use this proxy
to train a classifier to discover the features of good
splitpoints which can be generalized and produce
a more robust system.
The proxy labeling scheme we developed is
based on conservative components common to a
variety of theories of discourse. Discourse theo-
ries aim to model the relationships which exist be-
tween adjacent utterances in a coherent discourse.
Since we consider a sports commentary to be a co-
herent discourse, we can leverage results from dis-
course theory in producing our proxy labels.
Temporal Discourse: Events in a soccer match
occur in a temporal sequence, and so it is reason-
able to assume that the language used to describe
them will employ temporal discourse relations to
distinguish fragments describing separate events.
Pitler et al. (2008) have constructed a list of dis-
course relations which can be easily automatically
identified, including temporal discourse relations.
These are indicated by the presence of discourse
markers — alternately known as cue phrases. We
hypothesize that cue phrases can be used to iden-
tify splitpoints and use them in our proxy labeling
scheme. This method is not restricted to tempo-
rally related discourse: some contingency, expan-
sion, and comparison relations are also analyzed
as “easily identifiable”. As such, our segmentation
process can also be used to ground language into
a world state where these relations would hold.
Prosodic Discourse: We also make use of
prosodic discourse cues. Pierrehumbert and
Hirschberg (1990) claim that intonational phrases
play an important role in discourse segmentation.
Therefore, we hypothesize that the edges of in-
tonational phrases are very likely to correspond
with correct splitpoints. Viewing the commen-
tary transcriptions as a noisy channel of the ac-
tual speech signal, we can identify the intona-
tional phrase boundaries with the punctuation in-
serted in the transcription process. Chafe (1988)
confirms that punctuation in written language has
a strong correspondence with intonational phrase
boundaries, and an assumption like ours has been
successfully implemented in speech synthesis sys-
tems (Black and Lenzo, 2000). Thus, we include
bigrams containing punctuation as splitpoints in
our proxy labels.
</bodyText>
<table confidence="0.9044694">
Feature Description for splitpoint classifier
Is wi/wi+1 a discourse marker?
Is wi/wi+1 punctuation?
Is wi/wi+1 a player name?
Part of speech of wi/wi+1
</table>
<tableCaption confidence="0.9349997">
Is one of wi/wi+1 a dependent of the other?
Are wi and wi+1 dependents of the same governor?
Dependency relations that hold across splitpoint
Height of wi/wi+1 in the dependency tree
Difference in height of wi/wi+1 in dependency tree
ψ(wi, ej) of all words left versus right of splitpoint
Symmetric difference of best affinity scores for wi/wi+1
Are best affinity scores from the same event?
Table 1: Feature description for splitpoint classifier
Chamakh rises highest...
</tableCaption>
<figureCaption confidence="0.9934885">
Figure 4: We use a trellis to allow for dynamic programming
optimization of the objective function
</figureCaption>
<bodyText confidence="0.99744925">
Splitpoint Classifier: All other bigrams besides
those above are labeled as negative examples, and
a linear SVM is fit to the data. The features for the
classifier include structural, discourse, and statis-
tical features. We make use of dependency parse
information from the Stanford dependency parser
(De Marneffe and Manning, 2008). The full fea-
tures list is explained in Table 1.
</bodyText>
<subsectionHeader confidence="0.993973">
4.3 Optimization
</subsectionHeader>
<bodyText confidence="0.9998575625">
We want to maximize the function in Equation 1,
and we have explained that we can approximate
this by maximizing the factored form in Equation
2. By the above methods, we can produce values
for the functions Ψalign and Φseg. What remains is
to optimize Equation 2.
We take advantage of the factorization by using
a dynamic programming approach to optimiza-
tion. Figure 4 illustrates the setup. For each word
wi of the utterance, we create a column of nodes
in our trellis, with one row for each event ej ∈ E.
The nodes represent the affinity of a given word wi
with event ej. The weights on these nodes come
from ψatr.(wi, ej) described in section 4.2.
The nodes in column wi are connected to the
nodes in column wi+1 by edges whose weights
</bodyText>
<figure confidence="0.857134588235294">
w1 w2 w3
Unsuccessful Cross
Pass
Successful
Aerial Head Pass
Missed
Head Pass
ψ(w1,e2)
ψ(w1,e3)
1/φ(W1, W2)
ψ(w1,e1) ψ(w2,e1) ψ(w3,e1)
φ(W1,W0
φ(W1,W0
ψ(w2,e3)
ψ(w2,e2)
ψ(w3,e2)
ψ(w3,e3)
</figure>
<page confidence="0.993503">
391
</page>
<table confidence="0.991837333333333">
Method Precision Recall F1
Liang et al. (2009) 0.513 0.393 0.445
Our approach 0.603 0.481 0.535
</table>
<tableCaption confidence="0.974288">
Table 2: Fragment-level alignments starting from gold
utterance-level alignments
</tableCaption>
<table confidence="0.999777333333333">
Method Precision Recall F1
Liang et al. (2009) 0.211 0.135 0.165
Our approach 0.235 0.255 0.245
</table>
<tableCaption confidence="0.99985">
Table 3: Fragment-level alignments starting from raw data
</tableCaption>
<bodyText confidence="0.9999410625">
are drawn from the splitpoint classifier response
O(wi, wz+1). We label the edges between adja-
cent nodes corresponding to different events with
the responses from the splitpoint classifier, and the
inverse of these responses for edges connecting
nodes corresponding to the same event.
We then use the Viterbi algorithm (Viterbi,
1967) to find the maximum scoring path through
this trellis. The maximum scoring path optimizes
Equation 2, and serves as our approximation of the
optimization of Equation 1. We choose the top k
diverse paths through the trellis and use the associ-
ations therein as our alignments. See Figure 5 for
a detailed example of how our Viterbi path coin-
cides with the responses from the attribute:value
classifiers.
</bodyText>
<sectionHeader confidence="0.999706" genericHeader="method">
5 Experiments
</sectionHeader>
<bodyText confidence="0.999981375">
One justification for multi-resolution language
grounding would be if finer-resolution grounding
improves coarser-resolution grounding and vice
versa. If so, we expect that better utterance-level
alignments will improve fragment-level align-
ments, and that in turn those fragment-level align-
ments will improve utterance-level alignments.
We evaluate both of these hypotheses.
</bodyText>
<subsectionHeader confidence="0.98413">
5.1 Experimental Setup
</subsectionHeader>
<bodyText confidence="0.999895111111111">
Dataset: We use the publicly available Profes-
sional Soccer Commentary (PSC) dataset intro-
duced in Hajishirzi et al. (2012). This dataset is
composed of professional commentaries from the
2010-2011 season of the English Premier League,
along with a human-annotated data feed produced
for each game by Opta Sportsdata (Opta, 2012)
which describes all events occurring around the
ball. Events include passes, shots, misses, cards,
</bodyText>
<table confidence="0.9819055">
Method Precision Recall F1
Liang et al. (2009) 0.327 0.418 0.367
Hajishirzi et al. (2012) 0.355 0.576 0.439
Our approach 0.407 0.520 0.457
</table>
<tableCaption confidence="0.999656">
Table 4: Utterance-level alignment results
</tableCaption>
<bodyText confidence="0.9998548">
tackles, and other relevant game details. Each
event category is defined precisely and the feed
is annotated by professionals according to strict
event description guidelines.
The PSC also provides ground truth alignment
of full utterances to events in the data feed, and for
this work we have augmented it with ground truth
fragment-level annotations5.
We use data from 7 games of the PSC. These
games consist of 778 utterances totaling 13,692
words. There are 12,275 events. This data is la-
beled with ground truth utterance- and fragment-
alignments.
Metric: There are 1,295 correct utterance-to-
event alignments. For evaluation we use precision,
recall, and F1 of our utterance-level alignments.
The evaluation of fragment-level alignments is
less straight forward. This is due to the two fea-
tures of a correct fragment alignment: picking
the correct fragment boundaries and associating
the fragment with the correct event. We evaluate
fragment-level alignment on a per word basis. We
consider precision in this task to be the number of
correct word to event alignments versus the total
number of alignments produced by a system. Re-
call is the number of correct word to event align-
ments versus the total gold word to event align-
ments, of which there are 18,147.
Comparisons: We compare to two previous
works: Liang et al. (2009), which produces
both segmentation and alignment results; and Ha-
jishirzi et al. (2012), which produces state-of-the-
art alignments. When evaluating segmentation,
we compare how well the systems perform start-
ing from the raw dataset, and starting from gold
utterance-level alignments. This allows us to iso-
late the segmentation process from the overall sys-
tem architectures. It also gives us some insight
into the effect of event priors on the segmentation
and alignment processes.
</bodyText>
<footnote confidence="0.988251">
5The full dataset is available at http://ssli.
ee.washington.edu/tial/projects/multires/
</footnote>
<page confidence="0.994354">
392
</page>
<figureCaption confidence="0.993752333333333">
Figure 5: A successful grounding at multiple resolutions. Thin blue lines separate
the attribute:value pairs corresponding to the three events. Values of ψ(wi, ej)
are shown on each node. The shaded bands indicate the gold fragment-level align-
ments. Thick line connecting the green nodes indicates the classifier responses
used in the Viterbi best path through our trellis. The red dashed edge indicates a
high response from the splitpoint classifier. This figure is best viewed in color.
</figureCaption>
<table confidence="0.9996775">
Method Precision Recall F1
Ours 0.235 0.255 0.245
- Taffinity 0.213 0.133 0.164
- 4)seg 0.205 0.232 0.218
</table>
<tableCaption confidence="0.97552125">
Table 5: Ablation studies for fragment-level
alignments by removing Taffinity and 4)seg
from our model by replacing them with uni-
form function.
</tableCaption>
<table confidence="0.99990575">
Method Precision Recall F1
Ours 0.407 0.520 0.457
- Taffinity 0.446 0.189 0.265
- 4)seg 0.376 0.563 0.451
</table>
<tableCaption confidence="0.985454">
Table 6: Ablation studies for utterance-level
alignments by removing Taffinity and 4)seg
from our model by replacing them with uni-
form function.
</tableCaption>
<figure confidence="0.856461545454545">
Chamakh rises highest and aims a header towards goal which is narrowly wide
.33 .04 .02 0
.33 .01 .01 0
.33 .07 .02 0
pass:head pass
outcome:unsuccessful
type:out
pass:head pass
outcome:successful
type:miss
.01 0 .02 .05 .23 0 0 .02 .1
0 0 .06 0 .03 0 0 01 .03
.02 0 .15 .01 .11 0 0 .06 .50
pass:head pass
outcome:successful
type:aerial
.33 .04 .02 0 .01 0 .02 .05 .23 0 0 .02 .1
.33 .02 .01 0 0 0 .06 0 .03 0 0 .01 .03
.33 .06 .03 0 .04 0 .02 .05 .1 0 0 .03 .13
.33 .04 .02 0 .01 0 .02 .05 .23 0 0 .02 .1
.33 .02 .01 0 0 0 .03 .01 .04 0 0 .01 .04
.33 .01 .01 0 .01 0 .07 0 0 0 0 .03 .28
</figure>
<subsectionHeader confidence="0.882443">
5.2 Results
</subsectionHeader>
<bodyText confidence="0.999905468085106">
We evaluate our method on its alignments at the
fragment-level and at the utterance-level. The re-
sults are as follows:
Fragment-level: Our results for segmentation can
be seen in Tables 2 and 3. Table 2 shows the results
achieved on the fragment-level alignment task us-
ing human-labeled utterance to event alignments.
In this setting, all and only the correct events for
each utterance are present. Still, there are sev-
eral ambiguities in the data. Some fragments are
aligned in the gold data with multiple events, and
some are aligned to no event. Our method out-
performs the previous by a large margin in terms
of both precision and recall. We show below how
this is due to our system’s accommodation of dis-
course structure when making segmentation deci-
sions and the factored form of our optimization.
Table 3 shows the results for fragment-level
alignment by applying each system starting from
the raw data. Here, in addition to the ambiguities
mentioned above, the problem is further compli-
cated by the fact that some correct events are miss-
ing from the alignments produced by each system
and some incorrect events are included in these
alignments (see Error Analysis below for details).
Still our method achieves a significant improve-
ment, with a 48% increase in F1 versus prior work.
Table 5 shows ablation results for the effect of
the factors used in our optimization for fragment-
level alignments. These results demonstrate the
value of each factor in the fragment-level align-
ment process. We cannot ascribe the benefit of this
method to one factor or another alone – it is their
concert that improves performance.
Utterance-level: We have posited that good finer-
resolution alignments will improve the coarser-
resolution utterance to event alignments. Our re-
sults confirm this hypothesis. Table 4 shows our
results on these alignments. We are able to im-
prove F1 versus a state-of-the-art system which
is tuned to maximize its F1 score. The major-
ity of our improvement comes from the increased
precision of our system, due to the influence of
the finer-resolution fragment-level alignments on
these coarser, utterance-level alignments. We pro-
vide a detailed example of this below. Ablation
results are shown in Table 6.
</bodyText>
<subsectionHeader confidence="0.999685">
5.3 Qualitative Analysis
</subsectionHeader>
<bodyText confidence="0.999574555555556">
A qualitative analysis of our system reveals the
power of our factored objective, double-sided
compositional approach, and leveraging of dis-
course structure. Figure 5 shows the best path
through the trellis of the example sentence used
in the introduction. For explanatory purposes,
we have split every event into its three compo-
nent attributes. This allows us to see how the
attribute:value classifiers combine to produce an
alignment.
Discourse Structure: The fragment-level align-
ment we have produced for this utterance is per-
fect: it correctly identifies the single splitpoint and
correctly identifies each fragment with the associ-
ated event.
The identification of the splitpoint “and” comes
from the fact that this word has, among other uses,
a discourse connective meaning. Thus, the edges
</bodyText>
<page confidence="0.998245">
393
</page>
<bodyText confidence="0.999988303571429">
in our trellis between different events are weighted
higher than edges between the same event in the
edges between the nodes for “highest” and “and”,
encouraging the Viterbi path to change events at
this point.
Compositionality: We can see effect of the com-
positional approach we have taken – composing
ψaffinity(s, ej) from the attribute:value classifier
scores of each ψ(wZ, eaj) – by looking at how the
best path makes use of different attributes of the
same event. For the “miss” event aligned with
the second part of the sentence, we can see that
the best path makes use of both values from the
type:miss and pass event:head pass classifiers.
Affinities: A few interesting associations are
worth pointing out. First, we note that the word
“header” has a stronger affinity for the type:miss
attribute than it does for the pass events:head pass
attribute. On first blush, this seems like a mistake
in our classifier. However, we can see that even
in this single trellis all three events have the pass
events:head pass attribute. The utterance-level
alignment uses this association already, align-
ing utterances containing the word “header” with
events that have a pass events:head pass attribute.
At a finer-resolution, it is necessary to make a
different distinction between events. Our method
finds that the presence of the word “header” is a
stronger indicator of an event with a type:miss at-
tribute, and thus this association is made.
Words that are better for the coarser-resolution
association with the pass events:head pass at-
tribute are “towards” and “goal”. Out of the 10
utterances containing the word “towards” in the
dataset, 3 of these are aligned with at least 1 pass
events:head pass event, making this strong asso-
ciation a correct one. The word “goal” also has
an affinity for the pass events:head pass attribute
due to the fact that many events with this attribute
are attempts on goal. This correlates with domain
knowledge about soccer, because, although there
may be other uses of their head by a player in the
game, shots on goal are events which will nearly
always be commented upon by an announcer.
Factorization: We have shown that finer-
resolution fragment-level alignments can improve
utterance-level alignments. From the exemplar
SVMs, we are given an utterance-level alignment
of the three events shown in the trellis with the
utterance. This alignment is incorrect: the gold
utterance alignment only includes the bottom two
events. But by building an utterance-level align-
ment from the results of our fragment level align-
ment, we are left with only the two correct events.
We prune the topmost event due to its failure to
participate in a finer-resolution alignment.
</bodyText>
<subsectionHeader confidence="0.81067">
5.4 Error Analysis
</subsectionHeader>
<bodyText confidence="0.999965166666667">
The majority of the errors made on our fragment-
level alignments come in one of two flavors:
Firstly, we sometimes erroneously identify a frag-
ment as referring to an event when in truth it refers
to no event. Commentators often describe facts
about players or the weather or previous games
which have no extension in the current game.
However, our system cannot distinguish such lan-
guage from the language referring to this game.
This is a good avenue for future exploration.
The second set of errors we make in fragmen-
tation are caused by bad event priors. Our current
setup cannot increase recall: we can only improve
the precision of the utterance-level alignments we
are given. Therefore, if an event is overlooked
in the first-pass of utterance-level alignments, we
cannot reintroduce it through a fragment align-
ment. This is a direction for future work as well.
</bodyText>
<sectionHeader confidence="0.999986" genericHeader="method">
6 Related Work
</sectionHeader>
<bodyText confidence="0.999944521739131">
Early semantic parsing work made use of fully su-
pervised training (Zettlemoyer and Collins, 2005;
Ge and Mooney, 2006; Snyder and Barzilay,
2007), but more recent work has focused on re-
ducing the amount of supervision required (Artzi
and Zettlemoyer, 2013). A few unsupervised ap-
proaches exist (Poon and Domingos, 2009; Poon,
2013), but these are specific to translating lan-
guage into queries in highly structured database
and cannot be applied to our more flexible domain.
There are few datasets as detailed as the Profes-
sional Soccer Commentary Dataset. Early work
in understanding soccer commentaries focused on
RoboCup soccer (Chen and Mooney, 2008; Chen
et al., 2010; Bordes et al., 2010; Hajishirzi et
al., 2011) where simple language describes each
event, and events are in a one-to-one correspon-
dence with utterances. Another dataset used for
language grounding is the Weather Report Dataset
(Liang et al., 2009). Here, again, however, we
have mostly single utterances paired with single
events, and many alignments are made via nu-
merical string matching rather than learning lex-
</bodyText>
<page confidence="0.996329">
394
</page>
<bodyText confidence="0.999957615384615">
ical cues. The NFL Recap dataset (Snyder and
Barzilay, 2007) is also laden with numerical fact
matching, and does not include the fragment-level
segmentation annotation that the PSC dataset pro-
vides.
Impressive advances have been made grounding
language in instructions. Branavan et al. (2009)
and Vogel and Jurafsky (2010) work in the do-
main of computer technical support instructions,
mapping language to actions using reinforcement
learning. Matuszek et al. (2012b) parses sim-
ple language to robot control instructions. Our
work focuses on dealing with a richer space, both
in terms of the language used and the world-
representation into which it is grounded, and lever-
aging the multiple resolutions of reference.
An exciting direction of research, closer to our
own, aims to ground natural language in visual
perception systems. Matuszek et al. (2012a) at-
tempts to learn a joint model of language and ob-
ject characteristics of a workplace environment.
Yu and Siskind (2013) grounds moderately rich
language in automatically annotated video clips.
Again, the contribution of our work versus the
above is in the complexity of the language with
which we deal and our multi-resolution model.
</bodyText>
<sectionHeader confidence="0.99901" genericHeader="conclusions">
7 Conclusion
</sectionHeader>
<bodyText confidence="0.999973348837209">
The problem of grounding complex natural hu-
man language such as soccer commentaries is
extremely difficult at all resolutions, and it is
most challenging at finer resolutions where data is
sparsest and small errors cannot be as easily nor-
malized. Our work will help open new avenues of
research into this difficult and exciting problem.
This paper presents a new method for the multi-
resolution grounding of complex natural language
in a detailed world representation. Our factor
graph allows us to decompose the grounding prob-
lem into the more tractable subproblems of seg-
menting the language into fragments and aligning
the fragments with the world representation. In
the segmentation phase, we make use of linguis-
tic theories of discourse to create a proxy of labels
from which we learn statistical and structural fea-
tures of good splitpoints. In the alignment phase,
we bootstrap the learning of finer-grained corre-
spondences between the language and the world
representation with rough alignments from a state-
of-the-art system. We combine these phases in a
dynamic programming setup which allows us to
efficiently optimize our objective.
We have shown that factoring the acquisition
problem into separate alignment and segmentation
phases improves performance on several evalua-
tion metrics. We achieve considerable improve-
ments over the previous state of the art on finer-
resolution alignments in the domain of profes-
sional soccer commentaries, and we show that we
can leverage groundings at one resolution to im-
prove alignments in another.
Several extensions of this work are possible. We
would like to annotate more games to improve
our dataset. We could improve our model by en-
coding the dynamics of the environment. We did
not attempt to learn this information in our pro-
cess, but it is likely that modeling the event tran-
sition probabilities could provide better results. A
larger future work would extend the method out-
lined herein to produce templates for automated
commentary generation.
</bodyText>
<sectionHeader confidence="0.998385" genericHeader="acknowledgments">
Acknowledgments
</sectionHeader>
<bodyText confidence="0.999812857142857">
This research was supported in part by a grant
from the NSF (IIS-1352249), and the Royalty Re-
search Fund (RRF) at the University of Washing-
ton. The authors also wish to thank Gina-Anne
Levow, Yoav Artzi, Ben Hixon, and the anony-
mous reviewers for their valuable feedback on this
work.
</bodyText>
<sectionHeader confidence="0.999507" genericHeader="references">
References
</sectionHeader>
<reference confidence="0.998843761904762">
Yoav Artzi and Luke Zettlemoyer. 2013. Weakly su-
pervised learning of semantic parsers for mapping
instructions to actions. Transactions of the Associa-
tion for Computational Linguistics, 1(1):49–62.
Alan Black and Kevin Lenzo. 2000. Building voices
in the festival speech synthesis system.
Antoine Bordes, Nicolas Usunier, and Jason Weston.
2010. Label ranking under ambiguous supervision
for learning semantic correspondences. In Proceed-
ings of The 27th International Conference on Ma-
chine Learning, pages 103–110.
S. R. K. Branavan, Harr Chen, Luke Zettlemoyer, and
Regina Barzilay. 2009. Reinforcement learning for
mapping instructions to actions. In Proceedings of
the Joint Conference of the 47th Annual Meeting of
the Association for Computational Linguistics and
4th International Joint Conference on Natural Lan-
guage Processing of the AFNLP, pages 82–90.
Wallace Chafe. 1988. Punctuation and the prosody
of written language. Written communication,
5(4):395–426.
</reference>
<page confidence="0.986894">
395
</page>
<reference confidence="0.999797047169812">
David L. Chen and Raymond J. Mooney. 2008. Learn-
ing to sportscast: a test of grounded language ac-
quisition. In Proceedings of the 25th International
Conference on Machine Learning, pages 128–135.
David L. Chen, Joohyun Kim, and Raymond J.
Mooney. 2010. Training a multilingual sportscaster:
Using perceptual context to learn language. Journal
of Artificial Intelligence Research (JAIR), 37:397–
435.
Donald Davidson. 1967. The logical form of action
sentences. The logic of Decision and Action.
Marie-Catherine De Marneffe and Christopher D Man-
ning. 2008. Stanford typed dependencies manual.
URL http://nlp. stanford. edu/software/dependencies
manual. pdf.
Rong-En Fan, Kai-Wei Chang, Cho-Jui Hsieh, Xiang-
Rui Wang, and Chih-Jen Lin. 2008. LIBLINEAR:
A library for large linear classification. Journal of
Machine Learning Research, 9:1871–1874.
Ruifang Ge and Raymond J. Mooney. 2006. Discrim-
inative reranking for semantic parsing. In Proceed-
ings of the 44th Annual Meeting of the Association
for Computational Linguistics.
Hannaneh Hajishirzi, Julia Hockenmaier, Erik T.
Mueller, and Eyal Amir. 2011. Reasoning about
robocup soccer narratives. In Proceedings of the
27th conference on Uncertainty in Artificial Intelli-
gence, pages 291–300.
Hannaneh Hajishirzi, Mohammad Rastegari, Ali
Farhadi, and Jessica K Hodgins. 2012. Semantic
understanding of professional soccer commentaries.
In Proceedings of the 28th conference on Uncer-
tainty in Artificial Intelligence.
Rohit J. Kate and Raymond J. Mooney. 2007. Learn-
ing language semantics from ambiguous supervi-
sion. In Proceedings of the Twenty-Second AAAI
Conference on Artificial Intelligence, pages 895–
900.
Percy Liang, Michael I. Jordan, and Dan Klein. 2009.
Learning semantic correspondences with less super-
vision. In Proceedings of the Joint Conference of the
47th Annual Meeting of the Association for Com-
putational Linguistics and 4th International Joint
Conference on Natural Language Processing of the
AFNLP, pages 91–99.
Tomasz Malisiewicz, Abhinav Gupta, and Alexei A.
Efros. 2011. Ensemble of exemplar-svms for object
detection and beyond. In Proceedings of the 13th
International Conference on Computer Vision.
Cynthia Matuszek, Nicholas FitzGerald, Luke Zettle-
moyer, Liefeng Bo, and Dieter Fox. 2012a. A Joint
Model of Language and Perception for Grounded
Attribute Learning. In Proc. of the 2012 Interna-
tional Conference on Machine Learning, Edinburgh,
Scotland, June.
Cynthia Matuszek, Evan Herbst, Luke Zettlemoyer,
and Dieter Fox. 2012b. Learning to parse natural
language commands to a robot control system. In
Proc. of the 13th International Symposium on Ex-
perimental Robotics (ISER), June.
Opta. 2012. http://www.optasports.com.
Janet Pierrehumbert and Julia Hirschberg. 1990. The
meaning of intonational contours in the interpreta-
tion of discourse. Intentions in Communication,
271.
Emily Pitler, Mridhula Raghupathy, Hena Mehta, Ani
Nenkova, Alan Lee, and Aravind K Joshi. 2008.
Easily identifiable discourse relations. Technical
Reports (CIS), page 884.
Hoifung Poon and Pedro Domingos. 2009. Unsuper-
vised semantic parsing. In Proceedings of the 2009
Conference on Empirical Methods in Natural Lan-
guage Processing, pages 1–10.
Hoifung Poon. 2013. Grounded unsupervised seman-
tic parsing. In Proceedings of the 51st Annual Meet-
ing of the Association for Computational Linguis-
tics.
Abhinav Shrivastava, Tomasz Malisiewicz, Abhinav
Gupta, and Alexei A. Efros. 2011. Data-driven
visual similarity for cross-domain image matching.
ACM Transaction of Graphics (TOG) (Proceedings
ofACMSIGGRAPHASIA), 30(6).
Benjamin Snyder and Regina Barzilay. 2007.
Database-text alignment via structured multilabel
classification. In Proceedings of the 20th Inter-
national Joint Conference on Artificial Intelligence,
pages 1713–1718.
Andrew J Viterbi. 1967. Error bounds for convolu-
tional codes and an asymptotically optimum decod-
ing algorithm. Information Theory, IEEE Transac-
tions on, 13(2):260–269.
Adam Vogel and Daniel Jurafsky. 2010. Learning to
follow navigational directions. In Proceedings of the
48th Annual Meeting of the Association for Compu-
tational Linguistics, pages 806–814.
Haonan Yu and Jeffrey Mark Siskind. 2013. Grounded
language learning from video described with sen-
tences. In Proceedings of the 51st Annual Meet-
ing of the Association for Computational Linguis-
tics, volume 1, pages 53–63.
Luke S. Zettlemoyer and Michael Collins. 2005.
Learning to map sentences to logical form: Struc-
tured classification with probabilistic categorial
grammars. In Proceedings of the 21st Conference
on Uncertainty in Artificial Intelligence, pages 658–
666.
</reference>
<page confidence="0.999081">
396
</page>
</variant>
</algorithm>
<algorithm name="ParsHed" version="110505">
<variant no="0" confidence="0.877021">
<title confidence="0.999991">Multi-Resolution Language Grounding with Weak Supervision</title>
<author confidence="0.995252">R Koncel-Kedziorski</author>
<author confidence="0.995252">Hannaneh Hajishirzi</author>
<author confidence="0.995252">Ali</author>
<affiliation confidence="0.999921">University of Washington</affiliation>
<email confidence="0.999707">kedzior@washington.edu</email>
<email confidence="0.999707">hannaneh@washington.edu</email>
<email confidence="0.999707">farhadi@washington.edu</email>
<abstract confidence="0.999929882352941">Language is given meaning through its correspondence with a world representation. This correspondence can be at mullevels of granularity or In this paper, we introduce an approach to multi-resolution language grounding in the extremely challenging domain of professional soccer commentaries. We define and optimize a factored objective function that allows us to leverage discourse structure and the compositional nature of both language and game events. We show that finer resolution grounding helps coarser resolution grounding, and vice versa. Our method results in an F1 improvement of more than 48% versus the previous state</abstract>
<intro confidence="0.882448">the art for fine-resolution</intro>
</variant>
</algorithm>
<algorithm name="ParsCit" version="110505">
<citationList>
<citation valid="true">
<authors>
<author>Yoav Artzi</author>
<author>Luke Zettlemoyer</author>
</authors>
<title>Weakly supervised learning of semantic parsers for mapping instructions to actions.</title>
<date>2013</date>
<journal>Transactions of the Association for Computational Linguistics,</journal>
<volume>1</volume>
<issue>1</issue>
<contexts>
<context position="36579" citStr="Artzi and Zettlemoyer, 2013" startWordPosition="5967" endWordPosition="5970"> fragmentation are caused by bad event priors. Our current setup cannot increase recall: we can only improve the precision of the utterance-level alignments we are given. Therefore, if an event is overlooked in the first-pass of utterance-level alignments, we cannot reintroduce it through a fragment alignment. This is a direction for future work as well. 6 Related Work Early semantic parsing work made use of fully supervised training (Zettlemoyer and Collins, 2005; Ge and Mooney, 2006; Snyder and Barzilay, 2007), but more recent work has focused on reducing the amount of supervision required (Artzi and Zettlemoyer, 2013). A few unsupervised approaches exist (Poon and Domingos, 2009; Poon, 2013), but these are specific to translating language into queries in highly structured database and cannot be applied to our more flexible domain. There are few datasets as detailed as the Professional Soccer Commentary Dataset. Early work in understanding soccer commentaries focused on RoboCup soccer (Chen and Mooney, 2008; Chen et al., 2010; Bordes et al., 2010; Hajishirzi et al., 2011) where simple language describes each event, and events are in a one-to-one correspondence with utterances. Another dataset used for langu</context>
</contexts>
<marker>Artzi, Zettlemoyer, 2013</marker>
<rawString>Yoav Artzi and Luke Zettlemoyer. 2013. Weakly supervised learning of semantic parsers for mapping instructions to actions. Transactions of the Association for Computational Linguistics, 1(1):49–62.</rawString>
</citation>
<citation valid="true">
<authors>
<author>Alan Black</author>
<author>Kevin Lenzo</author>
</authors>
<title>Building voices in the festival speech synthesis system.</title>
<date>2000</date>
<contexts>
<context position="21954" citStr="Black and Lenzo, 2000" startWordPosition="3580" endWordPosition="3583">national phrases play an important role in discourse segmentation. Therefore, we hypothesize that the edges of intonational phrases are very likely to correspond with correct splitpoints. Viewing the commentary transcriptions as a noisy channel of the actual speech signal, we can identify the intonational phrase boundaries with the punctuation inserted in the transcription process. Chafe (1988) confirms that punctuation in written language has a strong correspondence with intonational phrase boundaries, and an assumption like ours has been successfully implemented in speech synthesis systems (Black and Lenzo, 2000). Thus, we include bigrams containing punctuation as splitpoints in our proxy labels. Feature Description for splitpoint classifier Is wi/wi+1 a discourse marker? Is wi/wi+1 punctuation? Is wi/wi+1 a player name? Part of speech of wi/wi+1 Is one of wi/wi+1 a dependent of the other? Are wi and wi+1 dependents of the same governor? Dependency relations that hold across splitpoint Height of wi/wi+1 in the dependency tree Difference in height of wi/wi+1 in dependency tree ψ(wi, ej) of all words left versus right of splitpoint Symmetric difference of best affinity scores for wi/wi+1 Are best affini</context>
</contexts>
<marker>Black, Lenzo, 2000</marker>
<rawString>Alan Black and Kevin Lenzo. 2000. Building voices in the festival speech synthesis system.</rawString>
</citation>
<citation valid="true">
<authors>
<author>Antoine Bordes</author>
<author>Nicolas Usunier</author>
<author>Jason Weston</author>
</authors>
<title>Label ranking under ambiguous supervision for learning semantic correspondences.</title>
<date>2010</date>
<booktitle>In Proceedings of The 27th International Conference on Machine Learning,</booktitle>
<pages>103--110</pages>
<contexts>
<context position="37015" citStr="Bordes et al., 2010" startWordPosition="6037" endWordPosition="6040">moyer and Collins, 2005; Ge and Mooney, 2006; Snyder and Barzilay, 2007), but more recent work has focused on reducing the amount of supervision required (Artzi and Zettlemoyer, 2013). A few unsupervised approaches exist (Poon and Domingos, 2009; Poon, 2013), but these are specific to translating language into queries in highly structured database and cannot be applied to our more flexible domain. There are few datasets as detailed as the Professional Soccer Commentary Dataset. Early work in understanding soccer commentaries focused on RoboCup soccer (Chen and Mooney, 2008; Chen et al., 2010; Bordes et al., 2010; Hajishirzi et al., 2011) where simple language describes each event, and events are in a one-to-one correspondence with utterances. Another dataset used for language grounding is the Weather Report Dataset (Liang et al., 2009). Here, again, however, we have mostly single utterances paired with single events, and many alignments are made via numerical string matching rather than learning lex394 ical cues. The NFL Recap dataset (Snyder and Barzilay, 2007) is also laden with numerical fact matching, and does not include the fragment-level segmentation annotation that the PSC dataset provides. I</context>
</contexts>
<marker>Bordes, Usunier, Weston, 2010</marker>
<rawString>Antoine Bordes, Nicolas Usunier, and Jason Weston. 2010. Label ranking under ambiguous supervision for learning semantic correspondences. In Proceedings of The 27th International Conference on Machine Learning, pages 103–110.</rawString>
</citation>
<citation valid="true">
<authors>
<author>S R K Branavan</author>
<author>Harr Chen</author>
<author>Luke Zettlemoyer</author>
<author>Regina Barzilay</author>
</authors>
<title>Reinforcement learning for mapping instructions to actions.</title>
<date>2009</date>
<booktitle>In Proceedings of the Joint Conference of the 47th Annual Meeting of the Association for Computational Linguistics and 4th International Joint Conference on Natural Language Processing of the AFNLP,</booktitle>
<pages>82--90</pages>
<contexts>
<context position="37707" citStr="Branavan et al. (2009)" startWordPosition="6143" endWordPosition="6146"> and events are in a one-to-one correspondence with utterances. Another dataset used for language grounding is the Weather Report Dataset (Liang et al., 2009). Here, again, however, we have mostly single utterances paired with single events, and many alignments are made via numerical string matching rather than learning lex394 ical cues. The NFL Recap dataset (Snyder and Barzilay, 2007) is also laden with numerical fact matching, and does not include the fragment-level segmentation annotation that the PSC dataset provides. Impressive advances have been made grounding language in instructions. Branavan et al. (2009) and Vogel and Jurafsky (2010) work in the domain of computer technical support instructions, mapping language to actions using reinforcement learning. Matuszek et al. (2012b) parses simple language to robot control instructions. Our work focuses on dealing with a richer space, both in terms of the language used and the worldrepresentation into which it is grounded, and leveraging the multiple resolutions of reference. An exciting direction of research, closer to our own, aims to ground natural language in visual perception systems. Matuszek et al. (2012a) attempts to learn a joint model of la</context>
</contexts>
<marker>Branavan, Chen, Zettlemoyer, Barzilay, 2009</marker>
<rawString>S. R. K. Branavan, Harr Chen, Luke Zettlemoyer, and Regina Barzilay. 2009. Reinforcement learning for mapping instructions to actions. In Proceedings of the Joint Conference of the 47th Annual Meeting of the Association for Computational Linguistics and 4th International Joint Conference on Natural Language Processing of the AFNLP, pages 82–90.</rawString>
</citation>
<citation valid="true">
<authors>
<author>Wallace Chafe</author>
</authors>
<title>Punctuation and the prosody of written language. Written communication,</title>
<date>1988</date>
<pages>5--4</pages>
<contexts>
<context position="5181" citStr="Chafe (1988)" startWordPosition="784" endWordPosition="785">ous resolutions. However, the expense of producing or obtaining supervised training data at multiple resolutions is prohibitive. To address all these complications, we introduce weakly-supervised multi-resolution language grounding. Our method makes use of a factorized objective function which allows us to model the complex interplay of resolutions. Our language model takes advantage of the discourse structure of the commentaries, making it robust enough to handle the unique language of the soccer domain. Finally, our method relies only on 2Though it is tempting to discritize meaning in text, Chafe (1988) shows that readers imbue text with meaningful intonational patterns drawn from the potentially continuous space of auditory signals. loose temporal co-occurrence of events and utterances as supervision and does not require expensive annotated training data. To test our method we augment the Professional Soccer Commentary Dataset (Hajishirzi et al., 2012) with fragment-level event alignment annotations. This dataset is composed of commentaries for soccer matches paired with event logs produced by Opta Sportsdata and includes human annotated gold alignments3. We achieve an F1 improvement of ove</context>
<context position="21729" citStr="Chafe (1988)" startWordPosition="3550" endWordPosition="3551">n process can also be used to ground language into a world state where these relations would hold. Prosodic Discourse: We also make use of prosodic discourse cues. Pierrehumbert and Hirschberg (1990) claim that intonational phrases play an important role in discourse segmentation. Therefore, we hypothesize that the edges of intonational phrases are very likely to correspond with correct splitpoints. Viewing the commentary transcriptions as a noisy channel of the actual speech signal, we can identify the intonational phrase boundaries with the punctuation inserted in the transcription process. Chafe (1988) confirms that punctuation in written language has a strong correspondence with intonational phrase boundaries, and an assumption like ours has been successfully implemented in speech synthesis systems (Black and Lenzo, 2000). Thus, we include bigrams containing punctuation as splitpoints in our proxy labels. Feature Description for splitpoint classifier Is wi/wi+1 a discourse marker? Is wi/wi+1 punctuation? Is wi/wi+1 a player name? Part of speech of wi/wi+1 Is one of wi/wi+1 a dependent of the other? Are wi and wi+1 dependents of the same governor? Dependency relations that hold across split</context>
</contexts>
<marker>Chafe, 1988</marker>
<rawString>Wallace Chafe. 1988. Punctuation and the prosody of written language. Written communication, 5(4):395–426.</rawString>
</citation>
<citation valid="true">
<authors>
<author>David L Chen</author>
<author>Raymond J Mooney</author>
</authors>
<title>Learning to sportscast: a test of grounded language acquisition.</title>
<date>2008</date>
<booktitle>In Proceedings of the 25th International Conference on Machine Learning,</booktitle>
<pages>128--135</pages>
<contexts>
<context position="36975" citStr="Chen and Mooney, 2008" startWordPosition="6029" endWordPosition="6032">e use of fully supervised training (Zettlemoyer and Collins, 2005; Ge and Mooney, 2006; Snyder and Barzilay, 2007), but more recent work has focused on reducing the amount of supervision required (Artzi and Zettlemoyer, 2013). A few unsupervised approaches exist (Poon and Domingos, 2009; Poon, 2013), but these are specific to translating language into queries in highly structured database and cannot be applied to our more flexible domain. There are few datasets as detailed as the Professional Soccer Commentary Dataset. Early work in understanding soccer commentaries focused on RoboCup soccer (Chen and Mooney, 2008; Chen et al., 2010; Bordes et al., 2010; Hajishirzi et al., 2011) where simple language describes each event, and events are in a one-to-one correspondence with utterances. Another dataset used for language grounding is the Weather Report Dataset (Liang et al., 2009). Here, again, however, we have mostly single utterances paired with single events, and many alignments are made via numerical string matching rather than learning lex394 ical cues. The NFL Recap dataset (Snyder and Barzilay, 2007) is also laden with numerical fact matching, and does not include the fragment-level segmentation ann</context>
</contexts>
<marker>Chen, Mooney, 2008</marker>
<rawString>David L. Chen and Raymond J. Mooney. 2008. Learning to sportscast: a test of grounded language acquisition. In Proceedings of the 25th International Conference on Machine Learning, pages 128–135.</rawString>
</citation>
<citation valid="true">
<authors>
<author>David L Chen</author>
<author>Joohyun Kim</author>
<author>Raymond J Mooney</author>
</authors>
<title>Training a multilingual sportscaster: Using perceptual context to learn language.</title>
<date>2010</date>
<journal>Journal of Artificial Intelligence Research (JAIR),</journal>
<volume>37</volume>
<pages>435</pages>
<contexts>
<context position="36994" citStr="Chen et al., 2010" startWordPosition="6033" endWordPosition="6036">ed training (Zettlemoyer and Collins, 2005; Ge and Mooney, 2006; Snyder and Barzilay, 2007), but more recent work has focused on reducing the amount of supervision required (Artzi and Zettlemoyer, 2013). A few unsupervised approaches exist (Poon and Domingos, 2009; Poon, 2013), but these are specific to translating language into queries in highly structured database and cannot be applied to our more flexible domain. There are few datasets as detailed as the Professional Soccer Commentary Dataset. Early work in understanding soccer commentaries focused on RoboCup soccer (Chen and Mooney, 2008; Chen et al., 2010; Bordes et al., 2010; Hajishirzi et al., 2011) where simple language describes each event, and events are in a one-to-one correspondence with utterances. Another dataset used for language grounding is the Weather Report Dataset (Liang et al., 2009). Here, again, however, we have mostly single utterances paired with single events, and many alignments are made via numerical string matching rather than learning lex394 ical cues. The NFL Recap dataset (Snyder and Barzilay, 2007) is also laden with numerical fact matching, and does not include the fragment-level segmentation annotation that the PS</context>
</contexts>
<marker>Chen, Kim, Mooney, 2010</marker>
<rawString>David L. Chen, Joohyun Kim, and Raymond J. Mooney. 2010. Training a multilingual sportscaster: Using perceptual context to learn language. Journal of Artificial Intelligence Research (JAIR), 37:397– 435.</rawString>
</citation>
<citation valid="true">
<authors>
<author>Donald Davidson</author>
</authors>
<title>The logical form of action sentences. The logic of Decision and Action.</title>
<date>1967</date>
<contexts>
<context position="6653" citStr="Davidson (1967)" startWordPosition="1008" endWordPosition="1009">ions: Syntax is used to structure the information provided by an utterance, and so it seems intuitive that syntactic relations could be leveraged in this task. For example, consider utterance (III) in Figure 2. The multi-resolution grounding of (III) would provide a segmentation of the utterance – or a division of the utterance into the fragments which refer to separate events. In (III), there is an obvious syntactic correlate to the correct segmentation: each verb phrase within the conjunction headed by “and” identifies a separate event. Parsing (III) to an event-based semantics like that of Davidson (1967), one could associate each verb in an utterance with a game event and achieve the desired segmentation. 3Our updated dataset is available at http://ssli. ee.washington.edu/tial/projects/multires/ 387 Unfortunately, there is a preponderance of examples such as (II) in Figure 2, where 4 verbs are used to describe a single “miss” event. (II) illustrates just one of the many difficulties of using syntactic information – elsewhere, events are referenced without an explicit verb whatsoever (such as the use of the phrase “into the books” to refer to a foul event). What is needed instead is a language</context>
</contexts>
<marker>Davidson, 1967</marker>
<rawString>Donald Davidson. 1967. The logical form of action sentences. The logic of Decision and Action.</rawString>
</citation>
<citation valid="true">
<authors>
<author>Marie-Catherine De Marneffe</author>
<author>Christopher D Manning</author>
</authors>
<date>2008</date>
<note>Stanford typed dependencies manual. URL http://nlp. stanford. edu/software/dependencies manual. pdf.</note>
<marker>De Marneffe, Manning, 2008</marker>
<rawString>Marie-Catherine De Marneffe and Christopher D Manning. 2008. Stanford typed dependencies manual. URL http://nlp. stanford. edu/software/dependencies manual. pdf.</rawString>
</citation>
<citation valid="true">
<authors>
<author>Rong-En Fan</author>
<author>Kai-Wei Chang</author>
<author>Cho-Jui Hsieh</author>
<author>XiangRui Wang</author>
<author>Chih-Jen Lin</author>
</authors>
<title>LIBLINEAR: A library for large linear classification.</title>
<date>2008</date>
<journal>Journal of Machine Learning Research,</journal>
<pages>9--1871</pages>
<contexts>
<context position="17133" citStr="Fan et al., 2008" startWordPosition="2780" endWordPosition="2783">r.(wi, ej) wi∈s,ej∈E ψ(wi, eja) (4) where ej is the event to which s is aligned in alignment E, ψatr.(wi, ej) is the affinity between wi and event ej, and ψ(wi, eaj) is the affinity between wi and attribute a of ej. In order to determine the affinity of a word and an event attribute, we create attribute:value classifiers – one for each attribute:value pair that occurs in any event. For example, for goals we create a type:goal classifier, and for unsuccessful events we create an outcome:unsuccessful classifier. For the categorical attributes Type, Outcome, and Pass Events, we fit a linear SVM (Fan et al., 2008) using the utterance-level alignments provided by Ψprior (the exemplar SVMs) to determine the positive and negative examples. For instance, we use all the utterances which are aligned with an event whose type value is “pass” as positive examples for our type:pass classifier, and all other utterances as negative examples. The weight assigned to each dimension in a linear SVM describes the relative importance of that dimension in the classification process. The dimensions of our attribute:value SVMs are the words of the corpus, normalized for case and minus punctuation and stop words. Therefore,</context>
</contexts>
<marker>Fan, Chang, Hsieh, Wang, Lin, 2008</marker>
<rawString>Rong-En Fan, Kai-Wei Chang, Cho-Jui Hsieh, XiangRui Wang, and Chih-Jen Lin. 2008. LIBLINEAR: A library for large linear classification. Journal of Machine Learning Research, 9:1871–1874.</rawString>
</citation>
<citation valid="true">
<authors>
<author>Ruifang Ge</author>
<author>Raymond J Mooney</author>
</authors>
<title>Discriminative reranking for semantic parsing.</title>
<date>2006</date>
<booktitle>In Proceedings of the 44th Annual Meeting of the Association for Computational Linguistics.</booktitle>
<contexts>
<context position="36440" citStr="Ge and Mooney, 2006" startWordPosition="5945" endWordPosition="5948">nguage from the language referring to this game. This is a good avenue for future exploration. The second set of errors we make in fragmentation are caused by bad event priors. Our current setup cannot increase recall: we can only improve the precision of the utterance-level alignments we are given. Therefore, if an event is overlooked in the first-pass of utterance-level alignments, we cannot reintroduce it through a fragment alignment. This is a direction for future work as well. 6 Related Work Early semantic parsing work made use of fully supervised training (Zettlemoyer and Collins, 2005; Ge and Mooney, 2006; Snyder and Barzilay, 2007), but more recent work has focused on reducing the amount of supervision required (Artzi and Zettlemoyer, 2013). A few unsupervised approaches exist (Poon and Domingos, 2009; Poon, 2013), but these are specific to translating language into queries in highly structured database and cannot be applied to our more flexible domain. There are few datasets as detailed as the Professional Soccer Commentary Dataset. Early work in understanding soccer commentaries focused on RoboCup soccer (Chen and Mooney, 2008; Chen et al., 2010; Bordes et al., 2010; Hajishirzi et al., 2011</context>
</contexts>
<marker>Ge, Mooney, 2006</marker>
<rawString>Ruifang Ge and Raymond J. Mooney. 2006. Discriminative reranking for semantic parsing. In Proceedings of the 44th Annual Meeting of the Association for Computational Linguistics.</rawString>
</citation>
<citation valid="true">
<authors>
<author>Hannaneh Hajishirzi</author>
<author>Julia Hockenmaier</author>
<author>Erik T Mueller</author>
<author>Eyal Amir</author>
</authors>
<title>Reasoning about robocup soccer narratives.</title>
<date>2011</date>
<booktitle>In Proceedings of the 27th conference on Uncertainty in Artificial Intelligence,</booktitle>
<pages>291--300</pages>
<contexts>
<context position="37041" citStr="Hajishirzi et al., 2011" startWordPosition="6041" endWordPosition="6044">05; Ge and Mooney, 2006; Snyder and Barzilay, 2007), but more recent work has focused on reducing the amount of supervision required (Artzi and Zettlemoyer, 2013). A few unsupervised approaches exist (Poon and Domingos, 2009; Poon, 2013), but these are specific to translating language into queries in highly structured database and cannot be applied to our more flexible domain. There are few datasets as detailed as the Professional Soccer Commentary Dataset. Early work in understanding soccer commentaries focused on RoboCup soccer (Chen and Mooney, 2008; Chen et al., 2010; Bordes et al., 2010; Hajishirzi et al., 2011) where simple language describes each event, and events are in a one-to-one correspondence with utterances. Another dataset used for language grounding is the Weather Report Dataset (Liang et al., 2009). Here, again, however, we have mostly single utterances paired with single events, and many alignments are made via numerical string matching rather than learning lex394 ical cues. The NFL Recap dataset (Snyder and Barzilay, 2007) is also laden with numerical fact matching, and does not include the fragment-level segmentation annotation that the PSC dataset provides. Impressive advances have be</context>
</contexts>
<marker>Hajishirzi, Hockenmaier, Mueller, Amir, 2011</marker>
<rawString>Hannaneh Hajishirzi, Julia Hockenmaier, Erik T. Mueller, and Eyal Amir. 2011. Reasoning about robocup soccer narratives. In Proceedings of the 27th conference on Uncertainty in Artificial Intelligence, pages 291–300.</rawString>
</citation>
<citation valid="true">
<authors>
<author>Hannaneh Hajishirzi</author>
<author>Mohammad Rastegari</author>
<author>Ali Farhadi</author>
<author>Jessica K Hodgins</author>
</authors>
<title>Semantic understanding of professional soccer commentaries.</title>
<date>2012</date>
<booktitle>In Proceedings of the 28th conference on Uncertainty in Artificial Intelligence.</booktitle>
<contexts>
<context position="2270" citStr="Hajishirzi et al., 2012" startWordPosition="325" endWordPosition="328">vailable at http://ssli. ee.washington.edu/tial/projects/multires/ Figure 1: An example of the multiple resolutions at which soccer commentaries refer to events: The utterance level alignments are shown in the black dashed boxes. The first utterance can be further broken into the fragment-level alignments shown; the second cannot be decomposed further. language acquisition is moving into real-world environments (Yu and Siskind, 2013). Grounding sports commentaries in game events is a specific instance of this problem that has attracted attention (Liang et al., 2009; Snyder and Barzilay, 2007; Hajishirzi et al., 2012), in part because of the complexity of both the language and the world representation involved. The language employed in soccer commentaries is difficult to ground due to its dense information structure, novel vocabulary and word senses, and colorful, non-traditional syntax. These challenges conspire to foil most language processing techniques including automated parsers and wordsense disambiguation systems. In addition to the structural problems presented by the language of soccer commentaries, the problem of reference is further complicated by the fact that for game events (and other real-wo</context>
<context position="5538" citStr="Hajishirzi et al., 2012" startWordPosition="836" endWordPosition="839">tions. Our language model takes advantage of the discourse structure of the commentaries, making it robust enough to handle the unique language of the soccer domain. Finally, our method relies only on 2Though it is tempting to discritize meaning in text, Chafe (1988) shows that readers imbue text with meaningful intonational patterns drawn from the potentially continuous space of auditory signals. loose temporal co-occurrence of events and utterances as supervision and does not require expensive annotated training data. To test our method we augment the Professional Soccer Commentary Dataset (Hajishirzi et al., 2012) with fragment-level event alignment annotations. This dataset is composed of commentaries for soccer matches paired with event logs produced by Opta Sportsdata and includes human annotated gold alignments3. We achieve an F1 improvement of over 48% on fragment-level alignment versus a previous state-of-the-art. We are also able to leverage the interplay of fragment- and utterance- level alignments to improve the previous state-of-the-art utterance-alignment system. 2 Challenges Syntactic Limitations: Syntax is used to structure the information provided by an utterance, and so it seems intuitiv</context>
<context position="15730" citStr="Hajishirzi et al. (2012)" startWordPosition="2522" endWordPosition="2525">2,w3} {w4,w5} {w1,w2} S1 S2 Sn {w3,w4,w5} E1 E2 E3 E4 Chamakh raises highest and aims... pass aerial miss out Figure 3: Factor graph for P(S, EDW). Here the wi are the words of utterance W, Sj are the possible segmentations of W, and Ek are different events. {w3,w4,w5} W S E 389 regards to s. It is here that we produce the multiresolution alignments; s can vary in size from a single word to a whole utterance. ψalign decomposes as the following: Ψalign(E, s) = Ψprior(E) * Ψaffinity(s, E) (3) where the priors (Ψprior) are confidence scores for an alignment E with the whole utterance as given by Hajishirzi et al. (2012), which fits an exemplar SVM to each utterance/event pair. An exemplar SVM is an SVM fit with one positive and many negative instances, allowing us to define an example by what it is not (Malisiewicz et al., 2011; Shrivastava et al., 2011). Ψaffinity scores the affinity between a fragment s and the event ej to which it is aligned. We use the term affinity as a measure of the goodness of an alignment. Intuitively, a fragment s will have a higher affinity for an event ej if s describes that event well. Formally, the affinity between s and ej amounts to a product of the affinity between each word</context>
<context position="25696" citStr="Hajishirzi et al. (2012)" startWordPosition="4170" endWordPosition="4173">f how our Viterbi path coincides with the responses from the attribute:value classifiers. 5 Experiments One justification for multi-resolution language grounding would be if finer-resolution grounding improves coarser-resolution grounding and vice versa. If so, we expect that better utterance-level alignments will improve fragment-level alignments, and that in turn those fragment-level alignments will improve utterance-level alignments. We evaluate both of these hypotheses. 5.1 Experimental Setup Dataset: We use the publicly available Professional Soccer Commentary (PSC) dataset introduced in Hajishirzi et al. (2012). This dataset is composed of professional commentaries from the 2010-2011 season of the English Premier League, along with a human-annotated data feed produced for each game by Opta Sportsdata (Opta, 2012) which describes all events occurring around the ball. Events include passes, shots, misses, cards, Method Precision Recall F1 Liang et al. (2009) 0.327 0.418 0.367 Hajishirzi et al. (2012) 0.355 0.576 0.439 Our approach 0.407 0.520 0.457 Table 4: Utterance-level alignment results tackles, and other relevant game details. Each event category is defined precisely and the feed is annotated by </context>
<context position="27598" citStr="Hajishirzi et al. (2012)" startWordPosition="4470" endWordPosition="4474">eatures of a correct fragment alignment: picking the correct fragment boundaries and associating the fragment with the correct event. We evaluate fragment-level alignment on a per word basis. We consider precision in this task to be the number of correct word to event alignments versus the total number of alignments produced by a system. Recall is the number of correct word to event alignments versus the total gold word to event alignments, of which there are 18,147. Comparisons: We compare to two previous works: Liang et al. (2009), which produces both segmentation and alignment results; and Hajishirzi et al. (2012), which produces state-of-theart alignments. When evaluating segmentation, we compare how well the systems perform starting from the raw dataset, and starting from gold utterance-level alignments. This allows us to isolate the segmentation process from the overall system architectures. It also gives us some insight into the effect of event priors on the segmentation and alignment processes. 5The full dataset is available at http://ssli. ee.washington.edu/tial/projects/multires/ 392 Figure 5: A successful grounding at multiple resolutions. Thin blue lines separate the attribute:value pairs corr</context>
</contexts>
<marker>Hajishirzi, Rastegari, Farhadi, Hodgins, 2012</marker>
<rawString>Hannaneh Hajishirzi, Mohammad Rastegari, Ali Farhadi, and Jessica K Hodgins. 2012. Semantic understanding of professional soccer commentaries. In Proceedings of the 28th conference on Uncertainty in Artificial Intelligence.</rawString>
</citation>
<citation valid="true">
<authors>
<author>Rohit J Kate</author>
<author>Raymond J Mooney</author>
</authors>
<title>Learning language semantics from ambiguous supervision.</title>
<date>2007</date>
<booktitle>In Proceedings of the Twenty-Second AAAI Conference on Artificial Intelligence,</booktitle>
<pages>895--900</pages>
<contexts>
<context position="17940" citStr="Kate and Mooney, 2007" startWordPosition="2914" endWordPosition="2917">ith an event whose type value is “pass” as positive examples for our type:pass classifier, and all other utterances as negative examples. The weight assigned to each dimension in a linear SVM describes the relative importance of that dimension in the classification process. The dimensions of our attribute:value SVMs are the words of the corpus, normalized for case and minus punctuation and stop words. Therefore, the affinity of a word wi and the attribute:value ea j is the weight of the dimension corresponding to wi in the ea j attribute:value classifier. Following others (Liang et al., 2009; Kate and Mooney, 2007), we use string matches to determine the affinity between a word and the Player attribute. In order to make comparisons between the importance of a word in the decision process for different classifiers, we normalize the weight vectors for each. These attribute:value classifiers produce our finest resolution alignments, allowing us to define a correspondence between a single word and a single attribute of any event. By considering ej in terms of its attributes, we are able to compose a score for ej with fragment s. This is a kind of double-sided compositional semantics, where both the meaningf</context>
</contexts>
<marker>Kate, Mooney, 2007</marker>
<rawString>Rohit J. Kate and Raymond J. Mooney. 2007. Learning language semantics from ambiguous supervision. In Proceedings of the Twenty-Second AAAI Conference on Artificial Intelligence, pages 895– 900.</rawString>
</citation>
<citation valid="true">
<authors>
<author>Percy Liang</author>
<author>Michael I Jordan</author>
<author>Dan Klein</author>
</authors>
<title>Learning semantic correspondences with less supervision.</title>
<date>2009</date>
<booktitle>In Proceedings of the Joint Conference of the 47th Annual Meeting of the Association for Computational Linguistics and 4th International Joint Conference on Natural Language Processing of the AFNLP,</booktitle>
<pages>91--99</pages>
<contexts>
<context position="2217" citStr="Liang et al., 2009" startWordPosition="317" endWordPosition="320">esearch in grounded 1Source code and data are available at http://ssli. ee.washington.edu/tial/projects/multires/ Figure 1: An example of the multiple resolutions at which soccer commentaries refer to events: The utterance level alignments are shown in the black dashed boxes. The first utterance can be further broken into the fragment-level alignments shown; the second cannot be decomposed further. language acquisition is moving into real-world environments (Yu and Siskind, 2013). Grounding sports commentaries in game events is a specific instance of this problem that has attracted attention (Liang et al., 2009; Snyder and Barzilay, 2007; Hajishirzi et al., 2012), in part because of the complexity of both the language and the world representation involved. The language employed in soccer commentaries is difficult to ground due to its dense information structure, novel vocabulary and word senses, and colorful, non-traditional syntax. These challenges conspire to foil most language processing techniques including automated parsers and wordsense disambiguation systems. In addition to the structural problems presented by the language of soccer commentaries, the problem of reference is further complicate</context>
<context position="9487" citStr="Liang et al., 2009" startWordPosition="1455" endWordPosition="1458"> described in the second fragment. A human (with sufficient understanding of soccer) knows that, despite being separated by the discourse marker, the phrases “bring him down” and “set piece” both refer to the foul. A language grounding algorithm that can model the interaction between such word-level and utterance-level cues can successfully segment both (I) and (III). Supervision: For language grounding generally, and multi-resolution grounding specifically, supervised training data is expensive to produce. Also, the various grounding domains of interest are highly independent of one another (Liang et al., 2009). In the face of these issues, the ideal correspondence between language and world representation would be learned with as little supervision as possible. 3 Problem Definition We define the problem of multi-resolution language grounding as follows: Given a temporal evolution of a world state (a sequence of events) and an overlapping natural language text (a sequence of utterances), we want to learn the best correspondences between the language and the world at different levels of granularity (Figure 2). To set up notations, for each utterance represented as a set of words W = {w1, w2,. . . , w</context>
<context position="17916" citStr="Liang et al., 2009" startWordPosition="2910" endWordPosition="2913"> which are aligned with an event whose type value is “pass” as positive examples for our type:pass classifier, and all other utterances as negative examples. The weight assigned to each dimension in a linear SVM describes the relative importance of that dimension in the classification process. The dimensions of our attribute:value SVMs are the words of the corpus, normalized for case and minus punctuation and stop words. Therefore, the affinity of a word wi and the attribute:value ea j is the weight of the dimension corresponding to wi in the ea j attribute:value classifier. Following others (Liang et al., 2009; Kate and Mooney, 2007), we use string matches to determine the affinity between a word and the Player attribute. In order to make comparisons between the importance of a word in the decision process for different classifiers, we normalize the weight vectors for each. These attribute:value classifiers produce our finest resolution alignments, allowing us to define a correspondence between a single word and a single attribute of any event. By considering ej in terms of its attributes, we are able to compose a score for ej with fragment s. This is a kind of double-sided compositional semantics,</context>
<context position="24140" citStr="Liang et al. (2009)" startWordPosition="3939" endWordPosition="3942">n. Figure 4 illustrates the setup. For each word wi of the utterance, we create a column of nodes in our trellis, with one row for each event ej ∈ E. The nodes represent the affinity of a given word wi with event ej. The weights on these nodes come from ψatr.(wi, ej) described in section 4.2. The nodes in column wi are connected to the nodes in column wi+1 by edges whose weights w1 w2 w3 Unsuccessful Cross Pass Successful Aerial Head Pass Missed Head Pass ψ(w1,e2) ψ(w1,e3) 1/φ(W1, W2) ψ(w1,e1) ψ(w2,e1) ψ(w3,e1) φ(W1,W0 φ(W1,W0 ψ(w2,e3) ψ(w2,e2) ψ(w3,e2) ψ(w3,e3) 391 Method Precision Recall F1 Liang et al. (2009) 0.513 0.393 0.445 Our approach 0.603 0.481 0.535 Table 2: Fragment-level alignments starting from gold utterance-level alignments Method Precision Recall F1 Liang et al. (2009) 0.211 0.135 0.165 Our approach 0.235 0.255 0.245 Table 3: Fragment-level alignments starting from raw data are drawn from the splitpoint classifier response O(wi, wz+1). We label the edges between adjacent nodes corresponding to different events with the responses from the splitpoint classifier, and the inverse of these responses for edges connecting nodes corresponding to the same event. We then use the Viterbi algori</context>
<context position="26048" citStr="Liang et al. (2009)" startWordPosition="4223" endWordPosition="4226">at in turn those fragment-level alignments will improve utterance-level alignments. We evaluate both of these hypotheses. 5.1 Experimental Setup Dataset: We use the publicly available Professional Soccer Commentary (PSC) dataset introduced in Hajishirzi et al. (2012). This dataset is composed of professional commentaries from the 2010-2011 season of the English Premier League, along with a human-annotated data feed produced for each game by Opta Sportsdata (Opta, 2012) which describes all events occurring around the ball. Events include passes, shots, misses, cards, Method Precision Recall F1 Liang et al. (2009) 0.327 0.418 0.367 Hajishirzi et al. (2012) 0.355 0.576 0.439 Our approach 0.407 0.520 0.457 Table 4: Utterance-level alignment results tackles, and other relevant game details. Each event category is defined precisely and the feed is annotated by professionals according to strict event description guidelines. The PSC also provides ground truth alignment of full utterances to events in the data feed, and for this work we have augmented it with ground truth fragment-level annotations5. We use data from 7 games of the PSC. These games consist of 778 utterances totaling 13,692 words. There are 12</context>
<context position="27512" citStr="Liang et al. (2009)" startWordPosition="4458" endWordPosition="4461">n of fragment-level alignments is less straight forward. This is due to the two features of a correct fragment alignment: picking the correct fragment boundaries and associating the fragment with the correct event. We evaluate fragment-level alignment on a per word basis. We consider precision in this task to be the number of correct word to event alignments versus the total number of alignments produced by a system. Recall is the number of correct word to event alignments versus the total gold word to event alignments, of which there are 18,147. Comparisons: We compare to two previous works: Liang et al. (2009), which produces both segmentation and alignment results; and Hajishirzi et al. (2012), which produces state-of-theart alignments. When evaluating segmentation, we compare how well the systems perform starting from the raw dataset, and starting from gold utterance-level alignments. This allows us to isolate the segmentation process from the overall system architectures. It also gives us some insight into the effect of event priors on the segmentation and alignment processes. 5The full dataset is available at http://ssli. ee.washington.edu/tial/projects/multires/ 392 Figure 5: A successful grou</context>
<context position="37243" citStr="Liang et al., 2009" startWordPosition="6072" endWordPosition="6075">nd Domingos, 2009; Poon, 2013), but these are specific to translating language into queries in highly structured database and cannot be applied to our more flexible domain. There are few datasets as detailed as the Professional Soccer Commentary Dataset. Early work in understanding soccer commentaries focused on RoboCup soccer (Chen and Mooney, 2008; Chen et al., 2010; Bordes et al., 2010; Hajishirzi et al., 2011) where simple language describes each event, and events are in a one-to-one correspondence with utterances. Another dataset used for language grounding is the Weather Report Dataset (Liang et al., 2009). Here, again, however, we have mostly single utterances paired with single events, and many alignments are made via numerical string matching rather than learning lex394 ical cues. The NFL Recap dataset (Snyder and Barzilay, 2007) is also laden with numerical fact matching, and does not include the fragment-level segmentation annotation that the PSC dataset provides. Impressive advances have been made grounding language in instructions. Branavan et al. (2009) and Vogel and Jurafsky (2010) work in the domain of computer technical support instructions, mapping language to actions using reinforc</context>
</contexts>
<marker>Liang, Jordan, Klein, 2009</marker>
<rawString>Percy Liang, Michael I. Jordan, and Dan Klein. 2009. Learning semantic correspondences with less supervision. In Proceedings of the Joint Conference of the 47th Annual Meeting of the Association for Computational Linguistics and 4th International Joint Conference on Natural Language Processing of the AFNLP, pages 91–99.</rawString>
</citation>
<citation valid="true">
<authors>
<author>Tomasz Malisiewicz</author>
<author>Abhinav Gupta</author>
<author>Alexei A Efros</author>
</authors>
<title>Ensemble of exemplar-svms for object detection and beyond.</title>
<date>2011</date>
<booktitle>In Proceedings of the 13th International Conference on Computer Vision.</booktitle>
<contexts>
<context position="15942" citStr="Malisiewicz et al., 2011" startWordPosition="2561" endWordPosition="2564">gmentations of W, and Ek are different events. {w3,w4,w5} W S E 389 regards to s. It is here that we produce the multiresolution alignments; s can vary in size from a single word to a whole utterance. ψalign decomposes as the following: Ψalign(E, s) = Ψprior(E) * Ψaffinity(s, E) (3) where the priors (Ψprior) are confidence scores for an alignment E with the whole utterance as given by Hajishirzi et al. (2012), which fits an exemplar SVM to each utterance/event pair. An exemplar SVM is an SVM fit with one positive and many negative instances, allowing us to define an example by what it is not (Malisiewicz et al., 2011; Shrivastava et al., 2011). Ψaffinity scores the affinity between a fragment s and the event ej to which it is aligned. We use the term affinity as a measure of the goodness of an alignment. Intuitively, a fragment s will have a higher affinity for an event ej if s describes that event well. Formally, the affinity between s and ej amounts to a product of the affinity between each word wi E s and ej. Since ej is defined by a collection of attributes, we can compose a score for wi with ej from the affinity between wi and each attribute a of ej. 11 Ψaffinity(s, E) = ψatr.(wi, ej) wi∈s,ej∈E ψ(wi,</context>
</contexts>
<marker>Malisiewicz, Gupta, Efros, 2011</marker>
<rawString>Tomasz Malisiewicz, Abhinav Gupta, and Alexei A. Efros. 2011. Ensemble of exemplar-svms for object detection and beyond. In Proceedings of the 13th International Conference on Computer Vision.</rawString>
</citation>
<citation valid="true">
<authors>
<author>Cynthia Matuszek</author>
<author>Nicholas FitzGerald</author>
<author>Luke Zettlemoyer</author>
<author>Liefeng Bo</author>
<author>Dieter Fox</author>
</authors>
<title>A Joint Model of Language and Perception for Grounded Attribute Learning.</title>
<date>2012</date>
<booktitle>In Proc. of the 2012 International Conference on Machine Learning,</booktitle>
<location>Edinburgh, Scotland,</location>
<contexts>
<context position="37880" citStr="Matuszek et al. (2012" startWordPosition="6169" endWordPosition="6172">owever, we have mostly single utterances paired with single events, and many alignments are made via numerical string matching rather than learning lex394 ical cues. The NFL Recap dataset (Snyder and Barzilay, 2007) is also laden with numerical fact matching, and does not include the fragment-level segmentation annotation that the PSC dataset provides. Impressive advances have been made grounding language in instructions. Branavan et al. (2009) and Vogel and Jurafsky (2010) work in the domain of computer technical support instructions, mapping language to actions using reinforcement learning. Matuszek et al. (2012b) parses simple language to robot control instructions. Our work focuses on dealing with a richer space, both in terms of the language used and the worldrepresentation into which it is grounded, and leveraging the multiple resolutions of reference. An exciting direction of research, closer to our own, aims to ground natural language in visual perception systems. Matuszek et al. (2012a) attempts to learn a joint model of language and object characteristics of a workplace environment. Yu and Siskind (2013) grounds moderately rich language in automatically annotated video clips. Again, the contr</context>
</contexts>
<marker>Matuszek, FitzGerald, Zettlemoyer, Bo, Fox, 2012</marker>
<rawString>Cynthia Matuszek, Nicholas FitzGerald, Luke Zettlemoyer, Liefeng Bo, and Dieter Fox. 2012a. A Joint Model of Language and Perception for Grounded Attribute Learning. In Proc. of the 2012 International Conference on Machine Learning, Edinburgh, Scotland, June.</rawString>
</citation>
<citation valid="true">
<authors>
<author>Cynthia Matuszek</author>
<author>Evan Herbst</author>
<author>Luke Zettlemoyer</author>
<author>Dieter Fox</author>
</authors>
<title>Learning to parse natural language commands to a robot control system.</title>
<date>2012</date>
<booktitle>In Proc. of the 13th International Symposium on Experimental Robotics (ISER),</booktitle>
<contexts>
<context position="37880" citStr="Matuszek et al. (2012" startWordPosition="6169" endWordPosition="6172">owever, we have mostly single utterances paired with single events, and many alignments are made via numerical string matching rather than learning lex394 ical cues. The NFL Recap dataset (Snyder and Barzilay, 2007) is also laden with numerical fact matching, and does not include the fragment-level segmentation annotation that the PSC dataset provides. Impressive advances have been made grounding language in instructions. Branavan et al. (2009) and Vogel and Jurafsky (2010) work in the domain of computer technical support instructions, mapping language to actions using reinforcement learning. Matuszek et al. (2012b) parses simple language to robot control instructions. Our work focuses on dealing with a richer space, both in terms of the language used and the worldrepresentation into which it is grounded, and leveraging the multiple resolutions of reference. An exciting direction of research, closer to our own, aims to ground natural language in visual perception systems. Matuszek et al. (2012a) attempts to learn a joint model of language and object characteristics of a workplace environment. Yu and Siskind (2013) grounds moderately rich language in automatically annotated video clips. Again, the contr</context>
</contexts>
<marker>Matuszek, Herbst, Zettlemoyer, Fox, 2012</marker>
<rawString>Cynthia Matuszek, Evan Herbst, Luke Zettlemoyer, and Dieter Fox. 2012b. Learning to parse natural language commands to a robot control system. In Proc. of the 13th International Symposium on Experimental Robotics (ISER), June.</rawString>
</citation>
<citation valid="true">
<authors>
<author>Opta</author>
</authors>
<date>2012</date>
<note>http://www.optasports.com.</note>
<contexts>
<context position="25902" citStr="Opta, 2012" startWordPosition="4203" endWordPosition="4204">esolution grounding and vice versa. If so, we expect that better utterance-level alignments will improve fragment-level alignments, and that in turn those fragment-level alignments will improve utterance-level alignments. We evaluate both of these hypotheses. 5.1 Experimental Setup Dataset: We use the publicly available Professional Soccer Commentary (PSC) dataset introduced in Hajishirzi et al. (2012). This dataset is composed of professional commentaries from the 2010-2011 season of the English Premier League, along with a human-annotated data feed produced for each game by Opta Sportsdata (Opta, 2012) which describes all events occurring around the ball. Events include passes, shots, misses, cards, Method Precision Recall F1 Liang et al. (2009) 0.327 0.418 0.367 Hajishirzi et al. (2012) 0.355 0.576 0.439 Our approach 0.407 0.520 0.457 Table 4: Utterance-level alignment results tackles, and other relevant game details. Each event category is defined precisely and the feed is annotated by professionals according to strict event description guidelines. The PSC also provides ground truth alignment of full utterances to events in the data feed, and for this work we have augmented it with ground</context>
</contexts>
<marker>Opta, 2012</marker>
<rawString>Opta. 2012. http://www.optasports.com.</rawString>
</citation>
<citation valid="true">
<authors>
<author>Janet Pierrehumbert</author>
<author>Julia Hirschberg</author>
</authors>
<title>The meaning of intonational contours in the interpretation of discourse.</title>
<date>1990</date>
<journal>Intentions in Communication,</journal>
<pages>271</pages>
<contexts>
<context position="21316" citStr="Pierrehumbert and Hirschberg (1990)" startWordPosition="3485" endWordPosition="3488">ified, including temporal discourse relations. These are indicated by the presence of discourse markers — alternately known as cue phrases. We hypothesize that cue phrases can be used to identify splitpoints and use them in our proxy labeling scheme. This method is not restricted to temporally related discourse: some contingency, expansion, and comparison relations are also analyzed as “easily identifiable”. As such, our segmentation process can also be used to ground language into a world state where these relations would hold. Prosodic Discourse: We also make use of prosodic discourse cues. Pierrehumbert and Hirschberg (1990) claim that intonational phrases play an important role in discourse segmentation. Therefore, we hypothesize that the edges of intonational phrases are very likely to correspond with correct splitpoints. Viewing the commentary transcriptions as a noisy channel of the actual speech signal, we can identify the intonational phrase boundaries with the punctuation inserted in the transcription process. Chafe (1988) confirms that punctuation in written language has a strong correspondence with intonational phrase boundaries, and an assumption like ours has been successfully implemented in speech syn</context>
</contexts>
<marker>Pierrehumbert, Hirschberg, 1990</marker>
<rawString>Janet Pierrehumbert and Julia Hirschberg. 1990. The meaning of intonational contours in the interpretation of discourse. Intentions in Communication, 271.</rawString>
</citation>
<citation valid="true">
<authors>
<author>Emily Pitler</author>
<author>Mridhula Raghupathy</author>
<author>Hena Mehta</author>
<author>Ani Nenkova</author>
<author>Alan Lee</author>
<author>Aravind K Joshi</author>
</authors>
<title>Easily identifiable discourse relations. Technical Reports (CIS),</title>
<date>2008</date>
<pages>884</pages>
<contexts>
<context position="20594" citStr="Pitler et al. (2008)" startWordPosition="3374" endWordPosition="3377">eme we developed is based on conservative components common to a variety of theories of discourse. Discourse theories aim to model the relationships which exist between adjacent utterances in a coherent discourse. Since we consider a sports commentary to be a coherent discourse, we can leverage results from discourse theory in producing our proxy labels. Temporal Discourse: Events in a soccer match occur in a temporal sequence, and so it is reasonable to assume that the language used to describe them will employ temporal discourse relations to distinguish fragments describing separate events. Pitler et al. (2008) have constructed a list of discourse relations which can be easily automatically identified, including temporal discourse relations. These are indicated by the presence of discourse markers — alternately known as cue phrases. We hypothesize that cue phrases can be used to identify splitpoints and use them in our proxy labeling scheme. This method is not restricted to temporally related discourse: some contingency, expansion, and comparison relations are also analyzed as “easily identifiable”. As such, our segmentation process can also be used to ground language into a world state where these </context>
</contexts>
<marker>Pitler, Raghupathy, Mehta, Nenkova, Lee, Joshi, 2008</marker>
<rawString>Emily Pitler, Mridhula Raghupathy, Hena Mehta, Ani Nenkova, Alan Lee, and Aravind K Joshi. 2008. Easily identifiable discourse relations. Technical Reports (CIS), page 884.</rawString>
</citation>
<citation valid="true">
<authors>
<author>Hoifung Poon</author>
<author>Pedro Domingos</author>
</authors>
<title>Unsupervised semantic parsing.</title>
<date>2009</date>
<booktitle>In Proceedings of the 2009 Conference on Empirical Methods in Natural Language Processing,</booktitle>
<pages>1--10</pages>
<contexts>
<context position="36641" citStr="Poon and Domingos, 2009" startWordPosition="5977" endWordPosition="5980">nnot increase recall: we can only improve the precision of the utterance-level alignments we are given. Therefore, if an event is overlooked in the first-pass of utterance-level alignments, we cannot reintroduce it through a fragment alignment. This is a direction for future work as well. 6 Related Work Early semantic parsing work made use of fully supervised training (Zettlemoyer and Collins, 2005; Ge and Mooney, 2006; Snyder and Barzilay, 2007), but more recent work has focused on reducing the amount of supervision required (Artzi and Zettlemoyer, 2013). A few unsupervised approaches exist (Poon and Domingos, 2009; Poon, 2013), but these are specific to translating language into queries in highly structured database and cannot be applied to our more flexible domain. There are few datasets as detailed as the Professional Soccer Commentary Dataset. Early work in understanding soccer commentaries focused on RoboCup soccer (Chen and Mooney, 2008; Chen et al., 2010; Bordes et al., 2010; Hajishirzi et al., 2011) where simple language describes each event, and events are in a one-to-one correspondence with utterances. Another dataset used for language grounding is the Weather Report Dataset (Liang et al., 200</context>
</contexts>
<marker>Poon, Domingos, 2009</marker>
<rawString>Hoifung Poon and Pedro Domingos. 2009. Unsupervised semantic parsing. In Proceedings of the 2009 Conference on Empirical Methods in Natural Language Processing, pages 1–10.</rawString>
</citation>
<citation valid="true">
<authors>
<author>Hoifung Poon</author>
</authors>
<title>Grounded unsupervised semantic parsing.</title>
<date>2013</date>
<booktitle>In Proceedings of the 51st Annual Meeting of the Association for Computational Linguistics.</booktitle>
<contexts>
<context position="36654" citStr="Poon, 2013" startWordPosition="5981" endWordPosition="5982">can only improve the precision of the utterance-level alignments we are given. Therefore, if an event is overlooked in the first-pass of utterance-level alignments, we cannot reintroduce it through a fragment alignment. This is a direction for future work as well. 6 Related Work Early semantic parsing work made use of fully supervised training (Zettlemoyer and Collins, 2005; Ge and Mooney, 2006; Snyder and Barzilay, 2007), but more recent work has focused on reducing the amount of supervision required (Artzi and Zettlemoyer, 2013). A few unsupervised approaches exist (Poon and Domingos, 2009; Poon, 2013), but these are specific to translating language into queries in highly structured database and cannot be applied to our more flexible domain. There are few datasets as detailed as the Professional Soccer Commentary Dataset. Early work in understanding soccer commentaries focused on RoboCup soccer (Chen and Mooney, 2008; Chen et al., 2010; Bordes et al., 2010; Hajishirzi et al., 2011) where simple language describes each event, and events are in a one-to-one correspondence with utterances. Another dataset used for language grounding is the Weather Report Dataset (Liang et al., 2009). Here, aga</context>
</contexts>
<marker>Poon, 2013</marker>
<rawString>Hoifung Poon. 2013. Grounded unsupervised semantic parsing. In Proceedings of the 51st Annual Meeting of the Association for Computational Linguistics.</rawString>
</citation>
<citation valid="true">
<authors>
<author>Abhinav Shrivastava</author>
<author>Tomasz Malisiewicz</author>
<author>Abhinav Gupta</author>
<author>Alexei A Efros</author>
</authors>
<title>Data-driven visual similarity for cross-domain image matching.</title>
<date>2011</date>
<journal>ACM Transaction of Graphics (TOG) (Proceedings ofACMSIGGRAPHASIA),</journal>
<volume>30</volume>
<issue>6</issue>
<contexts>
<context position="15969" citStr="Shrivastava et al., 2011" startWordPosition="2565" endWordPosition="2568">re different events. {w3,w4,w5} W S E 389 regards to s. It is here that we produce the multiresolution alignments; s can vary in size from a single word to a whole utterance. ψalign decomposes as the following: Ψalign(E, s) = Ψprior(E) * Ψaffinity(s, E) (3) where the priors (Ψprior) are confidence scores for an alignment E with the whole utterance as given by Hajishirzi et al. (2012), which fits an exemplar SVM to each utterance/event pair. An exemplar SVM is an SVM fit with one positive and many negative instances, allowing us to define an example by what it is not (Malisiewicz et al., 2011; Shrivastava et al., 2011). Ψaffinity scores the affinity between a fragment s and the event ej to which it is aligned. We use the term affinity as a measure of the goodness of an alignment. Intuitively, a fragment s will have a higher affinity for an event ej if s describes that event well. Formally, the affinity between s and ej amounts to a product of the affinity between each word wi E s and ej. Since ej is defined by a collection of attributes, we can compose a score for wi with ej from the affinity between wi and each attribute a of ej. 11 Ψaffinity(s, E) = ψatr.(wi, ej) wi∈s,ej∈E ψ(wi, eja) (4) where ej is the e</context>
</contexts>
<marker>Shrivastava, Malisiewicz, Gupta, Efros, 2011</marker>
<rawString>Abhinav Shrivastava, Tomasz Malisiewicz, Abhinav Gupta, and Alexei A. Efros. 2011. Data-driven visual similarity for cross-domain image matching. ACM Transaction of Graphics (TOG) (Proceedings ofACMSIGGRAPHASIA), 30(6).</rawString>
</citation>
<citation valid="true">
<authors>
<author>Benjamin Snyder</author>
<author>Regina Barzilay</author>
</authors>
<title>Database-text alignment via structured multilabel classification.</title>
<date>2007</date>
<booktitle>In Proceedings of the 20th International Joint Conference on Artificial Intelligence,</booktitle>
<pages>1713--1718</pages>
<contexts>
<context position="2244" citStr="Snyder and Barzilay, 2007" startWordPosition="321" endWordPosition="324">1Source code and data are available at http://ssli. ee.washington.edu/tial/projects/multires/ Figure 1: An example of the multiple resolutions at which soccer commentaries refer to events: The utterance level alignments are shown in the black dashed boxes. The first utterance can be further broken into the fragment-level alignments shown; the second cannot be decomposed further. language acquisition is moving into real-world environments (Yu and Siskind, 2013). Grounding sports commentaries in game events is a specific instance of this problem that has attracted attention (Liang et al., 2009; Snyder and Barzilay, 2007; Hajishirzi et al., 2012), in part because of the complexity of both the language and the world representation involved. The language employed in soccer commentaries is difficult to ground due to its dense information structure, novel vocabulary and word senses, and colorful, non-traditional syntax. These challenges conspire to foil most language processing techniques including automated parsers and wordsense disambiguation systems. In addition to the structural problems presented by the language of soccer commentaries, the problem of reference is further complicated by the fact that for game</context>
<context position="36468" citStr="Snyder and Barzilay, 2007" startWordPosition="5949" endWordPosition="5952">age referring to this game. This is a good avenue for future exploration. The second set of errors we make in fragmentation are caused by bad event priors. Our current setup cannot increase recall: we can only improve the precision of the utterance-level alignments we are given. Therefore, if an event is overlooked in the first-pass of utterance-level alignments, we cannot reintroduce it through a fragment alignment. This is a direction for future work as well. 6 Related Work Early semantic parsing work made use of fully supervised training (Zettlemoyer and Collins, 2005; Ge and Mooney, 2006; Snyder and Barzilay, 2007), but more recent work has focused on reducing the amount of supervision required (Artzi and Zettlemoyer, 2013). A few unsupervised approaches exist (Poon and Domingos, 2009; Poon, 2013), but these are specific to translating language into queries in highly structured database and cannot be applied to our more flexible domain. There are few datasets as detailed as the Professional Soccer Commentary Dataset. Early work in understanding soccer commentaries focused on RoboCup soccer (Chen and Mooney, 2008; Chen et al., 2010; Bordes et al., 2010; Hajishirzi et al., 2011) where simple language desc</context>
</contexts>
<marker>Snyder, Barzilay, 2007</marker>
<rawString>Benjamin Snyder and Regina Barzilay. 2007. Database-text alignment via structured multilabel classification. In Proceedings of the 20th International Joint Conference on Artificial Intelligence, pages 1713–1718.</rawString>
</citation>
<citation valid="true">
<authors>
<author>Andrew J Viterbi</author>
</authors>
<title>Error bounds for convolutional codes and an asymptotically optimum decoding algorithm. Information Theory,</title>
<date>1967</date>
<journal>IEEE Transactions on,</journal>
<volume>13</volume>
<issue>2</issue>
<contexts>
<context position="24759" citStr="Viterbi, 1967" startWordPosition="4033" endWordPosition="4034">3 0.393 0.445 Our approach 0.603 0.481 0.535 Table 2: Fragment-level alignments starting from gold utterance-level alignments Method Precision Recall F1 Liang et al. (2009) 0.211 0.135 0.165 Our approach 0.235 0.255 0.245 Table 3: Fragment-level alignments starting from raw data are drawn from the splitpoint classifier response O(wi, wz+1). We label the edges between adjacent nodes corresponding to different events with the responses from the splitpoint classifier, and the inverse of these responses for edges connecting nodes corresponding to the same event. We then use the Viterbi algorithm (Viterbi, 1967) to find the maximum scoring path through this trellis. The maximum scoring path optimizes Equation 2, and serves as our approximation of the optimization of Equation 1. We choose the top k diverse paths through the trellis and use the associations therein as our alignments. See Figure 5 for a detailed example of how our Viterbi path coincides with the responses from the attribute:value classifiers. 5 Experiments One justification for multi-resolution language grounding would be if finer-resolution grounding improves coarser-resolution grounding and vice versa. If so, we expect that better utt</context>
</contexts>
<marker>Viterbi, 1967</marker>
<rawString>Andrew J Viterbi. 1967. Error bounds for convolutional codes and an asymptotically optimum decoding algorithm. Information Theory, IEEE Transactions on, 13(2):260–269.</rawString>
</citation>
<citation valid="true">
<authors>
<author>Adam Vogel</author>
<author>Daniel Jurafsky</author>
</authors>
<title>Learning to follow navigational directions.</title>
<date>2010</date>
<booktitle>In Proceedings of the 48th Annual Meeting of the Association for Computational Linguistics,</booktitle>
<pages>806--814</pages>
<contexts>
<context position="37737" citStr="Vogel and Jurafsky (2010)" startWordPosition="6148" endWordPosition="6151">-one correspondence with utterances. Another dataset used for language grounding is the Weather Report Dataset (Liang et al., 2009). Here, again, however, we have mostly single utterances paired with single events, and many alignments are made via numerical string matching rather than learning lex394 ical cues. The NFL Recap dataset (Snyder and Barzilay, 2007) is also laden with numerical fact matching, and does not include the fragment-level segmentation annotation that the PSC dataset provides. Impressive advances have been made grounding language in instructions. Branavan et al. (2009) and Vogel and Jurafsky (2010) work in the domain of computer technical support instructions, mapping language to actions using reinforcement learning. Matuszek et al. (2012b) parses simple language to robot control instructions. Our work focuses on dealing with a richer space, both in terms of the language used and the worldrepresentation into which it is grounded, and leveraging the multiple resolutions of reference. An exciting direction of research, closer to our own, aims to ground natural language in visual perception systems. Matuszek et al. (2012a) attempts to learn a joint model of language and object characterist</context>
</contexts>
<marker>Vogel, Jurafsky, 2010</marker>
<rawString>Adam Vogel and Daniel Jurafsky. 2010. Learning to follow navigational directions. In Proceedings of the 48th Annual Meeting of the Association for Computational Linguistics, pages 806–814.</rawString>
</citation>
<citation valid="true">
<authors>
<author>Haonan Yu</author>
<author>Jeffrey Mark Siskind</author>
</authors>
<title>Grounded language learning from video described with sentences.</title>
<date>2013</date>
<booktitle>In Proceedings of the 51st Annual Meeting of the Association for Computational Linguistics,</booktitle>
<volume>1</volume>
<pages>53--63</pages>
<contexts>
<context position="2083" citStr="Yu and Siskind, 2013" startWordPosition="296" endWordPosition="299">these units. Historically, language grounding was only possible over simple controlled domains and rigidly structured language. Current research in grounded 1Source code and data are available at http://ssli. ee.washington.edu/tial/projects/multires/ Figure 1: An example of the multiple resolutions at which soccer commentaries refer to events: The utterance level alignments are shown in the black dashed boxes. The first utterance can be further broken into the fragment-level alignments shown; the second cannot be decomposed further. language acquisition is moving into real-world environments (Yu and Siskind, 2013). Grounding sports commentaries in game events is a specific instance of this problem that has attracted attention (Liang et al., 2009; Snyder and Barzilay, 2007; Hajishirzi et al., 2012), in part because of the complexity of both the language and the world representation involved. The language employed in soccer commentaries is difficult to ground due to its dense information structure, novel vocabulary and word senses, and colorful, non-traditional syntax. These challenges conspire to foil most language processing techniques including automated parsers and wordsense disambiguation systems. I</context>
<context position="38390" citStr="Yu and Siskind (2013)" startWordPosition="6253" endWordPosition="6256">technical support instructions, mapping language to actions using reinforcement learning. Matuszek et al. (2012b) parses simple language to robot control instructions. Our work focuses on dealing with a richer space, both in terms of the language used and the worldrepresentation into which it is grounded, and leveraging the multiple resolutions of reference. An exciting direction of research, closer to our own, aims to ground natural language in visual perception systems. Matuszek et al. (2012a) attempts to learn a joint model of language and object characteristics of a workplace environment. Yu and Siskind (2013) grounds moderately rich language in automatically annotated video clips. Again, the contribution of our work versus the above is in the complexity of the language with which we deal and our multi-resolution model. 7 Conclusion The problem of grounding complex natural human language such as soccer commentaries is extremely difficult at all resolutions, and it is most challenging at finer resolutions where data is sparsest and small errors cannot be as easily normalized. Our work will help open new avenues of research into this difficult and exciting problem. This paper presents a new method fo</context>
</contexts>
<marker>Yu, Siskind, 2013</marker>
<rawString>Haonan Yu and Jeffrey Mark Siskind. 2013. Grounded language learning from video described with sentences. In Proceedings of the 51st Annual Meeting of the Association for Computational Linguistics, volume 1, pages 53–63.</rawString>
</citation>
<citation valid="true">
<authors>
<author>Luke S Zettlemoyer</author>
<author>Michael Collins</author>
</authors>
<title>Learning to map sentences to logical form: Structured classification with probabilistic categorial grammars.</title>
<date>2005</date>
<booktitle>In Proceedings of the 21st Conference on Uncertainty in Artificial Intelligence,</booktitle>
<pages>658--666</pages>
<contexts>
<context position="36419" citStr="Zettlemoyer and Collins, 2005" startWordPosition="5941" endWordPosition="5944">stem cannot distinguish such language from the language referring to this game. This is a good avenue for future exploration. The second set of errors we make in fragmentation are caused by bad event priors. Our current setup cannot increase recall: we can only improve the precision of the utterance-level alignments we are given. Therefore, if an event is overlooked in the first-pass of utterance-level alignments, we cannot reintroduce it through a fragment alignment. This is a direction for future work as well. 6 Related Work Early semantic parsing work made use of fully supervised training (Zettlemoyer and Collins, 2005; Ge and Mooney, 2006; Snyder and Barzilay, 2007), but more recent work has focused on reducing the amount of supervision required (Artzi and Zettlemoyer, 2013). A few unsupervised approaches exist (Poon and Domingos, 2009; Poon, 2013), but these are specific to translating language into queries in highly structured database and cannot be applied to our more flexible domain. There are few datasets as detailed as the Professional Soccer Commentary Dataset. Early work in understanding soccer commentaries focused on RoboCup soccer (Chen and Mooney, 2008; Chen et al., 2010; Bordes et al., 2010; Ha</context>
</contexts>
<marker>Zettlemoyer, Collins, 2005</marker>
<rawString>Luke S. Zettlemoyer and Michael Collins. 2005. Learning to map sentences to logical form: Structured classification with probabilistic categorial grammars. In Proceedings of the 21st Conference on Uncertainty in Artificial Intelligence, pages 658– 666.</rawString>
</citation>
</citationList>
</algorithm>
</algorithms>
<?xml version="1.0" encoding="UTF-8"?>
<algorithms version="110505">
<algorithm name="SectLabel" version="110505">
<variant no="0" confidence="0.000021">
<title confidence="0.995657">
Translation Rules with Right-Hand Side Lattices
</title>
<author confidence="0.99017">
Fabien Cromières
</author>
<affiliation confidence="0.97773">
Japan Science and Technology Agency
</affiliation>
<note confidence="0.4290035">
Kawaguchi-shi
Saitama 332-0012
</note>
<email confidence="0.988539">
fabien@pa.jst.jp
</email>
<sectionHeader confidence="0.978875" genericHeader="abstract">
Abstract
</sectionHeader>
<bodyText confidence="0.999701454545454">
In Corpus-Based Machine Translation,
the search space of the translation
candidates for a given input sentence
is often defined by a set of (cycle-
free) context-free grammar rules. This
happens naturally in Syntax-Based
Machine Translation and Hierarchi-
cal Phrase-Based Machine Translation
(where the representation will be the
set of the target-side half of the syn-
chronous rules used to parse the input
sentence). But it is also possible to
describe Phrase-Based Machine Trans-
lation in this framework. We propose
a natural extension to this representa-
tion by using lattice-rules that allow
to easily encode an exponential num-
ber of variations of each rules. We also
demonstrate how the representation of
the search space has an impact on de-
coding efficiency, and how it is possible
to optimize this representation.
</bodyText>
<sectionHeader confidence="0.996303" genericHeader="keywords">
1 Introduction
</sectionHeader>
<bodyText confidence="0.9996261875">
A popular approach to modern Machine
Translation is to decompose the translation
problem into a modeling step and a search
step. The modeling step will consist in defin-
ing implicitly a set of possible translations T
for each input sentence. Each translation in
T being associated with a real-valued model
score. The search step will then consist in find-
ing the translation in T with the highest model
score. The search is non-trivial because it is
usually impossible to enumerate all members
of T (its cardinality being typically exponen-
tially dependent on the size of the sentence to
be translated).
Since at least (Chiang, 2007), a common
way of representing T has been through a
</bodyText>
<note confidence="0.9236">
Sadao Kurohashi
Graduate School of Informatics
Kyoto University
Kyoto 606-8501
</note>
<email confidence="0.978498">
kuro@i.kyoto-u.ac.jp
</email>
<bodyText confidence="0.998209707317073">
cycle-free context-free grammar. In such
a grammar, T is represented as a set of
context-free rules such as can be seen on fig-
ure 1. These rules themselves can be gener-
ated by the modeling step through the use
of phrase tables, synchronous parsing, tree-to-
string rules, etc. If the model score of each
translation is taken to be the sum of rule scores
independently given to each rule, the search
for the optimal translation is easy with some
classic dynamic programming techniques.
However, if the model score is going to take
into account informations such as the lan-
guage model score of each sentence, it cannot
be expressed in such a way. Since the lan-
guage model score has proven empirically to
be a very good source of information, (Chiang,
2007) proposed an approximate search algo-
rithm called cube pruning.
We propose here to represent T using
context-free lattice-rules such as shown in fig-
ure 2. This allows us to compactly encode a
large number of rules. One benefit is that it
adds flexibility to the modeling step, making
it easier: many choices such as whether or not
a function word should be included, the rela-
tive position of words and non-terminal in the
translation, as well as morphological variations
can be delegated to the search step by encod-
ing them in the lattice rules. While it is true
that the same could be achieved by an explicit
enumeration, lattice rules make this easier and
more efficient.
In particular, we show that a decoding al-
gorithm working with such lattice rules can
be more efficient than one working directly on
the enumeration of the rules encoded in the
lattice.
A distinct but related idea of this paper is
to consider how transforming the structure of
the rules defining T can lead to improvements
</bodyText>
<page confidence="0.954416">
577
</page>
<note confidence="0.79097">
Proceedings of the 2014 Conference on Empirical Methods in Natural Language Processing (EMNLP), pages 577–588,
October 25-29, 2014, Doha, Qatar. c�2014 Association for Computational Linguistics
</note>
<figureCaption confidence="0.9846775">
Figure 1: A simple cycle-free context grammar
describing a set of possible translations.
</figureCaption>
<bodyText confidence="0.98532180952381">
in the speed/memory performances of the de-
coding. In particular, we propose a method to
merge and reduce the size of the lattice rules
and show that it translates into better perfor-
mances at decoding time.
In this paper, we will first define more pre-
cisely our concept of lattice-rules, then try to
give some motivation for them in the context
of a tree-to-tree MT system (section 3). In sec-
tion 4, we then propose an algorithm for pre-
processing a representation given in a lattice-
rule form that allows for more efficient search.
In section 5, we describe a decoding algorithm
specially designed for handling lattice-rules.
In section 6, we perform some experiments
demonstrating the merit of our approach.
2 Notations and Terminology
Here, we define semi-formally the terms we
will use in this paper. We assume knowledge
of the classic terminology of graph theory and
context-free grammar.
</bodyText>
<subsectionHeader confidence="0.895791">
2.1 Expansion rules
</subsectionHeader>
<bodyText confidence="0.999929363636364">
A flat expansion rule is the association of a
non-terminal and a “flat” right hand side that
we note RHS. A flat RHS is a sequence of
words and non-terminal. See figure 1 for an
example of a set of flat expansion rules.
A set of expansion rules is often produced
in Hierarchical or Syntax-Based MT, by pars-
ing with synchronous grammars or otherwise.
In such a case, the set of rules define a rep-
resentation of the (weighted) set of possible
translations T of an input sentence.
</bodyText>
<subsectionHeader confidence="0.980782">
2.2 Lattice
</subsectionHeader>
<bodyText confidence="0.99837725">
In the general sense, a lattice can be described
as a labeled directed acyclic graph. More pre-
cisely, the type of lattice that we consider in
this work is such that:
</bodyText>
<listItem confidence="0.9559136">
• Edges are labeled by either a word, a
non-terminal or an epsilon (ie. an empty
string).
• Vertices are only labeled by a unique id
by which they can be designated.
</listItem>
<bodyText confidence="0.9992685">
Additionally, edges can also be labeled by a
real-valued edge score and some real-valued
edge features. Alternatively, a lattice could
also be seen as an acyclic Finite State Automa-
ton, with vertices and edges corresponding to
states and transitions in the FSA terminology.
For simplicity, we also set the constraint
that each lattice has a unique “start” ver-
tex labeled vS from which each vertex can be
reached and a unique “end” vertex vE that can
be reached from each vertex. Each path from
vS to vE define thus a flat RHS, with score
and features obtained by summing the score
and features of each edge of the path.
A lattice expansion rule is similar to a flat
expansion rule, but with the RHS being a lat-
tice. Thus a set of lattice expansion rules can
also define a set of possible translations T of
an input sentence.
For a given lattice L, we will often note v E
L a vertex of L and e : v1 —* v2 E L an edge
of L going from vertex v1 to vertex v2.
Figures 2 and 3 show examples of such lat-
tices.
</bodyText>
<subsectionHeader confidence="0.94143">
2.3 Translation set and
Representations
</subsectionHeader>
<bodyText confidence="0.9992135625">
We note T a set of weighted sentences. T is in-
tended as representing the set of scored trans-
lation candidates generated by a MT system
for a given input sentence. As is customary in
Corpus-Based MT literature, we will call de-
coding the process of searching for the trans-
lation with highest score in T .
A representation of T , noted RT is a set of
rules in a given formalism that implicitly de-
fine T . As we mentioned earlier, in MT, RT is
often a set of cycle-free context-free grammar
rules.
In this paper, we consider representations
RT consisting in a set of lattice expansion
rules. With normal context-free grammar, it
is usually necessary that a non-terminal is the
</bodyText>
<page confidence="0.997433">
578
</page>
<figureCaption confidence="0.753472">
Figure 2: A simple example of lattice rule for
non-terminal X0. The lower part list the set
of “flat” rules that would be equivalent to the
ones expressed by the lattice.
</figureCaption>
<bodyText confidence="0.999232">
left-hand side of several rules. Using lattice
expansion rules, however, it is not necessary,
as one lattice RHS can encode an arbitrary
number of flat rules (see for example the RHS
of X0 in figure 3). Therefore, we set the con-
straint that there is only one lattice expansion
rule for each left-hand non-terminal. And we
will note unambiguously RHS(X) the lattice
that is the right hand side of this rule.
</bodyText>
<sectionHeader confidence="0.993794" genericHeader="introduction">
3 Motivation
</sectionHeader>
<subsectionHeader confidence="0.996723">
3.1 Setting
</subsectionHeader>
<bodyText confidence="0.999959136363636">
This work was developed mainly in the context
of a syntactic-dependency-based tree-to-tree
translation system described in (Richardson et
al., 2014). Although it is a tree-to-tree sys-
tem, we simplify the decoding step by “flatten-
ing” the target-side tree translation rules into
string expansion rules (keeping track of the de-
pendency structure in state features). Thus
our setting is actually quite similar to that
of many tree-to-string and string-to-string sys-
tems. Aiming at simplicity and generality, we
will set aside the question of target-side syn-
tactic information and only describe our algo-
rithms in a “tree-to-string” setting. We will
also consider a n-gram language model score
as our only stateful non-local feature.
However, this tree-to-tree original setting
should be kept in mind, in particular when
we describe the issue of the relative position
of heads and dependents in section 3.2.2, as
such issues do not appear as commonly in “X-
to-string” settings.
</bodyText>
<subsectionHeader confidence="0.998627">
3.2 Rule ambiguities
</subsectionHeader>
<bodyText confidence="0.9561326">
Expansion rules are typically created by
matching part of the input sentence with
some aligned example bilingual sentence. The
alignment (and the linguistic structure of
the phrase in the case of Syntax-Based Ma-
chine Translation) is then used to produce the
target-side rule. However, it is often the case
that it is difficult to fully specify a rule from
an example. Such cases often come from two
main reasons:
</bodyText>
<listItem confidence="0.958739857142857">
• Imperfect knowledge (eg. it is unclear
whether a given unaligned word should
belong to the translation)
• Context dependency (eg. the question of
whether “to be” should be in plural form
or not, depending on its subject in the
constructed translation).
</listItem>
<bodyText confidence="0.999017818181818">
In both situation, it seems like it would be
better to delay the full specification of the
rule until decoding time, when the decoder
can have access to the surrounding context of
the rule and make a more informed choice. In
particular, we can expect features such as lan-
guage model or governor-dependent features
(in the case of tree-to-tree Machine transla-
tion) to help remove the ambiguities.
We detail some cases for which we encode
variations as lattice-rule.
</bodyText>
<sectionHeader confidence="0.468738" genericHeader="method">
3.2.1 Non-aligned words
</sectionHeader>
<bodyText confidence="0.999643833333333">
When rules are extracted from aligned exam-
ples, we often find some target words which
are not aligned to any source-side word and
for which it is difficult to decide whether or
not they should be included in the rule. Such
words are often function words that do not
have an equivalent in the source language.
In Japanese-English translations, for example,
articles such as “a” and “the” do not typically
have equivalent in the Japanese side, and their
necessity in the final sentence will often be a
matter of context. We can make these edges
</bodyText>
<page confidence="0.990571">
579
</page>
<bodyText confidence="0.999860833333333">
optionals by doubling them with an epsilon-
edge. Different weights and features can be
given to the epsilon edges to balance the ten-
dency of the decoder to skip edges. In figure 2,
this is illustrated by the epsilon edges allowing
to skip “for” and “the”
</bodyText>
<subsubsectionHeader confidence="0.794037">
3.2.2 Non-terminal positions
</subsubsectionHeader>
<bodyText confidence="0.999916636363636">
In the context of our tree-to-tree translation
system, we often find that we know which tar-
get word should be the governor of a given
non-terminal, but that we are unsure of the
order of the words and non-terminals sharing
a common governor. It can be convenient to
represent such ambiguities in a lattice format
as shown in figure 2. In this figure, one can see
that the RHS of X0 encode two possible order-
ing for the word “bus” and the non-terminal
X2.
</bodyText>
<subsubsectionHeader confidence="0.661693">
3.2.3 Word variations
</subsubsectionHeader>
<bodyText confidence="0.999785225806452">
Linguistics phenomenons such as morpholog-
ical variations can naturally create many mi-
nor problems in the setting of Corpus-Based
Translation. Especially if the variations in
the target language have no equivalence in
the source language. An example of this in
Japanese-English translation is the fact that
verbs in Japanese are “plural-independent”,
while the verb “to be” in English is not. There-
fore, a RHS that is a candidate for translating
a large part of a Japanese input sentence can
easily use one of the variant of “to be” that is
not consistent with the full sentence. To solve
this, for each edge corresponding to the words
“is” or “are”, we add an alternative edge with
the same start and end vertices as the other
word. The decoder will then be able to choose
the edge that gives the best language model
score. The same can be done, for example, for
the article “a/an”. Figure 2 provides an exam-
ple of this, with two edges “is” and “are” in
the RHS of X0.
Alternative edges can be labeled with differ-
ent weights and features to tune the tendency
of the decoder to choose a morphological vari-
ation.
While such variations could be fixed in a
post-processing step, we feel it is a better op-
tion to let the decoder be aware of the possible
options, lest it would discard rules due to lan-
guage model considerations when these rules
</bodyText>
<figureCaption confidence="0.8724695">
Figure 3: The lattice RHS(X0) optimized with
the algorithm described in section 4
</figureCaption>
<bodyText confidence="0.980551">
could actually have been useful with a simple
change.
</bodyText>
<sectionHeader confidence="0.956268" genericHeader="method">
4 Representation optimisation
</sectionHeader>
<subsectionHeader confidence="0.869777">
4.1 Goal
</subsectionHeader>
<bodyText confidence="0.999949357142857">
Given a description as a set of rule and scores
RT of T , it is often possible to find another de-
scription RT of T having the same formalism
but a different set of rules. Although the T
that is described remains the same, the same
search algorithm applied to R�� or R�� might
make approximations in a different way, be
faster or use less memory.
It is an interesting question to try to trans-
form an initial representation RT into a rep-
resentation RT that will make the search step
faster. This is especially interesting if one is
going to search the same T several times, as is
often done when one is fine-tuning the param-
eters of a model, as this representation opti-
misation needs only be done once.
The optimisation we propose is a natural fit
to our framework of lattice rules. As lattice are
a special case of Finite-State Automata (FSA),
it is easy to adapt existing algorithms for FSA
minimization. We describe a procedure in al-
gorithm 1, which is essentially a simplification
and adaptation to our case of the more gen-
eral algorithm of (Hopcroft, 1971) for FSA.
The central parts of the algorithm are the two
sub-procedures backward vertex merging and
forward vertex merging. An example of the
result of an optimisation is given on figure 3.
</bodyText>
<page confidence="0.973831">
580
</page>
<figure confidence="0.985507368421053">
Data: Representation RT
Result: Optimized Representation
1 for non-terminal X E RT do
2 Apply backward vertex merging to
RH5(X);
3 Apply forward vertex merging to
RH5(X);
4 end
Algorithm 1: Representation optimisation
Data: Lattice RHS G
Result: Optimized Lattice RHS
1 P ; //processed vertices;
2 C {vS} //candidate set ;
3 while |C |&gt; 0 do
for v E C do
Eliminate duplicate edges in
incoming(v);
Mark edges in outgoing(v);
end
</figure>
<subsectionHeader confidence="0.701073">
4.2 Forward and backward merging
</subsectionHeader>
<bodyText confidence="0.9969379">
We describe the forward vertex merging in
algorithm 2. This merging will merge ver-
tices and suppress redundant edges, proceed-
ing from left to right. The end result is a
lattice with a reduced number of vertices and
edges, but encoding the same paths as the ini-
tial one.
The basic idea here is to check the vertices
from left to right and merge the ones that have
identical incoming edges. After having been
processed by the algorithm, a vertex is put in
the set P (line 9). At each iteration, the can-
didate set C contains the set of vertices that
can potentially be merged together. It is up-
dated at each iteration to contain the set of
not-yet-processed vertices for which all incom-
ing edges come from processed vertices (done
by marking edges at line 6 and then updating
C at line 10). At each iteration, the merging
process consists in:
</bodyText>
<listItem confidence="0.975997166666667">
1. Eliminating duplicate edges from the pro-
cessed vertices to the candidate vertices
(line 5). These duplicate edges could have
been introduced by the merging of previ-
ously processed vertices.
2. Merging vertices whose set of incom-
</listItem>
<bodyText confidence="0.999819090909091">
ing edges is identical. Here, merg-
ing two vertices v1 and v2 means
that we create a third vertex v3
such that incoming(v3) = incoming(v1)
= incoming(v2), and outgoing(v3) =
outgoing(v31) ∪ outgoing(v2), then re-
move v1 and v2.
The backward vertex merging is defined
similarly to the forward merging, but with go-
ing right to left and inverting the role of the
incoming and outgoing edges.
</bodyText>
<equation confidence="0.888102857142857">
Merge all vertices v1, v2 E C such that
incoming(v1) = incoming(v2);
9 P P ∪C;
10 C {v E G ∖ P s.t. all edges in
incoming(v) are marked};
11 end
Algorithm 2: Forward Vertex Merging
</equation>
<bodyText confidence="0.98387525">
4.3 Optimizing the whole
representation
Algorithm 1 describe the global optimisation
procedure. For each lattice RHS, we just per-
form first a backward merge and then a for-
ward merge.
We have set the constraint in section 2.3
that each non-terminal should have only one
lattice RHS. Note here that if there are sev-
eral RHS for a given non-terminal, we can first
merge them by merging their start vertex and
end vertex, then apply this optimisation al-
gorithm to obtain a representation with one
optimised RHS per non-terminal.
This optimisation could be seen as doing
some form of hypothesis recombination, but of-
fline.
In term of rule optimisations, we only con-
sider here transformations that do not mod-
ify the number of non-terminals. But it is
worthwhile to note that there are some se-
quence appearing in the middle of some rules
that cannot be merged through a lattice rep-
resentation, but could be factored as sub-rules
appearing in different non-terminals. Indeed,
a lattice rule could actually be encoded as a
set of “flat” rules by introducing a sufficient
number of non-terminals, but this could pos-
sibly be less efficient from the search algorithm
point of view. We plan to investigate the ef-
fects of this type of rule optimisations in con-
junction with the described lattice-type opti-
</bodyText>
<figure confidence="0.9664886">
4
5
6
7
8
</figure>
<page confidence="0.978108">
581
</page>
<bodyText confidence="0.930791">
misations in the future.
</bodyText>
<subsectionHeader confidence="0.985282">
4.4 Handling of Edge Features
</subsectionHeader>
<bodyText confidence="0.999988394736842">
In the context of parameter tuning, we usually
want the decoder to output not only the trans-
lations, but also a list of features characteriz-
ing the way the translation was constructed.
Such features are, for example, the number of
rules used, the language model of the transla-
tion, etc. In out context, some features will be
dependent on the specific edges used in a rule.
For example, the epsilon edge used to option-
ally skip non-aligned words (see section 3.2.1)
is labeled with a feature “nb-words-skipped”
set to 1, so that we can obtain the number
of words skipped in a given translation and
tune a score penalty for skipping such words.
Similar features also exist for picking a word
variation (section 3.2.3).
In the description of the merging process
of section 4.2, one should thus be aware that
two edges are to be considered identical only
if both their associated word and their set of
feature values are identical. This can some-
times prevent useful merging of states to take
place. A solution to this could be to follow
(de Gispert et al., 2010) and to discard all
these features information during the decod-
ing. The features values are then re-estimated
afterward by aligning the translation and the
input with a constrained version of the de-
coder.
We prefer to actually keep track of the fea-
tures values, even if it can reduce the efficiency
of vertex merging. In that setting, we can also
adapt the so-called Weight Pushing algorithm
(Mohri, 2004) to a multivalues case in order
to improve the “mergeability” of vertices. The
results of section 6.1 shows that it is still pos-
sible to strongly reduce the size of the lattices
even when keeping track of the features values.
</bodyText>
<sectionHeader confidence="0.992014" genericHeader="method">
5 Decoding algorithm
</sectionHeader>
<bodyText confidence="0.999983">
In order to make an optimal use of these
lattice-rule representations, we developed a
decoding algorithm for translation candidate
sets represented as a set of lattice-rules. For
the most part, this algorithm re-use many of
the techniques previously developed for decod-
ing translation search spaces, but adapt them
to our setting.
</bodyText>
<subsectionHeader confidence="0.802612">
5.1 Overview
</subsectionHeader>
<bodyText confidence="0.999842">
The outline of the decoding algorithm is de-
scribed by algorithm 3. For simplicity, the
description only compute the optimal model
score over the translations in the candidate set.
It is however trivial to adapt the description
to keep track of which sentence correspond to
this optimal score and output it instead of the
score. Likewise, using the technique described
in (Huang and Chiang, 2005), one can easily
output k-best lists of translations. For sim-
plicity again, we consider that a n-gram lan-
guage model score is the only stateful non-
local feature used for computing the model
score, although in a tree-to-tree setting, other
features (local in a tree representation but not
in a string representation) could be used. The
model score of a translation t has therefore the
shape:
</bodyText>
<equation confidence="0.9871395">
score(t) � A · lm(t) + ∑ score(e)
e
</equation>
<bodyText confidence="0.99993625">
where A is the weight of the language model,
lm(t) is the language model log-probability of
t and the sum is over all edges e crossed to
obtain t.
</bodyText>
<subsectionHeader confidence="0.968585">
5.2 Scored language model states
</subsectionHeader>
<bodyText confidence="0.996780863636364">
Conceptually, in a lattice G, at each vertex
v, we can consider the partial translations ob-
tained by starting at vS and concatenating the
words labeling each edge not labeled by a non-
terminal until v. If an edge is labeled by a non-
terminal X, we first traverse the correspond-
ing lattice RHS(X) following the same pro-
cess. Such a partial translation can be reduced
compactly to a scored language model state
(l, r, s), where l represent the first n wordsi of
the partial translation, r its last n words and s
its partial score. It is clear that if two partial
translations have the same l and r parts but
different score, we can discard the one with
the lowest score, as it cannot be a part of the
optimal translation.
Further, using the state reduction tech-
niques described in (Li and Khudanpur, 2008)
and (Heafield et al., 2011), we can often reduce
the size of l and r to less than n, allowing fur-
ther opportunities for discarding sub-optimal
In being the order of the language mode
</bodyText>
<page confidence="0.994702">
582
</page>
<bodyText confidence="0.999952428571429">
partial translations. For better behavior dur-
ing the cube-pruning step of the algorithm (see
later), the partial score s of a partial transla-
tion includes rest-costs estimates (Heafield et
al., 2012).
We define the concatenation operation
on scored language model states to be:
</bodyText>
<equation confidence="0.9890075">
(l1, r1, s1) ® (l2, r2, s2) = (l3, r3, s3), where
s3 = s1 + s2 + Alm(r1,l2), with lm(r1,l2) be-
</equation>
<bodyText confidence="0.938606">
ing the language model probability of l2 given
r1 with rest-costs adjustments. r3 and l3 are
the resulting minimized states. Similarly, if
an edge e is labeled by a word, we define
the concatenation of a scored state with an
edge to be (l1, r1, s1) ® e = (l2, r2, s2) where
</bodyText>
<equation confidence="0.848472">
s2 = s1 + score(e) + Alm(word(e)|r1).
</equation>
<bodyText confidence="0.99739375">
Conveniently for us, the KenLM2 open-
source library (Heafield, 2011) provides func-
tionalities for easily computing such concate-
nation operations.
</bodyText>
<subsectionHeader confidence="0.956737">
5.3 Algorithm
</subsectionHeader>
<bodyText confidence="0.9998785">
Having defined these operations, we can now
more easily describe algorithm 3. Each vertex
v has a list best[v] of the scored states of the
best partial translations found to be ending
at v. On line 1, we initialize best[vS] with
(., ., 0), where “.” represent an empty language
model state. We then traverse the vertices of
the lattice in topological order.
For each edge e : v1 —* v2, we compute new
scored states for best[v2] as follow:
</bodyText>
<listItem confidence="0.899628333333333">
• if e is labeled by a word or an epsilon, we
create a state st2 = st1 ® e for each st1 in
best[v1] (line 10).
• if e is labeled by a non-terminal X, we re-
cursively call the decoding algorithm on
the lattice RHS(X). The value returned
by the line 15 will be a set of states corre-
sponding to optimal partial translations
traversing RHS(X). We can concate-
nate these states with the ones in best[v1]
to obtain states corresponding to partial
translations ending at v2 (line 6).
</listItem>
<bodyText confidence="0.98971575">
Results of the calls decode(X) are memo-
ized, as the same non-terminal is likely to ap-
pear in several edges of a RHS and in several
RHS.
</bodyText>
<footnote confidence="0.801781">
2http://kheafield.com/code/kenlm/
</footnote>
<bodyText confidence="0.999941285714286">
Lines 5 and 6 are the “cube-pruning-like”
part of the algorithm. The function pruneK
returns the K best combinations of states
in best[v] and decode(RHS(X)), where best
means “whose sum of partial score is highest”.
It can be implemented efficiently through the
algorithms proposed in (Huang and Chiang,
2005) or (Chiang, 2007).
The L st operation on lines 6 and
10 has the following meaning: L is a list of
scored language model state and st is a scored
language model state. L st means that,
if L already contains a state st2 with same left
and right state as st, L is updated to contain
only the scored state with the maximum score.
If L do not contain a state similar to st, st in
simply inserted into L. This is the “hypothe-
sis recombination” part of the algorithm. The
function truncK′ truncate the list best[v] to its
K′ highest-scored elements.
The final result is obtained by calling
decode(X0), where X0 is the “top-level” non-
terminal. The result of decode(X0) will
contain only one scored state of the form
(BOS, EOS, s), with s being the optimal
score.
The search procedure of algorithm 3 could
be described as “breadth-first”, since we sys-
tematically visit each edge of the lattice. An
alternative would be to use a “best-first”
search with an A*-like procedure. We have
tried this, but either because of optimisation
issues or heuristics of insufficient qualities, we
did not obtain better results than with the al-
gorithm we describe here.
</bodyText>
<sectionHeader confidence="0.997328" genericHeader="method">
6 Evaluation
</sectionHeader>
<bodyText confidence="0.999953166666667">
We now describe a set of experiments aimed
at evaluating our approach.
We use the Japanese-English data from the
NTCIR-10 Patent MT task3 (Goto et al.,
2013). The training data contains 3 millions
parallel sentences for Japanese-English.
</bodyText>
<subsectionHeader confidence="0.9434805">
6.1 Effect of Lattice Representation
and Optimisation
</subsectionHeader>
<bodyText confidence="0.999498666666667">
We first evaluate the impact of the lattice rep-
resentation on the performances of our decod-
ing algorithm. This will allow us to measure
</bodyText>
<footnote confidence="0.952153">
3http://ntcir.nii.ac.jp/PatentMT-2/
</footnote>
<page confidence="0.997841">
583
</page>
<figure confidence="0.972521842105263">
Data: Lattice RHS G average. This leads to a 2-fold speed improve-
Result: Sorted list of best states ment in the decoding step, as well as a large
1 best[vE] = {(.,.,0.0)}; reduction of memory usage.
2 for vertex v E G in topological order do
3 for
4 edge e : v —* v2 E outgoing(v) do
if label(e) = X then
5 for st1, st2 E pruneK(best[v],
decode(RHS(X)) do
6 best[v2]1--max st1 ® st2;
7 end
8 else
9 for st E truncK′(best[v]) do
10 best[v2]1--max st ® e;
11 end
12 end
13 end
14 end
15 return best[vE];
</figure>
<figureCaption confidence="0.618271">
Algorithm 3: Lattice-rule decoding. See
body for detailed explanations.
</figureCaption>
<bodyText confidence="0.9969847">
the benefits of our compact lattice represen-
tation of rules, as well as the benefits of the
representation optimisation algorithm of sec-
tion 4.
We use our Syntactic-dependency system to
generate a lattice-rule representation of the
possible translations of the 1800 sentences of
the development set of the NTCIR-10 Patent
MT task. We then produce two additional rep-
resentations:
</bodyText>
<listItem confidence="0.976802333333333">
1. An optimized lattice-rule representation
using the method described in section 4.
2. An expanded representation, that un-
</listItem>
<bodyText confidence="0.9838705">
fold the original lattice-rule representa-
tion into “flat rules” enumerating each
path in the original lattice-rule represen-
tation (like the list X0′ enumerate the lat-
tice X0 in figure 2).
Table 1 shows 3 columns. One for each of
these 3 representations. We can see that, as
expected, the performances in term of average
search time or peak memory used are directly
related to the number of vertices and edges
in the representation. We can also see that
our representation optimisation step is quite
efficient, since it is able to divide by two the
number of vertices in the representation, on
</bodyText>
<subsectionHeader confidence="0.999557">
6.2 Decoding performances
</subsectionHeader>
<bodyText confidence="0.999976955555556">
In order to further evaluate the merit of our
approach, we now compare the results ob-
tained by using our decoder with lattice-rules
with using a state-of-the-art decoder on the
set of flat expanded rules equivalent to these
lattice rules.
We use the decoder described in (Heafield
et al., 2013), which is available under an open-
source license (henceforth called K-decoder).
In this experience, we expanded the lattice
rules generated by our MT system for 1800
sentences into files having the required format
for the K-decoder. This basically mean we
computed an equivalent of the expanded rep-
resentation of section 6.1. This process gener-
ated files ranging in size from 20MB to 17GB
depending on the sentence. We then ran the
K-decoder on these files and compared the re-
sults with our own. We used a beam-width
of 10000 for the K-decoder. Experiments were
run in single thread mode. Partly to obtain
more consistent results, and partly because the
K-decoder was risking using too much memory
for our system.
The results on table 3 show that, as the K-
decoder do not have access to a more compact
representation of the rules, it end up needing
a much larger amount of memory for decoding
the same sentences.
In term of model score obtained, the perfor-
mances are quite similar, with the lattice-rule
decoder providing slightly better model score.
It is interesting to note that, on “fair-
ground” comparison, that is if our decoder do
not have the benefit of a more compact lattice-
rule representation, it actually perform quite
worse as we can see by comparing with the
third column of table 1 (at least in term of de-
coding time and memory usage, while it would
still have a very slight edge in term of model
score with the selected settings). On the other
hand, the K-decoder is a rather strong base-
line, shown to perform several times faster
than a previous state-of-the-art implementa-
tion in (Heafield et al., 2013). It is well opti-
</bodyText>
<footnote confidence="0.980016">
4http://kheafield.com/code/search/
</footnote>
<page confidence="0.990126">
584
</page>
<table confidence="0.9992904">
Representation: Original Optimized Expanded
Peak memory used 39 GB 16GB 85GB
Average search time 6.13s 3.31s 9.95s
#vertices (avg/max) 65K (1300K) 32K (446K) 263K (5421K)
#edges (avg/max) 92K (1512K) 83K (541K) 263K (5421K)
</table>
<tableCaption confidence="0.99306">
Table 1: Impact of the lattice representation on performances.
</tableCaption>
<table confidence="0.99781375">
System JA–EN
Lattice 29.43
No-variations 28.91
Moses (for scale) 28.86
</table>
<tableCaption confidence="0.9831695">
Table 2: Impact on BLEU of using flexible
lattice rules.
</tableCaption>
<bodyText confidence="0.9969442">
mized and makes use of advanced techniques
with the language model (as the one described
in (Heafield et al., 2013)) for which we do not
have implemented an equivalent yet. There-
fore, we are hopeful we can further improve
our decoder in the future.
Also, note that, for practical reason, while
we only measured the decoding time for our
decoder 5, the K-decoder time include the time
taken for loading the rule files.
</bodyText>
<subsectionHeader confidence="0.994946">
6.3 Translation quality
</subsectionHeader>
<bodyText confidence="0.999498363636364">
Finally, we evaluate the advantages of ex-
tracting lattice rules such as proposed in sec-
tion 3. That is, we consider rules for which
null-aligned words are bypassable by epsilon-
edges, for which Non-terminal are allowed to
take several alternative positions around the
word that is thought to be their governor, and
for which we consider alternative morphologies
of a few words (“is/are”, “a/an”). We compare
this approach with heuristically selecting only
one possibility for each variation present in the
lattice rule extracted from a single example.
Results shown on figure 2 show that we
do obtain a significant improvement in trans-
lation quality. Note that the Moses score
(Koehn et al., 2007), taken from the official re-
sults of NTCIR-10 is only here “for scale”, as
our MT system uses a quite different pipeline.
Sin particular, we factored out the representation
optimisation time, which is reasonable if we are in the
setting of a parameter tuning step in which the same
sentences are translated repeatedly
</bodyText>
<sectionHeader confidence="0.999685" genericHeader="method">
7 Related work
</sectionHeader>
<bodyText confidence="0.999979634146341">
Searching for the most optimal translation in
an implicitly defined set has been the focus of
a lot of research in Machine Translation and
it would be difficult to cover all of it. Among
the most influential approaches, (Koehn et al.,
2003) was using a form of stack based de-
coding for Phrase-Based Machine Translation.
(Chiang, 2007) introduced the cube-pruning
approach, which has been further improved
in the previously mentioned (Heafield et al.,
2013). (Rush and Collins, 2011) recently pro-
posed an algorithm promising to find the op-
timal solution, but that is rather slow in prac-
tice.
Weighted Finite State Machines have seen
a variety of use in NLP (Mohri, 1997). More
specifically, some other previous work on Ma-
chine Translation have used lattices (or more
generally Weighted Finite State Machines). In
the context of Corpus-Based Machine Trans-
lation, (Knight and Al-Onaizan, 1998) was al-
ready proposing to use Weighted Transducers
to decode the “IBM” models of translation
(Brown et al., 1993). (Casacuberta and Vi-
dal, 2004) and (Kumar et al., 2006) also pro-
pose to directly model the translation process
with Finite State Transducers. (Graehl and
Knight, 2004) propose to use Tree Transducers
for modeling Syntactic Machine Translation.
These approaches are however based on differ-
ent paradigm, typically trying to directly learn
a transducer rather than extracting SCFG-like
rules.
Closer to our context, (de Gispert et al.,
2010) propose to use Finite-State Transducers
in the context of Hierarchical Phrase Based
Translation. Their method is to iteratively
construct and minimize the full “top-level lat-
tice” representing the whole set of translations
bottom-up. It is an approach more focused
on the Finite State Machine aspect than our,
</bodyText>
<page confidence="0.99482">
585
</page>
<table confidence="0.9998036">
System K-decoder Lattice-rule decoder
Peak memory used 52G 16G
Average search time 3.47s 3.31s
Average model score -107.55 -107.39
Nb wins 401 579
</table>
<tableCaption confidence="0.998949">
Table 3: Evaluation of the performances of our lattice-rule decoder compared with a state-of-
</tableCaption>
<bodyText confidence="0.995458909090909">
the-art decoder using an expanded flat representation of the lattice rules. “Nb wins” is the
number of times one of the decoder found a strictly better model score than the other one, out
of 1800 search.
which is more of an hybrid approach that stays
closer to the paradigm of cube-pruning. The
merit of their approach is that they can apply
minimization globally, allowing for more possi-
bilities for vertex merging. On the other hand,
for large grammars, the “top-level lattice” will
be huge, creating the need to prune vertices
during the construction. Furthermore, the
composition of the “top-level lattice” with a
language model will imply redundant compu-
tations (as lower-level lattices will potentially
be expanded several times in the top-level lat-
tice). As we do not construct the global lattice
explicitly, we do not need to prune vertices (we
only prune language model states). And each
edge of each lattice rule is crossed only once
during our decoding.
Very recently, (Heafield et al., 2014) also
considered using the redundancy of translation
hypotheses to optimize phrase-based stack de-
coding. To do so, they group the partial hy-
potheses in a trie structure.
We are not aware of other work proposing
“lattice rules” as a native format for express-
ing translational equivalences. Work like (de
Gispert et al., 2010) rely on SCFG rules cre-
ated along the (Chiang, 2007) approach, while
work like (Casacuberta and Vidal, 2004) adopt
a pure Finite State Transducer paradigm (thus
without explicit SCFG-like rules).
</bodyText>
<sectionHeader confidence="0.998608" genericHeader="conclusions">
8 Conclusion
</sectionHeader>
<bodyText confidence="0.999791">
This work proposes to use a lattice-rule repre-
sentation of the translation search space with
two main goals:
</bodyText>
<listItem confidence="0.99933275">
• Easily represent the translation ambigui-
ties that arise either due to lack of context
or imperfect knowledge.
• Have a method for optimizing the repre-
</listItem>
<bodyText confidence="0.996859384615385">
sentation of a search space to make this
search more efficient.
We demonstrate that many types of am-
biguities arising when extracting translation
rules can easily be expressed in this frame-
work, and that making these ambiguities ex-
plicit and solvable at compile time through
lattice-rules leads to improvement in transla-
tion quality.
We also demonstrate that making a direct
use of the lattice-rules representation allows a
decoder to perform better than if working on
the expanded set of corresponding “flat rules”.
And we propose an algorithm for computing
more efficient representations of a translation
candidate set.
We believe that the the link between the
representation of a candidate set and the de-
coding efficiency is an interesting issue and
we intend to explore further the possibilities
of optimizing representations both in the con-
texts we considered in this paper and in others
such as Phrase-Based Machine Translation.
The code of the decoder we implemented for
this paper is to be released under a GPL li-
cense6.
</bodyText>
<sectionHeader confidence="0.997282" genericHeader="acknowledgments">
Acknowledgements
</sectionHeader>
<bodyText confidence="0.99975225">
This work is supported by the Japanese Sci-
ence and Technology Agency. We want to
thank the anonymous reviewers for many very
useful comments.
</bodyText>
<sectionHeader confidence="0.999034" genericHeader="references">
References
</sectionHeader>
<reference confidence="0.898374">
Peter F Brown, Vincent J Della Pietra, Stephen
A Della Pietra, and Robert L Mercer. 1993. The
mathematics of statistical machine translation:
</reference>
<footnote confidence="0.885239">
6http://nlp.ist.i.kyoto-u.ac.jp/kyotoebmt/
</footnote>
<page confidence="0.994962">
586
</page>
<reference confidence="0.99949218018018">
Parameter estimation. Computational linguis-
tics, 19(2):263–311.
Francisco Casacuberta and Enrique Vidal. 2004.
Machine translation with inferred stochastic
finite-state transducers. Computational Linguis-
tics, 30(2):205–225.
David Chiang. 2007. Hierarchical phrase-
based translation. computational linguistics,
33(2):201–228.
Adrià de Gispert, Gonzalo Iglesias, Graeme
Blackwood, Eduardo R Banga, and William
Byrne. 2010. Hierarchical phrase-based transla-
tion with weighted finite-state transducers and
shallow-n grammars. Computational linguistics,
36(3):505–533.
Isao Goto, Ka Po Chow, Bin Lu, Eiichiro Sumita,
and Benjamin K Tsou. 2013. Overview of the
patent machine translation task at the ntcir-
10 workshop. In Proceedings of the 10th NT-
CIR Workshop Meeting on Evaluation of Infor-
mation Access Technologies: Information Re-
trieval, Question Answering and Cross-Lingual
Information Access, NTCIR-10.
Jonathan Graehl and Kevin Knight. 2004. Train-
ing tree transducers. In Proceedings of HLT-
NAACL.
Kenneth Heafield, Hieu Hoang, Philipp Koehn,
Tetsuo Kiso, and Marcello Federico. 2011.
Left language model state for syntactic ma-
chine translation. In Proceedings of the Inter-
national Workshop on Spoken Language Trans-
lation, pages 183–190, San Francisco, California,
USA, December.
Kenneth Heafield, Philipp Koehn, and Alon Lavie.
2012. Language model rest costs and space-
efficient storage. In Proceedings of the Joint
Conference on Empirical Methods in Natural
Language Processing and Computational Natu-
ral Language Learning, pages 1169–1178, Jeju
Island, Korea, July.
Kenneth Heafield, Philipp Koehn, and Alon Lavie.
2013. Grouping language model boundary
words to speed k-best extraction from hyper-
graphs. In Proceedings of the 2013 Conference
of the North American Chapter of the Asso-
ciation for Computational Linguistics: Human
Language Technologies, pages 958–968, Atlanta,
Georgia, USA, June.
Kenneth Heafield, Michael Kayser, and Christo-
pher D. Manning. 2014. Faster Phrase-Based
decoding by refining feature state. In Proceed-
ings of the Association for Computational Lin-
guistics, Baltimore, MD, USA, June.
Kenneth Heafield. 2011. KenLM: faster and
smaller language model queries. In Proceedings
of the EMNLP 2011 Sixth Workshop on Statis-
tical Machine Translation, pages 187–197, Edin-
burgh, Scotland, United Kingdom, July.
John Hopcroft. 1971. An n log n algorithm for
minimizing states in a finite automaton. Theory
of Machines and Computations, pages 189–196.
Liang Huang and David Chiang. 2005. Better k-
best parsing. In Proceedings of the Ninth In-
ternational Workshop on Parsing Technology,
pages 53–64. Association for Computational Lin-
guistics.
Kevin Knight and Yaser Al-Onaizan. 1998. Trans-
lation with finite-state devices. In Machine
translation and the information soup, pages 421–
437. Springer.
Philipp Koehn, Franz Josef Och, and Daniel
Marcu. 2003. Statistical phrase-based transla-
tion. In Proceedings of the 2003 Conference of
the North American Chapter of the Association
for Computational Linguistics on Human Lan-
guage Technology-Volume 1, pages 48–54. Asso-
ciation for Computational Linguistics.
Philipp Koehn, Hieu Hoang, Alexandra Birch,
Chris Callison-Burch, Marcello Federico, Nicola
Bertoldi, Brooke Cowan, Wade Shen, Christine
Moran, Richard Zens, et al. 2007. Moses: Open
source toolkit for statistical machine translation.
In Proceedings of the 45th Annual Meeting of the
ACL on Interactive Poster and Demonstration
Sessions, pages 177–180. Association for Com-
putational Linguistics.
Shankar Kumar, Yonggang Deng, and William
Byrne. 2006. A weighted finite state transducer
translation template model for statistical ma-
chine translation. Natural Language Engineer-
ing, 12(01):35–75.
Zhifei Li and Sanjeev Khudanpur. 2008. A scal-
able decoder for parsing-based machine transla-
tion with equivalent language model state main-
tenance. In Proceedings of the Second Workshop
on Syntax and Structure in Statistical Trans-
lation, pages 10–18. Association for Computa-
tional Linguistics.
Mehryar Mohri. 1997. Finite-state transducers in
language and speech processing. Computational
linguistics, 23(2):269–311.
Mehryar Mohri. 2004. Weighted finite-state
transducer algorithms. an overview. In For-
mal Languages and Applications, pages 551–563.
Springer.
John Richardson, Fabien Cromières, Toshiaki
Nakazawa, and Sadao Kurohashi. 2014. Ky-
otoebmt: An example-based dependency-to-
dependency translation framework. In Proceed-
ings of ACL (System Demonstration), Balti-
more, MD, USA, June.
</reference>
<page confidence="0.975923">
587
</page>
<reference confidence="0.992001857142857">
Alexander M Rush and Michael Collins. 2011.
Exact decoding of syntactic translation mod-
els through lagrangian relaxation. In Proceed-
ings of the 49th Annual Meeting of the Asso-
ciation for Computational Linguistics: Human
Language Technologies-Volume 1, pages 72–82.
Association for Computational Linguistics.
</reference>
<page confidence="0.99733">
588
</page>
</variant>
</algorithm>
<algorithm name="ParsHed" version="110505">
<variant no="0" confidence="0.757004">
<title confidence="0.999952">Translation Rules with Right-Hand Side Lattices</title>
<author confidence="0.978417">Fabien</author>
<affiliation confidence="0.996387">Japan Science and Technology</affiliation>
<address confidence="0.82252">Saitama</address>
<email confidence="0.994978">fabien@pa.jst.jp</email>
<abstract confidence="0.995649391304348">In Corpus-Based Machine Translation, the search space of the translation candidates for a given input sentence is often defined by a set of (cyclefree) context-free grammar rules. This happens naturally in Syntax-Based Machine Translation and Hierarchical Phrase-Based Machine Translation (where the representation will be the set of the target-side half of the synchronous rules used to parse the input sentence). But it is also possible to describe Phrase-Based Machine Translation in this framework. We propose a natural extension to this representation by using lattice-rules that allow to easily encode an exponential number of variations of each rules. We also demonstrate how the representation of the search space has an impact on decoding efficiency, and how it is possible to optimize this representation.</abstract>
</variant>
</algorithm>
<algorithm name="ParsCit" version="110505">
<citationList>
<citation valid="true">
<authors>
<author>Peter F Brown</author>
<author>Vincent J Della Pietra</author>
<author>Stephen A Della Pietra</author>
<author>Robert L Mercer</author>
</authors>
<title>The mathematics of statistical machine translation: Parameter estimation.</title>
<date>1993</date>
<journal>Computational linguistics,</journal>
<volume>19</volume>
<issue>2</issue>
<contexts>
<context position="32007" citStr="Brown et al., 1993" startWordPosition="5497" endWordPosition="5500">een further improved in the previously mentioned (Heafield et al., 2013). (Rush and Collins, 2011) recently proposed an algorithm promising to find the optimal solution, but that is rather slow in practice. Weighted Finite State Machines have seen a variety of use in NLP (Mohri, 1997). More specifically, some other previous work on Machine Translation have used lattices (or more generally Weighted Finite State Machines). In the context of Corpus-Based Machine Translation, (Knight and Al-Onaizan, 1998) was already proposing to use Weighted Transducers to decode the “IBM” models of translation (Brown et al., 1993). (Casacuberta and Vidal, 2004) and (Kumar et al., 2006) also propose to directly model the translation process with Finite State Transducers. (Graehl and Knight, 2004) propose to use Tree Transducers for modeling Syntactic Machine Translation. These approaches are however based on different paradigm, typically trying to directly learn a transducer rather than extracting SCFG-like rules. Closer to our context, (de Gispert et al., 2010) propose to use Finite-State Transducers in the context of Hierarchical Phrase Based Translation. Their method is to iteratively construct and minimize the full </context>
</contexts>
<marker>Brown, Pietra, Pietra, Mercer, 1993</marker>
<rawString>Peter F Brown, Vincent J Della Pietra, Stephen A Della Pietra, and Robert L Mercer. 1993. The mathematics of statistical machine translation: Parameter estimation. Computational linguistics, 19(2):263–311.</rawString>
</citation>
<citation valid="true">
<authors>
<author>Francisco Casacuberta</author>
<author>Enrique Vidal</author>
</authors>
<title>Machine translation with inferred stochastic finite-state transducers.</title>
<date>2004</date>
<journal>Computational Linguistics,</journal>
<volume>30</volume>
<issue>2</issue>
<contexts>
<context position="32038" citStr="Casacuberta and Vidal, 2004" startWordPosition="5501" endWordPosition="5505">n the previously mentioned (Heafield et al., 2013). (Rush and Collins, 2011) recently proposed an algorithm promising to find the optimal solution, but that is rather slow in practice. Weighted Finite State Machines have seen a variety of use in NLP (Mohri, 1997). More specifically, some other previous work on Machine Translation have used lattices (or more generally Weighted Finite State Machines). In the context of Corpus-Based Machine Translation, (Knight and Al-Onaizan, 1998) was already proposing to use Weighted Transducers to decode the “IBM” models of translation (Brown et al., 1993). (Casacuberta and Vidal, 2004) and (Kumar et al., 2006) also propose to directly model the translation process with Finite State Transducers. (Graehl and Knight, 2004) propose to use Tree Transducers for modeling Syntactic Machine Translation. These approaches are however based on different paradigm, typically trying to directly learn a transducer rather than extracting SCFG-like rules. Closer to our context, (de Gispert et al., 2010) propose to use Finite-State Transducers in the context of Hierarchical Phrase Based Translation. Their method is to iteratively construct and minimize the full “top-level lattice” representin</context>
<context position="34435" citStr="Casacuberta and Vidal, 2004" startWordPosition="5885" endWordPosition="5888">licitly, we do not need to prune vertices (we only prune language model states). And each edge of each lattice rule is crossed only once during our decoding. Very recently, (Heafield et al., 2014) also considered using the redundancy of translation hypotheses to optimize phrase-based stack decoding. To do so, they group the partial hypotheses in a trie structure. We are not aware of other work proposing “lattice rules” as a native format for expressing translational equivalences. Work like (de Gispert et al., 2010) rely on SCFG rules created along the (Chiang, 2007) approach, while work like (Casacuberta and Vidal, 2004) adopt a pure Finite State Transducer paradigm (thus without explicit SCFG-like rules). 8 Conclusion This work proposes to use a lattice-rule representation of the translation search space with two main goals: • Easily represent the translation ambiguities that arise either due to lack of context or imperfect knowledge. • Have a method for optimizing the representation of a search space to make this search more efficient. We demonstrate that many types of ambiguities arising when extracting translation rules can easily be expressed in this framework, and that making these ambiguities explicit </context>
</contexts>
<marker>Casacuberta, Vidal, 2004</marker>
<rawString>Francisco Casacuberta and Enrique Vidal. 2004. Machine translation with inferred stochastic finite-state transducers. Computational Linguistics, 30(2):205–225.</rawString>
</citation>
<citation valid="true">
<authors>
<author>David Chiang</author>
</authors>
<title>Hierarchical phrasebased translation. computational linguistics,</title>
<date>2007</date>
<pages>33--2</pages>
<contexts>
<context position="1619" citStr="Chiang, 2007" startWordPosition="252" endWordPosition="253">h to modern Machine Translation is to decompose the translation problem into a modeling step and a search step. The modeling step will consist in defining implicitly a set of possible translations T for each input sentence. Each translation in T being associated with a real-valued model score. The search step will then consist in finding the translation in T with the highest model score. The search is non-trivial because it is usually impossible to enumerate all members of T (its cardinality being typically exponentially dependent on the size of the sentence to be translated). Since at least (Chiang, 2007), a common way of representing T has been through a Sadao Kurohashi Graduate School of Informatics Kyoto University Kyoto 606-8501 kuro@i.kyoto-u.ac.jp cycle-free context-free grammar. In such a grammar, T is represented as a set of context-free rules such as can be seen on figure 1. These rules themselves can be generated by the modeling step through the use of phrase tables, synchronous parsing, tree-tostring rules, etc. If the model score of each translation is taken to be the sum of rule scores independently given to each rule, the search for the optimal translation is easy with some class</context>
<context position="23846" citStr="Chiang, 2007" startWordPosition="4128" endWordPosition="4129">hese states with the ones in best[v1] to obtain states corresponding to partial translations ending at v2 (line 6). Results of the calls decode(X) are memoized, as the same non-terminal is likely to appear in several edges of a RHS and in several RHS. 2http://kheafield.com/code/kenlm/ Lines 5 and 6 are the “cube-pruning-like” part of the algorithm. The function pruneK returns the K best combinations of states in best[v] and decode(RHS(X)), where best means “whose sum of partial score is highest”. It can be implemented efficiently through the algorithms proposed in (Huang and Chiang, 2005) or (Chiang, 2007). The L st operation on lines 6 and 10 has the following meaning: L is a list of scored language model state and st is a scored language model state. L st means that, if L already contains a state st2 with same left and right state as st, L is updated to contain only the scored state with the maximum score. If L do not contain a state similar to st, st in simply inserted into L. This is the “hypothesis recombination” part of the algorithm. The function truncK′ truncate the list best[v] to its K′ highest-scored elements. The final result is obtained by calling decode(X0), where X0 is the “top-l</context>
<context position="31338" citStr="Chiang, 2007" startWordPosition="5393" endWordPosition="5394">here “for scale”, as our MT system uses a quite different pipeline. Sin particular, we factored out the representation optimisation time, which is reasonable if we are in the setting of a parameter tuning step in which the same sentences are translated repeatedly 7 Related work Searching for the most optimal translation in an implicitly defined set has been the focus of a lot of research in Machine Translation and it would be difficult to cover all of it. Among the most influential approaches, (Koehn et al., 2003) was using a form of stack based decoding for Phrase-Based Machine Translation. (Chiang, 2007) introduced the cube-pruning approach, which has been further improved in the previously mentioned (Heafield et al., 2013). (Rush and Collins, 2011) recently proposed an algorithm promising to find the optimal solution, but that is rather slow in practice. Weighted Finite State Machines have seen a variety of use in NLP (Mohri, 1997). More specifically, some other previous work on Machine Translation have used lattices (or more generally Weighted Finite State Machines). In the context of Corpus-Based Machine Translation, (Knight and Al-Onaizan, 1998) was already proposing to use Weighted Trans</context>
<context position="34379" citStr="Chiang, 2007" startWordPosition="5879" endWordPosition="5880">e do not construct the global lattice explicitly, we do not need to prune vertices (we only prune language model states). And each edge of each lattice rule is crossed only once during our decoding. Very recently, (Heafield et al., 2014) also considered using the redundancy of translation hypotheses to optimize phrase-based stack decoding. To do so, they group the partial hypotheses in a trie structure. We are not aware of other work proposing “lattice rules” as a native format for expressing translational equivalences. Work like (de Gispert et al., 2010) rely on SCFG rules created along the (Chiang, 2007) approach, while work like (Casacuberta and Vidal, 2004) adopt a pure Finite State Transducer paradigm (thus without explicit SCFG-like rules). 8 Conclusion This work proposes to use a lattice-rule representation of the translation search space with two main goals: • Easily represent the translation ambiguities that arise either due to lack of context or imperfect knowledge. • Have a method for optimizing the representation of a search space to make this search more efficient. We demonstrate that many types of ambiguities arising when extracting translation rules can easily be expressed in thi</context>
</contexts>
<marker>Chiang, 2007</marker>
<rawString>David Chiang. 2007. Hierarchical phrasebased translation. computational linguistics, 33(2):201–228.</rawString>
</citation>
<citation valid="true">
<authors>
<author>Adrià de Gispert</author>
<author>Gonzalo Iglesias</author>
<author>Graeme Blackwood</author>
<author>Eduardo R Banga</author>
<author>William Byrne</author>
</authors>
<title>Hierarchical phrase-based translation with weighted finite-state transducers and shallow-n grammars.</title>
<date>2010</date>
<journal>Computational linguistics,</journal>
<pages>36--3</pages>
<marker>de Gispert, Iglesias, Blackwood, Banga, Byrne, 2010</marker>
<rawString>Adrià de Gispert, Gonzalo Iglesias, Graeme Blackwood, Eduardo R Banga, and William Byrne. 2010. Hierarchical phrase-based translation with weighted finite-state transducers and shallow-n grammars. Computational linguistics, 36(3):505–533.</rawString>
</citation>
<citation valid="true">
<authors>
<author>Isao Goto</author>
<author>Ka Po Chow</author>
<author>Bin Lu</author>
<author>Eiichiro Sumita</author>
<author>Benjamin K Tsou</author>
</authors>
<title>Overview of the patent machine translation task at the ntcir10 workshop.</title>
<date>2013</date>
<booktitle>In Proceedings of the 10th NTCIR Workshop Meeting on Evaluation of Information Access Technologies: Information Retrieval, Question Answering and Cross-Lingual Information Access, NTCIR-10.</booktitle>
<contexts>
<context position="25143" citStr="Goto et al., 2013" startWordPosition="4355" endWordPosition="4358">of the form (BOS, EOS, s), with s being the optimal score. The search procedure of algorithm 3 could be described as “breadth-first”, since we systematically visit each edge of the lattice. An alternative would be to use a “best-first” search with an A*-like procedure. We have tried this, but either because of optimisation issues or heuristics of insufficient qualities, we did not obtain better results than with the algorithm we describe here. 6 Evaluation We now describe a set of experiments aimed at evaluating our approach. We use the Japanese-English data from the NTCIR-10 Patent MT task3 (Goto et al., 2013). The training data contains 3 millions parallel sentences for Japanese-English. 6.1 Effect of Lattice Representation and Optimisation We first evaluate the impact of the lattice representation on the performances of our decoding algorithm. This will allow us to measure 3http://ntcir.nii.ac.jp/PatentMT-2/ 583 Data: Lattice RHS G average. This leads to a 2-fold speed improveResult: Sorted list of best states ment in the decoding step, as well as a large 1 best[vE] = {(.,.,0.0)}; reduction of memory usage. 2 for vertex v E G in topological order do 3 for 4 edge e : v —* v2 E outgoing(v) do if la</context>
</contexts>
<marker>Goto, Chow, Lu, Sumita, Tsou, 2013</marker>
<rawString>Isao Goto, Ka Po Chow, Bin Lu, Eiichiro Sumita, and Benjamin K Tsou. 2013. Overview of the patent machine translation task at the ntcir10 workshop. In Proceedings of the 10th NTCIR Workshop Meeting on Evaluation of Information Access Technologies: Information Retrieval, Question Answering and Cross-Lingual Information Access, NTCIR-10.</rawString>
</citation>
<citation valid="true">
<authors>
<author>Jonathan Graehl</author>
<author>Kevin Knight</author>
</authors>
<title>Training tree transducers.</title>
<date>2004</date>
<booktitle>In Proceedings of HLTNAACL.</booktitle>
<contexts>
<context position="32175" citStr="Graehl and Knight, 2004" startWordPosition="5524" endWordPosition="5527">ution, but that is rather slow in practice. Weighted Finite State Machines have seen a variety of use in NLP (Mohri, 1997). More specifically, some other previous work on Machine Translation have used lattices (or more generally Weighted Finite State Machines). In the context of Corpus-Based Machine Translation, (Knight and Al-Onaizan, 1998) was already proposing to use Weighted Transducers to decode the “IBM” models of translation (Brown et al., 1993). (Casacuberta and Vidal, 2004) and (Kumar et al., 2006) also propose to directly model the translation process with Finite State Transducers. (Graehl and Knight, 2004) propose to use Tree Transducers for modeling Syntactic Machine Translation. These approaches are however based on different paradigm, typically trying to directly learn a transducer rather than extracting SCFG-like rules. Closer to our context, (de Gispert et al., 2010) propose to use Finite-State Transducers in the context of Hierarchical Phrase Based Translation. Their method is to iteratively construct and minimize the full “top-level lattice” representing the whole set of translations bottom-up. It is an approach more focused on the Finite State Machine aspect than our, 585 System K-decod</context>
</contexts>
<marker>Graehl, Knight, 2004</marker>
<rawString>Jonathan Graehl and Kevin Knight. 2004. Training tree transducers. In Proceedings of HLTNAACL.</rawString>
</citation>
<citation valid="true">
<authors>
<author>Kenneth Heafield</author>
<author>Hieu Hoang</author>
<author>Philipp Koehn</author>
<author>Tetsuo Kiso</author>
<author>Marcello Federico</author>
</authors>
<title>Left language model state for syntactic machine translation.</title>
<date>2011</date>
<booktitle>In Proceedings of the International Workshop on Spoken Language Translation,</booktitle>
<pages>183--190</pages>
<location>San Francisco, California, USA,</location>
<contexts>
<context position="21432" citStr="Heafield et al., 2011" startWordPosition="3698" endWordPosition="3701">edge is labeled by a nonterminal X, we first traverse the corresponding lattice RHS(X) following the same process. Such a partial translation can be reduced compactly to a scored language model state (l, r, s), where l represent the first n wordsi of the partial translation, r its last n words and s its partial score. It is clear that if two partial translations have the same l and r parts but different score, we can discard the one with the lowest score, as it cannot be a part of the optimal translation. Further, using the state reduction techniques described in (Li and Khudanpur, 2008) and (Heafield et al., 2011), we can often reduce the size of l and r to less than n, allowing further opportunities for discarding sub-optimal In being the order of the language mode 582 partial translations. For better behavior during the cube-pruning step of the algorithm (see later), the partial score s of a partial translation includes rest-costs estimates (Heafield et al., 2012). We define the concatenation operation on scored language model states to be: (l1, r1, s1) ® (l2, r2, s2) = (l3, r3, s3), where s3 = s1 + s2 + Alm(r1,l2), with lm(r1,l2) being the language model probability of l2 given r1 with rest-costs ad</context>
</contexts>
<marker>Heafield, Hoang, Koehn, Kiso, Federico, 2011</marker>
<rawString>Kenneth Heafield, Hieu Hoang, Philipp Koehn, Tetsuo Kiso, and Marcello Federico. 2011. Left language model state for syntactic machine translation. In Proceedings of the International Workshop on Spoken Language Translation, pages 183–190, San Francisco, California, USA, December.</rawString>
</citation>
<citation valid="true">
<authors>
<author>Kenneth Heafield</author>
<author>Philipp Koehn</author>
<author>Alon Lavie</author>
</authors>
<title>Language model rest costs and spaceefficient storage.</title>
<date>2012</date>
<booktitle>In Proceedings of the Joint Conference on Empirical Methods in Natural Language Processing and Computational Natural Language Learning,</booktitle>
<pages>1169--1178</pages>
<location>Jeju Island, Korea,</location>
<contexts>
<context position="21791" citStr="Heafield et al., 2012" startWordPosition="3759" endWordPosition="3762">tions have the same l and r parts but different score, we can discard the one with the lowest score, as it cannot be a part of the optimal translation. Further, using the state reduction techniques described in (Li and Khudanpur, 2008) and (Heafield et al., 2011), we can often reduce the size of l and r to less than n, allowing further opportunities for discarding sub-optimal In being the order of the language mode 582 partial translations. For better behavior during the cube-pruning step of the algorithm (see later), the partial score s of a partial translation includes rest-costs estimates (Heafield et al., 2012). We define the concatenation operation on scored language model states to be: (l1, r1, s1) ® (l2, r2, s2) = (l3, r3, s3), where s3 = s1 + s2 + Alm(r1,l2), with lm(r1,l2) being the language model probability of l2 given r1 with rest-costs adjustments. r3 and l3 are the resulting minimized states. Similarly, if an edge e is labeled by a word, we define the concatenation of a scored state with an edge to be (l1, r1, s1) ® e = (l2, r2, s2) where s2 = s1 + score(e) + Alm(word(e)|r1). Conveniently for us, the KenLM2 opensource library (Heafield, 2011) provides functionalities for easily computing s</context>
</contexts>
<marker>Heafield, Koehn, Lavie, 2012</marker>
<rawString>Kenneth Heafield, Philipp Koehn, and Alon Lavie. 2012. Language model rest costs and spaceefficient storage. In Proceedings of the Joint Conference on Empirical Methods in Natural Language Processing and Computational Natural Language Learning, pages 1169–1178, Jeju Island, Korea, July.</rawString>
</citation>
<citation valid="true">
<authors>
<author>Kenneth Heafield</author>
<author>Philipp Koehn</author>
<author>Alon Lavie</author>
</authors>
<title>Grouping language model boundary words to speed k-best extraction from hypergraphs.</title>
<date>2013</date>
<booktitle>In Proceedings of the 2013 Conference of the North American Chapter of the Association for Computational Linguistics: Human Language Technologies,</booktitle>
<pages>958--968</pages>
<location>Atlanta, Georgia, USA,</location>
<contexts>
<context position="27443" citStr="Heafield et al., 2013" startWordPosition="4740" endWordPosition="4743">formances in term of average search time or peak memory used are directly related to the number of vertices and edges in the representation. We can also see that our representation optimisation step is quite efficient, since it is able to divide by two the number of vertices in the representation, on 6.2 Decoding performances In order to further evaluate the merit of our approach, we now compare the results obtained by using our decoder with lattice-rules with using a state-of-the-art decoder on the set of flat expanded rules equivalent to these lattice rules. We use the decoder described in (Heafield et al., 2013), which is available under an opensource license (henceforth called K-decoder). In this experience, we expanded the lattice rules generated by our MT system for 1800 sentences into files having the required format for the K-decoder. This basically mean we computed an equivalent of the expanded representation of section 6.1. This process generated files ranging in size from 20MB to 17GB depending on the sentence. We then ran the K-decoder on these files and compared the results with our own. We used a beam-width of 10000 for the K-decoder. Experiments were run in single thread mode. Partly to o</context>
<context position="29062" citStr="Heafield et al., 2013" startWordPosition="5021" endWordPosition="5024">-rule decoder providing slightly better model score. It is interesting to note that, on “fairground” comparison, that is if our decoder do not have the benefit of a more compact latticerule representation, it actually perform quite worse as we can see by comparing with the third column of table 1 (at least in term of decoding time and memory usage, while it would still have a very slight edge in term of model score with the selected settings). On the other hand, the K-decoder is a rather strong baseline, shown to perform several times faster than a previous state-of-the-art implementation in (Heafield et al., 2013). It is well opti4http://kheafield.com/code/search/ 584 Representation: Original Optimized Expanded Peak memory used 39 GB 16GB 85GB Average search time 6.13s 3.31s 9.95s #vertices (avg/max) 65K (1300K) 32K (446K) 263K (5421K) #edges (avg/max) 92K (1512K) 83K (541K) 263K (5421K) Table 1: Impact of the lattice representation on performances. System JA–EN Lattice 29.43 No-variations 28.91 Moses (for scale) 28.86 Table 2: Impact on BLEU of using flexible lattice rules. mized and makes use of advanced techniques with the language model (as the one described in (Heafield et al., 2013)) for which we</context>
<context position="31460" citStr="Heafield et al., 2013" startWordPosition="5408" endWordPosition="5411">ation optimisation time, which is reasonable if we are in the setting of a parameter tuning step in which the same sentences are translated repeatedly 7 Related work Searching for the most optimal translation in an implicitly defined set has been the focus of a lot of research in Machine Translation and it would be difficult to cover all of it. Among the most influential approaches, (Koehn et al., 2003) was using a form of stack based decoding for Phrase-Based Machine Translation. (Chiang, 2007) introduced the cube-pruning approach, which has been further improved in the previously mentioned (Heafield et al., 2013). (Rush and Collins, 2011) recently proposed an algorithm promising to find the optimal solution, but that is rather slow in practice. Weighted Finite State Machines have seen a variety of use in NLP (Mohri, 1997). More specifically, some other previous work on Machine Translation have used lattices (or more generally Weighted Finite State Machines). In the context of Corpus-Based Machine Translation, (Knight and Al-Onaizan, 1998) was already proposing to use Weighted Transducers to decode the “IBM” models of translation (Brown et al., 1993). (Casacuberta and Vidal, 2004) and (Kumar et al., 20</context>
</contexts>
<marker>Heafield, Koehn, Lavie, 2013</marker>
<rawString>Kenneth Heafield, Philipp Koehn, and Alon Lavie. 2013. Grouping language model boundary words to speed k-best extraction from hypergraphs. In Proceedings of the 2013 Conference of the North American Chapter of the Association for Computational Linguistics: Human Language Technologies, pages 958–968, Atlanta, Georgia, USA, June.</rawString>
</citation>
<citation valid="true">
<authors>
<author>Kenneth Heafield</author>
<author>Michael Kayser</author>
<author>Christopher D Manning</author>
</authors>
<title>Faster Phrase-Based decoding by refining feature state.</title>
<date>2014</date>
<booktitle>In Proceedings of the Association for Computational Linguistics,</booktitle>
<location>Baltimore, MD, USA,</location>
<contexts>
<context position="34003" citStr="Heafield et al., 2014" startWordPosition="5814" endWordPosition="5817">e possibilities for vertex merging. On the other hand, for large grammars, the “top-level lattice” will be huge, creating the need to prune vertices during the construction. Furthermore, the composition of the “top-level lattice” with a language model will imply redundant computations (as lower-level lattices will potentially be expanded several times in the top-level lattice). As we do not construct the global lattice explicitly, we do not need to prune vertices (we only prune language model states). And each edge of each lattice rule is crossed only once during our decoding. Very recently, (Heafield et al., 2014) also considered using the redundancy of translation hypotheses to optimize phrase-based stack decoding. To do so, they group the partial hypotheses in a trie structure. We are not aware of other work proposing “lattice rules” as a native format for expressing translational equivalences. Work like (de Gispert et al., 2010) rely on SCFG rules created along the (Chiang, 2007) approach, while work like (Casacuberta and Vidal, 2004) adopt a pure Finite State Transducer paradigm (thus without explicit SCFG-like rules). 8 Conclusion This work proposes to use a lattice-rule representation of the tran</context>
</contexts>
<marker>Heafield, Kayser, Manning, 2014</marker>
<rawString>Kenneth Heafield, Michael Kayser, and Christopher D. Manning. 2014. Faster Phrase-Based decoding by refining feature state. In Proceedings of the Association for Computational Linguistics, Baltimore, MD, USA, June.</rawString>
</citation>
<citation valid="true">
<authors>
<author>Kenneth Heafield</author>
</authors>
<title>KenLM: faster and smaller language model queries.</title>
<date>2011</date>
<booktitle>In Proceedings of the EMNLP 2011 Sixth Workshop on Statistical Machine Translation,</booktitle>
<pages>187--197</pages>
<location>Edinburgh, Scotland, United Kingdom,</location>
<contexts>
<context position="22343" citStr="Heafield, 2011" startWordPosition="3865" endWordPosition="3866">anslation includes rest-costs estimates (Heafield et al., 2012). We define the concatenation operation on scored language model states to be: (l1, r1, s1) ® (l2, r2, s2) = (l3, r3, s3), where s3 = s1 + s2 + Alm(r1,l2), with lm(r1,l2) being the language model probability of l2 given r1 with rest-costs adjustments. r3 and l3 are the resulting minimized states. Similarly, if an edge e is labeled by a word, we define the concatenation of a scored state with an edge to be (l1, r1, s1) ® e = (l2, r2, s2) where s2 = s1 + score(e) + Alm(word(e)|r1). Conveniently for us, the KenLM2 opensource library (Heafield, 2011) provides functionalities for easily computing such concatenation operations. 5.3 Algorithm Having defined these operations, we can now more easily describe algorithm 3. Each vertex v has a list best[v] of the scored states of the best partial translations found to be ending at v. On line 1, we initialize best[vS] with (., ., 0), where “.” represent an empty language model state. We then traverse the vertices of the lattice in topological order. For each edge e : v1 —* v2, we compute new scored states for best[v2] as follow: • if e is labeled by a word or an epsilon, we create a state st2 = st</context>
</contexts>
<marker>Heafield, 2011</marker>
<rawString>Kenneth Heafield. 2011. KenLM: faster and smaller language model queries. In Proceedings of the EMNLP 2011 Sixth Workshop on Statistical Machine Translation, pages 187–197, Edinburgh, Scotland, United Kingdom, July.</rawString>
</citation>
<citation valid="true">
<authors>
<author>John Hopcroft</author>
</authors>
<title>An n log n algorithm for minimizing states in a finite automaton. Theory of Machines and Computations,</title>
<date>1971</date>
<pages>189--196</pages>
<contexts>
<context position="13893" citStr="Hopcroft, 1971" startWordPosition="2385" endWordPosition="2386">on RT that will make the search step faster. This is especially interesting if one is going to search the same T several times, as is often done when one is fine-tuning the parameters of a model, as this representation optimisation needs only be done once. The optimisation we propose is a natural fit to our framework of lattice rules. As lattice are a special case of Finite-State Automata (FSA), it is easy to adapt existing algorithms for FSA minimization. We describe a procedure in algorithm 1, which is essentially a simplification and adaptation to our case of the more general algorithm of (Hopcroft, 1971) for FSA. The central parts of the algorithm are the two sub-procedures backward vertex merging and forward vertex merging. An example of the result of an optimisation is given on figure 3. 580 Data: Representation RT Result: Optimized Representation 1 for non-terminal X E RT do 2 Apply backward vertex merging to RH5(X); 3 Apply forward vertex merging to RH5(X); 4 end Algorithm 1: Representation optimisation Data: Lattice RHS G Result: Optimized Lattice RHS 1 P ; //processed vertices; 2 C {vS} //candidate set ; 3 while |C |&gt; 0 do for v E C do Eliminate duplicate edges in incoming(v); Mark edge</context>
</contexts>
<marker>Hopcroft, 1971</marker>
<rawString>John Hopcroft. 1971. An n log n algorithm for minimizing states in a finite automaton. Theory of Machines and Computations, pages 189–196.</rawString>
</citation>
<citation valid="true">
<authors>
<author>Liang Huang</author>
<author>David Chiang</author>
</authors>
<title>Better kbest parsing.</title>
<date>2005</date>
<booktitle>In Proceedings of the Ninth International Workshop on Parsing Technology,</booktitle>
<pages>53--64</pages>
<publisher>Association</publisher>
<institution>for Computational Linguistics.</institution>
<contexts>
<context position="19996" citStr="Huang and Chiang, 2005" startWordPosition="3437" endWordPosition="3440">tion candidate sets represented as a set of lattice-rules. For the most part, this algorithm re-use many of the techniques previously developed for decoding translation search spaces, but adapt them to our setting. 5.1 Overview The outline of the decoding algorithm is described by algorithm 3. For simplicity, the description only compute the optimal model score over the translations in the candidate set. It is however trivial to adapt the description to keep track of which sentence correspond to this optimal score and output it instead of the score. Likewise, using the technique described in (Huang and Chiang, 2005), one can easily output k-best lists of translations. For simplicity again, we consider that a n-gram language model score is the only stateful nonlocal feature used for computing the model score, although in a tree-to-tree setting, other features (local in a tree representation but not in a string representation) could be used. The model score of a translation t has therefore the shape: score(t) � A · lm(t) + ∑ score(e) e where A is the weight of the language model, lm(t) is the language model log-probability of t and the sum is over all edges e crossed to obtain t. 5.2 Scored language model </context>
<context position="23828" citStr="Huang and Chiang, 2005" startWordPosition="4123" endWordPosition="4126">RHS(X). We can concatenate these states with the ones in best[v1] to obtain states corresponding to partial translations ending at v2 (line 6). Results of the calls decode(X) are memoized, as the same non-terminal is likely to appear in several edges of a RHS and in several RHS. 2http://kheafield.com/code/kenlm/ Lines 5 and 6 are the “cube-pruning-like” part of the algorithm. The function pruneK returns the K best combinations of states in best[v] and decode(RHS(X)), where best means “whose sum of partial score is highest”. It can be implemented efficiently through the algorithms proposed in (Huang and Chiang, 2005) or (Chiang, 2007). The L st operation on lines 6 and 10 has the following meaning: L is a list of scored language model state and st is a scored language model state. L st means that, if L already contains a state st2 with same left and right state as st, L is updated to contain only the scored state with the maximum score. If L do not contain a state similar to st, st in simply inserted into L. This is the “hypothesis recombination” part of the algorithm. The function truncK′ truncate the list best[v] to its K′ highest-scored elements. The final result is obtained by calling decode(X0), wher</context>
</contexts>
<marker>Huang, Chiang, 2005</marker>
<rawString>Liang Huang and David Chiang. 2005. Better kbest parsing. In Proceedings of the Ninth International Workshop on Parsing Technology, pages 53–64. Association for Computational Linguistics.</rawString>
</citation>
<citation valid="true">
<authors>
<author>Kevin Knight</author>
<author>Yaser Al-Onaizan</author>
</authors>
<title>Translation with finite-state devices.</title>
<date>1998</date>
<booktitle>In Machine translation and the information soup,</booktitle>
<pages>421--437</pages>
<publisher>Springer.</publisher>
<contexts>
<context position="31894" citStr="Knight and Al-Onaizan, 1998" startWordPosition="5478" endWordPosition="5481">tack based decoding for Phrase-Based Machine Translation. (Chiang, 2007) introduced the cube-pruning approach, which has been further improved in the previously mentioned (Heafield et al., 2013). (Rush and Collins, 2011) recently proposed an algorithm promising to find the optimal solution, but that is rather slow in practice. Weighted Finite State Machines have seen a variety of use in NLP (Mohri, 1997). More specifically, some other previous work on Machine Translation have used lattices (or more generally Weighted Finite State Machines). In the context of Corpus-Based Machine Translation, (Knight and Al-Onaizan, 1998) was already proposing to use Weighted Transducers to decode the “IBM” models of translation (Brown et al., 1993). (Casacuberta and Vidal, 2004) and (Kumar et al., 2006) also propose to directly model the translation process with Finite State Transducers. (Graehl and Knight, 2004) propose to use Tree Transducers for modeling Syntactic Machine Translation. These approaches are however based on different paradigm, typically trying to directly learn a transducer rather than extracting SCFG-like rules. Closer to our context, (de Gispert et al., 2010) propose to use Finite-State Transducers in the </context>
</contexts>
<marker>Knight, Al-Onaizan, 1998</marker>
<rawString>Kevin Knight and Yaser Al-Onaizan. 1998. Translation with finite-state devices. In Machine translation and the information soup, pages 421– 437. Springer.</rawString>
</citation>
<citation valid="true">
<authors>
<author>Philipp Koehn</author>
<author>Franz Josef Och</author>
<author>Daniel Marcu</author>
</authors>
<title>Statistical phrase-based translation.</title>
<date>2003</date>
<booktitle>In Proceedings of the 2003 Conference of the North American Chapter of the Association for Computational Linguistics on Human Language Technology-Volume 1,</booktitle>
<pages>48--54</pages>
<publisher>Association</publisher>
<institution>for Computational Linguistics.</institution>
<contexts>
<context position="31244" citStr="Koehn et al., 2003" startWordPosition="5376" endWordPosition="5379">Note that the Moses score (Koehn et al., 2007), taken from the official results of NTCIR-10 is only here “for scale”, as our MT system uses a quite different pipeline. Sin particular, we factored out the representation optimisation time, which is reasonable if we are in the setting of a parameter tuning step in which the same sentences are translated repeatedly 7 Related work Searching for the most optimal translation in an implicitly defined set has been the focus of a lot of research in Machine Translation and it would be difficult to cover all of it. Among the most influential approaches, (Koehn et al., 2003) was using a form of stack based decoding for Phrase-Based Machine Translation. (Chiang, 2007) introduced the cube-pruning approach, which has been further improved in the previously mentioned (Heafield et al., 2013). (Rush and Collins, 2011) recently proposed an algorithm promising to find the optimal solution, but that is rather slow in practice. Weighted Finite State Machines have seen a variety of use in NLP (Mohri, 1997). More specifically, some other previous work on Machine Translation have used lattices (or more generally Weighted Finite State Machines). In the context of Corpus-Based </context>
</contexts>
<marker>Koehn, Och, Marcu, 2003</marker>
<rawString>Philipp Koehn, Franz Josef Och, and Daniel Marcu. 2003. Statistical phrase-based translation. In Proceedings of the 2003 Conference of the North American Chapter of the Association for Computational Linguistics on Human Language Technology-Volume 1, pages 48–54. Association for Computational Linguistics.</rawString>
</citation>
<citation valid="true">
<authors>
<author>Philipp Koehn</author>
<author>Hieu Hoang</author>
<author>Alexandra Birch</author>
<author>Chris Callison-Burch</author>
<author>Marcello Federico</author>
<author>Nicola Bertoldi</author>
<author>Brooke Cowan</author>
<author>Wade Shen</author>
<author>Christine Moran</author>
<author>Richard Zens</author>
</authors>
<title>Moses: Open source toolkit for statistical machine translation.</title>
<date>2007</date>
<booktitle>In Proceedings of the 45th Annual Meeting of the ACL on Interactive Poster and Demonstration Sessions,</booktitle>
<pages>177--180</pages>
<publisher>Association</publisher>
<institution>for Computational Linguistics.</institution>
<contexts>
<context position="30671" citStr="Koehn et al., 2007" startWordPosition="5278" endWordPosition="5281">sed in section 3. That is, we consider rules for which null-aligned words are bypassable by epsilonedges, for which Non-terminal are allowed to take several alternative positions around the word that is thought to be their governor, and for which we consider alternative morphologies of a few words (“is/are”, “a/an”). We compare this approach with heuristically selecting only one possibility for each variation present in the lattice rule extracted from a single example. Results shown on figure 2 show that we do obtain a significant improvement in translation quality. Note that the Moses score (Koehn et al., 2007), taken from the official results of NTCIR-10 is only here “for scale”, as our MT system uses a quite different pipeline. Sin particular, we factored out the representation optimisation time, which is reasonable if we are in the setting of a parameter tuning step in which the same sentences are translated repeatedly 7 Related work Searching for the most optimal translation in an implicitly defined set has been the focus of a lot of research in Machine Translation and it would be difficult to cover all of it. Among the most influential approaches, (Koehn et al., 2003) was using a form of stack </context>
</contexts>
<marker>Koehn, Hoang, Birch, Callison-Burch, Federico, Bertoldi, Cowan, Shen, Moran, Zens, 2007</marker>
<rawString>Philipp Koehn, Hieu Hoang, Alexandra Birch, Chris Callison-Burch, Marcello Federico, Nicola Bertoldi, Brooke Cowan, Wade Shen, Christine Moran, Richard Zens, et al. 2007. Moses: Open source toolkit for statistical machine translation. In Proceedings of the 45th Annual Meeting of the ACL on Interactive Poster and Demonstration Sessions, pages 177–180. Association for Computational Linguistics.</rawString>
</citation>
<citation valid="true">
<authors>
<author>Shankar Kumar</author>
<author>Yonggang Deng</author>
<author>William Byrne</author>
</authors>
<title>A weighted finite state transducer translation template model for statistical machine translation.</title>
<date>2006</date>
<journal>Natural Language Engineering,</journal>
<volume>12</volume>
<issue>01</issue>
<contexts>
<context position="32063" citStr="Kumar et al., 2006" startWordPosition="5507" endWordPosition="5510">ld et al., 2013). (Rush and Collins, 2011) recently proposed an algorithm promising to find the optimal solution, but that is rather slow in practice. Weighted Finite State Machines have seen a variety of use in NLP (Mohri, 1997). More specifically, some other previous work on Machine Translation have used lattices (or more generally Weighted Finite State Machines). In the context of Corpus-Based Machine Translation, (Knight and Al-Onaizan, 1998) was already proposing to use Weighted Transducers to decode the “IBM” models of translation (Brown et al., 1993). (Casacuberta and Vidal, 2004) and (Kumar et al., 2006) also propose to directly model the translation process with Finite State Transducers. (Graehl and Knight, 2004) propose to use Tree Transducers for modeling Syntactic Machine Translation. These approaches are however based on different paradigm, typically trying to directly learn a transducer rather than extracting SCFG-like rules. Closer to our context, (de Gispert et al., 2010) propose to use Finite-State Transducers in the context of Hierarchical Phrase Based Translation. Their method is to iteratively construct and minimize the full “top-level lattice” representing the whole set of transl</context>
</contexts>
<marker>Kumar, Deng, Byrne, 2006</marker>
<rawString>Shankar Kumar, Yonggang Deng, and William Byrne. 2006. A weighted finite state transducer translation template model for statistical machine translation. Natural Language Engineering, 12(01):35–75.</rawString>
</citation>
<citation valid="true">
<authors>
<author>Zhifei Li</author>
<author>Sanjeev Khudanpur</author>
</authors>
<title>A scalable decoder for parsing-based machine translation with equivalent language model state maintenance.</title>
<date>2008</date>
<booktitle>In Proceedings of the Second Workshop on Syntax and Structure in Statistical Translation,</booktitle>
<pages>10--18</pages>
<publisher>Association</publisher>
<institution>for Computational Linguistics.</institution>
<contexts>
<context position="21404" citStr="Li and Khudanpur, 2008" startWordPosition="3693" endWordPosition="3696">a nonterminal until v. If an edge is labeled by a nonterminal X, we first traverse the corresponding lattice RHS(X) following the same process. Such a partial translation can be reduced compactly to a scored language model state (l, r, s), where l represent the first n wordsi of the partial translation, r its last n words and s its partial score. It is clear that if two partial translations have the same l and r parts but different score, we can discard the one with the lowest score, as it cannot be a part of the optimal translation. Further, using the state reduction techniques described in (Li and Khudanpur, 2008) and (Heafield et al., 2011), we can often reduce the size of l and r to less than n, allowing further opportunities for discarding sub-optimal In being the order of the language mode 582 partial translations. For better behavior during the cube-pruning step of the algorithm (see later), the partial score s of a partial translation includes rest-costs estimates (Heafield et al., 2012). We define the concatenation operation on scored language model states to be: (l1, r1, s1) ® (l2, r2, s2) = (l3, r3, s3), where s3 = s1 + s2 + Alm(r1,l2), with lm(r1,l2) being the language model probability of l2</context>
</contexts>
<marker>Li, Khudanpur, 2008</marker>
<rawString>Zhifei Li and Sanjeev Khudanpur. 2008. A scalable decoder for parsing-based machine translation with equivalent language model state maintenance. In Proceedings of the Second Workshop on Syntax and Structure in Statistical Translation, pages 10–18. Association for Computational Linguistics.</rawString>
</citation>
<citation valid="true">
<authors>
<author>Mehryar Mohri</author>
</authors>
<title>Finite-state transducers in language and speech processing.</title>
<date>1997</date>
<journal>Computational linguistics,</journal>
<volume>23</volume>
<issue>2</issue>
<contexts>
<context position="31673" citStr="Mohri, 1997" startWordPosition="5448" endWordPosition="5449">ly defined set has been the focus of a lot of research in Machine Translation and it would be difficult to cover all of it. Among the most influential approaches, (Koehn et al., 2003) was using a form of stack based decoding for Phrase-Based Machine Translation. (Chiang, 2007) introduced the cube-pruning approach, which has been further improved in the previously mentioned (Heafield et al., 2013). (Rush and Collins, 2011) recently proposed an algorithm promising to find the optimal solution, but that is rather slow in practice. Weighted Finite State Machines have seen a variety of use in NLP (Mohri, 1997). More specifically, some other previous work on Machine Translation have used lattices (or more generally Weighted Finite State Machines). In the context of Corpus-Based Machine Translation, (Knight and Al-Onaizan, 1998) was already proposing to use Weighted Transducers to decode the “IBM” models of translation (Brown et al., 1993). (Casacuberta and Vidal, 2004) and (Kumar et al., 2006) also propose to directly model the translation process with Finite State Transducers. (Graehl and Knight, 2004) propose to use Tree Transducers for modeling Syntactic Machine Translation. These approaches are </context>
</contexts>
<marker>Mohri, 1997</marker>
<rawString>Mehryar Mohri. 1997. Finite-state transducers in language and speech processing. Computational linguistics, 23(2):269–311.</rawString>
</citation>
<citation valid="true">
<authors>
<author>Mehryar Mohri</author>
</authors>
<title>Weighted finite-state transducer algorithms. an overview.</title>
<date>2004</date>
<booktitle>In Formal Languages and Applications,</booktitle>
<pages>551--563</pages>
<publisher>Springer.</publisher>
<contexts>
<context position="19010" citStr="Mohri, 2004" startWordPosition="3278" endWordPosition="3279">if both their associated word and their set of feature values are identical. This can sometimes prevent useful merging of states to take place. A solution to this could be to follow (de Gispert et al., 2010) and to discard all these features information during the decoding. The features values are then re-estimated afterward by aligning the translation and the input with a constrained version of the decoder. We prefer to actually keep track of the features values, even if it can reduce the efficiency of vertex merging. In that setting, we can also adapt the so-called Weight Pushing algorithm (Mohri, 2004) to a multivalues case in order to improve the “mergeability” of vertices. The results of section 6.1 shows that it is still possible to strongly reduce the size of the lattices even when keeping track of the features values. 5 Decoding algorithm In order to make an optimal use of these lattice-rule representations, we developed a decoding algorithm for translation candidate sets represented as a set of lattice-rules. For the most part, this algorithm re-use many of the techniques previously developed for decoding translation search spaces, but adapt them to our setting. 5.1 Overview The outli</context>
</contexts>
<marker>Mohri, 2004</marker>
<rawString>Mehryar Mohri. 2004. Weighted finite-state transducer algorithms. an overview. In Formal Languages and Applications, pages 551–563. Springer.</rawString>
</citation>
<citation valid="true">
<authors>
<author>John Richardson</author>
<author>Fabien Cromières</author>
<author>Toshiaki Nakazawa</author>
<author>Sadao Kurohashi</author>
</authors>
<title>Kyotoebmt: An example-based dependency-todependency translation framework.</title>
<date>2014</date>
<booktitle>In Proceedings of ACL (System Demonstration),</booktitle>
<location>Baltimore, MD, USA,</location>
<contexts>
<context position="8010" citStr="Richardson et al., 2014" startWordPosition="1367" endWordPosition="1370">alent to the ones expressed by the lattice. left-hand side of several rules. Using lattice expansion rules, however, it is not necessary, as one lattice RHS can encode an arbitrary number of flat rules (see for example the RHS of X0 in figure 3). Therefore, we set the constraint that there is only one lattice expansion rule for each left-hand non-terminal. And we will note unambiguously RHS(X) the lattice that is the right hand side of this rule. 3 Motivation 3.1 Setting This work was developed mainly in the context of a syntactic-dependency-based tree-to-tree translation system described in (Richardson et al., 2014). Although it is a tree-to-tree system, we simplify the decoding step by “flattening” the target-side tree translation rules into string expansion rules (keeping track of the dependency structure in state features). Thus our setting is actually quite similar to that of many tree-to-string and string-to-string systems. Aiming at simplicity and generality, we will set aside the question of target-side syntactic information and only describe our algorithms in a “tree-to-string” setting. We will also consider a n-gram language model score as our only stateful non-local feature. However, this tree-</context>
</contexts>
<marker>Richardson, Cromières, Nakazawa, Kurohashi, 2014</marker>
<rawString>John Richardson, Fabien Cromières, Toshiaki Nakazawa, and Sadao Kurohashi. 2014. Kyotoebmt: An example-based dependency-todependency translation framework. In Proceedings of ACL (System Demonstration), Baltimore, MD, USA, June.</rawString>
</citation>
<citation valid="true">
<authors>
<author>Alexander M Rush</author>
<author>Michael Collins</author>
</authors>
<title>Exact decoding of syntactic translation models through lagrangian relaxation.</title>
<date>2011</date>
<booktitle>In Proceedings of the 49th Annual Meeting of the Association for Computational Linguistics: Human Language Technologies-Volume 1,</booktitle>
<pages>72--82</pages>
<publisher>Association</publisher>
<institution>for Computational Linguistics.</institution>
<contexts>
<context position="31486" citStr="Rush and Collins, 2011" startWordPosition="5412" endWordPosition="5415">which is reasonable if we are in the setting of a parameter tuning step in which the same sentences are translated repeatedly 7 Related work Searching for the most optimal translation in an implicitly defined set has been the focus of a lot of research in Machine Translation and it would be difficult to cover all of it. Among the most influential approaches, (Koehn et al., 2003) was using a form of stack based decoding for Phrase-Based Machine Translation. (Chiang, 2007) introduced the cube-pruning approach, which has been further improved in the previously mentioned (Heafield et al., 2013). (Rush and Collins, 2011) recently proposed an algorithm promising to find the optimal solution, but that is rather slow in practice. Weighted Finite State Machines have seen a variety of use in NLP (Mohri, 1997). More specifically, some other previous work on Machine Translation have used lattices (or more generally Weighted Finite State Machines). In the context of Corpus-Based Machine Translation, (Knight and Al-Onaizan, 1998) was already proposing to use Weighted Transducers to decode the “IBM” models of translation (Brown et al., 1993). (Casacuberta and Vidal, 2004) and (Kumar et al., 2006) also propose to direct</context>
</contexts>
<marker>Rush, Collins, 2011</marker>
<rawString>Alexander M Rush and Michael Collins. 2011. Exact decoding of syntactic translation models through lagrangian relaxation. In Proceedings of the 49th Annual Meeting of the Association for Computational Linguistics: Human Language Technologies-Volume 1, pages 72–82. Association for Computational Linguistics.</rawString>
</citation>
</citationList>
</algorithm>
</algorithms>
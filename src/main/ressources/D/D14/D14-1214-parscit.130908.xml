<?xml version="1.0" encoding="UTF-8"?>
<algorithms version="110505">
<algorithm name="SectLabel" version="110505">
<variant no="0" confidence="0.000003">
<title confidence="0.99679">
Major Life Event Extraction from Twitter based on
Congratulations/Condolences Speech Acts
</title>
<author confidence="0.99949">
Jiwei Li&apos;, Alan Ritter2, Claire Cardie3 and Eduard Hovy4
</author>
<affiliation confidence="0.99944525">
&apos;Computer Science Department, Stanford University, Stanford, CA 94305, USA
2Department of Computer Science and Engineering, the Ohio State University, OH 43210, USA
3Computer Science Department, Cornell University, Ithaca, NY 14853, USA
4Language Technology Institute, Carnegie Mellon University, PA 15213, USA
</affiliation>
<email confidence="0.99659">
jiweil@stanford.edu ritter.1492@osu.edu
cardie@cs.cornell.edu ehovy@andrew.cmu.edu
</email>
<sectionHeader confidence="0.993875" genericHeader="abstract">
Abstract
</sectionHeader>
<bodyText confidence="0.999901894736842">
Social media websites provide a platform
for anyone to describe significant events
taking place in their lives in realtime.
Currently, the majority of personal news
and life events are published in a tex-
tual format, motivating information ex-
traction systems that can provide a struc-
tured representations of major life events
(weddings, graduation, etc... ). This pa-
per demonstrates the feasibility of accu-
rately extracting major life events. Our
system extracts a fine-grained description
of users’ life events based on their pub-
lished tweets. We are optimistic that our
system can help Twitter users more easily
grasp information from users they take in-
terest in following and also facilitate many
downstream applications, for example re-
altime friend recommendation.
</bodyText>
<sectionHeader confidence="0.998994" genericHeader="keywords">
1 Introduction
</sectionHeader>
<bodyText confidence="0.985431283018868">
Social networking websites such as Facebook and
Twitter have recently challenged mainstream me-
dia as the freshest source of information on im-
portant news events. In addition to an important
source for breaking news, social media presents a
unique source of information on private events, for
example a friend’s engagement or college gradua-
tion (examples are presented in Figure 1). While
a significant amount of previous work has inves-
tigated event extraction from Twitter (e.g., (Rit-
ter et al., 2012; Diao et al., 2012)), existing ap-
proaches mostly focus on public bursty event ex-
traction, and little progress has been made towards
the problem of automatically extracting the major
life events of ordinary users.
A system which can automatically extract ma-
jor life events and generate fine-grained descrip-
tions as in Figure 1 will not only help Twitter
users with the problem of information overload by
summarizing important events taking place in their
friends lives, but could also facilitate downstream
applications such as friend recommendation (e.g.,
friend recommendation in realtime to people who
were just admitted into the same university, get
the same jobs or internships), targeted online ad-
vertising (e.g., recommend baby care products to
newly expecting mothers, or wedding services to
new couples), information extraction, etc.
Before getting started, we first identify a num-
ber of key challenges in extracting significant life
events from user-generated text, which account the
reason for the lack of previous work in this area:
Challenge 1: Ambiguous Definition for Ma-
jor Life Events Major life event identification
is an open-domain problem. While many types of
events (e.g., marriage, engagement, finding a new
job, giving birth) are universally agreed to be im-
portant, it is difficult to robustly predefine a list of
characteristics for important life events on which
algorithms can rely for extraction or classification.
Challenge 2: Noisiness of Twitter Data: The
user-generated text found in social media websites
such as Twitter is extremely noisy. The language
used to describe life events is highly varied and
ambiguous and social media users frequently dis-
cuss public news and mundane events from their
daily lives, for instance what they ate for lunch.
Even for a predefined life event category, such
as marriage, it is still difficult to accurately iden-
tify mentions. For instance, a search for the
keyphrase ”get married” using Twitter Search1 re-
sults in a large number of returned results that do
not correspond to a personal event:
</bodyText>
<listItem confidence="0.949884333333333">
• I want to get married once. No divorce &amp; no
cheating, just us two till the end.
(error: wishes)
</listItem>
<footnote confidence="0.9730295">
1https://twitter.com/search?q=
get˜married
</footnote>
<page confidence="0.880659">
1997
</page>
<note confidence="0.9909635">
Proceedings of the 2014 Conference on Empirical Methods in Natural Language Processing (EMNLP), pages 1997–2007,
October 25-29, 2014, Doha, Qatar. c�2014 Association for Computational Linguistics
</note>
<figureCaption confidence="0.999322">
Figure 1: Examples of users mentioning personal life events on Twitter.
</figureCaption>
<listItem confidence="0.9982916">
• Can Adam Sandler and Drew Barrymore just
drop the pretense and get married already?
(error: somebody else)
• I got married and had kids on purpose
(error: past)
</listItem>
<bodyText confidence="0.983515962962963">
Challenge 3: the Lack of Training Data Col-
lecting sufficient training data in this task for ma-
chine learning models is difficult for a number of
reasons: (1) A traditional, supervised learning ap-
proach, requires explicit annotation guidelines for
labeling, though it is difficult to know which cat-
egories are most representative in the data apriori.
(2) Unlike public events which are easily identi-
fied based on message volume, significant private
events are only mentioned by one or several users
directly involved in the event. Many important cat-
egories are relatively infrequent, so even a large
annotated dataset may contain just a few or no ex-
amples of these categories, making classification
difficult.
In this paper, we present a pipelined system that
addresses these challenges and extracts a struc-
tured representation of individual life events based
on users’ Twitter feeds. We exploit the insight to
automatically gather large volumes of major life
events which can be used as training examples for
machine learning models. Although personal life
events are difficult to identify using traditional
approaches due to their highly diverse nature, we
noticed that users’ followers often directly reply
to such messages with CONGRATULATIONS or
CONDOLENCES speech acts, for example:
</bodyText>
<footnote confidence="0.359016">
User1: I got accepted into Harvard !
User2: Congratulations !
</footnote>
<bodyText confidence="0.999922944444445">
These speech acts are easy to identify with high
precision because the possible ways to express
them are relatively constrained. Instead of directly
inspecting tweets to determine whether they corre-
spond to major life events, we start by identifying
replies corresponding to CONGRATULATIONS or
CONDOLENCES, and then retrieve the message
they are in response to, which we assume refer to
important life events.
The proposed system automatically identifies
major life events and then extracts correspondent
event properties. Through the proposed system,
we demonstrate that it is feasible to automatically
reconstruct a detailed list of individual life events
based on users’ Twitter streams. We hope that
work presented in this paper will facilitate down-
stream applications and encourage follow-up work
on this task.
</bodyText>
<sectionHeader confidence="0.942818" genericHeader="introduction">
2 System Overview
</sectionHeader>
<bodyText confidence="0.9998024">
An overview of the components of the system is
presented in Figure 2. Pipeline1 first identifies
the major life event category the input tweet talks
about and filters out the irrelevant tweets and will
be described in Section 4. Next, Pipeline2, as,
demonstrated in Section 5, identifies whether the
speaker is directly involved in the life event. Fi-
nally, Pipeline3 extracts the property of event and
will be illustrated in Section 6.
Section 3 serves as the preparing step for the
pipelined system, describing how we collect train-
ing data in large-scale. The experimental evalua-
tion regarding each pipeline of the system is pre-
sented in the corresponding section (i.e., Section
4,5,6) and the end-to-end evaluation will be pre-
</bodyText>
<page confidence="0.99581">
1998
</page>
<figureCaption confidence="0.821486428571429">
Figure 2: System Overview. Blue: original input tweets. Red: filtered out tweets. Magenta: life event
category. Green: life event property. Pipeline 1 identifies the life category the input tweet talks about
(e.g., marriage, graduation) and filter out irrelevant tweets (e.g., I had beef stick for lunch). Pipeline 2
identifies whether the speaker is directly involved in the event. It will preserve self-reported information
(i.e. “I got married”) and filtered out unrelated tweets (e.g., “my friend Chris got married”). Pipeline
3 extracts the property of event (e.g. to whom the speaker married or the speaker admitted by which
university).
</figureCaption>
<bodyText confidence="0.862288">
sented in Section 7.
</bodyText>
<sectionHeader confidence="0.995485" genericHeader="method">
3 Personal Life Event Clustering
</sectionHeader>
<bodyText confidence="0.9999396">
In this section, we describe how we identify com-
mon categories of major life events by leverag-
ing large quantities of unlabeled data and obtain
a collection of tweets corresponding to each type
of identified event.
</bodyText>
<subsectionHeader confidence="0.999497">
3.1 Response based Life Event Detection
</subsectionHeader>
<bodyText confidence="0.999384714285714">
While not all major life events will elicit CON-
GRATULATIONS or CONDOLENCES from a user’s
followers, this technique allows us to collect large
volumes of high-precision personal life events
which can be used to train models to recognize the
diverse categories of major life events discussed
by social media users.
</bodyText>
<subsectionHeader confidence="0.999716">
3.2 Life Event Clustering
</subsectionHeader>
<bodyText confidence="0.999968333333333">
Based on the above intuition, we develop an ap-
proach to obtain a list of individual life event clus-
ters. We first define a small set of seed responses
which capture common CONGRATULATIONS and
CONDOLENCES, including the phrases: ”Congrat-
ulations”, ”Congrats”, ”Sorry to hear that”, ”Awe-
some”, and gather tweets that were observed with
seed responses. Next, an LDA (Blei et al., 2003)2
based topic model is used to cluster the gathered
</bodyText>
<footnote confidence="0.864962">
2Topic Number is set to 120.
</footnote>
<bodyText confidence="0.999757652173913">
tweets to automatically identify important cate-
gories of major life events in an unsupervised way.
In our approach, we model the whole conversation
dialogue as a document3 with the response seeds
(e.g., congratulation) masked out. We furthermore
associate each sentence with a single topic, fol-
lowing strategies adopted by (Ritter et al., 2010;
Gruber et al., 2007). We limit the words in our
document collection to verbs and nouns which
we found to lead to clearer topic representations,
and used collapsed Gibbs Sampling for inference
(Griffiths and Steyvers, 2004).
Next one of the authors manually inspected the
resulting major life event types inferred by the
model, and manually assigned them labels such
as ”getting a job”, ”graduation” or ”marriage”
and discarded incoherent topics4. Our methodol-
ogy is inspired by (Ritter et al., 2012) that uses
a LDA-CLUSTERING+HUMAN-IDENTIFICATION
strategy to identify public events from Twitter.
Similar strategies have been widely used in un-
supervised information extraction (Bejan et al.,
2009; Yao et al., 2011) and selectional preference
</bodyText>
<footnote confidence="0.997736">
3Each whole conversation usually contains multiple
tweets and users.
4While we applied manual labeling and coherence eval-
uation in this work, an interesting direction for future work
is automatically labeling major life event categories follow-
ing previous work on labeling topics in traditional document-
based topic models (Mimno et al., 2011; Newman et al.,
2010).
</footnote>
<page confidence="0.997498">
1999
</page>
<figureCaption confidence="0.970555333333333">
Figure 3: Illustration of bootstrapping process.
of bootstrapping.
Figure 5: Illustration of data retrieved in each step
</figureCaption>
<figure confidence="0.477164">
Input: Reply seed list E = {e}, Tweet conversation col-
lection T = {t}, Retrieved Tweets Collection D = φ.
Identified topic list L=φ
Begin
While not stopping:
</figure>
<listItem confidence="0.973126647058823">
1. For unprocessed conversation t E T
if t contains reply e E E,
• add t to D: D = D + t.
• remove t from T: T = T − t
2. Run streaming LDA (Yao et al., 2009) on newly added
tweets in D.
3. Manually Identify meaningful/trash topics, giving label
to meaningful topics.
4. Add newly detected meaningful topic l to L.
5. For conversation t belonging to trash topics
• remove t from D: D = D − t
6. Harvest more tweets based on topic distribution.
7. Manually identify top 20 responses to tweets harvested
from Step 6.
8. Add meaningful responses to E.
End
Output: Identified topic list L. Tweet collection D.
</listItem>
<figureCaption confidence="0.9905605">
Figure 4: Bootstrapping Algorithm for Response-
based Life event identification.
</figureCaption>
<bodyText confidence="0.9716962">
modeling (Kozareva and Hovy, 2010a; Roberts
and Harabagiu, 2011).
Conversation data was extracted from the CMU
Twitter Warehouse of 2011 which contains a total
number of 10% of all published tweets in that year.
</bodyText>
<subsectionHeader confidence="0.99953">
3.3 Expanding dataset using Bootstrapping
</subsectionHeader>
<bodyText confidence="0.999983857142857">
While our seed patterns for identifying mes-
sages expressing CONGRATULATIONS and CON-
DOLENCES are very high precision, they don’t
cover all the possible ways these speech acts
can be expressed. We therefore adopt a semi-
supervised bootstrapping approach to expand our
reply seeds and event-related tweets. Our boot-
strapping approach is related to previous work
on semi-supervised information harvesting (e.g.,
(Kozareva and Hovy, 2010b; Davidov et al.,
2007)). To preserve the labeled topics from the
first iteration, we apply a streaming approach to
inference (Yao et al., 2009) over unlabeled tweets
(those which did not match one of the response
</bodyText>
<tableCaption confidence="0.805920666666667">
congratulations (cong, congrats); (that’s) fantastic; (so) cool;
(I’m) (very) sorry to hear that; (that’s) great (good) new;
awesome; what a pity; have fun; great; that sucks; too
bad; (that’s) unfortunate; how sad; fabulous; (that’s)
terrific; (that’s) (so) wonderful; my deepest condolences;
Table 1: Responses retrieved from Bootstrapping.
</tableCaption>
<bodyText confidence="0.999960142857143">
seeds). We collect responses to the newly added
tweets, then select the top 20 frequent replies5.
Next we manually inspect and filter the top ranked
replies, and use them to harvest more tweets. This
process is then repeated with another round of
inference in LDA including manual labeling of
newly inferred topics, etc... An illustration of our
approach is presented in Figure 3 and the details
are presented in Figure 4. The algorithm outputs
a collection of personal life topics L, and a collec-
tion of retrieved tweets D. Each tweet d ∈ D is
associated with a life event topic l, l ∈ L.
We repeat the bootstrapping process for 4 iter-
ations and end up with 30 different CONGRATU-
LATIONS and CONDOLENCES patterns (shown in
Table 1) and 42 coherent event types which refer to
significant life events (statistics for harvested data
from each step is shown in Figure 5). We show
examples of the mined topics with correspondent
human labels in Table 3, grouped according to a
specific kind of resemblance.
</bodyText>
<subsectionHeader confidence="0.991972">
3.4 Summary and Discussion
</subsectionHeader>
<bodyText confidence="0.947936090909091">
The objective of this section is (1) identifying a
category of life events (2) identifying tweets asso-
ciated with each event type which can be used as
candidates for latter self reported personal infor-
mation and life event category identification.
We understand that the event list retrieved from
our approach based on replies in the conversation
is far from covering all types of personal events
(especially the less frequent life events). But our
5We only treat the first sentence that responds to the be-
ginning of the conversation as replies.
</bodyText>
<page confidence="0.957134">
2000
</page>
<table confidence="0.999360470588235">
Life Event Proportion
Birthday 9.78
Job 8.39
Wedding 7.24
Engagement
Award 6.20
Sports 6.08
Anniversary 5.44
Give Birth 4.28
Graduate 3.86
Death 3.80
Admission 3.54
Interview 3.44
Internship
Moving 3.26
Travel 3.24
Illness 2.45
</table>
<tableCaption confidence="0.946537">
Table 2: List of automatically discovered life event
types with percentage (%) of data covered.
</tableCaption>
<bodyText confidence="0.999607375">
list is still able to cover a large proportion of IM-
PORTANT and COMMON life events. Our latter
work is focused on given a random tweet, identi-
fying whether it corresponds to one of the 42 types
of life events in our list.
Another thing worth noting here is that, while
current section is not focused on self-reported in-
formation identification, we have already obtained
a relatively clean set of data with a large pro-
portion of non self-reported information related
tweets being screened: people do not usually re-
spond to non self-reported information with com-
monly used replies, or in other words, with replies
that will pass our next step human test6. These non
self-reported tweets would therefore be excluded
from training data.
</bodyText>
<sectionHeader confidence="0.978321" genericHeader="method">
4 Life Event Identification
</sectionHeader>
<bodyText confidence="0.999356230769231">
In this section, we focused on deciding whether a
given tweet corresponds to one of the 42 prede-
fined life events.
Our training dataset consists of approximately
72,000 tweets from 42 different categories of life
events inferred by our topic model as described
in Section 3. We used the top 25% of tweets for
which our model assigned highest probability to
each topic. For sparsely populated topics we used
the top 50% of tweets to ensure sufficient cover-
age.
We further collected a random sample of about
10 million tweets from Twitter API7 as non-life
</bodyText>
<footnote confidence="0.9955255">
6For example, people don’t normally respond to ”I want
to get married once” (example in Challenge 2, Section 1)
with ”Congratulations”.
7https://dev.twitter.com/
</footnote>
<table confidence="0.999853138461539">
Human Label Top words
Wedding wedding, love, ring, engagement,
&amp;engagement engaged, bride, video, marrying
Relationship boyfriend, girlfriend, date, check,
Begin relationship, see, look
Anniversary anniversary, years, year, married,
celebrating, wife, celebrate, love
Relation End/ relationship, ended, hurt, hate, de-
Devoice voice, blessings, single
Graduation graduation, school, college, gradu-
ate, graduating, year, grad
Admission admitted, university, admission, ac-
cepted, college, offer, school
Exam passed, exam, test, school,
semester, finished, exams,
midterms
Research research, presentation, journalism,
paper, conference, go, writing
Essay &amp; Thesis essay, thesis, reading, statement,
dissertation, complete, project
Job job, accepted, announce, join, join-
ing, offer, starting, announced,
work
Interview&amp; In- interview, position, accepted, in-
ternship ternship, offered, start, work
Moving house, moving, move, city, home,
car, place, apartment, town, leaving
Travel leave, leaving, flight, home, miss,
house, airport, packing, morning
Vacation vocation, family, trip, country, go,
flying, visited, holiday, Hawaii
Winning Award won, award, support, awards, win-
ning, honor, scholarship, prize
Election/ president, elected, run, nominated,
Promotion/ named, promotion, cel, selected,
Nomination business, vote
Publishing book, sold, writing, finished, read,
copy, review, release, books, cover
Contract signed, contract, deal, agreements,
agreed, produce, dollar, meeting
song/ video/ al- video, song, album, check, show,
bum release see, making, radio, love
Acting play, role, acting, drama, played,
series, movie, actor, theater
Death dies, passed, cancer, family, hospi-
tal, dad, grandma, mom, grandpa
Give Birth baby, born, boy, pregnant, girl, lbs,
name, son, world, daughter, birth
Illness ill, hospital, feeling, sick, cold, flu,
getting, fever, doctors, cough
Surgery surgery, got, test, emergency, blood,
tumor, stomachs, hospital, pain,
brain
Sports win, game, team, season, fans,
played, winning, football, luck
Running run, race, finished, race, marathon,
ran, miles, running, finish, goal
New Car car, buy, bought, cars, get, drive,
pick, seat, color, dollar, meet
Lost Weight weight, lost, week, pounds, loss,
weeks, gym, exercise, running
Birthday birthday, come, celebrate, party,
friends, dinner, tonight, friend
Lawsuit sue, sued, file, lawsuit, lawyer, dol-
lars, illegal, court, jury.
</table>
<tableCaption confidence="0.998598">
Table 3: Example event types with top words dis-
covered by our model.
</tableCaption>
<table confidence="0.9996718125">
Life Event Proportion
Vacation 2.24
Relationship 2.16
Exams 2.02
Election 1.85
New Car 1.65
Running 1.42
Surgery 1.20
Lawsuit 0.64
Acting 0.50
Research 0.48
Essay 0.35
Lost Weight 0.35
Publishing 0.28
Song 0.22
OTHER 15.31
</table>
<page confidence="0.925008">
2001
</page>
<bodyText confidence="0.8272075">
event examples and trained a 43-class maximum
entropy classifier based on the following features:
</bodyText>
<listItem confidence="0.9855155">
• Word: The sequence of words in the tweet.
• NER: Named entity Tag.
• Dictionary: Word matching a dictionaries of
the top 40 words for each life event category
(automatically inferred by the topic model).
The feature value is the term’s probability
generated by correspondent event.
• Window: If a dictionary term exists, left and
right context words within a window of 3
words and their part-of-speech tags.
</listItem>
<bodyText confidence="0.999365409090909">
Name entity tag is assigned from Ritter et al’s
Twitter NER system (Ritter et al., 2011). Part-of-
Speech tags are assigned based on Twitter POS
package (Owoputi et al., 2013) developed by
CMU ARK Lab. Dictionary and Window are
constructed based on the topic-term distribution
obtained from the previous section.
The average precision and recall are shown in
Table 4. And as we can observe, the dictionary
(with probability) contributes a lot to the perfor-
mance and by taking into account a more compre-
hensive set of information around the key word,
classifier on All feature setting generate signifi-
cantly better performance, with 0.382 prevision
and 0.48 recall, which is acceptable considering
(1) This is is a 43-way classification with much
more negative data than positive (2) Some types of
events are very close to each other (e.g., Leaving
and Vocation). Note that recall is valued more than
precision here as false-positive examples will be
further screened in self-reported information iden-
tification process in the following section.
</bodyText>
<table confidence="0.9995015">
Feature Setting Precision Recall
Word+NER 0.204 0.326
Word+NER+Dictionary 0.362 0.433
All 0.382 0.487
</table>
<tableCaption confidence="0.94239475">
Table 4: Average Performance of Multi-Class
Classifier on Different Feature Settings. Negative
examples (non important event type) are not con-
sidered.
</tableCaption>
<sectionHeader confidence="0.9983795" genericHeader="method">
5 Self-Reported Information
Identification
</sectionHeader>
<bodyText confidence="0.9994296">
Although a message might refer to a topic cor-
responding to a life event such as marriage, the
event still might be one in which the speaker is
not directly involved. In this section we describe
the self reported event identification portion of our
pipeline, which takes output from Section 4 and
further identifies whether each tweet refers to an
event directly involving the user who publishes it.
Direct labeling of randomly sampled Twitter
messages is infeasible for the following reasons:
</bodyText>
<listItem confidence="0.953676666666667">
(1) Class imbalance: self-reported events are rela-
tively rare in randomly sampled Twitter messages.
(2) A large proportion of self-reported information
refers to mundane, everyday topics (e.g., “I just
finished dinner!”). Fortunately, many of the tweets
retrieved from Section 3 consist of self-reported
information and describe major life events. The
candidates for annotation are therefore largely nar-
rowed down.
</listItem>
<bodyText confidence="0.999961785714286">
We manually annotated 800 positive examples
of self-reported events distributed across the event
categories identified in Section 3. We ensured
good coverage by first randomly sampling 10 ex-
amples from each category, the remainder were
sampled from the class distribution in the data.
Negative examples of self-reported information
consisted of a combination of examples from the
original dataset8 and randomly sampled messages
gathered by searching for the top terms in each of
the pre-identified topics using the Twitter Search
interface 9. Due to great varieties of negative sce-
narios, the negative dataset constitutes about 2500
tweets.
</bodyText>
<subsectionHeader confidence="0.714945">
5.1 Features
</subsectionHeader>
<bodyText confidence="0.9996635">
Identifying self-reported tweet requires sophisti-
cated feature engineering. Let u denote the term
within the tweet that gets the highest possibility
generated by the correspondent topic. We experi-
mented with combinations of the following types
of features (results are presented in Table ??):
</bodyText>
<listItem confidence="0.998586636363636">
• Bigram: Bigrams within each tweet (punctu-
ation included).
• Window: A window of k E {0, 1, 2} words
adjacent to u and their part-of-speech tags.
• Tense: A binary feature indicating past tense
identified in by the presence of past tense
verb (VBD).
• Factuality: Factuality denotes whether one
expression is presented as corresponding to
real situations in the world (Saur´ı and Puste-
jovsky, 2007). We use Stanford PragBank10,
</listItem>
<footnote confidence="0.944616">
8Most tweets in the bootstrapping output are positive.
9The majority of results returned by Twitter Search are
negative examples.
10http://compprag.christopherpotts.net/
factbank.html
</footnote>
<page confidence="0.997556">
2002
</page>
<bodyText confidence="0.99946525">
an extension of FactBank (Saur´ı and Puste-
jovsky, 2009) which contains a list of modal
words such as “might”, “will”, “want to”
etc11.
</bodyText>
<listItem confidence="0.840901833333333">
• I: Whether the subject of the tweet is first per-
son singular.
• Dependency: If the subject is first person
singular and the u is a verb, the dependency
path between the subject and u (or non-
dependency).
</listItem>
<bodyText confidence="0.999900666666667">
Tweet dependency paths were obtained from
(Kong et al., 2014). As the tweet parser we use
only supports one-to-one dependency path iden-
tification but no dependency properties, Depen-
dency is a binary feature. The subject of each
tweet is determined by the dependency link to the
root of the tweet from the parser.
Among the features we explore, Word encodes
the general information within the tweet. Win-
dow addresses the information around topic key
word. The rest of the features specifically address
each of the negative situations described in Chal-
lenge 2, Section 1: Tense captures past event de-
scription, Factuality filters out wishes or imagi-
nation, I and Dependency correspond to whether
the described event involves the speaker. We built
a linear SVM classifier using SVMlight package
(Joachims, 1999).
</bodyText>
<subsectionHeader confidence="0.938834">
5.2 Evaluation
</subsectionHeader>
<bodyText confidence="0.794207">
Feature Setting Acc Pre Rec
Bigram+Window 0.76 0.47 0.44
Bigram+Window 0.77 0.47 0.46
+Tense+Factuality
all 0.82 0.51 0.48
Table 5: Performance for self-report information
identification regarding different feature settings.
We report performance on the task of identi-
fying self-reported information in this subsection.
We employ 5-fold cross validation and report Ac-
curacy (Accu), Prevision (Prec) and Recall (Rec)
regarding different feature settings. The Tense,
Factuality, I and Dependency features positively
contribute to performance respectively and the
best performance is obtained when all types of fea-
tures are included.
</bodyText>
<footnote confidence="0.5615065">
11Due to the colloquial property of tweets, we also intro-
duced terms such as “gonna”, “wanna”, “bona”.
</footnote>
<table confidence="0.99707">
precision recall F1
0.82 0.86 0.84
</table>
<tableCaption confidence="0.982568">
Table 7: Performance for identifying properties.
</tableCaption>
<sectionHeader confidence="0.976614" genericHeader="method">
6 Event Property Extraction
</sectionHeader>
<bodyText confidence="0.999693454545455">
Thus far we have described how to automatically
identify tweets referring to major life events. In
addition, it is desirable to extract important prop-
erties of the event, for example the name of the
university the speaker was admitted to (See Figure
1). In this section we take a supervised approach to
event property extraction, based on manually an-
notated data for a handfull of the major life event
categories automatically identified by our system.
While this approach is unlikely to scale to the di-
versity of important personal events Twitter users
are discussing, our experiments demonstrate that
event property extraction is indeed feasible.
We cast the problem of event property extrac-
tion as a sequence labeling task, using Conditional
Random Fields (Lafferty et al., 2001) for learning
and inference. To make best use of the labeled
data, we trained a unified CRF model for closely
related event categories which often share proper-
ties; the full list is presented in Table 6 and we
labeled 300 tweets in total. Features we used in-
clude:
</bodyText>
<listItem confidence="0.997097857142857">
• word token, capitalization, POS
• left and right context words within a window
of 3 and the correspondent part-of-speech
tags
• word shape, NER
• a gazetteer of universities and employers bor-
rowed from NELL12.
</listItem>
<bodyText confidence="0.9994325">
We use 5-fold cross-validation and report results
in Table 7.
</bodyText>
<sectionHeader confidence="0.995191" genericHeader="method">
7 End-to-End Experiment
</sectionHeader>
<bodyText confidence="0.9999664">
The evaluation for each part of our system has
been demonstrated in the corresponding section.
We now present a real-world evaluation: to what
degree can our trained system automatically iden-
tify life events in real world.
</bodyText>
<subsectionHeader confidence="0.970189">
7.1 Dataset
</subsectionHeader>
<bodyText confidence="0.866437">
We constructed a gold-standard life event dataset
using annotators from Amazon’s Mechanical Turk
(Snow et al., 2008) using 2 approaches:
</bodyText>
<footnote confidence="0.903142">
12http://rtw.ml.cmu.edu/rtw/kbbrowser/
</footnote>
<page confidence="0.942472">
2003
</page>
<figure confidence="0.495966">
Life Event Property
(a) Acceptance, Graduation Name of University/College
(b) Wedding, Engagement, Falling love Name of Spouse/ partner/ bf/ gf
(c) Getting a job, interview, internship Name of Enterprise
(d) Moving to New Places, Trip, Vocation, Leaving Place, Origin, Destination
(e) Winning Award Name of Award, Prize
</figure>
<tableCaption confidence="0.794271">
Table 6: Labeling Event Property.
</tableCaption>
<listItem confidence="0.9997988">
• Ask Twitter users to label their own tweets
(Participants include friends, colleagues of
the authors and Turkers from Amazon Me-
chanical Turk13).
• Ask Turkers to label other people’s tweets.
</listItem>
<bodyText confidence="0.999921272727273">
For option 1, we asked participants to directly la-
bel their own published tweets. For option 2, for
each tweet, we employed 2 Turkers. Due to the
ambiguity in defining life events, the value co-
hen’s kappa14 as a measure of inter-rater agree-
ment is 0.54; this does not show significant inter-
annotator agreement. The authors examined dis-
agreements and also verified all positively labeled
tweets. The resulting dataset contains around 900
positive tweets and about 60,000 negative tweets.
To demonstrate the advantage of leveraging
large quantities of unlabeled data, the first base-
line we investigate is a Supervised model which is
trained on the manually annotated labeled dataset,
and evaluated using 5 fold cross validation. Our
Supervised baseline consists of a linear SVM
classifier using bag of words, NER and POS fea-
tures. We also tested a second baseline that
combines Supervised algorithm with an our self-
reported information classifier, denoted as Super-
vised+Self.
Results are reported in Table 8; as we can ob-
serve, the fully supervised approach is not suitable
for this task with only one digit F1 score. The
explanations are as follows: (1) the labeled data
can only cover a small proportion of life events
(2) supervised learning does not separate impor-
tant event categories and will therefore classify
any tweet with highly weighted features (e.g., the
mention of “I” or “marriage”) as positive. By us-
ing an additional self-reported information classi-
fier in Supervised+Self, we get a significant boost
in precision with a minor recall loss.
</bodyText>
<footnote confidence="0.995407666666667">
13https://www.mturk.com/mturk/welcome
14http://en.wikipedia.org/wiki/Cohen’s_
kappa
</footnote>
<table confidence="0.999894">
Approach Precision Recall
Our approach 0.62 0.48
Supervised 0.13 0.20
Supervised+Self 0.25 0.18
</table>
<tableCaption confidence="0.948119">
Table 8: Performance for different approaches for
identifying life events in real world.
</tableCaption>
<table confidence="0.99966075">
Approach Precision Recall
Step 1 0.65 0.36
Step 2 0.64 0.43
Step 3 0.62 0.48
</table>
<tableCaption confidence="0.9544595">
Table 9: Performance for different steps of boot-
strapping for identifying life events in real world.
</tableCaption>
<bodyText confidence="0.999520916666667">
Another interesting question is to what degree
the bootstrapping contributes to the final results.
We keep the self-reported information classifier
fixed (though it’s based the ultimate identified
data source), and train the personal event classifier
based on topic distributions identified from each
of the three steps of bootstrapping15. Precision
and recall at various stages of bootstrapping are
presented in Table 9. As bootstrapping continues,
the precision remains roughly constant, but recall
increases as more life events and CONGRATULA-
TIONS and CONDOLENCES are discovered.
</bodyText>
<sectionHeader confidence="0.999719" genericHeader="related work">
8 Related Work
</sectionHeader>
<bodyText confidence="0.895379142857143">
Our work is related to three lines of NLP re-
searches. (1) user-level information extraction on
social media (2) public event extraction on social
media. (3) Data harvesting in Information Extrac-
tion, each of which contains large amount of re-
lated work, to which we can not do fully justice.
User Information Extraction from Twitter
Some early approaches towards understanding
user level information on social media is focused
on user profile/attribute prediction (e.g.,(Ciot et
al., 2013)) user-specific content extraction (Diao
15which are 24, 38, 42-class classifiers, where 24, 38, 42
denoted the number of topics discovered in each step of boot-
strapping (see Figure 5).
</bodyText>
<page confidence="0.993321">
2004
</page>
<bodyText confidence="0.992350933333333">
et al., 2012; Diao and Jiang, 2013; Li et al., 2014)
or user personalization (Low et al., 2011) identifi-
cation.
The problem of user life event extraction was
first studied by Li and Cardie’s (2014). They at-
tempted to construct a chronological timeline for
Twitter users from their published tweets based on
two criterion: a personal event should be personal
and time-specific. Their system does not explic-
itly identify a global category of life events (and
tweets discussing correspondent event) but identi-
fies the topics/events that are personal and time-
specific to a given user using an unsupervised ap-
proach, which helps them avoids the nuisance of
explicit definition for life event characteristics and
acquisition of labeled data. However, their sys-
tem has the short-coming that each personal topic
needs to be adequately discussed by the user and
their followers in order to be detected16.
Public Event Extraction from Twitter Twitter
serves as a good source for event detection owing
to its real time nature and large number of users.
These approaches include identifying bursty pub-
lic topics (e.g.,(Diao et al., 2012)), topic evolution
(Becker et al., 2011) or disaster outbreak (Sakaki
et al., 2010; Li and Cardie, 2013) by spotting the
increase/decrease of word frequency. Some other
approaches are focused on generating a structured
representation of events (Ritter et al., 2012; Ben-
son et al., 2011).
</bodyText>
<subsectionHeader confidence="0.612367">
Data Acquisition in Information Extraction
</subsectionHeader>
<bodyText confidence="0.999945714285714">
Our work is also related with semi-supervised data
harvesting approaches, the key idea of which is
that some patterns are learned based on seeds.
They are then used to find additional terms, which
are subsequently used as new seeds in the patterns
to search for additional new patterns (Kozareva
and Hovy, 2010b; Davidov et al., 2007; Riloff
et al., 1999; Igo and Riloff, 2009; Kozareva et
al., 2008). Also related approaches are distant or
weakly supervision (Mintz et al., 2009; Craven et
al., 1999; Hoffmann et al., 2011) that rely on avail-
able structured data sources as a weak source of
supervision for pattern extraction from related text
corpora.
</bodyText>
<sectionHeader confidence="0.938614" genericHeader="conclusions">
9 Conclusion and Discussion
</sectionHeader>
<bodyText confidence="0.99996678125">
In this paper, we propose a pipelined system for
major life event extraction from Twitter. Experi-
mental results show that our model is able to ex-
tract a wide variety of major life events.
The key strategy adopted in this work is to ob-
tain a relatively clean training dataset from large
quantity of Twitter data by relying on minimum
efforts of human supervision, and sometimes is at
the sacrifice of recall. To achieve this goal, we rely
on a couple of restrictions and manual screenings,
such as relying on replies, LDA topic identifica-
tion and seed screening. Each part of system de-
pends on the early steps. For example, topic clus-
tering in Section 3 not only offers training data for
event identification in Section 4, but prepares the
training data for self-information identification in
Section 5. .
We acknowledge that our approach is not
perfect due to the following ways: (1) The system
is only capable of discovering a few categories
of life events with many others left unidentified.
(2) Each step of the system will induce errors and
negatively affected the following parts. (3) Some
parts of evaluations are not comprehensive due
to the lack of gold-standard data. (4) Among all
pipelines, event property identification in Section
6 still requires full supervision in CRF model,
making it hard to scale to every event type17.
How to address these aspects and generate a more
accurate, comprehensive and fine-grained life
event list for Twitter users constitute our further
work.
</bodyText>
<sectionHeader confidence="0.994949" genericHeader="acknowledgments">
Acknowledgements
</sectionHeader>
<bodyText confidence="0.999960583333333">
A special thanks is owned to Myle Ott for sug-
gestions on bootstrapping procedure in data har-
vesting. The authors want to thank Noah Smith,
Chris Dyer and Alok Kothari for useful com-
ments, discussions and suggestions regarding dif-
ferent steps of the system and evaluations. We
thank Lingpeng Kong and members of Noah’s
ARK group at CMU for providing the tweet de-
pendency parser. All data used in this work is ex-
tracted from CMU Twitter Warehouse maintained
by Brendan O’Connor, to whom we want to ex-
press our gratitude.
</bodyText>
<footnote confidence="0.99880625">
16The reason is that topic models use word frequency for
topic modeling.
17We view weakly supervised life event property extrac-
tion as an interesting direction for future work.
</footnote>
<page confidence="0.992929">
2005
</page>
<bodyText confidence="0.985131">
Annual Meeting of the Association for Computa-
tional Linguistics: Human Language Technologies-
Volume 1, pages 541–550. Association for Compu-
tational Linguistics.
</bodyText>
<note confidence="0.72664525">
References
Hila Becker, Mor Naaman, and Luis Gravano. 2011.
Beyond trending topics: Real-world event identifi-
cation on twitter. ICWSM, 11:438–441.
</note>
<reference confidence="0.998332653061224">
Cosmin Adrian Bejan, Matthew Titsworth, Andrew
Hickl, and Sanda M Harabagiu. 2009. Nonparamet-
ric bayesian models for unsupervised event corefer-
ence resolution. In NIPS, pages 73–81.
Edward Benson, Aria Haghighi, and Regina Barzilay.
2011. Event discovery in social media feeds. In
Proceedings of the 49th Annual Meeting of the Asso-
ciation for Computational Linguistics: Human Lan-
guage Technologies-Volume 1, pages 389–398. As-
sociation for Computational Linguistics.
David M Blei, Andrew Y Ng, and Michael I Jordan.
2003. Latent dirichlet allocation. the Journal of ma-
chine Learning research, 3:993–1022.
Morgane Ciot, Morgan Sonderegger, and Derek Ruths.
2013. Gender inference of twitter users in non-
english contexts. In Proceedings of the 2013 Con-
ference on Empirical Methods in Natural Language
Processing, Seattle, Wash, pages 18–21.
Mark Craven, Johan Kumlien, et al. 1999. Construct-
ing biological knowledge bases by extracting infor-
mation from text sources. In ISMB, volume 1999,
pages 77–86.
Dmitry Davidov, Ari Rappoport, and Moshe Koppel.
2007. Fully unsupervised discovery of concept-
specific relationships by web mining. In Annual
Meeting-Association For Computational Linguis-
tics, volume 45, page 232.
Qiming Diao and Jing Jiang. 2013. A unified model
for topics, events and users on twitter. In Proceed-
ings of the 2013 Conference on Empirical Methods
in Natural Language Processing, pages 1869–1879.
Qiming Diao, Jing Jiang, Feida Zhu, and Ee-Peng
Lim. 2012. Finding bursty topics from microblogs.
In Proceedings of the 50th Annual Meeting of the
Association for Computational Linguistics: Long
Papers-Volume 1, pages 536–544. Association for
Computational Linguistics.
Thomas L Griffiths and Mark Steyvers. 2004. Find-
ing scientific topics. Proceedings of the National
academy of Sciences of the United States of Amer-
ica, 101(Suppl 1):5228–5235.
Amit Gruber, Yair Weiss, and Michal Rosen-Zvi.
2007. Hidden topic markov models. In Inter-
national Conference on Artificial Intelligence and
Statistics, pages 163–170.
Raphael Hoffmann, Congle Zhang, Xiao Ling, Luke
Zettlemoyer, and Daniel S Weld. 2011. Knowledge-
based weak supervision for information extraction
of overlapping relations. In Proceedings of the 49th
Sean P Igo and Ellen Riloff. 2009. Corpus-based se-
mantic lexicon induction with web-based corrobora-
tion. In Proceedings of the Workshop on Unsuper-
vised and Minimally Supervised Learning of Lexical
Semantics, pages 18–26. Association for Computa-
tional Linguistics.
Thorsten Joachims. 1999. Making large scale svm
learning practical.
Lingpeng Kong, Nathan Schneider, Swabha
Swayamdipta, Archna Bhatia, Chris Dyer, and
Noah Smith. 2014. A dependency parser for tweets.
In EMNLP.
Zornitsa Kozareva and Eduard Hovy. 2010a. Learn-
ing arguments and supertypes of semantic relations
using recursive patterns. In Proceedings of the
48th Annual Meeting of the Association for Compu-
tational Linguistics, pages 1482–1491. Association
for Computational Linguistics.
Zornitsa Kozareva and Eduard Hovy. 2010b. Not
all seeds are equal: Measuring the quality of text
mining seeds. In Human Language Technologies:
The 2010 Annual Conference of the North American
Chapter of the Association for Computational Lin-
guistics, pages 618–626. Association for Computa-
tional Linguistics.
Zornitsa Kozareva, Ellen Riloff, and Eduard H Hovy.
2008. Semantic class learning from the web with
hyponym pattern linkage graphs. In ACL, volume 8,
pages 1048–1056.
John Lafferty, Andrew McCallum, and Fernando CN
Pereira. 2001. Conditional random fields: Prob-
abilistic models for segmenting and labeling se-
quence data.
Jiwei Li and Claire Cardie. 2013. Early stage
influenza detection from twitter. arXiv preprint
arXiv:1309.7340.
Jiwei Li and Claire Cardie. 2014. Timeline generation:
Tracking individuals on twitter. WWW, 2014.
Jiwei Li, Alan Ritter, and Eduard Hovy. 2014.
Weakly supervised user profile extraction from twit-
ter. ACL.
Yucheng Low, Deepak Agarwal, and Alexander J
Smola. 2011. Multiple domain user personaliza-
tion. In Proceedings of the 17th ACM SIGKDD in-
ternational conference on Knowledge discovery and
data mining, pages 123–131. ACM.
David Mimno, Hanna M Wallach, Edmund Talley,
Miriam Leenders, and Andrew McCallum. 2011.
Optimizing semantic coherence in topic models. In
</reference>
<page confidence="0.805401">
2006
</page>
<reference confidence="0.999661890410959">
Proceedings of the Conference on Empirical Meth-
ods in Natural Language Processing, pages 262–
272. Association for Computational Linguistics.
Mike Mintz, Steven Bills, Rion Snow, and Dan Ju-
rafsky. 2009. Distant supervision for relation ex-
traction without labeled data. In Proceedings of the
Joint Conference of the 47th Annual Meeting of the
ACL and the 4th International Joint Conference on
Natural Language Processing of the AFNLP: Vol-
ume 2-Volume 2, pages 1003–1011. Association for
Computational Linguistics.
David Newman, Jey Han Lau, Karl Grieser, and Tim-
othy Baldwin. 2010. Automatic evaluation of
topic coherence. In Human Language Technologies:
The 2010 Annual Conference of the North American
Chapter of the Association for Computational Lin-
guistics, pages 100–108. Association for Computa-
tional Linguistics.
Olutobi Owoputi, Brendan O’Connor, Chris Dyer,
Kevin Gimpel, Nathan Schneider, and Noah A
Smith. 2013. Improved part-of-speech tagging for
online conversational text with word clusters. In
Proceedings of NAACL-HLT, pages 380–390.
Ellen Riloff, Rosie Jones, et al. 1999. Learning dic-
tionaries for information extraction by multi-level
bootstrapping. In AAAI/IAAI, pages 474–479.
Alan Ritter, Colin Cherry, and Bill Dolan. 2010. Un-
supervised modeling of twitter conversations.
Alan Ritter, Sam Clark, Oren Etzioni, et al. 2011.
Named entity recognition in tweets: an experimental
study. In Proceedings of the Conference on Empiri-
cal Methods in Natural Language Processing, pages
1524–1534. Association for Computational Linguis-
tics.
Alan Ritter, Oren Etzioni, Sam Clark, et al. 2012.
Open domain event extraction from twitter. In Pro-
ceedings of the 18th ACM SIGKDD international
conference on Knowledge discovery and data min-
ing, pages 1104–1112. ACM.
Kirk Roberts and Sanda M Harabagiu. 2011. Unsuper-
vised learning of selectional restrictions and detec-
tion of argument coercions. In Proceedings of the
Conference on Empirical Methods in Natural Lan-
guage Processing, pages 980–990. Association for
Computational Linguistics.
Takeshi Sakaki, Makoto Okazaki, and Yutaka Matsuo.
2010. Earthquake shakes twitter users: real-time
event detection by social sensors. In Proceedings
of the 19th international conference on World wide
web, pages 851–860. ACM.
Roser Sauriand James Pustejovsky. 2007. Deter-
mining modality and factuality for text entailment.
In Semantic Computing, 2007. ICSC 2007. Interna-
tional Conference on, pages 509–516. IEEE.
Roser Saur´ı and James Pustejovsky. 2009. Factbank:
A corpus annotated with event factuality. Language
resources and evaluation, 43(3):227–268.
Rion Snow, Brendan O’Connor, Daniel Jurafsky, and
Andrew Y Ng. 2008. Cheap and fast—but is it
good?: evaluating non-expert annotations for natu-
ral language tasks. In Proceedings of the conference
on empirical methods in natural language process-
ing, pages 254–263. Association for Computational
Linguistics.
Limin Yao, David Mimno, and Andrew McCallum.
2009. Efficient methods for topic model inference
on streaming document collections.
Limin Yao, Aria Haghighi, Sebastian Riedel, and An-
drew McCallum. 2011. Structured relation discov-
ery using generative models. In Proceedings of the
Conference on Empirical Methods in Natural Lan-
guage Processing, pages 1456–1466. Association
for Computational Linguistics.
</reference>
<page confidence="0.993974">
2007
</page>
</variant>
</algorithm>
<algorithm name="ParsHed" version="110505">
<variant no="0" confidence="0.403288">
<title confidence="0.9953295">Major Life Event Extraction from Twitter based Congratulations/Condolences Speech Acts</title>
<author confidence="0.993693">Alan Claire</author>
<author confidence="0.993693">Eduard</author>
<affiliation confidence="0.692692">Science Department, Stanford University, Stanford, CA 94305,</affiliation>
<address confidence="0.851752333333333">of Computer Science and Engineering, the Ohio State University, OH 43210, Science Department, Cornell University, Ithaca, NY 14853, Technology Institute, Carnegie Mellon University, PA 15213, USA</address>
<email confidence="0.9958875">jiweil@stanford.eduritter.1492@osu.educardie@cs.cornell.eduehovy@andrew.cmu.edu</email>
<abstract confidence="0.99769705">Social media websites provide a platform for anyone to describe significant events taking place in their lives in realtime. Currently, the majority of personal news and life events are published in a textual format, motivating information extraction systems that can provide a structured representations of major life events (weddings, graduation, etc... ). This paper demonstrates the feasibility of accurately extracting major life events. Our system extracts a fine-grained description of users’ life events based on their published tweets. We are optimistic that our system can help Twitter users more easily grasp information from users they take interest in following and also facilitate many downstream applications, for example realtime friend recommendation.</abstract>
</variant>
</algorithm>
<algorithm name="ParsCit" version="110505">
<citationList>
<citation valid="true">
<authors>
<author>Cosmin Adrian Bejan</author>
<author>Matthew Titsworth</author>
<author>Andrew Hickl</author>
<author>Sanda M Harabagiu</author>
</authors>
<title>Nonparametric bayesian models for unsupervised event coreference resolution.</title>
<date>2009</date>
<booktitle>In NIPS,</booktitle>
<pages>73--81</pages>
<contexts>
<context position="10187" citStr="Bejan et al., 2009" startWordPosition="1569" endWordPosition="1572">nd nouns which we found to lead to clearer topic representations, and used collapsed Gibbs Sampling for inference (Griffiths and Steyvers, 2004). Next one of the authors manually inspected the resulting major life event types inferred by the model, and manually assigned them labels such as ”getting a job”, ”graduation” or ”marriage” and discarded incoherent topics4. Our methodology is inspired by (Ritter et al., 2012) that uses a LDA-CLUSTERING+HUMAN-IDENTIFICATION strategy to identify public events from Twitter. Similar strategies have been widely used in unsupervised information extraction (Bejan et al., 2009; Yao et al., 2011) and selectional preference 3Each whole conversation usually contains multiple tweets and users. 4While we applied manual labeling and coherence evaluation in this work, an interesting direction for future work is automatically labeling major life event categories following previous work on labeling topics in traditional documentbased topic models (Mimno et al., 2011; Newman et al., 2010). 1999 Figure 3: Illustration of bootstrapping process. of bootstrapping. Figure 5: Illustration of data retrieved in each step Input: Reply seed list E = {e}, Tweet conversation collection </context>
</contexts>
<marker>Bejan, Titsworth, Hickl, Harabagiu, 2009</marker>
<rawString>Cosmin Adrian Bejan, Matthew Titsworth, Andrew Hickl, and Sanda M Harabagiu. 2009. Nonparametric bayesian models for unsupervised event coreference resolution. In NIPS, pages 73–81.</rawString>
</citation>
<citation valid="true">
<authors>
<author>Edward Benson</author>
<author>Aria Haghighi</author>
<author>Regina Barzilay</author>
</authors>
<title>Event discovery in social media feeds.</title>
<date>2011</date>
<booktitle>In Proceedings of the 49th Annual Meeting of the Association for Computational Linguistics: Human Language Technologies-Volume 1,</booktitle>
<pages>389--398</pages>
<publisher>Association</publisher>
<institution>for Computational Linguistics.</institution>
<contexts>
<context position="32228" citStr="Benson et al., 2011" startWordPosition="4998" endWordPosition="5002">rsonal topic needs to be adequately discussed by the user and their followers in order to be detected16. Public Event Extraction from Twitter Twitter serves as a good source for event detection owing to its real time nature and large number of users. These approaches include identifying bursty public topics (e.g.,(Diao et al., 2012)), topic evolution (Becker et al., 2011) or disaster outbreak (Sakaki et al., 2010; Li and Cardie, 2013) by spotting the increase/decrease of word frequency. Some other approaches are focused on generating a structured representation of events (Ritter et al., 2012; Benson et al., 2011). Data Acquisition in Information Extraction Our work is also related with semi-supervised data harvesting approaches, the key idea of which is that some patterns are learned based on seeds. They are then used to find additional terms, which are subsequently used as new seeds in the patterns to search for additional new patterns (Kozareva and Hovy, 2010b; Davidov et al., 2007; Riloff et al., 1999; Igo and Riloff, 2009; Kozareva et al., 2008). Also related approaches are distant or weakly supervision (Mintz et al., 2009; Craven et al., 1999; Hoffmann et al., 2011) that rely on available structu</context>
</contexts>
<marker>Benson, Haghighi, Barzilay, 2011</marker>
<rawString>Edward Benson, Aria Haghighi, and Regina Barzilay. 2011. Event discovery in social media feeds. In Proceedings of the 49th Annual Meeting of the Association for Computational Linguistics: Human Language Technologies-Volume 1, pages 389–398. Association for Computational Linguistics.</rawString>
</citation>
<citation valid="true">
<authors>
<author>David M Blei</author>
<author>Andrew Y Ng</author>
<author>Michael I Jordan</author>
</authors>
<title>Latent dirichlet allocation.</title>
<date>2003</date>
<journal>the Journal of machine Learning research,</journal>
<pages>3--993</pages>
<contexts>
<context position="9065" citStr="Blei et al., 2003" startWordPosition="1395" endWordPosition="1398">s technique allows us to collect large volumes of high-precision personal life events which can be used to train models to recognize the diverse categories of major life events discussed by social media users. 3.2 Life Event Clustering Based on the above intuition, we develop an approach to obtain a list of individual life event clusters. We first define a small set of seed responses which capture common CONGRATULATIONS and CONDOLENCES, including the phrases: ”Congratulations”, ”Congrats”, ”Sorry to hear that”, ”Awesome”, and gather tweets that were observed with seed responses. Next, an LDA (Blei et al., 2003)2 based topic model is used to cluster the gathered 2Topic Number is set to 120. tweets to automatically identify important categories of major life events in an unsupervised way. In our approach, we model the whole conversation dialogue as a document3 with the response seeds (e.g., congratulation) masked out. We furthermore associate each sentence with a single topic, following strategies adopted by (Ritter et al., 2010; Gruber et al., 2007). We limit the words in our document collection to verbs and nouns which we found to lead to clearer topic representations, and used collapsed Gibbs Sampl</context>
</contexts>
<marker>Blei, Ng, Jordan, 2003</marker>
<rawString>David M Blei, Andrew Y Ng, and Michael I Jordan. 2003. Latent dirichlet allocation. the Journal of machine Learning research, 3:993–1022.</rawString>
</citation>
<citation valid="true">
<authors>
<author>Morgane Ciot</author>
<author>Morgan Sonderegger</author>
<author>Derek Ruths</author>
</authors>
<title>Gender inference of twitter users in nonenglish contexts.</title>
<date>2013</date>
<booktitle>In Proceedings of the 2013 Conference on Empirical Methods in Natural Language Processing,</booktitle>
<pages>18--21</pages>
<location>Seattle, Wash,</location>
<contexts>
<context position="30626" citStr="Ciot et al., 2013" startWordPosition="4742" endWordPosition="4745">ins roughly constant, but recall increases as more life events and CONGRATULATIONS and CONDOLENCES are discovered. 8 Related Work Our work is related to three lines of NLP researches. (1) user-level information extraction on social media (2) public event extraction on social media. (3) Data harvesting in Information Extraction, each of which contains large amount of related work, to which we can not do fully justice. User Information Extraction from Twitter Some early approaches towards understanding user level information on social media is focused on user profile/attribute prediction (e.g.,(Ciot et al., 2013)) user-specific content extraction (Diao 15which are 24, 38, 42-class classifiers, where 24, 38, 42 denoted the number of topics discovered in each step of bootstrapping (see Figure 5). 2004 et al., 2012; Diao and Jiang, 2013; Li et al., 2014) or user personalization (Low et al., 2011) identification. The problem of user life event extraction was first studied by Li and Cardie’s (2014). They attempted to construct a chronological timeline for Twitter users from their published tweets based on two criterion: a personal event should be personal and time-specific. Their system does not explicitly</context>
</contexts>
<marker>Ciot, Sonderegger, Ruths, 2013</marker>
<rawString>Morgane Ciot, Morgan Sonderegger, and Derek Ruths. 2013. Gender inference of twitter users in nonenglish contexts. In Proceedings of the 2013 Conference on Empirical Methods in Natural Language Processing, Seattle, Wash, pages 18–21.</rawString>
</citation>
<citation valid="true">
<authors>
<author>Mark Craven</author>
<author>Johan Kumlien</author>
</authors>
<title>Constructing biological knowledge bases by extracting information from text sources.</title>
<date>1999</date>
<booktitle>In ISMB,</booktitle>
<volume>volume</volume>
<pages>77--86</pages>
<marker>Craven, Kumlien, 1999</marker>
<rawString>Mark Craven, Johan Kumlien, et al. 1999. Constructing biological knowledge bases by extracting information from text sources. In ISMB, volume 1999, pages 77–86.</rawString>
</citation>
<citation valid="true">
<authors>
<author>Dmitry Davidov</author>
<author>Ari Rappoport</author>
<author>Moshe Koppel</author>
</authors>
<title>Fully unsupervised discovery of conceptspecific relationships by web mining.</title>
<date>2007</date>
<booktitle>In Annual Meeting-Association For Computational Linguistics,</booktitle>
<volume>45</volume>
<pages>232</pages>
<contexts>
<context position="12277" citStr="Davidov et al., 2007" startWordPosition="1914" endWordPosition="1917">extracted from the CMU Twitter Warehouse of 2011 which contains a total number of 10% of all published tweets in that year. 3.3 Expanding dataset using Bootstrapping While our seed patterns for identifying messages expressing CONGRATULATIONS and CONDOLENCES are very high precision, they don’t cover all the possible ways these speech acts can be expressed. We therefore adopt a semisupervised bootstrapping approach to expand our reply seeds and event-related tweets. Our bootstrapping approach is related to previous work on semi-supervised information harvesting (e.g., (Kozareva and Hovy, 2010b; Davidov et al., 2007)). To preserve the labeled topics from the first iteration, we apply a streaming approach to inference (Yao et al., 2009) over unlabeled tweets (those which did not match one of the response congratulations (cong, congrats); (that’s) fantastic; (so) cool; (I’m) (very) sorry to hear that; (that’s) great (good) new; awesome; what a pity; have fun; great; that sucks; too bad; (that’s) unfortunate; how sad; fabulous; (that’s) terrific; (that’s) (so) wonderful; my deepest condolences; Table 1: Responses retrieved from Bootstrapping. seeds). We collect responses to the newly added tweets, then selec</context>
<context position="32606" citStr="Davidov et al., 2007" startWordPosition="5060" endWordPosition="5063">disaster outbreak (Sakaki et al., 2010; Li and Cardie, 2013) by spotting the increase/decrease of word frequency. Some other approaches are focused on generating a structured representation of events (Ritter et al., 2012; Benson et al., 2011). Data Acquisition in Information Extraction Our work is also related with semi-supervised data harvesting approaches, the key idea of which is that some patterns are learned based on seeds. They are then used to find additional terms, which are subsequently used as new seeds in the patterns to search for additional new patterns (Kozareva and Hovy, 2010b; Davidov et al., 2007; Riloff et al., 1999; Igo and Riloff, 2009; Kozareva et al., 2008). Also related approaches are distant or weakly supervision (Mintz et al., 2009; Craven et al., 1999; Hoffmann et al., 2011) that rely on available structured data sources as a weak source of supervision for pattern extraction from related text corpora. 9 Conclusion and Discussion In this paper, we propose a pipelined system for major life event extraction from Twitter. Experimental results show that our model is able to extract a wide variety of major life events. The key strategy adopted in this work is to obtain a relatively</context>
</contexts>
<marker>Davidov, Rappoport, Koppel, 2007</marker>
<rawString>Dmitry Davidov, Ari Rappoport, and Moshe Koppel. 2007. Fully unsupervised discovery of conceptspecific relationships by web mining. In Annual Meeting-Association For Computational Linguistics, volume 45, page 232.</rawString>
</citation>
<citation valid="true">
<authors>
<author>Qiming Diao</author>
<author>Jing Jiang</author>
</authors>
<title>A unified model for topics, events and users on twitter.</title>
<date>2013</date>
<booktitle>In Proceedings of the 2013 Conference on Empirical Methods in Natural Language Processing,</booktitle>
<pages>1869--1879</pages>
<contexts>
<context position="30851" citStr="Diao and Jiang, 2013" startWordPosition="4779" endWordPosition="4782">n social media (2) public event extraction on social media. (3) Data harvesting in Information Extraction, each of which contains large amount of related work, to which we can not do fully justice. User Information Extraction from Twitter Some early approaches towards understanding user level information on social media is focused on user profile/attribute prediction (e.g.,(Ciot et al., 2013)) user-specific content extraction (Diao 15which are 24, 38, 42-class classifiers, where 24, 38, 42 denoted the number of topics discovered in each step of bootstrapping (see Figure 5). 2004 et al., 2012; Diao and Jiang, 2013; Li et al., 2014) or user personalization (Low et al., 2011) identification. The problem of user life event extraction was first studied by Li and Cardie’s (2014). They attempted to construct a chronological timeline for Twitter users from their published tweets based on two criterion: a personal event should be personal and time-specific. Their system does not explicitly identify a global category of life events (and tweets discussing correspondent event) but identifies the topics/events that are personal and timespecific to a given user using an unsupervised approach, which helps them avoid</context>
</contexts>
<marker>Diao, Jiang, 2013</marker>
<rawString>Qiming Diao and Jing Jiang. 2013. A unified model for topics, events and users on twitter. In Proceedings of the 2013 Conference on Empirical Methods in Natural Language Processing, pages 1869–1879.</rawString>
</citation>
<citation valid="true">
<authors>
<author>Qiming Diao</author>
<author>Jing Jiang</author>
<author>Feida Zhu</author>
<author>Ee-Peng Lim</author>
</authors>
<title>Finding bursty topics from microblogs.</title>
<date>2012</date>
<booktitle>In Proceedings of the 50th Annual Meeting of the Association for Computational Linguistics: Long Papers-Volume 1,</booktitle>
<pages>536--544</pages>
<publisher>Association</publisher>
<institution>for Computational Linguistics.</institution>
<contexts>
<context position="1853" citStr="Diao et al., 2012" startWordPosition="265" endWordPosition="268">cilitate many downstream applications, for example realtime friend recommendation. 1 Introduction Social networking websites such as Facebook and Twitter have recently challenged mainstream media as the freshest source of information on important news events. In addition to an important source for breaking news, social media presents a unique source of information on private events, for example a friend’s engagement or college graduation (examples are presented in Figure 1). While a significant amount of previous work has investigated event extraction from Twitter (e.g., (Ritter et al., 2012; Diao et al., 2012)), existing approaches mostly focus on public bursty event extraction, and little progress has been made towards the problem of automatically extracting the major life events of ordinary users. A system which can automatically extract major life events and generate fine-grained descriptions as in Figure 1 will not only help Twitter users with the problem of information overload by summarizing important events taking place in their friends lives, but could also facilitate downstream applications such as friend recommendation (e.g., friend recommendation in realtime to people who were just admit</context>
<context position="31942" citStr="Diao et al., 2012" startWordPosition="4954" endWordPosition="4957">ies the topics/events that are personal and timespecific to a given user using an unsupervised approach, which helps them avoids the nuisance of explicit definition for life event characteristics and acquisition of labeled data. However, their system has the short-coming that each personal topic needs to be adequately discussed by the user and their followers in order to be detected16. Public Event Extraction from Twitter Twitter serves as a good source for event detection owing to its real time nature and large number of users. These approaches include identifying bursty public topics (e.g.,(Diao et al., 2012)), topic evolution (Becker et al., 2011) or disaster outbreak (Sakaki et al., 2010; Li and Cardie, 2013) by spotting the increase/decrease of word frequency. Some other approaches are focused on generating a structured representation of events (Ritter et al., 2012; Benson et al., 2011). Data Acquisition in Information Extraction Our work is also related with semi-supervised data harvesting approaches, the key idea of which is that some patterns are learned based on seeds. They are then used to find additional terms, which are subsequently used as new seeds in the patterns to search for additio</context>
</contexts>
<marker>Diao, Jiang, Zhu, Lim, 2012</marker>
<rawString>Qiming Diao, Jing Jiang, Feida Zhu, and Ee-Peng Lim. 2012. Finding bursty topics from microblogs. In Proceedings of the 50th Annual Meeting of the Association for Computational Linguistics: Long Papers-Volume 1, pages 536–544. Association for Computational Linguistics.</rawString>
</citation>
<citation valid="true">
<authors>
<author>Thomas L Griffiths</author>
<author>Mark Steyvers</author>
</authors>
<title>Finding scientific topics.</title>
<date>2004</date>
<booktitle>Proceedings of the National academy of Sciences of the United States of America, 101(Suppl</booktitle>
<pages>1--5228</pages>
<contexts>
<context position="9713" citStr="Griffiths and Steyvers, 2004" startWordPosition="1499" endWordPosition="1502"> is used to cluster the gathered 2Topic Number is set to 120. tweets to automatically identify important categories of major life events in an unsupervised way. In our approach, we model the whole conversation dialogue as a document3 with the response seeds (e.g., congratulation) masked out. We furthermore associate each sentence with a single topic, following strategies adopted by (Ritter et al., 2010; Gruber et al., 2007). We limit the words in our document collection to verbs and nouns which we found to lead to clearer topic representations, and used collapsed Gibbs Sampling for inference (Griffiths and Steyvers, 2004). Next one of the authors manually inspected the resulting major life event types inferred by the model, and manually assigned them labels such as ”getting a job”, ”graduation” or ”marriage” and discarded incoherent topics4. Our methodology is inspired by (Ritter et al., 2012) that uses a LDA-CLUSTERING+HUMAN-IDENTIFICATION strategy to identify public events from Twitter. Similar strategies have been widely used in unsupervised information extraction (Bejan et al., 2009; Yao et al., 2011) and selectional preference 3Each whole conversation usually contains multiple tweets and users. 4While we </context>
</contexts>
<marker>Griffiths, Steyvers, 2004</marker>
<rawString>Thomas L Griffiths and Mark Steyvers. 2004. Finding scientific topics. Proceedings of the National academy of Sciences of the United States of America, 101(Suppl 1):5228–5235.</rawString>
</citation>
<citation valid="true">
<authors>
<author>Amit Gruber</author>
<author>Yair Weiss</author>
<author>Michal Rosen-Zvi</author>
</authors>
<title>Hidden topic markov models.</title>
<date>2007</date>
<booktitle>In International Conference on Artificial Intelligence and Statistics,</booktitle>
<pages>163--170</pages>
<contexts>
<context position="9511" citStr="Gruber et al., 2007" startWordPosition="1467" endWordPosition="1470">luding the phrases: ”Congratulations”, ”Congrats”, ”Sorry to hear that”, ”Awesome”, and gather tweets that were observed with seed responses. Next, an LDA (Blei et al., 2003)2 based topic model is used to cluster the gathered 2Topic Number is set to 120. tweets to automatically identify important categories of major life events in an unsupervised way. In our approach, we model the whole conversation dialogue as a document3 with the response seeds (e.g., congratulation) masked out. We furthermore associate each sentence with a single topic, following strategies adopted by (Ritter et al., 2010; Gruber et al., 2007). We limit the words in our document collection to verbs and nouns which we found to lead to clearer topic representations, and used collapsed Gibbs Sampling for inference (Griffiths and Steyvers, 2004). Next one of the authors manually inspected the resulting major life event types inferred by the model, and manually assigned them labels such as ”getting a job”, ”graduation” or ”marriage” and discarded incoherent topics4. Our methodology is inspired by (Ritter et al., 2012) that uses a LDA-CLUSTERING+HUMAN-IDENTIFICATION strategy to identify public events from Twitter. Similar strategies have</context>
</contexts>
<marker>Gruber, Weiss, Rosen-Zvi, 2007</marker>
<rawString>Amit Gruber, Yair Weiss, and Michal Rosen-Zvi. 2007. Hidden topic markov models. In International Conference on Artificial Intelligence and Statistics, pages 163–170.</rawString>
</citation>
<citation valid="true">
<authors>
<author>Raphael Hoffmann</author>
<author>Congle Zhang</author>
<author>Xiao Ling</author>
<author>Luke Zettlemoyer</author>
<author>Daniel S Weld</author>
</authors>
<title>Knowledgebased weak supervision for information extraction of overlapping relations.</title>
<date>2011</date>
<booktitle>In Proceedings of the 49th</booktitle>
<contexts>
<context position="32797" citStr="Hoffmann et al., 2011" startWordPosition="5092" endWordPosition="5095"> of events (Ritter et al., 2012; Benson et al., 2011). Data Acquisition in Information Extraction Our work is also related with semi-supervised data harvesting approaches, the key idea of which is that some patterns are learned based on seeds. They are then used to find additional terms, which are subsequently used as new seeds in the patterns to search for additional new patterns (Kozareva and Hovy, 2010b; Davidov et al., 2007; Riloff et al., 1999; Igo and Riloff, 2009; Kozareva et al., 2008). Also related approaches are distant or weakly supervision (Mintz et al., 2009; Craven et al., 1999; Hoffmann et al., 2011) that rely on available structured data sources as a weak source of supervision for pattern extraction from related text corpora. 9 Conclusion and Discussion In this paper, we propose a pipelined system for major life event extraction from Twitter. Experimental results show that our model is able to extract a wide variety of major life events. The key strategy adopted in this work is to obtain a relatively clean training dataset from large quantity of Twitter data by relying on minimum efforts of human supervision, and sometimes is at the sacrifice of recall. To achieve this goal, we rely on a</context>
</contexts>
<marker>Hoffmann, Zhang, Ling, Zettlemoyer, Weld, 2011</marker>
<rawString>Raphael Hoffmann, Congle Zhang, Xiao Ling, Luke Zettlemoyer, and Daniel S Weld. 2011. Knowledgebased weak supervision for information extraction of overlapping relations. In Proceedings of the 49th</rawString>
</citation>
<citation valid="true">
<authors>
<author>Sean P Igo</author>
<author>Ellen Riloff</author>
</authors>
<title>Corpus-based semantic lexicon induction with web-based corroboration.</title>
<date>2009</date>
<booktitle>In Proceedings of the Workshop on Unsupervised and Minimally Supervised Learning of Lexical Semantics,</booktitle>
<pages>18--26</pages>
<publisher>Association</publisher>
<institution>for Computational Linguistics.</institution>
<contexts>
<context position="32649" citStr="Igo and Riloff, 2009" startWordPosition="5068" endWordPosition="5071">and Cardie, 2013) by spotting the increase/decrease of word frequency. Some other approaches are focused on generating a structured representation of events (Ritter et al., 2012; Benson et al., 2011). Data Acquisition in Information Extraction Our work is also related with semi-supervised data harvesting approaches, the key idea of which is that some patterns are learned based on seeds. They are then used to find additional terms, which are subsequently used as new seeds in the patterns to search for additional new patterns (Kozareva and Hovy, 2010b; Davidov et al., 2007; Riloff et al., 1999; Igo and Riloff, 2009; Kozareva et al., 2008). Also related approaches are distant or weakly supervision (Mintz et al., 2009; Craven et al., 1999; Hoffmann et al., 2011) that rely on available structured data sources as a weak source of supervision for pattern extraction from related text corpora. 9 Conclusion and Discussion In this paper, we propose a pipelined system for major life event extraction from Twitter. Experimental results show that our model is able to extract a wide variety of major life events. The key strategy adopted in this work is to obtain a relatively clean training dataset from large quantity</context>
</contexts>
<marker>Igo, Riloff, 2009</marker>
<rawString>Sean P Igo and Ellen Riloff. 2009. Corpus-based semantic lexicon induction with web-based corroboration. In Proceedings of the Workshop on Unsupervised and Minimally Supervised Learning of Lexical Semantics, pages 18–26. Association for Computational Linguistics.</rawString>
</citation>
<citation valid="true">
<authors>
<author>Thorsten Joachims</author>
</authors>
<title>Making large scale svm learning practical.</title>
<date>1999</date>
<contexts>
<context position="24366" citStr="Joachims, 1999" startWordPosition="3781" endWordPosition="3782">inary feature. The subject of each tweet is determined by the dependency link to the root of the tweet from the parser. Among the features we explore, Word encodes the general information within the tweet. Window addresses the information around topic key word. The rest of the features specifically address each of the negative situations described in Challenge 2, Section 1: Tense captures past event description, Factuality filters out wishes or imagination, I and Dependency correspond to whether the described event involves the speaker. We built a linear SVM classifier using SVMlight package (Joachims, 1999). 5.2 Evaluation Feature Setting Acc Pre Rec Bigram+Window 0.76 0.47 0.44 Bigram+Window 0.77 0.47 0.46 +Tense+Factuality all 0.82 0.51 0.48 Table 5: Performance for self-report information identification regarding different feature settings. We report performance on the task of identifying self-reported information in this subsection. We employ 5-fold cross validation and report Accuracy (Accu), Prevision (Prec) and Recall (Rec) regarding different feature settings. The Tense, Factuality, I and Dependency features positively contribute to performance respectively and the best performance is ob</context>
</contexts>
<marker>Joachims, 1999</marker>
<rawString>Thorsten Joachims. 1999. Making large scale svm learning practical.</rawString>
</citation>
<citation valid="true">
<authors>
<author>Lingpeng Kong</author>
<author>Nathan Schneider</author>
<author>Swabha Swayamdipta</author>
<author>Archna Bhatia</author>
<author>Chris Dyer</author>
<author>Noah Smith</author>
</authors>
<title>A dependency parser for tweets.</title>
<date>2014</date>
<booktitle>In EMNLP.</booktitle>
<contexts>
<context position="23619" citStr="Kong et al., 2014" startWordPosition="3659" endWordPosition="3662">We use Stanford PragBank10, 8Most tweets in the bootstrapping output are positive. 9The majority of results returned by Twitter Search are negative examples. 10http://compprag.christopherpotts.net/ factbank.html 2002 an extension of FactBank (Saur´ı and Pustejovsky, 2009) which contains a list of modal words such as “might”, “will”, “want to” etc11. • I: Whether the subject of the tweet is first person singular. • Dependency: If the subject is first person singular and the u is a verb, the dependency path between the subject and u (or nondependency). Tweet dependency paths were obtained from (Kong et al., 2014). As the tweet parser we use only supports one-to-one dependency path identification but no dependency properties, Dependency is a binary feature. The subject of each tweet is determined by the dependency link to the root of the tweet from the parser. Among the features we explore, Word encodes the general information within the tweet. Window addresses the information around topic key word. The rest of the features specifically address each of the negative situations described in Challenge 2, Section 1: Tense captures past event description, Factuality filters out wishes or imagination, I and </context>
</contexts>
<marker>Kong, Schneider, Swayamdipta, Bhatia, Dyer, Smith, 2014</marker>
<rawString>Lingpeng Kong, Nathan Schneider, Swabha Swayamdipta, Archna Bhatia, Chris Dyer, and Noah Smith. 2014. A dependency parser for tweets. In EMNLP.</rawString>
</citation>
<citation valid="true">
<authors>
<author>Zornitsa Kozareva</author>
<author>Eduard Hovy</author>
</authors>
<title>Learning arguments and supertypes of semantic relations using recursive patterns.</title>
<date>2010</date>
<booktitle>In Proceedings of the 48th Annual Meeting of the Association for Computational Linguistics,</booktitle>
<pages>1482--1491</pages>
<publisher>Association</publisher>
<institution>for Computational Linguistics.</institution>
<contexts>
<context position="11601" citStr="Kozareva and Hovy, 2010" startWordPosition="1812" endWordPosition="1815">ove t from T: T = T − t 2. Run streaming LDA (Yao et al., 2009) on newly added tweets in D. 3. Manually Identify meaningful/trash topics, giving label to meaningful topics. 4. Add newly detected meaningful topic l to L. 5. For conversation t belonging to trash topics • remove t from D: D = D − t 6. Harvest more tweets based on topic distribution. 7. Manually identify top 20 responses to tweets harvested from Step 6. 8. Add meaningful responses to E. End Output: Identified topic list L. Tweet collection D. Figure 4: Bootstrapping Algorithm for Responsebased Life event identification. modeling (Kozareva and Hovy, 2010a; Roberts and Harabagiu, 2011). Conversation data was extracted from the CMU Twitter Warehouse of 2011 which contains a total number of 10% of all published tweets in that year. 3.3 Expanding dataset using Bootstrapping While our seed patterns for identifying messages expressing CONGRATULATIONS and CONDOLENCES are very high precision, they don’t cover all the possible ways these speech acts can be expressed. We therefore adopt a semisupervised bootstrapping approach to expand our reply seeds and event-related tweets. Our bootstrapping approach is related to previous work on semi-supervised in</context>
<context position="32583" citStr="Kozareva and Hovy, 2010" startWordPosition="5056" endWordPosition="5059"> (Becker et al., 2011) or disaster outbreak (Sakaki et al., 2010; Li and Cardie, 2013) by spotting the increase/decrease of word frequency. Some other approaches are focused on generating a structured representation of events (Ritter et al., 2012; Benson et al., 2011). Data Acquisition in Information Extraction Our work is also related with semi-supervised data harvesting approaches, the key idea of which is that some patterns are learned based on seeds. They are then used to find additional terms, which are subsequently used as new seeds in the patterns to search for additional new patterns (Kozareva and Hovy, 2010b; Davidov et al., 2007; Riloff et al., 1999; Igo and Riloff, 2009; Kozareva et al., 2008). Also related approaches are distant or weakly supervision (Mintz et al., 2009; Craven et al., 1999; Hoffmann et al., 2011) that rely on available structured data sources as a weak source of supervision for pattern extraction from related text corpora. 9 Conclusion and Discussion In this paper, we propose a pipelined system for major life event extraction from Twitter. Experimental results show that our model is able to extract a wide variety of major life events. The key strategy adopted in this work is</context>
</contexts>
<marker>Kozareva, Hovy, 2010</marker>
<rawString>Zornitsa Kozareva and Eduard Hovy. 2010a. Learning arguments and supertypes of semantic relations using recursive patterns. In Proceedings of the 48th Annual Meeting of the Association for Computational Linguistics, pages 1482–1491. Association for Computational Linguistics.</rawString>
</citation>
<citation valid="true">
<authors>
<author>Zornitsa Kozareva</author>
<author>Eduard Hovy</author>
</authors>
<title>Not all seeds are equal: Measuring the quality of text mining seeds.</title>
<date>2010</date>
<booktitle>In Human Language Technologies: The 2010 Annual Conference of the North American Chapter of the Association for Computational Linguistics,</booktitle>
<pages>618--626</pages>
<publisher>Association</publisher>
<institution>for Computational Linguistics.</institution>
<contexts>
<context position="11601" citStr="Kozareva and Hovy, 2010" startWordPosition="1812" endWordPosition="1815">ove t from T: T = T − t 2. Run streaming LDA (Yao et al., 2009) on newly added tweets in D. 3. Manually Identify meaningful/trash topics, giving label to meaningful topics. 4. Add newly detected meaningful topic l to L. 5. For conversation t belonging to trash topics • remove t from D: D = D − t 6. Harvest more tweets based on topic distribution. 7. Manually identify top 20 responses to tweets harvested from Step 6. 8. Add meaningful responses to E. End Output: Identified topic list L. Tweet collection D. Figure 4: Bootstrapping Algorithm for Responsebased Life event identification. modeling (Kozareva and Hovy, 2010a; Roberts and Harabagiu, 2011). Conversation data was extracted from the CMU Twitter Warehouse of 2011 which contains a total number of 10% of all published tweets in that year. 3.3 Expanding dataset using Bootstrapping While our seed patterns for identifying messages expressing CONGRATULATIONS and CONDOLENCES are very high precision, they don’t cover all the possible ways these speech acts can be expressed. We therefore adopt a semisupervised bootstrapping approach to expand our reply seeds and event-related tweets. Our bootstrapping approach is related to previous work on semi-supervised in</context>
<context position="32583" citStr="Kozareva and Hovy, 2010" startWordPosition="5056" endWordPosition="5059"> (Becker et al., 2011) or disaster outbreak (Sakaki et al., 2010; Li and Cardie, 2013) by spotting the increase/decrease of word frequency. Some other approaches are focused on generating a structured representation of events (Ritter et al., 2012; Benson et al., 2011). Data Acquisition in Information Extraction Our work is also related with semi-supervised data harvesting approaches, the key idea of which is that some patterns are learned based on seeds. They are then used to find additional terms, which are subsequently used as new seeds in the patterns to search for additional new patterns (Kozareva and Hovy, 2010b; Davidov et al., 2007; Riloff et al., 1999; Igo and Riloff, 2009; Kozareva et al., 2008). Also related approaches are distant or weakly supervision (Mintz et al., 2009; Craven et al., 1999; Hoffmann et al., 2011) that rely on available structured data sources as a weak source of supervision for pattern extraction from related text corpora. 9 Conclusion and Discussion In this paper, we propose a pipelined system for major life event extraction from Twitter. Experimental results show that our model is able to extract a wide variety of major life events. The key strategy adopted in this work is</context>
</contexts>
<marker>Kozareva, Hovy, 2010</marker>
<rawString>Zornitsa Kozareva and Eduard Hovy. 2010b. Not all seeds are equal: Measuring the quality of text mining seeds. In Human Language Technologies: The 2010 Annual Conference of the North American Chapter of the Association for Computational Linguistics, pages 618–626. Association for Computational Linguistics.</rawString>
</citation>
<citation valid="true">
<authors>
<author>Zornitsa Kozareva</author>
<author>Ellen Riloff</author>
<author>Eduard H Hovy</author>
</authors>
<title>Semantic class learning from the web with hyponym pattern linkage graphs.</title>
<date>2008</date>
<booktitle>In ACL,</booktitle>
<volume>8</volume>
<pages>1048--1056</pages>
<contexts>
<context position="32673" citStr="Kozareva et al., 2008" startWordPosition="5072" endWordPosition="5075">potting the increase/decrease of word frequency. Some other approaches are focused on generating a structured representation of events (Ritter et al., 2012; Benson et al., 2011). Data Acquisition in Information Extraction Our work is also related with semi-supervised data harvesting approaches, the key idea of which is that some patterns are learned based on seeds. They are then used to find additional terms, which are subsequently used as new seeds in the patterns to search for additional new patterns (Kozareva and Hovy, 2010b; Davidov et al., 2007; Riloff et al., 1999; Igo and Riloff, 2009; Kozareva et al., 2008). Also related approaches are distant or weakly supervision (Mintz et al., 2009; Craven et al., 1999; Hoffmann et al., 2011) that rely on available structured data sources as a weak source of supervision for pattern extraction from related text corpora. 9 Conclusion and Discussion In this paper, we propose a pipelined system for major life event extraction from Twitter. Experimental results show that our model is able to extract a wide variety of major life events. The key strategy adopted in this work is to obtain a relatively clean training dataset from large quantity of Twitter data by rely</context>
</contexts>
<marker>Kozareva, Riloff, Hovy, 2008</marker>
<rawString>Zornitsa Kozareva, Ellen Riloff, and Eduard H Hovy. 2008. Semantic class learning from the web with hyponym pattern linkage graphs. In ACL, volume 8, pages 1048–1056.</rawString>
</citation>
<citation valid="true">
<authors>
<author>John Lafferty</author>
<author>Andrew McCallum</author>
<author>Fernando CN Pereira</author>
</authors>
<title>Conditional random fields: Probabilistic models for segmenting and labeling sequence data.</title>
<date>2001</date>
<contexts>
<context position="26011" citStr="Lafferty et al., 2001" startWordPosition="4028" endWordPosition="4031">the event, for example the name of the university the speaker was admitted to (See Figure 1). In this section we take a supervised approach to event property extraction, based on manually annotated data for a handfull of the major life event categories automatically identified by our system. While this approach is unlikely to scale to the diversity of important personal events Twitter users are discussing, our experiments demonstrate that event property extraction is indeed feasible. We cast the problem of event property extraction as a sequence labeling task, using Conditional Random Fields (Lafferty et al., 2001) for learning and inference. To make best use of the labeled data, we trained a unified CRF model for closely related event categories which often share properties; the full list is presented in Table 6 and we labeled 300 tweets in total. Features we used include: • word token, capitalization, POS • left and right context words within a window of 3 and the correspondent part-of-speech tags • word shape, NER • a gazetteer of universities and employers borrowed from NELL12. We use 5-fold cross-validation and report results in Table 7. 7 End-to-End Experiment The evaluation for each part of our s</context>
</contexts>
<marker>Lafferty, McCallum, Pereira, 2001</marker>
<rawString>John Lafferty, Andrew McCallum, and Fernando CN Pereira. 2001. Conditional random fields: Probabilistic models for segmenting and labeling sequence data.</rawString>
</citation>
<citation valid="true">
<authors>
<author>Jiwei Li</author>
<author>Claire Cardie</author>
</authors>
<title>Early stage influenza detection from twitter. arXiv preprint arXiv:1309.7340.</title>
<date>2013</date>
<contexts>
<context position="32046" citStr="Li and Cardie, 2013" startWordPosition="4971" endWordPosition="4974">h, which helps them avoids the nuisance of explicit definition for life event characteristics and acquisition of labeled data. However, their system has the short-coming that each personal topic needs to be adequately discussed by the user and their followers in order to be detected16. Public Event Extraction from Twitter Twitter serves as a good source for event detection owing to its real time nature and large number of users. These approaches include identifying bursty public topics (e.g.,(Diao et al., 2012)), topic evolution (Becker et al., 2011) or disaster outbreak (Sakaki et al., 2010; Li and Cardie, 2013) by spotting the increase/decrease of word frequency. Some other approaches are focused on generating a structured representation of events (Ritter et al., 2012; Benson et al., 2011). Data Acquisition in Information Extraction Our work is also related with semi-supervised data harvesting approaches, the key idea of which is that some patterns are learned based on seeds. They are then used to find additional terms, which are subsequently used as new seeds in the patterns to search for additional new patterns (Kozareva and Hovy, 2010b; Davidov et al., 2007; Riloff et al., 1999; Igo and Riloff, 2</context>
</contexts>
<marker>Li, Cardie, 2013</marker>
<rawString>Jiwei Li and Claire Cardie. 2013. Early stage influenza detection from twitter. arXiv preprint arXiv:1309.7340.</rawString>
</citation>
<citation valid="true">
<authors>
<author>Jiwei Li</author>
<author>Claire Cardie</author>
</authors>
<title>Timeline generation: Tracking individuals on twitter.</title>
<date>2014</date>
<publisher>WWW,</publisher>
<marker>Li, Cardie, 2014</marker>
<rawString>Jiwei Li and Claire Cardie. 2014. Timeline generation: Tracking individuals on twitter. WWW, 2014.</rawString>
</citation>
<citation valid="true">
<authors>
<author>Jiwei Li</author>
<author>Alan Ritter</author>
<author>Eduard Hovy</author>
</authors>
<title>Weakly supervised user profile extraction from twitter.</title>
<date>2014</date>
<publisher>ACL.</publisher>
<contexts>
<context position="30869" citStr="Li et al., 2014" startWordPosition="4783" endWordPosition="4786">lic event extraction on social media. (3) Data harvesting in Information Extraction, each of which contains large amount of related work, to which we can not do fully justice. User Information Extraction from Twitter Some early approaches towards understanding user level information on social media is focused on user profile/attribute prediction (e.g.,(Ciot et al., 2013)) user-specific content extraction (Diao 15which are 24, 38, 42-class classifiers, where 24, 38, 42 denoted the number of topics discovered in each step of bootstrapping (see Figure 5). 2004 et al., 2012; Diao and Jiang, 2013; Li et al., 2014) or user personalization (Low et al., 2011) identification. The problem of user life event extraction was first studied by Li and Cardie’s (2014). They attempted to construct a chronological timeline for Twitter users from their published tweets based on two criterion: a personal event should be personal and time-specific. Their system does not explicitly identify a global category of life events (and tweets discussing correspondent event) but identifies the topics/events that are personal and timespecific to a given user using an unsupervised approach, which helps them avoids the nuisance of </context>
</contexts>
<marker>Li, Ritter, Hovy, 2014</marker>
<rawString>Jiwei Li, Alan Ritter, and Eduard Hovy. 2014. Weakly supervised user profile extraction from twitter. ACL.</rawString>
</citation>
<citation valid="true">
<authors>
<author>Yucheng Low</author>
<author>Deepak Agarwal</author>
<author>Alexander J Smola</author>
</authors>
<title>Multiple domain user personalization.</title>
<date>2011</date>
<booktitle>In Proceedings of the 17th ACM SIGKDD international conference on Knowledge discovery and data mining,</booktitle>
<pages>123--131</pages>
<publisher>ACM.</publisher>
<contexts>
<context position="30912" citStr="Low et al., 2011" startWordPosition="4790" endWordPosition="4793">Data harvesting in Information Extraction, each of which contains large amount of related work, to which we can not do fully justice. User Information Extraction from Twitter Some early approaches towards understanding user level information on social media is focused on user profile/attribute prediction (e.g.,(Ciot et al., 2013)) user-specific content extraction (Diao 15which are 24, 38, 42-class classifiers, where 24, 38, 42 denoted the number of topics discovered in each step of bootstrapping (see Figure 5). 2004 et al., 2012; Diao and Jiang, 2013; Li et al., 2014) or user personalization (Low et al., 2011) identification. The problem of user life event extraction was first studied by Li and Cardie’s (2014). They attempted to construct a chronological timeline for Twitter users from their published tweets based on two criterion: a personal event should be personal and time-specific. Their system does not explicitly identify a global category of life events (and tweets discussing correspondent event) but identifies the topics/events that are personal and timespecific to a given user using an unsupervised approach, which helps them avoids the nuisance of explicit definition for life event characte</context>
</contexts>
<marker>Low, Agarwal, Smola, 2011</marker>
<rawString>Yucheng Low, Deepak Agarwal, and Alexander J Smola. 2011. Multiple domain user personalization. In Proceedings of the 17th ACM SIGKDD international conference on Knowledge discovery and data mining, pages 123–131. ACM.</rawString>
</citation>
<citation valid="true">
<authors>
<author>David Mimno</author>
<author>Hanna M Wallach</author>
<author>Edmund Talley</author>
<author>Miriam Leenders</author>
<author>Andrew McCallum</author>
</authors>
<title>Optimizing semantic coherence in topic models.</title>
<date>2011</date>
<booktitle>In Proceedings of the Conference on Empirical Methods in Natural Language Processing,</booktitle>
<pages>262--272</pages>
<publisher>Association</publisher>
<institution>for Computational Linguistics.</institution>
<contexts>
<context position="10575" citStr="Mimno et al., 2011" startWordPosition="1627" endWordPosition="1630">inspired by (Ritter et al., 2012) that uses a LDA-CLUSTERING+HUMAN-IDENTIFICATION strategy to identify public events from Twitter. Similar strategies have been widely used in unsupervised information extraction (Bejan et al., 2009; Yao et al., 2011) and selectional preference 3Each whole conversation usually contains multiple tweets and users. 4While we applied manual labeling and coherence evaluation in this work, an interesting direction for future work is automatically labeling major life event categories following previous work on labeling topics in traditional documentbased topic models (Mimno et al., 2011; Newman et al., 2010). 1999 Figure 3: Illustration of bootstrapping process. of bootstrapping. Figure 5: Illustration of data retrieved in each step Input: Reply seed list E = {e}, Tweet conversation collection T = {t}, Retrieved Tweets Collection D = φ. Identified topic list L=φ Begin While not stopping: 1. For unprocessed conversation t E T if t contains reply e E E, • add t to D: D = D + t. • remove t from T: T = T − t 2. Run streaming LDA (Yao et al., 2009) on newly added tweets in D. 3. Manually Identify meaningful/trash topics, giving label to meaningful topics. 4. Add newly detected me</context>
</contexts>
<marker>Mimno, Wallach, Talley, Leenders, McCallum, 2011</marker>
<rawString>David Mimno, Hanna M Wallach, Edmund Talley, Miriam Leenders, and Andrew McCallum. 2011. Optimizing semantic coherence in topic models. In Proceedings of the Conference on Empirical Methods in Natural Language Processing, pages 262– 272. Association for Computational Linguistics.</rawString>
</citation>
<citation valid="true">
<authors>
<author>Mike Mintz</author>
<author>Steven Bills</author>
<author>Rion Snow</author>
<author>Dan Jurafsky</author>
</authors>
<title>Distant supervision for relation extraction without labeled data.</title>
<date>2009</date>
<booktitle>In Proceedings of the Joint Conference of the 47th Annual Meeting of the ACL and the 4th International Joint Conference on Natural Language Processing of the AFNLP: Volume</booktitle>
<volume>2</volume>
<pages>1003--1011</pages>
<publisher>Association</publisher>
<institution>for Computational Linguistics.</institution>
<contexts>
<context position="32752" citStr="Mintz et al., 2009" startWordPosition="5084" endWordPosition="5087">on generating a structured representation of events (Ritter et al., 2012; Benson et al., 2011). Data Acquisition in Information Extraction Our work is also related with semi-supervised data harvesting approaches, the key idea of which is that some patterns are learned based on seeds. They are then used to find additional terms, which are subsequently used as new seeds in the patterns to search for additional new patterns (Kozareva and Hovy, 2010b; Davidov et al., 2007; Riloff et al., 1999; Igo and Riloff, 2009; Kozareva et al., 2008). Also related approaches are distant or weakly supervision (Mintz et al., 2009; Craven et al., 1999; Hoffmann et al., 2011) that rely on available structured data sources as a weak source of supervision for pattern extraction from related text corpora. 9 Conclusion and Discussion In this paper, we propose a pipelined system for major life event extraction from Twitter. Experimental results show that our model is able to extract a wide variety of major life events. The key strategy adopted in this work is to obtain a relatively clean training dataset from large quantity of Twitter data by relying on minimum efforts of human supervision, and sometimes is at the sacrifice </context>
</contexts>
<marker>Mintz, Bills, Snow, Jurafsky, 2009</marker>
<rawString>Mike Mintz, Steven Bills, Rion Snow, and Dan Jurafsky. 2009. Distant supervision for relation extraction without labeled data. In Proceedings of the Joint Conference of the 47th Annual Meeting of the ACL and the 4th International Joint Conference on Natural Language Processing of the AFNLP: Volume 2-Volume 2, pages 1003–1011. Association for Computational Linguistics.</rawString>
</citation>
<citation valid="true">
<authors>
<author>David Newman</author>
<author>Jey Han Lau</author>
<author>Karl Grieser</author>
<author>Timothy Baldwin</author>
</authors>
<title>Automatic evaluation of topic coherence.</title>
<date>2010</date>
<booktitle>In Human Language Technologies: The 2010 Annual Conference of the North American Chapter of the Association for Computational Linguistics,</booktitle>
<pages>100--108</pages>
<publisher>Association</publisher>
<institution>for Computational Linguistics.</institution>
<contexts>
<context position="10597" citStr="Newman et al., 2010" startWordPosition="1631" endWordPosition="1634">et al., 2012) that uses a LDA-CLUSTERING+HUMAN-IDENTIFICATION strategy to identify public events from Twitter. Similar strategies have been widely used in unsupervised information extraction (Bejan et al., 2009; Yao et al., 2011) and selectional preference 3Each whole conversation usually contains multiple tweets and users. 4While we applied manual labeling and coherence evaluation in this work, an interesting direction for future work is automatically labeling major life event categories following previous work on labeling topics in traditional documentbased topic models (Mimno et al., 2011; Newman et al., 2010). 1999 Figure 3: Illustration of bootstrapping process. of bootstrapping. Figure 5: Illustration of data retrieved in each step Input: Reply seed list E = {e}, Tweet conversation collection T = {t}, Retrieved Tweets Collection D = φ. Identified topic list L=φ Begin While not stopping: 1. For unprocessed conversation t E T if t contains reply e E E, • add t to D: D = D + t. • remove t from T: T = T − t 2. Run streaming LDA (Yao et al., 2009) on newly added tweets in D. 3. Manually Identify meaningful/trash topics, giving label to meaningful topics. 4. Add newly detected meaningful topic l to L.</context>
</contexts>
<marker>Newman, Lau, Grieser, Baldwin, 2010</marker>
<rawString>David Newman, Jey Han Lau, Karl Grieser, and Timothy Baldwin. 2010. Automatic evaluation of topic coherence. In Human Language Technologies: The 2010 Annual Conference of the North American Chapter of the Association for Computational Linguistics, pages 100–108. Association for Computational Linguistics.</rawString>
</citation>
<citation valid="true">
<authors>
<author>Olutobi Owoputi</author>
<author>Brendan O’Connor</author>
<author>Chris Dyer</author>
<author>Kevin Gimpel</author>
<author>Nathan Schneider</author>
<author>Noah A Smith</author>
</authors>
<title>Improved part-of-speech tagging for online conversational text with word clusters.</title>
<date>2013</date>
<booktitle>In Proceedings of NAACL-HLT,</booktitle>
<pages>380--390</pages>
<marker>Owoputi, O’Connor, Dyer, Gimpel, Schneider, Smith, 2013</marker>
<rawString>Olutobi Owoputi, Brendan O’Connor, Chris Dyer, Kevin Gimpel, Nathan Schneider, and Noah A Smith. 2013. Improved part-of-speech tagging for online conversational text with word clusters. In Proceedings of NAACL-HLT, pages 380–390.</rawString>
</citation>
<citation valid="true">
<authors>
<author>Ellen Riloff</author>
<author>Rosie Jones</author>
</authors>
<title>Learning dictionaries for information extraction by multi-level bootstrapping.</title>
<date>1999</date>
<booktitle>In AAAI/IAAI,</booktitle>
<pages>474--479</pages>
<marker>Riloff, Jones, 1999</marker>
<rawString>Ellen Riloff, Rosie Jones, et al. 1999. Learning dictionaries for information extraction by multi-level bootstrapping. In AAAI/IAAI, pages 474–479.</rawString>
</citation>
<citation valid="true">
<authors>
<author>Alan Ritter</author>
<author>Colin Cherry</author>
<author>Bill Dolan</author>
</authors>
<title>Unsupervised modeling of twitter conversations.</title>
<date>2010</date>
<contexts>
<context position="9489" citStr="Ritter et al., 2010" startWordPosition="1463" endWordPosition="1466"> and CONDOLENCES, including the phrases: ”Congratulations”, ”Congrats”, ”Sorry to hear that”, ”Awesome”, and gather tweets that were observed with seed responses. Next, an LDA (Blei et al., 2003)2 based topic model is used to cluster the gathered 2Topic Number is set to 120. tweets to automatically identify important categories of major life events in an unsupervised way. In our approach, we model the whole conversation dialogue as a document3 with the response seeds (e.g., congratulation) masked out. We furthermore associate each sentence with a single topic, following strategies adopted by (Ritter et al., 2010; Gruber et al., 2007). We limit the words in our document collection to verbs and nouns which we found to lead to clearer topic representations, and used collapsed Gibbs Sampling for inference (Griffiths and Steyvers, 2004). Next one of the authors manually inspected the resulting major life event types inferred by the model, and manually assigned them labels such as ”getting a job”, ”graduation” or ”marriage” and discarded incoherent topics4. Our methodology is inspired by (Ritter et al., 2012) that uses a LDA-CLUSTERING+HUMAN-IDENTIFICATION strategy to identify public events from Twitter. S</context>
</contexts>
<marker>Ritter, Cherry, Dolan, 2010</marker>
<rawString>Alan Ritter, Colin Cherry, and Bill Dolan. 2010. Unsupervised modeling of twitter conversations.</rawString>
</citation>
<citation valid="true">
<authors>
<author>Alan Ritter</author>
<author>Sam Clark</author>
<author>Oren Etzioni</author>
</authors>
<title>Named entity recognition in tweets: an experimental study.</title>
<date>2011</date>
<booktitle>In Proceedings of the Conference on Empirical Methods in Natural Language Processing,</booktitle>
<pages>1524--1534</pages>
<publisher>Association</publisher>
<institution>for Computational Linguistics.</institution>
<contexts>
<context position="19494" citStr="Ritter et al., 2011" startWordPosition="3024" endWordPosition="3027">g 0.22 OTHER 15.31 2001 event examples and trained a 43-class maximum entropy classifier based on the following features: • Word: The sequence of words in the tweet. • NER: Named entity Tag. • Dictionary: Word matching a dictionaries of the top 40 words for each life event category (automatically inferred by the topic model). The feature value is the term’s probability generated by correspondent event. • Window: If a dictionary term exists, left and right context words within a window of 3 words and their part-of-speech tags. Name entity tag is assigned from Ritter et al’s Twitter NER system (Ritter et al., 2011). Part-ofSpeech tags are assigned based on Twitter POS package (Owoputi et al., 2013) developed by CMU ARK Lab. Dictionary and Window are constructed based on the topic-term distribution obtained from the previous section. The average precision and recall are shown in Table 4. And as we can observe, the dictionary (with probability) contributes a lot to the performance and by taking into account a more comprehensive set of information around the key word, classifier on All feature setting generate significantly better performance, with 0.382 prevision and 0.48 recall, which is acceptable consi</context>
</contexts>
<marker>Ritter, Clark, Etzioni, 2011</marker>
<rawString>Alan Ritter, Sam Clark, Oren Etzioni, et al. 2011. Named entity recognition in tweets: an experimental study. In Proceedings of the Conference on Empirical Methods in Natural Language Processing, pages 1524–1534. Association for Computational Linguistics.</rawString>
</citation>
<citation valid="true">
<authors>
<author>Alan Ritter</author>
<author>Oren Etzioni</author>
<author>Sam Clark</author>
</authors>
<title>Open domain event extraction from twitter.</title>
<date>2012</date>
<booktitle>In Proceedings of the 18th ACM SIGKDD international conference on Knowledge discovery and data mining,</booktitle>
<pages>1104--1112</pages>
<publisher>ACM.</publisher>
<contexts>
<context position="1833" citStr="Ritter et al., 2012" startWordPosition="260" endWordPosition="264">following and also facilitate many downstream applications, for example realtime friend recommendation. 1 Introduction Social networking websites such as Facebook and Twitter have recently challenged mainstream media as the freshest source of information on important news events. In addition to an important source for breaking news, social media presents a unique source of information on private events, for example a friend’s engagement or college graduation (examples are presented in Figure 1). While a significant amount of previous work has investigated event extraction from Twitter (e.g., (Ritter et al., 2012; Diao et al., 2012)), existing approaches mostly focus on public bursty event extraction, and little progress has been made towards the problem of automatically extracting the major life events of ordinary users. A system which can automatically extract major life events and generate fine-grained descriptions as in Figure 1 will not only help Twitter users with the problem of information overload by summarizing important events taking place in their friends lives, but could also facilitate downstream applications such as friend recommendation (e.g., friend recommendation in realtime to people</context>
<context position="9990" citStr="Ritter et al., 2012" startWordPosition="1543" endWordPosition="1546">d out. We furthermore associate each sentence with a single topic, following strategies adopted by (Ritter et al., 2010; Gruber et al., 2007). We limit the words in our document collection to verbs and nouns which we found to lead to clearer topic representations, and used collapsed Gibbs Sampling for inference (Griffiths and Steyvers, 2004). Next one of the authors manually inspected the resulting major life event types inferred by the model, and manually assigned them labels such as ”getting a job”, ”graduation” or ”marriage” and discarded incoherent topics4. Our methodology is inspired by (Ritter et al., 2012) that uses a LDA-CLUSTERING+HUMAN-IDENTIFICATION strategy to identify public events from Twitter. Similar strategies have been widely used in unsupervised information extraction (Bejan et al., 2009; Yao et al., 2011) and selectional preference 3Each whole conversation usually contains multiple tweets and users. 4While we applied manual labeling and coherence evaluation in this work, an interesting direction for future work is automatically labeling major life event categories following previous work on labeling topics in traditional documentbased topic models (Mimno et al., 2011; Newman et al.</context>
<context position="32206" citStr="Ritter et al., 2012" startWordPosition="4994" endWordPosition="4997">t-coming that each personal topic needs to be adequately discussed by the user and their followers in order to be detected16. Public Event Extraction from Twitter Twitter serves as a good source for event detection owing to its real time nature and large number of users. These approaches include identifying bursty public topics (e.g.,(Diao et al., 2012)), topic evolution (Becker et al., 2011) or disaster outbreak (Sakaki et al., 2010; Li and Cardie, 2013) by spotting the increase/decrease of word frequency. Some other approaches are focused on generating a structured representation of events (Ritter et al., 2012; Benson et al., 2011). Data Acquisition in Information Extraction Our work is also related with semi-supervised data harvesting approaches, the key idea of which is that some patterns are learned based on seeds. They are then used to find additional terms, which are subsequently used as new seeds in the patterns to search for additional new patterns (Kozareva and Hovy, 2010b; Davidov et al., 2007; Riloff et al., 1999; Igo and Riloff, 2009; Kozareva et al., 2008). Also related approaches are distant or weakly supervision (Mintz et al., 2009; Craven et al., 1999; Hoffmann et al., 2011) that rel</context>
</contexts>
<marker>Ritter, Etzioni, Clark, 2012</marker>
<rawString>Alan Ritter, Oren Etzioni, Sam Clark, et al. 2012. Open domain event extraction from twitter. In Proceedings of the 18th ACM SIGKDD international conference on Knowledge discovery and data mining, pages 1104–1112. ACM.</rawString>
</citation>
<citation valid="true">
<authors>
<author>Kirk Roberts</author>
<author>Sanda M Harabagiu</author>
</authors>
<title>Unsupervised learning of selectional restrictions and detection of argument coercions.</title>
<date>2011</date>
<booktitle>In Proceedings of the Conference on Empirical Methods in Natural Language Processing,</booktitle>
<pages>980--990</pages>
<publisher>Association</publisher>
<institution>for Computational Linguistics.</institution>
<contexts>
<context position="11632" citStr="Roberts and Harabagiu, 2011" startWordPosition="1816" endWordPosition="1819"> Run streaming LDA (Yao et al., 2009) on newly added tweets in D. 3. Manually Identify meaningful/trash topics, giving label to meaningful topics. 4. Add newly detected meaningful topic l to L. 5. For conversation t belonging to trash topics • remove t from D: D = D − t 6. Harvest more tweets based on topic distribution. 7. Manually identify top 20 responses to tweets harvested from Step 6. 8. Add meaningful responses to E. End Output: Identified topic list L. Tweet collection D. Figure 4: Bootstrapping Algorithm for Responsebased Life event identification. modeling (Kozareva and Hovy, 2010a; Roberts and Harabagiu, 2011). Conversation data was extracted from the CMU Twitter Warehouse of 2011 which contains a total number of 10% of all published tweets in that year. 3.3 Expanding dataset using Bootstrapping While our seed patterns for identifying messages expressing CONGRATULATIONS and CONDOLENCES are very high precision, they don’t cover all the possible ways these speech acts can be expressed. We therefore adopt a semisupervised bootstrapping approach to expand our reply seeds and event-related tweets. Our bootstrapping approach is related to previous work on semi-supervised information harvesting (e.g., (Ko</context>
</contexts>
<marker>Roberts, Harabagiu, 2011</marker>
<rawString>Kirk Roberts and Sanda M Harabagiu. 2011. Unsupervised learning of selectional restrictions and detection of argument coercions. In Proceedings of the Conference on Empirical Methods in Natural Language Processing, pages 980–990. Association for Computational Linguistics.</rawString>
</citation>
<citation valid="true">
<authors>
<author>Takeshi Sakaki</author>
<author>Makoto Okazaki</author>
<author>Yutaka Matsuo</author>
</authors>
<title>Earthquake shakes twitter users: real-time event detection by social sensors.</title>
<date>2010</date>
<booktitle>In Proceedings of the 19th international conference on World wide web,</booktitle>
<pages>851--860</pages>
<publisher>ACM.</publisher>
<contexts>
<context position="32024" citStr="Sakaki et al., 2010" startWordPosition="4967" endWordPosition="4970"> unsupervised approach, which helps them avoids the nuisance of explicit definition for life event characteristics and acquisition of labeled data. However, their system has the short-coming that each personal topic needs to be adequately discussed by the user and their followers in order to be detected16. Public Event Extraction from Twitter Twitter serves as a good source for event detection owing to its real time nature and large number of users. These approaches include identifying bursty public topics (e.g.,(Diao et al., 2012)), topic evolution (Becker et al., 2011) or disaster outbreak (Sakaki et al., 2010; Li and Cardie, 2013) by spotting the increase/decrease of word frequency. Some other approaches are focused on generating a structured representation of events (Ritter et al., 2012; Benson et al., 2011). Data Acquisition in Information Extraction Our work is also related with semi-supervised data harvesting approaches, the key idea of which is that some patterns are learned based on seeds. They are then used to find additional terms, which are subsequently used as new seeds in the patterns to search for additional new patterns (Kozareva and Hovy, 2010b; Davidov et al., 2007; Riloff et al., 1</context>
</contexts>
<marker>Sakaki, Okazaki, Matsuo, 2010</marker>
<rawString>Takeshi Sakaki, Makoto Okazaki, and Yutaka Matsuo. 2010. Earthquake shakes twitter users: real-time event detection by social sensors. In Proceedings of the 19th international conference on World wide web, pages 851–860. ACM.</rawString>
</citation>
<citation valid="true">
<authors>
<author>Roser Sauriand James Pustejovsky</author>
</authors>
<title>Determining modality and factuality for text entailment.</title>
<date>2007</date>
<booktitle>In Semantic Computing,</booktitle>
<pages>509--516</pages>
<publisher>IEEE.</publisher>
<contexts>
<context position="22999" citStr="Pustejovsky, 2007" startWordPosition="3562" endWordPosition="3564">. Let u denote the term within the tweet that gets the highest possibility generated by the correspondent topic. We experimented with combinations of the following types of features (results are presented in Table ??): • Bigram: Bigrams within each tweet (punctuation included). • Window: A window of k E {0, 1, 2} words adjacent to u and their part-of-speech tags. • Tense: A binary feature indicating past tense identified in by the presence of past tense verb (VBD). • Factuality: Factuality denotes whether one expression is presented as corresponding to real situations in the world (Saur´ı and Pustejovsky, 2007). We use Stanford PragBank10, 8Most tweets in the bootstrapping output are positive. 9The majority of results returned by Twitter Search are negative examples. 10http://compprag.christopherpotts.net/ factbank.html 2002 an extension of FactBank (Saur´ı and Pustejovsky, 2009) which contains a list of modal words such as “might”, “will”, “want to” etc11. • I: Whether the subject of the tweet is first person singular. • Dependency: If the subject is first person singular and the u is a verb, the dependency path between the subject and u (or nondependency). Tweet dependency paths were obtained from</context>
</contexts>
<marker>Pustejovsky, 2007</marker>
<rawString>Roser Sauriand James Pustejovsky. 2007. Determining modality and factuality for text entailment. In Semantic Computing, 2007. ICSC 2007. International Conference on, pages 509–516. IEEE.</rawString>
</citation>
<citation valid="true">
<authors>
<author>Roser Saur´ı</author>
<author>James Pustejovsky</author>
</authors>
<title>Factbank: A corpus annotated with event factuality. Language resources and evaluation,</title>
<date>2009</date>
<pages>43--3</pages>
<marker>Saur´ı, Pustejovsky, 2009</marker>
<rawString>Roser Saur´ı and James Pustejovsky. 2009. Factbank: A corpus annotated with event factuality. Language resources and evaluation, 43(3):227–268.</rawString>
</citation>
<citation valid="true">
<authors>
<author>Rion Snow</author>
<author>Brendan O’Connor</author>
<author>Daniel Jurafsky</author>
<author>Andrew Y Ng</author>
</authors>
<title>Cheap and fast—but is it good?: evaluating non-expert annotations for natural language tasks.</title>
<date>2008</date>
<booktitle>In Proceedings of the conference on empirical methods in natural language processing,</booktitle>
<pages>254--263</pages>
<publisher>Association</publisher>
<institution>for Computational Linguistics.</institution>
<marker>Snow, O’Connor, Jurafsky, Ng, 2008</marker>
<rawString>Rion Snow, Brendan O’Connor, Daniel Jurafsky, and Andrew Y Ng. 2008. Cheap and fast—but is it good?: evaluating non-expert annotations for natural language tasks. In Proceedings of the conference on empirical methods in natural language processing, pages 254–263. Association for Computational Linguistics.</rawString>
</citation>
<citation valid="true">
<authors>
<author>Limin Yao</author>
<author>David Mimno</author>
<author>Andrew McCallum</author>
</authors>
<title>Efficient methods for topic model inference on streaming document collections.</title>
<date>2009</date>
<contexts>
<context position="11041" citStr="Yao et al., 2009" startWordPosition="1719" endWordPosition="1722">matically labeling major life event categories following previous work on labeling topics in traditional documentbased topic models (Mimno et al., 2011; Newman et al., 2010). 1999 Figure 3: Illustration of bootstrapping process. of bootstrapping. Figure 5: Illustration of data retrieved in each step Input: Reply seed list E = {e}, Tweet conversation collection T = {t}, Retrieved Tweets Collection D = φ. Identified topic list L=φ Begin While not stopping: 1. For unprocessed conversation t E T if t contains reply e E E, • add t to D: D = D + t. • remove t from T: T = T − t 2. Run streaming LDA (Yao et al., 2009) on newly added tweets in D. 3. Manually Identify meaningful/trash topics, giving label to meaningful topics. 4. Add newly detected meaningful topic l to L. 5. For conversation t belonging to trash topics • remove t from D: D = D − t 6. Harvest more tweets based on topic distribution. 7. Manually identify top 20 responses to tweets harvested from Step 6. 8. Add meaningful responses to E. End Output: Identified topic list L. Tweet collection D. Figure 4: Bootstrapping Algorithm for Responsebased Life event identification. modeling (Kozareva and Hovy, 2010a; Roberts and Harabagiu, 2011). Convers</context>
<context position="12398" citStr="Yao et al., 2009" startWordPosition="1934" endWordPosition="1937">.3 Expanding dataset using Bootstrapping While our seed patterns for identifying messages expressing CONGRATULATIONS and CONDOLENCES are very high precision, they don’t cover all the possible ways these speech acts can be expressed. We therefore adopt a semisupervised bootstrapping approach to expand our reply seeds and event-related tweets. Our bootstrapping approach is related to previous work on semi-supervised information harvesting (e.g., (Kozareva and Hovy, 2010b; Davidov et al., 2007)). To preserve the labeled topics from the first iteration, we apply a streaming approach to inference (Yao et al., 2009) over unlabeled tweets (those which did not match one of the response congratulations (cong, congrats); (that’s) fantastic; (so) cool; (I’m) (very) sorry to hear that; (that’s) great (good) new; awesome; what a pity; have fun; great; that sucks; too bad; (that’s) unfortunate; how sad; fabulous; (that’s) terrific; (that’s) (so) wonderful; my deepest condolences; Table 1: Responses retrieved from Bootstrapping. seeds). We collect responses to the newly added tweets, then select the top 20 frequent replies5. Next we manually inspect and filter the top ranked replies, and use them to harvest more </context>
</contexts>
<marker>Yao, Mimno, McCallum, 2009</marker>
<rawString>Limin Yao, David Mimno, and Andrew McCallum. 2009. Efficient methods for topic model inference on streaming document collections.</rawString>
</citation>
<citation valid="true">
<authors>
<author>Limin Yao</author>
<author>Aria Haghighi</author>
<author>Sebastian Riedel</author>
<author>Andrew McCallum</author>
</authors>
<title>Structured relation discovery using generative models.</title>
<date>2011</date>
<booktitle>In Proceedings of the Conference on Empirical Methods in Natural Language Processing,</booktitle>
<pages>1456--1466</pages>
<publisher>Association</publisher>
<institution>for Computational Linguistics.</institution>
<contexts>
<context position="10206" citStr="Yao et al., 2011" startWordPosition="1573" endWordPosition="1576">und to lead to clearer topic representations, and used collapsed Gibbs Sampling for inference (Griffiths and Steyvers, 2004). Next one of the authors manually inspected the resulting major life event types inferred by the model, and manually assigned them labels such as ”getting a job”, ”graduation” or ”marriage” and discarded incoherent topics4. Our methodology is inspired by (Ritter et al., 2012) that uses a LDA-CLUSTERING+HUMAN-IDENTIFICATION strategy to identify public events from Twitter. Similar strategies have been widely used in unsupervised information extraction (Bejan et al., 2009; Yao et al., 2011) and selectional preference 3Each whole conversation usually contains multiple tweets and users. 4While we applied manual labeling and coherence evaluation in this work, an interesting direction for future work is automatically labeling major life event categories following previous work on labeling topics in traditional documentbased topic models (Mimno et al., 2011; Newman et al., 2010). 1999 Figure 3: Illustration of bootstrapping process. of bootstrapping. Figure 5: Illustration of data retrieved in each step Input: Reply seed list E = {e}, Tweet conversation collection T = {t}, Retrieved </context>
</contexts>
<marker>Yao, Haghighi, Riedel, McCallum, 2011</marker>
<rawString>Limin Yao, Aria Haghighi, Sebastian Riedel, and Andrew McCallum. 2011. Structured relation discovery using generative models. In Proceedings of the Conference on Empirical Methods in Natural Language Processing, pages 1456–1466. Association for Computational Linguistics.</rawString>
</citation>
</citationList>
</algorithm>
</algorithms>
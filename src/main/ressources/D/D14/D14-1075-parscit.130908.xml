<?xml version="1.0" encoding="UTF-8"?>
<algorithms version="110505">
<algorithm name="SectLabel" version="110505">
<variant no="0" confidence="0.000004">
<title confidence="0.995551">
Fear the REAPER: A System for Automatic Multi-Document
Summarization with Reinforcement Learning
</title>
<author confidence="0.975038">
Cody Rioux Sadid A. Hasan Yllias Chali
</author>
<affiliation confidence="0.975221">
University of Lethbridge Philips Research North America University of Lethbridge
</affiliation>
<note confidence="0.428359">
Lethbridge, AB, Canada Briarcliff Manor, NY, USA Lethbridge, AB, Canada
</note>
<email confidence="0.923371">
cody.rioux@uleth.ca sadid.hasan@philips.com chali@cs.uleth.ca
</email>
<sectionHeader confidence="0.98171" genericHeader="abstract">
Abstract
</sectionHeader>
<bodyText confidence="0.999777884615385">
This paper explores alternate algorithms,
reward functions and feature sets for per-
forming multi-document summarization
using reinforcement learning with a high
focus on reproducibility. We show that
ROUGE results can be improved using
a unigram and bigram similarity metric
when training a learner to select sentences
for summarization. Learners are trained
to summarize document clusters based on
various algorithms and reward functions
and then evaluated using ROUGE. Our ex-
periments show a statistically significant
improvement of 1.33%, 1.58%, and 2.25%
for ROUGE-1, ROUGE-2 and ROUGE-
L scores, respectively, when compared
with the performance of the state of the
art in automatic summarization with re-
inforcement learning on the DUC2004
dataset. Furthermore query focused exten-
sions of our approach show an improve-
ment of 1.37% and 2.31% for ROUGE-2
and ROUGE-SU4 respectively over query
focused extensions of the state of the
art with reinforcement learning on the
DUC2006 dataset.
</bodyText>
<sectionHeader confidence="0.992333" genericHeader="keywords">
1 Introduction
</sectionHeader>
<bodyText confidence="0.999814365384616">
The multi-document summarization problem has
received much attention recently (Lyngbaek,
2013; Sood, 2013; Qian and Liu, 2013) due to
its ability to reduce large quantities of text to a
human processable amount as well as its appli-
cation in other fields such as question answering
(Liu et al., 2008; Chali et al., 2009a; Chali et al.,
2009b; Chali et al., 2011b). We expect this trend
to further increase as the amount of linguistic data
on the web from sources such as social media,
wikipedia, and online newswire increases. This
paper focuses specifically on utilizing reinforce-
ment learning (Sutton and Barto, 1998; Szepesv,
2009) to create a policy for summarizing clusters
of multiple documents related to the same topic.
The task of extractive automated multi-
document summarization (Mani, 2001) is to se-
lect a subset of textual units, in this case sentences,
from the source document cluster to form a sum-
mary of the cluster in question. This extractive
approach allows the learner to construct a sum-
mary without concern for the linguistic quality of
the sentences generated, as the source documents
are assumed to be of a certain linguistic quality.
This paper aims to expand on the techniques used
in Ryang and Abekawa (2012) which uses a re-
inforcement learner, specifically TD(λ), to create
summaries of document clusters. We achieve this
through introducing a new algorithm, varying the
feature space and utilizing alternate reward func-
tions.
The TD(λ) learner used in Ryang and
Abekawa (2012) is a very early reinforcement
learning implementation. We explore the option of
leveraging more recent research in reinforcement
learning algorithms to improve results. To this end
we explore the use of 5AR5A which is a deriva-
tive of TD(λ) that models the action space in ad-
dition to the state space modelled by TD(λ). Fur-
thermore we explore the use of an algorithm not
based on temporal difference methods, but instead
on policy iteration techniques. Approximate Pol-
icy Iteration (Lagoudakis and Parr, 2003) gener-
ates a policy, then evaluates and iterates until con-
vergence.
The reward function in Ryang and Abekawa
(2012) is a delayed reward based on tf∗idf values.
We further explore the reward space by introduc-
ing similarity metric calculations used in ROUGE
(Lin, 2004) and base our ideas on Saggion et al.
(2010). The difference between immediate re-
wards and delayed rewards is that the learner re-
</bodyText>
<page confidence="0.441955">
681
</page>
<note confidence="0.979606">
Proceedings of the 2014 Conference on Empirical Methods in Natural Language Processing (EMNLP), pages 681–690,
October 25-29, 2014, Doha, Qatar. c�2014 Association for Computational Linguistics
</note>
<bodyText confidence="0.99988503030303">
ceives immediate feedback at every action in the
former and feedback only at the end of the episode
in the latter. We explore the performance differ-
ence of both reward types. Finally we develop
query focused extensions to both reward functions
and present their results on more recent Document
Understanding Conference (DUC) datasets which
ran a query focused task.
We first evaluate our systems using the
DUC2004 dataset for comparison with the results
in Ryang and Abekawa (2012). We then present
the results of query focused reward functions
against the DUC2006 dataset to provide refer-
ence with a more recent dataset and a more recent
task, specifically a query-focused summarization
task. Evaluations are performed using ROUGE
for ROUGE-1, ROUGE-2 and ROUGE-L values
for general summarization, while ROUGE-2 and
ROUGE-SU4 is used for query-focused summa-
rization. Furthermore we selected a small subset
of query focused summaries to be subjected to hu-
man evaluations and present the results.
Our implementation is named REAPER
(Relatedness-focused Extractive Automatic
summary Preparation Exploiting Reinfocement
learning) thusly for its ability to harvest a docu-
ment cluster for ideal sentences for performing
the automatic summarization task. REAPER is
not just a reward function and feature set, it is a
full framework for implementing summarization
tasks using reinforcement learning and is avail-
able online for experimentation.1 The primary
contributions of our experiments are as follows:
</bodyText>
<listItem confidence="0.993644166666667">
• Exploration of TD(λ), 5AR5A and Ap-
proximate Policy Iteration.
• Alternate REAPER reward function.
• Alternate REAPER feature set.
• Query focused extensions of automatic sum-
marization using reinforcement learning.
</listItem>
<sectionHeader confidence="0.609388" genericHeader="introduction">
2 Previous Work and Motivation
</sectionHeader>
<bodyText confidence="0.953427">
Previous work using reinforcement learning for
natural language processing tasks (Branavan et
al., 2009; Wan, 2007; Ryang and Abekawa,
2012; Chali et al., 2011a; Chali et al., 2012)
inspired us to use a similar approach in our
experiments. Ryang and Abekawa (2012) im-
plemented a reinforcement learning approach to
</bodyText>
<footnote confidence="0.468459">
1https://github.com/codyrioux/REAPER
</footnote>
<bodyText confidence="0.999582568627451">
multi-document summarization which they named
Automatic Summarization using Reinforcement
Learning (ASRL). ASRL uses TD(λ) to learn and
then execute a policy for summarizing a cluster of
documents. The algorithm performs N summa-
rizations from a blank state to termination, updat-
ing a set of state-value predictions as it does so.
From these N episodes a policy is created using
the estimated state-value pairs, this policy greed-
ily selects the best action until the summary enters
its terminal state. This summary produced is the
output of ASRL and is evaluated using ROUGE-
1, ROUGE-2, and ROUGE-L (Lin, 2004). The
results segment of the paper indicates that ASRL
outperforms greedy and integer linear program-
ming (ILP) techniques for the same task.
There are two notable details that provide the
motivation for our experiments; TD(λ) is rela-
tively old as far as reinforcement learning (RL)
algorithms are concerned, and the optimal ILP did
not outperform ASRL using the same reward func-
tion. The intuition gathered from this is that if
the optimal ILP algorithm did not outperform the
suboptimal ASRL on the ROUGE evaluation, us-
ing the same reward function, then there is clearly
room for improvement in the reward function’s
ability to accurately model values in the state
space. Furthermore one may expect to achieve
a performance boost exploiting more recent re-
search by utilizing an algorithm that intends to
improve upon the concepts on which TD(λ) is
based. These provide the motivation for the re-
mainder of the research preformed.
Query focused multi-document summarization
(Li and Li, 2013; Chali and Hasan, 2012b; Yin et
al., 2012; Wang et al., 2013) has recently gained
much attention due to increasing amounts of tex-
tual data, as well as increasingly specific user de-
mands for extracting information from said data.
This is reflected in the query focused tasks run in
the Document Understanding Conference (DUC)
and Text Analysis Conference (TAC) over the past
decade. This has motivated us to design and im-
plement query focused extensions to these rein-
forcement learning approaches to summarization.
There has been some research into the effects of
sentence compression on the output of automatic
summarization systems (Chali and Hasan, 2012a),
specifically the evaluation results garnered from
compressing sentences before evaluation (Qian
and Liu, 2013; Lin and Rey, 2003; Ryang and
</bodyText>
<page confidence="0.857672">
682
</page>
<bodyText confidence="0.978653333333333">
Abekawa, 2012). However Ryang and Abekawa
(2012) found this technique to be ineffective in im-
proving ROUGE metrics using a similar reinforce-
ment learning approach to this paper, as a result
we will not perform any further exploration into
the effects of sentence compression.
</bodyText>
<sectionHeader confidence="0.97719" genericHeader="method">
3 Problem Definition
</sectionHeader>
<bodyText confidence="0.999262411764706">
We use an identical problem definition to Ryang
and Abekawa (2012). Assume the given cluster
of documents is represented as a set of textual
units D = {x1, x2, · · · , xn} where |D |= n and
xi represents a single textual unit. Textual units
for the purposes of this experiment are the indi-
vidual sentences in the document cluster, that is
D = D1 U D2 U · · · U Dm where m is the number
of documents in the cluster and each Di represents
a document.
The next necessary component is the score
function, which is to be used as the reward for the
learner. The function score(s) can be applied to
any s C D. s is a summary of the given document
or cluster of documents.
Given these parameters, and a length limitation
k we can define an optimal summary s* as:
</bodyText>
<equation confidence="0.993866">
s* = argmax score(s)
</equation>
<bodyText confidence="0.9942736">
where s C D and length(s) &lt; k (1)
It is the objective of our learner to create a pol-
icy that produces the optimal summary for its pro-
vided document cluster D. Henceforth the length
limitations used for general summarization will be
665 bytes, and query focused summarization will
use 250 words. These limitations on summary
length match those set by the Document Under-
standing Conferences associated with the dataset
utilized in the respective experiments.
</bodyText>
<sectionHeader confidence="0.992424" genericHeader="method">
4 Algorithms
</sectionHeader>
<bodyText confidence="0.99971775">
TD(λ) and 5AR5A (Sutton and Barto, 1998) are
temporal difference methods in which the primary
difference is that TD(λ) models state value pre-
dictions, and 5AR5A models state-action value
predictions. Approximate Policy Iteration (API)
follows a different paradigm by iteratively improv-
ing a policy for a markov decision process until the
policy converges.
</bodyText>
<subsectionHeader confidence="0.986543">
4.1 TD(λ)
</subsectionHeader>
<bodyText confidence="0.9995486">
In the ASRL implementation of TD(λ) the learn-
ing rate αk and temperature τk decay as learning
progresses with the following equations with k set
to the number of learning episodes that had taken
place.
</bodyText>
<equation confidence="0.9998685">
αk = 0.001 · 101/(100 + k1.1) (2)
τk = 1.0 * 0.987k−1 (3)
</equation>
<bodyText confidence="0.999669133333333">
One can infer from the decreasing values of αk
that as the number of elapsed episodes increases
the learner adjusts itself at a smaller rate. Simi-
larly as the temperature τk decreases the action se-
lection policy becomes greedier and thus performs
less exploration, this is evident in (5) below.
Note that unlike traditional TD(λ) implementa-
tions the eligibility trace e resets on every episode.
The reasons for this will become evident in the
experiments section of the paper in which λ =
1,γ = 1 and thus there is no decay during an
episode and complete decay after an episode. The
same holds true for 5AR5A below.
The action-value estimation Q(s, a) is approxi-
mated as:
</bodyText>
<equation confidence="0.9907806">
Q(s, a) = r + γV (s&apos;) (4)
The policy is implemented as such:
eQ(s,a)/τ
policy(a|s; θ; τ) = (5)
Ea∈A eQ(s,a)/τ
</equation>
<bodyText confidence="0.99985325">
Actions are selected probabilistically using soft-
max selection (Sutton and Barto, 1998) from a
Boltzmann distribution. As the value of τ ap-
proaches 0 the distribution becomes greedier.
</bodyText>
<subsectionHeader confidence="0.943653">
4.2 5AR5A
</subsectionHeader>
<bodyText confidence="0.999644142857143">
5AR5A is implemented in a very similar manner
and shares αk, τk, φ(s), m, and policy(s) with the
TD(λ) implementation above. 5AR5A is also
a temporal difference algorithm and thus behaves
similarly to TD(λ) with the exception that values
are estimated not only on the state s but a state-
action pair [s, a].
</bodyText>
<subsectionHeader confidence="0.999719">
4.3 Approximate Policy Iteration
</subsectionHeader>
<bodyText confidence="0.999863333333333">
The third algorithm in our experiment uses Ap-
proximate Policy Iteration (Lagoudakis and Parr,
2003) to implement a reinforcement learner. The
</bodyText>
<page confidence="0.810408">
683
</page>
<bodyText confidence="0.9999615">
novelty introduced by (Lagoudakis and Parr,
2003) is that they eschew standard representations
for a policy and instead use a classifier to represent
the current policy π. Further details on the algo-
rithm can be obtained from Lagoudakis and Parr
(2003).
</bodyText>
<sectionHeader confidence="0.997753" genericHeader="method">
5 Experiments
</sectionHeader>
<bodyText confidence="0.999992692307692">
Our state space S is represented simply as a three-
tuple [s a f] in which s is the set of textual units
(sentences) that have been added to the summary,
a is a sequence of actions that have been per-
formed on the summary and f is a boolean with
value 0 representing non-terminal states and 1 rep-
resenting a summary that has been terminated.
The individual units in our action space are de-
fined as [:insert xi] where xi is a textual unit as
described earlier, let us define Di as the set [:in-
sert xi] for all xi E D where D is the document
set. We also have one additional action [:finish]
and thus we can define our action space.
</bodyText>
<equation confidence="0.705714">
A = Di U {[: finish]} (6)
</equation>
<bodyText confidence="0.851041333333333">
The actions eligible to be executed on any given
state s is defined by a function actions(A, s):
�
</bodyText>
<equation confidence="0.9310495">
[: finish] if length(s) &gt; k
actions(A, s) =
A − at otherwise
(7)
</equation>
<bodyText confidence="0.923620333333333">
The state-action transitions are defined below:
one but depends on the presence of top bigrams
instead of tf ∗ idf words.
</bodyText>
<listItem confidence="0.993417230769231">
• One bit b E 0,1 for each of the top n bigrams
(Manning and Sch¨utze, 1999) present in the
summary.
• Coverage ratio calculated as the sum of the
bits in the previous feature divided by n.
• Redundancy Ratio calculated as the number
of redundant times a bit in the first feature is
flipped on, divided by n.
• Length Ratio calculated as length(s)/k
where k is the length limit.
• Longest common subsequence length.
• Length violation bit. Set to 1 if length(s) &gt;
k
</listItem>
<bodyText confidence="0.9999015">
Summaries which exceed the length limitation
k are subject to the same reduction as the ASRL
feature set (Ryang and Abekawa, 2012) to an all
zero vector with the final bit set to one.
</bodyText>
<subsectionHeader confidence="0.996982">
5.2 Reward Function
</subsectionHeader>
<bodyText confidence="0.94422">
Our reward function (termed as REAPER reward)
is based on the n-gram concurrence score metric,
and the longest-common-subsequence recall met-
ric contained within ROUGE (Lin, 2004).
[st , at , 0] a=insertxi −−−−−−−) [st U xi , at U a , 0] (8) reward(s) = { −1, if length(s) &gt; k
[st , at , 0] :finish score(s) if s is terminal
−−−−) [st ,at , 1] (9) 0 otherwise
(11)
</bodyText>
<equation confidence="0.919141">
[st , at+1 ,1] any −−) [st , at ,1] (10)
</equation>
<bodyText confidence="0.9995685">
Insertion adds both the content of the textual
unit xi to the set s as well as the action itself to
set a. Conversely finishing does not alter s or a
but it flips the f bit to on. Notice from (10) that
once a state is terminal any further actions have no
effect.
</bodyText>
<subsectionHeader confidence="0.991161">
5.1 Feature Space
</subsectionHeader>
<bodyText confidence="0.999705142857143">
We present an alternate feature set called
REAPER feature set based on the ideas presented
in Ryang and Abekawa (2012). Our proposed fea-
ture set follows a similar format to the previous
Where score is defined identically to ASRL,
with the exception that Sim is a new equation
based on ROUGE metrics.
</bodyText>
<equation confidence="0.969687875">
�score(s) = λsRel(xi)−
xi∈S
� (1 − λs)Red(xi, xj)
xi,xj∈S,i&lt;j
(12)
Rel(xi) = Sim(xi, D) + Pos(xi)−1 (13)
Red(xi, xj) = Sim(xi, xj) (14)
684
</equation>
<bodyText confidence="0.9997065">
Ryang and Abekawa (2012) experimentally de-
termined a value of 0.9 for the As parameter. That
value is used herein unless otherwise specified.
Sim has been redefined as:
</bodyText>
<equation confidence="0.9998352">
Sim(s) =1 * ngco(1, D, s)+
4 * ngco(2, D, s)+
1 * ngco(3, D, s)+ (15)
1 * ngco(4, D, s)+
1 * rlcs(D, s)
</equation>
<bodyText confidence="0.959594">
and ngco is the ngram co-occurence score met-
ric as defined by Lin (2004).
</bodyText>
<equation confidence="0.996905666666667">
ngco(n, D, s) =
Er∈D Engram∈r Countmatch(ngram)
ES,∈D Engram∈r Count(ngram) (16)
</equation>
<bodyText confidence="0.998483538461539">
Where n is the n-gram count for example 2 for
bigrams, D is the set of documents, and s is the
summary in question. Countmatch is the maxi-
mum number of times the ngram occurred in either
D or s.
The rlcs(R, S) is also a recall oriented mea-
sure based on longest common subsequence
(Hirschberg, 1977). Recall was selected as
DUC2004 tasks favoured a Q value for F-Measure
(Lin, 2004) high enough that only recall would
be considered. lcs is the longest common sub-
sequence, and length(D) is the total number of
tokens in the reference set D.
</bodyText>
<equation confidence="0.999332333333333">
lcs(D, s)
rlcs(D, s) = (17)
length(D)
</equation>
<bodyText confidence="0.9999242">
We are measuring similarity between sentences
and our entire reference set, and thusly our D is
the set of documents defined in section 3. This
is also a delayed reward as the provided reward is
zero until the summary is terminal.
</bodyText>
<subsectionHeader confidence="0.74582">
5.2.1 Query Focused Rewards
</subsectionHeader>
<bodyText confidence="0.999989352941177">
We have proposed an extension to both reward
functions to allow for query focused (QF) summa-
rization. We define a function score0 which aims
to balance the summarization abilities of the re-
ward with a preference for selecting textual units
related to the provided query q. Both ASRL and
REAPER score functions have been extended in
the following manner where Sim is the same sim-
ilarity functions used in equation (13) and (15).
The parameter Q is a balancing factor between
query similarity and overall summary score in
which 0 &lt;= Q &lt;= 1, we used an arbitrarily cho-
sen value of 0.9 in these experiments. In the case
of ASRL the parameter q is the vectorized version
of the query function with tf * idf values, and for
Sim q is a sequence of tokens which make up the
query, stemmed and stop-words removed.
</bodyText>
<subsubsectionHeader confidence="0.645657">
5.2.2 Immediate Rewards
</subsubsectionHeader>
<bodyText confidence="0.9999852">
Finally we also employ immediate versions of the
reward functions which behave similarly to their
delayed counterparts with the exception that the
score is always provided to the caller regardless of
the terminal status of state s.
</bodyText>
<sectionHeader confidence="0.999873" genericHeader="method">
6 Results
</sectionHeader>
<bodyText confidence="0.9997415">
We first present results2 of our experiments, spec-
ifying parameters, and withholding discussion un-
til the following section. We establish a bench-
mark using ASRL and other top-scoring sum-
marization systems compared with REAPER us-
ing ROUGE. For generic multi-document sum-
marization we run experiments on all 50 docu-
ment clusters, each containing 10 documents, of
DUC2004 task 2 with parameters for REAPER
and ASRL fixed at A = 1, γ = 1, and k = 665.
Sentences were stemmed using a Porter Stemmer
(Porter, 1980) and had the ROUGE stop word set
removed. All summaries were processed in this
manner and then projected back into their original
(unstemmed, with stop-words) state and output to
disk.
</bodyText>
<table confidence="0.996844714285714">
Config R-1 R-2 R-L
REAPER 0.40339 0.11397 0.36574
ASRL 0.39013 0.09479 0.33769
MCKP 0.39033 0.09613 0.34225
PEER65 0.38279 0.09217 0.33099
ILP 0.34712 0.07528 0.31241
GREEDY 0.30618 0.06400 0.27507
</table>
<equation confidence="0.686600666666667">
reward(s) = �
−1, if length(s) &gt; k
score(s) otherwise
(19)
score0(q, s) = QSim(q, s) + (1 − Q)score(s)
(18) 2ROUGE-1.5.5 run with -m -s -p 0
</equation>
<tableCaption confidence="0.6675755">
Table 1: Experimental results with ROUGE-1,
ROUGE-2 and ROUGE-L scores on DUC2004.
</tableCaption>
<page confidence="0.805997">
685
</page>
<bodyText confidence="0.9984642">
Table 1 presents results for REAPER, ASRL
(Ryang and Abekawa, 2012), MCKP (Takamura
and Okumura, 2009), PEER65 (Conroy et al.,
2004) , and GREEDY (Ryang and Abekawa,
2012) algorithms on the same task. This allows
us to make a direct comparison with the results of
Ryang and Abekawa (2012).
REAPER results are shown using the TD(λ)
algorithm, REAPER reward function, and ASRL
feature set. This is to establish the validity of the
reward function holding all other factors constant.
REAPER results for ROUGE-1, ROUGE-2 and
ROUGE-L are statistically significant compared to
the result set presented in Table 2 of Ryang and
Abekawa (2012) using p &lt; 0.01.
</bodyText>
<table confidence="0.999617833333333">
Run R-1 R-2 R-L
REAPER 0 0.39536 0.10679 0.35654
REAPER 1 0.40176 0.11048 0.36450
REAPER 2 0.39272 0.11171 0.35766
REAPER 3 0.39505 0.11021 0.35972
REAPER 4 0.40259 0.11396 0.36539
REAPER 5 0.40184 0.11306 0.36391
REAPER 6 0.39311 0.10873 0.35481
REAPER 7 0.39814 0.11001 0.35786
REAPER 8 0.39443 0.10740 0.35586
REAPER 9 0.40233 0.11397 0.36483
Average 0.39773 0.11063 0.36018
</table>
<tableCaption confidence="0.998756">
Table 2: REAPER run 10 times on the DUC2004.
</tableCaption>
<bodyText confidence="0.999114444444444">
We present the results of 10 runs of REAPER,
with REAPER feature set.. As with ASRL,
REAPER does not converge on a stable solution
which is attributable to the random elements of
TD(λ). Results in all three metrics are again
statistically significant compared to ASRL results
presented in the Ryang and Abekawa (2012) pa-
per. All further REAPER experiments use the bi-
gram oriented feature space.
</bodyText>
<table confidence="0.996479333333333">
Reward R-1 R-2 R-L
Delayed 0.39773 0.11397 0.36018
Immediate 0.32981 0.07709 0.30003
</table>
<tableCaption confidence="0.60759325">
Table 3: REAPER with delayed and immediate re-
wards on DUC2004.
Table 3 shows the performance difference of
REAPER when using a delayed and immediate re-
ward. The immediate version of REAPER pro-
vides feedback on every learning step, unlike the
delayed version which only provides score at the
end of the episode.
</tableCaption>
<table confidence="0.997658666666667">
Features R-1 R-2 R-L
ASRL 0.40339 0.11397 0.36574
REAPER 0.40259 0.11396 0.36539
</table>
<tableCaption confidence="0.937588">
Table 4: REAPER with alternate feature spaces on
DUC2004.
</tableCaption>
<bodyText confidence="0.92358925">
We can observe the results of using REAPER
with various feature sets in Table 4. Experiments
were run using REAPER reward, TD(λ), and the
specified feature set.
</bodyText>
<table confidence="0.99793075">
Algorithm R-1 R-2 R-L
TD(λ) 0.39773 0.11063 0.36018
SARSA 0.28287 0.04858 0.26172
API 0.29163 0.06570 0.26542
</table>
<tableCaption confidence="0.9506655">
Table 5: REAPER with alternate algorithms on
DUC2004.
</tableCaption>
<bodyText confidence="0.9223584">
Table 5 displays the performance of REAPER
with alternate algorithms. TD(λ) and SARSA
are run using the delayed reward feature, while
API requires an immediate reward and was thus
run with the immediate reward.
</bodyText>
<table confidence="0.9956864">
System R-2 R-SU4
REAPER 0.07008 0.11689
ASRL 0.05639 0.09379
S24 0.09505 0.15464
Baseline 0.04947 0.09788
</table>
<tableCaption confidence="0.998444">
Table 6: QF-REAPER on DUC2006.
</tableCaption>
<bodyText confidence="0.999351928571429">
For query-focused multi-document summariza-
tion we experimented with the DUC2006 system
task, which contained 50 document clusters con-
sisting of 25 documents each. Parameters were
fixed to λ = 1, γ = 1 and k = 250 words. In
Table 6 we can observe the results3 of our query
focused systems against DUC2006’s top scorer
(S24) for ROUGE-2, and a baseline. The baseline
was generated by taking the most recent document
in the cluster and outputting the first 250 words.
Human Evaluations: We had three native
English-speaking human annotators evaluate a set
of four randomly chosen summaries produced by
REAPER on the DUC2004 dataset.
</bodyText>
<table confidence="0.8523978">
3ROUGE-1.5.5 run with -n 2 -x -m -2 4 -u -c 95 -r 1000
-f A -p 0.5 -t 0 -l 250
686
Metric A1 A2 A3 AVG
Grammaticality 3.00 4.00 4.00 3.67
Redundancy 4.75 4.25 2.75 3.92
Referential Clarity 4.00 4.50 3.50 4.00
Focus 4.50 3.50 2.25 3.42
Structure 3.50 4.00 3.00 3.50
Responsiveness 4.25 3.75 3.00 3.67
</table>
<tableCaption confidence="0.999588">
Table 7: Human evaluation scores on DUC2004.
</tableCaption>
<bodyText confidence="0.984136272727273">
Table 7 shows the evaluation results accord-
ing to the DUC2006 human evaluation guidelines.
The first five metrics are related entirely to the lin-
guistic quality of the summary in question and the
final metric, Responsiveness, rates the summary
on its relevance to the source documents. Columns
represent the average provided by a given annota-
tor over the four summaries, and the AVG column
represents the average score for all three annota-
tors over all four summaries. Score values are an
integer between 1 and 5 inclusive.
</bodyText>
<sectionHeader confidence="0.993389" genericHeader="method">
7 Discussion
</sectionHeader>
<bodyText confidence="0.999963656716419">
First we present a sample of a summary generated
from a randomly selected cluster. The following
summary was generated from cluster D30017 of
the DUC 2004 dataset using REAPER reward with
TD(A) and REAPER feature space.
A congressman who visited remote parts of
North Korea last week said Saturday that the food
and health situation there was desperate and de-
teriorating, and that millions of North Koreans
might have starved to death in the last few years.
North Korea is entering its fourth winter of chronic
food shortages with its people malnourished and
at risk of dying from normally curable illnesses,
senior Red Cross officials said Tuesday. More than
five years of severe food shortages and a near-total
breakdown in the public health system have led to
devastating malnutrition in North Korea and prob-
ably left an entire generation of children physi-
cally and mentally impaired, a new study by in-
ternational aid groups has found. Years of food
shortages have stunted the growth of millions of
North Korean children, with two-thirds of children
under age seven suffering malnourishment, U.N.
experts said Wednesday. The founder of South Ko-
rea’s largest conglomerate plans to visit his native
North Korea again next week with a gift of 501
cattle, company officials said Thursday. “There is
enough rice.
We can observe that the summary is both syn-
tactically sound, and elegantly summarizes the
source documents.
Our baseline results table (Table 1) shows
REAPER outperforming ASRL in a statistically
significant manner on all three ROUGE metrics
in question. However we can see from the abso-
lute differences in score that very few additional
important words were extracted (ROUGE-1) how-
ever REAPER showed a significant improvement
in the structuring and ordering of those words
(ROUGE-2, and ROUGE-L).
The balancing factors used in the REAPER re-
ward function are responsible for the behaviour of
the reward function, and thus largely responsible
for the behaviour of the reinforcement learner. In
equation 15 we can see balance numbers of 1, 4, 1,
1, 1 for 1-grams, 2-grams, 3-grams, 4-grams, and
LCS respectively. In adjusting these values a user
can express a preference for a single metric or a
specific mixture of these metrics. Given that the
magnitude of scores for n-grams decrease as n in-
creases and given that the magnitude of scores for
1-grams is generally 3 to 4 times larger, in our ex-
perience, we can see that this specific reward func-
tion favours bigram similarity over unigram simi-
larity. These balance values can be adjusted to suit
the specific needs of a given situation, however we
leave exploration of this concept for future work.
We can observe in Figure 1 that ASRL does not
converge on a stable value, and dips towards the
300th episode while in Figure 2 REAPER does
not take nearly such a dramatic dip. These fig-
ures display average normalized reward for all 50
document clusters on a single run. Furthermore
we can observe that ASRL reaches it’s peak re-
ward around episode 225 while REAPER does so
around episode 175 suggesting that REAPER con-
verges faster.
</bodyText>
<subsectionHeader confidence="0.995086">
7.1 Delayed vs. Immediate Rewards
</subsectionHeader>
<bodyText confidence="0.999816555555555">
The delayed vs. immediate rewards results in Ta-
ble 3 clearly show that delaying the reward pro-
vides a significant improvement in globally opti-
mizing the summary for ROUGE score. This can
be attributed to the A = 1 and γ = 1 parame-
ter values being suboptimal for the immediate re-
ward situation. This has the added benefit of be-
ing much more performant computationally as far
fewer reward calculations need be done.
</bodyText>
<page confidence="0.622129">
687
</page>
<figureCaption confidence="0.9996395">
Figure 1: ASRL normalized reward.
Figure 2: REAPER normalized reward.
</figureCaption>
<subsectionHeader confidence="0.992488">
7.2 Feature Space
</subsectionHeader>
<bodyText confidence="0.99999">
The feature space experiments in Table 4 seem to
imply that REAPER performs similarly with both
feature sets. We are confident that an improve-
ment could be made through further experimenta-
tion. Feature engineering, however, is a very broad
field and we plan to pursue this topic in depth in
the future.
</bodyText>
<subsectionHeader confidence="0.974733">
7.3 Algorithms
</subsectionHeader>
<bodyText confidence="0.999968">
TD(λ) significantly outperformed both 5AR5A
and API in the algorithm comparison. Ryang and
Abekawa (2012) conclude that the feature space is
largely responsible for the algorithm performance.
This is due to the fact that poor states such as those
that are too long, or those that contain few impor-
tant words will reduce to the same feature set and
receive negative rewards collectively. 5AR5A
loses this benefit as a result of its modelling of
state-action pairs.
API on the other hand may have suffered a per-
formance loss due to its requirements of an imme-
diate reward, this is because when using a delayed
reward if the trajectory of a rollout does not reach
a terminal state the algorithm will not be able to
make any estimations about the value of the state
in question. We propose altering the policy iter-
ation algorithm to use a trajectory length of one
episode instead of a fixed number of actions in or-
der to counter the need for an immediate reward
function.
</bodyText>
<subsectionHeader confidence="0.986965">
7.4 Query Focused Rewards
</subsectionHeader>
<bodyText confidence="0.9999674">
From the ROUGE results in Table 6 we can infer
that while REAPER outperformed ASRL on the
query focused task, however it is notable that both
systems under performed when compared to the
top system from the DUC2006 conference.
We can gather from these results that it is not
enough to simply naively calculate similarity with
the provided query in order to produce a query-
focused result. Given that the results produced by
the generic summarization task is rather accept-
able according to our human evaluations we sug-
gest that further research be focused on a proper
similarity metric between the query and summary
to improve the reward function’s overall ability to
score summaries in a query-focused setting.
</bodyText>
<sectionHeader confidence="0.945478" genericHeader="conclusions">
8 Conclusion and Future Work
</sectionHeader>
<bodyText confidence="0.999980181818182">
We have explored alternate reward functions, fea-
ture sets, and algorithms for the task of automatic
summarization using reinforcement learning. We
have shown that REAPER outperforms ASRL on
both generic summarization and the query focused
tasks. This suggests the effectiveness of our re-
ward function and feature space. Our results also
confirm that TD(λ) performs best for this task
compared to 5AR5A and API.
Due to the acceptable human evaluation scores
on the general summarization task it is clear that
the algorithm produces acceptable summaries of
newswire data. Given that we have a framework
for generating general summaries, and the cur-
rent popularity of the query-focused summariza-
tion task, we propose that the bulk of future work
in this area be focused on the query-focused task
specifically in assessing the relevance of a sum-
mary to a provided query. Therefore we intend to
pursue future research in utilizing word-sense dis-
ambiguation and synonyms, as well as other tech-
niques for furthering REAPER’s query similarity
</bodyText>
<page confidence="0.820372">
688
</page>
<bodyText confidence="0.9899045">
metrics in order to improve its ROUGE and human
evaluation scores on query-focused tasks.
</bodyText>
<sectionHeader confidence="0.995457" genericHeader="acknowledgments">
Acknowledgments
</sectionHeader>
<bodyText confidence="0.999938125">
We would like to thank the anonymous review-
ers for their useful comments. The research re-
ported in this paper was supported by the Nat-
ural Sciences and Engineering Research Council
(NSERC) of Canada - discovery grant and the Uni-
versity of Lethbridge. This work was done when
the second author was at the University of Leth-
bridge.
</bodyText>
<sectionHeader confidence="0.982724" genericHeader="references">
References
</sectionHeader>
<reference confidence="0.999803451612903">
S . Branavan, H. Chen, L. S. Zettlemoyer, and R. Barzi-
lay. 2009. Reinforcement Learning for Mapping
Instructions to Actions. In Proceedings of the 47th
Annual Meeting of the ACL and the 4th IJCNLP of
the AFNLP, pages 82–90.
Y. Chali and S. A. Hasan. 2012a. On the Effectiveness
of Using Sentence Compression Models for Query-
Focused Multi-Document Summarization. In Pro-
ceedings of the 24th International Conference on
Computational Linguistics (COLING 2012), pages
457–474. Mumbai, India.
Y. Chali and S. A. Hasan. 2012b. Query-
focused Multi-document Summarization: Auto-
matic Data Annotations and Supervised Learning
Approaches. Journal of Natural Language Engi-
neering, 18(1):109–145.
Y. Chali, S. A. Hasan, and S. R. Joty. 2009a. Do Auto-
matic Annotation Techniques Have Any Impact on
Supervised Complex Question Answering? Pro-
ceedings of the Joint conference of the 47th Annual
Meeting of the Association for Computational Lin-
guistics (ACL-IJCNLP 2009), pages 329–332.
Y. Chali, S. R. Joty, and S. A. Hasan. 2009b. Com-
plex Question Answering: Unsupervised Learning
Approaches and Experiments. Journal of Artificial
Intelligence Research, 35:1–47.
Y. Chali, S. A. Hasan, and K. Imam. 2011a. A
Reinforcement Learning Framework for Answering
Complex Questions. In Proceedings of the 16th
International Conference on Intelligent User Inter-
faces, pages 307–310. ACM, Palo Alto, CA, USA.
Y. Chali, S. A. Hasan, and S. R. Joty. 2011b. Improv-
ing Graph-based Random Walks for Complex Ques-
tion Answering Using Syntactic, Shallow Semantic
and Extended String Subsequence Kernels. Infor-
mation Processing and Management (IPM), Special
Issue on Question Answering, 47(6):843–855.
Y. Chali, S. A. Hasan, and K. Imam. 2012. Improv-
ing the Performance of the Reinforcement Learn-
ing Model for Answering Complex Questions. In
Proceedings of the 21st ACM Conference on Infor-
mation and Knowledge Management (CIKM 2012),
pages 2499–2502. ACM, Maui, Hawaii, USA.
J. Conroy, J. Goldstein, and D. Leary. 2004. Left-Brain
/ Right-Brain Multi-Document Summarization. In
Proceedings of the Document Un- derstanding Con-
ference (DUC 2004).
D. S. Hirschberg. 1977. Algorithms for the Longest
Common Subsequence Problem. In Journal of the
ACM, 24(4):664–675, October.
M. Lagoudakis and R. Parr. 2003. Reinforcement
learning as classification: Leveraging modern classi-
fiers. In Proceedings of the Twentieth International
Conference on Machine Learning, 20(1):424.
J. Li and S. Li. 2013. A Novel Feature-based Bayesian
Model for Query Focused Multi-document Summa-
rization. In Transactions of the Association for
Computational Linguistics, 1:89–98.
C. Lin and M. Rey. 2003. Improving Summariza-
tion Performance by Sentence Compression A Pi-
lot Study. In Proceedings of the Sixth International
Workshop on Information Retrieval with Asian Lan-
guages.
C. Lin. 2004. ROUGE : A Package for Automatic
Evaluation of Summaries. In Information Sciences,
16(1):25–26.
Y. Liu, S. Li, Y. Cao, C. Lin, D. Han, and Y. Yu.
2008. Understanding and Summarizing Answers
in Community-Based Question Answering Services.
In COLING ’08 Proceedings of the 22nd Inter-
national Conference on Computational Linguistics,
1(August):497–504.
S. Lyngbaek. 2013. SPORK: A Summarization
Pipeline for Online Repositories of Knowledge.
M.sc. thesis, California Polytechnic State Univer-
sity.
I. Mani. 2001. Automatic Summarization. John Ben-
jamins Publishing.
C. D. Manning and H. Sch¨utze. 1999. Foundations of
Statistical Natural Language Processing, volume 26
of . MIT Press.
M. F. Porter. 1980. An algorithm for suffix stripping.
Program, 14(3):130–137.
X. Qian and Y. Liu. 2013. Fast Joint Compression and
Summarization via Graph Cuts. In Conference on
Empirical Methods in Natural Language Process-
ing.
S. Ryang and T. Abekawa. 2012. Framework of Au-
tomatic Text Summarization Using Reinforcement
Learning. In Proceedings of the 2012 Joint Con-
ference on Empirical Methods in Natural Language
Processing and Computational Natural Language
Learning, 1:256–265.
</reference>
<page confidence="0.650223">
689
</page>
<reference confidence="0.998381787878788">
H. Saggion, C. Iria, T. M. Juan-Manuel, and E. San-
Juan. 2010. Multilingual Summarization Evalua-
tion without Human Models. In COLING ’10 Pro-
ceedings of the 23rd International Conference on
Computational Linguistics: Posters, 1:1059–1067.
A. Sood. 2013. Towards Summarization of Written
Text Conversations. M.sc. thesis, International Insti-
tute of Information Technology, Hyderabad, India.
R. S. Sutton and A. G. Barto. 1998. Reinforcement
Learning: An Introduction. MIT Press.
C. A. Szepesv. 2009. Algorithms for Reinforcement
Learning. Morgan &amp; Claypool Publishers.
H. Takamura and M. Okumura. 2009. Text Summa-
rization Model based on Maximum Coverage Prob-
lem and its Variant. In EACL ’09 Proceedings
of the 12th Conference of the European Chapter
of the Association for Computational Linguistics,
(April):781–789.
X Wan. 2007. Towards an Iterative Reinforcement Ap-
proach for Simultaneous Document Summarization
and Keyword Extraction. In Proceedings of the 45th
Annual Meeting of the Association of Computational
Linguistics, pages 552–559.
L. Wang, H. Raghavan, V. Castelli, R. Florian, and
C. Cardie. 2013. A Sentence Compression Based
Framework to Query-Focused. In Proceedings of
the 51st Annual Meeting of the Association for Com-
putational Linguistics, 1:1384–1394.
W. Yin, Y. Pei, F. Zhang, and L. Huang. 2012. Query-
focused multi-document summarization based on
query-sensitive feature space. In Proceedings of the
21st ACM international conference on Information
and knowledge management - CIKM ’12, page 1652.
</reference>
<page confidence="0.895119">
690
</page>
</variant>
</algorithm>
<algorithm name="ParsHed" version="110505">
<variant no="0" confidence="0.715643">
<title confidence="0.998835">Fear the REAPER: A System for Automatic Summarization with Reinforcement Learning</title>
<author confidence="0.997623">Cody Rioux Sadid A Hasan Yllias Chali</author>
<affiliation confidence="0.999988">University of Lethbridge Philips Research North America University of Lethbridge</affiliation>
<address confidence="0.971864">Lethbridge, AB, Canada Briarcliff Manor, NY, USA Lethbridge, AB,</address>
<email confidence="0.921532">cody.rioux@uleth.casadid.hasan@philips.comchali@cs.uleth.ca</email>
<abstract confidence="0.991114296296296">This paper explores alternate algorithms, reward functions and feature sets for performing multi-document summarization using reinforcement learning with a high focus on reproducibility. We show that ROUGE results can be improved using a unigram and bigram similarity metric when training a learner to select sentences for summarization. Learners are trained to summarize document clusters based on various algorithms and reward functions and then evaluated using ROUGE. Our experiments show a statistically significant improvement of 1.33%, 1.58%, and 2.25% for ROUGE-1, ROUGE-2 and ROUGE- L scores, respectively, when compared with the performance of the state of the art in automatic summarization with reinforcement learning on the DUC2004 dataset. Furthermore query focused extensions of our approach show an improvement of 1.37% and 2.31% for ROUGE-2 and ROUGE-SU4 respectively over query focused extensions of the state of the art with reinforcement learning on the DUC2006 dataset.</abstract>
</variant>
</algorithm>
<algorithm name="ParsCit" version="110505">
<citationList>
<citation valid="true">
<authors>
<author>H Chen Branavan</author>
<author>L S Zettlemoyer</author>
<author>R Barzilay</author>
</authors>
<title>Reinforcement Learning for Mapping Instructions to Actions.</title>
<date>2009</date>
<booktitle>In Proceedings of the 47th Annual Meeting of the ACL and the 4th IJCNLP of the AFNLP,</booktitle>
<pages>82--90</pages>
<contexts>
<context position="5818" citStr="Branavan et al., 2009" startWordPosition="886" endWordPosition="889"> summarization task. REAPER is not just a reward function and feature set, it is a full framework for implementing summarization tasks using reinforcement learning and is available online for experimentation.1 The primary contributions of our experiments are as follows: • Exploration of TD(λ), 5AR5A and Approximate Policy Iteration. • Alternate REAPER reward function. • Alternate REAPER feature set. • Query focused extensions of automatic summarization using reinforcement learning. 2 Previous Work and Motivation Previous work using reinforcement learning for natural language processing tasks (Branavan et al., 2009; Wan, 2007; Ryang and Abekawa, 2012; Chali et al., 2011a; Chali et al., 2012) inspired us to use a similar approach in our experiments. Ryang and Abekawa (2012) implemented a reinforcement learning approach to 1https://github.com/codyrioux/REAPER multi-document summarization which they named Automatic Summarization using Reinforcement Learning (ASRL). ASRL uses TD(λ) to learn and then execute a policy for summarizing a cluster of documents. The algorithm performs N summarizations from a blank state to termination, updating a set of state-value predictions as it does so. From these N episodes </context>
</contexts>
<marker>Branavan, Zettlemoyer, Barzilay, 2009</marker>
<rawString>S . Branavan, H. Chen, L. S. Zettlemoyer, and R. Barzilay. 2009. Reinforcement Learning for Mapping Instructions to Actions. In Proceedings of the 47th Annual Meeting of the ACL and the 4th IJCNLP of the AFNLP, pages 82–90.</rawString>
</citation>
<citation valid="true">
<authors>
<author>Y Chali</author>
<author>S A Hasan</author>
</authors>
<title>On the Effectiveness of Using Sentence Compression Models for QueryFocused Multi-Document Summarization.</title>
<date>2012</date>
<booktitle>In Proceedings of the 24th International Conference on Computational Linguistics (COLING 2012),</booktitle>
<pages>457--474</pages>
<location>Mumbai, India.</location>
<contexts>
<context position="7686" citStr="Chali and Hasan, 2012" startWordPosition="1183" endWordPosition="1186">uition gathered from this is that if the optimal ILP algorithm did not outperform the suboptimal ASRL on the ROUGE evaluation, using the same reward function, then there is clearly room for improvement in the reward function’s ability to accurately model values in the state space. Furthermore one may expect to achieve a performance boost exploiting more recent research by utilizing an algorithm that intends to improve upon the concepts on which TD(λ) is based. These provide the motivation for the remainder of the research preformed. Query focused multi-document summarization (Li and Li, 2013; Chali and Hasan, 2012b; Yin et al., 2012; Wang et al., 2013) has recently gained much attention due to increasing amounts of textual data, as well as increasingly specific user demands for extracting information from said data. This is reflected in the query focused tasks run in the Document Understanding Conference (DUC) and Text Analysis Conference (TAC) over the past decade. This has motivated us to design and implement query focused extensions to these reinforcement learning approaches to summarization. There has been some research into the effects of sentence compression on the output of automatic summarizati</context>
</contexts>
<marker>Chali, Hasan, 2012</marker>
<rawString>Y. Chali and S. A. Hasan. 2012a. On the Effectiveness of Using Sentence Compression Models for QueryFocused Multi-Document Summarization. In Proceedings of the 24th International Conference on Computational Linguistics (COLING 2012), pages 457–474. Mumbai, India.</rawString>
</citation>
<citation valid="true">
<authors>
<author>Y Chali</author>
<author>S A Hasan</author>
</authors>
<title>Queryfocused Multi-document Summarization: Automatic Data Annotations and Supervised Learning Approaches.</title>
<date>2012</date>
<journal>Journal of Natural Language Engineering,</journal>
<volume>18</volume>
<issue>1</issue>
<contexts>
<context position="7686" citStr="Chali and Hasan, 2012" startWordPosition="1183" endWordPosition="1186">uition gathered from this is that if the optimal ILP algorithm did not outperform the suboptimal ASRL on the ROUGE evaluation, using the same reward function, then there is clearly room for improvement in the reward function’s ability to accurately model values in the state space. Furthermore one may expect to achieve a performance boost exploiting more recent research by utilizing an algorithm that intends to improve upon the concepts on which TD(λ) is based. These provide the motivation for the remainder of the research preformed. Query focused multi-document summarization (Li and Li, 2013; Chali and Hasan, 2012b; Yin et al., 2012; Wang et al., 2013) has recently gained much attention due to increasing amounts of textual data, as well as increasingly specific user demands for extracting information from said data. This is reflected in the query focused tasks run in the Document Understanding Conference (DUC) and Text Analysis Conference (TAC) over the past decade. This has motivated us to design and implement query focused extensions to these reinforcement learning approaches to summarization. There has been some research into the effects of sentence compression on the output of automatic summarizati</context>
</contexts>
<marker>Chali, Hasan, 2012</marker>
<rawString>Y. Chali and S. A. Hasan. 2012b. Queryfocused Multi-document Summarization: Automatic Data Annotations and Supervised Learning Approaches. Journal of Natural Language Engineering, 18(1):109–145.</rawString>
</citation>
<citation valid="true">
<authors>
<author>Y Chali</author>
<author>S A Hasan</author>
<author>S R Joty</author>
</authors>
<title>Do Automatic Annotation Techniques Have Any Impact on Supervised Complex Question Answering?</title>
<date>2009</date>
<booktitle>Proceedings of the Joint conference of the 47th Annual Meeting of the Association for Computational Linguistics (ACL-IJCNLP</booktitle>
<pages>329--332</pages>
<contexts>
<context position="1681" citStr="Chali et al., 2009" startWordPosition="242" endWordPosition="245"> reinforcement learning on the DUC2004 dataset. Furthermore query focused extensions of our approach show an improvement of 1.37% and 2.31% for ROUGE-2 and ROUGE-SU4 respectively over query focused extensions of the state of the art with reinforcement learning on the DUC2006 dataset. 1 Introduction The multi-document summarization problem has received much attention recently (Lyngbaek, 2013; Sood, 2013; Qian and Liu, 2013) due to its ability to reduce large quantities of text to a human processable amount as well as its application in other fields such as question answering (Liu et al., 2008; Chali et al., 2009a; Chali et al., 2009b; Chali et al., 2011b). We expect this trend to further increase as the amount of linguistic data on the web from sources such as social media, wikipedia, and online newswire increases. This paper focuses specifically on utilizing reinforcement learning (Sutton and Barto, 1998; Szepesv, 2009) to create a policy for summarizing clusters of multiple documents related to the same topic. The task of extractive automated multidocument summarization (Mani, 2001) is to select a subset of textual units, in this case sentences, from the source document cluster to form a summary of</context>
</contexts>
<marker>Chali, Hasan, Joty, 2009</marker>
<rawString>Y. Chali, S. A. Hasan, and S. R. Joty. 2009a. Do Automatic Annotation Techniques Have Any Impact on Supervised Complex Question Answering? Proceedings of the Joint conference of the 47th Annual Meeting of the Association for Computational Linguistics (ACL-IJCNLP 2009), pages 329–332.</rawString>
</citation>
<citation valid="true">
<authors>
<author>Y Chali</author>
<author>S R Joty</author>
<author>S A Hasan</author>
</authors>
<title>Complex Question Answering: Unsupervised Learning Approaches and Experiments.</title>
<date>2009</date>
<journal>Journal of Artificial Intelligence Research,</journal>
<pages>35--1</pages>
<contexts>
<context position="1681" citStr="Chali et al., 2009" startWordPosition="242" endWordPosition="245"> reinforcement learning on the DUC2004 dataset. Furthermore query focused extensions of our approach show an improvement of 1.37% and 2.31% for ROUGE-2 and ROUGE-SU4 respectively over query focused extensions of the state of the art with reinforcement learning on the DUC2006 dataset. 1 Introduction The multi-document summarization problem has received much attention recently (Lyngbaek, 2013; Sood, 2013; Qian and Liu, 2013) due to its ability to reduce large quantities of text to a human processable amount as well as its application in other fields such as question answering (Liu et al., 2008; Chali et al., 2009a; Chali et al., 2009b; Chali et al., 2011b). We expect this trend to further increase as the amount of linguistic data on the web from sources such as social media, wikipedia, and online newswire increases. This paper focuses specifically on utilizing reinforcement learning (Sutton and Barto, 1998; Szepesv, 2009) to create a policy for summarizing clusters of multiple documents related to the same topic. The task of extractive automated multidocument summarization (Mani, 2001) is to select a subset of textual units, in this case sentences, from the source document cluster to form a summary of</context>
</contexts>
<marker>Chali, Joty, Hasan, 2009</marker>
<rawString>Y. Chali, S. R. Joty, and S. A. Hasan. 2009b. Complex Question Answering: Unsupervised Learning Approaches and Experiments. Journal of Artificial Intelligence Research, 35:1–47.</rawString>
</citation>
<citation valid="true">
<authors>
<author>Y Chali</author>
<author>S A Hasan</author>
<author>K Imam</author>
</authors>
<title>A Reinforcement Learning Framework for Answering Complex Questions.</title>
<date>2011</date>
<booktitle>In Proceedings of the 16th International Conference on Intelligent User Interfaces,</booktitle>
<pages>307--310</pages>
<publisher>ACM,</publisher>
<location>Palo Alto, CA, USA.</location>
<contexts>
<context position="1723" citStr="Chali et al., 2011" startWordPosition="250" endWordPosition="253">aset. Furthermore query focused extensions of our approach show an improvement of 1.37% and 2.31% for ROUGE-2 and ROUGE-SU4 respectively over query focused extensions of the state of the art with reinforcement learning on the DUC2006 dataset. 1 Introduction The multi-document summarization problem has received much attention recently (Lyngbaek, 2013; Sood, 2013; Qian and Liu, 2013) due to its ability to reduce large quantities of text to a human processable amount as well as its application in other fields such as question answering (Liu et al., 2008; Chali et al., 2009a; Chali et al., 2009b; Chali et al., 2011b). We expect this trend to further increase as the amount of linguistic data on the web from sources such as social media, wikipedia, and online newswire increases. This paper focuses specifically on utilizing reinforcement learning (Sutton and Barto, 1998; Szepesv, 2009) to create a policy for summarizing clusters of multiple documents related to the same topic. The task of extractive automated multidocument summarization (Mani, 2001) is to select a subset of textual units, in this case sentences, from the source document cluster to form a summary of the cluster in question. This extractive </context>
<context position="5874" citStr="Chali et al., 2011" startWordPosition="896" endWordPosition="899">nd feature set, it is a full framework for implementing summarization tasks using reinforcement learning and is available online for experimentation.1 The primary contributions of our experiments are as follows: • Exploration of TD(λ), 5AR5A and Approximate Policy Iteration. • Alternate REAPER reward function. • Alternate REAPER feature set. • Query focused extensions of automatic summarization using reinforcement learning. 2 Previous Work and Motivation Previous work using reinforcement learning for natural language processing tasks (Branavan et al., 2009; Wan, 2007; Ryang and Abekawa, 2012; Chali et al., 2011a; Chali et al., 2012) inspired us to use a similar approach in our experiments. Ryang and Abekawa (2012) implemented a reinforcement learning approach to 1https://github.com/codyrioux/REAPER multi-document summarization which they named Automatic Summarization using Reinforcement Learning (ASRL). ASRL uses TD(λ) to learn and then execute a policy for summarizing a cluster of documents. The algorithm performs N summarizations from a blank state to termination, updating a set of state-value predictions as it does so. From these N episodes a policy is created using the estimated state-value pair</context>
</contexts>
<marker>Chali, Hasan, Imam, 2011</marker>
<rawString>Y. Chali, S. A. Hasan, and K. Imam. 2011a. A Reinforcement Learning Framework for Answering Complex Questions. In Proceedings of the 16th International Conference on Intelligent User Interfaces, pages 307–310. ACM, Palo Alto, CA, USA.</rawString>
</citation>
<citation valid="true">
<authors>
<author>Y Chali</author>
<author>S A Hasan</author>
<author>S R Joty</author>
</authors>
<title>Improving Graph-based Random Walks for Complex Question Answering Using Syntactic, Shallow Semantic and Extended String Subsequence Kernels.</title>
<date>2011</date>
<booktitle>Information Processing and Management (IPM), Special Issue on Question Answering,</booktitle>
<pages>47--6</pages>
<contexts>
<context position="1723" citStr="Chali et al., 2011" startWordPosition="250" endWordPosition="253">aset. Furthermore query focused extensions of our approach show an improvement of 1.37% and 2.31% for ROUGE-2 and ROUGE-SU4 respectively over query focused extensions of the state of the art with reinforcement learning on the DUC2006 dataset. 1 Introduction The multi-document summarization problem has received much attention recently (Lyngbaek, 2013; Sood, 2013; Qian and Liu, 2013) due to its ability to reduce large quantities of text to a human processable amount as well as its application in other fields such as question answering (Liu et al., 2008; Chali et al., 2009a; Chali et al., 2009b; Chali et al., 2011b). We expect this trend to further increase as the amount of linguistic data on the web from sources such as social media, wikipedia, and online newswire increases. This paper focuses specifically on utilizing reinforcement learning (Sutton and Barto, 1998; Szepesv, 2009) to create a policy for summarizing clusters of multiple documents related to the same topic. The task of extractive automated multidocument summarization (Mani, 2001) is to select a subset of textual units, in this case sentences, from the source document cluster to form a summary of the cluster in question. This extractive </context>
<context position="5874" citStr="Chali et al., 2011" startWordPosition="896" endWordPosition="899">nd feature set, it is a full framework for implementing summarization tasks using reinforcement learning and is available online for experimentation.1 The primary contributions of our experiments are as follows: • Exploration of TD(λ), 5AR5A and Approximate Policy Iteration. • Alternate REAPER reward function. • Alternate REAPER feature set. • Query focused extensions of automatic summarization using reinforcement learning. 2 Previous Work and Motivation Previous work using reinforcement learning for natural language processing tasks (Branavan et al., 2009; Wan, 2007; Ryang and Abekawa, 2012; Chali et al., 2011a; Chali et al., 2012) inspired us to use a similar approach in our experiments. Ryang and Abekawa (2012) implemented a reinforcement learning approach to 1https://github.com/codyrioux/REAPER multi-document summarization which they named Automatic Summarization using Reinforcement Learning (ASRL). ASRL uses TD(λ) to learn and then execute a policy for summarizing a cluster of documents. The algorithm performs N summarizations from a blank state to termination, updating a set of state-value predictions as it does so. From these N episodes a policy is created using the estimated state-value pair</context>
</contexts>
<marker>Chali, Hasan, Joty, 2011</marker>
<rawString>Y. Chali, S. A. Hasan, and S. R. Joty. 2011b. Improving Graph-based Random Walks for Complex Question Answering Using Syntactic, Shallow Semantic and Extended String Subsequence Kernels. Information Processing and Management (IPM), Special Issue on Question Answering, 47(6):843–855.</rawString>
</citation>
<citation valid="true">
<authors>
<author>Y Chali</author>
<author>S A Hasan</author>
<author>K Imam</author>
</authors>
<title>Improving the Performance of the Reinforcement Learning Model for Answering Complex Questions.</title>
<date>2012</date>
<booktitle>In Proceedings of the 21st ACM Conference on Information and Knowledge Management (CIKM 2012),</booktitle>
<pages>2499--2502</pages>
<publisher>ACM, Maui,</publisher>
<location>Hawaii, USA.</location>
<contexts>
<context position="5896" citStr="Chali et al., 2012" startWordPosition="900" endWordPosition="903"> a full framework for implementing summarization tasks using reinforcement learning and is available online for experimentation.1 The primary contributions of our experiments are as follows: • Exploration of TD(λ), 5AR5A and Approximate Policy Iteration. • Alternate REAPER reward function. • Alternate REAPER feature set. • Query focused extensions of automatic summarization using reinforcement learning. 2 Previous Work and Motivation Previous work using reinforcement learning for natural language processing tasks (Branavan et al., 2009; Wan, 2007; Ryang and Abekawa, 2012; Chali et al., 2011a; Chali et al., 2012) inspired us to use a similar approach in our experiments. Ryang and Abekawa (2012) implemented a reinforcement learning approach to 1https://github.com/codyrioux/REAPER multi-document summarization which they named Automatic Summarization using Reinforcement Learning (ASRL). ASRL uses TD(λ) to learn and then execute a policy for summarizing a cluster of documents. The algorithm performs N summarizations from a blank state to termination, updating a set of state-value predictions as it does so. From these N episodes a policy is created using the estimated state-value pairs, this policy greedil</context>
</contexts>
<marker>Chali, Hasan, Imam, 2012</marker>
<rawString>Y. Chali, S. A. Hasan, and K. Imam. 2012. Improving the Performance of the Reinforcement Learning Model for Answering Complex Questions. In Proceedings of the 21st ACM Conference on Information and Knowledge Management (CIKM 2012), pages 2499–2502. ACM, Maui, Hawaii, USA.</rawString>
</citation>
<citation valid="true">
<authors>
<author>J Conroy</author>
<author>J Goldstein</author>
<author>D Leary</author>
</authors>
<title>Left-Brain / Right-Brain Multi-Document Summarization.</title>
<date>2004</date>
<booktitle>In Proceedings of the Document Un- derstanding Conference (DUC</booktitle>
<contexts>
<context position="18689" citStr="Conroy et al., 2004" startWordPosition="3128" endWordPosition="3131">iginal (unstemmed, with stop-words) state and output to disk. Config R-1 R-2 R-L REAPER 0.40339 0.11397 0.36574 ASRL 0.39013 0.09479 0.33769 MCKP 0.39033 0.09613 0.34225 PEER65 0.38279 0.09217 0.33099 ILP 0.34712 0.07528 0.31241 GREEDY 0.30618 0.06400 0.27507 reward(s) = � −1, if length(s) &gt; k score(s) otherwise (19) score0(q, s) = QSim(q, s) + (1 − Q)score(s) (18) 2ROUGE-1.5.5 run with -m -s -p 0 Table 1: Experimental results with ROUGE-1, ROUGE-2 and ROUGE-L scores on DUC2004. 685 Table 1 presents results for REAPER, ASRL (Ryang and Abekawa, 2012), MCKP (Takamura and Okumura, 2009), PEER65 (Conroy et al., 2004) , and GREEDY (Ryang and Abekawa, 2012) algorithms on the same task. This allows us to make a direct comparison with the results of Ryang and Abekawa (2012). REAPER results are shown using the TD(λ) algorithm, REAPER reward function, and ASRL feature set. This is to establish the validity of the reward function holding all other factors constant. REAPER results for ROUGE-1, ROUGE-2 and ROUGE-L are statistically significant compared to the result set presented in Table 2 of Ryang and Abekawa (2012) using p &lt; 0.01. Run R-1 R-2 R-L REAPER 0 0.39536 0.10679 0.35654 REAPER 1 0.40176 0.11048 0.36450</context>
</contexts>
<marker>Conroy, Goldstein, Leary, 2004</marker>
<rawString>J. Conroy, J. Goldstein, and D. Leary. 2004. Left-Brain / Right-Brain Multi-Document Summarization. In Proceedings of the Document Un- derstanding Conference (DUC 2004).</rawString>
</citation>
<citation valid="true">
<authors>
<author>D S Hirschberg</author>
</authors>
<title>Algorithms for the Longest Common Subsequence Problem.</title>
<date>1977</date>
<journal>In Journal of the ACM,</journal>
<volume>24</volume>
<issue>4</issue>
<contexts>
<context position="15829" citStr="Hirschberg, 1977" startWordPosition="2643" endWordPosition="2644">rein unless otherwise specified. Sim has been redefined as: Sim(s) =1 * ngco(1, D, s)+ 4 * ngco(2, D, s)+ 1 * ngco(3, D, s)+ (15) 1 * ngco(4, D, s)+ 1 * rlcs(D, s) and ngco is the ngram co-occurence score metric as defined by Lin (2004). ngco(n, D, s) = Er∈D Engram∈r Countmatch(ngram) ES,∈D Engram∈r Count(ngram) (16) Where n is the n-gram count for example 2 for bigrams, D is the set of documents, and s is the summary in question. Countmatch is the maximum number of times the ngram occurred in either D or s. The rlcs(R, S) is also a recall oriented measure based on longest common subsequence (Hirschberg, 1977). Recall was selected as DUC2004 tasks favoured a Q value for F-Measure (Lin, 2004) high enough that only recall would be considered. lcs is the longest common subsequence, and length(D) is the total number of tokens in the reference set D. lcs(D, s) rlcs(D, s) = (17) length(D) We are measuring similarity between sentences and our entire reference set, and thusly our D is the set of documents defined in section 3. This is also a delayed reward as the provided reward is zero until the summary is terminal. 5.2.1 Query Focused Rewards We have proposed an extension to both reward functions to allo</context>
</contexts>
<marker>Hirschberg, 1977</marker>
<rawString>D. S. Hirschberg. 1977. Algorithms for the Longest Common Subsequence Problem. In Journal of the ACM, 24(4):664–675, October.</rawString>
</citation>
<citation valid="true">
<authors>
<author>M Lagoudakis</author>
<author>R Parr</author>
</authors>
<title>Reinforcement learning as classification: Leveraging modern classifiers.</title>
<date>2003</date>
<booktitle>In Proceedings of the Twentieth International Conference on Machine Learning,</booktitle>
<pages>20--1</pages>
<contexts>
<context position="3375" citStr="Lagoudakis and Parr, 2003" startWordPosition="517" endWordPosition="520">arying the feature space and utilizing alternate reward functions. The TD(λ) learner used in Ryang and Abekawa (2012) is a very early reinforcement learning implementation. We explore the option of leveraging more recent research in reinforcement learning algorithms to improve results. To this end we explore the use of 5AR5A which is a derivative of TD(λ) that models the action space in addition to the state space modelled by TD(λ). Furthermore we explore the use of an algorithm not based on temporal difference methods, but instead on policy iteration techniques. Approximate Policy Iteration (Lagoudakis and Parr, 2003) generates a policy, then evaluates and iterates until convergence. The reward function in Ryang and Abekawa (2012) is a delayed reward based on tf∗idf values. We further explore the reward space by introducing similarity metric calculations used in ROUGE (Lin, 2004) and base our ideas on Saggion et al. (2010). The difference between immediate rewards and delayed rewards is that the learner re681 Proceedings of the 2014 Conference on Empirical Methods in Natural Language Processing (EMNLP), pages 681–690, October 25-29, 2014, Doha, Qatar. c�2014 Association for Computational Linguistics ceives</context>
<context position="12052" citStr="Lagoudakis and Parr, 2003" startWordPosition="1929" endWordPosition="1932">are selected probabilistically using softmax selection (Sutton and Barto, 1998) from a Boltzmann distribution. As the value of τ approaches 0 the distribution becomes greedier. 4.2 5AR5A 5AR5A is implemented in a very similar manner and shares αk, τk, φ(s), m, and policy(s) with the TD(λ) implementation above. 5AR5A is also a temporal difference algorithm and thus behaves similarly to TD(λ) with the exception that values are estimated not only on the state s but a stateaction pair [s, a]. 4.3 Approximate Policy Iteration The third algorithm in our experiment uses Approximate Policy Iteration (Lagoudakis and Parr, 2003) to implement a reinforcement learner. The 683 novelty introduced by (Lagoudakis and Parr, 2003) is that they eschew standard representations for a policy and instead use a classifier to represent the current policy π. Further details on the algorithm can be obtained from Lagoudakis and Parr (2003). 5 Experiments Our state space S is represented simply as a threetuple [s a f] in which s is the set of textual units (sentences) that have been added to the summary, a is a sequence of actions that have been performed on the summary and f is a boolean with value 0 representing non-terminal states a</context>
</contexts>
<marker>Lagoudakis, Parr, 2003</marker>
<rawString>M. Lagoudakis and R. Parr. 2003. Reinforcement learning as classification: Leveraging modern classifiers. In Proceedings of the Twentieth International Conference on Machine Learning, 20(1):424.</rawString>
</citation>
<citation valid="true">
<authors>
<author>J Li</author>
<author>S Li</author>
</authors>
<title>A Novel Feature-based Bayesian Model for Query Focused Multi-document Summarization.</title>
<date>2013</date>
<booktitle>In Transactions of the Association for Computational Linguistics,</booktitle>
<pages>1--89</pages>
<contexts>
<context position="7663" citStr="Li and Li, 2013" startWordPosition="1179" endWordPosition="1182">function. The intuition gathered from this is that if the optimal ILP algorithm did not outperform the suboptimal ASRL on the ROUGE evaluation, using the same reward function, then there is clearly room for improvement in the reward function’s ability to accurately model values in the state space. Furthermore one may expect to achieve a performance boost exploiting more recent research by utilizing an algorithm that intends to improve upon the concepts on which TD(λ) is based. These provide the motivation for the remainder of the research preformed. Query focused multi-document summarization (Li and Li, 2013; Chali and Hasan, 2012b; Yin et al., 2012; Wang et al., 2013) has recently gained much attention due to increasing amounts of textual data, as well as increasingly specific user demands for extracting information from said data. This is reflected in the query focused tasks run in the Document Understanding Conference (DUC) and Text Analysis Conference (TAC) over the past decade. This has motivated us to design and implement query focused extensions to these reinforcement learning approaches to summarization. There has been some research into the effects of sentence compression on the output o</context>
</contexts>
<marker>Li, Li, 2013</marker>
<rawString>J. Li and S. Li. 2013. A Novel Feature-based Bayesian Model for Query Focused Multi-document Summarization. In Transactions of the Association for Computational Linguistics, 1:89–98.</rawString>
</citation>
<citation valid="true">
<authors>
<author>C Lin</author>
<author>M Rey</author>
</authors>
<title>Improving Summarization Performance by Sentence Compression A Pilot Study.</title>
<date>2003</date>
<booktitle>In Proceedings of the Sixth International Workshop on Information Retrieval with Asian Languages.</booktitle>
<contexts>
<context position="8451" citStr="Lin and Rey, 2003" startWordPosition="1302" endWordPosition="1305">ic user demands for extracting information from said data. This is reflected in the query focused tasks run in the Document Understanding Conference (DUC) and Text Analysis Conference (TAC) over the past decade. This has motivated us to design and implement query focused extensions to these reinforcement learning approaches to summarization. There has been some research into the effects of sentence compression on the output of automatic summarization systems (Chali and Hasan, 2012a), specifically the evaluation results garnered from compressing sentences before evaluation (Qian and Liu, 2013; Lin and Rey, 2003; Ryang and 682 Abekawa, 2012). However Ryang and Abekawa (2012) found this technique to be ineffective in improving ROUGE metrics using a similar reinforcement learning approach to this paper, as a result we will not perform any further exploration into the effects of sentence compression. 3 Problem Definition We use an identical problem definition to Ryang and Abekawa (2012). Assume the given cluster of documents is represented as a set of textual units D = {x1, x2, · · · , xn} where |D |= n and xi represents a single textual unit. Textual units for the purposes of this experiment are the in</context>
</contexts>
<marker>Lin, Rey, 2003</marker>
<rawString>C. Lin and M. Rey. 2003. Improving Summarization Performance by Sentence Compression A Pilot Study. In Proceedings of the Sixth International Workshop on Information Retrieval with Asian Languages.</rawString>
</citation>
<citation valid="true">
<authors>
<author>C Lin</author>
</authors>
<title>ROUGE : A Package for Automatic Evaluation of Summaries.</title>
<date>2004</date>
<booktitle>In Information Sciences,</booktitle>
<volume>16</volume>
<issue>1</issue>
<contexts>
<context position="3642" citStr="Lin, 2004" startWordPosition="563" endWordPosition="564">ts. To this end we explore the use of 5AR5A which is a derivative of TD(λ) that models the action space in addition to the state space modelled by TD(λ). Furthermore we explore the use of an algorithm not based on temporal difference methods, but instead on policy iteration techniques. Approximate Policy Iteration (Lagoudakis and Parr, 2003) generates a policy, then evaluates and iterates until convergence. The reward function in Ryang and Abekawa (2012) is a delayed reward based on tf∗idf values. We further explore the reward space by introducing similarity metric calculations used in ROUGE (Lin, 2004) and base our ideas on Saggion et al. (2010). The difference between immediate rewards and delayed rewards is that the learner re681 Proceedings of the 2014 Conference on Empirical Methods in Natural Language Processing (EMNLP), pages 681–690, October 25-29, 2014, Doha, Qatar. c�2014 Association for Computational Linguistics ceives immediate feedback at every action in the former and feedback only at the end of the episode in the latter. We explore the performance difference of both reward types. Finally we develop query focused extensions to both reward functions and present their results on </context>
<context position="6674" citStr="Lin, 2004" startWordPosition="1021" endWordPosition="1022">R multi-document summarization which they named Automatic Summarization using Reinforcement Learning (ASRL). ASRL uses TD(λ) to learn and then execute a policy for summarizing a cluster of documents. The algorithm performs N summarizations from a blank state to termination, updating a set of state-value predictions as it does so. From these N episodes a policy is created using the estimated state-value pairs, this policy greedily selects the best action until the summary enters its terminal state. This summary produced is the output of ASRL and is evaluated using ROUGE1, ROUGE-2, and ROUGE-L (Lin, 2004). The results segment of the paper indicates that ASRL outperforms greedy and integer linear programming (ILP) techniques for the same task. There are two notable details that provide the motivation for our experiments; TD(λ) is relatively old as far as reinforcement learning (RL) algorithms are concerned, and the optimal ILP did not outperform ASRL using the same reward function. The intuition gathered from this is that if the optimal ILP algorithm did not outperform the suboptimal ASRL on the ROUGE evaluation, using the same reward function, then there is clearly room for improvement in the </context>
<context position="14155" citStr="Lin, 2004" startWordPosition="2317" endWordPosition="2318">edundant times a bit in the first feature is flipped on, divided by n. • Length Ratio calculated as length(s)/k where k is the length limit. • Longest common subsequence length. • Length violation bit. Set to 1 if length(s) &gt; k Summaries which exceed the length limitation k are subject to the same reduction as the ASRL feature set (Ryang and Abekawa, 2012) to an all zero vector with the final bit set to one. 5.2 Reward Function Our reward function (termed as REAPER reward) is based on the n-gram concurrence score metric, and the longest-common-subsequence recall metric contained within ROUGE (Lin, 2004). [st , at , 0] a=insertxi −−−−−−−) [st U xi , at U a , 0] (8) reward(s) = { −1, if length(s) &gt; k [st , at , 0] :finish score(s) if s is terminal −−−−) [st ,at , 1] (9) 0 otherwise (11) [st , at+1 ,1] any −−) [st , at ,1] (10) Insertion adds both the content of the textual unit xi to the set s as well as the action itself to set a. Conversely finishing does not alter s or a but it flips the f bit to on. Notice from (10) that once a state is terminal any further actions have no effect. 5.1 Feature Space We present an alternate feature set called REAPER feature set based on the ideas presented i</context>
<context position="15448" citStr="Lin (2004)" startWordPosition="2575" endWordPosition="2576">e previous Where score is defined identically to ASRL, with the exception that Sim is a new equation based on ROUGE metrics. �score(s) = λsRel(xi)− xi∈S � (1 − λs)Red(xi, xj) xi,xj∈S,i&lt;j (12) Rel(xi) = Sim(xi, D) + Pos(xi)−1 (13) Red(xi, xj) = Sim(xi, xj) (14) 684 Ryang and Abekawa (2012) experimentally determined a value of 0.9 for the As parameter. That value is used herein unless otherwise specified. Sim has been redefined as: Sim(s) =1 * ngco(1, D, s)+ 4 * ngco(2, D, s)+ 1 * ngco(3, D, s)+ (15) 1 * ngco(4, D, s)+ 1 * rlcs(D, s) and ngco is the ngram co-occurence score metric as defined by Lin (2004). ngco(n, D, s) = Er∈D Engram∈r Countmatch(ngram) ES,∈D Engram∈r Count(ngram) (16) Where n is the n-gram count for example 2 for bigrams, D is the set of documents, and s is the summary in question. Countmatch is the maximum number of times the ngram occurred in either D or s. The rlcs(R, S) is also a recall oriented measure based on longest common subsequence (Hirschberg, 1977). Recall was selected as DUC2004 tasks favoured a Q value for F-Measure (Lin, 2004) high enough that only recall would be considered. lcs is the longest common subsequence, and length(D) is the total number of tokens in</context>
</contexts>
<marker>Lin, 2004</marker>
<rawString>C. Lin. 2004. ROUGE : A Package for Automatic Evaluation of Summaries. In Information Sciences, 16(1):25–26.</rawString>
</citation>
<citation valid="true">
<authors>
<author>Y Liu</author>
<author>S Li</author>
<author>Y Cao</author>
<author>C Lin</author>
<author>D Han</author>
<author>Y Yu</author>
</authors>
<title>Understanding and Summarizing Answers in Community-Based Question Answering Services.</title>
<date>2008</date>
<booktitle>In COLING ’08 Proceedings of the 22nd International Conference on Computational Linguistics, 1(August):497–504.</booktitle>
<contexts>
<context position="1661" citStr="Liu et al., 2008" startWordPosition="238" endWordPosition="241">summarization with reinforcement learning on the DUC2004 dataset. Furthermore query focused extensions of our approach show an improvement of 1.37% and 2.31% for ROUGE-2 and ROUGE-SU4 respectively over query focused extensions of the state of the art with reinforcement learning on the DUC2006 dataset. 1 Introduction The multi-document summarization problem has received much attention recently (Lyngbaek, 2013; Sood, 2013; Qian and Liu, 2013) due to its ability to reduce large quantities of text to a human processable amount as well as its application in other fields such as question answering (Liu et al., 2008; Chali et al., 2009a; Chali et al., 2009b; Chali et al., 2011b). We expect this trend to further increase as the amount of linguistic data on the web from sources such as social media, wikipedia, and online newswire increases. This paper focuses specifically on utilizing reinforcement learning (Sutton and Barto, 1998; Szepesv, 2009) to create a policy for summarizing clusters of multiple documents related to the same topic. The task of extractive automated multidocument summarization (Mani, 2001) is to select a subset of textual units, in this case sentences, from the source document cluster </context>
</contexts>
<marker>Liu, Li, Cao, Lin, Han, Yu, 2008</marker>
<rawString>Y. Liu, S. Li, Y. Cao, C. Lin, D. Han, and Y. Yu. 2008. Understanding and Summarizing Answers in Community-Based Question Answering Services. In COLING ’08 Proceedings of the 22nd International Conference on Computational Linguistics, 1(August):497–504.</rawString>
</citation>
<citation valid="true">
<authors>
<author>S Lyngbaek</author>
</authors>
<title>SPORK: A Summarization Pipeline for Online Repositories of Knowledge.</title>
<date>2013</date>
<tech>M.sc. thesis,</tech>
<institution>California Polytechnic State University.</institution>
<contexts>
<context position="1456" citStr="Lyngbaek, 2013" startWordPosition="202" endWordPosition="203">ents show a statistically significant improvement of 1.33%, 1.58%, and 2.25% for ROUGE-1, ROUGE-2 and ROUGEL scores, respectively, when compared with the performance of the state of the art in automatic summarization with reinforcement learning on the DUC2004 dataset. Furthermore query focused extensions of our approach show an improvement of 1.37% and 2.31% for ROUGE-2 and ROUGE-SU4 respectively over query focused extensions of the state of the art with reinforcement learning on the DUC2006 dataset. 1 Introduction The multi-document summarization problem has received much attention recently (Lyngbaek, 2013; Sood, 2013; Qian and Liu, 2013) due to its ability to reduce large quantities of text to a human processable amount as well as its application in other fields such as question answering (Liu et al., 2008; Chali et al., 2009a; Chali et al., 2009b; Chali et al., 2011b). We expect this trend to further increase as the amount of linguistic data on the web from sources such as social media, wikipedia, and online newswire increases. This paper focuses specifically on utilizing reinforcement learning (Sutton and Barto, 1998; Szepesv, 2009) to create a policy for summarizing clusters of multiple doc</context>
</contexts>
<marker>Lyngbaek, 2013</marker>
<rawString>S. Lyngbaek. 2013. SPORK: A Summarization Pipeline for Online Repositories of Knowledge. M.sc. thesis, California Polytechnic State University.</rawString>
</citation>
<citation valid="true">
<authors>
<author>I Mani</author>
</authors>
<title>Automatic Summarization.</title>
<date>2001</date>
<publisher>John Benjamins Publishing.</publisher>
<contexts>
<context position="2163" citStr="Mani, 2001" startWordPosition="319" endWordPosition="320">an processable amount as well as its application in other fields such as question answering (Liu et al., 2008; Chali et al., 2009a; Chali et al., 2009b; Chali et al., 2011b). We expect this trend to further increase as the amount of linguistic data on the web from sources such as social media, wikipedia, and online newswire increases. This paper focuses specifically on utilizing reinforcement learning (Sutton and Barto, 1998; Szepesv, 2009) to create a policy for summarizing clusters of multiple documents related to the same topic. The task of extractive automated multidocument summarization (Mani, 2001) is to select a subset of textual units, in this case sentences, from the source document cluster to form a summary of the cluster in question. This extractive approach allows the learner to construct a summary without concern for the linguistic quality of the sentences generated, as the source documents are assumed to be of a certain linguistic quality. This paper aims to expand on the techniques used in Ryang and Abekawa (2012) which uses a reinforcement learner, specifically TD(λ), to create summaries of document clusters. We achieve this through introducing a new algorithm, varying the fea</context>
</contexts>
<marker>Mani, 2001</marker>
<rawString>I. Mani. 2001. Automatic Summarization. John Benjamins Publishing.</rawString>
</citation>
<citation valid="true">
<authors>
<author>C D Manning</author>
<author>H Sch¨utze</author>
</authors>
<title>of .</title>
<date>1999</date>
<journal>Foundations of Statistical Natural Language Processing,</journal>
<volume>26</volume>
<publisher>MIT Press.</publisher>
<marker>Manning, Sch¨utze, 1999</marker>
<rawString>C. D. Manning and H. Sch¨utze. 1999. Foundations of Statistical Natural Language Processing, volume 26 of . MIT Press.</rawString>
</citation>
<citation valid="true">
<authors>
<author>M F Porter</author>
</authors>
<title>An algorithm for suffix stripping.</title>
<date>1980</date>
<journal>Program,</journal>
<volume>14</volume>
<issue>3</issue>
<contexts>
<context position="17946" citStr="Porter, 1980" startWordPosition="3010" endWordPosition="3011"> that the score is always provided to the caller regardless of the terminal status of state s. 6 Results We first present results2 of our experiments, specifying parameters, and withholding discussion until the following section. We establish a benchmark using ASRL and other top-scoring summarization systems compared with REAPER using ROUGE. For generic multi-document summarization we run experiments on all 50 document clusters, each containing 10 documents, of DUC2004 task 2 with parameters for REAPER and ASRL fixed at A = 1, γ = 1, and k = 665. Sentences were stemmed using a Porter Stemmer (Porter, 1980) and had the ROUGE stop word set removed. All summaries were processed in this manner and then projected back into their original (unstemmed, with stop-words) state and output to disk. Config R-1 R-2 R-L REAPER 0.40339 0.11397 0.36574 ASRL 0.39013 0.09479 0.33769 MCKP 0.39033 0.09613 0.34225 PEER65 0.38279 0.09217 0.33099 ILP 0.34712 0.07528 0.31241 GREEDY 0.30618 0.06400 0.27507 reward(s) = � −1, if length(s) &gt; k score(s) otherwise (19) score0(q, s) = QSim(q, s) + (1 − Q)score(s) (18) 2ROUGE-1.5.5 run with -m -s -p 0 Table 1: Experimental results with ROUGE-1, ROUGE-2 and ROUGE-L scores on DU</context>
</contexts>
<marker>Porter, 1980</marker>
<rawString>M. F. Porter. 1980. An algorithm for suffix stripping. Program, 14(3):130–137.</rawString>
</citation>
<citation valid="true">
<authors>
<author>X Qian</author>
<author>Y Liu</author>
</authors>
<title>Fast Joint Compression and Summarization via Graph Cuts.</title>
<date>2013</date>
<booktitle>In Conference on Empirical Methods in Natural Language Processing.</booktitle>
<contexts>
<context position="1489" citStr="Qian and Liu, 2013" startWordPosition="206" endWordPosition="209">gnificant improvement of 1.33%, 1.58%, and 2.25% for ROUGE-1, ROUGE-2 and ROUGEL scores, respectively, when compared with the performance of the state of the art in automatic summarization with reinforcement learning on the DUC2004 dataset. Furthermore query focused extensions of our approach show an improvement of 1.37% and 2.31% for ROUGE-2 and ROUGE-SU4 respectively over query focused extensions of the state of the art with reinforcement learning on the DUC2006 dataset. 1 Introduction The multi-document summarization problem has received much attention recently (Lyngbaek, 2013; Sood, 2013; Qian and Liu, 2013) due to its ability to reduce large quantities of text to a human processable amount as well as its application in other fields such as question answering (Liu et al., 2008; Chali et al., 2009a; Chali et al., 2009b; Chali et al., 2011b). We expect this trend to further increase as the amount of linguistic data on the web from sources such as social media, wikipedia, and online newswire increases. This paper focuses specifically on utilizing reinforcement learning (Sutton and Barto, 1998; Szepesv, 2009) to create a policy for summarizing clusters of multiple documents related to the same topic.</context>
<context position="8432" citStr="Qian and Liu, 2013" startWordPosition="1298" endWordPosition="1301"> increasingly specific user demands for extracting information from said data. This is reflected in the query focused tasks run in the Document Understanding Conference (DUC) and Text Analysis Conference (TAC) over the past decade. This has motivated us to design and implement query focused extensions to these reinforcement learning approaches to summarization. There has been some research into the effects of sentence compression on the output of automatic summarization systems (Chali and Hasan, 2012a), specifically the evaluation results garnered from compressing sentences before evaluation (Qian and Liu, 2013; Lin and Rey, 2003; Ryang and 682 Abekawa, 2012). However Ryang and Abekawa (2012) found this technique to be ineffective in improving ROUGE metrics using a similar reinforcement learning approach to this paper, as a result we will not perform any further exploration into the effects of sentence compression. 3 Problem Definition We use an identical problem definition to Ryang and Abekawa (2012). Assume the given cluster of documents is represented as a set of textual units D = {x1, x2, · · · , xn} where |D |= n and xi represents a single textual unit. Textual units for the purposes of this ex</context>
</contexts>
<marker>Qian, Liu, 2013</marker>
<rawString>X. Qian and Y. Liu. 2013. Fast Joint Compression and Summarization via Graph Cuts. In Conference on Empirical Methods in Natural Language Processing.</rawString>
</citation>
<citation valid="true">
<authors>
<author>S Ryang</author>
<author>T Abekawa</author>
</authors>
<title>Framework of Automatic Text Summarization Using Reinforcement Learning.</title>
<date>2012</date>
<booktitle>In Proceedings of the 2012 Joint Conference on Empirical Methods in Natural Language Processing and Computational Natural Language Learning,</booktitle>
<pages>1--256</pages>
<contexts>
<context position="2596" citStr="Ryang and Abekawa (2012)" startWordPosition="393" endWordPosition="396">to, 1998; Szepesv, 2009) to create a policy for summarizing clusters of multiple documents related to the same topic. The task of extractive automated multidocument summarization (Mani, 2001) is to select a subset of textual units, in this case sentences, from the source document cluster to form a summary of the cluster in question. This extractive approach allows the learner to construct a summary without concern for the linguistic quality of the sentences generated, as the source documents are assumed to be of a certain linguistic quality. This paper aims to expand on the techniques used in Ryang and Abekawa (2012) which uses a reinforcement learner, specifically TD(λ), to create summaries of document clusters. We achieve this through introducing a new algorithm, varying the feature space and utilizing alternate reward functions. The TD(λ) learner used in Ryang and Abekawa (2012) is a very early reinforcement learning implementation. We explore the option of leveraging more recent research in reinforcement learning algorithms to improve results. To this end we explore the use of 5AR5A which is a derivative of TD(λ) that models the action space in addition to the state space modelled by TD(λ). Furthermor</context>
<context position="4450" citStr="Ryang and Abekawa (2012)" startWordPosition="688" endWordPosition="691">hods in Natural Language Processing (EMNLP), pages 681–690, October 25-29, 2014, Doha, Qatar. c�2014 Association for Computational Linguistics ceives immediate feedback at every action in the former and feedback only at the end of the episode in the latter. We explore the performance difference of both reward types. Finally we develop query focused extensions to both reward functions and present their results on more recent Document Understanding Conference (DUC) datasets which ran a query focused task. We first evaluate our systems using the DUC2004 dataset for comparison with the results in Ryang and Abekawa (2012). We then present the results of query focused reward functions against the DUC2006 dataset to provide reference with a more recent dataset and a more recent task, specifically a query-focused summarization task. Evaluations are performed using ROUGE for ROUGE-1, ROUGE-2 and ROUGE-L values for general summarization, while ROUGE-2 and ROUGE-SU4 is used for query-focused summarization. Furthermore we selected a small subset of query focused summaries to be subjected to human evaluations and present the results. Our implementation is named REAPER (Relatedness-focused Extractive Automatic summary </context>
<context position="5854" citStr="Ryang and Abekawa, 2012" startWordPosition="892" endWordPosition="895"> just a reward function and feature set, it is a full framework for implementing summarization tasks using reinforcement learning and is available online for experimentation.1 The primary contributions of our experiments are as follows: • Exploration of TD(λ), 5AR5A and Approximate Policy Iteration. • Alternate REAPER reward function. • Alternate REAPER feature set. • Query focused extensions of automatic summarization using reinforcement learning. 2 Previous Work and Motivation Previous work using reinforcement learning for natural language processing tasks (Branavan et al., 2009; Wan, 2007; Ryang and Abekawa, 2012; Chali et al., 2011a; Chali et al., 2012) inspired us to use a similar approach in our experiments. Ryang and Abekawa (2012) implemented a reinforcement learning approach to 1https://github.com/codyrioux/REAPER multi-document summarization which they named Automatic Summarization using Reinforcement Learning (ASRL). ASRL uses TD(λ) to learn and then execute a policy for summarizing a cluster of documents. The algorithm performs N summarizations from a blank state to termination, updating a set of state-value predictions as it does so. From these N episodes a policy is created using the estima</context>
<context position="8515" citStr="Ryang and Abekawa (2012)" startWordPosition="1312" endWordPosition="1315">. This is reflected in the query focused tasks run in the Document Understanding Conference (DUC) and Text Analysis Conference (TAC) over the past decade. This has motivated us to design and implement query focused extensions to these reinforcement learning approaches to summarization. There has been some research into the effects of sentence compression on the output of automatic summarization systems (Chali and Hasan, 2012a), specifically the evaluation results garnered from compressing sentences before evaluation (Qian and Liu, 2013; Lin and Rey, 2003; Ryang and 682 Abekawa, 2012). However Ryang and Abekawa (2012) found this technique to be ineffective in improving ROUGE metrics using a similar reinforcement learning approach to this paper, as a result we will not perform any further exploration into the effects of sentence compression. 3 Problem Definition We use an identical problem definition to Ryang and Abekawa (2012). Assume the given cluster of documents is represented as a set of textual units D = {x1, x2, · · · , xn} where |D |= n and xi represents a single textual unit. Textual units for the purposes of this experiment are the individual sentences in the document cluster, that is D = D1 U D2 </context>
<context position="13903" citStr="Ryang and Abekawa, 2012" startWordPosition="2274" endWordPosition="2277"> instead of tf ∗ idf words. • One bit b E 0,1 for each of the top n bigrams (Manning and Sch¨utze, 1999) present in the summary. • Coverage ratio calculated as the sum of the bits in the previous feature divided by n. • Redundancy Ratio calculated as the number of redundant times a bit in the first feature is flipped on, divided by n. • Length Ratio calculated as length(s)/k where k is the length limit. • Longest common subsequence length. • Length violation bit. Set to 1 if length(s) &gt; k Summaries which exceed the length limitation k are subject to the same reduction as the ASRL feature set (Ryang and Abekawa, 2012) to an all zero vector with the final bit set to one. 5.2 Reward Function Our reward function (termed as REAPER reward) is based on the n-gram concurrence score metric, and the longest-common-subsequence recall metric contained within ROUGE (Lin, 2004). [st , at , 0] a=insertxi −−−−−−−) [st U xi , at U a , 0] (8) reward(s) = { −1, if length(s) &gt; k [st , at , 0] :finish score(s) if s is terminal −−−−) [st ,at , 1] (9) 0 otherwise (11) [st , at+1 ,1] any −−) [st , at ,1] (10) Insertion adds both the content of the textual unit xi to the set s as well as the action itself to set a. Conversely fin</context>
<context position="15127" citStr="Ryang and Abekawa (2012)" startWordPosition="2509" endWordPosition="2512">ishing does not alter s or a but it flips the f bit to on. Notice from (10) that once a state is terminal any further actions have no effect. 5.1 Feature Space We present an alternate feature set called REAPER feature set based on the ideas presented in Ryang and Abekawa (2012). Our proposed feature set follows a similar format to the previous Where score is defined identically to ASRL, with the exception that Sim is a new equation based on ROUGE metrics. �score(s) = λsRel(xi)− xi∈S � (1 − λs)Red(xi, xj) xi,xj∈S,i&lt;j (12) Rel(xi) = Sim(xi, D) + Pos(xi)−1 (13) Red(xi, xj) = Sim(xi, xj) (14) 684 Ryang and Abekawa (2012) experimentally determined a value of 0.9 for the As parameter. That value is used herein unless otherwise specified. Sim has been redefined as: Sim(s) =1 * ngco(1, D, s)+ 4 * ngco(2, D, s)+ 1 * ngco(3, D, s)+ (15) 1 * ngco(4, D, s)+ 1 * rlcs(D, s) and ngco is the ngram co-occurence score metric as defined by Lin (2004). ngco(n, D, s) = Er∈D Engram∈r Countmatch(ngram) ES,∈D Engram∈r Count(ngram) (16) Where n is the n-gram count for example 2 for bigrams, D is the set of documents, and s is the summary in question. Countmatch is the maximum number of times the ngram occurred in either D or s. T</context>
<context position="18624" citStr="Ryang and Abekawa, 2012" startWordPosition="3118" endWordPosition="3121">s were processed in this manner and then projected back into their original (unstemmed, with stop-words) state and output to disk. Config R-1 R-2 R-L REAPER 0.40339 0.11397 0.36574 ASRL 0.39013 0.09479 0.33769 MCKP 0.39033 0.09613 0.34225 PEER65 0.38279 0.09217 0.33099 ILP 0.34712 0.07528 0.31241 GREEDY 0.30618 0.06400 0.27507 reward(s) = � −1, if length(s) &gt; k score(s) otherwise (19) score0(q, s) = QSim(q, s) + (1 − Q)score(s) (18) 2ROUGE-1.5.5 run with -m -s -p 0 Table 1: Experimental results with ROUGE-1, ROUGE-2 and ROUGE-L scores on DUC2004. 685 Table 1 presents results for REAPER, ASRL (Ryang and Abekawa, 2012), MCKP (Takamura and Okumura, 2009), PEER65 (Conroy et al., 2004) , and GREEDY (Ryang and Abekawa, 2012) algorithms on the same task. This allows us to make a direct comparison with the results of Ryang and Abekawa (2012). REAPER results are shown using the TD(λ) algorithm, REAPER reward function, and ASRL feature set. This is to establish the validity of the reward function holding all other factors constant. REAPER results for ROUGE-1, ROUGE-2 and ROUGE-L are statistically significant compared to the result set presented in Table 2 of Ryang and Abekawa (2012) using p &lt; 0.01. Run R-1 R-2 R-L </context>
<context position="19948" citStr="Ryang and Abekawa (2012)" startWordPosition="3334" endWordPosition="3337">EAPER 3 0.39505 0.11021 0.35972 REAPER 4 0.40259 0.11396 0.36539 REAPER 5 0.40184 0.11306 0.36391 REAPER 6 0.39311 0.10873 0.35481 REAPER 7 0.39814 0.11001 0.35786 REAPER 8 0.39443 0.10740 0.35586 REAPER 9 0.40233 0.11397 0.36483 Average 0.39773 0.11063 0.36018 Table 2: REAPER run 10 times on the DUC2004. We present the results of 10 runs of REAPER, with REAPER feature set.. As with ASRL, REAPER does not converge on a stable solution which is attributable to the random elements of TD(λ). Results in all three metrics are again statistically significant compared to ASRL results presented in the Ryang and Abekawa (2012) paper. All further REAPER experiments use the bigram oriented feature space. Reward R-1 R-2 R-L Delayed 0.39773 0.11397 0.36018 Immediate 0.32981 0.07709 0.30003 Table 3: REAPER with delayed and immediate rewards on DUC2004. Table 3 shows the performance difference of REAPER when using a delayed and immediate reward. The immediate version of REAPER provides feedback on every learning step, unlike the delayed version which only provides score at the end of the episode. Features R-1 R-2 R-L ASRL 0.40339 0.11397 0.36574 REAPER 0.40259 0.11396 0.36539 Table 4: REAPER with alternate feature spaces</context>
<context position="26801" citStr="Ryang and Abekawa (2012)" startWordPosition="4475" endWordPosition="4478">e added benefit of being much more performant computationally as far fewer reward calculations need be done. 687 Figure 1: ASRL normalized reward. Figure 2: REAPER normalized reward. 7.2 Feature Space The feature space experiments in Table 4 seem to imply that REAPER performs similarly with both feature sets. We are confident that an improvement could be made through further experimentation. Feature engineering, however, is a very broad field and we plan to pursue this topic in depth in the future. 7.3 Algorithms TD(λ) significantly outperformed both 5AR5A and API in the algorithm comparison. Ryang and Abekawa (2012) conclude that the feature space is largely responsible for the algorithm performance. This is due to the fact that poor states such as those that are too long, or those that contain few important words will reduce to the same feature set and receive negative rewards collectively. 5AR5A loses this benefit as a result of its modelling of state-action pairs. API on the other hand may have suffered a performance loss due to its requirements of an immediate reward, this is because when using a delayed reward if the trajectory of a rollout does not reach a terminal state the algorithm will not be a</context>
</contexts>
<marker>Ryang, Abekawa, 2012</marker>
<rawString>S. Ryang and T. Abekawa. 2012. Framework of Automatic Text Summarization Using Reinforcement Learning. In Proceedings of the 2012 Joint Conference on Empirical Methods in Natural Language Processing and Computational Natural Language Learning, 1:256–265.</rawString>
</citation>
<citation valid="true">
<authors>
<author>H Saggion</author>
<author>C Iria</author>
<author>T M Juan-Manuel</author>
<author>E SanJuan</author>
</authors>
<title>Multilingual Summarization Evaluation without Human Models.</title>
<date>2010</date>
<booktitle>In COLING ’10 Proceedings of the 23rd International Conference on Computational Linguistics: Posters,</booktitle>
<pages>1--1059</pages>
<contexts>
<context position="3686" citStr="Saggion et al. (2010)" startWordPosition="570" endWordPosition="573">e of 5AR5A which is a derivative of TD(λ) that models the action space in addition to the state space modelled by TD(λ). Furthermore we explore the use of an algorithm not based on temporal difference methods, but instead on policy iteration techniques. Approximate Policy Iteration (Lagoudakis and Parr, 2003) generates a policy, then evaluates and iterates until convergence. The reward function in Ryang and Abekawa (2012) is a delayed reward based on tf∗idf values. We further explore the reward space by introducing similarity metric calculations used in ROUGE (Lin, 2004) and base our ideas on Saggion et al. (2010). The difference between immediate rewards and delayed rewards is that the learner re681 Proceedings of the 2014 Conference on Empirical Methods in Natural Language Processing (EMNLP), pages 681–690, October 25-29, 2014, Doha, Qatar. c�2014 Association for Computational Linguistics ceives immediate feedback at every action in the former and feedback only at the end of the episode in the latter. We explore the performance difference of both reward types. Finally we develop query focused extensions to both reward functions and present their results on more recent Document Understanding Conferenc</context>
</contexts>
<marker>Saggion, Iria, Juan-Manuel, SanJuan, 2010</marker>
<rawString>H. Saggion, C. Iria, T. M. Juan-Manuel, and E. SanJuan. 2010. Multilingual Summarization Evaluation without Human Models. In COLING ’10 Proceedings of the 23rd International Conference on Computational Linguistics: Posters, 1:1059–1067.</rawString>
</citation>
<citation valid="true">
<authors>
<author>A Sood</author>
</authors>
<title>Towards Summarization of Written Text Conversations.</title>
<date>2013</date>
<tech>M.sc. thesis,</tech>
<institution>International Institute of Information Technology,</institution>
<location>Hyderabad, India.</location>
<contexts>
<context position="1468" citStr="Sood, 2013" startWordPosition="204" endWordPosition="205">istically significant improvement of 1.33%, 1.58%, and 2.25% for ROUGE-1, ROUGE-2 and ROUGEL scores, respectively, when compared with the performance of the state of the art in automatic summarization with reinforcement learning on the DUC2004 dataset. Furthermore query focused extensions of our approach show an improvement of 1.37% and 2.31% for ROUGE-2 and ROUGE-SU4 respectively over query focused extensions of the state of the art with reinforcement learning on the DUC2006 dataset. 1 Introduction The multi-document summarization problem has received much attention recently (Lyngbaek, 2013; Sood, 2013; Qian and Liu, 2013) due to its ability to reduce large quantities of text to a human processable amount as well as its application in other fields such as question answering (Liu et al., 2008; Chali et al., 2009a; Chali et al., 2009b; Chali et al., 2011b). We expect this trend to further increase as the amount of linguistic data on the web from sources such as social media, wikipedia, and online newswire increases. This paper focuses specifically on utilizing reinforcement learning (Sutton and Barto, 1998; Szepesv, 2009) to create a policy for summarizing clusters of multiple documents relat</context>
</contexts>
<marker>Sood, 2013</marker>
<rawString>A. Sood. 2013. Towards Summarization of Written Text Conversations. M.sc. thesis, International Institute of Information Technology, Hyderabad, India.</rawString>
</citation>
<citation valid="true">
<authors>
<author>R S Sutton</author>
<author>A G Barto</author>
</authors>
<title>Reinforcement Learning: An Introduction.</title>
<date>1998</date>
<publisher>MIT Press.</publisher>
<contexts>
<context position="1980" citStr="Sutton and Barto, 1998" startWordPosition="290" endWordPosition="293">tion The multi-document summarization problem has received much attention recently (Lyngbaek, 2013; Sood, 2013; Qian and Liu, 2013) due to its ability to reduce large quantities of text to a human processable amount as well as its application in other fields such as question answering (Liu et al., 2008; Chali et al., 2009a; Chali et al., 2009b; Chali et al., 2011b). We expect this trend to further increase as the amount of linguistic data on the web from sources such as social media, wikipedia, and online newswire increases. This paper focuses specifically on utilizing reinforcement learning (Sutton and Barto, 1998; Szepesv, 2009) to create a policy for summarizing clusters of multiple documents related to the same topic. The task of extractive automated multidocument summarization (Mani, 2001) is to select a subset of textual units, in this case sentences, from the source document cluster to form a summary of the cluster in question. This extractive approach allows the learner to construct a summary without concern for the linguistic quality of the sentences generated, as the source documents are assumed to be of a certain linguistic quality. This paper aims to expand on the techniques used in Ryang an</context>
<context position="10050" citStr="Sutton and Barto, 1998" startWordPosition="1590" endWordPosition="1593">. Given these parameters, and a length limitation k we can define an optimal summary s* as: s* = argmax score(s) where s C D and length(s) &lt; k (1) It is the objective of our learner to create a policy that produces the optimal summary for its provided document cluster D. Henceforth the length limitations used for general summarization will be 665 bytes, and query focused summarization will use 250 words. These limitations on summary length match those set by the Document Understanding Conferences associated with the dataset utilized in the respective experiments. 4 Algorithms TD(λ) and 5AR5A (Sutton and Barto, 1998) are temporal difference methods in which the primary difference is that TD(λ) models state value predictions, and 5AR5A models state-action value predictions. Approximate Policy Iteration (API) follows a different paradigm by iteratively improving a policy for a markov decision process until the policy converges. 4.1 TD(λ) In the ASRL implementation of TD(λ) the learning rate αk and temperature τk decay as learning progresses with the following equations with k set to the number of learning episodes that had taken place. αk = 0.001 · 101/(100 + k1.1) (2) τk = 1.0 * 0.987k−1 (3) One can infer </context>
<context position="11505" citStr="Sutton and Barto, 1998" startWordPosition="1838" endWordPosition="1841"> less exploration, this is evident in (5) below. Note that unlike traditional TD(λ) implementations the eligibility trace e resets on every episode. The reasons for this will become evident in the experiments section of the paper in which λ = 1,γ = 1 and thus there is no decay during an episode and complete decay after an episode. The same holds true for 5AR5A below. The action-value estimation Q(s, a) is approximated as: Q(s, a) = r + γV (s&apos;) (4) The policy is implemented as such: eQ(s,a)/τ policy(a|s; θ; τ) = (5) Ea∈A eQ(s,a)/τ Actions are selected probabilistically using softmax selection (Sutton and Barto, 1998) from a Boltzmann distribution. As the value of τ approaches 0 the distribution becomes greedier. 4.2 5AR5A 5AR5A is implemented in a very similar manner and shares αk, τk, φ(s), m, and policy(s) with the TD(λ) implementation above. 5AR5A is also a temporal difference algorithm and thus behaves similarly to TD(λ) with the exception that values are estimated not only on the state s but a stateaction pair [s, a]. 4.3 Approximate Policy Iteration The third algorithm in our experiment uses Approximate Policy Iteration (Lagoudakis and Parr, 2003) to implement a reinforcement learner. The 683 novelt</context>
</contexts>
<marker>Sutton, Barto, 1998</marker>
<rawString>R. S. Sutton and A. G. Barto. 1998. Reinforcement Learning: An Introduction. MIT Press.</rawString>
</citation>
<citation valid="true">
<authors>
<author>C A Szepesv</author>
</authors>
<title>Algorithms for Reinforcement Learning.</title>
<date>2009</date>
<publisher>Morgan &amp; Claypool Publishers.</publisher>
<contexts>
<context position="1996" citStr="Szepesv, 2009" startWordPosition="294" endWordPosition="295">summarization problem has received much attention recently (Lyngbaek, 2013; Sood, 2013; Qian and Liu, 2013) due to its ability to reduce large quantities of text to a human processable amount as well as its application in other fields such as question answering (Liu et al., 2008; Chali et al., 2009a; Chali et al., 2009b; Chali et al., 2011b). We expect this trend to further increase as the amount of linguistic data on the web from sources such as social media, wikipedia, and online newswire increases. This paper focuses specifically on utilizing reinforcement learning (Sutton and Barto, 1998; Szepesv, 2009) to create a policy for summarizing clusters of multiple documents related to the same topic. The task of extractive automated multidocument summarization (Mani, 2001) is to select a subset of textual units, in this case sentences, from the source document cluster to form a summary of the cluster in question. This extractive approach allows the learner to construct a summary without concern for the linguistic quality of the sentences generated, as the source documents are assumed to be of a certain linguistic quality. This paper aims to expand on the techniques used in Ryang and Abekawa (2012)</context>
</contexts>
<marker>Szepesv, 2009</marker>
<rawString>C. A. Szepesv. 2009. Algorithms for Reinforcement Learning. Morgan &amp; Claypool Publishers.</rawString>
</citation>
<citation valid="true">
<authors>
<author>H Takamura</author>
<author>M Okumura</author>
</authors>
<title>Text Summarization Model based on Maximum Coverage Problem and its Variant.</title>
<date>2009</date>
<booktitle>In EACL ’09 Proceedings of the 12th Conference of the European Chapter of the Association for Computational Linguistics,</booktitle>
<location>(April):781–789.</location>
<contexts>
<context position="18659" citStr="Takamura and Okumura, 2009" startWordPosition="3123" endWordPosition="3126">and then projected back into their original (unstemmed, with stop-words) state and output to disk. Config R-1 R-2 R-L REAPER 0.40339 0.11397 0.36574 ASRL 0.39013 0.09479 0.33769 MCKP 0.39033 0.09613 0.34225 PEER65 0.38279 0.09217 0.33099 ILP 0.34712 0.07528 0.31241 GREEDY 0.30618 0.06400 0.27507 reward(s) = � −1, if length(s) &gt; k score(s) otherwise (19) score0(q, s) = QSim(q, s) + (1 − Q)score(s) (18) 2ROUGE-1.5.5 run with -m -s -p 0 Table 1: Experimental results with ROUGE-1, ROUGE-2 and ROUGE-L scores on DUC2004. 685 Table 1 presents results for REAPER, ASRL (Ryang and Abekawa, 2012), MCKP (Takamura and Okumura, 2009), PEER65 (Conroy et al., 2004) , and GREEDY (Ryang and Abekawa, 2012) algorithms on the same task. This allows us to make a direct comparison with the results of Ryang and Abekawa (2012). REAPER results are shown using the TD(λ) algorithm, REAPER reward function, and ASRL feature set. This is to establish the validity of the reward function holding all other factors constant. REAPER results for ROUGE-1, ROUGE-2 and ROUGE-L are statistically significant compared to the result set presented in Table 2 of Ryang and Abekawa (2012) using p &lt; 0.01. Run R-1 R-2 R-L REAPER 0 0.39536 0.10679 0.35654 RE</context>
</contexts>
<marker>Takamura, Okumura, 2009</marker>
<rawString>H. Takamura and M. Okumura. 2009. Text Summarization Model based on Maximum Coverage Problem and its Variant. In EACL ’09 Proceedings of the 12th Conference of the European Chapter of the Association for Computational Linguistics, (April):781–789.</rawString>
</citation>
<citation valid="true">
<authors>
<author>X Wan</author>
</authors>
<title>Towards an Iterative Reinforcement Approach for Simultaneous Document Summarization and Keyword Extraction.</title>
<date>2007</date>
<booktitle>In Proceedings of the 45th Annual Meeting of the Association of Computational Linguistics,</booktitle>
<pages>552--559</pages>
<contexts>
<context position="5829" citStr="Wan, 2007" startWordPosition="890" endWordPosition="891">APER is not just a reward function and feature set, it is a full framework for implementing summarization tasks using reinforcement learning and is available online for experimentation.1 The primary contributions of our experiments are as follows: • Exploration of TD(λ), 5AR5A and Approximate Policy Iteration. • Alternate REAPER reward function. • Alternate REAPER feature set. • Query focused extensions of automatic summarization using reinforcement learning. 2 Previous Work and Motivation Previous work using reinforcement learning for natural language processing tasks (Branavan et al., 2009; Wan, 2007; Ryang and Abekawa, 2012; Chali et al., 2011a; Chali et al., 2012) inspired us to use a similar approach in our experiments. Ryang and Abekawa (2012) implemented a reinforcement learning approach to 1https://github.com/codyrioux/REAPER multi-document summarization which they named Automatic Summarization using Reinforcement Learning (ASRL). ASRL uses TD(λ) to learn and then execute a policy for summarizing a cluster of documents. The algorithm performs N summarizations from a blank state to termination, updating a set of state-value predictions as it does so. From these N episodes a policy is</context>
</contexts>
<marker>Wan, 2007</marker>
<rawString>X Wan. 2007. Towards an Iterative Reinforcement Approach for Simultaneous Document Summarization and Keyword Extraction. In Proceedings of the 45th Annual Meeting of the Association of Computational Linguistics, pages 552–559.</rawString>
</citation>
<citation valid="true">
<authors>
<author>L Wang</author>
<author>H Raghavan</author>
<author>V Castelli</author>
<author>R Florian</author>
<author>C Cardie</author>
</authors>
<title>A Sentence Compression Based Framework to Query-Focused.</title>
<date>2013</date>
<booktitle>In Proceedings of the 51st Annual Meeting of the Association for Computational Linguistics,</booktitle>
<pages>1--1384</pages>
<contexts>
<context position="7725" citStr="Wang et al., 2013" startWordPosition="1191" endWordPosition="1194">ptimal ILP algorithm did not outperform the suboptimal ASRL on the ROUGE evaluation, using the same reward function, then there is clearly room for improvement in the reward function’s ability to accurately model values in the state space. Furthermore one may expect to achieve a performance boost exploiting more recent research by utilizing an algorithm that intends to improve upon the concepts on which TD(λ) is based. These provide the motivation for the remainder of the research preformed. Query focused multi-document summarization (Li and Li, 2013; Chali and Hasan, 2012b; Yin et al., 2012; Wang et al., 2013) has recently gained much attention due to increasing amounts of textual data, as well as increasingly specific user demands for extracting information from said data. This is reflected in the query focused tasks run in the Document Understanding Conference (DUC) and Text Analysis Conference (TAC) over the past decade. This has motivated us to design and implement query focused extensions to these reinforcement learning approaches to summarization. There has been some research into the effects of sentence compression on the output of automatic summarization systems (Chali and Hasan, 2012a), sp</context>
</contexts>
<marker>Wang, Raghavan, Castelli, Florian, Cardie, 2013</marker>
<rawString>L. Wang, H. Raghavan, V. Castelli, R. Florian, and C. Cardie. 2013. A Sentence Compression Based Framework to Query-Focused. In Proceedings of the 51st Annual Meeting of the Association for Computational Linguistics, 1:1384–1394.</rawString>
</citation>
<citation valid="true">
<authors>
<author>W Yin</author>
<author>Y Pei</author>
<author>F Zhang</author>
<author>L Huang</author>
</authors>
<title>Queryfocused multi-document summarization based on query-sensitive feature space.</title>
<date>2012</date>
<booktitle>In Proceedings of the 21st ACM international conference on Information and knowledge management - CIKM ’12,</booktitle>
<pages>1652</pages>
<contexts>
<context position="7705" citStr="Yin et al., 2012" startWordPosition="1187" endWordPosition="1190">s is that if the optimal ILP algorithm did not outperform the suboptimal ASRL on the ROUGE evaluation, using the same reward function, then there is clearly room for improvement in the reward function’s ability to accurately model values in the state space. Furthermore one may expect to achieve a performance boost exploiting more recent research by utilizing an algorithm that intends to improve upon the concepts on which TD(λ) is based. These provide the motivation for the remainder of the research preformed. Query focused multi-document summarization (Li and Li, 2013; Chali and Hasan, 2012b; Yin et al., 2012; Wang et al., 2013) has recently gained much attention due to increasing amounts of textual data, as well as increasingly specific user demands for extracting information from said data. This is reflected in the query focused tasks run in the Document Understanding Conference (DUC) and Text Analysis Conference (TAC) over the past decade. This has motivated us to design and implement query focused extensions to these reinforcement learning approaches to summarization. There has been some research into the effects of sentence compression on the output of automatic summarization systems (Chali a</context>
</contexts>
<marker>Yin, Pei, Zhang, Huang, 2012</marker>
<rawString>W. Yin, Y. Pei, F. Zhang, and L. Huang. 2012. Queryfocused multi-document summarization based on query-sensitive feature space. In Proceedings of the 21st ACM international conference on Information and knowledge management - CIKM ’12, page 1652.</rawString>
</citation>
</citationList>
</algorithm>
</algorithms>
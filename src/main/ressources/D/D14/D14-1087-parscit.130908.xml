<?xml version="1.0" encoding="UTF-8"?>
<algorithms version="110505">
<algorithm name="SectLabel" version="110505">
<variant no="0" confidence="0.000001">
<title confidence="0.99694">
Unsupervised Template Mining for Semantic Category Understanding
</title>
<author confidence="0.999269">
Lei Shi1,2∗, Shuming Shi3, Chin-Yew Lin3, Yi-Dong Shen1, Yong Rui3
</author>
<affiliation confidence="0.9274285">
1State Key Laboratory of Computer Science,
Institute of Software, Chinese Academy of Sciences
2University of Chinese Academy of Sciences
3Microsoft Research
</affiliation>
<email confidence="0.991803">
{shilei,ydshen}@ios.ac.cn
{shumings,cyl,yongrui}@microsoft.com
</email>
<sectionHeader confidence="0.994748" genericHeader="abstract">
Abstract
</sectionHeader>
<bodyText confidence="0.999881444444445">
We propose an unsupervised approach to
constructing templates from a large collec-
tion of semantic category names, and use
the templates as the semantic representa-
tion of categories. The main challenge is
that many terms have multiple meanings,
resulting in a lot of wrong templates. Sta-
tistical data and semantic knowledge are
extracted from a web corpus to improve
template generation. A nonlinear scoring
function is proposed and demonstrated to
be effective. Experiments show that our
approach achieves significantly better re-
sults than baseline methods. As an imme-
diate application, we apply the extracted
templates to the cleaning of a category col-
lection and see promising results (preci-
sion improved from 81% to 89%).
</bodyText>
<sectionHeader confidence="0.998783" genericHeader="keywords">
1 Introduction
</sectionHeader>
<bodyText confidence="0.995163375">
A semantic category is a collection of items shar-
ing common semantic properties. For example,
all cities in Germany form a semantic category
named “city in Germany” or “German city”. In
Wikipedia, the category names of an entity are
manually edited and displayed at the end of the
page for the entity. There have been quite a lot of
approaches (Hearst, 1992; Pantel and Ravichan-
dran, 2004; Van Durme and Pasca, 2008; Zhang et
al., 2011) in the literature to automatically extract-
ing category names and instances (also called is-a
or hypernymy relations) from the web.
Most existing work simply treats a category
name as a text string containing one or multiple
words, without caring about its internal structure.
In this paper, we explore the semantic structure
of category names (or simply called “categories”).
∗This work was performed when the first author was vis-
iting Microsoft Research Asia.
For example, both “CEO of General Motors” and
“CEO of Yahoo” have structure “CEO of [com-
pany]”. We call such a structure a category tem-
plate. Taking a large collection of open-domain
categories as input, we construct a list of category
templates and build a mapping from categories to
templates. Figure 1 shows some example semantic
categories and their corresponding templates.
Templates can be treated as additional features
of semantic categories. The new features can be
exploited to improve some upper-layer applica-
tions like web search and question answering. In
addition, by linking categories to templates, it is
possible (for a computer program) to infer the se-
mantic meaning of the categories. For example in
Figure 1, from the two templates linking to cat-
egory “symptom of insulin deficiency”, it is rea-
sonable to interpret the category as: “a symptom
of a medical condition called insulin deficiency
which is about the deficiency of one type of hor-
mone called insulin.” In this way, our knowledge
about a category can go beyond a simple string
and its member entities. An immediate application
of templates is removing invalid category names
from a noisy category collection. Promising re-
sults are observed for this application in our ex-
periments.
An intuitive approach to this task (i.e., extract-
ing templates from a collection of category names)
</bodyText>
<subsubsectionHeader confidence="0.274876">
Semantic Categories Category templates
</subsubsectionHeader>
<figureCaption confidence="0.992869">
Figure 1: Examples of semantic categories and
their corresponding templates.
</figureCaption>
<figure confidence="0.942189823529412">
football player
[sport] player
basketball player
national holiday of Brazil national holiday of [country]
(instances: Carnival, Christmas...)
national holiday of South Africa
(instances: Heritage Day, Christmas...)
symptom of insulin deficiency
(instances: nocturia, weight loss...)
symptom of cortisol deficiency
(instances: low blood sugar...)
school in Denver
school in Houston
school in [place]
school in [city]
symptom of [medical condition]
symptom of [hormone] deficiency
</figure>
<page confidence="0.979277">
799
</page>
<note confidence="0.910363">
Proceedings of the 2014 Conference on Empirical Methods in Natural Language Processing (EMNLP), pages 799–809,
October 25-29, 2014, Doha, Qatar. c�2014 Association for Computational Linguistics
</note>
<bodyText confidence="0.996458509803921">
contains two stages: category labeling, and tem-
plate scoring.
Category labeling: Divide a category name
into multiple segments and replace some key seg-
ments with its hypernyms. As an example, as-
sume “CEO of Delphinus” is divided to three seg-
ments “CEO + of + Delphinus”; and the last seg-
ment (Delphinus) has hypernyms “constellation”,
“company”, etc. By replacing this segment with
its hypernyms, we get candidate templates “CEO
of [constellation]” (a wrong template), “CEO of
[company]”, and the like.
Template scoring: Compute the score of each
candidate template by aggregating the information
obtained in the first phase.
A major challenge here is that many segments
(like “Delphinus” in the above example) have mul-
tiple meanings. As a result, wrong hypernyms
may be adopted to generate incorrect candidate
templates (like “CEO of [constellation]”). In this
paper, we focus on improving the template scor-
ing stage, with the goal of assigning lower scores
to bad templates and larger scores to high-quality
ones.
There have been some research efforts (Third,
2012; Fernandez-Breis et al., 2010; Quesada-
Martınez et al., 2012) on exploring the structure of
category names by building patterns. However, we
automatically assign semantic types to the pattern
variables (or called arguments) while they do not.
For example, our template has the form of “city
in [country]” while their patterns are like “city in
[X]”. More details are given in the related work
section.
A similar task is query understanding, including
query tagging and query template mining. Query
tagging (Li et al., 2009; Reisinger and Pasca,
2011) corresponds to the category labeling stage
described above. It is different from template gen-
eration because the results are for one query only,
without merging the information of all queries to
generate the final templates. Category template
construction are slightly different from query tem-
plate construction. First, some useful features
such as query click-through is not available in cat-
egory template construction. Second, categories
should be valid natural language phrases, while
queries need not. For example, “city Germany” is
a query but not a valid category name. We discuss
in more details in the related work section.
Our major contributions are as follows.
</bodyText>
<listItem confidence="0.795493133333333">
1) To the best of our knowledge, this is the first
work of template generation specifically for cate-
gories in unsupervised manner.
2) We extract semantic knowledge and statisti-
cal information from a web corpus for improving
template generation. Significant performance im-
provement is obtained in our experiments.
3) We study the characteristics of the scoring
function from the viewpoint of probabilistic evi-
dence combination and demonstrate that nonlinear
functions are more effective in this task.
4) We employ the output templates to clean
our category collection mined from the web, and
get apparent quality improvement (precision im-
proved from 81% to 89%).
</listItem>
<bodyText confidence="0.994058">
After discussing related work in Section 2, we
define the problem and describe one baseline ap-
proach in Section 3. Then we introduce our ap-
proach in Section 4. Experimental results are re-
ported and analyzed in Section 5. We conclude the
paper in Section 6.
</bodyText>
<sectionHeader confidence="0.999813" genericHeader="introduction">
2 Related work
</sectionHeader>
<subsectionHeader confidence="0.862397">
Several kinds of work are related to ours.
</subsectionHeader>
<bodyText confidence="0.999235814814815">
Hypernymy relation extraction: Hypernymy
relation extraction is an important task in text min-
ing. There have been a lot of efforts (Hearst, 1992;
Pantel and Ravichandran, 2004; Van Durme and
Pasca, 2008; Zhang et al., 2011) in the literature to
extract hypernymy (or is-a) relations from the web.
Our target here is not hypernymy extraction, but
discovering the semantic structure of hypernyms
(or category names).
Category name exploration: Category name
patterns are explored and built in some ex-
isting research work. Third (2012) pro-
posed to find axiom patterns among category
names on an existing ontology. For ex-
ample, infer axiom pattern “SubClassOf(AB,
B)” from “SubClassOf(junior school school)”
and “SubClassOf(domestic mammal mammal)”.
Fernandez-Breis et al. (2010) and Quesada-
Martınez et al. (2012) proposed to find lexical pat-
terns in category names to define axioms (in med-
ical domain). One example pattern mentioned in
their papers is “[X] binding”. They need man-
ual intervention to determine what X means. The
main difference between the above work and ours
is that we automatically assign semantic types to
the pattern variables (or called arguments) while
they do not.
</bodyText>
<page confidence="0.989717">
800
</page>
<bodyText confidence="0.996221129032258">
Template mining for IE: Some research work
in information extraction (IE) involves patterns.
Yangarber (2003) and Stevenson and Greenwood
(2005) proposed to learn patterns which were in
the form of [subject, verb, object]. The category
names and learned templates in our work are not
in this form. Another difference between our work
and their work is that, their methods need a super-
vised name classifer to generate the candidate pat-
terns while our approach is unsupervised. Cham-
bers and Jurafsky (2011) leverage templates to de-
scribe an event while the templates in our work are
for understanding category names (a kind of short
text).
Query tagging/labeling: Some research work
in recent years focuses on segmenting web search
queries and assigning semantic tags to key seg-
ments. Li et al. (2009) and Li (2010) employed
CRF (Conditional Random Field) or semi-CRF
models for query tagging. A crowdsourcing-
assisted method was proposed by Han et al. (2013)
for query structure interpretation. These super-
vised or semi-supervised approaches require much
manual annotation effort. Unsupervised meth-
ods were proposed by Sarkas et al. (2010) and
Reisinger and Pasca (2011). As been discussed
in the introduction section, query tagging is only
one of the two stages of template generation. The
tagging results are for one query only, without ag-
gregating the global information of all queries to
generate the final templates.
Query template construction: Some existing
work leveraged query templates or patterns for
query understanding. A semi-supervised random
walk based method was proposed by Agarwal et
al. (2010) to generate a ranked templates list which
are relevant to a domain of interest. A predefined
domain schema and seed information is needed for
this method. Pandey and Punera (2012) proposed
an unsupervised method based on graphical mod-
els to mine query templates. The above methods
are either domain-specific (i.e., generating tem-
plates for a specific domain), or have some degree
of supervision (supervised or semi-supervised).
Cheung and Li (2012) proposed an unsupervised
method to generate query templates by the aid of
knowledge bases. An approach was proposed in
(Szpektor et al., 2011) to improve query recom-
mendation via query templates. Query session in-
formation (which is not available in our task) is
needed in this approach for templates generation.
Li et al. (2013) proposed an clustering algorithm
to group existing query templates by search intents
of users.
Compared to the open-domain unsupervised
methods for query template construction, our ap-
proach improves on two aspects. First, we propose
to incorporate multiple types of semantic knowl-
edge (e.g., term peer similarity and term clusters)
to improve template generation. Second, we pro-
pose a nonlinear template scoring function which
is demonstrated to be more effective.
</bodyText>
<sectionHeader confidence="0.764524" genericHeader="method">
3 Problem Definition and Analysis
</sectionHeader>
<subsectionHeader confidence="0.998927">
3.1 Problem definition
</subsectionHeader>
<bodyText confidence="0.999822135135135">
The goal of this paper is to construct a list of cat-
egory templates from a collection of open-domain
category names.
Input: The input is a collection of category
names, which can either be manually compiled
(like Wikipedia categories) or be automatically ex-
tracted. The categories used in our experiments
were automatically mined from the web, by fol-
lowing existing work (Hearst, 1992, Pantel and
Ravichandran 2004; Snow et al., 2005; Talukdar
et al., 2008; Zhang et al., 2011). Specifically,
we applied Hearst patterns (e.g., “NP [,] (such
as  |including) {NP, }∗ {and|or} NP”) and is-
a patterns (“NP (is|are|was|were|being) (a|an|the)
NP”) to a large corpus containing 3 billion En-
glish web pages. As a result, we obtained a
term—*hypernym bi-partite graph containing 40
million terms, 74 million hypernyms (i.e., cate-
gory names), and 321 million edges (e.g., one
example edge is “Berlin”—*“city in Germany”,
where “Berlin” is a term and “city in Germany” is
the corresponding hypernym). Then all the multi-
word hypernyms are used as the input category
collection.
Output: The output is a list of templates, each
having a score indicating how likely it is valid. A
template is a multi-word string with one headword
and at least one argument. For example, in tem-
plate “national holiday of [country]”, “holiday” is
the headword, and “[country]” is the argument.
We only consider one-argument templates in this
paper, and the case of multiple arguments is left as
future work. A template is valid if it is syntacti-
cally and semantically correct. “CEO of [constel-
lation]” (wrongly generated from “CEO of Del-
phinus”, “CEO of Aquila”, etc.) is not valid be-
cause it is semantically unreasonable.
</bodyText>
<page confidence="0.986907">
801
</page>
<subsectionHeader confidence="0.999614">
3.2 Baseline approach
</subsectionHeader>
<bodyText confidence="0.99991725">
An intuitive approach to this task contains two
stages: category labeling and template scoring.
Figure 2 shows its workflow with simple exam-
ples.
</bodyText>
<subsubsectionHeader confidence="0.778169">
3.2.1 Phase-1: Category labeling
</subsubsectionHeader>
<bodyText confidence="0.989439729166667">
At this stage, each category name is automatically
segmented and labeled; and some candidate tem-
plate tuples (CTTs) are derived based on the la-
beling results. This can be done in the following
steps.
Category segmentation: Divide each cate-
gory name into multiple segments (e.g., “holi-
day of South Africa” to “holiday + of + South
Africa”). Each segment is one word or a phrase
appearing in an entity dictionary. The dictionary
used in this paper is comprised of all Freebase
(www.freebase.com) entities.
Segment to hypernym: Find hypernyms for
every segment (except for the headword and some
trivial segments like prepositions and articles), by
referring to a term→hypernym mapping graph.
Following most existing query labeling work, we
derive the term→hypernym graph from a dump of
Freebase. Below are some examples of Freebase
types (hypernyms),
German city (id: /location/de city)
Italian province (id: /location/it province)
Poem character (id: /book/poem character)
Book (id: /book/book)
To avoid generating too fine-grained templates
like “mayor of [Germany city]” and “mayor of
[Italian city]” (semantically “mayor of [city]”
is more desirable), we discard type modifiers
and map terms to the headwords of Freebase
types. For example, “Berlin” is mapped to
“city”. In this way, we build our basic version of
term→hypernym mapping which contains 16.13
million terms and 696 hypernyms. Since “South
Africa” is both a country and a book name in Free-
base, hypernyms “country”, “book”, and others
are assigned to the segment “South Africa” in this
step.
CTT generation: Construct CTTs by choosing
one segment (called the target segment) each time
and replacing the segment with its hypernyms. An
CTT is formed by the candidate template (with
one argument), the target segment (as an argument
value), and the tuple score (indicating tuple qual-
ity). Below are example CTTs obtained after the
last segment of “holiday + of + South Africa” is
processed,
U1: (holiday of [country], South Africa, w1)
U2: (holiday of [book], South Africa, w2)
</bodyText>
<subsubsectionHeader confidence="0.715976">
3.2.2 Phase-2: Template scoring
</subsubsectionHeader>
<bodyText confidence="0.9999015">
The main objective of this stage is to merge all
the CTTs obtained from the previous stage and to
compute a final score for each template. In this
stage, the CTTs are first grouped by the first ele-
ment (i.e., the template string). For example, tu-
ples for “holiday of [country]” may include,
</bodyText>
<listItem confidence="0.719113333333333">
U1: (holiday of [country], South Africa, w1)
U2: (holiday of [country], Brazil, w2)
U3: (holiday of [country], Germany, w3)
</listItem>
<bodyText confidence="0.995259">
...
Then a scoring function is employed to calcu-
late the template score from the tuple scores. For-
mally, given n tuples U=(U1, U2..., Un) for a tem-
plate, the goal is to find a score fusion function
F(U) which yields large values for high-quality
templates and small (or zero) values for invalid
ones.
Borrowing the idea of TF-IDF from information
retrieval, a reasonable scoring function is,
</bodyText>
<equation confidence="0.985689666666667">
n
F(U) = wi · IDF(h) (1)
i=1
</equation>
<bodyText confidence="0.999712222222222">
where h is the argument type (i.e., the hypernym
of the argument value) of each tuple. TF means
the “term frequency” and IDF means the “inverse
document frequency”. An IDF function assigns
lower scores to common hypernyms (like person
and music track which contain a lot of entities).
Let DF(h) be the number of entities having hy-
pernym h, we test two IDF functions in our exper-
iments,
</bodyText>
<equation confidence="0.99997325">
1 + N (2)
IDF1(h) = log
1 + DF(h)
IDF2(h) = 1/sqrt(DF(h))
</equation>
<bodyText confidence="0.999925571428571">
where N is total number of entities in the entity
dictionary.
The next problem is estimating tuple score wi.
Please note that there is no weight or score infor-
mation in the term→hypernym mapping of Free-
base. So we have to set wi to be constant in the
baseline,
</bodyText>
<equation confidence="0.9778">
wi = 1 (3)
</equation>
<page confidence="0.992187">
802
</page>
<figureCaption confidence="0.995183">
Figure 2: Problem definition and baseline approach.
</figureCaption>
<figure confidence="0.900527068965517">
Term-hypernym mapping
Brazil 4 country
Brazil 4 book
South Africa 4 country
South Africa 4 book
...
argument value
tuple score
Phase-1: Category labeling
Phase-2: Template scoring
head argument
argument
holiday of Brazil
holiday of South Africa
...
holiday of [country], Brazil, w1
holiday of [book], Brazil, w2
holiday of [country], South Africa, w3
holiday of [book], South Africa, w4
...
Phase-2
holiday of [country], S1
holiday of [book], S2
...
Input: Category names
Candidate template tuples (CTTs)
Output: Category templates
Wikipedia
Phase-1
</figure>
<sectionHeader confidence="0.7058025" genericHeader="method">
4 Approach: Enhancing Template
Scoring
</sectionHeader>
<bodyText confidence="0.999946375">
In our approach, we follow the same framework
as in the above baseline approach, and focus on
improving the template scoring phase (i.e., phase-
2).
We try three techniques: First, a better tuple
score wz is calculated in Section 4.1 by performing
statistics on a large corpus. The corpus is a collec-
tion of 3 billion web pages crawled in early 2013
by ourselves. During this paper, we use “our web
corpus” or “our corpus” to refer to this corpus.
Second, a nonlinear function is adopted in Sec-
tion 4.2 to replace the baseline tuple fusion func-
tion (Formula 1). Third, we extract term peer sim-
ilarity and term clusters from our corpus and use
them as additional semantic knowledge to refine
template scores.
</bodyText>
<subsectionHeader confidence="0.993756">
4.1 Enhancing tuple scoring
</subsectionHeader>
<bodyText confidence="0.995713961538462">
Let’s examine the following two template tuples,
U1: (holiday of [country], South Africa, w1)
U2: (holiday of [book], South Africa, w2)
Intuitively, “South Africa” is more likely to be
a country than a book when it appears in text. So
for a reasonable tuple scoring formula, we should
have w1 &gt; w2.
The main idea is to automatically calculate
the popularity of a hypernym given a term, by
referring to a large corpus. Then by adding
the popularity information to (the edges of) the
term→hypernym graph of Freebase, we obtain a
weighted term→hypernym graph. The weighted
graph is then employed to enhance the estimation
of wz.
For popularity calculation, we apply Hearst
patterns (Hearst, 1992) and is-a patterns (“NP
(is|are|was|were|being) (a|an|the) NP”) to every
sentence of our web corpus. For a (term, hyper-
nym) pair, its popularity F is calculated as the
number of sentences in which the term and the hy-
pernym co-occur and also follow at least one of
the patterns.
For a template tuple Uz with argument type h
and argument value v, we test two ways of esti-
mating the tuple score wz,
</bodyText>
<equation confidence="0.999738333333333">
wz = log (1 + F(v, h)) (4)
A + Eh,∈H F(v, h9)
F(v, h)) (5)
</equation>
<bodyText confidence="0.999973272727273">
where F(v, h) is the popularity of the (v, h) pair
in our corpus, H is the set of all hypernyms for v
in the weighted term→hypernym graph. Parame-
ter A (=1.0 in our experiments) is introduced for
smoothing purpose. Note that the second formula
is the conditional probability of hypernym h given
term v.
Since it is intuitive to estimate tuple scores with
their frequencies in a corpus, we treat the approach
with the improved wz as another baseline (our
strong baseline).
</bodyText>
<subsectionHeader confidence="0.997927">
4.2 Enhancing tuple combination function
</subsectionHeader>
<bodyText confidence="0.996805692307692">
Now we study the possibility of improving the tu-
ple combination function (Formula 1), by examin-
ing the tuple fusion problem from the viewpoint
of probabilistic evidence combination. We first
demonstrate that the linear function in Formula 1
corresponds to the conditional independence as-
sumption of the tuples. Then we propose to adopt
a series of nonlinear functions for combining tuple
scores.
We define the following events:
T: Template T is a valid template;
T: T is an invalid template;
Ez: The observation of tuple Uz.
</bodyText>
<equation confidence="0.914108">
wz =
</equation>
<page confidence="0.97608">
803
</page>
<bodyText confidence="0.99991125">
Let’s compute the posterior odds of event T,
given two tuples U1 and U2. Assuming E1 and
E2 are conditionally independent given T or T,
according to the Bayes rule, we have,
</bodyText>
<equation confidence="0.9924282">
P(T |E1, E2) P(E1, E2|T) · P(T)
P(T |E1, E2) P(E1, E2|T) · P(T)
P(E1|T ) P(E2|T) P(T)
P(E1|T) · P(E2|T) · P(T)
P(T|E1) · P(T) P(T|E2) · P(T) P(T)
P(T|E1) · P(T) · P(T|E2) · P(T) · P(T)
(6)
Define the log-odds-gain of T given E as,
P (T |E) P (T )
G(T|E) = log P (T |E) − log P (T ) (7)
</equation>
<bodyText confidence="0.595742333333333">
Here G means the gain of the log-odds of T af-
ter E occurs. By combining formulas 6 and 7, we
get
</bodyText>
<equation confidence="0.989831">
G(T |E1, E2) = G(T|E1) + G(T|E2) (8)
</equation>
<bodyText confidence="0.932629">
It is easy to prove that the above conclusion
holds true when n &gt; 2, i.e.,
</bodyText>
<equation confidence="0.994132666666667">
n
G(T |E1, ..., En) = G(T |EZ) (9)
Z=1
</equation>
<bodyText confidence="0.999948533333333">
If we treat G(T |EZ) as the score of template T
when only UZ is observed, and G(T |E1, ..., En) as
the template score after the n tuples are observed,
then the above equation means that the combined
template score should be the sum of wZ · IDF(h),
which is exactly Formula 1. Please keep in mind
that Equation 9 is based on the assumption that the
tuples are conditional independent. This assump-
tion, however, may not hold in reality. The case
of conditional dependence was studied in (Zhang
et al., 2011), where a group of nonlinear combina-
tion functions were proposed and achieved good
performance in their task of hypernymy extrac-
tion. We choose p-Norm as our nonlinear fusion
functions, as below,
</bodyText>
<equation confidence="0.99745">
wp · IDF(h) (p &gt; 1) (10)
Z
</equation>
<bodyText confidence="0.977694333333333">
where p (=2 in experiments) is a parameter.
Experiments show that the above nonlinear
function performs better than the linear function
of Formula 1. Let’s use an example to show the
intuition. Consider a good template “city of [coun-
try]” corresponding to CTTs ~UA and a wrong tem-
plate “city of [book]” having tuples ~UB. Sup-
pose |~UA |= 200 (including most countries in
the world) and  |~UB |= 1000 (considering that
many place names have already been used as book
names). We observe that each tuple score corre-
sponding to “city of [country]” is larger than the
tuple score corresponding to “city of [book]”. For
simplicity, we assume each tuple in ~UA has score
1.0 and each tuple in ~UB has score 0.2. With the
linear and nonlinear (p=2) fusion functions, we
can get,
Linear:
</bodyText>
<equation confidence="0.999780428571428">
F(~UA) = 200 * 1.0 = 200
(11)
F(~UB) = 1000 * 0.2 = 200
Nonlinear:
F(
F(~UA) = 14.1
~UB) = 6.32 (12)
</equation>
<bodyText confidence="0.99972625">
In the above settings the nonlinear function
yields a much higher score for the good template
(than for the invalid template), while the linear one
does not.
</bodyText>
<subsectionHeader confidence="0.997261">
4.3 Refinement with term similarity and
</subsectionHeader>
<bodyText confidence="0.974556">
term clusters
The above techniques neglect the similarity among
terms, which has a high potential to improve the
template scoring process. Intuitively, for a toy set
{“city in Brazil”, “city in South Africa”,“city in
China”, “city in Japan”J, since “Brazil”, “South
Africa”, “China” and “Japan” are very similar to
each other and they all have a large probability to
be a “country”, so we have more confidence that
“city in [country]” is a good template. In this sec-
tion, we propose to leverage the term similarity
information to improve the template scoring pro-
cess.
We start with building a large group of small
and overlapped clusters from our web corpus.
</bodyText>
<subsectionHeader confidence="0.698783">
4.3.1 Building term clusters
</subsectionHeader>
<bodyText confidence="0.9997805">
Term clusters are built in three steps.
Mining term peer similarity: Two terms are
peers if they share a common hypernym and they
are semantically correlated. For example, “dog”
and “cat” should have a high peer similarity score.
Following existing work (Hearst, 1992; Kozareva
</bodyText>
<equation confidence="0.9375102">
� �
�
F(~U) = P �
n
Z=1
</equation>
<page confidence="0.984605">
804
</page>
<bodyText confidence="0.99985252173913">
et al., 2008; Shi et al., 2010; Agirre et al., 2009;
Pantel et al., 2009), we built a peer similarity graph
containing about 40.5 million nodes and 1.33 bil-
lion edges.
Clustering: For each term, choose its top-30
neighbors from the peer similarity graph and run a
hierarchical clustering algorithm, resulting in one
or multiple clusters. Then we merge highly du-
plicated clusters. The algorithm is similar to the
first part of CBC (Pantel and Lin, 2002), with the
difference that a very high merging threshold is
adopted here in order to generate small and over-
lapped clusters. Please note that one term may be
included in many clusters.
Assigning top hypernyms: Up to two hyper-
nyms are assigned for each term cluster by major-
ity voting of its member terms, with the aid of the
weighted term→hypernym graph of Section 4.1.
To be an eligible hypernym for the cluster, it has
to be the hypernym of at least 70% of terms in the
cluster. The score of each hypernym is the aver-
age of the term→hypernym weights over all the
member terms.
</bodyText>
<subsectionHeader confidence="0.751565">
4.3.2 Template score refinement
</subsectionHeader>
<bodyText confidence="0.999962666666667">
With term clusters at hand, now we describe the
score refinement procedure for a template T hav-
ing argument type h and supporting tuples �U=(U1,
U2..., U,,,). Denote V = {V1, V2,..., V,,,} to be the
set of argument values for the tuples (where VZ is
the argument value of UZ).
By computing the intersection of V and every
term cluster, we can get a distribution of the argu-
ment values in the clusters. We find that for a good
template like “holiday in [country]”, we can often
find at least one cluster (one of the country clus-
ters in this example) which has hypernym h and
also contains many elements in V . However, for
invalid templates like “holiday of [book]”, every
cluster having hypernym h (=“book” here) only
contains a few elements in V . Inspired by such
an observation, our score refinement algorithm for
template T is as follows,
Step-1. Calculating supporting scores: For
each term cluster C having hypernym h, compute
its supporting score to T as follows:
</bodyText>
<equation confidence="0.954808">
5(C, T) = k(C,V ) · w(C,h) (13)
</equation>
<bodyText confidence="0.9989285">
where k(C,V ) is the number of elements shared
by C and V , and w(C, h) is hypernym score of h
to C (computed in the last step of building clus-
ters).
Step-2. Calculating the final template score:
Let term cluster C* has the maximal supporting
score to T, the final template score is computed
as,
</bodyText>
<equation confidence="0.999723">
5(T) = F(U) · 5(C*, T) (14)
</equation>
<bodyText confidence="0.993727">
where F(U) is the template score before refine-
ment.
</bodyText>
<sectionHeader confidence="0.999674" genericHeader="method">
5 Experiments
</sectionHeader>
<subsectionHeader confidence="0.927099">
5.1 Experimental setup
</subsectionHeader>
<subsubsectionHeader confidence="0.405694">
5.1.1 Methods for comparison
</subsubsectionHeader>
<bodyText confidence="0.997318482758621">
We make a comparison among 10 methods.
SC: The method is proposed in (Cheung and Li,
2012) to construct templates from queries. The
method firstly represents a query as a matrix based
on Freebase data. Then a hierarchical clustering
algorithm is employed to group queries having the
same structure and meaning. Then an intent sum-
marization algorithm is employed to create tem-
plates for each query group.
Base: The linear function in Formula 1 is
adopted to combine the tuple scores. We use
IDF2 here because it achieves higher precision
than IDF1 in this setting.
LW: The linear function in Formula 1 is
adopted to combine the tuple scores generated by
Formula 4. IDF1 is used rather than IDF2 for
better performance.
LP: The linear function in Formula 1 is adopted
to combine the tuple scores generated by Formula
5. IDF2 is used rather than IDF1 for better per-
formance.
NLW: The nonlinear fusion function in For-
mula 10 is used. Other settings are the same as
LW.
NLP: The nonlinear fusion function in Formula
10 is used. Other settings are the same as LP.
LW+C, LP+C, NLW+C, NLP+C: All the set-
tings of LW, LP, NLW, NLP respectively, with the
refinement technology in Section 4.3 applied.
</bodyText>
<subsubsectionHeader confidence="0.492434">
5.1.2 Data sets, annotation and evaluation
</subsubsectionHeader>
<bodyText confidence="0.934085666666667">
metrics
The input category names for experiments are au-
tomatically extracted from a web corpus (Section
3.1). Two test-sets are built for evaluation from the
output templates of various methods.
Subsets: In order to conveniently compare the
performance of different methods, we create 20
sub-collections (called subsets) from the whole in-
put category collection. Each subset contains all
</bodyText>
<page confidence="0.9964">
805
</page>
<bodyText confidence="0.9999859">
the categories having the same headword (e.g.,
“symptom of insulin deficiency” and “depression
symptom” are in the same subset because they
share the same headword “symptom”). To choose
the 20 headwords, we first sample 100 at ran-
dom from the set of all headwords; then manu-
ally choose 20 for diversity. The headwords in-
clude symptom, school, food, gem, hero, weapon,
model, etc. We run the 10 methods on these sub-
sets and sort the output templates by their scores.
Top-30 templates from each method on each sub-
set are selected and mixed together for annotation.
Fullset: We run method NLP+C (which has
the best performance according to our subsets
experiments) on the input categories and sort
the output templates by their scores. Then we
split the templates into 9 sections according
to their ranking position. The sections are:
[1∼100], (100∼1K], (1K∼10K], (10K∼100K],
(100K,120K], (120K∼140K], (140K∼160K],
(160K∼180K], (180K∼200K]. Then 40 templates
are randomly chosen from each section and mixed
together for annotation.
The selected templates (from subsets and the
fullset) are annotated by six annotators, with each
template assigned to two annotators. A template is
assigned a label of “good”, “fair”, or “bad” by an
annotator. The percentage agreement between the
annotators is 80.2%, with kappa 0.624.
For the subset experiments, we adopt
Precision@k (k=10,20,30) to evaluate the
top templates generated by each method. The
scores for “good”, “fair”, and “bad” are 1, 0.5,
and 0. The score of each template is the average
annotation score over two annotators (e.g., if a
template is annotated “good” by one annotator and
“fair” by another, its score is (1.0+0.5)/2=0.75).
The evaluation score of a method is the average
over the 20 subsets. For the fullset experiments,
we report the precision for each section.
</bodyText>
<subsectionHeader confidence="0.7104295">
5.2 Experimental results
5.2.1 Results for subsets
</subsectionHeader>
<bodyText confidence="0.999815">
The results of each method on the 20 subsets
are presented in Table 1. A few observations
can be made. First, by comparing the per-
formance of baseline-1 (Base) and the methods
adopting term→hypernym weight (LW and LP),
we can see big performance improvement. The
bad performance of baseline-1 is mainly due to
the lack of weight (or frequency) information on
</bodyText>
<table confidence="0.999807333333333">
Method P@10 P@20 P@30
Base (baseline-1) 0.359 0.361 0.358
SC (Cheung and Li, 2012) 0.382 0.366 0.371
Weighted LW 0.633 0.582 0.559
(baseline-2)
LP 0.771 0.734 0.707
Nonlinear NLW 0.711 0.671 0.638
NLP 0.818 0.791 0.765
Term cluster LW+C 0.813 0.786 0.754
NLW+C 0.854 0.833 0.808
LP+C 0.818 0.788 0.778
NLP+C 0.868 0.839 0.788
</table>
<tableCaption confidence="0.992507">
Table 1: Performance comparison among the
methods on subset.
</tableCaption>
<bodyText confidence="0.996701717948718">
term→hypernym edges. The results demonstrate
that edge scores are critical for generating high
quality templates. Manually built semantic re-
sources typically lack such kinds of scores. There-
fore, it is very important to enhance them by de-
riving statistical data from a large corpus. Since
it is relatively easy to have the idea of adopt-
ing a weighted term→hypernym graph, we treat
LW and LP as another (stronger) baseline named
baseline-2.
As the second observation, the results show that
the nonlinear methods (NLP and NLW) achieve
performance improvement over their linear ver-
sions (LW and LP).
Third, let’s examine the methods with template
scores refined by term similarity and term clus-
ters (LW+C, NLW+C, LP+C, NLP+C). It is shown
that the refine-by-cluster technology brings addi-
tional performance gains on all the four settings
(linear and nonlinear, two different ways of calcu-
lating tuple scores). So we can conclude that the
peer similarity and term clusters are quite effective
in improving template generation.
Fourth, the best performance is achieved
when the three techniques (i.e., term→hypernym
weight, nonlinear fusion function, and refine-by-
cluster) are combined together. For instance, by
comparing the P@20 scores of baseline-2 and
NLP+C, we see a performance improvement of
14.3% (from 0.734 to 0.839). Therefore every
technique studied in this paper has its own merit
in template generation.
Finally, by comparing the method SC (Cheung
and Li, 2012) with other methods, we can see that
SC is slightly better than baseline-1, but has much
lower performance than others. The major reason
may be that this method did not employ a weighted
term→hypernym graph or term peer similarity in-
formation in template construction.
</bodyText>
<page confidence="0.996931">
806
</page>
<table confidence="0.999492388888889">
P@10 Base SC LP NLP LP+C
SC �
LP &gt; ** &gt; **
NLP &gt; ** &gt; ** &gt;
LP+C &gt; ** &gt; ** &gt; ** �
NLP+C &gt; ** &gt; ** &gt; ** &gt; ** &gt;
P@20 Base SC LP NLP LP+C
SC �
LP &gt; ** &gt; **
NLP &gt; ** &gt; ** &gt; **
LP+C &gt; ** &gt; ** &gt; ** �
NLP+C &gt; ** &gt; ** &gt; ** &gt; ** &gt; **
P@30 Base SC LP NLP LP+C
SC �
LP &gt; ** &gt; **
NLP &gt; ** &gt; ** &gt; **
LP+C &gt; ** &gt; ** &gt; ** �
NLP+C &gt; ** &gt; ** &gt; ** &gt; �
</table>
<tableCaption confidence="0.989976">
Table 2: Paired t-test results on subsets.
</tableCaption>
<table confidence="0.999921666666667">
P@10 Base SC LW NLW LW+C
SC �
LW &gt; ** &gt; **
NLW &gt; ** &gt; ** &gt; *
LW+C &gt; ** &gt; ** &gt; ** &gt; **
NLW+C &gt; ** &gt; ** &gt; ** &gt; ** &gt; *
P@20 Base SC LW NLW LW+C
SC �
LW &gt; ** &gt; **
NLW &gt; ** &gt; ** &gt; **
LW+C &gt; ** &gt; ** &gt; ** &gt; **
NLW+C &gt; ** &gt; ** &gt; ** &gt; ** &gt; **
P@30 Base SC LW NLW LW+C
SC �
LW &gt; ** &gt; **
NLW &gt; ** &gt; ** &gt; **
LW+C &gt; ** &gt; ** &gt; ** &gt; **
NLW+C &gt; ** &gt; ** &gt; ** &gt; ** &gt; **
</table>
<tableCaption confidence="0.999943">
Table 3: Paired t-test results on subsets.
</tableCaption>
<bodyText confidence="0.998486">
Are the performance differences between meth-
ods significant enough for us to say that one is bet-
ter than the other? To answer this question, we run
paired two-tailed t-test on every pair of methods.
We report the t-test values among methods in ta-
bles 2, 3 and 4.
The meaning of the symbols in the tables are,
—: The method on the row and the one on the
column have similar performance.
&gt;: The method on the row outperforms the
method on the column, but the performance dif-
ference is not statistically significant (0.05 &lt; P &lt;
0.1 in two-tailed t-test).
&gt; *: The performance difference is statistically
significant (P &lt; 0.05 in two-tailed t-test).
&gt; **: The performance difference is statisti-
cally highly significant (P &lt; 0.01 in two-tailed
t-test).
</bodyText>
<table confidence="0.996987">
P@10 P@20 P@30
LP V.S. LW &gt; ** &gt; ** &gt; **
NLP V.S. NLW &gt; ** &gt; ** &gt; **
LP+C V.S. LW+C � � �
NLP+C V.S. NLW+C � � �
</table>
<tableCaption confidence="0.999839">
Table 4: Paired t-test results on subsets.
</tableCaption>
<subsectionHeader confidence="0.264616">
Section ID
</subsectionHeader>
<figureCaption confidence="0.976259">
Figure 3: Precision by section in the fullset.
</figureCaption>
<subsubsectionHeader confidence="0.703336">
5.2.2 Fullset results
</subsubsectionHeader>
<bodyText confidence="0.999981153846154">
As described in the Section 5.1.2, for the fullset
experiments, we conduct a section-wise evalua-
tion, selecting 40 templates from each of the 9 sec-
tions of the NLP+C results. The results are shown
in Figure 3. It can be observed that the precision
for each section decreases when the section ID in-
creases. The results indicate the effectiveness of
our approach, since it can rank good templates in
top sections and bad templates in bottom sections.
According to the section-wise precision data, we
are able to determine the template score threshold
for choosing different numbers of top templates in
different applications.
</bodyText>
<sectionHeader confidence="0.7442955" genericHeader="method">
5.2.3 Templates for category collection
cleaning
</sectionHeader>
<bodyText confidence="0.999954125">
Since our input category collection is automati-
cally constructed from the web, some wrong or
invalid category names is inevitably contained. In
this subsection, we apply our category templates
to clean the category collection. The basic idea is
that if a category can match a template, it is more
likely to be correct. We compute a new score for
every category name H as follows,
</bodyText>
<equation confidence="0.997096">
S.,(H) = log(1 + S(H)) · S(T*) (15)
</equation>
<bodyText confidence="0.9999707">
where S(H) is the existing category score, deter-
mined by its frequency in the corpus. Here S(T*)
is the score of template T*, the best template (i.e.,
the template with the highest score) for the cate-
gory.
Then we re-rank the categories according to
their new scores to get a re-ranked category list.
We randomly sampled 150 category names from
the top 2 million categories of each list (the old list
and the new list) and asked annotators to judge the
</bodyText>
<figure confidence="0.999034923076923">
1 2 3 4 5 6 7 8 9
Precision
0.9
0.8
0.7
0.6
0.5
0.4
0.3
0.2
0.1
0
1
</figure>
<page confidence="0.987876">
807
</page>
<bodyText confidence="0.999968">
quality of the categories. The annotation results
show that, after re-ranking, the precision increases
from 0.81 to 0.89 (i.e., the percent of invalid cate-
gory names decreases from 19% to 11%).
</bodyText>
<sectionHeader confidence="0.998212" genericHeader="conclusions">
6 Conclusion
</sectionHeader>
<bodyText confidence="0.999957285714286">
In this paper, we studied the problem of build-
ing templates for a large collection of category
names. We tested three techniques (tuple scor-
ing by weighted term→hypernym mapping, non-
linear score fusion, refinement by term clusters)
and found that all of them are very effective and
their combination achieves the best performance.
By employing the output templates to clean our
category collection mined from the web, we get
apparent quality improvement. Future work in-
cludes supporting multi-argument templates, dis-
ambiguating headwords of category names and ap-
plying our approach to general short text template
mining.
</bodyText>
<sectionHeader confidence="0.997235" genericHeader="acknowledgments">
Acknowledgments
</sectionHeader>
<bodyText confidence="0.999729833333333">
We would like to thank the annotators for their ef-
forts in annotating the templates. Thanks to the
anonymous reviewers for their helpful comments
and suggestions. This work is supported in part by
China National 973 program 2014CB340301 and
NSFC grant 61379043.
</bodyText>
<sectionHeader confidence="0.997847" genericHeader="references">
References
</sectionHeader>
<reference confidence="0.999597051948052">
Ganesh Agarwal, Govind Kabra, and Kevin Chen-
Chuan Chang. 2010. Towards rich query interpreta-
tion: walking back and forth for mining query tem-
plates. In Proceedings of the 19th international con-
ference on World wide web, pages 1–10. ACM.
Eneko Agirre, Enrique Alfonseca, Keith Hall, Jana
Kravalova, Marius Pas¸ca, and Aitor Soroa. 2009.
A study on similarity and relatedness using distribu-
tional and wordnet-based approaches. In Proceed-
ings of Human Language Technologies: The 2009
Annual Conference of the North American Chap-
ter of the Association for Computational Linguistics,
pages 19–27. Association for Computational Lin-
guistics.
Nathanael Chambers and Dan Jurafsky. 2011.
Template-based information extraction without the
templates. In Proceedings of the 49th Annual
Meeting of the Association for Computational Lin-
guistics: Human Language Technologies-Volume 1,
pages 976–986. Association for Computational Lin-
guistics.
Jackie Chi Kit Cheung and Xiao Li. 2012. Sequence
clustering and labeling for unsupervised query intent
discovery. In Proceedings of the fifth ACM interna-
tional conference on Web search and data mining,
pages 383–392. ACM.
Jesualdo Tomas Fernandez-Breis, Luigi Iannone, Ig-
nazio Palmisano, Alan L Rector, and Robert
Stevens. 2010. Enriching the gene ontology via the
dissection of labels using the ontology pre-processor
language. In Knowledge Engineering and Manage-
ment by the Masses, pages 59–73. Springer.
Jun Han, Ju Fan, and Lizhu Zhou. 2013.
Crowdsourcing-assisted query structure interpreta-
tion. In Proceedings of the Twenty-Third inter-
national joint conference on Artificial Intelligence,
pages 2092–2098. AAAI Press.
Marti A. Hearst. 1992. Automatic acquisition of hy-
ponyms from large text corpora. In Proceedings of
the 14th conference on Computational linguistics -
Volume 2, COLING ’92, pages 539–545, Strouds-
burg, PA, USA. Association for Computational Lin-
guistics.
Zornitsa Kozareva, Ellen Riloff, and Eduard H Hovy.
2008. Semantic class learning from the web with
hyponym pattern linkage graphs. In ACL, volume 8,
pages 1048–1056.
Xiao Li, Ye-Yi Wang, and Alex Acero. 2009. Extract-
ing structured information from user queries with
semi-supervised conditional random fields. In Pro-
ceedings of the 32nd international ACM SIGIR con-
ference on Research and development in information
retrieval, pages 572–579. ACM.
Yanen Li, Bo-June Paul Hsu, and ChengXiang Zhai.
2013. Unsupervised identification of synonymous
query intent templates for attribute intents. In Pro-
ceedings of the 22nd ACM international conference
on Conference on information &amp; knowledge man-
agement, pages 2029–2038. ACM.
Xiao Li. 2010. Understanding the semantic struc-
ture of noun phrase queries. In Proceedings of the
48th Annual Meeting of the Association for Compu-
tational Linguistics, pages 1337–1345. Association
for Computational Linguistics.
Sandeep Pandey and Kunal Punera. 2012. Unsuper-
vised extraction of template structure in web search
queries. In Proceedings of the 21st international
conference on World Wide Web, pages 409–418.
ACM.
Patrick Pantel and Dekang Lin. 2002. Discovering
word senses from text. In Proceedings of the eighth
ACM SIGKDD international conference on Knowl-
edge discovery and data mining, pages 613–619.
ACM.
Patrick Pantel and Deepak Ravichandran. 2004. Au-
tomatically labeling semantic classes. In HLT-
NAACL, volume 4, pages 321–328.
</reference>
<page confidence="0.981217">
808
</page>
<reference confidence="0.998441140350877">
Patrick Pantel, Eric Crestan, Arkady Borkovsky, Ana-
Maria Popescu, and Vishnu Vyas. 2009. Web-scale
distributional similarity and entity set expansion. In
Proceedings of the 2009 Conference on Empirical
Methods in Natural Language Processing: Volume
2-Volume 2, pages 938–947. Association for Com-
putational Linguistics.
Manuel Quesada-Martınez, Jesualdo Tom´as
Fern´andez-Breis, and Robert Stevens. 2012.
Enrichment of owl ontologies: a method for defin-
ing axioms from labels. In Proceedings of the First
International Workshop on Capturing and Refining
Knowledge in the Medical Domain (K-MED 2012),
Galway, Ireland, pages 1–10.
Joseph Reisinger and Marius Pasca. 2011. Fine-
grained class label markup of search queries. In
ACL, pages 1200–1209.
Nikos Sarkas, Stelios Paparizos, and Panayiotis
Tsaparas. 2010. Structured annotations of web
queries. In Proceedings of the 2010 ACM SIGMOD
International Conference on Management of data,
pages 771–782. ACM.
Shuming Shi, Huibin Zhang, Xiaojie Yuan, and Ji-
Rong Wen. 2010. Corpus-based semantic class
mining: distributional vs. pattern-based approaches.
In Proceedings of the 23rd International Conference
on Computational Linguistics, pages 993–1001. As-
sociation for Computational Linguistics.
Mark Stevenson and Mark A Greenwood. 2005. A
semantic approach to ie pattern induction. In Pro-
ceedings of the 43rd Annual Meeting on Association
for Computational Linguistics, pages 379–386. As-
sociation for Computational Linguistics.
Idan Szpektor, Aristides Gionis, and Yoelle Maarek.
2011. Improving recommendation for long-tail
queries via templates. In Proceedings of the 20th
international conference on World wide web, pages
47–56. ACM.
Allan Third. 2012. Hidden semantics: what can we
learn from the names in an ontology? In Proceed-
ings of the Seventh International Natural Language
Generation Conference, pages 67–75. Association
for Computational Linguistics.
Benjamin Van Durme and Marius Pasca. 2008. Find-
ing cars, goddesses and enzymes: Parametrizable
acquisition of labeled instances for open-domain in-
formation extraction. In AAAI, volume 8, pages
1243–1248.
Roman Yangarber. 2003. Counter-training in discov-
ery of semantic patterns. In Proceedings of the 41st
Annual Meeting on Association for Computational
Linguistics-Volume 1, pages 343–350. Association
for Computational Linguistics.
Fan Zhang, Shuming Shi, Jing Liu, Shuqi Sun, and
Chin-Yew Lin. 2011. Nonlinear evidence fusion
and propagation for hyponymy relation mining. In
ACL, volume 11, pages 1159–1168.
</reference>
<page confidence="0.998835">
809
</page>
</variant>
</algorithm>
<algorithm name="ParsHed" version="110505">
<variant no="0" confidence="0.137583">
<title confidence="0.998099">Unsupervised Template Mining for Semantic Category Understanding</title>
<author confidence="0.82217">Shuming Chin-Yew Yi-Dong Yong</author>
<affiliation confidence="0.66736">Key Laboratory of Computer Institute of Software, Chinese Academy of of Chinese Academy of</affiliation>
<abstract confidence="0.998274666666667">We propose an unsupervised approach to constructing templates from a large collection of semantic category names, and use the templates as the semantic representation of categories. The main challenge is that many terms have multiple meanings, resulting in a lot of wrong templates. Statistical data and semantic knowledge are extracted from a web corpus to improve template generation. A nonlinear scoring function is proposed and demonstrated to be effective. Experiments show that our approach achieves significantly better results than baseline methods. As an immediate application, we apply the extracted templates to the cleaning of a category collection and see promising results (preci-</abstract>
<intro confidence="0.420559">improved from</intro>
</variant>
</algorithm>
<algorithm name="ParsCit" version="110505">
<citationList>
<citation valid="true">
<authors>
<author>Ganesh Agarwal</author>
<author>Govind Kabra</author>
<author>Kevin ChenChuan Chang</author>
</authors>
<title>Towards rich query interpretation: walking back and forth for mining query templates.</title>
<date>2010</date>
<booktitle>In Proceedings of the 19th international conference on World wide web,</booktitle>
<pages>1--10</pages>
<publisher>ACM.</publisher>
<contexts>
<context position="10219" citStr="Agarwal et al. (2010)" startWordPosition="1582" endWordPosition="1585">n. These supervised or semi-supervised approaches require much manual annotation effort. Unsupervised methods were proposed by Sarkas et al. (2010) and Reisinger and Pasca (2011). As been discussed in the introduction section, query tagging is only one of the two stages of template generation. The tagging results are for one query only, without aggregating the global information of all queries to generate the final templates. Query template construction: Some existing work leveraged query templates or patterns for query understanding. A semi-supervised random walk based method was proposed by Agarwal et al. (2010) to generate a ranked templates list which are relevant to a domain of interest. A predefined domain schema and seed information is needed for this method. Pandey and Punera (2012) proposed an unsupervised method based on graphical models to mine query templates. The above methods are either domain-specific (i.e., generating templates for a specific domain), or have some degree of supervision (supervised or semi-supervised). Cheung and Li (2012) proposed an unsupervised method to generate query templates by the aid of knowledge bases. An approach was proposed in (Szpektor et al., 2011) to impr</context>
</contexts>
<marker>Agarwal, Kabra, Chang, 2010</marker>
<rawString>Ganesh Agarwal, Govind Kabra, and Kevin ChenChuan Chang. 2010. Towards rich query interpretation: walking back and forth for mining query templates. In Proceedings of the 19th international conference on World wide web, pages 1–10. ACM.</rawString>
</citation>
<citation valid="true">
<authors>
<author>Eneko Agirre</author>
<author>Enrique Alfonseca</author>
<author>Keith Hall</author>
<author>Jana Kravalova</author>
<author>Marius Pas¸ca</author>
<author>Aitor Soroa</author>
</authors>
<title>A study on similarity and relatedness using distributional and wordnet-based approaches.</title>
<date>2009</date>
<booktitle>In Proceedings of Human Language Technologies: The 2009 Annual Conference of the North American Chapter of the Association for Computational Linguistics,</booktitle>
<pages>pages</pages>
<marker>Agirre, Alfonseca, Hall, Kravalova, Pas¸ca, Soroa, 2009</marker>
<rawString>Eneko Agirre, Enrique Alfonseca, Keith Hall, Jana Kravalova, Marius Pas¸ca, and Aitor Soroa. 2009. A study on similarity and relatedness using distributional and wordnet-based approaches. In Proceedings of Human Language Technologies: The 2009 Annual Conference of the North American Chapter of the Association for Computational Linguistics, pages 19–27. Association for Computational Linguistics.</rawString>
</citation>
<citation valid="true">
<authors>
<author>Nathanael Chambers</author>
<author>Dan Jurafsky</author>
</authors>
<title>Template-based information extraction without the templates.</title>
<date>2011</date>
<booktitle>In Proceedings of the 49th Annual Meeting of the Association for Computational Linguistics: Human Language Technologies-Volume 1,</booktitle>
<pages>976--986</pages>
<publisher>Association</publisher>
<institution>for Computational Linguistics.</institution>
<contexts>
<context position="9112" citStr="Chambers and Jurafsky (2011)" startWordPosition="1409" endWordPosition="1413">d ours is that we automatically assign semantic types to the pattern variables (or called arguments) while they do not. 800 Template mining for IE: Some research work in information extraction (IE) involves patterns. Yangarber (2003) and Stevenson and Greenwood (2005) proposed to learn patterns which were in the form of [subject, verb, object]. The category names and learned templates in our work are not in this form. Another difference between our work and their work is that, their methods need a supervised name classifer to generate the candidate patterns while our approach is unsupervised. Chambers and Jurafsky (2011) leverage templates to describe an event while the templates in our work are for understanding category names (a kind of short text). Query tagging/labeling: Some research work in recent years focuses on segmenting web search queries and assigning semantic tags to key segments. Li et al. (2009) and Li (2010) employed CRF (Conditional Random Field) or semi-CRF models for query tagging. A crowdsourcingassisted method was proposed by Han et al. (2013) for query structure interpretation. These supervised or semi-supervised approaches require much manual annotation effort. Unsupervised methods were</context>
</contexts>
<marker>Chambers, Jurafsky, 2011</marker>
<rawString>Nathanael Chambers and Dan Jurafsky. 2011. Template-based information extraction without the templates. In Proceedings of the 49th Annual Meeting of the Association for Computational Linguistics: Human Language Technologies-Volume 1, pages 976–986. Association for Computational Linguistics.</rawString>
</citation>
<citation valid="true">
<authors>
<author>Jackie Chi Kit Cheung</author>
<author>Xiao Li</author>
</authors>
<title>Sequence clustering and labeling for unsupervised query intent discovery.</title>
<date>2012</date>
<booktitle>In Proceedings of the fifth ACM international conference on Web search and data mining,</booktitle>
<pages>383--392</pages>
<publisher>ACM.</publisher>
<contexts>
<context position="10668" citStr="Cheung and Li (2012)" startWordPosition="1652" endWordPosition="1655">ruction: Some existing work leveraged query templates or patterns for query understanding. A semi-supervised random walk based method was proposed by Agarwal et al. (2010) to generate a ranked templates list which are relevant to a domain of interest. A predefined domain schema and seed information is needed for this method. Pandey and Punera (2012) proposed an unsupervised method based on graphical models to mine query templates. The above methods are either domain-specific (i.e., generating templates for a specific domain), or have some degree of supervision (supervised or semi-supervised). Cheung and Li (2012) proposed an unsupervised method to generate query templates by the aid of knowledge bases. An approach was proposed in (Szpektor et al., 2011) to improve query recommendation via query templates. Query session information (which is not available in our task) is needed in this approach for templates generation. Li et al. (2013) proposed an clustering algorithm to group existing query templates by search intents of users. Compared to the open-domain unsupervised methods for query template construction, our approach improves on two aspects. First, we propose to incorporate multiple types of sema</context>
<context position="26714" citStr="Cheung and Li, 2012" startWordPosition="4414" endWordPosition="4417">pernym h, compute its supporting score to T as follows: 5(C, T) = k(C,V ) · w(C,h) (13) where k(C,V ) is the number of elements shared by C and V , and w(C, h) is hypernym score of h to C (computed in the last step of building clusters). Step-2. Calculating the final template score: Let term cluster C* has the maximal supporting score to T, the final template score is computed as, 5(T) = F(U) · 5(C*, T) (14) where F(U) is the template score before refinement. 5 Experiments 5.1 Experimental setup 5.1.1 Methods for comparison We make a comparison among 10 methods. SC: The method is proposed in (Cheung and Li, 2012) to construct templates from queries. The method firstly represents a query as a matrix based on Freebase data. Then a hierarchical clustering algorithm is employed to group queries having the same structure and meaning. Then an intent summarization algorithm is employed to create templates for each query group. Base: The linear function in Formula 1 is adopted to combine the tuple scores. We use IDF2 here because it achieves higher precision than IDF1 in this setting. LW: The linear function in Formula 1 is adopted to combine the tuple scores generated by Formula 4. IDF1 is used rather than I</context>
<context position="30566" citStr="Cheung and Li, 2012" startWordPosition="5042" endWordPosition="5045">on score of a method is the average over the 20 subsets. For the fullset experiments, we report the precision for each section. 5.2 Experimental results 5.2.1 Results for subsets The results of each method on the 20 subsets are presented in Table 1. A few observations can be made. First, by comparing the performance of baseline-1 (Base) and the methods adopting term→hypernym weight (LW and LP), we can see big performance improvement. The bad performance of baseline-1 is mainly due to the lack of weight (or frequency) information on Method P@10 P@20 P@30 Base (baseline-1) 0.359 0.361 0.358 SC (Cheung and Li, 2012) 0.382 0.366 0.371 Weighted LW 0.633 0.582 0.559 (baseline-2) LP 0.771 0.734 0.707 Nonlinear NLW 0.711 0.671 0.638 NLP 0.818 0.791 0.765 Term cluster LW+C 0.813 0.786 0.754 NLW+C 0.854 0.833 0.808 LP+C 0.818 0.788 0.778 NLP+C 0.868 0.839 0.788 Table 1: Performance comparison among the methods on subset. term→hypernym edges. The results demonstrate that edge scores are critical for generating high quality templates. Manually built semantic resources typically lack such kinds of scores. Therefore, it is very important to enhance them by deriving statistical data from a large corpus. Since it is </context>
<context position="32342" citStr="Cheung and Li, 2012" startWordPosition="5321" endWordPosition="5324">inear, two different ways of calculating tuple scores). So we can conclude that the peer similarity and term clusters are quite effective in improving template generation. Fourth, the best performance is achieved when the three techniques (i.e., term→hypernym weight, nonlinear fusion function, and refine-bycluster) are combined together. For instance, by comparing the P@20 scores of baseline-2 and NLP+C, we see a performance improvement of 14.3% (from 0.734 to 0.839). Therefore every technique studied in this paper has its own merit in template generation. Finally, by comparing the method SC (Cheung and Li, 2012) with other methods, we can see that SC is slightly better than baseline-1, but has much lower performance than others. The major reason may be that this method did not employ a weighted term→hypernym graph or term peer similarity information in template construction. 806 P@10 Base SC LP NLP LP+C SC � LP &gt; ** &gt; ** NLP &gt; ** &gt; ** &gt; LP+C &gt; ** &gt; ** &gt; ** � NLP+C &gt; ** &gt; ** &gt; ** &gt; ** &gt; P@20 Base SC LP NLP LP+C SC � LP &gt; ** &gt; ** NLP &gt; ** &gt; ** &gt; ** LP+C &gt; ** &gt; ** &gt; ** � NLP+C &gt; ** &gt; ** &gt; ** &gt; ** &gt; ** P@30 Base SC LP NLP LP+C SC � LP &gt; ** &gt; ** NLP &gt; ** &gt; ** &gt; ** LP+C &gt; ** &gt; ** &gt; ** � NLP+C &gt; ** &gt; ** &gt; *</context>
</contexts>
<marker>Cheung, Li, 2012</marker>
<rawString>Jackie Chi Kit Cheung and Xiao Li. 2012. Sequence clustering and labeling for unsupervised query intent discovery. In Proceedings of the fifth ACM international conference on Web search and data mining, pages 383–392. ACM.</rawString>
</citation>
<citation valid="true">
<authors>
<author>Jesualdo Tomas Fernandez-Breis</author>
<author>Luigi Iannone</author>
<author>Ignazio Palmisano</author>
<author>Alan L Rector</author>
<author>Robert Stevens</author>
</authors>
<title>Enriching the gene ontology via the dissection of labels using the ontology pre-processor language.</title>
<date>2010</date>
<booktitle>In Knowledge Engineering and Management by the Masses,</booktitle>
<pages>59--73</pages>
<publisher>Springer.</publisher>
<contexts>
<context position="5248" citStr="Fernandez-Breis et al., 2010" startWordPosition="797" endWordPosition="800"> “CEO of [company]”, and the like. Template scoring: Compute the score of each candidate template by aggregating the information obtained in the first phase. A major challenge here is that many segments (like “Delphinus” in the above example) have multiple meanings. As a result, wrong hypernyms may be adopted to generate incorrect candidate templates (like “CEO of [constellation]”). In this paper, we focus on improving the template scoring stage, with the goal of assigning lower scores to bad templates and larger scores to high-quality ones. There have been some research efforts (Third, 2012; Fernandez-Breis et al., 2010; QuesadaMartınez et al., 2012) on exploring the structure of category names by building patterns. However, we automatically assign semantic types to the pattern variables (or called arguments) while they do not. For example, our template has the form of “city in [country]” while their patterns are like “city in [X]”. More details are given in the related work section. A similar task is query understanding, including query tagging and query template mining. Query tagging (Li et al., 2009; Reisinger and Pasca, 2011) corresponds to the category labeling stage described above. It is different fro</context>
<context position="8193" citStr="Fernandez-Breis et al. (2010)" startWordPosition="1259" endWordPosition="1262">Pantel and Ravichandran, 2004; Van Durme and Pasca, 2008; Zhang et al., 2011) in the literature to extract hypernymy (or is-a) relations from the web. Our target here is not hypernymy extraction, but discovering the semantic structure of hypernyms (or category names). Category name exploration: Category name patterns are explored and built in some existing research work. Third (2012) proposed to find axiom patterns among category names on an existing ontology. For example, infer axiom pattern “SubClassOf(AB, B)” from “SubClassOf(junior school school)” and “SubClassOf(domestic mammal mammal)”. Fernandez-Breis et al. (2010) and QuesadaMartınez et al. (2012) proposed to find lexical patterns in category names to define axioms (in medical domain). One example pattern mentioned in their papers is “[X] binding”. They need manual intervention to determine what X means. The main difference between the above work and ours is that we automatically assign semantic types to the pattern variables (or called arguments) while they do not. 800 Template mining for IE: Some research work in information extraction (IE) involves patterns. Yangarber (2003) and Stevenson and Greenwood (2005) proposed to learn patterns which were in</context>
</contexts>
<marker>Fernandez-Breis, Iannone, Palmisano, Rector, Stevens, 2010</marker>
<rawString>Jesualdo Tomas Fernandez-Breis, Luigi Iannone, Ignazio Palmisano, Alan L Rector, and Robert Stevens. 2010. Enriching the gene ontology via the dissection of labels using the ontology pre-processor language. In Knowledge Engineering and Management by the Masses, pages 59–73. Springer.</rawString>
</citation>
<citation valid="true">
<authors>
<author>Jun Han</author>
<author>Ju Fan</author>
<author>Lizhu Zhou</author>
</authors>
<title>Crowdsourcing-assisted query structure interpretation.</title>
<date>2013</date>
<booktitle>In Proceedings of the Twenty-Third international joint conference on Artificial Intelligence,</booktitle>
<pages>2092--2098</pages>
<publisher>AAAI Press.</publisher>
<contexts>
<context position="9564" citStr="Han et al. (2013)" startWordPosition="1484" endWordPosition="1487">heir work is that, their methods need a supervised name classifer to generate the candidate patterns while our approach is unsupervised. Chambers and Jurafsky (2011) leverage templates to describe an event while the templates in our work are for understanding category names (a kind of short text). Query tagging/labeling: Some research work in recent years focuses on segmenting web search queries and assigning semantic tags to key segments. Li et al. (2009) and Li (2010) employed CRF (Conditional Random Field) or semi-CRF models for query tagging. A crowdsourcingassisted method was proposed by Han et al. (2013) for query structure interpretation. These supervised or semi-supervised approaches require much manual annotation effort. Unsupervised methods were proposed by Sarkas et al. (2010) and Reisinger and Pasca (2011). As been discussed in the introduction section, query tagging is only one of the two stages of template generation. The tagging results are for one query only, without aggregating the global information of all queries to generate the final templates. Query template construction: Some existing work leveraged query templates or patterns for query understanding. A semi-supervised random </context>
</contexts>
<marker>Han, Fan, Zhou, 2013</marker>
<rawString>Jun Han, Ju Fan, and Lizhu Zhou. 2013. Crowdsourcing-assisted query structure interpretation. In Proceedings of the Twenty-Third international joint conference on Artificial Intelligence, pages 2092–2098. AAAI Press.</rawString>
</citation>
<citation valid="true">
<authors>
<author>Marti A Hearst</author>
</authors>
<title>Automatic acquisition of hyponyms from large text corpora.</title>
<date>1992</date>
<booktitle>In Proceedings of the 14th conference on Computational linguistics -Volume 2, COLING ’92,</booktitle>
<pages>539--545</pages>
<publisher>Association</publisher>
<institution>for Computational Linguistics.</institution>
<location>Stroudsburg, PA, USA.</location>
<contexts>
<context position="1458" citStr="Hearst, 1992" startWordPosition="217" endWordPosition="218">ur approach achieves significantly better results than baseline methods. As an immediate application, we apply the extracted templates to the cleaning of a category collection and see promising results (precision improved from 81% to 89%). 1 Introduction A semantic category is a collection of items sharing common semantic properties. For example, all cities in Germany form a semantic category named “city in Germany” or “German city”. In Wikipedia, the category names of an entity are manually edited and displayed at the end of the page for the entity. There have been quite a lot of approaches (Hearst, 1992; Pantel and Ravichandran, 2004; Van Durme and Pasca, 2008; Zhang et al., 2011) in the literature to automatically extracting category names and instances (also called is-a or hypernymy relations) from the web. Most existing work simply treats a category name as a text string containing one or multiple words, without caring about its internal structure. In this paper, we explore the semantic structure of category names (or simply called “categories”). ∗This work was performed when the first author was visiting Microsoft Research Asia. For example, both “CEO of General Motors” and “CEO of Yahoo</context>
<context position="7562" citStr="Hearst, 1992" startWordPosition="1168" endWordPosition="1169">loy the output templates to clean our category collection mined from the web, and get apparent quality improvement (precision improved from 81% to 89%). After discussing related work in Section 2, we define the problem and describe one baseline approach in Section 3. Then we introduce our approach in Section 4. Experimental results are reported and analyzed in Section 5. We conclude the paper in Section 6. 2 Related work Several kinds of work are related to ours. Hypernymy relation extraction: Hypernymy relation extraction is an important task in text mining. There have been a lot of efforts (Hearst, 1992; Pantel and Ravichandran, 2004; Van Durme and Pasca, 2008; Zhang et al., 2011) in the literature to extract hypernymy (or is-a) relations from the web. Our target here is not hypernymy extraction, but discovering the semantic structure of hypernyms (or category names). Category name exploration: Category name patterns are explored and built in some existing research work. Third (2012) proposed to find axiom patterns among category names on an existing ontology. For example, infer axiom pattern “SubClassOf(AB, B)” from “SubClassOf(junior school school)” and “SubClassOf(domestic mammal mammal)”</context>
<context position="11904" citStr="Hearst, 1992" startWordPosition="1848" endWordPosition="1849">rm peer similarity and term clusters) to improve template generation. Second, we propose a nonlinear template scoring function which is demonstrated to be more effective. 3 Problem Definition and Analysis 3.1 Problem definition The goal of this paper is to construct a list of category templates from a collection of open-domain category names. Input: The input is a collection of category names, which can either be manually compiled (like Wikipedia categories) or be automatically extracted. The categories used in our experiments were automatically mined from the web, by following existing work (Hearst, 1992, Pantel and Ravichandran 2004; Snow et al., 2005; Talukdar et al., 2008; Zhang et al., 2011). Specifically, we applied Hearst patterns (e.g., “NP [,] (such as |including) {NP, }∗ {and|or} NP”) and isa patterns (“NP (is|are|was|were|being) (a|an|the) NP”) to a large corpus containing 3 billion English web pages. As a result, we obtained a term—*hypernym bi-partite graph containing 40 million terms, 74 million hypernyms (i.e., category names), and 321 million edges (e.g., one example edge is “Berlin”—*“city in Germany”, where “Berlin” is a term and “city in Germany” is the corresponding hyperny</context>
<context position="19108" citStr="Hearst, 1992" startWordPosition="3038" endWordPosition="3039">h Africa, w1) U2: (holiday of [book], South Africa, w2) Intuitively, “South Africa” is more likely to be a country than a book when it appears in text. So for a reasonable tuple scoring formula, we should have w1 &gt; w2. The main idea is to automatically calculate the popularity of a hypernym given a term, by referring to a large corpus. Then by adding the popularity information to (the edges of) the term→hypernym graph of Freebase, we obtain a weighted term→hypernym graph. The weighted graph is then employed to enhance the estimation of wz. For popularity calculation, we apply Hearst patterns (Hearst, 1992) and is-a patterns (“NP (is|are|was|were|being) (a|an|the) NP”) to every sentence of our web corpus. For a (term, hypernym) pair, its popularity F is calculated as the number of sentences in which the term and the hypernym co-occur and also follow at least one of the patterns. For a template tuple Uz with argument type h and argument value v, we test two ways of estimating the tuple score wz, wz = log (1 + F(v, h)) (4) A + Eh,∈H F(v, h9) F(v, h)) (5) where F(v, h) is the popularity of the (v, h) pair in our corpus, H is the set of all hypernyms for v in the weighted term→hypernym graph. Parame</context>
<context position="24077" citStr="Hearst, 1992" startWordPosition="3939" endWordPosition="3940"> a large probability to be a “country”, so we have more confidence that “city in [country]” is a good template. In this section, we propose to leverage the term similarity information to improve the template scoring process. We start with building a large group of small and overlapped clusters from our web corpus. 4.3.1 Building term clusters Term clusters are built in three steps. Mining term peer similarity: Two terms are peers if they share a common hypernym and they are semantically correlated. For example, “dog” and “cat” should have a high peer similarity score. Following existing work (Hearst, 1992; Kozareva � � � F(~U) = P � n Z=1 804 et al., 2008; Shi et al., 2010; Agirre et al., 2009; Pantel et al., 2009), we built a peer similarity graph containing about 40.5 million nodes and 1.33 billion edges. Clustering: For each term, choose its top-30 neighbors from the peer similarity graph and run a hierarchical clustering algorithm, resulting in one or multiple clusters. Then we merge highly duplicated clusters. The algorithm is similar to the first part of CBC (Pantel and Lin, 2002), with the difference that a very high merging threshold is adopted here in order to generate small and overl</context>
</contexts>
<marker>Hearst, 1992</marker>
<rawString>Marti A. Hearst. 1992. Automatic acquisition of hyponyms from large text corpora. In Proceedings of the 14th conference on Computational linguistics -Volume 2, COLING ’92, pages 539–545, Stroudsburg, PA, USA. Association for Computational Linguistics.</rawString>
</citation>
<citation valid="true">
<authors>
<author>Zornitsa Kozareva</author>
<author>Ellen Riloff</author>
<author>Eduard H Hovy</author>
</authors>
<title>Semantic class learning from the web with hyponym pattern linkage graphs.</title>
<date>2008</date>
<booktitle>In ACL,</booktitle>
<volume>8</volume>
<pages>1048--1056</pages>
<marker>Kozareva, Riloff, Hovy, 2008</marker>
<rawString>Zornitsa Kozareva, Ellen Riloff, and Eduard H Hovy. 2008. Semantic class learning from the web with hyponym pattern linkage graphs. In ACL, volume 8, pages 1048–1056.</rawString>
</citation>
<citation valid="true">
<authors>
<author>Xiao Li</author>
<author>Ye-Yi Wang</author>
<author>Alex Acero</author>
</authors>
<title>Extracting structured information from user queries with semi-supervised conditional random fields.</title>
<date>2009</date>
<booktitle>In Proceedings of the 32nd international ACM SIGIR conference on Research and development in information retrieval,</booktitle>
<pages>572--579</pages>
<publisher>ACM.</publisher>
<contexts>
<context position="5740" citStr="Li et al., 2009" startWordPosition="876" endWordPosition="879">es and larger scores to high-quality ones. There have been some research efforts (Third, 2012; Fernandez-Breis et al., 2010; QuesadaMartınez et al., 2012) on exploring the structure of category names by building patterns. However, we automatically assign semantic types to the pattern variables (or called arguments) while they do not. For example, our template has the form of “city in [country]” while their patterns are like “city in [X]”. More details are given in the related work section. A similar task is query understanding, including query tagging and query template mining. Query tagging (Li et al., 2009; Reisinger and Pasca, 2011) corresponds to the category labeling stage described above. It is different from template generation because the results are for one query only, without merging the information of all queries to generate the final templates. Category template construction are slightly different from query template construction. First, some useful features such as query click-through is not available in category template construction. Second, categories should be valid natural language phrases, while queries need not. For example, “city Germany” is a query but not a valid category n</context>
<context position="9407" citStr="Li et al. (2009)" startWordPosition="1459" endWordPosition="1462">in the form of [subject, verb, object]. The category names and learned templates in our work are not in this form. Another difference between our work and their work is that, their methods need a supervised name classifer to generate the candidate patterns while our approach is unsupervised. Chambers and Jurafsky (2011) leverage templates to describe an event while the templates in our work are for understanding category names (a kind of short text). Query tagging/labeling: Some research work in recent years focuses on segmenting web search queries and assigning semantic tags to key segments. Li et al. (2009) and Li (2010) employed CRF (Conditional Random Field) or semi-CRF models for query tagging. A crowdsourcingassisted method was proposed by Han et al. (2013) for query structure interpretation. These supervised or semi-supervised approaches require much manual annotation effort. Unsupervised methods were proposed by Sarkas et al. (2010) and Reisinger and Pasca (2011). As been discussed in the introduction section, query tagging is only one of the two stages of template generation. The tagging results are for one query only, without aggregating the global information of all queries to generate </context>
</contexts>
<marker>Li, Wang, Acero, 2009</marker>
<rawString>Xiao Li, Ye-Yi Wang, and Alex Acero. 2009. Extracting structured information from user queries with semi-supervised conditional random fields. In Proceedings of the 32nd international ACM SIGIR conference on Research and development in information retrieval, pages 572–579. ACM.</rawString>
</citation>
<citation valid="true">
<authors>
<author>Yanen Li</author>
<author>Bo-June Paul Hsu</author>
<author>ChengXiang Zhai</author>
</authors>
<title>Unsupervised identification of synonymous query intent templates for attribute intents.</title>
<date>2013</date>
<booktitle>In Proceedings of the 22nd ACM international conference on Conference on information &amp; knowledge management,</booktitle>
<pages>2029--2038</pages>
<publisher>ACM.</publisher>
<contexts>
<context position="10997" citStr="Li et al. (2013)" startWordPosition="1706" endWordPosition="1709"> and Punera (2012) proposed an unsupervised method based on graphical models to mine query templates. The above methods are either domain-specific (i.e., generating templates for a specific domain), or have some degree of supervision (supervised or semi-supervised). Cheung and Li (2012) proposed an unsupervised method to generate query templates by the aid of knowledge bases. An approach was proposed in (Szpektor et al., 2011) to improve query recommendation via query templates. Query session information (which is not available in our task) is needed in this approach for templates generation. Li et al. (2013) proposed an clustering algorithm to group existing query templates by search intents of users. Compared to the open-domain unsupervised methods for query template construction, our approach improves on two aspects. First, we propose to incorporate multiple types of semantic knowledge (e.g., term peer similarity and term clusters) to improve template generation. Second, we propose a nonlinear template scoring function which is demonstrated to be more effective. 3 Problem Definition and Analysis 3.1 Problem definition The goal of this paper is to construct a list of category templates from a co</context>
</contexts>
<marker>Li, Hsu, Zhai, 2013</marker>
<rawString>Yanen Li, Bo-June Paul Hsu, and ChengXiang Zhai. 2013. Unsupervised identification of synonymous query intent templates for attribute intents. In Proceedings of the 22nd ACM international conference on Conference on information &amp; knowledge management, pages 2029–2038. ACM.</rawString>
</citation>
<citation valid="true">
<authors>
<author>Xiao Li</author>
</authors>
<title>Understanding the semantic structure of noun phrase queries.</title>
<date>2010</date>
<booktitle>In Proceedings of the 48th Annual Meeting of the Association for Computational Linguistics,</booktitle>
<pages>1337--1345</pages>
<publisher>Association</publisher>
<institution>for Computational Linguistics.</institution>
<contexts>
<context position="9421" citStr="Li (2010)" startWordPosition="1464" endWordPosition="1465">ct, verb, object]. The category names and learned templates in our work are not in this form. Another difference between our work and their work is that, their methods need a supervised name classifer to generate the candidate patterns while our approach is unsupervised. Chambers and Jurafsky (2011) leverage templates to describe an event while the templates in our work are for understanding category names (a kind of short text). Query tagging/labeling: Some research work in recent years focuses on segmenting web search queries and assigning semantic tags to key segments. Li et al. (2009) and Li (2010) employed CRF (Conditional Random Field) or semi-CRF models for query tagging. A crowdsourcingassisted method was proposed by Han et al. (2013) for query structure interpretation. These supervised or semi-supervised approaches require much manual annotation effort. Unsupervised methods were proposed by Sarkas et al. (2010) and Reisinger and Pasca (2011). As been discussed in the introduction section, query tagging is only one of the two stages of template generation. The tagging results are for one query only, without aggregating the global information of all queries to generate the final temp</context>
</contexts>
<marker>Li, 2010</marker>
<rawString>Xiao Li. 2010. Understanding the semantic structure of noun phrase queries. In Proceedings of the 48th Annual Meeting of the Association for Computational Linguistics, pages 1337–1345. Association for Computational Linguistics.</rawString>
</citation>
<citation valid="true">
<authors>
<author>Sandeep Pandey</author>
<author>Kunal Punera</author>
</authors>
<title>Unsupervised extraction of template structure in web search queries.</title>
<date>2012</date>
<booktitle>In Proceedings of the 21st international conference on World Wide Web,</booktitle>
<pages>409--418</pages>
<publisher>ACM.</publisher>
<contexts>
<context position="10399" citStr="Pandey and Punera (2012)" startWordPosition="1612" endWordPosition="1615">). As been discussed in the introduction section, query tagging is only one of the two stages of template generation. The tagging results are for one query only, without aggregating the global information of all queries to generate the final templates. Query template construction: Some existing work leveraged query templates or patterns for query understanding. A semi-supervised random walk based method was proposed by Agarwal et al. (2010) to generate a ranked templates list which are relevant to a domain of interest. A predefined domain schema and seed information is needed for this method. Pandey and Punera (2012) proposed an unsupervised method based on graphical models to mine query templates. The above methods are either domain-specific (i.e., generating templates for a specific domain), or have some degree of supervision (supervised or semi-supervised). Cheung and Li (2012) proposed an unsupervised method to generate query templates by the aid of knowledge bases. An approach was proposed in (Szpektor et al., 2011) to improve query recommendation via query templates. Query session information (which is not available in our task) is needed in this approach for templates generation. Li et al. (2013) p</context>
</contexts>
<marker>Pandey, Punera, 2012</marker>
<rawString>Sandeep Pandey and Kunal Punera. 2012. Unsupervised extraction of template structure in web search queries. In Proceedings of the 21st international conference on World Wide Web, pages 409–418. ACM.</rawString>
</citation>
<citation valid="true">
<authors>
<author>Patrick Pantel</author>
<author>Dekang Lin</author>
</authors>
<title>Discovering word senses from text.</title>
<date>2002</date>
<booktitle>In Proceedings of the eighth ACM SIGKDD international conference on Knowledge discovery and data mining,</booktitle>
<pages>613--619</pages>
<publisher>ACM.</publisher>
<contexts>
<context position="24568" citStr="Pantel and Lin, 2002" startWordPosition="4025" endWordPosition="4028">antically correlated. For example, “dog” and “cat” should have a high peer similarity score. Following existing work (Hearst, 1992; Kozareva � � � F(~U) = P � n Z=1 804 et al., 2008; Shi et al., 2010; Agirre et al., 2009; Pantel et al., 2009), we built a peer similarity graph containing about 40.5 million nodes and 1.33 billion edges. Clustering: For each term, choose its top-30 neighbors from the peer similarity graph and run a hierarchical clustering algorithm, resulting in one or multiple clusters. Then we merge highly duplicated clusters. The algorithm is similar to the first part of CBC (Pantel and Lin, 2002), with the difference that a very high merging threshold is adopted here in order to generate small and overlapped clusters. Please note that one term may be included in many clusters. Assigning top hypernyms: Up to two hypernyms are assigned for each term cluster by majority voting of its member terms, with the aid of the weighted term→hypernym graph of Section 4.1. To be an eligible hypernym for the cluster, it has to be the hypernym of at least 70% of terms in the cluster. The score of each hypernym is the average of the term→hypernym weights over all the member terms. 4.3.2 Template score </context>
</contexts>
<marker>Pantel, Lin, 2002</marker>
<rawString>Patrick Pantel and Dekang Lin. 2002. Discovering word senses from text. In Proceedings of the eighth ACM SIGKDD international conference on Knowledge discovery and data mining, pages 613–619. ACM.</rawString>
</citation>
<citation valid="true">
<authors>
<author>Patrick Pantel</author>
<author>Deepak Ravichandran</author>
</authors>
<title>Automatically labeling semantic classes.</title>
<date>2004</date>
<booktitle>In HLTNAACL,</booktitle>
<volume>4</volume>
<pages>321--328</pages>
<contexts>
<context position="1489" citStr="Pantel and Ravichandran, 2004" startWordPosition="219" endWordPosition="223">hieves significantly better results than baseline methods. As an immediate application, we apply the extracted templates to the cleaning of a category collection and see promising results (precision improved from 81% to 89%). 1 Introduction A semantic category is a collection of items sharing common semantic properties. For example, all cities in Germany form a semantic category named “city in Germany” or “German city”. In Wikipedia, the category names of an entity are manually edited and displayed at the end of the page for the entity. There have been quite a lot of approaches (Hearst, 1992; Pantel and Ravichandran, 2004; Van Durme and Pasca, 2008; Zhang et al., 2011) in the literature to automatically extracting category names and instances (also called is-a or hypernymy relations) from the web. Most existing work simply treats a category name as a text string containing one or multiple words, without caring about its internal structure. In this paper, we explore the semantic structure of category names (or simply called “categories”). ∗This work was performed when the first author was visiting Microsoft Research Asia. For example, both “CEO of General Motors” and “CEO of Yahoo” have structure “CEO of [compa</context>
<context position="7593" citStr="Pantel and Ravichandran, 2004" startWordPosition="1170" endWordPosition="1173"> templates to clean our category collection mined from the web, and get apparent quality improvement (precision improved from 81% to 89%). After discussing related work in Section 2, we define the problem and describe one baseline approach in Section 3. Then we introduce our approach in Section 4. Experimental results are reported and analyzed in Section 5. We conclude the paper in Section 6. 2 Related work Several kinds of work are related to ours. Hypernymy relation extraction: Hypernymy relation extraction is an important task in text mining. There have been a lot of efforts (Hearst, 1992; Pantel and Ravichandran, 2004; Van Durme and Pasca, 2008; Zhang et al., 2011) in the literature to extract hypernymy (or is-a) relations from the web. Our target here is not hypernymy extraction, but discovering the semantic structure of hypernyms (or category names). Category name exploration: Category name patterns are explored and built in some existing research work. Third (2012) proposed to find axiom patterns among category names on an existing ontology. For example, infer axiom pattern “SubClassOf(AB, B)” from “SubClassOf(junior school school)” and “SubClassOf(domestic mammal mammal)”. Fernandez-Breis et al. (2010)</context>
<context position="11934" citStr="Pantel and Ravichandran 2004" startWordPosition="1850" endWordPosition="1853">rity and term clusters) to improve template generation. Second, we propose a nonlinear template scoring function which is demonstrated to be more effective. 3 Problem Definition and Analysis 3.1 Problem definition The goal of this paper is to construct a list of category templates from a collection of open-domain category names. Input: The input is a collection of category names, which can either be manually compiled (like Wikipedia categories) or be automatically extracted. The categories used in our experiments were automatically mined from the web, by following existing work (Hearst, 1992, Pantel and Ravichandran 2004; Snow et al., 2005; Talukdar et al., 2008; Zhang et al., 2011). Specifically, we applied Hearst patterns (e.g., “NP [,] (such as |including) {NP, }∗ {and|or} NP”) and isa patterns (“NP (is|are|was|were|being) (a|an|the) NP”) to a large corpus containing 3 billion English web pages. As a result, we obtained a term—*hypernym bi-partite graph containing 40 million terms, 74 million hypernyms (i.e., category names), and 321 million edges (e.g., one example edge is “Berlin”—*“city in Germany”, where “Berlin” is a term and “city in Germany” is the corresponding hypernym). Then all the multiword hyp</context>
</contexts>
<marker>Pantel, Ravichandran, 2004</marker>
<rawString>Patrick Pantel and Deepak Ravichandran. 2004. Automatically labeling semantic classes. In HLTNAACL, volume 4, pages 321–328.</rawString>
</citation>
<citation valid="true">
<authors>
<author>Patrick Pantel</author>
</authors>
<title>Eric Crestan, Arkady Borkovsky, AnaMaria Popescu, and Vishnu Vyas.</title>
<date>2009</date>
<booktitle>In Proceedings of the 2009 Conference on Empirical Methods in Natural Language Processing: Volume</booktitle>
<volume>2</volume>
<pages>938--947</pages>
<publisher>Association</publisher>
<institution>for Computational Linguistics.</institution>
<marker>Pantel, 2009</marker>
<rawString>Patrick Pantel, Eric Crestan, Arkady Borkovsky, AnaMaria Popescu, and Vishnu Vyas. 2009. Web-scale distributional similarity and entity set expansion. In Proceedings of the 2009 Conference on Empirical Methods in Natural Language Processing: Volume 2-Volume 2, pages 938–947. Association for Computational Linguistics.</rawString>
</citation>
<citation valid="true">
<authors>
<author>Manuel Quesada-Martınez</author>
<author>Jesualdo Tom´as Fern´andez-Breis</author>
<author>Robert Stevens</author>
</authors>
<title>Enrichment of owl ontologies: a method for defining axioms from labels.</title>
<date>2012</date>
<booktitle>In Proceedings of the First International Workshop on Capturing and Refining Knowledge in the Medical Domain (K-MED 2012),</booktitle>
<pages>1--10</pages>
<location>Galway, Ireland,</location>
<marker>Quesada-Martınez, Fern´andez-Breis, Stevens, 2012</marker>
<rawString>Manuel Quesada-Martınez, Jesualdo Tom´as Fern´andez-Breis, and Robert Stevens. 2012. Enrichment of owl ontologies: a method for defining axioms from labels. In Proceedings of the First International Workshop on Capturing and Refining Knowledge in the Medical Domain (K-MED 2012), Galway, Ireland, pages 1–10.</rawString>
</citation>
<citation valid="true">
<authors>
<author>Joseph Reisinger</author>
<author>Marius Pasca</author>
</authors>
<title>Finegrained class label markup of search queries.</title>
<date>2011</date>
<booktitle>In ACL,</booktitle>
<pages>1200--1209</pages>
<contexts>
<context position="5768" citStr="Reisinger and Pasca, 2011" startWordPosition="880" endWordPosition="883">res to high-quality ones. There have been some research efforts (Third, 2012; Fernandez-Breis et al., 2010; QuesadaMartınez et al., 2012) on exploring the structure of category names by building patterns. However, we automatically assign semantic types to the pattern variables (or called arguments) while they do not. For example, our template has the form of “city in [country]” while their patterns are like “city in [X]”. More details are given in the related work section. A similar task is query understanding, including query tagging and query template mining. Query tagging (Li et al., 2009; Reisinger and Pasca, 2011) corresponds to the category labeling stage described above. It is different from template generation because the results are for one query only, without merging the information of all queries to generate the final templates. Category template construction are slightly different from query template construction. First, some useful features such as query click-through is not available in category template construction. Second, categories should be valid natural language phrases, while queries need not. For example, “city Germany” is a query but not a valid category name. We discuss in more deta</context>
<context position="9776" citStr="Reisinger and Pasca (2011)" startWordPosition="1514" endWordPosition="1517">nt while the templates in our work are for understanding category names (a kind of short text). Query tagging/labeling: Some research work in recent years focuses on segmenting web search queries and assigning semantic tags to key segments. Li et al. (2009) and Li (2010) employed CRF (Conditional Random Field) or semi-CRF models for query tagging. A crowdsourcingassisted method was proposed by Han et al. (2013) for query structure interpretation. These supervised or semi-supervised approaches require much manual annotation effort. Unsupervised methods were proposed by Sarkas et al. (2010) and Reisinger and Pasca (2011). As been discussed in the introduction section, query tagging is only one of the two stages of template generation. The tagging results are for one query only, without aggregating the global information of all queries to generate the final templates. Query template construction: Some existing work leveraged query templates or patterns for query understanding. A semi-supervised random walk based method was proposed by Agarwal et al. (2010) to generate a ranked templates list which are relevant to a domain of interest. A predefined domain schema and seed information is needed for this method. P</context>
</contexts>
<marker>Reisinger, Pasca, 2011</marker>
<rawString>Joseph Reisinger and Marius Pasca. 2011. Finegrained class label markup of search queries. In ACL, pages 1200–1209.</rawString>
</citation>
<citation valid="true">
<authors>
<author>Nikos Sarkas</author>
</authors>
<title>Stelios Paparizos, and Panayiotis Tsaparas.</title>
<date>2010</date>
<booktitle>In Proceedings of the 2010 ACM SIGMOD International Conference on Management of data,</booktitle>
<pages>771--782</pages>
<publisher>ACM.</publisher>
<marker>Sarkas, 2010</marker>
<rawString>Nikos Sarkas, Stelios Paparizos, and Panayiotis Tsaparas. 2010. Structured annotations of web queries. In Proceedings of the 2010 ACM SIGMOD International Conference on Management of data, pages 771–782. ACM.</rawString>
</citation>
<citation valid="true">
<authors>
<author>Shuming Shi</author>
<author>Huibin Zhang</author>
<author>Xiaojie Yuan</author>
<author>JiRong Wen</author>
</authors>
<title>Corpus-based semantic class mining: distributional vs. pattern-based approaches.</title>
<date>2010</date>
<booktitle>In Proceedings of the 23rd International Conference on Computational Linguistics,</booktitle>
<pages>993--1001</pages>
<publisher>Association</publisher>
<institution>for Computational Linguistics.</institution>
<contexts>
<context position="24146" citStr="Shi et al., 2010" startWordPosition="3955" endWordPosition="3958">e that “city in [country]” is a good template. In this section, we propose to leverage the term similarity information to improve the template scoring process. We start with building a large group of small and overlapped clusters from our web corpus. 4.3.1 Building term clusters Term clusters are built in three steps. Mining term peer similarity: Two terms are peers if they share a common hypernym and they are semantically correlated. For example, “dog” and “cat” should have a high peer similarity score. Following existing work (Hearst, 1992; Kozareva � � � F(~U) = P � n Z=1 804 et al., 2008; Shi et al., 2010; Agirre et al., 2009; Pantel et al., 2009), we built a peer similarity graph containing about 40.5 million nodes and 1.33 billion edges. Clustering: For each term, choose its top-30 neighbors from the peer similarity graph and run a hierarchical clustering algorithm, resulting in one or multiple clusters. Then we merge highly duplicated clusters. The algorithm is similar to the first part of CBC (Pantel and Lin, 2002), with the difference that a very high merging threshold is adopted here in order to generate small and overlapped clusters. Please note that one term may be included in many clu</context>
</contexts>
<marker>Shi, Zhang, Yuan, Wen, 2010</marker>
<rawString>Shuming Shi, Huibin Zhang, Xiaojie Yuan, and JiRong Wen. 2010. Corpus-based semantic class mining: distributional vs. pattern-based approaches. In Proceedings of the 23rd International Conference on Computational Linguistics, pages 993–1001. Association for Computational Linguistics.</rawString>
</citation>
<citation valid="true">
<authors>
<author>Mark Stevenson</author>
<author>Mark A Greenwood</author>
</authors>
<title>A semantic approach to ie pattern induction.</title>
<date>2005</date>
<booktitle>In Proceedings of the 43rd Annual Meeting on Association for Computational Linguistics,</booktitle>
<pages>379--386</pages>
<publisher>Association</publisher>
<institution>for Computational Linguistics.</institution>
<contexts>
<context position="8752" citStr="Stevenson and Greenwood (2005)" startWordPosition="1349" endWordPosition="1352">and “SubClassOf(domestic mammal mammal)”. Fernandez-Breis et al. (2010) and QuesadaMartınez et al. (2012) proposed to find lexical patterns in category names to define axioms (in medical domain). One example pattern mentioned in their papers is “[X] binding”. They need manual intervention to determine what X means. The main difference between the above work and ours is that we automatically assign semantic types to the pattern variables (or called arguments) while they do not. 800 Template mining for IE: Some research work in information extraction (IE) involves patterns. Yangarber (2003) and Stevenson and Greenwood (2005) proposed to learn patterns which were in the form of [subject, verb, object]. The category names and learned templates in our work are not in this form. Another difference between our work and their work is that, their methods need a supervised name classifer to generate the candidate patterns while our approach is unsupervised. Chambers and Jurafsky (2011) leverage templates to describe an event while the templates in our work are for understanding category names (a kind of short text). Query tagging/labeling: Some research work in recent years focuses on segmenting web search queries and as</context>
</contexts>
<marker>Stevenson, Greenwood, 2005</marker>
<rawString>Mark Stevenson and Mark A Greenwood. 2005. A semantic approach to ie pattern induction. In Proceedings of the 43rd Annual Meeting on Association for Computational Linguistics, pages 379–386. Association for Computational Linguistics.</rawString>
</citation>
<citation valid="true">
<authors>
<author>Idan Szpektor</author>
<author>Aristides Gionis</author>
<author>Yoelle Maarek</author>
</authors>
<title>Improving recommendation for long-tail queries via templates.</title>
<date>2011</date>
<booktitle>In Proceedings of the 20th international conference on World wide web,</booktitle>
<pages>47--56</pages>
<publisher>ACM.</publisher>
<contexts>
<context position="10811" citStr="Szpektor et al., 2011" startWordPosition="1675" endWordPosition="1678">posed by Agarwal et al. (2010) to generate a ranked templates list which are relevant to a domain of interest. A predefined domain schema and seed information is needed for this method. Pandey and Punera (2012) proposed an unsupervised method based on graphical models to mine query templates. The above methods are either domain-specific (i.e., generating templates for a specific domain), or have some degree of supervision (supervised or semi-supervised). Cheung and Li (2012) proposed an unsupervised method to generate query templates by the aid of knowledge bases. An approach was proposed in (Szpektor et al., 2011) to improve query recommendation via query templates. Query session information (which is not available in our task) is needed in this approach for templates generation. Li et al. (2013) proposed an clustering algorithm to group existing query templates by search intents of users. Compared to the open-domain unsupervised methods for query template construction, our approach improves on two aspects. First, we propose to incorporate multiple types of semantic knowledge (e.g., term peer similarity and term clusters) to improve template generation. Second, we propose a nonlinear template scoring f</context>
</contexts>
<marker>Szpektor, Gionis, Maarek, 2011</marker>
<rawString>Idan Szpektor, Aristides Gionis, and Yoelle Maarek. 2011. Improving recommendation for long-tail queries via templates. In Proceedings of the 20th international conference on World wide web, pages 47–56. ACM.</rawString>
</citation>
<citation valid="true">
<authors>
<author>Allan Third</author>
</authors>
<title>Hidden semantics: what can we learn from the names in an ontology?</title>
<date>2012</date>
<booktitle>In Proceedings of the Seventh International Natural Language Generation Conference,</booktitle>
<pages>67--75</pages>
<publisher>Association</publisher>
<institution>for Computational Linguistics.</institution>
<contexts>
<context position="5218" citStr="Third, 2012" startWordPosition="795" endWordPosition="796">ng template), “CEO of [company]”, and the like. Template scoring: Compute the score of each candidate template by aggregating the information obtained in the first phase. A major challenge here is that many segments (like “Delphinus” in the above example) have multiple meanings. As a result, wrong hypernyms may be adopted to generate incorrect candidate templates (like “CEO of [constellation]”). In this paper, we focus on improving the template scoring stage, with the goal of assigning lower scores to bad templates and larger scores to high-quality ones. There have been some research efforts (Third, 2012; Fernandez-Breis et al., 2010; QuesadaMartınez et al., 2012) on exploring the structure of category names by building patterns. However, we automatically assign semantic types to the pattern variables (or called arguments) while they do not. For example, our template has the form of “city in [country]” while their patterns are like “city in [X]”. More details are given in the related work section. A similar task is query understanding, including query tagging and query template mining. Query tagging (Li et al., 2009; Reisinger and Pasca, 2011) corresponds to the category labeling stage descri</context>
<context position="7950" citStr="Third (2012)" startWordPosition="1228" endWordPosition="1229">e paper in Section 6. 2 Related work Several kinds of work are related to ours. Hypernymy relation extraction: Hypernymy relation extraction is an important task in text mining. There have been a lot of efforts (Hearst, 1992; Pantel and Ravichandran, 2004; Van Durme and Pasca, 2008; Zhang et al., 2011) in the literature to extract hypernymy (or is-a) relations from the web. Our target here is not hypernymy extraction, but discovering the semantic structure of hypernyms (or category names). Category name exploration: Category name patterns are explored and built in some existing research work. Third (2012) proposed to find axiom patterns among category names on an existing ontology. For example, infer axiom pattern “SubClassOf(AB, B)” from “SubClassOf(junior school school)” and “SubClassOf(domestic mammal mammal)”. Fernandez-Breis et al. (2010) and QuesadaMartınez et al. (2012) proposed to find lexical patterns in category names to define axioms (in medical domain). One example pattern mentioned in their papers is “[X] binding”. They need manual intervention to determine what X means. The main difference between the above work and ours is that we automatically assign semantic types to the patte</context>
</contexts>
<marker>Third, 2012</marker>
<rawString>Allan Third. 2012. Hidden semantics: what can we learn from the names in an ontology? In Proceedings of the Seventh International Natural Language Generation Conference, pages 67–75. Association for Computational Linguistics.</rawString>
</citation>
<citation valid="true">
<authors>
<author>Benjamin Van Durme</author>
<author>Marius Pasca</author>
</authors>
<title>Finding cars, goddesses and enzymes: Parametrizable acquisition of labeled instances for open-domain information extraction.</title>
<date>2008</date>
<booktitle>In AAAI,</booktitle>
<volume>8</volume>
<pages>1243--1248</pages>
<marker>Van Durme, Pasca, 2008</marker>
<rawString>Benjamin Van Durme and Marius Pasca. 2008. Finding cars, goddesses and enzymes: Parametrizable acquisition of labeled instances for open-domain information extraction. In AAAI, volume 8, pages 1243–1248.</rawString>
</citation>
<citation valid="true">
<authors>
<author>Roman Yangarber</author>
</authors>
<title>Counter-training in discovery of semantic patterns.</title>
<date>2003</date>
<booktitle>In Proceedings of the 41st Annual Meeting on Association for Computational Linguistics-Volume 1,</booktitle>
<pages>343--350</pages>
<publisher>Association</publisher>
<institution>for Computational Linguistics.</institution>
<contexts>
<context position="8717" citStr="Yangarber (2003)" startWordPosition="1346" endWordPosition="1347">nior school school)” and “SubClassOf(domestic mammal mammal)”. Fernandez-Breis et al. (2010) and QuesadaMartınez et al. (2012) proposed to find lexical patterns in category names to define axioms (in medical domain). One example pattern mentioned in their papers is “[X] binding”. They need manual intervention to determine what X means. The main difference between the above work and ours is that we automatically assign semantic types to the pattern variables (or called arguments) while they do not. 800 Template mining for IE: Some research work in information extraction (IE) involves patterns. Yangarber (2003) and Stevenson and Greenwood (2005) proposed to learn patterns which were in the form of [subject, verb, object]. The category names and learned templates in our work are not in this form. Another difference between our work and their work is that, their methods need a supervised name classifer to generate the candidate patterns while our approach is unsupervised. Chambers and Jurafsky (2011) leverage templates to describe an event while the templates in our work are for understanding category names (a kind of short text). Query tagging/labeling: Some research work in recent years focuses on s</context>
</contexts>
<marker>Yangarber, 2003</marker>
<rawString>Roman Yangarber. 2003. Counter-training in discovery of semantic patterns. In Proceedings of the 41st Annual Meeting on Association for Computational Linguistics-Volume 1, pages 343–350. Association for Computational Linguistics.</rawString>
</citation>
<citation valid="true">
<authors>
<author>Fan Zhang</author>
<author>Shuming Shi</author>
<author>Jing Liu</author>
<author>Shuqi Sun</author>
<author>Chin-Yew Lin</author>
</authors>
<title>Nonlinear evidence fusion and propagation for hyponymy relation mining.</title>
<date>2011</date>
<booktitle>In ACL,</booktitle>
<volume>11</volume>
<pages>1159--1168</pages>
<contexts>
<context position="1537" citStr="Zhang et al., 2011" startWordPosition="229" endWordPosition="232"> As an immediate application, we apply the extracted templates to the cleaning of a category collection and see promising results (precision improved from 81% to 89%). 1 Introduction A semantic category is a collection of items sharing common semantic properties. For example, all cities in Germany form a semantic category named “city in Germany” or “German city”. In Wikipedia, the category names of an entity are manually edited and displayed at the end of the page for the entity. There have been quite a lot of approaches (Hearst, 1992; Pantel and Ravichandran, 2004; Van Durme and Pasca, 2008; Zhang et al., 2011) in the literature to automatically extracting category names and instances (also called is-a or hypernymy relations) from the web. Most existing work simply treats a category name as a text string containing one or multiple words, without caring about its internal structure. In this paper, we explore the semantic structure of category names (or simply called “categories”). ∗This work was performed when the first author was visiting Microsoft Research Asia. For example, both “CEO of General Motors” and “CEO of Yahoo” have structure “CEO of [company]”. We call such a structure a category templa</context>
<context position="7641" citStr="Zhang et al., 2011" startWordPosition="1179" endWordPosition="1182"> web, and get apparent quality improvement (precision improved from 81% to 89%). After discussing related work in Section 2, we define the problem and describe one baseline approach in Section 3. Then we introduce our approach in Section 4. Experimental results are reported and analyzed in Section 5. We conclude the paper in Section 6. 2 Related work Several kinds of work are related to ours. Hypernymy relation extraction: Hypernymy relation extraction is an important task in text mining. There have been a lot of efforts (Hearst, 1992; Pantel and Ravichandran, 2004; Van Durme and Pasca, 2008; Zhang et al., 2011) in the literature to extract hypernymy (or is-a) relations from the web. Our target here is not hypernymy extraction, but discovering the semantic structure of hypernyms (or category names). Category name exploration: Category name patterns are explored and built in some existing research work. Third (2012) proposed to find axiom patterns among category names on an existing ontology. For example, infer axiom pattern “SubClassOf(AB, B)” from “SubClassOf(junior school school)” and “SubClassOf(domestic mammal mammal)”. Fernandez-Breis et al. (2010) and QuesadaMartınez et al. (2012) proposed to f</context>
<context position="11997" citStr="Zhang et al., 2011" startWordPosition="1862" endWordPosition="1865">se a nonlinear template scoring function which is demonstrated to be more effective. 3 Problem Definition and Analysis 3.1 Problem definition The goal of this paper is to construct a list of category templates from a collection of open-domain category names. Input: The input is a collection of category names, which can either be manually compiled (like Wikipedia categories) or be automatically extracted. The categories used in our experiments were automatically mined from the web, by following existing work (Hearst, 1992, Pantel and Ravichandran 2004; Snow et al., 2005; Talukdar et al., 2008; Zhang et al., 2011). Specifically, we applied Hearst patterns (e.g., “NP [,] (such as |including) {NP, }∗ {and|or} NP”) and isa patterns (“NP (is|are|was|were|being) (a|an|the) NP”) to a large corpus containing 3 billion English web pages. As a result, we obtained a term—*hypernym bi-partite graph containing 40 million terms, 74 million hypernyms (i.e., category names), and 321 million edges (e.g., one example edge is “Berlin”—*“city in Germany”, where “Berlin” is a term and “city in Germany” is the corresponding hypernym). Then all the multiword hypernyms are used as the input category collection. Output: The o</context>
<context position="21822" citStr="Zhang et al., 2011" startWordPosition="3548" endWordPosition="3551">It is easy to prove that the above conclusion holds true when n &gt; 2, i.e., n G(T |E1, ..., En) = G(T |EZ) (9) Z=1 If we treat G(T |EZ) as the score of template T when only UZ is observed, and G(T |E1, ..., En) as the template score after the n tuples are observed, then the above equation means that the combined template score should be the sum of wZ · IDF(h), which is exactly Formula 1. Please keep in mind that Equation 9 is based on the assumption that the tuples are conditional independent. This assumption, however, may not hold in reality. The case of conditional dependence was studied in (Zhang et al., 2011), where a group of nonlinear combination functions were proposed and achieved good performance in their task of hypernymy extraction. We choose p-Norm as our nonlinear fusion functions, as below, wp · IDF(h) (p &gt; 1) (10) Z where p (=2 in experiments) is a parameter. Experiments show that the above nonlinear function performs better than the linear function of Formula 1. Let’s use an example to show the intuition. Consider a good template “city of [country]” corresponding to CTTs ~UA and a wrong template “city of [book]” having tuples ~UB. Suppose |~UA |= 200 (including most countries in the wo</context>
</contexts>
<marker>Zhang, Shi, Liu, Sun, Lin, 2011</marker>
<rawString>Fan Zhang, Shuming Shi, Jing Liu, Shuqi Sun, and Chin-Yew Lin. 2011. Nonlinear evidence fusion and propagation for hyponymy relation mining. In ACL, volume 11, pages 1159–1168.</rawString>
</citation>
</citationList>
</algorithm>
</algorithms>
<?xml version="1.0" encoding="UTF-8"?>
<algorithms version="110505">
<algorithm name="SectLabel" version="110505">
<variant no="0" confidence="0.000007">
<title confidence="0.994902">
Positive Unlabeled Learning for Deceptive Reviews Detection
</title>
<author confidence="0.989759">
Yafeng Ren Donghong Ji Hongbin Zhang
</author>
<affiliation confidence="0.897087666666667">
Computer School
Wuhan University
Wuhan 430072, China
</affiliation>
<email confidence="0.997247">
{renyafeng,dhji,zhanghongbin}@whu.edu.cn
</email>
<sectionHeader confidence="0.994754" genericHeader="abstract">
Abstract
</sectionHeader>
<bodyText confidence="0.999982866666667">
Deceptive reviews detection has attract-
ed significant attention from both business
and research communities. However, due
to the difficulty of human labeling need-
ed for supervised learning, the problem re-
mains to be highly challenging. This pa-
per proposed a novel angle to the prob-
lem by modeling PU (positive unlabeled)
learning. A semi-supervised model, called
mixing population and individual proper-
ty PU learning (MPIPUL), is proposed.
Firstly, some reliable negative examples
are identified from the unlabeled dataset.
Secondly, some representative positive ex-
amples and negative examples are gener-
ated based on LDA (Latent Dirichlet Al-
location). Thirdly, for the remaining un-
labeled examples (we call them spy ex-
amples), which can not be explicitly iden-
tified as positive and negative, two simi-
larity weights are assigned, by which the
probability of a spy example belonging to
the positive class and the negative class
are displayed. Finally, spy examples and
their similarity weights are incorporated
into SVM (Support Vector Machine) to
build an accurate classifier. Experiments
on gold-standard dataset demonstrate the
effectiveness of MPIPUL which outper-
forms the state-of-the-art baselines.
</bodyText>
<sectionHeader confidence="0.999162" genericHeader="keywords">
1 Introduction
</sectionHeader>
<bodyText confidence="0.997927826086957">
The Web has dramatically changed the way peo-
ple express themselves and interact with others,
people frequently write reviews on e-commerce
sites, forums and blogs to achieve these purpos-
es. For NLP (Natural Language Processing), these
user-generated contents are of great value in that
they contain abundant information related to peo-
ple’s opinions on certain topics. Currently, on-
line reviews on products and services are used
extensively by consumers and businesses to con-
duct decisive purchase, product design and mar-
keting strategies. Hence, sentiment analysis and
opinion mining based on product reviews have
become a popular topic of NLP (Pang and Lee,
2008; Liu, 2012). However, since reviews infor-
mation can guide people’s purchase behavior, pos-
itive reviews can result in huge economic benefit-
s and fame for organizations or individuals. This
leaves room for promoting the generation of re-
view spams. Through observations and studies of
the predecessors (Jindal and Liu, 2008; Ott et al.,
2011), review spams are divided into the following
two classes:
</bodyText>
<listItem confidence="0.9480386">
• Deceptive Reviews: Those deliberately mis-
lead readers by giving undeserving positive
reviews to some target objects in order to pro-
mote the objects, or by giving unjust nega-
tive reviews to some target objects in order to
damage their reputation.
• Disruptive Reviews: Those are non-reviews,
which mainly include advertisements and
other irrelevant reviews containing no opin-
ion.
</listItem>
<bodyText confidence="0.9998125">
Disruptive reviews pose little threat to peo-
ple, because human can easily identify and ignore
them. In this paper, we focus on the more chal-
lenging ones: deceptive reviews. Generally, de-
ceptive reviews detection is deemed to be a classi-
fication problem (Ott et al., 2011; Li et al., 2011;
Feng et al., 2012). Based on the positive and neg-
ative examples annotated by people, supervised
learning is utilized to build a classifier, and then an
unlabeled review can be predicted as deceptive re-
view or truthful one. But the work from Ott et al.
(2011) shows that human cannot identify decep-
tive reviews from their prior knowledge, which in-
dicates that human-annotated review datasets must
</bodyText>
<page confidence="0.975305">
488
</page>
<note confidence="0.9107485">
Proceedings of the 2014 Conference on Empirical Methods in Natural Language Processing (EMNLP), pages 488–498,
October 25-29, 2014, Doha, Qatar. c�2014 Association for Computational Linguistics
</note>
<bodyText confidence="0.998133057142857">
include some mislabeled examples. These exam-
ples will disturb the generation ability of the clas-
sifiers. So simple supervised learning is regarded
as unsuitable for this task.
It is difficult to come by human labeling need-
ed for supervised learning and evaluation, we can-
not obtain the datasets containing deceptive re-
views. However, we can get some truthful reviews
with high confidence by heuristic rules and prior
knowledge. Meanwhile, a lot of unlabeled reviews
are available. The problem thus is this: based on
some truthful reviews and a lot of unlabeled re-
views, can we build an accurate classifier to iden-
tify deceptive reviews.
PU (positive unlabeled) learning can be utilized
to deal with the above situation (Liu et al., 2002;
Liu et al., 2003). Different from traditional super-
vised learning, PU learning can still build an ac-
curate classifier even without the negative training
examples. Several PU learning techniques have
been applied successfully in document classifica-
tion with promising results (Zhang, 2005; Elkan
and Noto, 2008; Li et al., 2009; Xiao et al., 2011),
while they have yet to be applied in detecting de-
ceptive reviews. Here, we will study how to design
PU learning to detect deceptive reviews.
An important challenge is how to deal with
spy examples (easily mislabeled) of unlabeled re-
views, which is not easily handled by the previous
PU learning techniques. In this paper, we propose
a novel approach, mixing population and individ-
ual property PU learning (MPIPUL), by assigning
similarity weights and incorporating weights into
SVM learning phase. This paper makes the fol-
lowing contributions:
</bodyText>
<listItem confidence="0.99947875">
• For the first time, PU learning is defined in
the environment of identifying deceptive re-
views.
• A novel PU learning is proposed based on L-
DA and SVM.
• Experimental results demonstrate that our
proposed method outperforms the curren-
t baselines.
</listItem>
<sectionHeader confidence="0.999588" genericHeader="introduction">
2 Related Work
</sectionHeader>
<subsectionHeader confidence="0.994875">
2.1 Deceptive Reviews Detection
</subsectionHeader>
<bodyText confidence="0.999944592592593">
Spam has historically been investigated in the con-
texts of e-mail (Drucker et al., 1999; Gyongyi et
al., 2004) and the Web (Ntoulas et al., 2006). In
recent years, researchers have started to look at de-
ceptive reviews.
Jindal and Liu (2008) found that opinion s-
pam was widespread and different from e-mail
and Web spam in essence (Jindal and Liu, 2008).
They trained models using product review data,
by defining features to distinguish duplicate opin-
ion and non-duplicate based on the review tex-
t, reviewers and product information. Wu et al.
(2010) proposed an alternative strategy of popu-
larity rankings (Wu et al., 2010).
Ott et al. (2011) developed the first dataset con-
taining gold-standard deceptive reviews by crowd-
sourcing (Ott et al., 2011), and presented three su-
pervised learning methods to detect deceptive re-
views by integrating knowledge from psycholin-
guistics and computational linguistics. This gold-
standard dataset will be used in the paper. Li et al.
(2011) manually built a review dataset from their
crawled reviews (Li et al., 2011), and exploited
semi-supervised co-training algorithm to identify
deceptive reviews.
Feng et al. (2012) verified the connection be-
tween the deceptive reviews and the abnormal dis-
tributions (Feng et al., 2012a). Later, they (Feng et
al., 2012b) demonstrated that features driven from
CFG (Context Free Grammar) parsing trees con-
sistently improve the detection performance.
Mukherjee et al. (2012) proposed detect-
ing group spammers (a group of reviewers who
work collaboratively to write deceptive reviews) in
product reviews (Mukherjee et al., 2012). The pro-
posed method first used frequent itemset mining
to find a set of candidate groups. Then GSRank
was presented which can consider relationships a-
mong groups, individual reviewers and products
they reviewed to detect spammer groups. Later,
they also proposed exploiting observed reviewing
behaviors to detect opinion spammers in an unsu-
pervised Bayesian inference framework (Mukher-
jee et al., 2013).
Ren et al. (2014) assumed that there must be
some difference on language structure and sen-
timent polarity between deceptive reviews and
truthful ones (Ren et al., 2014a), then they de-
fined the features related to the review text and
used genetic algorithm for feature selection, fi-
nally they combined two unsupervised clustering
algorithm to identify deceptive reviews. Later,
they (Ren et al., 2014b) present a new approach,
from the viewpoint of correcting the mislabeled
</bodyText>
<page confidence="0.998682">
489
</page>
<bodyText confidence="0.999831181818182">
examples, to find deceptive reviews. Firstly, they
partition a dataset into several subsets.Then they
construct a classifier set for each subset and s-
elect the best one to evaluate the whole dataset.
Meanwhile, error variables are defined to compute
the probability that the examples have been mis-
labeled. Finally, the mislabeled examples are cor-
rected based on two threshold schemes, majority
and non-objection.
Unlike previous studies, PU learning is imple-
mented to identify deceptive reviews.
</bodyText>
<subsectionHeader confidence="0.999406">
2.2 Positive Unlabeled Learning
</subsectionHeader>
<bodyText confidence="0.999940172413793">
According to the use of the unlabeled data, PU
learning can be divided into two classes.
One family of methods built the final classifier
by using positive examples dataset and some ex-
amples of the unlabeled dataset (Liu et al., 2002;
Liu et al., 2003). The basic idea is to find a set
of reliable negative examples from the unlabeled
data firstly, and then to learn a classifier using EM
(Expectation Maximization) or SVM. The perfor-
mance is limited for neglecting the rest examples
of unlabeled dataset.
Another family of methods learned the final
classifier by using positive examples dataset and
all examples of the unlabeled dataset. Li et al.
(Li et al., 2009) studied PU learning in the data
stream environment, they proposed a PU learn-
ing LELC (PU Learning by Extracting Likely
positive and negative micro-Clusters) for docu-
ment classification, they assume that the exam-
ples close together shared the same labels. Xi-
ao et al. (Xiao et al., 2011) proposed a method,
called SPUL (similarity-based PU learning), the
local similarity-based and global similarity-based
mechanisms are proposed to generate the similar-
ity weights for the easily mislabeled examples,
respectively. Experimental results show global
SPUL generally performs better than local SPUL.
In this paper, a novel PU learning (MPIPUL) is
proposed to identify deceptive reviews.
</bodyText>
<sectionHeader confidence="0.996764" genericHeader="method">
3 Preliminary
</sectionHeader>
<bodyText confidence="0.959455333333333">
Before we introduce the proposed method, we
briefly review SVM, which has proven to be an
effective classification algorithm (Vapnik, 1998).
Let T = {(x(1), y(1)), (x(2), y(2)), ... , (x(|T|), y(|T|
)} be a training set, where x(i) ∈ Rd and
y(i) ∈ {+1, −1}. SVM aims to seek an optimal
separating hyperplane wT x(i) + b = 0, the hyper-
plane can be obtained by solving the following
optimization problem:
</bodyText>
<equation confidence="0.999769">
|T|
min F(w,b,Ei) = 2||w||2 + C�
i=1
s.t. y(i)(wTx(i) + b) ≥ 1 − Ei, i = 1, ... , |T|
Ei ≥ 0,i = 1,...,|T|
(1)
</equation>
<bodyText confidence="0.999917363636364">
where wT represents the transpose of w, C is a
parameter to balance the classification errors and
Ei are variables to relax the margin constraints.
The optimal classifier can be achieved by using
the Lagrange function. For a test example x, if
wT x+b &lt; 0, it is classified into the negative class;
otherwise, it is positive.
In the following, SVM is extended to incorpo-
rate the spy examples and their weights, such that
the spy examples can contribute differently to the
classifier construction.
</bodyText>
<sectionHeader confidence="0.989602" genericHeader="method">
4 The Proposed Method
</sectionHeader>
<bodyText confidence="0.99847625">
In this section, we will introduce the proposed ap-
proach in details. In our PU learning (MPIPUL),
truthful reviews are named positive examples, and
deceptive reviews are called negative examples. P
is defined as a set which contains all positive ex-
amples. U is a set for all unlabeled examples. PU
learning aims at building a classifier using P and
U. MPIPUL adopts the following four steps:
</bodyText>
<listItem confidence="0.999880571428571">
• Step 1: Extract the reliable negative exam-
ples;
• Step 2: Compute the representative positive
and negative examples;
• Step 3: Generate the similarity weights for
the spy examples;
• Step 4: Build the final SVM classifier;
</listItem>
<subsectionHeader confidence="0.980107">
4.1 Extracting Reliable Negative Examples
</subsectionHeader>
<bodyText confidence="0.9988255">
Considering only positive and unlabeled examples
are available in PU learning, some negative ex-
amples need to be extracted firstly. These exam-
ples will influence the performance of the follow-
ing three steps. So high-quality negative examples
must be guaranteed. Previous works solved the
)problem with the Spy technique (Liu et al., 2002)
or the Rocchio technique (Liu et al., 2003), we in-
tegrate them in order to get reliable negative ex-
amples. Let subsets N51 and N52 contain the
</bodyText>
<equation confidence="0.437172">
Ei
</equation>
<page confidence="0.938415">
490
</page>
<bodyText confidence="0.999881545454545">
corresponding reliable negative examples extract-
ed by the two techniques, respectively. Examples
are considered to be a reliable negative only if both
techniques agree that they are negative. That is,
NS = NS1 n NS2, where NS contains the reli-
able negative examples.
After reliable negative examples are extracted,
there are still some unlabeled examples (we call
spy examples) in set U, let subset US = U − NS,
which stores all the spy examples. It is crucial to
determine how to deal with these spy examples.
</bodyText>
<subsectionHeader confidence="0.998712">
4.2 Computing Representative Positive and
Negative Examples
</subsectionHeader>
<bodyText confidence="0.999968419354839">
Generally, a classifier can be constructed to pre-
dict deceptive reviews based on the positive ex-
amples set P and the reliable negative examples
set NS. But the classifier is not accurate enough
for lacking of making full use of unlabeled dataset
U. In order to utilize spy examples in subset US,
some representative positive and negative exam-
ples are calculated firstly. Since the examples have
different styles in sentiment polarity and topic dis-
tribution, for every class, computing one repre-
sentative example is not suitable. For the posi-
tive class or the negative class, to ensure there is
a big difference between the different representa-
tive examples. This paper proposes clustering re-
liable negative examples into several groups based
on LDA (Latent Dirichlet Allocation) topic mod-
el and K-means, and then multiple representative
examples can be obtained.
LDA topic model is known as a parametric
Bayesian clustering model (Blei et al., 2003), and
assumes that each document can be represented
as the distribution of several topics, each docu-
ment is associated with common topics. LDA can
well capture the relationship between internal doc-
uments.
In our experiments based on LDA model, we
can get the topic distribution for the reliable neg-
ative examples, then some reliable negative exam-
ples which are similar in topic distribution will be
clustered into a group by K-means. Finally, these
reliable negative examples can be clustered into n
</bodyText>
<equation confidence="0.751165">
micro-clusters (NS1, NS2, ... , NSn). Here,
n = 30 * |NS|/(|US |+ |NS|) (2)
</equation>
<bodyText confidence="0.99995625">
Here, according to the suggestion of previous
work (Xiao et al., 2011), we examine the impact
of the different parameter (from 10 to 60) on over-
all performance, and select the best value 30.
Based on the modified Rocchio formula (Buck-
ley et al., 1999), n representative positive exam-
ples (pk) and n negative ones (nk) can be obtained
using the following formula:
</bodyText>
<equation confidence="0.998786857142857">
1 |NSk |x(i)
 |NSk |� II x(i) II
1 |P |x(i)
II x(i) II − |P |-1- II x(i) II
i=1
k = 1,...,n
(3)
</equation>
<bodyText confidence="0.999851666666667">
According to previous works (Buckley et al.,
1994), where the value of α and β are set to 16
and 4 respectively. The research from Buckley et
al. demonstrate that this combination emphasizes
occurrences in the relevant documents as opposed
to non-relevant documents.
</bodyText>
<subsectionHeader confidence="0.999117">
4.3 Generating Similarity Weights
</subsectionHeader>
<bodyText confidence="0.999567666666667">
For a spy example x, since we do not know which
class it should belong to, enforcing x to the posi-
tive class or the negative class will lead to some
mislabeled examples, which disturbs the perfor-
mance of final classifier. We represent a spy ex-
ample x using the following probability model:
</bodyText>
<equation confidence="0.894409">
{x, (p+(x), p−(x))}, p+(x) + p−(x) = 1 (4)
</equation>
<bodyText confidence="0.999987733333333">
Where p+(x) and p−(x) are similarity weight-
s which represent the probability of x belonging
to the positive class and the negative class, re-
spectively. For example, {x, (1, 0)} means that x
is positive, while {x, (0, 1)} indicates that x is i-
dentified to be negative. For {x, (p+(x), p−(x))},
where 0 &lt; p+(x) &lt; 1 and 0 &lt; p−(x) &lt; 1, it
implies that the probability of x belonging to the
positive class and the negative class are both con-
sidered.
In this section, similarity weights are decided by
mixing global information (population property)
and local information (individual property). Then
all spy examples and their similarity weights are
incorporated into a SVM-based learning model.
</bodyText>
<subsubsectionHeader confidence="0.678339">
4.3.1 Population Property
</subsubsectionHeader>
<bodyText confidence="0.999753857142857">
Population property means that the examples in
each micro-cluster share the similarity in sen-
timent polarity and topic distribution, and they
belong to the same category with a high pos-
sibility. In our framework, in order to com-
pare with the representative examples, all spy ex-
amples are firstly clustered into n micro-clusters
</bodyText>
<figure confidence="0.86429525">
1
x(i)
|P|
pk = α
II x(i) II
� |P|
i=1
nk = α 1 |NSk|
�
i=1
|NSk|
x(i)
</figure>
<page confidence="0.9932">
491
</page>
<bodyText confidence="0.99942375">
(US1, US2, ... , USn) based on LDA and K-
means. Then, for every spy example x in one
micro-cluster USi, we tags with temporary label
by finding its most similar representative example.
Finally, we can get the similarity weights for a spy
example x in micro-cluster USi, their probability
pertaining to the positive class and negative class
can be represented by the following formula:
</bodyText>
<equation confidence="0.996888666666667">
p pop(x) = |positi|e|
|USi
n pop(x) = |USi|
</equation>
<bodyText confidence="0.999839">
where |USi |represents the number of all examples
in micro-cluster USi, |positive |means the num-
ber of the examples which is called temporary pos-
itive in USi, and |negative |means the number of
the examples which is called temporary negative
in USi.
For example, Figure 1 shows the part (C1, C2,
C3, C4) of the clustering results for the spy exam-
ples based on LDA and K-means, the examples
x in C4 are assigned with weights p pop(x) =
4, n pop(x) = 5�, the examples x in C1 are as-
signed with weights p pop(x) = 1, n pop(x) = 0.
</bodyText>
<figureCaption confidence="0.999708">
Figure 1: Illustration of population property
</figureCaption>
<bodyText confidence="0.999889142857143">
The advantage of population property lies in the
fact that it considers the similar relationship be-
tween the examples, from which the same micro-
cluster are assigned the same similarity weight.
However, it cannot distinguish the difference of
examples in one micro-cluster. In fact, the simi-
larity weights of examples from the same micro-
cluster can be different, since they are located
physically different. For example, for the spy ex-
ample y and z in micro-cluster C4, it is apparent-
ly unreasonable that we assign the same similarity
weights to them. So we should join the local in-
formation (individual property) when we are com-
puting the similarity weights for a spy example.
</bodyText>
<subsubsectionHeader confidence="0.763423">
4.3.2 Individual Property
</subsubsectionHeader>
<bodyText confidence="0.999896625">
Individual property is taken into account to mea-
sure the relationship between every spy example
and all representative ones. Specifically, for ex-
ample x, we firstly compute its similarity to each
of the representative examples, and then the prob-
ability of the example x belonging to the positive
class and negative class can be calculated using the
following formula:
</bodyText>
<equation confidence="0.964691">
�n k=1 sim(x, pk)
p ind(x) = �n k=1(sim(x,pk) + sim(x,nk))
En
n ind(x) = Enk=1( iM( x1,p ) (+ si )
sim (x, nk))
(6)
In the above formula,
p−(x) = λ · n pop(x) + (1 − λ) · n ind(x) (7)
</equation>
<bodyText confidence="0.945137333333333">
Where λ is a parameter to balance the informa-
tion from population property and individual prop-
erty. In the remaining section, we will examine
the impact of the parameter λ on overall perfor-
mance. Meanwhile, it can be easily proved that
p+(x) + p−(x) = 1.
</bodyText>
<subsectionHeader confidence="0.998777">
4.4 Constructing SVM Classifier
</subsectionHeader>
<bodyText confidence="0.999891285714286">
After performing the third step, each spy example
x is assigned two similarity weights: p+(x) and
p−(x). In this section, we will extend the formu-
lation of SVM by incorporating the examples in
positive set P, reliable negative set NS, spy ex-
amples set US and their similarity weights into a
SVM-based learning model.
</bodyText>
<subsubsectionHeader confidence="0.603052">
4.4.1 Primal Problem
</subsubsectionHeader>
<bodyText confidence="0.9987365">
Since the similarity weights p+(x) and p−(x) in-
dicate the probability for a spy example x belong-
ing to the positive class and the negative class, re-
spectively. The optimization formula (1) can be
</bodyText>
<equation confidence="0.9700598">
(5)
|negative|
x · y
sim(x, y) =
||x ||· ||y||
</equation>
<subsubsectionHeader confidence="0.433359">
4.3.3 Similarity Weights
</subsubsectionHeader>
<bodyText confidence="0.9919302">
A scheme mixing population and individual prop-
erty is designed to generate the similarity weights
of spy examples. Specifically, for spy example x,
their similarity weights can be obtained by the fol-
lowing formula:
</bodyText>
<equation confidence="0.9934">
p+(x) = λ · p pop(x) + (1 − λ) · p ind(x)
</equation>
<page confidence="0.971336">
492
</page>
<bodyText confidence="0.997266">
rewritten as the following optimization problem: process):
</bodyText>
<equation confidence="0.998906">
s.t. y(i)(wT x(i) + b) &gt; 1 − Ei, x(i) E P
y(j)(wTx(j) + b) &gt; 1 − Ej, x(j) E US
y(m)(wTx(m) + b) &gt; 1 − Em, x(m) E US
y(n)(wTx(n) + b) &gt; 1 − En, x(n) E NS
Ei &gt; 0, Ej &gt; 0, Em &gt; 0, En &gt; 0
(8)
</equation>
<bodyText confidence="0.9994325">
Where C1, C2, C3 and C4 are penalty factors con-
trolling the tradeoff between the hyperplane mar-
gin and the errors, Ei, Ej, Em and En are the error
terms. p+(x(j))Ej and p−(x(m))Em can be consid-
ered as errors with different weights. Note that,
a bigger value of p+(x(j)) can increase the effect
of parameter Ej, so that the corresponding example
x(j) becomes more significant towards the positive
class. In the following, we will find the dual form
to address the above optimization problem.
</bodyText>
<subsubsectionHeader confidence="0.536583">
4.4.2 Dual Problem
</subsubsectionHeader>
<bodyText confidence="0.999761333333333">
Assume αi and αj are Lagrange multipliers. To
simplify the presentation, we redefine some nota-
tions as follows:
</bodyText>
<equation confidence="0.9897676">
{ C+i C1, x(i) E P
=
C2p+(x(j)), x(j) E US
{ _ C3p (x(m)), x(m) E US
Cj C4, x(n) E NS
</equation>
<bodyText confidence="0.999784263157895">
Based on the above definitions, we let T+ =
P U US, T− = US U NS and T∗ = T+ U T−.
The Wolfe dual of primal formulation can be ob-
tained as follows (Appendix A for the calculation
where &lt; x(i), x(j) &gt; is the inner product of x(i)
and x(j). In order to get the better performance, we
can replace them by using kernel function O(x(i))
and O(x(j)), respectively. The kernel track can
convert the input space into a high-dimension fea-
ture space. It can solve the uneven distribution of
dataset and complex problem from heterogeneous
data sources, which allows data to get a better ex-
pression in the new space (Lanckriet et al., 2004;
Lee et al., 2007).
After solving the above problem, w can be ob-
tained, then b can also be obtained by using KKT
(Karush-Kuhn-Tucker) conditions. For a test ex-
ample x, if wT x + b &gt; 0, it belongs to the positive
class. Otherwise, it is negative.
</bodyText>
<sectionHeader confidence="0.999399" genericHeader="method">
5 Experiments
</sectionHeader>
<bodyText confidence="0.999955571428571">
We aim to evaluate whether our proposed PU
learning can identify deceptive reviews properly.
We firstly describe the gold-standard dataset, and
then introduce the way to generate the positive
examples P and unlabeled examples U. Finally
we present human performance in gold-standard
dataset.
</bodyText>
<subsectionHeader confidence="0.960883">
5.1 Datasets
</subsectionHeader>
<bodyText confidence="0.999996166666667">
There is very little progress in detection of de-
ceptive reviews, one reason is the lack of stan-
dard dataset for algorithm evaluation. The gold-
standard dataset is created based on crowdsourc-
ing platform (Ott et al., 2011), which is also adopt-
ed as the experimental dataset in this paper.
</bodyText>
<subsubsectionHeader confidence="0.644861">
5.1.1 Deceptive Reviews
</subsubsectionHeader>
<bodyText confidence="0.99989425">
Crowdsourcing services can carry out massive da-
ta collection and annotation; it defines the task in
the network platform, and paid for online anony-
mous workers to complete the task.
</bodyText>
<equation confidence="0.999404068965517">
∑ |T*|
i=1
αi −
|T *|
1 ∑
2
i=1,j=1
αiαjy(i)·
max W (α) =
y(j) &lt; x(i),x(j) &gt;
s.t. C+i &gt; αi &gt; 0, x(i) E T+
C−j &gt; αj &gt; 0, x(j) E T−
|T+|
αj = 0
(9)
∑
i=1
|T ∑ |
j=1
αi −
|US |∑ p+(x(j))Ej + C3 |US |∑ p−(x(m))Em
j=1 m=1
+C4 |NS |En
∑
n=1
|P|
min F (w, b, E) = 2 ||w   ||2 + C1 ∑
i=1
Ei + C2·
</equation>
<page confidence="0.995083">
493
</page>
<bodyText confidence="0.9998948">
Humans cannot be precisely distinguish decep-
tive ones from existing reviews, but they can create
deceptive reviews as one part of the dataset. Ott et
al. (2011) accomplish this work by AMT (Ama-
zon Mechanical Turk). They set 400 tasks for 20
hotels, in which each hotel gets 20 tasks. Specif-
ic task is: If you are a hotel market department
employee, for each positive review you wrote for
the benefit for hotel development, you may get one
dollar. They collect 400 deceptive reviews.
</bodyText>
<subsubsectionHeader confidence="0.875893">
5.1.2 Truthful Reviews
</subsubsectionHeader>
<bodyText confidence="0.99949425">
For the collection of truthful reviews, they get
6977 reviews from TripAdvisor1 based on the
same 20 Chicago hotels, and remove some reviews
on the basis of the following constraints:
</bodyText>
<listItem confidence="0.998996666666667">
• Delete all non-five star reviews;
• Delete all non-English reviews;
• Delete all reviews which are less than 75
characters;
• Delete all reviews written by first-time au-
thors;
</listItem>
<bodyText confidence="0.996901833333333">
2124 reviews are gathered after filtering. 400 of
them are chosen as truthful ones for balancing the
number of deceptive reviews, as well as maintain-
ing consistent with the distribution of the length of
deceptive reviews. 800 reviews constitute whole
gold-standard dataset at last.
</bodyText>
<subsectionHeader confidence="0.998091">
5.2 Experiment Setup
</subsectionHeader>
<bodyText confidence="0.999948944444444">
We conduct 10-fold cross-validation: the dataset
is randomly split into ten folds, where nine fold-
s are selected for training and the tenth fold for
test. In training dataset, it contains 360 truthful
reviews and 360 deceptive ones. This paper is in-
tended to apply PU learning to identify deceptive
reviews. We specially make the following setting:
take 20% of the truthful reviews in training set as
positive examples dataset P, all remaining truthful
and deceptive reviews in training set as the unla-
beled dataset U. Therefore, during one round of
the algorithm, the training set contains 720 exam-
ples including 72 positive examples (set P) and
648 unlabeled examples (set U), and the test set
contains 80 examples including 40 positive and 40
negative ones. In order to verify the stability of
the proposed method, we also experiment anoth-
er two different settings, which account for 30%
</bodyText>
<footnote confidence="0.978773">
1http://www.tripadvisor.com
</footnote>
<bodyText confidence="0.9931175">
and 40% of the truthful reviews in training set as
positive examples dataset P respectively.
</bodyText>
<subsectionHeader confidence="0.999297">
5.3 Human Performance
</subsectionHeader>
<bodyText confidence="0.9999786">
Human performance reflects the degree of difficul-
ty to address this task. The rationality of PU learn-
ing is closely related to human performance.
We solicit the help of three volunteer students,
who were asked to make judgments on test sub-
set (corresponding to the tenth fold of our cross-
validation experiments, contains 40 deceptive re-
views and 40 truthful reviews). Additionally, to
test the extent to which the individual human
judges are biased, we evaluate the performance of
two virtual meta-judges: one is the MAJORITY
meta-judge when at last two out of three human
judge believe the review to be deceptive, and the
other is the SKEPTIC when any human judge be-
lieves the review to be deceptive. It is apparent
from the results that human judges are not par-
ticularly effective at this task (Table 1). Inter-
annotator agreement among the three judges, com-
puted using Fleiss’ kappa, is 0.09. Landis and
Koch (Landis and Koch, 1977) suggest that s-
cores in the range (0.00, 0.20) correspond to “s-
light agreemen” between annotators. The largest
pairwise Cohen’s kappa is 0.11 between JUDGE-
1 and JUDGE-3, far below generally accepted
pairwise agreement levels. We can infer that the
dataset which are annotated by people will include
a lot of mislabeled examples. Identifying decep-
tive reviews by simply using supervised learning
methods is not appropriate. So we propose ad-
dressing this issue by using PU learning.
</bodyText>
<tableCaption confidence="0.999534">
Table 1: Human performance
</tableCaption>
<table confidence="0.998941666666667">
Methods Accuracy (%)
JUDGE-1 57.9
Human JUDGE-2 55.4
JUDGE-3 61.7
META MAJORITY 58.3
SKEPTIC 62.4
</table>
<sectionHeader confidence="0.998206" genericHeader="evaluation">
6 Results and Analysis
</sectionHeader>
<bodyText confidence="0.999872">
In order to verify the effectiveness of our proposed
method, we perform two PU learning (LELC and
SPUL) in the gold-standard dataset.
</bodyText>
<page confidence="0.997327">
494
</page>
<subsectionHeader confidence="0.993478">
6.1 Experimental Results
</subsectionHeader>
<bodyText confidence="0.993205785714286">
Table 2 shows that the experimental results com-
pared with different PU learning techniques. In
Table 2, P(20%) means that we randomly select
20 percentages of truthful reviews to form the pos-
itive examples subset P. In our MPIPUL frame-
work, we set A = 0.3. We can see that our pro-
posed method can obtain 83.91%, 85.43% and
86.69% in accuracy from different experimental
settings, respectively. Compared to the curren-
t best method (SPUL-global), the accuracy can be
improved 2.06% on average. MPUPUL can im-
prove 3.21% on average than LELC. The above
discussion shows our proposed methods consis-
tently outperform the other PU baselines.
</bodyText>
<tableCaption confidence="0.998227">
Table 2: Accuracy on the different PU learning
</tableCaption>
<table confidence="0.9992424">
Baselines P(20%) P(30%) P(40%)
LELC 81.12 82.08 83.21
SPUL-local 81.43 82.71 84.09
SPUL-global 81.89 83.24 84.73
MPIPUL (0.3) 83.91 85.43 86.69
</table>
<bodyText confidence="0.9980516">
PU learning framework in this paper can obtain
the better performance. Two factors contribute to
the improved performance. Firstly, LDA can cap-
ture the deeper information of the reviews in topic
distribution. Secondly, strategies of mixing pop-
ulation and individual property can generate the
similarity weights for spy examples, and these ex-
amples and their similarity weights are extended
into SVM, which can build a more accurate clas-
sifier.
</bodyText>
<subsectionHeader confidence="0.999719">
6.2 Parameter Sensitivity
</subsectionHeader>
<bodyText confidence="0.997027692307692">
For the spy examples, the similarity weights are
generated by population property and individual
property. Should we select the more population
information or individual information? In MPIP-
UL, parameter A is utilized to adjust this process.
So we experiment with the different value of the
parameter A on MPUPUL performance (Figure 2).
As showed in Figure 2, for P(20%), if A &lt; 0.3,
the performance increases linearly, if A &gt; 0.3,
the performance will decrease linearly. Mean-
while, we can get the same trends for P(30%) and
P(40%). Based on the above discussion, MPIP-
UL can get the best performance when A ≈ 0.3.
</bodyText>
<figureCaption confidence="0.886161">
Figure 2: Algorithm performance on different pa-
rameter
</figureCaption>
<sectionHeader confidence="0.978663" genericHeader="conclusions">
7 Conclusions and Future Work
</sectionHeader>
<bodyText confidence="0.999974166666667">
This paper proposes a novel PU learning (MPIP-
UL) technique to identify deceptive reviews based
on LDA and SVM. Firstly, the spy examples are
assigned similarity weights by integrating the in-
formation from the population property and in-
dividual property. Then the spy examples and
their similarity weights are incorporated into SVM
learning phase to build an accurate classifier. Ex-
perimental results on gold-standard dataset show
the effectiveness of our method.
In future work, we will discuss the application
of our proposed method in the massive dataset.
</bodyText>
<sectionHeader confidence="0.985044" genericHeader="acknowledgments">
Acknowledgments
</sectionHeader>
<bodyText confidence="0.999452888888889">
We are grateful to the anonymous reviewer-
s for their thoughtful comments. This work
is supported by the State Key Program of
National Natural Science Foundation of China
(Grant No.61133012), the National Natural Sci-
ence Foundation of China (Grant No.61173062,
61373108) and the National Philosophy Social
Science Major Bidding Project of China (Grant
No. 11&amp;ZD189).
</bodyText>
<sectionHeader confidence="0.998469" genericHeader="references">
References
</sectionHeader>
<reference confidence="0.999439888888889">
Alexandros Ntoulas, Marc Najork, Mark Manasse, and
Dennis Fetterly. 2006. Detecting spam web pages
through content analysis. In Proceedings of the 15th
International Conference on World Wide Web, page
83-92, Edinburgh, Scotland.
Arjun Mukherjee, Abhinav Kumar, Bing Liu, Junhui
Wang, Meichun Hsu, Malu Castellanos, and Riddhi-
man Ghosh. 2013. Spotting opinion spammers us-
ing behavioral footprints. In Proceeding of the 19th
</reference>
<page confidence="0.997519">
495
</page>
<note confidence="0.619322875">
ACM SIGKDD International Conference on Knowl-
edge Discovery and Data Ming, page 632-640, Ly-
on, France.
Arjun Mukherjee, Bing Liu, and Natalie Glance. 2012.
Spotting fake reviewer groups in consumer reviews.
In Proceeding of the 21st International Conference
on World Wide Web, page 191-200, New York, US-
A.
</note>
<reference confidence="0.999390474747474">
Bing Liu. 2012. Sentiment analysis and opinion min-
ing. Morgan &amp; Claypool Publishers. San Rafael,
USA.
Bing Liu, Wee Sun Lee, Philip S. Yu, and Xiaoli Li.
2002. Partially supervised classification of text doc-
uments. In Proceedings of the 19th International
Conference on Machine Learning, page 387-394,
San Francisco, USA.
Bing Liu, Yang Dai, Xiaoli Li, Wee Sun Lee, and Philip
S. Yu. 2003. Building text classifiers using positive
and unlabeled examples. In Proceedings of the 3rd
IEEE International Conference on Data Ming, page
179-182, Washington, USA.
Bo Pang and Lillian Lee. 2008. Opinion mining and
sentiment analysis. Foundations and Trends in In-
formation Retrieval, 2(1-2):1-135.
Charles Elkan and Keith Noto. 2008. Learning clas-
sifiers from only positive and unlabeled data. In
Proceedings of the 14th ACM SIGKDD Internation-
al Conference on Knowledge Discovery and Data
Ming, page 213-220, Las Vegas, USA.
Chirs Buckley, Bgrard Salton, and James Allan. 1994.
The effect of adding relevance information in a rele-
vance feedback environment. In Proceedings of the
17th Annual International SIGIR Conference on Re-
search and Development Retrieval, page 292-300,
Dublin, Ireland.
David M. Blei, Andrew Y. Ng, and Michael I. Jordan.
2003. Latent dirichlet allocation. Journal of Ma-
chine Learning Research, 3:993-1022.
Dell Zhang. 2005. A simple probabilistic approach
to learning from positive and unlabeled examples.
In Proceedings of the 5th Annual UK Workshop on
Computational Intelligence, page 83-87.
Fangtao Li, Minlie Huang, Yi Yang, and Xiaoyan
Zhu. 2011. Learning to identify review spam. In
Proceeding of the 22nd International Joint Confer-
ence on Artificial Intelligence, page 2488-2493,
Barcelona, Spain.
Fang Wu and Bernardo A. Huberman. 2010. Opinion
formation under costly express. ACM Transactions
on Intelligence System Technology, 1(5):1-13.
Gert R. G. Lanckeriet, Nello Cristianini, Peter Bartlet-
t, Laurent EI Ghaoui, and Michael I.Jordan. 2004.
Learning the kernel matrix with seim-difinit pro-
gramming. Journal of Machine Learning Research,
5:27-72.
Harris Drucker, Donghui Wu, and Vladimir N. Vap-
nik. 1999. Support vector machines for spam cate-
gorization. IEEE Transactions on Neural Networks,
10(5):1048-1054.
Kumar Ankita and Sminchisescu Cristian. 2006. Sup-
port kernel machines for object recognition. In Pro-
ceedings of the IEEE 11th International Conference
on Computer Vision, page 1-8, Rio de Janeiro, Brza-
il.
Myle Ott, Yelin Choi, Claire Caridie, and Jeffrey T.
Hancock. 2011. Finding deceptive opinion spam
by any stretch of the imagination. In Proceeding
of the 49th Annual Meeting of the Association for
Computational Linguistics: Human Language Tech-
noloies, page 309-319, Portland, USA.
Nitin Jindal and Bing Liu. 2008. Opinion spam and
analysis. In Proceeding of the 1st ACM Interna-
tional Conference on Web Search and Data Mining,
page 137-142, California, USA.
Richard Landis and Gary G. Koch. 1977. The mea-
surement of observer agreement for categorical data.
Biometrics, 33(1):159-174.
Song Feng, Longfei Xing, Anupam Gogar, and Yejin
Choi. 2012. Distributional footprints of deceptive
product reviews. In Proceeding of the 6th Inter-
national AAAI Conference on WebBlogs and Social
Media, page 98-105, Dublin, Ireland.
Song Feng, Ritwik Banerjee, and Yejin Choi. 2012.
Syntactic stylometry for deception detection. In
Proceeding of the 50th Annual Meeting of the As-
sociation for Computational Linguistics, page 171-
175, Jeju Island, Korea.
Vladimir N. Vapnik. 1998. Statistical learning theory.
Springer. New York, USA.
Wanjui Lee, Sergey Verzakov, and Robert P. Duin.
2007. Kernel combination versus classifier com-
bination. In Proceedings of the 7th International
Workshop on Multiple Classifier Systems, page 22-
31, Rrague, Czech Republic.
Xiaoli Li, Philip S. Yu, Bing Liu, and See Kiong Ng.
2009. Positive unlabeled learning for data stream
classification. In Proceedings of the SIAM Inter-
national Conference on Data Ming, page 257-268,
Nevada, USA.
Yafeng Ren, Donghong Ji, Lan Yin, and Hongbin
Zhang. 2014. Finding deceptive opinion spam by
correcting the mislabled instances. Chinese Journal
of Electronics, 23(4):702-707.
Yafeng Ren, Lan Yin, and Donghong Ji. 2014. De-
ceptive reviews detection based on language struc-
ture and sentiment polarity. Journal of Frontiers of
Computer Science and Technology, 8(3):313-320.
</reference>
<page confidence="0.989946">
496
</page>
<reference confidence="0.990822636363636">
Yanshan Xiao, Bing Liu, Jie Yin, Longbing Cao,
Chengqi Zhang, and Zhifeng Hao. 2011.
Similarity-based approach for positive and unla-
beled learning. In Proceeding of the 22nd Inter-
national Joint Conference on Artifical Intelligence,
page 1577-1582, Barcelona, Spain.
Zoltan Gyongyi, Hector Garcia-Molina, and Jan
Pedesen. 2004. Combating web spam web with
trustrank. In Proceedings of the 30th International
Conference on Very Large Data Bases, page 576-
587, Toronto, Canada.
</reference>
<sectionHeader confidence="0.950836" genericHeader="conclusions">
Appendix A
</sectionHeader>
<bodyText confidence="0.99365825">
The optimization problem is as follows:
minimize L(w, b, ϵ, α, γ) with respect to w and b,
we will do by setting the derivatives of L with re-
spect to w and b to zero, we have:
</bodyText>
<equation confidence="0.716186">
x(n) = 0
(12)
</equation>
<bodyText confidence="0.285649">
This implies that
</bodyText>
<figure confidence="0.84999415625">
∂L(w, b, ϵ, α, γ) = w − ∑|P |αiy(i)x(i) − |US |∑
∂w i=1 j=1
αjy(j)x(j) − |US |∑ αmy(m)x(m) − |NS |αny(n)·
m=1 ∑
n=1
∑ |P|
1
min F(w, b, ϵ) = �||w||2 + C1
|US |∑ p+(x(j))ϵj + C3
j=1
ϵi + C2·
|US |∑ αjy(j)x(j) +
j=1
|P|
w = ∑
i=1
αiy(i)x(i) +
y(m)x(m) +
αny(n)x(n)
(13)
|NS|
∑
n=1
i=1
|US |∑ p−(x(m))ϵm+
m=1
C4 |NS |ϵn
∑
n=1
αm·
|US |∑
m=1
</figure>
<equation confidence="0.850678333333333">
s.t. y(i)(wT x(i) + b) &gt; 1 − ϵi, x(i) E P
y(j)(wTx(j) + b) &gt; 1 − ϵj, x(j) E US
y(m)(wTx(m) + b) &gt; 1 − ϵm, x(m)EUS
</equation>
<bodyText confidence="0.9262885">
Here, to simplify the presentation, we redefine
some notations in the following:
</bodyText>
<equation confidence="0.95202875">
T+ = P U US,T− = US U NS,T* = T+ U T−
y(n)(wTx(n) + b) &gt; 1 − ϵn, x(n) E NS i C+= C1, x(i) E P
ϵi &gt; 0, ϵj &gt; 0, ϵm &gt; 0, ϵn &gt; 0 C2p+x(j), x(j) E US
(10)
</equation>
<bodyText confidence="0.994136">
We construct the Lagrangian function for the C−j = { C3p−x(m), x(m) E US
above optimization problem, we have: C4, x(n) ENS
</bodyText>
<equation confidence="0.915067">
L(w, b, ϵ, α, γ) = F(w, b, ϵ) +
(wTx(i) + b) + 1 − ϵi] +
so we obtain
w = ∑ |T* |αiy(i)x(i) (14)
i=1
</equation>
<bodyText confidence="0.938052">
As for the derivative with respect to b, we obtain
</bodyText>
<figure confidence="0.931702695652174">
∑ |P |αi[−y(i)·
i=1
|US |∑ αj[−y(j)(wT x(j)+
j=1
b) + 1 − ϵj] + |US |∑ αm[−y(m)(wT x(m) + b) + 1
m=1
−ϵm] + |NS |αn[−y(n)(wT x(n) + b) + 1 − ϵn]−
∑
n=1
∑ |P |γiϵi − |US |∑ γjϵj − |US |∑ γmϵm − |NS |γnϵn
i=1 j=1 m=1 ∑
n=1
|P |αiy(i) − |US |αjy(j)
∑ ∑
i=1 j=1
αmy(m) − |NS |αny(n) = 0
∑ (15)
n=1
∂L(w, b, ϵ, α, γ)
∂b
− |US |∑
m=1
(11) We get: ∑ |T* |αiy(i) = 0 (16)
</figure>
<bodyText confidence="0.9511345">
Here, the α and γ are Lagrange multipliers. To i=1
find the dual form of the problem, we need to first
</bodyText>
<page confidence="0.952558">
497
</page>
<bodyText confidence="0.99791">
If we take Equation (14) and (16) back into the
Lagrangian function (Equation 11), and simplify,
we get
</bodyText>
<equation confidence="0.976998">
L(w, b, E, α,γ) = � |T* |αi − 1 � |T* |y(i)y(j)αi·
i=1 2 i,j=1
αj &lt; x(i),x(j) &gt;
(17)
</equation>
<bodyText confidence="0.9997465">
To the primal optimization formula (10), we can
obtain the following dual optimization problem:
</bodyText>
<equation confidence="0.998860222222222">
max W (α) = � |T* |αi − |T * |αiαjy(i)·
i=1 1 �
2 i=1,j=1
y(j) &lt; x(i),x(j) &gt;
s.t. C+i &gt; αi &gt; 0, x(i) E T+
C−j &gt; αj &gt; 0, x(j) E T−
|T�+ |αi − |T � � |αj = 0
i=1 j=1
(18)
</equation>
<bodyText confidence="0.994645666666667">
where &lt; x(i), x(j) &gt; is the inner product of x(i)
and x(j), we can replace them by using kernel
function O(x(i)) and O(x(j)), respectively.
</bodyText>
<page confidence="0.99759">
498
</page>
</variant>
</algorithm>
<algorithm name="ParsHed" version="110505">
<variant no="0" confidence="0.753297">
<title confidence="0.999949">Positive Unlabeled Learning for Deceptive Reviews Detection</title>
<author confidence="0.993989">Yafeng Ren Donghong Ji Hongbin Zhang</author>
<affiliation confidence="0.883226">Computer Wuhan</affiliation>
<address confidence="0.974439">Wuhan 430072,</address>
<abstract confidence="0.99967864516129">Deceptive reviews detection has attracted significant attention from both business and research communities. However, due to the difficulty of human labeling needed for supervised learning, the problem remains to be highly challenging. This paper proposed a novel angle to the problem by modeling PU (positive unlabeled) learning. A semi-supervised model, called mixing population and individual property PU learning (MPIPUL), is proposed. Firstly, some reliable negative examples are identified from the unlabeled dataset. Secondly, some representative positive examples and negative examples are generated based on LDA (Latent Dirichlet Allocation). Thirdly, for the remaining unexamples (we call them examples), which can not be explicitly identified as positive and negative, two similarity weights are assigned, by which the of a belonging to the positive class and the negative class displayed. Finally, and their similarity weights are incorporated into SVM (Support Vector Machine) to build an accurate classifier. Experiments on gold-standard dataset demonstrate the effectiveness of MPIPUL which outperforms the state-of-the-art baselines.</abstract>
</variant>
</algorithm>
<algorithm name="ParsCit" version="110505">
<citationList>
<citation valid="true">
<authors>
<author>Alexandros Ntoulas</author>
<author>Marc Najork</author>
<author>Mark Manasse</author>
<author>Dennis Fetterly</author>
</authors>
<title>Detecting spam web pages through content analysis.</title>
<date>2006</date>
<booktitle>In Proceedings of the 15th International Conference on World Wide Web,</booktitle>
<pages>83--92</pages>
<location>Edinburgh, Scotland.</location>
<contexts>
<context position="5815" citStr="Ntoulas et al., 2006" startWordPosition="905" endWordPosition="908">tion and individual property PU learning (MPIPUL), by assigning similarity weights and incorporating weights into SVM learning phase. This paper makes the following contributions: • For the first time, PU learning is defined in the environment of identifying deceptive reviews. • A novel PU learning is proposed based on LDA and SVM. • Experimental results demonstrate that our proposed method outperforms the current baselines. 2 Related Work 2.1 Deceptive Reviews Detection Spam has historically been investigated in the contexts of e-mail (Drucker et al., 1999; Gyongyi et al., 2004) and the Web (Ntoulas et al., 2006). In recent years, researchers have started to look at deceptive reviews. Jindal and Liu (2008) found that opinion spam was widespread and different from e-mail and Web spam in essence (Jindal and Liu, 2008). They trained models using product review data, by defining features to distinguish duplicate opinion and non-duplicate based on the review text, reviewers and product information. Wu et al. (2010) proposed an alternative strategy of popularity rankings (Wu et al., 2010). Ott et al. (2011) developed the first dataset containing gold-standard deceptive reviews by crowdsourcing (Ott et al., </context>
</contexts>
<marker>Ntoulas, Najork, Manasse, Fetterly, 2006</marker>
<rawString>Alexandros Ntoulas, Marc Najork, Mark Manasse, and Dennis Fetterly. 2006. Detecting spam web pages through content analysis. In Proceedings of the 15th International Conference on World Wide Web, page 83-92, Edinburgh, Scotland.</rawString>
</citation>
<citation valid="true">
<authors>
<author>Arjun Mukherjee</author>
<author>Abhinav Kumar</author>
<author>Bing Liu</author>
<author>Junhui Wang</author>
<author>Meichun Hsu</author>
<author>Malu Castellanos</author>
<author>Riddhiman Ghosh</author>
</authors>
<title>Spotting opinion spammers using behavioral footprints.</title>
<date>2013</date>
<booktitle>In Proceeding of the 19th</booktitle>
<contexts>
<context position="7675" citStr="Mukherjee et al., 2013" startWordPosition="1192" endWordPosition="1196">stently improve the detection performance. Mukherjee et al. (2012) proposed detecting group spammers (a group of reviewers who work collaboratively to write deceptive reviews) in product reviews (Mukherjee et al., 2012). The proposed method first used frequent itemset mining to find a set of candidate groups. Then GSRank was presented which can consider relationships among groups, individual reviewers and products they reviewed to detect spammer groups. Later, they also proposed exploiting observed reviewing behaviors to detect opinion spammers in an unsupervised Bayesian inference framework (Mukherjee et al., 2013). Ren et al. (2014) assumed that there must be some difference on language structure and sentiment polarity between deceptive reviews and truthful ones (Ren et al., 2014a), then they defined the features related to the review text and used genetic algorithm for feature selection, finally they combined two unsupervised clustering algorithm to identify deceptive reviews. Later, they (Ren et al., 2014b) present a new approach, from the viewpoint of correcting the mislabeled 489 examples, to find deceptive reviews. Firstly, they partition a dataset into several subsets.Then they construct a classi</context>
</contexts>
<marker>Mukherjee, Kumar, Liu, Wang, Hsu, Castellanos, Ghosh, 2013</marker>
<rawString>Arjun Mukherjee, Abhinav Kumar, Bing Liu, Junhui Wang, Meichun Hsu, Malu Castellanos, and Riddhiman Ghosh. 2013. Spotting opinion spammers using behavioral footprints. In Proceeding of the 19th</rawString>
</citation>
<citation valid="true">
<authors>
<author>Bing Liu</author>
</authors>
<title>Sentiment analysis and opinion mining.</title>
<date>2012</date>
<publisher>Morgan &amp; Claypool Publishers.</publisher>
<location>San Rafael, USA.</location>
<contexts>
<context position="2093" citStr="Liu, 2012" startWordPosition="307" endWordPosition="308">s and interact with others, people frequently write reviews on e-commerce sites, forums and blogs to achieve these purposes. For NLP (Natural Language Processing), these user-generated contents are of great value in that they contain abundant information related to people’s opinions on certain topics. Currently, online reviews on products and services are used extensively by consumers and businesses to conduct decisive purchase, product design and marketing strategies. Hence, sentiment analysis and opinion mining based on product reviews have become a popular topic of NLP (Pang and Lee, 2008; Liu, 2012). However, since reviews information can guide people’s purchase behavior, positive reviews can result in huge economic benefits and fame for organizations or individuals. This leaves room for promoting the generation of review spams. Through observations and studies of the predecessors (Jindal and Liu, 2008; Ott et al., 2011), review spams are divided into the following two classes: • Deceptive Reviews: Those deliberately mislead readers by giving undeserving positive reviews to some target objects in order to promote the objects, or by giving unjust negative reviews to some target objects in</context>
</contexts>
<marker>Liu, 2012</marker>
<rawString>Bing Liu. 2012. Sentiment analysis and opinion mining. Morgan &amp; Claypool Publishers. San Rafael, USA.</rawString>
</citation>
<citation valid="true">
<authors>
<author>Bing Liu</author>
<author>Wee Sun Lee</author>
<author>Philip S Yu</author>
<author>Xiaoli Li</author>
</authors>
<title>Partially supervised classification of text documents.</title>
<date>2002</date>
<booktitle>In Proceedings of the 19th International Conference on Machine Learning,</booktitle>
<pages>387--394</pages>
<location>San Francisco, USA.</location>
<contexts>
<context position="4480" citStr="Liu et al., 2002" startWordPosition="688" endWordPosition="691">vised learning is regarded as unsuitable for this task. It is difficult to come by human labeling needed for supervised learning and evaluation, we cannot obtain the datasets containing deceptive reviews. However, we can get some truthful reviews with high confidence by heuristic rules and prior knowledge. Meanwhile, a lot of unlabeled reviews are available. The problem thus is this: based on some truthful reviews and a lot of unlabeled reviews, can we build an accurate classifier to identify deceptive reviews. PU (positive unlabeled) learning can be utilized to deal with the above situation (Liu et al., 2002; Liu et al., 2003). Different from traditional supervised learning, PU learning can still build an accurate classifier even without the negative training examples. Several PU learning techniques have been applied successfully in document classification with promising results (Zhang, 2005; Elkan and Noto, 2008; Li et al., 2009; Xiao et al., 2011), while they have yet to be applied in detecting deceptive reviews. Here, we will study how to design PU learning to detect deceptive reviews. An important challenge is how to deal with spy examples (easily mislabeled) of unlabeled reviews, which is no</context>
<context position="8916" citStr="Liu et al., 2002" startWordPosition="1389" endWordPosition="1392">and select the best one to evaluate the whole dataset. Meanwhile, error variables are defined to compute the probability that the examples have been mislabeled. Finally, the mislabeled examples are corrected based on two threshold schemes, majority and non-objection. Unlike previous studies, PU learning is implemented to identify deceptive reviews. 2.2 Positive Unlabeled Learning According to the use of the unlabeled data, PU learning can be divided into two classes. One family of methods built the final classifier by using positive examples dataset and some examples of the unlabeled dataset (Liu et al., 2002; Liu et al., 2003). The basic idea is to find a set of reliable negative examples from the unlabeled data firstly, and then to learn a classifier using EM (Expectation Maximization) or SVM. The performance is limited for neglecting the rest examples of unlabeled dataset. Another family of methods learned the final classifier by using positive examples dataset and all examples of the unlabeled dataset. Li et al. (Li et al., 2009) studied PU learning in the data stream environment, they proposed a PU learning LELC (PU Learning by Extracting Likely positive and negative micro-Clusters) for docum</context>
<context position="12075" citStr="Liu et al., 2002" startWordPosition="1917" endWordPosition="1920">four steps: • Step 1: Extract the reliable negative examples; • Step 2: Compute the representative positive and negative examples; • Step 3: Generate the similarity weights for the spy examples; • Step 4: Build the final SVM classifier; 4.1 Extracting Reliable Negative Examples Considering only positive and unlabeled examples are available in PU learning, some negative examples need to be extracted firstly. These examples will influence the performance of the following three steps. So high-quality negative examples must be guaranteed. Previous works solved the )problem with the Spy technique (Liu et al., 2002) or the Rocchio technique (Liu et al., 2003), we integrate them in order to get reliable negative examples. Let subsets N51 and N52 contain the Ei 490 corresponding reliable negative examples extracted by the two techniques, respectively. Examples are considered to be a reliable negative only if both techniques agree that they are negative. That is, NS = NS1 n NS2, where NS contains the reliable negative examples. After reliable negative examples are extracted, there are still some unlabeled examples (we call spy examples) in set U, let subset US = U − NS, which stores all the spy examples. It</context>
</contexts>
<marker>Liu, Lee, Yu, Li, 2002</marker>
<rawString>Bing Liu, Wee Sun Lee, Philip S. Yu, and Xiaoli Li. 2002. Partially supervised classification of text documents. In Proceedings of the 19th International Conference on Machine Learning, page 387-394, San Francisco, USA.</rawString>
</citation>
<citation valid="true">
<authors>
<author>Bing Liu</author>
<author>Yang Dai</author>
<author>Xiaoli Li</author>
<author>Wee Sun Lee</author>
<author>Philip S Yu</author>
</authors>
<title>Building text classifiers using positive and unlabeled examples.</title>
<date>2003</date>
<booktitle>In Proceedings of the 3rd IEEE International Conference on Data Ming,</booktitle>
<pages>179--182</pages>
<location>Washington, USA.</location>
<contexts>
<context position="4499" citStr="Liu et al., 2003" startWordPosition="692" endWordPosition="695">regarded as unsuitable for this task. It is difficult to come by human labeling needed for supervised learning and evaluation, we cannot obtain the datasets containing deceptive reviews. However, we can get some truthful reviews with high confidence by heuristic rules and prior knowledge. Meanwhile, a lot of unlabeled reviews are available. The problem thus is this: based on some truthful reviews and a lot of unlabeled reviews, can we build an accurate classifier to identify deceptive reviews. PU (positive unlabeled) learning can be utilized to deal with the above situation (Liu et al., 2002; Liu et al., 2003). Different from traditional supervised learning, PU learning can still build an accurate classifier even without the negative training examples. Several PU learning techniques have been applied successfully in document classification with promising results (Zhang, 2005; Elkan and Noto, 2008; Li et al., 2009; Xiao et al., 2011), while they have yet to be applied in detecting deceptive reviews. Here, we will study how to design PU learning to detect deceptive reviews. An important challenge is how to deal with spy examples (easily mislabeled) of unlabeled reviews, which is not easily handled by</context>
<context position="8935" citStr="Liu et al., 2003" startWordPosition="1393" endWordPosition="1396">t one to evaluate the whole dataset. Meanwhile, error variables are defined to compute the probability that the examples have been mislabeled. Finally, the mislabeled examples are corrected based on two threshold schemes, majority and non-objection. Unlike previous studies, PU learning is implemented to identify deceptive reviews. 2.2 Positive Unlabeled Learning According to the use of the unlabeled data, PU learning can be divided into two classes. One family of methods built the final classifier by using positive examples dataset and some examples of the unlabeled dataset (Liu et al., 2002; Liu et al., 2003). The basic idea is to find a set of reliable negative examples from the unlabeled data firstly, and then to learn a classifier using EM (Expectation Maximization) or SVM. The performance is limited for neglecting the rest examples of unlabeled dataset. Another family of methods learned the final classifier by using positive examples dataset and all examples of the unlabeled dataset. Li et al. (Li et al., 2009) studied PU learning in the data stream environment, they proposed a PU learning LELC (PU Learning by Extracting Likely positive and negative micro-Clusters) for document classification,</context>
<context position="12119" citStr="Liu et al., 2003" startWordPosition="1925" endWordPosition="1928">egative examples; • Step 2: Compute the representative positive and negative examples; • Step 3: Generate the similarity weights for the spy examples; • Step 4: Build the final SVM classifier; 4.1 Extracting Reliable Negative Examples Considering only positive and unlabeled examples are available in PU learning, some negative examples need to be extracted firstly. These examples will influence the performance of the following three steps. So high-quality negative examples must be guaranteed. Previous works solved the )problem with the Spy technique (Liu et al., 2002) or the Rocchio technique (Liu et al., 2003), we integrate them in order to get reliable negative examples. Let subsets N51 and N52 contain the Ei 490 corresponding reliable negative examples extracted by the two techniques, respectively. Examples are considered to be a reliable negative only if both techniques agree that they are negative. That is, NS = NS1 n NS2, where NS contains the reliable negative examples. After reliable negative examples are extracted, there are still some unlabeled examples (we call spy examples) in set U, let subset US = U − NS, which stores all the spy examples. It is crucial to determine how to deal with th</context>
</contexts>
<marker>Liu, Dai, Li, Lee, Yu, 2003</marker>
<rawString>Bing Liu, Yang Dai, Xiaoli Li, Wee Sun Lee, and Philip S. Yu. 2003. Building text classifiers using positive and unlabeled examples. In Proceedings of the 3rd IEEE International Conference on Data Ming, page 179-182, Washington, USA.</rawString>
</citation>
<citation valid="true">
<authors>
<author>Bo Pang</author>
<author>Lillian Lee</author>
</authors>
<title>Opinion mining and sentiment analysis.</title>
<date>2008</date>
<booktitle>Foundations and Trends in Information Retrieval,</booktitle>
<pages>2--1</pages>
<contexts>
<context position="2081" citStr="Pang and Lee, 2008" startWordPosition="303" endWordPosition="306">le express themselves and interact with others, people frequently write reviews on e-commerce sites, forums and blogs to achieve these purposes. For NLP (Natural Language Processing), these user-generated contents are of great value in that they contain abundant information related to people’s opinions on certain topics. Currently, online reviews on products and services are used extensively by consumers and businesses to conduct decisive purchase, product design and marketing strategies. Hence, sentiment analysis and opinion mining based on product reviews have become a popular topic of NLP (Pang and Lee, 2008; Liu, 2012). However, since reviews information can guide people’s purchase behavior, positive reviews can result in huge economic benefits and fame for organizations or individuals. This leaves room for promoting the generation of review spams. Through observations and studies of the predecessors (Jindal and Liu, 2008; Ott et al., 2011), review spams are divided into the following two classes: • Deceptive Reviews: Those deliberately mislead readers by giving undeserving positive reviews to some target objects in order to promote the objects, or by giving unjust negative reviews to some targe</context>
</contexts>
<marker>Pang, Lee, 2008</marker>
<rawString>Bo Pang and Lillian Lee. 2008. Opinion mining and sentiment analysis. Foundations and Trends in Information Retrieval, 2(1-2):1-135.</rawString>
</citation>
<citation valid="true">
<authors>
<author>Charles Elkan</author>
<author>Keith Noto</author>
</authors>
<title>Learning classifiers from only positive and unlabeled data.</title>
<date>2008</date>
<booktitle>In Proceedings of the 14th ACM SIGKDD International Conference on Knowledge Discovery and Data Ming,</booktitle>
<pages>213--220</pages>
<location>Las Vegas, USA.</location>
<contexts>
<context position="4791" citStr="Elkan and Noto, 2008" startWordPosition="734" endWordPosition="737"> Meanwhile, a lot of unlabeled reviews are available. The problem thus is this: based on some truthful reviews and a lot of unlabeled reviews, can we build an accurate classifier to identify deceptive reviews. PU (positive unlabeled) learning can be utilized to deal with the above situation (Liu et al., 2002; Liu et al., 2003). Different from traditional supervised learning, PU learning can still build an accurate classifier even without the negative training examples. Several PU learning techniques have been applied successfully in document classification with promising results (Zhang, 2005; Elkan and Noto, 2008; Li et al., 2009; Xiao et al., 2011), while they have yet to be applied in detecting deceptive reviews. Here, we will study how to design PU learning to detect deceptive reviews. An important challenge is how to deal with spy examples (easily mislabeled) of unlabeled reviews, which is not easily handled by the previous PU learning techniques. In this paper, we propose a novel approach, mixing population and individual property PU learning (MPIPUL), by assigning similarity weights and incorporating weights into SVM learning phase. This paper makes the following contributions: • For the first t</context>
</contexts>
<marker>Elkan, Noto, 2008</marker>
<rawString>Charles Elkan and Keith Noto. 2008. Learning classifiers from only positive and unlabeled data. In Proceedings of the 14th ACM SIGKDD International Conference on Knowledge Discovery and Data Ming, page 213-220, Las Vegas, USA.</rawString>
</citation>
<citation valid="true">
<authors>
<author>Chirs Buckley</author>
<author>Bgrard Salton</author>
<author>James Allan</author>
</authors>
<title>The effect of adding relevance information in a relevance feedback environment.</title>
<date>1994</date>
<booktitle>In Proceedings of the 17th Annual International SIGIR Conference on Research and Development Retrieval,</booktitle>
<pages>292--300</pages>
<location>Dublin, Ireland.</location>
<contexts>
<context position="14830" citStr="Buckley et al., 1994" startWordPosition="2385" endWordPosition="2388">ive examples can be clustered into n micro-clusters (NS1, NS2, ... , NSn). Here, n = 30 * |NS|/(|US |+ |NS|) (2) Here, according to the suggestion of previous work (Xiao et al., 2011), we examine the impact of the different parameter (from 10 to 60) on overall performance, and select the best value 30. Based on the modified Rocchio formula (Buckley et al., 1999), n representative positive examples (pk) and n negative ones (nk) can be obtained using the following formula: 1 |NSk |x(i) |NSk |� II x(i) II 1 |P |x(i) II x(i) II − |P |-1- II x(i) II i=1 k = 1,...,n (3) According to previous works (Buckley et al., 1994), where the value of α and β are set to 16 and 4 respectively. The research from Buckley et al. demonstrate that this combination emphasizes occurrences in the relevant documents as opposed to non-relevant documents. 4.3 Generating Similarity Weights For a spy example x, since we do not know which class it should belong to, enforcing x to the positive class or the negative class will lead to some mislabeled examples, which disturbs the performance of final classifier. We represent a spy example x using the following probability model: {x, (p+(x), p−(x))}, p+(x) + p−(x) = 1 (4) Where p+(x) and </context>
</contexts>
<marker>Buckley, Salton, Allan, 1994</marker>
<rawString>Chirs Buckley, Bgrard Salton, and James Allan. 1994. The effect of adding relevance information in a relevance feedback environment. In Proceedings of the 17th Annual International SIGIR Conference on Research and Development Retrieval, page 292-300, Dublin, Ireland.</rawString>
</citation>
<citation valid="true">
<authors>
<author>David M Blei</author>
<author>Andrew Y Ng</author>
<author>Michael I Jordan</author>
</authors>
<title>Latent dirichlet allocation.</title>
<date>2003</date>
<journal>Journal of Machine Learning Research,</journal>
<pages>3--993</pages>
<contexts>
<context position="13746" citStr="Blei et al., 2003" startWordPosition="2193" endWordPosition="2196">gative examples are calculated firstly. Since the examples have different styles in sentiment polarity and topic distribution, for every class, computing one representative example is not suitable. For the positive class or the negative class, to ensure there is a big difference between the different representative examples. This paper proposes clustering reliable negative examples into several groups based on LDA (Latent Dirichlet Allocation) topic model and K-means, and then multiple representative examples can be obtained. LDA topic model is known as a parametric Bayesian clustering model (Blei et al., 2003), and assumes that each document can be represented as the distribution of several topics, each document is associated with common topics. LDA can well capture the relationship between internal documents. In our experiments based on LDA model, we can get the topic distribution for the reliable negative examples, then some reliable negative examples which are similar in topic distribution will be clustered into a group by K-means. Finally, these reliable negative examples can be clustered into n micro-clusters (NS1, NS2, ... , NSn). Here, n = 30 * |NS|/(|US |+ |NS|) (2) Here, according to the s</context>
</contexts>
<marker>Blei, Ng, Jordan, 2003</marker>
<rawString>David M. Blei, Andrew Y. Ng, and Michael I. Jordan. 2003. Latent dirichlet allocation. Journal of Machine Learning Research, 3:993-1022.</rawString>
</citation>
<citation valid="true">
<authors>
<author>Dell Zhang</author>
</authors>
<title>A simple probabilistic approach to learning from positive and unlabeled examples.</title>
<date>2005</date>
<booktitle>In Proceedings of the 5th Annual UK Workshop on Computational Intelligence,</booktitle>
<pages>83--87</pages>
<contexts>
<context position="4769" citStr="Zhang, 2005" startWordPosition="732" endWordPosition="733">or knowledge. Meanwhile, a lot of unlabeled reviews are available. The problem thus is this: based on some truthful reviews and a lot of unlabeled reviews, can we build an accurate classifier to identify deceptive reviews. PU (positive unlabeled) learning can be utilized to deal with the above situation (Liu et al., 2002; Liu et al., 2003). Different from traditional supervised learning, PU learning can still build an accurate classifier even without the negative training examples. Several PU learning techniques have been applied successfully in document classification with promising results (Zhang, 2005; Elkan and Noto, 2008; Li et al., 2009; Xiao et al., 2011), while they have yet to be applied in detecting deceptive reviews. Here, we will study how to design PU learning to detect deceptive reviews. An important challenge is how to deal with spy examples (easily mislabeled) of unlabeled reviews, which is not easily handled by the previous PU learning techniques. In this paper, we propose a novel approach, mixing population and individual property PU learning (MPIPUL), by assigning similarity weights and incorporating weights into SVM learning phase. This paper makes the following contributi</context>
</contexts>
<marker>Zhang, 2005</marker>
<rawString>Dell Zhang. 2005. A simple probabilistic approach to learning from positive and unlabeled examples. In Proceedings of the 5th Annual UK Workshop on Computational Intelligence, page 83-87.</rawString>
</citation>
<citation valid="true">
<authors>
<author>Fangtao Li</author>
<author>Minlie Huang</author>
<author>Yi Yang</author>
<author>Xiaoyan Zhu</author>
</authors>
<title>Learning to identify review spam.</title>
<date>2011</date>
<booktitle>In Proceeding of the 22nd International Joint Conference on Artificial Intelligence,</booktitle>
<pages>2488--2493</pages>
<location>Barcelona,</location>
<contexts>
<context position="3148" citStr="Li et al., 2011" startWordPosition="475" endWordPosition="478">rs by giving undeserving positive reviews to some target objects in order to promote the objects, or by giving unjust negative reviews to some target objects in order to damage their reputation. • Disruptive Reviews: Those are non-reviews, which mainly include advertisements and other irrelevant reviews containing no opinion. Disruptive reviews pose little threat to people, because human can easily identify and ignore them. In this paper, we focus on the more challenging ones: deceptive reviews. Generally, deceptive reviews detection is deemed to be a classification problem (Ott et al., 2011; Li et al., 2011; Feng et al., 2012). Based on the positive and negative examples annotated by people, supervised learning is utilized to build a classifier, and then an unlabeled review can be predicted as deceptive review or truthful one. But the work from Ott et al. (2011) shows that human cannot identify deceptive reviews from their prior knowledge, which indicates that human-annotated review datasets must 488 Proceedings of the 2014 Conference on Empirical Methods in Natural Language Processing (EMNLP), pages 488–498, October 25-29, 2014, Doha, Qatar. c�2014 Association for Computational Linguistics incl</context>
<context position="6646" citStr="Li et al. (2011)" startWordPosition="1039" endWordPosition="1042">rained models using product review data, by defining features to distinguish duplicate opinion and non-duplicate based on the review text, reviewers and product information. Wu et al. (2010) proposed an alternative strategy of popularity rankings (Wu et al., 2010). Ott et al. (2011) developed the first dataset containing gold-standard deceptive reviews by crowdsourcing (Ott et al., 2011), and presented three supervised learning methods to detect deceptive reviews by integrating knowledge from psycholinguistics and computational linguistics. This goldstandard dataset will be used in the paper. Li et al. (2011) manually built a review dataset from their crawled reviews (Li et al., 2011), and exploited semi-supervised co-training algorithm to identify deceptive reviews. Feng et al. (2012) verified the connection between the deceptive reviews and the abnormal distributions (Feng et al., 2012a). Later, they (Feng et al., 2012b) demonstrated that features driven from CFG (Context Free Grammar) parsing trees consistently improve the detection performance. Mukherjee et al. (2012) proposed detecting group spammers (a group of reviewers who work collaboratively to write deceptive reviews) in product reviews</context>
</contexts>
<marker>Li, Huang, Yang, Zhu, 2011</marker>
<rawString>Fangtao Li, Minlie Huang, Yi Yang, and Xiaoyan Zhu. 2011. Learning to identify review spam. In Proceeding of the 22nd International Joint Conference on Artificial Intelligence, page 2488-2493, Barcelona, Spain.</rawString>
</citation>
<citation valid="true">
<authors>
<author>Fang Wu</author>
<author>Bernardo A Huberman</author>
</authors>
<title>Opinion formation under costly express.</title>
<date>2010</date>
<journal>ACM Transactions on Intelligence System Technology,</journal>
<pages>1--5</pages>
<marker>Wu, Huberman, 2010</marker>
<rawString>Fang Wu and Bernardo A. Huberman. 2010. Opinion formation under costly express. ACM Transactions on Intelligence System Technology, 1(5):1-13.</rawString>
</citation>
<citation valid="true">
<authors>
<author>Gert R G Lanckeriet</author>
<author>Nello Cristianini</author>
<author>Peter Bartlett</author>
<author>Laurent EI Ghaoui</author>
<author>Michael I Jordan</author>
</authors>
<title>Learning the kernel matrix with seim-difinit programming.</title>
<date>2004</date>
<journal>Journal of Machine Learning Research,</journal>
<pages>5--27</pages>
<marker>Lanckeriet, Cristianini, Bartlett, Ghaoui, Jordan, 2004</marker>
<rawString>Gert R. G. Lanckeriet, Nello Cristianini, Peter Bartlett, Laurent EI Ghaoui, and Michael I.Jordan. 2004. Learning the kernel matrix with seim-difinit programming. Journal of Machine Learning Research, 5:27-72.</rawString>
</citation>
<citation valid="true">
<authors>
<author>Harris Drucker</author>
<author>Donghui Wu</author>
<author>Vladimir N Vapnik</author>
</authors>
<title>Support vector machines for spam categorization.</title>
<date>1999</date>
<journal>IEEE Transactions on Neural Networks,</journal>
<pages>10--5</pages>
<contexts>
<context position="5757" citStr="Drucker et al., 1999" startWordPosition="894" endWordPosition="897">In this paper, we propose a novel approach, mixing population and individual property PU learning (MPIPUL), by assigning similarity weights and incorporating weights into SVM learning phase. This paper makes the following contributions: • For the first time, PU learning is defined in the environment of identifying deceptive reviews. • A novel PU learning is proposed based on LDA and SVM. • Experimental results demonstrate that our proposed method outperforms the current baselines. 2 Related Work 2.1 Deceptive Reviews Detection Spam has historically been investigated in the contexts of e-mail (Drucker et al., 1999; Gyongyi et al., 2004) and the Web (Ntoulas et al., 2006). In recent years, researchers have started to look at deceptive reviews. Jindal and Liu (2008) found that opinion spam was widespread and different from e-mail and Web spam in essence (Jindal and Liu, 2008). They trained models using product review data, by defining features to distinguish duplicate opinion and non-duplicate based on the review text, reviewers and product information. Wu et al. (2010) proposed an alternative strategy of popularity rankings (Wu et al., 2010). Ott et al. (2011) developed the first dataset containing gold</context>
</contexts>
<marker>Drucker, Wu, Vapnik, 1999</marker>
<rawString>Harris Drucker, Donghui Wu, and Vladimir N. Vapnik. 1999. Support vector machines for spam categorization. IEEE Transactions on Neural Networks, 10(5):1048-1054.</rawString>
</citation>
<citation valid="true">
<authors>
<author>Kumar Ankita</author>
<author>Sminchisescu Cristian</author>
</authors>
<title>Support kernel machines for object recognition.</title>
<date>2006</date>
<booktitle>In Proceedings of the IEEE 11th International Conference on Computer Vision,</booktitle>
<pages>1--8</pages>
<location>Rio de Janeiro, Brzail.</location>
<marker>Ankita, Cristian, 2006</marker>
<rawString>Kumar Ankita and Sminchisescu Cristian. 2006. Support kernel machines for object recognition. In Proceedings of the IEEE 11th International Conference on Computer Vision, page 1-8, Rio de Janeiro, Brzail.</rawString>
</citation>
<citation valid="true">
<authors>
<author>Myle Ott</author>
<author>Yelin Choi</author>
<author>Claire Caridie</author>
<author>Jeffrey T Hancock</author>
</authors>
<title>Finding deceptive opinion spam by any stretch of the imagination.</title>
<date>2011</date>
<booktitle>In Proceeding of the 49th Annual Meeting of the Association for Computational Linguistics: Human Language Technoloies,</booktitle>
<pages>309--319</pages>
<location>Portland, USA.</location>
<contexts>
<context position="2421" citStr="Ott et al., 2011" startWordPosition="357" endWordPosition="360">reviews on products and services are used extensively by consumers and businesses to conduct decisive purchase, product design and marketing strategies. Hence, sentiment analysis and opinion mining based on product reviews have become a popular topic of NLP (Pang and Lee, 2008; Liu, 2012). However, since reviews information can guide people’s purchase behavior, positive reviews can result in huge economic benefits and fame for organizations or individuals. This leaves room for promoting the generation of review spams. Through observations and studies of the predecessors (Jindal and Liu, 2008; Ott et al., 2011), review spams are divided into the following two classes: • Deceptive Reviews: Those deliberately mislead readers by giving undeserving positive reviews to some target objects in order to promote the objects, or by giving unjust negative reviews to some target objects in order to damage their reputation. • Disruptive Reviews: Those are non-reviews, which mainly include advertisements and other irrelevant reviews containing no opinion. Disruptive reviews pose little threat to people, because human can easily identify and ignore them. In this paper, we focus on the more challenging ones: decept</context>
<context position="6313" citStr="Ott et al. (2011)" startWordPosition="988" endWordPosition="991"> investigated in the contexts of e-mail (Drucker et al., 1999; Gyongyi et al., 2004) and the Web (Ntoulas et al., 2006). In recent years, researchers have started to look at deceptive reviews. Jindal and Liu (2008) found that opinion spam was widespread and different from e-mail and Web spam in essence (Jindal and Liu, 2008). They trained models using product review data, by defining features to distinguish duplicate opinion and non-duplicate based on the review text, reviewers and product information. Wu et al. (2010) proposed an alternative strategy of popularity rankings (Wu et al., 2010). Ott et al. (2011) developed the first dataset containing gold-standard deceptive reviews by crowdsourcing (Ott et al., 2011), and presented three supervised learning methods to detect deceptive reviews by integrating knowledge from psycholinguistics and computational linguistics. This goldstandard dataset will be used in the paper. Li et al. (2011) manually built a review dataset from their crawled reviews (Li et al., 2011), and exploited semi-supervised co-training algorithm to identify deceptive reviews. Feng et al. (2012) verified the connection between the deceptive reviews and the abnormal distributions (</context>
<context position="22312" citStr="Ott et al., 2011" startWordPosition="3726" endWordPosition="3729">if wT x + b &gt; 0, it belongs to the positive class. Otherwise, it is negative. 5 Experiments We aim to evaluate whether our proposed PU learning can identify deceptive reviews properly. We firstly describe the gold-standard dataset, and then introduce the way to generate the positive examples P and unlabeled examples U. Finally we present human performance in gold-standard dataset. 5.1 Datasets There is very little progress in detection of deceptive reviews, one reason is the lack of standard dataset for algorithm evaluation. The goldstandard dataset is created based on crowdsourcing platform (Ott et al., 2011), which is also adopted as the experimental dataset in this paper. 5.1.1 Deceptive Reviews Crowdsourcing services can carry out massive data collection and annotation; it defines the task in the network platform, and paid for online anonymous workers to complete the task. ∑ |T*| i=1 αi − |T *| 1 ∑ 2 i=1,j=1 αiαjy(i)· max W (α) = y(j) &lt; x(i),x(j) &gt; s.t. C+i &gt; αi &gt; 0, x(i) E T+ C−j &gt; αj &gt; 0, x(j) E T− |T+| αj = 0 (9) ∑ i=1 |T ∑ | j=1 αi − |US |∑ p+(x(j))Ej + C3 |US |∑ p−(x(m))Em j=1 m=1 +C4 |NS |En ∑ n=1 |P| min F (w, b, E) = 2 ||w ||2 + C1 ∑ i=1 Ei + C2· 493 Humans cannot be precisely distingui</context>
</contexts>
<marker>Ott, Choi, Caridie, Hancock, 2011</marker>
<rawString>Myle Ott, Yelin Choi, Claire Caridie, and Jeffrey T. Hancock. 2011. Finding deceptive opinion spam by any stretch of the imagination. In Proceeding of the 49th Annual Meeting of the Association for Computational Linguistics: Human Language Technoloies, page 309-319, Portland, USA.</rawString>
</citation>
<citation valid="true">
<authors>
<author>Nitin Jindal</author>
<author>Bing Liu</author>
</authors>
<title>Opinion spam and analysis.</title>
<date>2008</date>
<booktitle>In Proceeding of the 1st ACM International Conference on Web Search and Data Mining,</booktitle>
<pages>137--142</pages>
<location>California, USA.</location>
<contexts>
<context position="2402" citStr="Jindal and Liu, 2008" startWordPosition="353" endWordPosition="356">cs. Currently, online reviews on products and services are used extensively by consumers and businesses to conduct decisive purchase, product design and marketing strategies. Hence, sentiment analysis and opinion mining based on product reviews have become a popular topic of NLP (Pang and Lee, 2008; Liu, 2012). However, since reviews information can guide people’s purchase behavior, positive reviews can result in huge economic benefits and fame for organizations or individuals. This leaves room for promoting the generation of review spams. Through observations and studies of the predecessors (Jindal and Liu, 2008; Ott et al., 2011), review spams are divided into the following two classes: • Deceptive Reviews: Those deliberately mislead readers by giving undeserving positive reviews to some target objects in order to promote the objects, or by giving unjust negative reviews to some target objects in order to damage their reputation. • Disruptive Reviews: Those are non-reviews, which mainly include advertisements and other irrelevant reviews containing no opinion. Disruptive reviews pose little threat to people, because human can easily identify and ignore them. In this paper, we focus on the more chall</context>
<context position="5910" citStr="Jindal and Liu (2008)" startWordPosition="921" endWordPosition="924">ating weights into SVM learning phase. This paper makes the following contributions: • For the first time, PU learning is defined in the environment of identifying deceptive reviews. • A novel PU learning is proposed based on LDA and SVM. • Experimental results demonstrate that our proposed method outperforms the current baselines. 2 Related Work 2.1 Deceptive Reviews Detection Spam has historically been investigated in the contexts of e-mail (Drucker et al., 1999; Gyongyi et al., 2004) and the Web (Ntoulas et al., 2006). In recent years, researchers have started to look at deceptive reviews. Jindal and Liu (2008) found that opinion spam was widespread and different from e-mail and Web spam in essence (Jindal and Liu, 2008). They trained models using product review data, by defining features to distinguish duplicate opinion and non-duplicate based on the review text, reviewers and product information. Wu et al. (2010) proposed an alternative strategy of popularity rankings (Wu et al., 2010). Ott et al. (2011) developed the first dataset containing gold-standard deceptive reviews by crowdsourcing (Ott et al., 2011), and presented three supervised learning methods to detect deceptive reviews by integrati</context>
</contexts>
<marker>Jindal, Liu, 2008</marker>
<rawString>Nitin Jindal and Bing Liu. 2008. Opinion spam and analysis. In Proceeding of the 1st ACM International Conference on Web Search and Data Mining, page 137-142, California, USA.</rawString>
</citation>
<citation valid="true">
<authors>
<author>Richard Landis</author>
<author>Gary G Koch</author>
</authors>
<title>The measurement of observer agreement for categorical data.</title>
<date>1977</date>
<journal>Biometrics,</journal>
<pages>33--1</pages>
<contexts>
<context position="26015" citStr="Landis and Koch, 1977" startWordPosition="4374" endWordPosition="4377">contains 40 deceptive reviews and 40 truthful reviews). Additionally, to test the extent to which the individual human judges are biased, we evaluate the performance of two virtual meta-judges: one is the MAJORITY meta-judge when at last two out of three human judge believe the review to be deceptive, and the other is the SKEPTIC when any human judge believes the review to be deceptive. It is apparent from the results that human judges are not particularly effective at this task (Table 1). Interannotator agreement among the three judges, computed using Fleiss’ kappa, is 0.09. Landis and Koch (Landis and Koch, 1977) suggest that scores in the range (0.00, 0.20) correspond to “slight agreemen” between annotators. The largest pairwise Cohen’s kappa is 0.11 between JUDGE1 and JUDGE-3, far below generally accepted pairwise agreement levels. We can infer that the dataset which are annotated by people will include a lot of mislabeled examples. Identifying deceptive reviews by simply using supervised learning methods is not appropriate. So we propose addressing this issue by using PU learning. Table 1: Human performance Methods Accuracy (%) JUDGE-1 57.9 Human JUDGE-2 55.4 JUDGE-3 61.7 META MAJORITY 58.3 SKEPTIC</context>
</contexts>
<marker>Landis, Koch, 1977</marker>
<rawString>Richard Landis and Gary G. Koch. 1977. The measurement of observer agreement for categorical data. Biometrics, 33(1):159-174.</rawString>
</citation>
<citation valid="true">
<authors>
<author>Song Feng</author>
<author>Longfei Xing</author>
<author>Anupam Gogar</author>
<author>Yejin Choi</author>
</authors>
<title>Distributional footprints of deceptive product reviews.</title>
<date>2012</date>
<booktitle>In Proceeding of the 6th International AAAI Conference on WebBlogs and Social Media,</booktitle>
<pages>98--105</pages>
<location>Dublin, Ireland.</location>
<contexts>
<context position="3168" citStr="Feng et al., 2012" startWordPosition="479" endWordPosition="482">serving positive reviews to some target objects in order to promote the objects, or by giving unjust negative reviews to some target objects in order to damage their reputation. • Disruptive Reviews: Those are non-reviews, which mainly include advertisements and other irrelevant reviews containing no opinion. Disruptive reviews pose little threat to people, because human can easily identify and ignore them. In this paper, we focus on the more challenging ones: deceptive reviews. Generally, deceptive reviews detection is deemed to be a classification problem (Ott et al., 2011; Li et al., 2011; Feng et al., 2012). Based on the positive and negative examples annotated by people, supervised learning is utilized to build a classifier, and then an unlabeled review can be predicted as deceptive review or truthful one. But the work from Ott et al. (2011) shows that human cannot identify deceptive reviews from their prior knowledge, which indicates that human-annotated review datasets must 488 Proceedings of the 2014 Conference on Empirical Methods in Natural Language Processing (EMNLP), pages 488–498, October 25-29, 2014, Doha, Qatar. c�2014 Association for Computational Linguistics include some mislabeled </context>
<context position="6826" citStr="Feng et al. (2012)" startWordPosition="1065" endWordPosition="1068">t al. (2010) proposed an alternative strategy of popularity rankings (Wu et al., 2010). Ott et al. (2011) developed the first dataset containing gold-standard deceptive reviews by crowdsourcing (Ott et al., 2011), and presented three supervised learning methods to detect deceptive reviews by integrating knowledge from psycholinguistics and computational linguistics. This goldstandard dataset will be used in the paper. Li et al. (2011) manually built a review dataset from their crawled reviews (Li et al., 2011), and exploited semi-supervised co-training algorithm to identify deceptive reviews. Feng et al. (2012) verified the connection between the deceptive reviews and the abnormal distributions (Feng et al., 2012a). Later, they (Feng et al., 2012b) demonstrated that features driven from CFG (Context Free Grammar) parsing trees consistently improve the detection performance. Mukherjee et al. (2012) proposed detecting group spammers (a group of reviewers who work collaboratively to write deceptive reviews) in product reviews (Mukherjee et al., 2012). The proposed method first used frequent itemset mining to find a set of candidate groups. Then GSRank was presented which can consider relationships amon</context>
</contexts>
<marker>Feng, Xing, Gogar, Choi, 2012</marker>
<rawString>Song Feng, Longfei Xing, Anupam Gogar, and Yejin Choi. 2012. Distributional footprints of deceptive product reviews. In Proceeding of the 6th International AAAI Conference on WebBlogs and Social Media, page 98-105, Dublin, Ireland.</rawString>
</citation>
<citation valid="true">
<authors>
<author>Song Feng</author>
<author>Ritwik Banerjee</author>
<author>Yejin Choi</author>
</authors>
<title>Syntactic stylometry for deception detection.</title>
<date>2012</date>
<booktitle>In Proceeding of the 50th Annual Meeting of the Association for Computational Linguistics,</booktitle>
<pages>171--175</pages>
<location>Jeju Island,</location>
<contexts>
<context position="3168" citStr="Feng et al., 2012" startWordPosition="479" endWordPosition="482">serving positive reviews to some target objects in order to promote the objects, or by giving unjust negative reviews to some target objects in order to damage their reputation. • Disruptive Reviews: Those are non-reviews, which mainly include advertisements and other irrelevant reviews containing no opinion. Disruptive reviews pose little threat to people, because human can easily identify and ignore them. In this paper, we focus on the more challenging ones: deceptive reviews. Generally, deceptive reviews detection is deemed to be a classification problem (Ott et al., 2011; Li et al., 2011; Feng et al., 2012). Based on the positive and negative examples annotated by people, supervised learning is utilized to build a classifier, and then an unlabeled review can be predicted as deceptive review or truthful one. But the work from Ott et al. (2011) shows that human cannot identify deceptive reviews from their prior knowledge, which indicates that human-annotated review datasets must 488 Proceedings of the 2014 Conference on Empirical Methods in Natural Language Processing (EMNLP), pages 488–498, October 25-29, 2014, Doha, Qatar. c�2014 Association for Computational Linguistics include some mislabeled </context>
<context position="6826" citStr="Feng et al. (2012)" startWordPosition="1065" endWordPosition="1068">t al. (2010) proposed an alternative strategy of popularity rankings (Wu et al., 2010). Ott et al. (2011) developed the first dataset containing gold-standard deceptive reviews by crowdsourcing (Ott et al., 2011), and presented three supervised learning methods to detect deceptive reviews by integrating knowledge from psycholinguistics and computational linguistics. This goldstandard dataset will be used in the paper. Li et al. (2011) manually built a review dataset from their crawled reviews (Li et al., 2011), and exploited semi-supervised co-training algorithm to identify deceptive reviews. Feng et al. (2012) verified the connection between the deceptive reviews and the abnormal distributions (Feng et al., 2012a). Later, they (Feng et al., 2012b) demonstrated that features driven from CFG (Context Free Grammar) parsing trees consistently improve the detection performance. Mukherjee et al. (2012) proposed detecting group spammers (a group of reviewers who work collaboratively to write deceptive reviews) in product reviews (Mukherjee et al., 2012). The proposed method first used frequent itemset mining to find a set of candidate groups. Then GSRank was presented which can consider relationships amon</context>
</contexts>
<marker>Feng, Banerjee, Choi, 2012</marker>
<rawString>Song Feng, Ritwik Banerjee, and Yejin Choi. 2012. Syntactic stylometry for deception detection. In Proceeding of the 50th Annual Meeting of the Association for Computational Linguistics, page 171-175, Jeju Island, Korea.</rawString>
</citation>
<citation valid="true">
<authors>
<author>Vladimir N Vapnik</author>
</authors>
<title>Statistical learning theory.</title>
<date>1998</date>
<publisher>Springer.</publisher>
<location>New York, USA.</location>
<contexts>
<context position="10185" citStr="Vapnik, 1998" startWordPosition="1589" endWordPosition="1590">together shared the same labels. Xiao et al. (Xiao et al., 2011) proposed a method, called SPUL (similarity-based PU learning), the local similarity-based and global similarity-based mechanisms are proposed to generate the similarity weights for the easily mislabeled examples, respectively. Experimental results show global SPUL generally performs better than local SPUL. In this paper, a novel PU learning (MPIPUL) is proposed to identify deceptive reviews. 3 Preliminary Before we introduce the proposed method, we briefly review SVM, which has proven to be an effective classification algorithm (Vapnik, 1998). Let T = {(x(1), y(1)), (x(2), y(2)), ... , (x(|T|), y(|T| )} be a training set, where x(i) ∈ Rd and y(i) ∈ {+1, −1}. SVM aims to seek an optimal separating hyperplane wT x(i) + b = 0, the hyperplane can be obtained by solving the following optimization problem: |T| min F(w,b,Ei) = 2||w||2 + C� i=1 s.t. y(i)(wTx(i) + b) ≥ 1 − Ei, i = 1, ... , |T| Ei ≥ 0,i = 1,...,|T| (1) where wT represents the transpose of w, C is a parameter to balance the classification errors and Ei are variables to relax the margin constraints. The optimal classifier can be achieved by using the Lagrange function. For a </context>
</contexts>
<marker>Vapnik, 1998</marker>
<rawString>Vladimir N. Vapnik. 1998. Statistical learning theory. Springer. New York, USA.</rawString>
</citation>
<citation valid="true">
<authors>
<author>Wanjui Lee</author>
<author>Sergey Verzakov</author>
<author>Robert P Duin</author>
</authors>
<title>Kernel combination versus classifier combination.</title>
<date>2007</date>
<booktitle>In Proceedings of the 7th International Workshop on Multiple Classifier Systems,</booktitle>
<pages>22--31</pages>
<location>Rrague, Czech Republic.</location>
<contexts>
<context position="21545" citStr="Lee et al., 2007" startWordPosition="3597" endWordPosition="3600">s, we let T+ = P U US, T− = US U NS and T∗ = T+ U T−. The Wolfe dual of primal formulation can be obtained as follows (Appendix A for the calculation where &lt; x(i), x(j) &gt; is the inner product of x(i) and x(j). In order to get the better performance, we can replace them by using kernel function O(x(i)) and O(x(j)), respectively. The kernel track can convert the input space into a high-dimension feature space. It can solve the uneven distribution of dataset and complex problem from heterogeneous data sources, which allows data to get a better expression in the new space (Lanckriet et al., 2004; Lee et al., 2007). After solving the above problem, w can be obtained, then b can also be obtained by using KKT (Karush-Kuhn-Tucker) conditions. For a test example x, if wT x + b &gt; 0, it belongs to the positive class. Otherwise, it is negative. 5 Experiments We aim to evaluate whether our proposed PU learning can identify deceptive reviews properly. We firstly describe the gold-standard dataset, and then introduce the way to generate the positive examples P and unlabeled examples U. Finally we present human performance in gold-standard dataset. 5.1 Datasets There is very little progress in detection of decepti</context>
</contexts>
<marker>Lee, Verzakov, Duin, 2007</marker>
<rawString>Wanjui Lee, Sergey Verzakov, and Robert P. Duin. 2007. Kernel combination versus classifier combination. In Proceedings of the 7th International Workshop on Multiple Classifier Systems, page 22-31, Rrague, Czech Republic.</rawString>
</citation>
<citation valid="true">
<authors>
<author>Xiaoli Li</author>
<author>Philip S Yu</author>
<author>Bing Liu</author>
<author>See Kiong Ng</author>
</authors>
<title>Positive unlabeled learning for data stream classification.</title>
<date>2009</date>
<booktitle>In Proceedings of the SIAM International Conference on Data Ming,</booktitle>
<pages>257--268</pages>
<location>Nevada, USA.</location>
<contexts>
<context position="4808" citStr="Li et al., 2009" startWordPosition="738" endWordPosition="741">nlabeled reviews are available. The problem thus is this: based on some truthful reviews and a lot of unlabeled reviews, can we build an accurate classifier to identify deceptive reviews. PU (positive unlabeled) learning can be utilized to deal with the above situation (Liu et al., 2002; Liu et al., 2003). Different from traditional supervised learning, PU learning can still build an accurate classifier even without the negative training examples. Several PU learning techniques have been applied successfully in document classification with promising results (Zhang, 2005; Elkan and Noto, 2008; Li et al., 2009; Xiao et al., 2011), while they have yet to be applied in detecting deceptive reviews. Here, we will study how to design PU learning to detect deceptive reviews. An important challenge is how to deal with spy examples (easily mislabeled) of unlabeled reviews, which is not easily handled by the previous PU learning techniques. In this paper, we propose a novel approach, mixing population and individual property PU learning (MPIPUL), by assigning similarity weights and incorporating weights into SVM learning phase. This paper makes the following contributions: • For the first time, PU learning </context>
<context position="9349" citStr="Li et al., 2009" startWordPosition="1462" endWordPosition="1465">rning can be divided into two classes. One family of methods built the final classifier by using positive examples dataset and some examples of the unlabeled dataset (Liu et al., 2002; Liu et al., 2003). The basic idea is to find a set of reliable negative examples from the unlabeled data firstly, and then to learn a classifier using EM (Expectation Maximization) or SVM. The performance is limited for neglecting the rest examples of unlabeled dataset. Another family of methods learned the final classifier by using positive examples dataset and all examples of the unlabeled dataset. Li et al. (Li et al., 2009) studied PU learning in the data stream environment, they proposed a PU learning LELC (PU Learning by Extracting Likely positive and negative micro-Clusters) for document classification, they assume that the examples close together shared the same labels. Xiao et al. (Xiao et al., 2011) proposed a method, called SPUL (similarity-based PU learning), the local similarity-based and global similarity-based mechanisms are proposed to generate the similarity weights for the easily mislabeled examples, respectively. Experimental results show global SPUL generally performs better than local SPUL. In t</context>
</contexts>
<marker>Li, Yu, Liu, Ng, 2009</marker>
<rawString>Xiaoli Li, Philip S. Yu, Bing Liu, and See Kiong Ng. 2009. Positive unlabeled learning for data stream classification. In Proceedings of the SIAM International Conference on Data Ming, page 257-268, Nevada, USA.</rawString>
</citation>
<citation valid="true">
<authors>
<author>Yafeng Ren</author>
<author>Donghong Ji</author>
<author>Lan Yin</author>
<author>Hongbin Zhang</author>
</authors>
<title>Finding deceptive opinion spam by correcting the mislabled instances.</title>
<date>2014</date>
<journal>Chinese Journal of Electronics,</journal>
<pages>23--4</pages>
<contexts>
<context position="7694" citStr="Ren et al. (2014)" startWordPosition="1197" endWordPosition="1200">tion performance. Mukherjee et al. (2012) proposed detecting group spammers (a group of reviewers who work collaboratively to write deceptive reviews) in product reviews (Mukherjee et al., 2012). The proposed method first used frequent itemset mining to find a set of candidate groups. Then GSRank was presented which can consider relationships among groups, individual reviewers and products they reviewed to detect spammer groups. Later, they also proposed exploiting observed reviewing behaviors to detect opinion spammers in an unsupervised Bayesian inference framework (Mukherjee et al., 2013). Ren et al. (2014) assumed that there must be some difference on language structure and sentiment polarity between deceptive reviews and truthful ones (Ren et al., 2014a), then they defined the features related to the review text and used genetic algorithm for feature selection, finally they combined two unsupervised clustering algorithm to identify deceptive reviews. Later, they (Ren et al., 2014b) present a new approach, from the viewpoint of correcting the mislabeled 489 examples, to find deceptive reviews. Firstly, they partition a dataset into several subsets.Then they construct a classifier set for each s</context>
</contexts>
<marker>Ren, Ji, Yin, Zhang, 2014</marker>
<rawString>Yafeng Ren, Donghong Ji, Lan Yin, and Hongbin Zhang. 2014. Finding deceptive opinion spam by correcting the mislabled instances. Chinese Journal of Electronics, 23(4):702-707.</rawString>
</citation>
<citation valid="true">
<authors>
<author>Yafeng Ren</author>
<author>Lan Yin</author>
<author>Donghong Ji</author>
</authors>
<title>Deceptive reviews detection based on language structure and sentiment polarity.</title>
<date>2014</date>
<journal>Journal of Frontiers of Computer Science and Technology,</journal>
<pages>8--3</pages>
<contexts>
<context position="7694" citStr="Ren et al. (2014)" startWordPosition="1197" endWordPosition="1200">tion performance. Mukherjee et al. (2012) proposed detecting group spammers (a group of reviewers who work collaboratively to write deceptive reviews) in product reviews (Mukherjee et al., 2012). The proposed method first used frequent itemset mining to find a set of candidate groups. Then GSRank was presented which can consider relationships among groups, individual reviewers and products they reviewed to detect spammer groups. Later, they also proposed exploiting observed reviewing behaviors to detect opinion spammers in an unsupervised Bayesian inference framework (Mukherjee et al., 2013). Ren et al. (2014) assumed that there must be some difference on language structure and sentiment polarity between deceptive reviews and truthful ones (Ren et al., 2014a), then they defined the features related to the review text and used genetic algorithm for feature selection, finally they combined two unsupervised clustering algorithm to identify deceptive reviews. Later, they (Ren et al., 2014b) present a new approach, from the viewpoint of correcting the mislabeled 489 examples, to find deceptive reviews. Firstly, they partition a dataset into several subsets.Then they construct a classifier set for each s</context>
</contexts>
<marker>Ren, Yin, Ji, 2014</marker>
<rawString>Yafeng Ren, Lan Yin, and Donghong Ji. 2014. Deceptive reviews detection based on language structure and sentiment polarity. Journal of Frontiers of Computer Science and Technology, 8(3):313-320.</rawString>
</citation>
<citation valid="false">
<authors>
<author>Yanshan Xiao</author>
<author>Bing Liu</author>
</authors>
<location>Jie Yin, Longbing Cao,</location>
<marker>Xiao, Liu, </marker>
<rawString>Yanshan Xiao, Bing Liu, Jie Yin, Longbing Cao,</rawString>
</citation>
<citation valid="true">
<authors>
<author>Chengqi Zhang</author>
<author>Zhifeng Hao</author>
</authors>
<title>Similarity-based approach for positive and unlabeled learning.</title>
<date>2011</date>
<booktitle>In Proceeding of the 22nd International Joint Conference on Artifical Intelligence,</booktitle>
<pages>1577--1582</pages>
<location>Barcelona,</location>
<marker>Zhang, Hao, 2011</marker>
<rawString>Chengqi Zhang, and Zhifeng Hao. 2011. Similarity-based approach for positive and unlabeled learning. In Proceeding of the 22nd International Joint Conference on Artifical Intelligence, page 1577-1582, Barcelona, Spain.</rawString>
</citation>
<citation valid="true">
<authors>
<author>Zoltan Gyongyi</author>
<author>Hector Garcia-Molina</author>
<author>Jan Pedesen</author>
</authors>
<title>Combating web spam web with trustrank.</title>
<date>2004</date>
<booktitle>In Proceedings of the 30th International Conference on Very Large Data Bases,</booktitle>
<pages>576--587</pages>
<location>Toronto, Canada.</location>
<contexts>
<context position="5780" citStr="Gyongyi et al., 2004" startWordPosition="898" endWordPosition="901">ose a novel approach, mixing population and individual property PU learning (MPIPUL), by assigning similarity weights and incorporating weights into SVM learning phase. This paper makes the following contributions: • For the first time, PU learning is defined in the environment of identifying deceptive reviews. • A novel PU learning is proposed based on LDA and SVM. • Experimental results demonstrate that our proposed method outperforms the current baselines. 2 Related Work 2.1 Deceptive Reviews Detection Spam has historically been investigated in the contexts of e-mail (Drucker et al., 1999; Gyongyi et al., 2004) and the Web (Ntoulas et al., 2006). In recent years, researchers have started to look at deceptive reviews. Jindal and Liu (2008) found that opinion spam was widespread and different from e-mail and Web spam in essence (Jindal and Liu, 2008). They trained models using product review data, by defining features to distinguish duplicate opinion and non-duplicate based on the review text, reviewers and product information. Wu et al. (2010) proposed an alternative strategy of popularity rankings (Wu et al., 2010). Ott et al. (2011) developed the first dataset containing gold-standard deceptive rev</context>
</contexts>
<marker>Gyongyi, Garcia-Molina, Pedesen, 2004</marker>
<rawString>Zoltan Gyongyi, Hector Garcia-Molina, and Jan Pedesen. 2004. Combating web spam web with trustrank. In Proceedings of the 30th International Conference on Very Large Data Bases, page 576-587, Toronto, Canada.</rawString>
</citation>
</citationList>
</algorithm>
</algorithms>
<?xml version="1.0" encoding="UTF-8"?>
<algorithms version="110505">
<algorithm name="SectLabel" version="110505">
<variant no="0" confidence="0.000000">
<title confidence="0.966459">
Latent-Variable Synchronous CFGs for Hierarchical Translation
</title>
<author confidence="0.986032">
Avneesh Saluja and Chris Dyer Shay B. Cohen
</author>
<affiliation confidence="0.8363665">
Carnegie Mellon University University of Edinburgh
Pittsburgh, PA, 15213, USA Edinburgh EH8 9AB, UK
</affiliation>
<email confidence="0.996523">
{avneesh,cdyer}@cs.cmu.edu scohen@inf.ed.ac.uk
</email>
<sectionHeader confidence="0.994749" genericHeader="abstract">
Abstract
</sectionHeader>
<bodyText confidence="0.999376277777778">
Data-driven refinement of non-terminal
categories has been demonstrated to be
a reliable technique for improving mono-
lingual parsing with PCFGs. In this pa-
per, we extend these techniques to learn
latent refinements of single-category syn-
chronous grammars, so as to improve
translation performance. We compare two
estimators for this latent-variable model:
one based on EM and the other is a spec-
tral algorithm based on the method of mo-
ments. We evaluate their performance on a
Chinese–English translation task. The re-
sults indicate that we can achieve signifi-
cant gains over the baseline with both ap-
proaches, but in particular the moments-
based estimator is both faster and performs
better than EM.
</bodyText>
<sectionHeader confidence="0.998782" genericHeader="keywords">
1 Introduction
</sectionHeader>
<bodyText confidence="0.999935933333334">
Translation models based on synchronous context-
free grammars (SCFGs) treat the translation prob-
lem as a context-free parsing problem. A parser
constructs trees over the input sentence by pars-
ing with the source language projection of a syn-
chronous CFG, and each derivation induces trans-
lations in the target language (Chiang, 2007).
However, in contrast to syntactic parsing, where
linguistic intuitions can help elucidate the “right”
tree structure for a grammatical sentence, no such
intuitions are available for synchronous deriva-
tions, and so learning the “right” grammars is a
central challenge.
Of course, learning synchronous grammars
from parallel data is a widely studied problem
(Wu, 1997; Blunsom et al., 2008; Levenberg et
al., 2012, inter alia). However, there has been
less exploration of learning rich non-terminal cat-
egories, largely because previous efforts to learn
such categories have been coupled with efforts
to learn derivation structures—a computationally
formidable challenge. One popular approach has
been to derive categories from source and/or target
monolingual grammars (Galley et al., 2004; Zoll-
mann and Venugopal, 2006; Hanneman and Lavie,
2013). While often successful, accurate parsers
are not available in many languages: a more ap-
pealing approach is therefore to learn the category
structure from the data itself.
In this work, we take a different approach to
previous work in synchronous grammar induc-
tion by assuming that reasonable tree structures
for a parallel corpus can be chosen heuristically,
and then, fixing the trees (thereby enabling us to
sidestep the worst of the computational issues), we
learn non-terminal categories as latent variables to
explain the distribution of these synchronous trees.
This technique has a long history in monolingual
parsing (Petrov et al., 2006; Liang et al., 2007;
Cohen et al., 2014), where it reliably yields state-
of-the-art phrase structure parsers based on gen-
erative models, but we are the first to apply it to
translation.
We first generalize the concept of latent PCFGs
to latent-variable SCFGs (§2). We then follow
by a presentation of the tensor-based formulation
for our parameters, a representation that makes it
convenient to marginalize over latent states. Sub-
sequently, two methods for parameter estimation
are presented (§4): a spectral approach based on
the method of moments, and an EM-based likeli-
hood maximization. Results on a Chinese–English
evaluation set (§5) indicate significant gains over
baselines and point to the promise of using latent-
variable synchronous grammars in conjunction
with a smaller, simpler set of rules instead of un-
wieldy and bloated grammars extracted via exist-
ing heuristics, where a large number of context-
independent but un-generalizable rules are uti-
lized. Hence, the hope is that this work pro-
</bodyText>
<page confidence="0.89591">
1953
</page>
<note confidence="0.899971">
Proceedings of the 2014 Conference on Empirical Methods in Natural Language Processing (EMNLP), pages 1953–1964,
October 25-29, 2014, Doha, Qatar. c�2014 Association for Computational Linguistics
</note>
<bodyText confidence="0.999312375">
motes the move towards translation models that
directly model the conditional likelihood of trans-
lation rules via (potentially feature-rich) latent-
variable models which leverage information con-
tained in the synchronous tree structure, instead
of relying on a heuristic set of features based on
empirical relative frequencies (Koehn et al., 2003)
from non-hierarchical phrase-based translation.
</bodyText>
<sectionHeader confidence="0.990087" genericHeader="introduction">
2 Latent-Variable SCFGs
</sectionHeader>
<bodyText confidence="0.999734888888889">
Before discussing parameter learning, we in-
troduce latent-variable synchronous context-free
grammars (L-SCFGs) and discuss an inference al-
gorithm for marginalizing over latent states.
We extend the definition of L-PCFGs (Mat-
suzaki et al., 2005; Petrov et al., 2006) to syn-
chronous grammars as used in machine transla-
tion (Chiang, 2007). A latent-variable SCFG (L-
SCFG) is a 6-tuple (N, m, ns, nt, 7r, t) where:
</bodyText>
<listItem confidence="0.9994762">
• N is a set of non-terminal (NT) symbols in the
grammar. For hierarchical phrase-based transla-
tion (HPBT), the set consists of only two sym-
bols, X and a goal symbol S.
• [m] is the set of possible hidden states associ-
ated with NTs. Aligned pairs of NTs across the
source and target languages share the same hid-
den state.
• [ns] is the set of source side words, i.e., the
source-side vocabulary, with [ns] n N = 0.
• [nt] is the set of target side words, i.e., the
target-side vocabulary, with [nt] n N = 0.
• The synchronous production rules compose a
set R = R0 U R1 U R2:
• Arity 2 (binary) rules (R2):
</listItem>
<equation confidence="0.998452">
a(h1) —* (α1b(h2)α2c(h3)α3,01b(h2)02c(h3)03)
or
a(h1) —* (α1b(h2)α2c(h3)α3,01c(h2)02b(h3)03)
</equation>
<bodyText confidence="0.6467965">
where a, b, c E N, h1, h2, h3 E [m],
α1, α2, α3 E [ns]* and 01, 02, 03 E [nt]*.
</bodyText>
<listItem confidence="0.982699">
• Arity 1 (unary) rules (R1):
</listItem>
<equation confidence="0.965767">
a(h1) —* (α1b(h2)α2,01b(h2)02)
</equation>
<bodyText confidence="0.976417">
where a, b E N, h1, h2 E [m], α1, α2 E [ns]*
and 0, 02 E [nt]*.
</bodyText>
<listItem confidence="0.8490915">
• Pre-terminal rules (R0): a(h1) —* (α,0)
where a E N, α E [nt]* and 0 E [ns]*.
</listItem>
<bodyText confidence="0.954607705882353">
Each of these rules is associated with a proba-
bility t(a(h1) —* -y|a, h1) where -y is the right-
hand side (RHS) of the rule.
• For a E N, h E [m], 7r(a, h) is a parameter
specifying the root probability of a(h).
A skeletal tree (s-tree) for a sentence is the set
of rules in the synchronous derivation of that sen-
tence, without any additional latent state informa-
tion or decoration. A full tree consists of an s-
tree r1, ... , rN together with values h1, ... , hN
for every NT in the tree. An important point to
keep in mind in comparison to L-PCFGs is that
the right-hand side (RHS) non-terminals of syn-
chronous rules are aligned pairs across the source
and target languages.
In this work, we refine the one-category gram-
mar introduced by Chiang (2007) for HPBT in or-
der to learn additional latent NT categories. Thus,
the following discussion is restricted to these kinds
of grammars, although the method is equally ap-
plicable in other scenarios, e.g., the extended tree-
to-string transducer (xRs) formalism (Huang et
al., 2006; Graehl et al., 2008) commonly used in
syntax-directed translation, and phrase-based MT
(Koehn et al., 2003).
Marginal Inference with L-SCFGs. For a pa-
rameter t of rule r, the latent state h1 attached to
the left-hand side (LHS) NT of r is associated with
the outside tree for the sub-tree rooted at the LHS,
and the states attached to the RHS NTs are asso-
ciated with the inside trees of that NT. Since we
do not assume conditional independence of these
states, we need to consider all possible interac-
tions, which can be compactly represented as a
3rd-order tensor in the case of a binary rule, a ma-
trix (i.e., a 2nd-order tensor) for unary rules, and
a vector for pre-terminal (lexical) rules. Prefer-
ences for certain outside-inside tree combinations
are reflected in the values contained in these tensor
structures. In this manner, we intend to capture in-
teractions between non-local context of a phrase,
which can typically be represented via features de-
fined over outside trees of the node spanning the
phrase, and the interior context, correspondingly
defined via features over the inside trees. We re-
fer to these tensor structures collectively as Cr for
rules r E R, which encompass the parameters t.
For r E R0 : Cr E Rmx1; similarly for
r E R1 : Cr E Rmxm and r E R2 : Cr E
Rmxmxm. We also maintain a vector CS E R1xm
corresponding to the parameters 7r(S, h) for the
</bodyText>
<page confidence="0.886875">
1954
</page>
<equation confidence="0.6585966">
Inputs: Sentence f1 ... fw, L-SCFG (N, S, m, n), param-
eters Cr ∈ R(mxmxm), ∈ R(mxm), or ∈ R(mx1) for all
r ∈ R, Cs ∈ R(1xm), hypergraph H.
Data structures:
For each node q ∈ H:
</equation>
<listItem confidence="0.973681">
• α(q) ∈ Rmx1 is a column vector of inside terms.
• β(q) ∈ R1xm is a row vector of outside terms.
• For each incoming edge e ∈ B(q) to node q, µ(e) is a
marginal probability for edge (rule) e.
</listItem>
<subsectionHeader confidence="0.247105">
Algorithm:
</subsectionHeader>
<bodyText confidence="0.5286385">
� Inside Computation
For nodes q in topological order in H,
</bodyText>
<equation confidence="0.976539225806452">
α(q) = 0
For each incoming edge e ∈ B(q),
tail = t(e), rule = r(e)
if |tail |= 0, then α(q) = α(q) + Crule
else if |tail |= 1, then α(q) = α(q) +
Crule ×1 α(tail0)
else if |tail |= 2, then α(q) = α(q) +
Crule ×2 α(tail1) ×1 α(tail0)
� Outside Computation
For q ∈ H,
β(q) = 0
β(goal) = Cs
For q in reverse topological order in H,
For each incoming edge e ∈ B(q),
tail = t(e), rule = r(e)
if |tail |= 1, then
β(tail0) = β(tail0) + β(q) ×0 Crule
else if |tail |= 2, then
β(tail0) = β(tail0) +
β(q) ×0 Crule ×2 α(tail1)
β(tail1) = β(tail1) +
β(q) ×0 Crule ×1 α(tail0)
.Edge Marginals
Sentence probability g = α(goal) × β(goal)
For edge e ∈ H,
head = h(e), tail = t(e), rule = r(e)
if |tail |= 0, then µ(e) = (β(head) ×0 Crule)/g
else if |tail |= 1, then µ(e) = (β(head) ×0 Crule ×1
α(tail0))/g
else if |tail |= 2, then µ(e) = (β(head) ×0 Crule ×2
α(tail1) ×1 α(tail0))/g
</equation>
<figureCaption confidence="0.993743">
Figure 1: The tensor form of the hypergraph inside-
</figureCaption>
<bodyText confidence="0.861378857142857">
outside algorithm, for calculation of rule marginals µ(e). A
slight simplification in the marginal computation yields NT
marginals for spans µ(X, i, j). B(q) returns the incoming hy-
peredges for node q, and h(e), t(e), r(e) return the head node,
tail nodes, and rule for hyperedge e.
goal node (root). These parameters participate in
tensor-vector operations: a 3rd-order tensor C12
can be multiplied along each of its three modes
(x0, x1, x2), and if multiplied by an m x 1 vec-
tor, will produce an m x m matrix.1 Note that ma-
trix multiplication can be represented by x1 when
multiplying on the right and x0 when multiplying
on the left of the matrix. The decoder computes
marginal probabilities for each skeletal rule in the
</bodyText>
<footnote confidence="0.60035">
1This operation is sometimes called a contraction.
</footnote>
<bodyText confidence="0.999941">
parse forest of a source sentence by marginaliz-
ing over the latent states, which in practice corre-
sponds to simple tensor-vector products. This op-
eration is not dependent on the manner in which
the parameters were estimated.
Figure 1 presents the tensor version of the
inside-outside algorithm for decoding L-SCFGs.
The algorithm takes as input the parse forest of
the source sentence represented as a hypergraph
(Klein and Manning, 2001), which is computed
using a bottom-up parser with Earley-style rules
similar to the algorithm in Chiang (2007). Hyper-
graphs are a compact way to represent a forest of
multiple parse trees. Each node in the hypergraph
corresponds to an NT span, and can have multiple
incoming and outgoing hyperedges. Hyperedges,
which connect one or more tail nodes to a single
head node, correspond exactly to rules, and tail or
head nodes correspond to children (RHS NTs) or
parent (LHS NT). The function B(q) returns all in-
coming hyperedges to a node q, i.e., all rules such
that the LHS NT of the rule corresponds to the NT
span of the node q. The algorithm computes inside
and outside probabilities over the hypergraph us-
ing the tensor representations, and converts these
probabilities to marginal rule probabilities. It is
similar to the version presented in Cohen et al.
(2014), but adapted to hypergraph parse forests.
The complexity of this decoding algorithm is
O(n3m3|G|) where n is the length of the input
sentence, m is the number of latent states, and |G|
is the number of production rules in the grammar
without latent-variable annotations (i.e., m = 1).2
The bulk of the computation is a series of tensor-
vector products of relatively small size (each di-
mension is of length m), which can be computed
very quickly and in parallel. The tensor computa-
tions can be significantly sped up using techniques
described by Cohen and Collins (2012), so that
they are linear in m and not cubic.
</bodyText>
<sectionHeader confidence="0.97803" genericHeader="method">
3 Derivation Trees for Parallel Sentences
</sectionHeader>
<bodyText confidence="0.999463166666667">
To estimate the parameters t and π of an L-
SCFG (discussed in detail in the next section),
we assume the existence of a dataset composed
of synchronous s-trees, which can be acquired
from word alignments. Normally in phrase-based
translation models, we consider all possible phrase
</bodyText>
<footnote confidence="0.99984925">
2In practice, the term m3|G |can be replaced with a
smaller term, which separates the rules in G by the number of
NTs on the RHS. This idea relates to the notion of “effective
grammar size” which we discuss in §5.
</footnote>
<page confidence="0.996124">
1955
</page>
<bodyText confidence="0.999992326086957">
pairs consistent with the word alignments and es-
timate features based on surface statistics associ-
ated with the phrase pairs or rules. The weights of
these features are then learned using a discrimina-
tive training algorithm (Och, 2003; Chiang, 2012,
inter alia). In contrast, in this work we restrict
the number of possible synchronous derivations
for each sentence pair to just one; thus, derivation
forests do not have to be considered, making pa-
rameter estimation more tractable.3
To achieve this objective, for each sentence in
the training data we extract the minimal set of
synchronous rules consistent with the word align-
ments, as opposed to the composed set of rules
(Galley et al., 2006). Composed rules are ones that
can be formed from smaller rules in the grammar;
with these rules, there are multiple synchronous
trees consistent with the alignments for a given
sentence pair, and thus the total number of applica-
ble rules can be combinatorially larger than if we
just consider the set of rules that cannot be formed
from other rules, namely the minimal rules. The
rule types across all sentence pairs are combined
to form a minimal grammar.4 To extract a set of
minimal rules, we use the linear-time extraction
algorithm of Zhang et al. (2008). We give a rough
description of their method below, and refer the
reader to the original paper for additional details.
The algorithm returns a complete minimal
derivation tree for each word-aligned sentence
pair, and generalizes an approach for finding all
common intervals (pairs of phrases such that no
word pair in the alignment links a word inside
the phrase to a word outside the phrase) between
two permutations (Uno and Yagiura, 2000) to se-
quences with many-to-many alignment links be-
tween the two sides, as in word alignment. The
key idea is to encode all phrase pairs of a sen-
tence alignment in a tree of size proportional to
the source sentence length, which they call the
normalized decomposition tree. Each node cor-
responds to a phrase pair, with larger phrase spans
represented by higher nodes in the tree. Construct-
ing the tree is analogous to finding common in-
tervals in two permutations, a property that they
leverage to propose a linear-time algorithm for tree
</bodyText>
<footnote confidence="0.9773025">
3For future work, we will consider efficient algorithms for
parameter estimation over derivation forests, since there may
be multiple valid ways to explain the sentence pair via a syn-
chronous tree structure.
4Table 2 presents a comparison of grammar sizes for our
experiments (§5.1).
</footnote>
<bodyText confidence="0.999605625">
extraction. Converting the tree to a set of minimal
SCFG rules for the sentence pair is straightfor-
ward, by replacing nodes corresponding to spans
with lexical items or NTs in a bottom-up manner.5
By using minimal rules as a starting point
instead of the traditional heuristically-extracted
rules (Chiang, 2007) or arbitrary compositions of
minimal rules (Galley et al., 2006), we are also
able to explore the transition from minimal rules
to composed ones in a principled manner by en-
coding contextual information through the latent
states. Thus, a beneficial side effect of our re-
finement process is the creation of more context-
specific rules without increasing the overall size
of the baseline grammar, instead holding this in-
formation in our parameters C&apos;.
</bodyText>
<sectionHeader confidence="0.973008" genericHeader="method">
4 Parameter Estimation for L-SCFGs
</sectionHeader>
<bodyText confidence="0.999976192307693">
We explore two methods for estimating the param-
eters C&apos; of the model: a likelihood-maximization
approach based on EM (Dempster et al., 1977),
and a spectral approach based on the method of
moments (Hsu et al., 2009; Cohen et al., 2014),
where we identify a subspace using a singular
value decomposition (SVD) of the cross-product
feature space between inside and outside trees and
estimate parameters in this subspace.
Figure 2 presents a side-by-side comparison of
the two algorithms, which we discuss in this sec-
tion. In the spectral approach, we base our pa-
rameter estimates on low-rank representations of
moments of features, while EM explicitly maxi-
mizes a likelihood criterion. The parameter es-
timation algorithms are relatively similar, but in
lieu of sparse feature functions in the spectral case,
EM uses partial counts estimated with the current
set of parameters. The nature of EM allows it to
be susceptible to local optima, while the spectral
approach comes with guarantees on obtaining the
global optimum (Cohen et al., 2014). Lastly, com-
puting the SVD and estimating parameters in the
low-rank space is a one-shot operation, as opposed
to the iterative procedure of EM, and therefore is
much more computationally efficient.
</bodyText>
<subsectionHeader confidence="0.999116">
4.1 Estimation with Spectral Method
</subsectionHeader>
<bodyText confidence="0.860407166666667">
We generalize the parameter estimation algorithm
presented in Cohen et al. (2013) to the syn-
5We filtered rules with arity 3 and above (i.e., containing
more than 3 NTs on the RHS). While the L-SCFG formalism
is perfectly capable of handling such cases, it would have re-
sulted in higher order tensors for our parameter structures.
</bodyText>
<page confidence="0.975712">
1956
</page>
<sectionHeader confidence="0.302399" genericHeader="method">
Inputs:
</sectionHeader>
<bodyText confidence="0.285736571428571">
Training examples (r(i), t(i,1), t(i,2), t(i,3), o(i), b(i))
for i ∈ {1 ... M}, where r(i) is a context free rule;
t(i,1), t(i,2), and t(i,3) are inside trees; o(i) is an out-
side tree; and b(i) = 1 if the rule is at the root of tree,
0 otherwise. A function φ that maps inside trees t to
feature-vectors φ(t) ∈ Rd. A function ψ that maps
outside trees o to feature-vectors ψ(o) ∈ Rd0.
</bodyText>
<figure confidence="0.213149">
Algorithm:
. Step 0: Singular Value Decomposition
Inputs:
Training examples (r(i), t(i,1), t(i,2), t(i,3), o(i), b(i)) for i ∈
{1 ... M}, where r(i) is a context free rule; t(i,1), t(i,2), and
t(i,3) are inside trees; o(i) is an outside tree; b(i) = 1 if the rule
is at the root of tree, 0 otherwise; and MAX ITERATIONS.
Algorithm:
. Step 0: Parameter Initialization
For rule r ∈ R,
</figure>
<listItem confidence="0.998372666666667">
• if r ∈ R0: initialize ˆCr ∈ Rm×1
• if r ∈ R1: initialize ˆCrRm×m
• Compute the SVD of Eq. 1 to calculate matri- • if r ∈ R2: initialize ˆCrRm×m×m
</listItem>
<equation confidence="0.993284235294118">
ces Uˆ ∈ R(d×m) and Vˆ ∈ R(d0×m). Initialize ˆCS ∈ Rm×1
ˆCS0 = ˆCS
ˆCr0 = ˆCr,
For iteration t = 1, ... , MAX ITERATIONS,
. Step 1: Projection
Y (t) = U&gt;φ(t)
Z(o) = Σ−1V &gt;ψ(o)
. Step 2: Calculate Correlations
P
o∈Qr Z(o) if r ∈ R0
|Qr|
P
(o,t)∈Qr Z(o)⊗Y (t) if r ∈ R1
|Qr|
P(o,t2,t3)∈Qr Z(o)⊗Y (t2)⊗Y (t3)
if r ∈ R2
|Qr|
</equation>
<bodyText confidence="0.996416333333333">
Qr is the set of outside-inside tree triples for binary
rules, outside-inside tree pairs for unary rules, and
outside trees for pre-terminals.
</bodyText>
<listItem confidence="0.9916515">
. Step 3: Compute Final Parameters
• For all r ∈ R,
</listItem>
<equation confidence="0.874561">
ˆCr = count(r) × ˆEr
M
• For all r(i) ∈ {1, ... ,M} such that b(i) is 1,
ˆCS = ˆCS + Y (t(g1))
|Q 1
</equation>
<bodyText confidence="0.471924">
QS is the set of trees at the root.
</bodyText>
<listItem confidence="0.733266">
• Expectation Step:
. Estimate Y and Z
</listItem>
<bodyText confidence="0.200862">
Compute partial counts and total tree probabili-
ties g for all t and o using Fig. 1 and parameters
</bodyText>
<figure confidence="0.88074935">
ˆCr
t−1, ˆCS t−1.
. Calculate Correlations
. Update Parameters
For all r ∈ R, ˆCrt = ˆCrt−1 (D ˆEr
For all r(i) ∈ {1, ... , M} such that b(i) is 1,
ˆCSt = ˆCSt + (ˆCS t−1 O Y (r(i)))/g
QS is the set of trees at the root.
• Maximization Step
if r ∈ R0: ∀h1 :
if r ∈ R1: ∀h1, h2 : ˆCr(h1, h2) =
ˆCr(h1,h2)
P P ˆCr0 (h1,h2)
r0=r h2
if r ∈ R2: ∀h1, h2, h3 : ˆCr(h1, h2, h3) =
ˆCr(h1,h2,h3)
⎧
⎨⎪⎪ ⎪
⎪⎪⎪⎩
ˆEr =
Z(o)
g
if r ∈ R0
P
o,g∈Qr
Z(o)⊗Y (t)
g
if r ∈ R1
P
(o,t,g)∈Qr
⎧
⎨⎪⎪⎪⎪⎪
⎪⎪⎪⎪⎪⎩
ˆEr =
P
(o,t2,t3,g)∈Qr
Z(o)⊗Y (t2)⊗Y (t3)
g
if r ∈ R2
ˆCr(h1) = ˆCr(h1)
</figure>
<table confidence="0.8637366">
P P ˆCr0 (h1)
r0=r h1
P P ˆCr0(h1,h2,h3) ˆCr(h1) =
r0=r h2,h3
if LHS(r) = S: ∀h1 :
ˆCr(h1)
(a) The spectral learning algorithm for estimating pa- P P ˆCr0 (h1)
rameters of an L-SCFG. r0=r h1
(b) The EM-based algorithm for estimating parameters of an L-
SCFG.
</table>
<figureCaption confidence="0.9988045">
Figure 2: The two parameter estimation algorithms proposed for L-SCFGs; (a) method of moments; (b) expectation maxi-
mization. O is the element-wise multiplication operator.
</figureCaption>
<bodyText confidence="0.999992142857143">
chronous or bilingual case. The central concept
of the spectral parameter estimation algorithm is
to learn an m-dimensional representation of in-
side and outside trees by defining these trees in
terms of features, in combination with a projection
step (SVD), with the hope being that the lower-
dimensional space captures the syntactic and se-
mantic regularities among rules from the sparse
feature space. Every NT in an s-tree has an as-
sociated inside and outside tree; the inside tree
contains the entire sub-tree at and below the NT,
and the outside tree is everything else in the syn-
chronous s-tree except the inside tree. The inside
feature function φ maps the domain of inside tree
</bodyText>
<page confidence="0.981187">
1957
</page>
<bodyText confidence="0.997827533333334">
fragments to a d-dimensional Euclidean space,
and the outside feature function ψ maps the do-
main of outside tree fragments to a d0-dimensional
space. The specific features we used are discussed
in §5.2.
Let O be the set of all tuples of inside-outside
trees in our training corpus, whose size is equiva-
lent to the number of rule tokens (occurrences in
the corpus) M, and let φ(t) E Rdx1, ψ(o) E Rd0x1
be the inside and outside feature functions for in-
side tree t and outside tree o. By computing the
outer product ® between the inside and outside
feature vectors for each pair and aggregating, we
obtain the empirical inside-outside feature covari-
ance matrix:
</bodyText>
<equation confidence="0.980519333333333">
�
Qˆ = 1 φ(t) (ψ(o))T (1)
|O |(o,t)∈O
</equation>
<bodyText confidence="0.997940783783784">
If m is the desired latent space dimension, we
compute an m-rank truncated SVD of the empir-
ical covariance matrix Qˆ pz� UEVT, where U E
Rdxm and V E Rd0xm are the matrices containing
the left and right singular vectors, and E E Rmxm
is a diagonal matrix containing the m-largest sin-
gular values along its diagonal.
Figure 2a provides the remaining steps in the
algorithm. The M training examples are obtained
by considering all nodes in all of the synchronous
s-trees given as input. In step 1, for each inside
and outside tree, we project its high-dimensional
representation to the m-dimensional latent space.
Using the m-dimensional representations for in-
side and outside trees, in step 2 for each rule type r
we compute the covariance between the inside tree
vectors and the outside tree vector using the ten-
sor product, a generalized outer product to com-
pute covariances between more than two random
vectors. For binary rules, with two child inside
vectors and one outside vector, the result ˆEr is a
3-mode tensor; for unary rules, a regular matrix,
and for pre-terminal rules with no right-hand side
non-terminals, a vector. The final parameter es-
timate is then the associated tensor/matrix/vector,
scaled by the maximum likelihood estimate of the
rule r, as in step 3.
The corresponding theoretical guarantees from
Cohen et al. (2014) can also be generalized to
the synchronous case. Qˆ is an empirical esti-
mate of the true covariance matrix Q, and if Q
has rank m, then the marginals computed using
the spectrally-estimated parameters will converge
to the true marginals, with the sample complexity
for convergence inversely proportional to a poly-
nomial function of the mth largest singular value
of Q.
</bodyText>
<subsectionHeader confidence="0.997907">
4.2 Estimation with EM
</subsectionHeader>
<bodyText confidence="0.983093837209302">
A likelihood maximization approach can also be
used to learn the parameters of an L-SCFG. Pa-
rameters are initialized by sampling each param-
eter value ˆCr(h1, h2, h3) from the interval [0, 1]
uniformly at random.6 We first decode the train-
ing corpus using an existing set of parameters to
compute the inside and outside probability vectors
associated with NTs for every rule in each s-tree,
constrained to the tree structure of the training ex-
ample. These probabilities can be computed us-
ing the decoding algorithm in Figure 1 (where α
and β correspond to the inside and outside proba-
bilities respectively), except the parse forest con-
sists of a single tree only. These vectors repre-
sent partial counts over latent states. We then de-
fine functions Y and Z (analogous to the spectral
case) which map inside and outside tree instances
to m-dimensional vectors containing these partial
counts. In the spectral case, Y and Z are estimated
just once, while in the case of EM they have to be
re-estimated at each iteration.
The expectation step thus consists of comput-
ing the partial counts of inside and outside trees t
and o, i.e., recovering the functions Y and Z, and
updating parameters Cr by computing correla-
tions, which involves summing over partial counts
(across all occurrences of a rule in the corpus).
Each partial count’s contribution is divided by a
normalization factor g, which is the total probabil-
ity of the tree which t or o is part of. Note that
unlike the spectral case, there is a specific normal-
ization factor for each inside-outside tuple. Lastly,
the correlations are scaled by the existing parame-
ter estimates.
To obtain the next set of parameters, in the max-
imization step we normalize ˆCr for r E R such
that for every h1, r0=r,h2,h3 ˆCr0(h1,h2,h3) = 1
for r E R2, Er0=r,h2 ˆCr0(h1, h2) = 1 for r E R1,
and E ˆCr0(h2) = 1 for r E R0. We
r0=r,h2
also normalize the root rule parameters ˆCr where
LHS(r) = S. It is also possible to add sparse,
overlapping features to an EM-based estimation
</bodyText>
<footnote confidence="0.998467">
6In our experiments, we also tried the initialization
scheme described in Matsuzaki et al. (2005), but found that it
provided little benefit.
</footnote>
<page confidence="0.997144">
1958
</page>
<bodyText confidence="0.998914">
procedure (Berg-Kirkpatrick et al., 2010) and we
leave this extension for future work.
</bodyText>
<sectionHeader confidence="0.998632" genericHeader="method">
5 Experiments
</sectionHeader>
<bodyText confidence="0.999965482758621">
The goal of the experimental section is to evalu-
ate the performance of the latent-variable SCFG
in comparison to a baseline without any additional
NT annotations (MIN-GRAMMAR), and to com-
pare the performance of the two parameter esti-
mation algorithms. We also compare L-SCFGs to
a HIERO baseline (Chiang, 2007). The language
pair of evaluation is Chinese–English (ZH-EN).
We score translations using BLEU (Papineni
et al., 2002). The latent-variable model is inte-
grated into the standard MT pipeline by comput-
ing marginal probabilities for each rule in the parse
forest of a source sentence using the algorithm in
Figure 1 with the parameters estimated through
the algorithms in Figure 2, and is added as a fea-
ture for the rule during MERT (Och, 2003). These
probabilities are conditioned on the LHS (X), and
are thus joint probabilities for a source-target RHS
pair. We also write out as features the condi-
tional relative frequencies Pˆ(e|f) and Pˆ(f|e) as
estimated by our latent-variable model, i.e., con-
ditioned on the source and target RHS.
Overall, we find that both the spectral and
the EM-based estimators improve upon a mini-
mal grammar baseline with only a single cate-
gory, but the spectral approach does better. In fact,
it matches the performance of the standard HI-
ERO baseline, despite learning on top of a minimal
grammar.
</bodyText>
<subsectionHeader confidence="0.994102">
5.1 Data and Baselines
</subsectionHeader>
<bodyText confidence="0.999138375">
The ZH-EN data is the BTEC parallel corpus
(Paul, 2009); we combine the first and second
development sets in one, and evaluate on the third
development set. The development and test sets
are evaluated with 16 references. Statistics for
the data are shown in Table 1. We used the CDEC
decoder (Dyer et al., 2010) to extract word align-
ments and the baseline hierarchical grammars,
MERT tuning, and decoding. We used a 4-gram
language model built from the target-side of the
parallel training data. The Python-based imple-
mentation of the tensor-based decoder, as well as
the parameter estimation algorithms is available at
github.com/asaluja/spectral-scfg/.
The baseline HIERO system uses a grammar ex-
tracted by applying the commonly used heuris-
</bodyText>
<equation confidence="0.946133285714286">
ZH-EN
TRAIN (SRC) 334K
TRAIN (TGT) 366K
DEV (SRC) 7K
DEV (TGT) 7.6K
TEST (SRC) 3.8K
TEST (TGT) 3.9K
</equation>
<tableCaption confidence="0.987704">
Table 1: Corpus statistics (in words). For the target DEV and
TEST statistics, we take the first reference.
</tableCaption>
<bodyText confidence="0.9997109">
tics (Chiang, 2007). Each rule is decorated with
two lexical and phrasal features corresponding to
the forward (e|f) and backward (f|e) conditional
log frequencies, along with the log joint frequency
(e, f), the log frequency of the source phrase (f),
and whether the phrase pair or the source phrase
is a singleton. Weights for the language model
(and language model OOV), glue rule, and word
penalty are also tuned. The MIN-GRAMMAR
baseline7 maintains the same set of weights.
</bodyText>
<table confidence="0.943039833333333">
Grammar Number of Rules
HIERO 1.69M
MIN-GRAMMAR 59K
LV m = 1 27.56K
LV m = 8 3.18M
LV m = 16 22.22M
</table>
<tableCaption confidence="0.998868">
Table 2: Grammar sizes for the different systems; for the
latent-variable models, effective grammar sizes are provided.
</tableCaption>
<bodyText confidence="0.993100692307692">
Grammar sizes are presented in Table 2. For
the latent-variable models, we provide the effec-
tive grammar size, where the number of NTs on
the RHS of a rule is taken into account when com-
puting the grammar size, by assuming each possi-
ble latent variable configuration amongst the NTs
generates a different rule. Furthermore, all single-
tons are mapped to the OOV rule, while we in-
clude singletons in MIN-GRAMMAR.8 Hence, ef-
fective grammar size can be computed as m(1 +
|R&apos;1
0 |) +m2|R1 |+m3|R2|, where R0&apos;1 is the set
of pre-terminal rules that occur more than once.
</bodyText>
<subsectionHeader confidence="0.997734">
5.2 Spectral Features
</subsectionHeader>
<bodyText confidence="0.9996085">
We use the following set of sparse, binary features
in the spectral learning process:
</bodyText>
<footnote confidence="0.99965575">
7Code to extract the minimal derivation trees is available
atwww.cs.rochester.edu/u/gildea/mt/.
8This OOV mapping is done so that the latent-variable
model can handle unknown tokens.
</footnote>
<page confidence="0.9956">
1959
</page>
<listItem confidence="0.767847">
• Rule Indicator. For the inside features, we con-
</listItem>
<bodyText confidence="0.908822666666667">
sider the rule production containing the current
non-terminal on the left-hand side, as well as
the rules of the children (distinguishing between
left and right children for binary rules). For
the outside features, we consider the parent rule
production along with the rule production of the
sibling (if it exists).
• Lexical. for both the inside and outside fea-
tures, any lexical items that appear in the rule
productions are recorded. Furthermore, we con-
sider the first and last words of spans (left and
right child spans for inside features, distinguish-
ing between the two if both exist, and sibling
span for outside features). Source and target
words are treated separately.
</bodyText>
<listItem confidence="0.623135">
• Length. the span length of the tree and each
of its children for inside features, and the span
length of the parent and sibling for outside fea-
tures.
</listItem>
<bodyText confidence="0.999802333333333">
In our experiments, we instantiated a total of
170,000 rule indicator features, 155,000 lexical
features, and 80 length features.
</bodyText>
<subsectionHeader confidence="0.99281">
5.3 Chinese–English Experiments
</subsectionHeader>
<bodyText confidence="0.999072576923077">
Table 3 presents a comprehensive evaluation of the
ZH-EN experimental setup. The first section con-
sists of the various baselines we consider. In ad-
dition to the aforementioned baselines, we eval-
uated a setup where the spectral parameters sim-
ply consist of the joint maximum likelihood esti-
mates of the rules. This baseline should perform
en par with MIN-GRAMMAR, which we see is the
case on the development set. The performance
on the test set is better though, primarily because
we also include the reverse log relative frequency
(f|e) computed from the latent-variable model as
an additional feature in MERT. Furthermore, in
line with previous work (Galley et al., 2006) which
compares minimal and composed rules, we find
that minimal grammars take a hit of more than 2.5
BLEU points on the development set, compared to
composed (HIERO) grammars. The m = 1 spec-
tral baseline with only rule indicator features per-
forms slightly better than the minimal grammar
baseline, since it overtly takes into account inside-
outside tree combination preferences in the param-
eters, but improvement is minimal with one latent
state naturally and the performance on the test set
is in line with the MLE baseline.
On top of the baselines, we looked at a number
</bodyText>
<table confidence="0.9996211875">
Setup BLEU
Dev Test
Baselines HIERO 46.08 55.31
MIN- 43.38 51.78
GRAMMAR 43.24 52.80
MLE
Spectral m = 1 RI 44.18 52.62
m = 8 RI 44.60 53.63
m = 16 RI 46.06 55.83
m=16 RI+Lex+Sm 46.08 55.22
m=16 RI+Lex+Len 45.70 55.29
m=24 RI+Lex 43.00 51.28
m=32 RI+Lex 43.06 52.16
EM m = 8 40.53 (0.2) 49.78 (0.5)
m = 16 42.85 (0.2) 52.93 (0.9)
m = 32 41.07 (0.4) 49.95 (0.7)
</table>
<tableCaption confidence="0.972081090909091">
Table 3: Results for the ZH-EN corpus, comparing across
the baselines and the two parameter estimation techniques.
RI, Lex, and Len correspond to the rule indicator, lexical,
and length features respectively, and Sm denotes smoothing.
For the EM experiments, we selected the best scoring iter-
ation by tuning weights for parameters obtained after 25 it-
erations and evaluating other parameters with these weights.
Results for EM are averaged over 5 starting points, with stan-
dard deviation given in parentheses. Spectral, EM, and MLE
performances compared to the MIN-GRAMMAR baseline are
statistically significant (p &lt; 0.01).
</tableCaption>
<bodyText confidence="0.99996">
of feature combinations and latent states for the
spectral and EM-estimated latent-variable models.
For the spectral models, we tuned MERT parame-
ters separately for each rank on a set of parameters
estimated from rule indicator features only; subse-
quent variations within a given rank, e.g., the ad-
dition of lexical or length features or smoothing,
were evaluated with the same set of rank-specific
weights from MERT. For EM, we ran parame-
ter estimation with 5 randomly initialized starting
points for 50 iterations; we tuned the MERT pa-
rameters with EM parameters obtained after 25th
iterations. Similar to the spectral experiments,
we fixed the MERT weight values and evaluated
BLEU performance with parameters after every 5
iterations and chose the iteration with the highest
score on the development set. The results are av-
eraged over the 5 initializations, with standard de-
viation in parentheses.
Firstly, we can see a clear dependence on rank,
with peak performance for the spectral and EM
models occurring at m = 16. In this instance, the
spectral model roughly matches the performance
of the HIERO baseline, but it only uses rules ex-
tracted from a minimal grammar, whose size is a
fraction of the HIERO grammar. The gains seem
to level off at this rank; additional ranks seem to
add noise to the parameters. Feature-wise, addi-
tional lexical and length features add little, prob-
</bodyText>
<page confidence="0.984765">
1960
</page>
<bodyText confidence="0.999969294117647">
ably because much of this information is encap-
sulated in the rule indicator features. For EM,
m = 16 outperforms the minimal grammar base-
line, but is not at the level of the spectral results.
All EM, spectral, and MLE results are statistically
significant (p &lt; 0.01) with respect to the MIN-
GRAMMAR baseline (Zhang et al., 2004), and the
improvement over the HIERO baseline achieved by
the m = 16 rule indicator configuration is also sta-
tistically significant.
The two estimation algorithms differ signifi-
cantly in their estimation time. Given a feature
covariance matrix, the spectral algorithm (SVD,
which was done with Matlab, and correlation com-
putation steps) for m = 16 took 7 minutes, while
the EM algorithm took 5 minutes for each iteration
with this rank.
</bodyText>
<subsectionHeader confidence="0.993905">
5.4 Analysis
</subsectionHeader>
<bodyText confidence="0.999994571428571">
Figure 3 presents a comparison of the non-
terminal span marginals for two sentences in the
development set. We visualize these differences
through a heat map of the CKY parse chart, where
the starting word of the span is on the rows, and
the span end index is on the columns. Each cell is
shaded to represent the marginal of that particular
non-terminal span, with higher likelihoods in blue
and lower likelihoods in red.
For the most part, marginals at the leaves (i.e.,
pre-terminal marginals) tend to score relatively
similarly across different setups. Higher up in the
chart, the latent SCFG marginals look quite dif-
ferent than the MLE parameters. Most noticeably,
spans starting at the beginning of the sentence are
much more favored. It is these rules that allow
the right translation to be preferred since the MLE
chooses not to place the object of the sentence in
the subject’s span. However, the spectral param-
eters seem to discriminate between these higher-
level rules better than EM, which scores spans
starting with the first word uniformly highly. An-
other interesting point is that the range of likeli-
hoods is much larger in the EM case compared to
the MLE and spectral variants. For the second sen-
tence (row), the 1-best hypothesis produced by all
systems are the same, but the heat map accentuates
the previous observation.
</bodyText>
<sectionHeader confidence="0.999892" genericHeader="method">
6 Related Work
</sectionHeader>
<bodyText confidence="0.999892462962963">
The goal of refining single-category HPBT gram-
mars or automatically learning the NT categories
in a grammar, instead of relying on noisy parser
outputs, has been explored from several different
angles in the MT literature. Blunsom et al. (2008)
present a Bayesian model for synchronous gram-
mar induction, and place an appropriate nonpara-
metric prior on the parameters. However, their
starting point is to estimate a synchronous gram-
mar with multiple categories from parallel data
(using the word alignments as a prior), while we
aim to refine a fixed grammar with additional la-
tent states. Furthermore, their estimation proce-
dure is extremely expensive and is restricted to
learning up to five NT categories, via a series of
mean-field approximations.
Another approach is to explicitly attach a real-
valued vector to each NT: Huang et al. (2010) use
an external source-language parser for this pur-
pose and score rules based on the similarity be-
tween a source sentence parse and the information
contained in this vector, which explicitly requires
the integration of a good-quality source-language
parser. The EM-based algorithm that we propose
here is similar to what they propose, except that we
need to handle tensor structures. Mylonakis and
Sima’an (2011) select among linguistically moti-
vated non-terminal labels with a cross-validated
version of EM. Although they consider a restricted
hypothesis space, they do marginalize over dif-
ferent derivations therefore their inside-outside al-
gorithm is O(n6). In the syntax-directed trans-
lation literature, there have been efforts to relax
or coarsen the hard labels provided by a syntactic
parser in an automatic manner to promote param-
eter sharing (Venugopal et al., 2009; Hanneman
and Lavie, 2013), which is the complement of our
aim in this paper.
The idea of automatically learned grammar re-
finements comes from the monolingual parsing lit-
erature, where phenomena like head lexicalization
can be modeled through latent variables. Mat-
suzaki et al. (2005) look at a likelihood-based
method to split the NT categories of a gram-
mar into a fixed number of sub-categories, while
Petrov et al. (2006) learn a variable number of
sub-categories per NT. The latter’s extension may
be useful for finding the optimal number of latent
states from the data in our case.
The question of whether we can incorporate ad-
ditional contextual information in minimal rule
grammars in MT via auxiliary models instead of
using longer, composed rules has been investi-
gated before as well. n-gram translation mod-
</bodyText>
<page confidence="0.980283">
1961
</page>
<note confidence="0.61148">
Span End Span End Span End
</note>
<figure confidence="0.995630728395062">
Span starting at wo rd :
0 1 2 3 4
0.00
0.1
0.3
0.4
0.6
0.7
0.9
1.0
Span starting
l n ( su m) at wo rd :
0 1 2 3 4
0.00
0.2
0.5
0.7
1.0
1.2
1.5
1.7
2.0
Span starting at wo rd :
l n ( su m)
0 1 2 3 4
0
4
8
9
1
2
3
5
6
7
l n ( su m)
Span starting at word :
l n ( su m)
I go away . I ’ll bring it . I ’ll bring it .
(a) MLE (b) Spectral m = 16 RI (c)EMm=16
Span End Span End Span End
0 1 2 3 4 5 6 7
0.00
0.2
0.5
0.7
1.0
1.2
1.5
1.7
2.0
Span starting
l n (s um ) at wo rd :
0 1 x 3 4 5 6 7
0.0
0.
0.
1.
1.
x.
x.
x.
3.
starting at wo rd :
l n ( su m)
Span
0 1 2 3 4 5 6 7
0.0
4.5
9.0
1.5
3.0
6.0
7.5
10.5
I ’d like a shampoo and style.
(d) MLE
I ’d like a shampoo and style.
(e) Spectral m = 16 RI
I ’d like a shampoo and style .
(f) EM m = 16
</figure>
<figureCaption confidence="0.956894666666667">
Figure 3: A comparison of the CKY charts containing marginal probabilities of non-terminal spans µ(X, i, j) for the MLE,
spectral m = 16 with rule indicator features, and EM m = 16, for the two Chinese sentences. Higher likelihoods are in blue,
lower likelihoods in red. The hypotheses produced by each setup are below the heat maps.
</figureCaption>
<bodyText confidence="0.999526052631579">
els (Mari˜no et al., 2006; Durrani et al., 2011)
seek to model long-distance dependencies and re-
orderings through n-grams. Similarly, Vaswani
et al. (2011) use a Markov model in the context
of tree-to-string translation, where the parameters
are smoothed with absolute discounting (Ney et
al., 1994), while in our instance we capture this
smoothing effect through low rank or latent states.
Feng and Cohn (2013) also utilize a Markov model
for MT, but learn the parameters through a more
sophisticated estimation technique that makes use
of Pitman-Yor hierarchical priors.
Hsu et al. (2009) presented one of the initial
efforts at spectral-based parameter estimation (us-
ing SVD) of observed moments for latent-variable
models, in the case of Hidden Markov models.
This idea was extended to L-PCFGs (Cohen et al.,
2014), and our approach can be seen as a bilingual
or synchronous generalization.
</bodyText>
<sectionHeader confidence="0.998772" genericHeader="conclusions">
7 Conclusion
</sectionHeader>
<bodyText confidence="0.999945533333333">
In this work, we presented an approach to re-
fine synchronous grammars used in MT by in-
ferring the latent categories for the single non-
terminal in our grammar rules, and proposed two
algorithms to estimate parameters for our latent-
variable model. By fixing the synchronous deriva-
tions of each parallel sentence in the training data,
it is possible to avoid many of the computational
issues associated with synchronous grammar in-
duction. Improvements over a minimal grammar
baseline and equivalent performance to a hierar-
chical phrase-based baseline are achieved by the
spectral approach. For future work, we will seek
to relax this consideration and jointly reason about
non-terminal categories and derivation structures.
</bodyText>
<sectionHeader confidence="0.996522" genericHeader="acknowledgments">
Acknowledgements
</sectionHeader>
<bodyText confidence="0.999822888888889">
The authors would like to thank Daniel Gildea
for sharing his code to extract minimal derivation
trees, Stefan Riezler for useful discussions, Bren-
dan O’Connor for the CKY visualization advice,
and the anonymous reviewers for their feedback.
This work was supported by a grant from eBay
Inc. (Saluja), the U. S. Army Research Laboratory
and the U. S. Army Research Office under con-
tract/grant number W911NF-10-1-0533 (Dyer).
</bodyText>
<page confidence="0.995527">
1962
</page>
<sectionHeader confidence="0.993821" genericHeader="references">
References
</sectionHeader>
<reference confidence="0.997635893129771">
Taylor Berg-Kirkpatrick, Alexandre Bouchard-Cˆot´e,
John DeNero, and Dan Klein. 2010. Painless un-
supervised learning with features. In Proceedings of
NAACL.
Phil Blunsom, Trevor Cohn, and Miles Osborne. 2008.
Bayesian Synchronous Grammar Induction. In Pro-
ceedings of NIPS.
David Chiang. 2007. Hierarchical phrase-based trans-
lation. Computational Linguistics, 33(2):201–228,
June.
David Chiang. 2012. Hope and Fear for Dis-
criminative Training of Statistical Translation Mod-
els. Journal of Machine Learning Research, pages
1159–1187.
Shay B. Cohen and Michael Collins. 2012. Tensor
decomposition for fast parsing with latent-variable
PCFGs. In Proceedings of NIPS.
Shay B. Cohen, Karl Stratos, Michael Collins, Dean P.
Foster, and Lyle Ungar. 2013. Experiments with
spectral learning of latent-variable PCFGs. In Pro-
ceedings of NAACL.
Shay B. Cohen, Karl Stratos, Michael Collins, Dean P.
Foster, and Lyle Ungar. 2014. Spectral learning
of latent-variable PCFGs: Algorithms and sample
complexity. Journal of Machine Learning Research.
Arthur P. Dempster, Nan M. Laird, and Donald B. Ru-
bin. 1977. Maximum likelihood from incomplete
data via the EM algorithm. Journal of the Royal Sta-
tistical Society, Series B, 39(1):1–38.
Nadir Durrani, Helmut Schmid, and Alexander Fraser.
2011. A joint sequence translation model with inte-
grated reordering. In Proceedings ofACL.
Chris Dyer, Adam Lopez, Juri Ganitkevitch, Johnathan
Weese, Ferhan Ture, Phil Blunsom, Hendra Seti-
awan, Vladimir Eidelman, and Philip Resnik. 2010.
cdec: A decoder, alignment, and learning framework
for finite-state and context-free translation models.
In Proceedings of ACL.
Yang Feng and Trevor Cohn. 2013. A Markov
model of machine translation using non-parametric
bayesian inference. In Proceedings ofACL.
Michel Galley, Mark Hopkins, Kevin Knight, and
Daniel Marcu. 2004. What’s in a translation rule?
In Proceedings of HLT-NAACL.
Michel Galley, Jonathan Graehl, Kevin Knight, Daniel
Marcu, Steve DeNeefe, Wei Wang, and Ignacio
Thayer. 2006. Scalable inference and training of
context-rich syntactic translation models. In Pro-
ceedings of ACL.
Jonathan Graehl, Kevin Knight, and Jonathan May.
2008. Training tree transducers. Computational
Linguistics, 34(3):391–427, September.
Greg Hanneman and Alon Lavie. 2013. Improving
syntax-augmented machine translation by coarsen-
ing the label set. In Proceedings of NAACL.
Daniel Hsu, Sham M. Kakade, and Tong Zhang. 2009.
A Spectral Algorithm for Learning Hidden Markov
Models. In Proceedings of COLT.
Liang Huang, Kevin Knight, and Aravind Joshi. 2006.
Statistical syntax-directed translation with extended
domain of locality. In Proceedings ofAMTA.
Zhongqiang Huang, Martin ˇCmejrek, and Bowen
Zhou. 2010. Soft syntactic constraints for hierar-
chical phrase-based translation using latent syntactic
distributions. In Proceedings of EMNLP.
Dan Klein and Christopher D. Manning. 2001. Parsing
and hypergraphs. In Proceedings of IWPT.
Philipp Koehn, Franz Josef Och, and Daniel Marcu.
2003. Statistical phrase-based translation. In Pro-
ceedings of NAACL.
Abby Levenberg, Chris Dyer, and Phil Blunsom. 2012.
A Bayesian model for learning SCFGs with discon-
tiguous rules. In Proceedings of EMNLP-CoNLL.
Percy Liang, Slav Petrov, Michael I. Jordan, and Dan
Klein. 2007. The infinite PCFG using hierarchical
dirichlet processes. In Proceedings of EMNLP.
Jos´e B. Mari˜no, Rafael E. Banchs, Josep M. Crego,
Adri`a de Gispert, Patrik Lambert, Jos´e A. R. Fonol-
losa, and Marta R. Costa-juss`a. 2006. N-gram-
based machine translation. Computational Linguis-
tics, 32(4):527–549, December.
Takuya Matsuzaki, Yusuke Miyao, and Jun’ichi Tsujii.
2005. Probabilistic CFG with latent annotations. In
Proceedings ofACL.
Markos Mylonakis and Khalil Sima’an. 2011. Learn-
ing hierarchical translation structure with linguistic
annotations. In Proceedings of ACL.
Hermann Ney, Ute Essen, and Reinhard Kneser.
1994. On Structuring Probabilistic Dependencies in
Stochastic Language Modelling. Computer Speech
and Language, 8:1–38.
Franz Josef Och. 2003. Minimum error rate training
in statistical machine translation. In Proceedings of
ACL.
Kishore Papineni, Salim Roukos, Todd Ward, and Wei-
Jing Zhu. 2002. BLEU: A method for automatic
evaluation of machine translation. In Proceedings
ofACL.
Michael Paul. 2009. Overview of the IWSLT 2009
evaluation campaign. In Proceedings of IWSLT.
Slav Petrov, Leon Barrett, Romain Thibaux, and Dan
Klein. 2006. Learning accurate, compact, and inter-
pretable tree annotation. In Proceedings of ACL.
1963
Takeaki Uno and Mutsunori Yagiura. 2000. Fast al-
gorithms to enumerate all common intervals of two
permutations. Algorithmica, 26(2):290–309.
Ashish Vaswani, Haitao Mi, Liang Huang, and David
Chiang. 2011. Rule Markov models for fast tree-to-
string translation. In Proceedings of ACL.
Ashish Venugopal, Andreas Zollmann, Noah A. Smith,
and Stephan Vogel. 2009. Preference grammars:
Softening syntactic constraints to improve statistical
machine translation. In Proceedings of NAACL.
Dekai Wu. 1997. Stochastic inversion transduction
grammars and bilingual parsing of parallel corpora.
Computational Linguistics, 23(3):377–403, Septem-
ber.
Ying Zhang, Stephan Vogel, and Alex Waibel. 2004.
Interpreting BLEU/NIST scores: How much im-
provement do we need to have a better system. In
In Proceedings LREC.
Hao Zhang, Daniel Gildea, and David Chiang. 2008.
Extracting synchronous grammar rules from word-
level alignments in linear time. In Proceedings of
COLING.
Andreas Zollmann and Ashish Venugopal. 2006. Syn-
tax augmented machine translation via chart pars-
ing. In Proceedings of the Workshop on Statistical
Machine Translation, StatMT ’06, pages 138–141.
Association for Computational Linguistics.
</reference>
<page confidence="0.996473">
1964
</page>
</variant>
</algorithm>
<algorithm name="ParsHed" version="110505">
<variant no="0" confidence="0.990806">
<title confidence="0.999791">Latent-Variable Synchronous CFGs for Hierarchical Translation</title>
<author confidence="0.999957">Saluja Dyer Shay B Cohen</author>
<affiliation confidence="0.999981">Carnegie Mellon University University of Edinburgh</affiliation>
<address confidence="0.999798">Pittsburgh, PA, 15213, USA Edinburgh EH8 9AB, UK</address>
<email confidence="0.997687">scohen@inf.ed.ac.uk</email>
<abstract confidence="0.999633526315789">Data-driven refinement of non-terminal categories has been demonstrated to be a reliable technique for improving monolingual parsing with PCFGs. In this paper, we extend these techniques to learn latent refinements of single-category synchronous grammars, so as to improve translation performance. We compare two estimators for this latent-variable model: one based on EM and the other is a spectral algorithm based on the method of moments. We evaluate their performance on a Chinese–English translation task. The results indicate that we can achieve significant gains over the baseline with both approaches, but in particular the momentsbased estimator is both faster and performs better than EM.</abstract>
</variant>
</algorithm>
<algorithm name="ParsCit" version="110505">
<citationList>
<citation valid="true">
<authors>
<author>Taylor Berg-Kirkpatrick</author>
<author>Alexandre Bouchard-Cˆot´e</author>
<author>John DeNero</author>
<author>Dan Klein</author>
</authors>
<title>Painless unsupervised learning with features.</title>
<date>2010</date>
<booktitle>In Proceedings of NAACL.</booktitle>
<marker>Berg-Kirkpatrick, Bouchard-Cˆot´e, DeNero, Klein, 2010</marker>
<rawString>Taylor Berg-Kirkpatrick, Alexandre Bouchard-Cˆot´e, John DeNero, and Dan Klein. 2010. Painless unsupervised learning with features. In Proceedings of NAACL.</rawString>
</citation>
<citation valid="true">
<authors>
<author>Phil Blunsom</author>
<author>Trevor Cohn</author>
<author>Miles Osborne</author>
</authors>
<title>Bayesian Synchronous Grammar Induction.</title>
<date>2008</date>
<booktitle>In Proceedings of NIPS.</booktitle>
<contexts>
<context position="1696" citStr="Blunsom et al., 2008" startWordPosition="251" endWordPosition="254">s a context-free parsing problem. A parser constructs trees over the input sentence by parsing with the source language projection of a synchronous CFG, and each derivation induces translations in the target language (Chiang, 2007). However, in contrast to syntactic parsing, where linguistic intuitions can help elucidate the “right” tree structure for a grammatical sentence, no such intuitions are available for synchronous derivations, and so learning the “right” grammars is a central challenge. Of course, learning synchronous grammars from parallel data is a widely studied problem (Wu, 1997; Blunsom et al., 2008; Levenberg et al., 2012, inter alia). However, there has been less exploration of learning rich non-terminal categories, largely because previous efforts to learn such categories have been coupled with efforts to learn derivation structures—a computationally formidable challenge. One popular approach has been to derive categories from source and/or target monolingual grammars (Galley et al., 2004; Zollmann and Venugopal, 2006; Hanneman and Lavie, 2013). While often successful, accurate parsers are not available in many languages: a more appealing approach is therefore to learn the category st</context>
<context position="36724" citStr="Blunsom et al. (2008)" startWordPosition="6279" endWordPosition="6282">s better than EM, which scores spans starting with the first word uniformly highly. Another interesting point is that the range of likelihoods is much larger in the EM case compared to the MLE and spectral variants. For the second sentence (row), the 1-best hypothesis produced by all systems are the same, but the heat map accentuates the previous observation. 6 Related Work The goal of refining single-category HPBT grammars or automatically learning the NT categories in a grammar, instead of relying on noisy parser outputs, has been explored from several different angles in the MT literature. Blunsom et al. (2008) present a Bayesian model for synchronous grammar induction, and place an appropriate nonparametric prior on the parameters. However, their starting point is to estimate a synchronous grammar with multiple categories from parallel data (using the word alignments as a prior), while we aim to refine a fixed grammar with additional latent states. Furthermore, their estimation procedure is extremely expensive and is restricted to learning up to five NT categories, via a series of mean-field approximations. Another approach is to explicitly attach a realvalued vector to each NT: Huang et al. (2010)</context>
</contexts>
<marker>Blunsom, Cohn, Osborne, 2008</marker>
<rawString>Phil Blunsom, Trevor Cohn, and Miles Osborne. 2008. Bayesian Synchronous Grammar Induction. In Proceedings of NIPS.</rawString>
</citation>
<citation valid="true">
<authors>
<author>David Chiang</author>
</authors>
<title>Hierarchical phrase-based translation.</title>
<date>2007</date>
<journal>Computational Linguistics,</journal>
<volume>33</volume>
<issue>2</issue>
<contexts>
<context position="1307" citStr="Chiang, 2007" startWordPosition="195" endWordPosition="196"> moments. We evaluate their performance on a Chinese–English translation task. The results indicate that we can achieve significant gains over the baseline with both approaches, but in particular the momentsbased estimator is both faster and performs better than EM. 1 Introduction Translation models based on synchronous contextfree grammars (SCFGs) treat the translation problem as a context-free parsing problem. A parser constructs trees over the input sentence by parsing with the source language projection of a synchronous CFG, and each derivation induces translations in the target language (Chiang, 2007). However, in contrast to syntactic parsing, where linguistic intuitions can help elucidate the “right” tree structure for a grammatical sentence, no such intuitions are available for synchronous derivations, and so learning the “right” grammars is a central challenge. Of course, learning synchronous grammars from parallel data is a widely studied problem (Wu, 1997; Blunsom et al., 2008; Levenberg et al., 2012, inter alia). However, there has been less exploration of learning rich non-terminal categories, largely because previous efforts to learn such categories have been coupled with efforts </context>
<context position="4755" citStr="Chiang, 2007" startWordPosition="710" endWordPosition="711"> latentvariable models which leverage information contained in the synchronous tree structure, instead of relying on a heuristic set of features based on empirical relative frequencies (Koehn et al., 2003) from non-hierarchical phrase-based translation. 2 Latent-Variable SCFGs Before discussing parameter learning, we introduce latent-variable synchronous context-free grammars (L-SCFGs) and discuss an inference algorithm for marginalizing over latent states. We extend the definition of L-PCFGs (Matsuzaki et al., 2005; Petrov et al., 2006) to synchronous grammars as used in machine translation (Chiang, 2007). A latent-variable SCFG (LSCFG) is a 6-tuple (N, m, ns, nt, 7r, t) where: • N is a set of non-terminal (NT) symbols in the grammar. For hierarchical phrase-based translation (HPBT), the set consists of only two symbols, X and a goal symbol S. • [m] is the set of possible hidden states associated with NTs. Aligned pairs of NTs across the source and target languages share the same hidden state. • [ns] is the set of source side words, i.e., the source-side vocabulary, with [ns] n N = 0. • [nt] is the set of target side words, i.e., the target-side vocabulary, with [nt] n N = 0. • The synchronous</context>
<context position="6565" citStr="Chiang (2007)" startWordPosition="1061" endWordPosition="1062"> E [m], 7r(a, h) is a parameter specifying the root probability of a(h). A skeletal tree (s-tree) for a sentence is the set of rules in the synchronous derivation of that sentence, without any additional latent state information or decoration. A full tree consists of an stree r1, ... , rN together with values h1, ... , hN for every NT in the tree. An important point to keep in mind in comparison to L-PCFGs is that the right-hand side (RHS) non-terminals of synchronous rules are aligned pairs across the source and target languages. In this work, we refine the one-category grammar introduced by Chiang (2007) for HPBT in order to learn additional latent NT categories. Thus, the following discussion is restricted to these kinds of grammars, although the method is equally applicable in other scenarios, e.g., the extended treeto-string transducer (xRs) formalism (Huang et al., 2006; Graehl et al., 2008) commonly used in syntax-directed translation, and phrase-based MT (Koehn et al., 2003). Marginal Inference with L-SCFGs. For a parameter t of rule r, the latent state h1 attached to the left-hand side (LHS) NT of r is associated with the outside tree for the sub-tree rooted at the LHS, and the states </context>
<context position="10904" citStr="Chiang (2007)" startWordPosition="1857" endWordPosition="1858">l rule in the 1This operation is sometimes called a contraction. parse forest of a source sentence by marginalizing over the latent states, which in practice corresponds to simple tensor-vector products. This operation is not dependent on the manner in which the parameters were estimated. Figure 1 presents the tensor version of the inside-outside algorithm for decoding L-SCFGs. The algorithm takes as input the parse forest of the source sentence represented as a hypergraph (Klein and Manning, 2001), which is computed using a bottom-up parser with Earley-style rules similar to the algorithm in Chiang (2007). Hypergraphs are a compact way to represent a forest of multiple parse trees. Each node in the hypergraph corresponds to an NT span, and can have multiple incoming and outgoing hyperedges. Hyperedges, which connect one or more tail nodes to a single head node, correspond exactly to rules, and tail or head nodes correspond to children (RHS NTs) or parent (LHS NT). The function B(q) returns all incoming hyperedges to a node q, i.e., all rules such that the LHS NT of the rule corresponds to the NT span of the node q. The algorithm computes inside and outside probabilities over the hypergraph usi</context>
<context position="15648" citStr="Chiang, 2007" startWordPosition="2656" endWordPosition="2657">lgorithm for tree 3For future work, we will consider efficient algorithms for parameter estimation over derivation forests, since there may be multiple valid ways to explain the sentence pair via a synchronous tree structure. 4Table 2 presents a comparison of grammar sizes for our experiments (§5.1). extraction. Converting the tree to a set of minimal SCFG rules for the sentence pair is straightforward, by replacing nodes corresponding to spans with lexical items or NTs in a bottom-up manner.5 By using minimal rules as a starting point instead of the traditional heuristically-extracted rules (Chiang, 2007) or arbitrary compositions of minimal rules (Galley et al., 2006), we are also able to explore the transition from minimal rules to composed ones in a principled manner by encoding contextual information through the latent states. Thus, a beneficial side effect of our refinement process is the creation of more contextspecific rules without increasing the overall size of the baseline grammar, instead holding this information in our parameters C&apos;. 4 Parameter Estimation for L-SCFGs We explore two methods for estimating the parameters C&apos; of the model: a likelihood-maximization approach based on E</context>
<context position="26131" citStr="Chiang, 2007" startWordPosition="4516" endWordPosition="4517">sparse, overlapping features to an EM-based estimation 6In our experiments, we also tried the initialization scheme described in Matsuzaki et al. (2005), but found that it provided little benefit. 1958 procedure (Berg-Kirkpatrick et al., 2010) and we leave this extension for future work. 5 Experiments The goal of the experimental section is to evaluate the performance of the latent-variable SCFG in comparison to a baseline without any additional NT annotations (MIN-GRAMMAR), and to compare the performance of the two parameter estimation algorithms. We also compare L-SCFGs to a HIERO baseline (Chiang, 2007). The language pair of evaluation is Chinese–English (ZH-EN). We score translations using BLEU (Papineni et al., 2002). The latent-variable model is integrated into the standard MT pipeline by computing marginal probabilities for each rule in the parse forest of a source sentence using the algorithm in Figure 1 with the parameters estimated through the algorithms in Figure 2, and is added as a feature for the rule during MERT (Och, 2003). These probabilities are conditioned on the LHS (X), and are thus joint probabilities for a source-target RHS pair. We also write out as features the conditio</context>
<context position="28149" citStr="Chiang, 2007" startWordPosition="4849" endWordPosition="4850">hierarchical grammars, MERT tuning, and decoding. We used a 4-gram language model built from the target-side of the parallel training data. The Python-based implementation of the tensor-based decoder, as well as the parameter estimation algorithms is available at github.com/asaluja/spectral-scfg/. The baseline HIERO system uses a grammar extracted by applying the commonly used heurisZH-EN TRAIN (SRC) 334K TRAIN (TGT) 366K DEV (SRC) 7K DEV (TGT) 7.6K TEST (SRC) 3.8K TEST (TGT) 3.9K Table 1: Corpus statistics (in words). For the target DEV and TEST statistics, we take the first reference. tics (Chiang, 2007). Each rule is decorated with two lexical and phrasal features corresponding to the forward (e|f) and backward (f|e) conditional log frequencies, along with the log joint frequency (e, f), the log frequency of the source phrase (f), and whether the phrase pair or the source phrase is a singleton. Weights for the language model (and language model OOV), glue rule, and word penalty are also tuned. The MIN-GRAMMAR baseline7 maintains the same set of weights. Grammar Number of Rules HIERO 1.69M MIN-GRAMMAR 59K LV m = 1 27.56K LV m = 8 3.18M LV m = 16 22.22M Table 2: Grammar sizes for the different</context>
</contexts>
<marker>Chiang, 2007</marker>
<rawString>David Chiang. 2007. Hierarchical phrase-based translation. Computational Linguistics, 33(2):201–228, June.</rawString>
</citation>
<citation valid="true">
<authors>
<author>David Chiang</author>
</authors>
<title>Hope and Fear for Discriminative Training of Statistical Translation Models.</title>
<date>2012</date>
<journal>Journal of Machine Learning Research,</journal>
<pages>1159--1187</pages>
<contexts>
<context position="13066" citStr="Chiang, 2012" startWordPosition="2228" endWordPosition="2229">of synchronous s-trees, which can be acquired from word alignments. Normally in phrase-based translation models, we consider all possible phrase 2In practice, the term m3|G |can be replaced with a smaller term, which separates the rules in G by the number of NTs on the RHS. This idea relates to the notion of “effective grammar size” which we discuss in §5. 1955 pairs consistent with the word alignments and estimate features based on surface statistics associated with the phrase pairs or rules. The weights of these features are then learned using a discriminative training algorithm (Och, 2003; Chiang, 2012, inter alia). In contrast, in this work we restrict the number of possible synchronous derivations for each sentence pair to just one; thus, derivation forests do not have to be considered, making parameter estimation more tractable.3 To achieve this objective, for each sentence in the training data we extract the minimal set of synchronous rules consistent with the word alignments, as opposed to the composed set of rules (Galley et al., 2006). Composed rules are ones that can be formed from smaller rules in the grammar; with these rules, there are multiple synchronous trees consistent with t</context>
</contexts>
<marker>Chiang, 2012</marker>
<rawString>David Chiang. 2012. Hope and Fear for Discriminative Training of Statistical Translation Models. Journal of Machine Learning Research, pages 1159–1187.</rawString>
</citation>
<citation valid="true">
<authors>
<author>Shay B Cohen</author>
<author>Michael Collins</author>
</authors>
<title>Tensor decomposition for fast parsing with latent-variable PCFGs.</title>
<date>2012</date>
<booktitle>In Proceedings of NIPS.</booktitle>
<contexts>
<context position="12230" citStr="Cohen and Collins (2012)" startWordPosition="2082" endWordPosition="2085">similar to the version presented in Cohen et al. (2014), but adapted to hypergraph parse forests. The complexity of this decoding algorithm is O(n3m3|G|) where n is the length of the input sentence, m is the number of latent states, and |G| is the number of production rules in the grammar without latent-variable annotations (i.e., m = 1).2 The bulk of the computation is a series of tensorvector products of relatively small size (each dimension is of length m), which can be computed very quickly and in parallel. The tensor computations can be significantly sped up using techniques described by Cohen and Collins (2012), so that they are linear in m and not cubic. 3 Derivation Trees for Parallel Sentences To estimate the parameters t and π of an LSCFG (discussed in detail in the next section), we assume the existence of a dataset composed of synchronous s-trees, which can be acquired from word alignments. Normally in phrase-based translation models, we consider all possible phrase 2In practice, the term m3|G |can be replaced with a smaller term, which separates the rules in G by the number of NTs on the RHS. This idea relates to the notion of “effective grammar size” which we discuss in §5. 1955 pairs consis</context>
</contexts>
<marker>Cohen, Collins, 2012</marker>
<rawString>Shay B. Cohen and Michael Collins. 2012. Tensor decomposition for fast parsing with latent-variable PCFGs. In Proceedings of NIPS.</rawString>
</citation>
<citation valid="true">
<authors>
<author>Shay B Cohen</author>
<author>Karl Stratos</author>
<author>Michael Collins</author>
<author>Dean P Foster</author>
<author>Lyle Ungar</author>
</authors>
<title>Experiments with spectral learning of latent-variable PCFGs.</title>
<date>2013</date>
<booktitle>In Proceedings of NAACL.</booktitle>
<contexts>
<context position="17489" citStr="Cohen et al. (2013)" startWordPosition="2948" endWordPosition="2951">milar, but in lieu of sparse feature functions in the spectral case, EM uses partial counts estimated with the current set of parameters. The nature of EM allows it to be susceptible to local optima, while the spectral approach comes with guarantees on obtaining the global optimum (Cohen et al., 2014). Lastly, computing the SVD and estimating parameters in the low-rank space is a one-shot operation, as opposed to the iterative procedure of EM, and therefore is much more computationally efficient. 4.1 Estimation with Spectral Method We generalize the parameter estimation algorithm presented in Cohen et al. (2013) to the syn5We filtered rules with arity 3 and above (i.e., containing more than 3 NTs on the RHS). While the L-SCFG formalism is perfectly capable of handling such cases, it would have resulted in higher order tensors for our parameter structures. 1956 Inputs: Training examples (r(i), t(i,1), t(i,2), t(i,3), o(i), b(i)) for i ∈ {1 ... M}, where r(i) is a context free rule; t(i,1), t(i,2), and t(i,3) are inside trees; o(i) is an outside tree; and b(i) = 1 if the rule is at the root of tree, 0 otherwise. A function φ that maps inside trees t to feature-vectors φ(t) ∈ Rd. A function ψ that maps </context>
</contexts>
<marker>Cohen, Stratos, Collins, Foster, Ungar, 2013</marker>
<rawString>Shay B. Cohen, Karl Stratos, Michael Collins, Dean P. Foster, and Lyle Ungar. 2013. Experiments with spectral learning of latent-variable PCFGs. In Proceedings of NAACL.</rawString>
</citation>
<citation valid="true">
<authors>
<author>Shay B Cohen</author>
<author>Karl Stratos</author>
<author>Michael Collins</author>
<author>Dean P Foster</author>
<author>Lyle Ungar</author>
</authors>
<title>Spectral learning of latent-variable PCFGs: Algorithms and sample complexity.</title>
<date>2014</date>
<journal>Journal of Machine Learning Research.</journal>
<contexts>
<context position="2841" citStr="Cohen et al., 2014" startWordPosition="425" endWordPosition="428">languages: a more appealing approach is therefore to learn the category structure from the data itself. In this work, we take a different approach to previous work in synchronous grammar induction by assuming that reasonable tree structures for a parallel corpus can be chosen heuristically, and then, fixing the trees (thereby enabling us to sidestep the worst of the computational issues), we learn non-terminal categories as latent variables to explain the distribution of these synchronous trees. This technique has a long history in monolingual parsing (Petrov et al., 2006; Liang et al., 2007; Cohen et al., 2014), where it reliably yields stateof-the-art phrase structure parsers based on generative models, but we are the first to apply it to translation. We first generalize the concept of latent PCFGs to latent-variable SCFGs (§2). We then follow by a presentation of the tensor-based formulation for our parameters, a representation that makes it convenient to marginalize over latent states. Subsequently, two methods for parameter estimation are presented (§4): a spectral approach based on the method of moments, and an EM-based likelihood maximization. Results on a Chinese–English evaluation set (§5) i</context>
<context position="11661" citStr="Cohen et al. (2014)" startWordPosition="1985" endWordPosition="1988">can have multiple incoming and outgoing hyperedges. Hyperedges, which connect one or more tail nodes to a single head node, correspond exactly to rules, and tail or head nodes correspond to children (RHS NTs) or parent (LHS NT). The function B(q) returns all incoming hyperedges to a node q, i.e., all rules such that the LHS NT of the rule corresponds to the NT span of the node q. The algorithm computes inside and outside probabilities over the hypergraph using the tensor representations, and converts these probabilities to marginal rule probabilities. It is similar to the version presented in Cohen et al. (2014), but adapted to hypergraph parse forests. The complexity of this decoding algorithm is O(n3m3|G|) where n is the length of the input sentence, m is the number of latent states, and |G| is the number of production rules in the grammar without latent-variable annotations (i.e., m = 1).2 The bulk of the computation is a series of tensorvector products of relatively small size (each dimension is of length m), which can be computed very quickly and in parallel. The tensor computations can be significantly sped up using techniques described by Cohen and Collins (2012), so that they are linear in m </context>
<context position="16368" citStr="Cohen et al., 2014" startWordPosition="2773" endWordPosition="2776">ansition from minimal rules to composed ones in a principled manner by encoding contextual information through the latent states. Thus, a beneficial side effect of our refinement process is the creation of more contextspecific rules without increasing the overall size of the baseline grammar, instead holding this information in our parameters C&apos;. 4 Parameter Estimation for L-SCFGs We explore two methods for estimating the parameters C&apos; of the model: a likelihood-maximization approach based on EM (Dempster et al., 1977), and a spectral approach based on the method of moments (Hsu et al., 2009; Cohen et al., 2014), where we identify a subspace using a singular value decomposition (SVD) of the cross-product feature space between inside and outside trees and estimate parameters in this subspace. Figure 2 presents a side-by-side comparison of the two algorithms, which we discuss in this section. In the spectral approach, we base our parameter estimates on low-rank representations of moments of features, while EM explicitly maximizes a likelihood criterion. The parameter estimation algorithms are relatively similar, but in lieu of sparse feature functions in the spectral case, EM uses partial counts estima</context>
<context position="23169" citStr="Cohen et al. (2014)" startWordPosition="4007" endWordPosition="4010"> the covariance between the inside tree vectors and the outside tree vector using the tensor product, a generalized outer product to compute covariances between more than two random vectors. For binary rules, with two child inside vectors and one outside vector, the result ˆEr is a 3-mode tensor; for unary rules, a regular matrix, and for pre-terminal rules with no right-hand side non-terminals, a vector. The final parameter estimate is then the associated tensor/matrix/vector, scaled by the maximum likelihood estimate of the rule r, as in step 3. The corresponding theoretical guarantees from Cohen et al. (2014) can also be generalized to the synchronous case. Qˆ is an empirical estimate of the true covariance matrix Q, and if Q has rank m, then the marginals computed using the spectrally-estimated parameters will converge to the true marginals, with the sample complexity for convergence inversely proportional to a polynomial function of the mth largest singular value of Q. 4.2 Estimation with EM A likelihood maximization approach can also be used to learn the parameters of an L-SCFG. Parameters are initialized by sampling each parameter value ˆCr(h1, h2, h3) from the interval [0, 1] uniformly at ran</context>
<context position="40940" citStr="Cohen et al., 2014" startWordPosition="7058" endWordPosition="7061">o-string translation, where the parameters are smoothed with absolute discounting (Ney et al., 1994), while in our instance we capture this smoothing effect through low rank or latent states. Feng and Cohn (2013) also utilize a Markov model for MT, but learn the parameters through a more sophisticated estimation technique that makes use of Pitman-Yor hierarchical priors. Hsu et al. (2009) presented one of the initial efforts at spectral-based parameter estimation (using SVD) of observed moments for latent-variable models, in the case of Hidden Markov models. This idea was extended to L-PCFGs (Cohen et al., 2014), and our approach can be seen as a bilingual or synchronous generalization. 7 Conclusion In this work, we presented an approach to refine synchronous grammars used in MT by inferring the latent categories for the single nonterminal in our grammar rules, and proposed two algorithms to estimate parameters for our latentvariable model. By fixing the synchronous derivations of each parallel sentence in the training data, it is possible to avoid many of the computational issues associated with synchronous grammar induction. Improvements over a minimal grammar baseline and equivalent performance to</context>
</contexts>
<marker>Cohen, Stratos, Collins, Foster, Ungar, 2014</marker>
<rawString>Shay B. Cohen, Karl Stratos, Michael Collins, Dean P. Foster, and Lyle Ungar. 2014. Spectral learning of latent-variable PCFGs: Algorithms and sample complexity. Journal of Machine Learning Research.</rawString>
</citation>
<citation valid="true">
<authors>
<author>Arthur P Dempster</author>
<author>Nan M Laird</author>
<author>Donald B Rubin</author>
</authors>
<title>Maximum likelihood from incomplete data via the EM algorithm.</title>
<date>1977</date>
<journal>Journal of the Royal Statistical Society, Series B,</journal>
<volume>39</volume>
<issue>1</issue>
<contexts>
<context position="16273" citStr="Dempster et al., 1977" startWordPosition="2755" endWordPosition="2758"> arbitrary compositions of minimal rules (Galley et al., 2006), we are also able to explore the transition from minimal rules to composed ones in a principled manner by encoding contextual information through the latent states. Thus, a beneficial side effect of our refinement process is the creation of more contextspecific rules without increasing the overall size of the baseline grammar, instead holding this information in our parameters C&apos;. 4 Parameter Estimation for L-SCFGs We explore two methods for estimating the parameters C&apos; of the model: a likelihood-maximization approach based on EM (Dempster et al., 1977), and a spectral approach based on the method of moments (Hsu et al., 2009; Cohen et al., 2014), where we identify a subspace using a singular value decomposition (SVD) of the cross-product feature space between inside and outside trees and estimate parameters in this subspace. Figure 2 presents a side-by-side comparison of the two algorithms, which we discuss in this section. In the spectral approach, we base our parameter estimates on low-rank representations of moments of features, while EM explicitly maximizes a likelihood criterion. The parameter estimation algorithms are relatively simil</context>
</contexts>
<marker>Dempster, Laird, Rubin, 1977</marker>
<rawString>Arthur P. Dempster, Nan M. Laird, and Donald B. Rubin. 1977. Maximum likelihood from incomplete data via the EM algorithm. Journal of the Royal Statistical Society, Series B, 39(1):1–38.</rawString>
</citation>
<citation valid="true">
<authors>
<author>Nadir Durrani</author>
<author>Helmut Schmid</author>
<author>Alexander Fraser</author>
</authors>
<title>A joint sequence translation model with integrated reordering.</title>
<date>2011</date>
<booktitle>In Proceedings ofACL.</booktitle>
<contexts>
<context position="40170" citStr="Durrani et al., 2011" startWordPosition="6938" endWordPosition="6941">. x. x. x. 3. starting at wo rd : l n ( su m) Span 0 1 2 3 4 5 6 7 0.0 4.5 9.0 1.5 3.0 6.0 7.5 10.5 I ’d like a shampoo and style. (d) MLE I ’d like a shampoo and style. (e) Spectral m = 16 RI I ’d like a shampoo and style . (f) EM m = 16 Figure 3: A comparison of the CKY charts containing marginal probabilities of non-terminal spans µ(X, i, j) for the MLE, spectral m = 16 with rule indicator features, and EM m = 16, for the two Chinese sentences. Higher likelihoods are in blue, lower likelihoods in red. The hypotheses produced by each setup are below the heat maps. els (Mari˜no et al., 2006; Durrani et al., 2011) seek to model long-distance dependencies and reorderings through n-grams. Similarly, Vaswani et al. (2011) use a Markov model in the context of tree-to-string translation, where the parameters are smoothed with absolute discounting (Ney et al., 1994), while in our instance we capture this smoothing effect through low rank or latent states. Feng and Cohn (2013) also utilize a Markov model for MT, but learn the parameters through a more sophisticated estimation technique that makes use of Pitman-Yor hierarchical priors. Hsu et al. (2009) presented one of the initial efforts at spectral-based pa</context>
</contexts>
<marker>Durrani, Schmid, Fraser, 2011</marker>
<rawString>Nadir Durrani, Helmut Schmid, and Alexander Fraser. 2011. A joint sequence translation model with integrated reordering. In Proceedings ofACL.</rawString>
</citation>
<citation valid="true">
<authors>
<author>Chris Dyer</author>
<author>Adam Lopez</author>
<author>Juri Ganitkevitch</author>
<author>Johnathan Weese</author>
<author>Ferhan Ture</author>
<author>Phil Blunsom</author>
<author>Hendra Setiawan</author>
<author>Vladimir Eidelman</author>
<author>Philip Resnik</author>
</authors>
<title>cdec: A decoder, alignment, and learning framework for finite-state and context-free translation models.</title>
<date>2010</date>
<booktitle>In Proceedings of ACL.</booktitle>
<contexts>
<context position="27491" citStr="Dyer et al., 2010" startWordPosition="4744" endWordPosition="4747">l, we find that both the spectral and the EM-based estimators improve upon a minimal grammar baseline with only a single category, but the spectral approach does better. In fact, it matches the performance of the standard HIERO baseline, despite learning on top of a minimal grammar. 5.1 Data and Baselines The ZH-EN data is the BTEC parallel corpus (Paul, 2009); we combine the first and second development sets in one, and evaluate on the third development set. The development and test sets are evaluated with 16 references. Statistics for the data are shown in Table 1. We used the CDEC decoder (Dyer et al., 2010) to extract word alignments and the baseline hierarchical grammars, MERT tuning, and decoding. We used a 4-gram language model built from the target-side of the parallel training data. The Python-based implementation of the tensor-based decoder, as well as the parameter estimation algorithms is available at github.com/asaluja/spectral-scfg/. The baseline HIERO system uses a grammar extracted by applying the commonly used heurisZH-EN TRAIN (SRC) 334K TRAIN (TGT) 366K DEV (SRC) 7K DEV (TGT) 7.6K TEST (SRC) 3.8K TEST (TGT) 3.9K Table 1: Corpus statistics (in words). For the target DEV and TEST st</context>
</contexts>
<marker>Dyer, Lopez, Ganitkevitch, Weese, Ture, Blunsom, Setiawan, Eidelman, Resnik, 2010</marker>
<rawString>Chris Dyer, Adam Lopez, Juri Ganitkevitch, Johnathan Weese, Ferhan Ture, Phil Blunsom, Hendra Setiawan, Vladimir Eidelman, and Philip Resnik. 2010. cdec: A decoder, alignment, and learning framework for finite-state and context-free translation models. In Proceedings of ACL.</rawString>
</citation>
<citation valid="true">
<authors>
<author>Yang Feng</author>
<author>Trevor Cohn</author>
</authors>
<title>A Markov model of machine translation using non-parametric bayesian inference.</title>
<date>2013</date>
<booktitle>In Proceedings ofACL.</booktitle>
<contexts>
<context position="40533" citStr="Feng and Cohn (2013)" startWordPosition="6994" endWordPosition="6997">tral m = 16 with rule indicator features, and EM m = 16, for the two Chinese sentences. Higher likelihoods are in blue, lower likelihoods in red. The hypotheses produced by each setup are below the heat maps. els (Mari˜no et al., 2006; Durrani et al., 2011) seek to model long-distance dependencies and reorderings through n-grams. Similarly, Vaswani et al. (2011) use a Markov model in the context of tree-to-string translation, where the parameters are smoothed with absolute discounting (Ney et al., 1994), while in our instance we capture this smoothing effect through low rank or latent states. Feng and Cohn (2013) also utilize a Markov model for MT, but learn the parameters through a more sophisticated estimation technique that makes use of Pitman-Yor hierarchical priors. Hsu et al. (2009) presented one of the initial efforts at spectral-based parameter estimation (using SVD) of observed moments for latent-variable models, in the case of Hidden Markov models. This idea was extended to L-PCFGs (Cohen et al., 2014), and our approach can be seen as a bilingual or synchronous generalization. 7 Conclusion In this work, we presented an approach to refine synchronous grammars used in MT by inferring the laten</context>
</contexts>
<marker>Feng, Cohn, 2013</marker>
<rawString>Yang Feng and Trevor Cohn. 2013. A Markov model of machine translation using non-parametric bayesian inference. In Proceedings ofACL.</rawString>
</citation>
<citation valid="true">
<authors>
<author>Michel Galley</author>
<author>Mark Hopkins</author>
<author>Kevin Knight</author>
<author>Daniel Marcu</author>
</authors>
<title>What’s in a translation rule?</title>
<date>2004</date>
<booktitle>In Proceedings of HLT-NAACL.</booktitle>
<contexts>
<context position="2096" citStr="Galley et al., 2004" startWordPosition="307" endWordPosition="310">available for synchronous derivations, and so learning the “right” grammars is a central challenge. Of course, learning synchronous grammars from parallel data is a widely studied problem (Wu, 1997; Blunsom et al., 2008; Levenberg et al., 2012, inter alia). However, there has been less exploration of learning rich non-terminal categories, largely because previous efforts to learn such categories have been coupled with efforts to learn derivation structures—a computationally formidable challenge. One popular approach has been to derive categories from source and/or target monolingual grammars (Galley et al., 2004; Zollmann and Venugopal, 2006; Hanneman and Lavie, 2013). While often successful, accurate parsers are not available in many languages: a more appealing approach is therefore to learn the category structure from the data itself. In this work, we take a different approach to previous work in synchronous grammar induction by assuming that reasonable tree structures for a parallel corpus can be chosen heuristically, and then, fixing the trees (thereby enabling us to sidestep the worst of the computational issues), we learn non-terminal categories as latent variables to explain the distribution o</context>
</contexts>
<marker>Galley, Hopkins, Knight, Marcu, 2004</marker>
<rawString>Michel Galley, Mark Hopkins, Kevin Knight, and Daniel Marcu. 2004. What’s in a translation rule? In Proceedings of HLT-NAACL.</rawString>
</citation>
<citation valid="true">
<authors>
<author>Michel Galley</author>
<author>Jonathan Graehl</author>
<author>Kevin Knight</author>
<author>Daniel Marcu</author>
<author>Steve DeNeefe</author>
<author>Wei Wang</author>
<author>Ignacio Thayer</author>
</authors>
<title>Scalable inference and training of context-rich syntactic translation models.</title>
<date>2006</date>
<booktitle>In Proceedings of ACL.</booktitle>
<contexts>
<context position="13514" citStr="Galley et al., 2006" startWordPosition="2300" endWordPosition="2303">face statistics associated with the phrase pairs or rules. The weights of these features are then learned using a discriminative training algorithm (Och, 2003; Chiang, 2012, inter alia). In contrast, in this work we restrict the number of possible synchronous derivations for each sentence pair to just one; thus, derivation forests do not have to be considered, making parameter estimation more tractable.3 To achieve this objective, for each sentence in the training data we extract the minimal set of synchronous rules consistent with the word alignments, as opposed to the composed set of rules (Galley et al., 2006). Composed rules are ones that can be formed from smaller rules in the grammar; with these rules, there are multiple synchronous trees consistent with the alignments for a given sentence pair, and thus the total number of applicable rules can be combinatorially larger than if we just consider the set of rules that cannot be formed from other rules, namely the minimal rules. The rule types across all sentence pairs are combined to form a minimal grammar.4 To extract a set of minimal rules, we use the linear-time extraction algorithm of Zhang et al. (2008). We give a rough description of their m</context>
<context position="15713" citStr="Galley et al., 2006" startWordPosition="2664" endWordPosition="2667">ient algorithms for parameter estimation over derivation forests, since there may be multiple valid ways to explain the sentence pair via a synchronous tree structure. 4Table 2 presents a comparison of grammar sizes for our experiments (§5.1). extraction. Converting the tree to a set of minimal SCFG rules for the sentence pair is straightforward, by replacing nodes corresponding to spans with lexical items or NTs in a bottom-up manner.5 By using minimal rules as a starting point instead of the traditional heuristically-extracted rules (Chiang, 2007) or arbitrary compositions of minimal rules (Galley et al., 2006), we are also able to explore the transition from minimal rules to composed ones in a principled manner by encoding contextual information through the latent states. Thus, a beneficial side effect of our refinement process is the creation of more contextspecific rules without increasing the overall size of the baseline grammar, instead holding this information in our parameters C&apos;. 4 Parameter Estimation for L-SCFGs We explore two methods for estimating the parameters C&apos; of the model: a likelihood-maximization approach based on EM (Dempster et al., 1977), and a spectral approach based on the m</context>
<context position="31404" citStr="Galley et al., 2006" startWordPosition="5385" endWordPosition="5388">H-EN experimental setup. The first section consists of the various baselines we consider. In addition to the aforementioned baselines, we evaluated a setup where the spectral parameters simply consist of the joint maximum likelihood estimates of the rules. This baseline should perform en par with MIN-GRAMMAR, which we see is the case on the development set. The performance on the test set is better though, primarily because we also include the reverse log relative frequency (f|e) computed from the latent-variable model as an additional feature in MERT. Furthermore, in line with previous work (Galley et al., 2006) which compares minimal and composed rules, we find that minimal grammars take a hit of more than 2.5 BLEU points on the development set, compared to composed (HIERO) grammars. The m = 1 spectral baseline with only rule indicator features performs slightly better than the minimal grammar baseline, since it overtly takes into account insideoutside tree combination preferences in the parameters, but improvement is minimal with one latent state naturally and the performance on the test set is in line with the MLE baseline. On top of the baselines, we looked at a number Setup BLEU Dev Test Baselin</context>
</contexts>
<marker>Galley, Graehl, Knight, Marcu, DeNeefe, Wang, Thayer, 2006</marker>
<rawString>Michel Galley, Jonathan Graehl, Kevin Knight, Daniel Marcu, Steve DeNeefe, Wei Wang, and Ignacio Thayer. 2006. Scalable inference and training of context-rich syntactic translation models. In Proceedings of ACL.</rawString>
</citation>
<citation valid="true">
<authors>
<author>Jonathan Graehl</author>
<author>Kevin Knight</author>
<author>Jonathan May</author>
</authors>
<title>Training tree transducers.</title>
<date>2008</date>
<journal>Computational Linguistics,</journal>
<volume>34</volume>
<issue>3</issue>
<contexts>
<context position="6862" citStr="Graehl et al., 2008" startWordPosition="1107" endWordPosition="1110">together with values h1, ... , hN for every NT in the tree. An important point to keep in mind in comparison to L-PCFGs is that the right-hand side (RHS) non-terminals of synchronous rules are aligned pairs across the source and target languages. In this work, we refine the one-category grammar introduced by Chiang (2007) for HPBT in order to learn additional latent NT categories. Thus, the following discussion is restricted to these kinds of grammars, although the method is equally applicable in other scenarios, e.g., the extended treeto-string transducer (xRs) formalism (Huang et al., 2006; Graehl et al., 2008) commonly used in syntax-directed translation, and phrase-based MT (Koehn et al., 2003). Marginal Inference with L-SCFGs. For a parameter t of rule r, the latent state h1 attached to the left-hand side (LHS) NT of r is associated with the outside tree for the sub-tree rooted at the LHS, and the states attached to the RHS NTs are associated with the inside trees of that NT. Since we do not assume conditional independence of these states, we need to consider all possible interactions, which can be compactly represented as a 3rd-order tensor in the case of a binary rule, a matrix (i.e., a 2nd-ord</context>
</contexts>
<marker>Graehl, Knight, May, 2008</marker>
<rawString>Jonathan Graehl, Kevin Knight, and Jonathan May. 2008. Training tree transducers. Computational Linguistics, 34(3):391–427, September.</rawString>
</citation>
<citation valid="true">
<authors>
<author>Greg Hanneman</author>
<author>Alon Lavie</author>
</authors>
<title>Improving syntax-augmented machine translation by coarsening the label set.</title>
<date>2013</date>
<booktitle>In Proceedings of NAACL.</booktitle>
<contexts>
<context position="2153" citStr="Hanneman and Lavie, 2013" startWordPosition="316" endWordPosition="319">ing the “right” grammars is a central challenge. Of course, learning synchronous grammars from parallel data is a widely studied problem (Wu, 1997; Blunsom et al., 2008; Levenberg et al., 2012, inter alia). However, there has been less exploration of learning rich non-terminal categories, largely because previous efforts to learn such categories have been coupled with efforts to learn derivation structures—a computationally formidable challenge. One popular approach has been to derive categories from source and/or target monolingual grammars (Galley et al., 2004; Zollmann and Venugopal, 2006; Hanneman and Lavie, 2013). While often successful, accurate parsers are not available in many languages: a more appealing approach is therefore to learn the category structure from the data itself. In this work, we take a different approach to previous work in synchronous grammar induction by assuming that reasonable tree structures for a parallel corpus can be chosen heuristically, and then, fixing the trees (thereby enabling us to sidestep the worst of the computational issues), we learn non-terminal categories as latent variables to explain the distribution of these synchronous trees. This technique has a long hist</context>
<context position="38226" citStr="Hanneman and Lavie, 2013" startWordPosition="6513" endWordPosition="6516">ithm that we propose here is similar to what they propose, except that we need to handle tensor structures. Mylonakis and Sima’an (2011) select among linguistically motivated non-terminal labels with a cross-validated version of EM. Although they consider a restricted hypothesis space, they do marginalize over different derivations therefore their inside-outside algorithm is O(n6). In the syntax-directed translation literature, there have been efforts to relax or coarsen the hard labels provided by a syntactic parser in an automatic manner to promote parameter sharing (Venugopal et al., 2009; Hanneman and Lavie, 2013), which is the complement of our aim in this paper. The idea of automatically learned grammar refinements comes from the monolingual parsing literature, where phenomena like head lexicalization can be modeled through latent variables. Matsuzaki et al. (2005) look at a likelihood-based method to split the NT categories of a grammar into a fixed number of sub-categories, while Petrov et al. (2006) learn a variable number of sub-categories per NT. The latter’s extension may be useful for finding the optimal number of latent states from the data in our case. The question of whether we can incorpor</context>
</contexts>
<marker>Hanneman, Lavie, 2013</marker>
<rawString>Greg Hanneman and Alon Lavie. 2013. Improving syntax-augmented machine translation by coarsening the label set. In Proceedings of NAACL.</rawString>
</citation>
<citation valid="true">
<authors>
<author>Daniel Hsu</author>
<author>Sham M Kakade</author>
<author>Tong Zhang</author>
</authors>
<title>A Spectral Algorithm for Learning Hidden Markov Models.</title>
<date>2009</date>
<booktitle>In Proceedings of COLT.</booktitle>
<contexts>
<context position="16347" citStr="Hsu et al., 2009" startWordPosition="2769" endWordPosition="2772"> to explore the transition from minimal rules to composed ones in a principled manner by encoding contextual information through the latent states. Thus, a beneficial side effect of our refinement process is the creation of more contextspecific rules without increasing the overall size of the baseline grammar, instead holding this information in our parameters C&apos;. 4 Parameter Estimation for L-SCFGs We explore two methods for estimating the parameters C&apos; of the model: a likelihood-maximization approach based on EM (Dempster et al., 1977), and a spectral approach based on the method of moments (Hsu et al., 2009; Cohen et al., 2014), where we identify a subspace using a singular value decomposition (SVD) of the cross-product feature space between inside and outside trees and estimate parameters in this subspace. Figure 2 presents a side-by-side comparison of the two algorithms, which we discuss in this section. In the spectral approach, we base our parameter estimates on low-rank representations of moments of features, while EM explicitly maximizes a likelihood criterion. The parameter estimation algorithms are relatively similar, but in lieu of sparse feature functions in the spectral case, EM uses </context>
<context position="40712" citStr="Hsu et al. (2009)" startWordPosition="7022" endWordPosition="7025">p are below the heat maps. els (Mari˜no et al., 2006; Durrani et al., 2011) seek to model long-distance dependencies and reorderings through n-grams. Similarly, Vaswani et al. (2011) use a Markov model in the context of tree-to-string translation, where the parameters are smoothed with absolute discounting (Ney et al., 1994), while in our instance we capture this smoothing effect through low rank or latent states. Feng and Cohn (2013) also utilize a Markov model for MT, but learn the parameters through a more sophisticated estimation technique that makes use of Pitman-Yor hierarchical priors. Hsu et al. (2009) presented one of the initial efforts at spectral-based parameter estimation (using SVD) of observed moments for latent-variable models, in the case of Hidden Markov models. This idea was extended to L-PCFGs (Cohen et al., 2014), and our approach can be seen as a bilingual or synchronous generalization. 7 Conclusion In this work, we presented an approach to refine synchronous grammars used in MT by inferring the latent categories for the single nonterminal in our grammar rules, and proposed two algorithms to estimate parameters for our latentvariable model. By fixing the synchronous derivation</context>
</contexts>
<marker>Hsu, Kakade, Zhang, 2009</marker>
<rawString>Daniel Hsu, Sham M. Kakade, and Tong Zhang. 2009. A Spectral Algorithm for Learning Hidden Markov Models. In Proceedings of COLT.</rawString>
</citation>
<citation valid="true">
<authors>
<author>Liang Huang</author>
<author>Kevin Knight</author>
<author>Aravind Joshi</author>
</authors>
<title>Statistical syntax-directed translation with extended domain of locality.</title>
<date>2006</date>
<booktitle>In Proceedings ofAMTA.</booktitle>
<contexts>
<context position="6840" citStr="Huang et al., 2006" startWordPosition="1103" endWordPosition="1106"> stree r1, ... , rN together with values h1, ... , hN for every NT in the tree. An important point to keep in mind in comparison to L-PCFGs is that the right-hand side (RHS) non-terminals of synchronous rules are aligned pairs across the source and target languages. In this work, we refine the one-category grammar introduced by Chiang (2007) for HPBT in order to learn additional latent NT categories. Thus, the following discussion is restricted to these kinds of grammars, although the method is equally applicable in other scenarios, e.g., the extended treeto-string transducer (xRs) formalism (Huang et al., 2006; Graehl et al., 2008) commonly used in syntax-directed translation, and phrase-based MT (Koehn et al., 2003). Marginal Inference with L-SCFGs. For a parameter t of rule r, the latent state h1 attached to the left-hand side (LHS) NT of r is associated with the outside tree for the sub-tree rooted at the LHS, and the states attached to the RHS NTs are associated with the inside trees of that NT. Since we do not assume conditional independence of these states, we need to consider all possible interactions, which can be compactly represented as a 3rd-order tensor in the case of a binary rule, a m</context>
</contexts>
<marker>Huang, Knight, Joshi, 2006</marker>
<rawString>Liang Huang, Kevin Knight, and Aravind Joshi. 2006. Statistical syntax-directed translation with extended domain of locality. In Proceedings ofAMTA.</rawString>
</citation>
<citation valid="true">
<authors>
<author>Zhongqiang Huang</author>
<author>Martin ˇCmejrek</author>
<author>Bowen Zhou</author>
</authors>
<title>Soft syntactic constraints for hierarchical phrase-based translation using latent syntactic distributions.</title>
<date>2010</date>
<booktitle>In Proceedings of EMNLP.</booktitle>
<marker>Huang, ˇCmejrek, Zhou, 2010</marker>
<rawString>Zhongqiang Huang, Martin ˇCmejrek, and Bowen Zhou. 2010. Soft syntactic constraints for hierarchical phrase-based translation using latent syntactic distributions. In Proceedings of EMNLP.</rawString>
</citation>
<citation valid="true">
<authors>
<author>Dan Klein</author>
<author>Christopher D Manning</author>
</authors>
<title>Parsing and hypergraphs.</title>
<date>2001</date>
<booktitle>In Proceedings of IWPT.</booktitle>
<contexts>
<context position="10794" citStr="Klein and Manning, 2001" startWordPosition="1838" endWordPosition="1841">the right and x0 when multiplying on the left of the matrix. The decoder computes marginal probabilities for each skeletal rule in the 1This operation is sometimes called a contraction. parse forest of a source sentence by marginalizing over the latent states, which in practice corresponds to simple tensor-vector products. This operation is not dependent on the manner in which the parameters were estimated. Figure 1 presents the tensor version of the inside-outside algorithm for decoding L-SCFGs. The algorithm takes as input the parse forest of the source sentence represented as a hypergraph (Klein and Manning, 2001), which is computed using a bottom-up parser with Earley-style rules similar to the algorithm in Chiang (2007). Hypergraphs are a compact way to represent a forest of multiple parse trees. Each node in the hypergraph corresponds to an NT span, and can have multiple incoming and outgoing hyperedges. Hyperedges, which connect one or more tail nodes to a single head node, correspond exactly to rules, and tail or head nodes correspond to children (RHS NTs) or parent (LHS NT). The function B(q) returns all incoming hyperedges to a node q, i.e., all rules such that the LHS NT of the rule corresponds</context>
</contexts>
<marker>Klein, Manning, 2001</marker>
<rawString>Dan Klein and Christopher D. Manning. 2001. Parsing and hypergraphs. In Proceedings of IWPT.</rawString>
</citation>
<citation valid="true">
<authors>
<author>Philipp Koehn</author>
<author>Franz Josef Och</author>
<author>Daniel Marcu</author>
</authors>
<title>Statistical phrase-based translation.</title>
<date>2003</date>
<booktitle>In Proceedings of NAACL.</booktitle>
<contexts>
<context position="4347" citStr="Koehn et al., 2003" startWordPosition="651" endWordPosition="654">n-generalizable rules are utilized. Hence, the hope is that this work pro1953 Proceedings of the 2014 Conference on Empirical Methods in Natural Language Processing (EMNLP), pages 1953–1964, October 25-29, 2014, Doha, Qatar. c�2014 Association for Computational Linguistics motes the move towards translation models that directly model the conditional likelihood of translation rules via (potentially feature-rich) latentvariable models which leverage information contained in the synchronous tree structure, instead of relying on a heuristic set of features based on empirical relative frequencies (Koehn et al., 2003) from non-hierarchical phrase-based translation. 2 Latent-Variable SCFGs Before discussing parameter learning, we introduce latent-variable synchronous context-free grammars (L-SCFGs) and discuss an inference algorithm for marginalizing over latent states. We extend the definition of L-PCFGs (Matsuzaki et al., 2005; Petrov et al., 2006) to synchronous grammars as used in machine translation (Chiang, 2007). A latent-variable SCFG (LSCFG) is a 6-tuple (N, m, ns, nt, 7r, t) where: • N is a set of non-terminal (NT) symbols in the grammar. For hierarchical phrase-based translation (HPBT), the set c</context>
<context position="6949" citStr="Koehn et al., 2003" startWordPosition="1119" endWordPosition="1122">n mind in comparison to L-PCFGs is that the right-hand side (RHS) non-terminals of synchronous rules are aligned pairs across the source and target languages. In this work, we refine the one-category grammar introduced by Chiang (2007) for HPBT in order to learn additional latent NT categories. Thus, the following discussion is restricted to these kinds of grammars, although the method is equally applicable in other scenarios, e.g., the extended treeto-string transducer (xRs) formalism (Huang et al., 2006; Graehl et al., 2008) commonly used in syntax-directed translation, and phrase-based MT (Koehn et al., 2003). Marginal Inference with L-SCFGs. For a parameter t of rule r, the latent state h1 attached to the left-hand side (LHS) NT of r is associated with the outside tree for the sub-tree rooted at the LHS, and the states attached to the RHS NTs are associated with the inside trees of that NT. Since we do not assume conditional independence of these states, we need to consider all possible interactions, which can be compactly represented as a 3rd-order tensor in the case of a binary rule, a matrix (i.e., a 2nd-order tensor) for unary rules, and a vector for pre-terminal (lexical) rules. Preferences </context>
</contexts>
<marker>Koehn, Och, Marcu, 2003</marker>
<rawString>Philipp Koehn, Franz Josef Och, and Daniel Marcu. 2003. Statistical phrase-based translation. In Proceedings of NAACL.</rawString>
</citation>
<citation valid="true">
<authors>
<author>Abby Levenberg</author>
<author>Chris Dyer</author>
<author>Phil Blunsom</author>
</authors>
<title>A Bayesian model for learning SCFGs with discontiguous rules.</title>
<date>2012</date>
<booktitle>In Proceedings of EMNLP-CoNLL.</booktitle>
<contexts>
<context position="1720" citStr="Levenberg et al., 2012" startWordPosition="255" endWordPosition="258">ng problem. A parser constructs trees over the input sentence by parsing with the source language projection of a synchronous CFG, and each derivation induces translations in the target language (Chiang, 2007). However, in contrast to syntactic parsing, where linguistic intuitions can help elucidate the “right” tree structure for a grammatical sentence, no such intuitions are available for synchronous derivations, and so learning the “right” grammars is a central challenge. Of course, learning synchronous grammars from parallel data is a widely studied problem (Wu, 1997; Blunsom et al., 2008; Levenberg et al., 2012, inter alia). However, there has been less exploration of learning rich non-terminal categories, largely because previous efforts to learn such categories have been coupled with efforts to learn derivation structures—a computationally formidable challenge. One popular approach has been to derive categories from source and/or target monolingual grammars (Galley et al., 2004; Zollmann and Venugopal, 2006; Hanneman and Lavie, 2013). While often successful, accurate parsers are not available in many languages: a more appealing approach is therefore to learn the category structure from the data it</context>
</contexts>
<marker>Levenberg, Dyer, Blunsom, 2012</marker>
<rawString>Abby Levenberg, Chris Dyer, and Phil Blunsom. 2012. A Bayesian model for learning SCFGs with discontiguous rules. In Proceedings of EMNLP-CoNLL.</rawString>
</citation>
<citation valid="true">
<authors>
<author>Percy Liang</author>
<author>Slav Petrov</author>
<author>Michael I Jordan</author>
<author>Dan Klein</author>
</authors>
<title>The infinite PCFG using hierarchical dirichlet processes.</title>
<date>2007</date>
<booktitle>In Proceedings of EMNLP.</booktitle>
<contexts>
<context position="2820" citStr="Liang et al., 2007" startWordPosition="421" endWordPosition="424">t available in many languages: a more appealing approach is therefore to learn the category structure from the data itself. In this work, we take a different approach to previous work in synchronous grammar induction by assuming that reasonable tree structures for a parallel corpus can be chosen heuristically, and then, fixing the trees (thereby enabling us to sidestep the worst of the computational issues), we learn non-terminal categories as latent variables to explain the distribution of these synchronous trees. This technique has a long history in monolingual parsing (Petrov et al., 2006; Liang et al., 2007; Cohen et al., 2014), where it reliably yields stateof-the-art phrase structure parsers based on generative models, but we are the first to apply it to translation. We first generalize the concept of latent PCFGs to latent-variable SCFGs (§2). We then follow by a presentation of the tensor-based formulation for our parameters, a representation that makes it convenient to marginalize over latent states. Subsequently, two methods for parameter estimation are presented (§4): a spectral approach based on the method of moments, and an EM-based likelihood maximization. Results on a Chinese–English </context>
</contexts>
<marker>Liang, Petrov, Jordan, Klein, 2007</marker>
<rawString>Percy Liang, Slav Petrov, Michael I. Jordan, and Dan Klein. 2007. The infinite PCFG using hierarchical dirichlet processes. In Proceedings of EMNLP.</rawString>
</citation>
<citation valid="true">
<authors>
<author>Jos´e B Mari˜no</author>
<author>Rafael E Banchs</author>
<author>Josep M Crego</author>
<author>Adri`a de Gispert</author>
<author>Patrik Lambert</author>
<author>Jos´e A R Fonollosa</author>
<author>Marta R Costa-juss`a</author>
</authors>
<title>N-grambased machine translation.</title>
<date>2006</date>
<journal>Computational Linguistics,</journal>
<volume>32</volume>
<issue>4</issue>
<marker>Mari˜no, Banchs, Crego, de Gispert, Lambert, Fonollosa, Costa-juss`a, 2006</marker>
<rawString>Jos´e B. Mari˜no, Rafael E. Banchs, Josep M. Crego, Adri`a de Gispert, Patrik Lambert, Jos´e A. R. Fonollosa, and Marta R. Costa-juss`a. 2006. N-grambased machine translation. Computational Linguistics, 32(4):527–549, December.</rawString>
</citation>
<citation valid="true">
<authors>
<author>Takuya Matsuzaki</author>
<author>Yusuke Miyao</author>
<author>Jun’ichi Tsujii</author>
</authors>
<title>Probabilistic CFG with latent annotations.</title>
<date>2005</date>
<booktitle>In Proceedings ofACL.</booktitle>
<contexts>
<context position="4663" citStr="Matsuzaki et al., 2005" startWordPosition="691" endWordPosition="695">ls that directly model the conditional likelihood of translation rules via (potentially feature-rich) latentvariable models which leverage information contained in the synchronous tree structure, instead of relying on a heuristic set of features based on empirical relative frequencies (Koehn et al., 2003) from non-hierarchical phrase-based translation. 2 Latent-Variable SCFGs Before discussing parameter learning, we introduce latent-variable synchronous context-free grammars (L-SCFGs) and discuss an inference algorithm for marginalizing over latent states. We extend the definition of L-PCFGs (Matsuzaki et al., 2005; Petrov et al., 2006) to synchronous grammars as used in machine translation (Chiang, 2007). A latent-variable SCFG (LSCFG) is a 6-tuple (N, m, ns, nt, 7r, t) where: • N is a set of non-terminal (NT) symbols in the grammar. For hierarchical phrase-based translation (HPBT), the set consists of only two symbols, X and a goal symbol S. • [m] is the set of possible hidden states associated with NTs. Aligned pairs of NTs across the source and target languages share the same hidden state. • [ns] is the set of source side words, i.e., the source-side vocabulary, with [ns] n N = 0. • [nt] is the set </context>
<context position="25670" citStr="Matsuzaki et al. (2005)" startWordPosition="4441" endWordPosition="4444"> there is a specific normalization factor for each inside-outside tuple. Lastly, the correlations are scaled by the existing parameter estimates. To obtain the next set of parameters, in the maximization step we normalize ˆCr for r E R such that for every h1, r0=r,h2,h3 ˆCr0(h1,h2,h3) = 1 for r E R2, Er0=r,h2 ˆCr0(h1, h2) = 1 for r E R1, and E ˆCr0(h2) = 1 for r E R0. We r0=r,h2 also normalize the root rule parameters ˆCr where LHS(r) = S. It is also possible to add sparse, overlapping features to an EM-based estimation 6In our experiments, we also tried the initialization scheme described in Matsuzaki et al. (2005), but found that it provided little benefit. 1958 procedure (Berg-Kirkpatrick et al., 2010) and we leave this extension for future work. 5 Experiments The goal of the experimental section is to evaluate the performance of the latent-variable SCFG in comparison to a baseline without any additional NT annotations (MIN-GRAMMAR), and to compare the performance of the two parameter estimation algorithms. We also compare L-SCFGs to a HIERO baseline (Chiang, 2007). The language pair of evaluation is Chinese–English (ZH-EN). We score translations using BLEU (Papineni et al., 2002). The latent-variable</context>
<context position="38484" citStr="Matsuzaki et al. (2005)" startWordPosition="6553" endWordPosition="6557">stricted hypothesis space, they do marginalize over different derivations therefore their inside-outside algorithm is O(n6). In the syntax-directed translation literature, there have been efforts to relax or coarsen the hard labels provided by a syntactic parser in an automatic manner to promote parameter sharing (Venugopal et al., 2009; Hanneman and Lavie, 2013), which is the complement of our aim in this paper. The idea of automatically learned grammar refinements comes from the monolingual parsing literature, where phenomena like head lexicalization can be modeled through latent variables. Matsuzaki et al. (2005) look at a likelihood-based method to split the NT categories of a grammar into a fixed number of sub-categories, while Petrov et al. (2006) learn a variable number of sub-categories per NT. The latter’s extension may be useful for finding the optimal number of latent states from the data in our case. The question of whether we can incorporate additional contextual information in minimal rule grammars in MT via auxiliary models instead of using longer, composed rules has been investigated before as well. n-gram translation mod1961 Span End Span End Span End Span starting at wo rd : 0 1 2 3 4 0</context>
</contexts>
<marker>Matsuzaki, Miyao, Tsujii, 2005</marker>
<rawString>Takuya Matsuzaki, Yusuke Miyao, and Jun’ichi Tsujii. 2005. Probabilistic CFG with latent annotations. In Proceedings ofACL.</rawString>
</citation>
<citation valid="true">
<authors>
<author>Markos Mylonakis</author>
<author>Khalil Sima’an</author>
</authors>
<title>Learning hierarchical translation structure with linguistic annotations.</title>
<date>2011</date>
<booktitle>In Proceedings of ACL.</booktitle>
<marker>Mylonakis, Sima’an, 2011</marker>
<rawString>Markos Mylonakis and Khalil Sima’an. 2011. Learning hierarchical translation structure with linguistic annotations. In Proceedings of ACL.</rawString>
</citation>
<citation valid="true">
<authors>
<author>Hermann Ney</author>
<author>Ute Essen</author>
<author>Reinhard Kneser</author>
</authors>
<date>1994</date>
<booktitle>On Structuring Probabilistic Dependencies in Stochastic Language Modelling. Computer Speech and Language,</booktitle>
<pages>8--1</pages>
<contexts>
<context position="40421" citStr="Ney et al., 1994" startWordPosition="6975" endWordPosition="6978">arison of the CKY charts containing marginal probabilities of non-terminal spans µ(X, i, j) for the MLE, spectral m = 16 with rule indicator features, and EM m = 16, for the two Chinese sentences. Higher likelihoods are in blue, lower likelihoods in red. The hypotheses produced by each setup are below the heat maps. els (Mari˜no et al., 2006; Durrani et al., 2011) seek to model long-distance dependencies and reorderings through n-grams. Similarly, Vaswani et al. (2011) use a Markov model in the context of tree-to-string translation, where the parameters are smoothed with absolute discounting (Ney et al., 1994), while in our instance we capture this smoothing effect through low rank or latent states. Feng and Cohn (2013) also utilize a Markov model for MT, but learn the parameters through a more sophisticated estimation technique that makes use of Pitman-Yor hierarchical priors. Hsu et al. (2009) presented one of the initial efforts at spectral-based parameter estimation (using SVD) of observed moments for latent-variable models, in the case of Hidden Markov models. This idea was extended to L-PCFGs (Cohen et al., 2014), and our approach can be seen as a bilingual or synchronous generalization. 7 Co</context>
</contexts>
<marker>Ney, Essen, Kneser, 1994</marker>
<rawString>Hermann Ney, Ute Essen, and Reinhard Kneser. 1994. On Structuring Probabilistic Dependencies in Stochastic Language Modelling. Computer Speech and Language, 8:1–38.</rawString>
</citation>
<citation valid="true">
<authors>
<author>Franz Josef Och</author>
</authors>
<title>Minimum error rate training in statistical machine translation.</title>
<date>2003</date>
<booktitle>In Proceedings of ACL.</booktitle>
<contexts>
<context position="13052" citStr="Och, 2003" startWordPosition="2226" endWordPosition="2227">t composed of synchronous s-trees, which can be acquired from word alignments. Normally in phrase-based translation models, we consider all possible phrase 2In practice, the term m3|G |can be replaced with a smaller term, which separates the rules in G by the number of NTs on the RHS. This idea relates to the notion of “effective grammar size” which we discuss in §5. 1955 pairs consistent with the word alignments and estimate features based on surface statistics associated with the phrase pairs or rules. The weights of these features are then learned using a discriminative training algorithm (Och, 2003; Chiang, 2012, inter alia). In contrast, in this work we restrict the number of possible synchronous derivations for each sentence pair to just one; thus, derivation forests do not have to be considered, making parameter estimation more tractable.3 To achieve this objective, for each sentence in the training data we extract the minimal set of synchronous rules consistent with the word alignments, as opposed to the composed set of rules (Galley et al., 2006). Composed rules are ones that can be formed from smaller rules in the grammar; with these rules, there are multiple synchronous trees con</context>
<context position="26572" citStr="Och, 2003" startWordPosition="4590" endWordPosition="4591">ional NT annotations (MIN-GRAMMAR), and to compare the performance of the two parameter estimation algorithms. We also compare L-SCFGs to a HIERO baseline (Chiang, 2007). The language pair of evaluation is Chinese–English (ZH-EN). We score translations using BLEU (Papineni et al., 2002). The latent-variable model is integrated into the standard MT pipeline by computing marginal probabilities for each rule in the parse forest of a source sentence using the algorithm in Figure 1 with the parameters estimated through the algorithms in Figure 2, and is added as a feature for the rule during MERT (Och, 2003). These probabilities are conditioned on the LHS (X), and are thus joint probabilities for a source-target RHS pair. We also write out as features the conditional relative frequencies Pˆ(e|f) and Pˆ(f|e) as estimated by our latent-variable model, i.e., conditioned on the source and target RHS. Overall, we find that both the spectral and the EM-based estimators improve upon a minimal grammar baseline with only a single category, but the spectral approach does better. In fact, it matches the performance of the standard HIERO baseline, despite learning on top of a minimal grammar. 5.1 Data and Ba</context>
</contexts>
<marker>Och, 2003</marker>
<rawString>Franz Josef Och. 2003. Minimum error rate training in statistical machine translation. In Proceedings of ACL.</rawString>
</citation>
<citation valid="true">
<authors>
<author>Kishore Papineni</author>
<author>Salim Roukos</author>
<author>Todd Ward</author>
<author>WeiJing Zhu</author>
</authors>
<title>BLEU: A method for automatic evaluation of machine translation.</title>
<date>2002</date>
<booktitle>In Proceedings ofACL.</booktitle>
<contexts>
<context position="26249" citStr="Papineni et al., 2002" startWordPosition="4531" endWordPosition="4534">scheme described in Matsuzaki et al. (2005), but found that it provided little benefit. 1958 procedure (Berg-Kirkpatrick et al., 2010) and we leave this extension for future work. 5 Experiments The goal of the experimental section is to evaluate the performance of the latent-variable SCFG in comparison to a baseline without any additional NT annotations (MIN-GRAMMAR), and to compare the performance of the two parameter estimation algorithms. We also compare L-SCFGs to a HIERO baseline (Chiang, 2007). The language pair of evaluation is Chinese–English (ZH-EN). We score translations using BLEU (Papineni et al., 2002). The latent-variable model is integrated into the standard MT pipeline by computing marginal probabilities for each rule in the parse forest of a source sentence using the algorithm in Figure 1 with the parameters estimated through the algorithms in Figure 2, and is added as a feature for the rule during MERT (Och, 2003). These probabilities are conditioned on the LHS (X), and are thus joint probabilities for a source-target RHS pair. We also write out as features the conditional relative frequencies Pˆ(e|f) and Pˆ(f|e) as estimated by our latent-variable model, i.e., conditioned on the sourc</context>
</contexts>
<marker>Papineni, Roukos, Ward, Zhu, 2002</marker>
<rawString>Kishore Papineni, Salim Roukos, Todd Ward, and WeiJing Zhu. 2002. BLEU: A method for automatic evaluation of machine translation. In Proceedings ofACL.</rawString>
</citation>
<citation valid="true">
<authors>
<author>Michael Paul</author>
</authors>
<title>Overview of the IWSLT</title>
<date>2009</date>
<booktitle>In Proceedings of IWSLT.</booktitle>
<contexts>
<context position="27235" citStr="Paul, 2009" startWordPosition="4701" endWordPosition="4702">, and are thus joint probabilities for a source-target RHS pair. We also write out as features the conditional relative frequencies Pˆ(e|f) and Pˆ(f|e) as estimated by our latent-variable model, i.e., conditioned on the source and target RHS. Overall, we find that both the spectral and the EM-based estimators improve upon a minimal grammar baseline with only a single category, but the spectral approach does better. In fact, it matches the performance of the standard HIERO baseline, despite learning on top of a minimal grammar. 5.1 Data and Baselines The ZH-EN data is the BTEC parallel corpus (Paul, 2009); we combine the first and second development sets in one, and evaluate on the third development set. The development and test sets are evaluated with 16 references. Statistics for the data are shown in Table 1. We used the CDEC decoder (Dyer et al., 2010) to extract word alignments and the baseline hierarchical grammars, MERT tuning, and decoding. We used a 4-gram language model built from the target-side of the parallel training data. The Python-based implementation of the tensor-based decoder, as well as the parameter estimation algorithms is available at github.com/asaluja/spectral-scfg/. </context>
</contexts>
<marker>Paul, 2009</marker>
<rawString>Michael Paul. 2009. Overview of the IWSLT 2009 evaluation campaign. In Proceedings of IWSLT.</rawString>
</citation>
<citation valid="true">
<authors>
<author>Slav Petrov</author>
<author>Leon Barrett</author>
<author>Romain Thibaux</author>
<author>Dan Klein</author>
</authors>
<title>Learning accurate, compact, and interpretable tree annotation.</title>
<date>2006</date>
<booktitle>In Proceedings of ACL.</booktitle>
<contexts>
<context position="2800" citStr="Petrov et al., 2006" startWordPosition="417" endWordPosition="420">curate parsers are not available in many languages: a more appealing approach is therefore to learn the category structure from the data itself. In this work, we take a different approach to previous work in synchronous grammar induction by assuming that reasonable tree structures for a parallel corpus can be chosen heuristically, and then, fixing the trees (thereby enabling us to sidestep the worst of the computational issues), we learn non-terminal categories as latent variables to explain the distribution of these synchronous trees. This technique has a long history in monolingual parsing (Petrov et al., 2006; Liang et al., 2007; Cohen et al., 2014), where it reliably yields stateof-the-art phrase structure parsers based on generative models, but we are the first to apply it to translation. We first generalize the concept of latent PCFGs to latent-variable SCFGs (§2). We then follow by a presentation of the tensor-based formulation for our parameters, a representation that makes it convenient to marginalize over latent states. Subsequently, two methods for parameter estimation are presented (§4): a spectral approach based on the method of moments, and an EM-based likelihood maximization. Results o</context>
<context position="4685" citStr="Petrov et al., 2006" startWordPosition="696" endWordPosition="699">he conditional likelihood of translation rules via (potentially feature-rich) latentvariable models which leverage information contained in the synchronous tree structure, instead of relying on a heuristic set of features based on empirical relative frequencies (Koehn et al., 2003) from non-hierarchical phrase-based translation. 2 Latent-Variable SCFGs Before discussing parameter learning, we introduce latent-variable synchronous context-free grammars (L-SCFGs) and discuss an inference algorithm for marginalizing over latent states. We extend the definition of L-PCFGs (Matsuzaki et al., 2005; Petrov et al., 2006) to synchronous grammars as used in machine translation (Chiang, 2007). A latent-variable SCFG (LSCFG) is a 6-tuple (N, m, ns, nt, 7r, t) where: • N is a set of non-terminal (NT) symbols in the grammar. For hierarchical phrase-based translation (HPBT), the set consists of only two symbols, X and a goal symbol S. • [m] is the set of possible hidden states associated with NTs. Aligned pairs of NTs across the source and target languages share the same hidden state. • [ns] is the set of source side words, i.e., the source-side vocabulary, with [ns] n N = 0. • [nt] is the set of target side words, </context>
<context position="38624" citStr="Petrov et al. (2006)" startWordPosition="6579" endWordPosition="6582">cted translation literature, there have been efforts to relax or coarsen the hard labels provided by a syntactic parser in an automatic manner to promote parameter sharing (Venugopal et al., 2009; Hanneman and Lavie, 2013), which is the complement of our aim in this paper. The idea of automatically learned grammar refinements comes from the monolingual parsing literature, where phenomena like head lexicalization can be modeled through latent variables. Matsuzaki et al. (2005) look at a likelihood-based method to split the NT categories of a grammar into a fixed number of sub-categories, while Petrov et al. (2006) learn a variable number of sub-categories per NT. The latter’s extension may be useful for finding the optimal number of latent states from the data in our case. The question of whether we can incorporate additional contextual information in minimal rule grammars in MT via auxiliary models instead of using longer, composed rules has been investigated before as well. n-gram translation mod1961 Span End Span End Span End Span starting at wo rd : 0 1 2 3 4 0.00 0.1 0.3 0.4 0.6 0.7 0.9 1.0 Span starting l n ( su m) at wo rd : 0 1 2 3 4 0.00 0.2 0.5 0.7 1.0 1.2 1.5 1.7 2.0 Span starting at wo rd :</context>
</contexts>
<marker>Petrov, Barrett, Thibaux, Klein, 2006</marker>
<rawString>Slav Petrov, Leon Barrett, Romain Thibaux, and Dan Klein. 2006. Learning accurate, compact, and interpretable tree annotation. In Proceedings of ACL. 1963</rawString>
</citation>
<citation valid="true">
<authors>
<author>Takeaki Uno</author>
<author>Mutsunori Yagiura</author>
</authors>
<title>Fast algorithms to enumerate all common intervals of two permutations.</title>
<date>2000</date>
<journal>Algorithmica,</journal>
<volume>26</volume>
<issue>2</issue>
<contexts>
<context position="14516" citStr="Uno and Yagiura, 2000" startWordPosition="2468" endWordPosition="2471"> rule types across all sentence pairs are combined to form a minimal grammar.4 To extract a set of minimal rules, we use the linear-time extraction algorithm of Zhang et al. (2008). We give a rough description of their method below, and refer the reader to the original paper for additional details. The algorithm returns a complete minimal derivation tree for each word-aligned sentence pair, and generalizes an approach for finding all common intervals (pairs of phrases such that no word pair in the alignment links a word inside the phrase to a word outside the phrase) between two permutations (Uno and Yagiura, 2000) to sequences with many-to-many alignment links between the two sides, as in word alignment. The key idea is to encode all phrase pairs of a sentence alignment in a tree of size proportional to the source sentence length, which they call the normalized decomposition tree. Each node corresponds to a phrase pair, with larger phrase spans represented by higher nodes in the tree. Constructing the tree is analogous to finding common intervals in two permutations, a property that they leverage to propose a linear-time algorithm for tree 3For future work, we will consider efficient algorithms for par</context>
</contexts>
<marker>Uno, Yagiura, 2000</marker>
<rawString>Takeaki Uno and Mutsunori Yagiura. 2000. Fast algorithms to enumerate all common intervals of two permutations. Algorithmica, 26(2):290–309.</rawString>
</citation>
<citation valid="true">
<authors>
<author>Ashish Vaswani</author>
<author>Haitao Mi</author>
<author>Liang Huang</author>
<author>David Chiang</author>
</authors>
<title>Rule Markov models for fast tree-tostring translation.</title>
<date>2011</date>
<booktitle>In Proceedings of ACL.</booktitle>
<contexts>
<context position="40277" citStr="Vaswani et al. (2011)" startWordPosition="6953" endWordPosition="6956">ke a shampoo and style. (d) MLE I ’d like a shampoo and style. (e) Spectral m = 16 RI I ’d like a shampoo and style . (f) EM m = 16 Figure 3: A comparison of the CKY charts containing marginal probabilities of non-terminal spans µ(X, i, j) for the MLE, spectral m = 16 with rule indicator features, and EM m = 16, for the two Chinese sentences. Higher likelihoods are in blue, lower likelihoods in red. The hypotheses produced by each setup are below the heat maps. els (Mari˜no et al., 2006; Durrani et al., 2011) seek to model long-distance dependencies and reorderings through n-grams. Similarly, Vaswani et al. (2011) use a Markov model in the context of tree-to-string translation, where the parameters are smoothed with absolute discounting (Ney et al., 1994), while in our instance we capture this smoothing effect through low rank or latent states. Feng and Cohn (2013) also utilize a Markov model for MT, but learn the parameters through a more sophisticated estimation technique that makes use of Pitman-Yor hierarchical priors. Hsu et al. (2009) presented one of the initial efforts at spectral-based parameter estimation (using SVD) of observed moments for latent-variable models, in the case of Hidden Markov</context>
</contexts>
<marker>Vaswani, Mi, Huang, Chiang, 2011</marker>
<rawString>Ashish Vaswani, Haitao Mi, Liang Huang, and David Chiang. 2011. Rule Markov models for fast tree-tostring translation. In Proceedings of ACL.</rawString>
</citation>
<citation valid="true">
<authors>
<author>Ashish Venugopal</author>
<author>Andreas Zollmann</author>
<author>Noah A Smith</author>
<author>Stephan Vogel</author>
</authors>
<title>Preference grammars: Softening syntactic constraints to improve statistical machine translation.</title>
<date>2009</date>
<booktitle>In Proceedings of NAACL.</booktitle>
<contexts>
<context position="38199" citStr="Venugopal et al., 2009" startWordPosition="6509" endWordPosition="6512">rser. The EM-based algorithm that we propose here is similar to what they propose, except that we need to handle tensor structures. Mylonakis and Sima’an (2011) select among linguistically motivated non-terminal labels with a cross-validated version of EM. Although they consider a restricted hypothesis space, they do marginalize over different derivations therefore their inside-outside algorithm is O(n6). In the syntax-directed translation literature, there have been efforts to relax or coarsen the hard labels provided by a syntactic parser in an automatic manner to promote parameter sharing (Venugopal et al., 2009; Hanneman and Lavie, 2013), which is the complement of our aim in this paper. The idea of automatically learned grammar refinements comes from the monolingual parsing literature, where phenomena like head lexicalization can be modeled through latent variables. Matsuzaki et al. (2005) look at a likelihood-based method to split the NT categories of a grammar into a fixed number of sub-categories, while Petrov et al. (2006) learn a variable number of sub-categories per NT. The latter’s extension may be useful for finding the optimal number of latent states from the data in our case. The question</context>
</contexts>
<marker>Venugopal, Zollmann, Smith, Vogel, 2009</marker>
<rawString>Ashish Venugopal, Andreas Zollmann, Noah A. Smith, and Stephan Vogel. 2009. Preference grammars: Softening syntactic constraints to improve statistical machine translation. In Proceedings of NAACL.</rawString>
</citation>
<citation valid="true">
<authors>
<author>Dekai Wu</author>
</authors>
<title>Stochastic inversion transduction grammars and bilingual parsing of parallel corpora.</title>
<date>1997</date>
<journal>Computational Linguistics,</journal>
<volume>23</volume>
<issue>3</issue>
<contexts>
<context position="1674" citStr="Wu, 1997" startWordPosition="249" endWordPosition="250"> problem as a context-free parsing problem. A parser constructs trees over the input sentence by parsing with the source language projection of a synchronous CFG, and each derivation induces translations in the target language (Chiang, 2007). However, in contrast to syntactic parsing, where linguistic intuitions can help elucidate the “right” tree structure for a grammatical sentence, no such intuitions are available for synchronous derivations, and so learning the “right” grammars is a central challenge. Of course, learning synchronous grammars from parallel data is a widely studied problem (Wu, 1997; Blunsom et al., 2008; Levenberg et al., 2012, inter alia). However, there has been less exploration of learning rich non-terminal categories, largely because previous efforts to learn such categories have been coupled with efforts to learn derivation structures—a computationally formidable challenge. One popular approach has been to derive categories from source and/or target monolingual grammars (Galley et al., 2004; Zollmann and Venugopal, 2006; Hanneman and Lavie, 2013). While often successful, accurate parsers are not available in many languages: a more appealing approach is therefore to</context>
</contexts>
<marker>Wu, 1997</marker>
<rawString>Dekai Wu. 1997. Stochastic inversion transduction grammars and bilingual parsing of parallel corpora. Computational Linguistics, 23(3):377–403, September.</rawString>
</citation>
<citation valid="true">
<authors>
<author>Ying Zhang</author>
<author>Stephan Vogel</author>
<author>Alex Waibel</author>
</authors>
<title>Interpreting BLEU/NIST scores: How much improvement do we need to have a better system. In</title>
<date>2004</date>
<booktitle>In Proceedings LREC.</booktitle>
<contexts>
<context position="34678" citStr="Zhang et al., 2004" startWordPosition="5937" endWordPosition="5940">e HIERO baseline, but it only uses rules extracted from a minimal grammar, whose size is a fraction of the HIERO grammar. The gains seem to level off at this rank; additional ranks seem to add noise to the parameters. Feature-wise, additional lexical and length features add little, prob1960 ably because much of this information is encapsulated in the rule indicator features. For EM, m = 16 outperforms the minimal grammar baseline, but is not at the level of the spectral results. All EM, spectral, and MLE results are statistically significant (p &lt; 0.01) with respect to the MINGRAMMAR baseline (Zhang et al., 2004), and the improvement over the HIERO baseline achieved by the m = 16 rule indicator configuration is also statistically significant. The two estimation algorithms differ significantly in their estimation time. Given a feature covariance matrix, the spectral algorithm (SVD, which was done with Matlab, and correlation computation steps) for m = 16 took 7 minutes, while the EM algorithm took 5 minutes for each iteration with this rank. 5.4 Analysis Figure 3 presents a comparison of the nonterminal span marginals for two sentences in the development set. We visualize these differences through a he</context>
</contexts>
<marker>Zhang, Vogel, Waibel, 2004</marker>
<rawString>Ying Zhang, Stephan Vogel, and Alex Waibel. 2004. Interpreting BLEU/NIST scores: How much improvement do we need to have a better system. In In Proceedings LREC.</rawString>
</citation>
<citation valid="true">
<authors>
<author>Hao Zhang</author>
<author>Daniel Gildea</author>
<author>David Chiang</author>
</authors>
<title>Extracting synchronous grammar rules from wordlevel alignments in linear time.</title>
<date>2008</date>
<booktitle>In Proceedings of COLING.</booktitle>
<contexts>
<context position="14074" citStr="Zhang et al. (2008)" startWordPosition="2396" endWordPosition="2399"> opposed to the composed set of rules (Galley et al., 2006). Composed rules are ones that can be formed from smaller rules in the grammar; with these rules, there are multiple synchronous trees consistent with the alignments for a given sentence pair, and thus the total number of applicable rules can be combinatorially larger than if we just consider the set of rules that cannot be formed from other rules, namely the minimal rules. The rule types across all sentence pairs are combined to form a minimal grammar.4 To extract a set of minimal rules, we use the linear-time extraction algorithm of Zhang et al. (2008). We give a rough description of their method below, and refer the reader to the original paper for additional details. The algorithm returns a complete minimal derivation tree for each word-aligned sentence pair, and generalizes an approach for finding all common intervals (pairs of phrases such that no word pair in the alignment links a word inside the phrase to a word outside the phrase) between two permutations (Uno and Yagiura, 2000) to sequences with many-to-many alignment links between the two sides, as in word alignment. The key idea is to encode all phrase pairs of a sentence alignmen</context>
</contexts>
<marker>Zhang, Gildea, Chiang, 2008</marker>
<rawString>Hao Zhang, Daniel Gildea, and David Chiang. 2008. Extracting synchronous grammar rules from wordlevel alignments in linear time. In Proceedings of COLING.</rawString>
</citation>
<citation valid="true">
<authors>
<author>Andreas Zollmann</author>
<author>Ashish Venugopal</author>
</authors>
<title>Syntax augmented machine translation via chart parsing.</title>
<date>2006</date>
<booktitle>In Proceedings of the Workshop on Statistical Machine Translation, StatMT ’06,</booktitle>
<pages>138--141</pages>
<publisher>Association</publisher>
<institution>for Computational Linguistics.</institution>
<contexts>
<context position="2126" citStr="Zollmann and Venugopal, 2006" startWordPosition="311" endWordPosition="315">nous derivations, and so learning the “right” grammars is a central challenge. Of course, learning synchronous grammars from parallel data is a widely studied problem (Wu, 1997; Blunsom et al., 2008; Levenberg et al., 2012, inter alia). However, there has been less exploration of learning rich non-terminal categories, largely because previous efforts to learn such categories have been coupled with efforts to learn derivation structures—a computationally formidable challenge. One popular approach has been to derive categories from source and/or target monolingual grammars (Galley et al., 2004; Zollmann and Venugopal, 2006; Hanneman and Lavie, 2013). While often successful, accurate parsers are not available in many languages: a more appealing approach is therefore to learn the category structure from the data itself. In this work, we take a different approach to previous work in synchronous grammar induction by assuming that reasonable tree structures for a parallel corpus can be chosen heuristically, and then, fixing the trees (thereby enabling us to sidestep the worst of the computational issues), we learn non-terminal categories as latent variables to explain the distribution of these synchronous trees. Thi</context>
</contexts>
<marker>Zollmann, Venugopal, 2006</marker>
<rawString>Andreas Zollmann and Ashish Venugopal. 2006. Syntax augmented machine translation via chart parsing. In Proceedings of the Workshop on Statistical Machine Translation, StatMT ’06, pages 138–141. Association for Computational Linguistics.</rawString>
</citation>
</citationList>
</algorithm>
</algorithms>
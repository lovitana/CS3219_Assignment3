<?xml version="1.0" encoding="UTF-8"?>
<algorithms version="110505">
<algorithm name="SectLabel" version="110505">
<variant no="0" confidence="0.000000">
<title confidence="0.995885">
Finding Good Enough: A Task-Based Evaluation of Query Biased
Summarization for Cross Language Information Retrieval
</title>
<author confidence="0.88112">
Jennifer Williams, Sharon Tam, Wade Shen
</author>
<affiliation confidence="0.771605">
MIT Lincoln Laboratory Human Language Technology Group
</affiliation>
<address confidence="0.536128">
244 Wood Street, Lexington, MA 02420 USA
</address>
<email confidence="0.967503">
jennifer.williams@ll.mit.edu, sharontam@alum.mit.edu
swade@ll.mit.edu
</email>
<sectionHeader confidence="0.994747" genericHeader="abstract">
Abstract
</sectionHeader>
<bodyText confidence="0.999812090909091">
In this paper we present our task-based
evaluation of query biased summarization
for cross-language information retrieval
(CLIR) using relevance prediction. We de-
scribe our 13 summarization methods each
from one of four summarization strate-
gies. We show how well our methods
perform using Farsi text from the CLEF
2008 shared-task, which we translated to
English automtatically. We report preci-
sion/recall/F1, accuracy and time-on-task.
We found that different summarization
methods perform optimally for different
evaluation metrics, but overall query bi-
ased word clouds are the best summariza-
tion strategy. In our analysis, we demon-
strate that using the ROUGE metric on our
sentence-based summaries cannot make
the same kinds of distinctions as our evalu-
ation framework does. Finally, we present
our recommendations for creating much-
needed evaluation standards and datasets.
</bodyText>
<sectionHeader confidence="0.998127" genericHeader="keywords">
1 Introduction
</sectionHeader>
<bodyText confidence="0.992669122807017">
Despite many recent advances in query biased
summarization for cross-language information re-
trieval (CLIR), there are no existing evaluation
standards or datasets to make comparisons among
different methods, and across different languages
(Tombros and Sanderson, 1998; Pingali et al.,
2007; McCallum et al., 2012; Bhaskar and Bandy-
opadhyay, 2012). Consider that creating this
kind of summary requires familiarity with tech-
niques from machine translation (MT), summa-
rization, and information retrieval (IR). In this
This work was sponsored by the Federal Bureau of Inves-
tigation under Air Force Contract FA8721-05-C-0002. Opin-
ions, interpretations, conclusions, and recommendations are
those of the authors and are not necessarily endorsed by the
United States Government.
paper, we arrive at the intersection of each of
these research areas. Query biased summariza-
tion (also known as query-focused, query-relevant,
and query-dependent) involves automatically cap-
turing relevant ideas and content from a document
with respect to a given query, and presenting it as a
condensed version of the original document. This
kind of summarization is mostly used in search en-
gines because when search results are tailored to a
user’s information need, the user can find texts that
they are looking for more quickly and more ac-
curately (Tombros and Sanderson, 1998; Mori et
al., 2004). Query biased summarization is a valu-
able research area in natural language processing
(NLP), especially for CLIR. Users of CLIR sys-
tems meet their information needs by submitting
their queries in L1 to search through documents
that have been composed in L2, even though they
may not be familiar with L2 (Hovy et al., 1999;
Pingali et al., 2007).
There are no standards for objectively evaluat-
ing summaries for CLIR – a research gap that we
begin to address in this paper. The problem we
explore is two-fold: what kinds of summaries are
well-suited for CLIR applications, and how should
the summaries be evaluated. Our evaluation is ex-
trinsic, that is to say we are interested in how sum-
marization affects performance on a different task
(Mani et al., 2002; McKeown et al., 2005; Dorr
et al., 2005; Murray et al., 2009; McCallum et
al., 2012). We use relevance prediction as our ex-
trinsic task: a human must decide if a summary
for a given document is relevant to a particular in-
formation need, or not. Relevance prediction is
known to be useful as it correlates with some au-
tomatic intrinsic methods as well (President and
Dorr, 2006; Hobson et al., 2007). To the best of
our knowledge, we are the first to apply this eval-
uation framework to cross language query biased
summarization.
Each one of the summarization methods that we
</bodyText>
<note confidence="0.819558">
657
Proceedings of the 2014 Conference on Empirical Methods in Natural Language Processing (EMNLP), pages 657–669,
October 25-29, 2014, Doha, Qatar. c�2014 Association for Computational Linguistics
</note>
<bodyText confidence="0.999965763157895">
present in this paper belongs to one of the fol-
lowing strategies: (1) unbiased full machine trans-
lated text, (2) unbiased word clouds, (3) query bi-
ased word clouds, and (4) query biased sentence
summaries. The methods and strategies that we
present are fast, cheap, and language-independent.
All of these strategies are extractive, meaning that
we used existing parts of a document to create the
condensed version, or summary.
We approach our task as an engineering prob-
lem: the goal is to decide if summaries are good
enough to help CLIR system users find what they
are looking for. We have simplified the task by as-
suming that a set of documents has already been
retrieved from a search engine, as CLIR tech-
niques are outside the scope of this paper. We
predict that showing the full MT English text as
a summarization strategy would not be particu-
larly helpful in our relevance prediction task be-
cause the words in the text could be mixed-up,
or sentences could be nonsensical, resulting in
poor readability. For the same reasons, we expect
that showing the full MT English text would take
longer to arrive at a relevance decision. Finally,
we predict that query biased summaries will result
in faster, more accurate decisions from the partic-
ipants (Tombros and Sanderson, 1998).
We treat the actual CLIR search engine as if it
were a black box so that we can focus on evaluat-
ing if the summaries themselves are useful. As a
starting point, we begin with some principles that
we expect to hold true when we evaluate. These
principles provide us with the kind of framework
that we need for a productive and judicious dis-
cussion about how well a summarization method
works. We encourage the NLP community to
consider the following concepts when developing
evaluation standards for this problem:
</bodyText>
<listItem confidence="0.999802">
• End-user intelligiblity
• Query-salience
• Retrieval-relevance
</listItem>
<bodyText confidence="0.999584789473684">
Summaries should be presented to the end-user in
a way that is both concise and intelligible, even
if the machine translated text is difficult to under-
stand. Our notions of query-salience and retrieval-
relevance capture the expectation that good sum-
maries will be efficient enough to help end-users
fulfill their information needs. For query-salience,
we want users to positively identify relevant doc-
uments. Similarly, for retrieval-relevance we want
users to be able to find as many relevant docu-
ments as possible.
This paper is structured as follows: Section 2
presents related work; Section 3 describes our data
and pre-processing; Section 4 details our sum-
marization methods and strategies; Section 5 de-
scribes our experiments; Section 6 shows our re-
sults and analysis; and in Section 7, we conclude
and discuss some future directions for the NLP
community.
</bodyText>
<sectionHeader confidence="0.999785" genericHeader="introduction">
2 Related Work
</sectionHeader>
<bodyText confidence="0.998882125">
Automatic summarization is generally a well-
investigated research area. Summarization is a
way of describing the relationships of words in
documents to the information content of that doc-
ument (Luhn, 1958; Edmunson, 1969; Salton and
Yang, 1973; Robertson and Walker, 1994; Church
and Gale, 1999; Robertson, 2004). Recent work
has looked at creating summaries of single and
multiple documents (Radev et al., 2004; Erkan and
Radev, 2004; Wan et al., 2007; Yin et al., 2012;
Chatterjee et al., 2012), as well as summary eval-
uation (Jing et al., 1998; Tombros and Sanderson
1998; Mani et al., 1998; Mani et al., 1999; Mani,
2001; Lin and Hovy, 2003; Lin, 2004; Nenkova
et al., 2007; Hobson et al., 2007; Owczarzak
et al., 2012), query and topic biased summariza-
tion (Berger and Mittal, 2000; Otterbacher et al.,
2005; Daume and Marcu, 2006; Chali and Joty,
2008; Otterbacher et al., 2009; Bando et al., 2010;
Bhaskar and Bandyopadhyay, 2012; Harwath and
Hazen, 2012; Yin et al., 2012), and summarization
across languages (Pingali et al., 2007; Or˘asan and
Chiorean, 2008; Wan et al., 2010; Azarbonyad et
al., 2013).
</bodyText>
<subsectionHeader confidence="0.997611">
2.1 Query Biased Summarization
</subsectionHeader>
<bodyText confidence="0.999988769230769">
Previous work most closely related to our own
comes from Pingali et al., (2007). In their work,
they present their method for cross-language
query biased summarization for Telugu and En-
glish. Their work was motivated by the need for
people to have access to foreign-language docu-
ments from a search engine even though the users
were not familiar with the foreign language, in
their case English. They used language model-
ing and translation probability to translate a user’s
query into L2, and then summarized each docu-
ment in L2 with respect to the query. In their final
step, they translated the summary from L2 back
</bodyText>
<page confidence="0.89138">
658
</page>
<bodyText confidence="0.999924869565217">
to Li for the user. They evaluated their method
on the DUC 2005 query-focused summarization
shared-task with ROUGE scores. We compare our
methods to this work also on the DUC 2005 task.
Our work demonstrates the first attempt to draw at
a comparison between user-based studies and in-
trinsic evaluation with ROUGE. However, one of
the limitations with evaluating this way is that the
shared-task documents and queries are monolin-
gual.
Bhaskar and Bandyopadhyay (2012) tried a
subjective evaluation of extractive cross-language
query biased summarization for 7 different lan-
guages. They extracted sentences, then scored and
ranked the sentences to generate query dependent
snippets of documents for their cross lingual in-
formation access (CLIA) system. However, the
snippet quality was determined subjectively based
on scores on a scale of 0 to 1 (with 1 being best).
Each score indicated annotator satisfaction for a
given snippet. Our evaluation methodology is ob-
jective: we ask users to decide if a given document
is relevant to an information need, or not.
</bodyText>
<subsectionHeader confidence="0.997463">
2.2 Machine Translation Effects
</subsectionHeader>
<bodyText confidence="0.999992583333333">
Machine translation quality can affect summa-
rization quality. Wan et al. (2010) researched
the effects of MT quality prediction on cross-
language document summarization. They gener-
ated 5-sentence summaries in Chinese using En-
glish source documents. To select sentences, they
used predicted translation quality, sentence posi-
tion, and sentence informativeness. In their eval-
uation, they employed 4 Chinese-speakers to sub-
jectively rate summaries on a 5-point scale (5 be-
ing best) along the dimensions of content, read-
ability, and overall impression. They showed that
their approach of using MT quality scores did im-
prove summarization quality on average. While
their findings are important, their work did not ad-
dress query biasing or objective evaluation of the
summaries. We attempt to overcome limitations of
machine translation quality by using word clouds
as one of our summarization strategies.
Knowing when to translate is another challenge
for cross-language query biased summarization.
Several options exist for when and what to trans-
late during the summarization process: (1) the
source documents can be translated, (2) the user’s
query can be translated, (3) the final summary can
be translated, or (4) some combination of these.
An example of translating only the summaries
themselves can be found in Wan et al., (2010).
On the other hand, Pingali et al. (2007) translated
the queries and the summaries. In our work, we
used gold-translated queries from the CLEF 2008
dataset, and machine translated source documents.
We briefly address this in our work, but note that a
full discussion of when and what to translate, and
those effects on summarization quality, is outside
of the scope of this paper.
</bodyText>
<subsectionHeader confidence="0.999591">
2.3 Summarization Evaluation
</subsectionHeader>
<bodyText confidence="0.999959407407407">
There has been a lot of work towards developing
metrics for understanding what makes a summary
good. Evaluation metrics are either intrinsic or ex-
trinsic. Intrinsic metrics, such as ROUGE, mea-
sure the quality of a summary with respect to gold
human-generated summaries (Lin, 2004; Lin and
Hovy, 2003). Generating gold standard summaries
is expensive and time-consuming, a problem that
persists with cross-language query biased summa-
rization because those summaries must be query
biased as well as in a different language from the
source documents.
On the other hand, extrinsic metrics measure the
quality of summaries at the system level, by look-
ing at overall system performance on downstream
tasks (Jing et al, 1998; Tombros and Sanderson,
1998). One of the most important findings for
query biased summarization comes from Tombros
and Sanderson (1998). In their monolingual task-
based evaluation, they measured user speed and
accuracy at identifying relevant documents. They
found that query biased summarization improved
the user speed and accuracy when the user was
asked to make relevance judgements for IR tasks.
We also expect that our evaluation will demon-
strate that user speed and accuracy is better when
summaries are query biased.
</bodyText>
<sectionHeader confidence="0.928011" genericHeader="method">
3 Data and Pre-Processing
</sectionHeader>
<bodyText confidence="0.9998458">
We used data from the Farsi CLEF 2008 ad hoc
task (Agirre et al., 2009). Each of the queries in-
cluded in this dataset consisted of a title, narrative,
and description. Figure 1 shows an example of the
elements of a CLEF 2008 query. All of the au-
tomatic query-biasing in this work was based on
the query titles. For our human relevance predic-
tion task on Mechanical Turk, we used the nar-
rative version. The CLEF 2008 dataset included
a ground-truth answer key indicating which docu-
</bodyText>
<page confidence="0.799892">
659
</page>
<bodyText confidence="0.999430555555556">
ments were relevant to each query. For each query,
we randomly selected 5 documents that were rele-
vant as well as 5 documents that were not relevant.
The subset of CLEF 2008 data that we used there-
fore consisted of 500 original Farsi documents and
50 parallel English-Farsi queries. Next we will de-
scribe our text pre-processing steps for both lan-
guages as well as how we created our parallel En-
glish documents.
</bodyText>
<figureCaption confidence="0.563884">
Figure 1: Full MT English summary and CLEF
2008 English query (title, description, narrative).
</figureCaption>
<subsectionHeader confidence="0.998069">
3.1 English Documents
</subsectionHeader>
<bodyText confidence="0.9999806">
All of our English documents were created auto-
matically by translating the original Farsi docu-
ments into English (Drexler et al., 2012). The
translated documents were sentence-aligned with
one sentence per line. For all of our summariza-
tion experiments (except unbised full MT text),
we processed the text as follows: removed extra
spaces, removed punctuation, folded to lowercase,
and removed digits. We also removed common
English stopwords2 from the texts.
</bodyText>
<subsectionHeader confidence="0.999296">
3.2 Farsi Documents
</subsectionHeader>
<bodyText confidence="0.999983">
We used the original CLEF 2008 Farsi docu-
ments for two of our summarization methods. We
stemmed words in each document using automatic
morphological analysis with Morfessor CatMAP.
We note that within-sentence punctuation was re-
moved during this process (Creutz and Lagus,
2007). We also removed Farsi stopwords and dig-
its.
</bodyText>
<sectionHeader confidence="0.936567" genericHeader="method">
4 Summarization Strategies
</sectionHeader>
<bodyText confidence="0.978311">
All of our summarization methods were extrac-
tive except for unbiased full machine translated
text. In this section, we describe each of our
13 summarization methods which we have orga-
nized into one of the following strategies: (1) un-
biased full machine translated text, (2) unbiased
2English and Farsi stopword lists from:
http://members.unine.ch/jacques.savoy/clef/index.html
word cloud summaries, (3) query biased word
cloud summaries, and (4) query biased sentence
summaries. Regardless of which summarization
method used, we highlighted words in yellow that
also appeard in the query. Let t be a term in
document d where d ∈ DL and DL is a collec-
tion of documents in a particular language. Note
that for our summarization methods, term weight-
ings were calculated separately for each language.
While |D |= 1000, we calculated term weightings
based on |DE |= 500 and |DF |= 500. Finally,
let q be a query where q ∈ Q and Q is our set of
50 parallel English-Farsi CLEF queries. Assume
that log refers to logio.
Figure 2: Full MT English summary and CLEF
2008 English query.
</bodyText>
<subsectionHeader confidence="0.949381">
4.1 Unbiased Full Machine Translated
English
</subsectionHeader>
<bodyText confidence="0.999988230769231">
Our first baseline approach was to use all of the
raw machine translation output (no subsets of
the sentences were used). Each summary there-
fore consisted of the full text of an entire doc-
ument automatically translated from Farsi to En-
glish (Drexler et al., 2012). Figure 2 shows an ex-
ample full text document translated from Farsi to
English and a gold-standard English CLEF query.
Note that we use this particular document-query
pair as an example throughout this paper (docu-
ment: H-770622-42472S8, query: 10.2452/552-
AH). According to the CLEF answer key, the sam-
ple document is relevant to the sample query.
</bodyText>
<subsectionHeader confidence="0.993799">
4.2 Unbiased Word Clouds
</subsectionHeader>
<bodyText confidence="0.99857175">
For our second baseline approach, we ranked
terms in a document and displayed them as word
clouds. Word clouds are one a way to arrange
a collection of words where each word can vary
</bodyText>
<page confidence="0.45416">
660
</page>
<bodyText confidence="0.953046473684211">
in size. We used word clouds as a summariza-
tion strategy to overcome any potential disfluen-
cies from the machine translation output and also
to see if they are feasible at all for summarization.
All of our methods for word clouds used words
from machine translated English text. Each term-
ranking method below generates different ranked
lists of terms, which we used to create different
word clouds. We created one word cloud per doc-
ument using the top 12 ranked words. We used
the raw term scores to scale text font size, so that
words with a highter score appeared larger and
more prominent in a word cloud. Words were
shuffled such that the exact ordering of words was
at random.
I: Term Frequency (TF) Term frequency is
very commonly used for finding important terms
in a document. Given a term t in a document d,
the number of times that term occurs is:
</bodyText>
<equation confidence="0.910931">
tft,d = |t E d|
</equation>
<bodyText confidence="0.996518">
II: Inverse Document Frequency (IDF) The
idf term weighting is typically used in IR and
other text categorization tasks to make distinc-
tions between documents. The version of idf that
we used throughout our work came from Erkan
and Radev (2004) and Otterbacher et al. (2009),
in keeping consistent with theirs. Let N be the
number of documents in the collection, such that
N = |D |and nt is the number of documents that
contain term t, such that nt = |{d E D : t E d}|,
</bodyText>
<equation confidence="0.867787333333333">
then:
N + 1
idft =log 0.5 x nt
</equation>
<bodyText confidence="0.994869166666667">
While idf is usually thought of as a type of
heuristic, there have been some discussions about
its theoretical basis (Robertson, 2004; Robertson
and Walker, 1994; Church and Gale, 1999; Salton
and Yang, 1973). An example of this summary is
shown in Figure 3.
</bodyText>
<listItem confidence="0.868691">
III: Term Frequency Inverse Document Fre-
quency (TFIDF) We use tfidft,d term weight-
ing to find terms which are both rare and impor-
tant for a document, with respect to terms across
all other documents in the collection:
</listItem>
<equation confidence="0.89692">
tfidft,d = tft,d x idft
</equation>
<subsectionHeader confidence="0.990735">
4.3 Query Biased Word Clouds
</subsectionHeader>
<bodyText confidence="0.995182133333333">
We generated query biased word clouds following
the same principles as our unbiased word clouds,
Figure 3: Word cloud summary for inverse docu-
ment frequency (IDF), for query “Tehran’s stock
market”.
namely the text font scaling and highlighting re-
mained the same.
IV. Query Biased Term Frequency (TFQ) In
Figure 4 we show a sample word cloud summary
based on query biased term frequency. We define
query biased term frequency tfQ at the document
level, as:
Figure 4: Word cloud summary for query biased
term frequency (TFQ), for query “Tehran’s stock
market”.
</bodyText>
<listItem confidence="0.9874154">
V. Query Biased Inverse Document Frequency
(IDFQ) Since idf helps with identifying terms
that discriminate documents in a collection, we
would expect that query biased idf would help to
identify documents that are relevant to a query:
</listItem>
<equation confidence="0.9033985">
�
2idft, if t E q
idfQt,q =
idft, otherwise
</equation>
<listItem confidence="0.642436">
VI. Query Biased TFIDF (TFIDFQ) We de-
fine query biased tf x idf similarly to our TFQ
and IDFQ, at the document level:
</listItem>
<figure confidence="0.6316688">
�
2tft,d x idft, if t E q
tfidfQt,d,q =
tft,d x idft, otherwise
�
tft,d, otherwi
tfQt,d,q =
se
2tft,d, if t E q
661
</figure>
<figureCaption confidence="0.993270666666667">
Figure 5: Word cloud summary for scaled query
biased term frequency (SFQ) for query “Tehran’s
stock market”.
</figureCaption>
<bodyText confidence="0.981011666666667">
VII. Query Biased Scaled Frequency (SFQ)
This term weighting scheme, which we call scaled
query biased term frequency or sfQ, is a variant of
the traditional tf xidf weighting. First, we project
the usual term frequency into log-space, for a term
t in document d with:
</bodyText>
<equation confidence="0.954322">
tfSt,d = log(tft,d)
</equation>
<bodyText confidence="0.913840142857143">
We let tfSt,d Pz� 0 when tft,d = 1. We believe that
singleton terms in a document provide no indica-
tion that a document is query-relevant, and trea-
ment of singleton terms in this way would have the
potential to reduce false-positives in our relevance
prediction task. Note that scaled term frequency
differs from Robertson’s (2004) inverse total term
frequency in the sense that our method involves no
consideration of term position within a document.
Scaled query biased term frequency, shown in Fig-
ure 5, is defined as:
2t f St ,d x idft, if t E q
s f Qt,d,q =(tfSt,d x idft, otherwise
VIII. Word Relevance (W) We adapted an
existing relevance weighting from Allan et al.,
(2003), that was originally formulated for ranking
sentences with respect to a query. However, we
modified their originaly ranking method so that we
could rank individual terms in a document instead
of sentences. Our method for word relevance, W
is defined as:
</bodyText>
<equation confidence="0.864128">
Wt,d,q = log(tft,d + 1) x log(tft,q + 1) x idft
</equation>
<bodyText confidence="0.996483166666667">
In W, term frequency values are smoothed by
adding 1. The smoothing could especially af-
fect rare terms and singletons, when tft,d is very
low. All terms in a query or a document will
be weighted and each term could potentially con-
tribute to summary.
</bodyText>
<subsectionHeader confidence="0.996967">
4.4 Query Biased Sentence Summaries
</subsectionHeader>
<bodyText confidence="0.95737145">
Sentences are a canonical unit to use in extractive
summaries. In this section we describe four differ-
ent sentence scoring methods that we used. These
methods show how to calculate sentence scores for
a given document with respect to a given query.
Sentences for a document were always ranked us-
ing the raw score value output generated from a
scoring method. Each document summary con-
tained the top 3 ranked sentences where the sen-
tences were simply listed out. Each of these meth-
ods used sentence-aligned English machine trans-
lated documents, and two of them also used the
original Farsi text.
IX. Sentence Relevance (REL) Our sentence
relevance scoring method comes from Allan et al.
(2003). The sentence weight is a summation over
words that appear in the query. We provide their
sentence scoring formula here. This calculates the
relevance score for a sentence s from document d,
to a query q:
</bodyText>
<equation confidence="0.9330245">
Xrel(s|q) = log(tft,s +1)xlog(tft,q +1)xidft
tEs
</equation>
<bodyText confidence="0.88808875">
Terms will occur in either the sentence or the
query, or both. We applied this method to machine
tranlsated English text. The output of this method
is a relevance score for each sentence in a given
document. We used those scores to rank sentences
in each document from our English machine trans-
lated text.
X. Query Biased Lexrank (LQ) We imple-
mented query biased LexRank, a well-known
graph-based summarization method (Otterbacher
et al., 2009). It is a modified version of the orig-
inal LexRank algorithm (Erkan and Radev, 2004;
Page et al., 1998). The similarity metric, simx,y,
also known as idf-modified cosine similarity, mea-
sures the distance between two sentences x and y
in a document d, defined as:
</bodyText>
<equation confidence="0.947121">
PtEx,y tft,x x tft,y x (idft)2
simx,y =
q&amp;Ex tf idft,xq&amp;Ey t f idft y
</equation>
<bodyText confidence="0.962955">
We used simx,y to score the similarity of
sentence-to-sentence, resulting in a similarity
</bodyText>
<figure confidence="0.409584">
662
</figure>
<figureCaption confidence="0.8970694">
Figure 6: LQP - projecting Farsi sentence scores
onto parallel English sentences.
Figure 7: LQC - Farsi sentence scores are com-
bined with parallel English sentence scores to ob-
tain sentence re-ranking.
</figureCaption>
<bodyText confidence="0.998758375">
graph where each vertex was a sentence and each
edge was the cosine similarity between sentences.
We normalized the cosine matrix with a similarity
threshold (t = 0.05), so that sentences above this
threshold were given similarity 1, and 0 otherwise.
We used rel(s|q) to score sentence-to-query. The
LexRank score for each sentence was then calcu-
lated as:
</bodyText>
<equation confidence="0.9841186">
d × rels|q
LQs|q = + (1 − d) ×
Ez∈C relz|q
sims,v LQv
Er∈adj [v] simv,r q
</equation>
<bodyText confidence="0.996299960784314">
where C is the set of all sentences in a given doc-
ument. Here the parameter d is just a damper to
designate a probability of randomly jumping to
one of the sentences in the graph (d = 0.7). We
found the stationary distribution by applying the
power method (E = 5), which is guaranteed to
converge to a stationary distribution (Otterbacher
et al., 2009). The output of LQ is a score for each
sentence from a given document with respect to
a query. We used that score to rank sentences in
each document from our English machine trans-
lated text.
XI. Projected Cross-Language Query Biased
Lexrank (LQP) We introduce LQP to describe
a way of scoring and ranking sentences such that
the L1 (English) summaries are biased from the
L2 (Farsi) query and source document. Our gold-
standard Farsi queries were included with our
CLEF 2008 data, making them more reliable than
what we could get from automatic translation.
First, sentences from each Farsi document were
scored with Farsi queries using LQ, described
above. Then each LQ score was projected onto
sentence-aligned English. We demonstrate LQP
in Figure 6. By doing this, we simulated trans-
lating the user’s English query into Farsi with the
best possible query translation, before proceed-
ing with summarization. This approach to cross-
language summarization could be of interest for
CLIR systems that do query translation on-the-fly.
It is also of interest for summarization systems that
need to utilize previously translated source docu-
ments the capability is lacking to translate sum-
maries from L2 to L1.
XII. Combinatory Query Biased Lexrank
(LQC) Another variation of LexRank that we
introduce in this work is LQC, which combines
LexRank scores from both languages to re-rank
sentences. A visual summary of this method is
shown in Figure 7. We accomplished our re-
ranking by first running LQ on Farsi and English
separately, then adding the two scores together.
This combination of Farsi and English scores pro-
vided us with a different way to score and rank
sentences, compared with LQ and LQP. The
idea behind combinatory query biased LexRank
is to take advantage of sentences which are high-
ranking in Farsi but not in English. The LQC
method exploits all available resources in our
dataset: L1 and L2 queries as well as L1 and L2
documents.
</bodyText>
<sectionHeader confidence="0.999413" genericHeader="method">
5 Experiments
</sectionHeader>
<bodyText confidence="0.999968111111111">
We tested each of our summarization methods and
overall strategies in a task-based evaluation frame-
work using relevance prediction. We used Me-
chanical Turk for our experiments since it has been
shown to be useful for evaluating NLP systems
(Callison-Burch 2009; Gillick and Liu, 2010). We
obtained human judgments for whether or not a
document was considered relevant to a query, or
information need. We measured the relevance
</bodyText>
<figure confidence="0.5386445">
�
v∈adj[s]
</figure>
<page confidence="0.483144">
663
</page>
<bodyText confidence="0.965778333333333">
judgements by precision/recall/F1, accuracy, and
also time-on-task based on the average response
time per Human Intelligence Task (HIT).
</bodyText>
<subsectionHeader confidence="0.98479">
5.1 Mechanical Turk
</subsectionHeader>
<bodyText confidence="0.999978344827587">
In our Mechanical Turk experiment, we used ter-
minology from CLEF 2008 to describe a query
as an “information need”. All of the Mechanical
Turk workers were presented with the following
for their individual HIT: instructions, an informa-
tion need and one summary for a document. Work-
ers were asked to indicate if the given summary
for a document was relevant to the given informa-
tion need (Hobson et al., 2007). Workers were
not shown the original Farsi source documents.
We paid workers $0.01 per HIT. We obtained 5
HITs for each information need and summary pair.
We used a built-in approval rate qualification pro-
vided by Mechanical Turk to restrict which work-
ers could work on our tasks. Each worker had an
approval rate of at least 95
Instructions: Each image below consists
of a statement summarizing the informa-
tion you are trying to find from a set
of documents followed by a summary
of one of the documents returned when
you query the documents. Based on the
summary, choose whether you think the
document returned is relevant to the in-
formation need. NOTE: It may be diffi-
cult to distinguish whether the document
is relevant as the text may be difficult
to understand. Just use your best judg-
ment.
</bodyText>
<sectionHeader confidence="0.999166" genericHeader="method">
6 Results and Analysis
</sectionHeader>
<bodyText confidence="0.999998625">
We present our experiment results and additional
analysis. First, we report the results of our rel-
evance prediction task, showing performance for
individual summarization methods as well as per-
formance for the overall strategies. Then we
show analysis of our results from the monolin-
gual question-biased shared-task for DUC 2005,
as well as a comparison to previous work.
</bodyText>
<sectionHeader confidence="0.702034" genericHeader="method">
6.1 Results for Individual Methods
</sectionHeader>
<bodyText confidence="0.999961415094339">
Our results are shown in Table 1. We report perfor-
mance for 13 individual methods as well as over-
all peformance on the 4 different summarization
strategies. To calculate the performance for each
strategy, we used the arithmetic mean of the corre-
sponding individual methods. We measured preci-
sion, recall and F1 to give us a sense of our sum-
maries might influence document retrieval in an
actual CLIR system. We also measured accuracy
and time-on-task. For these latter two metrics, we
distinguish between summaries that were relevant
(R) and non-relevant (NR).
All of the summarization-based methods fa-
vored recall over precision: documents were
marked ‘relevant’ more often than ‘non-relevant’.
For many of the methods shown in Table 1, work-
ers spent more time correctly deciding ‘relevant’
than correctly deciding ‘non-relevant’. This sug-
gests some workers participated in our Mechanical
Turk task purposefully. For many of the summa-
rization methods, workers were able to positively
identify relevant documents.
From Table 1 we see that Full MT performed
better on precision than all of the other methods
and strategies, but we note that performance on
precision was generally very low. This might be
due to Mechanical Turk workers overgeneraliz-
ing by marking summaries as relevant when they
were not. Some individual methods preserve our
principle of retrieval-relevance, as indicated by
the higher recall scores for SQF, LQEF, and TFQ.
That is to say, these particular query biased sum-
marization methods can be used to assist users
with identifying more relevant documents. The ac-
curacy on relevant documents addresses our prin-
ciple of query-salience, and it is especially high
for our query-biased methods: LQEF, SQF, LQ,
and TFQ. The results also seem to fit our intuition
that the summary in Figure 3 seems less relevant
to the summaries shown in Figures 4 &amp; 5 even
though these are the same documents biased on
the same query “Tehran stock market”.
Overall, query biased word clouds outperform
the other summarization strategies for 5 out of
7 metrics. This could be due to the fact that
word clouds provide a very concise and overview
of a document, which is one of the main goals
for automatic summarization. Along these lines,
word clouds are probably not subject to the effects
of MT quality and we believe it is possible that
MT quality could have had a negative impact on
our query biased extracted sentence summaries, as
well as our full MT English texts.
</bodyText>
<page confidence="0.938705">
664
</page>
<tableCaption confidence="0.9886265">
Table 1: Individual method results: precision/recall/F1, time-on-task, and accuracy. Note that results for
time-on-task and accuracy scores are distinguished for relevant (R) and non-relevant (NR) documents.
</tableCaption>
<table confidence="0.999963611111111">
Summarization Strategy Precision, Recall, F1 Time-on-Task Accuracy
Prec. Rec. F1 R NR R NR
Unbiased Full MT English 0.653 0.636 0.644 219.5 77.6 0.696 0.712
TF 0.615 0.777 0.686 33.5 34.6 0.840 0.508
IDF 0.537 0.470 0.501 84.7 45.8 0.444 0.700
TFIDF 0.647 0.710 0.677 33.2 38.2 0.772 0.656
Unbiased Word Clouds 0.599 0.652 0.621 50.5 39.5 0.685 0.621
TFQ 0.605 0.809 0.692 55.3 82.4 0.864 0.436
IDFQ 0.582 0.793 0.671 23.6 31.6 0.844 0.436
TFIDFQ 0.599 0.738 0.661 37.9 26.9 0.804 0.500
SFQ 0.591 0.813 0.685 55.7 49.4 0.876 0.504
W 0.611 0.738 0.669 28.2 28.9 0.840 0.564
Query Biased Word Clouds 0.597 0.778 0.675 36.4 34.2 0.846 0.488
REL 0.582 0.746 0.654 30.6 44.3 0.832 0.548
LQ 0.549 0.783 0.646 64.4 54.8 0.868 0.292
LQP 0.578 0.734 0.647 28.2 28.0 0.768 0.472
LQC 0.557 0.810 0.660 33.9 38.8 0.896 0.292
Query Biased Sentences 0.566 0.768 0.651 39.2 41.5 0.841 0.401
</table>
<tableCaption confidence="0.9574555">
Table 2: Comparison of peer systems on DUC
2005 shared-task for monolingual question-biased
summarization, f-scores from ROUGE-2 and
ROUGE-SU4.
</tableCaption>
<table confidence="0.999159428571429">
Peer ID ROUGE-2 ROUGE-SU4
17 0.07170 0.12970
8 0.06960 0.12790
4 0.06850 0.12770
Tel-Eng-Sum 0.06048 0.12058
LQ 0.05124 0.09343
REL 0.04914 0.09081
</table>
<subsectionHeader confidence="0.999814">
6.2 Analysis with DUC 2005
</subsectionHeader>
<bodyText confidence="0.999923411764706">
We analysed our summarization methods by
comparing two of our sentence-based methods
(LQ and REL) with peers from the monolin-
gual question-biased summarization shared-task
for DUC 2005. Even though DUC 2005 is a mono-
lingual task, we decided to use it as part of our
analysis for two reasons: (1) to see how well we
could do with query/question biasing while ignor-
ing the variables introduced by MT and cross-
language text, and (2) to make a comparison to
previous work. Pingali et al., (2007) also used this
the same DUC task to assess their cross-language
query biased summarization system. Systems
from the DUC 2005 question-biased summariza-
tion task were evaluated automatically against hu-
man gold-standard summaries using ROUGE (Lin
and Hovy, 2003) . Our results from the DUC
2005 shared-task are shown in Table 2, reported
as ROUGE-2 and ROUGE-SU4 f-scores, as these
two variations of ROUGE are the most helpful
(Dang, 2005; Pingali et al., 2007).
Table 2 shows scores for several top peer sys-
tems, as well as results for the Tel-Eng-Sum
method from Pingali et al., (2007). While we have
reported f-scores in our analysis, we also note that
our implementations of LQ and REL outperform
all of the DUC 2005 peer systems for precision, as
shown in Table 3. We also know that ROUGE can-
not be used for comparing sentence summaries to
ranked lists of words and there are no existing in-
trinsic methods to make that kind of comparison.
Therefore we were able to successfully compare
just 2 of our sentence-based methods to previous
work using ROUGE.
</bodyText>
<sectionHeader confidence="0.981317" genericHeader="discussions">
7 Discussion and Future Work
</sectionHeader>
<bodyText confidence="0.999464">
Cross-language query biased summarization is an
important part of CLIR, because it helps the user
decide which foreign-language documents they
might want to read. But, how do we know if
</bodyText>
<page confidence="0.926381">
665
</page>
<tableCaption confidence="0.978139">
Table 3: Top 3 system precision scores for
ROUGE-2 and ROUGE-SU4.
</tableCaption>
<table confidence="0.9971055">
Peer ID ROUGE-2 ROUGE-SU4
LQ 0.08272 0.15197
REL 0.0809 0.15049
15 0.07249 0.13129
</table>
<bodyText confidence="0.998124234375001">
a query biased summary is “good enough” to be
used in a real-world CLIR system? We want to
be able to say that we can do query biased sum-
marization just as well for both monolingual and
cross-language IR systems. From previous work,
there has been some variability with regard to
when and what to translate - variables which have
no impact on monolingual summarization. We at-
tempted to address this issue with two of our meth-
ods: LQP and LQC. To fully exploit the MT vari-
able, we would need many more relevance pre-
diction experiments using humans who know L1
and others who know L2. Unfortunately in our
case, we were not able to find Farsi speakers on
Mechanical Turk. Access to these speakers would
have allowed us to try further experiments as well
as other kinds of analysis.
Our results on the relevance prediction task
tell us that query biased summarization strategies
help users identify relevant documents faster and
with better accuracy than unbiased summaries.
Our findings support the findings of Tombros and
Sanderson (1998). Another important finding is
that now we can weigh tradeoffs so that different
summarization methods could be used to optimize
over different metrics. For example, if we want
to optimize for retrieval-relevance we might select
a summarization method that tends to have higher
recall, such as scaled query biased term frequency
(SFQ). Similarly, we could optimize over accu-
racy on relevant documents, and use Combinatory
LexRank (LQC) with Farsi and English together.
We have shown that the relevance prediction
tasks can be crowdsourced on Mechanical Turk
with reasonable results. The data we used from
the Farsi CLEF 2008 ad-hoc task included an an-
swer key, but there were no parallel English docu-
ments. However, in order for the NLP community
to make strides in evaluating cross-language query
biased summarization for CLIR, we will need star-
dards and data. Optimal data would be parallel
datasets consisting of documents in L1 and L2
with queries in L1 and L2 along with an answer
key specifying which documents are relevant to
the queries. Further we would also need sets of
human gold-standard query biased summaries in
L1 and L2. These standards and data would al-
low us to compare method-to-method across dif-
ferent languages, while simultaneously allowing
us to tease apart other variables such as: when and
what to translate, translation quality, methods for
biasing, and type of summarization strategy (sen-
tences, words, etc). And of course it would be bet-
ter if this standard dataset was multilingual instead
of billingual, for obvious reasons.
We have approached cross-language query bi-
ased summarization as a stand-alone problem,
treating the CLIR system and document retrieval
as a black box. However, summaries need to pre-
serve query-salience: summaries should not make
it more difficult to positively identify relavant doc-
uments. And they should also preserve retrieval-
relevance: summaries should help users identify
as many relevant documents as possible.
</bodyText>
<sectionHeader confidence="0.985218" genericHeader="acknowledgments">
Acknowledgments
</sectionHeader>
<bodyText confidence="0.999966833333333">
We would like to express thanks to David Har-
wath at MIT Computer Science and Artificial In-
telligence Laboratory (CSAIL), who helped us de-
velop and implement ideas in this paper. We also
want to thank Terry Gleason from MIT Lincoln
Laboratory for providing machine translations.
</bodyText>
<sectionHeader confidence="0.995847" genericHeader="references">
References
</sectionHeader>
<reference confidence="0.997249409090909">
Eneko Agirre, Giorgio Maria Di Nunzio, Nicola Ferro,
Thomas Mandl, and Carol Peters. CLEF 2008: Ad
hoc track overview. In Evaluating Systems for Mul-
tilingual and Multimodal Information Access, pp
15–37. Springer Berlin Heidelberg, 2009.
James Allan, Courtney Wade, and Alvaro Bolivar. Re-
trieval and Novelty Detection at the Sentence Level.
In Proceedings of the 26th Annual International
ACM SIGIR Conference on Research and Develop-
ment in Informaion Retrieval, (SIGIR ’03). ACM,
New York, NY, USA, 314-321.
Hosein Azarbonyad, Azadeh Shakery, and Heshaam
Faili. Exploiting Multiple Translation Resources for
English-Persian Cross Language Information Re-
trieval. In P. Forner, H. M¨uller, R. Paredes, P. Rosso,
and B. Stein, editors, Information Access Evalua-
tion. Multilinguality, Multimodality, and Visualiza-
tion, volume 8138 of Lecture Notes in Computer Sci-
ence, pp 93–99. Springer Berlin Heidelberg, 2013.
Lorena Leal Bando, Falk Scholer, Andrew Turpin.
Constructing Query-biased Summaries: A Compar-
ison of Human and System Generated Snippets. In
</reference>
<page confidence="0.75881">
666
</page>
<reference confidence="0.997123719626168">
Proceedings of the Third Symposium on Information
Interaction in Context (IIiX ’10), ACM 2010, New
York, NY, USA, 195-204.
Adam Berger and Vibhu O Mittal. Query-Relevant
Summarization Using FAQs. In Proceedings of the
38th Annual Meeting on Association for Computa-
tional Linguistics, ACL 2000.
Pinaki Bhaskar and Sivaji Bandyopadhyay. Cross-
Lingual Query Dependent Snippet Generation. In-
ternational Journal of Computer Science and Infor-
mation Technology (IJCSIT), 3(4), 2012.
Pinaki Bhaskar and Sivaji Bandyopadhyay. Language
Independent Query Focused Snippet Generation. In
T. Catarci, P. Forner, D. Hiemstra, A. Pe˜nas, and
G. Santucci, editors, Information Access Evaluation.
Multilinguality, Multimodality, and Visual Analytics,
volume 7488 of Lecture Notes in Computer Science,
pp 138–140. Springer Berlin Heidelberg, 2012.
Stephen P. Borgatti, Kathleen M. Carley, David Krack-
hardt. On the Robustness of Centrality Measures
Under Conditions of Imperfect Data. Social Net-
works, (28):124–136, 2006.
Florian Boudin, St´ephane Huet, and Juan-Manuel
Torres-Moreno. A Graph-Based Approach to Cross-
Language Multi-Document Summarization. Poli-
bits, (43):113–118, 2011.
Chris Callison-Burch. Fast, Cheap, and Creative: Eval-
uating Translation Quality Using Amazon’s Me-
chanical Turk. In Proceedings of the 2009 Con-
ference on Empirical Methods in Natural Language
Processing (EMNLP), pp 286–295, Singapore, ACL
2009.
Yllias Chali and Shafiq R. Joty. Unsupervised Ap-
proach for Selecting Sentences in Query-Based
Summarization. In Proceedings of the Twenty-First
International FLAIRS Conference, 2008.
Niladri Chatterjee, Amol Mittal, and Shubham Goyal.
Single Document Extractive Text Summarization
Using Genetic Algorithms. In Emerging Applica-
tions of Information Technology (EAIT), 2012 Third
International Conference, pp 19–23, 2012.
Kenneth W. Church and William A. Gale. Inverse Doc-
ument Frequency (IDF): A Measure of Deviations
From Poisson. In Natural language processing us-
ing very large corpora, pages 283–295. Springer,
1999.
Mathias Creutz and Krista Lagus. Unsupervised Mod-
els for Morpheme Segmentation and Morphology
Learning. ACM Transactions on Speech and Lan-
guage Processing, 4(1):3:1–3:34, February 2007.
Hoa Trang Dang. Overview of DUC 2005. In Pro-
ceedings of the Document Understanding Confer-
ence, 2005.
Hal Daum´e III, Daniel Marcu. Bayesian Query-
Focused Summarization. In Proceedings of the 21st
International Conference on Computational Lin-
guistics and the 44th annual meeting of the Asso-
ciation for Computational Linguistics, ACL 2006.
Jennifer Drexler, Wade Shen, Terry P. Gleason, Timo-
thy R. Anderson, Raymond E. Slyh, Brian M. Ore,
and Eric G. Hansen. The MIT-LL/AFRL IWSLT-
2012 MT System. In Proceedings of the Interna-
tional Workshop on Spoken Language Translation
(IWSLT), Hong Kong, December 2012.
Bonnie J. Dorr, Christof Monz, Stacy President,
Richard Schwartz, and David Zajic. A Methodol-
ogy for Extrinsic Evaluation of Text Summarization:
Does ROUGE Correlate? In Proceedings of the
ACL Workshop on Intrinsic and Extrinsic Evalua-
tion Measures for Machine Translation and/or Sum-
marization, pp 1-8. Ann Arbor, ACL 2005.
H. P. Edmundson. New Methods in Automatic Extract-
ing. In Journal of the ACM, 16(2):264–285, April
1969.
G¨unes¸ Erkan and Dragomir R. Radev. Lexrank:
Graph-based lexical centrality as salience in text
summarization. Journal of Artificial Intelligence
Research, 22(1):457–479, December 2004.
Dan Gillick and Yang Liu. Non-Expert Evaluation
of Summarization Systems is Risky. In Proceed-
ings of NAACL HLT 2010 Workshop on Creating
Speech and Language Data with Amazon’s Mechan-
ical Turk, pp 148-151, Los Angeles, California,
USA, June, 2010.
David Harwath and Timothy J. Hazen. Topic Identi-
fication Based Extrinsic Evaluation of Summariza-
tion Techniques Applied to Conversational Speech.
In Proceedings of ICASSP, 2012: 5073-5076.
Stacy P. Hobson, Bonnie J. Dorr, Christof Monz, and
Richard Schwartz. Task-Eased Evaluation of Text
Summarization Using Relevance Prediction. In In-
formation Processing Management, 43(6): 1482-
1499, 2007.
Hongyan Jing, Regina Barzilay, Kathleen McKeown,
and Michael Elhadad. Summarization Evaluation
Methods: Experiments and Analysis. In Proceed-
ings of American Association for Artificial Ingelli-
gence (AAAI), 1998.
Reza Karimpour, Amineh Ghorbani, Azadeh Pishdad,
Mitra Mohtarami, Abolfazl AleAhmad, Hadi Amiri,
and Farhad Oroumchian. Improving Persian Infor-
mation Retrieval Systems Using Stemming and Part
of Speech Tagging. In Proceedings of the 9th Cross-
language Evaluation Forum Conference on Evaluat-
ing Systems for Multilingual and Multimodal Infor-
mation Access, CLEF 2008, pp 89–96, Berlin, Hei-
delberg, 2009. Springer-Verlag.
</reference>
<page confidence="0.617962">
667
</page>
<reference confidence="0.999197333333334">
Chin-Yew Lin. Looking For A Few Good Metrics:
Automatic Summarization Evaluation - How Many
Samples Are Enough? In Proceedings of NTCIR
Workshop 4, Tokyo, Japan, June 2004.
Annie Louis and Ani Nenkova. Automatic Summary
Evaluation without Human Models. In Proceedings
of Empirical Methods in Natural Language Process-
ing, EMNLP 2009.
H. P. Luhn. The Automatic Creation of Literature Ab-
stracts. IBM Journal of Research and Development,
2(2):159–165, April 1958.
Inderjeet Mani, Eric Bloedorn, and Barbara Gates. Us-
ing Cohesion and Coherence Models for Text Sum-
marization. In AAAI Symposium Technical Report
SS-989-06, AAAI Press, 69–76, 1998.
Inderjeet Mani, David House, Gary Klein, Lynette
Hirschman, Therese Firmin, and Beth Sundheim.
The TIPSTER SUMMAC Text Summarization
Evaluation. In Proceedings of European Associa-
tion for Coputational Linguistics, EACL 1999.
Inderjeet Mani. Summarization Evaluation: An
Overview. In Proceedings of the NTCIR Workshop,
Vol. 2, 2001.
Inderjeet Mani, Gary Klein, David House, Lynette
Hirschman, Therese Firmin, and Beth Sundheim.
SUMMAC: A Text Summarization Evaluation. Nat-
ural Language Engineering, 8(1) 43-68. March
2002.
Kathleen McKeown, Rebecca J. Passonneau, David K.
Elson, Ani Nenkova, and Julia Hirschberg. Do Sum-
maries Help? A Task-Based Evaluation of Multi-
Document Summarization. In Proceedings of the
28th Annual International ACM SIGIR Conference
on Research and Development in Information Re-
trieval, pp 210-217. ACM 2005.
Anthony McCallum, Gerald Penn, Cosmin Munteanu,
and Xiaodan Zhu. Ecological Validity and the Eval-
uation of Speech Summarization Quality. In Pro-
ceedings of Workshop on Evaluation Metrics and
System Comparison for Automatic Summarization.
2012 Association for Computational Linguistics,
Stroudsburg, PA, USA, 28-35.
Tatsunori Mori, Masanori Nozawa, and Yoshiaki
Asada. Multi-Answer Focused Multi-Document
Summarization Using a Question-Answering En-
gine. In Proceedings of the 20th International
Conference on Computational Linguistics, COLING
’04, Stroudsburg, PA, USA, ACL 2004.
Gabriel Murray, Thomas Kleinbauer, Peter Poller,
Tilman Becker, Steve Renals, and Jonathan Kilgour.
Extrinsic Summarization Evaluation: A Decision
Audit Task. ACM Transactions on Speech and Lan-
guage Processing, 6(2) Article 2, October 2009.
Ani Nenkova and Kathleen McKeown. A Survey of
Text Summarization Techniques. In C. C. Aggarwal
and C. Zhai, editors, Mining Text Data, pp 43–76.
Springer US, 2012.
Constantin Or˘asan and Oana Andreea Chiorean. Eval-
uation of a Cross-Lingual Romanian-English Multi-
Document Summariser. In Proceedings of Lan-
guage Resources and Evaluation Conference, LREC
2008.
Jahna Otterbacher, G¨unes¸ Erkan, and Dragomir R
Ravev. Using Random Walks for Question-focused
Sentence Retrieval. In Proceedings of Human Lan-
guage Technology Conference on Empirical Meth-
ods in Natural Language Processing, Vancouver,
Canada, pp 915-922, EMNLP 2005.
Jahna Otterbacher, G¨unes¸ Erkan, and Dragomir R.
Ravev. Biased LexRank: Passage Retrieval Using
Random Walks With Question-Based Priors. In In-
formation Processing Management, 45(1), January
2009, pp 42-54.
Karolina Owczarzak, John M. Conroy, Hoa Trang
Dang, and Ani Nenkova. An Assessment of the Ac-
curacy of Automatic Evaluation in Summarization.
In Proceedings of the Workshop on Evaluation Met-
rics and System Comparison for Automatic Summa-
rization, pp 1-9, Montr´eal, Canada, ACL 2012.
Lawrence Page, Sergey Brin, Rajeev Motwani, and
Terry Winograd. The Pagerank Citation Ranking:
Bringing Order to the Web. Technical report, Stan-
ford Digital Library Technologies Project, 1998.
Prasad Pingali, Jagadeesh Jagarlamudi, and Vasudeva
Varma. Experiments in Cross Language Query Fo-
cused Multi-Document Summarization In Work-
shop on Cross Lingual Information Access Address-
ing the Information Need of Multilingual Societies,
IJCAI 2007.
Stacy F. President and Bonnie J. Dorr. Text Sum-
marization Evaluation: Correlating Human Perfor-
mance on an Extrinsic Task With Automatic In-
trinsic Metrics. No. LAMP-TR-133. University of
Maryland College Park Language and Media Pro-
cessing Laboratory Institute for Advanced Computer
Studies (UMIACS), 2006.
Dragomir R. Radev, Hongyan Jing, Malgorzata Sty´s,
and Daniel Tam. Centroid-Based Summarization
of Multiple Documents. InProceedings of In-
formaion Processing Management, 40(6):919–938,
Nov. 2004.
Stephen Robertson. Understanding Inverse Docu-
ment Frequency: on Theoretical Arguments for IDF.
Journal of Documentation, 60(5):503–520, 2004.
Stephen E. Robertson and Steve Walker. Some Simple
Effective Approximations to the 2-Poisson Model
for Probabilistic Weighted Retrieval. In Proceed-
ings of the 17th annual international ACM SIGIR
</reference>
<page confidence="0.505714">
668
</page>
<reference confidence="0.999174756097561">
conference on Research and development in infor-
mation retrieval, pp 232–241. Springer-Verlag New
York, Inc., 1994.
Gerard Salton and Chung-Shu Yang. On the Specifica-
tion of Term Values in Automatic Indexing. Journal
of Documentation, 29(4):351–372, 1973.
Anastasios Tombros and Mark Sanderson. Advantages
of Query Biased Summaries in Information Re-
trieval. In Proceedings of the 21st annual interna-
tional ACM SIGIR conference on Research and de-
velopment in information retrieval, pp 2–10. ACM,
1998.
Xiaojun Wan and Jianguo Xiao. Graph-Based
Multi-Modality Learning for Topic-Focused Multi-
Document Summarization. In Proceedings of the
21st international jont conference on Artifical intel-
ligence (IJCAI’09), San Francisco, CA, USA, 1586-
1591.
Xiaojun Wan, Huiying Li, and Jianguo Xiao. Cross-
Language Document Summarization Based on Ma-
chine Translation Quality Prediction. In Proceed-
ings of the 48th Annual Meeting of the Association
for Computational Linguistics (ACL ’10). Associ-
ation for Computational Linguistics, Stroudsburg,
PA, USA, 917-926.
Xiaojun Wan, Houping Jia, Shanshan Huang, and Jian-
guo Xiao. Summarizing the Differences in Multilin-
gual News. In Proceedings of the 34th International
ACM SIGIR Conference on Research and Develop-
ment in Information Retrieval, SIGIR ’11, pp 735–
744, New York, NY, USA, 2011. ACM.
Wenpeng Yin, Yulong Pei, Fan Zhang, and Lian’en
Huang. SentTopic-MultiRank: A Novel Ranking
Model for Multi-Document Summarization. In Pro-
ceedings of COLING, pages 2977–2992, 2012.
Junlin Zhang, Le Sun, and Jinming Min. Using
the Web Corpus to Translate the Queries in Cross-
Lingual Information Retrieval. In Proceedings in
2005 IEEE International Conference on Natural
Language Processing and Knowledge Engineering,
2005, IEEE NLP-KE ’05, pp 493–498, 2005.
</reference>
<page confidence="0.931046">
669
</page>
</variant>
</algorithm>
<algorithm name="ParsHed" version="110505">
<variant no="0" confidence="0.914784">
<title confidence="0.9954435">Finding Good Enough: A Task-Based Evaluation of Query Summarization for Cross Language Information Retrieval</title>
<author confidence="0.999479">Jennifer Williams</author>
<author confidence="0.999479">Sharon Tam</author>
<author confidence="0.999479">Wade</author>
<affiliation confidence="0.993403">MIT Lincoln Laboratory Human Language Technology</affiliation>
<address confidence="0.999987">244 Wood Street, Lexington, MA 02420</address>
<email confidence="0.995413">jennifer.williams@ll.mit.edu,swade@ll.mit.edu</email>
<abstract confidence="0.997290521739131">In this paper we present our task-based evaluation of query biased summarization for cross-language information retrieval (CLIR) using relevance prediction. We describe our 13 summarization methods each from one of four summarization strategies. We show how well our methods perform using Farsi text from the CLEF 2008 shared-task, which we translated to English automtatically. We report precision/recall/F1, accuracy and time-on-task. We found that different summarization methods perform optimally for different evaluation metrics, but overall query biased word clouds are the best summarization strategy. In our analysis, we demonstrate that using the ROUGE metric on our sentence-based summaries cannot make the same kinds of distinctions as our evaluation framework does. Finally, we present our recommendations for creating muchneeded evaluation standards and datasets.</abstract>
</variant>
</algorithm>
<algorithm name="ParsCit" version="110505">
<citationList>
<citation valid="true">
<authors>
<author>Eneko Agirre</author>
<author>Giorgio Maria Di Nunzio</author>
<author>Nicola Ferro</author>
<author>Thomas Mandl</author>
<author>Carol Peters</author>
</authors>
<title>CLEF 2008: Ad hoc track overview.</title>
<date>2009</date>
<booktitle>In Evaluating Systems for Multilingual and Multimodal Information Access,</booktitle>
<pages>15--37</pages>
<publisher>Springer</publisher>
<location>Berlin Heidelberg,</location>
<marker>Agirre, Di Nunzio, Ferro, Mandl, Peters, 2009</marker>
<rawString>Eneko Agirre, Giorgio Maria Di Nunzio, Nicola Ferro, Thomas Mandl, and Carol Peters. CLEF 2008: Ad hoc track overview. In Evaluating Systems for Multilingual and Multimodal Information Access, pp 15–37. Springer Berlin Heidelberg, 2009.</rawString>
</citation>
<citation valid="false">
<authors>
<author>James Allan</author>
<author>Courtney Wade</author>
<author>Alvaro Bolivar</author>
</authors>
<title>Retrieval and Novelty Detection at the Sentence Level.</title>
<booktitle>In Proceedings of the 26th Annual International ACM SIGIR Conference on Research and Development in Informaion Retrieval, (SIGIR ’03).</booktitle>
<pages>314--321</pages>
<publisher>ACM,</publisher>
<location>New York, NY, USA,</location>
<marker>Allan, Wade, Bolivar, </marker>
<rawString>James Allan, Courtney Wade, and Alvaro Bolivar. Retrieval and Novelty Detection at the Sentence Level. In Proceedings of the 26th Annual International ACM SIGIR Conference on Research and Development in Informaion Retrieval, (SIGIR ’03). ACM, New York, NY, USA, 314-321.</rawString>
</citation>
<citation valid="true">
<authors>
<author>Hosein Azarbonyad</author>
</authors>
<title>Azadeh Shakery, and Heshaam Faili. Exploiting Multiple Translation Resources for English-Persian Cross Language Information Retrieval.</title>
<date>2013</date>
<booktitle>Information Access Evaluation. Multilinguality, Multimodality, and Visualization,</booktitle>
<volume>8138</volume>
<pages>93--99</pages>
<editor>In P. Forner, H. M¨uller, R. Paredes, P. Rosso, and B. Stein, editors,</editor>
<publisher>Springer</publisher>
<location>Berlin Heidelberg,</location>
<marker>Azarbonyad, 2013</marker>
<rawString>Hosein Azarbonyad, Azadeh Shakery, and Heshaam Faili. Exploiting Multiple Translation Resources for English-Persian Cross Language Information Retrieval. In P. Forner, H. M¨uller, R. Paredes, P. Rosso, and B. Stein, editors, Information Access Evaluation. Multilinguality, Multimodality, and Visualization, volume 8138 of Lecture Notes in Computer Science, pp 93–99. Springer Berlin Heidelberg, 2013.</rawString>
</citation>
<citation valid="true">
<authors>
<author>Lorena Leal Bando</author>
<author>Falk Scholer</author>
<author>Andrew Turpin</author>
</authors>
<title>Constructing Query-biased Summaries: A Comparison of Human and System Generated Snippets.</title>
<date></date>
<booktitle>In Proceedings of the Third Symposium on Information Interaction in Context (IIiX ’10), ACM 2010,</booktitle>
<location>New York, NY, USA,</location>
<marker>Bando, Scholer, Turpin, </marker>
<rawString>Lorena Leal Bando, Falk Scholer, Andrew Turpin. Constructing Query-biased Summaries: A Comparison of Human and System Generated Snippets. In Proceedings of the Third Symposium on Information Interaction in Context (IIiX ’10), ACM 2010, New York, NY, USA, 195-204.</rawString>
</citation>
<citation valid="true">
<authors>
<author>Adam Berger</author>
<author>Vibhu O Mittal</author>
</authors>
<title>Query-Relevant Summarization Using FAQs.</title>
<date>2000</date>
<booktitle>In Proceedings of the 38th Annual Meeting on Association for Computational Linguistics, ACL</booktitle>
<contexts>
<context position="7650" citStr="Berger and Mittal, 2000" startWordPosition="1216" endWordPosition="1219">tion content of that document (Luhn, 1958; Edmunson, 1969; Salton and Yang, 1973; Robertson and Walker, 1994; Church and Gale, 1999; Robertson, 2004). Recent work has looked at creating summaries of single and multiple documents (Radev et al., 2004; Erkan and Radev, 2004; Wan et al., 2007; Yin et al., 2012; Chatterjee et al., 2012), as well as summary evaluation (Jing et al., 1998; Tombros and Sanderson 1998; Mani et al., 1998; Mani et al., 1999; Mani, 2001; Lin and Hovy, 2003; Lin, 2004; Nenkova et al., 2007; Hobson et al., 2007; Owczarzak et al., 2012), query and topic biased summarization (Berger and Mittal, 2000; Otterbacher et al., 2005; Daume and Marcu, 2006; Chali and Joty, 2008; Otterbacher et al., 2009; Bando et al., 2010; Bhaskar and Bandyopadhyay, 2012; Harwath and Hazen, 2012; Yin et al., 2012), and summarization across languages (Pingali et al., 2007; Or˘asan and Chiorean, 2008; Wan et al., 2010; Azarbonyad et al., 2013). 2.1 Query Biased Summarization Previous work most closely related to our own comes from Pingali et al., (2007). In their work, they present their method for cross-language query biased summarization for Telugu and English. Their work was motivated by the need for people to </context>
</contexts>
<marker>Berger, Mittal, 2000</marker>
<rawString>Adam Berger and Vibhu O Mittal. Query-Relevant Summarization Using FAQs. In Proceedings of the 38th Annual Meeting on Association for Computational Linguistics, ACL 2000.</rawString>
</citation>
<citation valid="true">
<authors>
<author>Pinaki Bhaskar</author>
<author>Sivaji Bandyopadhyay</author>
</authors>
<title>CrossLingual Query Dependent Snippet Generation.</title>
<date>2012</date>
<journal>International Journal of Computer Science and Information Technology (IJCSIT),</journal>
<volume>3</volume>
<issue>4</issue>
<contexts>
<context position="1570" citStr="Bhaskar and Bandyopadhyay, 2012" startWordPosition="214" endWordPosition="218">y. In our analysis, we demonstrate that using the ROUGE metric on our sentence-based summaries cannot make the same kinds of distinctions as our evaluation framework does. Finally, we present our recommendations for creating muchneeded evaluation standards and datasets. 1 Introduction Despite many recent advances in query biased summarization for cross-language information retrieval (CLIR), there are no existing evaluation standards or datasets to make comparisons among different methods, and across different languages (Tombros and Sanderson, 1998; Pingali et al., 2007; McCallum et al., 2012; Bhaskar and Bandyopadhyay, 2012). Consider that creating this kind of summary requires familiarity with techniques from machine translation (MT), summarization, and information retrieval (IR). In this This work was sponsored by the Federal Bureau of Investigation under Air Force Contract FA8721-05-C-0002. Opinions, interpretations, conclusions, and recommendations are those of the authors and are not necessarily endorsed by the United States Government. paper, we arrive at the intersection of each of these research areas. Query biased summarization (also known as query-focused, query-relevant, and query-dependent) involves a</context>
<context position="7800" citStr="Bhaskar and Bandyopadhyay, 2012" startWordPosition="1240" endWordPosition="1243">, 2004). Recent work has looked at creating summaries of single and multiple documents (Radev et al., 2004; Erkan and Radev, 2004; Wan et al., 2007; Yin et al., 2012; Chatterjee et al., 2012), as well as summary evaluation (Jing et al., 1998; Tombros and Sanderson 1998; Mani et al., 1998; Mani et al., 1999; Mani, 2001; Lin and Hovy, 2003; Lin, 2004; Nenkova et al., 2007; Hobson et al., 2007; Owczarzak et al., 2012), query and topic biased summarization (Berger and Mittal, 2000; Otterbacher et al., 2005; Daume and Marcu, 2006; Chali and Joty, 2008; Otterbacher et al., 2009; Bando et al., 2010; Bhaskar and Bandyopadhyay, 2012; Harwath and Hazen, 2012; Yin et al., 2012), and summarization across languages (Pingali et al., 2007; Or˘asan and Chiorean, 2008; Wan et al., 2010; Azarbonyad et al., 2013). 2.1 Query Biased Summarization Previous work most closely related to our own comes from Pingali et al., (2007). In their work, they present their method for cross-language query biased summarization for Telugu and English. Their work was motivated by the need for people to have access to foreign-language documents from a search engine even though the users were not familiar with the foreign language, in their case Englis</context>
<context position="9095" citStr="Bhaskar and Bandyopadhyay (2012)" startWordPosition="1453" endWordPosition="1456">ranslate a user’s query into L2, and then summarized each document in L2 with respect to the query. In their final step, they translated the summary from L2 back 658 to Li for the user. They evaluated their method on the DUC 2005 query-focused summarization shared-task with ROUGE scores. We compare our methods to this work also on the DUC 2005 task. Our work demonstrates the first attempt to draw at a comparison between user-based studies and intrinsic evaluation with ROUGE. However, one of the limitations with evaluating this way is that the shared-task documents and queries are monolingual. Bhaskar and Bandyopadhyay (2012) tried a subjective evaluation of extractive cross-language query biased summarization for 7 different languages. They extracted sentences, then scored and ranked the sentences to generate query dependent snippets of documents for their cross lingual information access (CLIA) system. However, the snippet quality was determined subjectively based on scores on a scale of 0 to 1 (with 1 being best). Each score indicated annotator satisfaction for a given snippet. Our evaluation methodology is objective: we ask users to decide if a given document is relevant to an information need, or not. 2.2 Mac</context>
</contexts>
<marker>Bhaskar, Bandyopadhyay, 2012</marker>
<rawString>Pinaki Bhaskar and Sivaji Bandyopadhyay. CrossLingual Query Dependent Snippet Generation. International Journal of Computer Science and Information Technology (IJCSIT), 3(4), 2012.</rawString>
</citation>
<citation valid="true">
<authors>
<author>Pinaki Bhaskar</author>
<author>Sivaji Bandyopadhyay</author>
</authors>
<title>Language Independent Query Focused Snippet Generation. In</title>
<date>2012</date>
<booktitle>Information Access Evaluation. Multilinguality, Multimodality, and Visual Analytics,</booktitle>
<volume>7488</volume>
<pages>138--140</pages>
<editor>T. Catarci, P. Forner, D. Hiemstra, A. Pe˜nas, and G. Santucci, editors,</editor>
<publisher>Springer</publisher>
<location>Berlin Heidelberg,</location>
<contexts>
<context position="1570" citStr="Bhaskar and Bandyopadhyay, 2012" startWordPosition="214" endWordPosition="218">y. In our analysis, we demonstrate that using the ROUGE metric on our sentence-based summaries cannot make the same kinds of distinctions as our evaluation framework does. Finally, we present our recommendations for creating muchneeded evaluation standards and datasets. 1 Introduction Despite many recent advances in query biased summarization for cross-language information retrieval (CLIR), there are no existing evaluation standards or datasets to make comparisons among different methods, and across different languages (Tombros and Sanderson, 1998; Pingali et al., 2007; McCallum et al., 2012; Bhaskar and Bandyopadhyay, 2012). Consider that creating this kind of summary requires familiarity with techniques from machine translation (MT), summarization, and information retrieval (IR). In this This work was sponsored by the Federal Bureau of Investigation under Air Force Contract FA8721-05-C-0002. Opinions, interpretations, conclusions, and recommendations are those of the authors and are not necessarily endorsed by the United States Government. paper, we arrive at the intersection of each of these research areas. Query biased summarization (also known as query-focused, query-relevant, and query-dependent) involves a</context>
<context position="7800" citStr="Bhaskar and Bandyopadhyay, 2012" startWordPosition="1240" endWordPosition="1243">, 2004). Recent work has looked at creating summaries of single and multiple documents (Radev et al., 2004; Erkan and Radev, 2004; Wan et al., 2007; Yin et al., 2012; Chatterjee et al., 2012), as well as summary evaluation (Jing et al., 1998; Tombros and Sanderson 1998; Mani et al., 1998; Mani et al., 1999; Mani, 2001; Lin and Hovy, 2003; Lin, 2004; Nenkova et al., 2007; Hobson et al., 2007; Owczarzak et al., 2012), query and topic biased summarization (Berger and Mittal, 2000; Otterbacher et al., 2005; Daume and Marcu, 2006; Chali and Joty, 2008; Otterbacher et al., 2009; Bando et al., 2010; Bhaskar and Bandyopadhyay, 2012; Harwath and Hazen, 2012; Yin et al., 2012), and summarization across languages (Pingali et al., 2007; Or˘asan and Chiorean, 2008; Wan et al., 2010; Azarbonyad et al., 2013). 2.1 Query Biased Summarization Previous work most closely related to our own comes from Pingali et al., (2007). In their work, they present their method for cross-language query biased summarization for Telugu and English. Their work was motivated by the need for people to have access to foreign-language documents from a search engine even though the users were not familiar with the foreign language, in their case Englis</context>
<context position="9095" citStr="Bhaskar and Bandyopadhyay (2012)" startWordPosition="1453" endWordPosition="1456">ranslate a user’s query into L2, and then summarized each document in L2 with respect to the query. In their final step, they translated the summary from L2 back 658 to Li for the user. They evaluated their method on the DUC 2005 query-focused summarization shared-task with ROUGE scores. We compare our methods to this work also on the DUC 2005 task. Our work demonstrates the first attempt to draw at a comparison between user-based studies and intrinsic evaluation with ROUGE. However, one of the limitations with evaluating this way is that the shared-task documents and queries are monolingual. Bhaskar and Bandyopadhyay (2012) tried a subjective evaluation of extractive cross-language query biased summarization for 7 different languages. They extracted sentences, then scored and ranked the sentences to generate query dependent snippets of documents for their cross lingual information access (CLIA) system. However, the snippet quality was determined subjectively based on scores on a scale of 0 to 1 (with 1 being best). Each score indicated annotator satisfaction for a given snippet. Our evaluation methodology is objective: we ask users to decide if a given document is relevant to an information need, or not. 2.2 Mac</context>
</contexts>
<marker>Bhaskar, Bandyopadhyay, 2012</marker>
<rawString>Pinaki Bhaskar and Sivaji Bandyopadhyay. Language Independent Query Focused Snippet Generation. In T. Catarci, P. Forner, D. Hiemstra, A. Pe˜nas, and G. Santucci, editors, Information Access Evaluation. Multilinguality, Multimodality, and Visual Analytics, volume 7488 of Lecture Notes in Computer Science, pp 138–140. Springer Berlin Heidelberg, 2012.</rawString>
</citation>
<citation valid="true">
<authors>
<author>Stephen P Borgatti</author>
<author>Kathleen M Carley</author>
<author>David Krackhardt</author>
</authors>
<title>On the Robustness of Centrality Measures Under Conditions of Imperfect Data. Social Networks,</title>
<date>2006</date>
<marker>Borgatti, Carley, Krackhardt, 2006</marker>
<rawString>Stephen P. Borgatti, Kathleen M. Carley, David Krackhardt. On the Robustness of Centrality Measures Under Conditions of Imperfect Data. Social Networks, (28):124–136, 2006.</rawString>
</citation>
<citation valid="true">
<authors>
<author>Florian Boudin</author>
</authors>
<title>St´ephane Huet, and Juan-Manuel Torres-Moreno. A Graph-Based Approach to CrossLanguage Multi-Document Summarization. Polibits,</title>
<date>2011</date>
<marker>Boudin, 2011</marker>
<rawString>Florian Boudin, St´ephane Huet, and Juan-Manuel Torres-Moreno. A Graph-Based Approach to CrossLanguage Multi-Document Summarization. Polibits, (43):113–118, 2011.</rawString>
</citation>
<citation valid="true">
<authors>
<author>Fast</author>
</authors>
<title>Cheap, and Creative: Evaluating Translation Quality Using Amazon’s Mechanical Turk.</title>
<date>2009</date>
<booktitle>In Proceedings of the 2009 Conference on Empirical Methods in Natural Language Processing (EMNLP),</booktitle>
<pages>286--295</pages>
<location>Singapore, ACL</location>
<marker>Fast, 2009</marker>
<rawString>Chris Callison-Burch. Fast, Cheap, and Creative: Evaluating Translation Quality Using Amazon’s Mechanical Turk. In Proceedings of the 2009 Conference on Empirical Methods in Natural Language Processing (EMNLP), pp 286–295, Singapore, ACL 2009.</rawString>
</citation>
<citation valid="true">
<authors>
<author>Yllias Chali</author>
<author>Shafiq R Joty</author>
</authors>
<title>Unsupervised Approach for Selecting Sentences in Query-Based Summarization.</title>
<date>2008</date>
<booktitle>In Proceedings of the Twenty-First International FLAIRS Conference,</booktitle>
<contexts>
<context position="7721" citStr="Chali and Joty, 2008" startWordPosition="1228" endWordPosition="1231">, 1973; Robertson and Walker, 1994; Church and Gale, 1999; Robertson, 2004). Recent work has looked at creating summaries of single and multiple documents (Radev et al., 2004; Erkan and Radev, 2004; Wan et al., 2007; Yin et al., 2012; Chatterjee et al., 2012), as well as summary evaluation (Jing et al., 1998; Tombros and Sanderson 1998; Mani et al., 1998; Mani et al., 1999; Mani, 2001; Lin and Hovy, 2003; Lin, 2004; Nenkova et al., 2007; Hobson et al., 2007; Owczarzak et al., 2012), query and topic biased summarization (Berger and Mittal, 2000; Otterbacher et al., 2005; Daume and Marcu, 2006; Chali and Joty, 2008; Otterbacher et al., 2009; Bando et al., 2010; Bhaskar and Bandyopadhyay, 2012; Harwath and Hazen, 2012; Yin et al., 2012), and summarization across languages (Pingali et al., 2007; Or˘asan and Chiorean, 2008; Wan et al., 2010; Azarbonyad et al., 2013). 2.1 Query Biased Summarization Previous work most closely related to our own comes from Pingali et al., (2007). In their work, they present their method for cross-language query biased summarization for Telugu and English. Their work was motivated by the need for people to have access to foreign-language documents from a search engine even tho</context>
</contexts>
<marker>Chali, Joty, 2008</marker>
<rawString>Yllias Chali and Shafiq R. Joty. Unsupervised Approach for Selecting Sentences in Query-Based Summarization. In Proceedings of the Twenty-First International FLAIRS Conference, 2008.</rawString>
</citation>
<citation valid="true">
<authors>
<author>Niladri Chatterjee</author>
<author>Amol Mittal</author>
<author>Shubham Goyal</author>
</authors>
<title>Single Document Extractive Text Summarization Using Genetic Algorithms.</title>
<date>2012</date>
<booktitle>In Emerging Applications of Information Technology (EAIT), 2012 Third International Conference,</booktitle>
<pages>pp</pages>
<contexts>
<context position="7360" citStr="Chatterjee et al., 2012" startWordPosition="1164" endWordPosition="1167">our results and analysis; and in Section 7, we conclude and discuss some future directions for the NLP community. 2 Related Work Automatic summarization is generally a wellinvestigated research area. Summarization is a way of describing the relationships of words in documents to the information content of that document (Luhn, 1958; Edmunson, 1969; Salton and Yang, 1973; Robertson and Walker, 1994; Church and Gale, 1999; Robertson, 2004). Recent work has looked at creating summaries of single and multiple documents (Radev et al., 2004; Erkan and Radev, 2004; Wan et al., 2007; Yin et al., 2012; Chatterjee et al., 2012), as well as summary evaluation (Jing et al., 1998; Tombros and Sanderson 1998; Mani et al., 1998; Mani et al., 1999; Mani, 2001; Lin and Hovy, 2003; Lin, 2004; Nenkova et al., 2007; Hobson et al., 2007; Owczarzak et al., 2012), query and topic biased summarization (Berger and Mittal, 2000; Otterbacher et al., 2005; Daume and Marcu, 2006; Chali and Joty, 2008; Otterbacher et al., 2009; Bando et al., 2010; Bhaskar and Bandyopadhyay, 2012; Harwath and Hazen, 2012; Yin et al., 2012), and summarization across languages (Pingali et al., 2007; Or˘asan and Chiorean, 2008; Wan et al., 2010; Azarbonyad</context>
</contexts>
<marker>Chatterjee, Mittal, Goyal, 2012</marker>
<rawString>Niladri Chatterjee, Amol Mittal, and Shubham Goyal. Single Document Extractive Text Summarization Using Genetic Algorithms. In Emerging Applications of Information Technology (EAIT), 2012 Third International Conference, pp 19–23, 2012.</rawString>
</citation>
<citation valid="true">
<authors>
<author>Kenneth W Church</author>
<author>William A Gale</author>
</authors>
<title>Inverse Document Frequency (IDF): A Measure of Deviations From Poisson. In Natural language processing using very large corpora,</title>
<date>1999</date>
<pages>283--295</pages>
<publisher>Springer,</publisher>
<contexts>
<context position="7158" citStr="Church and Gale, 1999" startWordPosition="1130" endWordPosition="1133">ws: Section 2 presents related work; Section 3 describes our data and pre-processing; Section 4 details our summarization methods and strategies; Section 5 describes our experiments; Section 6 shows our results and analysis; and in Section 7, we conclude and discuss some future directions for the NLP community. 2 Related Work Automatic summarization is generally a wellinvestigated research area. Summarization is a way of describing the relationships of words in documents to the information content of that document (Luhn, 1958; Edmunson, 1969; Salton and Yang, 1973; Robertson and Walker, 1994; Church and Gale, 1999; Robertson, 2004). Recent work has looked at creating summaries of single and multiple documents (Radev et al., 2004; Erkan and Radev, 2004; Wan et al., 2007; Yin et al., 2012; Chatterjee et al., 2012), as well as summary evaluation (Jing et al., 1998; Tombros and Sanderson 1998; Mani et al., 1998; Mani et al., 1999; Mani, 2001; Lin and Hovy, 2003; Lin, 2004; Nenkova et al., 2007; Hobson et al., 2007; Owczarzak et al., 2012), query and topic biased summarization (Berger and Mittal, 2000; Otterbacher et al., 2005; Daume and Marcu, 2006; Chali and Joty, 2008; Otterbacher et al., 2009; Bando et </context>
<context position="18077" citStr="Church and Gale, 1999" startWordPosition="2938" endWordPosition="2941">ypically used in IR and other text categorization tasks to make distinctions between documents. The version of idf that we used throughout our work came from Erkan and Radev (2004) and Otterbacher et al. (2009), in keeping consistent with theirs. Let N be the number of documents in the collection, such that N = |D |and nt is the number of documents that contain term t, such that nt = |{d E D : t E d}|, then: N + 1 idft =log 0.5 x nt While idf is usually thought of as a type of heuristic, there have been some discussions about its theoretical basis (Robertson, 2004; Robertson and Walker, 1994; Church and Gale, 1999; Salton and Yang, 1973). An example of this summary is shown in Figure 3. III: Term Frequency Inverse Document Frequency (TFIDF) We use tfidft,d term weighting to find terms which are both rare and important for a document, with respect to terms across all other documents in the collection: tfidft,d = tft,d x idft 4.3 Query Biased Word Clouds We generated query biased word clouds following the same principles as our unbiased word clouds, Figure 3: Word cloud summary for inverse document frequency (IDF), for query “Tehran’s stock market”. namely the text font scaling and highlighting remained </context>
</contexts>
<marker>Church, Gale, 1999</marker>
<rawString>Kenneth W. Church and William A. Gale. Inverse Document Frequency (IDF): A Measure of Deviations From Poisson. In Natural language processing using very large corpora, pages 283–295. Springer, 1999.</rawString>
</citation>
<citation valid="true">
<authors>
<author>Mathias Creutz</author>
<author>Krista Lagus</author>
</authors>
<title>Unsupervised Models for Morpheme Segmentation and Morphology Learning.</title>
<date>2007</date>
<journal>ACM Transactions on Speech and Language Processing,</journal>
<volume>4</volume>
<issue>1</issue>
<contexts>
<context position="14500" citStr="Creutz and Lagus, 2007" startWordPosition="2315" endWordPosition="2318">. The translated documents were sentence-aligned with one sentence per line. For all of our summarization experiments (except unbised full MT text), we processed the text as follows: removed extra spaces, removed punctuation, folded to lowercase, and removed digits. We also removed common English stopwords2 from the texts. 3.2 Farsi Documents We used the original CLEF 2008 Farsi documents for two of our summarization methods. We stemmed words in each document using automatic morphological analysis with Morfessor CatMAP. We note that within-sentence punctuation was removed during this process (Creutz and Lagus, 2007). We also removed Farsi stopwords and digits. 4 Summarization Strategies All of our summarization methods were extractive except for unbiased full machine translated text. In this section, we describe each of our 13 summarization methods which we have organized into one of the following strategies: (1) unbiased full machine translated text, (2) unbiased 2English and Farsi stopword lists from: http://members.unine.ch/jacques.savoy/clef/index.html word cloud summaries, (3) query biased word cloud summaries, and (4) query biased sentence summaries. Regardless of which summarization method used, w</context>
</contexts>
<marker>Creutz, Lagus, 2007</marker>
<rawString>Mathias Creutz and Krista Lagus. Unsupervised Models for Morpheme Segmentation and Morphology Learning. ACM Transactions on Speech and Language Processing, 4(1):3:1–3:34, February 2007.</rawString>
</citation>
<citation valid="true">
<authors>
<author>Hoa Trang Dang</author>
</authors>
<title>Overview of DUC</title>
<date>2005</date>
<booktitle>In Proceedings of the Document Understanding Conference,</booktitle>
<contexts>
<context position="32962" citStr="Dang, 2005" startWordPosition="5438" endWordPosition="5439">uld do with query/question biasing while ignoring the variables introduced by MT and crosslanguage text, and (2) to make a comparison to previous work. Pingali et al., (2007) also used this the same DUC task to assess their cross-language query biased summarization system. Systems from the DUC 2005 question-biased summarization task were evaluated automatically against human gold-standard summaries using ROUGE (Lin and Hovy, 2003) . Our results from the DUC 2005 shared-task are shown in Table 2, reported as ROUGE-2 and ROUGE-SU4 f-scores, as these two variations of ROUGE are the most helpful (Dang, 2005; Pingali et al., 2007). Table 2 shows scores for several top peer systems, as well as results for the Tel-Eng-Sum method from Pingali et al., (2007). While we have reported f-scores in our analysis, we also note that our implementations of LQ and REL outperform all of the DUC 2005 peer systems for precision, as shown in Table 3. We also know that ROUGE cannot be used for comparing sentence summaries to ranked lists of words and there are no existing intrinsic methods to make that kind of comparison. Therefore we were able to successfully compare just 2 of our sentence-based methods to previou</context>
</contexts>
<marker>Dang, 2005</marker>
<rawString>Hoa Trang Dang. Overview of DUC 2005. In Proceedings of the Document Understanding Conference, 2005.</rawString>
</citation>
<citation valid="true">
<authors>
<author>Hal Daum´e Daniel Marcu</author>
</authors>
<title>Bayesian QueryFocused Summarization.</title>
<date>2006</date>
<booktitle>In Proceedings of the 21st International Conference on Computational Linguistics and the 44th annual meeting of the Association for Computational Linguistics, ACL</booktitle>
<contexts>
<context position="7699" citStr="Marcu, 2006" startWordPosition="1226" endWordPosition="1227">lton and Yang, 1973; Robertson and Walker, 1994; Church and Gale, 1999; Robertson, 2004). Recent work has looked at creating summaries of single and multiple documents (Radev et al., 2004; Erkan and Radev, 2004; Wan et al., 2007; Yin et al., 2012; Chatterjee et al., 2012), as well as summary evaluation (Jing et al., 1998; Tombros and Sanderson 1998; Mani et al., 1998; Mani et al., 1999; Mani, 2001; Lin and Hovy, 2003; Lin, 2004; Nenkova et al., 2007; Hobson et al., 2007; Owczarzak et al., 2012), query and topic biased summarization (Berger and Mittal, 2000; Otterbacher et al., 2005; Daume and Marcu, 2006; Chali and Joty, 2008; Otterbacher et al., 2009; Bando et al., 2010; Bhaskar and Bandyopadhyay, 2012; Harwath and Hazen, 2012; Yin et al., 2012), and summarization across languages (Pingali et al., 2007; Or˘asan and Chiorean, 2008; Wan et al., 2010; Azarbonyad et al., 2013). 2.1 Query Biased Summarization Previous work most closely related to our own comes from Pingali et al., (2007). In their work, they present their method for cross-language query biased summarization for Telugu and English. Their work was motivated by the need for people to have access to foreign-language documents from a </context>
</contexts>
<marker>Marcu, 2006</marker>
<rawString>Hal Daum´e III, Daniel Marcu. Bayesian QueryFocused Summarization. In Proceedings of the 21st International Conference on Computational Linguistics and the 44th annual meeting of the Association for Computational Linguistics, ACL 2006.</rawString>
</citation>
<citation valid="true">
<authors>
<author>Jennifer Drexler</author>
<author>Wade Shen</author>
<author>Terry P Gleason</author>
<author>Timothy R Anderson</author>
<author>Raymond E Slyh</author>
<author>Brian M Ore</author>
<author>Eric G Hansen</author>
</authors>
<date>2012</date>
<booktitle>The MIT-LL/AFRL IWSLT2012 MT System. In Proceedings of the International Workshop on Spoken Language Translation (IWSLT), Hong Kong,</booktitle>
<contexts>
<context position="13877" citStr="Drexler et al., 2012" startWordPosition="2221" endWordPosition="2224"> query, we randomly selected 5 documents that were relevant as well as 5 documents that were not relevant. The subset of CLEF 2008 data that we used therefore consisted of 500 original Farsi documents and 50 parallel English-Farsi queries. Next we will describe our text pre-processing steps for both languages as well as how we created our parallel English documents. Figure 1: Full MT English summary and CLEF 2008 English query (title, description, narrative). 3.1 English Documents All of our English documents were created automatically by translating the original Farsi documents into English (Drexler et al., 2012). The translated documents were sentence-aligned with one sentence per line. For all of our summarization experiments (except unbised full MT text), we processed the text as follows: removed extra spaces, removed punctuation, folded to lowercase, and removed digits. We also removed common English stopwords2 from the texts. 3.2 Farsi Documents We used the original CLEF 2008 Farsi documents for two of our summarization methods. We stemmed words in each document using automatic morphological analysis with Morfessor CatMAP. We note that within-sentence punctuation was removed during this process (</context>
<context position="15956" citStr="Drexler et al., 2012" startWordPosition="2557" endWordPosition="2560">ated separately for each language. While |D |= 1000, we calculated term weightings based on |DE |= 500 and |DF |= 500. Finally, let q be a query where q ∈ Q and Q is our set of 50 parallel English-Farsi CLEF queries. Assume that log refers to logio. Figure 2: Full MT English summary and CLEF 2008 English query. 4.1 Unbiased Full Machine Translated English Our first baseline approach was to use all of the raw machine translation output (no subsets of the sentences were used). Each summary therefore consisted of the full text of an entire document automatically translated from Farsi to English (Drexler et al., 2012). Figure 2 shows an example full text document translated from Farsi to English and a gold-standard English CLEF query. Note that we use this particular document-query pair as an example throughout this paper (document: H-770622-42472S8, query: 10.2452/552- AH). According to the CLEF answer key, the sample document is relevant to the sample query. 4.2 Unbiased Word Clouds For our second baseline approach, we ranked terms in a document and displayed them as word clouds. Word clouds are one a way to arrange a collection of words where each word can vary 660 in size. We used word clouds as a summ</context>
</contexts>
<marker>Drexler, Shen, Gleason, Anderson, Slyh, Ore, Hansen, 2012</marker>
<rawString>Jennifer Drexler, Wade Shen, Terry P. Gleason, Timothy R. Anderson, Raymond E. Slyh, Brian M. Ore, and Eric G. Hansen. The MIT-LL/AFRL IWSLT2012 MT System. In Proceedings of the International Workshop on Spoken Language Translation (IWSLT), Hong Kong, December 2012.</rawString>
</citation>
<citation valid="true">
<authors>
<author>Bonnie J Dorr</author>
<author>Christof Monz</author>
<author>Stacy President</author>
<author>Richard Schwartz</author>
<author>David Zajic</author>
</authors>
<title>A Methodology for Extrinsic Evaluation of Text Summarization: Does ROUGE Correlate?</title>
<date>2005</date>
<booktitle>In Proceedings of the ACL Workshop on Intrinsic and Extrinsic Evaluation Measures for Machine Translation and/or Summarization,</booktitle>
<pages>1--8</pages>
<location>Ann Arbor, ACL</location>
<contexts>
<context position="3394" citStr="Dorr et al., 2005" startWordPosition="511" endWordPosition="514">their queries in L1 to search through documents that have been composed in L2, even though they may not be familiar with L2 (Hovy et al., 1999; Pingali et al., 2007). There are no standards for objectively evaluating summaries for CLIR – a research gap that we begin to address in this paper. The problem we explore is two-fold: what kinds of summaries are well-suited for CLIR applications, and how should the summaries be evaluated. Our evaluation is extrinsic, that is to say we are interested in how summarization affects performance on a different task (Mani et al., 2002; McKeown et al., 2005; Dorr et al., 2005; Murray et al., 2009; McCallum et al., 2012). We use relevance prediction as our extrinsic task: a human must decide if a summary for a given document is relevant to a particular information need, or not. Relevance prediction is known to be useful as it correlates with some automatic intrinsic methods as well (President and Dorr, 2006; Hobson et al., 2007). To the best of our knowledge, we are the first to apply this evaluation framework to cross language query biased summarization. Each one of the summarization methods that we 657 Proceedings of the 2014 Conference on Empirical Methods in Na</context>
</contexts>
<marker>Dorr, Monz, President, Schwartz, Zajic, 2005</marker>
<rawString>Bonnie J. Dorr, Christof Monz, Stacy President, Richard Schwartz, and David Zajic. A Methodology for Extrinsic Evaluation of Text Summarization: Does ROUGE Correlate? In Proceedings of the ACL Workshop on Intrinsic and Extrinsic Evaluation Measures for Machine Translation and/or Summarization, pp 1-8. Ann Arbor, ACL 2005.</rawString>
</citation>
<citation valid="true">
<authors>
<author>H P Edmundson</author>
</authors>
<title>New Methods in Automatic Extracting.</title>
<date>1969</date>
<journal>In Journal of the ACM,</journal>
<volume>16</volume>
<issue>2</issue>
<marker>Edmundson, 1969</marker>
<rawString>H. P. Edmundson. New Methods in Automatic Extracting. In Journal of the ACM, 16(2):264–285, April 1969.</rawString>
</citation>
<citation valid="true">
<authors>
<author>G¨unes¸ Erkan</author>
<author>Dragomir R Radev</author>
</authors>
<title>Lexrank: Graph-based lexical centrality as salience in text summarization.</title>
<date>2004</date>
<journal>Journal of Artificial Intelligence Research,</journal>
<volume>22</volume>
<issue>1</issue>
<contexts>
<context position="7298" citStr="Erkan and Radev, 2004" startWordPosition="1152" endWordPosition="1155">gies; Section 5 describes our experiments; Section 6 shows our results and analysis; and in Section 7, we conclude and discuss some future directions for the NLP community. 2 Related Work Automatic summarization is generally a wellinvestigated research area. Summarization is a way of describing the relationships of words in documents to the information content of that document (Luhn, 1958; Edmunson, 1969; Salton and Yang, 1973; Robertson and Walker, 1994; Church and Gale, 1999; Robertson, 2004). Recent work has looked at creating summaries of single and multiple documents (Radev et al., 2004; Erkan and Radev, 2004; Wan et al., 2007; Yin et al., 2012; Chatterjee et al., 2012), as well as summary evaluation (Jing et al., 1998; Tombros and Sanderson 1998; Mani et al., 1998; Mani et al., 1999; Mani, 2001; Lin and Hovy, 2003; Lin, 2004; Nenkova et al., 2007; Hobson et al., 2007; Owczarzak et al., 2012), query and topic biased summarization (Berger and Mittal, 2000; Otterbacher et al., 2005; Daume and Marcu, 2006; Chali and Joty, 2008; Otterbacher et al., 2009; Bando et al., 2010; Bhaskar and Bandyopadhyay, 2012; Harwath and Hazen, 2012; Yin et al., 2012), and summarization across languages (Pingali et al., </context>
<context position="17636" citStr="Erkan and Radev (2004)" startWordPosition="2852" endWordPosition="2855"> to scale text font size, so that words with a highter score appeared larger and more prominent in a word cloud. Words were shuffled such that the exact ordering of words was at random. I: Term Frequency (TF) Term frequency is very commonly used for finding important terms in a document. Given a term t in a document d, the number of times that term occurs is: tft,d = |t E d| II: Inverse Document Frequency (IDF) The idf term weighting is typically used in IR and other text categorization tasks to make distinctions between documents. The version of idf that we used throughout our work came from Erkan and Radev (2004) and Otterbacher et al. (2009), in keeping consistent with theirs. Let N be the number of documents in the collection, such that N = |D |and nt is the number of documents that contain term t, such that nt = |{d E D : t E d}|, then: N + 1 idft =log 0.5 x nt While idf is usually thought of as a type of heuristic, there have been some discussions about its theoretical basis (Robertson, 2004; Robertson and Walker, 1994; Church and Gale, 1999; Salton and Yang, 1973). An example of this summary is shown in Figure 3. III: Term Frequency Inverse Document Frequency (TFIDF) We use tfidft,d term weightin</context>
<context position="22638" citStr="Erkan and Radev, 2004" startWordPosition="3723" endWordPosition="3726"> a sentence s from document d, to a query q: Xrel(s|q) = log(tft,s +1)xlog(tft,q +1)xidft tEs Terms will occur in either the sentence or the query, or both. We applied this method to machine tranlsated English text. The output of this method is a relevance score for each sentence in a given document. We used those scores to rank sentences in each document from our English machine translated text. X. Query Biased Lexrank (LQ) We implemented query biased LexRank, a well-known graph-based summarization method (Otterbacher et al., 2009). It is a modified version of the original LexRank algorithm (Erkan and Radev, 2004; Page et al., 1998). The similarity metric, simx,y, also known as idf-modified cosine similarity, measures the distance between two sentences x and y in a document d, defined as: PtEx,y tft,x x tft,y x (idft)2 simx,y = q&amp;Ex tf idft,xq&amp;Ey t f idft y We used simx,y to score the similarity of sentence-to-sentence, resulting in a similarity 662 Figure 6: LQP - projecting Farsi sentence scores onto parallel English sentences. Figure 7: LQC - Farsi sentence scores are combined with parallel English sentence scores to obtain sentence re-ranking. graph where each vertex was a sentence and each edge w</context>
</contexts>
<marker>Erkan, Radev, 2004</marker>
<rawString>G¨unes¸ Erkan and Dragomir R. Radev. Lexrank: Graph-based lexical centrality as salience in text summarization. Journal of Artificial Intelligence Research, 22(1):457–479, December 2004.</rawString>
</citation>
<citation valid="true">
<authors>
<author>Dan Gillick</author>
<author>Yang Liu</author>
</authors>
<title>Non-Expert Evaluation of Summarization Systems is Risky.</title>
<date>2010</date>
<booktitle>In Proceedings of NAACL HLT 2010 Workshop on Creating Speech and Language Data with Amazon’s Mechanical Turk,</booktitle>
<pages>148--151</pages>
<location>Los Angeles, California, USA,</location>
<contexts>
<context position="26206" citStr="Gillick and Liu, 2010" startWordPosition="4325" endWordPosition="4328">with a different way to score and rank sentences, compared with LQ and LQP. The idea behind combinatory query biased LexRank is to take advantage of sentences which are highranking in Farsi but not in English. The LQC method exploits all available resources in our dataset: L1 and L2 queries as well as L1 and L2 documents. 5 Experiments We tested each of our summarization methods and overall strategies in a task-based evaluation framework using relevance prediction. We used Mechanical Turk for our experiments since it has been shown to be useful for evaluating NLP systems (Callison-Burch 2009; Gillick and Liu, 2010). We obtained human judgments for whether or not a document was considered relevant to a query, or information need. We measured the relevance � v∈adj[s] 663 judgements by precision/recall/F1, accuracy, and also time-on-task based on the average response time per Human Intelligence Task (HIT). 5.1 Mechanical Turk In our Mechanical Turk experiment, we used terminology from CLEF 2008 to describe a query as an “information need”. All of the Mechanical Turk workers were presented with the following for their individual HIT: instructions, an information need and one summary for a document. Workers </context>
</contexts>
<marker>Gillick, Liu, 2010</marker>
<rawString>Dan Gillick and Yang Liu. Non-Expert Evaluation of Summarization Systems is Risky. In Proceedings of NAACL HLT 2010 Workshop on Creating Speech and Language Data with Amazon’s Mechanical Turk, pp 148-151, Los Angeles, California, USA, June, 2010.</rawString>
</citation>
<citation valid="true">
<authors>
<author>David Harwath</author>
<author>Timothy J Hazen</author>
</authors>
<title>Topic Identification Based Extrinsic Evaluation of Summarization Techniques Applied to Conversational Speech.</title>
<date>2012</date>
<booktitle>In Proceedings of ICASSP,</booktitle>
<pages>5073--5076</pages>
<contexts>
<context position="7825" citStr="Harwath and Hazen, 2012" startWordPosition="1244" endWordPosition="1247">t creating summaries of single and multiple documents (Radev et al., 2004; Erkan and Radev, 2004; Wan et al., 2007; Yin et al., 2012; Chatterjee et al., 2012), as well as summary evaluation (Jing et al., 1998; Tombros and Sanderson 1998; Mani et al., 1998; Mani et al., 1999; Mani, 2001; Lin and Hovy, 2003; Lin, 2004; Nenkova et al., 2007; Hobson et al., 2007; Owczarzak et al., 2012), query and topic biased summarization (Berger and Mittal, 2000; Otterbacher et al., 2005; Daume and Marcu, 2006; Chali and Joty, 2008; Otterbacher et al., 2009; Bando et al., 2010; Bhaskar and Bandyopadhyay, 2012; Harwath and Hazen, 2012; Yin et al., 2012), and summarization across languages (Pingali et al., 2007; Or˘asan and Chiorean, 2008; Wan et al., 2010; Azarbonyad et al., 2013). 2.1 Query Biased Summarization Previous work most closely related to our own comes from Pingali et al., (2007). In their work, they present their method for cross-language query biased summarization for Telugu and English. Their work was motivated by the need for people to have access to foreign-language documents from a search engine even though the users were not familiar with the foreign language, in their case English. They used language mod</context>
</contexts>
<marker>Harwath, Hazen, 2012</marker>
<rawString>David Harwath and Timothy J. Hazen. Topic Identification Based Extrinsic Evaluation of Summarization Techniques Applied to Conversational Speech. In Proceedings of ICASSP, 2012: 5073-5076.</rawString>
</citation>
<citation valid="true">
<authors>
<author>Stacy P Hobson</author>
<author>Bonnie J Dorr</author>
<author>Christof Monz</author>
<author>Richard Schwartz</author>
</authors>
<title>Task-Eased Evaluation of Text Summarization Using Relevance Prediction.</title>
<date>2007</date>
<booktitle>In Information Processing Management,</booktitle>
<volume>43</volume>
<issue>6</issue>
<pages>1482--1499</pages>
<contexts>
<context position="3753" citStr="Hobson et al., 2007" startWordPosition="575" endWordPosition="578"> well-suited for CLIR applications, and how should the summaries be evaluated. Our evaluation is extrinsic, that is to say we are interested in how summarization affects performance on a different task (Mani et al., 2002; McKeown et al., 2005; Dorr et al., 2005; Murray et al., 2009; McCallum et al., 2012). We use relevance prediction as our extrinsic task: a human must decide if a summary for a given document is relevant to a particular information need, or not. Relevance prediction is known to be useful as it correlates with some automatic intrinsic methods as well (President and Dorr, 2006; Hobson et al., 2007). To the best of our knowledge, we are the first to apply this evaluation framework to cross language query biased summarization. Each one of the summarization methods that we 657 Proceedings of the 2014 Conference on Empirical Methods in Natural Language Processing (EMNLP), pages 657–669, October 25-29, 2014, Doha, Qatar. c�2014 Association for Computational Linguistics present in this paper belongs to one of the following strategies: (1) unbiased full machine translated text, (2) unbiased word clouds, (3) query biased word clouds, and (4) query biased sentence summaries. The methods and stra</context>
<context position="7562" citStr="Hobson et al., 2007" startWordPosition="1202" endWordPosition="1205">zation is a way of describing the relationships of words in documents to the information content of that document (Luhn, 1958; Edmunson, 1969; Salton and Yang, 1973; Robertson and Walker, 1994; Church and Gale, 1999; Robertson, 2004). Recent work has looked at creating summaries of single and multiple documents (Radev et al., 2004; Erkan and Radev, 2004; Wan et al., 2007; Yin et al., 2012; Chatterjee et al., 2012), as well as summary evaluation (Jing et al., 1998; Tombros and Sanderson 1998; Mani et al., 1998; Mani et al., 1999; Mani, 2001; Lin and Hovy, 2003; Lin, 2004; Nenkova et al., 2007; Hobson et al., 2007; Owczarzak et al., 2012), query and topic biased summarization (Berger and Mittal, 2000; Otterbacher et al., 2005; Daume and Marcu, 2006; Chali and Joty, 2008; Otterbacher et al., 2009; Bando et al., 2010; Bhaskar and Bandyopadhyay, 2012; Harwath and Hazen, 2012; Yin et al., 2012), and summarization across languages (Pingali et al., 2007; Or˘asan and Chiorean, 2008; Wan et al., 2010; Azarbonyad et al., 2013). 2.1 Query Biased Summarization Previous work most closely related to our own comes from Pingali et al., (2007). In their work, they present their method for cross-language query biased s</context>
<context position="26929" citStr="Hobson et al., 2007" startWordPosition="4443" endWordPosition="4446">rmation need. We measured the relevance � v∈adj[s] 663 judgements by precision/recall/F1, accuracy, and also time-on-task based on the average response time per Human Intelligence Task (HIT). 5.1 Mechanical Turk In our Mechanical Turk experiment, we used terminology from CLEF 2008 to describe a query as an “information need”. All of the Mechanical Turk workers were presented with the following for their individual HIT: instructions, an information need and one summary for a document. Workers were asked to indicate if the given summary for a document was relevant to the given information need (Hobson et al., 2007). Workers were not shown the original Farsi source documents. We paid workers $0.01 per HIT. We obtained 5 HITs for each information need and summary pair. We used a built-in approval rate qualification provided by Mechanical Turk to restrict which workers could work on our tasks. Each worker had an approval rate of at least 95 Instructions: Each image below consists of a statement summarizing the information you are trying to find from a set of documents followed by a summary of one of the documents returned when you query the documents. Based on the summary, choose whether you think the docu</context>
</contexts>
<marker>Hobson, Dorr, Monz, Schwartz, 2007</marker>
<rawString>Stacy P. Hobson, Bonnie J. Dorr, Christof Monz, and Richard Schwartz. Task-Eased Evaluation of Text Summarization Using Relevance Prediction. In Information Processing Management, 43(6): 1482-1499, 2007.</rawString>
</citation>
<citation valid="true">
<authors>
<author>Hongyan Jing</author>
<author>Regina Barzilay</author>
<author>Kathleen McKeown</author>
<author>Michael Elhadad</author>
</authors>
<title>Summarization Evaluation Methods: Experiments and Analysis.</title>
<date>1998</date>
<booktitle>In Proceedings of American Association for Artificial Ingelligence (AAAI),</booktitle>
<contexts>
<context position="7410" citStr="Jing et al., 1998" startWordPosition="1174" endWordPosition="1177">nd discuss some future directions for the NLP community. 2 Related Work Automatic summarization is generally a wellinvestigated research area. Summarization is a way of describing the relationships of words in documents to the information content of that document (Luhn, 1958; Edmunson, 1969; Salton and Yang, 1973; Robertson and Walker, 1994; Church and Gale, 1999; Robertson, 2004). Recent work has looked at creating summaries of single and multiple documents (Radev et al., 2004; Erkan and Radev, 2004; Wan et al., 2007; Yin et al., 2012; Chatterjee et al., 2012), as well as summary evaluation (Jing et al., 1998; Tombros and Sanderson 1998; Mani et al., 1998; Mani et al., 1999; Mani, 2001; Lin and Hovy, 2003; Lin, 2004; Nenkova et al., 2007; Hobson et al., 2007; Owczarzak et al., 2012), query and topic biased summarization (Berger and Mittal, 2000; Otterbacher et al., 2005; Daume and Marcu, 2006; Chali and Joty, 2008; Otterbacher et al., 2009; Bando et al., 2010; Bhaskar and Bandyopadhyay, 2012; Harwath and Hazen, 2012; Yin et al., 2012), and summarization across languages (Pingali et al., 2007; Or˘asan and Chiorean, 2008; Wan et al., 2010; Azarbonyad et al., 2013). 2.1 Query Biased Summarization Pre</context>
<context position="12177" citStr="Jing et al, 1998" startWordPosition="1936" endWordPosition="1939">Evaluation metrics are either intrinsic or extrinsic. Intrinsic metrics, such as ROUGE, measure the quality of a summary with respect to gold human-generated summaries (Lin, 2004; Lin and Hovy, 2003). Generating gold standard summaries is expensive and time-consuming, a problem that persists with cross-language query biased summarization because those summaries must be query biased as well as in a different language from the source documents. On the other hand, extrinsic metrics measure the quality of summaries at the system level, by looking at overall system performance on downstream tasks (Jing et al, 1998; Tombros and Sanderson, 1998). One of the most important findings for query biased summarization comes from Tombros and Sanderson (1998). In their monolingual taskbased evaluation, they measured user speed and accuracy at identifying relevant documents. They found that query biased summarization improved the user speed and accuracy when the user was asked to make relevance judgements for IR tasks. We also expect that our evaluation will demonstrate that user speed and accuracy is better when summaries are query biased. 3 Data and Pre-Processing We used data from the Farsi CLEF 2008 ad hoc tas</context>
</contexts>
<marker>Jing, Barzilay, McKeown, Elhadad, 1998</marker>
<rawString>Hongyan Jing, Regina Barzilay, Kathleen McKeown, and Michael Elhadad. Summarization Evaluation Methods: Experiments and Analysis. In Proceedings of American Association for Artificial Ingelligence (AAAI), 1998.</rawString>
</citation>
<citation valid="false">
<authors>
<author>Reza Karimpour</author>
<author>Amineh Ghorbani</author>
</authors>
<title>Azadeh Pishdad, Mitra Mohtarami, Abolfazl AleAhmad, Hadi Amiri, and Farhad Oroumchian. Improving Persian Information Retrieval Systems Using Stemming and Part of Speech Tagging.</title>
<date>2008</date>
<booktitle>In Proceedings of the 9th Crosslanguage Evaluation Forum Conference on Evaluating Systems for Multilingual and Multimodal Information Access, CLEF</booktitle>
<pages>89--96</pages>
<publisher>Springer-Verlag.</publisher>
<location>Berlin, Heidelberg,</location>
<marker>Karimpour, Ghorbani, 2008</marker>
<rawString>Reza Karimpour, Amineh Ghorbani, Azadeh Pishdad, Mitra Mohtarami, Abolfazl AleAhmad, Hadi Amiri, and Farhad Oroumchian. Improving Persian Information Retrieval Systems Using Stemming and Part of Speech Tagging. In Proceedings of the 9th Crosslanguage Evaluation Forum Conference on Evaluating Systems for Multilingual and Multimodal Information Access, CLEF 2008, pp 89–96, Berlin, Heidelberg, 2009. Springer-Verlag.</rawString>
</citation>
<citation valid="true">
<authors>
<author>Chin-Yew Lin</author>
</authors>
<title>Looking For A Few Good Metrics: Automatic Summarization Evaluation - How Many Samples Are Enough?</title>
<date>2004</date>
<booktitle>In Proceedings of NTCIR Workshop 4,</booktitle>
<location>Tokyo, Japan,</location>
<contexts>
<context position="7519" citStr="Lin, 2004" startWordPosition="1196" endWordPosition="1197">vestigated research area. Summarization is a way of describing the relationships of words in documents to the information content of that document (Luhn, 1958; Edmunson, 1969; Salton and Yang, 1973; Robertson and Walker, 1994; Church and Gale, 1999; Robertson, 2004). Recent work has looked at creating summaries of single and multiple documents (Radev et al., 2004; Erkan and Radev, 2004; Wan et al., 2007; Yin et al., 2012; Chatterjee et al., 2012), as well as summary evaluation (Jing et al., 1998; Tombros and Sanderson 1998; Mani et al., 1998; Mani et al., 1999; Mani, 2001; Lin and Hovy, 2003; Lin, 2004; Nenkova et al., 2007; Hobson et al., 2007; Owczarzak et al., 2012), query and topic biased summarization (Berger and Mittal, 2000; Otterbacher et al., 2005; Daume and Marcu, 2006; Chali and Joty, 2008; Otterbacher et al., 2009; Bando et al., 2010; Bhaskar and Bandyopadhyay, 2012; Harwath and Hazen, 2012; Yin et al., 2012), and summarization across languages (Pingali et al., 2007; Or˘asan and Chiorean, 2008; Wan et al., 2010; Azarbonyad et al., 2013). 2.1 Query Biased Summarization Previous work most closely related to our own comes from Pingali et al., (2007). In their work, they present the</context>
<context position="11739" citStr="Lin, 2004" startWordPosition="1869" endWordPosition="1870">es. In our work, we used gold-translated queries from the CLEF 2008 dataset, and machine translated source documents. We briefly address this in our work, but note that a full discussion of when and what to translate, and those effects on summarization quality, is outside of the scope of this paper. 2.3 Summarization Evaluation There has been a lot of work towards developing metrics for understanding what makes a summary good. Evaluation metrics are either intrinsic or extrinsic. Intrinsic metrics, such as ROUGE, measure the quality of a summary with respect to gold human-generated summaries (Lin, 2004; Lin and Hovy, 2003). Generating gold standard summaries is expensive and time-consuming, a problem that persists with cross-language query biased summarization because those summaries must be query biased as well as in a different language from the source documents. On the other hand, extrinsic metrics measure the quality of summaries at the system level, by looking at overall system performance on downstream tasks (Jing et al, 1998; Tombros and Sanderson, 1998). One of the most important findings for query biased summarization comes from Tombros and Sanderson (1998). In their monolingual ta</context>
</contexts>
<marker>Lin, 2004</marker>
<rawString>Chin-Yew Lin. Looking For A Few Good Metrics: Automatic Summarization Evaluation - How Many Samples Are Enough? In Proceedings of NTCIR Workshop 4, Tokyo, Japan, June 2004.</rawString>
</citation>
<citation valid="true">
<authors>
<author>Annie Louis</author>
<author>Ani Nenkova</author>
</authors>
<title>Automatic Summary Evaluation without Human Models.</title>
<date>2009</date>
<booktitle>In Proceedings of Empirical Methods in Natural Language Processing, EMNLP</booktitle>
<marker>Louis, Nenkova, 2009</marker>
<rawString>Annie Louis and Ani Nenkova. Automatic Summary Evaluation without Human Models. In Proceedings of Empirical Methods in Natural Language Processing, EMNLP 2009.</rawString>
</citation>
<citation valid="true">
<authors>
<author>H P Luhn</author>
</authors>
<title>The Automatic Creation of Literature Abstracts.</title>
<date>1958</date>
<journal>IBM Journal of Research and Development,</journal>
<volume>2</volume>
<issue>2</issue>
<contexts>
<context position="7068" citStr="Luhn, 1958" startWordPosition="1118" endWordPosition="1119"> find as many relevant documents as possible. This paper is structured as follows: Section 2 presents related work; Section 3 describes our data and pre-processing; Section 4 details our summarization methods and strategies; Section 5 describes our experiments; Section 6 shows our results and analysis; and in Section 7, we conclude and discuss some future directions for the NLP community. 2 Related Work Automatic summarization is generally a wellinvestigated research area. Summarization is a way of describing the relationships of words in documents to the information content of that document (Luhn, 1958; Edmunson, 1969; Salton and Yang, 1973; Robertson and Walker, 1994; Church and Gale, 1999; Robertson, 2004). Recent work has looked at creating summaries of single and multiple documents (Radev et al., 2004; Erkan and Radev, 2004; Wan et al., 2007; Yin et al., 2012; Chatterjee et al., 2012), as well as summary evaluation (Jing et al., 1998; Tombros and Sanderson 1998; Mani et al., 1998; Mani et al., 1999; Mani, 2001; Lin and Hovy, 2003; Lin, 2004; Nenkova et al., 2007; Hobson et al., 2007; Owczarzak et al., 2012), query and topic biased summarization (Berger and Mittal, 2000; Otterbacher et a</context>
</contexts>
<marker>Luhn, 1958</marker>
<rawString>H. P. Luhn. The Automatic Creation of Literature Abstracts. IBM Journal of Research and Development, 2(2):159–165, April 1958.</rawString>
</citation>
<citation valid="true">
<authors>
<author>Inderjeet Mani</author>
<author>Eric Bloedorn</author>
<author>Barbara Gates</author>
</authors>
<title>Using Cohesion and Coherence Models for Text Summarization.</title>
<date>1998</date>
<booktitle>In AAAI Symposium</booktitle>
<tech>Technical Report SS-989-06,</tech>
<pages>69--76</pages>
<publisher>AAAI Press,</publisher>
<contexts>
<context position="7457" citStr="Mani et al., 1998" startWordPosition="1182" endWordPosition="1185">ommunity. 2 Related Work Automatic summarization is generally a wellinvestigated research area. Summarization is a way of describing the relationships of words in documents to the information content of that document (Luhn, 1958; Edmunson, 1969; Salton and Yang, 1973; Robertson and Walker, 1994; Church and Gale, 1999; Robertson, 2004). Recent work has looked at creating summaries of single and multiple documents (Radev et al., 2004; Erkan and Radev, 2004; Wan et al., 2007; Yin et al., 2012; Chatterjee et al., 2012), as well as summary evaluation (Jing et al., 1998; Tombros and Sanderson 1998; Mani et al., 1998; Mani et al., 1999; Mani, 2001; Lin and Hovy, 2003; Lin, 2004; Nenkova et al., 2007; Hobson et al., 2007; Owczarzak et al., 2012), query and topic biased summarization (Berger and Mittal, 2000; Otterbacher et al., 2005; Daume and Marcu, 2006; Chali and Joty, 2008; Otterbacher et al., 2009; Bando et al., 2010; Bhaskar and Bandyopadhyay, 2012; Harwath and Hazen, 2012; Yin et al., 2012), and summarization across languages (Pingali et al., 2007; Or˘asan and Chiorean, 2008; Wan et al., 2010; Azarbonyad et al., 2013). 2.1 Query Biased Summarization Previous work most closely related to our own come</context>
</contexts>
<marker>Mani, Bloedorn, Gates, 1998</marker>
<rawString>Inderjeet Mani, Eric Bloedorn, and Barbara Gates. Using Cohesion and Coherence Models for Text Summarization. In AAAI Symposium Technical Report SS-989-06, AAAI Press, 69–76, 1998.</rawString>
</citation>
<citation valid="true">
<authors>
<author>Inderjeet Mani</author>
<author>David House</author>
<author>Gary Klein</author>
<author>Lynette Hirschman</author>
<author>Therese Firmin</author>
<author>Beth Sundheim</author>
</authors>
<title>The TIPSTER SUMMAC Text Summarization Evaluation.</title>
<date>1999</date>
<booktitle>In Proceedings of European Association for Coputational Linguistics, EACL</booktitle>
<contexts>
<context position="7476" citStr="Mani et al., 1999" startWordPosition="1186" endWordPosition="1189"> Work Automatic summarization is generally a wellinvestigated research area. Summarization is a way of describing the relationships of words in documents to the information content of that document (Luhn, 1958; Edmunson, 1969; Salton and Yang, 1973; Robertson and Walker, 1994; Church and Gale, 1999; Robertson, 2004). Recent work has looked at creating summaries of single and multiple documents (Radev et al., 2004; Erkan and Radev, 2004; Wan et al., 2007; Yin et al., 2012; Chatterjee et al., 2012), as well as summary evaluation (Jing et al., 1998; Tombros and Sanderson 1998; Mani et al., 1998; Mani et al., 1999; Mani, 2001; Lin and Hovy, 2003; Lin, 2004; Nenkova et al., 2007; Hobson et al., 2007; Owczarzak et al., 2012), query and topic biased summarization (Berger and Mittal, 2000; Otterbacher et al., 2005; Daume and Marcu, 2006; Chali and Joty, 2008; Otterbacher et al., 2009; Bando et al., 2010; Bhaskar and Bandyopadhyay, 2012; Harwath and Hazen, 2012; Yin et al., 2012), and summarization across languages (Pingali et al., 2007; Or˘asan and Chiorean, 2008; Wan et al., 2010; Azarbonyad et al., 2013). 2.1 Query Biased Summarization Previous work most closely related to our own comes from Pingali et a</context>
</contexts>
<marker>Mani, House, Klein, Hirschman, Firmin, Sundheim, 1999</marker>
<rawString>Inderjeet Mani, David House, Gary Klein, Lynette Hirschman, Therese Firmin, and Beth Sundheim. The TIPSTER SUMMAC Text Summarization Evaluation. In Proceedings of European Association for Coputational Linguistics, EACL 1999.</rawString>
</citation>
<citation valid="true">
<authors>
<author>Inderjeet Mani</author>
</authors>
<title>Summarization Evaluation: An Overview.</title>
<date>2001</date>
<booktitle>In Proceedings of the NTCIR Workshop,</booktitle>
<volume>2</volume>
<contexts>
<context position="7488" citStr="Mani, 2001" startWordPosition="1190" endWordPosition="1191">marization is generally a wellinvestigated research area. Summarization is a way of describing the relationships of words in documents to the information content of that document (Luhn, 1958; Edmunson, 1969; Salton and Yang, 1973; Robertson and Walker, 1994; Church and Gale, 1999; Robertson, 2004). Recent work has looked at creating summaries of single and multiple documents (Radev et al., 2004; Erkan and Radev, 2004; Wan et al., 2007; Yin et al., 2012; Chatterjee et al., 2012), as well as summary evaluation (Jing et al., 1998; Tombros and Sanderson 1998; Mani et al., 1998; Mani et al., 1999; Mani, 2001; Lin and Hovy, 2003; Lin, 2004; Nenkova et al., 2007; Hobson et al., 2007; Owczarzak et al., 2012), query and topic biased summarization (Berger and Mittal, 2000; Otterbacher et al., 2005; Daume and Marcu, 2006; Chali and Joty, 2008; Otterbacher et al., 2009; Bando et al., 2010; Bhaskar and Bandyopadhyay, 2012; Harwath and Hazen, 2012; Yin et al., 2012), and summarization across languages (Pingali et al., 2007; Or˘asan and Chiorean, 2008; Wan et al., 2010; Azarbonyad et al., 2013). 2.1 Query Biased Summarization Previous work most closely related to our own comes from Pingali et al., (2007). </context>
</contexts>
<marker>Mani, 2001</marker>
<rawString>Inderjeet Mani. Summarization Evaluation: An Overview. In Proceedings of the NTCIR Workshop, Vol. 2, 2001.</rawString>
</citation>
<citation valid="true">
<authors>
<author>Inderjeet Mani</author>
<author>Gary Klein</author>
<author>David House</author>
<author>Lynette Hirschman</author>
<author>Therese Firmin</author>
<author>Beth Sundheim</author>
</authors>
<title>SUMMAC: A Text Summarization Evaluation.</title>
<date>2002</date>
<journal>Natural Language Engineering,</journal>
<volume>8</volume>
<issue>1</issue>
<pages>43--68</pages>
<contexts>
<context position="3353" citStr="Mani et al., 2002" startWordPosition="503" endWordPosition="506">et their information needs by submitting their queries in L1 to search through documents that have been composed in L2, even though they may not be familiar with L2 (Hovy et al., 1999; Pingali et al., 2007). There are no standards for objectively evaluating summaries for CLIR – a research gap that we begin to address in this paper. The problem we explore is two-fold: what kinds of summaries are well-suited for CLIR applications, and how should the summaries be evaluated. Our evaluation is extrinsic, that is to say we are interested in how summarization affects performance on a different task (Mani et al., 2002; McKeown et al., 2005; Dorr et al., 2005; Murray et al., 2009; McCallum et al., 2012). We use relevance prediction as our extrinsic task: a human must decide if a summary for a given document is relevant to a particular information need, or not. Relevance prediction is known to be useful as it correlates with some automatic intrinsic methods as well (President and Dorr, 2006; Hobson et al., 2007). To the best of our knowledge, we are the first to apply this evaluation framework to cross language query biased summarization. Each one of the summarization methods that we 657 Proceedings of the 2</context>
</contexts>
<marker>Mani, Klein, House, Hirschman, Firmin, Sundheim, 2002</marker>
<rawString>Inderjeet Mani, Gary Klein, David House, Lynette Hirschman, Therese Firmin, and Beth Sundheim. SUMMAC: A Text Summarization Evaluation. Natural Language Engineering, 8(1) 43-68. March 2002.</rawString>
</citation>
<citation valid="true">
<authors>
<author>Kathleen McKeown</author>
<author>Rebecca J Passonneau</author>
<author>David K Elson</author>
<author>Ani Nenkova</author>
<author>Julia Hirschberg</author>
</authors>
<title>Do Summaries Help? A Task-Based Evaluation of MultiDocument Summarization.</title>
<date>2005</date>
<booktitle>In Proceedings of the 28th Annual International ACM SIGIR Conference on Research and Development in Information Retrieval,</booktitle>
<pages>210--217</pages>
<publisher>ACM</publisher>
<contexts>
<context position="3375" citStr="McKeown et al., 2005" startWordPosition="507" endWordPosition="510">n needs by submitting their queries in L1 to search through documents that have been composed in L2, even though they may not be familiar with L2 (Hovy et al., 1999; Pingali et al., 2007). There are no standards for objectively evaluating summaries for CLIR – a research gap that we begin to address in this paper. The problem we explore is two-fold: what kinds of summaries are well-suited for CLIR applications, and how should the summaries be evaluated. Our evaluation is extrinsic, that is to say we are interested in how summarization affects performance on a different task (Mani et al., 2002; McKeown et al., 2005; Dorr et al., 2005; Murray et al., 2009; McCallum et al., 2012). We use relevance prediction as our extrinsic task: a human must decide if a summary for a given document is relevant to a particular information need, or not. Relevance prediction is known to be useful as it correlates with some automatic intrinsic methods as well (President and Dorr, 2006; Hobson et al., 2007). To the best of our knowledge, we are the first to apply this evaluation framework to cross language query biased summarization. Each one of the summarization methods that we 657 Proceedings of the 2014 Conference on Empi</context>
</contexts>
<marker>McKeown, Passonneau, Elson, Nenkova, Hirschberg, 2005</marker>
<rawString>Kathleen McKeown, Rebecca J. Passonneau, David K. Elson, Ani Nenkova, and Julia Hirschberg. Do Summaries Help? A Task-Based Evaluation of MultiDocument Summarization. In Proceedings of the 28th Annual International ACM SIGIR Conference on Research and Development in Information Retrieval, pp 210-217. ACM 2005.</rawString>
</citation>
<citation valid="false">
<authors>
<author>Anthony McCallum</author>
<author>Gerald Penn</author>
<author>Cosmin Munteanu</author>
<author>Xiaodan Zhu</author>
</authors>
<title>Ecological Validity and the Evaluation of Speech Summarization Quality.</title>
<booktitle>In Proceedings of Workshop on Evaluation Metrics and System Comparison for Automatic Summarization. 2012 Association for Computational Linguistics,</booktitle>
<pages>28--35</pages>
<location>Stroudsburg, PA, USA,</location>
<marker>McCallum, Penn, Munteanu, Zhu, </marker>
<rawString>Anthony McCallum, Gerald Penn, Cosmin Munteanu, and Xiaodan Zhu. Ecological Validity and the Evaluation of Speech Summarization Quality. In Proceedings of Workshop on Evaluation Metrics and System Comparison for Automatic Summarization. 2012 Association for Computational Linguistics, Stroudsburg, PA, USA, 28-35.</rawString>
</citation>
<citation valid="true">
<authors>
<author>Tatsunori Mori</author>
<author>Masanori Nozawa</author>
<author>Yoshiaki Asada</author>
</authors>
<title>Multi-Answer Focused Multi-Document Summarization Using a Question-Answering Engine.</title>
<date>2004</date>
<booktitle>In Proceedings of the 20th International Conference on Computational Linguistics, COLING ’04,</booktitle>
<location>Stroudsburg, PA, USA, ACL</location>
<contexts>
<context position="2596" citStr="Mori et al., 2004" startWordPosition="373" endWordPosition="376">tates Government. paper, we arrive at the intersection of each of these research areas. Query biased summarization (also known as query-focused, query-relevant, and query-dependent) involves automatically capturing relevant ideas and content from a document with respect to a given query, and presenting it as a condensed version of the original document. This kind of summarization is mostly used in search engines because when search results are tailored to a user’s information need, the user can find texts that they are looking for more quickly and more accurately (Tombros and Sanderson, 1998; Mori et al., 2004). Query biased summarization is a valuable research area in natural language processing (NLP), especially for CLIR. Users of CLIR systems meet their information needs by submitting their queries in L1 to search through documents that have been composed in L2, even though they may not be familiar with L2 (Hovy et al., 1999; Pingali et al., 2007). There are no standards for objectively evaluating summaries for CLIR – a research gap that we begin to address in this paper. The problem we explore is two-fold: what kinds of summaries are well-suited for CLIR applications, and how should the summarie</context>
</contexts>
<marker>Mori, Nozawa, Asada, 2004</marker>
<rawString>Tatsunori Mori, Masanori Nozawa, and Yoshiaki Asada. Multi-Answer Focused Multi-Document Summarization Using a Question-Answering Engine. In Proceedings of the 20th International Conference on Computational Linguistics, COLING ’04, Stroudsburg, PA, USA, ACL 2004.</rawString>
</citation>
<citation valid="true">
<authors>
<author>Gabriel Murray</author>
<author>Thomas Kleinbauer</author>
<author>Peter Poller</author>
<author>Tilman Becker</author>
<author>Steve Renals</author>
<author>Jonathan Kilgour</author>
</authors>
<title>Extrinsic Summarization Evaluation: A Decision Audit Task.</title>
<date>2009</date>
<journal>ACM Transactions on Speech and Language Processing, 6(2) Article</journal>
<volume>2</volume>
<contexts>
<context position="3415" citStr="Murray et al., 2009" startWordPosition="515" endWordPosition="518"> to search through documents that have been composed in L2, even though they may not be familiar with L2 (Hovy et al., 1999; Pingali et al., 2007). There are no standards for objectively evaluating summaries for CLIR – a research gap that we begin to address in this paper. The problem we explore is two-fold: what kinds of summaries are well-suited for CLIR applications, and how should the summaries be evaluated. Our evaluation is extrinsic, that is to say we are interested in how summarization affects performance on a different task (Mani et al., 2002; McKeown et al., 2005; Dorr et al., 2005; Murray et al., 2009; McCallum et al., 2012). We use relevance prediction as our extrinsic task: a human must decide if a summary for a given document is relevant to a particular information need, or not. Relevance prediction is known to be useful as it correlates with some automatic intrinsic methods as well (President and Dorr, 2006; Hobson et al., 2007). To the best of our knowledge, we are the first to apply this evaluation framework to cross language query biased summarization. Each one of the summarization methods that we 657 Proceedings of the 2014 Conference on Empirical Methods in Natural Language Proces</context>
</contexts>
<marker>Murray, Kleinbauer, Poller, Becker, Renals, Kilgour, 2009</marker>
<rawString>Gabriel Murray, Thomas Kleinbauer, Peter Poller, Tilman Becker, Steve Renals, and Jonathan Kilgour. Extrinsic Summarization Evaluation: A Decision Audit Task. ACM Transactions on Speech and Language Processing, 6(2) Article 2, October 2009.</rawString>
</citation>
<citation valid="true">
<authors>
<author>Ani Nenkova</author>
<author>Kathleen McKeown</author>
</authors>
<title>A Survey of Text Summarization Techniques. In</title>
<date>2012</date>
<booktitle>Mining Text Data,</booktitle>
<pages>43--76</pages>
<editor>C. C. Aggarwal and C. Zhai, editors,</editor>
<publisher>Springer US,</publisher>
<marker>Nenkova, McKeown, 2012</marker>
<rawString>Ani Nenkova and Kathleen McKeown. A Survey of Text Summarization Techniques. In C. C. Aggarwal and C. Zhai, editors, Mining Text Data, pp 43–76. Springer US, 2012.</rawString>
</citation>
<citation valid="true">
<authors>
<author>Constantin</author>
</authors>
<title>Or˘asan and Oana Andreea Chiorean. Evaluation of a Cross-Lingual Romanian-English MultiDocument Summariser.</title>
<date>2008</date>
<booktitle>In Proceedings of Language Resources and Evaluation Conference, LREC</booktitle>
<marker>Constantin, 2008</marker>
<rawString>Constantin Or˘asan and Oana Andreea Chiorean. Evaluation of a Cross-Lingual Romanian-English MultiDocument Summariser. In Proceedings of Language Resources and Evaluation Conference, LREC 2008.</rawString>
</citation>
<citation valid="true">
<authors>
<author>Jahna Otterbacher</author>
<author>G¨unes¸ Erkan</author>
<author>Dragomir R Ravev</author>
</authors>
<title>Using Random Walks for Question-focused Sentence Retrieval.</title>
<date>2005</date>
<booktitle>In Proceedings of Human Language Technology Conference on Empirical Methods in Natural Language Processing,</booktitle>
<pages>915--922</pages>
<location>Vancouver, Canada,</location>
<contexts>
<context position="7676" citStr="Otterbacher et al., 2005" startWordPosition="1220" endWordPosition="1223">ment (Luhn, 1958; Edmunson, 1969; Salton and Yang, 1973; Robertson and Walker, 1994; Church and Gale, 1999; Robertson, 2004). Recent work has looked at creating summaries of single and multiple documents (Radev et al., 2004; Erkan and Radev, 2004; Wan et al., 2007; Yin et al., 2012; Chatterjee et al., 2012), as well as summary evaluation (Jing et al., 1998; Tombros and Sanderson 1998; Mani et al., 1998; Mani et al., 1999; Mani, 2001; Lin and Hovy, 2003; Lin, 2004; Nenkova et al., 2007; Hobson et al., 2007; Owczarzak et al., 2012), query and topic biased summarization (Berger and Mittal, 2000; Otterbacher et al., 2005; Daume and Marcu, 2006; Chali and Joty, 2008; Otterbacher et al., 2009; Bando et al., 2010; Bhaskar and Bandyopadhyay, 2012; Harwath and Hazen, 2012; Yin et al., 2012), and summarization across languages (Pingali et al., 2007; Or˘asan and Chiorean, 2008; Wan et al., 2010; Azarbonyad et al., 2013). 2.1 Query Biased Summarization Previous work most closely related to our own comes from Pingali et al., (2007). In their work, they present their method for cross-language query biased summarization for Telugu and English. Their work was motivated by the need for people to have access to foreign-lan</context>
</contexts>
<marker>Otterbacher, Erkan, Ravev, 2005</marker>
<rawString>Jahna Otterbacher, G¨unes¸ Erkan, and Dragomir R Ravev. Using Random Walks for Question-focused Sentence Retrieval. In Proceedings of Human Language Technology Conference on Empirical Methods in Natural Language Processing, Vancouver, Canada, pp 915-922, EMNLP 2005.</rawString>
</citation>
<citation valid="true">
<authors>
<author>Jahna Otterbacher</author>
<author>G¨unes¸ Erkan</author>
<author>Dragomir R Ravev</author>
</authors>
<title>Biased LexRank: Passage Retrieval Using Random Walks With Question-Based Priors.</title>
<date>2009</date>
<booktitle>In Information Processing Management,</booktitle>
<volume>45</volume>
<issue>1</issue>
<pages>42--54</pages>
<contexts>
<context position="7747" citStr="Otterbacher et al., 2009" startWordPosition="1232" endWordPosition="1235">Walker, 1994; Church and Gale, 1999; Robertson, 2004). Recent work has looked at creating summaries of single and multiple documents (Radev et al., 2004; Erkan and Radev, 2004; Wan et al., 2007; Yin et al., 2012; Chatterjee et al., 2012), as well as summary evaluation (Jing et al., 1998; Tombros and Sanderson 1998; Mani et al., 1998; Mani et al., 1999; Mani, 2001; Lin and Hovy, 2003; Lin, 2004; Nenkova et al., 2007; Hobson et al., 2007; Owczarzak et al., 2012), query and topic biased summarization (Berger and Mittal, 2000; Otterbacher et al., 2005; Daume and Marcu, 2006; Chali and Joty, 2008; Otterbacher et al., 2009; Bando et al., 2010; Bhaskar and Bandyopadhyay, 2012; Harwath and Hazen, 2012; Yin et al., 2012), and summarization across languages (Pingali et al., 2007; Or˘asan and Chiorean, 2008; Wan et al., 2010; Azarbonyad et al., 2013). 2.1 Query Biased Summarization Previous work most closely related to our own comes from Pingali et al., (2007). In their work, they present their method for cross-language query biased summarization for Telugu and English. Their work was motivated by the need for people to have access to foreign-language documents from a search engine even though the users were not fam</context>
<context position="17666" citStr="Otterbacher et al. (2009)" startWordPosition="2857" endWordPosition="2860">o that words with a highter score appeared larger and more prominent in a word cloud. Words were shuffled such that the exact ordering of words was at random. I: Term Frequency (TF) Term frequency is very commonly used for finding important terms in a document. Given a term t in a document d, the number of times that term occurs is: tft,d = |t E d| II: Inverse Document Frequency (IDF) The idf term weighting is typically used in IR and other text categorization tasks to make distinctions between documents. The version of idf that we used throughout our work came from Erkan and Radev (2004) and Otterbacher et al. (2009), in keeping consistent with theirs. Let N be the number of documents in the collection, such that N = |D |and nt is the number of documents that contain term t, such that nt = |{d E D : t E d}|, then: N + 1 idft =log 0.5 x nt While idf is usually thought of as a type of heuristic, there have been some discussions about its theoretical basis (Robertson, 2004; Robertson and Walker, 1994; Church and Gale, 1999; Salton and Yang, 1973). An example of this summary is shown in Figure 3. III: Term Frequency Inverse Document Frequency (TFIDF) We use tfidft,d term weighting to find terms which are both</context>
<context position="22555" citStr="Otterbacher et al., 2009" startWordPosition="3708" endWordPosition="3711">We provide their sentence scoring formula here. This calculates the relevance score for a sentence s from document d, to a query q: Xrel(s|q) = log(tft,s +1)xlog(tft,q +1)xidft tEs Terms will occur in either the sentence or the query, or both. We applied this method to machine tranlsated English text. The output of this method is a relevance score for each sentence in a given document. We used those scores to rank sentences in each document from our English machine translated text. X. Query Biased Lexrank (LQ) We implemented query biased LexRank, a well-known graph-based summarization method (Otterbacher et al., 2009). It is a modified version of the original LexRank algorithm (Erkan and Radev, 2004; Page et al., 1998). The similarity metric, simx,y, also known as idf-modified cosine similarity, measures the distance between two sentences x and y in a document d, defined as: PtEx,y tft,x x tft,y x (idft)2 simx,y = q&amp;Ex tf idft,xq&amp;Ey t f idft y We used simx,y to score the similarity of sentence-to-sentence, resulting in a similarity 662 Figure 6: LQP - projecting Farsi sentence scores onto parallel English sentences. Figure 7: LQC - Farsi sentence scores are combined with parallel English sentence scores to</context>
<context position="23966" citStr="Otterbacher et al., 2009" startWordPosition="3955" endWordPosition="3958"> (t = 0.05), so that sentences above this threshold were given similarity 1, and 0 otherwise. We used rel(s|q) to score sentence-to-query. The LexRank score for each sentence was then calculated as: d × rels|q LQs|q = + (1 − d) × Ez∈C relz|q sims,v LQv Er∈adj [v] simv,r q where C is the set of all sentences in a given document. Here the parameter d is just a damper to designate a probability of randomly jumping to one of the sentences in the graph (d = 0.7). We found the stationary distribution by applying the power method (E = 5), which is guaranteed to converge to a stationary distribution (Otterbacher et al., 2009). The output of LQ is a score for each sentence from a given document with respect to a query. We used that score to rank sentences in each document from our English machine translated text. XI. Projected Cross-Language Query Biased Lexrank (LQP) We introduce LQP to describe a way of scoring and ranking sentences such that the L1 (English) summaries are biased from the L2 (Farsi) query and source document. Our goldstandard Farsi queries were included with our CLEF 2008 data, making them more reliable than what we could get from automatic translation. First, sentences from each Farsi document w</context>
</contexts>
<marker>Otterbacher, Erkan, Ravev, 2009</marker>
<rawString>Jahna Otterbacher, G¨unes¸ Erkan, and Dragomir R. Ravev. Biased LexRank: Passage Retrieval Using Random Walks With Question-Based Priors. In Information Processing Management, 45(1), January 2009, pp 42-54.</rawString>
</citation>
<citation valid="true">
<authors>
<author>Karolina Owczarzak</author>
<author>John M Conroy</author>
</authors>
<title>Hoa Trang Dang, and Ani Nenkova. An Assessment of the Accuracy of Automatic Evaluation in Summarization.</title>
<date>2012</date>
<booktitle>In Proceedings of the Workshop on Evaluation Metrics and System Comparison for Automatic Summarization,</booktitle>
<pages>1--9</pages>
<location>Montr´eal, Canada, ACL</location>
<marker>Owczarzak, Conroy, 2012</marker>
<rawString>Karolina Owczarzak, John M. Conroy, Hoa Trang Dang, and Ani Nenkova. An Assessment of the Accuracy of Automatic Evaluation in Summarization. In Proceedings of the Workshop on Evaluation Metrics and System Comparison for Automatic Summarization, pp 1-9, Montr´eal, Canada, ACL 2012.</rawString>
</citation>
<citation valid="true">
<authors>
<author>Lawrence Page</author>
<author>Sergey Brin</author>
<author>Rajeev Motwani</author>
<author>Terry Winograd</author>
</authors>
<title>The Pagerank Citation Ranking: Bringing Order to the Web. Technical report,</title>
<date>1998</date>
<booktitle>Stanford Digital Library Technologies Project,</booktitle>
<contexts>
<context position="22658" citStr="Page et al., 1998" startWordPosition="3727" endWordPosition="3730">ment d, to a query q: Xrel(s|q) = log(tft,s +1)xlog(tft,q +1)xidft tEs Terms will occur in either the sentence or the query, or both. We applied this method to machine tranlsated English text. The output of this method is a relevance score for each sentence in a given document. We used those scores to rank sentences in each document from our English machine translated text. X. Query Biased Lexrank (LQ) We implemented query biased LexRank, a well-known graph-based summarization method (Otterbacher et al., 2009). It is a modified version of the original LexRank algorithm (Erkan and Radev, 2004; Page et al., 1998). The similarity metric, simx,y, also known as idf-modified cosine similarity, measures the distance between two sentences x and y in a document d, defined as: PtEx,y tft,x x tft,y x (idft)2 simx,y = q&amp;Ex tf idft,xq&amp;Ey t f idft y We used simx,y to score the similarity of sentence-to-sentence, resulting in a similarity 662 Figure 6: LQP - projecting Farsi sentence scores onto parallel English sentences. Figure 7: LQC - Farsi sentence scores are combined with parallel English sentence scores to obtain sentence re-ranking. graph where each vertex was a sentence and each edge was the cosine simila</context>
</contexts>
<marker>Page, Brin, Motwani, Winograd, 1998</marker>
<rawString>Lawrence Page, Sergey Brin, Rajeev Motwani, and Terry Winograd. The Pagerank Citation Ranking: Bringing Order to the Web. Technical report, Stanford Digital Library Technologies Project, 1998.</rawString>
</citation>
<citation valid="true">
<authors>
<author>Prasad Pingali</author>
<author>Jagadeesh Jagarlamudi</author>
<author>Vasudeva Varma</author>
</authors>
<title>Experiments in Cross Language Query Focused Multi-Document Summarization In</title>
<date>2007</date>
<booktitle>Workshop on Cross Lingual Information Access Addressing the Information Need of Multilingual Societies, IJCAI</booktitle>
<contexts>
<context position="1513" citStr="Pingali et al., 2007" startWordPosition="206" endWordPosition="209">ord clouds are the best summarization strategy. In our analysis, we demonstrate that using the ROUGE metric on our sentence-based summaries cannot make the same kinds of distinctions as our evaluation framework does. Finally, we present our recommendations for creating muchneeded evaluation standards and datasets. 1 Introduction Despite many recent advances in query biased summarization for cross-language information retrieval (CLIR), there are no existing evaluation standards or datasets to make comparisons among different methods, and across different languages (Tombros and Sanderson, 1998; Pingali et al., 2007; McCallum et al., 2012; Bhaskar and Bandyopadhyay, 2012). Consider that creating this kind of summary requires familiarity with techniques from machine translation (MT), summarization, and information retrieval (IR). In this This work was sponsored by the Federal Bureau of Investigation under Air Force Contract FA8721-05-C-0002. Opinions, interpretations, conclusions, and recommendations are those of the authors and are not necessarily endorsed by the United States Government. paper, we arrive at the intersection of each of these research areas. Query biased summarization (also known as query</context>
<context position="2942" citStr="Pingali et al., 2007" startWordPosition="432" endWordPosition="435">al document. This kind of summarization is mostly used in search engines because when search results are tailored to a user’s information need, the user can find texts that they are looking for more quickly and more accurately (Tombros and Sanderson, 1998; Mori et al., 2004). Query biased summarization is a valuable research area in natural language processing (NLP), especially for CLIR. Users of CLIR systems meet their information needs by submitting their queries in L1 to search through documents that have been composed in L2, even though they may not be familiar with L2 (Hovy et al., 1999; Pingali et al., 2007). There are no standards for objectively evaluating summaries for CLIR – a research gap that we begin to address in this paper. The problem we explore is two-fold: what kinds of summaries are well-suited for CLIR applications, and how should the summaries be evaluated. Our evaluation is extrinsic, that is to say we are interested in how summarization affects performance on a different task (Mani et al., 2002; McKeown et al., 2005; Dorr et al., 2005; Murray et al., 2009; McCallum et al., 2012). We use relevance prediction as our extrinsic task: a human must decide if a summary for a given docum</context>
<context position="7902" citStr="Pingali et al., 2007" startWordPosition="1256" endWordPosition="1259"> and Radev, 2004; Wan et al., 2007; Yin et al., 2012; Chatterjee et al., 2012), as well as summary evaluation (Jing et al., 1998; Tombros and Sanderson 1998; Mani et al., 1998; Mani et al., 1999; Mani, 2001; Lin and Hovy, 2003; Lin, 2004; Nenkova et al., 2007; Hobson et al., 2007; Owczarzak et al., 2012), query and topic biased summarization (Berger and Mittal, 2000; Otterbacher et al., 2005; Daume and Marcu, 2006; Chali and Joty, 2008; Otterbacher et al., 2009; Bando et al., 2010; Bhaskar and Bandyopadhyay, 2012; Harwath and Hazen, 2012; Yin et al., 2012), and summarization across languages (Pingali et al., 2007; Or˘asan and Chiorean, 2008; Wan et al., 2010; Azarbonyad et al., 2013). 2.1 Query Biased Summarization Previous work most closely related to our own comes from Pingali et al., (2007). In their work, they present their method for cross-language query biased summarization for Telugu and English. Their work was motivated by the need for people to have access to foreign-language documents from a search engine even though the users were not familiar with the foreign language, in their case English. They used language modeling and translation probability to translate a user’s query into L2, and th</context>
<context position="11091" citStr="Pingali et al. (2007)" startWordPosition="1763" endWordPosition="1766">ion of the summaries. We attempt to overcome limitations of machine translation quality by using word clouds as one of our summarization strategies. Knowing when to translate is another challenge for cross-language query biased summarization. Several options exist for when and what to translate during the summarization process: (1) the source documents can be translated, (2) the user’s query can be translated, (3) the final summary can be translated, or (4) some combination of these. An example of translating only the summaries themselves can be found in Wan et al., (2010). On the other hand, Pingali et al. (2007) translated the queries and the summaries. In our work, we used gold-translated queries from the CLEF 2008 dataset, and machine translated source documents. We briefly address this in our work, but note that a full discussion of when and what to translate, and those effects on summarization quality, is outside of the scope of this paper. 2.3 Summarization Evaluation There has been a lot of work towards developing metrics for understanding what makes a summary good. Evaluation metrics are either intrinsic or extrinsic. Intrinsic metrics, such as ROUGE, measure the quality of a summary with resp</context>
<context position="32526" citStr="Pingali et al., (2007)" startWordPosition="5367" endWordPosition="5370"> 8 0.06960 0.12790 4 0.06850 0.12770 Tel-Eng-Sum 0.06048 0.12058 LQ 0.05124 0.09343 REL 0.04914 0.09081 6.2 Analysis with DUC 2005 We analysed our summarization methods by comparing two of our sentence-based methods (LQ and REL) with peers from the monolingual question-biased summarization shared-task for DUC 2005. Even though DUC 2005 is a monolingual task, we decided to use it as part of our analysis for two reasons: (1) to see how well we could do with query/question biasing while ignoring the variables introduced by MT and crosslanguage text, and (2) to make a comparison to previous work. Pingali et al., (2007) also used this the same DUC task to assess their cross-language query biased summarization system. Systems from the DUC 2005 question-biased summarization task were evaluated automatically against human gold-standard summaries using ROUGE (Lin and Hovy, 2003) . Our results from the DUC 2005 shared-task are shown in Table 2, reported as ROUGE-2 and ROUGE-SU4 f-scores, as these two variations of ROUGE are the most helpful (Dang, 2005; Pingali et al., 2007). Table 2 shows scores for several top peer systems, as well as results for the Tel-Eng-Sum method from Pingali et al., (2007). While we have</context>
</contexts>
<marker>Pingali, Jagarlamudi, Varma, 2007</marker>
<rawString>Prasad Pingali, Jagadeesh Jagarlamudi, and Vasudeva Varma. Experiments in Cross Language Query Focused Multi-Document Summarization In Workshop on Cross Lingual Information Access Addressing the Information Need of Multilingual Societies, IJCAI 2007.</rawString>
</citation>
<citation valid="true">
<authors>
<author>Stacy F President</author>
<author>Bonnie J Dorr</author>
</authors>
<title>Text Summarization Evaluation: Correlating Human Performance on an Extrinsic Task With Automatic Intrinsic Metrics.</title>
<date>2006</date>
<tech>No. LAMP-TR-133.</tech>
<institution>University of Maryland College Park Language and Media Processing Laboratory Institute for Advanced Computer Studies (UMIACS),</institution>
<contexts>
<context position="3731" citStr="President and Dorr, 2006" startWordPosition="571" endWordPosition="574">hat kinds of summaries are well-suited for CLIR applications, and how should the summaries be evaluated. Our evaluation is extrinsic, that is to say we are interested in how summarization affects performance on a different task (Mani et al., 2002; McKeown et al., 2005; Dorr et al., 2005; Murray et al., 2009; McCallum et al., 2012). We use relevance prediction as our extrinsic task: a human must decide if a summary for a given document is relevant to a particular information need, or not. Relevance prediction is known to be useful as it correlates with some automatic intrinsic methods as well (President and Dorr, 2006; Hobson et al., 2007). To the best of our knowledge, we are the first to apply this evaluation framework to cross language query biased summarization. Each one of the summarization methods that we 657 Proceedings of the 2014 Conference on Empirical Methods in Natural Language Processing (EMNLP), pages 657–669, October 25-29, 2014, Doha, Qatar. c�2014 Association for Computational Linguistics present in this paper belongs to one of the following strategies: (1) unbiased full machine translated text, (2) unbiased word clouds, (3) query biased word clouds, and (4) query biased sentence summaries</context>
</contexts>
<marker>President, Dorr, 2006</marker>
<rawString>Stacy F. President and Bonnie J. Dorr. Text Summarization Evaluation: Correlating Human Performance on an Extrinsic Task With Automatic Intrinsic Metrics. No. LAMP-TR-133. University of Maryland College Park Language and Media Processing Laboratory Institute for Advanced Computer Studies (UMIACS), 2006.</rawString>
</citation>
<citation valid="true">
<authors>
<author>Dragomir R Radev</author>
<author>Hongyan Jing</author>
<author>Malgorzata Sty´s</author>
<author>Daniel Tam</author>
</authors>
<title>Centroid-Based Summarization of Multiple Documents.</title>
<date>2004</date>
<journal>InProceedings of Informaion Processing Management,</journal>
<volume>40</volume>
<issue>6</issue>
<marker>Radev, Jing, Sty´s, Tam, 2004</marker>
<rawString>Dragomir R. Radev, Hongyan Jing, Malgorzata Sty´s, and Daniel Tam. Centroid-Based Summarization of Multiple Documents. InProceedings of Informaion Processing Management, 40(6):919–938, Nov. 2004.</rawString>
</citation>
<citation valid="true">
<authors>
<author>Stephen Robertson</author>
</authors>
<title>Understanding Inverse Document Frequency: on Theoretical Arguments for IDF.</title>
<date>2004</date>
<journal>Journal of Documentation,</journal>
<volume>60</volume>
<issue>5</issue>
<contexts>
<context position="7176" citStr="Robertson, 2004" startWordPosition="1134" endWordPosition="1135">related work; Section 3 describes our data and pre-processing; Section 4 details our summarization methods and strategies; Section 5 describes our experiments; Section 6 shows our results and analysis; and in Section 7, we conclude and discuss some future directions for the NLP community. 2 Related Work Automatic summarization is generally a wellinvestigated research area. Summarization is a way of describing the relationships of words in documents to the information content of that document (Luhn, 1958; Edmunson, 1969; Salton and Yang, 1973; Robertson and Walker, 1994; Church and Gale, 1999; Robertson, 2004). Recent work has looked at creating summaries of single and multiple documents (Radev et al., 2004; Erkan and Radev, 2004; Wan et al., 2007; Yin et al., 2012; Chatterjee et al., 2012), as well as summary evaluation (Jing et al., 1998; Tombros and Sanderson 1998; Mani et al., 1998; Mani et al., 1999; Mani, 2001; Lin and Hovy, 2003; Lin, 2004; Nenkova et al., 2007; Hobson et al., 2007; Owczarzak et al., 2012), query and topic biased summarization (Berger and Mittal, 2000; Otterbacher et al., 2005; Daume and Marcu, 2006; Chali and Joty, 2008; Otterbacher et al., 2009; Bando et al., 2010; Bhaskar</context>
<context position="18026" citStr="Robertson, 2004" startWordPosition="2932" endWordPosition="2933">t Frequency (IDF) The idf term weighting is typically used in IR and other text categorization tasks to make distinctions between documents. The version of idf that we used throughout our work came from Erkan and Radev (2004) and Otterbacher et al. (2009), in keeping consistent with theirs. Let N be the number of documents in the collection, such that N = |D |and nt is the number of documents that contain term t, such that nt = |{d E D : t E d}|, then: N + 1 idft =log 0.5 x nt While idf is usually thought of as a type of heuristic, there have been some discussions about its theoretical basis (Robertson, 2004; Robertson and Walker, 1994; Church and Gale, 1999; Salton and Yang, 1973). An example of this summary is shown in Figure 3. III: Term Frequency Inverse Document Frequency (TFIDF) We use tfidft,d term weighting to find terms which are both rare and important for a document, with respect to terms across all other documents in the collection: tfidft,d = tft,d x idft 4.3 Query Biased Word Clouds We generated query biased word clouds following the same principles as our unbiased word clouds, Figure 3: Word cloud summary for inverse document frequency (IDF), for query “Tehran’s stock market”. name</context>
</contexts>
<marker>Robertson, 2004</marker>
<rawString>Stephen Robertson. Understanding Inverse Document Frequency: on Theoretical Arguments for IDF. Journal of Documentation, 60(5):503–520, 2004.</rawString>
</citation>
<citation valid="true">
<authors>
<author>Stephen E Robertson</author>
<author>Steve Walker</author>
</authors>
<title>Some Simple Effective Approximations to the 2-Poisson Model for Probabilistic Weighted Retrieval.</title>
<date>1994</date>
<booktitle>In Proceedings of the 17th annual international ACM SIGIR conference on Research and development in information retrieval,</booktitle>
<pages>232--241</pages>
<publisher>Springer-Verlag</publisher>
<location>New York, Inc.,</location>
<contexts>
<context position="7135" citStr="Robertson and Walker, 1994" startWordPosition="1126" endWordPosition="1129">paper is structured as follows: Section 2 presents related work; Section 3 describes our data and pre-processing; Section 4 details our summarization methods and strategies; Section 5 describes our experiments; Section 6 shows our results and analysis; and in Section 7, we conclude and discuss some future directions for the NLP community. 2 Related Work Automatic summarization is generally a wellinvestigated research area. Summarization is a way of describing the relationships of words in documents to the information content of that document (Luhn, 1958; Edmunson, 1969; Salton and Yang, 1973; Robertson and Walker, 1994; Church and Gale, 1999; Robertson, 2004). Recent work has looked at creating summaries of single and multiple documents (Radev et al., 2004; Erkan and Radev, 2004; Wan et al., 2007; Yin et al., 2012; Chatterjee et al., 2012), as well as summary evaluation (Jing et al., 1998; Tombros and Sanderson 1998; Mani et al., 1998; Mani et al., 1999; Mani, 2001; Lin and Hovy, 2003; Lin, 2004; Nenkova et al., 2007; Hobson et al., 2007; Owczarzak et al., 2012), query and topic biased summarization (Berger and Mittal, 2000; Otterbacher et al., 2005; Daume and Marcu, 2006; Chali and Joty, 2008; Otterbacher </context>
<context position="18054" citStr="Robertson and Walker, 1994" startWordPosition="2934" endWordPosition="2937"> The idf term weighting is typically used in IR and other text categorization tasks to make distinctions between documents. The version of idf that we used throughout our work came from Erkan and Radev (2004) and Otterbacher et al. (2009), in keeping consistent with theirs. Let N be the number of documents in the collection, such that N = |D |and nt is the number of documents that contain term t, such that nt = |{d E D : t E d}|, then: N + 1 idft =log 0.5 x nt While idf is usually thought of as a type of heuristic, there have been some discussions about its theoretical basis (Robertson, 2004; Robertson and Walker, 1994; Church and Gale, 1999; Salton and Yang, 1973). An example of this summary is shown in Figure 3. III: Term Frequency Inverse Document Frequency (TFIDF) We use tfidft,d term weighting to find terms which are both rare and important for a document, with respect to terms across all other documents in the collection: tfidft,d = tft,d x idft 4.3 Query Biased Word Clouds We generated query biased word clouds following the same principles as our unbiased word clouds, Figure 3: Word cloud summary for inverse document frequency (IDF), for query “Tehran’s stock market”. namely the text font scaling and</context>
</contexts>
<marker>Robertson, Walker, 1994</marker>
<rawString>Stephen E. Robertson and Steve Walker. Some Simple Effective Approximations to the 2-Poisson Model for Probabilistic Weighted Retrieval. In Proceedings of the 17th annual international ACM SIGIR conference on Research and development in information retrieval, pp 232–241. Springer-Verlag New York, Inc., 1994.</rawString>
</citation>
<citation valid="true">
<authors>
<author>Gerard Salton</author>
<author>Chung-Shu Yang</author>
</authors>
<title>On the Specification of Term Values in Automatic Indexing.</title>
<date>1973</date>
<journal>Journal of Documentation,</journal>
<volume>29</volume>
<issue>4</issue>
<contexts>
<context position="7107" citStr="Salton and Yang, 1973" startWordPosition="1122" endWordPosition="1125">ents as possible. This paper is structured as follows: Section 2 presents related work; Section 3 describes our data and pre-processing; Section 4 details our summarization methods and strategies; Section 5 describes our experiments; Section 6 shows our results and analysis; and in Section 7, we conclude and discuss some future directions for the NLP community. 2 Related Work Automatic summarization is generally a wellinvestigated research area. Summarization is a way of describing the relationships of words in documents to the information content of that document (Luhn, 1958; Edmunson, 1969; Salton and Yang, 1973; Robertson and Walker, 1994; Church and Gale, 1999; Robertson, 2004). Recent work has looked at creating summaries of single and multiple documents (Radev et al., 2004; Erkan and Radev, 2004; Wan et al., 2007; Yin et al., 2012; Chatterjee et al., 2012), as well as summary evaluation (Jing et al., 1998; Tombros and Sanderson 1998; Mani et al., 1998; Mani et al., 1999; Mani, 2001; Lin and Hovy, 2003; Lin, 2004; Nenkova et al., 2007; Hobson et al., 2007; Owczarzak et al., 2012), query and topic biased summarization (Berger and Mittal, 2000; Otterbacher et al., 2005; Daume and Marcu, 2006; Chali </context>
<context position="18101" citStr="Salton and Yang, 1973" startWordPosition="2942" endWordPosition="2945"> other text categorization tasks to make distinctions between documents. The version of idf that we used throughout our work came from Erkan and Radev (2004) and Otterbacher et al. (2009), in keeping consistent with theirs. Let N be the number of documents in the collection, such that N = |D |and nt is the number of documents that contain term t, such that nt = |{d E D : t E d}|, then: N + 1 idft =log 0.5 x nt While idf is usually thought of as a type of heuristic, there have been some discussions about its theoretical basis (Robertson, 2004; Robertson and Walker, 1994; Church and Gale, 1999; Salton and Yang, 1973). An example of this summary is shown in Figure 3. III: Term Frequency Inverse Document Frequency (TFIDF) We use tfidft,d term weighting to find terms which are both rare and important for a document, with respect to terms across all other documents in the collection: tfidft,d = tft,d x idft 4.3 Query Biased Word Clouds We generated query biased word clouds following the same principles as our unbiased word clouds, Figure 3: Word cloud summary for inverse document frequency (IDF), for query “Tehran’s stock market”. namely the text font scaling and highlighting remained the same. IV. Query Bias</context>
</contexts>
<marker>Salton, Yang, 1973</marker>
<rawString>Gerard Salton and Chung-Shu Yang. On the Specification of Term Values in Automatic Indexing. Journal of Documentation, 29(4):351–372, 1973.</rawString>
</citation>
<citation valid="true">
<authors>
<author>Anastasios Tombros</author>
<author>Mark Sanderson</author>
</authors>
<title>Advantages of Query Biased Summaries in Information Retrieval.</title>
<date>1998</date>
<booktitle>In Proceedings of the 21st annual international ACM SIGIR conference on Research and development in information retrieval,</booktitle>
<pages>2--10</pages>
<publisher>ACM,</publisher>
<contexts>
<context position="1491" citStr="Tombros and Sanderson, 1998" startWordPosition="202" endWordPosition="205">s, but overall query biased word clouds are the best summarization strategy. In our analysis, we demonstrate that using the ROUGE metric on our sentence-based summaries cannot make the same kinds of distinctions as our evaluation framework does. Finally, we present our recommendations for creating muchneeded evaluation standards and datasets. 1 Introduction Despite many recent advances in query biased summarization for cross-language information retrieval (CLIR), there are no existing evaluation standards or datasets to make comparisons among different methods, and across different languages (Tombros and Sanderson, 1998; Pingali et al., 2007; McCallum et al., 2012; Bhaskar and Bandyopadhyay, 2012). Consider that creating this kind of summary requires familiarity with techniques from machine translation (MT), summarization, and information retrieval (IR). In this This work was sponsored by the Federal Bureau of Investigation under Air Force Contract FA8721-05-C-0002. Opinions, interpretations, conclusions, and recommendations are those of the authors and are not necessarily endorsed by the United States Government. paper, we arrive at the intersection of each of these research areas. Query biased summarizatio</context>
<context position="5408" citStr="Tombros and Sanderson, 1998" startWordPosition="849" endWordPosition="852">lready been retrieved from a search engine, as CLIR techniques are outside the scope of this paper. We predict that showing the full MT English text as a summarization strategy would not be particularly helpful in our relevance prediction task because the words in the text could be mixed-up, or sentences could be nonsensical, resulting in poor readability. For the same reasons, we expect that showing the full MT English text would take longer to arrive at a relevance decision. Finally, we predict that query biased summaries will result in faster, more accurate decisions from the participants (Tombros and Sanderson, 1998). We treat the actual CLIR search engine as if it were a black box so that we can focus on evaluating if the summaries themselves are useful. As a starting point, we begin with some principles that we expect to hold true when we evaluate. These principles provide us with the kind of framework that we need for a productive and judicious discussion about how well a summarization method works. We encourage the NLP community to consider the following concepts when developing evaluation standards for this problem: • End-user intelligiblity • Query-salience • Retrieval-relevance Summaries should be </context>
<context position="7438" citStr="Tombros and Sanderson 1998" startWordPosition="1178" endWordPosition="1181">ure directions for the NLP community. 2 Related Work Automatic summarization is generally a wellinvestigated research area. Summarization is a way of describing the relationships of words in documents to the information content of that document (Luhn, 1958; Edmunson, 1969; Salton and Yang, 1973; Robertson and Walker, 1994; Church and Gale, 1999; Robertson, 2004). Recent work has looked at creating summaries of single and multiple documents (Radev et al., 2004; Erkan and Radev, 2004; Wan et al., 2007; Yin et al., 2012; Chatterjee et al., 2012), as well as summary evaluation (Jing et al., 1998; Tombros and Sanderson 1998; Mani et al., 1998; Mani et al., 1999; Mani, 2001; Lin and Hovy, 2003; Lin, 2004; Nenkova et al., 2007; Hobson et al., 2007; Owczarzak et al., 2012), query and topic biased summarization (Berger and Mittal, 2000; Otterbacher et al., 2005; Daume and Marcu, 2006; Chali and Joty, 2008; Otterbacher et al., 2009; Bando et al., 2010; Bhaskar and Bandyopadhyay, 2012; Harwath and Hazen, 2012; Yin et al., 2012), and summarization across languages (Pingali et al., 2007; Or˘asan and Chiorean, 2008; Wan et al., 2010; Azarbonyad et al., 2013). 2.1 Query Biased Summarization Previous work most closely rela</context>
<context position="12207" citStr="Tombros and Sanderson, 1998" startWordPosition="1940" endWordPosition="1943"> are either intrinsic or extrinsic. Intrinsic metrics, such as ROUGE, measure the quality of a summary with respect to gold human-generated summaries (Lin, 2004; Lin and Hovy, 2003). Generating gold standard summaries is expensive and time-consuming, a problem that persists with cross-language query biased summarization because those summaries must be query biased as well as in a different language from the source documents. On the other hand, extrinsic metrics measure the quality of summaries at the system level, by looking at overall system performance on downstream tasks (Jing et al, 1998; Tombros and Sanderson, 1998). One of the most important findings for query biased summarization comes from Tombros and Sanderson (1998). In their monolingual taskbased evaluation, they measured user speed and accuracy at identifying relevant documents. They found that query biased summarization improved the user speed and accuracy when the user was asked to make relevance judgements for IR tasks. We also expect that our evaluation will demonstrate that user speed and accuracy is better when summaries are query biased. 3 Data and Pre-Processing We used data from the Farsi CLEF 2008 ad hoc task (Agirre et al., 2009). Each </context>
<context position="34987" citStr="Tombros and Sanderson (1998)" startWordPosition="5784" endWordPosition="5787"> two of our methods: LQP and LQC. To fully exploit the MT variable, we would need many more relevance prediction experiments using humans who know L1 and others who know L2. Unfortunately in our case, we were not able to find Farsi speakers on Mechanical Turk. Access to these speakers would have allowed us to try further experiments as well as other kinds of analysis. Our results on the relevance prediction task tell us that query biased summarization strategies help users identify relevant documents faster and with better accuracy than unbiased summaries. Our findings support the findings of Tombros and Sanderson (1998). Another important finding is that now we can weigh tradeoffs so that different summarization methods could be used to optimize over different metrics. For example, if we want to optimize for retrieval-relevance we might select a summarization method that tends to have higher recall, such as scaled query biased term frequency (SFQ). Similarly, we could optimize over accuracy on relevant documents, and use Combinatory LexRank (LQC) with Farsi and English together. We have shown that the relevance prediction tasks can be crowdsourced on Mechanical Turk with reasonable results. The data we used </context>
</contexts>
<marker>Tombros, Sanderson, 1998</marker>
<rawString>Anastasios Tombros and Mark Sanderson. Advantages of Query Biased Summaries in Information Retrieval. In Proceedings of the 21st annual international ACM SIGIR conference on Research and development in information retrieval, pp 2–10. ACM, 1998.</rawString>
</citation>
<citation valid="false">
<authors>
<author>Xiaojun Wan</author>
<author>Jianguo Xiao</author>
</authors>
<title>Graph-Based Multi-Modality Learning for Topic-Focused MultiDocument Summarization.</title>
<booktitle>In Proceedings of the 21st international jont conference on Artifical intelligence (IJCAI’09),</booktitle>
<pages>1586--1591</pages>
<location>San Francisco, CA, USA,</location>
<marker>Wan, Xiao, </marker>
<rawString>Xiaojun Wan and Jianguo Xiao. Graph-Based Multi-Modality Learning for Topic-Focused MultiDocument Summarization. In Proceedings of the 21st international jont conference on Artifical intelligence (IJCAI’09), San Francisco, CA, USA, 1586-1591.</rawString>
</citation>
<citation valid="false">
<authors>
<author>Xiaojun Wan</author>
<author>Huiying Li</author>
<author>Jianguo Xiao</author>
</authors>
<title>CrossLanguage Document Summarization Based on Machine Translation Quality Prediction.</title>
<booktitle>In Proceedings of the 48th Annual Meeting of the Association for Computational Linguistics (ACL ’10). Association for Computational Linguistics,</booktitle>
<pages>917--926</pages>
<location>Stroudsburg, PA, USA,</location>
<marker>Wan, Li, Xiao, </marker>
<rawString>Xiaojun Wan, Huiying Li, and Jianguo Xiao. CrossLanguage Document Summarization Based on Machine Translation Quality Prediction. In Proceedings of the 48th Annual Meeting of the Association for Computational Linguistics (ACL ’10). Association for Computational Linguistics, Stroudsburg, PA, USA, 917-926.</rawString>
</citation>
<citation valid="true">
<authors>
<author>Xiaojun Wan</author>
<author>Houping Jia</author>
<author>Shanshan Huang</author>
<author>Jianguo Xiao</author>
</authors>
<title>Summarizing the Differences in Multilingual News.</title>
<date>2011</date>
<booktitle>In Proceedings of the 34th International ACM SIGIR Conference on Research and Development in Information Retrieval, SIGIR ’11,</booktitle>
<pages>735--744</pages>
<publisher>ACM.</publisher>
<location>New York, NY, USA,</location>
<marker>Wan, Jia, Huang, Xiao, 2011</marker>
<rawString>Xiaojun Wan, Houping Jia, Shanshan Huang, and Jianguo Xiao. Summarizing the Differences in Multilingual News. In Proceedings of the 34th International ACM SIGIR Conference on Research and Development in Information Retrieval, SIGIR ’11, pp 735– 744, New York, NY, USA, 2011. ACM.</rawString>
</citation>
<citation valid="true">
<authors>
<author>Wenpeng Yin</author>
<author>Yulong Pei</author>
<author>Fan Zhang</author>
<author>Lian’en Huang</author>
</authors>
<title>SentTopic-MultiRank: A Novel Ranking Model for Multi-Document Summarization.</title>
<date>2012</date>
<booktitle>In Proceedings of COLING,</booktitle>
<pages>2977--2992</pages>
<contexts>
<context position="7334" citStr="Yin et al., 2012" startWordPosition="1160" endWordPosition="1163">; Section 6 shows our results and analysis; and in Section 7, we conclude and discuss some future directions for the NLP community. 2 Related Work Automatic summarization is generally a wellinvestigated research area. Summarization is a way of describing the relationships of words in documents to the information content of that document (Luhn, 1958; Edmunson, 1969; Salton and Yang, 1973; Robertson and Walker, 1994; Church and Gale, 1999; Robertson, 2004). Recent work has looked at creating summaries of single and multiple documents (Radev et al., 2004; Erkan and Radev, 2004; Wan et al., 2007; Yin et al., 2012; Chatterjee et al., 2012), as well as summary evaluation (Jing et al., 1998; Tombros and Sanderson 1998; Mani et al., 1998; Mani et al., 1999; Mani, 2001; Lin and Hovy, 2003; Lin, 2004; Nenkova et al., 2007; Hobson et al., 2007; Owczarzak et al., 2012), query and topic biased summarization (Berger and Mittal, 2000; Otterbacher et al., 2005; Daume and Marcu, 2006; Chali and Joty, 2008; Otterbacher et al., 2009; Bando et al., 2010; Bhaskar and Bandyopadhyay, 2012; Harwath and Hazen, 2012; Yin et al., 2012), and summarization across languages (Pingali et al., 2007; Or˘asan and Chiorean, 2008; Wa</context>
</contexts>
<marker>Yin, Pei, Zhang, Huang, 2012</marker>
<rawString>Wenpeng Yin, Yulong Pei, Fan Zhang, and Lian’en Huang. SentTopic-MultiRank: A Novel Ranking Model for Multi-Document Summarization. In Proceedings of COLING, pages 2977–2992, 2012.</rawString>
</citation>
<citation valid="true">
<authors>
<author>Junlin Zhang</author>
<author>Le Sun</author>
<author>Jinming Min</author>
</authors>
<title>Using the Web Corpus to Translate the Queries in CrossLingual Information Retrieval.</title>
<date>2005</date>
<journal>IEEE NLP-KE</journal>
<booktitle>In Proceedings in 2005 IEEE International Conference on Natural Language Processing and Knowledge Engineering,</booktitle>
<volume>05</volume>
<pages>493--498</pages>
<marker>Zhang, Le Sun, Min, 2005</marker>
<rawString>Junlin Zhang, Le Sun, and Jinming Min. Using the Web Corpus to Translate the Queries in CrossLingual Information Retrieval. In Proceedings in 2005 IEEE International Conference on Natural Language Processing and Knowledge Engineering, 2005, IEEE NLP-KE ’05, pp 493–498, 2005.</rawString>
</citation>
</citationList>
</algorithm>
</algorithms>
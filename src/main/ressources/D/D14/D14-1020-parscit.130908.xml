<?xml version="1.0" encoding="UTF-8"?>
<algorithms version="110505">
<algorithm name="SectLabel" version="110505">
<variant no="0" confidence="0.051275">
<title confidence="0.976965">
Testing for Significance of Increased Correlation with Human Judgment
</title>
<author confidence="0.994749">
Yvette Graham Timothy Baldwin
</author>
<affiliation confidence="0.9962035">
Department of Computing and Information Systems
The University of Melbourne
</affiliation>
<email confidence="0.98559">
graham.yvette@gmail.com, tb@ldwin.net
</email>
<sectionHeader confidence="0.993643" genericHeader="abstract">
Abstract
</sectionHeader>
<bodyText confidence="0.99986825">
Automatic metrics are widely used in ma-
chine translation as a substitute for hu-
man assessment. With the introduction
of any new metric comes the question of
just how well that metric mimics human
assessment of translation quality. This is
often measured by correlation with hu-
man judgment. Significance tests are gen-
erally not used to establish whether im-
provements over existing methods such as
BLEU are statistically significant or have
occurred simply by chance, however. In
this paper, we introduce a significance test
for comparing correlations of two metrics,
along with an open-source implementation
of the test. When applied to a range of
metrics across seven language pairs, tests
show that for a high proportion of metrics,
there is insufficient evidence to conclude
significant improvement over BLEU.
</bodyText>
<sectionHeader confidence="0.998993" genericHeader="introduction">
1 Introduction
</sectionHeader>
<bodyText confidence="0.999779807692308">
Within machine translation (MT), efforts are on-
going to improve evaluation metrics and find bet-
ter ways to automatically assess translation qual-
ity. The process of validating a new metric in-
volves demonstration that it correlates better with
human judgment than a standard metric such as
BLEU (Papineni et al., 2001). However, although
it is standard practice in MT evaluation to mea-
sure increases in automatic metric scores with sig-
nificance tests (Germann, 2003; Och, 2003; Ku-
mar and Byrne, 2004; Koehn, 2004; Riezler and
Maxwell, 2005; Graham et al., 2014), this has
not been the case in papers proposing new met-
rics. Thus it is possible that some reported im-
provements in correlation with human judgment
are attributable to chance rather than a systematic
improvement.
In this paper, we motivate and introduce a novel
significance test to assess the statistical signifi-
cance of differences in correlation with human
judgment for pairs of automatic metrics. We ap-
ply tests to the WMT-12 shared metrics task to
compare each of the participating methods, and
find that for a high proportion of metrics, there is
not enough evidence to conclude that they signifi-
cantly outperform BLEU.
</bodyText>
<sectionHeader confidence="0.892777" genericHeader="method">
2 Correlation with Human Judgment
</sectionHeader>
<bodyText confidence="0.999897233333333">
A common means of assessing automatic MT
evaluation metrics is Spearman’s rank correlation
with human judgments (Melamed et al., 2003),
which measures the relative degree of monotonic-
ity between the metric and human scores in the
range [−1, 1]. The standard justification for cal-
culating correlations over ranks rather than raw
scores is to: (a) reduce anomalies due to absolute
score differences; and (b) focus evaluation on what
is generally the primary area of interest, namely
the ranking of systems/translations.
An alternative means of evaluation is Pearson’s
correlation, which measures the linear correlation
between a metric and human scores (Leusch et al.,
2003). Debate on the relative merits of Spear-
man’s and Pearson’s correlation for the evaluation
of automatic metrics is ongoing, but there is an in-
creasing trend towards Pearson’s correlation, e.g.
in the recent WMT-14 shared metrics task.
Figure 1 presents the system-level results for
two evaluation metrics – AMBER (Chen et al.,
2012) and TERRORCAT (Fishel et al., 2012)
– over the WMT-12 Spanish-to-English metrics
task. These two metrics achieved the joint-highest
rank correlation (p = 0.965) for the task, but dif-
fer greatly in terms of Pearson’s correlation (r =
0.881 vs. 0.971, resp.). The largest contributor to
this artifact is the system with the lowest human
score, represented by the leftmost point in both
plots.
</bodyText>
<page confidence="0.9602">
172
</page>
<note confidence="0.486376">
Proceedings of the 2014 Conference on Empirical Methods in Natural Language Processing (EMNLP), pages 172–176,
October 25-29, 2014, Doha, Qatar. c�2014 Association for Computational Linguistics
</note>
<figure confidence="0.998791235294118">
●
●
●
●●●
●
●
●
●
●
AMBER
−3 −2 −1 0 1 2 3
TerrorCat
−3 −2 −1 0 1 2 3
Spearman: 0.965
Pearson: 0.881
●
Spearman: 0.965
Pearson: 0.971
●
●
●
●●
●
●
●
●
●
●●
−3 −2 −1 0 1 2 3
Human
(a) AMBER
−3 −2 −1 0 1 2 3
Human
(b) TERRORCAT
</figure>
<figureCaption confidence="0.9804335">
Figure 1: Scatter plot of human and automatic scores of WMT-12 Spanish-to-English systems for two
MT evaluation metrics (AMBER and TERRORCAT)
</figureCaption>
<bodyText confidence="0.999647">
Consistent with the WMT-14 metrics shared
task, we argue that Pearson’s correlation is more
sensitive than Spearman’s correlation. There is
still the question, however, of whether an observed
difference in Pearson’s r is statistically significant,
which we address in the next section.
</bodyText>
<sectionHeader confidence="0.944287" genericHeader="method">
3 Significance Testing
</sectionHeader>
<bodyText confidence="0.999972191489362">
Evaluation of a new automatic metric, Mnew,
commonly takes the form of quantifying the cor-
relation between the new metric and human judg-
ment, r(Mnew, H), and contrasting it with the cor-
relation for some baseline metric, r(Mbase, H). It
is very rare in the MT literature for significance
testing to be performed in such cases, however.
We introduce a statistical test which can be used
for this purpose, and apply the test to the evalua-
tion of metrics participating in the WMT-12 metric
evaluation task.
At first gloss, it might seem reasonable to per-
form significance testing in the following man-
ner when an increase in correlation with human
assessment is observed: apply a significance test
separately to the correlation of each metric with
human judgment, with the hope that the newly
proposed metric will achieve a significant correla-
tion where the baseline metric does not. However,
besides the fact that the correlation between al-
most any document-level metric and human judg-
ment will generally be significantly greater than
zero, the logic here is flawed: the fact that
one correlation is significantly higher than zero
(r(Mnew, H)) and that of another is not, does not
necessarily mean that the difference between the
two correlations is significant. Instead, a specific
test should be applied to the difference in corre-
lations on the data. For this same reason, con-
fidence intervals for individual correlations with
human judgment are also not particularly mean-
ingful.
In psychological studies, it is often the case that
samples that data are drawn from are independent,
and differences in correlations are computed on in-
dependent data sets. In such cases, the Fisher r
to z transformation is applied to test for signifi-
cant differences in correlations. In the case of au-
tomatic metric evaluation, however, the data sets
used are almost never independent. This means
that if r(Mbase, H) and r(Mnew, H) are both &gt; 0,
the correlation between the metric scores them-
selves, r(Mbase, Mnew), must also be &gt; 0. The
strength of this correlation, directly between pairs
of metrics, should be taken into account using a
significance test of the difference in correlation be-
tween r(Mbase, H) and r(Mnew, H).
</bodyText>
<subsectionHeader confidence="0.995906">
3.1 Correlated Correlations
</subsectionHeader>
<bodyText confidence="0.9999815">
Correlations computed for two separate automatic
metrics on the same data set are not independent,
and for this reason in order to test the difference in
correlation between them, the degree to which the
pair of metrics correlate with each other should be
taken into account. The Williams test (Williams,
</bodyText>
<page confidence="0.992095">
173
</page>
<figure confidence="0.99992506122449">
TerrorCat
TerrorCat
METEOR
METEOR
Sagan
Sagan
Sempos
Sempos
PosF
PosF
XEnErrCats
XEnErrCats
WBErrCats
WBErrCats
Amber
Amber
BErrCats
BErrCats
SimpBLEU
SimpBLEU
BLEU−4cc
BLEU−4cc
TER
TER
TerrorCat
METEOR
Sagan
Sempos
PosF
XEnErrCats
WBErrCats
Amber
BErrCats
SimpBLEU
BLEU.4cc
TER
TerrorCat
METEOR
Sagan
Sempos
PosF
XEnErrCats
WBErrCats
Amber
BErrCats
SimpBLEU
BLEU.4cc
TER
(a) Pearson’s correlation (b) Statistical significance
</figure>
<figureCaption confidence="0.8296905">
Figure 2: (a) Pearson’s correlation between pairs of automatic metrics; and (b) p-value of Williams
significance tests, where a colored cell in row i (named on y-axis), col j indicates that metric i (named
on x-axis) correlates significantly higher with human judgment than metric j; all results are based on the
WMT-12 Spanish-to-English data set.
</figureCaption>
<bodyText confidence="0.999371857142857">
1959)1 evaluates significance in a difference in de-
pendent correlations (Steiger, 1980). It is formu-
lated as follows, as a test of whether the population
correlation between X1 and X3 equals the popula-
tion correlation between X2 and X3:
where rij is the Pearson correlation between Xi
and Xj, n is the size of the population, and:
</bodyText>
<equation confidence="0.994338">
K = 1 − r122 − r132 − r232 + 2r12r13r23
</equation>
<bodyText confidence="0.999873571428571">
The Williams test is more powerful than the
equivalent for independent samples (Fisher r to
z), as it takes the correlations between X1 and
X2 (metric scores) into account. All else being
equal, the higher the correlation between the met-
ric scores, the greater the statistical power of the
test.
</bodyText>
<sectionHeader confidence="0.993825" genericHeader="evaluation">
4 Evaluation and Discussion
</sectionHeader>
<bodyText confidence="0.998866875">
Figure 2a is a heatmap of the degree to which au-
tomatic metrics correlate with one another when
computed on the same data set, in the form of the
Pearson’s correlation between each pair of met-
rics that participated in the WMT-12 metrics task
for Spanish-to-English evaluation. Metrics are or-
dered in all tables from highest to lowest correla-
tion with human assessment. In addition, for the
</bodyText>
<footnote confidence="0.9715555">
1Also sometimes referred to as the Hotelling–Williams
test.
</footnote>
<bodyText confidence="0.9833076">
purposes of significance testing, we take the abso-
lute value of all correlations, in order to compare
error-based metrics with non-error based ones.
In general, the correlation is high amongst all
pairs of metrics, with a high proportion of paired
metrics achieving a correlation in excess of r =
0.9. Two exceptions to this are TERRORCAT
(Fishel et al., 2012) and SAGAN (Castillo and Es-
trella, 2012), as seen in the regions of yellow and
white.
Figure 2b shows the results of Williams sig-
nificance tests for all pairs of metrics. Since we
are interested in not only identifying significant
differences in correlations, but ultimately ranking
competing metrics, we use a one-sided test. Here
again, the metrics are ordered from highest to low-
est (absolute) correlation with human judgment.
For the Spanish-to-English systems, approxi-
mately 60% of WMT-12 metric pairs show a sig-
nificant difference in correlation with human judg-
ment at p &lt; 0.05 (for one of the two metric di-
rections).2 As expected, the higher the correlation
with human judgment, the more metrics a given
method is superior to at a level of statistical signifi-
cance. Although TERRORCAT (Fishel et al., 2012)
achieves the highest absolute correlation with hu-
man judgment, it is not significantly better (p ≥
0.05) than the four next-best metrics (METEOR
(Denkowski and Lavie, 2011), SAGAN (Castillo
and Estrella, 2012), SEMPOS (Mach´aˇcek and Bo-
</bodyText>
<footnote confidence="0.710804666666667">
2Correlation matrices (red) are maximally filled, in con-
trast to one-sided significance test matrices (green), where, at
a maximum, fewer than half of the cells can be filled.
</footnote>
<equation confidence="0.994863166666667">
t(n 3)
(r13 − r23) Y (n − 1)(1 + r12)
— =
/2K (n−1) + (r23+r13)2 (1 − r12)3
V (n−3) 4
,
</equation>
<page confidence="0.996008">
174
</page>
<figure confidence="0.999962450819672">
(a) Czech-to-English
BLEU.4cc
SimpBLEU
Sempos
Amber
TER
Sagan
METEOR
TerrorCat
BErrCats
XEnErrCats
PosF
WBErrCats
Amber
TER
TerrorCat
XEnErrCats
WBErrCats
BLEU−4cc
SimpBLEU
Sempos
Sagan
METEOR
BErrCats
PosF
TerrorCat
EnXErrCats
Amber
BErrCats
WBErrCats
BLEU.4cc
PosF
SimpBLEU
TER
METEOR
Amber
TerrorCat
WBErrCats
TER
BLEU−4cc
SimpBLEU
EnXErrCats
BErrCats
PosF
METEOR
TerrorCat
Sempos
METEOR
SimpBLEU
BLEU.4cc
Amber
PosF
XEnErrCats
BErrCats
WBErrCats
TER
Sempos
METEOR
TerrorCat
Amber
BErrCats
PosF
WBErrCats
XEnErrCats
SimpBLEU
TER
BLEU−4cc
Sempos
METEOR
TerrorCat
Amber
BErrCats
PosF
WBErrCats
XEnErrCats
SimpBLEU
TER
BLEU.4cc
(c) German-to-English
TerrorCat
SimpBLEU
PosF
BErrCats
EnXErrCats
Amber
TER
WBErrCats
BLEU−4cc
METEOR
TerrorCat
SimpBLEU
PosF
BErrCats
EnXErrCats
Amber
TER
WBErrCats
BLEU.4cc
METEOR
TerrorCat
Sempos
METEOR
SimpBLEU
BLEU−4cc
Amber
PosF
XEnErrCats
BErrCats
WBErrCats
TER
(b) French-to-English
EnXErrCats
BErrCats
SimpBLEU
METEOR
WBErrCats
Amber
BLEU−4cc
TerrorCat
PosF
TER
(d) English-to-Spanish (e) English-to-French (f) English-to-German
</figure>
<figureCaption confidence="0.968098">
Figure 3: Significance results for pairs of automatic metrics for each WMT-12 language pair.
</figureCaption>
<figure confidence="0.9968173">
EnXErrCats
BErrCats
SimpBLEU
METEOR
WBErrCats
Amber
BLEU.4cc
TerrorCat
PosF
TER
</figure>
<bodyText confidence="0.998166826086957">
jar, 2011) and POSF (Popovic, 2012)). There is
not enough evidence to conclude, therefore, that
this metric is any better at evaluating Spanish-to-
English MT system quality than the next four met-
rics.
Figure 3 shows the results of significance tests
for the six other language pairs used in the WMT-
12 metrics shared task.3 For no language pair
is there an outright winner amongst the met-
rics, with proportions of significant differences be-
tween metrics for a given language pair ranging
from 3% for Czech-to-English to 82% for English-
to-French (p &lt; 0.05). The number of metrics that
significantly outperform BLEU for a given lan-
guage pair is only 34% (p &lt; 0.05), and no method
significantly outperforms BLEU over all language
pairs – indeed, even the best methods achieve sta-
tistical significance over BLEU for only a small
minority of language pairs. This underlines the
dangers of assessing metrics based solely on cor-
relation numbers, and emphasizes the importance
of statistical testing.
It is important to note that the number of com-
</bodyText>
<footnote confidence="0.5694285">
3We omit English-to-Czech due to some metric scores be-
ing omitted from the WMT-12 data set.
</footnote>
<bodyText confidence="0.999928529411765">
peting metrics a metric significantly outperforms
should not be used as the criterion for ranking
competing metrics. This is due to the fact that
the power of the Williams test to identify signifi-
cant differences between correlations changes de-
pending on the degree to which the pair of met-
rics correlate with each other. Therefore, a metric
that happens to correlate strongly with many other
metrics would be at an unfair advantage, were
numbers of significant wins to be used to rank met-
rics. For this reason, it is best to interpret pairwise
metric tests in isolation.
As part of this research, we have made avail-
able an open-source implementation of statis-
tical tests tailored to the assessment of MT
metrics available at https://github.com/
ygraham/significance-williams.
</bodyText>
<sectionHeader confidence="0.999634" genericHeader="conclusions">
5 Conclusions
</sectionHeader>
<bodyText confidence="0.999710833333333">
We have provided an analysis of current method-
ologies for evaluating automatic metrics in ma-
chine translation, and identified an issue with re-
spect to the lack of significance testing. We in-
troduced the Williams test as a means of cal-
culating the statistical significance of differences
</bodyText>
<page confidence="0.995936">
175
</page>
<bodyText confidence="0.999921">
in correlations for dependent samples. Analysis
of statistical significance in the WMT-12 metrics
shared task showed there is currently insufficient
evidence for a high proportion of metrics to con-
clude that they outperform BLEU.
</bodyText>
<sectionHeader confidence="0.995491" genericHeader="acknowledgments">
Acknowledgments
</sectionHeader>
<bodyText confidence="0.99935725">
We wish to thank the anonymous reviewers for
their valuable comments. This research was sup-
ported by funding from the Australian Research
Council.
</bodyText>
<sectionHeader confidence="0.998393" genericHeader="references">
References
</sectionHeader>
<reference confidence="0.999745226190477">
Julio Castillo and Paula Estrella. 2012. Semantic tex-
tual similarity for MT evaluation. In Proceedings of
the Seventh Workshop on Statistical Machine Trans-
lation, pages 52–58, Montr´eal, Canada.
Boxing Chen, Roland Kuhn, and George Foster. 2012.
Improving AMBER, an MT evaluation metric. In
Proceedings of the Seventh Workshop on Statisti-
cal Machine Translation, pages 59–63, Montr´eal,
Canada.
Michael Denkowski and Alon Lavie. 2011. Meteor
1.3: Automatic metric for reliable optimization and
evaluation of machine translation systems. In Pro-
ceedings of the Sixth Workshop on Statistical Ma-
chine Translation, pages 85–91, Edinburgh, UK.
Mark Fishel, Rico Sennrich, Maja Popovi´c, and Ond&amp;quot;rej
Bojar. 2012. TerrorCat: a translation error
categorization-based MT quality metric. In Pro-
ceedings of the Seventh Workshop on Statistical Ma-
chine Translation, pages 64–70, Montr´eal, Canada.
Ulrich Germann. 2003. Greedy decoding for statis-
tical machine translation in almost linear time. In
Proceedings of the 2003 Conference of the North
American Chapter of the Assoc. Computational Lin-
guistics on Human Language Technology-Volume 1,
pages 1–8, Edmonton, Canada.
Yvette Graham, Nitika Mathur, and Timothy Baldwin.
2014. Randomized significance tests in machine
translation. In Proceedings of the ACL 2014 Ninth
Workshop on Statistical Machine Translation, pages
266–274, Baltimore, USA.
Philipp Koehn. 2004. Statistical significance tests for
machine translation evaluation. In Proceedings of
Empirical Methods in Natural Language Processing
2004 (EMNLP 2004), pages 388–395, Barcelona,
Spain.
Shankar Kumar and William Byrne. 2004. Minimum
Bayes-risk decoding for statistical machine transla-
tion. In Proceedings of the 4th International Con-
ference on Human Language Technology Research
and 5th Annual Meeting of the NAACL (HLT-NAACL
2004), pages 169–176, Boston, USA.
Gregor Leusch, Nicola Ueffing, and Hermann Ney.
2003. A novel string-to-string distance measure
with applications to machine translation evaluation.
In Proceedings 9th Machine Translation Summit
(MT Summit IX), pages 240–247, New Orleans,
USA.
Matou&amp;quot;s Mach´a&amp;quot;cek and Ond&amp;quot;rej Bojar. 2011. Approx-
imating a deep-syntactic metric for MT evaluation
and tuning. In Proceedings of the Sixth Workshop on
Statistical Machine Translation, pages 92–98, Edin-
burgh, UK.
Dan Melamed, Ryan Green, and Joseph Turian. 2003.
Precision and recall of machine translation. In Pro-
ceedings of the 2003 Conference of the North Amer-
ican Chapter of the Association for Computational
Linguistics on Human Language Technology (HLT-
NAACL 2003) — Short Papers, pages 61–63, Ed-
monton, Canada.
Franz Josef Och. 2003. Minimum error rate train-
ing in statistical machine translation. In Proceed-
ings of the 41st Annual Meeting of the Association
for Computational Linguistics, pages 160–167, Sap-
poro, Japan.
Kishore Papineni, Salim Roukos, Todd Ward, and Wei-
Jing Zhu. 2001. BLEU: A method for automatic
evaluation of machine translation. Technical Report
RC22176 (W0109-022), IBM Research, Thomas J.
Watson Research Center.
Maja Popovic. 2012. Class error rates for evaluation
of machine translation output. In Proceedings of the
Seventh Workshop on Statistical Machine Transla-
tion, pages 71–75, Montr´eal, Canada.
Stefan Riezler and John T. Maxwell. 2005. On some
pitfalls in automatic evaluation and significance test-
ing for mt. In Proceedings of the ACL Workshop
on Intrinsic and Extrinsic Evaluation Measures for
Machine Translation and/or Summarization, pages
57–64, Ann Arbor, USA.
James H. Steiger. 1980. Tests for comparing ele-
ments of a correlation matrix. Psychological Bul-
letin, 87(2):245.
Evan J. Williams. 1959. Regression Analysis, vol-
ume 14. Wiley, New York, USA.
</reference>
<page confidence="0.998745">
176
</page>
</variant>
</algorithm>
<algorithm name="ParsHed" version="110505">
<variant no="0" confidence="0.637049">
<title confidence="0.998086">Testing for Significance of Increased Correlation with Human Judgment</title>
<author confidence="0.999758">Yvette Graham Timothy Baldwin</author>
<affiliation confidence="0.99788">Department of Computing and Information The University of Melbourne</affiliation>
<abstract confidence="0.880182">Automatic metrics are widely used in machine translation as a substitute for hu-</abstract>
</variant>
</algorithm>
<algorithm name="ParsCit" version="110505">
<citationList>
<citation valid="true">
<authors>
<author>Julio Castillo</author>
<author>Paula Estrella</author>
</authors>
<title>Semantic textual similarity for MT evaluation.</title>
<date>2012</date>
<booktitle>In Proceedings of the Seventh Workshop on Statistical Machine Translation,</booktitle>
<pages>52--58</pages>
<location>Montr´eal, Canada.</location>
<contexts>
<context position="9405" citStr="Castillo and Estrella, 2012" startWordPosition="1518" endWordPosition="1522">WMT-12 metrics task for Spanish-to-English evaluation. Metrics are ordered in all tables from highest to lowest correlation with human assessment. In addition, for the 1Also sometimes referred to as the Hotelling–Williams test. purposes of significance testing, we take the absolute value of all correlations, in order to compare error-based metrics with non-error based ones. In general, the correlation is high amongst all pairs of metrics, with a high proportion of paired metrics achieving a correlation in excess of r = 0.9. Two exceptions to this are TERRORCAT (Fishel et al., 2012) and SAGAN (Castillo and Estrella, 2012), as seen in the regions of yellow and white. Figure 2b shows the results of Williams significance tests for all pairs of metrics. Since we are interested in not only identifying significant differences in correlations, but ultimately ranking competing metrics, we use a one-sided test. Here again, the metrics are ordered from highest to lowest (absolute) correlation with human judgment. For the Spanish-to-English systems, approximately 60% of WMT-12 metric pairs show a significant difference in correlation with human judgment at p &lt; 0.05 (for one of the two metric directions).2 As expected, th</context>
</contexts>
<marker>Castillo, Estrella, 2012</marker>
<rawString>Julio Castillo and Paula Estrella. 2012. Semantic textual similarity for MT evaluation. In Proceedings of the Seventh Workshop on Statistical Machine Translation, pages 52–58, Montr´eal, Canada.</rawString>
</citation>
<citation valid="true">
<authors>
<author>Boxing Chen</author>
<author>Roland Kuhn</author>
<author>George Foster</author>
</authors>
<title>Improving AMBER, an MT evaluation metric.</title>
<date>2012</date>
<booktitle>In Proceedings of the Seventh Workshop on Statistical Machine Translation,</booktitle>
<pages>59--63</pages>
<location>Montr´eal, Canada.</location>
<contexts>
<context position="3274" citStr="Chen et al., 2012" startWordPosition="507" endWordPosition="510">ifferences; and (b) focus evaluation on what is generally the primary area of interest, namely the ranking of systems/translations. An alternative means of evaluation is Pearson’s correlation, which measures the linear correlation between a metric and human scores (Leusch et al., 2003). Debate on the relative merits of Spearman’s and Pearson’s correlation for the evaluation of automatic metrics is ongoing, but there is an increasing trend towards Pearson’s correlation, e.g. in the recent WMT-14 shared metrics task. Figure 1 presents the system-level results for two evaluation metrics – AMBER (Chen et al., 2012) and TERRORCAT (Fishel et al., 2012) – over the WMT-12 Spanish-to-English metrics task. These two metrics achieved the joint-highest rank correlation (p = 0.965) for the task, but differ greatly in terms of Pearson’s correlation (r = 0.881 vs. 0.971, resp.). The largest contributor to this artifact is the system with the lowest human score, represented by the leftmost point in both plots. 172 Proceedings of the 2014 Conference on Empirical Methods in Natural Language Processing (EMNLP), pages 172–176, October 25-29, 2014, Doha, Qatar. c�2014 Association for Computational Linguistics ● ● ● ●●● </context>
</contexts>
<marker>Chen, Kuhn, Foster, 2012</marker>
<rawString>Boxing Chen, Roland Kuhn, and George Foster. 2012. Improving AMBER, an MT evaluation metric. In Proceedings of the Seventh Workshop on Statistical Machine Translation, pages 59–63, Montr´eal, Canada.</rawString>
</citation>
<citation valid="true">
<authors>
<author>Michael Denkowski</author>
<author>Alon Lavie</author>
</authors>
<title>Meteor 1.3: Automatic metric for reliable optimization and evaluation of machine translation systems.</title>
<date>2011</date>
<booktitle>In Proceedings of the Sixth Workshop on Statistical Machine Translation,</booktitle>
<pages>85--91</pages>
<location>Edinburgh, UK.</location>
<contexts>
<context position="10351" citStr="Denkowski and Lavie, 2011" startWordPosition="1673" endWordPosition="1676">ed from highest to lowest (absolute) correlation with human judgment. For the Spanish-to-English systems, approximately 60% of WMT-12 metric pairs show a significant difference in correlation with human judgment at p &lt; 0.05 (for one of the two metric directions).2 As expected, the higher the correlation with human judgment, the more metrics a given method is superior to at a level of statistical significance. Although TERRORCAT (Fishel et al., 2012) achieves the highest absolute correlation with human judgment, it is not significantly better (p ≥ 0.05) than the four next-best metrics (METEOR (Denkowski and Lavie, 2011), SAGAN (Castillo and Estrella, 2012), SEMPOS (Mach´aˇcek and Bo2Correlation matrices (red) are maximally filled, in contrast to one-sided significance test matrices (green), where, at a maximum, fewer than half of the cells can be filled. t(n 3) (r13 − r23) Y (n − 1)(1 + r12) — = /2K (n−1) + (r23+r13)2 (1 − r12)3 V (n−3) 4 , 174 (a) Czech-to-English BLEU.4cc SimpBLEU Sempos Amber TER Sagan METEOR TerrorCat BErrCats XEnErrCats PosF WBErrCats Amber TER TerrorCat XEnErrCats WBErrCats BLEU−4cc SimpBLEU Sempos Sagan METEOR BErrCats PosF TerrorCat EnXErrCats Amber BErrCats WBErrCats BLEU.4cc PosF S</context>
</contexts>
<marker>Denkowski, Lavie, 2011</marker>
<rawString>Michael Denkowski and Alon Lavie. 2011. Meteor 1.3: Automatic metric for reliable optimization and evaluation of machine translation systems. In Proceedings of the Sixth Workshop on Statistical Machine Translation, pages 85–91, Edinburgh, UK.</rawString>
</citation>
<citation valid="true">
<authors>
<author>Mark Fishel</author>
<author>Rico Sennrich</author>
<author>Maja Popovi´c</author>
<author>Ondrej Bojar</author>
</authors>
<title>TerrorCat: a translation error categorization-based MT quality metric.</title>
<date>2012</date>
<booktitle>In Proceedings of the Seventh Workshop on Statistical Machine Translation,</booktitle>
<pages>64--70</pages>
<location>Montr´eal, Canada.</location>
<marker>Fishel, Sennrich, Popovi´c, Bojar, 2012</marker>
<rawString>Mark Fishel, Rico Sennrich, Maja Popovi´c, and Ond&amp;quot;rej Bojar. 2012. TerrorCat: a translation error categorization-based MT quality metric. In Proceedings of the Seventh Workshop on Statistical Machine Translation, pages 64–70, Montr´eal, Canada.</rawString>
</citation>
<citation valid="true">
<authors>
<author>Ulrich Germann</author>
</authors>
<title>Greedy decoding for statistical machine translation in almost linear time.</title>
<date>2003</date>
<booktitle>In Proceedings of the 2003 Conference of the North American Chapter of the Assoc. Computational Linguistics on Human Language Technology-Volume</booktitle>
<volume>1</volume>
<pages>1--8</pages>
<location>Edmonton, Canada.</location>
<contexts>
<context position="1513" citStr="Germann, 2003" startWordPosition="228" endWordPosition="229">pairs, tests show that for a high proportion of metrics, there is insufficient evidence to conclude significant improvement over BLEU. 1 Introduction Within machine translation (MT), efforts are ongoing to improve evaluation metrics and find better ways to automatically assess translation quality. The process of validating a new metric involves demonstration that it correlates better with human judgment than a standard metric such as BLEU (Papineni et al., 2001). However, although it is standard practice in MT evaluation to measure increases in automatic metric scores with significance tests (Germann, 2003; Och, 2003; Kumar and Byrne, 2004; Koehn, 2004; Riezler and Maxwell, 2005; Graham et al., 2014), this has not been the case in papers proposing new metrics. Thus it is possible that some reported improvements in correlation with human judgment are attributable to chance rather than a systematic improvement. In this paper, we motivate and introduce a novel significance test to assess the statistical significance of differences in correlation with human judgment for pairs of automatic metrics. We apply tests to the WMT-12 shared metrics task to compare each of the participating methods, and fin</context>
</contexts>
<marker>Germann, 2003</marker>
<rawString>Ulrich Germann. 2003. Greedy decoding for statistical machine translation in almost linear time. In Proceedings of the 2003 Conference of the North American Chapter of the Assoc. Computational Linguistics on Human Language Technology-Volume 1, pages 1–8, Edmonton, Canada.</rawString>
</citation>
<citation valid="true">
<authors>
<author>Yvette Graham</author>
<author>Nitika Mathur</author>
<author>Timothy Baldwin</author>
</authors>
<title>Randomized significance tests in machine translation.</title>
<date>2014</date>
<booktitle>In Proceedings of the ACL 2014 Ninth Workshop on Statistical Machine Translation,</booktitle>
<pages>266--274</pages>
<location>Baltimore, USA.</location>
<contexts>
<context position="1609" citStr="Graham et al., 2014" startWordPosition="243" endWordPosition="246">o conclude significant improvement over BLEU. 1 Introduction Within machine translation (MT), efforts are ongoing to improve evaluation metrics and find better ways to automatically assess translation quality. The process of validating a new metric involves demonstration that it correlates better with human judgment than a standard metric such as BLEU (Papineni et al., 2001). However, although it is standard practice in MT evaluation to measure increases in automatic metric scores with significance tests (Germann, 2003; Och, 2003; Kumar and Byrne, 2004; Koehn, 2004; Riezler and Maxwell, 2005; Graham et al., 2014), this has not been the case in papers proposing new metrics. Thus it is possible that some reported improvements in correlation with human judgment are attributable to chance rather than a systematic improvement. In this paper, we motivate and introduce a novel significance test to assess the statistical significance of differences in correlation with human judgment for pairs of automatic metrics. We apply tests to the WMT-12 shared metrics task to compare each of the participating methods, and find that for a high proportion of metrics, there is not enough evidence to conclude that they sign</context>
</contexts>
<marker>Graham, Mathur, Baldwin, 2014</marker>
<rawString>Yvette Graham, Nitika Mathur, and Timothy Baldwin. 2014. Randomized significance tests in machine translation. In Proceedings of the ACL 2014 Ninth Workshop on Statistical Machine Translation, pages 266–274, Baltimore, USA.</rawString>
</citation>
<citation valid="true">
<authors>
<author>Philipp Koehn</author>
</authors>
<title>Statistical significance tests for machine translation evaluation.</title>
<date>2004</date>
<booktitle>In Proceedings of Empirical Methods in Natural Language Processing</booktitle>
<pages>388--395</pages>
<location>Barcelona,</location>
<contexts>
<context position="1560" citStr="Koehn, 2004" startWordPosition="237" endWordPosition="238">etrics, there is insufficient evidence to conclude significant improvement over BLEU. 1 Introduction Within machine translation (MT), efforts are ongoing to improve evaluation metrics and find better ways to automatically assess translation quality. The process of validating a new metric involves demonstration that it correlates better with human judgment than a standard metric such as BLEU (Papineni et al., 2001). However, although it is standard practice in MT evaluation to measure increases in automatic metric scores with significance tests (Germann, 2003; Och, 2003; Kumar and Byrne, 2004; Koehn, 2004; Riezler and Maxwell, 2005; Graham et al., 2014), this has not been the case in papers proposing new metrics. Thus it is possible that some reported improvements in correlation with human judgment are attributable to chance rather than a systematic improvement. In this paper, we motivate and introduce a novel significance test to assess the statistical significance of differences in correlation with human judgment for pairs of automatic metrics. We apply tests to the WMT-12 shared metrics task to compare each of the participating methods, and find that for a high proportion of metrics, there </context>
</contexts>
<marker>Koehn, 2004</marker>
<rawString>Philipp Koehn. 2004. Statistical significance tests for machine translation evaluation. In Proceedings of Empirical Methods in Natural Language Processing 2004 (EMNLP 2004), pages 388–395, Barcelona, Spain.</rawString>
</citation>
<citation valid="true">
<authors>
<author>Shankar Kumar</author>
<author>William Byrne</author>
</authors>
<title>Minimum Bayes-risk decoding for statistical machine translation.</title>
<date>2004</date>
<booktitle>In Proceedings of the 4th International Conference on Human Language Technology Research and 5th Annual Meeting of the NAACL (HLT-NAACL 2004),</booktitle>
<pages>169--176</pages>
<location>Boston, USA.</location>
<contexts>
<context position="1547" citStr="Kumar and Byrne, 2004" startWordPosition="232" endWordPosition="236"> a high proportion of metrics, there is insufficient evidence to conclude significant improvement over BLEU. 1 Introduction Within machine translation (MT), efforts are ongoing to improve evaluation metrics and find better ways to automatically assess translation quality. The process of validating a new metric involves demonstration that it correlates better with human judgment than a standard metric such as BLEU (Papineni et al., 2001). However, although it is standard practice in MT evaluation to measure increases in automatic metric scores with significance tests (Germann, 2003; Och, 2003; Kumar and Byrne, 2004; Koehn, 2004; Riezler and Maxwell, 2005; Graham et al., 2014), this has not been the case in papers proposing new metrics. Thus it is possible that some reported improvements in correlation with human judgment are attributable to chance rather than a systematic improvement. In this paper, we motivate and introduce a novel significance test to assess the statistical significance of differences in correlation with human judgment for pairs of automatic metrics. We apply tests to the WMT-12 shared metrics task to compare each of the participating methods, and find that for a high proportion of me</context>
</contexts>
<marker>Kumar, Byrne, 2004</marker>
<rawString>Shankar Kumar and William Byrne. 2004. Minimum Bayes-risk decoding for statistical machine translation. In Proceedings of the 4th International Conference on Human Language Technology Research and 5th Annual Meeting of the NAACL (HLT-NAACL 2004), pages 169–176, Boston, USA.</rawString>
</citation>
<citation valid="true">
<authors>
<author>Gregor Leusch</author>
<author>Nicola Ueffing</author>
<author>Hermann Ney</author>
</authors>
<title>A novel string-to-string distance measure with applications to machine translation evaluation.</title>
<date>2003</date>
<booktitle>In Proceedings 9th Machine Translation Summit (MT Summit IX),</booktitle>
<pages>240--247</pages>
<location>New Orleans, USA.</location>
<contexts>
<context position="2942" citStr="Leusch et al., 2003" startWordPosition="454" endWordPosition="457">metrics is Spearman’s rank correlation with human judgments (Melamed et al., 2003), which measures the relative degree of monotonicity between the metric and human scores in the range [−1, 1]. The standard justification for calculating correlations over ranks rather than raw scores is to: (a) reduce anomalies due to absolute score differences; and (b) focus evaluation on what is generally the primary area of interest, namely the ranking of systems/translations. An alternative means of evaluation is Pearson’s correlation, which measures the linear correlation between a metric and human scores (Leusch et al., 2003). Debate on the relative merits of Spearman’s and Pearson’s correlation for the evaluation of automatic metrics is ongoing, but there is an increasing trend towards Pearson’s correlation, e.g. in the recent WMT-14 shared metrics task. Figure 1 presents the system-level results for two evaluation metrics – AMBER (Chen et al., 2012) and TERRORCAT (Fishel et al., 2012) – over the WMT-12 Spanish-to-English metrics task. These two metrics achieved the joint-highest rank correlation (p = 0.965) for the task, but differ greatly in terms of Pearson’s correlation (r = 0.881 vs. 0.971, resp.). The large</context>
</contexts>
<marker>Leusch, Ueffing, Ney, 2003</marker>
<rawString>Gregor Leusch, Nicola Ueffing, and Hermann Ney. 2003. A novel string-to-string distance measure with applications to machine translation evaluation. In Proceedings 9th Machine Translation Summit (MT Summit IX), pages 240–247, New Orleans, USA.</rawString>
</citation>
<citation valid="true">
<authors>
<author>Matous Mach´acek</author>
<author>Ondrej Bojar</author>
</authors>
<title>Approximating a deep-syntactic metric for MT evaluation and tuning.</title>
<date>2011</date>
<booktitle>In Proceedings of the Sixth Workshop on Statistical Machine Translation,</booktitle>
<pages>92--98</pages>
<location>Edinburgh, UK.</location>
<marker>Mach´acek, Bojar, 2011</marker>
<rawString>Matou&amp;quot;s Mach´a&amp;quot;cek and Ond&amp;quot;rej Bojar. 2011. Approximating a deep-syntactic metric for MT evaluation and tuning. In Proceedings of the Sixth Workshop on Statistical Machine Translation, pages 92–98, Edinburgh, UK.</rawString>
</citation>
<citation valid="true">
<authors>
<author>Dan Melamed</author>
<author>Ryan Green</author>
<author>Joseph Turian</author>
</authors>
<title>Precision and recall of machine translation.</title>
<date>2003</date>
<booktitle>In Proceedings of the 2003 Conference of the North American Chapter of the Association for Computational Linguistics on Human Language Technology (HLTNAACL 2003) — Short Papers,</booktitle>
<pages>61--63</pages>
<location>Edmonton, Canada.</location>
<contexts>
<context position="2404" citStr="Melamed et al., 2003" startWordPosition="371" endWordPosition="374">e rather than a systematic improvement. In this paper, we motivate and introduce a novel significance test to assess the statistical significance of differences in correlation with human judgment for pairs of automatic metrics. We apply tests to the WMT-12 shared metrics task to compare each of the participating methods, and find that for a high proportion of metrics, there is not enough evidence to conclude that they significantly outperform BLEU. 2 Correlation with Human Judgment A common means of assessing automatic MT evaluation metrics is Spearman’s rank correlation with human judgments (Melamed et al., 2003), which measures the relative degree of monotonicity between the metric and human scores in the range [−1, 1]. The standard justification for calculating correlations over ranks rather than raw scores is to: (a) reduce anomalies due to absolute score differences; and (b) focus evaluation on what is generally the primary area of interest, namely the ranking of systems/translations. An alternative means of evaluation is Pearson’s correlation, which measures the linear correlation between a metric and human scores (Leusch et al., 2003). Debate on the relative merits of Spearman’s and Pearson’s co</context>
</contexts>
<marker>Melamed, Green, Turian, 2003</marker>
<rawString>Dan Melamed, Ryan Green, and Joseph Turian. 2003. Precision and recall of machine translation. In Proceedings of the 2003 Conference of the North American Chapter of the Association for Computational Linguistics on Human Language Technology (HLTNAACL 2003) — Short Papers, pages 61–63, Edmonton, Canada.</rawString>
</citation>
<citation valid="true">
<authors>
<author>Franz Josef Och</author>
</authors>
<title>Minimum error rate training in statistical machine translation.</title>
<date>2003</date>
<booktitle>In Proceedings of the 41st Annual Meeting of the Association for Computational Linguistics,</booktitle>
<pages>160--167</pages>
<location>Sapporo, Japan.</location>
<contexts>
<context position="1524" citStr="Och, 2003" startWordPosition="230" endWordPosition="231">ow that for a high proportion of metrics, there is insufficient evidence to conclude significant improvement over BLEU. 1 Introduction Within machine translation (MT), efforts are ongoing to improve evaluation metrics and find better ways to automatically assess translation quality. The process of validating a new metric involves demonstration that it correlates better with human judgment than a standard metric such as BLEU (Papineni et al., 2001). However, although it is standard practice in MT evaluation to measure increases in automatic metric scores with significance tests (Germann, 2003; Och, 2003; Kumar and Byrne, 2004; Koehn, 2004; Riezler and Maxwell, 2005; Graham et al., 2014), this has not been the case in papers proposing new metrics. Thus it is possible that some reported improvements in correlation with human judgment are attributable to chance rather than a systematic improvement. In this paper, we motivate and introduce a novel significance test to assess the statistical significance of differences in correlation with human judgment for pairs of automatic metrics. We apply tests to the WMT-12 shared metrics task to compare each of the participating methods, and find that for </context>
</contexts>
<marker>Och, 2003</marker>
<rawString>Franz Josef Och. 2003. Minimum error rate training in statistical machine translation. In Proceedings of the 41st Annual Meeting of the Association for Computational Linguistics, pages 160–167, Sapporo, Japan.</rawString>
</citation>
<citation valid="true">
<authors>
<author>Kishore Papineni</author>
<author>Salim Roukos</author>
<author>Todd Ward</author>
<author>WeiJing Zhu</author>
</authors>
<title>BLEU: A method for automatic evaluation of machine translation.</title>
<date>2001</date>
<journal>IBM Research, Thomas J. Watson Research Center.</journal>
<tech>Technical Report RC22176 (W0109-022),</tech>
<contexts>
<context position="1366" citStr="Papineni et al., 2001" startWordPosition="203" endWordPosition="206">t for comparing correlations of two metrics, along with an open-source implementation of the test. When applied to a range of metrics across seven language pairs, tests show that for a high proportion of metrics, there is insufficient evidence to conclude significant improvement over BLEU. 1 Introduction Within machine translation (MT), efforts are ongoing to improve evaluation metrics and find better ways to automatically assess translation quality. The process of validating a new metric involves demonstration that it correlates better with human judgment than a standard metric such as BLEU (Papineni et al., 2001). However, although it is standard practice in MT evaluation to measure increases in automatic metric scores with significance tests (Germann, 2003; Och, 2003; Kumar and Byrne, 2004; Koehn, 2004; Riezler and Maxwell, 2005; Graham et al., 2014), this has not been the case in papers proposing new metrics. Thus it is possible that some reported improvements in correlation with human judgment are attributable to chance rather than a systematic improvement. In this paper, we motivate and introduce a novel significance test to assess the statistical significance of differences in correlation with hu</context>
</contexts>
<marker>Papineni, Roukos, Ward, Zhu, 2001</marker>
<rawString>Kishore Papineni, Salim Roukos, Todd Ward, and WeiJing Zhu. 2001. BLEU: A method for automatic evaluation of machine translation. Technical Report RC22176 (W0109-022), IBM Research, Thomas J. Watson Research Center.</rawString>
</citation>
<citation valid="true">
<authors>
<author>Maja Popovic</author>
</authors>
<title>Class error rates for evaluation of machine translation output.</title>
<date>2012</date>
<booktitle>In Proceedings of the Seventh Workshop on Statistical Machine Translation,</booktitle>
<pages>71--75</pages>
<location>Montr´eal, Canada.</location>
<contexts>
<context position="11957" citStr="Popovic, 2012" startWordPosition="1897" endWordPosition="1898">SimpBLEU PosF BErrCats EnXErrCats Amber TER WBErrCats BLEU−4cc METEOR TerrorCat SimpBLEU PosF BErrCats EnXErrCats Amber TER WBErrCats BLEU.4cc METEOR TerrorCat Sempos METEOR SimpBLEU BLEU−4cc Amber PosF XEnErrCats BErrCats WBErrCats TER (b) French-to-English EnXErrCats BErrCats SimpBLEU METEOR WBErrCats Amber BLEU−4cc TerrorCat PosF TER (d) English-to-Spanish (e) English-to-French (f) English-to-German Figure 3: Significance results for pairs of automatic metrics for each WMT-12 language pair. EnXErrCats BErrCats SimpBLEU METEOR WBErrCats Amber BLEU.4cc TerrorCat PosF TER jar, 2011) and POSF (Popovic, 2012)). There is not enough evidence to conclude, therefore, that this metric is any better at evaluating Spanish-toEnglish MT system quality than the next four metrics. Figure 3 shows the results of significance tests for the six other language pairs used in the WMT12 metrics shared task.3 For no language pair is there an outright winner amongst the metrics, with proportions of significant differences between metrics for a given language pair ranging from 3% for Czech-to-English to 82% for Englishto-French (p &lt; 0.05). The number of metrics that significantly outperform BLEU for a given language pa</context>
</contexts>
<marker>Popovic, 2012</marker>
<rawString>Maja Popovic. 2012. Class error rates for evaluation of machine translation output. In Proceedings of the Seventh Workshop on Statistical Machine Translation, pages 71–75, Montr´eal, Canada.</rawString>
</citation>
<citation valid="true">
<authors>
<author>Stefan Riezler</author>
<author>John T Maxwell</author>
</authors>
<title>On some pitfalls in automatic evaluation and significance testing for mt.</title>
<date>2005</date>
<booktitle>In Proceedings of the ACL Workshop on Intrinsic and Extrinsic Evaluation Measures for Machine Translation and/or Summarization,</booktitle>
<pages>57--64</pages>
<location>Ann Arbor, USA.</location>
<contexts>
<context position="1587" citStr="Riezler and Maxwell, 2005" startWordPosition="239" endWordPosition="242"> is insufficient evidence to conclude significant improvement over BLEU. 1 Introduction Within machine translation (MT), efforts are ongoing to improve evaluation metrics and find better ways to automatically assess translation quality. The process of validating a new metric involves demonstration that it correlates better with human judgment than a standard metric such as BLEU (Papineni et al., 2001). However, although it is standard practice in MT evaluation to measure increases in automatic metric scores with significance tests (Germann, 2003; Och, 2003; Kumar and Byrne, 2004; Koehn, 2004; Riezler and Maxwell, 2005; Graham et al., 2014), this has not been the case in papers proposing new metrics. Thus it is possible that some reported improvements in correlation with human judgment are attributable to chance rather than a systematic improvement. In this paper, we motivate and introduce a novel significance test to assess the statistical significance of differences in correlation with human judgment for pairs of automatic metrics. We apply tests to the WMT-12 shared metrics task to compare each of the participating methods, and find that for a high proportion of metrics, there is not enough evidence to c</context>
</contexts>
<marker>Riezler, Maxwell, 2005</marker>
<rawString>Stefan Riezler and John T. Maxwell. 2005. On some pitfalls in automatic evaluation and significance testing for mt. In Proceedings of the ACL Workshop on Intrinsic and Extrinsic Evaluation Measures for Machine Translation and/or Summarization, pages 57–64, Ann Arbor, USA.</rawString>
</citation>
<citation valid="true">
<authors>
<author>James H Steiger</author>
</authors>
<title>Tests for comparing elements of a correlation matrix.</title>
<date>1980</date>
<journal>Psychological Bulletin,</journal>
<volume>87</volume>
<issue>2</issue>
<contexts>
<context position="7947" citStr="Steiger, 1980" startWordPosition="1269" endWordPosition="1270">ErrCats SimpBLEU BLEU.4cc TER TerrorCat METEOR Sagan Sempos PosF XEnErrCats WBErrCats Amber BErrCats SimpBLEU BLEU.4cc TER (a) Pearson’s correlation (b) Statistical significance Figure 2: (a) Pearson’s correlation between pairs of automatic metrics; and (b) p-value of Williams significance tests, where a colored cell in row i (named on y-axis), col j indicates that metric i (named on x-axis) correlates significantly higher with human judgment than metric j; all results are based on the WMT-12 Spanish-to-English data set. 1959)1 evaluates significance in a difference in dependent correlations (Steiger, 1980). It is formulated as follows, as a test of whether the population correlation between X1 and X3 equals the population correlation between X2 and X3: where rij is the Pearson correlation between Xi and Xj, n is the size of the population, and: K = 1 − r122 − r132 − r232 + 2r12r13r23 The Williams test is more powerful than the equivalent for independent samples (Fisher r to z), as it takes the correlations between X1 and X2 (metric scores) into account. All else being equal, the higher the correlation between the metric scores, the greater the statistical power of the test. 4 Evaluation and Dis</context>
</contexts>
<marker>Steiger, 1980</marker>
<rawString>James H. Steiger. 1980. Tests for comparing elements of a correlation matrix. Psychological Bulletin, 87(2):245.</rawString>
</citation>
<citation valid="true">
<authors>
<author>Evan J Williams</author>
</authors>
<title>Regression Analysis,</title>
<date>1959</date>
<volume>14</volume>
<publisher>Wiley,</publisher>
<location>New York, USA.</location>
<marker>Williams, 1959</marker>
<rawString>Evan J. Williams. 1959. Regression Analysis, volume 14. Wiley, New York, USA.</rawString>
</citation>
</citationList>
</algorithm>
</algorithms>
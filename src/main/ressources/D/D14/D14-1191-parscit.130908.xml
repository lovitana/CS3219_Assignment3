<?xml version="1.0" encoding="UTF-8"?>
<algorithms version="110505">
<algorithm name="SectLabel" version="110505">
<variant no="0" confidence="0.031665">
<title confidence="0.998248">
Detecting Latent Ideology in Expert Text: Evidence From Academic
Papers in Economics
</title>
<author confidence="0.999778">
Zubin Jelveh&apos;, Bruce Kogut2, and Suresh Naidu3
</author>
<affiliation confidence="0.998131333333333">
&apos;Dept. of Computer Science &amp; Engineering, New York University
2Columbia Business School and Dept. of Sociology, Columbia University
3Dept. of Economics and SIPA, Columbia University
</affiliation>
<email confidence="0.986151">
zj292@nyu.edu, bruce.kogut@columbia.edu, sn2430@columbia.edu
</email>
<sectionHeader confidence="0.993784" genericHeader="abstract">
Abstract
</sectionHeader>
<bodyText confidence="0.999565611111111">
Previous work on extracting ideology
from text has focused on domains where
expression of political views is expected,
but it’s unclear if current technology can
work in domains where displays of ide-
ology are considered inappropriate. We
present a supervised ensemble n-gram
model for ideology extraction with topic
adjustments and apply it to one such do-
main: research papers written by academic
economists. We show economists’ polit-
ical leanings can be correctly predicted,
that our predictions generalize to new do-
mains, and that they correlate with public
policy-relevant research findings. We also
present evidence that unsupervised models
can under-perform in domains where ide-
ological expression is discouraged.
</bodyText>
<sectionHeader confidence="0.998988" genericHeader="keywords">
1 Introduction
</sectionHeader>
<bodyText confidence="0.999869017241379">
Recent advances in text mining demonstrate that
political ideology can be predicted from text –
often with great accuracy. Standard experimen-
tal settings in this literature are ones where ide-
ology is explicit, such as speeches by American
politicians or editorials by Israeli and Palestinian
authors. An open question is whether ideology
can be detected in arenas where it is strongly dis-
couraged. A further consideration for applied re-
searchers is whether these tools can offer insight
into questions of import for policymakers. To ad-
dress both of these issues, we examine one such
domain that is both policy-relevant and where ide-
ology is not overtly expressed: research papers
written by academic economists.
Why economics? Economic ideas are important
for shaping policy by influencing the public debate
and setting the range of expert opinion on various
policy options (Rodrik, 2014). Economics also
views itself as a science (Chetty, 2013) carefully
applying rigorous methodologies and using insti-
tutionalized safe-guards such as peer review. The
field’s most prominent research organization ex-
plicitly prohibits researchers from making policy
recommendations in papers that it releases (Na-
tional Bureau of Economic Research, 2010). De-
spite these measures, economics’ close proximity
to public policy decisions have led many to see it
as being driven by ideology (A.S., 2010). Does
this view of partisan economics have any empiri-
cal basis?
To answer the question of whether economics
is politicized or neutral, we present a supervised
ensemble n-gram model of ideology extraction
with topic adjustments.1 Our methodology is most
closely related to Taddy (2013) and Gentzkow and
Shapiro (2010), the latter of which used χ2 tests
to find phrases most associated with ideology as
proxied by the language of U.S. Congresspersons.
We improve on this methodology by accounting
for ideological word choice within topics and in-
corporating an ensemble approach that increases
predictive accuracy. We also motivate the need to
adjust for topics even if doing so does not improve
accuracy (although it does in this case). We further
provide evidence that fully unsupervised methods
(Mei et al., 2007; Lin et al., 2008; Ahmed and
Xing, 2010; Paul and Girju, 2010; Eisenstein et
al., 2011; Wang et al., 2012) may encounter dif-
ficulties learning latent ideological aspects when
those aspects are not first order in the data.
Our algorithm is able to correctly predict the
ideology of 69.2% of economists in our data
purely from their academic output. We also show
that our predictions generalize and are predictors
of responses by a panel of top economists on is-
sues of economic importance. In a companion
paper (Jelveh et al., 2014), we further show that
</bodyText>
<footnote confidence="0.992537">
1Grimmer and Stewart (2013) provide an overview of
models used for ideology detection.
</footnote>
<page confidence="0.954784">
1804
</page>
<bodyText confidence="0.939206666666667">
Proceedings of the 2014 Conference on Empirical Methods in Natural Language Processing (EMNLP), pages 1804–1809,
October 25-29, 2014, Doha, Qatar. c�2014 Association for Computational Linguistics
predicted ideologies are significantly correlated
to economists’ research findings. The latter re-
sult shows the relevance and applicability of these
tools beyond the task of ideology extraction.
</bodyText>
<sectionHeader confidence="0.980325" genericHeader="introduction">
2 Data
</sectionHeader>
<bodyText confidence="0.999413927083334">
Linking Economists to Their Political Activity:
We obtain the member directory of the Ameri-
can Economics Association (AEA) and link it to
two datasets: economists’ political campaign con-
tributions and petition signing activities. We ob-
tain campaign contribution data from the Federal
Election Commission’s website and petition sign-
ing data from Hedengren et al. (2010). From this
data, we construct a binary variable to indicate the
ground-truth ideologies of economists. See our
companion paper (Jelveh et al., 2014) for further
details on the construction of this dataset. Re-
vealed ideology through contributions and peti-
tions is largely consistent. Of 441 economists
appearing in both datasets, 83.4% showed agree-
ment between contributions and petitions. For
the final dataset of ground-truth authors we in-
clude all economists with campaign contribu-
tions and/or petition signatures, however, we drop
those economists whose ideologies where differ-
ent across the contribution and petition datasets.
Overall, 60% of 2,204 economists with imputed
ideologies in this final dataset are left-leaning
while 40% lean rightwards.
Economic Papers Corpus: To create our cor-
pus of academic writings by economists, we col-
lect 17,503 working papers from NBER’s website
covering June 1973 to October 2011. We also ob-
tained from JSTOR the fulltext of 62,888 research
articles published in 93 journals in economics for
the years 1991 to 2008. Combining the set of
economists and papers leaves us with 2,171 au-
thors with ground truth ideology and 17,870 pa-
pers they wrote. From the text of these papers we
create n-grams of length two through eight. While
n-grams greater than three words in length are un-
common, Margolin et al. (2013) demonstrate that
ideological word choice can be detected by longer
phrases. To capture other expressions of ideol-
ogy not revealed in adjacent terms, we also in-
clude skipgrams of length two by combining non-
adjacent terms that are three to five words apart.
We remove phrases used by fewer than five au-
thors.
Topic Adjustments: Table 1 presents the top
20 most conservative and liberal bigrams ranked
by χ2 scores from a Pearson’s test of indepen-
dence between phrase usage by left- and right-
leaning economists. It appears that top ideo-
logical phrases are related to specific research
subfields. For example, right-leaning terms
‘free bank’, ‘stock return’, and ‘feder reserv’ are
related to finance and left-leaning terms ‘men-
tal health’, ‘child care’, and ‘birth weight’ are re-
lated to health care. This observation leads us to
ask: Are apparently ideological phrases merely
a by-product of an economist’s research interest
rather than reflective of true ideology?
To see why this is a critical question, consider
that ideology has both direct and indirect effects
on word choice, the former of which is what we
wish to capture. The indirect pathway is through
topic: ideology may influence the research area
an economist enters into, but not the word choice
within that area. In that case, if more conserva-
tive economists choose macroeconomics, the ob-
served correlation between macro-related phrases
and right-leaning ideology would be spurious. The
implication is that accounting for topics may not
necessarily improve performance but provide evi-
dence to support an underlying model of how ide-
ology affects word choice. Therefore, to better
capture the direct effect of ideology on phrase us-
age we adjust our predictions by topic by creating
mappings from papers to topics. For a topic map-
ping, we predict economists’ ideologies from their
word choice within each topic and combine these
results to form and overall prediction. We com-
pare different supervised and unsupervised topic
mappings and assess their predictive ability.
To create supervised topic mappings, we take
advantage of the fact that economics papers are
manually categorized by the Journal of Economic
Literature (JEL). These codes are hierarchical in-
dicators of an article’s subject area. For exam-
ple, the code C51 can be read, in increasing order
of specificity, as Mathematical and Quantitative
Methods (C), Econometric Modeling (C5), Model
Construction and Estimation (C51). We construct
two sets of topic mappings: JEL1 derived from
the 1st-level codes (e.g. C) and JEL2 derived from
the 2nd-level codes (e.g. C5). The former cov-
ers broad areas (e.g. macroeconomics, microeco-
nomics, etc.) while the latter contains more refined
ones (e.g. monetary policy, firm behavior, etc.).
For unsupervised mappings, we run Latent
</bodyText>
<page confidence="0.980438">
1805
</page>
<table confidence="0.984915047619048">
Left-Leaning Bigrams Right-Leaning Bigrams
mental health public choic
post keynesian stock return
child care feder reserv
labor market yes yes
health care market valu
work time journal financi
keynesian econom bank note
high school money suppli
polici analys free bank
analys politiqu liquid effect
politiqu vol journal financ
birth weight median voter
labor forc law econom
journal post vote share
latin america war spend
mental ill journal law
medic care money demand
labour market gold reserv
social capit anna j
singl mother switch cost
</table>
<tableCaption confidence="0.999929">
Table 1: Top 20 bigrams and trigrams.
</tableCaption>
<bodyText confidence="0.999653142857143">
Dirichilet Allocation (Blei et al., 2003) on our cor-
pus. We use 30, 50, and 100 topics to create
LDA30, LDA50, and LDA100 topic mappings.
We use the topic distributions estimated by LDA
to assign articles to topics. A paper p is assigned
to a topic t if the probability that t appears in p
is greater than 5%. While 5% might seem to be a
lower threshold, the topic distributions estimated
by LDA tend to be sparse. For example, even with
50 topics to ‘choose’ from in LDA50 and a thresh-
old of 5%, 99.5% of the papers would be assigned
to five or fewer topics. This compares favorably
with JEL2 codings where 98.8% of papers have
five or fewer topics.
</bodyText>
<sectionHeader confidence="0.998484" genericHeader="method">
3 Algorithm
</sectionHeader>
<bodyText confidence="0.999972545454546">
There are two components to our topic-adjusted
algorithm for ideology prediction. First, we focus
on n-grams and skipgrams that are most correlated
with ideology in the training data. For each topic
within a topic mapping, we count the total num-
ber of times each phrase is used by all left- and
all right-leaning economists. Then, we compute
Pearson’s χ2 statistic and associated p-values and
keep phrases with p &lt; 0.05. As an additional fil-
ter, we split the data into ten folds and perform the
χ2 test within each fold. For each topic, we keep
phrases that are consistently ideological across all
folds. This greatly reduces the number of ideo-
logical phrases. For LDA50, the mean number of
ideological phrases per topic before the cross val-
idation filter is 12,932 but falls to 963 afterwards.
With the list of ideological phrases in hand, the
second step is to iterate over each topic and predict
the ideologies of economists in our test set. To
compute the predictions we perform partial least
squares (PLS): With our training data, we con-
struct the standardized frequency matrix Ft,train
where the (e, p)-th entry is the number of times
economist e used partisan phrase p across all of
e’s papers in t. This number is divided by the total
number of phrases used by e in topic t. For papers
with multiple authors, each author gets same count
of phrases. About 5% of the papers in our dataset
are written by authors with differing ideologies.
We do not treat these differently. Columns of
Ft,train are standardized to have unit variance. Let
y be the vector of ground-truth ideologies, test set
ideologies are predicted as follows:
</bodyText>
<listItem confidence="0.987578166666667">
1) Compute w = Corr(Ft,train, y), the corre-
lations between each phrase and ideology
2) Project to one dimension: z = Ft,trainw
3) Regress ideology, y, on the constructed vari-
able z: y = b1 z
4) Predict ideology ˆye of new economist by
</listItem>
<bodyText confidence="0.9936971">
ˆye = b1˜fe�w, (˜fe is scaled frequency vector)
To avoid over-fitting we introduce an ensemble
element: For each t, we sample from the list of
significant n-grams in t and sample with replace-
ment from the authors who have written in t.2 PLS
is performed on this sample data 125 times. Each
PLS iteration can be viewed as a vote on whether
an author is left- or right-leaning. We calculate
the vote as follows. For each iteration, we pre-
dict the ideologies of economists in the training
data. We find the threshold f that minimizes the
distance between the true and false positive rates
for the current iteration and the same rates for the
perfect classifier: 1.0 and 0.0, respectively. Then,
an author in the test set is voted left-leaning if
yt,test &lt; f and right-leaning otherwise.
For a given topic mapping, our algorithm re-
turns a three-dimensional array with the (e, t, c)-th
entry representing the number of votes economist
e received in topic t for ideology c (left- or right-
</bodyText>
<footnote confidence="0.9604455">
2The number of phrases sampled each iteration is the
square root of the number of ideological phrases in the topic.
</footnote>
<page confidence="0.994786">
1806
</page>
<bodyText confidence="0.999957083333333">
leaning). To produce a final prediction, we sum
across the second dimension and compute ideol-
ogy as the percentage of right-leaning votes re-
ceived across all topics within a topic-mapping.
Therefore, ideology values closer to zero are as-
sociated with a left-leaning ideology and values
closer to one are associated with a rightward lean.
To recap, we start with a topic mapping and then
for each topic run an ensemble algorithm with PLS
at its core.3 The output for each topic is a set of
votes. We sum across topics to compute a final
prediction for ideology.
</bodyText>
<sectionHeader confidence="0.981235" genericHeader="evaluation">
4 Validation and Results
</sectionHeader>
<bodyText confidence="0.999983375">
We split our ground-truth set of 2,171 authors
into training (80%) and test sets (20%) and com-
pute predictions as above. As our data exhibits
skew with 1.5 left-leaning for every right-leaning
economist, we report the area under the curve
(AUC) which is robust to class skew (Fawcett,
2006). It’s worth noting that a classifier that ran-
domly predicts a liberal economist 60% of the
time would have an AUC of 0.5. To compare
our model with fully unsupervised methods, we
also include results from running the Topic-Aspect
Model (TAM) (Paul and Girju, 2010) on our data.
TAM decomposes documents into two compo-
nents: one affecting topics and one affecting a la-
tent aspect that influences all topics in a similar
manner. We run TAM with 30 topics and 2 aspects
(TAM2/30). We follow Paul and Girju and use the
learned topic and aspect distributions as training
data for a SVM.4
Columns 2 to 4 from Table 2 show that our
models’ predictions have a clear association with
ground-truth ideology. The LDA topic mappings
outperform the supervised mappings as well as a
model that does not adjust for topics (NoTopic).
Perhaps not surprisingly, TAM does not perform
well in our domain. A drawback of unsupervised
methods is that the learned aspects may not be re-
lated to ideology but some other hidden factor.
For further insight into how well our model
generalizes, we use data from Gordon and Dahl
(2013) to compare our predictions to potentially
ideological responses of economists on a survey
</bodyText>
<footnote confidence="0.9273746">
3Other predictions algorithms could be dropped in for
PLS. Logistic regression and SVM produced similar results.
4Authors are treated as documents. TAM is run for 1,000
iterations with the following priors: α = 1.0, β = 1.0, γ0 =
1, γ1 = 1, S0 = 20, S1 = 80.
</footnote>
<table confidence="0.9674546">
(1) (2) (3) (4)
Topic Accu- Corr. w/ AUC
Map racy(%) Truth
LDA50 69.2 0.381 0.719
LDA100 66.3 0.364 0.707
LDA30 65.0 0.313 0.674
NoTopic 63.9 0.290 0.672
JEL1 61.0 0.263 0.647
JEL2 61.8 0.240 0.646
TAM2/30 61.5 0.228 0.580
</table>
<tableCaption confidence="0.876756">
Table 2: Model comparisons
</tableCaption>
<table confidence="0.99717475">
(1) (2) (3)
LDA50 1.814*** 2.457*** 2.243***
Log-Lik. -1075.0 -758.7 -740.6
JEL1 1.450*** 2.128*** 1.799***
Log-Lik. -1075.3 -757.4 -740.5
No Topic 0.524*** 0.659*** 0.824***
Log-Lik. -1075.3 -760.5 -741.0
Question No Yes Yes
Demog./Prof. No No Yes
Observations 715 715 715
Individuals 39 39 39
* p &lt; 0.10, ** p &lt; 0.05, *** p &lt; 0.01
</table>
<tableCaption confidence="0.998298">
Table 3: IGM correlations. Column (1) shows re-
</tableCaption>
<bodyText confidence="0.919220842105263">
sults of regression of response on predicted ideol-
ogy. Column (2) adds question dummies. Column
(3) adds demographic and professional variables.
conducted by the University of Chicago.5 Each
survey question asks for an economists opinion on
an issue of political relevance such as minimum
wages or tax rates. For further details on the data
see Gordon and Dahl. Of importance here is that
Gordon and Dahl categorize 22 questions where
agreement (disagreement) with the statement im-
plies belief a conservative (liberal) viewpoint.
To see if our predicted ideologies are corre-
lated with survey responses, we run an ordered-
logistic regression (McCullagh, 1980). Survey
responses are coded with the following order:
Strongly Disagree, Disagree, Uncertain, Agree,
Strongly Agree. We regress survey responses onto
predicted ideologies. We also include question-
level dummies and explanatory variables for a re-
</bodyText>
<footnote confidence="0.995022">
5http://igmchicago.org
</footnote>
<page confidence="0.994957">
1807
</page>
<bodyText confidence="0.999819677419355">
spondent’s gender, year of Ph.D., Ph.D. univer-
sity, NBER membership, and experience in federal
government. Table 3 shows the results of these re-
gressions for three topic mappings. The correla-
tion between our predictions and survey respon-
dents are all strongly significant.
One way to interpret these results is to com-
pare the change in predicted probability of pro-
viding an Agree or Strongly Agree answer (agree-
ing with the conservative view point) if we change
predicted ideology from most liberal to most con-
servative. For NoTopic, this predicted probabil-
ity is 35% when ideology is set to most liberal
and jumps to 73.7% when set to most conserva-
tive. This difference increases for topic-adjusted
models. For LDA50, the probability of a conser-
vative answer when ideology is set to most liberal
is 14.5% and 93.8% for most conservative.
Figure 1 compares the predicted probabilities of
choosing different answers when ideology is set
to most liberal and most conservative. Our topic-
adjusted models suggest that the most conserva-
tive economists are much more likely to strongly
agree with a conservative response than for the
most liberal economists to strongly agree with a
liberal response. It is worthwhile to note from
the small increase in log-likelihood in Table 3
when controls are added, suggesting that our ide-
ology scores are much better predictors of IGM
responses than demographic and professional con-
trols.
</bodyText>
<sectionHeader confidence="0.998177" genericHeader="conclusions">
5 Conclusions and Future Work
</sectionHeader>
<bodyText confidence="0.999774533333333">
We’ve presented a supervised methodology for ex-
tracting political sentiment in a domain where it’s
discouraged and shown how it even predicts the
partisanship calculated from completely unrelated
IGM survey data. In a companion paper (Jelveh et
al., 2014) we further demonstrate how this tool can
be used to aid policymakers in de-biasing research
findings. When compared to domains where ideo-
logical language is expected, our predictive ability
is reduced. Future work should disentangle how
much this difference is due to modeling decisions
and limitations versus actual absence of ideology.
Future works should also investigate how fully un-
supervised methods can be extended to match our
performance.
</bodyText>
<figure confidence="0.984087833333333">
60
50
40
30
20
10
0
60
50
40
30
20
10
0
60
50
40
30
20
10
0
Str. Dis. Dis. Uncert. Agr. Str. Agr.
conservative
liberal
</figure>
<figureCaption confidence="0.999337">
Figure 1: The predicted probability of agreeing
</figureCaption>
<bodyText confidence="0.527793333333333">
with a conservative response when ideology is
set to most liberal (gray) and most conservative
(black).
</bodyText>
<sectionHeader confidence="0.953749" genericHeader="acknowledgments">
Acknowledgement
</sectionHeader>
<bodyText confidence="0.9998615">
This work was supported in part by the NSF (un-
der grant 0966187). The views and conclusions
contained in this document are those of the authors
and should not be interpreted as necessarily rep-
resenting the official policies, either expressed or
implied, of any of the sponsors.
</bodyText>
<figure confidence="0.924793">
NOTOPIC
LDA50
JEL1
</figure>
<page confidence="0.973833">
1808
</page>
<sectionHeader confidence="0.964153" genericHeader="references">
References
</sectionHeader>
<reference confidence="0.995886056338028">
Amr Ahmed and Eric P. Xing. 2010. Staying in-
formed: supervised and semi-supervised multi-view
topical analysis of ideological perspective. In Pro-
ceedings of the 2010 Conference on Empirical Meth-
ods in Natural Language Processing, pages 1140–
1150. Association for Computational Linguistics.
A.S. 2010. Is economics a right-wing conspiracy?
The Economist, August.
David M. Blei, Andrew Y. Ng, and Michael I. Jordan.
2003. Latent dirichlet allocation. the Journal of ma-
chine Learning research, 3:993–1022.
Raj Chetty. 2013. Yes, economics is a science. The
New York Times, October.
Jacob Eisenstein, Amr Ahmed, and Eric P. Xing. 2011.
Sparse additive generative models of text. In Pro-
ceedings of the 28th International Conference on
Machine Learning (ICML-11), pages 1041–1048.
T. Fawcett. 2006. An introduction to ROC analysis.
Pattern recognition letters, 27(8):861–874.
Matthew Gentzkow and Jesse M. Shapiro. 2010. What
drives media slant? evidence from U.S. daily news-
papers. Econometrica, 78(1):35–71.
Roger Gordon and Gordon B Dahl. 2013. Views
among economists: Professional consensus or point-
counterpoint? American Economic Review,
103(3):629–635, May.
J. Grimmer and B. M. Stewart. 2013. Text as data:
The promise and pitfalls of automatic content anal-
ysis methods for political texts. Political Analysis,
21(3):267–297, January.
David Hedengren, Daniel B. Klein, and Carrie Mil-
ton. 2010. Economist petitions: Ideology revealed.
Econ Journal Watch, 7(3):288–319.
Zubin Jelveh, Bruce Kogut, and Suresh Naidu. 2014.
Political language in economics.
Wei-Hao Lin, Eric Xing, and Alexander Hauptmann.
2008. A joint topic and perspective model for
ideological discourse. In Machine Learning and
Knowledge Discovery in Databases, pages 17–32.
Springer.
Drew Margolin, Yu-Ru Lin, and David Lazer. 2013.
Why so similar?: Identifying semantic organizing
processes in large textual corpora.
Peter McCullagh. 1980. Regression models for ordinal
data. Journal of the royal statistical society. Series
B (Methodological), pages 109–142.
Qiaozhu Mei, Xu Ling, Matthew Wondra, Hang Su,
and ChengXiang Zhai. 2007. Topic sentiment mix-
ture: modeling facets and opinions in weblogs. In
Proceedings of the 16th international conference on
World Wide Web, pages 171–180. ACM.
National Bureau of Economic Research. 2010.
Amended and restated by-laws of national bureau of
economic research, inc.
Michael Paul and Roxana Girju. 2010. A two-
dimensional topic-aspect model for discovering
multi-faceted topics. Urbana, 51.
Dani Rodrik. 2014. When ideas trump interests:
Preferences, worldviews, and policy innovations .
Journal of Economic Perspectives, 28(1):189–208,
February.
Matt Taddy. 2013. Multinomial inverse regression for
text analysis. Journal of the American StatisticalAs-
sociation, 108.
William Yang Wang, Elijah Mayfield, Suresh Naidu,
and Jeremiah Dittmar. 2012. Historical analysis
of legal opinions with a sparse mixed-effects latent
variable model. In Proceedings of the 50th Annual
Meeting of the Association for Computational Lin-
guistics: Long Papers-Volume 1, pages 740–749.
Association for Computational Linguistics.
</reference>
<page confidence="0.99655">
1809
</page>
</variant>
</algorithm>
<algorithm name="ParsHed" version="110505">
<variant no="0" confidence="0.455987">
<title confidence="0.78021775">Detecting Latent Ideology in Expert Text: Evidence From Papers in Economics Bruce and Suresh of Computer Science &amp; Engineering, New York</title>
<author confidence="0.639469">of Sociology</author>
<author confidence="0.639469">Columbia</author>
<affiliation confidence="0.999208">of Economics and SIPA, Columbia University</affiliation>
<email confidence="0.987805">zj292@nyu.edu,bruce.kogut@columbia.edu,sn2430@columbia.edu</email>
<abstract confidence="0.997530526315789">Previous work on extracting ideology from text has focused on domains where expression of political views is expected, but it’s unclear if current technology can work in domains where displays of ideology are considered inappropriate. We a supervised ensemble model for ideology extraction with topic adjustments and apply it to one such domain: research papers written by academic economists. We show economists’ political leanings can be correctly predicted, that our predictions generalize to new domains, and that they correlate with public policy-relevant research findings. We also present evidence that unsupervised models can under-perform in domains where ideological expression is discouraged.</abstract>
</variant>
</algorithm>
<algorithm name="ParsCit" version="110505">
<citationList>
<citation valid="true">
<authors>
<author>Amr Ahmed</author>
<author>Eric P Xing</author>
</authors>
<title>Staying informed: supervised and semi-supervised multi-view topical analysis of ideological perspective.</title>
<date>2010</date>
<booktitle>In Proceedings of the 2010 Conference on Empirical Methods in Natural Language Processing,</booktitle>
<pages>1140--1150</pages>
<institution>Association for Computational Linguistics.</institution>
<contexts>
<context position="3352" citStr="Ahmed and Xing, 2010" startWordPosition="502" endWordPosition="505"> methodology is most closely related to Taddy (2013) and Gentzkow and Shapiro (2010), the latter of which used χ2 tests to find phrases most associated with ideology as proxied by the language of U.S. Congresspersons. We improve on this methodology by accounting for ideological word choice within topics and incorporating an ensemble approach that increases predictive accuracy. We also motivate the need to adjust for topics even if doing so does not improve accuracy (although it does in this case). We further provide evidence that fully unsupervised methods (Mei et al., 2007; Lin et al., 2008; Ahmed and Xing, 2010; Paul and Girju, 2010; Eisenstein et al., 2011; Wang et al., 2012) may encounter difficulties learning latent ideological aspects when those aspects are not first order in the data. Our algorithm is able to correctly predict the ideology of 69.2% of economists in our data purely from their academic output. We also show that our predictions generalize and are predictors of responses by a panel of top economists on issues of economic importance. In a companion paper (Jelveh et al., 2014), we further show that 1Grimmer and Stewart (2013) provide an overview of models used for ideology detection.</context>
</contexts>
<marker>Ahmed, Xing, 2010</marker>
<rawString>Amr Ahmed and Eric P. Xing. 2010. Staying informed: supervised and semi-supervised multi-view topical analysis of ideological perspective. In Proceedings of the 2010 Conference on Empirical Methods in Natural Language Processing, pages 1140– 1150. Association for Computational Linguistics.</rawString>
</citation>
<citation valid="true">
<authors>
<author>A S</author>
</authors>
<title>Is economics a right-wing conspiracy? The Economist,</title>
<date>2010</date>
<marker>S, 2010</marker>
<rawString>A.S. 2010. Is economics a right-wing conspiracy? The Economist, August.</rawString>
</citation>
<citation valid="true">
<authors>
<author>David M Blei</author>
<author>Andrew Y Ng</author>
<author>Michael I Jordan</author>
</authors>
<title>Latent dirichlet allocation.</title>
<date>2003</date>
<journal>the Journal of machine Learning research,</journal>
<pages>3--993</pages>
<contexts>
<context position="9494" citStr="Blei et al., 2003" startWordPosition="1467" endWordPosition="1470">ft-Leaning Bigrams Right-Leaning Bigrams mental health public choic post keynesian stock return child care feder reserv labor market yes yes health care market valu work time journal financi keynesian econom bank note high school money suppli polici analys free bank analys politiqu liquid effect politiqu vol journal financ birth weight median voter labor forc law econom journal post vote share latin america war spend mental ill journal law medic care money demand labour market gold reserv social capit anna j singl mother switch cost Table 1: Top 20 bigrams and trigrams. Dirichilet Allocation (Blei et al., 2003) on our corpus. We use 30, 50, and 100 topics to create LDA30, LDA50, and LDA100 topic mappings. We use the topic distributions estimated by LDA to assign articles to topics. A paper p is assigned to a topic t if the probability that t appears in p is greater than 5%. While 5% might seem to be a lower threshold, the topic distributions estimated by LDA tend to be sparse. For example, even with 50 topics to ‘choose’ from in LDA50 and a threshold of 5%, 99.5% of the papers would be assigned to five or fewer topics. This compares favorably with JEL2 codings where 98.8% of papers have five or fewe</context>
</contexts>
<marker>Blei, Ng, Jordan, 2003</marker>
<rawString>David M. Blei, Andrew Y. Ng, and Michael I. Jordan. 2003. Latent dirichlet allocation. the Journal of machine Learning research, 3:993–1022.</rawString>
</citation>
<citation valid="true">
<authors>
<author>Raj Chetty</author>
</authors>
<title>Yes, economics is a science.</title>
<date>2013</date>
<publisher>The</publisher>
<location>New York Times,</location>
<contexts>
<context position="2063" citStr="Chetty, 2013" startWordPosition="304" endWordPosition="305">ology can be detected in arenas where it is strongly discouraged. A further consideration for applied researchers is whether these tools can offer insight into questions of import for policymakers. To address both of these issues, we examine one such domain that is both policy-relevant and where ideology is not overtly expressed: research papers written by academic economists. Why economics? Economic ideas are important for shaping policy by influencing the public debate and setting the range of expert opinion on various policy options (Rodrik, 2014). Economics also views itself as a science (Chetty, 2013) carefully applying rigorous methodologies and using institutionalized safe-guards such as peer review. The field’s most prominent research organization explicitly prohibits researchers from making policy recommendations in papers that it releases (National Bureau of Economic Research, 2010). Despite these measures, economics’ close proximity to public policy decisions have led many to see it as being driven by ideology (A.S., 2010). Does this view of partisan economics have any empirical basis? To answer the question of whether economics is politicized or neutral, we present a supervised ense</context>
</contexts>
<marker>Chetty, 2013</marker>
<rawString>Raj Chetty. 2013. Yes, economics is a science. The New York Times, October.</rawString>
</citation>
<citation valid="true">
<authors>
<author>Jacob Eisenstein</author>
<author>Amr Ahmed</author>
<author>Eric P Xing</author>
</authors>
<title>Sparse additive generative models of text.</title>
<date>2011</date>
<booktitle>In Proceedings of the 28th International Conference on Machine Learning (ICML-11),</booktitle>
<pages>1041--1048</pages>
<contexts>
<context position="3399" citStr="Eisenstein et al., 2011" startWordPosition="510" endWordPosition="513">y (2013) and Gentzkow and Shapiro (2010), the latter of which used χ2 tests to find phrases most associated with ideology as proxied by the language of U.S. Congresspersons. We improve on this methodology by accounting for ideological word choice within topics and incorporating an ensemble approach that increases predictive accuracy. We also motivate the need to adjust for topics even if doing so does not improve accuracy (although it does in this case). We further provide evidence that fully unsupervised methods (Mei et al., 2007; Lin et al., 2008; Ahmed and Xing, 2010; Paul and Girju, 2010; Eisenstein et al., 2011; Wang et al., 2012) may encounter difficulties learning latent ideological aspects when those aspects are not first order in the data. Our algorithm is able to correctly predict the ideology of 69.2% of economists in our data purely from their academic output. We also show that our predictions generalize and are predictors of responses by a panel of top economists on issues of economic importance. In a companion paper (Jelveh et al., 2014), we further show that 1Grimmer and Stewart (2013) provide an overview of models used for ideology detection. 1804 Proceedings of the 2014 Conference on Emp</context>
</contexts>
<marker>Eisenstein, Ahmed, Xing, 2011</marker>
<rawString>Jacob Eisenstein, Amr Ahmed, and Eric P. Xing. 2011. Sparse additive generative models of text. In Proceedings of the 28th International Conference on Machine Learning (ICML-11), pages 1041–1048.</rawString>
</citation>
<citation valid="true">
<authors>
<author>T Fawcett</author>
</authors>
<title>An introduction to ROC analysis. Pattern recognition letters,</title>
<date>2006</date>
<pages>27--8</pages>
<contexts>
<context position="13964" citStr="Fawcett, 2006" startWordPosition="2254" endWordPosition="2255">ing ideology and values closer to one are associated with a rightward lean. To recap, we start with a topic mapping and then for each topic run an ensemble algorithm with PLS at its core.3 The output for each topic is a set of votes. We sum across topics to compute a final prediction for ideology. 4 Validation and Results We split our ground-truth set of 2,171 authors into training (80%) and test sets (20%) and compute predictions as above. As our data exhibits skew with 1.5 left-leaning for every right-leaning economist, we report the area under the curve (AUC) which is robust to class skew (Fawcett, 2006). It’s worth noting that a classifier that randomly predicts a liberal economist 60% of the time would have an AUC of 0.5. To compare our model with fully unsupervised methods, we also include results from running the Topic-Aspect Model (TAM) (Paul and Girju, 2010) on our data. TAM decomposes documents into two components: one affecting topics and one affecting a latent aspect that influences all topics in a similar manner. We run TAM with 30 topics and 2 aspects (TAM2/30). We follow Paul and Girju and use the learned topic and aspect distributions as training data for a SVM.4 Columns 2 to 4 f</context>
</contexts>
<marker>Fawcett, 2006</marker>
<rawString>T. Fawcett. 2006. An introduction to ROC analysis. Pattern recognition letters, 27(8):861–874.</rawString>
</citation>
<citation valid="true">
<authors>
<author>Matthew Gentzkow</author>
<author>Jesse M Shapiro</author>
</authors>
<title>What drives media slant? evidence from U.S. daily newspapers.</title>
<date>2010</date>
<journal>Econometrica,</journal>
<volume>78</volume>
<issue>1</issue>
<contexts>
<context position="2816" citStr="Gentzkow and Shapiro (2010)" startWordPosition="414" endWordPosition="417">nent research organization explicitly prohibits researchers from making policy recommendations in papers that it releases (National Bureau of Economic Research, 2010). Despite these measures, economics’ close proximity to public policy decisions have led many to see it as being driven by ideology (A.S., 2010). Does this view of partisan economics have any empirical basis? To answer the question of whether economics is politicized or neutral, we present a supervised ensemble n-gram model of ideology extraction with topic adjustments.1 Our methodology is most closely related to Taddy (2013) and Gentzkow and Shapiro (2010), the latter of which used χ2 tests to find phrases most associated with ideology as proxied by the language of U.S. Congresspersons. We improve on this methodology by accounting for ideological word choice within topics and incorporating an ensemble approach that increases predictive accuracy. We also motivate the need to adjust for topics even if doing so does not improve accuracy (although it does in this case). We further provide evidence that fully unsupervised methods (Mei et al., 2007; Lin et al., 2008; Ahmed and Xing, 2010; Paul and Girju, 2010; Eisenstein et al., 2011; Wang et al., 20</context>
</contexts>
<marker>Gentzkow, Shapiro, 2010</marker>
<rawString>Matthew Gentzkow and Jesse M. Shapiro. 2010. What drives media slant? evidence from U.S. daily newspapers. Econometrica, 78(1):35–71.</rawString>
</citation>
<citation valid="true">
<authors>
<author>Roger Gordon</author>
<author>Gordon B Dahl</author>
</authors>
<title>Views among economists: Professional consensus or pointcounterpoint?</title>
<date>2013</date>
<journal>American Economic Review,</journal>
<volume>103</volume>
<issue>3</issue>
<contexts>
<context position="15070" citStr="Gordon and Dahl (2013)" startWordPosition="2443" endWordPosition="2446">llow Paul and Girju and use the learned topic and aspect distributions as training data for a SVM.4 Columns 2 to 4 from Table 2 show that our models’ predictions have a clear association with ground-truth ideology. The LDA topic mappings outperform the supervised mappings as well as a model that does not adjust for topics (NoTopic). Perhaps not surprisingly, TAM does not perform well in our domain. A drawback of unsupervised methods is that the learned aspects may not be related to ideology but some other hidden factor. For further insight into how well our model generalizes, we use data from Gordon and Dahl (2013) to compare our predictions to potentially ideological responses of economists on a survey 3Other predictions algorithms could be dropped in for PLS. Logistic regression and SVM produced similar results. 4Authors are treated as documents. TAM is run for 1,000 iterations with the following priors: α = 1.0, β = 1.0, γ0 = 1, γ1 = 1, S0 = 20, S1 = 80. (1) (2) (3) (4) Topic Accu- Corr. w/ AUC Map racy(%) Truth LDA50 69.2 0.381 0.719 LDA100 66.3 0.364 0.707 LDA30 65.0 0.313 0.674 NoTopic 63.9 0.290 0.672 JEL1 61.0 0.263 0.647 JEL2 61.8 0.240 0.646 TAM2/30 61.5 0.228 0.580 Table 2: Model comparisons </context>
</contexts>
<marker>Gordon, Dahl, 2013</marker>
<rawString>Roger Gordon and Gordon B Dahl. 2013. Views among economists: Professional consensus or pointcounterpoint? American Economic Review, 103(3):629–635, May.</rawString>
</citation>
<citation valid="true">
<authors>
<author>J Grimmer</author>
<author>B M Stewart</author>
</authors>
<title>Text as data: The promise and pitfalls of automatic content analysis methods for political texts. Political Analysis,</title>
<date>2013</date>
<volume>21</volume>
<issue>3</issue>
<contexts>
<context position="3893" citStr="Grimmer and Stewart (2013)" startWordPosition="593" endWordPosition="596"> fully unsupervised methods (Mei et al., 2007; Lin et al., 2008; Ahmed and Xing, 2010; Paul and Girju, 2010; Eisenstein et al., 2011; Wang et al., 2012) may encounter difficulties learning latent ideological aspects when those aspects are not first order in the data. Our algorithm is able to correctly predict the ideology of 69.2% of economists in our data purely from their academic output. We also show that our predictions generalize and are predictors of responses by a panel of top economists on issues of economic importance. In a companion paper (Jelveh et al., 2014), we further show that 1Grimmer and Stewart (2013) provide an overview of models used for ideology detection. 1804 Proceedings of the 2014 Conference on Empirical Methods in Natural Language Processing (EMNLP), pages 1804–1809, October 25-29, 2014, Doha, Qatar. c�2014 Association for Computational Linguistics predicted ideologies are significantly correlated to economists’ research findings. The latter result shows the relevance and applicability of these tools beyond the task of ideology extraction. 2 Data Linking Economists to Their Political Activity: We obtain the member directory of the American Economics Association (AEA) and link it to</context>
</contexts>
<marker>Grimmer, Stewart, 2013</marker>
<rawString>J. Grimmer and B. M. Stewart. 2013. Text as data: The promise and pitfalls of automatic content analysis methods for political texts. Political Analysis, 21(3):267–297, January.</rawString>
</citation>
<citation valid="true">
<authors>
<author>David Hedengren</author>
<author>Daniel B Klein</author>
<author>Carrie Milton</author>
</authors>
<title>Economist petitions: Ideology revealed.</title>
<date>2010</date>
<journal>Econ Journal Watch,</journal>
<volume>7</volume>
<issue>3</issue>
<contexts>
<context position="4724" citStr="Hedengren et al. (2010)" startWordPosition="712" endWordPosition="715">ar. c�2014 Association for Computational Linguistics predicted ideologies are significantly correlated to economists’ research findings. The latter result shows the relevance and applicability of these tools beyond the task of ideology extraction. 2 Data Linking Economists to Their Political Activity: We obtain the member directory of the American Economics Association (AEA) and link it to two datasets: economists’ political campaign contributions and petition signing activities. We obtain campaign contribution data from the Federal Election Commission’s website and petition signing data from Hedengren et al. (2010). From this data, we construct a binary variable to indicate the ground-truth ideologies of economists. See our companion paper (Jelveh et al., 2014) for further details on the construction of this dataset. Revealed ideology through contributions and petitions is largely consistent. Of 441 economists appearing in both datasets, 83.4% showed agreement between contributions and petitions. For the final dataset of ground-truth authors we include all economists with campaign contributions and/or petition signatures, however, we drop those economists whose ideologies where different across the cont</context>
</contexts>
<marker>Hedengren, Klein, Milton, 2010</marker>
<rawString>David Hedengren, Daniel B. Klein, and Carrie Milton. 2010. Economist petitions: Ideology revealed. Econ Journal Watch, 7(3):288–319.</rawString>
</citation>
<citation valid="true">
<authors>
<author>Zubin Jelveh</author>
<author>Bruce Kogut</author>
<author>Suresh Naidu</author>
</authors>
<date>2014</date>
<note>Political language in economics.</note>
<contexts>
<context position="3843" citStr="Jelveh et al., 2014" startWordPosition="585" endWordPosition="588">this case). We further provide evidence that fully unsupervised methods (Mei et al., 2007; Lin et al., 2008; Ahmed and Xing, 2010; Paul and Girju, 2010; Eisenstein et al., 2011; Wang et al., 2012) may encounter difficulties learning latent ideological aspects when those aspects are not first order in the data. Our algorithm is able to correctly predict the ideology of 69.2% of economists in our data purely from their academic output. We also show that our predictions generalize and are predictors of responses by a panel of top economists on issues of economic importance. In a companion paper (Jelveh et al., 2014), we further show that 1Grimmer and Stewart (2013) provide an overview of models used for ideology detection. 1804 Proceedings of the 2014 Conference on Empirical Methods in Natural Language Processing (EMNLP), pages 1804–1809, October 25-29, 2014, Doha, Qatar. c�2014 Association for Computational Linguistics predicted ideologies are significantly correlated to economists’ research findings. The latter result shows the relevance and applicability of these tools beyond the task of ideology extraction. 2 Data Linking Economists to Their Political Activity: We obtain the member directory of the A</context>
<context position="18685" citStr="Jelveh et al., 2014" startWordPosition="3026" endWordPosition="3029">ith a conservative response than for the most liberal economists to strongly agree with a liberal response. It is worthwhile to note from the small increase in log-likelihood in Table 3 when controls are added, suggesting that our ideology scores are much better predictors of IGM responses than demographic and professional controls. 5 Conclusions and Future Work We’ve presented a supervised methodology for extracting political sentiment in a domain where it’s discouraged and shown how it even predicts the partisanship calculated from completely unrelated IGM survey data. In a companion paper (Jelveh et al., 2014) we further demonstrate how this tool can be used to aid policymakers in de-biasing research findings. When compared to domains where ideological language is expected, our predictive ability is reduced. Future work should disentangle how much this difference is due to modeling decisions and limitations versus actual absence of ideology. Future works should also investigate how fully unsupervised methods can be extended to match our performance. 60 50 40 30 20 10 0 60 50 40 30 20 10 0 60 50 40 30 20 10 0 Str. Dis. Dis. Uncert. Agr. Str. Agr. conservative liberal Figure 1: The predicted probabil</context>
</contexts>
<marker>Jelveh, Kogut, Naidu, 2014</marker>
<rawString>Zubin Jelveh, Bruce Kogut, and Suresh Naidu. 2014. Political language in economics.</rawString>
</citation>
<citation valid="true">
<authors>
<author>Wei-Hao Lin</author>
<author>Eric Xing</author>
<author>Alexander Hauptmann</author>
</authors>
<title>A joint topic and perspective model for ideological discourse.</title>
<date>2008</date>
<booktitle>In Machine Learning and Knowledge Discovery in Databases,</booktitle>
<pages>17--32</pages>
<publisher>Springer.</publisher>
<contexts>
<context position="3330" citStr="Lin et al., 2008" startWordPosition="498" endWordPosition="501"> adjustments.1 Our methodology is most closely related to Taddy (2013) and Gentzkow and Shapiro (2010), the latter of which used χ2 tests to find phrases most associated with ideology as proxied by the language of U.S. Congresspersons. We improve on this methodology by accounting for ideological word choice within topics and incorporating an ensemble approach that increases predictive accuracy. We also motivate the need to adjust for topics even if doing so does not improve accuracy (although it does in this case). We further provide evidence that fully unsupervised methods (Mei et al., 2007; Lin et al., 2008; Ahmed and Xing, 2010; Paul and Girju, 2010; Eisenstein et al., 2011; Wang et al., 2012) may encounter difficulties learning latent ideological aspects when those aspects are not first order in the data. Our algorithm is able to correctly predict the ideology of 69.2% of economists in our data purely from their academic output. We also show that our predictions generalize and are predictors of responses by a panel of top economists on issues of economic importance. In a companion paper (Jelveh et al., 2014), we further show that 1Grimmer and Stewart (2013) provide an overview of models used f</context>
</contexts>
<marker>Lin, Xing, Hauptmann, 2008</marker>
<rawString>Wei-Hao Lin, Eric Xing, and Alexander Hauptmann. 2008. A joint topic and perspective model for ideological discourse. In Machine Learning and Knowledge Discovery in Databases, pages 17–32. Springer.</rawString>
</citation>
<citation valid="true">
<authors>
<author>Drew Margolin</author>
<author>Yu-Ru Lin</author>
<author>David Lazer</author>
</authors>
<title>Why so similar?: Identifying semantic organizing processes in large textual corpora.</title>
<date>2013</date>
<contexts>
<context position="6075" citStr="Margolin et al. (2013)" startWordPosition="927" endWordPosition="930">hile 40% lean rightwards. Economic Papers Corpus: To create our corpus of academic writings by economists, we collect 17,503 working papers from NBER’s website covering June 1973 to October 2011. We also obtained from JSTOR the fulltext of 62,888 research articles published in 93 journals in economics for the years 1991 to 2008. Combining the set of economists and papers leaves us with 2,171 authors with ground truth ideology and 17,870 papers they wrote. From the text of these papers we create n-grams of length two through eight. While n-grams greater than three words in length are uncommon, Margolin et al. (2013) demonstrate that ideological word choice can be detected by longer phrases. To capture other expressions of ideology not revealed in adjacent terms, we also include skipgrams of length two by combining nonadjacent terms that are three to five words apart. We remove phrases used by fewer than five authors. Topic Adjustments: Table 1 presents the top 20 most conservative and liberal bigrams ranked by χ2 scores from a Pearson’s test of independence between phrase usage by left- and rightleaning economists. It appears that top ideological phrases are related to specific research subfields. For ex</context>
</contexts>
<marker>Margolin, Lin, Lazer, 2013</marker>
<rawString>Drew Margolin, Yu-Ru Lin, and David Lazer. 2013. Why so similar?: Identifying semantic organizing processes in large textual corpora.</rawString>
</citation>
<citation valid="true">
<authors>
<author>Peter McCullagh</author>
</authors>
<title>Regression models for ordinal data.</title>
<date>1980</date>
<journal>Journal of the royal statistical society. Series B (Methodological),</journal>
<pages>109--142</pages>
<contexts>
<context position="16706" citStr="McCullagh, 1980" startWordPosition="2717" endWordPosition="2718">ted ideology. Column (2) adds question dummies. Column (3) adds demographic and professional variables. conducted by the University of Chicago.5 Each survey question asks for an economists opinion on an issue of political relevance such as minimum wages or tax rates. For further details on the data see Gordon and Dahl. Of importance here is that Gordon and Dahl categorize 22 questions where agreement (disagreement) with the statement implies belief a conservative (liberal) viewpoint. To see if our predicted ideologies are correlated with survey responses, we run an orderedlogistic regression (McCullagh, 1980). Survey responses are coded with the following order: Strongly Disagree, Disagree, Uncertain, Agree, Strongly Agree. We regress survey responses onto predicted ideologies. We also include questionlevel dummies and explanatory variables for a re5http://igmchicago.org 1807 spondent’s gender, year of Ph.D., Ph.D. university, NBER membership, and experience in federal government. Table 3 shows the results of these regressions for three topic mappings. The correlation between our predictions and survey respondents are all strongly significant. One way to interpret these results is to compare the c</context>
</contexts>
<marker>McCullagh, 1980</marker>
<rawString>Peter McCullagh. 1980. Regression models for ordinal data. Journal of the royal statistical society. Series B (Methodological), pages 109–142.</rawString>
</citation>
<citation valid="true">
<authors>
<author>Qiaozhu Mei</author>
<author>Xu Ling</author>
<author>Matthew Wondra</author>
<author>Hang Su</author>
<author>ChengXiang Zhai</author>
</authors>
<title>Topic sentiment mixture: modeling facets and opinions in weblogs.</title>
<date>2007</date>
<booktitle>In Proceedings of the 16th international conference on World Wide Web,</booktitle>
<pages>171--180</pages>
<publisher>ACM.</publisher>
<contexts>
<context position="3312" citStr="Mei et al., 2007" startWordPosition="494" endWordPosition="497">raction with topic adjustments.1 Our methodology is most closely related to Taddy (2013) and Gentzkow and Shapiro (2010), the latter of which used χ2 tests to find phrases most associated with ideology as proxied by the language of U.S. Congresspersons. We improve on this methodology by accounting for ideological word choice within topics and incorporating an ensemble approach that increases predictive accuracy. We also motivate the need to adjust for topics even if doing so does not improve accuracy (although it does in this case). We further provide evidence that fully unsupervised methods (Mei et al., 2007; Lin et al., 2008; Ahmed and Xing, 2010; Paul and Girju, 2010; Eisenstein et al., 2011; Wang et al., 2012) may encounter difficulties learning latent ideological aspects when those aspects are not first order in the data. Our algorithm is able to correctly predict the ideology of 69.2% of economists in our data purely from their academic output. We also show that our predictions generalize and are predictors of responses by a panel of top economists on issues of economic importance. In a companion paper (Jelveh et al., 2014), we further show that 1Grimmer and Stewart (2013) provide an overvie</context>
</contexts>
<marker>Mei, Ling, Wondra, Su, Zhai, 2007</marker>
<rawString>Qiaozhu Mei, Xu Ling, Matthew Wondra, Hang Su, and ChengXiang Zhai. 2007. Topic sentiment mixture: modeling facets and opinions in weblogs. In Proceedings of the 16th international conference on World Wide Web, pages 171–180. ACM.</rawString>
</citation>
<citation valid="true">
<title>of Economic Research.</title>
<date>2010</date>
<pages>inc.</pages>
<institution>National Bureau</institution>
<contexts>
<context position="2816" citStr="(2010)" startWordPosition="417" endWordPosition="417">ation explicitly prohibits researchers from making policy recommendations in papers that it releases (National Bureau of Economic Research, 2010). Despite these measures, economics’ close proximity to public policy decisions have led many to see it as being driven by ideology (A.S., 2010). Does this view of partisan economics have any empirical basis? To answer the question of whether economics is politicized or neutral, we present a supervised ensemble n-gram model of ideology extraction with topic adjustments.1 Our methodology is most closely related to Taddy (2013) and Gentzkow and Shapiro (2010), the latter of which used χ2 tests to find phrases most associated with ideology as proxied by the language of U.S. Congresspersons. We improve on this methodology by accounting for ideological word choice within topics and incorporating an ensemble approach that increases predictive accuracy. We also motivate the need to adjust for topics even if doing so does not improve accuracy (although it does in this case). We further provide evidence that fully unsupervised methods (Mei et al., 2007; Lin et al., 2008; Ahmed and Xing, 2010; Paul and Girju, 2010; Eisenstein et al., 2011; Wang et al., 20</context>
<context position="4724" citStr="(2010)" startWordPosition="715" endWordPosition="715">ation for Computational Linguistics predicted ideologies are significantly correlated to economists’ research findings. The latter result shows the relevance and applicability of these tools beyond the task of ideology extraction. 2 Data Linking Economists to Their Political Activity: We obtain the member directory of the American Economics Association (AEA) and link it to two datasets: economists’ political campaign contributions and petition signing activities. We obtain campaign contribution data from the Federal Election Commission’s website and petition signing data from Hedengren et al. (2010). From this data, we construct a binary variable to indicate the ground-truth ideologies of economists. See our companion paper (Jelveh et al., 2014) for further details on the construction of this dataset. Revealed ideology through contributions and petitions is largely consistent. Of 441 economists appearing in both datasets, 83.4% showed agreement between contributions and petitions. For the final dataset of ground-truth authors we include all economists with campaign contributions and/or petition signatures, however, we drop those economists whose ideologies where different across the cont</context>
</contexts>
<marker>2010</marker>
<rawString>National Bureau of Economic Research. 2010. Amended and restated by-laws of national bureau of economic research, inc.</rawString>
</citation>
<citation valid="true">
<authors>
<author>Michael Paul</author>
<author>Roxana Girju</author>
</authors>
<title>A twodimensional topic-aspect model for discovering multi-faceted topics.</title>
<date>2010</date>
<location>Urbana,</location>
<contexts>
<context position="3374" citStr="Paul and Girju, 2010" startWordPosition="506" endWordPosition="509">losely related to Taddy (2013) and Gentzkow and Shapiro (2010), the latter of which used χ2 tests to find phrases most associated with ideology as proxied by the language of U.S. Congresspersons. We improve on this methodology by accounting for ideological word choice within topics and incorporating an ensemble approach that increases predictive accuracy. We also motivate the need to adjust for topics even if doing so does not improve accuracy (although it does in this case). We further provide evidence that fully unsupervised methods (Mei et al., 2007; Lin et al., 2008; Ahmed and Xing, 2010; Paul and Girju, 2010; Eisenstein et al., 2011; Wang et al., 2012) may encounter difficulties learning latent ideological aspects when those aspects are not first order in the data. Our algorithm is able to correctly predict the ideology of 69.2% of economists in our data purely from their academic output. We also show that our predictions generalize and are predictors of responses by a panel of top economists on issues of economic importance. In a companion paper (Jelveh et al., 2014), we further show that 1Grimmer and Stewart (2013) provide an overview of models used for ideology detection. 1804 Proceedings of t</context>
<context position="14229" citStr="Paul and Girju, 2010" startWordPosition="2297" endWordPosition="2300">compute a final prediction for ideology. 4 Validation and Results We split our ground-truth set of 2,171 authors into training (80%) and test sets (20%) and compute predictions as above. As our data exhibits skew with 1.5 left-leaning for every right-leaning economist, we report the area under the curve (AUC) which is robust to class skew (Fawcett, 2006). It’s worth noting that a classifier that randomly predicts a liberal economist 60% of the time would have an AUC of 0.5. To compare our model with fully unsupervised methods, we also include results from running the Topic-Aspect Model (TAM) (Paul and Girju, 2010) on our data. TAM decomposes documents into two components: one affecting topics and one affecting a latent aspect that influences all topics in a similar manner. We run TAM with 30 topics and 2 aspects (TAM2/30). We follow Paul and Girju and use the learned topic and aspect distributions as training data for a SVM.4 Columns 2 to 4 from Table 2 show that our models’ predictions have a clear association with ground-truth ideology. The LDA topic mappings outperform the supervised mappings as well as a model that does not adjust for topics (NoTopic). Perhaps not surprisingly, TAM does not perform</context>
</contexts>
<marker>Paul, Girju, 2010</marker>
<rawString>Michael Paul and Roxana Girju. 2010. A twodimensional topic-aspect model for discovering multi-faceted topics. Urbana, 51.</rawString>
</citation>
<citation valid="true">
<authors>
<author>Dani Rodrik</author>
</authors>
<title>When ideas trump interests: Preferences, worldviews, and policy innovations .</title>
<date>2014</date>
<journal>Journal of Economic Perspectives,</journal>
<volume>28</volume>
<issue>1</issue>
<contexts>
<context position="2006" citStr="Rodrik, 2014" startWordPosition="295" endWordPosition="296"> and Palestinian authors. An open question is whether ideology can be detected in arenas where it is strongly discouraged. A further consideration for applied researchers is whether these tools can offer insight into questions of import for policymakers. To address both of these issues, we examine one such domain that is both policy-relevant and where ideology is not overtly expressed: research papers written by academic economists. Why economics? Economic ideas are important for shaping policy by influencing the public debate and setting the range of expert opinion on various policy options (Rodrik, 2014). Economics also views itself as a science (Chetty, 2013) carefully applying rigorous methodologies and using institutionalized safe-guards such as peer review. The field’s most prominent research organization explicitly prohibits researchers from making policy recommendations in papers that it releases (National Bureau of Economic Research, 2010). Despite these measures, economics’ close proximity to public policy decisions have led many to see it as being driven by ideology (A.S., 2010). Does this view of partisan economics have any empirical basis? To answer the question of whether economic</context>
</contexts>
<marker>Rodrik, 2014</marker>
<rawString>Dani Rodrik. 2014. When ideas trump interests: Preferences, worldviews, and policy innovations . Journal of Economic Perspectives, 28(1):189–208, February.</rawString>
</citation>
<citation valid="true">
<authors>
<author>Matt Taddy</author>
</authors>
<title>Multinomial inverse regression for text analysis.</title>
<date>2013</date>
<journal>Journal of the American StatisticalAssociation,</journal>
<volume>108</volume>
<contexts>
<context position="2784" citStr="Taddy (2013)" startWordPosition="411" endWordPosition="412">ield’s most prominent research organization explicitly prohibits researchers from making policy recommendations in papers that it releases (National Bureau of Economic Research, 2010). Despite these measures, economics’ close proximity to public policy decisions have led many to see it as being driven by ideology (A.S., 2010). Does this view of partisan economics have any empirical basis? To answer the question of whether economics is politicized or neutral, we present a supervised ensemble n-gram model of ideology extraction with topic adjustments.1 Our methodology is most closely related to Taddy (2013) and Gentzkow and Shapiro (2010), the latter of which used χ2 tests to find phrases most associated with ideology as proxied by the language of U.S. Congresspersons. We improve on this methodology by accounting for ideological word choice within topics and incorporating an ensemble approach that increases predictive accuracy. We also motivate the need to adjust for topics even if doing so does not improve accuracy (although it does in this case). We further provide evidence that fully unsupervised methods (Mei et al., 2007; Lin et al., 2008; Ahmed and Xing, 2010; Paul and Girju, 2010; Eisenste</context>
</contexts>
<marker>Taddy, 2013</marker>
<rawString>Matt Taddy. 2013. Multinomial inverse regression for text analysis. Journal of the American StatisticalAssociation, 108.</rawString>
</citation>
<citation valid="true">
<authors>
<author>William Yang Wang</author>
<author>Elijah Mayfield</author>
<author>Suresh Naidu</author>
<author>Jeremiah Dittmar</author>
</authors>
<title>Historical analysis of legal opinions with a sparse mixed-effects latent variable model.</title>
<date>2012</date>
<booktitle>In Proceedings of the 50th Annual Meeting of the Association for Computational Linguistics: Long Papers-Volume 1,</booktitle>
<pages>740--749</pages>
<publisher>Association</publisher>
<institution>for Computational Linguistics.</institution>
<contexts>
<context position="3419" citStr="Wang et al., 2012" startWordPosition="514" endWordPosition="517"> Shapiro (2010), the latter of which used χ2 tests to find phrases most associated with ideology as proxied by the language of U.S. Congresspersons. We improve on this methodology by accounting for ideological word choice within topics and incorporating an ensemble approach that increases predictive accuracy. We also motivate the need to adjust for topics even if doing so does not improve accuracy (although it does in this case). We further provide evidence that fully unsupervised methods (Mei et al., 2007; Lin et al., 2008; Ahmed and Xing, 2010; Paul and Girju, 2010; Eisenstein et al., 2011; Wang et al., 2012) may encounter difficulties learning latent ideological aspects when those aspects are not first order in the data. Our algorithm is able to correctly predict the ideology of 69.2% of economists in our data purely from their academic output. We also show that our predictions generalize and are predictors of responses by a panel of top economists on issues of economic importance. In a companion paper (Jelveh et al., 2014), we further show that 1Grimmer and Stewart (2013) provide an overview of models used for ideology detection. 1804 Proceedings of the 2014 Conference on Empirical Methods in Na</context>
</contexts>
<marker>Wang, Mayfield, Naidu, Dittmar, 2012</marker>
<rawString>William Yang Wang, Elijah Mayfield, Suresh Naidu, and Jeremiah Dittmar. 2012. Historical analysis of legal opinions with a sparse mixed-effects latent variable model. In Proceedings of the 50th Annual Meeting of the Association for Computational Linguistics: Long Papers-Volume 1, pages 740–749. Association for Computational Linguistics.</rawString>
</citation>
</citationList>
</algorithm>
</algorithms>
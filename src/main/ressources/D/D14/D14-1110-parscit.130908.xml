<?xml version="1.0" encoding="UTF-8"?>
<algorithms version="110505">
<algorithm name="SectLabel" version="110505">
<variant no="0" confidence="0.000020">
<title confidence="0.998822">
A Unified Model for Word Sense Representation and Disambiguation
</title>
<author confidence="0.998035">
Xinxiong Chen, Zhiyuan Liu, Maosong Sun
</author>
<affiliation confidence="0.981627">
State Key Laboratory of Intelligent Technology and Systems
Tsinghua National Laboratory for Information Science and Technology
Department of Computer Science and Technology
Tsinghua University, Beijing 100084, China
</affiliation>
<email confidence="0.986209">
cxx.thu@gmail.com, {lzy, sms}@tsinghua.edu.cn
</email>
<sectionHeader confidence="0.997329" genericHeader="abstract">
Abstract
</sectionHeader>
<bodyText confidence="0.999789038461539">
Most word representation methods assume
that each word owns a single semantic vec-
tor. This is usually problematic because
lexical ambiguity is ubiquitous, which is
also the problem to be resolved by word
sense disambiguation. In this paper, we
present a unified model for joint word
sense representation and disambiguation,
which will assign distinct representation-
s for each word sense.1 The basic idea is
that both word sense representation (WS-
R) and word sense disambiguation (WS-
D) will benefit from each other: (1) high-
quality WSR will capture rich informa-
tion about words and senses, which should
be helpful for WSD, and (2) high-quality
WSD will provide reliable disambiguat-
ed corpora for learning better sense rep-
resentations. Experimental results show
that, our model improves the performance
of contextual word similarity compared to
existing WSR methods, outperforms state-
of-the-art supervised methods on domain-
specific WSD, and achieves competitive
performance on coarse-grained all-words
WSD.
</bodyText>
<sectionHeader confidence="0.99947" genericHeader="keywords">
1 Introduction
</sectionHeader>
<bodyText confidence="0.99898875">
Word representation aims to build vectors for each
word based on its context in a large corpus, usually
capturing both semantic and syntactic information
of words. These representations can be used as
features or inputs, which are widely employed in
information retrieval (Manning et al., 2008), doc-
ument classification (Sebastiani, 2002) and other
NLP tasks.
</bodyText>
<footnote confidence="0.990924">
1Our sense representations can be downloaded at http:
//pan.baidu.com/s/1eQcPK8i.
</footnote>
<bodyText confidence="0.998575705882353">
Most word representation methods assume each
word owns a single vector. However, this is usual-
ly problematic due to the homonymy and polyse-
my of many words. To remedy the issue, Reisinger
and Mooney (2010) proposed a multi-prototype
vector space model, where the contexts of each
word are first clustered into groups, and then each
cluster generates a distinct prototype vector for a
word by averaging over all context vectors with-
in the cluster. Huang et al. (2012) followed this
idea, but introduced continuous distributed vectors
based on probabilistic neural language models for
word representations.
These cluster-based models conduct unsuper-
vised word sense induction by clustering word
contexts and, thus, suffer from the following is-
sues:
</bodyText>
<listItem confidence="0.896440538461539">
• It is usually difficult for these cluster-based
models to determine the number of cluster-
s. Huang et al. (2012) simply cluster word
contexts into static K clusters for each word,
which is arbitrary and may introduce mis-
takes.
• These cluster-based models are typically off-
line , so they cannot be efficiently adapted to
new senses, new words or new data.
• It is also troublesome to find the sense that
a word prototype corresponds to; thus, these
cluster-based models cannot be directly used
to perform word sense disambiguation.
</listItem>
<bodyText confidence="0.999934222222222">
In reality, many large knowledge bases have
been constructed with word senses available
online, such as WordNet (Miller, 1995) and
Wikipedia. Utilizing these knowledge bases to
learn word representation and sense representation
is a natural choice. In this paper, we present a uni-
fied model for both word sense representation and
disambiguation based on these knowledge bases
and large-scale text corpora. The unified model
</bodyText>
<page confidence="0.951072">
1025
</page>
<note confidence="0.898636">
Proceedings of the 2014 Conference on Empirical Methods in Natural Language Processing (EMNLP), pages 1025–1035,
October 25-29, 2014, Doha, Qatar. c�2014 Association for Computational Linguistics
</note>
<bodyText confidence="0.999498357142857">
can (1) perform word sense disambiguation based
on vector representations, and (2) learn continu-
ous distributed vector representation for word and
sense jointly.
The basic idea is that, the tasks of word sense
representation (WSR) and word sense disam-
biguation (WSD) can benefit from each other: (1)
high-quality WSR will capture rich semantic and
syntactic information of words and senses, which
should be helpful for WSD; (2) high-quality WS-
D will provide reliable disambiguated corpora for
learning better sense representations.
By utilizing these knowledge bases, the prob-
lem mentioned above can be overcome:
</bodyText>
<listItem confidence="0.981401714285714">
• The number of senses of a word can be de-
cided by the expert annotators or web users.
• When a new sense appears, our model can be
easily applied to obtain a new sense represen-
tation.
• Every sense vector has a corresponding sense
in these knowledge bases.
</listItem>
<bodyText confidence="0.999675909090909">
We conduct experiments to investigate the per-
formance of our model for both WSR and WS-
D. We evaluate the performance of WSR using a
contextual word similarity task, and results show
that out model can significantly improve the cor-
relation with human judgments compared to base-
lines. We further evaluate the performance on
both domain-specific WSD and coarse-grained all-
words WSD, and results show that our model
yields performance competitive with state-of-the-
art supervised approaches.
</bodyText>
<sectionHeader confidence="0.998809" genericHeader="introduction">
2 Methodology
</sectionHeader>
<bodyText confidence="0.891416">
We describe our method as a 3-stage process:
1. Initializing word vectors and sense vectors.
Given large amounts of text data, we first use
the Skip-gram model (Mikolov et al., 2013),
a neural network based language model, to
learn word vectors. Then, we assign vector
representations for senses based on their def-
initions (e.g, glosses in WordNet).
</bodyText>
<sectionHeader confidence="0.781932" genericHeader="method">
2. Performing word sense disambiguation.
</sectionHeader>
<bodyText confidence="0.987941">
Given word vectors and sense vectors, we
propose two simple and efficient WSD algo-
rithms to obtain more relevant occurrences
for each sense.
3. Learning sense vectors from relevant oc-
currences. Based on the relevant occur-
rences of ambiguous words, we modify the
training objective of Skip-gram to learn word
vectors and sense vectors jointly. Then, we
obtain the sense vectors directly from the
model.
Before illustrating the three stages of our
method in Sections 2.2, 2.3 and 2.4, we briefly
introduce our sense inventory, WordNet, in Sec-
tion 2.1. Note that, although our experiments will
use the WordNet sense inventory, our model is not
limited to this particular lexicon. Other knowledge
bases containing word sense distinctions and defi-
nitions can also serve as input to our model.
</bodyText>
<subsectionHeader confidence="0.929185">
2.1 WordNet
</subsectionHeader>
<bodyText confidence="0.9999914">
WordNet (Miller, 1995) is the most widely used
computational lexicon of English where a concep-
t is represented as a synonym set, or synset. The
words in the same synset share a common mean-
ing. Each synset has a textual definition, or gloss.
Table 1 shows the synsets and the corresponding
glosses of the two common senses of bank.
Before introducing the method in detail, we in-
troduce the notations. The unlabeled texts are de-
noted as R, and the vocabulary of the texts is de-
noted as W. For a word w in W, wsi is the ith
sense in WordNet WN. Each sense wsi has a gloss
gloss(wsi) in WN. The word embedding of w is
denoted as vec(w), and the sense embedding of its
ith sense wsi is denoted as vec(wsi).
</bodyText>
<subsectionHeader confidence="0.9924765">
2.2 Initializing Word Vectors and Sense
Vectors
</subsectionHeader>
<bodyText confidence="0.9998793">
Initializing word vectors. First, we use Skip-
gram to train the word vectors from large amounts
of text data. We choose Skip-gram for its sim-
plicity and effectiveness. The training objective of
Skip-gram is to train word vector representations
that are good at predicting its context in the same
sentence (Mikolov et al., 2013).
More formally, given a sequence of training
words w1, w2, w3,...,wT, the objective of Skip-
gram is to maximize the average log probability
</bodyText>
<equation confidence="0.758655">
�log p(wt+j|wt) (1)
</equation>
<bodyText confidence="0.996098">
where k is the size of the training window. The
inner summation spans from −k to k to compute
</bodyText>
<equation confidence="0.889639">
∑
−k&lt;j&lt;k,jO0
1
T
T
∑
t=1
</equation>
<page confidence="0.940648">
1026
</page>
<bodyText confidence="0.928869625">
Sense Synset Gloss
banks1 bank (sloping land (especially the slope beside a body of water))
“they pulled the canoe up on the bank”;
“he sat on the bank of the river and watched the currents”
banks2 depository institution, (a financial institution that accepts deposits and channels the
bank, money into lending activities)
banking concern, “he cashed a check at the bank”;
banking company “that bank holds the mortgage on my home”
</bodyText>
<tableCaption confidence="0.996503">
Table 1: Example of a synset in WordNet.
</tableCaption>
<bodyText confidence="0.9893874">
the log probability of correctly predicting the word
wt+j given the word in the middle wt. The outer
summation covers all words in the training data.
The prediction task is performed via softmax, a
multiclass classifier. There, we have
</bodyText>
<equation confidence="0.998533">
exp(vec&apos;(wt+j)Tvec(wt))
p(wt+j|wt) = (2)
∑W w=1exp(vec&apos;(w)�vec(wt))
</equation>
<bodyText confidence="0.997206333333333">
where vec(w) and vec&apos;(w) are the “input” and
“output” vector representations of w. This formu-
lation is impractical because the cost of comput-
ing p(wt+j|wt) is proportional to W, which is often
large( 105 −107 terms).
Initializing sense vectors. After learning the
word vectors using the Skip-gram model, we ini-
tialize the sense vectors based on the glosses of
senses. The basic idea of the sense vector initial-
ization is to represent the sense by using the sim-
ilar words in the gloss. From the content words
in the gloss, we select those words whose cosine
similarities with the original word are larger than
a similarity threshold δ. Formally, for each sense
wsi in WN, we first define a candidate set from
</bodyText>
<equation confidence="0.933541333333333">
gloss(wsi)
cand(wsi) = {u|u E gloss(wsi),u =� w,
POS(u) E CW,cos(vec(w),vec(u)) &gt; δ} (3)
</equation>
<bodyText confidence="0.998217416666667">
where POS(u) is the part-of-speech tagging of the
word u and CW is the set of all possible part-of-
speech tags that content words could have. In this
paper, CW contains the following tags: noun, verb,
adjective and adverb.
Then the average of the word vectors in
cand(wsi) is used as the initialization value of the
sense vector vec(wsi).
For example, in WordNet, the gloss of the sense
banks1 is “sloping land (especially the slope beside
a body of water)) they pulled the canoe up on the
bank; he sat on the bank of the river and watched
the currents”. The gloss contains a definition of
the sense and two examples of the sense. The
content words and the cosine similarities with the
word “bank” are listed as follows: (sloping, 0.12),
(land, 0.21), (slope, 0.17), (body, 0.01), (water,
0.10), (pulled, 0.01), (canoe, 0.09), (sat, 0.06),
(river, 0.43), (watch, -0.11), (currents, 0.01). If
the threshold, δ, is set to 0.05, then cand(banks1)
is {sloping, land, slope, water, canoe, sat, riv-
er}. Then the average of the word vectors in
cand(banksi) is used as the initialization value of
vec(banksi).
</bodyText>
<subsectionHeader confidence="0.999674">
2.3 Performing Word Sense Disambiguation.
</subsectionHeader>
<bodyText confidence="0.997581772727273">
One of the state-of-the-art WSD results can be
obtained using exemplar models, i.e., the word
meaning is modeled by using relevant occurrences
only, rather than merging all of the occurrences in-
to a single word vector (Erk and Pado, 2010). In-
spired by this idea, we perform word sense disam-
biguation to obtain more relevant occurrences.
Here, we perform knowledge-based word sense
disambiguation for training data on an all-words
setting, i.e., we will disambiguate all of the con-
tent words in a sentence. Formally, the sentence S
is a sequence of words (w1,w2,...,wn), and we will
identify a mapping M from words to senses such
that M(i) E SensesWN(wi), where SensesWN(wi) is
the set of senses encoded in the WN for word wi.
For sentence S, there are ∏ni=1 |SenseWN(wi) |pos-
sible mapping answers, which are impractical to
compute. Thus, we design two simple algorithms,
L2R (left to right) algorithm and S2C (simple to
complex) algorithm, for word sense disambigua-
tion based on the sense vectors.
The main difference between L2R and S2C is
</bodyText>
<equation confidence="0.9573176">
1
vec(wsi) =
vec(u) (4)
|cand(wsi) |∑
uEcand(wsi)
</equation>
<page confidence="0.934192">
1027
</page>
<bodyText confidence="0.999927928571429">
the order of words when performing word sense
disambiguation. When given a sentence, the L2R
algorithm disambiguates the words from left to
right (the natural order of a sentence), whereas the
S2C algorithm disambiguates the words with few-
er senses first. The main idea of S2C algorithm
is that the words with fewer senses are easier to
disambiguate, and the disambiguation result can
be helpful to disambiguate the words with more
senses. Both of the algorithms have three steps:
Context vector initialization. Similar to the ini-
tialization of sense vectors, we use the average of
all of the content words’ vectors in a sentence as
the initialization vector of context.
</bodyText>
<equation confidence="0.9517466">
1
vec(context) = |cand(S) |uEcand(S)
vec(u) (5)
where cand(S) is the set of content words
cand(S) = {u|u E S,POS(u) E CW}.
</equation>
<bodyText confidence="0.99543315625">
Ranking words. For L2R, we do nothing in this
step. For S2C, we rank the words based on the
ascending order of |SensesWN(wi)|.
Word sense disambiguation. For both L2R and
S2C, we denote the order of words as L and per-
form word sense disambiguation according to L.
First, we skip a word if the word is not
a content word or the word is monosemous
(|SensesWN(wi) |= 1). Then, for each word in
L, we can compute the cosine similarities be-
tween the context vector and its sense vectors. We
choose the sense that yields the maximum cosine
similarity as its disambiguation result. If the s-
core margin between the maximum and the sec-
ond maximum is larger than the threshold e, we
are confident with the disambiguation result of wi
and then use the sense vector to replace the word
vector in the context vector. Thus, we obtain a
more accurate context vector for other words that
are still yet to be disambiguated.
For example, given a sentence “He sat on the
bank of the lake”, we first explain how S2C work-
s. In the sentence, there are three content word-
s, “sat”, “bank” and “lake”, to be disambiguated.
First, the sum of the three word vectors is used as
the initialization of the context vector. Then we
rank the words by |SensesWN(wi)|, in ascending
order, that is, lake (3 senses), bank (10 senses), sat
(10 senses). We first disambiguate the word “lake”
based on the similarities between its sense vectors
and context vector. If the score margin is larger
sat sit on the of the lake lake
</bodyText>
<figure confidence="0.8908055">
1 1
output
projection
input
</figure>
<figureCaption confidence="0.896805428571429">
Figure 1: The architecture of our model. The
training objective of Skip-gram is to train word
vector representations that are not only good at
predicting its context words but are also good at
predicting its context words’ senses. The center
word “bank” is used to predict not only its context
words but also the sense “sit1” and “lake1”.
</figureCaption>
<bodyText confidence="0.9993254">
than the threshold e, then we are confident with
this disambiguation result and replace the word
vector with the sense vector to update the contex-
t vector. It would be helpful to disambiguate the
next word, “bank”. We repeat this process until all
three words are disambiguated.
For L2R, the order of words to be disambiguat-
ed will be “sat”, “bank” and “lake”. In this time,
when disambiguating “bank” (10 senses), we still
don’t know the sense of “lake” (3 senses).
</bodyText>
<subsectionHeader confidence="0.898453">
2.4 Learning Sense Vectors from Relevant
Occurrences.
</subsectionHeader>
<bodyText confidence="0.975387823529412">
Based on the disambiguation result, we modify the
training objective of Skip-gram and train the sense
vectors directly from the large-scale corpus. Our
training objective is to train the vector representa-
tions that are not only good at predicting its con-
text words but are also good at predicting its con-
text words’ senses. The architecture of our model
is shown in Figure 1.
More formally, given the disambiguation result
M(w1), M(w2), M(w3),...,M(wT), the training ob-
jective is modified to
�log{p(wt+j|wt)p(M(wt+j)|wt)}
(6)
where k is the size of the training window. The
inner summation spans from −k to k to compute
the log probability of correctly predicting the word
wt+ j and the log probability of correctly predicting
</bodyText>
<figure confidence="0.894789444444445">
bank
1
T
T
∑
t=1
k
∑
j=−k
</figure>
<page confidence="0.972047">
1028
</page>
<bodyText confidence="0.9998581875">
the sense M(wt+j) given the word in the middle
wt. The outer summation covers all words in the
training data.
Because not all of the disambiguation results are
correct, we only disambiguate the words that we
are confident in. Similar to step 3 of our WSD
algorithm, we only disambiguate words under the
condition that the score margin between the max-
imum and the second maximum is larger than the
score margin threshold, ε.
We also use the softmax function to define
p(wt+jJwt) and p(M(wt+j)Jwt). Then, we use hi-
erarchical softmax (Morin and Bengio, 2005) to
greatly reduce the computational complexity and
learn the sense vectors directly from the relevant
occurrences.
</bodyText>
<sectionHeader confidence="0.999882" genericHeader="method">
3 Experiments
</sectionHeader>
<bodyText confidence="0.999845821428572">
In this section, we first present the nearest neigh-
bors of some words and their senses, showing that
our sense vectors can capture the semantics of
words. Then, we use three tasks to evaluate our u-
nified model: a contextual word similarity task to
evaluate our sense representations, and two stan-
dard WSD tasks to evaluate our knowledge-based
WSD algorithm based on the sense vectors. Ex-
perimental results show that our model not only
improves the correlation with human judgments
on the contextual word similarity task but also out-
performs state-of-the-art supervised WSD system-
s on domain-specific datasets and competes with
them in a coarse-grained all-words setting.
We choose Wikipedia as the corpus to train
the word vectors because of its wide coverage
of topics and words usages. We use an English
Wikipedia database dump from October 2013 2,
which includes roughly 3 million articles and 1
billion tokens. We use Wikipedia Extractor 3 to
preprocess the Wikipedia pages and only save the
content of the articles.
We use word2vec 4 to train Skip-gram. We use
the default parameters of word2vec and the dimen-
sion of the vector representations is 200.
We use WordNet 5 as our sense inventory. The
datasets for different tasks are tagged with differ-
ent versions of WordNet. The version of WordNet
</bodyText>
<footnote confidence="0.9751555">
2http://download.wikipedia.org.
3The tool is available from http://medialab.di.
unipi.it/wiki/Wikipedia_Extractor.
4The code is available from https://code.
google.com/p/word2vec/.
5http://wordnet.princeton.edu/.
</footnote>
<table confidence="0.999076461538461">
Word or sense Nearest neighbors
bank banks, IDBI, CitiBank
banks1 river, slope, Sooes
banks2 mortgage, lending, loans
star stars, stellar, trek
stars1 photosphere, radiation,
gamma-rays
stars2 someone, skilled, genuinely
plant plants, glavaticevo, herbaceous
plants1 factories, machinery,
manufacturing
plants2 locomotion, organism,
organisms
</table>
<tableCaption confidence="0.993237">
Table 2: Nearest neighbors of word vectors and
</tableCaption>
<bodyText confidence="0.974604705882353">
sense vectors learned by our model based on co-
sine similarity. The subscript of each sense label
corresponds to the index of the sense in Word-
Net. For example, banks2 is the second sense of
the word bank in WordNet.
is 1.7 for the domain-specific WSD task and 2.1
for the coarse-grained WSD task.
We use the S2C algorithm described in Section
2.3 to perform word sense disambiguation to ob-
tain more relevant occurrences for each sense. We
compare S2C and L2R on the coarse-grained WS-
D task in a all-words setting.
The experimental results of our model are ob-
tained by setting the similarity threshold as δ = 0
and the score margin threshold as ε = 0.1. The in-
fluence of parameters on our model can be found
in Section 3.5.
</bodyText>
<subsectionHeader confidence="0.962854">
3.1 Examples for Sense Vectors
</subsectionHeader>
<bodyText confidence="0.999914545454546">
Table 2 shows the nearest neighbors of word vec-
tors and sense vectors based on cosine similari-
ty. We see that our sense representations can i-
dentify different meanings of a word, allowing our
model to capture more semantic and syntactic re-
lationships between words and senses. Note that
each sense vector in our model corresponds to a
sense in WordNet; thus, our sense vectors can be
used to perform knowledge-based word sense dis-
ambiguation, whereas the vectors of cluster-based
models cannot.
</bodyText>
<subsectionHeader confidence="0.995092">
3.2 Contextual Word Similarity
</subsectionHeader>
<bodyText confidence="0.889608333333333">
Experimental setting. A standard dataset for e-
valuating a vector-space model is the WordSim-
353 dataset (Finkelstein et al., 2001), which con-
</bodyText>
<page confidence="0.930038">
1029
</page>
<table confidence="0.999934625">
Model ρ x 100
C&amp;W-S 57.0
Huang-S 58.6
Huang-M AvgSim 62.8
Huang-M AvgSimC 65.7
Our Model-S 64.2
Our Model-M AvgSim 66.2
Our Model-M AvgSimC 68.9
</table>
<tableCaption confidence="0.749265">
Table 3: Spearman’s ρ on the SCWS dataset. Our
Model-S uses one representation per word to com-
</tableCaption>
<bodyText confidence="0.999425684210526">
pute similarities, while Our Model-M uses one
representation per sense to compute similarities.
AvgSim calculates the similarity with each sense
contributing equally, while AvgSimC weighs the
sense according to the probability of the word
choosing that sense in context c.
sists of 353 pairs of nouns. However, each pair of
nouns in WordSim-353 is presented without con-
text. This is problematic because the meanings
of homonymous and polysemous words depend
highly on the words’ contexts. Thus we choose the
Stanford’s Contextual Word Similarities (SCWS)
dataset from (Huang et al., 2012) 6. The SCWS
dataset contains 2003 pairs of words and each pair
is associated with 10 human judgments on similar-
ity on a scale from 0 to 10. In the SCWS dataset,
each word in a pair has a sentential context.
In our experiments, the similarity between a
pair of words (w, w&apos;) is computed as follows:
</bodyText>
<equation confidence="0.991819333333333">
MN
i=1 j=1
p(i|w,c)p(j|w&apos;,c&apos;)d(vec(wsi),vec(w&apos;sj)) (7)
</equation>
<bodyText confidence="0.999925615384615">
where p(i|w,c) is the likelihood that word w
chooses its ith sense given context c. d(vec,vec&apos;)
is a function computing the similarity between two
vectors, and here we use cosine similarity.
Results and discussion. For evaluation, we
compute the Spearman correlation between a
model’s computed similarity scores and human
judgements. Table 3 shows our results com-
pared to previous methods, including (Collobert
and Weston, 2008)’s language model (C&amp;W), and
Huang’s model which utilize the global context
and multi-prototype to improve the word represen-
tations.
</bodyText>
<footnote confidence="0.911446">
6The dataset can be downloaded at http://ai.
stanford.edu/˜ehhuang/.
</footnote>
<bodyText confidence="0.764391">
From Table 3, we observe that:
</bodyText>
<listItem confidence="0.997804166666667">
• Our single-vector version outperforms
Huang’s single-vector version. This indi-
cates that, by training the word vectors and
sense vectors jointly, our model can better
capture the semantic relationships between
words and senses.
• With one representation per sense, our mod-
el can outperform the single-vector version
without using context (66.2 vs. 64.2).
• Our model obtains the best performance
(68.9) by using AvgSimC, which takes con-
text into account.
</listItem>
<subsectionHeader confidence="0.956262">
3.3 Domain-Specific WSD
</subsectionHeader>
<bodyText confidence="0.99990878125">
Experimental setting. We use Wikipedia as
training data because of its wide coverage for spe-
cific domains. To test our performance on do-
main word sense disambiguation, we evaluated
our system on the dataset published in (Koeling
et al., 2005). This dataset consists of examples
retrieved from the Sports and Finance sections of
the Reuters corpus. 41 words related to the Sports
and Finance domains were selected, with an aver-
age polysemy of 6.7 senses, ranging from 2 to 13
senses.
Approximately 100 examples for each word
were annotated with senses from WordNet v.1.7
by three reviewers, yielding an inter-tagger agree-
ment of 65%. (Koeling et al., 2005) did not clarify
how to select the “correct” sense for each word, so
we followed the work of (Agirre et al., 2009) and,
used the sense chosen by the majority of taggers
as the correct answer.
Baseline methods. As a baseline, we use the
most frequent WordNet sense (MFS), as well as
a random sense assignment. We also compare our
results with four systems 7: Static PageRank (A-
girre et al., 2009), the k nearest neighbor algorith-
m (k-NN), Degree (Navigli and Lapata, 2010) and
Personalized PageRank (Agirre et al., 2009).
Static PageRank applies traditional PageRank
over the semantic graph based on WordNet and
obtains a context-independent ranking of word
senses.
k-NN is a widely used classification method,
where neighbors are the k labeled examples most
</bodyText>
<footnote confidence="0.959673666666667">
7We compare only with those systems performing token-
based WSD, i.e., disambiguating each instance of a target
word separately.
</footnote>
<equation confidence="0.956803333333333">
M N
1
AvgSimC(w,w&apos;) = ∑ ∑
</equation>
<page confidence="0.966957">
1030
</page>
<table confidence="0.999794888888889">
Algorithm Sports Finance
Recall Recall
Random BL 19.5 19.6
MFS BL 19.6 37.1
k-NN 30.3 43.4
Static PR 20.1 39.6
Personalized PR 35.6 46.9
Degree 42.0 47.8
Our Model 57.3 60.6
</table>
<tableCaption confidence="0.9537865">
Table 4: Performance on the Sports and Finance
sections of the dataset from (Koeling et al., 2005).
</tableCaption>
<bodyText confidence="0.988504888888889">
similar to the test example. The k-NN system is
trained on SemCor (Miller et al., 1993), the largest
publicly available annotated corpus.
Degree and Personalized PageRank are state-
of-the-art systems that exploit WordNet to build
a semantic graph and exploit the structural proper-
ties of the graph in order to choose the appropriate
senses of words in context.
Results and discussion. Similar to other work
on this dataset, we use recall (the ratio of correct
sense labels to the total labels in the gold standard)
as our evaluation measure. Table 4 shows the re-
sults of different WSD systems on the dataset, and
the best results are shown in bold. The differences
between other results and the best result in each
column of the table are statistically significant at
p &lt; 0.05.
The results show that:
</bodyText>
<listItem confidence="0.997769">
• Our model outperforms k-NN on the t-
wo domains by a large margin, support-
ing the findings from (Agirre et al., 2009)
that knowledge-based systems perform bet-
ter than supervised systems when evaluated
across different domains.
• Our model also achieves better results than
</listItem>
<bodyText confidence="0.958515125">
the state-of-the-art system (+15.3% recall on
Sports and +12.8% recall on Finance against
Degree). The reason for this is that when
dealing with short sentences or context words
that are not in WordNet, our model can still
compute similarity based on the context vec-
tor and sense vectors, whereas Degree will
have difficulty building the semantic graph.
</bodyText>
<listItem confidence="0.971585333333333">
• Moreover, our model achieves the best per-
formance by only using the unlabeled text da-
ta and the definitions of senses, whereas other
</listItem>
<table confidence="0.999429916666667">
Algorithm Type Nouns only All words
F1 F1
Random BL U 63.5 62.7
MFS BL Semi 77.4 78.9
SUSSX-FR Semi 81.1 77.0
NUS-PT S 82.3 82.5
SSI Semi 84.1 83.2
Degree Semi 85.5 81.7
Our ModelL2R U 79.2 73.9
Our ModelS2C U 81.6 75.8
Our ModelL2R Semi 82.5 79.6
Our ModelS2C Semi 85.3 82.6
</table>
<tableCaption confidence="0.995219">
Table 5: Performance on Semeval-2007 coarse-
</tableCaption>
<bodyText confidence="0.990850444444444">
grained all-words WSD. In the type column,
U, Semi and S stand for unsupervised, semi-
supervised and supervised, respectively. The dif-
ferences between the results in bold in each col-
umn of the table are not statistically significant at
p &lt; 0.05.
methods rely greatly on high-quality seman-
tic relations or annotated data, which are hard
to acquire.
</bodyText>
<subsectionHeader confidence="0.919919">
3.4 Coarse-grained WSD
</subsectionHeader>
<bodyText confidence="0.999977260869565">
Experimental setting. We also evaluate our
WSD model on the Semeval-2007 coarse-grained
all-words WSD task (Navigli et al., 2007). There
are multiple reasons that we perform experiments
in a coarse-grained setting: first, it has been ar-
gued that the fine granularity of WordNet is one
of the main obstacles to accurate WSD (cf. the
discussion in (Navigli, 2009)); second, the train-
ing corpus of word representations is Wikipedia,
which is quite different from WordNet.
Baseline methods. We compare our model with
the best unsupervised system SUSSX-FR (Koel-
ing and McCarthy, 2007), and the best supervised
system, NUS-PT (Chan et al., 2007), participat-
ing in the Semeval-2007 coarse-grained all-words
task. We also compare with SSI (Navigli and Ve-
lardi, 2005) and the state-of-the-art system De-
gree (Navigli and Lapata, 2010). We use different
baseline methods for the two WSD tasks because
we want to compare our model with the state-
of-the-art systems that are applicable to different
datasets and show that our WSD method can per-
form robustly in these different WSD tasks.
</bodyText>
<page confidence="0.987361">
1031
</page>
<bodyText confidence="0.999767096774193">
Results and discussion. We report our results in
terms of F1-measure on the Semeval-2007 coarse-
grained all-words dataset (Navigli et al., 2007).
Table 5 reports the results for nouns (1,108 words)
and all words (2,269 words). The difference be-
tween unsupervised and semi-supervised methods
is whether the method uses MFS as a back-off s-
trategy.
We can see that the S2C algorithm outperforms
the L2R algorithm no matter on the nouns subset
or on the entire set. This indicates that words with
fewer senses are easier to disambiguate, and it can
be helpful to disambiguate the words with more
senses.
On the nouns subset, our model yields compa-
rable performance to SSI and Degree, and it out-
performs NUS-PT and SUSSX-FR. Moreover, our
unsupervised WSD method (S2C) beats the MF-
S baseline, which is notably a difficult competitor
for knowledge-based systems.
On the entire set, our semi-supervised model is
significantly better than SUSSX-FR, and it is com-
parable with SSI and Degree. In contrast to SSI,
our model is simple and does not rely on a cost-
ly annotation effort to engineer the set of semantic
relations.
Overall, our model achieves state-of-the-art per-
formance on the Semeval-2007 coarse-grained all-
words dataset compared to other systems, with a
simple WSD algorithm that only relies on a large-
scale unlabeled text corpora and a sense inventory.
</bodyText>
<subsectionHeader confidence="0.977707">
3.5 Parameter Influence
</subsectionHeader>
<bodyText confidence="0.995594058823529">
We investigate the influence of parameters on our
model with coarse-grained all-words WSD task.
The parameters include the similarity threshold, δ,
and the score margin threshold, ε.
Similarity threshold. In Table 6, we show the
performance of domain WSD when the similari-
ty threshold δ ranges from −0.1 to 0.3. The co-
sine similarity interval is [-1, 1], and we focus on
the performance in the interval [-0.1, 0.3] for two
reasons: first, no words are removed from glosses
when δ &lt; −0.1; second, nearly half of the word-
s are removed when δ &gt; 0.3 and the performance
drops significantly for the WSD task. From table
6, we can see that our model achieves the best per-
formance when δ = 0.0.
Score margin threshold. In Table 7, we show
the performance on the coarse-grained all-words
</bodyText>
<table confidence="0.996419">
Parameter Nouns only All words
δ = −0.10 79.8 74.3
δ = −0.05 81.0 74.6
δ = 0.00 81.6 75.8
δ = 0.05 81.3 75.4
δ = 0.10 80.8 75.2
δ = 0.15 80.0 75.0
δ = 0.20 77.1 73.3
δ = 0.30 75.0 72.1
</table>
<tableCaption confidence="0.912470333333333">
Table 6: Evaluation results on the coarse-grained
all-words WSD when the similarity threshold δ
ranges from −0.1 to 0.3.
</tableCaption>
<table confidence="0.99471">
Parameter Nouns only All words
ε = 0.00 78.2 72.9
ε = 0.05 79.5 74.5
ε = 0.10 81.6 75.8
ε = 0.15 81.2 74.7
ε = 0.20 80.9 75.1
ε = 0.25 80.2 74.8
ε = 0.30 80.4 74.9
</table>
<tableCaption confidence="0.703849333333333">
Table 7: Evaluation results on the coarse-grained
all-words WSD when the score margin threshold
ε ranges from 0.0 to 0.3.
</tableCaption>
<bodyText confidence="0.999730714285714">
WSD when the score margin threshold ε ranges
from 0.0 to 0.3. When ε = 0.0, we use every
disambiguation result to update the context vec-
tor. When ε =� 0, we only use the confident disam-
biguation results to update the context vector if the
score margin is larger than ε. Our model achieves
the best performance when ε = 0.1.
</bodyText>
<sectionHeader confidence="0.999988" genericHeader="evaluation">
4 Related Work
</sectionHeader>
<subsectionHeader confidence="0.999954">
4.1 Word Representations
</subsectionHeader>
<bodyText confidence="0.999800615384615">
Distributed representations for words were pro-
posed in (Rumelhart et al., 1986) and have been
successfully used in language models (Bengio et
al., 2006; Mnih and Hinton, 2008) and many nat-
ural language processing tasks, such as word rep-
resentation learning (Mikolov, 2012), named enti-
ty recognition (Turian et al., 2010), disambigua-
tion (Collobert et al., 2011), parsing and tag-
ging (Socher et al., 2011; Socher et al., 2013).
They are very useful in NLP tasks because they
can be used as inputs to learning algorithms or as
extra word features in NLP systems. Hence, many
NLP applications, such as keyword extraction (Li-
</bodyText>
<page confidence="0.99225">
1032
</page>
<bodyText confidence="0.999797846153846">
u et al., 2010; Liu et al., 2011b; Liu et al., 2012),
social tag suggestion (Liu et al., 2011a) and text
classification (Baker and McCallum, 1998), may
also potentially benefit from distributed word rep-
resentation. The main advantage is that the rep-
resentations of similar words are close in vector
space, which makes generalization to novel pat-
terns easier and model estimation more robust.
Word representations are hard to train due to the
computational complexity. Recently, (Mikolov et
al., 2013) proposed two particular models, Skip-
gram and CBOW, to learn word representations in
large amounts of text data. The training objective
of the CBOW model is to combine the representa-
tions of the surrounding words to predict the word
in the middle, while the Skip-gram model’s is to
learn word representations that are good at predict-
ing its context in the same sentence (Mikolov et
al., 2013). Our paper uses the model architecture
of Skip-gram.
Most of the previous vector-space models use
one representation per word. This is problematic
because many words have multiple senses. The
multi-prototype approach has been widely stud-
ied. (Reisinger and Mooney, 2010) proposed the
multi-prototype vector-space model. (Huang et
al., 2012) used the multi-prototype models to learn
the vector for different senses of a word. All of
these models use the clustering of contexts as a
word sense and can not be directly used in word
sense disambiguation.
After our paper was submitted, we perceive the
following recent advances: (Tian et al., 2014) pro-
posed a probabilistic model for multi-prototype
word representation. (Guo et al., 2014) explored
bilingual resources to learn sense-specific word
representation. (Neelakantan et al., 2014) pro-
posed an efficient non-parametric model for multi-
prototype word representation.
</bodyText>
<subsectionHeader confidence="0.833749">
4.2 Knowledge-based WSD
</subsectionHeader>
<bodyText confidence="0.999976024390244">
The objective of word sense disambiguation (WS-
D) is to computationally identify the meaning of
words in context (Navigli, 2009). There are t-
wo approaches of WSD that assign meaning of
words from a fixed sense inventory, supervised and
knowledge-based methods. Supervised approach-
es require large labeled training sets, which are
time consuming to create. In this paper, we on-
ly focus on knowledge-based word sense disam-
biguation.
Knowledge-based approaches exploit knowl-
edge resources (such as dictionaries, thesauri, on-
tologies, collocations, etc.) to determine the
senses of words in context. However, it has
been shown in (Cuadros and Rigau, 2006) that
the amount of lexical and semantic information
contained in such resources is typically insuf-
ficient for high-performance WSD. Much work
has been presented to automatically extend ex-
isting resources, including automatically linking
Wikipedia to WordNet to include full use of the
first WordNet sense heuristic (Suchanek et al.,
2008), a graph-based mapping of Wikipedia cat-
egories to WordNet synsets (Ponzetto and Nav-
igli, 2009), and automatically mapping Wikipedia
pages to WordNet synsets (Ponzetto and Navigli,
2010).
It was recently shown that word representation-
s can capture semantic and syntactic information
between words (Mikolov et al., 2013). Some re-
searchers tried to incorporate WordNet senses in a
neural model to learn better word representation-
s (Bordes et al., 2011). In this paper, we have pro-
posed a unified method for word sense representa-
tion and disambiguation to extend the information
contained in the vector representations to the ex-
isting resources. Our method only requires a large
amount of unlabeled text to train sense representa-
tions and a dictionary to provide the definitions of
word meanings, which makes it easily applicable
to other resources.
</bodyText>
<sectionHeader confidence="0.999449" genericHeader="conclusions">
5 Conclusion
</sectionHeader>
<bodyText confidence="0.999915214285714">
In this paper, we present a unified model for word
sense representation and disambiguation that us-
es one representation per sense. Experimental re-
sults show that our model improves the perfor-
mance of contextual word similarity compared to
existing WSR methods, outperforms state-of-the-
art supervised methods on domain-specific WSD,
and achieves competitive performance on coarse-
grained all-words WSD. Our model only requires
large-scale unlabeled text corpora and a sense in-
ventory for WSD, thus it can be easily applied to
other corpora and tasks.
There are still several open problems that
should be investigated further:
</bodyText>
<footnote confidence="0.65778175">
1. Because the senses of words change over
time (new senses appear), we will incorpo-
rate cluster-based methods in our model to
find senses that are not in the sense inventory.
</footnote>
<page confidence="0.981278">
1033
</page>
<bodyText confidence="0.839918090909091">
2. We can explore other WSD methods based
on sense vectors to improve our performance.
For example, (Li et al., 2010) used LDA to
perform data-driven WSD in a manner simi-
lar to our model. We may integrate the advan-
tages of these models and our model together
to build a more powerful WSD system.
3. To learn better sense vectors, we can exploit
the semantic relations (such as the hypernym
and hyponym relations defined in WordNet)
between senses in our model.
</bodyText>
<sectionHeader confidence="0.997884" genericHeader="acknowledgments">
Acknowledgments
</sectionHeader>
<bodyText confidence="0.987824">
This work is supported by National Key Ba-
sic Research Program of China (973 Program
2014CB340500) and National Natural Science
Foundation of China (NSFC 61133012).
</bodyText>
<sectionHeader confidence="0.999271" genericHeader="references">
References
</sectionHeader>
<reference confidence="0.996898738095238">
Eneko Agirre, Oier Lopez De Lacalle, Aitor Soroa, and
Informatika Fakultatea. 2009. Knowledge-based
wsd and specific domains: Performing better than
generic supervised wsd. In Proceedings of IJCAI,
pages 1501–1506.
L Douglas Baker and Andrew Kachites McCallum.
1998. Distributional clustering of words for text
classification. In Proceedings of SIGIR, pages 96–
103.
Yoshua Bengio, Holger Schwenk, Jean-S´ebastien
Sen´ecal, Fr´ederic Morin, and Jean-Luc Gauvain.
2006. Neural probabilistic language models. In In-
novations in Machine Learning, pages 137–186.
Antoine Bordes, Jason Weston, Ronan Collobert,
Yoshua Bengio, et al. 2011. Learning structured
embeddings of knowledge bases. In Proceedings of
AAAI, pages 301–306.
Yee Seng Chan, Hwee Tou Ng, and Zhi Zhong. 2007.
Nus-pt: exploiting parallel texts for word sense dis-
ambiguation in the english all-words tasks. In Pro-
ceedings of SemEval, pages 253–256.
Ronan Collobert and Jason Weston. 2008. A unified
architecture for natural language processing: Deep
neural networks with multitask learning. In Pro-
ceedings of ICML, pages 160–167.
Ronan Collobert, Jason Weston, L´eon Bottou, Michael
Karlen, Koray Kavukcuoglu, and Pavel Kuksa.
2011. Natural language processing (almost) from
scratch. JMLR, 12:2493–2537.
Montse Cuadros and German Rigau. 2006. Quality
assessment of large scale knowledge resources. In
Proceedings of EMNLP, pages 534–541.
Katrin Erk and Sebastian Pado. 2010. Exemplar-based
models for word meaning in context. In Proceedings
ofACL, pages 92–97.
Lev Finkelstein, Evgeniy Gabrilovich, Yossi Matias, E-
hud Rivlin, Zach Solan, Gadi Wolfman, and Eytan
Ruppin. 2001. Placing search in context: The con-
cept revisited. In Proceedings of WWW, pages 406–
414.
Jiang Guo, Wanxiang Che, Haifeng Wang, and Ting Li-
u. 2014. Learning sense-specific word embeddings
by exploiting bilingual resources. In Proceedings of
COLING, pages 497–507.
Eric H Huang, Richard Socher, Christopher D Man-
ning, and Andrew Y Ng. 2012. Improving word
representations via global context and multiple word
prototypes. In Proceedings of ACL, pages 873–882.
Rob Koeling and Diana McCarthy. 2007. Sussx: Ws-
d using automatically acquired predominant senses.
In Proceedings of SemEval, pages 314–317.
Rob Koeling, Diana McCarthy, and John Carroll.
2005. Domain-specific sense distributions and pre-
dominant sense acquisition. In Proceedings of HLT-
EMNLP, pages 419–426.
Linlin Li, Benjamin Roth, and Caroline Sporleder.
2010. Topic models for word sense disambiguation
and token-based idiom detection. In Proceedings of
ACL, pages 1138–1147.
Zhiyuan Liu, Wenyi Huang, Yabin Zheng, and
Maosong Sun. 2010. Automatic keyphrase extrac-
tion via topic decomposition. In Proceedings of
EMNLP, pages 366–376.
Zhiyuan Liu, Xinxiong Chen, and Maosong Sun.
2011a. A simple word trigger method for social tag
suggestion. In Proceedings of EMNLP, pages 1577–
1588.
Zhiyuan Liu, Xinxiong Chen, Yabin Zheng, and
Maosong Sun. 2011b. Automatic keyphrase extrac-
tion by bridging vocabulary gap. In Proceedings of
CoNLL, pages 135–144.
Zhiyuan Liu, Xinxiong Chen, and Maosong Sun. 2012.
Mining the interests of chinese microbloggers via
keyword extraction. Frontiers of Computer Science,
6(1):76–87.
Christopher D Manning, Prabhakar Raghavan, and
Hinrich Sch¨utze. 2008. Introduction to information
retrieval. Cambridge University Press Cambridge.
Tomas Mikolov, Kai Chen, Greg Corrado, and Jeffrey
Dean. 2013. Efficient estimation of word represen-
tations in vector space. Proceedings of ICLR.
Tomas Mikolov. 2012. Statistical Language Model-
s Based on Neural Networks. Ph.D. thesis, Ph. D.
thesis, Brno University of Technology.
</reference>
<page confidence="0.8853">
1034
</page>
<reference confidence="0.999925231884058">
George A Miller, Claudia Leacock, Randee Tengi, and
Ross T Bunker. 1993. A semantic concordance. In
Proceedings of the workshop on HLT, pages 303–
308.
George A Miller. 1995. Wordnet: a lexical
database for english. Communications of the ACM,
38(11):39–41.
Andriy Mnih and Geoffrey E Hinton. 2008. A s-
calable hierarchical distributed language model. In
Proceedings of NIPS, pages 1081–1088.
Frederic Morin and Yoshua Bengio. 2005. Hierarchi-
cal probabilistic neural network language model. In
Proceedings of the international workshop on artifi-
cial intelligence and statistics, pages 246–252.
Roberto Navigli and Mirella Lapata. 2010. An ex-
perimental study of graph connectivity for unsu-
pervised word sense disambiguation. IEEE PAMI,
32(4):678–692.
Roberto Navigli and Paola Velardi. 2005. Structural
semantic interconnections: a knowledge-based ap-
proach to word sense disambiguation. IEEE PAMI,
27(7):1075–1086.
Roberto Navigli, Kenneth C Litkowski, and Orin Har-
graves. 2007. Semeval-2007 task 07: Coarse-
grained english all-words task. In Proceedings of
SemEval, pages 30–35.
Roberto Navigli. 2009. Word sense disambiguation: A
survey. CSUR, 41(2):10.
Arvind Neelakantan, Jeevan Shankar, Alexandre Pas-
sos, and Andrew McCallum. 2014. Efficient non-
parametric estimation of multiple embeddings per
word in vector space. In Proceedings of EMNLP.
Simone Paolo Ponzetto and Roberto Navigli. 2009.
Large-scale taxonomy mapping for restructuring
and integrating wikipedia. In Proceedings of IJCAI,
volume 9, pages 2083–2088.
Simone Paolo Ponzetto and Roberto Navigli. 2010.
Knowledge-rich word sense disambiguation rivaling
supervised systems. In Proceedings of ACL, pages
1522–1531.
Joseph Reisinger and Raymond J Mooney. 2010.
Multi-prototype vector-space models of word mean-
ing. In Proceedings of HLT-NAACL, pages 109–
117.
David E Rumelhart, Geoffrey E Hintont, and Ronald J
Williams. 1986. Learning representations by back-
propagating errors. Nature, 323(6088):533–536.
Fabrizio Sebastiani. 2002. Machine learning in auto-
mated text categorization. CSUR, 34(1):1–47.
Richard Socher, Cliff C Lin, Andrew Ng, and Chris
Manning. 2011. Parsing natural scenes and natural
language with recursive neural networks. In Pro-
ceedings of ICML, pages 129–136.
Richard Socher, John Bauer, Christopher D Manning,
and Andrew Y Ng. 2013. Parsing with composi-
tional vector grammars. In Proceedings of ACL.
Fabian M Suchanek, Gjergji Kasneci, and Gerhard
Weikum. 2008. Yago: A large ontology from
wikipedia and wordnet. Web Semantics: Sci-
ence, Services and Agents on the World Wide Web,
6(3):203–217.
Fei Tian, Hanjun Dai, Jiang Bian, Bin Gao, Rui Zhang,
Enhong Chen, and Tie-Yan Liu. 2014. A probabilis-
tic model for learning multi-prototype word embed-
dings. In Proceedings of COLING, pages 151–160.
Joseph Turian, Lev Ratinov, and Yoshua Bengio. 2010.
Word representations: a simple and general method
for semi-supervised learning. In Proceedings of A-
CL, pages 384–394.
</reference>
<page confidence="0.992932">
1035
</page>
</variant>
</algorithm>
<algorithm name="ParsHed" version="110505">
<variant no="0" confidence="0.324044">
<title confidence="0.999872">A Unified Model for Word Sense Representation and Disambiguation</title>
<author confidence="0.799838">Xinxiong Chen</author>
<author confidence="0.799838">Zhiyuan Liu</author>
<author confidence="0.799838">Maosong</author>
<affiliation confidence="0.80792125">State Key Laboratory of Intelligent Technology and Tsinghua National Laboratory for Information Science and Department of Computer Science and Tsinghua University, Beijing 100084,</affiliation>
<abstract confidence="0.98834274074074">Most word representation methods assume that each word owns a single semantic vector. This is usually problematic because lexical ambiguity is ubiquitous, which is also the problem to be resolved by word sense disambiguation. In this paper, we present a unified model for joint word sense representation and disambiguation, which will assign distinct representationfor each word The basic idea is that both word sense representation (WS- R) and word sense disambiguation (WS- D) will benefit from each other: (1) highquality WSR will capture rich information about words and senses, which should be helpful for WSD, and (2) high-quality WSD will provide reliable disambiguated corpora for learning better sense representations. Experimental results show that, our model improves the performance of contextual word similarity compared to existing WSR methods, outperforms stateof-the-art supervised methods on domainspecific WSD, and achieves competitive performance on coarse-grained all-words WSD.</abstract>
</variant>
</algorithm>
<algorithm name="ParsCit" version="110505">
<citationList>
<citation valid="true">
<authors>
<author>Eneko Agirre</author>
</authors>
<title>Oier Lopez De Lacalle, Aitor Soroa, and Informatika Fakultatea.</title>
<date>2009</date>
<booktitle>In Proceedings of IJCAI,</booktitle>
<pages>1501--1506</pages>
<marker>Agirre, 2009</marker>
<rawString>Eneko Agirre, Oier Lopez De Lacalle, Aitor Soroa, and Informatika Fakultatea. 2009. Knowledge-based wsd and specific domains: Performing better than generic supervised wsd. In Proceedings of IJCAI, pages 1501–1506.</rawString>
</citation>
<citation valid="true">
<authors>
<author>L Douglas Baker</author>
<author>Andrew Kachites McCallum</author>
</authors>
<title>Distributional clustering of words for text classification.</title>
<date>1998</date>
<booktitle>In Proceedings of SIGIR,</booktitle>
<pages>96--103</pages>
<contexts>
<context position="30805" citStr="Baker and McCallum, 1998" startWordPosition="5090" endWordPosition="5093">2006; Mnih and Hinton, 2008) and many natural language processing tasks, such as word representation learning (Mikolov, 2012), named entity recognition (Turian et al., 2010), disambiguation (Collobert et al., 2011), parsing and tagging (Socher et al., 2011; Socher et al., 2013). They are very useful in NLP tasks because they can be used as inputs to learning algorithms or as extra word features in NLP systems. Hence, many NLP applications, such as keyword extraction (Li1032 u et al., 2010; Liu et al., 2011b; Liu et al., 2012), social tag suggestion (Liu et al., 2011a) and text classification (Baker and McCallum, 1998), may also potentially benefit from distributed word representation. The main advantage is that the representations of similar words are close in vector space, which makes generalization to novel patterns easier and model estimation more robust. Word representations are hard to train due to the computational complexity. Recently, (Mikolov et al., 2013) proposed two particular models, Skipgram and CBOW, to learn word representations in large amounts of text data. The training objective of the CBOW model is to combine the representations of the surrounding words to predict the word in the middle</context>
</contexts>
<marker>Baker, McCallum, 1998</marker>
<rawString>L Douglas Baker and Andrew Kachites McCallum. 1998. Distributional clustering of words for text classification. In Proceedings of SIGIR, pages 96– 103.</rawString>
</citation>
<citation valid="true">
<authors>
<author>Yoshua Bengio</author>
<author>Holger Schwenk</author>
<author>Jean-S´ebastien Sen´ecal</author>
<author>Fr´ederic Morin</author>
<author>Jean-Luc Gauvain</author>
</authors>
<title>Neural probabilistic language models.</title>
<date>2006</date>
<booktitle>In Innovations in Machine Learning,</booktitle>
<pages>137--186</pages>
<marker>Bengio, Schwenk, Sen´ecal, Morin, Gauvain, 2006</marker>
<rawString>Yoshua Bengio, Holger Schwenk, Jean-S´ebastien Sen´ecal, Fr´ederic Morin, and Jean-Luc Gauvain. 2006. Neural probabilistic language models. In Innovations in Machine Learning, pages 137–186.</rawString>
</citation>
<citation valid="true">
<authors>
<author>Antoine Bordes</author>
<author>Jason Weston</author>
<author>Ronan Collobert</author>
<author>Yoshua Bengio</author>
</authors>
<title>Learning structured embeddings of knowledge bases.</title>
<date>2011</date>
<booktitle>In Proceedings of AAAI,</booktitle>
<pages>301--306</pages>
<contexts>
<context position="33935" citStr="Bordes et al., 2011" startWordPosition="5570" endWordPosition="5573">matically extend existing resources, including automatically linking Wikipedia to WordNet to include full use of the first WordNet sense heuristic (Suchanek et al., 2008), a graph-based mapping of Wikipedia categories to WordNet synsets (Ponzetto and Navigli, 2009), and automatically mapping Wikipedia pages to WordNet synsets (Ponzetto and Navigli, 2010). It was recently shown that word representations can capture semantic and syntactic information between words (Mikolov et al., 2013). Some researchers tried to incorporate WordNet senses in a neural model to learn better word representations (Bordes et al., 2011). In this paper, we have proposed a unified method for word sense representation and disambiguation to extend the information contained in the vector representations to the existing resources. Our method only requires a large amount of unlabeled text to train sense representations and a dictionary to provide the definitions of word meanings, which makes it easily applicable to other resources. 5 Conclusion In this paper, we present a unified model for word sense representation and disambiguation that uses one representation per sense. Experimental results show that our model improves the perfo</context>
</contexts>
<marker>Bordes, Weston, Collobert, Bengio, 2011</marker>
<rawString>Antoine Bordes, Jason Weston, Ronan Collobert, Yoshua Bengio, et al. 2011. Learning structured embeddings of knowledge bases. In Proceedings of AAAI, pages 301–306.</rawString>
</citation>
<citation valid="true">
<authors>
<author>Yee Seng Chan</author>
<author>Hwee Tou Ng</author>
<author>Zhi Zhong</author>
</authors>
<title>Nus-pt: exploiting parallel texts for word sense disambiguation in the english all-words tasks.</title>
<date>2007</date>
<booktitle>In Proceedings of SemEval,</booktitle>
<pages>253--256</pages>
<contexts>
<context position="26477" citStr="Chan et al., 2007" startWordPosition="4332" endWordPosition="4335"> We also evaluate our WSD model on the Semeval-2007 coarse-grained all-words WSD task (Navigli et al., 2007). There are multiple reasons that we perform experiments in a coarse-grained setting: first, it has been argued that the fine granularity of WordNet is one of the main obstacles to accurate WSD (cf. the discussion in (Navigli, 2009)); second, the training corpus of word representations is Wikipedia, which is quite different from WordNet. Baseline methods. We compare our model with the best unsupervised system SUSSX-FR (Koeling and McCarthy, 2007), and the best supervised system, NUS-PT (Chan et al., 2007), participating in the Semeval-2007 coarse-grained all-words task. We also compare with SSI (Navigli and Velardi, 2005) and the state-of-the-art system Degree (Navigli and Lapata, 2010). We use different baseline methods for the two WSD tasks because we want to compare our model with the stateof-the-art systems that are applicable to different datasets and show that our WSD method can perform robustly in these different WSD tasks. 1031 Results and discussion. We report our results in terms of F1-measure on the Semeval-2007 coarsegrained all-words dataset (Navigli et al., 2007). Table 5 reports</context>
</contexts>
<marker>Chan, Ng, Zhong, 2007</marker>
<rawString>Yee Seng Chan, Hwee Tou Ng, and Zhi Zhong. 2007. Nus-pt: exploiting parallel texts for word sense disambiguation in the english all-words tasks. In Proceedings of SemEval, pages 253–256.</rawString>
</citation>
<citation valid="true">
<authors>
<author>Ronan Collobert</author>
<author>Jason Weston</author>
</authors>
<title>A unified architecture for natural language processing: Deep neural networks with multitask learning.</title>
<date>2008</date>
<booktitle>In Proceedings of ICML,</booktitle>
<pages>160--167</pages>
<contexts>
<context position="21028" citStr="Collobert and Weston, 2008" startWordPosition="3433" endWordPosition="3436">taset, each word in a pair has a sentential context. In our experiments, the similarity between a pair of words (w, w&apos;) is computed as follows: MN i=1 j=1 p(i|w,c)p(j|w&apos;,c&apos;)d(vec(wsi),vec(w&apos;sj)) (7) where p(i|w,c) is the likelihood that word w chooses its ith sense given context c. d(vec,vec&apos;) is a function computing the similarity between two vectors, and here we use cosine similarity. Results and discussion. For evaluation, we compute the Spearman correlation between a model’s computed similarity scores and human judgements. Table 3 shows our results compared to previous methods, including (Collobert and Weston, 2008)’s language model (C&amp;W), and Huang’s model which utilize the global context and multi-prototype to improve the word representations. 6The dataset can be downloaded at http://ai. stanford.edu/˜ehhuang/. From Table 3, we observe that: • Our single-vector version outperforms Huang’s single-vector version. This indicates that, by training the word vectors and sense vectors jointly, our model can better capture the semantic relationships between words and senses. • With one representation per sense, our model can outperform the single-vector version without using context (66.2 vs. 64.2). • Our mode</context>
</contexts>
<marker>Collobert, Weston, 2008</marker>
<rawString>Ronan Collobert and Jason Weston. 2008. A unified architecture for natural language processing: Deep neural networks with multitask learning. In Proceedings of ICML, pages 160–167.</rawString>
</citation>
<citation valid="true">
<authors>
<author>Ronan Collobert</author>
<author>Jason Weston</author>
<author>L´eon Bottou</author>
<author>Michael Karlen</author>
<author>Koray Kavukcuoglu</author>
<author>Pavel Kuksa</author>
</authors>
<title>Natural language processing (almost) from scratch.</title>
<date>2011</date>
<pages>12--2493</pages>
<publisher>JMLR,</publisher>
<contexts>
<context position="30394" citStr="Collobert et al., 2011" startWordPosition="5017" endWordPosition="5020">n result to update the context vector. When ε =� 0, we only use the confident disambiguation results to update the context vector if the score margin is larger than ε. Our model achieves the best performance when ε = 0.1. 4 Related Work 4.1 Word Representations Distributed representations for words were proposed in (Rumelhart et al., 1986) and have been successfully used in language models (Bengio et al., 2006; Mnih and Hinton, 2008) and many natural language processing tasks, such as word representation learning (Mikolov, 2012), named entity recognition (Turian et al., 2010), disambiguation (Collobert et al., 2011), parsing and tagging (Socher et al., 2011; Socher et al., 2013). They are very useful in NLP tasks because they can be used as inputs to learning algorithms or as extra word features in NLP systems. Hence, many NLP applications, such as keyword extraction (Li1032 u et al., 2010; Liu et al., 2011b; Liu et al., 2012), social tag suggestion (Liu et al., 2011a) and text classification (Baker and McCallum, 1998), may also potentially benefit from distributed word representation. The main advantage is that the representations of similar words are close in vector space, which makes generalization to</context>
</contexts>
<marker>Collobert, Weston, Bottou, Karlen, Kavukcuoglu, Kuksa, 2011</marker>
<rawString>Ronan Collobert, Jason Weston, L´eon Bottou, Michael Karlen, Koray Kavukcuoglu, and Pavel Kuksa. 2011. Natural language processing (almost) from scratch. JMLR, 12:2493–2537.</rawString>
</citation>
<citation valid="true">
<authors>
<author>Montse Cuadros</author>
<author>German Rigau</author>
</authors>
<title>Quality assessment of large scale knowledge resources.</title>
<date>2006</date>
<booktitle>In Proceedings of EMNLP,</booktitle>
<pages>534--541</pages>
<contexts>
<context position="33146" citStr="Cuadros and Rigau, 2006" startWordPosition="5451" endWordPosition="5454">e disambiguation (WSD) is to computationally identify the meaning of words in context (Navigli, 2009). There are two approaches of WSD that assign meaning of words from a fixed sense inventory, supervised and knowledge-based methods. Supervised approaches require large labeled training sets, which are time consuming to create. In this paper, we only focus on knowledge-based word sense disambiguation. Knowledge-based approaches exploit knowledge resources (such as dictionaries, thesauri, ontologies, collocations, etc.) to determine the senses of words in context. However, it has been shown in (Cuadros and Rigau, 2006) that the amount of lexical and semantic information contained in such resources is typically insufficient for high-performance WSD. Much work has been presented to automatically extend existing resources, including automatically linking Wikipedia to WordNet to include full use of the first WordNet sense heuristic (Suchanek et al., 2008), a graph-based mapping of Wikipedia categories to WordNet synsets (Ponzetto and Navigli, 2009), and automatically mapping Wikipedia pages to WordNet synsets (Ponzetto and Navigli, 2010). It was recently shown that word representations can capture semantic and </context>
</contexts>
<marker>Cuadros, Rigau, 2006</marker>
<rawString>Montse Cuadros and German Rigau. 2006. Quality assessment of large scale knowledge resources. In Proceedings of EMNLP, pages 534–541.</rawString>
</citation>
<citation valid="true">
<authors>
<author>Katrin Erk</author>
<author>Sebastian Pado</author>
</authors>
<title>Exemplar-based models for word meaning in context.</title>
<date>2010</date>
<booktitle>In Proceedings ofACL,</booktitle>
<pages>92--97</pages>
<contexts>
<context position="10620" citStr="Erk and Pado, 2010" startWordPosition="1709" endWordPosition="1712">7), (body, 0.01), (water, 0.10), (pulled, 0.01), (canoe, 0.09), (sat, 0.06), (river, 0.43), (watch, -0.11), (currents, 0.01). If the threshold, δ, is set to 0.05, then cand(banks1) is {sloping, land, slope, water, canoe, sat, river}. Then the average of the word vectors in cand(banksi) is used as the initialization value of vec(banksi). 2.3 Performing Word Sense Disambiguation. One of the state-of-the-art WSD results can be obtained using exemplar models, i.e., the word meaning is modeled by using relevant occurrences only, rather than merging all of the occurrences into a single word vector (Erk and Pado, 2010). Inspired by this idea, we perform word sense disambiguation to obtain more relevant occurrences. Here, we perform knowledge-based word sense disambiguation for training data on an all-words setting, i.e., we will disambiguate all of the content words in a sentence. Formally, the sentence S is a sequence of words (w1,w2,...,wn), and we will identify a mapping M from words to senses such that M(i) E SensesWN(wi), where SensesWN(wi) is the set of senses encoded in the WN for word wi. For sentence S, there are ∏ni=1 |SenseWN(wi) |possible mapping answers, which are impractical to compute. Thus, </context>
</contexts>
<marker>Erk, Pado, 2010</marker>
<rawString>Katrin Erk and Sebastian Pado. 2010. Exemplar-based models for word meaning in context. In Proceedings ofACL, pages 92–97.</rawString>
</citation>
<citation valid="true">
<authors>
<author>Lev Finkelstein</author>
<author>Evgeniy Gabrilovich</author>
<author>Yossi Matias</author>
<author>Ehud Rivlin</author>
<author>Zach Solan</author>
<author>Gadi Wolfman</author>
<author>Eytan Ruppin</author>
</authors>
<title>Placing search in context: The concept revisited.</title>
<date>2001</date>
<booktitle>In Proceedings of WWW,</booktitle>
<pages>406--414</pages>
<contexts>
<context position="19403" citStr="Finkelstein et al., 2001" startWordPosition="3171" endWordPosition="3174">hbors of word vectors and sense vectors based on cosine similarity. We see that our sense representations can identify different meanings of a word, allowing our model to capture more semantic and syntactic relationships between words and senses. Note that each sense vector in our model corresponds to a sense in WordNet; thus, our sense vectors can be used to perform knowledge-based word sense disambiguation, whereas the vectors of cluster-based models cannot. 3.2 Contextual Word Similarity Experimental setting. A standard dataset for evaluating a vector-space model is the WordSim353 dataset (Finkelstein et al., 2001), which con1029 Model ρ x 100 C&amp;W-S 57.0 Huang-S 58.6 Huang-M AvgSim 62.8 Huang-M AvgSimC 65.7 Our Model-S 64.2 Our Model-M AvgSim 66.2 Our Model-M AvgSimC 68.9 Table 3: Spearman’s ρ on the SCWS dataset. Our Model-S uses one representation per word to compute similarities, while Our Model-M uses one representation per sense to compute similarities. AvgSim calculates the similarity with each sense contributing equally, while AvgSimC weighs the sense according to the probability of the word choosing that sense in context c. sists of 353 pairs of nouns. However, each pair of nouns in WordSim-353 </context>
</contexts>
<marker>Finkelstein, Gabrilovich, Matias, Rivlin, Solan, Wolfman, Ruppin, 2001</marker>
<rawString>Lev Finkelstein, Evgeniy Gabrilovich, Yossi Matias, Ehud Rivlin, Zach Solan, Gadi Wolfman, and Eytan Ruppin. 2001. Placing search in context: The concept revisited. In Proceedings of WWW, pages 406– 414.</rawString>
</citation>
<citation valid="true">
<authors>
<author>Jiang Guo</author>
<author>Wanxiang Che</author>
<author>Haifeng Wang</author>
<author>Ting Liu</author>
</authors>
<title>Learning sense-specific word embeddings by exploiting bilingual resources.</title>
<date>2014</date>
<booktitle>In Proceedings of COLING,</booktitle>
<pages>497--507</pages>
<contexts>
<context position="32287" citStr="Guo et al., 2014" startWordPosition="5325" endWordPosition="5328">er word. This is problematic because many words have multiple senses. The multi-prototype approach has been widely studied. (Reisinger and Mooney, 2010) proposed the multi-prototype vector-space model. (Huang et al., 2012) used the multi-prototype models to learn the vector for different senses of a word. All of these models use the clustering of contexts as a word sense and can not be directly used in word sense disambiguation. After our paper was submitted, we perceive the following recent advances: (Tian et al., 2014) proposed a probabilistic model for multi-prototype word representation. (Guo et al., 2014) explored bilingual resources to learn sense-specific word representation. (Neelakantan et al., 2014) proposed an efficient non-parametric model for multiprototype word representation. 4.2 Knowledge-based WSD The objective of word sense disambiguation (WSD) is to computationally identify the meaning of words in context (Navigli, 2009). There are two approaches of WSD that assign meaning of words from a fixed sense inventory, supervised and knowledge-based methods. Supervised approaches require large labeled training sets, which are time consuming to create. In this paper, we only focus on know</context>
</contexts>
<marker>Guo, Che, Wang, Liu, 2014</marker>
<rawString>Jiang Guo, Wanxiang Che, Haifeng Wang, and Ting Liu. 2014. Learning sense-specific word embeddings by exploiting bilingual resources. In Proceedings of COLING, pages 497–507.</rawString>
</citation>
<citation valid="true">
<authors>
<author>Eric H Huang</author>
<author>Richard Socher</author>
<author>Christopher D Manning</author>
<author>Andrew Y Ng</author>
</authors>
<title>Improving word representations via global context and multiple word prototypes.</title>
<date>2012</date>
<booktitle>In Proceedings of ACL,</booktitle>
<pages>873--882</pages>
<contexts>
<context position="2304" citStr="Huang et al. (2012)" startWordPosition="338" endWordPosition="341">2008), document classification (Sebastiani, 2002) and other NLP tasks. 1Our sense representations can be downloaded at http: //pan.baidu.com/s/1eQcPK8i. Most word representation methods assume each word owns a single vector. However, this is usually problematic due to the homonymy and polysemy of many words. To remedy the issue, Reisinger and Mooney (2010) proposed a multi-prototype vector space model, where the contexts of each word are first clustered into groups, and then each cluster generates a distinct prototype vector for a word by averaging over all context vectors within the cluster. Huang et al. (2012) followed this idea, but introduced continuous distributed vectors based on probabilistic neural language models for word representations. These cluster-based models conduct unsupervised word sense induction by clustering word contexts and, thus, suffer from the following issues: • It is usually difficult for these cluster-based models to determine the number of clusters. Huang et al. (2012) simply cluster word contexts into static K clusters for each word, which is arbitrary and may introduce mistakes. • These cluster-based models are typically offline , so they cannot be efficiently adapted </context>
<context position="20246" citStr="Huang et al., 2012" startWordPosition="3306" endWordPosition="3309"> representation per word to compute similarities, while Our Model-M uses one representation per sense to compute similarities. AvgSim calculates the similarity with each sense contributing equally, while AvgSimC weighs the sense according to the probability of the word choosing that sense in context c. sists of 353 pairs of nouns. However, each pair of nouns in WordSim-353 is presented without context. This is problematic because the meanings of homonymous and polysemous words depend highly on the words’ contexts. Thus we choose the Stanford’s Contextual Word Similarities (SCWS) dataset from (Huang et al., 2012) 6. The SCWS dataset contains 2003 pairs of words and each pair is associated with 10 human judgments on similarity on a scale from 0 to 10. In the SCWS dataset, each word in a pair has a sentential context. In our experiments, the similarity between a pair of words (w, w&apos;) is computed as follows: MN i=1 j=1 p(i|w,c)p(j|w&apos;,c&apos;)d(vec(wsi),vec(w&apos;sj)) (7) where p(i|w,c) is the likelihood that word w chooses its ith sense given context c. d(vec,vec&apos;) is a function computing the similarity between two vectors, and here we use cosine similarity. Results and discussion. For evaluation, we compute the </context>
<context position="31892" citStr="Huang et al., 2012" startWordPosition="5260" endWordPosition="5263">he training objective of the CBOW model is to combine the representations of the surrounding words to predict the word in the middle, while the Skip-gram model’s is to learn word representations that are good at predicting its context in the same sentence (Mikolov et al., 2013). Our paper uses the model architecture of Skip-gram. Most of the previous vector-space models use one representation per word. This is problematic because many words have multiple senses. The multi-prototype approach has been widely studied. (Reisinger and Mooney, 2010) proposed the multi-prototype vector-space model. (Huang et al., 2012) used the multi-prototype models to learn the vector for different senses of a word. All of these models use the clustering of contexts as a word sense and can not be directly used in word sense disambiguation. After our paper was submitted, we perceive the following recent advances: (Tian et al., 2014) proposed a probabilistic model for multi-prototype word representation. (Guo et al., 2014) explored bilingual resources to learn sense-specific word representation. (Neelakantan et al., 2014) proposed an efficient non-parametric model for multiprototype word representation. 4.2 Knowledge-based </context>
</contexts>
<marker>Huang, Socher, Manning, Ng, 2012</marker>
<rawString>Eric H Huang, Richard Socher, Christopher D Manning, and Andrew Y Ng. 2012. Improving word representations via global context and multiple word prototypes. In Proceedings of ACL, pages 873–882.</rawString>
</citation>
<citation valid="true">
<authors>
<author>Rob Koeling</author>
<author>Diana McCarthy</author>
</authors>
<title>Sussx: Wsd using automatically acquired predominant senses.</title>
<date>2007</date>
<booktitle>In Proceedings of SemEval,</booktitle>
<pages>314--317</pages>
<contexts>
<context position="26417" citStr="Koeling and McCarthy, 2007" startWordPosition="4321" endWordPosition="4325">ich are hard to acquire. 3.4 Coarse-grained WSD Experimental setting. We also evaluate our WSD model on the Semeval-2007 coarse-grained all-words WSD task (Navigli et al., 2007). There are multiple reasons that we perform experiments in a coarse-grained setting: first, it has been argued that the fine granularity of WordNet is one of the main obstacles to accurate WSD (cf. the discussion in (Navigli, 2009)); second, the training corpus of word representations is Wikipedia, which is quite different from WordNet. Baseline methods. We compare our model with the best unsupervised system SUSSX-FR (Koeling and McCarthy, 2007), and the best supervised system, NUS-PT (Chan et al., 2007), participating in the Semeval-2007 coarse-grained all-words task. We also compare with SSI (Navigli and Velardi, 2005) and the state-of-the-art system Degree (Navigli and Lapata, 2010). We use different baseline methods for the two WSD tasks because we want to compare our model with the stateof-the-art systems that are applicable to different datasets and show that our WSD method can perform robustly in these different WSD tasks. 1031 Results and discussion. We report our results in terms of F1-measure on the Semeval-2007 coarsegrain</context>
</contexts>
<marker>Koeling, McCarthy, 2007</marker>
<rawString>Rob Koeling and Diana McCarthy. 2007. Sussx: Wsd using automatically acquired predominant senses. In Proceedings of SemEval, pages 314–317.</rawString>
</citation>
<citation valid="true">
<authors>
<author>Rob Koeling</author>
<author>Diana McCarthy</author>
<author>John Carroll</author>
</authors>
<title>Domain-specific sense distributions and predominant sense acquisition.</title>
<date>2005</date>
<booktitle>In Proceedings of HLTEMNLP,</booktitle>
<pages>419--426</pages>
<contexts>
<context position="21984" citStr="Koeling et al., 2005" startWordPosition="3579" endWordPosition="3582">g the word vectors and sense vectors jointly, our model can better capture the semantic relationships between words and senses. • With one representation per sense, our model can outperform the single-vector version without using context (66.2 vs. 64.2). • Our model obtains the best performance (68.9) by using AvgSimC, which takes context into account. 3.3 Domain-Specific WSD Experimental setting. We use Wikipedia as training data because of its wide coverage for specific domains. To test our performance on domain word sense disambiguation, we evaluated our system on the dataset published in (Koeling et al., 2005). This dataset consists of examples retrieved from the Sports and Finance sections of the Reuters corpus. 41 words related to the Sports and Finance domains were selected, with an average polysemy of 6.7 senses, ranging from 2 to 13 senses. Approximately 100 examples for each word were annotated with senses from WordNet v.1.7 by three reviewers, yielding an inter-tagger agreement of 65%. (Koeling et al., 2005) did not clarify how to select the “correct” sense for each word, so we followed the work of (Agirre et al., 2009) and, used the sense chosen by the majority of taggers as the correct ans</context>
<context position="23585" citStr="Koeling et al., 2005" startWordPosition="3848" endWordPosition="3851">ver the semantic graph based on WordNet and obtains a context-independent ranking of word senses. k-NN is a widely used classification method, where neighbors are the k labeled examples most 7We compare only with those systems performing tokenbased WSD, i.e., disambiguating each instance of a target word separately. M N 1 AvgSimC(w,w&apos;) = ∑ ∑ 1030 Algorithm Sports Finance Recall Recall Random BL 19.5 19.6 MFS BL 19.6 37.1 k-NN 30.3 43.4 Static PR 20.1 39.6 Personalized PR 35.6 46.9 Degree 42.0 47.8 Our Model 57.3 60.6 Table 4: Performance on the Sports and Finance sections of the dataset from (Koeling et al., 2005). similar to the test example. The k-NN system is trained on SemCor (Miller et al., 1993), the largest publicly available annotated corpus. Degree and Personalized PageRank are stateof-the-art systems that exploit WordNet to build a semantic graph and exploit the structural properties of the graph in order to choose the appropriate senses of words in context. Results and discussion. Similar to other work on this dataset, we use recall (the ratio of correct sense labels to the total labels in the gold standard) as our evaluation measure. Table 4 shows the results of different WSD systems on the</context>
</contexts>
<marker>Koeling, McCarthy, Carroll, 2005</marker>
<rawString>Rob Koeling, Diana McCarthy, and John Carroll. 2005. Domain-specific sense distributions and predominant sense acquisition. In Proceedings of HLTEMNLP, pages 419–426.</rawString>
</citation>
<citation valid="true">
<authors>
<author>Linlin Li</author>
<author>Benjamin Roth</author>
<author>Caroline Sporleder</author>
</authors>
<title>Topic models for word sense disambiguation and token-based idiom detection.</title>
<date>2010</date>
<booktitle>In Proceedings of ACL,</booktitle>
<pages>1138--1147</pages>
<marker>Li, Roth, Sporleder, 2010</marker>
<rawString>Linlin Li, Benjamin Roth, and Caroline Sporleder. 2010. Topic models for word sense disambiguation and token-based idiom detection. In Proceedings of ACL, pages 1138–1147.</rawString>
</citation>
<citation valid="true">
<authors>
<author>Zhiyuan Liu</author>
<author>Wenyi Huang</author>
<author>Yabin Zheng</author>
<author>Maosong Sun</author>
</authors>
<title>Automatic keyphrase extraction via topic decomposition.</title>
<date>2010</date>
<booktitle>In Proceedings of EMNLP,</booktitle>
<pages>366--376</pages>
<marker>Liu, Huang, Zheng, Sun, 2010</marker>
<rawString>Zhiyuan Liu, Wenyi Huang, Yabin Zheng, and Maosong Sun. 2010. Automatic keyphrase extraction via topic decomposition. In Proceedings of EMNLP, pages 366–376.</rawString>
</citation>
<citation valid="true">
<authors>
<author>Zhiyuan Liu</author>
<author>Xinxiong Chen</author>
<author>Maosong Sun</author>
</authors>
<title>A simple word trigger method for social tag suggestion.</title>
<date>2011</date>
<booktitle>In Proceedings of EMNLP,</booktitle>
<pages>1577--1588</pages>
<contexts>
<context position="30691" citStr="Liu et al., 2011" startWordPosition="5072" endWordPosition="5075"> proposed in (Rumelhart et al., 1986) and have been successfully used in language models (Bengio et al., 2006; Mnih and Hinton, 2008) and many natural language processing tasks, such as word representation learning (Mikolov, 2012), named entity recognition (Turian et al., 2010), disambiguation (Collobert et al., 2011), parsing and tagging (Socher et al., 2011; Socher et al., 2013). They are very useful in NLP tasks because they can be used as inputs to learning algorithms or as extra word features in NLP systems. Hence, many NLP applications, such as keyword extraction (Li1032 u et al., 2010; Liu et al., 2011b; Liu et al., 2012), social tag suggestion (Liu et al., 2011a) and text classification (Baker and McCallum, 1998), may also potentially benefit from distributed word representation. The main advantage is that the representations of similar words are close in vector space, which makes generalization to novel patterns easier and model estimation more robust. Word representations are hard to train due to the computational complexity. Recently, (Mikolov et al., 2013) proposed two particular models, Skipgram and CBOW, to learn word representations in large amounts of text data. The training object</context>
</contexts>
<marker>Liu, Chen, Sun, 2011</marker>
<rawString>Zhiyuan Liu, Xinxiong Chen, and Maosong Sun. 2011a. A simple word trigger method for social tag suggestion. In Proceedings of EMNLP, pages 1577– 1588.</rawString>
</citation>
<citation valid="true">
<authors>
<author>Zhiyuan Liu</author>
<author>Xinxiong Chen</author>
<author>Yabin Zheng</author>
<author>Maosong Sun</author>
</authors>
<title>Automatic keyphrase extraction by bridging vocabulary gap.</title>
<date>2011</date>
<booktitle>In Proceedings of CoNLL,</booktitle>
<pages>135--144</pages>
<contexts>
<context position="30691" citStr="Liu et al., 2011" startWordPosition="5072" endWordPosition="5075"> proposed in (Rumelhart et al., 1986) and have been successfully used in language models (Bengio et al., 2006; Mnih and Hinton, 2008) and many natural language processing tasks, such as word representation learning (Mikolov, 2012), named entity recognition (Turian et al., 2010), disambiguation (Collobert et al., 2011), parsing and tagging (Socher et al., 2011; Socher et al., 2013). They are very useful in NLP tasks because they can be used as inputs to learning algorithms or as extra word features in NLP systems. Hence, many NLP applications, such as keyword extraction (Li1032 u et al., 2010; Liu et al., 2011b; Liu et al., 2012), social tag suggestion (Liu et al., 2011a) and text classification (Baker and McCallum, 1998), may also potentially benefit from distributed word representation. The main advantage is that the representations of similar words are close in vector space, which makes generalization to novel patterns easier and model estimation more robust. Word representations are hard to train due to the computational complexity. Recently, (Mikolov et al., 2013) proposed two particular models, Skipgram and CBOW, to learn word representations in large amounts of text data. The training object</context>
</contexts>
<marker>Liu, Chen, Zheng, Sun, 2011</marker>
<rawString>Zhiyuan Liu, Xinxiong Chen, Yabin Zheng, and Maosong Sun. 2011b. Automatic keyphrase extraction by bridging vocabulary gap. In Proceedings of CoNLL, pages 135–144.</rawString>
</citation>
<citation valid="true">
<authors>
<author>Zhiyuan Liu</author>
<author>Xinxiong Chen</author>
<author>Maosong Sun</author>
</authors>
<title>Mining the interests of chinese microbloggers via keyword extraction.</title>
<date>2012</date>
<journal>Frontiers of Computer Science,</journal>
<volume>6</volume>
<issue>1</issue>
<contexts>
<context position="30711" citStr="Liu et al., 2012" startWordPosition="5076" endWordPosition="5079">hart et al., 1986) and have been successfully used in language models (Bengio et al., 2006; Mnih and Hinton, 2008) and many natural language processing tasks, such as word representation learning (Mikolov, 2012), named entity recognition (Turian et al., 2010), disambiguation (Collobert et al., 2011), parsing and tagging (Socher et al., 2011; Socher et al., 2013). They are very useful in NLP tasks because they can be used as inputs to learning algorithms or as extra word features in NLP systems. Hence, many NLP applications, such as keyword extraction (Li1032 u et al., 2010; Liu et al., 2011b; Liu et al., 2012), social tag suggestion (Liu et al., 2011a) and text classification (Baker and McCallum, 1998), may also potentially benefit from distributed word representation. The main advantage is that the representations of similar words are close in vector space, which makes generalization to novel patterns easier and model estimation more robust. Word representations are hard to train due to the computational complexity. Recently, (Mikolov et al., 2013) proposed two particular models, Skipgram and CBOW, to learn word representations in large amounts of text data. The training objective of the CBOW mode</context>
</contexts>
<marker>Liu, Chen, Sun, 2012</marker>
<rawString>Zhiyuan Liu, Xinxiong Chen, and Maosong Sun. 2012. Mining the interests of chinese microbloggers via keyword extraction. Frontiers of Computer Science, 6(1):76–87.</rawString>
</citation>
<citation valid="true">
<authors>
<author>Christopher D Manning</author>
<author>Prabhakar Raghavan</author>
<author>Hinrich Sch¨utze</author>
</authors>
<title>Introduction to information retrieval.</title>
<date>2008</date>
<publisher>Cambridge University Press Cambridge.</publisher>
<marker>Manning, Raghavan, Sch¨utze, 2008</marker>
<rawString>Christopher D Manning, Prabhakar Raghavan, and Hinrich Sch¨utze. 2008. Introduction to information retrieval. Cambridge University Press Cambridge.</rawString>
</citation>
<citation valid="true">
<authors>
<author>Tomas Mikolov</author>
<author>Kai Chen</author>
<author>Greg Corrado</author>
<author>Jeffrey Dean</author>
</authors>
<title>Efficient estimation of word representations in vector space.</title>
<date>2013</date>
<booktitle>Proceedings of ICLR.</booktitle>
<contexts>
<context position="5297" citStr="Mikolov et al., 2013" startWordPosition="808" endWordPosition="811">odel for both WSR and WSD. We evaluate the performance of WSR using a contextual word similarity task, and results show that out model can significantly improve the correlation with human judgments compared to baselines. We further evaluate the performance on both domain-specific WSD and coarse-grained allwords WSD, and results show that our model yields performance competitive with state-of-theart supervised approaches. 2 Methodology We describe our method as a 3-stage process: 1. Initializing word vectors and sense vectors. Given large amounts of text data, we first use the Skip-gram model (Mikolov et al., 2013), a neural network based language model, to learn word vectors. Then, we assign vector representations for senses based on their definitions (e.g, glosses in WordNet). 2. Performing word sense disambiguation. Given word vectors and sense vectors, we propose two simple and efficient WSD algorithms to obtain more relevant occurrences for each sense. 3. Learning sense vectors from relevant occurrences. Based on the relevant occurrences of ambiguous words, we modify the training objective of Skip-gram to learn word vectors and sense vectors jointly. Then, we obtain the sense vectors directly from </context>
<context position="7382" citStr="Mikolov et al., 2013" startWordPosition="1164" endWordPosition="1167">y of the texts is denoted as W. For a word w in W, wsi is the ith sense in WordNet WN. Each sense wsi has a gloss gloss(wsi) in WN. The word embedding of w is denoted as vec(w), and the sense embedding of its ith sense wsi is denoted as vec(wsi). 2.2 Initializing Word Vectors and Sense Vectors Initializing word vectors. First, we use Skipgram to train the word vectors from large amounts of text data. We choose Skip-gram for its simplicity and effectiveness. The training objective of Skip-gram is to train word vector representations that are good at predicting its context in the same sentence (Mikolov et al., 2013). More formally, given a sequence of training words w1, w2, w3,...,wT, the objective of Skipgram is to maximize the average log probability �log p(wt+j|wt) (1) where k is the size of the training window. The inner summation spans from −k to k to compute ∑ −k&lt;j&lt;k,jO0 1 T T ∑ t=1 1026 Sense Synset Gloss banks1 bank (sloping land (especially the slope beside a body of water)) “they pulled the canoe up on the bank”; “he sat on the bank of the river and watched the currents” banks2 depository institution, (a financial institution that accepts deposits and channels the bank, money into lending activ</context>
<context position="31159" citStr="Mikolov et al., 2013" startWordPosition="5144" endWordPosition="5147">ng algorithms or as extra word features in NLP systems. Hence, many NLP applications, such as keyword extraction (Li1032 u et al., 2010; Liu et al., 2011b; Liu et al., 2012), social tag suggestion (Liu et al., 2011a) and text classification (Baker and McCallum, 1998), may also potentially benefit from distributed word representation. The main advantage is that the representations of similar words are close in vector space, which makes generalization to novel patterns easier and model estimation more robust. Word representations are hard to train due to the computational complexity. Recently, (Mikolov et al., 2013) proposed two particular models, Skipgram and CBOW, to learn word representations in large amounts of text data. The training objective of the CBOW model is to combine the representations of the surrounding words to predict the word in the middle, while the Skip-gram model’s is to learn word representations that are good at predicting its context in the same sentence (Mikolov et al., 2013). Our paper uses the model architecture of Skip-gram. Most of the previous vector-space models use one representation per word. This is problematic because many words have multiple senses. The multi-prototype</context>
<context position="33804" citStr="Mikolov et al., 2013" startWordPosition="5548" endWordPosition="5551">tic information contained in such resources is typically insufficient for high-performance WSD. Much work has been presented to automatically extend existing resources, including automatically linking Wikipedia to WordNet to include full use of the first WordNet sense heuristic (Suchanek et al., 2008), a graph-based mapping of Wikipedia categories to WordNet synsets (Ponzetto and Navigli, 2009), and automatically mapping Wikipedia pages to WordNet synsets (Ponzetto and Navigli, 2010). It was recently shown that word representations can capture semantic and syntactic information between words (Mikolov et al., 2013). Some researchers tried to incorporate WordNet senses in a neural model to learn better word representations (Bordes et al., 2011). In this paper, we have proposed a unified method for word sense representation and disambiguation to extend the information contained in the vector representations to the existing resources. Our method only requires a large amount of unlabeled text to train sense representations and a dictionary to provide the definitions of word meanings, which makes it easily applicable to other resources. 5 Conclusion In this paper, we present a unified model for word sense re</context>
</contexts>
<marker>Mikolov, Chen, Corrado, Dean, 2013</marker>
<rawString>Tomas Mikolov, Kai Chen, Greg Corrado, and Jeffrey Dean. 2013. Efficient estimation of word representations in vector space. Proceedings of ICLR.</rawString>
</citation>
<citation valid="true">
<authors>
<author>Tomas Mikolov</author>
</authors>
<title>Statistical Language Models Based on Neural Networks.</title>
<date>2012</date>
<tech>Ph.D. thesis, Ph. D. thesis,</tech>
<institution>Brno University of Technology.</institution>
<contexts>
<context position="30305" citStr="Mikolov, 2012" startWordPosition="5005" endWordPosition="5006">gin threshold ε ranges from 0.0 to 0.3. When ε = 0.0, we use every disambiguation result to update the context vector. When ε =� 0, we only use the confident disambiguation results to update the context vector if the score margin is larger than ε. Our model achieves the best performance when ε = 0.1. 4 Related Work 4.1 Word Representations Distributed representations for words were proposed in (Rumelhart et al., 1986) and have been successfully used in language models (Bengio et al., 2006; Mnih and Hinton, 2008) and many natural language processing tasks, such as word representation learning (Mikolov, 2012), named entity recognition (Turian et al., 2010), disambiguation (Collobert et al., 2011), parsing and tagging (Socher et al., 2011; Socher et al., 2013). They are very useful in NLP tasks because they can be used as inputs to learning algorithms or as extra word features in NLP systems. Hence, many NLP applications, such as keyword extraction (Li1032 u et al., 2010; Liu et al., 2011b; Liu et al., 2012), social tag suggestion (Liu et al., 2011a) and text classification (Baker and McCallum, 1998), may also potentially benefit from distributed word representation. The main advantage is that the </context>
</contexts>
<marker>Mikolov, 2012</marker>
<rawString>Tomas Mikolov. 2012. Statistical Language Models Based on Neural Networks. Ph.D. thesis, Ph. D. thesis, Brno University of Technology.</rawString>
</citation>
<citation valid="true">
<authors>
<author>George A Miller</author>
<author>Claudia Leacock</author>
<author>Randee Tengi</author>
<author>Ross T Bunker</author>
</authors>
<title>A semantic concordance.</title>
<date>1993</date>
<booktitle>In Proceedings of the workshop on HLT,</booktitle>
<pages>303--308</pages>
<contexts>
<context position="23674" citStr="Miller et al., 1993" startWordPosition="3864" endWordPosition="3867">senses. k-NN is a widely used classification method, where neighbors are the k labeled examples most 7We compare only with those systems performing tokenbased WSD, i.e., disambiguating each instance of a target word separately. M N 1 AvgSimC(w,w&apos;) = ∑ ∑ 1030 Algorithm Sports Finance Recall Recall Random BL 19.5 19.6 MFS BL 19.6 37.1 k-NN 30.3 43.4 Static PR 20.1 39.6 Personalized PR 35.6 46.9 Degree 42.0 47.8 Our Model 57.3 60.6 Table 4: Performance on the Sports and Finance sections of the dataset from (Koeling et al., 2005). similar to the test example. The k-NN system is trained on SemCor (Miller et al., 1993), the largest publicly available annotated corpus. Degree and Personalized PageRank are stateof-the-art systems that exploit WordNet to build a semantic graph and exploit the structural properties of the graph in order to choose the appropriate senses of words in context. Results and discussion. Similar to other work on this dataset, we use recall (the ratio of correct sense labels to the total labels in the gold standard) as our evaluation measure. Table 4 shows the results of different WSD systems on the dataset, and the best results are shown in bold. The differences between other results a</context>
</contexts>
<marker>Miller, Leacock, Tengi, Bunker, 1993</marker>
<rawString>George A Miller, Claudia Leacock, Randee Tengi, and Ross T Bunker. 1993. A semantic concordance. In Proceedings of the workshop on HLT, pages 303– 308.</rawString>
</citation>
<citation valid="true">
<authors>
<author>George A Miller</author>
</authors>
<title>Wordnet: a lexical database for english.</title>
<date>1995</date>
<journal>Communications of the ACM,</journal>
<volume>38</volume>
<issue>11</issue>
<contexts>
<context position="3244" citStr="Miller, 1995" startWordPosition="487" endWordPosition="488">sed models to determine the number of clusters. Huang et al. (2012) simply cluster word contexts into static K clusters for each word, which is arbitrary and may introduce mistakes. • These cluster-based models are typically offline , so they cannot be efficiently adapted to new senses, new words or new data. • It is also troublesome to find the sense that a word prototype corresponds to; thus, these cluster-based models cannot be directly used to perform word sense disambiguation. In reality, many large knowledge bases have been constructed with word senses available online, such as WordNet (Miller, 1995) and Wikipedia. Utilizing these knowledge bases to learn word representation and sense representation is a natural choice. In this paper, we present a unified model for both word sense representation and disambiguation based on these knowledge bases and large-scale text corpora. The unified model 1025 Proceedings of the 2014 Conference on Empirical Methods in Natural Language Processing (EMNLP), pages 1025–1035, October 25-29, 2014, Doha, Qatar. c�2014 Association for Computational Linguistics can (1) perform word sense disambiguation based on vector representations, and (2) learn continuous d</context>
<context position="6328" citStr="Miller, 1995" startWordPosition="974" endWordPosition="975">levant occurrences of ambiguous words, we modify the training objective of Skip-gram to learn word vectors and sense vectors jointly. Then, we obtain the sense vectors directly from the model. Before illustrating the three stages of our method in Sections 2.2, 2.3 and 2.4, we briefly introduce our sense inventory, WordNet, in Section 2.1. Note that, although our experiments will use the WordNet sense inventory, our model is not limited to this particular lexicon. Other knowledge bases containing word sense distinctions and definitions can also serve as input to our model. 2.1 WordNet WordNet (Miller, 1995) is the most widely used computational lexicon of English where a concept is represented as a synonym set, or synset. The words in the same synset share a common meaning. Each synset has a textual definition, or gloss. Table 1 shows the synsets and the corresponding glosses of the two common senses of bank. Before introducing the method in detail, we introduce the notations. The unlabeled texts are denoted as R, and the vocabulary of the texts is denoted as W. For a word w in W, wsi is the ith sense in WordNet WN. Each sense wsi has a gloss gloss(wsi) in WN. The word embedding of w is denoted </context>
</contexts>
<marker>Miller, 1995</marker>
<rawString>George A Miller. 1995. Wordnet: a lexical database for english. Communications of the ACM, 38(11):39–41.</rawString>
</citation>
<citation valid="true">
<authors>
<author>Andriy Mnih</author>
<author>Geoffrey E Hinton</author>
</authors>
<title>A scalable hierarchical distributed language model.</title>
<date>2008</date>
<booktitle>In Proceedings of NIPS,</booktitle>
<pages>1081--1088</pages>
<contexts>
<context position="30208" citStr="Mnih and Hinton, 2008" startWordPosition="4988" endWordPosition="4991">se-grained all-words WSD when the score margin threshold ε ranges from 0.0 to 0.3. WSD when the score margin threshold ε ranges from 0.0 to 0.3. When ε = 0.0, we use every disambiguation result to update the context vector. When ε =� 0, we only use the confident disambiguation results to update the context vector if the score margin is larger than ε. Our model achieves the best performance when ε = 0.1. 4 Related Work 4.1 Word Representations Distributed representations for words were proposed in (Rumelhart et al., 1986) and have been successfully used in language models (Bengio et al., 2006; Mnih and Hinton, 2008) and many natural language processing tasks, such as word representation learning (Mikolov, 2012), named entity recognition (Turian et al., 2010), disambiguation (Collobert et al., 2011), parsing and tagging (Socher et al., 2011; Socher et al., 2013). They are very useful in NLP tasks because they can be used as inputs to learning algorithms or as extra word features in NLP systems. Hence, many NLP applications, such as keyword extraction (Li1032 u et al., 2010; Liu et al., 2011b; Liu et al., 2012), social tag suggestion (Liu et al., 2011a) and text classification (Baker and McCallum, 1998), m</context>
</contexts>
<marker>Mnih, Hinton, 2008</marker>
<rawString>Andriy Mnih and Geoffrey E Hinton. 2008. A scalable hierarchical distributed language model. In Proceedings of NIPS, pages 1081–1088.</rawString>
</citation>
<citation valid="true">
<authors>
<author>Frederic Morin</author>
<author>Yoshua Bengio</author>
</authors>
<title>Hierarchical probabilistic neural network language model.</title>
<date>2005</date>
<booktitle>In Proceedings of the international workshop on artificial intelligence and statistics,</booktitle>
<pages>246--252</pages>
<contexts>
<context position="15960" citStr="Morin and Bengio, 2005" startWordPosition="2628" endWordPosition="2631"> probability of correctly predicting bank 1 T T ∑ t=1 k ∑ j=−k 1028 the sense M(wt+j) given the word in the middle wt. The outer summation covers all words in the training data. Because not all of the disambiguation results are correct, we only disambiguate the words that we are confident in. Similar to step 3 of our WSD algorithm, we only disambiguate words under the condition that the score margin between the maximum and the second maximum is larger than the score margin threshold, ε. We also use the softmax function to define p(wt+jJwt) and p(M(wt+j)Jwt). Then, we use hierarchical softmax (Morin and Bengio, 2005) to greatly reduce the computational complexity and learn the sense vectors directly from the relevant occurrences. 3 Experiments In this section, we first present the nearest neighbors of some words and their senses, showing that our sense vectors can capture the semantics of words. Then, we use three tasks to evaluate our unified model: a contextual word similarity task to evaluate our sense representations, and two standard WSD tasks to evaluate our knowledge-based WSD algorithm based on the sense vectors. Experimental results show that our model not only improves the correlation with human</context>
</contexts>
<marker>Morin, Bengio, 2005</marker>
<rawString>Frederic Morin and Yoshua Bengio. 2005. Hierarchical probabilistic neural network language model. In Proceedings of the international workshop on artificial intelligence and statistics, pages 246–252.</rawString>
</citation>
<citation valid="true">
<authors>
<author>Roberto Navigli</author>
<author>Mirella Lapata</author>
</authors>
<title>An experimental study of graph connectivity for unsupervised word sense disambiguation.</title>
<date>2010</date>
<journal>IEEE PAMI,</journal>
<volume>32</volume>
<issue>4</issue>
<contexts>
<context position="22868" citStr="Navigli and Lapata, 2010" startWordPosition="3731" endWordPosition="3734">examples for each word were annotated with senses from WordNet v.1.7 by three reviewers, yielding an inter-tagger agreement of 65%. (Koeling et al., 2005) did not clarify how to select the “correct” sense for each word, so we followed the work of (Agirre et al., 2009) and, used the sense chosen by the majority of taggers as the correct answer. Baseline methods. As a baseline, we use the most frequent WordNet sense (MFS), as well as a random sense assignment. We also compare our results with four systems 7: Static PageRank (Agirre et al., 2009), the k nearest neighbor algorithm (k-NN), Degree (Navigli and Lapata, 2010) and Personalized PageRank (Agirre et al., 2009). Static PageRank applies traditional PageRank over the semantic graph based on WordNet and obtains a context-independent ranking of word senses. k-NN is a widely used classification method, where neighbors are the k labeled examples most 7We compare only with those systems performing tokenbased WSD, i.e., disambiguating each instance of a target word separately. M N 1 AvgSimC(w,w&apos;) = ∑ ∑ 1030 Algorithm Sports Finance Recall Recall Random BL 19.5 19.6 MFS BL 19.6 37.1 k-NN 30.3 43.4 Static PR 20.1 39.6 Personalized PR 35.6 46.9 Degree 42.0 47.8 O</context>
<context position="26662" citStr="Navigli and Lapata, 2010" startWordPosition="4360" endWordPosition="4363">rained setting: first, it has been argued that the fine granularity of WordNet is one of the main obstacles to accurate WSD (cf. the discussion in (Navigli, 2009)); second, the training corpus of word representations is Wikipedia, which is quite different from WordNet. Baseline methods. We compare our model with the best unsupervised system SUSSX-FR (Koeling and McCarthy, 2007), and the best supervised system, NUS-PT (Chan et al., 2007), participating in the Semeval-2007 coarse-grained all-words task. We also compare with SSI (Navigli and Velardi, 2005) and the state-of-the-art system Degree (Navigli and Lapata, 2010). We use different baseline methods for the two WSD tasks because we want to compare our model with the stateof-the-art systems that are applicable to different datasets and show that our WSD method can perform robustly in these different WSD tasks. 1031 Results and discussion. We report our results in terms of F1-measure on the Semeval-2007 coarsegrained all-words dataset (Navigli et al., 2007). Table 5 reports the results for nouns (1,108 words) and all words (2,269 words). The difference between unsupervised and semi-supervised methods is whether the method uses MFS as a back-off strategy. </context>
</contexts>
<marker>Navigli, Lapata, 2010</marker>
<rawString>Roberto Navigli and Mirella Lapata. 2010. An experimental study of graph connectivity for unsupervised word sense disambiguation. IEEE PAMI, 32(4):678–692.</rawString>
</citation>
<citation valid="true">
<authors>
<author>Roberto Navigli</author>
<author>Paola Velardi</author>
</authors>
<title>Structural semantic interconnections: a knowledge-based approach to word sense disambiguation.</title>
<date>2005</date>
<journal>IEEE PAMI,</journal>
<volume>27</volume>
<issue>7</issue>
<contexts>
<context position="26596" citStr="Navigli and Velardi, 2005" startWordPosition="4349" endWordPosition="4353">here are multiple reasons that we perform experiments in a coarse-grained setting: first, it has been argued that the fine granularity of WordNet is one of the main obstacles to accurate WSD (cf. the discussion in (Navigli, 2009)); second, the training corpus of word representations is Wikipedia, which is quite different from WordNet. Baseline methods. We compare our model with the best unsupervised system SUSSX-FR (Koeling and McCarthy, 2007), and the best supervised system, NUS-PT (Chan et al., 2007), participating in the Semeval-2007 coarse-grained all-words task. We also compare with SSI (Navigli and Velardi, 2005) and the state-of-the-art system Degree (Navigli and Lapata, 2010). We use different baseline methods for the two WSD tasks because we want to compare our model with the stateof-the-art systems that are applicable to different datasets and show that our WSD method can perform robustly in these different WSD tasks. 1031 Results and discussion. We report our results in terms of F1-measure on the Semeval-2007 coarsegrained all-words dataset (Navigli et al., 2007). Table 5 reports the results for nouns (1,108 words) and all words (2,269 words). The difference between unsupervised and semi-supervis</context>
</contexts>
<marker>Navigli, Velardi, 2005</marker>
<rawString>Roberto Navigli and Paola Velardi. 2005. Structural semantic interconnections: a knowledge-based approach to word sense disambiguation. IEEE PAMI, 27(7):1075–1086.</rawString>
</citation>
<citation valid="true">
<authors>
<author>Roberto Navigli</author>
<author>Kenneth C Litkowski</author>
<author>Orin Hargraves</author>
</authors>
<title>Semeval-2007 task 07: Coarsegrained english all-words task.</title>
<date>2007</date>
<booktitle>In Proceedings of SemEval,</booktitle>
<pages>30--35</pages>
<contexts>
<context position="25967" citStr="Navigli et al., 2007" startWordPosition="4250" endWordPosition="4253">r ModelS2C U 81.6 75.8 Our ModelL2R Semi 82.5 79.6 Our ModelS2C Semi 85.3 82.6 Table 5: Performance on Semeval-2007 coarsegrained all-words WSD. In the type column, U, Semi and S stand for unsupervised, semisupervised and supervised, respectively. The differences between the results in bold in each column of the table are not statistically significant at p &lt; 0.05. methods rely greatly on high-quality semantic relations or annotated data, which are hard to acquire. 3.4 Coarse-grained WSD Experimental setting. We also evaluate our WSD model on the Semeval-2007 coarse-grained all-words WSD task (Navigli et al., 2007). There are multiple reasons that we perform experiments in a coarse-grained setting: first, it has been argued that the fine granularity of WordNet is one of the main obstacles to accurate WSD (cf. the discussion in (Navigli, 2009)); second, the training corpus of word representations is Wikipedia, which is quite different from WordNet. Baseline methods. We compare our model with the best unsupervised system SUSSX-FR (Koeling and McCarthy, 2007), and the best supervised system, NUS-PT (Chan et al., 2007), participating in the Semeval-2007 coarse-grained all-words task. We also compare with SS</context>
</contexts>
<marker>Navigli, Litkowski, Hargraves, 2007</marker>
<rawString>Roberto Navigli, Kenneth C Litkowski, and Orin Hargraves. 2007. Semeval-2007 task 07: Coarsegrained english all-words task. In Proceedings of SemEval, pages 30–35.</rawString>
</citation>
<citation valid="true">
<authors>
<author>Roberto Navigli</author>
</authors>
<title>Word sense disambiguation: A survey.</title>
<date>2009</date>
<journal>CSUR,</journal>
<volume>41</volume>
<issue>2</issue>
<contexts>
<context position="26199" citStr="Navigli, 2009" startWordPosition="4291" endWordPosition="4292">ctively. The differences between the results in bold in each column of the table are not statistically significant at p &lt; 0.05. methods rely greatly on high-quality semantic relations or annotated data, which are hard to acquire. 3.4 Coarse-grained WSD Experimental setting. We also evaluate our WSD model on the Semeval-2007 coarse-grained all-words WSD task (Navigli et al., 2007). There are multiple reasons that we perform experiments in a coarse-grained setting: first, it has been argued that the fine granularity of WordNet is one of the main obstacles to accurate WSD (cf. the discussion in (Navigli, 2009)); second, the training corpus of word representations is Wikipedia, which is quite different from WordNet. Baseline methods. We compare our model with the best unsupervised system SUSSX-FR (Koeling and McCarthy, 2007), and the best supervised system, NUS-PT (Chan et al., 2007), participating in the Semeval-2007 coarse-grained all-words task. We also compare with SSI (Navigli and Velardi, 2005) and the state-of-the-art system Degree (Navigli and Lapata, 2010). We use different baseline methods for the two WSD tasks because we want to compare our model with the stateof-the-art systems that are </context>
<context position="32623" citStr="Navigli, 2009" startWordPosition="5373" endWordPosition="5374">tering of contexts as a word sense and can not be directly used in word sense disambiguation. After our paper was submitted, we perceive the following recent advances: (Tian et al., 2014) proposed a probabilistic model for multi-prototype word representation. (Guo et al., 2014) explored bilingual resources to learn sense-specific word representation. (Neelakantan et al., 2014) proposed an efficient non-parametric model for multiprototype word representation. 4.2 Knowledge-based WSD The objective of word sense disambiguation (WSD) is to computationally identify the meaning of words in context (Navigli, 2009). There are two approaches of WSD that assign meaning of words from a fixed sense inventory, supervised and knowledge-based methods. Supervised approaches require large labeled training sets, which are time consuming to create. In this paper, we only focus on knowledge-based word sense disambiguation. Knowledge-based approaches exploit knowledge resources (such as dictionaries, thesauri, ontologies, collocations, etc.) to determine the senses of words in context. However, it has been shown in (Cuadros and Rigau, 2006) that the amount of lexical and semantic information contained in such resour</context>
</contexts>
<marker>Navigli, 2009</marker>
<rawString>Roberto Navigli. 2009. Word sense disambiguation: A survey. CSUR, 41(2):10.</rawString>
</citation>
<citation valid="true">
<authors>
<author>Arvind Neelakantan</author>
<author>Jeevan Shankar</author>
<author>Alexandre Passos</author>
<author>Andrew McCallum</author>
</authors>
<title>Efficient nonparametric estimation of multiple embeddings per word in vector space.</title>
<date>2014</date>
<booktitle>In Proceedings of EMNLP.</booktitle>
<contexts>
<context position="32388" citStr="Neelakantan et al., 2014" startWordPosition="5337" endWordPosition="5340">roach has been widely studied. (Reisinger and Mooney, 2010) proposed the multi-prototype vector-space model. (Huang et al., 2012) used the multi-prototype models to learn the vector for different senses of a word. All of these models use the clustering of contexts as a word sense and can not be directly used in word sense disambiguation. After our paper was submitted, we perceive the following recent advances: (Tian et al., 2014) proposed a probabilistic model for multi-prototype word representation. (Guo et al., 2014) explored bilingual resources to learn sense-specific word representation. (Neelakantan et al., 2014) proposed an efficient non-parametric model for multiprototype word representation. 4.2 Knowledge-based WSD The objective of word sense disambiguation (WSD) is to computationally identify the meaning of words in context (Navigli, 2009). There are two approaches of WSD that assign meaning of words from a fixed sense inventory, supervised and knowledge-based methods. Supervised approaches require large labeled training sets, which are time consuming to create. In this paper, we only focus on knowledge-based word sense disambiguation. Knowledge-based approaches exploit knowledge resources (such a</context>
</contexts>
<marker>Neelakantan, Shankar, Passos, McCallum, 2014</marker>
<rawString>Arvind Neelakantan, Jeevan Shankar, Alexandre Passos, and Andrew McCallum. 2014. Efficient nonparametric estimation of multiple embeddings per word in vector space. In Proceedings of EMNLP.</rawString>
</citation>
<citation valid="true">
<authors>
<author>Simone Paolo Ponzetto</author>
<author>Roberto Navigli</author>
</authors>
<title>Large-scale taxonomy mapping for restructuring and integrating wikipedia.</title>
<date>2009</date>
<booktitle>In Proceedings of IJCAI,</booktitle>
<volume>9</volume>
<pages>2083--2088</pages>
<contexts>
<context position="33580" citStr="Ponzetto and Navigli, 2009" startWordPosition="5515" endWordPosition="5519">exploit knowledge resources (such as dictionaries, thesauri, ontologies, collocations, etc.) to determine the senses of words in context. However, it has been shown in (Cuadros and Rigau, 2006) that the amount of lexical and semantic information contained in such resources is typically insufficient for high-performance WSD. Much work has been presented to automatically extend existing resources, including automatically linking Wikipedia to WordNet to include full use of the first WordNet sense heuristic (Suchanek et al., 2008), a graph-based mapping of Wikipedia categories to WordNet synsets (Ponzetto and Navigli, 2009), and automatically mapping Wikipedia pages to WordNet synsets (Ponzetto and Navigli, 2010). It was recently shown that word representations can capture semantic and syntactic information between words (Mikolov et al., 2013). Some researchers tried to incorporate WordNet senses in a neural model to learn better word representations (Bordes et al., 2011). In this paper, we have proposed a unified method for word sense representation and disambiguation to extend the information contained in the vector representations to the existing resources. Our method only requires a large amount of unlabeled</context>
</contexts>
<marker>Ponzetto, Navigli, 2009</marker>
<rawString>Simone Paolo Ponzetto and Roberto Navigli. 2009. Large-scale taxonomy mapping for restructuring and integrating wikipedia. In Proceedings of IJCAI, volume 9, pages 2083–2088.</rawString>
</citation>
<citation valid="true">
<authors>
<author>Simone Paolo Ponzetto</author>
<author>Roberto Navigli</author>
</authors>
<title>Knowledge-rich word sense disambiguation rivaling supervised systems.</title>
<date>2010</date>
<booktitle>In Proceedings of ACL,</booktitle>
<pages>1522--1531</pages>
<contexts>
<context position="33671" citStr="Ponzetto and Navigli, 2010" startWordPosition="5528" endWordPosition="5531">) to determine the senses of words in context. However, it has been shown in (Cuadros and Rigau, 2006) that the amount of lexical and semantic information contained in such resources is typically insufficient for high-performance WSD. Much work has been presented to automatically extend existing resources, including automatically linking Wikipedia to WordNet to include full use of the first WordNet sense heuristic (Suchanek et al., 2008), a graph-based mapping of Wikipedia categories to WordNet synsets (Ponzetto and Navigli, 2009), and automatically mapping Wikipedia pages to WordNet synsets (Ponzetto and Navigli, 2010). It was recently shown that word representations can capture semantic and syntactic information between words (Mikolov et al., 2013). Some researchers tried to incorporate WordNet senses in a neural model to learn better word representations (Bordes et al., 2011). In this paper, we have proposed a unified method for word sense representation and disambiguation to extend the information contained in the vector representations to the existing resources. Our method only requires a large amount of unlabeled text to train sense representations and a dictionary to provide the definitions of word me</context>
</contexts>
<marker>Ponzetto, Navigli, 2010</marker>
<rawString>Simone Paolo Ponzetto and Roberto Navigli. 2010. Knowledge-rich word sense disambiguation rivaling supervised systems. In Proceedings of ACL, pages 1522–1531.</rawString>
</citation>
<citation valid="true">
<authors>
<author>Joseph Reisinger</author>
<author>Raymond J Mooney</author>
</authors>
<title>Multi-prototype vector-space models of word meaning.</title>
<date>2010</date>
<booktitle>In Proceedings of HLT-NAACL,</booktitle>
<pages>109--117</pages>
<contexts>
<context position="2043" citStr="Reisinger and Mooney (2010)" startWordPosition="295" endWordPosition="298">aims to build vectors for each word based on its context in a large corpus, usually capturing both semantic and syntactic information of words. These representations can be used as features or inputs, which are widely employed in information retrieval (Manning et al., 2008), document classification (Sebastiani, 2002) and other NLP tasks. 1Our sense representations can be downloaded at http: //pan.baidu.com/s/1eQcPK8i. Most word representation methods assume each word owns a single vector. However, this is usually problematic due to the homonymy and polysemy of many words. To remedy the issue, Reisinger and Mooney (2010) proposed a multi-prototype vector space model, where the contexts of each word are first clustered into groups, and then each cluster generates a distinct prototype vector for a word by averaging over all context vectors within the cluster. Huang et al. (2012) followed this idea, but introduced continuous distributed vectors based on probabilistic neural language models for word representations. These cluster-based models conduct unsupervised word sense induction by clustering word contexts and, thus, suffer from the following issues: • It is usually difficult for these cluster-based models t</context>
<context position="31822" citStr="Reisinger and Mooney, 2010" startWordPosition="5251" endWordPosition="5254">pgram and CBOW, to learn word representations in large amounts of text data. The training objective of the CBOW model is to combine the representations of the surrounding words to predict the word in the middle, while the Skip-gram model’s is to learn word representations that are good at predicting its context in the same sentence (Mikolov et al., 2013). Our paper uses the model architecture of Skip-gram. Most of the previous vector-space models use one representation per word. This is problematic because many words have multiple senses. The multi-prototype approach has been widely studied. (Reisinger and Mooney, 2010) proposed the multi-prototype vector-space model. (Huang et al., 2012) used the multi-prototype models to learn the vector for different senses of a word. All of these models use the clustering of contexts as a word sense and can not be directly used in word sense disambiguation. After our paper was submitted, we perceive the following recent advances: (Tian et al., 2014) proposed a probabilistic model for multi-prototype word representation. (Guo et al., 2014) explored bilingual resources to learn sense-specific word representation. (Neelakantan et al., 2014) proposed an efficient non-paramet</context>
</contexts>
<marker>Reisinger, Mooney, 2010</marker>
<rawString>Joseph Reisinger and Raymond J Mooney. 2010. Multi-prototype vector-space models of word meaning. In Proceedings of HLT-NAACL, pages 109– 117.</rawString>
</citation>
<citation valid="true">
<authors>
<author>David E Rumelhart</author>
<author>Geoffrey E Hintont</author>
<author>Ronald J Williams</author>
</authors>
<title>Learning representations by backpropagating errors.</title>
<date>1986</date>
<journal>Nature,</journal>
<volume>323</volume>
<issue>6088</issue>
<contexts>
<context position="30112" citStr="Rumelhart et al., 1986" startWordPosition="4972" endWordPosition="4975"> ε = 0.20 80.9 75.1 ε = 0.25 80.2 74.8 ε = 0.30 80.4 74.9 Table 7: Evaluation results on the coarse-grained all-words WSD when the score margin threshold ε ranges from 0.0 to 0.3. WSD when the score margin threshold ε ranges from 0.0 to 0.3. When ε = 0.0, we use every disambiguation result to update the context vector. When ε =� 0, we only use the confident disambiguation results to update the context vector if the score margin is larger than ε. Our model achieves the best performance when ε = 0.1. 4 Related Work 4.1 Word Representations Distributed representations for words were proposed in (Rumelhart et al., 1986) and have been successfully used in language models (Bengio et al., 2006; Mnih and Hinton, 2008) and many natural language processing tasks, such as word representation learning (Mikolov, 2012), named entity recognition (Turian et al., 2010), disambiguation (Collobert et al., 2011), parsing and tagging (Socher et al., 2011; Socher et al., 2013). They are very useful in NLP tasks because they can be used as inputs to learning algorithms or as extra word features in NLP systems. Hence, many NLP applications, such as keyword extraction (Li1032 u et al., 2010; Liu et al., 2011b; Liu et al., 2012),</context>
</contexts>
<marker>Rumelhart, Hintont, Williams, 1986</marker>
<rawString>David E Rumelhart, Geoffrey E Hintont, and Ronald J Williams. 1986. Learning representations by backpropagating errors. Nature, 323(6088):533–536.</rawString>
</citation>
<citation valid="true">
<authors>
<author>Fabrizio Sebastiani</author>
</authors>
<title>Machine learning in automated text categorization.</title>
<date>2002</date>
<journal>CSUR,</journal>
<volume>34</volume>
<issue>1</issue>
<contexts>
<context position="1734" citStr="Sebastiani, 2002" startWordPosition="249" endWordPosition="250">ental results show that, our model improves the performance of contextual word similarity compared to existing WSR methods, outperforms stateof-the-art supervised methods on domainspecific WSD, and achieves competitive performance on coarse-grained all-words WSD. 1 Introduction Word representation aims to build vectors for each word based on its context in a large corpus, usually capturing both semantic and syntactic information of words. These representations can be used as features or inputs, which are widely employed in information retrieval (Manning et al., 2008), document classification (Sebastiani, 2002) and other NLP tasks. 1Our sense representations can be downloaded at http: //pan.baidu.com/s/1eQcPK8i. Most word representation methods assume each word owns a single vector. However, this is usually problematic due to the homonymy and polysemy of many words. To remedy the issue, Reisinger and Mooney (2010) proposed a multi-prototype vector space model, where the contexts of each word are first clustered into groups, and then each cluster generates a distinct prototype vector for a word by averaging over all context vectors within the cluster. Huang et al. (2012) followed this idea, but intro</context>
</contexts>
<marker>Sebastiani, 2002</marker>
<rawString>Fabrizio Sebastiani. 2002. Machine learning in automated text categorization. CSUR, 34(1):1–47.</rawString>
</citation>
<citation valid="true">
<authors>
<author>Richard Socher</author>
<author>Cliff C Lin</author>
<author>Andrew Ng</author>
<author>Chris Manning</author>
</authors>
<title>Parsing natural scenes and natural language with recursive neural networks.</title>
<date>2011</date>
<booktitle>In Proceedings of ICML,</booktitle>
<pages>129--136</pages>
<contexts>
<context position="30436" citStr="Socher et al., 2011" startWordPosition="5025" endWordPosition="5028">=� 0, we only use the confident disambiguation results to update the context vector if the score margin is larger than ε. Our model achieves the best performance when ε = 0.1. 4 Related Work 4.1 Word Representations Distributed representations for words were proposed in (Rumelhart et al., 1986) and have been successfully used in language models (Bengio et al., 2006; Mnih and Hinton, 2008) and many natural language processing tasks, such as word representation learning (Mikolov, 2012), named entity recognition (Turian et al., 2010), disambiguation (Collobert et al., 2011), parsing and tagging (Socher et al., 2011; Socher et al., 2013). They are very useful in NLP tasks because they can be used as inputs to learning algorithms or as extra word features in NLP systems. Hence, many NLP applications, such as keyword extraction (Li1032 u et al., 2010; Liu et al., 2011b; Liu et al., 2012), social tag suggestion (Liu et al., 2011a) and text classification (Baker and McCallum, 1998), may also potentially benefit from distributed word representation. The main advantage is that the representations of similar words are close in vector space, which makes generalization to novel patterns easier and model estimatio</context>
</contexts>
<marker>Socher, Lin, Ng, Manning, 2011</marker>
<rawString>Richard Socher, Cliff C Lin, Andrew Ng, and Chris Manning. 2011. Parsing natural scenes and natural language with recursive neural networks. In Proceedings of ICML, pages 129–136.</rawString>
</citation>
<citation valid="true">
<authors>
<author>Richard Socher</author>
<author>John Bauer</author>
<author>Christopher D Manning</author>
<author>Andrew Y Ng</author>
</authors>
<title>Parsing with compositional vector grammars.</title>
<date>2013</date>
<booktitle>In Proceedings of ACL.</booktitle>
<contexts>
<context position="30458" citStr="Socher et al., 2013" startWordPosition="5029" endWordPosition="5032"> confident disambiguation results to update the context vector if the score margin is larger than ε. Our model achieves the best performance when ε = 0.1. 4 Related Work 4.1 Word Representations Distributed representations for words were proposed in (Rumelhart et al., 1986) and have been successfully used in language models (Bengio et al., 2006; Mnih and Hinton, 2008) and many natural language processing tasks, such as word representation learning (Mikolov, 2012), named entity recognition (Turian et al., 2010), disambiguation (Collobert et al., 2011), parsing and tagging (Socher et al., 2011; Socher et al., 2013). They are very useful in NLP tasks because they can be used as inputs to learning algorithms or as extra word features in NLP systems. Hence, many NLP applications, such as keyword extraction (Li1032 u et al., 2010; Liu et al., 2011b; Liu et al., 2012), social tag suggestion (Liu et al., 2011a) and text classification (Baker and McCallum, 1998), may also potentially benefit from distributed word representation. The main advantage is that the representations of similar words are close in vector space, which makes generalization to novel patterns easier and model estimation more robust. Word re</context>
</contexts>
<marker>Socher, Bauer, Manning, Ng, 2013</marker>
<rawString>Richard Socher, John Bauer, Christopher D Manning, and Andrew Y Ng. 2013. Parsing with compositional vector grammars. In Proceedings of ACL.</rawString>
</citation>
<citation valid="true">
<authors>
<author>Fabian M Suchanek</author>
<author>Gjergji Kasneci</author>
<author>Gerhard Weikum</author>
</authors>
<title>Yago: A large ontology from wikipedia and wordnet. Web Semantics: Science, Services and Agents on the World Wide Web,</title>
<date>2008</date>
<pages>6--3</pages>
<contexts>
<context position="33485" citStr="Suchanek et al., 2008" startWordPosition="5501" endWordPosition="5504">r, we only focus on knowledge-based word sense disambiguation. Knowledge-based approaches exploit knowledge resources (such as dictionaries, thesauri, ontologies, collocations, etc.) to determine the senses of words in context. However, it has been shown in (Cuadros and Rigau, 2006) that the amount of lexical and semantic information contained in such resources is typically insufficient for high-performance WSD. Much work has been presented to automatically extend existing resources, including automatically linking Wikipedia to WordNet to include full use of the first WordNet sense heuristic (Suchanek et al., 2008), a graph-based mapping of Wikipedia categories to WordNet synsets (Ponzetto and Navigli, 2009), and automatically mapping Wikipedia pages to WordNet synsets (Ponzetto and Navigli, 2010). It was recently shown that word representations can capture semantic and syntactic information between words (Mikolov et al., 2013). Some researchers tried to incorporate WordNet senses in a neural model to learn better word representations (Bordes et al., 2011). In this paper, we have proposed a unified method for word sense representation and disambiguation to extend the information contained in the vector </context>
</contexts>
<marker>Suchanek, Kasneci, Weikum, 2008</marker>
<rawString>Fabian M Suchanek, Gjergji Kasneci, and Gerhard Weikum. 2008. Yago: A large ontology from wikipedia and wordnet. Web Semantics: Science, Services and Agents on the World Wide Web, 6(3):203–217.</rawString>
</citation>
<citation valid="true">
<authors>
<author>Fei Tian</author>
<author>Hanjun Dai</author>
<author>Jiang Bian</author>
<author>Bin Gao</author>
<author>Rui Zhang</author>
<author>Enhong Chen</author>
<author>Tie-Yan Liu</author>
</authors>
<title>A probabilistic model for learning multi-prototype word embeddings.</title>
<date>2014</date>
<booktitle>In Proceedings of COLING,</booktitle>
<pages>151--160</pages>
<contexts>
<context position="32196" citStr="Tian et al., 2014" startWordPosition="5312" endWordPosition="5315">architecture of Skip-gram. Most of the previous vector-space models use one representation per word. This is problematic because many words have multiple senses. The multi-prototype approach has been widely studied. (Reisinger and Mooney, 2010) proposed the multi-prototype vector-space model. (Huang et al., 2012) used the multi-prototype models to learn the vector for different senses of a word. All of these models use the clustering of contexts as a word sense and can not be directly used in word sense disambiguation. After our paper was submitted, we perceive the following recent advances: (Tian et al., 2014) proposed a probabilistic model for multi-prototype word representation. (Guo et al., 2014) explored bilingual resources to learn sense-specific word representation. (Neelakantan et al., 2014) proposed an efficient non-parametric model for multiprototype word representation. 4.2 Knowledge-based WSD The objective of word sense disambiguation (WSD) is to computationally identify the meaning of words in context (Navigli, 2009). There are two approaches of WSD that assign meaning of words from a fixed sense inventory, supervised and knowledge-based methods. Supervised approaches require large labe</context>
</contexts>
<marker>Tian, Dai, Bian, Gao, Zhang, Chen, Liu, 2014</marker>
<rawString>Fei Tian, Hanjun Dai, Jiang Bian, Bin Gao, Rui Zhang, Enhong Chen, and Tie-Yan Liu. 2014. A probabilistic model for learning multi-prototype word embeddings. In Proceedings of COLING, pages 151–160.</rawString>
</citation>
<citation valid="true">
<authors>
<author>Joseph Turian</author>
<author>Lev Ratinov</author>
<author>Yoshua Bengio</author>
</authors>
<title>Word representations: a simple and general method for semi-supervised learning.</title>
<date>2010</date>
<booktitle>In Proceedings of ACL,</booktitle>
<pages>384--394</pages>
<contexts>
<context position="30353" citStr="Turian et al., 2010" startWordPosition="5011" endWordPosition="5014">en ε = 0.0, we use every disambiguation result to update the context vector. When ε =� 0, we only use the confident disambiguation results to update the context vector if the score margin is larger than ε. Our model achieves the best performance when ε = 0.1. 4 Related Work 4.1 Word Representations Distributed representations for words were proposed in (Rumelhart et al., 1986) and have been successfully used in language models (Bengio et al., 2006; Mnih and Hinton, 2008) and many natural language processing tasks, such as word representation learning (Mikolov, 2012), named entity recognition (Turian et al., 2010), disambiguation (Collobert et al., 2011), parsing and tagging (Socher et al., 2011; Socher et al., 2013). They are very useful in NLP tasks because they can be used as inputs to learning algorithms or as extra word features in NLP systems. Hence, many NLP applications, such as keyword extraction (Li1032 u et al., 2010; Liu et al., 2011b; Liu et al., 2012), social tag suggestion (Liu et al., 2011a) and text classification (Baker and McCallum, 1998), may also potentially benefit from distributed word representation. The main advantage is that the representations of similar words are close in ve</context>
</contexts>
<marker>Turian, Ratinov, Bengio, 2010</marker>
<rawString>Joseph Turian, Lev Ratinov, and Yoshua Bengio. 2010. Word representations: a simple and general method for semi-supervised learning. In Proceedings of ACL, pages 384–394.</rawString>
</citation>
</citationList>
</algorithm>
</algorithms>
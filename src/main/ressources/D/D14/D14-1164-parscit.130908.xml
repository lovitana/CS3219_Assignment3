<?xml version="1.0" encoding="UTF-8"?>
<algorithms version="110505">
<algorithm name="SectLabel" version="110505">
<variant no="0" confidence="0.000055">
<title confidence="0.703654">
Combining Distant and Partial Supervision for Relation Extraction
</title>
<author confidence="0.81806">
Gabor Angeli, Julie Tibshirani, Jean Y. Wu, Christopher D. Manning
</author>
<affiliation confidence="0.823555">
Stanford University
</affiliation>
<address confidence="0.693664">
Stanford, CA 94305
</address>
<email confidence="0.948969">
{angeli, jtibs, jeaneis, manning}@stanford.edu
</email>
<sectionHeader confidence="0.993816" genericHeader="abstract">
Abstract
</sectionHeader>
<bodyText confidence="0.999879611111111">
Broad-coverage relation extraction either
requires expensive supervised training
data, or suffers from drawbacks inherent
to distant supervision. We present an ap-
proach for providing partial supervision
to a distantly supervised relation extrac-
tor using a small number of carefully se-
lected examples. We compare against es-
tablished active learning criteria and pro-
pose a novel criterion to sample examples
which are both uncertain and representa-
tive. In this way, we combine the ben-
efits of fine-grained supervision for diffi-
cult examples with the coverage of a large
distantly supervised corpus. Our approach
gives a substantial increase of 3.9% end-
to-end F1 on the 2013 KBP Slot Filling
evaluation, yielding a net F1 of 37.7%.
</bodyText>
<sectionHeader confidence="0.998783" genericHeader="keywords">
1 Introduction
</sectionHeader>
<bodyText confidence="0.999972844827586">
Fully supervised relation extractors are limited to
relatively small training sets. While able to make
use of much more data, distantly supervised ap-
proaches either make dubious assumptions in or-
der to simulate fully supervised data, or make use
of latent-variable methods which get stuck in local
optima easily. We hope to combine the benefits
of supervised and distantly supervised methods by
annotating a small subset of the available data us-
ing selection criteria inspired by active learning.
To illustrate, our training corpus contains
1208 524 relation mentions; annotating all of
these mentions for a fully supervised classifier, at
an average of $0.13 per annotation, would cost ap-
proximately $160000. Distant supervision allows
us to make use of this large corpus without requir-
ing costly annotation. The traditional approach is
based on the assumption that every mention of an
entity pair (e.g., Obama and USA) participates in
the known relation between the two (i.e., born in).
However, this introduces noise, as not every men-
tion expresses the relation we are assigning to it.
We show that by providing annotations for only
10 000 informative examples, combined with a
large corpus of distantly labeled data, we can yield
notable improvements in performance over the
distantly supervised data alone. We report results
on three criteria for selecting examples to anno-
tate: a baseline of sampling examples uniformly
at random, an established active learning criterion,
and a new metric incorporating both the uncer-
tainty and the representativeness of an example.
We show that the choice of metric is important
– yielding as much as a 3% F1 difference – and
that our new proposed criterion outperforms the
standard method in many cases. Lastly, we train
a supervised classifier on these collected exam-
ples, and report performance comparable to dis-
tantly supervised methods. Furthermore, we no-
tice that initializing the distantly supervised model
using this supervised classifier is critical for ob-
taining performance improvements.
This work makes a number of concrete contri-
butions. We propose a novel application of active
learning techniques to distantly supervised rela-
tion extraction. To the best of the authors knowl-
edge, we are the first to apply active learning to the
class of latent-variable distantly supervised mod-
els presented in this paper. We show that anno-
tating a proportionally small number of examples
yields improvements in end-to-end accuracy. We
compare various selection criteria, and show that
this decision has a notable impact on the gain in
performance. In many ways this reconciles our
results with the negative results of Zhang et al.
(2012), who show limited gains from naively an-
notating examples. Lastly, we make our annota-
tions available to the research community.1
</bodyText>
<footnote confidence="0.9818795">
1http://nlp.stanford.edu/software/
mimlre.shtml
</footnote>
<page confidence="0.865917">
1556
</page>
<note confidence="0.970735">
Proceedings of the 2014 Conference on Empirical Methods in Natural Language Processing (EMNLP), pages 1556–1567,
October 25-29, 2014, Doha, Qatar. c�2014 Association for Computational Linguistics
</note>
<sectionHeader confidence="0.983984" genericHeader="introduction">
2 Background
</sectionHeader>
<subsectionHeader confidence="0.996803">
2.1 Relation Extraction
</subsectionHeader>
<bodyText confidence="0.999851678571429">
We are interested in extracting a set of relations
y1 ... yk from a fixed set of possible relations R,
given two entities e1 and e2. For example, we
would like to extract that Barack Obama was born
in Hawaii. The task is decomposed into two steps:
First, sentences containing mentions of both e1
and e2 are collected. The set of these sentences
x, marked with the entity mentions for e1 and e2,
becomes the input to the relation extractor, which
then produces a set of relations which hold be-
tween the mentions. We are predominantly in-
terested in the second step – classifying a set of
pairs of entity mentions into the relations they ex-
press. Figure 1 gives the general setting for re-
lation extraction, with entity pairs Barack Obama
and Hawaii, and Barack Obama and president.
Traditionally, relation extraction has fallen into
one of four broad approaches: supervised classi-
fication, as in the ACE task (Doddington et al.,
2004; GuoDong et al., 2005; Surdeanu and Cia-
ramita, 2007), distant supervision (Craven and
Kumlien, 1999; Wu and Weld, 2007; Mintz et
al., 2009; Sun et al., 2011; Roth and Klakow,
2013) deterministic rule-based systems (Soder-
land, 1997; Grishman and Min, 2010; Chen et al.,
2010), and translation from open domain informa-
tion extraction schema (Riedel et al., 2013). We
focus on the first two of these approaches.
</bodyText>
<subsectionHeader confidence="0.995407">
2.2 Supervised Relation Extraction
</subsectionHeader>
<bodyText confidence="0.9996781875">
Relation extraction can be naturally cast as a su-
pervised classification problem. A corpus of rela-
tion mentions is collected, and each mention x is
annotated with the relation y, if any, it expresses.
The classifier’s output is then aggregated to decide
the relations between the two entities.
However, annotating supervised training data
is generally expensive to perform at large scale.
Although resources such as Freebase or the TAC
KBP knowledge base have on the order of millions
of training tuples over entities it is not feasible to
manually annotate the corresponding mentions in
the text. This has led to the rise of distantly su-
pervised methods, which make use of this indirect
supervision, but do not necessitate mention-level
supervision.
</bodyText>
<figureCaption confidence="0.714404">
Figure 1: The relation extraction setup. For a
</figureCaption>
<bodyText confidence="0.937063166666667">
pair of entities, we collect sentences which men-
tion both entities. These sentences are then used
to predict one or more relations between those
entities. For instance, the sentences containing
both Barack Obama and Hawaii should support
the state of birth and state of residence relation.
</bodyText>
<subsectionHeader confidence="0.997562">
2.3 Distant Supervision
</subsectionHeader>
<bodyText confidence="0.999989870967742">
Traditional distant supervision makes the assump-
tion that for every triple (e1, y, e2) in a knowledge
base, every sentence containing mentions for e1
and e2 express the relation y. For instance, tak-
ing Figure 1, we would create a datum for each
of the three sentences containing BARACK OBAMA
and HAWAII labeled with state of birth, and like-
wise with state of residence, creating 6 training
examples overall. Similarly, both sentences in-
volving Barack Obama and president would be
marked as expressing the title relation.
While this allows us to leverage a large database
effectively, it nonetheless makes a number of naive
assumptions. First – explicit in the formulation of
the approach – it assumes that every mention ex-
presses some relation, and furthermore expresses
the known relation(s). For instance, the sen-
tence Obama visited Hawaii would be erroneously
treated as a positive example of the born in rela-
tion. Second, it implicitly assumes that our knowl-
edge base is complete: entity mentions with no
known relation are treated as negative examples.
The first of these assumptions is addressed by
multi-instance multi-label (MIML) learning, de-
scribed in Section 2.4. Min et al. (2013) address
the second assumption by extending the MIML
model with additional latent variables, while Xu
et al. (2013) allow feedback from a coarse relation
extractor to augment labels from the knowledge
base. These latter two approaches are compatible
with but are not implemented in this work.
</bodyText>
<subsectionHeader confidence="0.993801">
2.4 Multi-Instance Multi-Label Learning
</subsectionHeader>
<bodyText confidence="0.985162">
The multi-instance multi-label (MIML-RE) model
of Surdeanu et al. (2012), which builds upon work
</bodyText>
<table confidence="0.646504375">
state of residence
state of birth
Barack Obama was born in Hawaii.
Barack Obama visited Hawaii.
The president grew up in Hawaii.
title
Barack Obama met former president Clinton.
Obama became president in 2008.
</table>
<page confidence="0.943434">
1557
</page>
<figureCaption confidence="0.8154415">
Figure 2: The MIML-RE model, as shown in Sur-
deanu et al. (2012). The outer plate corresponds to
</figureCaption>
<bodyText confidence="0.999217666666666">
each of the n entity pairs in our knowledge base.
Each entity pair has a set of mention pairs Mi, and
a corresponding plate in the diagram for each men-
tion pair in Mi. The variable x represents the in-
put mention pair, whereas y represents the positive
and negative relations for the given pair of entities.
The latent variable z denotes a mention-level pre-
diction for each input. The weight vector for the
multinomial z classifier is given by wz, and there
is a weight vector wj for each binary y classifier.
by Hoffmann et al. (2011) and Riedel et al. (2010),
addresses the assumptions of distantly supervised
relations extractors in a principled way by positing
a latent mention-level annotation.
The model groups mentions according to their
entity pair – for instance, every mention pair with
Obama and Hawaii would be grouped together. A
latent variable zi is created for every mention i,
where zi E R U {None} takes a single relation
label, or a no relation marker. We create |R |bi-
nary variables y representing the known positive
and negative relations for the entity pair. A set of
binary classifiers (log-linear factors in the graphi-
cal model) links the latent predictions z1 ... z|Mi|
and each yj. These classifiers include two classes
of features: first, a binary feature which fires if at
least one of the mentions expresses a known rela-
tion between the entity pair, and second, a feature
for each co-occurrence of relations for a given en-
tity pair. Figure 2 describes the model.
</bodyText>
<subsectionHeader confidence="0.99815">
2.5 Background on Active Learning
</subsectionHeader>
<bodyText confidence="0.999976285714286">
We describe preliminaries and prior work on ac-
tive learning; we use this framework to propose
two sampling schemes in Section 3 which we use
to annotate mention-level labels for MIML-RE.
One way of expressing the generalization error
of a hypothesis hˆ is through its mean-squared error
with the true hypothesis h:
</bodyText>
<equation confidence="0.999517333333333">
E[(h(x) − ˆh(x))2]
= E[E[(h(x) − ˆh(x))2|x]]
= I E[(h(x) − h(x))2|x]p(x)dx.
</equation>
<bodyText confidence="0.994685">
The integrand can be further broken into bias
and variance terms:
</bodyText>
<equation confidence="0.9989375">
E[(h(x) − ˆh(x))2] = (E[ˆh(x)] − h(x))2
+ E[(ˆh(x) − E[ ˆh(x)])2]
</equation>
<bodyText confidence="0.999940162162162">
where for simplicity we’ve dropped the condition-
ing on x.
Many traditional sampling strategies, such as
query-by-committee (QBC) (Freund et al., 1992;
Freund et al., 1997) and uncertainty sampling
(Lewis and Gale, 1994), work by decreasing the
variance of the learned model. In QBC, we
first create a ‘committee’ of classifiers by ran-
domly sampling their parameters from a distribu-
tion based on the training data. These classifiers
then make predictions on the unlabeled examples,
and the examples on which there is the most dis-
agreement are selected for labeling. This strat-
egy can be seen as an attempt to decrease the ver-
sion space – the set of classifiers that are consis-
tent with the labeled data. Decreasing the version
space should lower variance, since variance is in-
versely related to the size of the hypothesis space.
In most scenarios, active learning does not con-
cern itself with the bias term. If a model is fun-
damentally misspecified, then no amount of ad-
ditional training data can lower its bias. How-
ever, our paradigm differs from the traditional set-
ting, in that we are annotating latent variables in
a model with a non-convex objective. These an-
notations may help increase the convexity of our
objective, leading us to a more accurate optimum
and thereby lowering bias.
The other component to consider is
fx · · · p(x)dx. This suggests that it is impor-
tant to choose examples that are representative
of the underlying distribution p(x), as we want
to label points that will improve the classifier’s
predictions on as many and as high-probability
examples as possible. Incorporating a repre-
sentativeness metric has been shown to provide
a significant improvement over plain QBC or
</bodyText>
<equation confidence="0.795039333333333">
. . . . . .
. . .
. . .
</equation>
<page confidence="0.923982">
1558
</page>
<bodyText confidence="0.669122">
uncertainty sampling (McCallum and Nigam,
1998; Settles, 2010).
</bodyText>
<subsectionHeader confidence="0.995744">
2.6 Active Learning for Relation Extraction
</subsectionHeader>
<bodyText confidence="0.999980846153846">
Several papers have explored active learning for
relation extraction. Fu and Grishman (2013) em-
ploy active learning to create a classifier quickly
for new relations, simulated from the ACE corpus.
Finn and Kushmerick (2003) compare a number
of selection criteria – including QBC – for a su-
pervised classifier. To the best of our knowledge,
we are the first to apply active learning to distantly
supervised relation extraction. Furthermore, we
evaluate our selection criteria live in a real-world
setting, collecting new sentences and evaluating
on an end-to-end task.
For latent variable models, McCallum and
Nigam (1998) apply active learning to semi-
supervised document classification. We take in-
spiration from their use of QBC and the choice of
metric for classifier disagreement. However their
model assumes a fully Bayesian set-up, whereas
ours does not require strong assumptions about the
parameter distributions.
Settles et al. (2008) use active learning to im-
prove a multiple-instance classifier. Their model
is simpler in that it does not allow for unobserved
variables or multiple labels, and the authors only
evaluate on image retrieval and synthetic text clas-
sification datasets.
</bodyText>
<sectionHeader confidence="0.987839" genericHeader="method">
3 Example Selection
</sectionHeader>
<bodyText confidence="0.9999954">
We describe three criteria for selection examples
to annotate. The first – sampling uniformly – is
a baseline for our hypothesis that intelligently se-
lecting examples is important. For this criterion,
we select mentions uniformly at random from the
training set to annotate. This is the approach used
in Zhang et al. (2012). The other two criteria rely
on a metric for disagreement provided by QBC;
we describe our adaptation of QBC for MIML-RE
as a preliminary to introducing these criteria.
</bodyText>
<subsectionHeader confidence="0.992716">
3.1 QBC For MIML-RE
</subsectionHeader>
<bodyText confidence="0.934626142857143">
We use a version of QBC based on bootstrap-
ping (Saar-Tsechansky and Provost, 2004). To
create the committee of classifiers, we re-sample
the training set with replacement 7 times and train
a model over each sampled dataset. We mea-
sure disagreement on z-labels among the classi-
fiers using a generalized Jensen-Shannon diver-
gence (McCallum and Nigam, 1998), taking the
average KL divergence of all classifier judgments.
We first calculate the mention-level confi-
dences. Note that z(m) i∈ Mi denotes the latent
variable in entity pair i with index m; z(−m)
i de-
notes the set of all latent variables except z(m)
</bodyText>
<equation confidence="0.999170222222222">
i :
p(z(m)
i |yi, xi) = p(yi, z(m)
i |xi)
p(yi|xi)
P
z(−m) ip(yi, zi|xi)
P.
zi(m) p(yi, z(m) i|xi)
</equation>
<bodyText confidence="0.99989">
Notice that the denominator just serves to nor-
malize the probability within a sentence group.
We can rewrite the numerator as follows:
</bodyText>
<equation confidence="0.978305571428571">
X
z(−m)
i
X=
z(−m)
i
X
= p(z(m)
i |xi)
z(−m)
i
For computational efficiency, we approximate
p(z(−m)
i |xi) with a point mass at its maximum.
</equation>
<bodyText confidence="0.999968142857143">
Next, we calculate the Jensen-Shannon (JS) diver-
gence from the k bootstrapped classifiers:
where pc is the probability assigned by each of the
k classifiers to the latent z(m)
i , and pmean is the av-
erage of these probabilities. We use this metric
to capture the disagreement of our model with re-
spect to a particular latent variable. This is then
used to inform our selection criteria.
We note that QBC may be especially useful in
our situation as our objective is highly nonconvex.
If two committee members disagree on a latent
variable, it is likely because they converged to dif-
ferent local optima; annotating that example could
help bring the classifiers into agreement.
The second selection criterion we consider is
the most straightforward application of QBC – se-
lecting the examples with the highest JS disagree-
ment. This allows us to compare our criterion, de-
scribed next, against an established criterion from
the active learning literature.
</bodyText>
<equation confidence="0.970867">
=
p(yi, zi|xi)
p(yi|zi)p(zi|xi)
p(yi|zi)p(z(−m) i|xi).
KL(pc(z(m)
i |yi,xi)||pmean(z(m)
i |yi,xi)) (1)
1
k
Xk
c=1
</equation>
<page confidence="0.980105">
1559
</page>
<subsectionHeader confidence="0.996228">
3.2 Sample by JS Disagreement
</subsectionHeader>
<bodyText confidence="0.998104076923077">
We propose a novel active learning sampling cri-
terion that incorporates not only disagreement but
also representativeness in selecting examples to
annotate. Prior work has taken a weighted combi-
nation of an example’s disagreement and a score
corresponding to whether the example is drawn
from a dense portion of the feature space (e.g.,
McCallum and Nigam (1998)). However, this re-
quires both selecting a criterion for defining den-
sity (e.g., distance metric in feature space), and
tuning a parameter for the relative weight of dis-
agreement versus representativeness.
Instead, we account for choosing representa-
tive examples by sampling without replacement
proportional to the example’s disagreement. For-
mally, we define the probability of selecting an
example z(m)
i to be proportional to the Jensen-
Shannon divergence in (1). Since the training set is
an approximation to the prior distribution over ex-
amples, sampling uniformly over the training set is
an approximation to sampling from the prior prob-
ability of seeing an input x. We can view our crite-
rion as an approximation to sampling proportional
to the product of two densities: a prior over exam-
ples x, and the JS divergence mentioned above.
</bodyText>
<sectionHeader confidence="0.9931345" genericHeader="method">
4 Incorporating Sentence-Level
Annotations
</sectionHeader>
<bodyText confidence="0.991209181818182">
Following Surdeanu et al. (2012), MIML-RE is
trained through hard discriminative Expectation
Maximization, inferring the latent z values in the
E-step and updating the weights for both the z and
y classifiers in the M-step. During the E-step, we
constrain the latent z to match our sentence-level
annotations when available.
It is worth noting that even in the hard-EM
regime, we can in principle incorporate annotator
uncertainty elegantly into the model. At each E
step, each zi is set according to
</bodyText>
<equation confidence="0.997202">
[
zi(m)∗ ≈ arg max p(z  |x(m)
i , wz) ×
R
z∈
p(y(r)  |z�i, wyr))J
</equation>
<bodyText confidence="0.993856555555556">
where zz contains the inferred labels from the
previous iteration, but with its mth component re-
placed by z(m)
i .
By setting the distribution p(z  |x(m)
i , wz) to re-
flect uncertainty among annotators, we can leave
open the possibility for the model to choose a re-
lation which annotators deemed unlikely, but the
model nonetheless prefers. For simplicity, how-
ever, we treat our annotations as a hard assign-
ment.
In addition to incorporating annotations during
training, we can also use this data to intelligently
initialize the model. Since the MIML-RE objec-
tive is non-convex, the initialization of the classi-
fier weights wy and wz is important. The y clas-
sifiers are initialized with the “at-least-once” as-
sumption of Hoffmann et al. (2011); wz can be ini-
tialized either using traditional distant supervision
or from a supervised classifier trained on the an-
notated sentences. If initialized with a supervised
classifier, the model can be viewed as augment-
ing this supervised model with a large distantly
labeled corpus, providing both additional entity
pairs to train from, and additional mentions for an
annotated entity pair.
</bodyText>
<sectionHeader confidence="0.986687" genericHeader="method">
5 Crowdsourced Example Annotation
</sectionHeader>
<bodyText confidence="0.999945172413793">
Most prior work on active learning is done by sim-
ulation on a fully labeled dataset; such a dataset
doesn’t exist for our case. Furthermore, a key aim
of this paper is to practically improve state-of-the-
art performance in relation extraction in addition
to evaluating active learning criteria. Therefore,
we develop and execute an annotation task for col-
lecting labels for our selected examples.
We utilize Amazon Mechanical Turk to crowd-
source annotations. For each task, the annotator
(Turker) is presented with the task description, fol-
lowed by 15 questions, 2 of which are randomly
placed controls. For each question, we present
Turkers with a relation mention and the top 5 re-
lation predictions from our classifier. The Turker
also has an option to freely specify a relation not
presented in the first five options, or mark that
there is no relation. We attempt to heuristically
match common free-form answers to official rela-
tions.
To maintain the quality of the results, we dis-
card all submissions in which both controls were
answered incorrectly, and additionally discard all
submissions from Turkers who failed the controls
on more than 13 of their submissions. Rejected
tasks were republished for other workers to com-
plete. We collect 5 annotations for each example,
and use the most commonly agreed answer as the
ground truth. Ties are broken arbitrarily, except in
</bodyText>
<equation confidence="0.6220765">
H
r
</equation>
<page confidence="0.732782">
1560
</page>
<figureCaption confidence="0.983072">
Figure 3: The task shown to Amazon Mechanical
Turk workers. A sentence along with the top 5 re-
</figureCaption>
<bodyText confidence="0.993967727272727">
lation predictions from our classifier are shown to
Turkers, as well as an option to specify a custom
relation or manually enter “no relation.” The cor-
rect response for this example should be either no
relation or a custom relation.
the case of deciding between a relation and no re-
lation, in which case the relation was always cho-
sen.
A total of 23 725 examples were annotated, cov-
ering 10 000 examples for each of the three selec-
tion criteria. Note that there is overlap between
the examples selected for the three criteria. In ad-
dition, 10 023 examples were annotated during de-
velopment; these are included in the set of all an-
notated examples, but excluded from any of the
three criteria. The compensation per task was 23
cents; the total cost of annotating examples was
$3156, in addition to $204 spent on developing the
task. Informally, Turkers achieved an accuracy of
around 75%, as evaluated by a paper author, per-
forming disproportionately well on identifying the
no relation label.
</bodyText>
<sectionHeader confidence="0.999606" genericHeader="evaluation">
6 Experiments
</sectionHeader>
<bodyText confidence="0.999997555555556">
We evaluate the three high-level research contri-
butions of this work: we show that we improve
the accuracy of MIML-RE, we validate the effec-
tiveness of our selection criteria, and we provide a
corpus of annotated examples, evaluating a super-
vised classifier trained on this corpus. The train-
ing and testing methodology for evaluating these
contributions is given in Sections 6.1 and 6.2; ex-
periments are given in Section 6.3.
</bodyText>
<subsectionHeader confidence="0.996061">
6.1 Training Setup
</subsectionHeader>
<bodyText confidence="0.999983785714286">
We adopt the setup of Surdeanu et al. (2012) for
training the MIML-RE model, with minor modifi-
cations. We use both the 2010 and 2013 KBP of-
ficial document collections, as well as a July 2013
dump of Wikipedia as our text corpus. We sub-
sample negatives such that s of our dataset con-
sists of entity pairs with no known relations. In all
experiments, MIML-RE is trained for 7 iterations
of EM; for efficiency, the z classifier is optimized
using stochastic gradient descent;2 the y classifiers
are optimized using L-BFGS.
Similarly to Surdeanu et al. (2011), we as-
sign negative relations which are either incompat-
ible with the known positive relations (e.g., re-
lations whose co-occurrence would violate type
constraints); or, are actually functional relations
in which another entity already participates. For
example, if we know that Obama was born in the
United States, we could add born in as a negative
relation to the pair Obama and Kenya.
Our dataset consists of 325 891 entity pairs with
at least one positive relation, and 158 091 entity
pairs with no positive relations. Pairs with at least
one known relation have an average of 4.56 men-
tions per group; groups with no known relations
have an average of 1.55 mentions per group. In to-
tal, 1208 524 distinct mentions are considered; the
annotated examples are selected from this pool.
</bodyText>
<subsectionHeader confidence="0.999727">
6.2 Testing Methodology
</subsectionHeader>
<bodyText confidence="0.999976818181818">
We compare against the original MIML-RE model
using the same dataset and evaluation methodol-
ogy as Surdeanu et al. (2012). This allows for an
evaluation where the only free variable between
this and prior work is the predictions of the rela-
tion extractor.
Additionally, we evaluate the relation extractors
in the context of Stanford’s end-to-end KBP sys-
tem (Angeli et al., 2014) using the NIST TAC-
KBP 2013 English Slotfilling evaluation. In the
end-to-end framework, the input to the system is a
query entity and a set of articles, and the output is
a set of slot fills – each slot fill is a candidate triple
in the knowledge base, the first element of which
is the query entity. This amounts to roughly pop-
ulating a data structure like Wikipedia infoboxes
automatically from a large corpus of text.
Importantly, an end-to-end evaluation in a top-
performing full system gives a more accurate idea
of the expected real-world gain from each model.
Both the information retrieval component provid-
ing candidates to the relation extractor, as well as
</bodyText>
<footnote confidence="0.992982666666667">
2For the sake of consistency, the supervised classifiers and
those in Mintz++ are trained identically to the z classifiers in
MIML-RE.
</footnote>
<page confidence="0.948752">
1561
</page>
<table confidence="0.999090142857143">
Method Init Not Used Active Learning Criterion Sample JS All Available
P R F1 Uniform High JS P R F1 P R F1
P R F1 P R F1
Mintz++ — 41.3 28.2 33.5 — — — —
MIML-RE Dist 38.0 30.5 33.8 39.2 30.4 34.2 41.7 28.9 34.1 36.6 31.1 33.6 37.5 30.6 33.7
Sup 35.1 35.6 35.4 34.4 35.0 34.7 46.2 30.8 37.0 39.4 36.2 37.7 36.0 37.1 36.5
Supervised — — 35.5 28.9 31.9 31.3 33.2 32.2 33.5 35.0 34.2 32.9 33.4 33.2
</table>
<tableCaption confidence="0.999483">
Table 1: A summary of results on the end-to-end KBP 2013 evaluation for various experiments. The
</tableCaption>
<bodyText confidence="0.999271181818182">
first column denotes the algorithm used: either traditional distant supervision (Mintz++), MIML-RE, or
a supervised classifier. In the case of MIML-RE, the model may be initialized either using Mintz++, or
the corresponding supervised classifier (the “Not Used” column is initialized with the “All” supervised
classifier). One of five active learning scenarios are evaluated: no annotated examples provided, the three
active learning criteria, and all available examples used. The entry in blue denotes the basic MIML-RE
model; entries in gray perform worse than this model. The bold items denote the best performance
among selection criteria.
the consistency and inference performed on the
classifier output introduce bias in this evaluation’s
sensitivity to particular types of errors. Mistakes
which are easy to filter, or are difficult to retrieve
using IR are less important in this evaluation; in
contrast, factors such as providing good confi-
dence scores for consistency become more impor-
tant.
For the end-to-end evaluation, we use the offi-
cial evaluation script with two changes: First, all
systems are evaluated with provenance ignored, so
as not to penalize any system for finding a new
provenance not validated in the official evaluation
key. Second, each system reports its optimal F1
along its P/R curve, yielding results which are
optimistic when compared against other systems
entered into the competition. However, this also
yields results which are invariant to threshold tun-
ing, and is therefore more appropriate for compar-
ing between systems in this paper.
Development was done on the KBP 2010–2012
queries, and results are reported using the 2013
queries as a simulated test set. Our best system
achieves an F1 of 37.7; the top two teams at KBP
2013 (of 18 entered) achieved F1 scores of 40.2
and 37.1 respectively, ignoring provenance.
</bodyText>
<subsectionHeader confidence="0.64626">
6.3 Results
</subsectionHeader>
<bodyText confidence="0.987408833333333">
Table 1 summarizes all results for the end-to-end
task; relevant features of the table are copied in
subsequent sections to illustrate key trends. Mod-
els which perform worse than the original MIML-
RE model (MIML-RE, initialized with “Dist,” un-
der “Not Used”) are denoted in gray. The best per-
</bodyText>
<table confidence="0.999701666666667">
System P R F1
Mintz++ 41.3 28.2 33.5
MIML + Dist 38.0 30.5 33.8
MIML + Sup 35.1 35.6 35.4
MIML + Dist + SampleJS 36.6 31.1 33.6
MIML + Sup + SampleJS 39.4 36.2 37.7
</table>
<tableCaption confidence="0.9113125">
Table 2: A summary of improvements to MIML-
RE on the end-to-end slotfilling task, copied from
Table 1. Mintz++ is the traditional distantly su-
pervised model. The second row corresponds to
</tableCaption>
<bodyText confidence="0.98558980952381">
the unmodified MIML-RE model. The third row
corresponds to MIML-RE initialized with a su-
pervised classifier (trained on all examples). The
fourth row is MIML-RE with annotated exam-
ples incorporated during training (but not initial-
ization). The last row shows the best results ob-
tained by our model.
forming model improves on the base model by 3.9
F1 points on the end-to-end task.
We evaluate each of the individual contribu-
tions of the paper: improving the accuracy of
the MIML-RE relation extractor, evaluating our
example selection criteria, and demonstrating the
annotated examples’ effectiveness for a fully-
supervised relation extractor.
Improve MIML-RE Accuracy A key goal of
this work is to improve the accuracy of the MIML-
RE model; we show that we improve the model
both on the end-to-end slotfilling task (Table 2) as
well as on a standard evaluation (Figure 5). Sim-
ilar to our work, recent work by Pershina et al.
</bodyText>
<page confidence="0.991271">
1562
</page>
<table confidence="0.993283571428571">
Precision
System P R F1
MIML + Sup 35.1 35.6 35.4
MIML + Sup + Uniform 34.4 35.0 34.7
MIML + Sup + HighJS 46.2 30.8 37.0
MIML + Sup + SampleJS 39.4 36.2 37.7
MIML + Sup + All 36.0 37.1 36.5
</table>
<tableCaption confidence="0.555126888888889">
Table 3: A summary of the performance of each
example selection criterion. In each case, the
model was initialized with a supervised classifier.
The first row corresponds to the MIML-RE model
initialized with a supervised classifier. The middle
three rows show performance for the three selec-
tion criteria, used both for initialization and during
training. The last row shows results if all available
annotations are used, independent of their source.
</tableCaption>
<table confidence="0.999838166666667">
System P R F1
Mintz++ 41.3 28.2 33.5
MIML + Dist 38.0 30.5 33.8
Supervised + SampleJS 33.5 35.0 34.2
MIML + Sup 35.1 35.6 35.5
MIML + Sup + SampleJS 39.4 36.2 37.7
</table>
<tableCaption confidence="0.994071">
Table 4: A comparison of the best performing su-
</tableCaption>
<bodyText confidence="0.99523404">
pervised classifier with other systems. The top
section compares the supervised classifier with
prior work. The lower section highlights the im-
provements gained from initializing MIML-RE
with a supervised classifier.
(2014) incorporates labeled data to guide MIML-
RE during training. They make use of labeled data
to extract training guidelines, which are intended
to generalize across many examples. We show that
we can match or outperform their improvements
with our best criterion.
A few interesting trends emerge from the end-
to-end results in Table 2. Using annotated sen-
tences during training alone did not improve per-
formance consistently, even hurting performance
when the SampleJS criterion was used. This
supports an intuition that the initialization of the
model is important, and that it is relatively difficult
to coax the model out of a local optimum if it is
initialized poorly. This is further supported by the
improvement in performance when the model is
initialized with a supervised classifier, even when
no examples are used during training. Similar
trends are reported in prior work, e.g., Smith and
Eisner (2007) Section 4.4.6.
</bodyText>
<figure confidence="0.9564345">
0 0.05 0.1 0.15 0.2 0.25 0.3
Recall
</figure>
<figureCaption confidence="0.91661625">
Figure 4: MIML-RE and Mintz++ evaluated ac-
cording to Surdeanu et al. (2012). The original
model from the paper is plotted for comparison, as
our training methodology is somewhat different.
</figureCaption>
<figure confidence="0.925524">
0 0.05 0.1 0.15 0.2 0.25 0.3
Recall
</figure>
<figureCaption confidence="0.697188666666667">
Figure 5: Our best active learning criterion evalu-
ated against our version of MIML-RE, alongside
the best system of Pershina et al. (2014).
</figureCaption>
<bodyText confidence="0.999827368421053">
Also interesting is the relatively small gain
MIML-RE provides over traditional distant super-
vision (Mintz++) in this setting. We conjecture
that the mistakes made by Mintz++ are often rel-
atively easily filtered by the downstream consis-
tency component. This is supported by Figure 4;
we evaluate our trained MIML-RE model against
Mintz++ and the results reported in Surdeanu et
al. (2012). We show that our model performs as
well or better than the original implementation,
and consistently outperforms Mintz++.
Evaluate Selection Criteria A key objective of
this work is to evaluate how much of an impact
careful selection of annotated examples has on the
overall performance of the system. We evaluate
the three selection criteria from Section 3.2, show-
ing the results for MIML-RE in Table 3; results
for the supervised classifier are given in Table 1.
In both cases, we show that the sampled JS cri-
</bodyText>
<figure confidence="0.951752619047619">
0.7
0.6
0.5
0.4
0.3
0.2
MIML-RE
Surdeanu et al. (2012)
Mintz++
Precision
0.7
0.6
0.5
0.4
0.3
0.2
Sample JS
Pershina et al. (2014)
MIML-RE
1563
Precision
</figure>
<bodyText confidence="0.999759078431373">
terion performs comparably to or better than the
other criteria.
At least two interesting trends can be noted from
these results: First, the uniformly sampled crite-
rion performed worse than MIML-RE initialized
with a supervised classifier. This may be due to
noise in the annotation: a small number of an-
notation errors on entity pairs with only a single
corresponding mention could introduce dangerous
noise into training. These singleton mentions will
rarely have disagreement between the committee
of classifiers, and therefore will generally only be
selected in the uniform criterion.
Second, adding in the full set of examples did
not improve performance – in fact, performance
generally dropped in this scenario. We conjecture
that this is due to the inclusion of the uniformly
sampled examples, with performance dropping for
the same reasons as above.
Both of these results can be reconciled with
the results of Zhang et al. (2012); like this work,
they annotated examples to analyze the trade-off
between adding more data to a distantly super-
vised system, and adding more direct supervi-
sion. They conclude that annotations provide only
a relatively small improvement in performance.
However, their examples were uniformly selected
from the training corpus, and did not make use
of the structure provided by MIML-RE. Our re-
sults agree in that neither the uniform selection
criterion nor the supervised classifier significantly
outperformed the unmodified MIML-RE model;
nonetheless, we show that if care is taken in se-
lecting these labeled examples we can achieve no-
ticeable improvements in accuracy.
We also evaluate our selection criteria on the
evaluation of Surdeanu et al. (2012), both initial-
ized with Mintz++ (Figure 7) and with the super-
vised classifier (Figure 6). These results mirror
those in the end-to-end evaluation; when initial-
ized with the supervised classifier the high dis-
agreement (High JS) and sampling proportional to
disagreement (Sample JS) criteria clearly outper-
form both the base MIML-RE model as well as
the uniformly sampling criterion. Using the an-
notated examples only during training yielded no
perceivable benefit over the base model (Figure 7).
Supervised Relation Extractor The examples
collected can be used to directly train a supervised
classifier, with results summarized in Table 4. The
most salient insight is that the performance of the
</bodyText>
<figure confidence="0.976989">
0 0.05 0.1 0.15 0.2 0.25 0.3
Recall
</figure>
<figureCaption confidence="0.991052">
Figure 6: A comparison of models trained with
various selection criteria on the evaluation of Sur-
deanu et al. (2012), all initialized with the corre-
sponding supervised classifier.
</figureCaption>
<figure confidence="0.9954025">
0 0.05 0.1 0.15 0.2 0.25 0.3
Recall
</figure>
<figureCaption confidence="0.965895333333333">
Figure 7: A comparison of models trained with
various selection criteria on the evaluation of Sur-
deanu et al. (2012), all initialized with Mintz++.
</figureCaption>
<bodyText confidence="0.999860533333333">
best supervised classifier is similar to that of the
MIML-RE model, despite being trained on nearly
two orders of magnitude less training data.
More interestingly, however, the supervised
classifier provides a noticeably better initializa-
tion for MIML-RE than Mintz++, yielding better
results even without enforcing the labels during
EM. These results suggest that the power gained
from the the more sophisticated MIML-RE model
is best used in conjunction with a small amount of
training data. That is, using MIML-RE as a princi-
pled model for combining a large distantly labeled
corpus and a small number of careful annotations
yields significant improvement over using either
of the two data sources alone.
</bodyText>
<figure confidence="0.999132333333333">
0.7
0.6
0.5
0.4
0.3
0.2
Sample JS
High JS
Uniform
MIML-RE
Precision
0.7
0.6
0.5
0.4
0.3
0.2
Sample JS
High JS
Uniform
MIML-RE
</figure>
<page confidence="0.965134">
1564
</page>
<table confidence="0.999892674418605">
Relation # P R F1
no relation 3073
employee of 1978 29 32 33 46 31 38
countries of res. 1061 30 42 7 40 11 41
states of residence 427 57 33 14 7 23 12
cities of residence 356 31 52 9 30 14 38
(org:)member of 290 0 0 0 0 0 0
country of hq 280 63 62 65 62 64 62
top members 221 36 26 50 60 42 36
country of birth 205 22 0 40 0 29 0
parents 196 10 26 31 54 15 35
city of hq 194 46 52 57 61 51 56
(org:)alt names 184 52 48 39 39 45 43
founded by 180 100 89 29 38 44 53
city of birth 145 17 50 8 17 11 25
state of hq 132 50 64 30 35 38 45
title 121 20 26 28 35 23 30
subsidiaries 105 33 25 6 3 10 5
founded 90 62 82 62 69 62 75
spouse 88 37 54 85 85 51 66
origin 86 42 43 68 70 51 53
state of birth 83 0 50 0 10 0 17
charges 69 54 54 16 16 24 24
cause of death 69 93 93 39 39 55 55
(per:)alt names 69 9 20 2 3 3 6
country of death 65 100 100 10 10 18 18
members 54 0 0 0 0 0 0
children 52 53 62 14 18 22 27
parents 50 64 64 28 28 39 39
city of death 38 42 75 16 19 23 30
dissolved 38 0 0 0 0 0 0
date of death 33 64 64 44 39 52 48
political affiliation 23 7 25 100 100 13 40
state of death 19 0 0 0 0 0 0
shareholders 19 0 0 0 0 0 0
siblings 16 50 50 33 33 40 40
schools attended 14 80 78 41 48 54 60
date of birth 11 100 100 85 85 92 92
other family 9 0 0 0 0 0 0
age 4 94 97 94 90 94 93
# of employees 3 0 0 0 0 0 0
religion 2 100 100 29 29 44 44
website 0 25 0 3 0 6 0
</table>
<tableCaption confidence="0.99489">
Table 5: A summary of relations annotated, and
</tableCaption>
<bodyText confidence="0.983143">
end-to-end slotfilling performance by relation.
The first column gives the relation; the second
shows the number of examples annotated. The
subsequent columns show the performance of the
unmodified MIML-RE model and our best per-
forming model (SampleJS). Changes in values are
bolded; positive changes are shown in green and
negative changes in red. The most frequent 10 re-
lations in the evaluation are likewise bolded.
</bodyText>
<subsectionHeader confidence="0.998745">
6.4 Analysis By Relation
</subsectionHeader>
<bodyText confidence="0.99991975">
In this section, we explore which of the KBP rela-
tions were shown to Turkers, and whether the im-
provements in accuracy correspond to these rela-
tions. We compare only the unmodified MIML-
RE model, and our best model (MIML-RE initial-
ized with the supervised classifier, under the Sam-
pleJS criterion). Results are shown in Table 5.
A few interesting trends emerge from this anal-
ysis. We note that annotating even 80+ examples
for a relation seems to provide a consistent boost
in accuracy, whereas relations with fewer anno-
tated examples tended to show little or no change.
However, the gains of our model are not univer-
sal across relation types, even dropping noticeably
on some – for instance, F1 drops on both state of
residence and country of birth. This could suggest
systematic noise from Turker judgments; e.g., for
foreign geography (state of residence) or ambigu-
ous relations (top members).
An additional insight from the table is the mis-
match between examples chosen to be annotated,
and the most popular relations in the KBP evalu-
ation. For instance, by far the most popular KBP
relation (title) had only 121 examples annotated.
</bodyText>
<sectionHeader confidence="0.998205" genericHeader="conclusions">
7 Conclusion
</sectionHeader>
<bodyText confidence="0.9999892">
We have shown that providing a relatively small
number of mention-level annotations can improve
the accuracy of MIML-RE, yielding an end-to-end
improvement of 3.9 F1 on the KBP task. Further-
more, we have introduced a new active learning
criterion, and shown both that the choice of crite-
rion is important, and that our new criterion per-
forms well. Lastly, we make available a dataset of
mention-level annotations for constructing a tradi-
tional supervised relation extractor.
</bodyText>
<sectionHeader confidence="0.996522" genericHeader="acknowledgments">
Acknowledgements
</sectionHeader>
<bodyText confidence="0.99955325">
We thank the anonymous reviewers for their
thoughtful comments, and Dan Weld for his feed-
back on additional experiments and analysis. We
gratefully acknowledge the support of the Defense
Advanced Research Projects Agency (DARPA)
Deep Exploration and Filtering of Text (DEFT)
Program under Air Force Research Laboratory
(AFRL) contract no. FA8750-13-2-0040. Any
opinions, findings, and conclusion or recommen-
dations expressed in this material are those of the
authors and do not necessarily reflect the view of
the DARPA, AFRL, or the US government.
</bodyText>
<page confidence="0.987855">
1565
</page>
<sectionHeader confidence="0.993862" genericHeader="references">
References
</sectionHeader>
<reference confidence="0.999853077669903">
Gabor Angeli, Arun Chaganty, Angel Chang, Kevin
Reschke, Julie Tibshirani, Jean Y. Wu, Osbert Bas-
tani, Keith Siilats, and Christopher D. Manning.
2014. Stanford’s 2013 KBP system. In TAC-KBP.
Zheng Chen, Suzanne Tamang, Adam Lee, Xiang
Li, Wen-Pin Lin, Matthew Snover, Javier Artiles,
Marissa Passantino, and Heng Ji. 2010. CUNY-
BLENDER. In TAC-KBP.
Mark Craven and Johan Kumlien. 1999. Constructing
biological knowledge bases by extracting informa-
tion from text sources. In AAAI.
George R Doddington, Alexis Mitchell, Mark A Przy-
bocki, Lance A Ramshaw, Stephanie Strassel, and
Ralph M Weischedel. 2004. The automatic content
extraction (ACE) program–tasks, data, and evalua-
tion. In LREC.
Aidan Finn and Nicolas Kushmerick. 2003. Active
learning selection strategies for information extrac-
tion. In International Workshop on Adaptive Text
Extraction and Mining.
Yoav Freund, H Sebastian Seung, Eli Shamir, and Naf-
tali Tishby. 1992. Information, prediction, and
query by committee. In NIPS.
Yoav Freund, H Sebastian Seung, Eli Shamir, and Naf-
tali Tishby. 1997. Selective sampling using the
query by committee algorithm. Machine learning,
28(2-3):133–168.
Lisheng Fu and Ralph Grishman. 2013. An efficient
active learning framework for new relation types. In
IJCNLP.
Ralph Grishman and Bonan Min. 2010. New York
University KBP 2010 slot-filling system. In Proc.
TAC 2010 Workshop.
Zhou GuoDong, Su Jian, Zhang Jie, and Zhang Min.
2005. Exploring various knowledge in relation ex-
traction. In ACL.
Raphael Hoffmann, Congle Zhang, Xiao Ling, Luke
Zettlemoyer, and Daniel S Weld. 2011. Knowledge-
based weak supervision for information extraction
of overlapping relations. In ACL-HLT.
David D Lewis and William A Gale. 1994. A sequen-
tial algorithm for training text classifiers. In SIGIR.
Andrew McCallum and Kamal Nigam. 1998. Employ-
ing EM and pool-based active learning for text clas-
sification. In ICML.
Bonan Min, Ralph Grishman, Li Wan, Chang Wang,
and David Gondek. 2013. Distant supervision for
relation extraction with an incomplete knowledge
base. In NAACL-HLT.
Mike Mintz, Steven Bills, Rion Snow, and Dan Juraf-
sky. 2009. Distant supervision for relation extrac-
tion without labeled data. In ACL.
Maria Pershina, Bonan Min, Wei Xu, and Ralph Gr-
ishman. 2014. Infusion of labeled data into distant
supervision for relation extraction. In ACL.
Sebastian Riedel, Limin Yao, and Andrew McCallum.
2010. Modeling relations and their mentions with-
out labeled text. In Machine Learning and Knowl-
edge Discovery in Databases. Springer.
Sebastian Riedel, Limin Yao, Andrew McCallum, and
Benjamin M Marlin. 2013. Relation extraction
with matrix factorization and universal schemas. In
NAACL-HLT.
Benjamin Roth and Dietrich Klakow. 2013. Feature-
based models for improving the quality of noisy
training data for relation extraction. In CIKM.
Maytal Saar-Tsechansky and Foster Provost. 2004.
Active sampling for class probability estimation and
ranking. Machine Learning, 54(2):153–178.
Burr Settles, Mark Craven, and Soumya Ray. 2008.
Multiple-instance active learning. In Advances in
neural information processing systems, pages 1289–
1296.
Burr Settles. 2010. Active learning literature survey.
University of Wisconsin Madison Technical Report
1648.
Noah Smith and Jason Eisner. 2007. Novel estimation
methods for unsupervised discovery of latent struc-
ture in natural language text. Ph.D. thesis, Johns
Hopkins.
Stephen G Soderland. 1997. Learning text analysis
rules for domain-specific natural language process-
ing. Ph.D. thesis, University of Massachusetts.
Ang Sun, Ralph Grishman, Wei Xu, and Bonan Min.
2011. New York University 2011 system for KBP
slot filling. In Proceedings of the Text Analytics
Conference.
Mihai Surdeanu and Massimiliano Ciaramita. 2007.
Robust information extraction with perceptrons. In
ACE07 Proceedings.
Mihai Surdeanu, Sonal Gupta, John Bauer, David Mc-
Closky, Angel X Chang, Valentin I Spitkovsky, and
Christopher D Manning. 2011. Stanfords distantly-
supervised slot-filling system. In Proceedings of the
Text Analytics Conference.
Mihai Surdeanu, Julie Tibshirani, Ramesh Nallap-
ati, and Christopher D. Manning. 2012. Multi-
instance multi-label learning for relation extraction.
In EMNLP.
Fei Wu and Daniel S Weld. 2007. Autonomously se-
mantifying wikipedia. In Proceedings of the six-
teenth ACM conference on information and knowl-
edge management. ACM.
</reference>
<page confidence="0.817742">
1566
</page>
<reference confidence="0.996407333333333">
Wei Xu, Le Zhao, Raphael Hoffman, and Ralph Grish-
man. 2013. Filling knowledge base gaps for distant
supervision of relation extraction. In ACL.
Ce Zhang, Feng Niu, Christopher R´e, and Jude Shav-
lik. 2012. Big data versus the crowd: Looking for
relationships in all the right places. In ACL.
</reference>
<page confidence="0.993671">
1567
</page>
</variant>
</algorithm>
<algorithm name="ParsHed" version="110505">
<variant no="0" confidence="0.652561">
<title confidence="0.998478">Combining Distant and Partial Supervision for Relation Extraction</title>
<author confidence="0.846093">Gabor Angeli</author>
<author confidence="0.846093">Julie Tibshirani</author>
<author confidence="0.846093">Jean Y Wu</author>
<author confidence="0.846093">D Christopher</author>
<affiliation confidence="0.805744">Stanford</affiliation>
<address confidence="0.886237">Stanford, CA</address>
<email confidence="0.904215">jtibs,jeaneis,</email>
<abstract confidence="0.993295">Broad-coverage relation extraction either requires expensive supervised training data, or suffers from drawbacks inherent to distant supervision. We present an approach for providing partial supervision to a distantly supervised relation extractor using a small number of carefully selected examples. We compare against established active learning criteria and propose a novel criterion to sample examples which are both uncertain and representative. In this way, we combine the benefits of fine-grained supervision for difficult examples with the coverage of a large distantly supervised corpus. Our approach a substantial increase of endthe 2013 KBP Slot Filling yielding a net</abstract>
</variant>
</algorithm>
<algorithm name="ParsCit" version="110505">
<citationList>
<citation valid="true">
<authors>
<author>Gabor Angeli</author>
<author>Arun Chaganty</author>
<author>Angel Chang</author>
<author>Kevin Reschke</author>
<author>Julie Tibshirani</author>
<author>Jean Y Wu</author>
<author>Osbert Bastani</author>
<author>Keith Siilats</author>
<author>Christopher D Manning</author>
</authors>
<title>Stanford’s</title>
<date>2014</date>
<booktitle>In TAC-KBP.</booktitle>
<contexts>
<context position="23826" citStr="Angeli et al., 2014" startWordPosition="3892" endWordPosition="3895">n average of 4.56 mentions per group; groups with no known relations have an average of 1.55 mentions per group. In total, 1208 524 distinct mentions are considered; the annotated examples are selected from this pool. 6.2 Testing Methodology We compare against the original MIML-RE model using the same dataset and evaluation methodology as Surdeanu et al. (2012). This allows for an evaluation where the only free variable between this and prior work is the predictions of the relation extractor. Additionally, we evaluate the relation extractors in the context of Stanford’s end-to-end KBP system (Angeli et al., 2014) using the NIST TACKBP 2013 English Slotfilling evaluation. In the end-to-end framework, the input to the system is a query entity and a set of articles, and the output is a set of slot fills – each slot fill is a candidate triple in the knowledge base, the first element of which is the query entity. This amounts to roughly populating a data structure like Wikipedia infoboxes automatically from a large corpus of text. Importantly, an end-to-end evaluation in a topperforming full system gives a more accurate idea of the expected real-world gain from each model. Both the information retrieval co</context>
</contexts>
<marker>Angeli, Chaganty, Chang, Reschke, Tibshirani, Wu, Bastani, Siilats, Manning, 2014</marker>
<rawString>Gabor Angeli, Arun Chaganty, Angel Chang, Kevin Reschke, Julie Tibshirani, Jean Y. Wu, Osbert Bastani, Keith Siilats, and Christopher D. Manning. 2014. Stanford’s 2013 KBP system. In TAC-KBP.</rawString>
</citation>
<citation valid="true">
<authors>
<author>Zheng Chen</author>
<author>Suzanne Tamang</author>
<author>Adam Lee</author>
<author>Xiang Li</author>
<author>Wen-Pin Lin</author>
<author>Matthew Snover</author>
<author>Javier Artiles</author>
<author>Marissa Passantino</author>
<author>Heng Ji</author>
</authors>
<date>2010</date>
<booktitle>CUNYBLENDER. In TAC-KBP.</booktitle>
<contexts>
<context position="5270" citStr="Chen et al., 2010" startWordPosition="824" endWordPosition="827">airs of entity mentions into the relations they express. Figure 1 gives the general setting for relation extraction, with entity pairs Barack Obama and Hawaii, and Barack Obama and president. Traditionally, relation extraction has fallen into one of four broad approaches: supervised classification, as in the ACE task (Doddington et al., 2004; GuoDong et al., 2005; Surdeanu and Ciaramita, 2007), distant supervision (Craven and Kumlien, 1999; Wu and Weld, 2007; Mintz et al., 2009; Sun et al., 2011; Roth and Klakow, 2013) deterministic rule-based systems (Soderland, 1997; Grishman and Min, 2010; Chen et al., 2010), and translation from open domain information extraction schema (Riedel et al., 2013). We focus on the first two of these approaches. 2.2 Supervised Relation Extraction Relation extraction can be naturally cast as a supervised classification problem. A corpus of relation mentions is collected, and each mention x is annotated with the relation y, if any, it expresses. The classifier’s output is then aggregated to decide the relations between the two entities. However, annotating supervised training data is generally expensive to perform at large scale. Although resources such as Freebase or th</context>
</contexts>
<marker>Chen, Tamang, Lee, Li, Lin, Snover, Artiles, Passantino, Ji, 2010</marker>
<rawString>Zheng Chen, Suzanne Tamang, Adam Lee, Xiang Li, Wen-Pin Lin, Matthew Snover, Javier Artiles, Marissa Passantino, and Heng Ji. 2010. CUNYBLENDER. In TAC-KBP.</rawString>
</citation>
<citation valid="true">
<authors>
<author>Mark Craven</author>
<author>Johan Kumlien</author>
</authors>
<title>Constructing biological knowledge bases by extracting information from text sources.</title>
<date>1999</date>
<booktitle>In AAAI.</booktitle>
<contexts>
<context position="5095" citStr="Craven and Kumlien, 1999" startWordPosition="794" endWordPosition="797"> input to the relation extractor, which then produces a set of relations which hold between the mentions. We are predominantly interested in the second step – classifying a set of pairs of entity mentions into the relations they express. Figure 1 gives the general setting for relation extraction, with entity pairs Barack Obama and Hawaii, and Barack Obama and president. Traditionally, relation extraction has fallen into one of four broad approaches: supervised classification, as in the ACE task (Doddington et al., 2004; GuoDong et al., 2005; Surdeanu and Ciaramita, 2007), distant supervision (Craven and Kumlien, 1999; Wu and Weld, 2007; Mintz et al., 2009; Sun et al., 2011; Roth and Klakow, 2013) deterministic rule-based systems (Soderland, 1997; Grishman and Min, 2010; Chen et al., 2010), and translation from open domain information extraction schema (Riedel et al., 2013). We focus on the first two of these approaches. 2.2 Supervised Relation Extraction Relation extraction can be naturally cast as a supervised classification problem. A corpus of relation mentions is collected, and each mention x is annotated with the relation y, if any, it expresses. The classifier’s output is then aggregated to decide t</context>
</contexts>
<marker>Craven, Kumlien, 1999</marker>
<rawString>Mark Craven and Johan Kumlien. 1999. Constructing biological knowledge bases by extracting information from text sources. In AAAI.</rawString>
</citation>
<citation valid="true">
<authors>
<author>George R Doddington</author>
<author>Alexis Mitchell</author>
<author>Mark A Przybocki</author>
<author>Lance A Ramshaw</author>
<author>Stephanie Strassel</author>
<author>Ralph M Weischedel</author>
</authors>
<title>The automatic content extraction (ACE) program–tasks, data, and evaluation.</title>
<date>2004</date>
<booktitle>In LREC.</booktitle>
<contexts>
<context position="4995" citStr="Doddington et al., 2004" startWordPosition="779" endWordPosition="782">collected. The set of these sentences x, marked with the entity mentions for e1 and e2, becomes the input to the relation extractor, which then produces a set of relations which hold between the mentions. We are predominantly interested in the second step – classifying a set of pairs of entity mentions into the relations they express. Figure 1 gives the general setting for relation extraction, with entity pairs Barack Obama and Hawaii, and Barack Obama and president. Traditionally, relation extraction has fallen into one of four broad approaches: supervised classification, as in the ACE task (Doddington et al., 2004; GuoDong et al., 2005; Surdeanu and Ciaramita, 2007), distant supervision (Craven and Kumlien, 1999; Wu and Weld, 2007; Mintz et al., 2009; Sun et al., 2011; Roth and Klakow, 2013) deterministic rule-based systems (Soderland, 1997; Grishman and Min, 2010; Chen et al., 2010), and translation from open domain information extraction schema (Riedel et al., 2013). We focus on the first two of these approaches. 2.2 Supervised Relation Extraction Relation extraction can be naturally cast as a supervised classification problem. A corpus of relation mentions is collected, and each mention x is annotat</context>
</contexts>
<marker>Doddington, Mitchell, Przybocki, Ramshaw, Strassel, Weischedel, 2004</marker>
<rawString>George R Doddington, Alexis Mitchell, Mark A Przybocki, Lance A Ramshaw, Stephanie Strassel, and Ralph M Weischedel. 2004. The automatic content extraction (ACE) program–tasks, data, and evaluation. In LREC.</rawString>
</citation>
<citation valid="true">
<authors>
<author>Aidan Finn</author>
<author>Nicolas Kushmerick</author>
</authors>
<title>Active learning selection strategies for information extraction.</title>
<date>2003</date>
<booktitle>In International Workshop on Adaptive Text Extraction and Mining.</booktitle>
<contexts>
<context position="12591" citStr="Finn and Kushmerick (2003)" startWordPosition="2034" endWordPosition="2037">ing distribution p(x), as we want to label points that will improve the classifier’s predictions on as many and as high-probability examples as possible. Incorporating a representativeness metric has been shown to provide a significant improvement over plain QBC or . . . . . . . . . . . . 1558 uncertainty sampling (McCallum and Nigam, 1998; Settles, 2010). 2.6 Active Learning for Relation Extraction Several papers have explored active learning for relation extraction. Fu and Grishman (2013) employ active learning to create a classifier quickly for new relations, simulated from the ACE corpus. Finn and Kushmerick (2003) compare a number of selection criteria – including QBC – for a supervised classifier. To the best of our knowledge, we are the first to apply active learning to distantly supervised relation extraction. Furthermore, we evaluate our selection criteria live in a real-world setting, collecting new sentences and evaluating on an end-to-end task. For latent variable models, McCallum and Nigam (1998) apply active learning to semisupervised document classification. We take inspiration from their use of QBC and the choice of metric for classifier disagreement. However their model assumes a fully Baye</context>
</contexts>
<marker>Finn, Kushmerick, 2003</marker>
<rawString>Aidan Finn and Nicolas Kushmerick. 2003. Active learning selection strategies for information extraction. In International Workshop on Adaptive Text Extraction and Mining.</rawString>
</citation>
<citation valid="true">
<authors>
<author>Yoav Freund</author>
<author>H Sebastian Seung</author>
<author>Eli Shamir</author>
<author>Naftali Tishby</author>
</authors>
<title>Information, prediction, and query by committee.</title>
<date>1992</date>
<booktitle>In NIPS.</booktitle>
<contexts>
<context position="10682" citStr="Freund et al., 1992" startWordPosition="1711" endWordPosition="1714">arning; we use this framework to propose two sampling schemes in Section 3 which we use to annotate mention-level labels for MIML-RE. One way of expressing the generalization error of a hypothesis hˆ is through its mean-squared error with the true hypothesis h: E[(h(x) − ˆh(x))2] = E[E[(h(x) − ˆh(x))2|x]] = I E[(h(x) − h(x))2|x]p(x)dx. The integrand can be further broken into bias and variance terms: E[(h(x) − ˆh(x))2] = (E[ˆh(x)] − h(x))2 + E[(ˆh(x) − E[ ˆh(x)])2] where for simplicity we’ve dropped the conditioning on x. Many traditional sampling strategies, such as query-by-committee (QBC) (Freund et al., 1992; Freund et al., 1997) and uncertainty sampling (Lewis and Gale, 1994), work by decreasing the variance of the learned model. In QBC, we first create a ‘committee’ of classifiers by randomly sampling their parameters from a distribution based on the training data. These classifiers then make predictions on the unlabeled examples, and the examples on which there is the most disagreement are selected for labeling. This strategy can be seen as an attempt to decrease the version space – the set of classifiers that are consistent with the labeled data. Decreasing the version space should lower vari</context>
</contexts>
<marker>Freund, Seung, Shamir, Tishby, 1992</marker>
<rawString>Yoav Freund, H Sebastian Seung, Eli Shamir, and Naftali Tishby. 1992. Information, prediction, and query by committee. In NIPS.</rawString>
</citation>
<citation valid="true">
<authors>
<author>Yoav Freund</author>
<author>H Sebastian Seung</author>
<author>Eli Shamir</author>
<author>Naftali Tishby</author>
</authors>
<title>Selective sampling using the query by committee algorithm.</title>
<date>1997</date>
<booktitle>Machine learning,</booktitle>
<pages>28--2</pages>
<contexts>
<context position="10704" citStr="Freund et al., 1997" startWordPosition="1715" endWordPosition="1718">ramework to propose two sampling schemes in Section 3 which we use to annotate mention-level labels for MIML-RE. One way of expressing the generalization error of a hypothesis hˆ is through its mean-squared error with the true hypothesis h: E[(h(x) − ˆh(x))2] = E[E[(h(x) − ˆh(x))2|x]] = I E[(h(x) − h(x))2|x]p(x)dx. The integrand can be further broken into bias and variance terms: E[(h(x) − ˆh(x))2] = (E[ˆh(x)] − h(x))2 + E[(ˆh(x) − E[ ˆh(x)])2] where for simplicity we’ve dropped the conditioning on x. Many traditional sampling strategies, such as query-by-committee (QBC) (Freund et al., 1992; Freund et al., 1997) and uncertainty sampling (Lewis and Gale, 1994), work by decreasing the variance of the learned model. In QBC, we first create a ‘committee’ of classifiers by randomly sampling their parameters from a distribution based on the training data. These classifiers then make predictions on the unlabeled examples, and the examples on which there is the most disagreement are selected for labeling. This strategy can be seen as an attempt to decrease the version space – the set of classifiers that are consistent with the labeled data. Decreasing the version space should lower variance, since variance i</context>
</contexts>
<marker>Freund, Seung, Shamir, Tishby, 1997</marker>
<rawString>Yoav Freund, H Sebastian Seung, Eli Shamir, and Naftali Tishby. 1997. Selective sampling using the query by committee algorithm. Machine learning, 28(2-3):133–168.</rawString>
</citation>
<citation valid="true">
<authors>
<author>Lisheng Fu</author>
<author>Ralph Grishman</author>
</authors>
<title>An efficient active learning framework for new relation types.</title>
<date>2013</date>
<booktitle>In IJCNLP.</booktitle>
<contexts>
<context position="12460" citStr="Fu and Grishman (2013)" startWordPosition="2013" endWordPosition="2016">nt to consider is fx · · · p(x)dx. This suggests that it is important to choose examples that are representative of the underlying distribution p(x), as we want to label points that will improve the classifier’s predictions on as many and as high-probability examples as possible. Incorporating a representativeness metric has been shown to provide a significant improvement over plain QBC or . . . . . . . . . . . . 1558 uncertainty sampling (McCallum and Nigam, 1998; Settles, 2010). 2.6 Active Learning for Relation Extraction Several papers have explored active learning for relation extraction. Fu and Grishman (2013) employ active learning to create a classifier quickly for new relations, simulated from the ACE corpus. Finn and Kushmerick (2003) compare a number of selection criteria – including QBC – for a supervised classifier. To the best of our knowledge, we are the first to apply active learning to distantly supervised relation extraction. Furthermore, we evaluate our selection criteria live in a real-world setting, collecting new sentences and evaluating on an end-to-end task. For latent variable models, McCallum and Nigam (1998) apply active learning to semisupervised document classification. We ta</context>
</contexts>
<marker>Fu, Grishman, 2013</marker>
<rawString>Lisheng Fu and Ralph Grishman. 2013. An efficient active learning framework for new relation types. In IJCNLP.</rawString>
</citation>
<citation valid="true">
<authors>
<author>Ralph Grishman</author>
<author>Bonan Min</author>
</authors>
<title>slot-filling system.</title>
<date>2010</date>
<booktitle>In Proc. TAC 2010 Workshop.</booktitle>
<location>New York University KBP</location>
<contexts>
<context position="5250" citStr="Grishman and Min, 2010" startWordPosition="820" endWordPosition="823">– classifying a set of pairs of entity mentions into the relations they express. Figure 1 gives the general setting for relation extraction, with entity pairs Barack Obama and Hawaii, and Barack Obama and president. Traditionally, relation extraction has fallen into one of four broad approaches: supervised classification, as in the ACE task (Doddington et al., 2004; GuoDong et al., 2005; Surdeanu and Ciaramita, 2007), distant supervision (Craven and Kumlien, 1999; Wu and Weld, 2007; Mintz et al., 2009; Sun et al., 2011; Roth and Klakow, 2013) deterministic rule-based systems (Soderland, 1997; Grishman and Min, 2010; Chen et al., 2010), and translation from open domain information extraction schema (Riedel et al., 2013). We focus on the first two of these approaches. 2.2 Supervised Relation Extraction Relation extraction can be naturally cast as a supervised classification problem. A corpus of relation mentions is collected, and each mention x is annotated with the relation y, if any, it expresses. The classifier’s output is then aggregated to decide the relations between the two entities. However, annotating supervised training data is generally expensive to perform at large scale. Although resources su</context>
</contexts>
<marker>Grishman, Min, 2010</marker>
<rawString>Ralph Grishman and Bonan Min. 2010. New York University KBP 2010 slot-filling system. In Proc. TAC 2010 Workshop.</rawString>
</citation>
<citation valid="true">
<authors>
<author>Zhou GuoDong</author>
<author>Su Jian</author>
<author>Zhang Jie</author>
<author>Zhang Min</author>
</authors>
<title>Exploring various knowledge in relation extraction.</title>
<date>2005</date>
<booktitle>In ACL.</booktitle>
<contexts>
<context position="5017" citStr="GuoDong et al., 2005" startWordPosition="783" endWordPosition="786">se sentences x, marked with the entity mentions for e1 and e2, becomes the input to the relation extractor, which then produces a set of relations which hold between the mentions. We are predominantly interested in the second step – classifying a set of pairs of entity mentions into the relations they express. Figure 1 gives the general setting for relation extraction, with entity pairs Barack Obama and Hawaii, and Barack Obama and president. Traditionally, relation extraction has fallen into one of four broad approaches: supervised classification, as in the ACE task (Doddington et al., 2004; GuoDong et al., 2005; Surdeanu and Ciaramita, 2007), distant supervision (Craven and Kumlien, 1999; Wu and Weld, 2007; Mintz et al., 2009; Sun et al., 2011; Roth and Klakow, 2013) deterministic rule-based systems (Soderland, 1997; Grishman and Min, 2010; Chen et al., 2010), and translation from open domain information extraction schema (Riedel et al., 2013). We focus on the first two of these approaches. 2.2 Supervised Relation Extraction Relation extraction can be naturally cast as a supervised classification problem. A corpus of relation mentions is collected, and each mention x is annotated with the relation y</context>
</contexts>
<marker>GuoDong, Jian, Jie, Min, 2005</marker>
<rawString>Zhou GuoDong, Su Jian, Zhang Jie, and Zhang Min. 2005. Exploring various knowledge in relation extraction. In ACL.</rawString>
</citation>
<citation valid="true">
<authors>
<author>Raphael Hoffmann</author>
<author>Congle Zhang</author>
<author>Xiao Ling</author>
<author>Luke Zettlemoyer</author>
<author>Daniel S Weld</author>
</authors>
<title>Knowledgebased weak supervision for information extraction of overlapping relations.</title>
<date>2011</date>
<booktitle>In ACL-HLT.</booktitle>
<contexts>
<context position="9019" citStr="Hoffmann et al. (2011)" startWordPosition="1436" endWordPosition="1439">: The MIML-RE model, as shown in Surdeanu et al. (2012). The outer plate corresponds to each of the n entity pairs in our knowledge base. Each entity pair has a set of mention pairs Mi, and a corresponding plate in the diagram for each mention pair in Mi. The variable x represents the input mention pair, whereas y represents the positive and negative relations for the given pair of entities. The latent variable z denotes a mention-level prediction for each input. The weight vector for the multinomial z classifier is given by wz, and there is a weight vector wj for each binary y classifier. by Hoffmann et al. (2011) and Riedel et al. (2010), addresses the assumptions of distantly supervised relations extractors in a principled way by positing a latent mention-level annotation. The model groups mentions according to their entity pair – for instance, every mention pair with Obama and Hawaii would be grouped together. A latent variable zi is created for every mention i, where zi E R U {None} takes a single relation label, or a no relation marker. We create |R |binary variables y representing the known positive and negative relations for the entity pair. A set of binary classifiers (log-linear factors in the</context>
<context position="18728" citStr="Hoffmann et al. (2011)" startWordPosition="3041" endWordPosition="3044">tting the distribution p(z |x(m) i , wz) to reflect uncertainty among annotators, we can leave open the possibility for the model to choose a relation which annotators deemed unlikely, but the model nonetheless prefers. For simplicity, however, we treat our annotations as a hard assignment. In addition to incorporating annotations during training, we can also use this data to intelligently initialize the model. Since the MIML-RE objective is non-convex, the initialization of the classifier weights wy and wz is important. The y classifiers are initialized with the “at-least-once” assumption of Hoffmann et al. (2011); wz can be initialized either using traditional distant supervision or from a supervised classifier trained on the annotated sentences. If initialized with a supervised classifier, the model can be viewed as augmenting this supervised model with a large distantly labeled corpus, providing both additional entity pairs to train from, and additional mentions for an annotated entity pair. 5 Crowdsourced Example Annotation Most prior work on active learning is done by simulation on a fully labeled dataset; such a dataset doesn’t exist for our case. Furthermore, a key aim of this paper is to practi</context>
</contexts>
<marker>Hoffmann, Zhang, Ling, Zettlemoyer, Weld, 2011</marker>
<rawString>Raphael Hoffmann, Congle Zhang, Xiao Ling, Luke Zettlemoyer, and Daniel S Weld. 2011. Knowledgebased weak supervision for information extraction of overlapping relations. In ACL-HLT.</rawString>
</citation>
<citation valid="true">
<authors>
<author>David D Lewis</author>
<author>William A Gale</author>
</authors>
<title>A sequential algorithm for training text classifiers.</title>
<date>1994</date>
<booktitle>In SIGIR.</booktitle>
<contexts>
<context position="10752" citStr="Lewis and Gale, 1994" startWordPosition="1722" endWordPosition="1725">tion 3 which we use to annotate mention-level labels for MIML-RE. One way of expressing the generalization error of a hypothesis hˆ is through its mean-squared error with the true hypothesis h: E[(h(x) − ˆh(x))2] = E[E[(h(x) − ˆh(x))2|x]] = I E[(h(x) − h(x))2|x]p(x)dx. The integrand can be further broken into bias and variance terms: E[(h(x) − ˆh(x))2] = (E[ˆh(x)] − h(x))2 + E[(ˆh(x) − E[ ˆh(x)])2] where for simplicity we’ve dropped the conditioning on x. Many traditional sampling strategies, such as query-by-committee (QBC) (Freund et al., 1992; Freund et al., 1997) and uncertainty sampling (Lewis and Gale, 1994), work by decreasing the variance of the learned model. In QBC, we first create a ‘committee’ of classifiers by randomly sampling their parameters from a distribution based on the training data. These classifiers then make predictions on the unlabeled examples, and the examples on which there is the most disagreement are selected for labeling. This strategy can be seen as an attempt to decrease the version space – the set of classifiers that are consistent with the labeled data. Decreasing the version space should lower variance, since variance is inversely related to the size of the hypothesi</context>
</contexts>
<marker>Lewis, Gale, 1994</marker>
<rawString>David D Lewis and William A Gale. 1994. A sequential algorithm for training text classifiers. In SIGIR.</rawString>
</citation>
<citation valid="true">
<authors>
<author>Andrew McCallum</author>
<author>Kamal Nigam</author>
</authors>
<title>Employing EM and pool-based active learning for text classification.</title>
<date>1998</date>
<booktitle>In ICML.</booktitle>
<contexts>
<context position="12306" citStr="McCallum and Nigam, 1998" startWordPosition="1992" endWordPosition="1995">tive. These annotations may help increase the convexity of our objective, leading us to a more accurate optimum and thereby lowering bias. The other component to consider is fx · · · p(x)dx. This suggests that it is important to choose examples that are representative of the underlying distribution p(x), as we want to label points that will improve the classifier’s predictions on as many and as high-probability examples as possible. Incorporating a representativeness metric has been shown to provide a significant improvement over plain QBC or . . . . . . . . . . . . 1558 uncertainty sampling (McCallum and Nigam, 1998; Settles, 2010). 2.6 Active Learning for Relation Extraction Several papers have explored active learning for relation extraction. Fu and Grishman (2013) employ active learning to create a classifier quickly for new relations, simulated from the ACE corpus. Finn and Kushmerick (2003) compare a number of selection criteria – including QBC – for a supervised classifier. To the best of our knowledge, we are the first to apply active learning to distantly supervised relation extraction. Furthermore, we evaluate our selection criteria live in a real-world setting, collecting new sentences and eval</context>
<context position="14447" citStr="McCallum and Nigam, 1998" startWordPosition="2328" endWordPosition="2331">om the training set to annotate. This is the approach used in Zhang et al. (2012). The other two criteria rely on a metric for disagreement provided by QBC; we describe our adaptation of QBC for MIML-RE as a preliminary to introducing these criteria. 3.1 QBC For MIML-RE We use a version of QBC based on bootstrapping (Saar-Tsechansky and Provost, 2004). To create the committee of classifiers, we re-sample the training set with replacement 7 times and train a model over each sampled dataset. We measure disagreement on z-labels among the classifiers using a generalized Jensen-Shannon divergence (McCallum and Nigam, 1998), taking the average KL divergence of all classifier judgments. We first calculate the mention-level confidences. Note that z(m) i∈ Mi denotes the latent variable in entity pair i with index m; z(−m) i denotes the set of all latent variables except z(m) i : p(z(m) i |yi, xi) = p(yi, z(m) i |xi) p(yi|xi) P z(−m) ip(yi, zi|xi) P. zi(m) p(yi, z(m) i|xi) Notice that the denominator just serves to normalize the probability within a sentence group. We can rewrite the numerator as follows: X z(−m) i X= z(−m) i X = p(z(m) i |xi) z(−m) i For computational efficiency, we approximate p(z(−m) i |xi) with </context>
<context position="16539" citStr="McCallum and Nigam (1998)" startWordPosition="2680" endWordPosition="2683">s to compare our criterion, described next, against an established criterion from the active learning literature. = p(yi, zi|xi) p(yi|zi)p(zi|xi) p(yi|zi)p(z(−m) i|xi). KL(pc(z(m) i |yi,xi)||pmean(z(m) i |yi,xi)) (1) 1 k Xk c=1 1559 3.2 Sample by JS Disagreement We propose a novel active learning sampling criterion that incorporates not only disagreement but also representativeness in selecting examples to annotate. Prior work has taken a weighted combination of an example’s disagreement and a score corresponding to whether the example is drawn from a dense portion of the feature space (e.g., McCallum and Nigam (1998)). However, this requires both selecting a criterion for defining density (e.g., distance metric in feature space), and tuning a parameter for the relative weight of disagreement versus representativeness. Instead, we account for choosing representative examples by sampling without replacement proportional to the example’s disagreement. Formally, we define the probability of selecting an example z(m) i to be proportional to the JensenShannon divergence in (1). Since the training set is an approximation to the prior distribution over examples, sampling uniformly over the training set is an appr</context>
</contexts>
<marker>McCallum, Nigam, 1998</marker>
<rawString>Andrew McCallum and Kamal Nigam. 1998. Employing EM and pool-based active learning for text classification. In ICML.</rawString>
</citation>
<citation valid="true">
<authors>
<author>Bonan Min</author>
<author>Ralph Grishman</author>
<author>Li Wan</author>
<author>Chang Wang</author>
<author>David Gondek</author>
</authors>
<title>Distant supervision for relation extraction with an incomplete knowledge base.</title>
<date>2013</date>
<booktitle>In NAACL-HLT.</booktitle>
<contexts>
<context position="7744" citStr="Min et al. (2013)" startWordPosition="1221" endWordPosition="1224">ctively, it nonetheless makes a number of naive assumptions. First – explicit in the formulation of the approach – it assumes that every mention expresses some relation, and furthermore expresses the known relation(s). For instance, the sentence Obama visited Hawaii would be erroneously treated as a positive example of the born in relation. Second, it implicitly assumes that our knowledge base is complete: entity mentions with no known relation are treated as negative examples. The first of these assumptions is addressed by multi-instance multi-label (MIML) learning, described in Section 2.4. Min et al. (2013) address the second assumption by extending the MIML model with additional latent variables, while Xu et al. (2013) allow feedback from a coarse relation extractor to augment labels from the knowledge base. These latter two approaches are compatible with but are not implemented in this work. 2.4 Multi-Instance Multi-Label Learning The multi-instance multi-label (MIML-RE) model of Surdeanu et al. (2012), which builds upon work state of residence state of birth Barack Obama was born in Hawaii. Barack Obama visited Hawaii. The president grew up in Hawaii. title Barack Obama met former president C</context>
</contexts>
<marker>Min, Grishman, Wan, Wang, Gondek, 2013</marker>
<rawString>Bonan Min, Ralph Grishman, Li Wan, Chang Wang, and David Gondek. 2013. Distant supervision for relation extraction with an incomplete knowledge base. In NAACL-HLT.</rawString>
</citation>
<citation valid="true">
<authors>
<author>Mike Mintz</author>
<author>Steven Bills</author>
<author>Rion Snow</author>
<author>Dan Jurafsky</author>
</authors>
<title>Distant supervision for relation extraction without labeled data.</title>
<date>2009</date>
<booktitle>In ACL.</booktitle>
<contexts>
<context position="5134" citStr="Mintz et al., 2009" startWordPosition="802" endWordPosition="805">produces a set of relations which hold between the mentions. We are predominantly interested in the second step – classifying a set of pairs of entity mentions into the relations they express. Figure 1 gives the general setting for relation extraction, with entity pairs Barack Obama and Hawaii, and Barack Obama and president. Traditionally, relation extraction has fallen into one of four broad approaches: supervised classification, as in the ACE task (Doddington et al., 2004; GuoDong et al., 2005; Surdeanu and Ciaramita, 2007), distant supervision (Craven and Kumlien, 1999; Wu and Weld, 2007; Mintz et al., 2009; Sun et al., 2011; Roth and Klakow, 2013) deterministic rule-based systems (Soderland, 1997; Grishman and Min, 2010; Chen et al., 2010), and translation from open domain information extraction schema (Riedel et al., 2013). We focus on the first two of these approaches. 2.2 Supervised Relation Extraction Relation extraction can be naturally cast as a supervised classification problem. A corpus of relation mentions is collected, and each mention x is annotated with the relation y, if any, it expresses. The classifier’s output is then aggregated to decide the relations between the two entities. </context>
</contexts>
<marker>Mintz, Bills, Snow, Jurafsky, 2009</marker>
<rawString>Mike Mintz, Steven Bills, Rion Snow, and Dan Jurafsky. 2009. Distant supervision for relation extraction without labeled data. In ACL.</rawString>
</citation>
<citation valid="true">
<authors>
<author>Maria Pershina</author>
<author>Bonan Min</author>
<author>Wei Xu</author>
<author>Ralph Grishman</author>
</authors>
<title>Infusion of labeled data into distant supervision for relation extraction.</title>
<date>2014</date>
<booktitle>In ACL.</booktitle>
<contexts>
<context position="30968" citStr="Pershina et al. (2014)" startWordPosition="5095" endWordPosition="5098">improvement in performance when the model is initialized with a supervised classifier, even when no examples are used during training. Similar trends are reported in prior work, e.g., Smith and Eisner (2007) Section 4.4.6. 0 0.05 0.1 0.15 0.2 0.25 0.3 Recall Figure 4: MIML-RE and Mintz++ evaluated according to Surdeanu et al. (2012). The original model from the paper is plotted for comparison, as our training methodology is somewhat different. 0 0.05 0.1 0.15 0.2 0.25 0.3 Recall Figure 5: Our best active learning criterion evaluated against our version of MIML-RE, alongside the best system of Pershina et al. (2014). Also interesting is the relatively small gain MIML-RE provides over traditional distant supervision (Mintz++) in this setting. We conjecture that the mistakes made by Mintz++ are often relatively easily filtered by the downstream consistency component. This is supported by Figure 4; we evaluate our trained MIML-RE model against Mintz++ and the results reported in Surdeanu et al. (2012). We show that our model performs as well or better than the original implementation, and consistently outperforms Mintz++. Evaluate Selection Criteria A key objective of this work is to evaluate how much of an</context>
</contexts>
<marker>Pershina, Min, Xu, Grishman, 2014</marker>
<rawString>Maria Pershina, Bonan Min, Wei Xu, and Ralph Grishman. 2014. Infusion of labeled data into distant supervision for relation extraction. In ACL.</rawString>
</citation>
<citation valid="true">
<authors>
<author>Sebastian Riedel</author>
<author>Limin Yao</author>
<author>Andrew McCallum</author>
</authors>
<title>Modeling relations and their mentions without labeled text.</title>
<date>2010</date>
<booktitle>In Machine Learning and Knowledge Discovery in Databases.</booktitle>
<publisher>Springer.</publisher>
<contexts>
<context position="9044" citStr="Riedel et al. (2010)" startWordPosition="1441" endWordPosition="1444">wn in Surdeanu et al. (2012). The outer plate corresponds to each of the n entity pairs in our knowledge base. Each entity pair has a set of mention pairs Mi, and a corresponding plate in the diagram for each mention pair in Mi. The variable x represents the input mention pair, whereas y represents the positive and negative relations for the given pair of entities. The latent variable z denotes a mention-level prediction for each input. The weight vector for the multinomial z classifier is given by wz, and there is a weight vector wj for each binary y classifier. by Hoffmann et al. (2011) and Riedel et al. (2010), addresses the assumptions of distantly supervised relations extractors in a principled way by positing a latent mention-level annotation. The model groups mentions according to their entity pair – for instance, every mention pair with Obama and Hawaii would be grouped together. A latent variable zi is created for every mention i, where zi E R U {None} takes a single relation label, or a no relation marker. We create |R |binary variables y representing the known positive and negative relations for the entity pair. A set of binary classifiers (log-linear factors in the graphical model) links t</context>
</contexts>
<marker>Riedel, Yao, McCallum, 2010</marker>
<rawString>Sebastian Riedel, Limin Yao, and Andrew McCallum. 2010. Modeling relations and their mentions without labeled text. In Machine Learning and Knowledge Discovery in Databases. Springer.</rawString>
</citation>
<citation valid="true">
<authors>
<author>Sebastian Riedel</author>
<author>Limin Yao</author>
<author>Andrew McCallum</author>
<author>Benjamin M Marlin</author>
</authors>
<title>Relation extraction with matrix factorization and universal schemas.</title>
<date>2013</date>
<booktitle>In NAACL-HLT.</booktitle>
<contexts>
<context position="5356" citStr="Riedel et al., 2013" startWordPosition="837" endWordPosition="840">setting for relation extraction, with entity pairs Barack Obama and Hawaii, and Barack Obama and president. Traditionally, relation extraction has fallen into one of four broad approaches: supervised classification, as in the ACE task (Doddington et al., 2004; GuoDong et al., 2005; Surdeanu and Ciaramita, 2007), distant supervision (Craven and Kumlien, 1999; Wu and Weld, 2007; Mintz et al., 2009; Sun et al., 2011; Roth and Klakow, 2013) deterministic rule-based systems (Soderland, 1997; Grishman and Min, 2010; Chen et al., 2010), and translation from open domain information extraction schema (Riedel et al., 2013). We focus on the first two of these approaches. 2.2 Supervised Relation Extraction Relation extraction can be naturally cast as a supervised classification problem. A corpus of relation mentions is collected, and each mention x is annotated with the relation y, if any, it expresses. The classifier’s output is then aggregated to decide the relations between the two entities. However, annotating supervised training data is generally expensive to perform at large scale. Although resources such as Freebase or the TAC KBP knowledge base have on the order of millions of training tuples over entitie</context>
</contexts>
<marker>Riedel, Yao, McCallum, Marlin, 2013</marker>
<rawString>Sebastian Riedel, Limin Yao, Andrew McCallum, and Benjamin M Marlin. 2013. Relation extraction with matrix factorization and universal schemas. In NAACL-HLT.</rawString>
</citation>
<citation valid="true">
<authors>
<author>Benjamin Roth</author>
<author>Dietrich Klakow</author>
</authors>
<title>Featurebased models for improving the quality of noisy training data for relation extraction.</title>
<date>2013</date>
<booktitle>In CIKM.</booktitle>
<contexts>
<context position="5176" citStr="Roth and Klakow, 2013" startWordPosition="810" endWordPosition="813"> between the mentions. We are predominantly interested in the second step – classifying a set of pairs of entity mentions into the relations they express. Figure 1 gives the general setting for relation extraction, with entity pairs Barack Obama and Hawaii, and Barack Obama and president. Traditionally, relation extraction has fallen into one of four broad approaches: supervised classification, as in the ACE task (Doddington et al., 2004; GuoDong et al., 2005; Surdeanu and Ciaramita, 2007), distant supervision (Craven and Kumlien, 1999; Wu and Weld, 2007; Mintz et al., 2009; Sun et al., 2011; Roth and Klakow, 2013) deterministic rule-based systems (Soderland, 1997; Grishman and Min, 2010; Chen et al., 2010), and translation from open domain information extraction schema (Riedel et al., 2013). We focus on the first two of these approaches. 2.2 Supervised Relation Extraction Relation extraction can be naturally cast as a supervised classification problem. A corpus of relation mentions is collected, and each mention x is annotated with the relation y, if any, it expresses. The classifier’s output is then aggregated to decide the relations between the two entities. However, annotating supervised training da</context>
</contexts>
<marker>Roth, Klakow, 2013</marker>
<rawString>Benjamin Roth and Dietrich Klakow. 2013. Featurebased models for improving the quality of noisy training data for relation extraction. In CIKM.</rawString>
</citation>
<citation valid="true">
<authors>
<author>Maytal Saar-Tsechansky</author>
<author>Foster Provost</author>
</authors>
<title>Active sampling for class probability estimation and ranking.</title>
<date>2004</date>
<booktitle>Machine Learning,</booktitle>
<volume>54</volume>
<issue>2</issue>
<contexts>
<context position="14175" citStr="Saar-Tsechansky and Provost, 2004" startWordPosition="2285" endWordPosition="2288">fication datasets. 3 Example Selection We describe three criteria for selection examples to annotate. The first – sampling uniformly – is a baseline for our hypothesis that intelligently selecting examples is important. For this criterion, we select mentions uniformly at random from the training set to annotate. This is the approach used in Zhang et al. (2012). The other two criteria rely on a metric for disagreement provided by QBC; we describe our adaptation of QBC for MIML-RE as a preliminary to introducing these criteria. 3.1 QBC For MIML-RE We use a version of QBC based on bootstrapping (Saar-Tsechansky and Provost, 2004). To create the committee of classifiers, we re-sample the training set with replacement 7 times and train a model over each sampled dataset. We measure disagreement on z-labels among the classifiers using a generalized Jensen-Shannon divergence (McCallum and Nigam, 1998), taking the average KL divergence of all classifier judgments. We first calculate the mention-level confidences. Note that z(m) i∈ Mi denotes the latent variable in entity pair i with index m; z(−m) i denotes the set of all latent variables except z(m) i : p(z(m) i |yi, xi) = p(yi, z(m) i |xi) p(yi|xi) P z(−m) ip(yi, zi|xi) P</context>
</contexts>
<marker>Saar-Tsechansky, Provost, 2004</marker>
<rawString>Maytal Saar-Tsechansky and Foster Provost. 2004. Active sampling for class probability estimation and ranking. Machine Learning, 54(2):153–178.</rawString>
</citation>
<citation valid="true">
<authors>
<author>Burr Settles</author>
<author>Mark Craven</author>
<author>Soumya Ray</author>
</authors>
<title>Multiple-instance active learning.</title>
<date>2008</date>
<booktitle>In Advances in neural information processing systems,</booktitle>
<pages>1289--1296</pages>
<contexts>
<context position="13309" citStr="Settles et al. (2008)" startWordPosition="2144" endWordPosition="2147">t of our knowledge, we are the first to apply active learning to distantly supervised relation extraction. Furthermore, we evaluate our selection criteria live in a real-world setting, collecting new sentences and evaluating on an end-to-end task. For latent variable models, McCallum and Nigam (1998) apply active learning to semisupervised document classification. We take inspiration from their use of QBC and the choice of metric for classifier disagreement. However their model assumes a fully Bayesian set-up, whereas ours does not require strong assumptions about the parameter distributions. Settles et al. (2008) use active learning to improve a multiple-instance classifier. Their model is simpler in that it does not allow for unobserved variables or multiple labels, and the authors only evaluate on image retrieval and synthetic text classification datasets. 3 Example Selection We describe three criteria for selection examples to annotate. The first – sampling uniformly – is a baseline for our hypothesis that intelligently selecting examples is important. For this criterion, we select mentions uniformly at random from the training set to annotate. This is the approach used in Zhang et al. (2012). The </context>
</contexts>
<marker>Settles, Craven, Ray, 2008</marker>
<rawString>Burr Settles, Mark Craven, and Soumya Ray. 2008. Multiple-instance active learning. In Advances in neural information processing systems, pages 1289– 1296.</rawString>
</citation>
<citation valid="true">
<authors>
<author>Burr Settles</author>
</authors>
<title>Active learning literature survey.</title>
<date>2010</date>
<tech>Technical Report 1648.</tech>
<institution>University of Wisconsin Madison</institution>
<contexts>
<context position="12322" citStr="Settles, 2010" startWordPosition="1996" endWordPosition="1997">y help increase the convexity of our objective, leading us to a more accurate optimum and thereby lowering bias. The other component to consider is fx · · · p(x)dx. This suggests that it is important to choose examples that are representative of the underlying distribution p(x), as we want to label points that will improve the classifier’s predictions on as many and as high-probability examples as possible. Incorporating a representativeness metric has been shown to provide a significant improvement over plain QBC or . . . . . . . . . . . . 1558 uncertainty sampling (McCallum and Nigam, 1998; Settles, 2010). 2.6 Active Learning for Relation Extraction Several papers have explored active learning for relation extraction. Fu and Grishman (2013) employ active learning to create a classifier quickly for new relations, simulated from the ACE corpus. Finn and Kushmerick (2003) compare a number of selection criteria – including QBC – for a supervised classifier. To the best of our knowledge, we are the first to apply active learning to distantly supervised relation extraction. Furthermore, we evaluate our selection criteria live in a real-world setting, collecting new sentences and evaluating on an end</context>
</contexts>
<marker>Settles, 2010</marker>
<rawString>Burr Settles. 2010. Active learning literature survey. University of Wisconsin Madison Technical Report 1648.</rawString>
</citation>
<citation valid="true">
<authors>
<author>Noah Smith</author>
<author>Jason Eisner</author>
</authors>
<title>Novel estimation methods for unsupervised discovery of latent structure in natural language text.</title>
<date>2007</date>
<tech>Ph.D. thesis,</tech>
<institution>Johns Hopkins.</institution>
<contexts>
<context position="30553" citStr="Smith and Eisner (2007)" startWordPosition="5024" endWordPosition="5027">emerge from the endto-end results in Table 2. Using annotated sentences during training alone did not improve performance consistently, even hurting performance when the SampleJS criterion was used. This supports an intuition that the initialization of the model is important, and that it is relatively difficult to coax the model out of a local optimum if it is initialized poorly. This is further supported by the improvement in performance when the model is initialized with a supervised classifier, even when no examples are used during training. Similar trends are reported in prior work, e.g., Smith and Eisner (2007) Section 4.4.6. 0 0.05 0.1 0.15 0.2 0.25 0.3 Recall Figure 4: MIML-RE and Mintz++ evaluated according to Surdeanu et al. (2012). The original model from the paper is plotted for comparison, as our training methodology is somewhat different. 0 0.05 0.1 0.15 0.2 0.25 0.3 Recall Figure 5: Our best active learning criterion evaluated against our version of MIML-RE, alongside the best system of Pershina et al. (2014). Also interesting is the relatively small gain MIML-RE provides over traditional distant supervision (Mintz++) in this setting. We conjecture that the mistakes made by Mintz++ are ofte</context>
</contexts>
<marker>Smith, Eisner, 2007</marker>
<rawString>Noah Smith and Jason Eisner. 2007. Novel estimation methods for unsupervised discovery of latent structure in natural language text. Ph.D. thesis, Johns Hopkins.</rawString>
</citation>
<citation valid="true">
<authors>
<author>Stephen G Soderland</author>
</authors>
<title>Learning text analysis rules for domain-specific natural language processing.</title>
<date>1997</date>
<tech>Ph.D. thesis,</tech>
<institution>University of Massachusetts.</institution>
<contexts>
<context position="5226" citStr="Soderland, 1997" startWordPosition="817" endWordPosition="819"> the second step – classifying a set of pairs of entity mentions into the relations they express. Figure 1 gives the general setting for relation extraction, with entity pairs Barack Obama and Hawaii, and Barack Obama and president. Traditionally, relation extraction has fallen into one of four broad approaches: supervised classification, as in the ACE task (Doddington et al., 2004; GuoDong et al., 2005; Surdeanu and Ciaramita, 2007), distant supervision (Craven and Kumlien, 1999; Wu and Weld, 2007; Mintz et al., 2009; Sun et al., 2011; Roth and Klakow, 2013) deterministic rule-based systems (Soderland, 1997; Grishman and Min, 2010; Chen et al., 2010), and translation from open domain information extraction schema (Riedel et al., 2013). We focus on the first two of these approaches. 2.2 Supervised Relation Extraction Relation extraction can be naturally cast as a supervised classification problem. A corpus of relation mentions is collected, and each mention x is annotated with the relation y, if any, it expresses. The classifier’s output is then aggregated to decide the relations between the two entities. However, annotating supervised training data is generally expensive to perform at large scal</context>
</contexts>
<marker>Soderland, 1997</marker>
<rawString>Stephen G Soderland. 1997. Learning text analysis rules for domain-specific natural language processing. Ph.D. thesis, University of Massachusetts.</rawString>
</citation>
<citation valid="true">
<authors>
<author>Ang Sun</author>
<author>Ralph Grishman</author>
<author>Wei Xu</author>
<author>Bonan Min</author>
</authors>
<title>system for KBP slot filling.</title>
<date>2011</date>
<booktitle>In Proceedings of the Text Analytics Conference.</booktitle>
<location>New York University</location>
<contexts>
<context position="5152" citStr="Sun et al., 2011" startWordPosition="806" endWordPosition="809">lations which hold between the mentions. We are predominantly interested in the second step – classifying a set of pairs of entity mentions into the relations they express. Figure 1 gives the general setting for relation extraction, with entity pairs Barack Obama and Hawaii, and Barack Obama and president. Traditionally, relation extraction has fallen into one of four broad approaches: supervised classification, as in the ACE task (Doddington et al., 2004; GuoDong et al., 2005; Surdeanu and Ciaramita, 2007), distant supervision (Craven and Kumlien, 1999; Wu and Weld, 2007; Mintz et al., 2009; Sun et al., 2011; Roth and Klakow, 2013) deterministic rule-based systems (Soderland, 1997; Grishman and Min, 2010; Chen et al., 2010), and translation from open domain information extraction schema (Riedel et al., 2013). We focus on the first two of these approaches. 2.2 Supervised Relation Extraction Relation extraction can be naturally cast as a supervised classification problem. A corpus of relation mentions is collected, and each mention x is annotated with the relation y, if any, it expresses. The classifier’s output is then aggregated to decide the relations between the two entities. However, annotatin</context>
</contexts>
<marker>Sun, Grishman, Xu, Min, 2011</marker>
<rawString>Ang Sun, Ralph Grishman, Wei Xu, and Bonan Min. 2011. New York University 2011 system for KBP slot filling. In Proceedings of the Text Analytics Conference.</rawString>
</citation>
<citation valid="true">
<authors>
<author>Mihai Surdeanu</author>
<author>Massimiliano Ciaramita</author>
</authors>
<title>Robust information extraction with perceptrons.</title>
<date>2007</date>
<booktitle>In ACE07 Proceedings.</booktitle>
<contexts>
<context position="5048" citStr="Surdeanu and Ciaramita, 2007" startWordPosition="787" endWordPosition="791"> with the entity mentions for e1 and e2, becomes the input to the relation extractor, which then produces a set of relations which hold between the mentions. We are predominantly interested in the second step – classifying a set of pairs of entity mentions into the relations they express. Figure 1 gives the general setting for relation extraction, with entity pairs Barack Obama and Hawaii, and Barack Obama and president. Traditionally, relation extraction has fallen into one of four broad approaches: supervised classification, as in the ACE task (Doddington et al., 2004; GuoDong et al., 2005; Surdeanu and Ciaramita, 2007), distant supervision (Craven and Kumlien, 1999; Wu and Weld, 2007; Mintz et al., 2009; Sun et al., 2011; Roth and Klakow, 2013) deterministic rule-based systems (Soderland, 1997; Grishman and Min, 2010; Chen et al., 2010), and translation from open domain information extraction schema (Riedel et al., 2013). We focus on the first two of these approaches. 2.2 Supervised Relation Extraction Relation extraction can be naturally cast as a supervised classification problem. A corpus of relation mentions is collected, and each mention x is annotated with the relation y, if any, it expresses. The cla</context>
</contexts>
<marker>Surdeanu, Ciaramita, 2007</marker>
<rawString>Mihai Surdeanu and Massimiliano Ciaramita. 2007. Robust information extraction with perceptrons. In ACE07 Proceedings.</rawString>
</citation>
<citation valid="true">
<authors>
<author>Mihai Surdeanu</author>
<author>Sonal Gupta</author>
<author>John Bauer</author>
<author>David McClosky</author>
<author>Angel X Chang</author>
<author>Valentin I Spitkovsky</author>
<author>Christopher D Manning</author>
</authors>
<title>Stanfords distantlysupervised slot-filling system.</title>
<date>2011</date>
<booktitle>In Proceedings of the Text Analytics Conference.</booktitle>
<contexts>
<context position="22638" citStr="Surdeanu et al. (2011)" startWordPosition="3695" endWordPosition="3698"> and 6.2; experiments are given in Section 6.3. 6.1 Training Setup We adopt the setup of Surdeanu et al. (2012) for training the MIML-RE model, with minor modifications. We use both the 2010 and 2013 KBP official document collections, as well as a July 2013 dump of Wikipedia as our text corpus. We subsample negatives such that s of our dataset consists of entity pairs with no known relations. In all experiments, MIML-RE is trained for 7 iterations of EM; for efficiency, the z classifier is optimized using stochastic gradient descent;2 the y classifiers are optimized using L-BFGS. Similarly to Surdeanu et al. (2011), we assign negative relations which are either incompatible with the known positive relations (e.g., relations whose co-occurrence would violate type constraints); or, are actually functional relations in which another entity already participates. For example, if we know that Obama was born in the United States, we could add born in as a negative relation to the pair Obama and Kenya. Our dataset consists of 325 891 entity pairs with at least one positive relation, and 158 091 entity pairs with no positive relations. Pairs with at least one known relation have an average of 4.56 mentions per g</context>
</contexts>
<marker>Surdeanu, Gupta, Bauer, McClosky, Chang, Spitkovsky, Manning, 2011</marker>
<rawString>Mihai Surdeanu, Sonal Gupta, John Bauer, David McClosky, Angel X Chang, Valentin I Spitkovsky, and Christopher D Manning. 2011. Stanfords distantlysupervised slot-filling system. In Proceedings of the Text Analytics Conference.</rawString>
</citation>
<citation valid="true">
<authors>
<author>Mihai Surdeanu</author>
<author>Julie Tibshirani</author>
<author>Ramesh Nallapati</author>
<author>Christopher D Manning</author>
</authors>
<title>Multiinstance multi-label learning for relation extraction.</title>
<date>2012</date>
<booktitle>In EMNLP.</booktitle>
<contexts>
<context position="8149" citStr="Surdeanu et al. (2012)" startWordPosition="1281" endWordPosition="1284">complete: entity mentions with no known relation are treated as negative examples. The first of these assumptions is addressed by multi-instance multi-label (MIML) learning, described in Section 2.4. Min et al. (2013) address the second assumption by extending the MIML model with additional latent variables, while Xu et al. (2013) allow feedback from a coarse relation extractor to augment labels from the knowledge base. These latter two approaches are compatible with but are not implemented in this work. 2.4 Multi-Instance Multi-Label Learning The multi-instance multi-label (MIML-RE) model of Surdeanu et al. (2012), which builds upon work state of residence state of birth Barack Obama was born in Hawaii. Barack Obama visited Hawaii. The president grew up in Hawaii. title Barack Obama met former president Clinton. Obama became president in 2008. 1557 Figure 2: The MIML-RE model, as shown in Surdeanu et al. (2012). The outer plate corresponds to each of the n entity pairs in our knowledge base. Each entity pair has a set of mention pairs Mi, and a corresponding plate in the diagram for each mention pair in Mi. The variable x represents the input mention pair, whereas y represents the positive and negative</context>
<context position="17453" citStr="Surdeanu et al. (2012)" startWordPosition="2824" endWordPosition="2827">nt proportional to the example’s disagreement. Formally, we define the probability of selecting an example z(m) i to be proportional to the JensenShannon divergence in (1). Since the training set is an approximation to the prior distribution over examples, sampling uniformly over the training set is an approximation to sampling from the prior probability of seeing an input x. We can view our criterion as an approximation to sampling proportional to the product of two densities: a prior over examples x, and the JS divergence mentioned above. 4 Incorporating Sentence-Level Annotations Following Surdeanu et al. (2012), MIML-RE is trained through hard discriminative Expectation Maximization, inferring the latent z values in the E-step and updating the weights for both the z and y classifiers in the M-step. During the E-step, we constrain the latent z to match our sentence-level annotations when available. It is worth noting that even in the hard-EM regime, we can in principle incorporate annotator uncertainty elegantly into the model. At each E step, each zi is set according to [ zi(m)∗ ≈ arg max p(z |x(m) i , wz) × R z∈ p(y(r) |z�i, wyr))J where zz contains the inferred labels from the previous iteration, </context>
<context position="22127" citStr="Surdeanu et al. (2012)" startWordPosition="3607" endWordPosition="3610">f around 75%, as evaluated by a paper author, performing disproportionately well on identifying the no relation label. 6 Experiments We evaluate the three high-level research contributions of this work: we show that we improve the accuracy of MIML-RE, we validate the effectiveness of our selection criteria, and we provide a corpus of annotated examples, evaluating a supervised classifier trained on this corpus. The training and testing methodology for evaluating these contributions is given in Sections 6.1 and 6.2; experiments are given in Section 6.3. 6.1 Training Setup We adopt the setup of Surdeanu et al. (2012) for training the MIML-RE model, with minor modifications. We use both the 2010 and 2013 KBP official document collections, as well as a July 2013 dump of Wikipedia as our text corpus. We subsample negatives such that s of our dataset consists of entity pairs with no known relations. In all experiments, MIML-RE is trained for 7 iterations of EM; for efficiency, the z classifier is optimized using stochastic gradient descent;2 the y classifiers are optimized using L-BFGS. Similarly to Surdeanu et al. (2011), we assign negative relations which are either incompatible with the known positive rela</context>
<context position="23569" citStr="Surdeanu et al. (2012)" startWordPosition="3850" endWordPosition="3853">ates, we could add born in as a negative relation to the pair Obama and Kenya. Our dataset consists of 325 891 entity pairs with at least one positive relation, and 158 091 entity pairs with no positive relations. Pairs with at least one known relation have an average of 4.56 mentions per group; groups with no known relations have an average of 1.55 mentions per group. In total, 1208 524 distinct mentions are considered; the annotated examples are selected from this pool. 6.2 Testing Methodology We compare against the original MIML-RE model using the same dataset and evaluation methodology as Surdeanu et al. (2012). This allows for an evaluation where the only free variable between this and prior work is the predictions of the relation extractor. Additionally, we evaluate the relation extractors in the context of Stanford’s end-to-end KBP system (Angeli et al., 2014) using the NIST TACKBP 2013 English Slotfilling evaluation. In the end-to-end framework, the input to the system is a query entity and a set of articles, and the output is a set of slot fills – each slot fill is a candidate triple in the knowledge base, the first element of which is the query entity. This amounts to roughly populating a data</context>
<context position="30680" citStr="Surdeanu et al. (2012)" startWordPosition="5047" endWordPosition="5050">tently, even hurting performance when the SampleJS criterion was used. This supports an intuition that the initialization of the model is important, and that it is relatively difficult to coax the model out of a local optimum if it is initialized poorly. This is further supported by the improvement in performance when the model is initialized with a supervised classifier, even when no examples are used during training. Similar trends are reported in prior work, e.g., Smith and Eisner (2007) Section 4.4.6. 0 0.05 0.1 0.15 0.2 0.25 0.3 Recall Figure 4: MIML-RE and Mintz++ evaluated according to Surdeanu et al. (2012). The original model from the paper is plotted for comparison, as our training methodology is somewhat different. 0 0.05 0.1 0.15 0.2 0.25 0.3 Recall Figure 5: Our best active learning criterion evaluated against our version of MIML-RE, alongside the best system of Pershina et al. (2014). Also interesting is the relatively small gain MIML-RE provides over traditional distant supervision (Mintz++) in this setting. We conjecture that the mistakes made by Mintz++ are often relatively easily filtered by the downstream consistency component. This is supported by Figure 4; we evaluate our trained MI</context>
<context position="31925" citStr="Surdeanu et al. (2012)" startWordPosition="5253" endWordPosition="5256">tz++ and the results reported in Surdeanu et al. (2012). We show that our model performs as well or better than the original implementation, and consistently outperforms Mintz++. Evaluate Selection Criteria A key objective of this work is to evaluate how much of an impact careful selection of annotated examples has on the overall performance of the system. We evaluate the three selection criteria from Section 3.2, showing the results for MIML-RE in Table 3; results for the supervised classifier are given in Table 1. In both cases, we show that the sampled JS cri0.7 0.6 0.5 0.4 0.3 0.2 MIML-RE Surdeanu et al. (2012) Mintz++ Precision 0.7 0.6 0.5 0.4 0.3 0.2 Sample JS Pershina et al. (2014) MIML-RE 1563 Precision terion performs comparably to or better than the other criteria. At least two interesting trends can be noted from these results: First, the uniformly sampled criterion performed worse than MIML-RE initialized with a supervised classifier. This may be due to noise in the annotation: a small number of annotation errors on entity pairs with only a single corresponding mention could introduce dangerous noise into training. These singleton mentions will rarely have disagreement between the committee </context>
<context position="33715" citStr="Surdeanu et al. (2012)" startWordPosition="5535" endWordPosition="5538">dding more direct supervision. They conclude that annotations provide only a relatively small improvement in performance. However, their examples were uniformly selected from the training corpus, and did not make use of the structure provided by MIML-RE. Our results agree in that neither the uniform selection criterion nor the supervised classifier significantly outperformed the unmodified MIML-RE model; nonetheless, we show that if care is taken in selecting these labeled examples we can achieve noticeable improvements in accuracy. We also evaluate our selection criteria on the evaluation of Surdeanu et al. (2012), both initialized with Mintz++ (Figure 7) and with the supervised classifier (Figure 6). These results mirror those in the end-to-end evaluation; when initialized with the supervised classifier the high disagreement (High JS) and sampling proportional to disagreement (Sample JS) criteria clearly outperform both the base MIML-RE model as well as the uniformly sampling criterion. Using the annotated examples only during training yielded no perceivable benefit over the base model (Figure 7). Supervised Relation Extractor The examples collected can be used to directly train a supervised classifie</context>
</contexts>
<marker>Surdeanu, Tibshirani, Nallapati, Manning, 2012</marker>
<rawString>Mihai Surdeanu, Julie Tibshirani, Ramesh Nallapati, and Christopher D. Manning. 2012. Multiinstance multi-label learning for relation extraction. In EMNLP.</rawString>
</citation>
<citation valid="true">
<authors>
<author>Fei Wu</author>
<author>Daniel S Weld</author>
</authors>
<title>Autonomously semantifying wikipedia.</title>
<date>2007</date>
<booktitle>In Proceedings of the sixteenth ACM conference on information and knowledge management.</booktitle>
<publisher>ACM.</publisher>
<contexts>
<context position="5114" citStr="Wu and Weld, 2007" startWordPosition="798" endWordPosition="801">ractor, which then produces a set of relations which hold between the mentions. We are predominantly interested in the second step – classifying a set of pairs of entity mentions into the relations they express. Figure 1 gives the general setting for relation extraction, with entity pairs Barack Obama and Hawaii, and Barack Obama and president. Traditionally, relation extraction has fallen into one of four broad approaches: supervised classification, as in the ACE task (Doddington et al., 2004; GuoDong et al., 2005; Surdeanu and Ciaramita, 2007), distant supervision (Craven and Kumlien, 1999; Wu and Weld, 2007; Mintz et al., 2009; Sun et al., 2011; Roth and Klakow, 2013) deterministic rule-based systems (Soderland, 1997; Grishman and Min, 2010; Chen et al., 2010), and translation from open domain information extraction schema (Riedel et al., 2013). We focus on the first two of these approaches. 2.2 Supervised Relation Extraction Relation extraction can be naturally cast as a supervised classification problem. A corpus of relation mentions is collected, and each mention x is annotated with the relation y, if any, it expresses. The classifier’s output is then aggregated to decide the relations betwee</context>
</contexts>
<marker>Wu, Weld, 2007</marker>
<rawString>Fei Wu and Daniel S Weld. 2007. Autonomously semantifying wikipedia. In Proceedings of the sixteenth ACM conference on information and knowledge management. ACM.</rawString>
</citation>
<citation valid="true">
<authors>
<author>Wei Xu</author>
<author>Raphael Hoffman Le Zhao</author>
<author>Ralph Grishman</author>
</authors>
<title>Filling knowledge base gaps for distant supervision of relation extraction.</title>
<date>2013</date>
<booktitle>In ACL.</booktitle>
<marker>Xu, Le Zhao, Grishman, 2013</marker>
<rawString>Wei Xu, Le Zhao, Raphael Hoffman, and Ralph Grishman. 2013. Filling knowledge base gaps for distant supervision of relation extraction. In ACL.</rawString>
</citation>
<citation valid="true">
<authors>
<author>Ce Zhang</author>
<author>Feng Niu</author>
<author>Christopher R´e</author>
<author>Jude Shavlik</author>
</authors>
<title>Big data versus the crowd: Looking for relationships in all the right places.</title>
<date>2012</date>
<booktitle>In ACL.</booktitle>
<marker>Zhang, Niu, R´e, Shavlik, 2012</marker>
<rawString>Ce Zhang, Feng Niu, Christopher R´e, and Jude Shavlik. 2012. Big data versus the crowd: Looking for relationships in all the right places. In ACL.</rawString>
</citation>
</citationList>
</algorithm>
</algorithms>
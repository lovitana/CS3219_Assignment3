<?xml version="1.0" encoding="UTF-8"?>
<algorithms version="110505">
<algorithm name="SectLabel" version="110505">
<variant no="0" confidence="0.000463">
<title confidence="0.995064">
Recursive Deep Models for Discourse Parsing
</title>
<author confidence="0.998956">
Jiwei Li&apos;, Rumeng Li2 and Eduard Hovy3
</author>
<affiliation confidence="0.999589">
&apos;Computer Science Department, Stanford University, Stanford, CA 94305, USA
2School of EECS, Peking University, Beijing 100871, P.R. China
3Language Technology Institute, Carnegie Mellon University, Pittsburgh, PA 15213, USA
</affiliation>
<email confidence="0.997321">
jiweil@stanford.edu alicerumeng@foxmail.com ehovy@andrew.cmu.edu
</email>
<sectionHeader confidence="0.993876" genericHeader="abstract">
Abstract
</sectionHeader>
<bodyText confidence="0.9998180625">
Text-level discourse parsing remains a
challenge: most approaches employ fea-
tures that fail to capture the intentional, se-
mantic, and syntactic aspects that govern
discourse coherence. In this paper, we pro-
pose a recursive model for discourse pars-
ing that jointly models distributed repre-
sentations for clauses, sentences, and en-
tire discourses. The learned representa-
tions can to some extent learn the seman-
tic and intentional import of words and
larger discourse units automatically,. The
proposed framework obtains comparable
performance regarding standard discours-
ing parsing evaluations when compared
against current state-of-art systems.
</bodyText>
<sectionHeader confidence="0.998992" genericHeader="keywords">
1 Introduction
</sectionHeader>
<bodyText confidence="0.999980451612903">
In a coherent text, units (clauses, sentences, and
larger multi-clause groupings) are tightly con-
nected semantically, syntactically, and logically.
Mann and Thompson (1988) define a text to be
coherent when it is possible to describe clearly
the role that each discourse unit (at any level of
grouping) plays with respect to the whole. In a
coherent text, no unit is completely isolated. Dis-
course parsing tries to identify how the units are
connected with each other and thereby uncover the
hierarchical structure of the text, from which mul-
tiple NLP tasks can benefit, including text sum-
marization (Louis et al., 2010), sentence compres-
sion (Sporleder and Lapata, 2005) or question-
answering (Verberne et al., 2007).
Despite recent progress in automatic discourse
segmentation and sentence-level parsing (e.g.,
(Fisher and Roark, 2007; Joty et al., 2012; Sori-
cut and Marcu, 2003), document-level discourse
parsing remains a significant challenge. Recent
attempts (e.g., (Hernault et al., 2010b; Feng and
Hirst, 2012; Joty et al., 2013)) are still consid-
erably inferior when compared to human gold-
standard discourse analysis. The challenge stems
from the fact that compared with sentence-level
dependency parsing, the set of relations between
discourse units is less straightforward to define.
Because there are no clause-level ‘parts of dis-
course’ analogous to word-level parts of speech,
there is no discourse-level grammar analogous to
sentence-level grammar. To understand how dis-
course units are connected, one has to understand
the communicative function of each unit, and the
role it plays within the context that encapsulates it,
taken recursively all the way up for the entire text.
Manually developed features relating to words and
other syntax-related cues, used in most of the re-
cent prevailing approaches (e.g., (Feng and Hirst,
2012; Hernault et al., 2010b)), are insufficient for
capturing such nested intentionality.
Recently, deep learning architectures have been
applied to various natural language processing
tasks (for details see Section 2) and have shown
the advantages to capture the relevant semantic
and syntactic aspects of units in context. As word
distributions are composed to form the meanings
of clauses, the goal is to extend distributed clause-
level representations to the single- and multi-
sentence (discourse) levels, and produce the hier-
archical structure of entire texts.
Inspired by this idea, we introduce in this pa-
per a deep learning approach for discourse pars-
ing. The proposed parsing algorithm relies on
a recursive neural network to decide (1) whether
two discourse units are connected and if so (2)
by what relation they are connected. Concretely,
the parsing algorithm takes as input a document of
any length, and first obtains the distributed repre-
sentation for each of its sentences using recursive
convolution based on the sentence parse tree. It
then proceeds bottom-up, applying a binary clas-
sifier to determine the probability of two adjacent
</bodyText>
<page confidence="0.935565">
2061
</page>
<note confidence="0.8967985">
Proceedings of the 2014 Conference on Empirical Methods in Natural Language Processing (EMNLP), pages 2061–2069,
October 25-29, 2014, Doha, Qatar. c�2014 Association for Computational Linguistics
</note>
<bodyText confidence="0.99987415">
discourse units being merged to form a new sub-
tree followed by a multi-class classifier to select
the appropriate discourse relation label, and cal-
culates the distributed representation for the sub-
tree so formed, gradually unifying subtrees un-
til a single overall tree spans the entire sentence.
The compositional distributed representation en-
ables the parser to make accurate parsing decisions
and capture relations between different sentences
and units. The binary and multi-class classifiers,
along with parameters involved in convolution, are
jointly trained from a collection of gold-standard
discourse structures.
The rest of this paper is organized as follows.
We present related work in Section 2 and de-
scribe the RST Discourse Treebank in Section 3.
The sentence convolution approach is illustrated in
Section 4 and the discourse parser model in Sec-
tion 5. We report experimental results in Section 6
and conclude in Section 7.
</bodyText>
<sectionHeader confidence="0.999909" genericHeader="introduction">
2 Related Work
</sectionHeader>
<subsectionHeader confidence="0.999912">
2.1 Discourse Analysis and Parsing
</subsectionHeader>
<bodyText confidence="0.999902605633803">
The basis of discourse structure lies in the recog-
nition that discourse units (minimally, clauses) are
related to one another in principled ways, and that
the juxtaposition of two units creates a joint mean-
ing larger than either unit’s meaning alone. In a
coherent text this juxtaposition is never random,
but serves the speaker’s communicative goals.
Considerable work on linguistic and computa-
tional discourse processing in the 1970s and 80s
led to the development of several proposals for re-
lations that combine units; for a compilation see
(Hovy and Maier, 1997). Of these the most influ-
ential is Rhetorical Structure Theory RST (Mann
and Thompson, 1988) that defines about 25 rela-
tions, each containing semantic constraints on its
component parts plus a description of the overall
functional/semantic effect produced as a unit when
the parts have been appropriately connected in the
text. For example, the SOLUTIONHOOD relation
connects one unit describing a problem situation
with another describing its solution, using phrases
such as “the answer is”; in successful communi-
cation the reader will understand that a problem is
described and its solution is given.
Since there is no syntactic definition of a prob-
lem or solution (they can each be stated in a sin-
gle clause, a paragraph, or an entire text), one has
to characterize discourse units by their commu-
nicative (rhetorical) function. The functions are
reflected in text as signals of the author’s inten-
tions, and take various forms (including expres-
sions such as “therefore”, “for example”, “the an-
swer is”, and so on; patterns of tense or pronoun
usage; syntactic forms; etc.). The signals govern
discourse blocks ranging from a clause to an en-
tire text, each one associated with some discourse
relation.
In order to build a text’s hierarchical structure,
a discourse parser needs to recognize these signals
and use them to appropriately compose the rela-
tionship and nesting. Early approaches (Marcu,
2000a; LeThanh et al., 2004) rely mainly on overt
discourse markers (or cue words) and use hand-
coded rules to build text structure trees, bottom-up
from clauses to sentences to paragraphs.... Since
a hierarchical discourse tree structure is analo-
gous to a constituency based syntactic tree, mod-
ern research explored syntactic parsing techniques
(e.g., CKY) for discourse parsing based on mul-
tiple text-level or sentence-level features (Soricut
and Marcu, 2003; Reitter, 2003; Baldridge and
Lascarides, 2005; Subba and Di Eugenio, 2009;
Lin et al., 2009; Luong et al., 2014).
A recent prevailing idea for discourse parsing
is to train two classifiers, namely a binary struc-
ture classifier for determining whether two adja-
cent text units should be merged to form a new
subtree, followed by a multi-class relation classi-
fier for determining which discourse relation label
should be assigned to the new subtree. The idea is
proposed by Hernault and his colleagues (Duverle
and Prendinger, 2009; Hernault et al., 2010a) and
followed by other work using more sophisticated
features (Feng and Hirst, 2012; Hernault et al.,
2010b). Current state-of-art performance for re-
lation identification is achieved by the recent rep-
resentation learning approach proposed by (Ji and
Eisenstein, 2014). The proposed framework pre-
sented in this paper is similar to (Ji and Eisenstein,
2014) for transforming the discourse units to the
abstract representations.
</bodyText>
<subsectionHeader confidence="0.996979">
2.2 Recursive Deep Learning
</subsectionHeader>
<bodyText confidence="0.999971666666667">
Recursive neural networks constitute one type of
deep learning frameworks which was first pro-
posed in (Goller and Kuchler, 1996). The recur-
sive framework relies and operates on structured
inputs (e.g., a parse tree) and computes the rep-
resentation for each parent based on its children
</bodyText>
<page confidence="0.991418">
2062
</page>
<bodyText confidence="0.996145142857143">
iteratively in a bottom-up fashion. A series of vari-
ations of RNN has been proposed to tailor differ-
ent task-specific requirements, including Matrix-
Vector RNN (Socher et al., 2012) that represents
every word as both a vector and a matrix, or Recur-
sive Neural Tensor Network (Socher et al., 2013)
that allows the model to have greater interactions
between the input vectors. Many tasks have ben-
efited from the recursive framework, including
parsing (Socher et al., 2011b), sentiment analysis
(Socher et al., 2013), textual entailment (Bowman,
2013), segmentation (Wang and Mansur, 2013;
Houfeng et al., 2013), and paraphrase detection
(Socher et al., 2011a).
</bodyText>
<sectionHeader confidence="0.996919" genericHeader="method">
3 The RST Discourse Treebank
</sectionHeader>
<bodyText confidence="0.999974675">
There are today two primary alternative discourse
treebanks suitable for training data: the Rhetor-
ical Structure Theory Discourse Treebank RST-
DT (Carlson et al., 2003) and the Penn Discourse
Treebank (Prasad et al., 2008). In this paper, we
select the former. In RST (Mann and Thompson,
1988), a coherent context or a document is repre-
sented as a hierarchical tree structure, the leaves
of which are clause-sized units called Elementary
Discourse Units (EDUs). Adjacent nodes (siblings
in the tree) are linked with discourse relations that
are either binary (hypotactic) or multi-child (parat-
actic). One child of each hypotactic relation is al-
ways more salient (called the NUCLEUS); its sib-
ling (the SATELLITE) is less salient compared and
may be omitted in summarization. Multi-nuclear
relations (e.g., CONJUNCTION) exhibit no distinc-
tion of salience between the units.
The RST Discourse Treebank contains 385 an-
notated documents (347 for training and 38 for
testing) from the Wall Street Journal. A total
of 110 fine-grained relations defined in (Marcu,
2000b) are used for tagging relations in RST-DT.
They are subtypes of 18 original high-level RST
categories. For fair comparison with existing sys-
tems, we use in this work the 18 coarse-grained re-
lation classes, which with nuclearity attached form
a set of 41 distinct relations. Non-binary relations
are converted into a cascade of right-branching bi-
nary relations.
Conventionally, discourse parsing in RST-DT
involves the following sub-tasks: (1) EDU seg-
mentation to segment the raw text into EDUs, (2)
tree-building. Since the segmentation task is es-
sentially clause delimitation and hence relatively
easy (with state-of-art accuracy at most 95%),
we focus on the latter problem. We assume that
the gold-standard EDU segmentations are already
given, as assumed in other past work (Feng and
Hirst, 2012).
</bodyText>
<sectionHeader confidence="0.996295" genericHeader="method">
4 EDU Model
</sectionHeader>
<bodyText confidence="0.999936857142857">
In this section, we describe how we compute
the distributed representation for a given sentence
based on its parse tree structure and contained
words. Our implementation is based on (Socher
et al., 2013). As the details can easily be found
there, we omit them for brevity.
Let s denote any given sentence, comprised of a
sequence of tokens s = {w1, w2, ..., wn3}, where
ns denotes the number of tokens in s. Each to-
ken w is associated with a specific vector embed-
ding e, = {e1�, e2 �, ..., e�� }, where K denotes the
dimension of the word embedding. We wish to
compute the vector representation hs for current
sentence, where hs = {h1s, h2s, ..., hs }.
Parse trees are obtained using the Stanford
Parser1, and each clause is treated as an EDU. For
a given parent p in the tree and its two children c1
(associated with vector representation h,1) and c2
(associated with vector representation h,2), stan-
dard recursive networks calculate the vector for
parent p as follows:
</bodyText>
<equation confidence="0.963148">
hp = f(W · [h,1, h,2] + b) (1)
</equation>
<bodyText confidence="0.999982571428571">
where [h,1, h,2] denotes the concatenating vector
for children representations h,1 and h,2; W is a
K x 2K matrix and b is the 1 x K bias vector;
and f(·) is the function tanh. Recursive neural
models compute parent vectors iteratively until the
root node’s representation is obtained, and use the
root embedding to represent the whole sentence.
</bodyText>
<sectionHeader confidence="0.988113" genericHeader="method">
5 Discourse Parsing
</sectionHeader>
<bodyText confidence="0.999979222222222">
Since recent work (Feng and Hirst, 2012; Hernault
et al., 2010b) has demonstrated the advantage of
combining the binary structure classifier (deter-
mining whether two adjacent text units should be
merged to form a new subtree) with the multi-class
classifier (determining which discourse relation la-
bel to assign to the new subtree) over the older
single multi-class classifier with the additional la-
bel NO-REL, our approach follows the modern
</bodyText>
<footnote confidence="0.973185">
1http://nlp.stanford.edu/software/
lex-parser.shtml
</footnote>
<page confidence="0.949444">
2063
</page>
<figureCaption confidence="0.999834">
Figure 1: RST Discourse Tree Structure.
</figureCaption>
<bodyText confidence="0.9774625">
strategy but trains binary and multi-class classi-
fiers jointly based on the discourse structure tree.
Figure 2 illustrates the structure of a discourse
parse tree. Each node e in the tree is associated
with a distributed vector he. e1, e2, e3 and e6
constitute the leaves of trees, the distributed vec-
tor representations of which are assumed to be al-
ready obtained from convolution in Section 4. Let
Nr denote the number of relations and we have
Nr = 41.
</bodyText>
<subsectionHeader confidence="0.99637">
5.1 Binary (Structure) Classification
</subsectionHeader>
<bodyText confidence="0.9998985">
In this subsection, we train a binary (structure)
classifier, which aims to decide whether two EDUs
or spans should be merged during discourse tree
reconstruction.
Let tbinary(ei, ej) be the binary valued variable
indicating whether ei and ej are related, or in other
words, whether a certain type of discourse rela-
tions holds between ei and ej. According to Fig-
ure 2, the following pairs constitute the training
data for binary classification:
</bodyText>
<equation confidence="0.999917">
tbinary(e1, e2) = 1, tbinary(e3, e4) = 1,
tbinary(e2, e3) = 0, tbinary(e3, e6) = 0,
tbinary(e5, e6) = 1
</equation>
<bodyText confidence="0.999951125">
To train the binary classifier, we adopt a three-
layer neural network structure, i.e., input layer,
hidden layer, and output layer. Let H = [hei, hej]
denote the concatenating vector for two spans ei
and ej. We first project the concatenating vector
H to the hidden layer with Nbinary hidden neurons.
The hidden layer convolutes the input with non-
linear tanh function as follows:
</bodyText>
<equation confidence="0.877315">
Lbinary
(ei,ej) = f(Gbinary * [hei, hej] + bbinary)
</equation>
<bodyText confidence="0.7458392">
where Gbinary is an Nbinary * 2K convolution ma-
trix and bbinary denotes the bias vector.
The output layer takes as input Lbinary
(ei,ej) and gen-
erates a scalar using the linear function Ubinary ·
</bodyText>
<sectionHeader confidence="0.375453" genericHeader="method">
Lbinary
</sectionHeader>
<bodyText confidence="0.99097825">
(ei,ej) + b. A sigmod function is then adopted to
project the value to a [0,1] probability space. The
execution at the output layer can be summarized
as:
</bodyText>
<equation confidence="0.983041333333333">
p[tbinary(ei, ej) = 1] = g(Ubinary·Lbinary
(ei,ej)+b∗ binary)
(2)
</equation>
<bodyText confidence="0.750136333333333">
where Ubinary is an Nbinary x 1 vector and bbinary
∗
denotes the bias. g(·) is the sigmod function.
</bodyText>
<subsectionHeader confidence="0.996714">
5.2 Multi-class Relation Classification
</subsectionHeader>
<bodyText confidence="0.99995775">
If tbinary(ei, ej) is determined to be 1, we next
use variable r(ei, ej) to denote the index of rela-
tion that holds between ei and ej. A multi-class
classifier is train based on a three-layer neural net-
work, in the similar way as binary classification in
Section 5.1. Concretely, a matrix GMulti and bias
vector bMulti are first adopted to convolute the con-
catenating node vectors to the hidden layer vector
</bodyText>
<equation confidence="0.941342">
Lmulti (ei,ej):
ei,ej) = f(Gmulti * [hei, hej] + bmulti) (3)
</equation>
<bodyText confidence="0.999743">
We then compute the posterior probability over
labels given the hidden layer vector L using the
softmax and obtain the Nr dimensional probabil-
ity vector P(e1,e2) for each EDU pair as follows:
</bodyText>
<equation confidence="0.999774">
S(ei,ej) = Umulti · Lmulti (4)
(ei,ej)
Ek exp(S(e1,e2))(k)
exp(S(e1,e2)(i)) (5)
</equation>
<bodyText confidence="0.99990625">
where Umulti is the Nr x 2K matrix. The ith ele-
ment in P(e1,e2) denotes the probability that ith re-
lation holds between ei and ej. To note, binary and
multi-class classifiers are trained independently.
</bodyText>
<subsectionHeader confidence="0.988849">
5.3 Distributed Vector for Spans
</subsectionHeader>
<bodyText confidence="0.999935545454546">
What is missing in the previous two subsections
are the distributed vectors for non-leaf nodes (i.e.,
e4 and e5 in Figure 1), which serve as structure and
relation classification. Again, we turn to recursive
deep learning network to obtain the distributed
vector for each node in the tree in a bottom-up
fashion.
Similar as for sentence parse-tree level compo-
sitionally, we extend a standard recursive neural
network by associating each type of relations r
with one specific K x 2K convolution matrix Wr.
</bodyText>
<equation confidence="0.972855">
Lmulti
(
P(e1,e2)(i) =
</equation>
<page confidence="0.974877">
2064
</page>
<figureCaption confidence="0.999813">
Figure 2: System Overview.
</figureCaption>
<bodyText confidence="0.999968857142857">
The representation for each node within the tree is
calculated based on the representations for its chil-
dren in a bottom-up fashion. Concretely, for a par-
ent node p, given the distributed representation hei
for left child, hej for right child, and the relation
r(e1, e2), its distributed vector hp is calculated as
follows:
</bodyText>
<equation confidence="0.995123">
hp = f(Wr(e1,e2) · [hei, hej] + br(e1,e2)) (6)
</equation>
<bodyText confidence="0.999846166666667">
where br(e1,e2) is the bias vector and f(·) is the
non-linear tanh function.
To note, our approach does not make any dis-
tinction between within-sentence text spans and
cross-sentence text spans, different from (Feng
and Hirst, 2012; Joty et al., 2013)
</bodyText>
<subsectionHeader confidence="0.990373">
5.4 Cost Function
</subsectionHeader>
<bodyText confidence="0.99989725">
The parameters to optimize include sentence-
level convolution parameters [W, b],
discourse-level convolution parameters
[{Wr}, {br}], binary classification parameters
[Gbinary, bbinary, Ubinary, b∗binary], and multi-class
parameters [Gmulti, bmulti, Umulti].
Suppose we have M1 binary training samples
and M2 multi-class training examples (M2 equals
the number of positive examples in M1, which
is also the non-leaf nodes within the training dis-
course trees). The cost function for our framework
with regularization on the training set is given by:
</bodyText>
<equation confidence="0.941686230769231">
�J(Obinary) = Jbinary(ei, ej)
(ei,ej)E{binary}
�+ Qbinary · θ2
BEObinary
(7)
�J(Omulti) = Jmulti(ei, ej)
(ei,ej)E{multi}
�+ Qmulti · θ2
BEOmulti
where
Jbinary(ei, ej) = −t(ei, ej) logp(t(ei, ej) = 1)
− (1 − t(ei, ej)) log[1 − p(t(ei, ej) = 1)]
Jmulti(ei, ej) = − log[p(r(ei, ej) = r)]
</equation>
<subsectionHeader confidence="0.922614">
5.5 Backward Propagation
</subsectionHeader>
<bodyText confidence="0.999874818181818">
The derivative for parameters involved is com-
puted through backward propagation. Here we
illustrate how we compute the derivative of
Jmulti(ei, ej) with respect to different parameters.
For each pair of nodes (ei, ej) ∈ multi, we
associate it with a Nr dimensional binary vector
R(ei, ej), which denotes the ground truth vector
with a 1 at the correct label r(ei, ej) and all other
entries 0. Integrating softmax error vector, for any
parameter θ, the derivative of Jmulti(ei, ej) with re-
spect to θ is given by:
</bodyText>
<equation confidence="0.736463">
aS(ei,ej)
aθ
</equation>
<bodyText confidence="0.999023375">
where ⊗ denotes the Hadamard product between
the two vectors. Each training pair recursively
backpropagates its error to some node in the dis-
course tree through [{Wr}, {br}], and then to
nodes in sentence parse tree through [W, b], and
the derivatives can be obtained according to stan-
dard backpropagation (Goller and Kuchler, 1996;
Socher et al., 2010).
</bodyText>
<equation confidence="0.8492725">
aJmulti(ei, ej) = [P(ei,ej) − R(ei,ej)] ⊗
aθ
</equation>
<page confidence="0.971243">
2065
</page>
<subsectionHeader confidence="0.972859">
5.6 Additional Features
</subsectionHeader>
<bodyText confidence="0.999775857142857">
When determining the structure/multi relation be-
tween individual EDUs, additional features are
also considered, the usefulness of which has been
illustrated in a bunch of existing work (Feng and
Hirst, 2012; Hernault et al., 2010b; Joty et al.,
2012). We consider the following simple text-level
features:
</bodyText>
<listItem confidence="0.893755">
• Tokens at the beginning and end of the EDUs.
• POS at the beginning and end of the EDUs.
• Whether two EDUs are in the same sentence.
5.7 Optimization
</listItem>
<bodyText confidence="0.999941">
We use the diagonal variant of AdaGrad (Duchi et
al., 2011) with minibatches, which is widely ap-
plied in deep learning literature (e.g.,(Socher et
al., 2011a; Pei et al., 2014)). The learning rate
in AdaGrad is adapted differently for different pa-
rameters at different steps. Concretely, let giτ de-
note the subgradient at time step t for parameter
Bi obtained from backpropagation, the parameter
update at time step t is given by:
</bodyText>
<equation confidence="0.988768333333333">
α i
Bτ = Bτ−1 − T i2 gτ (11)
ET ✓gτ
</equation>
<bodyText confidence="0.999116666666667">
where α denotes the learning rate and is set to 0.01
in our approach.
Elements in {Wr}, W, Gbinary, Gmulti, Ubinary,
Umulti are initialized by randomly drawing from
the uniform distribution [−E, E], where E is calcu-
lated as suggested in (Collobert et al., 2011). All
bias vectors are initialized with 0. Word embed-
dings {e} are borrowed from Senna (Collobert et
al., 2011; Collobert, 2011).
</bodyText>
<subsectionHeader confidence="0.568497">
5.8 Inference
</subsectionHeader>
<bodyText confidence="0.999959652173913">
For inference, the goal is to find the most proba-
ble discourse tree given the EDUs within the doc-
ument. Existing inference approach basically in-
clude the approach adopted in (Feng and Hirst,
2012; Hernault et al., 2010b) that merges the most
likely spans at each step and SPADE (Fisher and
Roark, 2007) that first finds the tree structure that
is globally optimal, then assigns the most probable
relations to the internal nodes.
In this paper, we implement a probabilistic
CKY-like bottom-up algorithm for computing the
most likely parse tree using dynamic program-
ming as are adopted in (Joty et al., 2012; Joty
et al., 2013; Jurafsky and Martin, 2000) for the
search of global optimum. For a document with
n EDUs, as different relations are characterized
with different compositions (thus leading to dif-
ferent vectors), we use a Nr x n x n dynamic pro-
gramming table Pr, the cell Pr[r, i, j] of which
represents the span contained EDUs from i to j
and stores the probability that relation r holds be-
tween the two spans within i to j. Pr[r, i, j] is
computed as follows:
</bodyText>
<equation confidence="0.9162154">
Pr[r, i, j] =maxr1,r2,kPr[r1, i, k] · Pr[r2, k, j]
xP(tbinary(e[i,k],e[k,j]) = 1)
xP(r(e[i,k],e[k,j]) = 1)
(12)
At each merging step, a distributed vector for the
</equation>
<bodyText confidence="0.997349666666667">
merged point is calculated according to Eq. 13 for
different relations. The CKY-like algorithms finds
the global optimal. To note, the worst-case run-
ning time of our inference algorithm is O(N2r n3),
where n denotes the number of sentences within
the document, which is much slower than the
greedy search. In this work, for simplification, we
simplify the framework by maintaining the top 10
options at each step.
</bodyText>
<sectionHeader confidence="0.999502" genericHeader="evaluation">
6 Experiments
</sectionHeader>
<bodyText confidence="0.99993">
A measure of the performance of the system is
realized by comparing the structure and labeling
of the RS-tree produced by our algorithm to gold-
standard annotations.
Standard evaluation of discourse parsing output
computes the ratio of the number of identical tree
constituents shared in the generated RS-trees and
the gold-standard trees against the total number
of constituents in the generated discourse trees2,
which is further divided to three matrices: Span
(on the blank tree structure), nuclearity (on the
tree structure with nuclearity indication), and rela-
tion (on the tree structure with rhetorical relation
indication but no nuclearity indication).
The nuclearity and relation decisions are made
based on the multi-class output labels from the
deep learning framework. As we do not consider
nuclearity when classifying different discourse re-
lations, the two labels attribute[N][S] and at-
tribute[S][N] made by multi-class classifier will
be treated as the same relation label ATTRIBUTE.
</bodyText>
<footnote confidence="0.54543425">
2Conventionally, evaluation matrices involve precision,
recall and F-score in terms of the comparison between tree
structures. But these are the same when manual segmenta-
tion is used (Marcu, 2000b).
</footnote>
<page confidence="0.969585">
2066
</page>
<table confidence="0.999959">
Approach Span Nuclearity Relation
HILDA 75.3 60.0 46.8
Joty et al. 82.5 68.4 55.7
Feng and Hirst 85.7 71.0 58.2
Ji and Eisenstein 82.1 71.1 61.6
Unified (with feature) 82.0 70.0 57.1
Ours (no feature) 82.4 69.2 56.8
Ours (with feature) 84.0 70.8 58.6
human 88.7 77.7 65.7
</table>
<tableCaption confidence="0.9783425">
Table 1: Performances for different approaches.
Performances for baselines are reprinted from
(Joty et al., 2013; Feng and Hirst, 2014; Ji and
Eisenstein, 2014).
</tableCaption>
<bodyText confidence="0.96881925">
Also, we do not train a separate classifier for NU-
CLEUS and SATELLITE identification. The nucle-
arity decision is made based on the relation type
produced by the multi-class classifier.
</bodyText>
<subsectionHeader confidence="0.997411">
6.1 Parameter Tuning
</subsectionHeader>
<bodyText confidence="0.999999">
The regularization parameter Q constitutes the
only parameter to tune in our framework. We tune
it on the 347 training documents. Concretely, we
employ a five-fold cross validation on the RST
dataset and tune Q on 5 different values: 0.01,
0.1, 0.5, 1.5, 2.5. The final model was tested on
the testing set after parameter tuning.
</bodyText>
<subsectionHeader confidence="0.999493">
6.2 Baselines
</subsectionHeader>
<bodyText confidence="0.999846361111111">
We compare our model against the following
currently prevailing discourse parsing baselines:
HILDA A discourse parser based on support
vector machine classification introduced by Her-
nault et al. (Hernault et al., 2010b). HILDA uses
the binary and multi-class classifier to reconstruct
the tree structure in a greedy way, where the
most likely nodes are merged at each step. The
results for HILDA are obtained by running the
system with default settings on the same inputs
we provided to our system.
Joty et al The discourse parser introduced by
Joty et al. (Joty et al., 2013). It relies on CRF
and combines intra-sentential and multi-sentential
parsers in two different ways. Joty et al. adopt
the global optimal inference as in our work. We
reported the performance from their paper (Joty et
al., 2013).
Feng and Hirst The linear-time discourse
parser introduced in (Feng and Hirst, 2014) which
relies on two linear-chain CRFs to obtain a se-
quence of discourse constituents.
Ji and Eisenstein The shift-reduce discourse
parser introduced in (Ji and Eisenstein, 2014)
which parses document by relying on the dis-
tributed representations obtained from deep learn-
ing framework.
Additionally, we implemented a simplified ver-
sion of our model called unified where we use
a unified convolutional function with unified pa-
rameters [Wsen, bsen] for span vector computation.
Concretely, for a parent node p, given the dis-
tributed representation hez for left child, hey for
right child, and the relation r(e1, e2), rather than
taking the inter relation between two children, its
distributed vector hp is calculated:
</bodyText>
<equation confidence="0.936539">
hp = f(Wsen · [hez, hey] + bsen) (13)
</equation>
<subsectionHeader confidence="0.980513">
6.3 Performance
</subsectionHeader>
<bodyText confidence="0.999964666666667">
Performances for different models approaches re-
ported in Table 1. And as we can observe, al-
though the proposed framework obtains compa-
rable result compared with existing state-of-state
performances regarding all evaluating parameters
for discourse parsing. Specifically, as for the three
measures, no system achieves top performance on
all three, though some systems outperform all oth-
ers for one of the measures. The proposed system
achieves high overall performance on all three, al-
though it does not achieve top score on any mea-
sure. The system gets a little bit performance
boost by considering text-level features illustrated
in Section 5.6. The simplified version of the orig-
inal model underperforms against the original ap-
proach due to lack of expressive power in convo-
lution. Performance plummets when different re-
lations are uniformly treated, which illustrates the
importance of taking into consideration different
types of relations in the span convolution proce-
dure.
</bodyText>
<sectionHeader confidence="0.99377" genericHeader="conclusions">
7 Conclusion
</sectionHeader>
<bodyText confidence="0.999364125">
In this paper, we describe an RST-style text-level
discourse parser based on a neural network model.
The incorporation of sentence-level distributed
vectors for discourse analysis obtains compara-
ble performance compared with current state-of-
art discourse parsing system.
Our future work will focus on extending
discourse-level distributed presentations to related
</bodyText>
<page confidence="0.980745">
2067
</page>
<bodyText confidence="0.9998426">
tasks, such as implicit discourse relation identifi-
cation or dialogue analysis. Further, once the tree
structure for a document can be determined, the
vector for the entire document can be obtained
in bottom-up fashion, as in this paper. One can
now investigate whether the discourse parse tree
is useful for acquiring a single document-level
vector representation, which would benefit mul-
tiple tasks, such as document classification or
macro-sentiment analysis.
</bodyText>
<sectionHeader confidence="0.99472" genericHeader="acknowledgments">
Acknowledgements
</sectionHeader>
<bodyText confidence="0.9999488">
The authors want to thank Vanessa Wei Feng and
Shafiq Joty for helpful discussions regarding RST
dataset. We also want to thank Richard Socher,
Zhengyan He and Pradeep Dasigi for the clarifica-
tion of deep learning techniques.
</bodyText>
<sectionHeader confidence="0.998823" genericHeader="references">
References
</sectionHeader>
<reference confidence="0.996249581395348">
Jason Baldridge and Alex Lascarides. 2005. Proba-
bilistic head-driven parsing for discourse structure.
In Proceedings of the Ninth Conference on Compu-
tational Natural Language Learning, pages 96–103.
Association for Computational Linguistics.
Samuel R Bowman. 2013. Can recursive neural tensor
networks learn logical reasoning? arXiv preprint
arXiv:1312.6192.
Lynn Carlson, Daniel Marcu, and Mary Ellen
Okurowski. 2003. Building a discourse-tagged cor-
pus in the framework of rhetorical structure theory.
Springer.
Ronan Collobert, Jason Weston, L´eon Bottou, Michael
Karlen, Koray Kavukcuoglu, and Pavel Kuksa.
2011. Natural language processing (almost) from
scratch. The Journal of Machine Learning Re-
search, 12:2493–2537.
Ronan Collobert. 2011. Deep learning for efficient dis-
criminative parsing. In International Conference on
Artificial Intelligence and Statistics, number EPFL-
CONF-192374.
John Duchi, Elad Hazan, and Yoram Singer. 2011.
Adaptive subgradient methods for online learning
and stochastic optimization. The Journal of Ma-
chine Learning Research, 12:2121–2159.
David A Duverle and Helmut Prendinger. 2009. A
novel discourse parser based on support vector ma-
chine classification. In Proceedings of the Joint
Conference of the 47th Annual Meeting of the ACL
and the 4th International Joint Conference on Natu-
ral Language Processing of the AFNLP: Volume 2-
Volume 2, pages 665–673. Association for Compu-
tational Linguistics.
Vanessa Wei Feng and Graeme Hirst. 2012. Text-
level discourse parsing with rich linguistic fea-
tures. In Proceedings of the 50th Annual Meeting
of the Association for Computational Linguistics:
Long Papers-Volume 1, pages 60–68. Association
for Computational Linguistics.
Vanessa Wei Feng and Graeme Hirst. 2014. A lin-
ear time bottom-up discourse parser with constraints
and post-editing. In ACL.
Seeger Fisher and Brian Roark. 2007. The utility of
parse-derived features for automatic discourse seg-
mentation. In ANNUAL MEETING-ASSOCIATION
FOR COMPUTATIONAL LINGUISTICS, vol-
ume 45, page 488.
Christoph Goller and Andreas Kuchler. 1996. Learn-
ing task-dependent distributed representations by
backpropagation through structure. In Neural Net-
works, 1996., IEEE International Conference on,
volume 1, pages 347–352. IEEE.
Hugo Hernault, Danushka Bollegala, and Mitsuru
Ishizuka. 2010a. A semi-supervised approach to
improve classification of infrequent discourse rela-
tions using feature vector extension. In Proceedings
of the 2010 Conference on Empirical Methods in
Natural Language Processing, pages 399–409. As-
sociation for Computational Linguistics.
Hugo Hernault, Helmut Prendinger, Mitsuru Ishizuka,
et al. 2010b. Hilda: a discourse parser using sup-
port vector machine classification. Dialogue &amp; Dis-
course, 1(3).
Wang Houfeng, Longkai Zhang, and Ni Sun. 2013.
Improving chinese word segmentation on micro-
blog using rich punctuations.
Eduard H Hovy and Elisabeth Maier. 1997. Parsimo-
nious or profligate: How many and which discourse
structure relations. Discourse Processes.
Yangfeng Ji and Jacob Eisenstein. 2014. Representa-
tion learning for text-level discourse parsing.
Shafiq Joty, Giuseppe Carenini, and Raymond T
Ng. 2012. A novel discriminative framework for
sentence-level discourse analysis. In Proceedings
of the 2012 Joint Conference on Empirical Methods
in Natural Language Processing and Computational
Natural Language Learning, pages 904–915. Asso-
ciation for Computational Linguistics.
Shafiq Joty, Giuseppe Carenini, Raymond Ng, and
Yashar Mehdad. 2013. Combining intra-and multi-
sentential rhetorical parsing for document-level dis-
course analysis. In Proceedings of the 51st annual
meeting of the association for computational lin-
guistics (ACL), pages 486–496.
Dan Jurafsky and James H Martin. 2000. Speech &amp;
Language Processing. Pearson Education India.
</reference>
<page confidence="0.83847">
2068
</page>
<reference confidence="0.999854039215687">
Huong LeThanh, Geetha Abeysinghe, and Christian
Huyck. 2004. Generating discourse structures for
written texts. In Proceedings of the 20th inter-
national conference on Computational Linguistics,
page 329. Association for Computational Linguis-
tics.
Ziheng Lin, Min-Yen Kan, and Hwee Tou Ng. 2009.
Recognizing implicit discourse relations in the penn
discourse treebank. In Proceedings of the 2009 Con-
ference on Empirical Methods in Natural Language
Processing: Volume 1-Volume 1, pages 343–351.
Association for Computational Linguistics.
Annie Louis, Aravind Joshi, and Ani Nenkova. 2010.
Discourse indicators for content selection in summa-
rization. In Proceedings of the 11th Annual Meeting
of the Special Interest Group on Discourse and Di-
alogue, pages 147–156. Association for Computa-
tional Linguistics.
Minh-Thang Luong, Michael C Frank, and Mark John-
son. 2014. Parsing entire discourses as very long
strings: Capturing topic continuity in grounded lan-
guage learning.
William C Mann and Sandra A Thompson. 1988.
Rhetorical structure theory: Toward a functional the-
ory of text organization. Text, 8(3):243–281.
Daniel Marcu. 2000a. The rhetorical parsing of unre-
stricted texts: A surface-based approach. Computa-
tional Linguistics, 26(3):395–448.
Daniel Marcu. 2000b. The theory and practice of dis-
course parsing and summarization. MIT Press.
Wenzhe Pei, Tao Ge, and Chang Baobao. 2014. Max-
margin tensor neural network for chinese word seg-
mentation. In Proceedings ofACL.
Rashmi Prasad, Nikhil Dinesh, Alan Lee, Eleni Milt-
sakaki, Livio Robaldo, Aravind K Joshi, and Bon-
nie L Webber. 2008. The penn discourse treebank
2.0. In LREC. Citeseer.
David Reitter. 2003. Simple signals for complex
rhetorics: On rhetorical analysis with rich-feature
support vector models. In LDV Forum, volume 18,
pages 38–52.
Richard Socher, Christopher D Manning, and An-
drew Y Ng. 2010. Learning continuous phrase
representations and syntactic parsing with recursive
neural networks. In Proceedings of the NIPS-2010
Deep Learning and Unsupervised Feature Learning
Workshop, pages 1–9.
Richard Socher, Eric H Huang, Jeffrey Pennington,
Andrew Y Ng, and Christopher D Manning. 2011a.
Dynamic pooling and unfolding recursive autoen-
coders for paraphrase detection. In NIPS, vol-
ume 24, pages 801–809.
Richard Socher, Cliff C Lin, Chris Manning, and An-
drew Y Ng. 2011b. Parsing natural scenes and nat-
ural language with recursive neural networks. In
Proceedings of the 28th International Conference on
Machine Learning (ICML-11), pages 129–136.
Richard Socher, Brody Huval, Christopher D Manning,
and Andrew Y Ng. 2012. Semantic compositional-
ity through recursive matrix-vector spaces. In Pro-
ceedings of the 2012 Joint Conference on Empiri-
cal Methods in Natural Language Processing and
Computational Natural Language Learning, pages
1201–1211. Association for Computational Linguis-
tics.
Richard Socher, Alex Perelygin, Jean Y Wu, Jason
Chuang, Christopher D Manning, Andrew Y Ng,
and Christopher Potts. 2013. Recursive deep mod-
els for semantic compositionality over a sentiment
treebank. In Proceedings of the Conference on Em-
pirical Methods in Natural Language Processing
(EMNLP), pages 1631–1642.
Radu Soricut and Daniel Marcu. 2003. Sentence level
discourse parsing using syntactic and lexical infor-
mation. In Proceedings of the 2003 Conference
of the North American Chapter of the Association
for Computational Linguistics on Human Language
Technology-Volume 1, pages 149–156. Association
for Computational Linguistics.
Caroline Sporleder and Mirella Lapata. 2005. Dis-
course chunking and its application to sentence com-
pression. In Proceedings of the conference on Hu-
man Language Technology and Empirical Methods
in Natural Language Processing, pages 257–264.
Association for Computational Linguistics.
Rajen Subba and Barbara Di Eugenio. 2009. An effec-
tive discourse parser that uses rich linguistic infor-
mation. In Proceedings of Human Language Tech-
nologies: The 2009 Annual Conference of the North
American Chapter of the Association for Computa-
tional Linguistics, pages 566–574. Association for
Computational Linguistics.
Suzan Verberne, Lou Boves, Nelleke Oostdijk, and
Peter-Arno Coppen. 2007. Evaluating discourse-
based answer extraction for why-question answer-
ing. In Proceedings of the 30th annual international
ACM SIGIR conference on Research and develop-
ment in information retrieval, pages 735–736. ACM.
Longkai Zhang Houfeng Wang and Xu Sun Mairgup
Mansur. 2013. Exploring representations from un-
labeled data with co-training for chinese word seg-
mentation.
</reference>
<page confidence="0.996347">
2069
</page>
</variant>
</algorithm>
<algorithm name="ParsHed" version="110505">
<variant no="0" confidence="0.436318">
<title confidence="0.845279">Recursive Deep Models for Discourse Parsing Rumeng and Eduard</title>
<address confidence="0.753838">Science Department, Stanford University, Stanford, CA 94305, of EECS, Peking University, Beijing 100871, P.R. Technology Institute, Carnegie Mellon University, Pittsburgh, PA 15213,</address>
<email confidence="0.900473">jiweil@stanford.edualicerumeng@foxmail.comehovy@andrew.cmu.edu</email>
<abstract confidence="0.999860882352941">Text-level discourse parsing remains a challenge: most approaches employ features that fail to capture the intentional, semantic, and syntactic aspects that govern discourse coherence. In this paper, we propose a recursive model for discourse parsing that jointly models distributed representations for clauses, sentences, and entire discourses. The learned representations can to some extent learn the semantic and intentional import of words and larger discourse units automatically,. The proposed framework obtains comparable performance regarding standard discoursing parsing evaluations when compared against current state-of-art systems.</abstract>
</variant>
</algorithm>
<algorithm name="ParsCit" version="110505">
<citationList>
<citation valid="true">
<authors>
<author>Jason Baldridge</author>
<author>Alex Lascarides</author>
</authors>
<title>Probabilistic head-driven parsing for discourse structure.</title>
<date>2005</date>
<booktitle>In Proceedings of the Ninth Conference on Computational Natural Language Learning,</booktitle>
<pages>96--103</pages>
<publisher>Association</publisher>
<institution>for Computational Linguistics.</institution>
<contexts>
<context position="7693" citStr="Baldridge and Lascarides, 2005" startWordPosition="1170" endWordPosition="1173">eds to recognize these signals and use them to appropriately compose the relationship and nesting. Early approaches (Marcu, 2000a; LeThanh et al., 2004) rely mainly on overt discourse markers (or cue words) and use handcoded rules to build text structure trees, bottom-up from clauses to sentences to paragraphs.... Since a hierarchical discourse tree structure is analogous to a constituency based syntactic tree, modern research explored syntactic parsing techniques (e.g., CKY) for discourse parsing based on multiple text-level or sentence-level features (Soricut and Marcu, 2003; Reitter, 2003; Baldridge and Lascarides, 2005; Subba and Di Eugenio, 2009; Lin et al., 2009; Luong et al., 2014). A recent prevailing idea for discourse parsing is to train two classifiers, namely a binary structure classifier for determining whether two adjacent text units should be merged to form a new subtree, followed by a multi-class relation classifier for determining which discourse relation label should be assigned to the new subtree. The idea is proposed by Hernault and his colleagues (Duverle and Prendinger, 2009; Hernault et al., 2010a) and followed by other work using more sophisticated features (Feng and Hirst, 2012; Hernaul</context>
</contexts>
<marker>Baldridge, Lascarides, 2005</marker>
<rawString>Jason Baldridge and Alex Lascarides. 2005. Probabilistic head-driven parsing for discourse structure. In Proceedings of the Ninth Conference on Computational Natural Language Learning, pages 96–103. Association for Computational Linguistics.</rawString>
</citation>
<citation valid="true">
<authors>
<author>Samuel R Bowman</author>
</authors>
<title>Can recursive neural tensor networks learn logical reasoning? arXiv preprint arXiv:1312.6192.</title>
<date>2013</date>
<contexts>
<context position="9493" citStr="Bowman, 2013" startWordPosition="1457" endWordPosition="1458">tes the representation for each parent based on its children 2062 iteratively in a bottom-up fashion. A series of variations of RNN has been proposed to tailor different task-specific requirements, including MatrixVector RNN (Socher et al., 2012) that represents every word as both a vector and a matrix, or Recursive Neural Tensor Network (Socher et al., 2013) that allows the model to have greater interactions between the input vectors. Many tasks have benefited from the recursive framework, including parsing (Socher et al., 2011b), sentiment analysis (Socher et al., 2013), textual entailment (Bowman, 2013), segmentation (Wang and Mansur, 2013; Houfeng et al., 2013), and paraphrase detection (Socher et al., 2011a). 3 The RST Discourse Treebank There are today two primary alternative discourse treebanks suitable for training data: the Rhetorical Structure Theory Discourse Treebank RSTDT (Carlson et al., 2003) and the Penn Discourse Treebank (Prasad et al., 2008). In this paper, we select the former. In RST (Mann and Thompson, 1988), a coherent context or a document is represented as a hierarchical tree structure, the leaves of which are clause-sized units called Elementary Discourse Units (EDUs).</context>
</contexts>
<marker>Bowman, 2013</marker>
<rawString>Samuel R Bowman. 2013. Can recursive neural tensor networks learn logical reasoning? arXiv preprint arXiv:1312.6192.</rawString>
</citation>
<citation valid="true">
<authors>
<author>Lynn Carlson</author>
<author>Daniel Marcu</author>
<author>Mary Ellen Okurowski</author>
</authors>
<title>Building a discourse-tagged corpus in the framework of rhetorical structure theory.</title>
<date>2003</date>
<publisher>Springer.</publisher>
<contexts>
<context position="9800" citStr="Carlson et al., 2003" startWordPosition="1501" endWordPosition="1504">trix, or Recursive Neural Tensor Network (Socher et al., 2013) that allows the model to have greater interactions between the input vectors. Many tasks have benefited from the recursive framework, including parsing (Socher et al., 2011b), sentiment analysis (Socher et al., 2013), textual entailment (Bowman, 2013), segmentation (Wang and Mansur, 2013; Houfeng et al., 2013), and paraphrase detection (Socher et al., 2011a). 3 The RST Discourse Treebank There are today two primary alternative discourse treebanks suitable for training data: the Rhetorical Structure Theory Discourse Treebank RSTDT (Carlson et al., 2003) and the Penn Discourse Treebank (Prasad et al., 2008). In this paper, we select the former. In RST (Mann and Thompson, 1988), a coherent context or a document is represented as a hierarchical tree structure, the leaves of which are clause-sized units called Elementary Discourse Units (EDUs). Adjacent nodes (siblings in the tree) are linked with discourse relations that are either binary (hypotactic) or multi-child (paratactic). One child of each hypotactic relation is always more salient (called the NUCLEUS); its sibling (the SATELLITE) is less salient compared and may be omitted in summariza</context>
</contexts>
<marker>Carlson, Marcu, Okurowski, 2003</marker>
<rawString>Lynn Carlson, Daniel Marcu, and Mary Ellen Okurowski. 2003. Building a discourse-tagged corpus in the framework of rhetorical structure theory. Springer.</rawString>
</citation>
<citation valid="true">
<authors>
<author>Ronan Collobert</author>
<author>Jason Weston</author>
<author>L´eon Bottou</author>
<author>Michael Karlen</author>
<author>Koray Kavukcuoglu</author>
<author>Pavel Kuksa</author>
</authors>
<title>Natural language processing (almost) from scratch.</title>
<date>2011</date>
<journal>The Journal of Machine Learning Research,</journal>
<pages>12--2493</pages>
<contexts>
<context position="20628" citStr="Collobert et al., 2011" startWordPosition="3298" endWordPosition="3301">deep learning literature (e.g.,(Socher et al., 2011a; Pei et al., 2014)). The learning rate in AdaGrad is adapted differently for different parameters at different steps. Concretely, let giτ denote the subgradient at time step t for parameter Bi obtained from backpropagation, the parameter update at time step t is given by: α i Bτ = Bτ−1 − T i2 gτ (11) ET ✓gτ where α denotes the learning rate and is set to 0.01 in our approach. Elements in {Wr}, W, Gbinary, Gmulti, Ubinary, Umulti are initialized by randomly drawing from the uniform distribution [−E, E], where E is calculated as suggested in (Collobert et al., 2011). All bias vectors are initialized with 0. Word embeddings {e} are borrowed from Senna (Collobert et al., 2011; Collobert, 2011). 5.8 Inference For inference, the goal is to find the most probable discourse tree given the EDUs within the document. Existing inference approach basically include the approach adopted in (Feng and Hirst, 2012; Hernault et al., 2010b) that merges the most likely spans at each step and SPADE (Fisher and Roark, 2007) that first finds the tree structure that is globally optimal, then assigns the most probable relations to the internal nodes. In this paper, we implement</context>
</contexts>
<marker>Collobert, Weston, Bottou, Karlen, Kavukcuoglu, Kuksa, 2011</marker>
<rawString>Ronan Collobert, Jason Weston, L´eon Bottou, Michael Karlen, Koray Kavukcuoglu, and Pavel Kuksa. 2011. Natural language processing (almost) from scratch. The Journal of Machine Learning Research, 12:2493–2537.</rawString>
</citation>
<citation valid="true">
<authors>
<author>Ronan Collobert</author>
</authors>
<title>Deep learning for efficient discriminative parsing.</title>
<date>2011</date>
<booktitle>In International Conference on Artificial Intelligence and Statistics, number EPFLCONF-192374.</booktitle>
<contexts>
<context position="20756" citStr="Collobert, 2011" startWordPosition="3321" endWordPosition="3322">rent parameters at different steps. Concretely, let giτ denote the subgradient at time step t for parameter Bi obtained from backpropagation, the parameter update at time step t is given by: α i Bτ = Bτ−1 − T i2 gτ (11) ET ✓gτ where α denotes the learning rate and is set to 0.01 in our approach. Elements in {Wr}, W, Gbinary, Gmulti, Ubinary, Umulti are initialized by randomly drawing from the uniform distribution [−E, E], where E is calculated as suggested in (Collobert et al., 2011). All bias vectors are initialized with 0. Word embeddings {e} are borrowed from Senna (Collobert et al., 2011; Collobert, 2011). 5.8 Inference For inference, the goal is to find the most probable discourse tree given the EDUs within the document. Existing inference approach basically include the approach adopted in (Feng and Hirst, 2012; Hernault et al., 2010b) that merges the most likely spans at each step and SPADE (Fisher and Roark, 2007) that first finds the tree structure that is globally optimal, then assigns the most probable relations to the internal nodes. In this paper, we implement a probabilistic CKY-like bottom-up algorithm for computing the most likely parse tree using dynamic programming as are adopted </context>
</contexts>
<marker>Collobert, 2011</marker>
<rawString>Ronan Collobert. 2011. Deep learning for efficient discriminative parsing. In International Conference on Artificial Intelligence and Statistics, number EPFLCONF-192374.</rawString>
</citation>
<citation valid="true">
<authors>
<author>John Duchi</author>
<author>Elad Hazan</author>
<author>Yoram Singer</author>
</authors>
<title>Adaptive subgradient methods for online learning and stochastic optimization.</title>
<date>2011</date>
<journal>The Journal of Machine Learning Research,</journal>
<pages>12--2121</pages>
<contexts>
<context position="19959" citStr="Duchi et al., 2011" startWordPosition="3180" endWordPosition="3183">996; Socher et al., 2010). aJmulti(ei, ej) = [P(ei,ej) − R(ei,ej)] ⊗ aθ 2065 5.6 Additional Features When determining the structure/multi relation between individual EDUs, additional features are also considered, the usefulness of which has been illustrated in a bunch of existing work (Feng and Hirst, 2012; Hernault et al., 2010b; Joty et al., 2012). We consider the following simple text-level features: • Tokens at the beginning and end of the EDUs. • POS at the beginning and end of the EDUs. • Whether two EDUs are in the same sentence. 5.7 Optimization We use the diagonal variant of AdaGrad (Duchi et al., 2011) with minibatches, which is widely applied in deep learning literature (e.g.,(Socher et al., 2011a; Pei et al., 2014)). The learning rate in AdaGrad is adapted differently for different parameters at different steps. Concretely, let giτ denote the subgradient at time step t for parameter Bi obtained from backpropagation, the parameter update at time step t is given by: α i Bτ = Bτ−1 − T i2 gτ (11) ET ✓gτ where α denotes the learning rate and is set to 0.01 in our approach. Elements in {Wr}, W, Gbinary, Gmulti, Ubinary, Umulti are initialized by randomly drawing from the uniform distribution [−</context>
</contexts>
<marker>Duchi, Hazan, Singer, 2011</marker>
<rawString>John Duchi, Elad Hazan, and Yoram Singer. 2011. Adaptive subgradient methods for online learning and stochastic optimization. The Journal of Machine Learning Research, 12:2121–2159.</rawString>
</citation>
<citation valid="true">
<authors>
<author>David A Duverle</author>
<author>Helmut Prendinger</author>
</authors>
<title>A novel discourse parser based on support vector machine classification.</title>
<date>2009</date>
<booktitle>In Proceedings of the Joint Conference of the 47th Annual Meeting of the ACL and the 4th International Joint Conference on Natural Language Processing of the AFNLP: Volume</booktitle>
<volume>2</volume>
<pages>665--673</pages>
<publisher>Association</publisher>
<institution>for Computational Linguistics.</institution>
<contexts>
<context position="8176" citStr="Duverle and Prendinger, 2009" startWordPosition="1250" endWordPosition="1253">discourse parsing based on multiple text-level or sentence-level features (Soricut and Marcu, 2003; Reitter, 2003; Baldridge and Lascarides, 2005; Subba and Di Eugenio, 2009; Lin et al., 2009; Luong et al., 2014). A recent prevailing idea for discourse parsing is to train two classifiers, namely a binary structure classifier for determining whether two adjacent text units should be merged to form a new subtree, followed by a multi-class relation classifier for determining which discourse relation label should be assigned to the new subtree. The idea is proposed by Hernault and his colleagues (Duverle and Prendinger, 2009; Hernault et al., 2010a) and followed by other work using more sophisticated features (Feng and Hirst, 2012; Hernault et al., 2010b). Current state-of-art performance for relation identification is achieved by the recent representation learning approach proposed by (Ji and Eisenstein, 2014). The proposed framework presented in this paper is similar to (Ji and Eisenstein, 2014) for transforming the discourse units to the abstract representations. 2.2 Recursive Deep Learning Recursive neural networks constitute one type of deep learning frameworks which was first proposed in (Goller and Kuchler</context>
</contexts>
<marker>Duverle, Prendinger, 2009</marker>
<rawString>David A Duverle and Helmut Prendinger. 2009. A novel discourse parser based on support vector machine classification. In Proceedings of the Joint Conference of the 47th Annual Meeting of the ACL and the 4th International Joint Conference on Natural Language Processing of the AFNLP: Volume 2-Volume 2, pages 665–673. Association for Computational Linguistics.</rawString>
</citation>
<citation valid="true">
<authors>
<author>Vanessa Wei Feng</author>
<author>Graeme Hirst</author>
</authors>
<title>Textlevel discourse parsing with rich linguistic features.</title>
<date>2012</date>
<booktitle>In Proceedings of the 50th Annual Meeting of the Association for Computational Linguistics: Long Papers-Volume 1,</booktitle>
<pages>60--68</pages>
<publisher>Association</publisher>
<institution>for Computational Linguistics.</institution>
<contexts>
<context position="2056" citStr="Feng and Hirst, 2012" startWordPosition="292" endWordPosition="295">e parsing tries to identify how the units are connected with each other and thereby uncover the hierarchical structure of the text, from which multiple NLP tasks can benefit, including text summarization (Louis et al., 2010), sentence compression (Sporleder and Lapata, 2005) or questionanswering (Verberne et al., 2007). Despite recent progress in automatic discourse segmentation and sentence-level parsing (e.g., (Fisher and Roark, 2007; Joty et al., 2012; Soricut and Marcu, 2003), document-level discourse parsing remains a significant challenge. Recent attempts (e.g., (Hernault et al., 2010b; Feng and Hirst, 2012; Joty et al., 2013)) are still considerably inferior when compared to human goldstandard discourse analysis. The challenge stems from the fact that compared with sentence-level dependency parsing, the set of relations between discourse units is less straightforward to define. Because there are no clause-level ‘parts of discourse’ analogous to word-level parts of speech, there is no discourse-level grammar analogous to sentence-level grammar. To understand how discourse units are connected, one has to understand the communicative function of each unit, and the role it plays within the context </context>
<context position="8284" citStr="Feng and Hirst, 2012" startWordPosition="1267" endWordPosition="1270">aldridge and Lascarides, 2005; Subba and Di Eugenio, 2009; Lin et al., 2009; Luong et al., 2014). A recent prevailing idea for discourse parsing is to train two classifiers, namely a binary structure classifier for determining whether two adjacent text units should be merged to form a new subtree, followed by a multi-class relation classifier for determining which discourse relation label should be assigned to the new subtree. The idea is proposed by Hernault and his colleagues (Duverle and Prendinger, 2009; Hernault et al., 2010a) and followed by other work using more sophisticated features (Feng and Hirst, 2012; Hernault et al., 2010b). Current state-of-art performance for relation identification is achieved by the recent representation learning approach proposed by (Ji and Eisenstein, 2014). The proposed framework presented in this paper is similar to (Ji and Eisenstein, 2014) for transforming the discourse units to the abstract representations. 2.2 Recursive Deep Learning Recursive neural networks constitute one type of deep learning frameworks which was first proposed in (Goller and Kuchler, 1996). The recursive framework relies and operates on structured inputs (e.g., a parse tree) and computes </context>
<context position="11494" citStr="Feng and Hirst, 2012" startWordPosition="1766" endWordPosition="1769">e-grained relation classes, which with nuclearity attached form a set of 41 distinct relations. Non-binary relations are converted into a cascade of right-branching binary relations. Conventionally, discourse parsing in RST-DT involves the following sub-tasks: (1) EDU segmentation to segment the raw text into EDUs, (2) tree-building. Since the segmentation task is essentially clause delimitation and hence relatively easy (with state-of-art accuracy at most 95%), we focus on the latter problem. We assume that the gold-standard EDU segmentations are already given, as assumed in other past work (Feng and Hirst, 2012). 4 EDU Model In this section, we describe how we compute the distributed representation for a given sentence based on its parse tree structure and contained words. Our implementation is based on (Socher et al., 2013). As the details can easily be found there, we omit them for brevity. Let s denote any given sentence, comprised of a sequence of tokens s = {w1, w2, ..., wn3}, where ns denotes the number of tokens in s. Each token w is associated with a specific vector embedding e, = {e1�, e2 �, ..., e�� }, where K denotes the dimension of the word embedding. We wish to compute the vector repres</context>
<context position="12914" citStr="Feng and Hirst, 2012" startWordPosition="2021" endWordPosition="2024">wo children c1 (associated with vector representation h,1) and c2 (associated with vector representation h,2), standard recursive networks calculate the vector for parent p as follows: hp = f(W · [h,1, h,2] + b) (1) where [h,1, h,2] denotes the concatenating vector for children representations h,1 and h,2; W is a K x 2K matrix and b is the 1 x K bias vector; and f(·) is the function tanh. Recursive neural models compute parent vectors iteratively until the root node’s representation is obtained, and use the root embedding to represent the whole sentence. 5 Discourse Parsing Since recent work (Feng and Hirst, 2012; Hernault et al., 2010b) has demonstrated the advantage of combining the binary structure classifier (determining whether two adjacent text units should be merged to form a new subtree) with the multi-class classifier (determining which discourse relation label to assign to the new subtree) over the older single multi-class classifier with the additional label NO-REL, our approach follows the modern 1http://nlp.stanford.edu/software/ lex-parser.shtml 2063 Figure 1: RST Discourse Tree Structure. strategy but trains binary and multi-class classifiers jointly based on the discourse structure tre</context>
<context position="17590" citStr="Feng and Hirst, 2012" startWordPosition="2795" endWordPosition="2798"> System Overview. The representation for each node within the tree is calculated based on the representations for its children in a bottom-up fashion. Concretely, for a parent node p, given the distributed representation hei for left child, hej for right child, and the relation r(e1, e2), its distributed vector hp is calculated as follows: hp = f(Wr(e1,e2) · [hei, hej] + br(e1,e2)) (6) where br(e1,e2) is the bias vector and f(·) is the non-linear tanh function. To note, our approach does not make any distinction between within-sentence text spans and cross-sentence text spans, different from (Feng and Hirst, 2012; Joty et al., 2013) 5.4 Cost Function The parameters to optimize include sentencelevel convolution parameters [W, b], discourse-level convolution parameters [{Wr}, {br}], binary classification parameters [Gbinary, bbinary, Ubinary, b∗binary], and multi-class parameters [Gmulti, bmulti, Umulti]. Suppose we have M1 binary training samples and M2 multi-class training examples (M2 equals the number of positive examples in M1, which is also the non-leaf nodes within the training discourse trees). The cost function for our framework with regularization on the training set is given by: �J(Obinary) =</context>
<context position="19647" citStr="Feng and Hirst, 2012" startWordPosition="3123" endWordPosition="3126"> the Hadamard product between the two vectors. Each training pair recursively backpropagates its error to some node in the discourse tree through [{Wr}, {br}], and then to nodes in sentence parse tree through [W, b], and the derivatives can be obtained according to standard backpropagation (Goller and Kuchler, 1996; Socher et al., 2010). aJmulti(ei, ej) = [P(ei,ej) − R(ei,ej)] ⊗ aθ 2065 5.6 Additional Features When determining the structure/multi relation between individual EDUs, additional features are also considered, the usefulness of which has been illustrated in a bunch of existing work (Feng and Hirst, 2012; Hernault et al., 2010b; Joty et al., 2012). We consider the following simple text-level features: • Tokens at the beginning and end of the EDUs. • POS at the beginning and end of the EDUs. • Whether two EDUs are in the same sentence. 5.7 Optimization We use the diagonal variant of AdaGrad (Duchi et al., 2011) with minibatches, which is widely applied in deep learning literature (e.g.,(Socher et al., 2011a; Pei et al., 2014)). The learning rate in AdaGrad is adapted differently for different parameters at different steps. Concretely, let giτ denote the subgradient at time step t for parameter</context>
<context position="20967" citStr="Feng and Hirst, 2012" startWordPosition="3355" endWordPosition="3358">T i2 gτ (11) ET ✓gτ where α denotes the learning rate and is set to 0.01 in our approach. Elements in {Wr}, W, Gbinary, Gmulti, Ubinary, Umulti are initialized by randomly drawing from the uniform distribution [−E, E], where E is calculated as suggested in (Collobert et al., 2011). All bias vectors are initialized with 0. Word embeddings {e} are borrowed from Senna (Collobert et al., 2011; Collobert, 2011). 5.8 Inference For inference, the goal is to find the most probable discourse tree given the EDUs within the document. Existing inference approach basically include the approach adopted in (Feng and Hirst, 2012; Hernault et al., 2010b) that merges the most likely spans at each step and SPADE (Fisher and Roark, 2007) that first finds the tree structure that is globally optimal, then assigns the most probable relations to the internal nodes. In this paper, we implement a probabilistic CKY-like bottom-up algorithm for computing the most likely parse tree using dynamic programming as are adopted in (Joty et al., 2012; Joty et al., 2013; Jurafsky and Martin, 2000) for the search of global optimum. For a document with n EDUs, as different relations are characterized with different compositions (thus leadi</context>
</contexts>
<marker>Feng, Hirst, 2012</marker>
<rawString>Vanessa Wei Feng and Graeme Hirst. 2012. Textlevel discourse parsing with rich linguistic features. In Proceedings of the 50th Annual Meeting of the Association for Computational Linguistics: Long Papers-Volume 1, pages 60–68. Association for Computational Linguistics.</rawString>
</citation>
<citation valid="true">
<authors>
<author>Vanessa Wei Feng</author>
<author>Graeme Hirst</author>
</authors>
<title>A linear time bottom-up discourse parser with constraints and post-editing.</title>
<date>2014</date>
<booktitle>In ACL.</booktitle>
<contexts>
<context position="24040" citStr="Feng and Hirst, 2014" startWordPosition="3853" endWordPosition="3856">TTRIBUTE. 2Conventionally, evaluation matrices involve precision, recall and F-score in terms of the comparison between tree structures. But these are the same when manual segmentation is used (Marcu, 2000b). 2066 Approach Span Nuclearity Relation HILDA 75.3 60.0 46.8 Joty et al. 82.5 68.4 55.7 Feng and Hirst 85.7 71.0 58.2 Ji and Eisenstein 82.1 71.1 61.6 Unified (with feature) 82.0 70.0 57.1 Ours (no feature) 82.4 69.2 56.8 Ours (with feature) 84.0 70.8 58.6 human 88.7 77.7 65.7 Table 1: Performances for different approaches. Performances for baselines are reprinted from (Joty et al., 2013; Feng and Hirst, 2014; Ji and Eisenstein, 2014). Also, we do not train a separate classifier for NUCLEUS and SATELLITE identification. The nuclearity decision is made based on the relation type produced by the multi-class classifier. 6.1 Parameter Tuning The regularization parameter Q constitutes the only parameter to tune in our framework. We tune it on the 347 training documents. Concretely, we employ a five-fold cross validation on the RST dataset and tune Q on 5 different values: 0.01, 0.1, 0.5, 1.5, 2.5. The final model was tested on the testing set after parameter tuning. 6.2 Baselines We compare our model a</context>
<context position="25508" citStr="Feng and Hirst, 2014" startWordPosition="4093" endWordPosition="4096"> to reconstruct the tree structure in a greedy way, where the most likely nodes are merged at each step. The results for HILDA are obtained by running the system with default settings on the same inputs we provided to our system. Joty et al The discourse parser introduced by Joty et al. (Joty et al., 2013). It relies on CRF and combines intra-sentential and multi-sentential parsers in two different ways. Joty et al. adopt the global optimal inference as in our work. We reported the performance from their paper (Joty et al., 2013). Feng and Hirst The linear-time discourse parser introduced in (Feng and Hirst, 2014) which relies on two linear-chain CRFs to obtain a sequence of discourse constituents. Ji and Eisenstein The shift-reduce discourse parser introduced in (Ji and Eisenstein, 2014) which parses document by relying on the distributed representations obtained from deep learning framework. Additionally, we implemented a simplified version of our model called unified where we use a unified convolutional function with unified parameters [Wsen, bsen] for span vector computation. Concretely, for a parent node p, given the distributed representation hez for left child, hey for right child, and the relat</context>
</contexts>
<marker>Feng, Hirst, 2014</marker>
<rawString>Vanessa Wei Feng and Graeme Hirst. 2014. A linear time bottom-up discourse parser with constraints and post-editing. In ACL.</rawString>
</citation>
<citation valid="true">
<authors>
<author>Seeger Fisher</author>
<author>Brian Roark</author>
</authors>
<title>The utility of parse-derived features for automatic discourse segmentation.</title>
<date>2007</date>
<booktitle>In ANNUAL MEETING-ASSOCIATION FOR COMPUTATIONAL LINGUISTICS,</booktitle>
<volume>45</volume>
<pages>488</pages>
<contexts>
<context position="1875" citStr="Fisher and Roark, 2007" startWordPosition="265" endWordPosition="268">possible to describe clearly the role that each discourse unit (at any level of grouping) plays with respect to the whole. In a coherent text, no unit is completely isolated. Discourse parsing tries to identify how the units are connected with each other and thereby uncover the hierarchical structure of the text, from which multiple NLP tasks can benefit, including text summarization (Louis et al., 2010), sentence compression (Sporleder and Lapata, 2005) or questionanswering (Verberne et al., 2007). Despite recent progress in automatic discourse segmentation and sentence-level parsing (e.g., (Fisher and Roark, 2007; Joty et al., 2012; Soricut and Marcu, 2003), document-level discourse parsing remains a significant challenge. Recent attempts (e.g., (Hernault et al., 2010b; Feng and Hirst, 2012; Joty et al., 2013)) are still considerably inferior when compared to human goldstandard discourse analysis. The challenge stems from the fact that compared with sentence-level dependency parsing, the set of relations between discourse units is less straightforward to define. Because there are no clause-level ‘parts of discourse’ analogous to word-level parts of speech, there is no discourse-level grammar analogous</context>
<context position="21074" citStr="Fisher and Roark, 2007" startWordPosition="3374" endWordPosition="3377">r}, W, Gbinary, Gmulti, Ubinary, Umulti are initialized by randomly drawing from the uniform distribution [−E, E], where E is calculated as suggested in (Collobert et al., 2011). All bias vectors are initialized with 0. Word embeddings {e} are borrowed from Senna (Collobert et al., 2011; Collobert, 2011). 5.8 Inference For inference, the goal is to find the most probable discourse tree given the EDUs within the document. Existing inference approach basically include the approach adopted in (Feng and Hirst, 2012; Hernault et al., 2010b) that merges the most likely spans at each step and SPADE (Fisher and Roark, 2007) that first finds the tree structure that is globally optimal, then assigns the most probable relations to the internal nodes. In this paper, we implement a probabilistic CKY-like bottom-up algorithm for computing the most likely parse tree using dynamic programming as are adopted in (Joty et al., 2012; Joty et al., 2013; Jurafsky and Martin, 2000) for the search of global optimum. For a document with n EDUs, as different relations are characterized with different compositions (thus leading to different vectors), we use a Nr x n x n dynamic programming table Pr, the cell Pr[r, i, j] of which r</context>
</contexts>
<marker>Fisher, Roark, 2007</marker>
<rawString>Seeger Fisher and Brian Roark. 2007. The utility of parse-derived features for automatic discourse segmentation. In ANNUAL MEETING-ASSOCIATION FOR COMPUTATIONAL LINGUISTICS, volume 45, page 488.</rawString>
</citation>
<citation valid="true">
<authors>
<author>Christoph Goller</author>
<author>Andreas Kuchler</author>
</authors>
<title>Learning task-dependent distributed representations by backpropagation through structure.</title>
<date>1996</date>
<booktitle>In Neural Networks, 1996., IEEE International Conference on,</booktitle>
<volume>1</volume>
<pages>347--352</pages>
<publisher>IEEE.</publisher>
<contexts>
<context position="8783" citStr="Goller and Kuchler, 1996" startWordPosition="1341" endWordPosition="1344">d Prendinger, 2009; Hernault et al., 2010a) and followed by other work using more sophisticated features (Feng and Hirst, 2012; Hernault et al., 2010b). Current state-of-art performance for relation identification is achieved by the recent representation learning approach proposed by (Ji and Eisenstein, 2014). The proposed framework presented in this paper is similar to (Ji and Eisenstein, 2014) for transforming the discourse units to the abstract representations. 2.2 Recursive Deep Learning Recursive neural networks constitute one type of deep learning frameworks which was first proposed in (Goller and Kuchler, 1996). The recursive framework relies and operates on structured inputs (e.g., a parse tree) and computes the representation for each parent based on its children 2062 iteratively in a bottom-up fashion. A series of variations of RNN has been proposed to tailor different task-specific requirements, including MatrixVector RNN (Socher et al., 2012) that represents every word as both a vector and a matrix, or Recursive Neural Tensor Network (Socher et al., 2013) that allows the model to have greater interactions between the input vectors. Many tasks have benefited from the recursive framework, includi</context>
<context position="19343" citStr="Goller and Kuchler, 1996" startWordPosition="3076" endWordPosition="3079">sociate it with a Nr dimensional binary vector R(ei, ej), which denotes the ground truth vector with a 1 at the correct label r(ei, ej) and all other entries 0. Integrating softmax error vector, for any parameter θ, the derivative of Jmulti(ei, ej) with respect to θ is given by: aS(ei,ej) aθ where ⊗ denotes the Hadamard product between the two vectors. Each training pair recursively backpropagates its error to some node in the discourse tree through [{Wr}, {br}], and then to nodes in sentence parse tree through [W, b], and the derivatives can be obtained according to standard backpropagation (Goller and Kuchler, 1996; Socher et al., 2010). aJmulti(ei, ej) = [P(ei,ej) − R(ei,ej)] ⊗ aθ 2065 5.6 Additional Features When determining the structure/multi relation between individual EDUs, additional features are also considered, the usefulness of which has been illustrated in a bunch of existing work (Feng and Hirst, 2012; Hernault et al., 2010b; Joty et al., 2012). We consider the following simple text-level features: • Tokens at the beginning and end of the EDUs. • POS at the beginning and end of the EDUs. • Whether two EDUs are in the same sentence. 5.7 Optimization We use the diagonal variant of AdaGrad (Duc</context>
</contexts>
<marker>Goller, Kuchler, 1996</marker>
<rawString>Christoph Goller and Andreas Kuchler. 1996. Learning task-dependent distributed representations by backpropagation through structure. In Neural Networks, 1996., IEEE International Conference on, volume 1, pages 347–352. IEEE.</rawString>
</citation>
<citation valid="true">
<authors>
<author>Hugo Hernault</author>
<author>Danushka Bollegala</author>
<author>Mitsuru Ishizuka</author>
</authors>
<title>A semi-supervised approach to improve classification of infrequent discourse relations using feature vector extension.</title>
<date>2010</date>
<booktitle>In Proceedings of the 2010 Conference on Empirical Methods in Natural Language Processing,</booktitle>
<pages>399--409</pages>
<publisher>Association</publisher>
<institution>for Computational Linguistics.</institution>
<contexts>
<context position="2033" citStr="Hernault et al., 2010" startWordPosition="288" endWordPosition="291">etely isolated. Discourse parsing tries to identify how the units are connected with each other and thereby uncover the hierarchical structure of the text, from which multiple NLP tasks can benefit, including text summarization (Louis et al., 2010), sentence compression (Sporleder and Lapata, 2005) or questionanswering (Verberne et al., 2007). Despite recent progress in automatic discourse segmentation and sentence-level parsing (e.g., (Fisher and Roark, 2007; Joty et al., 2012; Soricut and Marcu, 2003), document-level discourse parsing remains a significant challenge. Recent attempts (e.g., (Hernault et al., 2010b; Feng and Hirst, 2012; Joty et al., 2013)) are still considerably inferior when compared to human goldstandard discourse analysis. The challenge stems from the fact that compared with sentence-level dependency parsing, the set of relations between discourse units is less straightforward to define. Because there are no clause-level ‘parts of discourse’ analogous to word-level parts of speech, there is no discourse-level grammar analogous to sentence-level grammar. To understand how discourse units are connected, one has to understand the communicative function of each unit, and the role it pl</context>
<context position="8199" citStr="Hernault et al., 2010" startWordPosition="1254" endWordPosition="1257">tiple text-level or sentence-level features (Soricut and Marcu, 2003; Reitter, 2003; Baldridge and Lascarides, 2005; Subba and Di Eugenio, 2009; Lin et al., 2009; Luong et al., 2014). A recent prevailing idea for discourse parsing is to train two classifiers, namely a binary structure classifier for determining whether two adjacent text units should be merged to form a new subtree, followed by a multi-class relation classifier for determining which discourse relation label should be assigned to the new subtree. The idea is proposed by Hernault and his colleagues (Duverle and Prendinger, 2009; Hernault et al., 2010a) and followed by other work using more sophisticated features (Feng and Hirst, 2012; Hernault et al., 2010b). Current state-of-art performance for relation identification is achieved by the recent representation learning approach proposed by (Ji and Eisenstein, 2014). The proposed framework presented in this paper is similar to (Ji and Eisenstein, 2014) for transforming the discourse units to the abstract representations. 2.2 Recursive Deep Learning Recursive neural networks constitute one type of deep learning frameworks which was first proposed in (Goller and Kuchler, 1996). The recursive </context>
<context position="12937" citStr="Hernault et al., 2010" startWordPosition="2025" endWordPosition="2028">ated with vector representation h,1) and c2 (associated with vector representation h,2), standard recursive networks calculate the vector for parent p as follows: hp = f(W · [h,1, h,2] + b) (1) where [h,1, h,2] denotes the concatenating vector for children representations h,1 and h,2; W is a K x 2K matrix and b is the 1 x K bias vector; and f(·) is the function tanh. Recursive neural models compute parent vectors iteratively until the root node’s representation is obtained, and use the root embedding to represent the whole sentence. 5 Discourse Parsing Since recent work (Feng and Hirst, 2012; Hernault et al., 2010b) has demonstrated the advantage of combining the binary structure classifier (determining whether two adjacent text units should be merged to form a new subtree) with the multi-class classifier (determining which discourse relation label to assign to the new subtree) over the older single multi-class classifier with the additional label NO-REL, our approach follows the modern 1http://nlp.stanford.edu/software/ lex-parser.shtml 2063 Figure 1: RST Discourse Tree Structure. strategy but trains binary and multi-class classifiers jointly based on the discourse structure tree. Figure 2 illustrates</context>
<context position="19670" citStr="Hernault et al., 2010" startWordPosition="3127" endWordPosition="3130">between the two vectors. Each training pair recursively backpropagates its error to some node in the discourse tree through [{Wr}, {br}], and then to nodes in sentence parse tree through [W, b], and the derivatives can be obtained according to standard backpropagation (Goller and Kuchler, 1996; Socher et al., 2010). aJmulti(ei, ej) = [P(ei,ej) − R(ei,ej)] ⊗ aθ 2065 5.6 Additional Features When determining the structure/multi relation between individual EDUs, additional features are also considered, the usefulness of which has been illustrated in a bunch of existing work (Feng and Hirst, 2012; Hernault et al., 2010b; Joty et al., 2012). We consider the following simple text-level features: • Tokens at the beginning and end of the EDUs. • POS at the beginning and end of the EDUs. • Whether two EDUs are in the same sentence. 5.7 Optimization We use the diagonal variant of AdaGrad (Duchi et al., 2011) with minibatches, which is widely applied in deep learning literature (e.g.,(Socher et al., 2011a; Pei et al., 2014)). The learning rate in AdaGrad is adapted differently for different parameters at different steps. Concretely, let giτ denote the subgradient at time step t for parameter Bi obtained from backp</context>
<context position="20990" citStr="Hernault et al., 2010" startWordPosition="3359" endWordPosition="3362">ere α denotes the learning rate and is set to 0.01 in our approach. Elements in {Wr}, W, Gbinary, Gmulti, Ubinary, Umulti are initialized by randomly drawing from the uniform distribution [−E, E], where E is calculated as suggested in (Collobert et al., 2011). All bias vectors are initialized with 0. Word embeddings {e} are borrowed from Senna (Collobert et al., 2011; Collobert, 2011). 5.8 Inference For inference, the goal is to find the most probable discourse tree given the EDUs within the document. Existing inference approach basically include the approach adopted in (Feng and Hirst, 2012; Hernault et al., 2010b) that merges the most likely spans at each step and SPADE (Fisher and Roark, 2007) that first finds the tree structure that is globally optimal, then assigns the most probable relations to the internal nodes. In this paper, we implement a probabilistic CKY-like bottom-up algorithm for computing the most likely parse tree using dynamic programming as are adopted in (Joty et al., 2012; Joty et al., 2013; Jurafsky and Martin, 2000) for the search of global optimum. For a document with n EDUs, as different relations are characterized with different compositions (thus leading to different vectors</context>
<context position="24835" citStr="Hernault et al., 2010" startWordPosition="3980" endWordPosition="3983">roduced by the multi-class classifier. 6.1 Parameter Tuning The regularization parameter Q constitutes the only parameter to tune in our framework. We tune it on the 347 training documents. Concretely, we employ a five-fold cross validation on the RST dataset and tune Q on 5 different values: 0.01, 0.1, 0.5, 1.5, 2.5. The final model was tested on the testing set after parameter tuning. 6.2 Baselines We compare our model against the following currently prevailing discourse parsing baselines: HILDA A discourse parser based on support vector machine classification introduced by Hernault et al. (Hernault et al., 2010b). HILDA uses the binary and multi-class classifier to reconstruct the tree structure in a greedy way, where the most likely nodes are merged at each step. The results for HILDA are obtained by running the system with default settings on the same inputs we provided to our system. Joty et al The discourse parser introduced by Joty et al. (Joty et al., 2013). It relies on CRF and combines intra-sentential and multi-sentential parsers in two different ways. Joty et al. adopt the global optimal inference as in our work. We reported the performance from their paper (Joty et al., 2013). Feng and Hi</context>
</contexts>
<marker>Hernault, Bollegala, Ishizuka, 2010</marker>
<rawString>Hugo Hernault, Danushka Bollegala, and Mitsuru Ishizuka. 2010a. A semi-supervised approach to improve classification of infrequent discourse relations using feature vector extension. In Proceedings of the 2010 Conference on Empirical Methods in Natural Language Processing, pages 399–409. Association for Computational Linguistics.</rawString>
</citation>
<citation valid="false">
<authors>
<author>Hugo Hernault</author>
</authors>
<title>Helmut Prendinger, Mitsuru Ishizuka, et al. 2010b. Hilda: a discourse parser using support vector machine classification.</title>
<journal>Dialogue &amp; Discourse,</journal>
<volume>1</volume>
<issue>3</issue>
<marker>Hernault, </marker>
<rawString>Hugo Hernault, Helmut Prendinger, Mitsuru Ishizuka, et al. 2010b. Hilda: a discourse parser using support vector machine classification. Dialogue &amp; Discourse, 1(3).</rawString>
</citation>
<citation valid="true">
<authors>
<author>Wang Houfeng</author>
<author>Longkai Zhang</author>
<author>Ni Sun</author>
</authors>
<title>Improving chinese word segmentation on microblog using rich punctuations.</title>
<date>2013</date>
<contexts>
<context position="9553" citStr="Houfeng et al., 2013" startWordPosition="1464" endWordPosition="1467">children 2062 iteratively in a bottom-up fashion. A series of variations of RNN has been proposed to tailor different task-specific requirements, including MatrixVector RNN (Socher et al., 2012) that represents every word as both a vector and a matrix, or Recursive Neural Tensor Network (Socher et al., 2013) that allows the model to have greater interactions between the input vectors. Many tasks have benefited from the recursive framework, including parsing (Socher et al., 2011b), sentiment analysis (Socher et al., 2013), textual entailment (Bowman, 2013), segmentation (Wang and Mansur, 2013; Houfeng et al., 2013), and paraphrase detection (Socher et al., 2011a). 3 The RST Discourse Treebank There are today two primary alternative discourse treebanks suitable for training data: the Rhetorical Structure Theory Discourse Treebank RSTDT (Carlson et al., 2003) and the Penn Discourse Treebank (Prasad et al., 2008). In this paper, we select the former. In RST (Mann and Thompson, 1988), a coherent context or a document is represented as a hierarchical tree structure, the leaves of which are clause-sized units called Elementary Discourse Units (EDUs). Adjacent nodes (siblings in the tree) are linked with disco</context>
</contexts>
<marker>Houfeng, Zhang, Sun, 2013</marker>
<rawString>Wang Houfeng, Longkai Zhang, and Ni Sun. 2013. Improving chinese word segmentation on microblog using rich punctuations.</rawString>
</citation>
<citation valid="true">
<authors>
<author>Eduard H Hovy</author>
<author>Elisabeth Maier</author>
</authors>
<title>Parsimonious or profligate: How many and which discourse structure relations. Discourse Processes.</title>
<date>1997</date>
<contexts>
<context position="5788" citStr="Hovy and Maier, 1997" startWordPosition="869" endWordPosition="872">n 7. 2 Related Work 2.1 Discourse Analysis and Parsing The basis of discourse structure lies in the recognition that discourse units (minimally, clauses) are related to one another in principled ways, and that the juxtaposition of two units creates a joint meaning larger than either unit’s meaning alone. In a coherent text this juxtaposition is never random, but serves the speaker’s communicative goals. Considerable work on linguistic and computational discourse processing in the 1970s and 80s led to the development of several proposals for relations that combine units; for a compilation see (Hovy and Maier, 1997). Of these the most influential is Rhetorical Structure Theory RST (Mann and Thompson, 1988) that defines about 25 relations, each containing semantic constraints on its component parts plus a description of the overall functional/semantic effect produced as a unit when the parts have been appropriately connected in the text. For example, the SOLUTIONHOOD relation connects one unit describing a problem situation with another describing its solution, using phrases such as “the answer is”; in successful communication the reader will understand that a problem is described and its solution is give</context>
</contexts>
<marker>Hovy, Maier, 1997</marker>
<rawString>Eduard H Hovy and Elisabeth Maier. 1997. Parsimonious or profligate: How many and which discourse structure relations. Discourse Processes.</rawString>
</citation>
<citation valid="true">
<authors>
<author>Yangfeng Ji</author>
<author>Jacob Eisenstein</author>
</authors>
<title>Representation learning for text-level discourse parsing.</title>
<date>2014</date>
<contexts>
<context position="8468" citStr="Ji and Eisenstein, 2014" startWordPosition="1293" endWordPosition="1296"> a binary structure classifier for determining whether two adjacent text units should be merged to form a new subtree, followed by a multi-class relation classifier for determining which discourse relation label should be assigned to the new subtree. The idea is proposed by Hernault and his colleagues (Duverle and Prendinger, 2009; Hernault et al., 2010a) and followed by other work using more sophisticated features (Feng and Hirst, 2012; Hernault et al., 2010b). Current state-of-art performance for relation identification is achieved by the recent representation learning approach proposed by (Ji and Eisenstein, 2014). The proposed framework presented in this paper is similar to (Ji and Eisenstein, 2014) for transforming the discourse units to the abstract representations. 2.2 Recursive Deep Learning Recursive neural networks constitute one type of deep learning frameworks which was first proposed in (Goller and Kuchler, 1996). The recursive framework relies and operates on structured inputs (e.g., a parse tree) and computes the representation for each parent based on its children 2062 iteratively in a bottom-up fashion. A series of variations of RNN has been proposed to tailor different task-specific requ</context>
<context position="24066" citStr="Ji and Eisenstein, 2014" startWordPosition="3857" endWordPosition="3860">lly, evaluation matrices involve precision, recall and F-score in terms of the comparison between tree structures. But these are the same when manual segmentation is used (Marcu, 2000b). 2066 Approach Span Nuclearity Relation HILDA 75.3 60.0 46.8 Joty et al. 82.5 68.4 55.7 Feng and Hirst 85.7 71.0 58.2 Ji and Eisenstein 82.1 71.1 61.6 Unified (with feature) 82.0 70.0 57.1 Ours (no feature) 82.4 69.2 56.8 Ours (with feature) 84.0 70.8 58.6 human 88.7 77.7 65.7 Table 1: Performances for different approaches. Performances for baselines are reprinted from (Joty et al., 2013; Feng and Hirst, 2014; Ji and Eisenstein, 2014). Also, we do not train a separate classifier for NUCLEUS and SATELLITE identification. The nuclearity decision is made based on the relation type produced by the multi-class classifier. 6.1 Parameter Tuning The regularization parameter Q constitutes the only parameter to tune in our framework. We tune it on the 347 training documents. Concretely, we employ a five-fold cross validation on the RST dataset and tune Q on 5 different values: 0.01, 0.1, 0.5, 1.5, 2.5. The final model was tested on the testing set after parameter tuning. 6.2 Baselines We compare our model against the following curre</context>
<context position="25686" citStr="Ji and Eisenstein, 2014" startWordPosition="4120" endWordPosition="4123">settings on the same inputs we provided to our system. Joty et al The discourse parser introduced by Joty et al. (Joty et al., 2013). It relies on CRF and combines intra-sentential and multi-sentential parsers in two different ways. Joty et al. adopt the global optimal inference as in our work. We reported the performance from their paper (Joty et al., 2013). Feng and Hirst The linear-time discourse parser introduced in (Feng and Hirst, 2014) which relies on two linear-chain CRFs to obtain a sequence of discourse constituents. Ji and Eisenstein The shift-reduce discourse parser introduced in (Ji and Eisenstein, 2014) which parses document by relying on the distributed representations obtained from deep learning framework. Additionally, we implemented a simplified version of our model called unified where we use a unified convolutional function with unified parameters [Wsen, bsen] for span vector computation. Concretely, for a parent node p, given the distributed representation hez for left child, hey for right child, and the relation r(e1, e2), rather than taking the inter relation between two children, its distributed vector hp is calculated: hp = f(Wsen · [hez, hey] + bsen) (13) 6.3 Performance Performa</context>
</contexts>
<marker>Ji, Eisenstein, 2014</marker>
<rawString>Yangfeng Ji and Jacob Eisenstein. 2014. Representation learning for text-level discourse parsing.</rawString>
</citation>
<citation valid="true">
<authors>
<author>Shafiq Joty</author>
<author>Giuseppe Carenini</author>
<author>Raymond T Ng</author>
</authors>
<title>A novel discriminative framework for sentence-level discourse analysis.</title>
<date>2012</date>
<booktitle>In Proceedings of the 2012 Joint Conference on Empirical Methods in Natural Language Processing and Computational Natural Language Learning,</booktitle>
<pages>904--915</pages>
<publisher>Association</publisher>
<institution>for Computational Linguistics.</institution>
<contexts>
<context position="1894" citStr="Joty et al., 2012" startWordPosition="269" endWordPosition="272">arly the role that each discourse unit (at any level of grouping) plays with respect to the whole. In a coherent text, no unit is completely isolated. Discourse parsing tries to identify how the units are connected with each other and thereby uncover the hierarchical structure of the text, from which multiple NLP tasks can benefit, including text summarization (Louis et al., 2010), sentence compression (Sporleder and Lapata, 2005) or questionanswering (Verberne et al., 2007). Despite recent progress in automatic discourse segmentation and sentence-level parsing (e.g., (Fisher and Roark, 2007; Joty et al., 2012; Soricut and Marcu, 2003), document-level discourse parsing remains a significant challenge. Recent attempts (e.g., (Hernault et al., 2010b; Feng and Hirst, 2012; Joty et al., 2013)) are still considerably inferior when compared to human goldstandard discourse analysis. The challenge stems from the fact that compared with sentence-level dependency parsing, the set of relations between discourse units is less straightforward to define. Because there are no clause-level ‘parts of discourse’ analogous to word-level parts of speech, there is no discourse-level grammar analogous to sentence-level </context>
<context position="19691" citStr="Joty et al., 2012" startWordPosition="3131" endWordPosition="3134"> Each training pair recursively backpropagates its error to some node in the discourse tree through [{Wr}, {br}], and then to nodes in sentence parse tree through [W, b], and the derivatives can be obtained according to standard backpropagation (Goller and Kuchler, 1996; Socher et al., 2010). aJmulti(ei, ej) = [P(ei,ej) − R(ei,ej)] ⊗ aθ 2065 5.6 Additional Features When determining the structure/multi relation between individual EDUs, additional features are also considered, the usefulness of which has been illustrated in a bunch of existing work (Feng and Hirst, 2012; Hernault et al., 2010b; Joty et al., 2012). We consider the following simple text-level features: • Tokens at the beginning and end of the EDUs. • POS at the beginning and end of the EDUs. • Whether two EDUs are in the same sentence. 5.7 Optimization We use the diagonal variant of AdaGrad (Duchi et al., 2011) with minibatches, which is widely applied in deep learning literature (e.g.,(Socher et al., 2011a; Pei et al., 2014)). The learning rate in AdaGrad is adapted differently for different parameters at different steps. Concretely, let giτ denote the subgradient at time step t for parameter Bi obtained from backpropagation, the param</context>
<context position="21377" citStr="Joty et al., 2012" startWordPosition="3423" endWordPosition="3426">8 Inference For inference, the goal is to find the most probable discourse tree given the EDUs within the document. Existing inference approach basically include the approach adopted in (Feng and Hirst, 2012; Hernault et al., 2010b) that merges the most likely spans at each step and SPADE (Fisher and Roark, 2007) that first finds the tree structure that is globally optimal, then assigns the most probable relations to the internal nodes. In this paper, we implement a probabilistic CKY-like bottom-up algorithm for computing the most likely parse tree using dynamic programming as are adopted in (Joty et al., 2012; Joty et al., 2013; Jurafsky and Martin, 2000) for the search of global optimum. For a document with n EDUs, as different relations are characterized with different compositions (thus leading to different vectors), we use a Nr x n x n dynamic programming table Pr, the cell Pr[r, i, j] of which represents the span contained EDUs from i to j and stores the probability that relation r holds between the two spans within i to j. Pr[r, i, j] is computed as follows: Pr[r, i, j] =maxr1,r2,kPr[r1, i, k] · Pr[r2, k, j] xP(tbinary(e[i,k],e[k,j]) = 1) xP(r(e[i,k],e[k,j]) = 1) (12) At each merging step, a</context>
</contexts>
<marker>Joty, Carenini, Ng, 2012</marker>
<rawString>Shafiq Joty, Giuseppe Carenini, and Raymond T Ng. 2012. A novel discriminative framework for sentence-level discourse analysis. In Proceedings of the 2012 Joint Conference on Empirical Methods in Natural Language Processing and Computational Natural Language Learning, pages 904–915. Association for Computational Linguistics.</rawString>
</citation>
<citation valid="true">
<authors>
<author>Shafiq Joty</author>
<author>Giuseppe Carenini</author>
<author>Raymond Ng</author>
<author>Yashar Mehdad</author>
</authors>
<title>Combining intra-and multisentential rhetorical parsing for document-level discourse analysis.</title>
<date>2013</date>
<booktitle>In Proceedings of the 51st annual</booktitle>
<pages>486--496</pages>
<contexts>
<context position="2076" citStr="Joty et al., 2013" startWordPosition="296" endWordPosition="299">ntify how the units are connected with each other and thereby uncover the hierarchical structure of the text, from which multiple NLP tasks can benefit, including text summarization (Louis et al., 2010), sentence compression (Sporleder and Lapata, 2005) or questionanswering (Verberne et al., 2007). Despite recent progress in automatic discourse segmentation and sentence-level parsing (e.g., (Fisher and Roark, 2007; Joty et al., 2012; Soricut and Marcu, 2003), document-level discourse parsing remains a significant challenge. Recent attempts (e.g., (Hernault et al., 2010b; Feng and Hirst, 2012; Joty et al., 2013)) are still considerably inferior when compared to human goldstandard discourse analysis. The challenge stems from the fact that compared with sentence-level dependency parsing, the set of relations between discourse units is less straightforward to define. Because there are no clause-level ‘parts of discourse’ analogous to word-level parts of speech, there is no discourse-level grammar analogous to sentence-level grammar. To understand how discourse units are connected, one has to understand the communicative function of each unit, and the role it plays within the context that encapsulates it</context>
<context position="17610" citStr="Joty et al., 2013" startWordPosition="2799" endWordPosition="2802">representation for each node within the tree is calculated based on the representations for its children in a bottom-up fashion. Concretely, for a parent node p, given the distributed representation hei for left child, hej for right child, and the relation r(e1, e2), its distributed vector hp is calculated as follows: hp = f(Wr(e1,e2) · [hei, hej] + br(e1,e2)) (6) where br(e1,e2) is the bias vector and f(·) is the non-linear tanh function. To note, our approach does not make any distinction between within-sentence text spans and cross-sentence text spans, different from (Feng and Hirst, 2012; Joty et al., 2013) 5.4 Cost Function The parameters to optimize include sentencelevel convolution parameters [W, b], discourse-level convolution parameters [{Wr}, {br}], binary classification parameters [Gbinary, bbinary, Ubinary, b∗binary], and multi-class parameters [Gmulti, bmulti, Umulti]. Suppose we have M1 binary training samples and M2 multi-class training examples (M2 equals the number of positive examples in M1, which is also the non-leaf nodes within the training discourse trees). The cost function for our framework with regularization on the training set is given by: �J(Obinary) = Jbinary(ei, ej) (ei</context>
<context position="21396" citStr="Joty et al., 2013" startWordPosition="3427" endWordPosition="3430">erence, the goal is to find the most probable discourse tree given the EDUs within the document. Existing inference approach basically include the approach adopted in (Feng and Hirst, 2012; Hernault et al., 2010b) that merges the most likely spans at each step and SPADE (Fisher and Roark, 2007) that first finds the tree structure that is globally optimal, then assigns the most probable relations to the internal nodes. In this paper, we implement a probabilistic CKY-like bottom-up algorithm for computing the most likely parse tree using dynamic programming as are adopted in (Joty et al., 2012; Joty et al., 2013; Jurafsky and Martin, 2000) for the search of global optimum. For a document with n EDUs, as different relations are characterized with different compositions (thus leading to different vectors), we use a Nr x n x n dynamic programming table Pr, the cell Pr[r, i, j] of which represents the span contained EDUs from i to j and stores the probability that relation r holds between the two spans within i to j. Pr[r, i, j] is computed as follows: Pr[r, i, j] =maxr1,r2,kPr[r1, i, k] · Pr[r2, k, j] xP(tbinary(e[i,k],e[k,j]) = 1) xP(r(e[i,k],e[k,j]) = 1) (12) At each merging step, a distributed vector</context>
<context position="24018" citStr="Joty et al., 2013" startWordPosition="3849" endWordPosition="3852">me relation label ATTRIBUTE. 2Conventionally, evaluation matrices involve precision, recall and F-score in terms of the comparison between tree structures. But these are the same when manual segmentation is used (Marcu, 2000b). 2066 Approach Span Nuclearity Relation HILDA 75.3 60.0 46.8 Joty et al. 82.5 68.4 55.7 Feng and Hirst 85.7 71.0 58.2 Ji and Eisenstein 82.1 71.1 61.6 Unified (with feature) 82.0 70.0 57.1 Ours (no feature) 82.4 69.2 56.8 Ours (with feature) 84.0 70.8 58.6 human 88.7 77.7 65.7 Table 1: Performances for different approaches. Performances for baselines are reprinted from (Joty et al., 2013; Feng and Hirst, 2014; Ji and Eisenstein, 2014). Also, we do not train a separate classifier for NUCLEUS and SATELLITE identification. The nuclearity decision is made based on the relation type produced by the multi-class classifier. 6.1 Parameter Tuning The regularization parameter Q constitutes the only parameter to tune in our framework. We tune it on the 347 training documents. Concretely, we employ a five-fold cross validation on the RST dataset and tune Q on 5 different values: 0.01, 0.1, 0.5, 1.5, 2.5. The final model was tested on the testing set after parameter tuning. 6.2 Baselines </context>
<context position="25422" citStr="Joty et al., 2013" startWordPosition="4080" endWordPosition="4083">t et al. (Hernault et al., 2010b). HILDA uses the binary and multi-class classifier to reconstruct the tree structure in a greedy way, where the most likely nodes are merged at each step. The results for HILDA are obtained by running the system with default settings on the same inputs we provided to our system. Joty et al The discourse parser introduced by Joty et al. (Joty et al., 2013). It relies on CRF and combines intra-sentential and multi-sentential parsers in two different ways. Joty et al. adopt the global optimal inference as in our work. We reported the performance from their paper (Joty et al., 2013). Feng and Hirst The linear-time discourse parser introduced in (Feng and Hirst, 2014) which relies on two linear-chain CRFs to obtain a sequence of discourse constituents. Ji and Eisenstein The shift-reduce discourse parser introduced in (Ji and Eisenstein, 2014) which parses document by relying on the distributed representations obtained from deep learning framework. Additionally, we implemented a simplified version of our model called unified where we use a unified convolutional function with unified parameters [Wsen, bsen] for span vector computation. Concretely, for a parent node p, given</context>
</contexts>
<marker>Joty, Carenini, Ng, Mehdad, 2013</marker>
<rawString>Shafiq Joty, Giuseppe Carenini, Raymond Ng, and Yashar Mehdad. 2013. Combining intra-and multisentential rhetorical parsing for document-level discourse analysis. In Proceedings of the 51st annual meeting of the association for computational linguistics (ACL), pages 486–496.</rawString>
</citation>
<citation valid="true">
<authors>
<author>Dan Jurafsky</author>
<author>James H Martin</author>
</authors>
<date>2000</date>
<booktitle>Speech &amp; Language Processing.</booktitle>
<publisher>Pearson Education</publisher>
<contexts>
<context position="21424" citStr="Jurafsky and Martin, 2000" startWordPosition="3431" endWordPosition="3434"> to find the most probable discourse tree given the EDUs within the document. Existing inference approach basically include the approach adopted in (Feng and Hirst, 2012; Hernault et al., 2010b) that merges the most likely spans at each step and SPADE (Fisher and Roark, 2007) that first finds the tree structure that is globally optimal, then assigns the most probable relations to the internal nodes. In this paper, we implement a probabilistic CKY-like bottom-up algorithm for computing the most likely parse tree using dynamic programming as are adopted in (Joty et al., 2012; Joty et al., 2013; Jurafsky and Martin, 2000) for the search of global optimum. For a document with n EDUs, as different relations are characterized with different compositions (thus leading to different vectors), we use a Nr x n x n dynamic programming table Pr, the cell Pr[r, i, j] of which represents the span contained EDUs from i to j and stores the probability that relation r holds between the two spans within i to j. Pr[r, i, j] is computed as follows: Pr[r, i, j] =maxr1,r2,kPr[r1, i, k] · Pr[r2, k, j] xP(tbinary(e[i,k],e[k,j]) = 1) xP(r(e[i,k],e[k,j]) = 1) (12) At each merging step, a distributed vector for the merged point is cal</context>
</contexts>
<marker>Jurafsky, Martin, 2000</marker>
<rawString>Dan Jurafsky and James H Martin. 2000. Speech &amp; Language Processing. Pearson Education India.</rawString>
</citation>
<citation valid="true">
<authors>
<author>Huong LeThanh</author>
<author>Geetha Abeysinghe</author>
<author>Christian Huyck</author>
</authors>
<title>Generating discourse structures for written texts.</title>
<date>2004</date>
<booktitle>In Proceedings of the 20th international conference on Computational Linguistics,</booktitle>
<pages>329</pages>
<publisher>Association</publisher>
<institution>for Computational Linguistics.</institution>
<contexts>
<context position="7215" citStr="LeThanh et al., 2004" startWordPosition="1099" endWordPosition="1102">e (rhetorical) function. The functions are reflected in text as signals of the author’s intentions, and take various forms (including expressions such as “therefore”, “for example”, “the answer is”, and so on; patterns of tense or pronoun usage; syntactic forms; etc.). The signals govern discourse blocks ranging from a clause to an entire text, each one associated with some discourse relation. In order to build a text’s hierarchical structure, a discourse parser needs to recognize these signals and use them to appropriately compose the relationship and nesting. Early approaches (Marcu, 2000a; LeThanh et al., 2004) rely mainly on overt discourse markers (or cue words) and use handcoded rules to build text structure trees, bottom-up from clauses to sentences to paragraphs.... Since a hierarchical discourse tree structure is analogous to a constituency based syntactic tree, modern research explored syntactic parsing techniques (e.g., CKY) for discourse parsing based on multiple text-level or sentence-level features (Soricut and Marcu, 2003; Reitter, 2003; Baldridge and Lascarides, 2005; Subba and Di Eugenio, 2009; Lin et al., 2009; Luong et al., 2014). A recent prevailing idea for discourse parsing is to </context>
</contexts>
<marker>LeThanh, Abeysinghe, Huyck, 2004</marker>
<rawString>Huong LeThanh, Geetha Abeysinghe, and Christian Huyck. 2004. Generating discourse structures for written texts. In Proceedings of the 20th international conference on Computational Linguistics, page 329. Association for Computational Linguistics.</rawString>
</citation>
<citation valid="true">
<authors>
<author>Ziheng Lin</author>
<author>Min-Yen Kan</author>
<author>Hwee Tou Ng</author>
</authors>
<title>Recognizing implicit discourse relations in the penn discourse treebank.</title>
<date>2009</date>
<booktitle>In Proceedings of the 2009 Conference on Empirical Methods in Natural Language Processing: Volume</booktitle>
<volume>1</volume>
<pages>343--351</pages>
<publisher>Association</publisher>
<institution>for Computational Linguistics.</institution>
<contexts>
<context position="7739" citStr="Lin et al., 2009" startWordPosition="1179" endWordPosition="1182"> compose the relationship and nesting. Early approaches (Marcu, 2000a; LeThanh et al., 2004) rely mainly on overt discourse markers (or cue words) and use handcoded rules to build text structure trees, bottom-up from clauses to sentences to paragraphs.... Since a hierarchical discourse tree structure is analogous to a constituency based syntactic tree, modern research explored syntactic parsing techniques (e.g., CKY) for discourse parsing based on multiple text-level or sentence-level features (Soricut and Marcu, 2003; Reitter, 2003; Baldridge and Lascarides, 2005; Subba and Di Eugenio, 2009; Lin et al., 2009; Luong et al., 2014). A recent prevailing idea for discourse parsing is to train two classifiers, namely a binary structure classifier for determining whether two adjacent text units should be merged to form a new subtree, followed by a multi-class relation classifier for determining which discourse relation label should be assigned to the new subtree. The idea is proposed by Hernault and his colleagues (Duverle and Prendinger, 2009; Hernault et al., 2010a) and followed by other work using more sophisticated features (Feng and Hirst, 2012; Hernault et al., 2010b). Current state-of-art perform</context>
</contexts>
<marker>Lin, Kan, Ng, 2009</marker>
<rawString>Ziheng Lin, Min-Yen Kan, and Hwee Tou Ng. 2009. Recognizing implicit discourse relations in the penn discourse treebank. In Proceedings of the 2009 Conference on Empirical Methods in Natural Language Processing: Volume 1-Volume 1, pages 343–351. Association for Computational Linguistics.</rawString>
</citation>
<citation valid="true">
<authors>
<author>Annie Louis</author>
<author>Aravind Joshi</author>
<author>Ani Nenkova</author>
</authors>
<title>Discourse indicators for content selection in summarization.</title>
<date>2010</date>
<booktitle>In Proceedings of the 11th Annual Meeting of the Special Interest Group on Discourse and Dialogue,</booktitle>
<pages>147--156</pages>
<publisher>Association</publisher>
<institution>for Computational Linguistics.</institution>
<contexts>
<context position="1660" citStr="Louis et al., 2010" startWordPosition="236" endWordPosition="239">n a coherent text, units (clauses, sentences, and larger multi-clause groupings) are tightly connected semantically, syntactically, and logically. Mann and Thompson (1988) define a text to be coherent when it is possible to describe clearly the role that each discourse unit (at any level of grouping) plays with respect to the whole. In a coherent text, no unit is completely isolated. Discourse parsing tries to identify how the units are connected with each other and thereby uncover the hierarchical structure of the text, from which multiple NLP tasks can benefit, including text summarization (Louis et al., 2010), sentence compression (Sporleder and Lapata, 2005) or questionanswering (Verberne et al., 2007). Despite recent progress in automatic discourse segmentation and sentence-level parsing (e.g., (Fisher and Roark, 2007; Joty et al., 2012; Soricut and Marcu, 2003), document-level discourse parsing remains a significant challenge. Recent attempts (e.g., (Hernault et al., 2010b; Feng and Hirst, 2012; Joty et al., 2013)) are still considerably inferior when compared to human goldstandard discourse analysis. The challenge stems from the fact that compared with sentence-level dependency parsing, the se</context>
</contexts>
<marker>Louis, Joshi, Nenkova, 2010</marker>
<rawString>Annie Louis, Aravind Joshi, and Ani Nenkova. 2010. Discourse indicators for content selection in summarization. In Proceedings of the 11th Annual Meeting of the Special Interest Group on Discourse and Dialogue, pages 147–156. Association for Computational Linguistics.</rawString>
</citation>
<citation valid="true">
<authors>
<author>Minh-Thang Luong</author>
<author>Michael C Frank</author>
<author>Mark Johnson</author>
</authors>
<title>Parsing entire discourses as very long strings: Capturing topic continuity in grounded language learning.</title>
<date>2014</date>
<contexts>
<context position="7760" citStr="Luong et al., 2014" startWordPosition="1183" endWordPosition="1186">ionship and nesting. Early approaches (Marcu, 2000a; LeThanh et al., 2004) rely mainly on overt discourse markers (or cue words) and use handcoded rules to build text structure trees, bottom-up from clauses to sentences to paragraphs.... Since a hierarchical discourse tree structure is analogous to a constituency based syntactic tree, modern research explored syntactic parsing techniques (e.g., CKY) for discourse parsing based on multiple text-level or sentence-level features (Soricut and Marcu, 2003; Reitter, 2003; Baldridge and Lascarides, 2005; Subba and Di Eugenio, 2009; Lin et al., 2009; Luong et al., 2014). A recent prevailing idea for discourse parsing is to train two classifiers, namely a binary structure classifier for determining whether two adjacent text units should be merged to form a new subtree, followed by a multi-class relation classifier for determining which discourse relation label should be assigned to the new subtree. The idea is proposed by Hernault and his colleagues (Duverle and Prendinger, 2009; Hernault et al., 2010a) and followed by other work using more sophisticated features (Feng and Hirst, 2012; Hernault et al., 2010b). Current state-of-art performance for relation ide</context>
</contexts>
<marker>Luong, Frank, Johnson, 2014</marker>
<rawString>Minh-Thang Luong, Michael C Frank, and Mark Johnson. 2014. Parsing entire discourses as very long strings: Capturing topic continuity in grounded language learning.</rawString>
</citation>
<citation valid="true">
<authors>
<author>William C Mann</author>
<author>Sandra A Thompson</author>
</authors>
<title>Rhetorical structure theory: Toward a functional theory of text organization.</title>
<date>1988</date>
<tech>Text, 8(3):243–281.</tech>
<contexts>
<context position="1212" citStr="Mann and Thompson (1988)" startWordPosition="158" endWordPosition="161">ose a recursive model for discourse parsing that jointly models distributed representations for clauses, sentences, and entire discourses. The learned representations can to some extent learn the semantic and intentional import of words and larger discourse units automatically,. The proposed framework obtains comparable performance regarding standard discoursing parsing evaluations when compared against current state-of-art systems. 1 Introduction In a coherent text, units (clauses, sentences, and larger multi-clause groupings) are tightly connected semantically, syntactically, and logically. Mann and Thompson (1988) define a text to be coherent when it is possible to describe clearly the role that each discourse unit (at any level of grouping) plays with respect to the whole. In a coherent text, no unit is completely isolated. Discourse parsing tries to identify how the units are connected with each other and thereby uncover the hierarchical structure of the text, from which multiple NLP tasks can benefit, including text summarization (Louis et al., 2010), sentence compression (Sporleder and Lapata, 2005) or questionanswering (Verberne et al., 2007). Despite recent progress in automatic discourse segment</context>
<context position="5880" citStr="Mann and Thompson, 1988" startWordPosition="884" endWordPosition="887">ies in the recognition that discourse units (minimally, clauses) are related to one another in principled ways, and that the juxtaposition of two units creates a joint meaning larger than either unit’s meaning alone. In a coherent text this juxtaposition is never random, but serves the speaker’s communicative goals. Considerable work on linguistic and computational discourse processing in the 1970s and 80s led to the development of several proposals for relations that combine units; for a compilation see (Hovy and Maier, 1997). Of these the most influential is Rhetorical Structure Theory RST (Mann and Thompson, 1988) that defines about 25 relations, each containing semantic constraints on its component parts plus a description of the overall functional/semantic effect produced as a unit when the parts have been appropriately connected in the text. For example, the SOLUTIONHOOD relation connects one unit describing a problem situation with another describing its solution, using phrases such as “the answer is”; in successful communication the reader will understand that a problem is described and its solution is given. Since there is no syntactic definition of a problem or solution (they can each be stated </context>
<context position="9925" citStr="Mann and Thompson, 1988" startWordPosition="1523" endWordPosition="1526">the input vectors. Many tasks have benefited from the recursive framework, including parsing (Socher et al., 2011b), sentiment analysis (Socher et al., 2013), textual entailment (Bowman, 2013), segmentation (Wang and Mansur, 2013; Houfeng et al., 2013), and paraphrase detection (Socher et al., 2011a). 3 The RST Discourse Treebank There are today two primary alternative discourse treebanks suitable for training data: the Rhetorical Structure Theory Discourse Treebank RSTDT (Carlson et al., 2003) and the Penn Discourse Treebank (Prasad et al., 2008). In this paper, we select the former. In RST (Mann and Thompson, 1988), a coherent context or a document is represented as a hierarchical tree structure, the leaves of which are clause-sized units called Elementary Discourse Units (EDUs). Adjacent nodes (siblings in the tree) are linked with discourse relations that are either binary (hypotactic) or multi-child (paratactic). One child of each hypotactic relation is always more salient (called the NUCLEUS); its sibling (the SATELLITE) is less salient compared and may be omitted in summarization. Multi-nuclear relations (e.g., CONJUNCTION) exhibit no distinction of salience between the units. The RST Discourse Tre</context>
</contexts>
<marker>Mann, Thompson, 1988</marker>
<rawString>William C Mann and Sandra A Thompson. 1988. Rhetorical structure theory: Toward a functional theory of text organization. Text, 8(3):243–281.</rawString>
</citation>
<citation valid="true">
<authors>
<author>Daniel Marcu</author>
</authors>
<title>The rhetorical parsing of unrestricted texts: A surface-based approach.</title>
<date>2000</date>
<journal>Computational Linguistics,</journal>
<volume>26</volume>
<issue>3</issue>
<contexts>
<context position="7191" citStr="Marcu, 2000" startWordPosition="1097" endWordPosition="1098">r communicative (rhetorical) function. The functions are reflected in text as signals of the author’s intentions, and take various forms (including expressions such as “therefore”, “for example”, “the answer is”, and so on; patterns of tense or pronoun usage; syntactic forms; etc.). The signals govern discourse blocks ranging from a clause to an entire text, each one associated with some discourse relation. In order to build a text’s hierarchical structure, a discourse parser needs to recognize these signals and use them to appropriately compose the relationship and nesting. Early approaches (Marcu, 2000a; LeThanh et al., 2004) rely mainly on overt discourse markers (or cue words) and use handcoded rules to build text structure trees, bottom-up from clauses to sentences to paragraphs.... Since a hierarchical discourse tree structure is analogous to a constituency based syntactic tree, modern research explored syntactic parsing techniques (e.g., CKY) for discourse parsing based on multiple text-level or sentence-level features (Soricut and Marcu, 2003; Reitter, 2003; Baldridge and Lascarides, 2005; Subba and Di Eugenio, 2009; Lin et al., 2009; Luong et al., 2014). A recent prevailing idea for </context>
<context position="10693" citStr="Marcu, 2000" startWordPosition="1644" endWordPosition="1645">its (EDUs). Adjacent nodes (siblings in the tree) are linked with discourse relations that are either binary (hypotactic) or multi-child (paratactic). One child of each hypotactic relation is always more salient (called the NUCLEUS); its sibling (the SATELLITE) is less salient compared and may be omitted in summarization. Multi-nuclear relations (e.g., CONJUNCTION) exhibit no distinction of salience between the units. The RST Discourse Treebank contains 385 annotated documents (347 for training and 38 for testing) from the Wall Street Journal. A total of 110 fine-grained relations defined in (Marcu, 2000b) are used for tagging relations in RST-DT. They are subtypes of 18 original high-level RST categories. For fair comparison with existing systems, we use in this work the 18 coarse-grained relation classes, which with nuclearity attached form a set of 41 distinct relations. Non-binary relations are converted into a cascade of right-branching binary relations. Conventionally, discourse parsing in RST-DT involves the following sub-tasks: (1) EDU segmentation to segment the raw text into EDUs, (2) tree-building. Since the segmentation task is essentially clause delimitation and hence relatively </context>
<context position="23625" citStr="Marcu, 2000" startWordPosition="3786" endWordPosition="3787">e structure with rhetorical relation indication but no nuclearity indication). The nuclearity and relation decisions are made based on the multi-class output labels from the deep learning framework. As we do not consider nuclearity when classifying different discourse relations, the two labels attribute[N][S] and attribute[S][N] made by multi-class classifier will be treated as the same relation label ATTRIBUTE. 2Conventionally, evaluation matrices involve precision, recall and F-score in terms of the comparison between tree structures. But these are the same when manual segmentation is used (Marcu, 2000b). 2066 Approach Span Nuclearity Relation HILDA 75.3 60.0 46.8 Joty et al. 82.5 68.4 55.7 Feng and Hirst 85.7 71.0 58.2 Ji and Eisenstein 82.1 71.1 61.6 Unified (with feature) 82.0 70.0 57.1 Ours (no feature) 82.4 69.2 56.8 Ours (with feature) 84.0 70.8 58.6 human 88.7 77.7 65.7 Table 1: Performances for different approaches. Performances for baselines are reprinted from (Joty et al., 2013; Feng and Hirst, 2014; Ji and Eisenstein, 2014). Also, we do not train a separate classifier for NUCLEUS and SATELLITE identification. The nuclearity decision is made based on the relation type produced by </context>
</contexts>
<marker>Marcu, 2000</marker>
<rawString>Daniel Marcu. 2000a. The rhetorical parsing of unrestricted texts: A surface-based approach. Computational Linguistics, 26(3):395–448.</rawString>
</citation>
<citation valid="true">
<authors>
<author>Daniel Marcu</author>
</authors>
<title>The theory and practice of discourse parsing and summarization.</title>
<date>2000</date>
<publisher>MIT Press.</publisher>
<contexts>
<context position="7191" citStr="Marcu, 2000" startWordPosition="1097" endWordPosition="1098">r communicative (rhetorical) function. The functions are reflected in text as signals of the author’s intentions, and take various forms (including expressions such as “therefore”, “for example”, “the answer is”, and so on; patterns of tense or pronoun usage; syntactic forms; etc.). The signals govern discourse blocks ranging from a clause to an entire text, each one associated with some discourse relation. In order to build a text’s hierarchical structure, a discourse parser needs to recognize these signals and use them to appropriately compose the relationship and nesting. Early approaches (Marcu, 2000a; LeThanh et al., 2004) rely mainly on overt discourse markers (or cue words) and use handcoded rules to build text structure trees, bottom-up from clauses to sentences to paragraphs.... Since a hierarchical discourse tree structure is analogous to a constituency based syntactic tree, modern research explored syntactic parsing techniques (e.g., CKY) for discourse parsing based on multiple text-level or sentence-level features (Soricut and Marcu, 2003; Reitter, 2003; Baldridge and Lascarides, 2005; Subba and Di Eugenio, 2009; Lin et al., 2009; Luong et al., 2014). A recent prevailing idea for </context>
<context position="10693" citStr="Marcu, 2000" startWordPosition="1644" endWordPosition="1645">its (EDUs). Adjacent nodes (siblings in the tree) are linked with discourse relations that are either binary (hypotactic) or multi-child (paratactic). One child of each hypotactic relation is always more salient (called the NUCLEUS); its sibling (the SATELLITE) is less salient compared and may be omitted in summarization. Multi-nuclear relations (e.g., CONJUNCTION) exhibit no distinction of salience between the units. The RST Discourse Treebank contains 385 annotated documents (347 for training and 38 for testing) from the Wall Street Journal. A total of 110 fine-grained relations defined in (Marcu, 2000b) are used for tagging relations in RST-DT. They are subtypes of 18 original high-level RST categories. For fair comparison with existing systems, we use in this work the 18 coarse-grained relation classes, which with nuclearity attached form a set of 41 distinct relations. Non-binary relations are converted into a cascade of right-branching binary relations. Conventionally, discourse parsing in RST-DT involves the following sub-tasks: (1) EDU segmentation to segment the raw text into EDUs, (2) tree-building. Since the segmentation task is essentially clause delimitation and hence relatively </context>
<context position="23625" citStr="Marcu, 2000" startWordPosition="3786" endWordPosition="3787">e structure with rhetorical relation indication but no nuclearity indication). The nuclearity and relation decisions are made based on the multi-class output labels from the deep learning framework. As we do not consider nuclearity when classifying different discourse relations, the two labels attribute[N][S] and attribute[S][N] made by multi-class classifier will be treated as the same relation label ATTRIBUTE. 2Conventionally, evaluation matrices involve precision, recall and F-score in terms of the comparison between tree structures. But these are the same when manual segmentation is used (Marcu, 2000b). 2066 Approach Span Nuclearity Relation HILDA 75.3 60.0 46.8 Joty et al. 82.5 68.4 55.7 Feng and Hirst 85.7 71.0 58.2 Ji and Eisenstein 82.1 71.1 61.6 Unified (with feature) 82.0 70.0 57.1 Ours (no feature) 82.4 69.2 56.8 Ours (with feature) 84.0 70.8 58.6 human 88.7 77.7 65.7 Table 1: Performances for different approaches. Performances for baselines are reprinted from (Joty et al., 2013; Feng and Hirst, 2014; Ji and Eisenstein, 2014). Also, we do not train a separate classifier for NUCLEUS and SATELLITE identification. The nuclearity decision is made based on the relation type produced by </context>
</contexts>
<marker>Marcu, 2000</marker>
<rawString>Daniel Marcu. 2000b. The theory and practice of discourse parsing and summarization. MIT Press.</rawString>
</citation>
<citation valid="true">
<authors>
<author>Wenzhe Pei</author>
<author>Tao Ge</author>
<author>Chang Baobao</author>
</authors>
<title>Maxmargin tensor neural network for chinese word segmentation.</title>
<date>2014</date>
<booktitle>In Proceedings ofACL.</booktitle>
<contexts>
<context position="20076" citStr="Pei et al., 2014" startWordPosition="3199" endWordPosition="3202">he structure/multi relation between individual EDUs, additional features are also considered, the usefulness of which has been illustrated in a bunch of existing work (Feng and Hirst, 2012; Hernault et al., 2010b; Joty et al., 2012). We consider the following simple text-level features: • Tokens at the beginning and end of the EDUs. • POS at the beginning and end of the EDUs. • Whether two EDUs are in the same sentence. 5.7 Optimization We use the diagonal variant of AdaGrad (Duchi et al., 2011) with minibatches, which is widely applied in deep learning literature (e.g.,(Socher et al., 2011a; Pei et al., 2014)). The learning rate in AdaGrad is adapted differently for different parameters at different steps. Concretely, let giτ denote the subgradient at time step t for parameter Bi obtained from backpropagation, the parameter update at time step t is given by: α i Bτ = Bτ−1 − T i2 gτ (11) ET ✓gτ where α denotes the learning rate and is set to 0.01 in our approach. Elements in {Wr}, W, Gbinary, Gmulti, Ubinary, Umulti are initialized by randomly drawing from the uniform distribution [−E, E], where E is calculated as suggested in (Collobert et al., 2011). All bias vectors are initialized with 0. Word </context>
</contexts>
<marker>Pei, Ge, Baobao, 2014</marker>
<rawString>Wenzhe Pei, Tao Ge, and Chang Baobao. 2014. Maxmargin tensor neural network for chinese word segmentation. In Proceedings ofACL.</rawString>
</citation>
<citation valid="true">
<authors>
<author>Rashmi Prasad</author>
<author>Nikhil Dinesh</author>
<author>Alan Lee</author>
<author>Eleni Miltsakaki</author>
<author>Livio Robaldo</author>
<author>Aravind K Joshi</author>
<author>Bonnie L Webber</author>
</authors>
<title>The penn discourse treebank 2.0. In LREC.</title>
<date>2008</date>
<publisher>Citeseer.</publisher>
<contexts>
<context position="9854" citStr="Prasad et al., 2008" startWordPosition="1510" endWordPosition="1513">, 2013) that allows the model to have greater interactions between the input vectors. Many tasks have benefited from the recursive framework, including parsing (Socher et al., 2011b), sentiment analysis (Socher et al., 2013), textual entailment (Bowman, 2013), segmentation (Wang and Mansur, 2013; Houfeng et al., 2013), and paraphrase detection (Socher et al., 2011a). 3 The RST Discourse Treebank There are today two primary alternative discourse treebanks suitable for training data: the Rhetorical Structure Theory Discourse Treebank RSTDT (Carlson et al., 2003) and the Penn Discourse Treebank (Prasad et al., 2008). In this paper, we select the former. In RST (Mann and Thompson, 1988), a coherent context or a document is represented as a hierarchical tree structure, the leaves of which are clause-sized units called Elementary Discourse Units (EDUs). Adjacent nodes (siblings in the tree) are linked with discourse relations that are either binary (hypotactic) or multi-child (paratactic). One child of each hypotactic relation is always more salient (called the NUCLEUS); its sibling (the SATELLITE) is less salient compared and may be omitted in summarization. Multi-nuclear relations (e.g., CONJUNCTION) exhi</context>
</contexts>
<marker>Prasad, Dinesh, Lee, Miltsakaki, Robaldo, Joshi, Webber, 2008</marker>
<rawString>Rashmi Prasad, Nikhil Dinesh, Alan Lee, Eleni Miltsakaki, Livio Robaldo, Aravind K Joshi, and Bonnie L Webber. 2008. The penn discourse treebank 2.0. In LREC. Citeseer.</rawString>
</citation>
<citation valid="true">
<authors>
<author>David Reitter</author>
</authors>
<title>Simple signals for complex rhetorics: On rhetorical analysis with rich-feature support vector models.</title>
<date>2003</date>
<booktitle>In LDV Forum,</booktitle>
<volume>18</volume>
<pages>38--52</pages>
<contexts>
<context position="7661" citStr="Reitter, 2003" startWordPosition="1168" endWordPosition="1169">ourse parser needs to recognize these signals and use them to appropriately compose the relationship and nesting. Early approaches (Marcu, 2000a; LeThanh et al., 2004) rely mainly on overt discourse markers (or cue words) and use handcoded rules to build text structure trees, bottom-up from clauses to sentences to paragraphs.... Since a hierarchical discourse tree structure is analogous to a constituency based syntactic tree, modern research explored syntactic parsing techniques (e.g., CKY) for discourse parsing based on multiple text-level or sentence-level features (Soricut and Marcu, 2003; Reitter, 2003; Baldridge and Lascarides, 2005; Subba and Di Eugenio, 2009; Lin et al., 2009; Luong et al., 2014). A recent prevailing idea for discourse parsing is to train two classifiers, namely a binary structure classifier for determining whether two adjacent text units should be merged to form a new subtree, followed by a multi-class relation classifier for determining which discourse relation label should be assigned to the new subtree. The idea is proposed by Hernault and his colleagues (Duverle and Prendinger, 2009; Hernault et al., 2010a) and followed by other work using more sophisticated feature</context>
</contexts>
<marker>Reitter, 2003</marker>
<rawString>David Reitter. 2003. Simple signals for complex rhetorics: On rhetorical analysis with rich-feature support vector models. In LDV Forum, volume 18, pages 38–52.</rawString>
</citation>
<citation valid="true">
<authors>
<author>Richard Socher</author>
<author>Christopher D Manning</author>
<author>Andrew Y Ng</author>
</authors>
<title>Learning continuous phrase representations and syntactic parsing with recursive neural networks.</title>
<date>2010</date>
<booktitle>In Proceedings of the NIPS-2010 Deep Learning and Unsupervised Feature Learning Workshop,</booktitle>
<pages>1--9</pages>
<contexts>
<context position="19365" citStr="Socher et al., 2010" startWordPosition="3080" endWordPosition="3083">sional binary vector R(ei, ej), which denotes the ground truth vector with a 1 at the correct label r(ei, ej) and all other entries 0. Integrating softmax error vector, for any parameter θ, the derivative of Jmulti(ei, ej) with respect to θ is given by: aS(ei,ej) aθ where ⊗ denotes the Hadamard product between the two vectors. Each training pair recursively backpropagates its error to some node in the discourse tree through [{Wr}, {br}], and then to nodes in sentence parse tree through [W, b], and the derivatives can be obtained according to standard backpropagation (Goller and Kuchler, 1996; Socher et al., 2010). aJmulti(ei, ej) = [P(ei,ej) − R(ei,ej)] ⊗ aθ 2065 5.6 Additional Features When determining the structure/multi relation between individual EDUs, additional features are also considered, the usefulness of which has been illustrated in a bunch of existing work (Feng and Hirst, 2012; Hernault et al., 2010b; Joty et al., 2012). We consider the following simple text-level features: • Tokens at the beginning and end of the EDUs. • POS at the beginning and end of the EDUs. • Whether two EDUs are in the same sentence. 5.7 Optimization We use the diagonal variant of AdaGrad (Duchi et al., 2011) with </context>
</contexts>
<marker>Socher, Manning, Ng, 2010</marker>
<rawString>Richard Socher, Christopher D Manning, and Andrew Y Ng. 2010. Learning continuous phrase representations and syntactic parsing with recursive neural networks. In Proceedings of the NIPS-2010 Deep Learning and Unsupervised Feature Learning Workshop, pages 1–9.</rawString>
</citation>
<citation valid="true">
<authors>
<author>Richard Socher</author>
<author>Eric H Huang</author>
<author>Jeffrey Pennington</author>
<author>Andrew Y Ng</author>
<author>Christopher D Manning</author>
</authors>
<title>Dynamic pooling and unfolding recursive autoencoders for paraphrase detection.</title>
<date>2011</date>
<booktitle>In NIPS,</booktitle>
<volume>24</volume>
<pages>801--809</pages>
<contexts>
<context position="9414" citStr="Socher et al., 2011" startWordPosition="1445" endWordPosition="1448">ive framework relies and operates on structured inputs (e.g., a parse tree) and computes the representation for each parent based on its children 2062 iteratively in a bottom-up fashion. A series of variations of RNN has been proposed to tailor different task-specific requirements, including MatrixVector RNN (Socher et al., 2012) that represents every word as both a vector and a matrix, or Recursive Neural Tensor Network (Socher et al., 2013) that allows the model to have greater interactions between the input vectors. Many tasks have benefited from the recursive framework, including parsing (Socher et al., 2011b), sentiment analysis (Socher et al., 2013), textual entailment (Bowman, 2013), segmentation (Wang and Mansur, 2013; Houfeng et al., 2013), and paraphrase detection (Socher et al., 2011a). 3 The RST Discourse Treebank There are today two primary alternative discourse treebanks suitable for training data: the Rhetorical Structure Theory Discourse Treebank RSTDT (Carlson et al., 2003) and the Penn Discourse Treebank (Prasad et al., 2008). In this paper, we select the former. In RST (Mann and Thompson, 1988), a coherent context or a document is represented as a hierarchical tree structure, the l</context>
<context position="20056" citStr="Socher et al., 2011" startWordPosition="3195" endWordPosition="3198">res When determining the structure/multi relation between individual EDUs, additional features are also considered, the usefulness of which has been illustrated in a bunch of existing work (Feng and Hirst, 2012; Hernault et al., 2010b; Joty et al., 2012). We consider the following simple text-level features: • Tokens at the beginning and end of the EDUs. • POS at the beginning and end of the EDUs. • Whether two EDUs are in the same sentence. 5.7 Optimization We use the diagonal variant of AdaGrad (Duchi et al., 2011) with minibatches, which is widely applied in deep learning literature (e.g.,(Socher et al., 2011a; Pei et al., 2014)). The learning rate in AdaGrad is adapted differently for different parameters at different steps. Concretely, let giτ denote the subgradient at time step t for parameter Bi obtained from backpropagation, the parameter update at time step t is given by: α i Bτ = Bτ−1 − T i2 gτ (11) ET ✓gτ where α denotes the learning rate and is set to 0.01 in our approach. Elements in {Wr}, W, Gbinary, Gmulti, Ubinary, Umulti are initialized by randomly drawing from the uniform distribution [−E, E], where E is calculated as suggested in (Collobert et al., 2011). All bias vectors are initi</context>
</contexts>
<marker>Socher, Huang, Pennington, Ng, Manning, 2011</marker>
<rawString>Richard Socher, Eric H Huang, Jeffrey Pennington, Andrew Y Ng, and Christopher D Manning. 2011a. Dynamic pooling and unfolding recursive autoencoders for paraphrase detection. In NIPS, volume 24, pages 801–809.</rawString>
</citation>
<citation valid="true">
<authors>
<author>Richard Socher</author>
<author>Cliff C Lin</author>
<author>Chris Manning</author>
<author>Andrew Y Ng</author>
</authors>
<title>Parsing natural scenes and natural language with recursive neural networks.</title>
<date>2011</date>
<booktitle>In Proceedings of the 28th International Conference on Machine Learning (ICML-11),</booktitle>
<pages>129--136</pages>
<contexts>
<context position="9414" citStr="Socher et al., 2011" startWordPosition="1445" endWordPosition="1448">ive framework relies and operates on structured inputs (e.g., a parse tree) and computes the representation for each parent based on its children 2062 iteratively in a bottom-up fashion. A series of variations of RNN has been proposed to tailor different task-specific requirements, including MatrixVector RNN (Socher et al., 2012) that represents every word as both a vector and a matrix, or Recursive Neural Tensor Network (Socher et al., 2013) that allows the model to have greater interactions between the input vectors. Many tasks have benefited from the recursive framework, including parsing (Socher et al., 2011b), sentiment analysis (Socher et al., 2013), textual entailment (Bowman, 2013), segmentation (Wang and Mansur, 2013; Houfeng et al., 2013), and paraphrase detection (Socher et al., 2011a). 3 The RST Discourse Treebank There are today two primary alternative discourse treebanks suitable for training data: the Rhetorical Structure Theory Discourse Treebank RSTDT (Carlson et al., 2003) and the Penn Discourse Treebank (Prasad et al., 2008). In this paper, we select the former. In RST (Mann and Thompson, 1988), a coherent context or a document is represented as a hierarchical tree structure, the l</context>
<context position="20056" citStr="Socher et al., 2011" startWordPosition="3195" endWordPosition="3198">res When determining the structure/multi relation between individual EDUs, additional features are also considered, the usefulness of which has been illustrated in a bunch of existing work (Feng and Hirst, 2012; Hernault et al., 2010b; Joty et al., 2012). We consider the following simple text-level features: • Tokens at the beginning and end of the EDUs. • POS at the beginning and end of the EDUs. • Whether two EDUs are in the same sentence. 5.7 Optimization We use the diagonal variant of AdaGrad (Duchi et al., 2011) with minibatches, which is widely applied in deep learning literature (e.g.,(Socher et al., 2011a; Pei et al., 2014)). The learning rate in AdaGrad is adapted differently for different parameters at different steps. Concretely, let giτ denote the subgradient at time step t for parameter Bi obtained from backpropagation, the parameter update at time step t is given by: α i Bτ = Bτ−1 − T i2 gτ (11) ET ✓gτ where α denotes the learning rate and is set to 0.01 in our approach. Elements in {Wr}, W, Gbinary, Gmulti, Ubinary, Umulti are initialized by randomly drawing from the uniform distribution [−E, E], where E is calculated as suggested in (Collobert et al., 2011). All bias vectors are initi</context>
</contexts>
<marker>Socher, Lin, Manning, Ng, 2011</marker>
<rawString>Richard Socher, Cliff C Lin, Chris Manning, and Andrew Y Ng. 2011b. Parsing natural scenes and natural language with recursive neural networks. In Proceedings of the 28th International Conference on Machine Learning (ICML-11), pages 129–136.</rawString>
</citation>
<citation valid="true">
<authors>
<author>Richard Socher</author>
<author>Brody Huval</author>
<author>Christopher D Manning</author>
<author>Andrew Y Ng</author>
</authors>
<title>Semantic compositionality through recursive matrix-vector spaces.</title>
<date>2012</date>
<booktitle>In Proceedings of the 2012 Joint Conference on Empirical Methods in Natural Language Processing and Computational Natural Language Learning,</booktitle>
<pages>1201--1211</pages>
<publisher>Association</publisher>
<institution>for Computational Linguistics.</institution>
<contexts>
<context position="9126" citStr="Socher et al., 2012" startWordPosition="1397" endWordPosition="1400">this paper is similar to (Ji and Eisenstein, 2014) for transforming the discourse units to the abstract representations. 2.2 Recursive Deep Learning Recursive neural networks constitute one type of deep learning frameworks which was first proposed in (Goller and Kuchler, 1996). The recursive framework relies and operates on structured inputs (e.g., a parse tree) and computes the representation for each parent based on its children 2062 iteratively in a bottom-up fashion. A series of variations of RNN has been proposed to tailor different task-specific requirements, including MatrixVector RNN (Socher et al., 2012) that represents every word as both a vector and a matrix, or Recursive Neural Tensor Network (Socher et al., 2013) that allows the model to have greater interactions between the input vectors. Many tasks have benefited from the recursive framework, including parsing (Socher et al., 2011b), sentiment analysis (Socher et al., 2013), textual entailment (Bowman, 2013), segmentation (Wang and Mansur, 2013; Houfeng et al., 2013), and paraphrase detection (Socher et al., 2011a). 3 The RST Discourse Treebank There are today two primary alternative discourse treebanks suitable for training data: the R</context>
</contexts>
<marker>Socher, Huval, Manning, Ng, 2012</marker>
<rawString>Richard Socher, Brody Huval, Christopher D Manning, and Andrew Y Ng. 2012. Semantic compositionality through recursive matrix-vector spaces. In Proceedings of the 2012 Joint Conference on Empirical Methods in Natural Language Processing and Computational Natural Language Learning, pages 1201–1211. Association for Computational Linguistics.</rawString>
</citation>
<citation valid="true">
<authors>
<author>Richard Socher</author>
<author>Alex Perelygin</author>
<author>Jean Y Wu</author>
<author>Jason Chuang</author>
<author>Christopher D Manning</author>
<author>Andrew Y Ng</author>
<author>Christopher Potts</author>
</authors>
<title>Recursive deep models for semantic compositionality over a sentiment treebank.</title>
<date>2013</date>
<booktitle>In Proceedings of the Conference on Empirical Methods in Natural Language Processing (EMNLP),</booktitle>
<pages>1631--1642</pages>
<contexts>
<context position="9241" citStr="Socher et al., 2013" startWordPosition="1418" endWordPosition="1421">ions. 2.2 Recursive Deep Learning Recursive neural networks constitute one type of deep learning frameworks which was first proposed in (Goller and Kuchler, 1996). The recursive framework relies and operates on structured inputs (e.g., a parse tree) and computes the representation for each parent based on its children 2062 iteratively in a bottom-up fashion. A series of variations of RNN has been proposed to tailor different task-specific requirements, including MatrixVector RNN (Socher et al., 2012) that represents every word as both a vector and a matrix, or Recursive Neural Tensor Network (Socher et al., 2013) that allows the model to have greater interactions between the input vectors. Many tasks have benefited from the recursive framework, including parsing (Socher et al., 2011b), sentiment analysis (Socher et al., 2013), textual entailment (Bowman, 2013), segmentation (Wang and Mansur, 2013; Houfeng et al., 2013), and paraphrase detection (Socher et al., 2011a). 3 The RST Discourse Treebank There are today two primary alternative discourse treebanks suitable for training data: the Rhetorical Structure Theory Discourse Treebank RSTDT (Carlson et al., 2003) and the Penn Discourse Treebank (Prasad </context>
<context position="11711" citStr="Socher et al., 2013" startWordPosition="1802" endWordPosition="1805">n RST-DT involves the following sub-tasks: (1) EDU segmentation to segment the raw text into EDUs, (2) tree-building. Since the segmentation task is essentially clause delimitation and hence relatively easy (with state-of-art accuracy at most 95%), we focus on the latter problem. We assume that the gold-standard EDU segmentations are already given, as assumed in other past work (Feng and Hirst, 2012). 4 EDU Model In this section, we describe how we compute the distributed representation for a given sentence based on its parse tree structure and contained words. Our implementation is based on (Socher et al., 2013). As the details can easily be found there, we omit them for brevity. Let s denote any given sentence, comprised of a sequence of tokens s = {w1, w2, ..., wn3}, where ns denotes the number of tokens in s. Each token w is associated with a specific vector embedding e, = {e1�, e2 �, ..., e�� }, where K denotes the dimension of the word embedding. We wish to compute the vector representation hs for current sentence, where hs = {h1s, h2s, ..., hs }. Parse trees are obtained using the Stanford Parser1, and each clause is treated as an EDU. For a given parent p in the tree and its two children c1 (a</context>
</contexts>
<marker>Socher, Perelygin, Wu, Chuang, Manning, Ng, Potts, 2013</marker>
<rawString>Richard Socher, Alex Perelygin, Jean Y Wu, Jason Chuang, Christopher D Manning, Andrew Y Ng, and Christopher Potts. 2013. Recursive deep models for semantic compositionality over a sentiment treebank. In Proceedings of the Conference on Empirical Methods in Natural Language Processing (EMNLP), pages 1631–1642.</rawString>
</citation>
<citation valid="true">
<authors>
<author>Radu Soricut</author>
<author>Daniel Marcu</author>
</authors>
<title>Sentence level discourse parsing using syntactic and lexical information.</title>
<date>2003</date>
<booktitle>In Proceedings of the 2003 Conference of the North American Chapter of the Association for Computational Linguistics on Human Language Technology-Volume 1,</booktitle>
<pages>149--156</pages>
<publisher>Association</publisher>
<institution>for Computational Linguistics.</institution>
<contexts>
<context position="1920" citStr="Soricut and Marcu, 2003" startWordPosition="273" endWordPosition="277">each discourse unit (at any level of grouping) plays with respect to the whole. In a coherent text, no unit is completely isolated. Discourse parsing tries to identify how the units are connected with each other and thereby uncover the hierarchical structure of the text, from which multiple NLP tasks can benefit, including text summarization (Louis et al., 2010), sentence compression (Sporleder and Lapata, 2005) or questionanswering (Verberne et al., 2007). Despite recent progress in automatic discourse segmentation and sentence-level parsing (e.g., (Fisher and Roark, 2007; Joty et al., 2012; Soricut and Marcu, 2003), document-level discourse parsing remains a significant challenge. Recent attempts (e.g., (Hernault et al., 2010b; Feng and Hirst, 2012; Joty et al., 2013)) are still considerably inferior when compared to human goldstandard discourse analysis. The challenge stems from the fact that compared with sentence-level dependency parsing, the set of relations between discourse units is less straightforward to define. Because there are no clause-level ‘parts of discourse’ analogous to word-level parts of speech, there is no discourse-level grammar analogous to sentence-level grammar. To understand how</context>
<context position="7646" citStr="Soricut and Marcu, 2003" startWordPosition="1164" endWordPosition="1167">rchical structure, a discourse parser needs to recognize these signals and use them to appropriately compose the relationship and nesting. Early approaches (Marcu, 2000a; LeThanh et al., 2004) rely mainly on overt discourse markers (or cue words) and use handcoded rules to build text structure trees, bottom-up from clauses to sentences to paragraphs.... Since a hierarchical discourse tree structure is analogous to a constituency based syntactic tree, modern research explored syntactic parsing techniques (e.g., CKY) for discourse parsing based on multiple text-level or sentence-level features (Soricut and Marcu, 2003; Reitter, 2003; Baldridge and Lascarides, 2005; Subba and Di Eugenio, 2009; Lin et al., 2009; Luong et al., 2014). A recent prevailing idea for discourse parsing is to train two classifiers, namely a binary structure classifier for determining whether two adjacent text units should be merged to form a new subtree, followed by a multi-class relation classifier for determining which discourse relation label should be assigned to the new subtree. The idea is proposed by Hernault and his colleagues (Duverle and Prendinger, 2009; Hernault et al., 2010a) and followed by other work using more sophis</context>
</contexts>
<marker>Soricut, Marcu, 2003</marker>
<rawString>Radu Soricut and Daniel Marcu. 2003. Sentence level discourse parsing using syntactic and lexical information. In Proceedings of the 2003 Conference of the North American Chapter of the Association for Computational Linguistics on Human Language Technology-Volume 1, pages 149–156. Association for Computational Linguistics.</rawString>
</citation>
<citation valid="true">
<authors>
<author>Caroline Sporleder</author>
<author>Mirella Lapata</author>
</authors>
<title>Discourse chunking and its application to sentence compression.</title>
<date>2005</date>
<booktitle>In Proceedings of the conference on Human Language Technology and Empirical Methods in Natural Language Processing,</booktitle>
<pages>257--264</pages>
<publisher>Association</publisher>
<institution>for Computational Linguistics.</institution>
<contexts>
<context position="1711" citStr="Sporleder and Lapata, 2005" startWordPosition="243" endWordPosition="246">s, and larger multi-clause groupings) are tightly connected semantically, syntactically, and logically. Mann and Thompson (1988) define a text to be coherent when it is possible to describe clearly the role that each discourse unit (at any level of grouping) plays with respect to the whole. In a coherent text, no unit is completely isolated. Discourse parsing tries to identify how the units are connected with each other and thereby uncover the hierarchical structure of the text, from which multiple NLP tasks can benefit, including text summarization (Louis et al., 2010), sentence compression (Sporleder and Lapata, 2005) or questionanswering (Verberne et al., 2007). Despite recent progress in automatic discourse segmentation and sentence-level parsing (e.g., (Fisher and Roark, 2007; Joty et al., 2012; Soricut and Marcu, 2003), document-level discourse parsing remains a significant challenge. Recent attempts (e.g., (Hernault et al., 2010b; Feng and Hirst, 2012; Joty et al., 2013)) are still considerably inferior when compared to human goldstandard discourse analysis. The challenge stems from the fact that compared with sentence-level dependency parsing, the set of relations between discourse units is less stra</context>
</contexts>
<marker>Sporleder, Lapata, 2005</marker>
<rawString>Caroline Sporleder and Mirella Lapata. 2005. Discourse chunking and its application to sentence compression. In Proceedings of the conference on Human Language Technology and Empirical Methods in Natural Language Processing, pages 257–264. Association for Computational Linguistics.</rawString>
</citation>
<citation valid="true">
<authors>
<author>Rajen Subba</author>
<author>Barbara Di Eugenio</author>
</authors>
<title>An effective discourse parser that uses rich linguistic information.</title>
<date>2009</date>
<booktitle>In Proceedings of Human Language Technologies: The 2009 Annual Conference of the North American Chapter of the Association for Computational Linguistics,</booktitle>
<pages>566--574</pages>
<publisher>Association</publisher>
<institution>for Computational Linguistics.</institution>
<marker>Subba, Di Eugenio, 2009</marker>
<rawString>Rajen Subba and Barbara Di Eugenio. 2009. An effective discourse parser that uses rich linguistic information. In Proceedings of Human Language Technologies: The 2009 Annual Conference of the North American Chapter of the Association for Computational Linguistics, pages 566–574. Association for Computational Linguistics.</rawString>
</citation>
<citation valid="true">
<authors>
<author>Suzan Verberne</author>
<author>Lou Boves</author>
<author>Nelleke Oostdijk</author>
<author>Peter-Arno Coppen</author>
</authors>
<title>Evaluating discoursebased answer extraction for why-question answering.</title>
<date>2007</date>
<booktitle>In Proceedings of the 30th annual international ACM SIGIR conference on Research and development in information retrieval,</booktitle>
<pages>735--736</pages>
<publisher>ACM.</publisher>
<contexts>
<context position="1756" citStr="Verberne et al., 2007" startWordPosition="250" endWordPosition="253">connected semantically, syntactically, and logically. Mann and Thompson (1988) define a text to be coherent when it is possible to describe clearly the role that each discourse unit (at any level of grouping) plays with respect to the whole. In a coherent text, no unit is completely isolated. Discourse parsing tries to identify how the units are connected with each other and thereby uncover the hierarchical structure of the text, from which multiple NLP tasks can benefit, including text summarization (Louis et al., 2010), sentence compression (Sporleder and Lapata, 2005) or questionanswering (Verberne et al., 2007). Despite recent progress in automatic discourse segmentation and sentence-level parsing (e.g., (Fisher and Roark, 2007; Joty et al., 2012; Soricut and Marcu, 2003), document-level discourse parsing remains a significant challenge. Recent attempts (e.g., (Hernault et al., 2010b; Feng and Hirst, 2012; Joty et al., 2013)) are still considerably inferior when compared to human goldstandard discourse analysis. The challenge stems from the fact that compared with sentence-level dependency parsing, the set of relations between discourse units is less straightforward to define. Because there are no c</context>
</contexts>
<marker>Verberne, Boves, Oostdijk, Coppen, 2007</marker>
<rawString>Suzan Verberne, Lou Boves, Nelleke Oostdijk, and Peter-Arno Coppen. 2007. Evaluating discoursebased answer extraction for why-question answering. In Proceedings of the 30th annual international ACM SIGIR conference on Research and development in information retrieval, pages 735–736. ACM.</rawString>
</citation>
<citation valid="true">
<authors>
<author>Longkai Zhang</author>
</authors>
<title>Houfeng Wang and Xu Sun Mairgup Mansur.</title>
<date>2013</date>
<marker>Zhang, 2013</marker>
<rawString>Longkai Zhang Houfeng Wang and Xu Sun Mairgup Mansur. 2013. Exploring representations from unlabeled data with co-training for chinese word segmentation.</rawString>
</citation>
</citationList>
</algorithm>
</algorithms>
<?xml version="1.0" encoding="UTF-8"?>
<algorithms version="110505">
<algorithm name="SectLabel" version="110505">
<variant no="0" confidence="0.001740">
<title confidence="0.981094">
A Shortest-path Method for Arc-factored Semantic Role Labeling
</title>
<note confidence="0.91398475">
Xavier Lluis Xavier Carreras Lluis M`arquez
TALP Research Center Xerox Research Centre ALT Research Group
Universitat Polit`ecnica de Europe Qatar Computing Research
Catalunya xavier.carreras@xrce.xerox.com Institute
</note>
<email confidence="0.873099">
xlluis@cs.upc.edu lmarquez@qf.org.qa
</email>
<sectionHeader confidence="0.991408" genericHeader="abstract">
Abstract
</sectionHeader>
<bodyText confidence="0.999959055555556">
We introduce a Semantic Role Labeling
(SRL) parser that finds semantic roles for a
predicate together with the syntactic paths
linking predicates and arguments. Our
main contribution is to formulate SRL in
terms of shortest-path inference, on the as-
sumption that the SRL model is restricted
to arc-factored features of the syntactic
paths behind semantic roles. Overall, our
method for SRL is a novel way to ex-
ploit larger variability in the syntactic re-
alizations of predicate-argument relations,
moving away from pipeline architectures.
Experiments show that our approach im-
proves the robustness of the predictions,
producing arc-factored models that per-
form closely to methods using unrestricted
features from the syntax.
</bodyText>
<sectionHeader confidence="0.998992" genericHeader="introduction">
1 Introduction
</sectionHeader>
<bodyText confidence="0.999963983333333">
Semantic role labeling (SRL) consists of finding
the arguments of a predicate and labeling them
with semantic roles (Gildea and Jurafsky, 2002;
M`arquez et al., 2008). The arguments fill roles that
answer questions of the type “who” did “what” to
“whom”, “how”, and “why” for a given sentence
predicate. Most approaches to SRL are based on
a pipeline strategy, first parsing the sentence to
obtain a syntactic tree and then identifying and
classifying arguments (Gildea and Jurafsky, 2002;
Carreras and M`arquez, 2005).
SRL methods critically depend on features of
the syntactic structure, and consequently parsing
mistakes can harm the quality of semantic role
predictions (Gildea and Palmer, 2002). To allevi-
ate this dependence, previous work has explored
k-best parsers (Johansson and Nugues, 2008),
combination systems (Surdeanu et al., 2007) or
joint syntactic-semantic models (Johansson, 2009;
Henderson et al., 2008; Llu´ıs et al., 2013).
In this paper we take a different approach. In
our scenario SRL is the end goal, and we as-
sume that syntactic parsing is only an intermedi-
ate step to extract features to support SRL predic-
tions. In this setting we define a model that, given
a predicate, identifies each of the semantic roles
together with the syntactic path that links the pred-
icate with the argument. Thus, following previous
work (Moschitti, 2004; Johansson, 2009), we take
the syntactic path as the main source of syntac-
tic features, but instead of just conditioning on it,
we predict it together with the semantic role. The
main contribution of this paper is a formulation of
SRL parsing in terms of efficient shortest-path in-
ference, under the assumption that the SRL model
is restricted to arc-factored features of the syntac-
tic path linking the argument with the predicate.
Our assumption —that features of an SRL
model should factor over dependency arcs— is
supported by some empirical frequencies. Table 1
shows the most frequent path patterns on CoNLL-
2009 (Hajiˇc et al., 2009) data for several lan-
guages, where a path pattern is a sequence of as-
cending arcs from the predicate to some ancestor,
followed by descending arcs to the argument. For
English the distribution of path patterns is rather
simple: the majority of paths consists of a num-
ber of ascending arcs followed by zero or one de-
scending arc. Thus a common strategy in SRL sys-
tems, formulated by Xue and Palmer (2004), is to
look for arguments in the ancestors of the pred-
icate and their direct descendants. However, in
Czech and Japanese data we observe a large por-
tion of paths with two or more descending arcs,
which makes it difficult to characterize the syn-
tactic scope in which arguments are found. Also,
in the datasets for German, Czech and Chinese the
three most frequent patterns cover over the 90% of
all arguments. In contrast, Japanese exhibits much
more variability and a long tail of infrequent types
</bodyText>
<page confidence="0.939566">
430
</page>
<note confidence="0.51195">
Proceedings of the 2014 Conference on Empirical Methods in Natural Language Processing (EMNLP), pages 430–435,
October 25-29, 2014, Doha, Qatar. c�2014 Association for Computational Linguistics
</note>
<table confidence="0.999655619047619">
English German Czech Chinese Japanese
E % % path E % % path E % % path E % % path E % % path
63.63 63.6298 ↓ 77.22 77.2202 ↓ 63.90 63.8956 ↓ 78.09 78.0949 ↓ 37.20 37.1977 ↓↓
73.97 10.3429 ↑↓ 93.51 16.2854 ↑↓ 86.26 22.3613 ↓↓ 85.36 7.26962 ↑↓ 51.52 14.3230 ↓
80.63 6.65915 ◦ 97.43 3.92111 ↑↑↓ 90.24 3.98078 ↑↓ 91.27 5.90333 ↑↑↓ 60.79 9.27270 ↓↓↓
85.97 5.33352 ↑ 98.19 0.76147 ↓↓ 93.95 3.71713 ↓↓↓ 95.93 4.66039 ↑↑ 70.03 9.23857 ↑
90.78 4.81104 ↑↑↓ 98.70 0.51640 ↑↑↑↓ 95.48 1.52168 ↑↓↓ 97.53 1.60392 ↑ 74.17 4.13359 ↓↓↓↓
93.10 2.31928 ↑↑↑↓ 99.17 0.46096 ↑ 96.92 1.44091 ↑ 98.28 0.75086 ↑↑↑↓ 76.76 2.59117 ↑↑
95.19 2.09043 ↑↑ 99.43 0.26841 ↑↓↓ 97.68 0.76714 ↑↑↓ 98.77 0.48734 ↓↓ 78.82 2.06111 ↑↑↓↓
96.26 1.07468 ↑↑↑↑↓ 99.56 0.12837 ↑↑↓↓ 98.28 0.59684 ↓↓↓↓ 99.13 0.36270 ↑↑↑ 80.85 2.03381 ↓↓↓↓↓
97.19 0.92482 ↓↓ 99.67 0.10503 ↑↑↑↑↓ 98.60 0.31759 ↑↓↓↓ 99.45 0.31699 ↑↑↑↑↓ 82.66 1.80631 ↑↓↓
97.93 0.74041 ↑↑↑ 99.77 0.10503 ↑↑ 98.88 0.28227 ↑↑↓↓ 99.72 0.27041 ↑↑↑↑ 83.71 1.05558 ↑↑↑
98.41 0.48565 ↑↑↑↑↑↓ 99.82 0.04960 ↓↓↓ 99.15 0.26721 ↑↑↑↓ 99.82 0.10049 ↓↓↓ 84.74 1.02828 ↑↑↑↓↓
98.71 0.29769 ↑↑↑↑ 99.87 0.04960 ↑↑↑ 99.27 0.12430 ↓↓↓↓↓ 99.86 0.03623 ↑↓↓ 85.68 0.93500 ↑↑↓↓↓
98.94 0.22733 ↑↑↑↑↑↑↓ 99.90 0.02626 ◦ 99.37 0.10103 ↑↑↑↑↓ 99.89 0.02890 ↑↑↓↓ 86.61 0.93273 ↓↓↓↓↓↓
99.11 0.17805 ↑↓↓ 99.92 0.02042 ↑↑↑↓↓ 99.47 0.09747 ↑↑ 99.92 0.02890 ↑↑↑↑↑↓ 87.29 0.68249 ↑↑↑↑↓↓
99.27 0.15316 ↓↓↓ 99.94 0.02042 ↑↑↑↑↑↓ 99.56 0.08515 ↑↑↓↓↓ 99.94 0.02846 ◦ 87.90 0.60969 ↑↓↓↓
99.39 0.12065 ↑↑↑↑↑ 99.95 0.01459 ↑↑↓↓↓ 99.63 0.07419 ↑↑↑↓↓ 99.96 0.02070 ↑↑↑↑↑ 88.47 0.56646 ↑↑↓↓↓↓
99.50 0.11024 ↑↑↓↓ 99.96 0.01167 ↓↓↓↓ 99.69 0.05667 ↑↓↓↓↓ 99.97 0.00992 ↑↑↓↓↓ 89.01 0.53689 ↓↓↓↓↓↓↓
99.60 0.09931 ↑↑↑↑↑↑↑↓ 99.97 0.00875 ↑↓↓↓ 99.73 0.04216 ↑↑↑↑↑↓ 99.98 0.00733 ↑↑↑↑↑↑↓ 89.49 0.48684 ↑↑↑↓↓↓
99.65 0.05283 ↑↓↓↓ 99.98 0.00875 ↑↑↑↑↑↑↓ 99.76 0.02875 ↑↑↑↓↓↓ 99.99 0.00431 ↑↑↑↑↓↓ 89.94 0.45044 ↑↑↑↑
</table>
<tableCaption confidence="0.98160875">
Table 1: Summary of the most frequent paths on the CoNLL-2009 Shared Task datasets. ↑ indicates that we traverse a syntactic
dependency upwards from a modifier to a head. ↓ is for dependencies following a descending head to modifier edge. The
symbol ◦ represents that the argument is the predicate itself. We exclude from this table Catalan and Spanish as predicates and
arguments are always trivially related by a single syntactic dependency that descends.
</tableCaption>
<bodyText confidence="0.995347714285714">
of patterns. In general it is not feasible to capture
path patterns manually, and it is not desirable that
a statistical system depends on rather sparse non-
factored path features. For this reason in this paper
we explore arc-factored models for SRL.
Our method might be specially useful in appli-
cations were we are interested in some target se-
mantic role, i.e. retrieving agent relations for some
verb, since it processes semantic roles indepen-
dently of each other. Our method might also be
generalizable to other kinds of semantic relations
which strongly depend on syntactic patterns such
as relation extraction in information extraction or
discourse parsing.
</bodyText>
<sectionHeader confidence="0.994233" genericHeader="method">
2 Arc-factored SRL
</sectionHeader>
<bodyText confidence="0.999920642857143">
We define an SRL parsing model that re-
trieves predicate-argument relations based on arc-
factored syntactic representations of paths con-
necting predicates with their arguments. Through-
out the paper we assume a fixed sentence x =
x1, ... , xn and a fixed predicate index p. The
SRL output is an indicator vector z, where
zr,a = 1 indicates that token a is filling role
r for predicate p. Our SRL parser performs
argmaxzEZ(x,p) s(x, p, z), where i(x, p) defines
the set of valid argument structures for p, and
s(x, p, z) computes a plausibility score for z given
x and p. Our first assumption is that the score
function factors over role-argument pairs:
</bodyText>
<equation confidence="0.94141">
�s(x, p, z) = s(x, p, r, a) . (1)
zr,a=1
</equation>
<bodyText confidence="0.999279">
Then we assume two components in the model,
one that scores the role-argument pair alone, and
another that considers the best (max) syntactic de-
pendency path π that connects the predicate p with
the argument a:
</bodyText>
<equation confidence="0.797682333333333">
s(x, p, r, a) = s0(x, p, r, a) +
max ssyn(x, p, r, a, π) . (2)
π
</equation>
<bodyText confidence="0.99874475">
The model does not assume access to the syntac-
tic structure of x, hence in Eq. (2) we locally re-
trieve the maximum-scoring path for an argument-
role pair. A path π is a sequence of dependencies
(h, m, l) where h is the head, m the modifier and l
the syntactic label. We further assume that the syn-
tactic component factors over the dependencies in
the path:
</bodyText>
<equation confidence="0.685018">
�ssyn(x, p, r, a, π)= ssyn(x, p, r, a, (h, m, l)) .
(h,m,l)Eπ
(3)
</equation>
<bodyText confidence="0.999823916666667">
This will allow to employ efficient shortest-path
inference, which is the main contribution of this
paper and is described in the next section. Note
that since paths are locally retrieved per role-
argument pair, there is no guarantee that the set
of paths across roles forms a (sub)tree.
As a final note, in this paper we follow Lluis
et al. (2013) and consider a constrained space of
valid argument structures i(x, p): (a) each role is
realized at most once, and (b) each token fills at
most one role. As shown by Lluis et al. (2013),
this can be efficiently solved as a linear assign-
</bodyText>
<page confidence="0.99847">
431
</page>
<figureCaption confidence="0.784277461538461">
Figure 1: Graph representing all possible syntactic paths
from a single predicate to their arguments. We find in this
graph the best SRL using a shortest-path algorithm. Note that
many edges are omitted for clarity reasons. We labeled the
nodes and arcs as follows: p is the predicate and source ver-
tex; u1, ... , un are tokens reachable by an ascending path;
v1, ... , vn are tokens reachable by a ascending path (possi-
bly empty) followed by a descending path (possibly empty);
ai←j is an edge related to an ascending dependency from
node ui to node uj; di→j is a descending dependency from
node vi to node vj; 0i→i is a 0-weighted arc that connects the
ascending portion of the path ending at ui with the descend-
ing portion of the path starting at vi.
</figureCaption>
<bodyText confidence="0.9978865">
ment problem as long as the SRL model factors
over role-argument pairs, as in Eq. (1).
</bodyText>
<sectionHeader confidence="0.865926" genericHeader="method">
3 SRL as a Shortest-path Problem
</sectionHeader>
<bodyText confidence="0.990051586206897">
We now focus on solving the maximization over
syntactic paths in Eq. (2). We will turn it into a
minimization problem which can be solved with a
polynomial-cost algorithm, in our case a shortest-
path method. Assume a fixed argument and role,
and define 0(h,m,l) to be a non-negative penalty for
the syntactic dependency (h, m, l) to appear in the
predicate-argument path. We describe a shortest-
path method that finds the path of arcs with the
smaller penalty:
0(h,m,l) . (4)
We find these paths by appropriately constructing
a weighted graph G = (V, E) that represents the
problem. Later we show how to adapt the arc-
factored model scores to be non-negative penal-
ties, such that the solution to Eq. (4) will be the
negative of the maximizer of Eq. (2).
It remains only to define the graph construc-
tion where paths correspond to arc-factored edges
weighted by 0 penalties. We start by noting that
any path from a predicate p to an argument vi is
formed by a number of ascending syntactic arcs
followed by a number of descending arcs. The as-
cending segment connects p to some ancestor q (q
might be p itself, which implies an empty ascend-
ing segment); the descending segment connects q
with vi (which again might be empty). To com-
pactly represent all these possible paths we define
the graph as follows (see Figure 1):
</bodyText>
<listItem confidence="0.982933565217391">
1. Add node p as the source node of the graph.
2. Add nodes u1, ... , un for every token of the
sentence except p.
3. Link every pair of these nodes ui, uj with a
directed edge ai j weighted by the corre-
sponding ascending arc, namely minl 0(j,i,l).
Also add ascending edges from p to any ui
weighted by minl 0(i,p,l). So far we have
a connected component representing all as-
cending path segments.
4. Add nodes v1, ... , vn for every token of the
sentence except p, and add edges di j be-
tween them weighted by descending arcs,
namely minl 0(i l) . This adds a second
strongly-connected component representing
descending path segments.
5. For each i, add an edge from ui to vi with
weight 0. This ensures that ascending and
descending path segments are connected con-
sistently.
6. Add direct descending edges from p to all the
vi nodes to allow for only-descending paths,
weighted by minl 0(p,i,l).
</listItem>
<bodyText confidence="0.99789">
Dijkstra’s algorithm (Dijkstra, 1959) will find
the optimal path from predicate p to all tokens in
time O(V 2) (see Cormen et al. (2009) for an in-
depth description). Thus, our method runs this
algorithm for each possible role of the predicate,
obtaining the best paths to all arguments at each
run.
</bodyText>
<sectionHeader confidence="0.960398" genericHeader="method">
4 Adapting and Training Model Scores
</sectionHeader>
<bodyText confidence="0.999925333333333">
The shortest-path problem is undefined if a nega-
tive cycle is found in the graph as we may indefi-
nitely decrease the cost of a path by looping over
this cycle. Furthermore, Dijkstra’s algorithm re-
quires all arc scores to be non-negative penalties.
However, the model in Eq. (3) computes plausibil-
ity scores for dependencies, not penalties. And, if
we set this model to be a standard feature-based
linear predictor, it will predict unrestricted real-
valued scores.
One approach to map plausibility scores to
penalties is to assume a log-linear form for our
</bodyText>
<equation confidence="0.702610333333333">
�min
π
(h,m,l)Eπ
</equation>
<page confidence="0.979547">
432
</page>
<bodyText confidence="0.99740312">
model. Let us denote by x¯ the tuple hx, p, r, ai,
which we assume fixed in this section. The log-
linear model predicts:
(5)
where f(¯x, hh, m, li) is a feature vector for an
arc in the path, w are the parameters, and Z(¯x)
is the normalizer. We can turn predictions into
non-negative penalties by setting θ(h,m,l) to be
the negative log-probability of hh, m, li; namely
θ(h,m,l) = −w · f(¯x, hh, m, li) + log Z(¯x). Note
that log Z(¯x) shifts all values to the non-negative
side.
However, log-linear estimation of w is typically
expensive since it requires to repeatedly com-
pute feature expectations. Furthermore, our model
as defined in Eq. (2) combines arc-factored path
scores with path-independent scores, and it is de-
sirable to train these two components jointly. We
opt for a mistake-driven training strategy based
on the Structured Averaged Perceptron (Collins,
2002), which directly employs shortest-path infer-
ence as part of the training process.
To do so we predict plausibility scores for a de-
pendency directly as w · f(¯x, hh, m, li). To map
scores to penalties, we define
</bodyText>
<equation confidence="0.648003">
w · f(¯x, hh, m, li)
</equation>
<bodyText confidence="0.947448">
and we set
</bodyText>
<equation confidence="0.97764">
θ(h,m,l) = −w · f(¯x, hh, m, li) + θ0 .
</equation>
<bodyText confidence="0.981756777777778">
Thus, θ0 has a similar purpose as the log-
normalizer Z(¯x) in a log-linear model, i.e., it
shifts the negated scores to the positive side; but
in our version the normalizer is based on the max
value, not the sum of exponentiated predictions as
in log-linear models. If we set our model function
to be
ssyn(¯x, hh, m, li) = w · f(¯x, hh, m, li) − θ0
then the shortest-path method is exact.
</bodyText>
<sectionHeader confidence="0.998743" genericHeader="evaluation">
5 Experiments
</sectionHeader>
<bodyText confidence="0.999934980769231">
We present experiments using the CoNLL-2009
Shared Task datasets (Hajiˇc et al., 2009), for the
verbal predicates of English. Evaluation is based
on precision, recall and F1 over correct predicate-
argument relations1. Our system uses the fea-
ture set of the state-of-the-art system by Johansson
(2009), but ignoring the features that do not factor
over single arcs in the path.
The focus of these experiments is to see the per-
formance of the shortest-path method with respect
to the syntactic variability. Rather than running
the method with the full set of possible depen-
dency arcs in a sentence, i.e. O(n2), we only con-
sider a fraction of the most likely dependencies.
To do so employ a probabilistic dependency-based
model, following Koo et al. (2007), that computes
the distribution over head-label pairs for a given
modifier, Pr(h, l  |x, m). Specifically, for each
modifier token we only consider the dependencies
or heads whose probability is above a factor γ of
the most likely dependency for the given modi-
fier. Thus, γ = 1 selects only the most likely de-
pendency (similar to a pipeline system, but with-
out enforcing tree constraints), and as γ decreases
more dependencies are considered, to the point
where γ = 0 would select all possible dependen-
cies. Table 2 shows the ratio of dependencies in-
cluded with respect to a pipeline system for the de-
velopment set. As an example, if we set γ = 0.5,
for a given modifier we consider the most likely
dependency and also the dependencies with proba-
bility larger than 1/2 of the probability of the most
likely one. In this case the total number of depen-
dencies is 10.3% larger than only considering the
most likely one.
Table 3 shows results of the method on develop-
ment data, when training and testing with different
γ values. The general trend is that testing with the
most restricted syntactic graph results in the best
performance. However, we observe that as we al-
low for more syntactic variability during training,
the results largely improve. Setting γ = 1 for both
training and testing gives a semantic F1 of 75.9.
This configuration is similar to a pipeline approach
but considering only factored features. If we allow
to train with γ = 0.1 and we test with γ = 1 the
results improve by 1.96 points to a semantic F1
of 77.8 points. When syntactic variability is too
large, e.g., γ = 0.01, no improvements are ob-
served.
Finally, table 4 shows results on the verbal En-
glish WSJ test set using our best configuration
</bodyText>
<footnote confidence="0.983869333333333">
1Unlike in the official CoNLL-2009 evaluation, in this
work we exclude the predicate sense from the features and
the evaluation.
</footnote>
<equation confidence="0.995608666666667">
P((h, m, l) x) _ exp{w • f(x, hh, m, l))}
r
,
Z(¯x)
θ0 = max
(h,m,l)
</equation>
<page confidence="0.997709">
433
</page>
<table confidence="0.9983635">
Threshold -y 1 0.9 0.5 0.1 0.01
Ratio 1 1.014 1.103 1.500 2.843
</table>
<tableCaption confidence="0.947931666666667">
Table 2: Ratio of additional dependencies in the graphs with
respect to a single-tree pipeline model (-y = 1) on develop-
ment data.
</tableCaption>
<table confidence="0.99995484">
Threshold prec (%) rec (%) Fl
training ^y = 1
1 77.91 73.97 75.89
0.9 77.23 74.17 75.67
0.5 73.30 75.03 74.16
0.1 58.22 68.75 63.05
0.01 32.83 53.69 40.74
training ^y = 0.5
1 81.17 73.57 77.18
0.9 80.74 73.78 77.10
0.5 78.40 74.79 76.55
0.1 65.76 71.61 68.56
0.01 42.95 57.68 49.24
training ^y = 0.1
1 84.03 72.52 77.85
0.9 83.76 72.66 77.82
0.5 82.75 73.33 77.75
0.1 77.25 72.20 74.64
0.01 63.90 65.98 64.92
training ^y = 0.01
1 81.62 69.06 74.82
0.9 81.45 69.19 74.82
0.5 80.80 69.80 74.90
0.1 77.92 68.94 73.16
0.01 74.12 65.92 69.78
</table>
<tableCaption confidence="0.95999275">
Table 3: Results of our shortest-path system for different
number of allowed dependencies showing precision, recall
and Fl on development set for the verbal predicates of the
English language.
</tableCaption>
<bodyText confidence="0.999948">
from the development set. We compare to the
state-of-the art system by Zhao et al. (2009) that
was the top-performing system for the English lan-
guage in SRL at the CoNLL-2009 Shared Task.
We also show the results for a shortest-path system
trained and tested with γ = 1. In addition we in-
clude an equivalent pipeline system using all fea-
tures, both factored and non-factored, as defined
in Johansson (2009). We observe that by not be-
ing able to capture non-factored features the final
performance drops by 1.6 F1 points.
</bodyText>
<sectionHeader confidence="0.997701" genericHeader="conclusions">
6 Conclusions
</sectionHeader>
<bodyText confidence="0.9990754">
We have formulated SRL in terms of shortest-
path inference. Our model predicts semantic roles
together with associated syntactic paths, and as-
sumes an arc-factored representation of the path.
This property allows for efficient shortest-path al-
</bodyText>
<table confidence="0.9903426">
System prec(%) rec(%) Fl
Zhao et al. 2009 86.91 81.22 83.97
Non-factored 86.96 75.92 81.06
Factored -y = 1 79.88 76.12 77.96
Factored best 85.26 74.41 79.46
</table>
<tableCaption confidence="0.59426025">
Table 4: Test set results for verbal predicates of the in-domain
English dataset. The configurations are labeled as follows.
Factored -y = 1: our shortest-path system trained and tested
with -y = 1, similar to a pipeline system but without en-
forcing tree constraints and restricted to arc-factored features.
Factored best: our shortest-path system with the best results
from table 3. Non-factored: an equivalent pipeline system
that includes both factored and non-factored features.
</tableCaption>
<bodyText confidence="0.999976066666667">
gorithms that, given a predicate and a role, retrieve
the most likely argument and its path.
In the experimental section we prove the fea-
sibility of the approach. We observe that arc-
factored models are in fact more restricted, with a
drop in accuracy with respect to unrestricted mod-
els. However, we also observe that our method
largely improves the robustness of the arc-factored
method when training with a degree of syntac-
tic variability. Overall, ours is a simple strategy
to bring arc-factored models close to the perfor-
mance of unrestricted models. Future work should
explore further approaches to parse partial syntac-
tic structure specific to some target semantic rela-
tions.
</bodyText>
<sectionHeader confidence="0.997639" genericHeader="acknowledgments">
Acknowledgments
</sectionHeader>
<bodyText confidence="0.999974625">
This work was financed by the European Com-
mission for the XLike project (FP7-288342); and
by the Spanish Government for projects Tacardi
(TIN2012-38523-C02-00) and Skater (TIN2012-
38584-C06-01). For a large part of this work
Xavier Carreras was at the Universitat Polit`ecnica
de Catalunya under a Ram´on y Cajal contract
(RYC-2008-02223).
</bodyText>
<sectionHeader confidence="0.998473" genericHeader="references">
References
</sectionHeader>
<reference confidence="0.977984636363636">
Xavier Carreras and Llu´ıs M`arquez. 2005. Intro-
duction to the CoNLL-2005 shared task: Semantic
role labeling. In Proceedings of the Ninth Confer-
ence on Computational Natural Language Learning
(CoNLL-2005), pages 152–164, Ann Arbor, Michi-
gan, June. Association for Computational Linguis-
tics.
Michael Collins. 2002. Discriminative training meth-
ods for hidden markov models: Theory and experi-
ments with perceptron algorithms. In Proceedings
of the 2002 Conference on Empirical Methods in
</reference>
<page confidence="0.995997">
434
</page>
<reference confidence="0.999623333333333">
Natural Language Processing, pages 1–8. Associ-
ation for Computational Linguistics, July.
Thomas H. Cormen, Charles E. Leiserson, Ronald L.
Rivest, and Clifford Stein. 2009. Introduction to
Algorithms. The MIT Press.
Edsger W. Dijkstra. 1959. A note on two problems
in connexion with graphs. Numerische Mathematik,
1(1):269–271.
Daniel Gildea and Daniel Jurafsky. 2002. Automatic
labeling of semantic roles. Computational Linguis-
tics, 28(3):245–288, September.
Daniel Gildea and Martha Palmer. 2002. The necessity
of parsing for predicate argument recognition. In
Proceedings of 40th Annual Meeting of the Associa-
tion for Computational Linguistics, pages 239–246,
Philadelphia, Pennsylvania, USA, July. Association
for Computational Linguistics.
Jan Hajiˇc, Massimiliano Ciaramita, Richard Johans-
son, Daisuke Kawahara, Maria Ant`onia Mart´ı, Llu´ıs
M`arquez, Adam Meyers, Joakim Nivre, Sebastian
Pad´o, Jan ˇStˇep´anek, Pavel Straˇn´ak, Mihai Surdeanu,
Nianwen Xue, and Yi Zhang. 2009. The CoNLL-
2009 shared task: Syntactic and semantic depen-
dencies in multiple languages. In Proceedings of
the 13th Conference on Computational Natural Lan-
guage Learning (CoNLL-2009), June 4-5, Boulder,
Colorado, USA.
James Henderson, Paola Merlo, Gabriele Musillo, and
Ivan Titov. 2008. A latent variable model of syn-
chronous parsing for syntactic and semantic depen-
dencies. In Proceedings of CoNLL-2008 Shared
Task.
Richard Johansson and Pierre Nugues. 2008.
Dependency-based syntactic–semantic analysis with
propbank and nombank. In CoNLL 2008: Pro-
ceedings of the Twelfth Conference on Computa-
tional Natural Language Learning, pages 183–187,
Manchester, England, August. Coling 2008 Orga-
nizing Committee.
Richard Johansson. 2009. Statistical bistratal depen-
dency parsing. In Proceedings of the 2009 Con-
ference on Empirical Methods in Natural Language
Processing, pages 561–569, Singapore, August. As-
sociation for Computational Linguistics.
Terry Koo, Amir Globerson, Xavier Carreras, and
Michael Collins. 2007. Structured prediction mod-
els via the matrix-tree theorem. In Proceedings
of the 2007 Joint Conference on Empirical Meth-
ods in Natural Language Processing and Com-
putational Natural Language Learning (EMNLP-
CoNLL), pages 141–150, Prague, Czech Republic,
June. Association for Computational Linguistics.
Xavier Llu´ıs, Xavier Carreras, and Llu´ıs M`arquez.
2013. Joint Arc-factored Parsing of Syntactic and
Semantic Dependencies. Transactions of the As-
sociation for Computational Linguistics (TACL),
1(1):219–230, May.
Llu´ıs M`arquez, Xavier Carreras, Kenneth C.
Litkowski, and Suzanne Stevenson. 2008. Semantic
Role Labeling: An Introduction to the Special Issue.
Computational Linguistics, 34(2):145–159, June.
Alessandro Moschitti. 2004. A study on convolution
kernels for shallow statistic parsing. In Proceedings
of the 42nd Meeting of the Association for Compu-
tational Linguistics (ACL’04), Main Volume, pages
335–342, Barcelona, Spain, July.
Mihai Surdeanu, Llu´ıs M`arquez, Xavier Carreras, and
Pere R. Comas. 2007. Combination strategies for
semantic role labeling. Journal of Artificial Intelli-
gence Research.
Nianwen Xue and Martha Palmer. 2004. Calibrat-
ing features for semantic role labeling. In Dekang
Lin and Dekai Wu, editors, Proceedings of EMNLP
2004, pages 88–94, Barcelona, Spain, July. Associ-
ation for Computational Linguistics.
Hai Zhao, Wenliang Chen, Jun’ichi Kazama, Kiyotaka
Uchimoto, and Kentaro Torisawa. 2009. Multi-
lingual dependency learning: Exploiting rich fea-
tures for tagging syntactic and semantic dependen-
cies. In Proceedings of the Thirteenth Confer-
ence on Computational Natural Language Learning
(CoNLL 2009): Shared Task, pages 61–66, Boulder,
Colorado, June. Association for Computational Lin-
guistics.
</reference>
<page confidence="0.999213">
435
</page>
</variant>
</algorithm>
<algorithm name="ParsHed" version="110505">
<variant no="0" confidence="0.878315">
<title confidence="0.999953">A Shortest-path Method for Arc-factored Semantic Role Labeling</title>
<author confidence="0.997236">Xavier Xavier Lluis</author>
<affiliation confidence="0.993899">TALP Research Xerox Research ALT Research Universitat Polit`ecnica Europe Qatar Computing Catalunya xavier.carreras@xrce.xerox.com Institute</affiliation>
<email confidence="0.920012">xlluis@cs.upc.edulmarquez@qf.org.qa</email>
<abstract confidence="0.998434631578948">We introduce a Semantic Role Labeling (SRL) parser that finds semantic roles for a predicate together with the syntactic paths linking predicates and arguments. Our main contribution is to formulate SRL in terms of shortest-path inference, on the assumption that the SRL model is restricted to arc-factored features of the syntactic paths behind semantic roles. Overall, our method for SRL is a novel way to exploit larger variability in the syntactic realizations of predicate-argument relations, moving away from pipeline architectures. Experiments show that our approach improves the robustness of the predictions, producing arc-factored models that perform closely to methods using unrestricted features from the syntax.</abstract>
</variant>
</algorithm>
<algorithm name="ParsCit" version="110505">
<citationList>
<citation valid="true">
<authors>
<author>Xavier Carreras</author>
<author>Llu´ıs M`arquez</author>
</authors>
<title>Introduction to the CoNLL-2005 shared task: Semantic role labeling.</title>
<date>2005</date>
<booktitle>In Proceedings of the Ninth Conference on Computational Natural Language Learning (CoNLL-2005),</booktitle>
<pages>152--164</pages>
<publisher>Association</publisher>
<institution>for Computational Linguistics.</institution>
<location>Ann Arbor, Michigan,</location>
<marker>Carreras, M`arquez, 2005</marker>
<rawString>Xavier Carreras and Llu´ıs M`arquez. 2005. Introduction to the CoNLL-2005 shared task: Semantic role labeling. In Proceedings of the Ninth Conference on Computational Natural Language Learning (CoNLL-2005), pages 152–164, Ann Arbor, Michigan, June. Association for Computational Linguistics.</rawString>
</citation>
<citation valid="true">
<authors>
<author>Michael Collins</author>
</authors>
<title>Discriminative training methods for hidden markov models: Theory and experiments with perceptron algorithms.</title>
<date>2002</date>
<booktitle>In Proceedings of the 2002 Conference on Empirical Methods in Natural Language Processing,</booktitle>
<pages>1--8</pages>
<publisher>Association</publisher>
<institution>for Computational Linguistics,</institution>
<contexts>
<context position="14018" citStr="Collins, 2002" startWordPosition="2379" endWordPosition="2380">tions into non-negative penalties by setting θ(h,m,l) to be the negative log-probability of hh, m, li; namely θ(h,m,l) = −w · f(¯x, hh, m, li) + log Z(¯x). Note that log Z(¯x) shifts all values to the non-negative side. However, log-linear estimation of w is typically expensive since it requires to repeatedly compute feature expectations. Furthermore, our model as defined in Eq. (2) combines arc-factored path scores with path-independent scores, and it is desirable to train these two components jointly. We opt for a mistake-driven training strategy based on the Structured Averaged Perceptron (Collins, 2002), which directly employs shortest-path inference as part of the training process. To do so we predict plausibility scores for a dependency directly as w · f(¯x, hh, m, li). To map scores to penalties, we define w · f(¯x, hh, m, li) and we set θ(h,m,l) = −w · f(¯x, hh, m, li) + θ0 . Thus, θ0 has a similar purpose as the lognormalizer Z(¯x) in a log-linear model, i.e., it shifts the negated scores to the positive side; but in our version the normalizer is based on the max value, not the sum of exponentiated predictions as in log-linear models. If we set our model function to be ssyn(¯x, hh, m, l</context>
</contexts>
<marker>Collins, 2002</marker>
<rawString>Michael Collins. 2002. Discriminative training methods for hidden markov models: Theory and experiments with perceptron algorithms. In Proceedings of the 2002 Conference on Empirical Methods in Natural Language Processing, pages 1–8. Association for Computational Linguistics, July.</rawString>
</citation>
<citation valid="true">
<authors>
<author>Thomas H Cormen</author>
<author>Charles E Leiserson</author>
<author>Ronald L Rivest</author>
<author>Clifford Stein</author>
</authors>
<title>Introduction to Algorithms.</title>
<date>2009</date>
<publisher>The MIT Press.</publisher>
<contexts>
<context position="12368" citStr="Cormen et al. (2009)" startWordPosition="2097" endWordPosition="2100">, ... , vn for every token of the sentence except p, and add edges di j between them weighted by descending arcs, namely minl 0(i l) . This adds a second strongly-connected component representing descending path segments. 5. For each i, add an edge from ui to vi with weight 0. This ensures that ascending and descending path segments are connected consistently. 6. Add direct descending edges from p to all the vi nodes to allow for only-descending paths, weighted by minl 0(p,i,l). Dijkstra’s algorithm (Dijkstra, 1959) will find the optimal path from predicate p to all tokens in time O(V 2) (see Cormen et al. (2009) for an indepth description). Thus, our method runs this algorithm for each possible role of the predicate, obtaining the best paths to all arguments at each run. 4 Adapting and Training Model Scores The shortest-path problem is undefined if a negative cycle is found in the graph as we may indefinitely decrease the cost of a path by looping over this cycle. Furthermore, Dijkstra’s algorithm requires all arc scores to be non-negative penalties. However, the model in Eq. (3) computes plausibility scores for dependencies, not penalties. And, if we set this model to be a standard feature-based lin</context>
</contexts>
<marker>Cormen, Leiserson, Rivest, Stein, 2009</marker>
<rawString>Thomas H. Cormen, Charles E. Leiserson, Ronald L. Rivest, and Clifford Stein. 2009. Introduction to Algorithms. The MIT Press.</rawString>
</citation>
<citation valid="true">
<authors>
<author>Edsger W Dijkstra</author>
</authors>
<title>A note on two problems in connexion with graphs.</title>
<date>1959</date>
<journal>Numerische Mathematik,</journal>
<volume>1</volume>
<issue>1</issue>
<contexts>
<context position="12269" citStr="Dijkstra, 1959" startWordPosition="2079" endWordPosition="2080">So far we have a connected component representing all ascending path segments. 4. Add nodes v1, ... , vn for every token of the sentence except p, and add edges di j between them weighted by descending arcs, namely minl 0(i l) . This adds a second strongly-connected component representing descending path segments. 5. For each i, add an edge from ui to vi with weight 0. This ensures that ascending and descending path segments are connected consistently. 6. Add direct descending edges from p to all the vi nodes to allow for only-descending paths, weighted by minl 0(p,i,l). Dijkstra’s algorithm (Dijkstra, 1959) will find the optimal path from predicate p to all tokens in time O(V 2) (see Cormen et al. (2009) for an indepth description). Thus, our method runs this algorithm for each possible role of the predicate, obtaining the best paths to all arguments at each run. 4 Adapting and Training Model Scores The shortest-path problem is undefined if a negative cycle is found in the graph as we may indefinitely decrease the cost of a path by looping over this cycle. Furthermore, Dijkstra’s algorithm requires all arc scores to be non-negative penalties. However, the model in Eq. (3) computes plausibility s</context>
</contexts>
<marker>Dijkstra, 1959</marker>
<rawString>Edsger W. Dijkstra. 1959. A note on two problems in connexion with graphs. Numerische Mathematik, 1(1):269–271.</rawString>
</citation>
<citation valid="true">
<authors>
<author>Daniel Gildea</author>
<author>Daniel Jurafsky</author>
</authors>
<title>Automatic labeling of semantic roles.</title>
<date>2002</date>
<journal>Computational Linguistics,</journal>
<volume>28</volume>
<issue>3</issue>
<contexts>
<context position="1208" citStr="Gildea and Jurafsky, 2002" startWordPosition="166" endWordPosition="169"> that the SRL model is restricted to arc-factored features of the syntactic paths behind semantic roles. Overall, our method for SRL is a novel way to exploit larger variability in the syntactic realizations of predicate-argument relations, moving away from pipeline architectures. Experiments show that our approach improves the robustness of the predictions, producing arc-factored models that perform closely to methods using unrestricted features from the syntax. 1 Introduction Semantic role labeling (SRL) consists of finding the arguments of a predicate and labeling them with semantic roles (Gildea and Jurafsky, 2002; M`arquez et al., 2008). The arguments fill roles that answer questions of the type “who” did “what” to “whom”, “how”, and “why” for a given sentence predicate. Most approaches to SRL are based on a pipeline strategy, first parsing the sentence to obtain a syntactic tree and then identifying and classifying arguments (Gildea and Jurafsky, 2002; Carreras and M`arquez, 2005). SRL methods critically depend on features of the syntactic structure, and consequently parsing mistakes can harm the quality of semantic role predictions (Gildea and Palmer, 2002). To alleviate this dependence, previous wo</context>
</contexts>
<marker>Gildea, Jurafsky, 2002</marker>
<rawString>Daniel Gildea and Daniel Jurafsky. 2002. Automatic labeling of semantic roles. Computational Linguistics, 28(3):245–288, September.</rawString>
</citation>
<citation valid="true">
<authors>
<author>Daniel Gildea</author>
<author>Martha Palmer</author>
</authors>
<title>The necessity of parsing for predicate argument recognition.</title>
<date>2002</date>
<booktitle>In Proceedings of 40th Annual Meeting of the Association for Computational Linguistics,</booktitle>
<pages>239--246</pages>
<publisher>Association</publisher>
<institution>for Computational Linguistics.</institution>
<location>Philadelphia, Pennsylvania, USA,</location>
<contexts>
<context position="1765" citStr="Gildea and Palmer, 2002" startWordPosition="252" endWordPosition="255">te and labeling them with semantic roles (Gildea and Jurafsky, 2002; M`arquez et al., 2008). The arguments fill roles that answer questions of the type “who” did “what” to “whom”, “how”, and “why” for a given sentence predicate. Most approaches to SRL are based on a pipeline strategy, first parsing the sentence to obtain a syntactic tree and then identifying and classifying arguments (Gildea and Jurafsky, 2002; Carreras and M`arquez, 2005). SRL methods critically depend on features of the syntactic structure, and consequently parsing mistakes can harm the quality of semantic role predictions (Gildea and Palmer, 2002). To alleviate this dependence, previous work has explored k-best parsers (Johansson and Nugues, 2008), combination systems (Surdeanu et al., 2007) or joint syntactic-semantic models (Johansson, 2009; Henderson et al., 2008; Llu´ıs et al., 2013). In this paper we take a different approach. In our scenario SRL is the end goal, and we assume that syntactic parsing is only an intermediate step to extract features to support SRL predictions. In this setting we define a model that, given a predicate, identifies each of the semantic roles together with the syntactic path that links the predicate wit</context>
</contexts>
<marker>Gildea, Palmer, 2002</marker>
<rawString>Daniel Gildea and Martha Palmer. 2002. The necessity of parsing for predicate argument recognition. In Proceedings of 40th Annual Meeting of the Association for Computational Linguistics, pages 239–246, Philadelphia, Pennsylvania, USA, July. Association for Computational Linguistics.</rawString>
</citation>
<citation valid="false">
<authors>
<author>Jan Hajiˇc</author>
<author>Massimiliano Ciaramita</author>
<author>Richard Johansson</author>
<author>Daisuke Kawahara</author>
<author>Maria Ant`onia Mart´ı</author>
<author>Llu´ıs M`arquez</author>
<author>Adam Meyers</author>
<author>Joakim Nivre</author>
<author>Sebastian Pad´o</author>
<author>Jan ˇStˇep´anek</author>
<author>Pavel Straˇn´ak</author>
<author>Mihai Surdeanu</author>
<author>Nianwen Xue</author>
<author>Yi Zhang</author>
</authors>
<title>The CoNLL2009 shared task: Syntactic and semantic dependencies in multiple languages.</title>
<date>2009</date>
<booktitle>In Proceedings of the 13th Conference on Computational Natural Language Learning (CoNLL-2009),</booktitle>
<location>Boulder, Colorado, USA.</location>
<marker>Hajiˇc, Ciaramita, Johansson, Kawahara, Mart´ı, M`arquez, Meyers, Nivre, Pad´o, ˇStˇep´anek, Straˇn´ak, Surdeanu, Xue, Zhang, 2009</marker>
<rawString>Jan Hajiˇc, Massimiliano Ciaramita, Richard Johansson, Daisuke Kawahara, Maria Ant`onia Mart´ı, Llu´ıs M`arquez, Adam Meyers, Joakim Nivre, Sebastian Pad´o, Jan ˇStˇep´anek, Pavel Straˇn´ak, Mihai Surdeanu, Nianwen Xue, and Yi Zhang. 2009. The CoNLL2009 shared task: Syntactic and semantic dependencies in multiple languages. In Proceedings of the 13th Conference on Computational Natural Language Learning (CoNLL-2009), June 4-5, Boulder, Colorado, USA.</rawString>
</citation>
<citation valid="true">
<authors>
<author>James Henderson</author>
<author>Paola Merlo</author>
<author>Gabriele Musillo</author>
<author>Ivan Titov</author>
</authors>
<title>A latent variable model of synchronous parsing for syntactic and semantic dependencies.</title>
<date>2008</date>
<booktitle>In Proceedings of CoNLL-2008 Shared Task.</booktitle>
<contexts>
<context position="1988" citStr="Henderson et al., 2008" startWordPosition="283" endWordPosition="286">te. Most approaches to SRL are based on a pipeline strategy, first parsing the sentence to obtain a syntactic tree and then identifying and classifying arguments (Gildea and Jurafsky, 2002; Carreras and M`arquez, 2005). SRL methods critically depend on features of the syntactic structure, and consequently parsing mistakes can harm the quality of semantic role predictions (Gildea and Palmer, 2002). To alleviate this dependence, previous work has explored k-best parsers (Johansson and Nugues, 2008), combination systems (Surdeanu et al., 2007) or joint syntactic-semantic models (Johansson, 2009; Henderson et al., 2008; Llu´ıs et al., 2013). In this paper we take a different approach. In our scenario SRL is the end goal, and we assume that syntactic parsing is only an intermediate step to extract features to support SRL predictions. In this setting we define a model that, given a predicate, identifies each of the semantic roles together with the syntactic path that links the predicate with the argument. Thus, following previous work (Moschitti, 2004; Johansson, 2009), we take the syntactic path as the main source of syntactic features, but instead of just conditioning on it, we predict it together with the </context>
</contexts>
<marker>Henderson, Merlo, Musillo, Titov, 2008</marker>
<rawString>James Henderson, Paola Merlo, Gabriele Musillo, and Ivan Titov. 2008. A latent variable model of synchronous parsing for syntactic and semantic dependencies. In Proceedings of CoNLL-2008 Shared Task.</rawString>
</citation>
<citation valid="true">
<authors>
<author>Richard Johansson</author>
<author>Pierre Nugues</author>
</authors>
<title>Dependency-based syntactic–semantic analysis with propbank and nombank.</title>
<date>2008</date>
<journal>Organizing Committee.</journal>
<booktitle>In CoNLL 2008: Proceedings of the Twelfth Conference on Computational Natural Language Learning,</booktitle>
<pages>183--187</pages>
<location>Manchester, England,</location>
<contexts>
<context position="1867" citStr="Johansson and Nugues, 2008" startWordPosition="267" endWordPosition="270">guments fill roles that answer questions of the type “who” did “what” to “whom”, “how”, and “why” for a given sentence predicate. Most approaches to SRL are based on a pipeline strategy, first parsing the sentence to obtain a syntactic tree and then identifying and classifying arguments (Gildea and Jurafsky, 2002; Carreras and M`arquez, 2005). SRL methods critically depend on features of the syntactic structure, and consequently parsing mistakes can harm the quality of semantic role predictions (Gildea and Palmer, 2002). To alleviate this dependence, previous work has explored k-best parsers (Johansson and Nugues, 2008), combination systems (Surdeanu et al., 2007) or joint syntactic-semantic models (Johansson, 2009; Henderson et al., 2008; Llu´ıs et al., 2013). In this paper we take a different approach. In our scenario SRL is the end goal, and we assume that syntactic parsing is only an intermediate step to extract features to support SRL predictions. In this setting we define a model that, given a predicate, identifies each of the semantic roles together with the syntactic path that links the predicate with the argument. Thus, following previous work (Moschitti, 2004; Johansson, 2009), we take the syntacti</context>
</contexts>
<marker>Johansson, Nugues, 2008</marker>
<rawString>Richard Johansson and Pierre Nugues. 2008. Dependency-based syntactic–semantic analysis with propbank and nombank. In CoNLL 2008: Proceedings of the Twelfth Conference on Computational Natural Language Learning, pages 183–187, Manchester, England, August. Coling 2008 Organizing Committee.</rawString>
</citation>
<citation valid="true">
<authors>
<author>Richard Johansson</author>
</authors>
<title>Statistical bistratal dependency parsing.</title>
<date>2009</date>
<booktitle>In Proceedings of the 2009 Conference on Empirical Methods in Natural Language Processing,</booktitle>
<pages>561--569</pages>
<institution>Singapore, August. Association for Computational Linguistics.</institution>
<contexts>
<context position="1964" citStr="Johansson, 2009" startWordPosition="281" endWordPosition="282"> sentence predicate. Most approaches to SRL are based on a pipeline strategy, first parsing the sentence to obtain a syntactic tree and then identifying and classifying arguments (Gildea and Jurafsky, 2002; Carreras and M`arquez, 2005). SRL methods critically depend on features of the syntactic structure, and consequently parsing mistakes can harm the quality of semantic role predictions (Gildea and Palmer, 2002). To alleviate this dependence, previous work has explored k-best parsers (Johansson and Nugues, 2008), combination systems (Surdeanu et al., 2007) or joint syntactic-semantic models (Johansson, 2009; Henderson et al., 2008; Llu´ıs et al., 2013). In this paper we take a different approach. In our scenario SRL is the end goal, and we assume that syntactic parsing is only an intermediate step to extract features to support SRL predictions. In this setting we define a model that, given a predicate, identifies each of the semantic roles together with the syntactic path that links the predicate with the argument. Thus, following previous work (Moschitti, 2004; Johansson, 2009), we take the syntactic path as the main source of syntactic features, but instead of just conditioning on it, we predi</context>
<context position="15002" citStr="Johansson (2009)" startWordPosition="2559" endWordPosition="2560">, it shifts the negated scores to the positive side; but in our version the normalizer is based on the max value, not the sum of exponentiated predictions as in log-linear models. If we set our model function to be ssyn(¯x, hh, m, li) = w · f(¯x, hh, m, li) − θ0 then the shortest-path method is exact. 5 Experiments We present experiments using the CoNLL-2009 Shared Task datasets (Hajiˇc et al., 2009), for the verbal predicates of English. Evaluation is based on precision, recall and F1 over correct predicateargument relations1. Our system uses the feature set of the state-of-the-art system by Johansson (2009), but ignoring the features that do not factor over single arcs in the path. The focus of these experiments is to see the performance of the shortest-path method with respect to the syntactic variability. Rather than running the method with the full set of possible dependency arcs in a sentence, i.e. O(n2), we only consider a fraction of the most likely dependencies. To do so employ a probabilistic dependency-based model, following Koo et al. (2007), that computes the distribution over head-label pairs for a given modifier, Pr(h, l |x, m). Specifically, for each modifier token we only consider</context>
<context position="18708" citStr="Johansson (2009)" startWordPosition="3220" endWordPosition="3221">4.12 65.92 69.78 Table 3: Results of our shortest-path system for different number of allowed dependencies showing precision, recall and Fl on development set for the verbal predicates of the English language. from the development set. We compare to the state-of-the art system by Zhao et al. (2009) that was the top-performing system for the English language in SRL at the CoNLL-2009 Shared Task. We also show the results for a shortest-path system trained and tested with γ = 1. In addition we include an equivalent pipeline system using all features, both factored and non-factored, as defined in Johansson (2009). We observe that by not being able to capture non-factored features the final performance drops by 1.6 F1 points. 6 Conclusions We have formulated SRL in terms of shortestpath inference. Our model predicts semantic roles together with associated syntactic paths, and assumes an arc-factored representation of the path. This property allows for efficient shortest-path alSystem prec(%) rec(%) Fl Zhao et al. 2009 86.91 81.22 83.97 Non-factored 86.96 75.92 81.06 Factored -y = 1 79.88 76.12 77.96 Factored best 85.26 74.41 79.46 Table 4: Test set results for verbal predicates of the in-domain English</context>
</contexts>
<marker>Johansson, 2009</marker>
<rawString>Richard Johansson. 2009. Statistical bistratal dependency parsing. In Proceedings of the 2009 Conference on Empirical Methods in Natural Language Processing, pages 561–569, Singapore, August. Association for Computational Linguistics.</rawString>
</citation>
<citation valid="true">
<authors>
<author>Terry Koo</author>
<author>Amir Globerson</author>
<author>Xavier Carreras</author>
<author>Michael Collins</author>
</authors>
<title>Structured prediction models via the matrix-tree theorem.</title>
<date>2007</date>
<booktitle>In Proceedings of the 2007 Joint Conference on Empirical Methods in Natural Language Processing and Computational Natural Language Learning (EMNLPCoNLL),</booktitle>
<pages>141--150</pages>
<publisher>Association</publisher>
<institution>for Computational Linguistics.</institution>
<location>Prague, Czech Republic,</location>
<contexts>
<context position="15455" citStr="Koo et al. (2007)" startWordPosition="2635" endWordPosition="2638">n is based on precision, recall and F1 over correct predicateargument relations1. Our system uses the feature set of the state-of-the-art system by Johansson (2009), but ignoring the features that do not factor over single arcs in the path. The focus of these experiments is to see the performance of the shortest-path method with respect to the syntactic variability. Rather than running the method with the full set of possible dependency arcs in a sentence, i.e. O(n2), we only consider a fraction of the most likely dependencies. To do so employ a probabilistic dependency-based model, following Koo et al. (2007), that computes the distribution over head-label pairs for a given modifier, Pr(h, l |x, m). Specifically, for each modifier token we only consider the dependencies or heads whose probability is above a factor γ of the most likely dependency for the given modifier. Thus, γ = 1 selects only the most likely dependency (similar to a pipeline system, but without enforcing tree constraints), and as γ decreases more dependencies are considered, to the point where γ = 0 would select all possible dependencies. Table 2 shows the ratio of dependencies included with respect to a pipeline system for the d</context>
</contexts>
<marker>Koo, Globerson, Carreras, Collins, 2007</marker>
<rawString>Terry Koo, Amir Globerson, Xavier Carreras, and Michael Collins. 2007. Structured prediction models via the matrix-tree theorem. In Proceedings of the 2007 Joint Conference on Empirical Methods in Natural Language Processing and Computational Natural Language Learning (EMNLPCoNLL), pages 141–150, Prague, Czech Republic, June. Association for Computational Linguistics.</rawString>
</citation>
<citation valid="true">
<authors>
<author>Xavier Llu´ıs</author>
<author>Xavier Carreras</author>
<author>Llu´ıs M`arquez</author>
</authors>
<title>Joint Arc-factored Parsing of Syntactic and Semantic Dependencies.</title>
<date>2013</date>
<journal>Transactions of the Association for Computational Linguistics (TACL),</journal>
<volume>1</volume>
<issue>1</issue>
<marker>Llu´ıs, Carreras, M`arquez, 2013</marker>
<rawString>Xavier Llu´ıs, Xavier Carreras, and Llu´ıs M`arquez. 2013. Joint Arc-factored Parsing of Syntactic and Semantic Dependencies. Transactions of the Association for Computational Linguistics (TACL), 1(1):219–230, May.</rawString>
</citation>
<citation valid="true">
<authors>
<author>Llu´ıs M`arquez</author>
<author>Xavier Carreras</author>
<author>Kenneth C Litkowski</author>
<author>Suzanne Stevenson</author>
</authors>
<title>Semantic Role Labeling: An Introduction to the Special Issue.</title>
<date>2008</date>
<journal>Computational Linguistics,</journal>
<volume>34</volume>
<issue>2</issue>
<marker>M`arquez, Carreras, Litkowski, Stevenson, 2008</marker>
<rawString>Llu´ıs M`arquez, Xavier Carreras, Kenneth C. Litkowski, and Suzanne Stevenson. 2008. Semantic Role Labeling: An Introduction to the Special Issue. Computational Linguistics, 34(2):145–159, June.</rawString>
</citation>
<citation valid="true">
<authors>
<author>Alessandro Moschitti</author>
</authors>
<title>A study on convolution kernels for shallow statistic parsing.</title>
<date>2004</date>
<booktitle>In Proceedings of the 42nd Meeting of the Association for Computational Linguistics (ACL’04), Main Volume,</booktitle>
<pages>335--342</pages>
<location>Barcelona, Spain,</location>
<contexts>
<context position="2427" citStr="Moschitti, 2004" startWordPosition="362" endWordPosition="363">as explored k-best parsers (Johansson and Nugues, 2008), combination systems (Surdeanu et al., 2007) or joint syntactic-semantic models (Johansson, 2009; Henderson et al., 2008; Llu´ıs et al., 2013). In this paper we take a different approach. In our scenario SRL is the end goal, and we assume that syntactic parsing is only an intermediate step to extract features to support SRL predictions. In this setting we define a model that, given a predicate, identifies each of the semantic roles together with the syntactic path that links the predicate with the argument. Thus, following previous work (Moschitti, 2004; Johansson, 2009), we take the syntactic path as the main source of syntactic features, but instead of just conditioning on it, we predict it together with the semantic role. The main contribution of this paper is a formulation of SRL parsing in terms of efficient shortest-path inference, under the assumption that the SRL model is restricted to arc-factored features of the syntactic path linking the argument with the predicate. Our assumption —that features of an SRL model should factor over dependency arcs— is supported by some empirical frequencies. Table 1 shows the most frequent path patt</context>
</contexts>
<marker>Moschitti, 2004</marker>
<rawString>Alessandro Moschitti. 2004. A study on convolution kernels for shallow statistic parsing. In Proceedings of the 42nd Meeting of the Association for Computational Linguistics (ACL’04), Main Volume, pages 335–342, Barcelona, Spain, July.</rawString>
</citation>
<citation valid="true">
<authors>
<author>Mihai Surdeanu</author>
<author>Llu´ıs M`arquez</author>
<author>Xavier Carreras</author>
<author>Pere R Comas</author>
</authors>
<title>Combination strategies for semantic role labeling.</title>
<date>2007</date>
<journal>Journal of Artificial Intelligence Research.</journal>
<marker>Surdeanu, M`arquez, Carreras, Comas, 2007</marker>
<rawString>Mihai Surdeanu, Llu´ıs M`arquez, Xavier Carreras, and Pere R. Comas. 2007. Combination strategies for semantic role labeling. Journal of Artificial Intelligence Research.</rawString>
</citation>
<citation valid="true">
<authors>
<author>Nianwen Xue</author>
<author>Martha Palmer</author>
</authors>
<title>Calibrating features for semantic role labeling.</title>
<date>2004</date>
<booktitle>In Dekang Lin and Dekai Wu, editors, Proceedings of EMNLP 2004,</booktitle>
<pages>88--94</pages>
<publisher>Association for Computational Linguistics.</publisher>
<location>Barcelona, Spain,</location>
<contexts>
<context position="3469" citStr="Xue and Palmer (2004)" startWordPosition="539" endWordPosition="542"> predicate. Our assumption —that features of an SRL model should factor over dependency arcs— is supported by some empirical frequencies. Table 1 shows the most frequent path patterns on CoNLL2009 (Hajiˇc et al., 2009) data for several languages, where a path pattern is a sequence of ascending arcs from the predicate to some ancestor, followed by descending arcs to the argument. For English the distribution of path patterns is rather simple: the majority of paths consists of a number of ascending arcs followed by zero or one descending arc. Thus a common strategy in SRL systems, formulated by Xue and Palmer (2004), is to look for arguments in the ancestors of the predicate and their direct descendants. However, in Czech and Japanese data we observe a large portion of paths with two or more descending arcs, which makes it difficult to characterize the syntactic scope in which arguments are found. Also, in the datasets for German, Czech and Chinese the three most frequent patterns cover over the 90% of all arguments. In contrast, Japanese exhibits much more variability and a long tail of infrequent types 430 Proceedings of the 2014 Conference on Empirical Methods in Natural Language Processing (EMNLP), p</context>
</contexts>
<marker>Xue, Palmer, 2004</marker>
<rawString>Nianwen Xue and Martha Palmer. 2004. Calibrating features for semantic role labeling. In Dekang Lin and Dekai Wu, editors, Proceedings of EMNLP 2004, pages 88–94, Barcelona, Spain, July. Association for Computational Linguistics.</rawString>
</citation>
<citation valid="true">
<authors>
<author>Hai Zhao</author>
<author>Wenliang Chen</author>
<author>Jun’ichi Kazama</author>
<author>Kiyotaka Uchimoto</author>
<author>Kentaro Torisawa</author>
</authors>
<title>Multilingual dependency learning: Exploiting rich features for tagging syntactic and semantic dependencies.</title>
<date>2009</date>
<booktitle>In Proceedings of the Thirteenth Conference on Computational Natural Language Learning (CoNLL 2009): Shared Task,</booktitle>
<pages>61--66</pages>
<publisher>Association</publisher>
<institution>for Computational Linguistics.</institution>
<location>Boulder, Colorado,</location>
<contexts>
<context position="18391" citStr="Zhao et al. (2009)" startWordPosition="3163" endWordPosition="3166">4 73.78 77.10 0.5 78.40 74.79 76.55 0.1 65.76 71.61 68.56 0.01 42.95 57.68 49.24 training ^y = 0.1 1 84.03 72.52 77.85 0.9 83.76 72.66 77.82 0.5 82.75 73.33 77.75 0.1 77.25 72.20 74.64 0.01 63.90 65.98 64.92 training ^y = 0.01 1 81.62 69.06 74.82 0.9 81.45 69.19 74.82 0.5 80.80 69.80 74.90 0.1 77.92 68.94 73.16 0.01 74.12 65.92 69.78 Table 3: Results of our shortest-path system for different number of allowed dependencies showing precision, recall and Fl on development set for the verbal predicates of the English language. from the development set. We compare to the state-of-the art system by Zhao et al. (2009) that was the top-performing system for the English language in SRL at the CoNLL-2009 Shared Task. We also show the results for a shortest-path system trained and tested with γ = 1. In addition we include an equivalent pipeline system using all features, both factored and non-factored, as defined in Johansson (2009). We observe that by not being able to capture non-factored features the final performance drops by 1.6 F1 points. 6 Conclusions We have formulated SRL in terms of shortestpath inference. Our model predicts semantic roles together with associated syntactic paths, and assumes an arc-</context>
</contexts>
<marker>Zhao, Chen, Kazama, Uchimoto, Torisawa, 2009</marker>
<rawString>Hai Zhao, Wenliang Chen, Jun’ichi Kazama, Kiyotaka Uchimoto, and Kentaro Torisawa. 2009. Multilingual dependency learning: Exploiting rich features for tagging syntactic and semantic dependencies. In Proceedings of the Thirteenth Conference on Computational Natural Language Learning (CoNLL 2009): Shared Task, pages 61–66, Boulder, Colorado, June. Association for Computational Linguistics.</rawString>
</citation>
</citationList>
</algorithm>
</algorithms>
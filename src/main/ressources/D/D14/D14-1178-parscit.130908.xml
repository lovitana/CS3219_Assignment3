<?xml version="1.0" encoding="UTF-8"?>
<algorithms version="110505">
<algorithm name="SectLabel" version="110505">
<variant no="0" confidence="0.000000">
<title confidence="0.989613">
Random Manhattan Integer Indexing:
Incremental L1 Normed Vector Space Construction
</title>
<author confidence="0.616941">
Behrang Q. Zadeh† Siegfried Handschuh†‡
</author>
<affiliation confidence="0.627827">
† Insight Centre ‡ Dept. of Computer Science and Mathematics
National University of Ireland, Galway
</affiliation>
<address confidence="0.672125">
Galway, Ireland
</address>
<email confidence="0.830314">
behrang.qasemizadeh@insight-centre.org
</email>
<sectionHeader confidence="0.991457" genericHeader="abstract">
Abstract
</sectionHeader>
<bodyText confidence="0.999960448275862">
Vector space models (VSMs) are math-
ematically well-defined frameworks that
have been widely used in the distributional
approaches to semantics. In VSMs, high-
dimensional vectors represent linguistic
entities. In an application, the similar-
ity of vectors—and thus the entities that
they represent—is computed by a distance
formula. The high dimensionality of vec-
tors, however, is a barrier to the perfor-
mance of methods that employ VSMs.
Consequently, a dimensionality reduction
technique is employed to alleviate this
problem. This paper introduces a novel
technique called Random Manhattan In-
dexing (RMI) for the construction of i1
normed VSMs at reduced dimensionality.
RMI combines the construction of a VSM
and dimension reduction into an incre-
mental and thus scalable two-step proce-
dure. In order to attain its goal, RMI em-
ploys the sparse Cauchy random projec-
tions. We further introduce Random Man-
hattan Integer Indexing (RMII): a compu-
tationally enhanced version of RMI. As
shown in the reported experiments, RMI
and RMII can be used reliably to estimate
the i1 distances between vectors in a vec-
tor space of low dimensionality.
</bodyText>
<sectionHeader confidence="0.999164" genericHeader="introduction">
1 Introduction
</sectionHeader>
<bodyText confidence="0.999337125">
Distributional semantics embraces a set of meth-
ods that decipher the meaning of linguistic en-
tities using their usages in large corpora (Lenci,
2008). In these methods, the distributional proper-
ties of linguistic entities in various contexts, which
are collected from their observations in corpora,
are compared to quantify their meaning. Vector
spaces are intuitive, mathematically well-defined
</bodyText>
<affiliation confidence="0.602632">
University of Passau
Bavaria, Germany
</affiliation>
<bodyText confidence="0.988844307692308">
siegfried.handschuh@uni-passau.de
frameworks to represent and process such infor-
mation.1 In a vector space model (VSM), linguis-
tic entities are represented by vectors and a dis-
tance formula is employed to measure their distri-
butional similarities (Turney and Pantel, 2010).
In a VSM, each element si of the standard basis
of the vector space (informally, each dimension of
the VSM) represents a context element. Given n
context elements, an entity whose meaning is be-
ing analyzed is expressed by a vector v as a linear
combination of si and scalars αi E R such that
V = α1s1 + · · · +αnsn. The value of αi is derived
from the frequency of the occurrences of the entity
that v represents in/with the context element that
si represents. As a result, the values assigned to
the coordinates of a vector (i.e. αi) exhibit the cor-
relation of entities and context elements in an n-
dimensional real vector space Rn. Each vector can
be written as a 1xn row matrix, e.g. (α1, · · · , αn).
Therefore, a group of m vectors in a vector space
is often represented by a matrix Mm×n.
Latent semantic analysis (LSA) is a famil-
iar technique that employs a word-by-document
VSM (Deerwester et al., 1990).2 In this word-
by-document model, the meaning of words (i.e.
the linguistic entities) is described by their occur-
rences in documents (i.e. the context elements).
Given m words and n distinct documents, each
word is represented by an n-dimensional vector
vz = (αi1, · · · , αin), where αij is a numeric value
that associates the word vz represents to the doc-
ument dj, for 1 &lt; j &lt; n. For instance, the
value of αij may correspond to the frequency of
the word in the document. It is hypothesized that
the relevance of words can be assessed by count-
ing the documents in which they co-occur. There-
fore, words with similar vectors are assumed to
have the same meaning (Figure 1).
</bodyText>
<footnote confidence="0.836564666666667">
1Amongst other representation frameworks.
2See Martin and Berry (2007) for an overview of the
mathematical foundation of LSA.
</footnote>
<page confidence="0.695706">
1713
</page>
<note confidence="0.981009">
Proceedings of the 2014 Conference on Empirical Methods in Natural Language Processing (EMNLP), pages 1713–1723,
October 25-29, 2014, Doha, Qatar. c�2014 Association for Computational Linguistics
</note>
<figure confidence="0.524042727272727">
~s3 ↔ d3
α23
α13
�v1
�v2
α22
α11
α12
α21
~s1 ↔ d1
~s2 ↔ d2
</figure>
<bodyText confidence="0.999849111111111">
In Figure 1, the dashed line shows the Euclidean
distance between the two vectors. In `2 normed
vector spaces, various similarity metrics are de-
fined using different normalization of the Eu-
clidean distance between vectors, e.g. the cosine
similarity.
The similarity between vectors, however, can
also be computed in `1 normed spaces.3 The `1
norm for v~ is given by
</bodyText>
<figureCaption confidence="0.993483">
Figure 1: Illustration of a word-by-document
</figureCaption>
<bodyText confidence="0.950790678571429">
model consisting of 2 words and 3 documents.
The words are represented in a 3-dimensional vec-
tor space, in which each ~si (each dimension) rep-
resents each of the 3 documents in the model.
~v1 = (α11, α12, α13) and ~v2 = (α21, α22, α23)
represent the two words in the model. The dashed
line shows the Euclidean distance between the two
vectors that represent words, while the sum of
dash-dotted lines is the Manhattan distance be-
tween them.
In order to assess the similarity between vectors,
a vector space V is endowed with a norm struc-
ture. A norm k.k is a function that maps vectors
from V to the set of non-negative real numbers,
i.e. V 7→ [0, ∞). The pair of (V, k.k) is then called
a normed space. In a normed space, the similar-
ity between vectors is assessed by their distances.
The distance between vectors is defined by a func-
tion that satisfies certain axioms and assigns a real
value to each pair of vectors, i.e.
dist : V × V 7→ R, d(~v,~t) = k~v − ~uk. (1)
The smaller the distance between two vectors, the
more similar they are.
Euclidean space is the most familiar example
of a normed space. It is a vector space that is en-
dowed by the `2 norm. In Euclidean space, the `2
norm—which is also called the Euclidean norm—
of a vector v~ = (v1, · · · , vn) is defined as
</bodyText>
<equation confidence="0.998174">
k~vk2 = � � � � n v2i . (2)
i=1
</equation>
<bodyText confidence="0.990930666666667">
Using the definition of distance given in Equa-
tion 1 and the `2 norm, the Euclidean distance is
measured as
</bodyText>
<equation confidence="0.999505">
dist2(~v,~u) = k~v − ~uk2 = d n (vi − ui)2. (3)
i=1
n
k~vk1 = |vi|, (4)
i=1
</equation>
<bodyText confidence="0.9999298">
where |. |signifies the modulus. The distance in an
`1 normed vector space is often called the Man-
hattan or the city block distance. According to the
definition given in Equation 1, the Manhattan dis-
tance between two vectors v~ and u~ is given by
</bodyText>
<equation confidence="0.993570333333333">
n
dist1(~v, ~u) = k~v − ~uk1 = |vi − uj|. (5)
k=1
</equation>
<bodyText confidence="0.997340730769231">
In Figure 1, the collection of the dash-dotted lines
is the `1 distance between the two vectors. Similar
to the `2 spaces, various normalizations of the `1
distance4 define a family of `1 normed similarity
metrics.
As the number of text units that are being mod-
elled in a VSM increases, the number of context
elements that are required to be utilized to capture
their meaning escalates. This phenomenon is ex-
plained using power-law distributions of text units
in context elements (e.g. the familiar Zipfian dis-
tribution of words). As a result, extremely high-
dimensional vectors, which are also sparse—i.e.
most of the elements of the vectors are zero—
represent text units. The high dimensionality of
the vectors results in setbacks, which are colloqui-
ally known as the curse of dimensionality. For in-
stance, in a word-by-document model that consists
of a large number of documents, a word appears
only in a few documents, and the rest of the doc-
uments are irrelevant to the meaning of the word.
Few common documents between words results in
sparsity of the vectors; and the presence of irrele-
vant documents introduces noise.
Dimension reduction, which usually follows the
construction of a VSM, alleviates the problems
</bodyText>
<footnote confidence="0.989756">
3The definition of the norm is generalized to fv spaces
with 1 1 1 1v = ( Ei |vi|v)1&amp;quot;v, which is beyond the scope of
this paper.
4As long as the axioms in the distance definition hold.
</footnote>
<page confidence="0.995708">
1714
</page>
<bodyText confidence="0.999958145833333">
listed above by reducing the number of context el-
ements that are employed for the construction of
the VSM. In its simple form, dimensionality re-
duction can be performed using a selection pro-
cess: choose a subset of contexts and eliminate
the rest using a heuristic. Alternatively, transfor-
mation methods can be employed. A transforma-
tion method maps a vector space Un onto a Um of
lowered dimension, i.e. T : Un H Um, m « n.
The vector space at reduced dimension, i.e. Um,
is often the best approximation of the original Un
in a sense. LSA employs a dimension reduction
technique called truncated singular value decom-
position (SVD). In a standard truncated SVD, the
transformation guarantees the least distortion in
the i2 distances.5
Besides the problem of high computational
complexity of SVD computation,6 which can be
addressed by incremental techniques (see e.g.
Brand (2006)), matrix factorization methods such
as truncated SVD are data-sensitive: if the struc-
ture of the data being analyzed changes, i.e. when
either the linguistic entities or context elements
are updated, e.g. some are removed or new ones
are added, the transformation should be recom-
puted and reapplied to the whole VSM to reflect
the updates. In addition, a VSM at the original
high dimension must be first constructed. Follow-
ing the construction of the VSM, the dimension
of the VSM is reduced in an independent process.
Therefore, the VSM at reduced dimension is avail-
able for processing only after the whole sequence
of these processes. Construction of the VSM at
its original dimension is computationally expen-
sive and a delay in access to the VSM at reduced
dimension is not desirable. Hence, the application
of truncated SVD is not suitable in several appli-
cations, particularly when dealing with frequently
updated big text–data such as applications in the
web context.
Random indexing (RI) is an alternative method
that solves the problems stated above by combin-
ing the construction of a vector space and the di-
mensionality reduction process. RI, which is in-
troduced in Kanerva et al. (2000), constructs a
VSM directly at reduced dimension. Unlike meth-
ods that first construct a VSM at its original high
dimension and conduct a dimensionality reduction
</bodyText>
<footnote confidence="0.9985255">
5Please note that there are matrix factorization techniques
that guarantee the least distortion in the f, distances, see e.g.
Kwak (2008).
6Matrix factorization techniques, in general.
</footnote>
<bodyText confidence="0.999847377777778">
afterwards, the RI method avoids the construction
of the original high-dimensional VSM. Instead, it
merges the vector space construction and the di-
mensionality reduction process. RI, thus, signifi-
cantly enhances the computational complexity of
deriving a VSM from text. However, the appli-
cation of the RI technique (likewise the standard
truncated SVD in LSA) is limited to i2 normed
spaces, i.e. when similarities are assessed using a
measure based on the i2 distance. It can be verified
that using RI causes large distortions in the i1 dis-
tances between vectors (Brinkman and Charikar,
2005). Hence, if the similarities are computed us-
ing the i1 distance, then the RI technique is not
suitable for the VSM construction.
Depending on the distribution of vectors in
a VSM, the performance of similarity measures
based on the i1 and the i2 norms varies from one
task to another. For instance, it is known that
the i1 distance is more robust to the presence of
outliers and non-Gaussian noise than the i2 dis-
tance (e.g. see the problem description in Ke and
Kanade (2003)). Hence, the i1 distance can be
more reliable than the i2 distance in certain appli-
cations. For instance, Weeds et al. (2005) suggest
that the i1 distance outperforms other similarity
metrics in a term classification task. In another
experiment, Lee (1999) observed that the i1 dis-
tance gives more desirable results than the Cosine
and the i2 measures.
In this paper, we introduce a novel method
called Random Manhattan Indexing (RMI). RMI
constructs a vector space model directly at re-
duced dimension while it preserves the pairwise
i1 distances between vectors in the original high-
dimensional VSM. We then introduced a compu-
tationally enhanced version of RMI called Ran-
dom Manhattan Integer Indexing (RMII). RMI
and RMII, similar to RI, merge the construction
of a VSM and dimension reduction into an incre-
mental and thus efficient and scalable process.
In Section 2, we explain and evaluate the RMI
method. In Section 3, the RMII method is ex-
plained. We compare the proposed method with
RI in Section 4. We conclude in Section 5.
</bodyText>
<sectionHeader confidence="0.940744" genericHeader="method">
2 Random Manhattan Indexing
</sectionHeader>
<bodyText confidence="0.99993725">
We propose the RMI method: a novel technique
that adapts an incremental procedure for the con-
struction of i1 normed vector spaces at a reduced
dimension. The RMI method employs a two-step
</bodyText>
<page confidence="0.941498">
1715
</page>
<bodyText confidence="0.986023428571429">
procedure: (a) the creation of index vectors and (b)
the construction of context vectors .
In the first step, each context element is as-
signed exactly to one index vector ~ri. Index vec-
tors are high-dimensional and generated randomly
such that entries rj of index vectors have the fol-
lowing distribution:
</bodyText>
<equation confidence="0.99775225">
ri = { −1 , (6)
U1 with probability 2s
0 with probability 1 − s
U2 1with probability 2
</equation>
<bodyText confidence="0.999737833333333">
where U1 and U2 are independent uniform ran-
dom variables in (0, 1). In the second step, each
target linguistic entity that is being analyzed in
the model is assigned to a context vector ~vc in
which all the elements are initially set to 0. For
each encountered occurrence of a linguistic entity
and a context element—e.g. through a sequential
scan of an input text collection—~vc that represents
the linguistic entity is accumulated by the index
vector ~ri that represents the context element, i.e.
~vc = ~vc + ~ri. This process results in a VSM
of a reduced dimensionality that can be used to
estimate the `1 distances between linguistic enti-
ties. In the constructed VSM by RMI, the `1 dis-
tance between vectors is given by the sample me-
dian (Indyk, 2000). For given vectors v~ and ~u, the
approximate `1 distance between vectors is esti-
mated by
</bodyText>
<equation confidence="0.996375">
ˆL1(~u,~v) = median{|vi − ui|, i = 1, · · · , m}, (7)
</equation>
<bodyText confidence="0.992356880597015">
where m is the dimension of the VSM constructed
by RMI, and |. |denotes the modulus.
RMI is based on the random projection (RP)
technique for dimensionality reduction. In RP, a
high-dimensional vector space is mapped onto a
random subspace of lowered dimension expecting
that—with a high probability—relative distances
between vectors are approximately preserved. Us-
ing the matrix notation, this projection is given by
= MpxnRnxm, m « p, n, (8)
where R is often called the random projection ma-
trix, and M and M&apos; denote p vectors in the orig-
inal n-dimensional and reduced m-dimensional
vector spaces, respectively.
In RMI, the stated mapping in Equation 8
is given by Cauchy random projections. Indyk
(2000) suggests that vectors in a high-dimensional
space Rn can be mapped onto a vector space of
lowered dimension Rm while the relative pairwise
`1 distances between vectors are preserved with a
high probability. In Indyk (2000, Theorem 3) and
Indyk (2006, Theorem 5), it is shown that for an
m &gt; m0 = log(1/δ)O(1/0, where δ &gt; 0 and
E &lt; 1/2, there exists a mapping from Rn onto
Rm that guarantees the `1 distances between any
pair of vectors u~ and v~ in Rn after the mapping
does not increase by a factor more than 1 + c with
constant probability δ, and it does not decrease by
more than 1 − c with probability 1 − δ.
In Indyk (2000), this projection is proved to
be obtained using a random projection matrix R
that has Cauchy distribution—i.e. for rij in R,
rij ti C(0,1). Since R has a Cauchy distribu-
tion, for every two vectors u~ and v~ in the high-
dimensional space Rn, the projected differences
x = u~ˆ− v~ˆ also have Cauchy distribution, with
the scale parameter being the `1 distances, i.e.
x ti C(0, Eni=1 |ui − vi|). As a result, in Cauchy
random projections, estimating the `1 distances
boils down to the estimation of the Cauchy scale
parameter from independent and identically dis-
tributed (i.i.d.) samples x. Because the expectation
value of x is infinite,7 the sample mean cannot be
employed to estimate the Cauchy scale parameter.
Instead, using the 1-stability of Cauchy distribu-
tion, Indyk (2000) proves that the median can be
employed to estimate the Cauchy scale parame-
ter, and thus the `1 distances at the projected space
Rm.
Subsequent studies simplified the method pro-
posed by Indyk (2000). Li (2007) shows that R
with Cauchy distribution can be substituted by a
sparse R that has a mixture of symmetric 1-Pareto
distribution. A 1-Pareto distribution can be sam-
pled by 1/U, where U is an independent uniform
random variable in (0, 1). This results in a ran-
dom matrix R that has the same distribution as
described by Equation 6.
The RMI’s two-step procedure is explained us-
ing the basic properties of matrix arithmetic and
the descriptions given above. Given the projection
in Equation 8, the first step of RMI refers to the
construction of R: index vectors are the row vec-
tors of R. The second step of the process refers
to the construction of M&apos;: context vectors are the
row vectors of M&apos;. Using the distributive prop-
erty of multiplication over addition in matrices,8
</bodyText>
<footnote confidence="0.8261485">
7That is E(x) = ∞, since x has a Cauchy distribution.
8That is, (A + B)C = AC + BC.
</footnote>
<figure confidence="0.273902">
M&apos;
pxm
</figure>
<page confidence="0.919957">
1716
</page>
<bodyText confidence="0.99980725">
it can be verified that the explicit construction of
M and its multiplication to R can be substituted
by a number of summation operations. M can be
represented by the sum of unit vectors in which a
unit vector corresponds to the co-occurrence of a
linguistic entity and a context element. The result
of the multiplication of each unit vector and R is
the row vector that represents the context element
in R—i.e. the index vector. Therefore, M&apos; can be
computed by the accumulation of the row vectors
of R that represent encountered context elements,
as stated in the second step of the RMI procedure.
</bodyText>
<subsectionHeader confidence="0.926536">
2.1 Alternative Distance Estimators
</subsectionHeader>
<bodyText confidence="0.999934">
As stated above, Indyk (2000) suggests using the
sample median for the estimation of the `1 dis-
tances. However, Li (2008) argues that sam-
ple median estimator can be biased and inaccu-
rate, specifically if m—i.e. the targeted (reduced)
dimensionality—is small. Hence, Li (2008) sug-
gests using the geometric mean estimator instead
of the median sample:9
</bodyText>
<equation confidence="0.99744">
ˆL1(~u,~v) = (
</equation>
<bodyText confidence="0.998795333333333">
We suggest computing the ˆL1(~u,~v) in Equation
9 using arithmetic mean of logarithm-transformed
values of |ui − vi|. Therefore, using the logarith-
mic identities, the multiplications and the power in
Equation 9 are, respectively, transformed to a sum
and a multiplication:
</bodyText>
<equation confidence="0.585179">
�ln(|ui − vi|) . (10)
</equation>
<bodyText confidence="0.999875666666667">
Equation 10 for computing ˆL1 is more plausible
for computational implementation than Equation
9 (e.g. the overflow is less likely to happen dur-
ing the process). Moreover, calculating the median
involves sorting an array of real numbers. Thus,
computation of the geometric mean in logarithmic
scales can be faster than computation of the me-
dian sample, especially when the value of m is
large.
</bodyText>
<subsectionHeader confidence="0.969526">
2.2 RMI’s Parameters
</subsectionHeader>
<bodyText confidence="0.999969666666667">
In order to employ the RMI method for the con-
struction of a VSM at reduced dimension and the
estimation of the `1 distance between vectors, two
</bodyText>
<note confidence="0.46743">
9See also Li et al. (2007, Lemma 5–9).
</note>
<bodyText confidence="0.999665066666667">
model parameters should be decided: (a) the tar-
geted (reduced) dimensionality of the VSM, which
is indicated by m in Equation 8 and (b) the num-
ber of non-zero elements in index vectors, which
is determined by s in Equation 6. In contrast to the
classic one-dimension-per-context-element meth-
ods of VSM construction,10 the value of m in RPs
and thus in RMI is chosen independently of the
number of context elements in the model (n in
Equation 8).
In RMI, m determines the probability and the
maximum expected amount of distortions c in the
pairwise distance between vectors. Based on the
proposed refinements of Indyk (2000, Theorem 3)
by Li et al. (2007), it is verified that the pairwise
`1 distance between any p vectors is approximated
within a factor 1 f c, if m = O(logp/E2), with a
constant probability. Therefore, the value of c in
RMI is subject to the number of vectors p in the
model. For a fixed p, a larger m yields to lower
bounds on the distortion with a higher probabil-
ity. Because a small m is desirable from the com-
putational complexity outlook, the choice of m is
often a trade-off between accuracy and efficiency.
According to our experiment, m &gt; 400 is suitable
for most applications.
The number of non-zero elements in index vec-
tors, however, is decided by the number of context
elements n and the sparseness of the VSM β at
its original dimension. Li (2007) suggests 1
</bodyText>
<equation confidence="0.536452">
O(√βn)
</equation>
<bodyText confidence="0.9999827">
as the value of s in Equation 6. VSMs employed
in distributional semantics are highly sparse. The
sparsity of a VSM in its original dimension β is
often considered to be around 0.0001–0.01. As
the original dimension of VSM n is very large—
otherwise there would be no need for dimension-
ality reduction—the index vectors are often very
sparse. Similar to m, larger s produces smaller er-
rors; however, it imposes more processes during
the construction of a VSM.
</bodyText>
<subsectionHeader confidence="0.990148">
2.3 Experimental Evaluation of RMI
</subsectionHeader>
<bodyText confidence="0.999744857142857">
We report the performance of the RMI method
with respect to its ability to preserve the rela-
tive `1 distance between linguistic entities in a
VSM. Therefore, instead of a task-specific evalua-
tion, we show that the relative `1 distance between
a set of words in a high-dimensional word-by-
document model remains intact when the model
</bodyText>
<footnote confidence="0.867611">
10That is, n context elements are modelled in an n-
dimensional VSM.
</footnote>
<equation confidence="0.9538521">
1
M .
(9)
m
i=1
|ui − vi|)
1
ˆL1 (~u , ~v) = exp
(m
i=1
</equation>
<page confidence="0.842488">
1717
</page>
<bodyText confidence="0.999985673913043">
is constructed at reduced dimensionality using the
RMI technique. We further explore the effect of
the RMI’s parameter setting in the observed re-
sults.
Depending on the structure of the data that is
being analyzed and the objective of the task in
hand, the performance of the `1 distance for sim-
ilarity measurement varies from one application
to another.11 The purpose of our reported evalu-
ation, thus, is not to show the superiority of the `1
distance (thus RMI) to another similarity measure
(e.g. the `2 distance or the cosine similarity) and
employed techniques for dimensionality reduction
(e.g. RI or truncated SVD) in a specific task. If, in
a task, the `1 distance shows higher performance
than the `2 distance, then the RMI technique is
preferable to the RI technique or truncated SVD.
Contrariwise, if the `2 norm shows higher perfor-
mance than the `1, then RI or truncated SVD are
more desirable than the RMI method.
In our experiment, a word-by-document model
is first constructed from the UKWaC corpus at its
original high dimension. UKWaC is a freely avail-
able corpus of 2,692,692 web documents, nearly
2 billion tokens and 4 million types (Baroni et al.,
2009).12 Therefore, a word-by-document model
constructed from this corpus using the classic one-
dimension-per-context-element method has a di-
mension of 2.69 million. In order to keep the ex-
periments computationally tractable, the reported
results are limited to 31 words from this model,
which are listed in Table 1.
In the designed experiment, a word from the list
is taken as the reference and its `1 distance to the
remaining 30 words is calculated using the vec-
tor representations in the high-dimensional VSM.
These 30 words are then sorted in ascending or-
der by the calculated `1 distance. The procedure
is repeated for all the 31 words in the list, one by
one. Therefore, the procedure results in 31 sorted
lists, each containing 30 words. Figure 2 shows an
example of the obtained sorted list, in which the
reference is the word ‘research’.13
The procedure described above is replicated to
obtain the lists of sorted words from VSMs that
are constructed by the RMI method at reduced
</bodyText>
<footnote confidence="0.996247333333333">
11E.g. see the experiments in Bullinaria and Levy (2007).
12UkWaC can be obtained from http://goo.gl/
3isfIE.
13Please note that the number of possible arrangements of
30 words without repetition in a list in which the order is
important (i.e. all permutations of 30 words) is 30!.
</footnote>
<table confidence="0.999640666666667">
PoS Words
Noun website email support software
students skills project research
nhs link services organisations
Adj online digital mobile sustainable
global unique excellent disabled
new current fantastic innovative
Verb use visit improve provided
help ensure develop
</table>
<tableCaption confidence="0.999871">
Table 1: Words employed in the experiments.
</tableCaption>
<figureCaption confidence="0.748522666666667">
Figure 2: List of words sorted by their `1 distance
to the word ‘research’. The distance increases
from left to right and top to bottom.
</figureCaption>
<bodyText confidence="0.999934041666667">
dimensionality, when the method’s parameters—
i.e. the dimensionality of VSM and the number of
non-zero elements in index vectors—are set dif-
ferently. We expect the obtained relative `1 dis-
tances between each reference word and the 30
other words in an RMI-constructed VSM to be the
same as the obtained relative distances in the orig-
inal high-dimensional VSM. Therefore, for each
VSM that is constructed by the RMI technique,
the resulting sorted lists of words are compared by
the sorted lists that are obtained from the original
high-dimensional VSM.
We employ the Spearman’s rank correlation co-
efficient (ρ) to compare the sorted lists of words
and thus the degree of distance preservation in the
RMI-constructed VSMs at reduced dimensional-
ity. The Spearman’s rank correlation measures the
strength of association between two ranked vari-
ables, i.e. two lists of sorted words in our experi-
ments. Given a list of sorted words obtained from
the original high-dimensional VSM (list,,) and its
corresponding list obtained from a VSM of re-
duced dimensionality (listRMI), the Spearman’s
rank correlation for the two lists is calculated by
</bodyText>
<equation confidence="0.9360455">
2
P = 1 − n6 E d&apos;? (11)
</equation>
<bodyText confidence="0.99987275">
where di is the difference in paired ranks of words
in list,, and listRMI, and n = 30 is the number
of words in each list. We report the average of ρ
over the 31 lists of sorted words, denoted by ¯ρ, to
</bodyText>
<page confidence="0.964091">
1718
</page>
<figure confidence="0.810947">
ρ¯
</figure>
<figureCaption confidence="0.993395">
Figure 3: The ρ¯ axis shows the observed average
</figureCaption>
<bodyText confidence="0.982526041666667">
Spearman’ rank correlation between the order of
the words in the lists that are sorted by the `1 dis-
tance obtained from the original high-dimensional
VSM and the VSMs that are constructed by RMI
at reduced dimensionality using index vectors of
various numbers of non-zero elements.
indicate the performance of RMI with respect to
its ability for distance preservation. The closer ρ¯
is to 1, the better the performance of RMI.
Figure 3 shows the observed results at a glance
when the distances are estimated using the median
(Equation 7). As shown in the figure, when the di-
mension of the VSM is above 400 and the number
of non-zero elements is more than 12, the obtained
relative distances from the VSMs constructed by
the RMI technique start to be analogous to the rel-
ative distances that are obtained from the origi-
nal high-dimensional VSM, i.e. a high correlation
(¯ρ &gt; 0.90). For the baseline, we report the av-
erage correlation of ¯ρrandom = −0.004 between
the sorted lists of words obtained from the high-
dimensional VSM and 31 x 1000 lists of sorted
words that are obtained by randomly assigned dis-
tances.
Figure 4 shows the same results as Figure 3,
however, in minute detail and only for VSMs of
dimension m E {100, 400, 800, 3200}. In these
plots, squares ( ) indicate the ρ¯ while the error bars
show the best and the worst observed ρ amongst
all the sorted lists of words. The minimum value
of ρ-axis is set to 0.611, which is the worst ob-
served correlation in the baseline (i.e. randomly
generated distances). The dotted line (ρ = .591)
shows the best observed correlation in the baseline
and the dashed-dotted line shows the average cor-
relation in the baseline (ρ = −0.004). As sug-
gested in Section 2.2, it can be verified that an
Figure 5 represents the obtained results in the
same setting as above, however, when the dis-
tances are approximated using the geometric mean
(Equation 10). The obtained average correlations
ρ¯ from the geometric mean estimations are al-
most identical to the median estimations. How-
ever, as expected, the geometric mean estimations
are more reliable for small values of m; particu-
larly, the worst observed correlations when using
the geometric mean are higher than those observed
when using the median estimator.
</bodyText>
<figureCaption confidence="0.996877">
Figure 4: Detailed observation of the ob-
</figureCaption>
<bodyText confidence="0.999300428571429">
tained correlation between relative distances in
RMI-constructed VSMs and the original high-
dimensional VSM. The `1 distance is estimated
using the median. The squares denote ρ¯ and the er-
ror bars show the best and the worst observed cor-
relations. The dashed-dotted line shows the ran-
dom baseline.
</bodyText>
<figure confidence="0.97221053125">
1
0.9
0.5
100
200
400
64
800
1600 8 16
3200 4
32
ρ¯
0.7
m = 100 m = 400
2 12 20 40 60 70 2 1220 40 60 70
)non-zero elements) )non-zero elements)
ρ
ρ
−0.5
−0.5
1
0.9
0.8
0.5
0.9
0.8
0.5
0
0
1
m = 800
m = 3200
</figure>
<bodyText confidence="0.897437222222222">
increase in the dimension of VSMs (i.e. m) in-
creases the stability of the obtained results (i.e.
0.8 the probability of preserving distances increases).
Therefore, for large values of m (i.e. m &gt; 400),
0.6 the difference between the best and the worst ob-
0.4 served ρ decreases; average correlation ρ¯ → 1 and
the observed relative distances in RMI-constructed
VSMs tend to be identical to those in the original
high-dimensional VSM.
</bodyText>
<page confidence="0.985131">
1719
</page>
<figureCaption confidence="0.985509">
Figure 5: The observed results when the E1 dis-
tance in RMI-constructed VSMs is estimated us-
ing the geometric mean.
</figureCaption>
<sectionHeader confidence="0.928105" genericHeader="method">
3 Random Manhattan Integer Indexing
</sectionHeader>
<bodyText confidence="0.999909808510638">
The application of the RMI method is hindered by
two obstacles: float arithmetic operations required
for the construction and processing of the RMI-
constructed VSMs and the calculation of the prod-
uct of large numbers when E1 distances are esti-
mated using the geometric mean.
The proposed method for the generation of in-
dex vectors in RMI results in index vectors of
non-zero elements that are real numbers. Conse-
quently, index vectors and thus context vectors are
arrays of floating point numbers. These vectors
must be stored and accessed efficiently when using
the RMI technique. However, resources that are
required for the storage and processing of floating
numbers is high. Even if the requirement for the
storage of index vectors is alleviated, e.g., using
a derandomization technique for their generation,
context vectors that are derived from these index
vectors are still arrays of float numbers. To tackle
this problem, we suggest substituting the value of
non-zero elements of RMI’s index vectors (given
in Equation 6) from 1U to integer values of b 1U c,
where b1Uc =6 0. We argue that the resulting ran-
dom projection matrix still has a Cauchy distribu-
tion. Therefore, the proposed methodology to esti-
mate the E1 distance between vectors is also valid.
The E1 distance between context vectors must
be estimated using either the median or the geo-
metric mean. The use of the median estimator—
for the reasons stated in Section 2.1—is not plau-
sible. On the other hand, the computation of the
geometric mean can be laborious as the overflow
is highly likely to happen during its computation.
Using the value of b1Uc for non-zero elements of
index vectors, we know that for any pair of context
vectors i = (u1, · · · , um) and vI = (v1, · · · , vm),
if ui =6 vi then |ui − vi |≥ 1. Therefore, for ui =6
vi, ln |ui−vi |≥ 0 and thus Emi=1 ln(|ui−vi|) ≥ 0.
In this case, the exponent in Equation 10 is a scale
factor that can be discarded without a change in
the relative distances between vectors.14 Based on
the intuition that the distance between a vector and
itself is zero and the explanation given above, in-
spired by smoothing techniques and without being
able to provide mathematical proofs, we suggest
estimating the relative distances between vectors
using
</bodyText>
<equation confidence="0.996766">
ˆL1(Iu,Iv) = �m ln(|ui − vi|). (12)
i=1
ui=,4vi
</equation>
<bodyText confidence="0.9998942">
In order to distinguish the above changes in RMI,
we name the resulting technique random Manhat-
tan integer indexing (RMII). The experiment de-
scribed in Section 2.2 is repeated using the RMII
method. As shown in Figure 6, the obtained results
are almost identical to the observed results when
using the RMI technique. While RMI performs
slightly better than RMII in lower dimensions, e.g.
m = 400, RMII shows more stable behaviour than
RMI at higher dimensions, e.g. m = 800.
</bodyText>
<sectionHeader confidence="0.901496" genericHeader="method">
4 Comparison of RMI and RI
</sectionHeader>
<bodyText confidence="0.993777866666667">
RMI and RI utilize a similar two-step procedure
consisting of the creation of index vectors and the
construction of context vectors. Both methods are
incremental techniques that construct a VSM at
reduced dimensionality directly, without requiring
the VSM to be constructed at its original high di-
mension. Despite these similarities, RMI and RI
are motivated by different applications and math-
14Please note that according to the axioms in the distance
definition, the distance between two numbers is always a non-
negative value. When index vectors consist of non-zero ele-
ments of real numbers, the value of |ui − vi |can be between
0 and 1, i.e. 0 &lt; |ui − vi |&lt; 1. Therefore, ln(|ui − vi|) can
be a negative number and thus the exponent scale is required
to make sure that the result is a non-negative number.
</bodyText>
<figure confidence="0.989924854545455">
m = 100 m = 400
2 12 20 40 60 70 2 1220 40 60 70
|non-zero elements ||non-zero elements|
ρ
ρ
−0.5
−0.5
1
0.9
0.8
0.5
1
0.9
0.8
0.5
0
0
m = 800
m = 3200
1720
m = 100 m = 400 m = 400, the `1 distance m = 800, the `1 distance
P
−0.5
1
0.9
0.8
0.5
0
P
−0.5
0.9
0.8
0.5
0
1
m = 400, median estimator
m = 800, median estimator
m = 800
m = 3200
P
−0.5
1
0.9
0.8
0.5
0
P
−0.5
1
0.9
0.8
0.5
0
2 12 20 40 60 70 2 1220 40 60 70 2 12 20 40 60 70 2 1220 40 60 70
)non-zero elements) )non-zero elements) )non-zero elements) )non-zero elements)
</figure>
<figureCaption confidence="0.995123">
Figure 6: The observed results when using the
</figureCaption>
<bodyText confidence="0.989933214285714">
RMII method for the construction and estimation
of the `1 distances between vectors. The method is
evaluated in the same setup as the RMI technique.
ematical theorems. As described above, RMI ap-
proximates the `1 distance using a non-linear esti-
mator, which has not yet been employed for the
construction of VSMs and the calculation of `1
distances in distributional approaches to seman-
tics. Moreover, RMI is justified using Cauchy ran-
dom projections.
In contrast, RI approximates the `2 distance us-
ing a linear estimator. RI has initially been justi-
fied using the mathematical model of the sparse
distributed memory (SDM)15. Later, Sahlgren
(2005) delineates the RI method using the lemma
proposed by Johnson and Lindenstrauss (1984)—
which elucidates random projections in Euclidean
spaces—and the reported refinement in Achlioptas
(2001) for the projections employed in the lemma.
Although both the RMI and RI methods can
be established as α-stable random projections—
respectively for α = 1 and α = 2—the meth-
ods cannot be compared as they address different
goals. If, for a given task, the `1 norm outperforms
the `2 norm, then RMI is preferable to RI. Con-
trariwise, if the `2 norm outperforms the `1 norm,
then RI is preferable to RMI.
To support the earlier claim that RI-constructed
</bodyText>
<footnote confidence="0.602825">
15See Kanerva (1993) for an overview of the SDM model.
</footnote>
<figureCaption confidence="0.988466">
Figure 7: Evaluation of RI for the `1 distance esti-
</figureCaption>
<bodyText confidence="0.966536875">
mation for m = 400 and m = 800 when the dis-
tances are calculated using the standard definition
of distance in `1 normed spaces and the median es-
timator. The obtained results using RI do not show
correlation to the `1 distances in the original high-
dimensional VSM.
VSMs cannot be used for the `1 distance estima-
tion, we evaluate the RI method in the experimen-
tal setup that has been used for the evaluation of
RMI and RMII. In these experiments, however,
we use RI to construct vector spaces at reduced
dimensionality and estimate the `1 distance us-
ing Equation 5 (the standard `1 distance defini-
tion) and Equation 7 (the median estimator) for
m E 400, 800. As shown in Figure 7, the experi-
ments support the theoretical claims.
</bodyText>
<sectionHeader confidence="0.998799" genericHeader="conclusions">
5 Conclusion
</sectionHeader>
<bodyText confidence="0.999871833333333">
In this paper, we introduce a novel technique,
named Random Manhattan Indexing (RMI), for
the construction of `1 normed VSMs directly at
reduced dimensionality. We further suggest the
Random Manhattan Integer Indexing (RMII) tech-
nique, a computationally enhanced version of the
RMI technique. We demonstrated the `1 distance
preservation ability of the proposed technique in
an experimental setup using a word-by-document
model. In these experiments, we showed how the
variable parameters of the methods, i.e. the num-
ber of non-zero elements in index vectors and the
</bodyText>
<page confidence="0.961904">
1721
</page>
<bodyText confidence="0.99992875">
dimensionality of the VSM, influence the obtained
results. The proposed incremental (and thus effi-
cient and scalable) methods significantly enhance
the computation of the B1 distances in VSMs.
</bodyText>
<sectionHeader confidence="0.993967" genericHeader="acknowledgments">
Acknowledgements
</sectionHeader>
<bodyText confidence="0.9993845">
This publication has emanated from research
conducted with the financial support of Sci-
ence Foundation Ireland under Grant Number
SFI/12/RC/2289.
</bodyText>
<sectionHeader confidence="0.998834" genericHeader="references">
References
</sectionHeader>
<reference confidence="0.999902354166666">
[Achlioptas2001] Dimitris Achlioptas. 2001.
Database-friendly random projections. In Pro-
ceedings of the Twentieth ACM SIGMOD-SIGACT-
SIGART Symposium on Principles of Database
Systems, PODS ’01, pages 274–281, New York, NY,
USA. ACM.
[Baroni et al.2009] Marco Baroni, Silvia Bernardini,
Adriano Ferraresi, and Eros Zanchetta. 2009. The
wacky wide web: a collection of very large linguis-
tically processed web-crawled corpora. Language
Resources and Evaluation, 43(3):209–226.
[Brand2006] Matthew Brand. 2006. Fast low-rank
modifications of the thin singular value decom-
position. Linear Algebra and its Applications,
415(1):20–30. Special Issue on Large Scale Linear
and Nonlinear Eigenvalue Problems.
[Brinkman and Charikar2005] Bo Brinkman and Moses
Charikar. 2005. On the impossibility of dimension
reduction in l1. J. ACM, 52(5):766–788.
[Bullinaria and Levy2007] John A. Bullinaria and
Joseph P. Levy. 2007. Extracting semantic repre-
sentations from word co-occurrence statistics: A
computational study. Behavior Research Methods,
39:510–526.
[Deerwester et al.1990] Scott C. Deerwester, Susan T.
Dumais, Thomas K. Landauer, George W. Furnas,
and Richard A. Harshman. 1990. Indexing by latent
semantic analysis. Journal of the American Society
of Information Science, 41(6):391–407.
[Indyk2000] Piotr Indyk. 2000. Stable distribu-
tions, pseudorandom generators, embeddings and
data stream computation. In Foundations of Com-
puter Science, 2000. Proceedings. 41st Annual Sym-
posium on, pages 189–197.
[Indyk2006] Piotr Indyk. 2006. Stable distributions,
pseudorandom generators, embeddings, and data
stream computation. J. ACM, 53(3):307–323, May.
[Johnson and Lindenstrauss1984] William Johnson and
Joram Lindenstrauss. 1984. Extensions of Lips-
chitz mappings into a Hilbert space. In Conference
in modern analysis and probability (New Haven,
Conn., 1982), volume 26 of Contemporary Mathe-
matics, pages 189–206. American Mathematical So-
ciety.
[Kanerva et al.2000] Pentti Kanerva, Jan Kristoferson,
and Anders Holst. 2000. Random indexing of text
samples for latent semantic analysis. In Proceed-
ings of the 22nd Annual Conference of the Cognitive
Science Society, pages 103–6. Erlbaum.
[Kanerva1993] Pentti Kanerva. 1993. Sparse dis-
tributed memory and related models. In Mo-
hamad H. Hassoun, editor, Associative neural mem-
ories: theory and implementation, chapter 3, pages
50–76. Oxford University Press, Inc., New York,
NY, USA.
[Ke and Kanade2003] Qifa Ke and Takeo Kanade.
2003. Robust subspace computation using Bl norm.
Technical Report CMU-CS-03-172, School of Com-
puter Science, Carnegie Mellon University.
[Kwak2008] Nojun Kwak. 2008. Principal component
analysis based on l1-norm maximization. Pattern
Analysis and Machine Intelligence, IEEE Transac-
tions on, 30(9):1672–1680, Sept.
[Lee1999] Lillian Lee. 1999. Measures of distribu-
tional similarity. In Proceedings of the 37th An-
nual Meeting of the Association for Computational
Linguistics on Computational Linguistics, ACL ’99,
pages 25–32, Stroudsburg, PA, USA. Association
for Computational Linguistics.
[Lenci2008] Alessandro Lenci. 2008. Distributional
semantics in linguistic and cognitive research. From
context to meaning: Distributional models of the lex-
icon in linguistics and cognitive science, special is-
sue of the Italian Journal of Linguistics, 20/1:1–31.
[Li et al.2007] Ping Li, Trevor J. Hastie, and Ken-
neth W. Church. 2007. Nonlinear estimators and tail
bounds for dimension reduction in Ll using cauchy
random projections. J. Mach. Learn. Res., 8:2497–
2532.
[Li2007] Ping Li. 2007. Very sparse stable random
projections for dimension reduction in lα (0 &lt; α &lt;
2) norm. In Proceedings of the 13th ACM SIGKDD
International Conference on Knowledge Discovery
and Data Mining, KDD ’07, pages 440–449, New
York, NY, USA. ACM.
[Li2008] Ping Li. 2008. Estimators and tail bounds for
dimension reduction in Bα (0 &lt; α ≤ 2) using sta-
ble random projections. In Proceedings of the Nine-
teenth Annual ACM-SIAM Symposium on Discrete
Algorithms, SODA ’08, pages 10–19, Philadelphia,
PA, USA. Society for Industrial and Applied Math-
ematics.
[Martin and Berry2007] Dian I. Martin and Michael W.
Berry, 2007. Handbook of latent semantic analysis,
chapter Mathematical foundations behind latent se-
mantic analysis, pages 35–55. Ro.
</reference>
<page confidence="0.839869">
1722
</page>
<reference confidence="0.999816230769231">
[Sahlgren2005] Magnus Sahlgren. 2005. An introduc-
tion to random indexing. In Methods and Applica-
tions of Semantic Indexing Workshop at the 7th In-
ternational Conference on Terminology and Knowl-
edge Engineering, TKE 2005.
[Turney and Pantel2010] Peter D. Turney and Patrick
Pantel. 2010. From frequency to meaning: vec-
tor space models of semantics. J. Artif. Int. Res.,
37(1):141–188, January.
[Weeds et al.2005] Julie Weeds, James Dowdall,
Gerold Schneider, Bill Keller, and David Weir.
2005. Using distributional similarity to organise
biomedical terminology. Terminology, 11(1):3–4.
</reference>
<page confidence="0.955295">
1723
</page>
</variant>
</algorithm>
<algorithm name="ParsHed" version="110505">
<variant no="0" confidence="0.456937">
<title confidence="0.9930925">Random Manhattan Integer Indexing: Vector Space Construction</title>
<author confidence="0.486932">Q</author>
<affiliation confidence="0.9819255">Centre of Computer Science and Mathematics National University of Ireland,</affiliation>
<address confidence="0.998215">Galway, Ireland</address>
<email confidence="0.997489">behrang.qasemizadeh@insight-centre.org</email>
<abstract confidence="0.9981557">Vector space models (VSMs) are mathematically well-defined frameworks that have been widely used in the distributional approaches to semantics. In VSMs, highdimensional vectors represent linguistic entities. In an application, the similarity of vectors—and thus the entities that they represent—is computed by a distance formula. The high dimensionality of vectors, however, is a barrier to the performance of methods that employ VSMs. Consequently, a dimensionality reduction technique is employed to alleviate this problem. This paper introduces a novel technique called Random Manhattan In- (RMI) for the construction of normed VSMs at reduced dimensionality. RMI combines the construction of a VSM and dimension reduction into an incremental and thus scalable two-step procedure. In order to attain its goal, RMI employs the sparse Cauchy random projections. We further introduce Random Manhattan Integer Indexing (RMII): a computationally enhanced version of RMI. As shown in the reported experiments, RMI and RMII can be used reliably to estimate between vectors in a vector space of low dimensionality.</abstract>
</variant>
</algorithm>
<algorithm name="ParsCit" version="110505">
<citationList>
<citation valid="true">
<authors>
<author>Dimitris Achlioptas</author>
</authors>
<title>Database-friendly random projections.</title>
<date>2001</date>
<booktitle>In Proceedings of the Twentieth ACM SIGMOD-SIGACTSIGART Symposium on Principles of Database Systems, PODS ’01,</booktitle>
<pages>274--281</pages>
<publisher>ACM.</publisher>
<location>New York, NY, USA.</location>
<marker>[Achlioptas2001]</marker>
<rawString>Dimitris Achlioptas. 2001. Database-friendly random projections. In Proceedings of the Twentieth ACM SIGMOD-SIGACTSIGART Symposium on Principles of Database Systems, PODS ’01, pages 274–281, New York, NY, USA. ACM.</rawString>
</citation>
<citation valid="true">
<authors>
<author>Marco Baroni</author>
<author>Silvia Bernardini</author>
<author>Adriano Ferraresi</author>
<author>Eros Zanchetta</author>
</authors>
<title>The wacky wide web: a collection of very large linguistically processed web-crawled corpora. Language Resources and Evaluation,</title>
<date>2009</date>
<pages>43--3</pages>
<marker>[Baroni et al.2009]</marker>
<rawString>Marco Baroni, Silvia Bernardini, Adriano Ferraresi, and Eros Zanchetta. 2009. The wacky wide web: a collection of very large linguistically processed web-crawled corpora. Language Resources and Evaluation, 43(3):209–226.</rawString>
</citation>
<citation valid="true">
<authors>
<author>Matthew Brand</author>
</authors>
<title>Fast low-rank modifications of the thin singular value decomposition. Linear Algebra and its Applications, 415(1):20–30. Special Issue on Large Scale Linear and Nonlinear Eigenvalue Problems.</title>
<date>2006</date>
<marker>[Brand2006]</marker>
<rawString>Matthew Brand. 2006. Fast low-rank modifications of the thin singular value decomposition. Linear Algebra and its Applications, 415(1):20–30. Special Issue on Large Scale Linear and Nonlinear Eigenvalue Problems.</rawString>
</citation>
<citation valid="true">
<authors>
<author>Bo Brinkman</author>
<author>Moses Charikar</author>
</authors>
<title>On the impossibility of dimension reduction in l1.</title>
<date>2005</date>
<journal>J. ACM,</journal>
<volume>52</volume>
<issue>5</issue>
<marker>[Brinkman and Charikar2005]</marker>
<rawString>Bo Brinkman and Moses Charikar. 2005. On the impossibility of dimension reduction in l1. J. ACM, 52(5):766–788.</rawString>
</citation>
<citation valid="true">
<authors>
<author>John A Bullinaria</author>
<author>Joseph P Levy</author>
</authors>
<title>Extracting semantic representations from word co-occurrence statistics: A computational study.</title>
<date>2007</date>
<journal>Behavior Research Methods,</journal>
<pages>39--510</pages>
<marker>[Bullinaria and Levy2007]</marker>
<rawString>John A. Bullinaria and Joseph P. Levy. 2007. Extracting semantic representations from word co-occurrence statistics: A computational study. Behavior Research Methods, 39:510–526.</rawString>
</citation>
<citation valid="true">
<authors>
<author>Scott C Deerwester</author>
<author>Susan T Dumais</author>
<author>Thomas K Landauer</author>
<author>George W Furnas</author>
<author>Richard A Harshman</author>
</authors>
<title>Indexing by latent semantic analysis.</title>
<date>1990</date>
<journal>Journal of the American Society of Information Science,</journal>
<volume>41</volume>
<issue>6</issue>
<marker>[Deerwester et al.1990]</marker>
<rawString>Scott C. Deerwester, Susan T. Dumais, Thomas K. Landauer, George W. Furnas, and Richard A. Harshman. 1990. Indexing by latent semantic analysis. Journal of the American Society of Information Science, 41(6):391–407.</rawString>
</citation>
<citation valid="true">
<authors>
<author>Piotr Indyk</author>
</authors>
<title>Stable distributions, pseudorandom generators, embeddings and data stream computation.</title>
<date>2000</date>
<booktitle>In Foundations of Computer Science,</booktitle>
<pages>189--197</pages>
<marker>[Indyk2000]</marker>
<rawString>Piotr Indyk. 2000. Stable distributions, pseudorandom generators, embeddings and data stream computation. In Foundations of Computer Science, 2000. Proceedings. 41st Annual Symposium on, pages 189–197.</rawString>
</citation>
<citation valid="true">
<authors>
<author>Piotr Indyk</author>
</authors>
<title>Stable distributions, pseudorandom generators, embeddings, and data stream computation.</title>
<date>2006</date>
<journal>J. ACM,</journal>
<volume>53</volume>
<issue>3</issue>
<marker>[Indyk2006]</marker>
<rawString>Piotr Indyk. 2006. Stable distributions, pseudorandom generators, embeddings, and data stream computation. J. ACM, 53(3):307–323, May.</rawString>
</citation>
<citation valid="true">
<authors>
<author>William Johnson</author>
<author>Joram Lindenstrauss</author>
</authors>
<title>Extensions of Lipschitz mappings into a Hilbert space.</title>
<date>1984</date>
<journal>of Contemporary Mathematics,</journal>
<booktitle>In Conference in modern analysis and probability</booktitle>
<volume>26</volume>
<pages>189--206</pages>
<publisher>American Mathematical Society.</publisher>
<location>New Haven, Conn.,</location>
<marker>[Johnson and Lindenstrauss1984]</marker>
<rawString>William Johnson and Joram Lindenstrauss. 1984. Extensions of Lipschitz mappings into a Hilbert space. In Conference in modern analysis and probability (New Haven, Conn., 1982), volume 26 of Contemporary Mathematics, pages 189–206. American Mathematical Society.</rawString>
</citation>
<citation valid="true">
<authors>
<author>Pentti Kanerva</author>
<author>Jan Kristoferson</author>
<author>Anders Holst</author>
</authors>
<title>Random indexing of text samples for latent semantic analysis.</title>
<date>2000</date>
<booktitle>In Proceedings of the 22nd Annual Conference of the Cognitive Science Society,</booktitle>
<pages>103--6</pages>
<publisher>Erlbaum.</publisher>
<marker>[Kanerva et al.2000]</marker>
<rawString>Pentti Kanerva, Jan Kristoferson, and Anders Holst. 2000. Random indexing of text samples for latent semantic analysis. In Proceedings of the 22nd Annual Conference of the Cognitive Science Society, pages 103–6. Erlbaum.</rawString>
</citation>
<citation valid="true">
<authors>
<author>Pentti Kanerva</author>
</authors>
<title>Sparse distributed memory and related models.</title>
<date>1993</date>
<booktitle>Associative neural memories: theory and implementation, chapter 3,</booktitle>
<pages>50--76</pages>
<editor>In Mohamad H. Hassoun, editor,</editor>
<publisher>Oxford University Press, Inc.,</publisher>
<location>New York, NY, USA.</location>
<marker>[Kanerva1993]</marker>
<rawString>Pentti Kanerva. 1993. Sparse distributed memory and related models. In Mohamad H. Hassoun, editor, Associative neural memories: theory and implementation, chapter 3, pages 50–76. Oxford University Press, Inc., New York, NY, USA.</rawString>
</citation>
<citation valid="true">
<authors>
<author>Qifa Ke</author>
<author>Takeo Kanade</author>
</authors>
<title>Robust subspace computation using Bl norm.</title>
<date>2003</date>
<tech>Technical Report CMU-CS-03-172,</tech>
<institution>School of Computer Science, Carnegie Mellon University.</institution>
<marker>[Ke and Kanade2003]</marker>
<rawString>Qifa Ke and Takeo Kanade. 2003. Robust subspace computation using Bl norm. Technical Report CMU-CS-03-172, School of Computer Science, Carnegie Mellon University.</rawString>
</citation>
<citation valid="true">
<authors>
<author>Nojun Kwak</author>
</authors>
<title>Principal component analysis based on l1-norm maximization.</title>
<date>2008</date>
<journal>Pattern Analysis and Machine Intelligence, IEEE Transactions on,</journal>
<volume>30</volume>
<issue>9</issue>
<marker>[Kwak2008]</marker>
<rawString>Nojun Kwak. 2008. Principal component analysis based on l1-norm maximization. Pattern Analysis and Machine Intelligence, IEEE Transactions on, 30(9):1672–1680, Sept.</rawString>
</citation>
<citation valid="true">
<authors>
<author>Lillian Lee</author>
</authors>
<title>Measures of distributional similarity.</title>
<date>1999</date>
<booktitle>In Proceedings of the 37th Annual Meeting of the Association for Computational Linguistics on Computational Linguistics, ACL ’99,</booktitle>
<pages>25--32</pages>
<publisher>Association</publisher>
<institution>for Computational Linguistics.</institution>
<location>Stroudsburg, PA, USA.</location>
<marker>[Lee1999]</marker>
<rawString>Lillian Lee. 1999. Measures of distributional similarity. In Proceedings of the 37th Annual Meeting of the Association for Computational Linguistics on Computational Linguistics, ACL ’99, pages 25–32, Stroudsburg, PA, USA. Association for Computational Linguistics.</rawString>
</citation>
<citation valid="true">
<authors>
<author>Alessandro Lenci</author>
</authors>
<title>Distributional semantics in linguistic and cognitive research. From context to meaning: Distributional models of the lexicon in linguistics and cognitive science,</title>
<date>2008</date>
<journal>special issue of the Italian Journal of Linguistics,</journal>
<pages>20--1</pages>
<marker>[Lenci2008]</marker>
<rawString>Alessandro Lenci. 2008. Distributional semantics in linguistic and cognitive research. From context to meaning: Distributional models of the lexicon in linguistics and cognitive science, special issue of the Italian Journal of Linguistics, 20/1:1–31.</rawString>
</citation>
<citation valid="true">
<authors>
<author>Ping Li</author>
<author>Trevor J Hastie</author>
<author>Kenneth W Church</author>
</authors>
<title>Nonlinear estimators and tail bounds for dimension reduction in Ll using cauchy random projections.</title>
<date>2007</date>
<journal>J. Mach. Learn. Res.,</journal>
<volume>8</volume>
<pages>2532</pages>
<marker>[Li et al.2007]</marker>
<rawString>Ping Li, Trevor J. Hastie, and Kenneth W. Church. 2007. Nonlinear estimators and tail bounds for dimension reduction in Ll using cauchy random projections. J. Mach. Learn. Res., 8:2497– 2532.</rawString>
</citation>
<citation valid="true">
<authors>
<author>Ping Li</author>
</authors>
<title>Very sparse stable random projections for dimension reduction in lα (0 &lt; α &lt; 2) norm.</title>
<date>2007</date>
<booktitle>In Proceedings of the 13th ACM SIGKDD International Conference on Knowledge Discovery and Data Mining, KDD ’07,</booktitle>
<pages>440--449</pages>
<publisher>ACM.</publisher>
<location>New York, NY, USA.</location>
<marker>[Li2007]</marker>
<rawString>Ping Li. 2007. Very sparse stable random projections for dimension reduction in lα (0 &lt; α &lt; 2) norm. In Proceedings of the 13th ACM SIGKDD International Conference on Knowledge Discovery and Data Mining, KDD ’07, pages 440–449, New York, NY, USA. ACM.</rawString>
</citation>
<citation valid="true">
<authors>
<author>Ping Li</author>
</authors>
<title>Estimators and tail bounds for dimension reduction in Bα (0 &lt; α ≤ 2) using stable random projections.</title>
<date>2008</date>
<journal>Society for Industrial and Applied Mathematics.</journal>
<booktitle>In Proceedings of the Nineteenth Annual ACM-SIAM Symposium on Discrete Algorithms, SODA ’08,</booktitle>
<pages>10--19</pages>
<location>Philadelphia, PA, USA.</location>
<marker>[Li2008]</marker>
<rawString>Ping Li. 2008. Estimators and tail bounds for dimension reduction in Bα (0 &lt; α ≤ 2) using stable random projections. In Proceedings of the Nineteenth Annual ACM-SIAM Symposium on Discrete Algorithms, SODA ’08, pages 10–19, Philadelphia, PA, USA. Society for Industrial and Applied Mathematics.</rawString>
</citation>
<citation valid="true">
<authors>
<author>Dian I Martin</author>
<author>Michael W Berry</author>
</authors>
<title>Handbook of latent semantic analysis, chapter Mathematical foundations behind latent semantic analysis,</title>
<date>2007</date>
<pages>35--55</pages>
<location>Ro.</location>
<marker>[Martin and Berry2007]</marker>
<rawString>Dian I. Martin and Michael W. Berry, 2007. Handbook of latent semantic analysis, chapter Mathematical foundations behind latent semantic analysis, pages 35–55. Ro.</rawString>
</citation>
<citation valid="true">
<authors>
<author>Magnus Sahlgren</author>
</authors>
<title>An introduction to random indexing.</title>
<date>2005</date>
<booktitle>In Methods and Applications of Semantic Indexing Workshop at the 7th International Conference on Terminology and Knowledge Engineering, TKE</booktitle>
<marker>[Sahlgren2005]</marker>
<rawString>Magnus Sahlgren. 2005. An introduction to random indexing. In Methods and Applications of Semantic Indexing Workshop at the 7th International Conference on Terminology and Knowledge Engineering, TKE 2005.</rawString>
</citation>
<citation valid="true">
<authors>
<author>Peter D Turney</author>
<author>Patrick Pantel</author>
</authors>
<title>From frequency to meaning: vector space models of semantics.</title>
<date>2010</date>
<journal>J. Artif. Int. Res.,</journal>
<volume>37</volume>
<issue>1</issue>
<marker>[Turney and Pantel2010]</marker>
<rawString>Peter D. Turney and Patrick Pantel. 2010. From frequency to meaning: vector space models of semantics. J. Artif. Int. Res., 37(1):141–188, January.</rawString>
</citation>
<citation valid="true">
<authors>
<author>Julie Weeds</author>
<author>James Dowdall</author>
<author>Gerold Schneider</author>
<author>Bill Keller</author>
<author>David Weir</author>
</authors>
<title>Using distributional similarity to organise biomedical terminology.</title>
<date>2005</date>
<journal>Terminology,</journal>
<volume>11</volume>
<issue>1</issue>
<marker>[Weeds et al.2005]</marker>
<rawString>Julie Weeds, James Dowdall, Gerold Schneider, Bill Keller, and David Weir. 2005. Using distributional similarity to organise biomedical terminology. Terminology, 11(1):3–4.</rawString>
</citation>
</citationList>
</algorithm>
</algorithms>
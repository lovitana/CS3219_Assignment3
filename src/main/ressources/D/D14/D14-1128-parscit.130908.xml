<?xml version="1.0" encoding="UTF-8"?>
<algorithms version="110505">
<algorithm name="SectLabel" version="110505">
<variant no="0" confidence="0.009668">
<title confidence="0.997666">
Fine-Grained Contextual Predictions for Hard Sentiment Words
</title>
<author confidence="0.998198">
Sebastian Ebert and Hinrich Sch¨utze
</author>
<affiliation confidence="0.998224">
Center for Information and Language Processing
University of Munich, Germany
</affiliation>
<email confidence="0.986482">
ebert@cis.lmu.de,inquiries@cislmu.org
</email>
<sectionHeader confidence="0.993638" genericHeader="abstract">
Abstract
</sectionHeader>
<bodyText confidence="0.999986923076923">
We put forward the hypothesis that high-
accuracy sentiment analysis is only pos-
sible if word senses with different polar-
ity are accurately recognized. We pro-
vide evidence for this hypothesis in a case
study for the adjective “hard” and propose
contextually enhanced sentiment lexicons
that contain the information necessary for
sentiment-relevant sense disambiguation.
An experimental evaluation demonstrates
that senses with different polarity can be
distinguished well using a combination of
standard and novel features.
</bodyText>
<sectionHeader confidence="0.998801" genericHeader="keywords">
1 Introduction
</sectionHeader>
<bodyText confidence="0.999976444444444">
This paper deals with fine-grained sentiment anal-
ysis. We aim to make three contributions. First,
based on a detailed linguistic analysis of contexts
of the word “hard” (Section 3), we give evidence
that highly accurate sentiment analysis is only pos-
sible if senses with different polarity are accu-
rately recognized.
Second, based on this analysis, we propose to
return to a lexicon-based approach to sentiment
analysis that supports identifying sense distinc-
tions relevant to sentiment. Currently available
sentiment lexicons give the polarity for each word
or each sense, but this is of limited utility if senses
cannot be automatically identified in context. We
extend the lexicon-based approach by introducing
the concept of a contextually enhanced sentiment
lexicon (CESL). The lexicon entry of a word w in
CESL has three components: (i) the senses of w;
(ii) a sentiment annotation of each sense; (iii) a
data structure that, given a context in which w oc-
curs, allows to identify the sense of w used in that
context.
As we will see in Section 3, the CESL sense
inventory – (i) above – should be optimized for
sentiment analysis: closely related senses with the
same sentiment should be merged whereas subtle
semantic distinctions that give rise to different po-
larities should be distinguished.
The data structure in (iii) is a statistical classi-
fication model in the simplest case. We will give
one other example for (iii) below: it can also be a
set of centroids of context vector representations,
with a mapping of these centroids to the senses.
If sentiment-relevant sense disambiguation is
the first step in sentiment analysis, then power-
ful contextual features are necessary to support
making fine-grained distinctions. Our third con-
tribution is that we experiment with deep learn-
ing as a source of such features. We look at
two types of deep learning features: word em-
beddings and neural network language model pre-
dictions (Section 4). We show that deep learn-
ing features significantly improve the accuracy
of context-dependent polarity classification (Sec-
tion 5).
</bodyText>
<sectionHeader confidence="0.999493" genericHeader="introduction">
2 Related work
</sectionHeader>
<bodyText confidence="0.999856111111111">
Initial work on sentiment analysis was either based
on sentiment lexicons that listed words as posi-
tive or negative sentiment indicators (e.g., Turney
(2002), Yu and Hatzivassiloglou (2003)), on statis-
tical classification approaches that represent doc-
uments as ngrams (e.g., Pang et al. (2002)) or on
a combination of both (e.g., Riloff et al. (2003),
Whitelaw et al. (2005)). The underlying assump-
tion of lexicon-based sentiment analysis is that
a word always has the same sentiment. This is
clearly wrong because words can have senses with
different polarity, e.g., “hard wood” (neutral) vs.
“hard memory” (negative).
Ngram approaches are also limited because
ngram representations are not a good basis for
relevant generalizations. For example, the neu-
tral adverbial sense ‘intense’ of “hard” (“laugh
hard”, “try hard”) vs. the negative adjectival mean-
</bodyText>
<page confidence="0.848731">
1210
</page>
<note confidence="0.492798">
Proceedings of the 2014 Conference on Empirical Methods in Natural Language Processing (EMNLP), pages 1210–1215,
October 25-29, 2014, Doha, Qatar. c�2014 Association for Computational Linguistics
</note>
<table confidence="0.999456210526316">
Cobuild syntax meaning example patterns sent. # train # test
1 FIRM 1 ADJ firm, stiff hard floor neu 78 5
2 DIFFICULT 2, 4, 9, ADJ difficult hard question hard for, neg 2561 120
10, 11 hard on,
hard to V
3 ADVERB 3a, 5, ADV intensely work hard neu 425 19
6, 7
4 INTENSE 3b ADJ intense hard look be hard at neu 24 7
it
5 HARD-MAN 8 ADJ unkind hard man neg 15 0
6 HARD-TRUTH 12 attributive definitely hard truth neu 5 6
ADJ true
7 MUSIC ADJ hard-rock- hard beats neu 347 15
type music
8 CONTRAST ADJ opposite of hard edge neu 3 1
soft transi-
tion
9 NEGATIVE-P 13, 15 phrases neg 36 2
10 NEUTRAL-P 14, 16 phrases neu 375 27
</table>
<tableCaption confidence="0.999965">
Table 1: Sense inventory of “hard”.
</tableCaption>
<bodyText confidence="0.99966526">
ing ‘difficult’ (“hard life”, “hard memory”) cannot
be easily distinguished based on an ngram repre-
sentation. Moreover, although ngram approaches
could learn the polarity of these phrases they do
not generalize to new phrases.
More recent compositional approaches to senti-
ment analysis can outperform lexicon and ngram-
based methods (e.g., Socher et al. (2011), Socher
et al. (2013)). However, these approaches conflate
two different types of contextual effects: differ-
ences in sense or lexical meaning (“hard memory”
vs. “hard wood”) on the one hand and meaning
composition like negation on the other hand. From
the point of view of linguistic theory, these are dif-
ferent types of contextual effects that should not
be conflated. Recognizing that “hard” occurs in
the scope of negation is of no use if the basic po-
larity of the contextually evoked sense of “hard”
(e.g., negative in “no hard memories” vs. neutral
in “no hard wood”) is not recognized.
Wilson et al. (2009) present an approach to clas-
sify contextual polarity building on a two-step pro-
cess. First, they classify if a sentiment word is po-
lar in a phrase and if so, second, they classify its
polarity. Our approach can be seen as an exten-
sion of this approach; the main difference is that
we will show in our analysis of “hard” that the
polarity of phrases depends on the senses of the
words that are used. This is evidence that high-
accuracy polarity classification depends on sense
disambiguation.
There has been previous work on assigning po-
larity values to senses of words taken from Word-
Net (e.g., Baccianella et al. (2010), Wiebe and Mi-
halcea (2006)). However, these approaches are not
able to disambiguate the sense of a word given its
context.
Previous work on representation learning for
sentiment analysis includes (Maas and Ng, 2010)
and (Maas et al., 2011). Their models learn word
embeddings that capture semantic similarities and
word sentiment at the same time. Their approach
focuses on sentiment of entire sentences or docu-
ments and does not consider each sentiment word
instance at a local level.
We present experiments with one supervised
and one semisupervised approach to word sense
disambiguation (WSD) in this paper. Other
WSD approaches, e.g., thesaurus-based WSD
(Yarowsky, 1992), could also be used for CESL.
</bodyText>
<sectionHeader confidence="0.694326" genericHeader="method">
3 Linguistic analysis of sentiment
</sectionHeader>
<subsectionHeader confidence="0.583522">
contexts of “hard”
</subsectionHeader>
<bodyText confidence="0.999995230769231">
We took a random sample of 5000 contexts of
“hard” in the Amazon Product Review Data (Jin-
dal and Liu, 2008). We use 200 as a test set and set
aside 200 for future use. We analyzed the remain-
ing 4600 contexts using a tool we designed for this
study, which provides functionality for selecting
and sorting contexts, including a keyword in con-
text display. If a reliable pattern has been identi-
fied (e.g., the phrase “die hard”), then all contexts
matching the pattern can be labeled automatically.
Our goal is to identify the different uses of
“hard” that are relevant for sentiment. The basis
for our inventory is the Cobuild (Sinclair, 1987)
</bodyText>
<page confidence="0.940983">
1211
</page>
<bodyText confidence="0.999828555555556">
lexicon entry for “hard”. We use Cobuild because
it was compiled based on an empirical analysis of
corpus data and is therefore more likely to satisfy
the requirements of NLP applications than a tradi-
tional dictionary.
Cobuild lists 16 senses. One of these senses
(3) is split into two to distinguish the adverbial
(“to accelerate hard”) and adjectival (“hard accel-
eration”) uses of “hard” in the meaning ‘intense’.
We conflated five senses (2, 4, 9, 10, 11) refer-
ring to different types of difficulty: “hard ques-
tion” (2), “hard work” (4), “hard life” (11) and
two variants of “hard on”: “hard on someone”
(9), “hard on something” (10); and four differ-
ent senses (3a, 5, 6, 7) referring to different types
of intensity: “to work hard” (3a), “to look hard”
(5), “to kick hard” (6), “to laugh hard” (7). Fur-
thermore, we identified a number of noncompo-
sitional meanings or phrases (lists NEGATIVE-P
and NEUTRAL-P in the supplementary material1)
in addition to the four listed by Cobuild (13, 14,
15, 16). In addition, new senses for “hard” are in-
troduced for opposites of senses of “soft”: the op-
posite of ‘quiet/gentle voice/sound’ (7: MUSIC;
e.g., “hard beat”, “not too hard of a song”) and
the opposite of ‘smooth surface/texture’ (8: CON-
TRAST; e.g., “hard line”, “hard edge”).
Table 1 lists the 10 different uses that are the re-
sult of our analysis. For each use, we give the cor-
responding Cobuild sense numbers, syntactic in-
formation, meaning, an example, typical patterns,
polarity, and number of occurrences in training
and test sets.
7 uses are neutral and 3 are negative. As
“hard’s” polarity in most sentiment lexicons is
negative, but only 3 out of 7 senses are negative,
“hard” provides evidence for our hypothesis that
senses need to be disambiguated to allow for fine-
grained and accurate polarity recognition.
We hired two PhD students to label each of the
200 contexts in the test set with one of the 10 la-
bels in Table 1 (n = .78). Disagreement was re-
solved by a third person.
We have published the labeled data set of
4600+200 contexts as supplementary material.
</bodyText>
<sectionHeader confidence="0.987315" genericHeader="method">
4 Deep learning features
</sectionHeader>
<bodyText confidence="0.9993565">
We use two types of deep learning features to be
able to make the fine-grained distinctions neces-
</bodyText>
<footnote confidence="0.9918735">
1All supplementary material is available at http://
www.cis.lmu.de/ebert.
</footnote>
<bodyText confidence="0.999888193548387">
sary for sense disambiguation. First, we use word
embeddings similar to other recent work (see be-
low). Second, we use a deep learning language
model (LM) to predict the distribution of words for
the position at which the word of interest occurs.
For example, an LM will predict that words like
“granite” and “concrete” are likely in the context
“a * countertop” and that words like “serious” and
“difficult” are likely in the context “a * problem”.
This is then the basis for distinguishing contexts
in which “hard” is neutral (in the meaning ‘firm,
solid’) from contexts in which it is a sentiment in-
dicator (in the meaning ‘difficult’). We will use
the term predicted context distribution or PCD to
refer to the distribution predicted by the LM.
We use the vectorized log-bilinear language
model (vLBL) (Mnih and Kavukcuoglu, 2013)
because it has three appealing features. (i) It
learns state of the art word embeddings (Mnih and
Kavukcuoglu, 2013). (ii) The model is a language
model and can be used to calculate PCDs. (iii) As
a linear model, vLBL can be trained much faster
than other models (e.g., Bengio et al. (2003)).
The vLBL trains one set of word embeddings
for the input space (R) and one for the target space
(Q). We denote the input representation of word
w as rw and the target representation as qw. For a
given context c = w1, ... , wn the model predicts
a target representation q&amp;quot; by linearly combining the
context word representations with position depen-
dent weights:
</bodyText>
<equation confidence="0.989315333333333">
n
&amp;quot;q(c) = di O rwi
i=1
</equation>
<bodyText confidence="0.9927038">
where di E D is the weight vector associated
with position i in the context and O is point-
wise multiplication. Given the model parameters
θ = {R, Q, D, b} the similarity between q&amp;quot; and the
correct target word embedding is computed by the
similarity function
sθ(w, c) = &amp;quot;q(c)T qw + bw
where bw is a bias term.
We train the model with stochastic gradient
descent on mini-batches of size 100, following
the noise-contrastive estimation training proce-
dure of Mnih and Kavukcuoglu (2013). We use
AdaGrad (Duchi et al., 2011) with the initial learn-
ing rate set to q = 0.5. The embeddings size is set
to 100.
</bodyText>
<page confidence="0.988102">
1212
</page>
<figure confidence="0.983551736842106">
acc prec rec F1
1 .62 .62 1.00 .76
2 + .90 .91 .94 .92
3 + .90 .91 .92 .92
4 + .87 .87 .92 .90
5 + + .92 .92 .94 .93
6 + + .91 .90 .95 .92
7 + + .86 .83 .96 .89
8 + + + .92 .93 .95 .94
9 + .85 .87 .89 .88
10 + .85 .87 .89 .88
11 + .76 .73 .98 .83
12 + + .85 .87 .89 .88
13 + + .85 .87 .89 .88
14 + + .85 .89 .87 .88
15 + + + .86 .87 .90 .89
16 .66 .66 1.00 .80
17 + + + .90 .89 .96 .92
18 + + + .85 .85 .91 .88
</figure>
<tableCaption confidence="0.975842">
Table 2: Classification results; bl: baseline
</tableCaption>
<bodyText confidence="0.999955">
During training we do not need to normalize the
similarity explicitly, because the normalization is
implicitly learned by the model. However, nor-
malization is still necessary for prediction. The
normalized PCD for a context c of word w is com-
puted using the softmax function:
</bodyText>
<equation confidence="0.968502333333333">
exp(sθ(w,c))
Pθ c (w) = E
w, exp(sθ(w�, c))
</equation>
<bodyText confidence="0.998555777777778">
We use a window size of ws = 7 for training the
model. We found that the model did not capture
enough contextual phenomena for ws = 3 and that
results for ws = 11 did not have better quality
than ws = 7, but had a negative impact on the
training time. Using a vocabulary of the 100,000
most frequent words, we train the vLBL model for
4 epochs on 1.3 billion 7-grams randomly selected
from the English Wikipedia.
</bodyText>
<sectionHeader confidence="0.9996" genericHeader="evaluation">
5 Experiments
</sectionHeader>
<bodyText confidence="0.736409714285714">
The lexicon entry of “hard” in CESL consists of (i)
the senses, (ii) the polarity annotations (neutral or
negative) and (iii) the sense disambiguation data
structure. Components (i) and (ii) are shown in
Table 1. In this section, we evaluate two different
options for (iii) on the task of sentiment classifica-
tion.
</bodyText>
<figure confidence="0.992219666666667">
1 2 3 4 5 6 7 8
1
2 $
3 $
4 $ $ ·
5 $ $
6 $ $
7 $ $ * $ $
8 $ * * $ * $
</figure>
<tableCaption confidence="0.950065">
Table 3: Significant differences of lines 1–8 in Ta-
ble 2; $: p=0.01, *: p=0.05, ·: p=0.1
</tableCaption>
<bodyText confidence="0.989442842105263">
The first approach is to use a statistical classi-
fication model as the sense disambiguation struc-
ture. We use liblinear (Fan et al., 2008) with stan-
dard parameters for classification based on three
different feature types: ngrams, embeddings (em-
bed) and PCDs. Ngram features are all n-grams
for n E 11, 2, 3}. As embedding features we
use (i) the mean of the input space (R) embed-
dings and (ii) the mean of the target space (Q) em-
beddings of the words in the context (see Blacoe
and Lapata (2012) for justification of using simple
mean). As PCD features we use the PCD predicted
by vLBL for the sentiment word of interest, in our
case “hard”.
We split the set of 4600 contexts introduced in
Section 3 into a training set of 4000 and a devel-
opment set of 600.
Table 2 (lines 1–8) shows the classification re-
sults on the development set for all feature type
combinations. Significant differences between re-
sults – computed using the approximate random-
ization test (Pad´o, 2006) – are given in Table 3.
The majority baseline (bl), which assigns a nega-
tive label to all examples, reaches F1 = .76. The
classifier is significantly better than the baseline
for all feature combinations with F1 ranging from
.89 to .94. We obtain the best classification result
(.94) when all three feature types are combined
(significantly better than all other feature combi-
nations except for 5).
Manually labeling all occurrences of a word
is expensive. As an alternative we investigate
clustering of the contexts of the word of interest.
Therefore, we represent each of the 4000 con-
texts of “hard” in the training set as its PCD2, use
2To transform vectors into a format that is more appropri-
ate for the underlying Gaussian model of kmeans, we take the
square root of each probability in the PCD vectors.
</bodyText>
<figure confidence="0.996283">
ngram
PCD
embed
bl
development
fully
semi
bl
test
fully
semi
</figure>
<page confidence="0.978269">
1213
</page>
<bodyText confidence="0.9998894">
kmeans clustering with k = 100 and then label
each cluster. This decreases the cost of labeling
by an order of magnitude since only 100 clusters
have to be labeled instead of 4000 training set con-
texts.
Table 2 (lines 9–15) shows results for this
semisupervised approach to classification, using
the same classifier and the same feature types, but
the cluster-based labels instead of manual labels.
For most feature combinations, F1 drops com-
pared to fully supervised classification. The best
performing model for supervised classification
(ngram+PCD+embed) loses 5%.
This is not a large drop considering the savings
in manual labeling effort. All results are signifi-
cantly better than the baseline. There are no signif-
icant differences between the different feature sets
(lines 9–15) with the exception of embed, which
is significantly worse than the other 6 sets.
The centroids of the 100 clusters can serve as an
alternative sense disambiguation structure for the
lexicon entry of “hard” in CESL.3 Each sense s is
associated with the centroids of the clusters whose
majority sense is s.
As final experiment (lines 16–18 in Table 2),
we evaluate performance for the baseline and for
PCD+ngram+embed – the best feature set – on the
test set. On the test set, baseline performance is
.80 (.04 higher than .76 on line 1, Table 2); F1 of
PCD+ngram+embed is .92 (.02 less than develop-
ment set) for supervised classification and is .88
(.01 less) for semisupervised classification (com-
paring to lines 8 and 15 in Table 2). Both results
(.92 and .88) are significantly higher than the base-
line (.80).
</bodyText>
<sectionHeader confidence="0.999249" genericHeader="conclusions">
6 Conclusion
</sectionHeader>
<bodyText confidence="0.999659230769231">
The sentiment of a sentence or document is the
output of a causal chain that involves complex lin-
guistic processes like contextual modification and
negation. Our hypothesis in this paper was that
for high-accuracy sentiment analysis, we need to
model the root causes of this causal chain: the
meanings of individual words. This is in contrast
to other work in sentiment analysis that conflates
different linguistic phenomena (word sense ambi-
guity, contextual effects, negation) and attempts to
address all of them with a single model.
For sense disambiguation, the first step in the
causal chain of generating sentiment, we proposed
</bodyText>
<footnote confidence="0.63238">
3Included in supplementary material.
</footnote>
<bodyText confidence="0.999493538461538">
CESL, a contextually enhanced sentiment lexi-
con that for each word w holds the inventory of
senses of w, polarity annotations of these senses
and a data structure for assigning contexts of w
to the senses. We introduced new features for
sentiment analysis to be able to perform the fine-
grained modeling of context needed for CESL. In
a case study for the word “hard”, we showed that
high accuracy in sentiment disambiguation can be
achieved using our approach. In future work, we
would like to show that our findings generalize
from the case of “hard” to the entire sentiment lex-
icon.
</bodyText>
<sectionHeader confidence="0.998562" genericHeader="acknowledgments">
Acknowledgments
</sectionHeader>
<bodyText confidence="0.997744333333333">
This work was supported by DFG (grant SCHU
2246/10). We thank Lucia Krisnawati and Sascha
Rothe for their help with annotation.
</bodyText>
<sectionHeader confidence="0.998805" genericHeader="references">
References
</sectionHeader>
<reference confidence="0.998234774193548">
Stefano Baccianella, Andrea Esuli, and Fabrizio Sebas-
tiani. 2010. Sentiwordnet 3.0: An enhanced lexical
resource for sentiment analysis and opinion mining.
In International Conference on Language Resources
and Evaluation, pages 2200–2204.
Yoshua Bengio, R´ejean Ducharme, Pascal Vincent, and
Christian Jauvin. 2003. A neural probabilistic lan-
guage model. Journal of Machine Learning Re-
search, 3:1137–1155.
William Blacoe and Mirella Lapata. 2012. A com-
parison of vector-based representations for seman-
tic composition. In Joint Conference on Empirical
Methods in Natural Language Processing and Com-
putational Natural Language Learning, pages 546–
556. Association for Computational Linguistics.
John C. Duchi, Elad Hazan, and Yoram Singer. 2011.
Adaptive subgradient methods for online learning
and stochastic optimization. Journal of Machine
Learning Research, 12:2121–2159.
Rong-En Fan, Kai-Wei Chang, Cho-Jui Hsieh, Xiang-
Rui Wang, and Chih-Jen Lin. 2008. Liblinear: A
library for large linear classification. Journal of Ma-
chine Learning Research, 9:1871–1874.
Nitin Jindal and Bing Liu. 2008. Opinion spam
and analysis. In International Conference on Web
Search and Web Data Mining, pages 219–230.
Andrew L. Maas and Andrew Y. Ng. 2010. A proba-
bilistic model for semantic word vectors. In Annual
Conference on Advances in Neural Information Pro-
cessing Systems: Deep Learning and Unsupervised
Feature Learning Workshop.
</reference>
<page confidence="0.835343">
1214
</page>
<reference confidence="0.999771587301587">
Andrew L. Maas, Raymond E. Daly, Peter T. Pham,
Dan Huang, Andrew Y. Ng, and Christopher Potts.
2011. Learning word vectors for sentiment analysis.
In Annual Meeting of the Association for Computa-
tional Linguistics, pages 142–150.
Andriy Mnih and Koray Kavukcuoglu. 2013. Learning
word embeddings efficiently with noise-contrastive
estimation. In Annual Conference on Advances
in Neural Information Processing Systems, pages
2265–2273.
Sebastian Pad´o, 2006. User’s guide to sigf: Signifi-
cance testing by approximate randomisation.
Bo Pang, Lillian Lee, and Shivakumar Vaithyanathan.
2002. Thumbs up?: Sentiment classification using
machine learning techniques. In Conference on Em-
pirical Methods in Natural Language Processing,
pages 79–86.
Ellen Riloff, Janyce Wiebe, and Theresa Ann Wilson.
2003. Learning subjective nouns using extraction
pattern bootstrapping. In Conference on Natural
Language Learning, volume 4, pages 25–32.
John Sinclair. 1987. Looking Up: Account of the
Cobuild Project in Lexical Computing. Collins
CoBUILD.
Richard Socher, Jeffrey Pennington, Eric H. Huang,
Andrew Y. Ng, and Christopher D. Manning. 2011.
Semi-supervised recursive autoencoders for predict-
ing sentiment distributions. In Conference on Em-
pirical Methods in Natural Language Processing,
pages 151–161.
Richard Socher, Alex Perelygin, Jean Wu, Jason
Chuang, Christopher Manning, Andrew Ng, and
Christopher Potts. 2013. Recursive deep models
for semantic compositionality over a sentiment tree-
bank. In Conference on Empirical Methods in Nat-
ural Language Processing, pages 1631–1642.
Peter D. Turney. 2002. Thumbs up or thumbs down?
semantic orientation applied to unsupervised classi-
fication of reviews. In Annual Meeting of the Asso-
ciation for Computational Linguistics, pages 417–
424.
Casey Whitelaw, Navendu Garg, and Shlomo Arga-
mon. 2005. Using appraisal groups for sentiment
analysis. In International Conference on Informa-
tion and Knowledge Management, pages 625–631.
ACM.
Janyce Wiebe and Rada Mihalcea. 2006. Word sense
and subjectivity. In Annual Meeting of the Asso-
ciation for Computational Linguistics, pages 1065–
1072.
Theresa Wilson, Janyce Wiebe, and Paul Hoffmann.
2009. Recognizing contextual polarity: An explo-
ration of features for phrase-level sentiment analy-
sis. Computational Linguistics, 35(3):399–433.
David Yarowsky. 1992. Word-sense disambiguation
using statistical models of Roget’s categories trained
on large corpora. In International Conference on
Computational Linguistics, pages 454–460.
Hong Yu and Vasileios Hatzivassiloglou. 2003. To-
wards answering opinion questions: Separating facts
from opinions and identifying the polarity of opinion
sentences. In Conference on Empirical Methods in
Natural Language Processing, pages 129–136.
</reference>
<page confidence="0.991">
1215
</page>
</variant>
</algorithm>
<algorithm name="ParsHed" version="110505">
<variant no="0" confidence="0.936794">
<title confidence="0.998273">Fine-Grained Contextual Predictions for Hard Sentiment Words</title>
<author confidence="0.952179">Ebert</author>
<affiliation confidence="0.999426">Center for Information and Language University of Munich,</affiliation>
<abstract confidence="0.998889071428571">We put forward the hypothesis that highaccuracy sentiment analysis is only possible if word senses with different polarity are accurately recognized. We provide evidence for this hypothesis in a case study for the adjective “hard” and propose contextually enhanced sentiment lexicons that contain the information necessary for sentiment-relevant sense disambiguation. An experimental evaluation demonstrates that senses with different polarity can be distinguished well using a combination of standard and novel features.</abstract>
</variant>
</algorithm>
<algorithm name="ParsCit" version="110505">
<citationList>
<citation valid="true">
<authors>
<author>Stefano Baccianella</author>
<author>Andrea Esuli</author>
<author>Fabrizio Sebastiani</author>
</authors>
<title>Sentiwordnet 3.0: An enhanced lexical resource for sentiment analysis and opinion mining.</title>
<date>2010</date>
<booktitle>In International Conference on Language Resources and Evaluation,</booktitle>
<pages>2200--2204</pages>
<contexts>
<context position="6154" citStr="Baccianella et al. (2010)" startWordPosition="998" endWordPosition="1001">(2009) present an approach to classify contextual polarity building on a two-step process. First, they classify if a sentiment word is polar in a phrase and if so, second, they classify its polarity. Our approach can be seen as an extension of this approach; the main difference is that we will show in our analysis of “hard” that the polarity of phrases depends on the senses of the words that are used. This is evidence that highaccuracy polarity classification depends on sense disambiguation. There has been previous work on assigning polarity values to senses of words taken from WordNet (e.g., Baccianella et al. (2010), Wiebe and Mihalcea (2006)). However, these approaches are not able to disambiguate the sense of a word given its context. Previous work on representation learning for sentiment analysis includes (Maas and Ng, 2010) and (Maas et al., 2011). Their models learn word embeddings that capture semantic similarities and word sentiment at the same time. Their approach focuses on sentiment of entire sentences or documents and does not consider each sentiment word instance at a local level. We present experiments with one supervised and one semisupervised approach to word sense disambiguation (WSD) in </context>
</contexts>
<marker>Baccianella, Esuli, Sebastiani, 2010</marker>
<rawString>Stefano Baccianella, Andrea Esuli, and Fabrizio Sebastiani. 2010. Sentiwordnet 3.0: An enhanced lexical resource for sentiment analysis and opinion mining. In International Conference on Language Resources and Evaluation, pages 2200–2204.</rawString>
</citation>
<citation valid="true">
<authors>
<author>Yoshua Bengio</author>
<author>R´ejean Ducharme</author>
<author>Pascal Vincent</author>
<author>Christian Jauvin</author>
</authors>
<title>A neural probabilistic language model.</title>
<date>2003</date>
<journal>Journal of Machine Learning Research,</journal>
<pages>3--1137</pages>
<contexts>
<context position="10960" citStr="Bengio et al. (2003)" startWordPosition="1809" endWordPosition="1812">hich “hard” is neutral (in the meaning ‘firm, solid’) from contexts in which it is a sentiment indicator (in the meaning ‘difficult’). We will use the term predicted context distribution or PCD to refer to the distribution predicted by the LM. We use the vectorized log-bilinear language model (vLBL) (Mnih and Kavukcuoglu, 2013) because it has three appealing features. (i) It learns state of the art word embeddings (Mnih and Kavukcuoglu, 2013). (ii) The model is a language model and can be used to calculate PCDs. (iii) As a linear model, vLBL can be trained much faster than other models (e.g., Bengio et al. (2003)). The vLBL trains one set of word embeddings for the input space (R) and one for the target space (Q). We denote the input representation of word w as rw and the target representation as qw. For a given context c = w1, ... , wn the model predicts a target representation q&amp;quot; by linearly combining the context word representations with position dependent weights: n &amp;quot;q(c) = di O rwi i=1 where di E D is the weight vector associated with position i in the context and O is pointwise multiplication. Given the model parameters θ = {R, Q, D, b} the similarity between q&amp;quot; and the correct target word embed</context>
</contexts>
<marker>Bengio, Ducharme, Vincent, Jauvin, 2003</marker>
<rawString>Yoshua Bengio, R´ejean Ducharme, Pascal Vincent, and Christian Jauvin. 2003. A neural probabilistic language model. Journal of Machine Learning Research, 3:1137–1155.</rawString>
</citation>
<citation valid="true">
<authors>
<author>William Blacoe</author>
<author>Mirella Lapata</author>
</authors>
<title>A comparison of vector-based representations for semantic composition.</title>
<date>2012</date>
<booktitle>In Joint Conference on Empirical Methods in Natural Language Processing and Computational Natural Language Learning,</booktitle>
<pages>546--556</pages>
<publisher>Association</publisher>
<institution>for Computational Linguistics.</institution>
<contexts>
<context position="14129" citStr="Blacoe and Lapata (2012)" startWordPosition="2453" endWordPosition="2456"> 5 $ $ 6 $ $ 7 $ $ * $ $ 8 $ * * $ * $ Table 3: Significant differences of lines 1–8 in Table 2; $: p=0.01, *: p=0.05, ·: p=0.1 The first approach is to use a statistical classification model as the sense disambiguation structure. We use liblinear (Fan et al., 2008) with standard parameters for classification based on three different feature types: ngrams, embeddings (embed) and PCDs. Ngram features are all n-grams for n E 11, 2, 3}. As embedding features we use (i) the mean of the input space (R) embeddings and (ii) the mean of the target space (Q) embeddings of the words in the context (see Blacoe and Lapata (2012) for justification of using simple mean). As PCD features we use the PCD predicted by vLBL for the sentiment word of interest, in our case “hard”. We split the set of 4600 contexts introduced in Section 3 into a training set of 4000 and a development set of 600. Table 2 (lines 1–8) shows the classification results on the development set for all feature type combinations. Significant differences between results – computed using the approximate randomization test (Pad´o, 2006) – are given in Table 3. The majority baseline (bl), which assigns a negative label to all examples, reaches F1 = .76. Th</context>
</contexts>
<marker>Blacoe, Lapata, 2012</marker>
<rawString>William Blacoe and Mirella Lapata. 2012. A comparison of vector-based representations for semantic composition. In Joint Conference on Empirical Methods in Natural Language Processing and Computational Natural Language Learning, pages 546– 556. Association for Computational Linguistics.</rawString>
</citation>
<citation valid="true">
<authors>
<author>John C Duchi</author>
<author>Elad Hazan</author>
<author>Yoram Singer</author>
</authors>
<title>Adaptive subgradient methods for online learning and stochastic optimization.</title>
<date>2011</date>
<journal>Journal of Machine Learning Research,</journal>
<pages>12--2121</pages>
<contexts>
<context position="11865" citStr="Duchi et al., 2011" startWordPosition="1973" endWordPosition="1976"> combining the context word representations with position dependent weights: n &amp;quot;q(c) = di O rwi i=1 where di E D is the weight vector associated with position i in the context and O is pointwise multiplication. Given the model parameters θ = {R, Q, D, b} the similarity between q&amp;quot; and the correct target word embedding is computed by the similarity function sθ(w, c) = &amp;quot;q(c)T qw + bw where bw is a bias term. We train the model with stochastic gradient descent on mini-batches of size 100, following the noise-contrastive estimation training procedure of Mnih and Kavukcuoglu (2013). We use AdaGrad (Duchi et al., 2011) with the initial learning rate set to q = 0.5. The embeddings size is set to 100. 1212 acc prec rec F1 1 .62 .62 1.00 .76 2 + .90 .91 .94 .92 3 + .90 .91 .92 .92 4 + .87 .87 .92 .90 5 + + .92 .92 .94 .93 6 + + .91 .90 .95 .92 7 + + .86 .83 .96 .89 8 + + + .92 .93 .95 .94 9 + .85 .87 .89 .88 10 + .85 .87 .89 .88 11 + .76 .73 .98 .83 12 + + .85 .87 .89 .88 13 + + .85 .87 .89 .88 14 + + .85 .89 .87 .88 15 + + + .86 .87 .90 .89 16 .66 .66 1.00 .80 17 + + + .90 .89 .96 .92 18 + + + .85 .85 .91 .88 Table 2: Classification results; bl: baseline During training we do not need to normalize the similar</context>
</contexts>
<marker>Duchi, Hazan, Singer, 2011</marker>
<rawString>John C. Duchi, Elad Hazan, and Yoram Singer. 2011. Adaptive subgradient methods for online learning and stochastic optimization. Journal of Machine Learning Research, 12:2121–2159.</rawString>
</citation>
<citation valid="true">
<authors>
<author>Rong-En Fan</author>
<author>Kai-Wei Chang</author>
<author>Cho-Jui Hsieh</author>
<author>XiangRui Wang</author>
<author>Chih-Jen Lin</author>
</authors>
<title>Liblinear: A library for large linear classification.</title>
<date>2008</date>
<journal>Journal of Machine Learning Research,</journal>
<pages>9--1871</pages>
<contexts>
<context position="13771" citStr="Fan et al., 2008" startWordPosition="2387" endWordPosition="2390">ts The lexicon entry of “hard” in CESL consists of (i) the senses, (ii) the polarity annotations (neutral or negative) and (iii) the sense disambiguation data structure. Components (i) and (ii) are shown in Table 1. In this section, we evaluate two different options for (iii) on the task of sentiment classification. 1 2 3 4 5 6 7 8 1 2 $ 3 $ 4 $ $ · 5 $ $ 6 $ $ 7 $ $ * $ $ 8 $ * * $ * $ Table 3: Significant differences of lines 1–8 in Table 2; $: p=0.01, *: p=0.05, ·: p=0.1 The first approach is to use a statistical classification model as the sense disambiguation structure. We use liblinear (Fan et al., 2008) with standard parameters for classification based on three different feature types: ngrams, embeddings (embed) and PCDs. Ngram features are all n-grams for n E 11, 2, 3}. As embedding features we use (i) the mean of the input space (R) embeddings and (ii) the mean of the target space (Q) embeddings of the words in the context (see Blacoe and Lapata (2012) for justification of using simple mean). As PCD features we use the PCD predicted by vLBL for the sentiment word of interest, in our case “hard”. We split the set of 4600 contexts introduced in Section 3 into a training set of 4000 and a dev</context>
</contexts>
<marker>Fan, Chang, Hsieh, Wang, Lin, 2008</marker>
<rawString>Rong-En Fan, Kai-Wei Chang, Cho-Jui Hsieh, XiangRui Wang, and Chih-Jen Lin. 2008. Liblinear: A library for large linear classification. Journal of Machine Learning Research, 9:1871–1874.</rawString>
</citation>
<citation valid="true">
<authors>
<author>Nitin Jindal</author>
<author>Bing Liu</author>
</authors>
<title>Opinion spam and analysis.</title>
<date>2008</date>
<booktitle>In International Conference on Web Search and Web Data Mining,</booktitle>
<pages>219--230</pages>
<contexts>
<context position="7022" citStr="Jindal and Liu, 2008" startWordPosition="1136" endWordPosition="1140"> models learn word embeddings that capture semantic similarities and word sentiment at the same time. Their approach focuses on sentiment of entire sentences or documents and does not consider each sentiment word instance at a local level. We present experiments with one supervised and one semisupervised approach to word sense disambiguation (WSD) in this paper. Other WSD approaches, e.g., thesaurus-based WSD (Yarowsky, 1992), could also be used for CESL. 3 Linguistic analysis of sentiment contexts of “hard” We took a random sample of 5000 contexts of “hard” in the Amazon Product Review Data (Jindal and Liu, 2008). We use 200 as a test set and set aside 200 for future use. We analyzed the remaining 4600 contexts using a tool we designed for this study, which provides functionality for selecting and sorting contexts, including a keyword in context display. If a reliable pattern has been identified (e.g., the phrase “die hard”), then all contexts matching the pattern can be labeled automatically. Our goal is to identify the different uses of “hard” that are relevant for sentiment. The basis for our inventory is the Cobuild (Sinclair, 1987) 1211 lexicon entry for “hard”. We use Cobuild because it was comp</context>
</contexts>
<marker>Jindal, Liu, 2008</marker>
<rawString>Nitin Jindal and Bing Liu. 2008. Opinion spam and analysis. In International Conference on Web Search and Web Data Mining, pages 219–230.</rawString>
</citation>
<citation valid="true">
<authors>
<author>Andrew L Maas</author>
<author>Andrew Y Ng</author>
</authors>
<title>A probabilistic model for semantic word vectors.</title>
<date>2010</date>
<booktitle>In Annual Conference on Advances in Neural Information Processing Systems: Deep Learning and Unsupervised Feature Learning Workshop.</booktitle>
<contexts>
<context position="6370" citStr="Maas and Ng, 2010" startWordPosition="1032" endWordPosition="1035">n as an extension of this approach; the main difference is that we will show in our analysis of “hard” that the polarity of phrases depends on the senses of the words that are used. This is evidence that highaccuracy polarity classification depends on sense disambiguation. There has been previous work on assigning polarity values to senses of words taken from WordNet (e.g., Baccianella et al. (2010), Wiebe and Mihalcea (2006)). However, these approaches are not able to disambiguate the sense of a word given its context. Previous work on representation learning for sentiment analysis includes (Maas and Ng, 2010) and (Maas et al., 2011). Their models learn word embeddings that capture semantic similarities and word sentiment at the same time. Their approach focuses on sentiment of entire sentences or documents and does not consider each sentiment word instance at a local level. We present experiments with one supervised and one semisupervised approach to word sense disambiguation (WSD) in this paper. Other WSD approaches, e.g., thesaurus-based WSD (Yarowsky, 1992), could also be used for CESL. 3 Linguistic analysis of sentiment contexts of “hard” We took a random sample of 5000 contexts of “hard” in t</context>
</contexts>
<marker>Maas, Ng, 2010</marker>
<rawString>Andrew L. Maas and Andrew Y. Ng. 2010. A probabilistic model for semantic word vectors. In Annual Conference on Advances in Neural Information Processing Systems: Deep Learning and Unsupervised Feature Learning Workshop.</rawString>
</citation>
<citation valid="true">
<authors>
<author>Andrew L Maas</author>
<author>Raymond E Daly</author>
<author>Peter T Pham</author>
<author>Dan Huang</author>
<author>Andrew Y Ng</author>
<author>Christopher Potts</author>
</authors>
<title>Learning word vectors for sentiment analysis.</title>
<date>2011</date>
<booktitle>In Annual Meeting of the Association for Computational Linguistics,</booktitle>
<pages>142--150</pages>
<contexts>
<context position="6394" citStr="Maas et al., 2011" startWordPosition="1037" endWordPosition="1040">s approach; the main difference is that we will show in our analysis of “hard” that the polarity of phrases depends on the senses of the words that are used. This is evidence that highaccuracy polarity classification depends on sense disambiguation. There has been previous work on assigning polarity values to senses of words taken from WordNet (e.g., Baccianella et al. (2010), Wiebe and Mihalcea (2006)). However, these approaches are not able to disambiguate the sense of a word given its context. Previous work on representation learning for sentiment analysis includes (Maas and Ng, 2010) and (Maas et al., 2011). Their models learn word embeddings that capture semantic similarities and word sentiment at the same time. Their approach focuses on sentiment of entire sentences or documents and does not consider each sentiment word instance at a local level. We present experiments with one supervised and one semisupervised approach to word sense disambiguation (WSD) in this paper. Other WSD approaches, e.g., thesaurus-based WSD (Yarowsky, 1992), could also be used for CESL. 3 Linguistic analysis of sentiment contexts of “hard” We took a random sample of 5000 contexts of “hard” in the Amazon Product Review</context>
</contexts>
<marker>Maas, Daly, Pham, Huang, Ng, Potts, 2011</marker>
<rawString>Andrew L. Maas, Raymond E. Daly, Peter T. Pham, Dan Huang, Andrew Y. Ng, and Christopher Potts. 2011. Learning word vectors for sentiment analysis. In Annual Meeting of the Association for Computational Linguistics, pages 142–150.</rawString>
</citation>
<citation valid="true">
<authors>
<author>Andriy Mnih</author>
<author>Koray Kavukcuoglu</author>
</authors>
<title>Learning word embeddings efficiently with noise-contrastive estimation.</title>
<date>2013</date>
<booktitle>In Annual Conference on Advances in Neural Information Processing Systems,</booktitle>
<pages>2265--2273</pages>
<contexts>
<context position="10669" citStr="Mnih and Kavukcuoglu, 2013" startWordPosition="1757" endWordPosition="1760">n at which the word of interest occurs. For example, an LM will predict that words like “granite” and “concrete” are likely in the context “a * countertop” and that words like “serious” and “difficult” are likely in the context “a * problem”. This is then the basis for distinguishing contexts in which “hard” is neutral (in the meaning ‘firm, solid’) from contexts in which it is a sentiment indicator (in the meaning ‘difficult’). We will use the term predicted context distribution or PCD to refer to the distribution predicted by the LM. We use the vectorized log-bilinear language model (vLBL) (Mnih and Kavukcuoglu, 2013) because it has three appealing features. (i) It learns state of the art word embeddings (Mnih and Kavukcuoglu, 2013). (ii) The model is a language model and can be used to calculate PCDs. (iii) As a linear model, vLBL can be trained much faster than other models (e.g., Bengio et al. (2003)). The vLBL trains one set of word embeddings for the input space (R) and one for the target space (Q). We denote the input representation of word w as rw and the target representation as qw. For a given context c = w1, ... , wn the model predicts a target representation q&amp;quot; by linearly combining the context </context>
</contexts>
<marker>Mnih, Kavukcuoglu, 2013</marker>
<rawString>Andriy Mnih and Koray Kavukcuoglu. 2013. Learning word embeddings efficiently with noise-contrastive estimation. In Annual Conference on Advances in Neural Information Processing Systems, pages 2265–2273.</rawString>
</citation>
<citation valid="true">
<authors>
<author>Sebastian Pad´o</author>
</authors>
<title>User’s guide to sigf: Significance testing by approximate randomisation.</title>
<date>2006</date>
<marker>Pad´o, 2006</marker>
<rawString>Sebastian Pad´o, 2006. User’s guide to sigf: Significance testing by approximate randomisation.</rawString>
</citation>
<citation valid="true">
<authors>
<author>Bo Pang</author>
<author>Lillian Lee</author>
<author>Shivakumar Vaithyanathan</author>
</authors>
<title>Thumbs up?: Sentiment classification using machine learning techniques.</title>
<date>2002</date>
<booktitle>In Conference on Empirical Methods in Natural Language Processing,</booktitle>
<pages>79--86</pages>
<contexts>
<context position="3144" citStr="Pang et al. (2002)" startWordPosition="480" endWordPosition="483">periment with deep learning as a source of such features. We look at two types of deep learning features: word embeddings and neural network language model predictions (Section 4). We show that deep learning features significantly improve the accuracy of context-dependent polarity classification (Section 5). 2 Related work Initial work on sentiment analysis was either based on sentiment lexicons that listed words as positive or negative sentiment indicators (e.g., Turney (2002), Yu and Hatzivassiloglou (2003)), on statistical classification approaches that represent documents as ngrams (e.g., Pang et al. (2002)) or on a combination of both (e.g., Riloff et al. (2003), Whitelaw et al. (2005)). The underlying assumption of lexicon-based sentiment analysis is that a word always has the same sentiment. This is clearly wrong because words can have senses with different polarity, e.g., “hard wood” (neutral) vs. “hard memory” (negative). Ngram approaches are also limited because ngram representations are not a good basis for relevant generalizations. For example, the neutral adverbial sense ‘intense’ of “hard” (“laugh hard”, “try hard”) vs. the negative adjectival mean1210 Proceedings of the 2014 Conferenc</context>
</contexts>
<marker>Pang, Lee, Vaithyanathan, 2002</marker>
<rawString>Bo Pang, Lillian Lee, and Shivakumar Vaithyanathan. 2002. Thumbs up?: Sentiment classification using machine learning techniques. In Conference on Empirical Methods in Natural Language Processing, pages 79–86.</rawString>
</citation>
<citation valid="true">
<authors>
<author>Ellen Riloff</author>
<author>Janyce Wiebe</author>
<author>Theresa Ann Wilson</author>
</authors>
<title>Learning subjective nouns using extraction pattern bootstrapping.</title>
<date>2003</date>
<booktitle>In Conference on Natural Language Learning,</booktitle>
<volume>4</volume>
<pages>25--32</pages>
<contexts>
<context position="3201" citStr="Riloff et al. (2003)" startWordPosition="491" endWordPosition="494">s. We look at two types of deep learning features: word embeddings and neural network language model predictions (Section 4). We show that deep learning features significantly improve the accuracy of context-dependent polarity classification (Section 5). 2 Related work Initial work on sentiment analysis was either based on sentiment lexicons that listed words as positive or negative sentiment indicators (e.g., Turney (2002), Yu and Hatzivassiloglou (2003)), on statistical classification approaches that represent documents as ngrams (e.g., Pang et al. (2002)) or on a combination of both (e.g., Riloff et al. (2003), Whitelaw et al. (2005)). The underlying assumption of lexicon-based sentiment analysis is that a word always has the same sentiment. This is clearly wrong because words can have senses with different polarity, e.g., “hard wood” (neutral) vs. “hard memory” (negative). Ngram approaches are also limited because ngram representations are not a good basis for relevant generalizations. For example, the neutral adverbial sense ‘intense’ of “hard” (“laugh hard”, “try hard”) vs. the negative adjectival mean1210 Proceedings of the 2014 Conference on Empirical Methods in Natural Language Processing (EM</context>
</contexts>
<marker>Riloff, Wiebe, Wilson, 2003</marker>
<rawString>Ellen Riloff, Janyce Wiebe, and Theresa Ann Wilson. 2003. Learning subjective nouns using extraction pattern bootstrapping. In Conference on Natural Language Learning, volume 4, pages 25–32.</rawString>
</citation>
<citation valid="true">
<authors>
<author>John Sinclair</author>
</authors>
<title>Looking Up: Account of the Cobuild Project in Lexical Computing. Collins CoBUILD.</title>
<date>1987</date>
<contexts>
<context position="7556" citStr="Sinclair, 1987" startWordPosition="1231" endWordPosition="1232"> 5000 contexts of “hard” in the Amazon Product Review Data (Jindal and Liu, 2008). We use 200 as a test set and set aside 200 for future use. We analyzed the remaining 4600 contexts using a tool we designed for this study, which provides functionality for selecting and sorting contexts, including a keyword in context display. If a reliable pattern has been identified (e.g., the phrase “die hard”), then all contexts matching the pattern can be labeled automatically. Our goal is to identify the different uses of “hard” that are relevant for sentiment. The basis for our inventory is the Cobuild (Sinclair, 1987) 1211 lexicon entry for “hard”. We use Cobuild because it was compiled based on an empirical analysis of corpus data and is therefore more likely to satisfy the requirements of NLP applications than a traditional dictionary. Cobuild lists 16 senses. One of these senses (3) is split into two to distinguish the adverbial (“to accelerate hard”) and adjectival (“hard acceleration”) uses of “hard” in the meaning ‘intense’. We conflated five senses (2, 4, 9, 10, 11) referring to different types of difficulty: “hard question” (2), “hard work” (4), “hard life” (11) and two variants of “hard on”: “hard</context>
</contexts>
<marker>Sinclair, 1987</marker>
<rawString>John Sinclair. 1987. Looking Up: Account of the Cobuild Project in Lexical Computing. Collins CoBUILD.</rawString>
</citation>
<citation valid="true">
<authors>
<author>Richard Socher</author>
<author>Jeffrey Pennington</author>
<author>Eric H Huang</author>
<author>Andrew Y Ng</author>
<author>Christopher D Manning</author>
</authors>
<title>Semi-supervised recursive autoencoders for predicting sentiment distributions.</title>
<date>2011</date>
<booktitle>In Conference on Empirical Methods in Natural Language Processing,</booktitle>
<pages>151--161</pages>
<contexts>
<context position="4922" citStr="Socher et al. (2011)" startWordPosition="784" endWordPosition="787">ely hard truth neu 5 6 ADJ true 7 MUSIC ADJ hard-rock- hard beats neu 347 15 type music 8 CONTRAST ADJ opposite of hard edge neu 3 1 soft transition 9 NEGATIVE-P 13, 15 phrases neg 36 2 10 NEUTRAL-P 14, 16 phrases neu 375 27 Table 1: Sense inventory of “hard”. ing ‘difficult’ (“hard life”, “hard memory”) cannot be easily distinguished based on an ngram representation. Moreover, although ngram approaches could learn the polarity of these phrases they do not generalize to new phrases. More recent compositional approaches to sentiment analysis can outperform lexicon and ngrambased methods (e.g., Socher et al. (2011), Socher et al. (2013)). However, these approaches conflate two different types of contextual effects: differences in sense or lexical meaning (“hard memory” vs. “hard wood”) on the one hand and meaning composition like negation on the other hand. From the point of view of linguistic theory, these are different types of contextual effects that should not be conflated. Recognizing that “hard” occurs in the scope of negation is of no use if the basic polarity of the contextually evoked sense of “hard” (e.g., negative in “no hard memories” vs. neutral in “no hard wood”) is not recognized. Wilson </context>
</contexts>
<marker>Socher, Pennington, Huang, Ng, Manning, 2011</marker>
<rawString>Richard Socher, Jeffrey Pennington, Eric H. Huang, Andrew Y. Ng, and Christopher D. Manning. 2011. Semi-supervised recursive autoencoders for predicting sentiment distributions. In Conference on Empirical Methods in Natural Language Processing, pages 151–161.</rawString>
</citation>
<citation valid="true">
<authors>
<author>Richard Socher</author>
<author>Alex Perelygin</author>
<author>Jean Wu</author>
<author>Jason Chuang</author>
<author>Christopher Manning</author>
<author>Andrew Ng</author>
<author>Christopher Potts</author>
</authors>
<title>Recursive deep models for semantic compositionality over a sentiment treebank.</title>
<date>2013</date>
<booktitle>In Conference on Empirical Methods in Natural Language Processing,</booktitle>
<pages>1631--1642</pages>
<contexts>
<context position="4944" citStr="Socher et al. (2013)" startWordPosition="788" endWordPosition="791"> ADJ true 7 MUSIC ADJ hard-rock- hard beats neu 347 15 type music 8 CONTRAST ADJ opposite of hard edge neu 3 1 soft transition 9 NEGATIVE-P 13, 15 phrases neg 36 2 10 NEUTRAL-P 14, 16 phrases neu 375 27 Table 1: Sense inventory of “hard”. ing ‘difficult’ (“hard life”, “hard memory”) cannot be easily distinguished based on an ngram representation. Moreover, although ngram approaches could learn the polarity of these phrases they do not generalize to new phrases. More recent compositional approaches to sentiment analysis can outperform lexicon and ngrambased methods (e.g., Socher et al. (2011), Socher et al. (2013)). However, these approaches conflate two different types of contextual effects: differences in sense or lexical meaning (“hard memory” vs. “hard wood”) on the one hand and meaning composition like negation on the other hand. From the point of view of linguistic theory, these are different types of contextual effects that should not be conflated. Recognizing that “hard” occurs in the scope of negation is of no use if the basic polarity of the contextually evoked sense of “hard” (e.g., negative in “no hard memories” vs. neutral in “no hard wood”) is not recognized. Wilson et al. (2009) present </context>
</contexts>
<marker>Socher, Perelygin, Wu, Chuang, Manning, Ng, Potts, 2013</marker>
<rawString>Richard Socher, Alex Perelygin, Jean Wu, Jason Chuang, Christopher Manning, Andrew Ng, and Christopher Potts. 2013. Recursive deep models for semantic compositionality over a sentiment treebank. In Conference on Empirical Methods in Natural Language Processing, pages 1631–1642.</rawString>
</citation>
<citation valid="true">
<authors>
<author>Peter D Turney</author>
</authors>
<title>Thumbs up or thumbs down? semantic orientation applied to unsupervised classification of reviews.</title>
<date>2002</date>
<booktitle>In Annual Meeting of the Association for Computational Linguistics,</booktitle>
<pages>417--424</pages>
<contexts>
<context position="3008" citStr="Turney (2002)" startWordPosition="462" endWordPosition="463">, then powerful contextual features are necessary to support making fine-grained distinctions. Our third contribution is that we experiment with deep learning as a source of such features. We look at two types of deep learning features: word embeddings and neural network language model predictions (Section 4). We show that deep learning features significantly improve the accuracy of context-dependent polarity classification (Section 5). 2 Related work Initial work on sentiment analysis was either based on sentiment lexicons that listed words as positive or negative sentiment indicators (e.g., Turney (2002), Yu and Hatzivassiloglou (2003)), on statistical classification approaches that represent documents as ngrams (e.g., Pang et al. (2002)) or on a combination of both (e.g., Riloff et al. (2003), Whitelaw et al. (2005)). The underlying assumption of lexicon-based sentiment analysis is that a word always has the same sentiment. This is clearly wrong because words can have senses with different polarity, e.g., “hard wood” (neutral) vs. “hard memory” (negative). Ngram approaches are also limited because ngram representations are not a good basis for relevant generalizations. For example, the neutr</context>
</contexts>
<marker>Turney, 2002</marker>
<rawString>Peter D. Turney. 2002. Thumbs up or thumbs down? semantic orientation applied to unsupervised classification of reviews. In Annual Meeting of the Association for Computational Linguistics, pages 417– 424.</rawString>
</citation>
<citation valid="true">
<authors>
<author>Casey Whitelaw</author>
<author>Navendu Garg</author>
<author>Shlomo Argamon</author>
</authors>
<title>Using appraisal groups for sentiment analysis.</title>
<date>2005</date>
<booktitle>In International Conference on Information and Knowledge Management,</booktitle>
<pages>625--631</pages>
<publisher>ACM.</publisher>
<contexts>
<context position="3225" citStr="Whitelaw et al. (2005)" startWordPosition="495" endWordPosition="498">s of deep learning features: word embeddings and neural network language model predictions (Section 4). We show that deep learning features significantly improve the accuracy of context-dependent polarity classification (Section 5). 2 Related work Initial work on sentiment analysis was either based on sentiment lexicons that listed words as positive or negative sentiment indicators (e.g., Turney (2002), Yu and Hatzivassiloglou (2003)), on statistical classification approaches that represent documents as ngrams (e.g., Pang et al. (2002)) or on a combination of both (e.g., Riloff et al. (2003), Whitelaw et al. (2005)). The underlying assumption of lexicon-based sentiment analysis is that a word always has the same sentiment. This is clearly wrong because words can have senses with different polarity, e.g., “hard wood” (neutral) vs. “hard memory” (negative). Ngram approaches are also limited because ngram representations are not a good basis for relevant generalizations. For example, the neutral adverbial sense ‘intense’ of “hard” (“laugh hard”, “try hard”) vs. the negative adjectival mean1210 Proceedings of the 2014 Conference on Empirical Methods in Natural Language Processing (EMNLP), pages 1210–1215, O</context>
</contexts>
<marker>Whitelaw, Garg, Argamon, 2005</marker>
<rawString>Casey Whitelaw, Navendu Garg, and Shlomo Argamon. 2005. Using appraisal groups for sentiment analysis. In International Conference on Information and Knowledge Management, pages 625–631. ACM.</rawString>
</citation>
<citation valid="true">
<authors>
<author>Janyce Wiebe</author>
<author>Rada Mihalcea</author>
</authors>
<title>Word sense and subjectivity.</title>
<date>2006</date>
<booktitle>In Annual Meeting of the Association for Computational Linguistics,</booktitle>
<pages>1065--1072</pages>
<contexts>
<context position="6181" citStr="Wiebe and Mihalcea (2006)" startWordPosition="1002" endWordPosition="1006">to classify contextual polarity building on a two-step process. First, they classify if a sentiment word is polar in a phrase and if so, second, they classify its polarity. Our approach can be seen as an extension of this approach; the main difference is that we will show in our analysis of “hard” that the polarity of phrases depends on the senses of the words that are used. This is evidence that highaccuracy polarity classification depends on sense disambiguation. There has been previous work on assigning polarity values to senses of words taken from WordNet (e.g., Baccianella et al. (2010), Wiebe and Mihalcea (2006)). However, these approaches are not able to disambiguate the sense of a word given its context. Previous work on representation learning for sentiment analysis includes (Maas and Ng, 2010) and (Maas et al., 2011). Their models learn word embeddings that capture semantic similarities and word sentiment at the same time. Their approach focuses on sentiment of entire sentences or documents and does not consider each sentiment word instance at a local level. We present experiments with one supervised and one semisupervised approach to word sense disambiguation (WSD) in this paper. Other WSD appro</context>
</contexts>
<marker>Wiebe, Mihalcea, 2006</marker>
<rawString>Janyce Wiebe and Rada Mihalcea. 2006. Word sense and subjectivity. In Annual Meeting of the Association for Computational Linguistics, pages 1065– 1072.</rawString>
</citation>
<citation valid="true">
<authors>
<author>Theresa Wilson</author>
<author>Janyce Wiebe</author>
<author>Paul Hoffmann</author>
</authors>
<title>Recognizing contextual polarity: An exploration of features for phrase-level sentiment analysis.</title>
<date>2009</date>
<journal>Computational Linguistics,</journal>
<volume>35</volume>
<issue>3</issue>
<contexts>
<context position="5535" citStr="Wilson et al. (2009)" startWordPosition="888" endWordPosition="891"> (2011), Socher et al. (2013)). However, these approaches conflate two different types of contextual effects: differences in sense or lexical meaning (“hard memory” vs. “hard wood”) on the one hand and meaning composition like negation on the other hand. From the point of view of linguistic theory, these are different types of contextual effects that should not be conflated. Recognizing that “hard” occurs in the scope of negation is of no use if the basic polarity of the contextually evoked sense of “hard” (e.g., negative in “no hard memories” vs. neutral in “no hard wood”) is not recognized. Wilson et al. (2009) present an approach to classify contextual polarity building on a two-step process. First, they classify if a sentiment word is polar in a phrase and if so, second, they classify its polarity. Our approach can be seen as an extension of this approach; the main difference is that we will show in our analysis of “hard” that the polarity of phrases depends on the senses of the words that are used. This is evidence that highaccuracy polarity classification depends on sense disambiguation. There has been previous work on assigning polarity values to senses of words taken from WordNet (e.g., Baccia</context>
</contexts>
<marker>Wilson, Wiebe, Hoffmann, 2009</marker>
<rawString>Theresa Wilson, Janyce Wiebe, and Paul Hoffmann. 2009. Recognizing contextual polarity: An exploration of features for phrase-level sentiment analysis. Computational Linguistics, 35(3):399–433.</rawString>
</citation>
<citation valid="true">
<authors>
<author>David Yarowsky</author>
</authors>
<title>Word-sense disambiguation using statistical models of Roget’s categories trained on large corpora.</title>
<date>1992</date>
<booktitle>In International Conference on Computational Linguistics,</booktitle>
<pages>454--460</pages>
<contexts>
<context position="6830" citStr="Yarowsky, 1992" startWordPosition="1104" endWordPosition="1105">ot able to disambiguate the sense of a word given its context. Previous work on representation learning for sentiment analysis includes (Maas and Ng, 2010) and (Maas et al., 2011). Their models learn word embeddings that capture semantic similarities and word sentiment at the same time. Their approach focuses on sentiment of entire sentences or documents and does not consider each sentiment word instance at a local level. We present experiments with one supervised and one semisupervised approach to word sense disambiguation (WSD) in this paper. Other WSD approaches, e.g., thesaurus-based WSD (Yarowsky, 1992), could also be used for CESL. 3 Linguistic analysis of sentiment contexts of “hard” We took a random sample of 5000 contexts of “hard” in the Amazon Product Review Data (Jindal and Liu, 2008). We use 200 as a test set and set aside 200 for future use. We analyzed the remaining 4600 contexts using a tool we designed for this study, which provides functionality for selecting and sorting contexts, including a keyword in context display. If a reliable pattern has been identified (e.g., the phrase “die hard”), then all contexts matching the pattern can be labeled automatically. Our goal is to iden</context>
</contexts>
<marker>Yarowsky, 1992</marker>
<rawString>David Yarowsky. 1992. Word-sense disambiguation using statistical models of Roget’s categories trained on large corpora. In International Conference on Computational Linguistics, pages 454–460.</rawString>
</citation>
<citation valid="true">
<authors>
<author>Hong Yu</author>
<author>Vasileios Hatzivassiloglou</author>
</authors>
<title>Towards answering opinion questions: Separating facts from opinions and identifying the polarity of opinion sentences.</title>
<date>2003</date>
<booktitle>In Conference on Empirical Methods in Natural Language Processing,</booktitle>
<pages>129--136</pages>
<contexts>
<context position="3040" citStr="Yu and Hatzivassiloglou (2003)" startWordPosition="464" endWordPosition="467"> contextual features are necessary to support making fine-grained distinctions. Our third contribution is that we experiment with deep learning as a source of such features. We look at two types of deep learning features: word embeddings and neural network language model predictions (Section 4). We show that deep learning features significantly improve the accuracy of context-dependent polarity classification (Section 5). 2 Related work Initial work on sentiment analysis was either based on sentiment lexicons that listed words as positive or negative sentiment indicators (e.g., Turney (2002), Yu and Hatzivassiloglou (2003)), on statistical classification approaches that represent documents as ngrams (e.g., Pang et al. (2002)) or on a combination of both (e.g., Riloff et al. (2003), Whitelaw et al. (2005)). The underlying assumption of lexicon-based sentiment analysis is that a word always has the same sentiment. This is clearly wrong because words can have senses with different polarity, e.g., “hard wood” (neutral) vs. “hard memory” (negative). Ngram approaches are also limited because ngram representations are not a good basis for relevant generalizations. For example, the neutral adverbial sense ‘intense’ of </context>
</contexts>
<marker>Yu, Hatzivassiloglou, 2003</marker>
<rawString>Hong Yu and Vasileios Hatzivassiloglou. 2003. Towards answering opinion questions: Separating facts from opinions and identifying the polarity of opinion sentences. In Conference on Empirical Methods in Natural Language Processing, pages 129–136.</rawString>
</citation>
</citationList>
</algorithm>
</algorithms>
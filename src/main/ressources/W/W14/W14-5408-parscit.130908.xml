<?xml version="1.0" encoding="UTF-8"?>
<algorithms version="110505">
<algorithm name="SectLabel" version="110505">
<variant no="0" confidence="0.001226">
<title confidence="0.998486">
Twitter User Gender Inference Using Combined Analysis
of Text and Image Processing
</title>
<author confidence="0.988806">
Shigeyuki Sakaki, Yasuhide Miura, Xiaojun Ma, Keigo Hattori, and Tomoko Ohkuma
</author>
<affiliation confidence="0.968621">
Fuji Xerox Co., Ltd. / Japan
</affiliation>
<address confidence="0.762872">
6-1, Minatomirai, Nishi-ku, Yokohama-shi, Kanagawa
{sakaki.shigeyuki, yasuhide.miura, xiaojun.ma, keigo.hattori,
</address>
<email confidence="0.994969">
ohkuma.tomoko}@fujixerox.co.jp
</email>
<sectionHeader confidence="0.993609" genericHeader="abstract">
Abstract
</sectionHeader>
<bodyText confidence="0.985464285714286">
Profile inference of SNS users is valuable for marketing, target advertisement, and opinion polls. Sev-
eral studies examining profile inference have been reported to date. Although information of various
types is included in SNS, most such studies only use text information. It is expected that incorporating
information of other types into text classifiers can provide more accurate profile inference. As de-
scribed in this paper, we propose combined method of text processing and image processing to im-
prove gender inference accuracy. By applying the simple formula to combine two results derived from
a text processor and an image processor, significantly increased accuracy was confirmed.
</bodyText>
<sectionHeader confidence="0.999237" genericHeader="keywords">
1 Introduction
</sectionHeader>
<bodyText confidence="0.999965619047619">
Recently, several researches on profile inference of Social Networking Services (SNS) user conducted
by analyzing postings have been reported (Rao and Yarowsky, 2010; Han et al., 2013; Makazhanov et
al., 2013). User profile information such as gender, age, residential area, and political preference have
attracted attention because they are helpful for marketing, target advertisement, TV viewer rate calcu-
lations, and opinion polls. The major approach to this subject is building a machine learning classifier
trained by text in postings. However, images posted by a user are rarely used in profile inference. Im-
ages in postings also include features of user profiles. For example, if a user posts many dessert imag-
es, then the user might be female. Therefore, we assumed that highly accurate profile inference will be
available by analyzing image information and text information simultaneously.
As described in this paper, we implement gender inference of Japanese Twitter user using text in-
formation and image information. We propose a combined method consisting of text processing and
image processing, which accepts tweets as input data and outputs a gender probability score. The
combined method comprises of two steps: step 1) two gender probability scores are inferred respec-
tively by a text processor and an image processor; step 2) the combined score is calculated by merging
two gender scores with an appropriate ratio. This report is the first describing an attempt to apply the
combined method of text processing and image processing to profile inference of an SNS user.
This paper is presented as seven sections: section 2 presents a description of prior work; section 3
presents a description of the annotation data prepared for this study; section 4 introduces the proposed
method; section 5 explains preliminary experiments for optimizing the combined method parameter;
section 6 presents the experimentally obtained result; section 7 summarizes the paper and discusses
future work.
</bodyText>
<sectionHeader confidence="0.997527" genericHeader="introduction">
2 Prior Work
</sectionHeader>
<bodyText confidence="0.999648833333333">
Many reports have described studies examining gender inference. The conventional approach to
this theme is building a machine learning classifier such as Support Vector Machine (SVM) trained by
text features (Burger et al., 2011; Liu et al., 2012). Most of these studies specifically examine im-
provement of the machine classification methodology rather than expanding features or combining
features. Different from these studies, Liu et al. (2013) implemented gender inference with incorpora-
tion of a user name into the classifier based on text information. However, the expansion of features
</bodyText>
<footnote confidence="0.504551333333333">
This work is licensed under a Creative Commons Attribution 4.0 International License. Page numbers and pro-
ceedings footer are added by the organisers. License details: http://creativecommons.org/licenses
/by/4.0/
</footnote>
<page confidence="0.976579">
54
</page>
<note confidence="0.98336">
Proceedings of the 25th International Conference on Computational Linguistics, pages 54–61,
Dublin, Ireland, August 23-29 2014.
</note>
<bodyText confidence="0.999101090909091">
remains in the text field.
A few reports in the literature describe studies of systems that infer the SNS user gender with in-
formation aside from the text. Ikeda et al. (2013) leverages the accuracy of profile inference based on a
text feature classifier by combining user cluster information. According to their study, the accuracy of
classification that deals only with the user cluster is lower than that of the text classifier. The classifier
using both text and cluster information of a user outperforms their text classifier. This research shows
that information aside from the text is useful to leverage the performance of profile inference based on
text and text information is necessary to achieve high accuracy. However, we introduce image infor-
mation that is not used by Ikeda et al (2013).
Along with text information and cluster information, images are popular informative elements that
are included in SNS postings. An image includes enough information to infer what is printed in itself,
and researches to automatically annotate an image with semantic labels are already known (Zhang et
al., 2012). Automatic image annotation is a machine learning technique that involves a process by
which a computer system automatically assigns semantic labels to a digital image. These studies suc-
ceeded in inferences of various objects, such as person, dog, bicycle, chair etc. We supposed that such
objects in images posted by a user should be useful clues as to a profile inference of a twitter user. As
a matter of fact, gender inference by image information is reported by Ma et al. (2014), which imple-
mented gender inference by processing images in tweets. Their study, which ignored text information,
exhibited accuracy of less than 70%. It was much lower than most gender inference work using text
feature.
From results of these studies, we concluded that gender inference by text and image information
invites further study.
</bodyText>
<sectionHeader confidence="0.991349" genericHeader="method">
3 Proposed Method
</sectionHeader>
<bodyText confidence="0.991868666666667">
Our proposed method for combining text processing and image processing is presented in Figure 1.
First, data of 200 tweets of a user are separated into text data and image data. Each of separated data is
analyzed using a dedicated processor, a text processor, and an image processor. Both of the processors
</bodyText>
<figure confidence="0.99320895">
Text Processor Image Processor
200 tweets
posted by a user
Image
text
Object Classifiers
classifiers classifiers classifiers Pet-female
classifiers
Food-male Food-female Toy male
Object probability scores
Food-male Food-female image Toy-male image Pet-female image
probability probability probability probability
score of an image score of an score of an
score of an
Consolidation of scores
Gender probability score of a user
Gender probability
score of a user
Gender classifier
Combined method
</figure>
<figureCaption confidence="0.999108">
Figure 1. Combined method constitution.
</figureCaption>
<footnote confidence="0.231551">
Combined gender probability score of a user
</footnote>
<page confidence="0.994583">
55
</page>
<bodyText confidence="0.9981305">
output a user’s gender probability score, the upper/lower ends of which respectively correspond to
male and female labels. At the end of this process, the combined gender probability score is calculated
using two probability scores. In this section, details of the two processors and the method of combin-
ing their two results are described.
</bodyText>
<subsectionHeader confidence="0.9985">
3.1 Text Processing
</subsectionHeader>
<bodyText confidence="0.999939">
The text processor is constructed from a text classifier, which accepts text data in tweets and outputs
the gender probability score of a user. We defined the gender classifier in the text processor as an
SVM binary classification of a male and female. The SVM classifier is trained based on unigram Bag-
of-words with a linear kernel. The cost parameter C is set to 1.0. Then LIBSVM (Chang and Lin,
2001) is used as an implementation of SVM. Because words are not divided by spaces in a Japanese
sentence, Kuromoji (Atilika, 2011), a morphological analysis program for Japanese, is used to obtain
unigrams.
To combine two results from the text processor and the image processor, it is necessary to calculate
each result as a probability value. To retrieve probability scores, we used logistic regression. Logistic
function converts a distance from a hyper plane to probability scores of 0.0–1.0. The text classifier is a
male and female binary classification. Therefore, the upper and lower ends of the probability score
respectively correspond to male and female data. If a score is close to 0.0, then the user has high prob-
ability of being male. If it is close to 1.0, then a user is probably female.
</bodyText>
<subsectionHeader confidence="0.99741">
3.2 Image Processing
</subsectionHeader>
<bodyText confidence="0.999986896551724">
We first tried to infer a Twitter user gender directly by a two-class classifier trained by image feature
vector calculated by all images posted by a user. However, with some preliminary experiments, we
found that this approach does not work well, since the large variation of objects made the classifica-
tion difficult with single classifier setting. We, therefore, used the image processing method described
by Ma et al. (2014) which uses automatic image annotation classifiers (Zhang et al., 2012) to model
human recognition of different gender tendency in images. The method consists of two steps: step 1)
annotating images by an image annotation technique at the image level; step 2) consolidating gender
scores according to annotation results at the user level.
In the first step, the image labels are defined as the combination of the following two information:
the gender tendency in images of a user and the objects that images express. Ma et al. (2014) defined
10 categories of objects in SNS images based on observation on a real dataset. The defined labels are
cartoon/illustration, famous person, food, goods, memo/leaflet, outdoor/nature, person, pet, screen-
shot/capture, and other. They also indicated that gender tendency in images are coherent with user
gender, and set three gender labels, male, female, and unknown, for each object label. As a result, 30
labels constructed from object label and gender label (e.g. “male-person”) are used in this paper,
which is described in section 4.2. Then a bag-of-features (BOF) model (Tsai, 2012; Chatfield et al.,
2011) is applied to accomplish the image annotation task. We used local descriptors of SIFT (Lowe,
1999) and image features are encoded with a k-mean generated codebook with size of 2000. We ap-
plied LLC (Wang et al, 2010) and SPM (Lazebnik et al, 2006) to generating the final presentation of
image features. Then, the 30 SVM classifiers are trained based on the features of training images: each
classifier is trained per image label among one-versus-rest strategy. The SVM classifier annotates im-
ages of a user by computing scores, and logistic function is applied to the outputs of the image classi-
fiers in order to obtain probability scores. Each of 30 probability scores shows how an image is close
to the decision boundary of a particular label.
In the second step, we integrated the 30 scores of labels assigned to images to yield comprehensive
scores which imply a user’s gender. Two methods are suggested for the second step. One is computing
the average of all scores output from each classifier for each of the categories of male and female for a
user. The other is computing the mean value of only the highest scores of every image for each of the
categories of male and female for a user.
</bodyText>
<subsectionHeader confidence="0.995857">
3.3 Combined method of Text Processing and Image Processing
</subsectionHeader>
<bodyText confidence="0.9993215">
To combine two results derived from text processing and image processing, we used the function be-
low. Scoretext and Scoreimage respectively represent gender probability scores derived from the text pro-
</bodyText>
<page confidence="0.980626">
56
</page>
<bodyText confidence="0.999982">
cessor and the image processor. In the function,  is set as a ratio of the text score and an image score
to combine two scores appropriately. We introduced  as a reliability ratio parameter of the scores by
the text processor and the scores by the image processor.
</bodyText>
<sectionHeader confidence="0.997141" genericHeader="method">
4 Data
</sectionHeader>
<bodyText confidence="0.999981857142857">
We prepared user annotation data and image annotation data that we used as training data and evalua-
tion data. User annotation data are input data for the text processor, whereas image annotation data are
for the image processor. As it is required to prepare a huge number of annotated data as a training cor-
pus, the data is annotated by Yahoo Crowd Sourcing (Yahoo! Japan, 2013). Yahoo Crowd Sourcing is
a Japanese crowd sourcing service similar to Amazon Mechanical Turk (Amazon, 2005). Therefore
the annotation process aims to obtain annotation based on human recognition rather than to explore
truth about users and images of twitter.
</bodyText>
<subsectionHeader confidence="0.981906">
4.1 User Annotation Data
</subsectionHeader>
<bodyText confidence="0.9999445625">
We first collected Japanese Twitter users according to their streaming tweets. We ignored heavy users
and Twitter bots. A random sampling of tweets revealed that tweets from heavy users include much
information that is not useful for profile inference such as short representations of their actions (e.g.
“Going to bed now” and “Just waking up”). A Twitter bot is also classed as an uninformative user be-
cause it is a program that automatically generates tweets. During data collection, we filtered out those
users by setting conditions shown below in Table 1. Finally, we obtained 3976 Twitter users. We
gathered tweets on each user up to 200. By executing the processes above, we obtained tweet data of
each user corresponding to the user’s own 200 tweets.
To obtain gender annotation for this large dataset, we used Yahoo! Crowd Sourcing. As shown in
Figure 2(a), we set task for every Twitter user: please infer the gender of the user who posted the
tweets in the URL below. In this task, after reading 200 tweets of a user, the gender label of male or
female was asked of every Twitter user. To guarantee quality reliability, annotation tasks for one Twit-
ter user were duplicated 10 times by different workers; then a majority vote of 10 annotations was cal-
culated to obtain a gold label.
As a result of the crowd sourcing tasks, 1733 users were reported as male; 2067 users were reported
as female. There were 176 users whose votes were split equally between male and female. We re-
</bodyText>
<tableCaption confidence="0.999571">
Table 1. Filtering conditions used to disqualify heavy users and Twitter bots
</tableCaption>
<table confidence="0.976153">
User Types Definition for I Criteria
Twitter bots Number of tweets posted from Twitter clients on PC/mobile I&lt;150
by a user
Heavy Number of Friends or followers of a user I&gt;200
Number of Tweets posted in a day by a user I&gt;10
</table>
<figure confidence="0.904879233333333">
Please infer the gender of the user
who posted the tweets in the URL below.
http://www.abc.com/defg/index.html
Question :
Answer: ○ Male
✔
○ Female
Question 1:
Please guess the gender of the
user who uploaded the image
Answer: ○ Male
✔
○ Unknown
○ Female
Question 2:
Please choose the word most
suitable to express the objects
included in the image
Answer: ○ Cartoon/Illustration
✔
○ Memo/Leaflet
○ Person
○ Screenshot/Capture
○ Famous person
○ Goods
○ Outdoor/Nature
○ Pet
○ Others
○ Food
(a) User annotation task (b) Image annotation task
</figure>
<figureCaption confidence="0.999915">
Figure 2. Annotation tasks in crowd sourcing.
</figureCaption>
<page confidence="0.995561">
57
</page>
<bodyText confidence="0.999730333333333">
moved balanced users from the data. The male and female populations of annotation assumed users
are 45.6% and 54.4% respectively. This gender proportion tendency is consistent with those reported
from an earlier study showing that Twitter participants are 55% female (Heli and Piskorski, 2009;
Burger et al., 2011). Finally, we obtained gender annotation data of 3800 users. We divided these data
equally between training data and evaluation data: 1900 users for training data and 1900 users for
evaluation data.
</bodyText>
<subsectionHeader confidence="0.987992">
4.2 Image Annotation Data
</subsectionHeader>
<bodyText confidence="0.999965">
We first made a user list including 1523 users. After checking tweets from these users, we extracted
9996 images. Image annotation processes were also executed by Yahoo Crowd Sourcing.
Our image annotation process refers to rules proposed by Ma et al. (2014). As shown in Figure 2(b),
a worker is requested to provide responses of two kinds for every image: Q1. Please guess the gender
of the user who uploaded the image; Q2. Please choose the word most suitable to express the objects
included in the image. The possible responses for Q1 were male, female, and unknown. Those for Q2
were cartoon/illustration, famous person, food, goods, memo/leaflet, outdoor/nature, person, pet,
screenshot/capture, and other. It is sometimes difficult to infer a gender of a user solely based on one
image. Therefore, unknown is set for Q1. From those responses we obtained multiple labels for every
image, such as “male-person”. To avoid influence by poor-quality workers, each image was presented
to 10 different workers. A summation of 10 annotations was executed to obtain gold label data.
</bodyText>
<sectionHeader confidence="0.993677" genericHeader="method">
5 Preliminary Experiments
</sectionHeader>
<subsectionHeader confidence="0.89523">
5.1 Image Processing
</subsectionHeader>
<bodyText confidence="0.9999868">
We compared two consolidation methods, computing the average of all scores and computing the av-
erage of the highest scores for 30 object scores. We applied the two method to the training data of the
user annotation data, and tested them on the evaluation data. Results show that the accuracy of former
method is 60.11. That of the latter is 65.42. The reason the latter method is superior to the former one
is probably attributable to noise reduction effects of ignoring low scores.
</bodyText>
<subsectionHeader confidence="0.993427">
5.2 Combined method of Text Processing and Image Processing
</subsectionHeader>
<bodyText confidence="0.999775333333333">
To estimate the optimal value of , we conducted a preliminary experiment of the combined method
with training data. We first prepared text and image probability scores. The text score is obtained by
executing five-fold cross validation of the text processor for training data. We used the probability
score derived in section 5.1 as the image score. The accuracies were, respectively, 86.23 and 65.42.
Next, the combined formula was applied to these probability scores with moving  from 0 to 1. Figure
3 shows the correlation between accuracy and . To obtain the  value of the peak, we executed poly-
nomial fitting to a part of the correlation curve where  is 0.1–0.4. By differentiating this function, we
calculated the  value of the peak as equal to 0.244 indicated by the arrow in Figure 3. The accuracy
reaches 86.73% at the peak, which is 0.50 pt higher than that of the text processor.
</bodyText>
<figure confidence="0.995704375">
0.875
0.87
0.865
0.86
0.855
0.85
0 0.1 0.2 0.3 0.4 0.5 0.6 0.7 0.8 0.9 1

</figure>
<figureCaption confidence="0.9841075">
Figure 3. Correlation between accuracy and  in training data.
(Fitting curve function is 0.95193-0.91292+0.2756+0.8409)
</figureCaption>
<page confidence="0.997735">
58
</page>
<sectionHeader confidence="0.998607" genericHeader="evaluation">
6 Experimental Results
</sectionHeader>
<subsectionHeader confidence="0.999974">
6.1 Comparing the Accuracies between Three Methods
</subsectionHeader>
<bodyText confidence="0.999986">
We executed an evaluation experiment assessing the three methods: text processing, image processing
with a selected consolidation method, and the combined method with optimized a (0.235). Each
method is applied to evaluation data including 1900 gender-annotated data. Table 2 presents precision,
recall, F-measure, and accuracy obtained through the evaluation experiments. The text processing ac-
curacy achieves 84.63%, and image processing accuracy is 64.21%. The combined method achieves
85.11% accuracy, which is 0.48 pt higher than the text processing accuracy.1 We also confirmed that
both the male and female F-measures become higher than text processing. We concluded that signifi-
cantly increased accuracy obtained using the method combining text processing and image processing.
</bodyText>
<subsectionHeader confidence="0.99527">
6.2 Discussion
</subsectionHeader>
<bodyText confidence="0.999936">
We expected the optimal value of a to be large, since the accuracy of the text processor is explicitly
higher than that of the image processor. However, the actual optimal a resulted to the rather small val-
ue, 0.244. This small a is thought to be caused by a characteristic of the image processor’s gender
scores. Figure 4 (a) and (b) show the distributions of the gender scores derived by the text processor
and the image processor. The horizontal axis corresponds to a gender score of a user, ranging from 0,
highly probable female, to 1, highly probable male. The two distributions are clearly different from
</bodyText>
<tableCaption confidence="0.836561">
Table 2. Results obtained using text processing, image processing and combined method.
P, precision; R, recall; F, F-measure; Acc., Accurac
</tableCaption>
<table confidence="0.824331">
Male Female Acc.
P R F P R F
Text processing 84.65 82.39 83.50 84.62 86.64 83.50 84.63
Image processing 64.68 66.56 65.60 72.10 62.11 66.74 64.21
</table>
<figure confidence="0.983937">
Combined method (a = 0.244) 84.57 83.72 84.16 85.49 86.34 85.91 85.11
1000
900
800
700
600
500
400
300
200
100
0
1000
900
800
700
600
500
400
300
200
100
0
0.1 0.2 0.3 0.4 0.5 0.6 0.7 0.8 0.9 1.0 0.1 0.2 0.3 0.4 0.5 0.6 0.7 0.8 0.9 1.0
Probability score Probability score
(a) The distribution of the text processing scores (b) The distribution of the image processing scores
</figure>
<figureCaption confidence="0.998893">
Figure 4. The distributions of the probability scores.
</figureCaption>
<figure confidence="0.9897414">
7000
6000
5000
4000
3000
2000
1000
0
0.1 0.2 0.3 0.4 0.5 0.6 0.7 0.8 0.9 1.0
Probability score
</figure>
<figureCaption confidence="0.999824">
Figure 5. The distribution of the “male-person” score of training data of user annotation data.
</figureCaption>
<footnote confidence="0.913517">
1Significance improvement with paired t-test (p=0.09&lt;0.1).
</footnote>
<page confidence="0.998658">
59
</page>
<bodyText confidence="0.999803071428571">
each other: the variance of the image scores is much smaller than that of the text. From this character-
istic, the image scores were needed to be amplified in order to reflect them in the final result. In terms
of , this amplification corresponds to a small value.
The reason why the variance of the image scores became small is in its calculation process. In the
image processor, the gender score of a user is calculated as the mean of the highest object scores ex-
tracted from each image. Figure 5 shows a distribution of “male-person” label scores. Though a distri-
bution of each object probability scores centres not at 0.5, highest score selections and the averaging
of them leads to a mid-range value, in this case 0.5.
Our intuition behind the introduction of  was to provide a reliability ratio parameter of the text
processor and the image processor. But as a matter of fact, this parameter also worked to calibrate the
scale difference between the two probability scores. From this observation, a function that includes a
reliability parameter and a calibration parameter separately can be considered as an alternative to the
proposed function. Using this kind of function will provide further insights about combining a text
processing and image processing.
</bodyText>
<sectionHeader confidence="0.999046" genericHeader="conclusions">
7 Conclusion
</sectionHeader>
<bodyText confidence="0.999954882352941">
As described herein, we assembled two results retrieved by text and image processors respectively to
enhance the Twitter user gender inference. Even though the gender inference accuracy already reached
84.63 solely by the text classifier, we succeeded in improving efficiency further by 0.48 pt. Because
the image processing in our method is completely independent from the text processing, this combined
method is applicable to the other gender prediction methods, just like those of Burger and Liu (Burger,
2011; Liu, 2013). Reported studies about SNS user profile inference targeted basic attributes such as
gender, age, career, residential area, etc. More worthwhile attributes for marketing that directly indi-
cate user characteristics are desired to predict, for example, hobbies and lifestyles. Images in tweets
are expected to include clues about these profiles aside from gender. As a subject for future work, we
will apply our combined method to various profile attributes.
As the combined method in this paper is simple linear consolidation and ignores a capability of ana-
lyzing both text and image information at the same time, exploring more suitable combined method is
needed. The simplest way to analyze both text and image information simultaneously is early fusion
that first creates the large multi-model feature vector constructed by both text and image features and
then trains a classifier. Meta classifier which infers final class from the outputs of two modalities is
also considerable method for this subject. Applying more sophisticated combined methods is another
subject for future work.
</bodyText>
<sectionHeader confidence="0.998596" genericHeader="references">
References
</sectionHeader>
<reference confidence="0.999701142857143">
Amazon. 2005. Amazon Mechanical Turk (2005), Available: http://www.mturk/welcom
Atilika. 2011, Kuromoji. Available: http://www.atilika.org
John D. Burger, John Henderson, Gerorge Kim, Guido Zarrella. 2011. Discriminating Gender on
Twitter, In Proc. of the Conference on Empirical Methods in natural Language Processing
Ken Chatfield, Victor Lempitsky, Andrea Vedaldi, Andrew Zisserman. 2011. The devil is in the de-
tails: an evaluation of recent feature encoding methods, In Proc. of British Machine Vision Confer-
ence 2011
Chih-Chung Chang, Chih-Jen Lin, 2001. LIBSVM: a Library for Support Vector Machines. Available:
http://www.csie.ntu.edu.tw/~cjlin/libsvm
Bo Han, Paul Cook, and Timothy Baldwin. 2013. A Stacking-based Approach to Twitter User Geolo-
cation Prediction, In Proc. of the 51st Annual meeting of Association for Computational Linguistics,
pages 7-12
Bill Heli, Mikolaj Jan Piskorski. 2009. New Twitter Research: Men Follow Men and Nobody Tweets,
Harvard Business Review, June 1.
</reference>
<page confidence="0.966812">
60
</page>
<reference confidence="0.999728514285714">
Kazushi Ikeda, Gen Hattori, Chihiro Ono, Hideki Asoh, Teruo Higashino. 2013. Twitter User Profil-
ing Based on Text and Community Mining for Market Analysis, Knowledge Based Systems 51,
pages 35-47.
Svetlana Lazebnik, Cordelia Schmid, Jean Ponce, 2006. Beyond bags of features: Spatial Pyramid
Matching for Recognizing Natural Scene Categories, In Proc. of Computer Vision and Pattern
Recognition 2006, page 2169-2178
Wendy Liu, Faiyaz Al Zamal, Derek Ruths. 2012. Using Social Media to Infer Gender Composition of
Commuter Populations, In Proc. of the International Association for the Advancement of Artificial
Intelligence Conference on Weblogs and Social
Wendy Liu, Derek Ruths. 2013. What’s in a Name? Using First Names as Features for Gender Infer-
ence in Twitter, In Symposium on Analyzing Microtext
David G. Lowe. 1999. Object recognition from local scale-invariant features, In Proc. of the Interna-
tional Conference on Computer Vision, pages 1150-1157
Matt Lynley. 2012. Statistics That Reveal Instagram’s Mind-Blowing Success, Available:
http://www.businessinsider.com/statistics-that-reveal-instagrams-mind-blowing-success-2012-4
Xiaojun Ma, Yukihiro Tsuboshita, Noriji Kato. 2014. Gender Estimation for SNS User Profiling Au-
tomatic Image Annotation, In Proc. of the 1st International Workshop on Cross-media Analysis for
Social Multimedia
Aibek Makazhanov, Davood Refiei. 2013. Predicting Political Preference of Twitter Users, In Proc. of
the 2013 IEEE/ACM International Conference on Advances in Social Network and Mining, pages
298-305
Alan Mislove, Sune Lehmann, Yong-Yeol Ahn, Jukka-Pekka onnela, J. Hiels Rosenquist. 2011. Un-
dersanding the Demographics of Twitter Users, In Proc. of 5th International AAAI Conference on
Weblogs and Social Media, pages 554-557
Delip Rao and David Yarowsky. 2010. Detecting Latent User Properties in Social Media, In Proc. of
the Neural Information Processing Systems Foundation workshop on Machine Learning for Social
Networks
Chih-Fong Tsai. 2012. Bag-of-Words Representation in Image Annotation: A Review, International
Scholarly Research Notices Artificial Intelligence, Volume 2012, Article ID 376804, 19 pages
Jinjun Wang, Jinchao Yang, Kai Yu, Fengjun Lv, Thomas Huang, Yihong Gong. 2010. Locality-
constrained linear coding for image classification, In Proc. of Computer Vision and Pattern Recog-
nition 2010, page 626
Yahoo! Japan. 2013. Yahoo Crowd Sourcing. Available: http://crowdsourcing.yahoo.co.jp/
Dengsheng Zhang, Md Monirul Islam, Guojun Lu. 2012. A review on automatic image annotation,
Pattern Recognition 45, pages 346-362
</reference>
<page confidence="0.999272">
61
</page>
</variant>
</algorithm>
<algorithm name="ParsHed" version="110505">
<variant no="0" confidence="0.753070">
<title confidence="0.9983995">Twitter User Gender Inference Using Combined of Text and Image Processing</title>
<author confidence="0.950754">Shigeyuki Sakaki</author>
<author confidence="0.950754">Yasuhide Miura</author>
<author confidence="0.950754">Xiaojun Ma</author>
<author confidence="0.950754">Keigo Hattori</author>
<author confidence="0.950754">Tomoko Ohkuma</author>
<affiliation confidence="0.916498">Fuji Xerox Co., Ltd. / Japan</affiliation>
<address confidence="0.994152">6-1, Minatomirai, Nishi-ku, Yokohama-shi, Kanagawa</address>
<email confidence="0.9452555">sakaki.shigeyuki@fujixerox.co.jp</email>
<email confidence="0.9452555">yasuhide.miura@fujixerox.co.jp</email>
<email confidence="0.9452555">xiaojun.ma@fujixerox.co.jp</email>
<email confidence="0.9452555">keigo.hattori@fujixerox.co.jp</email>
<email confidence="0.9452555">ohkuma.tomoko@fujixerox.co.jp</email>
<abstract confidence="0.991163125">Profile inference of SNS users is valuable for marketing, target advertisement, and opinion polls. Several studies examining profile inference have been reported to date. Although information of various types is included in SNS, most such studies only use text information. It is expected that incorporating information of other types into text classifiers can provide more accurate profile inference. As described in this paper, we propose combined method of text processing and image processing to improve gender inference accuracy. By applying the simple formula to combine two results derived from a text processor and an image processor, significantly increased accuracy was confirmed.</abstract>
</variant>
</algorithm>
<algorithm name="ParsCit" version="110505">
<citationList>
<citation valid="true">
<authors>
<author>Amazon</author>
</authors>
<title>Amazon Mechanical Turk</title>
<date>2005</date>
<note>Kuromoji. Available: http://www.atilika.org</note>
<contexts>
<context position="12228" citStr="Amazon, 2005" startWordPosition="1932" endWordPosition="1933">ntroduced  as a reliability ratio parameter of the scores by the text processor and the scores by the image processor. 4 Data We prepared user annotation data and image annotation data that we used as training data and evaluation data. User annotation data are input data for the text processor, whereas image annotation data are for the image processor. As it is required to prepare a huge number of annotated data as a training corpus, the data is annotated by Yahoo Crowd Sourcing (Yahoo! Japan, 2013). Yahoo Crowd Sourcing is a Japanese crowd sourcing service similar to Amazon Mechanical Turk (Amazon, 2005). Therefore the annotation process aims to obtain annotation based on human recognition rather than to explore truth about users and images of twitter. 4.1 User Annotation Data We first collected Japanese Twitter users according to their streaming tweets. We ignored heavy users and Twitter bots. A random sampling of tweets revealed that tweets from heavy users include much information that is not useful for profile inference such as short representations of their actions (e.g. “Going to bed now” and “Just waking up”). A Twitter bot is also classed as an uninformative user because it is a progr</context>
</contexts>
<marker>Amazon, 2005</marker>
<rawString>Amazon. 2005. Amazon Mechanical Turk (2005), Available: http://www.mturk/welcom Atilika. 2011, Kuromoji. Available: http://www.atilika.org</rawString>
</citation>
<citation valid="true">
<authors>
<author>John D Burger</author>
<author>John Henderson</author>
<author>Gerorge Kim</author>
<author>Guido Zarrella</author>
</authors>
<title>Discriminating Gender on Twitter,</title>
<date>2011</date>
<booktitle>In Proc. of the Conference on Empirical Methods in natural Language Processing</booktitle>
<contexts>
<context position="3293" citStr="Burger et al., 2011" startWordPosition="487" endWordPosition="490">ons: section 2 presents a description of prior work; section 3 presents a description of the annotation data prepared for this study; section 4 introduces the proposed method; section 5 explains preliminary experiments for optimizing the combined method parameter; section 6 presents the experimentally obtained result; section 7 summarizes the paper and discusses future work. 2 Prior Work Many reports have described studies examining gender inference. The conventional approach to this theme is building a machine learning classifier such as Support Vector Machine (SVM) trained by text features (Burger et al., 2011; Liu et al., 2012). Most of these studies specifically examine improvement of the machine classification methodology rather than expanding features or combining features. Different from these studies, Liu et al. (2013) implemented gender inference with incorporation of a user name into the classifier based on text information. However, the expansion of features This work is licensed under a Creative Commons Attribution 4.0 International License. Page numbers and proceedings footer are added by the organisers. License details: http://creativecommons.org/licenses /by/4.0/ 54 Proceedings of the </context>
<context position="15099" citStr="Burger et al., 2011" startWordPosition="2415" endWordPosition="2418"> word most suitable to express the objects included in the image Answer: ○ Cartoon/Illustration ✔ ○ Memo/Leaflet ○ Person ○ Screenshot/Capture ○ Famous person ○ Goods ○ Outdoor/Nature ○ Pet ○ Others ○ Food (a) User annotation task (b) Image annotation task Figure 2. Annotation tasks in crowd sourcing. 57 moved balanced users from the data. The male and female populations of annotation assumed users are 45.6% and 54.4% respectively. This gender proportion tendency is consistent with those reported from an earlier study showing that Twitter participants are 55% female (Heli and Piskorski, 2009; Burger et al., 2011). Finally, we obtained gender annotation data of 3800 users. We divided these data equally between training data and evaluation data: 1900 users for training data and 1900 users for evaluation data. 4.2 Image Annotation Data We first made a user list including 1523 users. After checking tweets from these users, we extracted 9996 images. Image annotation processes were also executed by Yahoo Crowd Sourcing. Our image annotation process refers to rules proposed by Ma et al. (2014). As shown in Figure 2(b), a worker is requested to provide responses of two kinds for every image: Q1. Please guess </context>
</contexts>
<marker>Burger, Henderson, Kim, Zarrella, 2011</marker>
<rawString>John D. Burger, John Henderson, Gerorge Kim, Guido Zarrella. 2011. Discriminating Gender on Twitter, In Proc. of the Conference on Empirical Methods in natural Language Processing</rawString>
</citation>
<citation valid="true">
<authors>
<author>Ken Chatfield</author>
<author>Victor Lempitsky</author>
<author>Andrea Vedaldi</author>
<author>Andrew Zisserman</author>
</authors>
<title>The devil is in the details: an evaluation of recent feature encoding methods,</title>
<date>2011</date>
<booktitle>In Proc. of British Machine Vision Conference</booktitle>
<contexts>
<context position="10007" citStr="Chatfield et al., 2011" startWordPosition="1549" endWordPosition="1552">(2014) defined 10 categories of objects in SNS images based on observation on a real dataset. The defined labels are cartoon/illustration, famous person, food, goods, memo/leaflet, outdoor/nature, person, pet, screenshot/capture, and other. They also indicated that gender tendency in images are coherent with user gender, and set three gender labels, male, female, and unknown, for each object label. As a result, 30 labels constructed from object label and gender label (e.g. “male-person”) are used in this paper, which is described in section 4.2. Then a bag-of-features (BOF) model (Tsai, 2012; Chatfield et al., 2011) is applied to accomplish the image annotation task. We used local descriptors of SIFT (Lowe, 1999) and image features are encoded with a k-mean generated codebook with size of 2000. We applied LLC (Wang et al, 2010) and SPM (Lazebnik et al, 2006) to generating the final presentation of image features. Then, the 30 SVM classifiers are trained based on the features of training images: each classifier is trained per image label among one-versus-rest strategy. The SVM classifier annotates images of a user by computing scores, and logistic function is applied to the outputs of the image classifier</context>
</contexts>
<marker>Chatfield, Lempitsky, Vedaldi, Zisserman, 2011</marker>
<rawString>Ken Chatfield, Victor Lempitsky, Andrea Vedaldi, Andrew Zisserman. 2011. The devil is in the details: an evaluation of recent feature encoding methods, In Proc. of British Machine Vision Conference 2011</rawString>
</citation>
<citation valid="true">
<authors>
<author>Chih-Chung Chang</author>
<author>Chih-Jen Lin</author>
</authors>
<title>LIBSVM: a Library for Support Vector Machines.</title>
<date>2001</date>
<note>Available: http://www.csie.ntu.edu.tw/~cjlin/libsvm</note>
<contexts>
<context position="7609" citStr="Chang and Lin, 2001" startWordPosition="1163" endWordPosition="1166">ocess, the combined gender probability score is calculated using two probability scores. In this section, details of the two processors and the method of combining their two results are described. 3.1 Text Processing The text processor is constructed from a text classifier, which accepts text data in tweets and outputs the gender probability score of a user. We defined the gender classifier in the text processor as an SVM binary classification of a male and female. The SVM classifier is trained based on unigram Bagof-words with a linear kernel. The cost parameter C is set to 1.0. Then LIBSVM (Chang and Lin, 2001) is used as an implementation of SVM. Because words are not divided by spaces in a Japanese sentence, Kuromoji (Atilika, 2011), a morphological analysis program for Japanese, is used to obtain unigrams. To combine two results from the text processor and the image processor, it is necessary to calculate each result as a probability value. To retrieve probability scores, we used logistic regression. Logistic function converts a distance from a hyper plane to probability scores of 0.0–1.0. The text classifier is a male and female binary classification. Therefore, the upper and lower ends of the p</context>
</contexts>
<marker>Chang, Lin, 2001</marker>
<rawString>Chih-Chung Chang, Chih-Jen Lin, 2001. LIBSVM: a Library for Support Vector Machines. Available: http://www.csie.ntu.edu.tw/~cjlin/libsvm</rawString>
</citation>
<citation valid="true">
<authors>
<author>Bo Han</author>
<author>Paul Cook</author>
<author>Timothy Baldwin</author>
</authors>
<title>A Stacking-based Approach to Twitter User Geolocation Prediction,</title>
<date>2013</date>
<booktitle>In Proc. of the 51st Annual meeting of Association for Computational Linguistics,</booktitle>
<pages>7--12</pages>
<contexts>
<context position="1233" citStr="Han et al., 2013" startWordPosition="169" endWordPosition="172"> It is expected that incorporating information of other types into text classifiers can provide more accurate profile inference. As described in this paper, we propose combined method of text processing and image processing to improve gender inference accuracy. By applying the simple formula to combine two results derived from a text processor and an image processor, significantly increased accuracy was confirmed. 1 Introduction Recently, several researches on profile inference of Social Networking Services (SNS) user conducted by analyzing postings have been reported (Rao and Yarowsky, 2010; Han et al., 2013; Makazhanov et al., 2013). User profile information such as gender, age, residential area, and political preference have attracted attention because they are helpful for marketing, target advertisement, TV viewer rate calculations, and opinion polls. The major approach to this subject is building a machine learning classifier trained by text in postings. However, images posted by a user are rarely used in profile inference. Images in postings also include features of user profiles. For example, if a user posts many dessert images, then the user might be female. Therefore, we assumed that high</context>
</contexts>
<marker>Han, Cook, Baldwin, 2013</marker>
<rawString>Bo Han, Paul Cook, and Timothy Baldwin. 2013. A Stacking-based Approach to Twitter User Geolocation Prediction, In Proc. of the 51st Annual meeting of Association for Computational Linguistics, pages 7-12</rawString>
</citation>
<citation valid="true">
<authors>
<author>Bill Heli</author>
<author>Mikolaj Jan Piskorski</author>
</authors>
<title>New Twitter Research: Men Follow Men and Nobody Tweets, Harvard Business Review,</title>
<date>2009</date>
<contexts>
<context position="15077" citStr="Heli and Piskorski, 2009" startWordPosition="2411" endWordPosition="2414">stion 2: Please choose the word most suitable to express the objects included in the image Answer: ○ Cartoon/Illustration ✔ ○ Memo/Leaflet ○ Person ○ Screenshot/Capture ○ Famous person ○ Goods ○ Outdoor/Nature ○ Pet ○ Others ○ Food (a) User annotation task (b) Image annotation task Figure 2. Annotation tasks in crowd sourcing. 57 moved balanced users from the data. The male and female populations of annotation assumed users are 45.6% and 54.4% respectively. This gender proportion tendency is consistent with those reported from an earlier study showing that Twitter participants are 55% female (Heli and Piskorski, 2009; Burger et al., 2011). Finally, we obtained gender annotation data of 3800 users. We divided these data equally between training data and evaluation data: 1900 users for training data and 1900 users for evaluation data. 4.2 Image Annotation Data We first made a user list including 1523 users. After checking tweets from these users, we extracted 9996 images. Image annotation processes were also executed by Yahoo Crowd Sourcing. Our image annotation process refers to rules proposed by Ma et al. (2014). As shown in Figure 2(b), a worker is requested to provide responses of two kinds for every im</context>
</contexts>
<marker>Heli, Piskorski, 2009</marker>
<rawString>Bill Heli, Mikolaj Jan Piskorski. 2009. New Twitter Research: Men Follow Men and Nobody Tweets, Harvard Business Review, June 1.</rawString>
</citation>
<citation valid="true">
<authors>
<author>Kazushi Ikeda</author>
<author>Gen Hattori</author>
</authors>
<title>Chihiro Ono, Hideki Asoh, Teruo Higashino.</title>
<date>2013</date>
<pages>35--47</pages>
<marker>Ikeda, Hattori, 2013</marker>
<rawString>Kazushi Ikeda, Gen Hattori, Chihiro Ono, Hideki Asoh, Teruo Higashino. 2013. Twitter User Profiling Based on Text and Community Mining for Market Analysis, Knowledge Based Systems 51, pages 35-47.</rawString>
</citation>
<citation valid="true">
<authors>
<author>Svetlana Lazebnik</author>
<author>Cordelia Schmid</author>
<author>Jean Ponce</author>
</authors>
<title>Beyond bags of features: Spatial Pyramid Matching for Recognizing Natural Scene Categories,</title>
<date>2006</date>
<booktitle>In Proc. of Computer Vision and Pattern Recognition</booktitle>
<pages>2169--2178</pages>
<contexts>
<context position="10254" citStr="Lazebnik et al, 2006" startWordPosition="1593" endWordPosition="1596">o indicated that gender tendency in images are coherent with user gender, and set three gender labels, male, female, and unknown, for each object label. As a result, 30 labels constructed from object label and gender label (e.g. “male-person”) are used in this paper, which is described in section 4.2. Then a bag-of-features (BOF) model (Tsai, 2012; Chatfield et al., 2011) is applied to accomplish the image annotation task. We used local descriptors of SIFT (Lowe, 1999) and image features are encoded with a k-mean generated codebook with size of 2000. We applied LLC (Wang et al, 2010) and SPM (Lazebnik et al, 2006) to generating the final presentation of image features. Then, the 30 SVM classifiers are trained based on the features of training images: each classifier is trained per image label among one-versus-rest strategy. The SVM classifier annotates images of a user by computing scores, and logistic function is applied to the outputs of the image classifiers in order to obtain probability scores. Each of 30 probability scores shows how an image is close to the decision boundary of a particular label. In the second step, we integrated the 30 scores of labels assigned to images to yield comprehensive </context>
</contexts>
<marker>Lazebnik, Schmid, Ponce, 2006</marker>
<rawString>Svetlana Lazebnik, Cordelia Schmid, Jean Ponce, 2006. Beyond bags of features: Spatial Pyramid Matching for Recognizing Natural Scene Categories, In Proc. of Computer Vision and Pattern Recognition 2006, page 2169-2178</rawString>
</citation>
<citation valid="true">
<authors>
<author>Wendy Liu</author>
</authors>
<title>Faiyaz Al Zamal, Derek Ruths.</title>
<date>2012</date>
<booktitle>In Proc. of the International Association for the Advancement of Artificial Intelligence Conference on Weblogs and Social</booktitle>
<marker>Liu, 2012</marker>
<rawString>Wendy Liu, Faiyaz Al Zamal, Derek Ruths. 2012. Using Social Media to Infer Gender Composition of Commuter Populations, In Proc. of the International Association for the Advancement of Artificial Intelligence Conference on Weblogs and Social</rawString>
</citation>
<citation valid="true">
<authors>
<author>Wendy Liu</author>
<author>Derek Ruths</author>
</authors>
<title>What’s in a Name? Using First Names as Features for Gender Inference in Twitter, In</title>
<date>2013</date>
<booktitle>Symposium on Analyzing Microtext</booktitle>
<marker>Liu, Ruths, 2013</marker>
<rawString>Wendy Liu, Derek Ruths. 2013. What’s in a Name? Using First Names as Features for Gender Inference in Twitter, In Symposium on Analyzing Microtext</rawString>
</citation>
<citation valid="true">
<authors>
<author>David G Lowe</author>
</authors>
<title>Object recognition from local scale-invariant features,</title>
<date>1999</date>
<booktitle>In Proc. of the International Conference on Computer Vision,</booktitle>
<pages>1150--1157</pages>
<contexts>
<context position="10106" citStr="Lowe, 1999" startWordPosition="1567" endWordPosition="1568">s are cartoon/illustration, famous person, food, goods, memo/leaflet, outdoor/nature, person, pet, screenshot/capture, and other. They also indicated that gender tendency in images are coherent with user gender, and set three gender labels, male, female, and unknown, for each object label. As a result, 30 labels constructed from object label and gender label (e.g. “male-person”) are used in this paper, which is described in section 4.2. Then a bag-of-features (BOF) model (Tsai, 2012; Chatfield et al., 2011) is applied to accomplish the image annotation task. We used local descriptors of SIFT (Lowe, 1999) and image features are encoded with a k-mean generated codebook with size of 2000. We applied LLC (Wang et al, 2010) and SPM (Lazebnik et al, 2006) to generating the final presentation of image features. Then, the 30 SVM classifiers are trained based on the features of training images: each classifier is trained per image label among one-versus-rest strategy. The SVM classifier annotates images of a user by computing scores, and logistic function is applied to the outputs of the image classifiers in order to obtain probability scores. Each of 30 probability scores shows how an image is close </context>
</contexts>
<marker>Lowe, 1999</marker>
<rawString>David G. Lowe. 1999. Object recognition from local scale-invariant features, In Proc. of the International Conference on Computer Vision, pages 1150-1157</rawString>
</citation>
<citation valid="true">
<authors>
<author>Matt Lynley</author>
</authors>
<title>Statistics That Reveal Instagram’s Mind-Blowing Success,</title>
<date>2012</date>
<pages>2012--4</pages>
<location>Available:</location>
<marker>Lynley, 2012</marker>
<rawString>Matt Lynley. 2012. Statistics That Reveal Instagram’s Mind-Blowing Success, Available: http://www.businessinsider.com/statistics-that-reveal-instagrams-mind-blowing-success-2012-4</rawString>
</citation>
<citation valid="true">
<authors>
<author>Xiaojun Ma</author>
<author>Yukihiro Tsuboshita</author>
<author>Noriji Kato</author>
</authors>
<title>Gender Estimation for SNS User Profiling Automatic Image Annotation,</title>
<date>2014</date>
<booktitle>In Proc. of the 1st International Workshop on Cross-media Analysis for Social Multimedia</booktitle>
<contexts>
<context position="5598" citStr="Ma et al. (2014)" startWordPosition="846" endWordPosition="849"> is printed in itself, and researches to automatically annotate an image with semantic labels are already known (Zhang et al., 2012). Automatic image annotation is a machine learning technique that involves a process by which a computer system automatically assigns semantic labels to a digital image. These studies succeeded in inferences of various objects, such as person, dog, bicycle, chair etc. We supposed that such objects in images posted by a user should be useful clues as to a profile inference of a twitter user. As a matter of fact, gender inference by image information is reported by Ma et al. (2014), which implemented gender inference by processing images in tweets. Their study, which ignored text information, exhibited accuracy of less than 70%. It was much lower than most gender inference work using text feature. From results of these studies, we concluded that gender inference by text and image information invites further study. 3 Proposed Method Our proposed method for combining text processing and image processing is presented in Figure 1. First, data of 200 tweets of a user are separated into text data and image data. Each of separated data is analyzed using a dedicated processor, </context>
<context position="8857" citStr="Ma et al. (2014)" startWordPosition="1368" endWordPosition="1371">rrespond to male and female data. If a score is close to 0.0, then the user has high probability of being male. If it is close to 1.0, then a user is probably female. 3.2 Image Processing We first tried to infer a Twitter user gender directly by a two-class classifier trained by image feature vector calculated by all images posted by a user. However, with some preliminary experiments, we found that this approach does not work well, since the large variation of objects made the classification difficult with single classifier setting. We, therefore, used the image processing method described by Ma et al. (2014) which uses automatic image annotation classifiers (Zhang et al., 2012) to model human recognition of different gender tendency in images. The method consists of two steps: step 1) annotating images by an image annotation technique at the image level; step 2) consolidating gender scores according to annotation results at the user level. In the first step, the image labels are defined as the combination of the following two information: the gender tendency in images of a user and the objects that images express. Ma et al. (2014) defined 10 categories of objects in SNS images based on observatio</context>
<context position="15582" citStr="Ma et al. (2014)" startWordPosition="2492" endWordPosition="2495">those reported from an earlier study showing that Twitter participants are 55% female (Heli and Piskorski, 2009; Burger et al., 2011). Finally, we obtained gender annotation data of 3800 users. We divided these data equally between training data and evaluation data: 1900 users for training data and 1900 users for evaluation data. 4.2 Image Annotation Data We first made a user list including 1523 users. After checking tweets from these users, we extracted 9996 images. Image annotation processes were also executed by Yahoo Crowd Sourcing. Our image annotation process refers to rules proposed by Ma et al. (2014). As shown in Figure 2(b), a worker is requested to provide responses of two kinds for every image: Q1. Please guess the gender of the user who uploaded the image; Q2. Please choose the word most suitable to express the objects included in the image. The possible responses for Q1 were male, female, and unknown. Those for Q2 were cartoon/illustration, famous person, food, goods, memo/leaflet, outdoor/nature, person, pet, screenshot/capture, and other. It is sometimes difficult to infer a gender of a user solely based on one image. Therefore, unknown is set for Q1. From those responses we obtain</context>
</contexts>
<marker>Ma, Tsuboshita, Kato, 2014</marker>
<rawString>Xiaojun Ma, Yukihiro Tsuboshita, Noriji Kato. 2014. Gender Estimation for SNS User Profiling Automatic Image Annotation, In Proc. of the 1st International Workshop on Cross-media Analysis for Social Multimedia</rawString>
</citation>
<citation valid="true">
<authors>
<author>Aibek Makazhanov</author>
<author>Davood Refiei</author>
</authors>
<title>Predicting Political Preference of Twitter Users,</title>
<date>2013</date>
<booktitle>In Proc. of the 2013 IEEE/ACM International Conference on Advances in Social Network and Mining,</booktitle>
<pages>298--305</pages>
<marker>Makazhanov, Refiei, 2013</marker>
<rawString>Aibek Makazhanov, Davood Refiei. 2013. Predicting Political Preference of Twitter Users, In Proc. of the 2013 IEEE/ACM International Conference on Advances in Social Network and Mining, pages 298-305</rawString>
</citation>
<citation valid="true">
<authors>
<author>Alan Mislove</author>
<author>Sune Lehmann</author>
<author>Yong-Yeol Ahn</author>
<author>Jukka-Pekka onnela</author>
<author>J Hiels Rosenquist</author>
</authors>
<title>Undersanding the Demographics of Twitter Users,</title>
<date>2011</date>
<booktitle>In Proc. of 5th International AAAI Conference on Weblogs and Social Media,</booktitle>
<pages>554--557</pages>
<marker>Mislove, Lehmann, Ahn, onnela, Rosenquist, 2011</marker>
<rawString>Alan Mislove, Sune Lehmann, Yong-Yeol Ahn, Jukka-Pekka onnela, J. Hiels Rosenquist. 2011. Undersanding the Demographics of Twitter Users, In Proc. of 5th International AAAI Conference on Weblogs and Social Media, pages 554-557</rawString>
</citation>
<citation valid="true">
<authors>
<author>Delip Rao</author>
<author>David Yarowsky</author>
</authors>
<title>Detecting Latent User Properties in Social Media,</title>
<date>2010</date>
<booktitle>In Proc. of the Neural Information Processing Systems Foundation workshop on Machine Learning for Social Networks</booktitle>
<contexts>
<context position="1215" citStr="Rao and Yarowsky, 2010" startWordPosition="165" endWordPosition="168">ly use text information. It is expected that incorporating information of other types into text classifiers can provide more accurate profile inference. As described in this paper, we propose combined method of text processing and image processing to improve gender inference accuracy. By applying the simple formula to combine two results derived from a text processor and an image processor, significantly increased accuracy was confirmed. 1 Introduction Recently, several researches on profile inference of Social Networking Services (SNS) user conducted by analyzing postings have been reported (Rao and Yarowsky, 2010; Han et al., 2013; Makazhanov et al., 2013). User profile information such as gender, age, residential area, and political preference have attracted attention because they are helpful for marketing, target advertisement, TV viewer rate calculations, and opinion polls. The major approach to this subject is building a machine learning classifier trained by text in postings. However, images posted by a user are rarely used in profile inference. Images in postings also include features of user profiles. For example, if a user posts many dessert images, then the user might be female. Therefore, we</context>
</contexts>
<marker>Rao, Yarowsky, 2010</marker>
<rawString>Delip Rao and David Yarowsky. 2010. Detecting Latent User Properties in Social Media, In Proc. of the Neural Information Processing Systems Foundation workshop on Machine Learning for Social Networks</rawString>
</citation>
<citation valid="true">
<authors>
<author>Chih-Fong Tsai</author>
</authors>
<title>Bag-of-Words Representation in Image Annotation: A Review,</title>
<date>2012</date>
<booktitle>International Scholarly Research Notices Artificial Intelligence, Volume 2012, Article ID 376804, 19</booktitle>
<pages>pages</pages>
<contexts>
<context position="9982" citStr="Tsai, 2012" startWordPosition="1547" endWordPosition="1548">. Ma et al. (2014) defined 10 categories of objects in SNS images based on observation on a real dataset. The defined labels are cartoon/illustration, famous person, food, goods, memo/leaflet, outdoor/nature, person, pet, screenshot/capture, and other. They also indicated that gender tendency in images are coherent with user gender, and set three gender labels, male, female, and unknown, for each object label. As a result, 30 labels constructed from object label and gender label (e.g. “male-person”) are used in this paper, which is described in section 4.2. Then a bag-of-features (BOF) model (Tsai, 2012; Chatfield et al., 2011) is applied to accomplish the image annotation task. We used local descriptors of SIFT (Lowe, 1999) and image features are encoded with a k-mean generated codebook with size of 2000. We applied LLC (Wang et al, 2010) and SPM (Lazebnik et al, 2006) to generating the final presentation of image features. Then, the 30 SVM classifiers are trained based on the features of training images: each classifier is trained per image label among one-versus-rest strategy. The SVM classifier annotates images of a user by computing scores, and logistic function is applied to the output</context>
</contexts>
<marker>Tsai, 2012</marker>
<rawString>Chih-Fong Tsai. 2012. Bag-of-Words Representation in Image Annotation: A Review, International Scholarly Research Notices Artificial Intelligence, Volume 2012, Article ID 376804, 19 pages</rawString>
</citation>
<citation valid="true">
<authors>
<author>Jinjun Wang</author>
<author>Jinchao Yang</author>
<author>Kai Yu</author>
</authors>
<title>Fengjun Lv, Thomas Huang, Yihong Gong.</title>
<date>2010</date>
<booktitle>In Proc. of Computer Vision and Pattern Recognition 2010,</booktitle>
<pages>626</pages>
<contexts>
<context position="10223" citStr="Wang et al, 2010" startWordPosition="1587" endWordPosition="1590">apture, and other. They also indicated that gender tendency in images are coherent with user gender, and set three gender labels, male, female, and unknown, for each object label. As a result, 30 labels constructed from object label and gender label (e.g. “male-person”) are used in this paper, which is described in section 4.2. Then a bag-of-features (BOF) model (Tsai, 2012; Chatfield et al., 2011) is applied to accomplish the image annotation task. We used local descriptors of SIFT (Lowe, 1999) and image features are encoded with a k-mean generated codebook with size of 2000. We applied LLC (Wang et al, 2010) and SPM (Lazebnik et al, 2006) to generating the final presentation of image features. Then, the 30 SVM classifiers are trained based on the features of training images: each classifier is trained per image label among one-versus-rest strategy. The SVM classifier annotates images of a user by computing scores, and logistic function is applied to the outputs of the image classifiers in order to obtain probability scores. Each of 30 probability scores shows how an image is close to the decision boundary of a particular label. In the second step, we integrated the 30 scores of labels assigned to</context>
</contexts>
<marker>Wang, Yang, Yu, 2010</marker>
<rawString>Jinjun Wang, Jinchao Yang, Kai Yu, Fengjun Lv, Thomas Huang, Yihong Gong. 2010. Localityconstrained linear coding for image classification, In Proc. of Computer Vision and Pattern Recognition 2010, page 626</rawString>
</citation>
<citation valid="true">
<authors>
<author>Yahoo Japan</author>
</authors>
<title>Yahoo Crowd Sourcing.</title>
<date>2013</date>
<note>Available: http://crowdsourcing.yahoo.co.jp/</note>
<contexts>
<context position="12120" citStr="Japan, 2013" startWordPosition="1916" endWordPosition="1917">unction,  is set as a ratio of the text score and an image score to combine two scores appropriately. We introduced  as a reliability ratio parameter of the scores by the text processor and the scores by the image processor. 4 Data We prepared user annotation data and image annotation data that we used as training data and evaluation data. User annotation data are input data for the text processor, whereas image annotation data are for the image processor. As it is required to prepare a huge number of annotated data as a training corpus, the data is annotated by Yahoo Crowd Sourcing (Yahoo! Japan, 2013). Yahoo Crowd Sourcing is a Japanese crowd sourcing service similar to Amazon Mechanical Turk (Amazon, 2005). Therefore the annotation process aims to obtain annotation based on human recognition rather than to explore truth about users and images of twitter. 4.1 User Annotation Data We first collected Japanese Twitter users according to their streaming tweets. We ignored heavy users and Twitter bots. A random sampling of tweets revealed that tweets from heavy users include much information that is not useful for profile inference such as short representations of their actions (e.g. “Going to </context>
</contexts>
<marker>Japan, 2013</marker>
<rawString>Yahoo! Japan. 2013. Yahoo Crowd Sourcing. Available: http://crowdsourcing.yahoo.co.jp/</rawString>
</citation>
<citation valid="true">
<authors>
<author>Dengsheng Zhang</author>
</authors>
<title>Md Monirul Islam, Guojun Lu.</title>
<date>2012</date>
<journal>Pattern Recognition</journal>
<volume>45</volume>
<pages>346--362</pages>
<marker>Zhang, 2012</marker>
<rawString>Dengsheng Zhang, Md Monirul Islam, Guojun Lu. 2012. A review on automatic image annotation, Pattern Recognition 45, pages 346-362</rawString>
</citation>
</citationList>
</algorithm>
</algorithms>
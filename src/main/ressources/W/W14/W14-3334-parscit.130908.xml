<?xml version="1.0" encoding="UTF-8"?>
<algorithms version="110505">
<algorithm name="SectLabel" version="110505">
<variant no="0" confidence="0.000030">
<title confidence="0.984399">
Estimating Word Alignment Quality for SMT Reordering Tasks
</title>
<author confidence="0.994177">
Sara Stymne J¨org Tiedemann Joakim Nivre
</author>
<affiliation confidence="0.9954665">
Uppsala University
Department of Linguistics and Philology
</affiliation>
<email confidence="0.970799">
firstname.lastname@lingfil.uu.se
</email>
<sectionHeader confidence="0.994065" genericHeader="abstract">
Abstract
</sectionHeader>
<bodyText confidence="0.99993524">
Previous studies of the effect of word
alignment on translation quality in SMT
generally explore link level metrics only
and mostly do not show any clear connec-
tions between alignment and SMT qual-
ity. In this paper, we specifically inves-
tigate the impact of word alignment on
two pre-reordering tasks in translation, us-
ing a wider range of quality indicators
than previously done. Experiments on
German–English translation show that re-
ordering may require alignment models
different from those used by the core trans-
lation system. Sparse alignments with
high precision on the link level, for trans-
lation units, and on the subset of cross-
ing links, like intersected HMM models,
are preferred. Unlike SMT performance
the desired alignment characteristics are
similar for small and large training data
for the pre-reordering tasks. Moreover,
we confirm previous research showing that
the fuzzy reordering score is a useful and
cheap proxy for performance on SMT re-
ordering tasks.
</bodyText>
<sectionHeader confidence="0.998876" genericHeader="introduction">
1 Introduction
</sectionHeader>
<bodyText confidence="0.99992437037037">
Word alignment is a key component in all state-of-
the-art statistical machine translation (SMT) sys-
tems, and there has been some work exploring the
connection between word alignment quality and
translation quality (Och and Ney, 2003; Fraser and
Marcu, 2007; Lambert et al., 2012). The standard
way to evaluate word alignments in this context is
by using metrics like alignment error rate (AER)
and F-measure on the link level, and the general
conclusion appears to be that translation quality
benefits from alignments with high recall (rather
than precision), at least for large training data. Al-
though many other ways of measuring alignment
quality have been proposed, such as working on
translation units (Ahrenberg et al., 2000; Ayan and
Dorr, 2006; Søgaard and Kuhn, 2009) or using link
degree and related measures (Ahrenberg, 2010),
these methods have not been used to study the re-
lation between alignment and translation quality,
with the exception of Lambert et al. (2012).
Word alignment is also used for many other
tasks besides translation, including term bank
creation (Merkel and Foo, 2007), cross-lingual
annotation projection for part-of-speech tagging
(Yarowsky et al., 2001), semantic roles (Pado and
Lapata, 2005), pronoun anaphora (Postolache et
al., 2006), and cross-lingual clustering (T¨ackstr¨om
et al., 2012). Even within SMT itself, there are
tasks such as reordering that often make crucial
use of word alignments. For instance, source lan-
guage reordering commonly relies on rules learnt
automatically from word-aligned data (e.g., Xia
and McCord (2004)). As far as we know, no one
has studied the impact of alignment quality on
these additional tasks, and it seems to be tacitly
assumed that alignments that are good for transla-
tion are also good for other tasks.
In this paper we set out to explore the impact
of alignment quality on two pre-reordering tasks
for SMT. In doing so, we employ a wider range of
quality indicators than is customary, and for refer-
ence these indicators are used also to assess over-
all translation quality. To allow an in-depth explo-
ration of the connections between several aspects
of word alignment and reordering, we limit our
study to one language pair, German–English. We
think this is a suitable language pair for studying
reordering since it has both short range and long
range reorderings. Our main focus is on using rel-
atively large training data, 2M sentences, but we
also report results with small training data, 170K
sentences. The main conclusion of our study is
that alignments that are optimal for translation are
not necessarily optimal for reordering, where pre-
</bodyText>
<page confidence="0.9761">
275
</page>
<note confidence="0.7143785">
Proceedings of the Ninth Workshop on Statistical Machine Translation, pages 275–286,
Baltimore, Maryland USA, June 26–27, 2014. c�2014 Association for Computational Linguistics
</note>
<bodyText confidence="0.999788133333333">
cision is of greater importance than recall. For
SMT the best alignments are different depending
on corpus size, but for the reordering tasks results
are stable across training data size.
In section 2 we discuss previous work related
to word alignment and SMT. In section 3, we in-
troduce the word alignment quality indicators we
use, and show experimental results for a number
of alignment systems on an SMT task. In sec-
tion 4, we turn to reordering for SMT and use
the same quality indicators to study the impact of
alignment quality on reordering quality. In section
5 we briefly describe results using small training
data. In section 6, we conclude and suggest direc-
tions for future work.
</bodyText>
<sectionHeader confidence="0.902901" genericHeader="method">
2 Word Alignment and SMT
</sectionHeader>
<bodyText confidence="0.999943882352941">
Word alignment is the task of relating words
in one language to words in the translation in
another language, see an example in Figure 1.
Word alignment models can be learnt automati-
cally from large corpora of sentence aligned data.
Brown et al. (1993) proposed the so-called IBM
models, which are still widely used. These five
models estimate alignments from corpora using
the expectation-maximization algorithm, and each
model adds some complexity. Model 4 is com-
monly used in SMT systems. There have been
many later suggestions of alternatives to these
models. These are often alternatives to model 2,
such as the HMM model (Vogel et al., 1996) and
fast align (Dyer et al., 2013).
All these generative models produce directional
alignments where one word in the source can be
linked to many target words (1–m links) but not
vice versa. It is generally desirable to also allow
n–1 and n–m links, and to achieve this it is com-
mon practice to perform word alignment in both
directions and to symmetrize them using some
heuristic. A number of common symmetrization
strategies are described in Table 1 (Koehn et al.,
2005). There are also other alternatives, such as
the refined method (Och and Ney, 2003), or link
deletion from the union (Fossum et al., 2008).
There is also a wide range of alternative ap-
proaches to word alignment. For example, various
discriminative models have been proposed in the
literature (Liu et al., 2005; Moore, 2005; Taskar
et al., 2005). Their advantage is that they may
integrate a wide range of features that may lead
to improved alignment quality. However, most of
</bodyText>
<table confidence="0.504949833333333">
Symmetrization Description
int: intersection ATS ∩ AST
uni: union ATS ∪ AST
gd: grow-diag intersection plus adjacent links
from the union if both linked
words are unaligned
gdf: grow-diag-final gd with links from the union
added in a final step if either
linked word is unaligned
gdfa: gd with links from the union
grow-diag-final-and added in a final step if both linked
words are unaligned
</table>
<tableCaption confidence="0.763677">
Table 1: Symmetrization strategies for word align-
ments ATS and AST in two directions
</tableCaption>
<bodyText confidence="0.9997578125">
these models require external tools (for creating
linguistic features) and manually aligned training
data, which we do not have for our data sets (be-
sides the data we need for evaluation). Investigat-
ing these types of models are outside the scope of
our current work.
Word alignments are used as an important
knowledge source for training SMT systems. In
word-based SMT, the parameters of the gener-
ative word alignment models are essentially the
translation model of the system. In phrase-based
SMT (PBSMT) (Koehn et al., 2003), which is
among the state-of-the-art systems today, word
alignments are used as a basis for extracting
phrases and estimating phrase alignment probabil-
ities. Similarly, word alignments are also used for
estimating rule probabilities in various kinds of hi-
erarchical and syntactic SMT (Chiang, 2007; Ya-
mada and Knight, 2002; Galley et al., 2004).
Intrinsic evaluation of word alignment is gener-
ally based on a comparison to a gold standard of
human alignments. Based on the gold standard,
metrics like precision, recall and F-measure can
be calculated for each alignment link, see Eqs. 1–
2, where A are hypothesized alignment links and
G are gold standard links. Another common met-
ric is alignment error rate (AER) (Och and Ney,
2000), which is based on a distinction between
sure, S, and possible, P, links in the gold stan-
dard. 1−AER is identical to balanced F-measure
when the gold standard does not make a distinc-
tion between S and P.
</bodyText>
<equation confidence="0.999041166666667">
Precision(A, G) = |G ∩ A |(1)
|A|
Recall(A, G) = |G ∩ A |(2)
|G|
AER = 1 − |P ∩ A |+ |S ∩ A |(3)
|S |+ |A|
</equation>
<page confidence="0.997832">
276
</page>
<figure confidence="0.999594">
Crossing = 8
SKDT = 8/66 ≈ 0.65
6 1–1 links
3 multi links
0 null links
</figure>
<figureCaption confidence="0.999931">
Figure 1: An example alignment illustrating n–1, 1–m and crossing links.
</figureCaption>
<bodyText confidence="0.999874470588235">
The relation between word alignment qual-
ity and PBSMT has been studied by some re-
searchers. Och and Ney (2000) looked at the im-
pact of IBM and HMM models on the alignment
template approach (Och et al., 1999) in terms of
AER. They found that AER correlates with human
evaluation of sentence level quality, but not with
word error rate. Fraser and Marcu (2007) found
that there is no correlation between AER and Bleu
(Papineni et al., 2002), especially not when the P-
set is large. They found that a balanced F-measure
is a better indicator of Bleu, but that a weighted
F-measure is even better (see Eq. 4) mostly with
a higher weight for recall than for precision. This
weight, however, needs to be optimized for each
data set, language pair, and gold standard align-
ment separately.
</bodyText>
<equation confidence="0.990376">
�F(A, G, α) = Precision(A,G) + Recall(A,G)) (4)
</equation>
<bodyText confidence="0.9998476">
Ayan and Dorr (2006) on the other hand found
some evidence for the importance of precision
over recall. However, they used much smaller
training data than Fraser and Marcu (2007). They
also suggested using a measure called consistent
phrase error-rate (CPER), but found that it was
hard to assess the impact of alignment on MT, both
with AER and CPER. Lambert et al. (2012) per-
formed a study where they investigated the effect
of word alignment on MT using a large number of
word alignment indicators. They found that there
was a difference between large and small datasets
in that alignment precision was more important
with small data sets, and recall more important
with large data sets. Overall they did not find any
indicator that was significant over two language
pairs and different corpus sizes. There were more
significant indicators for large datasets, however.
Most researchers who propose new alignment
models perform both a gold standard evalua-
tion and an SMT evaluation (Liang et al., 2006;
Ganchev et al., 2008; Junczys-Dowmunt and Szał,
2012; Dyer et al., 2013). The relation between the
two types of evaluation is often quite weak. Sev-
eral of these studies only show AER on their gold
standard, despite its well-known shortcomings.
Even though many studies have shown some
relation between translation quality and AER or
weighted F-measure, it has rarely been investi-
gated thoroughly in its own right, and, as far as we
are aware, not for other tasks than SMT. Further-
more, most of these studies considers nothing else
but link level agreement. In this paper we take a
broader view on alignment quality and explore the
effect of other types of quality indicators as well.
</bodyText>
<sectionHeader confidence="0.992317" genericHeader="method">
3 Word Alignment Quality Indicators
</sectionHeader>
<bodyText confidence="0.999606333333333">
We investigate four groups of quality indicators.
The first group is the classic group where met-
rics are calculated on the alignment link level,
which has been used in several studies. In our
experiments we use a gold standard that does not
make use of distinctions between sure and possible
links, as suggested by Fraser and Marcu (2007).
With this, we can calculate the standard metrics
P(recision) R(ecall) and F(-measure). We will
mainly use balanced F-measure, but occasionally
also report weighted F-measure. As noted before,
1−AER is equivalent to balanced F when only
sure links are used, and will thus not be reported
separately.
Søgaard and Kuhn (2009) and Søgaard and Wu
(2009) suggested working on the translation unit
(TU) level, instead of the link level. A translation
unit, or cept (Goutte et al., 2004), is defined as
a maximally connected subgraph of an alignment.
In Figure 1, the twelve links form nine translation
units. Søgaard and Wu (2009) suggest the metric
TUER, translation unit error rate, shown in Eq. 5,
where AU are hypothesized translation units, and
GU are gold standard translation units.1 They use
TUER to establish lower bounds for the cover-
age of alignments from different formalisms, not
to evaluate SMT. While they only use TUER, it
</bodyText>
<footnote confidence="0.99891475">
1TUER is similar to CPER (Ayan and Dorr, 2006), which
measures the error rate of extracted phrases. Due to how
phrase extraction handle null links, there are differences,
however.
</footnote>
<page confidence="0.996115">
277
</page>
<bodyText confidence="0.839052">
is also possible to define Precision, Recall and F-
measure over translation units in the same way as
for alignment links. We will use these three mea-
sures to get a broader picture of TUs in alignment
evaluation. Also in this case, 1−TUER is equiva-
lent to F-measure.
</bodyText>
<equation confidence="0.997503">
2|AU ∩ GU|
TUER(A, G) = 1 − (5)
|AU |+ |GU|
</equation>
<bodyText confidence="0.999164470588235">
The TU metrics are quite strict, since they re-
quire exact matching of TUs. Tiedemann (2005)
suggested the MWU metrics for word alignment
evaluation, which also consider partial matches
of annotated multi-word units, which is a similar
concept to TUs. In those metrics, precision and
recall grow proportionally to the number of cor-
rectly aligned words within translation units. Pro-
posed links are in this way scored according to
their overlap with translation units in the gold stan-
dard. Precision and recall are defined in Eqs. 6–7,
where overlap(XU, Y ) is the number of source
and target words in XU that overlap with transla-
tion units in Y normalized by the size of XU (in
terms of source and target words). Note, that TUs
need to overlap in source and target. Otherwise,
their overlap will be counted as zero.
</bodyText>
<equation confidence="0.997337666666667">
overlap(AU, G) (6)
|A|
overlap(GU, A)
</equation>
<bodyText confidence="0.999993117647059">
There have also been attempts at classifying
alignments in other ways, not related to a gold
standard. Ahrenberg (2010) proposed several
ways to categorize human alignments, including
link degree, reordering of links, and structural cor-
respondence. He used these indicators to profile
hand-aligned corpora from different domains. We
will not use structural correspondence, which re-
quires a dependency parser, and which we believe
is error prone when performed automatically. We
will use what we call link degree, i.e., how many
alignment links each word obtains. Ahrenberg
(2010) used a fine-grained scheme of the percent-
age for different degrees, including isomorphism
1–1, deletion 0–1, reduction m–1, and paraphrase
m–n. Similar link degree classes were used by
Lambert et al. (2012). In this work we will re-
duce these classes into three: 1–1 links, null links,
which combine the 0–1 and 1–0 cases, and multi
links where there are many words on at least one
side.
Ahrenberg (2010) also proposed to measure re-
orderings. He does this by calculating the percent-
age of links with crossings of different lengths. To
define this he only considers adjacent links in the
source using the distance between corresponding
target words, which means that his metric becomes
a directional measure. Reorderings of alignments
was also used by Genzel (2010), who used cross-
ing score, the number of crossing links, to rank
reordering rules. This is non-directional and sim-
pler to calculate than Ahrenberg (2010)’s metrics,
and implicitly covers length since a long distance
reordering leads to a higher number of pairwise
crossing links. Birch and Osborne (2011) sug-
gest using squared Kendall τ distance (SKTD), see
Eq. 8, where n is the number of links, as a basis
of LR-score, an MT metric that takes reordering
into account. They found that squaring τ better
explained reordering, than using only τ. In this
study we will use both, crossing score and SKTD.
Figure 1 shows these scores for an example sen-
tence. These two measures only tell us how much
reordering there is. To quantify this relative to the
gold standard we also report the absolute differ-
ence between the number of gold standard cross-
ings and system crossings, which we call Crossd-
iff. To account for the quality of crossings, to some
extent, we will also report precision, recall, and F-
measure for the subset of translation units that are
involved in a crossing.
</bodyText>
<equation confidence="0.9816315">
SKTD = . I |crossing link pairs |(8)
v (n2 − n)/2
</equation>
<subsectionHeader confidence="0.998903">
3.1 Alignment Experiments
</subsectionHeader>
<bodyText confidence="0.999993882352941">
We perform all our experiments for German–
English. The alignment indicators are calculated
on a corpus of 987 hand aligned sentences (Pado
and Lapata, 2005). The gold standard contains
explicit null links, which the symmetrized auto-
matic alignments do not. To allow a straightfor-
ward comparison we consistently remove all null
links when comparing system alignments to the
gold standard.
For creating the automatic alignments we used
GIZA++ (Och and Ney, 2003) to compute direc-
tional alignments for model 2–4 and the HMM
model, and fast align (fa) (Dyer et al., 2013) as
newer alternatives to model 2. These models re-
quire large amounts of data to be estimated reli-
ably. To achieve this we concatenated the gold
standard with the large SMT training data (see
</bodyText>
<equation confidence="0.979861166666667">
�PMWU =
AU ∈A
�RMWU =
GU ∈G
(7)
|G|
</equation>
<page confidence="0.993967">
278
</page>
<table confidence="0.983997137931034">
Table 2: Values for alignment quality indicators for the different alignments, where 2–4, HMM, and fa are alignment models, and symmetrization strategies refer
to Table 1
Total SKTD P R F Crossdiff
30163 .292 – – – 0
11229 .251 .663 .522 .584 18934
8077 .221 .709 .417 .525 22086
21823 .270 .446 .444 .445 8340
26977 .275 .529 .562 .545 3044
17399 .246 .584 .475 .524 12764
34309 .379 .351 .445 .392 4146
71223 .349 .305 .396 .345 41060
61823 .355 .381 .523 .441 31660
38285 .396 .336 .407 .368 8122
10064 .267 .551 .463 .503 20099
9724 .246 .568 .471 .515 20439
21886 .278 .492 .523 .507 8277
21966 .259 .583 .517 .548 8197
14334 .231 .621 .411 .495 15829
25616 .279 .419 .473 .444 6718
27119 .294 .451 .561 .500 4547
20384 .247 .439 .465 .452 9779
59592 .338 .321 .438 .370 29429
36881 .309 .473 .499 .486 6718
18578 .242 .454 .447 .450 11585
47216 .322 .459 .585 .514 17053
57408 .333 .456 .564 .504 27245
12682 .274 .553 .521 .537 17481
50834 .344 .387 .552 .455 20671
42264 .319 .459 .475 .467 12101
</table>
<figure confidence="0.98409949122807">
Link crossings
1-1 null multi
.542 .328 .130
.500 .500 .000
.561 .439 .000
.559 .441 .000
.476 .524 .000
.531 .469 .000
.608 .270 .122
.589 .101 .310
.561 .260 .180
.524 .289 .186
.499 .407 .094
.548 .258 .194
.590 .216 .194
.673 .172 .154
.587 .214 .199
.579 .251 .169
.684 .067 .249
.682 .074 .244
.548 .111 .346
.605 .247 .148
.684 .188 .128
.565 .337 .098
.625 .002 .373
.712 .064 .225
.706 .070 .223
.584 .002 .413
Link degree
.849 .712 .774
.857 .776 .814
.854 .742 .794
.806 .827 .816
.817 .855 .836
.839 .886 .862
.802 .839 .820
.780 .841 .809
.786 .870 .826
.857 .851 .854
.761 .876 .814
.810 .922 .862
.851 .887 .869
.769 .865 .814
.857 .881 .869
.781 .867 .822
.901 .813 .855
.920 .750 .827
.878 .820 .848
.813 .901 .855
.785 .846 .815
.774 .869 .818
.791 .889 .837
.778 .891 .831
.826 .921 .871
P R F
– – –
MWU
</figure>
<table confidence="0.995161277777778">
15362 .701 .631 .664
16511 .599 .579 .589
15831 .709 .658 .682
17469 .575 .588 .582
18732 .621 .681 .650
18561 .645 .701 .672
17612 .575 .594 .584
17089 .511 .512 .512
18354 .588 .632 .609
18269 .628 .673 .650
17454 .524 .536 .530
17411 .636 .649 .642
16385 .512 .491 .504
16573 .707 .686 .697
14871 .768 .669 .715
15997 .696 .652 .673
17043 .589 .588 .588
16955 .685 .681 .683
16660 .651 .635 .643
15864 .503 .468 .485
17391 .592 .603 .597
15980 .649 .608 .628
16529 .743 .720 .731
17644 .650 .672 .661
17611 .670 .692 .681
Total P R F
17068 – – –
Translation units
15997 .857 .606 .710
22754 .768 .772 .770
23148 .702 .719 .710
23049 .736 .749 .742
23717 .693 .726 .710
30712 .566 .769 .652
27920 .670 .827 .740
15362 .850 .577 .687
16573 .860 .630 .727
22882 .702 .710 .706
23704 .751 .787 .769
29050 .591 .758 .664
23886 .725 .765 .744
26724 .633 .748 .686
28093 .636 .789 .704
27951 .612 .756 .676
16529 .903 .660 .763
21961 .757 .734 .745
23840 .687 .724 .705
26575 .660 .775 .713
26529 .693 .812 .748
24712 .707 .772 .738
14871 .922 .606 .731
19430 .812 .698 .751
20554 .799 .726 .761
Total P R F
</table>
<figure confidence="0.726709035714286">
22629 – – –
Alignment links
HMM-gdfa
HMM-gdf
HMM-uni
HMM-int
HMM-gd
fa-gdfa
2-gdfa
3-gdfa
4-gdfa
fa-gdf
fa-uni
2-gdf
3-gdf
4-gdf
fa-int
fa-gd
2-uni
3-uni
4-uni
2-int
3-int
4-int
2-gd
3-gd
4-gd
gold
</figure>
<page confidence="0.988213">
279
</page>
<bodyText confidence="0.999898777777778">
Section 3.2) of 2M sentences during alignment.
For symmetrization we used all methods in Table
1, as implemented in the Moses toolkit (Koehn et
al., 2007) and in fast align (Dyer et al., 2013).
Based on the automatically aligned gold stan-
dard, we calculated all alignment indicators for all
settings. The complete results can be found in
Table 2, where we have ordered the symmetriza-
tion methods with the most sparse, intersection, on
top. Overall we can see that while several of the
alignment methods create a much higher number
of alignment links than the gold standard, they do
not produce many more translation units. This is
very interesting and indicates why link level statis-
tics may not be accurate enough to predict the per-
formance of certain downstream applications. As
expected, the metric scores for translation units
are lower than for link level metrics. This is
partly due to the fact that these measures do not
count any partially correct links; the MWU met-
rics which considers partial matches often have
higher scores than link level metrics. Another
finding is that the number of crossings vary a lot
with more than twice as many as the reference for
model2+union, and less than three times as many
for HMM+intersection. The HMM and fa models
have fewer reorderings than the IBM models.
We are now interested in the relation between
alignment evaluation on the link level and on the
translation unit level, which has not been thor-
oughly investigated before. Table 3 shows the cor-
relations between the various metrics. Both preci-
sion and F-measure at the link level have signifi-
cant correlations to all TU metrics. Link level re-
call, on the other hand, is significantly negatively
correlated with TU precision, but not significantly
correlated to any other TU metric, not even TU re-
call. Link level precision is thus highly important
for matching translation units. We can also note
here that while there is a trade-off between preci-
sion and recall on link level, this is not the case for
translation units, which can have both high pre-
cision and high recall. The same is not true for
MWU, that allows partial matching, where we also
see at least some precision/recall trade-off.
</bodyText>
<subsectionHeader confidence="0.995826">
3.2 SMT Experiments
</subsectionHeader>
<bodyText confidence="0.994968">
For reference, we first study the impact of align-
ment on SMT performance. Our SMT system
is a standard PBSMT system trained on WMT13
</bodyText>
<table confidence="0.9997802">
Translation unit
Link level ↓ P R F
P .95 .77 .90
R −.57 −.22 −.42
F .70 .90 .83
</table>
<tableCaption confidence="0.70208825">
Table 3: Pearson correlations between gold stan-
dard word alignment evaluation on the link level
and on translation unit level. Significant correla-
tions are marked with bold (&lt; 0.01).
</tableCaption>
<bodyText confidence="0.998106916666667">
data.2 We trained a German–English system on
2M sentences from Europarl and News Commen-
tary. We used the target side of the parallel corpus
and the SRILM toolkit (Stolcke, 2002) to train a 5-
gram language model. For training the translation
model and for decoding we used the Moses toolkit
(Koehn et al., 2007). We applied a standard feature
set consisting of a language model feature, four
translation model features, word penalty, phrase
penalty, and distortion cost. For tuning we used
minimum error-rate training (Och, 2003). In or-
der to minimize the risk of tuning influencing the
results, we used a fixed set of weights for each
experiment, tuned on a model 4+gdfa alignment.3
For tuning we used newstest2009 with 2525 sen-
tences, and for testing we used newstest2013 with
3000 sentences. Evaluation was performed using
the Bleu metric (Papineni et al., 2002). The same
system setup was used for the SMT systems with
reordering.
Table 4 shows the results on the SMT task.
Model 3 and 4 with gd/gdfa symmetrization yield
the highest scores. There is a larger difference be-
tween systems with different symmetrization than
between systems with different alignment models.
The sparse intersection symmetrization gives the
poorest results. The top row in Table 5 shows
correlations between Bleu and all word alignment
quality indicators. There are significant correla-
tions with link level recall. A weighted link level
F-measure with α = 0.3 gives a significant corre-
lation of .72, which confirms the results of Fraser
and Marcu (2007). There are no significant corre-
lations with the TU metrics but a positive correla-
tion with the number of TUs. For the MWU met-
rics the correlations are similar to the link level,
</bodyText>
<footnote confidence="0.696092285714286">
2http://www.statmt.org/wmt13/
translation-task.html
3This could have disfavored the other alignments, so we
also performed control experiments where we ran separate
tunings for each alignment. While the absolute results varied
somewhat, the correlations with alignment indicators were
stable.
</footnote>
<page confidence="0.981956">
280
</page>
<table confidence="0.998817">
m2 m3 m4 HMM fa
inter 18.1 19.1 19.3 18.8 18.9
gd 20.4 20.9 20.9 20.5 20.6
gdfa 20.4 20.7 20.8 20.5 20.5
gdf 19.4 19.7 20.1 19.9 20.0
union 19.2 19.6 19.8 19.7 20.0
</table>
<tableCaption confidence="0.925751">
Table 4: Baseline Bleu scores for different sym-
metrization heuristics
</tableCaption>
<bodyText confidence="0.999671615384615">
suggesting that they measure similar things. Intu-
itively it seems important for SMT to match full
translation units, but it might be the case that the
phrase extraction strategy is robust as long as there
are partial matches. There are no significant cor-
relations with link degree or link crossings, ex-
cept a negative correlation with Crossdiff, which
means that it is good to have a similar number of
crossings as the baseline. These results confirm
results from previous studies that link level mea-
sures, especially recall and weighted F-measure
show some correlation with SMT quality whereas
precision does not.
</bodyText>
<sectionHeader confidence="0.976014" genericHeader="method">
4 Reordering Tasks for SMT
</sectionHeader>
<bodyText confidence="0.999991721518988">
Reordering is an important part of any SMT sys-
tem. One way to address it is to add reorder-
ing models to standard PBSMT systems, for in-
stance lexicalized reordering models (Koehn et al.,
2005), or to directly model reordering in hierarchi-
cal (Chiang, 2007) or syntactic translation models
(Yamada and Knight, 2002). Another type of ap-
proach is preordering, where the source side is re-
ordered to mimic the target side before translation.
There have also been approaches where reordering
is modeled as part of the evaluation of MT systems
(Birch and Osborne, 2011).
We can distinguish two main types of ap-
proaches to preordering in SMT, either by using
hand-written rules, which often operate on syn-
tactic trees (Collins et al., 2005), or by reordering
rules that are learnt automatically based on a word
aligned corpus (Xia and McCord, 2004). The lat-
ter approach is of interest to us, since it is based
on word alignments.
There has been much work on automatic learn-
ing of reordering rules, which can be based on dif-
ferent levels of annotation, such as part-of-speech
tags (Rottmann and Vogel, 2007; Niehues and
Kolss, 2009; Genzel, 2010), chunks (Zhang et
al., 2007) or parse trees (Xia and McCord, 2004).
In general, all these approaches lead to improve-
ments of translation quality. The reordering is
always applied on the translation input. It can
also be applied on the source side of the train-
ing corpora, which sometimes improves the results
(Rottmann and Vogel, 2007), but sometimes does
not make a difference (Stymne, 2012). When pre-
ordering is performed on the translation input, it
can be presented to the decoder as a 1-best reorder-
ing (Xia and McCord, 2004), as an n-best list (Li
et al., 2007), or as a lattice of possible reorderings
(Rottmann and Vogel, 2007; Zhang et al., 2007).
In the preordering studies cited above it is often
not even stated which alignment model was used.
A few authors mention the alignment tool that has
been applied but no comparison between different
alignment models is performed in any of the pa-
pers we are aware of. Li et al. (2007), for exam-
ple, simply state that they used GIZA++ and gdf
symmetrization and that they removed less proba-
ble multi links. Lerner and Petrov (2013) use the
intersection of HMM alignments and claims that
model 4 did not add much value. Genzel (2010)
did mention that using a standard model 4 was
not successful for his rule learning approach. In-
stead he used filtered model-1-alignments, which
he claims was more successful. However, there
are no further analyses or comparisons between
the alignments reported in any of these papers.
Another type of approach to reordering is to
only reorder the data in order to improve word
alignments, and to restore the original word or-
der before training the SMT system. This type
of approach has the advantage that no modifica-
tions are needed for the translation input. This ap-
proach has also been used both with hand-written
rules (Carpuat et al., 2010; Stymne et al., 2010)
and with rules based on initial word alignments on
non-reordered texts (Holmqvist et al., 2009). For
the latter approach a small study of the effect of gd
and gdfa symmetrizations was presented, which
only showed small variations in quality scores
(Holmqvist et al., 2012).
Below we present the two tasks that we study
in this paper: part-of-speech-based reordering for
creating input lattices for SMT and alignment-
based reordering for improving phrase-tables. We
evaluate the performance of these tasks in rela-
tion to the use of different alignment models and
symmetrization heuristics. For these tasks we are
mainly interested in the full translation task, for
which we report Bleu scores. In addition we also
show fuzzy reordering score (FRS), which focuses
</bodyText>
<page confidence="0.99473">
281
</page>
<table confidence="0.999846857142857">
Total Alignment links F Total Translation units F MWU F
P R P R P R
SMT, Bleu .33 −.25 .56 .46 .65 −.20 .16 −.02 −.29 .59 .44
POSReo, FRS −.80 .87 −.49 .75 −.23 .90 .81 .89 .82 −.45 .22
POSReo, Bleu −.64 .74 −.27 .85 .05 .80 .80 .86 .67 −.23 .35
AlignReo, FRS −.77 .88 −.43 .84 −.11 .90 .88 .92 .81 −.37 .31
AlignReo, Bleu −.81 .83 −.58 .61 −.24 .75 .64 .72 .71 −.53 .04
Link degree Total SKTD Link crossings F Crossdiff
1-1 null multi P R
SMT, Bleu .33 −.30 .21 −.05 −.14 −.09 .25 .07 −.63
POSReo, FRS −.41 .84 −.89 −.81 −.70 .90 .21 .86 −.41
POSReo, Bleu −.17 .66 −.80 −.71 −.60 .79 .42 .89 −.49
AlignReo, FRS −.32 .77 −.86 −.80 −.73 .94 .27 .92 −.38
AlignReo, Bleu −.57 .83 −.79 −.93 −.91 .86 −.07 .69 −.52
</table>
<tableCaption confidence="0.80671">
Table 5: Pearson correlations between different alignment characteristics and scores for the translation
and reordering tasks. Significant correlations are marked with bold (&lt; 0.01).
</tableCaption>
<bodyText confidence="0.999822888888889">
only on the reordering component (Talbot et al.,
2011). It compares a system reordering to a refer-
ence reordering, by measuring how many chunks
that have to be moved to get an identical word or-
der, see Eq. 9, where C is the number of con-
tiguously aligned chunks, and M the number of
words. To find the reference ordering we apply
the method of Holmqvist et al. (2009), described
in Section 4.2, to the gold standard alignment.
</bodyText>
<equation confidence="0.993634666666667">
C − 1
FRS = 1 − (9)
M − 1
</equation>
<subsectionHeader confidence="0.983376">
4.1 Part-of-Speech-Based Reordering
</subsectionHeader>
<bodyText confidence="0.999958947368421">
Our first reordering task is a part-of-speech-based
preordering method described by Rottmann and
Vogel (2007) and Niehues and Kolss (2009),
which was successfully used for German–English
translation. Rules are learnt from a word aligned
POS-tagged corpus. Based on the alignments, tag
patterns are identified that give rise to specific re-
orderings. These patterns are then scored based
on relative frequency.4 The rules are then applied
to the translation input to create a reordering lat-
tice, with normalized edge scores based on rule
scores. In our experiments we only use rules with
a score higher than 0.2, to limit the size of the lat-
tices. For calculating FRS, we pick the highest
scoring 1-best word order from the lattices.
We learn rules from our entire SMT training
corpus varying alignment models and symmetriza-
tion. To investigate only the effect of word align-
ment for creating reordering rules, we do not
</bodyText>
<footnote confidence="0.912969">
4Note that we do not use words (Rottmann and Vogel,
2007) or wild cards (Niehues and Kolss, 2009) in our rules.
</footnote>
<table confidence="0.999027">
m2 m3 m4 HMM fa
inter .577 .575 .581 .596 .567
gd .555 .559 .570 .589 .546
gdfa .540 .540 .559 .579 .539
gdf .439 .499 .542 .560 .495
union .442 .492 .544 .563 .486
</table>
<tableCaption confidence="0.968705">
Table 6: Fuzzy reordering scores for part-of-
speech-based reordering for different alignments
</tableCaption>
<table confidence="0.991593333333333">
m2 m3 m4 HMM fa
inter 21.4 21.6 21.8 21.6 21.6
gd 21.5 21.6 21.6 21.7 21.5
gdfa 21.4 21.5 21.7 21.7 21.4
gdf 20.3 21.0 21.4 21.5 21.0
union 20.3 21.5 21.6 21.5 20.8
</table>
<tableCaption confidence="0.910662">
Table 7: Bleu scores for part-of-speech-based re-
ordering for different alignments
</tableCaption>
<bodyText confidence="0.998767736842105">
change the SMT system, which is trained based
on model 4+gdfa alignments. The only thing that
varies for the translation task is thus the input lat-
tice given to this SMT system.
The results are shown in Tables 6 and 7. Most
Bleu scores are better than using the same SMT
system without preordering, with a Bleu score of
20.8. The results on FRS and Bleu are highly cor-
related at .94, despite the fact that we use a lattice
as SMT input, and the 1-best order for FRS. For
both metrics sparse symmetrization like intersec-
tion and gd performs best. Model 4 and HMM
perform best with similar Bleu scores, but FRS is
better for the HMM model.
Table 5 shows the correlations with the word
alignment indicators, in the rows labeled POSReo.
There are strong correlations with all TU metrics,
contrary to the SMT task. There are also signifi-
cant correlations with link level precision and bal-
</bodyText>
<page confidence="0.991528">
282
</page>
<bodyText confidence="0.999799761904762">
anced F-measure. The correlation with weighted
link level F-measure is even higher, .91 for α =
0.6. This is an indication that this algorithm is
more sensitive to precision than the SMT task. As
for the SMT task, the correlation patterns are simi-
lar for the MWU metrics as for link level. For link
degree, null alignments are correlated, but there is
a negative correlation for multi links. The correla-
tions with the number of crossings and SKTD are
negative, which means that it is better to have a
low number of crossings. This may seem counter-
intuitive, but note in Table 1 that many alignments
have a much higher number of crossings than the
baseline. The precision of the crossing links is
highly correlated with performance on this task,
while the recall is not. This tells us that it is impor-
tant that the crossings we find in the alignment are
good, but that it is less important that we find all
crossings. This makes sense since the rule learner
can then learn at least a subset of all existing cross-
ings well.
</bodyText>
<subsectionHeader confidence="0.919033">
4.2 Reordering for Alignment
</subsectionHeader>
<bodyText confidence="0.999961607142857">
In our second reordering task we investigate
alignment-based reordering for improving phrase-
tables (Holmqvist et al., 2009; Holmqvist et al.,
2012). This strategy first performs a word align-
ment, based on which the source text is reordered
to remove all crossings. A second alignment is
trained on the reordered data, which is then re-
stored to the original order before training the
full SMT system. In Holmqvist et al. (2012) it
was shown that this strategy leads to improve-
ments in link level recall and F-measure as well
as small translation improvements for English–
Swedish. It also led to small improvements for
German–English translation.
Similar to the previous experiments, we now
vary alignment models and symmetrization that
are used for reordering during the first step. The
second step is kept the same using model 4+gdfa
in order to focus on the reordering step in our com-
parisons. Tables 8 and 9 show the results of these
experiments. In this case the reordering strat-
egy was not successful, always producing lower
Bleu scores than the baseline of 20.8. However,
there are some interesting differences in these out-
comes. On this task as well, FRS and Bleu scores
are highly correlated at .89, which was expected,
since this method directly uses the reordered data
to train phrase tables. For the best systems, the
</bodyText>
<table confidence="0.997283833333333">
m2 m3 m4 HMM fa
inter .583 .604 .669 .654 .598
gd .548 .583 .646 .642 .561
gdfa .532 .564 .633 .645 .553
gdf .422 .482 .571 .574 .474
union .395 .455 .552 .545 .452
</table>
<tableCaption confidence="0.989391">
Table 8: Fuzzy reordering scores for alignment-
based reordering for different alignments
</tableCaption>
<table confidence="0.9998795">
m2 m3 m4 HMM fa
inter 19.5 19.5 19.9 20.2 19.4
gd 19.3 19.5 19.8 20.2 19.3
gdfa 19.1 19.2 19.6 20.0 19.2
gdf 18.3 18.2 18.6 19.0 18.9
union 17.4 17.8 18.4 18.8 18.8
</table>
<tableCaption confidence="0.9675595">
Table 9: Bleu scores for alignment-based reorder-
ing for different alignments
</tableCaption>
<bodyText confidence="0.999602347826087">
FRS scores are higher than for the previous task,
see Table 6, which shows that reordering directly
based on alignments is easier than learning and ap-
plying rules based on them, given suitable align-
ments. On this task, again, the sparser alignments
are the most successful on both tasks. Here, how-
ever, the HMM model gives the best Bleu scores,
and similar FRS scores to model 4.
Table 5 shows the correlations with the word
alignment indicators, in the rows labeled Align-
Reo. The correlation patterns are very similar
to the previous task. A few more indicators are
significantly negatively correlated with alignment-
based reordering than with the other reordering
tasks and metrics. The performance on our two
reordering tasks are significantly correlated at .76.
Again alignments with good scores on TU met-
rics, link level precision and crossing link preci-
sion are preferable. For this task, the best correla-
tion with weighted link level F-measure is .86 for
α = 0.8. Again, we thus see that sparse align-
ments with high precision on all measures includ-
ing the crossing subset, are important.
</bodyText>
<sectionHeader confidence="0.868807" genericHeader="method">
5 Small Training Data
</sectionHeader>
<bodyText confidence="0.999779333333333">
Since previous work has suggested that training
data size influences the relation between align-
ment and SMT quality for small and large training
data (Lambert et al., 2012), we investigated this is-
sue also for our reordering tasks. We repeated all
our experiments on a small dataset, only the News
Commentary data from WMT13, with 170K sen-
tences. Due to space constraints we cannot show
all results in the paper, but the main findings are
</bodyText>
<page confidence="0.99597">
283
</page>
<bodyText confidence="0.999180925">
summarized in this section.
To acquire alignment results we realigned the
gold standard concatenated with the smaller data,
to reflect the actual quality of alignment with a
small dataset. As expected the quality scores tend
to be lower with less data. Overall the same sys-
tems tend to perform good on each metric with the
small and large data, even though there is some
variation in the ranking between systems. On the
SMT task as well, the Bleu scores are lower, as
expected. In this case fast align is doing best fol-
lowed by model 4 and 3. The best symmetrization
is again gd and gdfa. There are also some differ-
ences in the correlation profile. Link recall and
number of translation units are no longer signifi-
cantly correlated, whereas the number of crossings
and SKTD are. The highest correlation for link
level F-measure is .60 for balanced F-measure,
showing that precision is equally important to re-
call with less data.
For the reordering tasks the scores are again
lower. The POS-based reorderings again help over
the baseline SMT, whereas the alignment-based
reordering leads to slightly lower scores. The cor-
relation profile look exactly the same for Bleu
for POS-based reordering. FRS for both tasks
and Bleu for alignment-based reordering have the
same correlation profiles as Bleu for alignment-
based reordering on large data. There are thus
very small differences in the word alignment qual-
ity indicators that are relevant with large and small
training data, while there are some differences on
the SMT task. For weighted link level F-measure,
the highest correlations are found with α = 0.6–
0.7 on the different metrics, again showing that
precision is more important than recall. For FRS
on both tasks and Bleu for alignment-based re-
ordering, model4 and HMM with intersection and
gd still perform best. For Bleu for POS-based re-
ordering, gdfa and model 3 also give good results.
</bodyText>
<sectionHeader confidence="0.996457" genericHeader="conclusions">
6 Conclusion and Future Work
</sectionHeader>
<bodyText confidence="0.999991814814815">
We have shown that the best combination of align-
ment and symmetrization models for SMT are not
the best models for reordering tasks in our ex-
perimental setting. For SMT, high recall is more
important than precision with large training data,
while precision and recall are of equal impor-
tance with small training data. This finding sup-
ports previous research (Fraser and Marcu, 2007;
Lambert et al., 2012). Translation unit metrics
are not predictive of SMT performance. For the
large data condition model 3 and 4 with gd and
gdfa symmetrization gave the best results, whereas
fast align with gd and gdfa was best with small
training data.
For the two preordering tasks we investigated,
however, link level weighted F-measure that gave
more weight to precision was important, as well as
all TU metrics. It was also important to have high
precision for the crossing subset of TUs. Hence,
it is more important to reliably find some cross-
ings than to find all crossings. This make sense
since the extracted rules or performed reorderings
are likely good in such cases, even if we are not
able to find all possible reorderings. In conclu-
sion, based on this study, we recommend intersec-
tion symmetrization with model 4 and HMM for
SMT reordering tasks.
We have studied two relatively different re-
ordering tasks with two training data sizes, but
found that they to a large extent prefer the same
types of alignments. Moreover, the results on
these two reordering tasks correlates strongly with
FRS, which is much cheaper to calculate than
SMT metrics that may even require retraining of
full SMT systems. This is consistent with Tal-
bot et al. (2011) who suggested FRS for preorder-
ing tasks. We thus would encourage developers
of alignment methods to not only give results for
SMT, but also for FRS, as a proxy for reordering
tasks. Furthermore, it is also useful to give results
on TU metrics in addition to link level metrics to
complement the evaluation.
In this paper, we have looked at existing genera-
tive alignment and symmetrization models. In fu-
ture work, we would also like to investigate other
models, including the removal of low-confidence
links, which has previously been proposed for pre-
reordering (Li et al., 2007; Genzel, 2010). Given
the results, it also seems motivated to develop
or adapt the existing models in general, to bet-
ter fit the properties of specific auxiliary tasks.
Furthermore, we need to validate our findings on
other language pairs, especially for non-related
languages with even more diverse word order.
</bodyText>
<sectionHeader confidence="0.998027" genericHeader="acknowledgments">
Acknowledgments
</sectionHeader>
<bodyText confidence="0.985052">
This work was supported by the Swedish strategic
research programme eSSENCE.
</bodyText>
<page confidence="0.996513">
284
</page>
<sectionHeader confidence="0.981171" genericHeader="references">
References
</sectionHeader>
<reference confidence="0.999582716981132">
Lars Ahrenberg, Magnus Merkel, Anna S˚agvall Hein,
and J¨org Tiedemann. 2000. Evaluation of word
alignment systems. In Proceedings of LREC, vol-
ume III, pages 1255–1261, Athens, Greece.
Lars Ahrenberg. 2010. Alignment-based profiling of
Europarl data in an English-Swedish parallel corpus.
In Proceedings of LREC, pages 3398–3404, Valetta,
Malta.
Necip Fazil Ayan and Bonnie J. Dorr. 2006. Going
beyond AER: An extensive analysis of word align-
ments and their impact on MT. In Proceedings of
Coling and ACL, pages 9–16, Sydney, Australia.
Alexandra Birch and Miles Osborne. 2011. Reorder-
ing metrics for MT. In Proceedings of ACL, pages
1027–1035, Portland, Oregon, USA.
Peter F. Brown, Stephen Della Pietra, Vincent J. Della
Pietra, and Robert L. Mercer. 1993. The mathe-
matics of statistical machine translation: Parameter
estimation. Computational Linguistics, 19(2):263–
311.
Marine Carpuat, Yuval Marton, and Nizar Habash.
2010. Improving Arabic-to-English statistical ma-
chine translation by reordering post-verbal subjects
for alignment. In Proceedings ofACL, Short Papers,
pages 178–183, Uppsala, Sweden.
David Chiang. 2007. Hierarchical phrase-based trans-
lation. Computational Linguistics, 33(2):202–228.
Michael Collins, Philipp Koehn, and Ivona Kuˇcerov´a.
2005. Clause restructuring for statistical machine
translation. In Proceedings of ACL, pages 531–540,
Ann Arbor, Michigan, USA.
Chris Dyer, Victor Chahuneau, and Noah A. Smith.
2013. A simple, fast, and effective reparameteriza-
tion of IBM model 2. In Proceedings of NAACL,
pages 644–648, Atlanta, Georgia, USA.
Victoria Fossum, Kevin Knight, and Steven Abney.
2008. Using syntax to improve word alignment pre-
cision for syntax-based machine translation. In Pro-
ceedings of WMT, pages 44–52, Columbus, Ohio.
Alexander Fraser and Daniel Marcu. 2007. Measuring
word alignment quality for statistical machine trans-
lation. Computational Linguistics, 33(3):293–303.
Michel Galley, Mark Hopkins, Kevin Knight, and
Daniel Marcu. 2004. What’s in a translation rule?
In Proceedings of NAACL, pages 273–280, Boston,
Massachusetts, USA.
Kuzman Ganchev, Jo˜ao V. Grac¸a, and Ben Taskar.
2008. Better alignments = better translations? In
Proceedings of ACL, pages 986–993, Columbus,
Ohio, USA.
Dmitriy Genzel. 2010. Automatically learning source-
side reordering rules for large scale machine trans-
lation. In Proceedings of Coling, pages 376–384,
Beijing, China.
Cyril Goutte, Kenji Yamada, and Eric Gaussier. 2004.
Aligning words using matrix factorisation. In Pro-
ceedings of ACL, pages 502–509, Barcelona, Spain.
Maria Holmqvist, Sara Stymne, Jody Foo, and Lars
Ahrenberg. 2009. Improving alignment for SMT
by reordering and augmenting the training corpus.
In Proceedings of WMT, pages 120–124, Athens,
Greece.
Maria Holmqvist, Sara Stymne, Lars Ahrenberg, and
Magnus Merkel. 2012. Alignment-based reordering
for SMT. In Proceedings of LREC, Istanbul, Turkey.
Marcin Junczys-Dowmunt and Arkadiusz Szał. 2012.
SyMGiza++: Symmetrized word alignment models
for statistical machine translation. In International
Joint Conference of Security and Intelligent Infor-
mation Systems, pages 379–390, Warsaw, Poland.
Philipp Koehn, Franz Josef Och, and Daniel Marcu.
2003. Statistical phrase-based translation. In Pro-
ceedings of NAACL, pages 48–54, Edmonton, Al-
berta, Canada.
Philipp Koehn, Amittai Axelrod, Alexandra Birch
Mayne, Chris Callison-Burch, Miles Osborne, and
David Talbot. 2005. Edinburgh system descrip-
tion for the 2005 IWSLT speech translation evalu-
ation. In Proceedings of the International Workshop
on Spoken Language Translation, Pittsburgh, Penn-
sylvania, USA.
Philipp Koehn, Hieu Hoang, Alexandra Birch, Chris
Callison-Burch, Marcello Federico, Nicola Bertoldi,
Brooke Cowan, Wade Shen, Christine Moran,
Richard Zens, Chris Dyer, Ondrej Bojar, Alexandra
Constantin, and Evan Herbst. 2007. Moses: Open
source toolkit for statistical machine translation. In
Proceedings of ACL, Demonstration Session, pages
177–180, Prague, Czech Republic.
Patrik Lambert, Simon Petitrenaud, Yanjun Ma, and
Andy Way. 2012. What types of word alignment
improve statistical machine translation? Machine
Translation, 26(4):289–323.
Uri Lerner and Slav Petrov. 2013. Source-side clas-
sifier preordering for machine translation. In Pro-
ceedings of EMNLP, pages 513–523, Seattle, Wash-
ington, USA.
Chi-Ho Li, Minghui Li, Dongdong Zhang, Mu Li,
Ming Zhou, and Yi Guan. 2007. A probabilistic ap-
proach to syntax-based reordering for statistical ma-
chine translation. In Proceedings of the 45th Annual
Meeting of the ACL, pages 720–727, Prague, Czech
Republic.
Percy Liang, Ben Taskar, and Dan Klein. 2006. Align-
ment by agreement. In Proceedings of NAACL,
pages 104–111, New York City, New York, USA.
</reference>
<page confidence="0.974689">
285
</page>
<reference confidence="0.999765837837839">
Yang Liu, Qun Liu, and Shouxun Lin. 2005. Log-
linear models for word alignment. In Proceedings of
ACL, pages 459–466, Ann Arbor, Michigan, USA.
Magnus Merkel and Jody Foo. 2007. Terminology
extraction and term ranking for standardizing term
banks. In Proceedings of the 16th Nordic Confer-
ence on Computational Linguistics, pages 349–354,
Tartu, Estonia.
Robert C. Moore. 2005. A discriminative framework
for bilingual word alignment. In Proceedings of
HLT and EMNLP, pages 81–88, Vancouver, British
Columbia, Canada.
Jan Niehues and Muntsin Kolss. 2009. A POS-based
model for long-range reorderings in SMT. In Pro-
ceedings of WMT, pages 206–214, Athens, Greece.
Franz Josef Och and Hermann Ney. 2000. A com-
parison of alignment models for statistical machine
translation. In Proceedings of Coling, pages 1086–
1090, Saarbr¨ucken, Germany.
Franz Josef Och and Hermann Ney. 2003. A sys-
tematic comparison of various statistical alignment
models. Computational Linguistics, 29(1):19–51.
Franz Josef Och, Christoph Tillmann, and Hermann
Ney. 1999. Improved alignment models for sta-
tistical machine translation. In Proceedings of the
Joint Conference of EMNLP and Very Large Cor-
pora, pages 20–28, College Park, Maryland, USA.
Franz Josef Och. 2003. Minimum error rate training
in statistical machine translation. In Proceedings of
ACL, pages 160–167, Sapporo, Japan.
Sebastian Pado and Mirella Lapata. 2005. Cross-
linguistic projection of role-semantic information.
In Proceedings of HLT and EMNLP, pages 859–866,
Vancouver, British Columbia, Canada.
Kishore Papineni, Salim Roukos, Todd Ward, and Wei-
Jing Zhu. 2002. BLEU: A method for automatic
evaluation of machine translation. In Proceedings
ofACL, pages 311–318, Philadelphia, Pennsylvania,
USA.
Oana Postolache, Dan Cristea, and Constantin Or˘asan.
a. 2006. Transferring coreference chains through
word alignment. In Proceedings of LREC, pages
889–892, Genoa, Italy.
Kay Rottmann and Stephan Vogel. 2007. Word re-
ordering in statistical machine translation with a
POS-based distortion model. In Proceedings of
the 11th International Conference on Theoretical
and Methodological Issues in Machine Translation,
pages 171–180, Sk¨ovde, Sweden.
Anders Søgaard and Jonas Kuhn. 2009. Empirical
lower bounds on alignment error rates in syntax-
based machine translation. In Proceedings of the
Third Workshop on Syntax and Structure in Statis-
tical Translation, pages 19–27, Boulder, Colorado,
USA.
Anders Søgaard and Dekai Wu. 2009. Empirical lower
bounds on translation unit error rate for the full class
of inversion transduction grammars. In Proceedings
of 11th International Conference on Parsing Tech-
nologies, pages 33–36, Paris, France.
Andreas Stolcke. 2002. SRILM – an extensible lan-
guage modeling toolkit. In Proceedings of ICSLP,
pages 901–904, Denver, Colorado, USA.
Sara Stymne, Maria Holmqvist, and Lars Ahrenberg.
2010. Vs and OOVs: Two problems for translation
between German and English. In Proceedings of
WMT and MetricsMATR, pages 183–188, Uppsala,
Sweden.
Sara Stymne. 2012. Clustered word classes for pre-
ordering in statistical machine translation. In Pro-
ceedings of ROBUS-UNSUP 2012: Joint Workshop
on Unsupervised and Semi-Supervised Learning in
NLP, pages 28–34, Avignon, France.
Oscar T¨ackstr¨om, Ryan McDonald, and Jakob Uszko-
reit. 2012. Cross-lingual word clusters for direct
transfer of linguistic structure. In Proceedings of
NAACL, pages 477–487, Montr´eal, Quebec, Canada.
David Talbot, Hideto Kazawa, Hiroshi Ichikawa, Jason
Katz-Brown, Masakazu Seno, and Franz Och. 2011.
A lightweight evaluation framework for machine
translation reordering. In Proceedings of WMT,
pages 12–21, Edinburgh, Scotland.
Ben Taskar, Lacoste-Julien Simon, and Dan Klein.
2005. A discriminative matching approach to word
alignment. In Proceedings of HLT and EMNLP,
pages 73–80, Vancouver, British Columbia, Canada.
J¨org Tiedemann. 2005. Optimisation of word
alignment clues. Natural Language Engineering,
11(03):279–293. Special Issue on Parallel Texts.
Stephan Vogel, Hermann Ney, and Christoph Tillman.
1996. HMM-based word alignment in statistical
translation. In Proceedings of Coling, pages 836–
841, Copenhagen, Denmark.
Fei Xia and Michael McCord. 2004. Improving
a statistical MT system with automatically learned
rewrite patterns. In Proceedings of Coling, pages
508–514, Geneva, Switzerland.
Kenji Yamada and Kevin Knight. 2002. A decoder for
syntax-based statistical MT. In Proceedings ofACL,
pages 303–310, Philadelphia, Pennsylvania, USA.
David Yarowsky, Grace Ngai, and Richard Wicen-
towski. 2001. Inducing multilingual text analysis
tools via robust projection across aligned corpora.
In Proceedings of the First International Conference
on Human Language Technology, pages 1–8, San
Diego, California, USA.
Yuqi Zhang, Richard Zens, and Hermann Ney. 2007.
Improved chunk-level reordering for statistical ma-
chine translation. In Proceedings of the Interna-
tional Workshop on Spoken Language Translation,
pages 21–28, Trento, Italy.
</reference>
<page confidence="0.998461">
286
</page>
</variant>
</algorithm>
<algorithm name="ParsHed" version="110505">
<variant no="0" confidence="0.650303">
<title confidence="0.999898">Estimating Word Alignment Quality for SMT Reordering Tasks</title>
<author confidence="0.995524">Sara Stymne J¨org Tiedemann Joakim</author>
<affiliation confidence="0.873535">Uppsala Department of Linguistics and</affiliation>
<email confidence="0.886521">firstname.lastname@lingfil.uu.se</email>
<abstract confidence="0.999168846153846">Previous studies of the effect of word alignment on translation quality in SMT generally explore link level metrics only and mostly do not show any clear connections between alignment and SMT quality. In this paper, we specifically investigate the impact of word alignment on two pre-reordering tasks in translation, using a wider range of quality indicators than previously done. Experiments on German–English translation show that reordering may require alignment models different from those used by the core translation system. Sparse alignments with high precision on the link level, for translation units, and on the subset of crossing links, like intersected HMM models, are preferred. Unlike SMT performance the desired alignment characteristics are similar for small and large training data for the pre-reordering tasks. Moreover, we confirm previous research showing that the fuzzy reordering score is a useful and cheap proxy for performance on SMT reordering tasks.</abstract>
</variant>
</algorithm>
<algorithm name="ParsCit" version="110505">
<citationList>
<citation valid="true">
<authors>
<author>Lars Ahrenberg</author>
<author>Magnus Merkel</author>
<author>Anna S˚agvall Hein</author>
<author>J¨org Tiedemann</author>
</authors>
<title>Evaluation of word alignment systems.</title>
<date>2000</date>
<booktitle>In Proceedings of LREC, volume III,</booktitle>
<pages>1255--1261</pages>
<location>Athens, Greece.</location>
<contexts>
<context position="1922" citStr="Ahrenberg et al., 2000" startWordPosition="292" endWordPosition="295"> there has been some work exploring the connection between word alignment quality and translation quality (Och and Ney, 2003; Fraser and Marcu, 2007; Lambert et al., 2012). The standard way to evaluate word alignments in this context is by using metrics like alignment error rate (AER) and F-measure on the link level, and the general conclusion appears to be that translation quality benefits from alignments with high recall (rather than precision), at least for large training data. Although many other ways of measuring alignment quality have been proposed, such as working on translation units (Ahrenberg et al., 2000; Ayan and Dorr, 2006; Søgaard and Kuhn, 2009) or using link degree and related measures (Ahrenberg, 2010), these methods have not been used to study the relation between alignment and translation quality, with the exception of Lambert et al. (2012). Word alignment is also used for many other tasks besides translation, including term bank creation (Merkel and Foo, 2007), cross-lingual annotation projection for part-of-speech tagging (Yarowsky et al., 2001), semantic roles (Pado and Lapata, 2005), pronoun anaphora (Postolache et al., 2006), and cross-lingual clustering (T¨ackstr¨om et al., 2012</context>
</contexts>
<marker>Ahrenberg, Merkel, Hein, Tiedemann, 2000</marker>
<rawString>Lars Ahrenberg, Magnus Merkel, Anna S˚agvall Hein, and J¨org Tiedemann. 2000. Evaluation of word alignment systems. In Proceedings of LREC, volume III, pages 1255–1261, Athens, Greece.</rawString>
</citation>
<citation valid="true">
<authors>
<author>Lars Ahrenberg</author>
</authors>
<title>Alignment-based profiling of Europarl data in an English-Swedish parallel corpus.</title>
<date>2010</date>
<booktitle>In Proceedings of LREC,</booktitle>
<pages>3398--3404</pages>
<location>Valetta,</location>
<contexts>
<context position="2028" citStr="Ahrenberg, 2010" startWordPosition="311" endWordPosition="312">nd Ney, 2003; Fraser and Marcu, 2007; Lambert et al., 2012). The standard way to evaluate word alignments in this context is by using metrics like alignment error rate (AER) and F-measure on the link level, and the general conclusion appears to be that translation quality benefits from alignments with high recall (rather than precision), at least for large training data. Although many other ways of measuring alignment quality have been proposed, such as working on translation units (Ahrenberg et al., 2000; Ayan and Dorr, 2006; Søgaard and Kuhn, 2009) or using link degree and related measures (Ahrenberg, 2010), these methods have not been used to study the relation between alignment and translation quality, with the exception of Lambert et al. (2012). Word alignment is also used for many other tasks besides translation, including term bank creation (Merkel and Foo, 2007), cross-lingual annotation projection for part-of-speech tagging (Yarowsky et al., 2001), semantic roles (Pado and Lapata, 2005), pronoun anaphora (Postolache et al., 2006), and cross-lingual clustering (T¨ackstr¨om et al., 2012). Even within SMT itself, there are tasks such as reordering that often make crucial use of word alignmen</context>
<context position="13807" citStr="Ahrenberg (2010)" startWordPosition="2298" endWordPosition="2299">in translation units. Proposed links are in this way scored according to their overlap with translation units in the gold standard. Precision and recall are defined in Eqs. 6–7, where overlap(XU, Y ) is the number of source and target words in XU that overlap with translation units in Y normalized by the size of XU (in terms of source and target words). Note, that TUs need to overlap in source and target. Otherwise, their overlap will be counted as zero. overlap(AU, G) (6) |A| overlap(GU, A) There have also been attempts at classifying alignments in other ways, not related to a gold standard. Ahrenberg (2010) proposed several ways to categorize human alignments, including link degree, reordering of links, and structural correspondence. He used these indicators to profile hand-aligned corpora from different domains. We will not use structural correspondence, which requires a dependency parser, and which we believe is error prone when performed automatically. We will use what we call link degree, i.e., how many alignment links each word obtains. Ahrenberg (2010) used a fine-grained scheme of the percentage for different degrees, including isomorphism 1–1, deletion 0–1, reduction m–1, and paraphrase </context>
<context position="15184" citStr="Ahrenberg (2010)" startWordPosition="2517" endWordPosition="2518">he 0–1 and 1–0 cases, and multi links where there are many words on at least one side. Ahrenberg (2010) also proposed to measure reorderings. He does this by calculating the percentage of links with crossings of different lengths. To define this he only considers adjacent links in the source using the distance between corresponding target words, which means that his metric becomes a directional measure. Reorderings of alignments was also used by Genzel (2010), who used crossing score, the number of crossing links, to rank reordering rules. This is non-directional and simpler to calculate than Ahrenberg (2010)’s metrics, and implicitly covers length since a long distance reordering leads to a higher number of pairwise crossing links. Birch and Osborne (2011) suggest using squared Kendall τ distance (SKTD), see Eq. 8, where n is the number of links, as a basis of LR-score, an MT metric that takes reordering into account. They found that squaring τ better explained reordering, than using only τ. In this study we will use both, crossing score and SKTD. Figure 1 shows these scores for an example sentence. These two measures only tell us how much reordering there is. To quantify this relative to the gol</context>
</contexts>
<marker>Ahrenberg, 2010</marker>
<rawString>Lars Ahrenberg. 2010. Alignment-based profiling of Europarl data in an English-Swedish parallel corpus. In Proceedings of LREC, pages 3398–3404, Valetta, Malta.</rawString>
</citation>
<citation valid="true">
<authors>
<author>Necip Fazil Ayan</author>
<author>Bonnie J Dorr</author>
</authors>
<title>Going beyond AER: An extensive analysis of word alignments and their impact on MT.</title>
<date>2006</date>
<booktitle>In Proceedings of Coling and ACL,</booktitle>
<pages>9--16</pages>
<location>Sydney, Australia.</location>
<contexts>
<context position="1943" citStr="Ayan and Dorr, 2006" startWordPosition="296" endWordPosition="299">k exploring the connection between word alignment quality and translation quality (Och and Ney, 2003; Fraser and Marcu, 2007; Lambert et al., 2012). The standard way to evaluate word alignments in this context is by using metrics like alignment error rate (AER) and F-measure on the link level, and the general conclusion appears to be that translation quality benefits from alignments with high recall (rather than precision), at least for large training data. Although many other ways of measuring alignment quality have been proposed, such as working on translation units (Ahrenberg et al., 2000; Ayan and Dorr, 2006; Søgaard and Kuhn, 2009) or using link degree and related measures (Ahrenberg, 2010), these methods have not been used to study the relation between alignment and translation quality, with the exception of Lambert et al. (2012). Word alignment is also used for many other tasks besides translation, including term bank creation (Merkel and Foo, 2007), cross-lingual annotation projection for part-of-speech tagging (Yarowsky et al., 2001), semantic roles (Pado and Lapata, 2005), pronoun anaphora (Postolache et al., 2006), and cross-lingual clustering (T¨ackstr¨om et al., 2012). Even within SMT it</context>
<context position="9366" citStr="Ayan and Dorr (2006)" startWordPosition="1548" endWordPosition="1551"> found that AER correlates with human evaluation of sentence level quality, but not with word error rate. Fraser and Marcu (2007) found that there is no correlation between AER and Bleu (Papineni et al., 2002), especially not when the Pset is large. They found that a balanced F-measure is a better indicator of Bleu, but that a weighted F-measure is even better (see Eq. 4) mostly with a higher weight for recall than for precision. This weight, however, needs to be optimized for each data set, language pair, and gold standard alignment separately. �F(A, G, α) = Precision(A,G) + Recall(A,G)) (4) Ayan and Dorr (2006) on the other hand found some evidence for the importance of precision over recall. However, they used much smaller training data than Fraser and Marcu (2007). They also suggested using a measure called consistent phrase error-rate (CPER), but found that it was hard to assess the impact of alignment on MT, both with AER and CPER. Lambert et al. (2012) performed a study where they investigated the effect of word alignment on MT using a large number of word alignment indicators. They found that there was a difference between large and small datasets in that alignment precision was more important</context>
<context position="12389" citStr="Ayan and Dorr, 2006" startWordPosition="2050" endWordPosition="2053"> working on the translation unit (TU) level, instead of the link level. A translation unit, or cept (Goutte et al., 2004), is defined as a maximally connected subgraph of an alignment. In Figure 1, the twelve links form nine translation units. Søgaard and Wu (2009) suggest the metric TUER, translation unit error rate, shown in Eq. 5, where AU are hypothesized translation units, and GU are gold standard translation units.1 They use TUER to establish lower bounds for the coverage of alignments from different formalisms, not to evaluate SMT. While they only use TUER, it 1TUER is similar to CPER (Ayan and Dorr, 2006), which measures the error rate of extracted phrases. Due to how phrase extraction handle null links, there are differences, however. 277 is also possible to define Precision, Recall and Fmeasure over translation units in the same way as for alignment links. We will use these three measures to get a broader picture of TUs in alignment evaluation. Also in this case, 1−TUER is equivalent to F-measure. 2|AU ∩ GU| TUER(A, G) = 1 − (5) |AU |+ |GU| The TU metrics are quite strict, since they require exact matching of TUs. Tiedemann (2005) suggested the MWU metrics for word alignment evaluation, whic</context>
</contexts>
<marker>Ayan, Dorr, 2006</marker>
<rawString>Necip Fazil Ayan and Bonnie J. Dorr. 2006. Going beyond AER: An extensive analysis of word alignments and their impact on MT. In Proceedings of Coling and ACL, pages 9–16, Sydney, Australia.</rawString>
</citation>
<citation valid="true">
<authors>
<author>Alexandra Birch</author>
<author>Miles Osborne</author>
</authors>
<title>Reordering metrics for MT.</title>
<date>2011</date>
<booktitle>In Proceedings of ACL,</booktitle>
<pages>1027--1035</pages>
<location>Portland, Oregon, USA.</location>
<contexts>
<context position="15335" citStr="Birch and Osborne (2011)" startWordPosition="2538" endWordPosition="2541">e does this by calculating the percentage of links with crossings of different lengths. To define this he only considers adjacent links in the source using the distance between corresponding target words, which means that his metric becomes a directional measure. Reorderings of alignments was also used by Genzel (2010), who used crossing score, the number of crossing links, to rank reordering rules. This is non-directional and simpler to calculate than Ahrenberg (2010)’s metrics, and implicitly covers length since a long distance reordering leads to a higher number of pairwise crossing links. Birch and Osborne (2011) suggest using squared Kendall τ distance (SKTD), see Eq. 8, where n is the number of links, as a basis of LR-score, an MT metric that takes reordering into account. They found that squaring τ better explained reordering, than using only τ. In this study we will use both, crossing score and SKTD. Figure 1 shows these scores for an example sentence. These two measures only tell us how much reordering there is. To quantify this relative to the gold standard we also report the absolute difference between the number of gold standard crossings and system crossings, which we call Crossdiff. To accou</context>
<context position="26180" citStr="Birch and Osborne, 2011" startWordPosition="4444" endWordPosition="4447">quality whereas precision does not. 4 Reordering Tasks for SMT Reordering is an important part of any SMT system. One way to address it is to add reordering models to standard PBSMT systems, for instance lexicalized reordering models (Koehn et al., 2005), or to directly model reordering in hierarchical (Chiang, 2007) or syntactic translation models (Yamada and Knight, 2002). Another type of approach is preordering, where the source side is reordered to mimic the target side before translation. There have also been approaches where reordering is modeled as part of the evaluation of MT systems (Birch and Osborne, 2011). We can distinguish two main types of approaches to preordering in SMT, either by using hand-written rules, which often operate on syntactic trees (Collins et al., 2005), or by reordering rules that are learnt automatically based on a word aligned corpus (Xia and McCord, 2004). The latter approach is of interest to us, since it is based on word alignments. There has been much work on automatic learning of reordering rules, which can be based on different levels of annotation, such as part-of-speech tags (Rottmann and Vogel, 2007; Niehues and Kolss, 2009; Genzel, 2010), chunks (Zhang et al., 2</context>
</contexts>
<marker>Birch, Osborne, 2011</marker>
<rawString>Alexandra Birch and Miles Osborne. 2011. Reordering metrics for MT. In Proceedings of ACL, pages 1027–1035, Portland, Oregon, USA.</rawString>
</citation>
<citation valid="true">
<authors>
<author>Peter F Brown</author>
<author>Stephen Della Pietra</author>
<author>Vincent J Della Pietra</author>
<author>Robert L Mercer</author>
</authors>
<title>The mathematics of statistical machine translation: Parameter estimation.</title>
<date>1993</date>
<journal>Computational Linguistics,</journal>
<volume>19</volume>
<issue>2</issue>
<pages>311</pages>
<contexts>
<context position="4974" citStr="Brown et al. (1993)" startWordPosition="790" endWordPosition="793">ental results for a number of alignment systems on an SMT task. In section 4, we turn to reordering for SMT and use the same quality indicators to study the impact of alignment quality on reordering quality. In section 5 we briefly describe results using small training data. In section 6, we conclude and suggest directions for future work. 2 Word Alignment and SMT Word alignment is the task of relating words in one language to words in the translation in another language, see an example in Figure 1. Word alignment models can be learnt automatically from large corpora of sentence aligned data. Brown et al. (1993) proposed the so-called IBM models, which are still widely used. These five models estimate alignments from corpora using the expectation-maximization algorithm, and each model adds some complexity. Model 4 is commonly used in SMT systems. There have been many later suggestions of alternatives to these models. These are often alternatives to model 2, such as the HMM model (Vogel et al., 1996) and fast align (Dyer et al., 2013). All these generative models produce directional alignments where one word in the source can be linked to many target words (1–m links) but not vice versa. It is general</context>
</contexts>
<marker>Brown, Pietra, Pietra, Mercer, 1993</marker>
<rawString>Peter F. Brown, Stephen Della Pietra, Vincent J. Della Pietra, and Robert L. Mercer. 1993. The mathematics of statistical machine translation: Parameter estimation. Computational Linguistics, 19(2):263– 311.</rawString>
</citation>
<citation valid="true">
<authors>
<author>Marine Carpuat</author>
<author>Yuval Marton</author>
<author>Nizar Habash</author>
</authors>
<title>Improving Arabic-to-English statistical machine translation by reordering post-verbal subjects for alignment.</title>
<date>2010</date>
<booktitle>In Proceedings ofACL, Short Papers,</booktitle>
<pages>178--183</pages>
<location>Uppsala,</location>
<contexts>
<context position="28586" citStr="Carpuat et al., 2010" startWordPosition="4858" endWordPosition="4861">a standard model 4 was not successful for his rule learning approach. Instead he used filtered model-1-alignments, which he claims was more successful. However, there are no further analyses or comparisons between the alignments reported in any of these papers. Another type of approach to reordering is to only reorder the data in order to improve word alignments, and to restore the original word order before training the SMT system. This type of approach has the advantage that no modifications are needed for the translation input. This approach has also been used both with hand-written rules (Carpuat et al., 2010; Stymne et al., 2010) and with rules based on initial word alignments on non-reordered texts (Holmqvist et al., 2009). For the latter approach a small study of the effect of gd and gdfa symmetrizations was presented, which only showed small variations in quality scores (Holmqvist et al., 2012). Below we present the two tasks that we study in this paper: part-of-speech-based reordering for creating input lattices for SMT and alignmentbased reordering for improving phrase-tables. We evaluate the performance of these tasks in relation to the use of different alignment models and symmetrization h</context>
</contexts>
<marker>Carpuat, Marton, Habash, 2010</marker>
<rawString>Marine Carpuat, Yuval Marton, and Nizar Habash. 2010. Improving Arabic-to-English statistical machine translation by reordering post-verbal subjects for alignment. In Proceedings ofACL, Short Papers, pages 178–183, Uppsala, Sweden.</rawString>
</citation>
<citation valid="true">
<authors>
<author>David Chiang</author>
</authors>
<title>Hierarchical phrase-based translation.</title>
<date>2007</date>
<journal>Computational Linguistics,</journal>
<volume>33</volume>
<issue>2</issue>
<contexts>
<context position="7620" citStr="Chiang, 2007" startWordPosition="1226" endWordPosition="1227">s of models are outside the scope of our current work. Word alignments are used as an important knowledge source for training SMT systems. In word-based SMT, the parameters of the generative word alignment models are essentially the translation model of the system. In phrase-based SMT (PBSMT) (Koehn et al., 2003), which is among the state-of-the-art systems today, word alignments are used as a basis for extracting phrases and estimating phrase alignment probabilities. Similarly, word alignments are also used for estimating rule probabilities in various kinds of hierarchical and syntactic SMT (Chiang, 2007; Yamada and Knight, 2002; Galley et al., 2004). Intrinsic evaluation of word alignment is generally based on a comparison to a gold standard of human alignments. Based on the gold standard, metrics like precision, recall and F-measure can be calculated for each alignment link, see Eqs. 1– 2, where A are hypothesized alignment links and G are gold standard links. Another common metric is alignment error rate (AER) (Och and Ney, 2000), which is based on a distinction between sure, S, and possible, P, links in the gold standard. 1−AER is identical to balanced F-measure when the gold standard doe</context>
<context position="25874" citStr="Chiang, 2007" startWordPosition="4396" endWordPosition="4397">link crossings, except a negative correlation with Crossdiff, which means that it is good to have a similar number of crossings as the baseline. These results confirm results from previous studies that link level measures, especially recall and weighted F-measure show some correlation with SMT quality whereas precision does not. 4 Reordering Tasks for SMT Reordering is an important part of any SMT system. One way to address it is to add reordering models to standard PBSMT systems, for instance lexicalized reordering models (Koehn et al., 2005), or to directly model reordering in hierarchical (Chiang, 2007) or syntactic translation models (Yamada and Knight, 2002). Another type of approach is preordering, where the source side is reordered to mimic the target side before translation. There have also been approaches where reordering is modeled as part of the evaluation of MT systems (Birch and Osborne, 2011). We can distinguish two main types of approaches to preordering in SMT, either by using hand-written rules, which often operate on syntactic trees (Collins et al., 2005), or by reordering rules that are learnt automatically based on a word aligned corpus (Xia and McCord, 2004). The latter app</context>
</contexts>
<marker>Chiang, 2007</marker>
<rawString>David Chiang. 2007. Hierarchical phrase-based translation. Computational Linguistics, 33(2):202–228.</rawString>
</citation>
<citation valid="true">
<authors>
<author>Michael Collins</author>
<author>Philipp Koehn</author>
<author>Ivona Kuˇcerov´a</author>
</authors>
<title>Clause restructuring for statistical machine translation.</title>
<date>2005</date>
<booktitle>In Proceedings of ACL,</booktitle>
<pages>531--540</pages>
<location>Ann Arbor, Michigan, USA.</location>
<marker>Collins, Koehn, Kuˇcerov´a, 2005</marker>
<rawString>Michael Collins, Philipp Koehn, and Ivona Kuˇcerov´a. 2005. Clause restructuring for statistical machine translation. In Proceedings of ACL, pages 531–540, Ann Arbor, Michigan, USA.</rawString>
</citation>
<citation valid="true">
<authors>
<author>Chris Dyer</author>
<author>Victor Chahuneau</author>
<author>Noah A Smith</author>
</authors>
<title>A simple, fast, and effective reparameterization of IBM model 2.</title>
<date>2013</date>
<booktitle>In Proceedings of NAACL,</booktitle>
<pages>644--648</pages>
<location>Atlanta, Georgia, USA.</location>
<contexts>
<context position="5404" citStr="Dyer et al., 2013" startWordPosition="860" endWordPosition="863">words in the translation in another language, see an example in Figure 1. Word alignment models can be learnt automatically from large corpora of sentence aligned data. Brown et al. (1993) proposed the so-called IBM models, which are still widely used. These five models estimate alignments from corpora using the expectation-maximization algorithm, and each model adds some complexity. Model 4 is commonly used in SMT systems. There have been many later suggestions of alternatives to these models. These are often alternatives to model 2, such as the HMM model (Vogel et al., 1996) and fast align (Dyer et al., 2013). All these generative models produce directional alignments where one word in the source can be linked to many target words (1–m links) but not vice versa. It is generally desirable to also allow n–1 and n–m links, and to achieve this it is common practice to perform word alignment in both directions and to symmetrize them using some heuristic. A number of common symmetrization strategies are described in Table 1 (Koehn et al., 2005). There are also other alternatives, such as the refined method (Och and Ney, 2003), or link deletion from the union (Fossum et al., 2008). There is also a wide r</context>
<context position="10423" citStr="Dyer et al., 2013" startWordPosition="1722" endWordPosition="1725">large number of word alignment indicators. They found that there was a difference between large and small datasets in that alignment precision was more important with small data sets, and recall more important with large data sets. Overall they did not find any indicator that was significant over two language pairs and different corpus sizes. There were more significant indicators for large datasets, however. Most researchers who propose new alignment models perform both a gold standard evaluation and an SMT evaluation (Liang et al., 2006; Ganchev et al., 2008; Junczys-Dowmunt and Szał, 2012; Dyer et al., 2013). The relation between the two types of evaluation is often quite weak. Several of these studies only show AER on their gold standard, despite its well-known shortcomings. Even though many studies have shown some relation between translation quality and AER or weighted F-measure, it has rarely been investigated thoroughly in its own right, and, as far as we are aware, not for other tasks than SMT. Furthermore, most of these studies considers nothing else but link level agreement. In this paper we take a broader view on alignment quality and explore the effect of other types of quality indicato</context>
<context position="16749" citStr="Dyer et al., 2013" startWordPosition="2782" endWordPosition="2785">pairs |(8) v (n2 − n)/2 3.1 Alignment Experiments We perform all our experiments for German– English. The alignment indicators are calculated on a corpus of 987 hand aligned sentences (Pado and Lapata, 2005). The gold standard contains explicit null links, which the symmetrized automatic alignments do not. To allow a straightforward comparison we consistently remove all null links when comparing system alignments to the gold standard. For creating the automatic alignments we used GIZA++ (Och and Ney, 2003) to compute directional alignments for model 2–4 and the HMM model, and fast align (fa) (Dyer et al., 2013) as newer alternatives to model 2. These models require large amounts of data to be estimated reliably. To achieve this we concatenated the gold standard with the large SMT training data (see �PMWU = AU ∈A �RMWU = GU ∈G (7) |G| 278 Table 2: Values for alignment quality indicators for the different alignments, where 2–4, HMM, and fa are alignment models, and symmetrization strategies refer to Table 1 Total SKTD P R F Crossdiff 30163 .292 – – – 0 11229 .251 .663 .522 .584 18934 8077 .221 .709 .417 .525 22086 21823 .270 .446 .444 .445 8340 26977 .275 .529 .562 .545 3044 17399 .246 .584 .475 .524 </context>
<context position="20307" citStr="Dyer et al., 2013" startWordPosition="3463" endWordPosition="3466">4 27951 .612 .756 .676 16529 .903 .660 .763 21961 .757 .734 .745 23840 .687 .724 .705 26575 .660 .775 .713 26529 .693 .812 .748 24712 .707 .772 .738 14871 .922 .606 .731 19430 .812 .698 .751 20554 .799 .726 .761 Total P R F 22629 – – – Alignment links HMM-gdfa HMM-gdf HMM-uni HMM-int HMM-gd fa-gdfa 2-gdfa 3-gdfa 4-gdfa fa-gdf fa-uni 2-gdf 3-gdf 4-gdf fa-int fa-gd 2-uni 3-uni 4-uni 2-int 3-int 4-int 2-gd 3-gd 4-gd gold 279 Section 3.2) of 2M sentences during alignment. For symmetrization we used all methods in Table 1, as implemented in the Moses toolkit (Koehn et al., 2007) and in fast align (Dyer et al., 2013). Based on the automatically aligned gold standard, we calculated all alignment indicators for all settings. The complete results can be found in Table 2, where we have ordered the symmetrization methods with the most sparse, intersection, on top. Overall we can see that while several of the alignment methods create a much higher number of alignment links than the gold standard, they do not produce many more translation units. This is very interesting and indicates why link level statistics may not be accurate enough to predict the performance of certain downstream applications. As expected, t</context>
</contexts>
<marker>Dyer, Chahuneau, Smith, 2013</marker>
<rawString>Chris Dyer, Victor Chahuneau, and Noah A. Smith. 2013. A simple, fast, and effective reparameterization of IBM model 2. In Proceedings of NAACL, pages 644–648, Atlanta, Georgia, USA.</rawString>
</citation>
<citation valid="true">
<authors>
<author>Victoria Fossum</author>
<author>Kevin Knight</author>
<author>Steven Abney</author>
</authors>
<title>Using syntax to improve word alignment precision for syntax-based machine translation.</title>
<date>2008</date>
<booktitle>In Proceedings of WMT,</booktitle>
<pages>44--52</pages>
<location>Columbus, Ohio.</location>
<contexts>
<context position="5980" citStr="Fossum et al., 2008" startWordPosition="959" endWordPosition="962">l., 1996) and fast align (Dyer et al., 2013). All these generative models produce directional alignments where one word in the source can be linked to many target words (1–m links) but not vice versa. It is generally desirable to also allow n–1 and n–m links, and to achieve this it is common practice to perform word alignment in both directions and to symmetrize them using some heuristic. A number of common symmetrization strategies are described in Table 1 (Koehn et al., 2005). There are also other alternatives, such as the refined method (Och and Ney, 2003), or link deletion from the union (Fossum et al., 2008). There is also a wide range of alternative approaches to word alignment. For example, various discriminative models have been proposed in the literature (Liu et al., 2005; Moore, 2005; Taskar et al., 2005). Their advantage is that they may integrate a wide range of features that may lead to improved alignment quality. However, most of Symmetrization Description int: intersection ATS ∩ AST uni: union ATS ∪ AST gd: grow-diag intersection plus adjacent links from the union if both linked words are unaligned gdf: grow-diag-final gd with links from the union added in a final step if either linked </context>
</contexts>
<marker>Fossum, Knight, Abney, 2008</marker>
<rawString>Victoria Fossum, Kevin Knight, and Steven Abney. 2008. Using syntax to improve word alignment precision for syntax-based machine translation. In Proceedings of WMT, pages 44–52, Columbus, Ohio.</rawString>
</citation>
<citation valid="true">
<authors>
<author>Alexander Fraser</author>
<author>Daniel Marcu</author>
</authors>
<title>Measuring word alignment quality for statistical machine translation.</title>
<date>2007</date>
<journal>Computational Linguistics,</journal>
<volume>33</volume>
<issue>3</issue>
<contexts>
<context position="1448" citStr="Fraser and Marcu, 2007" startWordPosition="216" endWordPosition="219">set of crossing links, like intersected HMM models, are preferred. Unlike SMT performance the desired alignment characteristics are similar for small and large training data for the pre-reordering tasks. Moreover, we confirm previous research showing that the fuzzy reordering score is a useful and cheap proxy for performance on SMT reordering tasks. 1 Introduction Word alignment is a key component in all state-ofthe-art statistical machine translation (SMT) systems, and there has been some work exploring the connection between word alignment quality and translation quality (Och and Ney, 2003; Fraser and Marcu, 2007; Lambert et al., 2012). The standard way to evaluate word alignments in this context is by using metrics like alignment error rate (AER) and F-measure on the link level, and the general conclusion appears to be that translation quality benefits from alignments with high recall (rather than precision), at least for large training data. Although many other ways of measuring alignment quality have been proposed, such as working on translation units (Ahrenberg et al., 2000; Ayan and Dorr, 2006; Søgaard and Kuhn, 2009) or using link degree and related measures (Ahrenberg, 2010), these methods have</context>
<context position="8875" citStr="Fraser and Marcu (2007)" startWordPosition="1462" endWordPosition="1465"> S and P. Precision(A, G) = |G ∩ A |(1) |A| Recall(A, G) = |G ∩ A |(2) |G| AER = 1 − |P ∩ A |+ |S ∩ A |(3) |S |+ |A| 276 Crossing = 8 SKDT = 8/66 ≈ 0.65 6 1–1 links 3 multi links 0 null links Figure 1: An example alignment illustrating n–1, 1–m and crossing links. The relation between word alignment quality and PBSMT has been studied by some researchers. Och and Ney (2000) looked at the impact of IBM and HMM models on the alignment template approach (Och et al., 1999) in terms of AER. They found that AER correlates with human evaluation of sentence level quality, but not with word error rate. Fraser and Marcu (2007) found that there is no correlation between AER and Bleu (Papineni et al., 2002), especially not when the Pset is large. They found that a balanced F-measure is a better indicator of Bleu, but that a weighted F-measure is even better (see Eq. 4) mostly with a higher weight for recall than for precision. This weight, however, needs to be optimized for each data set, language pair, and gold standard alignment separately. �F(A, G, α) = Precision(A,G) + Recall(A,G)) (4) Ayan and Dorr (2006) on the other hand found some evidence for the importance of precision over recall. However, they used much s</context>
<context position="11409" citStr="Fraser and Marcu (2007)" startWordPosition="1889" endWordPosition="1892"> aware, not for other tasks than SMT. Furthermore, most of these studies considers nothing else but link level agreement. In this paper we take a broader view on alignment quality and explore the effect of other types of quality indicators as well. 3 Word Alignment Quality Indicators We investigate four groups of quality indicators. The first group is the classic group where metrics are calculated on the alignment link level, which has been used in several studies. In our experiments we use a gold standard that does not make use of distinctions between sure and possible links, as suggested by Fraser and Marcu (2007). With this, we can calculate the standard metrics P(recision) R(ecall) and F(-measure). We will mainly use balanced F-measure, but occasionally also report weighted F-measure. As noted before, 1−AER is equivalent to balanced F when only sure links are used, and will thus not be reported separately. Søgaard and Kuhn (2009) and Søgaard and Wu (2009) suggested working on the translation unit (TU) level, instead of the link level. A translation unit, or cept (Goutte et al., 2004), is defined as a maximally connected subgraph of an alignment. In Figure 1, the twelve links form nine translation uni</context>
<context position="24266" citStr="Fraser and Marcu (2007)" startWordPosition="4129" endWordPosition="4132">tems with reordering. Table 4 shows the results on the SMT task. Model 3 and 4 with gd/gdfa symmetrization yield the highest scores. There is a larger difference between systems with different symmetrization than between systems with different alignment models. The sparse intersection symmetrization gives the poorest results. The top row in Table 5 shows correlations between Bleu and all word alignment quality indicators. There are significant correlations with link level recall. A weighted link level F-measure with α = 0.3 gives a significant correlation of .72, which confirms the results of Fraser and Marcu (2007). There are no significant correlations with the TU metrics but a positive correlation with the number of TUs. For the MWU metrics the correlations are similar to the link level, 2http://www.statmt.org/wmt13/ translation-task.html 3This could have disfavored the other alignments, so we also performed control experiments where we ran separate tunings for each alignment. While the absolute results varied somewhat, the correlations with alignment indicators were stable. 280 m2 m3 m4 HMM fa inter 18.1 19.1 19.3 18.8 18.9 gd 20.4 20.9 20.9 20.5 20.6 gdfa 20.4 20.7 20.8 20.5 20.5 gdf 19.4 19.7 20.1 </context>
<context position="39919" citStr="Fraser and Marcu, 2007" startWordPosition="6823" endWordPosition="6826">than recall. For FRS on both tasks and Bleu for alignment-based reordering, model4 and HMM with intersection and gd still perform best. For Bleu for POS-based reordering, gdfa and model 3 also give good results. 6 Conclusion and Future Work We have shown that the best combination of alignment and symmetrization models for SMT are not the best models for reordering tasks in our experimental setting. For SMT, high recall is more important than precision with large training data, while precision and recall are of equal importance with small training data. This finding supports previous research (Fraser and Marcu, 2007; Lambert et al., 2012). Translation unit metrics are not predictive of SMT performance. For the large data condition model 3 and 4 with gd and gdfa symmetrization gave the best results, whereas fast align with gd and gdfa was best with small training data. For the two preordering tasks we investigated, however, link level weighted F-measure that gave more weight to precision was important, as well as all TU metrics. It was also important to have high precision for the crossing subset of TUs. Hence, it is more important to reliably find some crossings than to find all crossings. This make sens</context>
</contexts>
<marker>Fraser, Marcu, 2007</marker>
<rawString>Alexander Fraser and Daniel Marcu. 2007. Measuring word alignment quality for statistical machine translation. Computational Linguistics, 33(3):293–303.</rawString>
</citation>
<citation valid="true">
<authors>
<author>Michel Galley</author>
<author>Mark Hopkins</author>
<author>Kevin Knight</author>
<author>Daniel Marcu</author>
</authors>
<title>What’s in a translation rule?</title>
<date>2004</date>
<booktitle>In Proceedings of NAACL,</booktitle>
<pages>273--280</pages>
<location>Boston, Massachusetts, USA.</location>
<contexts>
<context position="7667" citStr="Galley et al., 2004" startWordPosition="1233" endWordPosition="1236">r current work. Word alignments are used as an important knowledge source for training SMT systems. In word-based SMT, the parameters of the generative word alignment models are essentially the translation model of the system. In phrase-based SMT (PBSMT) (Koehn et al., 2003), which is among the state-of-the-art systems today, word alignments are used as a basis for extracting phrases and estimating phrase alignment probabilities. Similarly, word alignments are also used for estimating rule probabilities in various kinds of hierarchical and syntactic SMT (Chiang, 2007; Yamada and Knight, 2002; Galley et al., 2004). Intrinsic evaluation of word alignment is generally based on a comparison to a gold standard of human alignments. Based on the gold standard, metrics like precision, recall and F-measure can be calculated for each alignment link, see Eqs. 1– 2, where A are hypothesized alignment links and G are gold standard links. Another common metric is alignment error rate (AER) (Och and Ney, 2000), which is based on a distinction between sure, S, and possible, P, links in the gold standard. 1−AER is identical to balanced F-measure when the gold standard does not make a distinction between S and P. Preci</context>
</contexts>
<marker>Galley, Hopkins, Knight, Marcu, 2004</marker>
<rawString>Michel Galley, Mark Hopkins, Kevin Knight, and Daniel Marcu. 2004. What’s in a translation rule? In Proceedings of NAACL, pages 273–280, Boston, Massachusetts, USA.</rawString>
</citation>
<citation valid="true">
<authors>
<author>Kuzman Ganchev</author>
<author>Jo˜ao V Grac¸a</author>
<author>Ben Taskar</author>
</authors>
<title>Better alignments = better translations?</title>
<date>2008</date>
<booktitle>In Proceedings of ACL,</booktitle>
<pages>986--993</pages>
<location>Columbus, Ohio, USA.</location>
<marker>Ganchev, Grac¸a, Taskar, 2008</marker>
<rawString>Kuzman Ganchev, Jo˜ao V. Grac¸a, and Ben Taskar. 2008. Better alignments = better translations? In Proceedings of ACL, pages 986–993, Columbus, Ohio, USA.</rawString>
</citation>
<citation valid="true">
<authors>
<author>Dmitriy Genzel</author>
</authors>
<title>Automatically learning sourceside reordering rules for large scale machine translation.</title>
<date>2010</date>
<booktitle>In Proceedings of Coling,</booktitle>
<pages>376--384</pages>
<location>Beijing, China.</location>
<contexts>
<context position="15031" citStr="Genzel (2010)" startWordPosition="2492" endWordPosition="2493">r link degree classes were used by Lambert et al. (2012). In this work we will reduce these classes into three: 1–1 links, null links, which combine the 0–1 and 1–0 cases, and multi links where there are many words on at least one side. Ahrenberg (2010) also proposed to measure reorderings. He does this by calculating the percentage of links with crossings of different lengths. To define this he only considers adjacent links in the source using the distance between corresponding target words, which means that his metric becomes a directional measure. Reorderings of alignments was also used by Genzel (2010), who used crossing score, the number of crossing links, to rank reordering rules. This is non-directional and simpler to calculate than Ahrenberg (2010)’s metrics, and implicitly covers length since a long distance reordering leads to a higher number of pairwise crossing links. Birch and Osborne (2011) suggest using squared Kendall τ distance (SKTD), see Eq. 8, where n is the number of links, as a basis of LR-score, an MT metric that takes reordering into account. They found that squaring τ better explained reordering, than using only τ. In this study we will use both, crossing score and SKTD</context>
<context position="26755" citStr="Genzel, 2010" startWordPosition="4545" endWordPosition="4546">f MT systems (Birch and Osborne, 2011). We can distinguish two main types of approaches to preordering in SMT, either by using hand-written rules, which often operate on syntactic trees (Collins et al., 2005), or by reordering rules that are learnt automatically based on a word aligned corpus (Xia and McCord, 2004). The latter approach is of interest to us, since it is based on word alignments. There has been much work on automatic learning of reordering rules, which can be based on different levels of annotation, such as part-of-speech tags (Rottmann and Vogel, 2007; Niehues and Kolss, 2009; Genzel, 2010), chunks (Zhang et al., 2007) or parse trees (Xia and McCord, 2004). In general, all these approaches lead to improvements of translation quality. The reordering is always applied on the translation input. It can also be applied on the source side of the training corpora, which sometimes improves the results (Rottmann and Vogel, 2007), but sometimes does not make a difference (Stymne, 2012). When preordering is performed on the translation input, it can be presented to the decoder as a 1-best reordering (Xia and McCord, 2004), as an n-best list (Li et al., 2007), or as a lattice of possible re</context>
</contexts>
<marker>Genzel, 2010</marker>
<rawString>Dmitriy Genzel. 2010. Automatically learning sourceside reordering rules for large scale machine translation. In Proceedings of Coling, pages 376–384, Beijing, China.</rawString>
</citation>
<citation valid="true">
<authors>
<author>Cyril Goutte</author>
<author>Kenji Yamada</author>
<author>Eric Gaussier</author>
</authors>
<title>Aligning words using matrix factorisation.</title>
<date>2004</date>
<booktitle>In Proceedings of ACL,</booktitle>
<pages>502--509</pages>
<location>Barcelona,</location>
<contexts>
<context position="11890" citStr="Goutte et al., 2004" startWordPosition="1966" endWordPosition="1969">ents we use a gold standard that does not make use of distinctions between sure and possible links, as suggested by Fraser and Marcu (2007). With this, we can calculate the standard metrics P(recision) R(ecall) and F(-measure). We will mainly use balanced F-measure, but occasionally also report weighted F-measure. As noted before, 1−AER is equivalent to balanced F when only sure links are used, and will thus not be reported separately. Søgaard and Kuhn (2009) and Søgaard and Wu (2009) suggested working on the translation unit (TU) level, instead of the link level. A translation unit, or cept (Goutte et al., 2004), is defined as a maximally connected subgraph of an alignment. In Figure 1, the twelve links form nine translation units. Søgaard and Wu (2009) suggest the metric TUER, translation unit error rate, shown in Eq. 5, where AU are hypothesized translation units, and GU are gold standard translation units.1 They use TUER to establish lower bounds for the coverage of alignments from different formalisms, not to evaluate SMT. While they only use TUER, it 1TUER is similar to CPER (Ayan and Dorr, 2006), which measures the error rate of extracted phrases. Due to how phrase extraction handle null links,</context>
</contexts>
<marker>Goutte, Yamada, Gaussier, 2004</marker>
<rawString>Cyril Goutte, Kenji Yamada, and Eric Gaussier. 2004. Aligning words using matrix factorisation. In Proceedings of ACL, pages 502–509, Barcelona, Spain.</rawString>
</citation>
<citation valid="true">
<authors>
<author>Maria Holmqvist</author>
<author>Sara Stymne</author>
<author>Jody Foo</author>
<author>Lars Ahrenberg</author>
</authors>
<title>Improving alignment for SMT by reordering and augmenting the training corpus.</title>
<date>2009</date>
<booktitle>In Proceedings of WMT,</booktitle>
<pages>120--124</pages>
<location>Athens, Greece.</location>
<contexts>
<context position="28704" citStr="Holmqvist et al., 2009" startWordPosition="4877" endWordPosition="4880">which he claims was more successful. However, there are no further analyses or comparisons between the alignments reported in any of these papers. Another type of approach to reordering is to only reorder the data in order to improve word alignments, and to restore the original word order before training the SMT system. This type of approach has the advantage that no modifications are needed for the translation input. This approach has also been used both with hand-written rules (Carpuat et al., 2010; Stymne et al., 2010) and with rules based on initial word alignments on non-reordered texts (Holmqvist et al., 2009). For the latter approach a small study of the effect of gd and gdfa symmetrizations was presented, which only showed small variations in quality scores (Holmqvist et al., 2012). Below we present the two tasks that we study in this paper: part-of-speech-based reordering for creating input lattices for SMT and alignmentbased reordering for improving phrase-tables. We evaluate the performance of these tasks in relation to the use of different alignment models and symmetrization heuristics. For these tasks we are mainly interested in the full translation task, for which we report Bleu scores. In </context>
<context position="30634" citStr="Holmqvist et al. (2009)" startWordPosition="5225" endWordPosition="5228">94 .27 .92 −.38 AlignReo, Bleu −.57 .83 −.79 −.93 −.91 .86 −.07 .69 −.52 Table 5: Pearson correlations between different alignment characteristics and scores for the translation and reordering tasks. Significant correlations are marked with bold (&lt; 0.01). only on the reordering component (Talbot et al., 2011). It compares a system reordering to a reference reordering, by measuring how many chunks that have to be moved to get an identical word order, see Eq. 9, where C is the number of contiguously aligned chunks, and M the number of words. To find the reference ordering we apply the method of Holmqvist et al. (2009), described in Section 4.2, to the gold standard alignment. C − 1 FRS = 1 − (9) M − 1 4.1 Part-of-Speech-Based Reordering Our first reordering task is a part-of-speech-based preordering method described by Rottmann and Vogel (2007) and Niehues and Kolss (2009), which was successfully used for German–English translation. Rules are learnt from a word aligned POS-tagged corpus. Based on the alignments, tag patterns are identified that give rise to specific reorderings. These patterns are then scored based on relative frequency.4 The rules are then applied to the translation input to create a reor</context>
<context position="34351" citStr="Holmqvist et al., 2009" startWordPosition="5876" endWordPosition="5879">itive, but note in Table 1 that many alignments have a much higher number of crossings than the baseline. The precision of the crossing links is highly correlated with performance on this task, while the recall is not. This tells us that it is important that the crossings we find in the alignment are good, but that it is less important that we find all crossings. This makes sense since the rule learner can then learn at least a subset of all existing crossings well. 4.2 Reordering for Alignment In our second reordering task we investigate alignment-based reordering for improving phrasetables (Holmqvist et al., 2009; Holmqvist et al., 2012). This strategy first performs a word alignment, based on which the source text is reordered to remove all crossings. A second alignment is trained on the reordered data, which is then restored to the original order before training the full SMT system. In Holmqvist et al. (2012) it was shown that this strategy leads to improvements in link level recall and F-measure as well as small translation improvements for English– Swedish. It also led to small improvements for German–English translation. Similar to the previous experiments, we now vary alignment models and symmet</context>
</contexts>
<marker>Holmqvist, Stymne, Foo, Ahrenberg, 2009</marker>
<rawString>Maria Holmqvist, Sara Stymne, Jody Foo, and Lars Ahrenberg. 2009. Improving alignment for SMT by reordering and augmenting the training corpus. In Proceedings of WMT, pages 120–124, Athens, Greece.</rawString>
</citation>
<citation valid="true">
<authors>
<author>Maria Holmqvist</author>
<author>Sara Stymne</author>
<author>Lars Ahrenberg</author>
<author>Magnus Merkel</author>
</authors>
<title>Alignment-based reordering for SMT.</title>
<date>2012</date>
<booktitle>In Proceedings of LREC,</booktitle>
<location>Istanbul, Turkey.</location>
<contexts>
<context position="28881" citStr="Holmqvist et al., 2012" startWordPosition="4906" endWordPosition="4909">ordering is to only reorder the data in order to improve word alignments, and to restore the original word order before training the SMT system. This type of approach has the advantage that no modifications are needed for the translation input. This approach has also been used both with hand-written rules (Carpuat et al., 2010; Stymne et al., 2010) and with rules based on initial word alignments on non-reordered texts (Holmqvist et al., 2009). For the latter approach a small study of the effect of gd and gdfa symmetrizations was presented, which only showed small variations in quality scores (Holmqvist et al., 2012). Below we present the two tasks that we study in this paper: part-of-speech-based reordering for creating input lattices for SMT and alignmentbased reordering for improving phrase-tables. We evaluate the performance of these tasks in relation to the use of different alignment models and symmetrization heuristics. For these tasks we are mainly interested in the full translation task, for which we report Bleu scores. In addition we also show fuzzy reordering score (FRS), which focuses 281 Total Alignment links F Total Translation units F MWU F P R P R P R SMT, Bleu .33 −.25 .56 .46 .65 −.20 .16</context>
<context position="34376" citStr="Holmqvist et al., 2012" startWordPosition="5880" endWordPosition="5883"> 1 that many alignments have a much higher number of crossings than the baseline. The precision of the crossing links is highly correlated with performance on this task, while the recall is not. This tells us that it is important that the crossings we find in the alignment are good, but that it is less important that we find all crossings. This makes sense since the rule learner can then learn at least a subset of all existing crossings well. 4.2 Reordering for Alignment In our second reordering task we investigate alignment-based reordering for improving phrasetables (Holmqvist et al., 2009; Holmqvist et al., 2012). This strategy first performs a word alignment, based on which the source text is reordered to remove all crossings. A second alignment is trained on the reordered data, which is then restored to the original order before training the full SMT system. In Holmqvist et al. (2012) it was shown that this strategy leads to improvements in link level recall and F-measure as well as small translation improvements for English– Swedish. It also led to small improvements for German–English translation. Similar to the previous experiments, we now vary alignment models and symmetrization that are used fo</context>
</contexts>
<marker>Holmqvist, Stymne, Ahrenberg, Merkel, 2012</marker>
<rawString>Maria Holmqvist, Sara Stymne, Lars Ahrenberg, and Magnus Merkel. 2012. Alignment-based reordering for SMT. In Proceedings of LREC, Istanbul, Turkey.</rawString>
</citation>
<citation valid="true">
<authors>
<author>Marcin Junczys-Dowmunt</author>
<author>Arkadiusz Szał</author>
</authors>
<title>SyMGiza++: Symmetrized word alignment models for statistical machine translation.</title>
<date>2012</date>
<booktitle>In International Joint Conference of Security and Intelligent Information Systems,</booktitle>
<pages>379--390</pages>
<location>Warsaw,</location>
<contexts>
<context position="10403" citStr="Junczys-Dowmunt and Szał, 2012" startWordPosition="1718" endWordPosition="1721">of word alignment on MT using a large number of word alignment indicators. They found that there was a difference between large and small datasets in that alignment precision was more important with small data sets, and recall more important with large data sets. Overall they did not find any indicator that was significant over two language pairs and different corpus sizes. There were more significant indicators for large datasets, however. Most researchers who propose new alignment models perform both a gold standard evaluation and an SMT evaluation (Liang et al., 2006; Ganchev et al., 2008; Junczys-Dowmunt and Szał, 2012; Dyer et al., 2013). The relation between the two types of evaluation is often quite weak. Several of these studies only show AER on their gold standard, despite its well-known shortcomings. Even though many studies have shown some relation between translation quality and AER or weighted F-measure, it has rarely been investigated thoroughly in its own right, and, as far as we are aware, not for other tasks than SMT. Furthermore, most of these studies considers nothing else but link level agreement. In this paper we take a broader view on alignment quality and explore the effect of other types</context>
</contexts>
<marker>Junczys-Dowmunt, Szał, 2012</marker>
<rawString>Marcin Junczys-Dowmunt and Arkadiusz Szał. 2012. SyMGiza++: Symmetrized word alignment models for statistical machine translation. In International Joint Conference of Security and Intelligent Information Systems, pages 379–390, Warsaw, Poland.</rawString>
</citation>
<citation valid="true">
<authors>
<author>Philipp Koehn</author>
<author>Franz Josef Och</author>
<author>Daniel Marcu</author>
</authors>
<title>Statistical phrase-based translation.</title>
<date>2003</date>
<booktitle>In Proceedings of NAACL,</booktitle>
<pages>48--54</pages>
<location>Edmonton, Alberta, Canada.</location>
<contexts>
<context position="7322" citStr="Koehn et al., 2003" startWordPosition="1180" endWordPosition="1183">ligned Table 1: Symmetrization strategies for word alignments ATS and AST in two directions these models require external tools (for creating linguistic features) and manually aligned training data, which we do not have for our data sets (besides the data we need for evaluation). Investigating these types of models are outside the scope of our current work. Word alignments are used as an important knowledge source for training SMT systems. In word-based SMT, the parameters of the generative word alignment models are essentially the translation model of the system. In phrase-based SMT (PBSMT) (Koehn et al., 2003), which is among the state-of-the-art systems today, word alignments are used as a basis for extracting phrases and estimating phrase alignment probabilities. Similarly, word alignments are also used for estimating rule probabilities in various kinds of hierarchical and syntactic SMT (Chiang, 2007; Yamada and Knight, 2002; Galley et al., 2004). Intrinsic evaluation of word alignment is generally based on a comparison to a gold standard of human alignments. Based on the gold standard, metrics like precision, recall and F-measure can be calculated for each alignment link, see Eqs. 1– 2, where A </context>
</contexts>
<marker>Koehn, Och, Marcu, 2003</marker>
<rawString>Philipp Koehn, Franz Josef Och, and Daniel Marcu. 2003. Statistical phrase-based translation. In Proceedings of NAACL, pages 48–54, Edmonton, Alberta, Canada.</rawString>
</citation>
<citation valid="true">
<authors>
<author>Philipp Koehn</author>
<author>Amittai Axelrod</author>
<author>Alexandra Birch Mayne</author>
<author>Chris Callison-Burch</author>
<author>Miles Osborne</author>
<author>David Talbot</author>
</authors>
<title>Edinburgh system description for the 2005 IWSLT speech translation evaluation.</title>
<date>2005</date>
<booktitle>In Proceedings of the International Workshop on Spoken Language Translation,</booktitle>
<location>Pittsburgh, Pennsylvania, USA.</location>
<contexts>
<context position="5842" citStr="Koehn et al., 2005" startWordPosition="935" endWordPosition="938">e been many later suggestions of alternatives to these models. These are often alternatives to model 2, such as the HMM model (Vogel et al., 1996) and fast align (Dyer et al., 2013). All these generative models produce directional alignments where one word in the source can be linked to many target words (1–m links) but not vice versa. It is generally desirable to also allow n–1 and n–m links, and to achieve this it is common practice to perform word alignment in both directions and to symmetrize them using some heuristic. A number of common symmetrization strategies are described in Table 1 (Koehn et al., 2005). There are also other alternatives, such as the refined method (Och and Ney, 2003), or link deletion from the union (Fossum et al., 2008). There is also a wide range of alternative approaches to word alignment. For example, various discriminative models have been proposed in the literature (Liu et al., 2005; Moore, 2005; Taskar et al., 2005). Their advantage is that they may integrate a wide range of features that may lead to improved alignment quality. However, most of Symmetrization Description int: intersection ATS ∩ AST uni: union ATS ∪ AST gd: grow-diag intersection plus adjacent links f</context>
<context position="25810" citStr="Koehn et al., 2005" startWordPosition="4384" endWordPosition="4387">al matches. There are no significant correlations with link degree or link crossings, except a negative correlation with Crossdiff, which means that it is good to have a similar number of crossings as the baseline. These results confirm results from previous studies that link level measures, especially recall and weighted F-measure show some correlation with SMT quality whereas precision does not. 4 Reordering Tasks for SMT Reordering is an important part of any SMT system. One way to address it is to add reordering models to standard PBSMT systems, for instance lexicalized reordering models (Koehn et al., 2005), or to directly model reordering in hierarchical (Chiang, 2007) or syntactic translation models (Yamada and Knight, 2002). Another type of approach is preordering, where the source side is reordered to mimic the target side before translation. There have also been approaches where reordering is modeled as part of the evaluation of MT systems (Birch and Osborne, 2011). We can distinguish two main types of approaches to preordering in SMT, either by using hand-written rules, which often operate on syntactic trees (Collins et al., 2005), or by reordering rules that are learnt automatically based</context>
</contexts>
<marker>Koehn, Axelrod, Mayne, Callison-Burch, Osborne, Talbot, 2005</marker>
<rawString>Philipp Koehn, Amittai Axelrod, Alexandra Birch Mayne, Chris Callison-Burch, Miles Osborne, and David Talbot. 2005. Edinburgh system description for the 2005 IWSLT speech translation evaluation. In Proceedings of the International Workshop on Spoken Language Translation, Pittsburgh, Pennsylvania, USA.</rawString>
</citation>
<citation valid="true">
<authors>
<author>Philipp Koehn</author>
<author>Hieu Hoang</author>
<author>Alexandra Birch</author>
<author>Chris Callison-Burch</author>
<author>Marcello Federico</author>
<author>Nicola Bertoldi</author>
<author>Brooke Cowan</author>
<author>Wade Shen</author>
</authors>
<title>Moses: Open source toolkit for statistical machine translation.</title>
<date>2007</date>
<booktitle>In Proceedings of ACL, Demonstration Session,</booktitle>
<pages>177--180</pages>
<location>Christine Moran, Richard Zens, Chris Dyer, Ondrej Bojar, Alexandra</location>
<contexts>
<context position="20269" citStr="Koehn et al., 2007" startWordPosition="3455" endWordPosition="3458">6724 .633 .748 .686 28093 .636 .789 .704 27951 .612 .756 .676 16529 .903 .660 .763 21961 .757 .734 .745 23840 .687 .724 .705 26575 .660 .775 .713 26529 .693 .812 .748 24712 .707 .772 .738 14871 .922 .606 .731 19430 .812 .698 .751 20554 .799 .726 .761 Total P R F 22629 – – – Alignment links HMM-gdfa HMM-gdf HMM-uni HMM-int HMM-gd fa-gdfa 2-gdfa 3-gdfa 4-gdfa fa-gdf fa-uni 2-gdf 3-gdf 4-gdf fa-int fa-gd 2-uni 3-uni 4-uni 2-int 3-int 4-int 2-gd 3-gd 4-gd gold 279 Section 3.2) of 2M sentences during alignment. For symmetrization we used all methods in Table 1, as implemented in the Moses toolkit (Koehn et al., 2007) and in fast align (Dyer et al., 2013). Based on the automatically aligned gold standard, we calculated all alignment indicators for all settings. The complete results can be found in Table 2, where we have ordered the symmetrization methods with the most sparse, intersection, on top. Overall we can see that while several of the alignment methods create a much higher number of alignment links than the gold standard, they do not produce many more translation units. This is very interesting and indicates why link level statistics may not be accurate enough to predict the performance of certain d</context>
<context position="23041" citStr="Koehn et al., 2007" startWordPosition="3932" endWordPosition="3935">is a standard PBSMT system trained on WMT13 Translation unit Link level ↓ P R F P .95 .77 .90 R −.57 −.22 −.42 F .70 .90 .83 Table 3: Pearson correlations between gold standard word alignment evaluation on the link level and on translation unit level. Significant correlations are marked with bold (&lt; 0.01). data.2 We trained a German–English system on 2M sentences from Europarl and News Commentary. We used the target side of the parallel corpus and the SRILM toolkit (Stolcke, 2002) to train a 5- gram language model. For training the translation model and for decoding we used the Moses toolkit (Koehn et al., 2007). We applied a standard feature set consisting of a language model feature, four translation model features, word penalty, phrase penalty, and distortion cost. For tuning we used minimum error-rate training (Och, 2003). In order to minimize the risk of tuning influencing the results, we used a fixed set of weights for each experiment, tuned on a model 4+gdfa alignment.3 For tuning we used newstest2009 with 2525 sentences, and for testing we used newstest2013 with 3000 sentences. Evaluation was performed using the Bleu metric (Papineni et al., 2002). The same system setup was used for the SMT s</context>
</contexts>
<marker>Koehn, Hoang, Birch, Callison-Burch, Federico, Bertoldi, Cowan, Shen, 2007</marker>
<rawString>Philipp Koehn, Hieu Hoang, Alexandra Birch, Chris Callison-Burch, Marcello Federico, Nicola Bertoldi, Brooke Cowan, Wade Shen, Christine Moran, Richard Zens, Chris Dyer, Ondrej Bojar, Alexandra Constantin, and Evan Herbst. 2007. Moses: Open source toolkit for statistical machine translation. In Proceedings of ACL, Demonstration Session, pages 177–180, Prague, Czech Republic.</rawString>
</citation>
<citation valid="true">
<authors>
<author>Patrik Lambert</author>
<author>Simon Petitrenaud</author>
<author>Yanjun Ma</author>
<author>Andy Way</author>
</authors>
<title>What types of word alignment improve statistical machine translation?</title>
<date>2012</date>
<journal>Machine Translation,</journal>
<volume>26</volume>
<issue>4</issue>
<contexts>
<context position="1471" citStr="Lambert et al., 2012" startWordPosition="220" endWordPosition="223">ike intersected HMM models, are preferred. Unlike SMT performance the desired alignment characteristics are similar for small and large training data for the pre-reordering tasks. Moreover, we confirm previous research showing that the fuzzy reordering score is a useful and cheap proxy for performance on SMT reordering tasks. 1 Introduction Word alignment is a key component in all state-ofthe-art statistical machine translation (SMT) systems, and there has been some work exploring the connection between word alignment quality and translation quality (Och and Ney, 2003; Fraser and Marcu, 2007; Lambert et al., 2012). The standard way to evaluate word alignments in this context is by using metrics like alignment error rate (AER) and F-measure on the link level, and the general conclusion appears to be that translation quality benefits from alignments with high recall (rather than precision), at least for large training data. Although many other ways of measuring alignment quality have been proposed, such as working on translation units (Ahrenberg et al., 2000; Ayan and Dorr, 2006; Søgaard and Kuhn, 2009) or using link degree and related measures (Ahrenberg, 2010), these methods have not been used to study</context>
<context position="9719" citStr="Lambert et al. (2012)" startWordPosition="1608" endWordPosition="1611">ven better (see Eq. 4) mostly with a higher weight for recall than for precision. This weight, however, needs to be optimized for each data set, language pair, and gold standard alignment separately. �F(A, G, α) = Precision(A,G) + Recall(A,G)) (4) Ayan and Dorr (2006) on the other hand found some evidence for the importance of precision over recall. However, they used much smaller training data than Fraser and Marcu (2007). They also suggested using a measure called consistent phrase error-rate (CPER), but found that it was hard to assess the impact of alignment on MT, both with AER and CPER. Lambert et al. (2012) performed a study where they investigated the effect of word alignment on MT using a large number of word alignment indicators. They found that there was a difference between large and small datasets in that alignment precision was more important with small data sets, and recall more important with large data sets. Overall they did not find any indicator that was significant over two language pairs and different corpus sizes. There were more significant indicators for large datasets, however. Most researchers who propose new alignment models perform both a gold standard evaluation and an SMT </context>
<context position="14474" citStr="Lambert et al. (2012)" startWordPosition="2396" endWordPosition="2399">gnments, including link degree, reordering of links, and structural correspondence. He used these indicators to profile hand-aligned corpora from different domains. We will not use structural correspondence, which requires a dependency parser, and which we believe is error prone when performed automatically. We will use what we call link degree, i.e., how many alignment links each word obtains. Ahrenberg (2010) used a fine-grained scheme of the percentage for different degrees, including isomorphism 1–1, deletion 0–1, reduction m–1, and paraphrase m–n. Similar link degree classes were used by Lambert et al. (2012). In this work we will reduce these classes into three: 1–1 links, null links, which combine the 0–1 and 1–0 cases, and multi links where there are many words on at least one side. Ahrenberg (2010) also proposed to measure reorderings. He does this by calculating the percentage of links with crossings of different lengths. To define this he only considers adjacent links in the source using the distance between corresponding target words, which means that his metric becomes a directional measure. Reorderings of alignments was also used by Genzel (2010), who used crossing score, the number of cr</context>
<context position="37341" citStr="Lambert et al., 2012" startWordPosition="6388" endWordPosition="6391">sks and metrics. The performance on our two reordering tasks are significantly correlated at .76. Again alignments with good scores on TU metrics, link level precision and crossing link precision are preferable. For this task, the best correlation with weighted link level F-measure is .86 for α = 0.8. Again, we thus see that sparse alignments with high precision on all measures including the crossing subset, are important. 5 Small Training Data Since previous work has suggested that training data size influences the relation between alignment and SMT quality for small and large training data (Lambert et al., 2012), we investigated this issue also for our reordering tasks. We repeated all our experiments on a small dataset, only the News Commentary data from WMT13, with 170K sentences. Due to space constraints we cannot show all results in the paper, but the main findings are 283 summarized in this section. To acquire alignment results we realigned the gold standard concatenated with the smaller data, to reflect the actual quality of alignment with a small dataset. As expected the quality scores tend to be lower with less data. Overall the same systems tend to perform good on each metric with the small </context>
<context position="39942" citStr="Lambert et al., 2012" startWordPosition="6827" endWordPosition="6830">both tasks and Bleu for alignment-based reordering, model4 and HMM with intersection and gd still perform best. For Bleu for POS-based reordering, gdfa and model 3 also give good results. 6 Conclusion and Future Work We have shown that the best combination of alignment and symmetrization models for SMT are not the best models for reordering tasks in our experimental setting. For SMT, high recall is more important than precision with large training data, while precision and recall are of equal importance with small training data. This finding supports previous research (Fraser and Marcu, 2007; Lambert et al., 2012). Translation unit metrics are not predictive of SMT performance. For the large data condition model 3 and 4 with gd and gdfa symmetrization gave the best results, whereas fast align with gd and gdfa was best with small training data. For the two preordering tasks we investigated, however, link level weighted F-measure that gave more weight to precision was important, as well as all TU metrics. It was also important to have high precision for the crossing subset of TUs. Hence, it is more important to reliably find some crossings than to find all crossings. This make sense since the extracted r</context>
</contexts>
<marker>Lambert, Petitrenaud, Ma, Way, 2012</marker>
<rawString>Patrik Lambert, Simon Petitrenaud, Yanjun Ma, and Andy Way. 2012. What types of word alignment improve statistical machine translation? Machine Translation, 26(4):289–323.</rawString>
</citation>
<citation valid="true">
<authors>
<author>Uri Lerner</author>
<author>Slav Petrov</author>
</authors>
<title>Source-side classifier preordering for machine translation.</title>
<date>2013</date>
<booktitle>In Proceedings of EMNLP,</booktitle>
<pages>513--523</pages>
<location>Seattle, Washington, USA.</location>
<contexts>
<context position="27841" citStr="Lerner and Petrov (2013)" startWordPosition="4732" endWordPosition="4735">resented to the decoder as a 1-best reordering (Xia and McCord, 2004), as an n-best list (Li et al., 2007), or as a lattice of possible reorderings (Rottmann and Vogel, 2007; Zhang et al., 2007). In the preordering studies cited above it is often not even stated which alignment model was used. A few authors mention the alignment tool that has been applied but no comparison between different alignment models is performed in any of the papers we are aware of. Li et al. (2007), for example, simply state that they used GIZA++ and gdf symmetrization and that they removed less probable multi links. Lerner and Petrov (2013) use the intersection of HMM alignments and claims that model 4 did not add much value. Genzel (2010) did mention that using a standard model 4 was not successful for his rule learning approach. Instead he used filtered model-1-alignments, which he claims was more successful. However, there are no further analyses or comparisons between the alignments reported in any of these papers. Another type of approach to reordering is to only reorder the data in order to improve word alignments, and to restore the original word order before training the SMT system. This type of approach has the advantag</context>
</contexts>
<marker>Lerner, Petrov, 2013</marker>
<rawString>Uri Lerner and Slav Petrov. 2013. Source-side classifier preordering for machine translation. In Proceedings of EMNLP, pages 513–523, Seattle, Washington, USA.</rawString>
</citation>
<citation valid="true">
<authors>
<author>Chi-Ho Li</author>
<author>Minghui Li</author>
<author>Dongdong Zhang</author>
<author>Mu Li</author>
<author>Ming Zhou</author>
<author>Yi Guan</author>
</authors>
<title>A probabilistic approach to syntax-based reordering for statistical machine translation.</title>
<date>2007</date>
<booktitle>In Proceedings of the 45th Annual Meeting of the ACL,</booktitle>
<pages>720--727</pages>
<location>Prague, Czech Republic.</location>
<contexts>
<context position="27323" citStr="Li et al., 2007" startWordPosition="4641" endWordPosition="4644">el, 2007; Niehues and Kolss, 2009; Genzel, 2010), chunks (Zhang et al., 2007) or parse trees (Xia and McCord, 2004). In general, all these approaches lead to improvements of translation quality. The reordering is always applied on the translation input. It can also be applied on the source side of the training corpora, which sometimes improves the results (Rottmann and Vogel, 2007), but sometimes does not make a difference (Stymne, 2012). When preordering is performed on the translation input, it can be presented to the decoder as a 1-best reordering (Xia and McCord, 2004), as an n-best list (Li et al., 2007), or as a lattice of possible reorderings (Rottmann and Vogel, 2007; Zhang et al., 2007). In the preordering studies cited above it is often not even stated which alignment model was used. A few authors mention the alignment tool that has been applied but no comparison between different alignment models is performed in any of the papers we are aware of. Li et al. (2007), for example, simply state that they used GIZA++ and gdf symmetrization and that they removed less probable multi links. Lerner and Petrov (2013) use the intersection of HMM alignments and claims that model 4 did not add much v</context>
</contexts>
<marker>Li, Li, Zhang, Li, Zhou, Guan, 2007</marker>
<rawString>Chi-Ho Li, Minghui Li, Dongdong Zhang, Mu Li, Ming Zhou, and Yi Guan. 2007. A probabilistic approach to syntax-based reordering for statistical machine translation. In Proceedings of the 45th Annual Meeting of the ACL, pages 720–727, Prague, Czech Republic.</rawString>
</citation>
<citation valid="true">
<authors>
<author>Percy Liang</author>
<author>Ben Taskar</author>
<author>Dan Klein</author>
</authors>
<title>Alignment by agreement.</title>
<date>2006</date>
<booktitle>In Proceedings of NAACL,</booktitle>
<pages>104--111</pages>
<location>New York City, New York, USA.</location>
<contexts>
<context position="10349" citStr="Liang et al., 2006" startWordPosition="1710" endWordPosition="1713"> study where they investigated the effect of word alignment on MT using a large number of word alignment indicators. They found that there was a difference between large and small datasets in that alignment precision was more important with small data sets, and recall more important with large data sets. Overall they did not find any indicator that was significant over two language pairs and different corpus sizes. There were more significant indicators for large datasets, however. Most researchers who propose new alignment models perform both a gold standard evaluation and an SMT evaluation (Liang et al., 2006; Ganchev et al., 2008; Junczys-Dowmunt and Szał, 2012; Dyer et al., 2013). The relation between the two types of evaluation is often quite weak. Several of these studies only show AER on their gold standard, despite its well-known shortcomings. Even though many studies have shown some relation between translation quality and AER or weighted F-measure, it has rarely been investigated thoroughly in its own right, and, as far as we are aware, not for other tasks than SMT. Furthermore, most of these studies considers nothing else but link level agreement. In this paper we take a broader view on a</context>
</contexts>
<marker>Liang, Taskar, Klein, 2006</marker>
<rawString>Percy Liang, Ben Taskar, and Dan Klein. 2006. Alignment by agreement. In Proceedings of NAACL, pages 104–111, New York City, New York, USA.</rawString>
</citation>
<citation valid="true">
<authors>
<author>Yang Liu</author>
<author>Qun Liu</author>
<author>Shouxun Lin</author>
</authors>
<title>Loglinear models for word alignment.</title>
<date>2005</date>
<booktitle>In Proceedings of ACL,</booktitle>
<pages>459--466</pages>
<location>Ann Arbor, Michigan, USA.</location>
<contexts>
<context position="6151" citStr="Liu et al., 2005" startWordPosition="987" endWordPosition="990">inks) but not vice versa. It is generally desirable to also allow n–1 and n–m links, and to achieve this it is common practice to perform word alignment in both directions and to symmetrize them using some heuristic. A number of common symmetrization strategies are described in Table 1 (Koehn et al., 2005). There are also other alternatives, such as the refined method (Och and Ney, 2003), or link deletion from the union (Fossum et al., 2008). There is also a wide range of alternative approaches to word alignment. For example, various discriminative models have been proposed in the literature (Liu et al., 2005; Moore, 2005; Taskar et al., 2005). Their advantage is that they may integrate a wide range of features that may lead to improved alignment quality. However, most of Symmetrization Description int: intersection ATS ∩ AST uni: union ATS ∪ AST gd: grow-diag intersection plus adjacent links from the union if both linked words are unaligned gdf: grow-diag-final gd with links from the union added in a final step if either linked word is unaligned gdfa: gd with links from the union grow-diag-final-and added in a final step if both linked words are unaligned Table 1: Symmetrization strategies for wo</context>
</contexts>
<marker>Liu, Liu, Lin, 2005</marker>
<rawString>Yang Liu, Qun Liu, and Shouxun Lin. 2005. Loglinear models for word alignment. In Proceedings of ACL, pages 459–466, Ann Arbor, Michigan, USA.</rawString>
</citation>
<citation valid="true">
<authors>
<author>Magnus Merkel</author>
<author>Jody Foo</author>
</authors>
<title>Terminology extraction and term ranking for standardizing term banks.</title>
<date>2007</date>
<booktitle>In Proceedings of the 16th Nordic Conference on Computational Linguistics,</booktitle>
<pages>349--354</pages>
<location>Tartu, Estonia.</location>
<contexts>
<context position="2294" citStr="Merkel and Foo, 2007" startWordPosition="352" endWordPosition="355">ion quality benefits from alignments with high recall (rather than precision), at least for large training data. Although many other ways of measuring alignment quality have been proposed, such as working on translation units (Ahrenberg et al., 2000; Ayan and Dorr, 2006; Søgaard and Kuhn, 2009) or using link degree and related measures (Ahrenberg, 2010), these methods have not been used to study the relation between alignment and translation quality, with the exception of Lambert et al. (2012). Word alignment is also used for many other tasks besides translation, including term bank creation (Merkel and Foo, 2007), cross-lingual annotation projection for part-of-speech tagging (Yarowsky et al., 2001), semantic roles (Pado and Lapata, 2005), pronoun anaphora (Postolache et al., 2006), and cross-lingual clustering (T¨ackstr¨om et al., 2012). Even within SMT itself, there are tasks such as reordering that often make crucial use of word alignments. For instance, source language reordering commonly relies on rules learnt automatically from word-aligned data (e.g., Xia and McCord (2004)). As far as we know, no one has studied the impact of alignment quality on these additional tasks, and it seems to be tacit</context>
</contexts>
<marker>Merkel, Foo, 2007</marker>
<rawString>Magnus Merkel and Jody Foo. 2007. Terminology extraction and term ranking for standardizing term banks. In Proceedings of the 16th Nordic Conference on Computational Linguistics, pages 349–354, Tartu, Estonia.</rawString>
</citation>
<citation valid="true">
<authors>
<author>Robert C Moore</author>
</authors>
<title>A discriminative framework for bilingual word alignment.</title>
<date>2005</date>
<booktitle>In Proceedings of HLT and EMNLP,</booktitle>
<pages>81--88</pages>
<location>Vancouver, British Columbia, Canada.</location>
<contexts>
<context position="6164" citStr="Moore, 2005" startWordPosition="991" endWordPosition="992"> versa. It is generally desirable to also allow n–1 and n–m links, and to achieve this it is common practice to perform word alignment in both directions and to symmetrize them using some heuristic. A number of common symmetrization strategies are described in Table 1 (Koehn et al., 2005). There are also other alternatives, such as the refined method (Och and Ney, 2003), or link deletion from the union (Fossum et al., 2008). There is also a wide range of alternative approaches to word alignment. For example, various discriminative models have been proposed in the literature (Liu et al., 2005; Moore, 2005; Taskar et al., 2005). Their advantage is that they may integrate a wide range of features that may lead to improved alignment quality. However, most of Symmetrization Description int: intersection ATS ∩ AST uni: union ATS ∪ AST gd: grow-diag intersection plus adjacent links from the union if both linked words are unaligned gdf: grow-diag-final gd with links from the union added in a final step if either linked word is unaligned gdfa: gd with links from the union grow-diag-final-and added in a final step if both linked words are unaligned Table 1: Symmetrization strategies for word alignments</context>
</contexts>
<marker>Moore, 2005</marker>
<rawString>Robert C. Moore. 2005. A discriminative framework for bilingual word alignment. In Proceedings of HLT and EMNLP, pages 81–88, Vancouver, British Columbia, Canada.</rawString>
</citation>
<citation valid="true">
<authors>
<author>Jan Niehues</author>
<author>Muntsin Kolss</author>
</authors>
<title>A POS-based model for long-range reorderings in SMT.</title>
<date>2009</date>
<booktitle>In Proceedings of WMT,</booktitle>
<pages>206--214</pages>
<location>Athens, Greece.</location>
<contexts>
<context position="26740" citStr="Niehues and Kolss, 2009" startWordPosition="4541" endWordPosition="4544"> part of the evaluation of MT systems (Birch and Osborne, 2011). We can distinguish two main types of approaches to preordering in SMT, either by using hand-written rules, which often operate on syntactic trees (Collins et al., 2005), or by reordering rules that are learnt automatically based on a word aligned corpus (Xia and McCord, 2004). The latter approach is of interest to us, since it is based on word alignments. There has been much work on automatic learning of reordering rules, which can be based on different levels of annotation, such as part-of-speech tags (Rottmann and Vogel, 2007; Niehues and Kolss, 2009; Genzel, 2010), chunks (Zhang et al., 2007) or parse trees (Xia and McCord, 2004). In general, all these approaches lead to improvements of translation quality. The reordering is always applied on the translation input. It can also be applied on the source side of the training corpora, which sometimes improves the results (Rottmann and Vogel, 2007), but sometimes does not make a difference (Stymne, 2012). When preordering is performed on the translation input, it can be presented to the decoder as a 1-best reordering (Xia and McCord, 2004), as an n-best list (Li et al., 2007), or as a lattice</context>
<context position="30894" citStr="Niehues and Kolss (2009)" startWordPosition="5268" endWordPosition="5271">y on the reordering component (Talbot et al., 2011). It compares a system reordering to a reference reordering, by measuring how many chunks that have to be moved to get an identical word order, see Eq. 9, where C is the number of contiguously aligned chunks, and M the number of words. To find the reference ordering we apply the method of Holmqvist et al. (2009), described in Section 4.2, to the gold standard alignment. C − 1 FRS = 1 − (9) M − 1 4.1 Part-of-Speech-Based Reordering Our first reordering task is a part-of-speech-based preordering method described by Rottmann and Vogel (2007) and Niehues and Kolss (2009), which was successfully used for German–English translation. Rules are learnt from a word aligned POS-tagged corpus. Based on the alignments, tag patterns are identified that give rise to specific reorderings. These patterns are then scored based on relative frequency.4 The rules are then applied to the translation input to create a reordering lattice, with normalized edge scores based on rule scores. In our experiments we only use rules with a score higher than 0.2, to limit the size of the lattices. For calculating FRS, we pick the highest scoring 1-best word order from the lattices. We lea</context>
</contexts>
<marker>Niehues, Kolss, 2009</marker>
<rawString>Jan Niehues and Muntsin Kolss. 2009. A POS-based model for long-range reorderings in SMT. In Proceedings of WMT, pages 206–214, Athens, Greece.</rawString>
</citation>
<citation valid="true">
<authors>
<author>Franz Josef Och</author>
<author>Hermann Ney</author>
</authors>
<title>A comparison of alignment models for statistical machine translation.</title>
<date>2000</date>
<booktitle>In Proceedings of Coling,</booktitle>
<pages>1086--1090</pages>
<location>Saarbr¨ucken, Germany.</location>
<contexts>
<context position="8057" citStr="Och and Ney, 2000" startWordPosition="1299" endWordPosition="1302">stimating phrase alignment probabilities. Similarly, word alignments are also used for estimating rule probabilities in various kinds of hierarchical and syntactic SMT (Chiang, 2007; Yamada and Knight, 2002; Galley et al., 2004). Intrinsic evaluation of word alignment is generally based on a comparison to a gold standard of human alignments. Based on the gold standard, metrics like precision, recall and F-measure can be calculated for each alignment link, see Eqs. 1– 2, where A are hypothesized alignment links and G are gold standard links. Another common metric is alignment error rate (AER) (Och and Ney, 2000), which is based on a distinction between sure, S, and possible, P, links in the gold standard. 1−AER is identical to balanced F-measure when the gold standard does not make a distinction between S and P. Precision(A, G) = |G ∩ A |(1) |A| Recall(A, G) = |G ∩ A |(2) |G| AER = 1 − |P ∩ A |+ |S ∩ A |(3) |S |+ |A| 276 Crossing = 8 SKDT = 8/66 ≈ 0.65 6 1–1 links 3 multi links 0 null links Figure 1: An example alignment illustrating n–1, 1–m and crossing links. The relation between word alignment quality and PBSMT has been studied by some researchers. Och and Ney (2000) looked at the impact of IBM a</context>
</contexts>
<marker>Och, Ney, 2000</marker>
<rawString>Franz Josef Och and Hermann Ney. 2000. A comparison of alignment models for statistical machine translation. In Proceedings of Coling, pages 1086– 1090, Saarbr¨ucken, Germany.</rawString>
</citation>
<citation valid="true">
<authors>
<author>Franz Josef Och</author>
<author>Hermann Ney</author>
</authors>
<title>A systematic comparison of various statistical alignment models.</title>
<date>2003</date>
<journal>Computational Linguistics,</journal>
<volume>29</volume>
<issue>1</issue>
<contexts>
<context position="1424" citStr="Och and Ney, 2003" startWordPosition="212" endWordPosition="215">its, and on the subset of crossing links, like intersected HMM models, are preferred. Unlike SMT performance the desired alignment characteristics are similar for small and large training data for the pre-reordering tasks. Moreover, we confirm previous research showing that the fuzzy reordering score is a useful and cheap proxy for performance on SMT reordering tasks. 1 Introduction Word alignment is a key component in all state-ofthe-art statistical machine translation (SMT) systems, and there has been some work exploring the connection between word alignment quality and translation quality (Och and Ney, 2003; Fraser and Marcu, 2007; Lambert et al., 2012). The standard way to evaluate word alignments in this context is by using metrics like alignment error rate (AER) and F-measure on the link level, and the general conclusion appears to be that translation quality benefits from alignments with high recall (rather than precision), at least for large training data. Although many other ways of measuring alignment quality have been proposed, such as working on translation units (Ahrenberg et al., 2000; Ayan and Dorr, 2006; Søgaard and Kuhn, 2009) or using link degree and related measures (Ahrenberg, 2</context>
<context position="5925" citStr="Och and Ney, 2003" startWordPosition="949" endWordPosition="952">natives to model 2, such as the HMM model (Vogel et al., 1996) and fast align (Dyer et al., 2013). All these generative models produce directional alignments where one word in the source can be linked to many target words (1–m links) but not vice versa. It is generally desirable to also allow n–1 and n–m links, and to achieve this it is common practice to perform word alignment in both directions and to symmetrize them using some heuristic. A number of common symmetrization strategies are described in Table 1 (Koehn et al., 2005). There are also other alternatives, such as the refined method (Och and Ney, 2003), or link deletion from the union (Fossum et al., 2008). There is also a wide range of alternative approaches to word alignment. For example, various discriminative models have been proposed in the literature (Liu et al., 2005; Moore, 2005; Taskar et al., 2005). Their advantage is that they may integrate a wide range of features that may lead to improved alignment quality. However, most of Symmetrization Description int: intersection ATS ∩ AST uni: union ATS ∪ AST gd: grow-diag intersection plus adjacent links from the union if both linked words are unaligned gdf: grow-diag-final gd with links</context>
<context position="16642" citStr="Och and Ney, 2003" startWordPosition="2762" endWordPosition="2765">nd Fmeasure for the subset of translation units that are involved in a crossing. SKTD = . I |crossing link pairs |(8) v (n2 − n)/2 3.1 Alignment Experiments We perform all our experiments for German– English. The alignment indicators are calculated on a corpus of 987 hand aligned sentences (Pado and Lapata, 2005). The gold standard contains explicit null links, which the symmetrized automatic alignments do not. To allow a straightforward comparison we consistently remove all null links when comparing system alignments to the gold standard. For creating the automatic alignments we used GIZA++ (Och and Ney, 2003) to compute directional alignments for model 2–4 and the HMM model, and fast align (fa) (Dyer et al., 2013) as newer alternatives to model 2. These models require large amounts of data to be estimated reliably. To achieve this we concatenated the gold standard with the large SMT training data (see �PMWU = AU ∈A �RMWU = GU ∈G (7) |G| 278 Table 2: Values for alignment quality indicators for the different alignments, where 2–4, HMM, and fa are alignment models, and symmetrization strategies refer to Table 1 Total SKTD P R F Crossdiff 30163 .292 – – – 0 11229 .251 .663 .522 .584 18934 8077 .221 .7</context>
</contexts>
<marker>Och, Ney, 2003</marker>
<rawString>Franz Josef Och and Hermann Ney. 2003. A systematic comparison of various statistical alignment models. Computational Linguistics, 29(1):19–51.</rawString>
</citation>
<citation valid="true">
<authors>
<author>Franz Josef Och</author>
<author>Christoph Tillmann</author>
<author>Hermann Ney</author>
</authors>
<title>Improved alignment models for statistical machine translation.</title>
<date>1999</date>
<booktitle>In Proceedings of the Joint Conference of EMNLP and Very Large Corpora,</booktitle>
<pages>20--28</pages>
<location>College Park, Maryland, USA.</location>
<contexts>
<context position="8724" citStr="Och et al., 1999" startWordPosition="1436" endWordPosition="1439"> and possible, P, links in the gold standard. 1−AER is identical to balanced F-measure when the gold standard does not make a distinction between S and P. Precision(A, G) = |G ∩ A |(1) |A| Recall(A, G) = |G ∩ A |(2) |G| AER = 1 − |P ∩ A |+ |S ∩ A |(3) |S |+ |A| 276 Crossing = 8 SKDT = 8/66 ≈ 0.65 6 1–1 links 3 multi links 0 null links Figure 1: An example alignment illustrating n–1, 1–m and crossing links. The relation between word alignment quality and PBSMT has been studied by some researchers. Och and Ney (2000) looked at the impact of IBM and HMM models on the alignment template approach (Och et al., 1999) in terms of AER. They found that AER correlates with human evaluation of sentence level quality, but not with word error rate. Fraser and Marcu (2007) found that there is no correlation between AER and Bleu (Papineni et al., 2002), especially not when the Pset is large. They found that a balanced F-measure is a better indicator of Bleu, but that a weighted F-measure is even better (see Eq. 4) mostly with a higher weight for recall than for precision. This weight, however, needs to be optimized for each data set, language pair, and gold standard alignment separately. �F(A, G, α) = Precision(A,</context>
</contexts>
<marker>Och, Tillmann, Ney, 1999</marker>
<rawString>Franz Josef Och, Christoph Tillmann, and Hermann Ney. 1999. Improved alignment models for statistical machine translation. In Proceedings of the Joint Conference of EMNLP and Very Large Corpora, pages 20–28, College Park, Maryland, USA.</rawString>
</citation>
<citation valid="true">
<authors>
<author>Franz Josef Och</author>
</authors>
<title>Minimum error rate training in statistical machine translation.</title>
<date>2003</date>
<booktitle>In Proceedings of ACL,</booktitle>
<pages>160--167</pages>
<location>Sapporo, Japan.</location>
<contexts>
<context position="23259" citStr="Och, 2003" startWordPosition="3966" endWordPosition="3967"> translation unit level. Significant correlations are marked with bold (&lt; 0.01). data.2 We trained a German–English system on 2M sentences from Europarl and News Commentary. We used the target side of the parallel corpus and the SRILM toolkit (Stolcke, 2002) to train a 5- gram language model. For training the translation model and for decoding we used the Moses toolkit (Koehn et al., 2007). We applied a standard feature set consisting of a language model feature, four translation model features, word penalty, phrase penalty, and distortion cost. For tuning we used minimum error-rate training (Och, 2003). In order to minimize the risk of tuning influencing the results, we used a fixed set of weights for each experiment, tuned on a model 4+gdfa alignment.3 For tuning we used newstest2009 with 2525 sentences, and for testing we used newstest2013 with 3000 sentences. Evaluation was performed using the Bleu metric (Papineni et al., 2002). The same system setup was used for the SMT systems with reordering. Table 4 shows the results on the SMT task. Model 3 and 4 with gd/gdfa symmetrization yield the highest scores. There is a larger difference between systems with different symmetrization than bet</context>
</contexts>
<marker>Och, 2003</marker>
<rawString>Franz Josef Och. 2003. Minimum error rate training in statistical machine translation. In Proceedings of ACL, pages 160–167, Sapporo, Japan.</rawString>
</citation>
<citation valid="true">
<authors>
<author>Sebastian Pado</author>
<author>Mirella Lapata</author>
</authors>
<title>Crosslinguistic projection of role-semantic information.</title>
<date>2005</date>
<booktitle>In Proceedings of HLT and EMNLP,</booktitle>
<pages>859--866</pages>
<location>Vancouver, British Columbia, Canada.</location>
<contexts>
<context position="2422" citStr="Pado and Lapata, 2005" startWordPosition="368" endWordPosition="371">other ways of measuring alignment quality have been proposed, such as working on translation units (Ahrenberg et al., 2000; Ayan and Dorr, 2006; Søgaard and Kuhn, 2009) or using link degree and related measures (Ahrenberg, 2010), these methods have not been used to study the relation between alignment and translation quality, with the exception of Lambert et al. (2012). Word alignment is also used for many other tasks besides translation, including term bank creation (Merkel and Foo, 2007), cross-lingual annotation projection for part-of-speech tagging (Yarowsky et al., 2001), semantic roles (Pado and Lapata, 2005), pronoun anaphora (Postolache et al., 2006), and cross-lingual clustering (T¨ackstr¨om et al., 2012). Even within SMT itself, there are tasks such as reordering that often make crucial use of word alignments. For instance, source language reordering commonly relies on rules learnt automatically from word-aligned data (e.g., Xia and McCord (2004)). As far as we know, no one has studied the impact of alignment quality on these additional tasks, and it seems to be tacitly assumed that alignments that are good for translation are also good for other tasks. In this paper we set out to explore the </context>
<context position="16338" citStr="Pado and Lapata, 2005" startWordPosition="2715" endWordPosition="2718">s how much reordering there is. To quantify this relative to the gold standard we also report the absolute difference between the number of gold standard crossings and system crossings, which we call Crossdiff. To account for the quality of crossings, to some extent, we will also report precision, recall, and Fmeasure for the subset of translation units that are involved in a crossing. SKTD = . I |crossing link pairs |(8) v (n2 − n)/2 3.1 Alignment Experiments We perform all our experiments for German– English. The alignment indicators are calculated on a corpus of 987 hand aligned sentences (Pado and Lapata, 2005). The gold standard contains explicit null links, which the symmetrized automatic alignments do not. To allow a straightforward comparison we consistently remove all null links when comparing system alignments to the gold standard. For creating the automatic alignments we used GIZA++ (Och and Ney, 2003) to compute directional alignments for model 2–4 and the HMM model, and fast align (fa) (Dyer et al., 2013) as newer alternatives to model 2. These models require large amounts of data to be estimated reliably. To achieve this we concatenated the gold standard with the large SMT training data (s</context>
</contexts>
<marker>Pado, Lapata, 2005</marker>
<rawString>Sebastian Pado and Mirella Lapata. 2005. Crosslinguistic projection of role-semantic information. In Proceedings of HLT and EMNLP, pages 859–866, Vancouver, British Columbia, Canada.</rawString>
</citation>
<citation valid="true">
<authors>
<author>Kishore Papineni</author>
<author>Salim Roukos</author>
<author>Todd Ward</author>
<author>WeiJing Zhu</author>
</authors>
<title>BLEU: A method for automatic evaluation of machine translation.</title>
<date>2002</date>
<booktitle>In Proceedings ofACL,</booktitle>
<pages>311--318</pages>
<location>Philadelphia, Pennsylvania, USA.</location>
<contexts>
<context position="8955" citStr="Papineni et al., 2002" startWordPosition="1476" endWordPosition="1479">1 − |P ∩ A |+ |S ∩ A |(3) |S |+ |A| 276 Crossing = 8 SKDT = 8/66 ≈ 0.65 6 1–1 links 3 multi links 0 null links Figure 1: An example alignment illustrating n–1, 1–m and crossing links. The relation between word alignment quality and PBSMT has been studied by some researchers. Och and Ney (2000) looked at the impact of IBM and HMM models on the alignment template approach (Och et al., 1999) in terms of AER. They found that AER correlates with human evaluation of sentence level quality, but not with word error rate. Fraser and Marcu (2007) found that there is no correlation between AER and Bleu (Papineni et al., 2002), especially not when the Pset is large. They found that a balanced F-measure is a better indicator of Bleu, but that a weighted F-measure is even better (see Eq. 4) mostly with a higher weight for recall than for precision. This weight, however, needs to be optimized for each data set, language pair, and gold standard alignment separately. �F(A, G, α) = Precision(A,G) + Recall(A,G)) (4) Ayan and Dorr (2006) on the other hand found some evidence for the importance of precision over recall. However, they used much smaller training data than Fraser and Marcu (2007). They also suggested using a m</context>
<context position="23595" citStr="Papineni et al., 2002" startWordPosition="4021" endWordPosition="4024">odel and for decoding we used the Moses toolkit (Koehn et al., 2007). We applied a standard feature set consisting of a language model feature, four translation model features, word penalty, phrase penalty, and distortion cost. For tuning we used minimum error-rate training (Och, 2003). In order to minimize the risk of tuning influencing the results, we used a fixed set of weights for each experiment, tuned on a model 4+gdfa alignment.3 For tuning we used newstest2009 with 2525 sentences, and for testing we used newstest2013 with 3000 sentences. Evaluation was performed using the Bleu metric (Papineni et al., 2002). The same system setup was used for the SMT systems with reordering. Table 4 shows the results on the SMT task. Model 3 and 4 with gd/gdfa symmetrization yield the highest scores. There is a larger difference between systems with different symmetrization than between systems with different alignment models. The sparse intersection symmetrization gives the poorest results. The top row in Table 5 shows correlations between Bleu and all word alignment quality indicators. There are significant correlations with link level recall. A weighted link level F-measure with α = 0.3 gives a significant co</context>
</contexts>
<marker>Papineni, Roukos, Ward, Zhu, 2002</marker>
<rawString>Kishore Papineni, Salim Roukos, Todd Ward, and WeiJing Zhu. 2002. BLEU: A method for automatic evaluation of machine translation. In Proceedings ofACL, pages 311–318, Philadelphia, Pennsylvania, USA.</rawString>
</citation>
<citation valid="true">
<authors>
<author>a</author>
</authors>
<title>Transferring coreference chains through word alignment.</title>
<date>2006</date>
<booktitle>In Proceedings of LREC,</booktitle>
<pages>889--892</pages>
<location>Genoa, Italy.</location>
<marker>a, 2006</marker>
<rawString>Oana Postolache, Dan Cristea, and Constantin Or˘asan. a. 2006. Transferring coreference chains through word alignment. In Proceedings of LREC, pages 889–892, Genoa, Italy.</rawString>
</citation>
<citation valid="true">
<authors>
<author>Kay Rottmann</author>
<author>Stephan Vogel</author>
</authors>
<title>Word reordering in statistical machine translation with a POS-based distortion model.</title>
<date>2007</date>
<booktitle>In Proceedings of the 11th International Conference on Theoretical and Methodological Issues in Machine Translation,</booktitle>
<pages>171--180</pages>
<location>Sk¨ovde,</location>
<contexts>
<context position="26715" citStr="Rottmann and Vogel, 2007" startWordPosition="4537" endWordPosition="4540">e reordering is modeled as part of the evaluation of MT systems (Birch and Osborne, 2011). We can distinguish two main types of approaches to preordering in SMT, either by using hand-written rules, which often operate on syntactic trees (Collins et al., 2005), or by reordering rules that are learnt automatically based on a word aligned corpus (Xia and McCord, 2004). The latter approach is of interest to us, since it is based on word alignments. There has been much work on automatic learning of reordering rules, which can be based on different levels of annotation, such as part-of-speech tags (Rottmann and Vogel, 2007; Niehues and Kolss, 2009; Genzel, 2010), chunks (Zhang et al., 2007) or parse trees (Xia and McCord, 2004). In general, all these approaches lead to improvements of translation quality. The reordering is always applied on the translation input. It can also be applied on the source side of the training corpora, which sometimes improves the results (Rottmann and Vogel, 2007), but sometimes does not make a difference (Stymne, 2012). When preordering is performed on the translation input, it can be presented to the decoder as a 1-best reordering (Xia and McCord, 2004), as an n-best list (Li et al</context>
<context position="30865" citStr="Rottmann and Vogel (2007)" startWordPosition="5263" endWordPosition="5266">marked with bold (&lt; 0.01). only on the reordering component (Talbot et al., 2011). It compares a system reordering to a reference reordering, by measuring how many chunks that have to be moved to get an identical word order, see Eq. 9, where C is the number of contiguously aligned chunks, and M the number of words. To find the reference ordering we apply the method of Holmqvist et al. (2009), described in Section 4.2, to the gold standard alignment. C − 1 FRS = 1 − (9) M − 1 4.1 Part-of-Speech-Based Reordering Our first reordering task is a part-of-speech-based preordering method described by Rottmann and Vogel (2007) and Niehues and Kolss (2009), which was successfully used for German–English translation. Rules are learnt from a word aligned POS-tagged corpus. Based on the alignments, tag patterns are identified that give rise to specific reorderings. These patterns are then scored based on relative frequency.4 The rules are then applied to the translation input to create a reordering lattice, with normalized edge scores based on rule scores. In our experiments we only use rules with a score higher than 0.2, to limit the size of the lattices. For calculating FRS, we pick the highest scoring 1-best word or</context>
</contexts>
<marker>Rottmann, Vogel, 2007</marker>
<rawString>Kay Rottmann and Stephan Vogel. 2007. Word reordering in statistical machine translation with a POS-based distortion model. In Proceedings of the 11th International Conference on Theoretical and Methodological Issues in Machine Translation, pages 171–180, Sk¨ovde, Sweden.</rawString>
</citation>
<citation valid="true">
<authors>
<author>Anders Søgaard</author>
<author>Jonas Kuhn</author>
</authors>
<title>Empirical lower bounds on alignment error rates in syntaxbased machine translation.</title>
<date>2009</date>
<booktitle>In Proceedings of the Third Workshop on Syntax and Structure in Statistical Translation,</booktitle>
<pages>pages</pages>
<location>Boulder, Colorado, USA.</location>
<contexts>
<context position="1968" citStr="Søgaard and Kuhn, 2009" startWordPosition="300" endWordPosition="303">ction between word alignment quality and translation quality (Och and Ney, 2003; Fraser and Marcu, 2007; Lambert et al., 2012). The standard way to evaluate word alignments in this context is by using metrics like alignment error rate (AER) and F-measure on the link level, and the general conclusion appears to be that translation quality benefits from alignments with high recall (rather than precision), at least for large training data. Although many other ways of measuring alignment quality have been proposed, such as working on translation units (Ahrenberg et al., 2000; Ayan and Dorr, 2006; Søgaard and Kuhn, 2009) or using link degree and related measures (Ahrenberg, 2010), these methods have not been used to study the relation between alignment and translation quality, with the exception of Lambert et al. (2012). Word alignment is also used for many other tasks besides translation, including term bank creation (Merkel and Foo, 2007), cross-lingual annotation projection for part-of-speech tagging (Yarowsky et al., 2001), semantic roles (Pado and Lapata, 2005), pronoun anaphora (Postolache et al., 2006), and cross-lingual clustering (T¨ackstr¨om et al., 2012). Even within SMT itself, there are tasks suc</context>
<context position="11733" citStr="Søgaard and Kuhn (2009)" startWordPosition="1939" endWordPosition="1942">ndicators. The first group is the classic group where metrics are calculated on the alignment link level, which has been used in several studies. In our experiments we use a gold standard that does not make use of distinctions between sure and possible links, as suggested by Fraser and Marcu (2007). With this, we can calculate the standard metrics P(recision) R(ecall) and F(-measure). We will mainly use balanced F-measure, but occasionally also report weighted F-measure. As noted before, 1−AER is equivalent to balanced F when only sure links are used, and will thus not be reported separately. Søgaard and Kuhn (2009) and Søgaard and Wu (2009) suggested working on the translation unit (TU) level, instead of the link level. A translation unit, or cept (Goutte et al., 2004), is defined as a maximally connected subgraph of an alignment. In Figure 1, the twelve links form nine translation units. Søgaard and Wu (2009) suggest the metric TUER, translation unit error rate, shown in Eq. 5, where AU are hypothesized translation units, and GU are gold standard translation units.1 They use TUER to establish lower bounds for the coverage of alignments from different formalisms, not to evaluate SMT. While they only use</context>
</contexts>
<marker>Søgaard, Kuhn, 2009</marker>
<rawString>Anders Søgaard and Jonas Kuhn. 2009. Empirical lower bounds on alignment error rates in syntaxbased machine translation. In Proceedings of the Third Workshop on Syntax and Structure in Statistical Translation, pages 19–27, Boulder, Colorado, USA.</rawString>
</citation>
<citation valid="true">
<authors>
<author>Anders Søgaard</author>
<author>Dekai Wu</author>
</authors>
<title>Empirical lower bounds on translation unit error rate for the full class of inversion transduction grammars.</title>
<date>2009</date>
<booktitle>In Proceedings of 11th International Conference on Parsing Technologies,</booktitle>
<pages>33--36</pages>
<location>Paris, France.</location>
<contexts>
<context position="11759" citStr="Søgaard and Wu (2009)" startWordPosition="1944" endWordPosition="1947">s the classic group where metrics are calculated on the alignment link level, which has been used in several studies. In our experiments we use a gold standard that does not make use of distinctions between sure and possible links, as suggested by Fraser and Marcu (2007). With this, we can calculate the standard metrics P(recision) R(ecall) and F(-measure). We will mainly use balanced F-measure, but occasionally also report weighted F-measure. As noted before, 1−AER is equivalent to balanced F when only sure links are used, and will thus not be reported separately. Søgaard and Kuhn (2009) and Søgaard and Wu (2009) suggested working on the translation unit (TU) level, instead of the link level. A translation unit, or cept (Goutte et al., 2004), is defined as a maximally connected subgraph of an alignment. In Figure 1, the twelve links form nine translation units. Søgaard and Wu (2009) suggest the metric TUER, translation unit error rate, shown in Eq. 5, where AU are hypothesized translation units, and GU are gold standard translation units.1 They use TUER to establish lower bounds for the coverage of alignments from different formalisms, not to evaluate SMT. While they only use TUER, it 1TUER is similar</context>
</contexts>
<marker>Søgaard, Wu, 2009</marker>
<rawString>Anders Søgaard and Dekai Wu. 2009. Empirical lower bounds on translation unit error rate for the full class of inversion transduction grammars. In Proceedings of 11th International Conference on Parsing Technologies, pages 33–36, Paris, France.</rawString>
</citation>
<citation valid="true">
<authors>
<author>Andreas Stolcke</author>
</authors>
<title>SRILM – an extensible language modeling toolkit.</title>
<date>2002</date>
<booktitle>In Proceedings of ICSLP,</booktitle>
<pages>901--904</pages>
<location>Denver, Colorado, USA.</location>
<contexts>
<context position="22907" citStr="Stolcke, 2002" startWordPosition="3910" endWordPosition="3911">n/recall trade-off. 3.2 SMT Experiments For reference, we first study the impact of alignment on SMT performance. Our SMT system is a standard PBSMT system trained on WMT13 Translation unit Link level ↓ P R F P .95 .77 .90 R −.57 −.22 −.42 F .70 .90 .83 Table 3: Pearson correlations between gold standard word alignment evaluation on the link level and on translation unit level. Significant correlations are marked with bold (&lt; 0.01). data.2 We trained a German–English system on 2M sentences from Europarl and News Commentary. We used the target side of the parallel corpus and the SRILM toolkit (Stolcke, 2002) to train a 5- gram language model. For training the translation model and for decoding we used the Moses toolkit (Koehn et al., 2007). We applied a standard feature set consisting of a language model feature, four translation model features, word penalty, phrase penalty, and distortion cost. For tuning we used minimum error-rate training (Och, 2003). In order to minimize the risk of tuning influencing the results, we used a fixed set of weights for each experiment, tuned on a model 4+gdfa alignment.3 For tuning we used newstest2009 with 2525 sentences, and for testing we used newstest2013 wit</context>
</contexts>
<marker>Stolcke, 2002</marker>
<rawString>Andreas Stolcke. 2002. SRILM – an extensible language modeling toolkit. In Proceedings of ICSLP, pages 901–904, Denver, Colorado, USA.</rawString>
</citation>
<citation valid="true">
<authors>
<author>Sara Stymne</author>
<author>Maria Holmqvist</author>
<author>Lars Ahrenberg</author>
</authors>
<title>Vs and OOVs: Two problems for translation between German and English.</title>
<date>2010</date>
<booktitle>In Proceedings of WMT and MetricsMATR,</booktitle>
<pages>183--188</pages>
<location>Uppsala,</location>
<contexts>
<context position="28608" citStr="Stymne et al., 2010" startWordPosition="4862" endWordPosition="4865"> not successful for his rule learning approach. Instead he used filtered model-1-alignments, which he claims was more successful. However, there are no further analyses or comparisons between the alignments reported in any of these papers. Another type of approach to reordering is to only reorder the data in order to improve word alignments, and to restore the original word order before training the SMT system. This type of approach has the advantage that no modifications are needed for the translation input. This approach has also been used both with hand-written rules (Carpuat et al., 2010; Stymne et al., 2010) and with rules based on initial word alignments on non-reordered texts (Holmqvist et al., 2009). For the latter approach a small study of the effect of gd and gdfa symmetrizations was presented, which only showed small variations in quality scores (Holmqvist et al., 2012). Below we present the two tasks that we study in this paper: part-of-speech-based reordering for creating input lattices for SMT and alignmentbased reordering for improving phrase-tables. We evaluate the performance of these tasks in relation to the use of different alignment models and symmetrization heuristics. For these t</context>
</contexts>
<marker>Stymne, Holmqvist, Ahrenberg, 2010</marker>
<rawString>Sara Stymne, Maria Holmqvist, and Lars Ahrenberg. 2010. Vs and OOVs: Two problems for translation between German and English. In Proceedings of WMT and MetricsMATR, pages 183–188, Uppsala, Sweden.</rawString>
</citation>
<citation valid="true">
<authors>
<author>Sara Stymne</author>
</authors>
<title>Clustered word classes for preordering in statistical machine translation.</title>
<date>2012</date>
<booktitle>In Proceedings of ROBUS-UNSUP 2012: Joint Workshop on Unsupervised and Semi-Supervised Learning in NLP,</booktitle>
<pages>28--34</pages>
<location>Avignon, France.</location>
<contexts>
<context position="27148" citStr="Stymne, 2012" startWordPosition="4610" endWordPosition="4611">nts. There has been much work on automatic learning of reordering rules, which can be based on different levels of annotation, such as part-of-speech tags (Rottmann and Vogel, 2007; Niehues and Kolss, 2009; Genzel, 2010), chunks (Zhang et al., 2007) or parse trees (Xia and McCord, 2004). In general, all these approaches lead to improvements of translation quality. The reordering is always applied on the translation input. It can also be applied on the source side of the training corpora, which sometimes improves the results (Rottmann and Vogel, 2007), but sometimes does not make a difference (Stymne, 2012). When preordering is performed on the translation input, it can be presented to the decoder as a 1-best reordering (Xia and McCord, 2004), as an n-best list (Li et al., 2007), or as a lattice of possible reorderings (Rottmann and Vogel, 2007; Zhang et al., 2007). In the preordering studies cited above it is often not even stated which alignment model was used. A few authors mention the alignment tool that has been applied but no comparison between different alignment models is performed in any of the papers we are aware of. Li et al. (2007), for example, simply state that they used GIZA++ and</context>
</contexts>
<marker>Stymne, 2012</marker>
<rawString>Sara Stymne. 2012. Clustered word classes for preordering in statistical machine translation. In Proceedings of ROBUS-UNSUP 2012: Joint Workshop on Unsupervised and Semi-Supervised Learning in NLP, pages 28–34, Avignon, France.</rawString>
</citation>
<citation valid="true">
<authors>
<author>Oscar T¨ackstr¨om</author>
<author>Ryan McDonald</author>
<author>Jakob Uszkoreit</author>
</authors>
<title>Cross-lingual word clusters for direct transfer of linguistic structure.</title>
<date>2012</date>
<booktitle>In Proceedings of NAACL,</booktitle>
<pages>477--487</pages>
<location>Montr´eal, Quebec, Canada.</location>
<marker>T¨ackstr¨om, McDonald, Uszkoreit, 2012</marker>
<rawString>Oscar T¨ackstr¨om, Ryan McDonald, and Jakob Uszkoreit. 2012. Cross-lingual word clusters for direct transfer of linguistic structure. In Proceedings of NAACL, pages 477–487, Montr´eal, Quebec, Canada.</rawString>
</citation>
<citation valid="true">
<authors>
<author>David Talbot</author>
<author>Hideto Kazawa</author>
<author>Hiroshi Ichikawa</author>
<author>Jason Katz-Brown</author>
<author>Masakazu Seno</author>
<author>Franz Och</author>
</authors>
<title>A lightweight evaluation framework for machine translation reordering.</title>
<date>2011</date>
<booktitle>In Proceedings of WMT,</booktitle>
<pages>12--21</pages>
<location>Edinburgh, Scotland.</location>
<contexts>
<context position="30321" citStr="Talbot et al., 2011" startWordPosition="5165" endWordPosition="5168"> .83 −.58 .61 −.24 .75 .64 .72 .71 −.53 .04 Link degree Total SKTD Link crossings F Crossdiff 1-1 null multi P R SMT, Bleu .33 −.30 .21 −.05 −.14 −.09 .25 .07 −.63 POSReo, FRS −.41 .84 −.89 −.81 −.70 .90 .21 .86 −.41 POSReo, Bleu −.17 .66 −.80 −.71 −.60 .79 .42 .89 −.49 AlignReo, FRS −.32 .77 −.86 −.80 −.73 .94 .27 .92 −.38 AlignReo, Bleu −.57 .83 −.79 −.93 −.91 .86 −.07 .69 −.52 Table 5: Pearson correlations between different alignment characteristics and scores for the translation and reordering tasks. Significant correlations are marked with bold (&lt; 0.01). only on the reordering component (Talbot et al., 2011). It compares a system reordering to a reference reordering, by measuring how many chunks that have to be moved to get an identical word order, see Eq. 9, where C is the number of contiguously aligned chunks, and M the number of words. To find the reference ordering we apply the method of Holmqvist et al. (2009), described in Section 4.2, to the gold standard alignment. C − 1 FRS = 1 − (9) M − 1 4.1 Part-of-Speech-Based Reordering Our first reordering task is a part-of-speech-based preordering method described by Rottmann and Vogel (2007) and Niehues and Kolss (2009), which was successfully us</context>
<context position="41180" citStr="Talbot et al. (2011)" startWordPosition="7037" endWordPosition="7041">reorderings are likely good in such cases, even if we are not able to find all possible reorderings. In conclusion, based on this study, we recommend intersection symmetrization with model 4 and HMM for SMT reordering tasks. We have studied two relatively different reordering tasks with two training data sizes, but found that they to a large extent prefer the same types of alignments. Moreover, the results on these two reordering tasks correlates strongly with FRS, which is much cheaper to calculate than SMT metrics that may even require retraining of full SMT systems. This is consistent with Talbot et al. (2011) who suggested FRS for preordering tasks. We thus would encourage developers of alignment methods to not only give results for SMT, but also for FRS, as a proxy for reordering tasks. Furthermore, it is also useful to give results on TU metrics in addition to link level metrics to complement the evaluation. In this paper, we have looked at existing generative alignment and symmetrization models. In future work, we would also like to investigate other models, including the removal of low-confidence links, which has previously been proposed for prereordering (Li et al., 2007; Genzel, 2010). Given</context>
</contexts>
<marker>Talbot, Kazawa, Ichikawa, Katz-Brown, Seno, Och, 2011</marker>
<rawString>David Talbot, Hideto Kazawa, Hiroshi Ichikawa, Jason Katz-Brown, Masakazu Seno, and Franz Och. 2011. A lightweight evaluation framework for machine translation reordering. In Proceedings of WMT, pages 12–21, Edinburgh, Scotland.</rawString>
</citation>
<citation valid="true">
<authors>
<author>Ben Taskar</author>
<author>Lacoste-Julien Simon</author>
<author>Dan Klein</author>
</authors>
<title>A discriminative matching approach to word alignment.</title>
<date>2005</date>
<booktitle>In Proceedings of HLT and EMNLP,</booktitle>
<pages>73--80</pages>
<location>Vancouver, British Columbia, Canada.</location>
<contexts>
<context position="6186" citStr="Taskar et al., 2005" startWordPosition="993" endWordPosition="996"> generally desirable to also allow n–1 and n–m links, and to achieve this it is common practice to perform word alignment in both directions and to symmetrize them using some heuristic. A number of common symmetrization strategies are described in Table 1 (Koehn et al., 2005). There are also other alternatives, such as the refined method (Och and Ney, 2003), or link deletion from the union (Fossum et al., 2008). There is also a wide range of alternative approaches to word alignment. For example, various discriminative models have been proposed in the literature (Liu et al., 2005; Moore, 2005; Taskar et al., 2005). Their advantage is that they may integrate a wide range of features that may lead to improved alignment quality. However, most of Symmetrization Description int: intersection ATS ∩ AST uni: union ATS ∪ AST gd: grow-diag intersection plus adjacent links from the union if both linked words are unaligned gdf: grow-diag-final gd with links from the union added in a final step if either linked word is unaligned gdfa: gd with links from the union grow-diag-final-and added in a final step if both linked words are unaligned Table 1: Symmetrization strategies for word alignments ATS and AST in two di</context>
</contexts>
<marker>Taskar, Simon, Klein, 2005</marker>
<rawString>Ben Taskar, Lacoste-Julien Simon, and Dan Klein. 2005. A discriminative matching approach to word alignment. In Proceedings of HLT and EMNLP, pages 73–80, Vancouver, British Columbia, Canada.</rawString>
</citation>
<citation valid="true">
<authors>
<author>J¨org Tiedemann</author>
</authors>
<title>Optimisation of word alignment clues.</title>
<date>2005</date>
<journal>Natural Language Engineering,</journal>
<volume>11</volume>
<issue>03</issue>
<note>Special Issue on Parallel Texts.</note>
<contexts>
<context position="12927" citStr="Tiedemann (2005)" startWordPosition="2149" endWordPosition="2150">T. While they only use TUER, it 1TUER is similar to CPER (Ayan and Dorr, 2006), which measures the error rate of extracted phrases. Due to how phrase extraction handle null links, there are differences, however. 277 is also possible to define Precision, Recall and Fmeasure over translation units in the same way as for alignment links. We will use these three measures to get a broader picture of TUs in alignment evaluation. Also in this case, 1−TUER is equivalent to F-measure. 2|AU ∩ GU| TUER(A, G) = 1 − (5) |AU |+ |GU| The TU metrics are quite strict, since they require exact matching of TUs. Tiedemann (2005) suggested the MWU metrics for word alignment evaluation, which also consider partial matches of annotated multi-word units, which is a similar concept to TUs. In those metrics, precision and recall grow proportionally to the number of correctly aligned words within translation units. Proposed links are in this way scored according to their overlap with translation units in the gold standard. Precision and recall are defined in Eqs. 6–7, where overlap(XU, Y ) is the number of source and target words in XU that overlap with translation units in Y normalized by the size of XU (in terms of source</context>
</contexts>
<marker>Tiedemann, 2005</marker>
<rawString>J¨org Tiedemann. 2005. Optimisation of word alignment clues. Natural Language Engineering, 11(03):279–293. Special Issue on Parallel Texts.</rawString>
</citation>
<citation valid="true">
<authors>
<author>Stephan Vogel</author>
<author>Hermann Ney</author>
<author>Christoph Tillman</author>
</authors>
<title>HMM-based word alignment in statistical translation.</title>
<date>1996</date>
<booktitle>In Proceedings of Coling,</booktitle>
<pages>836--841</pages>
<location>Copenhagen, Denmark.</location>
<contexts>
<context position="5369" citStr="Vogel et al., 1996" startWordPosition="853" endWordPosition="856">f relating words in one language to words in the translation in another language, see an example in Figure 1. Word alignment models can be learnt automatically from large corpora of sentence aligned data. Brown et al. (1993) proposed the so-called IBM models, which are still widely used. These five models estimate alignments from corpora using the expectation-maximization algorithm, and each model adds some complexity. Model 4 is commonly used in SMT systems. There have been many later suggestions of alternatives to these models. These are often alternatives to model 2, such as the HMM model (Vogel et al., 1996) and fast align (Dyer et al., 2013). All these generative models produce directional alignments where one word in the source can be linked to many target words (1–m links) but not vice versa. It is generally desirable to also allow n–1 and n–m links, and to achieve this it is common practice to perform word alignment in both directions and to symmetrize them using some heuristic. A number of common symmetrization strategies are described in Table 1 (Koehn et al., 2005). There are also other alternatives, such as the refined method (Och and Ney, 2003), or link deletion from the union (Fossum et</context>
</contexts>
<marker>Vogel, Ney, Tillman, 1996</marker>
<rawString>Stephan Vogel, Hermann Ney, and Christoph Tillman. 1996. HMM-based word alignment in statistical translation. In Proceedings of Coling, pages 836– 841, Copenhagen, Denmark.</rawString>
</citation>
<citation valid="true">
<authors>
<author>Fei Xia</author>
<author>Michael McCord</author>
</authors>
<title>Improving a statistical MT system with automatically learned rewrite patterns.</title>
<date>2004</date>
<booktitle>In Proceedings of Coling,</booktitle>
<pages>508--514</pages>
<location>Geneva, Switzerland.</location>
<contexts>
<context position="2770" citStr="Xia and McCord (2004)" startWordPosition="419" endWordPosition="422"> Lambert et al. (2012). Word alignment is also used for many other tasks besides translation, including term bank creation (Merkel and Foo, 2007), cross-lingual annotation projection for part-of-speech tagging (Yarowsky et al., 2001), semantic roles (Pado and Lapata, 2005), pronoun anaphora (Postolache et al., 2006), and cross-lingual clustering (T¨ackstr¨om et al., 2012). Even within SMT itself, there are tasks such as reordering that often make crucial use of word alignments. For instance, source language reordering commonly relies on rules learnt automatically from word-aligned data (e.g., Xia and McCord (2004)). As far as we know, no one has studied the impact of alignment quality on these additional tasks, and it seems to be tacitly assumed that alignments that are good for translation are also good for other tasks. In this paper we set out to explore the impact of alignment quality on two pre-reordering tasks for SMT. In doing so, we employ a wider range of quality indicators than is customary, and for reference these indicators are used also to assess overall translation quality. To allow an in-depth exploration of the connections between several aspects of word alignment and reordering, we limi</context>
<context position="26458" citStr="Xia and McCord, 2004" startWordPosition="4491" endWordPosition="4494">dering in hierarchical (Chiang, 2007) or syntactic translation models (Yamada and Knight, 2002). Another type of approach is preordering, where the source side is reordered to mimic the target side before translation. There have also been approaches where reordering is modeled as part of the evaluation of MT systems (Birch and Osborne, 2011). We can distinguish two main types of approaches to preordering in SMT, either by using hand-written rules, which often operate on syntactic trees (Collins et al., 2005), or by reordering rules that are learnt automatically based on a word aligned corpus (Xia and McCord, 2004). The latter approach is of interest to us, since it is based on word alignments. There has been much work on automatic learning of reordering rules, which can be based on different levels of annotation, such as part-of-speech tags (Rottmann and Vogel, 2007; Niehues and Kolss, 2009; Genzel, 2010), chunks (Zhang et al., 2007) or parse trees (Xia and McCord, 2004). In general, all these approaches lead to improvements of translation quality. The reordering is always applied on the translation input. It can also be applied on the source side of the training corpora, which sometimes improves the r</context>
</contexts>
<marker>Xia, McCord, 2004</marker>
<rawString>Fei Xia and Michael McCord. 2004. Improving a statistical MT system with automatically learned rewrite patterns. In Proceedings of Coling, pages 508–514, Geneva, Switzerland.</rawString>
</citation>
<citation valid="true">
<authors>
<author>Kenji Yamada</author>
<author>Kevin Knight</author>
</authors>
<title>A decoder for syntax-based statistical MT.</title>
<date>2002</date>
<booktitle>In Proceedings ofACL,</booktitle>
<pages>303--310</pages>
<location>Philadelphia, Pennsylvania, USA.</location>
<contexts>
<context position="7645" citStr="Yamada and Knight, 2002" startWordPosition="1228" endWordPosition="1232">e outside the scope of our current work. Word alignments are used as an important knowledge source for training SMT systems. In word-based SMT, the parameters of the generative word alignment models are essentially the translation model of the system. In phrase-based SMT (PBSMT) (Koehn et al., 2003), which is among the state-of-the-art systems today, word alignments are used as a basis for extracting phrases and estimating phrase alignment probabilities. Similarly, word alignments are also used for estimating rule probabilities in various kinds of hierarchical and syntactic SMT (Chiang, 2007; Yamada and Knight, 2002; Galley et al., 2004). Intrinsic evaluation of word alignment is generally based on a comparison to a gold standard of human alignments. Based on the gold standard, metrics like precision, recall and F-measure can be calculated for each alignment link, see Eqs. 1– 2, where A are hypothesized alignment links and G are gold standard links. Another common metric is alignment error rate (AER) (Och and Ney, 2000), which is based on a distinction between sure, S, and possible, P, links in the gold standard. 1−AER is identical to balanced F-measure when the gold standard does not make a distinction </context>
<context position="25932" citStr="Yamada and Knight, 2002" startWordPosition="4402" endWordPosition="4405">ith Crossdiff, which means that it is good to have a similar number of crossings as the baseline. These results confirm results from previous studies that link level measures, especially recall and weighted F-measure show some correlation with SMT quality whereas precision does not. 4 Reordering Tasks for SMT Reordering is an important part of any SMT system. One way to address it is to add reordering models to standard PBSMT systems, for instance lexicalized reordering models (Koehn et al., 2005), or to directly model reordering in hierarchical (Chiang, 2007) or syntactic translation models (Yamada and Knight, 2002). Another type of approach is preordering, where the source side is reordered to mimic the target side before translation. There have also been approaches where reordering is modeled as part of the evaluation of MT systems (Birch and Osborne, 2011). We can distinguish two main types of approaches to preordering in SMT, either by using hand-written rules, which often operate on syntactic trees (Collins et al., 2005), or by reordering rules that are learnt automatically based on a word aligned corpus (Xia and McCord, 2004). The latter approach is of interest to us, since it is based on word alig</context>
</contexts>
<marker>Yamada, Knight, 2002</marker>
<rawString>Kenji Yamada and Kevin Knight. 2002. A decoder for syntax-based statistical MT. In Proceedings ofACL, pages 303–310, Philadelphia, Pennsylvania, USA.</rawString>
</citation>
<citation valid="true">
<authors>
<author>David Yarowsky</author>
<author>Grace Ngai</author>
<author>Richard Wicentowski</author>
</authors>
<title>Inducing multilingual text analysis tools via robust projection across aligned corpora.</title>
<date>2001</date>
<booktitle>In Proceedings of the First International Conference on Human Language Technology,</booktitle>
<pages>1--8</pages>
<location>San Diego, California, USA.</location>
<contexts>
<context position="2382" citStr="Yarowsky et al., 2001" startWordPosition="362" endWordPosition="365"> for large training data. Although many other ways of measuring alignment quality have been proposed, such as working on translation units (Ahrenberg et al., 2000; Ayan and Dorr, 2006; Søgaard and Kuhn, 2009) or using link degree and related measures (Ahrenberg, 2010), these methods have not been used to study the relation between alignment and translation quality, with the exception of Lambert et al. (2012). Word alignment is also used for many other tasks besides translation, including term bank creation (Merkel and Foo, 2007), cross-lingual annotation projection for part-of-speech tagging (Yarowsky et al., 2001), semantic roles (Pado and Lapata, 2005), pronoun anaphora (Postolache et al., 2006), and cross-lingual clustering (T¨ackstr¨om et al., 2012). Even within SMT itself, there are tasks such as reordering that often make crucial use of word alignments. For instance, source language reordering commonly relies on rules learnt automatically from word-aligned data (e.g., Xia and McCord (2004)). As far as we know, no one has studied the impact of alignment quality on these additional tasks, and it seems to be tacitly assumed that alignments that are good for translation are also good for other tasks. </context>
</contexts>
<marker>Yarowsky, Ngai, Wicentowski, 2001</marker>
<rawString>David Yarowsky, Grace Ngai, and Richard Wicentowski. 2001. Inducing multilingual text analysis tools via robust projection across aligned corpora. In Proceedings of the First International Conference on Human Language Technology, pages 1–8, San Diego, California, USA.</rawString>
</citation>
<citation valid="true">
<authors>
<author>Yuqi Zhang</author>
<author>Richard Zens</author>
<author>Hermann Ney</author>
</authors>
<title>Improved chunk-level reordering for statistical machine translation.</title>
<date>2007</date>
<booktitle>In Proceedings of the International Workshop on Spoken Language Translation,</booktitle>
<pages>21--28</pages>
<location>Trento, Italy.</location>
<contexts>
<context position="26784" citStr="Zhang et al., 2007" startWordPosition="4548" endWordPosition="4551"> Osborne, 2011). We can distinguish two main types of approaches to preordering in SMT, either by using hand-written rules, which often operate on syntactic trees (Collins et al., 2005), or by reordering rules that are learnt automatically based on a word aligned corpus (Xia and McCord, 2004). The latter approach is of interest to us, since it is based on word alignments. There has been much work on automatic learning of reordering rules, which can be based on different levels of annotation, such as part-of-speech tags (Rottmann and Vogel, 2007; Niehues and Kolss, 2009; Genzel, 2010), chunks (Zhang et al., 2007) or parse trees (Xia and McCord, 2004). In general, all these approaches lead to improvements of translation quality. The reordering is always applied on the translation input. It can also be applied on the source side of the training corpora, which sometimes improves the results (Rottmann and Vogel, 2007), but sometimes does not make a difference (Stymne, 2012). When preordering is performed on the translation input, it can be presented to the decoder as a 1-best reordering (Xia and McCord, 2004), as an n-best list (Li et al., 2007), or as a lattice of possible reorderings (Rottmann and Vogel</context>
</contexts>
<marker>Zhang, Zens, Ney, 2007</marker>
<rawString>Yuqi Zhang, Richard Zens, and Hermann Ney. 2007. Improved chunk-level reordering for statistical machine translation. In Proceedings of the International Workshop on Spoken Language Translation, pages 21–28, Trento, Italy.</rawString>
</citation>
</citationList>
</algorithm>
</algorithms>
<?xml version="1.0" encoding="UTF-8"?>
<algorithms version="110505">
<algorithm name="SectLabel" version="110505">
<variant no="0" confidence="0.000205">
<title confidence="0.968138">
Freebase QA: Information Extraction or Semantic Parsing?
</title>
<author confidence="0.974085">
Xuchen Yao 1 Jonathan Berant 3 Benjamin Van Durme 1,2
</author>
<affiliation confidence="0.9924404">
1Center for Language and Speech Processing
2Human Language Technology Center of Excellence
Johns Hopkins University
3Computer Science Department
Stanford University
</affiliation>
<sectionHeader confidence="0.954452" genericHeader="abstract">
Abstract
</sectionHeader>
<bodyText confidence="0.999799705882353">
We contrast two seemingly distinct ap-
proaches to the task of question answering
(QA) using Freebase: one based on infor-
mation extraction techniques, the other on
semantic parsing. Results over the same
test-set were collected from two state-of-
the-art, open-source systems, then ana-
lyzed in consultation with those systems’
creators. We conclude that the differ-
ences between these technologies, both
in task performance, and in how they
get there, is not significant. This sug-
gests that the semantic parsing commu-
nity should target answering more com-
positional open-domain questions that are
beyond the reach of more direct informa-
tion extraction methods.
</bodyText>
<sectionHeader confidence="0.992381" genericHeader="introduction">
1 Introduction
</sectionHeader>
<bodyText confidence="0.988124868852459">
Question Answering (QA) from structured data,
such as DBPedia (Auer et al., 2007), Freebase
(Bollacker et al., 2008) and Yago2 (Hoffart et
al., 2011), has drawn significant interest from
both knowledge base (KB) and semantic pars-
ing (SP) researchers. The majority of such work
treats the KB as a database, to which standard
database queries (SPARQL, MySQL, etc.) are is-
sued to retrieve answers. Language understand-
ing is modeled as the task of converting natu-
ral language questions into queries through inter-
mediate logical forms, with the popular two ap-
proaches including: CCG parsing (Zettlemoyer
and Collins, 2005; Zettlemoyer and Collins, 2007;
Zettlemoyer and Collins, 2009; Kwiatkowski et
al., 2010; Kwiatkowski et al., 2011; Krishna-
murthy and Mitchell, 2012; Kwiatkowski et al.,
2013; Cai and Yates, 2013a), and dependency-
based compositional semantics (Liang et al., 2011;
Berant et al., 2013; Berant and Liang, 2014).
We characterize semantic parsing as the task
of deriving a representation of meaning from lan-
guage, sufficient for a given task. Traditional
information extraction (IE) from text may be
coarsely characterized as representing a certain
level of semantic parsing, where the goal is to
derive enough meaning in order to populate a
database with factoids of a form matching a given
schema.1 Given the ease with which reasonably
accurate, deep syntactic structure can be automat-
ically derived over (English) text, it is not surpris-
ing that IE researchers would start including such
“features” in their models.
Our question is then: what is the difference be-
tween an IE system with access to syntax, as com-
pared to a semantic parser, when both are targeting
a factoid-extraction style task? While our conclu-
sions should hold generally for similar KBs, we
will focus on Freebase, such as explored by Kr-
ishnamurthy and Mitchell (2012), and then others
such as Cai and Yates (2013a) and Berant et al.
(2013). We compare two open-source, state-of-
the-art systems on the task of Freebase QA: the
semantic parsing system SEMPRE (Berant et al.,
2013), and the IE system jacana-freebase (Yao
and Van Durme, 2014).
We find that these two systems are on par with
each other, with no significant differences in terms
of accuracy between them. A major distinction be-
tween the work of Berant et al. (2013) and Yao
and Van Durme (2014) is the ability of the for-
mer to represent, and compose, aggregation oper-
ators (such as argmax, or count), as well as in-
tegrate disparate pieces of information. This rep-
resentational capability was important in previous,
closed-domain tasks such as GeoQuery. The move
to Freebase by the SP community was meant to
1So-called Open Information Extraction (OIE) is simply
a further blurring of the distinction between IE and SP, where
the schema is allowed to grow with the number of verbs, and
other predicative elements of the language.
</bodyText>
<page confidence="0.577511">
82
</page>
<note confidence="0.3883035">
Proceedings of the ACL 2014 Workshop on Semantic Parsing, pages 82–86,
Baltimore, Maryland USA, June 26 2014. c�2014 Association for Computational Linguistics
</note>
<bodyText confidence="0.999700571428571">
provide richer, open-domain challenges. While
the vocabulary increased, our analysis suggests
that compositionality and complexity decreased.
We therefore conclude that the semantic parsing
community should target more challenging open-
domain datasets, ones that “standard IE” methods
are less capable of attacking.
</bodyText>
<sectionHeader confidence="0.777371" genericHeader="related work">
2 IE and SP Systems
</sectionHeader>
<bodyText confidence="0.998097461538462">
jacana-freebase2 (Yao and Van Durme, 2014)
treats QA from a KB as a binary classification
problem. Freebase is a gigantic graph with mil-
lions of nodes (topics) and billions of edges (re-
lations). For each question, jacana-freebase
first selects a “view” of Freebase concerning only
involved topics and their close neighbors (this
“view” is called a topic graph). For instance,
for the question “who is the brother of justin
bieber?”, the topic graph of Justin Bieber, con-
taining all related nodes to the topic (think of the
“Justin Bieber” page displayed by the browser), is
selected and retrieved by the Freebase Topic API.
Usually such a topic graph contains hundreds to
thousands of nodes in close relation to the central
topic. Then each of the node is judged as answer
or not by a logistic regression learner.
Features for the logistic regression learner are
first extracted from both the question and the
topic graph. An analysis of the dependency
parse of the question characterizes the question
word, topic, verb, and named entities of the
main subject as the question features, such as
qword=who. Features on each node include the
types of relations and properties the node pos-
sesses, such as type=person. Finally features
from both the question and each node are com-
bined as the final features used by the learner, such
as qword=who|type=person. In this way the as-
sociation between the question and answer type
is enforced. Thus during decoding, for instance,
if there is a who question, the nodes with a per-
son property would be ranked higher as the an-
swer candidate.
SEMPRE3 is an open-source system for training
semantic parsers, that has been utilized to train a
semantic parser against Freebase by Berant et al.
(2013). SEMPRE maps NL utterances to logical
forms by performing bottom-up parsing. First, a
</bodyText>
<footnote confidence="0.689170666666667">
2https://code.google.com/p/jacana/
3http://www-nlp.stanford.edu/software/
sempre/
</footnote>
<bodyText confidence="0.999807142857143">
lexicon is used to map NL phrases to KB predi-
cates, and then predicates are combined to form a
full logical form by a context-free grammar. Since
logical forms can be derived in multiple ways from
the grammar, a log-linear model is used to rank
possible derivations. The parameters of the model
are trained from question-answer pairs.
</bodyText>
<sectionHeader confidence="0.995126" genericHeader="conclusions">
3 Analysis
</sectionHeader>
<subsectionHeader confidence="0.999651">
3.1 Evaluation Metrics
</subsectionHeader>
<bodyText confidence="0.999987625">
Both Berant et al. (2013) and Yao and
Van Durme (2014) tested their systems on
the WEBQUESTIONS dataset, which contains
3778 training questions and 2032 test questions
collected from the Google Suggest API. Each
question came with a standard answer from
Freebase annotated by Amazon Mechanical Turk.
Berant et al. (2013) reported a score of 31.4%
in terms of accuracy (with partial credit if inexact
match) on the test set and later in Berant and Liang
(2014) revised it to 35.7%. Berant et al. focused
on accuracy – how many questions were correctly
answered by the system. Since their system an-
swered almost all questions, accuracy is roughly
identical to F1. Yao and Van Durme (2014)’s sys-
tem on the other hand only answered 80% of all
test questions. Thus they report a score of 42%
in terms of F1 on this dataset. For the purpose of
comparing among all test questions, we lowered
the logistic regression prediction threshold (usu-
ally 0.5) on jacana-freebase for the other 20%
of questions where jacana-freebase had not pro-
posed an answer to, and selected the best-possible
prediction with the highest prediction score as the
answer. In this way jacana-freebase was able
to answer all questions with a lower accuracy of
35.4%. In the following we present analysis re-
sults based on the test questions where the two
systems had very similar performance (35.7% vs.
35.4%).4 The difference is not significant accord-
ing to the paired permutation test (Smucker et al.,
2007).
</bodyText>
<subsectionHeader confidence="0.999136">
3.2 Accuracy vs. Coverage
</subsectionHeader>
<bodyText confidence="0.987481125">
First, we were interested to see the proportions of
questions SEMPRE and jacana-freebase jointly
and separately answered correctly. The answer to
4In this setting accuracy equals averaged macro Fl: first
the Fl value on each question were computed, then averaged
among all questions, or put it in other words: “accuracy with
partial credit”. In this section our usage of the terms “accu-
racy” and “F1” can be exchanged.
</bodyText>
<page confidence="0.617701">
83
</page>
<table confidence="0.992903666666667">
SEMPRE
Accuracy
jacana (F1 = 1) jacana (F1 ≥ 0.5)
√ × √ ×
√ 153 (0.08) 383 (0.19) 429 (0.21) 321 (0.16)
× 136 (0.07) 1360 (0.67) 366 (0.18) 916 (0.45)
</table>
<tableCaption confidence="0.999022">
Table 1: The absolute and proportion of ques-
</tableCaption>
<bodyText confidence="0.978394">
tions SEMPRE and jacana-freebase answered
correctly (√) and incorrectly (×) jointly and sep-
arately, running a threshold F1 of 1 and 0.5.
many questions in the dataset is a set of answers,
for example what to see near sedona arizona?.
Since turkers did not exhaustively pick out all pos-
sible answers, evaluation is performed by comput-
ing the F1 between the set of answers given by
the system and the answers provided by turkers.
With a strict threshold of F1 = 1 and a permis-
sive threshold of F1 ≥ 0.5 to judge the correct-
ness, we list the pair-wise correctness matrix in
Table 1. Not surprisingly, both systems had most
questions wrong given that the averaged F1’s were
only around 35%. With the threshold F1 = 1,
SEMPRE answered more questions exactly cor-
rectly compared to jacana-freebase, while when
F1 ≥ 0.5, it was the other way around. This
shows that SEMPRE is more accurate in certain
questions. The reason behind this is that SEMPRE
always fires queries that return exactly one set of
answers from Freebase, while jacana-freebase
could potentially tag multiple nodes as the answer,
which may lower the accuracy.
We have shown that both systems can be more
accurate in certain questions, but when? Is there
a correlation between the system confidence and
accuracy? Thus we took the logistic decoding
score (between 0 and 1) from jacana-freebase
and the probability from the log-linear model used
by SEMPRE as confidence, and plotted an “accu-
racy vs. coverage” curve, which shows the accu-
racy of a QA engine with respect to its coverage
of all questions. The curve basically answers one
question: at a fixed accuracy, what is the propor-
tion of questions that can be answered? A better
system should be able to answer more questions
correctly with the same accuracy.
The curve was drawn in the following way. For
each question, we select the best answer candidate
with the highest confidence score. Then for the
whole test set, we have a list of (question, highest
ranked answer, confidence score) tuples. Running
</bodyText>
<figure confidence="0.980952">
60
50
40
30
200 10 20 30 40 50 60 70 80 90 100
Percent Answered
</figure>
<figureCaption confidence="0.9420375">
Figure 1: Precision with respect to proportion of
questions answered
</figureCaption>
<bodyText confidence="0.999776727272727">
a threshold from 1 to 0, we select those questions
with an answer confidence score above the thresh-
old and compute accuracy at this point. The X-
axis indicates the percentage of questions above
the threshold and the Y-axis the accuracy, shown
in Figure 1.
The two curves generally follow a similar trend,
but while jacana-freebase has higher accuracy
when coverage is low, SEMPRE obtains slightly
better accuracy when more questions are an-
swered.
</bodyText>
<subsectionHeader confidence="0.998916">
3.3 Accuracy by Question Length and Type
</subsectionHeader>
<bodyText confidence="0.9999866">
Do accuracies of the two systems differ with re-
spect to the complexity of questions? Since there
is no clear way to measure question complexity,
we use question length as a surrogate and report
accuracies by question length in Figure 2. Most of
the questions were 5 to 8 words long and there was
no substantial difference in terms of accuracies.
The major difference lies in questions of length 3,
12 and 13. However, the number of such ques-
tions was not high enough to show any statistical
significance.
Figure 3 further shows the accuracies with re-
spect to the question types (as reflected by the
WH-word). Again, there is no significant differ-
ence between the two systems.
</bodyText>
<subsectionHeader confidence="0.985716">
3.4 Learned Features
</subsectionHeader>
<bodyText confidence="0.999673833333333">
What did the systems learn during training? We
compare them by presenting the top features by
weight, as listed in Table 2. Clearly, the type of
knowledge learned by the systems in these fea-
tures is similar: both systems learn to associate
certain phrases with predicates from the KB.
</bodyText>
<figure confidence="0.697953692307692">
70 Accuracy vs. Coverage
jacana-freebase
SEMPRE
84
The X-axis specifies the question lengJa ca na-­‐fr
eeb ase mSEMP REmFig ur e2: Accura cy (Y-axis) byq u
i onl
u es
c i fi
h .
r of qu
in pa
0.4u0. 45 Awhata(9 29 )uwheren (357 )awh oh(
</figure>
<figureCaption confidence="0.592496">
fFigure 3: Accu rac ybyqu estiontype( andthen umber
</figureCaption>
<bodyText confidence="0.899652352941177">
ne (100) hhowve(8 )hJa cana-­‐f reeb aseaSEM PRE
ofq uestions) .We not e, howe ver, thatSEM PREal
soob tainsi n- forma tio nfro mthefull yconstr uct
ed log icalfor m.Forinst ance ,SEMPR Ele arns that
logi ca l formst ha tret urnanem pty se twhenexe cute
dagainst th eKBa reus ual ly incorrect
(t hew eig htforth isfea-t ur eis-8 .88). Int
hisr es pecttheS Pappr oac h“un-de rstan ds”
more tha n the IEa pproac h.We did not furt her
compare o notherdata setssuch a sGe oQuery(T anga
ndMoone y, 2001)and FREE917( Caiand Yat es,2013b).The
firston ei nvolves g eographi cin ferenc ean dmu
ltipleco n-traints in queries ,d irectlyfi ttin
gthe comp ositiona lnat ure o fsemantic pars ing
.The second on ewasmanu ally generate d byl ooki
ngat Freebasetop -ics.Bothdat aset s were le ssr
eal isti cthanthe WE BQUESTIONS da taset.B
</bodyText>
<figure confidence="0.911927">
othda
tasets were also lesschalle
ngi ng(accur ac y/F 1w erebetween 8 0%and90%
)compar edt oWEBQUESTIO NS(around40 %). 4Disc us
sionand Con clusionOur analysisoftwoQAappr
oaches, semant
icparsingandinformationextrac tion
,hasshownnosignificantdiffe renc
ebetweenthem.Notethefeature w eig
htqfocus=re ligionatype=R e l i g
ion8.60qfocus=moneyatype=Currency 5.56
qve rb=dieatype=Cau
seOfDeath5.35
qwo rd=whenatype=date time5
.11 qverb=borderare l=lo
cation.adjoins4 .56(
a)jacana-freebase f e a t
ur eweightdiefrom=CauseOfD eath
10. 23dieo
f=Cau se OfDeath 7.55
accept=Currency 7.30bea r=P
laceOfBirth7.11 ins witzerl
</figure>
<bodyText confidence="0.884918678571428">
topfe at ures a nd the ir weigh tsforja can a-fr
eebaseand SE MPRE .simi lari tyb etweenfeatu resu
and=Switze rland6. 86(b)SEM PRET ab le2: Learned
sed in bothsyst emsshown inTa ble2: th e system sle
arnedthes ame”kno wledge”fro m data, wit hthedi
stinctionth atthe IEa pp roach acq uiredthi sthr
oughadi rectas-soc ia tionbetweend epend encypa
rses a ndansw erpropertie s,whilethe SPapproa
cha cquire dthisth ro ugh opti miz ingo ninterm
ediatel ogicform s.Witha direct i nf ormation extr
actionte chnol-ogy e asi ly gett ingon par wit
hth e mo resop his -tica teds e manticpa rsingm
ethod,it su gge sts thatSP- based approache sf orQA
with Freebase ha snot yet show nits power from a“de
eper”under- standing ofthe q ue stions, a mon gque
st ionsofvar-i ouslen gths. We suggesttha tm orecom
pos itionalo pen- dom ain dat ase tssho ul dbecre at
ed,andth
a t
SPresearcherss
tare b eyo nd the reach of directI Emet hods. 5Ack
ho uldfocus onutterancesin exi st-ingdat asetstha
nowledge me ntWethank the Al le nInstitu te forArti
fic ial Int elli- ig this work. This gence for assistance in fund
on research sponsored matial is partially base
nt by the IIS-1249516 NSF under and g DARPA
under agreements number FA8750-13-2-0017 and
FA8750-13-2-0040 (the DEFT program).
</bodyText>
<figure confidence="0.987309457142857">
0.45
0.35
0.25
0.15
0.05
0.5
0.4
0.3
0.2
0.1
0
is s p
sthe
Th
e ng
tio n
X-a
e
ngth
nword
261)ew hi che(35)nwh e
andth
total
umb
stion
enth
sis.
0
0.15n0.2r0.25s0
0.35
3 ��
0.0
0.1
ra
85
</figure>
<sectionHeader confidence="0.907798" genericHeader="references">
References
</sectionHeader>
<reference confidence="0.999535">
S¨oren Auer, Christian Bizer, Georgi Kobilarov, Jens
Lehmann, Richard Cyganiak, and Zachary Ives.
2007. DBPedia: A nucleus for a web of open data.
In The semantic web, pages 722–735. Springer.
Jonathan Berant and Percy Liang. 2014. Semantic
parsing via paraphrasing. In Proceedings of ACL.
Jonathan Berant, Andrew Chou, Roy Frostig, and Percy
Liang. 2013. Semantic Parsing on Freebase from
Question-Answer Pairs. In Proceedings of EMNLP.
Kurt Bollacker, Colin Evans, Praveen Paritosh, Tim
Sturge, and Jamie Taylor. 2008. Freebase: a col-
laboratively created graph database for structuring
human knowledge. In Proceedings of the 2008 ACM
SIGMOD international conference on Management
of data, pages 1247–1250. ACM.
Qingqing Cai and Alexander Yates. 2013a. Large-
scale semantic parsing via schema matching and lex-
icon extension. In Proceedings of ACL.
Qingqing Cai and Alexander Yates. 2013b. Large-
scale semantic parsing via schema matching and lex-
icon extension. In Proceedings of ACL.
Johannes Hoffart, Fabian M Suchanek, Klaus
Berberich, Edwin Lewis-Kelham, Gerard De Melo,
and Gerhard Weikum. 2011. Yago2: exploring and
querying world knowledge in time, space, context,
and many languages. In Proceedings of the 20th
international conference companion on World Wide
Web, pages 229–232. ACM.
Jayant Krishnamurthy and Tom Mitchell. 2012.
Weakly supervised training of semantic parsers. In
Proceedings of EMNLP.
Tom Kwiatkowski, Luke Zettlemoyer, Sharon Goldwa-
ter, and Mark Steedman. 2010. Inducing probabilis-
tic CCG grammars from logical form with higher-
order unification. In Proceedings of EMNLP, pages
1223–1233.
Tom Kwiatkowski, Luke Zettlemoyer, Sharon Goldwa-
ter, and Mark Steedman. 2011. Lexical generaliza-
tion in CCG grammar induction for semantic pars-
ing. In Proceedings of EMNLP.
Tom Kwiatkowski, Eunsol Choi, Yoav Artzi, and Luke
Zettlemoyer. 2013. Scaling Semantic Parsers with
On-the-fly Ontology Matching. In Proceedings of
EMNLP.
Percy Liang, Michael I. Jordan, and Dan Klein.
2011. Learning Dependency-Based Compositional
Semantics. In Proceedings of ACL.
M.D. Smucker, J. Allan, and B. Carterette. 2007. A
comparison of statistical significance tests for in-
formation retrieval evaluation. In Proceedings of
the sixteenth ACM conference on Conference on in-
formation and knowledge management, pages 623–
632. ACM.
Lappoon R Tang and Raymond J Mooney. 2001. Us-
ing multiple clause constructors in inductive logic
programming for semantic parsing. In Machine
Learning: ECML 2001. Springer.
Xuchen Yao and Benjamin Van Durme. 2014. Infor-
mation extraction over structured data: Question an-
swering with freebase. In Proceedings of ACL.
Luke S Zettlemoyer and Michael Collins. 2005.
Learning to map sentences to logical form: Struc-
tured classification with probabilistic categorial
grammars. Uncertainty in Artificial Intelligence
(UAI).
Luke S. Zettlemoyer and Michael Collins. 2007. On-
line learning of relaxed CCG grammars for parsing
to logical form. In Proceedings of EMNLP-CoNLL.
Luke S Zettlemoyer and Michael Collins. 2009.
Learning context-dependent mappings from sen-
tences to logical form. In Proceedings of ACL-
CoNLL.
</reference>
<page confidence="0.894947">
86
</page>
</variant>
</algorithm>
<algorithm name="ParsHed" version="110505">
<variant no="0" confidence="0.747865">
<title confidence="0.997361">Freebase QA: Information Extraction or Semantic Parsing?</title>
<author confidence="0.986674">Yao Jonathan Berant Benjamin Van_Durme</author>
<affiliation confidence="0.9466322">for Language and Speech Language Technology Center of Johns Hopkins University Science Stanford University</affiliation>
<abstract confidence="0.998668388888889">We contrast two seemingly distinct approaches to the task of question answering (QA) using Freebase: one based on information extraction techniques, the other on semantic parsing. Results over the same test-set were collected from two state-ofthe-art, open-source systems, then analyzed in consultation with those systems’ creators. We conclude that the differences between these technologies, both in task performance, and in how they get there, is not significant. This suggests that the semantic parsing community should target answering more compositional open-domain questions that are beyond the reach of more direct information extraction methods.</abstract>
</variant>
</algorithm>
<algorithm name="ParsCit" version="110505">
<citationList>
<citation valid="true">
<authors>
<author>S¨oren Auer</author>
<author>Christian Bizer</author>
<author>Georgi Kobilarov</author>
<author>Jens Lehmann</author>
<author>Richard Cyganiak</author>
<author>Zachary Ives</author>
</authors>
<title>DBPedia: A nucleus for a web of open data.</title>
<date>2007</date>
<booktitle>In The semantic web,</booktitle>
<pages>722--735</pages>
<publisher>Springer.</publisher>
<contexts>
<context position="1036" citStr="Auer et al., 2007" startWordPosition="150" endWordPosition="153">ion techniques, the other on semantic parsing. Results over the same test-set were collected from two state-ofthe-art, open-source systems, then analyzed in consultation with those systems’ creators. We conclude that the differences between these technologies, both in task performance, and in how they get there, is not significant. This suggests that the semantic parsing community should target answering more compositional open-domain questions that are beyond the reach of more direct information extraction methods. 1 Introduction Question Answering (QA) from structured data, such as DBPedia (Auer et al., 2007), Freebase (Bollacker et al., 2008) and Yago2 (Hoffart et al., 2011), has drawn significant interest from both knowledge base (KB) and semantic parsing (SP) researchers. The majority of such work treats the KB as a database, to which standard database queries (SPARQL, MySQL, etc.) are issued to retrieve answers. Language understanding is modeled as the task of converting natural language questions into queries through intermediate logical forms, with the popular two approaches including: CCG parsing (Zettlemoyer and Collins, 2005; Zettlemoyer and Collins, 2007; Zettlemoyer and Collins, 2009; K</context>
</contexts>
<marker>Auer, Bizer, Kobilarov, Lehmann, Cyganiak, Ives, 2007</marker>
<rawString>S¨oren Auer, Christian Bizer, Georgi Kobilarov, Jens Lehmann, Richard Cyganiak, and Zachary Ives. 2007. DBPedia: A nucleus for a web of open data. In The semantic web, pages 722–735. Springer.</rawString>
</citation>
<citation valid="true">
<authors>
<author>Jonathan Berant</author>
<author>Percy Liang</author>
</authors>
<title>Semantic parsing via paraphrasing.</title>
<date>2014</date>
<booktitle>In Proceedings of ACL.</booktitle>
<contexts>
<context position="1879" citStr="Berant and Liang, 2014" startWordPosition="281" endWordPosition="284">ase, to which standard database queries (SPARQL, MySQL, etc.) are issued to retrieve answers. Language understanding is modeled as the task of converting natural language questions into queries through intermediate logical forms, with the popular two approaches including: CCG parsing (Zettlemoyer and Collins, 2005; Zettlemoyer and Collins, 2007; Zettlemoyer and Collins, 2009; Kwiatkowski et al., 2010; Kwiatkowski et al., 2011; Krishnamurthy and Mitchell, 2012; Kwiatkowski et al., 2013; Cai and Yates, 2013a), and dependencybased compositional semantics (Liang et al., 2011; Berant et al., 2013; Berant and Liang, 2014). We characterize semantic parsing as the task of deriving a representation of meaning from language, sufficient for a given task. Traditional information extraction (IE) from text may be coarsely characterized as representing a certain level of semantic parsing, where the goal is to derive enough meaning in order to populate a database with factoids of a form matching a given schema.1 Given the ease with which reasonably accurate, deep syntactic structure can be automatically derived over (English) text, it is not surprising that IE researchers would start including such “features” in their m</context>
<context position="7056" citStr="Berant and Liang (2014)" startWordPosition="1121" endWordPosition="1124">mar, a log-linear model is used to rank possible derivations. The parameters of the model are trained from question-answer pairs. 3 Analysis 3.1 Evaluation Metrics Both Berant et al. (2013) and Yao and Van Durme (2014) tested their systems on the WEBQUESTIONS dataset, which contains 3778 training questions and 2032 test questions collected from the Google Suggest API. Each question came with a standard answer from Freebase annotated by Amazon Mechanical Turk. Berant et al. (2013) reported a score of 31.4% in terms of accuracy (with partial credit if inexact match) on the test set and later in Berant and Liang (2014) revised it to 35.7%. Berant et al. focused on accuracy – how many questions were correctly answered by the system. Since their system answered almost all questions, accuracy is roughly identical to F1. Yao and Van Durme (2014)’s system on the other hand only answered 80% of all test questions. Thus they report a score of 42% in terms of F1 on this dataset. For the purpose of comparing among all test questions, we lowered the logistic regression prediction threshold (usually 0.5) on jacana-freebase for the other 20% of questions where jacana-freebase had not proposed an answer to, and selected</context>
</contexts>
<marker>Berant, Liang, 2014</marker>
<rawString>Jonathan Berant and Percy Liang. 2014. Semantic parsing via paraphrasing. In Proceedings of ACL.</rawString>
</citation>
<citation valid="true">
<authors>
<author>Jonathan Berant</author>
<author>Andrew Chou</author>
<author>Roy Frostig</author>
<author>Percy Liang</author>
</authors>
<title>Semantic Parsing on Freebase from Question-Answer Pairs.</title>
<date>2013</date>
<booktitle>In Proceedings of EMNLP.</booktitle>
<contexts>
<context position="1854" citStr="Berant et al., 2013" startWordPosition="277" endWordPosition="280">ats the KB as a database, to which standard database queries (SPARQL, MySQL, etc.) are issued to retrieve answers. Language understanding is modeled as the task of converting natural language questions into queries through intermediate logical forms, with the popular two approaches including: CCG parsing (Zettlemoyer and Collins, 2005; Zettlemoyer and Collins, 2007; Zettlemoyer and Collins, 2009; Kwiatkowski et al., 2010; Kwiatkowski et al., 2011; Krishnamurthy and Mitchell, 2012; Kwiatkowski et al., 2013; Cai and Yates, 2013a), and dependencybased compositional semantics (Liang et al., 2011; Berant et al., 2013; Berant and Liang, 2014). We characterize semantic parsing as the task of deriving a representation of meaning from language, sufficient for a given task. Traditional information extraction (IE) from text may be coarsely characterized as representing a certain level of semantic parsing, where the goal is to derive enough meaning in order to populate a database with factoids of a form matching a given schema.1 Given the ease with which reasonably accurate, deep syntactic structure can be automatically derived over (English) text, it is not surprising that IE researchers would start including s</context>
<context position="3266" citStr="Berant et al. (2013)" startWordPosition="513" endWordPosition="516">raction style task? While our conclusions should hold generally for similar KBs, we will focus on Freebase, such as explored by Krishnamurthy and Mitchell (2012), and then others such as Cai and Yates (2013a) and Berant et al. (2013). We compare two open-source, state-ofthe-art systems on the task of Freebase QA: the semantic parsing system SEMPRE (Berant et al., 2013), and the IE system jacana-freebase (Yao and Van Durme, 2014). We find that these two systems are on par with each other, with no significant differences in terms of accuracy between them. A major distinction between the work of Berant et al. (2013) and Yao and Van Durme (2014) is the ability of the former to represent, and compose, aggregation operators (such as argmax, or count), as well as integrate disparate pieces of information. This representational capability was important in previous, closed-domain tasks such as GeoQuery. The move to Freebase by the SP community was meant to 1So-called Open Information Extraction (OIE) is simply a further blurring of the distinction between IE and SP, where the schema is allowed to grow with the number of verbs, and other predicative elements of the language. 82 Proceedings of the ACL 2014 Works</context>
<context position="6059" citStr="Berant et al. (2013)" startWordPosition="966" endWordPosition="969">s on each node include the types of relations and properties the node possesses, such as type=person. Finally features from both the question and each node are combined as the final features used by the learner, such as qword=who|type=person. In this way the association between the question and answer type is enforced. Thus during decoding, for instance, if there is a who question, the nodes with a person property would be ranked higher as the answer candidate. SEMPRE3 is an open-source system for training semantic parsers, that has been utilized to train a semantic parser against Freebase by Berant et al. (2013). SEMPRE maps NL utterances to logical forms by performing bottom-up parsing. First, a 2https://code.google.com/p/jacana/ 3http://www-nlp.stanford.edu/software/ sempre/ lexicon is used to map NL phrases to KB predicates, and then predicates are combined to form a full logical form by a context-free grammar. Since logical forms can be derived in multiple ways from the grammar, a log-linear model is used to rank possible derivations. The parameters of the model are trained from question-answer pairs. 3 Analysis 3.1 Evaluation Metrics Both Berant et al. (2013) and Yao and Van Durme (2014) tested </context>
</contexts>
<marker>Berant, Chou, Frostig, Liang, 2013</marker>
<rawString>Jonathan Berant, Andrew Chou, Roy Frostig, and Percy Liang. 2013. Semantic Parsing on Freebase from Question-Answer Pairs. In Proceedings of EMNLP.</rawString>
</citation>
<citation valid="true">
<authors>
<author>Kurt Bollacker</author>
<author>Colin Evans</author>
<author>Praveen Paritosh</author>
<author>Tim Sturge</author>
<author>Jamie Taylor</author>
</authors>
<title>Freebase: a collaboratively created graph database for structuring human knowledge.</title>
<date>2008</date>
<booktitle>In Proceedings of the 2008 ACM SIGMOD international conference on Management of data,</booktitle>
<pages>1247--1250</pages>
<publisher>ACM.</publisher>
<contexts>
<context position="1071" citStr="Bollacker et al., 2008" startWordPosition="155" endWordPosition="158">emantic parsing. Results over the same test-set were collected from two state-ofthe-art, open-source systems, then analyzed in consultation with those systems’ creators. We conclude that the differences between these technologies, both in task performance, and in how they get there, is not significant. This suggests that the semantic parsing community should target answering more compositional open-domain questions that are beyond the reach of more direct information extraction methods. 1 Introduction Question Answering (QA) from structured data, such as DBPedia (Auer et al., 2007), Freebase (Bollacker et al., 2008) and Yago2 (Hoffart et al., 2011), has drawn significant interest from both knowledge base (KB) and semantic parsing (SP) researchers. The majority of such work treats the KB as a database, to which standard database queries (SPARQL, MySQL, etc.) are issued to retrieve answers. Language understanding is modeled as the task of converting natural language questions into queries through intermediate logical forms, with the popular two approaches including: CCG parsing (Zettlemoyer and Collins, 2005; Zettlemoyer and Collins, 2007; Zettlemoyer and Collins, 2009; Kwiatkowski et al., 2010; Kwiatkowsk</context>
</contexts>
<marker>Bollacker, Evans, Paritosh, Sturge, Taylor, 2008</marker>
<rawString>Kurt Bollacker, Colin Evans, Praveen Paritosh, Tim Sturge, and Jamie Taylor. 2008. Freebase: a collaboratively created graph database for structuring human knowledge. In Proceedings of the 2008 ACM SIGMOD international conference on Management of data, pages 1247–1250. ACM.</rawString>
</citation>
<citation valid="true">
<authors>
<author>Qingqing Cai</author>
<author>Alexander Yates</author>
</authors>
<title>Largescale semantic parsing via schema matching and lexicon extension.</title>
<date>2013</date>
<booktitle>In Proceedings of ACL.</booktitle>
<contexts>
<context position="1766" citStr="Cai and Yates, 2013" startWordPosition="264" endWordPosition="267">knowledge base (KB) and semantic parsing (SP) researchers. The majority of such work treats the KB as a database, to which standard database queries (SPARQL, MySQL, etc.) are issued to retrieve answers. Language understanding is modeled as the task of converting natural language questions into queries through intermediate logical forms, with the popular two approaches including: CCG parsing (Zettlemoyer and Collins, 2005; Zettlemoyer and Collins, 2007; Zettlemoyer and Collins, 2009; Kwiatkowski et al., 2010; Kwiatkowski et al., 2011; Krishnamurthy and Mitchell, 2012; Kwiatkowski et al., 2013; Cai and Yates, 2013a), and dependencybased compositional semantics (Liang et al., 2011; Berant et al., 2013; Berant and Liang, 2014). We characterize semantic parsing as the task of deriving a representation of meaning from language, sufficient for a given task. Traditional information extraction (IE) from text may be coarsely characterized as representing a certain level of semantic parsing, where the goal is to derive enough meaning in order to populate a database with factoids of a form matching a given schema.1 Given the ease with which reasonably accurate, deep syntactic structure can be automatically deriv</context>
</contexts>
<marker>Cai, Yates, 2013</marker>
<rawString>Qingqing Cai and Alexander Yates. 2013a. Largescale semantic parsing via schema matching and lexicon extension. In Proceedings of ACL.</rawString>
</citation>
<citation valid="true">
<authors>
<author>Qingqing Cai</author>
<author>Alexander Yates</author>
</authors>
<title>Largescale semantic parsing via schema matching and lexicon extension.</title>
<date>2013</date>
<booktitle>In Proceedings of ACL.</booktitle>
<contexts>
<context position="1766" citStr="Cai and Yates, 2013" startWordPosition="264" endWordPosition="267">knowledge base (KB) and semantic parsing (SP) researchers. The majority of such work treats the KB as a database, to which standard database queries (SPARQL, MySQL, etc.) are issued to retrieve answers. Language understanding is modeled as the task of converting natural language questions into queries through intermediate logical forms, with the popular two approaches including: CCG parsing (Zettlemoyer and Collins, 2005; Zettlemoyer and Collins, 2007; Zettlemoyer and Collins, 2009; Kwiatkowski et al., 2010; Kwiatkowski et al., 2011; Krishnamurthy and Mitchell, 2012; Kwiatkowski et al., 2013; Cai and Yates, 2013a), and dependencybased compositional semantics (Liang et al., 2011; Berant et al., 2013; Berant and Liang, 2014). We characterize semantic parsing as the task of deriving a representation of meaning from language, sufficient for a given task. Traditional information extraction (IE) from text may be coarsely characterized as representing a certain level of semantic parsing, where the goal is to derive enough meaning in order to populate a database with factoids of a form matching a given schema.1 Given the ease with which reasonably accurate, deep syntactic structure can be automatically deriv</context>
</contexts>
<marker>Cai, Yates, 2013</marker>
<rawString>Qingqing Cai and Alexander Yates. 2013b. Largescale semantic parsing via schema matching and lexicon extension. In Proceedings of ACL.</rawString>
</citation>
<citation valid="true">
<authors>
<author>Johannes Hoffart</author>
<author>Fabian M Suchanek</author>
<author>Klaus Berberich</author>
<author>Edwin Lewis-Kelham</author>
<author>Gerard De Melo</author>
<author>Gerhard Weikum</author>
</authors>
<title>Yago2: exploring and querying world knowledge in time, space, context, and many languages.</title>
<date>2011</date>
<booktitle>In Proceedings of the 20th international conference companion on World Wide Web,</booktitle>
<pages>229--232</pages>
<publisher>ACM.</publisher>
<marker>Hoffart, Suchanek, Berberich, Lewis-Kelham, De Melo, Weikum, 2011</marker>
<rawString>Johannes Hoffart, Fabian M Suchanek, Klaus Berberich, Edwin Lewis-Kelham, Gerard De Melo, and Gerhard Weikum. 2011. Yago2: exploring and querying world knowledge in time, space, context, and many languages. In Proceedings of the 20th international conference companion on World Wide Web, pages 229–232. ACM.</rawString>
</citation>
<citation valid="true">
<authors>
<author>Jayant Krishnamurthy</author>
<author>Tom Mitchell</author>
</authors>
<title>Weakly supervised training of semantic parsers.</title>
<date>2012</date>
<booktitle>In Proceedings of EMNLP.</booktitle>
<contexts>
<context position="1719" citStr="Krishnamurthy and Mitchell, 2012" startWordPosition="255" endWordPosition="259">art et al., 2011), has drawn significant interest from both knowledge base (KB) and semantic parsing (SP) researchers. The majority of such work treats the KB as a database, to which standard database queries (SPARQL, MySQL, etc.) are issued to retrieve answers. Language understanding is modeled as the task of converting natural language questions into queries through intermediate logical forms, with the popular two approaches including: CCG parsing (Zettlemoyer and Collins, 2005; Zettlemoyer and Collins, 2007; Zettlemoyer and Collins, 2009; Kwiatkowski et al., 2010; Kwiatkowski et al., 2011; Krishnamurthy and Mitchell, 2012; Kwiatkowski et al., 2013; Cai and Yates, 2013a), and dependencybased compositional semantics (Liang et al., 2011; Berant et al., 2013; Berant and Liang, 2014). We characterize semantic parsing as the task of deriving a representation of meaning from language, sufficient for a given task. Traditional information extraction (IE) from text may be coarsely characterized as representing a certain level of semantic parsing, where the goal is to derive enough meaning in order to populate a database with factoids of a form matching a given schema.1 Given the ease with which reasonably accurate, deep</context>
</contexts>
<marker>Krishnamurthy, Mitchell, 2012</marker>
<rawString>Jayant Krishnamurthy and Tom Mitchell. 2012. Weakly supervised training of semantic parsers. In Proceedings of EMNLP.</rawString>
</citation>
<citation valid="true">
<authors>
<author>Tom Kwiatkowski</author>
<author>Luke Zettlemoyer</author>
<author>Sharon Goldwater</author>
<author>Mark Steedman</author>
</authors>
<title>Inducing probabilistic CCG grammars from logical form with higherorder unification.</title>
<date>2010</date>
<booktitle>In Proceedings of EMNLP,</booktitle>
<pages>1223--1233</pages>
<contexts>
<context position="1659" citStr="Kwiatkowski et al., 2010" startWordPosition="247" endWordPosition="250">), Freebase (Bollacker et al., 2008) and Yago2 (Hoffart et al., 2011), has drawn significant interest from both knowledge base (KB) and semantic parsing (SP) researchers. The majority of such work treats the KB as a database, to which standard database queries (SPARQL, MySQL, etc.) are issued to retrieve answers. Language understanding is modeled as the task of converting natural language questions into queries through intermediate logical forms, with the popular two approaches including: CCG parsing (Zettlemoyer and Collins, 2005; Zettlemoyer and Collins, 2007; Zettlemoyer and Collins, 2009; Kwiatkowski et al., 2010; Kwiatkowski et al., 2011; Krishnamurthy and Mitchell, 2012; Kwiatkowski et al., 2013; Cai and Yates, 2013a), and dependencybased compositional semantics (Liang et al., 2011; Berant et al., 2013; Berant and Liang, 2014). We characterize semantic parsing as the task of deriving a representation of meaning from language, sufficient for a given task. Traditional information extraction (IE) from text may be coarsely characterized as representing a certain level of semantic parsing, where the goal is to derive enough meaning in order to populate a database with factoids of a form matching a given </context>
</contexts>
<marker>Kwiatkowski, Zettlemoyer, Goldwater, Steedman, 2010</marker>
<rawString>Tom Kwiatkowski, Luke Zettlemoyer, Sharon Goldwater, and Mark Steedman. 2010. Inducing probabilistic CCG grammars from logical form with higherorder unification. In Proceedings of EMNLP, pages 1223–1233.</rawString>
</citation>
<citation valid="true">
<authors>
<author>Tom Kwiatkowski</author>
<author>Luke Zettlemoyer</author>
<author>Sharon Goldwater</author>
<author>Mark Steedman</author>
</authors>
<title>Lexical generalization in CCG grammar induction for semantic parsing.</title>
<date>2011</date>
<booktitle>In Proceedings of EMNLP.</booktitle>
<contexts>
<context position="1685" citStr="Kwiatkowski et al., 2011" startWordPosition="251" endWordPosition="254">al., 2008) and Yago2 (Hoffart et al., 2011), has drawn significant interest from both knowledge base (KB) and semantic parsing (SP) researchers. The majority of such work treats the KB as a database, to which standard database queries (SPARQL, MySQL, etc.) are issued to retrieve answers. Language understanding is modeled as the task of converting natural language questions into queries through intermediate logical forms, with the popular two approaches including: CCG parsing (Zettlemoyer and Collins, 2005; Zettlemoyer and Collins, 2007; Zettlemoyer and Collins, 2009; Kwiatkowski et al., 2010; Kwiatkowski et al., 2011; Krishnamurthy and Mitchell, 2012; Kwiatkowski et al., 2013; Cai and Yates, 2013a), and dependencybased compositional semantics (Liang et al., 2011; Berant et al., 2013; Berant and Liang, 2014). We characterize semantic parsing as the task of deriving a representation of meaning from language, sufficient for a given task. Traditional information extraction (IE) from text may be coarsely characterized as representing a certain level of semantic parsing, where the goal is to derive enough meaning in order to populate a database with factoids of a form matching a given schema.1 Given the ease wi</context>
</contexts>
<marker>Kwiatkowski, Zettlemoyer, Goldwater, Steedman, 2011</marker>
<rawString>Tom Kwiatkowski, Luke Zettlemoyer, Sharon Goldwater, and Mark Steedman. 2011. Lexical generalization in CCG grammar induction for semantic parsing. In Proceedings of EMNLP.</rawString>
</citation>
<citation valid="true">
<authors>
<author>Tom Kwiatkowski</author>
<author>Eunsol Choi</author>
<author>Yoav Artzi</author>
<author>Luke Zettlemoyer</author>
</authors>
<title>Scaling Semantic Parsers with On-the-fly Ontology Matching.</title>
<date>2013</date>
<booktitle>In Proceedings of EMNLP.</booktitle>
<contexts>
<context position="1745" citStr="Kwiatkowski et al., 2013" startWordPosition="260" endWordPosition="263">ficant interest from both knowledge base (KB) and semantic parsing (SP) researchers. The majority of such work treats the KB as a database, to which standard database queries (SPARQL, MySQL, etc.) are issued to retrieve answers. Language understanding is modeled as the task of converting natural language questions into queries through intermediate logical forms, with the popular two approaches including: CCG parsing (Zettlemoyer and Collins, 2005; Zettlemoyer and Collins, 2007; Zettlemoyer and Collins, 2009; Kwiatkowski et al., 2010; Kwiatkowski et al., 2011; Krishnamurthy and Mitchell, 2012; Kwiatkowski et al., 2013; Cai and Yates, 2013a), and dependencybased compositional semantics (Liang et al., 2011; Berant et al., 2013; Berant and Liang, 2014). We characterize semantic parsing as the task of deriving a representation of meaning from language, sufficient for a given task. Traditional information extraction (IE) from text may be coarsely characterized as representing a certain level of semantic parsing, where the goal is to derive enough meaning in order to populate a database with factoids of a form matching a given schema.1 Given the ease with which reasonably accurate, deep syntactic structure can b</context>
</contexts>
<marker>Kwiatkowski, Choi, Artzi, Zettlemoyer, 2013</marker>
<rawString>Tom Kwiatkowski, Eunsol Choi, Yoav Artzi, and Luke Zettlemoyer. 2013. Scaling Semantic Parsers with On-the-fly Ontology Matching. In Proceedings of EMNLP.</rawString>
</citation>
<citation valid="true">
<authors>
<author>Percy Liang</author>
<author>Michael I Jordan</author>
<author>Dan Klein</author>
</authors>
<title>Learning Dependency-Based Compositional Semantics.</title>
<date>2011</date>
<booktitle>In Proceedings of ACL.</booktitle>
<contexts>
<context position="1833" citStr="Liang et al., 2011" startWordPosition="273" endWordPosition="276">ity of such work treats the KB as a database, to which standard database queries (SPARQL, MySQL, etc.) are issued to retrieve answers. Language understanding is modeled as the task of converting natural language questions into queries through intermediate logical forms, with the popular two approaches including: CCG parsing (Zettlemoyer and Collins, 2005; Zettlemoyer and Collins, 2007; Zettlemoyer and Collins, 2009; Kwiatkowski et al., 2010; Kwiatkowski et al., 2011; Krishnamurthy and Mitchell, 2012; Kwiatkowski et al., 2013; Cai and Yates, 2013a), and dependencybased compositional semantics (Liang et al., 2011; Berant et al., 2013; Berant and Liang, 2014). We characterize semantic parsing as the task of deriving a representation of meaning from language, sufficient for a given task. Traditional information extraction (IE) from text may be coarsely characterized as representing a certain level of semantic parsing, where the goal is to derive enough meaning in order to populate a database with factoids of a form matching a given schema.1 Given the ease with which reasonably accurate, deep syntactic structure can be automatically derived over (English) text, it is not surprising that IE researchers wo</context>
</contexts>
<marker>Liang, Jordan, Klein, 2011</marker>
<rawString>Percy Liang, Michael I. Jordan, and Dan Klein. 2011. Learning Dependency-Based Compositional Semantics. In Proceedings of ACL.</rawString>
</citation>
<citation valid="true">
<authors>
<author>M D Smucker</author>
<author>J Allan</author>
<author>B Carterette</author>
</authors>
<title>A comparison of statistical significance tests for information retrieval evaluation.</title>
<date>2007</date>
<booktitle>In Proceedings of the sixteenth ACM conference on Conference on information and knowledge management,</booktitle>
<pages>623--632</pages>
<publisher>ACM.</publisher>
<contexts>
<context position="8069" citStr="Smucker et al., 2007" startWordPosition="1292" endWordPosition="1295">ng all test questions, we lowered the logistic regression prediction threshold (usually 0.5) on jacana-freebase for the other 20% of questions where jacana-freebase had not proposed an answer to, and selected the best-possible prediction with the highest prediction score as the answer. In this way jacana-freebase was able to answer all questions with a lower accuracy of 35.4%. In the following we present analysis results based on the test questions where the two systems had very similar performance (35.7% vs. 35.4%).4 The difference is not significant according to the paired permutation test (Smucker et al., 2007). 3.2 Accuracy vs. Coverage First, we were interested to see the proportions of questions SEMPRE and jacana-freebase jointly and separately answered correctly. The answer to 4In this setting accuracy equals averaged macro Fl: first the Fl value on each question were computed, then averaged among all questions, or put it in other words: “accuracy with partial credit”. In this section our usage of the terms “accuracy” and “F1” can be exchanged. 83 SEMPRE Accuracy jacana (F1 = 1) jacana (F1 ≥ 0.5) √ × √ × √ 153 (0.08) 383 (0.19) 429 (0.21) 321 (0.16) × 136 (0.07) 1360 (0.67) 366 (0.18) 916 (0.45)</context>
</contexts>
<marker>Smucker, Allan, Carterette, 2007</marker>
<rawString>M.D. Smucker, J. Allan, and B. Carterette. 2007. A comparison of statistical significance tests for information retrieval evaluation. In Proceedings of the sixteenth ACM conference on Conference on information and knowledge management, pages 623– 632. ACM.</rawString>
</citation>
<citation valid="true">
<authors>
<author>Lappoon R Tang</author>
<author>Raymond J Mooney</author>
</authors>
<title>Using multiple clause constructors in inductive logic programming for semantic parsing.</title>
<date>2001</date>
<booktitle>In Machine Learning: ECML</booktitle>
<publisher>Springer.</publisher>
<marker>Tang, Mooney, 2001</marker>
<rawString>Lappoon R Tang and Raymond J Mooney. 2001. Using multiple clause constructors in inductive logic programming for semantic parsing. In Machine Learning: ECML 2001. Springer.</rawString>
</citation>
<citation valid="true">
<authors>
<author>Xuchen Yao</author>
<author>Benjamin Van Durme</author>
</authors>
<title>Information extraction over structured data: Question answering with freebase.</title>
<date>2014</date>
<booktitle>In Proceedings of ACL.</booktitle>
<marker>Yao, Van Durme, 2014</marker>
<rawString>Xuchen Yao and Benjamin Van Durme. 2014. Information extraction over structured data: Question answering with freebase. In Proceedings of ACL.</rawString>
</citation>
<citation valid="true">
<authors>
<author>Luke S Zettlemoyer</author>
<author>Michael Collins</author>
</authors>
<title>Learning to map sentences to logical form: Structured classification with probabilistic categorial grammars.</title>
<date>2005</date>
<booktitle>Uncertainty in Artificial Intelligence (UAI).</booktitle>
<contexts>
<context position="1571" citStr="Zettlemoyer and Collins, 2005" startWordPosition="235" endWordPosition="238">Introduction Question Answering (QA) from structured data, such as DBPedia (Auer et al., 2007), Freebase (Bollacker et al., 2008) and Yago2 (Hoffart et al., 2011), has drawn significant interest from both knowledge base (KB) and semantic parsing (SP) researchers. The majority of such work treats the KB as a database, to which standard database queries (SPARQL, MySQL, etc.) are issued to retrieve answers. Language understanding is modeled as the task of converting natural language questions into queries through intermediate logical forms, with the popular two approaches including: CCG parsing (Zettlemoyer and Collins, 2005; Zettlemoyer and Collins, 2007; Zettlemoyer and Collins, 2009; Kwiatkowski et al., 2010; Kwiatkowski et al., 2011; Krishnamurthy and Mitchell, 2012; Kwiatkowski et al., 2013; Cai and Yates, 2013a), and dependencybased compositional semantics (Liang et al., 2011; Berant et al., 2013; Berant and Liang, 2014). We characterize semantic parsing as the task of deriving a representation of meaning from language, sufficient for a given task. Traditional information extraction (IE) from text may be coarsely characterized as representing a certain level of semantic parsing, where the goal is to derive </context>
</contexts>
<marker>Zettlemoyer, Collins, 2005</marker>
<rawString>Luke S Zettlemoyer and Michael Collins. 2005. Learning to map sentences to logical form: Structured classification with probabilistic categorial grammars. Uncertainty in Artificial Intelligence (UAI).</rawString>
</citation>
<citation valid="true">
<authors>
<author>Luke S Zettlemoyer</author>
<author>Michael Collins</author>
</authors>
<title>Online learning of relaxed CCG grammars for parsing to logical form.</title>
<date>2007</date>
<booktitle>In Proceedings of EMNLP-CoNLL.</booktitle>
<contexts>
<context position="1602" citStr="Zettlemoyer and Collins, 2007" startWordPosition="239" endWordPosition="242"> (QA) from structured data, such as DBPedia (Auer et al., 2007), Freebase (Bollacker et al., 2008) and Yago2 (Hoffart et al., 2011), has drawn significant interest from both knowledge base (KB) and semantic parsing (SP) researchers. The majority of such work treats the KB as a database, to which standard database queries (SPARQL, MySQL, etc.) are issued to retrieve answers. Language understanding is modeled as the task of converting natural language questions into queries through intermediate logical forms, with the popular two approaches including: CCG parsing (Zettlemoyer and Collins, 2005; Zettlemoyer and Collins, 2007; Zettlemoyer and Collins, 2009; Kwiatkowski et al., 2010; Kwiatkowski et al., 2011; Krishnamurthy and Mitchell, 2012; Kwiatkowski et al., 2013; Cai and Yates, 2013a), and dependencybased compositional semantics (Liang et al., 2011; Berant et al., 2013; Berant and Liang, 2014). We characterize semantic parsing as the task of deriving a representation of meaning from language, sufficient for a given task. Traditional information extraction (IE) from text may be coarsely characterized as representing a certain level of semantic parsing, where the goal is to derive enough meaning in order to popu</context>
</contexts>
<marker>Zettlemoyer, Collins, 2007</marker>
<rawString>Luke S. Zettlemoyer and Michael Collins. 2007. Online learning of relaxed CCG grammars for parsing to logical form. In Proceedings of EMNLP-CoNLL.</rawString>
</citation>
<citation valid="true">
<authors>
<author>Luke S Zettlemoyer</author>
<author>Michael Collins</author>
</authors>
<title>Learning context-dependent mappings from sentences to logical form.</title>
<date>2009</date>
<booktitle>In Proceedings of ACLCoNLL.</booktitle>
<contexts>
<context position="1633" citStr="Zettlemoyer and Collins, 2009" startWordPosition="243" endWordPosition="246">h as DBPedia (Auer et al., 2007), Freebase (Bollacker et al., 2008) and Yago2 (Hoffart et al., 2011), has drawn significant interest from both knowledge base (KB) and semantic parsing (SP) researchers. The majority of such work treats the KB as a database, to which standard database queries (SPARQL, MySQL, etc.) are issued to retrieve answers. Language understanding is modeled as the task of converting natural language questions into queries through intermediate logical forms, with the popular two approaches including: CCG parsing (Zettlemoyer and Collins, 2005; Zettlemoyer and Collins, 2007; Zettlemoyer and Collins, 2009; Kwiatkowski et al., 2010; Kwiatkowski et al., 2011; Krishnamurthy and Mitchell, 2012; Kwiatkowski et al., 2013; Cai and Yates, 2013a), and dependencybased compositional semantics (Liang et al., 2011; Berant et al., 2013; Berant and Liang, 2014). We characterize semantic parsing as the task of deriving a representation of meaning from language, sufficient for a given task. Traditional information extraction (IE) from text may be coarsely characterized as representing a certain level of semantic parsing, where the goal is to derive enough meaning in order to populate a database with factoids o</context>
</contexts>
<marker>Zettlemoyer, Collins, 2009</marker>
<rawString>Luke S Zettlemoyer and Michael Collins. 2009. Learning context-dependent mappings from sentences to logical form. In Proceedings of ACLCoNLL.</rawString>
</citation>
</citationList>
</algorithm>
</algorithms>
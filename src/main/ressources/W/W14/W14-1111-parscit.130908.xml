<?xml version="1.0" encoding="UTF-8"?>
<algorithms version="110505">
<algorithm name="SectLabel" version="110505">
<variant no="0" confidence="0.070867">
<title confidence="0.987939">
Precise Medication Extraction using Agile Text Mining
</title>
<author confidence="0.998809">
Chaitanya Shivade*, James Cormack†, David Milward†
</author>
<affiliation confidence="0.997923">
*The Ohio State University, Columbus, Ohio, USA
</affiliation>
<address confidence="0.564698">
†Linguamatics Ltd, Cambridge, UK
</address>
<email confidence="0.9818035">
shivade@cse.ohio-state.edu,
{james.cormack,david.milward}@linguamatics.com
</email>
<sectionHeader confidence="0.993416" genericHeader="abstract">
Abstract
</sectionHeader>
<bodyText confidence="0.999613692307692">
Agile text mining is widely used for commercial
text mining in the pharmaceutical industry. It can
be applied without building an annotated training
corpus, so is well-suited to novel or one-off
extraction tasks. In this work we wanted to see how
efficiently it could be adapted for healthcare
extraction tasks such as medication extraction. The
aim was to identify medication names, associated
dosage, route of administration, frequency,
duration and reason, as specified in the 2009 i2b2
medication challenge.
Queries were constructed based on 696 discharge
summaries available as training data. Performance
was measured on a test dataset of 251 unseen
documents. F1-scores were calculated by
comparing system annotations against ground truth
provided for the test data.
Despite the short amount of time spent in adapting
the system to this task, it achieved high precision
and reasonable recall (precision of 0.92, recall of
0.715). It would have ranked fourth in comparison
to the original challenge participants on the basis of
its F-score of 0.805 for phrase level horizontal
evaluation. This shows that agile text mining is an
effective approach towards information extraction
that can yield highly accurate results.
</bodyText>
<sectionHeader confidence="0.999133" genericHeader="keywords">
1 Introduction
</sectionHeader>
<bodyText confidence="0.999963939393939">
Medication information occupies a sizeable
portion of clinical notes, especially discharge
summaries. This includes medications on
admission, during hospital course, and at
discharge. This information is useful for clinical
tasks such as inferring adverse drug reactions,
clinical trial recruitment, etc. The i2b2 Natural
Language Processing (NLP) challenges
encourage the development of systems for
clinical applications, using a shared task,
publicly available clinical data, and comparison
of performance with the other participating
systems, subject to rigid evaluation metrics. The
2009 challenge (Uzuner, Solti, &amp; Cadag, 2010)
aimed to extract mentions of medication names,
associated dosage, route of administration,
frequency, duration and the reason for
medication.
The project used the Linguamatics Interactive
Information Extraction (I2E) platform. This
combines NLP, terminologies and search
technology to provide a unique “agile” text
mining approach (Milward et al., 2005) that can
yield highly precise results in a small amount of
time. The approach involves semantic annotation
and indexing of data followed by interactive
design of queries that capture typical syntactic
and semantic features of the desired information.
While the system uses machine learning
approaches within its core linguistic processing,
the final set of queries are essentially
syntactic/semantic rules identifying specific
information in the text.
</bodyText>
<sectionHeader confidence="0.929989" genericHeader="introduction">
2 Section Identification
</sectionHeader>
<bodyText confidence="0.999667466666667">
Although discharge summaries are considered to
be unstructured data, there are typical
characteristics associated with them. There is a
specific flow of information within every
discharge summary, starting with details of
patient’s admission, followed by the hospital
course and ending with discharge instructions.
Other common sections include chief complaint,
physical examination, etc. There were more than
twenty headings to express discharge
medications in the training data (“Medications on
discharge,” “Discharge meds,” etc.). The training
data was processed to identify section headings
and multiple forms of the same heading were
normalized to a single heading. The plain text
</bodyText>
<page confidence="0.996847">
75
</page>
<note confidence="0.6972655">
Proceedings of the 5th International Workshop on Health Text Mining and Information Analysis (Louhi) @ EACL 2014, pages 75–79,
Gothenburg, Sweden, April 26-30 2014. c�2014 Association for Computational Linguistics
</note>
<bodyText confidence="0.965178">
was converted into XML with tags representing
section names.
</bodyText>
<sectionHeader confidence="0.877642" genericHeader="method">
3 Offset Information
</sectionHeader>
<bodyText confidence="0.99962675">
are constrained by linguistic boundaries, such as
the sentence. The output of the query can also be
customized so as to provide structured
representation of the query results.
To allow evaluation of results in the i2b2 format,
the text was preprocessed to include line
numbers and word numbers as further XML
annotations.
</bodyText>
<sectionHeader confidence="0.983889" genericHeader="method">
4 Natural Language Processing
</sectionHeader>
<bodyText confidence="0.999853285714286">
Indexing documents with I2E uses a standard
NLP pipeline involving tokenization of the text,
part-of-speech tagging, and linguistic chunking.
The output of the pipeline provides useful
linguistic information, particularly about the
location of noun phrases and verb phrases, for
use in entity extraction and querying.
</bodyText>
<sectionHeader confidence="0.998881" genericHeader="method">
5 Terminologies
</sectionHeader>
<bodyText confidence="0.999983230769231">
The I2E platform uses hierarchical terminologies
to extract entities from the text. These can
include freely available terminologies such as
MeSH, and the NCI thesaurus, as well as
proprietary terminologies such as MedDRA. A
series of regular expressions allow for the
indexing of numeric terms (integers, fractions,
decimal numbers) and measurement units (length,
time, weight, etc.). In addition, custom
terminologies can be created for specific tasks by
combining or merging existing terminologies, or
by using the system itself to help discover
terminology from the data.
</bodyText>
<sectionHeader confidence="0.987081" genericHeader="method">
6 Querying
</sectionHeader>
<bodyText confidence="0.999987944444445">
The I2E framework provides an interactive
querying experience that is similar to a web
search. While users can enter text queries just as
one might in an internet search engine, the query
interface also allows specification of linguistic
and non-linguistic units as ‘containers’ for other
units. For example, it is possible to search for a
noun phrase within a sentence and to specify
words, regular expressions and concepts from
terminologies. Non-linguistic units can be
customized to regulate the ordering of items
within the container, the number of items that
may occur between two items and whether they
As an example, one of the typical ways a
medication is prescribed follows the construct:
“Aspirin 625 mg p.o. b.i.d.” This means Aspirin
with a dosage of 625 milligrams is to be
consumed orally (p.o.), twice a day (b.i.d.). A
query to capture this construct can be constructed
as a non-linguistic phrase, starting with (a) a
pharmacological substance (a concept from the
appropriate branch of the NCI-thesaurus),
followed by (b) a numerical term, (c) a unit for
measuring weight, (d) a dosage abbreviation and
finally, (e) an abbreviation for the frequency of
medication.
A query containing items only for (a), (b) and (c)
will give results for all phrases containing a
pharmacological substance followed by its
dosage (Aspirin 625 mg, Tylenol 350 mg, etc.).
The graphical query interface is sufficiently
flexible to allow many different orderings of
these constructs and to negate false positive
results.
User defined terminologies can be
systematically constructed to allow consistent
matching of lists of terms and to generate concise
queries. For example, candidates for
abbreviations corresponding to the route of
administration were found by constructing a
query with items for (a), (b), (c) and (e) and an
empty word container for (d). This gave all
phrases containing (a), (b), (c), and any word in
the discharge summary that was followed by (e).
The results of this query were candidates for
route of administration. The efficiency of
querying in I2E provides an opportunity to
interactively refine parts of the final query and
discover terms in the training data that might be
missed by regular expressions and thesauri.
Queries can also be limited to specific sections
of the document. The pre-processing step
described above identified sections in discharge
summaries of the i2b2 medications challenge
</bodyText>
<page confidence="0.927027">
76
</page>
<bodyText confidence="0.999894888888889">
corpus. The queries can thus be limited to only a
few specific sections such as “Medications on
Admission” and “Medications on Discharge” by
embedding the query in a section container. The
challenge specified not to include medications
mentioned as allergies for a patient. Results
obtained in the allergies section of discharge
summaries were therefore ignored using this
approach.
</bodyText>
<sectionHeader confidence="0.891806" genericHeader="method">
7 Post-processing
</sectionHeader>
<bodyText confidence="0.999989340909091">
I2E’s default output is an HTML table with
columns corresponding to different containers
used in the query. Output can also be limited to
predefined columns of interest. Multiple queries
are often required to capture different pieces of
information spread across the corpus. In the i2b2
challenge, there are multiple fields associated
with every mention of medication. A single
structured record corresponding to every mention
of medication is expected as an output. Spasic et
al. (2010) view the challenge as a template filling
task where the participating system is expected
to fill slots in a template. Thus, the output can be
configured to be 6 columns representing each of
the templates. Following their terminology,
different semantic queries filled different slots of
the same template. These slots were aggregated
into a single template using post-processing.
Multiple issues had to be taken care of in this
step. Different queries captured parts of the text
corresponding to the same slot. For example, a
query aimed at capturing a particular linguistic
construct may extract frequency as “daily after
dinner,” while another query may capture its
substring “daily.” In this case, the former
extraction, which is the longer string, received
priority as per the challenge specifications.
Another important problem encountered was that
of multiple matches for the same field. For
example, Insulin and Aspart were identified as
separate pharmacological substances during the
indexing process. However, “Insulin aspart” is
considered as a single medication name as per
the challenge specifications. Two separate
templates are thus created. The results of the post
processing collapse them into one. Certain terms
from the terminologies did not match the
definition of a medication, since terminology
branches are often generic. For example, the
Chemicals and Drugs branch of MeSH
constitutes terms such as coffee. Therefore, a list
of false positives for medication names
corresponding to these matches was generated
from the training data.
</bodyText>
<sectionHeader confidence="0.998669" genericHeader="method">
8 Experiments
</sectionHeader>
<bodyText confidence="0.999943315789474">
The i2b2 website offers downloading of the NLP
dataset for the 2009 challenge after signing a
Data Usage Agreement. The training data
consists of 696 discharge summaries. A subset of
ten documents with gold standard annotations
has been made available by the organizers. The
test dataset consists of 251 documents which
were annotated by the participants under a
community annotation experiment conducted by
the organizers (Uzuner, Solti, Xia, et al. 2010).
These 251 documents and their corresponding
gold standard annotations are also available. The
performance was calculated using phrase level
and token level metrics for horizontal and
vertical evaluations as defined in (Uzuner, Solti,
&amp; Cadag, 2010). The phrase level horizontal
evaluation measures the performance of a system
across all six fields. This was used as a primary
metric to rank the results in the challenge.
</bodyText>
<table confidence="0.999912">
Terminology P R F1
NCI 0.953 0.657 0.777
MeSH 0.923 0.563 0.699
NCI + MeSH 0.932 0.688 0.792
NCI + FDA 0.947 0.678 0.790
MeSH + FDA 0.921 0.571 0.705
NCI + MeSH + 0.931 0.698 0.798
FDA
NCI + MeSH + 0.92 0.715 0.805
FDA + RxNorm
</table>
<tableCaption confidence="0.999977">
Table 1: Comparison of Different Terminologies.
</tableCaption>
<bodyText confidence="0.931226">
In order to assess the utility of different
terminologies, the same set of queries were
modified by replacing the concept from one with
the corresponding concept in another. For
example: Pharmacological substance from NCI
</bodyText>
<page confidence="0.996781">
77
</page>
<bodyText confidence="0.999970647058824">
was replaced with Chemicals and Drugs from
MeSH. This offered an objective way to compare
the coverage of MeSH and NCI with respect to
medication names. Coverage of multiple
terminologies can be leveraged by aggregating
the results of queries resulting from different
terminologies. NCI thesaurus, MeSH, a list of
FDA drug labels, and RxNorm were used. In
addition a custom terminology was prepared by
capturing medication names in the training data
that were missed by the terminologies. The best
F-score was obtained when query results for all
sources were aggregated. Addition of sources
resulted in a drop in precision but increased
recall. Table 1 summarizes these results, where
columns P and R denote precision and recall
respectively.
</bodyText>
<sectionHeader confidence="0.999919" genericHeader="evaluation">
9 Results
</sectionHeader>
<bodyText confidence="0.997919777777778">
Twenty teams representing 23 organizations and
nine countries participated in the medication
challenge. The other systems used a variety of
rule-based, machine-learning and hybrid systems,
with the most popular being rule-based systems
(Uzuner et al., 2010). The best ranked system,
detailed in Patrick &amp; Li (2009), was an example
of a hybrid system, using both rule-based and
statistical classifiers.
</bodyText>
<table confidence="0.9997876">
No. Group P R F1
1 USyd 0.896 0.82 0.857
2 Vanderbilt 0.840 0.803 0.821
3 Manchester 0.864 0.766 0.812
* I2E 0.920 0.715 0.805
4 NLM 0.784 0.823 0.803
5 BME - 0.841 0.758 0.797
Humboldt
6 OpenU 0.850 0.748 0.796
7 UParis 0.799 0.761 780
8 LIMSI 0.827 0.725 0.773
9 UofUtah 0.832 0.715 0.769
10 U 0.904 0.661 0.764
Wisconsin
Madison
</table>
<tableCaption confidence="0.999786">
Table 2: Phrase level horizontal evaluation
</tableCaption>
<bodyText confidence="0.9999903125">
Phrase level horizontal evaluation was used as a
metric to rank the performance of participants in
the challenge. Table 2 compares the performance
of I2E with the top ten participants in the
challenge using this metric. It achieves highly
precise results as compared to other participants
of the challenge. The vertical evaluation which
measures the performance along individual fields
showed that the system performed poorly on
duration and reason, in common with other
systems. As reported by the organizers of the
challenge (Uzuner et al., 2010), capturing
duration and reason is a hard task. They report
that this is primarily due to the variation in
length and content of these fields in the training
and testing data.
</bodyText>
<sectionHeader confidence="0.99665" genericHeader="conclusions">
10 Conclusion
</sectionHeader>
<bodyText confidence="0.9999902">
Extracting information through interactive design
of queries can achieve highly precise results in a
short amount of time. Much of the time in this
project was spent on pre-processing documents
to allow the results to conform to the i2b2 format.
The time taken on query development was of the
order of a few weeks, including a couple of days
training in the system at the start of the project.
This process requires far less specialist
knowledge of Artificial Intelligence than other
solutions to this challenge and the easy to use
interface means refinement is straightforward.
Clearly, recall still needs to be improved: our
best system would have been ranked 4th out of
21 systems in the phrase level horizontal
evaluation. Examination of the training material
suggests this is due to gaps in the drug coverage
provided by the terminologies rather than gaps in
the query patterns. We will therefore concentrate
on extending drug coverage in our future work.
</bodyText>
<sectionHeader confidence="0.998665" genericHeader="acknowledgments">
Acknowledgments
</sectionHeader>
<bodyText confidence="0.999685666666667">
The authors would like to thank Tracy Gregory,
Himanshu Agarwal and Matthijs Vakar for their
help in this project.
</bodyText>
<sectionHeader confidence="0.993574" genericHeader="references">
References
</sectionHeader>
<bodyText confidence="0.5326485">
Milward, D. et al., 2005. Ontology-based interactive
information extraction from scientific abstracts.
</bodyText>
<page confidence="0.989416">
78
</page>
<figure confidence="0.81263225">
Comparative and functional genomics, 6(1-2), pp.67–
71.
American Medical Informatics Association: JAMIA,
17(5), pp.519–23.
</figure>
<reference confidence="0.9830878">
Spasic, I. et al., 2010. Medication information
extraction with linguistic pattern matching and
semantic rules. Journal of the American Medical
Informatics Association: JAMIA, 17(5), pp.532–5.
Uzuner, O., Solti, I., Xia, F., et al., 2010. Community
annotation experiment for ground truth generation for
the i2b2 medication challenge. Journal of the
Uzuner, O. Solti, I. &amp; Cadag, E., 2010. Extracting
medication information from clinical text. Journal of
the American Medical Informatics Association:
JAMIA, 17(5), pp.514–8
Patrick J &amp; Li M. A Cascade Approach to Extracting
Medication Events. Proceedings of the Third i2b2
Workshop on Challenges in Natural Language
Processing for Clinical Data, 2009.
</reference>
<page confidence="0.999045">
79
</page>
</variant>
</algorithm>
<algorithm name="ParsHed" version="110505">
<variant no="0" confidence="0.986461">
<title confidence="0.999915">Precise Medication Extraction using Agile Text Mining</title>
<author confidence="0.999661">James David</author>
<affiliation confidence="0.999781">Ohio State University, Columbus, Ohio,</affiliation>
<address confidence="0.996223">Ltd, Cambridge, UK</address>
<email confidence="0.999663">james.cormack@linguamatics.com</email>
<email confidence="0.999663">david.milward@linguamatics.com</email>
<abstract confidence="0.999663111111111">Agile text mining is widely used for commercial text mining in the pharmaceutical industry. It can be applied without building an annotated training corpus, so is well-suited to novel or one-off extraction tasks. In this work we wanted to see how efficiently it could be adapted for healthcare extraction tasks such as medication extraction. The aim was to identify medication names, associated dosage, route of administration, frequency, duration and reason, as specified in the 2009 i2b2 medication challenge. Queries were constructed based on 696 discharge summaries available as training data. Performance was measured on a test dataset of 251 unseen documents. F1-scores were calculated by comparing system annotations against ground truth provided for the test data. Despite the short amount of time spent in adapting the system to this task, it achieved high precision and reasonable recall (precision of 0.92, recall of 0.715). It would have ranked fourth in comparison to the original challenge participants on the basis of its F-score of 0.805 for phrase level horizontal evaluation. This shows that agile text mining is an effective approach towards information extraction that can yield highly accurate results.</abstract>
</variant>
</algorithm>
<algorithm name="ParsCit" version="110505">
<citationList>
<citation valid="true">
<authors>
<author>I Spasic</author>
</authors>
<title>Medication information extraction with linguistic pattern matching and semantic rules.</title>
<date>2010</date>
<journal>Journal of the American Medical Informatics Association: JAMIA,</journal>
<volume>17</volume>
<issue>5</issue>
<pages>532--5</pages>
<marker>Spasic, 2010</marker>
<rawString>Spasic, I. et al., 2010. Medication information extraction with linguistic pattern matching and semantic rules. Journal of the American Medical Informatics Association: JAMIA, 17(5), pp.532–5.</rawString>
</citation>
<citation valid="true">
<authors>
<author>O Uzuner</author>
<author>I Solti</author>
<author>F Xia</author>
</authors>
<title>Community annotation experiment for ground truth generation for the i2b2 medication challenge.</title>
<date>2010</date>
<journal>Journal of the Uzuner, O. Solti, I. &amp; Cadag, E.,</journal>
<volume>17</volume>
<issue>5</issue>
<pages>514--8</pages>
<contexts>
<context position="12508" citStr="Uzuner et al., 2010" startWordPosition="1880" endWordPosition="1883">prepared by capturing medication names in the training data that were missed by the terminologies. The best F-score was obtained when query results for all sources were aggregated. Addition of sources resulted in a drop in precision but increased recall. Table 1 summarizes these results, where columns P and R denote precision and recall respectively. 9 Results Twenty teams representing 23 organizations and nine countries participated in the medication challenge. The other systems used a variety of rule-based, machine-learning and hybrid systems, with the most popular being rule-based systems (Uzuner et al., 2010). The best ranked system, detailed in Patrick &amp; Li (2009), was an example of a hybrid system, using both rule-based and statistical classifiers. No. Group P R F1 1 USyd 0.896 0.82 0.857 2 Vanderbilt 0.840 0.803 0.821 3 Manchester 0.864 0.766 0.812 * I2E 0.920 0.715 0.805 4 NLM 0.784 0.823 0.803 5 BME - 0.841 0.758 0.797 Humboldt 6 OpenU 0.850 0.748 0.796 7 UParis 0.799 0.761 780 8 LIMSI 0.827 0.725 0.773 9 UofUtah 0.832 0.715 0.769 10 U 0.904 0.661 0.764 Wisconsin Madison Table 2: Phrase level horizontal evaluation Phrase level horizontal evaluation was used as a metric to rank the performance</context>
</contexts>
<marker>Uzuner, Solti, Xia, 2010</marker>
<rawString>Uzuner, O., Solti, I., Xia, F., et al., 2010. Community annotation experiment for ground truth generation for the i2b2 medication challenge. Journal of the Uzuner, O. Solti, I. &amp; Cadag, E., 2010. Extracting medication information from clinical text. Journal of the American Medical Informatics Association: JAMIA, 17(5), pp.514–8</rawString>
</citation>
<citation valid="true">
<authors>
<author>J Patrick</author>
<author>M Li</author>
</authors>
<title>A Cascade Approach to Extracting Medication Events.</title>
<date>2009</date>
<booktitle>Proceedings of the Third i2b2 Workshop on Challenges in Natural Language Processing for Clinical Data,</booktitle>
<contexts>
<context position="12565" citStr="Patrick &amp; Li (2009)" startWordPosition="1890" endWordPosition="1893">a that were missed by the terminologies. The best F-score was obtained when query results for all sources were aggregated. Addition of sources resulted in a drop in precision but increased recall. Table 1 summarizes these results, where columns P and R denote precision and recall respectively. 9 Results Twenty teams representing 23 organizations and nine countries participated in the medication challenge. The other systems used a variety of rule-based, machine-learning and hybrid systems, with the most popular being rule-based systems (Uzuner et al., 2010). The best ranked system, detailed in Patrick &amp; Li (2009), was an example of a hybrid system, using both rule-based and statistical classifiers. No. Group P R F1 1 USyd 0.896 0.82 0.857 2 Vanderbilt 0.840 0.803 0.821 3 Manchester 0.864 0.766 0.812 * I2E 0.920 0.715 0.805 4 NLM 0.784 0.823 0.803 5 BME - 0.841 0.758 0.797 Humboldt 6 OpenU 0.850 0.748 0.796 7 UParis 0.799 0.761 780 8 LIMSI 0.827 0.725 0.773 9 UofUtah 0.832 0.715 0.769 10 U 0.904 0.661 0.764 Wisconsin Madison Table 2: Phrase level horizontal evaluation Phrase level horizontal evaluation was used as a metric to rank the performance of participants in the challenge. Table 2 compares the p</context>
</contexts>
<marker>Patrick, Li, 2009</marker>
<rawString>Patrick J &amp; Li M. A Cascade Approach to Extracting Medication Events. Proceedings of the Third i2b2 Workshop on Challenges in Natural Language Processing for Clinical Data, 2009.</rawString>
</citation>
</citationList>
</algorithm>
</algorithms>
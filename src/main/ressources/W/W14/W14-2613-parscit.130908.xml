<?xml version="1.0" encoding="UTF-8"?>
<algorithms version="110505">
<algorithm name="SectLabel" version="110505">
<variant no="0" confidence="0.002854">
<title confidence="0.994666">
Linguistically Informed Tweet Categorization for Online Reputation
Management
</title>
<author confidence="0.987487">
Gerard Lynch and P´adraig Cunningham
</author>
<affiliation confidence="0.896528">
Centre for Applied Data Analytics Research
(CeADAR)
University College Dublin
</affiliation>
<address confidence="0.719139">
Belfield Office Park
Dublin 4, Ireland
</address>
<email confidence="0.996779">
firstname.lastname@ucd.ie
</email>
<sectionHeader confidence="0.993825" genericHeader="abstract">
Abstract
</sectionHeader>
<bodyText confidence="0.999937535714286">
Determining relevant content automati-
cally is a challenging task for any ag-
gregation system. In the business intel-
ligence domain, particularly in the appli-
cation area of Online Reputation Manage-
ment, it may be desirable to label tweets
as either customer comments which de-
serve rapid attention or tweets from in-
dustry experts or sources regarding the
higher-level operations of a particular en-
tity. We present an approach using a com-
bination of linguistic and Twitter-specific
features to represent tweets and examine
the efficacy of these in distinguishing be-
tween tweets which have been labelled
using Amazon’s Mechanical Turk crowd-
sourcing platform. Features such as part-
of-speech tags and function words prove
highly effective at discriminating between
the two categories of tweet related to sev-
eral distinct entity types, with Twitter-
related metrics such as the presence of
hashtags, retweets and user mentions also
adding to classification accuracy. Accu-
racy of 86% is reported using an SVM
classifier and a mixed set of the aforemen-
tioned features on a corpus of tweets re-
lated to seven business entities.
</bodyText>
<sectionHeader confidence="0.992176" genericHeader="keywords">
1 Motivation
</sectionHeader>
<bodyText confidence="0.998522571428571">
Online Reputation Management (ORM) is a grow-
ing field of interest in the domain of business in-
telligence. Companies and individuals alike are
highly interested in monitoring the opinions of
others across social and traditional media and this
information can have considerable business value
for corporate entities in particular.
</bodyText>
<subsectionHeader confidence="0.977006">
1.1 Challenges
</subsectionHeader>
<bodyText confidence="0.999942714285714">
There are a number of challenges in creating an
end-to-end software solution for such purposes,
and several shared tasks have already been estab-
lished to tackle these issues1. The most recent
RepLab evaluation was concerned with four tasks
related to ORM, filtering, polarity for reputation,
topic detection and priority assignment. Based
on these evaluations, it is clear that although the
state of the art of topic-based filtering of tweets is
relatively accomplished (Perez-Tellez et al., 2011;
Yerva et al., 2011; Spina et al., 2013), other as-
pects of the task such as sentiment analysis and
prioritisation of tweets based on content are less
trivial and require further analysis.
Whether Twitter mentions of entities are ac-
tual customer comments or in fact represent the
views of traditional media or industry experts and
sources is an important distinction for ORM sys-
tems. With this study we investigate the degree to
which this task can be automated using supervised
learning methods.
</bodyText>
<sectionHeader confidence="0.999903" genericHeader="introduction">
2 Related Work
</sectionHeader>
<subsectionHeader confidence="0.999628">
2.1 Studies on Twitter data
</subsectionHeader>
<bodyText confidence="0.999113">
While the majority of research in the computa-
tional sciences on Twitter data has focused on is-
sues such as topic detection (Cataldi et al., 2010),
event detection, (Weng and Lee, 2011; Sakaki
et al., 2010), sentiment analysis, (Kouloumpis et
al., 2011), and other tasks based primarily on the
topical and/or semantic content of tweets, there
is a growing body of work which investigates
more subtle forms of information represented in
tweets, such as reputation and trustworthiness,
(O’Donovan et al., 2012), authorship attribution
(Layton et al., 2010; Bhargava et al., 2013) and
Twitter spam detection, (Benevenuto et al., 2010).
</bodyText>
<footnote confidence="0.9687">
1See (Amig´o et al., 2012) and (Amig´o et al., 2013) for
details of the RepLab series
</footnote>
<page confidence="0.995781">
73
</page>
<bodyText confidence="0.968352333333333">
Proceedings of the 5th Workshop on Computational Approaches to Subjectivity, Sentiment and Social Media Analysis, pages 73–78,
Baltimore, Maryland, USA. June 27, 2014. c�2014 Association for Computational Linguistics
These studies combine Twitter-specific and textual
features such as retweet counts, tweet lengths and
hashtag frequency, together with sentence-length,
character n-grams and punctuation counts.
</bodyText>
<subsectionHeader confidence="0.999736">
2.2 Studies on non-Twitter data
</subsectionHeader>
<bodyText confidence="0.999947222222222">
The textual features used in our work such
as n-grams of words and parts-of-speech have
been used for gender-based language classifica-
tion (Koppel et al., 2002), social profiling and per-
sonality type detection (Mairesse et al., 2007), na-
tive language detection from L2 text, (Brooke and
Hirst, 2012) translation source language detection,
(van Halteren, 2008; Lynch and Vogel, 2012) and
translation quality detection, (Vogel et al., 2013).
</bodyText>
<sectionHeader confidence="0.979785" genericHeader="method">
3 Experimental setup and corpus
</sectionHeader>
<bodyText confidence="0.9997155">
Tweets were gathered between June 2013 and Jan-
uary 2014 using the twitter4j Java library. A lan-
guage detector was used to filter only English-
language tweets.2 The criteria for inclusion were
that the entity name was present in the tweet. The
entities focused on in this study had relatively un-
ambigious business names, so no complex filtering
was necessary.
</bodyText>
<subsectionHeader confidence="0.999804">
3.1 Pilot study
</subsectionHeader>
<bodyText confidence="0.999992785714286">
A smaller pilot study was carried out before the
main study in order to examine response quality
and accuracy of instruction. Two hundred sam-
ple tweets concerning two airlines3 were anno-
tated using Amazon’s Mechanical Turk system by
fourteen Master annotators. After annotation, we
selected the subset (72%) of tweets for which both
annotators agreed on the category to train the clas-
sifier. During the pilot study, the tweets were
pre-processed4 to remove @ and # symbols and
punctuation to treat account names and hashtags
as words. Hyperlinks representations were main-
tained within the tweets. The Twitter-specific met-
rics were not employed in the pilot study.
</bodyText>
<subsectionHeader confidence="0.999583">
3.2 Full study
</subsectionHeader>
<bodyText confidence="0.999222">
In the full study, 2454 tweets concerning seven
business entities5 were tagged by forty annota-
tors as to whether they corresponded to one of the
</bodyText>
<footnote confidence="0.871352428571429">
2A small amount of non-English tweets were found in the
dataset, these were assigned to the Other category.
3Aer Lingus and Ryanair
4This was not done in the full study, these symbols were
counted and used as features.
5Aer Lingus, Ryanair, Bank of Ireland, C &amp; C Group,
Permanent TSB, Glanbia, Greencore
</footnote>
<bodyText confidence="0.999707153846154">
three categories described in Section 1.1. For 57%
of the tweets, annotators agreed on the categories
with disagreement in the remaining 43%. The dis-
puted tweets were annotated again by two anno-
tators. From this batch, a similar proportion were
agreed on. For the non-agreed tweets in the sec-
ond round, a majority category vote was reached
by combining the four annotations over the first
and second rounds. After this process, roughly
two hundred tweets remained as ambiguous (each
having two annotations for one of two particular
categories) and these were removed from the cor-
pus used in the experiments.
</bodyText>
<subsectionHeader confidence="0.999035">
3.3 Category breakdown
</subsectionHeader>
<bodyText confidence="0.999987473684211">
Table 5 displays the number of tweets for which
no majority category agreement was reached. The
majority disagreement class across all entities are
texts which have been labelled as both business
operations and other. For the airline entities, a
large proportion of tweets were annotated as both
customer comment and other, this appeared to be
a categorical issue which may have required clar-
ification in the instructions. The smallest cate-
gory for tied agreement is customer comment and
business operations, it appears that the distinc-
tion between these categories was clearer based
on the data provided to annotators. 2078 tweets
were used in the final experiments. The classes
were somewhat imbalanced for the final corpus,
the business operations category was the largest,
with 1184 examples, customer comments con-
tained 585 examples and the other category con-
tained 309 examples.
</bodyText>
<subsectionHeader confidence="0.993405">
3.4 Feature types
</subsectionHeader>
<bodyText confidence="0.999776">
The features used for classification purposes can
be divided into the following two categories:
</bodyText>
<listItem confidence="0.999513142857143">
1. Twitter-specific:
• Tweet is a retweet or not
• Tweet contains a mention
• Tweet contains a hashtag or a link
• Weight measure (See Fig 3)
• Retweet account for a tweet.
2. Linguistic: The linguistic features are based
</listItem>
<bodyText confidence="0.854174666666667">
on the textual content of the tweet repre-
sented as word unigrams, word bigrams and
part-of-speech bigrams.
</bodyText>
<page confidence="0.992091">
74
</page>
<bodyText confidence="0.99994375">
We used TagHelperTools, (Ros´e et al., 2008) for
textual feature creation which utilises the Stanford
NLP toolkit for NLP annotation and returns for-
matted representations of textual features which
can be employed in the Weka toolkit which imple-
ments various machine learning algorithms. All
linguistic feature frequencies were binarised in our
representations6.
</bodyText>
<sectionHeader confidence="0.999975" genericHeader="method">
4 Results
</sectionHeader>
<subsectionHeader confidence="0.999827">
4.1 Pilot study
</subsectionHeader>
<bodyText confidence="0.999486952380952">
Using the Naive Bayes classifier in the Weka
toolkit and a feature set consisting of 130 word
tokens, 80% classification accuracy was obtained
using ten-fold cross validation on the full set of
tweets . Table 1 shows the top word features when
ranked using 10-fold cross validation and the in-
formation gain metric for classification power over
the three classes. Using the top 50 ranked POS-
bigram features alone, 74% classification accuracy
was obtained using the Naive Bayes classifier. Ta-
ble 2 shows the top twenty features, again ranked
by information gain.
Combining the fifty POS-bigrams and the 130
word features, we obtained 84% classification ac-
curacy using the Naive Bayes classifier. Accuracy
was improved by removing all noun features from
the dataset and using the top seventy five features
from the remaining set ranked with information
gain, resulting in 86.6% accuracy using the SVM
classifier with a linear kernel. Table 3 displays the
top twenty combined features.
</bodyText>
<table confidence="0.999883363636364">
Rank Feature Rank Feature
1 http 11 investors
2 flight 12 would
3 talks 13 by
4 for 14 says
5 strike 15 profit
6 an 16 cabin
7 you 17 crew
8 I 18 via
9 that 19 at
10 action 20 since
</table>
<tableCaption confidence="0.9514405">
Table 1: Top 20 ranked word features for pilot
study
</tableCaption>
<table confidence="0.951953416666667">
61 if feature is present in a tweet, otherwise 0.
Rank Feature Rank Feature
1 NNP EOL 11 VB PRP
2 VBD JJ 12 NN NNS
3 NNP VBD 13 IN PRP$
4 NNP NN 14 BOL CD
5 BOL PRP 15 BOL JJS
6 VBD NNP 16 IN VBN
7 NNP CC 17 PRP$ JJ
8 TO NNP 18 PRP MD
9 NN RB 19 PRP$ VBG
10 RB JJ 20 CC VBP
</table>
<tableCaption confidence="0.964904">
Table 2: Top 20 ranked POS bigram features for
pilot study
</tableCaption>
<table confidence="0.999912181818182">
Rank Feature Rank Feature
1 http 11 TO NNP
2 NNP EOL 12 RB JJ
3 NNP VBD 13 that
4 VBD JJ 14 tells
5 NNP NN 15 way
6 BOL PRP 16 I
7 VBD NNP 17 would
8 NNP CC 18 you
9 for 19 NN RB
10 an 20 BOL JJS
</table>
<tableCaption confidence="0.8505165">
Table 3: Top 20 ranked combined features for pilot
study
</tableCaption>
<sectionHeader confidence="0.8593655" genericHeader="method">
4.2 Full study
4.2.1 Results
</sectionHeader>
<bodyText confidence="0.99997275">
Using the SMO classifier, Weka’s support vec-
tor machine implementation using a linear kernel,
a hybrid feature set containing linguistic, custom
and Twitter-specific features obtained 72% clas-
sification accuracy for the three categories. F-
measures were highest for the business operations
class, and lowest for the other class, which con-
tained the most diversity. Examining Figure 2, it
is clear that f-measures for the other class are al-
most zero. This indicates that tweets given this
category may not be homogeneous enough to cat-
egorise using the features defined in Table 7.
</bodyText>
<subsectionHeader confidence="0.997678">
4.3 Two classes
</subsectionHeader>
<bodyText confidence="0.9992422">
After the removal of the other class from the
experiment, the same feature set obtained 86%
classification accuracy between the two remain-
ing classes. The distinguishing features consisted
predominantly of pronouns (I, me, my), part-of-
</bodyText>
<page confidence="0.994338">
75
</page>
<table confidence="0.9994207">
Entity BO CC Other
Aer Lingus 174 138 44
Ryanair 58 212 52
AIB 69 29 43
BOI 208 85 40
C&amp;C 45 14 15
Glanbia 276 39 46
Greencore 37 4 13
Kerry Group 158 10 36
Permanent TSB 160 54 20
</table>
<tableCaption confidence="0.8015375">
Table 4: Tweets per entity by category: Majority
agreement
</tableCaption>
<table confidence="0.945452">
Figure 1: F-scores by category for pilot study
Entity CC+BO O-CC O-BO
Aer Lingus 4 24 15
Ryanair 7 30 8
AIB 4 5 11
BOI 9 5 16
C&amp;C 0 1 3
Glanbia 7 4 19
Greencore 0 0 2
Kerry Group 5 2 12
Permanent TSB 3 6 10
</table>
<tableCaption confidence="0.885373">
Table 5: Tweets per entity by category: Tied
agreement
</tableCaption>
<bodyText confidence="0.999114">
speech bigrams including pairs of plural nouns,
lines beginning with prepositions and function
words (so, just, new, it). Business operations
tweets were more likely to mention a user account
or be a retweet, personal pronouns were more
commonplace in customer comments and as ob-
served in the pilot study, customer comments were
more likely to begin with a preposition and busi-
ness operations tweets were more likely to contain
noun-noun compounds and pairs of coordinating
conjunctions and nouns.
</bodyText>
<subsectionHeader confidence="0.915607">
4.4 Features
</subsectionHeader>
<bodyText confidence="0.9998141">
Hashtags were slightly more common in business
operations tweets, however the number of hash-
tags was not counted, simply whether at least one
was present. Hashtags as a proportion of words
might be a useful feature for further studies. Func-
tion words and POS tags were highly discrimina-
tory, indicating that this classifier may be applica-
ble to different topic areas. Weight (See Figure 3)
was a distinguishing feature, with business opera-
tions tweets having higher weight scores, reflect-
</bodyText>
<figureCaption confidence="0.96797">
Figure 2: F-scores by category for full study
</figureCaption>
<bodyText confidence="0.994875666666667">
ing the tendency for these tweets to originate from
Twitter accounts linked to news sources or influ-
ential industry experts.
</bodyText>
<sectionHeader confidence="0.997895" genericHeader="evaluation">
5 Results per sub-category
</sectionHeader>
<bodyText confidence="0.999971076923077">
To investigate whether the entity domain had a
bearing on the results, we separated the data into
three subsets, airlines, banks and food industry
concerns. We performed the same feature selec-
tion as in previous experiments, calculating each
feature type separately, removing proper nouns,
hashtags and account names from the word n-
grams, then combining and ranking the features
using ten-fold cross validation and information
gain. The SVM classifier reported similar results
to the main study on the three class problem for
each sub-domain, and for the two class problem
results ranged between 86-87% accuracy, similar
</bodyText>
<figure confidence="0.802374">
Number of followers
(retweets)
Number following
</figure>
<figureCaption confidence="0.995946">
Figure 3: Twitter weight metric
</figureCaption>
<page confidence="0.951954">
76
</page>
<bodyText confidence="0.9998885">
to the results on the mixed set7. Thus, we be-
lieve that the individual subdomains do not war-
rant different classifiers for the problem, indeed
examining the top 20-ranked features for each sub-
domain, there is a large degree of overlap, as seen
in bold and italics in Table 6.
</bodyText>
<table confidence="0.999489904761905">
Banks Airlines Food
@ @ @
my NNP NNP PRP VBP
i i i
me BOL IN BOL IN
PRP VBP PRP VBP VB PRP
account DT NN BOL PRP
NNP VBZ IN PRP HASHASH
VB PRP the you
IN PRP new me
you PRP VBD know
BOL RB NNP VBZ my
RB JJ IN DT i know
NNP NNP you PRP CC
PRP VBD BOL PRP used
my bank ISRT BOL CC
DT NN it NNP CD
NN PRP me NN NNP
VBD PRP my CC PRP
BOL IN RB RB ISRT
i’m so CC NNP
</table>
<tableCaption confidence="0.983033">
Table 6: Top twenty ranked features by Informa-
tion Gain for three domains
</tableCaption>
<sectionHeader confidence="0.944595" genericHeader="conclusions">
6 Conclusions and future directions
</sectionHeader>
<subsectionHeader confidence="0.9986">
6.1 Classification results
</subsectionHeader>
<bodyText confidence="0.999932">
We found that accurate categorization of our pre-
defined tweet types was possible using shallow
linguistic features. This was aided by Twitter spe-
cific metrics but these did not add significantly to
the classification accuracy8. The lower score (72-
73%) in the three class categorization problem is
due to the linguistic diversity of the other tweet
category.
</bodyText>
<subsectionHeader confidence="0.999909">
6.2 Annotation and Mechanical Turk
</subsectionHeader>
<bodyText confidence="0.99999175">
We found the definition of categorization criteria
to be an important and challenging step when us-
ing Mechanical Turk for annotation. The high de-
gree of annotator disagreement reflected this, how-
ever it is important to note that in many cases,
tweets fit equally into two or more of our defined
categories. The use of extra annotations9 allowed
for agreement to be reached in the majority of
</bodyText>
<footnote confidence="0.995276">
7The food subset was highly imbalanced however, con-
taining only 43 customer comments and 313 business opera-
tions tweets, the other two subsets were relatively balanced.
8ca. 2% decrease in accuracy on removal.
9over the initial two annotators
</footnote>
<bodyText confidence="0.99992955">
cases, however employing more evaluations could
have also resulted in deadlock. Examples of am-
biguous tweets included: Cheap marketing tactics.
Well, if it ain’t broke, why fix it! RT @Ryanair’s
summer ’14 schedule is now on sale! where a
Twitter user has retweeted an official announce-
ment and added their own comment.
Another possible pitfall is that as Mechanical
Turk is a US-based service and requires workers to
have a US bank account in order to perform work,
Turkers tend to be US-based, and therefore an an-
notation task concerning non-US business entities
is perhaps more difficult without sufficient back-
ground awareness of the entities in question.
Future experiments will apply the methodology
developed here to a larger dataset of tweets, one
candidate would be the dataset used in the RepLab
2013 evaluation series which contains 2,200 an-
notated tweets for 61 business entities in four do-
mains.
</bodyText>
<sectionHeader confidence="0.997474" genericHeader="acknowledgments">
Acknowledgments
</sectionHeader>
<bodyText confidence="0.999277333333333">
The authors are grateful to Enterprise Ireland and
the IDA for funding this research and CeADAR
through their Technology Centre Programme.
</bodyText>
<table confidence="0.999863846153846">
Rank Feature Rank Feature
1 @ 26 NNP PRP
2 i 27 NN PRP
3 PRP VBP 28 VBP PRP
4 my 29 when
5 BOL IN 30 if
6 me 31 don’t
7 you 32 PRP MD
8 NNP NNP 33 they
9 IN PRP 34 like
10 VB PRP 35 PRP VB
11 PRP VBD 36 got
12 WEIGHT 37 CC NNP
13 so 38 but
14 NNP VBZ 39 RB IN
15 BOL PRP 40 RT
16 RB JJ 41 with
17 DT NN 42 PRP IN
18 BOL RB 43 a
19 it 44 NNS RB
20 PRP RB 45 CC PRP
21 RB RB 46 VBD PRP
22 IN DT 47 VBD DT
23 i’m 48 no
24 just 49 the
25 get 50 PRP$ NN
</table>
<tableCaption confidence="0.917363">
Table 7: Top 50 ranked mixed features for main
study
</tableCaption>
<page confidence="0.99847">
77
</page>
<sectionHeader confidence="0.989435" genericHeader="references">
References
</sectionHeader>
<reference confidence="0.999654247422681">
Enrique Amig´o, Adolfo Corujo, Julio Gonzalo, Edgar
Meij, and Maarten de Rijke. 2012. Overview
of replab 2012: Evaluating online reputation man-
agement systems. In CLEF (Online Working
Notes/Labs/Workshop).
Enrique Amig´o, Jorge Carrillo de Albornoz, Irina
Chugur, Adolfo Corujo, Julio Gonzalo, Tamara
Martin, Edgar Meij, Maarten de Rijke, and Dami-
ano Spina. 2013. Overview of replab 2013:
Evaluating online reputation monitoring systems.
In Information Access Evaluation. Multilinguality,
Multimodality, and Visualization, pages 333–352.
Springer.
Fabrıcio Benevenuto, Gabriel Magno, Tiago Ro-
drigues, and Virgılio Almeida. 2010. Detect-
ing spammers on twitter. In Collaboration, elec-
tronic messaging, anti-abuse and spam conference
(CEAS), volume 6.
Mudit Bhargava, Pulkit Mehndiratta, and Krishna
Asawa. 2013. Stylometric analysis for authorship
attribution on twitter. In Big Data Analytics, pages
37–47. Springer International Publishing.
Julian Brooke and Graeme Hirst. 2012. Measuring
interlanguage: Native language identification with
l1-influence metrics. In LREC, pages 779–784.
Mario Cataldi, Luigi Di Caro, and Claudio Schifanella.
2010. Emerging topic detection on twitter based on
temporal and social terms evaluation. In Proceed-
ings of the Tenth International Workshop on Multi-
media Data Mining, page 4. ACM.
Moshe Koppel, Shlomo Argamon, and Anat Rachel
Shimoni. 2002. Automatically categorizing writ-
ten texts by author gender. Literary and Linguistic
Computing, 17(4):401–412.
Efthymios Kouloumpis, Theresa Wilson, and Johanna
Moore. 2011. Twitter sentiment analysis: The good
the bad and the omg! In ICWSM.
Robert Layton, Paul Watters, and Richard Dazeley.
2010. Authorship attribution for twitter in 140 char-
acters or less. In Cybercrime and Trustworthy Com-
puting Workshop (CTC), 2010 Second, pages 1–8.
IEEE.
Gerard Lynch and Carl Vogel. 2012. Towards the au-
tomatic detection of the source language of a literary
translation. In COLING (Posters), pages 775–784.
Franc¸ois Mairesse, Marilyn A Walker, Matthias R
Mehl, and Roger K Moore. 2007. Using linguis-
tic cues for the automatic recognition of personality
in conversation and text. J. Artif. Intell. Res.(JAIR),
30:457–500.
John O’Donovan, Byungkyu Kang, Greg Meyer, To-
bias Hollerer, and Sibel Adalii. 2012. Credibility in
context: An analysis of feature distributions in twit-
ter. In Privacy, Security, Risk and Trust (PASSAT),
2012 International Conference on and 2012 Inter-
national Confernece on Social Computing (Social-
Com), pages 293–301. IEEE.
Fernando Perez-Tellez, David Pinto, John Cardiff, and
Paolo Rosso. 2011. On the difficulty of cluster-
ing microblog texts for online reputation manage-
ment. In Proceedings of the 2nd Workshop on Com-
putational Approaches to Subjectivity and Sentiment
Analysis, pages 146–152. Association for Computa-
tional Linguistics.
Carolyn Ros´e, Yi-Chia Wang, Yue Cui, Jaime Ar-
guello, Karsten Stegmann, Armin Weinberger, and
Frank Fischer. 2008. Analyzing collaborative
learning processes automatically: Exploiting the ad-
vances of computational linguistics in computer-
supported collaborative learning. International
journal of computer-supported collaborative learn-
ing, 3(3):237–271.
Takeshi Sakaki, Makoto Okazaki, and Yutaka Matsuo.
2010. Earthquake shakes twitter users: real-time
event detection by social sensors. In Proceedings
of the 19th international conference on World wide
web, pages 851–860. ACM.
Damiano Spina, Julio Gonzalo, and Enrique Amig´o.
2013. Discovering filter keywords for company
name disambiguation in twitter. Expert Systems with
Applications.
Hans van Halteren. 2008. Source language mark-
ers in europarl translations. In Proceedings of the
22nd International Conference on Computational
Linguistics-Volume 1, pages 937–944. Association
for Computational Linguistics.
Carl Vogel, Ger Lynch, Erwan Moreau, Liliana Ma-
mani Sanchez, and Phil Ritchie. 2013. Found in
translation: Computational discovery of translation
effects. Translation Spaces, 2(1):81–104.
Jianshu Weng and Bu-Sung Lee. 2011. Event detec-
tion in twitter. In ICWSM.
Surender Reddy Yerva, Zolt´an Mikl´os, and Karl
Aberer. 2011. What have fruits to do with technol-
ogy?: the case of orange, blackberry and apple. In
Proceedings of the International Conference on Web
Intelligence, Mining and Semantics, page 48. ACM.
</reference>
<page confidence="0.99881">
78
</page>
</variant>
</algorithm>
<algorithm name="ParsHed" version="110505">
<variant no="0" confidence="0.000000">
<title confidence="0.995735">Linguistically Informed Tweet Categorization for Online Reputation Management</title>
<author confidence="0.993213">Gerard Lynch</author>
<author confidence="0.993213">P´adraig</author>
<affiliation confidence="0.858349333333333">Centre for Applied Data Analytics University College Belfield Office</affiliation>
<address confidence="0.301983">Dublin 4,</address>
<email confidence="0.978363">firstname.lastname@ucd.ie</email>
<abstract confidence="0.989353724770642">Determining relevant content automatically is a challenging task for any aggregation system. In the business intelligence domain, particularly in the application area of Online Reputation Management, it may be desirable to label tweets as either customer comments which deserve rapid attention or tweets from industry experts or sources regarding the higher-level operations of a particular entity. We present an approach using a combination of linguistic and Twitter-specific features to represent tweets and examine the efficacy of these in distinguishing between tweets which have been labelled using Amazon’s Mechanical Turk crowdsourcing platform. Features such as partof-speech tags and function words prove highly effective at discriminating between the two categories of tweet related to several distinct entity types, with Twitterrelated metrics such as the presence of hashtags, retweets and user mentions also adding to classification accuracy. Accuracy of 86% is reported using an SVM classifier and a mixed set of the aforementioned features on a corpus of tweets related to seven business entities. 1 Motivation Online Reputation Management (ORM) is a growing field of interest in the domain of business intelligence. Companies and individuals alike are highly interested in monitoring the opinions of others across social and traditional media and this information can have considerable business value for corporate entities in particular. 1.1 Challenges There are a number of challenges in creating an end-to-end software solution for such purposes, and several shared tasks have already been estabto tackle these The most recent RepLab evaluation was concerned with four tasks to ORM, for detection Based on these evaluations, it is clear that although the state of the art of topic-based filtering of tweets is relatively accomplished (Perez-Tellez et al., 2011; Yerva et al., 2011; Spina et al., 2013), other aspects of the task such as sentiment analysis and prioritisation of tweets based on content are less trivial and require further analysis. Whether Twitter mentions of entities are actual customer comments or in fact represent the views of traditional media or industry experts and sources is an important distinction for ORM systems. With this study we investigate the degree to which this task can be automated using supervised learning methods. 2 Related Work 2.1 Studies on Twitter data While the majority of research in the computational sciences on Twitter data has focused on issues such as topic detection (Cataldi et al., 2010), event detection, (Weng and Lee, 2011; Sakaki et al., 2010), sentiment analysis, (Kouloumpis et al., 2011), and other tasks based primarily on the topical and/or semantic content of tweets, there is a growing body of work which investigates more subtle forms of information represented in tweets, such as reputation and trustworthiness, (O’Donovan et al., 2012), authorship attribution (Layton et al., 2010; Bhargava et al., 2013) and Twitter spam detection, (Benevenuto et al., 2010). (Amig´o et al., 2012) and (Amig´o et al., 2013) for details of the RepLab series 73 of the 5th Workshop on Computational Approaches to Subjectivity, Sentiment and Social Media pages 73–78, Maryland, USA. June 27, 2014. Association for Computational Linguistics These studies combine Twitter-specific and textual features such as retweet counts, tweet lengths and hashtag frequency, together with sentence-length, character n-grams and punctuation counts. 2.2 Studies on non-Twitter data The textual features used in our work such as n-grams of words and parts-of-speech have been used for gender-based language classification (Koppel et al., 2002), social profiling and personality type detection (Mairesse et al., 2007), native language detection from L2 text, (Brooke and Hirst, 2012) translation source language detection, (van Halteren, 2008; Lynch and Vogel, 2012) and translation quality detection, (Vogel et al., 2013). 3 Experimental setup and corpus Tweets were gathered between June 2013 and Jan- 2014 using the library. A language detector was used to filter only English- The criteria for inclusion were that the entity name was present in the tweet. The entities focused on in this study had relatively unambigious business names, so no complex filtering was necessary. 3.1 Pilot study A smaller pilot study was carried out before the main study in order to examine response quality and accuracy of instruction. Two hundred samtweets concerning two were annotated using Amazon’s Mechanical Turk system by fourteen Master annotators. After annotation, we selected the subset (72%) of tweets for which both annotators agreed on the category to train the classifier. During the pilot study, the tweets were to remove @ and # symbols and punctuation to treat account names and hashtags as words. Hyperlinks representations were maintained within the tweets. The Twitter-specific metrics were not employed in the pilot study. 3.2 Full study In the full study, 2454 tweets concerning seven were tagged by forty annotators as to whether they corresponded to one of the small amount of non-English tweets were found in the these were assigned to the Lingus and Ryanair was not done in the full study, these symbols were counted and used as features. Lingus, Ryanair, Bank of Ireland, C &amp; C Group, Permanent TSB, Glanbia, Greencore three categories described in Section 1.1. For 57% of the tweets, annotators agreed on the categories with disagreement in the remaining 43%. The disputed tweets were annotated again by two annotators. From this batch, a similar proportion were agreed on. For the non-agreed tweets in the second round, a majority category vote was reached by combining the four annotations over the first and second rounds. After this process, roughly two hundred tweets remained as ambiguous (each having two annotations for one of two particular categories) and these were removed from the corpus used in the experiments. 3.3 Category breakdown Table 5 displays the number of tweets for which no majority category agreement was reached. The majority disagreement class across all entities are texts which have been labelled as both business operations and other. For the airline entities, a large proportion of tweets were annotated as both customer comment and other, this appeared to be a categorical issue which may have required clarification in the instructions. The smallest category for tied agreement is customer comment and business operations, it appears that the distinction between these categories was clearer based on the data provided to annotators. 2078 tweets were used in the final experiments. The classes were somewhat imbalanced for the final corpus, operations was the largest, 1184 examples, comments con- 585 examples and the contained 309 examples. 3.4 Feature types The features used for classification purposes can be divided into the following two categories: • Tweet is a retweet or not • Tweet contains a mention • Tweet contains a hashtag or a link • Weight measure (See Fig 3) • Retweet account for a tweet. The linguistic features are based on the textual content of the tweet represented as word unigrams, word bigrams and part-of-speech bigrams. 74 We used TagHelperTools, (Ros´e et al., 2008) for textual feature creation which utilises the Stanford NLP toolkit for NLP annotation and returns formatted representations of textual features which can be employed in the Weka toolkit which implements various machine learning algorithms. All linguistic feature frequencies were binarised in our 4 Results 4.1 Pilot study Using the Naive Bayes classifier in the Weka toolkit and a feature set consisting of 130 word tokens, 80% classification accuracy was obtained using ten-fold cross validation on the full set of tweets . Table 1 shows the top word features when ranked using 10-fold cross validation and the information gain metric for classification power over the three classes. Using the top 50 ranked POSbigram features alone, 74% classification accuracy was obtained using the Naive Bayes classifier. Table 2 shows the top twenty features, again ranked by information gain. Combining the fifty POS-bigrams and the 130 word features, we obtained 84% classification accuracy using the Naive Bayes classifier. Accuracy was improved by removing all noun features from the dataset and using the top seventy five features from the remaining set ranked with information gain, resulting in 86.6% accuracy using the SVM classifier with a linear kernel. Table 3 displays the top twenty combined features. Rank Feature Rank Feature 1 http 11 investors 2 flight 12 would 3 talks 13 by 4 for 14 says 5 strike 15 profit 6 an 16 cabin 7 you 17 crew 8 I 18 via 9 that 19 at 10 action 20 since Table 1: Top 20 ranked word features for pilot study if feature is present in a tweet, otherwise 0.</abstract>
<author confidence="0.900227">Rank Feature Rank Feature</author>
<affiliation confidence="0.645995">1 NNP EOL 11 VB PRP</affiliation>
<address confidence="0.900257555555556">2 VBD JJ 12 NN NNS 3 NNP VBD 13 IN PRP$ 4 NNP NN 14 BOL CD 5 BOL PRP 15 BOL JJS 6 VBD NNP 16 IN VBN 7 NNP CC 17 PRP$ JJ 8 TO NNP 18 PRP MD 9 NN RB 19 PRP$ VBG 10 RB JJ 20 CC VBP</address>
<abstract confidence="0.9000938">Table 2: Top 20 ranked POS bigram features for pilot study Rank Feature Rank Feature 1 http 11 TO NNP 2 NNP EOL 12 RB JJ 3 NNP VBD 13 that 4 VBD JJ 14 tells 5 NNP NN 15 way 6 BOL PRP 16 I 7 VBD NNP 17 would 8 NNP CC 18 you 9 for 19 NN RB 10 an 20 BOL JJS Table 3: Top 20 ranked combined features for pilot study 4.2 Full study 4.2.1 Results Using the SMO classifier, Weka’s support vector machine implementation using a linear kernel, a hybrid feature set containing linguistic, custom and Twitter-specific features obtained 72% classification accuracy for the three categories. Fwere highest for the operations and lowest for the which contained the most diversity. Examining Figure 2, it clear that f-measures for the are almost zero. This indicates that tweets given this category may not be homogeneous enough to categorise using the features defined in Table 7. 4.3 Two classes the removal of the from the experiment, the same feature set obtained 86% classification accuracy between the two remaining classes. The distinguishing features consisted of pronouns me, part-of-</abstract>
<note confidence="0.73197616">75 Entity BO CC Other Aer Lingus 174 138 44 Ryanair 58 212 52 AIB 69 29 43 BOI 208 85 40 C&amp;C 45 14 15 Glanbia 276 39 46 Greencore 37 4 13 Kerry Group 158 10 36 Permanent TSB 160 54 20 Table 4: Tweets per entity by category: Majority agreement Figure 1: F-scores by category for pilot study Entity CC+BO O-CC O-BO Aer Lingus 4 24 15 Ryanair 7 30 8 AIB 4 5 11 BOI 9 5 16 C&amp;C 0 1 3 Glanbia 7 4 19 Greencore 0 0 2 Kerry Group 5 2 12 Permanent TSB 3 6 10 Table 5: Tweets per entity by category: Tied</note>
<abstract confidence="0.933691788135594">agreement speech bigrams including pairs of plural nouns, lines beginning with prepositions and function just, new, Business operations tweets were more likely to mention a user account or be a retweet, personal pronouns were more commonplace in customer comments and as observed in the pilot study, customer comments were more likely to begin with a preposition and business operations tweets were more likely to contain noun-noun compounds and pairs of coordinating conjunctions and nouns. 4.4 Features Hashtags were slightly more common in business operations tweets, however the number of hashtags was not counted, simply whether at least one was present. Hashtags as a proportion of words might be a useful feature for further studies. Function words and POS tags were highly discriminatory, indicating that this classifier may be applicable to different topic areas. Weight (See Figure 3) was a distinguishing feature, with business operatweets having higher weight scores, reflect- Figure 2: F-scores by category for full study ing the tendency for these tweets to originate from Twitter accounts linked to news sources or influential industry experts. 5 Results per sub-category To investigate whether the entity domain had a bearing on the results, we separated the data into three subsets, airlines, banks and food industry concerns. We performed the same feature selection as in previous experiments, calculating each feature type separately, removing proper nouns, hashtags and account names from the word ngrams, then combining and ranking the features using ten-fold cross validation and information gain. The SVM classifier reported similar results to the main study on the three class problem for each sub-domain, and for the two class problem results ranged between 86-87% accuracy, similar Number of followers Number following Figure 3: Twitter weight metric 76 the results on the mixed Thus, we believe that the individual subdomains do not warrant different classifiers for the problem, indeed examining the top 20-ranked features for each subdomain, there is a large degree of overlap, as seen in bold and italics in Table 6. Banks Airlines Food @ @ @ my NNP NNP PRP VBP i i i me BOL IN BOL IN PRP VBP PRP VBP VB PRP account DT NN BOL PRP NNP VBZ IN PRP HASHASH VB PRP the you IN PRP new me you PRP VBD know BOL RB NNP VBZ my RB JJ IN DT i know NNP NNP you PRP CC PRP VBD BOL PRP used my bank ISRT BOL CC DT NN it NNP CD NN PRP me NN NNP VBD PRP my CC PRP BOL IN RB RB ISRT i’m so CC NNP Table 6: Top twenty ranked features by Information Gain for three domains 6 Conclusions and future directions 6.1 Classification results We found that accurate categorization of our predefined tweet types was possible using shallow linguistic features. This was aided by Twitter specific metrics but these did not add significantly to classification The lower score (72- 73%) in the three class categorization problem is to the linguistic diversity of the category. 6.2 Annotation and Mechanical Turk We found the definition of categorization criteria to be an important and challenging step when using Mechanical Turk for annotation. The high degree of annotator disagreement reflected this, however it is important to note that in many cases, tweets fit equally into two or more of our defined The use of extra allowed for agreement to be reached in the majority of food subset was highly imbalanced however, containing only 43 customer comments and 313 business operations tweets, the other two subsets were relatively balanced. 2% decrease in accuracy on removal. the initial two annotators cases, however employing more evaluations could have also resulted in deadlock. Examples of amtweets included: marketing tactics. Well, if it ain’t broke, why fix it! RT @Ryanair’s ’14 schedule is now on sale! a Twitter user has retweeted an official announcement and added their own comment. Another possible pitfall is that as Mechanical Turk is a US-based service and requires workers to have a US bank account in order to perform work, Turkers tend to be US-based, and therefore an annotation task concerning non-US business entities is perhaps more difficult without sufficient background awareness of the entities in question. Future experiments will apply the methodology developed here to a larger dataset of tweets, one candidate would be the dataset used in the RepLab 2013 evaluation series which contains 2,200 annotated tweets for 61 business entities in four domains.</abstract>
<note confidence="0.797031476190476">Acknowledgments The authors are grateful to Enterprise Ireland and the IDA for funding this research and CeADAR through their Technology Centre Programme. Rank Feature Rank Feature 1 @ 26 NNP PRP 2 i 27 NN PRP 3 PRP VBP 28 VBP PRP 4 my 29 when 5 BOL IN 30 if 6 me 31 don’t 7 you 32 PRP MD 8 NNP NNP 33 they 9 IN PRP 34 like 10 VB PRP 35 PRP VB 11 PRP VBD 36 got 12 WEIGHT 37 CC NNP 13 so 38 but 14 NNP VBZ 39 RB IN 15 BOL PRP 40 RT 16 RB JJ 41 with</note>
<address confidence="0.881324666666667">17 DT NN 42 PRP IN 18 BOL RB 43 a 19 it 44 NNS RB 20 PRP RB 45 CC PRP 21 RB RB 46 VBD PRP 22 IN DT 47 VBD DT</address>
<note confidence="0.520559">23 i’m 48 no 24 just 49 the 25 get 50 PRP$ NN Table 7: Top 50 ranked mixed features for main study 77 References Enrique Amig´o, Adolfo Corujo, Julio Gonzalo, Edgar Meij, and Maarten de Rijke. 2012. Overview of replab 2012: Evaluating online reputation man-</note>
<title confidence="0.803094">systems. In (Online Working</title>
<author confidence="0.641197666666667">Enrique Amig´o</author>
<author confidence="0.641197666666667">Jorge Carrillo de_Albornoz</author>
<author confidence="0.641197666666667">Irina Chugur</author>
<author confidence="0.641197666666667">Adolfo Corujo</author>
<author confidence="0.641197666666667">Julio Gonzalo</author>
<author confidence="0.641197666666667">Tamara Martin</author>
<author confidence="0.641197666666667">Edgar Meij</author>
<author confidence="0.641197666666667">Maarten de_Rijke</author>
<author confidence="0.641197666666667">Dami-</author>
<abstract confidence="0.925182409090909">ano Spina. 2013. Overview of replab 2013: Evaluating online reputation monitoring systems. Access Evaluation. Multilinguality, and pages 333–352. Springer. Fabrıcio Benevenuto, Gabriel Magno, Tiago Rodrigues, and Virgılio Almeida. 2010. Detectspammers on twitter. In electronic messaging, anti-abuse and spam conference volume 6. Mudit Bhargava, Pulkit Mehndiratta, and Krishna Asawa. 2013. Stylometric analysis for authorship on twitter. In Data pages 37–47. Springer International Publishing. Julian Brooke and Graeme Hirst. 2012. Measuring interlanguage: Native language identification with metrics. In pages 779–784. Mario Cataldi, Luigi Di Caro, and Claudio Schifanella. 2010. Emerging topic detection on twitter based on and social terms evaluation. In Proceedings of the Tenth International Workshop on Multi- Data page 4. ACM.</abstract>
<note confidence="0.711947692307692">Moshe Koppel, Shlomo Argamon, and Anat Rachel Shimoni. 2002. Automatically categorizing writtexts by author gender. and Linguistic 17(4):401–412. Efthymios Kouloumpis, Theresa Wilson, and Johanna Moore. 2011. Twitter sentiment analysis: The good bad and the omg! In Robert Layton, Paul Watters, and Richard Dazeley. 2010. Authorship attribution for twitter in 140 charor less. In and Trustworthy Com- Workshop (CTC), 2010 pages 1–8. IEEE. Gerard Lynch and Carl Vogel. 2012. Towards the au-</note>
<abstract confidence="0.534359166666667">tomatic detection of the source language of a literary In pages 775–784. Mairesse, Marilyn A Walker, Matthias R Mehl, and Roger K Moore. 2007. Using linguistic cues for the automatic recognition of personality conversation and text. Artif. Intell.</abstract>
<note confidence="0.789693466666667">30:457–500. John O’Donovan, Byungkyu Kang, Greg Meyer, Tobias Hollerer, and Sibel Adalii. 2012. Credibility in context: An analysis of feature distributions in twit- In Security, Risk and Trust (PASSAT), 2012 International Conference on and 2012 International Confernece on Social Computing (Socialpages 293–301. IEEE. Fernando Perez-Tellez, David Pinto, John Cardiff, and Paolo Rosso. 2011. On the difficulty of clustering microblog texts for online reputation manage- In of the 2nd Workshop on Computational Approaches to Subjectivity and Sentiment pages 146–152. Association for Computational Linguistics.</note>
<author confidence="0.527794333333333">Analyzing collaborative</author>
<abstract confidence="0.728366931034483">learning processes automatically: Exploiting the adof computational linguistics in computercollaborative learning. journal of computer-supported collaborative learn- 3(3):237–271. Takeshi Sakaki, Makoto Okazaki, and Yutaka Matsuo. 2010. Earthquake shakes twitter users: real-time detection by social sensors. In of the 19th international conference on World wide pages 851–860. ACM. Damiano Spina, Julio Gonzalo, and Enrique Amig´o. 2013. Discovering filter keywords for company disambiguation in twitter. Systems with Hans van Halteren. 2008. Source language markin europarl translations. In of the 22nd International Conference on Computational pages 937–944. Association for Computational Linguistics. Carl Vogel, Ger Lynch, Erwan Moreau, Liliana Mamani Sanchez, and Phil Ritchie. 2013. Found in translation: Computational discovery of translation 2(1):81–104. Jianshu Weng and Bu-Sung Lee. 2011. Event detecin twitter. In Surender Reddy Yerva, Zolt´an Mikl´os, and Karl Aberer. 2011. What have fruits to do with technology?: the case of orange, blackberry and apple. In Proceedings of the International Conference on Web Mining and page 48. ACM.</abstract>
<intro confidence="0.383302">78</intro>
</variant>
</algorithm>
<algorithm name="ParsCit" version="110505">
<citationList>
<citation valid="true">
<authors>
<author>Enrique Amig´o</author>
<author>Adolfo Corujo</author>
<author>Julio Gonzalo</author>
<author>Edgar Meij</author>
<author>Maarten de Rijke</author>
</authors>
<title>Overview of replab 2012: Evaluating online reputation management systems.</title>
<date>2012</date>
<booktitle>In CLEF (Online Working Notes/Labs/Workshop).</booktitle>
<marker>Amig´o, Corujo, Gonzalo, Meij, de Rijke, 2012</marker>
<rawString>Enrique Amig´o, Adolfo Corujo, Julio Gonzalo, Edgar Meij, and Maarten de Rijke. 2012. Overview of replab 2012: Evaluating online reputation management systems. In CLEF (Online Working Notes/Labs/Workshop).</rawString>
</citation>
<citation valid="true">
<authors>
<author>Enrique Amig´o</author>
<author>Jorge Carrillo de Albornoz</author>
<author>Irina Chugur</author>
<author>Adolfo Corujo</author>
<author>Julio Gonzalo</author>
<author>Tamara Martin</author>
<author>Edgar Meij</author>
<author>Maarten de Rijke</author>
<author>Damiano Spina</author>
</authors>
<title>Overview of replab 2013: Evaluating online reputation monitoring systems.</title>
<date>2013</date>
<booktitle>In Information Access Evaluation. Multilinguality, Multimodality, and Visualization,</booktitle>
<pages>333--352</pages>
<publisher>Springer.</publisher>
<marker>Amig´o, de Albornoz, Chugur, Corujo, Gonzalo, Martin, Meij, de Rijke, Spina, 2013</marker>
<rawString>Enrique Amig´o, Jorge Carrillo de Albornoz, Irina Chugur, Adolfo Corujo, Julio Gonzalo, Tamara Martin, Edgar Meij, Maarten de Rijke, and Damiano Spina. 2013. Overview of replab 2013: Evaluating online reputation monitoring systems. In Information Access Evaluation. Multilinguality, Multimodality, and Visualization, pages 333–352. Springer.</rawString>
</citation>
<citation valid="true">
<authors>
<author>Fabrıcio Benevenuto</author>
<author>Gabriel Magno</author>
<author>Tiago Rodrigues</author>
<author>Virgılio Almeida</author>
</authors>
<title>Detecting spammers on twitter.</title>
<date>2010</date>
<booktitle>In Collaboration, electronic messaging, anti-abuse and spam conference (CEAS),</booktitle>
<volume>6</volume>
<contexts>
<context position="3403" citStr="Benevenuto et al., 2010" startWordPosition="523" endWordPosition="526">the majority of research in the computational sciences on Twitter data has focused on issues such as topic detection (Cataldi et al., 2010), event detection, (Weng and Lee, 2011; Sakaki et al., 2010), sentiment analysis, (Kouloumpis et al., 2011), and other tasks based primarily on the topical and/or semantic content of tweets, there is a growing body of work which investigates more subtle forms of information represented in tweets, such as reputation and trustworthiness, (O’Donovan et al., 2012), authorship attribution (Layton et al., 2010; Bhargava et al., 2013) and Twitter spam detection, (Benevenuto et al., 2010). 1See (Amig´o et al., 2012) and (Amig´o et al., 2013) for details of the RepLab series 73 Proceedings of the 5th Workshop on Computational Approaches to Subjectivity, Sentiment and Social Media Analysis, pages 73–78, Baltimore, Maryland, USA. June 27, 2014. c�2014 Association for Computational Linguistics These studies combine Twitter-specific and textual features such as retweet counts, tweet lengths and hashtag frequency, together with sentence-length, character n-grams and punctuation counts. 2.2 Studies on non-Twitter data The textual features used in our work such as n-grams of words and</context>
</contexts>
<marker>Benevenuto, Magno, Rodrigues, Almeida, 2010</marker>
<rawString>Fabrıcio Benevenuto, Gabriel Magno, Tiago Rodrigues, and Virgılio Almeida. 2010. Detecting spammers on twitter. In Collaboration, electronic messaging, anti-abuse and spam conference (CEAS), volume 6.</rawString>
</citation>
<citation valid="true">
<authors>
<author>Mudit Bhargava</author>
<author>Pulkit Mehndiratta</author>
<author>Krishna Asawa</author>
</authors>
<title>Stylometric analysis for authorship attribution on twitter.</title>
<date>2013</date>
<booktitle>In Big Data Analytics,</booktitle>
<pages>37--47</pages>
<publisher>Springer International Publishing.</publisher>
<contexts>
<context position="3349" citStr="Bhargava et al., 2013" startWordPosition="515" endWordPosition="518">s. 2 Related Work 2.1 Studies on Twitter data While the majority of research in the computational sciences on Twitter data has focused on issues such as topic detection (Cataldi et al., 2010), event detection, (Weng and Lee, 2011; Sakaki et al., 2010), sentiment analysis, (Kouloumpis et al., 2011), and other tasks based primarily on the topical and/or semantic content of tweets, there is a growing body of work which investigates more subtle forms of information represented in tweets, such as reputation and trustworthiness, (O’Donovan et al., 2012), authorship attribution (Layton et al., 2010; Bhargava et al., 2013) and Twitter spam detection, (Benevenuto et al., 2010). 1See (Amig´o et al., 2012) and (Amig´o et al., 2013) for details of the RepLab series 73 Proceedings of the 5th Workshop on Computational Approaches to Subjectivity, Sentiment and Social Media Analysis, pages 73–78, Baltimore, Maryland, USA. June 27, 2014. c�2014 Association for Computational Linguistics These studies combine Twitter-specific and textual features such as retweet counts, tweet lengths and hashtag frequency, together with sentence-length, character n-grams and punctuation counts. 2.2 Studies on non-Twitter data The textual </context>
</contexts>
<marker>Bhargava, Mehndiratta, Asawa, 2013</marker>
<rawString>Mudit Bhargava, Pulkit Mehndiratta, and Krishna Asawa. 2013. Stylometric analysis for authorship attribution on twitter. In Big Data Analytics, pages 37–47. Springer International Publishing.</rawString>
</citation>
<citation valid="true">
<authors>
<author>Julian Brooke</author>
<author>Graeme Hirst</author>
</authors>
<title>Measuring interlanguage: Native language identification with l1-influence metrics.</title>
<date>2012</date>
<booktitle>In LREC,</booktitle>
<pages>779--784</pages>
<contexts>
<context position="4236" citStr="Brooke and Hirst, 2012" startWordPosition="645" endWordPosition="648"> 73–78, Baltimore, Maryland, USA. June 27, 2014. c�2014 Association for Computational Linguistics These studies combine Twitter-specific and textual features such as retweet counts, tweet lengths and hashtag frequency, together with sentence-length, character n-grams and punctuation counts. 2.2 Studies on non-Twitter data The textual features used in our work such as n-grams of words and parts-of-speech have been used for gender-based language classification (Koppel et al., 2002), social profiling and personality type detection (Mairesse et al., 2007), native language detection from L2 text, (Brooke and Hirst, 2012) translation source language detection, (van Halteren, 2008; Lynch and Vogel, 2012) and translation quality detection, (Vogel et al., 2013). 3 Experimental setup and corpus Tweets were gathered between June 2013 and January 2014 using the twitter4j Java library. A language detector was used to filter only Englishlanguage tweets.2 The criteria for inclusion were that the entity name was present in the tweet. The entities focused on in this study had relatively unambigious business names, so no complex filtering was necessary. 3.1 Pilot study A smaller pilot study was carried out before the main</context>
</contexts>
<marker>Brooke, Hirst, 2012</marker>
<rawString>Julian Brooke and Graeme Hirst. 2012. Measuring interlanguage: Native language identification with l1-influence metrics. In LREC, pages 779–784.</rawString>
</citation>
<citation valid="true">
<authors>
<author>Mario Cataldi</author>
<author>Luigi Di Caro</author>
<author>Claudio Schifanella</author>
</authors>
<title>Emerging topic detection on twitter based on temporal and social terms evaluation.</title>
<date>2010</date>
<booktitle>In Proceedings of the Tenth International Workshop on Multimedia Data Mining,</booktitle>
<pages>4</pages>
<publisher>ACM.</publisher>
<marker>Cataldi, Di Caro, Schifanella, 2010</marker>
<rawString>Mario Cataldi, Luigi Di Caro, and Claudio Schifanella. 2010. Emerging topic detection on twitter based on temporal and social terms evaluation. In Proceedings of the Tenth International Workshop on Multimedia Data Mining, page 4. ACM.</rawString>
</citation>
<citation valid="true">
<authors>
<author>Moshe Koppel</author>
</authors>
<title>Shlomo Argamon, and Anat Rachel Shimoni.</title>
<date>2002</date>
<booktitle>Literary and Linguistic Computing,</booktitle>
<pages>17--4</pages>
<marker>Koppel, 2002</marker>
<rawString>Moshe Koppel, Shlomo Argamon, and Anat Rachel Shimoni. 2002. Automatically categorizing written texts by author gender. Literary and Linguistic Computing, 17(4):401–412.</rawString>
</citation>
<citation valid="true">
<authors>
<author>Efthymios Kouloumpis</author>
<author>Theresa Wilson</author>
<author>Johanna Moore</author>
</authors>
<title>Twitter sentiment analysis: The good the bad and the omg!</title>
<date>2011</date>
<booktitle>In ICWSM.</booktitle>
<contexts>
<context position="3025" citStr="Kouloumpis et al., 2011" startWordPosition="466" endWordPosition="469">re further analysis. Whether Twitter mentions of entities are actual customer comments or in fact represent the views of traditional media or industry experts and sources is an important distinction for ORM systems. With this study we investigate the degree to which this task can be automated using supervised learning methods. 2 Related Work 2.1 Studies on Twitter data While the majority of research in the computational sciences on Twitter data has focused on issues such as topic detection (Cataldi et al., 2010), event detection, (Weng and Lee, 2011; Sakaki et al., 2010), sentiment analysis, (Kouloumpis et al., 2011), and other tasks based primarily on the topical and/or semantic content of tweets, there is a growing body of work which investigates more subtle forms of information represented in tweets, such as reputation and trustworthiness, (O’Donovan et al., 2012), authorship attribution (Layton et al., 2010; Bhargava et al., 2013) and Twitter spam detection, (Benevenuto et al., 2010). 1See (Amig´o et al., 2012) and (Amig´o et al., 2013) for details of the RepLab series 73 Proceedings of the 5th Workshop on Computational Approaches to Subjectivity, Sentiment and Social Media Analysis, pages 73–78, Balt</context>
</contexts>
<marker>Kouloumpis, Wilson, Moore, 2011</marker>
<rawString>Efthymios Kouloumpis, Theresa Wilson, and Johanna Moore. 2011. Twitter sentiment analysis: The good the bad and the omg! In ICWSM.</rawString>
</citation>
<citation valid="true">
<authors>
<author>Robert Layton</author>
<author>Paul Watters</author>
<author>Richard Dazeley</author>
</authors>
<title>Authorship attribution for twitter in 140 characters or less.</title>
<date>2010</date>
<booktitle>In Cybercrime and Trustworthy Computing Workshop (CTC), 2010 Second,</booktitle>
<pages>1--8</pages>
<publisher>IEEE.</publisher>
<contexts>
<context position="3325" citStr="Layton et al., 2010" startWordPosition="511" endWordPosition="514">vised learning methods. 2 Related Work 2.1 Studies on Twitter data While the majority of research in the computational sciences on Twitter data has focused on issues such as topic detection (Cataldi et al., 2010), event detection, (Weng and Lee, 2011; Sakaki et al., 2010), sentiment analysis, (Kouloumpis et al., 2011), and other tasks based primarily on the topical and/or semantic content of tweets, there is a growing body of work which investigates more subtle forms of information represented in tweets, such as reputation and trustworthiness, (O’Donovan et al., 2012), authorship attribution (Layton et al., 2010; Bhargava et al., 2013) and Twitter spam detection, (Benevenuto et al., 2010). 1See (Amig´o et al., 2012) and (Amig´o et al., 2013) for details of the RepLab series 73 Proceedings of the 5th Workshop on Computational Approaches to Subjectivity, Sentiment and Social Media Analysis, pages 73–78, Baltimore, Maryland, USA. June 27, 2014. c�2014 Association for Computational Linguistics These studies combine Twitter-specific and textual features such as retweet counts, tweet lengths and hashtag frequency, together with sentence-length, character n-grams and punctuation counts. 2.2 Studies on non-T</context>
</contexts>
<marker>Layton, Watters, Dazeley, 2010</marker>
<rawString>Robert Layton, Paul Watters, and Richard Dazeley. 2010. Authorship attribution for twitter in 140 characters or less. In Cybercrime and Trustworthy Computing Workshop (CTC), 2010 Second, pages 1–8. IEEE.</rawString>
</citation>
<citation valid="true">
<authors>
<author>Gerard Lynch</author>
<author>Carl Vogel</author>
</authors>
<title>Towards the automatic detection of the source language of a literary translation.</title>
<date>2012</date>
<booktitle>In COLING (Posters),</booktitle>
<pages>775--784</pages>
<contexts>
<context position="4319" citStr="Lynch and Vogel, 2012" startWordPosition="656" endWordPosition="659">l Linguistics These studies combine Twitter-specific and textual features such as retweet counts, tweet lengths and hashtag frequency, together with sentence-length, character n-grams and punctuation counts. 2.2 Studies on non-Twitter data The textual features used in our work such as n-grams of words and parts-of-speech have been used for gender-based language classification (Koppel et al., 2002), social profiling and personality type detection (Mairesse et al., 2007), native language detection from L2 text, (Brooke and Hirst, 2012) translation source language detection, (van Halteren, 2008; Lynch and Vogel, 2012) and translation quality detection, (Vogel et al., 2013). 3 Experimental setup and corpus Tweets were gathered between June 2013 and January 2014 using the twitter4j Java library. A language detector was used to filter only Englishlanguage tweets.2 The criteria for inclusion were that the entity name was present in the tweet. The entities focused on in this study had relatively unambigious business names, so no complex filtering was necessary. 3.1 Pilot study A smaller pilot study was carried out before the main study in order to examine response quality and accuracy of instruction. Two hundre</context>
</contexts>
<marker>Lynch, Vogel, 2012</marker>
<rawString>Gerard Lynch and Carl Vogel. 2012. Towards the automatic detection of the source language of a literary translation. In COLING (Posters), pages 775–784.</rawString>
</citation>
<citation valid="true">
<authors>
<author>Franc¸ois Mairesse</author>
<author>Marilyn A Walker</author>
<author>Matthias R Mehl</author>
<author>Roger K Moore</author>
</authors>
<title>Using linguistic cues for the automatic recognition of personality in conversation and text.</title>
<date>2007</date>
<journal>J. Artif. Intell. Res.(JAIR),</journal>
<pages>30--457</pages>
<contexts>
<context position="4170" citStr="Mairesse et al., 2007" startWordPosition="634" endWordPosition="637">aches to Subjectivity, Sentiment and Social Media Analysis, pages 73–78, Baltimore, Maryland, USA. June 27, 2014. c�2014 Association for Computational Linguistics These studies combine Twitter-specific and textual features such as retweet counts, tweet lengths and hashtag frequency, together with sentence-length, character n-grams and punctuation counts. 2.2 Studies on non-Twitter data The textual features used in our work such as n-grams of words and parts-of-speech have been used for gender-based language classification (Koppel et al., 2002), social profiling and personality type detection (Mairesse et al., 2007), native language detection from L2 text, (Brooke and Hirst, 2012) translation source language detection, (van Halteren, 2008; Lynch and Vogel, 2012) and translation quality detection, (Vogel et al., 2013). 3 Experimental setup and corpus Tweets were gathered between June 2013 and January 2014 using the twitter4j Java library. A language detector was used to filter only Englishlanguage tweets.2 The criteria for inclusion were that the entity name was present in the tweet. The entities focused on in this study had relatively unambigious business names, so no complex filtering was necessary. 3.1</context>
</contexts>
<marker>Mairesse, Walker, Mehl, Moore, 2007</marker>
<rawString>Franc¸ois Mairesse, Marilyn A Walker, Matthias R Mehl, and Roger K Moore. 2007. Using linguistic cues for the automatic recognition of personality in conversation and text. J. Artif. Intell. Res.(JAIR), 30:457–500.</rawString>
</citation>
<citation valid="true">
<authors>
<author>John O’Donovan</author>
<author>Byungkyu Kang</author>
<author>Greg Meyer</author>
<author>Tobias Hollerer</author>
<author>Sibel Adalii</author>
</authors>
<title>Credibility in context: An analysis of feature distributions in twitter.</title>
<date>2012</date>
<booktitle>In Privacy, Security, Risk and Trust (PASSAT), 2012 International Conference on and 2012 International Confernece on Social Computing (SocialCom),</booktitle>
<pages>293--301</pages>
<publisher>IEEE.</publisher>
<marker>O’Donovan, Kang, Meyer, Hollerer, Adalii, 2012</marker>
<rawString>John O’Donovan, Byungkyu Kang, Greg Meyer, Tobias Hollerer, and Sibel Adalii. 2012. Credibility in context: An analysis of feature distributions in twitter. In Privacy, Security, Risk and Trust (PASSAT), 2012 International Conference on and 2012 International Confernece on Social Computing (SocialCom), pages 293–301. IEEE.</rawString>
</citation>
<citation valid="true">
<authors>
<author>Fernando Perez-Tellez</author>
<author>David Pinto</author>
<author>John Cardiff</author>
<author>Paolo Rosso</author>
</authors>
<title>On the difficulty of clustering microblog texts for online reputation management.</title>
<date>2011</date>
<booktitle>In Proceedings of the 2nd Workshop on Computational Approaches to Subjectivity and Sentiment Analysis,</booktitle>
<pages>146--152</pages>
<publisher>Association</publisher>
<institution>for Computational Linguistics.</institution>
<contexts>
<context position="2233" citStr="Perez-Tellez et al., 2011" startWordPosition="334" endWordPosition="337">traditional media and this information can have considerable business value for corporate entities in particular. 1.1 Challenges There are a number of challenges in creating an end-to-end software solution for such purposes, and several shared tasks have already been established to tackle these issues1. The most recent RepLab evaluation was concerned with four tasks related to ORM, filtering, polarity for reputation, topic detection and priority assignment. Based on these evaluations, it is clear that although the state of the art of topic-based filtering of tweets is relatively accomplished (Perez-Tellez et al., 2011; Yerva et al., 2011; Spina et al., 2013), other aspects of the task such as sentiment analysis and prioritisation of tweets based on content are less trivial and require further analysis. Whether Twitter mentions of entities are actual customer comments or in fact represent the views of traditional media or industry experts and sources is an important distinction for ORM systems. With this study we investigate the degree to which this task can be automated using supervised learning methods. 2 Related Work 2.1 Studies on Twitter data While the majority of research in the computational sciences</context>
</contexts>
<marker>Perez-Tellez, Pinto, Cardiff, Rosso, 2011</marker>
<rawString>Fernando Perez-Tellez, David Pinto, John Cardiff, and Paolo Rosso. 2011. On the difficulty of clustering microblog texts for online reputation management. In Proceedings of the 2nd Workshop on Computational Approaches to Subjectivity and Sentiment Analysis, pages 146–152. Association for Computational Linguistics.</rawString>
</citation>
<citation valid="true">
<authors>
<author>Carolyn Ros´e</author>
<author>Yi-Chia Wang</author>
<author>Yue Cui</author>
<author>Jaime Arguello</author>
<author>Karsten Stegmann</author>
<author>Armin Weinberger</author>
<author>Frank Fischer</author>
</authors>
<title>Analyzing collaborative learning processes automatically: Exploiting the advances of computational linguistics in computersupported collaborative learning. International journal of computer-supported collaborative learning,</title>
<date>2008</date>
<pages>3--3</pages>
<marker>Ros´e, Wang, Cui, Arguello, Stegmann, Weinberger, Fischer, 2008</marker>
<rawString>Carolyn Ros´e, Yi-Chia Wang, Yue Cui, Jaime Arguello, Karsten Stegmann, Armin Weinberger, and Frank Fischer. 2008. Analyzing collaborative learning processes automatically: Exploiting the advances of computational linguistics in computersupported collaborative learning. International journal of computer-supported collaborative learning, 3(3):237–271.</rawString>
</citation>
<citation valid="true">
<authors>
<author>Takeshi Sakaki</author>
<author>Makoto Okazaki</author>
<author>Yutaka Matsuo</author>
</authors>
<title>Earthquake shakes twitter users: real-time event detection by social sensors.</title>
<date>2010</date>
<booktitle>In Proceedings of the 19th international conference on World wide web,</booktitle>
<pages>851--860</pages>
<publisher>ACM.</publisher>
<contexts>
<context position="2978" citStr="Sakaki et al., 2010" startWordPosition="460" endWordPosition="463">based on content are less trivial and require further analysis. Whether Twitter mentions of entities are actual customer comments or in fact represent the views of traditional media or industry experts and sources is an important distinction for ORM systems. With this study we investigate the degree to which this task can be automated using supervised learning methods. 2 Related Work 2.1 Studies on Twitter data While the majority of research in the computational sciences on Twitter data has focused on issues such as topic detection (Cataldi et al., 2010), event detection, (Weng and Lee, 2011; Sakaki et al., 2010), sentiment analysis, (Kouloumpis et al., 2011), and other tasks based primarily on the topical and/or semantic content of tweets, there is a growing body of work which investigates more subtle forms of information represented in tweets, such as reputation and trustworthiness, (O’Donovan et al., 2012), authorship attribution (Layton et al., 2010; Bhargava et al., 2013) and Twitter spam detection, (Benevenuto et al., 2010). 1See (Amig´o et al., 2012) and (Amig´o et al., 2013) for details of the RepLab series 73 Proceedings of the 5th Workshop on Computational Approaches to Subjectivity, Sentime</context>
</contexts>
<marker>Sakaki, Okazaki, Matsuo, 2010</marker>
<rawString>Takeshi Sakaki, Makoto Okazaki, and Yutaka Matsuo. 2010. Earthquake shakes twitter users: real-time event detection by social sensors. In Proceedings of the 19th international conference on World wide web, pages 851–860. ACM.</rawString>
</citation>
<citation valid="true">
<authors>
<author>Damiano Spina</author>
<author>Julio Gonzalo</author>
<author>Enrique Amig´o</author>
</authors>
<title>Discovering filter keywords for company name disambiguation in twitter. Expert Systems with Applications.</title>
<date>2013</date>
<marker>Spina, Gonzalo, Amig´o, 2013</marker>
<rawString>Damiano Spina, Julio Gonzalo, and Enrique Amig´o. 2013. Discovering filter keywords for company name disambiguation in twitter. Expert Systems with Applications.</rawString>
</citation>
<citation valid="true">
<authors>
<author>Hans van Halteren</author>
</authors>
<title>Source language markers in europarl translations.</title>
<date>2008</date>
<booktitle>In Proceedings of the 22nd International Conference on Computational Linguistics-Volume 1,</booktitle>
<pages>937--944</pages>
<publisher>Association</publisher>
<institution>for Computational Linguistics.</institution>
<marker>van Halteren, 2008</marker>
<rawString>Hans van Halteren. 2008. Source language markers in europarl translations. In Proceedings of the 22nd International Conference on Computational Linguistics-Volume 1, pages 937–944. Association for Computational Linguistics.</rawString>
</citation>
<citation valid="true">
<authors>
<author>Carl Vogel</author>
<author>Ger Lynch</author>
<author>Erwan Moreau</author>
<author>Liliana Mamani Sanchez</author>
<author>Phil Ritchie</author>
</authors>
<title>Found in translation: Computational discovery of translation effects.</title>
<date>2013</date>
<journal>Translation Spaces,</journal>
<volume>2</volume>
<issue>1</issue>
<contexts>
<context position="4375" citStr="Vogel et al., 2013" startWordPosition="664" endWordPosition="667">xtual features such as retweet counts, tweet lengths and hashtag frequency, together with sentence-length, character n-grams and punctuation counts. 2.2 Studies on non-Twitter data The textual features used in our work such as n-grams of words and parts-of-speech have been used for gender-based language classification (Koppel et al., 2002), social profiling and personality type detection (Mairesse et al., 2007), native language detection from L2 text, (Brooke and Hirst, 2012) translation source language detection, (van Halteren, 2008; Lynch and Vogel, 2012) and translation quality detection, (Vogel et al., 2013). 3 Experimental setup and corpus Tweets were gathered between June 2013 and January 2014 using the twitter4j Java library. A language detector was used to filter only Englishlanguage tweets.2 The criteria for inclusion were that the entity name was present in the tweet. The entities focused on in this study had relatively unambigious business names, so no complex filtering was necessary. 3.1 Pilot study A smaller pilot study was carried out before the main study in order to examine response quality and accuracy of instruction. Two hundred sample tweets concerning two airlines3 were annotated </context>
</contexts>
<marker>Vogel, Lynch, Moreau, Sanchez, Ritchie, 2013</marker>
<rawString>Carl Vogel, Ger Lynch, Erwan Moreau, Liliana Mamani Sanchez, and Phil Ritchie. 2013. Found in translation: Computational discovery of translation effects. Translation Spaces, 2(1):81–104.</rawString>
</citation>
<citation valid="true">
<authors>
<author>Jianshu Weng</author>
<author>Bu-Sung Lee</author>
</authors>
<title>Event detection in twitter.</title>
<date>2011</date>
<booktitle>In ICWSM.</booktitle>
<contexts>
<context position="2956" citStr="Weng and Lee, 2011" startWordPosition="456" endWordPosition="459">itisation of tweets based on content are less trivial and require further analysis. Whether Twitter mentions of entities are actual customer comments or in fact represent the views of traditional media or industry experts and sources is an important distinction for ORM systems. With this study we investigate the degree to which this task can be automated using supervised learning methods. 2 Related Work 2.1 Studies on Twitter data While the majority of research in the computational sciences on Twitter data has focused on issues such as topic detection (Cataldi et al., 2010), event detection, (Weng and Lee, 2011; Sakaki et al., 2010), sentiment analysis, (Kouloumpis et al., 2011), and other tasks based primarily on the topical and/or semantic content of tweets, there is a growing body of work which investigates more subtle forms of information represented in tweets, such as reputation and trustworthiness, (O’Donovan et al., 2012), authorship attribution (Layton et al., 2010; Bhargava et al., 2013) and Twitter spam detection, (Benevenuto et al., 2010). 1See (Amig´o et al., 2012) and (Amig´o et al., 2013) for details of the RepLab series 73 Proceedings of the 5th Workshop on Computational Approaches to</context>
</contexts>
<marker>Weng, Lee, 2011</marker>
<rawString>Jianshu Weng and Bu-Sung Lee. 2011. Event detection in twitter. In ICWSM.</rawString>
</citation>
<citation valid="true">
<authors>
<author>Surender Reddy Yerva</author>
<author>Zolt´an Mikl´os</author>
<author>Karl Aberer</author>
</authors>
<title>What have fruits to do with technology?: the case of orange, blackberry and apple.</title>
<date>2011</date>
<booktitle>In Proceedings of the International Conference on Web Intelligence, Mining and Semantics,</booktitle>
<pages>48</pages>
<publisher>ACM.</publisher>
<marker>Yerva, Mikl´os, Aberer, 2011</marker>
<rawString>Surender Reddy Yerva, Zolt´an Mikl´os, and Karl Aberer. 2011. What have fruits to do with technology?: the case of orange, blackberry and apple. In Proceedings of the International Conference on Web Intelligence, Mining and Semantics, page 48. ACM.</rawString>
</citation>
</citationList>
</algorithm>
</algorithms>
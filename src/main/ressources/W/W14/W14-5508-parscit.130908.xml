<?xml version="1.0" encoding="UTF-8"?>
<algorithms version="110505">
<algorithm name="SectLabel" version="110505">
<variant no="0" confidence="0.000003">
<title confidence="0.99333">
Developing an interlingual translation lexicon using WordNets
and Grammatical Framework
</title>
<author confidence="0.994774">
Shafqat Mumtaz Virk
</author>
<affiliation confidence="0.9974925">
University of Gothenburg,
University of Eng. &amp; Tech. Lahore
</affiliation>
<email confidence="0.971394">
virk.shafqat@gmail.com
</email>
<author confidence="0.995298">
Aarne Ranta
</author>
<affiliation confidence="0.99803">
University of Gothenburg
</affiliation>
<email confidence="0.991877">
aarne@chalmers.se
</email>
<author confidence="0.980818">
K.V.S. Prasad
</author>
<affiliation confidence="0.992689">
Chalmers University of Technology
</affiliation>
<email confidence="0.956785">
prasad@chalmers.se
</email>
<author confidence="0.989055">
Krasimir Angelov
</author>
<affiliation confidence="0.997521">
University of Gothenburg
</affiliation>
<email confidence="0.994632">
krasimir@chalmers.se
</email>
<sectionHeader confidence="0.979731" genericHeader="abstract">
Abstract
</sectionHeader>
<bodyText confidence="0.999835875">
The Grammatical Framework (GF) offers perfect translation between controlled subsets
of natural languages. E.g., an abstract syntax for a set of sentences in school mathematics
is the interlingua between the corresponding sentences in English and Hindi, say. GF
“resource grammars” specify how to say something in English or Hindi; these are re-
used with “application grammars” that specify what can be said (mathematics, tourist
phrases, etc.). More recent robust parsing and parse-tree disambiguation allow GF to
parse arbitrary English text. We report here an experiment to linearise the resulting
tree directly to other languages (e.g. Hindi, German, etc.), i.e., we use a language-
independent resource grammar as the interlingua. We focus particularly on the last part
of the translation system, the interlingual lexicon and word sense disambiguation (WSD).
We improved the quality of the wide coverage interlingual translation lexicon by using
the Princeton and Universal WordNet data. We then integrated an existing WSD tool
and replaced the usual GF style lexicons, which give one target word per source word,
by the WordNet based lexicons. These new lexicons and WSD improve the quality of
translation in most cases, as we show by examples. Both WordNets and WSD in general
are well known, but this is the first use of these tools with GF.
</bodyText>
<sectionHeader confidence="0.998162" genericHeader="introduction">
1 Introduction
</sectionHeader>
<subsectionHeader confidence="0.967211">
1.1 Translation via an interlingua
</subsectionHeader>
<bodyText confidence="0.999913117647059">
Interlingual translation scales easily up to a large number of languages. Google translate, for
example, deals with all pairs of 60 languages mostly by using English as a pivot language. In
this way, it can do with just 2 * 59 = 118 sets of bilingual training data, instead of 60 * 59 =
3540 sets. It would be hard to collect and maintain so many pairs, and in many cases, there is
very little data to be found.
The roots of an inter-lingua are perhaps in the medieval idea of a universal grammar (Lyons,
1968), in which a universal representation of meaning can be expressed. Translating via this
interlingua then also means that meaning is conserved in going from the source to the tar-
get language. In recent decades, this idea appears in (Curry, 1961) where the interlingua is
called tectogrammar, in the Rosetta project (Rosetta, 1994), building on the semantic models
of (Montague, 1974), and in the UNL (Universal Networking Language) project.
Incidentally, interlingua is also the heart of modern compiler technology. For instance, the
GNU Compiler Collection (Stallman, 2001) uses a shared tree representation to factor out the
majority of compilation phases between a large number of source and target languages. Compiler
writers save work, and semantics is preserved by design. A compiler, then, is built as a pipeline
with parsing from a source language to an abstract syntax tree, which is analyzed and
optimized in the language-independent phases, and finally linearized to a target language.
</bodyText>
<footnote confidence="0.429838666666667">
This work is licenced under a Creative Commons Attribution 4.0 International License. Page numbers and
proceedings footer are added by the organizers. License details: http://creativecommons.org/licenses/by/4.
0/
</footnote>
<page confidence="0.982395">
55
</page>
<note confidence="0.990982">
Proceedings of the 5th Workshop on South and Southeast Asian NLP, 25th International Conference on Computational Linguistics, pages 55–64,
Dublin, Ireland, August 23-29 2014.
</note>
<bodyText confidence="0.998935">
It is easy to see an analogy between this pipeline and the way a human language translator
could work. But how to make it real? How to scale up to the full size of natural languages?
</bodyText>
<subsectionHeader confidence="0.905558">
1.2 WordNets
</subsectionHeader>
<bodyText confidence="0.996948166666667">
In current machine translation research, interlingual methods are marginal, despite the wide use
of pivot languages in systems like Google translate. Closest to the mainstream perhaps is the
development of linked WordNets. The original Princeton Wordnet for English (Miller, 1995) de-
fines a set of word senses, which many other wordnets map to other languages. Implementations
of this idea are Finnish (Lindén and Carlson., 2010) and Hindi (Hindi-WordNet, 2012).
In the linked Wordnet approach, the Princeton WordNet senses work as an interlingua, albeit
only on the level of the lexicon. (Lindén and Carlson., 2010) give strong arguments why in fact
this is a good way to go, despite the often emphasized fact that different languages divide the
world in different ways, so that the senses of their word don’t map one to one. The evidence from
the English-Finnish case shows that 80% of the mappings are one-to-one and un-problematic.
As this part of the lexicon can be easily reused, linguists and system builders can concentrate
their effort on the remaining 20%.
The Universal WordNet (de Melo and Weikum, 2009) works on the same lines. Building on
the Princeton WordNet, it populates the mappings to over 200 different languages by collecting
data from different sources (such as the Wikipedia) and using supervised machine learning
techniques to propagate the knowledge and infer more of it. What makes it a particularly
interesting resource is that it is freely available under the most liberal licenses, as is the original
Princeton WordNet,
</bodyText>
<subsectionHeader confidence="0.712312">
1.3 GF
</subsectionHeader>
<bodyText confidence="0.99979116">
Grammatical Framework (GF)(Ranta, 2004) is a grammar formalism tool based on Martin
Löf’s type theory (Martin-Löf, 1982). It can be seen as a tool to build interlingua based trans-
lation systems. GF works like a compiler: the source language is parsed to an abstract syntax
tree, which is then linearized to the target language. The parsing and linearization component
are defined by using Parallel Multiple Context-Free Grammars (PMCFG, (Seki et al., 1991),
(Ljunglöf, 2004)), which give GF an expressive power between mildly and fully context-sensitive
grammars. Thus GF can easily handle with language-specific variations in morphology, word
order, and discontinuous constituents, while maintaining a shared abstract syntax.
Historically, the main use of GF has been in controlled language implementations, e.g., (Ranta
and Angelov, 2010; Angelov and Enache, 2010; Ranta et al., 2012) and natural language
generation, e.g., (Dymetman et al., 2000), both applied in multilingual settings with up to 15
parallel languages. In recent years, the coverage of GF grammars and the processing performance
has enabled open-domain tasks such as treebank parsing (Angelov, 2011) and hybrid translation
of patents (Enache et al., 2012). The general purpose Resource Grammar Library (RGL)(Ranta,
2011) has grown to 30 languages. It includes the major European languages, South Asian
languages like Hindi/Urdu (Prasad and Shafqat, 2012), Nepali and Punjabi (Shafqat et al.,
2011), the Southeast Asian language Thai, and Japanese and Chinese.
However, GF has yet not been exploited for arbitrary text parsing and translation. To do
this, we have to meet several challenges: robust parsing, parse-tree disambiguation, word sense
disambiguation, and development of a wide-coverage interlingual translation lexicon. This paper
focuses on the latter two. We report first a method of using the WordNets (Princeton and
Universal) to build an interlingual full-form, multiple sense translation lexicon. Then, we show
how these lexicons together with a word sense disambiguation tool can be plugged in a translation
pipeline. Finally, we describe an experimental setup and give many examples to highlight the
effects of this work.
</bodyText>
<page confidence="0.9936">
56
</page>
<subsectionHeader confidence="0.958393">
1.4 South Asian languages
</subsectionHeader>
<bodyText confidence="0.999913833333333">
While the work described here can apply to any language, it is particularly interesting for South
Asian languages. In these languages, statistical tools do not have much bilingual training data to
work on, so Google translate and similar tools are not as useful as they are with better resourced
languages. At the same time, there is an urgent and widely recognised need for translations from
English to the various languages of South Asia. Fortunately, word nets are being built for many
of them, so that the techniques described here can be applied.
</bodyText>
<sectionHeader confidence="0.7234" genericHeader="method">
2 From Universal WordNet to a GF Lexicon
</sectionHeader>
<bodyText confidence="0.999660090909091">
The original Princeton WordNet (Miller, 1995) defines a set of word senses, and the Universal
WordNet (de Melo and Weikum, 2009) maps them to different languages. In this multilingual
scenario, the Princeton WordNet senses can be seen as an abstract representation, while the
Universal WordNet mappings can be seen as concrete representation of those senses in different
languages. GF grammars use very much the same technique of one common abstract and
multiple parallel concrete representations to achieve multilingualism. Due to this compatibility,
it is easy to build a multilingual GF lexicon using data from those two resources (i.e. Princeton
and Universal WordNets). This section briefly describes the experiment we did to build one
abstract and multiple concrete GF lexicons for a number of languages including German, French,
Finnish, Swedish, Hindi, and Bulgarian. The method is very general, so can be used to build a
similar lexicon for any other language for which data is available in the Universal WordNet.
</bodyText>
<subsectionHeader confidence="0.895693">
2.1 GF Abstract Lexicon
</subsectionHeader>
<bodyText confidence="0.999649714285714">
The Princeton WordNet data is distributed in the form of different database files. For each of
the four lexical categories (i.e. noun, verb, adjective, and adverb), two files named ‘index.pos’
and ‘data.pos’ are provided, where ‘pos’ is noun, verb, adj and adv. Each of the ‘index.pos’
files contains all words, including synonyms of the words, found in the corresponding part of
speech category. Each of the ‘data.pos’ files contains information about unique senses belonging
to the corresponding part of speech category. For our purposes, there were two possible choices
to build an abstract representation of the lexicon:
</bodyText>
<listItem confidence="0.95708825">
1. To include all words of the four lexical categories, and also their synonyms (i.e. to build
the lexicon from ‘index.pos’ files)
2. To include only unique senses of the four categories with one word per sense, but not the
synonyms (i.e. to build the lexicon from the data.pos’ files)
</listItem>
<bodyText confidence="0.9942754">
To better understand this difference, consider the words ‘brother’ and ‘buddy’. The word
‘brother’ has five senses with sense offsets ‘08111676’, ‘08112052’, ‘08112961’, ‘08112265’ and
‘08111905’ in the Princeton WordNet 1.7.11, while the word ‘buddy’ has only one sense with the
sense offset ‘08112961’. Choosing option (1) means that we have to include the following entries
in our abstract lexicon.
</bodyText>
<equation confidence="0.746683666666667">
brother_08111676_N
brother_08112052_N
brother_08112961_N
brother_08112265_N
brother_08111905_N
buddy_08112961_N
</equation>
<bodyText confidence="0.8803886">
We can see that the sense with the offset ‘08112961’ is duplicated in the lexicon: once with
the lemma ‘brother’ and then with the lemma ‘buddy’. However, if we choose option (2), we
end up with the following entries.
&apos;We choose WordNet 1.7.1, because the word sense disambiguator that we are using in our translation pipeline
is based on WordNet 1.7.1
</bodyText>
<page confidence="0.933004">
57
</page>
<equation confidence="0.6382272">
brother_08111676_N
brother_08112052_N
brother_08112265_N
brother_08111905_N
buddy_08112961_N
</equation>
<bodyText confidence="0.92606325">
Since the file ‘data.noun’ lists the unique senses rather than the words, their will be no
duplication of the senses. However, the choice has an obvious effect on the lexicon coverage, and
depending on whether we want to use it as a parsing or as a linearization lexicon, the choice
becomes critical. Currently, we choose option (2) for the following two reasons:
</bodyText>
<listItem confidence="0.997162777777778">
1. The Universal WordNet provides mappings for synsets (i.e. unique senses) but not for the
individual synonyms of the synsets. If we choose option (1), as mentioned previously, we
have to list all synonyms in our abstract representation. But, as translations are available
only for synsets, we have to put the same translation against each of the synonyms of the
synset in our concrete representations. This will not gain us anything (as long as we use
these lexicon as linearization lexicons), but will increase the size of the lexicon and hence
may have reduce the processing speed of the translation system.
2. At the current stage of our experiments we are using these lexicons as linearization lexicons,
so one translation of each unique sense is enough.
</listItem>
<bodyText confidence="0.93242">
Our abstract GF lexicon covers 91516 synsets out of around 111,273 synsets in the WordNet
1.7.1. We exclude some of the synsets with multi-word lemmas. We consider them more of a
syntactic category rather than a lexical category, and hence deal with them at the syntax level.
Here, we give a small segment of our abstract GF lexicon.
</bodyText>
<construct confidence="0.689848714285714">
abstract LinkedDictAbs = Cat ** {
fun consecutive_01624944_A : A ;
fun consequently_00061939_Adv : Adv ;
fun conservation_06171333_N : N ;
fun conspire_00562077_V : V ;
fun sing_01362553_V2 : V2 ;
}
</construct>
<bodyText confidence="0.9811294">
The first line in the above given code states that the module ‘LinkedDictAbs’ is an abstract
representation (note the keyword ‘abstract’). This module extends (achieved by ‘**’ operator)
another module labeled ‘Cat2’ which, in this case, has definitions for the morphological categories
‘A’, ‘Adv’, ‘N’ and ‘V’. These categories correspond to the ‘adjective’, ‘adverb’, ‘noun’, and ‘verb’
categories in the WordNet respectively. However, note that in GF resource grammars we have
a fine-grained morphological division for verbs. We sub-categorize them according to their
valencies i.e ‘V’ is for intransitive, and ‘V2’ for transitive verbs. We refer to (Bringert et al.,
2011) for more details on these divisions.
Each entry in this module is of the following general type:
fun lemma_senseOffset_t : t ;
Keyword ‘fun’ declares each entry as a function of the type ‘t’. The function name is composed
of lemma, sense offset and a type ‘t’, where lemma and sense offset are same as in the Princeton
WordNet, while ‘t’ is one of the morphological types in GF resource grammars.
This abstract representation will serve as a pivot for all concrete representations, which are
described next.
</bodyText>
<footnote confidence="0.991885">
2This module has definitions of different morphological and syntactic categories in the GF resource grammar
library
</footnote>
<page confidence="0.99608">
58
</page>
<subsectionHeader confidence="0.834062">
2.2 GF Concrete Lexicons
</subsectionHeader>
<bodyText confidence="0.999667333333333">
We build the concrete representations for different languages using the translations obtained
from the Universal WordNet data and GF morphological paradigms (Détrez and Ranta, 2012;
Bringert et al., 2011). The Universal WordNet translations are tagged with a sense offset from
WordNet 3.03 and also with a confidence score. As, an example consider the following segment
form the Universal WordNet data, showing German translations for the noun synset with offset
‘13810818’ and lemma ‘rest’ (in the sense of ‘remainder’).
</bodyText>
<figure confidence="0.6224982">
n13810818 Rest 1.052756
n13810818 Abbrand 0.95462
n13810818 Ruckstand 0.924376
Each entry is of the following general type.
posSenseOffset translation confidence-score
</figure>
<bodyText confidence="0.998827666666667">
If we have more than one candidate translation for the same sense (as in the above case),
we select the best one (i.e. with the maximum confidence score) and put it in the concrete
grammar. Next, we give a small segment from the German concrete lexicon.
</bodyText>
<construct confidence="0.638261375">
concrete LinkedDictGer of LinkedDictAbs = CatGer ** open
ParadigmsGer, IrregGer,Prelude in {
lin consecutive_01624944_A = mkA &amp;quot;aufeinanderfolgend&amp;quot; ;
lin consequently_00061939_Adv = mkAdv &amp;quot;infolgedessen&amp;quot; ;
lin conservation_06171333_N = mkN &amp;quot;Konservierung&amp;quot; ;
lin conspire_00562077_V = mkV &amp;quot;anzetteln&amp;quot; ;
lin sing_01362553_V2 = mkV2 (mkV &amp;quot;singen&amp;quot; ) ;
}
</construct>
<bodyText confidence="0.986442285714286">
The first line declares ‘LinkedDictGer’ to be the concrete representation of the previously
defined abstract representation (note the keyword ‘concrete’ at the start of the line). Each entry
in this representation is of the following general type:
lin lemma_senseOffset_t = paradigmName &amp;quot;translation&amp;quot; ;
Keyword ‘lin’ declares each entry to be a linearization of the corresponding function in the
abstract representation. ‘paradigmName’ is one of the morphological paradigms defined in the
‘ParadigmsGer’ module. So in the above code, ‘mkA’, ‘mkAdv’, ‘mkN’, ‘mkV’ and ‘mkV2’ are
the German morphological paradigms4 for different lexical categories of ‘adjective’, ‘adverb’,
‘noun’, ‘intransitive verb’, and ‘transitive verb’ respectively. “translation” is the best possible
translation obtained from the Universal WordNet. This translation is passed to a paradigm as
a base word, which then builds a full-form inflection table. These tables are then used in the
linearization phase of the translation system (see section 3)
Concrete lexicons for all other languages were developed using the same procedure. Table 1
gives some statistics about the coverage of these lexicons.
</bodyText>
<table confidence="0.9873318">
Language Number of Entries Language Number of Entries
Abstract 91516 German 49439
French 38261 Finnish 27673
Swedish 23862 Hindi 16654
Bulgarian 12425
</table>
<tableCaption confidence="0.999587">
Table 1: Lexicon Coverage Statistics
</tableCaption>
<footnote confidence="0.7678675">
3However, in our concrete lexicons we match them to WordNet 1.7.1 for the reasons mentioned previously
4See (Bringert et al., 2011) for more details on these paradigms
</footnote>
<page confidence="0.999137">
59
</page>
<sectionHeader confidence="0.909549" genericHeader="method">
3 System architecture
</sectionHeader>
<bodyText confidence="0.9994613125">
Figure 1 shows an architecture of the translation pipeline. The architecture is inter-lingual
and uses the Resource Grammar Library (RGL) of Grammatical Framework (Ranta, 2011) as
the syntax and semantics component, Penn Treebank data for parse-tree disambiguation and
IMS(It Makes Sense)(Zhong and Ng, 2010) as a word sense disambiguation tool. Even though
the syntax, semantics and parse-tree disambiguation are not the main topics of this paper,
we give the full architecture to show where the work reported in this paper fits. Internal GF
resources (e.g. resource grammars and dictionaries) are shown in rectangles while the external
components (e.g. PennTreebank and IMS(Zhong and Ng, 2010): a wide coverage word sense
disambiguation system for arbitrary text.) are shown in double-stroked rectangles.
With reference to Figure 1: The input is parsed using English resource grammar (EngRG)
and a comprehensive English dictionary (DictEng). If the input is syntactically ambiguous the
parser will return more than one parse-tree. These trees are disambiguated using a statistical
model build from the PennTreebank data. The best tree is further processed using the input
from the IMS to tag the lexical nodes with best sense identifiers. This tree is finally linearized
to the target language using the target language resource grammar (TLRG) together with the
target language lexicon (LinkedDict) discussed in section 2.
</bodyText>
<figure confidence="0.952622">
EngRG: English Resource Grammar
TLRG: Target Language Resource Grammar
</figure>
<figureCaption confidence="0.999967">
Figure 1: The translation pipeline.
</figureCaption>
<sectionHeader confidence="0.976248" genericHeader="method">
4 Experimental Setup and Evaluation
</sectionHeader>
<bodyText confidence="0.999791578947368">
Our experimental setup is as follows: We take some English text as source, and translate it to a
target language (German and Hindi in these experiments) by passing it through the translation
pipeline described in section 3. To show the usefulness of the lexicons described in section 2 and
for comparison, we translate the same source twice: with and without word sense disambiguation.
For the first attempt, we used exactly the same translation pipeline as shown in Figure 1,
except that to overcome the deficiencies of our existing parse-tree disambiguator, for some of
the examples, we used trees directly from the PennTreebank, which are supposed to be correct.
However, this should not damage the claims made in this paper which is about developing
wide coverage interlingual translation lexicons and then using them for WSD in an interlingual
translation pipeline.
For the second attempt, we plugged out the word sense disambiguation form the translation
pipeline and used our old GF style lexicons (one target word per source word irrespective of its
sense) in the linearization phase.
Finally, we compared both candidate translations to see if we have gained anything. We did
both manual and automatic evaluations to confirm our findings.
For a set of 25 sentences for English-German pair we got marginal BLEU score improvements
(from 0.3904 to 0.399 with ‘old’ and ‘new’ dictionaries). Manual inspection, however, was much
more encouraging, and explained the reasons for very low improvements in the BLEU scores in
some cases. The reason was that even if the word sense disambiguation, and hence, our new
</bodyText>
<figure confidence="0.998148214285714">
EngRG+DictEng
Parsing
Penn Treebank
Parse Tree
Disambigu
ation
Word
Sense
Disambigu
ation
IMS
TLRG+LinkedDict
Linearizati
on
</figure>
<page confidence="0.983217">
60
</page>
<bodyText confidence="0.9998055">
lexicon gives a better lexical choice, it will still be considered ‘wrong” by the evaluation tool if the
gold-standard has a different choice. It was also observed that there were cases where the ‘old’
lexicon produced a much better translation than the ‘new’ one. The reasons for this are obvious.
The word sense disambiguator has its own limitations and is known to make mistakes. Also, as
explained in Section 5, the lexicon cannot be guaranteed to always give the right translation.
Next, we give a number of example sentence with comments5 to show that how the new
lexicons improved the quality of translations, and also give some examples where it worked the
other way around.
</bodyText>
<listItem confidence="0.7923625">
4.1 German
1. Source He increases the board to seven
</listItem>
<bodyText confidence="0.9942325">
Without WSD er erhöht das Brett nach einigen sieben
With WSD er vergrö Bert die Behörde nach einigen sieben
Comments das Brett is a wooden board (wrong); erhöht means “to raise”. while
vergrö Bert means “increases the size”. Note the wrong preposition choice (“to” should
be zu rather than nach). Also, an indefinite determiner (einige, some) has been
wrongly added to the cardinal number is used as a noun phrase.
</bodyText>
<listItem confidence="0.455453">
2. Source the index uses a base of 100 in 1,982
</listItem>
<bodyText confidence="0.945207333333333">
Without WSD das Verzeichnis verwendet eine Base nach einige 100 in einigen
1982
With WSD der [index_11688271_N] nutzt einen Operationsbasis von einigen
100 in einigen 1982
Comments Note the untranslated word in the WSD version. Base means a chemical base,
the wrong meaning here. Operationsbasis is not the best choice, but acceptable.
</bodyText>
<listItem confidence="0.798125">
3. Source fear is the father of panic
With WSD Angst ist der Papa von Angst
Comment The traditional hilarious example, saying “fear is fear’s daddy”.
</listItem>
<subsectionHeader confidence="0.837152">
4.2 Hindi
</subsectionHeader>
<bodyText confidence="0.999371125">
To represent Hindi, we use an IPA style alphabet, with the usual values and conventions.
Retroflexed sounds are written with a dot under the letter: ṭ, ḍ, and ṛ (a flap) are com-
mon, while ṇ and ṣ occur in Sanskritised Hindi (though many dialects pronounce them n and
š). The palatalised spirant is shown š and aspirated stops are shown thus: kh. A macron over a
vowel denotes a long vowel, and ˜, nasalisation. In Hindi, e and o are always long, so the macron
is dropped. Finally, we use ñ to mean the nasal homorganic with the following consonant.
Here are examples from our evaluation showing that the WSD system works well; the versions
without WSD merely pick the first synonym in the lexicon.
</bodyText>
<listItem confidence="0.9215">
1. Source Mr Baris is a lawyer in New York.
</listItem>
<bodyText confidence="0.491704833333333">
Without WSD Mr Baris New York mẽ kānin kā pañḍit hæ
With WSD Mr Baris New York mẽ vak3l hæ
Word order Mr Baris New York in lawyer is
Comments kānin kā pañḍit is “expert/teacher in law”, while vak3l means “lawyer”.
2. Source we don’t depend on pharmaceutical companies for our support
Without WSD ham auṣadh3ya sahyōg3 par hamāre bharaṇ pōṣaṇ ke liye nahinirte hæ̃.
</bodyText>
<footnote confidence="0.9936155">
5For the comments on German, we are indebted to Erzsebet Galgoczy and Wolfgang Ahrendt, our colleagues
and German informants.
</footnote>
<page confidence="0.997399">
61
</page>
<bodyText confidence="0.93642575">
With WSD ham auṣadh3ya kañpan3 par hamāre nirvāh vyay ke liye nahiūte has̃.
Word order We pharmaceutical companies on our subsistence expenditure for
not ??? do
Comments sahyōg3 means “company” in the sense of “colleagues”, nirvāh vyay means
“subsistence expenditure” , while bharaṇ pōṣaṇ means “weight bearing”. The penul-
timate word in both versions is nonsense, and the lexicons need to be debugged.
3. Source you may recall that a triangle is also a polygon
Without WSD tum &amp;quot;recall may&amp;quot; ho ki ṭrāyengl &amp;quot;also&amp;quot; bahubhuj has
With WSD tum smaraṇ kar sakte ho ki trikoṇ bh3 bahubhuj has
Word order You recall do can that triangle also polygon is
Comments The version without WSD has several missing words. The WSD version of
“recall” is not idiomatic, but understandable.
It should be noted that the coverage of the Hindi lexicon is lowest of all the lexicons given
in Table 1. The result is that many sentences have missing words in the translations. Also,
there is considerable interference with Urdu words (some stemming from the shared base
grammar (Prasad and Shafqat, 2012)). Further, some mappings coming from the Universal
WordNet data are in roman, as opposed to Devanagari (the usual script for Hindi, and what
the grammar is based on), so these need to be transcribed. Finally, idiomatic phrases are
a problem (“before the law” is likely to be rendered “(temporally) before the law” rather
than “in the eyes of the law”).
</bodyText>
<sectionHeader confidence="0.925862" genericHeader="method">
5 The next steps
</sectionHeader>
<bodyText confidence="0.999935125">
Since the Universal WordNet mappings are produced from parallel data by machine learning
techniques, the translations are not always accurate and do not always make the best possible
choice. This leaves a window for improvement in the quality of the reported lexicons. One
way of improvement is the manual inspection/correction, not an easy task for a wide-coverage
lexicon with around 100 thousand entries, but not impossible either. This would be a one-time
task with a strong impact on the quality of the lexicon. Another way is to use manually built
WordNets, such as the Finnish and Hindi WordNets. In our work, the availability of some of
these resources was an issue, so we leave it for the future. Further, as mentioned in Section 4,
the Hindi lexicon has some script-related issues which should be fixed in future.
When it comes to interlingua-based arbitrary machine translation, an important concern is
the size of lexicons. We are aware of the fact that the size of our lexicons is not comparable to
some of the other similar systems such as ATLAS-II (Fujitsu), where the size of lexicons is in
millions. We have plan to extend the size of lexicons using some of the other publicly available
resources (such as Hindi WordNet) and/or using parallel corpus. The development of bilingual
lexicons form parallel corpus have been previously explored (Delpech et al., 2012; Qian et al.,
2012), and the same ideas can be applied in our case.
</bodyText>
<sectionHeader confidence="0.998848" genericHeader="conclusions">
6 Conclusion
</sectionHeader>
<bodyText confidence="0.9998915">
We have shown how to use existing lexical resources such as WordNets to develop an interlingual
translation lexicon in GF, and how to use it for the WSD task in an arbitrary text translation
pipeline. The improvements in the translation quality (lexical), shown by examples in Section
4, are encouraging and motivate further work in this direction. However, it should be noted
that there is still a lot of work to be done (especially in the open domain text parsing and
parse-tree disambiguation phases of the translation pipeline) to bring the translation system to
a competitive level. For the reasons noted in the introduction, we expect our techniques to be
particularly useful for South Asian languages.
</bodyText>
<page confidence="0.998875">
62
</page>
<sectionHeader confidence="0.978867" genericHeader="references">
References
</sectionHeader>
<reference confidence="0.999726829787234">
Angelov, K. (2011). The Mechanics of the Grammatical Framework. PhD thesis, Chalmers University
Of Technology. ISBN 978-91-7385-605-8.
Angelov, K. and Enache, R. (2010). Typeful Ontologies with Direct Multilingual Verbalization. In Fuchs,
N. and Rosner, M., editors, CNL 2010, Controlled Natural Language.
Bringert, B., Hallgren, T., and Ranta., A. (2011). GF resource grammar library synopsis.
www.grammaticalframework.org/lib/doc/synopsis.html.
Curry, H. B. (1961). Some logical aspects of grammatical structure. In Jakobson, R., editor, Structure of
Language and its Mathematical Aspects: Proceedings of the Twelfth Symposium in Applied Mathematics,
pages 56–68. American Mathematical Society.
de Melo, G. and Weikum, G. (2009). Towards a Universal Wordnet by learning from combined evidence.
In Proceedings of the 18th ACM Conference on Information and Knowledge Management (CIKM 2009),
pages 513–522, New York, NY, USA. ACM.
Delpech, E., Daille, B., Morin, E., and Lemaire, C. (2012). Extraction of domain-specific bilingual lexicon
from comparable corpora: Compositional translation and ranking. In Proceedings of COLING 2012,
pages 745–762, Mumbai, India. The COLING 2012 Organizing Committee.
Détrez, G. and Ranta, A. (2012). Smart paradigms and the predictability and complexity of inflectional
morphology. In EACL, pages 645–653.
Dymetman, M., Lux, V., and Ranta, A. (2000). XML and multilingual document authoring: Conver-
gent trends. In Proc. Computational Linguistics COLING, Saarbrücken, Germany, pages 243–249.
International Committee on Computational Linguistics.
Enache, R., España-Bonet, C., Ranta, A., and Márquez, L. (2012). A hybrid system for patent translation.
In Proceedings of the 16th Annual Conference of the European Association for Machine Translation
(EAMT12), Trento, Italy.
Hindi-WordNet (2012). Hindi Wordnet. 2012. Universal Word—Hindi Lexicon.
http://www.cfilt.iitb.ac.in.
Lindén, K. and Carlson., L. (2010). Finnwordnet—wordnet på finska via översättning. Lexi-
coNordica—Nordic Journal of Lexicography, 17:119–140.
Ljunglöf, P. (2004). The Expressivity and Complexity of Grammatical Framework. PhD thesis, Dept. of
Computing Science, Chalmers University of Technology and Gothenburg University. http://www.cs.
chalmers.se/~peb/pubs/p04-PhD-thesis.pdf.
Lyons, J. (1968). Introduction to theoretical linguistics. Cambridge: Cambridge University Press.
Martin-Löf, P. (1982). Constructive mathematics and computer programming. In Cohen, Los, Pfeif-
fer, and Podewski, editors, Logic, Methodology and Philosophy of Science VI, pages 153–175. North-
Holland, Amsterdam.
Miller, G. A. (1995). Wordnet: A lexical database for English. Communications of the ACM, 38:39–41.
Montague, R. (1974). Formal Philosophy. Yale University Press, New Haven. Collected papers edited
by Richmond Thomason.
Prasad, K. V. S. and Shafqat, M. V. (2012). Computational evidence that Hindi and Urdu share a
grammar but not the lexicon. In The 3rd Workshop on South and Southeast Asian NLP, COLING.
Qian, L., Wang, H., Zhou, G., and Zhu, Q. (2012). Bilingual lexicon construction from comparable
corpora via dependency mapping. In Proceedings of COLING 2012, pages 2275–2290, Mumbai, India.
The COLING 2012 Organizing Committee.
Ranta, A. (2004). Grammatical Framework: A Type-Theoretical Grammar Formalism. The Journal of
Functional Programming, 14(2):145–189. http://www.cse.chalmers.se/~aarne/articles/gf-jfp.
pdf.
Ranta, A. (2011). Grammatical Framework: Programming with Multilingual Grammars. CSLI Publica-
tions, Stanford. ISBN-10: 1-57586-626-9 (Paper), 1-57586-627-7 (Cloth).
</reference>
<page confidence="0.987078">
63
</page>
<reference confidence="0.999663428571429">
Ranta, A. and Angelov, K. (2010). Implementing Controlled Languages in GF. In Proceedings of CNL-
2009, Athens, volume 5972 of LNCS, pages 82–101.
Ranta, A., Détrez, G., and Enache, R. (2012). Controlled language for everyday use: the MOLTO
phrasebook. In CNL 2012: Controlled Natural Language, volume 7175 of LNCS/LNAI.
Rosetta, M. T. (1994). Compositional Translation. Kluwer, Dordrecht.
Seki, H., Matsumura, T., Fujii, M., and Kasami, T. (1991). On multiple context-free grammars. Theo-
retical Computer Science, 88:191–229.
Shafqat, M., Humayoun, M., and Aarne, R. (2011). An open source Punjabi resource grammar. In Pro-
ceedings of the International Conference Recent Advances in Natural Language Processing 2011, pages
70–76, Hissar, Bulgaria. RANLP 2011 Organising Committee. http://aclweb.org/anthology/R11-1010.
Stallman, R. (2001). Using and Porting the GNU Compiler Collection. Free Software Foundation.
Zhong, Z. and Ng, H. T. (2010). It makes sense: A wide-coverage word sense disambiguation system
for free text. In Proceedings of the ACL 2010 System Demonstrations, pages 78–83, Uppsala, Sweden.
Association for Computational Linguistics. http://www.aclweb.org/anthology/P10-4014.
</reference>
<page confidence="0.999418">
64
</page>
</variant>
</algorithm>
<algorithm name="ParsHed" version="110505">
<variant no="0" confidence="0.185319">
<title confidence="0.9759725">Developing an interlingual translation lexicon using and Grammatical Framework</title>
<author confidence="0.978298">Shafqat Mumtaz</author>
<affiliation confidence="0.99974">University of University of Eng. &amp; Tech.</affiliation>
<email confidence="0.998792">virk.shafqat@gmail.com</email>
<author confidence="0.583198">Aarne</author>
<affiliation confidence="0.962628">University of</affiliation>
<email confidence="0.531007">aarne@chalmers.se</email>
<author confidence="0.909728">K V S Prasad</author>
<affiliation confidence="0.958497">Chalmers University of Technology</affiliation>
<email confidence="0.815488">prasad@chalmers.se</email>
<author confidence="0.903776">Krasimir Angelov</author>
<affiliation confidence="0.99406">University of Gothenburg</affiliation>
<email confidence="0.933636">krasimir@chalmers.se</email>
<abstract confidence="0.999535058823529">The Grammatical Framework (GF) offers perfect translation between controlled subsets of natural languages. E.g., an abstract syntax for a set of sentences in school mathematics is the interlingua between the corresponding sentences in English and Hindi, say. GF “resource grammars” specify how to say something in English or Hindi; these are reused with “application grammars” that specify what can be said (mathematics, tourist phrases, etc.). More recent robust parsing and parse-tree disambiguation allow GF to parse arbitrary English text. We report here an experiment to linearise the resulting tree directly to other languages (e.g. Hindi, German, etc.), i.e., we use a languageindependent resource grammar as the interlingua. We focus particularly on the last part of the translation system, the interlingual lexicon and word sense disambiguation (WSD). We improved the quality of the wide coverage interlingual translation lexicon by using the Princeton and Universal WordNet data. We then integrated an existing WSD tool and replaced the usual GF style lexicons, which give one target word per source word, by the WordNet based lexicons. These new lexicons and WSD improve the quality of translation in most cases, as we show by examples. Both WordNets and WSD in general are well known, but this is the first use of these tools with GF.</abstract>
</variant>
</algorithm>
<algorithm name="ParsCit" version="110505">
<citationList>
<citation valid="true">
<authors>
<author>K Angelov</author>
</authors>
<title>The Mechanics of the Grammatical Framework.</title>
<date>2011</date>
<tech>PhD thesis,</tech>
<pages>978--91</pages>
<institution>Chalmers University Of Technology. ISBN</institution>
<contexts>
<context position="6601" citStr="Angelov, 2011" startWordPosition="1021" endWordPosition="1022">rammars. Thus GF can easily handle with language-specific variations in morphology, word order, and discontinuous constituents, while maintaining a shared abstract syntax. Historically, the main use of GF has been in controlled language implementations, e.g., (Ranta and Angelov, 2010; Angelov and Enache, 2010; Ranta et al., 2012) and natural language generation, e.g., (Dymetman et al., 2000), both applied in multilingual settings with up to 15 parallel languages. In recent years, the coverage of GF grammars and the processing performance has enabled open-domain tasks such as treebank parsing (Angelov, 2011) and hybrid translation of patents (Enache et al., 2012). The general purpose Resource Grammar Library (RGL)(Ranta, 2011) has grown to 30 languages. It includes the major European languages, South Asian languages like Hindi/Urdu (Prasad and Shafqat, 2012), Nepali and Punjabi (Shafqat et al., 2011), the Southeast Asian language Thai, and Japanese and Chinese. However, GF has yet not been exploited for arbitrary text parsing and translation. To do this, we have to meet several challenges: robust parsing, parse-tree disambiguation, word sense disambiguation, and development of a wide-coverage int</context>
</contexts>
<marker>Angelov, 2011</marker>
<rawString>Angelov, K. (2011). The Mechanics of the Grammatical Framework. PhD thesis, Chalmers University Of Technology. ISBN 978-91-7385-605-8.</rawString>
</citation>
<citation valid="true">
<authors>
<author>K Angelov</author>
<author>R Enache</author>
</authors>
<title>Typeful Ontologies with Direct Multilingual Verbalization.</title>
<date>2010</date>
<booktitle>CNL 2010, Controlled Natural Language.</booktitle>
<editor>In Fuchs, N. and Rosner, M., editors,</editor>
<contexts>
<context position="6297" citStr="Angelov and Enache, 2010" startWordPosition="973" endWordPosition="976">is parsed to an abstract syntax tree, which is then linearized to the target language. The parsing and linearization component are defined by using Parallel Multiple Context-Free Grammars (PMCFG, (Seki et al., 1991), (Ljunglöf, 2004)), which give GF an expressive power between mildly and fully context-sensitive grammars. Thus GF can easily handle with language-specific variations in morphology, word order, and discontinuous constituents, while maintaining a shared abstract syntax. Historically, the main use of GF has been in controlled language implementations, e.g., (Ranta and Angelov, 2010; Angelov and Enache, 2010; Ranta et al., 2012) and natural language generation, e.g., (Dymetman et al., 2000), both applied in multilingual settings with up to 15 parallel languages. In recent years, the coverage of GF grammars and the processing performance has enabled open-domain tasks such as treebank parsing (Angelov, 2011) and hybrid translation of patents (Enache et al., 2012). The general purpose Resource Grammar Library (RGL)(Ranta, 2011) has grown to 30 languages. It includes the major European languages, South Asian languages like Hindi/Urdu (Prasad and Shafqat, 2012), Nepali and Punjabi (Shafqat et al., 201</context>
</contexts>
<marker>Angelov, Enache, 2010</marker>
<rawString>Angelov, K. and Enache, R. (2010). Typeful Ontologies with Direct Multilingual Verbalization. In Fuchs, N. and Rosner, M., editors, CNL 2010, Controlled Natural Language.</rawString>
</citation>
<citation valid="true">
<authors>
<author>B Bringert</author>
<author>T Hallgren</author>
<author>A Ranta</author>
</authors>
<title>GF resource grammar library synopsis.</title>
<date>2011</date>
<note>www.grammaticalframework.org/lib/doc/synopsis.html.</note>
<contexts>
<context position="13519" citStr="Bringert et al., 2011" startWordPosition="2101" endWordPosition="2104"> module ‘LinkedDictAbs’ is an abstract representation (note the keyword ‘abstract’). This module extends (achieved by ‘**’ operator) another module labeled ‘Cat2’ which, in this case, has definitions for the morphological categories ‘A’, ‘Adv’, ‘N’ and ‘V’. These categories correspond to the ‘adjective’, ‘adverb’, ‘noun’, and ‘verb’ categories in the WordNet respectively. However, note that in GF resource grammars we have a fine-grained morphological division for verbs. We sub-categorize them according to their valencies i.e ‘V’ is for intransitive, and ‘V2’ for transitive verbs. We refer to (Bringert et al., 2011) for more details on these divisions. Each entry in this module is of the following general type: fun lemma_senseOffset_t : t ; Keyword ‘fun’ declares each entry as a function of the type ‘t’. The function name is composed of lemma, sense offset and a type ‘t’, where lemma and sense offset are same as in the Princeton WordNet, while ‘t’ is one of the morphological types in GF resource grammars. This abstract representation will serve as a pivot for all concrete representations, which are described next. 2This module has definitions of different morphological and syntactic categories in the GF </context>
<context position="16958" citStr="Bringert et al., 2011" startWordPosition="2616" endWordPosition="2619">e word, which then builds a full-form inflection table. These tables are then used in the linearization phase of the translation system (see section 3) Concrete lexicons for all other languages were developed using the same procedure. Table 1 gives some statistics about the coverage of these lexicons. Language Number of Entries Language Number of Entries Abstract 91516 German 49439 French 38261 Finnish 27673 Swedish 23862 Hindi 16654 Bulgarian 12425 Table 1: Lexicon Coverage Statistics 3However, in our concrete lexicons we match them to WordNet 1.7.1 for the reasons mentioned previously 4See (Bringert et al., 2011) for more details on these paradigms 59 3 System architecture Figure 1 shows an architecture of the translation pipeline. The architecture is inter-lingual and uses the Resource Grammar Library (RGL) of Grammatical Framework (Ranta, 2011) as the syntax and semantics component, Penn Treebank data for parse-tree disambiguation and IMS(It Makes Sense)(Zhong and Ng, 2010) as a word sense disambiguation tool. Even though the syntax, semantics and parse-tree disambiguation are not the main topics of this paper, we give the full architecture to show where the work reported in this paper fits. Interna</context>
</contexts>
<marker>Bringert, Hallgren, Ranta, 2011</marker>
<rawString>Bringert, B., Hallgren, T., and Ranta., A. (2011). GF resource grammar library synopsis. www.grammaticalframework.org/lib/doc/synopsis.html.</rawString>
</citation>
<citation valid="true">
<authors>
<author>H B Curry</author>
</authors>
<title>Some logical aspects of grammatical structure.</title>
<date>1961</date>
<booktitle>Structure of Language and its Mathematical Aspects: Proceedings of the Twelfth Symposium in Applied Mathematics,</booktitle>
<pages>56--68</pages>
<editor>In Jakobson, R., editor,</editor>
<publisher>American Mathematical Society.</publisher>
<contexts>
<context position="2536" citStr="Curry, 1961" startWordPosition="396" endWordPosition="397">mostly by using English as a pivot language. In this way, it can do with just 2 * 59 = 118 sets of bilingual training data, instead of 60 * 59 = 3540 sets. It would be hard to collect and maintain so many pairs, and in many cases, there is very little data to be found. The roots of an inter-lingua are perhaps in the medieval idea of a universal grammar (Lyons, 1968), in which a universal representation of meaning can be expressed. Translating via this interlingua then also means that meaning is conserved in going from the source to the target language. In recent decades, this idea appears in (Curry, 1961) where the interlingua is called tectogrammar, in the Rosetta project (Rosetta, 1994), building on the semantic models of (Montague, 1974), and in the UNL (Universal Networking Language) project. Incidentally, interlingua is also the heart of modern compiler technology. For instance, the GNU Compiler Collection (Stallman, 2001) uses a shared tree representation to factor out the majority of compilation phases between a large number of source and target languages. Compiler writers save work, and semantics is preserved by design. A compiler, then, is built as a pipeline with parsing from a sourc</context>
</contexts>
<marker>Curry, 1961</marker>
<rawString>Curry, H. B. (1961). Some logical aspects of grammatical structure. In Jakobson, R., editor, Structure of Language and its Mathematical Aspects: Proceedings of the Twelfth Symposium in Applied Mathematics, pages 56–68. American Mathematical Society.</rawString>
</citation>
<citation valid="true">
<authors>
<author>G de Melo</author>
<author>G Weikum</author>
</authors>
<title>Towards a Universal Wordnet by learning from combined evidence.</title>
<date>2009</date>
<booktitle>In Proceedings of the 18th ACM Conference on Information and Knowledge Management (CIKM</booktitle>
<pages>513--522</pages>
<publisher>ACM.</publisher>
<location>New York, NY, USA.</location>
<marker>de Melo, Weikum, 2009</marker>
<rawString>de Melo, G. and Weikum, G. (2009). Towards a Universal Wordnet by learning from combined evidence. In Proceedings of the 18th ACM Conference on Information and Knowledge Management (CIKM 2009), pages 513–522, New York, NY, USA. ACM.</rawString>
</citation>
<citation valid="true">
<authors>
<author>E Delpech</author>
<author>B Daille</author>
<author>E Morin</author>
<author>C Lemaire</author>
</authors>
<title>Extraction of domain-specific bilingual lexicon from comparable corpora: Compositional translation and ranking.</title>
<date>2012</date>
<booktitle>In Proceedings of COLING 2012,</booktitle>
<pages>745--762</pages>
<location>Mumbai,</location>
<contexts>
<context position="26087" citStr="Delpech et al., 2012" startWordPosition="4114" endWordPosition="4117">n has some script-related issues which should be fixed in future. When it comes to interlingua-based arbitrary machine translation, an important concern is the size of lexicons. We are aware of the fact that the size of our lexicons is not comparable to some of the other similar systems such as ATLAS-II (Fujitsu), where the size of lexicons is in millions. We have plan to extend the size of lexicons using some of the other publicly available resources (such as Hindi WordNet) and/or using parallel corpus. The development of bilingual lexicons form parallel corpus have been previously explored (Delpech et al., 2012; Qian et al., 2012), and the same ideas can be applied in our case. 6 Conclusion We have shown how to use existing lexical resources such as WordNets to develop an interlingual translation lexicon in GF, and how to use it for the WSD task in an arbitrary text translation pipeline. The improvements in the translation quality (lexical), shown by examples in Section 4, are encouraging and motivate further work in this direction. However, it should be noted that there is still a lot of work to be done (especially in the open domain text parsing and parse-tree disambiguation phases of the translat</context>
</contexts>
<marker>Delpech, Daille, Morin, Lemaire, 2012</marker>
<rawString>Delpech, E., Daille, B., Morin, E., and Lemaire, C. (2012). Extraction of domain-specific bilingual lexicon from comparable corpora: Compositional translation and ranking. In Proceedings of COLING 2012, pages 745–762, Mumbai, India. The COLING 2012 Organizing Committee.</rawString>
</citation>
<citation valid="true">
<authors>
<author>G Détrez</author>
<author>A Ranta</author>
</authors>
<title>Smart paradigms and the predictability and complexity of inflectional morphology.</title>
<date>2012</date>
<booktitle>In EACL,</booktitle>
<pages>645--653</pages>
<contexts>
<context position="14352" citStr="Détrez and Ranta, 2012" startWordPosition="2234" endWordPosition="2237">is composed of lemma, sense offset and a type ‘t’, where lemma and sense offset are same as in the Princeton WordNet, while ‘t’ is one of the morphological types in GF resource grammars. This abstract representation will serve as a pivot for all concrete representations, which are described next. 2This module has definitions of different morphological and syntactic categories in the GF resource grammar library 58 2.2 GF Concrete Lexicons We build the concrete representations for different languages using the translations obtained from the Universal WordNet data and GF morphological paradigms (Détrez and Ranta, 2012; Bringert et al., 2011). The Universal WordNet translations are tagged with a sense offset from WordNet 3.03 and also with a confidence score. As, an example consider the following segment form the Universal WordNet data, showing German translations for the noun synset with offset ‘13810818’ and lemma ‘rest’ (in the sense of ‘remainder’). n13810818 Rest 1.052756 n13810818 Abbrand 0.95462 n13810818 Ruckstand 0.924376 Each entry is of the following general type. posSenseOffset translation confidence-score If we have more than one candidate translation for the same sense (as in the above case), </context>
</contexts>
<marker>Détrez, Ranta, 2012</marker>
<rawString>Détrez, G. and Ranta, A. (2012). Smart paradigms and the predictability and complexity of inflectional morphology. In EACL, pages 645–653.</rawString>
</citation>
<citation valid="true">
<authors>
<author>M Dymetman</author>
<author>V Lux</author>
<author>A Ranta</author>
</authors>
<title>XML and multilingual document authoring: Convergent trends.</title>
<date>2000</date>
<booktitle>In Proc. Computational Linguistics COLING,</booktitle>
<pages>243--249</pages>
<location>Saarbrücken, Germany,</location>
<contexts>
<context position="6381" citStr="Dymetman et al., 2000" startWordPosition="986" endWordPosition="989"> The parsing and linearization component are defined by using Parallel Multiple Context-Free Grammars (PMCFG, (Seki et al., 1991), (Ljunglöf, 2004)), which give GF an expressive power between mildly and fully context-sensitive grammars. Thus GF can easily handle with language-specific variations in morphology, word order, and discontinuous constituents, while maintaining a shared abstract syntax. Historically, the main use of GF has been in controlled language implementations, e.g., (Ranta and Angelov, 2010; Angelov and Enache, 2010; Ranta et al., 2012) and natural language generation, e.g., (Dymetman et al., 2000), both applied in multilingual settings with up to 15 parallel languages. In recent years, the coverage of GF grammars and the processing performance has enabled open-domain tasks such as treebank parsing (Angelov, 2011) and hybrid translation of patents (Enache et al., 2012). The general purpose Resource Grammar Library (RGL)(Ranta, 2011) has grown to 30 languages. It includes the major European languages, South Asian languages like Hindi/Urdu (Prasad and Shafqat, 2012), Nepali and Punjabi (Shafqat et al., 2011), the Southeast Asian language Thai, and Japanese and Chinese. However, GF has yet</context>
</contexts>
<marker>Dymetman, Lux, Ranta, 2000</marker>
<rawString>Dymetman, M., Lux, V., and Ranta, A. (2000). XML and multilingual document authoring: Convergent trends. In Proc. Computational Linguistics COLING, Saarbrücken, Germany, pages 243–249. International Committee on Computational Linguistics.</rawString>
</citation>
<citation valid="true">
<authors>
<author>R Enache</author>
<author>C España-Bonet</author>
<author>A Ranta</author>
<author>L Márquez</author>
</authors>
<title>A hybrid system for patent translation.</title>
<date>2012</date>
<booktitle>In Proceedings of the 16th Annual Conference of the European Association for Machine Translation (EAMT12),</booktitle>
<location>Trento, Italy.</location>
<contexts>
<context position="6657" citStr="Enache et al., 2012" startWordPosition="1028" endWordPosition="1031">pecific variations in morphology, word order, and discontinuous constituents, while maintaining a shared abstract syntax. Historically, the main use of GF has been in controlled language implementations, e.g., (Ranta and Angelov, 2010; Angelov and Enache, 2010; Ranta et al., 2012) and natural language generation, e.g., (Dymetman et al., 2000), both applied in multilingual settings with up to 15 parallel languages. In recent years, the coverage of GF grammars and the processing performance has enabled open-domain tasks such as treebank parsing (Angelov, 2011) and hybrid translation of patents (Enache et al., 2012). The general purpose Resource Grammar Library (RGL)(Ranta, 2011) has grown to 30 languages. It includes the major European languages, South Asian languages like Hindi/Urdu (Prasad and Shafqat, 2012), Nepali and Punjabi (Shafqat et al., 2011), the Southeast Asian language Thai, and Japanese and Chinese. However, GF has yet not been exploited for arbitrary text parsing and translation. To do this, we have to meet several challenges: robust parsing, parse-tree disambiguation, word sense disambiguation, and development of a wide-coverage interlingual translation lexicon. This paper focuses on the</context>
</contexts>
<marker>Enache, España-Bonet, Ranta, Márquez, 2012</marker>
<rawString>Enache, R., España-Bonet, C., Ranta, A., and Márquez, L. (2012). A hybrid system for patent translation. In Proceedings of the 16th Annual Conference of the European Association for Machine Translation (EAMT12), Trento, Italy.</rawString>
</citation>
<citation valid="true">
<authors>
<author>Hindi-WordNet</author>
</authors>
<title>Hindi Wordnet.</title>
<date>2012</date>
<note>Universal Word—Hindi Lexicon. http://www.cfilt.iitb.ac.in.</note>
<contexts>
<context position="4336" citStr="Hindi-WordNet, 2012" startWordPosition="667" endWordPosition="668">this pipeline and the way a human language translator could work. But how to make it real? How to scale up to the full size of natural languages? 1.2 WordNets In current machine translation research, interlingual methods are marginal, despite the wide use of pivot languages in systems like Google translate. Closest to the mainstream perhaps is the development of linked WordNets. The original Princeton Wordnet for English (Miller, 1995) defines a set of word senses, which many other wordnets map to other languages. Implementations of this idea are Finnish (Lindén and Carlson., 2010) and Hindi (Hindi-WordNet, 2012). In the linked Wordnet approach, the Princeton WordNet senses work as an interlingua, albeit only on the level of the lexicon. (Lindén and Carlson., 2010) give strong arguments why in fact this is a good way to go, despite the often emphasized fact that different languages divide the world in different ways, so that the senses of their word don’t map one to one. The evidence from the English-Finnish case shows that 80% of the mappings are one-to-one and un-problematic. As this part of the lexicon can be easily reused, linguists and system builders can concentrate their effort on the remaining</context>
</contexts>
<marker>Hindi-WordNet, 2012</marker>
<rawString>Hindi-WordNet (2012). Hindi Wordnet. 2012. Universal Word—Hindi Lexicon. http://www.cfilt.iitb.ac.in.</rawString>
</citation>
<citation valid="true">
<authors>
<author>K Lindén</author>
<author>L Carlson</author>
</authors>
<title>Finnwordnet—wordnet på finska via översättning.</title>
<date>2010</date>
<journal>LexicoNordica—Nordic Journal of Lexicography,</journal>
<pages>17--119</pages>
<marker>Lindén, Carlson, 2010</marker>
<rawString>Lindén, K. and Carlson., L. (2010). Finnwordnet—wordnet på finska via översättning. LexicoNordica—Nordic Journal of Lexicography, 17:119–140.</rawString>
</citation>
<citation valid="true">
<authors>
<author>P Ljunglöf</author>
</authors>
<title>The Expressivity and Complexity of Grammatical Framework.</title>
<date>2004</date>
<tech>PhD thesis,</tech>
<institution>Dept. of Computing Science, Chalmers University of Technology and Gothenburg University.</institution>
<note>http://www.cs. chalmers.se/~peb/pubs/p04-PhD-thesis.pdf.</note>
<contexts>
<context position="5906" citStr="Ljunglöf, 2004" startWordPosition="921" endWordPosition="922">rticularly interesting resource is that it is freely available under the most liberal licenses, as is the original Princeton WordNet, 1.3 GF Grammatical Framework (GF)(Ranta, 2004) is a grammar formalism tool based on Martin Löf’s type theory (Martin-Löf, 1982). It can be seen as a tool to build interlingua based translation systems. GF works like a compiler: the source language is parsed to an abstract syntax tree, which is then linearized to the target language. The parsing and linearization component are defined by using Parallel Multiple Context-Free Grammars (PMCFG, (Seki et al., 1991), (Ljunglöf, 2004)), which give GF an expressive power between mildly and fully context-sensitive grammars. Thus GF can easily handle with language-specific variations in morphology, word order, and discontinuous constituents, while maintaining a shared abstract syntax. Historically, the main use of GF has been in controlled language implementations, e.g., (Ranta and Angelov, 2010; Angelov and Enache, 2010; Ranta et al., 2012) and natural language generation, e.g., (Dymetman et al., 2000), both applied in multilingual settings with up to 15 parallel languages. In recent years, the coverage of GF grammars and th</context>
</contexts>
<marker>Ljunglöf, 2004</marker>
<rawString>Ljunglöf, P. (2004). The Expressivity and Complexity of Grammatical Framework. PhD thesis, Dept. of Computing Science, Chalmers University of Technology and Gothenburg University. http://www.cs. chalmers.se/~peb/pubs/p04-PhD-thesis.pdf.</rawString>
</citation>
<citation valid="true">
<authors>
<author>J Lyons</author>
</authors>
<title>Introduction to theoretical linguistics. Cambridge:</title>
<date>1968</date>
<publisher>Cambridge University Press.</publisher>
<contexts>
<context position="2292" citStr="Lyons, 1968" startWordPosition="356" endWordPosition="357">, but this is the first use of these tools with GF. 1 Introduction 1.1 Translation via an interlingua Interlingual translation scales easily up to a large number of languages. Google translate, for example, deals with all pairs of 60 languages mostly by using English as a pivot language. In this way, it can do with just 2 * 59 = 118 sets of bilingual training data, instead of 60 * 59 = 3540 sets. It would be hard to collect and maintain so many pairs, and in many cases, there is very little data to be found. The roots of an inter-lingua are perhaps in the medieval idea of a universal grammar (Lyons, 1968), in which a universal representation of meaning can be expressed. Translating via this interlingua then also means that meaning is conserved in going from the source to the target language. In recent decades, this idea appears in (Curry, 1961) where the interlingua is called tectogrammar, in the Rosetta project (Rosetta, 1994), building on the semantic models of (Montague, 1974), and in the UNL (Universal Networking Language) project. Incidentally, interlingua is also the heart of modern compiler technology. For instance, the GNU Compiler Collection (Stallman, 2001) uses a shared tree represe</context>
</contexts>
<marker>Lyons, 1968</marker>
<rawString>Lyons, J. (1968). Introduction to theoretical linguistics. Cambridge: Cambridge University Press.</rawString>
</citation>
<citation valid="true">
<authors>
<author>P Martin-Löf</author>
</authors>
<title>Constructive mathematics and computer programming.</title>
<date>1982</date>
<booktitle>Methodology and Philosophy of Science VI,</booktitle>
<pages>153--175</pages>
<editor>In Cohen, Los, Pfeiffer, and Podewski, editors, Logic,</editor>
<publisher>NorthHolland,</publisher>
<location>Amsterdam.</location>
<contexts>
<context position="5552" citStr="Martin-Löf, 1982" startWordPosition="864" endWordPosition="865">g 20%. The Universal WordNet (de Melo and Weikum, 2009) works on the same lines. Building on the Princeton WordNet, it populates the mappings to over 200 different languages by collecting data from different sources (such as the Wikipedia) and using supervised machine learning techniques to propagate the knowledge and infer more of it. What makes it a particularly interesting resource is that it is freely available under the most liberal licenses, as is the original Princeton WordNet, 1.3 GF Grammatical Framework (GF)(Ranta, 2004) is a grammar formalism tool based on Martin Löf’s type theory (Martin-Löf, 1982). It can be seen as a tool to build interlingua based translation systems. GF works like a compiler: the source language is parsed to an abstract syntax tree, which is then linearized to the target language. The parsing and linearization component are defined by using Parallel Multiple Context-Free Grammars (PMCFG, (Seki et al., 1991), (Ljunglöf, 2004)), which give GF an expressive power between mildly and fully context-sensitive grammars. Thus GF can easily handle with language-specific variations in morphology, word order, and discontinuous constituents, while maintaining a shared abstract s</context>
</contexts>
<marker>Martin-Löf, 1982</marker>
<rawString>Martin-Löf, P. (1982). Constructive mathematics and computer programming. In Cohen, Los, Pfeiffer, and Podewski, editors, Logic, Methodology and Philosophy of Science VI, pages 153–175. NorthHolland, Amsterdam.</rawString>
</citation>
<citation valid="true">
<authors>
<author>G A Miller</author>
</authors>
<title>Wordnet: A lexical database for English.</title>
<date>1995</date>
<journal>Communications of the ACM,</journal>
<pages>38--39</pages>
<contexts>
<context position="4155" citStr="Miller, 1995" startWordPosition="638" endWordPosition="639">th and Southeast Asian NLP, 25th International Conference on Computational Linguistics, pages 55–64, Dublin, Ireland, August 23-29 2014. It is easy to see an analogy between this pipeline and the way a human language translator could work. But how to make it real? How to scale up to the full size of natural languages? 1.2 WordNets In current machine translation research, interlingual methods are marginal, despite the wide use of pivot languages in systems like Google translate. Closest to the mainstream perhaps is the development of linked WordNets. The original Princeton Wordnet for English (Miller, 1995) defines a set of word senses, which many other wordnets map to other languages. Implementations of this idea are Finnish (Lindén and Carlson., 2010) and Hindi (Hindi-WordNet, 2012). In the linked Wordnet approach, the Princeton WordNet senses work as an interlingua, albeit only on the level of the lexicon. (Lindén and Carlson., 2010) give strong arguments why in fact this is a good way to go, despite the often emphasized fact that different languages divide the world in different ways, so that the senses of their word don’t map one to one. The evidence from the English-Finnish case shows that</context>
<context position="8309" citStr="Miller, 1995" startWordPosition="1287" endWordPosition="1288">here can apply to any language, it is particularly interesting for South Asian languages. In these languages, statistical tools do not have much bilingual training data to work on, so Google translate and similar tools are not as useful as they are with better resourced languages. At the same time, there is an urgent and widely recognised need for translations from English to the various languages of South Asia. Fortunately, word nets are being built for many of them, so that the techniques described here can be applied. 2 From Universal WordNet to a GF Lexicon The original Princeton WordNet (Miller, 1995) defines a set of word senses, and the Universal WordNet (de Melo and Weikum, 2009) maps them to different languages. In this multilingual scenario, the Princeton WordNet senses can be seen as an abstract representation, while the Universal WordNet mappings can be seen as concrete representation of those senses in different languages. GF grammars use very much the same technique of one common abstract and multiple parallel concrete representations to achieve multilingualism. Due to this compatibility, it is easy to build a multilingual GF lexicon using data from those two resources (i.e. Princ</context>
</contexts>
<marker>Miller, 1995</marker>
<rawString>Miller, G. A. (1995). Wordnet: A lexical database for English. Communications of the ACM, 38:39–41.</rawString>
</citation>
<citation valid="true">
<authors>
<author>R Montague</author>
</authors>
<title>Formal Philosophy.</title>
<date>1974</date>
<publisher>Yale University Press,</publisher>
<location>New</location>
<contexts>
<context position="2674" citStr="Montague, 1974" startWordPosition="416" endWordPosition="417">60 * 59 = 3540 sets. It would be hard to collect and maintain so many pairs, and in many cases, there is very little data to be found. The roots of an inter-lingua are perhaps in the medieval idea of a universal grammar (Lyons, 1968), in which a universal representation of meaning can be expressed. Translating via this interlingua then also means that meaning is conserved in going from the source to the target language. In recent decades, this idea appears in (Curry, 1961) where the interlingua is called tectogrammar, in the Rosetta project (Rosetta, 1994), building on the semantic models of (Montague, 1974), and in the UNL (Universal Networking Language) project. Incidentally, interlingua is also the heart of modern compiler technology. For instance, the GNU Compiler Collection (Stallman, 2001) uses a shared tree representation to factor out the majority of compilation phases between a large number of source and target languages. Compiler writers save work, and semantics is preserved by design. A compiler, then, is built as a pipeline with parsing from a source language to an abstract syntax tree, which is analyzed and optimized in the language-independent phases, and finally linearized to a tar</context>
</contexts>
<marker>Montague, 1974</marker>
<rawString>Montague, R. (1974). Formal Philosophy. Yale University Press, New Haven. Collected papers edited by Richmond Thomason.</rawString>
</citation>
<citation valid="true">
<authors>
<author>K V S Prasad</author>
<author>M V Shafqat</author>
</authors>
<title>Computational evidence that Hindi and Urdu share a grammar but not the lexicon.</title>
<date>2012</date>
<booktitle>In The 3rd Workshop on South and Southeast Asian NLP, COLING.</booktitle>
<contexts>
<context position="6856" citStr="Prasad and Shafqat, 2012" startWordPosition="1056" endWordPosition="1059">ntations, e.g., (Ranta and Angelov, 2010; Angelov and Enache, 2010; Ranta et al., 2012) and natural language generation, e.g., (Dymetman et al., 2000), both applied in multilingual settings with up to 15 parallel languages. In recent years, the coverage of GF grammars and the processing performance has enabled open-domain tasks such as treebank parsing (Angelov, 2011) and hybrid translation of patents (Enache et al., 2012). The general purpose Resource Grammar Library (RGL)(Ranta, 2011) has grown to 30 languages. It includes the major European languages, South Asian languages like Hindi/Urdu (Prasad and Shafqat, 2012), Nepali and Punjabi (Shafqat et al., 2011), the Southeast Asian language Thai, and Japanese and Chinese. However, GF has yet not been exploited for arbitrary text parsing and translation. To do this, we have to meet several challenges: robust parsing, parse-tree disambiguation, word sense disambiguation, and development of a wide-coverage interlingual translation lexicon. This paper focuses on the latter two. We report first a method of using the WordNets (Princeton and Universal) to build an interlingual full-form, multiple sense translation lexicon. Then, we show how these lexicons together</context>
<context position="24338" citStr="Prasad and Shafqat, 2012" startWordPosition="3822" endWordPosition="3825"> polygon Without WSD tum &amp;quot;recall may&amp;quot; ho ki ṭrāyengl &amp;quot;also&amp;quot; bahubhuj has With WSD tum smaraṇ kar sakte ho ki trikoṇ bh3 bahubhuj has Word order You recall do can that triangle also polygon is Comments The version without WSD has several missing words. The WSD version of “recall” is not idiomatic, but understandable. It should be noted that the coverage of the Hindi lexicon is lowest of all the lexicons given in Table 1. The result is that many sentences have missing words in the translations. Also, there is considerable interference with Urdu words (some stemming from the shared base grammar (Prasad and Shafqat, 2012)). Further, some mappings coming from the Universal WordNet data are in roman, as opposed to Devanagari (the usual script for Hindi, and what the grammar is based on), so these need to be transcribed. Finally, idiomatic phrases are a problem (“before the law” is likely to be rendered “(temporally) before the law” rather than “in the eyes of the law”). 5 The next steps Since the Universal WordNet mappings are produced from parallel data by machine learning techniques, the translations are not always accurate and do not always make the best possible choice. This leaves a window for improvement i</context>
</contexts>
<marker>Prasad, Shafqat, 2012</marker>
<rawString>Prasad, K. V. S. and Shafqat, M. V. (2012). Computational evidence that Hindi and Urdu share a grammar but not the lexicon. In The 3rd Workshop on South and Southeast Asian NLP, COLING.</rawString>
</citation>
<citation valid="true">
<authors>
<author>L Qian</author>
<author>H Wang</author>
<author>G Zhou</author>
<author>Q Zhu</author>
</authors>
<title>Bilingual lexicon construction from comparable corpora via dependency mapping.</title>
<date>2012</date>
<booktitle>In Proceedings of COLING 2012,</booktitle>
<pages>2275--2290</pages>
<location>Mumbai,</location>
<contexts>
<context position="26107" citStr="Qian et al., 2012" startWordPosition="4118" endWordPosition="4121">ted issues which should be fixed in future. When it comes to interlingua-based arbitrary machine translation, an important concern is the size of lexicons. We are aware of the fact that the size of our lexicons is not comparable to some of the other similar systems such as ATLAS-II (Fujitsu), where the size of lexicons is in millions. We have plan to extend the size of lexicons using some of the other publicly available resources (such as Hindi WordNet) and/or using parallel corpus. The development of bilingual lexicons form parallel corpus have been previously explored (Delpech et al., 2012; Qian et al., 2012), and the same ideas can be applied in our case. 6 Conclusion We have shown how to use existing lexical resources such as WordNets to develop an interlingual translation lexicon in GF, and how to use it for the WSD task in an arbitrary text translation pipeline. The improvements in the translation quality (lexical), shown by examples in Section 4, are encouraging and motivate further work in this direction. However, it should be noted that there is still a lot of work to be done (especially in the open domain text parsing and parse-tree disambiguation phases of the translation pipeline) to bri</context>
</contexts>
<marker>Qian, Wang, Zhou, Zhu, 2012</marker>
<rawString>Qian, L., Wang, H., Zhou, G., and Zhu, Q. (2012). Bilingual lexicon construction from comparable corpora via dependency mapping. In Proceedings of COLING 2012, pages 2275–2290, Mumbai, India. The COLING 2012 Organizing Committee.</rawString>
</citation>
<citation valid="true">
<authors>
<author>A Ranta</author>
</authors>
<title>Grammatical Framework: A Type-Theoretical Grammar Formalism.</title>
<date>2004</date>
<journal>The Journal of Functional Programming,</journal>
<volume>14</volume>
<issue>2</issue>
<note>http://www.cse.chalmers.se/~aarne/articles/gf-jfp. pdf.</note>
<contexts>
<context position="5471" citStr="Ranta, 2004" startWordPosition="851" endWordPosition="852">, linguists and system builders can concentrate their effort on the remaining 20%. The Universal WordNet (de Melo and Weikum, 2009) works on the same lines. Building on the Princeton WordNet, it populates the mappings to over 200 different languages by collecting data from different sources (such as the Wikipedia) and using supervised machine learning techniques to propagate the knowledge and infer more of it. What makes it a particularly interesting resource is that it is freely available under the most liberal licenses, as is the original Princeton WordNet, 1.3 GF Grammatical Framework (GF)(Ranta, 2004) is a grammar formalism tool based on Martin Löf’s type theory (Martin-Löf, 1982). It can be seen as a tool to build interlingua based translation systems. GF works like a compiler: the source language is parsed to an abstract syntax tree, which is then linearized to the target language. The parsing and linearization component are defined by using Parallel Multiple Context-Free Grammars (PMCFG, (Seki et al., 1991), (Ljunglöf, 2004)), which give GF an expressive power between mildly and fully context-sensitive grammars. Thus GF can easily handle with language-specific variations in morphology, </context>
</contexts>
<marker>Ranta, 2004</marker>
<rawString>Ranta, A. (2004). Grammatical Framework: A Type-Theoretical Grammar Formalism. The Journal of Functional Programming, 14(2):145–189. http://www.cse.chalmers.se/~aarne/articles/gf-jfp. pdf.</rawString>
</citation>
<citation valid="true">
<authors>
<author>A Ranta</author>
</authors>
<title>Grammatical Framework: Programming with Multilingual Grammars.</title>
<date>2011</date>
<volume>10</volume>
<pages>1--57586</pages>
<publisher>CSLI Publications,</publisher>
<location>Stanford.</location>
<contexts>
<context position="6722" citStr="Ranta, 2011" startWordPosition="1038" endWordPosition="1039">nts, while maintaining a shared abstract syntax. Historically, the main use of GF has been in controlled language implementations, e.g., (Ranta and Angelov, 2010; Angelov and Enache, 2010; Ranta et al., 2012) and natural language generation, e.g., (Dymetman et al., 2000), both applied in multilingual settings with up to 15 parallel languages. In recent years, the coverage of GF grammars and the processing performance has enabled open-domain tasks such as treebank parsing (Angelov, 2011) and hybrid translation of patents (Enache et al., 2012). The general purpose Resource Grammar Library (RGL)(Ranta, 2011) has grown to 30 languages. It includes the major European languages, South Asian languages like Hindi/Urdu (Prasad and Shafqat, 2012), Nepali and Punjabi (Shafqat et al., 2011), the Southeast Asian language Thai, and Japanese and Chinese. However, GF has yet not been exploited for arbitrary text parsing and translation. To do this, we have to meet several challenges: robust parsing, parse-tree disambiguation, word sense disambiguation, and development of a wide-coverage interlingual translation lexicon. This paper focuses on the latter two. We report first a method of using the WordNets (Prin</context>
<context position="17196" citStr="Ranta, 2011" startWordPosition="2653" endWordPosition="2654"> some statistics about the coverage of these lexicons. Language Number of Entries Language Number of Entries Abstract 91516 German 49439 French 38261 Finnish 27673 Swedish 23862 Hindi 16654 Bulgarian 12425 Table 1: Lexicon Coverage Statistics 3However, in our concrete lexicons we match them to WordNet 1.7.1 for the reasons mentioned previously 4See (Bringert et al., 2011) for more details on these paradigms 59 3 System architecture Figure 1 shows an architecture of the translation pipeline. The architecture is inter-lingual and uses the Resource Grammar Library (RGL) of Grammatical Framework (Ranta, 2011) as the syntax and semantics component, Penn Treebank data for parse-tree disambiguation and IMS(It Makes Sense)(Zhong and Ng, 2010) as a word sense disambiguation tool. Even though the syntax, semantics and parse-tree disambiguation are not the main topics of this paper, we give the full architecture to show where the work reported in this paper fits. Internal GF resources (e.g. resource grammars and dictionaries) are shown in rectangles while the external components (e.g. PennTreebank and IMS(Zhong and Ng, 2010): a wide coverage word sense disambiguation system for arbitrary text.) are shown</context>
</contexts>
<marker>Ranta, 2011</marker>
<rawString>Ranta, A. (2011). Grammatical Framework: Programming with Multilingual Grammars. CSLI Publications, Stanford. ISBN-10: 1-57586-626-9 (Paper), 1-57586-627-7 (Cloth).</rawString>
</citation>
<citation valid="true">
<authors>
<author>A Ranta</author>
<author>K Angelov</author>
</authors>
<title>Implementing Controlled Languages in GF.</title>
<date>2010</date>
<booktitle>In Proceedings of CNL2009, Athens,</booktitle>
<volume>5972</volume>
<pages>82--101</pages>
<contexts>
<context position="6271" citStr="Ranta and Angelov, 2010" startWordPosition="969" endWordPosition="972">ler: the source language is parsed to an abstract syntax tree, which is then linearized to the target language. The parsing and linearization component are defined by using Parallel Multiple Context-Free Grammars (PMCFG, (Seki et al., 1991), (Ljunglöf, 2004)), which give GF an expressive power between mildly and fully context-sensitive grammars. Thus GF can easily handle with language-specific variations in morphology, word order, and discontinuous constituents, while maintaining a shared abstract syntax. Historically, the main use of GF has been in controlled language implementations, e.g., (Ranta and Angelov, 2010; Angelov and Enache, 2010; Ranta et al., 2012) and natural language generation, e.g., (Dymetman et al., 2000), both applied in multilingual settings with up to 15 parallel languages. In recent years, the coverage of GF grammars and the processing performance has enabled open-domain tasks such as treebank parsing (Angelov, 2011) and hybrid translation of patents (Enache et al., 2012). The general purpose Resource Grammar Library (RGL)(Ranta, 2011) has grown to 30 languages. It includes the major European languages, South Asian languages like Hindi/Urdu (Prasad and Shafqat, 2012), Nepali and Pu</context>
</contexts>
<marker>Ranta, Angelov, 2010</marker>
<rawString>Ranta, A. and Angelov, K. (2010). Implementing Controlled Languages in GF. In Proceedings of CNL2009, Athens, volume 5972 of LNCS, pages 82–101.</rawString>
</citation>
<citation valid="true">
<authors>
<author>A Ranta</author>
<author>G Détrez</author>
<author>R Enache</author>
</authors>
<title>Controlled language for everyday use: the MOLTO phrasebook.</title>
<date>2012</date>
<booktitle>In CNL 2012: Controlled Natural Language,</booktitle>
<volume>7175</volume>
<contexts>
<context position="6318" citStr="Ranta et al., 2012" startWordPosition="977" endWordPosition="980">yntax tree, which is then linearized to the target language. The parsing and linearization component are defined by using Parallel Multiple Context-Free Grammars (PMCFG, (Seki et al., 1991), (Ljunglöf, 2004)), which give GF an expressive power between mildly and fully context-sensitive grammars. Thus GF can easily handle with language-specific variations in morphology, word order, and discontinuous constituents, while maintaining a shared abstract syntax. Historically, the main use of GF has been in controlled language implementations, e.g., (Ranta and Angelov, 2010; Angelov and Enache, 2010; Ranta et al., 2012) and natural language generation, e.g., (Dymetman et al., 2000), both applied in multilingual settings with up to 15 parallel languages. In recent years, the coverage of GF grammars and the processing performance has enabled open-domain tasks such as treebank parsing (Angelov, 2011) and hybrid translation of patents (Enache et al., 2012). The general purpose Resource Grammar Library (RGL)(Ranta, 2011) has grown to 30 languages. It includes the major European languages, South Asian languages like Hindi/Urdu (Prasad and Shafqat, 2012), Nepali and Punjabi (Shafqat et al., 2011), the Southeast Asi</context>
</contexts>
<marker>Ranta, Détrez, Enache, 2012</marker>
<rawString>Ranta, A., Détrez, G., and Enache, R. (2012). Controlled language for everyday use: the MOLTO phrasebook. In CNL 2012: Controlled Natural Language, volume 7175 of LNCS/LNAI.</rawString>
</citation>
<citation valid="true">
<authors>
<author>M T Rosetta</author>
</authors>
<title>Compositional Translation.</title>
<date>1994</date>
<publisher>Kluwer,</publisher>
<location>Dordrecht.</location>
<contexts>
<context position="2621" citStr="Rosetta, 1994" startWordPosition="408" endWordPosition="409">9 = 118 sets of bilingual training data, instead of 60 * 59 = 3540 sets. It would be hard to collect and maintain so many pairs, and in many cases, there is very little data to be found. The roots of an inter-lingua are perhaps in the medieval idea of a universal grammar (Lyons, 1968), in which a universal representation of meaning can be expressed. Translating via this interlingua then also means that meaning is conserved in going from the source to the target language. In recent decades, this idea appears in (Curry, 1961) where the interlingua is called tectogrammar, in the Rosetta project (Rosetta, 1994), building on the semantic models of (Montague, 1974), and in the UNL (Universal Networking Language) project. Incidentally, interlingua is also the heart of modern compiler technology. For instance, the GNU Compiler Collection (Stallman, 2001) uses a shared tree representation to factor out the majority of compilation phases between a large number of source and target languages. Compiler writers save work, and semantics is preserved by design. A compiler, then, is built as a pipeline with parsing from a source language to an abstract syntax tree, which is analyzed and optimized in the languag</context>
</contexts>
<marker>Rosetta, 1994</marker>
<rawString>Rosetta, M. T. (1994). Compositional Translation. Kluwer, Dordrecht.</rawString>
</citation>
<citation valid="true">
<authors>
<author>H Seki</author>
<author>T Matsumura</author>
<author>M Fujii</author>
<author>T Kasami</author>
</authors>
<title>On multiple context-free grammars.</title>
<date>1991</date>
<journal>Theoretical Computer Science,</journal>
<pages>88--191</pages>
<contexts>
<context position="5888" citStr="Seki et al., 1991" startWordPosition="917" endWordPosition="920">t. What makes it a particularly interesting resource is that it is freely available under the most liberal licenses, as is the original Princeton WordNet, 1.3 GF Grammatical Framework (GF)(Ranta, 2004) is a grammar formalism tool based on Martin Löf’s type theory (Martin-Löf, 1982). It can be seen as a tool to build interlingua based translation systems. GF works like a compiler: the source language is parsed to an abstract syntax tree, which is then linearized to the target language. The parsing and linearization component are defined by using Parallel Multiple Context-Free Grammars (PMCFG, (Seki et al., 1991), (Ljunglöf, 2004)), which give GF an expressive power between mildly and fully context-sensitive grammars. Thus GF can easily handle with language-specific variations in morphology, word order, and discontinuous constituents, while maintaining a shared abstract syntax. Historically, the main use of GF has been in controlled language implementations, e.g., (Ranta and Angelov, 2010; Angelov and Enache, 2010; Ranta et al., 2012) and natural language generation, e.g., (Dymetman et al., 2000), both applied in multilingual settings with up to 15 parallel languages. In recent years, the coverage of </context>
</contexts>
<marker>Seki, Matsumura, Fujii, Kasami, 1991</marker>
<rawString>Seki, H., Matsumura, T., Fujii, M., and Kasami, T. (1991). On multiple context-free grammars. Theoretical Computer Science, 88:191–229.</rawString>
</citation>
<citation valid="true">
<authors>
<author>M Shafqat</author>
<author>M Humayoun</author>
<author>R Aarne</author>
</authors>
<title>An open source Punjabi resource grammar.</title>
<date>2011</date>
<journal>Organising Committee.</journal>
<booktitle>In Proceedings of the International Conference Recent Advances in Natural Language Processing</booktitle>
<pages>70--76</pages>
<location>Hissar, Bulgaria. RANLP</location>
<contexts>
<context position="6899" citStr="Shafqat et al., 2011" startWordPosition="1063" endWordPosition="1066">ov and Enache, 2010; Ranta et al., 2012) and natural language generation, e.g., (Dymetman et al., 2000), both applied in multilingual settings with up to 15 parallel languages. In recent years, the coverage of GF grammars and the processing performance has enabled open-domain tasks such as treebank parsing (Angelov, 2011) and hybrid translation of patents (Enache et al., 2012). The general purpose Resource Grammar Library (RGL)(Ranta, 2011) has grown to 30 languages. It includes the major European languages, South Asian languages like Hindi/Urdu (Prasad and Shafqat, 2012), Nepali and Punjabi (Shafqat et al., 2011), the Southeast Asian language Thai, and Japanese and Chinese. However, GF has yet not been exploited for arbitrary text parsing and translation. To do this, we have to meet several challenges: robust parsing, parse-tree disambiguation, word sense disambiguation, and development of a wide-coverage interlingual translation lexicon. This paper focuses on the latter two. We report first a method of using the WordNets (Princeton and Universal) to build an interlingual full-form, multiple sense translation lexicon. Then, we show how these lexicons together with a word sense disambiguation tool can </context>
</contexts>
<marker>Shafqat, Humayoun, Aarne, 2011</marker>
<rawString>Shafqat, M., Humayoun, M., and Aarne, R. (2011). An open source Punjabi resource grammar. In Proceedings of the International Conference Recent Advances in Natural Language Processing 2011, pages 70–76, Hissar, Bulgaria. RANLP 2011 Organising Committee. http://aclweb.org/anthology/R11-1010.</rawString>
</citation>
<citation valid="true">
<authors>
<author>R Stallman</author>
</authors>
<title>Using and Porting the GNU Compiler Collection. Free Software Foundation.</title>
<date>2001</date>
<contexts>
<context position="2865" citStr="Stallman, 2001" startWordPosition="442" endWordPosition="443"> idea of a universal grammar (Lyons, 1968), in which a universal representation of meaning can be expressed. Translating via this interlingua then also means that meaning is conserved in going from the source to the target language. In recent decades, this idea appears in (Curry, 1961) where the interlingua is called tectogrammar, in the Rosetta project (Rosetta, 1994), building on the semantic models of (Montague, 1974), and in the UNL (Universal Networking Language) project. Incidentally, interlingua is also the heart of modern compiler technology. For instance, the GNU Compiler Collection (Stallman, 2001) uses a shared tree representation to factor out the majority of compilation phases between a large number of source and target languages. Compiler writers save work, and semantics is preserved by design. A compiler, then, is built as a pipeline with parsing from a source language to an abstract syntax tree, which is analyzed and optimized in the language-independent phases, and finally linearized to a target language. This work is licenced under a Creative Commons Attribution 4.0 International License. Page numbers and proceedings footer are added by the organizers. License details: http://cr</context>
</contexts>
<marker>Stallman, 2001</marker>
<rawString>Stallman, R. (2001). Using and Porting the GNU Compiler Collection. Free Software Foundation.</rawString>
</citation>
<citation valid="true">
<authors>
<author>Z Zhong</author>
<author>H T Ng</author>
</authors>
<title>It makes sense: A wide-coverage word sense disambiguation system for free text.</title>
<date>2010</date>
<booktitle>In Proceedings of the ACL 2010 System Demonstrations,</booktitle>
<pages>78--83</pages>
<location>Uppsala,</location>
<contexts>
<context position="17328" citStr="Zhong and Ng, 2010" startWordPosition="2670" endWordPosition="2673">erman 49439 French 38261 Finnish 27673 Swedish 23862 Hindi 16654 Bulgarian 12425 Table 1: Lexicon Coverage Statistics 3However, in our concrete lexicons we match them to WordNet 1.7.1 for the reasons mentioned previously 4See (Bringert et al., 2011) for more details on these paradigms 59 3 System architecture Figure 1 shows an architecture of the translation pipeline. The architecture is inter-lingual and uses the Resource Grammar Library (RGL) of Grammatical Framework (Ranta, 2011) as the syntax and semantics component, Penn Treebank data for parse-tree disambiguation and IMS(It Makes Sense)(Zhong and Ng, 2010) as a word sense disambiguation tool. Even though the syntax, semantics and parse-tree disambiguation are not the main topics of this paper, we give the full architecture to show where the work reported in this paper fits. Internal GF resources (e.g. resource grammars and dictionaries) are shown in rectangles while the external components (e.g. PennTreebank and IMS(Zhong and Ng, 2010): a wide coverage word sense disambiguation system for arbitrary text.) are shown in double-stroked rectangles. With reference to Figure 1: The input is parsed using English resource grammar (EngRG) and a comprehe</context>
</contexts>
<marker>Zhong, Ng, 2010</marker>
<rawString>Zhong, Z. and Ng, H. T. (2010). It makes sense: A wide-coverage word sense disambiguation system for free text. In Proceedings of the ACL 2010 System Demonstrations, pages 78–83, Uppsala, Sweden. Association for Computational Linguistics. http://www.aclweb.org/anthology/P10-4014.</rawString>
</citation>
</citationList>
</algorithm>
</algorithms>
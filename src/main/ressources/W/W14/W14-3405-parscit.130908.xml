<?xml version="1.0" encoding="UTF-8"?>
<algorithms version="110505">
<algorithm name="SectLabel" version="110505">
<variant no="0" confidence="0.000416">
<title confidence="0.988702">
Decomposing Consumer Health Questions
</title>
<author confidence="0.998357">
Kirk Roberts, Halil Kilicoglu, Marcelo Fiszman, and Dina Demner-Fushman
</author>
<affiliation confidence="0.974552">
National Library of Medicine
National Institutes of Health
</affiliation>
<address confidence="0.670922">
Bethesda, MD 20894
</address>
<email confidence="0.994847">
robertske@nih.gov,{kilicogluh,fiszmanm,ddemner}@mail.nih.gov
</email>
<sectionHeader confidence="0.995551" genericHeader="abstract">
Abstract
</sectionHeader>
<bodyText confidence="0.999986705882353">
This paper presents a method for decom-
posing long, complex consumer health
questions. Our approach largely decom-
poses questions using their syntactic struc-
ture, recognizing independent questions
embedded in clauses, as well as coordi-
nations and exemplifying phrases. Addi-
tionally, we identify elements specific to
disease-related consumer health questions,
such as the focus disease and background
information. To achieve this, our approach
combines rank-and-filter machine learning
methods with rule-based methods. Our
results demonstrate significant improve-
ments over the heuristic methods typically
employed for question decomposition that
rely only on the syntactic parse tree.
</bodyText>
<sectionHeader confidence="0.999133" genericHeader="keywords">
1 Introduction
</sectionHeader>
<bodyText confidence="0.9998475">
Natural language questions provide an intuitive
method for consumers (non-experts) to query for
health-related content. The most intuitive way
for consumers to formulate written questions is
the same way they write to other humans: multi-
sentence, complex questions that contain back-
ground information and often more than one spe-
cific question. Consider the following:
</bodyText>
<listItem confidence="0.8053865">
• Will Fabry disease affect a transplanted kidney?
Previous to the transplant the disease was be-
</listItem>
<bodyText confidence="0.951177222222222">
ing managed with an enzyme supplement. Will
this need to be continued? What cautions or ad-
ditional treatments are required to manage the
disease with a transplanted kidney?
This complex question contains three question
sentences and one background sentence. The fo-
cus (Fabry disease) is stated in the first question
but is necessary for a full understanding of the
other questions as well. The background sentence
is necessary to understand the second question:
the anaphor this must be resolved to an enzyme
treatment, and the predicate continue’s implicit ar-
gument that must be re-constructed from the dis-
course (i.e., continue after a kidney transplant).
The final question sentence uses a coordination
to ask two separate questions (cautions and addi-
tional treatments). A decomposition of this com-
plex question would then result in four questions:
</bodyText>
<listItem confidence="0.998422285714286">
1. Will Fabry disease affect a transplanted kidney?
2. Will enzyme treatment for Fabry disease need to
be continued after a kidney transplant?
3. What cautions are required to manage Fabry
disease with a transplanted kidney?
4. What additional treatments are required to man-
age Fabry disease with a transplanted kidney?
</listItem>
<bodyText confidence="0.999723153846154">
Each question above could be independently an-
swered by a question answering (QA) system.
While previous work has discussed methods for
resolving co-reference and implicit arguments in
consumer health questions (Kilicoglu et al., 2013),
it does not address question decomposition.
In this work, we propose methods for auto-
matically recognizing six annotation types use-
ful for decomposing consumer health questions.
These annotations distinguish between sentences
that contain questions and background informa-
tion. They also identify when a question sentence
can be split in multiple independent questions, and
</bodyText>
<page confidence="0.987912">
29
</page>
<note confidence="0.787181">
Proceedings of the 2014 Workshop on Biomedical Natural Language Processing (BioNLP 2014), pages 29–37,
Baltimore, Maryland USA, June 26-27 2014. c�2014 Association for Computational Linguistics
</note>
<bodyText confidence="0.999760083333333">
when they contain optional or coordinated infor-
mation embedded within a question.
For each of these decomposition annotations,
we propose a combination of machine learning
(ML) and rule based methods. The ML methods
largely take the form of a 3-step rank-and-filter
approach, where candidates are generated, ranked
by an ML classifier, then the top-ranked candidate
is passed through a separate ML filtering classi-
fier. We evaluate each of these methods on a set of
1,467 consumer health questions related to genetic
and rare diseases.
</bodyText>
<sectionHeader confidence="0.985165" genericHeader="introduction">
2 Background
</sectionHeader>
<bodyText confidence="0.999979333333334">
QA in the biomedical domain has been well-
studied (Demner-Fushman and Lin, 2007; Cairns
et al., 2011; Cao et al., 2011) as a means for re-
trieving medical information. This work has typ-
ically focused, however, on questions posed by
medical professionals, and the methods proposed
for question analysis generally assume a single,
concise question. For example, Demner-Fushman
and Abhyankar (2012) propose a method for ex-
tracting frames from queries for the purpose of
cohort retrieval. Their method assumes syntactic
dependencies exist between the necessary frame
elements, and is thus not well-suited to handle
long, multi-sentence questions. Similarly, Ander-
sen et al. (2012) proposes a method for converting
a concise question into a structured query. How-
ever, many medical questions require background
information that is difficult to encode in a single
question sentence. Instead, it is often more natural
to ask multiple questions over several sentences,
providing background information to give context
to the questions. Yu and Cao (2008) use a ML
method to recognize question types in professional
health questions. Their method can identify more
than one type per complex question. Without de-
composing the full question into its sub-questions,
however, the type cannot be associated with its
specific span, or with other information specific to
the sub-question. This other information can in-
clude answer types, question focus, and other an-
swer constraints. By decomposing multi-sentence
questions, these question-specific attributes can be
extracted, and the discourse structure of the larger
question can be better understood.
Question decomposition has been utilized be-
fore in open-domain QA approaches, but rarely
evaluated on its own. Lacatusu et al. (2006)
demonstrates how question decomposition can im-
prove the performance of a multi-sentence sum-
marization system. They perform what we refer
to as syntactic question decomposition, where the
syntactic structure of the question is used to iden-
tify sub-questions that can be answered in isola-
tion. A second form of question decomposition is
semantic decomposition, which can semantically
break individual questions apart to answer them
in stages. For instance, the question “When did
the third U.S. President die?” can be semantically
decomposed “Who was the third U.S. President?”
and “When did X die?”, where the answer to the
first question is substituted into the second. Katz
and Grau (2005) discusses this kind of decompo-
sition using the syntactic structure, though it is not
empirically validated. Hartrumpf (2008) proposes
a decomposition method using only the deep se-
mantic structure. Finally, Harabagiu et al. (2006)
proposes a different type of question decomposi-
tion based on a random walk over similar ques-
tions extracted from a corpus. In our work, we
focus on syntactic question decomposition. We
demonstrate the importance of empirical evalua-
tion of question decomposition, notably the pit-
falls of heuristic approaches that rely entirely on
the syntactic parse tree. Syntactic parsers trained
on Treebank are particularly poor at both analyz-
ing questions (Judge et al., 2006) and coordination
boundaries (Hogan, 2007). Robust question de-
composition methods, therefore, must be able to
overcome many of these difficulties.
</bodyText>
<sectionHeader confidence="0.9840275" genericHeader="method">
3 Consumer Health Question
Decomposition
</sectionHeader>
<bodyText confidence="0.9988188">
Our goal is to decompose multi-sentence, multi-
faceted consumer health questions into concise
questions coupled with important contextual in-
formation. To this end, we utilize a set of an-
notations that identify the decomposable elements
and important contextual elements. A more de-
tailed description of these annotations is provided
in Roberts et al. (2014). The annotations are pub-
licly available at our institution website1. Here, we
briefly describe each annotation:
</bodyText>
<listItem confidence="0.99309025">
(1) BACKGROUND - a sentence indicating useful
contextual information, but lacks a question.
(2) QUESTION - a sentence or clause that indi-
cates an independent question.
</listItem>
<footnote confidence="0.9409395">
1http://lhncbc.nlm.nih.gov/project/consumer-health-
question-answering
</footnote>
<page confidence="0.993769">
30
</page>
<figure confidence="0.985641857142857">
Coordination Recognition
Question Recognition
SVM Candidate Ranking
Candidate Generation
Candidate Generation
SVM Candidate Filter
SVM Candidate Filter
Question
Request
Question
Sentence
WordNet
Stanford
Parser
Coordination
Sentence Classification
SVM Sentence Classification
Exemplification Recognition
Sentence Splitting
Ignore
Sentence
SVM Candidate Ranking
Candidate Generation
Exemplification
Candidate Filter
Background
Sentence
UMLS
Background Classification
SVM Family History Classification
SVM Comorbidity Classification
SVM Treatment Classification
SVM Diagnosis Classification
SVM Symptom Classification
SVM Lifestyle Classification
SVM Test Classification
SVM ISF Classification
SVM Candidate Ranking
Focus Recognition
Candidate Generation
Boundary Fixing
Focus
</figure>
<figureCaption confidence="0.9715365">
Figure 1: Question Decomposition Architecture. Modules with solid green lines indicate machine learn-
ing classifiers. Modules with dotted green lines indicate rule-based classifiers.
</figureCaption>
<listItem confidence="0.98040225">
(3) COORDINATION - a phrase that spans a set of
decomposable items.
(4) EXEMPLIFICATION - a phrase that spans an
optional item.
(5) IGNORE - a sentence indicating nothing of
value is present.
(6) FOCUS - an NP indicating the theme of the
consumer health question.
</listItem>
<bodyText confidence="0.999758952380953">
Further explanations of each annotation are pro-
vided in Sections 4-9. To convert these annota-
tions into separate, decomposed questions, a sim-
ple set of recursive rules is used. The rules enu-
merate all ways of including one conjunct from
each COORDINATION as well as whether or not
to include the phrase within an EXEMPLIFICA-
TION. These rules must be applied recursively to
handle overlapping annotations (e.g., a COORDI-
NATION within an EXEMPLIFICATION). Our im-
plementation is straight-forward and not discussed
further in this paper. The BACKGROUND and FO-
CUS annotations do not play a direct role in this
process, though they provide important contextual
elements and are useful for co-reference, and are
thus still considered part of the overall decompo-
sition process.
It should also be noted that some questions are
syntactically decomposable, but doing so alters
their original meaning. Consider the following
two question sentences:
</bodyText>
<listItem confidence="0.999973333333333">
• Can this disease be cured or can we only treat
the symptoms?
• Are males or females worse affected?
</listItem>
<bodyText confidence="0.999863625">
While the first example contains two “Can...”
questions and the second example contains the co-
ordination “males or females”, both questions are
providing a choice between two alternatives and
decomposing them would alter the semantic na-
ture of the original question. In these cases, we do
not consider the questions to be decomposable.
Data We use a set of consumer health ques-
tions collected from the Genetic and Rare Dis-
eases Information Center (GARD), which main-
tains a website2 with publicly available consumer-
submitted questions and professionally-authored
answers about genetic and rare diseases. We col-
lected 1,467 consumer health questions, consist-
ing of 4,115 sentences, 1,713 BACKGROUND sen-
tences, 37 IGNORE sentences, 2,465 QUESTIONs,
367 COORDINATIONs, 53 EXEMPLIFICATIONs,
and 1,513 FOCUS annotations. Questions with
more than one FOCUS are generally concerned
with the relation between diseases. Further infor-
mation about the corpus and the annotation pro-
cess can be found in Roberts et al. (2014).
System Architecture The architecture of our
question decomposition method is illustrated in
</bodyText>
<footnote confidence="0.963139">
2http://rarediseases.info.nih.gov/gard
</footnote>
<page confidence="0.999842">
31
</page>
<bodyText confidence="0.98714925">
Figure 1. To avoid confusion, in the rest of this
paper we refer to a complex consumer health ques-
tion simply as a request. Requests are sent to
the independent FOCUS recognition module (Sec-
tion 4), and then proceed through a pipeline that
includes the classification of sentences (Section 5),
the identification of separate QUESTIONs within
a question sentence (Section 6), the recognition
of COORDINATIONs (Section 7) and EXEMPLIFI-
CATIONs (Section 8), and the sub-classification of
BACKGROUND sentences (Section 9).
Experimental Setup The remainder of this pa-
per describes the individual modules in Figure 1.
For simplicity, we show results on the GARD data
for each task in its corresponding section. In all
cases, experiments are conducted using a 5-fold
cross-validation on the GARD data. The cross-
validation folds are organized at the request level
so that no two items from the same request will be
split between the training and testing data.
</bodyText>
<sectionHeader confidence="0.988393" genericHeader="method">
4 Identifying the Focal Disease
</sectionHeader>
<bodyText confidence="0.998606172413793">
The FOCUS is the condition that disease-centered
questions are centered around. Many other dis-
eases may be mentioned, but the FOCUS is the dis-
ease of central concern. This is similar to the as-
sumption made about a central disease in Medline
abstracts (Demner-Fushman and Lin, 2007). Of-
ten the FOCUS is stated in the first sentence (typ-
ically a BACKGROUND) of the request while the
questions are near the end. The questions can-
not generally be answered outside the context of
the FOCUS, however, so its identification is a crit-
ical part of decomposition. As shown in Figure 1,
we use a 3-step process: (1) a high-recall method
identifies potential FOCUS diseases in the data, (2)
a support vector machine (SVM) ranks the FO-
CUS candidates, and (3) the highest-ranking can-
didate’s boundary is modified with a set of rules to
better match our annotation standard.
To identify candidates for the FOCUS, we use a
lexicon constructed from UMLS (Lindberg et al.,
1993). UMLS includes very generic terms, such as
disease and cancer, that are too general to exactly
match a FOCUS in our data. We allow these terms
to be candidates so as to not miss any FOCUS that
doesn’t exactly match an entry in UMLS. When
such a general term is selected as the top-ranked
FOCUS, the rules described below are capable of
expanding the term to the full disease name.
To rank candidates, we utilize an SVM (Fan et
</bodyText>
<table confidence="0.999731">
E/R P R F1
1st UMLS Disorder E 19.6 19.0 19.3
R 28.2 27.4 27.8
SVM E 56.4 54.7 55.6
R 89.2 86.5 87.9
SVM + Rules E 74.8 72.5 73.6
R 89.5 86.8 88.1
</table>
<tableCaption confidence="0.8299085">
Table 1: FOCUS recognition results. E = exact
match; R = relaxed match.
</tableCaption>
<bodyText confidence="0.361841">
al., 2008) with a small number of feature types:
</bodyText>
<listItem confidence="0.991077545454545">
• Unigrams. Identifies generic words such as dis-
ease and syndrome that indicate good FOCUS
candidates, while also recognizing noisy UMLS
terms that are often false positives.
• UMLS semantic group (McCray et al., 2001).
• UMLS semantic type.
• Sentence Offset. The FOCUS is typically in the
first sentence, and is far more likely to be at the
beginning of the request than the end.
• Lexicon Offset. The FOCUS is typically the first
disease mentioned.
</listItem>
<bodyText confidence="0.999789258064516">
During training, the SVM considers any candidate
that overlaps the gold FOCUS to be correct. This
enables our approach to train on FOCUS examples
that do not perfectly align with a UMLS concept.
At test time, all candidates are classified, ranked
by the classifier’s confidence, and the top-ranked
candidate is considered the FOCUS.
As mentioned above, there are differences be-
tween how a FOCUS is annotated in our data and
how it is represented in the UMLS. We therefore
use a series of heuristics to alter the boundary to a
more usable FOCUS after it is chosen by the SVM.
The rules are applied iteratively to widen the FO-
CUS boundary until it cannot be expanded any fur-
ther. If a generic disease word is the only token
in the FOCUS, we add the token to the left. Con-
versely, if the token on the right is a generic dis-
ease word, it is added as well. If the word to the
left is capitalized, it is safe to assume it is part of
the disease’s name and so it is added as well. Fi-
nally, several rules recognize the various ways in
which a disease sub-type might be specified (e.g.,
Behcet’s syndrome vascular type, type 2 diabetes,
Charcot-Marie-Tooth disease type 2C).
We evaluate FOCUS recognition with both an
exact match, where the gold and automatic FOCUS
boundaries must line up perfectly, and a relaxed
match, which only requires a partial overlap. As a
baseline, we compare our results against a fully
rule-based system where the first UMLS Disor-
der term in the request is considered the FOCUS.
</bodyText>
<page confidence="0.996436">
32
</page>
<bodyText confidence="0.99996975">
We also evaluate the effectiveness of our bound-
ary altering rules by measuring performance with-
out these rules. The results are shown in Table 1.
The baseline method shows significant problems
in precision and recall. It is not able to ignore
noisy UMLS terms (e.g., aim is both a gene and
a treatment). The SVM improves upon the rule-
based method by over 50 points in F1 for relaxed
matching. Adding the boundary fixing rules has
little effect on relaxed matching, but greatly im-
proves exact matching: precision and recall are
improved by 18.4 and 17.8 points, respectively.
</bodyText>
<sectionHeader confidence="0.982748" genericHeader="method">
5 Classifying Sentences
</sectionHeader>
<bodyText confidence="0.99278625">
Before precise question boundaries can be rec-
ognized, we first identify sentences that con-
tain QUESTIONs, as distinguished from BACK-
GROUND and IGNORE sentences. It should be
noted that many of the question sentences in our
data are not typical wh-word questions. About
20% of the questions in our data end in a period.
For instance:
</bodyText>
<listItem confidence="0.9993066">
• Please tell me more about this condition.
• I was wondering if you could let me know where
I can find more information on this topic.
• I would like to get in contact with other families
that have this illness.
</listItem>
<bodyText confidence="0.98376925">
We consider a sentence to be a question if it con-
tains any information request, explicit or implicit.
After sentence splitting, we identify sentences
using a multi-class SVM with three feature types:
</bodyText>
<listItem confidence="0.996260333333333">
• Unigrams with parts-of-speech (POS). Reduces
unigram ambiguities, such as what-WP (a pro-
noun, indicative of a question) versus what-
WDT (a determiner, not indicative).
• Bigrams.
• Parse tree tags. All Treebank tags from the syn-
tactic parse tree. Captures syntactic question
clues such as the phrase tags SQ (question sen-
tence) and WHNP (wh-word noun phrase).
</listItem>
<bodyText confidence="0.999824166666667">
The SVM classifier performs at 97.8%. For com-
parison, an SVM with only unigram features per-
forms at 97.2%. While the unigram model does a
good job classifying sentences, suggesting this is
a very easy task, the improved feature set reduces
the number of errors by 20%.
</bodyText>
<sectionHeader confidence="0.995408" genericHeader="method">
6 Identifying Questions
</sectionHeader>
<bodyText confidence="0.783129666666667">
QUESTION recognition is the task of identifying
when a conjunction like and joins two independent
questions into a single sentence:
</bodyText>
<listItem confidence="0.97957825">
• [What causes the condition]QUESTION [and what
treatment is available?]QUESTION
• [What is this disease]QUESTION [and what steps
can I take to protect my daughter?]QUESTION
</listItem>
<bodyText confidence="0.9360034">
We consider the identification of separate QUES-
TIONs within a single sentence to be a differ-
ent task from COORDINATION recognition, which
finds phrases whose conjuncts can be treated in-
dependently. Linguistically, these tasks are quite
similar, but the distinction lies in whether the
right-conjunct syntactically depends on anything
to its left. For instance:
• I would like to learn [more about this condition
and what the prognosis is for a baby born with
it]COORDINATION.
Here, the right-conjunct starts with a question
stem (what), but is not a complete, grammatical
question on its own. Alternatively, this could be
re-formed into two separate QUESTIONs:
</bodyText>
<listItem confidence="0.998615666666667">
• [I would like to learn more about this
condition,]QUESTION [and what is the prognosis
is for a baby born with it.]QUESTION
</listItem>
<bodyText confidence="0.988721421052632">
We make this distinction because the QUESTION
recognition task requires one fewer step since the
boundaries extend to the entire sentence, prevent-
ing error propagation from an input module. Fur-
ther, the features that differentiate our QUESTION
and COORDINATION annotations are different.
The two-step process for recognizing QUES-
TIONs includes: (1) a high-recall candidate gener-
ator, and (2) an SVM to eliminate candidates that
are not separate QUESTIONs. The candidates for
QUESTION recognition are simply all the ways a
sentence can be split by the conjunctions and, or,
as well as, and the forward slash (“/”). In our data,
this candidate generation process has a recall of
98.6, as a few examples were missed where candi-
dates were not separated by one of the above con-
junctions.
To filter candidates, we use an SVM with three
features types:
</bodyText>
<listItem confidence="0.974781714285714">
• The conjunction separating the QUESTIONs.
• Unigrams in the left-conjunct. Identifies when
the left-conjunct is not a QUESTION, or when a
question is part of a COORDINATION.
• The right-conjunct’s parse tree tag. Recog-
nizes when the right-conjunct is an independent
clause that may safely be split.
</listItem>
<page confidence="0.997775">
33
</page>
<table confidence="0.999854">
P R F1
QUESTION split recognition
Baseline 24.7 82.4 38.0
SVM 67.7 64.7 66.2
Overall QUESTION recognition
Baseline 87.3 92.8 90.0
SVM 97.7 97.4 97.5
</table>
<tableCaption confidence="0.991393">
Table 2: QUESTION recognition results.
</tableCaption>
<bodyText confidence="0.999651214285714">
For evaluation, we measure both the F1 score
for correct candidates, and the overall F1 for all
QUESTION annotations (i.e., all QUESTION sen-
tences). We also evaluate a baseline method that
utilizes the parse tree to recognize separate QUES-
TIONs by splitting sentences where a conjunction
separates independent clauses. The results are
shown in Table 2. The baseline method has good
recall for recognizing where a sentence should be
split into multiple QUESTIONs, but it lacks preci-
sion. This is largely because it is unable to differ-
entiate clausal COORDINATIONs such as the above
example, as well as when the left-conjunct is not
actually a separate question. For instance:
</bodyText>
<listItem confidence="0.981491">
• Our grandson was diagnosed recently with this
disease and I am wondering if you could send
me information on it.
</listItem>
<bodyText confidence="0.9999598">
The SVM-based method can overcome this prob-
lem by looking at the words in the left-conjunct.
Both methods, however, fail to recognize when
two independent question clauses are asking the
same question but providing alternative answers:
</bodyText>
<listItem confidence="0.8800915">
• Will this condition be with him throughout his
life, or is it possible that it will clear up?
</listItem>
<bodyText confidence="0.9997076">
While there are methods for handling this issue
for COORDINATION recognition, addressed be-
low, recognizing non-splittable QUESTIONs re-
quires far deeper semantic understanding which
we leave to future work.
</bodyText>
<sectionHeader confidence="0.991816" genericHeader="method">
7 Identifying Coordinations
</sectionHeader>
<bodyText confidence="0.989452666666667">
COORDINATION recognition is the task of identi-
fying when a conjunction joins phrases within a
QUESTION that can in be separate questions:
</bodyText>
<listItem confidence="0.997113">
• How can I learn more about [treatments and
clinical trials]COORDINATION?
• Are [muscle twitching, muscle cramps, and
muscle pain]COORDINATION effects of having sil-
icosis?
</listItem>
<bodyText confidence="0.996204227272727">
Unlike QUESTION recognition, the boundaries of
a COORDINATION need to be determined as well
as whether the conjuncts can semantically be split
into separate questions. We thus use a three-step
process for recognizing COORDINATIONs: (1) a
high-recall candidate generator, (2) an SVM to
rank all the candidates for a given conjunction, and
(3) an SVM to filter out top-ranked candidates.
Candidate generation begins with the identifica-
tion of valid conjunctions within a QUESTION an-
notation. We use the same four conjunctions as in
QUESTION recognition: and, or, as well as, and
the forward slash. For each of these, all possi-
ble left and right boundaries are generated, so in
a QUESTION with 4 tokens on either side of the
conjunction, there would be 16 candidates. Addi-
tionally, two adjectives separated by a comma and
immediately followed by a noun are considered a
candidate (e.g., “a [safe, permanent]COORDINATION
treatment”). In our data, this candidate generation
process has a recall of 98.9, as a few instances ex-
ist in which a conjunction is not used, such as:
</bodyText>
<listItem confidence="0.978763333333333">
• I am looking for any information you have
about heavy metal toxicity, [treatment,
outcomes]EXEMPLIFICATION+COORDINATION.
</listItem>
<bodyText confidence="0.7526695">
To rank candidates, we use an SVM with the
following feature types:
</bodyText>
<listItem confidence="0.978876818181818">
• If the left-conjunct is congruent with the high-
est node in the syntactic parse tree whose right-
most leaf is also the right-most token in the left-
conjunct. Essentially, this is equivalent to say-
ing whether or not the syntactic parser agrees
with the left-conjunct’s boundary.
• The equivalent heuristic for the right-conjunct.
• If a noun is in both, just the left conjunct, just
the right conjunct, or neither conjunct.
• The Levenshtein distance between the POS tag
sequences for the left- and right-conjuncts.
</listItem>
<bodyText confidence="0.9999088125">
The first two features encode the information a
rule-based method would use if it relied entirely
on the syntactic parse tree. The remaining features
help the classifier overcome cases where the parser
may be wrong.
At training time, all candidates for a given con-
junction are generated and only the candidate that
matches the gold COORDINATION is considered
a positive example. Additionally, we annotated
the boundaries for negative COORDINATIONs (i.e.,
syntactic coordinations that do not fit our annota-
tion standard). There were 203 such instances in
the GARD data. These are considered gold CO-
ORDINATIONs for boundary ranking only.
To filter the top-ranked candidates, we use an
SVM with several feature types:
</bodyText>
<page confidence="0.997193">
34
</page>
<table confidence="0.9999086">
E/R P R F1
Baseline E 28.1 36.5 31.8
R 62.9 75.8 68.7
Rank + Filter E 38.2 34.8 36.4
R 78.5 69.0 73.5
</table>
<tableCaption confidence="0.876101">
Table 3: COORDINATION recognition results.
E = exact match; R = relaxed match.
</tableCaption>
<listItem confidence="0.963310866666667">
• The conjunction.
• Unigrams in the left-conjunct.
• POS of the first word in both conjuncts. CO-
ORDINATIONs often have the same first POS in
both conjuncts.
• The word immediately before the candidate.
E.g., between is a good negative indicator.
• Unigrams in the question but not the candidate.
• If the candidate takes up almost the entire ques-
tion (all but 3 tokens). Typically, COORDINA-
TIONs are much smaller than the full question.
• If more than one conjunction is in the candidate.
• If a word in the left-conjunct has an antonym
in the right conjunct. Antonyms are recognized
via WordNet (Fellbaum, 1998).
</listItem>
<bodyText confidence="0.999138611111111">
At training time, the positive examples are drawn
from the annotated COORDINATIONs, while the
negative examples are drawn from the 203 non-
gold annotations mentioned above.
In addition to evaluating this method, we
evaluate a baseline method that relies entirely
on the syntactic parse to identify COORDINA-
TION boundaries without filtering. The results
are shown in Table 3. The rank-and-filter ap-
proach shows significant gains over the rule-based
method in precision and F1. As can be seen in
the difference between exact and relaxed match-
ing, most of the loss for both the baseline and ML
methods come in boundary detection. Most meth-
ods overly rely upon the syntactic parser, which
performs poorly both on questions and coordina-
tions. The ML method, though, is sometimes able
to overcome this problem.
</bodyText>
<sectionHeader confidence="0.940441" genericHeader="method">
8 Identifying Exemplifications
</sectionHeader>
<bodyText confidence="0.995259833333333">
EXEMPLIFICATION recognition is the task of iden-
tifying when a phrase provides an optional, exem-
plifying example with a more specific type of in-
formation than that asked by the rest of the ques-
tion. For instance, the following contains both an
EXEMPLIFICATION and a COORDINATION:
</bodyText>
<listItem confidence="0.953474666666666">
• Is there anything out there that can help
him [such as [medications or alternative
therapies]COORDINATION]EXEMPLIFICATION?
We could consider this to denote 3 questions:
• Is there anything out there that can help him?
• Is there anything out there that can help him
such as medications?
• Is there anything out there that can help him
such as alternative therapies?
</listItem>
<bodyText confidence="0.999696">
In the latter two questions, we consider the phrase
such as to now denote a mandatory constraint on
the answer to each question, whereas in the origi-
nal question it would be considered optional.
EXEMPLIFICATION recognition is similar to
COORDINATION recognition, and its three-step
process is thus similar as well: (1) a high-recall
candidate generator, (2) an SVM to rank all the
candidates for a given trigger phrase, and (3) a set
of rules to filter out top-ranked candidates.
Candidate generation begins with the identifica-
tion of valid trigger words and phrases. These in-
clude: especially, including, particularly, specifi-
cally, and such as. For each of these, all possible
right boundaries are generated, thus EXEMPLIFI-
CATIONs have far fewer candidates than COORDI-
NATIONs. Additionally, all phrases within paren-
theses are added as EXEMPLIFICATIONs. In our
data, this candidate generation process has a recall
of 98.1, missing instances without a trigger (see
the example also missed by COORDINATION can-
didate generation in Section 7).
To rank candidates, we use an SVM with the
following feature types:
</bodyText>
<listItem confidence="0.995179833333333">
• If the right-conjunct is the highest parse node
as defined in the COORDINATION boundary fea-
ture.
• If a dependency relation crosses from the right-
conjunct to any word outside the candidate.
• POS of the word after the candidate.
</listItem>
<bodyText confidence="0.9998433">
As with COORDINATIONs, we annotated bound-
aries for negative EXEMPLIFICATIONs matching
the trigger words and used them as positive exam-
ples for boundary ranking.
To filter the top-ranked candidates, we use two
simple rules. First, EXEMPLIFICATIONs within
parentheses are filtered if they are acronyms or
acronym expansions. Second, cases such as the
below example are removed by looking at the
words before the candidate:
</bodyText>
<listItem confidence="0.7866755">
• I am particularly interested in learning more
about genetic testing for the syndrome.
</listItem>
<bodyText confidence="0.991198">
In addition to evaluating this method, we eval-
uate a baseline method that relies entirely on the
</bodyText>
<page confidence="0.996555">
35
</page>
<table confidence="0.9998952">
E/R P R F1
Baseline E 28.9 62.3 39.5
R 39.5 84.9 53.9
Rank + Filter E 60.8 58.5 59.6
R 80.4 77.4 78.8
</table>
<tableCaption confidence="0.902802">
Table 4: EXEMPLIFICATION recognition results.
E = exact match; R = relaxed match.
</tableCaption>
<bodyText confidence="0.994020888888889">
syntactic parser to identify EXEMPLIFICATION
boundaries and performs no filtering. The re-
sults are shown in Table 4. The rank-and-filter
approach shows significant gains over the rule-
based method in precision and F1, more than dou-
bling precision for both exact and relaxed match-
ing. There is still a drop in performance when go-
ing from relaxed to exact matching, again largely
due to the reliance on the syntactic parser.
</bodyText>
<sectionHeader confidence="0.937954" genericHeader="method">
9 Classifying Background Information
</sectionHeader>
<bodyText confidence="0.999550590909091">
BACKGROUND sentences contain contextual in-
formation, such as whether or not a patient has
been diagnosed with the focal disease or what
symptoms they are experiencing. This informa-
tion was annotated at the sentence level, partly be-
cause of annotation convenience, but also because
phrase boundaries are not always clear for medical
concepts (Hahn et al., 2012; Forbush et al., 2013).
A difficult factor in this task, and especially on
the GARD dataset, is that consumers are not al-
ways asking about a disease for themselves. In-
stead, often they ask on behalf of another individ-
ual, often a family member. The BACKGROUND
types are thus annotated based on the person of
interest, who we refer to as the patient (in the lin-
guistic sense). For instance, if a mother has a dis-
ease but is asking about her son (e.g., asking about
the probability of her son inheriting the disease),
that sentence would be a FAMILY HISTORY, as
opposed to a DIAGNOSIS sentence.
The GARD corpus is annotated with eight
BACKGROUND types:
</bodyText>
<listItem confidence="0.9962032">
• COMORBIDITY • LIFESTYLE
• DIAGNOSIS • SYMPTOM
• FAMILY HISTORY • TEST
• ISF (information • TREATMENT
search failure)
</listItem>
<bodyText confidence="0.999572833333333">
ISF sentences indicate previous attempts to find
the requested information have failed, and are a
good signal to the QA system to enable more in-
depth search strategies. LIFESTYLE sentences de-
scribe the patient’s life habits (e.g., smoking, ex-
ercise). Currently, the automatic identification of
</bodyText>
<table confidence="0.9999304">
Type P R F1 # Anns
COMORBIDITY 0.0 0.0 0.0 23
DIAGNOSIS 80.8 80.3 80.5 690
FAMILY HISTORY 67.4 38.4 48.9 151
ISF 75.0 65.9 70.1 41
LIFESTYLE 0.0 0.0 0.0 13
SYMPTOM 76.6 48.1 59.1 320
TEST 37.5 4.9 8.7 61
TREATMENT 87.3 35.0 50.0 137
Overall: Micro-F1: 67.3 Macro-F1:39.7
</table>
<tableCaption confidence="0.999036">
Table 5: BACKGROUND results.
</tableCaption>
<bodyText confidence="0.999755526315789">
BACKGROUND types has not been a major focus
of our effort as no handling exists for it within our
QA system. We report a baseline method and re-
sults here to provide some insight into the diffi-
culty of the task.
BACKGROUND types are a multi-labeling prob-
lem, so we use eight binary classifiers, one for
each type. Each classifier utilizes only unigram
and bigram features. The results for the mod-
els are shown in Table 5. COMORBIDITY and
LIFESTYLE are too rare in the data (23 and 13
instances, respectively) for the classifier to iden-
tify. DIAGNOSIS questions are identified fairly
well because this is the most common type (690
instances) and because of the constrained vocabu-
lary for expressing a diagnosis. The performance
of the rest of the types is largely proportional to
the number of instances in the data, though ISF
performs quite well given only 41 instances.
</bodyText>
<sectionHeader confidence="0.994698" genericHeader="conclusions">
10 Conclusion
</sectionHeader>
<bodyText confidence="0.999992476190476">
We have presented a method for decomposing
consumer health questions by recognizing six an-
notation types. Some of these types are general
enough to use in open-domain question decom-
position (BACKGROUND, IGNORE, QUESTION,
COORDINATION, EXEMPLIFICATION), while oth-
ers are targeted specifically at consumer health
questions (FOCUS and the BACKGROUND sub-
types). We demonstrate that ML methods can
improve upon heuristic methods relying on the
syntactic parse tree, though parse errors are of-
ten difficult to overcome. Since significant im-
provements in performance would likely require
major advances in open-domain syntactic parsing,
we instead envision further integration of the key
tasks in consumer health question analysis: (1) in-
tegration of co-reference and implicit argument in-
formation, (2) improved identification of BACK-
GROUND types, and (3) identification of discourse
relations within questions to further leverage ques-
tion decomposition.
</bodyText>
<page confidence="0.997466">
36
</page>
<sectionHeader confidence="0.996227" genericHeader="acknowledgments">
Acknowledgements
</sectionHeader>
<bodyText confidence="0.999892">
This work was supported by the intramural re-
search program at the U.S. National Library of
Medicine, National Institutes of Health. We would
additionally like to thank Stephanie M. Morri-
son and Janine Lewis for their help accessing the
GARD data.
</bodyText>
<sectionHeader confidence="0.999174" genericHeader="references">
References
</sectionHeader>
<reference confidence="0.999923948979592">
Ulrich Andersen, Anna Braasch, Lina Henriksen,
Csaba Huszka, Anders Johannsen, Lars Kayser,
Bente Maegaard, Ole Norgaard, Stefan Schulz, and
J¨urgen Wedekind. 2012. Creation and use of Lan-
guage Resources in a Question-Answering eHealth
System. In Proceedings of the Eighth International
Conference on Language Resources and Evaluation,
pages 2536–2542.
Brian L. Cairns, Rodney D. Nielsen, James J. Masanz,
James H. Martin, Martha S. Palmer, Wayne H. Ward,
and Guergana K. Savova. 2011. The MiPACQ Clin-
ical Question Answering System. In Proceedings of
the AMIA Annual Symposium, pages 171–180.
YongGang Cao, Feifan Liu, Pippa Simpson, Lamont
Antieau, Andrew Bennett, James J. Cimino, John
Ely, and Hong Yu. 2011. AskHERMES: An on-
line question answering system for complex clini-
cal questions. Journal of Biomedical Informatics,
44:277–288.
Dina Demner-Fushman and Swapna Abhyankar. 2012.
Syntactic-Semantic Frames for Clinical Cohort
Identification Queries. In Data Integration in the
Life Sciences, volume 7348 of Lecture Notes in
Computer Science, pages 100–112.
Dina Demner-Fushman and Jimmy Lin. 2007. An-
swering Clinical Questions with Knowledge-Based
and Statistical Techniques. Computational Linguis-
tics, 33(1).
Rong-En Fan, Kai-Wei Chang, Cho-Jui Hsieh, Xiang-
Rui Wang, and Chih-Jen Lin. 2008. LIBLINEAR:
A Library for Large Linear Classification. Journal
of Machine Learning Research, 9:1871–1874.
Christiane Fellbaum. 1998. WordNet: An Electronic
Lexical Database. MIT Press.
Tyler B. Forbush, Adi V. Gundlapalli, Miland N.
Palmer, Shuying Shen, Brett R. South, Guy Divita,
Marjorie Carter, Andrew Redd, Jorie M. Butler, and
Matthew Samore. 2013. “Sitting on Pins and Nee-
dles”: Characterization of Symptom Descriptions in
Clinical Notes. In AMIA Summit on Clinical Re-
search Informatics, pages 67–71.
Udo Hahn, Elena Beisswanger, Ekaterina Buyko, Erik
Faessler, Jenny Traum¨uller, Susann Schr¨oder, and
Kerstin Hornbostel. 2012. Iterative Refinement
and Quality Checking of Annotation Guidelines –
How to Deal Effectively with Semantically Sloppy
Named Entity Types, such as Pathological Phenom-
ena. In Proceedings of the Eighth International
Conference on Language Resources and Evaluation,
pages 3881–3885.
Sanda Harabagiu, Finley Lacatusu, and Andrew Hickl.
2006. Answer Complex Questions with Random
Walk Models. In Proceedings of the 29th Annual
ACM SIGIR Conference on Research and Develop-
ment in Information Retrieval, pages 220–227.
Sven Hartrumpf. 2008. Semantic Decomposition
for Question Answering. In Proceedings on the
18th European Conference on Artificial Intelligence,
pages 313–317.
Dierdre Hogan. 2007. Coordinate Noun Phrase Dis-
ambiguation in a Generative Parsing Model. In Pro-
ceedings of the 45th Annual Meeting of the Associa-
tion for Computational Linguistics, pages 680–687.
John Judge, Aoife Cahill, and Josef van Genabith.
2006. QuestionBank: Creating a Corpus of Parse-
Annotated Questions. In Proceedings of the 21st In-
ternational Conference on Computational Linguis-
tics and 44th Annual Meeting of the Association for
Computational Linguistics, pages 497–504.
Yarden Katz and Bernardo C. Grau. 2005. Repre-
senting Qualitative Spatial Information in OWL-DL.
Proceedings of OWL: Experiences and Directions.
Halil Kilicoglu, Marcelo Fiszman, and Dina Demner-
Fushman. 2013. Interpreting Consumer Health
Questions: The Role of Anaphora and Ellipsis. In
Proceedings of the 2013 BioNLP Workshop, pages
54–62.
Finley Lacatusu, Andrew Hickl, and Sanda Harabagiu.
2006. Impact of Question Decomposition on the
Quality of Answer Summaries. In Proceedings of
LREC, pages 1147–1152.
Donald A.B. Lindberg, Betsy L. Humphreys, and
Alexa T. McCray. 1993. The Unified Medical Lan-
guage System. Methods of Information in Medicine,
32(4):281–291.
Alexa T McCray, Anita Burgun, and Olivier Boden-
reider. 2001. Aggregating UMLS Semantic Types
for Reducing Conceptual Complexity. In Studies
in Health Technology and Informatics (MEDINFO),
volume 84(1), pages 216–220.
Kirk Roberts, Kate Masterton, Marcelo Fiszman, Halil
Kilicoglu, and Dina Demner-Fushman. 2014. An-
notating Question Decomposition on Complex Med-
ical Questions. In Proceedings of LREC.
Hong Yu and YongGang Cao. 2008. Automatically
Extracting Information Needs from Ad Hoc Clini-
cal Questions. In Proceedings of the AMIA Annual
Symposium.
</reference>
<page confidence="0.99961">
37
</page>
</variant>
</algorithm>
<algorithm name="ParsHed" version="110505">
<variant no="0" confidence="0.888365">
<title confidence="0.999908">Decomposing Consumer Health Questions</title>
<author confidence="0.995198">Kirk Roberts</author>
<author confidence="0.995198">Halil Kilicoglu</author>
<author confidence="0.995198">Marcelo Fiszman</author>
<author confidence="0.995198">Dina</author>
<affiliation confidence="0.9914445">National Library of National Institutes of</affiliation>
<address confidence="0.904952">Bethesda, MD</address>
<abstract confidence="0.999805833333334">This paper presents a method for decomposing long, complex consumer health questions. Our approach largely decomposes questions using their syntactic structure, recognizing independent questions embedded in clauses, as well as coordinations and exemplifying phrases. Additionally, we identify elements specific to disease-related consumer health questions, such as the focus disease and background information. To achieve this, our approach combines rank-and-filter machine learning methods with rule-based methods. Our results demonstrate significant improvements over the heuristic methods typically employed for question decomposition that rely only on the syntactic parse tree.</abstract>
</variant>
</algorithm>
<algorithm name="ParsCit" version="110505">
<citationList>
<citation valid="true">
<authors>
<author>Ulrich Andersen</author>
<author>Anna Braasch</author>
<author>Lina Henriksen</author>
<author>Csaba Huszka</author>
<author>Anders Johannsen</author>
<author>Lars Kayser</author>
<author>Bente Maegaard</author>
<author>Ole Norgaard</author>
<author>Stefan Schulz</author>
<author>J¨urgen Wedekind</author>
</authors>
<title>Creation and use of Language Resources in a Question-Answering eHealth System.</title>
<date>2012</date>
<booktitle>In Proceedings of the Eighth International Conference on Language Resources and Evaluation,</booktitle>
<pages>2536--2542</pages>
<contexts>
<context position="4620" citStr="Andersen et al. (2012)" startWordPosition="673" endWordPosition="677">mner-Fushman and Lin, 2007; Cairns et al., 2011; Cao et al., 2011) as a means for retrieving medical information. This work has typically focused, however, on questions posed by medical professionals, and the methods proposed for question analysis generally assume a single, concise question. For example, Demner-Fushman and Abhyankar (2012) propose a method for extracting frames from queries for the purpose of cohort retrieval. Their method assumes syntactic dependencies exist between the necessary frame elements, and is thus not well-suited to handle long, multi-sentence questions. Similarly, Andersen et al. (2012) proposes a method for converting a concise question into a structured query. However, many medical questions require background information that is difficult to encode in a single question sentence. Instead, it is often more natural to ask multiple questions over several sentences, providing background information to give context to the questions. Yu and Cao (2008) use a ML method to recognize question types in professional health questions. Their method can identify more than one type per complex question. Without decomposing the full question into its sub-questions, however, the type cannot</context>
</contexts>
<marker>Andersen, Braasch, Henriksen, Huszka, Johannsen, Kayser, Maegaard, Norgaard, Schulz, Wedekind, 2012</marker>
<rawString>Ulrich Andersen, Anna Braasch, Lina Henriksen, Csaba Huszka, Anders Johannsen, Lars Kayser, Bente Maegaard, Ole Norgaard, Stefan Schulz, and J¨urgen Wedekind. 2012. Creation and use of Language Resources in a Question-Answering eHealth System. In Proceedings of the Eighth International Conference on Language Resources and Evaluation, pages 2536–2542.</rawString>
</citation>
<citation valid="true">
<authors>
<author>Brian L Cairns</author>
<author>Rodney D Nielsen</author>
<author>James J Masanz</author>
<author>James H Martin</author>
<author>Martha S Palmer</author>
<author>Wayne H Ward</author>
<author>Guergana K Savova</author>
</authors>
<title>The MiPACQ Clinical Question Answering System.</title>
<date>2011</date>
<booktitle>In Proceedings of the AMIA Annual Symposium,</booktitle>
<pages>171--180</pages>
<contexts>
<context position="4045" citStr="Cairns et al., 2011" startWordPosition="588" endWordPosition="591"> coordinated information embedded within a question. For each of these decomposition annotations, we propose a combination of machine learning (ML) and rule based methods. The ML methods largely take the form of a 3-step rank-and-filter approach, where candidates are generated, ranked by an ML classifier, then the top-ranked candidate is passed through a separate ML filtering classifier. We evaluate each of these methods on a set of 1,467 consumer health questions related to genetic and rare diseases. 2 Background QA in the biomedical domain has been wellstudied (Demner-Fushman and Lin, 2007; Cairns et al., 2011; Cao et al., 2011) as a means for retrieving medical information. This work has typically focused, however, on questions posed by medical professionals, and the methods proposed for question analysis generally assume a single, concise question. For example, Demner-Fushman and Abhyankar (2012) propose a method for extracting frames from queries for the purpose of cohort retrieval. Their method assumes syntactic dependencies exist between the necessary frame elements, and is thus not well-suited to handle long, multi-sentence questions. Similarly, Andersen et al. (2012) proposes a method for co</context>
</contexts>
<marker>Cairns, Nielsen, Masanz, Martin, Palmer, Ward, Savova, 2011</marker>
<rawString>Brian L. Cairns, Rodney D. Nielsen, James J. Masanz, James H. Martin, Martha S. Palmer, Wayne H. Ward, and Guergana K. Savova. 2011. The MiPACQ Clinical Question Answering System. In Proceedings of the AMIA Annual Symposium, pages 171–180.</rawString>
</citation>
<citation valid="true">
<authors>
<author>YongGang Cao</author>
<author>Feifan Liu</author>
<author>Pippa Simpson</author>
<author>Lamont Antieau</author>
<author>Andrew Bennett</author>
<author>James J Cimino</author>
<author>John Ely</author>
<author>Hong Yu</author>
</authors>
<title>AskHERMES: An online question answering system for complex clinical questions.</title>
<date>2011</date>
<journal>Journal of Biomedical Informatics,</journal>
<pages>44--277</pages>
<contexts>
<context position="4064" citStr="Cao et al., 2011" startWordPosition="592" endWordPosition="595">ion embedded within a question. For each of these decomposition annotations, we propose a combination of machine learning (ML) and rule based methods. The ML methods largely take the form of a 3-step rank-and-filter approach, where candidates are generated, ranked by an ML classifier, then the top-ranked candidate is passed through a separate ML filtering classifier. We evaluate each of these methods on a set of 1,467 consumer health questions related to genetic and rare diseases. 2 Background QA in the biomedical domain has been wellstudied (Demner-Fushman and Lin, 2007; Cairns et al., 2011; Cao et al., 2011) as a means for retrieving medical information. This work has typically focused, however, on questions posed by medical professionals, and the methods proposed for question analysis generally assume a single, concise question. For example, Demner-Fushman and Abhyankar (2012) propose a method for extracting frames from queries for the purpose of cohort retrieval. Their method assumes syntactic dependencies exist between the necessary frame elements, and is thus not well-suited to handle long, multi-sentence questions. Similarly, Andersen et al. (2012) proposes a method for converting a concise </context>
</contexts>
<marker>Cao, Liu, Simpson, Antieau, Bennett, Cimino, Ely, Yu, 2011</marker>
<rawString>YongGang Cao, Feifan Liu, Pippa Simpson, Lamont Antieau, Andrew Bennett, James J. Cimino, John Ely, and Hong Yu. 2011. AskHERMES: An online question answering system for complex clinical questions. Journal of Biomedical Informatics, 44:277–288.</rawString>
</citation>
<citation valid="true">
<authors>
<author>Dina Demner-Fushman</author>
<author>Swapna Abhyankar</author>
</authors>
<title>Syntactic-Semantic Frames for Clinical Cohort Identification Queries.</title>
<date>2012</date>
<booktitle>In Data Integration in the Life Sciences,</booktitle>
<volume>7348</volume>
<pages>100--112</pages>
<contexts>
<context position="4339" citStr="Demner-Fushman and Abhyankar (2012)" startWordPosition="632" endWordPosition="635"> ranked by an ML classifier, then the top-ranked candidate is passed through a separate ML filtering classifier. We evaluate each of these methods on a set of 1,467 consumer health questions related to genetic and rare diseases. 2 Background QA in the biomedical domain has been wellstudied (Demner-Fushman and Lin, 2007; Cairns et al., 2011; Cao et al., 2011) as a means for retrieving medical information. This work has typically focused, however, on questions posed by medical professionals, and the methods proposed for question analysis generally assume a single, concise question. For example, Demner-Fushman and Abhyankar (2012) propose a method for extracting frames from queries for the purpose of cohort retrieval. Their method assumes syntactic dependencies exist between the necessary frame elements, and is thus not well-suited to handle long, multi-sentence questions. Similarly, Andersen et al. (2012) proposes a method for converting a concise question into a structured query. However, many medical questions require background information that is difficult to encode in a single question sentence. Instead, it is often more natural to ask multiple questions over several sentences, providing background information to</context>
</contexts>
<marker>Demner-Fushman, Abhyankar, 2012</marker>
<rawString>Dina Demner-Fushman and Swapna Abhyankar. 2012. Syntactic-Semantic Frames for Clinical Cohort Identification Queries. In Data Integration in the Life Sciences, volume 7348 of Lecture Notes in Computer Science, pages 100–112.</rawString>
</citation>
<citation valid="true">
<authors>
<author>Dina Demner-Fushman</author>
<author>Jimmy Lin</author>
</authors>
<date>2007</date>
<booktitle>Answering Clinical Questions with Knowledge-Based and Statistical Techniques. Computational Linguistics,</booktitle>
<volume>33</volume>
<issue>1</issue>
<contexts>
<context position="4024" citStr="Demner-Fushman and Lin, 2007" startWordPosition="584" endWordPosition="587"> when they contain optional or coordinated information embedded within a question. For each of these decomposition annotations, we propose a combination of machine learning (ML) and rule based methods. The ML methods largely take the form of a 3-step rank-and-filter approach, where candidates are generated, ranked by an ML classifier, then the top-ranked candidate is passed through a separate ML filtering classifier. We evaluate each of these methods on a set of 1,467 consumer health questions related to genetic and rare diseases. 2 Background QA in the biomedical domain has been wellstudied (Demner-Fushman and Lin, 2007; Cairns et al., 2011; Cao et al., 2011) as a means for retrieving medical information. This work has typically focused, however, on questions posed by medical professionals, and the methods proposed for question analysis generally assume a single, concise question. For example, Demner-Fushman and Abhyankar (2012) propose a method for extracting frames from queries for the purpose of cohort retrieval. Their method assumes syntactic dependencies exist between the necessary frame elements, and is thus not well-suited to handle long, multi-sentence questions. Similarly, Andersen et al. (2012) pro</context>
<context position="12667" citStr="Demner-Fushman and Lin, 2007" startWordPosition="1871" endWordPosition="1874">results on the GARD data for each task in its corresponding section. In all cases, experiments are conducted using a 5-fold cross-validation on the GARD data. The crossvalidation folds are organized at the request level so that no two items from the same request will be split between the training and testing data. 4 Identifying the Focal Disease The FOCUS is the condition that disease-centered questions are centered around. Many other diseases may be mentioned, but the FOCUS is the disease of central concern. This is similar to the assumption made about a central disease in Medline abstracts (Demner-Fushman and Lin, 2007). Often the FOCUS is stated in the first sentence (typically a BACKGROUND) of the request while the questions are near the end. The questions cannot generally be answered outside the context of the FOCUS, however, so its identification is a critical part of decomposition. As shown in Figure 1, we use a 3-step process: (1) a high-recall method identifies potential FOCUS diseases in the data, (2) a support vector machine (SVM) ranks the FOCUS candidates, and (3) the highest-ranking candidate’s boundary is modified with a set of rules to better match our annotation standard. To identify candidate</context>
</contexts>
<marker>Demner-Fushman, Lin, 2007</marker>
<rawString>Dina Demner-Fushman and Jimmy Lin. 2007. Answering Clinical Questions with Knowledge-Based and Statistical Techniques. Computational Linguistics, 33(1).</rawString>
</citation>
<citation valid="true">
<authors>
<author>Rong-En Fan</author>
<author>Kai-Wei Chang</author>
<author>Cho-Jui Hsieh</author>
<author>XiangRui Wang</author>
<author>Chih-Jen Lin</author>
</authors>
<title>LIBLINEAR: A Library for Large Linear Classification.</title>
<date>2008</date>
<journal>Journal of Machine Learning Research,</journal>
<pages>9--1871</pages>
<marker>Fan, Chang, Hsieh, Wang, Lin, 2008</marker>
<rawString>Rong-En Fan, Kai-Wei Chang, Cho-Jui Hsieh, XiangRui Wang, and Chih-Jen Lin. 2008. LIBLINEAR: A Library for Large Linear Classification. Journal of Machine Learning Research, 9:1871–1874.</rawString>
</citation>
<citation valid="true">
<authors>
<author>Christiane Fellbaum</author>
</authors>
<title>WordNet: An Electronic Lexical Database.</title>
<date>1998</date>
<publisher>MIT Press.</publisher>
<contexts>
<context position="25378" citStr="Fellbaum, 1998" startWordPosition="4007" endWordPosition="4008">. • The conjunction. • Unigrams in the left-conjunct. • POS of the first word in both conjuncts. COORDINATIONs often have the same first POS in both conjuncts. • The word immediately before the candidate. E.g., between is a good negative indicator. • Unigrams in the question but not the candidate. • If the candidate takes up almost the entire question (all but 3 tokens). Typically, COORDINATIONs are much smaller than the full question. • If more than one conjunction is in the candidate. • If a word in the left-conjunct has an antonym in the right conjunct. Antonyms are recognized via WordNet (Fellbaum, 1998). At training time, the positive examples are drawn from the annotated COORDINATIONs, while the negative examples are drawn from the 203 nongold annotations mentioned above. In addition to evaluating this method, we evaluate a baseline method that relies entirely on the syntactic parse to identify COORDINATION boundaries without filtering. The results are shown in Table 3. The rank-and-filter approach shows significant gains over the rule-based method in precision and F1. As can be seen in the difference between exact and relaxed matching, most of the loss for both the baseline and ML methods </context>
</contexts>
<marker>Fellbaum, 1998</marker>
<rawString>Christiane Fellbaum. 1998. WordNet: An Electronic Lexical Database. MIT Press.</rawString>
</citation>
<citation valid="true">
<authors>
<author>Tyler B Forbush</author>
<author>Adi V Gundlapalli</author>
<author>Miland N Palmer</author>
<author>Shuying Shen</author>
<author>Brett R South</author>
<author>Guy Divita</author>
<author>Marjorie Carter</author>
<author>Andrew Redd</author>
<author>Jorie M Butler</author>
<author>Matthew Samore</author>
</authors>
<date>2013</date>
<booktitle>Sitting on Pins and Needles”: Characterization of Symptom Descriptions in Clinical Notes. In AMIA Summit on Clinical Research Informatics,</booktitle>
<pages>67--71</pages>
<contexts>
<context position="29836" citStr="Forbush et al., 2013" startWordPosition="4729" endWordPosition="4732">e than doubling precision for both exact and relaxed matching. There is still a drop in performance when going from relaxed to exact matching, again largely due to the reliance on the syntactic parser. 9 Classifying Background Information BACKGROUND sentences contain contextual information, such as whether or not a patient has been diagnosed with the focal disease or what symptoms they are experiencing. This information was annotated at the sentence level, partly because of annotation convenience, but also because phrase boundaries are not always clear for medical concepts (Hahn et al., 2012; Forbush et al., 2013). A difficult factor in this task, and especially on the GARD dataset, is that consumers are not always asking about a disease for themselves. Instead, often they ask on behalf of another individual, often a family member. The BACKGROUND types are thus annotated based on the person of interest, who we refer to as the patient (in the linguistic sense). For instance, if a mother has a disease but is asking about her son (e.g., asking about the probability of her son inheriting the disease), that sentence would be a FAMILY HISTORY, as opposed to a DIAGNOSIS sentence. The GARD corpus is annotated </context>
</contexts>
<marker>Forbush, Gundlapalli, Palmer, Shen, South, Divita, Carter, Redd, Butler, Samore, 2013</marker>
<rawString>Tyler B. Forbush, Adi V. Gundlapalli, Miland N. Palmer, Shuying Shen, Brett R. South, Guy Divita, Marjorie Carter, Andrew Redd, Jorie M. Butler, and Matthew Samore. 2013. “Sitting on Pins and Needles”: Characterization of Symptom Descriptions in Clinical Notes. In AMIA Summit on Clinical Research Informatics, pages 67–71.</rawString>
</citation>
<citation valid="false">
<authors>
<author>Udo Hahn</author>
<author>Elena Beisswanger</author>
<author>Ekaterina Buyko</author>
<author>Erik Faessler</author>
<author>Jenny Traum¨uller</author>
<author>Susann Schr¨oder</author>
<author>Kerstin Hornbostel</author>
</authors>
<title>Iterative Refinement and Quality Checking of Annotation Guidelines – How to Deal Effectively with Semantically Sloppy Named Entity Types, such as Pathological Phenomena.</title>
<date>2012</date>
<booktitle>In Proceedings of the Eighth International Conference on Language Resources and Evaluation,</booktitle>
<pages>3881--3885</pages>
<marker>Hahn, Beisswanger, Buyko, Faessler, Traum¨uller, Schr¨oder, Hornbostel, 2012</marker>
<rawString>Udo Hahn, Elena Beisswanger, Ekaterina Buyko, Erik Faessler, Jenny Traum¨uller, Susann Schr¨oder, and Kerstin Hornbostel. 2012. Iterative Refinement and Quality Checking of Annotation Guidelines – How to Deal Effectively with Semantically Sloppy Named Entity Types, such as Pathological Phenomena. In Proceedings of the Eighth International Conference on Language Resources and Evaluation, pages 3881–3885.</rawString>
</citation>
<citation valid="true">
<authors>
<author>Sanda Harabagiu</author>
<author>Finley Lacatusu</author>
<author>Andrew Hickl</author>
</authors>
<title>Answer Complex Questions with Random Walk Models.</title>
<date>2006</date>
<booktitle>In Proceedings of the 29th Annual ACM SIGIR Conference on Research and Development in Information Retrieval,</booktitle>
<pages>220--227</pages>
<contexts>
<context position="6635" citStr="Harabagiu et al. (2006)" startWordPosition="980" endWordPosition="983">ond form of question decomposition is semantic decomposition, which can semantically break individual questions apart to answer them in stages. For instance, the question “When did the third U.S. President die?” can be semantically decomposed “Who was the third U.S. President?” and “When did X die?”, where the answer to the first question is substituted into the second. Katz and Grau (2005) discusses this kind of decomposition using the syntactic structure, though it is not empirically validated. Hartrumpf (2008) proposes a decomposition method using only the deep semantic structure. Finally, Harabagiu et al. (2006) proposes a different type of question decomposition based on a random walk over similar questions extracted from a corpus. In our work, we focus on syntactic question decomposition. We demonstrate the importance of empirical evaluation of question decomposition, notably the pitfalls of heuristic approaches that rely entirely on the syntactic parse tree. Syntactic parsers trained on Treebank are particularly poor at both analyzing questions (Judge et al., 2006) and coordination boundaries (Hogan, 2007). Robust question decomposition methods, therefore, must be able to overcome many of these di</context>
</contexts>
<marker>Harabagiu, Lacatusu, Hickl, 2006</marker>
<rawString>Sanda Harabagiu, Finley Lacatusu, and Andrew Hickl. 2006. Answer Complex Questions with Random Walk Models. In Proceedings of the 29th Annual ACM SIGIR Conference on Research and Development in Information Retrieval, pages 220–227.</rawString>
</citation>
<citation valid="true">
<authors>
<author>Sven Hartrumpf</author>
</authors>
<title>Semantic Decomposition for Question Answering.</title>
<date>2008</date>
<booktitle>In Proceedings on the 18th European Conference on Artificial Intelligence,</booktitle>
<pages>313--317</pages>
<contexts>
<context position="6530" citStr="Hartrumpf (2008)" startWordPosition="966" endWordPosition="967">ructure of the question is used to identify sub-questions that can be answered in isolation. A second form of question decomposition is semantic decomposition, which can semantically break individual questions apart to answer them in stages. For instance, the question “When did the third U.S. President die?” can be semantically decomposed “Who was the third U.S. President?” and “When did X die?”, where the answer to the first question is substituted into the second. Katz and Grau (2005) discusses this kind of decomposition using the syntactic structure, though it is not empirically validated. Hartrumpf (2008) proposes a decomposition method using only the deep semantic structure. Finally, Harabagiu et al. (2006) proposes a different type of question decomposition based on a random walk over similar questions extracted from a corpus. In our work, we focus on syntactic question decomposition. We demonstrate the importance of empirical evaluation of question decomposition, notably the pitfalls of heuristic approaches that rely entirely on the syntactic parse tree. Syntactic parsers trained on Treebank are particularly poor at both analyzing questions (Judge et al., 2006) and coordination boundaries (</context>
</contexts>
<marker>Hartrumpf, 2008</marker>
<rawString>Sven Hartrumpf. 2008. Semantic Decomposition for Question Answering. In Proceedings on the 18th European Conference on Artificial Intelligence, pages 313–317.</rawString>
</citation>
<citation valid="true">
<authors>
<author>Dierdre Hogan</author>
</authors>
<title>Coordinate Noun Phrase Disambiguation in a Generative Parsing Model.</title>
<date>2007</date>
<booktitle>In Proceedings of the 45th Annual Meeting of the Association for Computational Linguistics,</booktitle>
<pages>680--687</pages>
<contexts>
<context position="7142" citStr="Hogan, 2007" startWordPosition="1060" endWordPosition="1061"> proposes a decomposition method using only the deep semantic structure. Finally, Harabagiu et al. (2006) proposes a different type of question decomposition based on a random walk over similar questions extracted from a corpus. In our work, we focus on syntactic question decomposition. We demonstrate the importance of empirical evaluation of question decomposition, notably the pitfalls of heuristic approaches that rely entirely on the syntactic parse tree. Syntactic parsers trained on Treebank are particularly poor at both analyzing questions (Judge et al., 2006) and coordination boundaries (Hogan, 2007). Robust question decomposition methods, therefore, must be able to overcome many of these difficulties. 3 Consumer Health Question Decomposition Our goal is to decompose multi-sentence, multifaceted consumer health questions into concise questions coupled with important contextual information. To this end, we utilize a set of annotations that identify the decomposable elements and important contextual elements. A more detailed description of these annotations is provided in Roberts et al. (2014). The annotations are publicly available at our institution website1. Here, we briefly describe eac</context>
</contexts>
<marker>Hogan, 2007</marker>
<rawString>Dierdre Hogan. 2007. Coordinate Noun Phrase Disambiguation in a Generative Parsing Model. In Proceedings of the 45th Annual Meeting of the Association for Computational Linguistics, pages 680–687.</rawString>
</citation>
<citation valid="true">
<authors>
<author>John Judge</author>
<author>Aoife Cahill</author>
<author>Josef van Genabith</author>
</authors>
<title>QuestionBank: Creating a Corpus of ParseAnnotated Questions.</title>
<date>2006</date>
<booktitle>In Proceedings of the 21st International Conference on Computational Linguistics and 44th Annual Meeting of the Association for Computational Linguistics,</booktitle>
<pages>497--504</pages>
<marker>Judge, Cahill, van Genabith, 2006</marker>
<rawString>John Judge, Aoife Cahill, and Josef van Genabith. 2006. QuestionBank: Creating a Corpus of ParseAnnotated Questions. In Proceedings of the 21st International Conference on Computational Linguistics and 44th Annual Meeting of the Association for Computational Linguistics, pages 497–504.</rawString>
</citation>
<citation valid="true">
<authors>
<author>Yarden Katz</author>
<author>Bernardo C Grau</author>
</authors>
<title>Representing Qualitative Spatial Information in OWL-DL.</title>
<date>2005</date>
<booktitle>Proceedings of OWL: Experiences and Directions.</booktitle>
<contexts>
<context position="6405" citStr="Katz and Grau (2005)" startWordPosition="946" endWordPosition="949"> a multi-sentence summarization system. They perform what we refer to as syntactic question decomposition, where the syntactic structure of the question is used to identify sub-questions that can be answered in isolation. A second form of question decomposition is semantic decomposition, which can semantically break individual questions apart to answer them in stages. For instance, the question “When did the third U.S. President die?” can be semantically decomposed “Who was the third U.S. President?” and “When did X die?”, where the answer to the first question is substituted into the second. Katz and Grau (2005) discusses this kind of decomposition using the syntactic structure, though it is not empirically validated. Hartrumpf (2008) proposes a decomposition method using only the deep semantic structure. Finally, Harabagiu et al. (2006) proposes a different type of question decomposition based on a random walk over similar questions extracted from a corpus. In our work, we focus on syntactic question decomposition. We demonstrate the importance of empirical evaluation of question decomposition, notably the pitfalls of heuristic approaches that rely entirely on the syntactic parse tree. Syntactic par</context>
</contexts>
<marker>Katz, Grau, 2005</marker>
<rawString>Yarden Katz and Bernardo C. Grau. 2005. Representing Qualitative Spatial Information in OWL-DL. Proceedings of OWL: Experiences and Directions.</rawString>
</citation>
<citation valid="true">
<authors>
<author>Halil Kilicoglu</author>
<author>Marcelo Fiszman</author>
<author>Dina DemnerFushman</author>
</authors>
<title>Interpreting Consumer Health Questions: The Role of Anaphora and Ellipsis.</title>
<date>2013</date>
<booktitle>In Proceedings of the 2013 BioNLP Workshop,</booktitle>
<pages>54--62</pages>
<contexts>
<context position="2825" citStr="Kilicoglu et al., 2013" startWordPosition="406" endWordPosition="409">sition of this complex question would then result in four questions: 1. Will Fabry disease affect a transplanted kidney? 2. Will enzyme treatment for Fabry disease need to be continued after a kidney transplant? 3. What cautions are required to manage Fabry disease with a transplanted kidney? 4. What additional treatments are required to manage Fabry disease with a transplanted kidney? Each question above could be independently answered by a question answering (QA) system. While previous work has discussed methods for resolving co-reference and implicit arguments in consumer health questions (Kilicoglu et al., 2013), it does not address question decomposition. In this work, we propose methods for automatically recognizing six annotation types useful for decomposing consumer health questions. These annotations distinguish between sentences that contain questions and background information. They also identify when a question sentence can be split in multiple independent questions, and 29 Proceedings of the 2014 Workshop on Biomedical Natural Language Processing (BioNLP 2014), pages 29–37, Baltimore, Maryland USA, June 26-27 2014. c�2014 Association for Computational Linguistics when they contain optional o</context>
</contexts>
<marker>Kilicoglu, Fiszman, DemnerFushman, 2013</marker>
<rawString>Halil Kilicoglu, Marcelo Fiszman, and Dina DemnerFushman. 2013. Interpreting Consumer Health Questions: The Role of Anaphora and Ellipsis. In Proceedings of the 2013 BioNLP Workshop, pages 54–62.</rawString>
</citation>
<citation valid="true">
<authors>
<author>Finley Lacatusu</author>
<author>Andrew Hickl</author>
<author>Sanda Harabagiu</author>
</authors>
<title>Impact of Question Decomposition on the Quality of Answer Summaries.</title>
<date>2006</date>
<booktitle>In Proceedings of LREC,</booktitle>
<pages>1147--1152</pages>
<contexts>
<context position="5714" citStr="Lacatusu et al. (2006)" startWordPosition="838" endWordPosition="841"> more than one type per complex question. Without decomposing the full question into its sub-questions, however, the type cannot be associated with its specific span, or with other information specific to the sub-question. This other information can include answer types, question focus, and other answer constraints. By decomposing multi-sentence questions, these question-specific attributes can be extracted, and the discourse structure of the larger question can be better understood. Question decomposition has been utilized before in open-domain QA approaches, but rarely evaluated on its own. Lacatusu et al. (2006) demonstrates how question decomposition can improve the performance of a multi-sentence summarization system. They perform what we refer to as syntactic question decomposition, where the syntactic structure of the question is used to identify sub-questions that can be answered in isolation. A second form of question decomposition is semantic decomposition, which can semantically break individual questions apart to answer them in stages. For instance, the question “When did the third U.S. President die?” can be semantically decomposed “Who was the third U.S. President?” and “When did X die?”, </context>
</contexts>
<marker>Lacatusu, Hickl, Harabagiu, 2006</marker>
<rawString>Finley Lacatusu, Andrew Hickl, and Sanda Harabagiu. 2006. Impact of Question Decomposition on the Quality of Answer Summaries. In Proceedings of LREC, pages 1147–1152.</rawString>
</citation>
<citation valid="true">
<authors>
<author>Donald A B Lindberg</author>
<author>Betsy L Humphreys</author>
<author>Alexa T McCray</author>
</authors>
<title>The Unified Medical Language System.</title>
<date>1993</date>
<journal>Methods of Information in Medicine,</journal>
<volume>32</volume>
<issue>4</issue>
<contexts>
<context position="13346" citStr="Lindberg et al., 1993" startWordPosition="1988" endWordPosition="1991">lly a BACKGROUND) of the request while the questions are near the end. The questions cannot generally be answered outside the context of the FOCUS, however, so its identification is a critical part of decomposition. As shown in Figure 1, we use a 3-step process: (1) a high-recall method identifies potential FOCUS diseases in the data, (2) a support vector machine (SVM) ranks the FOCUS candidates, and (3) the highest-ranking candidate’s boundary is modified with a set of rules to better match our annotation standard. To identify candidates for the FOCUS, we use a lexicon constructed from UMLS (Lindberg et al., 1993). UMLS includes very generic terms, such as disease and cancer, that are too general to exactly match a FOCUS in our data. We allow these terms to be candidates so as to not miss any FOCUS that doesn’t exactly match an entry in UMLS. When such a general term is selected as the top-ranked FOCUS, the rules described below are capable of expanding the term to the full disease name. To rank candidates, we utilize an SVM (Fan et E/R P R F1 1st UMLS Disorder E 19.6 19.0 19.3 R 28.2 27.4 27.8 SVM E 56.4 54.7 55.6 R 89.2 86.5 87.9 SVM + Rules E 74.8 72.5 73.6 R 89.5 86.8 88.1 Table 1: FOCUS recognitio</context>
</contexts>
<marker>Lindberg, Humphreys, McCray, 1993</marker>
<rawString>Donald A.B. Lindberg, Betsy L. Humphreys, and Alexa T. McCray. 1993. The Unified Medical Language System. Methods of Information in Medicine, 32(4):281–291.</rawString>
</citation>
<citation valid="true">
<authors>
<author>Alexa T McCray</author>
<author>Anita Burgun</author>
<author>Olivier Bodenreider</author>
</authors>
<title>Aggregating UMLS Semantic Types for Reducing Conceptual Complexity.</title>
<date>2001</date>
<booktitle>In Studies in Health Technology and Informatics (MEDINFO),</booktitle>
<volume>84</volume>
<issue>1</issue>
<pages>216--220</pages>
<contexts>
<context position="14260" citStr="McCray et al., 2001" startWordPosition="2159" endWordPosition="2162">, the rules described below are capable of expanding the term to the full disease name. To rank candidates, we utilize an SVM (Fan et E/R P R F1 1st UMLS Disorder E 19.6 19.0 19.3 R 28.2 27.4 27.8 SVM E 56.4 54.7 55.6 R 89.2 86.5 87.9 SVM + Rules E 74.8 72.5 73.6 R 89.5 86.8 88.1 Table 1: FOCUS recognition results. E = exact match; R = relaxed match. al., 2008) with a small number of feature types: • Unigrams. Identifies generic words such as disease and syndrome that indicate good FOCUS candidates, while also recognizing noisy UMLS terms that are often false positives. • UMLS semantic group (McCray et al., 2001). • UMLS semantic type. • Sentence Offset. The FOCUS is typically in the first sentence, and is far more likely to be at the beginning of the request than the end. • Lexicon Offset. The FOCUS is typically the first disease mentioned. During training, the SVM considers any candidate that overlaps the gold FOCUS to be correct. This enables our approach to train on FOCUS examples that do not perfectly align with a UMLS concept. At test time, all candidates are classified, ranked by the classifier’s confidence, and the top-ranked candidate is considered the FOCUS. As mentioned above, there are dif</context>
</contexts>
<marker>McCray, Burgun, Bodenreider, 2001</marker>
<rawString>Alexa T McCray, Anita Burgun, and Olivier Bodenreider. 2001. Aggregating UMLS Semantic Types for Reducing Conceptual Complexity. In Studies in Health Technology and Informatics (MEDINFO), volume 84(1), pages 216–220.</rawString>
</citation>
<citation valid="true">
<authors>
<author>Kirk Roberts</author>
<author>Kate Masterton</author>
<author>Marcelo Fiszman</author>
<author>Halil Kilicoglu</author>
<author>Dina Demner-Fushman</author>
</authors>
<title>Annotating Question Decomposition on Complex Medical Questions.</title>
<date>2014</date>
<booktitle>In Proceedings of LREC.</booktitle>
<contexts>
<context position="7643" citStr="Roberts et al. (2014)" startWordPosition="1132" endWordPosition="1135">reebank are particularly poor at both analyzing questions (Judge et al., 2006) and coordination boundaries (Hogan, 2007). Robust question decomposition methods, therefore, must be able to overcome many of these difficulties. 3 Consumer Health Question Decomposition Our goal is to decompose multi-sentence, multifaceted consumer health questions into concise questions coupled with important contextual information. To this end, we utilize a set of annotations that identify the decomposable elements and important contextual elements. A more detailed description of these annotations is provided in Roberts et al. (2014). The annotations are publicly available at our institution website1. Here, we briefly describe each annotation: (1) BACKGROUND - a sentence indicating useful contextual information, but lacks a question. (2) QUESTION - a sentence or clause that indicates an independent question. 1http://lhncbc.nlm.nih.gov/project/consumer-healthquestion-answering 30 Coordination Recognition Question Recognition SVM Candidate Ranking Candidate Generation Candidate Generation SVM Candidate Filter SVM Candidate Filter Question Request Question Sentence WordNet Stanford Parser Coordination Sentence Classification</context>
<context position="11267" citStr="Roberts et al. (2014)" startWordPosition="1652" endWordPosition="1655">ed from the Genetic and Rare Diseases Information Center (GARD), which maintains a website2 with publicly available consumersubmitted questions and professionally-authored answers about genetic and rare diseases. We collected 1,467 consumer health questions, consisting of 4,115 sentences, 1,713 BACKGROUND sentences, 37 IGNORE sentences, 2,465 QUESTIONs, 367 COORDINATIONs, 53 EXEMPLIFICATIONs, and 1,513 FOCUS annotations. Questions with more than one FOCUS are generally concerned with the relation between diseases. Further information about the corpus and the annotation process can be found in Roberts et al. (2014). System Architecture The architecture of our question decomposition method is illustrated in 2http://rarediseases.info.nih.gov/gard 31 Figure 1. To avoid confusion, in the rest of this paper we refer to a complex consumer health question simply as a request. Requests are sent to the independent FOCUS recognition module (Section 4), and then proceed through a pipeline that includes the classification of sentences (Section 5), the identification of separate QUESTIONs within a question sentence (Section 6), the recognition of COORDINATIONs (Section 7) and EXEMPLIFICATIONs (Section 8), and the su</context>
</contexts>
<marker>Roberts, Masterton, Fiszman, Kilicoglu, Demner-Fushman, 2014</marker>
<rawString>Kirk Roberts, Kate Masterton, Marcelo Fiszman, Halil Kilicoglu, and Dina Demner-Fushman. 2014. Annotating Question Decomposition on Complex Medical Questions. In Proceedings of LREC.</rawString>
</citation>
<citation valid="true">
<authors>
<author>Hong Yu</author>
<author>YongGang Cao</author>
</authors>
<title>Automatically Extracting Information Needs from Ad Hoc Clinical Questions.</title>
<date>2008</date>
<booktitle>In Proceedings of the AMIA Annual Symposium.</booktitle>
<contexts>
<context position="4988" citStr="Yu and Cao (2008)" startWordPosition="730" endWordPosition="733"> frames from queries for the purpose of cohort retrieval. Their method assumes syntactic dependencies exist between the necessary frame elements, and is thus not well-suited to handle long, multi-sentence questions. Similarly, Andersen et al. (2012) proposes a method for converting a concise question into a structured query. However, many medical questions require background information that is difficult to encode in a single question sentence. Instead, it is often more natural to ask multiple questions over several sentences, providing background information to give context to the questions. Yu and Cao (2008) use a ML method to recognize question types in professional health questions. Their method can identify more than one type per complex question. Without decomposing the full question into its sub-questions, however, the type cannot be associated with its specific span, or with other information specific to the sub-question. This other information can include answer types, question focus, and other answer constraints. By decomposing multi-sentence questions, these question-specific attributes can be extracted, and the discourse structure of the larger question can be better understood. Questio</context>
</contexts>
<marker>Yu, Cao, 2008</marker>
<rawString>Hong Yu and YongGang Cao. 2008. Automatically Extracting Information Needs from Ad Hoc Clinical Questions. In Proceedings of the AMIA Annual Symposium.</rawString>
</citation>
</citationList>
</algorithm>
</algorithms>
<?xml version="1.0" encoding="UTF-8"?>
<algorithms version="110505">
<algorithm name="SectLabel" version="110505">
<variant no="0" confidence="0.002871">
<title confidence="0.943514">
Automated Error Detection in Digitized Cultural Heritage Documents
</title>
<author confidence="0.722823">
Kata G´abor
</author>
<note confidence="0.5222375">
INRIA &amp; Universit´e Paris 7
Domaine de Voluceau - BP 105
78153 Le Chesnay Cedex
FRANCE
</note>
<email confidence="0.995212">
kata.gabor@inria.fr
</email>
<sectionHeader confidence="0.993787" genericHeader="abstract">
Abstract
</sectionHeader>
<bodyText confidence="0.999785363636364">
The work reported in this paper aims
at performance optimization in the di-
gitization of documents pertaining to
the cultural heritage domain. A hybrid
method is proposed, combining statistical
classification algorithms and linguistic
knowledge to automatize post-OCR error
detection and correction. The current
paper deals with the integration of lin-
guistic modules and their impact on error
detection.
</bodyText>
<sectionHeader confidence="0.998799" genericHeader="introduction">
1 Introduction
</sectionHeader>
<bodyText confidence="0.99992875">
Providing wider access to national cultural her-
itage by massive digitization confronts the actors
of the field with a set of new challenges. State
of the art optical character recognition (OCR)
software currently achieve an error rate of around
1 to 10% depending on the age and the layout
of the text. While this quality may be adequate
for indexing, documents intended for reading
need to meet higher standards. A reduction of
the error rate by a factor of 10 to 100 becomes
necessary for the diffusion of digitized books
and journals through emerging technologies such
as e-books. Our paper deals with the automatic
post-processing of digitized documents with the
aim of reducing the OCR error rate by using
contextual information and linguistic processing,
by and large absent from current OCR engines. In
the current stage of the project, we are focusing
on French texts from the archives of the French
National Library (Biblioth`eque Nationale de
France) covering the period from 1646 to 1990.
We adopted a hybrid approach, making use
of both statistical classification techniques and
linguistically motivated modules to detect OCR
</bodyText>
<note confidence="0.951846">
Benoit Sagot
INRIA &amp; Universit´e Paris 7
Domaine de Voluceau - BP 105
78153 Le Chesnay Cedex
FRANCE
</note>
<email confidence="0.976956">
benoit.sagot@inria.fr
</email>
<bodyText confidence="0.99978495">
errors and generate correction candidates. The
technology is based on a symbolic linguistic pre-
processing, followed by a statistical module which
adpots the noisy channel model (Shannon, 1948).
Symbolic methods for error correction allow to
target specific phenomena with a high precision,
but they typically strongly rely on presumptions
about the nature of errors encountered. This draw-
back can be overcome by using the noisy channel
model (Kernighan et al., 1990; Brill and Moore,
2000; Kolak and Resnik, 2002; Mays et al., 1991;
Tong and Evans, 1996). However, error models in
such systems work best if they are created from
manually corrected training data, which are not
always available. Other alternatives to OCR error
correction include (weighted) FSTs (Beaufort
and Mancas-Thillou, 2007), voting systems using
the output of different OCR engines (Klein and
Kope, 2002), textual alignment combined with
dictionary lookup (Lund and Ringger, 2009), or
heuristic correction methods (Alex et al., 2012).
While correction systems rely less and less on
pre-existing external dictionaries, a shift can be
observed towards methods that dinamically create
lexicons either by exploiting the Web (Cucerzan
and Brill, 2004; Strohmaier et al., 2003) or from
the corpus (Reynaert, 2004).
As to linguistically enhanced models, POS
tagging was succesfully applied to spelling cor-
rection (Golding and Schabes, 1996; Schaback,
2007). However, to our knowledge, very little
work has been done to exploit linguistic analysis
for post-OCR correction (Francom and Hulden,
2013). We propose to apply a shallow processing
module to detect certain types of named entities
(NEs), and a POS tagger trained specifically to
deal with NE-tagged input. Our studies aim to
demonstrate that linguistic preprocessing can
efficiently contribute to reduce the error rate by
1) detecting false corrections proposed by the
</bodyText>
<page confidence="0.973047">
56
</page>
<note confidence="0.642626666666667">
Proceedings of the 8th Workshop on Language Technology for Cultural Heritage, Social Sciences, and Humanities (LaTeCH) @ EACL 2014, pages 56–61,
Gothenburg, Sweden, April 26 2014. c�2014 Association for Computational Linguistics
OCR output
</note>
<bodyText confidence="0.924189352941177">
decoding: creation
of hypothesis lattice
correction: ranking of the hypotheses
statistical correction module, 2) detecting OCR
errors which are unlikely to be detected by the
statistical correction module. We argue that
named entity grammars can be adapted to the
correction task at a low cost and they allow to
target specific types of errors with a very high
precision.
In what follows, we present the global architec-
ture of the post-OCR correction system (2), the
named entity recognition module (3), as well as
our experiments in named entity-aware POS tag-
ging (4). The predicted impact of the linguistic
modules is illustrated in section 5. Finally, we
present ongoing work and the conclusion (6).
</bodyText>
<sectionHeader confidence="0.961541" genericHeader="method">
2 System Architecture
</sectionHeader>
<bodyText confidence="0.999852111111111">
Our OCR error detection and correction system
uses a hybrid methodology with a symbolic mod-
ule for linguistic preprocessing, a POS tagger, fol-
lowed by statistical decoding and correction mod-
ules. The SxPipe toolchain (Sagot and Boullier,
2008) is used for shallow processing tasks (to-
kenisation, sentence segmentation, named entity
recognition). The NE-tagged text is input to POS
tagging with MElt-h, a hybrid version of the MElt
tagger (Denis and Sagot, 2010; Denis and Sagot,
2012). MELT-h can take both NE tagged texts and
raw text as input.
The decoding phase is based on the noisy channel
model (Shannon, 1948) adapted to spell checking
(Kernighan et al., 1990). In a noisy channel model,
given an input string s, we want to find the word w
which maximizes P(wjs). Using Bayes theorem,
this can be written as:
</bodyText>
<equation confidence="0.997015">
argmax(w)P(sjw) ~ P(w) (1)
</equation>
<bodyText confidence="0.999816285714286">
where P(w) is given by the language model ob-
tained from clean corpora. Both sentence-level
(Tong and Evans, 1996; Boswell, 2004) and word-
level (Mays et al., 1991) language models can be
used. P(sjw) is given by the error model, repre-
sented as a confusion matrix calculated from our
training corpus in which OCR output is aligned
with its manually corrected, noiseless equivalent.
The post-correction process is summarized in 1.
The integration of a symbolic module for NE
recognition and the use of part of speech and
named entity tags constitute a novel aspect in our
method. Moreover, linguistic preprocessing al-
lows us to challenge tokenisation decisions prior
</bodyText>
<figure confidence="0.73936925">
Preprocessing with Sx-
Pipe (character level)
POS and NE tagging
tokenization revisited
</figure>
<figureCaption confidence="0.999784">
Figure 1: Architecture
</figureCaption>
<bodyText confidence="0.999959090909091">
to and during the decoding phase (similarly to Ko-
lak (2005)) ; this constitutes a significant feature
as OCR errors often boil down to a fusion or split
of tokens.
The corpus we use comes from the archives of
the French National Library and contains 1 500
documents (50 000 000 tokens). This corpus is
available both as a ”reference corpus”, i.e., in a
manually corrected, clean version, and as a ”con-
trast corpus”, i.e., a noisy OCR output version.
These variants are aligned at the sentence level.
</bodyText>
<sectionHeader confidence="0.978621" genericHeader="method">
3 Named entity tagging
</sectionHeader>
<subsectionHeader confidence="0.97136">
3.1 NE recognition methodology
</subsectionHeader>
<bodyText confidence="0.999962473684211">
As a first step in error detection, the OCR out-
put is analysed in search of ”irregular” charac-
ter sequences such as named entities. This pro-
cess is implemented with SxPipe (Sagot and Boul-
lier, 2008), a freely available1, robust and modu-
lar multilingual processing chain for unrestricted
text. SxPipe contains modules for named en-
tity recognition, tokenization, sentence segmenta-
tion, non-deterministic multi-word expression de-
tection, spelling correction and lexicon-based pat-
terns detection. The SxPipe chain is fully cus-
tomizable with respect to input language, domain,
text type and the modules to be used. Users are
also free to add their own modules to the chain.
In accordance with our purposes, we defined
named entities as sequences of characters which
cannot be analysed morphologically or syntacti-
cally, yet follow productive patterns. Such enti-
ties do not adhere to regular tokenization patterns
</bodyText>
<footnote confidence="0.988338">
1https://gforge.inria.fr/projects/lingwb/
</footnote>
<page confidence="0.998583">
57
</page>
<bodyText confidence="0.999957">
since they often include punctuation marks, usu-
ally considered as separators. As compared to the
consensual use of the term (Maynard et al., 2001;
Chinchor, 1998; Sang and Meulder, 2003), our
definition covers a wider range of entities, e.g., nu-
merals, currency units, dimensions.2 The correct
annotation of these entities has a double relevance
for our project:
</bodyText>
<listItem confidence="0.980471555555556">
• NE tagging prior to POS tagging helps to im-
prove the accuracy of the latter.
• NE tagging allows to detect and, eventu-
ally, correct OCR errors which occur inside
NEs. Conversely, it can also contribute to de-
tect false correction candidates when the se-
quence of characters forming the NE would
otherwise be assigned a low probability by
the language model.
</listItem>
<bodyText confidence="0.999143181818182">
given entity category, based on the presence of
category-specific markers (lexical units, acronyms
etc.)4. However, chemical formulae were eval-
uated directly on sentences extracted from the
archives of the European Patent Office; no filter-
ing was needed due to the density of formulae in
these documents.
Legal IDs were evaluated on a legal corpus from
the Publications Office of the European Union,
while the rest of the grammars were evaluated us-
ing the BNF corpus.
</bodyText>
<table confidence="0.9637518">
Entity Type Precision Recall
DATE 0.98 0.97
ADDRESS 0.83 0.86
LEGAL 0.88 0.82
CHEMICAL 0.94 -
</table>
<tableCaption confidence="0.999933">
Table 1: Evaluation of NE grammars
</tableCaption>
<bodyText confidence="0.999980904761905">
The named entity recognition module is imple-
mented in Perl as a series of local grammars. Local
grammars constitute a simple and powerful tool to
recognize open classes of entities (Friburger and
Maurel, 2004; Maynard et al., 2002; Bontcheva
et al., 2002); we are concerned with time ex-
pressions, addresses, currency units, dimensions,
chemical formulae and legal IDs. Named entity
grammars are applied to the raw corpus before to-
kenization and segmentation. Our grammars are
robust in the sense that they inherently recognize
and correct some types of frequent OCR errors in
the input.3 SxPipe’s architecture allows to define
an OCR-specific correction mode as an input pa-
rameter and hence apply robust recognition and
correction to noisy output, while requiring exact
matching for clean texts. However, maximizing
precision remains our primary target, as a false
correction is more costly than the non-correction
of an eventual error at this stage. Therefore, our
grammars are built around unambiguous markers.
</bodyText>
<subsectionHeader confidence="0.999848">
3.2 Evaluation of NE tagging
</subsectionHeader>
<bodyText confidence="0.999931">
A manual, application-independent evaluation
was carried out, concentrating primarily on preci-
sion for the reasons mentioned in 3. For four types
of NEs, we collected a sample of 200 sentences
expected to contain one or more instances of the
</bodyText>
<footnote confidence="0.8543276">
2Our current experiments do not cover single-word proper
names.
3E.g., A numerical 0 inside a chemical formula is pre-
sumed in most cases to be an erroneous hypothesis for alpha-
betical O.
</footnote>
<sectionHeader confidence="0.991618" genericHeader="method">
4 POS tagging
</sectionHeader>
<subsectionHeader confidence="0.999525">
4.1 MEltFR and MElt-h
</subsectionHeader>
<bodyText confidence="0.999920217391304">
The following step in the chain is POS tagging
using a named entity-aware version of the MElt
tagger. MElt (Denis and Sagot, 2010; Denis and
Sagot, 2012) is a maximum entropy POS tagger
which differs from other systems in that it uses
both corpus-based features and a large-coverage
lexicon as an external source of information. Its
French version, MElt-FR was trained on the Leff
lexicon (Sagot, 2010) and on the French TreeBank
(FTB) (Abeill´e et al., 2003). The training corpus
uses a tagset consisting of 29 tags. MEltFR yields
state of the art results for French, namely 97.8%
accuracy on the test set.
In order to integrate MElt into our toolchain,
the tagger needed to be trained to read NE-tagged
texts as output by SxPipe. We thus extended
the FTB with 332 manually annotated sentences
(15 500 tokens) containing real examples for each
type of NE covered by our version of SxPipe. Sx-
Pipe’s output format was slightly modified to fa-
cilitate learning: entities covered by the grammars
were replaced by pseudo-words corresponding to
their category. The training corpus is the union
</bodyText>
<footnote confidence="0.935793">
4Although this sampling is biased towards entities with
a certain type of marker, it gives an approximation on the
recall, as opposed to simply extracting hits of our grammars
and evaluating only their precision.
</footnote>
<page confidence="0.998995">
58
</page>
<bodyText confidence="0.999921411764706">
of the FTB and the small NE corpus annotated
with 35 categories (29 POS and 6 named entity
categories). We used this corpus to train MElt-h,
a hybrid tagger compatible with our OCR post-
processing toolchain. MElt-h can tag both raw
corpora (using the 29 POS categories learnt from
the FTB), and NE-annotated texts (preprocessed
with SxPipe or any other tool, as long as the for-
mat is consistent with the output of SxPipe).
Training a tagger on a heterogeneous corpus like
the one we used is theoretically challengeable.
Therefore, careful attention was paid to evaluat-
ing it on both NE-annotated data and on the FTB
test corpus. The latter result is meant to indi-
cate whether there is a decrease in performance
compared to the “original” MEltFR tagger, trained
solely on FTB data.
</bodyText>
<subsectionHeader confidence="0.997402">
4.2 Evaluation of POS and NE tagging
</subsectionHeader>
<bodyText confidence="0.9995187">
A set of experiments were performed using
different sections of the NE-annotated training
data. First, we cut out 100 sentences at random
and used them as a test corpus. From the rest
of the sentences, we created diverse random
partitionings using 50, 100, 150 and all the 232
sentences as training data. We trained MElt-h
on each training corpus and evaluated it on the
test section of the FTB as well as on the 100
NE-annotated sentences.
</bodyText>
<table confidence="0.997695142857143">
#sentences Prec on FTB Prec on PACTE-NE
0 97.83 —
50 97.82 95.61
100 97.80 95.71
150 97.78 95.76
200 97.78 95.84
232 97.75 96.20
</table>
<tableCaption confidence="0.868082">
Table 2: Evaluation of MElt-h on the FTB and on
the NE-annotated corpus
</tableCaption>
<bodyText confidence="0.981382222222222">
The results confirm that adding NE-annotated
sentences to the training corpus does not decrease
precision on the FTB itself. Furthermore, we note
that the results on the NE corpus are slightly infe-
rior to the results on the FTB, but the figures sug-
gest that the learning curve did not reach a limit for
NE-annotated data: adding more NE-annotated
sentences will probably increase precision.
5 Expected impact on OCR error
reduction
While the major impact of named entity tagging
and NE-enriched POS tagging is expected to re-
sult from their integration into the language model,
series of experiments are currently being carried
out to estimate the efficiency of the symbolic cor-
rection module and the quantity of the remain-
ing OCR errors inside named entities. A sample
of 500.000 sentences (15.500.000 tokens) was ex-
tracted from the BNF corpus to be used for a com-
parison and case studies, both in the noisy OCR
output version and in the editorial quality version.
Both types of texts were tagged for NEs with Sx-
Pipe, using the “clean input” mode (without tol-
erance for errors and correction candidates). Only
65% of the recognized NEs are identical, implying
that 35% of the named entities are very likely to
contain an OCR error.5 To investigate further, we
applied the grammars one by one in “noisy input”
mode. This setting allows to detect certain types
of typical OCR errors, with an efficiency ranging
from 0 (no tolerance) to 10% i.e., up to this quan-
tity of erroneous input can be detected and cor-
rectly tagged with certain named entity grammars.
Detailed case studies are currently being carried
out to determine the exact precision of the correc-
tion module.
</bodyText>
<sectionHeader confidence="0.983917" genericHeader="conclusions">
6 Conclusion and Future Work
</sectionHeader>
<bodyText confidence="0.999958176470588">
We described an architecture for post-OCR error
detection in documents pertaining to the cultural
heritage domain. Among other characteristics, the
specificity of our model consists in a combina-
tion of linguistic analysis and statistical modules,
which interact at different stages in the error detec-
tion and correction process. The first experiments
carried out within the project suggest that linguis-
tically informed modules can efficiently comple-
ment statistical methods for post-OCR error detec-
tion. Our principal future direction is towards the
integration of NE-enriched POS tagging informa-
tion into the language models, in order to provide a
finer grained categorization and account for these
phenomena. A series of experiences are planned
to be undertaken, using different combinations of
token-level information.
</bodyText>
<footnote confidence="0.941317">
5In the less frequent case, divergences can also be due to
errors in the editorial quality text.
</footnote>
<page confidence="0.998925">
59
</page>
<sectionHeader confidence="0.961245" genericHeader="references">
References
</sectionHeader>
<reference confidence="0.969743318181818">
Anne Abeill´e, Lionel Cl´ement, and Franc¸ois Toussenel.
2003. Building a treebank for French. In Anne
Abeill´e, editor, Treebanks. Kluwer, Dordrecht.
Bea Alex, Claire Grover, Ewan Klein, and Richard To-
bin. 2012. Digitised historical text: Does it have
to be mediOCRe? In Proceedings of KONVENS
2012 (LThist 2012 workshop), pages 401–409, Vi-
enna, Austria.
Richard Beaufort and C´eline Mancas-Thillou. 2007.
A weighted finite-state framework for correcting er-
rors in natural scene OCR. In Proceedings of the
Ninth International Conference on Document Anal-
ysis and Recognition, pages 889–893, Washington,
DC, USA.
Kalina Bontcheva, Marin Dimitrov, Diana Maynard,
Valentin Tablan, and Hamish Cunningham. 2002.
Shallow methods for named entity coreference res-
olution. In Proceedings of the TALN 2002 Confer-
ence.
Dustin Boswell. 2004. Language models for spelling
correction. CSE, 256.
Eric Brill and Robert C. Moore. 2000. An Improved
Error Model for Noisy Channel Spelling Correction.
In Proceedings of the 38th ACL Conference, pages
286–293.
Nancy Chinchor. 1998. Muc-7 named entity task def-
inition. In Seventh Message Understanding Confer-
ence (MUC-7).
Silviu Cucerzan and Eric Brill. 2004. Spelling correc-
tion as an iterative process that exploits the collec-
tive knowledge of web users. In Proceedings of Pro-
ceedings of the 2004 Conference on Empirical Meth-
ods in Natural Language Processing (EMNLP’04),
pages 293–300, Barcelona, Spain.
Pascal Denis and Benoit Sagot. 2010. Exploita-
tion d’une ressource lexicale pour la construction
d’un ´etiqueteur morphosyntaxique ´etat-de-l’art du
franc¸ais. In Traitement Automatique des Langues
Naturelles : TALN 2010, Montr´eal, Canada.
Pascal Denis and Benoit Sagot. 2012. Coupling an
annotated corpus and a lexicon for state-of-the-art
POS tagging. Language Resources and Evaluation,
46(4):721–736.
Jerid Francom and Mans Hulden. 2013. Diacritic er-
ror detection and restoration via part-of-speech tags.
In Proceedings of the 6th Language and Technology
Conference.
Nathalie Friburger and Denis Maurel. 2004. Finite-
state transducer cascades to extract named entities in
texts. Theoretical Computer Science, 313:94–104.
Andrew Golding and Yves Schabes. 1996. Com-
bining trigram-based and feature-based methods for
context-sensitive spelling correction. In ACL, pages
71–78.
Mark Kernighan, Kenneth Church, and William Gale.
1990. A spelling correction program based on
a noisy channel model. In Proceedings of the
13th conference on Computational linguistics, pages
205–210.
Samuel Klein and Miri Kope. 2002. A voting system
for automatic OCR correction. In Proceedings of the
Workshop On Information Retrieval and OCR: From
Converting Content to Grasping Meaning, pages 1–
21, Tampere, Finland.
Okan Kolak and Philip Resnik. 2002. OCR error
correction using a noisy channel model. In Pro-
ceedings of the Second International Conference on
Human Language Technology Research (HLT’02),
pages 257–262, San Diego, USA.
Okan Kolak and Philip Resnik. 2005. OCR post-
processing for low density languages. In Proceed-
ings of the HLT-EMNLP Conference, pages 867–
874.
William Lund and Eric Ringger. 2009. Improv-
ing optical character recognition through efficient
multiple system alignment. In Proceedings of the
9th ACM/IEEE-CS Joint Conference on Digital Li-
braries (JCDL’09), pages 231–240, Austin, USA.
Diana Maynard, Valentin Tablan, Cristian Ursu,
Hamish Cunningham, and Yorick Wilks. 2001.
Named entity recognition from diverse text types. In
In Proceedings of the Recent Advances in Natural
Language Processing Conference, pages 257–274.
Diana Maynard, Valentin Tablan, Hamish Cunning-
ham, Cristian Ursu, Horacio Saggion, Kalina
Bontcheva, and Yorick Wilks. 2002. Architectural
elements of language engineering robustness. Jour-
nal of Natural Language Engineering - Special Issue
on Robust Methods in Analysis of Natural Language
Data, 8:257–274.
Eric Mays, Fred Damerau, and Robert Mercer. 1991.
Context based spelling correction. Information Pro-
cessing and Management, 23 (5):517–522.
Martin Reynaert. 2004. Multilingual text induced
spelling correction. In Proceedings of the Workshop
on Multilingual Linguistic Ressources (MLR’04),
pages 117–117.
Benoit Sagot and Pierre Boullier. 2008. SxPipe 2: ar-
chitecture pour le traitement pr´e-syntaxique de cor-
pus bruts. Traitement Automatique des Langues,
49(2):155–188.
Benoit Sagot. 2010. The lefff, a freely available
and large-coverage morphological and syntactic lex-
icon for french. In Proceedings of LREC 2010, La
Valette, Malte.
Erik Tjong Kim Sang and Fien De Meulder. 2003. In-
troduction to the conll-2003 shared task: Language-
independent named entity recognition. In In
Proceedings of Computational Natural Language
Learning, pages 142–147. ACL Press.
</reference>
<page confidence="0.966755">
60
</page>
<reference confidence="0.999743833333333">
Johannes Schaback. 2007. Multi-level feature extrac-
tion for spelling correction. In IJCAI Workshop on
Analytics for Noisy Unstructured Text Data, pages
79–86.
Claude Shannon. 1948. A Mathematical Theory of
Communication. Bell System Technical Journal, 27
(3):379–423.
Christan Strohmaier, Cristoph Ringlstetter, Klaus
Schulz, and Stoyan Mihov. 2003. Lexical post-
correction of OCR-results: the web as a dynamic
secondary dictionary? In Proceedings of the Sev-
enth International Conference on Document Anal-
ysis and Recognition (ICDAR’03), page 11331137,
Edinburgh, Royaume-Uni.
Xiang Tong and David Evans. 1996. A statistical ap-
proach to automatic OCR error correction in context.
In Proceedings of the Fourth Workshop on Very large
Corpora, pages 88–100.
</reference>
<page confidence="0.999272">
61
</page>
</variant>
</algorithm>
<algorithm name="ParsHed" version="110505">
<variant no="0" confidence="0.559636">
<title confidence="0.999402">Automated Error Detection in Digitized Cultural Heritage Documents</title>
<author confidence="0.992606">Kata</author>
<affiliation confidence="0.829409">INRIA &amp; Universit´e Paris Domaine de Voluceau - BP</affiliation>
<address confidence="0.99223">78153 Le Chesnay</address>
<email confidence="0.990956">kata.gabor@inria.fr</email>
<abstract confidence="0.988711583333333">The work reported in this paper aims at performance optimization in the digitization of documents pertaining to the cultural heritage domain. A hybrid method is proposed, combining statistical classification algorithms and linguistic knowledge to automatize post-OCR error detection and correction. The current paper deals with the integration of linguistic modules and their impact on error detection.</abstract>
</variant>
</algorithm>
<algorithm name="ParsCit" version="110505">
<citationList>
<citation valid="true">
<authors>
<author>Anne Abeill´e</author>
<author>Lionel Cl´ement</author>
<author>Franc¸ois Toussenel</author>
</authors>
<title>Building a treebank for French.</title>
<date>2003</date>
<editor>In Anne Abeill´e, editor, Treebanks.</editor>
<publisher>Kluwer,</publisher>
<location>Dordrecht.</location>
<marker>Abeill´e, Cl´ement, Toussenel, 2003</marker>
<rawString>Anne Abeill´e, Lionel Cl´ement, and Franc¸ois Toussenel. 2003. Building a treebank for French. In Anne Abeill´e, editor, Treebanks. Kluwer, Dordrecht.</rawString>
</citation>
<citation valid="true">
<authors>
<author>Bea Alex</author>
<author>Claire Grover</author>
<author>Ewan Klein</author>
<author>Richard Tobin</author>
</authors>
<title>Digitised historical text: Does it have to be mediOCRe?</title>
<date>2012</date>
<booktitle>In Proceedings of KONVENS 2012 (LThist 2012 workshop),</booktitle>
<pages>401--409</pages>
<location>Vienna, Austria.</location>
<contexts>
<context position="2880" citStr="Alex et al., 2012" startWordPosition="439" endWordPosition="442">ack can be overcome by using the noisy channel model (Kernighan et al., 1990; Brill and Moore, 2000; Kolak and Resnik, 2002; Mays et al., 1991; Tong and Evans, 1996). However, error models in such systems work best if they are created from manually corrected training data, which are not always available. Other alternatives to OCR error correction include (weighted) FSTs (Beaufort and Mancas-Thillou, 2007), voting systems using the output of different OCR engines (Klein and Kope, 2002), textual alignment combined with dictionary lookup (Lund and Ringger, 2009), or heuristic correction methods (Alex et al., 2012). While correction systems rely less and less on pre-existing external dictionaries, a shift can be observed towards methods that dinamically create lexicons either by exploiting the Web (Cucerzan and Brill, 2004; Strohmaier et al., 2003) or from the corpus (Reynaert, 2004). As to linguistically enhanced models, POS tagging was succesfully applied to spelling correction (Golding and Schabes, 1996; Schaback, 2007). However, to our knowledge, very little work has been done to exploit linguistic analysis for post-OCR correction (Francom and Hulden, 2013). We propose to apply a shallow processing </context>
</contexts>
<marker>Alex, Grover, Klein, Tobin, 2012</marker>
<rawString>Bea Alex, Claire Grover, Ewan Klein, and Richard Tobin. 2012. Digitised historical text: Does it have to be mediOCRe? In Proceedings of KONVENS 2012 (LThist 2012 workshop), pages 401–409, Vienna, Austria.</rawString>
</citation>
<citation valid="true">
<authors>
<author>Richard Beaufort</author>
<author>C´eline Mancas-Thillou</author>
</authors>
<title>A weighted finite-state framework for correcting errors in natural scene OCR.</title>
<date>2007</date>
<booktitle>In Proceedings of the Ninth International Conference on Document Analysis and Recognition,</booktitle>
<pages>889--893</pages>
<location>Washington, DC, USA.</location>
<contexts>
<context position="2670" citStr="Beaufort and Mancas-Thillou, 2007" startWordPosition="408" endWordPosition="411">channel model (Shannon, 1948). Symbolic methods for error correction allow to target specific phenomena with a high precision, but they typically strongly rely on presumptions about the nature of errors encountered. This drawback can be overcome by using the noisy channel model (Kernighan et al., 1990; Brill and Moore, 2000; Kolak and Resnik, 2002; Mays et al., 1991; Tong and Evans, 1996). However, error models in such systems work best if they are created from manually corrected training data, which are not always available. Other alternatives to OCR error correction include (weighted) FSTs (Beaufort and Mancas-Thillou, 2007), voting systems using the output of different OCR engines (Klein and Kope, 2002), textual alignment combined with dictionary lookup (Lund and Ringger, 2009), or heuristic correction methods (Alex et al., 2012). While correction systems rely less and less on pre-existing external dictionaries, a shift can be observed towards methods that dinamically create lexicons either by exploiting the Web (Cucerzan and Brill, 2004; Strohmaier et al., 2003) or from the corpus (Reynaert, 2004). As to linguistically enhanced models, POS tagging was succesfully applied to spelling correction (Golding and Scha</context>
</contexts>
<marker>Beaufort, Mancas-Thillou, 2007</marker>
<rawString>Richard Beaufort and C´eline Mancas-Thillou. 2007. A weighted finite-state framework for correcting errors in natural scene OCR. In Proceedings of the Ninth International Conference on Document Analysis and Recognition, pages 889–893, Washington, DC, USA.</rawString>
</citation>
<citation valid="true">
<authors>
<author>Kalina Bontcheva</author>
<author>Marin Dimitrov</author>
<author>Diana Maynard</author>
<author>Valentin Tablan</author>
<author>Hamish Cunningham</author>
</authors>
<title>Shallow methods for named entity coreference resolution.</title>
<date>2002</date>
<booktitle>In Proceedings of the TALN 2002 Conference.</booktitle>
<contexts>
<context position="9430" citStr="Bontcheva et al., 2002" startWordPosition="1483" endWordPosition="1486"> no filtering was needed due to the density of formulae in these documents. Legal IDs were evaluated on a legal corpus from the Publications Office of the European Union, while the rest of the grammars were evaluated using the BNF corpus. Entity Type Precision Recall DATE 0.98 0.97 ADDRESS 0.83 0.86 LEGAL 0.88 0.82 CHEMICAL 0.94 - Table 1: Evaluation of NE grammars The named entity recognition module is implemented in Perl as a series of local grammars. Local grammars constitute a simple and powerful tool to recognize open classes of entities (Friburger and Maurel, 2004; Maynard et al., 2002; Bontcheva et al., 2002); we are concerned with time expressions, addresses, currency units, dimensions, chemical formulae and legal IDs. Named entity grammars are applied to the raw corpus before tokenization and segmentation. Our grammars are robust in the sense that they inherently recognize and correct some types of frequent OCR errors in the input.3 SxPipe’s architecture allows to define an OCR-specific correction mode as an input parameter and hence apply robust recognition and correction to noisy output, while requiring exact matching for clean texts. However, maximizing precision remains our primary target, a</context>
</contexts>
<marker>Bontcheva, Dimitrov, Maynard, Tablan, Cunningham, 2002</marker>
<rawString>Kalina Bontcheva, Marin Dimitrov, Diana Maynard, Valentin Tablan, and Hamish Cunningham. 2002. Shallow methods for named entity coreference resolution. In Proceedings of the TALN 2002 Conference.</rawString>
</citation>
<citation valid="true">
<authors>
<author>Dustin Boswell</author>
</authors>
<title>Language models for spelling correction.</title>
<date>2004</date>
<journal>CSE,</journal>
<pages>256</pages>
<contexts>
<context position="5706" citStr="Boswell, 2004" startWordPosition="885" endWordPosition="886">ed text is input to POS tagging with MElt-h, a hybrid version of the MElt tagger (Denis and Sagot, 2010; Denis and Sagot, 2012). MELT-h can take both NE tagged texts and raw text as input. The decoding phase is based on the noisy channel model (Shannon, 1948) adapted to spell checking (Kernighan et al., 1990). In a noisy channel model, given an input string s, we want to find the word w which maximizes P(wjs). Using Bayes theorem, this can be written as: argmax(w)P(sjw) ~ P(w) (1) where P(w) is given by the language model obtained from clean corpora. Both sentence-level (Tong and Evans, 1996; Boswell, 2004) and wordlevel (Mays et al., 1991) language models can be used. P(sjw) is given by the error model, represented as a confusion matrix calculated from our training corpus in which OCR output is aligned with its manually corrected, noiseless equivalent. The post-correction process is summarized in 1. The integration of a symbolic module for NE recognition and the use of part of speech and named entity tags constitute a novel aspect in our method. Moreover, linguistic preprocessing allows us to challenge tokenisation decisions prior Preprocessing with SxPipe (character level) POS and NE tagging t</context>
</contexts>
<marker>Boswell, 2004</marker>
<rawString>Dustin Boswell. 2004. Language models for spelling correction. CSE, 256.</rawString>
</citation>
<citation valid="true">
<authors>
<author>Eric Brill</author>
<author>Robert C Moore</author>
</authors>
<title>An Improved Error Model for Noisy Channel Spelling Correction.</title>
<date>2000</date>
<booktitle>In Proceedings of the 38th ACL Conference,</booktitle>
<pages>286--293</pages>
<contexts>
<context position="2361" citStr="Brill and Moore, 2000" startWordPosition="361" endWordPosition="364">detect OCR Benoit Sagot INRIA &amp; Universit´e Paris 7 Domaine de Voluceau - BP 105 78153 Le Chesnay Cedex FRANCE benoit.sagot@inria.fr errors and generate correction candidates. The technology is based on a symbolic linguistic preprocessing, followed by a statistical module which adpots the noisy channel model (Shannon, 1948). Symbolic methods for error correction allow to target specific phenomena with a high precision, but they typically strongly rely on presumptions about the nature of errors encountered. This drawback can be overcome by using the noisy channel model (Kernighan et al., 1990; Brill and Moore, 2000; Kolak and Resnik, 2002; Mays et al., 1991; Tong and Evans, 1996). However, error models in such systems work best if they are created from manually corrected training data, which are not always available. Other alternatives to OCR error correction include (weighted) FSTs (Beaufort and Mancas-Thillou, 2007), voting systems using the output of different OCR engines (Klein and Kope, 2002), textual alignment combined with dictionary lookup (Lund and Ringger, 2009), or heuristic correction methods (Alex et al., 2012). While correction systems rely less and less on pre-existing external dictionari</context>
</contexts>
<marker>Brill, Moore, 2000</marker>
<rawString>Eric Brill and Robert C. Moore. 2000. An Improved Error Model for Noisy Channel Spelling Correction. In Proceedings of the 38th ACL Conference, pages 286–293.</rawString>
</citation>
<citation valid="true">
<authors>
<author>Nancy Chinchor</author>
</authors>
<title>Muc-7 named entity task definition.</title>
<date>1998</date>
<booktitle>In Seventh Message Understanding Conference (MUC-7).</booktitle>
<contexts>
<context position="8018" citStr="Chinchor, 1998" startWordPosition="1252" endWordPosition="1253">Pipe chain is fully customizable with respect to input language, domain, text type and the modules to be used. Users are also free to add their own modules to the chain. In accordance with our purposes, we defined named entities as sequences of characters which cannot be analysed morphologically or syntactically, yet follow productive patterns. Such entities do not adhere to regular tokenization patterns 1https://gforge.inria.fr/projects/lingwb/ 57 since they often include punctuation marks, usually considered as separators. As compared to the consensual use of the term (Maynard et al., 2001; Chinchor, 1998; Sang and Meulder, 2003), our definition covers a wider range of entities, e.g., numerals, currency units, dimensions.2 The correct annotation of these entities has a double relevance for our project: • NE tagging prior to POS tagging helps to improve the accuracy of the latter. • NE tagging allows to detect and, eventually, correct OCR errors which occur inside NEs. Conversely, it can also contribute to detect false correction candidates when the sequence of characters forming the NE would otherwise be assigned a low probability by the language model. given entity category, based on the pres</context>
</contexts>
<marker>Chinchor, 1998</marker>
<rawString>Nancy Chinchor. 1998. Muc-7 named entity task definition. In Seventh Message Understanding Conference (MUC-7).</rawString>
</citation>
<citation valid="true">
<authors>
<author>Silviu Cucerzan</author>
<author>Eric Brill</author>
</authors>
<title>Spelling correction as an iterative process that exploits the collective knowledge of web users.</title>
<date>2004</date>
<booktitle>In Proceedings of Proceedings of the 2004 Conference on Empirical Methods in Natural Language Processing (EMNLP’04),</booktitle>
<pages>293--300</pages>
<location>Barcelona,</location>
<contexts>
<context position="3092" citStr="Cucerzan and Brill, 2004" startWordPosition="470" endWordPosition="473">ork best if they are created from manually corrected training data, which are not always available. Other alternatives to OCR error correction include (weighted) FSTs (Beaufort and Mancas-Thillou, 2007), voting systems using the output of different OCR engines (Klein and Kope, 2002), textual alignment combined with dictionary lookup (Lund and Ringger, 2009), or heuristic correction methods (Alex et al., 2012). While correction systems rely less and less on pre-existing external dictionaries, a shift can be observed towards methods that dinamically create lexicons either by exploiting the Web (Cucerzan and Brill, 2004; Strohmaier et al., 2003) or from the corpus (Reynaert, 2004). As to linguistically enhanced models, POS tagging was succesfully applied to spelling correction (Golding and Schabes, 1996; Schaback, 2007). However, to our knowledge, very little work has been done to exploit linguistic analysis for post-OCR correction (Francom and Hulden, 2013). We propose to apply a shallow processing module to detect certain types of named entities (NEs), and a POS tagger trained specifically to deal with NE-tagged input. Our studies aim to demonstrate that linguistic preprocessing can efficiently contribute </context>
</contexts>
<marker>Cucerzan, Brill, 2004</marker>
<rawString>Silviu Cucerzan and Eric Brill. 2004. Spelling correction as an iterative process that exploits the collective knowledge of web users. In Proceedings of Proceedings of the 2004 Conference on Empirical Methods in Natural Language Processing (EMNLP’04), pages 293–300, Barcelona, Spain.</rawString>
</citation>
<citation valid="true">
<authors>
<author>Pascal Denis</author>
<author>Benoit Sagot</author>
</authors>
<title>Exploitation d’une ressource lexicale pour la construction d’un ´etiqueteur morphosyntaxique ´etat-de-l’art du franc¸ais.</title>
<date>2010</date>
<booktitle>In Traitement Automatique des Langues Naturelles : TALN 2010,</booktitle>
<location>Montr´eal, Canada.</location>
<contexts>
<context position="5195" citStr="Denis and Sagot, 2010" startWordPosition="794" endWordPosition="797"> (4). The predicted impact of the linguistic modules is illustrated in section 5. Finally, we present ongoing work and the conclusion (6). 2 System Architecture Our OCR error detection and correction system uses a hybrid methodology with a symbolic module for linguistic preprocessing, a POS tagger, followed by statistical decoding and correction modules. The SxPipe toolchain (Sagot and Boullier, 2008) is used for shallow processing tasks (tokenisation, sentence segmentation, named entity recognition). The NE-tagged text is input to POS tagging with MElt-h, a hybrid version of the MElt tagger (Denis and Sagot, 2010; Denis and Sagot, 2012). MELT-h can take both NE tagged texts and raw text as input. The decoding phase is based on the noisy channel model (Shannon, 1948) adapted to spell checking (Kernighan et al., 1990). In a noisy channel model, given an input string s, we want to find the word w which maximizes P(wjs). Using Bayes theorem, this can be written as: argmax(w)P(sjw) ~ P(w) (1) where P(w) is given by the language model obtained from clean corpora. Both sentence-level (Tong and Evans, 1996; Boswell, 2004) and wordlevel (Mays et al., 1991) language models can be used. P(sjw) is given by the er</context>
<context position="10811" citStr="Denis and Sagot, 2010" startWordPosition="1704" endWordPosition="1707">2 Evaluation of NE tagging A manual, application-independent evaluation was carried out, concentrating primarily on precision for the reasons mentioned in 3. For four types of NEs, we collected a sample of 200 sentences expected to contain one or more instances of the 2Our current experiments do not cover single-word proper names. 3E.g., A numerical 0 inside a chemical formula is presumed in most cases to be an erroneous hypothesis for alphabetical O. 4 POS tagging 4.1 MEltFR and MElt-h The following step in the chain is POS tagging using a named entity-aware version of the MElt tagger. MElt (Denis and Sagot, 2010; Denis and Sagot, 2012) is a maximum entropy POS tagger which differs from other systems in that it uses both corpus-based features and a large-coverage lexicon as an external source of information. Its French version, MElt-FR was trained on the Leff lexicon (Sagot, 2010) and on the French TreeBank (FTB) (Abeill´e et al., 2003). The training corpus uses a tagset consisting of 29 tags. MEltFR yields state of the art results for French, namely 97.8% accuracy on the test set. In order to integrate MElt into our toolchain, the tagger needed to be trained to read NE-tagged texts as output by SxPip</context>
</contexts>
<marker>Denis, Sagot, 2010</marker>
<rawString>Pascal Denis and Benoit Sagot. 2010. Exploitation d’une ressource lexicale pour la construction d’un ´etiqueteur morphosyntaxique ´etat-de-l’art du franc¸ais. In Traitement Automatique des Langues Naturelles : TALN 2010, Montr´eal, Canada.</rawString>
</citation>
<citation valid="true">
<authors>
<author>Pascal Denis</author>
<author>Benoit Sagot</author>
</authors>
<title>Coupling an annotated corpus and a lexicon for state-of-the-art POS tagging.</title>
<date>2012</date>
<journal>Language Resources and Evaluation,</journal>
<volume>46</volume>
<issue>4</issue>
<contexts>
<context position="5219" citStr="Denis and Sagot, 2012" startWordPosition="798" endWordPosition="801">act of the linguistic modules is illustrated in section 5. Finally, we present ongoing work and the conclusion (6). 2 System Architecture Our OCR error detection and correction system uses a hybrid methodology with a symbolic module for linguistic preprocessing, a POS tagger, followed by statistical decoding and correction modules. The SxPipe toolchain (Sagot and Boullier, 2008) is used for shallow processing tasks (tokenisation, sentence segmentation, named entity recognition). The NE-tagged text is input to POS tagging with MElt-h, a hybrid version of the MElt tagger (Denis and Sagot, 2010; Denis and Sagot, 2012). MELT-h can take both NE tagged texts and raw text as input. The decoding phase is based on the noisy channel model (Shannon, 1948) adapted to spell checking (Kernighan et al., 1990). In a noisy channel model, given an input string s, we want to find the word w which maximizes P(wjs). Using Bayes theorem, this can be written as: argmax(w)P(sjw) ~ P(w) (1) where P(w) is given by the language model obtained from clean corpora. Both sentence-level (Tong and Evans, 1996; Boswell, 2004) and wordlevel (Mays et al., 1991) language models can be used. P(sjw) is given by the error model, represented a</context>
<context position="10835" citStr="Denis and Sagot, 2012" startWordPosition="1708" endWordPosition="1711">ing A manual, application-independent evaluation was carried out, concentrating primarily on precision for the reasons mentioned in 3. For four types of NEs, we collected a sample of 200 sentences expected to contain one or more instances of the 2Our current experiments do not cover single-word proper names. 3E.g., A numerical 0 inside a chemical formula is presumed in most cases to be an erroneous hypothesis for alphabetical O. 4 POS tagging 4.1 MEltFR and MElt-h The following step in the chain is POS tagging using a named entity-aware version of the MElt tagger. MElt (Denis and Sagot, 2010; Denis and Sagot, 2012) is a maximum entropy POS tagger which differs from other systems in that it uses both corpus-based features and a large-coverage lexicon as an external source of information. Its French version, MElt-FR was trained on the Leff lexicon (Sagot, 2010) and on the French TreeBank (FTB) (Abeill´e et al., 2003). The training corpus uses a tagset consisting of 29 tags. MEltFR yields state of the art results for French, namely 97.8% accuracy on the test set. In order to integrate MElt into our toolchain, the tagger needed to be trained to read NE-tagged texts as output by SxPipe. We thus extended the </context>
</contexts>
<marker>Denis, Sagot, 2012</marker>
<rawString>Pascal Denis and Benoit Sagot. 2012. Coupling an annotated corpus and a lexicon for state-of-the-art POS tagging. Language Resources and Evaluation, 46(4):721–736.</rawString>
</citation>
<citation valid="true">
<authors>
<author>Jerid Francom</author>
<author>Mans Hulden</author>
</authors>
<title>Diacritic error detection and restoration via part-of-speech tags.</title>
<date>2013</date>
<booktitle>In Proceedings of the 6th Language and Technology Conference.</booktitle>
<contexts>
<context position="3437" citStr="Francom and Hulden, 2013" startWordPosition="521" endWordPosition="524"> Ringger, 2009), or heuristic correction methods (Alex et al., 2012). While correction systems rely less and less on pre-existing external dictionaries, a shift can be observed towards methods that dinamically create lexicons either by exploiting the Web (Cucerzan and Brill, 2004; Strohmaier et al., 2003) or from the corpus (Reynaert, 2004). As to linguistically enhanced models, POS tagging was succesfully applied to spelling correction (Golding and Schabes, 1996; Schaback, 2007). However, to our knowledge, very little work has been done to exploit linguistic analysis for post-OCR correction (Francom and Hulden, 2013). We propose to apply a shallow processing module to detect certain types of named entities (NEs), and a POS tagger trained specifically to deal with NE-tagged input. Our studies aim to demonstrate that linguistic preprocessing can efficiently contribute to reduce the error rate by 1) detecting false corrections proposed by the 56 Proceedings of the 8th Workshop on Language Technology for Cultural Heritage, Social Sciences, and Humanities (LaTeCH) @ EACL 2014, pages 56–61, Gothenburg, Sweden, April 26 2014. c�2014 Association for Computational Linguistics OCR output decoding: creation of hypot</context>
</contexts>
<marker>Francom, Hulden, 2013</marker>
<rawString>Jerid Francom and Mans Hulden. 2013. Diacritic error detection and restoration via part-of-speech tags. In Proceedings of the 6th Language and Technology Conference.</rawString>
</citation>
<citation valid="true">
<authors>
<author>Nathalie Friburger</author>
<author>Denis Maurel</author>
</authors>
<title>Finitestate transducer cascades to extract named entities in texts.</title>
<date>2004</date>
<journal>Theoretical Computer Science,</journal>
<pages>313--94</pages>
<contexts>
<context position="9383" citStr="Friburger and Maurel, 2004" startWordPosition="1475" endWordPosition="1478">d from the archives of the European Patent Office; no filtering was needed due to the density of formulae in these documents. Legal IDs were evaluated on a legal corpus from the Publications Office of the European Union, while the rest of the grammars were evaluated using the BNF corpus. Entity Type Precision Recall DATE 0.98 0.97 ADDRESS 0.83 0.86 LEGAL 0.88 0.82 CHEMICAL 0.94 - Table 1: Evaluation of NE grammars The named entity recognition module is implemented in Perl as a series of local grammars. Local grammars constitute a simple and powerful tool to recognize open classes of entities (Friburger and Maurel, 2004; Maynard et al., 2002; Bontcheva et al., 2002); we are concerned with time expressions, addresses, currency units, dimensions, chemical formulae and legal IDs. Named entity grammars are applied to the raw corpus before tokenization and segmentation. Our grammars are robust in the sense that they inherently recognize and correct some types of frequent OCR errors in the input.3 SxPipe’s architecture allows to define an OCR-specific correction mode as an input parameter and hence apply robust recognition and correction to noisy output, while requiring exact matching for clean texts. However, max</context>
</contexts>
<marker>Friburger, Maurel, 2004</marker>
<rawString>Nathalie Friburger and Denis Maurel. 2004. Finitestate transducer cascades to extract named entities in texts. Theoretical Computer Science, 313:94–104.</rawString>
</citation>
<citation valid="true">
<authors>
<author>Andrew Golding</author>
<author>Yves Schabes</author>
</authors>
<title>Combining trigram-based and feature-based methods for context-sensitive spelling correction.</title>
<date>1996</date>
<booktitle>In ACL,</booktitle>
<pages>71--78</pages>
<contexts>
<context position="3279" citStr="Golding and Schabes, 1996" startWordPosition="498" endWordPosition="501">s-Thillou, 2007), voting systems using the output of different OCR engines (Klein and Kope, 2002), textual alignment combined with dictionary lookup (Lund and Ringger, 2009), or heuristic correction methods (Alex et al., 2012). While correction systems rely less and less on pre-existing external dictionaries, a shift can be observed towards methods that dinamically create lexicons either by exploiting the Web (Cucerzan and Brill, 2004; Strohmaier et al., 2003) or from the corpus (Reynaert, 2004). As to linguistically enhanced models, POS tagging was succesfully applied to spelling correction (Golding and Schabes, 1996; Schaback, 2007). However, to our knowledge, very little work has been done to exploit linguistic analysis for post-OCR correction (Francom and Hulden, 2013). We propose to apply a shallow processing module to detect certain types of named entities (NEs), and a POS tagger trained specifically to deal with NE-tagged input. Our studies aim to demonstrate that linguistic preprocessing can efficiently contribute to reduce the error rate by 1) detecting false corrections proposed by the 56 Proceedings of the 8th Workshop on Language Technology for Cultural Heritage, Social Sciences, and Humanities</context>
</contexts>
<marker>Golding, Schabes, 1996</marker>
<rawString>Andrew Golding and Yves Schabes. 1996. Combining trigram-based and feature-based methods for context-sensitive spelling correction. In ACL, pages 71–78.</rawString>
</citation>
<citation valid="true">
<authors>
<author>Mark Kernighan</author>
<author>Kenneth Church</author>
<author>William Gale</author>
</authors>
<title>A spelling correction program based on a noisy channel model.</title>
<date>1990</date>
<booktitle>In Proceedings of the 13th conference on Computational linguistics,</booktitle>
<pages>205--210</pages>
<contexts>
<context position="2338" citStr="Kernighan et al., 1990" startWordPosition="357" endWordPosition="360">ly motivated modules to detect OCR Benoit Sagot INRIA &amp; Universit´e Paris 7 Domaine de Voluceau - BP 105 78153 Le Chesnay Cedex FRANCE benoit.sagot@inria.fr errors and generate correction candidates. The technology is based on a symbolic linguistic preprocessing, followed by a statistical module which adpots the noisy channel model (Shannon, 1948). Symbolic methods for error correction allow to target specific phenomena with a high precision, but they typically strongly rely on presumptions about the nature of errors encountered. This drawback can be overcome by using the noisy channel model (Kernighan et al., 1990; Brill and Moore, 2000; Kolak and Resnik, 2002; Mays et al., 1991; Tong and Evans, 1996). However, error models in such systems work best if they are created from manually corrected training data, which are not always available. Other alternatives to OCR error correction include (weighted) FSTs (Beaufort and Mancas-Thillou, 2007), voting systems using the output of different OCR engines (Klein and Kope, 2002), textual alignment combined with dictionary lookup (Lund and Ringger, 2009), or heuristic correction methods (Alex et al., 2012). While correction systems rely less and less on pre-exist</context>
<context position="5402" citStr="Kernighan et al., 1990" startWordPosition="830" endWordPosition="833">m uses a hybrid methodology with a symbolic module for linguistic preprocessing, a POS tagger, followed by statistical decoding and correction modules. The SxPipe toolchain (Sagot and Boullier, 2008) is used for shallow processing tasks (tokenisation, sentence segmentation, named entity recognition). The NE-tagged text is input to POS tagging with MElt-h, a hybrid version of the MElt tagger (Denis and Sagot, 2010; Denis and Sagot, 2012). MELT-h can take both NE tagged texts and raw text as input. The decoding phase is based on the noisy channel model (Shannon, 1948) adapted to spell checking (Kernighan et al., 1990). In a noisy channel model, given an input string s, we want to find the word w which maximizes P(wjs). Using Bayes theorem, this can be written as: argmax(w)P(sjw) ~ P(w) (1) where P(w) is given by the language model obtained from clean corpora. Both sentence-level (Tong and Evans, 1996; Boswell, 2004) and wordlevel (Mays et al., 1991) language models can be used. P(sjw) is given by the error model, represented as a confusion matrix calculated from our training corpus in which OCR output is aligned with its manually corrected, noiseless equivalent. The post-correction process is summarized in</context>
</contexts>
<marker>Kernighan, Church, Gale, 1990</marker>
<rawString>Mark Kernighan, Kenneth Church, and William Gale. 1990. A spelling correction program based on a noisy channel model. In Proceedings of the 13th conference on Computational linguistics, pages 205–210.</rawString>
</citation>
<citation valid="true">
<authors>
<author>Samuel Klein</author>
<author>Miri Kope</author>
</authors>
<title>A voting system for automatic OCR correction.</title>
<date>2002</date>
<booktitle>In Proceedings of the Workshop On Information Retrieval and OCR: From Converting Content to Grasping Meaning,</booktitle>
<pages>1--21</pages>
<location>Tampere, Finland.</location>
<contexts>
<context position="2751" citStr="Klein and Kope, 2002" startWordPosition="421" endWordPosition="424">phenomena with a high precision, but they typically strongly rely on presumptions about the nature of errors encountered. This drawback can be overcome by using the noisy channel model (Kernighan et al., 1990; Brill and Moore, 2000; Kolak and Resnik, 2002; Mays et al., 1991; Tong and Evans, 1996). However, error models in such systems work best if they are created from manually corrected training data, which are not always available. Other alternatives to OCR error correction include (weighted) FSTs (Beaufort and Mancas-Thillou, 2007), voting systems using the output of different OCR engines (Klein and Kope, 2002), textual alignment combined with dictionary lookup (Lund and Ringger, 2009), or heuristic correction methods (Alex et al., 2012). While correction systems rely less and less on pre-existing external dictionaries, a shift can be observed towards methods that dinamically create lexicons either by exploiting the Web (Cucerzan and Brill, 2004; Strohmaier et al., 2003) or from the corpus (Reynaert, 2004). As to linguistically enhanced models, POS tagging was succesfully applied to spelling correction (Golding and Schabes, 1996; Schaback, 2007). However, to our knowledge, very little work has been </context>
</contexts>
<marker>Klein, Kope, 2002</marker>
<rawString>Samuel Klein and Miri Kope. 2002. A voting system for automatic OCR correction. In Proceedings of the Workshop On Information Retrieval and OCR: From Converting Content to Grasping Meaning, pages 1– 21, Tampere, Finland.</rawString>
</citation>
<citation valid="true">
<authors>
<author>Okan Kolak</author>
<author>Philip Resnik</author>
</authors>
<title>OCR error correction using a noisy channel model.</title>
<date>2002</date>
<booktitle>In Proceedings of the Second International Conference on Human Language Technology Research (HLT’02),</booktitle>
<pages>257--262</pages>
<location>San Diego, USA.</location>
<contexts>
<context position="2385" citStr="Kolak and Resnik, 2002" startWordPosition="365" endWordPosition="368"> INRIA &amp; Universit´e Paris 7 Domaine de Voluceau - BP 105 78153 Le Chesnay Cedex FRANCE benoit.sagot@inria.fr errors and generate correction candidates. The technology is based on a symbolic linguistic preprocessing, followed by a statistical module which adpots the noisy channel model (Shannon, 1948). Symbolic methods for error correction allow to target specific phenomena with a high precision, but they typically strongly rely on presumptions about the nature of errors encountered. This drawback can be overcome by using the noisy channel model (Kernighan et al., 1990; Brill and Moore, 2000; Kolak and Resnik, 2002; Mays et al., 1991; Tong and Evans, 1996). However, error models in such systems work best if they are created from manually corrected training data, which are not always available. Other alternatives to OCR error correction include (weighted) FSTs (Beaufort and Mancas-Thillou, 2007), voting systems using the output of different OCR engines (Klein and Kope, 2002), textual alignment combined with dictionary lookup (Lund and Ringger, 2009), or heuristic correction methods (Alex et al., 2012). While correction systems rely less and less on pre-existing external dictionaries, a shift can be obser</context>
</contexts>
<marker>Kolak, Resnik, 2002</marker>
<rawString>Okan Kolak and Philip Resnik. 2002. OCR error correction using a noisy channel model. In Proceedings of the Second International Conference on Human Language Technology Research (HLT’02), pages 257–262, San Diego, USA.</rawString>
</citation>
<citation valid="true">
<authors>
<author>Okan Kolak</author>
<author>Philip Resnik</author>
</authors>
<title>OCR postprocessing for low density languages.</title>
<date>2005</date>
<booktitle>In Proceedings of the HLT-EMNLP Conference,</booktitle>
<pages>867--874</pages>
<marker>Kolak, Resnik, 2005</marker>
<rawString>Okan Kolak and Philip Resnik. 2005. OCR postprocessing for low density languages. In Proceedings of the HLT-EMNLP Conference, pages 867– 874.</rawString>
</citation>
<citation valid="true">
<authors>
<author>William Lund</author>
<author>Eric Ringger</author>
</authors>
<title>Improving optical character recognition through efficient multiple system alignment.</title>
<date>2009</date>
<booktitle>In Proceedings of the 9th ACM/IEEE-CS Joint Conference on Digital Libraries (JCDL’09),</booktitle>
<pages>231--240</pages>
<location>Austin, USA.</location>
<contexts>
<context position="2827" citStr="Lund and Ringger, 2009" startWordPosition="431" endWordPosition="434">mptions about the nature of errors encountered. This drawback can be overcome by using the noisy channel model (Kernighan et al., 1990; Brill and Moore, 2000; Kolak and Resnik, 2002; Mays et al., 1991; Tong and Evans, 1996). However, error models in such systems work best if they are created from manually corrected training data, which are not always available. Other alternatives to OCR error correction include (weighted) FSTs (Beaufort and Mancas-Thillou, 2007), voting systems using the output of different OCR engines (Klein and Kope, 2002), textual alignment combined with dictionary lookup (Lund and Ringger, 2009), or heuristic correction methods (Alex et al., 2012). While correction systems rely less and less on pre-existing external dictionaries, a shift can be observed towards methods that dinamically create lexicons either by exploiting the Web (Cucerzan and Brill, 2004; Strohmaier et al., 2003) or from the corpus (Reynaert, 2004). As to linguistically enhanced models, POS tagging was succesfully applied to spelling correction (Golding and Schabes, 1996; Schaback, 2007). However, to our knowledge, very little work has been done to exploit linguistic analysis for post-OCR correction (Francom and Hul</context>
</contexts>
<marker>Lund, Ringger, 2009</marker>
<rawString>William Lund and Eric Ringger. 2009. Improving optical character recognition through efficient multiple system alignment. In Proceedings of the 9th ACM/IEEE-CS Joint Conference on Digital Libraries (JCDL’09), pages 231–240, Austin, USA.</rawString>
</citation>
<citation valid="true">
<authors>
<author>Diana Maynard</author>
<author>Valentin Tablan</author>
<author>Cristian Ursu</author>
<author>Hamish Cunningham</author>
<author>Yorick Wilks</author>
</authors>
<title>Named entity recognition from diverse text types. In</title>
<date>2001</date>
<booktitle>In Proceedings of the Recent Advances in Natural Language Processing Conference,</booktitle>
<pages>257--274</pages>
<contexts>
<context position="8002" citStr="Maynard et al., 2001" startWordPosition="1248" endWordPosition="1251">erns detection. The SxPipe chain is fully customizable with respect to input language, domain, text type and the modules to be used. Users are also free to add their own modules to the chain. In accordance with our purposes, we defined named entities as sequences of characters which cannot be analysed morphologically or syntactically, yet follow productive patterns. Such entities do not adhere to regular tokenization patterns 1https://gforge.inria.fr/projects/lingwb/ 57 since they often include punctuation marks, usually considered as separators. As compared to the consensual use of the term (Maynard et al., 2001; Chinchor, 1998; Sang and Meulder, 2003), our definition covers a wider range of entities, e.g., numerals, currency units, dimensions.2 The correct annotation of these entities has a double relevance for our project: • NE tagging prior to POS tagging helps to improve the accuracy of the latter. • NE tagging allows to detect and, eventually, correct OCR errors which occur inside NEs. Conversely, it can also contribute to detect false correction candidates when the sequence of characters forming the NE would otherwise be assigned a low probability by the language model. given entity category, b</context>
</contexts>
<marker>Maynard, Tablan, Ursu, Cunningham, Wilks, 2001</marker>
<rawString>Diana Maynard, Valentin Tablan, Cristian Ursu, Hamish Cunningham, and Yorick Wilks. 2001. Named entity recognition from diverse text types. In In Proceedings of the Recent Advances in Natural Language Processing Conference, pages 257–274.</rawString>
</citation>
<citation valid="true">
<authors>
<author>Diana Maynard</author>
<author>Valentin Tablan</author>
<author>Hamish Cunningham</author>
<author>Cristian Ursu</author>
<author>Horacio Saggion</author>
<author>Kalina Bontcheva</author>
<author>Yorick Wilks</author>
</authors>
<title>Architectural elements of language engineering robustness.</title>
<date>2002</date>
<journal>Journal of Natural Language Engineering - Special Issue on Robust Methods in Analysis of Natural Language Data,</journal>
<pages>8--257</pages>
<contexts>
<context position="9405" citStr="Maynard et al., 2002" startWordPosition="1479" endWordPosition="1482">uropean Patent Office; no filtering was needed due to the density of formulae in these documents. Legal IDs were evaluated on a legal corpus from the Publications Office of the European Union, while the rest of the grammars were evaluated using the BNF corpus. Entity Type Precision Recall DATE 0.98 0.97 ADDRESS 0.83 0.86 LEGAL 0.88 0.82 CHEMICAL 0.94 - Table 1: Evaluation of NE grammars The named entity recognition module is implemented in Perl as a series of local grammars. Local grammars constitute a simple and powerful tool to recognize open classes of entities (Friburger and Maurel, 2004; Maynard et al., 2002; Bontcheva et al., 2002); we are concerned with time expressions, addresses, currency units, dimensions, chemical formulae and legal IDs. Named entity grammars are applied to the raw corpus before tokenization and segmentation. Our grammars are robust in the sense that they inherently recognize and correct some types of frequent OCR errors in the input.3 SxPipe’s architecture allows to define an OCR-specific correction mode as an input parameter and hence apply robust recognition and correction to noisy output, while requiring exact matching for clean texts. However, maximizing precision rema</context>
</contexts>
<marker>Maynard, Tablan, Cunningham, Ursu, Saggion, Bontcheva, Wilks, 2002</marker>
<rawString>Diana Maynard, Valentin Tablan, Hamish Cunningham, Cristian Ursu, Horacio Saggion, Kalina Bontcheva, and Yorick Wilks. 2002. Architectural elements of language engineering robustness. Journal of Natural Language Engineering - Special Issue on Robust Methods in Analysis of Natural Language Data, 8:257–274.</rawString>
</citation>
<citation valid="true">
<authors>
<author>Eric Mays</author>
<author>Fred Damerau</author>
<author>Robert Mercer</author>
</authors>
<title>Context based spelling correction.</title>
<date>1991</date>
<journal>Information Processing and Management,</journal>
<volume>23</volume>
<pages>5--517</pages>
<contexts>
<context position="2404" citStr="Mays et al., 1991" startWordPosition="369" endWordPosition="372">is 7 Domaine de Voluceau - BP 105 78153 Le Chesnay Cedex FRANCE benoit.sagot@inria.fr errors and generate correction candidates. The technology is based on a symbolic linguistic preprocessing, followed by a statistical module which adpots the noisy channel model (Shannon, 1948). Symbolic methods for error correction allow to target specific phenomena with a high precision, but they typically strongly rely on presumptions about the nature of errors encountered. This drawback can be overcome by using the noisy channel model (Kernighan et al., 1990; Brill and Moore, 2000; Kolak and Resnik, 2002; Mays et al., 1991; Tong and Evans, 1996). However, error models in such systems work best if they are created from manually corrected training data, which are not always available. Other alternatives to OCR error correction include (weighted) FSTs (Beaufort and Mancas-Thillou, 2007), voting systems using the output of different OCR engines (Klein and Kope, 2002), textual alignment combined with dictionary lookup (Lund and Ringger, 2009), or heuristic correction methods (Alex et al., 2012). While correction systems rely less and less on pre-existing external dictionaries, a shift can be observed towards methods</context>
<context position="5740" citStr="Mays et al., 1991" startWordPosition="890" endWordPosition="893">g with MElt-h, a hybrid version of the MElt tagger (Denis and Sagot, 2010; Denis and Sagot, 2012). MELT-h can take both NE tagged texts and raw text as input. The decoding phase is based on the noisy channel model (Shannon, 1948) adapted to spell checking (Kernighan et al., 1990). In a noisy channel model, given an input string s, we want to find the word w which maximizes P(wjs). Using Bayes theorem, this can be written as: argmax(w)P(sjw) ~ P(w) (1) where P(w) is given by the language model obtained from clean corpora. Both sentence-level (Tong and Evans, 1996; Boswell, 2004) and wordlevel (Mays et al., 1991) language models can be used. P(sjw) is given by the error model, represented as a confusion matrix calculated from our training corpus in which OCR output is aligned with its manually corrected, noiseless equivalent. The post-correction process is summarized in 1. The integration of a symbolic module for NE recognition and the use of part of speech and named entity tags constitute a novel aspect in our method. Moreover, linguistic preprocessing allows us to challenge tokenisation decisions prior Preprocessing with SxPipe (character level) POS and NE tagging tokenization revisited Figure 1: Ar</context>
</contexts>
<marker>Mays, Damerau, Mercer, 1991</marker>
<rawString>Eric Mays, Fred Damerau, and Robert Mercer. 1991. Context based spelling correction. Information Processing and Management, 23 (5):517–522.</rawString>
</citation>
<citation valid="true">
<authors>
<author>Martin Reynaert</author>
</authors>
<title>Multilingual text induced spelling correction.</title>
<date>2004</date>
<booktitle>In Proceedings of the Workshop on Multilingual Linguistic Ressources (MLR’04),</booktitle>
<pages>117--117</pages>
<contexts>
<context position="3154" citStr="Reynaert, 2004" startWordPosition="482" endWordPosition="483">ch are not always available. Other alternatives to OCR error correction include (weighted) FSTs (Beaufort and Mancas-Thillou, 2007), voting systems using the output of different OCR engines (Klein and Kope, 2002), textual alignment combined with dictionary lookup (Lund and Ringger, 2009), or heuristic correction methods (Alex et al., 2012). While correction systems rely less and less on pre-existing external dictionaries, a shift can be observed towards methods that dinamically create lexicons either by exploiting the Web (Cucerzan and Brill, 2004; Strohmaier et al., 2003) or from the corpus (Reynaert, 2004). As to linguistically enhanced models, POS tagging was succesfully applied to spelling correction (Golding and Schabes, 1996; Schaback, 2007). However, to our knowledge, very little work has been done to exploit linguistic analysis for post-OCR correction (Francom and Hulden, 2013). We propose to apply a shallow processing module to detect certain types of named entities (NEs), and a POS tagger trained specifically to deal with NE-tagged input. Our studies aim to demonstrate that linguistic preprocessing can efficiently contribute to reduce the error rate by 1) detecting false corrections pro</context>
</contexts>
<marker>Reynaert, 2004</marker>
<rawString>Martin Reynaert. 2004. Multilingual text induced spelling correction. In Proceedings of the Workshop on Multilingual Linguistic Ressources (MLR’04), pages 117–117.</rawString>
</citation>
<citation valid="true">
<authors>
<author>Benoit Sagot</author>
<author>Pierre Boullier</author>
</authors>
<title>SxPipe 2: architecture pour le traitement pr´e-syntaxique de corpus bruts.</title>
<date>2008</date>
<booktitle>Traitement Automatique des Langues,</booktitle>
<volume>49</volume>
<issue>2</issue>
<contexts>
<context position="4978" citStr="Sagot and Boullier, 2008" startWordPosition="760" endWordPosition="763"> with a very high precision. In what follows, we present the global architecture of the post-OCR correction system (2), the named entity recognition module (3), as well as our experiments in named entity-aware POS tagging (4). The predicted impact of the linguistic modules is illustrated in section 5. Finally, we present ongoing work and the conclusion (6). 2 System Architecture Our OCR error detection and correction system uses a hybrid methodology with a symbolic module for linguistic preprocessing, a POS tagger, followed by statistical decoding and correction modules. The SxPipe toolchain (Sagot and Boullier, 2008) is used for shallow processing tasks (tokenisation, sentence segmentation, named entity recognition). The NE-tagged text is input to POS tagging with MElt-h, a hybrid version of the MElt tagger (Denis and Sagot, 2010; Denis and Sagot, 2012). MELT-h can take both NE tagged texts and raw text as input. The decoding phase is based on the noisy channel model (Shannon, 1948) adapted to spell checking (Kernighan et al., 1990). In a noisy channel model, given an input string s, we want to find the word w which maximizes P(wjs). Using Bayes theorem, this can be written as: argmax(w)P(sjw) ~ P(w) (1) </context>
<context position="7103" citStr="Sagot and Boullier, 2008" startWordPosition="1116" endWordPosition="1120">down to a fusion or split of tokens. The corpus we use comes from the archives of the French National Library and contains 1 500 documents (50 000 000 tokens). This corpus is available both as a ”reference corpus”, i.e., in a manually corrected, clean version, and as a ”contrast corpus”, i.e., a noisy OCR output version. These variants are aligned at the sentence level. 3 Named entity tagging 3.1 NE recognition methodology As a first step in error detection, the OCR output is analysed in search of ”irregular” character sequences such as named entities. This process is implemented with SxPipe (Sagot and Boullier, 2008), a freely available1, robust and modular multilingual processing chain for unrestricted text. SxPipe contains modules for named entity recognition, tokenization, sentence segmentation, non-deterministic multi-word expression detection, spelling correction and lexicon-based patterns detection. The SxPipe chain is fully customizable with respect to input language, domain, text type and the modules to be used. Users are also free to add their own modules to the chain. In accordance with our purposes, we defined named entities as sequences of characters which cannot be analysed morphologically or</context>
</contexts>
<marker>Sagot, Boullier, 2008</marker>
<rawString>Benoit Sagot and Pierre Boullier. 2008. SxPipe 2: architecture pour le traitement pr´e-syntaxique de corpus bruts. Traitement Automatique des Langues, 49(2):155–188.</rawString>
</citation>
<citation valid="true">
<authors>
<author>Benoit Sagot</author>
</authors>
<title>The lefff, a freely available and large-coverage morphological and syntactic lexicon for french.</title>
<date>2010</date>
<booktitle>In Proceedings of LREC 2010,</booktitle>
<location>La Valette, Malte.</location>
<contexts>
<context position="5195" citStr="Sagot, 2010" startWordPosition="796" endWordPosition="797">predicted impact of the linguistic modules is illustrated in section 5. Finally, we present ongoing work and the conclusion (6). 2 System Architecture Our OCR error detection and correction system uses a hybrid methodology with a symbolic module for linguistic preprocessing, a POS tagger, followed by statistical decoding and correction modules. The SxPipe toolchain (Sagot and Boullier, 2008) is used for shallow processing tasks (tokenisation, sentence segmentation, named entity recognition). The NE-tagged text is input to POS tagging with MElt-h, a hybrid version of the MElt tagger (Denis and Sagot, 2010; Denis and Sagot, 2012). MELT-h can take both NE tagged texts and raw text as input. The decoding phase is based on the noisy channel model (Shannon, 1948) adapted to spell checking (Kernighan et al., 1990). In a noisy channel model, given an input string s, we want to find the word w which maximizes P(wjs). Using Bayes theorem, this can be written as: argmax(w)P(sjw) ~ P(w) (1) where P(w) is given by the language model obtained from clean corpora. Both sentence-level (Tong and Evans, 1996; Boswell, 2004) and wordlevel (Mays et al., 1991) language models can be used. P(sjw) is given by the er</context>
<context position="10811" citStr="Sagot, 2010" startWordPosition="1706" endWordPosition="1707">on of NE tagging A manual, application-independent evaluation was carried out, concentrating primarily on precision for the reasons mentioned in 3. For four types of NEs, we collected a sample of 200 sentences expected to contain one or more instances of the 2Our current experiments do not cover single-word proper names. 3E.g., A numerical 0 inside a chemical formula is presumed in most cases to be an erroneous hypothesis for alphabetical O. 4 POS tagging 4.1 MEltFR and MElt-h The following step in the chain is POS tagging using a named entity-aware version of the MElt tagger. MElt (Denis and Sagot, 2010; Denis and Sagot, 2012) is a maximum entropy POS tagger which differs from other systems in that it uses both corpus-based features and a large-coverage lexicon as an external source of information. Its French version, MElt-FR was trained on the Leff lexicon (Sagot, 2010) and on the French TreeBank (FTB) (Abeill´e et al., 2003). The training corpus uses a tagset consisting of 29 tags. MEltFR yields state of the art results for French, namely 97.8% accuracy on the test set. In order to integrate MElt into our toolchain, the tagger needed to be trained to read NE-tagged texts as output by SxPip</context>
</contexts>
<marker>Sagot, 2010</marker>
<rawString>Benoit Sagot. 2010. The lefff, a freely available and large-coverage morphological and syntactic lexicon for french. In Proceedings of LREC 2010, La Valette, Malte.</rawString>
</citation>
<citation valid="true">
<authors>
<author>Erik Tjong Kim Sang</author>
<author>Fien De Meulder</author>
</authors>
<title>Introduction to the conll-2003 shared task: Languageindependent named entity recognition. In</title>
<date>2003</date>
<booktitle>In Proceedings of Computational Natural Language Learning,</booktitle>
<pages>142--147</pages>
<publisher>ACL Press.</publisher>
<marker>Sang, De Meulder, 2003</marker>
<rawString>Erik Tjong Kim Sang and Fien De Meulder. 2003. Introduction to the conll-2003 shared task: Languageindependent named entity recognition. In In Proceedings of Computational Natural Language Learning, pages 142–147. ACL Press.</rawString>
</citation>
<citation valid="true">
<authors>
<author>Johannes Schaback</author>
</authors>
<title>Multi-level feature extraction for spelling correction.</title>
<date>2007</date>
<booktitle>In IJCAI Workshop on Analytics for Noisy Unstructured Text Data,</booktitle>
<pages>79--86</pages>
<contexts>
<context position="3296" citStr="Schaback, 2007" startWordPosition="502" endWordPosition="503">stems using the output of different OCR engines (Klein and Kope, 2002), textual alignment combined with dictionary lookup (Lund and Ringger, 2009), or heuristic correction methods (Alex et al., 2012). While correction systems rely less and less on pre-existing external dictionaries, a shift can be observed towards methods that dinamically create lexicons either by exploiting the Web (Cucerzan and Brill, 2004; Strohmaier et al., 2003) or from the corpus (Reynaert, 2004). As to linguistically enhanced models, POS tagging was succesfully applied to spelling correction (Golding and Schabes, 1996; Schaback, 2007). However, to our knowledge, very little work has been done to exploit linguistic analysis for post-OCR correction (Francom and Hulden, 2013). We propose to apply a shallow processing module to detect certain types of named entities (NEs), and a POS tagger trained specifically to deal with NE-tagged input. Our studies aim to demonstrate that linguistic preprocessing can efficiently contribute to reduce the error rate by 1) detecting false corrections proposed by the 56 Proceedings of the 8th Workshop on Language Technology for Cultural Heritage, Social Sciences, and Humanities (LaTeCH) @ EACL </context>
</contexts>
<marker>Schaback, 2007</marker>
<rawString>Johannes Schaback. 2007. Multi-level feature extraction for spelling correction. In IJCAI Workshop on Analytics for Noisy Unstructured Text Data, pages 79–86.</rawString>
</citation>
<citation valid="true">
<authors>
<author>Claude Shannon</author>
</authors>
<title>A Mathematical Theory of Communication.</title>
<date>1948</date>
<journal>Bell System Technical Journal,</journal>
<volume>27</volume>
<pages>3--379</pages>
<contexts>
<context position="2065" citStr="Shannon, 1948" startWordPosition="316" endWordPosition="317">ject, we are focusing on French texts from the archives of the French National Library (Biblioth`eque Nationale de France) covering the period from 1646 to 1990. We adopted a hybrid approach, making use of both statistical classification techniques and linguistically motivated modules to detect OCR Benoit Sagot INRIA &amp; Universit´e Paris 7 Domaine de Voluceau - BP 105 78153 Le Chesnay Cedex FRANCE benoit.sagot@inria.fr errors and generate correction candidates. The technology is based on a symbolic linguistic preprocessing, followed by a statistical module which adpots the noisy channel model (Shannon, 1948). Symbolic methods for error correction allow to target specific phenomena with a high precision, but they typically strongly rely on presumptions about the nature of errors encountered. This drawback can be overcome by using the noisy channel model (Kernighan et al., 1990; Brill and Moore, 2000; Kolak and Resnik, 2002; Mays et al., 1991; Tong and Evans, 1996). However, error models in such systems work best if they are created from manually corrected training data, which are not always available. Other alternatives to OCR error correction include (weighted) FSTs (Beaufort and Mancas-Thillou, </context>
<context position="5351" citStr="Shannon, 1948" startWordPosition="824" endWordPosition="825">r OCR error detection and correction system uses a hybrid methodology with a symbolic module for linguistic preprocessing, a POS tagger, followed by statistical decoding and correction modules. The SxPipe toolchain (Sagot and Boullier, 2008) is used for shallow processing tasks (tokenisation, sentence segmentation, named entity recognition). The NE-tagged text is input to POS tagging with MElt-h, a hybrid version of the MElt tagger (Denis and Sagot, 2010; Denis and Sagot, 2012). MELT-h can take both NE tagged texts and raw text as input. The decoding phase is based on the noisy channel model (Shannon, 1948) adapted to spell checking (Kernighan et al., 1990). In a noisy channel model, given an input string s, we want to find the word w which maximizes P(wjs). Using Bayes theorem, this can be written as: argmax(w)P(sjw) ~ P(w) (1) where P(w) is given by the language model obtained from clean corpora. Both sentence-level (Tong and Evans, 1996; Boswell, 2004) and wordlevel (Mays et al., 1991) language models can be used. P(sjw) is given by the error model, represented as a confusion matrix calculated from our training corpus in which OCR output is aligned with its manually corrected, noiseless equiv</context>
</contexts>
<marker>Shannon, 1948</marker>
<rawString>Claude Shannon. 1948. A Mathematical Theory of Communication. Bell System Technical Journal, 27 (3):379–423.</rawString>
</citation>
<citation valid="true">
<authors>
<author>Christan Strohmaier</author>
<author>Cristoph Ringlstetter</author>
<author>Klaus Schulz</author>
<author>Stoyan Mihov</author>
</authors>
<title>Lexical postcorrection of OCR-results: the web as a dynamic secondary dictionary?</title>
<date>2003</date>
<booktitle>In Proceedings of the Seventh International Conference on Document Analysis and Recognition (ICDAR’03),</booktitle>
<pages>11331137</pages>
<location>Edinburgh, Royaume-Uni.</location>
<contexts>
<context position="3118" citStr="Strohmaier et al., 2003" startWordPosition="474" endWordPosition="477">ed from manually corrected training data, which are not always available. Other alternatives to OCR error correction include (weighted) FSTs (Beaufort and Mancas-Thillou, 2007), voting systems using the output of different OCR engines (Klein and Kope, 2002), textual alignment combined with dictionary lookup (Lund and Ringger, 2009), or heuristic correction methods (Alex et al., 2012). While correction systems rely less and less on pre-existing external dictionaries, a shift can be observed towards methods that dinamically create lexicons either by exploiting the Web (Cucerzan and Brill, 2004; Strohmaier et al., 2003) or from the corpus (Reynaert, 2004). As to linguistically enhanced models, POS tagging was succesfully applied to spelling correction (Golding and Schabes, 1996; Schaback, 2007). However, to our knowledge, very little work has been done to exploit linguistic analysis for post-OCR correction (Francom and Hulden, 2013). We propose to apply a shallow processing module to detect certain types of named entities (NEs), and a POS tagger trained specifically to deal with NE-tagged input. Our studies aim to demonstrate that linguistic preprocessing can efficiently contribute to reduce the error rate b</context>
</contexts>
<marker>Strohmaier, Ringlstetter, Schulz, Mihov, 2003</marker>
<rawString>Christan Strohmaier, Cristoph Ringlstetter, Klaus Schulz, and Stoyan Mihov. 2003. Lexical postcorrection of OCR-results: the web as a dynamic secondary dictionary? In Proceedings of the Seventh International Conference on Document Analysis and Recognition (ICDAR’03), page 11331137, Edinburgh, Royaume-Uni.</rawString>
</citation>
<citation valid="true">
<authors>
<author>Xiang Tong</author>
<author>David Evans</author>
</authors>
<title>A statistical approach to automatic OCR error correction in context.</title>
<date>1996</date>
<booktitle>In Proceedings of the Fourth Workshop on Very large Corpora,</booktitle>
<pages>88--100</pages>
<contexts>
<context position="2427" citStr="Tong and Evans, 1996" startWordPosition="373" endWordPosition="376">uceau - BP 105 78153 Le Chesnay Cedex FRANCE benoit.sagot@inria.fr errors and generate correction candidates. The technology is based on a symbolic linguistic preprocessing, followed by a statistical module which adpots the noisy channel model (Shannon, 1948). Symbolic methods for error correction allow to target specific phenomena with a high precision, but they typically strongly rely on presumptions about the nature of errors encountered. This drawback can be overcome by using the noisy channel model (Kernighan et al., 1990; Brill and Moore, 2000; Kolak and Resnik, 2002; Mays et al., 1991; Tong and Evans, 1996). However, error models in such systems work best if they are created from manually corrected training data, which are not always available. Other alternatives to OCR error correction include (weighted) FSTs (Beaufort and Mancas-Thillou, 2007), voting systems using the output of different OCR engines (Klein and Kope, 2002), textual alignment combined with dictionary lookup (Lund and Ringger, 2009), or heuristic correction methods (Alex et al., 2012). While correction systems rely less and less on pre-existing external dictionaries, a shift can be observed towards methods that dinamically creat</context>
<context position="5690" citStr="Tong and Evans, 1996" startWordPosition="881" endWordPosition="884">ognition). The NE-tagged text is input to POS tagging with MElt-h, a hybrid version of the MElt tagger (Denis and Sagot, 2010; Denis and Sagot, 2012). MELT-h can take both NE tagged texts and raw text as input. The decoding phase is based on the noisy channel model (Shannon, 1948) adapted to spell checking (Kernighan et al., 1990). In a noisy channel model, given an input string s, we want to find the word w which maximizes P(wjs). Using Bayes theorem, this can be written as: argmax(w)P(sjw) ~ P(w) (1) where P(w) is given by the language model obtained from clean corpora. Both sentence-level (Tong and Evans, 1996; Boswell, 2004) and wordlevel (Mays et al., 1991) language models can be used. P(sjw) is given by the error model, represented as a confusion matrix calculated from our training corpus in which OCR output is aligned with its manually corrected, noiseless equivalent. The post-correction process is summarized in 1. The integration of a symbolic module for NE recognition and the use of part of speech and named entity tags constitute a novel aspect in our method. Moreover, linguistic preprocessing allows us to challenge tokenisation decisions prior Preprocessing with SxPipe (character level) POS </context>
</contexts>
<marker>Tong, Evans, 1996</marker>
<rawString>Xiang Tong and David Evans. 1996. A statistical approach to automatic OCR error correction in context. In Proceedings of the Fourth Workshop on Very large Corpora, pages 88–100.</rawString>
</citation>
</citationList>
</algorithm>
</algorithms>
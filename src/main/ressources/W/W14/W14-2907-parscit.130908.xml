<?xml version="1.0" encoding="UTF-8"?>
<algorithms version="110505">
<algorithm name="SectLabel" version="110505">
<variant no="0" confidence="0.000928">
<title confidence="0.9995015">
A Comparison of the Events and Relations Across ACE, ERE, TAC-KBP,
and FrameNet Annotation Standards
</title>
<author confidence="0.976044">
Jacqueline Aguilar and Charley Beller and Paul McNamee and Benjamin Van Durme
</author>
<affiliation confidence="0.9194035">
Human Language Technology Center of Excellence
Johns Hopkins University
</affiliation>
<address confidence="0.634083">
Baltimore, MD, USA
</address>
<author confidence="0.978979">
Stephanie Strassel and Zhiyi Song and Joe Ellis
</author>
<affiliation confidence="0.700035333333333">
University of Pennsylvania
Linguistic Data Consortium (LDC)
Philadelphia, PA, USA
</affiliation>
<sectionHeader confidence="0.977862" genericHeader="abstract">
Abstract
</sectionHeader>
<bodyText confidence="0.999254153846154">
The resurgence of effort within computa-
tional semantics has led to increased in-
terest in various types of relation extrac-
tion and semantic parsing. While var-
ious manually annotated resources exist
for enabling this work, these materials
have been developed with different stan-
dards and goals in mind. In an effort
to develop better general understanding
across these resources, we provide a sum-
mary overview of the standards underly-
ing ACE, ERE, TAC-KBP Slot-filling, and
FrameNet.
</bodyText>
<sectionHeader confidence="0.99549" genericHeader="keywords">
1 Overview
</sectionHeader>
<bodyText confidence="0.998872916666667">
ACE and ERE are comprehensive annotation stan-
dards that aim to consistently annotate Entities,
Events, and Relations within a variety of doc-
uments. The ACE (Automatic Content Extrac-
tion) standard was developed by NIST in 1999 and
has evolved over time to support different evalua-
tion cycles, the last evaluation having occurred in
2008. The ERE (Entities, Relations, Events) stan-
dard was created under the DARPA DEFT pro-
gram as a lighter-weight version of ACE with the
goal of making annotation easier, and more con-
sistent across annotators. ERE attempts to achieve
this goal by consolidating some of the annotation
type distinctions that were found to be the most
problematic in ACE, as well as removing some
more complex annotation features.
This paper provides an overview of the relation-
ship between these two standards and compares
them to the more restricted standard of the TAC-
KBP slot-filling task and the more expansive stan-
dard of FrameNet. Sections 3 and 4 examine Rela-
tions and Events in the ACE/ERE standards, sec-
tion 5 looks at TAC-KBP slot-filling, and section
6 compares FrameNet to the other standards.
</bodyText>
<sectionHeader confidence="0.715678" genericHeader="introduction">
2 ACE and ERE Entity Tagging
</sectionHeader>
<bodyText confidence="0.999831166666667">
Many of the differences in Relations and Events
annotation across the ACE and ERE standards
stem from differences in entity mention tagging.
This is simply because Relation and Event tagging
relies on the distinctions established in the entity
tagging portion of the annotation process. For ex-
ample, since ERE collapses the ACE Facility and
Location Types, any ACE Relation or Event that
relied on that distinction is revised in ERE. These
top-level differences are worth keeping in mind
when considering how Events and Relations tag-
ging is approached in ACE and ERE:
</bodyText>
<listItem confidence="0.995102235294118">
• Type Inventory: ACE and ERE share the Per-
son, Organization, Geo-Political Entity, and
Location Types. ACE has two additional
Types: Vehicle and Weapon. ERE does not
account for these Types and collapses the Fa-
cility and Location Types into Location. ERE
also includes a Title Type to address titles,
honorifics, roles, and professions (Linguis-
tic Data Consortium, 2006; Linguistic Data
Consortium, 2013a).
• Subtype Annotation: ACE further classifies
entity mentions by including Subtypes for
each determined Type; if the entity does not
fit into any Subtype, it is not annotated. ERE
annotation does not include any Subtypes.
• Entity Classes: In addition to Subtype, ACE
also classifies each entity mention according
</listItem>
<page confidence="0.991116">
45
</page>
<note confidence="0.856111">
Proceedings of the 2nd Workshop on EVENTS: Definition, Detection, Coreference, and Representation, pages 45–53,
Baltimore, Maryland, USA, June 22-27, 2014. c�2014 Association for Computational Linguistics
1996 1998 2000 2002 2004 2006 2008 2010 2012
</note>
<figureCaption confidence="0.998754">
Figure 1: Important Dates for the ACE, ERE, TAC-KBP, and FrameNet Standards
</figureCaption>
<bodyText confidence="0.7121665">
to entity class (Specific, Generic, Attributive,
and Underspecified).
</bodyText>
<listItem confidence="0.9956741875">
• Taggability: ACE tags Attributive, Generic,
Specific, and Underspecified entity mentions.
ERE only tags Specific entity mentions.
• Extents and Heads: ACE marks the full noun
phrase of an entity mention and tags a head
word. ERE handles tagging based on the
mention level of an entity; in Name mentions
(NAM) the name is the extent, in Nominal
mentions (NOM) the full noun phrase is the
extent, in Pronoun mentions (PRO) the pro-
noun is the extent.
• Tags: ERE only specifies Type and Men-
tion level (NAM, NOM, PRO). ACE speci-
fies Type, Subtype, Entity Class (Attributive,
Generic, Specific, Underspecified), and Men-
tion Level (NAM, NOM, PRO, Headless).
</listItem>
<sectionHeader confidence="0.924851" genericHeader="method">
3 Relations in ACE and ERE
</sectionHeader>
<bodyText confidence="0.999711071428572">
In the ACE and ERE annotation models, the goal
of the Relations task is to detect and character-
ize relations of the targeted types between enti-
ties (Linguistic Data Consortium, 2008; Linguistic
Data Consortium, 2013c). The purpose of this task
is to extract a representation of the meaning of the
text, not necessarily tied to underlying syntactic
or lexical semantic representations. Both models
share similar overarching guidelines for determin-
ing what is taggable. For relations the differences
lie in the absence or presence of additional fea-
tures, syntactic classes, as well as differences in
assertion, trigger words, and minor subtype varia-
tions.
</bodyText>
<subsectionHeader confidence="0.997674">
3.1 Similarities in Relations Annotation
</subsectionHeader>
<bodyText confidence="0.821589571428571">
In addition to comprising similar Types (both
models include Physical and Part.Whole Types as
well as slightly different Types to address Affilia-
tion and Social relations) used to characterize each
relation, ACE and ERE share important similar-
ities concerning their relation-tagging guidelines.
These include:
</bodyText>
<listItem confidence="0.999288115384615">
• Limiting relations to only those expressed in
a single sentence
• Tagging only for explicit mention
• No ‘promoting’ or ‘nesting’ of taggable en-
tities. In the sentence, Smith went to a hotel
in Brazil, (Smith, hotel) is a taggable Phys-
ical.Located relation, but (Smith, Brazil) is
not. This is because in order to tag this as
such, one would have to promote ‘Brazil’.
• Tagging for past and former relations
• Two different Argument slots (Arg1 and
Arg2) are provided for each relation to cap-
ture the importance of Argument ordering.
• Arguments can be more than one token (al-
though ACE marks the head as well)
• Using ‘templates’ for each relation
Type/Subtype (e.g., in a Physical.Located
relation, the Person that is located some-
where will always be assigned to Arg1 and
the place in which the person is located will
always be assigned to Arg2).
• Neither model tags for negative relations
• Both methods contain argument span bound-
aries. That is, the relations should only in-
clude tagged entities within the extent of a
sentence.
</listItem>
<subsectionHeader confidence="0.987685">
3.2 Differences in Assertion, Modality, and
Tense
</subsectionHeader>
<bodyText confidence="0.9850798">
A primary difference between these two annota-
tion models is a result of ERE only annotating as-
serted events while ACE also includes hypothet-
icals. ACE accounts for these cases by including
two Modality attributes: ASSERTED and OTHER
</bodyText>
<page confidence="0.998814">
46
</page>
<bodyText confidence="0.9969065">
(Linguistic Data Consortium, 2008). For exam-
ple, in the sentence, We are afraid that Al-Qaeda
terrorists will be in Baghdad, ACE would tag this
as an OTHER attribute, where OTHER pertains to
situations in “some other world defined by coun-
terfactual constraints elsewhere in the context”,
whereas ERE would simply not tag a relation in
this sentence. Additionally, while both ACE and
ERE tag past and former relations, ACE goes fur-
ther to mark the Tense of each relation by means
of four attributes: Past, Future, Present and Un-
specified.
</bodyText>
<subsectionHeader confidence="0.999018">
3.3 Syntactic Classes
</subsectionHeader>
<bodyText confidence="0.999110333333333">
ACE further justifies the tagging of each Relation
through Syntactic Classes. The primary function
of these classes is to serve as a sanity check on
taggability and as an additional constraint for tag-
ging. These classes include: Possessive, Prepo-
sition, PreMod, Coordination, Formulaic, Partic-
ipal, Verbal, Relations Expressed by Verbs, and
Other. Syntactic classes are not present in ERE
relations annotation.
</bodyText>
<subsectionHeader confidence="0.959134">
3.4 Triggers
</subsectionHeader>
<bodyText confidence="0.999985882352941">
Explicit trigger words do not exist in ACE relation
annotation; instead, the model annotates the full
syntactic clause that serves as the ‘trigger’ for the
relation. ERE attempts to minimize the annotated
span by allowing for the tagging of an optional
trigger word, defined as “the smallest extent of text
that indicates a relation Type and Subtype” (Lin-
guistic Data Consortium, 2013c). These triggers
are not limited to a single word, but can also be
composed of a phrase or any extent of the text that
indicates a Type/Subtype relation, left to the dis-
cretion of the annotator. It is common for preposi-
tions to be triggers, as in John is in Chicago. How-
ever, sometimes no trigger is needed because the
syntax of the sentence is such that it indicates a
particular relation Type/Subtype without a word to
explicitly signal the relation.
</bodyText>
<subsectionHeader confidence="0.985541">
3.5 Types and Subtypes of Relations
</subsectionHeader>
<bodyText confidence="0.983339357142857">
There are three types of relations that contain var-
ied Subtypes between ERE and ACE. These are
the Physical, Part-Whole, Social and Affiliation
Types. The differences are a result of ERE collaps-
ing ACE Types and Subtypes into more concise, if
less specific, Type groups.
Physical Relation Type Differences The main
differences in the handling of the physical rela-
tions between ACE and ERE are shown in Table
1. ACE only marks Location for PERSON enti-
ties (for Arg1). ERE uses Location for PERSON
entities being located somewhere as well as for
a geographical location being part of another ge-
ographical location. Additionally, ACE includes
‘Near’ as a Subtype. This is used for when an en-
tity is explicitly near another entity, but neither en-
tity is a part of the other or located in/at the other.
ERE does not have an equivalent Subtype to ac-
count for this physical relation. Instead, ERE in-
cludes ‘Origin’ as a Subtype. This is used to de-
scribe the relation between a PER and an ORG.
ACE does not have a Physical Type equivalent,
but it does account for this type of relation within
a separate General Affiliation Type and ‘Citizen-
Resident-Religion-Ethnicity’ Subtype.
Part-Whole Relation Differences In Table 2,
note that ACE has a ‘Geographical’ Subtype
which captures the location of a FAC, LOC, or
GPE in or at, or as part of another FAC, LOC,
or GPE. Examples of this would be India con-
trolled the region or a phrase such as the Atlanta
area. ERE does not include this type of annota-
tion option. Instead, ERE tags these regional re-
lations as Physical.Located. ACE and ERE do
share a ‘Subsidiary’ Subtype which is defined in
both models as a “category to capture the own-
ership, administrative and other hierarchical rela-
tionships between ORGs and/or GPEs” (Linguis-
tic Data Consortium, 2008; Linguistic Data Con-
sortium, 2013c).
Social and Affiliation Relation Differences
The most evident discrepancy in relation anno-
tation between the two models lies in the So-
cial and Affiliation Relation Types and Subtypes.
For social relations, ACE and ERE have three
Subtypes with similar goals (Business, Family,
Unspecified/Lasting-Personal) but ERE has an ad-
ditional ‘Membership’ Subtype, as shown in Ta-
ble 3. ACE addresses all ‘Membership’ relations
in its Affiliation Type. ERE also includes the ‘So-
cial.Role’ Subtype in order to address the TITLE
entity type, which only applies to ERE. How-
ever, both models agree that the arguments for
each relation must be PERSON entities and that
they should not include relationships implied from
interaction between two entities (e.g., President
</bodyText>
<page confidence="0.998503">
47
</page>
<table confidence="0.999733">
Relation Type Relation Subtype ARG1 Type ARG2 Type
ERE
Physical Located PER, GPE, LOC GPE, LOC
Physical Origin PER, ORG GPE, LOC
ACE
Physical Located PER FAC, LOC, GPE
Physical Near PER, FAC, GPE, LOC FAC, GPE, LOC
</table>
<tableCaption confidence="0.95741">
Table 1: Comparison of Permitted Relation Arguments for the Physical Type Distinction in the ERE and
ACE Guidelines
</tableCaption>
<table confidence="0.999629666666667">
Relation Type Relation Subtype ARG1 Type ARG2 Type
ERE
Part-Whole Subsidiary ORG ORG, GPE
ACE
Part-Whole Geographical FAC, LOC, GPE FAC, LOC, GPE
Part-Whole Subsidiary ORG ORG, GPE
</table>
<tableCaption confidence="0.9893945">
Table 2: Comparison of Permitted Relation Arguments for the Part-Whole Type and Subtype Distinctions
in the ERE and ACE Guidelines
</tableCaption>
<table confidence="0.999925">
Relation Type Relation Subtype ARG1 Type ARG2 Type
ERE
Social Business PER PER
Social Family PER PER
Social Membership PER PER
Social Role TTL PER
Social Unspecified PER PER
ACE
Personal-Social Business PER PER
Personal-Social Family PER PER
Personal-Social Lasting-Personal PER PER
</table>
<tableCaption confidence="0.972518">
Table 3: Comparison of Permitted Relation Arguments for the Social Type and Subtype Distinctions in
the ERE and ACE Guidelines
</tableCaption>
<table confidence="0.999962476190476">
Relation Type Relation Subtype ARG1 Type ARG2 Type
ERE
Affiliation Employment/Membership PER, ORG, ORG, GPE
Affiliation Leadership GPE ORG, GPE
PER
ACE
ORG-Affiliation Employment PER ORG, GPE
ORG-Affiliation Ownership PER ORG
ORG-Affiliation Founder PER, ORG ORG, GPE
ORG-Affiliation Student-Alum PER ORG.Educational
ORG-Affiliation Sports-Affiliation PER ORG
ORG-Affiliation Investor-Shareholder PER, ORG, ORG, GPE
ORG-Affiliation Membership GPE ORG
PER, ORG,
GPE
Agent-Artifact User-Owner-Inventor- PER, ORG, FAC
Manufacturer GPE
Gen-Affiliation Citizen-Resident-Religion- PER PER.Group,
Gen-Affiliation Ethnicity ORG LOC, GPE,
Org-Location-Origin ORG
LOC, GPE
</table>
<tableCaption confidence="0.9517725">
Table 4: Comparison of Permitted Relation Arguments for the Affiliation Type and Subtype Distinctions
in the ERE and ACE Guidelines
</tableCaption>
<page confidence="0.998107">
48
</page>
<bodyText confidence="0.996520714285714">
Clinton met with Yasser Arafat last week would
not be considered a social relation).
As for the differences in affiliation relations,
ACE includes many Subtype possibilities which
can more accurately represent affiliation, whereas
ERE only observes two Affiliation Subtype op-
tions (Table 4).
</bodyText>
<sectionHeader confidence="0.983334" genericHeader="method">
4 Events in ACE and ERE
</sectionHeader>
<bodyText confidence="0.9998564">
Events in both annotation methods are defined as
‘specific occurrences’, involving ‘specific partic-
ipants’ (Linguistic Data Consortium, 2005; Lin-
guistic Data Consortium, 2013b). The primary
goal of Event tagging is to detect and character-
ize events that include tagged entities. The central
Event tagging difference between ACE and ERE
is the level of specificity present in ACE, whereas
ERE tends to collapse tags for a more simplified
approach.
</bodyText>
<subsectionHeader confidence="0.99479">
4.1 Event Tagging Similarities
</subsectionHeader>
<bodyText confidence="0.999939035714286">
Both annotation schemas annotate the same ex-
act Event Types: LIFE, MOVEMENT, TRANS-
ACTION, BUSINESS, CONFLICT, CONTACT,
PERSONNEL, and JUSTICE events. Both anno-
tation ontologies also include 33 Subtypes for each
Type. Furthermore, both rely on the expression
of an occurrence through the use of a ‘Trigger’.
ACE, however, restricts the trigger to be a single
word that most clearly expresses the event occur-
rence (usually a main verb), while ERE allows for
the trigger to be a word or a phrase that instanti-
ates the event (Linguistic Data Consortium, 2005;
Linguistic Data Consortium, 2013b). Both meth-
ods annotate modifiers when they trigger events
as well as anaphors, when they refer to previously
mentioned events. Furthermore, when there is
any ambiguity about which trigger to select, both
methods have similar rules established, such as
the Stand-Alone Noun Rule (In cases where more
than one trigger is possible, the noun that can be
used by itself to refer to the event will be selected)
and the Stand-Alone Adjective Rule (Whenever a
verb and an adjective are used together to express
the occurrence of an Event, the adjective will be
chosen as the trigger whenever it can stand-alone
to express the resulting state brought about by the
Event). Additionally, both annotation guidelines
agree on the following:
</bodyText>
<listItem confidence="0.9833772">
• Tagging of Resultative Events (states that re-
sult from taggable Events)
• Nominalized Events are tagged as regular
events
• Reported Events are not tagged
• Implicit events are not tagged
• Light verbs are not tagged
• Coreferential Events are tagged
• Tagging of multi-part triggers (both parts are
tagged only if they are contiguous)
</listItem>
<subsectionHeader confidence="0.993521">
4.2 Event Tagging Differences
</subsectionHeader>
<bodyText confidence="0.999173129032258">
One of the more general differences between ERE
and ACE Event tagging is the way in which each
model addresses Event Extent. ACE defines the
extent as always being the ‘entire sentence within
which the Event is described’ (Linguistic Data
Consortium, 2005). In ERE, the extent is the
entire document unless an event is coreferenced
(in which case, the extent is defined as the ‘span
of a document from the first trigger for a par-
ticular event to the next trigger for a particular
event.’ This signifies that the span can cross
sentence boundaries). Unlike ACE, ERE does
not delve into indicating Polarity, Tense, Gener-
icity, and Modality. ERE simplifies any anno-
tator confusion engendered by these features by
simply not tagging negative, future, hypotheti-
cal, conditional, uncertain or generic events (al-
though it does tag for past events). While ERE
only tags attested Events, ACE allows for irrealis
events, and includes attributes for marking them
as such: Believed Events; Hypothetical Events;
Commanded and Requested Events; Threatened,
Proposed and Discussed Events; Desired Events;
Promised Events; and Otherwise Unclear Con-
structions. Additionally both ERE and ACE tag
Event arguments as long as the arguments occur
within the event mention extent (another way of
saying that a taggable Event argument will occur
in the same sentence as the trigger word for its
Event). However, ERE and ACE have a diverging
approach to argument tagging:
</bodyText>
<listItem confidence="0.988660111111111">
• ERE is limited to pre-specified arguments for
each event and relation subtype. The pos-
sible arguments for ACE are: Event partici-
pants (limited to pre-specified roles for each
event type); Event-specific attributes that are
associated with a particular event type (e.g.,
the victim of an attack); and General event
attributes that can apply to most or all event
types (e.g., time, place).
</listItem>
<page confidence="0.958632">
49
</page>
<listItem confidence="0.988295375">
• ACE tags arguments regardless of modal cer-
tainty of their involvement in the event. ERE
only tags asserted participants in the event.
• The full noun phrase is marked in both ERE
and ACE arguments, but the head is only
specified in ACE. This is because ACE han-
dles entity annotation slightly differently than
ERE does; ACE marks the full noun phrase
</listItem>
<bodyText confidence="0.987814379310345">
with a head word for entity mention, and ERE
treats mentions differently based on their syn-
tactic features (for named or pronominal en-
tity mentions the name or pronominal itself
is marked, whereas for nominal mentions the
full noun phrase is marked).
Event Type and Subtype Differences Both an-
notation methods have almost identical Event
Type and Subtype categories. The only differences
between both are present in the Contact and Move-
ment Event Types.
A minor distinction in Subtype exists as a re-
sult of the types of entities that can be trans-
ported within the Movement Type category. In
ACE, ARTIFACT entities (WEAPON or VEHI-
CLE) as well as PERSON entities can be trans-
ported, whereas in ERE, only PERSON entities
can be transported. The difference between the
Phone-Write and Communicate Subtypes merely
lies in the definition. Both Subtypes are the de-
fault Subtype to cover all Contact events where
a ‘face-to-face’ meeting between sender and re-
ceiver is not explicitly stated. In ACE, this contact
is limited to written or telephone communication
where at least two parties are specified to make
this event subtype less open-ended. In ERE, this
requirement is simply widened to comprise elec-
tronic communication as well, explicitly including
those via internet channels (e.g., Skype).
</bodyText>
<sectionHeader confidence="0.987265" genericHeader="method">
5 TAC-KBP
</sectionHeader>
<bodyText confidence="0.9999454375">
After the final ACE evaluation in 2008 there was
interest in the community to form an evaluation
explicitly focused on knowledge bases (KBs) cre-
ated from the output of extraction systems. NIST
had recently started the Text Analysis Conference
series for related NLP tasks such as Recognizing
Textual Entailment, Summarization, and Question
Answering. In 2009 the first Knowledge Base
Population track (TAC-KBP) was held featuring
two initial tasks: (a) Entity Linking — linking en-
tities to KB entities, and (b) Slot Filling — adding
information to entity profiles that is missing from
the KB (McNamee et al., 2010). Due to its gener-
ous license and large scale, a snapshot of English
Wikipedia from late 2008 has been used as the ref-
erence KB in the TAC-KBP evaluations.
</bodyText>
<subsectionHeader confidence="0.995768">
5.1 Slot Filling Overview
</subsectionHeader>
<bodyText confidence="0.999978166666667">
Unlike ACE and ERE, Slot Filling does not have
as its primary goal the annotation of text. Rather,
the aim is to identify knowledge nuggets about a
focal named entity using a fixed inventory of re-
lations and attributes. For example, given a fo-
cal entity such as former Ukrainian prime minister
Yulia Tymoshenko, the task is to identify attributes
such as schools she attended, occupations, and im-
mediate family members. This is the same sort
of information commonly listed about prominent
people in Wikipedia Infoboxes and in derivative
databases such as FreeBase and DBpedia.
Consequently, Slot Filling is somewhat of a hy-
brid between relation extraction and question an-
swering — slot fills can be considered as the cor-
rect responses to a fixed set of questions. The rela-
tions and attributes used in the 2013 task are pre-
sented in Table 5.
</bodyText>
<subsectionHeader confidence="0.949793">
5.2 Differences with ACE-style relation
extraction
</subsectionHeader>
<bodyText confidence="0.9585725">
Slot Filling in TAC-KBP differs from extraction in
ACE and ERE in several significant ways:
</bodyText>
<listItem confidence="0.9995914">
• information is sought for named entities,
chiefly PERs and ORGs;
• the focus is on values not mentions;
• assessment is more like QA; and,
• events are handled as uncorrelated slots
</listItem>
<bodyText confidence="0.999926466666667">
In traditional IE evaluation, there was an
implicit skew towards highly attested in-
formation such as leader(Bush, US), or
capital(Paris, France). In contrast, TAC-KBP
gives full credit for finding a single instance of a
correct fill instead of every attestation of that fact.
Slot Filling assessment is somewhat simpler
than IE annotation. The assessor must decide
if provenance text is supportive of a posited fact
about the focal entity instead of annotating a doc-
ument with all evidenced relations and events for
any entity. For clarity and to increase assessor
agreement, guidelines have been developed to jus-
tify when a posited relation is deemed adequately
supported from text. Additionally, the problem of
</bodyText>
<page confidence="0.983115">
50
</page>
<table confidence="0.999698411764706">
Relations Attributes
per:children org:shareholders per:alternate names org:alternate names
per:other family org:founded by per:date of birth org:political religious affiliation
per:parents org:top members employees per:age org:number of employees members
per:siblings org:member of per:origin org:date founded
per:spouse org:members per:date of death org:date dissolved
per:employee or member of org:parents per:cause of death org:website
per:schools attended org:subsidiaries per:title
per:city of birth org:city of headquarters per:religion
per:stateorprovince of birth org:stateorprovince of headquarters per:charges
per:country of birth org:country of headquarters
per:cities of residence
per:statesorprovinces of residence
per:countries of residence
per:city of death
per:stateorprovince of death
per:country of death
</table>
<tableCaption confidence="0.999772">
Table 5: Relation and attributes for PERs and ORGs.
</tableCaption>
<bodyText confidence="0.999844117647059">
slot value equivalence becomes an issue - a sys-
tem should be penalized for redundantly asserting
that a person has four children named Tim, Beth,
Timothy, and Elizabeth, or that a person is both a
cardiologist and a doctor.
Rather than explicitly modeling events, TAC-
KBP created relations that capture events, more
in line with the notion of Infobox filling or ques-
tion answering (McNamee et al., 2010). For exam-
ple, instead of a criminal event, there is a slot fill
for charges brought against an entity. Instead of a
founding event, there are slots like org:founded by
(who) and org:date founded (when). Thus a state-
ment that “Jobs is the founder and CEO of Apple”
is every bit as useful for the org:founded by rela-
tion as “Jobs founded Apple in 1976.” even though
the date is not included in the former sentence.
</bodyText>
<subsectionHeader confidence="0.999461">
5.3 Additional tasks
</subsectionHeader>
<bodyText confidence="0.9999775">
Starting in 2012 TAC-KBP introduced the “Cold
Start” task, which is to literally produce a KB
based on the Slot Filling schema. To date, Cold
Start KBs have been built from collections of
O(50,000) documents, and due to their large size,
they are assessed by sampling. There is also
an event argument detection evaluation in KBP
planned for 2014.
Other TAC-KBP tasks have been introduced in-
cluding determining the timeline when dynamic
slot fills are valid (e.g., CEO of Microsoft), and
targeted sentiment.
</bodyText>
<sectionHeader confidence="0.996526" genericHeader="method">
6 FrameNet
</sectionHeader>
<bodyText confidence="0.999986341463415">
The FrameNet project has rather different moti-
vations than either ACE/ERE or TAC-KBP, but
shares with them a goal of capturing informa-
tion about events and relations in text. FrameNet
stems from Charles Fillmore’s linguistic and lex-
icographic theory of Frame Semantics (Fillmore,
1976; Fillmore, 1982). Frames are descriptions
of event (or state) types and contain information
about event participants (frame elements), infor-
mation as to how event types relate to each other
(frame relations), and information about which
words or multi-word expressions can trigger a
given frame (lexical units).
FrameNet is designed with text annotation in
mind, but unlike ACE/ERE it prioritizes lexico-
graphic and linguistic completeness over ease of
annotation. As a result Frames tend to be much
finer grained than ACE/ERE events, and are more
numerous by an order of magnitude. The Berkeley
FrameNet Project (Baker et al., 1998) was devel-
oped as a machine readable database of distinct
frames and lexical units (words and multi-word
constructions) that were known to trigger specific
frames.1 FrameNet 1.5 includes 1020 identified
frames and 11830 lexical units.
One of the most widespread uses of FrameNet
has been as a resource for Semantic Role Label-
ing (SRL) (Gildea and Jurafsky, 2002). FrameNet
related SRL was promoted as a task by the
SENSEVAL-3 workshop (Litkowski, 2004), and
the SemEval-2007 workshop (Baker et al., 2007).
(Das et al., 2010) is a current system for automatic
FrameNet annotation.
The relation and attribute types of TAC-KBP
and the relation and event types in the ACE/ERE
standards can be mapped to FrameNet frames.
The mapping is complicated by two factors.
The first is that FrameNet frames are gener-
ally more fine-grained than the ACE/ERE cate-
gories. As a result the mapping is sometimes
one-to-many. For example, the ERE relation Af-
</bodyText>
<footnote confidence="0.999159666666667">
1This database is accessible via webpage (https:
//framenet.icsi.berkeley.edu/fndrupal/)
and as a collection of XML files by request.
</footnote>
<page confidence="0.994963">
51
</page>
<table confidence="0.980894772727273">
Relations
FrameNet ACE ERE TAC-KBP
Kinship Personal-Social.Family Social.Family per:children
per:other family
per:parents
per:siblings
per:spouse
Being Employed ORG-Affiliation.Employment Affiliation.Employment/Membership per:employee or member of
Membership org:member of
Being Located Physical.Located Physical.Located org:city of headquarters
org:stateorprovince of headquarters
org:country of headquarters
Events
FrameNet ACE ERE
Contacting Phone-Write Communicate
Extradition Justice-Extradition Justice-Extradition
Attack Conflict-Attack Conflict-Attack
Being Born Life-Be Born Life-Be Born
Attributes
FrameNet TAC-KBP
Being Named per:alternate names
Age per:age
</table>
<tableCaption confidence="0.999104">
Table 6: Rough mappings between subsets of FrameNet, ACE, ERE, and TAC-KBP
</tableCaption>
<bodyText confidence="0.999874592592593">
filiation.Employment/Membership covers both
the Being Employed frame and the Member-
ship frame. At the same time, while TAC-
KBP has only a handful of relations relative to
FrameNet, some of these relations are more fine-
grained than the analogous frames or ACE/ERE
relations. For example, the frame Kinship, which
maps to the single ERE relation Social.Family,
maps to five TAC-KBP relations, and the Be-
ing Located, which maps to the ACE/ERE rela-
tion Being.Located, maps to three TAC-KBP re-
lations. Rough mappings from a selection of rela-
tions, events, and attributes are given in Table 6.
The second complication arises from the fact
that FrameNet frames are more complex objects
than ERE/ACE events, and considerably more
complex than TAC-KBP relations. Rather than the
two entities related via a TAC-KBP or ACE/ERE
relation, some frames have upwards of 20 frame
elements. Table 7 shows in detail the mapping be-
tween frame elements in the Extradition frame and
ACE’s and ERE’s Justice-Extradition events. The
“core” frame elements map exactly to the ERE
event, the remaining two arguments in the ACE
event map to two non-core frame elements, and
the frame includes several more non-core elements
with no analogue in either ACE or ERE standards.
</bodyText>
<sectionHeader confidence="0.998933" genericHeader="conclusions">
7 Conclusion
</sectionHeader>
<bodyText confidence="0.999865625">
The ACE and ERE annotation schemas have
closely related goals of identifying similar in-
formation across various possible types of docu-
ments, though their approaches differ due to sepa-
rate goals regarding scope and replicability. ERE
differs from ACE in collapsing different Type dis-
tinctions and in removing annotation features in
order to eliminate annotator confusion and to im-
</bodyText>
<table confidence="0.971707230769231">
FrameNet ACE ERE
Authorities Agent-Arg Agent-Arg
Crime jursidiction Destination-Arg Destination-Arg
Current jursidiction Origin-Arg Origin-Arg
Suspect Person-Arg Person-Arg
Reason Crime-Arg
Time Time-Arg
Legal Basis
Manner
Means
Place
Purpose
Depictive
</table>
<tableCaption confidence="0.950584">
Table 7: Mapping between frame elements of Ex-
</tableCaption>
<figureCaption confidence="0.498214666666667">
tradition (FrameNet), and arguments of Justice-
Extradition (ACE/ERE): A line divides core frame
elements (above) from non-core (below).
</figureCaption>
<bodyText confidence="0.998587071428572">
prove consistency, efficiency, and higher inter-
annotator agreement. TAC-KPB slot-filling shares
some goals with ACE/ERE, but is wholly fo-
cused on a set collection of questions (slots to
be filled) concerning entities to the extent that
there is no explicit modeling of events. At the
other extreme, FrameNet seeks to capture the
full range of linguistic and lexicographic varia-
tion in event representations in text. In general, all
events, relations, and attributes that can be repre-
sented by ACE/ERE and TAC-KBP standards can
be mapped to FrameNet representations, though
adjustments need to be made for granularity of
event/relation types and granularity of arguments.
</bodyText>
<sectionHeader confidence="0.997454" genericHeader="acknowledgments">
Acknowledgements
</sectionHeader>
<bodyText confidence="0.85122275">
This material is partially based on research spon-
sored by the NSF under grant IIS-1249516 and
DARPA under agreement number FA8750-13-2-
0017 (the DEFT program).
</bodyText>
<page confidence="0.99799">
52
</page>
<sectionHeader confidence="0.995878" genericHeader="references">
References
</sectionHeader>
<reference confidence="0.999959703125">
Collin F Baker, Charles J Fillmore, and John B Lowe.
1998. The berkeley framenet project. In Proceed-
ings of the 17th international conference on Compu-
tational linguistics-Volume 1, pages 86–90. Associ-
ation for Computational Linguistics.
Collin Baker, Michael Ellsworth, and Katrin Erk.
2007. Semeval-2007 task 19: Frame semantic
structure extraction. In Proceedings of the Fourth
International Workshop on Semantic Evaluations
(SemEval-2007), pages 99–104, Prague, Czech Re-
public, June. Association for Computational Lin-
guistics.
Dipanjan Das, Nathan Schneider, Desai Chen, and
Noah A Smith. 2010. Probabilistic frame-semantic
parsing. In Proceedings of NAACL-HLT, pages 948–
956. Association for Computational Linguistics.
George Doddington, Alexis Mitchell, Mark Przybocki,
Lancec Ramshaw, Stephanie Strassel, and Ralph
Weischedel. 2004. The automatic content extrac-
tion (ace) program- tasks, data, and evaluation. In
Proceedings of LREC 2004: Fourth International
Conference on Language Resources and Evaluation,
Lisbon, May 24-30.
Charles J Fillmore. 1976. Frame semantics and the na-
ture of language. Annals of the New York Academy
of Sciences, 280(1):20–32.
Charles Fillmore. 1982. Frame semantics. In Linguis-
tics in the morning calm, pages 111–137. Hanshin
Publishing Co.
Daniel Gildea and Daniel Jurafsky. 2002. Automatic
labeling of semantic roles. Computational linguis-
tics, 28(3):245–288.
Linguistic Data Consortium. 2005. ACE (automatic
content extraction) English annotation guidelines
for events. https://www.ldc.upenn.edu/
collaborations/past-projects/ace.
Version 5.4.3 2005.07.01.
Linguistic Data Consortium. 2006. ACE (automatic
content extraction) English annotation guidelines
forentities. https://www.ldc.upenn.edu/
collaborations/past-projects/ace,
Version 5.6.6 2006.08.01.
Linguistic Data Consortium. 2008. ACE (automatic
content extraction) English annotation guidelines for
relations. https://www.ldc.upenn.edu/
collaborations/past-projects/ace.
Version 6.0 2008.01.07.
Linguistic Data Consortium. 2013a. DEFT ERE anno-
tation guidelines: Entities v1.1, 05.17.2013.
Linguistic Data Consortium. 2013b. DEFT ERE anno-
tation guidelines: Events v1.1. 05.17.2013.
Linguistic Data Consortium. 2013c. DEFT ERE anno-
tation guidelines: Relations v1.1. 05.17.2013.
Ken Litkowski. 2004. Senseval-3 task: Automatic
labeling of semantic roles. In Rada Mihalcea and
Phil Edmonds, editors, Senseval-3: Third Interna-
tional Workshop on the Evaluation of Systems for the
Semantic Analysis of Text, pages 9–12, Barcelona,
Spain, July. Association for Computational Linguis-
tics.
Paul McNamee, Hoa Trang Dang, Heather Simpson,
Patrick Schone, and Stephanie Strassel. 2010. An
evaluation of technologies for knowledge base pop-
ulation. In Proceedings of LREC.
</reference>
<page confidence="0.999351">
53
</page>
</variant>
</algorithm>
<algorithm name="ParsHed" version="110505">
<variant no="0" confidence="0.000000">
<title confidence="0.983127">A Comparison of the Events and Relations Across ACE, ERE, TAC-KBP, and FrameNet Annotation Standards</title>
<author confidence="0.968283">Aguilar Beller McNamee Van</author>
<affiliation confidence="0.6879135">Human Language Technology Center of Johns Hopkins</affiliation>
<address confidence="0.997753">Baltimore, MD, USA</address>
<author confidence="0.99737">Strassel Song</author>
<affiliation confidence="0.9802365">University of Linguistic Data Consortium</affiliation>
<address confidence="0.999075">Philadelphia, PA, USA</address>
<abstract confidence="0.966860808943089">The resurgence of effort within computational semantics has led to increased interest in various types of relation extraction and semantic parsing. While various manually annotated resources exist for enabling this work, these materials have been developed with different standards and goals in mind. In an effort to develop better general understanding across these resources, we provide a summary overview of the standards underlying ACE, ERE, TAC-KBP Slot-filling, and FrameNet. 1 Overview ACE and ERE are comprehensive annotation standards that aim to consistently annotate Entities, Events, and Relations within a variety of documents. The ACE (Automatic Content Extraction) standard was developed by NIST in 1999 and has evolved over time to support different evaluation cycles, the last evaluation having occurred in 2008. The ERE (Entities, Relations, Events) standard was created under the DARPA DEFT program as a lighter-weight version of ACE with the goal of making annotation easier, and more consistent across annotators. ERE attempts to achieve this goal by consolidating some of the annotation type distinctions that were found to be the most problematic in ACE, as well as removing some more complex annotation features. This paper provides an overview of the relationship between these two standards and compares them to the more restricted standard of the TAC- KBP slot-filling task and the more expansive standard of FrameNet. Sections 3 and 4 examine Relations and Events in the ACE/ERE standards, section 5 looks at TAC-KBP slot-filling, and section 6 compares FrameNet to the other standards. 2 ACE and ERE Entity Tagging Many of the differences in Relations and Events annotation across the ACE and ERE standards stem from differences in entity mention tagging. This is simply because Relation and Event tagging relies on the distinctions established in the entity tagging portion of the annotation process. For exsince ERE collapses the ACE any ACE Relation or Event that relied on that distinction is revised in ERE. These top-level differences are worth keeping in mind when considering how Events and Relations tagging is approached in ACE and ERE: Type Inventory: ACE and ERE share the Perand ACE has two additional ERE does not for these Types and collapses the Fainto ERE includes a to address titles, honorifics, roles, and professions (Linguistic Data Consortium, 2006; Linguistic Data Consortium, 2013a). • Subtype Annotation: ACE further classifies entity mentions by including Subtypes for each determined Type; if the entity does not fit into any Subtype, it is not annotated. ERE annotation does not include any Subtypes. • Entity Classes: In addition to Subtype, ACE also classifies each entity mention according 45 of the 2nd Workshop on EVENTS: Definition, Detection, Coreference, and pages Maryland, USA, June 22-27, 2014. Association for Computational Linguistics 1996 1998 2000 2002 2004 2006 2008 2010 2012 Figure 1: Important Dates for the ACE, ERE, TAC-KBP, and FrameNet Standards to entity class (Specific, Generic, Attributive, and Underspecified). • Taggability: ACE tags Attributive, Generic, Specific, and Underspecified entity mentions. ERE only tags Specific entity mentions. • Extents and Heads: ACE marks the full noun phrase of an entity mention and tags a head word. ERE handles tagging based on the mention level of an entity; in Name mentions (NAM) the name is the extent, in Nominal mentions (NOM) the full noun phrase is the extent, in Pronoun mentions (PRO) the pronoun is the extent. • Tags: ERE only specifies Type and Mention level (NAM, NOM, PRO). ACE specifies Type, Subtype, Entity Class (Attributive, Generic, Specific, Underspecified), and Mention Level (NAM, NOM, PRO, Headless). 3 Relations in ACE and ERE In the ACE and ERE annotation models, the goal of the Relations task is to detect and characterthe targeted enti- Data Consortium, 2008; Linguistic Data Consortium, 2013c). The purpose of this task is to extract a representation of the meaning of the text, not necessarily tied to underlying syntactic or lexical semantic representations. Both models share similar overarching guidelines for determinwhat is For relations the differences lie in the absence or presence of additional feaas well as differences in and minor variations. 3.1 Similarities in Relations Annotation In addition to comprising similar Types (both include as as slightly different Types to address Affiliaused to characterize each relation, ACE and ERE share important similarities concerning their relation-tagging guidelines. These include: • Limiting relations to only those expressed in a single sentence • Tagging only for explicit mention • No ‘promoting’ or ‘nesting’ of taggable en- In the sentence, went to a hotel (Smith, hotel) is a taggable Physbut (Smith, Brazil) is not. This is because in order to tag this as such, one would have to promote ‘Brazil’. • Tagging for past and former relations • Two different Argument slots (Arg1 and Arg2) are provided for each relation to capture the importance of Argument ordering. • Arguments can be more than one token (although ACE marks the head as well) • Using ‘templates’ for each relation a relation, the Person that is located somewhere will always be assigned to Arg1 and the place in which the person is located will always be assigned to Arg2). • Neither model tags for negative relations • Both methods contain argument span boundaries. That is, the relations should only inwithin the extent of a sentence. 3.2 Differences in Assertion, Modality, and Tense A primary difference between these two annotation models is a result of ERE only annotating asserted events while ACE also includes hypotheticals. ACE accounts for these cases by including two Modality attributes: ASSERTED and OTHER 46 (Linguistic Data Consortium, 2008). For examin the sentence, are afraid that Al-Qaeda will be in ACE would tag this as an OTHER attribute, where OTHER pertains to situations in “some other world defined by counterfactual constraints elsewhere in the context”, whereas ERE would simply not tag a relation in this sentence. Additionally, while both ACE and ERE tag past and former relations, ACE goes further to mark the Tense of each relation by means of four attributes: Past, Future, Present and Unspecified. 3.3 Syntactic Classes ACE further justifies the tagging of each Relation through Syntactic Classes. The primary function of these classes is to serve as a sanity check on taggability and as an additional constraint for tagging. These classes include: Possessive, Preposition, PreMod, Coordination, Formulaic, Participal, Verbal, Relations Expressed by Verbs, and Other. Syntactic classes are not present in ERE relations annotation. 3.4 Triggers Explicit trigger words do not exist in ACE relation annotation; instead, the model annotates the full syntactic clause that serves as the ‘trigger’ for the relation. ERE attempts to minimize the annotated span by allowing for the tagging of an optional trigger word, defined as “the smallest extent of text that indicates a relation Type and Subtype” (Linguistic Data Consortium, 2013c). These triggers are not limited to a single word, but can also be composed of a phrase or any extent of the text that indicates a Type/Subtype relation, left to the discretion of the annotator. It is common for preposito be triggers, as in is However, sometimes no trigger is needed because the syntax of the sentence is such that it indicates a particular relation Type/Subtype without a word to explicitly signal the relation. 3.5 Types and Subtypes of Relations There are three types of relations that contain varied Subtypes between ERE and ACE. These are Types. The differences are a result of ERE collapsing ACE Types and Subtypes into more concise, if less specific, Type groups. Relation Type Differences main differences in the handling of the physical relations between ACE and ERE are shown in Table 1. ACE only marks Location for PERSON entities (for Arg1). ERE uses Location for PERSON entities being located somewhere as well as for a geographical location being part of another geographical location. Additionally, ACE includes ‘Near’ as a Subtype. This is used for when an entity is explicitly near another entity, but neither entity is a part of the other or located in/at the other. ERE does not have an equivalent Subtype to account for this physical relation. Instead, ERE includes ‘Origin’ as a Subtype. This is used to describe the relation between a PER and an ORG. does not have a equivalent, but it does account for this type of relation within separate Affiliation and ‘Citizen- Resident-Religion-Ethnicity’ Subtype. Relation Differences Table 2, note that ACE has a ‘Geographical’ Subtype which captures the location of a FAC, LOC, or GPE in or at, or as part of another FAC, LOC, GPE. Examples of this would be conthe region a phrase such as Atlanta ERE does not include this type of annotation option. Instead, ERE tags these regional reas ACE and ERE do share a ‘Subsidiary’ Subtype which is defined in both models as a “category to capture the ownership, administrative and other hierarchical relationships between ORGs and/or GPEs” (Linguistic Data Consortium, 2008; Linguistic Data Consortium, 2013c). Social and Affiliation Relation Differences The most evident discrepancy in relation annobetween the two models lies in the So- Types and Subtypes. For social relations, ACE and ERE have three Subtypes with similar goals (Business, Family, Unspecified/Lasting-Personal) but ERE has an additional ‘Membership’ Subtype, as shown in Table 3. ACE addresses all ‘Membership’ relations its ERE also includes the ‘So- Subtype in order to address the entity type, which only applies to ERE. However, both models agree that the arguments for each relation must be PERSON entities and that they should not include relationships implied from between two entities President 47</abstract>
<title confidence="0.85727024">Relation Type Relation Subtype ARG1 Type ARG2 Type ERE Physical Located Origin PER, GPE, LOC PER, ORG GPE, LOC GPE, LOC ACE Physical Located Near PER FAC, LOC, PER, FAC, GPE, LOC FAC, GPE, LOC 1: Comparison of Permitted Relation Arguments for the Distinction in the ERE and ACE Guidelines Relation Type Relation Subtype ARG1 Type ARG2 Type ERE Part-Whole Subsidiary ORG ORG, GPE ACE Part-Whole Geographical Subsidiary FAC, LOC, GPE ORG FAC, LOC, GPE ORG, GPE 2: Comparison of Permitted Relation Arguments for the and Subtype Distinctions in the ERE and ACE Guidelines Relation Type Relation Subtype ARG1 Type ARG2 Type ERE Social Business PER PER Social Family PER PER Social Membership PER PER Social Role TTL PER Social Unspecified PER PER ACE Personal-Social Business PER PER Personal-Social Family PER PER Personal-Social Lasting-Personal PER PER 3: Comparison of Permitted Relation Arguments for the and Subtype Distinctions in the ERE and ACE Guidelines Relation Type Relation Subtype ARG1 Type ARG2 Type ERE Affiliation Affiliation Employment/Membership Leadership PER, ORG, ORG, GPE ORG, GPE PER ACE ORG-Affiliation ORG-Affiliation ORG-Affiliation ORG-Affiliation ORG-Affiliation ORG-Affiliation Employment Ownership PER ORG, GPE ORG ORG-Affiliation Founder Student-Alum Sports-Affiliation Investor-Shareholder PER ORG, GPE ORG.Educational ORG Membership PER, ORG ORG, GPE PER ORG PER PER, GPE PER, GPE Agent-Artifact User-Owner-Inventor-Manufacturer PER, ORG, FAC GPE Gen-Affiliation Citizen-Resident-Religion-Ethnicity PER PER.Group, Gen-Affiliation Org-Location-Origin ORG LOC, ORG LOC, GPE 4: Comparison of Permitted Relation Arguments for the and Subtype Distinctions in the ERE and ACE Guidelines</title>
<abstract confidence="0.99395745">48 met with Yasser Arafat last week not be considered a social relation). As for the differences in affiliation relations, ACE includes many Subtype possibilities which can more accurately represent affiliation, whereas ERE only observes two Affiliation Subtype options (Table 4). 4 Events in ACE and ERE Events in both annotation methods are defined as ‘specific occurrences’, involving ‘specific participants’ (Linguistic Data Consortium, 2005; Linguistic Data Consortium, 2013b). The primary goal of Event tagging is to detect and characterize events that include tagged entities. The central Event tagging difference between ACE and ERE is the level of specificity present in ACE, whereas ERE tends to collapse tags for a more simplified approach. 4.1 Event Tagging Similarities Both annotation schemas annotate the same exact Event Types: LIFE, MOVEMENT, TRANS- ACTION, BUSINESS, CONFLICT, CONTACT, PERSONNEL, and JUSTICE events. Both annotation ontologies also include 33 Subtypes for each Type. Furthermore, both rely on the expression of an occurrence through the use of a ‘Trigger’. ACE, however, restricts the trigger to be a single word that most clearly expresses the event occurrence (usually a main verb), while ERE allows for the trigger to be a word or a phrase that instantiates the event (Linguistic Data Consortium, 2005; Linguistic Data Consortium, 2013b). Both methods annotate modifiers when they trigger events as well as anaphors, when they refer to previously mentioned events. Furthermore, when there is any ambiguity about which trigger to select, both methods have similar rules established, such as Stand-Alone Noun Rule cases where more than one trigger is possible, the noun that can be by itself to refer to the event will be the Stand-Alone Adjective Rule a verb and an adjective are used together to express the occurrence of an Event, the adjective will be chosen as the trigger whenever it can stand-alone to express the resulting state brought about by the Additionally, both annotation guidelines agree on the following: • Tagging of Resultative Events (states that result from taggable Events) • Nominalized Events are tagged as regular events Reported Events are Implicit events are Light verbs are • Coreferential Events are tagged • Tagging of multi-part triggers (both parts are tagged only if they are contiguous) 4.2 Event Tagging Differences One of the more general differences between ERE and ACE Event tagging is the way in which each addresses ACE defines the extent as always being the ‘entire sentence within which the Event is described’ (Linguistic Data Consortium, 2005). In ERE, the extent is the entire document unless an event is coreferenced (in which case, the extent is defined as the ‘span of a document from the first trigger for a particular event to the next trigger for a particular event.’ This signifies that the span can cross sentence boundaries). Unlike ACE, ERE does not delve into indicating Polarity, Tense, Genericity, and Modality. ERE simplifies any annotator confusion engendered by these features by simply not tagging negative, future, hypothetical, conditional, uncertain or generic events (although it does tag for past events). While ERE only tags attested Events, ACE allows for irrealis events, and includes attributes for marking them as such: Believed Events; Hypothetical Events;</abstract>
<title confidence="0.867766666666667">Commanded and Requested Events; Threatened, Proposed and Discussed Events; Desired Events; Promised Events; and Otherwise Unclear Con-</title>
<author confidence="0.538525">Additionally both ERE</author>
<author confidence="0.538525">ACE tag</author>
<abstract confidence="0.994076859903381">Event arguments as long as the arguments occur within the event mention extent (another way of saying that a taggable Event argument will occur in the same sentence as the trigger word for its Event). However, ERE and ACE have a diverging approach to argument tagging: • ERE is limited to pre-specified arguments for each event and relation subtype. The possible arguments for ACE are: Event participants (limited to pre-specified roles for each event type); Event-specific attributes that are with a particular event type the victim of an attack); and General event attributes that can apply to most or all event place). 49 • ACE tags arguments regardless of modal certainty of their involvement in the event. ERE only tags asserted participants in the event. • The full noun phrase is marked in both ERE and ACE arguments, but the head is only specified in ACE. This is because ACE handles entity annotation slightly differently than ERE does; ACE marks the full noun phrase with a head word for entity mention, and ERE treats mentions differently based on their syntactic features (for named or pronominal entity mentions the name or pronominal itself is marked, whereas for nominal mentions the full noun phrase is marked). Type and Subtype Differences annotation methods have almost identical Event Type and Subtype categories. The only differences between both are present in the Contact and Movement Event Types. A minor distinction in Subtype exists as a result of the types of entities that can be transported within the Movement Type category. In ACE, ARTIFACT entities (WEAPON or VEHI- CLE) as well as PERSON entities can be transported, whereas in ERE, only PERSON entities can be transported. The difference between the Phone-Write and Communicate Subtypes merely lies in the definition. Both Subtypes are the default Subtype to cover all Contact events where a ‘face-to-face’ meeting between sender and receiver is not explicitly stated. In ACE, this contact is limited to written or telephone communication where at least two parties are specified to make this event subtype less open-ended. In ERE, this requirement is simply widened to comprise electronic communication as well, explicitly including via internet channels 5 TAC-KBP After the final ACE evaluation in 2008 there was interest in the community to form an evaluation explicitly focused on knowledge bases (KBs) created from the output of extraction systems. NIST had recently started the Text Analysis Conference series for related NLP tasks such as Recognizing Textual Entailment, Summarization, and Question Answering. In 2009 the first Knowledge Base Population track (TAC-KBP) was held featuring two initial tasks: (a) Entity Linking — linking entities to KB entities, and (b) Slot Filling — adding information to entity profiles that is missing from the KB (McNamee et al., 2010). Due to its generous license and large scale, a snapshot of English Wikipedia from late 2008 has been used as the reference KB in the TAC-KBP evaluations. 5.1 Slot Filling Overview Unlike ACE and ERE, Slot Filling does not have as its primary goal the annotation of text. Rather, the aim is to identify knowledge nuggets about a focal named entity using a fixed inventory of relations and attributes. For example, given a focal entity such as former Ukrainian prime minister Yulia Tymoshenko, the task is to identify attributes such as schools she attended, occupations, and immediate family members. This is the same sort of information commonly listed about prominent people in Wikipedia Infoboxes and in derivative databases such as FreeBase and DBpedia. Consequently, Slot Filling is somewhat of a hybrid between relation extraction and question answering — slot fills can be considered as the correct responses to a fixed set of questions. The relations and attributes used in the 2013 task are presented in Table 5. 5.2 Differences with ACE-style relation extraction Slot Filling in TAC-KBP differs from extraction in ACE and ERE in several significant ways: information is sought for chiefly PERs and ORGs; • the focus is on values not mentions; • assessment is more like QA; and, • events are handled as uncorrelated slots In traditional IE evaluation, there was an implicit skew towards highly attested insuch as or In contrast, TAC-KBP gives full credit for finding a single instance of a correct fill instead of every attestation of that fact. Slot Filling assessment is somewhat simpler than IE annotation. The assessor must decide if provenance text is supportive of a posited fact about the focal entity instead of annotating a document with all evidenced relations and events for any entity. For clarity and to increase assessor agreement, guidelines have been developed to justify when a posited relation is deemed adequately supported from text. Additionally, the problem of 50 Relations Attributes per:children org:shareholders per:alternate names org:alternate names per:other family org:founded by per:date of birth org:political religious affiliation per:parents org:top members employees per:age org:number of employees members per:siblings org:member of per:origin org:date founded per:spouse org:members per:date of death org:date dissolved per:employee or member of org:parents per:cause of death org:website per:schools attended org:subsidiaries per:title per:city of birth org:city of headquarters per:religion per:stateorprovince of birth org:stateorprovince of headquarters per:charges per:country of birth org:country of headquarters per:cities of residence per:statesorprovinces of residence per:countries of residence per:city of death per:stateorprovince of death per:country of death Table 5: Relation and attributes for PERs and ORGs. slot value equivalence becomes an issue a system should be penalized for redundantly asserting that a person has four children named Tim, Beth, Timothy, and Elizabeth, or that a person is both a cardiologist and a doctor. Rather than explicitly modeling events, TAC- KBP created relations that capture events, more in line with the notion of Infobox filling or question answering (McNamee et al., 2010). For example, instead of a criminal event, there is a slot fill for charges brought against an entity. Instead of a founding event, there are slots like org:founded by (who) and org:date founded (when). Thus a statement that “Jobs is the founder and CEO of Apple” is every bit as useful for the org:founded by relation as “Jobs founded Apple in 1976.” even though the date is not included in the former sentence. 5.3 Additional tasks Starting in 2012 TAC-KBP introduced the “Cold Start” task, which is to literally produce a KB based on the Slot Filling schema. To date, Cold Start KBs have been built from collections of O(50,000) documents, and due to their large size, they are assessed by sampling. There is also an event argument detection evaluation in KBP planned for 2014. Other TAC-KBP tasks have been introduced including determining the timeline when dynamic fills are valid of Microsoft), and targeted sentiment. 6 FrameNet The FrameNet project has rather different motivations than either ACE/ERE or TAC-KBP, but shares with them a goal of capturing information about events and relations in text. FrameNet from Charles Fillmore’s linguistic and lexicographic theory of Frame Semantics (Fillmore, 1976; Fillmore, 1982). Frames are descriptions of event (or state) types and contain information event participants information as to how event types relate to each other and information about which words or multi-word expressions can trigger a frame FrameNet is designed with text annotation in mind, but unlike ACE/ERE it prioritizes lexicographic and linguistic completeness over ease of annotation. As a result Frames tend to be much finer grained than ACE/ERE events, and are more numerous by an order of magnitude. The Berkeley FrameNet Project (Baker et al., 1998) was developed as a machine readable database of distinct frames and lexical units (words and multi-word constructions) that were known to trigger specific FrameNet 1.5 includes 1020 identified frames and 11830 lexical units. One of the most widespread uses of FrameNet has been as a resource for Semantic Role Labeling (SRL) (Gildea and Jurafsky, 2002). FrameNet related SRL was promoted as a task by the SENSEVAL-3 workshop (Litkowski, 2004), and the SemEval-2007 workshop (Baker et al., 2007). (Das et al., 2010) is a current system for automatic FrameNet annotation. The relation and attribute types of TAC-KBP and the relation and event types in the ACE/ERE standards can be mapped to FrameNet frames. The mapping is complicated by two factors. The first is that FrameNet frames are generally more fine-grained than the ACE/ERE categories. As a result the mapping is sometimes For example, the ERE relation Afdatabase is accessible via webpage and as a collection of XML files by request. 51</abstract>
<title confidence="0.976374333333333">Relations FrameNet ACE ERE TAC-KBP Kinship Personal-Social.Family Social.Family per:children per:other family per:parents per:siblings per:spouse Being Employed Membership ORG-Affiliation.Employment Affiliation.Employment/Membership per:employee or member of org:member of Being Located Physical.Located Physical.Located org:city of headquarters org:stateorprovince of headquarters org:country of headquarters Events FrameNet ACE ERE Contacting Phone-Write Communicate Extradition Justice-Extradition Justice-Extradition Attack Conflict-Attack Conflict-Attack Being Born Life-Be Born Life-Be Born Attributes FrameNet TAC-KBP Being Named per:alternate names</title>
<abstract confidence="0.9747435">Age per:age Table 6: Rough mappings between subsets of FrameNet, ACE, ERE, and TAC-KBP both Employed and the Member- At the same time, while TAC- KBP has only a handful of relations relative to FrameNet, some of these relations are more finegrained than the analogous frames or ACE/ERE For example, the frame which to the single ERE relation to five TAC-KBP relations, and the Bewhich maps to the ACE/ERE relamaps to three TAC-KBP relations. Rough mappings from a selection of relations, events, and attributes are given in Table 6. The second complication arises from the fact that FrameNet frames are more complex objects than ERE/ACE events, and considerably more complex than TAC-KBP relations. Rather than the two entities related via a TAC-KBP or ACE/ERE relation, some frames have upwards of 20 frame elements. Table 7 shows in detail the mapping between frame elements in the Extradition frame and ACE’s and ERE’s Justice-Extradition events. The “core” frame elements map exactly to the ERE event, the remaining two arguments in the ACE event map to two non-core frame elements, and the frame includes several more non-core elements with no analogue in either ACE or ERE standards. 7 Conclusion The ACE and ERE annotation schemas have closely related goals of identifying similar information across various possible types of documents, though their approaches differ due to separate goals regarding scope and replicability. ERE differs from ACE in collapsing different Type distinctions and in removing annotation features in to eliminate annotator confusion and to im-</abstract>
<title confidence="0.887577769230769">FrameNet ACE ERE Authorities Agent-Arg Agent-Arg Crime jursidiction Destination-Arg Destination-Arg Current jursidiction Origin-Arg Origin-Arg Suspect Person-Arg Person-Arg Reason Crime-Arg Time Time-Arg Legal Basis Manner Means Place Purpose Depictive</title>
<abstract confidence="0.992446555555556">Table 7: Mapping between frame elements of Extradition (FrameNet), and arguments of Justice- Extradition (ACE/ERE): A line divides core frame elements (above) from non-core (below). prove consistency, efficiency, and higher interannotator agreement. TAC-KPB slot-filling shares some goals with ACE/ERE, but is wholly focused on a set collection of questions (slots to be filled) concerning entities to the extent that there is no explicit modeling of events. At the other extreme, FrameNet seeks to capture the full range of linguistic and lexicographic variation in event representations in text. In general, all events, relations, and attributes that can be represented by ACE/ERE and TAC-KBP standards can be mapped to FrameNet representations, though adjustments need to be made for granularity of event/relation types and granularity of arguments.</abstract>
<note confidence="0.888258243243243">Acknowledgements This material is partially based on research sponsored by the NSF under grant IIS-1249516 and DARPA under agreement number FA8750-13-2- 0017 (the DEFT program). 52 References Collin F Baker, Charles J Fillmore, and John B Lowe. The berkeley framenet project. In Proceedings of the 17th international conference on Compulinguistics-Volume pages 86–90. Association for Computational Linguistics. Collin Baker, Michael Ellsworth, and Katrin Erk. 2007. Semeval-2007 task 19: Frame semantic extraction. In of the Fourth International Workshop on Semantic Evaluations pages 99–104, Prague, Czech Republic, June. Association for Computational Linguistics. Dipanjan Das, Nathan Schneider, Desai Chen, and Noah A Smith. 2010. Probabilistic frame-semantic In of pages 948– 956. Association for Computational Linguistics. George Doddington, Alexis Mitchell, Mark Przybocki, Lancec Ramshaw, Stephanie Strassel, and Ralph Weischedel. 2004. The automatic content extraction (ace) programtasks, data, and evaluation. In Proceedings of LREC 2004: Fourth International on Language Resources and Lisbon, May 24-30. Charles J Fillmore. 1976. Frame semantics and the naof language. of the New York Academy 280(1):20–32. Fillmore. 1982. Frame semantics. In Linguisin the morning pages 111–137. Hanshin Publishing Co. Daniel Gildea and Daniel Jurafsky. 2002. Automatic</note>
<abstract confidence="0.8453538">of semantic roles. linguis- 28(3):245–288. Linguistic Data Consortium. 2005. ACE (automatic content extraction) English annotation guidelines events.</abstract>
<note confidence="0.572756277777778">Version 5.4.3 2005.07.01. Linguistic Data Consortium. 2006. ACE (automatic content extraction) English annotation guidelines Version 5.6.6 2006.08.01. Linguistic Data Consortium. 2008. ACE (automatic content extraction) English annotation guidelines for Version 6.0 2008.01.07. Linguistic Data Consortium. 2013a. DEFT ERE annotation guidelines: Entities v1.1, 05.17.2013. Linguistic Data Consortium. 2013b. DEFT ERE annotation guidelines: Events v1.1. 05.17.2013. Linguistic Data Consortium. 2013c. DEFT ERE annotation guidelines: Relations v1.1. 05.17.2013. Ken Litkowski. 2004. Senseval-3 task: Automatic labeling of semantic roles. In Rada Mihalcea and Edmonds, editors, Third International Workshop on the Evaluation of Systems for the Analysis of pages 9–12, Barcelona,</note>
<address confidence="0.702155">Spain, July. Association for Computational Linguis-</address>
<email confidence="0.862792">tics.</email>
<author confidence="0.961241">An</author>
<affiliation confidence="0.7933355">evaluation of technologies for knowledge base pop- In of</affiliation>
<address confidence="0.436052">53</address>
</variant>
</algorithm>
<algorithm name="ParsCit" version="110505">
<citationList>
<citation valid="true">
<authors>
<author>Collin F Baker</author>
<author>Charles J Fillmore</author>
<author>John B Lowe</author>
</authors>
<title>The berkeley framenet project.</title>
<date>1998</date>
<booktitle>In Proceedings of the 17th international conference on Computational linguistics-Volume 1,</booktitle>
<pages>86--90</pages>
<publisher>Association</publisher>
<institution>for Computational Linguistics.</institution>
<contexts>
<context position="24847" citStr="Baker et al., 1998" startWordPosition="3958" endWordPosition="3961">82). Frames are descriptions of event (or state) types and contain information about event participants (frame elements), information as to how event types relate to each other (frame relations), and information about which words or multi-word expressions can trigger a given frame (lexical units). FrameNet is designed with text annotation in mind, but unlike ACE/ERE it prioritizes lexicographic and linguistic completeness over ease of annotation. As a result Frames tend to be much finer grained than ACE/ERE events, and are more numerous by an order of magnitude. The Berkeley FrameNet Project (Baker et al., 1998) was developed as a machine readable database of distinct frames and lexical units (words and multi-word constructions) that were known to trigger specific frames.1 FrameNet 1.5 includes 1020 identified frames and 11830 lexical units. One of the most widespread uses of FrameNet has been as a resource for Semantic Role Labeling (SRL) (Gildea and Jurafsky, 2002). FrameNet related SRL was promoted as a task by the SENSEVAL-3 workshop (Litkowski, 2004), and the SemEval-2007 workshop (Baker et al., 2007). (Das et al., 2010) is a current system for automatic FrameNet annotation. The relation and att</context>
</contexts>
<marker>Baker, Fillmore, Lowe, 1998</marker>
<rawString>Collin F Baker, Charles J Fillmore, and John B Lowe. 1998. The berkeley framenet project. In Proceedings of the 17th international conference on Computational linguistics-Volume 1, pages 86–90. Association for Computational Linguistics.</rawString>
</citation>
<citation valid="true">
<authors>
<author>Collin Baker</author>
<author>Michael Ellsworth</author>
<author>Katrin Erk</author>
</authors>
<title>Semeval-2007 task 19: Frame semantic structure extraction.</title>
<date>2007</date>
<booktitle>In Proceedings of the Fourth International Workshop on Semantic Evaluations (SemEval-2007),</booktitle>
<pages>99--104</pages>
<publisher>Association</publisher>
<institution>for Computational Linguistics.</institution>
<location>Prague, Czech Republic,</location>
<contexts>
<context position="25351" citStr="Baker et al., 2007" startWordPosition="4038" endWordPosition="4041"> ACE/ERE events, and are more numerous by an order of magnitude. The Berkeley FrameNet Project (Baker et al., 1998) was developed as a machine readable database of distinct frames and lexical units (words and multi-word constructions) that were known to trigger specific frames.1 FrameNet 1.5 includes 1020 identified frames and 11830 lexical units. One of the most widespread uses of FrameNet has been as a resource for Semantic Role Labeling (SRL) (Gildea and Jurafsky, 2002). FrameNet related SRL was promoted as a task by the SENSEVAL-3 workshop (Litkowski, 2004), and the SemEval-2007 workshop (Baker et al., 2007). (Das et al., 2010) is a current system for automatic FrameNet annotation. The relation and attribute types of TAC-KBP and the relation and event types in the ACE/ERE standards can be mapped to FrameNet frames. The mapping is complicated by two factors. The first is that FrameNet frames are generally more fine-grained than the ACE/ERE categories. As a result the mapping is sometimes one-to-many. For example, the ERE relation Af1This database is accessible via webpage (https: //framenet.icsi.berkeley.edu/fndrupal/) and as a collection of XML files by request. 51 Relations FrameNet ACE ERE TAC-</context>
</contexts>
<marker>Baker, Ellsworth, Erk, 2007</marker>
<rawString>Collin Baker, Michael Ellsworth, and Katrin Erk. 2007. Semeval-2007 task 19: Frame semantic structure extraction. In Proceedings of the Fourth International Workshop on Semantic Evaluations (SemEval-2007), pages 99–104, Prague, Czech Republic, June. Association for Computational Linguistics.</rawString>
</citation>
<citation valid="true">
<authors>
<author>Dipanjan Das</author>
<author>Nathan Schneider</author>
<author>Desai Chen</author>
<author>Noah A Smith</author>
</authors>
<title>Probabilistic frame-semantic parsing.</title>
<date>2010</date>
<booktitle>In Proceedings of NAACL-HLT,</booktitle>
<pages>948--956</pages>
<publisher>Association</publisher>
<institution>for Computational Linguistics.</institution>
<contexts>
<context position="25371" citStr="Das et al., 2010" startWordPosition="4042" endWordPosition="4045">re more numerous by an order of magnitude. The Berkeley FrameNet Project (Baker et al., 1998) was developed as a machine readable database of distinct frames and lexical units (words and multi-word constructions) that were known to trigger specific frames.1 FrameNet 1.5 includes 1020 identified frames and 11830 lexical units. One of the most widespread uses of FrameNet has been as a resource for Semantic Role Labeling (SRL) (Gildea and Jurafsky, 2002). FrameNet related SRL was promoted as a task by the SENSEVAL-3 workshop (Litkowski, 2004), and the SemEval-2007 workshop (Baker et al., 2007). (Das et al., 2010) is a current system for automatic FrameNet annotation. The relation and attribute types of TAC-KBP and the relation and event types in the ACE/ERE standards can be mapped to FrameNet frames. The mapping is complicated by two factors. The first is that FrameNet frames are generally more fine-grained than the ACE/ERE categories. As a result the mapping is sometimes one-to-many. For example, the ERE relation Af1This database is accessible via webpage (https: //framenet.icsi.berkeley.edu/fndrupal/) and as a collection of XML files by request. 51 Relations FrameNet ACE ERE TAC-KBP Kinship Personal</context>
</contexts>
<marker>Das, Schneider, Chen, Smith, 2010</marker>
<rawString>Dipanjan Das, Nathan Schneider, Desai Chen, and Noah A Smith. 2010. Probabilistic frame-semantic parsing. In Proceedings of NAACL-HLT, pages 948– 956. Association for Computational Linguistics.</rawString>
</citation>
<citation valid="true">
<authors>
<author>George Doddington</author>
<author>Alexis Mitchell</author>
<author>Mark Przybocki</author>
<author>Lancec Ramshaw</author>
<author>Stephanie Strassel</author>
<author>Ralph Weischedel</author>
</authors>
<title>The automatic content extraction (ace) program- tasks, data, and evaluation.</title>
<date>2004</date>
<booktitle>In Proceedings of LREC 2004: Fourth International Conference on Language Resources and Evaluation,</booktitle>
<pages>24--30</pages>
<location>Lisbon,</location>
<marker>Doddington, Mitchell, Przybocki, Ramshaw, Strassel, Weischedel, 2004</marker>
<rawString>George Doddington, Alexis Mitchell, Mark Przybocki, Lancec Ramshaw, Stephanie Strassel, and Ralph Weischedel. 2004. The automatic content extraction (ace) program- tasks, data, and evaluation. In Proceedings of LREC 2004: Fourth International Conference on Language Resources and Evaluation, Lisbon, May 24-30.</rawString>
</citation>
<citation valid="true">
<authors>
<author>Charles J Fillmore</author>
</authors>
<title>Frame semantics and the nature of language.</title>
<date>1976</date>
<journal>Annals of the New York Academy of Sciences,</journal>
<volume>280</volume>
<issue>1</issue>
<contexts>
<context position="24214" citStr="Fillmore, 1976" startWordPosition="3862" endWordPosition="3863">O(50,000) documents, and due to their large size, they are assessed by sampling. There is also an event argument detection evaluation in KBP planned for 2014. Other TAC-KBP tasks have been introduced including determining the timeline when dynamic slot fills are valid (e.g., CEO of Microsoft), and targeted sentiment. 6 FrameNet The FrameNet project has rather different motivations than either ACE/ERE or TAC-KBP, but shares with them a goal of capturing information about events and relations in text. FrameNet stems from Charles Fillmore’s linguistic and lexicographic theory of Frame Semantics (Fillmore, 1976; Fillmore, 1982). Frames are descriptions of event (or state) types and contain information about event participants (frame elements), information as to how event types relate to each other (frame relations), and information about which words or multi-word expressions can trigger a given frame (lexical units). FrameNet is designed with text annotation in mind, but unlike ACE/ERE it prioritizes lexicographic and linguistic completeness over ease of annotation. As a result Frames tend to be much finer grained than ACE/ERE events, and are more numerous by an order of magnitude. The Berkeley Fram</context>
</contexts>
<marker>Fillmore, 1976</marker>
<rawString>Charles J Fillmore. 1976. Frame semantics and the nature of language. Annals of the New York Academy of Sciences, 280(1):20–32.</rawString>
</citation>
<citation valid="true">
<authors>
<author>Charles Fillmore</author>
</authors>
<title>Frame semantics.</title>
<date>1982</date>
<booktitle>In Linguistics in the morning calm,</booktitle>
<pages>111--137</pages>
<publisher>Hanshin Publishing Co.</publisher>
<contexts>
<context position="24231" citStr="Fillmore, 1982" startWordPosition="3864" endWordPosition="3865">nts, and due to their large size, they are assessed by sampling. There is also an event argument detection evaluation in KBP planned for 2014. Other TAC-KBP tasks have been introduced including determining the timeline when dynamic slot fills are valid (e.g., CEO of Microsoft), and targeted sentiment. 6 FrameNet The FrameNet project has rather different motivations than either ACE/ERE or TAC-KBP, but shares with them a goal of capturing information about events and relations in text. FrameNet stems from Charles Fillmore’s linguistic and lexicographic theory of Frame Semantics (Fillmore, 1976; Fillmore, 1982). Frames are descriptions of event (or state) types and contain information about event participants (frame elements), information as to how event types relate to each other (frame relations), and information about which words or multi-word expressions can trigger a given frame (lexical units). FrameNet is designed with text annotation in mind, but unlike ACE/ERE it prioritizes lexicographic and linguistic completeness over ease of annotation. As a result Frames tend to be much finer grained than ACE/ERE events, and are more numerous by an order of magnitude. The Berkeley FrameNet Project (Bak</context>
</contexts>
<marker>Fillmore, 1982</marker>
<rawString>Charles Fillmore. 1982. Frame semantics. In Linguistics in the morning calm, pages 111–137. Hanshin Publishing Co.</rawString>
</citation>
<citation valid="true">
<authors>
<author>Daniel Gildea</author>
<author>Daniel Jurafsky</author>
</authors>
<title>Automatic labeling of semantic roles.</title>
<date>2002</date>
<journal>Computational linguistics,</journal>
<pages>28--3</pages>
<contexts>
<context position="25209" citStr="Gildea and Jurafsky, 2002" startWordPosition="4016" endWordPosition="4019">nlike ACE/ERE it prioritizes lexicographic and linguistic completeness over ease of annotation. As a result Frames tend to be much finer grained than ACE/ERE events, and are more numerous by an order of magnitude. The Berkeley FrameNet Project (Baker et al., 1998) was developed as a machine readable database of distinct frames and lexical units (words and multi-word constructions) that were known to trigger specific frames.1 FrameNet 1.5 includes 1020 identified frames and 11830 lexical units. One of the most widespread uses of FrameNet has been as a resource for Semantic Role Labeling (SRL) (Gildea and Jurafsky, 2002). FrameNet related SRL was promoted as a task by the SENSEVAL-3 workshop (Litkowski, 2004), and the SemEval-2007 workshop (Baker et al., 2007). (Das et al., 2010) is a current system for automatic FrameNet annotation. The relation and attribute types of TAC-KBP and the relation and event types in the ACE/ERE standards can be mapped to FrameNet frames. The mapping is complicated by two factors. The first is that FrameNet frames are generally more fine-grained than the ACE/ERE categories. As a result the mapping is sometimes one-to-many. For example, the ERE relation Af1This database is accessib</context>
</contexts>
<marker>Gildea, Jurafsky, 2002</marker>
<rawString>Daniel Gildea and Daniel Jurafsky. 2002. Automatic labeling of semantic roles. Computational linguistics, 28(3):245–288.</rawString>
</citation>
<citation valid="true">
<authors>
<author>Linguistic Data Consortium</author>
</authors>
<title>ACE (automatic content extraction) English annotation guidelines for events. https://www.ldc.upenn.edu/ collaborations/past-projects/ace.</title>
<date>2005</date>
<journal>Version</journal>
<volume>5</volume>
<pages>2005--07</pages>
<contexts>
<context position="13501" citStr="Consortium, 2005" startWordPosition="2144" endWordPosition="2145">tion-Origin ORG LOC, GPE Table 4: Comparison of Permitted Relation Arguments for the Affiliation Type and Subtype Distinctions in the ERE and ACE Guidelines 48 Clinton met with Yasser Arafat last week would not be considered a social relation). As for the differences in affiliation relations, ACE includes many Subtype possibilities which can more accurately represent affiliation, whereas ERE only observes two Affiliation Subtype options (Table 4). 4 Events in ACE and ERE Events in both annotation methods are defined as ‘specific occurrences’, involving ‘specific participants’ (Linguistic Data Consortium, 2005; Linguistic Data Consortium, 2013b). The primary goal of Event tagging is to detect and characterize events that include tagged entities. The central Event tagging difference between ACE and ERE is the level of specificity present in ACE, whereas ERE tends to collapse tags for a more simplified approach. 4.1 Event Tagging Similarities Both annotation schemas annotate the same exact Event Types: LIFE, MOVEMENT, TRANSACTION, BUSINESS, CONFLICT, CONTACT, PERSONNEL, and JUSTICE events. Both annotation ontologies also include 33 Subtypes for each Type. Furthermore, both rely on the expression of a</context>
<context position="15782" citStr="Consortium, 2005" startWordPosition="2515" endWordPosition="2516">ng of Resultative Events (states that result from taggable Events) • Nominalized Events are tagged as regular events • Reported Events are not tagged • Implicit events are not tagged • Light verbs are not tagged • Coreferential Events are tagged • Tagging of multi-part triggers (both parts are tagged only if they are contiguous) 4.2 Event Tagging Differences One of the more general differences between ERE and ACE Event tagging is the way in which each model addresses Event Extent. ACE defines the extent as always being the ‘entire sentence within which the Event is described’ (Linguistic Data Consortium, 2005). In ERE, the extent is the entire document unless an event is coreferenced (in which case, the extent is defined as the ‘span of a document from the first trigger for a particular event to the next trigger for a particular event.’ This signifies that the span can cross sentence boundaries). Unlike ACE, ERE does not delve into indicating Polarity, Tense, Genericity, and Modality. ERE simplifies any annotator confusion engendered by these features by simply not tagging negative, future, hypothetical, conditional, uncertain or generic events (although it does tag for past events). While ERE only</context>
</contexts>
<marker>Consortium, 2005</marker>
<rawString>Linguistic Data Consortium. 2005. ACE (automatic content extraction) English annotation guidelines for events. https://www.ldc.upenn.edu/ collaborations/past-projects/ace. Version 5.4.3 2005.07.01.</rawString>
</citation>
<citation valid="true">
<authors>
<author>Linguistic Data Consortium</author>
</authors>
<title>ACE (automatic content extraction) English annotation guidelines forentities. https://www.ldc.upenn.edu/ collaborations/past-projects/ace,</title>
<date>2006</date>
<journal>Version</journal>
<volume>5</volume>
<pages>2006--08</pages>
<contexts>
<context position="2989" citStr="Consortium, 2006" startWordPosition="476" endWordPosition="477">e ACE Facility and Location Types, any ACE Relation or Event that relied on that distinction is revised in ERE. These top-level differences are worth keeping in mind when considering how Events and Relations tagging is approached in ACE and ERE: • Type Inventory: ACE and ERE share the Person, Organization, Geo-Political Entity, and Location Types. ACE has two additional Types: Vehicle and Weapon. ERE does not account for these Types and collapses the Facility and Location Types into Location. ERE also includes a Title Type to address titles, honorifics, roles, and professions (Linguistic Data Consortium, 2006; Linguistic Data Consortium, 2013a). • Subtype Annotation: ACE further classifies entity mentions by including Subtypes for each determined Type; if the entity does not fit into any Subtype, it is not annotated. ERE annotation does not include any Subtypes. • Entity Classes: In addition to Subtype, ACE also classifies each entity mention according 45 Proceedings of the 2nd Workshop on EVENTS: Definition, Detection, Coreference, and Representation, pages 45–53, Baltimore, Maryland, USA, June 22-27, 2014. c�2014 Association for Computational Linguistics 1996 1998 2000 2002 2004 2006 2008 2010 2</context>
</contexts>
<marker>Consortium, 2006</marker>
<rawString>Linguistic Data Consortium. 2006. ACE (automatic content extraction) English annotation guidelines forentities. https://www.ldc.upenn.edu/ collaborations/past-projects/ace, Version 5.6.6 2006.08.01.</rawString>
</citation>
<citation valid="true">
<authors>
<author>Linguistic Data Consortium</author>
</authors>
<title>ACE (automatic content extraction) English annotation guidelines for relations. https://www.ldc.upenn.edu/ collaborations/past-projects/ace.</title>
<date>2008</date>
<journal>Version</journal>
<volume>6</volume>
<pages>2008--01</pages>
<contexts>
<context position="4601" citStr="Consortium, 2008" startWordPosition="730" endWordPosition="731"> based on the mention level of an entity; in Name mentions (NAM) the name is the extent, in Nominal mentions (NOM) the full noun phrase is the extent, in Pronoun mentions (PRO) the pronoun is the extent. • Tags: ERE only specifies Type and Mention level (NAM, NOM, PRO). ACE specifies Type, Subtype, Entity Class (Attributive, Generic, Specific, Underspecified), and Mention Level (NAM, NOM, PRO, Headless). 3 Relations in ACE and ERE In the ACE and ERE annotation models, the goal of the Relations task is to detect and characterize relations of the targeted types between entities (Linguistic Data Consortium, 2008; Linguistic Data Consortium, 2013c). The purpose of this task is to extract a representation of the meaning of the text, not necessarily tied to underlying syntactic or lexical semantic representations. Both models share similar overarching guidelines for determining what is taggable. For relations the differences lie in the absence or presence of additional features, syntactic classes, as well as differences in assertion, trigger words, and minor subtype variations. 3.1 Similarities in Relations Annotation In addition to comprising similar Types (both models include Physical and Part.Whole T</context>
<context position="6781" citStr="Consortium, 2008" startWordPosition="1080" endWordPosition="1081"> will always be assigned to Arg1 and the place in which the person is located will always be assigned to Arg2). • Neither model tags for negative relations • Both methods contain argument span boundaries. That is, the relations should only include tagged entities within the extent of a sentence. 3.2 Differences in Assertion, Modality, and Tense A primary difference between these two annotation models is a result of ERE only annotating asserted events while ACE also includes hypotheticals. ACE accounts for these cases by including two Modality attributes: ASSERTED and OTHER 46 (Linguistic Data Consortium, 2008). For example, in the sentence, We are afraid that Al-Qaeda terrorists will be in Baghdad, ACE would tag this as an OTHER attribute, where OTHER pertains to situations in “some other world defined by counterfactual constraints elsewhere in the context”, whereas ERE would simply not tag a relation in this sentence. Additionally, while both ACE and ERE tag past and former relations, ACE goes further to mark the Tense of each relation by means of four attributes: Past, Future, Present and Unspecified. 3.3 Syntactic Classes ACE further justifies the tagging of each Relation through Syntactic Class</context>
<context position="10401" citStr="Consortium, 2008" startWordPosition="1690" endWordPosition="1691">le Relation Differences In Table 2, note that ACE has a ‘Geographical’ Subtype which captures the location of a FAC, LOC, or GPE in or at, or as part of another FAC, LOC, or GPE. Examples of this would be India controlled the region or a phrase such as the Atlanta area. ERE does not include this type of annotation option. Instead, ERE tags these regional relations as Physical.Located. ACE and ERE do share a ‘Subsidiary’ Subtype which is defined in both models as a “category to capture the ownership, administrative and other hierarchical relationships between ORGs and/or GPEs” (Linguistic Data Consortium, 2008; Linguistic Data Consortium, 2013c). Social and Affiliation Relation Differences The most evident discrepancy in relation annotation between the two models lies in the Social and Affiliation Relation Types and Subtypes. For social relations, ACE and ERE have three Subtypes with similar goals (Business, Family, Unspecified/Lasting-Personal) but ERE has an additional ‘Membership’ Subtype, as shown in Table 3. ACE addresses all ‘Membership’ relations in its Affiliation Type. ERE also includes the ‘Social.Role’ Subtype in order to address the TITLE entity type, which only applies to ERE. However,</context>
</contexts>
<marker>Consortium, 2008</marker>
<rawString>Linguistic Data Consortium. 2008. ACE (automatic content extraction) English annotation guidelines for relations. https://www.ldc.upenn.edu/ collaborations/past-projects/ace. Version 6.0 2008.01.07.</rawString>
</citation>
<citation valid="false">
<booktitle>Linguistic Data Consortium. 2013a. DEFT ERE annotation guidelines: Entities v1.1,</booktitle>
<pages>05--17</pages>
<marker></marker>
<rawString>Linguistic Data Consortium. 2013a. DEFT ERE annotation guidelines: Entities v1.1, 05.17.2013.</rawString>
</citation>
<citation valid="false">
<booktitle>Linguistic Data Consortium. 2013b. DEFT ERE annotation guidelines: Events v1.1.</booktitle>
<pages>05--17</pages>
<marker></marker>
<rawString>Linguistic Data Consortium. 2013b. DEFT ERE annotation guidelines: Events v1.1. 05.17.2013.</rawString>
</citation>
<citation valid="false">
<booktitle>Linguistic Data Consortium. 2013c. DEFT ERE annotation guidelines: Relations v1.1.</booktitle>
<pages>05--17</pages>
<marker></marker>
<rawString>Linguistic Data Consortium. 2013c. DEFT ERE annotation guidelines: Relations v1.1. 05.17.2013.</rawString>
</citation>
<citation valid="true">
<authors>
<author>Ken Litkowski</author>
</authors>
<title>Senseval-3 task: Automatic labeling of semantic roles.</title>
<date>2004</date>
<booktitle>Senseval-3: Third International Workshop on the Evaluation of Systems for the Semantic Analysis of Text,</booktitle>
<pages>9--12</pages>
<editor>In Rada Mihalcea and Phil Edmonds, editors,</editor>
<publisher>Association for Computational Linguistics.</publisher>
<location>Barcelona, Spain,</location>
<contexts>
<context position="25299" citStr="Litkowski, 2004" startWordPosition="4032" endWordPosition="4033"> result Frames tend to be much finer grained than ACE/ERE events, and are more numerous by an order of magnitude. The Berkeley FrameNet Project (Baker et al., 1998) was developed as a machine readable database of distinct frames and lexical units (words and multi-word constructions) that were known to trigger specific frames.1 FrameNet 1.5 includes 1020 identified frames and 11830 lexical units. One of the most widespread uses of FrameNet has been as a resource for Semantic Role Labeling (SRL) (Gildea and Jurafsky, 2002). FrameNet related SRL was promoted as a task by the SENSEVAL-3 workshop (Litkowski, 2004), and the SemEval-2007 workshop (Baker et al., 2007). (Das et al., 2010) is a current system for automatic FrameNet annotation. The relation and attribute types of TAC-KBP and the relation and event types in the ACE/ERE standards can be mapped to FrameNet frames. The mapping is complicated by two factors. The first is that FrameNet frames are generally more fine-grained than the ACE/ERE categories. As a result the mapping is sometimes one-to-many. For example, the ERE relation Af1This database is accessible via webpage (https: //framenet.icsi.berkeley.edu/fndrupal/) and as a collection of XML </context>
</contexts>
<marker>Litkowski, 2004</marker>
<rawString>Ken Litkowski. 2004. Senseval-3 task: Automatic labeling of semantic roles. In Rada Mihalcea and Phil Edmonds, editors, Senseval-3: Third International Workshop on the Evaluation of Systems for the Semantic Analysis of Text, pages 9–12, Barcelona, Spain, July. Association for Computational Linguistics.</rawString>
</citation>
<citation valid="true">
<authors>
<author>Paul McNamee</author>
<author>Hoa Trang Dang</author>
<author>Heather Simpson</author>
<author>Patrick Schone</author>
<author>Stephanie Strassel</author>
</authors>
<title>An evaluation of technologies for knowledge base population.</title>
<date>2010</date>
<booktitle>In Proceedings of LREC.</booktitle>
<contexts>
<context position="19634" citStr="McNamee et al., 2010" startWordPosition="3140" endWordPosition="3143">AC-KBP After the final ACE evaluation in 2008 there was interest in the community to form an evaluation explicitly focused on knowledge bases (KBs) created from the output of extraction systems. NIST had recently started the Text Analysis Conference series for related NLP tasks such as Recognizing Textual Entailment, Summarization, and Question Answering. In 2009 the first Knowledge Base Population track (TAC-KBP) was held featuring two initial tasks: (a) Entity Linking — linking entities to KB entities, and (b) Slot Filling — adding information to entity profiles that is missing from the KB (McNamee et al., 2010). Due to its generous license and large scale, a snapshot of English Wikipedia from late 2008 has been used as the reference KB in the TAC-KBP evaluations. 5.1 Slot Filling Overview Unlike ACE and ERE, Slot Filling does not have as its primary goal the annotation of text. Rather, the aim is to identify knowledge nuggets about a focal named entity using a fixed inventory of relations and attributes. For example, given a focal entity such as former Ukrainian prime minister Yulia Tymoshenko, the task is to identify attributes such as schools she attended, occupations, and immediate family members</context>
<context position="22977" citStr="McNamee et al., 2010" startWordPosition="3652" endWordPosition="3655">headquarters per:cities of residence per:statesorprovinces of residence per:countries of residence per:city of death per:stateorprovince of death per:country of death Table 5: Relation and attributes for PERs and ORGs. slot value equivalence becomes an issue - a system should be penalized for redundantly asserting that a person has four children named Tim, Beth, Timothy, and Elizabeth, or that a person is both a cardiologist and a doctor. Rather than explicitly modeling events, TACKBP created relations that capture events, more in line with the notion of Infobox filling or question answering (McNamee et al., 2010). For example, instead of a criminal event, there is a slot fill for charges brought against an entity. Instead of a founding event, there are slots like org:founded by (who) and org:date founded (when). Thus a statement that “Jobs is the founder and CEO of Apple” is every bit as useful for the org:founded by relation as “Jobs founded Apple in 1976.” even though the date is not included in the former sentence. 5.3 Additional tasks Starting in 2012 TAC-KBP introduced the “Cold Start” task, which is to literally produce a KB based on the Slot Filling schema. To date, Cold Start KBs have been bui</context>
</contexts>
<marker>McNamee, Dang, Simpson, Schone, Strassel, 2010</marker>
<rawString>Paul McNamee, Hoa Trang Dang, Heather Simpson, Patrick Schone, and Stephanie Strassel. 2010. An evaluation of technologies for knowledge base population. In Proceedings of LREC.</rawString>
</citation>
</citationList>
</algorithm>
</algorithms>
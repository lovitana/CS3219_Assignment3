<?xml version="1.0" encoding="UTF-8"?>
<algorithms version="110505">
<algorithm name="SectLabel" version="110505">
<variant no="0" confidence="0.000000">
<title confidence="0.993381">
IBM’s Belief Tracker: Results On Dialog State Tracking Challenge
Datasets
</title>
<author confidence="0.984247">
Rudolf Kadlec, Jindˇrich Libovick´y, Jan Macek, and Jan Kleindienst
</author>
<affiliation confidence="0.917469">
IBM Czech Republic
</affiliation>
<address confidence="0.635627">
V Parku 4, Prague 4
Czech Republic
</address>
<email confidence="0.982818">
{rudolf kadlec, jindrich libovicky, jmacek2, jankle}@cz.ibm.com
</email>
<sectionHeader confidence="0.99717" genericHeader="abstract">
Abstract
</sectionHeader>
<bodyText confidence="0.999918555555556">
Accurate dialog state tracking is crucial
for the design of an efficient spoken dialog
system. Until recently, quantitative com-
parison of different state tracking meth-
ods was difficult. However the 2013 Dia-
log State Tracking Challenge (DSTC) in-
troduced a common dataset and metrics
that allow to evaluate the performance of
trackers on a standardized task. In this pa-
per we present our belief tracker based on
the Hidden Information State (HIS) model
with an adjusted user model component.
Further, we report the results of our tracker
on test3 dataset from DSTC. Our tracker
is competitive with trackers submitted to
DSTC, even without training it achieves
the best results in L2 metrics and it per-
forms between second and third place in
accuracy. After adjusting the tracker using
the provided data it outperformed the other
submissions also in accuracy and yet im-
proved in L2. Additionally we present pre-
liminary results on another two datasets,
test1 and test2, used in the DSTC. Strong
performance in L2 metric means that our
tracker produces well calibrated hypothe-
ses probabilities.
</bodyText>
<sectionHeader confidence="0.999472" genericHeader="keywords">
1 Introduction
</sectionHeader>
<bodyText confidence="0.999948045454546">
Spoken dialog systems need to keep a represen-
tation of the dialog state and the user goal to
follow an efficient interaction path. The perfor-
mance of state-of-the-art speech recognition sys-
tems varies widely with domain and environment
with word accuracy rates ranging from less than
70% to 98%, which often leads to misinterpreta-
tion of the user’s intention. Dialog state tracking
methods need to cope with such error-prone auto-
matic speech recognition (ASR) and spoken lan-
guage understanding (SLU) outputs. Traditional
dialog systems use hand-crafted rules to select
from the SLU outputs based on their confidence
scores. Recently, several data-driven approaches
to dialog state tracking were developed as a part
of end-to-end spoken dialog systems. However,
specifics of these systems render comparison of
dialog state tracking methods difficult.
The Dialog State Tracking Challenge (DSTC)
(Williams et al., 2013) provides a shared testbed
with datasets and tools for evaluation of dialog
state tracking methods. It abstracts from subsys-
tems of end-to-end spoken dialog systems focus-
ing only on the dialog state estimation and track-
ing. It does so by providing datasets of ASR and
SLU outputs with reference transcriptions together
with annotation on the level of dialog acts.
In this paper we report initial encouraging re-
sults of our generative belief state tracker. We plan
to investigate discriminative approaches in the fu-
ture.
The rest of the paper continues as follows. In
the next section we formally introduce the dia-
log tracking task together with datasets used in
the DSTC. Then in Section 3 we discuss related
work. Section 4 describes the belief update equa-
tions of our tracker. After that we introduce the
design of our whole tracking system, especially
how we trained the system in a supervised setting
on the train dataset and in an unsupervised setting
on the test dataset. In Section 6 we show results
of our trackers, compare them to other DSTC par-
ticipants, and discuss the results in the context of
design choices and task characteristics.
</bodyText>
<sectionHeader confidence="0.887667" genericHeader="introduction">
2 DSTC Problem Definition, Datasets
and Metrics
</sectionHeader>
<bodyText confidence="0.9999392">
The task of the DSTC can be formally defined
as computing P(gt|u0:t, a0:t). That is, for each
time step t of the dialog compute the proba-
bility distribution over the user’s hidden goal g
given a sequence of SLU hypotheses from the
</bodyText>
<page confidence="0.987173">
10
</page>
<note confidence="0.9834405">
Proceedings of the of the EACL 2014 Workshop on Dialogue in Motion (DM), pages 10–18,
Gothenburg, Sweden, April 26-30 2014. c�2014 Association for Computational Linguistics
</note>
<table confidence="0.999148">
Dataset System # Annotated
train1a A 1013 yes
train1b A 1117 no
train1c A 9502 no
train2 A 643 yes
train3 B 688 yes
test1 A 715 for eval. only
test2 A 750 for eval. only
test3 B 1020 for eval. only
test4 C 438 for eval. only
</table>
<tableCaption confidence="0.999633">
Table 1: Datasets description. The System col-
</tableCaption>
<bodyText confidence="0.951595166666667">
umn shows what dialog system was used to col-
lect the dataset. The # column shows the number
of dialogs in the dataset. The last column informs
whether the ground truth annotation was provided
with the dataset.
System Dial. model SLU scores
</bodyText>
<equation confidence="0.991108333333333">
open (− inf, 0)
fixed (0,1)
open (0,1)
</equation>
<bodyText confidence="0.982589263157895">
Table 2: Main features of the dialog managers
used to collect the datasets. System A and C use
open dialog structure where the user can respond
with any combination of slots on any machine
question. System B uses a fixed dialog structure
where the user can respond only with the concept
the system expects.
beginning of the dialog up to the time t de-
noted as u0:t and a sequence of machine ac-
tions a0:t. It is assumed that the goal is fixed
through the dialog, unless the user is informed
that the requested goal does not exist. In DSTC
the user’s goal consist of nine slots: route,
from.desc, from.neighborhood, from.monument,
to.desc, to.neighborhood, to.monument, date,
time.
The dialog datasets in the DSTC are partitioned
into five training sets and four test sets. Details
and differences of the datasets are summarized in
Table 1 and 2. The datasets come from dialog sys-
tems deployed by three teams denoted as A, B and
C. All the training datasets were transcribed but
only three of them were annotated on the level of
dialog acts. The SLU confidence scores from sys-
tem B are relatively well calibrated, meaning that
confidences can be directly interpreted as proba-
bilities of observing the SLU hypothesis. Confi-
dence scores from the system A are not well cali-
brated as noted by several DSTC participants (Lee
and Eskenazi, 2013; Kim et al., 2013).
The evaluation protocol is briefly described in
Section 6. Its detailed description can be found in
(Williams et al., 2012), its evaluation in (Williams
et al., 2013).
In 2013, nine teams with 27 trackers partici-
pated in the challenge. The results of the best
trackers will be discussed together with the results
of our tracker later in Section 6.
</bodyText>
<sectionHeader confidence="0.99996" genericHeader="method">
3 Related Work
</sectionHeader>
<bodyText confidence="0.999756">
This section shortly reviews current approaches to
dialog state tracking. We divide the trackers into
two broad families of generative and discrimina-
tive methods.
</bodyText>
<sectionHeader confidence="0.527683" genericHeader="method">
3.1 Generative Methods
</sectionHeader>
<bodyText confidence="0.999966647058824">
The HIS model (Young et al., 2010) introduces an
approximative method of solving the belief track-
ing as an inference in a dynamic Bayesian network
with SLU hypotheses and machine actions as ob-
served variables and the estimate of the user’s goal
as a hidden variable. The HIS model was im-
plemented several times (Williams, 2010; Gaˇsi´c,
2011). Recent criticism of generative methods for
belief tracking brought more attention to the dis-
criminative methods (Williams, 2012b).
In the DSTC only few generative system partic-
ipated. Kim et al. (2013) implemented the HIS
model with additional discriminative rescoring,
Wang and Lemon (2013) introduced a very simple
model based on hand-crafted rules. Both of them
scored between the second and the fourth place in
the challenge.
</bodyText>
<subsectionHeader confidence="0.925396">
3.2 Discriminative Methods
</subsectionHeader>
<bodyText confidence="0.999550384615385">
As was previously mentioned, the discriminative
methods received more attention recently.
The overall winner of the DSTC (Lee and Es-
kenazi, 2013) used a maximum entropy model,
which they claim to be outperformed by bringing
more structure to the model by using the Condi-
tional Random Fields (Lee, 2013). The same type
of model is used also by Ren et al. (2013). Usage
of Deep Neural Networks was tested by Hender-
son et al. (2013).
ˇZilka et al. (2013) compare a discriminative
maximum entropy model and a generative method
based on approximate inference in a Bayesian net-
</bodyText>
<figure confidence="0.951655666666667">
A
B
C
</figure>
<page confidence="0.996496">
11
</page>
<bodyText confidence="0.987965">
work, with the discriminative model preforming
better.
</bodyText>
<sectionHeader confidence="0.992946" genericHeader="method">
4 Model
</sectionHeader>
<bodyText confidence="0.998137673469388">
Our model is an implementation of the HIS
model (Young et al., 2010). In HIS the belief state
is viewed as a probability distribution over all pos-
sible user’s goals. The belief state is represented
by a set of so-called partitions, which are sets of
user’s goals that are indistinguishable based on ac-
tions the system observes. It means the probability
mass assigned to a partition spreads to the user’s
goals in the partition proportionally to their’s prior
probabilities. The belief update is performed in
two steps.
Belief refinement ensures that for each user ac-
tion on the SLU n-best list and each partition all
goals in the partition are either consistent with the
user action or not. This step does not change the
belief state, it only enables the actual belief update
to be computed using the update equation (Eq. 1).
The partitions are organized in a tree structure
for which it holds that a child and a parent partition
are identical in some slots and complementary in
the remaining ones. This is ensured by the belief
refinement procedure. For each observed user ac-
tion and each partition it first checks whether all of
the hypotheses in the partition are either consistent
with the action or not. If they are not, it splits the
partition into two partitions with the parent-child
relationship. The inconsistent hypotheses remain
in the parent partition and the consistent ones are
moved to the child. The belief of the original par-
tition is distributed between the new ones in the
ratio of their priors.
To prevent an exponential increase in the num-
ber of partitions during the dialog, a partition re-
combination strategy can be used that removes the
less probable partition and moves their hypothe-
ses to different partitions. We perform partition
recombination at the end of each turn (Henderson
and Lemon, 2008), during the recombination low
probability partitions are merged with their parents
exactly as suggested by Williams (2010).
For the actual belief update the following stan-
dard update equation is used:
taken in turn t, u is a set of observed user actions,
P(u|u) is the score of action u in the SLU n-best
list u. In this definition P0(p) is a prior probabil-
ity of partition p; the prior might be either uniform
or estimated from the training data. The list u is
extended with an unobserved action u˜ whose prob-
ability is:
</bodyText>
<equation confidence="0.851127">
P(˜u|u) = 1 − � P(u|u). (2)
uEu��˜u}
P(u|p, a) in the update equation is the user
</equation>
<bodyText confidence="0.958979">
model, i.e. how likely the user is to take an ac-
tion u given that the last machine action was a and
user’s goal is represented by partition p.
</bodyText>
<equation confidence="0.856471166666667">
In our case:
A(p, u, a)
P(u|p, a) =
1] A(p&apos;, u, a) · size(p&apos;)
p&apos; E partitions
(3)
</equation>
<bodyText confidence="0.998135625">
where size(p) is the number of possible user’s
goals represented by p and A(p, u, a) is an indi-
cator function that evaluates to 1 when user’s ac-
tion u is compatible with the goal represented by
p given the last machine’s action was a, otherwise
A evaluates to 0.
A is defined in the following way, for every ob-
served action u E u \ {˜u}:
</bodyText>
<equation confidence="0.830451">
A(p, u, a) = A&apos;(p, u, a) (4)
</equation>
<bodyText confidence="0.9994505">
where A&apos; is a deterministic function that encodes
the meanings of user and machine actions for a
given partition. The rules expressed by A&apos; are for
example:
</bodyText>
<equation confidence="0.965307">
�
1 if v = w
ba : A&apos;(ps=w, inform(s = v), a) =
0 if v =�w
and
�
1 if v = w
A&apos;(ps=w, yes(), conf (s = v)) =
0 if v =�w
</equation>
<bodyText confidence="0.999629">
where ps=w represents a partition where slot s has
value w, inform(s = v) is user’s action assigning
value v to the slot s and conf (s = v) is machine
action requiring confirmation that slot s has value
</bodyText>
<equation confidence="0.841031666666667">
� P(u|u) · P(u|p,a) (1) v. For an unobserved action u˜ we define A as:
Pt+1(p) = k · Pt(p) ·
uEu
</equation>
<bodyText confidence="0.9987285">
where k is a normalization constant, Pt(p) is belief �A(p, ˜u, a) = (1 − A&apos;(p, u, a)). (5)
in partition p after turn t, a is the machine action uEuV˜u}
</bodyText>
<page confidence="0.947428">
12
</page>
<bodyText confidence="0.999974107142857">
This definition assumes that user’s unobserved
action u˜ uniformly supports each partition not sup-
ported by any of the observed user’s actions u.
A(p, ˜u, a) evaluates to 1 if none of user’s actions
support given partition, otherwise it evaluates to
0. This can be viewed as an axiom of our system,
alternatively we could assume that u˜ supports all
partitions, not only those not supported by any ob-
served action.
The key property of the update equations for-
mulated in this way is that the probability of a par-
tition representing a hypothesis that a user’s goal
was not mentioned in any of the SLU lists up to
the time t does not outweigh probability of ob-
served goals even though the prior probability of
unobserved hypothesis is usually orders of mag-
nitude higher than the probability of all observed
hypotheses. However, when two goals are indis-
tinguishable based on the SLU input then the ratio
of their probabilities will be exactly the ratio of
their priors.
Belief update equations are generic and in-
dependent of the internal structure of partitions.
When the tracker has to be adapted to a new dia-
log domain with the fixed goal the application de-
veloper needs to supply only a new definition of
A&apos; and partition splitting mechanism adjusted ac-
cording to A&apos;.
</bodyText>
<subsectionHeader confidence="0.970757">
4.1 Differences to the Original HIS
</subsectionHeader>
<bodyText confidence="0.999935">
The key difference between our HIS implementa-
tion and previous HIS systems is in the formula-
tion of the user model. Previous HIS-based sys-
tems (Young et al., 2010; Gaˇsi´c, 2011) factorize
the user model as:
</bodyText>
<equation confidence="0.994954">
Porig(u|p,a) = k &apos; P(T (u)|T (a)) &apos; M(u,p,a)
</equation>
<bodyText confidence="0.995402333333333">
where P(T (u)|T (a)) is a dialog act type bigram
model and M is a deterministic item matching
model that is similar to our A. Based on a descrip-
tion of the item matching model given in (Keizer
et al., 2008; Young et al., 2010; Gaˇsi´c, 2011) we
deduce that it evaluates to a constant c+ instead of
1 when the user action is consistent with the parti-
tion and to c_ instead of 0 otherwise. It holds that
0 &lt; c_ «c+ &lt; 1, e.g. c_ = 0.1 and c+ = 0.9.
In our tracker, we omit the dialog act type model
since it is not a mandatory component of the user
model and it can be added later. However, the
most important systematic difference between our
tracker and the original HIS formulation is that in-
stead of using a reduced user model, which would
</bodyText>
<table confidence="0.998685">
Par. Pt Porig Pours
t+1 t+1
pa 1/3 1/3 1/4
pb 1/3 1/3 1/4
pc 1/3 1/3 1/2
</table>
<tableCaption confidence="0.998716">
Table 3: Comparison of the effects of original HIS
</tableCaption>
<bodyText confidence="0.93773372972973">
user model and our modified user model. Initially
all partitions are equally likely. After performing
belief update using Eq. 1 the original model out-
puts probabilities in the column Porig
t+1 ,the column
Porig
t+1 shows results of our user model.
be Porig(u|p, a) = A(p, u, a) in the original HIS,
we use the formulation given in Eq. 3. The origi-
nal HIS does not use a concept of partition’s size
(size(p&apos;) in Eq. 3) that we need for the definition
of our user model.
We will illustrate the difference between these
two approaches on a minimalistic abstract exam-
ple. Suppose the belief space consists of three par-
titions pa, pb and pc, each of them having probabil-
ity of 1/3 and representing one possible user’s goal
(i.e. size(p*) = 1). There are two actions on the
SLU list: ua,b that is consistent only with pa and
pb (i.e. A&apos;(pa, ua,b, *) = 1), and uc that is con-
sistent only with pc. Both ua,b and uc are equally
probable, P(ua,b|u) = P(uc|u) = 1/2. Accord-
ing to one intuition pa and pb should share support
given to them by action ua,b, on the other hand pc
does not share the action uc with any other par-
tition. Thus after updating the probability using
Eq. 1 one would expect Pt+1(pc) to be higher than
Pt+1(pa). Now we can compare the output of our
model and the original HIS side by side as shown
in Table 3. The user model as formulated in the
original HIS leads to a new belief state where all
partitions are equally probable. However, accord-
ing to our modified user model partition pc is twice
as probable than pa or pb. This is, we argue, closer
to human intuition.
The update equation for a partition p in this sim-
plistic example is:
</bodyText>
<equation confidence="0.998596">
Pt+1(p) = k &apos; P(p) &apos; (P(ua,b|u) &apos; P(ua,b|p,*)+
P(uc|u) &apos; P(uc|p, *)).
</equation>
<bodyText confidence="0.6491215">
For every partition the original model would
output the same probability:
</bodyText>
<equation confidence="0.971598">
orig 1 (1 1 1 _ 1
Pt+1 (p) = k1 3 2 c+ + 2 c_ JI 3
</equation>
<page confidence="0.97304">
13
</page>
<bodyText confidence="0.9971255">
However our model gives the following equa-
tion for both pa and pb:
</bodyText>
<equation confidence="0.9783375">
�1 �
1
Pour = 1
t+1(px) = k2 2 · 1
1+1+2 1 · 0
3 1 4
</equation>
<bodyText confidence="0.995564">
where x E {a, b}. The impact of ua,b on px is di-
vided by a factor of 2 since it is shared by two par-
titions each representing one possible user goal.
For pc we have:
</bodyText>
<equation confidence="0.995769">
1 �1 �
Pour = 1
t+1(pc) = k2 2 · 0
1+1+2 1 · 1 2.
3 1
</equation>
<bodyText confidence="0.986125863636364">
This is how values in Table 3 were computed.
Another extension of the original HIS is how
we handle the unobserved action. To our knowl-
edge, the original HIS systems (Young et al., 2010;
Gaˇsi´c, 2011) do not deal with probability of unob-
served action; Williams (2010) presents a differ-
ent way of handling the unobserved action. We
provide unified way how to handle unrecognized
mass on the SLU list. In the original HIS model,
partition punobs not supported by any of the ob-
served actions obtains probability by M evalu-
ating to c− on each observed action. In our
model, punobs receives non-zero probability due to
A(punobs, ˜u, ∗) evaluating to 1 (see Eq. 5).
5 Tracker Design and its Variants
The previous section gave detailed description of
the update equations of our HIS based tracker.
This section presents an overall design of differ-
ent implemented tracker variants. We will discuss
how we use the bus route database and how we
perform supervised and unsupervised prior adap-
tation.
</bodyText>
<subsectionHeader confidence="0.996079">
5.1 Single Slot Tracking versus Joint
Tracking of Multiple Slots
</subsectionHeader>
<bodyText confidence="0.998598125">
An advantage of a HIS-based systems is that they
make it possible to track a joint probability distri-
bution over a user’s goal. This advantage is two-
fold. First, it enables usage of a joint prior, either
learned from training data or from the bus sched-
ule database. Second, tracking a joint distribution
makes it possible to use more information from
SLU hypotheses. We will illustrate this on an ex-
ample. Suppose that SLU is able to extract multi-
ple slots from one user’s utterance, in our example
it might be interpreted as:
inform(route=61,to.desc=cmu) 0.5
inform(route=60,to.desc=zoo) 0.4
And the machine explicitly confirms the route:
expl-confirm(route=61)
If the user’s response is interpreted as:
</bodyText>
<equation confidence="0.9959245">
negate() 0.8
affirm() 0.1
</equation>
<bodyText confidence="0.9992108">
Then the system tracking only marginal proba-
bilities over single slots will correctly consider
route 60 as being more probable but user’s nega-
tion will have no effect on marginal distribution of
to.desc. However, a system tracking the joint
distribution will now correctly rank zoo higher
than cmu. The disadvantage of tracking joint hy-
potheses is that it requires more computational re-
sources. A tracker tracking all slots independently
with a uniform prior is denoted as IBMindep, a
</bodyText>
<subsubsectionHeader confidence="0.538335">
uniform
</subsubsectionHeader>
<bodyText confidence="0.858849">
tracker tracking joint hypotheses with a uniform
prior as IBMjointly
uniform.
</bodyText>
<subsectionHeader confidence="0.99872">
5.2 Bus Schedule Database
</subsectionHeader>
<bodyText confidence="0.999965454545455">
Along with the dialog dataset DSTC organizers
provided a database with bus schedules for routes
in Pittsburgh area. We tested possibility to use re-
lation between bus routes and bus stops that can be
extracted from the database. First, we normalized
bus stop names as found in the SLU hypotheses
(e.g. by removing prepositions), in this way we
were able to match 98 percent of bus stops found
in the SLU to stops in the database.
An initial analysis of the data revealed that
only around 55% of route, from.desc, to.desc
hypotheses annotated by human annotators as a
ground truth were also found in the database.
This means that either callers were often asking
for non-existing combinations or the database was
mismatched.
Our tracker utilizing the database tracked joint
hypotheses for route, from.desc and to.desc slots
and hypotheses with combinations not found in the
database were penalized. The prior of a joint par-
tition pr,f,t, for a route r from destination f to des-
tination t, was computed as:
</bodyText>
<equation confidence="0.9643925">
P(pr,f,t) = Puniform · DB(r, f, t)
Where DB is
�
1 if (r, f, t) E database
DB(r, f , t) = 1
c otherwise
</equation>
<bodyText confidence="0.9589966">
where parameter c is a penalty constant for hy-
potheses not in the database. The value of c is
estimated by parameter search on the train data.
This tracker will be denoted as IBMjointly
db .
</bodyText>
<page confidence="0.994155">
14
</page>
<bodyText confidence="0.755034">
Test set 3
Schedule 2 Schedule 3
joint avg. joint avg.
acc. acc. L2 L2
</bodyText>
<figure confidence="0.961616058823529">
Team 6 (Lee and Eskenazi, 2013) .558 .680 .801 .597 .589 .823 .779 .367
Team 8 (unknown authors) .424 .616 .845 .559 .408 .716 .878 .422
Team 9 (Kim et al., 2013) .499 .657 .914 .710 .551 .828 .928 .461
Team 3 (ˇZilka et al., 2013) .464 .645 .831 .669 .528 .794 .734 .390
1-best baseline .448 .620 .865 .611 .492 .703 .839 .514
joint avg. joint avg.
acc. acc. L2 L2
.521 .654 .785 .575 .557 .804 .746 .344
IBMjointly
uniform
i
ep
I
BM
uniform
IBMjointly
db
</figure>
<table confidence="0.660582333333333">
.521 .654 .786 .576 .558 .806 .746 .343
.523 .657 .774 .564 .559 .806 .738 .339
IBMindep .563 .680 .694 .513 .609 .828 .644 .285
train-to-test .573 .689 .685 .505 .611 .834 .634 .279
IBMindep
unsup
</table>
<tableCaption confidence="0.768784">
Table 4: Results on the DSTC test set 3. Higher accuracy is better, whereas lower L2 score is better.
Numbers in bold highlight performance of the best tracker in the selected metric. The first four rows
</tableCaption>
<bodyText confidence="0.996643142857143">
show teams that performed the best in at least one of the selected metrics. For each team in each metric
we show performance of the best submitted tracker. This means that numbers in one row do not have to be
from a single tracker. It is an upper bound of the team’s performance. The fifth row shows performance
of a 1-best baseline tracker that always picks the SLU hypothesis with the top confidence. The rest are
different variants of our tracker. Here the bold numbers show where our tracker performed better than the
best tracker submitted to the DSTC. A light gray highlight of a cell denotes the overall best performance
in online setting, a dark gray highlight denotes the best performance while tracking offline.
</bodyText>
<subsectionHeader confidence="0.996896">
5.3 Priors Adaptation
</subsectionHeader>
<bodyText confidence="0.995880181818182">
We tested two variants of adjusting prior probabili-
ties of user goals. We estimated prior probabilities
as a mixture of the uniform probability and empir-
ical distribution estimated on the training data.
In the first experiment the empirical probabili-
ties were estimated using the annotation that was
available in the training data. We tracked the
slots independently because the empirical joint
distribution would be too sparse to generalize on
the test data. We used one prior distribution to
guide the selection of route hypotheses Prroute
and one shared distribution for possible destina-
tion names Prdesc. This distribution is trained on
data from both from and to destinations thus gain-
ing a more robust estimate compared to using two
separate distributions for from.desc and to.desc.
This tracker will be denoted as IBMindep
train-to-test.
In the second experiment we used the test data
without the ground truth labels to estimate the em-
pirical prior. We first ran the tracker with the uni-
form prior on the testing set and we used the out-
put hypotheses as a basis for the empirical distri-
bution. The prior of a hypothesis is proportional
to a sum of all tracker output scores for the hy-
pothesis. This scheme is called unsupervised prior
adaptation by Lee and Eskenazi (2013). Note that
the prior was computed on the test dataset. Thus
this technique is not directly applicable to a realis-
tic setting where the belief tracker has to produce a
belief for each dialog from the test set the first time
it sees it. This tracker will be called IBMindep
unsup.
</bodyText>
<sectionHeader confidence="0.999026" genericHeader="evaluation">
6 Evaluation
</sectionHeader>
<bodyText confidence="0.962421764705882">
We evaluated all our tracker variants on the DSTC
test3 dataset using the protocol designed for the
challenge participants. We also present initial re-
sults of the basic IBMindep and IBMjointly track-
uniform uniform
ers for test1 and test2 datasets. Several quanti-
ties were measured in three different schedules,
which defines, which moments of the dialog the
evaluation is performed. Here we report results
for schedule 2 and 3. Schedule 2 takes into ac-
count all turns when the relevant concept appeared
on user’s SLU list or was mentioned by the dialog
system. Schedule 3 evaluates belief at the end of
the dialog, i.e. at the moment when the queried
information is presented to the user.
We report accuracy, which is the ratio of dialogs
where the user goal was correctly estimated, and
</bodyText>
<page confidence="0.994906">
15
</page>
<bodyText confidence="0.9994387">
the L2 score, which is the Euclidean distance of
the vector of the resulting belief from a vector hav-
ing 1 for the correct hypothesis and 0s for the oth-
ers. For both of these the average values over all
tracked slot is reported as well as the value for the
joint hypotheses. The accuracy informs us how of-
ten the correct query to the database will be made.
The L2 score tells us how well-calibrated the re-
sults are, which can be important for disambigua-
tion and for statistical policy optimization.
</bodyText>
<subsectionHeader confidence="0.999765">
6.1 Method
</subsectionHeader>
<bodyText confidence="0.994181583333333">
We used one thousand partitions as the limit for
the number of tracked hypotheses. For each
tracker ran on the test set 3 we used only the top
five SLU hypotheses.
All parameters for mixing the empirical prior
probability with uniform distribution in trackers
IBMindep
train-to-test and IBMindep
unsup were estimated us-
ing 3-fold cross validation scheme on the training
data. The best parameter setting on the training
data was then used in evaluation on the test set.
</bodyText>
<figure confidence="0.38115225">
IBMjointly
uniform
IBMindep
uniform
</figure>
<tableCaption confidence="0.361511166666667">
Table 5: Preliminary results for schedule 3 on the
DSTC test set 1 of our two trackers compared to
three overall well performing teams. For teams 6
and 9 see Table 4, team 2 is (Wang and Lemon,
2013). The legend of the table is the same as in
Table 4.
</tableCaption>
<bodyText confidence="0.999076916666667">
Even though we concentrated mainly on test-
ing the tracker on dataset 3, we also ran it on the
datasets 1 and 2. For the datasets 1 and 2 we used
the single best SLU hypothesis from the live sys-
tem. Such hypothesis was assigned 99% probabil-
ity and the remaining 1% was left for the unob-
served action. For the datasets 1 and 2 a post hoc
computed SLU hypotheses are available in addi-
tion to the live data. In our experiments, using the
post hoc computed SLU hypotheses with normal-
ized confidence scores yielded worse results for
our tracking systems.
</bodyText>
<table confidence="0.990675090909091">
Test set 2
joint avg. joint avg.
acc. acc. L2 L2
Team 6 .526 .854 .885 .311
Team 9 .268 .748 1.098 .450
Team 2 .320 .764 1.148 .470
1-best baseline .141 .487 1.185 .648
IBMjointly .431 .789 .846 .316
uniform .413 .778 .875 .332
IBMindep
uniform
</table>
<tableCaption confidence="0.813323333333333">
Table 6: Preliminary results for schedule 3 on the
DSTC test set 2. For teams see Tables 4 and 5.
The legend of the table is the same as in Table 4.
</tableCaption>
<subsectionHeader confidence="0.94438">
6.2 Results
</subsectionHeader>
<bodyText confidence="0.972894818181818">
Results of our trackers on the DSTC dataset 3 are
summarized in Table 4. Preliminary results of
the trackers on datasets 1 and 2 whose confidence
scores are not that well calibrated are shown in Ta-
bles 5 and 6. The running time of the trackers
was on average below 0.05 seconds per turn1. The
only exception is IBMjointly
db that executes plenty of
database queries. Although we did not focus on
the computational performance optimization most
of the trackers are suitable for on-line use.
</bodyText>
<subsectionHeader confidence="0.987385">
6.3 Discussion
</subsectionHeader>
<bodyText confidence="0.965414851851852">
Quantitative Comparison to DSTC Trackers.
First let us discuss results of our trackers on test 3
(Table 4). Here both basic variants of the tracker
IBMindep
uniform and IBMjointly
uniform perform almost identi-
cally. This is because test 3 uses fixed dialog flow
as discussed in Section 2, minor differences in per-
and IBMjointlye
uniform uniform
caused only by numerical issues. The trackers are
around the third place in accuracy. In joint L2 met-
rics they outperform the best tracker in DSTC sub-
mitted by Team 6 (Lee and Eskenazi, 2013).
Tracker utilizing database IBNfdbntly does not
show any significant improvement over the same
tracker without database-based prior IBMuni fly
We hypothesize that this is because of the fact that
people frequently asked for non-existing combina-
tions of routes and stops, which were penalized for
not being in the database, as discussed in Sec. 5.2.
Next follow the results of tracker IBM train-to-test
that learns priors for single slots on training
dataset and uses them while inferring user’s goal
on the test set. In test set 3 priors enhanced
per-
formance
</bodyText>
<figure confidence="0.963627333333333">
i
d
p
between IBM
n
e
</figure>
<footnote confidence="0.574998">
1On one core of Intel Xeon CPU E3-1230 V2, 3.30GHz,
with memory limitation of 1GB.
</footnote>
<figure confidence="0.992667777777778">
Test set 1
joint
acc.
avg.
acc.
joint
L2
avg.
L2
Team 6
Team 9
Team 2
.364 .862 .989 .278
.225 .789 1.154 .354
.206 .777 1.234 .409
1-best baseline .138 .626 1.220 .530
.332 .813 .992 .282
.331 .804 1.010 .304
</figure>
<page confidence="0.758138">
16
</page>
<figureCaption confidence="0.5309335">
tracker’s performance in all metrics and the tracker
outperformed all DSTC trackers.
</figureCaption>
<bodyText confidence="0.985704987179487">
Interesting results were achieved by IBMindep
unsup
that performed even better than the IBMindep train-to-test.
It uses a prior trained on the test set by running the
tracker with a uniform prior. The tracker was run
for three iterations each time using output of the
previous iteration as a new prior.
After running the experiments with the top 5
SLU hypotheses, we performed an experiment that
investigated influence of n-best list length on the
tracker’s accuracy. We evaluated five system vari-
ants that received 1, 2, 3, 4 and 5 best SLU hy-
potheses. The overall trend was that initially per-
formance increased as more SLU hypotheses were
provided however then performance started de-
creasing. The 3-best variant achieved about 1.5%
increase in joint accuracy compared to the 1-best.
However, when using more than 3 best hypothe-
ses, the performance slightly decreased. For in-
stance, IBMindep
uniform using 1-best hypothesis per-
formed comparable to the 5-best configuration.
Similar behavior of generative systems assuming
observation independence has already been ob-
served in different domains (Vail et al., 2007).
Based on these results we deduce two conclu-
sions. First, strong performance of IBMindep
uniform
1-best system compared to the 1-best baseline sys-
tem suggests that the main added value of our
tracker in this domain is in the aggregation of ob-
servations from multiple time steps, not in track-
ing multiple hypotheses from one turn. Sec-
ond, we attribute the effect of decreasing accu-
racy to the correlation of ASR errors from con-
secutive dialog turns. As noted by Williams
(2012b), correlated ASR errors violate the as-
sumption of observation independence that is as-
sumed by HIS. Extending the user model with
an auto-regressive component, that is with depen-
dence on observations from the previous time step
(i.e. P(ut|ut−1, p, a)), might help to tackle this
problem in generative models (Wellekens, 1987).
To summarize the results on test set 3, even
without any prior adaptation on the data our
tracker is competitive with the best submissions
to DSTC. After incorporating prior knowledge it
outperforms all submitted trackers.
On test set 1 and test set 2 (see Tables 5 and 6)
the trackers perform second in accuracy. In L2
metrics the trackers are competitive with the best
tracker in DSTC submitted by Team 6 and they
outperform it in one out of four cases. It is inter-
esting that our basic strategy that ignores live SLU
scores performed that strong.
However, on test 1 and test 2, which make it
possible to input multiple slots in one user utter-
ance, IBMjointly
uniform outperforms IBMindep
uniform, both in
accuracy and L2. We hypothesize that this is be-
cause of effect of tracking joint distributions de-
scribed in Section 5.1.
Qualitative Comparison to DSTC Trackers.
Compared to another HIS-based system (Kim et
al., 2013) participating in the DSTC, our imple-
mentation does not suffer from the problem of as-
signing high probability to the hypothesis that the
user goal was not observed so far. This might be
due to our modified user model. Therefore our im-
plementation does not need a final transformation
of belief scores as reported by Kim et al. (2013).
Additionally, our implementation does not
exhibit the forgetting behavior as experienced
by ˇZilka et al. (2013). Forgetting is undesirable
given the validity of assumption that the user’s
goal remains fixed in the whole dialog, which is
the case of DSTC bus schedule domains.
</bodyText>
<sectionHeader confidence="0.99901" genericHeader="conclusions">
7 Conclusion
</sectionHeader>
<bodyText confidence="0.999995736842105">
Although the use of generative trackers was re-
cently criticized by Williams (2012a), our re-
sults show that at least in some metrics (e.g. L2
metrics on dataset 3) a generative tracker can
outperform the best state-of-the-art discriminative
tracker (Lee and Eskenazi, 2013). Even though
we agree that the discriminative approach might be
more promising, it seems that in general there are
conditions where generative models learn faster
than discriminative models (Ng and Jordan, 2001).
Thus it might be beneficial to use a generative
tracker for a newly deployed dialog system with
only a few training dialogs available and switch to
a discriminative model once enough training data
from an already running system is collected. En-
semble trackers incorporating both generative and
discriminative models as used by Lee and Eske-
nazi (2013) might also be an interesting direction
for future research.
</bodyText>
<sectionHeader confidence="0.980236" genericHeader="acknowledgments">
Acknowledgment
</sectionHeader>
<bodyText confidence="0.99965175">
We would like to thank Jiˇr´ı Havelka for his valu-
able comments on a draft of this paper. This work
was partially funded by the GetHomeSafe project
(EU 7th Framework STREP project No. 288667).
</bodyText>
<page confidence="0.998693">
17
</page>
<sectionHeader confidence="0.998333" genericHeader="references">
References
</sectionHeader>
<reference confidence="0.999913741935484">
Milica Ga&amp;quot;si´c. 2011. Statistical Dialogue Modelling.
PhD thesis, University of Cambridge.
James Henderson and Oliver Lemon. 2008. Mixture
Model POMDPs for Efficient Handling of Uncer-
tainty in Dialogue Management. In Proc ACL-HLT,
pages 73–76.
Matthew Henderson, Blaise Thomson, and Steve
Young. 2013. Deep neural network approach for
the dialog state tracking challenge. In Proceedings
of the SIGDIAL 2013 Conference, pages 467–471,
Metz, France, August. Association for Computa-
tional Linguistics.
Simon Keizer, Milica Ga&amp;quot;si´c, Franc¸ois Mairesse, Blaise
Thomson, Kai Yu, and Steve Young. 2008. Mod-
elling user behaviour in the his-pomdp dialogue
manager. In Spoken Language Technology Work-
shop, 2008. SLT 2008. IEEE, pages 121–124. IEEE.
Daejoong Kim, Jaedeug Choi Choi, Kee-Eung Kim,
Jungsu Lee, and Jinho Sohn. 2013. Engineering sta-
tistical dialog state trackers: A case study on dstc.
In Proceedings of the SIGDIAL 2013 Conference,
pages 462–466, Metz, France, August. Association
for Computational Linguistics.
Sungjin Lee and Maxine Eskenazi. 2013. Recipe for
building robust spoken dialog state trackers: Dialog
state tracking challenge system description. In Pro-
ceedings of the SIGDIAL 2013 Conference, pages
414–422, Metz, France, August. Association for
Computational Linguistics.
Sungjin Lee. 2013. Structured Discriminative Model
For Dialog State Tracking. In Proceedings of the
SIGDIAL 2013 Conference, pages 442–451, Metz,
France, August. Association for Computational Lin-
guistics.
Andrew Ng and Michael Jordan. 2001. On discrim-
inative vs. generative classifiers: A comparison of
logistic regression and naive bayes. Neural Infor-
mation Processing Systems, pages 841–848.
Hang Ren, Weiqun Xu, Yan Zhang, and Yonghong Yan.
2013. Dialog state tracking using conditional ran-
dom fields. In Proceedings of the SIGDIAL 2013
Conference, pages 457–461, Metz, France, August.
Association for Computational Linguistics.
Douglas L Vail, Manuela M Veloso, and John D Laf-
ferty. 2007. Conditional Random Fields for Activ-
ity Recognition Categories and Subject Descriptors.
In Proceedings of the 6th International Joint Confer-
ence on Autonomous Agents and Multiagent systems
(AAMAS 2007).
Zhuoran Wang and Oliver Lemon. 2013. A sim-
ple and generic belief tracking mechanism for the
dialog state tracking challenge: On the believabil-
ity of observed information. In Proceedings of the
SIGDIAL 2013 Conference, pages 423–432, Metz,
France, August. Association for Computational Lin-
guistics.
Christian Wellekens. 1987. Explicit time correlation
in hidden markov models for speech recognition. In
Acoustics, Speech, and Signal Processing, IEEE In-
ternational Conference on ICASSP’87., volume 12,
pages 384–386. IEEE.
Jason D Williams, Antoine Raux, Deepak Ramachan-
dran, and Alan Black. 2012. Dialog state tracking
challenge handbook.
Jason Williams, Antoine Raux, Deepak Ramachan-
dran, and Alan Black. 2013. The Dialog State
Tracking Challenge. In Proceedings of the SIGDIAL
2013 Conference, pages 404–413, Metz, France,
August. Association for Computational Linguistics.
Jason D. Williams. 2010. Incremental partition re-
combination for efficient tracking of multiple dialog
states. In ICASSP, pages 5382–5385.
Jason D. Williams. 2012a. Challenges and Opportuni-
ties for State Tracking in Statistical Spoken Dialog
Systems: Results From Two Public Deployments.
IEEE Journal of Selected Topics in Signal Process-
ing, 6(8):959–970, December.
Jason D Williams. 2012b. A critical analysis of
two statistical spoken dialog systems in public use.
In Spoken Language Technology Workshop (SLT),
2012 IEEE, pages 55–60. IEEE.
Steve Young, Milica Ga&amp;quot;si´c, Simon Keizer, Franc¸ois
Mairesse, Jost Schatzmann, Blaise Thomson, and
Kai Yu. 2010. The Hidden Information State model:
A practical framework for POMDP-based spoken
dialogue management. Computer Speech &amp; Lan-
guage, 24(2):150–174, April.
Luk´a&amp;quot;s &amp;quot;Zilka, David Marek, Mat&amp;quot;ej Korvas, and Filip
Jur&amp;quot;c´ı&amp;quot;cek. 2013. Comparison of bayesian discrim-
inative and generative models for dialogue state
tracking. In Proceedings of the SIGDIAL 2013 Con-
ference, pages 452–456, Metz, France, August. As-
sociation for Computational Linguistics.
</reference>
<page confidence="0.999292">
18
</page>
</variant>
</algorithm>
<algorithm name="ParsHed" version="110505">
<variant no="0" confidence="0.151089">
<title confidence="0.9982695">IBM’s Belief Tracker: Results On Dialog State Tracking Challenge Datasets</title>
<author confidence="0.999443">Rudolf Kadlec</author>
<author confidence="0.999443">Jindˇrich Libovick´y</author>
<author confidence="0.999443">Jan Macek</author>
<author confidence="0.999443">Jan</author>
<affiliation confidence="0.924524">IBM Czech</affiliation>
<address confidence="0.468583">V Parku 4, Prague Czech</address>
<email confidence="0.424137">kadlec,jindrichlibovicky,jmacek2,</email>
<abstract confidence="0.999153">Accurate dialog state tracking is crucial for the design of an efficient spoken dialog system. Until recently, quantitative comparison of different state tracking methods was difficult. However the 2013 Dialog State Tracking Challenge (DSTC) introduced a common dataset and metrics that allow to evaluate the performance of trackers on a standardized task. In this paper we present our belief tracker based on the Hidden Information State (HIS) model with an adjusted user model component. Further, we report the results of our tracker on test3 dataset from DSTC. Our tracker is competitive with trackers submitted to DSTC, even without training it achieves the best results in L2 metrics and it performs between second and third place in accuracy. After adjusting the tracker using the provided data it outperformed the other submissions also in accuracy and yet improved in L2. Additionally we present preliminary results on another two datasets, test1 and test2, used in the DSTC. Strong performance in L2 metric means that our tracker produces well calibrated hypotheses probabilities.</abstract>
</variant>
</algorithm>
<algorithm name="ParsCit" version="110505">
<citationList>
<citation valid="true">
<authors>
<author>Milica Gasi´c</author>
</authors>
<title>Statistical Dialogue Modelling.</title>
<date>2011</date>
<tech>PhD thesis,</tech>
<institution>University of Cambridge.</institution>
<marker>Gasi´c, 2011</marker>
<rawString>Milica Ga&amp;quot;si´c. 2011. Statistical Dialogue Modelling. PhD thesis, University of Cambridge.</rawString>
</citation>
<citation valid="true">
<authors>
<author>James Henderson</author>
<author>Oliver Lemon</author>
</authors>
<title>Mixture Model POMDPs for Efficient Handling of Uncertainty in Dialogue Management.</title>
<date>2008</date>
<booktitle>In Proc ACL-HLT,</booktitle>
<pages>73--76</pages>
<contexts>
<context position="9615" citStr="Henderson and Lemon, 2008" startWordPosition="1592" endWordPosition="1595">on or not. If they are not, it splits the partition into two partitions with the parent-child relationship. The inconsistent hypotheses remain in the parent partition and the consistent ones are moved to the child. The belief of the original partition is distributed between the new ones in the ratio of their priors. To prevent an exponential increase in the number of partitions during the dialog, a partition recombination strategy can be used that removes the less probable partition and moves their hypotheses to different partitions. We perform partition recombination at the end of each turn (Henderson and Lemon, 2008), during the recombination low probability partitions are merged with their parents exactly as suggested by Williams (2010). For the actual belief update the following standard update equation is used: taken in turn t, u is a set of observed user actions, P(u|u) is the score of action u in the SLU n-best list u. In this definition P0(p) is a prior probability of partition p; the prior might be either uniform or estimated from the training data. The list u is extended with an unobserved action u˜ whose probability is: P(˜u|u) = 1 − � P(u|u). (2) uEu��˜u} P(u|p, a) in the update equation is the </context>
</contexts>
<marker>Henderson, Lemon, 2008</marker>
<rawString>James Henderson and Oliver Lemon. 2008. Mixture Model POMDPs for Efficient Handling of Uncertainty in Dialogue Management. In Proc ACL-HLT, pages 73–76.</rawString>
</citation>
<citation valid="true">
<authors>
<author>Matthew Henderson</author>
<author>Blaise Thomson</author>
<author>Steve Young</author>
</authors>
<title>Deep neural network approach for the dialog state tracking challenge.</title>
<date>2013</date>
<booktitle>In Proceedings of the SIGDIAL 2013 Conference,</booktitle>
<pages>467--471</pages>
<publisher>Association</publisher>
<institution>for Computational Linguistics.</institution>
<location>Metz, France,</location>
<contexts>
<context position="7577" citStr="Henderson et al. (2013)" startWordPosition="1247" endWordPosition="1251">oring, Wang and Lemon (2013) introduced a very simple model based on hand-crafted rules. Both of them scored between the second and the fourth place in the challenge. 3.2 Discriminative Methods As was previously mentioned, the discriminative methods received more attention recently. The overall winner of the DSTC (Lee and Eskenazi, 2013) used a maximum entropy model, which they claim to be outperformed by bringing more structure to the model by using the Conditional Random Fields (Lee, 2013). The same type of model is used also by Ren et al. (2013). Usage of Deep Neural Networks was tested by Henderson et al. (2013). ˇZilka et al. (2013) compare a discriminative maximum entropy model and a generative method based on approximate inference in a Bayesian netA B C 11 work, with the discriminative model preforming better. 4 Model Our model is an implementation of the HIS model (Young et al., 2010). In HIS the belief state is viewed as a probability distribution over all possible user’s goals. The belief state is represented by a set of so-called partitions, which are sets of user’s goals that are indistinguishable based on actions the system observes. It means the probability mass assigned to a partition spre</context>
</contexts>
<marker>Henderson, Thomson, Young, 2013</marker>
<rawString>Matthew Henderson, Blaise Thomson, and Steve Young. 2013. Deep neural network approach for the dialog state tracking challenge. In Proceedings of the SIGDIAL 2013 Conference, pages 467–471, Metz, France, August. Association for Computational Linguistics.</rawString>
</citation>
<citation valid="true">
<authors>
<author>Simon Keizer</author>
<author>Milica Gasi´c</author>
<author>Franc¸ois Mairesse</author>
<author>Blaise Thomson</author>
<author>Kai Yu</author>
<author>Steve Young</author>
</authors>
<title>Modelling user behaviour in the his-pomdp dialogue manager.</title>
<date>2008</date>
<booktitle>In Spoken Language Technology Workshop,</booktitle>
<pages>121--124</pages>
<publisher>IEEE,</publisher>
<marker>Keizer, Gasi´c, Mairesse, Thomson, Yu, Young, 2008</marker>
<rawString>Simon Keizer, Milica Ga&amp;quot;si´c, Franc¸ois Mairesse, Blaise Thomson, Kai Yu, and Steve Young. 2008. Modelling user behaviour in the his-pomdp dialogue manager. In Spoken Language Technology Workshop, 2008. SLT 2008. IEEE, pages 121–124. IEEE.</rawString>
</citation>
<citation valid="true">
<authors>
<author>Daejoong Kim</author>
<author>Jaedeug Choi Choi</author>
<author>Kee-Eung Kim</author>
<author>Jungsu Lee</author>
<author>Jinho Sohn</author>
</authors>
<title>Engineering statistical dialog state trackers: A case study on dstc.</title>
<date>2013</date>
<booktitle>In Proceedings of the SIGDIAL 2013 Conference,</booktitle>
<pages>462--466</pages>
<publisher>Association</publisher>
<institution>for Computational Linguistics.</institution>
<location>Metz, France,</location>
<contexts>
<context position="5796" citStr="Kim et al., 2013" startWordPosition="957" endWordPosition="960">training sets and four test sets. Details and differences of the datasets are summarized in Table 1 and 2. The datasets come from dialog systems deployed by three teams denoted as A, B and C. All the training datasets were transcribed but only three of them were annotated on the level of dialog acts. The SLU confidence scores from system B are relatively well calibrated, meaning that confidences can be directly interpreted as probabilities of observing the SLU hypothesis. Confidence scores from the system A are not well calibrated as noted by several DSTC participants (Lee and Eskenazi, 2013; Kim et al., 2013). The evaluation protocol is briefly described in Section 6. Its detailed description can be found in (Williams et al., 2012), its evaluation in (Williams et al., 2013). In 2013, nine teams with 27 trackers participated in the challenge. The results of the best trackers will be discussed together with the results of our tracker later in Section 6. 3 Related Work This section shortly reviews current approaches to dialog state tracking. We divide the trackers into two broad families of generative and discriminative methods. 3.1 Generative Methods The HIS model (Young et al., 2010) introduces an </context>
<context position="20069" citStr="Kim et al., 2013" startWordPosition="3533" endWordPosition="3536">partition pr,f,t, for a route r from destination f to destination t, was computed as: P(pr,f,t) = Puniform · DB(r, f, t) Where DB is � 1 if (r, f, t) E database DB(r, f , t) = 1 c otherwise where parameter c is a penalty constant for hypotheses not in the database. The value of c is estimated by parameter search on the train data. This tracker will be denoted as IBMjointly db . 14 Test set 3 Schedule 2 Schedule 3 joint avg. joint avg. acc. acc. L2 L2 Team 6 (Lee and Eskenazi, 2013) .558 .680 .801 .597 .589 .823 .779 .367 Team 8 (unknown authors) .424 .616 .845 .559 .408 .716 .878 .422 Team 9 (Kim et al., 2013) .499 .657 .914 .710 .551 .828 .928 .461 Team 3 (ˇZilka et al., 2013) .464 .645 .831 .669 .528 .794 .734 .390 1-best baseline .448 .620 .865 .611 .492 .703 .839 .514 joint avg. joint avg. acc. acc. L2 L2 .521 .654 .785 .575 .557 .804 .746 .344 IBMjointly uniform i ep I BM uniform IBMjointly db .521 .654 .786 .576 .558 .806 .746 .343 .523 .657 .774 .564 .559 .806 .738 .339 IBMindep .563 .680 .694 .513 .609 .828 .644 .285 train-to-test .573 .689 .685 .505 .611 .834 .634 .279 IBMindep unsup Table 4: Results on the DSTC test set 3. Higher accuracy is better, whereas lower L2 score is better. Numbe</context>
<context position="30942" citStr="Kim et al., 2013" startWordPosition="5414" endWordPosition="5417">racy. In L2 metrics the trackers are competitive with the best tracker in DSTC submitted by Team 6 and they outperform it in one out of four cases. It is interesting that our basic strategy that ignores live SLU scores performed that strong. However, on test 1 and test 2, which make it possible to input multiple slots in one user utterance, IBMjointly uniform outperforms IBMindep uniform, both in accuracy and L2. We hypothesize that this is because of effect of tracking joint distributions described in Section 5.1. Qualitative Comparison to DSTC Trackers. Compared to another HIS-based system (Kim et al., 2013) participating in the DSTC, our implementation does not suffer from the problem of assigning high probability to the hypothesis that the user goal was not observed so far. This might be due to our modified user model. Therefore our implementation does not need a final transformation of belief scores as reported by Kim et al. (2013). Additionally, our implementation does not exhibit the forgetting behavior as experienced by ˇZilka et al. (2013). Forgetting is undesirable given the validity of assumption that the user’s goal remains fixed in the whole dialog, which is the case of DSTC bus schedu</context>
</contexts>
<marker>Kim, Choi, Kim, Lee, Sohn, 2013</marker>
<rawString>Daejoong Kim, Jaedeug Choi Choi, Kee-Eung Kim, Jungsu Lee, and Jinho Sohn. 2013. Engineering statistical dialog state trackers: A case study on dstc. In Proceedings of the SIGDIAL 2013 Conference, pages 462–466, Metz, France, August. Association for Computational Linguistics.</rawString>
</citation>
<citation valid="true">
<authors>
<author>Sungjin Lee</author>
<author>Maxine Eskenazi</author>
</authors>
<title>Recipe for building robust spoken dialog state trackers: Dialog state tracking challenge system description.</title>
<date>2013</date>
<booktitle>In Proceedings of the SIGDIAL 2013 Conference,</booktitle>
<pages>414--422</pages>
<publisher>Association</publisher>
<institution>for Computational Linguistics.</institution>
<location>Metz, France,</location>
<contexts>
<context position="5777" citStr="Lee and Eskenazi, 2013" startWordPosition="953" endWordPosition="956">e partitioned into five training sets and four test sets. Details and differences of the datasets are summarized in Table 1 and 2. The datasets come from dialog systems deployed by three teams denoted as A, B and C. All the training datasets were transcribed but only three of them were annotated on the level of dialog acts. The SLU confidence scores from system B are relatively well calibrated, meaning that confidences can be directly interpreted as probabilities of observing the SLU hypothesis. Confidence scores from the system A are not well calibrated as noted by several DSTC participants (Lee and Eskenazi, 2013; Kim et al., 2013). The evaluation protocol is briefly described in Section 6. Its detailed description can be found in (Williams et al., 2012), its evaluation in (Williams et al., 2013). In 2013, nine teams with 27 trackers participated in the challenge. The results of the best trackers will be discussed together with the results of our tracker later in Section 6. 3 Related Work This section shortly reviews current approaches to dialog state tracking. We divide the trackers into two broad families of generative and discriminative methods. 3.1 Generative Methods The HIS model (Young et al., 2</context>
<context position="7293" citStr="Lee and Eskenazi, 2013" startWordPosition="1194" endWordPosition="1198">010; Gaˇsi´c, 2011). Recent criticism of generative methods for belief tracking brought more attention to the discriminative methods (Williams, 2012b). In the DSTC only few generative system participated. Kim et al. (2013) implemented the HIS model with additional discriminative rescoring, Wang and Lemon (2013) introduced a very simple model based on hand-crafted rules. Both of them scored between the second and the fourth place in the challenge. 3.2 Discriminative Methods As was previously mentioned, the discriminative methods received more attention recently. The overall winner of the DSTC (Lee and Eskenazi, 2013) used a maximum entropy model, which they claim to be outperformed by bringing more structure to the model by using the Conditional Random Fields (Lee, 2013). The same type of model is used also by Ren et al. (2013). Usage of Deep Neural Networks was tested by Henderson et al. (2013). ˇZilka et al. (2013) compare a discriminative maximum entropy model and a generative method based on approximate inference in a Bayesian netA B C 11 work, with the discriminative model preforming better. 4 Model Our model is an implementation of the HIS model (Young et al., 2010). In HIS the belief state is viewe</context>
<context position="19938" citStr="Lee and Eskenazi, 2013" startWordPosition="3507" endWordPosition="3510">s for route, from.desc and to.desc slots and hypotheses with combinations not found in the database were penalized. The prior of a joint partition pr,f,t, for a route r from destination f to destination t, was computed as: P(pr,f,t) = Puniform · DB(r, f, t) Where DB is � 1 if (r, f, t) E database DB(r, f , t) = 1 c otherwise where parameter c is a penalty constant for hypotheses not in the database. The value of c is estimated by parameter search on the train data. This tracker will be denoted as IBMjointly db . 14 Test set 3 Schedule 2 Schedule 3 joint avg. joint avg. acc. acc. L2 L2 Team 6 (Lee and Eskenazi, 2013) .558 .680 .801 .597 .589 .823 .779 .367 Team 8 (unknown authors) .424 .616 .845 .559 .408 .716 .878 .422 Team 9 (Kim et al., 2013) .499 .657 .914 .710 .551 .828 .928 .461 Team 3 (ˇZilka et al., 2013) .464 .645 .831 .669 .528 .794 .734 .390 1-best baseline .448 .620 .865 .611 .492 .703 .839 .514 joint avg. joint avg. acc. acc. L2 L2 .521 .654 .785 .575 .557 .804 .746 .344 IBMjointly uniform i ep I BM uniform IBMjointly db .521 .654 .786 .576 .558 .806 .746 .343 .523 .657 .774 .564 .559 .806 .738 .339 IBMindep .563 .680 .694 .513 .609 .828 .644 .285 train-to-test .573 .689 .685 .505 .611 .834 .</context>
<context position="22789" citStr="Lee and Eskenazi (2013)" startWordPosition="4002" endWordPosition="4005">from and to destinations thus gaining a more robust estimate compared to using two separate distributions for from.desc and to.desc. This tracker will be denoted as IBMindep train-to-test. In the second experiment we used the test data without the ground truth labels to estimate the empirical prior. We first ran the tracker with the uniform prior on the testing set and we used the output hypotheses as a basis for the empirical distribution. The prior of a hypothesis is proportional to a sum of all tracker output scores for the hypothesis. This scheme is called unsupervised prior adaptation by Lee and Eskenazi (2013). Note that the prior was computed on the test dataset. Thus this technique is not directly applicable to a realistic setting where the belief tracker has to produce a belief for each dialog from the test set the first time it sees it. This tracker will be called IBMindep unsup. 6 Evaluation We evaluated all our tracker variants on the DSTC test3 dataset using the protocol designed for the challenge participants. We also present initial results of the basic IBMindep and IBMjointly trackuniform uniform ers for test1 and test2 datasets. Several quantities were measured in three different schedul</context>
<context position="27136" citStr="Lee and Eskenazi, 2013" startWordPosition="4777" endWordPosition="4780">ormance optimization most of the trackers are suitable for on-line use. 6.3 Discussion Quantitative Comparison to DSTC Trackers. First let us discuss results of our trackers on test 3 (Table 4). Here both basic variants of the tracker IBMindep uniform and IBMjointly uniform perform almost identically. This is because test 3 uses fixed dialog flow as discussed in Section 2, minor differences in perand IBMjointlye uniform uniform caused only by numerical issues. The trackers are around the third place in accuracy. In joint L2 metrics they outperform the best tracker in DSTC submitted by Team 6 (Lee and Eskenazi, 2013). Tracker utilizing database IBNfdbntly does not show any significant improvement over the same tracker without database-based prior IBMuni fly We hypothesize that this is because of the fact that people frequently asked for non-existing combinations of routes and stops, which were penalized for not being in the database, as discussed in Sec. 5.2. Next follow the results of tracker IBM train-to-test that learns priors for single slots on training dataset and uses them while inferring user’s goal on the test set. In test set 3 priors enhanced performance i d p between IBM n e 1On one core of In</context>
<context position="31839" citStr="Lee and Eskenazi, 2013" startWordPosition="5561" endWordPosition="5564">formation of belief scores as reported by Kim et al. (2013). Additionally, our implementation does not exhibit the forgetting behavior as experienced by ˇZilka et al. (2013). Forgetting is undesirable given the validity of assumption that the user’s goal remains fixed in the whole dialog, which is the case of DSTC bus schedule domains. 7 Conclusion Although the use of generative trackers was recently criticized by Williams (2012a), our results show that at least in some metrics (e.g. L2 metrics on dataset 3) a generative tracker can outperform the best state-of-the-art discriminative tracker (Lee and Eskenazi, 2013). Even though we agree that the discriminative approach might be more promising, it seems that in general there are conditions where generative models learn faster than discriminative models (Ng and Jordan, 2001). Thus it might be beneficial to use a generative tracker for a newly deployed dialog system with only a few training dialogs available and switch to a discriminative model once enough training data from an already running system is collected. Ensemble trackers incorporating both generative and discriminative models as used by Lee and Eskenazi (2013) might also be an interesting direct</context>
</contexts>
<marker>Lee, Eskenazi, 2013</marker>
<rawString>Sungjin Lee and Maxine Eskenazi. 2013. Recipe for building robust spoken dialog state trackers: Dialog state tracking challenge system description. In Proceedings of the SIGDIAL 2013 Conference, pages 414–422, Metz, France, August. Association for Computational Linguistics.</rawString>
</citation>
<citation valid="true">
<authors>
<author>Sungjin Lee</author>
</authors>
<title>Structured Discriminative Model For Dialog State Tracking.</title>
<date>2013</date>
<booktitle>In Proceedings of the SIGDIAL 2013 Conference,</booktitle>
<pages>442--451</pages>
<location>Metz, France,</location>
<contexts>
<context position="7450" citStr="Lee, 2013" startWordPosition="1224" endWordPosition="1225">ew generative system participated. Kim et al. (2013) implemented the HIS model with additional discriminative rescoring, Wang and Lemon (2013) introduced a very simple model based on hand-crafted rules. Both of them scored between the second and the fourth place in the challenge. 3.2 Discriminative Methods As was previously mentioned, the discriminative methods received more attention recently. The overall winner of the DSTC (Lee and Eskenazi, 2013) used a maximum entropy model, which they claim to be outperformed by bringing more structure to the model by using the Conditional Random Fields (Lee, 2013). The same type of model is used also by Ren et al. (2013). Usage of Deep Neural Networks was tested by Henderson et al. (2013). ˇZilka et al. (2013) compare a discriminative maximum entropy model and a generative method based on approximate inference in a Bayesian netA B C 11 work, with the discriminative model preforming better. 4 Model Our model is an implementation of the HIS model (Young et al., 2010). In HIS the belief state is viewed as a probability distribution over all possible user’s goals. The belief state is represented by a set of so-called partitions, which are sets of user’s go</context>
</contexts>
<marker>Lee, 2013</marker>
<rawString>Sungjin Lee. 2013. Structured Discriminative Model For Dialog State Tracking. In Proceedings of the SIGDIAL 2013 Conference, pages 442–451, Metz, France, August. Association for Computational Linguistics.</rawString>
</citation>
<citation valid="true">
<authors>
<author>Andrew Ng</author>
<author>Michael Jordan</author>
</authors>
<title>On discriminative vs. generative classifiers: A comparison of logistic regression and naive bayes. Neural Information Processing Systems,</title>
<date>2001</date>
<pages>841--848</pages>
<contexts>
<context position="32051" citStr="Ng and Jordan, 2001" startWordPosition="5593" endWordPosition="5596">dity of assumption that the user’s goal remains fixed in the whole dialog, which is the case of DSTC bus schedule domains. 7 Conclusion Although the use of generative trackers was recently criticized by Williams (2012a), our results show that at least in some metrics (e.g. L2 metrics on dataset 3) a generative tracker can outperform the best state-of-the-art discriminative tracker (Lee and Eskenazi, 2013). Even though we agree that the discriminative approach might be more promising, it seems that in general there are conditions where generative models learn faster than discriminative models (Ng and Jordan, 2001). Thus it might be beneficial to use a generative tracker for a newly deployed dialog system with only a few training dialogs available and switch to a discriminative model once enough training data from an already running system is collected. Ensemble trackers incorporating both generative and discriminative models as used by Lee and Eskenazi (2013) might also be an interesting direction for future research. Acknowledgment We would like to thank Jiˇr´ı Havelka for his valuable comments on a draft of this paper. This work was partially funded by the GetHomeSafe project (EU 7th Framework STREP </context>
</contexts>
<marker>Ng, Jordan, 2001</marker>
<rawString>Andrew Ng and Michael Jordan. 2001. On discriminative vs. generative classifiers: A comparison of logistic regression and naive bayes. Neural Information Processing Systems, pages 841–848.</rawString>
</citation>
<citation valid="true">
<authors>
<author>Hang Ren</author>
<author>Weiqun Xu</author>
<author>Yan Zhang</author>
<author>Yonghong Yan</author>
</authors>
<title>Dialog state tracking using conditional random fields.</title>
<date>2013</date>
<booktitle>In Proceedings of the SIGDIAL 2013 Conference,</booktitle>
<pages>457--461</pages>
<publisher>Association</publisher>
<institution>for Computational Linguistics.</institution>
<location>Metz, France,</location>
<contexts>
<context position="7508" citStr="Ren et al. (2013)" startWordPosition="1235" endWordPosition="1238">) implemented the HIS model with additional discriminative rescoring, Wang and Lemon (2013) introduced a very simple model based on hand-crafted rules. Both of them scored between the second and the fourth place in the challenge. 3.2 Discriminative Methods As was previously mentioned, the discriminative methods received more attention recently. The overall winner of the DSTC (Lee and Eskenazi, 2013) used a maximum entropy model, which they claim to be outperformed by bringing more structure to the model by using the Conditional Random Fields (Lee, 2013). The same type of model is used also by Ren et al. (2013). Usage of Deep Neural Networks was tested by Henderson et al. (2013). ˇZilka et al. (2013) compare a discriminative maximum entropy model and a generative method based on approximate inference in a Bayesian netA B C 11 work, with the discriminative model preforming better. 4 Model Our model is an implementation of the HIS model (Young et al., 2010). In HIS the belief state is viewed as a probability distribution over all possible user’s goals. The belief state is represented by a set of so-called partitions, which are sets of user’s goals that are indistinguishable based on actions the system</context>
</contexts>
<marker>Ren, Xu, Zhang, Yan, 2013</marker>
<rawString>Hang Ren, Weiqun Xu, Yan Zhang, and Yonghong Yan. 2013. Dialog state tracking using conditional random fields. In Proceedings of the SIGDIAL 2013 Conference, pages 457–461, Metz, France, August. Association for Computational Linguistics.</rawString>
</citation>
<citation valid="true">
<authors>
<author>Douglas L Vail</author>
<author>Manuela M Veloso</author>
<author>John D Lafferty</author>
</authors>
<title>Conditional Random Fields for Activity Recognition Categories and Subject Descriptors.</title>
<date>2007</date>
<booktitle>In Proceedings of the 6th International Joint Conference on Autonomous Agents and Multiagent systems (AAMAS</booktitle>
<contexts>
<context position="29213" citStr="Vail et al., 2007" startWordPosition="5122" endWordPosition="5125">iants that received 1, 2, 3, 4 and 5 best SLU hypotheses. The overall trend was that initially performance increased as more SLU hypotheses were provided however then performance started decreasing. The 3-best variant achieved about 1.5% increase in joint accuracy compared to the 1-best. However, when using more than 3 best hypotheses, the performance slightly decreased. For instance, IBMindep uniform using 1-best hypothesis performed comparable to the 5-best configuration. Similar behavior of generative systems assuming observation independence has already been observed in different domains (Vail et al., 2007). Based on these results we deduce two conclusions. First, strong performance of IBMindep uniform 1-best system compared to the 1-best baseline system suggests that the main added value of our tracker in this domain is in the aggregation of observations from multiple time steps, not in tracking multiple hypotheses from one turn. Second, we attribute the effect of decreasing accuracy to the correlation of ASR errors from consecutive dialog turns. As noted by Williams (2012b), correlated ASR errors violate the assumption of observation independence that is assumed by HIS. Extending the user mode</context>
</contexts>
<marker>Vail, Veloso, Lafferty, 2007</marker>
<rawString>Douglas L Vail, Manuela M Veloso, and John D Lafferty. 2007. Conditional Random Fields for Activity Recognition Categories and Subject Descriptors. In Proceedings of the 6th International Joint Conference on Autonomous Agents and Multiagent systems (AAMAS 2007).</rawString>
</citation>
<citation valid="true">
<authors>
<author>Zhuoran Wang</author>
<author>Oliver Lemon</author>
</authors>
<title>A simple and generic belief tracking mechanism for the dialog state tracking challenge: On the believability of observed information.</title>
<date>2013</date>
<booktitle>In Proceedings of the SIGDIAL 2013 Conference,</booktitle>
<pages>423--432</pages>
<location>Metz,</location>
<contexts>
<context position="6982" citStr="Wang and Lemon (2013)" startWordPosition="1147" endWordPosition="1150">(Young et al., 2010) introduces an approximative method of solving the belief tracking as an inference in a dynamic Bayesian network with SLU hypotheses and machine actions as observed variables and the estimate of the user’s goal as a hidden variable. The HIS model was implemented several times (Williams, 2010; Gaˇsi´c, 2011). Recent criticism of generative methods for belief tracking brought more attention to the discriminative methods (Williams, 2012b). In the DSTC only few generative system participated. Kim et al. (2013) implemented the HIS model with additional discriminative rescoring, Wang and Lemon (2013) introduced a very simple model based on hand-crafted rules. Both of them scored between the second and the fourth place in the challenge. 3.2 Discriminative Methods As was previously mentioned, the discriminative methods received more attention recently. The overall winner of the DSTC (Lee and Eskenazi, 2013) used a maximum entropy model, which they claim to be outperformed by bringing more structure to the model by using the Conditional Random Fields (Lee, 2013). The same type of model is used also by Ren et al. (2013). Usage of Deep Neural Networks was tested by Henderson et al. (2013). ˇZi</context>
<context position="25087" citStr="Wang and Lemon, 2013" startWordPosition="4404" endWordPosition="4407">For each tracker ran on the test set 3 we used only the top five SLU hypotheses. All parameters for mixing the empirical prior probability with uniform distribution in trackers IBMindep train-to-test and IBMindep unsup were estimated using 3-fold cross validation scheme on the training data. The best parameter setting on the training data was then used in evaluation on the test set. IBMjointly uniform IBMindep uniform Table 5: Preliminary results for schedule 3 on the DSTC test set 1 of our two trackers compared to three overall well performing teams. For teams 6 and 9 see Table 4, team 2 is (Wang and Lemon, 2013). The legend of the table is the same as in Table 4. Even though we concentrated mainly on testing the tracker on dataset 3, we also ran it on the datasets 1 and 2. For the datasets 1 and 2 we used the single best SLU hypothesis from the live system. Such hypothesis was assigned 99% probability and the remaining 1% was left for the unobserved action. For the datasets 1 and 2 a post hoc computed SLU hypotheses are available in addition to the live data. In our experiments, using the post hoc computed SLU hypotheses with normalized confidence scores yielded worse results for our tracking systems</context>
</contexts>
<marker>Wang, Lemon, 2013</marker>
<rawString>Zhuoran Wang and Oliver Lemon. 2013. A simple and generic belief tracking mechanism for the dialog state tracking challenge: On the believability of observed information. In Proceedings of the SIGDIAL 2013 Conference, pages 423–432, Metz,</rawString>
</citation>
<citation valid="false">
<authors>
<author>August France</author>
</authors>
<title>Association for Computational Linguistics.</title>
<marker>France, </marker>
<rawString>France, August. Association for Computational Linguistics.</rawString>
</citation>
<citation valid="true">
<authors>
<author>Christian Wellekens</author>
</authors>
<title>Explicit time correlation in hidden markov models for speech recognition.</title>
<date>1987</date>
<booktitle>In Acoustics, Speech, and Signal Processing, IEEE International Conference on ICASSP’87.,</booktitle>
<volume>12</volume>
<pages>384--386</pages>
<publisher>IEEE.</publisher>
<contexts>
<context position="30015" citStr="Wellekens, 1987" startWordPosition="5257" endWordPosition="5258">of our tracker in this domain is in the aggregation of observations from multiple time steps, not in tracking multiple hypotheses from one turn. Second, we attribute the effect of decreasing accuracy to the correlation of ASR errors from consecutive dialog turns. As noted by Williams (2012b), correlated ASR errors violate the assumption of observation independence that is assumed by HIS. Extending the user model with an auto-regressive component, that is with dependence on observations from the previous time step (i.e. P(ut|ut−1, p, a)), might help to tackle this problem in generative models (Wellekens, 1987). To summarize the results on test set 3, even without any prior adaptation on the data our tracker is competitive with the best submissions to DSTC. After incorporating prior knowledge it outperforms all submitted trackers. On test set 1 and test set 2 (see Tables 5 and 6) the trackers perform second in accuracy. In L2 metrics the trackers are competitive with the best tracker in DSTC submitted by Team 6 and they outperform it in one out of four cases. It is interesting that our basic strategy that ignores live SLU scores performed that strong. However, on test 1 and test 2, which make it pos</context>
</contexts>
<marker>Wellekens, 1987</marker>
<rawString>Christian Wellekens. 1987. Explicit time correlation in hidden markov models for speech recognition. In Acoustics, Speech, and Signal Processing, IEEE International Conference on ICASSP’87., volume 12, pages 384–386. IEEE.</rawString>
</citation>
<citation valid="true">
<authors>
<author>Jason D Williams</author>
<author>Antoine Raux</author>
<author>Deepak Ramachandran</author>
<author>Alan Black</author>
</authors>
<title>Dialog state tracking challenge handbook.</title>
<date>2012</date>
<contexts>
<context position="5921" citStr="Williams et al., 2012" startWordPosition="977" endWordPosition="980">come from dialog systems deployed by three teams denoted as A, B and C. All the training datasets were transcribed but only three of them were annotated on the level of dialog acts. The SLU confidence scores from system B are relatively well calibrated, meaning that confidences can be directly interpreted as probabilities of observing the SLU hypothesis. Confidence scores from the system A are not well calibrated as noted by several DSTC participants (Lee and Eskenazi, 2013; Kim et al., 2013). The evaluation protocol is briefly described in Section 6. Its detailed description can be found in (Williams et al., 2012), its evaluation in (Williams et al., 2013). In 2013, nine teams with 27 trackers participated in the challenge. The results of the best trackers will be discussed together with the results of our tracker later in Section 6. 3 Related Work This section shortly reviews current approaches to dialog state tracking. We divide the trackers into two broad families of generative and discriminative methods. 3.1 Generative Methods The HIS model (Young et al., 2010) introduces an approximative method of solving the belief tracking as an inference in a dynamic Bayesian network with SLU hypotheses and mac</context>
</contexts>
<marker>Williams, Raux, Ramachandran, Black, 2012</marker>
<rawString>Jason D Williams, Antoine Raux, Deepak Ramachandran, and Alan Black. 2012. Dialog state tracking challenge handbook.</rawString>
</citation>
<citation valid="true">
<authors>
<author>Jason Williams</author>
<author>Antoine Raux</author>
<author>Deepak Ramachandran</author>
<author>Alan Black</author>
</authors>
<title>The Dialog State Tracking Challenge.</title>
<date>2013</date>
<booktitle>In Proceedings of the SIGDIAL 2013 Conference,</booktitle>
<pages>404--413</pages>
<publisher>Association</publisher>
<institution>for Computational Linguistics.</institution>
<location>Metz, France,</location>
<contexts>
<context position="2290" citStr="Williams et al., 2013" startWordPosition="350" endWordPosition="353">, which often leads to misinterpretation of the user’s intention. Dialog state tracking methods need to cope with such error-prone automatic speech recognition (ASR) and spoken language understanding (SLU) outputs. Traditional dialog systems use hand-crafted rules to select from the SLU outputs based on their confidence scores. Recently, several data-driven approaches to dialog state tracking were developed as a part of end-to-end spoken dialog systems. However, specifics of these systems render comparison of dialog state tracking methods difficult. The Dialog State Tracking Challenge (DSTC) (Williams et al., 2013) provides a shared testbed with datasets and tools for evaluation of dialog state tracking methods. It abstracts from subsystems of end-to-end spoken dialog systems focusing only on the dialog state estimation and tracking. It does so by providing datasets of ASR and SLU outputs with reference transcriptions together with annotation on the level of dialog acts. In this paper we report initial encouraging results of our generative belief state tracker. We plan to investigate discriminative approaches in the future. The rest of the paper continues as follows. In the next section we formally intr</context>
<context position="5964" citStr="Williams et al., 2013" startWordPosition="984" endWordPosition="987">teams denoted as A, B and C. All the training datasets were transcribed but only three of them were annotated on the level of dialog acts. The SLU confidence scores from system B are relatively well calibrated, meaning that confidences can be directly interpreted as probabilities of observing the SLU hypothesis. Confidence scores from the system A are not well calibrated as noted by several DSTC participants (Lee and Eskenazi, 2013; Kim et al., 2013). The evaluation protocol is briefly described in Section 6. Its detailed description can be found in (Williams et al., 2012), its evaluation in (Williams et al., 2013). In 2013, nine teams with 27 trackers participated in the challenge. The results of the best trackers will be discussed together with the results of our tracker later in Section 6. 3 Related Work This section shortly reviews current approaches to dialog state tracking. We divide the trackers into two broad families of generative and discriminative methods. 3.1 Generative Methods The HIS model (Young et al., 2010) introduces an approximative method of solving the belief tracking as an inference in a dynamic Bayesian network with SLU hypotheses and machine actions as observed variables and the </context>
</contexts>
<marker>Williams, Raux, Ramachandran, Black, 2013</marker>
<rawString>Jason Williams, Antoine Raux, Deepak Ramachandran, and Alan Black. 2013. The Dialog State Tracking Challenge. In Proceedings of the SIGDIAL 2013 Conference, pages 404–413, Metz, France, August. Association for Computational Linguistics.</rawString>
</citation>
<citation valid="true">
<authors>
<author>Jason D Williams</author>
</authors>
<title>Incremental partition recombination for efficient tracking of multiple dialog states.</title>
<date>2010</date>
<booktitle>In ICASSP,</booktitle>
<pages>5382--5385</pages>
<contexts>
<context position="6673" citStr="Williams, 2010" startWordPosition="1104" endWordPosition="1105">rackers will be discussed together with the results of our tracker later in Section 6. 3 Related Work This section shortly reviews current approaches to dialog state tracking. We divide the trackers into two broad families of generative and discriminative methods. 3.1 Generative Methods The HIS model (Young et al., 2010) introduces an approximative method of solving the belief tracking as an inference in a dynamic Bayesian network with SLU hypotheses and machine actions as observed variables and the estimate of the user’s goal as a hidden variable. The HIS model was implemented several times (Williams, 2010; Gaˇsi´c, 2011). Recent criticism of generative methods for belief tracking brought more attention to the discriminative methods (Williams, 2012b). In the DSTC only few generative system participated. Kim et al. (2013) implemented the HIS model with additional discriminative rescoring, Wang and Lemon (2013) introduced a very simple model based on hand-crafted rules. Both of them scored between the second and the fourth place in the challenge. 3.2 Discriminative Methods As was previously mentioned, the discriminative methods received more attention recently. The overall winner of the DSTC (Lee</context>
<context position="9738" citStr="Williams (2010)" startWordPosition="1611" endWordPosition="1612">s remain in the parent partition and the consistent ones are moved to the child. The belief of the original partition is distributed between the new ones in the ratio of their priors. To prevent an exponential increase in the number of partitions during the dialog, a partition recombination strategy can be used that removes the less probable partition and moves their hypotheses to different partitions. We perform partition recombination at the end of each turn (Henderson and Lemon, 2008), during the recombination low probability partitions are merged with their parents exactly as suggested by Williams (2010). For the actual belief update the following standard update equation is used: taken in turn t, u is a set of observed user actions, P(u|u) is the score of action u in the SLU n-best list u. In this definition P0(p) is a prior probability of partition p; the prior might be either uniform or estimated from the training data. The list u is extended with an unobserved action u˜ whose probability is: P(˜u|u) = 1 − � P(u|u). (2) uEu��˜u} P(u|p, a) in the update equation is the user model, i.e. how likely the user is to take an action u given that the last machine action was a and user’s goal is rep</context>
<context position="16419" citStr="Williams (2010)" startWordPosition="2914" endWordPosition="2915">I 3 13 However our model gives the following equation for both pa and pb: �1 � 1 Pour = 1 t+1(px) = k2 2 · 1 1+1+2 1 · 0 3 1 4 where x E {a, b}. The impact of ua,b on px is divided by a factor of 2 since it is shared by two partitions each representing one possible user goal. For pc we have: 1 �1 � Pour = 1 t+1(pc) = k2 2 · 0 1+1+2 1 · 1 2. 3 1 This is how values in Table 3 were computed. Another extension of the original HIS is how we handle the unobserved action. To our knowledge, the original HIS systems (Young et al., 2010; Gaˇsi´c, 2011) do not deal with probability of unobserved action; Williams (2010) presents a different way of handling the unobserved action. We provide unified way how to handle unrecognized mass on the SLU list. In the original HIS model, partition punobs not supported by any of the observed actions obtains probability by M evaluating to c− on each observed action. In our model, punobs receives non-zero probability due to A(punobs, ˜u, ∗) evaluating to 1 (see Eq. 5). 5 Tracker Design and its Variants The previous section gave detailed description of the update equations of our HIS based tracker. This section presents an overall design of different implemented tracker var</context>
</contexts>
<marker>Williams, 2010</marker>
<rawString>Jason D. Williams. 2010. Incremental partition recombination for efficient tracking of multiple dialog states. In ICASSP, pages 5382–5385.</rawString>
</citation>
<citation valid="true">
<authors>
<author>Jason D Williams</author>
</authors>
<title>Challenges and Opportunities for State Tracking in Statistical Spoken Dialog Systems: Results From Two Public Deployments.</title>
<date>2012</date>
<journal>IEEE Journal of Selected Topics in Signal Processing,</journal>
<volume>6</volume>
<issue>8</issue>
<contexts>
<context position="6818" citStr="Williams, 2012" startWordPosition="1124" endWordPosition="1125">ches to dialog state tracking. We divide the trackers into two broad families of generative and discriminative methods. 3.1 Generative Methods The HIS model (Young et al., 2010) introduces an approximative method of solving the belief tracking as an inference in a dynamic Bayesian network with SLU hypotheses and machine actions as observed variables and the estimate of the user’s goal as a hidden variable. The HIS model was implemented several times (Williams, 2010; Gaˇsi´c, 2011). Recent criticism of generative methods for belief tracking brought more attention to the discriminative methods (Williams, 2012b). In the DSTC only few generative system participated. Kim et al. (2013) implemented the HIS model with additional discriminative rescoring, Wang and Lemon (2013) introduced a very simple model based on hand-crafted rules. Both of them scored between the second and the fourth place in the challenge. 3.2 Discriminative Methods As was previously mentioned, the discriminative methods received more attention recently. The overall winner of the DSTC (Lee and Eskenazi, 2013) used a maximum entropy model, which they claim to be outperformed by bringing more structure to the model by using the Condi</context>
<context position="29689" citStr="Williams (2012" startWordPosition="5206" endWordPosition="5207">imilar behavior of generative systems assuming observation independence has already been observed in different domains (Vail et al., 2007). Based on these results we deduce two conclusions. First, strong performance of IBMindep uniform 1-best system compared to the 1-best baseline system suggests that the main added value of our tracker in this domain is in the aggregation of observations from multiple time steps, not in tracking multiple hypotheses from one turn. Second, we attribute the effect of decreasing accuracy to the correlation of ASR errors from consecutive dialog turns. As noted by Williams (2012b), correlated ASR errors violate the assumption of observation independence that is assumed by HIS. Extending the user model with an auto-regressive component, that is with dependence on observations from the previous time step (i.e. P(ut|ut−1, p, a)), might help to tackle this problem in generative models (Wellekens, 1987). To summarize the results on test set 3, even without any prior adaptation on the data our tracker is competitive with the best submissions to DSTC. After incorporating prior knowledge it outperforms all submitted trackers. On test set 1 and test set 2 (see Tables 5 and 6)</context>
<context position="31648" citStr="Williams (2012" startWordPosition="5533" endWordPosition="5534"> high probability to the hypothesis that the user goal was not observed so far. This might be due to our modified user model. Therefore our implementation does not need a final transformation of belief scores as reported by Kim et al. (2013). Additionally, our implementation does not exhibit the forgetting behavior as experienced by ˇZilka et al. (2013). Forgetting is undesirable given the validity of assumption that the user’s goal remains fixed in the whole dialog, which is the case of DSTC bus schedule domains. 7 Conclusion Although the use of generative trackers was recently criticized by Williams (2012a), our results show that at least in some metrics (e.g. L2 metrics on dataset 3) a generative tracker can outperform the best state-of-the-art discriminative tracker (Lee and Eskenazi, 2013). Even though we agree that the discriminative approach might be more promising, it seems that in general there are conditions where generative models learn faster than discriminative models (Ng and Jordan, 2001). Thus it might be beneficial to use a generative tracker for a newly deployed dialog system with only a few training dialogs available and switch to a discriminative model once enough training dat</context>
</contexts>
<marker>Williams, 2012</marker>
<rawString>Jason D. Williams. 2012a. Challenges and Opportunities for State Tracking in Statistical Spoken Dialog Systems: Results From Two Public Deployments. IEEE Journal of Selected Topics in Signal Processing, 6(8):959–970, December.</rawString>
</citation>
<citation valid="true">
<authors>
<author>Jason D Williams</author>
</authors>
<title>A critical analysis of two statistical spoken dialog systems in public use.</title>
<date>2012</date>
<booktitle>In Spoken Language Technology Workshop (SLT), 2012 IEEE,</booktitle>
<pages>55--60</pages>
<publisher>IEEE.</publisher>
<contexts>
<context position="6818" citStr="Williams, 2012" startWordPosition="1124" endWordPosition="1125">ches to dialog state tracking. We divide the trackers into two broad families of generative and discriminative methods. 3.1 Generative Methods The HIS model (Young et al., 2010) introduces an approximative method of solving the belief tracking as an inference in a dynamic Bayesian network with SLU hypotheses and machine actions as observed variables and the estimate of the user’s goal as a hidden variable. The HIS model was implemented several times (Williams, 2010; Gaˇsi´c, 2011). Recent criticism of generative methods for belief tracking brought more attention to the discriminative methods (Williams, 2012b). In the DSTC only few generative system participated. Kim et al. (2013) implemented the HIS model with additional discriminative rescoring, Wang and Lemon (2013) introduced a very simple model based on hand-crafted rules. Both of them scored between the second and the fourth place in the challenge. 3.2 Discriminative Methods As was previously mentioned, the discriminative methods received more attention recently. The overall winner of the DSTC (Lee and Eskenazi, 2013) used a maximum entropy model, which they claim to be outperformed by bringing more structure to the model by using the Condi</context>
<context position="29689" citStr="Williams (2012" startWordPosition="5206" endWordPosition="5207">imilar behavior of generative systems assuming observation independence has already been observed in different domains (Vail et al., 2007). Based on these results we deduce two conclusions. First, strong performance of IBMindep uniform 1-best system compared to the 1-best baseline system suggests that the main added value of our tracker in this domain is in the aggregation of observations from multiple time steps, not in tracking multiple hypotheses from one turn. Second, we attribute the effect of decreasing accuracy to the correlation of ASR errors from consecutive dialog turns. As noted by Williams (2012b), correlated ASR errors violate the assumption of observation independence that is assumed by HIS. Extending the user model with an auto-regressive component, that is with dependence on observations from the previous time step (i.e. P(ut|ut−1, p, a)), might help to tackle this problem in generative models (Wellekens, 1987). To summarize the results on test set 3, even without any prior adaptation on the data our tracker is competitive with the best submissions to DSTC. After incorporating prior knowledge it outperforms all submitted trackers. On test set 1 and test set 2 (see Tables 5 and 6)</context>
<context position="31648" citStr="Williams (2012" startWordPosition="5533" endWordPosition="5534"> high probability to the hypothesis that the user goal was not observed so far. This might be due to our modified user model. Therefore our implementation does not need a final transformation of belief scores as reported by Kim et al. (2013). Additionally, our implementation does not exhibit the forgetting behavior as experienced by ˇZilka et al. (2013). Forgetting is undesirable given the validity of assumption that the user’s goal remains fixed in the whole dialog, which is the case of DSTC bus schedule domains. 7 Conclusion Although the use of generative trackers was recently criticized by Williams (2012a), our results show that at least in some metrics (e.g. L2 metrics on dataset 3) a generative tracker can outperform the best state-of-the-art discriminative tracker (Lee and Eskenazi, 2013). Even though we agree that the discriminative approach might be more promising, it seems that in general there are conditions where generative models learn faster than discriminative models (Ng and Jordan, 2001). Thus it might be beneficial to use a generative tracker for a newly deployed dialog system with only a few training dialogs available and switch to a discriminative model once enough training dat</context>
</contexts>
<marker>Williams, 2012</marker>
<rawString>Jason D Williams. 2012b. A critical analysis of two statistical spoken dialog systems in public use. In Spoken Language Technology Workshop (SLT), 2012 IEEE, pages 55–60. IEEE.</rawString>
</citation>
<citation valid="true">
<authors>
<author>Steve Young</author>
<author>Milica Gasi´c</author>
<author>Simon Keizer</author>
<author>Franc¸ois Mairesse</author>
<author>Jost Schatzmann</author>
<author>Blaise Thomson</author>
<author>Kai Yu</author>
</authors>
<title>The Hidden Information State model: A practical framework for POMDP-based spoken dialogue management.</title>
<date>2010</date>
<journal>Computer Speech &amp; Language,</journal>
<volume>24</volume>
<issue>2</issue>
<marker>Young, Gasi´c, Keizer, Mairesse, Schatzmann, Thomson, Yu, 2010</marker>
<rawString>Steve Young, Milica Ga&amp;quot;si´c, Simon Keizer, Franc¸ois Mairesse, Jost Schatzmann, Blaise Thomson, and Kai Yu. 2010. The Hidden Information State model: A practical framework for POMDP-based spoken dialogue management. Computer Speech &amp; Language, 24(2):150–174, April.</rawString>
</citation>
<citation valid="true">
<authors>
<author>Luk´as Zilka</author>
<author>David Marek</author>
<author>Matej Korvas</author>
<author>Filip Jurc´ıcek</author>
</authors>
<title>Comparison of bayesian discriminative and generative models for dialogue state tracking.</title>
<date>2013</date>
<booktitle>In Proceedings of the SIGDIAL 2013 Conference,</booktitle>
<pages>452--456</pages>
<publisher>Association</publisher>
<institution>for Computational Linguistics.</institution>
<location>Metz, France,</location>
<marker>Zilka, Marek, Korvas, Jurc´ıcek, 2013</marker>
<rawString>Luk´a&amp;quot;s &amp;quot;Zilka, David Marek, Mat&amp;quot;ej Korvas, and Filip Jur&amp;quot;c´ı&amp;quot;cek. 2013. Comparison of bayesian discriminative and generative models for dialogue state tracking. In Proceedings of the SIGDIAL 2013 Conference, pages 452–456, Metz, France, August. Association for Computational Linguistics.</rawString>
</citation>
</citationList>
</algorithm>
</algorithms>
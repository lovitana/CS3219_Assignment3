<?xml version="1.0" encoding="UTF-8"?>
<algorithms version="110505">
<algorithm name="SectLabel" version="110505">
<variant no="0" confidence="0.092277">
<title confidence="0.997525">
Shared Task on Prediction of Dropout Over Time in Massively Open
Online Courses
</title>
<author confidence="0.963974">
Carolyn P. Rosé
</author>
<affiliation confidence="0.973492333333333">
Language Technologies Institute
and Human-Computer Interaction Institute
Carnegie Mellon University
</affiliation>
<address confidence="0.598877">
5000 Forbes Avenue, Pittsburgh, PA 15213
</address>
<email confidence="0.999445">
cprose@cs.cmu.edu
</email>
<author confidence="0.994852">
George Siemens
</author>
<affiliation confidence="0.997093">
Center for Distributed Education
University of Texas at Arlington
</affiliation>
<address confidence="0.899736">
701 South Nedderman Drive, Arlington, TX
76019
</address>
<email confidence="0.999502">
gsiemens@uta.edu
</email>
<sectionHeader confidence="0.995642" genericHeader="abstract">
Abstract
</sectionHeader>
<bodyText confidence="0.996673266666667">
The shared task on Prediction of Dropout
Over Time in MOOCs involves analysis
of data from 6 MOOCs offered through
Coursera. Data from one MOOC with ap-
proximately 30K students was distributed as
training data and consisted of discussion fo-
rum data (in SQL) and clickstream data (in
JSON format). The prediction task was Pre-
dicting Attrition Over Time. Based on behav-
ioral data from a week’s worth of activity in a
MOOC for a student, predict whether the stu-
dent will cease to actively participate after
that week. This paper describes the task.
A full write up of the results is published
separately (Rosé &amp; Siemens, 2014).
</bodyText>
<sectionHeader confidence="0.99536" genericHeader="keywords">
1 Overview
</sectionHeader>
<bodyText confidence="0.998427230769231">
Research on Massively Open Online Courses
(MOOCs)1 is an emerging area for real world impact
of technology for analysis of social media at a large
scale (Breslow et al., 2013). Modeling user experi-
ence in MOOCs supports research towards under-
standing user needs better so that experiences that are
more conducive to learning can be offered. Beyond
that, automated analyses enable adaptive technology
to tailor the experience of users in real time (Rosé et
al., 2014a). This paper describes a shared task de-
signed to enlist the involvement of the language tech-
nologies community in this endeavor and to identify
what value expertise within the field might bring.
</bodyText>
<footnote confidence="0.8635">
1 http://www.moocresearch.com/reports
</footnote>
<bodyText confidence="0.999909871794872">
One area for impact of natural language processing in
the MOOC space is in modeling behavior within the
threaded discussion forums. In a typical MOOC,
between 5% and 10% of students actively participate
in the threaded discussion forums. Previously pub-
lished research demonstrates that characteristics of
posting behavior are predictive of dropout along the
way (Rosé et al., 2014b; Wen et al., 2014a; Wen et
al., 2014b; Yang et al., 2013; Yang et al., 2014).
However, ideally, we would like to make predictions
for the other 90% to 95% of students who do not post.
Thus, in this shared task, we challenge participants to
use models of social interaction as displayed through
the text-based interaction between students in the
threaded discussions (from the minority of students
who participate in them) to make meaning from the
clickstream data we have from all students. If the
discussion data can be thus leveraged to make more
effective models of the clickstream data, then a mean-
ingful prediction about drop out along the way can
also be made about the students who do not post to
the discussion forums.
One of the biggest challenges in the shared task is that
the participants were only given data from one
Coursera MOOC as training and development data.
Their task was to produce a predictive model that
could be applied to data from other MOOCs they did
not have access to. A separate report describes a de-
tailed analysis of the results applying submitted mod-
els to each of 5 test MOOCs (Rosé &amp; Siemens, 2014).
12 research teams signed up for the shared task, in-
cluding an international assortment of academic and
industrial teams. Out of these 12 teams, only 4 sub-
mitted final models (Sinha et al., 2014; Sharkey &amp;
Sanders, 2014; Amnueypornsakul et al., 2014; Kloft
et al., 2014 ).
In the remainder of this paper we describe the shared
task in greater detail and discuss plans for future re-
lated research.
</bodyText>
<page confidence="0.995078">
39
</page>
<bodyText confidence="0.4386875">
Proceedings of the 2014 Conference on Empirical Methods in Natural Language Processing (EMNLP), pages 39–41,
October 25-29, 2014, Doha, Qatar. c�2014 Association for Computational Linguistics
</bodyText>
<sectionHeader confidence="0.888509" genericHeader="introduction">
2 Shared Task
</sectionHeader>
<bodyText confidence="0.999984649122807">
Participants in the shared task were given a complete
SQL dump and clickstream dump from one Coursera
MOOC as training data. The student-week was the
unit of analysis. In other words, a prediction was
made for each student for each week of their active
participation to predict whether that week was the last
week of their active participation. Scripts were pro-
vided to parse the data into a form that could be used
for the task, e.g., aggregating entries per user per
week. Scripts were also provided for running a test of
the trained model on test data. The purpose of the
scripts was to standardize the way in which each
team’s work would later be evaluated on the test
MOOCs that participants did not have access to.
A major part of the work in doing the task is in de-
termining what an effective representation would be
of the behavior trace associated with each student-
week that would enable making an accurate predic-
tion. In other words, the question is what are the dan-
ger signs that a student is especially vulnerable to
drop out? The rules of the task were such that the in-
formation the model was allowed to use for making
the prediction could be extracted from the whole par-
ticipation history of all training students (including
both the SQL data and the clickstream data) up to and
including the week a prediction was being made for.
Each of the four finalist teams submitted a final model
trained on the training MOOC and a write up include-
ing result trained on a designated subset of students
from the training MOOC and tested on the remaining
students. Results were presented in terms of preci-
sion, recall, and fmeasure for the held out users.
We recommend that participants make use of the text
data to bootstrap effective models that use only click-
stream data. However, participants were welcome to
leverage either type of data in the models they submit-
ted. In our evaluation presented separately (Rosé &amp;
Siemens, 2014), we evaluated the models on the test
MOOCs in three different ways: First, an evaluation
was conducted on data from students who actively
participated in the discussion forums. Second, an
evaluation was conducted on data from students who
never participated in the discussion forums. And fi-
nally, and evaluation was conducted on the set of stu-
dents that includes both types of students.
Each submission consisted of a write up describing
the technical approach and a link to a downloadable
zip file containing the trained model and code and/or
a script for using the trained model to make predic-
tions about the test sets. The code was required to be
runnable by launching a single script in Ubuntu 12.04.
A code stub for streamlining the preparation of the
submission was distributed with the data. The follow-
ing programming languages were acceptable: R 3.1,
C++ 4.7, Java 1.6, or Python 2.7. The script was re-
quired to be able to run within 24 hours on a 2400
MHz machine with 6 cores.
</bodyText>
<sectionHeader confidence="0.961618" genericHeader="method">
3 Looking Forward
</sectionHeader>
<bodyText confidence="0.999891862745098">
Computational modeling of massive scale social in-
teraction (as in MOOCs and other environments for
learning at scale) has the potential to yield new
knowledge about the inner-workings of interaction in
such environments so that support for healthy com-
munity formation can be designed and built. Howev-
er, the state-of-the-art in graphical models applied to
large scale social data provides representations of the
data that are challenging to interpret in light of specif-
ic questions that may be asked from a learning scienc-
es or social psychological perspective. What is need-
ed are new methodologies for development and inter-
pretation of models that bridge expertise from ma-
chine learning and language technologies on one side
and learning sciences, sociolinguistics, and social
psychology on the other side. The field of language
technologies has the human capital to take leadership
in making these breakthroughs.
The shared task described in this paper is the first one
like it where a data set from a Coursera MOOC has
been made publically available so that a wide range of
computational modeling techniques can be evaluated
side by side (Rosé &amp; Siemens, 2014). However, there
is recognition that such shared tasks may play an im-
portant role in shaping the future of the field of Learn-
ing Analytics going forward (Pea, 2014).
One of the major challenges in running a shared task
like this is ensuring the protection of privacy of the
MOOC participants. Such concerns have been the
focus of much discussion in the area of learning at
scale (Asilomar Convention, 2014).
Data sharing ethics were carefully considered in the
design of this shared task. In particular, all of the
students who participated in the MOOC that produced
the training data were told that their data would be
used for research purposes. The data was carefully
preprocessed to remove personal identifiers about the
students and the university that hosted the course. All
of the workshop participants who got access to the
data were required to participate in human subjects
training and to agree to use the data only for this
workshop, and not to share it beyond their team. Data
was shared through a secure web connection. Ap-
proval for use of the data in this fashion was approved
by the Institutional Review Board of the hosting uni-
versity as well as the university that ran the MOOC.
It was a goal in development of this shared task to
serve as a forerunner in what we hope will become a
more general practice of community wide collabora-
tion on large scale learning analytics (Suthers et al.,
2013).
</bodyText>
<page confidence="0.998466">
40
</page>
<sectionHeader confidence="0.98433" genericHeader="acknowledgments">
Acknowledgements
</sectionHeader>
<bodyText confidence="0.99755825">
The authors would like to thank Norman Bier for as-
sistance in working through the data sharing logistics.
This work was funded in part by NSF Grant OMA-
0836012.
</bodyText>
<sectionHeader confidence="0.958305" genericHeader="references">
References
</sectionHeader>
<reference confidence="0.996775902777778">
Amnueypornsakul, B., Bhat, S., &amp; Chinprutthiwong,
P. (2014). Predicting Attrition Along the Way:
The UIUC Model, in Proceedings of the 2014 Em-
pirical Methods in Natural Language Processing
Workshop on Modeling Large Scale Social Interac-
tion in Massively Open Online Courses, Qatar, Oc-
tober 2014.
Asilomar Convention (2014). The Asilomar Conven-
tion for Learning Research in Higher Education,
June 13, 2014.
Breslow, L., Pritchard, D., De Boer, J., Stump, G.,
Ho, A., &amp; Seaton, D. (2013). Studying Learning in
the Worldwide Classroom : Research into edX’s
First MOOC, Research &amp; Practice in Assessment
(8).
Kloft, M., Stiehler, F., Zheng, Z., &amp; Pinkward, N.
(2014). Predicting MOOC Dropout over Weeks
Using Machine Learning Methods, in Proceedings
of the 2014 Empirical Methods in Natural Lan-
guage Processing Workshop on Modeling Large
Scale Social Interaction in Massively Open Online
Courses, Qatar, October 2014.
Pea, R. (2014). The Learning Analytics Workgroup:
A Report on Building the Field of Learning Analyt-
ics for Personalized Learning at Scale, Stanford
University.
Rosé, C. P. &amp; Siemens, G. (2014). Shared Task Re-
port : Results of the EMNLP 2014 Shared Task on
Predictions of Dropout Over Time in MOOCs,
Langauge Technologies Institute Technical Report.
Rosé, C. P., Goldman, P., Sherer, J. Z., Resnick, L.
(2014a). Supportive Technologies for Group Dis-
cussion in MOOCs, Current Issues in Emerging
eLearning, Special issue on MOOCs, December
2014.
Rosé, C. P., Carlson, R., Yang, D., Wen, M., Resnick,
L., Goldman, P. &amp; Sherer, J. (2014b).Social Fac-
tors that Contribute to Attrition in MOOCs, in Pro-
ceedings of the First ACM Conference on Learning
@ Scale.
Sinha, T., Li, N., Jermann, P., &amp; Dillenbourg, P.
(2014). Capturing ‘attrition intensifying’ structural
traits from didactic interaction sequences of
MOOC learners, in Proceedings of the 2014 Em-
pirical Methods in Natural Language Processing
Workshop on Modeling Large Scale Social Interac-
tion in Massively Open Online Courses, Qatar, Oc-
tober 2014.
Sharkey, M. &amp; Sanders, R. (2014). A Process for
Predicting MOOC Attrition, in Proceedings of the
2014 Empirical Methods in Natural Language
Processing Workshop on Modeling Large Scale
Social Interaction in Massively Open Online
Courses, Qatar, October 2014.
Suthers, D., Lund, K., Rosé, C. P., Teplovs, C., Law,
N. (2013). Productive Multivocality in the Analy-
sis of Group Interactions, edited volume, Springer.
Wen, M., Yang, D., &amp; Rosé, C. P. (2014b). Linguistic
Reflections of Student Engagement in Massive
Open Online Courses, in Proceedings of the Inter-
national Conference on Weblogs and Social Media
Wen, M., Yang, D., &amp; Rosé, C. P. (2014a). Sentiment
Analysis in MOOC Discussion Forums: What does
it tell us? in Proceedings of Educational Data Min-
ing.
Yang, D., Sinha, T., Adamson, D., &amp; Rosé, C. P.
(2013). Turn on, Tune in, Drop out: Anticipating
student dropouts in Massive Open Online Courses,
in NIPS Data-Driven Education Workshop.
Yang, D., Wen, M., &amp; Rosé, C. P. (2014). Peer Influ-
ence on Attrition in Massively Open Online Cours-
es, in Proceedings of Educational Data Mining.
</reference>
<page confidence="0.999447">
41
</page>
</variant>
</algorithm>
<algorithm name="ParsHed" version="110505">
<variant no="0" confidence="0.000069">
<title confidence="0.9960465">Shared Task on Prediction of Dropout Over Time in Massively Online Courses</title>
<author confidence="0.934373">P Carolyn</author>
<affiliation confidence="0.681175">Language Technologies and Human-Computer Interaction Carnegie Mellon</affiliation>
<address confidence="0.999908">5000 Forbes Avenue, Pittsburgh, PA 15213</address>
<email confidence="0.996263">cprose@cs.cmu.edu</email>
<author confidence="0.781274">George</author>
<affiliation confidence="0.9922845">Center for Distributed University of Texas at</affiliation>
<address confidence="0.9785595">701 South Nedderman Drive, Arlington, 76019</address>
<email confidence="0.999791">gsiemens@uta.edu</email>
<abstract confidence="0.984789597883598">The shared task on Prediction of Dropout Over Time in MOOCs involves analysis of data from 6 MOOCs offered through from one MOOC with ap- 30K students as data and discussion forum data (in SQL) and clickstream data (in format). The prediction task Predicting Attrition Over Time. Based on behavdata from week’s worth of in a MOOC for a student, predict whether the student will cease to actively participate after week. paper describes the task. A full write up of the results is published separately (Rosé &amp; Siemens, 2014). 1 Overview Research on Massively Open Online Courses is an emerging area for real world impact of technology for analysis of social media at a large scale (Breslow et al., 2013). Modeling user experience in MOOCs supports research towards understanding user needs better so that experiences that are more conducive to learning can be offered. Beyond that, automated analyses enable adaptive technology to tailor the experience of users in real time (Rosé et al., 2014a). This paper describes a shared task designed to enlist the involvement of the language technologies community in this endeavor and to identify what value expertise within the field might bring. 1http://www.moocresearch.com/reports One area for impact of natural language processing in the MOOC space is in modeling behavior within the threaded discussion forums. In a typical MOOC, between 5% and 10% of students actively participate in the threaded discussion forums. Previously published research demonstrates that characteristics of posting behavior are predictive of dropout along the way (Rosé et al., 2014b; Wen et al., 2014a; Wen et al., 2014b; Yang et al., 2013; Yang et al., 2014). However, ideally, we would like to make predictions for the other 90% to 95% of students who do not post. Thus, in this shared task, we challenge participants to use models of social interaction as displayed through the text-based interaction between students in the threaded discussions (from the minority of students who participate in them) to make meaning from the clickstream data we have from all students. If the discussion data can be thus leveraged to make more effective models of the clickstream data, then a meaningful prediction about drop out along the way can also be made about the students who do not post to the discussion forums. One of the biggest challenges in the shared task is that the participants were only given data from one Coursera MOOC as training and development data. Their task was to produce a predictive model that could be applied to data from other MOOCs they did not have access to. A separate report describes a detailed analysis of the results applying submitted models to each of 5 test MOOCs (Rosé &amp; Siemens, 2014). 12 research teams signed up for the shared task, including an international assortment of academic and industrial teams. Out of these 12 teams, only 4 submitted final models (Sinha et al., 2014; Sharkey &amp; Sanders, 2014; Amnueypornsakul et al., 2014; Kloft et al., 2014 ). In the remainder of this paper we describe the shared task in greater detail and discuss plans for future related research. 39 of the 2014 Conference on Empirical Methods in Natural Language Processing pages 25-29, 2014, Doha, Qatar. Association for Computational Linguistics 2 Shared Task Participants in the shared task were given a complete SQL dump and clickstream dump from one Coursera MOOC as training data. The student-week was the unit of analysis. In other words, a prediction was made for each student for each week of their active participation to predict whether that week was the last week of their active participation. Scripts were provided to parse the data into a form that could be used for the task, e.g., aggregating entries per user per week. Scripts were also provided for running a test of the trained model on test data. The purpose of the scripts was to standardize the way in which each work would later be evaluated on the test MOOCs that participants did not have access to. A major part of the work in doing the task is in determining what an effective representation would be of the behavior trace associated with each studentweek that would enable making an accurate prediction. In other words, the question is what are the danger signs that a student is especially vulnerable to drop out? The rules of the task were such that the information the model was allowed to use for making the prediction could be extracted from the whole participation history of all training students (including both the SQL data and the clickstream data) up to and including the week a prediction was being made for. Each of the four finalist teams submitted a final model trained on the training MOOC and a write up includeing result trained on a designated subset of students from the training MOOC and tested on the remaining students. Results were presented in terms of precision, recall, and fmeasure for the held out users. We recommend that participants make use of the text data to bootstrap effective models that use only clickstream data. However, participants were welcome to leverage either type of data in the models they submitted. In our evaluation presented separately (Rosé &amp; Siemens, 2014), we evaluated the models on the test MOOCs in three different ways: First, an evaluation was conducted on data from students who actively participated in the discussion forums. Second, an evaluation was conducted on data from students who never participated in the discussion forums. And finally, and evaluation was conducted on the set of students that includes both types of students. Each submission consisted of a write up describing the technical approach and a link to a downloadable zip file containing the trained model and code and/or a script for using the trained model to make predictions about the test sets. The code was required to be runnable by launching a single script in Ubuntu 12.04. A code stub for streamlining the preparation of the submission was distributed with the data. The following programming languages were acceptable: R 3.1, C++ 4.7, Java 1.6, or Python 2.7. The script was required to be able to run within 24 hours on a 2400 MHz machine with 6 cores. 3 Looking Forward Computational modeling of massive scale social interaction (as in MOOCs and other environments for learning at scale) has the potential to yield new knowledge about the inner-workings of interaction in such environments so that support for healthy community formation can be designed and built. However, the state-of-the-art in graphical models applied to large scale social data provides representations of the data that are challenging to interpret in light of specific questions that may be asked from a learning sciences or social psychological perspective. What is needed are new methodologies for development and interpretation of models that bridge expertise from machine learning and language technologies on one side and learning sciences, sociolinguistics, and social psychology on the other side. The field of language technologies has the human capital to take leadership in making these breakthroughs. The shared task described in this paper is the first one like it where a data set from a Coursera MOOC has been made publically available so that a wide range of computational modeling techniques can be evaluated side by side (Rosé &amp; Siemens, 2014). However, there is recognition that such shared tasks may play an important role in shaping the future of the field of Learning Analytics going forward (Pea, 2014). One of the major challenges in running a shared task like this is ensuring the protection of privacy of the MOOC participants. Such concerns have been the focus of much discussion in the area of learning at scale (Asilomar Convention, 2014). Data sharing ethics were carefully considered in the design of this shared task. In particular, all of the students who participated in the MOOC that produced the training data were told that their data would be used for research purposes. The data was carefully preprocessed to remove personal identifiers about the students and the university that hosted the course. All of the workshop participants who got access to the data were required to participate in human subjects training and to agree to use the data only for this workshop, and not to share it beyond their team. Data was shared through a secure web connection. Approval for use of the data in this fashion was approved by the Institutional Review Board of the hosting university as well as the university that ran the MOOC. It was a goal in development of this shared task to serve as a forerunner in what we hope will become a more general practice of community wide collaboration on large scale learning analytics (Suthers et al., 2013). 40 Acknowledgements The authors would like to thank Norman Bier for assistance in working through the data sharing logistics. work was funded in part by NSF Grant OMA- 0836012.</abstract>
<note confidence="0.946976722222222">References Amnueypornsakul, B., Bhat, S., &amp; Chinprutthiwong, P. (2014). Predicting Attrition Along the Way: UIUC Model, in of the 2014 Empirical Methods in Natural Language Processing Workshop on Modeling Large Scale Social Interaction in Massively Open Online Courses, Qatar, October 2014. Convention (2014). Asilomar Convenfor Learning Research in Higher June 13, 2014. Breslow, L., Pritchard, D., De Boer, J., Stump, G., Ho, A., &amp; Seaton, D. (2013). Studying Learning in Worldwide Classroom : Research into MOOC, &amp; Practice in Assessment (8). Kloft, M., Stiehler, F., Zheng, Z., &amp; Pinkward, N. (2014). Predicting MOOC Dropout over Weeks</note>
<degree confidence="0.32056125">Machine Learning Methods, in of the 2014 Empirical Methods in Natural Language Processing Workshop on Modeling Large Scale Social Interaction in Massively Open Online</degree>
<note confidence="0.839472233333334">Courses, Qatar, October 2014. R. (2014). Learning Analytics Workgroup: A Report on Building the Field of Learning Analytfor Personalized Learning at Stanford University. C. P. &amp; Siemens, G. (2014). Task Report : Results of the EMNLP 2014 Shared Task on of Dropout Over Time in Langauge Technologies Institute Technical Report. Rosé, C. P., Goldman, P., Sherer, J. Z., Resnick, L. (2014a). Supportive Technologies for Group Disin MOOCs, Issues in Emerging Special issue on MOOCs, December 2014. Rosé, C. P., Carlson, R., Yang, D., Wen, M., Resnick, L., Goldman, P. &amp; Sherer, J. (2014b).Social Facthat Contribute to Attrition in MOOCs, in Proceedings of the First ACM Conference on Learning @ Scale. Sinha, T., Li, N., Jermann, P., &amp; Dillenbourg, P. Capturing traits from didactic interaction sequences of learners, in of the 2014 Empirical Methods in Natural Language Processing Workshop on Modeling Large Scale Social Interaction in Massively Open Online Courses, Qatar, October 2014. Sharkey, M. &amp; Sanders, R. (2014). A Process for MOOC Attrition, in of the 2014 Empirical Methods in Natural Language</note>
<title confidence="0.4083965">Processing Workshop on Modeling Large Scale Social Interaction in Massively Open Online</title>
<note confidence="0.7079028">Courses, Qatar, October 2014. Suthers, D., Lund, K., Rosé, C. P., Teplovs, C., Law, (2013). Multivocality in the Analyof Group Interactions, volume, Springer. Wen, M., Yang, D., &amp; Rosé, C. P. (2014b). Linguistic Reflections of Student Engagement in Massive Online Courses, in of the International Conference on Weblogs and Social Media Wen, M., Yang, D., &amp; Rosé, C. P. (2014a). Sentiment Analysis in MOOC Discussion Forums: What does tell us? in of Educational Data Mining. Yang, D., Sinha, T., Adamson, D., &amp; Rosé, C. P. (2013). Turn on, Tune in, Drop out: Anticipating student dropouts in Massive Open Online Courses, Data-Driven Education Yang, D., Wen, M., &amp; Rosé, C. P. (2014). Peer Influence on Attrition in Massively Open Online Coursin of Educational Data 41</note>
</variant>
</algorithm>
<algorithm name="ParsCit" version="110505">
<citationList>
<citation valid="true">
<authors>
<author>B Amnueypornsakul</author>
<author>S Bhat</author>
<author>P Chinprutthiwong</author>
</authors>
<title>Predicting Attrition Along the Way: The UIUC Model,</title>
<date>2014</date>
<booktitle>in Proceedings of the 2014 Empirical Methods in Natural Language Processing Workshop on Modeling Large Scale Social Interaction in Massively Open Online Courses,</booktitle>
<location>Qatar,</location>
<contexts>
<context position="3511" citStr="Amnueypornsakul et al., 2014" startWordPosition="570" endWordPosition="573">in the shared task is that the participants were only given data from one Coursera MOOC as training and development data. Their task was to produce a predictive model that could be applied to data from other MOOCs they did not have access to. A separate report describes a detailed analysis of the results applying submitted models to each of 5 test MOOCs (Rosé &amp; Siemens, 2014). 12 research teams signed up for the shared task, including an international assortment of academic and industrial teams. Out of these 12 teams, only 4 submitted final models (Sinha et al., 2014; Sharkey &amp; Sanders, 2014; Amnueypornsakul et al., 2014; Kloft et al., 2014 ). In the remainder of this paper we describe the shared task in greater detail and discuss plans for future related research. 39 Proceedings of the 2014 Conference on Empirical Methods in Natural Language Processing (EMNLP), pages 39–41, October 25-29, 2014, Doha, Qatar. c�2014 Association for Computational Linguistics 2 Shared Task Participants in the shared task were given a complete SQL dump and clickstream dump from one Coursera MOOC as training data. The student-week was the unit of analysis. In other words, a prediction was made for each student for each week of the</context>
</contexts>
<marker>Amnueypornsakul, Bhat, Chinprutthiwong, 2014</marker>
<rawString>Amnueypornsakul, B., Bhat, S., &amp; Chinprutthiwong, P. (2014). Predicting Attrition Along the Way: The UIUC Model, in Proceedings of the 2014 Empirical Methods in Natural Language Processing Workshop on Modeling Large Scale Social Interaction in Massively Open Online Courses, Qatar, October 2014.</rawString>
</citation>
<citation valid="true">
<authors>
<author>Asilomar Convention</author>
</authors>
<title>The Asilomar Convention for Learning Research</title>
<date>2014</date>
<booktitle>in Higher Education,</booktitle>
<contexts>
<context position="8378" citStr="Convention, 2014" startWordPosition="1400" endWordPosition="1401">er is the first one like it where a data set from a Coursera MOOC has been made publically available so that a wide range of computational modeling techniques can be evaluated side by side (Rosé &amp; Siemens, 2014). However, there is recognition that such shared tasks may play an important role in shaping the future of the field of Learning Analytics going forward (Pea, 2014). One of the major challenges in running a shared task like this is ensuring the protection of privacy of the MOOC participants. Such concerns have been the focus of much discussion in the area of learning at scale (Asilomar Convention, 2014). Data sharing ethics were carefully considered in the design of this shared task. In particular, all of the students who participated in the MOOC that produced the training data were told that their data would be used for research purposes. The data was carefully preprocessed to remove personal identifiers about the students and the university that hosted the course. All of the workshop participants who got access to the data were required to participate in human subjects training and to agree to use the data only for this workshop, and not to share it beyond their team. Data was shared throu</context>
</contexts>
<marker>Convention, 2014</marker>
<rawString>Asilomar Convention (2014). The Asilomar Convention for Learning Research in Higher Education, June 13, 2014.</rawString>
</citation>
<citation valid="true">
<authors>
<author>L Breslow</author>
<author>D Pritchard</author>
<author>J De Boer</author>
<author>G Stump</author>
<author>A Ho</author>
<author>D Seaton</author>
</authors>
<date>2013</date>
<booktitle>Studying Learning in the Worldwide Classroom : Research into edX’s First MOOC, Research &amp; Practice in Assessment (8).</booktitle>
<marker>Breslow, Pritchard, De Boer, Stump, Ho, Seaton, 2013</marker>
<rawString>Breslow, L., Pritchard, D., De Boer, J., Stump, G., Ho, A., &amp; Seaton, D. (2013). Studying Learning in the Worldwide Classroom : Research into edX’s First MOOC, Research &amp; Practice in Assessment (8).</rawString>
</citation>
<citation valid="true">
<authors>
<author>M Kloft</author>
<author>F Stiehler</author>
<author>Z Zheng</author>
<author>N Pinkward</author>
</authors>
<title>Predicting MOOC Dropout over Weeks Using Machine Learning Methods,</title>
<date>2014</date>
<booktitle>in Proceedings of the 2014 Empirical Methods in Natural Language Processing Workshop on Modeling Large Scale Social Interaction in Massively Open Online Courses,</booktitle>
<location>Qatar,</location>
<contexts>
<context position="3531" citStr="Kloft et al., 2014" startWordPosition="574" endWordPosition="577"> participants were only given data from one Coursera MOOC as training and development data. Their task was to produce a predictive model that could be applied to data from other MOOCs they did not have access to. A separate report describes a detailed analysis of the results applying submitted models to each of 5 test MOOCs (Rosé &amp; Siemens, 2014). 12 research teams signed up for the shared task, including an international assortment of academic and industrial teams. Out of these 12 teams, only 4 submitted final models (Sinha et al., 2014; Sharkey &amp; Sanders, 2014; Amnueypornsakul et al., 2014; Kloft et al., 2014 ). In the remainder of this paper we describe the shared task in greater detail and discuss plans for future related research. 39 Proceedings of the 2014 Conference on Empirical Methods in Natural Language Processing (EMNLP), pages 39–41, October 25-29, 2014, Doha, Qatar. c�2014 Association for Computational Linguistics 2 Shared Task Participants in the shared task were given a complete SQL dump and clickstream dump from one Coursera MOOC as training data. The student-week was the unit of analysis. In other words, a prediction was made for each student for each week of their active participat</context>
</contexts>
<marker>Kloft, Stiehler, Zheng, Pinkward, 2014</marker>
<rawString>Kloft, M., Stiehler, F., Zheng, Z., &amp; Pinkward, N. (2014). Predicting MOOC Dropout over Weeks Using Machine Learning Methods, in Proceedings of the 2014 Empirical Methods in Natural Language Processing Workshop on Modeling Large Scale Social Interaction in Massively Open Online Courses, Qatar, October 2014.</rawString>
</citation>
<citation valid="true">
<authors>
<author>R Pea</author>
</authors>
<title>The Learning Analytics Workgroup: A Report on Building the Field of Learning Analytics for Personalized Learning at Scale,</title>
<date>2014</date>
<institution>Stanford University.</institution>
<contexts>
<context position="8136" citStr="Pea, 2014" startWordPosition="1359" endWordPosition="1360">one side and learning sciences, sociolinguistics, and social psychology on the other side. The field of language technologies has the human capital to take leadership in making these breakthroughs. The shared task described in this paper is the first one like it where a data set from a Coursera MOOC has been made publically available so that a wide range of computational modeling techniques can be evaluated side by side (Rosé &amp; Siemens, 2014). However, there is recognition that such shared tasks may play an important role in shaping the future of the field of Learning Analytics going forward (Pea, 2014). One of the major challenges in running a shared task like this is ensuring the protection of privacy of the MOOC participants. Such concerns have been the focus of much discussion in the area of learning at scale (Asilomar Convention, 2014). Data sharing ethics were carefully considered in the design of this shared task. In particular, all of the students who participated in the MOOC that produced the training data were told that their data would be used for research purposes. The data was carefully preprocessed to remove personal identifiers about the students and the university that hosted</context>
</contexts>
<marker>Pea, 2014</marker>
<rawString>Pea, R. (2014). The Learning Analytics Workgroup: A Report on Building the Field of Learning Analytics for Personalized Learning at Scale, Stanford University.</rawString>
</citation>
<citation valid="true">
<authors>
<author>C P Rosé</author>
<author>G Siemens</author>
</authors>
<date>2014</date>
<booktitle>Shared Task Report : Results of the EMNLP 2014 Shared Task on Predictions of Dropout Over Time in MOOCs, Langauge Technologies Institute</booktitle>
<tech>Technical Report.</tech>
<contexts>
<context position="1031" citStr="Rosé &amp; Siemens, 2014" startWordPosition="157" endWordPosition="160">he shared task on Prediction of Dropout Over Time in MOOCs involves analysis of data from 6 MOOCs offered through Coursera. Data from one MOOC with approximately 30K students was distributed as training data and consisted of discussion forum data (in SQL) and clickstream data (in JSON format). The prediction task was Predicting Attrition Over Time. Based on behavioral data from a week’s worth of activity in a MOOC for a student, predict whether the student will cease to actively participate after that week. This paper describes the task. A full write up of the results is published separately (Rosé &amp; Siemens, 2014). 1 Overview Research on Massively Open Online Courses (MOOCs)1 is an emerging area for real world impact of technology for analysis of social media at a large scale (Breslow et al., 2013). Modeling user experience in MOOCs supports research towards understanding user needs better so that experiences that are more conducive to learning can be offered. Beyond that, automated analyses enable adaptive technology to tailor the experience of users in real time (Rosé et al., 2014a). This paper describes a shared task designed to enlist the involvement of the language technologies community in this e</context>
<context position="3261" citStr="Rosé &amp; Siemens, 2014" startWordPosition="528" endWordPosition="531">can be thus leveraged to make more effective models of the clickstream data, then a meaningful prediction about drop out along the way can also be made about the students who do not post to the discussion forums. One of the biggest challenges in the shared task is that the participants were only given data from one Coursera MOOC as training and development data. Their task was to produce a predictive model that could be applied to data from other MOOCs they did not have access to. A separate report describes a detailed analysis of the results applying submitted models to each of 5 test MOOCs (Rosé &amp; Siemens, 2014). 12 research teams signed up for the shared task, including an international assortment of academic and industrial teams. Out of these 12 teams, only 4 submitted final models (Sinha et al., 2014; Sharkey &amp; Sanders, 2014; Amnueypornsakul et al., 2014; Kloft et al., 2014 ). In the remainder of this paper we describe the shared task in greater detail and discuss plans for future related research. 39 Proceedings of the 2014 Conference on Empirical Methods in Natural Language Processing (EMNLP), pages 39–41, October 25-29, 2014, Doha, Qatar. c�2014 Association for Computational Linguistics 2 Share</context>
<context position="5803" citStr="Rosé &amp; Siemens, 2014" startWordPosition="967" endWordPosition="970">prediction was being made for. Each of the four finalist teams submitted a final model trained on the training MOOC and a write up includeing result trained on a designated subset of students from the training MOOC and tested on the remaining students. Results were presented in terms of precision, recall, and fmeasure for the held out users. We recommend that participants make use of the text data to bootstrap effective models that use only clickstream data. However, participants were welcome to leverage either type of data in the models they submitted. In our evaluation presented separately (Rosé &amp; Siemens, 2014), we evaluated the models on the test MOOCs in three different ways: First, an evaluation was conducted on data from students who actively participated in the discussion forums. Second, an evaluation was conducted on data from students who never participated in the discussion forums. And finally, and evaluation was conducted on the set of students that includes both types of students. Each submission consisted of a write up describing the technical approach and a link to a downloadable zip file containing the trained model and code and/or a script for using the trained model to make prediction</context>
<context position="7972" citStr="Rosé &amp; Siemens, 2014" startWordPosition="1328" endWordPosition="1331">ogical perspective. What is needed are new methodologies for development and interpretation of models that bridge expertise from machine learning and language technologies on one side and learning sciences, sociolinguistics, and social psychology on the other side. The field of language technologies has the human capital to take leadership in making these breakthroughs. The shared task described in this paper is the first one like it where a data set from a Coursera MOOC has been made publically available so that a wide range of computational modeling techniques can be evaluated side by side (Rosé &amp; Siemens, 2014). However, there is recognition that such shared tasks may play an important role in shaping the future of the field of Learning Analytics going forward (Pea, 2014). One of the major challenges in running a shared task like this is ensuring the protection of privacy of the MOOC participants. Such concerns have been the focus of much discussion in the area of learning at scale (Asilomar Convention, 2014). Data sharing ethics were carefully considered in the design of this shared task. In particular, all of the students who participated in the MOOC that produced the training data were told that </context>
</contexts>
<marker>Rosé, Siemens, 2014</marker>
<rawString>Rosé, C. P. &amp; Siemens, G. (2014). Shared Task Report : Results of the EMNLP 2014 Shared Task on Predictions of Dropout Over Time in MOOCs, Langauge Technologies Institute Technical Report.</rawString>
</citation>
<citation valid="true">
<authors>
<author>C P Rosé</author>
<author>P Goldman</author>
<author>J Z Sherer</author>
<author>L Resnick</author>
</authors>
<title>Supportive Technologies for Group Discussion in MOOCs, Current Issues in Emerging eLearning, Special issue on MOOCs,</title>
<date>2014</date>
<contexts>
<context position="1509" citStr="Rosé et al., 2014" startWordPosition="235" endWordPosition="238">y participate after that week. This paper describes the task. A full write up of the results is published separately (Rosé &amp; Siemens, 2014). 1 Overview Research on Massively Open Online Courses (MOOCs)1 is an emerging area for real world impact of technology for analysis of social media at a large scale (Breslow et al., 2013). Modeling user experience in MOOCs supports research towards understanding user needs better so that experiences that are more conducive to learning can be offered. Beyond that, automated analyses enable adaptive technology to tailor the experience of users in real time (Rosé et al., 2014a). This paper describes a shared task designed to enlist the involvement of the language technologies community in this endeavor and to identify what value expertise within the field might bring. 1 http://www.moocresearch.com/reports One area for impact of natural language processing in the MOOC space is in modeling behavior within the threaded discussion forums. In a typical MOOC, between 5% and 10% of students actively participate in the threaded discussion forums. Previously published research demonstrates that characteristics of posting behavior are predictive of dropout along the way (Ro</context>
</contexts>
<marker>Rosé, Goldman, Sherer, Resnick, 2014</marker>
<rawString>Rosé, C. P., Goldman, P., Sherer, J. Z., Resnick, L. (2014a). Supportive Technologies for Group Discussion in MOOCs, Current Issues in Emerging eLearning, Special issue on MOOCs, December 2014.</rawString>
</citation>
<citation valid="true">
<authors>
<author>C P Rosé</author>
<author>R Carlson</author>
<author>D Yang</author>
<author>M Wen</author>
<author>L Resnick</author>
<author>P Goldman</author>
<author>J Sherer</author>
</authors>
<title>Factors that Contribute to Attrition in MOOCs,</title>
<date>2014</date>
<booktitle>in Proceedings of the First ACM Conference on Learning @ Scale.</booktitle>
<contexts>
<context position="1509" citStr="Rosé et al., 2014" startWordPosition="235" endWordPosition="238">y participate after that week. This paper describes the task. A full write up of the results is published separately (Rosé &amp; Siemens, 2014). 1 Overview Research on Massively Open Online Courses (MOOCs)1 is an emerging area for real world impact of technology for analysis of social media at a large scale (Breslow et al., 2013). Modeling user experience in MOOCs supports research towards understanding user needs better so that experiences that are more conducive to learning can be offered. Beyond that, automated analyses enable adaptive technology to tailor the experience of users in real time (Rosé et al., 2014a). This paper describes a shared task designed to enlist the involvement of the language technologies community in this endeavor and to identify what value expertise within the field might bring. 1 http://www.moocresearch.com/reports One area for impact of natural language processing in the MOOC space is in modeling behavior within the threaded discussion forums. In a typical MOOC, between 5% and 10% of students actively participate in the threaded discussion forums. Previously published research demonstrates that characteristics of posting behavior are predictive of dropout along the way (Ro</context>
</contexts>
<marker>Rosé, Carlson, Yang, Wen, Resnick, Goldman, Sherer, 2014</marker>
<rawString>Rosé, C. P., Carlson, R., Yang, D., Wen, M., Resnick, L., Goldman, P. &amp; Sherer, J. (2014b).Social Factors that Contribute to Attrition in MOOCs, in Proceedings of the First ACM Conference on Learning @ Scale.</rawString>
</citation>
<citation valid="true">
<authors>
<author>T Sinha</author>
<author>N Li</author>
<author>P Jermann</author>
<author>P Dillenbourg</author>
</authors>
<title>Capturing ‘attrition intensifying’ structural traits from didactic interaction sequences of MOOC learners,</title>
<date>2014</date>
<booktitle>in Proceedings of the 2014 Empirical Methods in Natural Language Processing Workshop on Modeling Large Scale Social Interaction in Massively Open Online Courses,</booktitle>
<location>Qatar,</location>
<contexts>
<context position="3456" citStr="Sinha et al., 2014" startWordPosition="562" endWordPosition="565">ussion forums. One of the biggest challenges in the shared task is that the participants were only given data from one Coursera MOOC as training and development data. Their task was to produce a predictive model that could be applied to data from other MOOCs they did not have access to. A separate report describes a detailed analysis of the results applying submitted models to each of 5 test MOOCs (Rosé &amp; Siemens, 2014). 12 research teams signed up for the shared task, including an international assortment of academic and industrial teams. Out of these 12 teams, only 4 submitted final models (Sinha et al., 2014; Sharkey &amp; Sanders, 2014; Amnueypornsakul et al., 2014; Kloft et al., 2014 ). In the remainder of this paper we describe the shared task in greater detail and discuss plans for future related research. 39 Proceedings of the 2014 Conference on Empirical Methods in Natural Language Processing (EMNLP), pages 39–41, October 25-29, 2014, Doha, Qatar. c�2014 Association for Computational Linguistics 2 Shared Task Participants in the shared task were given a complete SQL dump and clickstream dump from one Coursera MOOC as training data. The student-week was the unit of analysis. In other words, a pr</context>
</contexts>
<marker>Sinha, Li, Jermann, Dillenbourg, 2014</marker>
<rawString>Sinha, T., Li, N., Jermann, P., &amp; Dillenbourg, P. (2014). Capturing ‘attrition intensifying’ structural traits from didactic interaction sequences of MOOC learners, in Proceedings of the 2014 Empirical Methods in Natural Language Processing Workshop on Modeling Large Scale Social Interaction in Massively Open Online Courses, Qatar, October 2014.</rawString>
</citation>
<citation valid="true">
<authors>
<author>M Sharkey</author>
<author>R Sanders</author>
</authors>
<title>A Process for Predicting MOOC Attrition,</title>
<date>2014</date>
<booktitle>in Proceedings of the 2014 Empirical Methods in Natural Language Processing Workshop on Modeling Large Scale Social Interaction in Massively Open Online Courses,</booktitle>
<location>Qatar,</location>
<contexts>
<context position="3481" citStr="Sharkey &amp; Sanders, 2014" startWordPosition="566" endWordPosition="569">f the biggest challenges in the shared task is that the participants were only given data from one Coursera MOOC as training and development data. Their task was to produce a predictive model that could be applied to data from other MOOCs they did not have access to. A separate report describes a detailed analysis of the results applying submitted models to each of 5 test MOOCs (Rosé &amp; Siemens, 2014). 12 research teams signed up for the shared task, including an international assortment of academic and industrial teams. Out of these 12 teams, only 4 submitted final models (Sinha et al., 2014; Sharkey &amp; Sanders, 2014; Amnueypornsakul et al., 2014; Kloft et al., 2014 ). In the remainder of this paper we describe the shared task in greater detail and discuss plans for future related research. 39 Proceedings of the 2014 Conference on Empirical Methods in Natural Language Processing (EMNLP), pages 39–41, October 25-29, 2014, Doha, Qatar. c�2014 Association for Computational Linguistics 2 Shared Task Participants in the shared task were given a complete SQL dump and clickstream dump from one Coursera MOOC as training data. The student-week was the unit of analysis. In other words, a prediction was made for eac</context>
</contexts>
<marker>Sharkey, Sanders, 2014</marker>
<rawString>Sharkey, M. &amp; Sanders, R. (2014). A Process for Predicting MOOC Attrition, in Proceedings of the 2014 Empirical Methods in Natural Language Processing Workshop on Modeling Large Scale Social Interaction in Massively Open Online Courses, Qatar, October 2014.</rawString>
</citation>
<citation valid="true">
<authors>
<author>D Suthers</author>
<author>K Lund</author>
<author>C P Rosé</author>
<author>C Teplovs</author>
<author>N Law</author>
</authors>
<title>Productive Multivocality in the Analysis of Group Interactions, edited volume,</title>
<date>2013</date>
<publisher>Springer.</publisher>
<marker>Suthers, Lund, Rosé, Teplovs, Law, 2013</marker>
<rawString>Suthers, D., Lund, K., Rosé, C. P., Teplovs, C., Law, N. (2013). Productive Multivocality in the Analysis of Group Interactions, edited volume, Springer.</rawString>
</citation>
<citation valid="true">
<authors>
<author>M Wen</author>
<author>D Yang</author>
<author>C P Rosé</author>
</authors>
<date>2014</date>
<booktitle>Linguistic Reflections of Student Engagement in Massive Open Online Courses, in Proceedings of the International Conference on Weblogs and Social</booktitle>
<location>Media</location>
<contexts>
<context position="2143" citStr="Wen et al., 2014" startWordPosition="332" endWordPosition="335">cribes a shared task designed to enlist the involvement of the language technologies community in this endeavor and to identify what value expertise within the field might bring. 1 http://www.moocresearch.com/reports One area for impact of natural language processing in the MOOC space is in modeling behavior within the threaded discussion forums. In a typical MOOC, between 5% and 10% of students actively participate in the threaded discussion forums. Previously published research demonstrates that characteristics of posting behavior are predictive of dropout along the way (Rosé et al., 2014b; Wen et al., 2014a; Wen et al., 2014b; Yang et al., 2013; Yang et al., 2014). However, ideally, we would like to make predictions for the other 90% to 95% of students who do not post. Thus, in this shared task, we challenge participants to use models of social interaction as displayed through the text-based interaction between students in the threaded discussions (from the minority of students who participate in them) to make meaning from the clickstream data we have from all students. If the discussion data can be thus leveraged to make more effective models of the clickstream data, then a meaningful predicti</context>
</contexts>
<marker>Wen, Yang, Rosé, 2014</marker>
<rawString>Wen, M., Yang, D., &amp; Rosé, C. P. (2014b). Linguistic Reflections of Student Engagement in Massive Open Online Courses, in Proceedings of the International Conference on Weblogs and Social Media</rawString>
</citation>
<citation valid="true">
<authors>
<author>M Wen</author>
<author>D Yang</author>
<author>C P Rosé</author>
</authors>
<title>Sentiment Analysis in MOOC Discussion Forums: What does it tell us?</title>
<date>2014</date>
<booktitle>in Proceedings of Educational Data Mining.</booktitle>
<contexts>
<context position="2143" citStr="Wen et al., 2014" startWordPosition="332" endWordPosition="335">cribes a shared task designed to enlist the involvement of the language technologies community in this endeavor and to identify what value expertise within the field might bring. 1 http://www.moocresearch.com/reports One area for impact of natural language processing in the MOOC space is in modeling behavior within the threaded discussion forums. In a typical MOOC, between 5% and 10% of students actively participate in the threaded discussion forums. Previously published research demonstrates that characteristics of posting behavior are predictive of dropout along the way (Rosé et al., 2014b; Wen et al., 2014a; Wen et al., 2014b; Yang et al., 2013; Yang et al., 2014). However, ideally, we would like to make predictions for the other 90% to 95% of students who do not post. Thus, in this shared task, we challenge participants to use models of social interaction as displayed through the text-based interaction between students in the threaded discussions (from the minority of students who participate in them) to make meaning from the clickstream data we have from all students. If the discussion data can be thus leveraged to make more effective models of the clickstream data, then a meaningful predicti</context>
</contexts>
<marker>Wen, Yang, Rosé, 2014</marker>
<rawString>Wen, M., Yang, D., &amp; Rosé, C. P. (2014a). Sentiment Analysis in MOOC Discussion Forums: What does it tell us? in Proceedings of Educational Data Mining.</rawString>
</citation>
<citation valid="true">
<authors>
<author>D Yang</author>
<author>T Sinha</author>
<author>D Adamson</author>
<author>C P Rosé</author>
</authors>
<title>Turn on, Tune in, Drop out: Anticipating student dropouts</title>
<date>2013</date>
<booktitle>in Massive Open Online Courses, in NIPS Data-Driven Education Workshop.</booktitle>
<contexts>
<context position="2182" citStr="Yang et al., 2013" startWordPosition="340" endWordPosition="343">t the involvement of the language technologies community in this endeavor and to identify what value expertise within the field might bring. 1 http://www.moocresearch.com/reports One area for impact of natural language processing in the MOOC space is in modeling behavior within the threaded discussion forums. In a typical MOOC, between 5% and 10% of students actively participate in the threaded discussion forums. Previously published research demonstrates that characteristics of posting behavior are predictive of dropout along the way (Rosé et al., 2014b; Wen et al., 2014a; Wen et al., 2014b; Yang et al., 2013; Yang et al., 2014). However, ideally, we would like to make predictions for the other 90% to 95% of students who do not post. Thus, in this shared task, we challenge participants to use models of social interaction as displayed through the text-based interaction between students in the threaded discussions (from the minority of students who participate in them) to make meaning from the clickstream data we have from all students. If the discussion data can be thus leveraged to make more effective models of the clickstream data, then a meaningful prediction about drop out along the way can als</context>
</contexts>
<marker>Yang, Sinha, Adamson, Rosé, 2013</marker>
<rawString>Yang, D., Sinha, T., Adamson, D., &amp; Rosé, C. P. (2013). Turn on, Tune in, Drop out: Anticipating student dropouts in Massive Open Online Courses, in NIPS Data-Driven Education Workshop.</rawString>
</citation>
<citation valid="true">
<authors>
<author>D Yang</author>
<author>M Wen</author>
<author>C P Rosé</author>
</authors>
<date>2014</date>
<booktitle>Peer Influence on Attrition in Massively Open Online Courses, in Proceedings of Educational Data Mining.</booktitle>
<contexts>
<context position="2202" citStr="Yang et al., 2014" startWordPosition="344" endWordPosition="347">f the language technologies community in this endeavor and to identify what value expertise within the field might bring. 1 http://www.moocresearch.com/reports One area for impact of natural language processing in the MOOC space is in modeling behavior within the threaded discussion forums. In a typical MOOC, between 5% and 10% of students actively participate in the threaded discussion forums. Previously published research demonstrates that characteristics of posting behavior are predictive of dropout along the way (Rosé et al., 2014b; Wen et al., 2014a; Wen et al., 2014b; Yang et al., 2013; Yang et al., 2014). However, ideally, we would like to make predictions for the other 90% to 95% of students who do not post. Thus, in this shared task, we challenge participants to use models of social interaction as displayed through the text-based interaction between students in the threaded discussions (from the minority of students who participate in them) to make meaning from the clickstream data we have from all students. If the discussion data can be thus leveraged to make more effective models of the clickstream data, then a meaningful prediction about drop out along the way can also be made about the </context>
</contexts>
<marker>Yang, Wen, Rosé, 2014</marker>
<rawString>Yang, D., Wen, M., &amp; Rosé, C. P. (2014). Peer Influence on Attrition in Massively Open Online Courses, in Proceedings of Educational Data Mining.</rawString>
</citation>
</citationList>
</algorithm>
</algorithms>
<?xml version="1.0" encoding="UTF-8"?>
<algorithms version="110505">
<algorithm name="SectLabel" version="110505">
<variant no="0" confidence="0.000018">
<title confidence="0.999589">
Toward Macro-Insights for Suicide Prevention:
Analyzing Fine-Grained Distress at Scale
</title>
<author confidence="0.992165">
Christopher M. Homan&apos; Ravdeep Johar&apos; Tong Liu&apos;
Megan Lytle&apos; Vincent Silenzio&apos; Cecilia O. Alm3
</author>
<affiliation confidence="0.992952">
&apos; Golisano College of Computing and Information Sciences, Rochester Institute of Technology
&apos; Department of Psychiatry, University of Rochester Medical Center
3 College of Liberal Arts, Rochester Institute of Technology
</affiliation>
<address confidence="0.636437">
{cmh§  |rsj7209*  |tl8313*  |Megan Lytle†  |Vincent Silenzio†  |coagla*}
</address>
<email confidence="0.997966">
§@cs.rit.edu *@rit.edu †@urmc.rochester.edu
</email>
<sectionHeader confidence="0.993845" genericHeader="abstract">
Abstract
</sectionHeader>
<bodyText confidence="0.9994018">
Suicide is a leading cause of death in the
United States. One of the major chal-
lenges to suicide prevention is that those
who may be most at risk cannot be re-
lied upon to report their conditions to clin-
icians. This paper takes an initial step
toward the automatic detection of suici-
dal risk factors through social media ac-
tivity, with no reliance on self-reporting.
We consider the performance of annota-
tors with various degrees of expertise in
suicide prevention at annotating microblog
data for the purpose of training text-based
models for detecting suicide risk behav-
iors. Consistent with crowdsourcing liter-
ature, we found that novice-novice anno-
tator pairs underperform expert annotators
and outperform automatic lexical analysis
tools, such as Linguistic Inquiry and Word
Count.
</bodyText>
<sectionHeader confidence="0.999135" genericHeader="introduction">
1 Introduction
</sectionHeader>
<bodyText confidence="0.999974111111112">
Suicide is among the leading causes of death for
individuals 10–44 years of age in the United States
(Heron and Tejada-Vera, 2009). Indeed, while
mortality rates for most illnesses decreased be-
tween 2008 and 2009, the rate of suicide increased
by 2.4% (Heron and Tejada-Vera, 2009). The life-
time prevalence for suicidal ideation is 5.6–14.3%
in the general population, and as high as 19.8–
24.0% among youth (Nock et al., 2008).
The first step toward suicide prevention is to
identify, ideally in consultation with clinical ex-
perts, the risk factors associated with suicide. Due
to social stigma among other sociocultural fac-
tors (Crosby et al., 2011), individuals at risk for
committing suicide may not always reach out to
professionals or, if they do, provide them with
accurate information. They may not even real-
ize their own level of suicide risk before it is too
late. Self-reporting, then, is not an entirely reliable
means of detecting and assessing suicide risk, and
research on suicide prevention can benefit from
also exploring other channels for assessing risk.
For instance, individuals may be more inclined
to seek support from informal resources, such as
social media, instead of seeking treatment (Crosby
et al., 2011; Bruffaerts et al., 2011; Ryan et al.,
2010). Evidence suggests that youth and emerg-
ing adults usually prefer to seek help from their
friends and families; however, higher levels of
suicidal ideation are associated with lower levels
of help-seeking from both formal or informal re-
sources (Deane et al., 2001).
These patterns in help-seeking behavior sug-
gest that social media might be an impor-
tant channel for discovering those at risk for—
and even preventing—suicide. Internet- and
telecommunications-driven activity is revolution-
izing the social sciences by providing data, much
of it publicly available, on human activity in situ,
at volumes and a level of time and space granu-
larity never before approached. Can such data im-
prove clinical preventative study and measures by
providing access to at-risk individuals who would
otherwise go undetected, and by leading to better
science about suicide risk behaviors?
The stress-diathesis model for suicidal behav-
ior (Mann et al., 1999) suggests that they might. It
says that (1) objective states, such as depression or
life events, as well as subjective states and traits,
such as substance abuse or family history of de-
pression, suicide, or substance abuse, are among
the risk factors that contribute to suicidal ideation
and (2) the presence of these factors could even-
tually lead to either externalizing (e.g., interper-
</bodyText>
<page confidence="0.984894">
107
</page>
<bodyText confidence="0.985600123076923">
Workshop on Computational Linguistics and Clinical Psychology: From Linguistic Signal to Clinical Reality, pages 107–117,
Baltimore, Maryland USA, June 27, 2014. c�2014 Association for Computational Linguistics
sonal violence) or internalizing aggression (e.g.,
attempting suicide).
Since the stress-diathesis model was developed
using risk factors for suicidal behavior, and be-
cause it makes a connection between internalized
and externalized acts, it is a suitable framework for
analyzing publicly available linguistic data from
social media outlets such as Twitter. Data from so-
cial media can be seen as a kind of natural exper-
iment on depression and suicidal ideation that is
unburdened by such sample biases as the willing-
ness of individuals to take part in research and/or
seek out formal sources of support. Moreover, this
approach may provide information about individ-
uals who are unlikely to engage in formal help-
seeking behaviors, or may inform effective meth-
ods of natural helping. Thus, this macro-level ap-
proach to monitoring suicidal behaviors may have
future implications not only for identifying indi-
viduals who have a higher prevalence for suicidal
behaviors but it could eventually lead to additional
methods for enhancing protective factors against
suicide.
In this paper, we take steps toward the auto-
matic detection of suicide risk among individuals
via social media. Suicide ideation is a complex be-
havior and its connection to suicide itself remains
poorly understood. We focus on a particular aspect
of suicidality, namely distress. While not equiva-
lent to suicide ideation, according to Nock et al.
(2010) distress is an important risk factor in sui-
cide, and one that is observable from microblog
text, though admittedly observing suicide risk be-
havior is a subjective and noisy venture.
Lehrman et al. (2012) conducted an early study
on the computational modeling of distress based
on short forum texts, yet left many areas wide open
for continued study. For example, analysis at scale
is one such open issue. More specifically, Pestian
and colleagues (Matykiewicz et al., 2009; Pestian
et al., 2008) used computational methods to under-
stand suicide notes. However, when it comes to
preventive contexts, such data are less insightful.
For preventive health, access to real-time health-
related data that dynamically evolve can allow us
to address macro-level analysis. Social media pro-
vide an additional opportunity to model the phe-
nomena of interest at scale.
We use methods that take advantage of lexical
analysis to retrieve microblog posts (tweets) from
Twitter and compare the performance of human
annotators—one being an expert, and others not—
to rate the level of distress of each tweet.
Clinical expert annotation, rather than general-
purpose tools for content and sentiment analy-
sis such as LIWC (Linguistic Inquiry and Word
Count) by Pennebaker et al. (2001), provides a ba-
sis for text-based statistical modeling. We show
that expertise-based keyword retrieval, departing
from knowledge about contributing risk factors,
results in better interannotator agreement in both
novice-novice and novice-expert annotation when
the keywords reflect the task at hand.
</bodyText>
<sectionHeader confidence="0.999724" genericHeader="related work">
2 Related Work
</sectionHeader>
<bodyText confidence="0.999871756756757">
Data on suicide traditionally comes from health-
care organizations, large-scale studies, or self re-
porting (Crosby et al., 2011; Horowitz and Bal-
lard, 2009). These sources are limited by sociocul-
tural barriers (Crosby et al., 2011), such as stigma
and shame. Moreover, data on suicide is never par-
ticularly reliable because suicide is a fundamen-
tally subjective, complex phenomenon with a low
base rate. For these reasons, many researchers
tend to focus on the relationship between risk fac-
tors and suicidal behavior, without relying heavily
on theoretical models (Nock et al., 2008).
Approximately one-third of all individuals who
reported suicidal ideation in their lifetime made a
plan to commit suicide. Nearly three-quarters of
those who reported making a suicide plan actu-
ally attempted. The odds of attempting suicide in-
creased exponentially when individuals endorsed
three or more risk factors, e.g., having a mood or
substance abuse disorder (Kessler et al., 1999).
Demographics, previous suicide attempts, men-
tal health concerns (i.e., depression, substance
abuse, suicidal ideation, self-harm, or impulsiv-
ity), family history of suicide, interpersonal con-
flicts (i.e., family violence or bullying), and means
for suicidal behavior (e.g., firearms), are com-
monly cited risk factors for suicidal behavior
(Nock et al., 2008; Crosby et al., 2011; Gaynes
et al., 2004; Harriss and Hawton, 2005; Shaffer et
al., 2004; Brown et al., 2000).
Regarding the use of annotation for predictive
modeling, evidence suggests that when it comes
to judgments that involve clinical phenomena, ex-
perts and novices behave differently (Li et al.,
2012; Womack et al., 2012). Such distinctions in-
tuitively make sense, as the learning of medical
domain knowledge requires advanced education in
</bodyText>
<page confidence="0.998555">
108
</page>
<bodyText confidence="0.998751433734941">
conjunction with substantial practical field experi-
ence.
In a task such as medical image inspection, the
subtle cues that point an observer to evidence that
allow them to identify a clinical condition, while
accessible to experts with training and perceptual
expertise to guide their exploration, are likely to be
missed by novices who lack that background and
clinical understanding. Such expertise can then be
integrated into human-centered health-IT systems
(Guo et al., 2014), in order to introduce novel ways
to retrieve medical images and take advantage of
an understanding of which information is useful.
It is reasonable to assume that this knowledge gap
also applies to other knowledge-intensive clinical
domains such as mental health. In this study, we
explore this question and study if novice vs. ex-
pert annotation makes a difference for identifying
distress in social media texts, as well as what the
impact of expert vs. novice annotation is for subse-
quent computational modeling with the annotated
data.
Affect in language is a phenomenon that has
been studied in the speech and text analysis do-
mains, and in many others (Calvo and D’Mello,
2010). Clearly, emotion is a key element in the
human experience, but it is notoriously difficult
to pin down and scholars in the affective sciences
lack a single agreed-upon definition for emotion.
Accordingly, different theoretical constructs have
been proposed to describe affect and affect-related
behaviors (Picard, 1997). In addition, research on
affect in language has shown that such phenom-
ena tend to be subjective, lack real ground truth
(often resulting in moderate kappa scores), and
have particularly fuzzy semantics in the gray zone
where neutrality and emotion meet (Alm, 2008).
These kinds of problem characteristics bring with
them their own set of demanding challenges from
a computational perspective (Alm, 2011). Yet, the
nature of such problems make them incredibly im-
portant to study, despite the challenges involved.
Sentiment analysis has been widely studied in
a number of computational settings, including on
various social networking sites. A rather substan-
tial body of work already exists on the use of
Twitter to study emotion (Bollen et al., 2011b;
Dodds et al., 2011; Wang et al., 2012; Pfitzner et
al., 2012; Kim et al., 2012; Bollen et al., 2011a;
Pfitzner et al., 2012; Bollen et al., 2011c; Moham-
mad, 2012; Golder and Macy, 2011; De Choud-
hury et al., 2012a; De Choudhury et al., 2012b;
De Choudhury et al., 2013; De Choudhury and
Counts, 2013; Hannak et al., 2012; Thelwall et
al., 2011; Pak and Paroubek, 2010). For in-
stance, Golder and and Macy study aggregate
global trends in “mood,” and show, among other
things, that people wake up in a relatively good
mood that decays as the day progresses (Golder
and Macy, 2011). Bollen et al. (2011c) show that
tweets from users who took a standard diagnos-
tic instrument for mood are often tied to current
events, such as elections and holidays.
Relatively little of this work has focused on sui-
cide or related psychological conditions. Masuda
et al. (2013) study suicide on mixi (a Japanese
social networking service). Cheng et al. (2012)
consider the ethical and political implications
of online data collection for suicide prevention.
Jashinsky et al. (2013) show correlations between
frequency in tweets related to suicide and ac-
tual suicide in the 50 United States of Amer-
ica. Sadilek et al. (2014) study depression on
Twitter. De Choudhury and collaborators studied
depression—in general and post-partum—in Twit-
ter (De Choudhury et al., 2012a; De Choudhury et
al., 2012b; De Choudhury et al., 2013; De Choud-
hury and Counts, 2013) and Facebook (De Choud-
hury et al., 2014). Homan et al. (2014) investigate
depression in TrevorSpace. A number of social
theories of suicide have been proposed (Wray et
al., 2011), but most of this work was with respect
to offline social systems.
</bodyText>
<sectionHeader confidence="0.998152" genericHeader="method">
3 Methods
</sectionHeader>
<bodyText confidence="0.999922071428571">
Our methods involve four main phases: (1) We fil-
tered a corpus, obtained from Sadilek et al. (2012),
of approximately 2.5 million tweets from 6,237
unique users in the New York City area that were
sent during a 1-month period between May and
June, 2010, into a set of 2,000 tweets that are rela-
tively likely to be centered around suicide risk fac-
tors. (2) We annotated each of these 2,000 tweets
with their level of distress, and also analyzed the
annotations in detail. (3) We then trained sup-
port vector machines and topic models with the
annotated data, except for a held-out subset of 200
tweets. (4) Finally, we assessed the effectiveness
of these methods on the held-out data.
</bodyText>
<page confidence="0.997138">
109
</page>
<table confidence="0.99935847826087">
Source Number of tweets 2,535,706
tweets Unique geo-active users 6,237
“Follows” relationships 102,739
“Friends” relationships 31,874
Filtered Number of tweets 2,000
tweets Unique users 1,467
Unique unigrams 1,714,167
Unique bigrams 9,246,715
Unique trigrams 1,306,1142
Categories LIWC sad 1,370
distribution
Depressive feeling 283
Suicide ideation 123
Depression symptoms 72
Self harm 67
Family violence/discord 47
Bullying 10
Gun ownership 10
Drug abuse 6
Impulsivity 6
Prior suicide attempts 2
Suicide around individual 2
Psychological disorders 2
</table>
<tableCaption confidence="0.999524">
Table 1: Summary statistics and thematic cate-
</tableCaption>
<bodyText confidence="0.992807">
gory distributions of the collected dataset. The
data were collected from NYC. Geo-active users
are those who geo-tag (i.e., automatically post the
GPS location of) their tweets relatively frequently
(more than 100 times per month).
</bodyText>
<subsectionHeader confidence="0.9997">
3.1 Filtering tweets
</subsectionHeader>
<bodyText confidence="0.999954291666666">
In order to facilitate the discovery of distress-
related tweets, we first (a) converted all text to
lower case; (b) stripped out punctuation and spe-
cial characters; and (c) mapped informal terms
(such as abbreviations and netspeak) to more stan-
dard ones, based on the noslang dictionary.1
We then used two different methods to filter
tweets that are relatively likely to center on sui-
cide risk factors. We used LIWC to capture 1,370
tweets by sampling randomly from among the
2,000 tweets with the highest LIWC sad score.
LIWC has been widely used to estimate emotion
in online social networks, and specifically to mood
on Twitter. This slight amount of randomness in
filtering tweets this way was intended to avoid se-
lecting obvious false positives, such as the use of
“sad” in nicknames.
Next, we adopted a collection of inclusive
search terms/phrases from Jashinsky et al. (2013),
which was designed specifically for capturing
tweets related to suicide risk factors, and applied
them to our source corpus. We added to these
more terms, from (Crosby et al., 2011) (see Ta-
ble 2). These terms yielded 630 tweets.
</bodyText>
<footnote confidence="0.811655">
1http://www.noslang.com/dictionary
</footnote>
<bodyText confidence="0.964313807692308">
depressive tired of living, leave this world,
feeling wanna die, hate my job,
feeling guilty, deserve to die,
desire to end own life,
feeling ignored,
tired of everything, feeling blue,
have blues
depression sleeping pill, have insomnia,
symptoms sleep forever, sleep disorder
drug clonazepam, drug overdose,
abuse imipramine
prior suicide tried suicide
attempts
suicide commit suicide,
ideation committing suicide,
feeling suicidal, want to suicide,
shoot myself, a gun to head,
hang myself, intention to die
self hurt myself, cut myself
harm
psychological sleep apnea
disorders
family lost my friend,
violence argument with wife,
discord argument with husband,
shouted at each other
</bodyText>
<tableCaption confidence="0.948194">
Table 2: Filtering terms added to those
from Jashinsky et al. (2013).
</tableCaption>
<subsectionHeader confidence="0.996636">
3.2 Novice and Expert Tweet Annotation
</subsectionHeader>
<bodyText confidence="0.999990909090909">
We then divided the resulting set of 2,000 fil-
tered tweets (1,370 from the LIWC sad dimension
and 630 from suicide-specific search terms), into
two randomized sets of 1,000 tweets each. Both
sets had the same proportion of LIWC-filtered and
suicide-specific-filtered tweets. A novice anno-
tated the first set and a counseling psychologist
with experience in suicide related research anno-
tated the second set. A second novice annotated
a subset of 250 tweets of the first set, to reveal
interannotator agreement between novices, as one
might expect a novice without training to be less
systematic. (The annotators were among the au-
thors.) Each tweet in each set was rated on a four-
point scale (H, ND, LD, HD) according to the level
of distress evident (Table 3).
Each tweet to be annotated was provided with
context in the form of the three tweets before and
after the tweet to be annotated that the tweeter
made, along with the timestamp of those tweets
and the thematic categories to which the tweet be-
longed, based on the filtering process (Figure 1).
</bodyText>
<subsectionHeader confidence="0.997264">
3.3 Modeling
</subsectionHeader>
<bodyText confidence="0.999904">
We then mapped each tweet to a feature space
composed of the unigrams, bigrams, and trigrams
in the corpus. For example, a simple tweet “I am
</bodyText>
<page confidence="0.989543">
110
</page>
<table confidence="0.879620555555555">
978: Date: XXXX
-3: dat man on maury is overreacting!!
he juss doin dat cuz he on
tv [-0:24:39]
-2: @XXXX cedes!!! [-0:21:25]
-1: yesssss! da weatherman was wronq
no rainy ass prom days!! yesss
prom is 2day guys!! class
of 2010! [-0:02:56]
&gt;&gt;&gt; @XXXX awwww thanks trae-trae
1: rt @XXXX: abt 2 hop in a kab
to skool i wouldn’t dare spend
over 2 dollars to get somewhere
i dnt wanna be n da first
place! [+0:00:57]
2: @XXXX yeaa [+0:03:59]
3: @XXXX wassup? [+0:05:28]
Msg_id: XXXX [Distress: ND, LIWC Sad: No]
</table>
<figureCaption confidence="0.873529">
Figure 1: Example input for annotator. The tweet
to be annotated is indicated by &gt;&gt;&gt;. Annotators
</figureCaption>
<bodyText confidence="0.942524142857143">
were given context in the form of the three tweets
immediately preceding—and the three tweets im-
mediately following—the tweet to be annotated
that the tweeter made, along with the relative time
at which each tweet was made. Each numerical la-
bel denotes one of these context tweets. (Tweeter
information has been blanked out.)
</bodyText>
<table confidence="0.9005498">
Code Distress Level
H happy
ND no distress
LD low distress
HD high distress
</table>
<tableCaption confidence="0.8342055">
Table 3: Distress-related categories used to anno-
tate the tweets.
</tableCaption>
<bodyText confidence="0.999388">
so happy” was represented as the following feature
vector: {I, am, so, happy, I am, am so, so happy,
I am so, am so happy}. Each feature is associated
with its tf-idf score (Manning et al., 2008).
We performed topic modeling on our dataset. A
topic is a set of lexical items that are likely to occur
in the same tweet. Topic models are capable of as-
sociating words with similar meanings and distin-
guishing among the different meanings of a single
word. We used latent Dirichlet allocation (LDA)
(Blei et al., 2003) to create these topics. Before
doing so, we removed stop words and words that
occur only once in the dataset. We then applied
LDA algorithm on the data to discover three top-
ics using 100 iterations.
We used support vector machines (SVMs)
(Joachims, 1998), a machine learning method that
is used to train a classification model that can as-
sign class labels to previously unseen tweets, to
assess the power of our annotations. SVMs treat
each tweet as a point in an extremely high dimen-
sional space (one dimension per uni-, bi-, and tri-
gram in the corpus). SVMs are a form of linear
separator that can also distinguish between non-
linearly separable classes of data by warping the
feature space (though in our case we perform no
such warping, or kernelization). They have proven
to be an extremely effective tool in classifying text
in numerous settings, including Twitter.
</bodyText>
<sectionHeader confidence="0.998726" genericHeader="method">
4 Results
</sectionHeader>
<figureCaption confidence="0.870751428571429">
Figure 2: Distribution of distress level annota-
tions on the tweets annotated by Novices 1 and 2
(N=250, identical set).
Figure 3: Distribution of distress level annota-
tions from Novice 1 and Expert. Note the these
two datasets are disjoint (N = 1000 tweets, respec-
tively).
</figureCaption>
<bodyText confidence="0.9966414">
Figure 2 shows the distribution of annotation la-
bels for the subset of tweets that Novices 1 and 2
both annotated, and Figure 3 compares the over-
all annotation distributions between Novice 1 and
the Expert. Interestingly, the novices are relatively
conservative, compared to the expert, in assign-
ing distressed labels, whereas the expert exhibits
a higher sensitivity toward low distress than either
of the novices. This suggests that it is important in
this domain not to rely too much on novice judg-
</bodyText>
<page confidence="0.994863">
111
</page>
<bodyText confidence="0.9998174">
ments, as novices are not trained to pick up on sub-
tle cues—in contrast to the clinically trained eye.
Note that there are very few happy tweets,
which confirms that our filtering was effective in
removing tweets of the opposite polarity.
</bodyText>
<table confidence="0.99813125">
Filtering method Kappa
LIWC sad 0.4
Thematic suicide risk factors 0.6
Both 0.5
</table>
<tableCaption confidence="0.964093">
Table 4: Cohen kappa interannotator agreement
</tableCaption>
<table confidence="0.9664535">
between Novice 1 and 2.
H ND LD HD
H 0 2 0 0
ND 1 85 2 1
LD 0 22 9 0
HD 0 1 0 2
</table>
<tableCaption confidence="0.980128333333333">
Table 5: Confusion matrix between Novices 1 and
2 on annotations of the LIWC-sad-based filtered
tweets.
</tableCaption>
<table confidence="0.9998006">
H ND LD HD
H 4 6 0 0
ND 0 55 12 1
LD 0 12 22 5
HD 0 1 3 4
</table>
<tableCaption confidence="0.992462">
Table 6: Confusion matrix between Novices 1 and
</tableCaption>
<bodyText confidence="0.908472111111111">
2 on annotations of tweets filtered by Jashinsky et
al. (2013)’s thematic suicide risk factors inclusion
terms.
Table 4 shows the Cohen kappa score between
Novices 1 and 2, when high and low distress vs.
no distress and happy, are grouped in a single cate-
gory and Tables 5—7 show the confusion matrices
between Novices 1 and 2. In all cases the kappa
score is moderate. However, it clearly improves
when annotation is restricted to just those tweets
filtered using the suicide-thematic inclusion terms
of Jashinsky et al. (2013). This again seems to
point to the usefulness of including clinical experts
into the training process.
Due to their sensitive nature, we decided not to
provide examples of high distress tweets. Here are
two examples of tweets labeled as low distress by
two annotators.
</bodyText>
<footnote confidence="0.726989">
9 insomnia night#56325897521365!!
sheesh can’t deal w/ this shit!
i have class in the morning got
dammit....
</footnote>
<table confidence="0.9987036">
H ND LD HD
H 4 8 0 0
ND 1 140 14 2
LD 0 34 31 5
HD 0 2 3 6
</table>
<tableCaption confidence="0.8104078">
Table 7: Confusion matrix between Novices 1 and
2 on annotations of all common tweets between
the two annotators.
9 @XXXX i’m still sad thoo. i feel
neglected! and i miss XXXX
</tableCaption>
<bodyText confidence="0.803041230769231">
And here are two examples of tweets labeled as
no distress by two annotators.
9 i did mad push-ups tryna get that
cut up look, then look at myself
after a shower ... #plandidntwork;
thats #whyiaintgotomiami
9 my son is gonna have blues eyes and
nappy hair! yes yes yes
The above examples are rather clear cut, how-
ever in many cases the tweets were more ambigu-
ous, even when annotators had the preceding and
succeeding three tweets from the user of the tweet
to be annotated to rely on for context. While con-
text and time offset information was useful for an-
notators, distress annotation is clearly a challeng-
ing task, as the confusion matrices in Tables 5–6
reveal. The lower agreement levels, and particu-
larly the fuzzy border between ‘no distress’ and
‘low distress’ are completely in line with prior
research, discussed above, on affective language
phenomena.
Another filtering and annotation challenge in-
volves tweets with mixed emotion, such as:
9 as much as i hate my job some of the
people i work with are amazing.
Beyond the targeted annotation categories of
distress level, there were emerging themes of
aggression, privilege and oppression, and daily
struggles, among others. For instance, jobs were
a popular source of distress:
9 i friggin hate these bastards my
job grimey ass bastards knew i
wanted the day off and tell me some
next shit
9 hate my job wit a passion! hate
every1 there.. they better do
sumthin about it, or im out!
Personal bias may have impacted annotation de-
cisions. For instance, numerous tweets contained
</bodyText>
<page confidence="0.996953">
112
</page>
<bodyText confidence="0.93138308">
irony and dark humor, which may result in anno-
tators underestimating or overlooking actual dis-
tress. In addition, by pulling data from Twitter,
any non-Twitter context behind the tweets is lost.
For example, a few individuals retweeted in a sar-
castic manner about what individuals should say
to someone who is considering suicide:
9 you wish!!! rt @XXXX: i think
suicide is funny. especially once
my mom does it
9 rt @XXXX: what do i say to a person
thats asking me for advice becuz
they thinking bout committing
suicide when i see there point?
lmao
Without knowing the circumstances of the original
message (beyond the provided context window) it
is difficult to classify such tweets.
Finally, a number of tweets seemed to show
compassion or empathy for others experiencing
stress. This suggests to us the profound role that
social support places in well-being and depression,
that one’s friends and associates can also provide
clues into one’s emotional state, and that social
media can reveal such behavior.
</bodyText>
<listItem confidence="0.7723058">
9 rt @XXXX: damn now what do i do? i
feel empty as f$% damit!! breathe
ocho, *tears* from liberty city to
(cont) http://XXXX
9 @XXXX that’s just sad i feel for you
</listItem>
<bodyText confidence="0.989323853658537">
High Distress Random
feel like, wanna cry, get good morning, last
hurt, miss 2, ima miss, win night, happy birthday,
lose, tired everything, broke look like, bout 2, can’t
bitches, gun range, one wait, video , know
person (cont), chris brown, jus
got
commit suicide, miss you!, feel like, let know,
miss baby, feel empty, make sure, bout go,
committing suicide, tired time get, don’t get, wats
living, sleep forever, lost good, .., don’t want,
phone, left alone, :( miss jus saw
hate job, feel sad, tummy don’t know, let’s go,
hurts, lost friend, feel looks like, what’s good,
helpless, leave alone, don’t go sleep, even tho, hell
wanna, worst feeling, leave yea, new single, r u?,
world, don’t let don’t wanna
Table 8: Topic analysis on bigrams of tweets la-
beled as high distress vs. randomly selected tweets
from the larger, unlabeled dataset. The high dis-
tress tweets clearly convey strong negative affect.
Table 8 shows the results of a 3-category topic
model on bigrams. The first column is taken just
from tweets labeled high distress by any one of
the three annotators (72 tweets total). The sec-
ond column comes from a randomly-chosen sam-
ple of 2000 tweets from the 2.3 million tweet cor-
pus. These results show that the lexical contents
of the annotated tweets are recognizeably differ-
ent from the random sample. By our judgement,
the topical groupings in the rows of the high dis-
tress column are all clearly marked by strong neg-
ative affect, and additionally they could arguably
be labeled—from top to bottom—as: “failure and
defeat,” “loss,” and “loneliness.” The rows of the
second column are less clear cut, and appear to
reflect a much broader scope of topics. One inter-
esting aspect of the second, random column is that
recording artist Chris Brown had released a new
album during the collection period, which seems
to explain why his name appeared.
</bodyText>
<table confidence="0.999335166666667">
Training Testing Precision Recall F-Measure
N1 N1 0.53 0.63 0.58
N1 E 0.58 0.27 0.37
E E 0.59 0.71 0.64
E N1 0.34 0.85 0.48
N1 + E N1 + E 0.33 0.41 0.37
</table>
<tableCaption confidence="0.988618">
Table 9: Performance of SVM-based classification
</tableCaption>
<bodyText confidence="0.994762769230769">
when the training and testing sets are alternately
Novice 1 (N1) or the Expert (E). Because we fo-
cus on distress classification, we report precision,
recall and F-measure for the distress class, which
combines LD and HD into a single class with re-
spect to binary (distress vs. non-distress) classifi-
cation. In each case, a held-out set of 100 ran-
domly selected tweets compose the test set and
the remaining 900 tweets from that annotator com-
pose the training set. The last row shows when the
two training sets (respectively, test sets) are com-
bined into a single set of 1800 (respectively, 200)
tweets.
For classification, because we are most inter-
ested in being able to separate distressed from
non-distressed tweets, we combine low distress
and high distress into a single distress class, and
no distress and happy into a non-distress class. Ta-
ble 9 shows the performance of the SVM-based
classifier when trained and tested on the Expert
and Novice 1 training sets. Four themes emerge:
(1) the SVM classifier is much more accurate (in
terms of F-measure) when the testing and training
data come from the same annotator (test and train-
ing data are disjoint), and the best performance
comes from the expert-annotated data. (2) When
</bodyText>
<page confidence="0.998231">
113
</page>
<bodyText confidence="0.999848941176471">
testing and training data are from different anno-
tators, the F-measure performance of the SVM
is lower when the training set is from the novice
rather than the expert. (3) When testing and train-
ing data are from different annotators, the SVM
has lower recall and higher precision when the
training set is from the novice rather than the ex-
pert. This is in part because the Expert was more
sensitive to distress than Novice 1. It is premature
to draw conclusions from this observation, but per-
haps this shows that training with expert-labeled
annotations is preferable to using novice-labeled
data, espectially when our goal is to discover dis-
tressful tweets for the purpose of identifying at-
risk individuals and err on the side of caution (high
recall). (4) Integrating more but mixed data does
not improve performance.
</bodyText>
<sectionHeader confidence="0.998896" genericHeader="method">
5 Discussion
</sectionHeader>
<bodyText confidence="0.9998163">
As previously mentioned, many of the risk fac-
tors for suicidal behavior may be linked to other
expressions of distress, such as aggression and
interpersonal violence (Mann et al., 1999). The
goal of this study is to determine the feasibility
of classifying distress to enable further study of
expressed suicidal behaviors. Consistent with the
stress diathesis model for suicidal behavior, ag-
gression was an emerging theme that arose from
the data. Here are some examples:
</bodyText>
<tableCaption confidence="0.692907285714286">
9 @XXXX i don’t feel sad 4 him. he
gets pissed n says wat he wants then
sends out fony apologies
9 @XXXX cuz he’s n a relationship
with that horseface bitch &amp; he
lied 2 me &amp; i feel so used &amp;
worthless now
</tableCaption>
<bodyText confidence="0.999540888888889">
Some individuals tweeted about feeling empty,
hopeless, angry, frustrated, and alone. Behaviors
indicating bullying and schadenfreude were also
observed. While these are all risk factors for inter-
nalizing aggression (i.e., suicidal behavior), they
are also associated with externalized aggression.
In addition to overt expressions of anger and vi-
olence, many of the humorous, ironic tweets also
had an aggressive undertone.
</bodyText>
<subsectionHeader confidence="0.99522">
5.1 Limitations
</subsectionHeader>
<bodyText confidence="0.9999872">
As ground truth, we rely on tweets hand-annotated
by expert and novice for classification. However,
the mental state of another individual, observed
from a few lines of text often written in an in-
formal register is necessarily hard to discern and,
even under less noisy conditions, extremely sub-
jective; even the observers’ personal understand-
ings of such concepts as “distress” may differ
drastically. This makes annotation quite a chal-
lenge, and does not reveal in an objective fashion a
tweeter’s true mental state. As we have mentioned
earlier, self-reporting has its own limitations, yet
it is often regarded as the gold standard for ground
truth about emotional state. Part of the problem in
assessing the effectiveness of self-reporting is the
relative rareness by which suicide occurs, and by
the inherent subjectivity of the act, which makes
any data on suicide fuzzy. We hope to explore in
future work the relationship between clinical ob-
servation in both on- and off-line settings and self-
reporting, including the integration of natural lan-
guage data of patients from clinical settings. We
also hope to explore distress annotation from dif-
ferent perspectives and levels of context.
Higher levels of suicidal ideation have an in-
verse relationship with all types of help-seeking
and a positive correlation with the decision to not
seek support (Deane et al., 2001). Thus, we would
expect suicidal individuals to generally be less ac-
tive on social media than those who are not. Nev-
ertheless, a number of studies have shown a posi-
tive correlation between online social network use
and negative mood. Perhaps this means in part that
individuals who are depressed are slower to disen-
gage on- rather than off-line.
</bodyText>
<sectionHeader confidence="0.996155" genericHeader="conclusions">
6 Conclusion
</sectionHeader>
<bodyText confidence="0.99997925">
We studied the performance of different ap-
proaches to training systems to detect evidence
of suicide risk behavior in microblog data. We
showed that both the methods used to automat-
ically collect training sets, as well as the ex-
pertise level of the annotator affect greatly the
performance of automatic systems for detecting
suicide risk factors. In general, our study and
its results—from filtering via data annotation to
classification—confirmed the critical importance
of bringing clinical expertise into the computa-
tional modeling loop.
</bodyText>
<sectionHeader confidence="0.989792" genericHeader="acknowledgments">
Acknowledgments
</sectionHeader>
<bodyText confidence="0.99744875">
This work was supported by a grant of the Kodak
Endowed Chair Fund from the Golisano College
of Computing and Information Sciences at RIT
and NSF award SES-1111016.
</bodyText>
<page confidence="0.998134">
114
</page>
<sectionHeader confidence="0.990268" genericHeader="references">
References
</sectionHeader>
<reference confidence="0.986986636363637">
Cecilia Ovesdotter Alm. 2008. Affect in Text and
Speech. Ph.D. thesis, University of Illinois at Ur-
bana Champaign.
Cecilia Ovesdotter Alm. 2011. Subjective natural
language problems: Motivations, applications, char-
acterizations, and implications. In Proceedings of
49th Annual Meeting of the Assoc. for Computa-
tional Linguistics: Human Language Technologies,
Portland, OR, pages 107–112.
David M. Blei, Andrew Y. Ng, and Michael I. Jordan.
2003. Latent Dirichlet allocation. Journal Mach-
ince Learning Research, 3:993–1022, March.
Johan Bollen, Bruno Gonc¸alves, Guangchen Ruan, and
Huina Mao. 2011a. Happiness is assortative in on-
line social networks. Artificial Life, 17(3):237–251.
Johan Bollen, Huina Mao, and Xiaojun Zeng. 2011b.
Twitter mood predicts the stock market. Journal of
Computational Science, 2(1):1–8.
Johan Bollen, Alberto Pepe, and Huina Mao. 2011c.
Modeling public mood and emotion: Twitter senti-
ment and socio-economic phenomena. In Proceed-
ings of the Fifth International AAAI Conference on
Weblogs and Social Media, pages 450–453.
Gregory K Brown, Aaron T Beck, Robert A Steer, and
Jessica R Grisham. 2000. Risk factors for sui-
cide in psychiatric outpatients: A 20-year prospec-
tive study. Journal of Consulting and Clinical Psy-
chology, 68(3):371.
Ronny Bruffaerts, Koen Demyttenaere, Irving Hwang,
Wai-Tat Chiu, Nancy Sampson, Ronald C Kessler,
Jordi Alonso, Guilherme Borges, Giovanni de Giro-
lamo, Ron de Graaf, et al. 2011. Treatment of suici-
dal people around the world. The British Journal of
Psychiatry, 199(1):64–70.
Rafael A. Calvo and Sidney D’Mello. 2010. Affect
detection: An interdisciplinary review of models,
methods, and their applications. IEEE Transactions
on Affective Computing, 1(1):18–37.
Qijin Cheng, Shu-Sen Chang, and Paul SF Yip.
2012. Opportunities and challenges of online data
collection for suicide prevention. The Lancet,
379(9830):e53–e54.
Alex E Crosby, LaVonne Ortega, and Cindi Melanson.
2011. Self-directed violence surveillance: Uniform
definitions and recommended data elements. Cen-
ters for Disease Control and Prevention, National
Center for Injury Prevention and Control, Division
of Violence Prevention.
Munmun De Choudhury and Scott Counts. 2013. Un-
derstanding affect in the workplace via social media.
In 16th ACM Conference on Computer Supported
Cooperative Work and Social Media (CSCW 2013),
pages 303–316. ACM.
Munmun De Choudhury, Scott Counts, and Michael
Gamon. 2012a. Not all moods are created equal!
Exploring human emotional states in social media.
In 6th International AAAI Conference on Weblogs
and Social Media.
Munmun De Choudhury, Michael Gamon, and Scott
Counts. 2012b. Happy, nervous or surprised? Clas-
sification of human affective states in social media.
In 6th International AAAI Conference on Weblogs
and Social Media.
Munmun De Choudhury, Scott Counts, and Eric
Horvitz. 2013. Major life changes and behavioral
markers in social media: case of childbirth. In Pro-
ceedings of the 2013 conference on Computer sup-
ported cooperative work, pages 1431–1442. ACM.
Munmun De Choudhury, Scott Counts, Eric J Horvitz,
and Aaron Hoff. 2014. Characterizing and pre-
dicting postpartum depression from shared facebook
data. In Proceedings of the 17th ACM conference
on Computer Supported Cooperative Work &amp; Social
Computing, pages 626–638. ACM.
Frank P Deane, Coralie J Wilson, and Joseph Ciarrochi.
2001. Suicidal ideation and help-negation: Not just
hopelessness or prior help. Journal of Clinical Psy-
chology, 57:901–914.
Peter Sheridan Dodds, Kameron Decker Harris, Is-
abel M Kloumann, Catherine A Bliss, and Christo-
pher M Danforth. 2011. Temporal patterns of hap-
piness and information in a global social network:
Hedonometrics and twitter. PloS one, 6(12):e26752.
Bradley N Gaynes, Suzanne L West, Carol A Ford,
Paul Frame, Jonathan Klein, and Kathleen N Lohr.
2004. Screening for suicide risk in adults: A sum-
mary of the evidence for the US Preventive Ser-
vices Task Force. Annals of Internal Medicine,
140(10):822–835.
S.A. Golder and M.W. Macy. 2011. Diurnal and sea-
sonal mood vary with work, sleep, and daylength
across diverse cultures. Science, 333(6051):1878–
1881.
Xuan Guo, Rui Li, Cecilia Ovesdotter Alm, Qi Yu, Jeff
Pelz, Pengcheng Shi, and Anne Haake. 2014. Infus-
ing perceptual expertise and domain knowledge into
a human-centered image retrieval system: A proto-
type application. In Proceedings of the Symposium
on Eye Tracking Research and Applications, pages
275–278. ACM.
Aniko Hannak, Eric Anderson, Lisa Feldman Barrett,
Sune Lehmann, Alan Mislove, and Mirek Riede-
wald. 2012. Tweetin in the rain: Exploring societal-
scale effects of weather on mood. In Proceedings of
the 6th International AAAI Conference on Weblogs
and Social Media (ICWSM12).
Louise Harriss and Keith Hawton. 2005. Suicidal in-
tent in deliberate self-harm and the risk of suicide:
The predictive power of the suicide intent scale.
Journal of Affective Disorders, 86(2):225–233.
</reference>
<page confidence="0.996435">
115
</page>
<reference confidence="0.997272235849057">
Melonie Heron and Betzaida Tejada-Vera. 2009.
Deaths: Leading causes for 2005. National Vital
Statistics Reports: From the Centers for Disease
Control and Prevention, National Center for Health
Statistics, National Vital Statistics System, 58(8):1–
97.
Christopher M Homan, Naiji Lu, Xin Tu, Megan C Ly-
tle, and Vincent Silenzio. 2014. Social structure
and depression in TrevorSpace. In Proceedings of
the 17th ACM Conference on Computer Supported
Cooperative Work &amp; Social Computing, pages 615–
625. ACM.
Lisa M Horowitz and Elizabeth D Ballard. 2009. Sui-
cide screening in schools, primary care and emer-
gency departments. Current Opinion in Pediatrics,
21(5):620–627.
Jared Jashinsky, Scott H Burton, Carl L Hanson, Josh
West, Christophe Giraud-Carrier, Michael D Barnes,
and Trenton Argyle. 2013. Tracking suicide risk
factors through Twitter in the US. Crisis, pages 1–
9.
T. Joachims. 1998. Text categorization with support
vector machines: Learning with many relevant fea-
tures. In European Conference on Machine Learn-
ing (ECML), pages 137–142, Berlin. Springer.
Ronald C Kessler, Guilherme Borges, and Ellen E Wal-
ters. 1999. Prevalence of and risk factors for life-
time suicide attempts in the national comorbidity
survey. Archives of General Psychiatry, 56(7):617–
Suin Kim, J Bak, and Alice Oh. 2012. Do you feel
what I feel? Social aspects of emotions in Twitter
conversations. In Proceedings of the AAAI Interna-
tional Conference on Weblogs and Social Media.
Michael Lehrman, Cecilia Ovesdotter Alm, and Ruben
Proano. 2012. Detecting distressed vs. non-
distressed affect state in short forum texts. In Pro-
ceedings of the Workshop on Language in Social
Media (LSM 2012) at the Conference of the North
American Chapter of the Association for Compu-
tational Linguistics-Human Language Technologies,
Montreal, Canada, pages 9–18.
Rui Li, Jeff Pelz, Pengcheng Shi, and Anne Haake.
2012. Learning image-derived eye movement
patterns to characterize perceptual expertise. In
CogSci, pages 1900–1905.
J John Mann, Christine Waternaux, Gretchen L Haas,
and Kevin M Malone. 1999. Toward a clinical
model of suicidal behavior in psychiatric patients.
American Journal of Psychiatry, 156(2):181–189.
Christopher D. Manning, Prabhakar Raghavan, and
Hinrich Sch¨utze. 2008. Introduction to Information
Retrieval. Cambridge University Press, New York,
NY, USA.
Naoki Masuda, Issei Kurahashi, and Hiroko Onari.
2013. Suicide ideation of individuals in online so-
cial networks. PloS one, 8(4):e62262.
Pawel Matykiewicz, Wlodzislav Duch, and John P.
Pestian. 2009. Clustering semantic spaces of sui-
cide notes and newsgroup articles. In Proceedings of
the Workshop on BioNLP, Boulder, Colorado, pages
179–184.
Saif M Mohammad. 2012. # Emotional tweets. In
Proceedings of the First Joint Conference on Lexical
and Computational Semantics-Volume 1: Proceed-
ings of the main conference and the shared task, and
Volume 2: Proceedings of the Sixth International
Workshop on Semantic Evaluation, pages 246–255.
Association for Computational Linguistics.
Matthew K Nock, Guilherme Borges, Evelyn J Bromet,
Christine B Cha, Ronald C Kessler, and Sing Lee.
2008. Suicide and suicidal behavior. Epidemiologic
Reviews, 30(1):133–154.
Matthew K Nock, Jennifer M Park, Christine T Finn,
Tara L Deliberto, Halina J Dour, and Mahzarin R
Banaji. 2010. Measuring the suicidal mind implicit
cognition predicts suicidal behavior. Psychological
Science, 21(4):511–517.
Alexander Pak and Patrick Paroubek. 2010. Twitter
as a corpus for sentiment analysis and opinion min-
ing. In Proceedings of Conference on Language Re-
sources and Evaluation.
James W Pennebaker, Martha E Francis, and Roger J
Booth. 2001. Linguistic inquiry and word count:
Liwc 2001. Mahway: Lawrence Erlbaum Asso-
ciates, 71:2001.
John P. Pestian, Pawel Matykiewicz, and Jacqueline
Grupp-Phelan. 2008. Using natural language pro-
cessing to classify suicide notes. In BioNLP 2008:
Current Trends in Biomedical Natural Language
Processing, Columbus, Ohio, pages 96–97.
Ren´e Pfitzner, Antonios Garas, and Frank Schweitzer.
2012. Emotional divergence influences information
spreading in twitter. Proceedings of the 6th Inter-
national AAAI Conference on Weblogs and Social
Media (ICWSM12), pages 2–5.
Rosalind W. Picard. 1997. Affective Computing. MIT
Press, Cambridge, MA, USA.
Megan L Ryan, Ian M Shochet, and Helen M Stallman.
2010. Universal online interventions might engage
psychologically distressed university students who
are unlikely to seek formal help. Advances in Men-
tal Health, 9(1):73–83.
Adam Sadilek, Henry A Kautz, and Vincent Silenzio.
2012. Predicting disease transmission from geo-
tagged micro-blog data. In Association for the Ad-
vancement of Articial Intelligence.
</reference>
<page confidence="0.989716">
116
</page>
<reference confidence="0.999636264705882">
Adam Sadilek, Christopher Homan, Walter S. Lasecki,
Vincent Silenzio, and Henry Kautz. 2014. Mod-
eling fine-grained dynamics of mood at scale. In
WSDM 2014 Workshop on Diffusion Networks and
Cascade Analytics.
David Shaffer, Michelle Scott, Holly Wilcox, Carey
Maslow, Roger Hicks, Christopher P Lucas, Robin
Garfinkel, and Steven Greenwald. 2004. The
Columbia SuicideScreen: Validity and reliability of
a screen for youth suicide and depression. Journal of
the American Academy of Child &amp; Adolescent Psy-
chiatry, 43(1):71–79.
Mike Thelwall, Kevan Buckley, and Georgios Pal-
toglou. 2011. Sentiment in Twitter events. Journal
of the American Society for Information Science and
Technology, 62(2):406–418.
Wenbo Wang, Lu Chen, Krishnaprasad Thirunarayan,
and Amit P Sheth. 2012. Harnessing Twit-
ter ‘Big Data’ for automatic emotion identifica-
tion. In Privacy, Security, Risk and Trust (PASSAT),
2012 International Conference on and 2012 Inter-
national Confernece on Social Computing (Social-
Com), pages 587–592. IEEE.
Kathryn Womack, Wilson McCoy, Cecilia Ovesdot-
ter Alm, Cara Calvelli, Jeff B. Pelz, Pengcheng
Shi, and Anne Haake. 2012. Disfluencies as
extra-propositional indicators of cognitive process-
ing. In Proceedings of the Workshop on Extra-
Propositional Aspects of Meaning in Computational
Linguistics, pages 1–9. Association for Computa-
tional Linguistics.
Matt Wray, Cynthia Colen, and Bernice Pescosolido.
2011. The sociology of suicide. Annual Review of
Sociology, 37:505–528.
</reference>
<page confidence="0.99814">
117
</page>
</variant>
</algorithm>
<algorithm name="ParsHed" version="110505">
<variant no="0" confidence="0.256698">
<title confidence="0.988002">Toward Macro-Insights for Suicide Analyzing Fine-Grained Distress at Scale</title>
<abstract confidence="0.943362884615385">M. O. College of Computing and Information Sciences, Rochester Institute of of Psychiatry, University of Rochester Medical 3College of Liberal Arts, Rochester Institute of  || Abstract Suicide is a leading cause of death in the United States. One of the major challenges to suicide prevention is that those who may be most at risk cannot be relied upon to report their conditions to clinicians. This paper takes an initial step toward the automatic detection of suicidal risk factors through social media activity, with no reliance on self-reporting. We consider the performance of annotators with various degrees of expertise in suicide prevention at annotating microblog data for the purpose of training text-based models for detecting suicide risk behaviors. Consistent with crowdsourcing literature, we found that novice-novice annotator pairs underperform expert annotators and outperform automatic lexical analysis tools, such as Linguistic Inquiry and Word</abstract>
<intro confidence="0.393737">Count.</intro>
</variant>
</algorithm>
<algorithm name="ParsCit" version="110505">
<citationList>
<citation valid="true">
<authors>
<author>Cecilia Ovesdotter Alm</author>
</authors>
<title>Affect in Text and Speech.</title>
<date>2008</date>
<tech>Ph.D. thesis,</tech>
<institution>University of Illinois at Urbana Champaign.</institution>
<contexts>
<context position="10672" citStr="Alm, 2008" startWordPosition="1649" endWordPosition="1650">alvo and D’Mello, 2010). Clearly, emotion is a key element in the human experience, but it is notoriously difficult to pin down and scholars in the affective sciences lack a single agreed-upon definition for emotion. Accordingly, different theoretical constructs have been proposed to describe affect and affect-related behaviors (Picard, 1997). In addition, research on affect in language has shown that such phenomena tend to be subjective, lack real ground truth (often resulting in moderate kappa scores), and have particularly fuzzy semantics in the gray zone where neutrality and emotion meet (Alm, 2008). These kinds of problem characteristics bring with them their own set of demanding challenges from a computational perspective (Alm, 2011). Yet, the nature of such problems make them incredibly important to study, despite the challenges involved. Sentiment analysis has been widely studied in a number of computational settings, including on various social networking sites. A rather substantial body of work already exists on the use of Twitter to study emotion (Bollen et al., 2011b; Dodds et al., 2011; Wang et al., 2012; Pfitzner et al., 2012; Kim et al., 2012; Bollen et al., 2011a; Pfitzner et</context>
</contexts>
<marker>Alm, 2008</marker>
<rawString>Cecilia Ovesdotter Alm. 2008. Affect in Text and Speech. Ph.D. thesis, University of Illinois at Urbana Champaign.</rawString>
</citation>
<citation valid="true">
<authors>
<author>Cecilia Ovesdotter Alm</author>
</authors>
<title>Subjective natural language problems: Motivations, applications, characterizations, and implications.</title>
<date>2011</date>
<booktitle>In Proceedings of 49th Annual Meeting of the Assoc. for Computational Linguistics: Human Language Technologies,</booktitle>
<pages>107--112</pages>
<location>Portland, OR,</location>
<contexts>
<context position="10811" citStr="Alm, 2011" startWordPosition="1669" endWordPosition="1670">s in the affective sciences lack a single agreed-upon definition for emotion. Accordingly, different theoretical constructs have been proposed to describe affect and affect-related behaviors (Picard, 1997). In addition, research on affect in language has shown that such phenomena tend to be subjective, lack real ground truth (often resulting in moderate kappa scores), and have particularly fuzzy semantics in the gray zone where neutrality and emotion meet (Alm, 2008). These kinds of problem characteristics bring with them their own set of demanding challenges from a computational perspective (Alm, 2011). Yet, the nature of such problems make them incredibly important to study, despite the challenges involved. Sentiment analysis has been widely studied in a number of computational settings, including on various social networking sites. A rather substantial body of work already exists on the use of Twitter to study emotion (Bollen et al., 2011b; Dodds et al., 2011; Wang et al., 2012; Pfitzner et al., 2012; Kim et al., 2012; Bollen et al., 2011a; Pfitzner et al., 2012; Bollen et al., 2011c; Mohammad, 2012; Golder and Macy, 2011; De Choudhury et al., 2012a; De Choudhury et al., 2012b; De Choudhu</context>
</contexts>
<marker>Alm, 2011</marker>
<rawString>Cecilia Ovesdotter Alm. 2011. Subjective natural language problems: Motivations, applications, characterizations, and implications. In Proceedings of 49th Annual Meeting of the Assoc. for Computational Linguistics: Human Language Technologies, Portland, OR, pages 107–112.</rawString>
</citation>
<citation valid="true">
<authors>
<author>David M Blei</author>
<author>Andrew Y Ng</author>
<author>Michael I Jordan</author>
</authors>
<title>Latent Dirichlet allocation.</title>
<date>2003</date>
<journal>Journal Machince Learning Research,</journal>
<pages>3--993</pages>
<contexts>
<context position="19127" citStr="Blei et al., 2003" startWordPosition="3044" endWordPosition="3047">ress LD low distress HD high distress Table 3: Distress-related categories used to annotate the tweets. so happy” was represented as the following feature vector: {I, am, so, happy, I am, am so, so happy, I am so, am so happy}. Each feature is associated with its tf-idf score (Manning et al., 2008). We performed topic modeling on our dataset. A topic is a set of lexical items that are likely to occur in the same tweet. Topic models are capable of associating words with similar meanings and distinguishing among the different meanings of a single word. We used latent Dirichlet allocation (LDA) (Blei et al., 2003) to create these topics. Before doing so, we removed stop words and words that occur only once in the dataset. We then applied LDA algorithm on the data to discover three topics using 100 iterations. We used support vector machines (SVMs) (Joachims, 1998), a machine learning method that is used to train a classification model that can assign class labels to previously unseen tweets, to assess the power of our annotations. SVMs treat each tweet as a point in an extremely high dimensional space (one dimension per uni-, bi-, and trigram in the corpus). SVMs are a form of linear separator that can</context>
</contexts>
<marker>Blei, Ng, Jordan, 2003</marker>
<rawString>David M. Blei, Andrew Y. Ng, and Michael I. Jordan. 2003. Latent Dirichlet allocation. Journal Machince Learning Research, 3:993–1022, March.</rawString>
</citation>
<citation valid="true">
<authors>
<author>Johan Bollen</author>
<author>Bruno Gonc¸alves</author>
<author>Guangchen Ruan</author>
<author>Huina Mao</author>
</authors>
<title>Happiness is assortative in online social networks.</title>
<date>2011</date>
<journal>Artificial Life,</journal>
<volume>17</volume>
<issue>3</issue>
<marker>Bollen, Gonc¸alves, Ruan, Mao, 2011</marker>
<rawString>Johan Bollen, Bruno Gonc¸alves, Guangchen Ruan, and Huina Mao. 2011a. Happiness is assortative in online social networks. Artificial Life, 17(3):237–251.</rawString>
</citation>
<citation valid="true">
<authors>
<author>Johan Bollen</author>
<author>Huina Mao</author>
<author>Xiaojun Zeng</author>
</authors>
<title>Twitter mood predicts the stock market.</title>
<date>2011</date>
<journal>Journal of Computational Science,</journal>
<volume>2</volume>
<issue>1</issue>
<contexts>
<context position="11156" citStr="Bollen et al., 2011" startWordPosition="1723" endWordPosition="1726">sulting in moderate kappa scores), and have particularly fuzzy semantics in the gray zone where neutrality and emotion meet (Alm, 2008). These kinds of problem characteristics bring with them their own set of demanding challenges from a computational perspective (Alm, 2011). Yet, the nature of such problems make them incredibly important to study, despite the challenges involved. Sentiment analysis has been widely studied in a number of computational settings, including on various social networking sites. A rather substantial body of work already exists on the use of Twitter to study emotion (Bollen et al., 2011b; Dodds et al., 2011; Wang et al., 2012; Pfitzner et al., 2012; Kim et al., 2012; Bollen et al., 2011a; Pfitzner et al., 2012; Bollen et al., 2011c; Mohammad, 2012; Golder and Macy, 2011; De Choudhury et al., 2012a; De Choudhury et al., 2012b; De Choudhury et al., 2013; De Choudhury and Counts, 2013; Hannak et al., 2012; Thelwall et al., 2011; Pak and Paroubek, 2010). For instance, Golder and and Macy study aggregate global trends in “mood,” and show, among other things, that people wake up in a relatively good mood that decays as the day progresses (Golder and Macy, 2011). Bollen et al. (201</context>
</contexts>
<marker>Bollen, Mao, Zeng, 2011</marker>
<rawString>Johan Bollen, Huina Mao, and Xiaojun Zeng. 2011b. Twitter mood predicts the stock market. Journal of Computational Science, 2(1):1–8.</rawString>
</citation>
<citation valid="true">
<authors>
<author>Johan Bollen</author>
<author>Alberto Pepe</author>
<author>Huina Mao</author>
</authors>
<title>Modeling public mood and emotion: Twitter sentiment and socio-economic phenomena.</title>
<date>2011</date>
<booktitle>In Proceedings of the Fifth International AAAI Conference on Weblogs and Social Media,</booktitle>
<pages>450--453</pages>
<contexts>
<context position="11156" citStr="Bollen et al., 2011" startWordPosition="1723" endWordPosition="1726">sulting in moderate kappa scores), and have particularly fuzzy semantics in the gray zone where neutrality and emotion meet (Alm, 2008). These kinds of problem characteristics bring with them their own set of demanding challenges from a computational perspective (Alm, 2011). Yet, the nature of such problems make them incredibly important to study, despite the challenges involved. Sentiment analysis has been widely studied in a number of computational settings, including on various social networking sites. A rather substantial body of work already exists on the use of Twitter to study emotion (Bollen et al., 2011b; Dodds et al., 2011; Wang et al., 2012; Pfitzner et al., 2012; Kim et al., 2012; Bollen et al., 2011a; Pfitzner et al., 2012; Bollen et al., 2011c; Mohammad, 2012; Golder and Macy, 2011; De Choudhury et al., 2012a; De Choudhury et al., 2012b; De Choudhury et al., 2013; De Choudhury and Counts, 2013; Hannak et al., 2012; Thelwall et al., 2011; Pak and Paroubek, 2010). For instance, Golder and and Macy study aggregate global trends in “mood,” and show, among other things, that people wake up in a relatively good mood that decays as the day progresses (Golder and Macy, 2011). Bollen et al. (201</context>
</contexts>
<marker>Bollen, Pepe, Mao, 2011</marker>
<rawString>Johan Bollen, Alberto Pepe, and Huina Mao. 2011c. Modeling public mood and emotion: Twitter sentiment and socio-economic phenomena. In Proceedings of the Fifth International AAAI Conference on Weblogs and Social Media, pages 450–453.</rawString>
</citation>
<citation valid="true">
<authors>
<author>Gregory K Brown</author>
<author>Aaron T Beck</author>
<author>Robert A Steer</author>
<author>Jessica R Grisham</author>
</authors>
<title>Risk factors for suicide in psychiatric outpatients: A 20-year prospective study.</title>
<date>2000</date>
<journal>Journal of Consulting and Clinical Psychology,</journal>
<volume>68</volume>
<issue>3</issue>
<contexts>
<context position="8582" citStr="Brown et al., 2000" startWordPosition="1320" endWordPosition="1323"> increased exponentially when individuals endorsed three or more risk factors, e.g., having a mood or substance abuse disorder (Kessler et al., 1999). Demographics, previous suicide attempts, mental health concerns (i.e., depression, substance abuse, suicidal ideation, self-harm, or impulsivity), family history of suicide, interpersonal conflicts (i.e., family violence or bullying), and means for suicidal behavior (e.g., firearms), are commonly cited risk factors for suicidal behavior (Nock et al., 2008; Crosby et al., 2011; Gaynes et al., 2004; Harriss and Hawton, 2005; Shaffer et al., 2004; Brown et al., 2000). Regarding the use of annotation for predictive modeling, evidence suggests that when it comes to judgments that involve clinical phenomena, experts and novices behave differently (Li et al., 2012; Womack et al., 2012). Such distinctions intuitively make sense, as the learning of medical domain knowledge requires advanced education in 108 conjunction with substantial practical field experience. In a task such as medical image inspection, the subtle cues that point an observer to evidence that allow them to identify a clinical condition, while accessible to experts with training and perceptual</context>
</contexts>
<marker>Brown, Beck, Steer, Grisham, 2000</marker>
<rawString>Gregory K Brown, Aaron T Beck, Robert A Steer, and Jessica R Grisham. 2000. Risk factors for suicide in psychiatric outpatients: A 20-year prospective study. Journal of Consulting and Clinical Psychology, 68(3):371.</rawString>
</citation>
<citation valid="true">
<authors>
<author>Ronny Bruffaerts</author>
<author>Koen Demyttenaere</author>
<author>Irving Hwang</author>
<author>Wai-Tat Chiu</author>
<author>Nancy Sampson</author>
<author>Ronald C Kessler</author>
<author>Jordi Alonso</author>
<author>Guilherme Borges</author>
<author>Giovanni de Girolamo</author>
<author>Ron de Graaf</author>
</authors>
<title>Treatment of suicidal people around the world.</title>
<date>2011</date>
<journal>The British Journal of Psychiatry,</journal>
<volume>199</volume>
<issue>1</issue>
<marker>Bruffaerts, Demyttenaere, Hwang, Chiu, Sampson, Kessler, Alonso, Borges, de Girolamo, de Graaf, 2011</marker>
<rawString>Ronny Bruffaerts, Koen Demyttenaere, Irving Hwang, Wai-Tat Chiu, Nancy Sampson, Ronald C Kessler, Jordi Alonso, Guilherme Borges, Giovanni de Girolamo, Ron de Graaf, et al. 2011. Treatment of suicidal people around the world. The British Journal of Psychiatry, 199(1):64–70.</rawString>
</citation>
<citation valid="true">
<authors>
<author>Rafael A Calvo</author>
<author>Sidney D’Mello</author>
</authors>
<title>Affect detection: An interdisciplinary review of models, methods, and their applications.</title>
<date>2010</date>
<journal>IEEE Transactions on Affective Computing,</journal>
<volume>1</volume>
<issue>1</issue>
<marker>Calvo, D’Mello, 2010</marker>
<rawString>Rafael A. Calvo and Sidney D’Mello. 2010. Affect detection: An interdisciplinary review of models, methods, and their applications. IEEE Transactions on Affective Computing, 1(1):18–37.</rawString>
</citation>
<citation valid="true">
<authors>
<author>Qijin Cheng</author>
<author>Shu-Sen Chang</author>
<author>Paul SF Yip</author>
</authors>
<title>Opportunities and challenges of online data collection for suicide prevention.</title>
<date>2012</date>
<journal>The Lancet,</journal>
<volume>379</volume>
<issue>9830</issue>
<contexts>
<context position="12098" citStr="Cheng et al. (2012)" startWordPosition="1888" endWordPosition="1891"> Thelwall et al., 2011; Pak and Paroubek, 2010). For instance, Golder and and Macy study aggregate global trends in “mood,” and show, among other things, that people wake up in a relatively good mood that decays as the day progresses (Golder and Macy, 2011). Bollen et al. (2011c) show that tweets from users who took a standard diagnostic instrument for mood are often tied to current events, such as elections and holidays. Relatively little of this work has focused on suicide or related psychological conditions. Masuda et al. (2013) study suicide on mixi (a Japanese social networking service). Cheng et al. (2012) consider the ethical and political implications of online data collection for suicide prevention. Jashinsky et al. (2013) show correlations between frequency in tweets related to suicide and actual suicide in the 50 United States of America. Sadilek et al. (2014) study depression on Twitter. De Choudhury and collaborators studied depression—in general and post-partum—in Twitter (De Choudhury et al., 2012a; De Choudhury et al., 2012b; De Choudhury et al., 2013; De Choudhury and Counts, 2013) and Facebook (De Choudhury et al., 2014). Homan et al. (2014) investigate depression in TrevorSpace. A </context>
</contexts>
<marker>Cheng, Chang, Yip, 2012</marker>
<rawString>Qijin Cheng, Shu-Sen Chang, and Paul SF Yip. 2012. Opportunities and challenges of online data collection for suicide prevention. The Lancet, 379(9830):e53–e54.</rawString>
</citation>
<citation valid="true">
<authors>
<author>Alex E Crosby</author>
<author>LaVonne Ortega</author>
<author>Cindi Melanson</author>
</authors>
<title>Self-directed violence surveillance: Uniform definitions and recommended data elements. Centers for Disease Control and Prevention, National Center for Injury Prevention and Control, Division of Violence Prevention.</title>
<date>2011</date>
<contexts>
<context position="1974" citStr="Crosby et al., 2011" startWordPosition="296" endWordPosition="299">uses of death for individuals 10–44 years of age in the United States (Heron and Tejada-Vera, 2009). Indeed, while mortality rates for most illnesses decreased between 2008 and 2009, the rate of suicide increased by 2.4% (Heron and Tejada-Vera, 2009). The lifetime prevalence for suicidal ideation is 5.6–14.3% in the general population, and as high as 19.8– 24.0% among youth (Nock et al., 2008). The first step toward suicide prevention is to identify, ideally in consultation with clinical experts, the risk factors associated with suicide. Due to social stigma among other sociocultural factors (Crosby et al., 2011), individuals at risk for committing suicide may not always reach out to professionals or, if they do, provide them with accurate information. They may not even realize their own level of suicide risk before it is too late. Self-reporting, then, is not an entirely reliable means of detecting and assessing suicide risk, and research on suicide prevention can benefit from also exploring other channels for assessing risk. For instance, individuals may be more inclined to seek support from informal resources, such as social media, instead of seeking treatment (Crosby et al., 2011; Bruffaerts et al</context>
<context position="7266" citStr="Crosby et al., 2011" startWordPosition="1118" endWordPosition="1121">weet. Clinical expert annotation, rather than generalpurpose tools for content and sentiment analysis such as LIWC (Linguistic Inquiry and Word Count) by Pennebaker et al. (2001), provides a basis for text-based statistical modeling. We show that expertise-based keyword retrieval, departing from knowledge about contributing risk factors, results in better interannotator agreement in both novice-novice and novice-expert annotation when the keywords reflect the task at hand. 2 Related Work Data on suicide traditionally comes from healthcare organizations, large-scale studies, or self reporting (Crosby et al., 2011; Horowitz and Ballard, 2009). These sources are limited by sociocultural barriers (Crosby et al., 2011), such as stigma and shame. Moreover, data on suicide is never particularly reliable because suicide is a fundamentally subjective, complex phenomenon with a low base rate. For these reasons, many researchers tend to focus on the relationship between risk factors and suicidal behavior, without relying heavily on theoretical models (Nock et al., 2008). Approximately one-third of all individuals who reported suicidal ideation in their lifetime made a plan to commit suicide. Nearly three-quarte</context>
<context position="8492" citStr="Crosby et al., 2011" startWordPosition="1304" endWordPosition="1307">hose who reported making a suicide plan actually attempted. The odds of attempting suicide increased exponentially when individuals endorsed three or more risk factors, e.g., having a mood or substance abuse disorder (Kessler et al., 1999). Demographics, previous suicide attempts, mental health concerns (i.e., depression, substance abuse, suicidal ideation, self-harm, or impulsivity), family history of suicide, interpersonal conflicts (i.e., family violence or bullying), and means for suicidal behavior (e.g., firearms), are commonly cited risk factors for suicidal behavior (Nock et al., 2008; Crosby et al., 2011; Gaynes et al., 2004; Harriss and Hawton, 2005; Shaffer et al., 2004; Brown et al., 2000). Regarding the use of annotation for predictive modeling, evidence suggests that when it comes to judgments that involve clinical phenomena, experts and novices behave differently (Li et al., 2012; Womack et al., 2012). Such distinctions intuitively make sense, as the learning of medical domain knowledge requires advanced education in 108 conjunction with substantial practical field experience. In a task such as medical image inspection, the subtle cues that point an observer to evidence that allow them </context>
<context position="15452" citStr="Crosby et al., 2011" startWordPosition="2428" endWordPosition="2431">mpling randomly from among the 2,000 tweets with the highest LIWC sad score. LIWC has been widely used to estimate emotion in online social networks, and specifically to mood on Twitter. This slight amount of randomness in filtering tweets this way was intended to avoid selecting obvious false positives, such as the use of “sad” in nicknames. Next, we adopted a collection of inclusive search terms/phrases from Jashinsky et al. (2013), which was designed specifically for capturing tweets related to suicide risk factors, and applied them to our source corpus. We added to these more terms, from (Crosby et al., 2011) (see Table 2). These terms yielded 630 tweets. 1http://www.noslang.com/dictionary depressive tired of living, leave this world, feeling wanna die, hate my job, feeling guilty, deserve to die, desire to end own life, feeling ignored, tired of everything, feeling blue, have blues depression sleeping pill, have insomnia, symptoms sleep forever, sleep disorder drug clonazepam, drug overdose, abuse imipramine prior suicide tried suicide attempts suicide commit suicide, ideation committing suicide, feeling suicidal, want to suicide, shoot myself, a gun to head, hang myself, intention to die self hu</context>
</contexts>
<marker>Crosby, Ortega, Melanson, 2011</marker>
<rawString>Alex E Crosby, LaVonne Ortega, and Cindi Melanson. 2011. Self-directed violence surveillance: Uniform definitions and recommended data elements. Centers for Disease Control and Prevention, National Center for Injury Prevention and Control, Division of Violence Prevention.</rawString>
</citation>
<citation valid="true">
<authors>
<author>Munmun De Choudhury</author>
<author>Scott Counts</author>
</authors>
<title>Understanding affect in the workplace via social media.</title>
<date>2013</date>
<booktitle>In 16th ACM Conference on Computer Supported Cooperative Work and Social Media (CSCW 2013),</booktitle>
<pages>303--316</pages>
<publisher>ACM.</publisher>
<marker>De Choudhury, Counts, 2013</marker>
<rawString>Munmun De Choudhury and Scott Counts. 2013. Understanding affect in the workplace via social media. In 16th ACM Conference on Computer Supported Cooperative Work and Social Media (CSCW 2013), pages 303–316. ACM.</rawString>
</citation>
<citation valid="true">
<authors>
<author>Munmun De Choudhury</author>
<author>Scott Counts</author>
<author>Michael Gamon</author>
</authors>
<title>Not all moods are created equal! Exploring human emotional states in social media.</title>
<date>2012</date>
<booktitle>In 6th International AAAI Conference on Weblogs and Social Media.</booktitle>
<marker>De Choudhury, Counts, Gamon, 2012</marker>
<rawString>Munmun De Choudhury, Scott Counts, and Michael Gamon. 2012a. Not all moods are created equal! Exploring human emotional states in social media. In 6th International AAAI Conference on Weblogs and Social Media.</rawString>
</citation>
<citation valid="true">
<authors>
<author>Munmun De Choudhury</author>
<author>Michael Gamon</author>
<author>Scott Counts</author>
</authors>
<title>Happy, nervous or surprised? Classification of human affective states in social media.</title>
<date>2012</date>
<booktitle>In 6th International AAAI Conference on Weblogs and Social Media.</booktitle>
<marker>De Choudhury, Gamon, Counts, 2012</marker>
<rawString>Munmun De Choudhury, Michael Gamon, and Scott Counts. 2012b. Happy, nervous or surprised? Classification of human affective states in social media. In 6th International AAAI Conference on Weblogs and Social Media.</rawString>
</citation>
<citation valid="true">
<authors>
<author>Munmun De Choudhury</author>
<author>Scott Counts</author>
<author>Eric Horvitz</author>
</authors>
<title>Major life changes and behavioral markers in social media: case of childbirth.</title>
<date>2013</date>
<booktitle>In Proceedings of the 2013 conference on Computer supported cooperative work,</booktitle>
<pages>1431--1442</pages>
<publisher>ACM.</publisher>
<marker>De Choudhury, Counts, Horvitz, 2013</marker>
<rawString>Munmun De Choudhury, Scott Counts, and Eric Horvitz. 2013. Major life changes and behavioral markers in social media: case of childbirth. In Proceedings of the 2013 conference on Computer supported cooperative work, pages 1431–1442. ACM.</rawString>
</citation>
<citation valid="true">
<authors>
<author>Munmun De Choudhury</author>
<author>Scott Counts</author>
<author>Eric J Horvitz</author>
<author>Aaron Hoff</author>
</authors>
<title>Characterizing and predicting postpartum depression from shared facebook data.</title>
<date>2014</date>
<booktitle>In Proceedings of the 17th ACM conference on Computer Supported Cooperative Work &amp; Social Computing,</booktitle>
<pages>626--638</pages>
<publisher>ACM.</publisher>
<marker>De Choudhury, Counts, Horvitz, Hoff, 2014</marker>
<rawString>Munmun De Choudhury, Scott Counts, Eric J Horvitz, and Aaron Hoff. 2014. Characterizing and predicting postpartum depression from shared facebook data. In Proceedings of the 17th ACM conference on Computer Supported Cooperative Work &amp; Social Computing, pages 626–638. ACM.</rawString>
</citation>
<citation valid="true">
<authors>
<author>Frank P Deane</author>
<author>Coralie J Wilson</author>
<author>Joseph Ciarrochi</author>
</authors>
<title>Suicidal ideation and help-negation: Not just hopelessness or prior help.</title>
<date>2001</date>
<journal>Journal of Clinical Psychology,</journal>
<pages>57--901</pages>
<contexts>
<context position="2865" citStr="Deane et al., 2001" startWordPosition="439" endWordPosition="442">e means of detecting and assessing suicide risk, and research on suicide prevention can benefit from also exploring other channels for assessing risk. For instance, individuals may be more inclined to seek support from informal resources, such as social media, instead of seeking treatment (Crosby et al., 2011; Bruffaerts et al., 2011; Ryan et al., 2010). Evidence suggests that youth and emerging adults usually prefer to seek help from their friends and families; however, higher levels of suicidal ideation are associated with lower levels of help-seeking from both formal or informal resources (Deane et al., 2001). These patterns in help-seeking behavior suggest that social media might be an important channel for discovering those at risk for— and even preventing—suicide. Internet- and telecommunications-driven activity is revolutionizing the social sciences by providing data, much of it publicly available, on human activity in situ, at volumes and a level of time and space granularity never before approached. Can such data improve clinical preventative study and measures by providing access to at-risk individuals who would otherwise go undetected, and by leading to better science about suicide risk be</context>
<context position="31924" citStr="Deane et al., 2001" startWordPosition="5244" endWordPosition="5247">ative rareness by which suicide occurs, and by the inherent subjectivity of the act, which makes any data on suicide fuzzy. We hope to explore in future work the relationship between clinical observation in both on- and off-line settings and selfreporting, including the integration of natural language data of patients from clinical settings. We also hope to explore distress annotation from different perspectives and levels of context. Higher levels of suicidal ideation have an inverse relationship with all types of help-seeking and a positive correlation with the decision to not seek support (Deane et al., 2001). Thus, we would expect suicidal individuals to generally be less active on social media than those who are not. Nevertheless, a number of studies have shown a positive correlation between online social network use and negative mood. Perhaps this means in part that individuals who are depressed are slower to disengage on- rather than off-line. 6 Conclusion We studied the performance of different approaches to training systems to detect evidence of suicide risk behavior in microblog data. We showed that both the methods used to automatically collect training sets, as well as the expertise level</context>
</contexts>
<marker>Deane, Wilson, Ciarrochi, 2001</marker>
<rawString>Frank P Deane, Coralie J Wilson, and Joseph Ciarrochi. 2001. Suicidal ideation and help-negation: Not just hopelessness or prior help. Journal of Clinical Psychology, 57:901–914.</rawString>
</citation>
<citation valid="true">
<authors>
<author>Peter Sheridan Dodds</author>
<author>Kameron Decker Harris</author>
<author>Isabel M Kloumann</author>
<author>Catherine A Bliss</author>
<author>Christopher M Danforth</author>
</authors>
<title>Temporal patterns of happiness and information in a global social network: Hedonometrics and twitter.</title>
<date>2011</date>
<journal>PloS one,</journal>
<pages>6--12</pages>
<contexts>
<context position="11177" citStr="Dodds et al., 2011" startWordPosition="1727" endWordPosition="1730">ppa scores), and have particularly fuzzy semantics in the gray zone where neutrality and emotion meet (Alm, 2008). These kinds of problem characteristics bring with them their own set of demanding challenges from a computational perspective (Alm, 2011). Yet, the nature of such problems make them incredibly important to study, despite the challenges involved. Sentiment analysis has been widely studied in a number of computational settings, including on various social networking sites. A rather substantial body of work already exists on the use of Twitter to study emotion (Bollen et al., 2011b; Dodds et al., 2011; Wang et al., 2012; Pfitzner et al., 2012; Kim et al., 2012; Bollen et al., 2011a; Pfitzner et al., 2012; Bollen et al., 2011c; Mohammad, 2012; Golder and Macy, 2011; De Choudhury et al., 2012a; De Choudhury et al., 2012b; De Choudhury et al., 2013; De Choudhury and Counts, 2013; Hannak et al., 2012; Thelwall et al., 2011; Pak and Paroubek, 2010). For instance, Golder and and Macy study aggregate global trends in “mood,” and show, among other things, that people wake up in a relatively good mood that decays as the day progresses (Golder and Macy, 2011). Bollen et al. (2011c) show that tweets </context>
</contexts>
<marker>Dodds, Harris, Kloumann, Bliss, Danforth, 2011</marker>
<rawString>Peter Sheridan Dodds, Kameron Decker Harris, Isabel M Kloumann, Catherine A Bliss, and Christopher M Danforth. 2011. Temporal patterns of happiness and information in a global social network: Hedonometrics and twitter. PloS one, 6(12):e26752.</rawString>
</citation>
<citation valid="true">
<authors>
<author>Bradley N Gaynes</author>
<author>Suzanne L West</author>
<author>Carol A Ford</author>
<author>Paul Frame</author>
<author>Jonathan Klein</author>
<author>Kathleen N Lohr</author>
</authors>
<title>Screening for suicide risk in adults: A summary of the evidence for the US Preventive Services Task Force.</title>
<date>2004</date>
<journal>Annals of Internal Medicine,</journal>
<volume>140</volume>
<issue>10</issue>
<contexts>
<context position="8513" citStr="Gaynes et al., 2004" startWordPosition="1308" endWordPosition="1311">ing a suicide plan actually attempted. The odds of attempting suicide increased exponentially when individuals endorsed three or more risk factors, e.g., having a mood or substance abuse disorder (Kessler et al., 1999). Demographics, previous suicide attempts, mental health concerns (i.e., depression, substance abuse, suicidal ideation, self-harm, or impulsivity), family history of suicide, interpersonal conflicts (i.e., family violence or bullying), and means for suicidal behavior (e.g., firearms), are commonly cited risk factors for suicidal behavior (Nock et al., 2008; Crosby et al., 2011; Gaynes et al., 2004; Harriss and Hawton, 2005; Shaffer et al., 2004; Brown et al., 2000). Regarding the use of annotation for predictive modeling, evidence suggests that when it comes to judgments that involve clinical phenomena, experts and novices behave differently (Li et al., 2012; Womack et al., 2012). Such distinctions intuitively make sense, as the learning of medical domain knowledge requires advanced education in 108 conjunction with substantial practical field experience. In a task such as medical image inspection, the subtle cues that point an observer to evidence that allow them to identify a clinica</context>
</contexts>
<marker>Gaynes, West, Ford, Frame, Klein, Lohr, 2004</marker>
<rawString>Bradley N Gaynes, Suzanne L West, Carol A Ford, Paul Frame, Jonathan Klein, and Kathleen N Lohr. 2004. Screening for suicide risk in adults: A summary of the evidence for the US Preventive Services Task Force. Annals of Internal Medicine, 140(10):822–835.</rawString>
</citation>
<citation valid="true">
<authors>
<author>S A Golder</author>
<author>M W Macy</author>
</authors>
<title>Diurnal and seasonal mood vary with work, sleep, and daylength across diverse cultures.</title>
<date>2011</date>
<journal>Science,</journal>
<volume>333</volume>
<issue>6051</issue>
<pages>1881</pages>
<contexts>
<context position="11343" citStr="Golder and Macy, 2011" startWordPosition="1758" endWordPosition="1761">h them their own set of demanding challenges from a computational perspective (Alm, 2011). Yet, the nature of such problems make them incredibly important to study, despite the challenges involved. Sentiment analysis has been widely studied in a number of computational settings, including on various social networking sites. A rather substantial body of work already exists on the use of Twitter to study emotion (Bollen et al., 2011b; Dodds et al., 2011; Wang et al., 2012; Pfitzner et al., 2012; Kim et al., 2012; Bollen et al., 2011a; Pfitzner et al., 2012; Bollen et al., 2011c; Mohammad, 2012; Golder and Macy, 2011; De Choudhury et al., 2012a; De Choudhury et al., 2012b; De Choudhury et al., 2013; De Choudhury and Counts, 2013; Hannak et al., 2012; Thelwall et al., 2011; Pak and Paroubek, 2010). For instance, Golder and and Macy study aggregate global trends in “mood,” and show, among other things, that people wake up in a relatively good mood that decays as the day progresses (Golder and Macy, 2011). Bollen et al. (2011c) show that tweets from users who took a standard diagnostic instrument for mood are often tied to current events, such as elections and holidays. Relatively little of this work has foc</context>
</contexts>
<marker>Golder, Macy, 2011</marker>
<rawString>S.A. Golder and M.W. Macy. 2011. Diurnal and seasonal mood vary with work, sleep, and daylength across diverse cultures. Science, 333(6051):1878– 1881.</rawString>
</citation>
<citation valid="true">
<authors>
<author>Xuan Guo</author>
<author>Rui Li</author>
<author>Cecilia Ovesdotter Alm</author>
<author>Qi Yu</author>
<author>Jeff Pelz</author>
<author>Pengcheng Shi</author>
<author>Anne Haake</author>
</authors>
<title>Infusing perceptual expertise and domain knowledge into a human-centered image retrieval system: A prototype application.</title>
<date>2014</date>
<booktitle>In Proceedings of the Symposium on Eye Tracking Research and Applications,</booktitle>
<pages>275--278</pages>
<publisher>ACM.</publisher>
<contexts>
<context position="9403" citStr="Guo et al., 2014" startWordPosition="1445" endWordPosition="1448">k et al., 2012). Such distinctions intuitively make sense, as the learning of medical domain knowledge requires advanced education in 108 conjunction with substantial practical field experience. In a task such as medical image inspection, the subtle cues that point an observer to evidence that allow them to identify a clinical condition, while accessible to experts with training and perceptual expertise to guide their exploration, are likely to be missed by novices who lack that background and clinical understanding. Such expertise can then be integrated into human-centered health-IT systems (Guo et al., 2014), in order to introduce novel ways to retrieve medical images and take advantage of an understanding of which information is useful. It is reasonable to assume that this knowledge gap also applies to other knowledge-intensive clinical domains such as mental health. In this study, we explore this question and study if novice vs. expert annotation makes a difference for identifying distress in social media texts, as well as what the impact of expert vs. novice annotation is for subsequent computational modeling with the annotated data. Affect in language is a phenomenon that has been studied in </context>
</contexts>
<marker>Guo, Li, Alm, Yu, Pelz, Shi, Haake, 2014</marker>
<rawString>Xuan Guo, Rui Li, Cecilia Ovesdotter Alm, Qi Yu, Jeff Pelz, Pengcheng Shi, and Anne Haake. 2014. Infusing perceptual expertise and domain knowledge into a human-centered image retrieval system: A prototype application. In Proceedings of the Symposium on Eye Tracking Research and Applications, pages 275–278. ACM.</rawString>
</citation>
<citation valid="true">
<authors>
<author>Aniko Hannak</author>
<author>Eric Anderson</author>
<author>Lisa Feldman Barrett</author>
<author>Sune Lehmann</author>
<author>Alan Mislove</author>
<author>Mirek Riedewald</author>
</authors>
<title>Tweetin in the rain: Exploring societalscale effects of weather on mood.</title>
<date>2012</date>
<booktitle>In Proceedings of the 6th International AAAI Conference on Weblogs and Social Media (ICWSM12).</booktitle>
<contexts>
<context position="11478" citStr="Hannak et al., 2012" startWordPosition="1783" endWordPosition="1786">redibly important to study, despite the challenges involved. Sentiment analysis has been widely studied in a number of computational settings, including on various social networking sites. A rather substantial body of work already exists on the use of Twitter to study emotion (Bollen et al., 2011b; Dodds et al., 2011; Wang et al., 2012; Pfitzner et al., 2012; Kim et al., 2012; Bollen et al., 2011a; Pfitzner et al., 2012; Bollen et al., 2011c; Mohammad, 2012; Golder and Macy, 2011; De Choudhury et al., 2012a; De Choudhury et al., 2012b; De Choudhury et al., 2013; De Choudhury and Counts, 2013; Hannak et al., 2012; Thelwall et al., 2011; Pak and Paroubek, 2010). For instance, Golder and and Macy study aggregate global trends in “mood,” and show, among other things, that people wake up in a relatively good mood that decays as the day progresses (Golder and Macy, 2011). Bollen et al. (2011c) show that tweets from users who took a standard diagnostic instrument for mood are often tied to current events, such as elections and holidays. Relatively little of this work has focused on suicide or related psychological conditions. Masuda et al. (2013) study suicide on mixi (a Japanese social networking service).</context>
</contexts>
<marker>Hannak, Anderson, Barrett, Lehmann, Mislove, Riedewald, 2012</marker>
<rawString>Aniko Hannak, Eric Anderson, Lisa Feldman Barrett, Sune Lehmann, Alan Mislove, and Mirek Riedewald. 2012. Tweetin in the rain: Exploring societalscale effects of weather on mood. In Proceedings of the 6th International AAAI Conference on Weblogs and Social Media (ICWSM12).</rawString>
</citation>
<citation valid="true">
<authors>
<author>Louise Harriss</author>
<author>Keith Hawton</author>
</authors>
<title>Suicidal intent in deliberate self-harm and the risk of suicide: The predictive power of the suicide intent scale.</title>
<date>2005</date>
<journal>Journal of Affective Disorders,</journal>
<volume>86</volume>
<issue>2</issue>
<contexts>
<context position="8539" citStr="Harriss and Hawton, 2005" startWordPosition="1312" endWordPosition="1315">tually attempted. The odds of attempting suicide increased exponentially when individuals endorsed three or more risk factors, e.g., having a mood or substance abuse disorder (Kessler et al., 1999). Demographics, previous suicide attempts, mental health concerns (i.e., depression, substance abuse, suicidal ideation, self-harm, or impulsivity), family history of suicide, interpersonal conflicts (i.e., family violence or bullying), and means for suicidal behavior (e.g., firearms), are commonly cited risk factors for suicidal behavior (Nock et al., 2008; Crosby et al., 2011; Gaynes et al., 2004; Harriss and Hawton, 2005; Shaffer et al., 2004; Brown et al., 2000). Regarding the use of annotation for predictive modeling, evidence suggests that when it comes to judgments that involve clinical phenomena, experts and novices behave differently (Li et al., 2012; Womack et al., 2012). Such distinctions intuitively make sense, as the learning of medical domain knowledge requires advanced education in 108 conjunction with substantial practical field experience. In a task such as medical image inspection, the subtle cues that point an observer to evidence that allow them to identify a clinical condition, while accessi</context>
</contexts>
<marker>Harriss, Hawton, 2005</marker>
<rawString>Louise Harriss and Keith Hawton. 2005. Suicidal intent in deliberate self-harm and the risk of suicide: The predictive power of the suicide intent scale. Journal of Affective Disorders, 86(2):225–233.</rawString>
</citation>
<citation valid="true">
<authors>
<author>Melonie Heron</author>
<author>Betzaida Tejada-Vera</author>
</authors>
<title>Deaths: Leading causes for 2005. National Vital Statistics Reports: From the Centers for Disease Control and Prevention, National Center for Health Statistics, National Vital Statistics System,</title>
<date>2009</date>
<volume>58</volume>
<issue>8</issue>
<pages>97</pages>
<contexts>
<context position="1453" citStr="Heron and Tejada-Vera, 2009" startWordPosition="212" endWordPosition="215">ugh social media activity, with no reliance on self-reporting. We consider the performance of annotators with various degrees of expertise in suicide prevention at annotating microblog data for the purpose of training text-based models for detecting suicide risk behaviors. Consistent with crowdsourcing literature, we found that novice-novice annotator pairs underperform expert annotators and outperform automatic lexical analysis tools, such as Linguistic Inquiry and Word Count. 1 Introduction Suicide is among the leading causes of death for individuals 10–44 years of age in the United States (Heron and Tejada-Vera, 2009). Indeed, while mortality rates for most illnesses decreased between 2008 and 2009, the rate of suicide increased by 2.4% (Heron and Tejada-Vera, 2009). The lifetime prevalence for suicidal ideation is 5.6–14.3% in the general population, and as high as 19.8– 24.0% among youth (Nock et al., 2008). The first step toward suicide prevention is to identify, ideally in consultation with clinical experts, the risk factors associated with suicide. Due to social stigma among other sociocultural factors (Crosby et al., 2011), individuals at risk for committing suicide may not always reach out to profes</context>
</contexts>
<marker>Heron, Tejada-Vera, 2009</marker>
<rawString>Melonie Heron and Betzaida Tejada-Vera. 2009. Deaths: Leading causes for 2005. National Vital Statistics Reports: From the Centers for Disease Control and Prevention, National Center for Health Statistics, National Vital Statistics System, 58(8):1– 97.</rawString>
</citation>
<citation valid="true">
<authors>
<author>Christopher M Homan</author>
<author>Naiji Lu</author>
<author>Xin Tu</author>
<author>Megan C Lytle</author>
<author>Vincent Silenzio</author>
</authors>
<title>Social structure and depression in TrevorSpace.</title>
<date>2014</date>
<booktitle>In Proceedings of the 17th ACM Conference on Computer Supported Cooperative Work &amp; Social Computing,</booktitle>
<pages>615--625</pages>
<publisher>ACM.</publisher>
<contexts>
<context position="12656" citStr="Homan et al. (2014)" startWordPosition="1978" endWordPosition="1981">i (a Japanese social networking service). Cheng et al. (2012) consider the ethical and political implications of online data collection for suicide prevention. Jashinsky et al. (2013) show correlations between frequency in tweets related to suicide and actual suicide in the 50 United States of America. Sadilek et al. (2014) study depression on Twitter. De Choudhury and collaborators studied depression—in general and post-partum—in Twitter (De Choudhury et al., 2012a; De Choudhury et al., 2012b; De Choudhury et al., 2013; De Choudhury and Counts, 2013) and Facebook (De Choudhury et al., 2014). Homan et al. (2014) investigate depression in TrevorSpace. A number of social theories of suicide have been proposed (Wray et al., 2011), but most of this work was with respect to offline social systems. 3 Methods Our methods involve four main phases: (1) We filtered a corpus, obtained from Sadilek et al. (2012), of approximately 2.5 million tweets from 6,237 unique users in the New York City area that were sent during a 1-month period between May and June, 2010, into a set of 2,000 tweets that are relatively likely to be centered around suicide risk factors. (2) We annotated each of these 2,000 tweets with thei</context>
</contexts>
<marker>Homan, Lu, Tu, Lytle, Silenzio, 2014</marker>
<rawString>Christopher M Homan, Naiji Lu, Xin Tu, Megan C Lytle, and Vincent Silenzio. 2014. Social structure and depression in TrevorSpace. In Proceedings of the 17th ACM Conference on Computer Supported Cooperative Work &amp; Social Computing, pages 615– 625. ACM.</rawString>
</citation>
<citation valid="true">
<authors>
<author>Lisa M Horowitz</author>
<author>Elizabeth D Ballard</author>
</authors>
<title>Suicide screening in schools, primary care and emergency departments.</title>
<date>2009</date>
<journal>Current Opinion in Pediatrics,</journal>
<volume>21</volume>
<issue>5</issue>
<contexts>
<context position="7295" citStr="Horowitz and Ballard, 2009" startWordPosition="1122" endWordPosition="1126"> annotation, rather than generalpurpose tools for content and sentiment analysis such as LIWC (Linguistic Inquiry and Word Count) by Pennebaker et al. (2001), provides a basis for text-based statistical modeling. We show that expertise-based keyword retrieval, departing from knowledge about contributing risk factors, results in better interannotator agreement in both novice-novice and novice-expert annotation when the keywords reflect the task at hand. 2 Related Work Data on suicide traditionally comes from healthcare organizations, large-scale studies, or self reporting (Crosby et al., 2011; Horowitz and Ballard, 2009). These sources are limited by sociocultural barriers (Crosby et al., 2011), such as stigma and shame. Moreover, data on suicide is never particularly reliable because suicide is a fundamentally subjective, complex phenomenon with a low base rate. For these reasons, many researchers tend to focus on the relationship between risk factors and suicidal behavior, without relying heavily on theoretical models (Nock et al., 2008). Approximately one-third of all individuals who reported suicidal ideation in their lifetime made a plan to commit suicide. Nearly three-quarters of those who reported maki</context>
</contexts>
<marker>Horowitz, Ballard, 2009</marker>
<rawString>Lisa M Horowitz and Elizabeth D Ballard. 2009. Suicide screening in schools, primary care and emergency departments. Current Opinion in Pediatrics, 21(5):620–627.</rawString>
</citation>
<citation valid="true">
<authors>
<author>Jared Jashinsky</author>
<author>Scott H Burton</author>
<author>Carl L Hanson</author>
<author>Josh West</author>
<author>Christophe Giraud-Carrier</author>
<author>Michael D Barnes</author>
<author>Trenton Argyle</author>
</authors>
<title>Tracking suicide risk factors through Twitter in the US. Crisis,</title>
<date>2013</date>
<pages>1--9</pages>
<contexts>
<context position="12220" citStr="Jashinsky et al. (2013)" startWordPosition="1905" endWordPosition="1908">ood,” and show, among other things, that people wake up in a relatively good mood that decays as the day progresses (Golder and Macy, 2011). Bollen et al. (2011c) show that tweets from users who took a standard diagnostic instrument for mood are often tied to current events, such as elections and holidays. Relatively little of this work has focused on suicide or related psychological conditions. Masuda et al. (2013) study suicide on mixi (a Japanese social networking service). Cheng et al. (2012) consider the ethical and political implications of online data collection for suicide prevention. Jashinsky et al. (2013) show correlations between frequency in tweets related to suicide and actual suicide in the 50 United States of America. Sadilek et al. (2014) study depression on Twitter. De Choudhury and collaborators studied depression—in general and post-partum—in Twitter (De Choudhury et al., 2012a; De Choudhury et al., 2012b; De Choudhury et al., 2013; De Choudhury and Counts, 2013) and Facebook (De Choudhury et al., 2014). Homan et al. (2014) investigate depression in TrevorSpace. A number of social theories of suicide have been proposed (Wray et al., 2011), but most of this work was with respect to off</context>
<context position="15269" citStr="Jashinsky et al. (2013)" startWordPosition="2398" endWordPosition="2401">d on the noslang dictionary.1 We then used two different methods to filter tweets that are relatively likely to center on suicide risk factors. We used LIWC to capture 1,370 tweets by sampling randomly from among the 2,000 tweets with the highest LIWC sad score. LIWC has been widely used to estimate emotion in online social networks, and specifically to mood on Twitter. This slight amount of randomness in filtering tweets this way was intended to avoid selecting obvious false positives, such as the use of “sad” in nicknames. Next, we adopted a collection of inclusive search terms/phrases from Jashinsky et al. (2013), which was designed specifically for capturing tweets related to suicide risk factors, and applied them to our source corpus. We added to these more terms, from (Crosby et al., 2011) (see Table 2). These terms yielded 630 tweets. 1http://www.noslang.com/dictionary depressive tired of living, leave this world, feeling wanna die, hate my job, feeling guilty, deserve to die, desire to end own life, feeling ignored, tired of everything, feeling blue, have blues depression sleeping pill, have insomnia, symptoms sleep forever, sleep disorder drug clonazepam, drug overdose, abuse imipramine prior su</context>
<context position="21503" citStr="Jashinsky et al. (2013)" startWordPosition="3473" endWordPosition="3476">ined eye. Note that there are very few happy tweets, which confirms that our filtering was effective in removing tweets of the opposite polarity. Filtering method Kappa LIWC sad 0.4 Thematic suicide risk factors 0.6 Both 0.5 Table 4: Cohen kappa interannotator agreement between Novice 1 and 2. H ND LD HD H 0 2 0 0 ND 1 85 2 1 LD 0 22 9 0 HD 0 1 0 2 Table 5: Confusion matrix between Novices 1 and 2 on annotations of the LIWC-sad-based filtered tweets. H ND LD HD H 4 6 0 0 ND 0 55 12 1 LD 0 12 22 5 HD 0 1 3 4 Table 6: Confusion matrix between Novices 1 and 2 on annotations of tweets filtered by Jashinsky et al. (2013)’s thematic suicide risk factors inclusion terms. Table 4 shows the Cohen kappa score between Novices 1 and 2, when high and low distress vs. no distress and happy, are grouped in a single category and Tables 5—7 show the confusion matrices between Novices 1 and 2. In all cases the kappa score is moderate. However, it clearly improves when annotation is restricted to just those tweets filtered using the suicide-thematic inclusion terms of Jashinsky et al. (2013). This again seems to point to the usefulness of including clinical experts into the training process. Due to their sensitive nature, </context>
</contexts>
<marker>Jashinsky, Burton, Hanson, West, Giraud-Carrier, Barnes, Argyle, 2013</marker>
<rawString>Jared Jashinsky, Scott H Burton, Carl L Hanson, Josh West, Christophe Giraud-Carrier, Michael D Barnes, and Trenton Argyle. 2013. Tracking suicide risk factors through Twitter in the US. Crisis, pages 1– 9.</rawString>
</citation>
<citation valid="true">
<authors>
<author>T Joachims</author>
</authors>
<title>Text categorization with support vector machines: Learning with many relevant features.</title>
<date>1998</date>
<booktitle>In European Conference on Machine Learning (ECML),</booktitle>
<pages>137--142</pages>
<publisher>Springer.</publisher>
<location>Berlin.</location>
<contexts>
<context position="19382" citStr="Joachims, 1998" startWordPosition="3090" endWordPosition="3091">h its tf-idf score (Manning et al., 2008). We performed topic modeling on our dataset. A topic is a set of lexical items that are likely to occur in the same tweet. Topic models are capable of associating words with similar meanings and distinguishing among the different meanings of a single word. We used latent Dirichlet allocation (LDA) (Blei et al., 2003) to create these topics. Before doing so, we removed stop words and words that occur only once in the dataset. We then applied LDA algorithm on the data to discover three topics using 100 iterations. We used support vector machines (SVMs) (Joachims, 1998), a machine learning method that is used to train a classification model that can assign class labels to previously unseen tweets, to assess the power of our annotations. SVMs treat each tweet as a point in an extremely high dimensional space (one dimension per uni-, bi-, and trigram in the corpus). SVMs are a form of linear separator that can also distinguish between nonlinearly separable classes of data by warping the feature space (though in our case we perform no such warping, or kernelization). They have proven to be an extremely effective tool in classifying text in numerous settings, in</context>
</contexts>
<marker>Joachims, 1998</marker>
<rawString>T. Joachims. 1998. Text categorization with support vector machines: Learning with many relevant features. In European Conference on Machine Learning (ECML), pages 137–142, Berlin. Springer.</rawString>
</citation>
<citation valid="true">
<authors>
<author>Ronald C Kessler</author>
<author>Guilherme Borges</author>
<author>Ellen E Walters</author>
</authors>
<title>Prevalence of and risk factors for lifetime suicide attempts in the national comorbidity survey.</title>
<date>1999</date>
<journal>Archives of General Psychiatry,</journal>
<volume>56</volume>
<issue>7</issue>
<contexts>
<context position="8112" citStr="Kessler et al., 1999" startWordPosition="1250" endWordPosition="1253">ubjective, complex phenomenon with a low base rate. For these reasons, many researchers tend to focus on the relationship between risk factors and suicidal behavior, without relying heavily on theoretical models (Nock et al., 2008). Approximately one-third of all individuals who reported suicidal ideation in their lifetime made a plan to commit suicide. Nearly three-quarters of those who reported making a suicide plan actually attempted. The odds of attempting suicide increased exponentially when individuals endorsed three or more risk factors, e.g., having a mood or substance abuse disorder (Kessler et al., 1999). Demographics, previous suicide attempts, mental health concerns (i.e., depression, substance abuse, suicidal ideation, self-harm, or impulsivity), family history of suicide, interpersonal conflicts (i.e., family violence or bullying), and means for suicidal behavior (e.g., firearms), are commonly cited risk factors for suicidal behavior (Nock et al., 2008; Crosby et al., 2011; Gaynes et al., 2004; Harriss and Hawton, 2005; Shaffer et al., 2004; Brown et al., 2000). Regarding the use of annotation for predictive modeling, evidence suggests that when it comes to judgments that involve clinical</context>
</contexts>
<marker>Kessler, Borges, Walters, 1999</marker>
<rawString>Ronald C Kessler, Guilherme Borges, and Ellen E Walters. 1999. Prevalence of and risk factors for lifetime suicide attempts in the national comorbidity survey. Archives of General Psychiatry, 56(7):617–</rawString>
</citation>
<citation valid="true">
<authors>
<author>Suin Kim</author>
<author>J Bak</author>
<author>Alice Oh</author>
</authors>
<title>Do you feel what I feel? Social aspects of emotions in Twitter conversations.</title>
<date>2012</date>
<booktitle>In Proceedings of the AAAI International Conference on Weblogs and Social Media.</booktitle>
<contexts>
<context position="11237" citStr="Kim et al., 2012" startWordPosition="1739" endWordPosition="1742"> zone where neutrality and emotion meet (Alm, 2008). These kinds of problem characteristics bring with them their own set of demanding challenges from a computational perspective (Alm, 2011). Yet, the nature of such problems make them incredibly important to study, despite the challenges involved. Sentiment analysis has been widely studied in a number of computational settings, including on various social networking sites. A rather substantial body of work already exists on the use of Twitter to study emotion (Bollen et al., 2011b; Dodds et al., 2011; Wang et al., 2012; Pfitzner et al., 2012; Kim et al., 2012; Bollen et al., 2011a; Pfitzner et al., 2012; Bollen et al., 2011c; Mohammad, 2012; Golder and Macy, 2011; De Choudhury et al., 2012a; De Choudhury et al., 2012b; De Choudhury et al., 2013; De Choudhury and Counts, 2013; Hannak et al., 2012; Thelwall et al., 2011; Pak and Paroubek, 2010). For instance, Golder and and Macy study aggregate global trends in “mood,” and show, among other things, that people wake up in a relatively good mood that decays as the day progresses (Golder and Macy, 2011). Bollen et al. (2011c) show that tweets from users who took a standard diagnostic instrument for moo</context>
</contexts>
<marker>Kim, Bak, Oh, 2012</marker>
<rawString>Suin Kim, J Bak, and Alice Oh. 2012. Do you feel what I feel? Social aspects of emotions in Twitter conversations. In Proceedings of the AAAI International Conference on Weblogs and Social Media.</rawString>
</citation>
<citation valid="true">
<authors>
<author>Michael Lehrman</author>
<author>Cecilia Ovesdotter Alm</author>
<author>Ruben Proano</author>
</authors>
<title>Detecting distressed vs. nondistressed affect state in short forum texts.</title>
<date>2012</date>
<booktitle>In Proceedings of the Workshop on Language in Social Media (LSM 2012) at the Conference of the North American Chapter of the Association for Computational Linguistics-Human Language Technologies,</booktitle>
<pages>9--18</pages>
<location>Montreal, Canada,</location>
<contexts>
<context position="5767" citStr="Lehrman et al. (2012)" startWordPosition="889" endWordPosition="892">ional methods for enhancing protective factors against suicide. In this paper, we take steps toward the automatic detection of suicide risk among individuals via social media. Suicide ideation is a complex behavior and its connection to suicide itself remains poorly understood. We focus on a particular aspect of suicidality, namely distress. While not equivalent to suicide ideation, according to Nock et al. (2010) distress is an important risk factor in suicide, and one that is observable from microblog text, though admittedly observing suicide risk behavior is a subjective and noisy venture. Lehrman et al. (2012) conducted an early study on the computational modeling of distress based on short forum texts, yet left many areas wide open for continued study. For example, analysis at scale is one such open issue. More specifically, Pestian and colleagues (Matykiewicz et al., 2009; Pestian et al., 2008) used computational methods to understand suicide notes. However, when it comes to preventive contexts, such data are less insightful. For preventive health, access to real-time healthrelated data that dynamically evolve can allow us to address macro-level analysis. Social media provide an additional opport</context>
</contexts>
<marker>Lehrman, Alm, Proano, 2012</marker>
<rawString>Michael Lehrman, Cecilia Ovesdotter Alm, and Ruben Proano. 2012. Detecting distressed vs. nondistressed affect state in short forum texts. In Proceedings of the Workshop on Language in Social Media (LSM 2012) at the Conference of the North American Chapter of the Association for Computational Linguistics-Human Language Technologies, Montreal, Canada, pages 9–18.</rawString>
</citation>
<citation valid="true">
<authors>
<author>Rui Li</author>
<author>Jeff Pelz</author>
<author>Pengcheng Shi</author>
<author>Anne Haake</author>
</authors>
<title>Learning image-derived eye movement patterns to characterize perceptual expertise. In CogSci,</title>
<date>2012</date>
<pages>1900--1905</pages>
<contexts>
<context position="8779" citStr="Li et al., 2012" startWordPosition="1350" endWordPosition="1353">alth concerns (i.e., depression, substance abuse, suicidal ideation, self-harm, or impulsivity), family history of suicide, interpersonal conflicts (i.e., family violence or bullying), and means for suicidal behavior (e.g., firearms), are commonly cited risk factors for suicidal behavior (Nock et al., 2008; Crosby et al., 2011; Gaynes et al., 2004; Harriss and Hawton, 2005; Shaffer et al., 2004; Brown et al., 2000). Regarding the use of annotation for predictive modeling, evidence suggests that when it comes to judgments that involve clinical phenomena, experts and novices behave differently (Li et al., 2012; Womack et al., 2012). Such distinctions intuitively make sense, as the learning of medical domain knowledge requires advanced education in 108 conjunction with substantial practical field experience. In a task such as medical image inspection, the subtle cues that point an observer to evidence that allow them to identify a clinical condition, while accessible to experts with training and perceptual expertise to guide their exploration, are likely to be missed by novices who lack that background and clinical understanding. Such expertise can then be integrated into human-centered health-IT sy</context>
</contexts>
<marker>Li, Pelz, Shi, Haake, 2012</marker>
<rawString>Rui Li, Jeff Pelz, Pengcheng Shi, and Anne Haake. 2012. Learning image-derived eye movement patterns to characterize perceptual expertise. In CogSci, pages 1900–1905.</rawString>
</citation>
<citation valid="true">
<authors>
<author>J John Mann</author>
<author>Christine Waternaux</author>
<author>Gretchen L Haas</author>
<author>Kevin M Malone</author>
</authors>
<title>Toward a clinical model of suicidal behavior in psychiatric patients.</title>
<date>1999</date>
<journal>American Journal of Psychiatry,</journal>
<volume>156</volume>
<issue>2</issue>
<contexts>
<context position="3542" citStr="Mann et al., 1999" startWordPosition="543" endWordPosition="546">al media might be an important channel for discovering those at risk for— and even preventing—suicide. Internet- and telecommunications-driven activity is revolutionizing the social sciences by providing data, much of it publicly available, on human activity in situ, at volumes and a level of time and space granularity never before approached. Can such data improve clinical preventative study and measures by providing access to at-risk individuals who would otherwise go undetected, and by leading to better science about suicide risk behaviors? The stress-diathesis model for suicidal behavior (Mann et al., 1999) suggests that they might. It says that (1) objective states, such as depression or life events, as well as subjective states and traits, such as substance abuse or family history of depression, suicide, or substance abuse, are among the risk factors that contribute to suicidal ideation and (2) the presence of these factors could eventually lead to either externalizing (e.g., interper107 Workshop on Computational Linguistics and Clinical Psychology: From Linguistic Signal to Clinical Reality, pages 107–117, Baltimore, Maryland USA, June 27, 2014. c�2014 Association for Computational Linguistic</context>
<context position="29619" citStr="Mann et al., 1999" startWordPosition="4871" endWordPosition="4874">o distress than Novice 1. It is premature to draw conclusions from this observation, but perhaps this shows that training with expert-labeled annotations is preferable to using novice-labeled data, espectially when our goal is to discover distressful tweets for the purpose of identifying atrisk individuals and err on the side of caution (high recall). (4) Integrating more but mixed data does not improve performance. 5 Discussion As previously mentioned, many of the risk factors for suicidal behavior may be linked to other expressions of distress, such as aggression and interpersonal violence (Mann et al., 1999). The goal of this study is to determine the feasibility of classifying distress to enable further study of expressed suicidal behaviors. Consistent with the stress diathesis model for suicidal behavior, aggression was an emerging theme that arose from the data. Here are some examples: 9 @XXXX i don’t feel sad 4 him. he gets pissed n says wat he wants then sends out fony apologies 9 @XXXX cuz he’s n a relationship with that horseface bitch &amp; he lied 2 me &amp; i feel so used &amp; worthless now Some individuals tweeted about feeling empty, hopeless, angry, frustrated, and alone. Behaviors indicating b</context>
</contexts>
<marker>Mann, Waternaux, Haas, Malone, 1999</marker>
<rawString>J John Mann, Christine Waternaux, Gretchen L Haas, and Kevin M Malone. 1999. Toward a clinical model of suicidal behavior in psychiatric patients. American Journal of Psychiatry, 156(2):181–189.</rawString>
</citation>
<citation valid="true">
<authors>
<author>Christopher D Manning</author>
<author>Prabhakar Raghavan</author>
<author>Hinrich Sch¨utze</author>
</authors>
<title>Introduction to Information Retrieval.</title>
<date>2008</date>
<publisher>Cambridge University Press,</publisher>
<location>New York, NY, USA.</location>
<marker>Manning, Raghavan, Sch¨utze, 2008</marker>
<rawString>Christopher D. Manning, Prabhakar Raghavan, and Hinrich Sch¨utze. 2008. Introduction to Information Retrieval. Cambridge University Press, New York, NY, USA.</rawString>
</citation>
<citation valid="true">
<authors>
<author>Naoki Masuda</author>
<author>Issei Kurahashi</author>
<author>Hiroko Onari</author>
</authors>
<title>Suicide ideation of individuals in online social networks.</title>
<date>2013</date>
<tech>PloS one, 8(4):e62262.</tech>
<contexts>
<context position="12016" citStr="Masuda et al. (2013)" startWordPosition="1875" endWordPosition="1878">12b; De Choudhury et al., 2013; De Choudhury and Counts, 2013; Hannak et al., 2012; Thelwall et al., 2011; Pak and Paroubek, 2010). For instance, Golder and and Macy study aggregate global trends in “mood,” and show, among other things, that people wake up in a relatively good mood that decays as the day progresses (Golder and Macy, 2011). Bollen et al. (2011c) show that tweets from users who took a standard diagnostic instrument for mood are often tied to current events, such as elections and holidays. Relatively little of this work has focused on suicide or related psychological conditions. Masuda et al. (2013) study suicide on mixi (a Japanese social networking service). Cheng et al. (2012) consider the ethical and political implications of online data collection for suicide prevention. Jashinsky et al. (2013) show correlations between frequency in tweets related to suicide and actual suicide in the 50 United States of America. Sadilek et al. (2014) study depression on Twitter. De Choudhury and collaborators studied depression—in general and post-partum—in Twitter (De Choudhury et al., 2012a; De Choudhury et al., 2012b; De Choudhury et al., 2013; De Choudhury and Counts, 2013) and Facebook (De Chou</context>
</contexts>
<marker>Masuda, Kurahashi, Onari, 2013</marker>
<rawString>Naoki Masuda, Issei Kurahashi, and Hiroko Onari. 2013. Suicide ideation of individuals in online social networks. PloS one, 8(4):e62262.</rawString>
</citation>
<citation valid="true">
<authors>
<author>Pawel Matykiewicz</author>
<author>Wlodzislav Duch</author>
<author>John P Pestian</author>
</authors>
<title>Clustering semantic spaces of suicide notes and newsgroup articles.</title>
<date>2009</date>
<booktitle>In Proceedings of the Workshop on BioNLP,</booktitle>
<pages>179--184</pages>
<location>Boulder, Colorado,</location>
<contexts>
<context position="6036" citStr="Matykiewicz et al., 2009" startWordPosition="932" endWordPosition="935"> understood. We focus on a particular aspect of suicidality, namely distress. While not equivalent to suicide ideation, according to Nock et al. (2010) distress is an important risk factor in suicide, and one that is observable from microblog text, though admittedly observing suicide risk behavior is a subjective and noisy venture. Lehrman et al. (2012) conducted an early study on the computational modeling of distress based on short forum texts, yet left many areas wide open for continued study. For example, analysis at scale is one such open issue. More specifically, Pestian and colleagues (Matykiewicz et al., 2009; Pestian et al., 2008) used computational methods to understand suicide notes. However, when it comes to preventive contexts, such data are less insightful. For preventive health, access to real-time healthrelated data that dynamically evolve can allow us to address macro-level analysis. Social media provide an additional opportunity to model the phenomena of interest at scale. We use methods that take advantage of lexical analysis to retrieve microblog posts (tweets) from Twitter and compare the performance of human annotators—one being an expert, and others not— to rate the level of distres</context>
</contexts>
<marker>Matykiewicz, Duch, Pestian, 2009</marker>
<rawString>Pawel Matykiewicz, Wlodzislav Duch, and John P. Pestian. 2009. Clustering semantic spaces of suicide notes and newsgroup articles. In Proceedings of the Workshop on BioNLP, Boulder, Colorado, pages 179–184.</rawString>
</citation>
<citation valid="true">
<authors>
<author>Saif M Mohammad</author>
</authors>
<title>Emotional tweets.</title>
<date>2012</date>
<booktitle>In Proceedings of the First Joint Conference on Lexical and Computational Semantics-Volume 1: Proceedings of the main conference and the shared task, and Volume 2: Proceedings of the Sixth International Workshop on Semantic Evaluation,</booktitle>
<pages>246--255</pages>
<publisher>Association</publisher>
<institution>for Computational Linguistics.</institution>
<contexts>
<context position="11320" citStr="Mohammad, 2012" startWordPosition="1755" endWordPosition="1757">istics bring with them their own set of demanding challenges from a computational perspective (Alm, 2011). Yet, the nature of such problems make them incredibly important to study, despite the challenges involved. Sentiment analysis has been widely studied in a number of computational settings, including on various social networking sites. A rather substantial body of work already exists on the use of Twitter to study emotion (Bollen et al., 2011b; Dodds et al., 2011; Wang et al., 2012; Pfitzner et al., 2012; Kim et al., 2012; Bollen et al., 2011a; Pfitzner et al., 2012; Bollen et al., 2011c; Mohammad, 2012; Golder and Macy, 2011; De Choudhury et al., 2012a; De Choudhury et al., 2012b; De Choudhury et al., 2013; De Choudhury and Counts, 2013; Hannak et al., 2012; Thelwall et al., 2011; Pak and Paroubek, 2010). For instance, Golder and and Macy study aggregate global trends in “mood,” and show, among other things, that people wake up in a relatively good mood that decays as the day progresses (Golder and Macy, 2011). Bollen et al. (2011c) show that tweets from users who took a standard diagnostic instrument for mood are often tied to current events, such as elections and holidays. Relatively litt</context>
</contexts>
<marker>Mohammad, 2012</marker>
<rawString>Saif M Mohammad. 2012. # Emotional tweets. In Proceedings of the First Joint Conference on Lexical and Computational Semantics-Volume 1: Proceedings of the main conference and the shared task, and Volume 2: Proceedings of the Sixth International Workshop on Semantic Evaluation, pages 246–255. Association for Computational Linguistics.</rawString>
</citation>
<citation valid="true">
<authors>
<author>Matthew K Nock</author>
<author>Guilherme Borges</author>
<author>Evelyn J Bromet</author>
<author>Christine B Cha</author>
<author>Ronald C Kessler</author>
<author>Sing Lee</author>
</authors>
<title>Suicide and suicidal behavior.</title>
<date>2008</date>
<journal>Epidemiologic Reviews,</journal>
<volume>30</volume>
<issue>1</issue>
<contexts>
<context position="1750" citStr="Nock et al., 2008" startWordPosition="261" endWordPosition="264">erature, we found that novice-novice annotator pairs underperform expert annotators and outperform automatic lexical analysis tools, such as Linguistic Inquiry and Word Count. 1 Introduction Suicide is among the leading causes of death for individuals 10–44 years of age in the United States (Heron and Tejada-Vera, 2009). Indeed, while mortality rates for most illnesses decreased between 2008 and 2009, the rate of suicide increased by 2.4% (Heron and Tejada-Vera, 2009). The lifetime prevalence for suicidal ideation is 5.6–14.3% in the general population, and as high as 19.8– 24.0% among youth (Nock et al., 2008). The first step toward suicide prevention is to identify, ideally in consultation with clinical experts, the risk factors associated with suicide. Due to social stigma among other sociocultural factors (Crosby et al., 2011), individuals at risk for committing suicide may not always reach out to professionals or, if they do, provide them with accurate information. They may not even realize their own level of suicide risk before it is too late. Self-reporting, then, is not an entirely reliable means of detecting and assessing suicide risk, and research on suicide prevention can benefit from als</context>
<context position="7722" citStr="Nock et al., 2008" startWordPosition="1191" endWordPosition="1194">ct the task at hand. 2 Related Work Data on suicide traditionally comes from healthcare organizations, large-scale studies, or self reporting (Crosby et al., 2011; Horowitz and Ballard, 2009). These sources are limited by sociocultural barriers (Crosby et al., 2011), such as stigma and shame. Moreover, data on suicide is never particularly reliable because suicide is a fundamentally subjective, complex phenomenon with a low base rate. For these reasons, many researchers tend to focus on the relationship between risk factors and suicidal behavior, without relying heavily on theoretical models (Nock et al., 2008). Approximately one-third of all individuals who reported suicidal ideation in their lifetime made a plan to commit suicide. Nearly three-quarters of those who reported making a suicide plan actually attempted. The odds of attempting suicide increased exponentially when individuals endorsed three or more risk factors, e.g., having a mood or substance abuse disorder (Kessler et al., 1999). Demographics, previous suicide attempts, mental health concerns (i.e., depression, substance abuse, suicidal ideation, self-harm, or impulsivity), family history of suicide, interpersonal conflicts (i.e., fam</context>
</contexts>
<marker>Nock, Borges, Bromet, Cha, Kessler, Lee, 2008</marker>
<rawString>Matthew K Nock, Guilherme Borges, Evelyn J Bromet, Christine B Cha, Ronald C Kessler, and Sing Lee. 2008. Suicide and suicidal behavior. Epidemiologic Reviews, 30(1):133–154.</rawString>
</citation>
<citation valid="true">
<authors>
<author>Matthew K Nock</author>
<author>Jennifer M Park</author>
<author>Christine T Finn</author>
<author>Tara L Deliberto</author>
<author>Halina J Dour</author>
<author>Mahzarin R Banaji</author>
</authors>
<title>Measuring the suicidal mind implicit cognition predicts suicidal behavior.</title>
<date>2010</date>
<journal>Psychological Science,</journal>
<volume>21</volume>
<issue>4</issue>
<contexts>
<context position="5563" citStr="Nock et al. (2010)" startWordPosition="855" endWordPosition="858">vel approach to monitoring suicidal behaviors may have future implications not only for identifying individuals who have a higher prevalence for suicidal behaviors but it could eventually lead to additional methods for enhancing protective factors against suicide. In this paper, we take steps toward the automatic detection of suicide risk among individuals via social media. Suicide ideation is a complex behavior and its connection to suicide itself remains poorly understood. We focus on a particular aspect of suicidality, namely distress. While not equivalent to suicide ideation, according to Nock et al. (2010) distress is an important risk factor in suicide, and one that is observable from microblog text, though admittedly observing suicide risk behavior is a subjective and noisy venture. Lehrman et al. (2012) conducted an early study on the computational modeling of distress based on short forum texts, yet left many areas wide open for continued study. For example, analysis at scale is one such open issue. More specifically, Pestian and colleagues (Matykiewicz et al., 2009; Pestian et al., 2008) used computational methods to understand suicide notes. However, when it comes to preventive contexts, </context>
</contexts>
<marker>Nock, Park, Finn, Deliberto, Dour, Banaji, 2010</marker>
<rawString>Matthew K Nock, Jennifer M Park, Christine T Finn, Tara L Deliberto, Halina J Dour, and Mahzarin R Banaji. 2010. Measuring the suicidal mind implicit cognition predicts suicidal behavior. Psychological Science, 21(4):511–517.</rawString>
</citation>
<citation valid="true">
<authors>
<author>Alexander Pak</author>
<author>Patrick Paroubek</author>
</authors>
<title>Twitter as a corpus for sentiment analysis and opinion mining.</title>
<date>2010</date>
<booktitle>In Proceedings of Conference on Language Resources and Evaluation.</booktitle>
<contexts>
<context position="11526" citStr="Pak and Paroubek, 2010" startWordPosition="1791" endWordPosition="1794">lenges involved. Sentiment analysis has been widely studied in a number of computational settings, including on various social networking sites. A rather substantial body of work already exists on the use of Twitter to study emotion (Bollen et al., 2011b; Dodds et al., 2011; Wang et al., 2012; Pfitzner et al., 2012; Kim et al., 2012; Bollen et al., 2011a; Pfitzner et al., 2012; Bollen et al., 2011c; Mohammad, 2012; Golder and Macy, 2011; De Choudhury et al., 2012a; De Choudhury et al., 2012b; De Choudhury et al., 2013; De Choudhury and Counts, 2013; Hannak et al., 2012; Thelwall et al., 2011; Pak and Paroubek, 2010). For instance, Golder and and Macy study aggregate global trends in “mood,” and show, among other things, that people wake up in a relatively good mood that decays as the day progresses (Golder and Macy, 2011). Bollen et al. (2011c) show that tweets from users who took a standard diagnostic instrument for mood are often tied to current events, such as elections and holidays. Relatively little of this work has focused on suicide or related psychological conditions. Masuda et al. (2013) study suicide on mixi (a Japanese social networking service). Cheng et al. (2012) consider the ethical and po</context>
</contexts>
<marker>Pak, Paroubek, 2010</marker>
<rawString>Alexander Pak and Patrick Paroubek. 2010. Twitter as a corpus for sentiment analysis and opinion mining. In Proceedings of Conference on Language Resources and Evaluation.</rawString>
</citation>
<citation valid="true">
<authors>
<author>James W Pennebaker</author>
<author>Martha E Francis</author>
<author>Roger J Booth</author>
</authors>
<title>Linguistic inquiry and word count: Liwc</title>
<date>2001</date>
<pages>71--2001</pages>
<contexts>
<context position="6825" citStr="Pennebaker et al. (2001)" startWordPosition="1056" endWordPosition="1059">ventive health, access to real-time healthrelated data that dynamically evolve can allow us to address macro-level analysis. Social media provide an additional opportunity to model the phenomena of interest at scale. We use methods that take advantage of lexical analysis to retrieve microblog posts (tweets) from Twitter and compare the performance of human annotators—one being an expert, and others not— to rate the level of distress of each tweet. Clinical expert annotation, rather than generalpurpose tools for content and sentiment analysis such as LIWC (Linguistic Inquiry and Word Count) by Pennebaker et al. (2001), provides a basis for text-based statistical modeling. We show that expertise-based keyword retrieval, departing from knowledge about contributing risk factors, results in better interannotator agreement in both novice-novice and novice-expert annotation when the keywords reflect the task at hand. 2 Related Work Data on suicide traditionally comes from healthcare organizations, large-scale studies, or self reporting (Crosby et al., 2011; Horowitz and Ballard, 2009). These sources are limited by sociocultural barriers (Crosby et al., 2011), such as stigma and shame. Moreover, data on suicide i</context>
</contexts>
<marker>Pennebaker, Francis, Booth, 2001</marker>
<rawString>James W Pennebaker, Martha E Francis, and Roger J Booth. 2001. Linguistic inquiry and word count: Liwc 2001. Mahway: Lawrence Erlbaum Associates, 71:2001.</rawString>
</citation>
<citation valid="true">
<authors>
<author>John P Pestian</author>
<author>Pawel Matykiewicz</author>
<author>Jacqueline Grupp-Phelan</author>
</authors>
<title>Using natural language processing to classify suicide notes.</title>
<date>2008</date>
<booktitle>In BioNLP 2008: Current Trends in Biomedical Natural Language Processing,</booktitle>
<pages>96--97</pages>
<location>Columbus, Ohio,</location>
<contexts>
<context position="6059" citStr="Pestian et al., 2008" startWordPosition="936" endWordPosition="939"> particular aspect of suicidality, namely distress. While not equivalent to suicide ideation, according to Nock et al. (2010) distress is an important risk factor in suicide, and one that is observable from microblog text, though admittedly observing suicide risk behavior is a subjective and noisy venture. Lehrman et al. (2012) conducted an early study on the computational modeling of distress based on short forum texts, yet left many areas wide open for continued study. For example, analysis at scale is one such open issue. More specifically, Pestian and colleagues (Matykiewicz et al., 2009; Pestian et al., 2008) used computational methods to understand suicide notes. However, when it comes to preventive contexts, such data are less insightful. For preventive health, access to real-time healthrelated data that dynamically evolve can allow us to address macro-level analysis. Social media provide an additional opportunity to model the phenomena of interest at scale. We use methods that take advantage of lexical analysis to retrieve microblog posts (tweets) from Twitter and compare the performance of human annotators—one being an expert, and others not— to rate the level of distress of each tweet. Clinic</context>
</contexts>
<marker>Pestian, Matykiewicz, Grupp-Phelan, 2008</marker>
<rawString>John P. Pestian, Pawel Matykiewicz, and Jacqueline Grupp-Phelan. 2008. Using natural language processing to classify suicide notes. In BioNLP 2008: Current Trends in Biomedical Natural Language Processing, Columbus, Ohio, pages 96–97.</rawString>
</citation>
<citation valid="true">
<authors>
<author>Ren´e Pfitzner</author>
<author>Antonios Garas</author>
<author>Frank Schweitzer</author>
</authors>
<title>Emotional divergence influences information spreading in twitter.</title>
<date>2012</date>
<booktitle>Proceedings of the 6th International AAAI Conference on Weblogs and Social Media (ICWSM12),</booktitle>
<pages>2--5</pages>
<contexts>
<context position="11219" citStr="Pfitzner et al., 2012" startWordPosition="1735" endWordPosition="1738">y semantics in the gray zone where neutrality and emotion meet (Alm, 2008). These kinds of problem characteristics bring with them their own set of demanding challenges from a computational perspective (Alm, 2011). Yet, the nature of such problems make them incredibly important to study, despite the challenges involved. Sentiment analysis has been widely studied in a number of computational settings, including on various social networking sites. A rather substantial body of work already exists on the use of Twitter to study emotion (Bollen et al., 2011b; Dodds et al., 2011; Wang et al., 2012; Pfitzner et al., 2012; Kim et al., 2012; Bollen et al., 2011a; Pfitzner et al., 2012; Bollen et al., 2011c; Mohammad, 2012; Golder and Macy, 2011; De Choudhury et al., 2012a; De Choudhury et al., 2012b; De Choudhury et al., 2013; De Choudhury and Counts, 2013; Hannak et al., 2012; Thelwall et al., 2011; Pak and Paroubek, 2010). For instance, Golder and and Macy study aggregate global trends in “mood,” and show, among other things, that people wake up in a relatively good mood that decays as the day progresses (Golder and Macy, 2011). Bollen et al. (2011c) show that tweets from users who took a standard diagnostic </context>
</contexts>
<marker>Pfitzner, Garas, Schweitzer, 2012</marker>
<rawString>Ren´e Pfitzner, Antonios Garas, and Frank Schweitzer. 2012. Emotional divergence influences information spreading in twitter. Proceedings of the 6th International AAAI Conference on Weblogs and Social Media (ICWSM12), pages 2–5.</rawString>
</citation>
<citation valid="true">
<authors>
<author>Rosalind W Picard</author>
</authors>
<title>Affective Computing.</title>
<date>1997</date>
<publisher>MIT Press,</publisher>
<location>Cambridge, MA, USA.</location>
<contexts>
<context position="10406" citStr="Picard, 1997" startWordPosition="1606" endWordPosition="1607">s in social media texts, as well as what the impact of expert vs. novice annotation is for subsequent computational modeling with the annotated data. Affect in language is a phenomenon that has been studied in the speech and text analysis domains, and in many others (Calvo and D’Mello, 2010). Clearly, emotion is a key element in the human experience, but it is notoriously difficult to pin down and scholars in the affective sciences lack a single agreed-upon definition for emotion. Accordingly, different theoretical constructs have been proposed to describe affect and affect-related behaviors (Picard, 1997). In addition, research on affect in language has shown that such phenomena tend to be subjective, lack real ground truth (often resulting in moderate kappa scores), and have particularly fuzzy semantics in the gray zone where neutrality and emotion meet (Alm, 2008). These kinds of problem characteristics bring with them their own set of demanding challenges from a computational perspective (Alm, 2011). Yet, the nature of such problems make them incredibly important to study, despite the challenges involved. Sentiment analysis has been widely studied in a number of computational settings, incl</context>
</contexts>
<marker>Picard, 1997</marker>
<rawString>Rosalind W. Picard. 1997. Affective Computing. MIT Press, Cambridge, MA, USA.</rawString>
</citation>
<citation valid="true">
<authors>
<author>Megan L Ryan</author>
<author>Ian M Shochet</author>
<author>Helen M Stallman</author>
</authors>
<title>Universal online interventions might engage psychologically distressed university students who are unlikely to seek formal help.</title>
<date>2010</date>
<booktitle>Advances in Mental Health,</booktitle>
<pages>9--1</pages>
<contexts>
<context position="2601" citStr="Ryan et al., 2010" startWordPosition="397" endWordPosition="400">uals at risk for committing suicide may not always reach out to professionals or, if they do, provide them with accurate information. They may not even realize their own level of suicide risk before it is too late. Self-reporting, then, is not an entirely reliable means of detecting and assessing suicide risk, and research on suicide prevention can benefit from also exploring other channels for assessing risk. For instance, individuals may be more inclined to seek support from informal resources, such as social media, instead of seeking treatment (Crosby et al., 2011; Bruffaerts et al., 2011; Ryan et al., 2010). Evidence suggests that youth and emerging adults usually prefer to seek help from their friends and families; however, higher levels of suicidal ideation are associated with lower levels of help-seeking from both formal or informal resources (Deane et al., 2001). These patterns in help-seeking behavior suggest that social media might be an important channel for discovering those at risk for— and even preventing—suicide. Internet- and telecommunications-driven activity is revolutionizing the social sciences by providing data, much of it publicly available, on human activity in situ, at volume</context>
</contexts>
<marker>Ryan, Shochet, Stallman, 2010</marker>
<rawString>Megan L Ryan, Ian M Shochet, and Helen M Stallman. 2010. Universal online interventions might engage psychologically distressed university students who are unlikely to seek formal help. Advances in Mental Health, 9(1):73–83.</rawString>
</citation>
<citation valid="true">
<authors>
<author>Adam Sadilek</author>
<author>Henry A Kautz</author>
<author>Vincent Silenzio</author>
</authors>
<title>Predicting disease transmission from geotagged micro-blog data. In Association for the Advancement of Articial Intelligence.</title>
<date>2012</date>
<contexts>
<context position="12950" citStr="Sadilek et al. (2012)" startWordPosition="2028" endWordPosition="2031">of America. Sadilek et al. (2014) study depression on Twitter. De Choudhury and collaborators studied depression—in general and post-partum—in Twitter (De Choudhury et al., 2012a; De Choudhury et al., 2012b; De Choudhury et al., 2013; De Choudhury and Counts, 2013) and Facebook (De Choudhury et al., 2014). Homan et al. (2014) investigate depression in TrevorSpace. A number of social theories of suicide have been proposed (Wray et al., 2011), but most of this work was with respect to offline social systems. 3 Methods Our methods involve four main phases: (1) We filtered a corpus, obtained from Sadilek et al. (2012), of approximately 2.5 million tweets from 6,237 unique users in the New York City area that were sent during a 1-month period between May and June, 2010, into a set of 2,000 tweets that are relatively likely to be centered around suicide risk factors. (2) We annotated each of these 2,000 tweets with their level of distress, and also analyzed the annotations in detail. (3) We then trained support vector machines and topic models with the annotated data, except for a held-out subset of 200 tweets. (4) Finally, we assessed the effectiveness of these methods on the held-out data. 109 Source Numbe</context>
</contexts>
<marker>Sadilek, Kautz, Silenzio, 2012</marker>
<rawString>Adam Sadilek, Henry A Kautz, and Vincent Silenzio. 2012. Predicting disease transmission from geotagged micro-blog data. In Association for the Advancement of Articial Intelligence.</rawString>
</citation>
<citation valid="true">
<authors>
<author>Adam Sadilek</author>
<author>Christopher Homan</author>
<author>Walter S Lasecki</author>
<author>Vincent Silenzio</author>
<author>Henry Kautz</author>
</authors>
<title>Modeling fine-grained dynamics of mood at scale.</title>
<date>2014</date>
<booktitle>In WSDM 2014 Workshop on Diffusion Networks and Cascade Analytics.</booktitle>
<contexts>
<context position="12362" citStr="Sadilek et al. (2014)" startWordPosition="1930" endWordPosition="1933">len et al. (2011c) show that tweets from users who took a standard diagnostic instrument for mood are often tied to current events, such as elections and holidays. Relatively little of this work has focused on suicide or related psychological conditions. Masuda et al. (2013) study suicide on mixi (a Japanese social networking service). Cheng et al. (2012) consider the ethical and political implications of online data collection for suicide prevention. Jashinsky et al. (2013) show correlations between frequency in tweets related to suicide and actual suicide in the 50 United States of America. Sadilek et al. (2014) study depression on Twitter. De Choudhury and collaborators studied depression—in general and post-partum—in Twitter (De Choudhury et al., 2012a; De Choudhury et al., 2012b; De Choudhury et al., 2013; De Choudhury and Counts, 2013) and Facebook (De Choudhury et al., 2014). Homan et al. (2014) investigate depression in TrevorSpace. A number of social theories of suicide have been proposed (Wray et al., 2011), but most of this work was with respect to offline social systems. 3 Methods Our methods involve four main phases: (1) We filtered a corpus, obtained from Sadilek et al. (2012), of approxi</context>
</contexts>
<marker>Sadilek, Homan, Lasecki, Silenzio, Kautz, 2014</marker>
<rawString>Adam Sadilek, Christopher Homan, Walter S. Lasecki, Vincent Silenzio, and Henry Kautz. 2014. Modeling fine-grained dynamics of mood at scale. In WSDM 2014 Workshop on Diffusion Networks and Cascade Analytics.</rawString>
</citation>
<citation valid="true">
<authors>
<author>David Shaffer</author>
<author>Michelle Scott</author>
<author>Holly Wilcox</author>
<author>Carey Maslow</author>
<author>Roger Hicks</author>
<author>Christopher P Lucas</author>
<author>Robin Garfinkel</author>
<author>Steven Greenwald</author>
</authors>
<title>The Columbia SuicideScreen: Validity and reliability of a screen for youth suicide and depression.</title>
<date>2004</date>
<journal>Journal of the American Academy of Child &amp; Adolescent Psychiatry,</journal>
<volume>43</volume>
<issue>1</issue>
<contexts>
<context position="8561" citStr="Shaffer et al., 2004" startWordPosition="1316" endWordPosition="1319"> of attempting suicide increased exponentially when individuals endorsed three or more risk factors, e.g., having a mood or substance abuse disorder (Kessler et al., 1999). Demographics, previous suicide attempts, mental health concerns (i.e., depression, substance abuse, suicidal ideation, self-harm, or impulsivity), family history of suicide, interpersonal conflicts (i.e., family violence or bullying), and means for suicidal behavior (e.g., firearms), are commonly cited risk factors for suicidal behavior (Nock et al., 2008; Crosby et al., 2011; Gaynes et al., 2004; Harriss and Hawton, 2005; Shaffer et al., 2004; Brown et al., 2000). Regarding the use of annotation for predictive modeling, evidence suggests that when it comes to judgments that involve clinical phenomena, experts and novices behave differently (Li et al., 2012; Womack et al., 2012). Such distinctions intuitively make sense, as the learning of medical domain knowledge requires advanced education in 108 conjunction with substantial practical field experience. In a task such as medical image inspection, the subtle cues that point an observer to evidence that allow them to identify a clinical condition, while accessible to experts with tr</context>
</contexts>
<marker>Shaffer, Scott, Wilcox, Maslow, Hicks, Lucas, Garfinkel, Greenwald, 2004</marker>
<rawString>David Shaffer, Michelle Scott, Holly Wilcox, Carey Maslow, Roger Hicks, Christopher P Lucas, Robin Garfinkel, and Steven Greenwald. 2004. The Columbia SuicideScreen: Validity and reliability of a screen for youth suicide and depression. Journal of the American Academy of Child &amp; Adolescent Psychiatry, 43(1):71–79.</rawString>
</citation>
<citation valid="true">
<authors>
<author>Mike Thelwall</author>
<author>Kevan Buckley</author>
<author>Georgios Paltoglou</author>
</authors>
<title>Sentiment in Twitter events.</title>
<date>2011</date>
<journal>Journal of the American Society for Information Science and Technology,</journal>
<volume>62</volume>
<issue>2</issue>
<contexts>
<context position="11501" citStr="Thelwall et al., 2011" startWordPosition="1787" endWordPosition="1790">study, despite the challenges involved. Sentiment analysis has been widely studied in a number of computational settings, including on various social networking sites. A rather substantial body of work already exists on the use of Twitter to study emotion (Bollen et al., 2011b; Dodds et al., 2011; Wang et al., 2012; Pfitzner et al., 2012; Kim et al., 2012; Bollen et al., 2011a; Pfitzner et al., 2012; Bollen et al., 2011c; Mohammad, 2012; Golder and Macy, 2011; De Choudhury et al., 2012a; De Choudhury et al., 2012b; De Choudhury et al., 2013; De Choudhury and Counts, 2013; Hannak et al., 2012; Thelwall et al., 2011; Pak and Paroubek, 2010). For instance, Golder and and Macy study aggregate global trends in “mood,” and show, among other things, that people wake up in a relatively good mood that decays as the day progresses (Golder and Macy, 2011). Bollen et al. (2011c) show that tweets from users who took a standard diagnostic instrument for mood are often tied to current events, such as elections and holidays. Relatively little of this work has focused on suicide or related psychological conditions. Masuda et al. (2013) study suicide on mixi (a Japanese social networking service). Cheng et al. (2012) co</context>
</contexts>
<marker>Thelwall, Buckley, Paltoglou, 2011</marker>
<rawString>Mike Thelwall, Kevan Buckley, and Georgios Paltoglou. 2011. Sentiment in Twitter events. Journal of the American Society for Information Science and Technology, 62(2):406–418.</rawString>
</citation>
<citation valid="true">
<authors>
<author>Wenbo Wang</author>
<author>Lu Chen</author>
<author>Krishnaprasad Thirunarayan</author>
<author>Amit P Sheth</author>
</authors>
<title>Harnessing Twitter ‘Big Data’ for automatic emotion identification.</title>
<date>2012</date>
<booktitle>In Privacy, Security, Risk and Trust (PASSAT), 2012 International Conference on and 2012 International Confernece on Social Computing (SocialCom),</booktitle>
<pages>587--592</pages>
<publisher>IEEE.</publisher>
<contexts>
<context position="11196" citStr="Wang et al., 2012" startWordPosition="1731" endWordPosition="1734">e particularly fuzzy semantics in the gray zone where neutrality and emotion meet (Alm, 2008). These kinds of problem characteristics bring with them their own set of demanding challenges from a computational perspective (Alm, 2011). Yet, the nature of such problems make them incredibly important to study, despite the challenges involved. Sentiment analysis has been widely studied in a number of computational settings, including on various social networking sites. A rather substantial body of work already exists on the use of Twitter to study emotion (Bollen et al., 2011b; Dodds et al., 2011; Wang et al., 2012; Pfitzner et al., 2012; Kim et al., 2012; Bollen et al., 2011a; Pfitzner et al., 2012; Bollen et al., 2011c; Mohammad, 2012; Golder and Macy, 2011; De Choudhury et al., 2012a; De Choudhury et al., 2012b; De Choudhury et al., 2013; De Choudhury and Counts, 2013; Hannak et al., 2012; Thelwall et al., 2011; Pak and Paroubek, 2010). For instance, Golder and and Macy study aggregate global trends in “mood,” and show, among other things, that people wake up in a relatively good mood that decays as the day progresses (Golder and Macy, 2011). Bollen et al. (2011c) show that tweets from users who took</context>
</contexts>
<marker>Wang, Chen, Thirunarayan, Sheth, 2012</marker>
<rawString>Wenbo Wang, Lu Chen, Krishnaprasad Thirunarayan, and Amit P Sheth. 2012. Harnessing Twitter ‘Big Data’ for automatic emotion identification. In Privacy, Security, Risk and Trust (PASSAT), 2012 International Conference on and 2012 International Confernece on Social Computing (SocialCom), pages 587–592. IEEE.</rawString>
</citation>
<citation valid="true">
<authors>
<author>Kathryn Womack</author>
<author>Wilson McCoy</author>
<author>Cecilia Ovesdotter Alm</author>
<author>Cara Calvelli</author>
<author>Jeff B Pelz</author>
<author>Pengcheng Shi</author>
<author>Anne Haake</author>
</authors>
<title>Disfluencies as extra-propositional indicators of cognitive processing.</title>
<date>2012</date>
<booktitle>In Proceedings of the Workshop on ExtraPropositional Aspects of Meaning in Computational Linguistics,</booktitle>
<pages>1--9</pages>
<publisher>Association</publisher>
<institution>for Computational Linguistics.</institution>
<contexts>
<context position="8801" citStr="Womack et al., 2012" startWordPosition="1354" endWordPosition="1357">e., depression, substance abuse, suicidal ideation, self-harm, or impulsivity), family history of suicide, interpersonal conflicts (i.e., family violence or bullying), and means for suicidal behavior (e.g., firearms), are commonly cited risk factors for suicidal behavior (Nock et al., 2008; Crosby et al., 2011; Gaynes et al., 2004; Harriss and Hawton, 2005; Shaffer et al., 2004; Brown et al., 2000). Regarding the use of annotation for predictive modeling, evidence suggests that when it comes to judgments that involve clinical phenomena, experts and novices behave differently (Li et al., 2012; Womack et al., 2012). Such distinctions intuitively make sense, as the learning of medical domain knowledge requires advanced education in 108 conjunction with substantial practical field experience. In a task such as medical image inspection, the subtle cues that point an observer to evidence that allow them to identify a clinical condition, while accessible to experts with training and perceptual expertise to guide their exploration, are likely to be missed by novices who lack that background and clinical understanding. Such expertise can then be integrated into human-centered health-IT systems (Guo et al., 201</context>
</contexts>
<marker>Womack, McCoy, Alm, Calvelli, Pelz, Shi, Haake, 2012</marker>
<rawString>Kathryn Womack, Wilson McCoy, Cecilia Ovesdotter Alm, Cara Calvelli, Jeff B. Pelz, Pengcheng Shi, and Anne Haake. 2012. Disfluencies as extra-propositional indicators of cognitive processing. In Proceedings of the Workshop on ExtraPropositional Aspects of Meaning in Computational Linguistics, pages 1–9. Association for Computational Linguistics.</rawString>
</citation>
<citation valid="true">
<authors>
<author>Matt Wray</author>
<author>Cynthia Colen</author>
<author>Bernice Pescosolido</author>
</authors>
<title>The sociology of suicide. Annual Review of Sociology,</title>
<date>2011</date>
<pages>37--505</pages>
<contexts>
<context position="12773" citStr="Wray et al., 2011" startWordPosition="1996" endWordPosition="1999">e data collection for suicide prevention. Jashinsky et al. (2013) show correlations between frequency in tweets related to suicide and actual suicide in the 50 United States of America. Sadilek et al. (2014) study depression on Twitter. De Choudhury and collaborators studied depression—in general and post-partum—in Twitter (De Choudhury et al., 2012a; De Choudhury et al., 2012b; De Choudhury et al., 2013; De Choudhury and Counts, 2013) and Facebook (De Choudhury et al., 2014). Homan et al. (2014) investigate depression in TrevorSpace. A number of social theories of suicide have been proposed (Wray et al., 2011), but most of this work was with respect to offline social systems. 3 Methods Our methods involve four main phases: (1) We filtered a corpus, obtained from Sadilek et al. (2012), of approximately 2.5 million tweets from 6,237 unique users in the New York City area that were sent during a 1-month period between May and June, 2010, into a set of 2,000 tweets that are relatively likely to be centered around suicide risk factors. (2) We annotated each of these 2,000 tweets with their level of distress, and also analyzed the annotations in detail. (3) We then trained support vector machines and top</context>
</contexts>
<marker>Wray, Colen, Pescosolido, 2011</marker>
<rawString>Matt Wray, Cynthia Colen, and Bernice Pescosolido. 2011. The sociology of suicide. Annual Review of Sociology, 37:505–528.</rawString>
</citation>
</citationList>
</algorithm>
</algorithms>
<?xml version="1.0" encoding="UTF-8"?>
<algorithms version="110505">
<algorithm name="SectLabel" version="110505">
<variant no="0" confidence="0.000274">
<title confidence="0.994773">
Metaphor Detection through Term Relevance
</title>
<author confidence="0.979073">
Marc Schulder Eduard Hovy
</author>
<affiliation confidence="0.819943333333333">
Spoken Language Systems Language Technologies Institute
Saarland University Carnegie Mellon University
Saarbr¨ucken, Germany Pittsburth, PA, USA
</affiliation>
<email confidence="0.998574">
marc.schulder@lsv.uni-saarland.de hovy@cs.cmu.edu
</email>
<sectionHeader confidence="0.993877" genericHeader="abstract">
Abstract
</sectionHeader>
<bodyText confidence="0.999935538461538">
Most computational approaches to
metaphor detection try to leverage either
conceptual metaphor mappings or selec-
tional preferences. Both require extensive
knowledge of the mappings/preferences
in question, as well as sufficient data for
all involved conceptual domains. Creating
these resources is expensive and often
limits the scope of these systems.
We propose a statistical approach to
metaphor detection that utilizes the rarity
of novel metaphors, marking words that
do not match a text’s typical vocabulary
as metaphor candidates. No knowledge
of semantic concepts or the metaphor’s
source domain is required.
We analyze the performance of this
approach as a stand-alone classifier and
as a feature in a machine learning model,
reporting improvements in F1 measure
over a random baseline of 58% and 68%,
respectively. We also observe that, as
a feature, it appears to be particularly
useful when data is sparse, while its effect
diminishes as the amount of training data
increases.
</bodyText>
<sectionHeader confidence="0.999133" genericHeader="introduction">
1 Introduction
</sectionHeader>
<bodyText confidence="0.999929075471698">
Metaphors are used to replace complicated or un-
familiar ideas with familiar, yet unrelated concepts
that share an important attribute with the intended
idea. In NLP, detecting metaphors and other non-
literal figures of speech is necessary to interpret
their meaning correctly. As metaphors are a pro-
ductive part of language, listing known examples
is not sufficient. Most computational approaches
to metaphor detection are based either on the the-
ory of conceptual mappings (Lakoff and John-
son, 1980) or that of preference violation (Wilks,
1978).
Lakoff and Johnson (1980) showed that
metaphors have underlying mappings between
two conceptual domains: The figurative source
domain that the metaphor is taken from and the
literal target domain of the surrounding context in
which it has to be interpreted. Various metaphors
can be based on the same conceptual metaphor
mapping, e.g. both “The economy is a house of
cards” and “the stakes of our debates appear
small” match POLITICS IS A GAME.
Another attribute of metaphors is that they
violate semantic selectional preferences (Wilks,
1978). The theory of selectional preference ob-
serves that verbs constrain their syntactic argu-
ments by the semantic concepts they accept in
these positions. Metaphors violate these con-
straints, combining incompatible concepts.
To make use of these theories, extensive knowl-
edge of pairings (either mappings or preferences)
and the involved conceptual domains is required.
Especially in the case of conceptual mappings, this
makes it very difficult for automated systems to
achieve appropriate coverage of metaphors. Even
when limited to a single target domain, detecting
all metaphors would require knowledge of many
metaphoric source domains to cover all relevant
mappings (which themselves have to be known,
too). As a result of this, many systems attempt
to achieve high precision for specific mappings,
rather than provide general coverage.
Many approaches (Gedigian et al., 2006; Krish-
nakumaran and Zhu, 2007; Mohler et al., 2013;
Tsvetkov et al., 2013, and more) make use of man-
ually crafted knowledge bases such as WordNet or
FrameNet to establish concept domains. Other re-
cent works establish domains via topic modeling
(Shutova et al., 2010; Heintz et al., 2013), ad-hoc
clustering (Strzalkowski et al., 2013) or by using
semantic similarity vectors (Hovy et al., 2013).
We introduce term relevance as a measure for
how “out of place” a word is in a given con-
</bodyText>
<page confidence="0.989011">
18
</page>
<note confidence="0.806226">
Proceedings of the Second Workshop on Metaphor in NLP, pages 18–26,
Baltimore, MD, USA, 26 June 2014. c�2014 Association for Computational Linguistics
</note>
<bodyText confidence="0.99560395">
text. Our hypothesis is that words will often be
out of place because they are not meant literally,
but rather metaphorically. Term relevance is based
on term frequency measures for target domains
and mixed-domain data. The advantage of this
approach is that it only requires knowledge of a
text’s literal target domain, but none about any
source domains or conceptual mappings. As it
does not require sentence structure information,
it is also resistant to noisy data, allowing the use
of large, uncurated corpora. While some works
that utilize domain-mappings circumvent the need
for pre-existing source data by generating it them-
selves (Strzalkowski et al., 2013; Mohler et al.,
2013), our approach is truly source-independent.
We present a threshold classifier that uses term
relevance as its only metric for metaphor detec-
tion. In addition we evaluate the impact of term
relevance at different training sizes.
Our contributions are:
</bodyText>
<listItem confidence="0.990988625">
• We present a measure for non-literalness that
only requires data for the literal domain(s) of
a text.
• Our approach detects metaphors indepen-
dently of their source domain.
• We report improvements for F1 of 58%
(stand-alone) and 68% (multi-feature) over a
random baseline.
</listItem>
<sectionHeader confidence="0.949261" genericHeader="method">
2 Term Relevance
</sectionHeader>
<bodyText confidence="0.999975642857143">
We hypothesize that novel metaphoric language
is marked by its unusualness in a given context.
There will be a clash of domains, so the vocab-
ulary will be noticeably different1. Therefore, an
unusual choice of words may indicate metaphoric-
ity (or non-literalness, at the least).
We measure this fact through a domain-specific
term relevance metric. The metric consists of
two features: Domain relevance, which measures
whether a term is typical for the literal target do-
main of the text, and common relevance, which
indicates terms that are so commonly used across
domains that they have no discriminative power.
If a term is not typical for a text’s domain (i.e.
</bodyText>
<footnote confidence="0.9871168">
1Strongly conventionalized metaphors will not meet this
expectation, as they have become part of the target domain’s
vocabulary. Such metaphors can be easily detected by con-
ventional means, such as knowledge bases. Our concern is
therefore focused on novel metaphors.
</footnote>
<bodyText confidence="0.999635625">
has a low relevance), but is not very common ei-
ther, it is considered a metaphor candidate. This
can of course be extended to multiple literal do-
mains (e.g. a political speech on fishing regula-
tions will have both governance and maritime vo-
cabulary), in which case a word is only considered
as a metaphor if it is untypical for all domains in-
volved.
</bodyText>
<subsectionHeader confidence="0.965462">
2.1 Metric
</subsectionHeader>
<bodyText confidence="0.999430076923077">
We base domain relevance on TF-IDF (term fre-
quency inverse document frequency), which is
commonly used to measure the impact of a term
on a particular document. Terms with a great im-
pact receive high scores, while low scores are as-
signed to words that are either not frequent in the
document or otherwise too frequent among other
documents.
We adapt this method for domain relevance (dr)
by treating all texts of a domain as a single “doc-
ument”. This new term frequency inverse domain
frequency measures the impact of a term on the
domain.
</bodyText>
<equation confidence="0.9589406">
tf t d = # of term t in domain d 1
dom (, ) # of terms in domain d ( )
# of domains
idfdom(t) = log (2) # of domains containing t
dr(t, d) = tfdom(t, d) x idfdom(t) (3)
</equation>
<bodyText confidence="0.968361142857143">
To detect metaphors, we look for terms with low
scores in this feature. However, due to the nature
of TF-IDF, a low score might also indicate a word
that is common among all domains. To filter out
such candidates, we use normalized document fre-
quency as a common relevance indicator.
# of documents containing t
</bodyText>
<equation confidence="0.9405865">
cr(t) = (4)
# of documents
</equation>
<bodyText confidence="0.999970166666666">
In theory, we could also use domain frequency
to determine common relevance, as we already
compute it for domain relevance. However, as this
reduces the feature’s granularity and otherwise be-
haves the same (as long as domains are of equal
size), we keep regular document frequency.
</bodyText>
<subsectionHeader confidence="0.998463">
2.2 Generating Domains
</subsectionHeader>
<bodyText confidence="0.9820075">
We need an adequate number of documents for
each domain of interest to compute domain rele-
vance for it. We require specific data for the literal
domain(s) of a text, but none for the metaphor’s
</bodyText>
<page confidence="0.998547">
19
</page>
<bodyText confidence="0.999795454545454">
source domains. This reduces the required num-
ber of domain data sets significantly without rul-
ing out any particular metaphor mappings.
We extract domain-specific document collec-
tions from a larger general corpus, using the key-
word query search of Apache Lucene2, a software
for indexed databases. The keywords of the query
search are a set of seed terms that are considered
typical literal terms for a domain. They can be
manually chosen or extracted from sample data.
For each domain we extract the 10,000 highest
ranking documents and use them as the domain’s
dataset.
Afterwards, all remaining documents are ran-
domly assigned to equally sized pseudo-domain
datasets. These pseudo-domains allow us to com-
pute the inverse of the domain frequency for the
TF-IDF without the effort of assigning all docu-
ments to proper domains. The documentfrequency
score that will be used as common relevance is di-
rectly computed on the documents of the complete
corpus.
</bodyText>
<sectionHeader confidence="0.996648" genericHeader="method">
3 Data
</sectionHeader>
<bodyText confidence="0.9999590625">
We make use of two different corpora. The first is
the domain-independent corpus required for com-
puting term relevance. The second is an evalua-
tion corpus for the governance domain on which
we train and test our systems.
Both corpora are preprocessed using NLTK
(Loper and Bird, 2002)3. After tokenization, stop-
words and punctuation are removed, contractions
expanded (e.g. we’ve to we have) and numbers
generalized (e.g. 1990’s to @’s). The remaining
words are reduced to their stem to avoid data spar-
sity due to morphological variation.
In case of the domain corpus, we also removed
generic web document contents, such as HTML
mark-up, JavaScript/CSS code blocks and similar
boilerplate code4.
</bodyText>
<subsectionHeader confidence="0.997037">
3.1 Domain Corpus
</subsectionHeader>
<bodyText confidence="0.999489333333333">
As a basis for term relevance, we require a large
corpus that is domain-independent and ideally also
style-independent (i.e. not a newspaper corpus or
</bodyText>
<footnote confidence="0.993909166666667">
2http://lucene.apache.org/core/
3http://nltk.org
4Mark-up and boilerplate removal scripts adapted
from http://love-python.blogspot.com/
2011/04/html-to-text-in-python.html and
http://effbot.org/zone/re-sub.htm
</footnote>
<bodyText confidence="0.999693136363636">
Wikipedia). The world wide web meets these re-
quirements. However, we cannot use public online
search engines, such as Google or Bing, because
they do not allow a complete overview of their in-
dexed documents. As we require this provide to
generate pseudo-domains and compute the inverse
document/domain frequencies, we use a precom-
piled web corpus instead.
ClueWeb095 contains one billion web pages,
half of which are English. For reasons of process-
ing time and data storage, we limited our experi-
ments to a single segment (en0000), containing 3
million documents. The time and storage consid-
erations apply to the generation of term relevance
values during preprocessing, due to the require-
ments of database indexing. They do not affect
the actual metaphor detection process, therefore,
we do not expect scalability to be an issue. As
ClueWeb09 is an unfiltered web corpus, spam fil-
tering was required. We removed 1.2 million spam
documents using the Waterloo Spam Ranking for
ClueWeb096 by Cormack et al. (2011).
</bodyText>
<subsectionHeader confidence="0.9988">
3.2 Evaluation Corpus
</subsectionHeader>
<bodyText confidence="0.9998336">
Evaluation of the two classifiers is done with a cor-
pus of documents related to the concept of gov-
ernance. Texts were annotated for metaphoric
phrases and phrases that are decidedly in-domain,
as well as other factors (e.g. affect) that we will not
concern ourselves with. The focus of annotation
was to exhaustively mark metaphors, irrespective
of their novelty, but avoid idioms and metonymy.
The corpus is created as part of the MICS:
Metaphor Interpretation in terms of Culturally-
relevant Schemas project by the U.S. Intelligence
Advanced Research Projects Activity (IARPA).
We use a snapshot containing 2,510 English sen-
tences, taken from 312 documents. Of the 2,078
sentences that contain metaphors, 72% contain
only a single metaphoric phrase. The corpus con-
sists of around 48k tokens, 12% of which are parts
of metaphors. Removing stopwords and punctua-
tion reduces it to 23k tokens and slightly skews the
distribution, resulting in 15% being metaphors.
We divide the evaluation data into 80% devel-
opment and 20% test data. All reported results are
based on test data. Where training data is required
for model training (see section 5), ten-fold cross
validation is performed on the development set.
</bodyText>
<footnote confidence="0.999481333333333">
5http://lemurproject.org/clueweb09/
6http://plg.uwaterloo.ca/˜gvcormac/
clueweb09spam/
</footnote>
<page confidence="0.990036">
20
</page>
<subsectionHeader confidence="0.868677">
Subdomain Seed Terms
</subsectionHeader>
<bodyText confidence="0.999274357142857">
Executive administer rule govern lead
Legislative pass law regulate debate parliament
Judicial judge hearing case rule case
sentence
Administr. administer manage issue permits
analyze study facilitate obstruct
Enforcement enforce allow permit require war
make mandate defeat overcome
Economy budget tax spend plan finances
Election vote campaign canvass elect defeat
form party create platform
Acceptance government distrust (de)legitimize
authority reject oppose strike flag
protest pride salute march accept
</bodyText>
<tableCaption confidence="0.884726">
Table 1: Manually selected seed terms for docu-
</tableCaption>
<bodyText confidence="0.936659666666667">
ment search queries. The 10k documents with the
highest relevance to the seeds are assigned to the
subdomain cluster.
</bodyText>
<sectionHeader confidence="0.986012" genericHeader="method">
4 Basic Classification
</sectionHeader>
<bodyText confidence="0.998814166666667">
To gain an impression of the differentiating power
of tf-idf in metaphor detection, we use a basic
threshold classifier (tc) that uses domain relevance
(dr) and common relevance (cr) as its only fea-
tures. Given a word w, a target domain d and two
thresholds 6 and γ:
</bodyText>
<equation confidence="0.635524666666667">
metaphor if dr(w, d) &lt; 6
and cr(w) &lt; γ (5)
literal otherwise
</equation>
<bodyText confidence="0.99996925">
In cases where a text has more than one literal do-
main or multiple relevant subdomains are avail-
able, a word is only declared a metaphor if it is
not considered literal for any of the (sub)domains.
</bodyText>
<subsectionHeader confidence="0.996934">
4.1 Seed Terms
</subsectionHeader>
<bodyText confidence="0.973598875">
The threshold classifier is evaluated using two dif-
ferent sets of seed terms. The first set is com-
posed of 60 manually chosen terms7 from eight
governance subdomains. These are shown in table
1. Each subdomain corpus consists of its 10,000
highest ranking documents. We do not subdi-
vide the evaluation corpus into these subdomains.
Rather, we assume that each sentence belongs to
</bodyText>
<footnote confidence="0.466373666666667">
7Terms were chosen according to human understanding
of typical terms for governance. No optimization of the term
choices was performed thereafter.
</footnote>
<table confidence="0.7043467">
principl financi legisl congress crisi
corpor famili middl compani futur
countri global negoti medicaid unit
industri promis polici constitut save
obama health creat capitalist hous
clinton nation dream american busi
nuclear amend great medicar care
econom million feder recoveri job
commun potenti polit freedom law
prosper energi elect program new
</table>
<tableCaption confidence="0.819464">
Table 2: The fifty stems with the highest tf-idf
</tableCaption>
<bodyText confidence="0.923225230769231">
score in the gold data. Used as seed terms for doc-
ument search, generating a single governance do-
main. Stems are listed in no particular order.
all eight subdomains8, so a word is only consid-
ered a metaphor if it is non-literal for all of them.
Preliminary experiments showed that this provides
better performance than using a single domain cor-
pus with more documents.
As the first set of seeds is chosen without sta-
tistical basis, the resulting clusters might miss im-
portant aspects of the domain. To ensure that our
evaluation is not influenced by this, we also in-
troduce a second seed set, which is directly based
on the development data. As we mentioned in
section 3.2, sentences in the MICS corpus were
not only annotated for metaphoric phrases, but
also for such that are decidedly domain-relevant.
For example in the sentence “Our economy is the
strongest on earth”, economy is annotated as in-
domain and strongest as metaphor.
Based on these annotations, we divide the en-
tire development data into three bags of words,
one each for metaphor, in-domain and unmarked
words. We then compute TF-IDF values for these
bags, as we did for the domain clusters. The fifty
terms9 that score highest for the in-domain bag
(i.e. those that make the texts identifiable as gover-
nance texts) are used as the second set of seeds (ta-
ble 2). It should be noted that while the seeds were
based on the evaluation corpus, the resulting term
relevance features were nevertheless computed us-
ing clusters extracted from the web corpus.
8As our evaluation corpus does not specify secondary do-
mains for its texts (e.g. fishery), we chose not to define any
further domains at this point.
9Various sizes were tried for the seed set. Using fifty
terms offered the best performance, being neither too specific
nor watering down the cluster quality. It is also close to the
size of our first seed set.
</bodyText>
<equation confidence="0.4465485">
⎧
⎨
⎩
tc(w, d) =
</equation>
<page confidence="0.967489">
21
</page>
<table confidence="0.9994866">
F1 Prec Rec
Random 0.222 0.142 0.500
All Metaphor 0.249 0.142 1.000
T-hold: Manual Seeds 0.350 0.276 0.478
T-hold: 50-best Seeds 0.346 0.245 0.591
</table>
<tableCaption confidence="0.998446">
Table 3: Summary of best performing settings
</tableCaption>
<bodyText confidence="0.8491544">
for each threshold classifier model. Bold num-
bers indicate best performance; slanted bold num-
bers: best threshold classifier recall. All results
are significantly different from the baselines with
p &lt; 0.01.
</bodyText>
<subsectionHeader confidence="0.869307">
4.2 Evaluation
</subsectionHeader>
<bodyText confidence="0.971685476190477">
We evaluate and optimize our systems for the F1
metric. In addition we provide precision and re-
call. Accuracy on the other hand proved an inap-
propriate metric, as the prevalence of literal words
in our data resulted in a heavy bias. We eval-
uate on a token-basis, as half of the metaphoric
phrases consist of a single word and less than 15%
are more than three words long (including stop-
words, which are filtered out later). Additionally,
evaluating on a phrase-basis would have required
grouping non-metaphor sections into phrases of a
similar format.
Based on dev set performance, we choose a do-
main relevance threshold 6 = 0.02 and a common
relevance threshold -y = 0.1. We provide a ran-
dom baseline, as well as one that labels all words
as metaphors, as they are the most frequently en-
countered baselines in related works. Results are
shown in table 3.
Both seed sets achieve similar F-scores, beating
the baselines by between 39% and 58%, but their
precision and recall performance differs notably.
Both models are significantly better than the base-
line and significantly different from one another
with p &lt; 0.01. Significance was computed for a
two-tailed t-test using sigf (Pad´o, 2006)10.
Using manually chosen seed terms results in a
recall rate that is slightly worse than chance, but
it is made up by the highest precision. The fact
that this was achieved without expert knowledge
or term optimization is encouraging.
The classifier using the fifty best governance
terms shows a stronger recall, most likely be-
10http://www.nlpado.de/˜sebastian/
software/sigf.shtml
cause the seeds are directly based on the develop-
ment data, resulting in a domain cluster that more
closely resembles the evaluation corpus. Preci-
sion, on the other hand, is slightly below that of the
manual seed classifier. This might be an effect of
the coarser granularity that a single domain score
offers, as opposed to eight subdomain scores.
</bodyText>
<sectionHeader confidence="0.997831" genericHeader="method">
5 Multi-Feature Classification
</sectionHeader>
<bodyText confidence="0.999967961538461">
Using term relevance as the only factor for
metaphor detection is probably insufficient.
Rather, we anticipate to use it either as a pre-
filtering step or as a feature for a more complex
metaphor detection system. To simulate the latter,
we use an off-the-shelf machine learning classifier
with which we test how term relevance interacts
with other typical word features, such as part of
speech. As we classify all words of a sentence, we
treat the task as a binary sequential labeling task.
Preliminary tests were performed with HMM,
CRF and SVM classifiers. CRF performance was
the most promising. We use CRFsuite (Okazaki,
2007)11, an implementation of conditional random
fields that supports continuous values via scaling
factors. Training is performed on the development
set using ten-fold cross validation.
We present results for bigram models. Larger n-
grams were inspected, too, including models with
look-ahead functionality. While they were slightly
more robust with regard to parameter changes,
there was no improvement over the best bigram
model. Also, as metaphor processing still is a low
resource task for which sufficient training data is
hard to come by, bigrams are the most accessible
and representative option.
</bodyText>
<subsectionHeader confidence="0.997215">
5.1 Training Features
</subsectionHeader>
<bodyText confidence="0.9997655">
We experimented with different representations
for the term relevance features. As they are con-
tinuous values, they could be used as continuous
features. Alternatively, they could be represented
as binary features, using a cut-off value as for our
threshold classifier. In the end, we chose a hy-
brid approach where thresholds are used to create
binary features, but are also scaled according to
their score. Thresholds were again determined on
the dev set and set to 6 = 0.02 and -y = 0.79.
Each domain receives an individual domain rel-
evance feature. There is only a single common rel-
</bodyText>
<footnote confidence="0.976605">
11http://www.chokkan.org/software/
crfsuite/
</footnote>
<page confidence="0.996951">
22
</page>
<table confidence="0.999379285714286">
F1 Prec Rec
All Metaphor 0.249 0.142 1.000
T-hold: Manual Seeds 0.350 0.276 0.478
CRF: Basic 0.187 0.706 0.108
CRF: Rel 0.219 0.683 0.130
CRF: PosLex 0.340 0.654 0.230
CRF: PosLexRel 0.373 0.640 0.263
</table>
<tableCaption confidence="0.9741604">
Table 4: Summary of best performing settings for
each CRF model. Bold numbers indicate best per-
formance; slanted bold numbers: best CRF re-
call. All results are significantly different from the
baseline with p &lt; 0.01.
</tableCaption>
<bodyText confidence="0.999075588235294">
evance feature, as it is domain-independent. Sur-
prisingly, we found no noteworthy difference in
performance between the two seed sets (manual
and 50-best). Therefore we only report results for
the manual seeds.
In addition to term relevance, we also provide
part of speech (pos) and lexicographer sense (lex)
as generic features. The part of speech is auto-
matically generated using NLTK’s Maximum En-
tropy POS Tagger, which was trained on the Penn
Treebank. To have a semantic feature to compare
our relevance weights to, we include WordNet’s
lexicographer senses (Fellbaum, 1998), which are
coarse-grained semantic classes. Where a word
has more than one sense, the first was chosen. If no
sense exists for a word, the word is given a sense
unknown placeholder value.
</bodyText>
<subsectionHeader confidence="0.997681">
5.2 Performance Evaluation
</subsectionHeader>
<bodyText confidence="0.999964875">
Performance of the CRF system (see table 4)
seems slightly disappointing at first when com-
pared to our threshold classifier. The best-
performing CRF beats the threshold classifier by
only two points of F-score, despite considerably
richer training input. Precision and recall perfor-
mance are reversed, i.e. the CRF provides a higher
precision of 0.6, but only detects one out of four
metaphor words. All models provide stable results
for all folds, their standard deviation (about 0.01
for F1) being almost equal to that of the baseline.
All results are significantly different from the
baseline as well as from each other with p &lt; 0.01,
except for the precision scores of the three non-
basic CRF models, which are significantly differ-
ent from each other with p &lt; 0.05.
</bodyText>
<figure confidence="0.993931481481482">
F1
0.35
0.25
0.15
0.4
0.3
0.2
0 0
200 400 600 800 1000 1200 1400 1600 1800
CRF Basic
CRF PosLex
Threshold
Models
+ Relevance
+ Relevance
Baseline
0.4
0.35
0.3
0.25
0.2
0.15
0.1
0.05
0.1
0.05
Number of Training Sentences
</figure>
<figureCaption confidence="0.94896375">
Figure 1: Performance curves for various training data sizes. Models with term relevance features (solid
lines) outperform models without term relevance (dashed lines) at 1400 sentences. 1800 sentences rep-
resent the entire training set. Baseline (thin line) and best threshold classifier (dotted line) provided for
reference.
</figureCaption>
<page confidence="0.994335">
23
</page>
<bodyText confidence="0.9999118">
Adding term relevance provides a consistent
boost of 0.025 to the F-score. This boost, however,
is rather marginal in comparison to the one pro-
vided by part of speech and lexicographer sense.
A possible reason for this could be that the item
weights learned during training correspond too
closely to our term relevance scores, thus making
them obsolete when enough training data is pro-
vided. The next section explores this possibility
by comparing different amounts of training data.
</bodyText>
<subsectionHeader confidence="0.999498">
5.3 Training Size Evaluation
</subsectionHeader>
<bodyText confidence="0.99922964">
With 2000 metaphoric sentences, the dataset we
used was already among the largest annotated cor-
pora. By reducing the amount of training data we
evaluate whether term relevance is an efficient fea-
ture when data is sparse. To this end, we repeat
our ten-fold cross validations, but withhold some
of the folds from each training set.
Figure 1 compares the performance of CRF fea-
ture configurations with and without term rele-
vance. In both cases adding term relevance out-
performs the standard configuration’s top perfor-
mance with 400 sentences less, saving about a
quarter of the training data.
In figure 2 we also visualize the relative gain
that adding term relevance provides. As one can
see, small datasets profit considerably more from
our metric. Given only 200 sentences, the PosLex
model receives 4.7 times the performance gain
from term relevance it got at at maximum training
size. The basic model has a factor of 6.8. This sup-
ports our assumption that term relevance is similar
to the item weights learned during CRF training.
As labeled training data is considerably more ex-
pensive to create than corpora for term relevance,
this is an encouraging observation.
</bodyText>
<sectionHeader confidence="0.999879" genericHeader="method">
6 Related Work
</sectionHeader>
<bodyText confidence="0.999915888888889">
For a comprehensive review on computational
metaphor detection, see Shutova (2010). We limit
our discussion to publications that were not cov-
ered by the review. While there are several papers
evaluating on the same domain, direct comparison
proved to be difficult, as many works were either
evaluated on a sentence level (which our data was
inappropriate for, as 80% of sentences contained
metaphors) or did not provide coverage informa-
tion. Another difference was that most evaluations
were performed on balanced datasets, while our
own data was naturally skewed for literal terms.
Strzalkowski et al. (2013) follow a related hy-
pothesis, assuming that metaphors lack topical re-
latedness to in-domain words while being syntac-
tically connected to them. Instead of using the
metaphor candidate’s relevance to a target domain
corpus to judge relatedness, they circumvent the
</bodyText>
<figure confidence="0.99897925">
Relative Gain
150 %
125 %
100 %
75 %
50 %
25 %
0 %
CRF Basic
CRF PosLex
Models
75 %
50 %
25 %
0 %
150 %
125 %
100 %
200 400 600 800 1000 1200 1400 1600 1800
Number of Training Sentences
</figure>
<figureCaption confidence="0.998067">
Figure 2: Relative performance gain of models obtained from addition of term relevance features.
</figureCaption>
<page confidence="0.993978">
24
</page>
<bodyText confidence="0.999894208955224">
need for pre-existing source data by generating
ad-hoc collocation clusters and check whether the
two highest ranked source clusters share vocab-
ulary with the target domain. Further factors in
their decision process are co-ocurrences in sur-
rounding sentences and psycholinguistic image-
ability scores (i.e. how easy it is to form a men-
tal picture of a word). Evaluating on data in the
governance domain, they achieve an accuracy of
71% against an all metaphor baseline of 46%, but
report no precision or recall.
Mohler et al. (2013) and Heintz et al. (2013)
also evaluate on the governance domain. Rather
than detecting metaphors at a word-level, both de-
tect whether sentences contain metaphors. Mohler
et al. (2013) compare semantic signatures of sen-
tences to signatures of known metaphors. They,
too, face a strong bias against the metaphor label
and show how this can influence the balance be-
tween precision and recall. Heintz et al. (2013)
classify sentences as containing metaphors if their
content is related to both a target and source do-
main. They create clusters via topic modeling and,
like us, use manually chosen seed terms to asso-
ciate them with domains. Unlike our approach,
theirs also requires seeds of all relevant source do-
mains. They observe that identifying metaphors,
even on a sentence level, is difficult even for ex-
perienced annotators, as evidenced by an inter-
annotator agreement of κ = 0.48.
Shutova et al. (2010) use manually annotated
seed sentences to generate source and target do-
main vocabularies via spectral clustering. The re-
sulting domain clusters are used for selectional
preference induction in verb-noun relations. They
report a high precision of 0.79, but have no data on
recall. Target concepts appearing in similar lexico-
syntactic contexts are mapped to the same source
concepts. The resulting mappings are then used to
detect metaphors. This approach is notable for its
combination of distributional clustering and selec-
tional preference induction. Verbs and nouns are
clustered into topics and linked through induction
of selectional preferences, from which metaphoric
mappings are deduced. Other works (S´eaghdha,
2010; Ritter et al., 2010) use topic modeling to di-
rectly induce selectional preferences, but have not
yet been applied to metaphor detection.
Hovy et al. (2013) generalize semantic prefer-
ence violations from verb-noun relations to any
syntactic relation and learn these in a supervised
manner, using SVM and CRF models. The CRF
is not the overall best-performing system, but
achieves the highest precision of 0.74 against an
all-metaphor baseline of 0.49. This is in line
with our own observations. While they argue
that metaphor detection should eventually be per-
formed on every word, their evaluation is limited
to a single expression per sentence.
Our work is also related to that of Sporleder and
Li (2009) and Li and Sporleder (2010), in which
they detect idioms through their lack of seman-
tic cohesiveness with their context. Cohesiveness
is measured via co-occurence of idiom candidates
with other parts of a text in web searches. They
do not make use of domains, basing their measure
entirely on the lexical context instead.
</bodyText>
<sectionHeader confidence="0.999046" genericHeader="conclusions">
7 Conclusion
</sectionHeader>
<bodyText confidence="0.99998121875">
We have presented term relevance as a non-
literalness indicator and its use for metaphor de-
tection. We showed that even on its own, term rel-
evance clearly outperforms the baseline by 58%
when detecting metaphors on a word basis.
We also evaluated the utility of term relevance
as a feature in a larger system. Results for this
were mixed, as the general performance of our
system, a sequential CRF classifier, was lower
than anticipated. However, tests on smaller train-
ing sets suggest that term relevance can help when
data is sparse (as it often is for metaphor process-
ing). Also, precision was considerably higher for
CRF, so it might be more useful for cases where
coverage is of secondary importance.
For future work we plan to reimplement the
underlying idea of term relevance with different
means. Domain datasets could be generated via
topic modeling or other clustering means (Shutova
et al., 2010; Heintz et al., 2013) and should also
cover dynamically detected secondary target do-
mains. Instead of using TF-IDF, term relevance
can be modeled using semantic vector spaces
(see Hovy et al. (2013)). While our preliminary
tests showed better performance for CRF than
for SVM, such a change in feature representation
would also justify a re-evaluation of our classifier
choice. To avoid false positives (and thus improve
precision), we could generate ad-hoc source do-
mains, like Strzalkowski et al. (2013) or Shutova
et al. (2010) do, to detect overlooked literal con-
nections between source and target domain.
</bodyText>
<page confidence="0.997401">
25
</page>
<sectionHeader confidence="0.99737" genericHeader="acknowledgments">
Acknowledgements
</sectionHeader>
<bodyText confidence="0.99987445">
We would like to thank the reviewers and proof-
readers for their valuable input.
This research effort was in part supported by
the German Academic Exchange Service (DAAD)
scholarship program PROMOS with funds from
the Federal Ministry of Education and Research
(BMBF), awarded by the International Office of
Saarland University as well as by the Intelligence
Advanced Research Projects Activity (IARPA)
via Department of Defense US Army Research
Laboratory contract number W911NF-12-C-0025.
The U.S. Government is authorized to reproduce
and distribute reprints for Governmental purposes
notwithstanding any copyright annotation thereon.
Disclaimer: The views and conclusions con-
tained herein are those of the authors and should
not be interpreted as necessarily representing the
official policies or endorsements, either expressed
or implied, of DAAD, BMBF, IARPA, DoD/ARL
or the German or U.S. Government.
</bodyText>
<sectionHeader confidence="0.999088" genericHeader="references">
References
</sectionHeader>
<reference confidence="0.999902597560975">
Gordon V. Cormack, Mark D. Smucker, and Charles
L. A. Clarke. 2011. Efficient and effective spam
filtering and re-ranking for large web datasets. In-
formation retrieval, 14(5):441–465.
Christiane Fellbaum. 1998. ed. WordNet: an elec-
tronic lexical database. MIT Press, Cambridge MA,
1:998.
Matt Gedigian, John Bryant, Srini Narayanan, and Bra-
nimir Ciric. 2006. Catching metaphors. In Pro-
ceedings of the HLT/NAACL-06 Workshop on Scal-
able Natural Language Understanding, pages 41–
48.
Ilana Heintz, Ryan Gabbard, Mahesh Srinivasan, David
Barner, Donald S Black, Marjorie Freedman, and
Ralph Weischedel. 2013. Automatic extraction of
linguistic metaphor with lda topic modeling. Pro-
ceedings of the ACL-13 Workshop on Metaphor,
page 58.
Dirk Hovy, Shashank Srivastava, Sujay Kumar Jauhar,
Mrinmaya Sachan, Kartik Goyal, Huiying Li, Whit-
ney Sanders, and Eduard Hovy. 2013. Identifying
metaphorical word use with tree kernels. Proceed-
ings of the ACL-13 Workshop on Metaphor, page 52.
Saisuresh Krishnakumaran and Xiaojin Zhu. 2007.
Hunting elusive metaphors using lexical resources.
In Proceedings of the HLT/NAACL-07 Workshop on
Computational Approaches to Figurative Language,
pages 13–20. Association for Computational Lin-
guistics.
George Lakoff and Mark Johnson. 1980. Metaphors
we live by, volume 111. University of Chicago
Press.
Linlin Li and Caroline Sporleder. 2010. Using gaus-
sian mixture models to detect figurative language in
context. In Proceedings of NAACL-10, pages 297–
300. Association for Computational Linguistics.
Edward Loper and Steven Bird. 2002. NLTK: The
natural language toolkit. In Proceedings of the
COLING/ACL-02 workshop on Interactive presen-
tation sessions, pages 63–70. Association for Com-
putational Linguistics.
Michael Mohler, David Bracewell, David Hinote, and
Marc Tomlinson. 2013. Semantic signatures for
example-based linguistic metaphor detection. Pro-
ceedings of the ACL-13 Workshop on Metaphor,
page 27.
Naoaki Okazaki, 2007. CRFsuite: a fast implementa-
tion of conditional random fields (CRFs).
Sebastian Pad´o, 2006. User’s guide to sigf: Signifi-
cance testing by approximate randomisation.
Alan Ritter, Oren Etzioni, et al. 2010. A latent dirich-
let allocation method for selectional preferences. In
ACT-10, pages 424–434. Association for Computa-
tional Linguistics.
Diarmuid O´ S´eaghdha. 2010. Latent variable models
of selectional preference. In Proceedings of ACL-
10, pages 435–444. Association for Computational
Linguistics.
Ekaterina Shutova, Lin Sun, and Anna Korhonen.
2010. Metaphor identification using verb and noun
clustering. In Proceedings of COLING-10, pages
1002–1010. Association for Computational Linguis-
tics.
Ekaterina Shutova. 2010. Models of metaphor in NLP.
In Proceedings of ACL-10, pages 688–697. Associ-
ation for Computational Linguistics.
Caroline Sporleder and Linlin Li. 2009. Unsupervised
recognition of literal and non-literal use of idiomatic
expressions. In Proceedings of EACL09, pages 754–
762. Association for Computational Linguistics.
Tomek Strzalkowski, George Aaron Broadwell, Sarah
Taylor, Laurie Feldman, Boris Yamrom, Samira
Shaikh, Ting Liu, Kit Cho, Umit Boz, Ignacio Cases,
et al. 2013. Robust extraction of metaphors from
novel data. Proceedings of the ACL-13 Workshop
on Metaphor, page 67.
Yulia Tsvetkov, Elena Mukomel, and Anatole Gersh-
man. 2013. Cross-lingual metaphor detection us-
ing common semantic features. Proceedings of the
ACL-13 Workshop on Metaphor, page 45.
Yorick Wilks. 1978. Making preferences more active.
Artificial Intelligence, 11(3):197–223.
</reference>
<page confidence="0.998058">
26
</page>
</variant>
</algorithm>
<algorithm name="ParsHed" version="110505">
<variant no="0" confidence="0.938540">
<title confidence="0.999307">Metaphor Detection through Term Relevance</title>
<author confidence="0.999827">Marc Schulder Eduard Hovy</author>
<affiliation confidence="0.9985955">Spoken Language Systems Language Technologies Institute Saarland University Carnegie Mellon University</affiliation>
<address confidence="0.999703">Saarbr¨ucken, Germany Pittsburth, PA, USA</address>
<email confidence="0.998662">marc.schulder@lsv.uni-saarland.dehovy@cs.cmu.edu</email>
<abstract confidence="0.997890259259259">Most computational approaches to metaphor detection try to leverage either conceptual metaphor mappings or selectional preferences. Both require extensive knowledge of the mappings/preferences in question, as well as sufficient data for all involved conceptual domains. Creating these resources is expensive and often limits the scope of these systems. We propose a statistical approach to metaphor detection that utilizes the rarity of novel metaphors, marking words that do not match a text’s typical vocabulary as metaphor candidates. No knowledge of semantic concepts or the metaphor’s source domain is required. We analyze the performance of this approach as a stand-alone classifier and as a feature in a machine learning model, improvements in over a random baseline of 58% and 68%, respectively. We also observe that, as a feature, it appears to be particularly useful when data is sparse, while its effect diminishes as the amount of training data increases.</abstract>
</variant>
</algorithm>
<algorithm name="ParsCit" version="110505">
<citationList>
<citation valid="true">
<authors>
<author>Gordon V Cormack</author>
<author>Mark D Smucker</author>
<author>Charles L A Clarke</author>
</authors>
<title>Efficient and effective spam filtering and re-ranking for large web datasets. Information retrieval,</title>
<date>2011</date>
<pages>14--5</pages>
<contexts>
<context position="11005" citStr="Cormack et al. (2011)" startWordPosition="1752" endWordPosition="1755">b pages, half of which are English. For reasons of processing time and data storage, we limited our experiments to a single segment (en0000), containing 3 million documents. The time and storage considerations apply to the generation of term relevance values during preprocessing, due to the requirements of database indexing. They do not affect the actual metaphor detection process, therefore, we do not expect scalability to be an issue. As ClueWeb09 is an unfiltered web corpus, spam filtering was required. We removed 1.2 million spam documents using the Waterloo Spam Ranking for ClueWeb096 by Cormack et al. (2011). 3.2 Evaluation Corpus Evaluation of the two classifiers is done with a corpus of documents related to the concept of governance. Texts were annotated for metaphoric phrases and phrases that are decidedly in-domain, as well as other factors (e.g. affect) that we will not concern ourselves with. The focus of annotation was to exhaustively mark metaphors, irrespective of their novelty, but avoid idioms and metonymy. The corpus is created as part of the MICS: Metaphor Interpretation in terms of Culturallyrelevant Schemas project by the U.S. Intelligence Advanced Research Projects Activity (IARPA</context>
</contexts>
<marker>Cormack, Smucker, Clarke, 2011</marker>
<rawString>Gordon V. Cormack, Mark D. Smucker, and Charles L. A. Clarke. 2011. Efficient and effective spam filtering and re-ranking for large web datasets. Information retrieval, 14(5):441–465.</rawString>
</citation>
<citation valid="true">
<authors>
<author>Christiane Fellbaum</author>
</authors>
<title>WordNet: an electronic lexical database.</title>
<date>1998</date>
<editor>ed.</editor>
<publisher>MIT Press,</publisher>
<location>Cambridge MA,</location>
<contexts>
<context position="21659" citStr="Fellbaum, 1998" startWordPosition="3475" endWordPosition="3476">nt from the baseline with p &lt; 0.01. evance feature, as it is domain-independent. Surprisingly, we found no noteworthy difference in performance between the two seed sets (manual and 50-best). Therefore we only report results for the manual seeds. In addition to term relevance, we also provide part of speech (pos) and lexicographer sense (lex) as generic features. The part of speech is automatically generated using NLTK’s Maximum Entropy POS Tagger, which was trained on the Penn Treebank. To have a semantic feature to compare our relevance weights to, we include WordNet’s lexicographer senses (Fellbaum, 1998), which are coarse-grained semantic classes. Where a word has more than one sense, the first was chosen. If no sense exists for a word, the word is given a sense unknown placeholder value. 5.2 Performance Evaluation Performance of the CRF system (see table 4) seems slightly disappointing at first when compared to our threshold classifier. The bestperforming CRF beats the threshold classifier by only two points of F-score, despite considerably richer training input. Precision and recall performance are reversed, i.e. the CRF provides a higher precision of 0.6, but only detects one out of four m</context>
</contexts>
<marker>Fellbaum, 1998</marker>
<rawString>Christiane Fellbaum. 1998. ed. WordNet: an electronic lexical database. MIT Press, Cambridge MA, 1:998.</rawString>
</citation>
<citation valid="true">
<authors>
<author>Matt Gedigian</author>
<author>John Bryant</author>
<author>Srini Narayanan</author>
<author>Branimir Ciric</author>
</authors>
<title>Catching metaphors.</title>
<date>2006</date>
<booktitle>In Proceedings of the HLT/NAACL-06 Workshop on Scalable Natural Language Understanding,</booktitle>
<pages>41--48</pages>
<contexts>
<context position="3243" citStr="Gedigian et al., 2006" startWordPosition="479" endWordPosition="482">nsive knowledge of pairings (either mappings or preferences) and the involved conceptual domains is required. Especially in the case of conceptual mappings, this makes it very difficult for automated systems to achieve appropriate coverage of metaphors. Even when limited to a single target domain, detecting all metaphors would require knowledge of many metaphoric source domains to cover all relevant mappings (which themselves have to be known, too). As a result of this, many systems attempt to achieve high precision for specific mappings, rather than provide general coverage. Many approaches (Gedigian et al., 2006; Krishnakumaran and Zhu, 2007; Mohler et al., 2013; Tsvetkov et al., 2013, and more) make use of manually crafted knowledge bases such as WordNet or FrameNet to establish concept domains. Other recent works establish domains via topic modeling (Shutova et al., 2010; Heintz et al., 2013), ad-hoc clustering (Strzalkowski et al., 2013) or by using semantic similarity vectors (Hovy et al., 2013). We introduce term relevance as a measure for how “out of place” a word is in a given con18 Proceedings of the Second Workshop on Metaphor in NLP, pages 18–26, Baltimore, MD, USA, 26 June 2014. c�2014 Ass</context>
</contexts>
<marker>Gedigian, Bryant, Narayanan, Ciric, 2006</marker>
<rawString>Matt Gedigian, John Bryant, Srini Narayanan, and Branimir Ciric. 2006. Catching metaphors. In Proceedings of the HLT/NAACL-06 Workshop on Scalable Natural Language Understanding, pages 41– 48.</rawString>
</citation>
<citation valid="true">
<authors>
<author>Ilana Heintz</author>
<author>Ryan Gabbard</author>
<author>Mahesh Srinivasan</author>
<author>David Barner</author>
<author>Donald S Black</author>
<author>Marjorie Freedman</author>
<author>Ralph Weischedel</author>
</authors>
<title>Automatic extraction of linguistic metaphor with lda topic modeling.</title>
<date>2013</date>
<booktitle>Proceedings of the ACL-13 Workshop on Metaphor,</booktitle>
<pages>58</pages>
<contexts>
<context position="3531" citStr="Heintz et al., 2013" startWordPosition="528" endWordPosition="531">t domain, detecting all metaphors would require knowledge of many metaphoric source domains to cover all relevant mappings (which themselves have to be known, too). As a result of this, many systems attempt to achieve high precision for specific mappings, rather than provide general coverage. Many approaches (Gedigian et al., 2006; Krishnakumaran and Zhu, 2007; Mohler et al., 2013; Tsvetkov et al., 2013, and more) make use of manually crafted knowledge bases such as WordNet or FrameNet to establish concept domains. Other recent works establish domains via topic modeling (Shutova et al., 2010; Heintz et al., 2013), ad-hoc clustering (Strzalkowski et al., 2013) or by using semantic similarity vectors (Hovy et al., 2013). We introduce term relevance as a measure for how “out of place” a word is in a given con18 Proceedings of the Second Workshop on Metaphor in NLP, pages 18–26, Baltimore, MD, USA, 26 June 2014. c�2014 Association for Computational Linguistics text. Our hypothesis is that words will often be out of place because they are not meant literally, but rather metaphorically. Term relevance is based on term frequency measures for target domains and mixed-domain data. The advantage of this approac</context>
<context position="26603" citStr="Heintz et al. (2013)" startWordPosition="4297" endWordPosition="4300">of models obtained from addition of term relevance features. 24 need for pre-existing source data by generating ad-hoc collocation clusters and check whether the two highest ranked source clusters share vocabulary with the target domain. Further factors in their decision process are co-ocurrences in surrounding sentences and psycholinguistic imageability scores (i.e. how easy it is to form a mental picture of a word). Evaluating on data in the governance domain, they achieve an accuracy of 71% against an all metaphor baseline of 46%, but report no precision or recall. Mohler et al. (2013) and Heintz et al. (2013) also evaluate on the governance domain. Rather than detecting metaphors at a word-level, both detect whether sentences contain metaphors. Mohler et al. (2013) compare semantic signatures of sentences to signatures of known metaphors. They, too, face a strong bias against the metaphor label and show how this can influence the balance between precision and recall. Heintz et al. (2013) classify sentences as containing metaphors if their content is related to both a target and source domain. They create clusters via topic modeling and, like us, use manually chosen seed terms to associate them wit</context>
<context position="30178" citStr="Heintz et al., 2013" startWordPosition="4875" endWordPosition="4878">. Results for this were mixed, as the general performance of our system, a sequential CRF classifier, was lower than anticipated. However, tests on smaller training sets suggest that term relevance can help when data is sparse (as it often is for metaphor processing). Also, precision was considerably higher for CRF, so it might be more useful for cases where coverage is of secondary importance. For future work we plan to reimplement the underlying idea of term relevance with different means. Domain datasets could be generated via topic modeling or other clustering means (Shutova et al., 2010; Heintz et al., 2013) and should also cover dynamically detected secondary target domains. Instead of using TF-IDF, term relevance can be modeled using semantic vector spaces (see Hovy et al. (2013)). While our preliminary tests showed better performance for CRF than for SVM, such a change in feature representation would also justify a re-evaluation of our classifier choice. To avoid false positives (and thus improve precision), we could generate ad-hoc source domains, like Strzalkowski et al. (2013) or Shutova et al. (2010) do, to detect overlooked literal connections between source and target domain. 25 Acknowle</context>
</contexts>
<marker>Heintz, Gabbard, Srinivasan, Barner, Black, Freedman, Weischedel, 2013</marker>
<rawString>Ilana Heintz, Ryan Gabbard, Mahesh Srinivasan, David Barner, Donald S Black, Marjorie Freedman, and Ralph Weischedel. 2013. Automatic extraction of linguistic metaphor with lda topic modeling. Proceedings of the ACL-13 Workshop on Metaphor, page 58.</rawString>
</citation>
<citation valid="true">
<authors>
<author>Dirk Hovy</author>
<author>Shashank Srivastava</author>
<author>Sujay Kumar Jauhar</author>
<author>Mrinmaya Sachan</author>
<author>Kartik Goyal</author>
<author>Huiying Li</author>
<author>Whitney Sanders</author>
<author>Eduard Hovy</author>
</authors>
<title>Identifying metaphorical word use with tree kernels.</title>
<date>2013</date>
<booktitle>Proceedings of the ACL-13 Workshop on Metaphor,</booktitle>
<pages>52</pages>
<contexts>
<context position="3638" citStr="Hovy et al., 2013" startWordPosition="544" endWordPosition="547">vant mappings (which themselves have to be known, too). As a result of this, many systems attempt to achieve high precision for specific mappings, rather than provide general coverage. Many approaches (Gedigian et al., 2006; Krishnakumaran and Zhu, 2007; Mohler et al., 2013; Tsvetkov et al., 2013, and more) make use of manually crafted knowledge bases such as WordNet or FrameNet to establish concept domains. Other recent works establish domains via topic modeling (Shutova et al., 2010; Heintz et al., 2013), ad-hoc clustering (Strzalkowski et al., 2013) or by using semantic similarity vectors (Hovy et al., 2013). We introduce term relevance as a measure for how “out of place” a word is in a given con18 Proceedings of the Second Workshop on Metaphor in NLP, pages 18–26, Baltimore, MD, USA, 26 June 2014. c�2014 Association for Computational Linguistics text. Our hypothesis is that words will often be out of place because they are not meant literally, but rather metaphorically. Term relevance is based on term frequency measures for target domains and mixed-domain data. The advantage of this approach is that it only requires knowledge of a text’s literal target domain, but none about any source domains o</context>
<context position="28365" citStr="Hovy et al. (2013)" startWordPosition="4574" endWordPosition="4577">o data on recall. Target concepts appearing in similar lexicosyntactic contexts are mapped to the same source concepts. The resulting mappings are then used to detect metaphors. This approach is notable for its combination of distributional clustering and selectional preference induction. Verbs and nouns are clustered into topics and linked through induction of selectional preferences, from which metaphoric mappings are deduced. Other works (S´eaghdha, 2010; Ritter et al., 2010) use topic modeling to directly induce selectional preferences, but have not yet been applied to metaphor detection. Hovy et al. (2013) generalize semantic preference violations from verb-noun relations to any syntactic relation and learn these in a supervised manner, using SVM and CRF models. The CRF is not the overall best-performing system, but achieves the highest precision of 0.74 against an all-metaphor baseline of 0.49. This is in line with our own observations. While they argue that metaphor detection should eventually be performed on every word, their evaluation is limited to a single expression per sentence. Our work is also related to that of Sporleder and Li (2009) and Li and Sporleder (2010), in which they detect</context>
<context position="30355" citStr="Hovy et al. (2013)" startWordPosition="4903" endWordPosition="4906">hat term relevance can help when data is sparse (as it often is for metaphor processing). Also, precision was considerably higher for CRF, so it might be more useful for cases where coverage is of secondary importance. For future work we plan to reimplement the underlying idea of term relevance with different means. Domain datasets could be generated via topic modeling or other clustering means (Shutova et al., 2010; Heintz et al., 2013) and should also cover dynamically detected secondary target domains. Instead of using TF-IDF, term relevance can be modeled using semantic vector spaces (see Hovy et al. (2013)). While our preliminary tests showed better performance for CRF than for SVM, such a change in feature representation would also justify a re-evaluation of our classifier choice. To avoid false positives (and thus improve precision), we could generate ad-hoc source domains, like Strzalkowski et al. (2013) or Shutova et al. (2010) do, to detect overlooked literal connections between source and target domain. 25 Acknowledgements We would like to thank the reviewers and proofreaders for their valuable input. This research effort was in part supported by the German Academic Exchange Service (DAAD</context>
</contexts>
<marker>Hovy, Srivastava, Jauhar, Sachan, Goyal, Li, Sanders, Hovy, 2013</marker>
<rawString>Dirk Hovy, Shashank Srivastava, Sujay Kumar Jauhar, Mrinmaya Sachan, Kartik Goyal, Huiying Li, Whitney Sanders, and Eduard Hovy. 2013. Identifying metaphorical word use with tree kernels. Proceedings of the ACL-13 Workshop on Metaphor, page 52.</rawString>
</citation>
<citation valid="true">
<authors>
<author>Saisuresh Krishnakumaran</author>
<author>Xiaojin Zhu</author>
</authors>
<title>Hunting elusive metaphors using lexical resources.</title>
<date>2007</date>
<booktitle>In Proceedings of the HLT/NAACL-07 Workshop on Computational Approaches to Figurative Language,</booktitle>
<pages>13--20</pages>
<publisher>Association</publisher>
<institution>for Computational Linguistics.</institution>
<contexts>
<context position="3273" citStr="Krishnakumaran and Zhu, 2007" startWordPosition="483" endWordPosition="487">ings (either mappings or preferences) and the involved conceptual domains is required. Especially in the case of conceptual mappings, this makes it very difficult for automated systems to achieve appropriate coverage of metaphors. Even when limited to a single target domain, detecting all metaphors would require knowledge of many metaphoric source domains to cover all relevant mappings (which themselves have to be known, too). As a result of this, many systems attempt to achieve high precision for specific mappings, rather than provide general coverage. Many approaches (Gedigian et al., 2006; Krishnakumaran and Zhu, 2007; Mohler et al., 2013; Tsvetkov et al., 2013, and more) make use of manually crafted knowledge bases such as WordNet or FrameNet to establish concept domains. Other recent works establish domains via topic modeling (Shutova et al., 2010; Heintz et al., 2013), ad-hoc clustering (Strzalkowski et al., 2013) or by using semantic similarity vectors (Hovy et al., 2013). We introduce term relevance as a measure for how “out of place” a word is in a given con18 Proceedings of the Second Workshop on Metaphor in NLP, pages 18–26, Baltimore, MD, USA, 26 June 2014. c�2014 Association for Computational Lin</context>
</contexts>
<marker>Krishnakumaran, Zhu, 2007</marker>
<rawString>Saisuresh Krishnakumaran and Xiaojin Zhu. 2007. Hunting elusive metaphors using lexical resources. In Proceedings of the HLT/NAACL-07 Workshop on Computational Approaches to Figurative Language, pages 13–20. Association for Computational Linguistics.</rawString>
</citation>
<citation valid="true">
<authors>
<author>George Lakoff</author>
<author>Mark Johnson</author>
</authors>
<title>Metaphors we live by, volume 111.</title>
<date>1980</date>
<publisher>University of Chicago Press.</publisher>
<contexts>
<context position="1774" citStr="Lakoff and Johnson, 1980" startWordPosition="254" endWordPosition="258"> particularly useful when data is sparse, while its effect diminishes as the amount of training data increases. 1 Introduction Metaphors are used to replace complicated or unfamiliar ideas with familiar, yet unrelated concepts that share an important attribute with the intended idea. In NLP, detecting metaphors and other nonliteral figures of speech is necessary to interpret their meaning correctly. As metaphors are a productive part of language, listing known examples is not sufficient. Most computational approaches to metaphor detection are based either on the theory of conceptual mappings (Lakoff and Johnson, 1980) or that of preference violation (Wilks, 1978). Lakoff and Johnson (1980) showed that metaphors have underlying mappings between two conceptual domains: The figurative source domain that the metaphor is taken from and the literal target domain of the surrounding context in which it has to be interpreted. Various metaphors can be based on the same conceptual metaphor mapping, e.g. both “The economy is a house of cards” and “the stakes of our debates appear small” match POLITICS IS A GAME. Another attribute of metaphors is that they violate semantic selectional preferences (Wilks, 1978). The the</context>
</contexts>
<marker>Lakoff, Johnson, 1980</marker>
<rawString>George Lakoff and Mark Johnson. 1980. Metaphors we live by, volume 111. University of Chicago Press.</rawString>
</citation>
<citation valid="true">
<authors>
<author>Linlin Li</author>
<author>Caroline Sporleder</author>
</authors>
<title>Using gaussian mixture models to detect figurative language in context.</title>
<date>2010</date>
<booktitle>In Proceedings of NAACL-10,</booktitle>
<pages>297--300</pages>
<publisher>Association</publisher>
<institution>for Computational Linguistics.</institution>
<contexts>
<context position="28943" citStr="Li and Sporleder (2010)" startWordPosition="4668" endWordPosition="4671">ied to metaphor detection. Hovy et al. (2013) generalize semantic preference violations from verb-noun relations to any syntactic relation and learn these in a supervised manner, using SVM and CRF models. The CRF is not the overall best-performing system, but achieves the highest precision of 0.74 against an all-metaphor baseline of 0.49. This is in line with our own observations. While they argue that metaphor detection should eventually be performed on every word, their evaluation is limited to a single expression per sentence. Our work is also related to that of Sporleder and Li (2009) and Li and Sporleder (2010), in which they detect idioms through their lack of semantic cohesiveness with their context. Cohesiveness is measured via co-occurence of idiom candidates with other parts of a text in web searches. They do not make use of domains, basing their measure entirely on the lexical context instead. 7 Conclusion We have presented term relevance as a nonliteralness indicator and its use for metaphor detection. We showed that even on its own, term relevance clearly outperforms the baseline by 58% when detecting metaphors on a word basis. We also evaluated the utility of term relevance as a feature in </context>
</contexts>
<marker>Li, Sporleder, 2010</marker>
<rawString>Linlin Li and Caroline Sporleder. 2010. Using gaussian mixture models to detect figurative language in context. In Proceedings of NAACL-10, pages 297– 300. Association for Computational Linguistics.</rawString>
</citation>
<citation valid="true">
<authors>
<author>Edward Loper</author>
<author>Steven Bird</author>
</authors>
<title>NLTK: The natural language toolkit.</title>
<date>2002</date>
<booktitle>In Proceedings of the COLING/ACL-02 workshop on Interactive presentation sessions,</booktitle>
<pages>63--70</pages>
<publisher>Association</publisher>
<institution>for Computational Linguistics.</institution>
<contexts>
<context position="9203" citStr="Loper and Bird, 2002" startWordPosition="1485" endWordPosition="1488">ned to equally sized pseudo-domain datasets. These pseudo-domains allow us to compute the inverse of the domain frequency for the TF-IDF without the effort of assigning all documents to proper domains. The documentfrequency score that will be used as common relevance is directly computed on the documents of the complete corpus. 3 Data We make use of two different corpora. The first is the domain-independent corpus required for computing term relevance. The second is an evaluation corpus for the governance domain on which we train and test our systems. Both corpora are preprocessed using NLTK (Loper and Bird, 2002)3. After tokenization, stopwords and punctuation are removed, contractions expanded (e.g. we’ve to we have) and numbers generalized (e.g. 1990’s to @’s). The remaining words are reduced to their stem to avoid data sparsity due to morphological variation. In case of the domain corpus, we also removed generic web document contents, such as HTML mark-up, JavaScript/CSS code blocks and similar boilerplate code4. 3.1 Domain Corpus As a basis for term relevance, we require a large corpus that is domain-independent and ideally also style-independent (i.e. not a newspaper corpus or 2http://lucene.apac</context>
</contexts>
<marker>Loper, Bird, 2002</marker>
<rawString>Edward Loper and Steven Bird. 2002. NLTK: The natural language toolkit. In Proceedings of the COLING/ACL-02 workshop on Interactive presentation sessions, pages 63–70. Association for Computational Linguistics.</rawString>
</citation>
<citation valid="true">
<authors>
<author>Michael Mohler</author>
<author>David Bracewell</author>
<author>David Hinote</author>
<author>Marc Tomlinson</author>
</authors>
<title>Semantic signatures for example-based linguistic metaphor detection.</title>
<date>2013</date>
<booktitle>Proceedings of the ACL-13 Workshop on Metaphor,</booktitle>
<pages>27</pages>
<contexts>
<context position="3294" citStr="Mohler et al., 2013" startWordPosition="488" endWordPosition="491">rences) and the involved conceptual domains is required. Especially in the case of conceptual mappings, this makes it very difficult for automated systems to achieve appropriate coverage of metaphors. Even when limited to a single target domain, detecting all metaphors would require knowledge of many metaphoric source domains to cover all relevant mappings (which themselves have to be known, too). As a result of this, many systems attempt to achieve high precision for specific mappings, rather than provide general coverage. Many approaches (Gedigian et al., 2006; Krishnakumaran and Zhu, 2007; Mohler et al., 2013; Tsvetkov et al., 2013, and more) make use of manually crafted knowledge bases such as WordNet or FrameNet to establish concept domains. Other recent works establish domains via topic modeling (Shutova et al., 2010; Heintz et al., 2013), ad-hoc clustering (Strzalkowski et al., 2013) or by using semantic similarity vectors (Hovy et al., 2013). We introduce term relevance as a measure for how “out of place” a word is in a given con18 Proceedings of the Second Workshop on Metaphor in NLP, pages 18–26, Baltimore, MD, USA, 26 June 2014. c�2014 Association for Computational Linguistics text. Our hy</context>
<context position="4569" citStr="Mohler et al., 2013" startWordPosition="695" endWordPosition="698">se they are not meant literally, but rather metaphorically. Term relevance is based on term frequency measures for target domains and mixed-domain data. The advantage of this approach is that it only requires knowledge of a text’s literal target domain, but none about any source domains or conceptual mappings. As it does not require sentence structure information, it is also resistant to noisy data, allowing the use of large, uncurated corpora. While some works that utilize domain-mappings circumvent the need for pre-existing source data by generating it themselves (Strzalkowski et al., 2013; Mohler et al., 2013), our approach is truly source-independent. We present a threshold classifier that uses term relevance as its only metric for metaphor detection. In addition we evaluate the impact of term relevance at different training sizes. Our contributions are: • We present a measure for non-literalness that only requires data for the literal domain(s) of a text. • Our approach detects metaphors independently of their source domain. • We report improvements for F1 of 58% (stand-alone) and 68% (multi-feature) over a random baseline. 2 Term Relevance We hypothesize that novel metaphoric language is marked </context>
<context position="26578" citStr="Mohler et al. (2013)" startWordPosition="4292" endWordPosition="4295">elative performance gain of models obtained from addition of term relevance features. 24 need for pre-existing source data by generating ad-hoc collocation clusters and check whether the two highest ranked source clusters share vocabulary with the target domain. Further factors in their decision process are co-ocurrences in surrounding sentences and psycholinguistic imageability scores (i.e. how easy it is to form a mental picture of a word). Evaluating on data in the governance domain, they achieve an accuracy of 71% against an all metaphor baseline of 46%, but report no precision or recall. Mohler et al. (2013) and Heintz et al. (2013) also evaluate on the governance domain. Rather than detecting metaphors at a word-level, both detect whether sentences contain metaphors. Mohler et al. (2013) compare semantic signatures of sentences to signatures of known metaphors. They, too, face a strong bias against the metaphor label and show how this can influence the balance between precision and recall. Heintz et al. (2013) classify sentences as containing metaphors if their content is related to both a target and source domain. They create clusters via topic modeling and, like us, use manually chosen seed te</context>
</contexts>
<marker>Mohler, Bracewell, Hinote, Tomlinson, 2013</marker>
<rawString>Michael Mohler, David Bracewell, David Hinote, and Marc Tomlinson. 2013. Semantic signatures for example-based linguistic metaphor detection. Proceedings of the ACL-13 Workshop on Metaphor, page 27.</rawString>
</citation>
<citation valid="true">
<authors>
<author>Naoaki Okazaki</author>
</authors>
<title>CRFsuite: a fast implementation of conditional random fields (CRFs).</title>
<date>2007</date>
<contexts>
<context position="19408" citStr="Okazaki, 2007" startWordPosition="3120" endWordPosition="3121">ance as the only factor for metaphor detection is probably insufficient. Rather, we anticipate to use it either as a prefiltering step or as a feature for a more complex metaphor detection system. To simulate the latter, we use an off-the-shelf machine learning classifier with which we test how term relevance interacts with other typical word features, such as part of speech. As we classify all words of a sentence, we treat the task as a binary sequential labeling task. Preliminary tests were performed with HMM, CRF and SVM classifiers. CRF performance was the most promising. We use CRFsuite (Okazaki, 2007)11, an implementation of conditional random fields that supports continuous values via scaling factors. Training is performed on the development set using ten-fold cross validation. We present results for bigram models. Larger ngrams were inspected, too, including models with look-ahead functionality. While they were slightly more robust with regard to parameter changes, there was no improvement over the best bigram model. Also, as metaphor processing still is a low resource task for which sufficient training data is hard to come by, bigrams are the most accessible and representative option. 5</context>
</contexts>
<marker>Okazaki, 2007</marker>
<rawString>Naoaki Okazaki, 2007. CRFsuite: a fast implementation of conditional random fields (CRFs).</rawString>
</citation>
<citation valid="true">
<authors>
<author>Sebastian Pad´o</author>
</authors>
<title>User’s guide to sigf: Significance testing by approximate randomisation.</title>
<date>2006</date>
<marker>Pad´o, 2006</marker>
<rawString>Sebastian Pad´o, 2006. User’s guide to sigf: Significance testing by approximate randomisation.</rawString>
</citation>
<citation valid="true">
<authors>
<author>Alan Ritter</author>
<author>Oren Etzioni</author>
</authors>
<title>A latent dirichlet allocation method for selectional preferences. In</title>
<date>2010</date>
<booktitle>ACT-10,</booktitle>
<pages>424--434</pages>
<publisher>Association</publisher>
<institution>for Computational Linguistics.</institution>
<marker>Ritter, Etzioni, 2010</marker>
<rawString>Alan Ritter, Oren Etzioni, et al. 2010. A latent dirichlet allocation method for selectional preferences. In ACT-10, pages 424–434. Association for Computational Linguistics.</rawString>
</citation>
<citation valid="true">
<authors>
<author>Diarmuid O´ S´eaghdha</author>
</authors>
<title>Latent variable models of selectional preference.</title>
<date>2010</date>
<booktitle>In Proceedings of ACL10,</booktitle>
<pages>435--444</pages>
<publisher>Association</publisher>
<institution>for Computational Linguistics.</institution>
<marker>S´eaghdha, 2010</marker>
<rawString>Diarmuid O´ S´eaghdha. 2010. Latent variable models of selectional preference. In Proceedings of ACL10, pages 435–444. Association for Computational Linguistics.</rawString>
</citation>
<citation valid="true">
<authors>
<author>Ekaterina Shutova</author>
<author>Lin Sun</author>
<author>Anna Korhonen</author>
</authors>
<title>Metaphor identification using verb and noun clustering.</title>
<date>2010</date>
<booktitle>In Proceedings of COLING-10,</booktitle>
<pages>1002--1010</pages>
<publisher>Association</publisher>
<institution>for Computational Linguistics.</institution>
<contexts>
<context position="3509" citStr="Shutova et al., 2010" startWordPosition="524" endWordPosition="527">ited to a single target domain, detecting all metaphors would require knowledge of many metaphoric source domains to cover all relevant mappings (which themselves have to be known, too). As a result of this, many systems attempt to achieve high precision for specific mappings, rather than provide general coverage. Many approaches (Gedigian et al., 2006; Krishnakumaran and Zhu, 2007; Mohler et al., 2013; Tsvetkov et al., 2013, and more) make use of manually crafted knowledge bases such as WordNet or FrameNet to establish concept domains. Other recent works establish domains via topic modeling (Shutova et al., 2010; Heintz et al., 2013), ad-hoc clustering (Strzalkowski et al., 2013) or by using semantic similarity vectors (Hovy et al., 2013). We introduce term relevance as a measure for how “out of place” a word is in a given con18 Proceedings of the Second Workshop on Metaphor in NLP, pages 18–26, Baltimore, MD, USA, 26 June 2014. c�2014 Association for Computational Linguistics text. Our hypothesis is that words will often be out of place because they are not meant literally, but rather metaphorically. Term relevance is based on term frequency measures for target domains and mixed-domain data. The adv</context>
<context position="27485" citStr="Shutova et al. (2010)" startWordPosition="4442" endWordPosition="4445">ias against the metaphor label and show how this can influence the balance between precision and recall. Heintz et al. (2013) classify sentences as containing metaphors if their content is related to both a target and source domain. They create clusters via topic modeling and, like us, use manually chosen seed terms to associate them with domains. Unlike our approach, theirs also requires seeds of all relevant source domains. They observe that identifying metaphors, even on a sentence level, is difficult even for experienced annotators, as evidenced by an interannotator agreement of κ = 0.48. Shutova et al. (2010) use manually annotated seed sentences to generate source and target domain vocabularies via spectral clustering. The resulting domain clusters are used for selectional preference induction in verb-noun relations. They report a high precision of 0.79, but have no data on recall. Target concepts appearing in similar lexicosyntactic contexts are mapped to the same source concepts. The resulting mappings are then used to detect metaphors. This approach is notable for its combination of distributional clustering and selectional preference induction. Verbs and nouns are clustered into topics and li</context>
<context position="30156" citStr="Shutova et al., 2010" startWordPosition="4871" endWordPosition="4874">ure in a larger system. Results for this were mixed, as the general performance of our system, a sequential CRF classifier, was lower than anticipated. However, tests on smaller training sets suggest that term relevance can help when data is sparse (as it often is for metaphor processing). Also, precision was considerably higher for CRF, so it might be more useful for cases where coverage is of secondary importance. For future work we plan to reimplement the underlying idea of term relevance with different means. Domain datasets could be generated via topic modeling or other clustering means (Shutova et al., 2010; Heintz et al., 2013) and should also cover dynamically detected secondary target domains. Instead of using TF-IDF, term relevance can be modeled using semantic vector spaces (see Hovy et al. (2013)). While our preliminary tests showed better performance for CRF than for SVM, such a change in feature representation would also justify a re-evaluation of our classifier choice. To avoid false positives (and thus improve precision), we could generate ad-hoc source domains, like Strzalkowski et al. (2013) or Shutova et al. (2010) do, to detect overlooked literal connections between source and targ</context>
</contexts>
<marker>Shutova, Sun, Korhonen, 2010</marker>
<rawString>Ekaterina Shutova, Lin Sun, and Anna Korhonen. 2010. Metaphor identification using verb and noun clustering. In Proceedings of COLING-10, pages 1002–1010. Association for Computational Linguistics.</rawString>
</citation>
<citation valid="true">
<authors>
<author>Ekaterina Shutova</author>
</authors>
<title>Models of metaphor in NLP.</title>
<date>2010</date>
<booktitle>In Proceedings of ACL-10,</booktitle>
<pages>688--697</pages>
<publisher>Association</publisher>
<institution>for Computational Linguistics.</institution>
<contexts>
<context position="24971" citStr="Shutova (2010)" startWordPosition="4020" endWordPosition="4021">erm relevance provides. As one can see, small datasets profit considerably more from our metric. Given only 200 sentences, the PosLex model receives 4.7 times the performance gain from term relevance it got at at maximum training size. The basic model has a factor of 6.8. This supports our assumption that term relevance is similar to the item weights learned during CRF training. As labeled training data is considerably more expensive to create than corpora for term relevance, this is an encouraging observation. 6 Related Work For a comprehensive review on computational metaphor detection, see Shutova (2010). We limit our discussion to publications that were not covered by the review. While there are several papers evaluating on the same domain, direct comparison proved to be difficult, as many works were either evaluated on a sentence level (which our data was inappropriate for, as 80% of sentences contained metaphors) or did not provide coverage information. Another difference was that most evaluations were performed on balanced datasets, while our own data was naturally skewed for literal terms. Strzalkowski et al. (2013) follow a related hypothesis, assuming that metaphors lack topical relate</context>
</contexts>
<marker>Shutova, 2010</marker>
<rawString>Ekaterina Shutova. 2010. Models of metaphor in NLP. In Proceedings of ACL-10, pages 688–697. Association for Computational Linguistics.</rawString>
</citation>
<citation valid="true">
<authors>
<author>Caroline Sporleder</author>
<author>Linlin Li</author>
</authors>
<title>Unsupervised recognition of literal and non-literal use of idiomatic expressions.</title>
<date>2009</date>
<booktitle>In Proceedings of EACL09,</booktitle>
<pages>754--762</pages>
<publisher>Association</publisher>
<institution>for Computational Linguistics.</institution>
<contexts>
<context position="28915" citStr="Sporleder and Li (2009)" startWordPosition="4663" endWordPosition="4666">, but have not yet been applied to metaphor detection. Hovy et al. (2013) generalize semantic preference violations from verb-noun relations to any syntactic relation and learn these in a supervised manner, using SVM and CRF models. The CRF is not the overall best-performing system, but achieves the highest precision of 0.74 against an all-metaphor baseline of 0.49. This is in line with our own observations. While they argue that metaphor detection should eventually be performed on every word, their evaluation is limited to a single expression per sentence. Our work is also related to that of Sporleder and Li (2009) and Li and Sporleder (2010), in which they detect idioms through their lack of semantic cohesiveness with their context. Cohesiveness is measured via co-occurence of idiom candidates with other parts of a text in web searches. They do not make use of domains, basing their measure entirely on the lexical context instead. 7 Conclusion We have presented term relevance as a nonliteralness indicator and its use for metaphor detection. We showed that even on its own, term relevance clearly outperforms the baseline by 58% when detecting metaphors on a word basis. We also evaluated the utility of ter</context>
</contexts>
<marker>Sporleder, Li, 2009</marker>
<rawString>Caroline Sporleder and Linlin Li. 2009. Unsupervised recognition of literal and non-literal use of idiomatic expressions. In Proceedings of EACL09, pages 754– 762. Association for Computational Linguistics.</rawString>
</citation>
<citation valid="true">
<authors>
<author>Tomek Strzalkowski</author>
<author>George Aaron Broadwell</author>
<author>Sarah Taylor</author>
<author>Laurie Feldman</author>
<author>Boris Yamrom</author>
<author>Samira Shaikh</author>
<author>Ting Liu</author>
<author>Kit Cho</author>
<author>Umit Boz</author>
<author>Ignacio Cases</author>
</authors>
<title>Robust extraction of metaphors from novel data.</title>
<date>2013</date>
<booktitle>Proceedings of the ACL-13 Workshop on Metaphor,</booktitle>
<pages>67</pages>
<contexts>
<context position="3578" citStr="Strzalkowski et al., 2013" startWordPosition="534" endWordPosition="537">equire knowledge of many metaphoric source domains to cover all relevant mappings (which themselves have to be known, too). As a result of this, many systems attempt to achieve high precision for specific mappings, rather than provide general coverage. Many approaches (Gedigian et al., 2006; Krishnakumaran and Zhu, 2007; Mohler et al., 2013; Tsvetkov et al., 2013, and more) make use of manually crafted knowledge bases such as WordNet or FrameNet to establish concept domains. Other recent works establish domains via topic modeling (Shutova et al., 2010; Heintz et al., 2013), ad-hoc clustering (Strzalkowski et al., 2013) or by using semantic similarity vectors (Hovy et al., 2013). We introduce term relevance as a measure for how “out of place” a word is in a given con18 Proceedings of the Second Workshop on Metaphor in NLP, pages 18–26, Baltimore, MD, USA, 26 June 2014. c�2014 Association for Computational Linguistics text. Our hypothesis is that words will often be out of place because they are not meant literally, but rather metaphorically. Term relevance is based on term frequency measures for target domains and mixed-domain data. The advantage of this approach is that it only requires knowledge of a text’</context>
<context position="25498" citStr="Strzalkowski et al. (2013)" startWordPosition="4102" endWordPosition="4105">. 6 Related Work For a comprehensive review on computational metaphor detection, see Shutova (2010). We limit our discussion to publications that were not covered by the review. While there are several papers evaluating on the same domain, direct comparison proved to be difficult, as many works were either evaluated on a sentence level (which our data was inappropriate for, as 80% of sentences contained metaphors) or did not provide coverage information. Another difference was that most evaluations were performed on balanced datasets, while our own data was naturally skewed for literal terms. Strzalkowski et al. (2013) follow a related hypothesis, assuming that metaphors lack topical relatedness to in-domain words while being syntactically connected to them. Instead of using the metaphor candidate’s relevance to a target domain corpus to judge relatedness, they circumvent the Relative Gain 150 % 125 % 100 % 75 % 50 % 25 % 0 % CRF Basic CRF PosLex Models 75 % 50 % 25 % 0 % 150 % 125 % 100 % 200 400 600 800 1000 1200 1400 1600 1800 Number of Training Sentences Figure 2: Relative performance gain of models obtained from addition of term relevance features. 24 need for pre-existing source data by generating ad-</context>
<context position="30662" citStr="Strzalkowski et al. (2013)" startWordPosition="4950" endWordPosition="4953">h different means. Domain datasets could be generated via topic modeling or other clustering means (Shutova et al., 2010; Heintz et al., 2013) and should also cover dynamically detected secondary target domains. Instead of using TF-IDF, term relevance can be modeled using semantic vector spaces (see Hovy et al. (2013)). While our preliminary tests showed better performance for CRF than for SVM, such a change in feature representation would also justify a re-evaluation of our classifier choice. To avoid false positives (and thus improve precision), we could generate ad-hoc source domains, like Strzalkowski et al. (2013) or Shutova et al. (2010) do, to detect overlooked literal connections between source and target domain. 25 Acknowledgements We would like to thank the reviewers and proofreaders for their valuable input. This research effort was in part supported by the German Academic Exchange Service (DAAD) scholarship program PROMOS with funds from the Federal Ministry of Education and Research (BMBF), awarded by the International Office of Saarland University as well as by the Intelligence Advanced Research Projects Activity (IARPA) via Department of Defense US Army Research Laboratory contract number W91</context>
</contexts>
<marker>Strzalkowski, Broadwell, Taylor, Feldman, Yamrom, Shaikh, Liu, Cho, Boz, Cases, 2013</marker>
<rawString>Tomek Strzalkowski, George Aaron Broadwell, Sarah Taylor, Laurie Feldman, Boris Yamrom, Samira Shaikh, Ting Liu, Kit Cho, Umit Boz, Ignacio Cases, et al. 2013. Robust extraction of metaphors from novel data. Proceedings of the ACL-13 Workshop on Metaphor, page 67.</rawString>
</citation>
<citation valid="true">
<authors>
<author>Yulia Tsvetkov</author>
<author>Elena Mukomel</author>
<author>Anatole Gershman</author>
</authors>
<title>Cross-lingual metaphor detection using common semantic features.</title>
<date>2013</date>
<booktitle>Proceedings of the ACL-13 Workshop on Metaphor,</booktitle>
<pages>45</pages>
<contexts>
<context position="3317" citStr="Tsvetkov et al., 2013" startWordPosition="492" endWordPosition="495">ved conceptual domains is required. Especially in the case of conceptual mappings, this makes it very difficult for automated systems to achieve appropriate coverage of metaphors. Even when limited to a single target domain, detecting all metaphors would require knowledge of many metaphoric source domains to cover all relevant mappings (which themselves have to be known, too). As a result of this, many systems attempt to achieve high precision for specific mappings, rather than provide general coverage. Many approaches (Gedigian et al., 2006; Krishnakumaran and Zhu, 2007; Mohler et al., 2013; Tsvetkov et al., 2013, and more) make use of manually crafted knowledge bases such as WordNet or FrameNet to establish concept domains. Other recent works establish domains via topic modeling (Shutova et al., 2010; Heintz et al., 2013), ad-hoc clustering (Strzalkowski et al., 2013) or by using semantic similarity vectors (Hovy et al., 2013). We introduce term relevance as a measure for how “out of place” a word is in a given con18 Proceedings of the Second Workshop on Metaphor in NLP, pages 18–26, Baltimore, MD, USA, 26 June 2014. c�2014 Association for Computational Linguistics text. Our hypothesis is that words </context>
</contexts>
<marker>Tsvetkov, Mukomel, Gershman, 2013</marker>
<rawString>Yulia Tsvetkov, Elena Mukomel, and Anatole Gershman. 2013. Cross-lingual metaphor detection using common semantic features. Proceedings of the ACL-13 Workshop on Metaphor, page 45.</rawString>
</citation>
<citation valid="true">
<authors>
<author>Yorick Wilks</author>
</authors>
<title>Making preferences more active.</title>
<date>1978</date>
<journal>Artificial Intelligence,</journal>
<volume>11</volume>
<issue>3</issue>
<contexts>
<context position="1820" citStr="Wilks, 1978" startWordPosition="264" endWordPosition="265">diminishes as the amount of training data increases. 1 Introduction Metaphors are used to replace complicated or unfamiliar ideas with familiar, yet unrelated concepts that share an important attribute with the intended idea. In NLP, detecting metaphors and other nonliteral figures of speech is necessary to interpret their meaning correctly. As metaphors are a productive part of language, listing known examples is not sufficient. Most computational approaches to metaphor detection are based either on the theory of conceptual mappings (Lakoff and Johnson, 1980) or that of preference violation (Wilks, 1978). Lakoff and Johnson (1980) showed that metaphors have underlying mappings between two conceptual domains: The figurative source domain that the metaphor is taken from and the literal target domain of the surrounding context in which it has to be interpreted. Various metaphors can be based on the same conceptual metaphor mapping, e.g. both “The economy is a house of cards” and “the stakes of our debates appear small” match POLITICS IS A GAME. Another attribute of metaphors is that they violate semantic selectional preferences (Wilks, 1978). The theory of selectional preference observes that ve</context>
</contexts>
<marker>Wilks, 1978</marker>
<rawString>Yorick Wilks. 1978. Making preferences more active. Artificial Intelligence, 11(3):197–223.</rawString>
</citation>
</citationList>
</algorithm>
</algorithms>
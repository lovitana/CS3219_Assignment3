<?xml version="1.0" encoding="UTF-8"?>
<algorithms version="110505">
<algorithm name="SectLabel" version="110505">
<variant no="0" confidence="0.000001">
<title confidence="0.987151">
Chinese Spelling Check System Based on Tri-gram Model
</title>
<author confidence="0.993525">
Qiang Huang, Peijie Huang*, Xinrui Zhang, Weijian Xie, Kaiduo Hong, Bingzhou
Chen, Lei Huang
</author>
<affiliation confidence="0.997639">
College of Informatics, South China Agricultural University,
</affiliation>
<address confidence="0.937983">
Guangzhou 510642, Guangdong, China
</address>
<email confidence="0.66313">
kasim0079@qq.com, pjhuang@scau.edu.cn,
nealrichardrui@gmail.com, tsewkviko@gmail.com,
HKDNZ@hotmail.com, cbtpkzm@163.com, hl mark@163.com
</email>
<page confidence="0.831411">
_
</page>
<sectionHeader confidence="0.986255" genericHeader="abstract">
Abstract
</sectionHeader>
<bodyText confidence="0.999948764705882">
This paper describes our system in the
Chinese spelling check (CSC) task of
CLP-SIGHAN Bake-Off 2014. CSC is
still an open problem today. To the best of
our knowledge, n-gram language
modeling (LM) is widely used in CSC
because of its simplicity and fair
predictive power. Our work in this paper
continues this general line of research by
using a tri-gram LM to detect and correct
possible spelling errors. In addition, we
use dynamic programming to improve the
efficiency of the algorithm, and additive
smoothing to solve the data sparseness
problem in training set. Empirical
evaluation results demonstrate the utility
of our CSC system.
</bodyText>
<sectionHeader confidence="0.998128" genericHeader="introduction">
1 Introduction
</sectionHeader>
<bodyText confidence="0.9993588125">
Spelling check is a common task in every written
language, which is an automatic mechanism to
detect and correct human errors (Wu et al., 2013).
The problem of devising algorithms and
techniques for automatically correcting words in
text began as early as the 1960s on computer
techniques for automatic spelling correction and
automatic text recognition (Kukich, 1992), and it
has continued up to the present. A spelling
checker should have both capabilities consisting
of error detection and error correction. Spelling
error detection is to indicate the various types of
spelling errors in the text. Spelling error
correction is further to suggest the correct
characters of detected errors.
Chinese as a foreign language (CFL) have
</bodyText>
<note confidence="0.732465">
* Corresponding author
</note>
<bodyText confidence="0.999855974358975">
attracted more and more attention, and this trend
is continuing. For this purpose, at the SIGHAN
Bake-offs, Chinese spelling check (CSC) task are
organized to provide an evaluation platform for
developing and implementing automatic Chinese
spelling checkers. However, spelling check in
Chinese is very different from that in English or
other alphabetic languages. There are no word
delimiters between words and the length of each
word is very short. A Chinese “word” usually
comprises two or more characters. The difficulty
of Chinese processing is that many Chinese
characters have similar shapes or similar (or
same) pronunciations. Some characters are even
similar in both shape and pronunciation (Wu et
al., 2010; Liu et al., 2011).
There are many research effort developed for
CSC recently, including rule-based model (Jiang
et al., 2012; Chiu et al., 2013), n-gram model
(Wu et al., 2010; Wang et al., 2013b; Chen et al.,
2013), graph theory (Bao et al., 2011; Jia et al.,
2013), statistical learning method (Han and
Chang, 2013), etc. Some of them are hybrid
model.
Language modeling (LM) is widely used in
CSC, and the most widely-used and well-
practiced language model, by far, is the n-gram
LM (Jelinek, 1999), because of its simplicity and
fair predictive power. Our work in this paper
continues this general line of research by using a
tri-gram LM to detect and correct possible
spelling errors. In addition, in order to solve the
high complexity in the computation process of
the tri-gram based CSC, dynamic programming
is used to improve the efficiency of the algorithm.
Moreover, additive smoothing to solve the data
sparseness problem in training set.
The rest of this paper is organized as follows.
In Section 2, we briefly present the proposed
</bodyText>
<page confidence="0.720123">
173
</page>
<note confidence="0.907223">
Proceedings of the Third CIPS-SIGHAN Joint Conference on Chinese Language Processing, pages 173–178,
Wuhan, China, 20-21 October 2014
</note>
<bodyText confidence="0.9923512">
CSC system, confusion sets and the choice of n-
gram order. Section 3 details our Chinese tri-
gram model. Evaluation results are presented in
Section 4. Finally, the last section summarizes
this paper and describes our future work.
</bodyText>
<sectionHeader confidence="0.989109" genericHeader="method">
2 The Proposed System
</sectionHeader>
<subsectionHeader confidence="0.996078">
2.1 System Overview
</subsectionHeader>
<bodyText confidence="0.746219">
Figure 1 shows the flowchart of our CSC system.
</bodyText>
<figure confidence="0.6765135">
Output
sentence
</figure>
<figureCaption confidence="0.998817">
Figure 1. The flowchart of the CSC system.
</figureCaption>
<bodyText confidence="0.997906333333333">
The system is mainly composed by three
components: confusion sets, corpus and language
model. It performs CSC in the following steps:
</bodyText>
<listItem confidence="0.667531666666667">
1. Given a test sentence, the CSC system gets
the confusion sets of each character in the
sentence.
2. For each character in this sentence, the
system will enumerate every character of its
confusion set to replace the original character.
We will get a candidate sentence set after this
step.
3. The system will calculate the score of every
</listItem>
<bodyText confidence="0.975464375">
candidate sentence by using the n-gram model.
We use the corpus of CCL 1 and sogou 2 to
generate the frequency of n-gram. Finally, the
sentence with highest score will be chosen as the
final output.
Due to the high complexity of step 2 and step
3, we optimize the algorithm by using dynamic
programming.
</bodyText>
<subsectionHeader confidence="0.999398">
2.2 Confusion Set
</subsectionHeader>
<bodyText confidence="0.9924405">
Confusion set is a ready set of commonly
confused characters plays an important role in
</bodyText>
<footnote confidence="0.4789635">
1ccl.pku.edu.cn:8080/ccl_corpus/index.jsp?dir=xiandai
2 www.sogou.com/labs/dl/c.html
</footnote>
<bodyText confidence="0.999955">
spelling error detection and correction in texts
(Wang et al., 2013a). Most Chinese characters
have other characters similar to them in either
shape or pronunciation. Since pinyin input
method is currently the most popular Chinese
input method, the confusion sets used in our
system is constructed from a homophone
dictionary of qingsongcha website 3 . Some
Chinese characters with similar pronunciation,
such as the nasal and the lateral consonants,
retroflex and non-retroflex, etc., are also added to
the confusion sets in our system.
</bodyText>
<subsectionHeader confidence="0.99924">
2.3 Language Modeling
</subsectionHeader>
<bodyText confidence="0.999984375">
Language modeling can be used to quantify the
quality of a given word string, and most previous
researches have adopted it as a method to predict
which word might be a correct word to replace
the possible erroneous word (Chen et al., 2009;
Liu et al., 2011; Wu et al., 2010). The most
widely-used and well-practiced language model,
by far, is the n-gram language model (Jelinek,
1999), because of its simplicity and fair
predictive power.
In n-gram modeling, choosing a proper
order of the n-gram is important. On the one
hand, higher order n-gram models along with
larger corpora tend to increase their quality, and
thus will yield lower perplexity for human-
generated text. On the other hand, the higher
order n-gram models, such as four-gram or five-
gram, usually suffer from the data sparseness
problem, which leads to some zero conditional
probabilities (Chen et al., 2013). For these
reasons, we have developed a Chinese character
tri-gram model to determine the best character
sequence as the answers for detection and
correction.
</bodyText>
<sectionHeader confidence="0.982708" genericHeader="method">
3 Chinese Tri-gram Model
</sectionHeader>
<subsectionHeader confidence="0.837706">
3.1 Tri-gram Model
</subsectionHeader>
<bodyText confidence="0.9993614">
Given a Chinese character string C = C1, C2, -,CL ,
the probability of the character string in tri-gram
model is approximated by the product of a series
of conditional probabilities as follows (Jelinek,
1999),
</bodyText>
<equation confidence="0.9711696">
l-
L L
1) fj P(cl cl-2 , cl-1) . (1)
l=3 l=
3
</equation>
<bodyText confidence="0.999892333333333">
In the above tri-gram model, we make the
approximation that the probability of a character
depends only on the two immediately preceding
</bodyText>
<figure confidence="0.980898714285714">
3 www.qingsongcha.com/
Input
sentence
Language
model
Enumerate all
candidate sentences
Find the sentence
with highest score
Confusion
sets
Corpus
P(C) = fj P(
cl C
174
一心一&amp;quot;
N(&amp;quot;
)
一心一&amp;quot;
)
一心&amp;quot;
N(&amp;quot;
)
心一億&amp;quot;
N(&amp;quot;
)
)
0
心一&amp;quot;
)
N(&amp;quot;
getscore(&amp;quot;
getscore(&amp;quot; 心一億&amp;quot;
  0.00248
words.
</figure>
<bodyText confidence="0.995083666666667">
The easiest way to estimate the conditional
probability in Eq. (1) is to use the maximum
likelihood (ML) estimation as follows,
</bodyText>
<equation confidence="0.915332375">
(2)
, -- - - - - - - - - - - - - - - - - - - - - - - - - - - - - - -
- - - - - -
)
一心一&amp;quot;
N(&amp;quot;
一心一 &amp;quot;
)
</equation>
<bodyText confidence="0.9537365">
where I (CI-2, CI-1, cl ) and denote
the number of times the character strings
“ CI-2, CI-1, ” and “ ” occur in a given
training corpus, respectively.
</bodyText>
<equation confidence="0.779661">
getscore(&amp;quot;
getscore(&amp;quot; 心一意&amp;quot;
N(&amp;quot; )
N(&amp;quot; 心一意 &amp;quot; ) 
  0.00248
0.01574
)
心一&amp;quot;
N(&amp;quot;
</equation>
<figureCaption confidence="0.988965">
Figure 3. Getscore function calculating example.
</figureCaption>
<subsectionHeader confidence="0.711887">
3.2 Getscore Function Definition
</subsectionHeader>
<bodyText confidence="0.958780222222222">
We define the candidate sentence as
C&apos; = ci , c2,..., c&apos;L , which is the character string
derived from the original sentence by
replacing some characters using their confusion
sets. The getscore function is used to select the
most suitable candidate sentence. Figure 2 shows
the pseudo-code of the getscore function by
using tri-gram model.
function getscore( ci-2 , ci-1 , ci )
</bodyText>
<figure confidence="0.979910533333333">
begin
(c
i-2, ci-1 , ci
)
I (c 
, c i -1)
i-2
if ci  ci then
begin
ret  ret  
end
end
I
ret

</figure>
<figureCaption confidence="0.999966">
Figure 2. Pseudo-code of getscore function.
</figureCaption>
<bodyText confidence="0.943710777777778">
Now we add a rule if , it will get an
extra score . In the future work, we will add
other rules or algorithms to improve the getscore
function.
For example, in “ 一 心 一 { 億 , 意 }”, in
comparing with other string candidates as shown
in Figure 3, we found the string of the highest
score “一心一意”. So we detect the error spot
and select ‘意’ as the corrected character.
</bodyText>
<subsectionHeader confidence="0.553395">
3.3 Dynamic Programming
</subsectionHeader>
<bodyText confidence="0.995112">
Due to the high complexity of enumerating
candidate sentences, we use the dynamic
programming (DP) to optimize the tri-gram
model.
The confusion set of is defined as ,
and each element in the confusion set is label by
</bodyText>
<equation confidence="0.967808333333333">
)

一心 &amp;quot;
</equation>
<bodyText confidence="0.893039333333333">
, so the element in will be
represented as . The score of the candidate
sentence with the maximum score is defined as
</bodyText>
<equation confidence="0.81992">
dp[i] [ j] [k] , where is the length, is
</equation>
<bodyText confidence="0.99985875">
the character, and is theith
character. Because tri-gram model depends only
on last three characters, we can deduce the state
transition equation of the DP algorithm as follow:
</bodyText>
<equation confidence="0.992736">
strtmp=V[i-1]U],V[i][k],V[i+1][Z] , (3)
(4)
</equation>
<bodyText confidence="0.990030666666667">
Pseudo-code of dynamic programming is
shown in Figure 4. The complexity of the
algorithm is reduced to acceptable level as
O(MN 3) , where M is the length of the input
sentence, and I is the size of a confusion set.
x a i =1,2, ... ,d, (5)
</bodyText>
<subsectionHeader confidence="0.991665">
3.4 Additive Smoothing
</subsectionHeader>
<bodyText confidence="0.999615571428571">
In statistics, additive smoothing, which also
called Laplace smoothing, or Lidstone smoothing,
is a technique used to smooth categorical data.
Given an observation from a
multinomial distribution with I trials and
parameter vector , a &amp;quot;smoothed&amp;quot;
version of the data gives the estimator:
</bodyText>
<equation confidence="0.995320666666667">
0 =
+ad
N
</equation>
<bodyText confidence="0.999933818181818">
where α &gt; 0 is the smoothing parameter (α = 0
corresponds to no smoothing). Additive
smoothing is a type of shrinkage estimator, as the
resulting estimate will be between the empirical
estimate , and the uniform probability .
Using Laplace&apos;s rule of succession, some authors
have argued that α should be 1 (in which case the
term add-one smoothing is also used), although
in practice a smaller value is typically chosen.
In a tri-gram model, the data consists of the
number of occurrences of each string in corpus.
</bodyText>
<figure confidence="0.88166">
;
+
175
</figure>
<figureCaption confidence="0.995328">
Figure 4. Pseudo-code of dynamic programming.
</figureCaption>
<figure confidence="0.939617444444445">
Procedure DP()
begin
for F
for j F 0 to V[i -1].size do
F
F V[i +1].size do
begin
strtmp F V[i— 1] U] , V[i] [k] , V[
end
</figure>
<equation confidence="0.738696142857143">
end
[i + 1] [k] U
] max( [
F dp i +
i +1] [ Z]
))
dp
</equation>
<bodyText confidence="0.988599222222222">
i 3 to str.length do
for k 0 to V[i].size do
for l 0 to
Additive smoothing allows the assignment of
non-zero probabilities to Chinese characters
which do not occur in the training set. So we use
additive smoothing to process the data sparse
problem.
We redefine the new getscore function as
</bodyText>
<figureCaption confidence="0.891094333333333">
Figure 5.
Figure 5. Pseudo-code of getscore function with
additive smoothing.
</figureCaption>
<sectionHeader confidence="0.986439" genericHeader="method">
4 Empirical Evaluation
</sectionHeader>
<subsectionHeader confidence="0.952761">
4.1 Task
</subsectionHeader>
<bodyText confidence="0.999863647058823">
The goal of this shared task, i.e. the Chinese
spelling check (CSC) task, in CLP-SIGHAN
Bake-Off 2014 is developing the computer
assisted tools to detect (combining error
checking and correction) several kinds of
grammatical errors, i.e., redundant word, missing
word, word disorder, and word selection. The
system should return the locations of the
improper characters and must point out the
correct characters. Passages of CFL (Chinese as
a Foreign Language) learners’ essays selected
from the National Taiwan Normal University
(NTNU) learner corpus are used for training
other having 4823
spelling errors) are provided as practice. The
final test data set for the evaluation consists of
1062 passages cover different complexities.
</bodyText>
<subsectionHeader confidence="0.990085">
4.2 Metrics
</subsectionHeader>
<bodyText confidence="0.998341047619048">
1] [k] [Z] , dp[i] [j] [k] *getscore(strtmp
purpose. Two training datas (one consisting of
461 spelling errors and an
The criteria for judging correctness are: (1)
Detection level: binary classification of a given
sentence, i.e., correct or incorrect should be
completely identical with the gold standard. All
error types will be regarded as incorrect. (2)
Identification level: this level could be
considered as a multi-class categorization
problem. In addition to correct instances, all
error types should be clearly identified.
In CSC task of CLP-SIGHAN Bake-Off 2014,
ninth metrics are measured in both levels to score
the performance of a CSC system. They are
False Positive Rate (FPR), Detection Accuracy
(DA), Detection Precision (DP), Detection
Recall (DR), Detection F-score (DF), Correction
Accuracy (CA), Correction Precision (CP),
Correction Recall (CR) and Correction F-score
(CF).
</bodyText>
<subsectionHeader confidence="0.999618">
4.3 Evaluation Results
</subsectionHeader>
<bodyText confidence="0.999152272727273">
The CSC task of CLP-SIGHAN Bake-Off 2014
attracted 19 research teams. Among 19 registered
research teams, 13 participants submitted their
testing results. For formal testing, each
participant can submit at most three runs that use
different models or parameter settings. Finally,
there are 34 runs submitted in total.
Table 1 shows the evaluation results of the
final test. Run1, run2 and run3 are the three runs
of our system with different in getscore
function mentioned in Subsection 3.2. We have
</bodyText>
<table confidence="0.51627156">
function getscore( ci-2 , ci-1 , ci )
begin
ret i-2, ) +a
+— i-1,
c
c
ci
I (c , ci-1) + ad
i-2
if ca = ca then
begin
ret
+—
ret
X A
end
I (
end
176
FPR DA DP DR DF CA CP CR CF
Run1 0.2034 0.4821 0.4518 0.1676 0.2445 0.4774 0.4375 0.1582 0.2324
Run2 0.6441 0.275 0.2315 0.194 0.2111 0.2627 0.2083 0.1695 0.1869
Run3 0.5009 0.3522 0.2907 0.2053 0.2406 0.3427 0.2712 0.1864 0.221
Average 0.2841 0.4633 0.4958 0.2106 0.2836 0.4485 0.4616 0.1811 0.2498
Best 0.032 0.7194 0.9146 0.484 0.633 0.7081 0.9108 0.4614 0.6125
</table>
<tableCaption confidence="0.999958">
Table 1. Evaluation results of final test.
</tableCaption>
<bodyText confidence="0.9999254375">
chosen three runs with different estimated recall
levels as submissions. The “Best” indicates the
high score of each metric achieved in CSC task.
The “Average” represents the average of the 34
runs.
As we can see from Table 1, we achieve a
result close to the average level. The major
weakness of our system is its low recall rate,
which might be the result of not applying a
separate error detection module.
It is our first attempt on Chinese spelling
check. The potential of the n-gram method is far
from fully exploited. Some typical errors of our
current system will be presented in the next
subsection, and the corresponding improvements
are summarized in the last section.
</bodyText>
<subsectionHeader confidence="0.931646">
4.4 Error Analysis
</subsectionHeader>
<bodyText confidence="0.989073">
Figure 6 shows some typical error examples of
our system (“O” original, “M” modified):
</bodyText>
<equation confidence="0.978465777777778">
Case1:
O: 我 戴著 藍色 的 帽子
M: 我 帶著 藍色 的 帽子
Case 2:
O: 我們 在 健缸 中心 門口 等
M: 我們 在 健缸 中心 門口 等
Case 3:
O: 我們 十一點半 在 南門 碰頭
M:我們 是 一點半 在 南門 碰頭
</equation>
<figureCaption confidence="0.986984">
Figure 6. Error examples.
</figureCaption>
<bodyText confidence="0.999982066666667">
The first case is an overkill error that belongs
to long distance error correction problem. Our
system didn’t recognize the dependencies of
“戴” and “帽子”, and “我帶著” get a highest
score in tri-gram model. So our system select
“帶” to replace “戴”, and leads to error at the
same time.
In the second case, because “康” is not in the
confusion set of “缸”, we can&apos;t correct the error
of “健缸” to “健康” .
The third case is also an overkill error which is
due to the out of vocabulary (OOV) problem. In
this case, the original sentence is in fact correct
but unfortunately, the our system didn’t
recognize “十一點半” and gave it high penalty.
</bodyText>
<sectionHeader confidence="0.997828" genericHeader="conclusions">
5 Conclusions and Future Work
</sectionHeader>
<bodyText confidence="0.99999737037037">
This paper presents the development and
preliminary evaluation of the system from team
of South China Agricultural University (SCAU)
that participated in the Bake-Off 2014 task. We
have developed a Chinese character tri-gram
language model to determine the best character
sequence as the answers for detection and
correction. It is our first attempt on Chinese
spelling check, and tentative experiment shows
we achieve a not bad result. However, we still
have a long way from the state-of-arts results.
There are many possible and promising
research directions for the near future. A separate
module for possible spelling error detection will
be added to the system to improve the detection
accuracy. In addition, although language
modeling has been widely used in CSC, the n-
gram language models only aim at capturing the
local contextual information or the lexical
regularity of a language. Future work will
explore long-span semantic information for
language modeling to further improve the CSC.
Moreover, characters of similar shapes are not as
frequent, but still exist with a significant
proportion (Liu et al., 2011). Orthographically
similar characters will be added to the confusion
sets of our CSC system.
</bodyText>
<sectionHeader confidence="0.99456" genericHeader="acknowledgments">
Acknowledgments
</sectionHeader>
<bodyText confidence="0.98897275">
This work was partially supported by the
Innovation Training Project for College Students
of Guangdong Province under Grant
No.1056413096 and No.201410564290.
</bodyText>
<sectionHeader confidence="0.990361" genericHeader="references">
References
</sectionHeader>
<reference confidence="0.964923">
Zhuowei Bao, Benny Kimelfeld, Yunyao Li. 2011. A
Graph Approach to Spelling Correction in Domain-
Centric Search. In Proceedings of the 49th
</reference>
<page confidence="0.67226">
177
</page>
<reference confidence="0.999698845070422">
Annual Meeting of the Association for
Computational Linguistics (ACL 2011), pp.
905–914.
Berlin Chen. 2009. Word Topic Models for Spoken
Document Retrieval and Transcription. ACM
Transactions on Asian Language Information
Processing, Vol. 8, No. 1, pp. 2:1-2:27.
Hsun-wen Chiu, Jian-cheng Wu and Jason S. Chang.
2013. Chinese Spelling Checker Based on
Statistical Machine Translation. In Proceedings
of the 7th SIGHAN Workshop on Chinese
Language Processing (SIGHAN’13), Nagoya,
Japan, 14 October, 2013, pp. 49-53.
Kuan-Yu Chen, Hung-Shin Lee, Chung-Han Lee , et
al.. 2013. A Study of Language Modeling for
Chinese Spelling Check. In Proceedings of the
7th SIGHAN Workshop on Chinese Language
Processing (SIGHAN’13), Nagoya, Japan, 14
October, 2013, pp. 79-83.
Dongxu Han, Baobao Chang. 2013. A Maximum
Entropy Approach to Chinese Spelling Check. In
Proceedings of the 7th SIGHAN Workshop on
Chinese Language Processing (SIGHAN’13),
Nagoya, Japan, 14 October, 2013, pp. 74-78.
Frederick Jelinek. 1999. Statistical Methods for
Speech Recognition. The MIT Press.
Zhongye Jia, Peilu Wang and Hai Zhao. 2013. Graph
Model for Chinese Spell Checking. In
Proceedings of the 7th SIGHAN Workshop on
Chinese Language Processing (SIGHAN’13),
Nagoya, Japan, 14 October, 2013, pp. 88-92.
Ying Jiang, Tong Wang, Tao Lin, et al. 2012. A rule
based Chinese spelling and grammar detection
system utility. In Proceedings of the 2012
International Conference on System Science
and Engineering (ICSSE), pp. 437-440.
Karen Kukich. 1992. Techniques for Automatically
Correcting Words in Text. ACM Computing
Surveys, Vol. 24, No.4, pp. 377-439.
Chao-Lin Liu, Min-Hua Lai, Kan-Wen Tien, et al..
2011. Visually and Phonologically Similar
Characters in Incorrect Chinese Words: Analyses,
Identification, and Applications. ACM
Transactions on Asian Language Information
Processing, Vol. 10, No. 2, pp. 1-39.
Yih-Ru Wang, Jason S. Chang, Jian-Cheng Wu, et al..
2013a. Automatic Chinese Confusion Words
Extraction Using Conditional Random Fields and
the Web. In Proceedings of the 7th SIGHAN
Workshop on Chinese Language Processing
(SIGHAN’13), Nagoya, Japan, 14 October, 2013,
pp. 64-68.
Yih-Ru Wang, Yuan-Fu Liao, Yeh-Kuang Wu, et al..
2013b. Conditional Random Field-based Parser
and Language Model for Traditional Chinese
Spelling Checker. In Proceedings of the 7th
SIGHAN Workshop on Chinese Language
Processing (SIGHAN’13), Nagoya, Japan, 14
October, 2013, pp. 69-73.
Shih-Hung Wu, Yong-Zhi Chen, Ping-Che Yang, et
al.. 2010. Reducing the False Alarm Rate of
Chinese Character Error Detection and Correction.
In Proceeding of CIPSSIGHAN Joint
Conference on Chinese Language Processing
(CLP 2010), Beijing, 28-29 Aug., 2010, pp. 54-61.
Shih-Hung Wu, Chao-Lin Liu, and Lung-Hao Lee.
2013. Chinese Spelling Check Evaluation at
SIGHAN Bake-off 2013. In Proceedings of the
7th SIGHAN Workshop on Chinese Language
Processing (SIGHAN’13), Nagoya, Japan, 14
October, 2013, pp. 35-42.
</reference>
<page confidence="0.916998">
178
</page>
</variant>
</algorithm>
<algorithm name="ParsHed" version="110505">
<variant no="0" confidence="0.386424">
<title confidence="0.8112245">Chinese Spelling Check System Based on Tri-gram Model Huang, Peijie Xinrui Zhang, Weijian Xie, Kaiduo Hong,</title>
<author confidence="0.998353">Lei Huang Chen</author>
<affiliation confidence="0.99999">College of Informatics, South China Agricultural University,</affiliation>
<address confidence="0.979293">Guangzhou 510642, Guangdong,</address>
<email confidence="0.887121">kasim0079@qq.com,nealrichardrui@gmail.com,HKDNZ@hotmail.com,cbtpkzm@163.com,hlmark@163.com_</email>
<abstract confidence="0.997698">This paper describes our system in the Chinese spelling check (CSC) task of CLP-SIGHAN Bake-Off 2014. CSC is still an open problem today. To the best of our knowledge, n-gram language modeling (LM) is widely used in CSC because of its simplicity and fair predictive power. Our work in this paper continues this general line of research by using a tri-gram LM to detect and correct possible spelling errors. In addition, we use dynamic programming to improve the efficiency of the algorithm, and additive smoothing to solve the data sparseness problem in training set. Empirical evaluation results demonstrate the utility of our CSC system.</abstract>
</variant>
</algorithm>
<algorithm name="ParsCit" version="110505">
<citationList>
<citation valid="true">
<authors>
<author>Zhuowei Bao</author>
<author>Benny Kimelfeld</author>
<author>Yunyao Li</author>
</authors>
<title>A Graph Approach to Spelling Correction in DomainCentric Search.</title>
<date>2011</date>
<booktitle>In Proceedings of the 49th</booktitle>
<contexts>
<context position="2768" citStr="Bao et al., 2011" startWordPosition="416" endWordPosition="419">tic languages. There are no word delimiters between words and the length of each word is very short. A Chinese “word” usually comprises two or more characters. The difficulty of Chinese processing is that many Chinese characters have similar shapes or similar (or same) pronunciations. Some characters are even similar in both shape and pronunciation (Wu et al., 2010; Liu et al., 2011). There are many research effort developed for CSC recently, including rule-based model (Jiang et al., 2012; Chiu et al., 2013), n-gram model (Wu et al., 2010; Wang et al., 2013b; Chen et al., 2013), graph theory (Bao et al., 2011; Jia et al., 2013), statistical learning method (Han and Chang, 2013), etc. Some of them are hybrid model. Language modeling (LM) is widely used in CSC, and the most widely-used and wellpracticed language model, by far, is the n-gram LM (Jelinek, 1999), because of its simplicity and fair predictive power. Our work in this paper continues this general line of research by using a tri-gram LM to detect and correct possible spelling errors. In addition, in order to solve the high complexity in the computation process of the tri-gram based CSC, dynamic programming is used to improve the efficiency</context>
</contexts>
<marker>Bao, Kimelfeld, Li, 2011</marker>
<rawString>Zhuowei Bao, Benny Kimelfeld, Yunyao Li. 2011. A Graph Approach to Spelling Correction in DomainCentric Search. In Proceedings of the 49th</rawString>
</citation>
<citation valid="true">
<date>2011</date>
<booktitle>Annual Meeting of the Association for Computational Linguistics (ACL</booktitle>
<pages>905--914</pages>
<marker>2011</marker>
<rawString>Annual Meeting of the Association for Computational Linguistics (ACL 2011), pp. 905–914.</rawString>
</citation>
<citation valid="true">
<authors>
<author>Berlin Chen</author>
</authors>
<title>Word Topic Models for Spoken Document Retrieval and Transcription.</title>
<date>2009</date>
<journal>ACM Transactions on Asian Language Information Processing,</journal>
<volume>8</volume>
<pages>2--1</pages>
<marker>Chen, 2009</marker>
<rawString>Berlin Chen. 2009. Word Topic Models for Spoken Document Retrieval and Transcription. ACM Transactions on Asian Language Information Processing, Vol. 8, No. 1, pp. 2:1-2:27.</rawString>
</citation>
<citation valid="true">
<authors>
<author>Hsun-wen Chiu</author>
<author>Jian-cheng Wu</author>
<author>Jason S Chang</author>
</authors>
<title>Chinese Spelling Checker Based on Statistical Machine Translation.</title>
<date>2013</date>
<booktitle>In Proceedings of the 7th SIGHAN Workshop on Chinese Language Processing (SIGHAN’13),</booktitle>
<pages>49--53</pages>
<location>Nagoya,</location>
<contexts>
<context position="2665" citStr="Chiu et al., 2013" startWordPosition="396" endWordPosition="399">ling checkers. However, spelling check in Chinese is very different from that in English or other alphabetic languages. There are no word delimiters between words and the length of each word is very short. A Chinese “word” usually comprises two or more characters. The difficulty of Chinese processing is that many Chinese characters have similar shapes or similar (or same) pronunciations. Some characters are even similar in both shape and pronunciation (Wu et al., 2010; Liu et al., 2011). There are many research effort developed for CSC recently, including rule-based model (Jiang et al., 2012; Chiu et al., 2013), n-gram model (Wu et al., 2010; Wang et al., 2013b; Chen et al., 2013), graph theory (Bao et al., 2011; Jia et al., 2013), statistical learning method (Han and Chang, 2013), etc. Some of them are hybrid model. Language modeling (LM) is widely used in CSC, and the most widely-used and wellpracticed language model, by far, is the n-gram LM (Jelinek, 1999), because of its simplicity and fair predictive power. Our work in this paper continues this general line of research by using a tri-gram LM to detect and correct possible spelling errors. In addition, in order to solve the high complexity in t</context>
</contexts>
<marker>Chiu, Wu, Chang, 2013</marker>
<rawString>Hsun-wen Chiu, Jian-cheng Wu and Jason S. Chang. 2013. Chinese Spelling Checker Based on Statistical Machine Translation. In Proceedings of the 7th SIGHAN Workshop on Chinese Language Processing (SIGHAN’13), Nagoya, Japan, 14 October, 2013, pp. 49-53.</rawString>
</citation>
<citation valid="true">
<authors>
<author>Kuan-Yu Chen</author>
<author>Hung-Shin Lee</author>
<author>Chung-Han Lee</author>
</authors>
<title>A Study of Language Modeling for Chinese Spelling Check.</title>
<date>2013</date>
<booktitle>In Proceedings of the 7th SIGHAN Workshop on Chinese Language Processing (SIGHAN’13),</booktitle>
<pages>79--83</pages>
<location>Nagoya,</location>
<contexts>
<context position="2736" citStr="Chen et al., 2013" startWordPosition="410" endWordPosition="413">m that in English or other alphabetic languages. There are no word delimiters between words and the length of each word is very short. A Chinese “word” usually comprises two or more characters. The difficulty of Chinese processing is that many Chinese characters have similar shapes or similar (or same) pronunciations. Some characters are even similar in both shape and pronunciation (Wu et al., 2010; Liu et al., 2011). There are many research effort developed for CSC recently, including rule-based model (Jiang et al., 2012; Chiu et al., 2013), n-gram model (Wu et al., 2010; Wang et al., 2013b; Chen et al., 2013), graph theory (Bao et al., 2011; Jia et al., 2013), statistical learning method (Han and Chang, 2013), etc. Some of them are hybrid model. Language modeling (LM) is widely used in CSC, and the most widely-used and wellpracticed language model, by far, is the n-gram LM (Jelinek, 1999), because of its simplicity and fair predictive power. Our work in this paper continues this general line of research by using a tri-gram LM to detect and correct possible spelling errors. In addition, in order to solve the high complexity in the computation process of the tri-gram based CSC, dynamic programming i</context>
<context position="6485" citStr="Chen et al., 2013" startWordPosition="1020" endWordPosition="1023">Wu et al., 2010). The most widely-used and well-practiced language model, by far, is the n-gram language model (Jelinek, 1999), because of its simplicity and fair predictive power. In n-gram modeling, choosing a proper order of the n-gram is important. On the one hand, higher order n-gram models along with larger corpora tend to increase their quality, and thus will yield lower perplexity for humangenerated text. On the other hand, the higher order n-gram models, such as four-gram or fivegram, usually suffer from the data sparseness problem, which leads to some zero conditional probabilities (Chen et al., 2013). For these reasons, we have developed a Chinese character tri-gram model to determine the best character sequence as the answers for detection and correction. 3 Chinese Tri-gram Model 3.1 Tri-gram Model Given a Chinese character string C = C1, C2, -,CL , the probability of the character string in tri-gram model is approximated by the product of a series of conditional probabilities as follows (Jelinek, 1999), lL L 1) fj P(cl cl-2 , cl-1) . (1) l=3 l= 3 In the above tri-gram model, we make the approximation that the probability of a character depends only on the two immediately preceding 3 www</context>
</contexts>
<marker>Chen, Lee, Lee, 2013</marker>
<rawString>Kuan-Yu Chen, Hung-Shin Lee, Chung-Han Lee , et al.. 2013. A Study of Language Modeling for Chinese Spelling Check. In Proceedings of the 7th SIGHAN Workshop on Chinese Language Processing (SIGHAN’13), Nagoya, Japan, 14 October, 2013, pp. 79-83.</rawString>
</citation>
<citation valid="true">
<authors>
<author>Dongxu Han</author>
<author>Baobao Chang</author>
</authors>
<title>A Maximum Entropy Approach to Chinese Spelling Check.</title>
<date>2013</date>
<booktitle>In Proceedings of the 7th SIGHAN Workshop on Chinese Language Processing (SIGHAN’13),</booktitle>
<pages>74--78</pages>
<location>Nagoya,</location>
<contexts>
<context position="2838" citStr="Han and Chang, 2013" startWordPosition="427" endWordPosition="430">length of each word is very short. A Chinese “word” usually comprises two or more characters. The difficulty of Chinese processing is that many Chinese characters have similar shapes or similar (or same) pronunciations. Some characters are even similar in both shape and pronunciation (Wu et al., 2010; Liu et al., 2011). There are many research effort developed for CSC recently, including rule-based model (Jiang et al., 2012; Chiu et al., 2013), n-gram model (Wu et al., 2010; Wang et al., 2013b; Chen et al., 2013), graph theory (Bao et al., 2011; Jia et al., 2013), statistical learning method (Han and Chang, 2013), etc. Some of them are hybrid model. Language modeling (LM) is widely used in CSC, and the most widely-used and wellpracticed language model, by far, is the n-gram LM (Jelinek, 1999), because of its simplicity and fair predictive power. Our work in this paper continues this general line of research by using a tri-gram LM to detect and correct possible spelling errors. In addition, in order to solve the high complexity in the computation process of the tri-gram based CSC, dynamic programming is used to improve the efficiency of the algorithm. Moreover, additive smoothing to solve the data spar</context>
</contexts>
<marker>Han, Chang, 2013</marker>
<rawString>Dongxu Han, Baobao Chang. 2013. A Maximum Entropy Approach to Chinese Spelling Check. In Proceedings of the 7th SIGHAN Workshop on Chinese Language Processing (SIGHAN’13), Nagoya, Japan, 14 October, 2013, pp. 74-78.</rawString>
</citation>
<citation valid="true">
<authors>
<author>Frederick Jelinek</author>
</authors>
<title>Statistical Methods for Speech Recognition.</title>
<date>1999</date>
<publisher>The MIT Press.</publisher>
<contexts>
<context position="3021" citStr="Jelinek, 1999" startWordPosition="461" endWordPosition="462">ilar (or same) pronunciations. Some characters are even similar in both shape and pronunciation (Wu et al., 2010; Liu et al., 2011). There are many research effort developed for CSC recently, including rule-based model (Jiang et al., 2012; Chiu et al., 2013), n-gram model (Wu et al., 2010; Wang et al., 2013b; Chen et al., 2013), graph theory (Bao et al., 2011; Jia et al., 2013), statistical learning method (Han and Chang, 2013), etc. Some of them are hybrid model. Language modeling (LM) is widely used in CSC, and the most widely-used and wellpracticed language model, by far, is the n-gram LM (Jelinek, 1999), because of its simplicity and fair predictive power. Our work in this paper continues this general line of research by using a tri-gram LM to detect and correct possible spelling errors. In addition, in order to solve the high complexity in the computation process of the tri-gram based CSC, dynamic programming is used to improve the efficiency of the algorithm. Moreover, additive smoothing to solve the data sparseness problem in training set. The rest of this paper is organized as follows. In Section 2, we briefly present the proposed 173 Proceedings of the Third CIPS-SIGHAN Joint Conference</context>
<context position="5993" citStr="Jelinek, 1999" startWordPosition="942" endWordPosition="943">ingsongcha website 3 . Some Chinese characters with similar pronunciation, such as the nasal and the lateral consonants, retroflex and non-retroflex, etc., are also added to the confusion sets in our system. 2.3 Language Modeling Language modeling can be used to quantify the quality of a given word string, and most previous researches have adopted it as a method to predict which word might be a correct word to replace the possible erroneous word (Chen et al., 2009; Liu et al., 2011; Wu et al., 2010). The most widely-used and well-practiced language model, by far, is the n-gram language model (Jelinek, 1999), because of its simplicity and fair predictive power. In n-gram modeling, choosing a proper order of the n-gram is important. On the one hand, higher order n-gram models along with larger corpora tend to increase their quality, and thus will yield lower perplexity for humangenerated text. On the other hand, the higher order n-gram models, such as four-gram or fivegram, usually suffer from the data sparseness problem, which leads to some zero conditional probabilities (Chen et al., 2013). For these reasons, we have developed a Chinese character tri-gram model to determine the best character se</context>
</contexts>
<marker>Jelinek, 1999</marker>
<rawString>Frederick Jelinek. 1999. Statistical Methods for Speech Recognition. The MIT Press.</rawString>
</citation>
<citation valid="true">
<authors>
<author>Zhongye Jia</author>
<author>Peilu Wang</author>
<author>Hai Zhao</author>
</authors>
<title>Graph Model for Chinese Spell Checking.</title>
<date>2013</date>
<booktitle>In Proceedings of the 7th SIGHAN Workshop on Chinese Language Processing (SIGHAN’13),</booktitle>
<pages>88--92</pages>
<location>Nagoya,</location>
<contexts>
<context position="2787" citStr="Jia et al., 2013" startWordPosition="420" endWordPosition="423">re are no word delimiters between words and the length of each word is very short. A Chinese “word” usually comprises two or more characters. The difficulty of Chinese processing is that many Chinese characters have similar shapes or similar (or same) pronunciations. Some characters are even similar in both shape and pronunciation (Wu et al., 2010; Liu et al., 2011). There are many research effort developed for CSC recently, including rule-based model (Jiang et al., 2012; Chiu et al., 2013), n-gram model (Wu et al., 2010; Wang et al., 2013b; Chen et al., 2013), graph theory (Bao et al., 2011; Jia et al., 2013), statistical learning method (Han and Chang, 2013), etc. Some of them are hybrid model. Language modeling (LM) is widely used in CSC, and the most widely-used and wellpracticed language model, by far, is the n-gram LM (Jelinek, 1999), because of its simplicity and fair predictive power. Our work in this paper continues this general line of research by using a tri-gram LM to detect and correct possible spelling errors. In addition, in order to solve the high complexity in the computation process of the tri-gram based CSC, dynamic programming is used to improve the efficiency of the algorithm. </context>
</contexts>
<marker>Jia, Wang, Zhao, 2013</marker>
<rawString>Zhongye Jia, Peilu Wang and Hai Zhao. 2013. Graph Model for Chinese Spell Checking. In Proceedings of the 7th SIGHAN Workshop on Chinese Language Processing (SIGHAN’13), Nagoya, Japan, 14 October, 2013, pp. 88-92.</rawString>
</citation>
<citation valid="true">
<authors>
<author>Ying Jiang</author>
<author>Tong Wang</author>
<author>Tao Lin</author>
</authors>
<title>A rule based Chinese spelling and grammar detection system utility.</title>
<date>2012</date>
<booktitle>In Proceedings of the 2012 International Conference on System Science and Engineering (ICSSE),</booktitle>
<pages>437--440</pages>
<contexts>
<context position="2645" citStr="Jiang et al., 2012" startWordPosition="392" endWordPosition="395">tomatic Chinese spelling checkers. However, spelling check in Chinese is very different from that in English or other alphabetic languages. There are no word delimiters between words and the length of each word is very short. A Chinese “word” usually comprises two or more characters. The difficulty of Chinese processing is that many Chinese characters have similar shapes or similar (or same) pronunciations. Some characters are even similar in both shape and pronunciation (Wu et al., 2010; Liu et al., 2011). There are many research effort developed for CSC recently, including rule-based model (Jiang et al., 2012; Chiu et al., 2013), n-gram model (Wu et al., 2010; Wang et al., 2013b; Chen et al., 2013), graph theory (Bao et al., 2011; Jia et al., 2013), statistical learning method (Han and Chang, 2013), etc. Some of them are hybrid model. Language modeling (LM) is widely used in CSC, and the most widely-used and wellpracticed language model, by far, is the n-gram LM (Jelinek, 1999), because of its simplicity and fair predictive power. Our work in this paper continues this general line of research by using a tri-gram LM to detect and correct possible spelling errors. In addition, in order to solve the </context>
</contexts>
<marker>Jiang, Wang, Lin, 2012</marker>
<rawString>Ying Jiang, Tong Wang, Tao Lin, et al. 2012. A rule based Chinese spelling and grammar detection system utility. In Proceedings of the 2012 International Conference on System Science and Engineering (ICSSE), pp. 437-440.</rawString>
</citation>
<citation valid="true">
<authors>
<author>Karen Kukich</author>
</authors>
<title>Techniques for Automatically Correcting Words in Text.</title>
<date>1992</date>
<journal>ACM Computing Surveys,</journal>
<volume>24</volume>
<pages>377--439</pages>
<contexts>
<context position="1415" citStr="Kukich, 1992" startWordPosition="204" endWordPosition="205">addition, we use dynamic programming to improve the efficiency of the algorithm, and additive smoothing to solve the data sparseness problem in training set. Empirical evaluation results demonstrate the utility of our CSC system. 1 Introduction Spelling check is a common task in every written language, which is an automatic mechanism to detect and correct human errors (Wu et al., 2013). The problem of devising algorithms and techniques for automatically correcting words in text began as early as the 1960s on computer techniques for automatic spelling correction and automatic text recognition (Kukich, 1992), and it has continued up to the present. A spelling checker should have both capabilities consisting of error detection and error correction. Spelling error detection is to indicate the various types of spelling errors in the text. Spelling error correction is further to suggest the correct characters of detected errors. Chinese as a foreign language (CFL) have * Corresponding author attracted more and more attention, and this trend is continuing. For this purpose, at the SIGHAN Bake-offs, Chinese spelling check (CSC) task are organized to provide an evaluation platform for developing and imp</context>
</contexts>
<marker>Kukich, 1992</marker>
<rawString>Karen Kukich. 1992. Techniques for Automatically Correcting Words in Text. ACM Computing Surveys, Vol. 24, No.4, pp. 377-439.</rawString>
</citation>
<citation valid="true">
<authors>
<author>Chao-Lin Liu</author>
<author>Min-Hua Lai</author>
<author>Kan-Wen Tien</author>
</authors>
<title>Visually and Phonologically Similar Characters in Incorrect Chinese Words: Analyses, Identification, and Applications.</title>
<date>2011</date>
<journal>ACM Transactions on Asian Language Information Processing,</journal>
<volume>10</volume>
<pages>1--39</pages>
<contexts>
<context position="2538" citStr="Liu et al., 2011" startWordPosition="376" endWordPosition="379">elling check (CSC) task are organized to provide an evaluation platform for developing and implementing automatic Chinese spelling checkers. However, spelling check in Chinese is very different from that in English or other alphabetic languages. There are no word delimiters between words and the length of each word is very short. A Chinese “word” usually comprises two or more characters. The difficulty of Chinese processing is that many Chinese characters have similar shapes or similar (or same) pronunciations. Some characters are even similar in both shape and pronunciation (Wu et al., 2010; Liu et al., 2011). There are many research effort developed for CSC recently, including rule-based model (Jiang et al., 2012; Chiu et al., 2013), n-gram model (Wu et al., 2010; Wang et al., 2013b; Chen et al., 2013), graph theory (Bao et al., 2011; Jia et al., 2013), statistical learning method (Han and Chang, 2013), etc. Some of them are hybrid model. Language modeling (LM) is widely used in CSC, and the most widely-used and wellpracticed language model, by far, is the n-gram LM (Jelinek, 1999), because of its simplicity and fair predictive power. Our work in this paper continues this general line of research</context>
<context position="5865" citStr="Liu et al., 2011" startWordPosition="920" endWordPosition="923">ently the most popular Chinese input method, the confusion sets used in our system is constructed from a homophone dictionary of qingsongcha website 3 . Some Chinese characters with similar pronunciation, such as the nasal and the lateral consonants, retroflex and non-retroflex, etc., are also added to the confusion sets in our system. 2.3 Language Modeling Language modeling can be used to quantify the quality of a given word string, and most previous researches have adopted it as a method to predict which word might be a correct word to replace the possible erroneous word (Chen et al., 2009; Liu et al., 2011; Wu et al., 2010). The most widely-used and well-practiced language model, by far, is the n-gram language model (Jelinek, 1999), because of its simplicity and fair predictive power. In n-gram modeling, choosing a proper order of the n-gram is important. On the one hand, higher order n-gram models along with larger corpora tend to increase their quality, and thus will yield lower perplexity for humangenerated text. On the other hand, the higher order n-gram models, such as four-gram or fivegram, usually suffer from the data sparseness problem, which leads to some zero conditional probabilities</context>
</contexts>
<marker>Liu, Lai, Tien, 2011</marker>
<rawString>Chao-Lin Liu, Min-Hua Lai, Kan-Wen Tien, et al.. 2011. Visually and Phonologically Similar Characters in Incorrect Chinese Words: Analyses, Identification, and Applications. ACM Transactions on Asian Language Information Processing, Vol. 10, No. 2, pp. 1-39.</rawString>
</citation>
<citation valid="true">
<authors>
<author>Yih-Ru Wang</author>
<author>Jason S Chang</author>
<author>Jian-Cheng Wu</author>
</authors>
<title>Automatic Chinese Confusion Words Extraction Using Conditional Random Fields and the Web.</title>
<date>2013</date>
<booktitle>In Proceedings of the 7th SIGHAN Workshop on Chinese Language Processing (SIGHAN’13),</booktitle>
<pages>64--68</pages>
<location>Nagoya,</location>
<contexts>
<context position="2715" citStr="Wang et al., 2013" startWordPosition="406" endWordPosition="409">s very different from that in English or other alphabetic languages. There are no word delimiters between words and the length of each word is very short. A Chinese “word” usually comprises two or more characters. The difficulty of Chinese processing is that many Chinese characters have similar shapes or similar (or same) pronunciations. Some characters are even similar in both shape and pronunciation (Wu et al., 2010; Liu et al., 2011). There are many research effort developed for CSC recently, including rule-based model (Jiang et al., 2012; Chiu et al., 2013), n-gram model (Wu et al., 2010; Wang et al., 2013b; Chen et al., 2013), graph theory (Bao et al., 2011; Jia et al., 2013), statistical learning method (Han and Chang, 2013), etc. Some of them are hybrid model. Language modeling (LM) is widely used in CSC, and the most widely-used and wellpracticed language model, by far, is the n-gram LM (Jelinek, 1999), because of its simplicity and fair predictive power. Our work in this paper continues this general line of research by using a tri-gram LM to detect and correct possible spelling errors. In addition, in order to solve the high complexity in the computation process of the tri-gram based CSC, </context>
<context position="5116" citStr="Wang et al., 2013" startWordPosition="798" endWordPosition="801"> step. 3. The system will calculate the score of every candidate sentence by using the n-gram model. We use the corpus of CCL 1 and sogou 2 to generate the frequency of n-gram. Finally, the sentence with highest score will be chosen as the final output. Due to the high complexity of step 2 and step 3, we optimize the algorithm by using dynamic programming. 2.2 Confusion Set Confusion set is a ready set of commonly confused characters plays an important role in 1ccl.pku.edu.cn:8080/ccl_corpus/index.jsp?dir=xiandai 2 www.sogou.com/labs/dl/c.html spelling error detection and correction in texts (Wang et al., 2013a). Most Chinese characters have other characters similar to them in either shape or pronunciation. Since pinyin input method is currently the most popular Chinese input method, the confusion sets used in our system is constructed from a homophone dictionary of qingsongcha website 3 . Some Chinese characters with similar pronunciation, such as the nasal and the lateral consonants, retroflex and non-retroflex, etc., are also added to the confusion sets in our system. 2.3 Language Modeling Language modeling can be used to quantify the quality of a given word string, and most previous researches </context>
</contexts>
<marker>Wang, Chang, Wu, 2013</marker>
<rawString>Yih-Ru Wang, Jason S. Chang, Jian-Cheng Wu, et al.. 2013a. Automatic Chinese Confusion Words Extraction Using Conditional Random Fields and the Web. In Proceedings of the 7th SIGHAN Workshop on Chinese Language Processing (SIGHAN’13), Nagoya, Japan, 14 October, 2013, pp. 64-68.</rawString>
</citation>
<citation valid="true">
<authors>
<author>Yih-Ru Wang</author>
<author>Yuan-Fu Liao</author>
<author>Yeh-Kuang Wu</author>
</authors>
<title>Conditional Random Field-based Parser and Language Model for Traditional Chinese Spelling Checker.</title>
<date>2013</date>
<booktitle>In Proceedings of the 7th SIGHAN Workshop on Chinese Language Processing (SIGHAN’13),</booktitle>
<pages>69--73</pages>
<location>Nagoya,</location>
<contexts>
<context position="2715" citStr="Wang et al., 2013" startWordPosition="406" endWordPosition="409">s very different from that in English or other alphabetic languages. There are no word delimiters between words and the length of each word is very short. A Chinese “word” usually comprises two or more characters. The difficulty of Chinese processing is that many Chinese characters have similar shapes or similar (or same) pronunciations. Some characters are even similar in both shape and pronunciation (Wu et al., 2010; Liu et al., 2011). There are many research effort developed for CSC recently, including rule-based model (Jiang et al., 2012; Chiu et al., 2013), n-gram model (Wu et al., 2010; Wang et al., 2013b; Chen et al., 2013), graph theory (Bao et al., 2011; Jia et al., 2013), statistical learning method (Han and Chang, 2013), etc. Some of them are hybrid model. Language modeling (LM) is widely used in CSC, and the most widely-used and wellpracticed language model, by far, is the n-gram LM (Jelinek, 1999), because of its simplicity and fair predictive power. Our work in this paper continues this general line of research by using a tri-gram LM to detect and correct possible spelling errors. In addition, in order to solve the high complexity in the computation process of the tri-gram based CSC, </context>
<context position="5116" citStr="Wang et al., 2013" startWordPosition="798" endWordPosition="801"> step. 3. The system will calculate the score of every candidate sentence by using the n-gram model. We use the corpus of CCL 1 and sogou 2 to generate the frequency of n-gram. Finally, the sentence with highest score will be chosen as the final output. Due to the high complexity of step 2 and step 3, we optimize the algorithm by using dynamic programming. 2.2 Confusion Set Confusion set is a ready set of commonly confused characters plays an important role in 1ccl.pku.edu.cn:8080/ccl_corpus/index.jsp?dir=xiandai 2 www.sogou.com/labs/dl/c.html spelling error detection and correction in texts (Wang et al., 2013a). Most Chinese characters have other characters similar to them in either shape or pronunciation. Since pinyin input method is currently the most popular Chinese input method, the confusion sets used in our system is constructed from a homophone dictionary of qingsongcha website 3 . Some Chinese characters with similar pronunciation, such as the nasal and the lateral consonants, retroflex and non-retroflex, etc., are also added to the confusion sets in our system. 2.3 Language Modeling Language modeling can be used to quantify the quality of a given word string, and most previous researches </context>
</contexts>
<marker>Wang, Liao, Wu, 2013</marker>
<rawString>Yih-Ru Wang, Yuan-Fu Liao, Yeh-Kuang Wu, et al.. 2013b. Conditional Random Field-based Parser and Language Model for Traditional Chinese Spelling Checker. In Proceedings of the 7th SIGHAN Workshop on Chinese Language Processing (SIGHAN’13), Nagoya, Japan, 14 October, 2013, pp. 69-73.</rawString>
</citation>
<citation valid="true">
<authors>
<author>Shih-Hung Wu</author>
<author>Yong-Zhi Chen</author>
<author>Ping-Che Yang</author>
</authors>
<title>Reducing the False Alarm Rate of Chinese Character Error Detection and Correction.</title>
<date>2010</date>
<booktitle>In Proceeding of CIPSSIGHAN Joint Conference on Chinese Language Processing (CLP 2010),</booktitle>
<pages>54--61</pages>
<location>Beijing,</location>
<contexts>
<context position="2519" citStr="Wu et al., 2010" startWordPosition="372" endWordPosition="375">-offs, Chinese spelling check (CSC) task are organized to provide an evaluation platform for developing and implementing automatic Chinese spelling checkers. However, spelling check in Chinese is very different from that in English or other alphabetic languages. There are no word delimiters between words and the length of each word is very short. A Chinese “word” usually comprises two or more characters. The difficulty of Chinese processing is that many Chinese characters have similar shapes or similar (or same) pronunciations. Some characters are even similar in both shape and pronunciation (Wu et al., 2010; Liu et al., 2011). There are many research effort developed for CSC recently, including rule-based model (Jiang et al., 2012; Chiu et al., 2013), n-gram model (Wu et al., 2010; Wang et al., 2013b; Chen et al., 2013), graph theory (Bao et al., 2011; Jia et al., 2013), statistical learning method (Han and Chang, 2013), etc. Some of them are hybrid model. Language modeling (LM) is widely used in CSC, and the most widely-used and wellpracticed language model, by far, is the n-gram LM (Jelinek, 1999), because of its simplicity and fair predictive power. Our work in this paper continues this gener</context>
<context position="5883" citStr="Wu et al., 2010" startWordPosition="924" endWordPosition="927">ular Chinese input method, the confusion sets used in our system is constructed from a homophone dictionary of qingsongcha website 3 . Some Chinese characters with similar pronunciation, such as the nasal and the lateral consonants, retroflex and non-retroflex, etc., are also added to the confusion sets in our system. 2.3 Language Modeling Language modeling can be used to quantify the quality of a given word string, and most previous researches have adopted it as a method to predict which word might be a correct word to replace the possible erroneous word (Chen et al., 2009; Liu et al., 2011; Wu et al., 2010). The most widely-used and well-practiced language model, by far, is the n-gram language model (Jelinek, 1999), because of its simplicity and fair predictive power. In n-gram modeling, choosing a proper order of the n-gram is important. On the one hand, higher order n-gram models along with larger corpora tend to increase their quality, and thus will yield lower perplexity for humangenerated text. On the other hand, the higher order n-gram models, such as four-gram or fivegram, usually suffer from the data sparseness problem, which leads to some zero conditional probabilities (Chen et al., 201</context>
</contexts>
<marker>Wu, Chen, Yang, 2010</marker>
<rawString>Shih-Hung Wu, Yong-Zhi Chen, Ping-Che Yang, et al.. 2010. Reducing the False Alarm Rate of Chinese Character Error Detection and Correction. In Proceeding of CIPSSIGHAN Joint Conference on Chinese Language Processing (CLP 2010), Beijing, 28-29 Aug., 2010, pp. 54-61.</rawString>
</citation>
<citation valid="true">
<authors>
<author>Shih-Hung Wu</author>
<author>Chao-Lin Liu</author>
<author>Lung-Hao Lee</author>
</authors>
<title>Chinese Spelling Check Evaluation at SIGHAN Bake-off</title>
<date>2013</date>
<booktitle>In Proceedings of the 7th SIGHAN Workshop on Chinese Language Processing (SIGHAN’13),</booktitle>
<pages>35--42</pages>
<location>Nagoya,</location>
<contexts>
<context position="1190" citStr="Wu et al., 2013" startWordPosition="170" endWordPosition="173">e modeling (LM) is widely used in CSC because of its simplicity and fair predictive power. Our work in this paper continues this general line of research by using a tri-gram LM to detect and correct possible spelling errors. In addition, we use dynamic programming to improve the efficiency of the algorithm, and additive smoothing to solve the data sparseness problem in training set. Empirical evaluation results demonstrate the utility of our CSC system. 1 Introduction Spelling check is a common task in every written language, which is an automatic mechanism to detect and correct human errors (Wu et al., 2013). The problem of devising algorithms and techniques for automatically correcting words in text began as early as the 1960s on computer techniques for automatic spelling correction and automatic text recognition (Kukich, 1992), and it has continued up to the present. A spelling checker should have both capabilities consisting of error detection and error correction. Spelling error detection is to indicate the various types of spelling errors in the text. Spelling error correction is further to suggest the correct characters of detected errors. Chinese as a foreign language (CFL) have * Correspo</context>
</contexts>
<marker>Wu, Liu, Lee, 2013</marker>
<rawString>Shih-Hung Wu, Chao-Lin Liu, and Lung-Hao Lee. 2013. Chinese Spelling Check Evaluation at SIGHAN Bake-off 2013. In Proceedings of the 7th SIGHAN Workshop on Chinese Language Processing (SIGHAN’13), Nagoya, Japan, 14 October, 2013, pp. 35-42.</rawString>
</citation>
</citationList>
</algorithm>
</algorithms>
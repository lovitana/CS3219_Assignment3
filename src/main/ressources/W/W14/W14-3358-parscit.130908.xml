<?xml version="1.0" encoding="UTF-8"?>
<algorithms version="110505">
<algorithm name="SectLabel" version="110505">
<variant no="0" confidence="0.000003">
<title confidence="0.994864">
Dynamic Topic Adaptation for SMT using Distributional Profiles
</title>
<author confidence="0.9999">
Eva Hasler1 Barry Haddow1 Philipp Koehn1,2
</author>
<affiliation confidence="0.9118005">
1School of Informatics, University of Edinburgh
2Center for Language and Speech Processing, Johns Hopkins University
</affiliation>
<email confidence="0.99121">
e.hasler@ed.ac.uk, {bhaddow,pkoehn}@inf.ed.ac.uk
</email>
<sectionHeader confidence="0.993775" genericHeader="abstract">
Abstract
</sectionHeader>
<bodyText confidence="0.99951105">
Despite its potential to improve lexical
selection, most state-of-the-art machine
translation systems take only minimal con-
textual information into account. We cap-
ture context with a topic model over dis-
tributional profiles built from the context
words of each translation unit. Topic dis-
tributions are inferred for each transla-
tion unit and used to adapt the translation
model dynamically to a given test context
by measuring their similarity. We show
that combining information from both lo-
cal and global test contexts helps to im-
prove lexical selection and outperforms a
baseline system by up to 1.15 BLEU. We
test our topic-adapted model on a diverse
data set containing documents from three
different domains and achieve competitive
performance in comparison with two su-
pervised domain-adapted systems.
</bodyText>
<sectionHeader confidence="0.998998" genericHeader="keywords">
1 Introduction
</sectionHeader>
<bodyText confidence="0.999957542372882">
The task of lexical selection plays an important
role in statistical machine translation (SMT). It
strongly depends on context and is particularly dif-
ficult when the domain of a test document is un-
known, for example when translating web doc-
uments from diverse sources. Selecting transla-
tions of words or phrases that preserve the sense
of the source words is closely related to the field
of word sense disambiguation (WSD), which has
been studied extensively in the past.
Most approaches to WSD model context at the
sentence level and do not take the wider context
of a word into account. Some of the ideas from
the field of WSD have been adapted for machine
translation (Carpuat and Wu, 2007b; Carpuat and
Wu, 2007a; Chan et al., 2007). For example,
Carpuat and Wu (2007a) extend word sense dis-
ambiguation to phrase sense disambiguation and
show improved performance due to the better fit
with multiple possible segmentations in a phrase-
based system. Carpuat (2009) test the “one sense
per discourse” hypothesis (Gale et al., 1992) for
MT and find that enforcing it as a constraint at the
document level could potentially improve transla-
tion quality. Our goal is to make correct lexical
choices in a given context without explicitly en-
forcing translation consistency.
More recent work in SMT uses latent repre-
sentations of the document context to dynam-
ically adapt the translation model with either
monolingual topic models (Eidelman et al., 2012;
Hewavitharana et al., 2013) or bilingual topic
models (Hasler et al., 2014), thereby allowing the
translation system to disambiguate source phrases
using document context. Eidelman et al. (2012)
also apply a topic model to each test sentence and
find that sentence context is sufficient for pick-
ing good translations, but they do not attempt to
combine sentence and document level informa-
tion. Sentence-level topic adaptation for SMT has
also been employed by Hasler et al. (2012). Other
approaches to topic adaptation for SMT include
Zhao and Xing (2007) and Tam et al. (2008), both
of which use adapted lexical weights.
In this paper, we present a topic model that
learns latent distributional representations of the
context of a phrase pair which can be applied to
both local and global contexts at test time. We
introduce similarity features that compare latent
representations of phrase pair types to test con-
texts to disambiguate senses for improved lexi-
cal selection. We also propose different strate-
gies for combining local and global topical context
and show that using clues from both levels of con-
texts is beneficial for translation model adaptation.
We evaluate our model on a dynamic adaptation
task where the domain of a test document is un-
known and hence the problem of lexical selection
is harder.
</bodyText>
<page confidence="0.988688">
445
</page>
<note confidence="0.737785">
Proceedings of the Ninth Workshop on Statistical Machine Translation, pages 445–456,
Baltimore, Maryland USA, June 26–27, 2014. c�2014 Association for Computational Linguistics
</note>
<sectionHeader confidence="0.997686" genericHeader="introduction">
2 Related work
</sectionHeader>
<bodyText confidence="0.999980271428572">
Most work in the WSD literature has modelled
disambiguation using a limited window of con-
text around the word to disambiguate. Cai et al.
(2007), Boyd-graber and Blei (2007) and Li et al.
(2010) further tried to integrate the notion of la-
tent topics to address the sparsity problem of the
lexicalised features typically used in WSD classi-
fiers. The most closely related work in the area
of sense disambiguation is by Dinu and Lapata
(2010) who propose a disambiguation method for
solving lexical similarity and substitution tasks.
They measure word similarity in context by learn-
ing distributions over senses for each target word
in the form of lower-dimensional distributional
representations. Before computing word similar-
ities, they contextualise the global sense distribu-
tion of a word using the sense distribution of words
in the test context, thereby shifting the sense distri-
bution towards the test context. We adopt a simi-
lar distributional representation, but argue that our
representation does not need this disambiguation
step because at the level of phrase pairs the ambi-
guity is already much reduced.
Our model performs adaptation using similar-
ity features which is similar to the approach of
Costa-juss`a and Banchs (2010) who learn a vec-
tor space model that captures the source context
of every training sentence. In Banchs and Costa-
juss`a (2011), the vector space model is replaced
with representations inferred by Latent Seman-
tic Indexing. However, because their latent rep-
resentations are learned over training sentences,
they have to compare the current test sentence to
the latent vector of every training instance associ-
ated with a translation unit. The highest similar-
ity value is then used as a feature value. Instead,
our model learns latent distributional representa-
tions of phrase pairs that can be directly compared
to test contexts and are likely to be more robust.
Because context words of a phrase pair are tied to-
gether in the distributional representations, we can
use sparse priors to cluster context words associ-
ated with the same phrase pair into few topics.
Recently, Chen et al. (2013) have proposed a
vector space model for domain adaptation where
phrase pairs are assigned vectors that are defined
in terms of the training corpora. A similar vector
is built for an in-domain development set and the
similarity to the development set is used as a fea-
ture during translation. While their vector repre-
sentations are similar to our latent topic represen-
tations, their model has no notion of structure be-
yond corpus boundaries and is adapted towards a
single target domain (cross-domain). Instead, our
model learns the latent topical structure automati-
cally and the translation model is adapted dynam-
ically to each test instance.
We are not aware of prior work in the field of
MT that investigates combinations of local and
global context. In their recent work on neural lan-
guage models, Huang et al. (2012) combine the
scores of two neural networks modelling the word
embeddings of previous words in a sequence as
well as those of words from the surrounding doc-
ument by averaging over all word embeddings oc-
curring in the same document. The score of the
next word in a sequence is computed as the sum of
the scores of both networks, but they do not con-
sider alternative ways of combining contextual in-
formation.
</bodyText>
<sectionHeader confidence="0.961998" genericHeader="method">
3 Phrase pair topic model (PPT)
</sectionHeader>
<bodyText confidence="0.99983396">
Our proposed model aims to capture the relation-
ship between phrase pairs and source words that
frequently occur in the local context of a phrase
pair, that is, context words occurring in the same
sentence. It therefore follows the distributional
hypothesis (Harris, 1954) which states that words
that occur in the same contexts tend to have sim-
ilar meanings. For a phrase pair, the idea is that
words that occur frequently in its context are in-
dicative of the sense that is captured by the target
phrase translating the source phrase.
We assume that all phrase pairs share a global
set of topics and during topic inference the distri-
bution over topics for each phrase pair is induced
from the latent topic of its context words in the
training data. In order to learn topic distributions
for each phrase pair, we represent phrase pairs as
documents containing all context words from the
source sentence context in the training data. These
distributional profiles of phrase pairs are the in-
put to the topic modelling algorithm which learns
topic clusters over context words.
Figure 1a shows a graphical representation of
the following generative process for training. For
each of P phrase pairs ppz in the collection
</bodyText>
<listItem confidence="0.8805424">
1. Draw a topic distribution from an asymmetric
Dirichlet prior, Op — Dirichlet(α0, α ... α).
2. For each position c in the distributional pro-
file of ppz, draw a topic from that distribution,
zp1, — Multinomial(Op).
</listItem>
<page confidence="0.99752">
446
</page>
<figure confidence="0.999362">
(a) Inference on phrase pair documents (training).
(b) Inference on local test contexts (test).
</figure>
<figureCaption confidence="0.9803535">
Figure 1: Graphical representation of the phrase
pair topic (PPT) model.
</figureCaption>
<listItem confidence="0.613874">
3. Conditioned on topic zp,c, choose a context
word wp,c ∼ Multinomial(ψzp,,).
</listItem>
<bodyText confidence="0.9512369">
α and Q are parameters of the Dirichlet distribu-
tions and φk denotes topic-dependent vocabularies
over context words. Test contexts are generated
similarly by drawing topic mixtures Bl for each test
context1 as shown in Figure 1b, drawing topics z
for each context position and then drawing context
words w for each z. The asymmetric prior on topic
distributions (α0 for topic 0 and α for all other top-
ics) encodes the intuition that there are words oc-
curring in the context of many phrase pairs which
</bodyText>
<footnote confidence="0.7963778">
1A local test context is defined as all words in the test
sentence excluding stop words, while contexts of phrase pairs
in training do not include the words belonging to the source
phrase. The naming in the figure refers to local test contexts
L, but global test contexts will be defined similarly.
</footnote>
<bodyText confidence="0.999669428571429">
can be grouped under a topic with higher a priori
probability than the other topics. Figure 1a shows
the model for training inference on the distribu-
tional representations for each phrase pair, where
Cl−all denotes the number of context words in all
sentence contexts that the phrase pair was seen in
the training data, P denotes the number of phrase
pairs and K denotes the number of latent topics.
The model in Figure 1b has the same structure
but shows inference on test contexts, where Cl de-
notes the number of context words in the test sen-
tence context and L denotes the number of test in-
stances. Bp and Bl denote the topic distribution for
a phrase pair and a test context, respectively.
</bodyText>
<subsectionHeader confidence="0.688188">
3.1 Inference for PPT model
</subsectionHeader>
<bodyText confidence="0.99996275">
We use collapsed variational Bayes (Teh et al.,
2006) to infer the parameters of the PPT model.
The posterior distribution over topics is computed
as shown below
</bodyText>
<equation confidence="0.999594125">
(EP(zp,c = k|z−(p,c), wc,p, α, Q) a
ˆq[n−(p,c)
(E(E.,k,w, ] + Q) ·
ˆq[n−(p,c)
d,k,. ] + α)
ˆq[n−(p,c)
.,k,. ] + Wc · Q)
(1)
</equation>
<bodyText confidence="0.9949944">
where zp,c denotes the topic at position c in
the distributional profile p, wc denotes all con-
text word tokens in the collection, Wc is the total
number of context words and Eˆq is the expecta-
tion under the variationalosterior. n−(p,c) and
</bodyText>
<equation confidence="0.630007">
p .,k,w,
n−(p,c) are counts of topics occurring with context
p,k,.
</equation>
<bodyText confidence="0.991500722222222">
words and distributional profiles, respectively, and
n−(p,c) is a topic occurrence count.
.,k,.
Before training the topic model, we remove stop
words from all documents. When inferring top-
ics for test contexts, we ignore unseen words be-
cause they do not contribute information for topic
inference. In order to speed up training inference,
we limit the documents in the collection to those
corresponding to phrase pairs that are needed to
translate the test set2. Inference was run for 50 it-
erations on the distributional profiles for training
and for 10 iterations on the test contexts. The out-
put of the training inference step is a model file
with all the necessary statistics to compute pos-
terior topic distributions (which are loaded before
running test inference), and the set of topic vectors
for all phrase pairs. The output of test inference is
</bodyText>
<footnote confidence="0.9395415">
2Reducing the training contexts by scaling or sampling
would be expected to speed up inference considerably.
</footnote>
<page confidence="0.99736">
447
</page>
<bodyText confidence="0.998945">
the set of induced topic vectors for all test con-
texts.
</bodyText>
<subsectionHeader confidence="0.999978">
3.2 Modelling local and global context
</subsectionHeader>
<bodyText confidence="0.999282842105263">
At training time, our model has access to context
words only from the local contexts of each
phrase pair in their distributional profiles, that is,
other words in the same source sentence as the
phrase pair. This is useful for reducing noise and
constraining the semantic space that the model
considers for each phrase pair during training. At
test time, however, we are not limited to applying
the model only to the immediate surroundings of
a source phrase to disambiguate its meaning. We
can potentially take any size of test context into
account to disambiguate the possible senses of a
source phrase, but for simplicity we consider two
sizes of context here which we refer to as local
and global context.
Local context Words appearing in the sentence
around a test source phrase, excluding stop words.
Global context Words appearing in the document
around a test source phrase, excluding stop words.
</bodyText>
<sectionHeader confidence="0.980952" genericHeader="method">
4 Similarity features
</sectionHeader>
<bodyText confidence="0.953873142857143">
We define similarity features that compare the
topic vector θp assigned to a phrase pair3 to the
topic vector assigned to a test context, The fea-
ture is defined for each source phrase and all its
possible translations in the phrase table, as shown
below
sim(ppi, test context) = cosine(θpi, θc),
</bodyText>
<equation confidence="0.953598">
Vppi E Ippi1¯s —* ¯ti1 (2)
</equation>
<bodyText confidence="0.999928615384615">
Unlike Banchs and Costa-juss`a (2011), we do
not learn topic vectors for every training sentence
which results in a topic vector per phrase pair to-
ken, but instead we learn topic vectors for each
phrase pair type. This is more efficient but also
more appealing from a modelling point of view, as
the topic distributions associated with phrase pairs
can be thought of as expected latent contexts. The
application of the similarity feature is visualised
in Figure 2. On the left, there are two applicable
phrase pairs for the source phrase noyau, noyau
—* kernel and noyau —* nucleus, with their distri-
butional representations (words belonging to the
</bodyText>
<footnote confidence="0.485311666666667">
3The mass of topic 0 is removed from the vectors and
the vectors are renormalised before computing similarity fea-
tures.
</footnote>
<bodyText confidence="0.992898380952381">
IT topic versus the scientific topic) and assigned
topic vectors θp. The local and global test contexts
are similarly represented by a document contain-
ing the context words and a resulting topic vector
θl or θg. The test context vector θc can be one of
θl and θg or a combination of both. In this ex-
ample, the distributional representation of noyau
—* kernel has a larger topical overlap with the test
context and will more likely be selected during de-
coding.
Figure 2: Similarity between topic vectors of two
applicable phrase pairs θp and the topic vectors θl
and θg from the local and global test context dur-
ing test time.
While this work focuses on exploring vec-
tor space similarity for adaptation, mostly for
computational ease, it may be possible to derive
probabilistic translation features from the PPT
model. This could be a useful addition to the
model and we leave this as an avenue for future
work.
</bodyText>
<subsectionHeader confidence="0.562511">
Types of similarity features
</subsectionHeader>
<bodyText confidence="0.999995076923077">
We experiment with local and global phrase simi-
larity features, phrSim-local and phrSim-global, to
perform dynamic topic adaptation. These two sim-
ilarity features can be combined by adding them
both to the log-linear SMT model, in which case
each receive separate feature weights. Whenever
we use the + symbol in our results tables, the
additional features were combined with existing
features log-linearly. However, we also experi-
mented with an alternative combination of local
and global information where we combine the lo-
cal and global topic vectors for each test context
before computing similarity features.4 We were
</bodyText>
<footnote confidence="0.7461755">
4The combined topic vectors were renormalised before
computing their similarities with each candidate phrase pair.
</footnote>
<page confidence="0.995477">
448
</page>
<bodyText confidence="0.999962">
motivated by the observation that there are cases
where the local and global features have an op-
posite preference for one translation over another,
but the log-linear combination can only learn a
global preference for one of the features. Com-
bining the topic vectors allows us to potentially
encode a preference for one of the contexts that
depends on each test instance.
For similarity features derived from combined
topic vectors, ⊕ denotes the additive combination
of topic vectors, ⊗ denotes the multiplicative com-
bination of topic vectors and OO denotes a combina-
tion that favours the local context for longer sen-
tences and backs off incrementally to the global
context for shorter sentences.5 The intuition be-
hind this combination is that if there is already suf-
ficient evidence in the local context, the local topic
mixture may be more reliable than the global mix-
ture.
We also experiment with a combination of the
phrase pair similarity features derived from the
PPT model with a document similarity feature
from the pLDA model described in Hasler et al.
(2014). The motivation is that their model learns
topic mixtures for documents and uses phrases in-
stead of words to infer the topical context. There-
fore, it might provide additional information to our
similarity features.
</bodyText>
<sectionHeader confidence="0.967287" genericHeader="method">
5 Data and experimental setup
</sectionHeader>
<bodyText confidence="0.982051208333333">
Our experiments were carried out on a mixed
French-English data set containing the TED cor-
pus (Cettolo et al., 2012), parts of the News Com-
mentary corpus (NC) and parts of the Common-
crawl corpus (CC) from the WMT13 shared task
(Bojar et al., 2013) as described in Table 1. To
ensure that the baseline model does not have an
implicit preference for any particular domain, we
selected subsets of the NC and CC corpora such
that the training data contains 2.7M English words
per domain. We were guided by two constraints
in chosing our data set in order to simulate an
environment where very diverse documents have
to be translated, which is a typical scenario for
web translation engines: 1) the data has docu-
ment boundaries and the content of each docu-
ment is assumed to be topically related, 2) there is
some degree of topical variation within each data
set. This setup allows us to evaluate our dynamic
5The interpolation weights between local and global topic
vectors were set proportional to sentence lengths between 1
and 30. The length of longer sentences was clipped to 30.
topic adaptation approach because the test docu-
ments are from different domains and also differ
within each domain, which makes lexical selec-
tion a much harder problem. The topic adaptation
approach does not make use of the domain labels
in training or test, because it infers topic mixtures
in an unsupervised way. However, we compare the
performance of our dynamic approach to domain
adaptation methods by providing them the domain
labels for each document in training and test.
In order to abstract away from adaptation ef-
fects that concern tuning of length penalties and
language models, we use a mixed tuning set con-
taining data from all three domains and train one
language model on the concatenation of the tar-
get sides of the training data. Word alignments
are trained on the concatenation of all training data
and fixed for all models. Table 2 shows the aver-
age length of a document for each domain. While
a CC document contains 29.1 sentences on aver-
age, documents from NC and TED are on average
more than twice as long. The length of a document
could have an influence on how reliable global
topic information is but also on how important it
is to have information from both local and global
test contexts.
</bodyText>
<table confidence="0.999294">
Data Mixed CC NC TED
Train 354K (6450) 110K 103K 140K
Dev 2453 (39) 818 817 818
Test 5664 (112) 1892 1878 1894
</table>
<tableCaption confidence="0.8073545">
Table 1: Number of sentence pairs and documents
(in brackets) in the data sets.
</tableCaption>
<table confidence="0.999892333333333">
Data CC NC TED
Test documents 65 31 24
Avg sentences/doc 29.1 60.6 78.9
</table>
<tableCaption confidence="0.9915855">
Table 2: Average number of sentences per docu-
ment in the test set (per domain).
</tableCaption>
<subsectionHeader confidence="0.988872">
5.1 Unadapted baseline system
</subsectionHeader>
<bodyText confidence="0.999770375">
Our baseline is a phrase-based French-English
system trained on the concatenation of all parallel
data. It was built with the Moses toolkit (Koehn
et al., 2007) using the 14 standard core features
including a 5-gram language model. Translation
quality is evaluated on a large test set, using the
average feature weights of three optimisation runs
with PRO (Hopkins and May, 2011). We use the
</bodyText>
<page confidence="0.995737">
449
</page>
<figure confidence="0.535381">
noyau → kernel noyau → nucleus noyau → core
</figure>
<figureCaption confidence="0.747412">
Figure 3: Topic distributions for source phrase noyau and three of its translations (20 topics without topic
0). Colored bars correspond to topics IT, politics, science, economy with topic proportions &gt;10%.
mteval-v13a.pl script to compute case-insensitive
BLEU scores.
</figureCaption>
<subsectionHeader confidence="0.986651">
5.2 Domain-adapted benchmark systems
</subsectionHeader>
<bodyText confidence="0.999994722222222">
As domain-aware benchmark systems, we use
the linear mixture model (DOMAIN1) of Sen-
nrich (2012) and the phrase table fill-up method
(DOMAIN2) of Bisazza et al. (2011) (both avail-
able in the Moses toolkit). For both systems,
the domain labels of the documents are used to
group documents of the same domain together. We
build adapted tables for each domain by treating
the remaining documents as out-of-domain data
and combining in-domain with out-of-domain ta-
bles. For development and test, the domain labels
are used to select the respective domain-adapted
model for decoding. Both systems have an advan-
tage over our model because of their knowledge
of domain boundaries in the data. This allows for
much more confident lexical choices than using an
unadapted system but is not possible without prior
knowledge about each document.
</bodyText>
<subsectionHeader confidence="0.999076">
5.3 Implementation of similarity features
</subsectionHeader>
<bodyText confidence="0.99994475">
After all topic vectors have been computed, a fea-
ture generation step precomputes the similarity
features for all pairs of test contexts and applica-
ble phrase pairs for translating source phrases in
a test instance. The phrase table of the baseline
model is filtered for every test instance (a sentence
or document, depending on the context setting)
and each entry is augmented with features that ex-
press its semantic similarity to the test context. We
use a wrapper around the Moses decoder to reload
the phrase table for each test instance, which en-
ables us to run parameter optimisation (PRO) in
the usual way to get one set of tuned weights for
all test sentences. It would be conceivable to use
topic-specific weights instead of one set of global
weights, but this is not the focus of this work.
</bodyText>
<sectionHeader confidence="0.9688195" genericHeader="method">
6 Qualitative evaluation of phrase pair
topic distributions
</sectionHeader>
<bodyText confidence="0.999956620689655">
In order to verify that the topic model is learning
useful topic representations for phrase pairs, we
inspect the inferred topic distributions for three
phrase pairs where the translation of the same
source word differs depending on the topical
context: noyau —* kernel, noyau —* nucleus
and noyau —* core. Figure 3 shows the topic
distributions for a PPT model with 20 topics
(with topic 0 removed) and highlights the most
prominent topics with labels describing their
content (politics, IT, science, economy)6. The
most peaked topic distribution was learned for
the phrase pair noyau —* kernel which would be
expected to occur mostly in an IT context and
the topic with the largest probability mass is in
fact related to IT. The most prominent topic for
the phrase pair noyau —* nucleus is the science
topic, though it seems to be occurring in with the
political topic as well. The phrase pair noyau
—* core was assigned the most ambiguous topic
distribution with peaks at the politics, economy
and IT topics. Note also that its topic distribution
overlaps with those of the other translations, for
example, like the phrase pair noyau —* kernel,
it can occur in IT contexts. This shows that the
model captures the fact that even within a given
topic there can still be ambiguity about the correct
translation (both target phrases kernel and core
are plausible translations in an IT context).
</bodyText>
<footnote confidence="0.975231">
6Topic labels were assigned by inspecting the most prob-
able context words for each topic according to the model.
</footnote>
<page confidence="0.996418">
450
</page>
<bodyText confidence="0.978985789473684">
Ambiguity of phrase pair topic vectors
The examples in the previous section show that
the level of ambiguity differs between phrase pairs
that constitute translations of the same source
phrase. It is worth noting that introducing bilin-
gual information into topic modelling reduces the
sense ambiguity present in monolingual text by
preserving only the intersection of the senses of
source and target phrases. For example, the distri-
butional profiles of the source phrase noyau would
contain words that belong to the senses IT, poli-
tics, science and economy, while the words in the
context of the target phrase kernel can belong to
the senses IT and food (with source context words
such as grain, prot´eines, produire). Thus, the
monolingual representations would still contain a
relatively high level of ambiguity while the distri-
butional profile of the phrase pair noyau → kernel
preserves only the IT sense.
</bodyText>
<sectionHeader confidence="0.999032" genericHeader="evaluation">
7 Results and discussion
</sectionHeader>
<bodyText confidence="0.999396142857143">
In this section we present experimental results
of our model with different context settings and
against different baselines. We used bootstrap re-
sampling (Koehn, 2004) to measure significance
on the mixed test set and marked all statistically
significant results compared to the respective base-
lines with asterisk (*: p ≤ 0.01).
</bodyText>
<subsectionHeader confidence="0.99701">
7.1 Local context
</subsectionHeader>
<bodyText confidence="0.999960230769231">
In Table 3 we compare the results of the con-
catenation baseline and a model containing the
phrSim-local feature in addition to the baseline
features, for different numbers of latent topics. We
show results for the mixed test set containing doc-
uments from all three domains as well as the in-
dividual results on the documents from each do-
main. While all topic settings yield improvements
over the baseline, the largest improvement on the
mixed test set (+0.48 BLEU) is achieved with 50
topics. Topic adaptation is most effective on the
TED portion of the test set where the increase in
BLEU is 0.59.
</bodyText>
<subsectionHeader confidence="0.995686">
7.2 Global context
</subsectionHeader>
<bodyText confidence="0.999484">
Table 4 shows the results of the baseline plus the
phrSim-global feature that takes into account the
whole document context of a test sentence. While
the largest overall improvement on the mixed test
set is equal to the improvement of the local feature,
there are differences in performance for the indi-
vidual domains. For Commoncrawl documents,
</bodyText>
<table confidence="0.999023428571429">
Model Mixed CC NC TED
Baseline 26.86 19.61 29.42 31.88
10 topics *27.15 19.87 29.63 32.36
20 topics *27.19 19.92 29.76 32.31
50 topics *27.34 20.13 29.70 32.47
100 topics *27.26 20.02 29.75 32.40
&gt;Baseline +0.48 +0.52 +0.34 +0.59
</table>
<tableCaption confidence="0.690250333333333">
Table 3: BLEU scores of baseline system +
phrSim-local feature for different numbers of top-
ics.
</tableCaption>
<bodyText confidence="0.999555333333333">
the results vary slightly but the largest improve-
ment is still achieved with 50 topics and is al-
most the same for both. For News Commentary,
the scores with the local feature are consistently
higher than the scores with the global feature (0.20
and 0.22 BLEU higher for 20 and 50 topics). For
TED, the trend is opposite with the global feature
performing better than the local feature for all top-
ics (0.28 and 0.40 BLEU higher for 10 and 20 top-
ics). The best improvement over the baseline for
TED is 0.83 BLEU, which is higher than the im-
provement with the local feature.
</bodyText>
<table confidence="0.998768857142857">
Model Mixed CC NC TED
Baseline 26.86 19.61 29.42 31.88
10 topics *27.30 20.01 29.61 32.64
20 topics *27.34 20.07 29.56 32.71
50 topics *27.27 20.12 29.48 32.55
100 topics *27.24 19.95 29.66 32.52
&gt;Baseline +0.48 +0.51 +0.24 +0.83
</table>
<tableCaption confidence="0.918992666666667">
Table 4: BLEU scores of baseline system +
phrSim-global feature for different numbers of
topics.
</tableCaption>
<subsectionHeader confidence="0.998089">
7.3 Relation to properties of test documents
</subsectionHeader>
<bodyText confidence="0.999995769230769">
To make these results more interpretable, Ta-
ble 5 lists some of the properties of the test doc-
uments per domain. Of the three domains, CC
has the shortest documents on average and TED
the longest. To understand how this affects topic
inference, we measure topical drift as the aver-
age divergence (cosine distance) of the local topic
distributions for each test sentence to the global
topic distribution of their surrounding document.
There seems to be a correlation between docu-
ment length and topical drift, with CC documents
showing the least topical drift and TED documents
showing the most. This makes sense intuitively
</bodyText>
<page confidence="0.997717">
451
</page>
<bodyText confidence="0.999676193548387">
because the longer a document is, the more likely
it is that the content of a given sentence diverges
from the overall topical structure of the document.
While this can explain why for CC documents us-
ing local or global context results in similar perfor-
mance, it does not explain the better performance
of the local feature for NC documents. The last
row of Table 5 shows that sentences in the NC
documents are on average the longest and longer
sentences would be expected to yield more reli-
able topic estimates than shorter sentences. Thus,
we assume that local context yields better perfor-
mance for NC because on average the sentences
are long enough to yield reliable topic estimates.
When local context provides reliable information,
it may be more informative than global context be-
cause it can be more specific.
For TED, we see the largest topical drift per
document, which could lead us to believe that the
document topic mixtures do not reflect the topical
content of the sentences too well. But considering
that the sentences are on average shorter than for
the other two domains, it is more likely that the
local context in TED documents can be unreliable
when the sentences are too short. TED documents
contain transcribed speech and are probably less
dense in terms of information content than News
commentary documents. Therefore, the global
context may be more informative for TED which
could explain why relying on the global topic
mixtures yields better results.
</bodyText>
<table confidence="0.9992544">
Property CC NC TED
Per document
Avg number of sentences 29.1 60.6 78.9
Avg topical divergence 0.35 0.43 0.49
Avg sentence length 26.2 31.5 21.7
</table>
<tableCaption confidence="0.973069">
Table 5: Properties of test documents per domain.
</tableCaption>
<bodyText confidence="0.595229666666667">
Average topical divergence is defined as the aver-
age cosine distance of local to global topic distri-
butions in a document.
</bodyText>
<subsectionHeader confidence="0.999725">
7.4 Combinations of local and global context
</subsectionHeader>
<bodyText confidence="0.999915857142857">
In Table 6 we compare a system that already con-
tains the global feature from a model with 50 top-
ics to the combinations of local and global simi-
larity features described in Section 4.
Of the four combinations, the additive combi-
nation of topic vectors (®) yields the largest im-
provement over the baseline with +0.63 BLEU on
</bodyText>
<table confidence="0.999190625">
Model Mixed CC NC TED
Baseline 26.86 19.61 29.42 31.88
+ global 27.27 20.12 29.48 32.55
+ local *27.43 20.18 29.65 32.79
® local *27.49 20.30 29.66 32.76
⊗ local 27.34 20.24 29.61 32.50
(9 local *27.45 20.22 29.51 32.79
® &gt;BL +0.63 +0.69 +0.24 +0.88
</table>
<tableCaption confidence="0.987495">
Table 6: BLEU scores of baseline and combina-
</tableCaption>
<bodyText confidence="0.9590787">
tions of phrase pair similarity features with local
and global context (significance compared to base-
line+global). All models were trained with 50 top-
ics.
the mixed test set and +0.88 BLEU on TED. The
improvements of the combined model are larger
than the improvements for each context on its own,
with the only exception being the NC portion of
the test set where the improvement is not larger
than using just the local context. A possible reason
is that when one feature is consistently better for
one of the domains (local context for NC), the log-
linear combination of both features (tuned on data
from all domains) would result in a weaker overall
model for that domain. However, if both features
encode similar information, as we assume to be the
case for CC documents, the presence of both fea-
tures would reinforce the preference of each and
result in equal or better performance. For the ad-
ditive combination, we expect a similar effect be-
cause adding together two topics vectors that have
peaks at different topics would make the resulting
topic vector less peaked than either of the original
vectors.
The additive topic vector combination is
slightly better than the log-linear feature combina-
tion, though the difference is small. Nevertheless,
it shows that combining topic vectors before com-
puting similarity features is a viable alternative
to log-linear combination, with the potential to
design more expressive combination functions.
The multiplicative combination performs slightly
worse than the additive combination, which
suggests that the information provided by the two
contexts is not always in agreement. In some
cases, the global context may be more reliable
while in other cases the local context may have
more accurate topic estimates and a voting ap-
proach does not take advantage of complementary
information. The combination of topic vectors
</bodyText>
<page confidence="0.997652">
452
</page>
<tableCaption confidence="0.857117375">
Source: Le noyau contient de nombreux pilotes, afin de fonctionner chez la plupart des utilisateurs.
Reference: The precompiled kernel includes a lot of drivers, in order to work for most users.
Source: Il est prudent de consulter les pages de manuel ou les faq sp´ecifiques a` votre os.
Reference: It’s best to consult the man pages or faqs for your os.
Source: Nous fournissons nano (un petit ´editeur), vim (vi am´elior´e), qemacs (clone de emacs), elvis, joe .
Reference: Nano (a lightweight editor), vim (vi improved), qemacs (emacs clone), elvis and joe.
Source: Elle a introduit des politiques [..] a` cot´e des relations de gouvernement a` gouvernement traditionnelles.
Reference: She has introduced policies [..] alongside traditional government-to-government relations.
</tableCaption>
<figureCaption confidence="0.9972905">
Figure 4: Examples of test sentences and reference translations with the ambiguous source words and
their translations in bold.
</figureCaption>
<bodyText confidence="0.999220571428571">
depending on sentence length (0*) performs well
for CC and TED but less well for NC where we
would expect that it helps to prefer the local
information. This indicates that the rather ad-
hoc way in which we encoded dependency on
the sentence length may need further refinement
to make better use of the local context information.
</bodyText>
<table confidence="0.992411">
Model noyau --+ os --+
Baseline nucleus bones
global kernel* os*
local nucleus bones
global®local kernel* os*
</table>
<tableCaption confidence="0.982431">
Table 7: Translations of ambiguous source words
</tableCaption>
<bodyText confidence="0.6998695">
where global context yields the correct translation
(* denotes the correct translation).
</bodyText>
<table confidence="0.8053176">
Model elvis --+ relations --+
Baseline elvis* relations*
global the king relationship
local elvis* relations*
global®local the king relations*
</table>
<tableCaption confidence="0.815434">
Table 8: Translations of ambiguous source words
</tableCaption>
<bodyText confidence="0.9807615">
where local context yields the correct translation
(* denotes the correct translation).
</bodyText>
<subsectionHeader confidence="0.998609">
7.5 Effect of contexts on translation
</subsectionHeader>
<bodyText confidence="0.9999972">
To give an intuition of how lexical selection is af-
fected by contextual information, Figure 4 shows
four test sentences with an ambiguous source word
and its translation in bold. The corresponding
translations with the baseline, the global and lo-
cal similarity features and the additive combina-
tion are shown in Table 7 for the first two examples
where the global context yields the correct transla-
tion (as indicated by *) and in Table 8 for the last
two examples where the local context yields the
correct translation.7 In Table 7, the additive com-
bination preserves the choice of the global model
and yields the correct translations, while in Table 8
only the second example is translated correctly by
the combined model. A possible explanation is
that the topical signal from the global context is
stronger and results in more discriminative simi-
larity values. In that case, the preference of the
global context would be likely to have a larger in-
fluence on the similarity values in the combined
model. A useful extension could be to try to de-
tect for a given test instance which context pro-
vides more reliable information (beyond encoding
sentence length) and boost the topic distribution
from that context in the combination.
</bodyText>
<subsectionHeader confidence="0.997162">
7.6 Comparison with domain adaptation
</subsectionHeader>
<bodyText confidence="0.999953555555556">
Table 9 compares the additive model (®) to the
two domain-adapted systems that know the do-
main label of each document during training and
test. Our topic-adapted model yields overall com-
petitive performance with improvements of +0.37
and +0.25 BLEU on the mixed test set, respec-
tively. While it yields slightly lower performance
on the NC documents, it achieves equal perfor-
mance on TED documents and improves by up to
+0.94 BLEU on Commoncrawl documents. This
can be explained by the fact that Commoncrawl is
the most diverse of the three domains with docu-
ments crawled from all over web, thus we expect
topic adaptation to be most effective in compari-
son to domain adaptation in this scenario. Our dy-
namic approach allows us to adapt the similarity
features to each test sentence and test document
individually and is therefore more flexible than
</bodyText>
<footnote confidence="0.9961545">
7For these examples, the local model happens to yield the
same translations as the baseline model.
</footnote>
<page confidence="0.994902">
453
</page>
<table confidence="0.9988405">
Type of adaptation Model Mixed CC NC TED
Domain-adapted DOMAIN1 27.24 19.61 29.87 32.73
DOMAIN2 27.12 19.36 29.78 32.71
Topic-adapted global ® local *27.49 20.30 29.66 32.76
&gt;DOMAIN1 +0.25 +0.69 -0.21 +0.03
&gt;DOMAIN2 +0.37 +0.94 -0.12 +0.05
</table>
<tableCaption confidence="0.9414105">
Table 9: BLEU scores of translation model using similarity features derived from PPT model (50 topics)
in comparison with two (supervised) domain-adapted systems.
</tableCaption>
<table confidence="0.999683166666667">
Model Mixed CC NC TED
Baseline 26.86 19.61 29.42 31.88
+ docSim 27.22 20.11 29.63 32.40
+ phrSim-global ® phrSim-local *27.58 20.34 29.71 32.96
+ phrSim-global O phrSim-local *27.60 20.35 29.70 33.03
global0local&gt;BL +0.74 +0.74 +0.38 +1.15
</table>
<tableCaption confidence="0.9691645">
Table 10: BLEU scores of baseline, baseline + document similarity feature and additional phrase pair
similarity features (significance compared to baseline+docSim). All models were trained with 50 topics.
</tableCaption>
<bodyText confidence="0.963516">
cross-domain adaptation approaches while requir-
ing no information about the domain of a test in-
stance.
</bodyText>
<subsectionHeader confidence="0.991239">
7.7 Combination with an additional
document similarity feature
</subsectionHeader>
<bodyText confidence="0.99997655">
To find out whether similarity features derived
from different types of topic models can provide
complementary information, we add the phrSim
features to a system that already includes a docu-
ment similarity feature (docSim) derived from the
pLDA model (Hasler et al., 2014) which learns
topic distributions at the document level and uses
phrases instead of words as the minimal units. The
results are shown in Table 10. Adding the two
best combinations of local and global context from
Table 6 yields the best results on TED documents
with an increase of 0.63 BLEU over the baseline +
docSim model and 1.15 BLEU over the baseline.
On the mixed test set, the improvement is 0.38
BLEU over the baseline + docSim model and 0.74
BLEU over the baseline. Thus, we show that com-
bining different scopes and granularities of sim-
ilarity features consistently improves translation
results and yields larger gains than using each of
the similarity features alone.
</bodyText>
<sectionHeader confidence="0.998306" genericHeader="conclusions">
8 Conclusion
</sectionHeader>
<bodyText confidence="0.9999295">
We have presented a new topic model for dynamic
adaptation of machine translation systems that
learns topic distributions for phrase pairs. These
latent topic representations can be compared to la-
tent representations of local or global test contexts
and integrated into the translation model via simi-
larity features.
Our experimental results show that it is ben-
eficial for adaptation to use contextual informa-
tion from both local and global contexts, with
BLEU improvements of up to 1.15 over the base-
line system on TED documents and 0.74 on a
large mixed test set with documents from three do-
mains. Among four different combinations of lo-
cal and global information, we found that the ad-
ditive combination of topic vectors performs best.
We conclude that information from both contexts
should be combined to correct potential topic de-
tection errors in either of the two contexts. We
also show that our dynamic adaptation approach
performs competitively in comparison with two
supervised domain-adapted systems and that the
largest improvement is achieved for the most di-
verse portion of the test set.
In future work, we would like to experiment
with more compact distributional profiles to speed
up inference and explore the possibilities of de-
riving probabilistic translation features from the
PPT model as an extension to the current model.
Another avenue for future work could be to com-
bine contextual information that captures different
types of information, for example, to distinguish
between semantic and syntactic aspects in the lo-
cal context.
</bodyText>
<page confidence="0.998963">
454
</page>
<sectionHeader confidence="0.997379" genericHeader="acknowledgments">
Acknowledgements
</sectionHeader>
<bodyText confidence="0.999530555555556">
This work was supported by funding from the
Scottish Informatics and Computer Science Al-
liance (Eva Hasler) and funding from the Eu-
ropean Union Seventh Framework Programme
(FP7/2007-2013) under grant agreement 287658
(EU BRIDGE) and grant agreement 288769 (AC-
CEPT). Thanks to Annie Louis for helpful com-
ments on a draft of this paper and thanks to the
anonymous reviewers for their useful feedback.
</bodyText>
<sectionHeader confidence="0.999163" genericHeader="references">
References
</sectionHeader>
<reference confidence="0.999852680851064">
Rafael E Banchs and Marta R Costa-juss`a. 2011. A Se-
mantic Feature for Statistical Machine Translation.
In SSST-5 Proceedings of the Fifth Workshop on Syn-
tax, Semantics and Structure in Statistical Transla-
tion, pages 126–134.
Arianna Bisazza, Nick Ruiz, and Marcello Federico.
2011. Fill-up versus Interpolation Methods for
Phrase-based SMT Adaptation. In Proceedings of
IWSLT.
Ondrej Bojar, Christian Buck, Chris Callison-Burch,
Christian Federmann, Barry Haddow, Philipp
Koehn, Christof Monz, Matt Post, Radu Soricut, and
Lucia Specia. 2013. Findings of the 2013 workshop
on statistical machine translation. In Proceedings of
WMT 2013.
Jordan Boyd-graber and David Blei. 2007. A Topic
Model for Word Sense Disambiguation. In Proceed-
ings of EMNLP-CoNLL, pages 1024–1033.
Jun Fu Cai, Wee Sun Lee, and Yee Whye Teh. 2007.
Improving Word Sense Disambiguation Using Topic
Features. In Proceedings of EMNLP, pages 1015–
1023.
Marine Carpuat and Dekai Wu. 2007a. How Phrase
Sense Disambiguation outperforms Word Sense Dis-
ambiguation for Statistical Machine Translation.
In International Conference on Theoretical and
Methodological Issues in MT.
Marine Carpuat and Dekai Wu. 2007b. Improving Sta-
tistical Machine Translation using Word Sense Dis-
ambiguation. In Proceedings of EMNLP, pages 61–
72.
Marine Carpuat. 2009. One Translation per Discourse.
In Proceedings of the NAACL HLT Workshop on Se-
mantic Evaluations: Recent Achievements and Fu-
ture Directions, pages 19–27.
Mauro Cettolo, Christian Girardi, and Marcello Fed-
erico. 2012. WIT3: Web Inventory of Transcribed
and Translated Talks. In Proceedings of EAMT.
Yee Seng Chan, Hwee Tou Ng, and David Chiang.
2007. Word Sense Disambiguation Improves Statis-
tical Machine Translation. In Proceedings of ACL.
Boxing Chen, Roland Kuhn, and George Foster. 2013.
Vector Space Model for Adaptation in Statistical
Machine Translation. In Proceedings of ACL, pages
1285–1293.
Marta R. Costa-juss`a and Rafael E. Banchs. 2010. A
vector-space dynamic feature for phrase-based sta-
tistical machine translation. Journal of Intelligent
Information Systems, 37(2):139–154, August.
Georgiana Dinu and Mirella Lapata. 2010. Measuring
Distributional Similarity in Context. In Proceedings
of EMNLP, pages 1162–1172.
Vladimir Eidelman, Jordan Boyd-Graber, and Philip
Resnik. 2012. Topic Models for Dynamic Trans-
lation Model Adaptation. In Proceedings of ACL.
William A Gale, Kenneth W Church, and David
Yarowsky. 1992. One Sense Per Discourse. In
Proceedings of the workshop on Speech and Natu-
ral Language.
Zellig Harris. 1954. Distributional structure. Word,
10(23):146–162.
Eva Hasler, Barry Haddow, and Philipp Koehn. 2012.
Sparse Lexicalised Features and Topic Adaptation
for SMT. In Proceedings of IWSLT.
Eva Hasler, Phil Blunsom, Philipp Koehn, and Barry
Haddow. 2014. Dynamic Topic Adaptation for
Phrase-based MT. In Proceedings of the 14th Con-
ference of the European Chapter of the Association
for Computational Linguistics, Gothenburg, Swe-
den.
Sanjika Hewavitharana, Dennis N Mehay, and
Sankaranarayanan Ananthakrishnan. 2013. Incre-
mental Topic-Based Translation Model Adaptation
for Conversational Spoken Language Translation.
In Proceedings ofACL, pages 697–701.
Mark Hopkins and Jonathan May. 2011. Tuning as
ranking. In Proceedings of the Conference on Em-
pirical Methods in Natural Language Processing,
Edinburgh, United Kingdom.
Eric H Huang, Richard Socher, Christopher D Man-
ning, and Andrew Y Ng. 2012. Improving Word
Representations via Global Context and Multiple
Word Prototypes. In Proceedings of ACL, pages
873–882.
Philipp Koehn, Hieu Hoang, Alexandra Birch, Chris
Callison-Burch, Marcello Federico, Nicola Bertoldi,
Brooke Cowan, Wade Shen, Christine Moran,
Richard Zens, Chris Dyer, Ondrej Bojar, Alexandra
Constantin, and Evan Herbst. 2007. Moses: Open
source toolkit for SMT. In Proceedings of ACL:
Demo and poster sessions.
Philipp Koehn. 2004. Statistical Significance Tests
for Machine Translation Evaluation. In Proc. of
EMNLP.
</reference>
<page confidence="0.987047">
455
</page>
<reference confidence="0.998373235294118">
Linlin Li, Benjamin Roth, and Caroline Sporleder.
2010. Topic Models for Word Sense Disambigua-
tion and Token-based Idiom Detection. In Proceed-
ings ofACL, pages 1138–1147.
Rico Sennrich. 2012. Perplexity Minimization for
Translation Model Domain Adaptation in Statistical
Machine Translation. In Proceedings of EACL.
Yik-Cheung Tam, Ian Lane, and Tanja Schultz. 2008.
Bilingual LSA-based adaptation for statistical ma-
chine translation. Machine Translation, 21(4):187–
207, November.
Yee Whye Teh, David Newman, and Max Welling.
2006. A collapsed variational Bayesian inference
algorithm for LDA. In Proceedings of NIPS.
B Zhao and E P Xing. 2007. HM-BiTAM: Bilingual
topic exploration, word alignment, and translation.
Neural Information Processing.
</reference>
<page confidence="0.999073">
456
</page>
</variant>
</algorithm>
<algorithm name="ParsHed" version="110505">
<variant no="0" confidence="0.334629">
<title confidence="0.999236">Dynamic Topic Adaptation for SMT using Distributional Profiles</title>
<author confidence="0.997477">Barry Philipp</author>
<affiliation confidence="0.562402">of Informatics, University of</affiliation>
<title confidence="0.60021">for Language and Speech Processing, Johns Hopkins</title>
<abstract confidence="0.998883428571429">Despite its potential to improve lexical selection, most state-of-the-art machine translation systems take only minimal contextual information into account. We capture context with a topic model over distributional profiles built from the context words of each translation unit. Topic distributions are inferred for each translation unit and used to adapt the translation model dynamically to a given test context by measuring their similarity. We show that combining information from both local and global test contexts helps to improve lexical selection and outperforms a system by up to 1.15 We test our topic-adapted model on a diverse data set containing documents from three different domains and achieve competitive performance in comparison with two supervised domain-adapted systems.</abstract>
</variant>
</algorithm>
<algorithm name="ParsCit" version="110505">
<citationList>
<citation valid="true">
<authors>
<author>Rafael E Banchs</author>
<author>Marta R Costa-juss`a</author>
</authors>
<title>A Semantic Feature for Statistical Machine Translation. In</title>
<date>2011</date>
<booktitle>SSST-5 Proceedings of the Fifth Workshop on Syntax, Semantics and Structure in Statistical Translation,</booktitle>
<pages>126--134</pages>
<marker>Banchs, Costa-juss`a, 2011</marker>
<rawString>Rafael E Banchs and Marta R Costa-juss`a. 2011. A Semantic Feature for Statistical Machine Translation. In SSST-5 Proceedings of the Fifth Workshop on Syntax, Semantics and Structure in Statistical Translation, pages 126–134.</rawString>
</citation>
<citation valid="true">
<authors>
<author>Arianna Bisazza</author>
<author>Nick Ruiz</author>
<author>Marcello Federico</author>
</authors>
<title>Fill-up versus Interpolation Methods for Phrase-based SMT Adaptation.</title>
<date>2011</date>
<booktitle>In Proceedings of IWSLT.</booktitle>
<contexts>
<context position="20902" citStr="Bisazza et al. (2011)" startWordPosition="3466" endWordPosition="3469">the average feature weights of three optimisation runs with PRO (Hopkins and May, 2011). We use the 449 noyau → kernel noyau → nucleus noyau → core Figure 3: Topic distributions for source phrase noyau and three of its translations (20 topics without topic 0). Colored bars correspond to topics IT, politics, science, economy with topic proportions &gt;10%. mteval-v13a.pl script to compute case-insensitive BLEU scores. 5.2 Domain-adapted benchmark systems As domain-aware benchmark systems, we use the linear mixture model (DOMAIN1) of Sennrich (2012) and the phrase table fill-up method (DOMAIN2) of Bisazza et al. (2011) (both available in the Moses toolkit). For both systems, the domain labels of the documents are used to group documents of the same domain together. We build adapted tables for each domain by treating the remaining documents as out-of-domain data and combining in-domain with out-of-domain tables. For development and test, the domain labels are used to select the respective domain-adapted model for decoding. Both systems have an advantage over our model because of their knowledge of domain boundaries in the data. This allows for much more confident lexical choices than using an unadapted syste</context>
</contexts>
<marker>Bisazza, Ruiz, Federico, 2011</marker>
<rawString>Arianna Bisazza, Nick Ruiz, and Marcello Federico. 2011. Fill-up versus Interpolation Methods for Phrase-based SMT Adaptation. In Proceedings of IWSLT.</rawString>
</citation>
<citation valid="true">
<authors>
<author>Ondrej Bojar</author>
<author>Christian Buck</author>
<author>Chris Callison-Burch</author>
<author>Christian Federmann</author>
<author>Barry Haddow</author>
<author>Philipp Koehn</author>
<author>Christof Monz</author>
<author>Matt Post</author>
<author>Radu Soricut</author>
<author>Lucia Specia</author>
</authors>
<title>Findings of the 2013 workshop on statistical machine translation.</title>
<date>2013</date>
<booktitle>In Proceedings of WMT</booktitle>
<contexts>
<context position="17565" citStr="Bojar et al., 2013" startWordPosition="2899" endWordPosition="2902">y features derived from the PPT model with a document similarity feature from the pLDA model described in Hasler et al. (2014). The motivation is that their model learns topic mixtures for documents and uses phrases instead of words to infer the topical context. Therefore, it might provide additional information to our similarity features. 5 Data and experimental setup Our experiments were carried out on a mixed French-English data set containing the TED corpus (Cettolo et al., 2012), parts of the News Commentary corpus (NC) and parts of the Commoncrawl corpus (CC) from the WMT13 shared task (Bojar et al., 2013) as described in Table 1. To ensure that the baseline model does not have an implicit preference for any particular domain, we selected subsets of the NC and CC corpora such that the training data contains 2.7M English words per domain. We were guided by two constraints in chosing our data set in order to simulate an environment where very diverse documents have to be translated, which is a typical scenario for web translation engines: 1) the data has document boundaries and the content of each document is assumed to be topically related, 2) there is some degree of topical variation within eac</context>
</contexts>
<marker>Bojar, Buck, Callison-Burch, Federmann, Haddow, Koehn, Monz, Post, Soricut, Specia, 2013</marker>
<rawString>Ondrej Bojar, Christian Buck, Chris Callison-Burch, Christian Federmann, Barry Haddow, Philipp Koehn, Christof Monz, Matt Post, Radu Soricut, and Lucia Specia. 2013. Findings of the 2013 workshop on statistical machine translation. In Proceedings of WMT 2013.</rawString>
</citation>
<citation valid="true">
<authors>
<author>Jordan Boyd-graber</author>
<author>David Blei</author>
</authors>
<title>A Topic Model for Word Sense Disambiguation.</title>
<date>2007</date>
<booktitle>In Proceedings of EMNLP-CoNLL,</booktitle>
<pages>1024--1033</pages>
<contexts>
<context position="4236" citStr="Boyd-graber and Blei (2007)" startWordPosition="665" endWordPosition="668">xt and show that using clues from both levels of contexts is beneficial for translation model adaptation. We evaluate our model on a dynamic adaptation task where the domain of a test document is unknown and hence the problem of lexical selection is harder. 445 Proceedings of the Ninth Workshop on Statistical Machine Translation, pages 445–456, Baltimore, Maryland USA, June 26–27, 2014. c�2014 Association for Computational Linguistics 2 Related work Most work in the WSD literature has modelled disambiguation using a limited window of context around the word to disambiguate. Cai et al. (2007), Boyd-graber and Blei (2007) and Li et al. (2010) further tried to integrate the notion of latent topics to address the sparsity problem of the lexicalised features typically used in WSD classifiers. The most closely related work in the area of sense disambiguation is by Dinu and Lapata (2010) who propose a disambiguation method for solving lexical similarity and substitution tasks. They measure word similarity in context by learning distributions over senses for each target word in the form of lower-dimensional distributional representations. Before computing word similarities, they contextualise the global sense distri</context>
</contexts>
<marker>Boyd-graber, Blei, 2007</marker>
<rawString>Jordan Boyd-graber and David Blei. 2007. A Topic Model for Word Sense Disambiguation. In Proceedings of EMNLP-CoNLL, pages 1024–1033.</rawString>
</citation>
<citation valid="true">
<authors>
<author>Jun Fu Cai</author>
<author>Wee Sun Lee</author>
<author>Yee Whye Teh</author>
</authors>
<title>Improving Word Sense Disambiguation Using Topic Features.</title>
<date>2007</date>
<booktitle>In Proceedings of EMNLP,</booktitle>
<pages>1015--1023</pages>
<contexts>
<context position="4207" citStr="Cai et al. (2007)" startWordPosition="661" endWordPosition="664">lobal topical context and show that using clues from both levels of contexts is beneficial for translation model adaptation. We evaluate our model on a dynamic adaptation task where the domain of a test document is unknown and hence the problem of lexical selection is harder. 445 Proceedings of the Ninth Workshop on Statistical Machine Translation, pages 445–456, Baltimore, Maryland USA, June 26–27, 2014. c�2014 Association for Computational Linguistics 2 Related work Most work in the WSD literature has modelled disambiguation using a limited window of context around the word to disambiguate. Cai et al. (2007), Boyd-graber and Blei (2007) and Li et al. (2010) further tried to integrate the notion of latent topics to address the sparsity problem of the lexicalised features typically used in WSD classifiers. The most closely related work in the area of sense disambiguation is by Dinu and Lapata (2010) who propose a disambiguation method for solving lexical similarity and substitution tasks. They measure word similarity in context by learning distributions over senses for each target word in the form of lower-dimensional distributional representations. Before computing word similarities, they contextu</context>
</contexts>
<marker>Cai, Lee, Teh, 2007</marker>
<rawString>Jun Fu Cai, Wee Sun Lee, and Yee Whye Teh. 2007. Improving Word Sense Disambiguation Using Topic Features. In Proceedings of EMNLP, pages 1015– 1023.</rawString>
</citation>
<citation valid="true">
<authors>
<author>Marine Carpuat</author>
<author>Dekai Wu</author>
</authors>
<title>How Phrase Sense Disambiguation outperforms Word Sense Disambiguation for Statistical Machine Translation.</title>
<date>2007</date>
<booktitle>In International Conference on Theoretical and Methodological Issues in MT.</booktitle>
<contexts>
<context position="1796" citStr="Carpuat and Wu, 2007" startWordPosition="274" endWordPosition="277">hine translation (SMT). It strongly depends on context and is particularly difficult when the domain of a test document is unknown, for example when translating web documents from diverse sources. Selecting translations of words or phrases that preserve the sense of the source words is closely related to the field of word sense disambiguation (WSD), which has been studied extensively in the past. Most approaches to WSD model context at the sentence level and do not take the wider context of a word into account. Some of the ideas from the field of WSD have been adapted for machine translation (Carpuat and Wu, 2007b; Carpuat and Wu, 2007a; Chan et al., 2007). For example, Carpuat and Wu (2007a) extend word sense disambiguation to phrase sense disambiguation and show improved performance due to the better fit with multiple possible segmentations in a phrasebased system. Carpuat (2009) test the “one sense per discourse” hypothesis (Gale et al., 1992) for MT and find that enforcing it as a constraint at the document level could potentially improve translation quality. Our goal is to make correct lexical choices in a given context without explicitly enforcing translation consistency. More recent work in SMT</context>
</contexts>
<marker>Carpuat, Wu, 2007</marker>
<rawString>Marine Carpuat and Dekai Wu. 2007a. How Phrase Sense Disambiguation outperforms Word Sense Disambiguation for Statistical Machine Translation. In International Conference on Theoretical and Methodological Issues in MT.</rawString>
</citation>
<citation valid="true">
<authors>
<author>Marine Carpuat</author>
<author>Dekai Wu</author>
</authors>
<title>Improving Statistical Machine Translation using Word Sense Disambiguation.</title>
<date>2007</date>
<booktitle>In Proceedings of EMNLP,</booktitle>
<pages>61--72</pages>
<contexts>
<context position="1796" citStr="Carpuat and Wu, 2007" startWordPosition="274" endWordPosition="277">hine translation (SMT). It strongly depends on context and is particularly difficult when the domain of a test document is unknown, for example when translating web documents from diverse sources. Selecting translations of words or phrases that preserve the sense of the source words is closely related to the field of word sense disambiguation (WSD), which has been studied extensively in the past. Most approaches to WSD model context at the sentence level and do not take the wider context of a word into account. Some of the ideas from the field of WSD have been adapted for machine translation (Carpuat and Wu, 2007b; Carpuat and Wu, 2007a; Chan et al., 2007). For example, Carpuat and Wu (2007a) extend word sense disambiguation to phrase sense disambiguation and show improved performance due to the better fit with multiple possible segmentations in a phrasebased system. Carpuat (2009) test the “one sense per discourse” hypothesis (Gale et al., 1992) for MT and find that enforcing it as a constraint at the document level could potentially improve translation quality. Our goal is to make correct lexical choices in a given context without explicitly enforcing translation consistency. More recent work in SMT</context>
</contexts>
<marker>Carpuat, Wu, 2007</marker>
<rawString>Marine Carpuat and Dekai Wu. 2007b. Improving Statistical Machine Translation using Word Sense Disambiguation. In Proceedings of EMNLP, pages 61– 72.</rawString>
</citation>
<citation valid="true">
<authors>
<author>Marine Carpuat</author>
</authors>
<title>One Translation per Discourse.</title>
<date>2009</date>
<booktitle>In Proceedings of the NAACL HLT Workshop on Semantic Evaluations: Recent Achievements and Future Directions,</booktitle>
<pages>pages</pages>
<contexts>
<context position="2070" citStr="Carpuat (2009)" startWordPosition="319" endWordPosition="320">rds is closely related to the field of word sense disambiguation (WSD), which has been studied extensively in the past. Most approaches to WSD model context at the sentence level and do not take the wider context of a word into account. Some of the ideas from the field of WSD have been adapted for machine translation (Carpuat and Wu, 2007b; Carpuat and Wu, 2007a; Chan et al., 2007). For example, Carpuat and Wu (2007a) extend word sense disambiguation to phrase sense disambiguation and show improved performance due to the better fit with multiple possible segmentations in a phrasebased system. Carpuat (2009) test the “one sense per discourse” hypothesis (Gale et al., 1992) for MT and find that enforcing it as a constraint at the document level could potentially improve translation quality. Our goal is to make correct lexical choices in a given context without explicitly enforcing translation consistency. More recent work in SMT uses latent representations of the document context to dynamically adapt the translation model with either monolingual topic models (Eidelman et al., 2012; Hewavitharana et al., 2013) or bilingual topic models (Hasler et al., 2014), thereby allowing the translation system </context>
</contexts>
<marker>Carpuat, 2009</marker>
<rawString>Marine Carpuat. 2009. One Translation per Discourse. In Proceedings of the NAACL HLT Workshop on Semantic Evaluations: Recent Achievements and Future Directions, pages 19–27.</rawString>
</citation>
<citation valid="true">
<authors>
<author>Mauro Cettolo</author>
<author>Christian Girardi</author>
<author>Marcello Federico</author>
</authors>
<title>WIT3: Web Inventory of Transcribed and Translated Talks.</title>
<date>2012</date>
<booktitle>In Proceedings of EAMT.</booktitle>
<contexts>
<context position="17434" citStr="Cettolo et al., 2012" startWordPosition="2874" endWordPosition="2877"> local topic mixture may be more reliable than the global mixture. We also experiment with a combination of the phrase pair similarity features derived from the PPT model with a document similarity feature from the pLDA model described in Hasler et al. (2014). The motivation is that their model learns topic mixtures for documents and uses phrases instead of words to infer the topical context. Therefore, it might provide additional information to our similarity features. 5 Data and experimental setup Our experiments were carried out on a mixed French-English data set containing the TED corpus (Cettolo et al., 2012), parts of the News Commentary corpus (NC) and parts of the Commoncrawl corpus (CC) from the WMT13 shared task (Bojar et al., 2013) as described in Table 1. To ensure that the baseline model does not have an implicit preference for any particular domain, we selected subsets of the NC and CC corpora such that the training data contains 2.7M English words per domain. We were guided by two constraints in chosing our data set in order to simulate an environment where very diverse documents have to be translated, which is a typical scenario for web translation engines: 1) the data has document boun</context>
</contexts>
<marker>Cettolo, Girardi, Federico, 2012</marker>
<rawString>Mauro Cettolo, Christian Girardi, and Marcello Federico. 2012. WIT3: Web Inventory of Transcribed and Translated Talks. In Proceedings of EAMT.</rawString>
</citation>
<citation valid="true">
<authors>
<author>Yee Seng Chan</author>
<author>Hwee Tou Ng</author>
<author>David Chiang</author>
</authors>
<title>Word Sense Disambiguation Improves Statistical Machine Translation.</title>
<date>2007</date>
<booktitle>In Proceedings of ACL.</booktitle>
<contexts>
<context position="1840" citStr="Chan et al., 2007" startWordPosition="282" endWordPosition="285"> context and is particularly difficult when the domain of a test document is unknown, for example when translating web documents from diverse sources. Selecting translations of words or phrases that preserve the sense of the source words is closely related to the field of word sense disambiguation (WSD), which has been studied extensively in the past. Most approaches to WSD model context at the sentence level and do not take the wider context of a word into account. Some of the ideas from the field of WSD have been adapted for machine translation (Carpuat and Wu, 2007b; Carpuat and Wu, 2007a; Chan et al., 2007). For example, Carpuat and Wu (2007a) extend word sense disambiguation to phrase sense disambiguation and show improved performance due to the better fit with multiple possible segmentations in a phrasebased system. Carpuat (2009) test the “one sense per discourse” hypothesis (Gale et al., 1992) for MT and find that enforcing it as a constraint at the document level could potentially improve translation quality. Our goal is to make correct lexical choices in a given context without explicitly enforcing translation consistency. More recent work in SMT uses latent representations of the document</context>
</contexts>
<marker>Chan, Ng, Chiang, 2007</marker>
<rawString>Yee Seng Chan, Hwee Tou Ng, and David Chiang. 2007. Word Sense Disambiguation Improves Statistical Machine Translation. In Proceedings of ACL.</rawString>
</citation>
<citation valid="true">
<authors>
<author>Boxing Chen</author>
<author>Roland Kuhn</author>
<author>George Foster</author>
</authors>
<title>Vector Space Model for Adaptation in Statistical Machine Translation.</title>
<date>2013</date>
<booktitle>In Proceedings of ACL,</booktitle>
<pages>1285--1293</pages>
<contexts>
<context position="6185" citStr="Chen et al. (2013)" startWordPosition="979" endWordPosition="982">re learned over training sentences, they have to compare the current test sentence to the latent vector of every training instance associated with a translation unit. The highest similarity value is then used as a feature value. Instead, our model learns latent distributional representations of phrase pairs that can be directly compared to test contexts and are likely to be more robust. Because context words of a phrase pair are tied together in the distributional representations, we can use sparse priors to cluster context words associated with the same phrase pair into few topics. Recently, Chen et al. (2013) have proposed a vector space model for domain adaptation where phrase pairs are assigned vectors that are defined in terms of the training corpora. A similar vector is built for an in-domain development set and the similarity to the development set is used as a feature during translation. While their vector representations are similar to our latent topic representations, their model has no notion of structure beyond corpus boundaries and is adapted towards a single target domain (cross-domain). Instead, our model learns the latent topical structure automatically and the translation model is a</context>
</contexts>
<marker>Chen, Kuhn, Foster, 2013</marker>
<rawString>Boxing Chen, Roland Kuhn, and George Foster. 2013. Vector Space Model for Adaptation in Statistical Machine Translation. In Proceedings of ACL, pages 1285–1293.</rawString>
</citation>
<citation valid="true">
<authors>
<author>Marta R Costa-juss`a</author>
<author>Rafael E Banchs</author>
</authors>
<title>A vector-space dynamic feature for phrase-based statistical machine translation.</title>
<date>2010</date>
<journal>Journal of Intelligent Information Systems,</journal>
<volume>37</volume>
<issue>2</issue>
<marker>Costa-juss`a, Banchs, 2010</marker>
<rawString>Marta R. Costa-juss`a and Rafael E. Banchs. 2010. A vector-space dynamic feature for phrase-based statistical machine translation. Journal of Intelligent Information Systems, 37(2):139–154, August.</rawString>
</citation>
<citation valid="true">
<authors>
<author>Georgiana Dinu</author>
<author>Mirella Lapata</author>
</authors>
<title>Measuring Distributional Similarity in Context.</title>
<date>2010</date>
<booktitle>In Proceedings of EMNLP,</booktitle>
<pages>1162--1172</pages>
<contexts>
<context position="4502" citStr="Dinu and Lapata (2010)" startWordPosition="712" endWordPosition="715">gs of the Ninth Workshop on Statistical Machine Translation, pages 445–456, Baltimore, Maryland USA, June 26–27, 2014. c�2014 Association for Computational Linguistics 2 Related work Most work in the WSD literature has modelled disambiguation using a limited window of context around the word to disambiguate. Cai et al. (2007), Boyd-graber and Blei (2007) and Li et al. (2010) further tried to integrate the notion of latent topics to address the sparsity problem of the lexicalised features typically used in WSD classifiers. The most closely related work in the area of sense disambiguation is by Dinu and Lapata (2010) who propose a disambiguation method for solving lexical similarity and substitution tasks. They measure word similarity in context by learning distributions over senses for each target word in the form of lower-dimensional distributional representations. Before computing word similarities, they contextualise the global sense distribution of a word using the sense distribution of words in the test context, thereby shifting the sense distribution towards the test context. We adopt a similar distributional representation, but argue that our representation does not need this disambiguation step b</context>
</contexts>
<marker>Dinu, Lapata, 2010</marker>
<rawString>Georgiana Dinu and Mirella Lapata. 2010. Measuring Distributional Similarity in Context. In Proceedings of EMNLP, pages 1162–1172.</rawString>
</citation>
<citation valid="true">
<authors>
<author>Vladimir Eidelman</author>
<author>Jordan Boyd-Graber</author>
<author>Philip Resnik</author>
</authors>
<title>Topic Models for Dynamic Translation Model Adaptation.</title>
<date>2012</date>
<booktitle>In Proceedings of ACL.</booktitle>
<contexts>
<context position="2551" citStr="Eidelman et al., 2012" startWordPosition="395" endWordPosition="398">ambiguation and show improved performance due to the better fit with multiple possible segmentations in a phrasebased system. Carpuat (2009) test the “one sense per discourse” hypothesis (Gale et al., 1992) for MT and find that enforcing it as a constraint at the document level could potentially improve translation quality. Our goal is to make correct lexical choices in a given context without explicitly enforcing translation consistency. More recent work in SMT uses latent representations of the document context to dynamically adapt the translation model with either monolingual topic models (Eidelman et al., 2012; Hewavitharana et al., 2013) or bilingual topic models (Hasler et al., 2014), thereby allowing the translation system to disambiguate source phrases using document context. Eidelman et al. (2012) also apply a topic model to each test sentence and find that sentence context is sufficient for picking good translations, but they do not attempt to combine sentence and document level information. Sentence-level topic adaptation for SMT has also been employed by Hasler et al. (2012). Other approaches to topic adaptation for SMT include Zhao and Xing (2007) and Tam et al. (2008), both of which use a</context>
</contexts>
<marker>Eidelman, Boyd-Graber, Resnik, 2012</marker>
<rawString>Vladimir Eidelman, Jordan Boyd-Graber, and Philip Resnik. 2012. Topic Models for Dynamic Translation Model Adaptation. In Proceedings of ACL.</rawString>
</citation>
<citation valid="true">
<authors>
<author>William A Gale</author>
<author>Kenneth W Church</author>
<author>David Yarowsky</author>
</authors>
<title>One Sense Per Discourse.</title>
<date>1992</date>
<booktitle>In Proceedings of the workshop on Speech and Natural Language.</booktitle>
<contexts>
<context position="2136" citStr="Gale et al., 1992" startWordPosition="328" endWordPosition="331">on (WSD), which has been studied extensively in the past. Most approaches to WSD model context at the sentence level and do not take the wider context of a word into account. Some of the ideas from the field of WSD have been adapted for machine translation (Carpuat and Wu, 2007b; Carpuat and Wu, 2007a; Chan et al., 2007). For example, Carpuat and Wu (2007a) extend word sense disambiguation to phrase sense disambiguation and show improved performance due to the better fit with multiple possible segmentations in a phrasebased system. Carpuat (2009) test the “one sense per discourse” hypothesis (Gale et al., 1992) for MT and find that enforcing it as a constraint at the document level could potentially improve translation quality. Our goal is to make correct lexical choices in a given context without explicitly enforcing translation consistency. More recent work in SMT uses latent representations of the document context to dynamically adapt the translation model with either monolingual topic models (Eidelman et al., 2012; Hewavitharana et al., 2013) or bilingual topic models (Hasler et al., 2014), thereby allowing the translation system to disambiguate source phrases using document context. Eidelman et</context>
</contexts>
<marker>Gale, Church, Yarowsky, 1992</marker>
<rawString>William A Gale, Kenneth W Church, and David Yarowsky. 1992. One Sense Per Discourse. In Proceedings of the workshop on Speech and Natural Language.</rawString>
</citation>
<citation valid="true">
<authors>
<author>Zellig Harris</author>
</authors>
<date>1954</date>
<journal>Distributional structure. Word,</journal>
<volume>10</volume>
<issue>23</issue>
<contexts>
<context position="7713" citStr="Harris, 1954" startWordPosition="1240" endWordPosition="1241">sequence as well as those of words from the surrounding document by averaging over all word embeddings occurring in the same document. The score of the next word in a sequence is computed as the sum of the scores of both networks, but they do not consider alternative ways of combining contextual information. 3 Phrase pair topic model (PPT) Our proposed model aims to capture the relationship between phrase pairs and source words that frequently occur in the local context of a phrase pair, that is, context words occurring in the same sentence. It therefore follows the distributional hypothesis (Harris, 1954) which states that words that occur in the same contexts tend to have similar meanings. For a phrase pair, the idea is that words that occur frequently in its context are indicative of the sense that is captured by the target phrase translating the source phrase. We assume that all phrase pairs share a global set of topics and during topic inference the distribution over topics for each phrase pair is induced from the latent topic of its context words in the training data. In order to learn topic distributions for each phrase pair, we represent phrase pairs as documents containing all context </context>
</contexts>
<marker>Harris, 1954</marker>
<rawString>Zellig Harris. 1954. Distributional structure. Word, 10(23):146–162.</rawString>
</citation>
<citation valid="true">
<authors>
<author>Eva Hasler</author>
<author>Barry Haddow</author>
<author>Philipp Koehn</author>
</authors>
<title>Sparse Lexicalised Features and Topic Adaptation for SMT.</title>
<date>2012</date>
<booktitle>In Proceedings of IWSLT.</booktitle>
<contexts>
<context position="3033" citStr="Hasler et al. (2012)" startWordPosition="471" endWordPosition="474">esentations of the document context to dynamically adapt the translation model with either monolingual topic models (Eidelman et al., 2012; Hewavitharana et al., 2013) or bilingual topic models (Hasler et al., 2014), thereby allowing the translation system to disambiguate source phrases using document context. Eidelman et al. (2012) also apply a topic model to each test sentence and find that sentence context is sufficient for picking good translations, but they do not attempt to combine sentence and document level information. Sentence-level topic adaptation for SMT has also been employed by Hasler et al. (2012). Other approaches to topic adaptation for SMT include Zhao and Xing (2007) and Tam et al. (2008), both of which use adapted lexical weights. In this paper, we present a topic model that learns latent distributional representations of the context of a phrase pair which can be applied to both local and global contexts at test time. We introduce similarity features that compare latent representations of phrase pair types to test contexts to disambiguate senses for improved lexical selection. We also propose different strategies for combining local and global topical context and show that using c</context>
</contexts>
<marker>Hasler, Haddow, Koehn, 2012</marker>
<rawString>Eva Hasler, Barry Haddow, and Philipp Koehn. 2012. Sparse Lexicalised Features and Topic Adaptation for SMT. In Proceedings of IWSLT.</rawString>
</citation>
<citation valid="true">
<authors>
<author>Eva Hasler</author>
<author>Phil Blunsom</author>
<author>Philipp Koehn</author>
<author>Barry Haddow</author>
</authors>
<title>Dynamic Topic Adaptation for Phrase-based MT.</title>
<date>2014</date>
<booktitle>In Proceedings of the 14th Conference of the European Chapter of the Association for Computational Linguistics,</booktitle>
<location>Gothenburg,</location>
<contexts>
<context position="2628" citStr="Hasler et al., 2014" startWordPosition="407" endWordPosition="410">possible segmentations in a phrasebased system. Carpuat (2009) test the “one sense per discourse” hypothesis (Gale et al., 1992) for MT and find that enforcing it as a constraint at the document level could potentially improve translation quality. Our goal is to make correct lexical choices in a given context without explicitly enforcing translation consistency. More recent work in SMT uses latent representations of the document context to dynamically adapt the translation model with either monolingual topic models (Eidelman et al., 2012; Hewavitharana et al., 2013) or bilingual topic models (Hasler et al., 2014), thereby allowing the translation system to disambiguate source phrases using document context. Eidelman et al. (2012) also apply a topic model to each test sentence and find that sentence context is sufficient for picking good translations, but they do not attempt to combine sentence and document level information. Sentence-level topic adaptation for SMT has also been employed by Hasler et al. (2012). Other approaches to topic adaptation for SMT include Zhao and Xing (2007) and Tam et al. (2008), both of which use adapted lexical weights. In this paper, we present a topic model that learns l</context>
<context position="17072" citStr="Hasler et al. (2014)" startWordPosition="2815" endWordPosition="2818">ditive combination of topic vectors, ⊗ denotes the multiplicative combination of topic vectors and OO denotes a combination that favours the local context for longer sentences and backs off incrementally to the global context for shorter sentences.5 The intuition behind this combination is that if there is already sufficient evidence in the local context, the local topic mixture may be more reliable than the global mixture. We also experiment with a combination of the phrase pair similarity features derived from the PPT model with a document similarity feature from the pLDA model described in Hasler et al. (2014). The motivation is that their model learns topic mixtures for documents and uses phrases instead of words to infer the topical context. Therefore, it might provide additional information to our similarity features. 5 Data and experimental setup Our experiments were carried out on a mixed French-English data set containing the TED corpus (Cettolo et al., 2012), parts of the News Commentary corpus (NC) and parts of the Commoncrawl corpus (CC) from the WMT13 shared task (Bojar et al., 2013) as described in Table 1. To ensure that the baseline model does not have an implicit preference for any pa</context>
<context position="37768" citStr="Hasler et al., 2014" startWordPosition="6237" endWordPosition="6240"> baseline, baseline + document similarity feature and additional phrase pair similarity features (significance compared to baseline+docSim). All models were trained with 50 topics. cross-domain adaptation approaches while requiring no information about the domain of a test instance. 7.7 Combination with an additional document similarity feature To find out whether similarity features derived from different types of topic models can provide complementary information, we add the phrSim features to a system that already includes a document similarity feature (docSim) derived from the pLDA model (Hasler et al., 2014) which learns topic distributions at the document level and uses phrases instead of words as the minimal units. The results are shown in Table 10. Adding the two best combinations of local and global context from Table 6 yields the best results on TED documents with an increase of 0.63 BLEU over the baseline + docSim model and 1.15 BLEU over the baseline. On the mixed test set, the improvement is 0.38 BLEU over the baseline + docSim model and 0.74 BLEU over the baseline. Thus, we show that combining different scopes and granularities of similarity features consistently improves translation res</context>
</contexts>
<marker>Hasler, Blunsom, Koehn, Haddow, 2014</marker>
<rawString>Eva Hasler, Phil Blunsom, Philipp Koehn, and Barry Haddow. 2014. Dynamic Topic Adaptation for Phrase-based MT. In Proceedings of the 14th Conference of the European Chapter of the Association for Computational Linguistics, Gothenburg, Sweden.</rawString>
</citation>
<citation valid="true">
<authors>
<author>Sanjika Hewavitharana</author>
<author>Dennis N Mehay</author>
<author>Sankaranarayanan Ananthakrishnan</author>
</authors>
<title>Incremental Topic-Based Translation Model Adaptation for Conversational Spoken Language Translation.</title>
<date>2013</date>
<booktitle>In Proceedings ofACL,</booktitle>
<pages>697--701</pages>
<contexts>
<context position="2580" citStr="Hewavitharana et al., 2013" startWordPosition="399" endWordPosition="402">proved performance due to the better fit with multiple possible segmentations in a phrasebased system. Carpuat (2009) test the “one sense per discourse” hypothesis (Gale et al., 1992) for MT and find that enforcing it as a constraint at the document level could potentially improve translation quality. Our goal is to make correct lexical choices in a given context without explicitly enforcing translation consistency. More recent work in SMT uses latent representations of the document context to dynamically adapt the translation model with either monolingual topic models (Eidelman et al., 2012; Hewavitharana et al., 2013) or bilingual topic models (Hasler et al., 2014), thereby allowing the translation system to disambiguate source phrases using document context. Eidelman et al. (2012) also apply a topic model to each test sentence and find that sentence context is sufficient for picking good translations, but they do not attempt to combine sentence and document level information. Sentence-level topic adaptation for SMT has also been employed by Hasler et al. (2012). Other approaches to topic adaptation for SMT include Zhao and Xing (2007) and Tam et al. (2008), both of which use adapted lexical weights. In th</context>
</contexts>
<marker>Hewavitharana, Mehay, Ananthakrishnan, 2013</marker>
<rawString>Sanjika Hewavitharana, Dennis N Mehay, and Sankaranarayanan Ananthakrishnan. 2013. Incremental Topic-Based Translation Model Adaptation for Conversational Spoken Language Translation. In Proceedings ofACL, pages 697–701.</rawString>
</citation>
<citation valid="true">
<authors>
<author>Mark Hopkins</author>
<author>Jonathan May</author>
</authors>
<title>Tuning as ranking.</title>
<date>2011</date>
<booktitle>In Proceedings of the Conference on Empirical Methods in Natural Language Processing,</booktitle>
<location>Edinburgh, United Kingdom.</location>
<contexts>
<context position="20368" citStr="Hopkins and May, 2011" startWordPosition="3384" endWordPosition="3387">sentence pairs and documents (in brackets) in the data sets. Data CC NC TED Test documents 65 31 24 Avg sentences/doc 29.1 60.6 78.9 Table 2: Average number of sentences per document in the test set (per domain). 5.1 Unadapted baseline system Our baseline is a phrase-based French-English system trained on the concatenation of all parallel data. It was built with the Moses toolkit (Koehn et al., 2007) using the 14 standard core features including a 5-gram language model. Translation quality is evaluated on a large test set, using the average feature weights of three optimisation runs with PRO (Hopkins and May, 2011). We use the 449 noyau → kernel noyau → nucleus noyau → core Figure 3: Topic distributions for source phrase noyau and three of its translations (20 topics without topic 0). Colored bars correspond to topics IT, politics, science, economy with topic proportions &gt;10%. mteval-v13a.pl script to compute case-insensitive BLEU scores. 5.2 Domain-adapted benchmark systems As domain-aware benchmark systems, we use the linear mixture model (DOMAIN1) of Sennrich (2012) and the phrase table fill-up method (DOMAIN2) of Bisazza et al. (2011) (both available in the Moses toolkit). For both systems, the doma</context>
</contexts>
<marker>Hopkins, May, 2011</marker>
<rawString>Mark Hopkins and Jonathan May. 2011. Tuning as ranking. In Proceedings of the Conference on Empirical Methods in Natural Language Processing, Edinburgh, United Kingdom.</rawString>
</citation>
<citation valid="true">
<authors>
<author>Eric H Huang</author>
<author>Richard Socher</author>
<author>Christopher D Manning</author>
<author>Andrew Y Ng</author>
</authors>
<title>Improving Word Representations via Global Context and Multiple Word Prototypes.</title>
<date>2012</date>
<booktitle>In Proceedings of ACL,</booktitle>
<pages>873--882</pages>
<contexts>
<context position="7004" citStr="Huang et al. (2012)" startWordPosition="1116" endWordPosition="1119">ent set and the similarity to the development set is used as a feature during translation. While their vector representations are similar to our latent topic representations, their model has no notion of structure beyond corpus boundaries and is adapted towards a single target domain (cross-domain). Instead, our model learns the latent topical structure automatically and the translation model is adapted dynamically to each test instance. We are not aware of prior work in the field of MT that investigates combinations of local and global context. In their recent work on neural language models, Huang et al. (2012) combine the scores of two neural networks modelling the word embeddings of previous words in a sequence as well as those of words from the surrounding document by averaging over all word embeddings occurring in the same document. The score of the next word in a sequence is computed as the sum of the scores of both networks, but they do not consider alternative ways of combining contextual information. 3 Phrase pair topic model (PPT) Our proposed model aims to capture the relationship between phrase pairs and source words that frequently occur in the local context of a phrase pair, that is, co</context>
</contexts>
<marker>Huang, Socher, Manning, Ng, 2012</marker>
<rawString>Eric H Huang, Richard Socher, Christopher D Manning, and Andrew Y Ng. 2012. Improving Word Representations via Global Context and Multiple Word Prototypes. In Proceedings of ACL, pages 873–882.</rawString>
</citation>
<citation valid="true">
<authors>
<author>Philipp Koehn</author>
<author>Hieu Hoang</author>
<author>Alexandra Birch</author>
<author>Chris Callison-Burch</author>
<author>Marcello Federico</author>
<author>Nicola Bertoldi</author>
<author>Brooke Cowan</author>
<author>Wade Shen</author>
</authors>
<title>Moses: Open source toolkit for SMT.</title>
<date>2007</date>
<booktitle>In Proceedings of ACL: Demo</booktitle>
<location>Christine Moran, Richard Zens, Chris Dyer, Ondrej Bojar, Alexandra</location>
<note>and poster sessions.</note>
<contexts>
<context position="20149" citStr="Koehn et al., 2007" startWordPosition="3349" endWordPosition="3352"> on how important it is to have information from both local and global test contexts. Data Mixed CC NC TED Train 354K (6450) 110K 103K 140K Dev 2453 (39) 818 817 818 Test 5664 (112) 1892 1878 1894 Table 1: Number of sentence pairs and documents (in brackets) in the data sets. Data CC NC TED Test documents 65 31 24 Avg sentences/doc 29.1 60.6 78.9 Table 2: Average number of sentences per document in the test set (per domain). 5.1 Unadapted baseline system Our baseline is a phrase-based French-English system trained on the concatenation of all parallel data. It was built with the Moses toolkit (Koehn et al., 2007) using the 14 standard core features including a 5-gram language model. Translation quality is evaluated on a large test set, using the average feature weights of three optimisation runs with PRO (Hopkins and May, 2011). We use the 449 noyau → kernel noyau → nucleus noyau → core Figure 3: Topic distributions for source phrase noyau and three of its translations (20 topics without topic 0). Colored bars correspond to topics IT, politics, science, economy with topic proportions &gt;10%. mteval-v13a.pl script to compute case-insensitive BLEU scores. 5.2 Domain-adapted benchmark systems As domain-awa</context>
</contexts>
<marker>Koehn, Hoang, Birch, Callison-Burch, Federico, Bertoldi, Cowan, Shen, 2007</marker>
<rawString>Philipp Koehn, Hieu Hoang, Alexandra Birch, Chris Callison-Burch, Marcello Federico, Nicola Bertoldi, Brooke Cowan, Wade Shen, Christine Moran, Richard Zens, Chris Dyer, Ondrej Bojar, Alexandra Constantin, and Evan Herbst. 2007. Moses: Open source toolkit for SMT. In Proceedings of ACL: Demo and poster sessions.</rawString>
</citation>
<citation valid="true">
<authors>
<author>Philipp Koehn</author>
</authors>
<title>Statistical Significance Tests for Machine Translation Evaluation.</title>
<date>2004</date>
<booktitle>In Proc. of EMNLP.</booktitle>
<contexts>
<context position="25086" citStr="Koehn, 2004" startWordPosition="4157" endWordPosition="4158">words that belong to the senses IT, politics, science and economy, while the words in the context of the target phrase kernel can belong to the senses IT and food (with source context words such as grain, prot´eines, produire). Thus, the monolingual representations would still contain a relatively high level of ambiguity while the distributional profile of the phrase pair noyau → kernel preserves only the IT sense. 7 Results and discussion In this section we present experimental results of our model with different context settings and against different baselines. We used bootstrap resampling (Koehn, 2004) to measure significance on the mixed test set and marked all statistically significant results compared to the respective baselines with asterisk (*: p ≤ 0.01). 7.1 Local context In Table 3 we compare the results of the concatenation baseline and a model containing the phrSim-local feature in addition to the baseline features, for different numbers of latent topics. We show results for the mixed test set containing documents from all three domains as well as the individual results on the documents from each domain. While all topic settings yield improvements over the baseline, the largest imp</context>
</contexts>
<marker>Koehn, 2004</marker>
<rawString>Philipp Koehn. 2004. Statistical Significance Tests for Machine Translation Evaluation. In Proc. of EMNLP.</rawString>
</citation>
<citation valid="true">
<authors>
<author>Linlin Li</author>
<author>Benjamin Roth</author>
<author>Caroline Sporleder</author>
</authors>
<title>Topic Models for Word Sense Disambiguation and Token-based Idiom Detection.</title>
<date>2010</date>
<booktitle>In Proceedings ofACL,</booktitle>
<pages>1138--1147</pages>
<contexts>
<context position="4257" citStr="Li et al. (2010)" startWordPosition="670" endWordPosition="673">m both levels of contexts is beneficial for translation model adaptation. We evaluate our model on a dynamic adaptation task where the domain of a test document is unknown and hence the problem of lexical selection is harder. 445 Proceedings of the Ninth Workshop on Statistical Machine Translation, pages 445–456, Baltimore, Maryland USA, June 26–27, 2014. c�2014 Association for Computational Linguistics 2 Related work Most work in the WSD literature has modelled disambiguation using a limited window of context around the word to disambiguate. Cai et al. (2007), Boyd-graber and Blei (2007) and Li et al. (2010) further tried to integrate the notion of latent topics to address the sparsity problem of the lexicalised features typically used in WSD classifiers. The most closely related work in the area of sense disambiguation is by Dinu and Lapata (2010) who propose a disambiguation method for solving lexical similarity and substitution tasks. They measure word similarity in context by learning distributions over senses for each target word in the form of lower-dimensional distributional representations. Before computing word similarities, they contextualise the global sense distribution of a word usin</context>
</contexts>
<marker>Li, Roth, Sporleder, 2010</marker>
<rawString>Linlin Li, Benjamin Roth, and Caroline Sporleder. 2010. Topic Models for Word Sense Disambiguation and Token-based Idiom Detection. In Proceedings ofACL, pages 1138–1147.</rawString>
</citation>
<citation valid="true">
<authors>
<author>Rico Sennrich</author>
</authors>
<title>Perplexity Minimization for Translation Model Domain Adaptation in Statistical Machine Translation.</title>
<date>2012</date>
<booktitle>In Proceedings of EACL.</booktitle>
<contexts>
<context position="20831" citStr="Sennrich (2012)" startWordPosition="3455" endWordPosition="3457">del. Translation quality is evaluated on a large test set, using the average feature weights of three optimisation runs with PRO (Hopkins and May, 2011). We use the 449 noyau → kernel noyau → nucleus noyau → core Figure 3: Topic distributions for source phrase noyau and three of its translations (20 topics without topic 0). Colored bars correspond to topics IT, politics, science, economy with topic proportions &gt;10%. mteval-v13a.pl script to compute case-insensitive BLEU scores. 5.2 Domain-adapted benchmark systems As domain-aware benchmark systems, we use the linear mixture model (DOMAIN1) of Sennrich (2012) and the phrase table fill-up method (DOMAIN2) of Bisazza et al. (2011) (both available in the Moses toolkit). For both systems, the domain labels of the documents are used to group documents of the same domain together. We build adapted tables for each domain by treating the remaining documents as out-of-domain data and combining in-domain with out-of-domain tables. For development and test, the domain labels are used to select the respective domain-adapted model for decoding. Both systems have an advantage over our model because of their knowledge of domain boundaries in the data. This allow</context>
</contexts>
<marker>Sennrich, 2012</marker>
<rawString>Rico Sennrich. 2012. Perplexity Minimization for Translation Model Domain Adaptation in Statistical Machine Translation. In Proceedings of EACL.</rawString>
</citation>
<citation valid="true">
<authors>
<author>Yik-Cheung Tam</author>
<author>Ian Lane</author>
<author>Tanja Schultz</author>
</authors>
<title>Bilingual LSA-based adaptation for statistical machine translation.</title>
<date>2008</date>
<journal>Machine Translation,</journal>
<volume>21</volume>
<issue>4</issue>
<pages>207</pages>
<contexts>
<context position="3130" citStr="Tam et al. (2008)" startWordPosition="488" endWordPosition="491">al topic models (Eidelman et al., 2012; Hewavitharana et al., 2013) or bilingual topic models (Hasler et al., 2014), thereby allowing the translation system to disambiguate source phrases using document context. Eidelman et al. (2012) also apply a topic model to each test sentence and find that sentence context is sufficient for picking good translations, but they do not attempt to combine sentence and document level information. Sentence-level topic adaptation for SMT has also been employed by Hasler et al. (2012). Other approaches to topic adaptation for SMT include Zhao and Xing (2007) and Tam et al. (2008), both of which use adapted lexical weights. In this paper, we present a topic model that learns latent distributional representations of the context of a phrase pair which can be applied to both local and global contexts at test time. We introduce similarity features that compare latent representations of phrase pair types to test contexts to disambiguate senses for improved lexical selection. We also propose different strategies for combining local and global topical context and show that using clues from both levels of contexts is beneficial for translation model adaptation. We evaluate our</context>
</contexts>
<marker>Tam, Lane, Schultz, 2008</marker>
<rawString>Yik-Cheung Tam, Ian Lane, and Tanja Schultz. 2008. Bilingual LSA-based adaptation for statistical machine translation. Machine Translation, 21(4):187– 207, November.</rawString>
</citation>
<citation valid="true">
<authors>
<author>Yee Whye Teh</author>
<author>David Newman</author>
<author>Max Welling</author>
</authors>
<title>A collapsed variational Bayesian inference algorithm for LDA.</title>
<date>2006</date>
<booktitle>In Proceedings of NIPS.</booktitle>
<contexts>
<context position="10701" citStr="Teh et al., 2006" startWordPosition="1751" endWordPosition="1754">presentations for each phrase pair, where Cl−all denotes the number of context words in all sentence contexts that the phrase pair was seen in the training data, P denotes the number of phrase pairs and K denotes the number of latent topics. The model in Figure 1b has the same structure but shows inference on test contexts, where Cl denotes the number of context words in the test sentence context and L denotes the number of test instances. Bp and Bl denote the topic distribution for a phrase pair and a test context, respectively. 3.1 Inference for PPT model We use collapsed variational Bayes (Teh et al., 2006) to infer the parameters of the PPT model. The posterior distribution over topics is computed as shown below (EP(zp,c = k|z−(p,c), wc,p, α, Q) a ˆq[n−(p,c) (E(E.,k,w, ] + Q) · ˆq[n−(p,c) d,k,. ] + α) ˆq[n−(p,c) .,k,. ] + Wc · Q) (1) where zp,c denotes the topic at position c in the distributional profile p, wc denotes all context word tokens in the collection, Wc is the total number of context words and Eˆq is the expectation under the variationalosterior. n−(p,c) and p .,k,w, n−(p,c) are counts of topics occurring with context p,k,. words and distributional profiles, respectively, and n−(p,c)</context>
</contexts>
<marker>Teh, Newman, Welling, 2006</marker>
<rawString>Yee Whye Teh, David Newman, and Max Welling. 2006. A collapsed variational Bayesian inference algorithm for LDA. In Proceedings of NIPS.</rawString>
</citation>
<citation valid="true">
<authors>
<author>B Zhao</author>
<author>E P Xing</author>
</authors>
<title>HM-BiTAM: Bilingual topic exploration, word alignment, and translation. Neural Information Processing.</title>
<date>2007</date>
<contexts>
<context position="3108" citStr="Zhao and Xing (2007)" startWordPosition="483" endWordPosition="486">del with either monolingual topic models (Eidelman et al., 2012; Hewavitharana et al., 2013) or bilingual topic models (Hasler et al., 2014), thereby allowing the translation system to disambiguate source phrases using document context. Eidelman et al. (2012) also apply a topic model to each test sentence and find that sentence context is sufficient for picking good translations, but they do not attempt to combine sentence and document level information. Sentence-level topic adaptation for SMT has also been employed by Hasler et al. (2012). Other approaches to topic adaptation for SMT include Zhao and Xing (2007) and Tam et al. (2008), both of which use adapted lexical weights. In this paper, we present a topic model that learns latent distributional representations of the context of a phrase pair which can be applied to both local and global contexts at test time. We introduce similarity features that compare latent representations of phrase pair types to test contexts to disambiguate senses for improved lexical selection. We also propose different strategies for combining local and global topical context and show that using clues from both levels of contexts is beneficial for translation model adapt</context>
</contexts>
<marker>Zhao, Xing, 2007</marker>
<rawString>B Zhao and E P Xing. 2007. HM-BiTAM: Bilingual topic exploration, word alignment, and translation. Neural Information Processing.</rawString>
</citation>
</citationList>
</algorithm>
</algorithms>
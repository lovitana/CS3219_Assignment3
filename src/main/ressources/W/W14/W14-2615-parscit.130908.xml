<?xml version="1.0" encoding="UTF-8"?>
<algorithms version="110505">
<algorithm name="SectLabel" version="110505">
<variant no="0" confidence="0.000189">
<title confidence="0.981024">
Opinion Mining and Topic Categorization with Novel Term Weighting
</title>
<author confidence="0.994851">
Tatiana Gasanova
</author>
<affiliation confidence="0.9388815">
Institute of Communications Engineer-
ing, Ulm University, Germany
</affiliation>
<email confidence="0.543392">
tatiana.gasanova@uni-
ulm.de
</email>
<author confidence="0.945045">
Shakhnaz Akhmedova
</author>
<affiliation confidence="0.899201666666667">
Institute of Computer Science and
Telecommunications, Siberian State
Aerospace University, Russia
</affiliation>
<email confidence="0.98838">
shahnaz@inbox.ru
</email>
<author confidence="0.998701">
Wolfgang Minker
</author>
<affiliation confidence="0.981204">
Institute of Communications Engineer-
ing, Ulm University, Germany
</affiliation>
<email confidence="0.415861">
wolfgang.minker@uni-ulm.de
</email>
<author confidence="0.98583">
Roman Sergienko
</author>
<affiliation confidence="0.93497">
Institute of Communications Engineer-
ing, Ulm University, Germany
</affiliation>
<email confidence="0.725163">
roman.sergienko@uni-ulm.de
</email>
<author confidence="0.991686">
Eugene Semenkin
</author>
<affiliation confidence="0.899133">
Institute of Computer Science and
Telecommunications, Siberian State
Aerospace University, Russia
</affiliation>
<email confidence="0.993463">
eugenesemenkin@yandex.com
</email>
<sectionHeader confidence="0.997634" genericHeader="abstract">
Abstract
</sectionHeader>
<bodyText confidence="0.995142333333333">
In this paper we investigate the efficiency of
the novel term weighting algorithm for opin-
ion mining and topic categorization of arti-
cles from newspapers and Internet. We com-
pare the novel term weighting technique with
existing approaches such as TF-IDF and
ConfWeight. The performance on the data
from the text-mining campaigns DEFT‚Äô07
and DEFT‚Äô08 shows that the proposed meth-
od can compete with existing information re-
trieval models in classification quality and
that it is computationally faster. The pro-
posed text preprocessing method can be ap-
plied in large-scale information retrieval and
data mining problems and it can be easily
transported to different domains and different
languages since it does not require any do-
main-related or linguistic information.
</bodyText>
<sectionHeader confidence="0.999629" genericHeader="introduction">
1 Introduction
</sectionHeader>
<bodyText confidence="0.996051717948718">
Nowadays, Internet and social media generate a
huge amount of textual information. It is in-
creasingly important to develop methods of text
processing such as text classification. Text clas-
sification is very important for such problems
as automatic opining mining (sentiment analy-
sis) and topic categorization of different articles
from newspapers and Internet.
Text classification can be considered to be a
part of natural language understanding, where
there is a set of predefined categories and the
task is to automatically assign new documents
to one of these categories. The method of text
preprocessing and text representation influences
the results that are obtained even with the same
classification algorithms.
The most popular model for text classifica-
tion is vector space model. In this case text cat-
egorization may be considered as a machine
learning problem. Complexity of text categori-
zation with vector space model is compounded
by the need to extract the numerical data from
text information before applying machine learn-
ing methods. Therefore text categorization con-
sists of two parts: text preprocessing and classi-
fication using obtained numerical data.
All text preprocessing methods are based on
the idea that the category of the document de-
pends on the words or phrases from this docu-
ment. The simplest approach is to take each
word of the document as a binary coordinate
and the dimension of the feature space will be
the number of words in our dictionary.
There exist more advanced approaches for
text preprocessing to overcome this problem
such as TF-IDF (Salton and Buckley, 1988) and
ConfWeight methods (Soucy and Mineau,
2005). A novel term weighting method (Gasa-
nova et al., 2013) is also considered, which has
</bodyText>
<page confidence="0.974479">
84
</page>
<bodyText confidence="0.995707636363636">
Proceedings of the 5th Workshop on Computational Approaches to Subjectivity, Sentiment and Social Media Analysis, pages 84‚Äì89,
Baltimore, Maryland, USA. June 27, 2014. cÔøΩ2014 Association for Computational Linguistics
some similarities with the ConfWeight method,
but has improved computational efficiency. It is
important to notice that we use no morphologi-
cal or stop-word filtering before text prepro-
cessing. It means that the text preprocessing can
be performed without expert or linguistic
knowledge and that the text preprocessing is
language-independent.
In this paper we have used k-nearest neigh-
bors algorithm, Bayes Classifier, support vector
machine (SVM) generated and optimized with
COBRA (Co-Operation of Biology Related Al-
gorithms) which has been proposed by
Akhmedova and Semenkin (2013), Rocchio
Classifier or Nearest Centroid Algorithm (Roc-
chio, 1971) and Neural Network as classifica-
tion methods. RapidMiner and Microsoft Visual
Studio C++ 2010 have been used as implemen-
tation software.
For the application of algorithms and com-
parison of the results we have used the DEFT
(‚ÄúD√©fi Fouille de Texte‚Äù) Evaluation Package
2008 (Proceedings of the 4th DEFT Workshop,
2008) which has been provided by ELRA and
publically available corpora from DEFT‚Äô07
(Proceedings of the 3rd DEFT Workshop,
2007).
The main aim of this work is to evaluate the
competitiveness of the novel term weighting
(Gasanova et al., 2013) in comparison with the
state-of-the-art techniques for opining mining
and topic categorization. The criteria using in
the evaluation are classification quality and
computational efficiency.
This paper is organized as follows: in Section
2, we describe details of the corpora. Section 3
presents text preprocessing methods. In Section
4 we describe the classification algorithms
which we have used to compare different text
preprocessing techniques. Section 5 reports on
the experimental results. Finally, we provide
concluding remarks in Section 6.
</bodyText>
<sectionHeader confidence="0.979401" genericHeader="method">
2 Corpora Description
</sectionHeader>
<bodyText confidence="0.9998584">
The focus of DEFT 2007 campaign is the sen-
timent analysis, also called opinion mining. We
have used 3 publically available corpora: re-
views on books and movies (Books), reviews on
video games (Games) and political debates
about energy project (Debates).
The topic of DEFT 2008 edition is related to
the text classification by categories and genres.
The data consists of two corpora (T1 and T2)
containing articles of two genres: articles ex-
tracted from French daily newspaper Le Monde
and encyclopedic articles from Wikipedia in
French language. This paper reports on the re-
sults obtained using both tasks of the campaign
and focuses on detecting the category.
</bodyText>
<table confidence="0.9987597">
Corpus Size Classes
Books Train size = 2074 0: negative,
Test size = 1386 1: neutral,
Vocabulary = 52507 2: positive
Games Train size = 2537 0: negative,
Test size = 1694 1: neutral,
Vocabulary = 63144 2: positive
Debates Train size = 17299 0: against,
Test size = 11533 1: for
Vocabulary = 59615
</table>
<tableCaption confidence="0.99676">
Table 1. Corpora description (DEFT‚Äô07)
</tableCaption>
<table confidence="0.9983752">
Corpus Size Classes
T1 Train size = 15223 0: Sport,
Test size = 10596 1: Economy,
Vocabulary = 202979 2: Art,
3: Television
T2 Train size = 23550 0: France,
Test size = 15693 1: International,
Vocabulary = 262400 2: Literature,
3: Science,
4: Society
</table>
<tableCaption confidence="0.999648">
Table 2. Corpora description (DEFT‚Äô08)
</tableCaption>
<bodyText confidence="0.999898625">
All databases are divided into a training
(60% of the whole number of articles) and a test
set (40%). To apply our algorithms we extract-
ed all words which appear in the training set
regardless of the letter case and we also exclud-
ed dots, commas and other punctual signs. We
have not used any additional filtering as exclud-
ing the stop or ignore words.
</bodyText>
<sectionHeader confidence="0.982029" genericHeader="method">
3 Text Preprocessing Methods
</sectionHeader>
<subsectionHeader confidence="0.999713">
3.1 Binary preprocessing
</subsectionHeader>
<bodyText confidence="0.999932333333333">
We take each word of the document as a binary
coordinate and the size of the feature space will
be the size of our vocabulary (‚Äúbag of words‚Äù).
</bodyText>
<subsectionHeader confidence="0.998431">
3.2 TF-IDF
</subsectionHeader>
<bodyText confidence="0.999623666666667">
TF-IDF is a well-known approach for text pre-
processing based on multiplication of term fre-
quency tfij (ratio between the number of times
the ith word occurs in the jth document and the
document size) and inverse document frequen-
cy idfi.
</bodyText>
<equation confidence="0.9967105">
ÔøΩÔøΩÔøΩ (1)
tÔøΩÔøΩd = ÔøΩÔøΩ,
</equation>
<page confidence="0.983978">
85
</page>
<bodyText confidence="0.999794">
where tij is the number of times the ith word oc-
curs in the jth document. Tj is the document size
(number of the words in the document).
There are different ways to calculate the
weight of each word. In this paper we run clas-
sification algorithms with the following vari-
ants.
</bodyText>
<equation confidence="0.98983075">
1) TF-IDF 1
idfi = log |D |(2)
,
L
</equation>
<bodyText confidence="0.999729">
where |D |is the number of document in the
training set and ni is the number of documents
that have the ith word.
</bodyText>
<listItem confidence="0.365567">
2) TF-IDF 2
</listItem>
<bodyText confidence="0.921753333333333">
The formula is given by equation (2) except
ni is calculated as the number of times ith word
appears in all documents from the training set.
</bodyText>
<equation confidence="0.973949">
3) TF-IDF 3
idfi = (|D|) a ,a E (0, 1), ( 3)
ni
</equation>
<bodyText confidence="0.875254666666667">
where ni is calculated as in TF-IDF 1 and Œ± is
the parameter (in this paper we have tested Œ± =
0.1, 0.5, 0.9).
4) TF-IDF 4
The formula is given by equation (3) except
ni is calculated as in TF-IDF 4.
</bodyText>
<subsectionHeader confidence="0.988413">
3.3 ConfWeight
</subsectionHeader>
<bodyText confidence="0.999395727272727">
Maximum Strength (Maxstr) is an alternative
method to find the word weights. This approach
has been proposed by Soucy and Mineau
(2005). It implicitly does feature selection since
all frequent words have zero weights. The main
idea of the method is that the feature f has a
non-zero weight in class c only if the f frequen-
cy in documents of the c class is greater than
the f frequency in all other classes.
The ConfWeight method uses Maxstr as an
analog of IDF:
</bodyText>
<equation confidence="0.812672">
ConfWeightij = log(tfij + 1) * Maxstr(i).
</equation>
<bodyText confidence="0.999546090909091">
Numerical experiments (Soucy and Mineau,
2005) have shown that the ConfWeight method
could be more effective than TF-IDF with SVM
and k-NN as classification methods. The main
drawback of the ConfWeight method is compu-
tational complexity. This method is more com-
putationally demanding than TF-IDF method
because the ConfWeight method requires time-
consuming statistical calculations such as Stu-
dent distribution calculation and confidence
interval definition for each word.
</bodyText>
<subsectionHeader confidence="0.903073">
3.4 Novel Term Weighting (TW)
</subsectionHeader>
<bodyText confidence="0.999858619047619">
The main idea of the method (Gasanova et al.,
2013) is similar to ConfWeight but it is not so
time-consuming. The idea is that every word
that appears in the article has to contribute
some value to the certain class and the class
with the biggest value we define as a winner for
this article.
For each term we assign a real number term
relevance that depends on the frequency in ut-
terances. Term weight is calculated using a
modified formula of fuzzy rules relevance esti-
mation for fuzzy classifiers (Ishibuchi et al.,
1999). Membership function has been replaced
by word frequency in the current class. The de-
tails of the procedure are the following:
Let L be the number of classes; ni is the
number of articles which belong to the ith class;
Iij is the number of the jth word occurrence in
all articles from the ith class; Tij = Iij / ni is the
relative frequency of the jth word occurrence in
the ith class.
</bodyText>
<equation confidence="0.837542">
Rj = maxi Tij , Sj = arg (maxiTij) is the
</equation>
<bodyText confidence="0.634222">
number of class which we assign to the jth word;
The term relevance, Cj, is given by
</bodyText>
<equation confidence="0.9944095">
L 4
Cj = L 1 (Rj ‚àí 1 ‚àëTij
‚â†Sj
i=
</equation>
<bodyText confidence="0.991852875">
Cj is higher if the word occurs more often in
one class than if it appears in many classes. We
use novel TW as an analog of IDF for text pre-
processing.
The learning phase consists of counting the C
values for each term; it means that this
algorithm uses the statistical information
obtained from the training set.
</bodyText>
<sectionHeader confidence="0.999903" genericHeader="method">
4 Classification Methods
</sectionHeader>
<bodyText confidence="0.985702315789474">
We have considered 11 different text prepro-
cessing methods (4 modifications of TF-IDF,
two of them with three different values of Œ±
parameter, binary representation, ConfWeight
and the novel TW method) and compared them
using different classification algorithms. The
methods have been implemented using
RapidMiner (Shafait, 2010) and Microsoft Vis-
ual Studio C++ 2010 for Rocchio classifier and
SVM. The classification methods are:
- k-nearest neighbors algorithm with dis-
tance weighting (we have varied k from 1 to
15);
- kernel Bayes classifier with Laplace cor-
rection;
- neural network with error back propaga-
tion (standard setting in RapidMiner);
- Rocchio classifier with different metrics
and y parameter;
</bodyText>
<figure confidence="0.9789235">
‚àë Tji
L
).
1
</figure>
<page confidence="0.990034">
86
</page>
<bodyText confidence="0.999631142857143">
- support vector machine (SVM) generated
and optimized with Co-Operation of Biology
Related Algorithms (COBRA).
Rocchio classifier (Rocchio, 1971) is a well-
known classifier based on the search of the
nearest centroid. For each category we calculate
a weighted centroid:
</bodyText>
<equation confidence="0.9934945">
ùëîùëê = |ùë£ùëê |‚àë ùëë ‚àí ùõæ 1
1 ùëë‚ààùë£ùëê ÔøΩùë£ùëê,ùëòÔøΩÔøΩÔøΩÔøΩÔøΩÔøΩ ‚àëùëë‚ààùë£ùëê,ùëò ùëë,
</equation>
<bodyText confidence="0.999893333333333">
where ùë£ùëê is a set of documents which belong to
the class c; ùë£ùëê,ùëòÔøΩÔøΩÔøΩÔøΩÔøΩ are k documents which do not
belong to the class c and which are close to the
</bodyText>
<equation confidence="0.7420205">
centroid 1
|ùë£ùëê |‚àëùëë‚ààùë£ùëê ùëë; ùõæ is parameter corresponds
</equation>
<bodyText confidence="0.999961869565217">
to relative importance of negative precedents.
The given document is put to the class with the
nearest centroid. In this work we have applied
Rocchio classifier with ùõæ ‚àà (0.1; 0.9) and with
three different metrics: taxicab distance,
Euclidean metric and cosine similarity.
COBRA is a new meta-heuristic algorithm
which has been proposed by Akhmedova and
Semenkin (2013). It is based on cooperation of
biology inspired algorithms such as Particle
Swarm Optimization (Kennedy and Eberhart,
1995), Wolf Pack Search Algorithm (Yang,
2007), Firefly Algorithm (Yang, 2008), Cuckoo
Search Algorithm (Yang and Deb, 2009) and
Bat Algorithm (Yang, 2010). For generating
SVM-machine the original COBRA is used:
each individual in all populations represents a
set of kernel function‚Äôs parameters Œ±,Œ≤,d.
Then for each individual constrained modifica-
tion of COBRA is applied for finding vector w
and shift factor b. And finally individual that
showed the best classification rate is chosen as
the designed classifier.
</bodyText>
<sectionHeader confidence="0.996603" genericHeader="evaluation">
5 Experimental Results
</sectionHeader>
<bodyText confidence="0.99897324">
The DEFT (‚ÄúD√©fi Fouille de Texte‚Äù) Evaluation
Package 2008 and publically available corpora
from DEFT‚Äô07 (Books, Games and Debates)
have been used for algorithms application and
results comparison. In order to evaluate ob-
tained results with the campaign participants we
have to use the same measure of classification
quality: precision, recall and F-score.
Precision for each class i is calculated as the
number of correctly classified articles for class i
divided by the number of all articles which al-
gorithm assigned for this class. Recall is the
number of correctly classified articles for class i
divided by the number of articles that should
have been in this class. Overall precision and
recall are calculated as the arithmetic mean of
the precisions and recalls for all classes (macro-
average). F-score is calculated as the harmonic
mean of precision and recall.
Tables 3-7 present the F-scores obtained on
the test corpora. The best values for each prob-
lem are shown in bold. Results of the all classi-
fication algorithms are presented with the best
parameters. We also present for each corpus
only the best TF-IDF modification.
</bodyText>
<table confidence="0.9997715">
Classification Binary TF- Conf Novel
algorithm IDF Weight TW
Bayes 0.489 0.506 0.238 0.437
k-NN 0.488 0.517 0.559 0.488
Rocchio 0.479 0.498 0.557 0.537
SVM (CO- 0.558 0.580 0.588 0.619
BRA)
Neural network 0.475 0.505 0.570 0.493
</table>
<tableCaption confidence="0.978196">
Table 3. Classification results for Books
</tableCaption>
<table confidence="0.999904125">
Classification Binary TF- Conf Novel
algorithm IDF Weight TW
Bayes 0.653 0.652 0.210 0.675
k-NN 0.703 0.701 0.720 0.700
Rocchio 0.659 0.678 0.717 0.712
SVM (CO- 0.682 0.687 0.645 0.696
BRA)
Neural network 0.701 0.679 0.717 0.691
</table>
<tableCaption confidence="0.994007">
Table 4. Classification results for Games
</tableCaption>
<table confidence="0.999887375">
Classification Binary TF- Conf Novel
algorithm IDF Weight TW
Bayes 0.555 0.645 0.363 0.616
k-NN 0.645 0.648 0.695 0.695
Rocchio 0.636 0.646 0.697 0.696
SVM (CO- 0.673 0.669 0.714 0.700
BRA)
Neural network 0.656 0.647 0.705 0.697
</table>
<tableCaption confidence="0.989064">
Table 5. Classification results for Debates
</tableCaption>
<table confidence="0.988941">
Classification Binary TF- Conf Novel
algorithm IDF Weight TW
Bayes 0.501 0.690 0.837 0.794
k-NN 0.800 0.816 0.855 0.837
Rocchio 0.794 0.825 0.853 0.838
SVM (CO- 0.788 0.827 0.840 0.856
BRA)
Neural network 0.783 0.830 0.853 0.854
Table 6. Classification results for T1
Classification Binary TF- Conf Novel
algorithm IDF Weight TW
Bayes 0.569 0.728 0.712 0.746
k-NN 0.728 0.786 0.785 0.811
Rocchio 0.765 0.825 0.803 0.834
SVM (CO- 0.794 0.837 0.813 0.851
BRA)
Neural network 0.799 0.838 0.820 0.843
</table>
<tableCaption confidence="0.999016">
Table 7. Classification results for T2
</tableCaption>
<page confidence="0.999027">
87
</page>
<bodyText confidence="0.992144222222222">
We can see from the Tables 3-7 that the best
F-scores have been obtained with either
ConfWeight or novel Term Weighting prepro-
cessing. The algorithm performances on the
Games and Debates corpora achieved the best
results with ConfWeight; however, we can see
that the F-scores obtained with novel Term
Weighting preprocessing are very similar
(0.712 and 0.720 for Games; 0.700 and 0.714
for Debates). Almost all best results have been
obtained with SVM except the Games database
where we achieved the highest F-score with k-
NN algorithm.
This paper focuses on the text preprocessing
methods which do not require language or do-
main-related information; therefore, we have
not tried to achieve the best possible classifica-
tion quality. However, the result obtained on
Books corpus with novel TW preprocessing and
SVM (generated using COBRA) as classifica-
tion algorithm has reached 0.619 F-score which
is higher than the best known performance
0.603 (Proceedings of the 3rd DEFT Workshop,
2007). Performances on other corpora have
achieved close F-score values to the best sub-
missions of the DEFT‚Äô07 and DEFT‚Äô08 partici-
pants.
We have also measured computational effi-
ciency of each text preprocessing technique.
We have run each method 20 times using the
Baden-W√ºrttemberg Grid (bwGRiD) Cluster
Ulm (Every blade comprehends two 4-Core
Intel Harpertown CPUs with 2.83 GHz and 16
GByte RAM). After that we calculated average
values and checked statistical significance of
the results.
</bodyText>
<figureCaption confidence="0.89420875">
Figure 1 and Figure 2 compare average com-
putational time in minutes for different prepro-
cessing methods applied on DEFT‚Äô07 and
DEFT‚Äô08 corpora.
Figure 1. Computational efficiency of text pre-
processing methods (DEFT‚Äô07)
Figure 2. Computational efficiency of text pre-
processing methods (DEFT‚Äô08)
</figureCaption>
<bodyText confidence="0.999978272727273">
The average value for all TF-IDF modifica-
tions is presented because the time variation for
the modifications is not significant.
We can see in Figure 1 and Figure 2 that TF-
IDF and novel TW require almost the same
computational time. The most time-consuming
method is ConfWeight (CW). It requires ap-
proximately six times more time than TF-IDF
and novel TW for DEFT‚Äô08 corpora and about
three-four times more time than TF-IDF and
novel TW for DEFT‚Äô07 databases.
</bodyText>
<sectionHeader confidence="0.99943" genericHeader="conclusions">
6 Conclusion
</sectionHeader>
<bodyText confidence="0.99986304">
This paper reported on text classification exper-
iments on 5 different corpora of opinion mining
and topic categorization using several classifi-
cation methods with different text prepro-
cessing. We have used ‚Äúbag of words‚Äù, TF-IDF
modifications, ConfWeight and the novel term
weighting approach as preprocessing tech-
niques. K-nearest neighbors algorithms, Bayes
classifier, Rocchio classifier, support vector
machine trained by COBRA and Neural Net-
work have been applied as classification algo-
rithms.
The novel term weighting method gives simi-
lar or better classification quality than the
ConfWeight method but it requires the same
amount of time as TF-IDF. Almost all best re-
sults have been obtained with SVM generated
and optimized with Co-Operation of Biology
Related Algorithms (COBRA).
We can conclude that numerical experiments
have shown computational and classification
efficiency of the proposed method (the novel
TW) in comparison with existing text prepro-
cessing techniques for opinion mining and topic
categorization.
</bodyText>
<page confidence="0.995336">
88
</page>
<bodyText confidence="0.9344425">
operative Strategies for Optimization (NISCO
2010):65-74.
</bodyText>
<sectionHeader confidence="0.995497" genericHeader="references">
References
</sectionHeader>
<reference confidence="0.999837980769231">
Akhmedova Sh. and Semenkin E. 2013. Co-
Operation of Biology Related Algorithms. Pro-
ceedings of the IEEE Congress on Evolutionary
Computation (CEC 2013):2207-2214.
Association Fran√ßaise d‚ÄôIntelligence Artificielle.
2007. Proceedings of the 3rd DEFT Workshop.
DEFT &apos;07. AFIA, Grenoble, France.
Gasanova T., Sergienko R., Minker W., Semenkin
E. and Zhukov E. 2013. A Semi-supervised Ap-
proach for Natural Language Call Routing. Pro-
ceedings of the SIGDIAL 2013 Conference:344-
348.
Ishibuchi H., Nakashima T., and Murata T. 1999.
Performance evaluation of fuzzy classifier sys-
tems for multidimensional pattern classification
problems. IEEE Trans. on Systems, Man, and Cy-
bernetics, 29:601-618.
Kennedy J. and Eberhart R. 1995. Particle Swarm
Optimization. Proceedings of IEEE International
Conference on Neural Networks:1942-1948.
Le traitement automatique du langage naturel ou de
la langue naturelle. 2008. Proceedings of the 4th
DEFT Workshop. DEFT &apos;08. TALN, Avignon,
France.
Salton G. and Buckley C. 1988. Term-Weighting
Approaches in Automatic Text Retrieval. Infor-
mation Processing and Management:513-523.
Shafait F., Reif M., Kofler C., and Breuel T. M.
2010. Pattern Recognition Engineering.
RapidMiner Community Meeting and Conference,
9.
Soucy P. and Mineau G.W. 2005. Beyond TFIDF
Weighting for Text Categorization in the Vector
Space Model. Proceedings of the 19th Interna-
tional Joint Conference on Artificial Intelligence
(IJCAI 2005):1130-1135.
Rocchio J. 1971. Relevance Feedback in Infor-
mation Retrieval. The SMART Retrieval System-
Experiments in Automatic Document Processing,
Prentice-Hall:313-323.
Yang Ch. 2007. Algorithm of Marriage in Honey
Bees Optimization Based on the Wolf Pack
Search. Proceedings of International Conference
on Intelligent Pervasive Computing:462-467.
Yang X.S. 2008. Nature-Inspired Metaheuristic Al-
gorithms.
Yang X.S. and Deb S. 2009. Cuckoo search via
Levy flights. Proceedings of World Congress on
Nature &amp; Biologically Inspired Computing:210-
214.
Yang X.S. 2010. A New Metaheuristic Bat-Inspired
Algorithm. Proceedings of Nature Inspired Co-
</reference>
<page confidence="0.999753">
89
</page>
</variant>
</algorithm>
<algorithm name="ParsHed" version="110505">
<variant no="0" confidence="0.114257">
<title confidence="0.999747">Opinion Mining and Topic Categorization with Novel Term Weighting</title>
<author confidence="0.999767">Tatiana Gasanova</author>
<affiliation confidence="0.999912">Institute of Communications Engineer-</affiliation>
<address confidence="0.974461">ing, Ulm University, Germany</address>
<email confidence="0.998921">ulm.de</email>
<author confidence="0.684916">Shakhnaz</author>
<affiliation confidence="0.875441">Institute of Computer Science Telecommunications, Siberian Aerospace University, Russia</affiliation>
<email confidence="0.992819">shahnaz@inbox.ru</email>
<author confidence="0.952429">Wolfgang Minker</author>
<affiliation confidence="0.999547">Institute of Communications Engineer-</affiliation>
<address confidence="0.973059">ing, Ulm University, Germany</address>
<email confidence="0.998661">wolfgang.minker@uni-ulm.de</email>
<author confidence="0.997582">Roman Sergienko</author>
<affiliation confidence="0.999889">Institute of Communications Engineer-</affiliation>
<address confidence="0.97533">ing, Ulm University, Germany</address>
<email confidence="0.999293">roman.sergienko@uni-ulm.de</email>
<author confidence="0.497978">Eugene</author>
<affiliation confidence="0.880628">Institute of Computer Science Telecommunications, Siberian Aerospace University, Russia</affiliation>
<email confidence="0.999814">eugenesemenkin@yandex.com</email>
<abstract confidence="0.99546552631579">In this paper we investigate the efficiency of the novel term weighting algorithm for opinion mining and topic categorization of articles from newspapers and Internet. We compare the novel term weighting technique with existing approaches such as TF-IDF and ConfWeight. The performance on the data from the text-mining campaigns DEFT‚Äô07 and DEFT‚Äô08 shows that the proposed method can compete with existing information retrieval models in classification quality and that it is computationally faster. The proposed text preprocessing method can be applied in large-scale information retrieval and data mining problems and it can be easily transported to different domains and different languages since it does not require any domain-related or linguistic information.</abstract>
</variant>
</algorithm>
<algorithm name="ParsCit" version="110505">
<citationList>
<citation valid="true">
<authors>
<author>E Semenkin</author>
</authors>
<title>CoOperation of Biology Related Algorithms.</title>
<date>2013</date>
<journal>Proceedings of the IEEE Congress on Evolutionary Computation (CEC</journal>
<pages>2013--2207</pages>
<contexts>
<context position="3998" citStr="Semenkin (2013)" startWordPosition="583" endWordPosition="584">for Computational Linguistics some similarities with the ConfWeight method, but has improved computational efficiency. It is important to notice that we use no morphological or stop-word filtering before text preprocessing. It means that the text preprocessing can be performed without expert or linguistic knowledge and that the text preprocessing is language-independent. In this paper we have used k-nearest neighbors algorithm, Bayes Classifier, support vector machine (SVM) generated and optimized with COBRA (Co-Operation of Biology Related Algorithms) which has been proposed by Akhmedova and Semenkin (2013), Rocchio Classifier or Nearest Centroid Algorithm (Rocchio, 1971) and Neural Network as classification methods. RapidMiner and Microsoft Visual Studio C++ 2010 have been used as implementation software. For the application of algorithms and comparison of the results we have used the DEFT (‚ÄúD√©fi Fouille de Texte‚Äù) Evaluation Package 2008 (Proceedings of the 4th DEFT Workshop, 2008) which has been provided by ELRA and publically available corpora from DEFT‚Äô07 (Proceedings of the 3rd DEFT Workshop, 2007). The main aim of this work is to evaluate the competitiveness of the novel term weighting (G</context>
<context position="12172" citStr="Semenkin (2013)" startWordPosition="2000" endWordPosition="2001"> |ùë£ùëê |‚àë ùëë ‚àí ùõæ 1 1 ùëë‚ààùë£ùëê ÔøΩùë£ùëê,ùëòÔøΩÔøΩÔøΩÔøΩÔøΩÔøΩ ‚àëùëë‚ààùë£ùëê,ùëò ùëë, where ùë£ùëê is a set of documents which belong to the class c; ùë£ùëê,ùëòÔøΩÔøΩÔøΩÔøΩÔøΩ are k documents which do not belong to the class c and which are close to the centroid 1 |ùë£ùëê |‚àëùëë‚ààùë£ùëê ùëë; ùõæ is parameter corresponds to relative importance of negative precedents. The given document is put to the class with the nearest centroid. In this work we have applied Rocchio classifier with ùõæ ‚àà (0.1; 0.9) and with three different metrics: taxicab distance, Euclidean metric and cosine similarity. COBRA is a new meta-heuristic algorithm which has been proposed by Akhmedova and Semenkin (2013). It is based on cooperation of biology inspired algorithms such as Particle Swarm Optimization (Kennedy and Eberhart, 1995), Wolf Pack Search Algorithm (Yang, 2007), Firefly Algorithm (Yang, 2008), Cuckoo Search Algorithm (Yang and Deb, 2009) and Bat Algorithm (Yang, 2010). For generating SVM-machine the original COBRA is used: each individual in all populations represents a set of kernel function‚Äôs parameters Œ±,Œ≤,d. Then for each individual constrained modification of COBRA is applied for finding vector w and shift factor b. And finally individual that showed the best classification rate is </context>
</contexts>
<marker>Semenkin, 2013</marker>
<rawString>Akhmedova Sh. and Semenkin E. 2013. CoOperation of Biology Related Algorithms. Proceedings of the IEEE Congress on Evolutionary Computation (CEC 2013):2207-2214.</rawString>
</citation>
<citation valid="true">
<title>Association Fran√ßaise d‚ÄôIntelligence Artificielle.</title>
<date>2007</date>
<booktitle>Proceedings of the 3rd DEFT Workshop. DEFT &apos;07. AFIA,</booktitle>
<location>Grenoble, France.</location>
<marker>2007</marker>
<rawString>Association Fran√ßaise d‚ÄôIntelligence Artificielle. 2007. Proceedings of the 3rd DEFT Workshop. DEFT &apos;07. AFIA, Grenoble, France.</rawString>
</citation>
<citation valid="true">
<authors>
<author>T Gasanova</author>
<author>R Sergienko</author>
<author>W Minker</author>
<author>E Semenkin</author>
<author>E Zhukov</author>
</authors>
<title>A Semi-supervised Approach for Natural Language Call Routing.</title>
<date>2013</date>
<booktitle>Proceedings of the SIGDIAL</booktitle>
<pages>344--348</pages>
<contexts>
<context position="3162" citStr="Gasanova et al., 2013" startWordPosition="460" endWordPosition="464">ts of two parts: text preprocessing and classification using obtained numerical data. All text preprocessing methods are based on the idea that the category of the document depends on the words or phrases from this document. The simplest approach is to take each word of the document as a binary coordinate and the dimension of the feature space will be the number of words in our dictionary. There exist more advanced approaches for text preprocessing to overcome this problem such as TF-IDF (Salton and Buckley, 1988) and ConfWeight methods (Soucy and Mineau, 2005). A novel term weighting method (Gasanova et al., 2013) is also considered, which has 84 Proceedings of the 5th Workshop on Computational Approaches to Subjectivity, Sentiment and Social Media Analysis, pages 84‚Äì89, Baltimore, Maryland, USA. June 27, 2014. cÔøΩ2014 Association for Computational Linguistics some similarities with the ConfWeight method, but has improved computational efficiency. It is important to notice that we use no morphological or stop-word filtering before text preprocessing. It means that the text preprocessing can be performed without expert or linguistic knowledge and that the text preprocessing is language-independent. In th</context>
<context position="4619" citStr="Gasanova et al., 2013" startWordPosition="680" endWordPosition="683">), Rocchio Classifier or Nearest Centroid Algorithm (Rocchio, 1971) and Neural Network as classification methods. RapidMiner and Microsoft Visual Studio C++ 2010 have been used as implementation software. For the application of algorithms and comparison of the results we have used the DEFT (‚ÄúD√©fi Fouille de Texte‚Äù) Evaluation Package 2008 (Proceedings of the 4th DEFT Workshop, 2008) which has been provided by ELRA and publically available corpora from DEFT‚Äô07 (Proceedings of the 3rd DEFT Workshop, 2007). The main aim of this work is to evaluate the competitiveness of the novel term weighting (Gasanova et al., 2013) in comparison with the state-of-the-art techniques for opining mining and topic categorization. The criteria using in the evaluation are classification quality and computational efficiency. This paper is organized as follows: in Section 2, we describe details of the corpora. Section 3 presents text preprocessing methods. In Section 4 we describe the classification algorithms which we have used to compare different text preprocessing techniques. Section 5 reports on the experimental results. Finally, we provide concluding remarks in Section 6. 2 Corpora Description The focus of DEFT 2007 campa</context>
<context position="9197" citStr="Gasanova et al., 2013" startWordPosition="1474" endWordPosition="1477">axstr as an analog of IDF: ConfWeightij = log(tfij + 1) * Maxstr(i). Numerical experiments (Soucy and Mineau, 2005) have shown that the ConfWeight method could be more effective than TF-IDF with SVM and k-NN as classification methods. The main drawback of the ConfWeight method is computational complexity. This method is more computationally demanding than TF-IDF method because the ConfWeight method requires timeconsuming statistical calculations such as Student distribution calculation and confidence interval definition for each word. 3.4 Novel Term Weighting (TW) The main idea of the method (Gasanova et al., 2013) is similar to ConfWeight but it is not so time-consuming. The idea is that every word that appears in the article has to contribute some value to the certain class and the class with the biggest value we define as a winner for this article. For each term we assign a real number term relevance that depends on the frequency in utterances. Term weight is calculated using a modified formula of fuzzy rules relevance estimation for fuzzy classifiers (Ishibuchi et al., 1999). Membership function has been replaced by word frequency in the current class. The details of the procedure are the following:</context>
</contexts>
<marker>Gasanova, Sergienko, Minker, Semenkin, Zhukov, 2013</marker>
<rawString>Gasanova T., Sergienko R., Minker W., Semenkin E. and Zhukov E. 2013. A Semi-supervised Approach for Natural Language Call Routing. Proceedings of the SIGDIAL 2013 Conference:344-348.</rawString>
</citation>
<citation valid="true">
<authors>
<author>H Ishibuchi</author>
<author>T Nakashima</author>
<author>T Murata</author>
</authors>
<title>Performance evaluation of fuzzy classifier systems for multidimensional pattern classification problems.</title>
<date>1999</date>
<journal>IEEE Trans. on Systems, Man, and Cybernetics,</journal>
<pages>29--601</pages>
<contexts>
<context position="9670" citStr="Ishibuchi et al., 1999" startWordPosition="1558" endWordPosition="1561">bution calculation and confidence interval definition for each word. 3.4 Novel Term Weighting (TW) The main idea of the method (Gasanova et al., 2013) is similar to ConfWeight but it is not so time-consuming. The idea is that every word that appears in the article has to contribute some value to the certain class and the class with the biggest value we define as a winner for this article. For each term we assign a real number term relevance that depends on the frequency in utterances. Term weight is calculated using a modified formula of fuzzy rules relevance estimation for fuzzy classifiers (Ishibuchi et al., 1999). Membership function has been replaced by word frequency in the current class. The details of the procedure are the following: Let L be the number of classes; ni is the number of articles which belong to the ith class; Iij is the number of the jth word occurrence in all articles from the ith class; Tij = Iij / ni is the relative frequency of the jth word occurrence in the ith class. Rj = maxi Tij , Sj = arg (maxiTij) is the number of class which we assign to the jth word; The term relevance, Cj, is given by L 4 Cj = L 1 (Rj ‚àí 1 ‚àëTij ‚â†Sj i= Cj is higher if the word occurs more often in one cla</context>
</contexts>
<marker>Ishibuchi, Nakashima, Murata, 1999</marker>
<rawString>Ishibuchi H., Nakashima T., and Murata T. 1999. Performance evaluation of fuzzy classifier systems for multidimensional pattern classification problems. IEEE Trans. on Systems, Man, and Cybernetics, 29:601-618.</rawString>
</citation>
<citation valid="true">
<authors>
<author>J Kennedy</author>
<author>R Eberhart</author>
</authors>
<title>Particle Swarm Optimization.</title>
<date>1995</date>
<booktitle>Proceedings of IEEE International Conference on Neural Networks:1942-1948.</booktitle>
<contexts>
<context position="12296" citStr="Kennedy and Eberhart, 1995" startWordPosition="2016" endWordPosition="2019">ÔøΩÔøΩÔøΩ are k documents which do not belong to the class c and which are close to the centroid 1 |ùë£ùëê |‚àëùëë‚ààùë£ùëê ùëë; ùõæ is parameter corresponds to relative importance of negative precedents. The given document is put to the class with the nearest centroid. In this work we have applied Rocchio classifier with ùõæ ‚àà (0.1; 0.9) and with three different metrics: taxicab distance, Euclidean metric and cosine similarity. COBRA is a new meta-heuristic algorithm which has been proposed by Akhmedova and Semenkin (2013). It is based on cooperation of biology inspired algorithms such as Particle Swarm Optimization (Kennedy and Eberhart, 1995), Wolf Pack Search Algorithm (Yang, 2007), Firefly Algorithm (Yang, 2008), Cuckoo Search Algorithm (Yang and Deb, 2009) and Bat Algorithm (Yang, 2010). For generating SVM-machine the original COBRA is used: each individual in all populations represents a set of kernel function‚Äôs parameters Œ±,Œ≤,d. Then for each individual constrained modification of COBRA is applied for finding vector w and shift factor b. And finally individual that showed the best classification rate is chosen as the designed classifier. 5 Experimental Results The DEFT (‚ÄúD√©fi Fouille de Texte‚Äù) Evaluation Package 2008 and pub</context>
</contexts>
<marker>Kennedy, Eberhart, 1995</marker>
<rawString>Kennedy J. and Eberhart R. 1995. Particle Swarm Optimization. Proceedings of IEEE International Conference on Neural Networks:1942-1948.</rawString>
</citation>
<citation valid="true">
<title>Le traitement automatique du langage naturel ou de la langue naturelle.</title>
<date>2008</date>
<booktitle>Proceedings of the 4th DEFT Workshop. DEFT &apos;08. TALN,</booktitle>
<location>Avignon, France.</location>
<marker>2008</marker>
<rawString>Le traitement automatique du langage naturel ou de la langue naturelle. 2008. Proceedings of the 4th DEFT Workshop. DEFT &apos;08. TALN, Avignon, France.</rawString>
</citation>
<citation valid="true">
<authors>
<author>G Salton</author>
<author>C Buckley</author>
</authors>
<date>1988</date>
<booktitle>Term-Weighting Approaches in Automatic Text Retrieval. Information Processing and Management:513-523.</booktitle>
<contexts>
<context position="3059" citStr="Salton and Buckley, 1988" startWordPosition="444" endWordPosition="447"> data from text information before applying machine learning methods. Therefore text categorization consists of two parts: text preprocessing and classification using obtained numerical data. All text preprocessing methods are based on the idea that the category of the document depends on the words or phrases from this document. The simplest approach is to take each word of the document as a binary coordinate and the dimension of the feature space will be the number of words in our dictionary. There exist more advanced approaches for text preprocessing to overcome this problem such as TF-IDF (Salton and Buckley, 1988) and ConfWeight methods (Soucy and Mineau, 2005). A novel term weighting method (Gasanova et al., 2013) is also considered, which has 84 Proceedings of the 5th Workshop on Computational Approaches to Subjectivity, Sentiment and Social Media Analysis, pages 84‚Äì89, Baltimore, Maryland, USA. June 27, 2014. cÔøΩ2014 Association for Computational Linguistics some similarities with the ConfWeight method, but has improved computational efficiency. It is important to notice that we use no morphological or stop-word filtering before text preprocessing. It means that the text preprocessing can be performe</context>
</contexts>
<marker>Salton, Buckley, 1988</marker>
<rawString>Salton G. and Buckley C. 1988. Term-Weighting Approaches in Automatic Text Retrieval. Information Processing and Management:513-523.</rawString>
</citation>
<citation valid="true">
<authors>
<author>F Shafait</author>
<author>M Reif</author>
<author>C Kofler</author>
<author>T M Breuel</author>
</authors>
<date>2010</date>
<booktitle>Pattern Recognition Engineering. RapidMiner Community Meeting and Conference,</booktitle>
<volume>9</volume>
<marker>Shafait, Reif, Kofler, Breuel, 2010</marker>
<rawString>Shafait F., Reif M., Kofler C., and Breuel T. M. 2010. Pattern Recognition Engineering. RapidMiner Community Meeting and Conference, 9.</rawString>
</citation>
<citation valid="true">
<authors>
<author>P Soucy</author>
<author>G W Mineau</author>
</authors>
<title>Beyond TFIDF Weighting for Text Categorization in the Vector Space Model.</title>
<date>2005</date>
<booktitle>Proceedings of the 19th International Joint Conference on Artificial Intelligence (IJCAI</booktitle>
<pages>2005--1130</pages>
<contexts>
<context position="3107" citStr="Soucy and Mineau, 2005" startWordPosition="451" endWordPosition="454">e learning methods. Therefore text categorization consists of two parts: text preprocessing and classification using obtained numerical data. All text preprocessing methods are based on the idea that the category of the document depends on the words or phrases from this document. The simplest approach is to take each word of the document as a binary coordinate and the dimension of the feature space will be the number of words in our dictionary. There exist more advanced approaches for text preprocessing to overcome this problem such as TF-IDF (Salton and Buckley, 1988) and ConfWeight methods (Soucy and Mineau, 2005). A novel term weighting method (Gasanova et al., 2013) is also considered, which has 84 Proceedings of the 5th Workshop on Computational Approaches to Subjectivity, Sentiment and Social Media Analysis, pages 84‚Äì89, Baltimore, Maryland, USA. June 27, 2014. cÔøΩ2014 Association for Computational Linguistics some similarities with the ConfWeight method, but has improved computational efficiency. It is important to notice that we use no morphological or stop-word filtering before text preprocessing. It means that the text preprocessing can be performed without expert or linguistic knowledge and tha</context>
<context position="8275" citStr="Soucy and Mineau (2005)" startWordPosition="1322" endWordPosition="1325"> training set and ni is the number of documents that have the ith word. 2) TF-IDF 2 The formula is given by equation (2) except ni is calculated as the number of times ith word appears in all documents from the training set. 3) TF-IDF 3 idfi = (|D|) a ,a E (0, 1), ( 3) ni where ni is calculated as in TF-IDF 1 and Œ± is the parameter (in this paper we have tested Œ± = 0.1, 0.5, 0.9). 4) TF-IDF 4 The formula is given by equation (3) except ni is calculated as in TF-IDF 4. 3.3 ConfWeight Maximum Strength (Maxstr) is an alternative method to find the word weights. This approach has been proposed by Soucy and Mineau (2005). It implicitly does feature selection since all frequent words have zero weights. The main idea of the method is that the feature f has a non-zero weight in class c only if the f frequency in documents of the c class is greater than the f frequency in all other classes. The ConfWeight method uses Maxstr as an analog of IDF: ConfWeightij = log(tfij + 1) * Maxstr(i). Numerical experiments (Soucy and Mineau, 2005) have shown that the ConfWeight method could be more effective than TF-IDF with SVM and k-NN as classification methods. The main drawback of the ConfWeight method is computational compl</context>
</contexts>
<marker>Soucy, Mineau, 2005</marker>
<rawString>Soucy P. and Mineau G.W. 2005. Beyond TFIDF Weighting for Text Categorization in the Vector Space Model. Proceedings of the 19th International Joint Conference on Artificial Intelligence (IJCAI 2005):1130-1135.</rawString>
</citation>
<citation valid="true">
<authors>
<author>J Rocchio</author>
</authors>
<title>Relevance Feedback in Information Retrieval.</title>
<date>1971</date>
<booktitle>The SMART Retrieval SystemExperiments in Automatic Document Processing,</booktitle>
<pages>313--323</pages>
<contexts>
<context position="4064" citStr="Rocchio, 1971" startWordPosition="591" endWordPosition="593"> method, but has improved computational efficiency. It is important to notice that we use no morphological or stop-word filtering before text preprocessing. It means that the text preprocessing can be performed without expert or linguistic knowledge and that the text preprocessing is language-independent. In this paper we have used k-nearest neighbors algorithm, Bayes Classifier, support vector machine (SVM) generated and optimized with COBRA (Co-Operation of Biology Related Algorithms) which has been proposed by Akhmedova and Semenkin (2013), Rocchio Classifier or Nearest Centroid Algorithm (Rocchio, 1971) and Neural Network as classification methods. RapidMiner and Microsoft Visual Studio C++ 2010 have been used as implementation software. For the application of algorithms and comparison of the results we have used the DEFT (‚ÄúD√©fi Fouille de Texte‚Äù) Evaluation Package 2008 (Proceedings of the 4th DEFT Workshop, 2008) which has been provided by ELRA and publically available corpora from DEFT‚Äô07 (Proceedings of the 3rd DEFT Workshop, 2007). The main aim of this work is to evaluate the competitiveness of the novel term weighting (Gasanova et al., 2013) in comparison with the state-of-the-art tech</context>
<context position="11429" citStr="Rocchio, 1971" startWordPosition="1868" endWordPosition="1869">ms. The methods have been implemented using RapidMiner (Shafait, 2010) and Microsoft Visual Studio C++ 2010 for Rocchio classifier and SVM. The classification methods are: - k-nearest neighbors algorithm with distance weighting (we have varied k from 1 to 15); - kernel Bayes classifier with Laplace correction; - neural network with error back propagation (standard setting in RapidMiner); - Rocchio classifier with different metrics and y parameter; ‚àë Tji L ). 1 86 - support vector machine (SVM) generated and optimized with Co-Operation of Biology Related Algorithms (COBRA). Rocchio classifier (Rocchio, 1971) is a wellknown classifier based on the search of the nearest centroid. For each category we calculate a weighted centroid: ùëîùëê = |ùë£ùëê |‚àë ùëë ‚àí ùõæ 1 1 ùëë‚ààùë£ùëê ÔøΩùë£ùëê,ùëòÔøΩÔøΩÔøΩÔøΩÔøΩÔøΩ ‚àëùëë‚ààùë£ùëê,ùëò ùëë, where ùë£ùëê is a set of documents which belong to the class c; ùë£ùëê,ùëòÔøΩÔøΩÔøΩÔøΩÔøΩ are k documents which do not belong to the class c and which are close to the centroid 1 |ùë£ùëê |‚àëùëë‚ààùë£ùëê ùëë; ùõæ is parameter corresponds to relative importance of negative precedents. The given document is put to the class with the nearest centroid. In this work we have applied Rocchio classifier with ùõæ ‚àà (0.1; 0.9) and with three different metrics: taxicab dis</context>
</contexts>
<marker>Rocchio, 1971</marker>
<rawString>Rocchio J. 1971. Relevance Feedback in Information Retrieval. The SMART Retrieval SystemExperiments in Automatic Document Processing, Prentice-Hall:313-323.</rawString>
</citation>
<citation valid="true">
<authors>
<author>Yang Ch</author>
</authors>
<title>Algorithm of Marriage in Honey Bees Optimization Based on the Wolf Pack Search.</title>
<date>2007</date>
<booktitle>Proceedings of International Conference on Intelligent Pervasive Computing:462-467.</booktitle>
<marker>Ch, 2007</marker>
<rawString>Yang Ch. 2007. Algorithm of Marriage in Honey Bees Optimization Based on the Wolf Pack Search. Proceedings of International Conference on Intelligent Pervasive Computing:462-467.</rawString>
</citation>
<citation valid="true">
<authors>
<author>X S Yang</author>
</authors>
<title>Nature-Inspired Metaheuristic Algorithms.</title>
<date>2008</date>
<contexts>
<context position="12369" citStr="Yang, 2008" startWordPosition="2028" endWordPosition="2029">d 1 |ùë£ùëê |‚àëùëë‚ààùë£ùëê ùëë; ùõæ is parameter corresponds to relative importance of negative precedents. The given document is put to the class with the nearest centroid. In this work we have applied Rocchio classifier with ùõæ ‚àà (0.1; 0.9) and with three different metrics: taxicab distance, Euclidean metric and cosine similarity. COBRA is a new meta-heuristic algorithm which has been proposed by Akhmedova and Semenkin (2013). It is based on cooperation of biology inspired algorithms such as Particle Swarm Optimization (Kennedy and Eberhart, 1995), Wolf Pack Search Algorithm (Yang, 2007), Firefly Algorithm (Yang, 2008), Cuckoo Search Algorithm (Yang and Deb, 2009) and Bat Algorithm (Yang, 2010). For generating SVM-machine the original COBRA is used: each individual in all populations represents a set of kernel function‚Äôs parameters Œ±,Œ≤,d. Then for each individual constrained modification of COBRA is applied for finding vector w and shift factor b. And finally individual that showed the best classification rate is chosen as the designed classifier. 5 Experimental Results The DEFT (‚ÄúD√©fi Fouille de Texte‚Äù) Evaluation Package 2008 and publically available corpora from DEFT‚Äô07 (Books, Games and Debates) have be</context>
</contexts>
<marker>Yang, 2008</marker>
<rawString>Yang X.S. 2008. Nature-Inspired Metaheuristic Algorithms.</rawString>
</citation>
<citation valid="true">
<authors>
<author>X S Yang</author>
<author>S Deb</author>
</authors>
<title>Cuckoo search via Levy flights.</title>
<date>2009</date>
<booktitle>Proceedings of World Congress on Nature &amp; Biologically Inspired Computing:210-214.</booktitle>
<contexts>
<context position="12415" citStr="Yang and Deb, 2009" startWordPosition="2033" endWordPosition="2036">sponds to relative importance of negative precedents. The given document is put to the class with the nearest centroid. In this work we have applied Rocchio classifier with ùõæ ‚àà (0.1; 0.9) and with three different metrics: taxicab distance, Euclidean metric and cosine similarity. COBRA is a new meta-heuristic algorithm which has been proposed by Akhmedova and Semenkin (2013). It is based on cooperation of biology inspired algorithms such as Particle Swarm Optimization (Kennedy and Eberhart, 1995), Wolf Pack Search Algorithm (Yang, 2007), Firefly Algorithm (Yang, 2008), Cuckoo Search Algorithm (Yang and Deb, 2009) and Bat Algorithm (Yang, 2010). For generating SVM-machine the original COBRA is used: each individual in all populations represents a set of kernel function‚Äôs parameters Œ±,Œ≤,d. Then for each individual constrained modification of COBRA is applied for finding vector w and shift factor b. And finally individual that showed the best classification rate is chosen as the designed classifier. 5 Experimental Results The DEFT (‚ÄúD√©fi Fouille de Texte‚Äù) Evaluation Package 2008 and publically available corpora from DEFT‚Äô07 (Books, Games and Debates) have been used for algorithms application and results</context>
</contexts>
<marker>Yang, Deb, 2009</marker>
<rawString>Yang X.S. and Deb S. 2009. Cuckoo search via Levy flights. Proceedings of World Congress on Nature &amp; Biologically Inspired Computing:210-214.</rawString>
</citation>
<citation valid="true">
<authors>
<author>X S Yang</author>
</authors>
<title>A New Metaheuristic Bat-Inspired Algorithm.</title>
<date>2010</date>
<booktitle>Proceedings of Nature Inspired Co-</booktitle>
<contexts>
<context position="12446" citStr="Yang, 2010" startWordPosition="2040" endWordPosition="2041">ve precedents. The given document is put to the class with the nearest centroid. In this work we have applied Rocchio classifier with ùõæ ‚àà (0.1; 0.9) and with three different metrics: taxicab distance, Euclidean metric and cosine similarity. COBRA is a new meta-heuristic algorithm which has been proposed by Akhmedova and Semenkin (2013). It is based on cooperation of biology inspired algorithms such as Particle Swarm Optimization (Kennedy and Eberhart, 1995), Wolf Pack Search Algorithm (Yang, 2007), Firefly Algorithm (Yang, 2008), Cuckoo Search Algorithm (Yang and Deb, 2009) and Bat Algorithm (Yang, 2010). For generating SVM-machine the original COBRA is used: each individual in all populations represents a set of kernel function‚Äôs parameters Œ±,Œ≤,d. Then for each individual constrained modification of COBRA is applied for finding vector w and shift factor b. And finally individual that showed the best classification rate is chosen as the designed classifier. 5 Experimental Results The DEFT (‚ÄúD√©fi Fouille de Texte‚Äù) Evaluation Package 2008 and publically available corpora from DEFT‚Äô07 (Books, Games and Debates) have been used for algorithms application and results comparison. In order to evalua</context>
</contexts>
<marker>Yang, 2010</marker>
<rawString>Yang X.S. 2010. A New Metaheuristic Bat-Inspired Algorithm. Proceedings of Nature Inspired Co-</rawString>
</citation>
</citationList>
</algorithm>
</algorithms>
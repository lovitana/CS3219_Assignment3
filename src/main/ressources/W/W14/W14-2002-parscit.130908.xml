<?xml version="1.0" encoding="UTF-8"?>
<algorithms version="110505">
<algorithm name="SectLabel" version="110505">
<variant no="0" confidence="0.002344">
<title confidence="0.992964">
Investigating the role of entropy in sentence processing
</title>
<author confidence="0.996192">
Tal Linzen T. Florian Jaeger
</author>
<affiliation confidence="0.9574285">
Department of Linguistics Brain and Cognitive Sciences
New York University University of Rochester
</affiliation>
<email confidence="0.997461">
linzen@nyu.edu fjaeger@bcs.rochester.edu
</email>
<sectionHeader confidence="0.993851" genericHeader="abstract">
Abstract
</sectionHeader>
<bodyText confidence="0.999902105263158">
We outline four ways in which uncertainty
might affect comprehension difficulty in
human sentence processing. These four
hypotheses motivate a self-paced reading
experiment, in which we used verb sub-
categorization distributions to manipulate
the uncertainty over the next step in the
syntactic derivation (single step entropy)
and the surprisal of the verb’s comple-
ment. We additionally estimate word-
by-word surprisal and total entropy over
parses of the sentence using a probabilistic
context-free grammar (PCFG). Surprisal
and total entropy, but not single step en-
tropy, were significant predictors of read-
ing times in different parts of the sen-
tence. This suggests that a complete model
of sentence processing should incorporate
both entropy and surprisal.
</bodyText>
<sectionHeader confidence="0.998992" genericHeader="introduction">
1 Introduction
</sectionHeader>
<bodyText confidence="0.999967419354839">
Predictable linguistic elements are processed
faster than unpredictable ones. Specifically, pro-
cessing load on an element A in context C is lin-
early correlated with its surprisal, −lo92 P(A|C)
(Smith and Levy, 2013). This suggests that read-
ers maintain expectations as to the upcoming ele-
ments: likely elements are accessed or constructed
in advance of being read. While there is substan-
tial amount of work on the effect of predictability
on processing difficulty, the role (if any) of the dis-
tribution over expectations is less well understood.
Surprisal predicts that the distribution over
competing predicted elements should not affect
reading times: if the conditional probability of a
word A is P(A|C), reading times on the word
will be proportional to −lo92 P(A|C), regardless
of whether the remaining probability mass is dis-
tributed among two or a hundred options.
The entropy reduction hypothesis (Hale,
2003; Hale, 2006), on the other hand, accords
a central role to the distribution over predicted
parses. According to this hypothesis, an incom-
ing element is costly to process when it entails a
change from a state of high uncertainty (e.g., mul-
tiple equiprobable parses) to a state of low uncer-
tainty (e.g., one where a single parse is much more
likely than the others). Uncertainty is quantified
as the entropy of the distribution over complete
parses of the sentence; that is, if Ai is the set of
all possible parses of the sentence after word wi,
then the uncertainty following wi is given by
</bodyText>
<equation confidence="0.9599745">
1: Hwi = − P(a)lo92 P(a) (1)
a∈Ai
</equation>
<bodyText confidence="0.9810455">
Processing load in this hypothesis is propor-
tional to the entropy reduction caused by wn:1
</bodyText>
<equation confidence="0.999008">
ER(wn) = max{Hwn−1 − Hwn, 01 (2)
</equation>
<bodyText confidence="0.999613352941176">
A third hypothesis, which we term the com-
petition hypothesis, predicts that higher compe-
tition among potential outcomes should result in
increased processing load at the point at which
the competing parses are still valid (McRae et al.,
1998; Tabor and Tanenhaus, 1999). This contrasts
with the entropy reduction hypothesis, according
to which processing cost arises when competition
is resolved. Intuitively, the two hypotheses make
inversely correlated predictions: on average, there
will be less competition following words that re-
duce entropy. A recent study found that reading
times on wi correlated positively with entropy fol-
lowing wi, providing support for this hypothesis
(Roark et al., 2009).
The fourth hypothesis we consider, which we
term the commitment hypothesis, is derived from
</bodyText>
<footnote confidence="0.852695">
1No processing load is predicted for words that increase
uncertainty.
</footnote>
<page confidence="0.971779">
10
</page>
<note confidence="0.8260935">
Proceedings of the 2014 ACL Workshop on Cognitive Modeling and Computational Linguistics, pages 10–18,
Baltimore, Maryland USA, June 26 2014. c�2014 Association for Computational Linguistics
</note>
<figure confidence="0.997651733333333">
c
0.25
a
0.25
d
0.5
0.25
e
0.5
0.25
0.25
b
0.75
f
start
</figure>
<figureCaption confidence="0.999867">
Figure 1: Example language. Output strings are indicated
</figureCaption>
<bodyText confidence="0.981179761904762">
inside the nodes, and transition probabilities are indicated on
the edges. For example, the probability of the sentence bf is
0.5 × 0.75.
the event-related potential (ERP) literature on con-
textual constraint. Studies in this tradition have
compared the responses to a low-predictability
word across two types of context: high-constraint
contexts, in which there is a strong expectation for
a (different) word, and low-constraint ones, which
are not strongly predictive of any individual word.
There is increasing evidence for an ERP compo-
nent that responds to violations of a strong pre-
diction (Federmeier, 2007; Van Petten and Luka,
2012). This component can be interpreted as re-
flecting disproportional commitment to high prob-
ability predictions at the expense of lower proba-
bility ones, a more extreme version of the proposal
that low-probability parses are pruned in the pres-
ence of a high-probability parse (Jurafsky, 1996).
Surprisal is therefore expected to have a larger ef-
fect in high constraint contexts, in which entropy
was low before the word being read. Commitment
to a high probability prediction may also result in
increased processing load at the point at which the
commitment is made.
We illustrate these four hypotheses using the
simple language sketched in Figure 1. Consider
the predictions made by the four hypotheses for
the sentences ae and be. Surprisal predicts no dif-
ference in reading times between these sentences,
since the conditional probabilities of the words in
the two sentences are identical (0.5 and 0.25 re-
spectively).
The competition hypothesis predicts increased
reading times on the first word in ae compared to
be, because the entropy following a is higher than
the entropy following b (2 bits compared to 0.71).
Since all sentences in the language are two word
long, entropy goes down to 0 after the second word
in both sentences. This hypothesis therefore does
not predict a reading time difference on the second
word e.
Moving on to the entropy reduction hypothesis,
five of the six possible sentences in the language
have probability 0.5 × 0.25, and the sixth one (bf)
has probability 0.5 × 0.75. The full entropy of the
grammar is therefore 2.4 bits. The first word re-
duces entropy in both ae and be (to 2 and 0.71 bits
respectively), but entropy reduction is higher when
the first word is b. The entropy reduction hypoth-
esis therefore predicts longer reading times on the
first word in be than in ae. Conversely, since en-
tropy goes down to 0 in both cases, but from 2 bits
in ae compared to 0.71 bits in be, this hypothesis
predicts longer reading times on e in ae than in be.
Finally, the commitment hypothesis predicts
that after b the reader will become committed to
the prediction that the second word will be f. This
will lead to longer reading times on e in be than
in ae, despite the fact that its conditional proba-
bility is identical in both cases. If commitment to
a prediction entails additional work, this hypothe-
sis predicts longer reading times on the first word
when it is b.
This paper presents an reading time study that
aims to test these hypotheses. Empirical tests
of computational theories of sentence processing
have employed either reading time corpora (Dem-
berg and Keller, 2008) or controlled experimen-
tal materials (Yun et al., 2010). The current paper
adopts the latter approach, trading off a decrease
in lexical and syntactic heterogeneity for increased
control. This paper is divided into two parts. Sec-
tion 2 describes a reading time experiment, which
tested the predictions of the surprisal, competi-
tion and commitment hypotheses, as applied to the
entropy over the next single step in the syntactic
derivation.2 We then calculate the total entropy
(up to an unbounded number of derivation steps)
at each word using a PCFG; Section 3 describes
how this grammar was constructed, overviews the
predictions that it yielded in light of the four hy-
potheses, and evaluates these predictions on the re-
sults of the reading time experiment.
</bodyText>
<footnote confidence="0.975369">
2We do not test the predictions of the entropy reduction
hypothesis in this part of the paper, since that theory explicitly
only applies to total rather than single-step entropy.
</footnote>
<page confidence="0.99961">
11
</page>
<sectionHeader confidence="0.916993" genericHeader="method">
2 Reading time experiment
</sectionHeader>
<subsectionHeader confidence="0.853415">
2.1 Design
</subsectionHeader>
<bodyText confidence="0.982881452830189">
To keep syntactic structure constant while ma-
nipulating surprisal and entropy over the next
derivation step, we took advantage of the fact
that verbs vary in the probability distribution
of their syntactic complements (subcategorization
frames). Several studies have demonstrated that
readers are sensitive to subcategorization probabil-
ities (Trueswell et al., 1993; Garnsey et al., 1997).
The structure of the experimental materials is
shown in Table 1. In a 2x2x2 factorial design, we
crossed the surprisal of a sentential complement
(SC) given the verb, the entropy of the verb’s sub-
categorization distribution, and the presence or ab-
sence of the complementizer that. When the com-
plementizer is absent, the region the island is am-
biguous between a direct object and an embedded
subject.
Surprisal theory predicts an effect of SC sur-
prisal on the disambiguating region in ambiguous
sentences (sentences without that), as obtained in
previous studies (Garnsey et al., 1997), and an ef-
fect of SC surprisal on the complementizer that
in unambiguous sentences. Reading times should
not differ at the verb: in the minimal context we
used (the men), the surprisal of the verb should
be closely approximated by its lexical frequency,
which was matched across conditions.
The competition hypothesis predicts a positive
main effect of subcategorization frame entropy
(subcategorization frame entropy) at the verb:
higher uncertainty over the syntactic category of
the complement should result in slower reading
times.
The commitment hypothesis predicts that the
effect of surprisal in the disambiguating region
should be amplified when subcategorization frame
entropy is low, since the readers will have commit-
ted to the competing high probability frame. If the
commitment step in itself incurs a processing cost,
there should be a negative main effect of subcate-
gorization frame entropy at the verb.
This experimental design varies the entropy
over the single next derivation step: it assumes
that the parser only predicts the identity of the sub-
categorization frame, but not its internal structure.
Since the predictions of the entropy reduction hy-
pothesis crucially depend on predicting the inter-
nal structure as well, we defer the discussion of
that hypothesis until Section 3.
The men discovered (that) the island
mat. subj. verb that emb. subj.
had been invaded by the enemy.
emb. verb complex rest
</bodyText>
<tableCaption confidence="0.9917375">
Table 1: Structure of experimental materials (mat. = matrix,
emb. = embedded, subj. = subject).
</tableCaption>
<subsectionHeader confidence="0.958429">
2.2 Methods
2.2.1 Participants
</subsectionHeader>
<bodyText confidence="0.999845333333333">
128 participants were recruited through Amazon
Mechanical Turk and were paid $1.75 for their
participation.
</bodyText>
<subsectionHeader confidence="0.565811">
2.2.2 Materials
</subsectionHeader>
<bodyText confidence="0.999924344827586">
32 verbs were selected from the Gahl et al. (2004)
subcategorization frequency database, in 4 con-
ditions: high vs. low SC surprisal and high vs.
low subcategorization frame entropy (see Table 2).
Verbs were matched across conditions for length
in characters and for frequency in SUBTLEX-US
corpus (Brysbaert and New, 2009). A sentence
was created for each verb, following the structure
in Table 1. Each sentence had two versions: one
with the complementizer that after the verb and
one without it. The matrix subjects were mini-
mally informative two-word NPs (e.g. the men).
Following the complementizer (or the verb, if the
complementizer was omitted) was a definite NP
(the island), which was always a plausible direct
object of the matrix verb.
The embedded verb complex region consisted
of three words: two auxiliary verbs (had been) or
an auxiliary verb and negation (would not), fol-
lowed by a past participle form (invaded). Each
of the function words appeared the same num-
ber of times in each condition. The embedded
verb complex was followed by three more words.
The nouns and verbs in the embedded clause were
matched for frequency and length across condi-
tions.
In addition to the target sentences, the exper-
iment contained 64 filler sentences, with various
complex syntactic structures.
</bodyText>
<subsectionHeader confidence="0.910779">
2.2.3 Procedure
</subsectionHeader>
<bodyText confidence="0.9999835">
The sentences were presented word by word in a
self-paced moving window paradigm. The partic-
ipants were presented with a Y/N comprehension
question after each trial. The participants did not
</bodyText>
<page confidence="0.995636">
12
</page>
<table confidence="0.9975348">
NP Inf PP SC SC s. SFE
forget 0.55 0.14 0.2 0.09 3.46 1.7
hear 0.72 0 0.17 0.11 3.22 1.12
claim 0.36 0.12 0 0.45 1.15 1.71
sense 0.61 0 0.02 0.34 1.55 1.18
</table>
<tableCaption confidence="0.5974292">
Table 2: A example verb from each of the four conditions.
On the left, probabilities of complement types: noun phrase
(NP), infinitive (Inf), prepositional phrase (PP), sentential
complement (SC); on the right, SC surprisal and subcatego-
rization frame entropy.
</tableCaption>
<bodyText confidence="0.999836">
receive feedback on their responses. The experi-
ment was conducted online using a Flash applica-
tion written by Harry Tily (now at Nuance Com-
munications).
</bodyText>
<subsectionHeader confidence="0.981993">
2.2.4 Statistical analysis
</subsectionHeader>
<bodyText confidence="0.999950208333333">
Subjects were excluded if their answer accuracy
was lower than 75% (two subjects), or if their
mean reading time (RT) differed by more than
2.5 standard deviations from the overall mean RT
across subjects (two subjects). The results re-
ported in what follows are based on the remaining
124 subjects (97%).
We followed standard preprocessing procedure.
Individual words were excluded if their raw RT
was less than 100 ms or more than 2000 ms, or if
the log-transformed RT was more than 3 standard
deviations away from the participant’s mean. Log
RTs were length-corrected by taking the residuals
of a mixed-effects model (Bates et al., 2012) that
had log RT as the response variable, word length
as a fixed effect, and a by-subject intercept and
slope.
The length-corrected reading times were re-
gressed against the predictors of interest, sepa-
rately for each region. We used a maximal random
effect structure. All p values for fixed effects were
calculated using model comparison with a simpler
model with the same random effect structure that
did not contain that fixed effect.
</bodyText>
<subsectionHeader confidence="0.858583">
2.3 Results
</subsectionHeader>
<bodyText confidence="0.99996072972973">
Reading times on the matrix subject (the men) or
matrix verb (discovered) did not vary significantly
across conditions.
The embedded subject the island was read faster
in unambiguous sentences (p &lt; 0.001). Read-
ing times on this region were longer when SC sur-
prisal was high (p = 0.04). Models fitted to am-
biguous and unambiguous sentences separately re-
vealed that the simple effect of SC surprisal on the
embedded subject was significant for unambigu-
ous sentences (p = 0.02) but not for ambiguous
sentences (p = 0.46), though the interaction be-
tween SC surprisal and ambiguity did not reach
significance (p = 0.22).
The embedded verb complex (had been in-
vaded) was read faster in unambiguous than in am-
biguous sentences (p &lt; 0.001). Reading times
in this region were longer overall in the high SC
surprisal condition (p = 0.03). As expected, this
effect interacted with the presence of that (p =
0.01): the simple effect of SC surprisal was not
significant in unambiguous sentences (p = 0.28),
but was highly significant in ambiguous ones (p =
0.007). We did not find an interaction between SC
surprisal and subcategorization frame entropy (of
the sort predicted by the commitment hypothesis).
Subcategorization frame entropy did not have a
significant effect in any of the regions of the sen-
tence. It was only strictly predicted to have an ef-
fect on the matrix verb: longer reading times ac-
cording to the competition hypothesis, and (possi-
bly) shorter reading times according to the com-
mitment hypothesis. The absence of an subcat-
egorization frame entropy effect provides weak
support for the predictions of surprisal theory, ac-
cording to which entropy should not affect reading
times.
</bodyText>
<sectionHeader confidence="0.93486" genericHeader="method">
3 Deriving predictions from a PCFG
</sectionHeader>
<subsectionHeader confidence="0.999466">
3.1 Calculating entropy
</subsectionHeader>
<bodyText confidence="0.999648">
As mentioned above, the entropy of the next
derivation step following the current word (which
we term single-step entropy) is calculated as fol-
lows. If ai is a nonterminal, Hi is the set of rules
rewriting ai, and pr is the application probability
of rule r, then the single-step entropy of ai is given
by
</bodyText>
<equation confidence="0.985314">
�h(ai) = − pr log2 pr (3)
r∈Πi
</equation>
<figureCaption confidence="0.9777155">
Figure 3: Entropy calculation example: the single step en-
tropy after discover is 1 bit; the overall entropy is 1 + 0.5 ×
</figureCaption>
<figure confidence="0.986089217391304">
14 + 0.5 × 50 = 33 bits.
0.5
NP
(14 bits)
discover
0.5
SC
(50 bits)
13
Ambiguous
Unambiguous
Sentential complement
surprisal
Low
High
Subcategorization
entropy
Low
High
0.05
0.00
−0.05
−0.10
</figure>
<figureCaption confidence="0.999951">
Figure 2: Results of the self-paced reading experiment
</figureCaption>
<bodyText confidence="0.999766">
The entropy of all derivations starting with ai
(which we term total entropy) is then given by the
following recurrence:
</bodyText>
<equation confidence="0.959512333333333">
�
H(ai) = h(ai) +
r∈rli
</equation>
<bodyText confidence="0.999972230769231">
where ar,1, ... , ar,kr are the nonterminals on
the right-hand side of r. This recurrence has
a closed form solution (Wetherell, 1980; Hale,
2006). The expectation matrix A is a square ma-
trix with N rows and columns, where N is the set
of nonterminals. Each element Aij indicates the
expected number of times nonterminal aj will oc-
cur when ai is rewritten using exactly one rule of
the grammar. If h = (h1, ... , hN) is the vector of
all single-step entropy values for the N nontermi-
nal types in the grammar, and H = (H1, ... , HN)
is the vector of all total entropy values, then the
closed form solution for the recurrence is given by
</bodyText>
<equation confidence="0.998971">
H = (I − A)−1h (5)
</equation>
<bodyText confidence="0.99992515625">
where I is the identity matrix. The entropy af-
ter the first n words of the sentence, Hwn, can be
calculated by applying Equation 5 to the grammar
formed by intersecting the original grammar with
the prefix w1, ... , wn (i.e., considering only the
parses that are compatible with the words encoun-
tered so far) (Hale, 2006).
Two points are worth noting about these equa-
tions. First, Equation 5 shows that calculating the
entropy of a PCFG requires inverting the matrix
I − A, which is the size of the number of non-
terminal symbols in the grammar. This makes it
impractical to use a lexicalized grammar, as advo-
cated by Roark et al. (2009), since those grammars
have a very large number of nonterminal types.
Second, Equation 4 shows that the entropy of a
nonterminal is the sum of its single-step entropy
and a weighted average of entropy of the nonter-
minals it derives. In the context of subcategoriza-
tion decisions, the number of possible subcatego-
rization frames is small, and the single-step en-
tropy is on the order of magnitude of 1 or 2 bits.
The entropy of a typical complement, on the other
hand, is much higher (consider all of the possible
internal structures that an SC could have). This
means that the total entropy H after processing
the verb is dominated by the entropy of its po-
tential complements rather than the verb’s single-
step entropy h (see Figure 3 for an illustration). A
lookahead of a single word (as used in Roark et
al. (2009)) may therefore be only weakly related
to total entropy.
</bodyText>
<subsectionHeader confidence="0.999726">
3.2 Constructing the grammar
</subsectionHeader>
<bodyText confidence="0.999979111111111">
We used a PCFG induced from the Penn Treebank
(Marcus et al., 1993). As mentioned above, the
grammar was mostly unlexicalized; however, in
order for the predictions to depend on the identity
of the verb, the grammar had to contain lexically
specific rules for each verb. We discuss these rules
at end of this section.
The Penn Treebank tag set is often expanded
by adding to each node’s tag an annotation of the
</bodyText>
<equation confidence="0.8783658">
H(ar,j) (4)
pr
kr
L
j=1
</equation>
<page confidence="0.99248">
14
</page>
<bodyText confidence="0.999940525">
node’s parent, e.g., marking an NP whose parent
is a VP as NP VP (Klein and Manning, 2003).
While systematic parent annotation would have in-
creased the size of the grammar dramatically, we
did take the following minimal steps to improve
parsing accuracy. First, the word that is tagged
in the Penn Treebank as a preposition (IN) when
it occurs as a subordinating conjunction. This re-
sulted in SCs being erroneously parsed as preposi-
tional phrases. To deal with this issue, we replaced
the generic IN with IN[that] whenever it referred
to that.
Second, the parser assigned high probability
parses to reduced relative clauses in implausible
contexts. We made sure that cases that should not
be reduced relative clauses were not parsed as such
by splitting the VP category into sub-categories
based on the leftmost child of the VP (since only
VP[VPN] should be able to be a reduced rela-
tive), and by splitting SBAR into SBAR[overt]
when the SBAR had an overt complementizer and
SBAR[none] when it did not.
Following standard practice, we removed gram-
matical role information and filler-gap annota-
tions, e.g., NP-SUBJ-2 was treated as NP. To re-
duce the number of rules in the grammar as much
as possible, we removed punctuation and the silent
element NONE (used to mark gaps, silent comple-
mentizers, etc.), rules that occurred less than 100
times (out of the total 1320490 nonterminal pro-
ductions), and rules that had a probability of less
than 0.01. These steps resulted in the removal of
13%, 14% and 10% rule tokens respectively. We
then applied horizontal Markovization (Klein and
Manning, 2003).
Finally, we added lexically specific rules to
capture the verbs’ subcategorization preferences,
based on the Gahl et al. (2004) subcategorization
database. The probability of frame fj following
verb vi was calculated as:
</bodyText>
<equation confidence="0.979188666666667">
1 P(vi)P(fj|vi)
P(VP[VBD] → vi fj) = (6)
2 Ei P(vi)
</equation>
<bodyText confidence="0.999986142857143">
In other words, half of the probability mass of
production rules deriving VP[VBD] (VP headed
by past tense verbs) was taken away from the un-
lexicalized rules and assigned to the verb-specific
rules. The same procedure was performed for
VP[VBN] (VP headed by a past participle, with
the exception of the verbs forgot and wrote, which
are not ambiguous between the past and past par-
ticiple forms. The total probability of all rules de-
riving VP as a specific verb (e.g., discovered) was
estimated as the corpus frequency of that verb di-
vided by the total corpus frequency of all 32 verbs
used in the experiment, yielding a normalized es-
timate of the relative frequency of that verb.
</bodyText>
<subsectionHeader confidence="0.9707845">
3.3 Surprisal, entropy and entropy reduction
profiles
</subsectionHeader>
<bodyText confidence="0.999917634146341">
Word-by-word surprisal, entropy and entropy re-
duction values for each item were derived from the
equations in Section 3.1 using the Cornell Con-
ditional Probability Calculator (provided by John
Hale). Figure 4 shows the predictions averaged by
the conditions of the factorial design. Surprisal on
the verb is always high because this is the only part
of the grammar that encodes lexical identity; sur-
prisal on the verb therefore conflates lexical and
syntactic surprisal. Surprisal values on all other
words are low, with the exception of the point
at which the reader gets the information that the
verb’s complement is an SC: the embedded verb
complex in ambiguous sentences, and the comple-
mentizer in unambiguous sentence.
The entropy profile is dominated by the fact that
SCs have much higher internal entropy than NPs.
As a consequence, entropy after the verb is higher
whenever an SC is a more likely subcategorization
frame. The entropy after high subcategorization
frame entropy verbs is higher than that after low
subcategorization frame entropy verbs, though the
difference is small in comparison to the effect of
SC surprisal. In ambiguous sentences, entropy re-
mains higher for low SC surprisal verbs through-
out the ambiguous region. Somewhat counterin-
tuitively, entropy increases when the parse is dis-
ambiguated in favor of an SC. This is again a
consequence of the higher internal entropy of a
SC: the entropy of the ambiguity between SC and
NP is dwarfed by the internal entropy of a SC.
The entropy profile for unambiguous sentences
is straightforward: it increases sharply when the
reader finds out that the complement is a SC, then
decreases gradually as more details are revealed
about the internal structure of the SC.
The reading time predictions made by the en-
tropy reduction hypothesis are therefore very dif-
ferent than those made by surprisal theory. On
the verb, the entropy reduction hypothesis predicts
that high SC surprisal verbs will be read more
</bodyText>
<page confidence="0.953311">
15
</page>
<figure confidence="0.999531176470588">
Ambiguous
● ●
●
●
●
●
●
● ●
●
●
Unambiguous
●
●
●
●
●
●
●
Surprisal
●
●
●
●
Entropy
Ent. Reduction
● ●
●
●
●
●
●
●
●
●
●
●
●
●●
●●
●
● ●
● ●
●
●
●
●
●
●
● ●
● ● ●●●
●● ●
●
●
10
0
20
10
20
10
0
Sentential complement
surprisal
Low
High
Subcategorization
entropy
Low
High
</figure>
<figureCaption confidence="0.9980825">
Figure 4: Parser-derived surprisal, entropy and entropy reduction estimates for the stimuli in our experiments, averaged within
each condition of the factorial design (first word of sentence and rest region excluded).
</figureCaption>
<bodyText confidence="0.9998787">
slowly than low SC surprisal verbs, whereas sur-
prisal predicts no difference. On the disambiguat-
ing region in ambiguous sentences, the entropy re-
duction hypothesis predicts no reading time dif-
ferences at all, since an increase in entropy is
not predicted to affect reading times. In fact, en-
tropy reduction on the word had is positive only in
unambiguous sentences, so the entropy reduction
hypothesis predicts a slowdown in unambiguous
compared to ambiguous sentences.
</bodyText>
<subsectionHeader confidence="0.997815">
3.4 Evaluation on reading times
</subsectionHeader>
<bodyText confidence="0.999296017857143">
We tested whether reading times could be pre-
dicted by the word-by-word estimates derived
from the PCFG. Since total entropy, entropy re-
duction and surprisal values did not line up with
the factorial design, we used continuous regres-
sion instead, again using lme4 with a maximal ran-
dom effects structure. We only analyzed words
for which the predictions depended on the prop-
erties of the verb (as Figure 4 shows, this is only
the case for a minority of the words). As outcome
variables, we considered both reading times on the
word wi, and a spillover variable computed as the
sum of the reading times on wi and the next word
wi+1. The predictors were standardized (sepa-
rately for each word) to facilitate effect compar-
ison.
Parser-derived entropy reduction values varied
the most on the main verb. Since the word follow-
ing the verb differs between the ambiguous and
unambiguous conditions, we added a categorical
control variable for sentence ambiguity. In the
resulting model, lower entropy (or equivalently,
higher entropy reduction values), caused an in-
crease in reading times (no spillover: , ˆ = 0.014,
p = 0.05; one word spillover: , ˆ = 0.022, p =
0.04). Our design does not enable us to determine
whether the effect of entropy on the verb is due to
entropy reduction or simply entropy. The commit-
ment hypothesis is therefore equally supported by
this pattern as is the entropy reduction hypothesis.
The only other word on which entropy reduc-
tion values varied across verbs was the first word
the of the ambiguous region. Neither entropy re-
duction nor surprisal were significant predictors of
reading times on this word.
There was also some variation across verbs in
entropy (though not entropy reduction) on the sec-
ond word of the embedded subject (island) in am-
biguous sentences; however, entropy was not a
significant predictor of reading times on that word.
In general, entropy is much higher in the embed-
ded subject region in unambiguous than ambigu-
ous sentences, since it is already known that the
complement is an SC, and the entropy of an SC
is higher. Yet as mentioned above, reading times
on the embedded subject were higher when it was
ambiguous (p &lt; 0.001).
Finally, PCFG-based surprisal was a significant
predictor of reading times on the disambiguating
word in ambiguous sentences (no spillover: n.s.;
one word spillover: βˆ = 0.037, p = 0.02; two-
word spillover: βˆ = 0.058, p = 0.001). In con-
trast with simple SC surprisal (see Section 2.2.4),
PCFG-based surprisal was not a significant predic-
tor of reading times on the complementizer that in
unambiguous sentences.
</bodyText>
<sectionHeader confidence="0.999863" genericHeader="method">
4 Discussion
</sectionHeader>
<bodyText confidence="0.999994666666667">
We presented four hypotheses as to the role of en-
tropy in syntactic processing, and evaluated them
on the results of a reading time study. We did not
find significant effects of subcategorization frame
entropy, which is the entropy over the next deriva-
tion step following the verb. Entropy over com-
plete derivations, on the other hand, was a signifi-
cant predictor of reading time on the verb. The ef-
fect went in the direction predicted by the entropy
reduction and commitment hypotheses, and oppo-
site to that predicted by the competition hypothe-
sis: reading times were higher when post-verb en-
tropy was lower.
Reading times on the embedded subject in am-
biguous sentences were increased compared to un-
ambiguous sentences. This can be seen as sup-
porting the competition hypothesis: the SC and
NP parses both need to be maintained, which in-
creases processing cost. Yet the parser predic-
tions showed that total entropy on the embedded
subject was higher in unambiguous than ambigu-
ous sentences, since the probability of the high-
entropy sentential complement is 1 in unambigu-
ous sentences. In this case, then, total entropy,
which entails searching enormous amounts of pre-
dicted structure, may not be the right measure, and
single-step (or n-step) entropy may be a better pre-
dictor.
In related work, Frank (2013) tested a version of
the entropy reduction hypothesis whereby entropy
reduction was not bounded by 0 (was allowed to
take negative values). A Simple Recurrent Net-
work was used to predict the next four words in
the sentence; the uncertainty following the current
word was estimated as the entropy of this quadri-
gram distribution. Higher (modified) entropy re-
duction resulted in increased reading times. These
results are not directly comparable to the present
results, however. Frank (2013) tested a theory that
takes into account both positive and negative en-
tropy changes. In addition, a four-word lookahead
may not capture the dramatic difference in internal
entropy between SCs and NPs, which is responsi-
ble for the differential reading times predicted on
the matrix. This caveat applies even more strongly
to the one-word lookahead in Roark et al. (2009).
In contrast with much previous work, we cal-
culated total entropy using a realistic PCFG ac-
quired from a Treebank corpus. In future work,
this method can be used to investigate the ef-
fect of entropy in a naturalistic reading time cor-
pus. It will be important to explore the extent to
which the reading time predictions derived from
the grammar are affected by representational de-
cisions (e.g., the parent annotations we used in
Section 3.2). This applies in particular to entropy,
which is sensitive to the distribution over syntactic
parses active at the word; surprisal depends only
the conditional probability assigned to the word
by the grammar, irrespective of the number and
distribution over the parses that predict the current
word, and is therefore somewhat less sensitive to
representational assumptions.
</bodyText>
<sectionHeader confidence="0.999127" genericHeader="conclusions">
5 Conclusion
</sectionHeader>
<bodyText confidence="0.999982363636364">
This paper described four hypotheses regarding
the role of uncertainty in sentence processing. A
reading time study replicated a known effect of
surprisal, and found a previously undocumented
effect of entropy. Entropy predicted reading times
only when it was calculated over complete deriva-
tions of the sentence, and not when it was calcu-
lated over the single next derivation step. Our re-
sults suggest that a full theory of sentence process-
ing would need to take both surprisal and uncer-
tainty into account.
</bodyText>
<sectionHeader confidence="0.998861" genericHeader="acknowledgments">
Acknowledgments
</sectionHeader>
<bodyText confidence="0.98106575">
We thank Alec Marantz for discussion and An-
drew Watts for technical assistance. This work
was supported by an Alfred P. Sloan Fellowship
to T. Florian Jaeger.
</bodyText>
<page confidence="0.998715">
17
</page>
<sectionHeader confidence="0.995893" genericHeader="references">
References
</sectionHeader>
<reference confidence="0.9999387125">
D. Bates, M. Maechler, and B. Bolker, 2012. lme4:
Linear mixed-effects models using S4 classes. R
package version 0.999999-0.
M. Brysbaert and B. New. 2009. Moving beyond
Kuˇcera and Francis: A critical evaluation of cur-
rent word frequency norms and the introduction of
a new and improved word frequency measure for
American English. Behavior Research Methods,
41(4):977–990.
V. Demberg and F. Keller. 2008. Data from eye-
tracking corpora as evidence for theories of syntactic
processing complexity. Cognition, 109(2):193–210.
K. D. Federmeier. 2007. Thinking ahead: The role
and roots of prediction in language comprehension.
Psychophysiology, 44(4):491–505.
S. L. Frank. 2013. Uncertainty reduction as a measure
of cognitive load in sentence comprehension. Topics
in Cognitive Science, 5(3):475–494.
S. Gahl, D. Jurafsky, and D. Roland. 2004. Verb
subcategorization frequencies: American English
corpus data, methodological studies, and cross-
corpus comparisons. Behavior Research Methods,
36(3):432–443.
S. Garnsey, N. Pearlmutter, E. Myers, and M. Lotocky.
1997. The contributions of verb bias and plausi-
bility to the comprehension of temporarily ambigu-
ous sentences. Journal of Memory and Language,
37(1):58–93.
J. Hale. 2003. The information conveyed by words
in sentences. Journal of Psycholinguistic Research,
32(2):101–123.
J. Hale. 2006. Uncertainty about the rest of the sen-
tence. Cognitive Science, 30(4):643–672.
D. Jurafsky. 1996. A probabilistic model of lexical
and syntactic access and disambiguation. Cognitive
Science, 20(2):137–194.
D. Klein and C. D. Manning. 2003. Accurate un-
lexicalized parsing. In Proceedings of the 41st
Annual Meeting on Association for Computational
Linguistics-Volume 1, pages 423–430. Association
for Computational Linguistics.
M. P. Marcus, M. A. Marcinkiewicz, and B. Santorini.
1993. Building a large annotated corpus of En-
glish: The Penn Treebank. Computational Linguis-
tics, 19(2):313–330.
K. McRae, M. Spivey-Knowlton, and M. Tanenhaus.
1998. Modeling the influence of thematic fit (and
other constraints) in on-line sentence comprehen-
sion. Journal of Memory and Language, 38(3):283–
312.
B. Roark, A. Bachrach, C. Cardenas, and C. Pallier.
2009. Deriving lexical and syntactic expectation-
based measures for psycholinguistic modeling via
incremental top-down parsing. In Proceedings of
the 2009 Conference on Empirical Methods in Nat-
ural Language Processing: Volume 1-Volume 1,
pages 324–333. Association for Computational Lin-
guistics.
N. J. Smith and R. Levy. 2013. The effect of word
predictability on reading time is logarithmic. Cog-
nition, 128(3):302–319.
W. Tabor and M. K. Tanenhaus. 1999. Dynamical
models of sentence processing. Cognitive Science,
23(4):491–515.
J. Trueswell, M. Tanenhaus, and C. Kello. 1993. Verb-
specific constraints in sentence processing: Sep-
arating effects of lexical preference from garden-
paths. Journal of Experimental Psychology: Learn-
ing, Memory, and Cognition, 19(3):528–553.
C. Van Petten and B. Luka. 2012. Prediction during
language comprehension: Benefits, costs, and ERP
components. International Journal of Psychophysi-
ology, 83(2):176–190.
C. S. Wetherell. 1980. Probabilistic languages: A re-
view and some open questions. ACM Computing
Surveys (CSUR), 12(4):361–379.
J. Yun, J. Whitman, and J. Hale. 2010. Subject-object
asymmetries in Korean sentence comprehension. In
Proceedings of the 32nd Annual Meeting of the Cog-
nitive Science Society.
</reference>
<page confidence="0.999291">
18
</page>
</variant>
</algorithm>
<algorithm name="ParsHed" version="110505">
<variant no="0" confidence="0.898246">
<title confidence="0.999816">Investigating the role of entropy in sentence processing</title>
<author confidence="0.999956">Tal Linzen T Florian Jaeger</author>
<affiliation confidence="0.951149">Department of Linguistics Brain and Cognitive Sciences New York University University of Rochester</affiliation>
<email confidence="0.999785">linzen@nyu.edufjaeger@bcs.rochester.edu</email>
<abstract confidence="0.9996674">We outline four ways in which uncertainty might affect comprehension difficulty in human sentence processing. These four hypotheses motivate a self-paced reading experiment, in which we used verb subcategorization distributions to manipulate the uncertainty over the next step in the syntactic derivation (single step entropy) and the surprisal of the verb’s complement. We additionally estimate wordby-word surprisal and total entropy over parses of the sentence using a probabilistic context-free grammar (PCFG). Surprisal and total entropy, but not single step entropy, were significant predictors of reading times in different parts of the sentence. This suggests that a complete model of sentence processing should incorporate both entropy and surprisal.</abstract>
</variant>
</algorithm>
<algorithm name="ParsCit" version="110505">
<citationList>
<citation valid="true">
<authors>
<author>D Bates</author>
<author>M Maechler</author>
<author>B Bolker</author>
</authors>
<title>lme4: Linear mixed-effects models using S4 classes. R package version 0.999999-0.</title>
<date>2012</date>
<contexts>
<context position="13467" citStr="Bates et al., 2012" startWordPosition="2162" endWordPosition="2165">ed if their answer accuracy was lower than 75% (two subjects), or if their mean reading time (RT) differed by more than 2.5 standard deviations from the overall mean RT across subjects (two subjects). The results reported in what follows are based on the remaining 124 subjects (97%). We followed standard preprocessing procedure. Individual words were excluded if their raw RT was less than 100 ms or more than 2000 ms, or if the log-transformed RT was more than 3 standard deviations away from the participant’s mean. Log RTs were length-corrected by taking the residuals of a mixed-effects model (Bates et al., 2012) that had log RT as the response variable, word length as a fixed effect, and a by-subject intercept and slope. The length-corrected reading times were regressed against the predictors of interest, separately for each region. We used a maximal random effect structure. All p values for fixed effects were calculated using model comparison with a simpler model with the same random effect structure that did not contain that fixed effect. 2.3 Results Reading times on the matrix subject (the men) or matrix verb (discovered) did not vary significantly across conditions. The embedded subject the islan</context>
</contexts>
<marker>Bates, Maechler, Bolker, 2012</marker>
<rawString>D. Bates, M. Maechler, and B. Bolker, 2012. lme4: Linear mixed-effects models using S4 classes. R package version 0.999999-0.</rawString>
</citation>
<citation valid="true">
<authors>
<author>M Brysbaert</author>
<author>B New</author>
</authors>
<title>Moving beyond Kuˇcera and Francis: A critical evaluation of current word frequency norms and the introduction of a new and improved word frequency measure for American English.</title>
<date>2009</date>
<journal>Behavior Research Methods,</journal>
<volume>41</volume>
<issue>4</issue>
<contexts>
<context position="11047" citStr="Brysbaert and New, 2009" startWordPosition="1761" endWordPosition="1764"> been invaded by the enemy. emb. verb complex rest Table 1: Structure of experimental materials (mat. = matrix, emb. = embedded, subj. = subject). 2.2 Methods 2.2.1 Participants 128 participants were recruited through Amazon Mechanical Turk and were paid $1.75 for their participation. 2.2.2 Materials 32 verbs were selected from the Gahl et al. (2004) subcategorization frequency database, in 4 conditions: high vs. low SC surprisal and high vs. low subcategorization frame entropy (see Table 2). Verbs were matched across conditions for length in characters and for frequency in SUBTLEX-US corpus (Brysbaert and New, 2009). A sentence was created for each verb, following the structure in Table 1. Each sentence had two versions: one with the complementizer that after the verb and one without it. The matrix subjects were minimally informative two-word NPs (e.g. the men). Following the complementizer (or the verb, if the complementizer was omitted) was a definite NP (the island), which was always a plausible direct object of the matrix verb. The embedded verb complex region consisted of three words: two auxiliary verbs (had been) or an auxiliary verb and negation (would not), followed by a past participle form (in</context>
</contexts>
<marker>Brysbaert, New, 2009</marker>
<rawString>M. Brysbaert and B. New. 2009. Moving beyond Kuˇcera and Francis: A critical evaluation of current word frequency norms and the introduction of a new and improved word frequency measure for American English. Behavior Research Methods, 41(4):977–990.</rawString>
</citation>
<citation valid="true">
<authors>
<author>V Demberg</author>
<author>F Keller</author>
</authors>
<title>Data from eyetracking corpora as evidence for theories of syntactic processing complexity.</title>
<date>2008</date>
<journal>Cognition,</journal>
<volume>109</volume>
<issue>2</issue>
<contexts>
<context position="7096" citStr="Demberg and Keller, 2008" startWordPosition="1141" endWordPosition="1145"> Finally, the commitment hypothesis predicts that after b the reader will become committed to the prediction that the second word will be f. This will lead to longer reading times on e in be than in ae, despite the fact that its conditional probability is identical in both cases. If commitment to a prediction entails additional work, this hypothesis predicts longer reading times on the first word when it is b. This paper presents an reading time study that aims to test these hypotheses. Empirical tests of computational theories of sentence processing have employed either reading time corpora (Demberg and Keller, 2008) or controlled experimental materials (Yun et al., 2010). The current paper adopts the latter approach, trading off a decrease in lexical and syntactic heterogeneity for increased control. This paper is divided into two parts. Section 2 describes a reading time experiment, which tested the predictions of the surprisal, competition and commitment hypotheses, as applied to the entropy over the next single step in the syntactic derivation.2 We then calculate the total entropy (up to an unbounded number of derivation steps) at each word using a PCFG; Section 3 describes how this grammar was constr</context>
</contexts>
<marker>Demberg, Keller, 2008</marker>
<rawString>V. Demberg and F. Keller. 2008. Data from eyetracking corpora as evidence for theories of syntactic processing complexity. Cognition, 109(2):193–210.</rawString>
</citation>
<citation valid="true">
<authors>
<author>K D Federmeier</author>
</authors>
<title>Thinking ahead: The role and roots of prediction in language comprehension.</title>
<date>2007</date>
<journal>Psychophysiology,</journal>
<volume>44</volume>
<issue>4</issue>
<contexts>
<context position="4458" citStr="Federmeier, 2007" startWordPosition="693" endWordPosition="694">ated inside the nodes, and transition probabilities are indicated on the edges. For example, the probability of the sentence bf is 0.5 × 0.75. the event-related potential (ERP) literature on contextual constraint. Studies in this tradition have compared the responses to a low-predictability word across two types of context: high-constraint contexts, in which there is a strong expectation for a (different) word, and low-constraint ones, which are not strongly predictive of any individual word. There is increasing evidence for an ERP component that responds to violations of a strong prediction (Federmeier, 2007; Van Petten and Luka, 2012). This component can be interpreted as reflecting disproportional commitment to high probability predictions at the expense of lower probability ones, a more extreme version of the proposal that low-probability parses are pruned in the presence of a high-probability parse (Jurafsky, 1996). Surprisal is therefore expected to have a larger effect in high constraint contexts, in which entropy was low before the word being read. Commitment to a high probability prediction may also result in increased processing load at the point at which the commitment is made. We illus</context>
</contexts>
<marker>Federmeier, 2007</marker>
<rawString>K. D. Federmeier. 2007. Thinking ahead: The role and roots of prediction in language comprehension. Psychophysiology, 44(4):491–505.</rawString>
</citation>
<citation valid="true">
<authors>
<author>S L Frank</author>
</authors>
<title>Uncertainty reduction as a measure of cognitive load in sentence comprehension.</title>
<date>2013</date>
<journal>Topics in Cognitive Science,</journal>
<volume>5</volume>
<issue>3</issue>
<contexts>
<context position="28622" citStr="Frank (2013)" startWordPosition="4769" endWordPosition="4770">to unambiguous sentences. This can be seen as supporting the competition hypothesis: the SC and NP parses both need to be maintained, which increases processing cost. Yet the parser predictions showed that total entropy on the embedded subject was higher in unambiguous than ambiguous sentences, since the probability of the highentropy sentential complement is 1 in unambiguous sentences. In this case, then, total entropy, which entails searching enormous amounts of predicted structure, may not be the right measure, and single-step (or n-step) entropy may be a better predictor. In related work, Frank (2013) tested a version of the entropy reduction hypothesis whereby entropy reduction was not bounded by 0 (was allowed to take negative values). A Simple Recurrent Network was used to predict the next four words in the sentence; the uncertainty following the current word was estimated as the entropy of this quadrigram distribution. Higher (modified) entropy reduction resulted in increased reading times. These results are not directly comparable to the present results, however. Frank (2013) tested a theory that takes into account both positive and negative entropy changes. In addition, a four-word l</context>
</contexts>
<marker>Frank, 2013</marker>
<rawString>S. L. Frank. 2013. Uncertainty reduction as a measure of cognitive load in sentence comprehension. Topics in Cognitive Science, 5(3):475–494.</rawString>
</citation>
<citation valid="true">
<authors>
<author>S Gahl</author>
<author>D Jurafsky</author>
<author>D Roland</author>
</authors>
<title>Verb subcategorization frequencies: American English corpus data, methodological studies, and crosscorpus comparisons.</title>
<date>2004</date>
<journal>Behavior Research Methods,</journal>
<volume>36</volume>
<issue>3</issue>
<contexts>
<context position="10775" citStr="Gahl et al. (2004)" startWordPosition="1720" endWordPosition="1723">l structure. Since the predictions of the entropy reduction hypothesis crucially depend on predicting the internal structure as well, we defer the discussion of that hypothesis until Section 3. The men discovered (that) the island mat. subj. verb that emb. subj. had been invaded by the enemy. emb. verb complex rest Table 1: Structure of experimental materials (mat. = matrix, emb. = embedded, subj. = subject). 2.2 Methods 2.2.1 Participants 128 participants were recruited through Amazon Mechanical Turk and were paid $1.75 for their participation. 2.2.2 Materials 32 verbs were selected from the Gahl et al. (2004) subcategorization frequency database, in 4 conditions: high vs. low SC surprisal and high vs. low subcategorization frame entropy (see Table 2). Verbs were matched across conditions for length in characters and for frequency in SUBTLEX-US corpus (Brysbaert and New, 2009). A sentence was created for each verb, following the structure in Table 1. Each sentence had two versions: one with the complementizer that after the verb and one without it. The matrix subjects were minimally informative two-word NPs (e.g. the men). Following the complementizer (or the verb, if the complementizer was omitted</context>
<context position="20867" citStr="Gahl et al. (2004)" startWordPosition="3447" endWordPosition="3450">NP-SUBJ-2 was treated as NP. To reduce the number of rules in the grammar as much as possible, we removed punctuation and the silent element NONE (used to mark gaps, silent complementizers, etc.), rules that occurred less than 100 times (out of the total 1320490 nonterminal productions), and rules that had a probability of less than 0.01. These steps resulted in the removal of 13%, 14% and 10% rule tokens respectively. We then applied horizontal Markovization (Klein and Manning, 2003). Finally, we added lexically specific rules to capture the verbs’ subcategorization preferences, based on the Gahl et al. (2004) subcategorization database. The probability of frame fj following verb vi was calculated as: 1 P(vi)P(fj|vi) P(VP[VBD] → vi fj) = (6) 2 Ei P(vi) In other words, half of the probability mass of production rules deriving VP[VBD] (VP headed by past tense verbs) was taken away from the unlexicalized rules and assigned to the verb-specific rules. The same procedure was performed for VP[VBN] (VP headed by a past participle, with the exception of the verbs forgot and wrote, which are not ambiguous between the past and past participle forms. The total probability of all rules deriving VP as a specifi</context>
</contexts>
<marker>Gahl, Jurafsky, Roland, 2004</marker>
<rawString>S. Gahl, D. Jurafsky, and D. Roland. 2004. Verb subcategorization frequencies: American English corpus data, methodological studies, and crosscorpus comparisons. Behavior Research Methods, 36(3):432–443.</rawString>
</citation>
<citation valid="true">
<authors>
<author>S Garnsey</author>
<author>N Pearlmutter</author>
<author>E Myers</author>
<author>M Lotocky</author>
</authors>
<title>The contributions of verb bias and plausibility to the comprehension of temporarily ambiguous sentences.</title>
<date>1997</date>
<journal>Journal of Memory and Language,</journal>
<volume>37</volume>
<issue>1</issue>
<contexts>
<context position="8464" citStr="Garnsey et al., 1997" startWordPosition="1356" endWordPosition="1359">e experiment. 2We do not test the predictions of the entropy reduction hypothesis in this part of the paper, since that theory explicitly only applies to total rather than single-step entropy. 11 2 Reading time experiment 2.1 Design To keep syntactic structure constant while manipulating surprisal and entropy over the next derivation step, we took advantage of the fact that verbs vary in the probability distribution of their syntactic complements (subcategorization frames). Several studies have demonstrated that readers are sensitive to subcategorization probabilities (Trueswell et al., 1993; Garnsey et al., 1997). The structure of the experimental materials is shown in Table 1. In a 2x2x2 factorial design, we crossed the surprisal of a sentential complement (SC) given the verb, the entropy of the verb’s subcategorization distribution, and the presence or absence of the complementizer that. When the complementizer is absent, the region the island is ambiguous between a direct object and an embedded subject. Surprisal theory predicts an effect of SC surprisal on the disambiguating region in ambiguous sentences (sentences without that), as obtained in previous studies (Garnsey et al., 1997), and an effec</context>
</contexts>
<marker>Garnsey, Pearlmutter, Myers, Lotocky, 1997</marker>
<rawString>S. Garnsey, N. Pearlmutter, E. Myers, and M. Lotocky. 1997. The contributions of verb bias and plausibility to the comprehension of temporarily ambiguous sentences. Journal of Memory and Language, 37(1):58–93.</rawString>
</citation>
<citation valid="true">
<authors>
<author>J Hale</author>
</authors>
<title>The information conveyed by words in sentences.</title>
<date>2003</date>
<journal>Journal of Psycholinguistic Research,</journal>
<volume>32</volume>
<issue>2</issue>
<contexts>
<context position="1926" citStr="Hale, 2003" startWordPosition="286" endWordPosition="287">ents are accessed or constructed in advance of being read. While there is substantial amount of work on the effect of predictability on processing difficulty, the role (if any) of the distribution over expectations is less well understood. Surprisal predicts that the distribution over competing predicted elements should not affect reading times: if the conditional probability of a word A is P(A|C), reading times on the word will be proportional to −lo92 P(A|C), regardless of whether the remaining probability mass is distributed among two or a hundred options. The entropy reduction hypothesis (Hale, 2003; Hale, 2006), on the other hand, accords a central role to the distribution over predicted parses. According to this hypothesis, an incoming element is costly to process when it entails a change from a state of high uncertainty (e.g., multiple equiprobable parses) to a state of low uncertainty (e.g., one where a single parse is much more likely than the others). Uncertainty is quantified as the entropy of the distribution over complete parses of the sentence; that is, if Ai is the set of all possible parses of the sentence after word wi, then the uncertainty following wi is given by 1: Hwi = </context>
</contexts>
<marker>Hale, 2003</marker>
<rawString>J. Hale. 2003. The information conveyed by words in sentences. Journal of Psycholinguistic Research, 32(2):101–123.</rawString>
</citation>
<citation valid="true">
<authors>
<author>J Hale</author>
</authors>
<title>Uncertainty about the rest of the sentence.</title>
<date>2006</date>
<journal>Cognitive Science,</journal>
<volume>30</volume>
<issue>4</issue>
<contexts>
<context position="1939" citStr="Hale, 2006" startWordPosition="288" endWordPosition="289">essed or constructed in advance of being read. While there is substantial amount of work on the effect of predictability on processing difficulty, the role (if any) of the distribution over expectations is less well understood. Surprisal predicts that the distribution over competing predicted elements should not affect reading times: if the conditional probability of a word A is P(A|C), reading times on the word will be proportional to −lo92 P(A|C), regardless of whether the remaining probability mass is distributed among two or a hundred options. The entropy reduction hypothesis (Hale, 2003; Hale, 2006), on the other hand, accords a central role to the distribution over predicted parses. According to this hypothesis, an incoming element is costly to process when it entails a change from a state of high uncertainty (e.g., multiple equiprobable parses) to a state of low uncertainty (e.g., one where a single parse is much more likely than the others). Uncertainty is quantified as the entropy of the distribution over complete parses of the sentence; that is, if Ai is the set of all possible parses of the sentence after word wi, then the uncertainty following wi is given by 1: Hwi = − P(a)lo92 P(</context>
<context position="16658" citStr="Hale, 2006" startWordPosition="2707" endWordPosition="2708">le step entropy after discover is 1 bit; the overall entropy is 1 + 0.5 × 14 + 0.5 × 50 = 33 bits. 0.5 NP (14 bits) discover 0.5 SC (50 bits) 13 Ambiguous Unambiguous Sentential complement surprisal Low High Subcategorization entropy Low High 0.05 0.00 −0.05 −0.10 Figure 2: Results of the self-paced reading experiment The entropy of all derivations starting with ai (which we term total entropy) is then given by the following recurrence: � H(ai) = h(ai) + r∈rli where ar,1, ... , ar,kr are the nonterminals on the right-hand side of r. This recurrence has a closed form solution (Wetherell, 1980; Hale, 2006). The expectation matrix A is a square matrix with N rows and columns, where N is the set of nonterminals. Each element Aij indicates the expected number of times nonterminal aj will occur when ai is rewritten using exactly one rule of the grammar. If h = (h1, ... , hN) is the vector of all single-step entropy values for the N nonterminal types in the grammar, and H = (H1, ... , HN) is the vector of all total entropy values, then the closed form solution for the recurrence is given by H = (I − A)−1h (5) where I is the identity matrix. The entropy after the first n words of the sentence, Hwn, c</context>
</contexts>
<marker>Hale, 2006</marker>
<rawString>J. Hale. 2006. Uncertainty about the rest of the sentence. Cognitive Science, 30(4):643–672.</rawString>
</citation>
<citation valid="true">
<authors>
<author>D Jurafsky</author>
</authors>
<title>A probabilistic model of lexical and syntactic access and disambiguation.</title>
<date>1996</date>
<journal>Cognitive Science,</journal>
<volume>20</volume>
<issue>2</issue>
<contexts>
<context position="4775" citStr="Jurafsky, 1996" startWordPosition="743" endWordPosition="744">ontext: high-constraint contexts, in which there is a strong expectation for a (different) word, and low-constraint ones, which are not strongly predictive of any individual word. There is increasing evidence for an ERP component that responds to violations of a strong prediction (Federmeier, 2007; Van Petten and Luka, 2012). This component can be interpreted as reflecting disproportional commitment to high probability predictions at the expense of lower probability ones, a more extreme version of the proposal that low-probability parses are pruned in the presence of a high-probability parse (Jurafsky, 1996). Surprisal is therefore expected to have a larger effect in high constraint contexts, in which entropy was low before the word being read. Commitment to a high probability prediction may also result in increased processing load at the point at which the commitment is made. We illustrate these four hypotheses using the simple language sketched in Figure 1. Consider the predictions made by the four hypotheses for the sentences ae and be. Surprisal predicts no difference in reading times between these sentences, since the conditional probabilities of the words in the two sentences are identical </context>
</contexts>
<marker>Jurafsky, 1996</marker>
<rawString>D. Jurafsky. 1996. A probabilistic model of lexical and syntactic access and disambiguation. Cognitive Science, 20(2):137–194.</rawString>
</citation>
<citation valid="true">
<authors>
<author>D Klein</author>
<author>C D Manning</author>
</authors>
<title>Accurate unlexicalized parsing.</title>
<date>2003</date>
<booktitle>In Proceedings of the 41st Annual Meeting on Association for Computational Linguistics-Volume 1,</booktitle>
<pages>423--430</pages>
<publisher>Association</publisher>
<institution>for Computational Linguistics.</institution>
<contexts>
<context position="19233" citStr="Klein and Manning, 2003" startWordPosition="3179" endWordPosition="3182">l. (2009)) may therefore be only weakly related to total entropy. 3.2 Constructing the grammar We used a PCFG induced from the Penn Treebank (Marcus et al., 1993). As mentioned above, the grammar was mostly unlexicalized; however, in order for the predictions to depend on the identity of the verb, the grammar had to contain lexically specific rules for each verb. We discuss these rules at end of this section. The Penn Treebank tag set is often expanded by adding to each node’s tag an annotation of the H(ar,j) (4) pr kr L j=1 14 node’s parent, e.g., marking an NP whose parent is a VP as NP VP (Klein and Manning, 2003). While systematic parent annotation would have increased the size of the grammar dramatically, we did take the following minimal steps to improve parsing accuracy. First, the word that is tagged in the Penn Treebank as a preposition (IN) when it occurs as a subordinating conjunction. This resulted in SCs being erroneously parsed as prepositional phrases. To deal with this issue, we replaced the generic IN with IN[that] whenever it referred to that. Second, the parser assigned high probability parses to reduced relative clauses in implausible contexts. We made sure that cases that should not b</context>
<context position="20738" citStr="Klein and Manning, 2003" startWordPosition="3428" endWordPosition="3431">and SBAR[none] when it did not. Following standard practice, we removed grammatical role information and filler-gap annotations, e.g., NP-SUBJ-2 was treated as NP. To reduce the number of rules in the grammar as much as possible, we removed punctuation and the silent element NONE (used to mark gaps, silent complementizers, etc.), rules that occurred less than 100 times (out of the total 1320490 nonterminal productions), and rules that had a probability of less than 0.01. These steps resulted in the removal of 13%, 14% and 10% rule tokens respectively. We then applied horizontal Markovization (Klein and Manning, 2003). Finally, we added lexically specific rules to capture the verbs’ subcategorization preferences, based on the Gahl et al. (2004) subcategorization database. The probability of frame fj following verb vi was calculated as: 1 P(vi)P(fj|vi) P(VP[VBD] → vi fj) = (6) 2 Ei P(vi) In other words, half of the probability mass of production rules deriving VP[VBD] (VP headed by past tense verbs) was taken away from the unlexicalized rules and assigned to the verb-specific rules. The same procedure was performed for VP[VBN] (VP headed by a past participle, with the exception of the verbs forgot and wrote</context>
</contexts>
<marker>Klein, Manning, 2003</marker>
<rawString>D. Klein and C. D. Manning. 2003. Accurate unlexicalized parsing. In Proceedings of the 41st Annual Meeting on Association for Computational Linguistics-Volume 1, pages 423–430. Association for Computational Linguistics.</rawString>
</citation>
<citation valid="true">
<authors>
<author>M P Marcus</author>
<author>M A Marcinkiewicz</author>
<author>B Santorini</author>
</authors>
<title>Building a large annotated corpus of English: The Penn Treebank. Computational Linguistics,</title>
<date>1993</date>
<contexts>
<context position="18771" citStr="Marcus et al., 1993" startWordPosition="3094" endWordPosition="3097">ngle-step entropy is on the order of magnitude of 1 or 2 bits. The entropy of a typical complement, on the other hand, is much higher (consider all of the possible internal structures that an SC could have). This means that the total entropy H after processing the verb is dominated by the entropy of its potential complements rather than the verb’s singlestep entropy h (see Figure 3 for an illustration). A lookahead of a single word (as used in Roark et al. (2009)) may therefore be only weakly related to total entropy. 3.2 Constructing the grammar We used a PCFG induced from the Penn Treebank (Marcus et al., 1993). As mentioned above, the grammar was mostly unlexicalized; however, in order for the predictions to depend on the identity of the verb, the grammar had to contain lexically specific rules for each verb. We discuss these rules at end of this section. The Penn Treebank tag set is often expanded by adding to each node’s tag an annotation of the H(ar,j) (4) pr kr L j=1 14 node’s parent, e.g., marking an NP whose parent is a VP as NP VP (Klein and Manning, 2003). While systematic parent annotation would have increased the size of the grammar dramatically, we did take the following minimal steps to</context>
</contexts>
<marker>Marcus, Marcinkiewicz, Santorini, 1993</marker>
<rawString>M. P. Marcus, M. A. Marcinkiewicz, and B. Santorini. 1993. Building a large annotated corpus of English: The Penn Treebank. Computational Linguistics, 19(2):313–330.</rawString>
</citation>
<citation valid="true">
<authors>
<author>K McRae</author>
<author>M Spivey-Knowlton</author>
<author>M Tanenhaus</author>
</authors>
<title>Modeling the influence of thematic fit (and other constraints) in on-line sentence comprehension.</title>
<date>1998</date>
<journal>Journal of Memory and Language,</journal>
<volume>38</volume>
<issue>3</issue>
<pages>312</pages>
<contexts>
<context position="2916" citStr="McRae et al., 1998" startWordPosition="457" endWordPosition="460">ainty is quantified as the entropy of the distribution over complete parses of the sentence; that is, if Ai is the set of all possible parses of the sentence after word wi, then the uncertainty following wi is given by 1: Hwi = − P(a)lo92 P(a) (1) a∈Ai Processing load in this hypothesis is proportional to the entropy reduction caused by wn:1 ER(wn) = max{Hwn−1 − Hwn, 01 (2) A third hypothesis, which we term the competition hypothesis, predicts that higher competition among potential outcomes should result in increased processing load at the point at which the competing parses are still valid (McRae et al., 1998; Tabor and Tanenhaus, 1999). This contrasts with the entropy reduction hypothesis, according to which processing cost arises when competition is resolved. Intuitively, the two hypotheses make inversely correlated predictions: on average, there will be less competition following words that reduce entropy. A recent study found that reading times on wi correlated positively with entropy following wi, providing support for this hypothesis (Roark et al., 2009). The fourth hypothesis we consider, which we term the commitment hypothesis, is derived from 1No processing load is predicted for words tha</context>
</contexts>
<marker>McRae, Spivey-Knowlton, Tanenhaus, 1998</marker>
<rawString>K. McRae, M. Spivey-Knowlton, and M. Tanenhaus. 1998. Modeling the influence of thematic fit (and other constraints) in on-line sentence comprehension. Journal of Memory and Language, 38(3):283– 312.</rawString>
</citation>
<citation valid="true">
<authors>
<author>B Roark</author>
<author>A Bachrach</author>
<author>C Cardenas</author>
<author>C Pallier</author>
</authors>
<title>Deriving lexical and syntactic expectationbased measures for psycholinguistic modeling via incremental top-down parsing.</title>
<date>2009</date>
<booktitle>In Proceedings of the 2009 Conference on Empirical Methods in Natural Language Processing: Volume</booktitle>
<volume>1</volume>
<pages>324--333</pages>
<publisher>Association</publisher>
<institution>for Computational Linguistics.</institution>
<contexts>
<context position="3376" citStr="Roark et al., 2009" startWordPosition="524" endWordPosition="527"> competition among potential outcomes should result in increased processing load at the point at which the competing parses are still valid (McRae et al., 1998; Tabor and Tanenhaus, 1999). This contrasts with the entropy reduction hypothesis, according to which processing cost arises when competition is resolved. Intuitively, the two hypotheses make inversely correlated predictions: on average, there will be less competition following words that reduce entropy. A recent study found that reading times on wi correlated positively with entropy following wi, providing support for this hypothesis (Roark et al., 2009). The fourth hypothesis we consider, which we term the commitment hypothesis, is derived from 1No processing load is predicted for words that increase uncertainty. 10 Proceedings of the 2014 ACL Workshop on Cognitive Modeling and Computational Linguistics, pages 10–18, Baltimore, Maryland USA, June 26 2014. c�2014 Association for Computational Linguistics c 0.25 a 0.25 d 0.5 0.25 e 0.5 0.25 0.25 b 0.75 f start Figure 1: Example language. Output strings are indicated inside the nodes, and transition probabilities are indicated on the edges. For example, the probability of the sentence bf is 0.5</context>
<context position="17803" citStr="Roark et al. (2009)" startWordPosition="2923" endWordPosition="2926">entity matrix. The entropy after the first n words of the sentence, Hwn, can be calculated by applying Equation 5 to the grammar formed by intersecting the original grammar with the prefix w1, ... , wn (i.e., considering only the parses that are compatible with the words encountered so far) (Hale, 2006). Two points are worth noting about these equations. First, Equation 5 shows that calculating the entropy of a PCFG requires inverting the matrix I − A, which is the size of the number of nonterminal symbols in the grammar. This makes it impractical to use a lexicalized grammar, as advocated by Roark et al. (2009), since those grammars have a very large number of nonterminal types. Second, Equation 4 shows that the entropy of a nonterminal is the sum of its single-step entropy and a weighted average of entropy of the nonterminals it derives. In the context of subcategorization decisions, the number of possible subcategorization frames is small, and the single-step entropy is on the order of magnitude of 1 or 2 bits. The entropy of a typical complement, on the other hand, is much higher (consider all of the possible internal structures that an SC could have). This means that the total entropy H after pr</context>
<context position="29480" citStr="Roark et al. (2009)" startWordPosition="4904" endWordPosition="4907"> following the current word was estimated as the entropy of this quadrigram distribution. Higher (modified) entropy reduction resulted in increased reading times. These results are not directly comparable to the present results, however. Frank (2013) tested a theory that takes into account both positive and negative entropy changes. In addition, a four-word lookahead may not capture the dramatic difference in internal entropy between SCs and NPs, which is responsible for the differential reading times predicted on the matrix. This caveat applies even more strongly to the one-word lookahead in Roark et al. (2009). In contrast with much previous work, we calculated total entropy using a realistic PCFG acquired from a Treebank corpus. In future work, this method can be used to investigate the effect of entropy in a naturalistic reading time corpus. It will be important to explore the extent to which the reading time predictions derived from the grammar are affected by representational decisions (e.g., the parent annotations we used in Section 3.2). This applies in particular to entropy, which is sensitive to the distribution over syntactic parses active at the word; surprisal depends only the conditiona</context>
</contexts>
<marker>Roark, Bachrach, Cardenas, Pallier, 2009</marker>
<rawString>B. Roark, A. Bachrach, C. Cardenas, and C. Pallier. 2009. Deriving lexical and syntactic expectationbased measures for psycholinguistic modeling via incremental top-down parsing. In Proceedings of the 2009 Conference on Empirical Methods in Natural Language Processing: Volume 1-Volume 1, pages 324–333. Association for Computational Linguistics.</rawString>
</citation>
<citation valid="true">
<authors>
<author>N J Smith</author>
<author>R Levy</author>
</authors>
<title>The effect of word predictability on reading time is logarithmic.</title>
<date>2013</date>
<journal>Cognition,</journal>
<volume>128</volume>
<issue>3</issue>
<contexts>
<context position="1225" citStr="Smith and Levy, 2013" startWordPosition="173" endWordPosition="176">. We additionally estimate wordby-word surprisal and total entropy over parses of the sentence using a probabilistic context-free grammar (PCFG). Surprisal and total entropy, but not single step entropy, were significant predictors of reading times in different parts of the sentence. This suggests that a complete model of sentence processing should incorporate both entropy and surprisal. 1 Introduction Predictable linguistic elements are processed faster than unpredictable ones. Specifically, processing load on an element A in context C is linearly correlated with its surprisal, −lo92 P(A|C) (Smith and Levy, 2013). This suggests that readers maintain expectations as to the upcoming elements: likely elements are accessed or constructed in advance of being read. While there is substantial amount of work on the effect of predictability on processing difficulty, the role (if any) of the distribution over expectations is less well understood. Surprisal predicts that the distribution over competing predicted elements should not affect reading times: if the conditional probability of a word A is P(A|C), reading times on the word will be proportional to −lo92 P(A|C), regardless of whether the remaining probabi</context>
</contexts>
<marker>Smith, Levy, 2013</marker>
<rawString>N. J. Smith and R. Levy. 2013. The effect of word predictability on reading time is logarithmic. Cognition, 128(3):302–319.</rawString>
</citation>
<citation valid="true">
<authors>
<author>W Tabor</author>
<author>M K Tanenhaus</author>
</authors>
<title>Dynamical models of sentence processing.</title>
<date>1999</date>
<journal>Cognitive Science,</journal>
<volume>23</volume>
<issue>4</issue>
<contexts>
<context position="2944" citStr="Tabor and Tanenhaus, 1999" startWordPosition="461" endWordPosition="464">as the entropy of the distribution over complete parses of the sentence; that is, if Ai is the set of all possible parses of the sentence after word wi, then the uncertainty following wi is given by 1: Hwi = − P(a)lo92 P(a) (1) a∈Ai Processing load in this hypothesis is proportional to the entropy reduction caused by wn:1 ER(wn) = max{Hwn−1 − Hwn, 01 (2) A third hypothesis, which we term the competition hypothesis, predicts that higher competition among potential outcomes should result in increased processing load at the point at which the competing parses are still valid (McRae et al., 1998; Tabor and Tanenhaus, 1999). This contrasts with the entropy reduction hypothesis, according to which processing cost arises when competition is resolved. Intuitively, the two hypotheses make inversely correlated predictions: on average, there will be less competition following words that reduce entropy. A recent study found that reading times on wi correlated positively with entropy following wi, providing support for this hypothesis (Roark et al., 2009). The fourth hypothesis we consider, which we term the commitment hypothesis, is derived from 1No processing load is predicted for words that increase uncertainty. 10 P</context>
</contexts>
<marker>Tabor, Tanenhaus, 1999</marker>
<rawString>W. Tabor and M. K. Tanenhaus. 1999. Dynamical models of sentence processing. Cognitive Science, 23(4):491–515.</rawString>
</citation>
<citation valid="true">
<authors>
<author>J Trueswell</author>
<author>M Tanenhaus</author>
<author>C Kello</author>
</authors>
<title>Verbspecific constraints in sentence processing: Separating effects of lexical preference from gardenpaths.</title>
<date>1993</date>
<journal>Journal of Experimental Psychology: Learning, Memory, and Cognition,</journal>
<volume>19</volume>
<issue>3</issue>
<contexts>
<context position="8441" citStr="Trueswell et al., 1993" startWordPosition="1352" endWordPosition="1355">sults of the reading time experiment. 2We do not test the predictions of the entropy reduction hypothesis in this part of the paper, since that theory explicitly only applies to total rather than single-step entropy. 11 2 Reading time experiment 2.1 Design To keep syntactic structure constant while manipulating surprisal and entropy over the next derivation step, we took advantage of the fact that verbs vary in the probability distribution of their syntactic complements (subcategorization frames). Several studies have demonstrated that readers are sensitive to subcategorization probabilities (Trueswell et al., 1993; Garnsey et al., 1997). The structure of the experimental materials is shown in Table 1. In a 2x2x2 factorial design, we crossed the surprisal of a sentential complement (SC) given the verb, the entropy of the verb’s subcategorization distribution, and the presence or absence of the complementizer that. When the complementizer is absent, the region the island is ambiguous between a direct object and an embedded subject. Surprisal theory predicts an effect of SC surprisal on the disambiguating region in ambiguous sentences (sentences without that), as obtained in previous studies (Garnsey et a</context>
</contexts>
<marker>Trueswell, Tanenhaus, Kello, 1993</marker>
<rawString>J. Trueswell, M. Tanenhaus, and C. Kello. 1993. Verbspecific constraints in sentence processing: Separating effects of lexical preference from gardenpaths. Journal of Experimental Psychology: Learning, Memory, and Cognition, 19(3):528–553.</rawString>
</citation>
<citation valid="true">
<authors>
<author>C Van Petten</author>
<author>B Luka</author>
</authors>
<title>Prediction during language comprehension: Benefits, costs, and ERP components.</title>
<date>2012</date>
<journal>International Journal of Psychophysiology,</journal>
<volume>83</volume>
<issue>2</issue>
<marker>Van Petten, Luka, 2012</marker>
<rawString>C. Van Petten and B. Luka. 2012. Prediction during language comprehension: Benefits, costs, and ERP components. International Journal of Psychophysiology, 83(2):176–190.</rawString>
</citation>
<citation valid="true">
<authors>
<author>C S Wetherell</author>
</authors>
<title>Probabilistic languages: A review and some open questions.</title>
<date>1980</date>
<journal>ACM Computing Surveys (CSUR),</journal>
<volume>12</volume>
<issue>4</issue>
<contexts>
<context position="16645" citStr="Wetherell, 1980" startWordPosition="2705" endWordPosition="2706">example: the single step entropy after discover is 1 bit; the overall entropy is 1 + 0.5 × 14 + 0.5 × 50 = 33 bits. 0.5 NP (14 bits) discover 0.5 SC (50 bits) 13 Ambiguous Unambiguous Sentential complement surprisal Low High Subcategorization entropy Low High 0.05 0.00 −0.05 −0.10 Figure 2: Results of the self-paced reading experiment The entropy of all derivations starting with ai (which we term total entropy) is then given by the following recurrence: � H(ai) = h(ai) + r∈rli where ar,1, ... , ar,kr are the nonterminals on the right-hand side of r. This recurrence has a closed form solution (Wetherell, 1980; Hale, 2006). The expectation matrix A is a square matrix with N rows and columns, where N is the set of nonterminals. Each element Aij indicates the expected number of times nonterminal aj will occur when ai is rewritten using exactly one rule of the grammar. If h = (h1, ... , hN) is the vector of all single-step entropy values for the N nonterminal types in the grammar, and H = (H1, ... , HN) is the vector of all total entropy values, then the closed form solution for the recurrence is given by H = (I − A)−1h (5) where I is the identity matrix. The entropy after the first n words of the sen</context>
</contexts>
<marker>Wetherell, 1980</marker>
<rawString>C. S. Wetherell. 1980. Probabilistic languages: A review and some open questions. ACM Computing Surveys (CSUR), 12(4):361–379.</rawString>
</citation>
<citation valid="true">
<authors>
<author>J Yun</author>
<author>J Whitman</author>
<author>J Hale</author>
</authors>
<title>Subject-object asymmetries in Korean sentence comprehension.</title>
<date>2010</date>
<booktitle>In Proceedings of the 32nd Annual Meeting of the Cognitive Science Society.</booktitle>
<contexts>
<context position="7152" citStr="Yun et al., 2010" startWordPosition="1151" endWordPosition="1154">ader will become committed to the prediction that the second word will be f. This will lead to longer reading times on e in be than in ae, despite the fact that its conditional probability is identical in both cases. If commitment to a prediction entails additional work, this hypothesis predicts longer reading times on the first word when it is b. This paper presents an reading time study that aims to test these hypotheses. Empirical tests of computational theories of sentence processing have employed either reading time corpora (Demberg and Keller, 2008) or controlled experimental materials (Yun et al., 2010). The current paper adopts the latter approach, trading off a decrease in lexical and syntactic heterogeneity for increased control. This paper is divided into two parts. Section 2 describes a reading time experiment, which tested the predictions of the surprisal, competition and commitment hypotheses, as applied to the entropy over the next single step in the syntactic derivation.2 We then calculate the total entropy (up to an unbounded number of derivation steps) at each word using a PCFG; Section 3 describes how this grammar was constructed, overviews the predictions that it yielded in ligh</context>
</contexts>
<marker>Yun, Whitman, Hale, 2010</marker>
<rawString>J. Yun, J. Whitman, and J. Hale. 2010. Subject-object asymmetries in Korean sentence comprehension. In Proceedings of the 32nd Annual Meeting of the Cognitive Science Society.</rawString>
</citation>
</citationList>
</algorithm>
</algorithms>
<?xml version="1.0" encoding="UTF-8"?>
<algorithms version="110505">
<algorithm name="SectLabel" version="110505">
<variant no="0" confidence="0.014349">
<title confidence="0.996136">
InproTKS: A Toolkit for Incremental Situated Processing
</title>
<author confidence="0.970643">
Casey Kennington Spyros Kousidis David Schlangen
</author>
<affiliation confidence="0.9729405">
CITEC, Dialogue Systems Dialogue Systems Group Dialogue Systems Group
Group, Bielefeld University Bielefeld University Bielefeld University
</affiliation>
<note confidence="0.5533405">
ckennington1 spyros.kousidis2 david.schlangen2
1@cit-ec.uni-bielefeld.de
</note>
<email confidence="0.505185">
2@uni-bielefeld.de
</email>
<sectionHeader confidence="0.976163" genericHeader="abstract">
Abstract
</sectionHeader>
<bodyText confidence="0.999975125">
In order to process incremental situated
dialogue, it is necessary to accept infor-
mation from various sensors, each track-
ing, in real-time, different aspects of the
physical situation. We present extensions
of the incremental processing toolkit IN-
PROTK which make it possible to plug in
such multimodal sensors and to achieve
situated, real-time dialogue. We also de-
scribe a new module which enables the use
in INPROTK of the Google Web Speech
API, which offers speech recognition with
a very large vocabulary and a wide choice
of languages. We illustrate the use of these
extensions with a description of two sys-
tems handling different situated settings.
</bodyText>
<sectionHeader confidence="0.998989" genericHeader="introduction">
1 Introduction
</sectionHeader>
<bodyText confidence="0.999927609756097">
Realising incremental processing of speech in-
and output – a prerequisite to interpretation and
possibly production of speech concurrently with
the other dialogue participant – requires some fun-
damental changes in the way that components
of dialogue systems operate and communicate
with each other (Schlangen and Skantze, 2011;
Schlangen and Skantze, 2009). Processing situ-
ated communication, that is, communication that
requires reference to the physical setting in which
it occurs, makes it necessary to accept (and fuse)
information from various different sensors, each
tracking different aspects of the physical situation,
making the system multimodal (Atrey et al., 2010;
Dumas et al., 2009; Waibel et al., 1996).
Incremental situated processing brings together
these requirements. In this paper, we present a col-
lection of extensions to the incremental process-
ing toolkit INPROTK (Baumann and Schlangen,
2012) that make it capable of processing situ-
ated communication in an incremental fashion:
we have developed a general architecture for
plugging in multimodal sensors whith we denote
INPROTKS, which includes instantiations for mo-
tion capture (via e.g. via Microsoft Kinect and
Leap Motion) and eye tracking (Seeingmachines
FaceLAB). We also describe a new module we
built that makes it possible to perform (large vo-
cabulary, open domain) speech recognition via the
Google Web Speech API. We describe these com-
ponents individually and give as use-cases in a
driving simulation setup, as well as real-time gaze
and gesture recognition.
In the next section, we will give some back-
ground on incremental processing, then describe
the new methods of plugging in multimodal sen-
sors, specifically using XML-RPC, the Robotics
Service Bus, and the InstantReality framework.
We then explain how we incorporated the Google
Web Speech API into InproTK, offer some use
cases for these new modules, and conclude.
</bodyText>
<sectionHeader confidence="0.835602" genericHeader="method">
2 Background: The IU model, INPROTK
</sectionHeader>
<bodyText confidence="0.999789263157895">
As described in (Baumann and Schlangen, 2012),
INPROTK realizes the IU-model of incremen-
tal processing (Schlangen and Skantze, 2011;
Schlangen and Skantze, 2009), where incremental
systems consist of a network of processing mod-
ules. A typical module takes input from its left
buffer, performs some kind of processing on that
data, and places the processed result onto its right
buffer. The data are packaged as the payload of
incremental units (IUs) which are passed between
modules.
The IUs themselves are also interconnected via
so-called same level links (SLL) and grounded-in
links (GRIN), the former allowing the linking of
IUs as a growing sequence, the latter allowing that
sequence to convey what IUs directly affect it (see
Figure 1 for an example). A complication partic-
ular to incremental processing is that modules can
“change their mind” about what the best hypothe-
</bodyText>
<page confidence="0.974399">
84
</page>
<bodyText confidence="0.9295719">
Proceedings of the SIGDIAL 2014 Conference, pages 84–88,
Philadelphia, U.S.A., 18-20 June 2014. c�2014 Association for Computational Linguistics
the listeners are linked to each other via SLLs.
As with audio inputs in previous version of IN-
PROTK, these IUs are considered basedata and not
explictly linked via GRINs in the sensor data. The
modules defined so far also simply add IUs and do
not revoke.
We will now explain the three new methods of
getting data into and out of INPROTKS.
</bodyText>
<figureCaption confidence="0.7841065">
Figure 1: Example of IU network; part-of-speech
tags are grounded into words, tags and words have
same level links with left IU; four is revoked and
replaced with forty.
</figureCaption>
<bodyText confidence="0.9927877">
sis is, in light of later information, thus IUs can be
added, revoked, or committed to a network of IUs.
INPROTK determines how a module network is
“connected” via an XML-formatted configuration
file, which states module instantiations, includ-
ing the connections between left buffers and right
buffers of the various modules. Also part of the
toolkit is a selection of “incremental processing-
ready” modules, and so makes it possible to realise
responsive speech-based systems.
</bodyText>
<sectionHeader confidence="0.990544" genericHeader="method">
3 InproTK and new I/O: InproTKs
</sectionHeader>
<bodyText confidence="0.99999348">
The new additions introduced here are realised as
INPROTKS modules. The new modules that input
information to an INPROTKS module network are
called listeners in that they “listen” to their respec-
tive message passing systems, and modules that
output information from the network are called
informers. Listeners are specific to their method
of receiving information, explained in each sec-
tion below. Data received from listeners are pack-
aged into an IU and put onto the module’s right
buffer. Listener module left buffers are not used
in the standard way; left buffers receive data from
their respective message passing protocols. An in-
former takes all IUs from its left buffer, and sends
their payload via that module’s specific output
method, serving as a kind of right buffer. Figure
2 gives an example of how such listeners and in-
formers can be used. At the moment, only strings
can be read by listeners and sent by informers; fu-
ture extensions could allow for more complicated
data types.
Listener modules add new IUs to the network;
correspondingly, further modules have to be de-
signed in instatiated systems then can make use
of these information types. These IUs created by
</bodyText>
<subsectionHeader confidence="0.998012">
3.1 XML-RPC
</subsectionHeader>
<bodyText confidence="0.9859032">
XML-RPC is a remote procedure call protocol
which uses XML to encode its calls, and HTTP as a
transport mechanism. This requires a server/client
relationship where the listener is implemented as
the server on a specified port.1 Remote sensors
(e.g., an eye tracker) are realised as clients and can
send data (encoded as a string) to the server using
a specific procedural call. The informer is also re-
alised as an XML-RPC client, which sends data to a
defined server. XML-RPC was introduced in 1998
and is widely implemented in many programming
languages.
Figure 2: Example architecture using new mod-
ules: motion is captured and processed externally
and class labels are sent to a listener, which adds
them to the IU network. Arrows denote connec-
tions from right buffers to left buffers. Information
from the DM is sent via an Informer to an external
logger. External gray modules denote input, white
modules denote output.
</bodyText>
<subsectionHeader confidence="0.999101">
3.2 Robotics Service Bus
</subsectionHeader>
<bodyText confidence="0.9699355">
The Robotics Service Bus (RSB) is a middleware
environment originally designed for message-
passing in robotics systems (Wienke and Wrede,
2011).2 As opposed to XML-RPC which requires
</bodyText>
<footnote confidence="0.999314666666667">
1The specification can be found at http://xmlrpc.
scripting.com/spec.html
2https://code.cor-lab.de/projects/rsb
</footnote>
<figure confidence="0.997999">
Motion
Sensor
Gesture
Classifier
Speaker
Logger
Mic
Listener
ASR
NLG
Informer
InproTKs
NLU
DM
</figure>
<page confidence="0.995636">
85
</page>
<bodyText confidence="0.999902259259259">
point-to-point connections, RSB serves as a bus
across specified transport mechanisms. Simply,
a network of communication nodes can either in-
form by sending events (with a payload), or lis-
ten, i.e., receive events. Informers can send in-
formation on a specific scope which establishes
a visibility for listeners (e.g., a listener that re-
ceives events on scope /one/ will receive all events
that fall under the /one/ scope, whereas a listener
with added constants on the scope, e.g., /one/two/
will not receive events from different added con-
stants /one/three/, but the scope /one/ can listen
on all three of these scopes). A listener mod-
ule is realised in INPROTKS by setting the de-
sired scope in the configuration file, allowing IN-
PROTKS seamless interconnectivity with commu-
nication on RSB.
There is no theoretical limit to the number of in-
formers or listeners; events from a single informer
can be received by multiple listeners. Events are
typed and any new types can be added to the avail-
able set. RSB is under active development and is
becoming more widely used. Java, Python, and
C++ programming languages are currently sup-
ported. In our experience, RSB makes it particu-
larly convenient for setting distributed sensor pro-
cessing networks.
</bodyText>
<subsectionHeader confidence="0.950424">
3.3 InstantReality
</subsectionHeader>
<bodyText confidence="0.999937285714286">
In (Kousidis et al., 2013), the InstantReality
framework, a virtual reality environment, was
used for monitoring and recording data in a real-
time multimodal interaction.3 Each information
source (sensor) runs on its own dedicated work-
station and transmits the sensor data across a net-
work using the InstantIO interface. The data can
be received by different components such as In-
stantPlayer (3D visualization engine; invaluable
for monitoring of data integrity when recording
experimental sessions) or a logger that saves all
data to disk. Network communication is achieved
via multicast, which makes it possible to have any
number of listeners for a server and vice-versa.
The InstantIO API is currently available in C++
and Java. It comes with a non-extensible set of
types (primitives, 2D and 3D vectors, rotations,
images, sounds) which is however adequate for
most tracking applications. InstantIO listeners and
informers are easily configured in INPROTKS con-
figuration file.
</bodyText>
<footnote confidence="0.911947">
3http://www.instantreality.org/
</footnote>
<subsectionHeader confidence="0.760631">
3.4 Venice: Bridging the Interfaces
</subsectionHeader>
<bodyText confidence="0.999938884615385">
To make these different components/interfaces
compatible with each other, we have developed a
collection of bridging tools named Venice. Venice
serves two distinct functions. First, Venice.HUB,
which pushes data to/from any of the following
interfaces: disk (logger/replayer), InstantIO, and
RSB. This allows seamless setup of networks for
logging, playback, real-time processing (or com-
binations; e.g, for simulations), minimizing the
need for adaptations to handle different situations.
Second, Venice.IPC allows interprocess communi-
cation and mainly serves as a quick and efficient
way to create network components for new types
of sensors, regardless of the platform or language.
Venice.IPC acts as a server to which TCP clients
(a common interface for sensors) can connect. It
is highly configurable, readily accepting various
sensor data outputs, and sends data in real-time to
the InstantIO network.
Both Venice components operate on all three
major platforms (Linux, Windows, Mac OS X),
allowing great flexibility in software and sensors
that can be plugged in the architecture, regardless
of the vendor’s native API programming language
or supported platform. We discuss some use cases
in section 5.
</bodyText>
<sectionHeader confidence="0.99043" genericHeader="method">
4 Google Web Speech
</sectionHeader>
<bodyText confidence="0.997605176470588">
One barrier to dialogue system development is
handling ASR. Open source toolkits are available,
each supporting a handful of languages, with each
language having a varying vocabulary size. A step
in overcoming this barrier is “outsourcing” the
problem by making use of the Google Web Speech
API.4 This interface supports many languages, in
most cases with a large, open domain of vocabu-
lary. We have been able to access the API directly
using INPROTKS, similar to (Henderson, 2014).5
INPROTKS already supports an incremental vari-
ant of Sphinx4; a system designer can now choose
from these two alternatives.
At the moment, only the Google Chrome
browser implements the Web Speech API. When
the INPROTKS Web Speech module is invoked,
it creates a service which can be reached from
</bodyText>
<footnote confidence="0.998926666666667">
4The Web Speech API Specificiation: https:
//dvcs.w3.org/hg/speech-api/raw-file/
tip/speechapi.html
5Indeed, we used Matthew Henderson’s webdial project
as a basis: https://bitbucket.org/matthen/
webdialog
</footnote>
<page confidence="0.998155">
86
</page>
<bodyText confidence="0.999828391304348">
the Chrome browser via an URL (and hence, mi-
crophone client, dialogue processor and speech
recogniser can run on different machines). Navi-
gating to that URL shows a simple web page where
one can control the microphone. Figure 3 shows
how the components fit together.
While this setup improves recognition as com-
pared to the Sphinx4-based recognition previously
only available in INPROTK, there are some ar-
eas of concern. First, there is a delay caused by
the remote processing (on Google’s servers), re-
quiring alignment with data from other sensors.
Second, the returned transcription results are only
‘semi-incremental’; sometimes chunks of words
are treated as single increments. Third, n-best lists
can only be obtained when the API detects the end
of the utterance (incrementally, only the top hy-
pothesis is returned). Fourth, the results have a
crude timestamp which signifies the end of the au-
dio segment. We use this timestamp in our con-
struction of word IUs, which in informal tests have
been found to be acceptable for our needs; we de-
fer more systematic testing to future work.
</bodyText>
<figureCaption confidence="0.8878035">
Figure 3: Data flow of Google Web Speech API:
Chrome browser controls the microphone, sends
audio to API and receives incremental hypotheses,
which are directly sent to InproTKS.
</figureCaption>
<sectionHeader confidence="0.910236" genericHeader="method">
5 INPROTKs in Use
</sectionHeader>
<bodyText confidence="0.999609357142857">
We exemplify the utility of INPROTKS in two ex-
periments recently performed in our lab.
In-car situated communication We have tested
a “pause and resume” strategy for adaptive in-
formation presentation in a driving simulation
scenario (see Figure 4), using INPROTKS and
OpenDS (Math et al., 2013). Our dialogue man-
ager – implemented using OpenDial (Lison, 2012)
– receives trigger events from OpenDS in order to
update its state, while it verbalises calendar events
and presents them via speech. This is achieved
by means of InstantIO servers we integrated into
OpenDS and respective listeners in INPROTKS. In
turn, InstantIO informers send data that is logged
</bodyText>
<figureCaption confidence="0.802511">
Figure 4: Participant performing driving test while
listening to iNLG speech delivered by InProTKS.
</figureCaption>
<bodyText confidence="0.998097190476191">
by Venice.HUB. The results of this study are pub-
lished in (Kousidis et al., 2014). Having available
the modules described here made it surprisingly
straightforward to implement the interaction with
the driving simulator (treated as a kind of sensor).
Real-time gaze fixation and pointing gesture
detection Using the tools described here, we
have recently tested a real-time situated commu-
nication environment that uses speech, gaze, and
gesture simultaneously. Data from a Microsoft
Kinect and a Seeingmachines Facelab eye tracker
are logged in realtime to the InstantIO network.
A Venice.HUB component receives this data and
sends it over RSB to external components that
perform detection of gaze fixation and pointing
gestures, as described in (Kousidis et al., 2013).
These class labels are sent in turn over RSB to
INPROTKS listeners, aggregating these modalities
with the ASR in a language understanding module.
Again, this was only enabled by the framework de-
scribed here.
</bodyText>
<sectionHeader confidence="0.999249" genericHeader="conclusions">
6 Conclusion
</sectionHeader>
<bodyText confidence="0.993411">
We have developed methods of providing mul-
timodal information to the incremental dialogue
middleware INPROTK. We have tested these
methods in real-time interaction and have found
them to work well, simplifying the process of
connecting external sensors necessary for multi-
modal, situated dialogue. We have further ex-
tended its options for ASR, connecting the Google
Web Speech API. We have also discussed Venice,
a tool for bridging RSB and InstantIO interfaces,
which can log real-time data in a time-aligned
manner, and replay that data. We also offered
some use-cases for our extensions.
INPROTKS is freely available and accessible.6
</bodyText>
<footnote confidence="0.98075">
6https://bitbucket.org/inpro/inprotk
</footnote>
<page confidence="0.999133">
87
</page>
<bodyText confidence="0.999084">
Acknowledgements Thank you to the anony-
mous reviewers for their useful comments and to
Oliver Eickmeyer for helping with InstantReality.
</bodyText>
<sectionHeader confidence="0.996445" genericHeader="references">
References
</sectionHeader>
<reference confidence="0.999917469387755">
Pradeep K. Atrey, M. Anwar Hossain, Abdulmotaleb
El Saddik, and Mohan S. Kankanhalli. 2010. Multi-
modal fusion for multimedia analysis: a survey, vol-
ume 16. April.
Timo Baumann and David Schlangen. 2012. The In-
proTK 2012 Release. In NAACL.
Bruno Dumas, Denis Lalanne, and Sharon Oviatt.
2009. Multimodal Interfaces : A Survey of Princi-
ples , Models and Frameworks. In Human Machine
Interaction, pages 1–25.
Matthew Henderson. 2014. The webdialog Frame-
work for Spoken Dialog in the Browser. Technical
report, Cambridge Engineering Department.
Spyros Kousidis, Casey Kennington, and David
Schlangen. 2013. Investigating speaker gaze and
pointing behaviour in human-computer interaction
with the mint.tools collection. In SIGdial 2013.
Spyros Kousidis, Casey Kennington, Timo Baumann,
Hendrik Buschmeier, Stefan Kopp, and David
Schlangen. 2014. Situationally Aware In-Car Infor-
mation Presentation Using Incremental Speech Gen-
eration: Safer, and More Effective. In Workshop on
Dialog in Motion, EACL 2014.
Pierre Lison. 2012. Probabilistic Dialogue Mod-
els with Prior Domain Knowledge. In Proceedings
of the 13th Annual Meeting of the Special Interest
Group on Discourse and Dialogue, pages 179–188,
Seoul, South Korea, July. Association for Computa-
tional Linguistics.
Rafael Math, Angela Mahr, Mohammad M Moniri,
and Christian M¨uller. 2013. OpenDS: A new
open-source driving simulator for research. GMM-
Fachbericht-AmE 2013.
David Schlangen and Gabriel Skantze. 2009. A Gen-
eral, Abstract Model of Incremental Dialogue Pro-
cessing. In Proceedings of the 10th EACL, number
April, pages 710–718, Athens, Greece. Association
for Computational Linguistics.
David Schlangen and Gabriel Skantze. 2011. A Gen-
eral, Abstract Model of Incremental Dialogue Pro-
cessing. Dialoge &amp; Discourse, 2(1):83–111.
Alex Waibel, Minh Tue Vo, Paul Duchnowski, and Ste-
fan Manke. 1996. Multimodal interfaces. Artificial
Intelligence Review, 10(3-4):299–319.
Johannes Wienke and Sebastian Wrede. 2011. A
middleware for collaborative research in experimen-
tal robotics. In System Integration (SII), 2011
IEEE/SICE International Symposium on, pages
1183–1190.
</reference>
<page confidence="0.999408">
88
</page>
</variant>
</algorithm>
<algorithm name="ParsHed" version="110505">
<variant no="0" confidence="0.947150">
<title confidence="0.999734">A Toolkit for Incremental Situated Processing</title>
<author confidence="0.999831">Casey Kennington Spyros Kousidis David Schlangen</author>
<affiliation confidence="0.9788935">CITEC, Dialogue Systems Dialogue Systems Group Dialogue Systems Group Group, Bielefeld University Bielefeld University Bielefeld University</affiliation>
<abstract confidence="0.999367294117647">In order to process incremental situated dialogue, it is necessary to accept information from various sensors, each tracking, in real-time, different aspects of the physical situation. We present extensions the incremental processing toolkit which make it possible to plug in such multimodal sensors and to achieve situated, real-time dialogue. We also describe a new module which enables the use of the Google Web Speech API, which offers speech recognition with a very large vocabulary and a wide choice of languages. We illustrate the use of these extensions with a description of two systems handling different situated settings.</abstract>
</variant>
</algorithm>
<algorithm name="ParsCit" version="110505">
<citationList>
<citation valid="true">
<authors>
<author>Pradeep K Atrey</author>
<author>M Anwar Hossain</author>
<author>Abdulmotaleb El Saddik</author>
<author>Mohan S Kankanhalli</author>
</authors>
<title>Multimodal fusion for multimedia analysis: a survey,</title>
<date>2010</date>
<volume>16</volume>
<marker>Atrey, Hossain, El Saddik, Kankanhalli, 2010</marker>
<rawString>Pradeep K. Atrey, M. Anwar Hossain, Abdulmotaleb El Saddik, and Mohan S. Kankanhalli. 2010. Multimodal fusion for multimedia analysis: a survey, volume 16. April.</rawString>
</citation>
<citation valid="true">
<authors>
<author>Timo Baumann</author>
<author>David Schlangen</author>
</authors>
<title>The InproTK</title>
<date>2012</date>
<booktitle>In NAACL.</booktitle>
<contexts>
<context position="1931" citStr="Baumann and Schlangen, 2012" startWordPosition="273" endWordPosition="276">each other (Schlangen and Skantze, 2011; Schlangen and Skantze, 2009). Processing situated communication, that is, communication that requires reference to the physical setting in which it occurs, makes it necessary to accept (and fuse) information from various different sensors, each tracking different aspects of the physical situation, making the system multimodal (Atrey et al., 2010; Dumas et al., 2009; Waibel et al., 1996). Incremental situated processing brings together these requirements. In this paper, we present a collection of extensions to the incremental processing toolkit INPROTK (Baumann and Schlangen, 2012) that make it capable of processing situated communication in an incremental fashion: we have developed a general architecture for plugging in multimodal sensors whith we denote INPROTKS, which includes instantiations for motion capture (via e.g. via Microsoft Kinect and Leap Motion) and eye tracking (Seeingmachines FaceLAB). We also describe a new module we built that makes it possible to perform (large vocabulary, open domain) speech recognition via the Google Web Speech API. We describe these components individually and give as use-cases in a driving simulation setup, as well as real-time g</context>
</contexts>
<marker>Baumann, Schlangen, 2012</marker>
<rawString>Timo Baumann and David Schlangen. 2012. The InproTK 2012 Release. In NAACL.</rawString>
</citation>
<citation valid="true">
<authors>
<author>Bruno Dumas</author>
<author>Denis Lalanne</author>
<author>Sharon Oviatt</author>
</authors>
<title>Multimodal Interfaces : A Survey of Principles , Models and Frameworks.</title>
<date>2009</date>
<booktitle>In Human Machine Interaction,</booktitle>
<pages>1--25</pages>
<contexts>
<context position="1711" citStr="Dumas et al., 2009" startWordPosition="241" endWordPosition="244">interpretation and possibly production of speech concurrently with the other dialogue participant – requires some fundamental changes in the way that components of dialogue systems operate and communicate with each other (Schlangen and Skantze, 2011; Schlangen and Skantze, 2009). Processing situated communication, that is, communication that requires reference to the physical setting in which it occurs, makes it necessary to accept (and fuse) information from various different sensors, each tracking different aspects of the physical situation, making the system multimodal (Atrey et al., 2010; Dumas et al., 2009; Waibel et al., 1996). Incremental situated processing brings together these requirements. In this paper, we present a collection of extensions to the incremental processing toolkit INPROTK (Baumann and Schlangen, 2012) that make it capable of processing situated communication in an incremental fashion: we have developed a general architecture for plugging in multimodal sensors whith we denote INPROTKS, which includes instantiations for motion capture (via e.g. via Microsoft Kinect and Leap Motion) and eye tracking (Seeingmachines FaceLAB). We also describe a new module we built that makes it</context>
</contexts>
<marker>Dumas, Lalanne, Oviatt, 2009</marker>
<rawString>Bruno Dumas, Denis Lalanne, and Sharon Oviatt. 2009. Multimodal Interfaces : A Survey of Principles , Models and Frameworks. In Human Machine Interaction, pages 1–25.</rawString>
</citation>
<citation valid="true">
<authors>
<author>Matthew Henderson</author>
</authors>
<title>The webdialog Framework for Spoken Dialog in the Browser.</title>
<date>2014</date>
<tech>Technical report,</tech>
<institution>Cambridge Engineering Department.</institution>
<contexts>
<context position="11561" citStr="Henderson, 2014" startWordPosition="1798" endWordPosition="1799">ess of the vendor’s native API programming language or supported platform. We discuss some use cases in section 5. 4 Google Web Speech One barrier to dialogue system development is handling ASR. Open source toolkits are available, each supporting a handful of languages, with each language having a varying vocabulary size. A step in overcoming this barrier is “outsourcing” the problem by making use of the Google Web Speech API.4 This interface supports many languages, in most cases with a large, open domain of vocabulary. We have been able to access the API directly using INPROTKS, similar to (Henderson, 2014).5 INPROTKS already supports an incremental variant of Sphinx4; a system designer can now choose from these two alternatives. At the moment, only the Google Chrome browser implements the Web Speech API. When the INPROTKS Web Speech module is invoked, it creates a service which can be reached from 4The Web Speech API Specificiation: https: //dvcs.w3.org/hg/speech-api/raw-file/ tip/speechapi.html 5Indeed, we used Matthew Henderson’s webdial project as a basis: https://bitbucket.org/matthen/ webdialog 86 the Chrome browser via an URL (and hence, microphone client, dialogue processor and speech re</context>
</contexts>
<marker>Henderson, 2014</marker>
<rawString>Matthew Henderson. 2014. The webdialog Framework for Spoken Dialog in the Browser. Technical report, Cambridge Engineering Department.</rawString>
</citation>
<citation valid="true">
<authors>
<author>Spyros Kousidis</author>
<author>Casey Kennington</author>
<author>David Schlangen</author>
</authors>
<title>Investigating speaker gaze and pointing behaviour in human-computer interaction with the mint.tools collection.</title>
<date>2013</date>
<booktitle>In SIGdial</booktitle>
<contexts>
<context position="8828" citStr="Kousidis et al., 2013" startWordPosition="1385" endWordPosition="1388">etting the desired scope in the configuration file, allowing INPROTKS seamless interconnectivity with communication on RSB. There is no theoretical limit to the number of informers or listeners; events from a single informer can be received by multiple listeners. Events are typed and any new types can be added to the available set. RSB is under active development and is becoming more widely used. Java, Python, and C++ programming languages are currently supported. In our experience, RSB makes it particularly convenient for setting distributed sensor processing networks. 3.3 InstantReality In (Kousidis et al., 2013), the InstantReality framework, a virtual reality environment, was used for monitoring and recording data in a realtime multimodal interaction.3 Each information source (sensor) runs on its own dedicated workstation and transmits the sensor data across a network using the InstantIO interface. The data can be received by different components such as InstantPlayer (3D visualization engine; invaluable for monitoring of data integrity when recording experimental sessions) or a logger that saves all data to disk. Network communication is achieved via multicast, which makes it possible to have any n</context>
<context position="14881" citStr="Kousidis et al., 2013" startWordPosition="2314" endWordPosition="2317"> straightforward to implement the interaction with the driving simulator (treated as a kind of sensor). Real-time gaze fixation and pointing gesture detection Using the tools described here, we have recently tested a real-time situated communication environment that uses speech, gaze, and gesture simultaneously. Data from a Microsoft Kinect and a Seeingmachines Facelab eye tracker are logged in realtime to the InstantIO network. A Venice.HUB component receives this data and sends it over RSB to external components that perform detection of gaze fixation and pointing gestures, as described in (Kousidis et al., 2013). These class labels are sent in turn over RSB to INPROTKS listeners, aggregating these modalities with the ASR in a language understanding module. Again, this was only enabled by the framework described here. 6 Conclusion We have developed methods of providing multimodal information to the incremental dialogue middleware INPROTK. We have tested these methods in real-time interaction and have found them to work well, simplifying the process of connecting external sensors necessary for multimodal, situated dialogue. We have further extended its options for ASR, connecting the Google Web Speech </context>
</contexts>
<marker>Kousidis, Kennington, Schlangen, 2013</marker>
<rawString>Spyros Kousidis, Casey Kennington, and David Schlangen. 2013. Investigating speaker gaze and pointing behaviour in human-computer interaction with the mint.tools collection. In SIGdial 2013.</rawString>
</citation>
<citation valid="true">
<authors>
<author>Spyros Kousidis</author>
<author>Casey Kennington</author>
<author>Timo Baumann</author>
<author>Hendrik Buschmeier</author>
<author>Stefan Kopp</author>
<author>David Schlangen</author>
</authors>
<title>Situationally Aware In-Car Information Presentation Using Incremental Speech Generation: Safer, and More Effective.</title>
<date>2014</date>
<booktitle>In Workshop on Dialog in Motion, EACL</booktitle>
<contexts>
<context position="14193" citStr="Kousidis et al., 2014" startWordPosition="2212" endWordPosition="2215">ation scenario (see Figure 4), using INPROTKS and OpenDS (Math et al., 2013). Our dialogue manager – implemented using OpenDial (Lison, 2012) – receives trigger events from OpenDS in order to update its state, while it verbalises calendar events and presents them via speech. This is achieved by means of InstantIO servers we integrated into OpenDS and respective listeners in INPROTKS. In turn, InstantIO informers send data that is logged Figure 4: Participant performing driving test while listening to iNLG speech delivered by InProTKS. by Venice.HUB. The results of this study are published in (Kousidis et al., 2014). Having available the modules described here made it surprisingly straightforward to implement the interaction with the driving simulator (treated as a kind of sensor). Real-time gaze fixation and pointing gesture detection Using the tools described here, we have recently tested a real-time situated communication environment that uses speech, gaze, and gesture simultaneously. Data from a Microsoft Kinect and a Seeingmachines Facelab eye tracker are logged in realtime to the InstantIO network. A Venice.HUB component receives this data and sends it over RSB to external components that perform d</context>
</contexts>
<marker>Kousidis, Kennington, Baumann, Buschmeier, Kopp, Schlangen, 2014</marker>
<rawString>Spyros Kousidis, Casey Kennington, Timo Baumann, Hendrik Buschmeier, Stefan Kopp, and David Schlangen. 2014. Situationally Aware In-Car Information Presentation Using Incremental Speech Generation: Safer, and More Effective. In Workshop on Dialog in Motion, EACL 2014.</rawString>
</citation>
<citation valid="true">
<authors>
<author>Pierre Lison</author>
</authors>
<title>Probabilistic Dialogue Models with Prior Domain Knowledge.</title>
<date>2012</date>
<booktitle>In Proceedings of the 13th Annual Meeting of the Special Interest Group on Discourse and Dialogue,</booktitle>
<pages>179--188</pages>
<publisher>Association</publisher>
<institution>for Computational Linguistics.</institution>
<location>Seoul, South</location>
<contexts>
<context position="13712" citStr="Lison, 2012" startWordPosition="2137" endWordPosition="2138">needs; we defer more systematic testing to future work. Figure 3: Data flow of Google Web Speech API: Chrome browser controls the microphone, sends audio to API and receives incremental hypotheses, which are directly sent to InproTKS. 5 INPROTKs in Use We exemplify the utility of INPROTKS in two experiments recently performed in our lab. In-car situated communication We have tested a “pause and resume” strategy for adaptive information presentation in a driving simulation scenario (see Figure 4), using INPROTKS and OpenDS (Math et al., 2013). Our dialogue manager – implemented using OpenDial (Lison, 2012) – receives trigger events from OpenDS in order to update its state, while it verbalises calendar events and presents them via speech. This is achieved by means of InstantIO servers we integrated into OpenDS and respective listeners in INPROTKS. In turn, InstantIO informers send data that is logged Figure 4: Participant performing driving test while listening to iNLG speech delivered by InProTKS. by Venice.HUB. The results of this study are published in (Kousidis et al., 2014). Having available the modules described here made it surprisingly straightforward to implement the interaction with th</context>
</contexts>
<marker>Lison, 2012</marker>
<rawString>Pierre Lison. 2012. Probabilistic Dialogue Models with Prior Domain Knowledge. In Proceedings of the 13th Annual Meeting of the Special Interest Group on Discourse and Dialogue, pages 179–188, Seoul, South Korea, July. Association for Computational Linguistics.</rawString>
</citation>
<citation valid="true">
<authors>
<author>Rafael Math</author>
<author>Angela Mahr</author>
<author>Mohammad M Moniri</author>
<author>Christian M¨uller</author>
</authors>
<title>OpenDS: A new open-source driving simulator for research. GMMFachbericht-AmE</title>
<date>2013</date>
<marker>Math, Mahr, Moniri, M¨uller, 2013</marker>
<rawString>Rafael Math, Angela Mahr, Mohammad M Moniri, and Christian M¨uller. 2013. OpenDS: A new open-source driving simulator for research. GMMFachbericht-AmE 2013.</rawString>
</citation>
<citation valid="true">
<authors>
<author>David Schlangen</author>
<author>Gabriel Skantze</author>
</authors>
<title>A General, Abstract Model of Incremental Dialogue Processing.</title>
<date>2009</date>
<booktitle>In Proceedings of the 10th EACL, number April,</booktitle>
<pages>710--718</pages>
<publisher>Association</publisher>
<institution>for Computational Linguistics.</institution>
<location>Athens, Greece.</location>
<contexts>
<context position="1372" citStr="Schlangen and Skantze, 2009" startWordPosition="191" endWordPosition="194">s the use in INPROTK of the Google Web Speech API, which offers speech recognition with a very large vocabulary and a wide choice of languages. We illustrate the use of these extensions with a description of two systems handling different situated settings. 1 Introduction Realising incremental processing of speech inand output – a prerequisite to interpretation and possibly production of speech concurrently with the other dialogue participant – requires some fundamental changes in the way that components of dialogue systems operate and communicate with each other (Schlangen and Skantze, 2011; Schlangen and Skantze, 2009). Processing situated communication, that is, communication that requires reference to the physical setting in which it occurs, makes it necessary to accept (and fuse) information from various different sensors, each tracking different aspects of the physical situation, making the system multimodal (Atrey et al., 2010; Dumas et al., 2009; Waibel et al., 1996). Incremental situated processing brings together these requirements. In this paper, we present a collection of extensions to the incremental processing toolkit INPROTK (Baumann and Schlangen, 2012) that make it capable of processing situa</context>
<context position="3121" citStr="Schlangen and Skantze, 2009" startWordPosition="458" endWordPosition="461">mulation setup, as well as real-time gaze and gesture recognition. In the next section, we will give some background on incremental processing, then describe the new methods of plugging in multimodal sensors, specifically using XML-RPC, the Robotics Service Bus, and the InstantReality framework. We then explain how we incorporated the Google Web Speech API into InproTK, offer some use cases for these new modules, and conclude. 2 Background: The IU model, INPROTK As described in (Baumann and Schlangen, 2012), INPROTK realizes the IU-model of incremental processing (Schlangen and Skantze, 2011; Schlangen and Skantze, 2009), where incremental systems consist of a network of processing modules. A typical module takes input from its left buffer, performs some kind of processing on that data, and places the processed result onto its right buffer. The data are packaged as the payload of incremental units (IUs) which are passed between modules. The IUs themselves are also interconnected via so-called same level links (SLL) and grounded-in links (GRIN), the former allowing the linking of IUs as a growing sequence, the latter allowing that sequence to convey what IUs directly affect it (see Figure 1 for an example). A </context>
</contexts>
<marker>Schlangen, Skantze, 2009</marker>
<rawString>David Schlangen and Gabriel Skantze. 2009. A General, Abstract Model of Incremental Dialogue Processing. In Proceedings of the 10th EACL, number April, pages 710–718, Athens, Greece. Association for Computational Linguistics.</rawString>
</citation>
<citation valid="true">
<authors>
<author>David Schlangen</author>
<author>Gabriel Skantze</author>
</authors>
<title>A General,</title>
<date>2011</date>
<booktitle>Abstract Model of Incremental Dialogue Processing. Dialoge &amp; Discourse,</booktitle>
<volume>2</volume>
<issue>1</issue>
<contexts>
<context position="1342" citStr="Schlangen and Skantze, 2011" startWordPosition="187" endWordPosition="190">ibe a new module which enables the use in INPROTK of the Google Web Speech API, which offers speech recognition with a very large vocabulary and a wide choice of languages. We illustrate the use of these extensions with a description of two systems handling different situated settings. 1 Introduction Realising incremental processing of speech inand output – a prerequisite to interpretation and possibly production of speech concurrently with the other dialogue participant – requires some fundamental changes in the way that components of dialogue systems operate and communicate with each other (Schlangen and Skantze, 2011; Schlangen and Skantze, 2009). Processing situated communication, that is, communication that requires reference to the physical setting in which it occurs, makes it necessary to accept (and fuse) information from various different sensors, each tracking different aspects of the physical situation, making the system multimodal (Atrey et al., 2010; Dumas et al., 2009; Waibel et al., 1996). Incremental situated processing brings together these requirements. In this paper, we present a collection of extensions to the incremental processing toolkit INPROTK (Baumann and Schlangen, 2012) that make </context>
<context position="3091" citStr="Schlangen and Skantze, 2011" startWordPosition="454" endWordPosition="457"> as use-cases in a driving simulation setup, as well as real-time gaze and gesture recognition. In the next section, we will give some background on incremental processing, then describe the new methods of plugging in multimodal sensors, specifically using XML-RPC, the Robotics Service Bus, and the InstantReality framework. We then explain how we incorporated the Google Web Speech API into InproTK, offer some use cases for these new modules, and conclude. 2 Background: The IU model, INPROTK As described in (Baumann and Schlangen, 2012), INPROTK realizes the IU-model of incremental processing (Schlangen and Skantze, 2011; Schlangen and Skantze, 2009), where incremental systems consist of a network of processing modules. A typical module takes input from its left buffer, performs some kind of processing on that data, and places the processed result onto its right buffer. The data are packaged as the payload of incremental units (IUs) which are passed between modules. The IUs themselves are also interconnected via so-called same level links (SLL) and grounded-in links (GRIN), the former allowing the linking of IUs as a growing sequence, the latter allowing that sequence to convey what IUs directly affect it (se</context>
</contexts>
<marker>Schlangen, Skantze, 2011</marker>
<rawString>David Schlangen and Gabriel Skantze. 2011. A General, Abstract Model of Incremental Dialogue Processing. Dialoge &amp; Discourse, 2(1):83–111.</rawString>
</citation>
<citation valid="true">
<authors>
<author>Alex Waibel</author>
<author>Minh Tue Vo</author>
<author>Paul Duchnowski</author>
<author>Stefan Manke</author>
</authors>
<title>Multimodal interfaces.</title>
<date>1996</date>
<journal>Artificial Intelligence Review,</journal>
<pages>10--3</pages>
<contexts>
<context position="1733" citStr="Waibel et al., 1996" startWordPosition="245" endWordPosition="248">ossibly production of speech concurrently with the other dialogue participant – requires some fundamental changes in the way that components of dialogue systems operate and communicate with each other (Schlangen and Skantze, 2011; Schlangen and Skantze, 2009). Processing situated communication, that is, communication that requires reference to the physical setting in which it occurs, makes it necessary to accept (and fuse) information from various different sensors, each tracking different aspects of the physical situation, making the system multimodal (Atrey et al., 2010; Dumas et al., 2009; Waibel et al., 1996). Incremental situated processing brings together these requirements. In this paper, we present a collection of extensions to the incremental processing toolkit INPROTK (Baumann and Schlangen, 2012) that make it capable of processing situated communication in an incremental fashion: we have developed a general architecture for plugging in multimodal sensors whith we denote INPROTKS, which includes instantiations for motion capture (via e.g. via Microsoft Kinect and Leap Motion) and eye tracking (Seeingmachines FaceLAB). We also describe a new module we built that makes it possible to perform (</context>
</contexts>
<marker>Waibel, Vo, Duchnowski, Manke, 1996</marker>
<rawString>Alex Waibel, Minh Tue Vo, Paul Duchnowski, and Stefan Manke. 1996. Multimodal interfaces. Artificial Intelligence Review, 10(3-4):299–319.</rawString>
</citation>
<citation valid="true">
<authors>
<author>Johannes Wienke</author>
<author>Sebastian Wrede</author>
</authors>
<title>A middleware for collaborative research in experimental robotics.</title>
<date>2011</date>
<booktitle>In System Integration (SII), 2011 IEEE/SICE International Symposium on,</booktitle>
<pages>1183--1190</pages>
<contexts>
<context position="7289" citStr="Wienke and Wrede, 2011" startWordPosition="1143" endWordPosition="1146">XML-RPC was introduced in 1998 and is widely implemented in many programming languages. Figure 2: Example architecture using new modules: motion is captured and processed externally and class labels are sent to a listener, which adds them to the IU network. Arrows denote connections from right buffers to left buffers. Information from the DM is sent via an Informer to an external logger. External gray modules denote input, white modules denote output. 3.2 Robotics Service Bus The Robotics Service Bus (RSB) is a middleware environment originally designed for messagepassing in robotics systems (Wienke and Wrede, 2011).2 As opposed to XML-RPC which requires 1The specification can be found at http://xmlrpc. scripting.com/spec.html 2https://code.cor-lab.de/projects/rsb Motion Sensor Gesture Classifier Speaker Logger Mic Listener ASR NLG Informer InproTKs NLU DM 85 point-to-point connections, RSB serves as a bus across specified transport mechanisms. Simply, a network of communication nodes can either inform by sending events (with a payload), or listen, i.e., receive events. Informers can send information on a specific scope which establishes a visibility for listeners (e.g., a listener that receives events o</context>
</contexts>
<marker>Wienke, Wrede, 2011</marker>
<rawString>Johannes Wienke and Sebastian Wrede. 2011. A middleware for collaborative research in experimental robotics. In System Integration (SII), 2011 IEEE/SICE International Symposium on, pages 1183–1190.</rawString>
</citation>
</citationList>
</algorithm>
</algorithms>
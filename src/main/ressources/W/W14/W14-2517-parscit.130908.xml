<?xml version="1.0" encoding="UTF-8"?>
<algorithms version="110505">
<algorithm name="SectLabel" version="110505">
<variant no="0" confidence="0.001845">
<title confidence="0.981106">
Temporal Analysis of Language through Neural Language Models
</title>
<author confidence="0.998399">
Yoon Kim* Yi-I Chiu* Kentaro Hanaki* Darshan Hegde* Slav Petrov*
</author>
<affiliation confidence="0.955361">
*New York University, New York
*Google Inc., New York
</affiliation>
<email confidence="0.962572">
{yhk255, yic211, kh1615, dh1806}@nyu.edu
slav@google.com
</email>
<sectionHeader confidence="0.993816" genericHeader="abstract">
Abstract
</sectionHeader>
<bodyText confidence="0.999966076923077">
We provide a method for automatically
detecting change in language across time
through a chronologically trained neural
language model. We train the model on
the Google Books Ngram corpus to ob-
tain word vector representations specific
to each year, and identify words that have
changed significantly from 1900 to 2009.
The model identifies words such as cell
and gay as having changed during that
time period. The model simultaneously
identifies the specific years during which
such words underwent change.
</bodyText>
<sectionHeader confidence="0.9988" genericHeader="introduction">
1 Introduction
</sectionHeader>
<bodyText confidence="0.999938256410257">
Language changes across time. Existing words
adopt additional senses (gay), new words are cre-
ated (internet), and some words ‘die out’ (many
irregular verbs, such as burnt, are being replaced
by their regularized counterparts (Lieberman et al.,
2007)). Traditionally, scarcity of digitized histori-
cal corpora has prevented applications of contem-
porary machine learning algorithms—which typi-
cally require large amounts of data—in such tem-
poral analyses. Publication of the Google Books
Ngram corpus in 2009, however, has contributed
to an increased interest in culturomics, wherein
researchers analyze changes in human culture
through digitized texts (Michel et al., 2011).
Developing computational methods for detect-
ing and quantifying change in language is of in-
terest to theoretical linguists as well as NLP re-
searchers working with diachronic corpora. Meth-
ods employed in previous work have been var-
ied, from analyses of word frequencies to more in-
volved techniques (Guolordava et al. (2011); Mi-
halcea and Nataste (2012)). In our framework,
we train a Neural Language Model (NLM) on
yearly corpora to obtain word vectors for each year
from 1900 to 2009. We chronologically train the
model by initializing word vectors for subsequent
years with the word vectors obtained from previ-
ous years.
We compare the cosine similarity of the word
vectors for same words in different years to iden-
tify words that have moved significantly in the
vector space during that time period. Our model
identifies words such as cell and gay as having
changed between 1900–2009. The model addi-
tionally identifies words whose change is more
subtle. We also analyze the yearly movement of
words across the vector space to identify the spe-
cific periods during which they changed. The
trained word vectors are publicly available.1
</bodyText>
<sectionHeader confidence="0.999692" genericHeader="related work">
2 Related Work
</sectionHeader>
<bodyText confidence="0.9992565">
Previously, researchers have computationally in-
vestigated diachronic language change in various
ways. Mihalcea and Nastase (2012) take a super-
vised learning approach and predict the time pe-
riod to which a word belongs given its surrounding
context. Sagi et al. (2009) use a variation of Latent
Semantic Analysis to identify semantic change of
specific words from early to modern English. Wi-
jaya and Yeniterzi (2011) utilize a Topics-over-
Time model and K-means clustering to identify
periods during which selected words move from
one topic/cluster to another. They correlate their
findings with the underlying historical events dur-
ing that time. Gulordava and Baroni (2011) use
co-occurrence counts of words from 1960s and
1990s to detect semantic change. They find that
the words identified by the model are consistent
with evaluations from human raters. Popescu and
Strapparava (2013) employ statistical tests on fre-
quencies of political, social, and emotional words
to identify and characterize epochs.
Our work contributes to the domain in sev-
</bodyText>
<footnote confidence="0.990874">
1http://www.yoon.io
</footnote>
<page confidence="0.98162">
61
</page>
<note confidence="0.511729">
Proceedings of the ACL 2014 Workshop on Language Technologies and Computational Social Science, pages 61–65,
</note>
<page confidence="0.346638">
Baltimore, Maryland, USA, June 26, 2014. c�2014 Association for Computational Linguistics
</page>
<bodyText confidence="0.9998293">
eral ways. Whereas previous work has generally
involved researchers manually identifying words
that have changed (with the exception of Gulor-
dava and Baroni (2011)), we are able to automat-
ically identify them. We are additionally able to
capture a word’s yearly movement and identify
periods of rapid change. In contrast to previous
work, we simultaneously identify words that have
changed and also the specific periods during which
they changed.
</bodyText>
<sectionHeader confidence="0.991538" genericHeader="method">
3 Neural Language Models
</sectionHeader>
<bodyText confidence="0.999789552631579">
Similar to traditional language models, NLMs in-
volve predicting a set of future word given some
history of previous words. In NLMs however,
words are projected from a sparse, 1-of-V encod-
ing (where V is the size of the vocabulary) onto a
lower dimensional vector space via a hidden layer.
This allows for better representation of semantic
properties of words compared to traditional lan-
guage models (wherein words are represented as
indices in a vocabulary set). Thus, words that
are semantically close to one another would have
word vectors that are likewise ‘close’ (as measured
by a distance metric) in the vector space. In fact,
Mikolov et al. (2013a) report that word vectors ob-
tained through NLMs capture much deeper level
of semantic information than had been previously
thought. For example, if xw is the word vector
for word w, they note that xapple − xapples ≈
xcar − xcars ≈ xfamily − xfamilies. That is, the
concept of pluralization is learned by the vector
representations (see Mikolov et al. (2013a) for
more examples).
NLMs are but one of many methods to ob-
tain word vectors—other techniques include La-
tent Semantic Analysis (LSA) (Deerwester et al.,
1990), Latent Dirichlet Allocation (LDA) (Blei et
al., 2003), and variations thereof. And even within
NLMs there exist various architectures for learn-
ing word vectors (Bengio et al. (2003); Mikolov
et al. (2010); Collobert et al. (2011); Yih et
al. (2011)). We utilize an architecture introduced
by Mikolov et al. (2013b), called the Skip-gram,
which allows for efficient estimation of word vec-
tors from large corpora.
In a Skip-gram model, each word in the corpus
is used to predict a window of surrounding words
(Figure 1). To ensure that words closer to the cur-
rent word are given more weight in training, dis-
</bodyText>
<figureCaption confidence="0.994389">
Figure 1: Architecture of a Skip-gram model (Mikolov et al.,
2013b).
</figureCaption>
<bodyText confidence="0.999990777777778">
tant words are sampled less frequently.2 Training
is done through stochastic gradient descent and
backpropagation. The word representations are
found in the hidden layer. Despite its simplicity—
and thus, computational efficiency—compared to
other NLMs, Mikolov et al. (2013b) note that the
Skip-gram is competitive with other vector space
models in the Semantic-Syntactic Word Relation-
ship test set when trained on the same data.
</bodyText>
<subsectionHeader confidence="0.998552">
3.1 Training
</subsectionHeader>
<bodyText confidence="0.941515038461539">
The Google Books Ngram corpus contains
Ngrams from approximately 8 million books, or
6% of all books published (Lin et al., 2012). We
sample 10 million 5-grams from the English fic-
tion corpus for every year from 1850–2009. We
lower-case all words after sampling and restrict the
vocabulary to words that occurred at least 10 times
in the 1850–2009 corpus.
For the model, we use a window size of 4 and
dimensionality of 200 for the word vectors. Within
each year, we iterate over epochs until conver-
gence, where the measure of convergence is de-
fined as the average angular change in word vec-
tors between epochs. That is, if V (y) is the vo-
cabulary set for year y, and xw(y, e) is the word
vector for word w in year y and epoch number e,
we continue iterating over epochs until,
xw(y, e) · xw(y, e − 1)
kxw(y, e)kkxw(y, e − 1)k
is below some threshold. The learning rate is set
to 0.01 at the start of each epoch and linearly de-
creased to 0.0001.
2Specifically, given a maximum window size of W, a ran-
dom integer R is picked from range [1, W] for each training
word. The current training word is used to predict R previous
and R future words.
</bodyText>
<equation confidence="0.9096395">
1
|V (y) |w∈V (y) arccos
</equation>
<page confidence="0.971572">
62
</page>
<table confidence="0.999928416666667">
Most Changed Least Changed
Word Similarity Word Similarity
checked 0.3831 by 0.9331
check 0.4073 than 0.9327
gay 0.4079 for 0.9313
actually 0.4086 more 0.9274
supposed 0.4232 other 0.9272
guess 0.4233 an 0.9268
cell 0.4413 own 0.9259
headed 0.4453 with 0.9257
ass 0.4549 down 0.9252
mail 0.4573 very 0.9239
</table>
<tableCaption confidence="0.989341">
Table 1: Top 10 most/least changed words from 1900–2009,
based on cosine similarity of words in 2009 against their 1900
counterparts. Infrequent words (words that occurred less than
500 times) are omitted.
</tableCaption>
<bodyText confidence="0.999907571428571">
Once the word vectors for year y have con-
verged, we initialize the word vectors for year y+1
with the previous year’s word vectors and train
on the y + 1 data until convergence. We repeat
this process for 1850–2009. Using an open source
implementation in the gensim package, training
took approximately 4 days on a 2.9 GHz machine.
</bodyText>
<sectionHeader confidence="0.99973" genericHeader="evaluation">
4 Results and Discussion
</sectionHeader>
<bodyText confidence="0.9979665">
For the analysis, we treat 1850–1899 as an initial-
ization period and begin our study from 1900.
</bodyText>
<subsectionHeader confidence="0.993057">
4.1 Word Comparisons
</subsectionHeader>
<bodyText confidence="0.999985941176471">
By comparing the cosine similarity between same
words across different time periods, we are able
to detect words whose usage has changed. We are
also able to identify words that did not change. Ta-
ble 1 has a list of 10 most/least changed words be-
tween 1900 and 2009. We note that almost all of
the least changed words are function words. For
the changed words, many of the identified words
agree with intuition (e.g. gay, cell, ass). Oth-
ers are not so obvious (e.g. checked, headed, ac-
tually). To better understand how these words
have changed, we look at the composition of their
neighboring words for 1900 and 2009 (Table 2).
As a further check, we search Google Books
for sentences that contain the above words. Below
are some example sentences from 1900 and 2009
with the word checked:
</bodyText>
<footnote confidence="0.8094118">
1900: “However, he checked himself in time, saying —”
1900: “She was about to say something further, but she
checked herself.”
2009: “He’d checked his facts on a notepad from his back
pocket.”
</footnote>
<table confidence="0.974099444444444">
2009: “I checked out the house before I let them go inside.”
Word Neighboring Words in
1900 2009
gay cheerful lesbian
pleasant bisexual
brilliant lesbians
cell closet phone
dungeon cordless
tent cellular
checked checking checking
recollecting consulted
straightened check
headed haired heading
faced sprinted
skinned marched
actually evidently really
accidentally obviously
already nonetheless
</table>
<tableCaption confidence="0.986931">
Table 2: Top 3 neighboring words (based on cosine similar-
ity) specific to each time period for the words identified as
having changed.
</tableCaption>
<bodyText confidence="0.999846727272727">
At the risk of oversimplifying, the resulting sen-
tences indicate that in the past, checked was more
frequently used with the meaning “to hold in re-
straint”, whereas now, it is more frequently used
with the meaning “to verify by consulting an au-
thority” or “to inspect so as to determine accu-
racy”. Given that check is a highly polysemous
word, this seems to be a case in which the popu-
larity of a word’s sense changed over time.
Conducting a similar exercise for actually, we
obtain the following sentences:
</bodyText>
<footnote confidence="0.6877038">
1900: “But if ever he actually came into property, she must
recognize the change in his position.”
1900: “Whenever a young gentleman was not actually
engaged with his knife and fork or spoon —”
2009: “I can’t believe he actually did that!”
</footnote>
<bodyText confidence="0.949773166666667">
2009: “Our date was actually one of the most fun and
creative ones I had in years.”
Like the above, this seems to be a case in
which the popularity of a word’s sense changed
over time (from “to refer to what is true or real”
to “to express wonder or surprise”).
</bodyText>
<subsectionHeader confidence="0.999643">
4.2 Periods of Change
</subsectionHeader>
<bodyText confidence="0.9967459">
As we chronologically train the model year-by-
year, we can plot the time series of a word’s
distance to its neighboring words (from differ-
ent years) to detect periods of change. Figure 2
(above) has such a plot for the word cell compared
to its early neighbors, closet and dungeon, and the
more recent neighbors, phone and cordless. Fig-
ure 2 (below) has a similar plot for gay.
Such plots allow us to identify a word’s pe-
riod of change relative to its neighboring words,
</bodyText>
<page confidence="0.999149">
63
</page>
<figureCaption confidence="0.9968072">
Figure 2: (Above) Time trend of the cosine similarity be-
tween cell and its neighboring words in 1900 (closet, dun-
geon) and 2009 (phone, cordless). (Below) Similar plot of
gay and its neighboring words in 1900 (cheerful, pleasant)
and 2009 (lesbian, bisexual).
</figureCaption>
<bodyText confidence="0.999917789473684">
and thus provide context as to how it evolved.
This may be of use to researchers interested in
understanding (say) when gay started being used
as a synonym for homosexual. We can also iden-
tify periods of change independent of neighboring
words by analyzing the cosine similarity of a word
against itself from a reference year (Figure 3). As
some of the change is due to sampling and random
drift, we additionally plot the average cosine simi-
larity of all words against their reference points in
Figure 3. This allows us to detect whether a word’s
change during a given period is greater (or less)
than would be expected from chance. We note
that for cell, the identified period of change (1985–
2009) coincides with the introduction—and sub-
sequent adoption—of the cell phone by the gen-
eral public.3 Likewise, the period of change for
gay agrees with the gay movement which began
around the 1970s (Wijaya and Yeniterzi, 2011).
</bodyText>
<subsectionHeader confidence="0.997091">
4.3 Limitations
</subsectionHeader>
<bodyText confidence="0.806956">
In the present work, identification of a changed
word is conditioned on its occurring often enough
</bodyText>
<footnote confidence="0.948457">
3http://library.thinkquest.org/04oct/02001/origin.htm
</footnote>
<figureCaption confidence="0.9977948">
Figure 3: Plot of the cosine similarity of changed (gay, cell)
and unchanged (by, than) words against their 1900 starting
points. Middle line is the average cosine similarity of all
words against their starting points in 1900. Shaded region
corresponds to one standard deviation of errors.
</figureCaption>
<bodyText confidence="0.99990575">
in the study period. If a word’s usage decreased
dramatically (or stopped being used altogether),
its word vector will have remained the same and
hence it will not show up as having changed.
One way to overcome this may be to combine
the cosine distance and the frequency to define a
new metric that measures how a word’s usage has
changed.
</bodyText>
<sectionHeader confidence="0.998052" genericHeader="conclusions">
5 Conclusions and Future Work
</sectionHeader>
<bodyText confidence="0.999740863636363">
In this paper we provided a method for analyz-
ing change in the written language across time
through word vectors obtained from a chronolog-
ically trained neural language model. Extending
previous work, we are able to not only automat-
ically identify words that have changed but also
the periods during which they changed. While we
have not extensively looked for connections be-
tween periods identified by the model and real his-
torical events, they are nevertheless apparent.
An interesting direction of research could in-
volve analysis and characterization of the differ-
ent types of change. With a few exceptions, we
have been deliberately general in our analysis by
saying that a word’s usage has changed. We have
avoided inferring the type of change (e.g. semantic
vs syntactic, broadening vs narrowing, pejoration
vs amelioration). It may be the case that words that
undergo (say) a broadening in senses exhibit reg-
ularities in how they move about the vector space,
allowing researchers to characterize the type of
change that occurred.
</bodyText>
<page confidence="0.999145">
64
</page>
<sectionHeader confidence="0.990283" genericHeader="references">
References
</sectionHeader>
<reference confidence="0.999615507936508">
Y. Bengio, R. Ducharme, P. Vincent. 2003. Neu-
ral Probabilitistic Language Model. Journal of Ma-
chine Learning Research 3:1137–1155.
D. Blei, A. Ng, M. Jordan, J. Lafferty. 2003. Latent
Dirichlet Allocation. Journal of Machine Learning
Research 3:993–1022.
R. Collobert, J. Weston, L. Bottou, M. Karlen, K.
Kavukcuglu, P. Kuksa. 2011. Natural Language
Processing (Almost) from Scratch. Journal of Ma-
chine Learning Research 12:2493–2537.
S. Deerwester, S. Dumais, G. Furnas, T. Landauer, R.
Harshman. 2011. Indexing by Latent Semantic
Analysis. Journal of the American Society for In-
formation Science, 41(6):391–407.
K. Gulordava, M. Baroni. 2011. A Distributional
Similarity Approach to the Detection of Semantic
Change in the Google Books Ngram Corpus. Pro-
ceedings of the GEMS 2011 Workshop.
E. Lieberman, J.B. Michel, J. Jackson, T. Tang, M.A.
Nowak. 2007. Quantifying the evolutionary dynam-
ics of language. Nature, 449: 716–716, October.
Y. Lin, J.B. Michel, E.L. Aiden, J. Orwant, W. Brock-
man, S. Petrov. 2012. Syntactic Annotations for the
Google Books Ngram Corpus. Proceedings of the
Association for Computational Linguistics 2012.
J.B Michel, Y.K. Shen, A.P. Aiden, A. Veres, M.K.
Gray, J.P. Pickett, D. Hoiberg, D. Clancy, P. Norvig,
J. Orwant, S.Pinker, M.A. Nowak, E.L. Aiden.
2011. Quantitative Analysis of Culture Using Mil-
lions of Digitized Books. Science, 331(6014): 176–
182, January.
R. Mihalcea, V. Nastase. 2012. Word Epoch Disam-
biguation: Finding How Words Change Over Time.
Proceedings of the Association for Computational
Linguistics 2012.
T. Mikolov, M. Karafiat, L. Burget, J. Cernocky, S.
Khudanpur. 2010. Recurrent Neural Network
Based Language Model. Proceedings of Inter-
speech.
T. Mikolov, W.T Yih, G. Zweig. 2013a. Linguistic
Regularities in Continuous Space Word Representa-
tions. Proceedings of NAACL-HLT 2013, 746–751.
T. Mikolov, K. Chen, G. Corrado, J.Dean. 2013b. Effi-
cient Estimation of Word Representations in Vector
Space arXiv Preprint.
O. Popescu, C. Strapparava. 2013. Behind the Times:
Detecting Epoch Changes using Large Corpora. In-
ternational Joint Conference on Natural Language
Processing, 347–355
E. Sagi, S. Kaufmann, B. Clark 2009. Semantic
Density Analysis: Comparing Word Meaning across
Time and Phonetic Space. Proceedings of the EACL
2009 Workshop on GEMS: 104–111.
D.T. Wijaya, R. Yeniterzi. 2011. Understanding se-
mantic change of words over centuries. Proceed-
ings of the 2011 international workshop on DEtect-
ing and Exploiting Cultural diversiTy on the social
web: 35–40.
W. Yih, K. Toutanova, J. Platt, C. Meek. 2011. Learn-
ing Discriminative Projections for Text Similarity
Measures. Proceedings of the Fifteenth Confer-
ence on Computational Natural Language Learning,
247–256.
</reference>
<page confidence="0.999615">
65
</page>
</variant>
</algorithm>
<algorithm name="ParsHed" version="110505">
<variant no="0" confidence="0.647116">
<title confidence="0.999251">Temporal Analysis of Language through Neural Language Models</title>
<author confidence="0.965124">Yi-I Kentaro Darshan Slav</author>
<affiliation confidence="0.996775">York University, New</affiliation>
<address confidence="0.908973">Inc., New York</address>
<email confidence="0.860204">yic211,kh1615,slav@google.com</email>
<abstract confidence="0.9994">We provide a method for automatically detecting change in language across time through a chronologically trained neural language model. We train the model on the Google Books Ngram corpus to obtain word vector representations specific to each year, and identify words that have changed significantly from 1900 to 2009. model identifies words such as having changed during that time period. The model simultaneously identifies the specific years during which such words underwent change.</abstract>
</variant>
</algorithm>
<algorithm name="ParsCit" version="110505">
<citationList>
<citation valid="true">
<authors>
<author>Y Bengio</author>
<author>R Ducharme</author>
<author>P Vincent</author>
</authors>
<title>Neural Probabilitistic Language Model.</title>
<date>2003</date>
<journal>Journal of Machine Learning Research</journal>
<pages>3--1137</pages>
<contexts>
<context position="5684" citStr="Bengio et al. (2003)" startWordPosition="884" endWordPosition="887">mantic information than had been previously thought. For example, if xw is the word vector for word w, they note that xapple − xapples ≈ xcar − xcars ≈ xfamily − xfamilies. That is, the concept of pluralization is learned by the vector representations (see Mikolov et al. (2013a) for more examples). NLMs are but one of many methods to obtain word vectors—other techniques include Latent Semantic Analysis (LSA) (Deerwester et al., 1990), Latent Dirichlet Allocation (LDA) (Blei et al., 2003), and variations thereof. And even within NLMs there exist various architectures for learning word vectors (Bengio et al. (2003); Mikolov et al. (2010); Collobert et al. (2011); Yih et al. (2011)). We utilize an architecture introduced by Mikolov et al. (2013b), called the Skip-gram, which allows for efficient estimation of word vectors from large corpora. In a Skip-gram model, each word in the corpus is used to predict a window of surrounding words (Figure 1). To ensure that words closer to the current word are given more weight in training, disFigure 1: Architecture of a Skip-gram model (Mikolov et al., 2013b). tant words are sampled less frequently.2 Training is done through stochastic gradient descent and backpropa</context>
</contexts>
<marker>Bengio, Ducharme, Vincent, 2003</marker>
<rawString>Y. Bengio, R. Ducharme, P. Vincent. 2003. Neural Probabilitistic Language Model. Journal of Machine Learning Research 3:1137–1155.</rawString>
</citation>
<citation valid="true">
<authors>
<author>D Blei</author>
<author>A Ng</author>
<author>M Jordan</author>
<author>J Lafferty</author>
</authors>
<title>Latent Dirichlet Allocation.</title>
<date>2003</date>
<journal>Journal of Machine Learning Research</journal>
<pages>3--993</pages>
<contexts>
<context position="5556" citStr="Blei et al., 2003" startWordPosition="864" endWordPosition="867">e vector space. In fact, Mikolov et al. (2013a) report that word vectors obtained through NLMs capture much deeper level of semantic information than had been previously thought. For example, if xw is the word vector for word w, they note that xapple − xapples ≈ xcar − xcars ≈ xfamily − xfamilies. That is, the concept of pluralization is learned by the vector representations (see Mikolov et al. (2013a) for more examples). NLMs are but one of many methods to obtain word vectors—other techniques include Latent Semantic Analysis (LSA) (Deerwester et al., 1990), Latent Dirichlet Allocation (LDA) (Blei et al., 2003), and variations thereof. And even within NLMs there exist various architectures for learning word vectors (Bengio et al. (2003); Mikolov et al. (2010); Collobert et al. (2011); Yih et al. (2011)). We utilize an architecture introduced by Mikolov et al. (2013b), called the Skip-gram, which allows for efficient estimation of word vectors from large corpora. In a Skip-gram model, each word in the corpus is used to predict a window of surrounding words (Figure 1). To ensure that words closer to the current word are given more weight in training, disFigure 1: Architecture of a Skip-gram model (Mik</context>
</contexts>
<marker>Blei, Ng, Jordan, Lafferty, 2003</marker>
<rawString>D. Blei, A. Ng, M. Jordan, J. Lafferty. 2003. Latent Dirichlet Allocation. Journal of Machine Learning Research 3:993–1022.</rawString>
</citation>
<citation valid="true">
<authors>
<author>R Collobert</author>
<author>J Weston</author>
<author>L Bottou</author>
<author>M Karlen</author>
<author>K Kavukcuglu</author>
<author>P Kuksa</author>
</authors>
<title>Natural Language Processing (Almost) from Scratch.</title>
<date>2011</date>
<journal>Journal of Machine Learning Research</journal>
<pages>12--2493</pages>
<contexts>
<context position="5732" citStr="Collobert et al. (2011)" startWordPosition="892" endWordPosition="895">hought. For example, if xw is the word vector for word w, they note that xapple − xapples ≈ xcar − xcars ≈ xfamily − xfamilies. That is, the concept of pluralization is learned by the vector representations (see Mikolov et al. (2013a) for more examples). NLMs are but one of many methods to obtain word vectors—other techniques include Latent Semantic Analysis (LSA) (Deerwester et al., 1990), Latent Dirichlet Allocation (LDA) (Blei et al., 2003), and variations thereof. And even within NLMs there exist various architectures for learning word vectors (Bengio et al. (2003); Mikolov et al. (2010); Collobert et al. (2011); Yih et al. (2011)). We utilize an architecture introduced by Mikolov et al. (2013b), called the Skip-gram, which allows for efficient estimation of word vectors from large corpora. In a Skip-gram model, each word in the corpus is used to predict a window of surrounding words (Figure 1). To ensure that words closer to the current word are given more weight in training, disFigure 1: Architecture of a Skip-gram model (Mikolov et al., 2013b). tant words are sampled less frequently.2 Training is done through stochastic gradient descent and backpropagation. The word representations are found in th</context>
</contexts>
<marker>Collobert, Weston, Bottou, Karlen, Kavukcuglu, Kuksa, 2011</marker>
<rawString>R. Collobert, J. Weston, L. Bottou, M. Karlen, K. Kavukcuglu, P. Kuksa. 2011. Natural Language Processing (Almost) from Scratch. Journal of Machine Learning Research 12:2493–2537.</rawString>
</citation>
<citation valid="true">
<authors>
<author>S Deerwester</author>
<author>S Dumais</author>
<author>G Furnas</author>
<author>T Landauer</author>
<author>R Harshman</author>
</authors>
<title>Indexing by Latent Semantic Analysis.</title>
<date>2011</date>
<journal>Journal of the American Society for Information Science,</journal>
<volume>41</volume>
<issue>6</issue>
<marker>Deerwester, Dumais, Furnas, Landauer, Harshman, 2011</marker>
<rawString>S. Deerwester, S. Dumais, G. Furnas, T. Landauer, R. Harshman. 2011. Indexing by Latent Semantic Analysis. Journal of the American Society for Information Science, 41(6):391–407.</rawString>
</citation>
<citation valid="true">
<authors>
<author>K Gulordava</author>
<author>M Baroni</author>
</authors>
<title>A Distributional Similarity Approach to the Detection of Semantic Change in the Google Books Ngram Corpus.</title>
<date>2011</date>
<booktitle>Proceedings of the GEMS 2011 Workshop.</booktitle>
<contexts>
<context position="3262" citStr="Gulordava and Baroni (2011)" startWordPosition="500" endWordPosition="503">investigated diachronic language change in various ways. Mihalcea and Nastase (2012) take a supervised learning approach and predict the time period to which a word belongs given its surrounding context. Sagi et al. (2009) use a variation of Latent Semantic Analysis to identify semantic change of specific words from early to modern English. Wijaya and Yeniterzi (2011) utilize a Topics-overTime model and K-means clustering to identify periods during which selected words move from one topic/cluster to another. They correlate their findings with the underlying historical events during that time. Gulordava and Baroni (2011) use co-occurrence counts of words from 1960s and 1990s to detect semantic change. They find that the words identified by the model are consistent with evaluations from human raters. Popescu and Strapparava (2013) employ statistical tests on frequencies of political, social, and emotional words to identify and characterize epochs. Our work contributes to the domain in sev1http://www.yoon.io 61 Proceedings of the ACL 2014 Workshop on Language Technologies and Computational Social Science, pages 61–65, Baltimore, Maryland, USA, June 26, 2014. c�2014 Association for Computational Linguistics eral</context>
</contexts>
<marker>Gulordava, Baroni, 2011</marker>
<rawString>K. Gulordava, M. Baroni. 2011. A Distributional Similarity Approach to the Detection of Semantic Change in the Google Books Ngram Corpus. Proceedings of the GEMS 2011 Workshop.</rawString>
</citation>
<citation valid="true">
<authors>
<author>E Lieberman</author>
<author>J B Michel</author>
<author>J Jackson</author>
<author>T Tang</author>
<author>M A Nowak</author>
</authors>
<title>Quantifying the evolutionary dynamics of language.</title>
<date>2007</date>
<journal>Nature,</journal>
<volume>449</volume>
<pages>716--716</pages>
<contexts>
<context position="1018" citStr="Lieberman et al., 2007" startWordPosition="148" endWordPosition="151">model on the Google Books Ngram corpus to obtain word vector representations specific to each year, and identify words that have changed significantly from 1900 to 2009. The model identifies words such as cell and gay as having changed during that time period. The model simultaneously identifies the specific years during which such words underwent change. 1 Introduction Language changes across time. Existing words adopt additional senses (gay), new words are created (internet), and some words ‘die out’ (many irregular verbs, such as burnt, are being replaced by their regularized counterparts (Lieberman et al., 2007)). Traditionally, scarcity of digitized historical corpora has prevented applications of contemporary machine learning algorithms—which typically require large amounts of data—in such temporal analyses. Publication of the Google Books Ngram corpus in 2009, however, has contributed to an increased interest in culturomics, wherein researchers analyze changes in human culture through digitized texts (Michel et al., 2011). Developing computational methods for detecting and quantifying change in language is of interest to theoretical linguists as well as NLP researchers working with diachronic corp</context>
</contexts>
<marker>Lieberman, Michel, Jackson, Tang, Nowak, 2007</marker>
<rawString>E. Lieberman, J.B. Michel, J. Jackson, T. Tang, M.A. Nowak. 2007. Quantifying the evolutionary dynamics of language. Nature, 449: 716–716, October.</rawString>
</citation>
<citation valid="true">
<authors>
<author>Y Lin</author>
<author>J B Michel</author>
<author>E L Aiden</author>
<author>J Orwant</author>
<author>W Brockman</author>
<author>S Petrov</author>
</authors>
<title>Syntactic Annotations for the Google Books Ngram Corpus.</title>
<date>2012</date>
<booktitle>Proceedings of the Association for Computational Linguistics</booktitle>
<contexts>
<context position="6750" citStr="Lin et al., 2012" startWordPosition="1056" endWordPosition="1059"> a Skip-gram model (Mikolov et al., 2013b). tant words are sampled less frequently.2 Training is done through stochastic gradient descent and backpropagation. The word representations are found in the hidden layer. Despite its simplicity— and thus, computational efficiency—compared to other NLMs, Mikolov et al. (2013b) note that the Skip-gram is competitive with other vector space models in the Semantic-Syntactic Word Relationship test set when trained on the same data. 3.1 Training The Google Books Ngram corpus contains Ngrams from approximately 8 million books, or 6% of all books published (Lin et al., 2012). We sample 10 million 5-grams from the English fiction corpus for every year from 1850–2009. We lower-case all words after sampling and restrict the vocabulary to words that occurred at least 10 times in the 1850–2009 corpus. For the model, we use a window size of 4 and dimensionality of 200 for the word vectors. Within each year, we iterate over epochs until convergence, where the measure of convergence is defined as the average angular change in word vectors between epochs. That is, if V (y) is the vocabulary set for year y, and xw(y, e) is the word vector for word w in year y and epoch num</context>
</contexts>
<marker>Lin, Michel, Aiden, Orwant, Brockman, Petrov, 2012</marker>
<rawString>Y. Lin, J.B. Michel, E.L. Aiden, J. Orwant, W. Brockman, S. Petrov. 2012. Syntactic Annotations for the Google Books Ngram Corpus. Proceedings of the Association for Computational Linguistics 2012.</rawString>
</citation>
<citation valid="true">
<authors>
<author>J B Michel</author>
<author>Y K Shen</author>
<author>A P Aiden</author>
<author>A Veres</author>
<author>M K Gray</author>
<author>J P Pickett</author>
<author>D Hoiberg</author>
<author>D Clancy</author>
<author>P Norvig</author>
<author>J Orwant</author>
<author>M A Nowak S Pinker</author>
<author>E L Aiden</author>
</authors>
<title>Quantitative Analysis of Culture Using Millions of Digitized Books.</title>
<date>2011</date>
<journal>Science,</journal>
<volume>331</volume>
<issue>6014</issue>
<pages>176--182</pages>
<contexts>
<context position="1439" citStr="Michel et al., 2011" startWordPosition="207" endWordPosition="210">additional senses (gay), new words are created (internet), and some words ‘die out’ (many irregular verbs, such as burnt, are being replaced by their regularized counterparts (Lieberman et al., 2007)). Traditionally, scarcity of digitized historical corpora has prevented applications of contemporary machine learning algorithms—which typically require large amounts of data—in such temporal analyses. Publication of the Google Books Ngram corpus in 2009, however, has contributed to an increased interest in culturomics, wherein researchers analyze changes in human culture through digitized texts (Michel et al., 2011). Developing computational methods for detecting and quantifying change in language is of interest to theoretical linguists as well as NLP researchers working with diachronic corpora. Methods employed in previous work have been varied, from analyses of word frequencies to more involved techniques (Guolordava et al. (2011); Mihalcea and Nataste (2012)). In our framework, we train a Neural Language Model (NLM) on yearly corpora to obtain word vectors for each year from 1900 to 2009. We chronologically train the model by initializing word vectors for subsequent years with the word vectors obtaine</context>
</contexts>
<marker>Michel, Shen, Aiden, Veres, Gray, Pickett, Hoiberg, Clancy, Norvig, Orwant, Pinker, Aiden, 2011</marker>
<rawString>J.B Michel, Y.K. Shen, A.P. Aiden, A. Veres, M.K. Gray, J.P. Pickett, D. Hoiberg, D. Clancy, P. Norvig, J. Orwant, S.Pinker, M.A. Nowak, E.L. Aiden. 2011. Quantitative Analysis of Culture Using Millions of Digitized Books. Science, 331(6014): 176– 182, January.</rawString>
</citation>
<citation valid="true">
<authors>
<author>R Mihalcea</author>
<author>V Nastase</author>
</authors>
<title>Word Epoch Disambiguation: Finding How Words Change Over Time.</title>
<date>2012</date>
<booktitle>Proceedings of the Association for Computational Linguistics</booktitle>
<contexts>
<context position="2719" citStr="Mihalcea and Nastase (2012)" startWordPosition="413" endWordPosition="416">ty of the word vectors for same words in different years to identify words that have moved significantly in the vector space during that time period. Our model identifies words such as cell and gay as having changed between 1900–2009. The model additionally identifies words whose change is more subtle. We also analyze the yearly movement of words across the vector space to identify the specific periods during which they changed. The trained word vectors are publicly available.1 2 Related Work Previously, researchers have computationally investigated diachronic language change in various ways. Mihalcea and Nastase (2012) take a supervised learning approach and predict the time period to which a word belongs given its surrounding context. Sagi et al. (2009) use a variation of Latent Semantic Analysis to identify semantic change of specific words from early to modern English. Wijaya and Yeniterzi (2011) utilize a Topics-overTime model and K-means clustering to identify periods during which selected words move from one topic/cluster to another. They correlate their findings with the underlying historical events during that time. Gulordava and Baroni (2011) use co-occurrence counts of words from 1960s and 1990s t</context>
</contexts>
<marker>Mihalcea, Nastase, 2012</marker>
<rawString>R. Mihalcea, V. Nastase. 2012. Word Epoch Disambiguation: Finding How Words Change Over Time. Proceedings of the Association for Computational Linguistics 2012.</rawString>
</citation>
<citation valid="true">
<authors>
<author>T Mikolov</author>
<author>M Karafiat</author>
<author>L Burget</author>
<author>J Cernocky</author>
<author>S Khudanpur</author>
</authors>
<title>Recurrent Neural Network Based Language Model.</title>
<date>2010</date>
<booktitle>Proceedings of Interspeech.</booktitle>
<contexts>
<context position="5707" citStr="Mikolov et al. (2010)" startWordPosition="888" endWordPosition="891">n had been previously thought. For example, if xw is the word vector for word w, they note that xapple − xapples ≈ xcar − xcars ≈ xfamily − xfamilies. That is, the concept of pluralization is learned by the vector representations (see Mikolov et al. (2013a) for more examples). NLMs are but one of many methods to obtain word vectors—other techniques include Latent Semantic Analysis (LSA) (Deerwester et al., 1990), Latent Dirichlet Allocation (LDA) (Blei et al., 2003), and variations thereof. And even within NLMs there exist various architectures for learning word vectors (Bengio et al. (2003); Mikolov et al. (2010); Collobert et al. (2011); Yih et al. (2011)). We utilize an architecture introduced by Mikolov et al. (2013b), called the Skip-gram, which allows for efficient estimation of word vectors from large corpora. In a Skip-gram model, each word in the corpus is used to predict a window of surrounding words (Figure 1). To ensure that words closer to the current word are given more weight in training, disFigure 1: Architecture of a Skip-gram model (Mikolov et al., 2013b). tant words are sampled less frequently.2 Training is done through stochastic gradient descent and backpropagation. The word repres</context>
</contexts>
<marker>Mikolov, Karafiat, Burget, Cernocky, Khudanpur, 2010</marker>
<rawString>T. Mikolov, M. Karafiat, L. Burget, J. Cernocky, S. Khudanpur. 2010. Recurrent Neural Network Based Language Model. Proceedings of Interspeech.</rawString>
</citation>
<citation valid="true">
<authors>
<author>T Mikolov</author>
<author>W T Yih</author>
<author>G Zweig</author>
</authors>
<title>Linguistic Regularities in Continuous Space Word Representations.</title>
<date>2013</date>
<booktitle>Proceedings of NAACL-HLT 2013,</booktitle>
<pages>746--751</pages>
<contexts>
<context position="4983" citStr="Mikolov et al. (2013" startWordPosition="768" endWordPosition="771">odels, NLMs involve predicting a set of future word given some history of previous words. In NLMs however, words are projected from a sparse, 1-of-V encoding (where V is the size of the vocabulary) onto a lower dimensional vector space via a hidden layer. This allows for better representation of semantic properties of words compared to traditional language models (wherein words are represented as indices in a vocabulary set). Thus, words that are semantically close to one another would have word vectors that are likewise ‘close’ (as measured by a distance metric) in the vector space. In fact, Mikolov et al. (2013a) report that word vectors obtained through NLMs capture much deeper level of semantic information than had been previously thought. For example, if xw is the word vector for word w, they note that xapple − xapples ≈ xcar − xcars ≈ xfamily − xfamilies. That is, the concept of pluralization is learned by the vector representations (see Mikolov et al. (2013a) for more examples). NLMs are but one of many methods to obtain word vectors—other techniques include Latent Semantic Analysis (LSA) (Deerwester et al., 1990), Latent Dirichlet Allocation (LDA) (Blei et al., 2003), and variations thereof. A</context>
<context position="6451" citStr="Mikolov et al. (2013" startWordPosition="1007" endWordPosition="1010">kip-gram, which allows for efficient estimation of word vectors from large corpora. In a Skip-gram model, each word in the corpus is used to predict a window of surrounding words (Figure 1). To ensure that words closer to the current word are given more weight in training, disFigure 1: Architecture of a Skip-gram model (Mikolov et al., 2013b). tant words are sampled less frequently.2 Training is done through stochastic gradient descent and backpropagation. The word representations are found in the hidden layer. Despite its simplicity— and thus, computational efficiency—compared to other NLMs, Mikolov et al. (2013b) note that the Skip-gram is competitive with other vector space models in the Semantic-Syntactic Word Relationship test set when trained on the same data. 3.1 Training The Google Books Ngram corpus contains Ngrams from approximately 8 million books, or 6% of all books published (Lin et al., 2012). We sample 10 million 5-grams from the English fiction corpus for every year from 1850–2009. We lower-case all words after sampling and restrict the vocabulary to words that occurred at least 10 times in the 1850–2009 corpus. For the model, we use a window size of 4 and dimensionality of 200 for the</context>
</contexts>
<marker>Mikolov, Yih, Zweig, 2013</marker>
<rawString>T. Mikolov, W.T Yih, G. Zweig. 2013a. Linguistic Regularities in Continuous Space Word Representations. Proceedings of NAACL-HLT 2013, 746–751.</rawString>
</citation>
<citation valid="false">
<authors>
<author>T Mikolov</author>
<author>K Chen</author>
<author>G Corrado</author>
<author>J Dean</author>
</authors>
<booktitle>2013b. Efficient Estimation of Word Representations in Vector Space arXiv Preprint.</booktitle>
<marker>Mikolov, Chen, Corrado, Dean, </marker>
<rawString>T. Mikolov, K. Chen, G. Corrado, J.Dean. 2013b. Efficient Estimation of Word Representations in Vector Space arXiv Preprint.</rawString>
</citation>
<citation valid="true">
<authors>
<author>O Popescu</author>
<author>C Strapparava</author>
</authors>
<title>Behind the Times: Detecting Epoch Changes using Large Corpora.</title>
<date>2013</date>
<booktitle>International Joint Conference on Natural Language Processing,</booktitle>
<pages>347--355</pages>
<contexts>
<context position="3475" citStr="Popescu and Strapparava (2013)" startWordPosition="533" endWordPosition="536">t al. (2009) use a variation of Latent Semantic Analysis to identify semantic change of specific words from early to modern English. Wijaya and Yeniterzi (2011) utilize a Topics-overTime model and K-means clustering to identify periods during which selected words move from one topic/cluster to another. They correlate their findings with the underlying historical events during that time. Gulordava and Baroni (2011) use co-occurrence counts of words from 1960s and 1990s to detect semantic change. They find that the words identified by the model are consistent with evaluations from human raters. Popescu and Strapparava (2013) employ statistical tests on frequencies of political, social, and emotional words to identify and characterize epochs. Our work contributes to the domain in sev1http://www.yoon.io 61 Proceedings of the ACL 2014 Workshop on Language Technologies and Computational Social Science, pages 61–65, Baltimore, Maryland, USA, June 26, 2014. c�2014 Association for Computational Linguistics eral ways. Whereas previous work has generally involved researchers manually identifying words that have changed (with the exception of Gulordava and Baroni (2011)), we are able to automatically identify them. We are </context>
</contexts>
<marker>Popescu, Strapparava, 2013</marker>
<rawString>O. Popescu, C. Strapparava. 2013. Behind the Times: Detecting Epoch Changes using Large Corpora. International Joint Conference on Natural Language Processing, 347–355</rawString>
</citation>
<citation valid="true">
<authors>
<author>E Sagi</author>
<author>S Kaufmann</author>
<author>B Clark</author>
</authors>
<title>Semantic Density Analysis: Comparing Word Meaning across Time and Phonetic Space.</title>
<date>2009</date>
<booktitle>Proceedings of the EACL 2009 Workshop on GEMS:</booktitle>
<pages>104--111</pages>
<contexts>
<context position="2857" citStr="Sagi et al. (2009)" startWordPosition="438" endWordPosition="441">d. Our model identifies words such as cell and gay as having changed between 1900–2009. The model additionally identifies words whose change is more subtle. We also analyze the yearly movement of words across the vector space to identify the specific periods during which they changed. The trained word vectors are publicly available.1 2 Related Work Previously, researchers have computationally investigated diachronic language change in various ways. Mihalcea and Nastase (2012) take a supervised learning approach and predict the time period to which a word belongs given its surrounding context. Sagi et al. (2009) use a variation of Latent Semantic Analysis to identify semantic change of specific words from early to modern English. Wijaya and Yeniterzi (2011) utilize a Topics-overTime model and K-means clustering to identify periods during which selected words move from one topic/cluster to another. They correlate their findings with the underlying historical events during that time. Gulordava and Baroni (2011) use co-occurrence counts of words from 1960s and 1990s to detect semantic change. They find that the words identified by the model are consistent with evaluations from human raters. Popescu and </context>
</contexts>
<marker>Sagi, Kaufmann, Clark, 2009</marker>
<rawString>E. Sagi, S. Kaufmann, B. Clark 2009. Semantic Density Analysis: Comparing Word Meaning across Time and Phonetic Space. Proceedings of the EACL 2009 Workshop on GEMS: 104–111.</rawString>
</citation>
<citation valid="true">
<authors>
<author>D T Wijaya</author>
<author>R Yeniterzi</author>
</authors>
<title>Understanding semantic change of words over centuries.</title>
<date>2011</date>
<booktitle>Proceedings of the 2011 international workshop on DEtecting and Exploiting Cultural diversiTy on the social web:</booktitle>
<pages>35--40</pages>
<contexts>
<context position="3005" citStr="Wijaya and Yeniterzi (2011)" startWordPosition="461" endWordPosition="465">e is more subtle. We also analyze the yearly movement of words across the vector space to identify the specific periods during which they changed. The trained word vectors are publicly available.1 2 Related Work Previously, researchers have computationally investigated diachronic language change in various ways. Mihalcea and Nastase (2012) take a supervised learning approach and predict the time period to which a word belongs given its surrounding context. Sagi et al. (2009) use a variation of Latent Semantic Analysis to identify semantic change of specific words from early to modern English. Wijaya and Yeniterzi (2011) utilize a Topics-overTime model and K-means clustering to identify periods during which selected words move from one topic/cluster to another. They correlate their findings with the underlying historical events during that time. Gulordava and Baroni (2011) use co-occurrence counts of words from 1960s and 1990s to detect semantic change. They find that the words identified by the model are consistent with evaluations from human raters. Popescu and Strapparava (2013) employ statistical tests on frequencies of political, social, and emotional words to identify and characterize epochs. Our work c</context>
<context position="12978" citStr="Wijaya and Yeniterzi, 2011" startWordPosition="2143" endWordPosition="2146">itself from a reference year (Figure 3). As some of the change is due to sampling and random drift, we additionally plot the average cosine similarity of all words against their reference points in Figure 3. This allows us to detect whether a word’s change during a given period is greater (or less) than would be expected from chance. We note that for cell, the identified period of change (1985– 2009) coincides with the introduction—and subsequent adoption—of the cell phone by the general public.3 Likewise, the period of change for gay agrees with the gay movement which began around the 1970s (Wijaya and Yeniterzi, 2011). 4.3 Limitations In the present work, identification of a changed word is conditioned on its occurring often enough 3http://library.thinkquest.org/04oct/02001/origin.htm Figure 3: Plot of the cosine similarity of changed (gay, cell) and unchanged (by, than) words against their 1900 starting points. Middle line is the average cosine similarity of all words against their starting points in 1900. Shaded region corresponds to one standard deviation of errors. in the study period. If a word’s usage decreased dramatically (or stopped being used altogether), its word vector will have remained the sa</context>
</contexts>
<marker>Wijaya, Yeniterzi, 2011</marker>
<rawString>D.T. Wijaya, R. Yeniterzi. 2011. Understanding semantic change of words over centuries. Proceedings of the 2011 international workshop on DEtecting and Exploiting Cultural diversiTy on the social web: 35–40.</rawString>
</citation>
<citation valid="true">
<authors>
<author>W Yih</author>
<author>K Toutanova</author>
<author>J Platt</author>
<author>C Meek</author>
</authors>
<title>Learning Discriminative Projections for Text Similarity Measures.</title>
<date>2011</date>
<booktitle>Proceedings of the Fifteenth Conference on Computational Natural Language Learning,</booktitle>
<pages>247--256</pages>
<contexts>
<context position="5751" citStr="Yih et al. (2011)" startWordPosition="896" endWordPosition="899">w is the word vector for word w, they note that xapple − xapples ≈ xcar − xcars ≈ xfamily − xfamilies. That is, the concept of pluralization is learned by the vector representations (see Mikolov et al. (2013a) for more examples). NLMs are but one of many methods to obtain word vectors—other techniques include Latent Semantic Analysis (LSA) (Deerwester et al., 1990), Latent Dirichlet Allocation (LDA) (Blei et al., 2003), and variations thereof. And even within NLMs there exist various architectures for learning word vectors (Bengio et al. (2003); Mikolov et al. (2010); Collobert et al. (2011); Yih et al. (2011)). We utilize an architecture introduced by Mikolov et al. (2013b), called the Skip-gram, which allows for efficient estimation of word vectors from large corpora. In a Skip-gram model, each word in the corpus is used to predict a window of surrounding words (Figure 1). To ensure that words closer to the current word are given more weight in training, disFigure 1: Architecture of a Skip-gram model (Mikolov et al., 2013b). tant words are sampled less frequently.2 Training is done through stochastic gradient descent and backpropagation. The word representations are found in the hidden layer. Des</context>
</contexts>
<marker>Yih, Toutanova, Platt, Meek, 2011</marker>
<rawString>W. Yih, K. Toutanova, J. Platt, C. Meek. 2011. Learning Discriminative Projections for Text Similarity Measures. Proceedings of the Fifteenth Conference on Computational Natural Language Learning, 247–256.</rawString>
</citation>
</citationList>
</algorithm>
</algorithms>
<?xml version="1.0" encoding="UTF-8"?>
<algorithms version="110505">
<algorithm name="SectLabel" version="110505">
<variant no="0" confidence="0.032033">
<title confidence="0.9774575">
NCTU and NTUT’s Entry to CLP-2014 Chinese Spelling Check Eval-
uation
</title>
<author confidence="0.998459">
Yih-Ru Wang Yuan-Fu Liao
</author>
<affiliation confidence="0.967302">
National Chiao Tung University National Taipei University of Tech-
HsinChu, Taiwan nology, Taipei, Taiwan
</affiliation>
<email confidence="0.995782">
yrwang@mail.nctu.edu.tw yfliao@ntut.edu.tw
</email>
<sectionHeader confidence="0.993808" genericHeader="abstract">
Abstract
</sectionHeader>
<bodyText confidence="0.999676266666667">
This paper describes our Chinese spelling
check system submitted to SIGHAN Bake-off
2014 evaluation. The system’s main compo-
nents are still the conditional random field
(CRF)-based word segmentation/part-of-
speech (POS) tagger and tri-gram language
model (LM) used last year. But we tried to re-
fine the misspelling rules, decision-making
threshold and improve LM rescoring speed to
reduce false alarm rate and improve rescoring
speed. Bake-off 2014 evaluation results show
that one of our system (Run2) did achieve rea-
sonable performance with about 0.485/0.468
accuracies and 0.226/0.180 F1 scores in the de-
tection/correction metrics.
</bodyText>
<sectionHeader confidence="0.998992" genericHeader="introduction">
1 Introduction
</sectionHeader>
<bodyText confidence="0.987869196721312">
Chinese spelling check could be treated as an ab-
normal word sequence detection problem. There-
fore, word segmentation, part-of-speech (POS)
parser and language models (LM) are usually
adopted to correct the sentence (Bengio 2003).
Therefore, a Chinese spelling checker (Wang
2013) had been built by integrating our condi-
tional random field (CRF)-based parser and a
100K tri-gram LM. Although, these two compo-
nents are originally designed for automatic speech
recognizer (ASR), the system did get some suc-
cess on Bake-off 2013 evaluation (Wu 2013).
These results have confirmed the generalization
and sophistication of our parser and LM.
However, there are still many issues in our sys-
tem. Especially, our system often produces a large
amount of false alarms and requires very long pro-
cessing time on Bake-off 2013 evaluation. There-
fore, the focus of this report is on how to reduce
the false alarm rate, reduce search space and in-
crease computing speed.
2 Summary of the proposed system
The proposed system is an open-set Chinese
spelling check system, i.e., no any training data
prepared by the Bake-off 2014 evaluation organ-
izers were used in the system.
The block diagram of our system is shown in
Fig. 1. There are three main components in the
system including (1) a misspelling rules frontend,
(2) a CRF-based Chinese parser and (3) a 100k
trigram LM.
Basically, our approach is to exchange poten-
tial error characters with their confusable ones and
rescore the modified sentence using our CRF-
based parser and tri-gram LM to see if the modi-
fied one could get better word segmentation result
and higher LM score or not. By this way, potential
spelling error could be detected and corrected.
In this scheme, the input text is first checked
and corrected if there are some high frequency
misspelled words in the rule-based replacement
frontend. The sentence is then segmented into a
word sequence using our CRF-based parser and
scored with a tri-gram LM. Then each character in
short words (less than 3 characters) is considered
as a potential error character and is replaced with
character that has similar shape or pronunciation.
The modified sentence is further re-segmented
and re-scored to get a LM score. This process is
repeated until the best modification (with maxi-
mum LM score) is found.
It could be found that a lot of re-segmentation
and re-scoring computations are required by this
approach. These steps, especially the LM
rescoring, are very time-consumption. Therefore,
the computation of LM score should be done as
efficient as possible.
In the following subsections, the architecture
and performance of the CRF-based parser and LM
modules will be further summarized for better un-
derstanding our approach.
</bodyText>
<page confidence="0.987766">
216
</page>
<note confidence="0.909116">
Proceedings of the Third CIPS-SIGHAN Joint Conference on Chinese Language Processing, pages 216–219,
Wuhan, China, 20-21 October 2014
</note>
<figureCaption confidence="0.987787666666667">
Fig. 1: The schematic diagram of the proposed Chinese
spelling checker. Those shaded blocks had been im-
proved for participating Bake-off 2014 evaluation.
</figureCaption>
<subsectionHeader confidence="0.974344">
2.1 CRF-based traditional Chinese parser
</subsectionHeader>
<bodyText confidence="0.994954055555555">
The block diagram of traditional Chinese parser
is shown in Fig. 2. There are three blocks includ-
ing (1) text normalization, (2) word segmentation
and (3) POS tagging.
Both the word segmentation and POS tagging
modules were based on CRF and trained using
Sinica Balanced Corpus version 4.01. The corpus
had been manually checked and about 1% of in-
consist word-segmentations were corrected. The
word segmentation is basically implemented fol-
lowing Zhan’s work (Zhao 2006), only the radix
cues of the characters (in Chinese, “bushu”) are
add as new features (Wang 2013).
The F-measure of the word segmentation is
96.72% for the original database and 97.50% for
the manually corrected corpus. The difference be-
tween precision and recall rates is less than 0.06%.
About the parser, the accuracy of the 47-type POS
</bodyText>
<footnote confidence="0.99573">
1 http://www.aclclp.org.tw/use_asbc_c.php
2 https://catalog.ldc.upenn.edu/LDC2005T14
</footnote>
<bodyText confidence="0.927235333333333">
tagging is 94.22%. According to these evaluation
results, it is believed that our traditional Chinese
parser is sophisticated enough.
</bodyText>
<figureCaption confidence="0.994618">
Fig. 2: The schematic diagram of the proposed Chinese
parser.
</figureCaption>
<subsectionHeader confidence="0.99629">
2.2 LM construction
</subsectionHeader>
<bodyText confidence="0.999892315789474">
Four text corpora, the LDC Chinese Giga-byte2,
Sinica Balanced Corpus, CIRB0303 (Chinese In-
formation Retrieval Benchmark, version 3.03),
the Taiwan Panorama Magazine4 and context of
Wikipedia (zh_tw version) were used to construct
a 100k tri-gram LM.
There are in total 440 million words in the cor-
pora. They were first parsed and post-processed
(text normalization, word variation replacement,
numbers into short-word conversion, etc.). Then,
a 100k lexicon with most frequently words (with-
out POS information) that have document fre-
quency (DF) higher than a threshold was estab-
lished. Finally, SRLIM toolkit (Stolcke 2000) ver-
sion 1.7.0 was used to build a tri-gram LM for tra-
ditional Chinese.
This LM had been adopted to assist ASR and
got significant improvement (Chen 2012), it is
therefore a well-established LM.
</bodyText>
<footnote confidence="0.924019">
3 http://www.aclclp.org.tw/use_cir.php
4 http://www.aclclp.org.tw/use_gh_c.php (in Chinese)
</footnote>
<figure confidence="0.9990605">
Characters sequence
Symbols
Normalization
Top-N Candidates of
Words sequence
Words/POSs
Sequence with
Base-phrase tagging
Words/POSs
sequence
Characters sequence
Word Segmentation
Word construction
Correction of
Frequently Error
Words (I)
Correction of
Frequently Error
Words (II)
POS Tagger
Base-Phrase
Chunker
Word
construction
Rules
System
Lexicon
User
Lexicon
Words/POSs
</figure>
<page confidence="0.984205">
217
</page>
<sectionHeader confidence="0.984237" genericHeader="method">
3 System improvement
</sectionHeader>
<bodyText confidence="0.951735875">
To speed up the rescoring computation and re-
duce the false alarm rate, several modifications
had been done in this year’s system. They are (1)
misspelling rule expansion, (2) inline language
model computation, (3) decision-making thresh-
old and (4) potential error and exchange candidate
selection. They are all shown as shaded blocks in
Fig. 1.
</bodyText>
<subsectionHeader confidence="0.999453">
3.1 Misspelling rule expansion
</subsectionHeader>
<bodyText confidence="0.9917902">
About 400 more (in total about 1000 now) high
frequency error words were added into our mis-
spelling rules. Those words are also collected
from Internet. The new rules to replace error
words are in general as follows (in Chinese):
</bodyText>
<equation confidence="0.7886555">
腹漲 → 腹脹
行逕 → 行徑
幅射線 → 輻射線
檢查署 → 檢察署
排洩物 → 排泄物
可見一班 → 可見一斑
分道揚鏢 → 分道揚鑣
遺憾終身 → 遺憾終生
</equation>
<figureCaption confidence="0.933148">
Fig. 3: Typical examples of misspelled Chinese word
rules used in the frontend module.
</figureCaption>
<subsectionHeader confidence="0.993803">
3.2 Language model computation
</subsectionHeader>
<bodyText confidence="0.999832695652174">
The confusing tables used in the system in-
cludes many similar shape or pronunciation char-
acters (Liu 2010). There are about 5400 characters
in both the similar shape and pronunciation lists.
Beside, each character has about 26 and 71 similar
shape and pronunciation characters, respectively.
The LM rescoring procedure is therefore very
time-consuming. In fact, it is the major bottleneck
of our system and often requires several days to
finish the evaluation.
Two approaches had been tried to alleviate this
problem. The first one is to change the format of
LM file from an ASCII to a compressed binary
one. The other one is to directly call SRILM’s li-
braries instead of the executables in the rescoring
program.
To call SRILM’s library, three function calls
(as shown in Fig. 2) were embedded into our main
program to load LM, check word index/out-of-vo-
cabulary (OOV) and compute LM score, respec-
tively. By this way, the 100k tri-gram LM was
loaded only once and therefore the LM rescoring
time is significantly improved.
</bodyText>
<table confidence="0.9991318">
// srilm headers
#include “Ngram.h”
// srilm library -loolm -ldstruct -lmisc
// global variables
Vocab vocab;
Ngram*ngram;
//function calls
void srilm_init(const char* fname, int order) {
File file(fname, &amp;quot;r&amp;quot;, 0);
assert(file);
ngram = new Ngram(vocab, order);
ngram-&gt;read(file, false);
cerr &lt;&lt; &amp;quot;Done\n&amp;quot;;
}
int srilm_getvoc(const char* word) {
return vocab.getIndex((VocabString)word);
}
float srilm_wordprob(int w, int* context) {
return (float)ngram-&gt;wordProb(w, (VocabIndex*)context);
}
</table>
<figureCaption confidence="0.770894">
Fig. 4: Application programming interface (APIs) for
initialize SRILM, check word index/OOV and com-
pute LM scores.
</figureCaption>
<subsectionHeader confidence="0.999491">
3.3 Decision-making threshold
</subsectionHeader>
<bodyText confidence="0.9999887">
In our scheme, each sentence is repeatedly
modified, re-segmented and re-scored to find a
word sequence with maximum LM score. How-
ever, the LM scores for different word segmenta-
tions in fact can’t be compared fairly.
To alleviate this issue, a high score threshold
was added into the decision-making logic. In other
words, only those hypotheses that have significant
LM score improvement were selected as candi-
dates.
</bodyText>
<subsectionHeader confidence="0.857646">
3.4 Error and exchange candidate selection
</subsectionHeader>
<bodyText confidence="0.999708529411765">
As mentioned in Section 3.2, for each potential
error character there are many similar shape or
pronunciation confusable ones. However, those
tables may be over-completed.
To save some time, two heuristic rules that take
advantage of a unigram model are applied. The
first one is not to replace those high-frequency
characters. The other one is to ignore those very
low-frequency candidates. By this way, the search
space is dramatically reduced. Bakeoff 2014 Eval-
uation Results
The goal of the checker is to return the locations
of incorrect characters of an input sentence and
suggest the correct characters. The criteria for
judging correctness are: (1) Detection level: all lo-
cations of incorrect characters in a given passage
should be completely identical with the gold
</bodyText>
<page confidence="0.996421">
218
</page>
<bodyText confidence="0.9989316">
standard. (2) Correction level: all locations and
corresponding corrections of incorrect characters
should be completely identical with the gold
standard. There are in total 1,062 test sentences in
the Bake-off 2014 evaluation.
</bodyText>
<sectionHeader confidence="0.974254" genericHeader="method">
4 Evaluation Results
</sectionHeader>
<bodyText confidence="0.990365285714286">
Four configurations of our system (Run1~4)
were tested. Run1 applied only the rule-based
frontend. Run2~4 explored different search space
and LM score threshold. The settings of the dif-
ferent runs are shown in Table 1. Among them,
the search range of Run1~2 is very restricted and
Run3~4 are much larger than others.
</bodyText>
<table confidence="0.9287712">
Run Error Candidate Log
1 - - -
2 50~2000 100~4000 3.0
3 1~3000 1~5000 3.0
4 1~3000 1~5000 1.5
</table>
<tableCaption confidence="0.952605">
Table 1: Character frequency ranking range and LM
</tableCaption>
<bodyText confidence="0.812957461538462">
score threshold settings for different Runs. Here “Error”
and “Candidate” mean the character frequency ranking
range to be considered as potential errors and as ex-
change candidates, respectively.
Table 2 show the all evaluation results. From
Table 2, it can be found that Run1 and Run2 do
have very low false alarm rate, but higher accu-
racy in both measures. The reason is that they only
modified few errors with high confidence. On the
other hand, Run3 and Run4 have higher recall rate
and F1 scores but induce more false alarms. In
summary, these results show our systems, espe-
cially Run1~2, are much conserved.
</bodyText>
<table confidence="0.998373285714286">
Run F/P Detection Level Correction Level
Rate
Acc. Pre. Rec. F1 Acc. Pre. Rec. F1
1 0.038 0.513 0.630 0.064 0.116 0.509 0.600 0.057 0.103
2 0.181 0.485 0.455 0.150 0.226 0.468 0.392 0.117 0.180
3 0.281 0.461 0.420 0.203 0.274 0.435 0.349 0.151 0.211
4 0.642 0.313 0.294 0.267 0.280 0.276 0.232 0.194 0.211
</table>
<tableCaption confidence="0.8954486">
Table 2: Evaluation results of the proposed system on
Bake-off 2014 Chinese spelling check task. The table
shows the false positive (F/P) rate, accuracy (Acc.),
precision (Pre.), recall (Rec.), and F1 score for both the
detection and correction levels.
</tableCaption>
<sectionHeader confidence="0.999232" genericHeader="conclusions">
5 Conclusions
</sectionHeader>
<bodyText confidence="0.999749230769231">
In this paper, several modifications have been
made to improve our Chinese spelling check sys-
tem. Evaluation results show that our systems
have achieved reasonable performance. Espe-
cially, Run2 gains about 0.485/0.468 accuracies
and 0.226/0.180 F1 scores in the detection/correc-
tion levels.
Experimental results also show that a machine
learning-based spelling error detector/classifier
should be added on top of parser and LM to fur-
ther improve system’s performance. Finally, our
latest traditional Chinese parser is available on-
line at http://parser.speech.cm.nctu.edu.tw.
</bodyText>
<sectionHeader confidence="0.99828" genericHeader="acknowledgments">
Acknowledgments
</sectionHeader>
<bodyText confidence="0.99703225">
This work was supported by the Ministry of Sci-
ence and Technology, Taiwan, under the projects
with contract MOST 103-2221-E-027-079 and
MOST 103-2221-E-009-125-MY2.
</bodyText>
<sectionHeader confidence="0.99933" genericHeader="references">
References
</sectionHeader>
<reference confidence="0.999752171428571">
Y. Bengio, R. Ducharme, P. Vincent, and C. Jauvin
(2003), “A neural probabilistic language model,
Journal of Machine Learning Research”, 2003, No.
3(2), pp. 1137–1155.
Sin-Horng Chen, Jyh-Her Yang, Chen-Yu Chiang,
Ming-Chieh Liu and Yih-Ru Wang (2012), &amp;quot;A New
Prosody-Assisted Mandarin ASR System&amp;quot;, IEEE
Trans. on Audio, Speech and Language Processing,
vol.20, no.6, pp.1669,1684, Aug. 2012.
Chao-Lin Liu, Min-Hua Lai, Kan-Wen Tien, Yi-Hsuan
Chuang, Shih-Hung Wu, and Chia-Ying Lee
(2011). Visually and phonologically similar charac-
ters in incorrect Chinese words: Analyses, identifi-
cation, and applications, ACM Trans. Asian Lang.
Inform. Process. 10, 2, Article 10 (June 2011).
A. Stolcke (2002), SRILM -- An Extensible Language
Modeling Toolkit. Proc. Intl. Conf. on Spoken Lan-
guage Processing, vol. 2, pp. 901-904, Denver.
Yih-Ru Wang, Yuan-Fu Liao, Yeh-Kuang Wu and
Liang-Chun Chang (2013). Traditional Chinese Par-
ser and Language Model-Based Chinese Spelling
Checker. Proceedings of the 7th SIGHAN Work-
shop on Chinese Language Processing
(SIGHAN&apos;13), Nagoya, Japan, 14 October, 2013, pp.
69-73.
Shih-Hung Wu, Chao-Lin Liu, and Lung-Hao Lee
(2013). Chinese Spelling Check Evaluation at
SIGHAN Bake-off 2013. Proceedings of the 7th
SIGHAN Workshop on Chinese Language Pro-
cessing (SIGHAN&apos;13), Nagoya, Japan, 14 October,
2013, pp. 35-42.
H. Zhao, C. N. Huang and M. Li (2006), “An Improved
Chinese Word Segmentation System with Condi-
tional Random Field”, the Fifth SIGHAN Workshop
on Chinese Language Processing 2006, pp. 108-117.
</reference>
<page confidence="0.999208">
219
</page>
</variant>
</algorithm>
<algorithm name="ParsHed" version="110505">
<variant no="0" confidence="0.770144">
<title confidence="0.958581">and NTUT’s Entry to CLP-2014 Chinese Spelling Check Evaluation</title>
<author confidence="0.997405">Yih-Ru Wang Yuan-Fu Liao</author>
<affiliation confidence="0.981365">Chiao Tung University National Taipei University of Tech-</affiliation>
<address confidence="0.928067">HsinChu, Taiwan nology, Taipei, Taiwan</address>
<email confidence="0.988692">yrwang@mail.nctu.edu.twyfliao@ntut.edu.tw</email>
<abstract confidence="0.9944480625">This paper describes our Chinese spelling check system submitted to SIGHAN Bake-off 2014 evaluation. The system’s main components are still the conditional random field (CRF)-based word segmentation/part-ofspeech (POS) tagger and tri-gram language model (LM) used last year. But we tried to refine the misspelling rules, decision-making threshold and improve LM rescoring speed to reduce false alarm rate and improve rescoring speed. Bake-off 2014 evaluation results show that one of our system (Run2) did achieve reasonable performance with about 0.485/0.468 accuracies and 0.226/0.180 F1 scores in the detection/correction metrics.</abstract>
</variant>
</algorithm>
<algorithm name="ParsCit" version="110505">
<citationList>
<citation valid="true">
<authors>
<author>Y Bengio</author>
<author>R Ducharme</author>
<author>P Vincent</author>
<author>C Jauvin</author>
</authors>
<title>A neural probabilistic language model,</title>
<date>2003</date>
<journal>Journal of Machine Learning Research”,</journal>
<volume>3</volume>
<issue>2</issue>
<pages>1137--1155</pages>
<marker>Bengio, Ducharme, Vincent, Jauvin, 2003</marker>
<rawString>Y. Bengio, R. Ducharme, P. Vincent, and C. Jauvin (2003), “A neural probabilistic language model, Journal of Machine Learning Research”, 2003, No. 3(2), pp. 1137–1155.</rawString>
</citation>
<citation valid="true">
<authors>
<author>Sin-Horng Chen</author>
</authors>
<title>Jyh-Her Yang, Chen-Yu Chiang, Ming-Chieh Liu and Yih-Ru Wang</title>
<date>2012</date>
<journal>IEEE Trans. on Audio, Speech and Language Processing,</journal>
<volume>20</volume>
<pages>1669--1684</pages>
<contexts>
<context position="5842" citStr="Chen 2012" startWordPosition="904" endWordPosition="905">nd context of Wikipedia (zh_tw version) were used to construct a 100k tri-gram LM. There are in total 440 million words in the corpora. They were first parsed and post-processed (text normalization, word variation replacement, numbers into short-word conversion, etc.). Then, a 100k lexicon with most frequently words (without POS information) that have document frequency (DF) higher than a threshold was established. Finally, SRLIM toolkit (Stolcke 2000) version 1.7.0 was used to build a tri-gram LM for traditional Chinese. This LM had been adopted to assist ASR and got significant improvement (Chen 2012), it is therefore a well-established LM. 3 http://www.aclclp.org.tw/use_cir.php 4 http://www.aclclp.org.tw/use_gh_c.php (in Chinese) Characters sequence Symbols Normalization Top-N Candidates of Words sequence Words/POSs Sequence with Base-phrase tagging Words/POSs sequence Characters sequence Word Segmentation Word construction Correction of Frequently Error Words (I) Correction of Frequently Error Words (II) POS Tagger Base-Phrase Chunker Word construction Rules System Lexicon User Lexicon Words/POSs 217 3 System improvement To speed up the rescoring computation and reduce the false alarm ra</context>
</contexts>
<marker>Chen, 2012</marker>
<rawString>Sin-Horng Chen, Jyh-Her Yang, Chen-Yu Chiang, Ming-Chieh Liu and Yih-Ru Wang (2012), &amp;quot;A New Prosody-Assisted Mandarin ASR System&amp;quot;, IEEE Trans. on Audio, Speech and Language Processing, vol.20, no.6, pp.1669,1684, Aug. 2012.</rawString>
</citation>
<citation valid="true">
<authors>
<author>Chao-Lin Liu</author>
<author>Min-Hua Lai</author>
<author>Kan-Wen Tien</author>
<author>Yi-Hsuan Chuang</author>
<author>Shih-Hung Wu</author>
<author>Chia-Ying Lee</author>
</authors>
<title>Visually and phonologically similar characters in incorrect Chinese words: Analyses, identification, and applications,</title>
<date>2011</date>
<journal>ACM Trans. Asian Lang. Inform. Process.</journal>
<volume>10</volume>
<marker>Liu, Lai, Tien, Chuang, Wu, Lee, 2011</marker>
<rawString>Chao-Lin Liu, Min-Hua Lai, Kan-Wen Tien, Yi-Hsuan Chuang, Shih-Hung Wu, and Chia-Ying Lee (2011). Visually and phonologically similar characters in incorrect Chinese words: Analyses, identification, and applications, ACM Trans. Asian Lang. Inform. Process. 10, 2, Article 10 (June 2011).</rawString>
</citation>
<citation valid="true">
<authors>
<author>A Stolcke</author>
</authors>
<title>SRILM -- An Extensible Language Modeling Toolkit.</title>
<date>2002</date>
<booktitle>Proc. Intl. Conf. on Spoken Language Processing,</booktitle>
<volume>2</volume>
<pages>901--904</pages>
<location>Denver.</location>
<marker>Stolcke, 2002</marker>
<rawString>A. Stolcke (2002), SRILM -- An Extensible Language Modeling Toolkit. Proc. Intl. Conf. on Spoken Language Processing, vol. 2, pp. 901-904, Denver.</rawString>
</citation>
<citation valid="true">
<authors>
<author>Yih-Ru Wang</author>
<author>Yuan-Fu Liao</author>
</authors>
<title>Yeh-Kuang Wu and Liang-Chun Chang (2013). Traditional Chinese Parser and Language Model-Based Chinese Spelling Checker.</title>
<date>2013</date>
<booktitle>Proceedings of the 7th SIGHAN Workshop on Chinese Language Processing (SIGHAN&apos;13),</booktitle>
<pages>69--73</pages>
<location>Nagoya,</location>
<marker>Wang, Liao, 2013</marker>
<rawString>Yih-Ru Wang, Yuan-Fu Liao, Yeh-Kuang Wu and Liang-Chun Chang (2013). Traditional Chinese Parser and Language Model-Based Chinese Spelling Checker. Proceedings of the 7th SIGHAN Workshop on Chinese Language Processing (SIGHAN&apos;13), Nagoya, Japan, 14 October, 2013, pp. 69-73.</rawString>
</citation>
<citation valid="true">
<authors>
<author>Shih-Hung Wu</author>
<author>Chao-Lin Liu</author>
<author>Lung-Hao Lee</author>
</authors>
<title>Chinese Spelling Check Evaluation at SIGHAN Bake-off</title>
<date>2013</date>
<booktitle>Proceedings of the 7th SIGHAN Workshop on Chinese Language Processing (SIGHAN&apos;13),</booktitle>
<pages>35--42</pages>
<location>Nagoya,</location>
<marker>Wu, Liu, Lee, 2013</marker>
<rawString>Shih-Hung Wu, Chao-Lin Liu, and Lung-Hao Lee (2013). Chinese Spelling Check Evaluation at SIGHAN Bake-off 2013. Proceedings of the 7th SIGHAN Workshop on Chinese Language Processing (SIGHAN&apos;13), Nagoya, Japan, 14 October, 2013, pp. 35-42.</rawString>
</citation>
<citation valid="true">
<authors>
<author>H Zhao</author>
<author>C N Huang</author>
<author>M Li</author>
</authors>
<date>2006</date>
<booktitle>An Improved Chinese Word Segmentation System with Conditional Random Field”, the Fifth SIGHAN Workshop on Chinese Language Processing</booktitle>
<pages>108--117</pages>
<marker>Zhao, Huang, Li, 2006</marker>
<rawString>H. Zhao, C. N. Huang and M. Li (2006), “An Improved Chinese Word Segmentation System with Conditional Random Field”, the Fifth SIGHAN Workshop on Chinese Language Processing 2006, pp. 108-117.</rawString>
</citation>
</citationList>
</algorithm>
</algorithms>
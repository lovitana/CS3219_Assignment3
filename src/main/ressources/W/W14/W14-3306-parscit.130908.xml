<?xml version="1.0" encoding="UTF-8"?>
<algorithms version="110505">
<algorithm name="SectLabel" version="110505">
<variant no="0" confidence="0.004110">
<title confidence="0.99795">
English-to-Hindi system description for WMT 2014:
Deep Source-Context Features for Moses
</title>
<author confidence="0.977903">
Marta R. Costa-juss`a1, Parth Gupta2, Rafael E. Banchs3 and Paolo Rosso2
</author>
<affiliation confidence="0.925698666666667">
1Centro de Investigaci´on en Computaci´on, Instituto Polit´ecnico Nacional, Mexico
2NLE Lab, PRHLT Research Center, Universitat Polit`ecnica de Val`encia
3Human Language Technology, Institute for Infocomm Research, Singapore
</affiliation>
<email confidence="0.9780575">
1marta@nlp.cic.ipn.mx,2Jpgupta,prosso}@dsic.upv.es,
3rembanchs@i2r.a-star.edu.sg
</email>
<sectionHeader confidence="0.993723" genericHeader="abstract">
Abstract
</sectionHeader>
<bodyText confidence="0.9999464">
This paper describes the IPN-UPV partici-
pation on the English-to-Hindi translation
task from WMT 2014 International Evalu-
ation Campaign. The system presented is
based on Moses and enhanced with deep
learning by means of a source-context fea-
ture function. This feature depends on the
input sentence to translate, which makes
it more challenging to adapt it into the
Moses framework. This work reports the
experimental details of the system putting
special emphasis on: how the feature func-
tion is integrated in Moses and how the
deep learning representations are trained
and used.
</bodyText>
<sectionHeader confidence="0.998993" genericHeader="introduction">
1 Introduction
</sectionHeader>
<bodyText confidence="0.997443424242424">
This paper describes the joint participation of the
Instituto Polit´ecnico Nacional (IPN) and the Uni-
versitat Polit`ecnica de Valencia (UPV) in cooper-
ation with Institute for Infocomm Research (I2R)
on the 9th Workshop on Statistical Machine Trans-
lation (WMT 2014). In particular, our participa-
tion was in the English-to-Hindi translation task.
Our baseline system is an standard phrase-
based SMT system built with Moses (Koehn et al.,
2007). Starting from this system we propose to in-
troduce a source-context feature function inspired
by previous works (R. Costa-juss`a and Banchs,
2011; Banchs and Costa-juss`a, 2011). The main
novelty of this work is that the source-context fea-
ture is computed in a new deep representation.
The rest of the paper is organized as follows.
Section 2 presents the motivation of this seman-
tic feature and the description of how the source
context feature function is added to Moses. Sec-
tion 3 explains how both the latent semantic in-
dexing and deep representation of sentences are
used to better compute similarities among source
contexts. Section 4 details the WMT experimental
framework and results, which proves the relevance
of the technique proposed. Finally, section 5 re-
ports the main conclusions of this system descrip-
tion paper.
2 Integration of a deep source-context
feature function in Moses
This section reports the motivation and descrip-
tion of the source-context feature function, to-
gether with the explanation of how it is integrated
in Moses.
</bodyText>
<subsectionHeader confidence="0.998496">
2.1 Motivation and description
</subsectionHeader>
<bodyText confidence="0.99984796">
Source context information in the phrase-based
system is limited to the length of the translation
units (phrases). Also, all training sentences con-
tribute equally to the final translation.
We propose a source-context feature func-
tion which measures the similarity between
the input sentence and all training sen-
tences. In this way, the translation unit
should be extended from source target to
source target trainingsentence, with the
trainingsentence the sentence from which
the source and target phrases were extracted.
The measured similarity is used to favour those
translation units that have been extracted from
training sentences that are similar to the current
sentence to be translated and to penalize those
translation units that have been extracted from
unrelated or dissimilar training sentences as
shown in Figure 2.1.
In the proposed feature, sentence similarity is
measured by means of the cosine distance in a
reduced dimension vector-space model, which is
constructed either by means of standard latent se-
mantic analysis or using deep representation as de-
cribed in section 3.
</bodyText>
<page confidence="0.981196">
79
</page>
<bodyText confidence="0.782694375">
Proceedings of the Ninth Workshop on Statistical Machine Translation, pages 79–83,
Baltimore, Maryland USA, June 26–27, 2014. c�2014 Association for Computational Linguistics
S1: we could not book the room in time 5. Each phrase list LS is collapsed into a phrase
T1: hm smy m�\ EVkV aArE?ft nhF\ kr sk�\ table TS by removing repetitions (when re-
moving repeated entries in the list, the largest
S2: I want to write the book in time value of the source-context similarity feature
T2: m{\ smy m�\ EktAb ElKnA cAhtA h~ is retained).
Input: i am reading a nice book
</bodyText>
<figureCaption confidence="0.999558">
Figure 1: Illustration of the method
</figureCaption>
<subsectionHeader confidence="0.99369">
2.2 Integration in Moses
</subsectionHeader>
<bodyText confidence="0.999684294117647">
As defined in the section above and, previously,
in (R. Costa-juss`a and Banchs, 2011; Banchs
and Costa-juss`a, 2011), the value of the proposed
source context similarity feature depends on each
individual input sentence to be translated by the
system. We are computing the similarity between
the source input sentence and all the source train-
ing sentences.
This definition implies the feature function de-
pends on the input sentence to be translated. To
implement this requirement, we followed our pre-
vious implementation of an off-line version of the
proposed methodology, which, although very in-
efficient in the practice, allows us to evaluate the
impact of the source-context feature on a state-of-
the-art phrase-based translation system. This prac-
tical implementation follows the next procedure:
</bodyText>
<listItem confidence="0.998146772727273">
1. Two sentence similarity matrices are com-
puted: one between sentences in the develop-
ment and training sets, and the other between
sentences in the test and training datasets.
2. Each matrix entry mid should contain the
similarity score between the ith sentence in
the training set and the jth sentence in the de-
velopment (or test) set.
3. For each sentence s in the test and develop-
ment sets, a phrase pair list LS of all poten-
tial phrases that can be used during decoding
is extracted from the aligned training set.
4. The corresponding source-context similarity
values are assigned to each phrase in lists LS
according to values in the corresponding sim-
ilarity matrices.
6. Each phrase table is completed by adding
standard feature values (which are computed
in the standard manner).
7. Moses is used on a sentence-per-sentence ba-
sis, using a different translation table for each
development (or test) sentence.
</listItem>
<sectionHeader confidence="0.638501" genericHeader="method">
3 Representation of Sentences
</sectionHeader>
<bodyText confidence="0.999907166666667">
We represent the sentences of the source language
in the latent space by means of linear and non-
linear dimensionality reduction techniques. Such
models can be seen as topic models where the low-
dimensional embedding of the sentences represent
conditional latent topics.
</bodyText>
<subsectionHeader confidence="0.998579">
3.1 Deep Autoencoders
</subsectionHeader>
<bodyText confidence="0.999924260869565">
The non-linear dimensionality reduction tech-
nique we employ is based on the concept of deep
learning, specifically deep autoencoders. Autoen-
coders are three-layer networks (input layer, hid-
den layer and output layer) which try to learn an
identity function. In the neural network represen-
tation of autoencoder (Rumelhart et al., 1986), the
visible layer corresponds to the input layer and
hidden layer corresponds to the latent features.
The autoencoder tries to learn an abstract repre-
sentation of the data in the hidden layer in such
a way that minimizes reconstruction error. When
the dimension of the hidden layer is sufficiently
small, autoencoder is able to generalise and derive
powerful low-dimensional representation of data.
We consider bag-of-words representation of text
sentences where the visible layer is binary feature
vector (v) where vi corresponds to the presence
or absence of ith word. We use binary restricted
Boltzmann machines to construct an autoencoder
as shown in (Hinton et al., 2006). Latent repre-
sentation of the input sentence can be obtained as
shown below:
</bodyText>
<equation confidence="0.999127">
p(h|v) = u(W ∗ v + b) (1)
</equation>
<bodyText confidence="0.997602333333333">
where W is the symmetric weight matrix between
visible and hidden layer and b is hidden layer
bias vector and u(x) is sigmoid logistic function
</bodyText>
<page confidence="0.335839">
1/(1 + exp(−x)).
</page>
<figure confidence="0.9830336">
Input
book :EktAb
book : aArE?ft krnA
S1
S2
</figure>
<page confidence="0.951337">
80
</page>
<bodyText confidence="0.998734307692308">
Autoencoders with single hidden layer do not
have any advantage over linear methods like
PCA (Bourlard and Kamp, 1988), hence we
consider deep autoencoder by stacking multiple
RBMs on top of each other (Hinton and Salakhut-
dinov, 2006). The autoencoders have always been
difficult to train through back-propagation until
greedy layerwise pre-training was found (Hinton
and Salakhutdinov, 2006; Hinton et al., 2006; Ben-
gio et al., 2006). The pre-training initialises the
network parameters in such a way that fine-tuning
them through back-propagation becomes very ef-
fective and efficient (Erhan et al., 2010).
</bodyText>
<subsectionHeader confidence="0.999298">
3.2 Latent Semantic Indexing
</subsectionHeader>
<bodyText confidence="0.999960555555556">
Linear dimensionality reduction technique, latent
semantic indexing (LSI) is used to represent sen-
tences in abstract space (Deerwester et al., 1990).
The term-sentence matrix (X) is created where xij
denotes the occurrence of ith term in jth sentence.
Matrix X is factorized using singular value decom-
position (SVD) method to obtain top m principle
components and the sentences are represented in
this m dimensional latent space.
</bodyText>
<sectionHeader confidence="0.999586" genericHeader="method">
4 Experiments
</sectionHeader>
<bodyText confidence="0.99998524">
This section describes the experiments carried out
in the context of WMT 2014. For English-Hindi
the parallel training data was collected by Charles
University and consisted of 3.6M English words
and 3.97M Hindi words. There was a monolingual
corpus for Hindi comming from different sources
which consisted of 790.8M Hindi words. In ad-
dition, there was a development corpus of news
data translated specifically for the task which con-
sisted of 10.3m English words and 10.1m Hindi
words. For internal experimentation we built a
test set extracted from the training set. We se-
lected randomly 429 sentences from the training
corpus which appeared only once, removed them
from training and used them as internal test set.
Monolingual Hindi corpus was used to build a
larger language model. The language model was
computed doing an interpolation of the language
model trained on the Hindi part of the bilingual
corpus (3.97M words) and the language model
trained on the monolingual Hindi corpus (790.8M
words). Interpolation was optimised in the de-
velopment set provided by the organizers. Both
language models interpolated were 5-grams using
Kneser-Ney smoothing.
The preprocessing of the corpus was done with
the standard tools from Moses. English was low-
ercased and tokenized. Hindi was tokenized with
the simple tokenizer provided by the organizers.
We cleaned the corpus using standard parameters
(i.e. we keep sentences between 1 and 80 words
of length).
For training, we used the default Moses op-
tions, which include: the grow-diag-final and
word alignment symmetrization, the lexicalized
reordering, relative frequencies (conditional and
posterior probabilities) with phrase discounting,
lexical weights and phrase bonus for the trans-
lation model (with phrases up to length 10), a
language model (see details below) and a word
bonus model. Optimisation was done using the
MERT algorithm available in Moses. Optimisa-
tion is slow because of the way integration of the
feature function is done that it requires one phrase
table for each input sentence.
During translation, we dropped unknown words
and used the option of minimum bayes risk de-
coding. Postprocessing consisted in de-tokenizing
Hindi using the standard detokenizer of Moses
(the English version).
</bodyText>
<subsectionHeader confidence="0.998888">
4.1 Autoencoder training
</subsectionHeader>
<bodyText confidence="0.997739428571429">
The architecture of autoencoder we consider was
n-500-128-500-n where n is the vocabulary size.
The training sentences were stemmed, stopwords
were removed and also the terms with sentences
frequency1 less than 20 were also removed. This
resulted in vocabulary size n=7299.
The RBMs were pretrained using Contrastive
Divergence (CD) with step size 1 (Hinton, 2002).
After pretraining, the RBMs were stacked on top
of each other and unrolled to create deep autoen-
coder (Hinton and Salakhutdinov, 2006). During
the fine-tuning stage, we backpropagated the re-
construction error to update network parameters.
The size of mini-batches during pretraining and
fine-tuning were 25 and 100 respectively. Weight
decay was used to prevent overfitting. Addition-
ally, in order to encourage sparsity in the hid-
den units, Kullback-Leibler sparsity regularization
was used. We used GPU2 based implementation of
autoencoder to train the models which took around
4.5 hours for full training.
</bodyText>
<footnote confidence="0.99924725">
1total number of training sentences in which the term ap-
pears
2NVIDIA GeForce GTX Titan with Memory 5 GiB and
2688 CUDA cores
</footnote>
<page confidence="0.998487">
81
</page>
<sectionHeader confidence="0.523341" genericHeader="evaluation">
4.2 Results
</sectionHeader>
<bodyText confidence="0.9630725">
Table 1 shows the improvements in terms of
BLEU of adding deep context over the baseline
system for English-to-Hindi (En2Hi) over devel-
opment and test sets. Adding source-context infor-
mation using deep learning outperforms the latent
semantic analysis methodology.
</bodyText>
<table confidence="0.9984634">
En2Hi
Dev Test
baseline 9.42 14.99
+lsi 9.83 15.12
+deep context 10.40† 15.43†
</table>
<tableCaption confidence="0.9685055">
Table 1: BLEU scores for En2Hi translation task..
† depicts statistical significance (p-value&lt;0.05).
</tableCaption>
<bodyText confidence="0.999034333333333">
Our source-context feature function may be
more discriminative in a task like English-to-Hindi
where the target language has a larger vocabulary
than the source one.
Table 2 shows an example of how the translation
is improving in terms of lexical semantics which is
the goal of the methodology presented in the pa-
per. The most frequent sense of word cry is ronA,
which literally means “to cry” while the example
in Table 2 refers to the sense of cry as cFK, which
means to scream. Our method could identify the
context and hence the source context feature (scf)
of the unit cry cFK is higher than for the unit
scf(cry ronA) as shown in Table 3 and for this
particular input sentence.
</bodyText>
<sectionHeader confidence="0.998904" genericHeader="conclusions">
5 Conclusion
</sectionHeader>
<bodyText confidence="0.9996055">
This paper reports the IPN-UPV participation in
the WMT 2014 Evaluation Campaign. The system
is Moses-based with an additional feature function
based on deep learning. This feature function in-
troduces source-context information in the stan-
dard Moses system by adding the information of
how similar is the input sentence to the different
training sentences. Significant improvements over
</bodyText>
<table confidence="0.92434">
cp pp scf
cry ronA 0.23 0.06 0.85
cry cFK 0.15 0.04 0.90
</table>
<tableCaption confidence="0.964589">
Table 3: Probability values (conditional, cp, and
</tableCaption>
<bodyText confidence="0.958336625">
posterior, pp, as standard features in a phrase-
based system) for the word cry and two Hindi
translations.
the baseline system are reported in the task from
English to Hindi.
As further work, we will implement our feature
function in Moses using suffix arrays in order to
make it more efficient.
</bodyText>
<sectionHeader confidence="0.996934" genericHeader="acknowledgments">
Acknowledgements
</sectionHeader>
<bodyText confidence="0.9981099">
This work has been supported in part by
Spanish Ministerio de Economia y Compet-
itividad, contract TEC2012-38939-C03-02 as
well as from the European Regional Develop-
ment Fund (ERDF/FEDER). The work of the
second and fourth authors is also supported
by WIQ-EI (IRSES grant n. 269180) and
DIANA-APPLICATIONS (TIN2012-38603-C02-
01) project and VLC/CAMPUS Microcluster on
Multimodal Interaction in Intelligent Systems.
</bodyText>
<sectionHeader confidence="0.99602" genericHeader="references">
References
</sectionHeader>
<reference confidence="0.917986321428571">
Rafael E. Banchs and Marta R. Costa-juss`a. 2011. A
semantic feature for statistical machine translation.
In Proceedings of the Fifth Workshop on Syntax,
Semantics and Structure in Statistical Translation,
SSST-5, pages 126–134.
Yoshua Bengio, Pascal Lamblin, Dan Popovici, and
Hugo Larochelle. 2006. Greedy layer-wise training
of deep networks. In NIPS, pages 153–160.
Herv´e Bourlard and Yves Kamp. 1988. Auto-
association by multilayer perceptrons and singu-
lar value decomposition. Biological Cybernetics,
59(4):291–294, September.
System Translation
Source soft cry from the depth
Baseline ghrAiyo\ s� m�lAym ron� lgt~
+deep context ghrAiyo\ s� m�lAym cFK
Reference ghrAiyo\s�komlcFK
Table 2: Manual analysis of a translation output.
Scott Deerwester, Susan T. Dumais, George W. Fur-
nas, Thomas K. Landauer, and Richard Harshman.
1990. Indexing by latent semantic analysis. Jour-
nal of the American Society for Information Science,
41(6):391–407.
Dumitru Erhan, Yoshua Bengio, Aaron C. Courville,
Pierre-Antoine Manzagol, Pascal Vincent, and Samy
Bengio. 2010. Why does unsupervised pre-training
help deep learning? Journal of Machine Learning
Research, 11:625–660.
</reference>
<page confidence="0.980473">
82
</page>
<reference confidence="0.999679892857143">
Geoffrey Hinton and Ruslan Salakhutdinov. 2006. Re-
ducing the dimensionality of data with neural net-
works. Science, 313(5786):504 – 507.
Geoffrey E. Hinton, Simon Osindero, and Yee Whye
Teh. 2006. A fast learning algorithm for deep belief
nets. Neural Computation, 18(7):1527–1554.
Geoffrey E. Hinton. 2002. Training products of ex-
perts by minimizing contrastive divergence. Neural
Computation, 14(8):1771–1800.
Philipp Koehn, Hieu Hoang, Alexandra Birch, Chris
Callison-Burch, Marcello Federico, Nicola Bertoldi,
Brooke Cowan, Wade Shen, Christine Moran,
Richard Zens, Chris Dyer, Ondˇrej Bojar, Alexandra
Constantin, and Evan Herbst. 2007. Moses: Open
source toolkit for statistical machine translation. In
Proceedings of the 45th Annual Meeting of the ACL
on Interactive Poster and Demonstration Sessions,
ACL ’07, pages 177–180.
Marta R. Costa-juss`a and Rafael E. Banchs. 2011. The
bm-i2r haitian-cr´eole-to-english translation system
description for the wmt 2011 evaluation campaign.
In Proceedings of the Sixth Workshop on Statistical
Machine Translation, pages 452–456, Edinburgh,
Scotland, July. Association for Computational Lin-
guistics.
David E. Rumelhart, Geoffrey E. Hinton, and Ronald J.
Williams. 1986. Learning representations by back-
propagating errors. Nature, 323(6088):533–536.
</reference>
<page confidence="0.999312">
83
</page>
</variant>
</algorithm>
<algorithm name="ParsHed" version="110505">
<variant no="0" confidence="0.662913">
<title confidence="0.997262">English-to-Hindi system description for WMT Deep Source-Context Features for Moses</title>
<author confidence="0.975952">R Parth Rafael E</author>
<author confidence="0.975952">Paolo</author>
<affiliation confidence="0.857006">de Investigaci´on en Computaci´on, Instituto Polit´ecnico Nacional, Lab, PRHLT Research Center, Universitat Polit`ecnica de Language Technology, Institute for Infocomm Research,</affiliation>
<abstract confidence="0.9987494375">This paper describes the IPN-UPV participation on the English-to-Hindi translation task from WMT 2014 International Evaluation Campaign. The system presented is based on Moses and enhanced with deep learning by means of a source-context feature function. This feature depends on the input sentence to translate, which makes it more challenging to adapt it into the Moses framework. This work reports the experimental details of the system putting special emphasis on: how the feature function is integrated in Moses and how the deep learning representations are trained and used.</abstract>
</variant>
</algorithm>
<algorithm name="ParsCit" version="110505">
<citationList>
<citation valid="true">
<authors>
<author>Rafael E Banchs</author>
<author>Marta R Costa-juss`a</author>
</authors>
<title>A semantic feature for statistical machine translation.</title>
<date>2011</date>
<booktitle>In Proceedings of the Fifth Workshop on Syntax, Semantics and Structure in Statistical Translation, SSST-5,</booktitle>
<pages>126--134</pages>
<marker>Banchs, Costa-juss`a, 2011</marker>
<rawString>Rafael E. Banchs and Marta R. Costa-juss`a. 2011. A semantic feature for statistical machine translation. In Proceedings of the Fifth Workshop on Syntax, Semantics and Structure in Statistical Translation, SSST-5, pages 126–134.</rawString>
</citation>
<citation valid="true">
<authors>
<author>Yoshua Bengio</author>
<author>Pascal Lamblin</author>
<author>Dan Popovici</author>
<author>Hugo Larochelle</author>
</authors>
<title>Greedy layer-wise training of deep networks.</title>
<date>2006</date>
<booktitle>In NIPS,</booktitle>
<pages>153--160</pages>
<contexts>
<context position="8108" citStr="Bengio et al., 2006" startWordPosition="1265" endWordPosition="1269">ic weight matrix between visible and hidden layer and b is hidden layer bias vector and u(x) is sigmoid logistic function 1/(1 + exp(−x)). Input book :EktAb book : aArE?ft krnA S1 S2 80 Autoencoders with single hidden layer do not have any advantage over linear methods like PCA (Bourlard and Kamp, 1988), hence we consider deep autoencoder by stacking multiple RBMs on top of each other (Hinton and Salakhutdinov, 2006). The autoencoders have always been difficult to train through back-propagation until greedy layerwise pre-training was found (Hinton and Salakhutdinov, 2006; Hinton et al., 2006; Bengio et al., 2006). The pre-training initialises the network parameters in such a way that fine-tuning them through back-propagation becomes very effective and efficient (Erhan et al., 2010). 3.2 Latent Semantic Indexing Linear dimensionality reduction technique, latent semantic indexing (LSI) is used to represent sentences in abstract space (Deerwester et al., 1990). The term-sentence matrix (X) is created where xij denotes the occurrence of ith term in jth sentence. Matrix X is factorized using singular value decomposition (SVD) method to obtain top m principle components and the sentences are represented in </context>
</contexts>
<marker>Bengio, Lamblin, Popovici, Larochelle, 2006</marker>
<rawString>Yoshua Bengio, Pascal Lamblin, Dan Popovici, and Hugo Larochelle. 2006. Greedy layer-wise training of deep networks. In NIPS, pages 153–160.</rawString>
</citation>
<citation valid="true">
<authors>
<author>Herv´e Bourlard</author>
<author>Yves Kamp</author>
</authors>
<title>Autoassociation by multilayer perceptrons and singular value decomposition.</title>
<date>1988</date>
<journal>Biological Cybernetics,</journal>
<volume>59</volume>
<issue>4</issue>
<contexts>
<context position="7792" citStr="Bourlard and Kamp, 1988" startWordPosition="1218" endWordPosition="1221">s binary feature vector (v) where vi corresponds to the presence or absence of ith word. We use binary restricted Boltzmann machines to construct an autoencoder as shown in (Hinton et al., 2006). Latent representation of the input sentence can be obtained as shown below: p(h|v) = u(W ∗ v + b) (1) where W is the symmetric weight matrix between visible and hidden layer and b is hidden layer bias vector and u(x) is sigmoid logistic function 1/(1 + exp(−x)). Input book :EktAb book : aArE?ft krnA S1 S2 80 Autoencoders with single hidden layer do not have any advantage over linear methods like PCA (Bourlard and Kamp, 1988), hence we consider deep autoencoder by stacking multiple RBMs on top of each other (Hinton and Salakhutdinov, 2006). The autoencoders have always been difficult to train through back-propagation until greedy layerwise pre-training was found (Hinton and Salakhutdinov, 2006; Hinton et al., 2006; Bengio et al., 2006). The pre-training initialises the network parameters in such a way that fine-tuning them through back-propagation becomes very effective and efficient (Erhan et al., 2010). 3.2 Latent Semantic Indexing Linear dimensionality reduction technique, latent semantic indexing (LSI) is used</context>
</contexts>
<marker>Bourlard, Kamp, 1988</marker>
<rawString>Herv´e Bourlard and Yves Kamp. 1988. Autoassociation by multilayer perceptrons and singular value decomposition. Biological Cybernetics, 59(4):291–294, September.</rawString>
</citation>
<citation valid="false">
<title>System Translation Source Baseline +deep context Reference soft cry from the depth ghrAiyo\ s� m�lAym ron� lgt~ ghrAiyo\ s� m�lAym cFK ghrAiyo\s�komlcFK</title>
<marker></marker>
<rawString>System Translation Source Baseline +deep context Reference soft cry from the depth ghrAiyo\ s� m�lAym ron� lgt~ ghrAiyo\ s� m�lAym cFK ghrAiyo\s�komlcFK</rawString>
</citation>
</citationList>
</algorithm>
</algorithms>
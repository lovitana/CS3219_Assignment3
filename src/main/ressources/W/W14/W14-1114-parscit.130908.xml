<?xml version="1.0" encoding="UTF-8"?>
<algorithms version="110505">
<algorithm name="SectLabel" version="110505">
<variant no="0" confidence="0.164407">
<title confidence="0.996134">
Reducing VSM data sparseness by generalizing contexts:
application to health text mining
</title>
<author confidence="0.965678">
Amandine P´erinet Thierry Hamon
</author>
<affiliation confidence="0.637907">
INSERM, U1142, LIMICS, Paris, France LIMSI-CNRS, Orsay, France
Sorbonne Universit´es, UPMC Univ Paris 06, Paris, France Universit´e Paris 13, Sorbonne Paris Cit´e
Universit´e Paris 13, Sorbonne Paris Cit´e, Villetaneuse, France Villetaneuse, France
</affiliation>
<email confidence="0.995128">
amandine.perinet@edu.univ-paris13.fr hamon@limsi.fr
</email>
<sectionHeader confidence="0.997276" genericHeader="abstract">
Abstract
</sectionHeader>
<bodyText confidence="0.99868">
Vector Space Models are limited with low
frequency words due to few available con-
texts and data sparseness. To tackle this
problem, we generalize contexts by inte-
grating semantic relations acquired with
linguistic approaches. We use three meth-
ods that acquire hypernymy relations on a
EHR corpus. Context Generalization ob-
tains the best results when performed with
hypernyms, the quality of the relations be-
ing more important than the quantity.
</bodyText>
<sectionHeader confidence="0.999391" genericHeader="keywords">
1 Introduction
</sectionHeader>
<bodyText confidence="0.999870022727273">
Distributional Analysis (DA) (Harris, 1954; Firth,
1957) computes a similarity between target words
from the contexts shared by those two words.
This hypothesis is applied with geometric meth-
ods, such as the Vector Space Model (VSM) (Tur-
ney and Pantel, 2010). The advantage of the VSM
is that the similarity of word meaning can be easily
quantified by measuring their distance in the vec-
tor space, or the cosine of the angle between them
(Mitchell and Lapata, 2010). On the other hand,
a major inconvenience is data sparseness within
the matrix that represents the vector space (Tur-
ney and Pantel, 2010). The data sparseness prob-
lem is the consequence of the word distribution in
a corpus (Baroni et al., 2009): in any corpus, most
of the words have a very low frequency and ap-
pear only a few times. Thus, those words have a
limited set of contexts and similarity is difficult to
catch. Thus, methods based on DA perform better
when more information is available (Weeds and
Weir, 2005; van der Plas, 2008) and are efficient
with large corpora of general language. But with
specialized texts, as EHR texts that are usually of
smaller size, reducing data sparseness is a major
issue and methods need to be adapted.
Semantic grouping of contexts should decrease
their diversity, and thus increase the frequency of
the remaining generalized contexts. We assume
that generalizing contexts may influence the distri-
butional context frequencies. Information for gen-
eralization can be issued from existing resources
or can be computed by linguistic approaches. In
this paper, we propose to use semantic relations
acquired by relation acquisition methods to group
words in contexts. We define a method that
switches words in DA contexts for their hierar-
chical parent or morphosyntactic variant that have
been computed on the corpus with linguistic ap-
proaches before applying the VSM method.
In the following, we first present the related
work, then our method and we finally describe the
different experiments we led. The results obtained
on the EHR corpus are then evaluated in terms of
precision and MAP, and analyzed.
</bodyText>
<sectionHeader confidence="0.999868" genericHeader="introduction">
2 Related work
</sectionHeader>
<bodyText confidence="0.999885681818182">
Our approach relates with works that influence
distributional contexts to improve the performance
of VSMs. Some of them intend to change the
way to consider contexts; Broda et al. (2009) do
not use the raw context frequency in DA, but they
first rank contexts according to their frequency,
and take the rank into account. Other models
use statistical language models to determine the
most likely substitutes to represent the contexts
(Baskaya et al., 2013). They assign probabilities to
arbitrary sequences of words that are then used to
create word pairs to feed a co-occurrence model,
before performing a clustered algorithm (Yuret,
2012). The limit of such methods is that their per-
formance is proportional to vocabulary size and re-
quires the availability of training data.
Influence on contexts may also be performed by
embedding additional semantic information. The
semantic relations may be issued from an exist-
ing resource or automatically computed. With
a method based on bootstrapping, Zhitomirsky-
Geffet and Dagan (2009) modify the weights of
</bodyText>
<page confidence="0.98126">
90
</page>
<note confidence="0.6863045">
Proceedings of the 5th International Workshop on Health Text Mining and Information Analysis (Louhi) @ EACL 2014, pages 90–95,
Gothenburg, Sweden, April 26-30 2014. c�2014 Association for Computational Linguistics
</note>
<bodyText confidence="0.999882592592593">
the elements in contexts relying on the seman-
tic neighbors found with a distributional similar-
ity measure. Based on this work, Ferret (2013)
uses a set of examples selected from an origi-
nal distributional thesaurus to train a supervised
classifier. This classifier is then applied for
reranking the neighbors of the thesaurus selec-
tion. Within Vector Space Model, Tsatsaronis and
Panagiotopoulou (2009) use a word thesaurus to
interpret the orthogonality of terms and measure
semantic relatedness.
With the same purpose of solving the problem of
data sparseness, other methods are based on di-
mensionality reduction, such as Latent Semantic
Analysis (LSA) in (Pad´o and Lapata, 2007) or
Non-negative Matrix Factorization (NMF) (Zheng
et al., 2011). Matrix decomposition techniques are
usually applied to reduce the dimensionality of the
original matrix, thereby rendering it more infor-
mative (Mitchell and Lapata, 2010).
Our approach differs from the aforementioned
ones in that we add semantic information in con-
texts to reduce the number of contexts and to in-
crease their frequency. Contrary to these latter ap-
proaches, we do not reduce the contexts by remov-
ing information but by generalyzing information
and integrating extra semantic knowledge.
</bodyText>
<sectionHeader confidence="0.98945" genericHeader="method">
3 VSM and context generalization
</sectionHeader>
<bodyText confidence="0.998920647887324">
The contexts in which occurs a target word have
associated frequencies which may be used to form
probability estimates. The goal of our method is
to influence the distributional context frequencies
by generalizing contexts.
Step 1: target and context definition During
this step, we define targets and contexts, with dif-
ferent constraints for their extraction. To adapt
our method to specialized texts, we identify terms
(specific terminological entities that denote an
event) with a term extractor (YATEA (Aubin and
Hamon, 2006)). Target words are both nouns
and terms (T). Their distributional contexts corre-
spond to a graphical window of n number of words
around the targets (Wilks et al., 1990; Sch¨utze,
1998; Lund and Burgess, 1996). We consider two
different window sizes defined in section 4.
Linguistic approaches During the generaliza-
tion process, we use three existing linguistic ap-
proaches: two that acquire hypernymy relations
and one to get morphosyntactic variants. Lexico-
syntactic Patterns (LSP) acquire hypernymy re-
lations. We use the patterns defined by (Hearst,
1992). Lexical Inclusion (LI) acquires hypernymy
relations and uses the syntactic analysis of the
terms. Based on the hypothesis that if a term is
lexically included in another, generally there is a
hypernymy relation between the two terms (kid-
ney transplant - cadaveric kidney transplant) (Bo-
denreider et al., 2001). Terminological Variation
(TV) acquires both hypernyms and synonyms. TV
uses rules that define a morpho-syntactic transfor-
mation, mainly the insertion (blood transfusion -
blood cell transfusion (Jacquemin, 1996).
Step 2: context generalization Once targets
and contexts are defined, we generalize contexts
with the relations acquired by the three linguis-
tic approaches we mentioned. To integrate the
relations in contexts, we replace words in con-
text by their hypernym or morphosyntactic variant.
We define two rules: (1) if the context matches
with one hypernym, context is replaced by this
hypernym. (2) if the context matches with sev-
eral hypernyms or variants, we take the hypernym
or variant frequency into account, and choose the
most frequent hypernym/variant. The generaliza-
tion step is individually or sequentially performed
when several relation sets are available.
Step 3: computation of semantic similarity
After the generalization step, similarity between
target words is computed. As we previously de-
crease diversity in contexts, we choose a mea-
sure that favors words appearing in similar con-
texts. We use the Jaccard Index (Grefenstette,
1994) which normalizes the number of contexts
shared by two words by the total number of con-
texts of those two words.
Parameter: thresholds The huge number of re-
lations we obtain after computing similarity be-
tween targets leads us to remove the supposed
wrong relations with three thresholds: (i) number
of shared lemmatized contexts (2 for a large win-
dow, 1 for a small window) ; (ii) number of the
lemmatized contexts (2 for a large window, 1 for
a small window) ; (iii) number of the lemmatized
targets (3 for both window sizes). For each pa-
rameter, the threshold is automatically computed,
according to the corpus, as the mean of the values
of parameters on the corpus. And we experiment
two thresholds on similarity score we empirically
defined: sim &gt; 0.001 and sim &gt; 0.0005.
</bodyText>
<page confidence="0.997801">
91
</page>
<sectionHeader confidence="0.999461" genericHeader="method">
4 Experiments
</sectionHeader>
<bodyText confidence="0.9999865">
In this section, we present the material we use for
the experiments and evaluation, and the distribu-
tional parameter values of the VSM automatically
determined from the data. We then describe the
generalization sets we experiment and the evalua-
tion measures we used for evaluation.
</bodyText>
<subsectionHeader confidence="0.991745">
4.1 Corpus
</subsectionHeader>
<bodyText confidence="0.999613125">
We use the collection of anonymous clinical En-
glish texts provided by the 2012 i2b2/VA chal-
lenge (Sun et al., 2013).
The corpus is pre-processed within the Ogmios
platform (Hamon et al., 2007). We perform mor-
phosyntactic tagging and lemmatization with Tree
Tagger (Schmid, 1994), and term extraction with
YATEA (Aubin and Hamon, 2006).
</bodyText>
<subsectionHeader confidence="0.985558">
4.2 Distributional parameters
</subsectionHeader>
<bodyText confidence="0.999960923076923">
We consider two window sizes: a large window
of 21 words (± 10 words, centered on the tar-
get, henceforth W21) and a narrow one of 5 words
(± 2 words, centered on the target, W5).
The window size influences on the type, the
volume and the quality of the acquired relations.
Generally, the smaller windows allow to acquire
more relevant contexts for a target, but increase
the data sparseness problem (Rapp, 2003). They
give better results for classical types of relations
(eg. synonymy), whereas larger windows are more
appropriate for domain relations (eg. colloca-
tions)(Sahlgren, 2006; Peirsman et al., 2008).
</bodyText>
<subsectionHeader confidence="0.999677">
4.3 Generalizing distributional contexts
</subsectionHeader>
<bodyText confidence="0.998922793103448">
We define several sets of context generalization.
We experiment in step 2 different ways of gener-
alizing contexts. We use as a baseline the VSM
without any generalization in the contexts (VS-
Monly), and compare the generalization sets to it.
Regarding context generalization, we first ex-
ploit the relations acquired from only one linguis-
tic approach. We apply the method described
at the section 3 (step 2) by separately using the
three different sets of relations automatically ac-
quired. Distributional contexts are replaced by
their hypernym acquired with lexico-syntactic pat-
terns (VSM/LSP) and lexical inclusion (VSM/LI),
and by their morphosyntactic variants acquired
with terminological variation (VSM/TV). Then,
we replace contexts with relations acquired by two
approaches (TV then LI, LSP then TV, etc.). This
generalization is done sequentially: we generalize
all the contexts with the relations acquired by one
method (e.g. LI), and then with the relations ac-
quired by another method (e.g. TV). And finally,
similarly to what we perform with two methods,
we experiment the generalization of contexts by
relations acquired with the three different linguis-
tic approaches (e.g. LSP then LI then TV). We
experiment all the possible combinations. With
both the single and multiple generalization, we
aim at evaluating the contribution of each method
but also the impact of the order of the methods.
</bodyText>
<subsectionHeader confidence="0.986822">
4.4 Evaluation
</subsectionHeader>
<bodyText confidence="0.999985818181818">
In order to evaluate the quality of the acquired re-
lations, we compare our relations to the 53,203
UMLS relations between terms occurring in our
EHR corpus. We perform the evaluation with
the Mean Average Precision (MAP) (Buckley and
Voorhees, 2005) and the macro-precision com-
puted for each target word: semantic neighbors
found in the resource by the total semantic neigh-
bors acquired by our method. We consider three
sets of neighbors: precision after examining 1
(P@1), 5 (P@5) and 10 (P@10) neighbors.
</bodyText>
<sectionHeader confidence="0.999592" genericHeader="evaluation">
5 Results and discussion
</sectionHeader>
<bodyText confidence="0.999986125">
Best results are obtained with a large window of
21 words, with a precision P@1 of 0.243 against
0.032 for a 5 word window, both for VSMonly,
with a threshold of 0.001. Thus, a high thresh-
old on the similarity score is not always relevant.
We observe on this corpus that the generalization
with the several linguistic approaches does not im-
prove the results. For instance, VSM/LI obtains
0.250 of P@1 with a &gt; 0.001 threshold, and this
precision is the same with VSM/LI+TV and with
VSM/LI+LSP. This is an interesting behavior, dif-
ferent from what have been observed so far on
more general French corpora that contains cook-
ing recipes (P´erinet and Hamon, 2013).
We discuss here the results we obtain for terms,
for the two thresholds on the similarity score: a
low and a higher thresholds, with relations with a
similarity above 0.0005 and above 0.001. We ob-
serve that with a higher threshold, the precision is
higher, with a P@1 of 0.243 against 0.187 for the
lower threshold (when considering VSMonly). As
for the number of relations acquired, with a lower
threshold we obtain more relations (3,936 rela-
tions acquired for the baseline) than with a higher
</bodyText>
<page confidence="0.990662">
92
</page>
<bodyText confidence="0.999821263157895">
threshold (326 relations for the baseline).
We evaluate precison after examining three
groups of neighbors. The best results are ob-
tained with P@1, and in most cases, precision de-
creases when we consider more neighbors: the
more neighbors we consider, the lower precision
is. For a 0.001 threshold, the generalized experi-
ment sets obtain a higher precision than VSMonly,
in any case. While for a 0005 threshold, the use of
LI to generalize contexts decreases the precision.
We also observe that when considering generali-
sation with TV or LSP only, or their combination,
the P@10 is slightly better than P@5.
The MAP values are higher when the thresold
on the similarity measure is low, with 0.446 for
VSM/LI against 0.089 with the &gt; 0.001 thresh-
old. It means that some correct relations are not
well ranked with the similarity score, but are still
present. We observe that the MAP values are
always higher with the generalization sets than
with the baseline with both thresholds: 0.089 for
VSM/LI, 0.446 for VSM/LI+LSP, etc.
Comparison of the experimental sets When
considering the relations found in the UMLS, we
observe that the generalization with LSP brings
the same relations that the baseline VSMonly plus
22 relations, the generalization with TV brings 16
more relations that VSMonly, and finally that the
generalization with LI decreases the number of re-
lations acquired. When the generalization of the
contexts is performed with LI, only with LI or with
LI combined to another method, it decreases the
number of relations acquired as well as the num-
ber of relations found in the resource. On the con-
trary, generalizing contexts with LSP increases the
number of relations acquired as well as the num-
ber of relations found in the UMLS resource. We
obtain the highest number of relations when gener-
alizing contexts with LSP, with 454 relations, and
the highest precision with 0.273 for P@1.
Comparing those results with the relations ac-
quired with the linguistic approaches on the EHR
corpus shows a correlation between the quality of
the relations acquired with the generalized sets and
the relations used for generalization. Indeed, LI
gives the highest number of relations with 14,437
relations, then TV gives 631 relations, and fi-
nally LSP acquires only three relations: pancre-
atic complication - necrosis, pancreatic complica-
tion - abscess, gentle laxative - milk of magnesia.
With these relations, if the second term (eg.
necrosis) is found in the context, it is replaced by
the first term (eg. pancreatic complication). These
three relations used for generalization give better
results in terms of precision that the many relations
given by the two other approaches. We could de-
duce that the number of relations may not be as im-
portant as their quality when they are used for gen-
eralization. But when the LSP are used after TV or
LI, they do not improve the results. From this ob-
servation, we make the hypothesis that these sec-
ond terms may have already been replaced during
the generalization with LI or TV. To confirm or re-
ject this hypothesis, we look closer to the relations
acquired with TV and LI. In TV, we find no rela-
tion including any of these second terms. On the
contrary, with LI, we found the relation milk - milk
of magnesia that inhibits one of the three relations
acquired with the LSP.
We deduce that even if the quality of the re-
lations used for generalization is more important
than their number, the number of relations still
matters. If generalization is first performed with
a great number of relations, then a small number
of relations used for generalization is not enough
and does not improve the results.
</bodyText>
<sectionHeader confidence="0.954447" genericHeader="conclusions">
6 Conclusion and perspectives
</sectionHeader>
<bodyText confidence="0.999785217391304">
In this work, we face the problem of data sparese-
ness of distributional methods. This problem espe-
cially arises from specialized corpora which have
a smaller size and in which words and terms have
lower frequencies.
To achieve this goal, we propose to generalize
distributional contexts with hypernyms and vari-
ants acquired by three existing approaches. We fo-
cus on the acquistion of relations between terms.
We experimented several generalization sets, us-
ing one, two or the three methods sequentially
to replace words in context by their hypernym or
variant. Evaluation of the method has been per-
formed on an EHR English text collection. Gen-
eralization obtains the best results when realized
with hypernyms. The quality of the relations mat-
ters much more than their number: few but good
relations used to generalize contexts give better re-
sults than many relations of poorer quality. For
future work, we plan to use for generalization re-
lations issued from different distributional and ter-
minological resources. Finally, we will intend to
combine the methods before normalization.
</bodyText>
<page confidence="0.998103">
93
</page>
<sectionHeader confidence="0.996092" genericHeader="references">
References
</sectionHeader>
<reference confidence="0.999019352941177">
Sophie Aubin and Thierry Hamon. 2006. Improving
term extraction with terminological resources. In
Advances in Natural Language Processing, number
4139 in LNAI, pages 380–387. Springer.
Marco Baroni, Silvia Bernardini, Adriano Ferraresi,
and Eros Zanchetta. 2009. The wacky wide web:
a collection of very large linguistically processed
web-crawled corpora. Language Resources and
Evaluation, 43(3):209–226.
Osman Baskaya, Enis Sert, Volkan Cirik, and Deniz
Yuret. 2013. Ai-ku: Using substitute vectors and
co-occurrence modeling for word sense induction
and disambiguation. In Proceedings of SemEval -
2013, pages 300–306, Atlanta, Georgia, USA. As-
sociation for Computational Linguistics.
Olivier Bodenreider, Anita Burgun, and Thomas Rind-
flesch. 2001. Lexically-suggested hyponymic rela-
tions among medical terms and their representation
in the umls. In TIA 2001, pages 11–21.
Bartosz Broda, Maciej Piasecki, and Stan Szpakowicz.
2009. Rank-based transformation in measuring se-
mantic relatedness. In Yong Gao and Nathalie Jap-
kowicz, editors, Canadian Conference on AI, vol-
ume 5549, pages 187–190. Springer.
Chris Buckley and Ellen Voorhees. 2005. Retrieval
system evaluation. In Ellen Voorhees and Donna
Harman, editors, TREC: Experiment and Evaluation
in Information Retrieval, chapter 3. MIT Press.
Olivier Ferret. 2013. S´election non supervis´ee de
relations s´emantiques pour am´eliorer un th´esaurus
distributionnel. In TALN 2013, pages 48–61, Les
Sables d’Olonne, France.
J.R. Firth. 1957. A synopsis of linguistic theory 1930-
1955. Studies in linguistic analysis, pages 1–32.
Gregory Grefenstette. 1994. Corpus-derived first, sec-
ond and third-order word affinities. In Sixth Euralex
International Congress, pages 279–290.
T. Hamon, A. Nazarenko, T. Poibeau, S. Aubin, and
J. Derivi`ere. 2007. A robust linguistic platform for
efficient and domain specific web content analysis.
In RIAO 2007, Pittsburgh, USA.
Zellig Harris. 1954. Distributional structure. Word,
10(23):146–162.
Marti A. Hearst. 1992. Automatic acquisition of hy-
ponyms from large text corpora. In International
Conference on Computational Linguistics, pages
539–545, Nantes, France.
Christian Jacquemin. 1996. A symbolic and surgi-
cal acquisition of terms through variation. In CoRR,
pages 425–438.
K. Lund and C. Burgess. 1996. Producing
high-dimensional semantic spaces from lexical co-
occurrence. Behavior Research Methods, Instru-
mentation, and Computers, 28:203–208.
Jeff Mitchell and Mirella Lapata. 2010. Composition
in distributional models of semantics. Cognitive Sci-
ence, 34(8):1388–1429.
Sebastian Pad´o and Mirella Lapata. 2007.
Dependency-based construction of semantic
space models. Comput. Linguist., 33(2):161–199.
Yves Peirsman, Heylen Kris, and Geeraerts Dirk.
2008. Size matters. tight and loose context def-
initions in english word space models. In ESS-
LLI Workshop on Distributional Lexical Semantics,
Hamburg, Germany.
Amandine P´erinet and Thierry Hamon. 2013. Hybrid
acquisition of semantic relations based on context
normalization in distributional analysis. In Proceed-
ings of TIA 2013, pages 113–120, Paris, France.
Reinhard Rapp. 2003. Word sense discovery based on
sense descriptor dissimilarity. In MT Summit’2003,
pages 315–322.
Magnus Sahlgren. 2006. The Word-Space Model:
Using Distributional Analysis to Represent Syntag-
matic and Paradigmatic Relations between Words
in High-Dimensional Vector Spaces. Ph.D. thesis,
Stockholm University, Stockholm, Sweden.
H. Schmid. 1994. Probabilistic part-of-speech tagging
using decision trees. In New Methods in Language
Processing, pages 44–49, Manchester, UK.
Hinrich Sch¨utze. 1998. Automatic word sense dis-
crimination. Comput. Linguist., 24(1):97–123.
Weiyi Sun, Anna Rumshisky, and ¨Ozlem Uzuner.
2013. Evaluating temporal relations in clinical text:
2012 i2b2 challenge. JAMIA, 20(5):806–813.
George Tsatsaronis and Vicky Panagiotopoulou. 2009.
A generalized vector space model for text retrieval
based on semantic relatedness. In EACL 2009,
pages 70–78, Stroudsburg, PA, USA.
Peter D. Turney and Patrick Pantel. 2010. From fre-
quency to meaning: Vector space models of seman-
tics. JAIR, 37:141–188.
Lonneke van der Plas. 2008. Automatic lexico-
semantic acquisition for question answering. Th`ese
de doctorat, University of Groningen, Groningen.
Julie Weeds and David Weir. 2005. Co-occurrence
retrieval: A flexible framework for lexical distribu-
tional similarity. Comput. Linguist., 31(4):439–475.
Yorick A. Wilks, Dan, James E. Mcdonald, Tony
Plate, and Brian M. Slator. 1990. Providing ma-
chine tractable dictionary tools. Journal of Machine
Translation, 2.
</reference>
<page confidence="0.990767">
94
</page>
<reference confidence="0.998577923076923">
Deniz Yuret. 2012. Fastsubs: An efficient and exact
procedure for finding the most likely lexical substi-
tutes based on an n-gram language model. IEEE
Signal Process. Lett., 19(11):725–728.
Wenbin Zheng, Yuntao Qian, and Hong Tang. 2011.
Dimensionality reduction with category information
fusion and non-negative matrix factorization for text
categorization. In Hepu Deng, Duoqian Miao, Jing-
sheng Lei, and Fu Lee Wang, editors, AICI, volume
7004 of LNCS, pages 505–512.
Maayan Zhitomirsky-Geffet and Ido Dagan. 2009.
Bootstrapping distributional feature vector quality.
Comput. Linguist., 35(3):435–461.
</reference>
<page confidence="0.99908">
95
</page>
</variant>
</algorithm>
<algorithm name="ParsHed" version="110505">
<variant no="0" confidence="0.265549">
<title confidence="0.947984">Reducing VSM data sparseness by generalizing contexts: application to health text mining</title>
<author confidence="0.581825">Amandine P´erinet Thierry Hamon</author>
<note confidence="0.75509975">INSERM, U1142, LIMICS, Paris, France LIMSI-CNRS, Orsay, France Sorbonne Universit´es, UPMC Univ Paris 06, Paris, France Universit´e Paris 13, Sorbonne Paris Cit´e Universit´e Paris 13, Sorbonne Paris Cit´e, Villetaneuse, France Villetaneuse, France amandine.perinet@edu.univ-paris13.fr hamon@limsi.fr</note>
<abstract confidence="0.99795025">Vector Space Models are limited with low frequency words due to few available contexts and data sparseness. To tackle this problem, we generalize contexts by integrating semantic relations acquired with linguistic approaches. We use three methods that acquire hypernymy relations on a EHR corpus. Context Generalization obtains the best results when performed with hypernyms, the quality of the relations being more important than the quantity.</abstract>
</variant>
</algorithm>
<algorithm name="ParsCit" version="110505">
<citationList>
<citation valid="true">
<authors>
<author>Sophie Aubin</author>
<author>Thierry Hamon</author>
</authors>
<title>Improving term extraction with terminological resources.</title>
<date>2006</date>
<booktitle>In Advances in Natural Language Processing, number 4139 in LNAI,</booktitle>
<pages>380--387</pages>
<publisher>Springer.</publisher>
<contexts>
<context position="6097" citStr="Aubin and Hamon, 2006" startWordPosition="944" endWordPosition="947">yzing information and integrating extra semantic knowledge. 3 VSM and context generalization The contexts in which occurs a target word have associated frequencies which may be used to form probability estimates. The goal of our method is to influence the distributional context frequencies by generalizing contexts. Step 1: target and context definition During this step, we define targets and contexts, with different constraints for their extraction. To adapt our method to specialized texts, we identify terms (specific terminological entities that denote an event) with a term extractor (YATEA (Aubin and Hamon, 2006)). Target words are both nouns and terms (T). Their distributional contexts correspond to a graphical window of n number of words around the targets (Wilks et al., 1990; Sch¨utze, 1998; Lund and Burgess, 1996). We consider two different window sizes defined in section 4. Linguistic approaches During the generalization process, we use three existing linguistic approaches: two that acquire hypernymy relations and one to get morphosyntactic variants. Lexicosyntactic Patterns (LSP) acquire hypernymy relations. We use the patterns defined by (Hearst, 1992). Lexical Inclusion (LI) acquires hypernymy</context>
<context position="9545" citStr="Aubin and Hamon, 2006" startWordPosition="1493" endWordPosition="1496">is section, we present the material we use for the experiments and evaluation, and the distributional parameter values of the VSM automatically determined from the data. We then describe the generalization sets we experiment and the evaluation measures we used for evaluation. 4.1 Corpus We use the collection of anonymous clinical English texts provided by the 2012 i2b2/VA challenge (Sun et al., 2013). The corpus is pre-processed within the Ogmios platform (Hamon et al., 2007). We perform morphosyntactic tagging and lemmatization with Tree Tagger (Schmid, 1994), and term extraction with YATEA (Aubin and Hamon, 2006). 4.2 Distributional parameters We consider two window sizes: a large window of 21 words (± 10 words, centered on the target, henceforth W21) and a narrow one of 5 words (± 2 words, centered on the target, W5). The window size influences on the type, the volume and the quality of the acquired relations. Generally, the smaller windows allow to acquire more relevant contexts for a target, but increase the data sparseness problem (Rapp, 2003). They give better results for classical types of relations (eg. synonymy), whereas larger windows are more appropriate for domain relations (eg. collocation</context>
</contexts>
<marker>Aubin, Hamon, 2006</marker>
<rawString>Sophie Aubin and Thierry Hamon. 2006. Improving term extraction with terminological resources. In Advances in Natural Language Processing, number 4139 in LNAI, pages 380–387. Springer.</rawString>
</citation>
<citation valid="true">
<authors>
<author>Marco Baroni</author>
<author>Silvia Bernardini</author>
<author>Adriano Ferraresi</author>
<author>Eros Zanchetta</author>
</authors>
<title>The wacky wide web: a collection of very large linguistically processed web-crawled corpora. Language Resources and Evaluation,</title>
<date>2009</date>
<pages>43--3</pages>
<contexts>
<context position="1601" citStr="Baroni et al., 2009" startWordPosition="239" endWordPosition="242">get words from the contexts shared by those two words. This hypothesis is applied with geometric methods, such as the Vector Space Model (VSM) (Turney and Pantel, 2010). The advantage of the VSM is that the similarity of word meaning can be easily quantified by measuring their distance in the vector space, or the cosine of the angle between them (Mitchell and Lapata, 2010). On the other hand, a major inconvenience is data sparseness within the matrix that represents the vector space (Turney and Pantel, 2010). The data sparseness problem is the consequence of the word distribution in a corpus (Baroni et al., 2009): in any corpus, most of the words have a very low frequency and appear only a few times. Thus, those words have a limited set of contexts and similarity is difficult to catch. Thus, methods based on DA perform better when more information is available (Weeds and Weir, 2005; van der Plas, 2008) and are efficient with large corpora of general language. But with specialized texts, as EHR texts that are usually of smaller size, reducing data sparseness is a major issue and methods need to be adapted. Semantic grouping of contexts should decrease their diversity, and thus increase the frequency of</context>
</contexts>
<marker>Baroni, Bernardini, Ferraresi, Zanchetta, 2009</marker>
<rawString>Marco Baroni, Silvia Bernardini, Adriano Ferraresi, and Eros Zanchetta. 2009. The wacky wide web: a collection of very large linguistically processed web-crawled corpora. Language Resources and Evaluation, 43(3):209–226.</rawString>
</citation>
<citation valid="true">
<authors>
<author>Osman Baskaya</author>
<author>Enis Sert</author>
<author>Volkan Cirik</author>
<author>Deniz Yuret</author>
</authors>
<title>Ai-ku: Using substitute vectors and co-occurrence modeling for word sense induction and disambiguation.</title>
<date>2013</date>
<booktitle>In Proceedings of SemEval -2013,</booktitle>
<pages>300--306</pages>
<publisher>Association</publisher>
<institution>for Computational Linguistics.</institution>
<location>Atlanta, Georgia, USA.</location>
<contexts>
<context position="3473" citStr="Baskaya et al., 2013" startWordPosition="544" endWordPosition="547">lly describe the different experiments we led. The results obtained on the EHR corpus are then evaluated in terms of precision and MAP, and analyzed. 2 Related work Our approach relates with works that influence distributional contexts to improve the performance of VSMs. Some of them intend to change the way to consider contexts; Broda et al. (2009) do not use the raw context frequency in DA, but they first rank contexts according to their frequency, and take the rank into account. Other models use statistical language models to determine the most likely substitutes to represent the contexts (Baskaya et al., 2013). They assign probabilities to arbitrary sequences of words that are then used to create word pairs to feed a co-occurrence model, before performing a clustered algorithm (Yuret, 2012). The limit of such methods is that their performance is proportional to vocabulary size and requires the availability of training data. Influence on contexts may also be performed by embedding additional semantic information. The semantic relations may be issued from an existing resource or automatically computed. With a method based on bootstrapping, ZhitomirskyGeffet and Dagan (2009) modify the weights of 90 P</context>
</contexts>
<marker>Baskaya, Sert, Cirik, Yuret, 2013</marker>
<rawString>Osman Baskaya, Enis Sert, Volkan Cirik, and Deniz Yuret. 2013. Ai-ku: Using substitute vectors and co-occurrence modeling for word sense induction and disambiguation. In Proceedings of SemEval -2013, pages 300–306, Atlanta, Georgia, USA. Association for Computational Linguistics.</rawString>
</citation>
<citation valid="true">
<authors>
<author>Olivier Bodenreider</author>
<author>Anita Burgun</author>
<author>Thomas Rindflesch</author>
</authors>
<title>Lexically-suggested hyponymic relations among medical terms and their representation in the umls. In TIA</title>
<date>2001</date>
<pages>11--21</pages>
<contexts>
<context position="6965" citStr="Bodenreider et al., 2001" startWordPosition="1078" endWordPosition="1082"> sizes defined in section 4. Linguistic approaches During the generalization process, we use three existing linguistic approaches: two that acquire hypernymy relations and one to get morphosyntactic variants. Lexicosyntactic Patterns (LSP) acquire hypernymy relations. We use the patterns defined by (Hearst, 1992). Lexical Inclusion (LI) acquires hypernymy relations and uses the syntactic analysis of the terms. Based on the hypothesis that if a term is lexically included in another, generally there is a hypernymy relation between the two terms (kidney transplant - cadaveric kidney transplant) (Bodenreider et al., 2001). Terminological Variation (TV) acquires both hypernyms and synonyms. TV uses rules that define a morpho-syntactic transformation, mainly the insertion (blood transfusion - blood cell transfusion (Jacquemin, 1996). Step 2: context generalization Once targets and contexts are defined, we generalize contexts with the relations acquired by the three linguistic approaches we mentioned. To integrate the relations in contexts, we replace words in context by their hypernym or morphosyntactic variant. We define two rules: (1) if the context matches with one hypernym, context is replaced by this hypern</context>
</contexts>
<marker>Bodenreider, Burgun, Rindflesch, 2001</marker>
<rawString>Olivier Bodenreider, Anita Burgun, and Thomas Rindflesch. 2001. Lexically-suggested hyponymic relations among medical terms and their representation in the umls. In TIA 2001, pages 11–21.</rawString>
</citation>
<citation valid="true">
<authors>
<author>Bartosz Broda</author>
<author>Maciej Piasecki</author>
<author>Stan Szpakowicz</author>
</authors>
<title>Rank-based transformation in measuring semantic relatedness.</title>
<date>2009</date>
<booktitle>In Yong Gao and Nathalie Japkowicz, editors, Canadian Conference on AI,</booktitle>
<volume>5549</volume>
<pages>187--190</pages>
<publisher>Springer.</publisher>
<contexts>
<context position="3203" citStr="Broda et al. (2009)" startWordPosition="500" endWordPosition="503">hat switches words in DA contexts for their hierarchical parent or morphosyntactic variant that have been computed on the corpus with linguistic approaches before applying the VSM method. In the following, we first present the related work, then our method and we finally describe the different experiments we led. The results obtained on the EHR corpus are then evaluated in terms of precision and MAP, and analyzed. 2 Related work Our approach relates with works that influence distributional contexts to improve the performance of VSMs. Some of them intend to change the way to consider contexts; Broda et al. (2009) do not use the raw context frequency in DA, but they first rank contexts according to their frequency, and take the rank into account. Other models use statistical language models to determine the most likely substitutes to represent the contexts (Baskaya et al., 2013). They assign probabilities to arbitrary sequences of words that are then used to create word pairs to feed a co-occurrence model, before performing a clustered algorithm (Yuret, 2012). The limit of such methods is that their performance is proportional to vocabulary size and requires the availability of training data. Influence</context>
</contexts>
<marker>Broda, Piasecki, Szpakowicz, 2009</marker>
<rawString>Bartosz Broda, Maciej Piasecki, and Stan Szpakowicz. 2009. Rank-based transformation in measuring semantic relatedness. In Yong Gao and Nathalie Japkowicz, editors, Canadian Conference on AI, volume 5549, pages 187–190. Springer.</rawString>
</citation>
<citation valid="true">
<authors>
<author>Chris Buckley</author>
<author>Ellen Voorhees</author>
</authors>
<title>Retrieval system evaluation.</title>
<date>2005</date>
<booktitle>Experiment and Evaluation in Information Retrieval, chapter 3.</booktitle>
<editor>In Ellen Voorhees and Donna Harman, editors, TREC:</editor>
<publisher>MIT Press.</publisher>
<contexts>
<context position="11897" citStr="Buckley and Voorhees, 2005" startWordPosition="1862" endWordPosition="1865">rform with two methods, we experiment the generalization of contexts by relations acquired with the three different linguistic approaches (e.g. LSP then LI then TV). We experiment all the possible combinations. With both the single and multiple generalization, we aim at evaluating the contribution of each method but also the impact of the order of the methods. 4.4 Evaluation In order to evaluate the quality of the acquired relations, we compare our relations to the 53,203 UMLS relations between terms occurring in our EHR corpus. We perform the evaluation with the Mean Average Precision (MAP) (Buckley and Voorhees, 2005) and the macro-precision computed for each target word: semantic neighbors found in the resource by the total semantic neighbors acquired by our method. We consider three sets of neighbors: precision after examining 1 (P@1), 5 (P@5) and 10 (P@10) neighbors. 5 Results and discussion Best results are obtained with a large window of 21 words, with a precision P@1 of 0.243 against 0.032 for a 5 word window, both for VSMonly, with a threshold of 0.001. Thus, a high threshold on the similarity score is not always relevant. We observe on this corpus that the generalization with the several linguistic</context>
</contexts>
<marker>Buckley, Voorhees, 2005</marker>
<rawString>Chris Buckley and Ellen Voorhees. 2005. Retrieval system evaluation. In Ellen Voorhees and Donna Harman, editors, TREC: Experiment and Evaluation in Information Retrieval, chapter 3. MIT Press.</rawString>
</citation>
<citation valid="true">
<authors>
<author>Olivier Ferret</author>
</authors>
<title>S´election non supervis´ee de relations s´emantiques pour am´eliorer un th´esaurus distributionnel.</title>
<date>2013</date>
<booktitle>In TALN 2013,</booktitle>
<pages>48--61</pages>
<location>Les Sables d’Olonne, France.</location>
<contexts>
<context position="4426" citStr="Ferret (2013)" startWordPosition="691" endWordPosition="692">ts may also be performed by embedding additional semantic information. The semantic relations may be issued from an existing resource or automatically computed. With a method based on bootstrapping, ZhitomirskyGeffet and Dagan (2009) modify the weights of 90 Proceedings of the 5th International Workshop on Health Text Mining and Information Analysis (Louhi) @ EACL 2014, pages 90–95, Gothenburg, Sweden, April 26-30 2014. c�2014 Association for Computational Linguistics the elements in contexts relying on the semantic neighbors found with a distributional similarity measure. Based on this work, Ferret (2013) uses a set of examples selected from an original distributional thesaurus to train a supervised classifier. This classifier is then applied for reranking the neighbors of the thesaurus selection. Within Vector Space Model, Tsatsaronis and Panagiotopoulou (2009) use a word thesaurus to interpret the orthogonality of terms and measure semantic relatedness. With the same purpose of solving the problem of data sparseness, other methods are based on dimensionality reduction, such as Latent Semantic Analysis (LSA) in (Pad´o and Lapata, 2007) or Non-negative Matrix Factorization (NMF) (Zheng et al.,</context>
</contexts>
<marker>Ferret, 2013</marker>
<rawString>Olivier Ferret. 2013. S´election non supervis´ee de relations s´emantiques pour am´eliorer un th´esaurus distributionnel. In TALN 2013, pages 48–61, Les Sables d’Olonne, France.</rawString>
</citation>
<citation valid="true">
<authors>
<author>J R Firth</author>
</authors>
<title>A synopsis of linguistic theory 1930-1955. Studies in linguistic analysis,</title>
<date>1957</date>
<pages>1--32</pages>
<contexts>
<context position="947" citStr="Firth, 1957" startWordPosition="128" endWordPosition="129">neuse, France Villetaneuse, France amandine.perinet@edu.univ-paris13.fr hamon@limsi.fr Abstract Vector Space Models are limited with low frequency words due to few available contexts and data sparseness. To tackle this problem, we generalize contexts by integrating semantic relations acquired with linguistic approaches. We use three methods that acquire hypernymy relations on a EHR corpus. Context Generalization obtains the best results when performed with hypernyms, the quality of the relations being more important than the quantity. 1 Introduction Distributional Analysis (DA) (Harris, 1954; Firth, 1957) computes a similarity between target words from the contexts shared by those two words. This hypothesis is applied with geometric methods, such as the Vector Space Model (VSM) (Turney and Pantel, 2010). The advantage of the VSM is that the similarity of word meaning can be easily quantified by measuring their distance in the vector space, or the cosine of the angle between them (Mitchell and Lapata, 2010). On the other hand, a major inconvenience is data sparseness within the matrix that represents the vector space (Turney and Pantel, 2010). The data sparseness problem is the consequence of t</context>
</contexts>
<marker>Firth, 1957</marker>
<rawString>J.R. Firth. 1957. A synopsis of linguistic theory 1930-1955. Studies in linguistic analysis, pages 1–32.</rawString>
</citation>
<citation valid="true">
<authors>
<author>Gregory Grefenstette</author>
</authors>
<title>Corpus-derived first, second and third-order word affinities.</title>
<date>1994</date>
<booktitle>In Sixth Euralex International Congress,</booktitle>
<pages>279--290</pages>
<contexts>
<context position="8125" citStr="Grefenstette, 1994" startWordPosition="1255" endWordPosition="1256">tches with one hypernym, context is replaced by this hypernym. (2) if the context matches with several hypernyms or variants, we take the hypernym or variant frequency into account, and choose the most frequent hypernym/variant. The generalization step is individually or sequentially performed when several relation sets are available. Step 3: computation of semantic similarity After the generalization step, similarity between target words is computed. As we previously decrease diversity in contexts, we choose a measure that favors words appearing in similar contexts. We use the Jaccard Index (Grefenstette, 1994) which normalizes the number of contexts shared by two words by the total number of contexts of those two words. Parameter: thresholds The huge number of relations we obtain after computing similarity between targets leads us to remove the supposed wrong relations with three thresholds: (i) number of shared lemmatized contexts (2 for a large window, 1 for a small window) ; (ii) number of the lemmatized contexts (2 for a large window, 1 for a small window) ; (iii) number of the lemmatized targets (3 for both window sizes). For each parameter, the threshold is automatically computed, according t</context>
</contexts>
<marker>Grefenstette, 1994</marker>
<rawString>Gregory Grefenstette. 1994. Corpus-derived first, second and third-order word affinities. In Sixth Euralex International Congress, pages 279–290.</rawString>
</citation>
<citation valid="true">
<authors>
<author>T Hamon</author>
<author>A Nazarenko</author>
<author>T Poibeau</author>
<author>S Aubin</author>
<author>J Derivi`ere</author>
</authors>
<title>A robust linguistic platform for efficient and domain specific web content analysis.</title>
<date>2007</date>
<booktitle>In RIAO 2007,</booktitle>
<location>Pittsburgh, USA.</location>
<marker>Hamon, Nazarenko, Poibeau, Aubin, Derivi`ere, 2007</marker>
<rawString>T. Hamon, A. Nazarenko, T. Poibeau, S. Aubin, and J. Derivi`ere. 2007. A robust linguistic platform for efficient and domain specific web content analysis. In RIAO 2007, Pittsburgh, USA.</rawString>
</citation>
<citation valid="true">
<authors>
<author>Zellig Harris</author>
</authors>
<date>1954</date>
<journal>Distributional structure. Word,</journal>
<volume>10</volume>
<issue>23</issue>
<contexts>
<context position="933" citStr="Harris, 1954" startWordPosition="126" endWordPosition="127">Cit´e, Villetaneuse, France Villetaneuse, France amandine.perinet@edu.univ-paris13.fr hamon@limsi.fr Abstract Vector Space Models are limited with low frequency words due to few available contexts and data sparseness. To tackle this problem, we generalize contexts by integrating semantic relations acquired with linguistic approaches. We use three methods that acquire hypernymy relations on a EHR corpus. Context Generalization obtains the best results when performed with hypernyms, the quality of the relations being more important than the quantity. 1 Introduction Distributional Analysis (DA) (Harris, 1954; Firth, 1957) computes a similarity between target words from the contexts shared by those two words. This hypothesis is applied with geometric methods, such as the Vector Space Model (VSM) (Turney and Pantel, 2010). The advantage of the VSM is that the similarity of word meaning can be easily quantified by measuring their distance in the vector space, or the cosine of the angle between them (Mitchell and Lapata, 2010). On the other hand, a major inconvenience is data sparseness within the matrix that represents the vector space (Turney and Pantel, 2010). The data sparseness problem is the co</context>
</contexts>
<marker>Harris, 1954</marker>
<rawString>Zellig Harris. 1954. Distributional structure. Word, 10(23):146–162.</rawString>
</citation>
<citation valid="true">
<authors>
<author>Marti A Hearst</author>
</authors>
<title>Automatic acquisition of hyponyms from large text corpora.</title>
<date>1992</date>
<booktitle>In International Conference on Computational Linguistics,</booktitle>
<pages>539--545</pages>
<location>Nantes, France.</location>
<contexts>
<context position="6654" citStr="Hearst, 1992" startWordPosition="1032" endWordPosition="1033">nt) with a term extractor (YATEA (Aubin and Hamon, 2006)). Target words are both nouns and terms (T). Their distributional contexts correspond to a graphical window of n number of words around the targets (Wilks et al., 1990; Sch¨utze, 1998; Lund and Burgess, 1996). We consider two different window sizes defined in section 4. Linguistic approaches During the generalization process, we use three existing linguistic approaches: two that acquire hypernymy relations and one to get morphosyntactic variants. Lexicosyntactic Patterns (LSP) acquire hypernymy relations. We use the patterns defined by (Hearst, 1992). Lexical Inclusion (LI) acquires hypernymy relations and uses the syntactic analysis of the terms. Based on the hypothesis that if a term is lexically included in another, generally there is a hypernymy relation between the two terms (kidney transplant - cadaveric kidney transplant) (Bodenreider et al., 2001). Terminological Variation (TV) acquires both hypernyms and synonyms. TV uses rules that define a morpho-syntactic transformation, mainly the insertion (blood transfusion - blood cell transfusion (Jacquemin, 1996). Step 2: context generalization Once targets and contexts are defined, we g</context>
</contexts>
<marker>Hearst, 1992</marker>
<rawString>Marti A. Hearst. 1992. Automatic acquisition of hyponyms from large text corpora. In International Conference on Computational Linguistics, pages 539–545, Nantes, France.</rawString>
</citation>
<citation valid="true">
<authors>
<author>Christian Jacquemin</author>
</authors>
<title>A symbolic and surgical acquisition of terms through variation. In CoRR,</title>
<date>1996</date>
<pages>425--438</pages>
<contexts>
<context position="7178" citStr="Jacquemin, 1996" startWordPosition="1109" endWordPosition="1110">ic Patterns (LSP) acquire hypernymy relations. We use the patterns defined by (Hearst, 1992). Lexical Inclusion (LI) acquires hypernymy relations and uses the syntactic analysis of the terms. Based on the hypothesis that if a term is lexically included in another, generally there is a hypernymy relation between the two terms (kidney transplant - cadaveric kidney transplant) (Bodenreider et al., 2001). Terminological Variation (TV) acquires both hypernyms and synonyms. TV uses rules that define a morpho-syntactic transformation, mainly the insertion (blood transfusion - blood cell transfusion (Jacquemin, 1996). Step 2: context generalization Once targets and contexts are defined, we generalize contexts with the relations acquired by the three linguistic approaches we mentioned. To integrate the relations in contexts, we replace words in context by their hypernym or morphosyntactic variant. We define two rules: (1) if the context matches with one hypernym, context is replaced by this hypernym. (2) if the context matches with several hypernyms or variants, we take the hypernym or variant frequency into account, and choose the most frequent hypernym/variant. The generalization step is individually or </context>
</contexts>
<marker>Jacquemin, 1996</marker>
<rawString>Christian Jacquemin. 1996. A symbolic and surgical acquisition of terms through variation. In CoRR, pages 425–438.</rawString>
</citation>
<citation valid="true">
<authors>
<author>K Lund</author>
<author>C Burgess</author>
</authors>
<title>Producing high-dimensional semantic spaces from lexical cooccurrence.</title>
<date>1996</date>
<journal>Behavior Research Methods, Instrumentation, and Computers,</journal>
<pages>28--203</pages>
<contexts>
<context position="6306" citStr="Lund and Burgess, 1996" startWordPosition="979" endWordPosition="982">tes. The goal of our method is to influence the distributional context frequencies by generalizing contexts. Step 1: target and context definition During this step, we define targets and contexts, with different constraints for their extraction. To adapt our method to specialized texts, we identify terms (specific terminological entities that denote an event) with a term extractor (YATEA (Aubin and Hamon, 2006)). Target words are both nouns and terms (T). Their distributional contexts correspond to a graphical window of n number of words around the targets (Wilks et al., 1990; Sch¨utze, 1998; Lund and Burgess, 1996). We consider two different window sizes defined in section 4. Linguistic approaches During the generalization process, we use three existing linguistic approaches: two that acquire hypernymy relations and one to get morphosyntactic variants. Lexicosyntactic Patterns (LSP) acquire hypernymy relations. We use the patterns defined by (Hearst, 1992). Lexical Inclusion (LI) acquires hypernymy relations and uses the syntactic analysis of the terms. Based on the hypothesis that if a term is lexically included in another, generally there is a hypernymy relation between the two terms (kidney transplan</context>
</contexts>
<marker>Lund, Burgess, 1996</marker>
<rawString>K. Lund and C. Burgess. 1996. Producing high-dimensional semantic spaces from lexical cooccurrence. Behavior Research Methods, Instrumentation, and Computers, 28:203–208.</rawString>
</citation>
<citation valid="true">
<authors>
<author>Jeff Mitchell</author>
<author>Mirella Lapata</author>
</authors>
<title>Composition in distributional models of semantics.</title>
<date>2010</date>
<journal>Cognitive Science,</journal>
<volume>34</volume>
<issue>8</issue>
<contexts>
<context position="1356" citStr="Mitchell and Lapata, 2010" startWordPosition="197" endWordPosition="200">ntext Generalization obtains the best results when performed with hypernyms, the quality of the relations being more important than the quantity. 1 Introduction Distributional Analysis (DA) (Harris, 1954; Firth, 1957) computes a similarity between target words from the contexts shared by those two words. This hypothesis is applied with geometric methods, such as the Vector Space Model (VSM) (Turney and Pantel, 2010). The advantage of the VSM is that the similarity of word meaning can be easily quantified by measuring their distance in the vector space, or the cosine of the angle between them (Mitchell and Lapata, 2010). On the other hand, a major inconvenience is data sparseness within the matrix that represents the vector space (Turney and Pantel, 2010). The data sparseness problem is the consequence of the word distribution in a corpus (Baroni et al., 2009): in any corpus, most of the words have a very low frequency and appear only a few times. Thus, those words have a limited set of contexts and similarity is difficult to catch. Thus, methods based on DA perform better when more information is available (Weeds and Weir, 2005; van der Plas, 2008) and are efficient with large corpora of general language. B</context>
<context position="5204" citStr="Mitchell and Lapata, 2010" startWordPosition="805" endWordPosition="808">ng the neighbors of the thesaurus selection. Within Vector Space Model, Tsatsaronis and Panagiotopoulou (2009) use a word thesaurus to interpret the orthogonality of terms and measure semantic relatedness. With the same purpose of solving the problem of data sparseness, other methods are based on dimensionality reduction, such as Latent Semantic Analysis (LSA) in (Pad´o and Lapata, 2007) or Non-negative Matrix Factorization (NMF) (Zheng et al., 2011). Matrix decomposition techniques are usually applied to reduce the dimensionality of the original matrix, thereby rendering it more informative (Mitchell and Lapata, 2010). Our approach differs from the aforementioned ones in that we add semantic information in contexts to reduce the number of contexts and to increase their frequency. Contrary to these latter approaches, we do not reduce the contexts by removing information but by generalyzing information and integrating extra semantic knowledge. 3 VSM and context generalization The contexts in which occurs a target word have associated frequencies which may be used to form probability estimates. The goal of our method is to influence the distributional context frequencies by generalizing contexts. Step 1: targ</context>
</contexts>
<marker>Mitchell, Lapata, 2010</marker>
<rawString>Jeff Mitchell and Mirella Lapata. 2010. Composition in distributional models of semantics. Cognitive Science, 34(8):1388–1429.</rawString>
</citation>
<citation valid="true">
<authors>
<author>Sebastian Pad´o</author>
<author>Mirella Lapata</author>
</authors>
<title>Dependency-based construction of semantic space models.</title>
<date>2007</date>
<journal>Comput. Linguist.,</journal>
<volume>33</volume>
<issue>2</issue>
<marker>Pad´o, Lapata, 2007</marker>
<rawString>Sebastian Pad´o and Mirella Lapata. 2007. Dependency-based construction of semantic space models. Comput. Linguist., 33(2):161–199.</rawString>
</citation>
<citation valid="true">
<authors>
<author>Yves Peirsman</author>
<author>Heylen Kris</author>
<author>Geeraerts Dirk</author>
</authors>
<title>Size matters. tight and loose context definitions in english word space models.</title>
<date>2008</date>
<booktitle>In ESSLLI Workshop on Distributional Lexical Semantics,</booktitle>
<location>Hamburg, Germany.</location>
<contexts>
<context position="10186" citStr="Peirsman et al., 2008" startWordPosition="1597" endWordPosition="1600">l parameters We consider two window sizes: a large window of 21 words (± 10 words, centered on the target, henceforth W21) and a narrow one of 5 words (± 2 words, centered on the target, W5). The window size influences on the type, the volume and the quality of the acquired relations. Generally, the smaller windows allow to acquire more relevant contexts for a target, but increase the data sparseness problem (Rapp, 2003). They give better results for classical types of relations (eg. synonymy), whereas larger windows are more appropriate for domain relations (eg. collocations)(Sahlgren, 2006; Peirsman et al., 2008). 4.3 Generalizing distributional contexts We define several sets of context generalization. We experiment in step 2 different ways of generalizing contexts. We use as a baseline the VSM without any generalization in the contexts (VSMonly), and compare the generalization sets to it. Regarding context generalization, we first exploit the relations acquired from only one linguistic approach. We apply the method described at the section 3 (step 2) by separately using the three different sets of relations automatically acquired. Distributional contexts are replaced by their hypernym acquired with </context>
</contexts>
<marker>Peirsman, Kris, Dirk, 2008</marker>
<rawString>Yves Peirsman, Heylen Kris, and Geeraerts Dirk. 2008. Size matters. tight and loose context definitions in english word space models. In ESSLLI Workshop on Distributional Lexical Semantics, Hamburg, Germany.</rawString>
</citation>
<citation valid="true">
<authors>
<author>Amandine P´erinet</author>
<author>Thierry Hamon</author>
</authors>
<title>Hybrid acquisition of semantic relations based on context normalization in distributional analysis.</title>
<date>2013</date>
<booktitle>In Proceedings of TIA 2013,</booktitle>
<pages>113--120</pages>
<location>Paris, France.</location>
<marker>P´erinet, Hamon, 2013</marker>
<rawString>Amandine P´erinet and Thierry Hamon. 2013. Hybrid acquisition of semantic relations based on context normalization in distributional analysis. In Proceedings of TIA 2013, pages 113–120, Paris, France.</rawString>
</citation>
<citation valid="true">
<authors>
<author>Reinhard Rapp</author>
</authors>
<title>Word sense discovery based on sense descriptor dissimilarity.</title>
<date>2003</date>
<booktitle>In MT Summit’2003,</booktitle>
<pages>315--322</pages>
<contexts>
<context position="9988" citStr="Rapp, 2003" startWordPosition="1571" endWordPosition="1572">tform (Hamon et al., 2007). We perform morphosyntactic tagging and lemmatization with Tree Tagger (Schmid, 1994), and term extraction with YATEA (Aubin and Hamon, 2006). 4.2 Distributional parameters We consider two window sizes: a large window of 21 words (± 10 words, centered on the target, henceforth W21) and a narrow one of 5 words (± 2 words, centered on the target, W5). The window size influences on the type, the volume and the quality of the acquired relations. Generally, the smaller windows allow to acquire more relevant contexts for a target, but increase the data sparseness problem (Rapp, 2003). They give better results for classical types of relations (eg. synonymy), whereas larger windows are more appropriate for domain relations (eg. collocations)(Sahlgren, 2006; Peirsman et al., 2008). 4.3 Generalizing distributional contexts We define several sets of context generalization. We experiment in step 2 different ways of generalizing contexts. We use as a baseline the VSM without any generalization in the contexts (VSMonly), and compare the generalization sets to it. Regarding context generalization, we first exploit the relations acquired from only one linguistic approach. We apply </context>
</contexts>
<marker>Rapp, 2003</marker>
<rawString>Reinhard Rapp. 2003. Word sense discovery based on sense descriptor dissimilarity. In MT Summit’2003, pages 315–322.</rawString>
</citation>
<citation valid="true">
<authors>
<author>Magnus Sahlgren</author>
</authors>
<title>The Word-Space Model: Using Distributional Analysis to Represent Syntagmatic and Paradigmatic Relations between Words in High-Dimensional Vector Spaces.</title>
<date>2006</date>
<tech>Ph.D. thesis,</tech>
<institution>Stockholm University,</institution>
<location>Stockholm,</location>
<contexts>
<context position="10162" citStr="Sahlgren, 2006" startWordPosition="1594" endWordPosition="1596">.2 Distributional parameters We consider two window sizes: a large window of 21 words (± 10 words, centered on the target, henceforth W21) and a narrow one of 5 words (± 2 words, centered on the target, W5). The window size influences on the type, the volume and the quality of the acquired relations. Generally, the smaller windows allow to acquire more relevant contexts for a target, but increase the data sparseness problem (Rapp, 2003). They give better results for classical types of relations (eg. synonymy), whereas larger windows are more appropriate for domain relations (eg. collocations)(Sahlgren, 2006; Peirsman et al., 2008). 4.3 Generalizing distributional contexts We define several sets of context generalization. We experiment in step 2 different ways of generalizing contexts. We use as a baseline the VSM without any generalization in the contexts (VSMonly), and compare the generalization sets to it. Regarding context generalization, we first exploit the relations acquired from only one linguistic approach. We apply the method described at the section 3 (step 2) by separately using the three different sets of relations automatically acquired. Distributional contexts are replaced by their</context>
</contexts>
<marker>Sahlgren, 2006</marker>
<rawString>Magnus Sahlgren. 2006. The Word-Space Model: Using Distributional Analysis to Represent Syntagmatic and Paradigmatic Relations between Words in High-Dimensional Vector Spaces. Ph.D. thesis, Stockholm University, Stockholm, Sweden.</rawString>
</citation>
<citation valid="true">
<authors>
<author>H Schmid</author>
</authors>
<title>Probabilistic part-of-speech tagging using decision trees.</title>
<date>1994</date>
<booktitle>In New Methods in Language Processing,</booktitle>
<pages>44--49</pages>
<location>Manchester, UK.</location>
<contexts>
<context position="9489" citStr="Schmid, 1994" startWordPosition="1486" endWordPosition="1487"> 0.001 and sim &gt; 0.0005. 91 4 Experiments In this section, we present the material we use for the experiments and evaluation, and the distributional parameter values of the VSM automatically determined from the data. We then describe the generalization sets we experiment and the evaluation measures we used for evaluation. 4.1 Corpus We use the collection of anonymous clinical English texts provided by the 2012 i2b2/VA challenge (Sun et al., 2013). The corpus is pre-processed within the Ogmios platform (Hamon et al., 2007). We perform morphosyntactic tagging and lemmatization with Tree Tagger (Schmid, 1994), and term extraction with YATEA (Aubin and Hamon, 2006). 4.2 Distributional parameters We consider two window sizes: a large window of 21 words (± 10 words, centered on the target, henceforth W21) and a narrow one of 5 words (± 2 words, centered on the target, W5). The window size influences on the type, the volume and the quality of the acquired relations. Generally, the smaller windows allow to acquire more relevant contexts for a target, but increase the data sparseness problem (Rapp, 2003). They give better results for classical types of relations (eg. synonymy), whereas larger windows ar</context>
</contexts>
<marker>Schmid, 1994</marker>
<rawString>H. Schmid. 1994. Probabilistic part-of-speech tagging using decision trees. In New Methods in Language Processing, pages 44–49, Manchester, UK.</rawString>
</citation>
<citation valid="true">
<authors>
<author>Hinrich Sch¨utze</author>
</authors>
<title>Automatic word sense discrimination.</title>
<date>1998</date>
<journal>Comput. Linguist.,</journal>
<volume>24</volume>
<issue>1</issue>
<marker>Sch¨utze, 1998</marker>
<rawString>Hinrich Sch¨utze. 1998. Automatic word sense discrimination. Comput. Linguist., 24(1):97–123.</rawString>
</citation>
<citation valid="true">
<authors>
<author>Weiyi Sun</author>
<author>Anna Rumshisky</author>
<author>¨Ozlem Uzuner</author>
</authors>
<title>Evaluating temporal relations in clinical text: 2012 i2b2 challenge.</title>
<date>2013</date>
<journal>JAMIA,</journal>
<volume>20</volume>
<issue>5</issue>
<contexts>
<context position="9326" citStr="Sun et al., 2013" startWordPosition="1460" endWordPosition="1463">ted, according to the corpus, as the mean of the values of parameters on the corpus. And we experiment two thresholds on similarity score we empirically defined: sim &gt; 0.001 and sim &gt; 0.0005. 91 4 Experiments In this section, we present the material we use for the experiments and evaluation, and the distributional parameter values of the VSM automatically determined from the data. We then describe the generalization sets we experiment and the evaluation measures we used for evaluation. 4.1 Corpus We use the collection of anonymous clinical English texts provided by the 2012 i2b2/VA challenge (Sun et al., 2013). The corpus is pre-processed within the Ogmios platform (Hamon et al., 2007). We perform morphosyntactic tagging and lemmatization with Tree Tagger (Schmid, 1994), and term extraction with YATEA (Aubin and Hamon, 2006). 4.2 Distributional parameters We consider two window sizes: a large window of 21 words (± 10 words, centered on the target, henceforth W21) and a narrow one of 5 words (± 2 words, centered on the target, W5). The window size influences on the type, the volume and the quality of the acquired relations. Generally, the smaller windows allow to acquire more relevant contexts for a</context>
</contexts>
<marker>Sun, Rumshisky, Uzuner, 2013</marker>
<rawString>Weiyi Sun, Anna Rumshisky, and ¨Ozlem Uzuner. 2013. Evaluating temporal relations in clinical text: 2012 i2b2 challenge. JAMIA, 20(5):806–813.</rawString>
</citation>
<citation valid="true">
<authors>
<author>George Tsatsaronis</author>
<author>Vicky Panagiotopoulou</author>
</authors>
<title>A generalized vector space model for text retrieval based on semantic relatedness.</title>
<date>2009</date>
<booktitle>In EACL 2009,</booktitle>
<pages>70--78</pages>
<location>Stroudsburg, PA, USA.</location>
<contexts>
<context position="4688" citStr="Tsatsaronis and Panagiotopoulou (2009)" startWordPosition="728" endWordPosition="731">ify the weights of 90 Proceedings of the 5th International Workshop on Health Text Mining and Information Analysis (Louhi) @ EACL 2014, pages 90–95, Gothenburg, Sweden, April 26-30 2014. c�2014 Association for Computational Linguistics the elements in contexts relying on the semantic neighbors found with a distributional similarity measure. Based on this work, Ferret (2013) uses a set of examples selected from an original distributional thesaurus to train a supervised classifier. This classifier is then applied for reranking the neighbors of the thesaurus selection. Within Vector Space Model, Tsatsaronis and Panagiotopoulou (2009) use a word thesaurus to interpret the orthogonality of terms and measure semantic relatedness. With the same purpose of solving the problem of data sparseness, other methods are based on dimensionality reduction, such as Latent Semantic Analysis (LSA) in (Pad´o and Lapata, 2007) or Non-negative Matrix Factorization (NMF) (Zheng et al., 2011). Matrix decomposition techniques are usually applied to reduce the dimensionality of the original matrix, thereby rendering it more informative (Mitchell and Lapata, 2010). Our approach differs from the aforementioned ones in that we add semantic informat</context>
</contexts>
<marker>Tsatsaronis, Panagiotopoulou, 2009</marker>
<rawString>George Tsatsaronis and Vicky Panagiotopoulou. 2009. A generalized vector space model for text retrieval based on semantic relatedness. In EACL 2009, pages 70–78, Stroudsburg, PA, USA.</rawString>
</citation>
<citation valid="true">
<authors>
<author>Peter D Turney</author>
<author>Patrick Pantel</author>
</authors>
<title>From frequency to meaning: Vector space models of semantics.</title>
<date>2010</date>
<journal>JAIR,</journal>
<pages>37--141</pages>
<contexts>
<context position="1149" citStr="Turney and Pantel, 2010" startWordPosition="159" endWordPosition="163">a sparseness. To tackle this problem, we generalize contexts by integrating semantic relations acquired with linguistic approaches. We use three methods that acquire hypernymy relations on a EHR corpus. Context Generalization obtains the best results when performed with hypernyms, the quality of the relations being more important than the quantity. 1 Introduction Distributional Analysis (DA) (Harris, 1954; Firth, 1957) computes a similarity between target words from the contexts shared by those two words. This hypothesis is applied with geometric methods, such as the Vector Space Model (VSM) (Turney and Pantel, 2010). The advantage of the VSM is that the similarity of word meaning can be easily quantified by measuring their distance in the vector space, or the cosine of the angle between them (Mitchell and Lapata, 2010). On the other hand, a major inconvenience is data sparseness within the matrix that represents the vector space (Turney and Pantel, 2010). The data sparseness problem is the consequence of the word distribution in a corpus (Baroni et al., 2009): in any corpus, most of the words have a very low frequency and appear only a few times. Thus, those words have a limited set of contexts and simil</context>
</contexts>
<marker>Turney, Pantel, 2010</marker>
<rawString>Peter D. Turney and Patrick Pantel. 2010. From frequency to meaning: Vector space models of semantics. JAIR, 37:141–188.</rawString>
</citation>
<citation valid="true">
<authors>
<author>Lonneke van der Plas</author>
</authors>
<title>Automatic lexicosemantic acquisition for question answering. Th`ese de doctorat,</title>
<date>2008</date>
<institution>University of Groningen, Groningen.</institution>
<marker>van der Plas, 2008</marker>
<rawString>Lonneke van der Plas. 2008. Automatic lexicosemantic acquisition for question answering. Th`ese de doctorat, University of Groningen, Groningen.</rawString>
</citation>
<citation valid="true">
<authors>
<author>Julie Weeds</author>
<author>David Weir</author>
</authors>
<title>Co-occurrence retrieval: A flexible framework for lexical distributional similarity.</title>
<date>2005</date>
<journal>Comput. Linguist.,</journal>
<volume>31</volume>
<issue>4</issue>
<contexts>
<context position="1875" citStr="Weeds and Weir, 2005" startWordPosition="289" endWordPosition="292">eir distance in the vector space, or the cosine of the angle between them (Mitchell and Lapata, 2010). On the other hand, a major inconvenience is data sparseness within the matrix that represents the vector space (Turney and Pantel, 2010). The data sparseness problem is the consequence of the word distribution in a corpus (Baroni et al., 2009): in any corpus, most of the words have a very low frequency and appear only a few times. Thus, those words have a limited set of contexts and similarity is difficult to catch. Thus, methods based on DA perform better when more information is available (Weeds and Weir, 2005; van der Plas, 2008) and are efficient with large corpora of general language. But with specialized texts, as EHR texts that are usually of smaller size, reducing data sparseness is a major issue and methods need to be adapted. Semantic grouping of contexts should decrease their diversity, and thus increase the frequency of the remaining generalized contexts. We assume that generalizing contexts may influence the distributional context frequencies. Information for generalization can be issued from existing resources or can be computed by linguistic approaches. In this paper, we propose to use</context>
</contexts>
<marker>Weeds, Weir, 2005</marker>
<rawString>Julie Weeds and David Weir. 2005. Co-occurrence retrieval: A flexible framework for lexical distributional similarity. Comput. Linguist., 31(4):439–475.</rawString>
</citation>
<citation valid="true">
<authors>
<author>Yorick A Wilks</author>
<author>James E Mcdonald Dan</author>
<author>Tony Plate</author>
<author>Brian M Slator</author>
</authors>
<title>Providing machine tractable dictionary tools.</title>
<date>1990</date>
<journal>Journal of Machine Translation,</journal>
<volume>2</volume>
<contexts>
<context position="6265" citStr="Wilks et al., 1990" startWordPosition="973" endWordPosition="976">y be used to form probability estimates. The goal of our method is to influence the distributional context frequencies by generalizing contexts. Step 1: target and context definition During this step, we define targets and contexts, with different constraints for their extraction. To adapt our method to specialized texts, we identify terms (specific terminological entities that denote an event) with a term extractor (YATEA (Aubin and Hamon, 2006)). Target words are both nouns and terms (T). Their distributional contexts correspond to a graphical window of n number of words around the targets (Wilks et al., 1990; Sch¨utze, 1998; Lund and Burgess, 1996). We consider two different window sizes defined in section 4. Linguistic approaches During the generalization process, we use three existing linguistic approaches: two that acquire hypernymy relations and one to get morphosyntactic variants. Lexicosyntactic Patterns (LSP) acquire hypernymy relations. We use the patterns defined by (Hearst, 1992). Lexical Inclusion (LI) acquires hypernymy relations and uses the syntactic analysis of the terms. Based on the hypothesis that if a term is lexically included in another, generally there is a hypernymy relatio</context>
</contexts>
<marker>Wilks, Dan, Plate, Slator, 1990</marker>
<rawString>Yorick A. Wilks, Dan, James E. Mcdonald, Tony Plate, and Brian M. Slator. 1990. Providing machine tractable dictionary tools. Journal of Machine Translation, 2.</rawString>
</citation>
<citation valid="true">
<authors>
<author>Deniz Yuret</author>
</authors>
<title>Fastsubs: An efficient and exact procedure for finding the most likely lexical substitutes based on an n-gram language model.</title>
<date>2012</date>
<journal>IEEE Signal Process. Lett.,</journal>
<volume>19</volume>
<issue>11</issue>
<contexts>
<context position="3657" citStr="Yuret, 2012" startWordPosition="574" endWordPosition="575">rks that influence distributional contexts to improve the performance of VSMs. Some of them intend to change the way to consider contexts; Broda et al. (2009) do not use the raw context frequency in DA, but they first rank contexts according to their frequency, and take the rank into account. Other models use statistical language models to determine the most likely substitutes to represent the contexts (Baskaya et al., 2013). They assign probabilities to arbitrary sequences of words that are then used to create word pairs to feed a co-occurrence model, before performing a clustered algorithm (Yuret, 2012). The limit of such methods is that their performance is proportional to vocabulary size and requires the availability of training data. Influence on contexts may also be performed by embedding additional semantic information. The semantic relations may be issued from an existing resource or automatically computed. With a method based on bootstrapping, ZhitomirskyGeffet and Dagan (2009) modify the weights of 90 Proceedings of the 5th International Workshop on Health Text Mining and Information Analysis (Louhi) @ EACL 2014, pages 90–95, Gothenburg, Sweden, April 26-30 2014. c�2014 Association f</context>
</contexts>
<marker>Yuret, 2012</marker>
<rawString>Deniz Yuret. 2012. Fastsubs: An efficient and exact procedure for finding the most likely lexical substitutes based on an n-gram language model. IEEE Signal Process. Lett., 19(11):725–728.</rawString>
</citation>
<citation valid="true">
<authors>
<author>Wenbin Zheng</author>
<author>Yuntao Qian</author>
<author>Hong Tang</author>
</authors>
<title>Dimensionality reduction with category information fusion and non-negative matrix factorization for text categorization.</title>
<date>2011</date>
<volume>7004</volume>
<pages>505--512</pages>
<editor>In Hepu Deng, Duoqian Miao, Jingsheng Lei, and Fu Lee Wang, editors, AICI,</editor>
<contexts>
<context position="5032" citStr="Zheng et al., 2011" startWordPosition="781" endWordPosition="784">Ferret (2013) uses a set of examples selected from an original distributional thesaurus to train a supervised classifier. This classifier is then applied for reranking the neighbors of the thesaurus selection. Within Vector Space Model, Tsatsaronis and Panagiotopoulou (2009) use a word thesaurus to interpret the orthogonality of terms and measure semantic relatedness. With the same purpose of solving the problem of data sparseness, other methods are based on dimensionality reduction, such as Latent Semantic Analysis (LSA) in (Pad´o and Lapata, 2007) or Non-negative Matrix Factorization (NMF) (Zheng et al., 2011). Matrix decomposition techniques are usually applied to reduce the dimensionality of the original matrix, thereby rendering it more informative (Mitchell and Lapata, 2010). Our approach differs from the aforementioned ones in that we add semantic information in contexts to reduce the number of contexts and to increase their frequency. Contrary to these latter approaches, we do not reduce the contexts by removing information but by generalyzing information and integrating extra semantic knowledge. 3 VSM and context generalization The contexts in which occurs a target word have associated frequ</context>
</contexts>
<marker>Zheng, Qian, Tang, 2011</marker>
<rawString>Wenbin Zheng, Yuntao Qian, and Hong Tang. 2011. Dimensionality reduction with category information fusion and non-negative matrix factorization for text categorization. In Hepu Deng, Duoqian Miao, Jingsheng Lei, and Fu Lee Wang, editors, AICI, volume 7004 of LNCS, pages 505–512.</rawString>
</citation>
<citation valid="true">
<authors>
<author>Maayan Zhitomirsky-Geffet</author>
<author>Ido Dagan</author>
</authors>
<title>Bootstrapping distributional feature vector quality.</title>
<date>2009</date>
<journal>Comput. Linguist.,</journal>
<volume>35</volume>
<issue>3</issue>
<marker>Zhitomirsky-Geffet, Dagan, 2009</marker>
<rawString>Maayan Zhitomirsky-Geffet and Ido Dagan. 2009. Bootstrapping distributional feature vector quality. Comput. Linguist., 35(3):435–461.</rawString>
</citation>
</citationList>
</algorithm>
</algorithms>
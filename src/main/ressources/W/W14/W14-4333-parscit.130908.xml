<?xml version="1.0" encoding="UTF-8"?>
<algorithms version="110505">
<algorithm name="SectLabel" version="110505">
<variant no="0" confidence="0.211360">
<title confidence="0.993136">
SAWDUST: a Semi-Automated Wizard Dialogue Utterance Selection Tool
for domain-independent large-domain dialogue
</title>
<author confidence="0.99493">
Sudeep Gandhe David Traum
</author>
<affiliation confidence="0.999357">
University of Southern California, Institute for Creative Technologies
</affiliation>
<email confidence="0.996131">
srgandhe@gmail.com, traum@ict.usc.edu
</email>
<sectionHeader confidence="0.993845" genericHeader="abstract">
Abstract
</sectionHeader>
<bodyText confidence="0.999978928571429">
We present a tool that allows human wiz-
ards to select appropriate response utter-
ances for a given dialogue context from
a set of utterances observed in a dia-
logue corpus. Such a tool can be used
in Wizard-of-Oz studies and for collecting
data which can be used for training and/or
evaluating automatic dialogue models. We
also propose to incorporate such automatic
dialogue models back into the tool as an
aid in selecting utterances from a large di-
alogue corpus. The tool allows a user to
rank candidate utterances for selection ac-
cording to these automatic models.
</bodyText>
<sectionHeader confidence="0.987921" genericHeader="keywords">
1 Motivation
</sectionHeader>
<bodyText confidence="0.999981625">
Dialogue corpora play an increasingly important
role as a resource for dialogue system creation.
In addition to its traditional roles, such as train-
ing language models for speech recognition and
natural language understanding, the dialogue cor-
pora can be directly used for the selection ap-
proach to response formation (Gandhe and Traum,
2010). In the selection approach, the response is
formulated by simply picking the appropriate ut-
terance from a set of previously observed utter-
ances. This approach is used in many wizard of
oz systems, where the wizard presses a button to
select an utterance, as well as in many automated
dialogue systems (Leuski et al., 2006; Zukerman
and Marom, 2006; Sellberg and J¨onsson, 2008)
The resources required for the selection ap-
proach are a set of utterances to choose from and
optionally, a set of pairs of (context, response
utterance) to train automatic dialogue models. A
wizard can generate such resources by performing
two types of tasks. First is the traditional Wizard-
of-Oz dialogue collection, where a wizard inter-
acts with a user of the dialogue system. Here the
wizard selects an appropriate response utterance
for a context that is being updated in a dynamic
fashion as the dialogue proceeds (dynamic context
setting). The second task is geared towards gather-
ing data for training/evaluating automatic dialogue
models, where a wizard is required to select ap-
propriate responses (perhaps more than one) for a
context which is extracted from a human-human
dialogue. The context does not change based on
the wizard’s choices (static context setting).
A wizard tool should help with the challenges
presented by these tasks. A challenge for both
of these tasks is that if the number of utterances
in the corpus is large (e.g., more than the num-
ber of buttons that can be placed on a computer
screen), it may be very difficult for a wizard to lo-
cate appropriate utterances. For the second task of
creating human-verified training/evaluation data,
tools like NPCEditor (Leuski and Traum, 2010)
have been developed which, allow the tagging of
a many to many relationships between contexts
(approximated simply as input utterance) and re-
sponses. In other cases, a corpus of dialogues is
used to acquire the set of selectable utterances, in
which each context is followed by a single next
utterance, and many utterances appear only once.
This sparsity of data makes the selection task hard.
Moreover, it may be the case that there are many
possible continuations of a context or contexts in
which an utterance may be appropriate (DeVault
et al., 2011).
We address these needs with a semi-automated
wizard tool that allows a wizard to engage in dy-
namic or static context utterance selection, select
multiple responses, and use several kinds of search
tools to locate promising utterances from a large
set that can’t all be displayed or remembered. In
the next section we describe the tool and how it
can be used. Then we describe how this tool was
used to create evaluation data in the static context
setting.
</bodyText>
<page confidence="0.917878">
251
</page>
<note confidence="0.356945">
Proceedings of the SIGDIAL 2014 Conference, pages 251–253,
Philadelphia, U.S.A., 18-20 June 2014. c�2014 Association for Computational Linguistics
</note>
<figureCaption confidence="0.9963358">
Figure 2: A Histogram for the
number of selected appropriate
responses.
Figure 1: A screenshot of the interface for the wizard data collection Figure 3: Avg. cardinality of the
in static context setting. set for different values of |R|.
</figureCaption>
<sectionHeader confidence="0.903812" genericHeader="introduction">
2 Wizard Tool
</sectionHeader>
<bodyText confidence="0.999962212765958">
Our wizard tool consists of several different views
(see figure 1), and is similar in some respects to the
IORelator annotation tool (DeVault et al., 2010),
but specialized to act as a wizard interface. The
first view (left pane) is a dialogue context, that
shows the recent history of the dialogue, before
the wizard’s decision point. The second view (top
right pane) shows a list of possible utterances that
can be selected from. This view can be ordered
in several different ways, as described below. Fi-
nally, there is a view of selected utterances (bot-
tom right pane). In the case of dynamic context,
the wizard will probably only select one utterance
and then a dialogue partner will respond with a
new utterance that extends the previous context.
In the case of static evaluation, however, used for
training and/or evaluating automated selection al-
gorithms, it is often helpful to select multiple ut-
terances if more than one is appropriate.
To help wizards explore the set of all possible
utterances, we provide the ability to rank the utter-
ances by various automated scores. Our configu-
ration used in the static context task uses Score] as
the score calculated using one of the automatic di-
alogue models, specifically Nearest Context model
(Gandhe and Traum, 2007) - this model orders
candidate utterances from the corpus by the sim-
ilarity of their previous two utterances to the cur-
rent dialogue context. Score2 is surface text sim-
ilarity, computed as the METEOR score (Lavie
and Denkowski, 2009) between the candidate ut-
terance and the actual response utterance present
at that location in original human-human dialogue
(which is not available to the wizard). Wizards can
also search the set of utterances for specific key-
words and the third column, Relevance, shows the
score for the search string entered by the wizards.
The last column RF stands for relevance feedback
and ranks the utterances by similarity to the utter-
ances that have already been chosen by the wiz-
ard. This allows wizards to easily find paraphrases
of already selected response utterances. Clicking
the header of any of these columns will reorder the
utterance list by the automated score, by relevance
(assuming a search term has been entered) or by
relevance feedback (assuming one or more utter-
ances have already been chosen).
</bodyText>
<sectionHeader confidence="0.998969" genericHeader="method">
3 Evaluation
</sectionHeader>
<bodyText confidence="0.999963">
We evaluated the tool by having four human vol-
unteers (wizards) use it in order to establish an up-
per baseline for human-level performance in the
static context evaluation task described in (Gandhe
and Traum, 2013). Wizards were instructed in how
to use the search and relevance feedback features.
In order to not bias the wizards, they were not told
exactly what score] and score2 indicate, but just
that the scores can be useful in search.
Each wizard is presented with a set of utter-
ances (Utrain) (|Utrain |≈ 500) and is asked to
select a subset from these that will be appropri-
ate as a response for the presented dialogue con-
text. Each wizard was requested to select some-
where between 5 to 10 (at-least one) appropriate
responses for each dialogue context extracted from
</bodyText>
<page confidence="0.992949">
252
</page>
<bodyText confidence="0.999967695652174">
five different human-human dialogues. There are
a total of 89 dialogue contexts for the role that
the wizards were to play. Figure 2 shows the his-
togram for the number of utterances selected as
appropriate responses by the four wizards. As ex-
pected, wizards frequently chose multiple utter-
ances as appropriate responses (mean = 7.80, min
= 1, max = 25).
To get an idea about how much the wizards
agree among themselves for this task, we calcu-
lated the overlap between the utterances selected
by a specific wizard and the utterances selected by
another wizard or a set of wizards. Let UTc be a set
of utterances selected by a wizard T for a dialogue
context c. Let R be a set of wizards (T ∈/ R) and
URc be the union of sets of utterances selected by
the set of wizards (R) for the same context c. Then
we define the following overlap measures,
We compute the average values of these over-
lap measures for all contexts and for all possible
settings of test wizards and reference wizards. Ta-
ble 1 shows the results with different values for the
number of wizards used as reference.
</bodyText>
<table confidence="0.9984555">
#ref Prec. Rec. Jacc. Dice Meteor
1 0.145 0.145 0.077 0.141 0.290
2 0.244 0.134 0.093 0.170 0.412
3 0.311 0.121 0.094 0.171 0.478
</table>
<tableCaption confidence="0.999807">
Table 1: Inter-wizard agreement
</tableCaption>
<bodyText confidence="0.99754494117647">
Precision can be interpreted as the probability
that a response utterance selected by a wizard is
also considered appropriate by at least one other
wizard. Precision rapidly increases along with
the number of reference wizards used. This hap-
pens because the size of the set URc steadily in-
creases with more reference wizards. Figure 3
shows this observed increase and the expected in-
crease if there were no overlap between the wiz-
ards. The near-linear increase in |URc  |suggests
that selecting appropriate responses is a hard task
and may require a lot more than four wizards to
achieve convergence.
Subjectively, the wizards reported no major us-
ability problems with the tool, and were able to
use all four utterance ordering techniques to find
appropriate utterances.
</bodyText>
<sectionHeader confidence="0.999436" genericHeader="method">
4 Future Work
</sectionHeader>
<bodyText confidence="0.999990846153846">
Future work involves performing some formal
evaluations comparing this tool to other tools (that
are missing some of the features of this tool) in
terms of amount of time taken to make selections
and quality of the selections, using the same eval-
uation techniques as (Gandhe and Traum, 2013).
We also see a promising future for semi-
automated selection, which blurs the line between
a pure algorithmic response and pure wizard se-
lection. Here the wizard can select appropriate re-
sponses, which can be used by algorithms as su-
pervised training data, meanwhile the algorithms
can be used to seed the wizard’s selection.
</bodyText>
<sectionHeader confidence="0.998911" genericHeader="references">
References
</sectionHeader>
<reference confidence="0.997988714285714">
David DeVault, Susan Robinson, and David Traum. 2010.
IORelator: A graphical user interface to enable rapid se-
mantic annotation for data-driven natural language under-
standing. In Proc. of the 5th Joint ISO-ACL/SIGSEM
Workshop on Interoperable Semantic Annotation (ISA-5).
David DeVault, Anton Leuski, and Kenji Sagae. 2011. An
evaluation of alternative strategies for implementing dia-
logue policies using statistical classification and rules. In
Proceedings of the IJCNLP 2011, Nov.
Sudeep Gandhe and David Traum. 2007. Creating spoken
dialogue characters from corpora without annotations. In
Proceedings of Interspeech-07, Antwerp, Belgium.
Sudeep Gandhe and David Traum. 2010. I’ve said it be-
fore, and I’ll say it again: an empirical investigation of
the upper bound of the selection approach to dialogue. In
Proceedings of the SIGDIAL ’10, Tokyo, Japan.
Sudeep Gandhe and David Traum. 2013. Surface text based
dialogue models for virtual humans. In Proceedings of the
SIGDIAL 2013, Metz, France.
A. Lavie and M. J. Denkowski. 2009. The meteor metric
for automatic evaluation of machine translation. Machine
Translation, 23:105–115.
Anton Leuski and David R. Traum. 2010. NPCEditor: A
tool for building question-answering characters. In Pro-
ceedings of LREC 2010, Valletta, Malta.
Anton Leuski, Ronakkumar Patel, David Traum, and Bran-
don Kennedy. 2006. Building effective question answer-
ing characters. In Proc. of SIGDIAL ’06, Australia.
Linus Sellberg and Arne J¨onsson. 2008. Using random in-
dexing to improve singular value decomposition for latent
semantic analysis. In Proceedings of LREC’08, Morocco.
Ingrid Zukerman and Yuval Marom. 2006. A corpus-based
approach to help-desk response generation. In Computa-
tional Intelligence for Modelling, Control and Automation
(CIMCA 2006), IAWTIC 2006.
</reference>
<figure confidence="0.994525090909091">
c  |Recallc = |UTc ∩ UR c |
|UR c |
Jaccardc = |UTc ∩ UR c  |2|UT c ∩ UR c |
|UT c ∪ UR c  |Dicec = |UTc  |+ |URc |
T∩
Precisionc =  |Uc
UR c |
|UT
1 |EUTc  |METEOR (ut, URc ) ∀ut ∈ UTc
Meteorc =
ut
</figure>
<page confidence="0.986302">
253
</page>
</variant>
</algorithm>
<algorithm name="ParsHed" version="110505">
<variant no="0" confidence="0.008052">
<title confidence="0.9942045">SAWDUST: a Semi-Automated Wizard Dialogue Utterance Selection for domain-independent large-domain dialogue</title>
<author confidence="0.999186">Sudeep Gandhe David Traum</author>
<affiliation confidence="0.998547">University of Southern California, Institute for Creative</affiliation>
<email confidence="0.999036">srgandhe@gmail.com,traum@ict.usc.edu</email>
<abstract confidence="0.978050532407408">We present a tool that allows human wizards to select appropriate response utterances for a given dialogue context from a set of utterances observed in a dialogue corpus. Such a tool can be used in Wizard-of-Oz studies and for collecting data which can be used for training and/or evaluating automatic dialogue models. We also propose to incorporate such automatic dialogue models back into the tool as an aid in selecting utterances from a large dialogue corpus. The tool allows a user to rank candidate utterances for selection according to these automatic models. 1 Motivation Dialogue corpora play an increasingly important role as a resource for dialogue system creation. In addition to its traditional roles, such as training language models for speech recognition and natural language understanding, the dialogue corcan be directly used for the apresponse formation (Gandhe and Traum, In the the response is formulated by simply picking the appropriate utterance from a set of previously observed utterances. This approach is used in many wizard of oz systems, where the wizard presses a button to select an utterance, as well as in many automated dialogue systems (Leuski et al., 2006; Zukerman and Marom, 2006; Sellberg and J¨onsson, 2008) resources required for the approach are a set of utterances to choose from and optionally, a set of pairs of (context, response utterance) to train automatic dialogue models. A wizard can generate such resources by performing two types of tasks. First is the traditional Wizardof-Oz dialogue collection, where a wizard interacts with a user of the dialogue system. Here the wizard selects an appropriate response utterance for a context that is being updated in a dynamic as the dialogue proceeds context The second task is geared towards gathering data for training/evaluating automatic dialogue models, where a wizard is required to select appropriate responses (perhaps more than one) for a context which is extracted from a human-human dialogue. The context does not change based on wizard’s choices context A wizard tool should help with the challenges presented by these tasks. A challenge for both of these tasks is that if the number of utterances in the corpus is large (e.g., more than the number of buttons that can be placed on a computer screen), it may be very difficult for a wizard to locate appropriate utterances. For the second task of creating human-verified training/evaluation data, tools like NPCEditor (Leuski and Traum, 2010) have been developed which, allow the tagging of a many to many relationships between contexts (approximated simply as input utterance) and responses. In other cases, a corpus of dialogues is used to acquire the set of selectable utterances, in which each context is followed by a single next utterance, and many utterances appear only once. This sparsity of data makes the selection task hard. Moreover, it may be the case that there are many possible continuations of a context or contexts in which an utterance may be appropriate (DeVault et al., 2011). We address these needs with a semi-automated wizard tool that allows a wizard to engage in dynamic or static context utterance selection, select multiple responses, and use several kinds of search tools to locate promising utterances from a large set that can’t all be displayed or remembered. In the next section we describe the tool and how it can be used. Then we describe how this tool was used to create evaluation data in the static context setting. 251 of the SIGDIAL 2014 pages 251–253, U.S.A., 18-20 June 2014. Association for Computational Linguistics Figure 2: A Histogram for the number of selected appropriate responses. Figure 1: A screenshot of the interface for the wizard data collection in static context setting. Figure 3: Avg. cardinality of the for different values of 2 Wizard Tool Our wizard tool consists of several different views (see figure 1), and is similar in some respects to the IORelator annotation tool (DeVault et al., 2010), but specialized to act as a wizard interface. The first view (left pane) is a dialogue context, that shows the recent history of the dialogue, before the wizard’s decision point. The second view (top right pane) shows a list of possible utterances that can be selected from. This view can be ordered in several different ways, as described below. Finally, there is a view of selected utterances (bottom right pane). In the case of dynamic context, the wizard will probably only select one utterance and then a dialogue partner will respond with a new utterance that extends the previous context. In the case of static evaluation, however, used for training and/or evaluating automated selection algorithms, it is often helpful to select multiple utterances if more than one is appropriate. To help wizards explore the set of all possible utterances, we provide the ability to rank the utterances by various automated scores. Our configuused in the static context task uses the score calculated using one of the automatic dimodels, specifically Context (Gandhe and Traum, 2007) this model orders candidate utterances from the corpus by the similarity of their previous two utterances to the curdialogue context. surface text simcomputed as the (Lavie and Denkowski, 2009) between the candidate utterance and the actual response utterance present at that location in original human-human dialogue (which is not available to the wizard). Wizards can also search the set of utterances for specific keyand the third column, shows the score for the search string entered by the wizards. last column for relevance feedback and ranks the utterances by similarity to the utterances that have already been chosen by the wizard. This allows wizards to easily find paraphrases of already selected response utterances. Clicking the header of any of these columns will reorder the utterance list by the automated score, by relevance (assuming a search term has been entered) or by relevance feedback (assuming one or more utterances have already been chosen). 3 Evaluation We evaluated the tool by having four human volunteers (wizards) use it in order to establish an upper baseline for human-level performance in the static context evaluation task described in (Gandhe and Traum, 2013). Wizards were instructed in how to use the search and relevance feedback features. In order to not bias the wizards, they were not told what but just that the scores can be useful in search. Each wizard is presented with a set of utter- ≈ and is asked to select a subset from these that will be appropriate as a response for the presented dialogue context. Each wizard was requested to select somewhere between 5 to 10 (at-least one) appropriate responses for each dialogue context extracted from 252 five different human-human dialogues. There are a total of 89 dialogue contexts for the role that the wizards were to play. Figure 2 shows the histogram for the number of utterances selected as appropriate responses by the four wizards. As expected, wizards frequently chose multiple utterances as appropriate responses (mean = 7.80, min = 1, max = 25). To get an idea about how much the wizards agree among themselves for this task, we calculated the overlap between the utterances selected by a specific wizard and the utterances selected by wizard or a set of wizards. Let a set utterances selected by a wizard a dialogue Let a set of wizards and the union of sets of utterances selected by set of wizards for the same context Then we define the following overlap measures, We compute the average values of these overlap measures for all contexts and for all possible settings of test wizards and reference wizards. Table 1 shows the results with different values for the number of wizards used as reference. 1 0.145 0.145 0.077 0.141 0.290 2 0.244 0.134 0.093 0.170 0.412 3 0.311 0.121 0.094 0.171 0.478 Table 1: Inter-wizard agreement Precision can be interpreted as the probability that a response utterance selected by a wizard is also considered appropriate by at least one other wizard. Precision rapidly increases along with the number of reference wizards used. This hapbecause the size of the set increases with more reference wizards. Figure 3 shows this observed increase and the expected increase if there were no overlap between the wiz- The near-linear increase in that selecting appropriate responses is a hard task and may require a lot more than four wizards to achieve convergence. Subjectively, the wizards reported no major usability problems with the tool, and were able to use all four utterance ordering techniques to find appropriate utterances. 4 Future Work Future work involves performing some formal evaluations comparing this tool to other tools (that are missing some of the features of this tool) in terms of amount of time taken to make selections and quality of the selections, using the same evaluation techniques as (Gandhe and Traum, 2013). We also see a promising future for semiautomated selection, which blurs the line between a pure algorithmic response and pure wizard selection. Here the wizard can select appropriate responses, which can be used by algorithms as supervised training data, meanwhile the algorithms can be used to seed the wizard’s selection. References David DeVault, Susan Robinson, and David Traum. 2010. IORelator: A graphical user interface to enable rapid semantic annotation for data-driven natural language under- In of the 5th Joint ISO-ACL/SIGSEM</abstract>
<title confidence="0.868499">on Interoperable Semantic Annotation</title>
<author confidence="0.859253">An</author>
<abstract confidence="0.79469548">evaluation of alternative strategies for implementing dialogue policies using statistical classification and rules. In of the IJCNLP Nov. Sudeep Gandhe and David Traum. 2007. Creating spoken dialogue characters from corpora without annotations. In of Antwerp, Belgium. Sudeep Gandhe and David Traum. 2010. I’ve said it before, and I’ll say it again: an empirical investigation of the upper bound of the selection approach to dialogue. In of the SIGDIAL Tokyo, Japan. Sudeep Gandhe and David Traum. 2013. Surface text based models for virtual humans. In of the Metz, France. A. Lavie and M. J. Denkowski. 2009. The meteor metric automatic evaluation of machine translation. 23:105–115. Anton Leuski and David R. Traum. 2010. NPCEditor: A for building question-answering characters. In Proof LREC Valletta, Malta. Anton Leuski, Ronakkumar Patel, David Traum, and Brandon Kennedy. 2006. Building effective question answercharacters. In of SIGDIAL Australia. Linus Sellberg and Arne J¨onsson. 2008. Using random indexing to improve singular value decomposition for latent analysis. In of Morocco.</abstract>
<note confidence="0.892504">Ingrid Zukerman and Yuval Marom. 2006. A corpus-based to help-desk response generation. In Computational Intelligence for Modelling, Control and Automation 2006), IAWTIC c c c 253</note>
</variant>
</algorithm>
<algorithm name="ParsCit" version="110505">
<citationList>
<citation valid="true">
<authors>
<author>David DeVault</author>
<author>Susan Robinson</author>
<author>David Traum</author>
</authors>
<title>IORelator: A graphical user interface to enable rapid semantic annotation for data-driven natural language understanding.</title>
<date>2010</date>
<booktitle>In Proc. of the 5th Joint ISO-ACL/SIGSEM Workshop on Interoperable Semantic Annotation (ISA-5).</booktitle>
<contexts>
<context position="4440" citStr="DeVault et al., 2010" startWordPosition="713" endWordPosition="716">s used to create evaluation data in the static context setting. 251 Proceedings of the SIGDIAL 2014 Conference, pages 251–253, Philadelphia, U.S.A., 18-20 June 2014. c�2014 Association for Computational Linguistics Figure 2: A Histogram for the number of selected appropriate responses. Figure 1: A screenshot of the interface for the wizard data collection Figure 3: Avg. cardinality of the in static context setting. set for different values of |R|. 2 Wizard Tool Our wizard tool consists of several different views (see figure 1), and is similar in some respects to the IORelator annotation tool (DeVault et al., 2010), but specialized to act as a wizard interface. The first view (left pane) is a dialogue context, that shows the recent history of the dialogue, before the wizard’s decision point. The second view (top right pane) shows a list of possible utterances that can be selected from. This view can be ordered in several different ways, as described below. Finally, there is a view of selected utterances (bottom right pane). In the case of dynamic context, the wizard will probably only select one utterance and then a dialogue partner will respond with a new utterance that extends the previous context. In</context>
</contexts>
<marker>DeVault, Robinson, Traum, 2010</marker>
<rawString>David DeVault, Susan Robinson, and David Traum. 2010. IORelator: A graphical user interface to enable rapid semantic annotation for data-driven natural language understanding. In Proc. of the 5th Joint ISO-ACL/SIGSEM Workshop on Interoperable Semantic Annotation (ISA-5).</rawString>
</citation>
<citation valid="true">
<authors>
<author>David DeVault</author>
<author>Anton Leuski</author>
<author>Kenji Sagae</author>
</authors>
<title>An evaluation of alternative strategies for implementing dialogue policies using statistical classification and rules.</title>
<date>2011</date>
<booktitle>In Proceedings of the IJCNLP</booktitle>
<contexts>
<context position="3425" citStr="DeVault et al., 2011" startWordPosition="547" endWordPosition="550">ing/evaluation data, tools like NPCEditor (Leuski and Traum, 2010) have been developed which, allow the tagging of a many to many relationships between contexts (approximated simply as input utterance) and responses. In other cases, a corpus of dialogues is used to acquire the set of selectable utterances, in which each context is followed by a single next utterance, and many utterances appear only once. This sparsity of data makes the selection task hard. Moreover, it may be the case that there are many possible continuations of a context or contexts in which an utterance may be appropriate (DeVault et al., 2011). We address these needs with a semi-automated wizard tool that allows a wizard to engage in dynamic or static context utterance selection, select multiple responses, and use several kinds of search tools to locate promising utterances from a large set that can’t all be displayed or remembered. In the next section we describe the tool and how it can be used. Then we describe how this tool was used to create evaluation data in the static context setting. 251 Proceedings of the SIGDIAL 2014 Conference, pages 251–253, Philadelphia, U.S.A., 18-20 June 2014. c�2014 Association for Computational Lin</context>
</contexts>
<marker>DeVault, Leuski, Sagae, 2011</marker>
<rawString>David DeVault, Anton Leuski, and Kenji Sagae. 2011. An evaluation of alternative strategies for implementing dialogue policies using statistical classification and rules. In Proceedings of the IJCNLP 2011, Nov.</rawString>
</citation>
<citation valid="true">
<authors>
<author>Sudeep Gandhe</author>
<author>David Traum</author>
</authors>
<title>Creating spoken dialogue characters from corpora without annotations.</title>
<date>2007</date>
<booktitle>In Proceedings of Interspeech-07,</booktitle>
<location>Antwerp, Belgium.</location>
<contexts>
<context position="5556" citStr="Gandhe and Traum, 2007" startWordPosition="900" endWordPosition="903">terance and then a dialogue partner will respond with a new utterance that extends the previous context. In the case of static evaluation, however, used for training and/or evaluating automated selection algorithms, it is often helpful to select multiple utterances if more than one is appropriate. To help wizards explore the set of all possible utterances, we provide the ability to rank the utterances by various automated scores. Our configuration used in the static context task uses Score] as the score calculated using one of the automatic dialogue models, specifically Nearest Context model (Gandhe and Traum, 2007) - this model orders candidate utterances from the corpus by the similarity of their previous two utterances to the current dialogue context. Score2 is surface text similarity, computed as the METEOR score (Lavie and Denkowski, 2009) between the candidate utterance and the actual response utterance present at that location in original human-human dialogue (which is not available to the wizard). Wizards can also search the set of utterances for specific keywords and the third column, Relevance, shows the score for the search string entered by the wizards. The last column RF stands for relevance</context>
</contexts>
<marker>Gandhe, Traum, 2007</marker>
<rawString>Sudeep Gandhe and David Traum. 2007. Creating spoken dialogue characters from corpora without annotations. In Proceedings of Interspeech-07, Antwerp, Belgium.</rawString>
</citation>
<citation valid="true">
<authors>
<author>Sudeep Gandhe</author>
<author>David Traum</author>
</authors>
<title>I’ve said it before, and I’ll say it again: an empirical investigation of the upper bound of the selection approach to dialogue.</title>
<date>2010</date>
<booktitle>In Proceedings of the SIGDIAL ’10,</booktitle>
<location>Tokyo, Japan.</location>
<contexts>
<context position="1178" citStr="Gandhe and Traum, 2010" startWordPosition="175" endWordPosition="178">ic dialogue models. We also propose to incorporate such automatic dialogue models back into the tool as an aid in selecting utterances from a large dialogue corpus. The tool allows a user to rank candidate utterances for selection according to these automatic models. 1 Motivation Dialogue corpora play an increasingly important role as a resource for dialogue system creation. In addition to its traditional roles, such as training language models for speech recognition and natural language understanding, the dialogue corpora can be directly used for the selection approach to response formation (Gandhe and Traum, 2010). In the selection approach, the response is formulated by simply picking the appropriate utterance from a set of previously observed utterances. This approach is used in many wizard of oz systems, where the wizard presses a button to select an utterance, as well as in many automated dialogue systems (Leuski et al., 2006; Zukerman and Marom, 2006; Sellberg and J¨onsson, 2008) The resources required for the selection approach are a set of utterances to choose from and optionally, a set of pairs of (context, response utterance) to train automatic dialogue models. A wizard can generate such resou</context>
</contexts>
<marker>Gandhe, Traum, 2010</marker>
<rawString>Sudeep Gandhe and David Traum. 2010. I’ve said it before, and I’ll say it again: an empirical investigation of the upper bound of the selection approach to dialogue. In Proceedings of the SIGDIAL ’10, Tokyo, Japan.</rawString>
</citation>
<citation valid="true">
<authors>
<author>Sudeep Gandhe</author>
<author>David Traum</author>
</authors>
<title>Surface text based dialogue models for virtual humans.</title>
<date>2013</date>
<booktitle>In Proceedings of the SIGDIAL 2013,</booktitle>
<location>Metz, France.</location>
<contexts>
<context position="6823" citStr="Gandhe and Traum, 2013" startWordPosition="1109" endWordPosition="1112">rity to the utterances that have already been chosen by the wizard. This allows wizards to easily find paraphrases of already selected response utterances. Clicking the header of any of these columns will reorder the utterance list by the automated score, by relevance (assuming a search term has been entered) or by relevance feedback (assuming one or more utterances have already been chosen). 3 Evaluation We evaluated the tool by having four human volunteers (wizards) use it in order to establish an upper baseline for human-level performance in the static context evaluation task described in (Gandhe and Traum, 2013). Wizards were instructed in how to use the search and relevance feedback features. In order to not bias the wizards, they were not told exactly what score] and score2 indicate, but just that the scores can be useful in search. Each wizard is presented with a set of utterances (Utrain) (|Utrain |≈ 500) and is asked to select a subset from these that will be appropriate as a response for the presented dialogue context. Each wizard was requested to select somewhere between 5 to 10 (at-least one) appropriate responses for each dialogue context extracted from 252 five different human-human dialogu</context>
</contexts>
<marker>Gandhe, Traum, 2013</marker>
<rawString>Sudeep Gandhe and David Traum. 2013. Surface text based dialogue models for virtual humans. In Proceedings of the SIGDIAL 2013, Metz, France.</rawString>
</citation>
<citation valid="true">
<authors>
<author>A Lavie</author>
<author>M J Denkowski</author>
</authors>
<title>The meteor metric for automatic evaluation of machine translation.</title>
<date>2009</date>
<journal>Machine Translation,</journal>
<pages>23--105</pages>
<contexts>
<context position="5789" citStr="Lavie and Denkowski, 2009" startWordPosition="939" endWordPosition="942">lpful to select multiple utterances if more than one is appropriate. To help wizards explore the set of all possible utterances, we provide the ability to rank the utterances by various automated scores. Our configuration used in the static context task uses Score] as the score calculated using one of the automatic dialogue models, specifically Nearest Context model (Gandhe and Traum, 2007) - this model orders candidate utterances from the corpus by the similarity of their previous two utterances to the current dialogue context. Score2 is surface text similarity, computed as the METEOR score (Lavie and Denkowski, 2009) between the candidate utterance and the actual response utterance present at that location in original human-human dialogue (which is not available to the wizard). Wizards can also search the set of utterances for specific keywords and the third column, Relevance, shows the score for the search string entered by the wizards. The last column RF stands for relevance feedback and ranks the utterances by similarity to the utterances that have already been chosen by the wizard. This allows wizards to easily find paraphrases of already selected response utterances. Clicking the header of any of the</context>
</contexts>
<marker>Lavie, Denkowski, 2009</marker>
<rawString>A. Lavie and M. J. Denkowski. 2009. The meteor metric for automatic evaluation of machine translation. Machine Translation, 23:105–115.</rawString>
</citation>
<citation valid="true">
<authors>
<author>Anton Leuski</author>
<author>David R Traum</author>
</authors>
<title>NPCEditor: A tool for building question-answering characters.</title>
<date>2010</date>
<booktitle>In Proceedings of LREC 2010,</booktitle>
<location>Valletta,</location>
<contexts>
<context position="2870" citStr="Leuski and Traum, 2010" startWordPosition="454" endWordPosition="457">iate responses (perhaps more than one) for a context which is extracted from a human-human dialogue. The context does not change based on the wizard’s choices (static context setting). A wizard tool should help with the challenges presented by these tasks. A challenge for both of these tasks is that if the number of utterances in the corpus is large (e.g., more than the number of buttons that can be placed on a computer screen), it may be very difficult for a wizard to locate appropriate utterances. For the second task of creating human-verified training/evaluation data, tools like NPCEditor (Leuski and Traum, 2010) have been developed which, allow the tagging of a many to many relationships between contexts (approximated simply as input utterance) and responses. In other cases, a corpus of dialogues is used to acquire the set of selectable utterances, in which each context is followed by a single next utterance, and many utterances appear only once. This sparsity of data makes the selection task hard. Moreover, it may be the case that there are many possible continuations of a context or contexts in which an utterance may be appropriate (DeVault et al., 2011). We address these needs with a semi-automate</context>
</contexts>
<marker>Leuski, Traum, 2010</marker>
<rawString>Anton Leuski and David R. Traum. 2010. NPCEditor: A tool for building question-answering characters. In Proceedings of LREC 2010, Valletta, Malta.</rawString>
</citation>
<citation valid="true">
<authors>
<author>Anton Leuski</author>
<author>Ronakkumar Patel</author>
<author>David Traum</author>
<author>Brandon Kennedy</author>
</authors>
<title>Building effective question answering characters.</title>
<date>2006</date>
<booktitle>In Proc. of SIGDIAL ’06,</booktitle>
<contexts>
<context position="1500" citStr="Leuski et al., 2006" startWordPosition="230" endWordPosition="233">nt role as a resource for dialogue system creation. In addition to its traditional roles, such as training language models for speech recognition and natural language understanding, the dialogue corpora can be directly used for the selection approach to response formation (Gandhe and Traum, 2010). In the selection approach, the response is formulated by simply picking the appropriate utterance from a set of previously observed utterances. This approach is used in many wizard of oz systems, where the wizard presses a button to select an utterance, as well as in many automated dialogue systems (Leuski et al., 2006; Zukerman and Marom, 2006; Sellberg and J¨onsson, 2008) The resources required for the selection approach are a set of utterances to choose from and optionally, a set of pairs of (context, response utterance) to train automatic dialogue models. A wizard can generate such resources by performing two types of tasks. First is the traditional Wizardof-Oz dialogue collection, where a wizard interacts with a user of the dialogue system. Here the wizard selects an appropriate response utterance for a context that is being updated in a dynamic fashion as the dialogue proceeds (dynamic context setting</context>
</contexts>
<marker>Leuski, Patel, Traum, Kennedy, 2006</marker>
<rawString>Anton Leuski, Ronakkumar Patel, David Traum, and Brandon Kennedy. 2006. Building effective question answering characters. In Proc. of SIGDIAL ’06, Australia.</rawString>
</citation>
<citation valid="true">
<authors>
<author>Linus Sellberg</author>
<author>Arne J¨onsson</author>
</authors>
<title>Using random indexing to improve singular value decomposition for latent semantic analysis.</title>
<date>2008</date>
<booktitle>In Proceedings of LREC’08,</booktitle>
<marker>Sellberg, J¨onsson, 2008</marker>
<rawString>Linus Sellberg and Arne J¨onsson. 2008. Using random indexing to improve singular value decomposition for latent semantic analysis. In Proceedings of LREC’08, Morocco.</rawString>
</citation>
<citation valid="true">
<authors>
<author>Ingrid Zukerman</author>
<author>Yuval Marom</author>
</authors>
<title>A corpus-based approach to help-desk response generation.</title>
<date>2006</date>
<booktitle>In Computational Intelligence for Modelling, Control and Automation (CIMCA</booktitle>
<contexts>
<context position="1526" citStr="Zukerman and Marom, 2006" startWordPosition="234" endWordPosition="237"> for dialogue system creation. In addition to its traditional roles, such as training language models for speech recognition and natural language understanding, the dialogue corpora can be directly used for the selection approach to response formation (Gandhe and Traum, 2010). In the selection approach, the response is formulated by simply picking the appropriate utterance from a set of previously observed utterances. This approach is used in many wizard of oz systems, where the wizard presses a button to select an utterance, as well as in many automated dialogue systems (Leuski et al., 2006; Zukerman and Marom, 2006; Sellberg and J¨onsson, 2008) The resources required for the selection approach are a set of utterances to choose from and optionally, a set of pairs of (context, response utterance) to train automatic dialogue models. A wizard can generate such resources by performing two types of tasks. First is the traditional Wizardof-Oz dialogue collection, where a wizard interacts with a user of the dialogue system. Here the wizard selects an appropriate response utterance for a context that is being updated in a dynamic fashion as the dialogue proceeds (dynamic context setting). The second task is gear</context>
</contexts>
<marker>Zukerman, Marom, 2006</marker>
<rawString>Ingrid Zukerman and Yuval Marom. 2006. A corpus-based approach to help-desk response generation. In Computational Intelligence for Modelling, Control and Automation (CIMCA 2006), IAWTIC 2006.</rawString>
</citation>
</citationList>
</algorithm>
</algorithms>
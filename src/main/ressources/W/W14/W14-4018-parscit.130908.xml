<?xml version="1.0" encoding="UTF-8"?>
<algorithms version="110505">
<algorithm name="SectLabel" version="110505">
<variant no="0" confidence="0.003284">
<title confidence="0.994516">
Preference Grammars and Soft Syntactic Constraints
for GHKM Syntax-based Statistical Machine Translation
</title>
<author confidence="0.996753">
Matthias Huck and Hieu Hoang and Philipp Koehn
</author>
<affiliation confidence="0.998371">
School of Informatics
University of Edinburgh
</affiliation>
<address confidence="0.994733">
10 Crichton Street
Edinburgh EH8 9AB, UK
</address>
<email confidence="0.999058">
{mhuck,hhoang,pkoehn}@inf.ed.ac.uk
</email>
<sectionHeader confidence="0.997389" genericHeader="abstract">
Abstract
</sectionHeader>
<bodyText confidence="0.999606481481482">
In this work, we investigate the effec-
tiveness of two techniques for a feature-
based integration of syntactic information
into GHKM string-to-tree statistical ma-
chine translation (Galley et al., 2004):
(1.) Preference grammars on the tar-
get language side promote syntactic well-
formedness during decoding while also al-
lowing for derivations that are not linguis-
tically motivated (as in hierarchical trans-
lation). (2.) Soft syntactic constraints aug-
ment the system with additional source-
side syntax features while not modifying
the set of string-to-tree translation rules or
the baseline feature scores.
We conduct experiments with a state-
of-the-art setup on an English→German
translation task. Our results suggest that
preference grammars for GHKM trans-
lation are inferior to the plain target-
syntactified model, whereas the enhance-
ment with soft source syntactic constraints
provides consistent gains. By employ-
ing soft source syntactic constraints with
sparse features, we are able to achieve im-
provements of up to 0.7 points BLEU and
1.0 points TER.
</bodyText>
<sectionHeader confidence="0.999464" genericHeader="keywords">
1 Introduction
</sectionHeader>
<bodyText confidence="0.9984887">
Previous research in both formally syntax-based
(i.e., hierarchical) and linguistically syntax-based
statistical machine translation has demonstrated
that significant quality gains can be achieved via
integration of syntactic information as features in
a non-obtrusive manner, rather than as hard con-
straints.
We implemented two feature-based extensions
for a GHKM-style string-to-tree translation sys-
tem (Galley et al., 2004):
</bodyText>
<listItem confidence="0.9712752">
• Preference grammars to soften the hard
target-side syntactic constraints that are im-
posed by the target non-terminal labels.
• Soft source-side syntactic constraints that
enhance the string-to-tree translation model
</listItem>
<bodyText confidence="0.9862116">
with input tree features based on source syn-
tax labels.
The empirical results on an English→German
translation task are twofold. Target-side prefer-
ence grammars do not show an improvement over
the string-to-tree baseline with syntactified trans-
lation rules. Source-side syntactic constraints, on
the other hand, yield consistent moderate gains if
applied as supplementary features in the string-to-
tree setup.
</bodyText>
<sectionHeader confidence="0.999687" genericHeader="introduction">
2 Outline
</sectionHeader>
<bodyText confidence="0.9994275">
The paper is structured as follows: First we give an
overview of important related publications (Sec-
tion 3). In Section 4, we review the fundamentals
of syntax-based translation in general, and in par-
ticular those of GHKM string-to-tree translation.
We present preference grammars for GHKM
translation in Section 5. Our technique for ap-
plying soft source syntactic constraints in GHKM
string-to-tree translation is described in Section 6.
Section 7 contains the empirical part of the pa-
per. We first describe our experimental setup (7.1),
followed by a presentation and discussion of the
translation results (7.2). We conclude the paper in
Section 8.
</bodyText>
<sectionHeader confidence="0.999987" genericHeader="related work">
3 Related Work
</sectionHeader>
<bodyText confidence="0.9993828">
Our syntactic translation model conforms to the
GHKM syntax approach as proposed by Galley,
Hopkins, Knight, and Marcu (Galley et al., 2004)
with composed rules as in (Galley et al., 2006)
and (DeNeefe et al., 2007). Systems based on
</bodyText>
<page confidence="0.965357">
148
</page>
<note confidence="0.7840595">
Proceedings of SSST-8, Eighth Workshop on Syntax, Semantics and Structure in Statistical Translation, pages 148–156,
October 25, 2014, Doha, Qatar. c�2014 Association for Computational Linguistics
</note>
<bodyText confidence="0.999592333333333">
this paradigm have recently been among the top-
ranked submissions to public evaluation cam-
paigns (Williams et al., 2014; Bojar et al., 2014).
Our soft source syntactic constraints features
borrow ideas from Marton and Resnik (2008) who
proposed a comparable approach for hierarchical
machine translation. The major difference is that
the features of Marton and Resnik (2008) are only
based on the labels from the input trees as seen in
tuning and decoding. They penalize violations of
constituent boundaries but do not employ syntactic
parse annotation of the source side of the training
data. We, in contrast, equip the rules with latent
source label properties, allowing for features that
can check for conformance of input tree labels and
source labels that have been seen in training.
Other groups have applied similar techniques
to a string-to-dependency system (Huang et al.,
2013) and—like in our work—a GHKM string-
to-tree system (Zhang et al., 2011). Both Huang
et al. (2013) and Zhang et al. (2011) store source
labels as additional information with the rules.
They however investigate somewhat different fea-
ture functions than we do.
Marton and Resnik (2008) evaluated their
method on the NIST Chinese→English and
Arabic→English tasks. Huang et al. (2013) and
Zhang et al. (2011) present results on the NIST
Chinese→English task. We focus our attention on
a very different task: English→German.
</bodyText>
<sectionHeader confidence="0.997577" genericHeader="method">
4 Syntax-based Translation
</sectionHeader>
<bodyText confidence="0.99958524137931">
In syntax-based translation, a probabilistic syn-
chronous context-free grammar (SCFG) is in-
duced from bilingual training corpora. The par-
allel training data is word-aligned and annotated
with syntactic parses on either target side (string-
to-tree), source side (tree-to-string), or both (tree-
to-tree). A syntactic rule extraction procedure ex-
tracts rules which are consistent with the word-
alignment and comply with certain syntactic va-
lidity constraints.
Extracted rules are of the form A,B → (α,β,∼ �.
The right-hand side of the rule (α,β) is a bilingual
phrase pair that may contain non-terminal sym-
bols, i.e. α E (VF U NF)+ and β E (VE U NE)+,
where VF and VE denote the source and target
terminal vocabulary, and NF and NE denote the
source and target non-terminal vocabulary, respec-
tively. The non-terminals on the source side and
on the target side of rules are linked in a one-to-
one correspondence. The ∼ relation defines this
one-to-one correspondence. The left-hand side
of the rule is a pair of source and target non-
terminals, A E NF and B E NE.
Decoding is typically carried out with a parsing-
based algorithm, in our case a customized version
of CYK+ (Chappelier and Rajman, 1998). The
parsing algorithm is extended to handle transla-
tion candidates and to incorporate language model
scores via cube pruning (Chiang, 2007).
</bodyText>
<subsectionHeader confidence="0.998222">
4.1 GHKM String-to-Tree Translation
</subsectionHeader>
<bodyText confidence="0.999985527777778">
In GHKM string-to-tree translation (Galley et al.,
2004; Galley et al., 2006; DeNeefe et al., 2007),
rules are extracted from training instances which
consist of a source sentence, a target sentence
along with its constituent parse tree, and a word
alignment matrix. This tuple is interpreted as a
directed graph (the alignment graph), with edges
pointing away from the root of the tree, and word
alignment links being edges as well. A set of
nodes (the frontier set) is determined that con-
tains only nodes with non-overlapping closure of
their spans.1 By computing frontier graph frag-
ments—fragments of the alignment graph such
that their root and all sinks are in the frontier set—
the GHKM extractor is able to induce a minimal
set of rules which explain the training instance.
The internal tree structure can be discarded to ob-
tain flat SCFG rules. Minimal rules can be assem-
bled to build larger composed rules.
Non-terminals on target sides of string-to-tree
rules are syntactified. The target non-terminal vo-
cabulary of the SCFG contains the set of labels of
the frontier nodes, which is in turn a subset of (or
equal to) the set of constituent labels in the parse
tree. The target non-terminal vocabulary further-
more contains an initial non-terminal symbol Q.
Source sides of the rules are not decorated with
syntactic annotation. The source non-terminal vo-
cabulary contains a single generic non-terminal
symbol X.
In addition to the extracted grammar, the trans-
lation system makes use of a special glue grammar
with an initial rule, glue rules, a final rule, and top
rules. The glue rules provide a fall back method
to just monotonically concatenate partial deriva-
tions during decoding. As we add tokens which
</bodyText>
<footnote confidence="0.99215325">
1The span of a node in the alignment graph is defined
as the set of source-side words that are reachable from this
node. The closure of a span is the smallest interval of source
sentence positions that covers the span.
</footnote>
<page confidence="0.998152">
149
</page>
<bodyText confidence="0.922572333333333">
mark the sentence start (“&lt;s&gt;”) and the sentence
end (“&lt;/s&gt;”), the rules in the glue grammar are of
the following form:
</bodyText>
<figure confidence="0.9861685">
Initial rule:
X,Q → h&lt;s&gt; X∼0,&lt;s&gt; Q∼0i
Glue rules:
X,Q → hX∼0X∼1,Q∼0B∼1i
for all B ∈ NE
Final rule:
X,Q → hX∼0 &lt;/s&gt;,Q∼0 &lt;/s&gt;i
Top rules:
X,Q → h&lt;s&gt; X∼0 &lt;/s&gt;,&lt;s&gt; B∼0 &lt;/s&gt;i
for all B ∈ NE
</figure>
<sectionHeader confidence="0.887295" genericHeader="method">
5 Preference Grammars
</sectionHeader>
<bodyText confidence="0.99987284">
Preference grammars store a set of implicit label
vectors as additional information with each SCFG
rule, along with their relative frequencies given
the rule. Venugopal et al. (2009) have introduced
this technique for hierarchical phrase-based trans-
lation. The implicit label set refines the label set
of the underlying synchronous context-free gram-
mar.
We apply this idea to GHKM translation by
not decorating the target-side non-terminals of the
extracted GHKM rules with syntactic labels, but
with a single generic label. The (explicit) tar-
get non-terminal vocabulary NE thus also con-
tains only the generic non-terminal symbol X, just
like the source non-terminal vocabulary NF. The
extraction method remains syntax-directed and is
still guided by the syntactic annotation over the
target side of the data, but the syntactic labels are
stripped off from the SCFG rules. Rules which
differ only with respect to their non-terminal la-
bels are collapsed to a single entry in the rule ta-
ble, and their rule counts are pooled. However,
the syntactic label vectors that have been seen with
this rule during extraction are stored as implicit la-
bel vectors of the rule.
</bodyText>
<subsectionHeader confidence="0.987476">
5.1 Feature Computation
</subsectionHeader>
<bodyText confidence="0.999961388888889">
Two features are added to the log-linear model
combination in order to rate the syntactic well-
formedness of derivations. The first feature is
similar to the one suggested by Venugopal et al.
(2009) and computes a score based on the relative
frequencies of implicit label vectors of those rules
which are involved in the derivation. The second
feature is a simple binary feature which supple-
ments the first one by penalizing a rule application
if none of the implicit label vectors match.
We will now formally specify the first feature.2
We give a recursive definition of the feature score
hsyn(d) for a derivation d.
Let r be the top rule in derivation d, with n
right-hand side non-terminals. Let dj denote the
sub-derivation of d at the j-th right-hand side non-
terminal of r, 1 ≤ j ≤ n. hsyn(d) is recursively
defined as
</bodyText>
<equation confidence="0.990594333333333">
n
hsyn(d) = ˆtsyn(d) + ∑ hsyn(dj). (1)
j=1
</equation>
<bodyText confidence="0.666995">
In this equation, ˆtsyn(d) is a simple auxiliary
function:
</bodyText>
<equation confidence="0.989923">
ˆtsyn(d) = logtsyn(d) if tsyn(d) =6 0 (2)
0 otherwise
</equation>
<bodyText confidence="0.9982102">
Denoting with S the implicit label set of the
preference grammar, we define tsyn(d) as a func-
tion that assesses the degree of agreement of
the preferences of the current rule with the sub-
derivations:
</bodyText>
<equation confidence="0.997368">
�ˆth(s[k]|dk−1) (3)
</equation>
<bodyText confidence="0.999933285714286">
We use the notation [·] to address the elements of a
vector. The first element of an n + 1-dimensional
vector s of implicit labels is an implicit label bind-
ing of the left-hand side non-terminal of the rule r.
p(s|r) is the preference distribution of the rule.
Here, ˆth(Y|d) is another auxiliary function that
renormalizes the values of th(Y|d):
</bodyText>
<equation confidence="0.996507">
th(Y|d)
ˆth(Y|d) = (4)
∑Y0∈Sth(Y0|d)
</equation>
<bodyText confidence="0.999965">
It provides us with a probability that the derivation
d has the implicit label Y ∈ S as its root. Finally,
the function th(Y|d) is defined as
</bodyText>
<equation confidence="0.973841666666667">
th(Y|d) =
�ph(s[k]|dk−1) .
(5)
</equation>
<bodyText confidence="0.66729925">
Note that the denominator in Equation (4) thus
equals tsyn(d).
2Our notational conventions roughly follow the ones by
Stein et al. (2010).
</bodyText>
<figure confidence="0.946612833333333">
tsyn(d) = ∑ p(s|r) · n+1
s∈Sn+1 ∏
k=2
∑ p(s|r) · n+1
s∈Sn+1:s[1]=Y ∏
k=2
</figure>
<page confidence="0.980659">
150
</page>
<bodyText confidence="0.99904">
This concludes the formal specification of the
first features. The second feature hauxSyn(d) penal-
izes rule applications in cases where tsyn(d) evalu-
ates to 0:
</bodyText>
<equation confidence="0.994107">
hauxSyn (d) = 0 if tsyn(d) 74 0
(6)
1 otherwise
</equation>
<bodyText confidence="0.993895714285714">
Its intuition is that rule applications that do not
contribute to hsyn(d) should be punished. Deriva-
tions with tsyn(d) = 0 could alternatively be
dropped completely, but our approach is to avoid
hard constraints. We will later demonstrate empir-
ically that discarding such derivations harms trans-
lation quality.
</bodyText>
<sectionHeader confidence="0.985703" genericHeader="method">
6 Soft Source Syntactic Constraints
</sectionHeader>
<bodyText confidence="0.988921029411765">
Similar to the implicit target-side label vectors
which we store in preference grammars, we can
likewise memorize sets of source-side syntactic la-
bel vectors with GHKM rules. In contrast to pref-
erence grammars, the rule inventory of the string-
to-tree system remains untouched. The target non-
terminals of the SCFG stay syntactified, and the
source non-terminal vocabulary is not extended
beyond the single generic non-terminal.
Source-side syntactic labels are an additional la-
tent property of the rules. We obtain this property
by parsing the source side of the training data and
collecting the source labels that cover the source-
side span of non-terminals during GHKM rule ex-
traction. As the source-side span is frequently not
covered by a constituent in the syntactic parse tree,
we employ the composite symbols as suggested
by Zollmann and Venugopal (2006) for the SAMT
system.3 In cases where a span is still not covered
by a symbol, we nevertheless memorize a source-
side syntactic label vector but indicate the failure
for the uncovered non-terminal with a special la-
bel. The set of source label vectors that are seen
with a rule during extraction is stored with it in the
rule table as an additional property. This informa-
tion can be used to implement feature-based soft
source syntactic constraints.
Table 1 shows an example of a set of source
label vectors stored with a grammar rule. The
first element of each vector is an implicit source-
syntactic label for the left-hand side non-terminal
of the rule, the remaining elements are implicit
3Specifically, we apply relax-parse --SAMT 2 as
implemented in the Moses toolkit (Koehn et al., 2007).
</bodyText>
<table confidence="0.956477166666667">
source label vector frequency
(IN+NP,NN,NN) 7
(IN+NP,NNP,NNP) 3
(IN++NP,NNS,NNS) 2
(IN+NP,NP,NP) 2
(PP//SBAR,NP,NP) 1
</table>
<tableCaption confidence="0.998492">
Table 1: The set of source label vec-
</tableCaption>
<bodyText confidence="0.988851272727273">
tors (along with their frequencies in the
training data) for the rule X,PP-MO —*
(between X-1 and X-0,zwischen NN-0 und NN-1).
The overall rule frequency is 15.
source-syntactic labels for the right-hand side
source non-terminals.
The basic idea for soft source syntactic con-
straints features is to also parse the input data in
a preprocessing step and try to match input labels
and source label vectors that are associated with
SCFG rules.
</bodyText>
<subsectionHeader confidence="0.995327">
6.1 Feature Computation
</subsectionHeader>
<bodyText confidence="0.9999861">
Upon application of an SCFG rule, each of the
non-terminals of the rule covers a distinct span of
the input sentence. An input label from the input
parse may be available for this span. We say that
a non-terminal has a match in a given source la-
bel vector of the rule if its label in the vector is the
same as a corresponding input label over the span.
We define three simple features to score
matches and mismatches of the impicit source syn-
tactic labels with the labels from the input data:
</bodyText>
<listItem confidence="0.967729214285714">
• A binary feature that fires if a rule is applied
which possesses a source syntactic label vec-
tor that fully matches the input labels. This
feature rewards exact source label matches of
complete rules, i.e., the existance of a vector
in which all non-terminals of the rule have
matches.
• A binary feature that fires if a rule is applied
which does not possess any source syntactic
label vector with a match of the label for the
left-hand side non-terminal. This feature pe-
nalizes left-hand side mismatches.
• A count feature that for each rule application
adds a cost equal to the number of right-hand
</listItem>
<bodyText confidence="0.567149">
side non-terminals that do not have a match
with a corresponding input label in any of the
source syntactic label vectors. This feature
penalizes right-hand side mismatches.
</bodyText>
<page confidence="0.994139">
151
</page>
<bodyText confidence="0.999927666666667">
The second and third feature are less strict than the
first one and give the system a more detailed clue
about the magnitude of mismatch.
</bodyText>
<subsectionHeader confidence="0.99936">
6.2 Sparse Features
</subsectionHeader>
<bodyText confidence="0.999612666666667">
We can optionally add a larger number of sparse
features that depend on the identity of the source-
side syntactic label:
</bodyText>
<listItem confidence="0.989833083333333">
• Sparse features which fire if a specific input
label is matched. We say that the input la-
bel is matched in case the corresponding non-
terminal that covers the span has a match in
any of the source syntactic label vectors of
the applied rule. We distinguish input label
matches via left-hand side and via right-hand
side non-terminals.
• Sparse features which fire if the span of a spe-
cific input label is covered by a non-terminal
of an applied rule, but the input label is not
matched.
</listItem>
<bodyText confidence="0.99986575">
The first set of sparse features rewards matches,
the second set of sparse features penalizes mis-
matches.
All sparse features have individual scaling fac-
tors in the log-linear model combination. We how-
ever implemented a means of restricting the num-
ber of sparse features by providing a core set of
source labels. If such a core set is specified, then
only those sparse features are active that depend
on the identity of labels within this set. All sparse
features for source labels outside of the core set
are inactive.
</bodyText>
<sectionHeader confidence="0.999626" genericHeader="evaluation">
7 Experiments
</sectionHeader>
<bodyText confidence="0.999850818181818">
We empirically evaluate the effectiveness of
preference grammars and soft source syntac-
tic constraints for GHKM translation on the
English→German language pair using the stan-
dard newstest sets of the Workshop on Statisti-
cal Machine Translation (WMT) for testing.4 The
experiments are conducted with the open-source
Moses implementations of GHKM rule extraction
(Williams and Koehn, 2012) and decoding with
CYK+ parsing and cube pruning (Hoang et al.,
2009).
</bodyText>
<footnote confidence="0.96409">
4http://www.statmt.org/wmt14/
translation-task.html
</footnote>
<subsectionHeader confidence="0.976145">
7.1 Experimental Setup
</subsectionHeader>
<bodyText confidence="0.99997094">
We work with an English–German parallel train-
ing corpus of around 4.5 M sentence pairs (af-
ter corpus cleaning). The parallel data origi-
nates from three different sources which have
been eligible for the constrained track of the
ACL 2014 Ninth Workshop on Statistical Ma-
chine Translation shared translation task: Europarl
(Koehn, 2005), News Commentary, and the Com-
mon Crawl corpus as provided on the WMT web-
site. Word alignments are created by aligning the
data in both directions with MGIZA++ (Gao and
Vogel, 2008) and symmetrizing the two trained
alignments (Och and Ney, 2003; Koehn et al.,
2003). The German target side training data is
parsed with BitPar (Schmid, 2004). We remove
grammatical case and function information from
the annotation obtained with BitPar and apply
right binarization of the German parse trees prior
to rule extraction (Wang et al., 2007; Wang et al.,
2010; Nadejde et al., 2013). For the soft source
syntactic constraints, we parse the English source
side of the parallel data with the English Berkeley
Parser (Petrov et al., 2006) and produce composite
SAMT-style labels as discussed in Section 6.
When extracting syntactic rules, we impose sev-
eral restrictions for composed rules, in particular
a maximum number of 100 tree nodes per rule,
a maximum depth of seven, and a maximum size
of seven. We discard rules with non-terminals on
their right-hand side if they are singletons in the
training data.
For efficiency reasons, we also enforce a limit
on the number of label vectors that are stored
as additional properties. Label vectors are only
stored if they occur at least as often as the 50th
most frequent label vector of the given rule. This
limit is applied separately for both source-side la-
bel vectors (which are used by the soft syntactic
contraints) and target-side label vectors (which are
used by the preference grammar).
Only the 200 best translation options per dis-
tinct rule source side with respect to the weighted
rule-level model scores are loaded by the decoder.
Search is carried out with a maximum chart span
of 25, a rule limit of 500, a stack limit of 200, and
a k-best limit of 1000 for cube pruning.
A standard set of models is used in the base-
line, comprising rule translation probabilities and
lexical translation probabilities in both directions,
word penalty and rule penalty, an n-gram language
</bodyText>
<page confidence="0.995174">
152
</page>
<table confidence="0.999748307692308">
system dev TER newstest2013 TER newstest2014 TER
BLEU BLEU BLEU
GHKM string-to-tree baseline 34.7 47.3 20.0 63.3 19.4 65.6
+ soft source syntactic constraints 35.1 47.0 20.3 62.7 19.7 64.9
+ sparse features 35.8 46.5 20.3 62.8 19.6 65.1
+ sparse features (core = non-composite) 35.4 46.8 20.2 62.9 19.6 65.1
+ sparse features (core = dev-min-occ100) 35.6 46.7 20.2 62.9 19.6 65.2
+ sparse features (core = dev-min-occ1000) 35.4 46.9 20.3 62.8 19.6 65.2
+ hard source syntactic constraints 34.6 47.4 19.9 63.4 19.4 65.6
string-to-string (GHKM syntax-directed rule extraction) 33.8 48.0 19.3 63.8 18.7 66.2
+ preference grammar 33.9 47.7 19.3 63.7 18.8 66.0
+ soft source syntactic constraints 34.6 47.0 19.8 62.9 19.5 65.2
+ drop derivations with tsyn(d) = 0 34.0 47.5 19.7 63.0 18.8 65.8
</table>
<tableCaption confidence="0.9509135">
Table 2: English→German experimental results (truecase). BLEU scores are given in percentage.
A selection of 2000 sentences from the newstest2008-2012 sets is used as development set.
</tableCaption>
<bodyText confidence="0.999610307692308">
model, a rule rareness penalty, and the monolin-
gual PCFG probability of the tree fragment from
which the rule was extracted (Williams et al.,
2014). Rule translation probabilities are smoothed
via Good-Turing smoothing.
The language model (LM) is a large inter-
polated 5-gram LM with modified Kneser-Ney
smoothing (Kneser and Ney, 1995; Chen and
Goodman, 1998). The target side of the parallel
corpus and the monolingual German News Crawl
corpora are employed as training data. We use
the SRILM toolkit (Stolcke, 2002) to train the LM
and rely on KenLM (Heafield, 2011) for language
model scoring during decoding.
Model weights are optimized to maximize
BLEU (Papineni et al., 2002) with batch MIRA
(Cherry and Foster, 2012) on 1000-best lists. We
selected 2000 sentences from the newstest2008-
2012 sets as a development set. The selected sen-
tences obtained high sentence-level BLEU scores
when being translated with a baseline phrase-
based system, and do each contain less than
30 words for more rapid tuning. newstest2013 and
newstest2014 are used as unseen test sets. Trans-
lation quality is measured in truecase with BLEU
and TER (Snover et al., 2006).5
</bodyText>
<subsectionHeader confidence="0.998317">
7.2 Translation Results
</subsectionHeader>
<bodyText confidence="0.98317925">
The results of the empirical evaluation are given in
Table 2. Our GHKM string-to-tree system attains
state-of-the-art performance on newstest2013 and
newstest2014.
</bodyText>
<footnote confidence="0.9504535">
5TER scores are computed with tercom version 0.7.25
and parameters -N -s.
</footnote>
<subsubsectionHeader confidence="0.67106">
7.2.1 Soft Source Syntactic Constraints
</subsubsectionHeader>
<bodyText confidence="0.99999106060606">
Adding the three dense soft source syntactic con-
straints features from Section 6.1 improves the
baseline scores by 0.3 points BLEU and 0.6 points
TER on newstest2013 and by 0.3 points BLEU and
0.7 points TER on newstest2014.
Somewhat surprisingly, the sparse features from
Section 6.2 do not boost translation quality further
on any of the two test sets. We observe a consid-
erable improvement on the development set, but it
does not carry over to the test sets. We attributed
this to an overfitting effect. Our source-side soft
syntactic label set of composite SAMT-style la-
bels comprises 8504 different labels that appear on
the source-side of the parallel training data. Four
times the amount of sparse features are possible
(left-hand side/right-hand side matches and mis-
matches for each label), though not all of them fire
on the development set. 3989 sparse weights are
tuned to non-zero values in the experiment. Due to
the sparse nature of the features, overfitting cannot
be ruled out.
We attempted to take measures in order to avoid
overfitting by specifying a core set of source la-
bels and deactivating all sparse features for source
labels outside of the core set (cf. Section 6.2).
First we specified the core label set as all non-
composite labels. Non-composite labels are the
plain constituent labels as given by the syntactic
parser. Complex SAMT-style labels are not in-
cluded. The size of this set is 71 (non-composite
labels that have been observed during rule extrac-
tion). Translation performance on the develop-
ment set drops in the sparse features (core = non-
</bodyText>
<page confidence="0.99706">
153
</page>
<table confidence="0.999589428571429">
system (tuned on newstest2012) newstest2012 TER newstest2013 TER newstest2014 TER
BLEU BLEU BLEU
GHKM string-to-tree baseline 17.9 65.7 19.9 63.2 19.4 65.3
+ soft source syntactic constraints 18.2 65.3 20.3 62.6 19.7 64.7
+ sparse features 18.6 64.9 20.4 62.5 19.8 64.7
+ sparse features (core = non-composite) 18.4 65.1 20.3 62.7 19.8 64.7
+ sparse features (core = dev-min-occ100) 18.4 64.8 20.6 62.2 19.9 64.4
</table>
<tableCaption confidence="0.9785425">
Table 3: English→German experimental results (truecase). BLEU scores are given in percentage.
newstest2012 is used as development set.
</tableCaption>
<bodyText confidence="0.989854153846154">
composite) setup, but performance does not in-
crease on the test sets.
Next we specified the core label set in another
way: We counted how often each source label oc-
curs in the input data on the development set. We
then applied a minimum occurrence count thresh-
old and added labels to the core set if they did not
appear more rarely than the threshold. We tried
values of 100 and 1000 for the minimum occur-
rence, resulting in 277 and 37 labels being in the
core label set, respectively. Neither the sparse fea-
tures (core = dev-min-occ100) experiment nor the
sparse features (core = dev-min-occ1000) experi-
ment yields better translation quality than what we
see in the setup without sparse features.
We eventually conjectured that the choice of our
development set might be a reason for the ineffec-
tiveness of the sparse features, as on a fine-grained
level it could possibly be too different from the
test sets with respect to its syntactic properties.
We therefore repeated some of the experiments
with scaling factors optimized on newstest2012
(Table 3). The sparse features (core = dev-min-
occ100) setup indeed performs better when tuned
on newstest2012, with improvements of 0.7 points
BLEU and 1.0 points TER on newstest2013 and
of 0.5 points BLEU and 0.9 points TER on news-
test2014 over the baseline tuned on the same set.
Finally, we were interested in demonstrating
that soft source syntactic constraints are superior
to hard source syntactic constraints. We built a
setup that forces the decoder to match source-side
syntactic label vectors in the rules with input la-
bels.6 Hard source syntactic constraints are in-
deed worse than soft source syntactic constraints
(by 0.4 BLEU on newstest2013 and 0.3 BLEU on
newstest2014). The setup with hard source syntac-
tic constraints performs almost exactly at the level
of the baseline.
</bodyText>
<footnote confidence="0.968603">
6Glue rules are an exception. They do not need to match
the input labels.
</footnote>
<subsectionHeader confidence="0.803401">
7.2.2 Preference Grammar
</subsectionHeader>
<bodyText confidence="0.999985358974359">
In the series of experiments with a preference
grammar, we first evaluated a setup with the un-
derlying SCFG of the preference grammar sys-
tem, but without preference grammar. We de-
note this setup as string-to-string (GHKM syntax-
directed rule extraction) in Table 2. The ex-
traction method for this string-to-string system is
GHKM syntax-directed with right-binarized syn-
tactic target-side parses from BitPar, as in the
string-to-tree setup. The constituent labels from
the syntactic parses are however not used to dec-
orate non-terminals. The grammar contains rules
with a single generic non-terminal instead of syn-
tactic ones. The string-to-string (GHKM syntax-
directed rule extraction) setup is on newstest2013
0.7 BLEU (0.5 TER) worse and on newstest2014
0.7 BLEU (0.6 TER) worse than the standard
GHKM string-to-tree baseline.
We then activated the preference grammar as
described in Section 5. GHKM translation with a
preference grammar instead of a syntactified target
non-terminal vocabulary in the SCFG is consider-
ably worse than the standard GHKM string-to-tree
baseline and barely improves over the string-to-
string setup.
We added soft source syntactic constraints on
top of the preference grammar system, thus com-
bining the two techniques. Soft source syntactic
constraints give a nice gain over the preference
grammar system, but the best setup without a pref-
erence grammar is not outperformed. In another
experiment, we investigated the effect of dropping
derivations with tsyn(d) = 0 (cf. Section 5.1). Note
that the second feature hauxSyn(d) is not useful in
this setup, as the system is forced to discard all
derivations that would be penalized by that fea-
ture. We deactivated hauxSyn(d) for the experi-
ment. The hard decision of dropping derivations
with tsyn(d) = 0 leads to a performance loss of
</bodyText>
<page confidence="0.999309">
154
</page>
<table confidence="0.9164066">
0.1 BLEU on newstest2013 and a more severe de-
terioration of 0.7 BLEU on newstest2014.
David Chiang. 2007. Hierarchical Phrase-Based
Translation. Computational Linguistics, 33(2):201–
228, June.
</table>
<sectionHeader confidence="0.997173" genericHeader="conclusions">
8 Conclusions
</sectionHeader>
<bodyText confidence="0.9999386875">
We investigated two soft syntactic extensions for
GHKM translation: Target-side preference gram-
mars and soft source syntactic constraints.
Soft source syntactic constraints proved to be
suitable for advancing the translation quality over
a strong string-to-tree baseline. Sparse features
are beneficial beyond just three dense features, but
they require the utilization of an appropriate devel-
opment set. We also showed that the soft integra-
tion of source syntactic constraints is crucial: Hard
constraints do not yield gains over the baseline.
Preference grammars did not perform well in
our experiments, suggesting that translation mod-
els with syntactic target non-terminal vocabular-
ies are a better choice when building string-to-tree
systems.
</bodyText>
<sectionHeader confidence="0.996921" genericHeader="acknowledgments">
Acknowledgements
</sectionHeader>
<bodyText confidence="0.9999636">
The research leading to these results has re-
ceived funding from the European Union Sev-
enth Framework Programme (FP7/2007-2013) un-
der grant agreements no 287658 (EU-BRIDGE)
and no 288487 (MosesCore).
</bodyText>
<sectionHeader confidence="0.999154" genericHeader="references">
References
</sectionHeader>
<reference confidence="0.999825922077922">
Ondrej Bojar, Christian Buck, Christian Federmann,
Barry Haddow, Philipp Koehn, Johannes Leveling,
Christof Monz, Pavel Pecina, Matt Post, Herve
Saint-Amand, Radu Soricut, Lucia Specia, and Aleš
Tamchyna. 2014. Findings of the 2014 Work-
shop on Statistical Machine Translation. In Proc. of
the Workshop on Statistical Machine Translation
(WMT), pages 12–58, Baltimore, MD, USA, June.
Jean-Cédric Chappelier and Martin Rajman. 1998. A
Generalized CYK Algorithm for Parsing Stochas-
tic CFG. In Proc. of the First Workshop on Tab-
ulation in Parsing and Deduction, pages 133–137,
Paris, France, April.
Stanley F. Chen and Joshua Goodman. 1998. An
Empirical Study of Smoothing Techniques for Lan-
guage Modeling. Technical Report TR-10-98, Com-
puter Science Group, Harvard University, Cam-
bridge, MA, USA, August.
Colin Cherry and George Foster. 2012. Batch Tun-
ing Strategies for Statistical Machine Translation. In
Proc. of the Human Language Technology Conf. /
North American Chapter of the Assoc. for Compu-
tational Linguistics (HLT-NAACL), pages 427–436,
Montréal, Canada, June.
Steve DeNeefe, Kevin Knight, Wei Wang, and Daniel
Marcu. 2007. What Can Syntax-Based MT Learn
from Phrase-Based MT? In Proc. of the 2007
Joint Conf. on Empirical Methods in Natural Lan-
guage Processing and Computational Natural Lan-
guage Learning (EMNLP-CoNLL), pages 755–763,
Prague, Czech Republic, June.
Michel Galley, Mark Hopkins, Kevin Knight, and
Daniel Marcu. 2004. What’s in a translation rule?
In Proc. of the Human Language Technology Conf.
/North American Chapter of the Assoc. for Compu-
tational Linguistics (HLT-NAACL), pages 273–280,
Boston, MA, USA, May.
Michel Galley, Jonathan Graehl, Kevin Knight, Daniel
Marcu, Steve DeNeefe, Wei Wang, and Ignacio
Thayer. 2006. Scalable Inference and Training
of Context-Rich Syntactic Translation Models. In
Proc. of the 21st Int. Conf. on Computational Lin-
guistics and 44th Annual Meeting of the Assoc. for
Computational Linguistics, pages 961–968, Sydney,
Australia, July.
Qin Gao and Stephan Vogel. 2008. Parallel Implemen-
tations of Word Alignment Tool. In Software Engi-
neering, Testing, and Quality Assurance for Natural
Language Processing, SETQA-NLP ’08, pages 49–
57, Columbus, OH, USA, June.
Kenneth Heafield. 2011. KenLM: Faster and Smaller
Language Model Queries. In Proc. of the Workshop
on Statistical Machine Translation (WMT), pages
187–197, Edinburgh, Scotland, UK, July.
Hieu Hoang, Philipp Koehn, and Adam Lopez. 2009.
A Unified Framework for Phrase-Based, Hierarchi-
cal, and Syntax-Based Statistical Machine Transla-
tion. In Proc. of the Int. Workshop on Spoken Lan-
guage Translation (IWSLT), pages 152–159, Tokyo,
Japan, December.
Zhongqiang Huang, Jacob Devlin, and Rabih Zbib.
2013. Factored Soft Source Syntactic Constraints
for Hierarchical Machine Translation. In Proc. of
the Conf. on Empirical Methods for Natural Lan-
guage Processing (EMNLP), pages 556–566, Seat-
tle, WA, USA, October.
Reinhard Kneser and Hermann Ney. 1995. Improved
Backing-Off for M-gram Language Modeling. In
Proceedings of the Int. Conf. on Acoustics, Speech,
and Signal Processing, volume 1, pages 181–184,
Detroit, MI, USA, May.
Philipp Koehn, Franz Joseph Och, and Daniel Marcu.
2003. Statistical Phrase-Based Translation. In Proc.
of the Human Language Technology Conf. / North
American Chapter of the Assoc. for Computational
Linguistics (HLT-NAACL), pages 127–133, Edmon-
ton, Canada, May/June.
</reference>
<page confidence="0.986754">
155
</page>
<reference confidence="0.999788303030303">
P. Koehn, H. Hoang, A. Birch, C. Callison-Burch,
M. Federico, N. Bertoldi, B. Cowan, W. Shen,
C. Moran, R. Zens, C. Dyer, O. Bojar, A. Con-
stantin, and E. Herbst. 2007. Moses: Open Source
Toolkit for Statistical Machine Translation. In Proc.
of the Annual Meeting of the Assoc. for Computa-
tional Linguistics (ACL), pages 177–180, Prague,
Czech Republic, June.
Philipp Koehn. 2005. Europarl: A parallel corpus for
statistical machine translation. In Proc. of the MT
Summit X, Phuket, Thailand, September.
Yuval Marton and Philip Resnik. 2008. Soft Syn-
tactic Constraints for Hierarchical Phrased-Based
Translation. In Proc. of the Annual Meeting of the
Assoc. for Computational Linguistics (ACL), pages
1003–1011, Columbus, OH, USA, June.
Maria Nadejde, Philip Williams, and Philipp Koehn.
2013. Edinburgh’s Syntax-Based Machine Transla-
tion Systems. In Proc. of the Workshop on Statistical
Machine Translation (WMT), pages 170–176, Sofia,
Bulgaria, August.
Franz Josef Och and Hermann Ney. 2003. A System-
atic Comparison of Various Statistical Alignment
Models. Computational Linguistics, 29(1):19–51,
March.
Kishore Papineni, Salim Roukos, Todd Ward, and Wei-
Jing Zhu. 2002. Bleu: a Method for Automatic
Evaluation of Machine Translation. In Proc. of the
Annual Meeting of the Assoc. for Computational
Linguistics (ACL), pages 311–318, Philadelphia, PA,
USA, July.
Slav Petrov, Leon Barrett, Romain Thibaux, and Dan
Klein. 2006. Learning Accurate, Compact, and In-
terpretable Tree Annotation. In Proc. of the 21st Int.
Conf. on Computational Linguistics and 44th An-
nual Meeting of the Assoc. for Computational Lin-
guistics, pages 433–440, Sydney, Australia, July.
Helmut Schmid. 2004. Efficient Parsing of Highly
Ambiguous Context-Free Grammars with Bit Vec-
tors. In Proc. of the Int. Conf. on Computational
Linguistics (COLING), Geneva, Switzerland, Au-
gust.
Matthew Snover, Bonnie Dorr, Richard Schwartz, Lin-
nea Micciulla, and John Makhoul. 2006. A Study
of Translation Edit Rate with Targeted Human An-
notation. In Proc. of the Conf. of the Assoc. for
Machine Translation in the Americas (AMTA), pages
223–231, Cambridge, MA, USA, August.
Daniel Stein, Stephan Peitz, David Vilar, and Hermann
Ney. 2010. A Cocktail of Deep Syntactic Fea-
tures for Hierarchical Machine Translation. In Proc.
of the Conf. of the Assoc. for Machine Translation
in the Americas (AMTA), Denver, CO, USA, Octo-
ber/November.
Andreas Stolcke. 2002. SRILM – an Extensible Lan-
guage Modeling Toolkit. In Proc. of the Int. Conf.
on Spoken Language Processing (ICSLP), volume 3,
Denver, CO, USA, September.
Ashish Venugopal, Andreas Zollmann, Noah A. Smith,
and Stephan Vogel. 2009. Preference Grammars:
Softening Syntactic Constraints to Improve Statis-
tical Machine Translation. In Proc. of the Hu-
man Language Technology Conf. /North American
Chapter of the Assoc. for Computational Linguistics
(HLT-NAACL), pages 236–244, Boulder, CO, USA,
June.
Wei Wang, Kevin Knight, and Daniel Marcu. 2007.
Binarizing Syntax Trees to Improve Syntax-Based
Machine Translation Accuracy. In Proc. of the 2007
Joint Conf. on Empirical Methods in Natural Lan-
guage Processing and Computational Natural Lan-
guage Learning (EMNLP-CoNLL), pages 746–754,
Prague, Czech Republic, June.
Wei Wang, Jonathan May, Kevin Knight, and Daniel
Marcu. 2010. Re-structuring, Re-labeling, and
Re-aligning for Syntax-based Machine Translation.
Computational Linguistics, 36(2):247–277, June.
Philip Williams and Philipp Koehn. 2012. GHKM
Rule Extraction and Scope-3 Parsing in Moses. In
Proc. of the Workshop on Statistical Machine Trans-
lation (WMT), pages 388–394, Montréal, Canada,
June.
Philip Williams, Rico Sennrich, Maria Nadejde,
Matthias Huck, Eva Hasler, and Philipp Koehn.
2014. Edinburgh’s Syntax-Based Systems at
WMT 2014. In Proc. of the Workshop on Statis-
tical Machine Translation (WMT), pages 207–214,
Baltimore, MD, USA, June.
Jiajun Zhang, Feifei Zhai, and Chengqing Zong. 2011.
Augmenting String-to-Tree Translation Models with
Fuzzy Use of Source-side Syntax. In Proc. of the
Conf. on Empirical Methods for Natural Language
Processing (EMNLP), pages 204–215, Edinburgh,
Scotland, UK, July.
Andreas Zollmann and Ashish Venugopal. 2006. Syn-
tax Augmented Machine Translation via Chart Pars-
ing. In Proc. of the Workshop on Statistical Machine
Translation (WMT), pages 138–141, New York City,
NY, USA, June.
</reference>
<page confidence="0.998785">
156
</page>
</variant>
</algorithm>
<algorithm name="ParsHed" version="110505">
<variant no="0" confidence="0.265088">
<title confidence="0.9992975">Preference Grammars and Soft Syntactic for GHKM Syntax-based Statistical Machine Translation</title>
<author confidence="0.999861">Huck Hoang</author>
<affiliation confidence="0.9972945">School of University of</affiliation>
<address confidence="0.652901">10 Crichton Edinburgh EH8 9AB,</address>
<email confidence="0.995363">mhuck@inf.ed.ac.uk</email>
<email confidence="0.995363">hhoang@inf.ed.ac.uk</email>
<email confidence="0.995363">pkoehn@inf.ed.ac.uk</email>
<abstract confidence="0.998544333333333">In this work, we investigate the effectiveness of two techniques for a featurebased integration of syntactic information into GHKM string-to-tree statistical machine translation (Galley et al., 2004): grammars the target language side promote syntactic wellformedness during decoding while also allowing for derivations that are not linguistically motivated (as in hierarchical transsyntactic constraints augment the system with additional sourceside syntax features while not modifying the set of string-to-tree translation rules or the baseline feature scores. We conduct experiments with a statesetup on an translation task. Our results suggest that preference grammars for GHKM translation are inferior to the plain targetsyntactified model, whereas the enhancement with soft source syntactic constraints provides consistent gains. By employing soft source syntactic constraints with sparse features, we are able to achieve imof up to 0.7 points</abstract>
<intro confidence="0.579602">points</intro>
</variant>
</algorithm>
<algorithm name="ParsCit" version="110505">
<citationList>
<citation valid="true">
<authors>
<author>Ondrej Bojar</author>
<author>Christian Buck</author>
<author>Christian Federmann</author>
<author>Barry Haddow</author>
<author>Philipp Koehn</author>
<author>Johannes Leveling</author>
<author>Christof Monz</author>
<author>Pavel Pecina</author>
<author>Matt Post</author>
</authors>
<title>Herve Saint-Amand, Radu Soricut, Lucia Specia, and Aleš Tamchyna.</title>
<date>2014</date>
<booktitle>Findings of the 2014 Workshop on Statistical Machine Translation. In Proc. of the Workshop on Statistical Machine Translation (WMT),</booktitle>
<pages>12--58</pages>
<location>Baltimore, MD, USA,</location>
<contexts>
<context position="3653" citStr="Bojar et al., 2014" startWordPosition="533" endWordPosition="536">clude the paper in Section 8. 3 Related Work Our syntactic translation model conforms to the GHKM syntax approach as proposed by Galley, Hopkins, Knight, and Marcu (Galley et al., 2004) with composed rules as in (Galley et al., 2006) and (DeNeefe et al., 2007). Systems based on 148 Proceedings of SSST-8, Eighth Workshop on Syntax, Semantics and Structure in Statistical Translation, pages 148–156, October 25, 2014, Doha, Qatar. c�2014 Association for Computational Linguistics this paradigm have recently been among the topranked submissions to public evaluation campaigns (Williams et al., 2014; Bojar et al., 2014). Our soft source syntactic constraints features borrow ideas from Marton and Resnik (2008) who proposed a comparable approach for hierarchical machine translation. The major difference is that the features of Marton and Resnik (2008) are only based on the labels from the input trees as seen in tuning and decoding. They penalize violations of constituent boundaries but do not employ syntactic parse annotation of the source side of the training data. We, in contrast, equip the rules with latent source label properties, allowing for features that can check for conformance of input tree labels an</context>
</contexts>
<marker>Bojar, Buck, Federmann, Haddow, Koehn, Leveling, Monz, Pecina, Post, 2014</marker>
<rawString>Ondrej Bojar, Christian Buck, Christian Federmann, Barry Haddow, Philipp Koehn, Johannes Leveling, Christof Monz, Pavel Pecina, Matt Post, Herve Saint-Amand, Radu Soricut, Lucia Specia, and Aleš Tamchyna. 2014. Findings of the 2014 Workshop on Statistical Machine Translation. In Proc. of the Workshop on Statistical Machine Translation (WMT), pages 12–58, Baltimore, MD, USA, June.</rawString>
</citation>
<citation valid="true">
<authors>
<author>Jean-Cédric Chappelier</author>
<author>Martin Rajman</author>
</authors>
<title>A Generalized CYK Algorithm for Parsing Stochastic CFG.</title>
<date>1998</date>
<booktitle>In Proc. of the First Workshop on Tabulation in Parsing and Deduction,</booktitle>
<pages>133--137</pages>
<location>Paris, France,</location>
<contexts>
<context position="6134" citStr="Chappelier and Rajman, 1998" startWordPosition="935" endWordPosition="938">pair that may contain non-terminal symbols, i.e. α E (VF U NF)+ and β E (VE U NE)+, where VF and VE denote the source and target terminal vocabulary, and NF and NE denote the source and target non-terminal vocabulary, respectively. The non-terminals on the source side and on the target side of rules are linked in a one-toone correspondence. The ∼ relation defines this one-to-one correspondence. The left-hand side of the rule is a pair of source and target nonterminals, A E NF and B E NE. Decoding is typically carried out with a parsingbased algorithm, in our case a customized version of CYK+ (Chappelier and Rajman, 1998). The parsing algorithm is extended to handle translation candidates and to incorporate language model scores via cube pruning (Chiang, 2007). 4.1 GHKM String-to-Tree Translation In GHKM string-to-tree translation (Galley et al., 2004; Galley et al., 2006; DeNeefe et al., 2007), rules are extracted from training instances which consist of a source sentence, a target sentence along with its constituent parse tree, and a word alignment matrix. This tuple is interpreted as a directed graph (the alignment graph), with edges pointing away from the root of the tree, and word alignment links being ed</context>
</contexts>
<marker>Chappelier, Rajman, 1998</marker>
<rawString>Jean-Cédric Chappelier and Martin Rajman. 1998. A Generalized CYK Algorithm for Parsing Stochastic CFG. In Proc. of the First Workshop on Tabulation in Parsing and Deduction, pages 133–137, Paris, France, April.</rawString>
</citation>
<citation valid="true">
<authors>
<author>Stanley F Chen</author>
<author>Joshua Goodman</author>
</authors>
<title>An Empirical Study of Smoothing Techniques for Language Modeling.</title>
<date>1998</date>
<tech>Technical Report TR-10-98,</tech>
<institution>Computer Science Group, Harvard University,</institution>
<location>Cambridge, MA, USA,</location>
<contexts>
<context position="21367" citStr="Chen and Goodman, 1998" startWordPosition="3474" endWordPosition="3477">2.9 19.5 65.2 + drop derivations with tsyn(d) = 0 34.0 47.5 19.7 63.0 18.8 65.8 Table 2: English→German experimental results (truecase). BLEU scores are given in percentage. A selection of 2000 sentences from the newstest2008-2012 sets is used as development set. model, a rule rareness penalty, and the monolingual PCFG probability of the tree fragment from which the rule was extracted (Williams et al., 2014). Rule translation probabilities are smoothed via Good-Turing smoothing. The language model (LM) is a large interpolated 5-gram LM with modified Kneser-Ney smoothing (Kneser and Ney, 1995; Chen and Goodman, 1998). The target side of the parallel corpus and the monolingual German News Crawl corpora are employed as training data. We use the SRILM toolkit (Stolcke, 2002) to train the LM and rely on KenLM (Heafield, 2011) for language model scoring during decoding. Model weights are optimized to maximize BLEU (Papineni et al., 2002) with batch MIRA (Cherry and Foster, 2012) on 1000-best lists. We selected 2000 sentences from the newstest2008- 2012 sets as a development set. The selected sentences obtained high sentence-level BLEU scores when being translated with a baseline phrasebased system, and do each</context>
</contexts>
<marker>Chen, Goodman, 1998</marker>
<rawString>Stanley F. Chen and Joshua Goodman. 1998. An Empirical Study of Smoothing Techniques for Language Modeling. Technical Report TR-10-98, Computer Science Group, Harvard University, Cambridge, MA, USA, August.</rawString>
</citation>
<citation valid="true">
<authors>
<author>Colin Cherry</author>
<author>George Foster</author>
</authors>
<title>Batch Tuning Strategies for Statistical Machine Translation.</title>
<date>2012</date>
<booktitle>In Proc. of the Human Language Technology Conf. / North American Chapter of the Assoc. for Computational Linguistics (HLT-NAACL),</booktitle>
<pages>427--436</pages>
<location>Montréal, Canada,</location>
<contexts>
<context position="21731" citStr="Cherry and Foster, 2012" startWordPosition="3534" endWordPosition="3537">h the rule was extracted (Williams et al., 2014). Rule translation probabilities are smoothed via Good-Turing smoothing. The language model (LM) is a large interpolated 5-gram LM with modified Kneser-Ney smoothing (Kneser and Ney, 1995; Chen and Goodman, 1998). The target side of the parallel corpus and the monolingual German News Crawl corpora are employed as training data. We use the SRILM toolkit (Stolcke, 2002) to train the LM and rely on KenLM (Heafield, 2011) for language model scoring during decoding. Model weights are optimized to maximize BLEU (Papineni et al., 2002) with batch MIRA (Cherry and Foster, 2012) on 1000-best lists. We selected 2000 sentences from the newstest2008- 2012 sets as a development set. The selected sentences obtained high sentence-level BLEU scores when being translated with a baseline phrasebased system, and do each contain less than 30 words for more rapid tuning. newstest2013 and newstest2014 are used as unseen test sets. Translation quality is measured in truecase with BLEU and TER (Snover et al., 2006).5 7.2 Translation Results The results of the empirical evaluation are given in Table 2. Our GHKM string-to-tree system attains state-of-the-art performance on newstest20</context>
</contexts>
<marker>Cherry, Foster, 2012</marker>
<rawString>Colin Cherry and George Foster. 2012. Batch Tuning Strategies for Statistical Machine Translation. In Proc. of the Human Language Technology Conf. / North American Chapter of the Assoc. for Computational Linguistics (HLT-NAACL), pages 427–436, Montréal, Canada, June.</rawString>
</citation>
<citation valid="true">
<authors>
<author>Steve DeNeefe</author>
<author>Kevin Knight</author>
<author>Wei Wang</author>
<author>Daniel Marcu</author>
</authors>
<title>What Can Syntax-Based MT Learn from Phrase-Based MT?</title>
<date>2007</date>
<booktitle>In Proc. of the 2007 Joint Conf. on Empirical Methods in Natural Language Processing and Computational Natural Language Learning (EMNLP-CoNLL),</booktitle>
<pages>755--763</pages>
<location>Prague, Czech Republic,</location>
<contexts>
<context position="3294" citStr="DeNeefe et al., 2007" startWordPosition="481" endWordPosition="484"> preference grammars for GHKM translation in Section 5. Our technique for applying soft source syntactic constraints in GHKM string-to-tree translation is described in Section 6. Section 7 contains the empirical part of the paper. We first describe our experimental setup (7.1), followed by a presentation and discussion of the translation results (7.2). We conclude the paper in Section 8. 3 Related Work Our syntactic translation model conforms to the GHKM syntax approach as proposed by Galley, Hopkins, Knight, and Marcu (Galley et al., 2004) with composed rules as in (Galley et al., 2006) and (DeNeefe et al., 2007). Systems based on 148 Proceedings of SSST-8, Eighth Workshop on Syntax, Semantics and Structure in Statistical Translation, pages 148–156, October 25, 2014, Doha, Qatar. c�2014 Association for Computational Linguistics this paradigm have recently been among the topranked submissions to public evaluation campaigns (Williams et al., 2014; Bojar et al., 2014). Our soft source syntactic constraints features borrow ideas from Marton and Resnik (2008) who proposed a comparable approach for hierarchical machine translation. The major difference is that the features of Marton and Resnik (2008) are on</context>
<context position="6412" citStr="DeNeefe et al., 2007" startWordPosition="976" endWordPosition="979">t side of rules are linked in a one-toone correspondence. The ∼ relation defines this one-to-one correspondence. The left-hand side of the rule is a pair of source and target nonterminals, A E NF and B E NE. Decoding is typically carried out with a parsingbased algorithm, in our case a customized version of CYK+ (Chappelier and Rajman, 1998). The parsing algorithm is extended to handle translation candidates and to incorporate language model scores via cube pruning (Chiang, 2007). 4.1 GHKM String-to-Tree Translation In GHKM string-to-tree translation (Galley et al., 2004; Galley et al., 2006; DeNeefe et al., 2007), rules are extracted from training instances which consist of a source sentence, a target sentence along with its constituent parse tree, and a word alignment matrix. This tuple is interpreted as a directed graph (the alignment graph), with edges pointing away from the root of the tree, and word alignment links being edges as well. A set of nodes (the frontier set) is determined that contains only nodes with non-overlapping closure of their spans.1 By computing frontier graph fragments—fragments of the alignment graph such that their root and all sinks are in the frontier set— the GHKM extrac</context>
</contexts>
<marker>DeNeefe, Knight, Wang, Marcu, 2007</marker>
<rawString>Steve DeNeefe, Kevin Knight, Wei Wang, and Daniel Marcu. 2007. What Can Syntax-Based MT Learn from Phrase-Based MT? In Proc. of the 2007 Joint Conf. on Empirical Methods in Natural Language Processing and Computational Natural Language Learning (EMNLP-CoNLL), pages 755–763, Prague, Czech Republic, June.</rawString>
</citation>
<citation valid="true">
<authors>
<author>Michel Galley</author>
<author>Mark Hopkins</author>
<author>Kevin Knight</author>
<author>Daniel Marcu</author>
</authors>
<title>What’s in a translation rule?</title>
<date>2004</date>
<booktitle>In Proc. of the Human Language Technology Conf. /North American Chapter of the Assoc. for Computational Linguistics (HLT-NAACL),</booktitle>
<pages>273--280</pages>
<location>Boston, MA, USA,</location>
<contexts>
<context position="1774" citStr="Galley et al., 2004" startWordPosition="249" endWordPosition="252">s provides consistent gains. By employing soft source syntactic constraints with sparse features, we are able to achieve improvements of up to 0.7 points BLEU and 1.0 points TER. 1 Introduction Previous research in both formally syntax-based (i.e., hierarchical) and linguistically syntax-based statistical machine translation has demonstrated that significant quality gains can be achieved via integration of syntactic information as features in a non-obtrusive manner, rather than as hard constraints. We implemented two feature-based extensions for a GHKM-style string-to-tree translation system (Galley et al., 2004): • Preference grammars to soften the hard target-side syntactic constraints that are imposed by the target non-terminal labels. • Soft source-side syntactic constraints that enhance the string-to-tree translation model with input tree features based on source syntax labels. The empirical results on an English→German translation task are twofold. Target-side preference grammars do not show an improvement over the string-to-tree baseline with syntactified translation rules. Source-side syntactic constraints, on the other hand, yield consistent moderate gains if applied as supplementary features</context>
<context position="3219" citStr="Galley et al., 2004" startWordPosition="467" endWordPosition="470">al, and in particular those of GHKM string-to-tree translation. We present preference grammars for GHKM translation in Section 5. Our technique for applying soft source syntactic constraints in GHKM string-to-tree translation is described in Section 6. Section 7 contains the empirical part of the paper. We first describe our experimental setup (7.1), followed by a presentation and discussion of the translation results (7.2). We conclude the paper in Section 8. 3 Related Work Our syntactic translation model conforms to the GHKM syntax approach as proposed by Galley, Hopkins, Knight, and Marcu (Galley et al., 2004) with composed rules as in (Galley et al., 2006) and (DeNeefe et al., 2007). Systems based on 148 Proceedings of SSST-8, Eighth Workshop on Syntax, Semantics and Structure in Statistical Translation, pages 148–156, October 25, 2014, Doha, Qatar. c�2014 Association for Computational Linguistics this paradigm have recently been among the topranked submissions to public evaluation campaigns (Williams et al., 2014; Bojar et al., 2014). Our soft source syntactic constraints features borrow ideas from Marton and Resnik (2008) who proposed a comparable approach for hierarchical machine translation. T</context>
<context position="6368" citStr="Galley et al., 2004" startWordPosition="968" endWordPosition="971">minals on the source side and on the target side of rules are linked in a one-toone correspondence. The ∼ relation defines this one-to-one correspondence. The left-hand side of the rule is a pair of source and target nonterminals, A E NF and B E NE. Decoding is typically carried out with a parsingbased algorithm, in our case a customized version of CYK+ (Chappelier and Rajman, 1998). The parsing algorithm is extended to handle translation candidates and to incorporate language model scores via cube pruning (Chiang, 2007). 4.1 GHKM String-to-Tree Translation In GHKM string-to-tree translation (Galley et al., 2004; Galley et al., 2006; DeNeefe et al., 2007), rules are extracted from training instances which consist of a source sentence, a target sentence along with its constituent parse tree, and a word alignment matrix. This tuple is interpreted as a directed graph (the alignment graph), with edges pointing away from the root of the tree, and word alignment links being edges as well. A set of nodes (the frontier set) is determined that contains only nodes with non-overlapping closure of their spans.1 By computing frontier graph fragments—fragments of the alignment graph such that their root and all si</context>
</contexts>
<marker>Galley, Hopkins, Knight, Marcu, 2004</marker>
<rawString>Michel Galley, Mark Hopkins, Kevin Knight, and Daniel Marcu. 2004. What’s in a translation rule? In Proc. of the Human Language Technology Conf. /North American Chapter of the Assoc. for Computational Linguistics (HLT-NAACL), pages 273–280, Boston, MA, USA, May.</rawString>
</citation>
<citation valid="true">
<authors>
<author>Michel Galley</author>
<author>Jonathan Graehl</author>
<author>Kevin Knight</author>
<author>Daniel Marcu</author>
<author>Steve DeNeefe</author>
<author>Wei Wang</author>
<author>Ignacio Thayer</author>
</authors>
<title>Scalable Inference and Training of Context-Rich Syntactic Translation Models.</title>
<date>2006</date>
<booktitle>In Proc. of the 21st Int. Conf. on Computational Linguistics and 44th Annual Meeting of the Assoc. for Computational Linguistics,</booktitle>
<pages>961--968</pages>
<location>Sydney, Australia,</location>
<contexts>
<context position="3267" citStr="Galley et al., 2006" startWordPosition="476" endWordPosition="479">ee translation. We present preference grammars for GHKM translation in Section 5. Our technique for applying soft source syntactic constraints in GHKM string-to-tree translation is described in Section 6. Section 7 contains the empirical part of the paper. We first describe our experimental setup (7.1), followed by a presentation and discussion of the translation results (7.2). We conclude the paper in Section 8. 3 Related Work Our syntactic translation model conforms to the GHKM syntax approach as proposed by Galley, Hopkins, Knight, and Marcu (Galley et al., 2004) with composed rules as in (Galley et al., 2006) and (DeNeefe et al., 2007). Systems based on 148 Proceedings of SSST-8, Eighth Workshop on Syntax, Semantics and Structure in Statistical Translation, pages 148–156, October 25, 2014, Doha, Qatar. c�2014 Association for Computational Linguistics this paradigm have recently been among the topranked submissions to public evaluation campaigns (Williams et al., 2014; Bojar et al., 2014). Our soft source syntactic constraints features borrow ideas from Marton and Resnik (2008) who proposed a comparable approach for hierarchical machine translation. The major difference is that the features of Mart</context>
<context position="6389" citStr="Galley et al., 2006" startWordPosition="972" endWordPosition="975">side and on the target side of rules are linked in a one-toone correspondence. The ∼ relation defines this one-to-one correspondence. The left-hand side of the rule is a pair of source and target nonterminals, A E NF and B E NE. Decoding is typically carried out with a parsingbased algorithm, in our case a customized version of CYK+ (Chappelier and Rajman, 1998). The parsing algorithm is extended to handle translation candidates and to incorporate language model scores via cube pruning (Chiang, 2007). 4.1 GHKM String-to-Tree Translation In GHKM string-to-tree translation (Galley et al., 2004; Galley et al., 2006; DeNeefe et al., 2007), rules are extracted from training instances which consist of a source sentence, a target sentence along with its constituent parse tree, and a word alignment matrix. This tuple is interpreted as a directed graph (the alignment graph), with edges pointing away from the root of the tree, and word alignment links being edges as well. A set of nodes (the frontier set) is determined that contains only nodes with non-overlapping closure of their spans.1 By computing frontier graph fragments—fragments of the alignment graph such that their root and all sinks are in the fronti</context>
</contexts>
<marker>Galley, Graehl, Knight, Marcu, DeNeefe, Wang, Thayer, 2006</marker>
<rawString>Michel Galley, Jonathan Graehl, Kevin Knight, Daniel Marcu, Steve DeNeefe, Wei Wang, and Ignacio Thayer. 2006. Scalable Inference and Training of Context-Rich Syntactic Translation Models. In Proc. of the 21st Int. Conf. on Computational Linguistics and 44th Annual Meeting of the Assoc. for Computational Linguistics, pages 961–968, Sydney, Australia, July.</rawString>
</citation>
<citation valid="true">
<authors>
<author>Qin Gao</author>
<author>Stephan Vogel</author>
</authors>
<title>Parallel Implementations of Word Alignment Tool.</title>
<date>2008</date>
<booktitle>In Software Engineering, Testing, and Quality Assurance for Natural Language Processing, SETQA-NLP ’08,</booktitle>
<pages>49--57</pages>
<location>Columbus, OH, USA,</location>
<contexts>
<context position="18189" citStr="Gao and Vogel, 2008" startWordPosition="2953" endWordPosition="2956">pruning (Hoang et al., 2009). 4http://www.statmt.org/wmt14/ translation-task.html 7.1 Experimental Setup We work with an English–German parallel training corpus of around 4.5 M sentence pairs (after corpus cleaning). The parallel data originates from three different sources which have been eligible for the constrained track of the ACL 2014 Ninth Workshop on Statistical Machine Translation shared translation task: Europarl (Koehn, 2005), News Commentary, and the Common Crawl corpus as provided on the WMT website. Word alignments are created by aligning the data in both directions with MGIZA++ (Gao and Vogel, 2008) and symmetrizing the two trained alignments (Och and Ney, 2003; Koehn et al., 2003). The German target side training data is parsed with BitPar (Schmid, 2004). We remove grammatical case and function information from the annotation obtained with BitPar and apply right binarization of the German parse trees prior to rule extraction (Wang et al., 2007; Wang et al., 2010; Nadejde et al., 2013). For the soft source syntactic constraints, we parse the English source side of the parallel data with the English Berkeley Parser (Petrov et al., 2006) and produce composite SAMT-style labels as discussed</context>
</contexts>
<marker>Gao, Vogel, 2008</marker>
<rawString>Qin Gao and Stephan Vogel. 2008. Parallel Implementations of Word Alignment Tool. In Software Engineering, Testing, and Quality Assurance for Natural Language Processing, SETQA-NLP ’08, pages 49– 57, Columbus, OH, USA, June.</rawString>
</citation>
<citation valid="true">
<authors>
<author>Kenneth Heafield</author>
</authors>
<title>KenLM: Faster and Smaller Language Model Queries.</title>
<date>2011</date>
<booktitle>In Proc. of the Workshop on Statistical Machine Translation (WMT),</booktitle>
<pages>187--197</pages>
<location>Edinburgh, Scotland, UK,</location>
<contexts>
<context position="21576" citStr="Heafield, 2011" startWordPosition="3512" endWordPosition="3513">test2008-2012 sets is used as development set. model, a rule rareness penalty, and the monolingual PCFG probability of the tree fragment from which the rule was extracted (Williams et al., 2014). Rule translation probabilities are smoothed via Good-Turing smoothing. The language model (LM) is a large interpolated 5-gram LM with modified Kneser-Ney smoothing (Kneser and Ney, 1995; Chen and Goodman, 1998). The target side of the parallel corpus and the monolingual German News Crawl corpora are employed as training data. We use the SRILM toolkit (Stolcke, 2002) to train the LM and rely on KenLM (Heafield, 2011) for language model scoring during decoding. Model weights are optimized to maximize BLEU (Papineni et al., 2002) with batch MIRA (Cherry and Foster, 2012) on 1000-best lists. We selected 2000 sentences from the newstest2008- 2012 sets as a development set. The selected sentences obtained high sentence-level BLEU scores when being translated with a baseline phrasebased system, and do each contain less than 30 words for more rapid tuning. newstest2013 and newstest2014 are used as unseen test sets. Translation quality is measured in truecase with BLEU and TER (Snover et al., 2006).5 7.2 Translat</context>
</contexts>
<marker>Heafield, 2011</marker>
<rawString>Kenneth Heafield. 2011. KenLM: Faster and Smaller Language Model Queries. In Proc. of the Workshop on Statistical Machine Translation (WMT), pages 187–197, Edinburgh, Scotland, UK, July.</rawString>
</citation>
<citation valid="true">
<authors>
<author>Hieu Hoang</author>
<author>Philipp Koehn</author>
<author>Adam Lopez</author>
</authors>
<title>A Unified Framework for Phrase-Based, Hierarchical, and Syntax-Based Statistical Machine Translation.</title>
<date>2009</date>
<booktitle>In Proc. of the Int. Workshop on Spoken Language Translation (IWSLT),</booktitle>
<pages>152--159</pages>
<location>Tokyo, Japan,</location>
<contexts>
<context position="17597" citStr="Hoang et al., 2009" startWordPosition="2863" endWordPosition="2866">atures are active that depend on the identity of labels within this set. All sparse features for source labels outside of the core set are inactive. 7 Experiments We empirically evaluate the effectiveness of preference grammars and soft source syntactic constraints for GHKM translation on the English→German language pair using the standard newstest sets of the Workshop on Statistical Machine Translation (WMT) for testing.4 The experiments are conducted with the open-source Moses implementations of GHKM rule extraction (Williams and Koehn, 2012) and decoding with CYK+ parsing and cube pruning (Hoang et al., 2009). 4http://www.statmt.org/wmt14/ translation-task.html 7.1 Experimental Setup We work with an English–German parallel training corpus of around 4.5 M sentence pairs (after corpus cleaning). The parallel data originates from three different sources which have been eligible for the constrained track of the ACL 2014 Ninth Workshop on Statistical Machine Translation shared translation task: Europarl (Koehn, 2005), News Commentary, and the Common Crawl corpus as provided on the WMT website. Word alignments are created by aligning the data in both directions with MGIZA++ (Gao and Vogel, 2008) and sym</context>
</contexts>
<marker>Hoang, Koehn, Lopez, 2009</marker>
<rawString>Hieu Hoang, Philipp Koehn, and Adam Lopez. 2009. A Unified Framework for Phrase-Based, Hierarchical, and Syntax-Based Statistical Machine Translation. In Proc. of the Int. Workshop on Spoken Language Translation (IWSLT), pages 152–159, Tokyo, Japan, December.</rawString>
</citation>
<citation valid="true">
<authors>
<author>Zhongqiang Huang</author>
<author>Jacob Devlin</author>
<author>Rabih Zbib</author>
</authors>
<title>Factored Soft Source Syntactic Constraints for Hierarchical Machine Translation.</title>
<date>2013</date>
<booktitle>In Proc. of the Conf. on Empirical Methods for Natural Language Processing (EMNLP),</booktitle>
<pages>556--566</pages>
<location>Seattle, WA, USA,</location>
<contexts>
<context position="4400" citStr="Huang et al., 2013" startWordPosition="650" endWordPosition="653">or hierarchical machine translation. The major difference is that the features of Marton and Resnik (2008) are only based on the labels from the input trees as seen in tuning and decoding. They penalize violations of constituent boundaries but do not employ syntactic parse annotation of the source side of the training data. We, in contrast, equip the rules with latent source label properties, allowing for features that can check for conformance of input tree labels and source labels that have been seen in training. Other groups have applied similar techniques to a string-to-dependency system (Huang et al., 2013) and—like in our work—a GHKM stringto-tree system (Zhang et al., 2011). Both Huang et al. (2013) and Zhang et al. (2011) store source labels as additional information with the rules. They however investigate somewhat different feature functions than we do. Marton and Resnik (2008) evaluated their method on the NIST Chinese→English and Arabic→English tasks. Huang et al. (2013) and Zhang et al. (2011) present results on the NIST Chinese→English task. We focus our attention on a very different task: English→German. 4 Syntax-based Translation In syntax-based translation, a probabilistic synchronou</context>
</contexts>
<marker>Huang, Devlin, Zbib, 2013</marker>
<rawString>Zhongqiang Huang, Jacob Devlin, and Rabih Zbib. 2013. Factored Soft Source Syntactic Constraints for Hierarchical Machine Translation. In Proc. of the Conf. on Empirical Methods for Natural Language Processing (EMNLP), pages 556–566, Seattle, WA, USA, October.</rawString>
</citation>
<citation valid="true">
<authors>
<author>Reinhard Kneser</author>
<author>Hermann Ney</author>
</authors>
<title>Improved Backing-Off for M-gram Language Modeling.</title>
<date>1995</date>
<booktitle>In Proceedings of the Int. Conf. on Acoustics, Speech, and Signal Processing,</booktitle>
<volume>1</volume>
<pages>181--184</pages>
<location>Detroit, MI, USA,</location>
<contexts>
<context position="21342" citStr="Kneser and Ney, 1995" startWordPosition="3470" endWordPosition="3473">aints 34.6 47.0 19.8 62.9 19.5 65.2 + drop derivations with tsyn(d) = 0 34.0 47.5 19.7 63.0 18.8 65.8 Table 2: English→German experimental results (truecase). BLEU scores are given in percentage. A selection of 2000 sentences from the newstest2008-2012 sets is used as development set. model, a rule rareness penalty, and the monolingual PCFG probability of the tree fragment from which the rule was extracted (Williams et al., 2014). Rule translation probabilities are smoothed via Good-Turing smoothing. The language model (LM) is a large interpolated 5-gram LM with modified Kneser-Ney smoothing (Kneser and Ney, 1995; Chen and Goodman, 1998). The target side of the parallel corpus and the monolingual German News Crawl corpora are employed as training data. We use the SRILM toolkit (Stolcke, 2002) to train the LM and rely on KenLM (Heafield, 2011) for language model scoring during decoding. Model weights are optimized to maximize BLEU (Papineni et al., 2002) with batch MIRA (Cherry and Foster, 2012) on 1000-best lists. We selected 2000 sentences from the newstest2008- 2012 sets as a development set. The selected sentences obtained high sentence-level BLEU scores when being translated with a baseline phrase</context>
</contexts>
<marker>Kneser, Ney, 1995</marker>
<rawString>Reinhard Kneser and Hermann Ney. 1995. Improved Backing-Off for M-gram Language Modeling. In Proceedings of the Int. Conf. on Acoustics, Speech, and Signal Processing, volume 1, pages 181–184, Detroit, MI, USA, May.</rawString>
</citation>
<citation valid="true">
<authors>
<author>Philipp Koehn</author>
<author>Franz Joseph Och</author>
<author>Daniel Marcu</author>
</authors>
<title>Statistical Phrase-Based Translation.</title>
<date>2003</date>
<booktitle>In Proc. of the Human Language Technology Conf. / North American Chapter of the Assoc. for Computational Linguistics (HLT-NAACL),</booktitle>
<pages>127--133</pages>
<location>Edmonton, Canada, May/June.</location>
<contexts>
<context position="18273" citStr="Koehn et al., 2003" startWordPosition="2967" endWordPosition="2970"> Experimental Setup We work with an English–German parallel training corpus of around 4.5 M sentence pairs (after corpus cleaning). The parallel data originates from three different sources which have been eligible for the constrained track of the ACL 2014 Ninth Workshop on Statistical Machine Translation shared translation task: Europarl (Koehn, 2005), News Commentary, and the Common Crawl corpus as provided on the WMT website. Word alignments are created by aligning the data in both directions with MGIZA++ (Gao and Vogel, 2008) and symmetrizing the two trained alignments (Och and Ney, 2003; Koehn et al., 2003). The German target side training data is parsed with BitPar (Schmid, 2004). We remove grammatical case and function information from the annotation obtained with BitPar and apply right binarization of the German parse trees prior to rule extraction (Wang et al., 2007; Wang et al., 2010; Nadejde et al., 2013). For the soft source syntactic constraints, we parse the English source side of the parallel data with the English Berkeley Parser (Petrov et al., 2006) and produce composite SAMT-style labels as discussed in Section 6. When extracting syntactic rules, we impose several restrictions for c</context>
</contexts>
<marker>Koehn, Och, Marcu, 2003</marker>
<rawString>Philipp Koehn, Franz Joseph Och, and Daniel Marcu. 2003. Statistical Phrase-Based Translation. In Proc. of the Human Language Technology Conf. / North American Chapter of the Assoc. for Computational Linguistics (HLT-NAACL), pages 127–133, Edmonton, Canada, May/June.</rawString>
</citation>
<citation valid="true">
<authors>
<author>P Koehn</author>
<author>H Hoang</author>
<author>A Birch</author>
<author>C Callison-Burch</author>
<author>M Federico</author>
<author>N Bertoldi</author>
<author>B Cowan</author>
<author>W Shen</author>
<author>C Moran</author>
<author>R Zens</author>
<author>C Dyer</author>
<author>O Bojar</author>
<author>A Constantin</author>
<author>E Herbst</author>
</authors>
<title>Moses: Open Source Toolkit for Statistical Machine Translation.</title>
<date>2007</date>
<booktitle>In Proc. of the Annual Meeting of the Assoc. for Computational Linguistics (ACL),</booktitle>
<pages>177--180</pages>
<location>Prague, Czech Republic,</location>
<contexts>
<context position="13945" citStr="Koehn et al., 2007" startWordPosition="2242" endWordPosition="2245">e uncovered non-terminal with a special label. The set of source label vectors that are seen with a rule during extraction is stored with it in the rule table as an additional property. This information can be used to implement feature-based soft source syntactic constraints. Table 1 shows an example of a set of source label vectors stored with a grammar rule. The first element of each vector is an implicit sourcesyntactic label for the left-hand side non-terminal of the rule, the remaining elements are implicit 3Specifically, we apply relax-parse --SAMT 2 as implemented in the Moses toolkit (Koehn et al., 2007). source label vector frequency (IN+NP,NN,NN) 7 (IN+NP,NNP,NNP) 3 (IN++NP,NNS,NNS) 2 (IN+NP,NP,NP) 2 (PP//SBAR,NP,NP) 1 Table 1: The set of source label vectors (along with their frequencies in the training data) for the rule X,PP-MO —* (between X-1 and X-0,zwischen NN-0 und NN-1). The overall rule frequency is 15. source-syntactic labels for the right-hand side source non-terminals. The basic idea for soft source syntactic constraints features is to also parse the input data in a preprocessing step and try to match input labels and source label vectors that are associated with SCFG rules. 6.1</context>
</contexts>
<marker>Koehn, Hoang, Birch, Callison-Burch, Federico, Bertoldi, Cowan, Shen, Moran, Zens, Dyer, Bojar, Constantin, Herbst, 2007</marker>
<rawString>P. Koehn, H. Hoang, A. Birch, C. Callison-Burch, M. Federico, N. Bertoldi, B. Cowan, W. Shen, C. Moran, R. Zens, C. Dyer, O. Bojar, A. Constantin, and E. Herbst. 2007. Moses: Open Source Toolkit for Statistical Machine Translation. In Proc. of the Annual Meeting of the Assoc. for Computational Linguistics (ACL), pages 177–180, Prague, Czech Republic, June.</rawString>
</citation>
<citation valid="true">
<authors>
<author>Philipp Koehn</author>
</authors>
<title>Europarl: A parallel corpus for statistical machine translation.</title>
<date>2005</date>
<booktitle>In Proc. of the MT Summit X,</booktitle>
<location>Phuket, Thailand,</location>
<contexts>
<context position="18008" citStr="Koehn, 2005" startWordPosition="2923" endWordPosition="2924">esting.4 The experiments are conducted with the open-source Moses implementations of GHKM rule extraction (Williams and Koehn, 2012) and decoding with CYK+ parsing and cube pruning (Hoang et al., 2009). 4http://www.statmt.org/wmt14/ translation-task.html 7.1 Experimental Setup We work with an English–German parallel training corpus of around 4.5 M sentence pairs (after corpus cleaning). The parallel data originates from three different sources which have been eligible for the constrained track of the ACL 2014 Ninth Workshop on Statistical Machine Translation shared translation task: Europarl (Koehn, 2005), News Commentary, and the Common Crawl corpus as provided on the WMT website. Word alignments are created by aligning the data in both directions with MGIZA++ (Gao and Vogel, 2008) and symmetrizing the two trained alignments (Och and Ney, 2003; Koehn et al., 2003). The German target side training data is parsed with BitPar (Schmid, 2004). We remove grammatical case and function information from the annotation obtained with BitPar and apply right binarization of the German parse trees prior to rule extraction (Wang et al., 2007; Wang et al., 2010; Nadejde et al., 2013). For the soft source syn</context>
</contexts>
<marker>Koehn, 2005</marker>
<rawString>Philipp Koehn. 2005. Europarl: A parallel corpus for statistical machine translation. In Proc. of the MT Summit X, Phuket, Thailand, September.</rawString>
</citation>
<citation valid="true">
<authors>
<author>Yuval Marton</author>
<author>Philip Resnik</author>
</authors>
<title>Soft Syntactic Constraints for Hierarchical Phrased-Based Translation.</title>
<date>2008</date>
<booktitle>In Proc. of the Annual Meeting of the Assoc. for Computational Linguistics (ACL),</booktitle>
<pages>1003--1011</pages>
<location>Columbus, OH, USA,</location>
<contexts>
<context position="3744" citStr="Marton and Resnik (2008)" startWordPosition="546" endWordPosition="549">to the GHKM syntax approach as proposed by Galley, Hopkins, Knight, and Marcu (Galley et al., 2004) with composed rules as in (Galley et al., 2006) and (DeNeefe et al., 2007). Systems based on 148 Proceedings of SSST-8, Eighth Workshop on Syntax, Semantics and Structure in Statistical Translation, pages 148–156, October 25, 2014, Doha, Qatar. c�2014 Association for Computational Linguistics this paradigm have recently been among the topranked submissions to public evaluation campaigns (Williams et al., 2014; Bojar et al., 2014). Our soft source syntactic constraints features borrow ideas from Marton and Resnik (2008) who proposed a comparable approach for hierarchical machine translation. The major difference is that the features of Marton and Resnik (2008) are only based on the labels from the input trees as seen in tuning and decoding. They penalize violations of constituent boundaries but do not employ syntactic parse annotation of the source side of the training data. We, in contrast, equip the rules with latent source label properties, allowing for features that can check for conformance of input tree labels and source labels that have been seen in training. Other groups have applied similar techniqu</context>
</contexts>
<marker>Marton, Resnik, 2008</marker>
<rawString>Yuval Marton and Philip Resnik. 2008. Soft Syntactic Constraints for Hierarchical Phrased-Based Translation. In Proc. of the Annual Meeting of the Assoc. for Computational Linguistics (ACL), pages 1003–1011, Columbus, OH, USA, June.</rawString>
</citation>
<citation valid="true">
<authors>
<author>Maria Nadejde</author>
<author>Philip Williams</author>
<author>Philipp Koehn</author>
</authors>
<title>Edinburgh’s Syntax-Based Machine Translation Systems.</title>
<date>2013</date>
<booktitle>In Proc. of the Workshop on Statistical Machine Translation (WMT),</booktitle>
<pages>170--176</pages>
<location>Sofia, Bulgaria,</location>
<contexts>
<context position="18583" citStr="Nadejde et al., 2013" startWordPosition="3017" endWordPosition="3020">hared translation task: Europarl (Koehn, 2005), News Commentary, and the Common Crawl corpus as provided on the WMT website. Word alignments are created by aligning the data in both directions with MGIZA++ (Gao and Vogel, 2008) and symmetrizing the two trained alignments (Och and Ney, 2003; Koehn et al., 2003). The German target side training data is parsed with BitPar (Schmid, 2004). We remove grammatical case and function information from the annotation obtained with BitPar and apply right binarization of the German parse trees prior to rule extraction (Wang et al., 2007; Wang et al., 2010; Nadejde et al., 2013). For the soft source syntactic constraints, we parse the English source side of the parallel data with the English Berkeley Parser (Petrov et al., 2006) and produce composite SAMT-style labels as discussed in Section 6. When extracting syntactic rules, we impose several restrictions for composed rules, in particular a maximum number of 100 tree nodes per rule, a maximum depth of seven, and a maximum size of seven. We discard rules with non-terminals on their right-hand side if they are singletons in the training data. For efficiency reasons, we also enforce a limit on the number of label vect</context>
</contexts>
<marker>Nadejde, Williams, Koehn, 2013</marker>
<rawString>Maria Nadejde, Philip Williams, and Philipp Koehn. 2013. Edinburgh’s Syntax-Based Machine Translation Systems. In Proc. of the Workshop on Statistical Machine Translation (WMT), pages 170–176, Sofia, Bulgaria, August.</rawString>
</citation>
<citation valid="true">
<authors>
<author>Franz Josef Och</author>
<author>Hermann Ney</author>
</authors>
<title>A Systematic Comparison of Various Statistical Alignment Models.</title>
<date>2003</date>
<journal>Computational Linguistics,</journal>
<volume>29</volume>
<issue>1</issue>
<contexts>
<context position="18252" citStr="Och and Ney, 2003" startWordPosition="2963" endWordPosition="2966">ation-task.html 7.1 Experimental Setup We work with an English–German parallel training corpus of around 4.5 M sentence pairs (after corpus cleaning). The parallel data originates from three different sources which have been eligible for the constrained track of the ACL 2014 Ninth Workshop on Statistical Machine Translation shared translation task: Europarl (Koehn, 2005), News Commentary, and the Common Crawl corpus as provided on the WMT website. Word alignments are created by aligning the data in both directions with MGIZA++ (Gao and Vogel, 2008) and symmetrizing the two trained alignments (Och and Ney, 2003; Koehn et al., 2003). The German target side training data is parsed with BitPar (Schmid, 2004). We remove grammatical case and function information from the annotation obtained with BitPar and apply right binarization of the German parse trees prior to rule extraction (Wang et al., 2007; Wang et al., 2010; Nadejde et al., 2013). For the soft source syntactic constraints, we parse the English source side of the parallel data with the English Berkeley Parser (Petrov et al., 2006) and produce composite SAMT-style labels as discussed in Section 6. When extracting syntactic rules, we impose sever</context>
</contexts>
<marker>Och, Ney, 2003</marker>
<rawString>Franz Josef Och and Hermann Ney. 2003. A Systematic Comparison of Various Statistical Alignment Models. Computational Linguistics, 29(1):19–51, March.</rawString>
</citation>
<citation valid="true">
<authors>
<author>Kishore Papineni</author>
<author>Salim Roukos</author>
<author>Todd Ward</author>
<author>WeiJing Zhu</author>
</authors>
<title>Bleu: a Method for Automatic Evaluation of Machine Translation.</title>
<date>2002</date>
<booktitle>In Proc. of the Annual Meeting of the Assoc. for Computational Linguistics (ACL),</booktitle>
<pages>311--318</pages>
<location>Philadelphia, PA, USA,</location>
<contexts>
<context position="21689" citStr="Papineni et al., 2002" startWordPosition="3527" endWordPosition="3530">obability of the tree fragment from which the rule was extracted (Williams et al., 2014). Rule translation probabilities are smoothed via Good-Turing smoothing. The language model (LM) is a large interpolated 5-gram LM with modified Kneser-Ney smoothing (Kneser and Ney, 1995; Chen and Goodman, 1998). The target side of the parallel corpus and the monolingual German News Crawl corpora are employed as training data. We use the SRILM toolkit (Stolcke, 2002) to train the LM and rely on KenLM (Heafield, 2011) for language model scoring during decoding. Model weights are optimized to maximize BLEU (Papineni et al., 2002) with batch MIRA (Cherry and Foster, 2012) on 1000-best lists. We selected 2000 sentences from the newstest2008- 2012 sets as a development set. The selected sentences obtained high sentence-level BLEU scores when being translated with a baseline phrasebased system, and do each contain less than 30 words for more rapid tuning. newstest2013 and newstest2014 are used as unseen test sets. Translation quality is measured in truecase with BLEU and TER (Snover et al., 2006).5 7.2 Translation Results The results of the empirical evaluation are given in Table 2. Our GHKM string-to-tree system attains </context>
</contexts>
<marker>Papineni, Roukos, Ward, Zhu, 2002</marker>
<rawString>Kishore Papineni, Salim Roukos, Todd Ward, and WeiJing Zhu. 2002. Bleu: a Method for Automatic Evaluation of Machine Translation. In Proc. of the Annual Meeting of the Assoc. for Computational Linguistics (ACL), pages 311–318, Philadelphia, PA, USA, July.</rawString>
</citation>
<citation valid="true">
<authors>
<author>Slav Petrov</author>
<author>Leon Barrett</author>
<author>Romain Thibaux</author>
<author>Dan Klein</author>
</authors>
<title>Learning Accurate, Compact, and Interpretable Tree Annotation.</title>
<date>2006</date>
<booktitle>In Proc. of the 21st Int. Conf. on Computational Linguistics and 44th Annual Meeting of the Assoc. for Computational Linguistics,</booktitle>
<pages>433--440</pages>
<location>Sydney, Australia,</location>
<contexts>
<context position="18736" citStr="Petrov et al., 2006" startWordPosition="3042" endWordPosition="3045">y aligning the data in both directions with MGIZA++ (Gao and Vogel, 2008) and symmetrizing the two trained alignments (Och and Ney, 2003; Koehn et al., 2003). The German target side training data is parsed with BitPar (Schmid, 2004). We remove grammatical case and function information from the annotation obtained with BitPar and apply right binarization of the German parse trees prior to rule extraction (Wang et al., 2007; Wang et al., 2010; Nadejde et al., 2013). For the soft source syntactic constraints, we parse the English source side of the parallel data with the English Berkeley Parser (Petrov et al., 2006) and produce composite SAMT-style labels as discussed in Section 6. When extracting syntactic rules, we impose several restrictions for composed rules, in particular a maximum number of 100 tree nodes per rule, a maximum depth of seven, and a maximum size of seven. We discard rules with non-terminals on their right-hand side if they are singletons in the training data. For efficiency reasons, we also enforce a limit on the number of label vectors that are stored as additional properties. Label vectors are only stored if they occur at least as often as the 50th most frequent label vector of the</context>
</contexts>
<marker>Petrov, Barrett, Thibaux, Klein, 2006</marker>
<rawString>Slav Petrov, Leon Barrett, Romain Thibaux, and Dan Klein. 2006. Learning Accurate, Compact, and Interpretable Tree Annotation. In Proc. of the 21st Int. Conf. on Computational Linguistics and 44th Annual Meeting of the Assoc. for Computational Linguistics, pages 433–440, Sydney, Australia, July.</rawString>
</citation>
<citation valid="true">
<authors>
<author>Helmut Schmid</author>
</authors>
<title>Efficient Parsing of Highly Ambiguous Context-Free Grammars with Bit Vectors.</title>
<date>2004</date>
<booktitle>In Proc. of the Int. Conf. on Computational Linguistics (COLING),</booktitle>
<location>Geneva, Switzerland,</location>
<contexts>
<context position="18348" citStr="Schmid, 2004" startWordPosition="2981" endWordPosition="2982">ound 4.5 M sentence pairs (after corpus cleaning). The parallel data originates from three different sources which have been eligible for the constrained track of the ACL 2014 Ninth Workshop on Statistical Machine Translation shared translation task: Europarl (Koehn, 2005), News Commentary, and the Common Crawl corpus as provided on the WMT website. Word alignments are created by aligning the data in both directions with MGIZA++ (Gao and Vogel, 2008) and symmetrizing the two trained alignments (Och and Ney, 2003; Koehn et al., 2003). The German target side training data is parsed with BitPar (Schmid, 2004). We remove grammatical case and function information from the annotation obtained with BitPar and apply right binarization of the German parse trees prior to rule extraction (Wang et al., 2007; Wang et al., 2010; Nadejde et al., 2013). For the soft source syntactic constraints, we parse the English source side of the parallel data with the English Berkeley Parser (Petrov et al., 2006) and produce composite SAMT-style labels as discussed in Section 6. When extracting syntactic rules, we impose several restrictions for composed rules, in particular a maximum number of 100 tree nodes per rule, a</context>
</contexts>
<marker>Schmid, 2004</marker>
<rawString>Helmut Schmid. 2004. Efficient Parsing of Highly Ambiguous Context-Free Grammars with Bit Vectors. In Proc. of the Int. Conf. on Computational Linguistics (COLING), Geneva, Switzerland, August.</rawString>
</citation>
<citation valid="true">
<authors>
<author>Matthew Snover</author>
<author>Bonnie Dorr</author>
<author>Richard Schwartz</author>
<author>Linnea Micciulla</author>
<author>John Makhoul</author>
</authors>
<title>A Study of Translation Edit Rate with Targeted Human Annotation.</title>
<date>2006</date>
<booktitle>In Proc. of the Conf. of the Assoc. for Machine Translation in the Americas (AMTA),</booktitle>
<pages>223--231</pages>
<location>Cambridge, MA, USA,</location>
<contexts>
<context position="22161" citStr="Snover et al., 2006" startWordPosition="3604" endWordPosition="3607"> and rely on KenLM (Heafield, 2011) for language model scoring during decoding. Model weights are optimized to maximize BLEU (Papineni et al., 2002) with batch MIRA (Cherry and Foster, 2012) on 1000-best lists. We selected 2000 sentences from the newstest2008- 2012 sets as a development set. The selected sentences obtained high sentence-level BLEU scores when being translated with a baseline phrasebased system, and do each contain less than 30 words for more rapid tuning. newstest2013 and newstest2014 are used as unseen test sets. Translation quality is measured in truecase with BLEU and TER (Snover et al., 2006).5 7.2 Translation Results The results of the empirical evaluation are given in Table 2. Our GHKM string-to-tree system attains state-of-the-art performance on newstest2013 and newstest2014. 5TER scores are computed with tercom version 0.7.25 and parameters -N -s. 7.2.1 Soft Source Syntactic Constraints Adding the three dense soft source syntactic constraints features from Section 6.1 improves the baseline scores by 0.3 points BLEU and 0.6 points TER on newstest2013 and by 0.3 points BLEU and 0.7 points TER on newstest2014. Somewhat surprisingly, the sparse features from Section 6.2 do not boo</context>
</contexts>
<marker>Snover, Dorr, Schwartz, Micciulla, Makhoul, 2006</marker>
<rawString>Matthew Snover, Bonnie Dorr, Richard Schwartz, Linnea Micciulla, and John Makhoul. 2006. A Study of Translation Edit Rate with Targeted Human Annotation. In Proc. of the Conf. of the Assoc. for Machine Translation in the Americas (AMTA), pages 223–231, Cambridge, MA, USA, August.</rawString>
</citation>
<citation valid="true">
<authors>
<author>Daniel Stein</author>
<author>Stephan Peitz</author>
<author>David Vilar</author>
<author>Hermann Ney</author>
</authors>
<title>A Cocktail of Deep Syntactic Features for Hierarchical Machine Translation.</title>
<date>2010</date>
<booktitle>In Proc. of the Conf. of the Assoc. for Machine Translation in the Americas (AMTA),</booktitle>
<location>Denver, CO, USA, October/November.</location>
<contexts>
<context position="11662" citStr="Stein et al. (2010)" startWordPosition="1865" endWordPosition="1868">ent of an n + 1-dimensional vector s of implicit labels is an implicit label binding of the left-hand side non-terminal of the rule r. p(s|r) is the preference distribution of the rule. Here, ˆth(Y|d) is another auxiliary function that renormalizes the values of th(Y|d): th(Y|d) ˆth(Y|d) = (4) ∑Y0∈Sth(Y0|d) It provides us with a probability that the derivation d has the implicit label Y ∈ S as its root. Finally, the function th(Y|d) is defined as th(Y|d) = �ph(s[k]|dk−1) . (5) Note that the denominator in Equation (4) thus equals tsyn(d). 2Our notational conventions roughly follow the ones by Stein et al. (2010). tsyn(d) = ∑ p(s|r) · n+1 s∈Sn+1 ∏ k=2 ∑ p(s|r) · n+1 s∈Sn+1:s[1]=Y ∏ k=2 150 This concludes the formal specification of the first features. The second feature hauxSyn(d) penalizes rule applications in cases where tsyn(d) evaluates to 0: hauxSyn (d) = 0 if tsyn(d) 74 0 (6) 1 otherwise Its intuition is that rule applications that do not contribute to hsyn(d) should be punished. Derivations with tsyn(d) = 0 could alternatively be dropped completely, but our approach is to avoid hard constraints. We will later demonstrate empirically that discarding such derivations harms translation quality. 6 </context>
</contexts>
<marker>Stein, Peitz, Vilar, Ney, 2010</marker>
<rawString>Daniel Stein, Stephan Peitz, David Vilar, and Hermann Ney. 2010. A Cocktail of Deep Syntactic Features for Hierarchical Machine Translation. In Proc. of the Conf. of the Assoc. for Machine Translation in the Americas (AMTA), Denver, CO, USA, October/November.</rawString>
</citation>
<citation valid="true">
<authors>
<author>Andreas Stolcke</author>
</authors>
<title>SRILM – an Extensible Language Modeling Toolkit.</title>
<date>2002</date>
<booktitle>In Proc. of the Int. Conf. on Spoken Language Processing (ICSLP),</booktitle>
<volume>3</volume>
<location>Denver, CO, USA,</location>
<contexts>
<context position="21525" citStr="Stolcke, 2002" startWordPosition="3502" endWordPosition="3503">ntage. A selection of 2000 sentences from the newstest2008-2012 sets is used as development set. model, a rule rareness penalty, and the monolingual PCFG probability of the tree fragment from which the rule was extracted (Williams et al., 2014). Rule translation probabilities are smoothed via Good-Turing smoothing. The language model (LM) is a large interpolated 5-gram LM with modified Kneser-Ney smoothing (Kneser and Ney, 1995; Chen and Goodman, 1998). The target side of the parallel corpus and the monolingual German News Crawl corpora are employed as training data. We use the SRILM toolkit (Stolcke, 2002) to train the LM and rely on KenLM (Heafield, 2011) for language model scoring during decoding. Model weights are optimized to maximize BLEU (Papineni et al., 2002) with batch MIRA (Cherry and Foster, 2012) on 1000-best lists. We selected 2000 sentences from the newstest2008- 2012 sets as a development set. The selected sentences obtained high sentence-level BLEU scores when being translated with a baseline phrasebased system, and do each contain less than 30 words for more rapid tuning. newstest2013 and newstest2014 are used as unseen test sets. Translation quality is measured in truecase wit</context>
</contexts>
<marker>Stolcke, 2002</marker>
<rawString>Andreas Stolcke. 2002. SRILM – an Extensible Language Modeling Toolkit. In Proc. of the Int. Conf. on Spoken Language Processing (ICSLP), volume 3, Denver, CO, USA, September.</rawString>
</citation>
<citation valid="true">
<authors>
<author>Ashish Venugopal</author>
<author>Andreas Zollmann</author>
<author>Noah A Smith</author>
<author>Stephan Vogel</author>
</authors>
<title>Preference Grammars: Softening Syntactic Constraints to Improve Statistical Machine Translation.</title>
<date>2009</date>
<booktitle>In Proc. of the Human Language Technology Conf. /North American Chapter of the Assoc. for Computational Linguistics (HLT-NAACL),</booktitle>
<pages>236--244</pages>
<location>Boulder, CO, USA,</location>
<contexts>
<context position="8763" citStr="Venugopal et al. (2009)" startWordPosition="1376" endWordPosition="1379"> this node. The closure of a span is the smallest interval of source sentence positions that covers the span. 149 mark the sentence start (“&lt;s&gt;”) and the sentence end (“&lt;/s&gt;”), the rules in the glue grammar are of the following form: Initial rule: X,Q → h&lt;s&gt; X∼0,&lt;s&gt; Q∼0i Glue rules: X,Q → hX∼0X∼1,Q∼0B∼1i for all B ∈ NE Final rule: X,Q → hX∼0 &lt;/s&gt;,Q∼0 &lt;/s&gt;i Top rules: X,Q → h&lt;s&gt; X∼0 &lt;/s&gt;,&lt;s&gt; B∼0 &lt;/s&gt;i for all B ∈ NE 5 Preference Grammars Preference grammars store a set of implicit label vectors as additional information with each SCFG rule, along with their relative frequencies given the rule. Venugopal et al. (2009) have introduced this technique for hierarchical phrase-based translation. The implicit label set refines the label set of the underlying synchronous context-free grammar. We apply this idea to GHKM translation by not decorating the target-side non-terminals of the extracted GHKM rules with syntactic labels, but with a single generic label. The (explicit) target non-terminal vocabulary NE thus also contains only the generic non-terminal symbol X, just like the source non-terminal vocabulary NF. The extraction method remains syntax-directed and is still guided by the syntactic annotation over t</context>
</contexts>
<marker>Venugopal, Zollmann, Smith, Vogel, 2009</marker>
<rawString>Ashish Venugopal, Andreas Zollmann, Noah A. Smith, and Stephan Vogel. 2009. Preference Grammars: Softening Syntactic Constraints to Improve Statistical Machine Translation. In Proc. of the Human Language Technology Conf. /North American Chapter of the Assoc. for Computational Linguistics (HLT-NAACL), pages 236–244, Boulder, CO, USA, June.</rawString>
</citation>
<citation valid="true">
<authors>
<author>Wei Wang</author>
<author>Kevin Knight</author>
<author>Daniel Marcu</author>
</authors>
<title>Binarizing Syntax Trees to Improve Syntax-Based Machine Translation Accuracy.</title>
<date>2007</date>
<booktitle>In Proc. of the 2007 Joint Conf. on Empirical Methods in Natural Language Processing and Computational Natural Language Learning (EMNLP-CoNLL),</booktitle>
<pages>746--754</pages>
<location>Prague, Czech Republic,</location>
<contexts>
<context position="18541" citStr="Wang et al., 2007" startWordPosition="3009" endWordPosition="3012">p on Statistical Machine Translation shared translation task: Europarl (Koehn, 2005), News Commentary, and the Common Crawl corpus as provided on the WMT website. Word alignments are created by aligning the data in both directions with MGIZA++ (Gao and Vogel, 2008) and symmetrizing the two trained alignments (Och and Ney, 2003; Koehn et al., 2003). The German target side training data is parsed with BitPar (Schmid, 2004). We remove grammatical case and function information from the annotation obtained with BitPar and apply right binarization of the German parse trees prior to rule extraction (Wang et al., 2007; Wang et al., 2010; Nadejde et al., 2013). For the soft source syntactic constraints, we parse the English source side of the parallel data with the English Berkeley Parser (Petrov et al., 2006) and produce composite SAMT-style labels as discussed in Section 6. When extracting syntactic rules, we impose several restrictions for composed rules, in particular a maximum number of 100 tree nodes per rule, a maximum depth of seven, and a maximum size of seven. We discard rules with non-terminals on their right-hand side if they are singletons in the training data. For efficiency reasons, we also e</context>
</contexts>
<marker>Wang, Knight, Marcu, 2007</marker>
<rawString>Wei Wang, Kevin Knight, and Daniel Marcu. 2007. Binarizing Syntax Trees to Improve Syntax-Based Machine Translation Accuracy. In Proc. of the 2007 Joint Conf. on Empirical Methods in Natural Language Processing and Computational Natural Language Learning (EMNLP-CoNLL), pages 746–754, Prague, Czech Republic, June.</rawString>
</citation>
<citation valid="true">
<authors>
<author>Wei Wang</author>
<author>Jonathan May</author>
<author>Kevin Knight</author>
<author>Daniel Marcu</author>
</authors>
<title>Re-structuring, Re-labeling, and Re-aligning for Syntax-based Machine Translation.</title>
<date>2010</date>
<journal>Computational Linguistics,</journal>
<volume>36</volume>
<issue>2</issue>
<contexts>
<context position="18560" citStr="Wang et al., 2010" startWordPosition="3013" endWordPosition="3016">chine Translation shared translation task: Europarl (Koehn, 2005), News Commentary, and the Common Crawl corpus as provided on the WMT website. Word alignments are created by aligning the data in both directions with MGIZA++ (Gao and Vogel, 2008) and symmetrizing the two trained alignments (Och and Ney, 2003; Koehn et al., 2003). The German target side training data is parsed with BitPar (Schmid, 2004). We remove grammatical case and function information from the annotation obtained with BitPar and apply right binarization of the German parse trees prior to rule extraction (Wang et al., 2007; Wang et al., 2010; Nadejde et al., 2013). For the soft source syntactic constraints, we parse the English source side of the parallel data with the English Berkeley Parser (Petrov et al., 2006) and produce composite SAMT-style labels as discussed in Section 6. When extracting syntactic rules, we impose several restrictions for composed rules, in particular a maximum number of 100 tree nodes per rule, a maximum depth of seven, and a maximum size of seven. We discard rules with non-terminals on their right-hand side if they are singletons in the training data. For efficiency reasons, we also enforce a limit on t</context>
</contexts>
<marker>Wang, May, Knight, Marcu, 2010</marker>
<rawString>Wei Wang, Jonathan May, Kevin Knight, and Daniel Marcu. 2010. Re-structuring, Re-labeling, and Re-aligning for Syntax-based Machine Translation. Computational Linguistics, 36(2):247–277, June.</rawString>
</citation>
<citation valid="true">
<authors>
<author>Philip Williams</author>
<author>Philipp Koehn</author>
</authors>
<title>GHKM Rule Extraction and Scope-3 Parsing in Moses.</title>
<date>2012</date>
<booktitle>In Proc. of the Workshop on Statistical Machine Translation (WMT),</booktitle>
<pages>388--394</pages>
<location>Montréal, Canada,</location>
<contexts>
<context position="17528" citStr="Williams and Koehn, 2012" startWordPosition="2851" endWordPosition="2854">f source labels. If such a core set is specified, then only those sparse features are active that depend on the identity of labels within this set. All sparse features for source labels outside of the core set are inactive. 7 Experiments We empirically evaluate the effectiveness of preference grammars and soft source syntactic constraints for GHKM translation on the English→German language pair using the standard newstest sets of the Workshop on Statistical Machine Translation (WMT) for testing.4 The experiments are conducted with the open-source Moses implementations of GHKM rule extraction (Williams and Koehn, 2012) and decoding with CYK+ parsing and cube pruning (Hoang et al., 2009). 4http://www.statmt.org/wmt14/ translation-task.html 7.1 Experimental Setup We work with an English–German parallel training corpus of around 4.5 M sentence pairs (after corpus cleaning). The parallel data originates from three different sources which have been eligible for the constrained track of the ACL 2014 Ninth Workshop on Statistical Machine Translation shared translation task: Europarl (Koehn, 2005), News Commentary, and the Common Crawl corpus as provided on the WMT website. Word alignments are created by aligning t</context>
</contexts>
<marker>Williams, Koehn, 2012</marker>
<rawString>Philip Williams and Philipp Koehn. 2012. GHKM Rule Extraction and Scope-3 Parsing in Moses. In Proc. of the Workshop on Statistical Machine Translation (WMT), pages 388–394, Montréal, Canada, June.</rawString>
</citation>
<citation valid="true">
<authors>
<author>Philip Williams</author>
<author>Rico Sennrich</author>
<author>Maria Nadejde</author>
<author>Matthias Huck</author>
<author>Eva Hasler</author>
<author>Philipp Koehn</author>
</authors>
<title>Edinburgh’s Syntax-Based Systems at WMT</title>
<date>2014</date>
<booktitle>In Proc. of the Workshop on Statistical Machine Translation (WMT),</booktitle>
<pages>207--214</pages>
<location>Baltimore, MD, USA,</location>
<contexts>
<context position="3632" citStr="Williams et al., 2014" startWordPosition="529" endWordPosition="532">n results (7.2). We conclude the paper in Section 8. 3 Related Work Our syntactic translation model conforms to the GHKM syntax approach as proposed by Galley, Hopkins, Knight, and Marcu (Galley et al., 2004) with composed rules as in (Galley et al., 2006) and (DeNeefe et al., 2007). Systems based on 148 Proceedings of SSST-8, Eighth Workshop on Syntax, Semantics and Structure in Statistical Translation, pages 148–156, October 25, 2014, Doha, Qatar. c�2014 Association for Computational Linguistics this paradigm have recently been among the topranked submissions to public evaluation campaigns (Williams et al., 2014; Bojar et al., 2014). Our soft source syntactic constraints features borrow ideas from Marton and Resnik (2008) who proposed a comparable approach for hierarchical machine translation. The major difference is that the features of Marton and Resnik (2008) are only based on the labels from the input trees as seen in tuning and decoding. They penalize violations of constituent boundaries but do not employ syntactic parse annotation of the source side of the training data. We, in contrast, equip the rules with latent source label properties, allowing for features that can check for conformance of</context>
<context position="21155" citStr="Williams et al., 2014" startWordPosition="3443" endWordPosition="3446">4 19.9 63.4 19.4 65.6 string-to-string (GHKM syntax-directed rule extraction) 33.8 48.0 19.3 63.8 18.7 66.2 + preference grammar 33.9 47.7 19.3 63.7 18.8 66.0 + soft source syntactic constraints 34.6 47.0 19.8 62.9 19.5 65.2 + drop derivations with tsyn(d) = 0 34.0 47.5 19.7 63.0 18.8 65.8 Table 2: English→German experimental results (truecase). BLEU scores are given in percentage. A selection of 2000 sentences from the newstest2008-2012 sets is used as development set. model, a rule rareness penalty, and the monolingual PCFG probability of the tree fragment from which the rule was extracted (Williams et al., 2014). Rule translation probabilities are smoothed via Good-Turing smoothing. The language model (LM) is a large interpolated 5-gram LM with modified Kneser-Ney smoothing (Kneser and Ney, 1995; Chen and Goodman, 1998). The target side of the parallel corpus and the monolingual German News Crawl corpora are employed as training data. We use the SRILM toolkit (Stolcke, 2002) to train the LM and rely on KenLM (Heafield, 2011) for language model scoring during decoding. Model weights are optimized to maximize BLEU (Papineni et al., 2002) with batch MIRA (Cherry and Foster, 2012) on 1000-best lists. We </context>
</contexts>
<marker>Williams, Sennrich, Nadejde, Huck, Hasler, Koehn, 2014</marker>
<rawString>Philip Williams, Rico Sennrich, Maria Nadejde, Matthias Huck, Eva Hasler, and Philipp Koehn. 2014. Edinburgh’s Syntax-Based Systems at WMT 2014. In Proc. of the Workshop on Statistical Machine Translation (WMT), pages 207–214, Baltimore, MD, USA, June.</rawString>
</citation>
<citation valid="true">
<authors>
<author>Jiajun Zhang</author>
<author>Feifei Zhai</author>
<author>Chengqing Zong</author>
</authors>
<title>Augmenting String-to-Tree Translation Models with Fuzzy Use of Source-side Syntax.</title>
<date>2011</date>
<booktitle>In Proc. of the Conf. on Empirical Methods for Natural Language Processing (EMNLP),</booktitle>
<pages>204--215</pages>
<location>Edinburgh, Scotland, UK,</location>
<contexts>
<context position="4470" citStr="Zhang et al., 2011" startWordPosition="662" endWordPosition="665">features of Marton and Resnik (2008) are only based on the labels from the input trees as seen in tuning and decoding. They penalize violations of constituent boundaries but do not employ syntactic parse annotation of the source side of the training data. We, in contrast, equip the rules with latent source label properties, allowing for features that can check for conformance of input tree labels and source labels that have been seen in training. Other groups have applied similar techniques to a string-to-dependency system (Huang et al., 2013) and—like in our work—a GHKM stringto-tree system (Zhang et al., 2011). Both Huang et al. (2013) and Zhang et al. (2011) store source labels as additional information with the rules. They however investigate somewhat different feature functions than we do. Marton and Resnik (2008) evaluated their method on the NIST Chinese→English and Arabic→English tasks. Huang et al. (2013) and Zhang et al. (2011) present results on the NIST Chinese→English task. We focus our attention on a very different task: English→German. 4 Syntax-based Translation In syntax-based translation, a probabilistic synchronous context-free grammar (SCFG) is induced from bilingual training corpo</context>
</contexts>
<marker>Zhang, Zhai, Zong, 2011</marker>
<rawString>Jiajun Zhang, Feifei Zhai, and Chengqing Zong. 2011. Augmenting String-to-Tree Translation Models with Fuzzy Use of Source-side Syntax. In Proc. of the Conf. on Empirical Methods for Natural Language Processing (EMNLP), pages 204–215, Edinburgh, Scotland, UK, July.</rawString>
</citation>
<citation valid="true">
<authors>
<author>Andreas Zollmann</author>
<author>Ashish Venugopal</author>
</authors>
<title>Syntax Augmented Machine Translation via Chart Parsing.</title>
<date>2006</date>
<booktitle>In Proc. of the Workshop on Statistical Machine Translation (WMT),</booktitle>
<pages>138--141</pages>
<location>New York City, NY, USA,</location>
<contexts>
<context position="13155" citStr="Zollmann and Venugopal (2006)" startWordPosition="2107" endWordPosition="2110">of the stringto-tree system remains untouched. The target nonterminals of the SCFG stay syntactified, and the source non-terminal vocabulary is not extended beyond the single generic non-terminal. Source-side syntactic labels are an additional latent property of the rules. We obtain this property by parsing the source side of the training data and collecting the source labels that cover the sourceside span of non-terminals during GHKM rule extraction. As the source-side span is frequently not covered by a constituent in the syntactic parse tree, we employ the composite symbols as suggested by Zollmann and Venugopal (2006) for the SAMT system.3 In cases where a span is still not covered by a symbol, we nevertheless memorize a sourceside syntactic label vector but indicate the failure for the uncovered non-terminal with a special label. The set of source label vectors that are seen with a rule during extraction is stored with it in the rule table as an additional property. This information can be used to implement feature-based soft source syntactic constraints. Table 1 shows an example of a set of source label vectors stored with a grammar rule. The first element of each vector is an implicit sourcesyntactic la</context>
</contexts>
<marker>Zollmann, Venugopal, 2006</marker>
<rawString>Andreas Zollmann and Ashish Venugopal. 2006. Syntax Augmented Machine Translation via Chart Parsing. In Proc. of the Workshop on Statistical Machine Translation (WMT), pages 138–141, New York City, NY, USA, June.</rawString>
</citation>
</citationList>
</algorithm>
</algorithms>
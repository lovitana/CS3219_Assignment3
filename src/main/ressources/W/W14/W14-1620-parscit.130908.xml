<?xml version="1.0" encoding="UTF-8"?>
<algorithms version="110505">
<algorithm name="SectLabel" version="110505">
<variant no="0" confidence="0.004463">
<title confidence="0.996538">
A Rudimentary Lexicon and Semantics Help Bootstrap Phoneme
Acquisition
</title>
<author confidence="0.958702">
Abdellah Fourtassi Emmanuel Dupoux
</author>
<affiliation confidence="0.91203">
Laboratoire de Sciences Cognitives et Psycholinguistique, ENS/EHESS/CNRS, Paris
</affiliation>
<email confidence="0.997057">
{abdellah.fourtassi, emmanuel.dupoux}@gmail.com
</email>
<sectionHeader confidence="0.993875" genericHeader="abstract">
Abstract
</sectionHeader>
<bodyText confidence="0.99999034">
Infants spontaneously discover the rele-
vant phonemes of their language without
any direct supervision. This acquisition
is puzzling because it seems to require
the availability of high levels of linguistic
structures (lexicon, semantics), that logi-
cally suppose the infants having a set of
phonemes already. We show how this cir-
cularity can be broken by testing, in real-
size language corpora, a scenario whereby
infants would learn approximate represen-
tations at all levels, and then refine them in
a mutually constraining way. We start with
corpora of spontaneous speech that have
been encoded in a varying number of de-
tailed context-dependent allophones. We
derive, in an unsupervised way, an approx-
imate lexicon and a rudimentary seman-
tic representation. Despite the fact that
all these representations are poor approxi-
mations of the ground truth, they help re-
organize the fine grained categories into
phoneme-like categories with a high de-
gree of accuracy.
One of the most fascinating facts about human
infants is the speed at which they acquire their
native language. During the first year alone, i.e.,
before they are able to speak, infants achieve im-
pressive landmarks regarding three key language
components. First, they tune in on the phone-
mic categories of their language (Werker and Tees,
1984). Second, they learn to segment the continu-
ous speech stream into discrete units (Jusczyk and
Aslin, 1995). Third, they start to recognize fre-
quent words (Ngon et al., 2013), as well as the
semantics of many of them (Bergelson and Swing-
ley, 2012).
Even though these landmarks have been doc-
umented in detail over the past 40 years of re-
search, little is still known about the mechanisms
that are operative in infant’s brain to achieve such
a result. Current work in early language acquisi-
tion has proposed two competing but incomplete
hypotheses that purports to account for this stun-
ning development path. The bottom-up hypothesis
holds that infants converge onto the linguistic units
of their language through a statistical analysis over
of their input. In contrast, the top-down hypothesis
emphasizes the role of higher levels of linguistic
structure in learning the lower level units.
</bodyText>
<sectionHeader confidence="0.860826" genericHeader="method">
1 A chicken-and-egg problem
</sectionHeader>
<subsectionHeader confidence="0.993182">
1.1 Bottom-up is not enough
</subsectionHeader>
<bodyText confidence="0.999831666666667">
Several studies have documented the fact that in-
fants become attuned to the native sounds of their
language, starting at 6 months of age (see Ger-
vain &amp; Mehler, 2010 for a review). Some re-
searchers have claimed that such an early attune-
ment is due to a statistical learning mechanism that
only takes into account the distributional prop-
erties of the sounds present in the native input
(Maye et al., 2002). Unsupervised clustering al-
gorithms running on simplified input have, indeed,
provided a proof of principle for bottom-up learn-
ing of phonemic categories from speech (see for
instance Vallabha et al., 2007).
It is clear, however, that distributional learning
cannot account for the entire developmental pat-
tern. In fact, phoneme tokens in real speech ex-
hibit high acoustic variability and result in phone-
mic categories with a high degree of overlap (Hil-
lenbrand et al., 1995). When purely bottom up
clustering algorithms are tested on realistic input,
they ended up in either a too large number of sub-
phonemic units (Varadarajan et al., 2008) or a too
small number of coarse grained categories (Feld-
man et al., 2013a).
</bodyText>
<page confidence="0.981073">
191
</page>
<note confidence="0.7043125">
Proceedings of the Eighteenth Conference on Computational Language Learning, pages 191–200,
Baltimore, Maryland USA, June 26-27 2014. c�2014 Association for Computational Linguistics
</note>
<subsectionHeader confidence="0.920308">
1.2 The top-down hypothesis
</subsectionHeader>
<bodyText confidence="0.999983342105263">
Inspection of the developmental data shows that
infants do not wait to have completed the acqui-
sition of their native phonemes to start to learn
words. In fact, lexical and phonological acquisi-
tion largely overlap. Infant can recognize highly
frequent word forms like their own names, by as
early as 4 months of age (Mandel et al., 1995).
Vice versa, the refinement of phonemic categories
does not stop at 12 months. The sensitivity to pho-
netic contrasts has been reported to continue at 3
years of age (Nittrouer, 1996) and beyond (Hazan
and Barrett, 2000), on par with the development of
the lexicon.
Some researchers have therefore suggested that
there might be a learning synergy which allows in-
fants to base some of their acquisition not only on
bottom up information, but also on statistics over
lexical items or even on the basis of word mean-
ing (Feldman et al., 2013a; Feldman et al., 2013b;
Yeung and Werker, 2009)
These experiments and computational models,
however, have focused on simplified input or/and
used already segmented words. It remains to be
shown whether the said top-down strategies scale
up when real size corpora and more realistic repre-
sentations are used. There are indeed indications
that, in the absence of a proper phonological repre-
sentation, lexical learning becomes very difficult.
For example, word segmentation algorithms that
work on the basis of phoneme-like units tend to
degrade quickly if phonemes are replaced by con-
textual allophones (Boruta et al., 2011) or with the
output of phone recognizers (Jansen et al., 2013;
Ludusan et al., 2014).
In brief, we are facing a chicken-and-egg prob-
lem: lexical and semantic information could help
to learn the phonemes, but phonemes are needed
to acquire lexical information.
</bodyText>
<subsectionHeader confidence="0.8998255">
1.3 Breaking the circularity: An incremental
discovery procedure
</subsectionHeader>
<bodyText confidence="0.999985818181818">
Here, we explore the idea that instead of learning
adult-like hierarchically organized representations
in a sequential fashion (phonemes, words, seman-
tics), infants learn approximate, provisional lin-
guistic representations in parallel. These approxi-
mate representations are subsequently used to im-
prove each other.
More precisely, we make four assumptions.
First, we assume that infants start by paying atten-
tion to fine grained variation in the acoustic input,
thus constructing perceptual phonetic categories
that are not phonemes, but segments encoding fine
grained phonetic details (Werker and Curtin, 2005;
Pierrehumbert, 2003). Second, we assume that
these units enable infants to segment proto-words
from continuous speech and store them in this de-
tailed format. Importantly, this proto-lexicon will
not be adult-like: it will contain badly segmented
word forms, and store several alternant forms for
the same word. Ngon et al. (2013) have shown
that 11 month old infants recognize frequent sound
sequences that do not necessarily map to adult
words. Third, we assume that infants can use this
imperfect lexicon to acquire some semantic repre-
sentation. As shown in Shukla et al. (2011), in-
fants can simultaneously segment words and asso-
ciate them with a visual referent. Fourth, we as-
sume that as their exposure to language develops,
infants reorganize these initial categories along the
relevant dimensions of their native language based
on cues from all these representations.
The aim of this work is to provide a proof of
principle for this general scenario, using real size
corpora in two typologically different languages,
and state-of-the-art learning algorithms.
The paper is organized as follows. We begin
by describing how we generated the input and
how we modeled different levels of representation.
Then, we explain how information from the higher
levels (word forms and semantics) can be used to
refine the learning of the lower level (phonetic cat-
egories). Next, we present the results of our sim-
ulations and discuss the potential implications for
the language learning process.
</bodyText>
<sectionHeader confidence="0.727319" genericHeader="method">
2 Modeling the representations
</sectionHeader>
<bodyText confidence="0.99982025">
Here, we describe how we model different levels
of representation (phonetic categories, lexicon and
semantics) starting from raw speech in English
and Japanese.
</bodyText>
<subsectionHeader confidence="0.977649">
2.1 Corpus
</subsectionHeader>
<bodyText confidence="0.99996725">
We use two speech corpora: the Buckeye Speech
corpus (Pitt et al., 2007), which contains 40 hours
of spontaneous conversations in American En-
glish, and the 40 hours core of the Corpus of Spon-
taneous Japanese (Maekawa et al., 2000), which
contains spontaneous conversations and public
speeches in different fields, ranging from engi-
neering to humanities. Following Boruta (2012),
</bodyText>
<page confidence="0.997384">
192
</page>
<bodyText confidence="0.998607">
we use an inventory of 25 phonemes for transcrib-
ing Japanese, and for English, we use the set of 45
phonemes in the phonemic transcription of Pitt et
al. (2007).
</bodyText>
<subsectionHeader confidence="0.999385">
2.2 Phonetic categories
</subsectionHeader>
<bodyText confidence="0.927783533333333">
Here, we describe how we model the percep-
tual phonetic categories infants learn in a first
step before converging on the functional cate-
gories (phonemes). We make the assumption that
these initial categories correspond to fine grained
allophones, i.e., different systematic realizations
of phonemes, depending on context. Allophonic
variation can range from categorical effects due to
phonological rules to gradient effects due to coar-
ticulation, i.e, the phenomenon whereby adjacent
sounds affect the physical realization of a given
phoneme. An example of a rather categorical allo-
phonic rule is given by /r/ devoicing in French:
� [X] / before a voiceless obstruent
/r/ →[x] elsewhere
</bodyText>
<figureCaption confidence="0.996866">
Figure 1: Allophonic variation of French /r/
</figureCaption>
<bodyText confidence="0.999969790697675">
The phoneme /r/ surfaces as voiced ([x]) be-
fore a voiced obstruent like in [kanax Son] (“ca-
nard jaune”, yellow duck) and as voiceless ([X])
before a voiceless obstruent as in [kanaX puXpx]
(“canard pourpre”, purple duck). The challenge
facing the leaner is, therefore, to distinguish pairs
of segments that are in an allophonic relationship
([x], [X]) from pairs that are two distinct phonemes
and can carry a meaning difference ([x],[l]).
Previous work has generated allophonic varia-
tion artificially (Martin et al., 2013). Here, we fol-
low Fourtassi et al. (2014b) in using a linguisti-
cally and statistically controlled method, starting
from audio recordings and using a standard Hid-
den Markov Models (HMM) phone recognizer to
generate them, as follows.
We convert the raw speech waveform into suc-
cessive 10ms frames containing a vector of Mel
Frequency Cepstrum Coefficients (MFCC). We
use 12 MFC coefficients (plus the energy) com-
puted over a 25ms window, to which we add the
first and second order derivatives, yielding 39 di-
mensions per frame.
The HMM training starts with one three-state
model per phoneme. Each state is modeled by
a mixture of 17 diagonal Gaussians. After train-
ing, each phoneme model is cloned into context-
dependent triphone models, for each context in
which the phoneme actually occurs (for example,
the phoneme /A/ occurs in the context [d–A–g] as
in the word /dAg/ (“dog”). The triphone models
cloned from the phonemes are then retrained, but,
this time, only on the relevant subset of the data,
corresponding to the given triphone context. Fi-
nally, these detailed models are clustered back into
inventories of various sizes (from 2 to 20 times
the size of the phonemic inventory) and retrained.
Clustering is done state by state using a phonetic
feature-based decision tree, and results in tying
together the HMM states of linguistically simi-
lar triphones so as to maximize the likelihood of
the data. The HMM were built using the HMM
Toolkit (HTK: Young et al., 2006).
</bodyText>
<subsectionHeader confidence="0.996116">
2.3 The proto-lexicon
</subsectionHeader>
<bodyText confidence="0.999996304347826">
Finding word boundaries in the continuous se-
quence of phones is part of the problem infants
have to solve without direct supervision. We
model this segmentation using a state-of-the-art
unsupervised word segmentation model based on
the Adaptor Grammar framework (Johnson et al.,
2007). The input consists of a phonetic transcrip-
tion of the corpus, with boundaries between words
eliminated (we vary this transcription to corre-
spond to different inventories with different granu-
larity in the allophonic representation as explained
above). The model tries to reconstruct the bound-
aries based on a Pitman-Yor process (Pitman and
Yor, 1997), which uses a language-general sta-
tistical learning process to find a compact rep-
resentation of the input. The algorithm stores
high frequency chunks and re-uses them to parse
novel utterances. We use a grammar which learns
a hierarchy of three levels of chunking and use
the intermediate level to correspond to the lexi-
cal level. This grammar was shown by Fourtassi
et al. (2013) to avoid both over-segmentation and
under-segmentation.
</bodyText>
<subsectionHeader confidence="0.993818">
2.4 The proto-semantics
</subsectionHeader>
<bodyText confidence="0.999939">
It has been shown that infants can keep track of co-
occurrence statistics (see Lany and Saffran (2013)
for a review). This ability can be used to develop a
sense of semantic similarity as suggested by Har-
ris (1954). The intuition behind the distributional
hypothesis is that words that are similar in mean-
ing occur in similar contexts. In order to model
the acquisition of this semantic similarity from a
</bodyText>
<page confidence="0.995664">
193
</page>
<bodyText confidence="0.999981478260869">
transcribed and segmented corpus, we use one of
the simplest and most commonly used distribu-
tional semantic models, Latent Semantic Analysis
(LSA: Landauer &amp; Dumais, 1997). The LSA al-
gorithm takes as input a matrix consisting of rows
representing word types and columns represent-
ing contexts in which tokens of the word type oc-
cur. A context is defined as a fixed number of
utterances. Singular value decomposition (a kind
of matrix factorization) is used to extract a more
compact representation. The cosine of the angle
between vectors in the resulting space is used to
measure the semantic similarity between words.
Two words have a high semantic similarity if they
have similar distributions, i.e., if they co-occur in
most contexts. The model parameters, namely the
dimension of the semantic space and the number
of utterances to be taken as defining the context
of a given word form, are set in an unsupervised
way to optimize the latent structure of the seman-
tic model (Fourtassi and Dupoux, 2013). Thus, we
use 20 utterances as a semantic window and set the
semantic space to 100 dimensions.
</bodyText>
<sectionHeader confidence="0.981656" genericHeader="method">
3 Method
</sectionHeader>
<bodyText confidence="0.999821777777778">
Here we explore whether the approximate high
level representations, built bottom-up and with-
out supervision, still contain useful information
one can use to refine the phonetic categories into
phoneme-like units. To this end, we extract po-
tential cues from the lexical and the semantic in-
formation, and test their performance in discrim-
inating allophonic contrasts from non-allophonic
(phonemic) contrasts.
</bodyText>
<subsectionHeader confidence="0.999763">
3.1 Top down cues
3.1.1 Lexical cue
</subsectionHeader>
<bodyText confidence="0.999982714285715">
The top down information from the lexicon is
based on the insight of Martin et al. (2013). It rests
on the idea that true lexical minimal pairs are not
very frequent in human languages, as compared to
minimal pairs due to mere phonological processes
(figure 1). The latter creates alternants of the same
lexical item since adjacent sounds condition the
realization of the first and final phoneme. There-
fore, finding a minimal pair of words differing in
the first or last segment (as in [kanaX] and [kanair])
is good evidence that these two phones ([ir], [X])
are allophones of one another. Conversely, if a
pair of phones is not forming any minimal pair,
it is classified as non-allophonic (phonemic).
However, this binary strategy clearly gives rise
to false alarms in the (albeit relatively rare) case
of true minimal pairs like [kanaX] (“duck”) and
[kanal] (“canal”), where ([X], [l]) will be mis-
takenly labeled as allophonic. In order to miti-
gate the problem of false alarms, we use Boruta’s
continuous version (Boruta, 2011) and we define
the lexical cue of a pair of phones Lex(x, y) as
the number of lexical minimal pairs that vary on
the first segment (xA, yA) or the last segment
(Ax, Ay). The higher this number, the more the
pair of phones is likely to be considered as allo-
phonic.
The lexical cue is consistent with experimen-
tal findings. For example Feldman et al. (2013b)
showed that 8 month-old infants pay attention
to word level information, and demonstrated that
they do not discriminate between sound contrasts
that occur in minimal pairs (as suggested by our
cue), and, conversely, discriminate contrasts that
occur in non-minimal pairs.
</bodyText>
<subsectionHeader confidence="0.885154">
3.1.2 Semantic cue
</subsectionHeader>
<bodyText confidence="0.999734035714286">
The semantic cue is based on the intuition that
true minimal pairs ([kanaX] and [kanal]) are asso-
ciated with different events, whereas alternants of
the same word ([kanaX] and [kanal]) are expected
to co-occur with similar events.
We operationalize the semantic cue associated
with a pair of phones Sem(x, y) as the average
semantic similarity between all the lexical mini-
mal pairs generated by this pair of phones. The
higher the average semantic similarity, the more
the learner is prone to classify them as allophonic.
We take as a measure of the semantic similar-
ity, the cosine of the angle between word vec-
tors of the pairs that vary on the final segment
cos(Ax, Ay) or the first segment cos(xA, yA).
This strategy is similar in principle to the phe-
nomenon of acquired distinctiveness, according
to which, pairing two target stimuli with distinct
events enhances their perceptual differentiation,
and acquired equivalence, whereby pairing two
target stimuli with the same event, impairs their
subsequent differentiation (Lawrence, 1949). In
the same vein, Yeung and Werker (2009) tested 9
month-olds english learning infants in a task that
consists in discriminating two non-native phonetic
categories. They found that infants succeeded only
when the categories co-occurred with two distinct
visual cues.
</bodyText>
<page confidence="0.993386">
194
</page>
<table confidence="0.999890428571429">
Segmentation Lexicon
English Japanese English Japanese
Allo./phon. F P R F P R F P R F P R
2 0.61 0.57 0.65 0.45 0.44 0.47 0.29 0.42 0.22 0.23 0.54 0.15
4 0.52 0.46 0.59 0.38 0.34 0.43 0.22 0.37 0.15 0.16 0.50 0.10
10 0.51 0.45 0.59 0.34 0.30 0.38 0.21 0.34 0.16 0.16 0.41 0.10
20 0.42 0.38 0.47 0.28 0.26 0.32 0.21 0.29 0.17 0.16 0.32 0.10
</table>
<tableCaption confidence="0.9981105">
Table 1 : Scores of the segmentation and the resulting lexicon, as a function of the average number of
allophones per phoneme. P=Precison, R=Recall and F=F-score.
</tableCaption>
<subsectionHeader confidence="0.810589">
3.1.3 Combined cue
</subsectionHeader>
<bodyText confidence="0.995545">
Finally, we consider the combination of both cues
in one single cue where the contextual information
(semantics) is used as a weighing scheme of the
lexical information, as follows:
</bodyText>
<equation confidence="0.998822666666667">
cos(Ax, Ay) + E cos(xA, yA)
(xA,VA)∈L&apos;
(1)
</equation>
<bodyText confidence="0.999585">
where {Ax E L} is the set of words in the lex-
icon L that end in the phone x, and {(Ax, Ay) E
L2} is the set of phonological minimal pairs in
L x L that vary on the final segment.
The lexical cue is incremented by one, for ev-
ery minimal pair. The combined cue is, instead,
incremented by one, times the cosine of the angle
between the word vectors of this pair. When the
words have similar distributions, the angle goes to
zero and the cosine goes to 1, and when the words
have orthogonal distributions, the angle goes to
90◦ and the cosine goes to 0.
The semantic information here would basically
enable us to avoid false alarms generated by poten-
tial true minimal pairs like the above-mentioned
example of ( [kanaX] and [kanal]). Such a pair will
probably score high as far as the lexical cue is con-
cerned, but it will score low on the semantic level.
Thus, by taking the combination, the model will
be less prone to mistakenly classify ([X], [l]) as al-
lophones.
</bodyText>
<subsectionHeader confidence="0.994002">
3.2 Task
</subsectionHeader>
<bodyText confidence="0.999942571428571">
For each corpus we list all possible pairs of al-
lophones. Some of these pairs are allophones of
the same phoneme (allophonic pair) and others are
allophones of different phonemes (non-allophonic
pairs). The task is a same-different classification,
whereby each of these pairs is given a score from
the cue that is being tested. A good cue gives
higher scores to allophonic pairs.
Only pairs of phones that generate at least one
lexical minimal pair are considered. Phonetic vari-
ation that does not cause lexical variation is “in-
visible” to top down strategies, and is, therefore,
more probably clustered through purely bottom up
strategies (Fourtassi et al., 2014b)
</bodyText>
<subsectionHeader confidence="0.97555">
3.3 Evaluation
</subsectionHeader>
<bodyText confidence="0.999971565217391">
We use the same evaluation procedure as Martin et
al. (2013). This is carried out by computing the as-
sociated ROC curve (varying the z-score threshold
and computing the resulting proportions of misses
and false alarms). We then derive the Area Under
the Curve (AUC), which also corresponds to the
probability that given two pairs of phones, one al-
lophonic, one not, they are correctly classified on
the basis of the score. A value of 0.5 represents
chance and a value of 1 represents perfect perfor-
mance.
In order to lessen the potential influence of the
structure of the corpus (mainly the order of the ut-
terances) on the results, we use a statistical resam-
pling scheme. The corpus is divided into small
blocks of 20 utterances each (the semantic win-
dow). In each run, we draw randomly with re-
placement from this set of blocks a sample of
the same size as the original corpus. This sam-
ple is then used to retrain the acoustic models and
generate a phonetic inventory that we used to re-
transcribe the corpus and re-compute the cues. We
report scores averaged over 5 such runs.
</bodyText>
<sectionHeader confidence="0.999612" genericHeader="evaluation">
4 Results and discussion
</sectionHeader>
<subsectionHeader confidence="0.980701">
4.1 Segmentation
</subsectionHeader>
<bodyText confidence="0.999348166666667">
We first explore how phonetic variation influences
the quality of the segmentation and the resulting
lexicon. For the evaluation, we use the same mea-
sures as Brent (1999) and Goldwater et al. (2009),
namely Segmentation Precision (P), Recall (R)
and F-score (F). Segmentation precision is defined
</bodyText>
<equation confidence="0.9655145">
Comb(x, y) = F,
(Ax,AV)∈L&apos;
</equation>
<page confidence="0.989056">
195
</page>
<bodyText confidence="0.9999108">
as the number of correct word tokens found, out of
all tokens posited. Recall is the number of correct
word tokens found, out of all tokens in the ideal
segmentation. The F-score is defined as the har-
monic mean of Precision and Recall:
</bodyText>
<equation confidence="0.999076">
2 * P * R
P + R
</equation>
<bodyText confidence="0.99991205882353">
We define similar measures for word types (lex-
icon). Table 1 shows the scores as a function of
the number of allophones per phonemes. For both
corpora, the segmentation performance decreases
as we increase the number of allophones. As for
the lexicon, the recall scores show that only 15
to 22% of the ’words’ found by the algorithm in
the English corpus are real words; in Japanese,
this number is even lower (between 10 and 15%).
This pattern can be attributed in part to the fact
that increasing the number of allophones increases
the number of word forms, which occur therefore
with less frequency, making the statistical learn-
ing harder. Table 2 shows the average number of
word forms per word as a function of the average
number of allophones per phoneme, in the case of
ideal segmentation.
</bodyText>
<table confidence="0.991709833333333">
Allo./Phon. W. forms/Word
English Japanese
2 1.56 1.20
4 2.03 1.64
10 2.69 2.11
20 3.47 2.83
</table>
<tableCaption confidence="0.632602">
Table 2 : Average number of word-forms per
word as a function of the average number of
allophones per phoneme.
</tableCaption>
<bodyText confidence="0.999831142857143">
Another effect seen in Table 1 is the lower
overall performance of Japanese compared to En-
glish. This difference was shown by Fourtassi et
al. (2013) to be linked to the intrinsic segmenta-
tion ambiguity of Japanese, caused by the fact that
Japanese words contain more syllables compared
to English.
</bodyText>
<subsectionHeader confidence="0.774635">
4.2 Allophonic vs phonemic status of sound
contrasts
</subsectionHeader>
<bodyText confidence="0.98865852631579">
Here we test the performance of the cues described
above, in discriminating between allophonic con-
trasts from phonemic ones. We vary the number
of allophones per phoneme, on the one hand (Fig-
ure 2a), and the amount of data available to the
learner, on the other hand, in the case of two allo-
phones per phonemes (Figure 2b). In both situa-
tions, we compare the case wherein the lexical and
semantic cues are computed on the output of the
unsupervised segmentation (right), to the control
case where these cues are computed on the ideally
segmented speech (left).
We see that the overall accuracy of the cues is
quite high, even in the case of bad word segmen-
tation and very small amount of data.
The lexical cue is robust to extreme variation
and to the scarcity of data. Indeed, it does not seem
to vary monotonically neither with the number of
allophones, nor with the size of the corpus. The as-
sociated f-score generally remains above the value
of 0.7 (chance level is 0.5). The semantics, on
the other hand, gets better as the variability de-
creases and as the amount of data increases. This
is a natural consequence of the fact that the se-
mantic structure is more accurate with more data
and with word forms consistent enough to sustain
a reasonable co-occurrence statistics.
The comparison with the ideal segmentation,
shows, interestingly, that the semantics is more ro-
bust to segmentation errors than the lexical cue. In
fact, while the lexical strategy performs, overall,
better than the semantics under the ideal segmen-
tation, the patterns reverses as we move to a a more
realistic (unsupervised) segmentation.
These results suggest that both lexical and se-
mantic strategies can be crucial to learning the
phonemic status of phonetic categories since they
provide non-redundant information. This finding
is summarized by the combined cue which resists
to both variation and segmentation errors, overall,
better than each of the cues taken alone.
From a developmental point of view, this shows
that infants can, in principle, benefit from higher
level linguistic structures to refine their phonetic
categories, even if these structures are rudimen-
tary. Previous studies about top down strategies
have mainly emphasized the role of word forms;
the results of this work show that the semantics
can be at least as useful. Note that the notion
of semantics used here is weaker than the clas-
sic notion of referential semantics as in a word-
concept matching. The latter might, indeed, not
be fully operative at the early stages of the child
development, since it requires some advanced con-
ceptual abilities (like forming symbolic represen-
tations and understanding a speaker’s referential
F=
</bodyText>
<page confidence="0.874554">
196
</page>
<figure confidence="0.9998673125">
a)
English
English
Japanese
Allophones/Phoneme
Japanese
Ideal Unsupervised
1.0 Cues
0.9
AUC 0.8 Lexical
0.7 Semantic
0.6 Combined
0.5
Size (in hours)
Ideal
Unsupervised
Allophones/Phoneme
AUC
0.9
0.8
0.7
0.6
0.5
1.0
b)
Ideal
Unsupervised
Size (in hours)
AUC
0.9
0.8
0.7
0.6
0.5
1.0
0.9
AUC
0.8
0.7
0.6
0.5
Ideal
1.0
Unsupervised
Cues
Lexical
Semantic
Combined
</figure>
<figureCaption confidence="0.882178333333333">
Figure 2: Same-different scores (AUC) for different cues as a function of the average number of allo-
phones per phoneme (a), and as a function of the size of the corpus, in the case of two allophones per
phonemes (b). The scores are shown for both ideal and unsupervised word segmentation in English and
</figureCaption>
<bodyText confidence="0.981004076923077">
Japanese. The points show the mean scores over 5 runs. The lines are smoothed interpolations (local
regressions) through the means. The grey band shows a 95% confidence interval.
intentions) (Waxman and Gelman, 2009). What
we call the “semantics” of a word in this study, is
the general context provided by the co-occurrence
with other words. Infants have been shown to have
a powerful mechanism for tracking co-occurrence
relationships both in the speech and the visual do-
main (Lany and Saffran, 2013) . Our experiments
demonstrate that a similar mechanism could be
enough to develop a sense of semantic similarity
that can successfully be used to refine phonetic
categories.
</bodyText>
<sectionHeader confidence="0.850959" genericHeader="conclusions">
5 General discussion and future work
</sectionHeader>
<bodyText confidence="0.999842586206896">
Phonemes are abstract categories that form the ba-
sis for words in the lexicon. There is a traditional
view that they should be defined by their ability to
contrast word meanings (Trubetzkoy, 1939). Their
full acquisition, therefore, requires lexical and se-
mantic top-down information. However, since the
quality of the semantic representations depends on
the quality of the phonemic representations that
are used to build the lexicon, we face a chicken-
and-egg problem. In this paper, we proposed a
way to break the circularity by building approxi-
mate representation at all the levels.
The infants’ initial attunement to language-
specific categories was represented in a way that
mirrors the linguistic and statistical properties of
the speech closely. We showed that this de-
tailed (proto-phonemic) inventory enabled word
segmentation from continuous transcribed speech,
but, as expected, resulted in a low quality lexicon.
The poorly segmented corpus was then used to de-
rive a semantic similarity matrix between pairs of
words, based on their co-occurrence statistics. The
results showed that information from the derived
lexicon and semantics, albeit very rudimentary,
help discriminate between allophonic and phone-
mic contrasts, with a high degree of accuracy.
Thus, this works strongly support the claim that
the lexicon and semantics play a role in the re-
finement of the phonemic inventory (Feldman et
</bodyText>
<page confidence="0.995744">
197
</page>
<bodyText confidence="0.99870368627451">
al., 2013a; Frank et al., 2014), and, interestingly,
that this role remains functional under more realis-
tic assumptions (unsupervised word segmentation,
and bottom-up inferred semantics). We also found
that lexical and semantic information were not re-
dundant and could be usefully combined, the for-
mer being more resistant to the scarcity of data
and variation, and the latter being more resistant
to segmentation errors.
That being said, this work relies on the assump-
tion that infants start with initial perceptual cate-
gories (allophones), but we did not show how such
categories could be constructed from raw speech.
More work is needed to explore the robustness of
the model when these units are learned in an unsu-
pervised fashion (Lee and Glass, 2012; Huijbregts
et al., 2011; Jansen and Church, 2011; Varadarajan
et al., 2008).
This work could be seen as a proof of princi-
ple for an iterative learning algorithm, whereby
phonemes emerge from the interaction of low level
perceptual categories, word forms, and the seman-
tics (see Werker and Curtin (2005) for a similar
theoretical proposition). The algorithm has yet to
be implemented, but it has to address at least two
major issues: First, the fact that some sound pairs
are not captured by top down cues because they
do not surface as minimal word forms. For in-
stance, in English, /h/ and /N/ occur in different
syllable positions and therefore, cannot appear in
any minimal pair. Second, even if we have enough
information about how phonetic categories are or-
ganized in the perceptual space, we still need to
know how many categories are relevant in a par-
ticular language (i.e., where to stop the categoriza-
tion process).
For the first problem, Fourtassi et al. (2014b)
showed that the gap could, in principle, be filled by
bottom-up information (like acoustic similarity).
As for the second problem, a possible direction
could be found in the notion of Self-Consistency.
In fact, (Fourtassi et al., 2014a) proposed that an
optimal level of clustering is also a level that glob-
ally optimizes the predictive power of the lexicon.
Too detailed allophones result in too many syn-
onyms. Too broad classes result in too many ho-
mophones. Somewhere in the middle, the optimal
number of phonemes optimizes how lexical items
predict each other. Future work will address these
issues in more detail in order to propose a com-
plete phoneme learning algorithm.
</bodyText>
<sectionHeader confidence="0.99549" genericHeader="acknowledgments">
Acknowledgments
</sectionHeader>
<bodyText confidence="0.999741142857143">
This work was supported in part by the Euro-
pean Research Council (ERC-2011-AdG-295810
BOOTPHON), the Agence Nationale pour la
Recherche (ANR-10-LABX-0087 IEC, ANR-10-
IDEX-0001-02 PSL*), the Fondation de France,
the Ecole de Neurosciences de Paris, and the
R´egion Ile de France (DIM cerveau et pens´ee).
</bodyText>
<sectionHeader confidence="0.998415" genericHeader="references">
References
</sectionHeader>
<reference confidence="0.996839186046512">
Elika Bergelson and Daniel Swingley. 2012. At 6
to 9 months, human infants know the meanings of
many common nouns. Proceedings of the National
Academy of Sciences, 109(9).
Luc Boruta, Sharon Peperkamp, Benoit Crabb´e, and
Emmanuel Dupoux. 2011. Testing the robustness
of online word segmentation: Effects of linguistic
diversity and phonetic variation. In Proceedings of
CMCL, pages 1–9. Association for Computational
Linguistics.
Luc Boruta. 2011. Combining Indicators of Al-
lophony. In Proceedings ACL-SRW, pages 88–93.
Luc Boruta. 2012. Indicateurs d’allophonie et
de phon´emicit´e. Doctoral dissertation, Universit´e
Paris-Diderot - Paris VII.
M. Brent. 1999. An efficient, probabilistically sound
algorithm for segmentation and word discovery.
Machine Learning, 34:71–105.
N. Feldman, T. Griffiths, S. Goldwater, and J. Morgan.
2013a. A role for the developing lexicon in pho-
netic category acquisition. Psychological Review,
120(4):751–778.
N. Feldman, B. Myers, K. White, T. Griffiths, and
J. Morgan. 2013b. Word-level information influ-
ences phonetic learning in adults and infants. Cog-
nition, 127:427–438.
Abdellah Fourtassi and Emmanuel Dupoux. 2013. A
corpus-based evaluation method for distributional
semantic models. In 51st Annual Meeting of the
Association for Computational Linguistics Proceed-
ings of the Student Research Workshop, pages 165–
171, Sofia, Bulgaria. Association for Computational
Linguistics.
Abdellah Fourtassi, Benjamin B¨orschinger, Mark
Johnson, and Emmanuel Dupoux. 2013. WhyisEn-
glishsoeasytosegment? In Proceedings of CMCL,
pages 1–10. Association for Computational Linguis-
tics.
Abdellah Fourtassi, Ewan Dunbar, and Emmanuel
Dupoux. 2014a. Self-consistency as an inductive
bias in early language acquisition. In Proceedings
of the 36th annual meeting of the Cognitive Science
Society.
</reference>
<page confidence="0.992346">
198
</page>
<reference confidence="0.999198252336449">
Abdellah Fourtassi, Thomas Schatz, Balakrishnan
Varadarajan, and Emmanuel Dupoux. 2014b. Ex-
ploring the Relative Role of Bottom-up and Top-
down Information in Phoneme Learning. In Pro-
ceedings of the 52nd Annual Meeting of the Asso-
ciation for Computational Linguistics.
Stella Frank, Naomi Feldman, and Sharon Goldwater.
2014. Weak semantic context helps phonetic learn-
ing in a model of infant language acquisition. In
Proceedings of the 52nd Annual Meeting of the As-
sociation of Computational Linguistics.
Judit Gervain and Jacques Mehler. 2010. Speech per-
ception and language acquisition in the first year of
life. Annual Review of Psychology, 61:191–218.
Sharon Goldwater, Thomas L. Griffiths, and Mark
Johnson. 2009. A Bayesian framework for word
segmentation: Exploring the effects of context.
Cognition, 112(1):21–54.
Zellig Harris. 1954. Distributional structure. Word,
10(23):146–162.
Valerie Hazan and Sarah Barrett. 2000. The develop-
ment of phonemic categorization in children aged 6
to12. Journal of Phonetics, 28:377–396.
James Hillenbrand, Laura A. Getty, Michael J. Clark,
and Kimberlee Wheeler. 1995. Acoustic charac-
teristics of american english vowels. Journal of the
Acoustical Society of America, 97:3099–3109.
M. Huijbregts, M. McLaren, and D. van Leeuwen.
2011. Unsupervised acoustic sub-word unit detec-
tion for query-by-example spoken term detection. In
Proceedings of ICASSP, pages 4436–4439.
A. Jansen and K. Church. 2011. Towards unsupervised
training of speaker independent acoustic models. In
Proceedings of INTERSPEECH, pages 1693–1696.
Aren Jansen, Emmanuel Dupoux, Sharon Goldwa-
ter, Mark Johnson, Sanjeev Khudanpur, Kenneth
Church, Naomi Feldman, Hynek Hermansky, Flo-
rian Metze, Richard Rose, Mike Seltzer, Pascal
Clark, Ian McGraw, Balakrishnan Varadarajan, Erin
Bennett, Benjamin Borschinger, Justin Chiu, Ewan
Dunbar, Abdallah Fourtassi, David Harwath, Chia
ying Lee, Keith Levin, Atta Norouzian, Vijay
Peddinti, Rachel Richardson, Thomas Schatz, and
Samuel Thomas. 2013. A summary of the 2012 jhu
clsp workshop on zero resource speech technologies
and models of early language acquisition. In Pro-
ceedings of ICASSP.
Mark Johnson, Thomas L. Griffiths, and Sharon Gold-
water. 2007. Adaptor Grammars: A framework for
specifying compositional nonparametric Bayesian
models. In B. Sch¨olkopf, J. Platt, and T. Hoffman,
editors, Advances in Neural Information Processing
Systems 19, pages 641–648. MIT Press, Cambridge,
MA.
Peter W Jusczyk and Richard N Aslin. 1995. Infants’
detection of the sound patterns of words in fluent
speech. Cognitive psychology, 29(1):1–23.
Thomas K Landauer and Susan T Dumais. 1997. A
solution to Plato’s problem: The Latent Semantic
Analysis theory of acquisition, induction and rep-
resentation of knowledge. Psychological Review,
104(2):211–240.
J. Lany and J. Saffran. 2013. Statistical learning mech-
anisms in infancy. In J. Rubenstein and P. Rakic, ed-
itors, Comprehensive Developmental Neuroscience:
Neural Circuit Development and Function in the
Brain, volume 3, pages 231–248. Elsevier, Amster-
dam.
D.H. Lawrence. 1949. Acquired distinctiveness of
cues: I. transfer between discriminations on the ba-
sis of familiarity with the stimulus. Journal of Ex-
perimental Psychology, 39(6):770–784.
C. Lee and J. Glass. 2012. A nonparametric bayesian
approach to acoustic model discovery. In Proceed-
ings of the 50th Annual Meeting of the Associa-
tion for Computational Linguistics: Long Papers-
Volume 1, pages 40–49.
Bogdan Ludusan, Maarten Versteegh, Aren Jansen,
Guillaume Gravier, Xuan-Nga Cao, Mark Johnson,
and Emmanuel Dupoux. 2014. Bridging the gap be-
tween speech technology and natural language pro-
cessing: an evaluation toolbox for term discovery
systems. In Proceedings of LREC.
Kikuo Maekawa, Hanae Koiso, Sadaoki Furui, and Hi-
toshi Isahara. 2000. Spontaneous speech corpus of
japanese. In LREC, pages 947–952, Athens, Greece.
D.R. Mandel, P.W. Jusczyk, and D.B. Pisoni. 1995. In-
fants’ recognition of the sound patterns of their own
names. Psychological Science, 6(5):314–317.
Andrew Martin, Sharon Peperkamp, and Emmanuel
Dupoux. 2013. Learning phonemes with a proto-
lexicon. Cognitive Science, 37(1):103–124.
J. Maye, J. F. Werker, and L. Gerken. 2002. Infant sen-
sitivity to distributional information can affect pho-
netic discrimination. Cognition, 82:B101–B111.
C. Ngon, A. Martin, E. Dupoux, D. Cabrol, M. Duthat,
and S. Peperkamp. 2013. (non)words, (non)words,
(non)words: evidence for a protolexicon during the
first year of life. Developmental Science, 16(1):24–
34.
S. Nittrouer. 1996. Discriminability and perceptual
weighting of some acoustic cues to speech percep-
tion by 3-year-olds. Journal of Speech and Hearing
Research, 39:278–297.
J. B. Pierrehumbert. 2003. Phonetic diversity, statis-
tical learning, and acquisition of phonology. Lan-
guage and Speech, 46(2-3):115–154.
</reference>
<page confidence="0.985537">
199
</page>
<reference confidence="0.999769048780488">
J. Pitman and M. Yor. 1997. The two-parameter
Poisson-Dirichlet distribution derived from a stable
subordinator. Annals of Probability, 25:855–900.
M. A. Pitt, L. Dilley, K. Johnson, S. Kiesling, W. Ray-
mond, E. Hume, and Fosler-Lussier. 2007. Buckeye
corpus of conversational speech.
M Shukla, K White, and R Aslin. 2011. Prosody
guides the rapid mapping of auditory word forms
onto visual objects in 6-mo-old infants. Proceedings
of the NationalAcademyofSciences, 108(15):6038–
6043.
N. S. Trubetzkoy. 1939. Grundz¨uge der Phonolo-
gie (Principles of phonology). Vandenhoeck &amp;
Ruprecht, G¨ottingen, Germany.
G. K. Vallabha, J. L. McClelland, F. Pons, J. F.
Werker, and S. Amano. 2007. Unsupervised learn-
ing of vowel categories from infant-directed speech.
Proceedings of the National Academy of Sciences,
104(33):13273.
Balakrishnan Varadarajan, Sanjeev Khudanpur, and
Emmanuel Dupoux. 2008. Unsupervised learning
of acoustic sub-word units. In Proceedings of ACL-
08: HLT, Short Papers, pages 165–168. Association
for Computational Linguistics.
Sandra R. Waxman and Susan A. Gelman. 2009. Early
word-learning entails reference, not merely associa-
tions. Trends in Cognitive Sciences, 13(6):258–263.
J. F. Werker and S. Curtin. 2005. PRIMIR: A develop-
mental framework of infant speech processing. Lan-
guage Learning and Development, 1(2):197–234.
Janet F. Werker and Richard C. Tees. 1984. Cross-
language speech perception: Evidence for percep-
tual reorganization during the first year of life. In-
fant Behavior and Development, 7(1):49 – 63.
H Yeung and J Werker. 2009. Learning words’ sounds
before learning how words sound: 9-month-olds use
distinct objects as cues to categorize speech infor-
mation. Cognition, 113:234–243.
Steve J. Young, D. Kershaw, J. Odell, D. Ollason,
V. Valtchev, and P. Woodland. 2006. The HTK Book
Version 3.4. Cambridge University Press.
</reference>
<page confidence="0.996619">
200
</page>
</variant>
</algorithm>
<algorithm name="ParsHed" version="110505">
<variant no="0" confidence="0.000000">
<title confidence="0.997823">A Rudimentary Lexicon and Semantics Help Bootstrap Phoneme Acquisition</title>
<author confidence="0.997192">Abdellah Fourtassi Emmanuel Dupoux</author>
<affiliation confidence="0.873091">Laboratoire de Sciences Cognitives et Psycholinguistique, ENS/EHESS/CNRS,</affiliation>
<abstract confidence="0.999094620689655">Infants spontaneously discover the relevant phonemes of their language without any direct supervision. This acquisition is puzzling because it seems to require the availability of high levels of linguistic structures (lexicon, semantics), that logically suppose the infants having a set of phonemes already. We show how this circularity can be broken by testing, in realsize language corpora, a scenario whereby infants would learn approximate representations at all levels, and then refine them in a mutually constraining way. We start with corpora of spontaneous speech that have been encoded in a varying number of detailed context-dependent allophones. We derive, in an unsupervised way, an approximate lexicon and a rudimentary semantic representation. Despite the fact that all these representations are poor approximations of the ground truth, they help reorganize the fine grained categories into phoneme-like categories with a high degree of accuracy. One of the most fascinating facts about human infants is the speed at which they acquire their native language. During the first year alone, i.e., before they are able to speak, infants achieve impressive landmarks regarding three key language components. First, they tune in on the phonemic categories of their language (Werker and Tees, 1984). Second, they learn to segment the continuous speech stream into discrete units (Jusczyk and Aslin, 1995). Third, they start to recognize frequent words (Ngon et al., 2013), as well as the semantics of many of them (Bergelson and Swingley, 2012). Even though these landmarks have been documented in detail over the past 40 years of research, little is still known about the mechanisms that are operative in infant’s brain to achieve such a result. Current work in early language acquisition has proposed two competing but incomplete hypotheses that purports to account for this stunning development path. The bottom-up hypothesis holds that infants converge onto the linguistic units of their language through a statistical analysis over of their input. In contrast, the top-down hypothesis emphasizes the role of higher levels of linguistic structure in learning the lower level units. 1 A chicken-and-egg problem 1.1 Bottom-up is not enough Several studies have documented the fact that infants become attuned to the native sounds of their language, starting at 6 months of age (see Gervain &amp; Mehler, 2010 for a review). Some researchers have claimed that such an early attunement is due to a statistical learning mechanism that only takes into account the distributional properties of the sounds present in the native input (Maye et al., 2002). Unsupervised clustering algorithms running on simplified input have, indeed, provided a proof of principle for bottom-up learning of phonemic categories from speech (see for instance Vallabha et al., 2007). It is clear, however, that distributional learning cannot account for the entire developmental pattern. In fact, phoneme tokens in real speech exhibit high acoustic variability and result in phonemic categories with a high degree of overlap (Hillenbrand et al., 1995). When purely bottom up clustering algorithms are tested on realistic input, they ended up in either a too large number of subphonemic units (Varadarajan et al., 2008) or a too small number of coarse grained categories (Feldman et al., 2013a). 191 of the Eighteenth Conference on Computational Language pages Maryland USA, June 26-27 2014. Association for Computational Linguistics 1.2 The top-down hypothesis Inspection of the developmental data shows that infants do not wait to have completed the acquisition of their native phonemes to start to learn words. In fact, lexical and phonological acquisition largely overlap. Infant can recognize highly frequent word forms like their own names, by as early as 4 months of age (Mandel et al., 1995). Vice versa, the refinement of phonemic categories does not stop at 12 months. The sensitivity to phonetic contrasts has been reported to continue at 3 years of age (Nittrouer, 1996) and beyond (Hazan and Barrett, 2000), on par with the development of the lexicon. Some researchers have therefore suggested that there might be a learning synergy which allows infants to base some of their acquisition not only on bottom up information, but also on statistics over lexical items or even on the basis of word meaning (Feldman et al., 2013a; Feldman et al., 2013b; Yeung and Werker, 2009) These experiments and computational models, however, have focused on simplified input or/and used already segmented words. It remains to be shown whether the said top-down strategies scale up when real size corpora and more realistic representations are used. There are indeed indications that, in the absence of a proper phonological representation, lexical learning becomes very difficult. For example, word segmentation algorithms that work on the basis of phoneme-like units tend to degrade quickly if phonemes are replaced by contextual allophones (Boruta et al., 2011) or with the output of phone recognizers (Jansen et al., 2013; Ludusan et al., 2014). In brief, we are facing a chicken-and-egg problem: lexical and semantic information could help to learn the phonemes, but phonemes are needed to acquire lexical information. 1.3 Breaking the circularity: An incremental discovery procedure Here, we explore the idea that instead of learning adult-like hierarchically organized representations in a sequential fashion (phonemes, words, semantics), infants learn approximate, provisional linguistic representations in parallel. These approximate representations are subsequently used to improve each other. More precisely, we make four assumptions. First, we assume that infants start by paying attention to fine grained variation in the acoustic input, thus constructing perceptual phonetic categories that are not phonemes, but segments encoding fine grained phonetic details (Werker and Curtin, 2005; Pierrehumbert, 2003). Second, we assume that these units enable infants to segment proto-words from continuous speech and store them in this detailed format. Importantly, this proto-lexicon will not be adult-like: it will contain badly segmented word forms, and store several alternant forms for the same word. Ngon et al. (2013) have shown that 11 month old infants recognize frequent sound sequences that do not necessarily map to adult words. Third, we assume that infants can use this imperfect lexicon to acquire some semantic representation. As shown in Shukla et al. (2011), infants can simultaneously segment words and associate them with a visual referent. Fourth, we assume that as their exposure to language develops, infants reorganize these initial categories along the relevant dimensions of their native language based on cues from all these representations. The aim of this work is to provide a proof of principle for this general scenario, using real size corpora in two typologically different languages, and state-of-the-art learning algorithms. The paper is organized as follows. We begin by describing how we generated the input and how we modeled different levels of representation. Then, we explain how information from the higher levels (word forms and semantics) can be used to refine the learning of the lower level (phonetic categories). Next, we present the results of our simulations and discuss the potential implications for the language learning process. 2 Modeling the representations Here, we describe how we model different levels of representation (phonetic categories, lexicon and semantics) starting from raw speech in English and Japanese. 2.1 Corpus We use two speech corpora: the Buckeye Speech corpus (Pitt et al., 2007), which contains 40 hours of spontaneous conversations in American English, and the 40 hours core of the Corpus of Spontaneous Japanese (Maekawa et al., 2000), which contains spontaneous conversations and public speeches in different fields, ranging from engineering to humanities. Following Boruta (2012), 192 we use an inventory of 25 phonemes for transcribing Japanese, and for English, we use the set of 45 phonemes in the phonemic transcription of Pitt et al. (2007). 2.2 Phonetic categories Here, we describe how we model the perceptual phonetic categories infants learn in a first step before converging on the functional categories (phonemes). We make the assumption that these initial categories correspond to fine grained i.e., different systematic realizations of phonemes, depending on context. Allophonic variation can range from categorical effects due to phonological rules to gradient effects due to coarticulation, i.e, the phenomenon whereby adjacent sounds affect the physical realization of a given phoneme. An example of a rather categorical allophonic rule is given by /r/ devoicing in French: a voiceless obstruent elsewhere Figure 1: Allophonic variation of French /r/ phoneme /r/ surfaces as voiced bea voiced obstruent like in (“cajaune”, yellow duck) and as voiceless a voiceless obstruent as in (“canard pourpre”, purple duck). The challenge facing the leaner is, therefore, to distinguish pairs of segments that are in an allophonic relationship from pairs that are two distinct phonemes can carry a meaning difference Previous work has generated allophonic variation artificially (Martin et al., 2013). Here, we follow Fourtassi et al. (2014b) in using a linguistically and statistically controlled method, starting from audio recordings and using a standard Hidden Markov Models (HMM) phone recognizer to generate them, as follows. We convert the raw speech waveform into successive 10ms frames containing a vector of Mel Frequency Cepstrum Coefficients (MFCC). We use 12 MFC coefficients (plus the energy) computed over a 25ms window, to which we add the first and second order derivatives, yielding 39 dimensions per frame. The HMM training starts with one three-state model per phoneme. Each state is modeled by a mixture of 17 diagonal Gaussians. After training, each phoneme model is cloned into contextdependent triphone models, for each context in which the phoneme actually occurs (for example, phoneme occurs in the context as the word (“dog”). The triphone models cloned from the phonemes are then retrained, but, this time, only on the relevant subset of the data, corresponding to the given triphone context. Finally, these detailed models are clustered back into inventories of various sizes (from 2 to 20 times the size of the phonemic inventory) and retrained. Clustering is done state by state using a phonetic feature-based decision tree, and results in tying together the HMM states of linguistically similar triphones so as to maximize the likelihood of the data. The HMM were built using the HMM Toolkit (HTK: Young et al., 2006). 2.3 The proto-lexicon Finding word boundaries in the continuous sequence of phones is part of the problem infants have to solve without direct supervision. We model this segmentation using a state-of-the-art unsupervised word segmentation model based on the Adaptor Grammar framework (Johnson et al., 2007). The input consists of a phonetic transcription of the corpus, with boundaries between words eliminated (we vary this transcription to correspond to different inventories with different granularity in the allophonic representation as explained above). The model tries to reconstruct the boundaries based on a Pitman-Yor process (Pitman and Yor, 1997), which uses a language-general statistical learning process to find a compact representation of the input. The algorithm stores high frequency chunks and re-uses them to parse novel utterances. We use a grammar which learns a hierarchy of three levels of chunking and use the intermediate level to correspond to the lexical level. This grammar was shown by Fourtassi et al. (2013) to avoid both over-segmentation and under-segmentation. 2.4 The proto-semantics It has been shown that infants can keep track of cooccurrence statistics (see Lany and Saffran (2013) for a review). This ability can be used to develop a sense of semantic similarity as suggested by Har- (1954). The intuition behind the that words that are similar in meaning occur in similar contexts. In order to model the acquisition of this semantic similarity from a 193 transcribed and segmented corpus, we use one of the simplest and most commonly used distributional semantic models, Latent Semantic Analysis (LSA: Landauer &amp; Dumais, 1997). The LSA algorithm takes as input a matrix consisting of rows representing word types and columns representing contexts in which tokens of the word type occur. A context is defined as a fixed number of utterances. Singular value decomposition (a kind of matrix factorization) is used to extract a more compact representation. The cosine of the angle between vectors in the resulting space is used to measure the semantic similarity between words. Two words have a high semantic similarity if they have similar distributions, i.e., if they co-occur in most contexts. The model parameters, namely the dimension of the semantic space and the number of utterances to be taken as defining the context of a given word form, are set in an unsupervised way to optimize the latent structure of the semantic model (Fourtassi and Dupoux, 2013). Thus, we use 20 utterances as a semantic window and set the semantic space to 100 dimensions. 3 Method Here we explore whether the approximate high level representations, built bottom-up and without supervision, still contain useful information one can use to refine the phonetic categories into phoneme-like units. To this end, we extract potential cues from the lexical and the semantic information, and test their performance in discriminating allophonic contrasts from non-allophonic (phonemic) contrasts. 3.1 Top down cues 3.1.1 Lexical cue The top down information from the lexicon is based on the insight of Martin et al. (2013). It rests on the idea that true lexical minimal pairs are not very frequent in human languages, as compared to minimal pairs due to mere phonological processes (figure 1). The latter creates alternants of the same lexical item since adjacent sounds condition the realization of the first and final phoneme. Therefore, finding a minimal pair of words differing in first or last segment (as in and good evidence that these two phones are allophones of one another. Conversely, if a pair of phones is not forming any minimal pair, it is classified as non-allophonic (phonemic). However, this binary strategy clearly gives rise to false alarms in the (albeit relatively rare) case true minimal pairs like (“duck”) and (“canal”), where will be mistakenly labeled as allophonic. In order to mitigate the problem of false alarms, we use Boruta’s continuous version (Boruta, 2011) and we define lexical cue of a pair of phones the number of lexical minimal pairs that vary on first segment the last segment The higher this number, the more the pair of phones is likely to be considered as allophonic. The lexical cue is consistent with experimental findings. For example Feldman et al. (2013b) showed that 8 month-old infants pay attention to word level information, and demonstrated that they do not discriminate between sound contrasts that occur in minimal pairs (as suggested by our cue), and, conversely, discriminate contrasts that occur in non-minimal pairs. 3.1.2 Semantic cue The semantic cue is based on the intuition that minimal pairs and are associated with different events, whereas alternants of same word and are expected to co-occur with similar events. We operationalize the semantic cue associated a pair of phones the average semantic similarity between all the lexical minimal pairs generated by this pair of phones. The higher the average semantic similarity, the more the learner is prone to classify them as allophonic. We take as a measure of the semantic similarity, the cosine of the angle between word vectors of the pairs that vary on the final segment the first segment This strategy is similar in principle to the pheof according to which, pairing two target stimuli with distinct events enhances their perceptual differentiation, whereby pairing two target stimuli with the same event, impairs their subsequent differentiation (Lawrence, 1949). In the same vein, Yeung and Werker (2009) tested 9 month-olds english learning infants in a task that consists in discriminating two non-native phonetic categories. They found that infants succeeded only when the categories co-occurred with two distinct visual cues.</abstract>
<note confidence="0.317642">194</note>
<title confidence="0.7932615">Segmentation Lexicon English Japanese English Japanese</title>
<author confidence="0.498751">F P R F P R F P R F P R</author>
<phone confidence="0.642497">2 0.61 0.57 0.65 0.45 0.44 0.47 0.29 0.42 0.22 0.23 0.54 0.15</phone>
<address confidence="0.564802">4 0.52 0.46 0.59 0.38 0.34 0.43 0.22 0.37 0.15 0.16 0.50 0.10 10 0.51 0.45 0.59 0.34 0.30 0.38 0.21 0.34 0.16 0.16 0.41 0.10</address>
<phone confidence="0.579726">20 0.42 0.38 0.47 0.28 0.26 0.32 0.21 0.29 0.17 0.16 0.32 0.10</phone>
<abstract confidence="0.960458972826088">Table 1 : Scores of the segmentation and the resulting lexicon, as a function of the average number of allophones per phoneme. P=Precison, R=Recall and F=F-score. 3.1.3 Combined cue Finally, we consider the combination of both cues in one single cue where the contextual information (semantics) is used as a weighing scheme of the lexical information, as follows: + (1) the set of words in the lexend in the phone and the set of phonological minimal pairs in vary on the final segment. The lexical cue is incremented by one, for every minimal pair. The combined cue is, instead, incremented by one, times the cosine of the angle between the word vectors of this pair. When the words have similar distributions, the angle goes to zero and the cosine goes to 1, and when the words have orthogonal distributions, the angle goes to the cosine goes to 0. The semantic information here would basically enable us to avoid false alarms generated by potential true minimal pairs like the above-mentioned of ( and Such a pair will probably score high as far as the lexical cue is concerned, but it will score low on the semantic level. Thus, by taking the combination, the model will less prone to mistakenly classify as allophones. 3.2 Task For each corpus we list all possible pairs of allophones. Some of these pairs are allophones of the same phoneme (allophonic pair) and others are allophones of different phonemes (non-allophonic pairs). The task is a same-different classification, whereby each of these pairs is given a score from the cue that is being tested. A good cue gives higher scores to allophonic pairs. Only pairs of phones that generate at least one lexical minimal pair are considered. Phonetic variation that does not cause lexical variation is “invisible” to top down strategies, and is, therefore, more probably clustered through purely bottom up strategies (Fourtassi et al., 2014b) 3.3 Evaluation We use the same evaluation procedure as Martin et al. (2013). This is carried out by computing the associated ROC curve (varying the z-score threshold and computing the resulting proportions of misses and false alarms). We then derive the Area Under the Curve (AUC), which also corresponds to the probability that given two pairs of phones, one allophonic, one not, they are correctly classified on the basis of the score. A value of 0.5 represents chance and a value of 1 represents perfect performance. In order to lessen the potential influence of the structure of the corpus (mainly the order of the utterances) on the results, we use a statistical resampling scheme. The corpus is divided into small blocks of 20 utterances each (the semantic window). In each run, we draw randomly with replacement from this set of blocks a sample of the same size as the original corpus. This sample is then used to retrain the acoustic models and generate a phonetic inventory that we used to retranscribe the corpus and re-compute the cues. We report scores averaged over 5 such runs. 4 Results and discussion 4.1 Segmentation We first explore how phonetic variation influences the quality of the segmentation and the resulting lexicon. For the evaluation, we use the same measures as Brent (1999) and Goldwater et al. (2009), namely Segmentation Precision (P), Recall (R) and F-score (F). Segmentation precision is defined = 195 as the number of correct word tokens found, out of all tokens posited. Recall is the number of correct word tokens found, out of all tokens in the ideal segmentation. The F-score is defined as the harmonic mean of Precision and Recall: P * R We define similar measures for word types (lexicon). Table 1 shows the scores as a function of the number of allophones per phonemes. For both corpora, the segmentation performance decreases as we increase the number of allophones. As for the lexicon, the recall scores show that only 15 to 22% of the ’words’ found by the algorithm in the English corpus are real words; in Japanese, this number is even lower (between 10 and 15%). This pattern can be attributed in part to the fact that increasing the number of allophones increases the number of word forms, which occur therefore with less frequency, making the statistical learning harder. Table 2 shows the average number of word forms per word as a function of the average number of allophones per phoneme, in the case of ideal segmentation. Allo./Phon. W. forms/Word English Japanese 2 1.56 1.20 4 2.03 1.64 10 2.69 2.11 20 3.47 2.83 Table 2 : Average number of word-forms per word as a function of the average number of allophones per phoneme. Another effect seen in Table 1 is the lower overall performance of Japanese compared to English. This difference was shown by Fourtassi et al. (2013) to be linked to the intrinsic segmentation ambiguity of Japanese, caused by the fact that Japanese words contain more syllables compared to English. 4.2 Allophonic vs phonemic status of sound contrasts Here we test the performance of the cues described above, in discriminating between allophonic contrasts from phonemic ones. We vary the number of allophones per phoneme, on the one hand (Figure 2a), and the amount of data available to the learner, on the other hand, in the case of two allophones per phonemes (Figure 2b). In both situations, we compare the case wherein the lexical and semantic cues are computed on the output of the unsupervised segmentation (right), to the control case where these cues are computed on the ideally segmented speech (left). We see that the overall accuracy of the cues is quite high, even in the case of bad word segmentation and very small amount of data. The lexical cue is robust to extreme variation and to the scarcity of data. Indeed, it does not seem to vary monotonically neither with the number of allophones, nor with the size of the corpus. The associated f-score generally remains above the value of 0.7 (chance level is 0.5). The semantics, on the other hand, gets better as the variability decreases and as the amount of data increases. This is a natural consequence of the fact that the semantic structure is more accurate with more data and with word forms consistent enough to sustain a reasonable co-occurrence statistics. The comparison with the ideal segmentation, shows, interestingly, that the semantics is more robust to segmentation errors than the lexical cue. In fact, while the lexical strategy performs, overall, better than the semantics under the ideal segmentation, the patterns reverses as we move to a a more realistic (unsupervised) segmentation. These results suggest that both lexical and semantic strategies can be crucial to learning the phonemic status of phonetic categories since they provide non-redundant information. This finding is summarized by the combined cue which resists to both variation and segmentation errors, overall, better than each of the cues taken alone. From a developmental point of view, this shows that infants can, in principle, benefit from higher level linguistic structures to refine their phonetic categories, even if these structures are rudimentary. Previous studies about top down strategies have mainly emphasized the role of word forms; the results of this work show that the semantics can be at least as useful. Note that the notion of semantics used here is weaker than the classic notion of referential semantics as in a wordconcept matching. The latter might, indeed, not be fully operative at the early stages of the child development, since it requires some advanced conceptual abilities (like forming symbolic representations and understanding a speaker’s referential F= 196 a) English English Japanese Allophones/Phoneme Japanese</abstract>
<note confidence="0.81992475">Ideal Unsupervised 1.0 Cues 0.9 AUC 0.8 0.7 0.6 0.5 Lexical Semantic Combined Size (in hours) Ideal Unsupervised Allophones/Phoneme AUC 0.9 0.8 0.7 0.6 0.5 1.0 b) Ideal Unsupervised Size (in hours) AUC 0.9 0.8 0.7 0.6 0.5 1.0 0.9 AUC 0.8 0.7 0.6 0.5 Ideal 1.0 Unsupervised Cues Lexical Semantic Combined Figure 2: Same-different scores (AUC) for different cues as a function of the average number of allo-</note>
<abstract confidence="0.98793875257732">phones per phoneme (a), and as a function of the size of the corpus, in the case of two allophones per phonemes (b). The scores are shown for both ideal and unsupervised word segmentation in English and Japanese. The points show the mean scores over 5 runs. The lines are smoothed interpolations (local regressions) through the means. The grey band shows a 95% confidence interval. intentions) (Waxman and Gelman, 2009). What we call the “semantics” of a word in this study, is the general context provided by the co-occurrence with other words. Infants have been shown to have a powerful mechanism for tracking co-occurrence relationships both in the speech and the visual domain (Lany and Saffran, 2013) . Our experiments demonstrate that a similar mechanism could be enough to develop a sense of semantic similarity that can successfully be used to refine phonetic categories. 5 General discussion and future work Phonemes are abstract categories that form the basis for words in the lexicon. There is a traditional view that they should be defined by their ability to contrast word meanings (Trubetzkoy, 1939). Their full acquisition, therefore, requires lexical and semantic top-down information. However, since the quality of the semantic representations depends on the quality of the phonemic representations that are used to build the lexicon, we face a chickenand-egg problem. In this paper, we proposed a way to break the circularity by building approximate representation at all the levels. The infants’ initial attunement to languagespecific categories was represented in a way that mirrors the linguistic and statistical properties of the speech closely. We showed that this detailed (proto-phonemic) inventory enabled word segmentation from continuous transcribed speech, but, as expected, resulted in a low quality lexicon. The poorly segmented corpus was then used to derive a semantic similarity matrix between pairs of words, based on their co-occurrence statistics. The results showed that information from the derived lexicon and semantics, albeit very rudimentary, help discriminate between allophonic and phonemic contrasts, with a high degree of accuracy. Thus, this works strongly support the claim that the lexicon and semantics play a role in the refinement of the phonemic inventory (Feldman et 197 al., 2013a; Frank et al., 2014), and, interestingly, that this role remains functional under more realistic assumptions (unsupervised word segmentation, and bottom-up inferred semantics). We also found that lexical and semantic information were not redundant and could be usefully combined, the former being more resistant to the scarcity of data and variation, and the latter being more resistant to segmentation errors. That being said, this work relies on the assumption that infants start with initial perceptual categories (allophones), but we did not show how such categories could be constructed from raw speech. More work is needed to explore the robustness of the model when these units are learned in an unsupervised fashion (Lee and Glass, 2012; Huijbregts et al., 2011; Jansen and Church, 2011; Varadarajan et al., 2008). This work could be seen as a proof of principle for an iterative learning algorithm, whereby phonemes emerge from the interaction of low level perceptual categories, word forms, and the semantics (see Werker and Curtin (2005) for a similar theoretical proposition). The algorithm has yet to be implemented, but it has to address at least two major issues: First, the fact that some sound pairs are not captured by top down cues because they do not surface as minimal word forms. For inin English, and occur in different syllable positions and therefore, cannot appear in any minimal pair. Second, even if we have enough information about how phonetic categories are organized in the perceptual space, we still need to know how many categories are relevant in a particular language (i.e., where to stop the categorization process). For the first problem, Fourtassi et al. (2014b) showed that the gap could, in principle, be filled by bottom-up information (like acoustic similarity). As for the second problem, a possible direction be found in the notion of In fact, (Fourtassi et al., 2014a) proposed that an optimal level of clustering is also a level that globally optimizes the predictive power of the lexicon. Too detailed allophones result in too many synonyms. Too broad classes result in too many homophones. Somewhere in the middle, the optimal number of phonemes optimizes how lexical items predict each other. Future work will address these issues in more detail in order to propose a complete phoneme learning algorithm.</abstract>
<note confidence="0.766480759259259">Acknowledgments This work was supported in part by the European Research Council (ERC-2011-AdG-295810 BOOTPHON), the Agence Nationale pour la Recherche (ANR-10-LABX-0087 IEC, ANR-10- IDEX-0001-02 PSL*), the Fondation de France, the Ecole de Neurosciences de Paris, and the R´egion Ile de France (DIM cerveau et pens´ee). References Elika Bergelson and Daniel Swingley. 2012. At 6 to 9 months, human infants know the meanings of common nouns. of the National of 109(9). Luc Boruta, Sharon Peperkamp, Benoit Crabb´e, and Emmanuel Dupoux. 2011. Testing the robustness of online word segmentation: Effects of linguistic and phonetic variation. In of pages 1–9. Association for Computational Linguistics. Luc Boruta. 2011. Combining Indicators of Al- In pages 88–93. Boruta. 2012. d’allophonie et Doctoral dissertation, Universit´e Paris-Diderot - Paris VII. M. Brent. 1999. An efficient, probabilistically sound algorithm for segmentation and word discovery. 34:71–105. N. Feldman, T. Griffiths, S. Goldwater, and J. Morgan. 2013a. A role for the developing lexicon in phocategory acquisition. 120(4):751–778. N. Feldman, B. Myers, K. White, T. Griffiths, and J. Morgan. 2013b. Word-level information influphonetic learning in adults and infants. Cog- 127:427–438. Abdellah Fourtassi and Emmanuel Dupoux. 2013. A corpus-based evaluation method for distributional models. In Annual Meeting of the Association for Computational Linguistics Proceedof the Student Research pages 165– 171, Sofia, Bulgaria. Association for Computational Linguistics. Abdellah Fourtassi, Benjamin B¨orschinger, Mark Johnson, and Emmanuel Dupoux. 2013. WhyisEn- In of pages 1–10. Association for Computational Linguistics. Abdellah Fourtassi, Ewan Dunbar, and Emmanuel Dupoux. 2014a. Self-consistency as an inductive in early language acquisition. In of the 36th annual meeting of the Cognitive Science 198 Abdellah Fourtassi, Thomas Schatz, Balakrishnan Varadarajan, and Emmanuel Dupoux. 2014b. Ex-</note>
<title confidence="0.53735975">ploring the Relative Role of Bottom-up and Top- Information in Phoneme Learning. In Proceedings of the 52nd Annual Meeting of the Assofor Computational</title>
<author confidence="0.713571">Stella Frank</author>
<author confidence="0.713571">Naomi Feldman</author>
<author confidence="0.713571">Sharon Goldwater</author>
<note confidence="0.96355175">2014. Weak semantic context helps phonetic learning in a model of infant language acquisition. In Proceedings of the 52nd Annual Meeting of the Asof Computational Judit Gervain and Jacques Mehler. 2010. Speech perception and language acquisition in the first year of Review of 61:191–218. Sharon Goldwater, Thomas L. Griffiths, and Mark Johnson. 2009. A Bayesian framework for word segmentation: Exploring the effects of context. 112(1):21–54. Harris. 1954. Distributional structure. 10(23):146–162. Valerie Hazan and Sarah Barrett. 2000. The development of phonemic categorization in children aged 6 of 28:377–396.</note>
<author confidence="0.6644335">Acoustic charac-</author>
<note confidence="0.5010906">of american english vowels. of the Society of 97:3099–3109. M. Huijbregts, M. McLaren, and D. van Leeuwen. 2011. Unsupervised acoustic sub-word unit detection for query-by-example spoken term detection. In of pages 4436–4439. A. Jansen and K. Church. 2011. Towards unsupervised training of speaker independent acoustic models. In of pages 1693–1696. Aren Jansen, Emmanuel Dupoux, Sharon Goldwa-</note>
<degree confidence="0.840009111111111">ter, Mark Johnson, Sanjeev Khudanpur, Kenneth Church, Naomi Feldman, Hynek Hermansky, Florian Metze, Richard Rose, Mike Seltzer, Pascal Clark, Ian McGraw, Balakrishnan Varadarajan, Erin Bennett, Benjamin Borschinger, Justin Chiu, Ewan Dunbar, Abdallah Fourtassi, David Harwath, Chia ying Lee, Keith Levin, Atta Norouzian, Vijay Peddinti, Rachel Richardson, Thomas Schatz, and Samuel Thomas. 2013. A summary of the 2012 jhu</degree>
<abstract confidence="0.856131424242424">clsp workshop on zero resource speech technologies models of early language acquisition. In Proof Mark Johnson, Thomas L. Griffiths, and Sharon Goldwater. 2007. Adaptor Grammars: A framework for specifying compositional nonparametric Bayesian models. In B. Sch¨olkopf, J. Platt, and T. Hoffman, in Neural Information Processing pages 641–648. MIT Press, Cambridge, MA. Peter W Jusczyk and Richard N Aslin. 1995. Infants’ detection of the sound patterns of words in fluent 29(1):1–23. Thomas K Landauer and Susan T Dumais. 1997. A solution to Plato’s problem: The Latent Semantic Analysis theory of acquisition, induction and repof knowledge. 104(2):211–240. J. Lany and J. Saffran. 2013. Statistical learning mechanisms in infancy. In J. Rubenstein and P. Rakic, ed- Developmental Neuroscience: Neural Circuit Development and Function in the volume 3, pages 231–248. Elsevier, Amsterdam. D.H. Lawrence. 1949. Acquired distinctiveness of cues: I. transfer between discriminations on the baof familiarity with the stimulus. of Ex- 39(6):770–784. C. Lee and J. Glass. 2012. A nonparametric bayesian to acoustic model discovery. In Proceedings of the 50th Annual Meeting of the Association for Computational Linguistics: Long Paperspages 40–49.</abstract>
<author confidence="0.8737665">Bogdan Ludusan</author>
<author confidence="0.8737665">Maarten Versteegh</author>
<author confidence="0.8737665">Aren Jansen</author>
<author confidence="0.8737665">Guillaume Gravier</author>
<author confidence="0.8737665">Xuan-Nga Cao</author>
<author confidence="0.8737665">Mark Johnson</author>
<abstract confidence="0.833016343283582">and Emmanuel Dupoux. 2014. Bridging the gap between speech technology and natural language processing: an evaluation toolbox for term discovery In of Kikuo Maekawa, Hanae Koiso, Sadaoki Furui, and Hitoshi Isahara. 2000. Spontaneous speech corpus of In pages 947–952, Athens, Greece. D.R. Mandel, P.W. Jusczyk, and D.B. Pisoni. 1995. Infants’ recognition of the sound patterns of their own 6(5):314–317. Andrew Martin, Sharon Peperkamp, and Emmanuel Dupoux. 2013. Learning phonemes with a proto- 37(1):103–124. J. Maye, J. F. Werker, and L. Gerken. 2002. Infant sensitivity to distributional information can affect phodiscrimination. 82:B101–B111. C. Ngon, A. Martin, E. Dupoux, D. Cabrol, M. Duthat, and S. Peperkamp. 2013. (non)words, (non)words, (non)words: evidence for a protolexicon during the year of life. 16(1):24– 34. S. Nittrouer. 1996. Discriminability and perceptual weighting of some acoustic cues to speech percepby 3-year-olds. of Speech and Hearing 39:278–297. J. B. Pierrehumbert. 2003. Phonetic diversity, statislearning, and acquisition of phonology. Lanand 46(2-3):115–154. 199 J. Pitman and M. Yor. 1997. The two-parameter Poisson-Dirichlet distribution derived from a stable of 25:855–900. M. A. Pitt, L. Dilley, K. Johnson, S. Kiesling, W. Raymond, E. Hume, and Fosler-Lussier. 2007. Buckeye corpus of conversational speech. M Shukla, K White, and R Aslin. 2011. Prosody guides the rapid mapping of auditory word forms visual objects in 6-mo-old infants. the 108(15):6038– 6043. S. Trubetzkoy. 1939. der Phonolo- (Principles of Vandenhoeck &amp; Ruprecht, G¨ottingen, Germany. G. K. Vallabha, J. L. McClelland, F. Pons, J. F. Werker, and S. Amano. 2007. Unsupervised learning of vowel categories from infant-directed speech. of the National Academy of 104(33):13273. Balakrishnan Varadarajan, Sanjeev Khudanpur, and Emmanuel Dupoux. 2008. Unsupervised learning acoustic sub-word units. In of ACL- HLT, Short pages 165–168. Association for Computational Linguistics. Sandra R. Waxman and Susan A. Gelman. 2009. Early word-learning entails reference, not merely associain Cognitive 13(6):258–263. J. F. Werker and S. Curtin. 2005. PRIMIR: A developframework of infant speech processing. Lan- Learning and 1(2):197–234. Janet F. Werker and Richard C. Tees. 1984. Crosslanguage speech perception: Evidence for percepreorganization during the first year of life. In- Behavior and 7(1):49 – 63. H Yeung and J Werker. 2009. Learning words’ sounds before learning how words sound: 9-month-olds use distinct objects as cues to categorize speech infor- 113:234–243.</abstract>
<author confidence="0.984913">Steve J Young</author>
<author confidence="0.984913">D Kershaw</author>
<author confidence="0.984913">J Odell</author>
<author confidence="0.984913">D Ollason</author>
<affiliation confidence="0.770291">Valtchev, and P. Woodland. 2006. HTK Book Cambridge University Press.</affiliation>
<address confidence="0.932437">200</address>
</variant>
</algorithm>
<algorithm name="ParsCit" version="110505">
<citationList>
<citation valid="true">
<authors>
<author>Elika Bergelson</author>
<author>Daniel Swingley</author>
</authors>
<title>At 6 to 9 months, human infants know the meanings of many common nouns.</title>
<date>2012</date>
<booktitle>Proceedings of the National Academy of Sciences,</booktitle>
<volume>109</volume>
<issue>9</issue>
<contexts>
<context position="1794" citStr="Bergelson and Swingley, 2012" startWordPosition="270" endWordPosition="274">gories with a high degree of accuracy. One of the most fascinating facts about human infants is the speed at which they acquire their native language. During the first year alone, i.e., before they are able to speak, infants achieve impressive landmarks regarding three key language components. First, they tune in on the phonemic categories of their language (Werker and Tees, 1984). Second, they learn to segment the continuous speech stream into discrete units (Jusczyk and Aslin, 1995). Third, they start to recognize frequent words (Ngon et al., 2013), as well as the semantics of many of them (Bergelson and Swingley, 2012). Even though these landmarks have been documented in detail over the past 40 years of research, little is still known about the mechanisms that are operative in infant’s brain to achieve such a result. Current work in early language acquisition has proposed two competing but incomplete hypotheses that purports to account for this stunning development path. The bottom-up hypothesis holds that infants converge onto the linguistic units of their language through a statistical analysis over of their input. In contrast, the top-down hypothesis emphasizes the role of higher levels of linguistic str</context>
</contexts>
<marker>Bergelson, Swingley, 2012</marker>
<rawString>Elika Bergelson and Daniel Swingley. 2012. At 6 to 9 months, human infants know the meanings of many common nouns. Proceedings of the National Academy of Sciences, 109(9).</rawString>
</citation>
<citation valid="true">
<authors>
<author>Luc Boruta</author>
<author>Sharon Peperkamp</author>
<author>Benoit Crabb´e</author>
<author>Emmanuel Dupoux</author>
</authors>
<title>Testing the robustness of online word segmentation: Effects of linguistic diversity and phonetic variation.</title>
<date>2011</date>
<booktitle>In Proceedings of CMCL,</booktitle>
<pages>1--9</pages>
<publisher>Association</publisher>
<institution>for Computational Linguistics.</institution>
<marker>Boruta, Peperkamp, Crabb´e, Dupoux, 2011</marker>
<rawString>Luc Boruta, Sharon Peperkamp, Benoit Crabb´e, and Emmanuel Dupoux. 2011. Testing the robustness of online word segmentation: Effects of linguistic diversity and phonetic variation. In Proceedings of CMCL, pages 1–9. Association for Computational Linguistics.</rawString>
</citation>
<citation valid="true">
<authors>
<author>Luc Boruta</author>
</authors>
<title>Combining Indicators of Allophony.</title>
<date>2011</date>
<booktitle>In Proceedings ACL-SRW,</booktitle>
<pages>88--93</pages>
<contexts>
<context position="15363" citStr="Boruta, 2011" startWordPosition="2449" endWordPosition="2450">a minimal pair of words differing in the first or last segment (as in [kanaX] and [kanair]) is good evidence that these two phones ([ir], [X]) are allophones of one another. Conversely, if a pair of phones is not forming any minimal pair, it is classified as non-allophonic (phonemic). However, this binary strategy clearly gives rise to false alarms in the (albeit relatively rare) case of true minimal pairs like [kanaX] (“duck”) and [kanal] (“canal”), where ([X], [l]) will be mistakenly labeled as allophonic. In order to mitigate the problem of false alarms, we use Boruta’s continuous version (Boruta, 2011) and we define the lexical cue of a pair of phones Lex(x, y) as the number of lexical minimal pairs that vary on the first segment (xA, yA) or the last segment (Ax, Ay). The higher this number, the more the pair of phones is likely to be considered as allophonic. The lexical cue is consistent with experimental findings. For example Feldman et al. (2013b) showed that 8 month-old infants pay attention to word level information, and demonstrated that they do not discriminate between sound contrasts that occur in minimal pairs (as suggested by our cue), and, conversely, discriminate contrasts that</context>
</contexts>
<marker>Boruta, 2011</marker>
<rawString>Luc Boruta. 2011. Combining Indicators of Allophony. In Proceedings ACL-SRW, pages 88–93.</rawString>
</citation>
<citation valid="true">
<authors>
<author>Luc Boruta</author>
</authors>
<title>Indicateurs d’allophonie et de phon´emicit´e. Doctoral dissertation, Universit´e Paris-Diderot -</title>
<date>2012</date>
<location>Paris VII.</location>
<contexts>
<context position="8332" citStr="Boruta (2012)" startWordPosition="1306" endWordPosition="1307">r the language learning process. 2 Modeling the representations Here, we describe how we model different levels of representation (phonetic categories, lexicon and semantics) starting from raw speech in English and Japanese. 2.1 Corpus We use two speech corpora: the Buckeye Speech corpus (Pitt et al., 2007), which contains 40 hours of spontaneous conversations in American English, and the 40 hours core of the Corpus of Spontaneous Japanese (Maekawa et al., 2000), which contains spontaneous conversations and public speeches in different fields, ranging from engineering to humanities. Following Boruta (2012), 192 we use an inventory of 25 phonemes for transcribing Japanese, and for English, we use the set of 45 phonemes in the phonemic transcription of Pitt et al. (2007). 2.2 Phonetic categories Here, we describe how we model the perceptual phonetic categories infants learn in a first step before converging on the functional categories (phonemes). We make the assumption that these initial categories correspond to fine grained allophones, i.e., different systematic realizations of phonemes, depending on context. Allophonic variation can range from categorical effects due to phonological rules to g</context>
</contexts>
<marker>Boruta, 2012</marker>
<rawString>Luc Boruta. 2012. Indicateurs d’allophonie et de phon´emicit´e. Doctoral dissertation, Universit´e Paris-Diderot - Paris VII.</rawString>
</citation>
<citation valid="true">
<authors>
<author>M Brent</author>
</authors>
<title>An efficient, probabilistically sound algorithm for segmentation and word discovery.</title>
<date>1999</date>
<booktitle>Machine Learning,</booktitle>
<pages>34--71</pages>
<contexts>
<context position="21016" citStr="Brent (1999)" startWordPosition="3423" endWordPosition="3424">orpus is divided into small blocks of 20 utterances each (the semantic window). In each run, we draw randomly with replacement from this set of blocks a sample of the same size as the original corpus. This sample is then used to retrain the acoustic models and generate a phonetic inventory that we used to retranscribe the corpus and re-compute the cues. We report scores averaged over 5 such runs. 4 Results and discussion 4.1 Segmentation We first explore how phonetic variation influences the quality of the segmentation and the resulting lexicon. For the evaluation, we use the same measures as Brent (1999) and Goldwater et al. (2009), namely Segmentation Precision (P), Recall (R) and F-score (F). Segmentation precision is defined Comb(x, y) = F, (Ax,AV)∈L&apos; 195 as the number of correct word tokens found, out of all tokens posited. Recall is the number of correct word tokens found, out of all tokens in the ideal segmentation. The F-score is defined as the harmonic mean of Precision and Recall: 2 * P * R P + R We define similar measures for word types (lexicon). Table 1 shows the scores as a function of the number of allophones per phonemes. For both corpora, the segmentation performance decreases</context>
</contexts>
<marker>Brent, 1999</marker>
<rawString>M. Brent. 1999. An efficient, probabilistically sound algorithm for segmentation and word discovery. Machine Learning, 34:71–105.</rawString>
</citation>
<citation valid="true">
<authors>
<author>N Feldman</author>
<author>T Griffiths</author>
<author>S Goldwater</author>
<author>J Morgan</author>
</authors>
<title>A role for the developing lexicon in phonetic category acquisition.</title>
<date>2013</date>
<journal>Psychological Review,</journal>
<volume>120</volume>
<issue>4</issue>
<contexts>
<context position="3611" citStr="Feldman et al., 2013" startWordPosition="570" endWordPosition="574">of principle for bottom-up learning of phonemic categories from speech (see for instance Vallabha et al., 2007). It is clear, however, that distributional learning cannot account for the entire developmental pattern. In fact, phoneme tokens in real speech exhibit high acoustic variability and result in phonemic categories with a high degree of overlap (Hillenbrand et al., 1995). When purely bottom up clustering algorithms are tested on realistic input, they ended up in either a too large number of subphonemic units (Varadarajan et al., 2008) or a too small number of coarse grained categories (Feldman et al., 2013a). 191 Proceedings of the Eighteenth Conference on Computational Language Learning, pages 191–200, Baltimore, Maryland USA, June 26-27 2014. c�2014 Association for Computational Linguistics 1.2 The top-down hypothesis Inspection of the developmental data shows that infants do not wait to have completed the acquisition of their native phonemes to start to learn words. In fact, lexical and phonological acquisition largely overlap. Infant can recognize highly frequent word forms like their own names, by as early as 4 months of age (Mandel et al., 1995). Vice versa, the refinement of phonemic cat</context>
<context position="15717" citStr="Feldman et al. (2013" startWordPosition="2514" endWordPosition="2517">rms in the (albeit relatively rare) case of true minimal pairs like [kanaX] (“duck”) and [kanal] (“canal”), where ([X], [l]) will be mistakenly labeled as allophonic. In order to mitigate the problem of false alarms, we use Boruta’s continuous version (Boruta, 2011) and we define the lexical cue of a pair of phones Lex(x, y) as the number of lexical minimal pairs that vary on the first segment (xA, yA) or the last segment (Ax, Ay). The higher this number, the more the pair of phones is likely to be considered as allophonic. The lexical cue is consistent with experimental findings. For example Feldman et al. (2013b) showed that 8 month-old infants pay attention to word level information, and demonstrated that they do not discriminate between sound contrasts that occur in minimal pairs (as suggested by our cue), and, conversely, discriminate contrasts that occur in non-minimal pairs. 3.1.2 Semantic cue The semantic cue is based on the intuition that true minimal pairs ([kanaX] and [kanal]) are associated with different events, whereas alternants of the same word ([kanaX] and [kanal]) are expected to co-occur with similar events. We operationalize the semantic cue associated with a pair of phones Sem(x, </context>
</contexts>
<marker>Feldman, Griffiths, Goldwater, Morgan, 2013</marker>
<rawString>N. Feldman, T. Griffiths, S. Goldwater, and J. Morgan. 2013a. A role for the developing lexicon in phonetic category acquisition. Psychological Review, 120(4):751–778.</rawString>
</citation>
<citation valid="true">
<authors>
<author>N Feldman</author>
<author>B Myers</author>
<author>K White</author>
<author>T Griffiths</author>
<author>J Morgan</author>
</authors>
<title>Word-level information influences phonetic learning in adults and infants.</title>
<date>2013</date>
<journal>Cognition,</journal>
<pages>127--427</pages>
<contexts>
<context position="3611" citStr="Feldman et al., 2013" startWordPosition="570" endWordPosition="574">of principle for bottom-up learning of phonemic categories from speech (see for instance Vallabha et al., 2007). It is clear, however, that distributional learning cannot account for the entire developmental pattern. In fact, phoneme tokens in real speech exhibit high acoustic variability and result in phonemic categories with a high degree of overlap (Hillenbrand et al., 1995). When purely bottom up clustering algorithms are tested on realistic input, they ended up in either a too large number of subphonemic units (Varadarajan et al., 2008) or a too small number of coarse grained categories (Feldman et al., 2013a). 191 Proceedings of the Eighteenth Conference on Computational Language Learning, pages 191–200, Baltimore, Maryland USA, June 26-27 2014. c�2014 Association for Computational Linguistics 1.2 The top-down hypothesis Inspection of the developmental data shows that infants do not wait to have completed the acquisition of their native phonemes to start to learn words. In fact, lexical and phonological acquisition largely overlap. Infant can recognize highly frequent word forms like their own names, by as early as 4 months of age (Mandel et al., 1995). Vice versa, the refinement of phonemic cat</context>
<context position="15717" citStr="Feldman et al. (2013" startWordPosition="2514" endWordPosition="2517">rms in the (albeit relatively rare) case of true minimal pairs like [kanaX] (“duck”) and [kanal] (“canal”), where ([X], [l]) will be mistakenly labeled as allophonic. In order to mitigate the problem of false alarms, we use Boruta’s continuous version (Boruta, 2011) and we define the lexical cue of a pair of phones Lex(x, y) as the number of lexical minimal pairs that vary on the first segment (xA, yA) or the last segment (Ax, Ay). The higher this number, the more the pair of phones is likely to be considered as allophonic. The lexical cue is consistent with experimental findings. For example Feldman et al. (2013b) showed that 8 month-old infants pay attention to word level information, and demonstrated that they do not discriminate between sound contrasts that occur in minimal pairs (as suggested by our cue), and, conversely, discriminate contrasts that occur in non-minimal pairs. 3.1.2 Semantic cue The semantic cue is based on the intuition that true minimal pairs ([kanaX] and [kanal]) are associated with different events, whereas alternants of the same word ([kanaX] and [kanal]) are expected to co-occur with similar events. We operationalize the semantic cue associated with a pair of phones Sem(x, </context>
</contexts>
<marker>Feldman, Myers, White, Griffiths, Morgan, 2013</marker>
<rawString>N. Feldman, B. Myers, K. White, T. Griffiths, and J. Morgan. 2013b. Word-level information influences phonetic learning in adults and infants. Cognition, 127:427–438.</rawString>
</citation>
<citation valid="true">
<authors>
<author>Abdellah Fourtassi</author>
<author>Emmanuel Dupoux</author>
</authors>
<title>A corpus-based evaluation method for distributional semantic models.</title>
<date>2013</date>
<booktitle>In 51st Annual Meeting of the Association for Computational Linguistics Proceedings of the Student Research Workshop,</booktitle>
<pages>165--171</pages>
<institution>Sofia, Bulgaria. Association for Computational Linguistics.</institution>
<contexts>
<context position="13786" citStr="Fourtassi and Dupoux, 2013" startWordPosition="2190" endWordPosition="2193">erances. Singular value decomposition (a kind of matrix factorization) is used to extract a more compact representation. The cosine of the angle between vectors in the resulting space is used to measure the semantic similarity between words. Two words have a high semantic similarity if they have similar distributions, i.e., if they co-occur in most contexts. The model parameters, namely the dimension of the semantic space and the number of utterances to be taken as defining the context of a given word form, are set in an unsupervised way to optimize the latent structure of the semantic model (Fourtassi and Dupoux, 2013). Thus, we use 20 utterances as a semantic window and set the semantic space to 100 dimensions. 3 Method Here we explore whether the approximate high level representations, built bottom-up and without supervision, still contain useful information one can use to refine the phonetic categories into phoneme-like units. To this end, we extract potential cues from the lexical and the semantic information, and test their performance in discriminating allophonic contrasts from non-allophonic (phonemic) contrasts. 3.1 Top down cues 3.1.1 Lexical cue The top down information from the lexicon is based o</context>
</contexts>
<marker>Fourtassi, Dupoux, 2013</marker>
<rawString>Abdellah Fourtassi and Emmanuel Dupoux. 2013. A corpus-based evaluation method for distributional semantic models. In 51st Annual Meeting of the Association for Computational Linguistics Proceedings of the Student Research Workshop, pages 165– 171, Sofia, Bulgaria. Association for Computational Linguistics.</rawString>
</citation>
<citation valid="true">
<authors>
<author>Abdellah Fourtassi</author>
<author>Benjamin B¨orschinger</author>
<author>Mark Johnson</author>
<author>Emmanuel Dupoux</author>
</authors>
<title>WhyisEnglishsoeasytosegment?</title>
<date>2013</date>
<booktitle>In Proceedings of CMCL,</booktitle>
<pages>1--10</pages>
<publisher>Association</publisher>
<institution>for Computational Linguistics.</institution>
<marker>Fourtassi, B¨orschinger, Johnson, Dupoux, 2013</marker>
<rawString>Abdellah Fourtassi, Benjamin B¨orschinger, Mark Johnson, and Emmanuel Dupoux. 2013. WhyisEnglishsoeasytosegment? In Proceedings of CMCL, pages 1–10. Association for Computational Linguistics.</rawString>
</citation>
<citation valid="true">
<authors>
<author>Abdellah Fourtassi</author>
<author>Ewan Dunbar</author>
<author>Emmanuel Dupoux</author>
</authors>
<title>Self-consistency as an inductive bias in early language acquisition.</title>
<date>2014</date>
<booktitle>In Proceedings of the 36th annual meeting of the Cognitive Science Society.</booktitle>
<contexts>
<context position="9819" citStr="Fourtassi et al. (2014" startWordPosition="1541" endWordPosition="1544">[x] elsewhere Figure 1: Allophonic variation of French /r/ The phoneme /r/ surfaces as voiced ([x]) before a voiced obstruent like in [kanax Son] (“canard jaune”, yellow duck) and as voiceless ([X]) before a voiceless obstruent as in [kanaX puXpx] (“canard pourpre”, purple duck). The challenge facing the leaner is, therefore, to distinguish pairs of segments that are in an allophonic relationship ([x], [X]) from pairs that are two distinct phonemes and can carry a meaning difference ([x],[l]). Previous work has generated allophonic variation artificially (Martin et al., 2013). Here, we follow Fourtassi et al. (2014b) in using a linguistically and statistically controlled method, starting from audio recordings and using a standard Hidden Markov Models (HMM) phone recognizer to generate them, as follows. We convert the raw speech waveform into successive 10ms frames containing a vector of Mel Frequency Cepstrum Coefficients (MFCC). We use 12 MFC coefficients (plus the energy) computed over a 25ms window, to which we add the first and second order derivatives, yielding 39 dimensions per frame. The HMM training starts with one three-state model per phoneme. Each state is modeled by a mixture of 17 diagonal </context>
<context position="19709" citStr="Fourtassi et al., 2014" startWordPosition="3193" endWordPosition="3196">s of allophones. Some of these pairs are allophones of the same phoneme (allophonic pair) and others are allophones of different phonemes (non-allophonic pairs). The task is a same-different classification, whereby each of these pairs is given a score from the cue that is being tested. A good cue gives higher scores to allophonic pairs. Only pairs of phones that generate at least one lexical minimal pair are considered. Phonetic variation that does not cause lexical variation is “invisible” to top down strategies, and is, therefore, more probably clustered through purely bottom up strategies (Fourtassi et al., 2014b) 3.3 Evaluation We use the same evaluation procedure as Martin et al. (2013). This is carried out by computing the associated ROC curve (varying the z-score threshold and computing the resulting proportions of misses and false alarms). We then derive the Area Under the Curve (AUC), which also corresponds to the probability that given two pairs of phones, one allophonic, one not, they are correctly classified on the basis of the score. A value of 0.5 represents chance and a value of 1 represents perfect performance. In order to lessen the potential influence of the structure of the corpus (ma</context>
<context position="29985" citStr="Fourtassi et al. (2014" startWordPosition="4908" endWordPosition="4911">algorithm has yet to be implemented, but it has to address at least two major issues: First, the fact that some sound pairs are not captured by top down cues because they do not surface as minimal word forms. For instance, in English, /h/ and /N/ occur in different syllable positions and therefore, cannot appear in any minimal pair. Second, even if we have enough information about how phonetic categories are organized in the perceptual space, we still need to know how many categories are relevant in a particular language (i.e., where to stop the categorization process). For the first problem, Fourtassi et al. (2014b) showed that the gap could, in principle, be filled by bottom-up information (like acoustic similarity). As for the second problem, a possible direction could be found in the notion of Self-Consistency. In fact, (Fourtassi et al., 2014a) proposed that an optimal level of clustering is also a level that globally optimizes the predictive power of the lexicon. Too detailed allophones result in too many synonyms. Too broad classes result in too many homophones. Somewhere in the middle, the optimal number of phonemes optimizes how lexical items predict each other. Future work will address these i</context>
</contexts>
<marker>Fourtassi, Dunbar, Dupoux, 2014</marker>
<rawString>Abdellah Fourtassi, Ewan Dunbar, and Emmanuel Dupoux. 2014a. Self-consistency as an inductive bias in early language acquisition. In Proceedings of the 36th annual meeting of the Cognitive Science Society.</rawString>
</citation>
<citation valid="true">
<authors>
<author>Abdellah Fourtassi</author>
<author>Thomas Schatz</author>
<author>Balakrishnan Varadarajan</author>
<author>Emmanuel Dupoux</author>
</authors>
<title>Exploring the Relative Role of Bottom-up and Topdown Information in Phoneme Learning.</title>
<date>2014</date>
<booktitle>In Proceedings of the 52nd Annual Meeting of the Association for Computational Linguistics.</booktitle>
<contexts>
<context position="9819" citStr="Fourtassi et al. (2014" startWordPosition="1541" endWordPosition="1544">[x] elsewhere Figure 1: Allophonic variation of French /r/ The phoneme /r/ surfaces as voiced ([x]) before a voiced obstruent like in [kanax Son] (“canard jaune”, yellow duck) and as voiceless ([X]) before a voiceless obstruent as in [kanaX puXpx] (“canard pourpre”, purple duck). The challenge facing the leaner is, therefore, to distinguish pairs of segments that are in an allophonic relationship ([x], [X]) from pairs that are two distinct phonemes and can carry a meaning difference ([x],[l]). Previous work has generated allophonic variation artificially (Martin et al., 2013). Here, we follow Fourtassi et al. (2014b) in using a linguistically and statistically controlled method, starting from audio recordings and using a standard Hidden Markov Models (HMM) phone recognizer to generate them, as follows. We convert the raw speech waveform into successive 10ms frames containing a vector of Mel Frequency Cepstrum Coefficients (MFCC). We use 12 MFC coefficients (plus the energy) computed over a 25ms window, to which we add the first and second order derivatives, yielding 39 dimensions per frame. The HMM training starts with one three-state model per phoneme. Each state is modeled by a mixture of 17 diagonal </context>
<context position="19709" citStr="Fourtassi et al., 2014" startWordPosition="3193" endWordPosition="3196">s of allophones. Some of these pairs are allophones of the same phoneme (allophonic pair) and others are allophones of different phonemes (non-allophonic pairs). The task is a same-different classification, whereby each of these pairs is given a score from the cue that is being tested. A good cue gives higher scores to allophonic pairs. Only pairs of phones that generate at least one lexical minimal pair are considered. Phonetic variation that does not cause lexical variation is “invisible” to top down strategies, and is, therefore, more probably clustered through purely bottom up strategies (Fourtassi et al., 2014b) 3.3 Evaluation We use the same evaluation procedure as Martin et al. (2013). This is carried out by computing the associated ROC curve (varying the z-score threshold and computing the resulting proportions of misses and false alarms). We then derive the Area Under the Curve (AUC), which also corresponds to the probability that given two pairs of phones, one allophonic, one not, they are correctly classified on the basis of the score. A value of 0.5 represents chance and a value of 1 represents perfect performance. In order to lessen the potential influence of the structure of the corpus (ma</context>
<context position="29985" citStr="Fourtassi et al. (2014" startWordPosition="4908" endWordPosition="4911">algorithm has yet to be implemented, but it has to address at least two major issues: First, the fact that some sound pairs are not captured by top down cues because they do not surface as minimal word forms. For instance, in English, /h/ and /N/ occur in different syllable positions and therefore, cannot appear in any minimal pair. Second, even if we have enough information about how phonetic categories are organized in the perceptual space, we still need to know how many categories are relevant in a particular language (i.e., where to stop the categorization process). For the first problem, Fourtassi et al. (2014b) showed that the gap could, in principle, be filled by bottom-up information (like acoustic similarity). As for the second problem, a possible direction could be found in the notion of Self-Consistency. In fact, (Fourtassi et al., 2014a) proposed that an optimal level of clustering is also a level that globally optimizes the predictive power of the lexicon. Too detailed allophones result in too many synonyms. Too broad classes result in too many homophones. Somewhere in the middle, the optimal number of phonemes optimizes how lexical items predict each other. Future work will address these i</context>
</contexts>
<marker>Fourtassi, Schatz, Varadarajan, Dupoux, 2014</marker>
<rawString>Abdellah Fourtassi, Thomas Schatz, Balakrishnan Varadarajan, and Emmanuel Dupoux. 2014b. Exploring the Relative Role of Bottom-up and Topdown Information in Phoneme Learning. In Proceedings of the 52nd Annual Meeting of the Association for Computational Linguistics.</rawString>
</citation>
<citation valid="true">
<authors>
<author>Stella Frank</author>
<author>Naomi Feldman</author>
<author>Sharon Goldwater</author>
</authors>
<title>Weak semantic context helps phonetic learning in a model of infant language acquisition.</title>
<date>2014</date>
<booktitle>In Proceedings of the 52nd Annual Meeting of the Association of Computational Linguistics.</booktitle>
<contexts>
<context position="28290" citStr="Frank et al., 2014" startWordPosition="4627" endWordPosition="4630">egmentation from continuous transcribed speech, but, as expected, resulted in a low quality lexicon. The poorly segmented corpus was then used to derive a semantic similarity matrix between pairs of words, based on their co-occurrence statistics. The results showed that information from the derived lexicon and semantics, albeit very rudimentary, help discriminate between allophonic and phonemic contrasts, with a high degree of accuracy. Thus, this works strongly support the claim that the lexicon and semantics play a role in the refinement of the phonemic inventory (Feldman et 197 al., 2013a; Frank et al., 2014), and, interestingly, that this role remains functional under more realistic assumptions (unsupervised word segmentation, and bottom-up inferred semantics). We also found that lexical and semantic information were not redundant and could be usefully combined, the former being more resistant to the scarcity of data and variation, and the latter being more resistant to segmentation errors. That being said, this work relies on the assumption that infants start with initial perceptual categories (allophones), but we did not show how such categories could be constructed from raw speech. More work i</context>
</contexts>
<marker>Frank, Feldman, Goldwater, 2014</marker>
<rawString>Stella Frank, Naomi Feldman, and Sharon Goldwater. 2014. Weak semantic context helps phonetic learning in a model of infant language acquisition. In Proceedings of the 52nd Annual Meeting of the Association of Computational Linguistics.</rawString>
</citation>
<citation valid="true">
<authors>
<author>Judit Gervain</author>
<author>Jacques Mehler</author>
</authors>
<title>Speech perception and language acquisition in the first year of life. Annual Review of Psychology,</title>
<date>2010</date>
<pages>61--191</pages>
<contexts>
<context position="2656" citStr="Gervain &amp; Mehler, 2010" startWordPosition="411" endWordPosition="415">ition has proposed two competing but incomplete hypotheses that purports to account for this stunning development path. The bottom-up hypothesis holds that infants converge onto the linguistic units of their language through a statistical analysis over of their input. In contrast, the top-down hypothesis emphasizes the role of higher levels of linguistic structure in learning the lower level units. 1 A chicken-and-egg problem 1.1 Bottom-up is not enough Several studies have documented the fact that infants become attuned to the native sounds of their language, starting at 6 months of age (see Gervain &amp; Mehler, 2010 for a review). Some researchers have claimed that such an early attunement is due to a statistical learning mechanism that only takes into account the distributional properties of the sounds present in the native input (Maye et al., 2002). Unsupervised clustering algorithms running on simplified input have, indeed, provided a proof of principle for bottom-up learning of phonemic categories from speech (see for instance Vallabha et al., 2007). It is clear, however, that distributional learning cannot account for the entire developmental pattern. In fact, phoneme tokens in real speech exhibit h</context>
</contexts>
<marker>Gervain, Mehler, 2010</marker>
<rawString>Judit Gervain and Jacques Mehler. 2010. Speech perception and language acquisition in the first year of life. Annual Review of Psychology, 61:191–218.</rawString>
</citation>
<citation valid="true">
<authors>
<author>Sharon Goldwater</author>
<author>Thomas L Griffiths</author>
<author>Mark Johnson</author>
</authors>
<title>A Bayesian framework for word segmentation: Exploring the effects of context.</title>
<date>2009</date>
<journal>Cognition,</journal>
<volume>112</volume>
<issue>1</issue>
<contexts>
<context position="21044" citStr="Goldwater et al. (2009)" startWordPosition="3426" endWordPosition="3429">into small blocks of 20 utterances each (the semantic window). In each run, we draw randomly with replacement from this set of blocks a sample of the same size as the original corpus. This sample is then used to retrain the acoustic models and generate a phonetic inventory that we used to retranscribe the corpus and re-compute the cues. We report scores averaged over 5 such runs. 4 Results and discussion 4.1 Segmentation We first explore how phonetic variation influences the quality of the segmentation and the resulting lexicon. For the evaluation, we use the same measures as Brent (1999) and Goldwater et al. (2009), namely Segmentation Precision (P), Recall (R) and F-score (F). Segmentation precision is defined Comb(x, y) = F, (Ax,AV)∈L&apos; 195 as the number of correct word tokens found, out of all tokens posited. Recall is the number of correct word tokens found, out of all tokens in the ideal segmentation. The F-score is defined as the harmonic mean of Precision and Recall: 2 * P * R P + R We define similar measures for word types (lexicon). Table 1 shows the scores as a function of the number of allophones per phonemes. For both corpora, the segmentation performance decreases as we increase the number o</context>
</contexts>
<marker>Goldwater, Griffiths, Johnson, 2009</marker>
<rawString>Sharon Goldwater, Thomas L. Griffiths, and Mark Johnson. 2009. A Bayesian framework for word segmentation: Exploring the effects of context. Cognition, 112(1):21–54.</rawString>
</citation>
<citation valid="true">
<authors>
<author>Zellig Harris</author>
</authors>
<date>1954</date>
<journal>Distributional structure. Word,</journal>
<volume>10</volume>
<issue>23</issue>
<contexts>
<context position="12587" citStr="Harris (1954)" startWordPosition="1992" endWordPosition="1994">o find a compact representation of the input. The algorithm stores high frequency chunks and re-uses them to parse novel utterances. We use a grammar which learns a hierarchy of three levels of chunking and use the intermediate level to correspond to the lexical level. This grammar was shown by Fourtassi et al. (2013) to avoid both over-segmentation and under-segmentation. 2.4 The proto-semantics It has been shown that infants can keep track of cooccurrence statistics (see Lany and Saffran (2013) for a review). This ability can be used to develop a sense of semantic similarity as suggested by Harris (1954). The intuition behind the distributional hypothesis is that words that are similar in meaning occur in similar contexts. In order to model the acquisition of this semantic similarity from a 193 transcribed and segmented corpus, we use one of the simplest and most commonly used distributional semantic models, Latent Semantic Analysis (LSA: Landauer &amp; Dumais, 1997). The LSA algorithm takes as input a matrix consisting of rows representing word types and columns representing contexts in which tokens of the word type occur. A context is defined as a fixed number of utterances. Singular value deco</context>
</contexts>
<marker>Harris, 1954</marker>
<rawString>Zellig Harris. 1954. Distributional structure. Word, 10(23):146–162.</rawString>
</citation>
<citation valid="true">
<authors>
<author>Valerie Hazan</author>
<author>Sarah Barrett</author>
</authors>
<title>The development of phonemic categorization in children aged 6 to12.</title>
<date>2000</date>
<journal>Journal of Phonetics,</journal>
<pages>28--377</pages>
<contexts>
<context position="4387" citStr="Hazan and Barrett, 2000" startWordPosition="693" endWordPosition="696">ociation for Computational Linguistics 1.2 The top-down hypothesis Inspection of the developmental data shows that infants do not wait to have completed the acquisition of their native phonemes to start to learn words. In fact, lexical and phonological acquisition largely overlap. Infant can recognize highly frequent word forms like their own names, by as early as 4 months of age (Mandel et al., 1995). Vice versa, the refinement of phonemic categories does not stop at 12 months. The sensitivity to phonetic contrasts has been reported to continue at 3 years of age (Nittrouer, 1996) and beyond (Hazan and Barrett, 2000), on par with the development of the lexicon. Some researchers have therefore suggested that there might be a learning synergy which allows infants to base some of their acquisition not only on bottom up information, but also on statistics over lexical items or even on the basis of word meaning (Feldman et al., 2013a; Feldman et al., 2013b; Yeung and Werker, 2009) These experiments and computational models, however, have focused on simplified input or/and used already segmented words. It remains to be shown whether the said top-down strategies scale up when real size corpora and more realistic</context>
</contexts>
<marker>Hazan, Barrett, 2000</marker>
<rawString>Valerie Hazan and Sarah Barrett. 2000. The development of phonemic categorization in children aged 6 to12. Journal of Phonetics, 28:377–396.</rawString>
</citation>
<citation valid="true">
<authors>
<author>James Hillenbrand</author>
<author>Laura A Getty</author>
<author>Michael J Clark</author>
<author>Kimberlee Wheeler</author>
</authors>
<title>Acoustic characteristics of american english vowels.</title>
<date>1995</date>
<journal>Journal of the Acoustical Society of America,</journal>
<pages>97--3099</pages>
<contexts>
<context position="3371" citStr="Hillenbrand et al., 1995" startWordPosition="528" endWordPosition="532">atistical learning mechanism that only takes into account the distributional properties of the sounds present in the native input (Maye et al., 2002). Unsupervised clustering algorithms running on simplified input have, indeed, provided a proof of principle for bottom-up learning of phonemic categories from speech (see for instance Vallabha et al., 2007). It is clear, however, that distributional learning cannot account for the entire developmental pattern. In fact, phoneme tokens in real speech exhibit high acoustic variability and result in phonemic categories with a high degree of overlap (Hillenbrand et al., 1995). When purely bottom up clustering algorithms are tested on realistic input, they ended up in either a too large number of subphonemic units (Varadarajan et al., 2008) or a too small number of coarse grained categories (Feldman et al., 2013a). 191 Proceedings of the Eighteenth Conference on Computational Language Learning, pages 191–200, Baltimore, Maryland USA, June 26-27 2014. c�2014 Association for Computational Linguistics 1.2 The top-down hypothesis Inspection of the developmental data shows that infants do not wait to have completed the acquisition of their native phonemes to start to le</context>
</contexts>
<marker>Hillenbrand, Getty, Clark, Wheeler, 1995</marker>
<rawString>James Hillenbrand, Laura A. Getty, Michael J. Clark, and Kimberlee Wheeler. 1995. Acoustic characteristics of american english vowels. Journal of the Acoustical Society of America, 97:3099–3109.</rawString>
</citation>
<citation valid="true">
<authors>
<author>M Huijbregts</author>
<author>M McLaren</author>
<author>D van Leeuwen</author>
</authors>
<title>Unsupervised acoustic sub-word unit detection for query-by-example spoken term detection.</title>
<date>2011</date>
<booktitle>In Proceedings of ICASSP,</booktitle>
<pages>4436--4439</pages>
<marker>Huijbregts, McLaren, van Leeuwen, 2011</marker>
<rawString>M. Huijbregts, M. McLaren, and D. van Leeuwen. 2011. Unsupervised acoustic sub-word unit detection for query-by-example spoken term detection. In Proceedings of ICASSP, pages 4436–4439.</rawString>
</citation>
<citation valid="true">
<authors>
<author>A Jansen</author>
<author>K Church</author>
</authors>
<title>Towards unsupervised training of speaker independent acoustic models.</title>
<date>2011</date>
<booktitle>In Proceedings of INTERSPEECH,</booktitle>
<pages>1693--1696</pages>
<contexts>
<context position="29064" citStr="Jansen and Church, 2011" startWordPosition="4751" endWordPosition="4754">ics). We also found that lexical and semantic information were not redundant and could be usefully combined, the former being more resistant to the scarcity of data and variation, and the latter being more resistant to segmentation errors. That being said, this work relies on the assumption that infants start with initial perceptual categories (allophones), but we did not show how such categories could be constructed from raw speech. More work is needed to explore the robustness of the model when these units are learned in an unsupervised fashion (Lee and Glass, 2012; Huijbregts et al., 2011; Jansen and Church, 2011; Varadarajan et al., 2008). This work could be seen as a proof of principle for an iterative learning algorithm, whereby phonemes emerge from the interaction of low level perceptual categories, word forms, and the semantics (see Werker and Curtin (2005) for a similar theoretical proposition). The algorithm has yet to be implemented, but it has to address at least two major issues: First, the fact that some sound pairs are not captured by top down cues because they do not surface as minimal word forms. For instance, in English, /h/ and /N/ occur in different syllable positions and therefore, c</context>
</contexts>
<marker>Jansen, Church, 2011</marker>
<rawString>A. Jansen and K. Church. 2011. Towards unsupervised training of speaker independent acoustic models. In Proceedings of INTERSPEECH, pages 1693–1696.</rawString>
</citation>
<citation valid="false">
<authors>
<author>Aren Jansen</author>
<author>Emmanuel Dupoux</author>
<author>Sharon Goldwater</author>
<author>Mark Johnson</author>
<author>Sanjeev Khudanpur</author>
<author>Kenneth Church</author>
<author>Naomi Feldman</author>
<author>Hynek Hermansky</author>
<author>Florian Metze</author>
<author>Richard Rose</author>
<author>Mike Seltzer</author>
<author>Pascal Clark</author>
<author>Ian McGraw</author>
<author>Balakrishnan Varadarajan</author>
<author>Erin Bennett</author>
<author>Benjamin Borschinger</author>
</authors>
<title>A summary of the 2012 jhu clsp workshop on zero resource speech technologies and models of early language acquisition.</title>
<date>2013</date>
<booktitle>In Proceedings of ICASSP.</booktitle>
<location>Justin Chiu, Ewan Dunbar, Abdallah Fourtassi, David Harwath, Chia ying Lee, Keith Levin, Atta Norouzian, Vijay Peddinti, Rachel Richardson, Thomas</location>
<contexts>
<context position="5389" citStr="Jansen et al., 2013" startWordPosition="855" endWordPosition="858">s and computational models, however, have focused on simplified input or/and used already segmented words. It remains to be shown whether the said top-down strategies scale up when real size corpora and more realistic representations are used. There are indeed indications that, in the absence of a proper phonological representation, lexical learning becomes very difficult. For example, word segmentation algorithms that work on the basis of phoneme-like units tend to degrade quickly if phonemes are replaced by contextual allophones (Boruta et al., 2011) or with the output of phone recognizers (Jansen et al., 2013; Ludusan et al., 2014). In brief, we are facing a chicken-and-egg problem: lexical and semantic information could help to learn the phonemes, but phonemes are needed to acquire lexical information. 1.3 Breaking the circularity: An incremental discovery procedure Here, we explore the idea that instead of learning adult-like hierarchically organized representations in a sequential fashion (phonemes, words, semantics), infants learn approximate, provisional linguistic representations in parallel. These approximate representations are subsequently used to improve each other. More precisely, we ma</context>
</contexts>
<marker>Jansen, Dupoux, Goldwater, Johnson, Khudanpur, Church, Feldman, Hermansky, Metze, Rose, Seltzer, Clark, McGraw, Varadarajan, Bennett, Borschinger, 2013</marker>
<rawString>Aren Jansen, Emmanuel Dupoux, Sharon Goldwater, Mark Johnson, Sanjeev Khudanpur, Kenneth Church, Naomi Feldman, Hynek Hermansky, Florian Metze, Richard Rose, Mike Seltzer, Pascal Clark, Ian McGraw, Balakrishnan Varadarajan, Erin Bennett, Benjamin Borschinger, Justin Chiu, Ewan Dunbar, Abdallah Fourtassi, David Harwath, Chia ying Lee, Keith Levin, Atta Norouzian, Vijay Peddinti, Rachel Richardson, Thomas Schatz, and Samuel Thomas. 2013. A summary of the 2012 jhu clsp workshop on zero resource speech technologies and models of early language acquisition. In Proceedings of ICASSP.</rawString>
</citation>
<citation valid="true">
<authors>
<author>Mark Johnson</author>
<author>Thomas L Griffiths</author>
<author>Sharon Goldwater</author>
</authors>
<title>Adaptor Grammars: A framework for specifying compositional nonparametric Bayesian models.</title>
<date>2007</date>
<booktitle>Advances in Neural Information Processing Systems 19,</booktitle>
<pages>641--648</pages>
<editor>In B. Sch¨olkopf, J. Platt, and T. Hoffman, editors,</editor>
<publisher>MIT Press,</publisher>
<location>Cambridge, MA.</location>
<contexts>
<context position="11561" citStr="Johnson et al., 2007" startWordPosition="1825" endWordPosition="1828"> the phonemic inventory) and retrained. Clustering is done state by state using a phonetic feature-based decision tree, and results in tying together the HMM states of linguistically similar triphones so as to maximize the likelihood of the data. The HMM were built using the HMM Toolkit (HTK: Young et al., 2006). 2.3 The proto-lexicon Finding word boundaries in the continuous sequence of phones is part of the problem infants have to solve without direct supervision. We model this segmentation using a state-of-the-art unsupervised word segmentation model based on the Adaptor Grammar framework (Johnson et al., 2007). The input consists of a phonetic transcription of the corpus, with boundaries between words eliminated (we vary this transcription to correspond to different inventories with different granularity in the allophonic representation as explained above). The model tries to reconstruct the boundaries based on a Pitman-Yor process (Pitman and Yor, 1997), which uses a language-general statistical learning process to find a compact representation of the input. The algorithm stores high frequency chunks and re-uses them to parse novel utterances. We use a grammar which learns a hierarchy of three lev</context>
</contexts>
<marker>Johnson, Griffiths, Goldwater, 2007</marker>
<rawString>Mark Johnson, Thomas L. Griffiths, and Sharon Goldwater. 2007. Adaptor Grammars: A framework for specifying compositional nonparametric Bayesian models. In B. Sch¨olkopf, J. Platt, and T. Hoffman, editors, Advances in Neural Information Processing Systems 19, pages 641–648. MIT Press, Cambridge, MA.</rawString>
</citation>
<citation valid="true">
<authors>
<author>Peter W Jusczyk</author>
<author>Richard N Aslin</author>
</authors>
<title>Infants’ detection of the sound patterns of words in fluent speech.</title>
<date>1995</date>
<booktitle>Cognitive psychology,</booktitle>
<pages>29--1</pages>
<contexts>
<context position="1654" citStr="Jusczyk and Aslin, 1995" startWordPosition="245" endWordPosition="248">se representations are poor approximations of the ground truth, they help reorganize the fine grained categories into phoneme-like categories with a high degree of accuracy. One of the most fascinating facts about human infants is the speed at which they acquire their native language. During the first year alone, i.e., before they are able to speak, infants achieve impressive landmarks regarding three key language components. First, they tune in on the phonemic categories of their language (Werker and Tees, 1984). Second, they learn to segment the continuous speech stream into discrete units (Jusczyk and Aslin, 1995). Third, they start to recognize frequent words (Ngon et al., 2013), as well as the semantics of many of them (Bergelson and Swingley, 2012). Even though these landmarks have been documented in detail over the past 40 years of research, little is still known about the mechanisms that are operative in infant’s brain to achieve such a result. Current work in early language acquisition has proposed two competing but incomplete hypotheses that purports to account for this stunning development path. The bottom-up hypothesis holds that infants converge onto the linguistic units of their language thr</context>
</contexts>
<marker>Jusczyk, Aslin, 1995</marker>
<rawString>Peter W Jusczyk and Richard N Aslin. 1995. Infants’ detection of the sound patterns of words in fluent speech. Cognitive psychology, 29(1):1–23.</rawString>
</citation>
<citation valid="true">
<authors>
<author>Thomas K Landauer</author>
<author>Susan T Dumais</author>
</authors>
<title>A solution to Plato’s problem: The Latent Semantic Analysis theory of acquisition, induction and representation of knowledge.</title>
<date>1997</date>
<journal>Psychological Review,</journal>
<volume>104</volume>
<issue>2</issue>
<contexts>
<context position="12953" citStr="Landauer &amp; Dumais, 1997" startWordPosition="2049" endWordPosition="2052"> under-segmentation. 2.4 The proto-semantics It has been shown that infants can keep track of cooccurrence statistics (see Lany and Saffran (2013) for a review). This ability can be used to develop a sense of semantic similarity as suggested by Harris (1954). The intuition behind the distributional hypothesis is that words that are similar in meaning occur in similar contexts. In order to model the acquisition of this semantic similarity from a 193 transcribed and segmented corpus, we use one of the simplest and most commonly used distributional semantic models, Latent Semantic Analysis (LSA: Landauer &amp; Dumais, 1997). The LSA algorithm takes as input a matrix consisting of rows representing word types and columns representing contexts in which tokens of the word type occur. A context is defined as a fixed number of utterances. Singular value decomposition (a kind of matrix factorization) is used to extract a more compact representation. The cosine of the angle between vectors in the resulting space is used to measure the semantic similarity between words. Two words have a high semantic similarity if they have similar distributions, i.e., if they co-occur in most contexts. The model parameters, namely the </context>
</contexts>
<marker>Landauer, Dumais, 1997</marker>
<rawString>Thomas K Landauer and Susan T Dumais. 1997. A solution to Plato’s problem: The Latent Semantic Analysis theory of acquisition, induction and representation of knowledge. Psychological Review, 104(2):211–240.</rawString>
</citation>
<citation valid="true">
<authors>
<author>J Lany</author>
<author>J Saffran</author>
</authors>
<title>Statistical learning mechanisms in infancy.</title>
<date>2013</date>
<booktitle>Comprehensive Developmental Neuroscience: Neural Circuit Development and Function in the Brain,</booktitle>
<volume>3</volume>
<pages>231--248</pages>
<editor>In J. Rubenstein and P. Rakic, editors,</editor>
<publisher>Elsevier,</publisher>
<location>Amsterdam.</location>
<contexts>
<context position="12475" citStr="Lany and Saffran (2013)" startWordPosition="1970" endWordPosition="1973">ndaries based on a Pitman-Yor process (Pitman and Yor, 1997), which uses a language-general statistical learning process to find a compact representation of the input. The algorithm stores high frequency chunks and re-uses them to parse novel utterances. We use a grammar which learns a hierarchy of three levels of chunking and use the intermediate level to correspond to the lexical level. This grammar was shown by Fourtassi et al. (2013) to avoid both over-segmentation and under-segmentation. 2.4 The proto-semantics It has been shown that infants can keep track of cooccurrence statistics (see Lany and Saffran (2013) for a review). This ability can be used to develop a sense of semantic similarity as suggested by Harris (1954). The intuition behind the distributional hypothesis is that words that are similar in meaning occur in similar contexts. In order to model the acquisition of this semantic similarity from a 193 transcribed and segmented corpus, we use one of the simplest and most commonly used distributional semantic models, Latent Semantic Analysis (LSA: Landauer &amp; Dumais, 1997). The LSA algorithm takes as input a matrix consisting of rows representing word types and columns representing contexts i</context>
<context position="26638" citStr="Lany and Saffran, 2013" startWordPosition="4370" endWordPosition="4373">f two allophones per phonemes (b). The scores are shown for both ideal and unsupervised word segmentation in English and Japanese. The points show the mean scores over 5 runs. The lines are smoothed interpolations (local regressions) through the means. The grey band shows a 95% confidence interval. intentions) (Waxman and Gelman, 2009). What we call the “semantics” of a word in this study, is the general context provided by the co-occurrence with other words. Infants have been shown to have a powerful mechanism for tracking co-occurrence relationships both in the speech and the visual domain (Lany and Saffran, 2013) . Our experiments demonstrate that a similar mechanism could be enough to develop a sense of semantic similarity that can successfully be used to refine phonetic categories. 5 General discussion and future work Phonemes are abstract categories that form the basis for words in the lexicon. There is a traditional view that they should be defined by their ability to contrast word meanings (Trubetzkoy, 1939). Their full acquisition, therefore, requires lexical and semantic top-down information. However, since the quality of the semantic representations depends on the quality of the phonemic repre</context>
</contexts>
<marker>Lany, Saffran, 2013</marker>
<rawString>J. Lany and J. Saffran. 2013. Statistical learning mechanisms in infancy. In J. Rubenstein and P. Rakic, editors, Comprehensive Developmental Neuroscience: Neural Circuit Development and Function in the Brain, volume 3, pages 231–248. Elsevier, Amsterdam.</rawString>
</citation>
<citation valid="true">
<authors>
<author>D H Lawrence</author>
</authors>
<title>Acquired distinctiveness of cues: I. transfer between discriminations on the basis of familiarity with the stimulus.</title>
<date>1949</date>
<journal>Journal of Experimental Psychology,</journal>
<volume>39</volume>
<issue>6</issue>
<contexts>
<context position="17052" citStr="Lawrence, 1949" startWordPosition="2725" endWordPosition="2726"> the average semantic similarity, the more the learner is prone to classify them as allophonic. We take as a measure of the semantic similarity, the cosine of the angle between word vectors of the pairs that vary on the final segment cos(Ax, Ay) or the first segment cos(xA, yA). This strategy is similar in principle to the phenomenon of acquired distinctiveness, according to which, pairing two target stimuli with distinct events enhances their perceptual differentiation, and acquired equivalence, whereby pairing two target stimuli with the same event, impairs their subsequent differentiation (Lawrence, 1949). In the same vein, Yeung and Werker (2009) tested 9 month-olds english learning infants in a task that consists in discriminating two non-native phonetic categories. They found that infants succeeded only when the categories co-occurred with two distinct visual cues. 194 Segmentation Lexicon English Japanese English Japanese Allo./phon. F P R F P R F P R F P R 2 0.61 0.57 0.65 0.45 0.44 0.47 0.29 0.42 0.22 0.23 0.54 0.15 4 0.52 0.46 0.59 0.38 0.34 0.43 0.22 0.37 0.15 0.16 0.50 0.10 10 0.51 0.45 0.59 0.34 0.30 0.38 0.21 0.34 0.16 0.16 0.41 0.10 20 0.42 0.38 0.47 0.28 0.26 0.32 0.21 0.29 0.17 0</context>
</contexts>
<marker>Lawrence, 1949</marker>
<rawString>D.H. Lawrence. 1949. Acquired distinctiveness of cues: I. transfer between discriminations on the basis of familiarity with the stimulus. Journal of Experimental Psychology, 39(6):770–784.</rawString>
</citation>
<citation valid="true">
<authors>
<author>C Lee</author>
<author>J Glass</author>
</authors>
<title>A nonparametric bayesian approach to acoustic model discovery.</title>
<date>2012</date>
<booktitle>In Proceedings of the 50th Annual Meeting of the Association for Computational Linguistics: Long PapersVolume</booktitle>
<volume>1</volume>
<pages>40--49</pages>
<contexts>
<context position="29014" citStr="Lee and Glass, 2012" startWordPosition="4743" endWordPosition="4746">rd segmentation, and bottom-up inferred semantics). We also found that lexical and semantic information were not redundant and could be usefully combined, the former being more resistant to the scarcity of data and variation, and the latter being more resistant to segmentation errors. That being said, this work relies on the assumption that infants start with initial perceptual categories (allophones), but we did not show how such categories could be constructed from raw speech. More work is needed to explore the robustness of the model when these units are learned in an unsupervised fashion (Lee and Glass, 2012; Huijbregts et al., 2011; Jansen and Church, 2011; Varadarajan et al., 2008). This work could be seen as a proof of principle for an iterative learning algorithm, whereby phonemes emerge from the interaction of low level perceptual categories, word forms, and the semantics (see Werker and Curtin (2005) for a similar theoretical proposition). The algorithm has yet to be implemented, but it has to address at least two major issues: First, the fact that some sound pairs are not captured by top down cues because they do not surface as minimal word forms. For instance, in English, /h/ and /N/ occu</context>
</contexts>
<marker>Lee, Glass, 2012</marker>
<rawString>C. Lee and J. Glass. 2012. A nonparametric bayesian approach to acoustic model discovery. In Proceedings of the 50th Annual Meeting of the Association for Computational Linguistics: Long PapersVolume 1, pages 40–49.</rawString>
</citation>
<citation valid="true">
<authors>
<author>Bogdan Ludusan</author>
<author>Maarten Versteegh</author>
<author>Aren Jansen</author>
<author>Guillaume Gravier</author>
<author>Xuan-Nga Cao</author>
<author>Mark Johnson</author>
<author>Emmanuel Dupoux</author>
</authors>
<title>Bridging the gap between speech technology and natural language processing: an evaluation toolbox for term discovery systems.</title>
<date>2014</date>
<booktitle>In Proceedings of LREC.</booktitle>
<contexts>
<context position="5412" citStr="Ludusan et al., 2014" startWordPosition="859" endWordPosition="862">odels, however, have focused on simplified input or/and used already segmented words. It remains to be shown whether the said top-down strategies scale up when real size corpora and more realistic representations are used. There are indeed indications that, in the absence of a proper phonological representation, lexical learning becomes very difficult. For example, word segmentation algorithms that work on the basis of phoneme-like units tend to degrade quickly if phonemes are replaced by contextual allophones (Boruta et al., 2011) or with the output of phone recognizers (Jansen et al., 2013; Ludusan et al., 2014). In brief, we are facing a chicken-and-egg problem: lexical and semantic information could help to learn the phonemes, but phonemes are needed to acquire lexical information. 1.3 Breaking the circularity: An incremental discovery procedure Here, we explore the idea that instead of learning adult-like hierarchically organized representations in a sequential fashion (phonemes, words, semantics), infants learn approximate, provisional linguistic representations in parallel. These approximate representations are subsequently used to improve each other. More precisely, we make four assumptions. Fi</context>
</contexts>
<marker>Ludusan, Versteegh, Jansen, Gravier, Cao, Johnson, Dupoux, 2014</marker>
<rawString>Bogdan Ludusan, Maarten Versteegh, Aren Jansen, Guillaume Gravier, Xuan-Nga Cao, Mark Johnson, and Emmanuel Dupoux. 2014. Bridging the gap between speech technology and natural language processing: an evaluation toolbox for term discovery systems. In Proceedings of LREC.</rawString>
</citation>
<citation valid="true">
<authors>
<author>Kikuo Maekawa</author>
<author>Hanae Koiso</author>
<author>Sadaoki Furui</author>
<author>Hitoshi Isahara</author>
</authors>
<title>Spontaneous speech corpus of japanese.</title>
<date>2000</date>
<booktitle>In LREC,</booktitle>
<pages>947--952</pages>
<location>Athens, Greece.</location>
<contexts>
<context position="8185" citStr="Maekawa et al., 2000" startWordPosition="1285" endWordPosition="1288"> to refine the learning of the lower level (phonetic categories). Next, we present the results of our simulations and discuss the potential implications for the language learning process. 2 Modeling the representations Here, we describe how we model different levels of representation (phonetic categories, lexicon and semantics) starting from raw speech in English and Japanese. 2.1 Corpus We use two speech corpora: the Buckeye Speech corpus (Pitt et al., 2007), which contains 40 hours of spontaneous conversations in American English, and the 40 hours core of the Corpus of Spontaneous Japanese (Maekawa et al., 2000), which contains spontaneous conversations and public speeches in different fields, ranging from engineering to humanities. Following Boruta (2012), 192 we use an inventory of 25 phonemes for transcribing Japanese, and for English, we use the set of 45 phonemes in the phonemic transcription of Pitt et al. (2007). 2.2 Phonetic categories Here, we describe how we model the perceptual phonetic categories infants learn in a first step before converging on the functional categories (phonemes). We make the assumption that these initial categories correspond to fine grained allophones, i.e., differen</context>
</contexts>
<marker>Maekawa, Koiso, Furui, Isahara, 2000</marker>
<rawString>Kikuo Maekawa, Hanae Koiso, Sadaoki Furui, and Hitoshi Isahara. 2000. Spontaneous speech corpus of japanese. In LREC, pages 947–952, Athens, Greece.</rawString>
</citation>
<citation valid="true">
<authors>
<author>D R Mandel</author>
<author>P W Jusczyk</author>
<author>D B Pisoni</author>
</authors>
<title>Infants’ recognition of the sound patterns of their own names.</title>
<date>1995</date>
<journal>Psychological Science,</journal>
<volume>6</volume>
<issue>5</issue>
<contexts>
<context position="4167" citStr="Mandel et al., 1995" startWordPosition="656" endWordPosition="659"> small number of coarse grained categories (Feldman et al., 2013a). 191 Proceedings of the Eighteenth Conference on Computational Language Learning, pages 191–200, Baltimore, Maryland USA, June 26-27 2014. c�2014 Association for Computational Linguistics 1.2 The top-down hypothesis Inspection of the developmental data shows that infants do not wait to have completed the acquisition of their native phonemes to start to learn words. In fact, lexical and phonological acquisition largely overlap. Infant can recognize highly frequent word forms like their own names, by as early as 4 months of age (Mandel et al., 1995). Vice versa, the refinement of phonemic categories does not stop at 12 months. The sensitivity to phonetic contrasts has been reported to continue at 3 years of age (Nittrouer, 1996) and beyond (Hazan and Barrett, 2000), on par with the development of the lexicon. Some researchers have therefore suggested that there might be a learning synergy which allows infants to base some of their acquisition not only on bottom up information, but also on statistics over lexical items or even on the basis of word meaning (Feldman et al., 2013a; Feldman et al., 2013b; Yeung and Werker, 2009) These experim</context>
</contexts>
<marker>Mandel, Jusczyk, Pisoni, 1995</marker>
<rawString>D.R. Mandel, P.W. Jusczyk, and D.B. Pisoni. 1995. Infants’ recognition of the sound patterns of their own names. Psychological Science, 6(5):314–317.</rawString>
</citation>
<citation valid="true">
<authors>
<author>Andrew Martin</author>
<author>Sharon Peperkamp</author>
<author>Emmanuel Dupoux</author>
</authors>
<title>Learning phonemes with a protolexicon.</title>
<date>2013</date>
<journal>Cognitive Science,</journal>
<volume>37</volume>
<issue>1</issue>
<contexts>
<context position="9779" citStr="Martin et al., 2013" startWordPosition="1533" endWordPosition="1536">] / before a voiceless obstruent /r/ →[x] elsewhere Figure 1: Allophonic variation of French /r/ The phoneme /r/ surfaces as voiced ([x]) before a voiced obstruent like in [kanax Son] (“canard jaune”, yellow duck) and as voiceless ([X]) before a voiceless obstruent as in [kanaX puXpx] (“canard pourpre”, purple duck). The challenge facing the leaner is, therefore, to distinguish pairs of segments that are in an allophonic relationship ([x], [X]) from pairs that are two distinct phonemes and can carry a meaning difference ([x],[l]). Previous work has generated allophonic variation artificially (Martin et al., 2013). Here, we follow Fourtassi et al. (2014b) in using a linguistically and statistically controlled method, starting from audio recordings and using a standard Hidden Markov Models (HMM) phone recognizer to generate them, as follows. We convert the raw speech waveform into successive 10ms frames containing a vector of Mel Frequency Cepstrum Coefficients (MFCC). We use 12 MFC coefficients (plus the energy) computed over a 25ms window, to which we add the first and second order derivatives, yielding 39 dimensions per frame. The HMM training starts with one three-state model per phoneme. Each state</context>
<context position="14423" citStr="Martin et al. (2013)" startWordPosition="2292" endWordPosition="2295"> utterances as a semantic window and set the semantic space to 100 dimensions. 3 Method Here we explore whether the approximate high level representations, built bottom-up and without supervision, still contain useful information one can use to refine the phonetic categories into phoneme-like units. To this end, we extract potential cues from the lexical and the semantic information, and test their performance in discriminating allophonic contrasts from non-allophonic (phonemic) contrasts. 3.1 Top down cues 3.1.1 Lexical cue The top down information from the lexicon is based on the insight of Martin et al. (2013). It rests on the idea that true lexical minimal pairs are not very frequent in human languages, as compared to minimal pairs due to mere phonological processes (figure 1). The latter creates alternants of the same lexical item since adjacent sounds condition the realization of the first and final phoneme. Therefore, finding a minimal pair of words differing in the first or last segment (as in [kanaX] and [kanair]) is good evidence that these two phones ([ir], [X]) are allophones of one another. Conversely, if a pair of phones is not forming any minimal pair, it is classified as non-allophonic</context>
<context position="19787" citStr="Martin et al. (2013)" startWordPosition="3206" endWordPosition="3209">nic pair) and others are allophones of different phonemes (non-allophonic pairs). The task is a same-different classification, whereby each of these pairs is given a score from the cue that is being tested. A good cue gives higher scores to allophonic pairs. Only pairs of phones that generate at least one lexical minimal pair are considered. Phonetic variation that does not cause lexical variation is “invisible” to top down strategies, and is, therefore, more probably clustered through purely bottom up strategies (Fourtassi et al., 2014b) 3.3 Evaluation We use the same evaluation procedure as Martin et al. (2013). This is carried out by computing the associated ROC curve (varying the z-score threshold and computing the resulting proportions of misses and false alarms). We then derive the Area Under the Curve (AUC), which also corresponds to the probability that given two pairs of phones, one allophonic, one not, they are correctly classified on the basis of the score. A value of 0.5 represents chance and a value of 1 represents perfect performance. In order to lessen the potential influence of the structure of the corpus (mainly the order of the utterances) on the results, we use a statistical resampl</context>
</contexts>
<marker>Martin, Peperkamp, Dupoux, 2013</marker>
<rawString>Andrew Martin, Sharon Peperkamp, and Emmanuel Dupoux. 2013. Learning phonemes with a protolexicon. Cognitive Science, 37(1):103–124.</rawString>
</citation>
<citation valid="true">
<authors>
<author>J Maye</author>
<author>J F Werker</author>
<author>L Gerken</author>
</authors>
<title>Infant sensitivity to distributional information can affect phonetic discrimination.</title>
<date>2002</date>
<tech>Cognition, 82:B101–B111.</tech>
<contexts>
<context position="2895" citStr="Maye et al., 2002" startWordPosition="454" endWordPosition="457">ysis over of their input. In contrast, the top-down hypothesis emphasizes the role of higher levels of linguistic structure in learning the lower level units. 1 A chicken-and-egg problem 1.1 Bottom-up is not enough Several studies have documented the fact that infants become attuned to the native sounds of their language, starting at 6 months of age (see Gervain &amp; Mehler, 2010 for a review). Some researchers have claimed that such an early attunement is due to a statistical learning mechanism that only takes into account the distributional properties of the sounds present in the native input (Maye et al., 2002). Unsupervised clustering algorithms running on simplified input have, indeed, provided a proof of principle for bottom-up learning of phonemic categories from speech (see for instance Vallabha et al., 2007). It is clear, however, that distributional learning cannot account for the entire developmental pattern. In fact, phoneme tokens in real speech exhibit high acoustic variability and result in phonemic categories with a high degree of overlap (Hillenbrand et al., 1995). When purely bottom up clustering algorithms are tested on realistic input, they ended up in either a too large number of s</context>
</contexts>
<marker>Maye, Werker, Gerken, 2002</marker>
<rawString>J. Maye, J. F. Werker, and L. Gerken. 2002. Infant sensitivity to distributional information can affect phonetic discrimination. Cognition, 82:B101–B111.</rawString>
</citation>
<citation valid="true">
<authors>
<author>C Ngon</author>
<author>A Martin</author>
<author>E Dupoux</author>
<author>D Cabrol</author>
<author>M Duthat</author>
<author>S Peperkamp</author>
</authors>
<title>(non)words, (non)words, (non)words: evidence for a protolexicon during the first year of life.</title>
<date>2013</date>
<journal>Developmental Science,</journal>
<volume>16</volume>
<issue>1</issue>
<pages>34</pages>
<contexts>
<context position="1721" citStr="Ngon et al., 2013" startWordPosition="257" endWordPosition="260"> reorganize the fine grained categories into phoneme-like categories with a high degree of accuracy. One of the most fascinating facts about human infants is the speed at which they acquire their native language. During the first year alone, i.e., before they are able to speak, infants achieve impressive landmarks regarding three key language components. First, they tune in on the phonemic categories of their language (Werker and Tees, 1984). Second, they learn to segment the continuous speech stream into discrete units (Jusczyk and Aslin, 1995). Third, they start to recognize frequent words (Ngon et al., 2013), as well as the semantics of many of them (Bergelson and Swingley, 2012). Even though these landmarks have been documented in detail over the past 40 years of research, little is still known about the mechanisms that are operative in infant’s brain to achieve such a result. Current work in early language acquisition has proposed two competing but incomplete hypotheses that purports to account for this stunning development path. The bottom-up hypothesis holds that infants converge onto the linguistic units of their language through a statistical analysis over of their input. In contrast, the t</context>
<context position="6594" citStr="Ngon et al. (2013)" startWordPosition="1031" endWordPosition="1034">cisely, we make four assumptions. First, we assume that infants start by paying attention to fine grained variation in the acoustic input, thus constructing perceptual phonetic categories that are not phonemes, but segments encoding fine grained phonetic details (Werker and Curtin, 2005; Pierrehumbert, 2003). Second, we assume that these units enable infants to segment proto-words from continuous speech and store them in this detailed format. Importantly, this proto-lexicon will not be adult-like: it will contain badly segmented word forms, and store several alternant forms for the same word. Ngon et al. (2013) have shown that 11 month old infants recognize frequent sound sequences that do not necessarily map to adult words. Third, we assume that infants can use this imperfect lexicon to acquire some semantic representation. As shown in Shukla et al. (2011), infants can simultaneously segment words and associate them with a visual referent. Fourth, we assume that as their exposure to language develops, infants reorganize these initial categories along the relevant dimensions of their native language based on cues from all these representations. The aim of this work is to provide a proof of principle</context>
</contexts>
<marker>Ngon, Martin, Dupoux, Cabrol, Duthat, Peperkamp, 2013</marker>
<rawString>C. Ngon, A. Martin, E. Dupoux, D. Cabrol, M. Duthat, and S. Peperkamp. 2013. (non)words, (non)words, (non)words: evidence for a protolexicon during the first year of life. Developmental Science, 16(1):24– 34.</rawString>
</citation>
<citation valid="true">
<authors>
<author>S Nittrouer</author>
</authors>
<title>Discriminability and perceptual weighting of some acoustic cues to speech perception by 3-year-olds.</title>
<date>1996</date>
<journal>Journal of Speech and Hearing Research,</journal>
<pages>39--278</pages>
<contexts>
<context position="4350" citStr="Nittrouer, 1996" startWordPosition="689" endWordPosition="690">, June 26-27 2014. c�2014 Association for Computational Linguistics 1.2 The top-down hypothesis Inspection of the developmental data shows that infants do not wait to have completed the acquisition of their native phonemes to start to learn words. In fact, lexical and phonological acquisition largely overlap. Infant can recognize highly frequent word forms like their own names, by as early as 4 months of age (Mandel et al., 1995). Vice versa, the refinement of phonemic categories does not stop at 12 months. The sensitivity to phonetic contrasts has been reported to continue at 3 years of age (Nittrouer, 1996) and beyond (Hazan and Barrett, 2000), on par with the development of the lexicon. Some researchers have therefore suggested that there might be a learning synergy which allows infants to base some of their acquisition not only on bottom up information, but also on statistics over lexical items or even on the basis of word meaning (Feldman et al., 2013a; Feldman et al., 2013b; Yeung and Werker, 2009) These experiments and computational models, however, have focused on simplified input or/and used already segmented words. It remains to be shown whether the said top-down strategies scale up when</context>
</contexts>
<marker>Nittrouer, 1996</marker>
<rawString>S. Nittrouer. 1996. Discriminability and perceptual weighting of some acoustic cues to speech perception by 3-year-olds. Journal of Speech and Hearing Research, 39:278–297.</rawString>
</citation>
<citation valid="true">
<authors>
<author>J B Pierrehumbert</author>
</authors>
<title>Phonetic diversity, statistical learning, and acquisition of phonology.</title>
<date>2003</date>
<journal>Language and Speech,</journal>
<pages>46--2</pages>
<contexts>
<context position="6285" citStr="Pierrehumbert, 2003" startWordPosition="984" endWordPosition="985">lore the idea that instead of learning adult-like hierarchically organized representations in a sequential fashion (phonemes, words, semantics), infants learn approximate, provisional linguistic representations in parallel. These approximate representations are subsequently used to improve each other. More precisely, we make four assumptions. First, we assume that infants start by paying attention to fine grained variation in the acoustic input, thus constructing perceptual phonetic categories that are not phonemes, but segments encoding fine grained phonetic details (Werker and Curtin, 2005; Pierrehumbert, 2003). Second, we assume that these units enable infants to segment proto-words from continuous speech and store them in this detailed format. Importantly, this proto-lexicon will not be adult-like: it will contain badly segmented word forms, and store several alternant forms for the same word. Ngon et al. (2013) have shown that 11 month old infants recognize frequent sound sequences that do not necessarily map to adult words. Third, we assume that infants can use this imperfect lexicon to acquire some semantic representation. As shown in Shukla et al. (2011), infants can simultaneously segment wor</context>
</contexts>
<marker>Pierrehumbert, 2003</marker>
<rawString>J. B. Pierrehumbert. 2003. Phonetic diversity, statistical learning, and acquisition of phonology. Language and Speech, 46(2-3):115–154.</rawString>
</citation>
<citation valid="true">
<authors>
<author>J Pitman</author>
<author>M Yor</author>
</authors>
<title>The two-parameter Poisson-Dirichlet distribution derived from a stable subordinator. Annals of Probability,</title>
<date>1997</date>
<pages>25--855</pages>
<contexts>
<context position="11912" citStr="Pitman and Yor, 1997" startWordPosition="1879" endWordPosition="1882">oundaries in the continuous sequence of phones is part of the problem infants have to solve without direct supervision. We model this segmentation using a state-of-the-art unsupervised word segmentation model based on the Adaptor Grammar framework (Johnson et al., 2007). The input consists of a phonetic transcription of the corpus, with boundaries between words eliminated (we vary this transcription to correspond to different inventories with different granularity in the allophonic representation as explained above). The model tries to reconstruct the boundaries based on a Pitman-Yor process (Pitman and Yor, 1997), which uses a language-general statistical learning process to find a compact representation of the input. The algorithm stores high frequency chunks and re-uses them to parse novel utterances. We use a grammar which learns a hierarchy of three levels of chunking and use the intermediate level to correspond to the lexical level. This grammar was shown by Fourtassi et al. (2013) to avoid both over-segmentation and under-segmentation. 2.4 The proto-semantics It has been shown that infants can keep track of cooccurrence statistics (see Lany and Saffran (2013) for a review). This ability can be u</context>
</contexts>
<marker>Pitman, Yor, 1997</marker>
<rawString>J. Pitman and M. Yor. 1997. The two-parameter Poisson-Dirichlet distribution derived from a stable subordinator. Annals of Probability, 25:855–900.</rawString>
</citation>
<citation valid="true">
<authors>
<author>M A Pitt</author>
<author>L Dilley</author>
<author>K Johnson</author>
<author>S Kiesling</author>
<author>W Raymond</author>
<author>E Hume</author>
<author>Fosler-Lussier</author>
</authors>
<title>Buckeye corpus of conversational speech.</title>
<date>2007</date>
<contexts>
<context position="8027" citStr="Pitt et al., 2007" startWordPosition="1258" endWordPosition="1261">input and how we modeled different levels of representation. Then, we explain how information from the higher levels (word forms and semantics) can be used to refine the learning of the lower level (phonetic categories). Next, we present the results of our simulations and discuss the potential implications for the language learning process. 2 Modeling the representations Here, we describe how we model different levels of representation (phonetic categories, lexicon and semantics) starting from raw speech in English and Japanese. 2.1 Corpus We use two speech corpora: the Buckeye Speech corpus (Pitt et al., 2007), which contains 40 hours of spontaneous conversations in American English, and the 40 hours core of the Corpus of Spontaneous Japanese (Maekawa et al., 2000), which contains spontaneous conversations and public speeches in different fields, ranging from engineering to humanities. Following Boruta (2012), 192 we use an inventory of 25 phonemes for transcribing Japanese, and for English, we use the set of 45 phonemes in the phonemic transcription of Pitt et al. (2007). 2.2 Phonetic categories Here, we describe how we model the perceptual phonetic categories infants learn in a first step before </context>
</contexts>
<marker>Pitt, Dilley, Johnson, Kiesling, Raymond, Hume, Fosler-Lussier, 2007</marker>
<rawString>M. A. Pitt, L. Dilley, K. Johnson, S. Kiesling, W. Raymond, E. Hume, and Fosler-Lussier. 2007. Buckeye corpus of conversational speech.</rawString>
</citation>
<citation valid="true">
<authors>
<author>M Shukla</author>
<author>K White</author>
<author>R Aslin</author>
</authors>
<title>Prosody guides the rapid mapping of auditory word forms onto visual objects in 6-mo-old infants.</title>
<date>2011</date>
<booktitle>Proceedings of the NationalAcademyofSciences,</booktitle>
<volume>108</volume>
<issue>15</issue>
<pages>6043</pages>
<contexts>
<context position="6845" citStr="Shukla et al. (2011)" startWordPosition="1073" endWordPosition="1076">netic details (Werker and Curtin, 2005; Pierrehumbert, 2003). Second, we assume that these units enable infants to segment proto-words from continuous speech and store them in this detailed format. Importantly, this proto-lexicon will not be adult-like: it will contain badly segmented word forms, and store several alternant forms for the same word. Ngon et al. (2013) have shown that 11 month old infants recognize frequent sound sequences that do not necessarily map to adult words. Third, we assume that infants can use this imperfect lexicon to acquire some semantic representation. As shown in Shukla et al. (2011), infants can simultaneously segment words and associate them with a visual referent. Fourth, we assume that as their exposure to language develops, infants reorganize these initial categories along the relevant dimensions of their native language based on cues from all these representations. The aim of this work is to provide a proof of principle for this general scenario, using real size corpora in two typologically different languages, and state-of-the-art learning algorithms. The paper is organized as follows. We begin by describing how we generated the input and how we modeled different l</context>
</contexts>
<marker>Shukla, White, Aslin, 2011</marker>
<rawString>M Shukla, K White, and R Aslin. 2011. Prosody guides the rapid mapping of auditory word forms onto visual objects in 6-mo-old infants. Proceedings of the NationalAcademyofSciences, 108(15):6038– 6043.</rawString>
</citation>
<citation valid="true">
<authors>
<author>N S Trubetzkoy</author>
</authors>
<date>1939</date>
<booktitle>Grundz¨uge der Phonologie (Principles of phonology). Vandenhoeck &amp; Ruprecht,</booktitle>
<location>G¨ottingen, Germany.</location>
<contexts>
<context position="27046" citStr="Trubetzkoy, 1939" startWordPosition="4438" endWordPosition="4439">t provided by the co-occurrence with other words. Infants have been shown to have a powerful mechanism for tracking co-occurrence relationships both in the speech and the visual domain (Lany and Saffran, 2013) . Our experiments demonstrate that a similar mechanism could be enough to develop a sense of semantic similarity that can successfully be used to refine phonetic categories. 5 General discussion and future work Phonemes are abstract categories that form the basis for words in the lexicon. There is a traditional view that they should be defined by their ability to contrast word meanings (Trubetzkoy, 1939). Their full acquisition, therefore, requires lexical and semantic top-down information. However, since the quality of the semantic representations depends on the quality of the phonemic representations that are used to build the lexicon, we face a chickenand-egg problem. In this paper, we proposed a way to break the circularity by building approximate representation at all the levels. The infants’ initial attunement to languagespecific categories was represented in a way that mirrors the linguistic and statistical properties of the speech closely. We showed that this detailed (proto-phonemic)</context>
</contexts>
<marker>Trubetzkoy, 1939</marker>
<rawString>N. S. Trubetzkoy. 1939. Grundz¨uge der Phonologie (Principles of phonology). Vandenhoeck &amp; Ruprecht, G¨ottingen, Germany.</rawString>
</citation>
<citation valid="true">
<authors>
<author>G K Vallabha</author>
<author>J L McClelland</author>
<author>F Pons</author>
<author>J F Werker</author>
<author>S Amano</author>
</authors>
<title>Unsupervised learning of vowel categories from infant-directed speech.</title>
<date>2007</date>
<booktitle>Proceedings of the National Academy of Sciences,</booktitle>
<volume>104</volume>
<issue>33</issue>
<contexts>
<context position="3102" citStr="Vallabha et al., 2007" startWordPosition="485" endWordPosition="488"> not enough Several studies have documented the fact that infants become attuned to the native sounds of their language, starting at 6 months of age (see Gervain &amp; Mehler, 2010 for a review). Some researchers have claimed that such an early attunement is due to a statistical learning mechanism that only takes into account the distributional properties of the sounds present in the native input (Maye et al., 2002). Unsupervised clustering algorithms running on simplified input have, indeed, provided a proof of principle for bottom-up learning of phonemic categories from speech (see for instance Vallabha et al., 2007). It is clear, however, that distributional learning cannot account for the entire developmental pattern. In fact, phoneme tokens in real speech exhibit high acoustic variability and result in phonemic categories with a high degree of overlap (Hillenbrand et al., 1995). When purely bottom up clustering algorithms are tested on realistic input, they ended up in either a too large number of subphonemic units (Varadarajan et al., 2008) or a too small number of coarse grained categories (Feldman et al., 2013a). 191 Proceedings of the Eighteenth Conference on Computational Language Learning, pages </context>
</contexts>
<marker>Vallabha, McClelland, Pons, Werker, Amano, 2007</marker>
<rawString>G. K. Vallabha, J. L. McClelland, F. Pons, J. F. Werker, and S. Amano. 2007. Unsupervised learning of vowel categories from infant-directed speech. Proceedings of the National Academy of Sciences, 104(33):13273.</rawString>
</citation>
<citation valid="true">
<authors>
<author>Balakrishnan Varadarajan</author>
<author>Sanjeev Khudanpur</author>
<author>Emmanuel Dupoux</author>
</authors>
<title>Unsupervised learning of acoustic sub-word units.</title>
<date>2008</date>
<booktitle>In Proceedings of ACL08: HLT, Short Papers,</booktitle>
<pages>165--168</pages>
<publisher>Association</publisher>
<institution>for Computational Linguistics.</institution>
<contexts>
<context position="3538" citStr="Varadarajan et al., 2008" startWordPosition="557" endWordPosition="560">stering algorithms running on simplified input have, indeed, provided a proof of principle for bottom-up learning of phonemic categories from speech (see for instance Vallabha et al., 2007). It is clear, however, that distributional learning cannot account for the entire developmental pattern. In fact, phoneme tokens in real speech exhibit high acoustic variability and result in phonemic categories with a high degree of overlap (Hillenbrand et al., 1995). When purely bottom up clustering algorithms are tested on realistic input, they ended up in either a too large number of subphonemic units (Varadarajan et al., 2008) or a too small number of coarse grained categories (Feldman et al., 2013a). 191 Proceedings of the Eighteenth Conference on Computational Language Learning, pages 191–200, Baltimore, Maryland USA, June 26-27 2014. c�2014 Association for Computational Linguistics 1.2 The top-down hypothesis Inspection of the developmental data shows that infants do not wait to have completed the acquisition of their native phonemes to start to learn words. In fact, lexical and phonological acquisition largely overlap. Infant can recognize highly frequent word forms like their own names, by as early as 4 months</context>
<context position="29091" citStr="Varadarajan et al., 2008" startWordPosition="4755" endWordPosition="4758">lexical and semantic information were not redundant and could be usefully combined, the former being more resistant to the scarcity of data and variation, and the latter being more resistant to segmentation errors. That being said, this work relies on the assumption that infants start with initial perceptual categories (allophones), but we did not show how such categories could be constructed from raw speech. More work is needed to explore the robustness of the model when these units are learned in an unsupervised fashion (Lee and Glass, 2012; Huijbregts et al., 2011; Jansen and Church, 2011; Varadarajan et al., 2008). This work could be seen as a proof of principle for an iterative learning algorithm, whereby phonemes emerge from the interaction of low level perceptual categories, word forms, and the semantics (see Werker and Curtin (2005) for a similar theoretical proposition). The algorithm has yet to be implemented, but it has to address at least two major issues: First, the fact that some sound pairs are not captured by top down cues because they do not surface as minimal word forms. For instance, in English, /h/ and /N/ occur in different syllable positions and therefore, cannot appear in any minimal</context>
</contexts>
<marker>Varadarajan, Khudanpur, Dupoux, 2008</marker>
<rawString>Balakrishnan Varadarajan, Sanjeev Khudanpur, and Emmanuel Dupoux. 2008. Unsupervised learning of acoustic sub-word units. In Proceedings of ACL08: HLT, Short Papers, pages 165–168. Association for Computational Linguistics.</rawString>
</citation>
<citation valid="true">
<authors>
<author>Sandra R Waxman</author>
<author>Susan A Gelman</author>
</authors>
<title>Early word-learning entails reference, not merely associations.</title>
<date>2009</date>
<booktitle>Trends in Cognitive Sciences,</booktitle>
<volume>13</volume>
<issue>6</issue>
<contexts>
<context position="26352" citStr="Waxman and Gelman, 2009" startWordPosition="4322" endWordPosition="4325">UC 0.9 0.8 0.7 0.6 0.5 1.0 0.9 AUC 0.8 0.7 0.6 0.5 Ideal 1.0 Unsupervised Cues Lexical Semantic Combined Figure 2: Same-different scores (AUC) for different cues as a function of the average number of allophones per phoneme (a), and as a function of the size of the corpus, in the case of two allophones per phonemes (b). The scores are shown for both ideal and unsupervised word segmentation in English and Japanese. The points show the mean scores over 5 runs. The lines are smoothed interpolations (local regressions) through the means. The grey band shows a 95% confidence interval. intentions) (Waxman and Gelman, 2009). What we call the “semantics” of a word in this study, is the general context provided by the co-occurrence with other words. Infants have been shown to have a powerful mechanism for tracking co-occurrence relationships both in the speech and the visual domain (Lany and Saffran, 2013) . Our experiments demonstrate that a similar mechanism could be enough to develop a sense of semantic similarity that can successfully be used to refine phonetic categories. 5 General discussion and future work Phonemes are abstract categories that form the basis for words in the lexicon. There is a traditional </context>
</contexts>
<marker>Waxman, Gelman, 2009</marker>
<rawString>Sandra R. Waxman and Susan A. Gelman. 2009. Early word-learning entails reference, not merely associations. Trends in Cognitive Sciences, 13(6):258–263.</rawString>
</citation>
<citation valid="true">
<authors>
<author>J F Werker</author>
<author>S Curtin</author>
</authors>
<title>PRIMIR: A developmental framework of infant speech processing.</title>
<date>2005</date>
<journal>Language Learning and Development,</journal>
<volume>1</volume>
<issue>2</issue>
<contexts>
<context position="6263" citStr="Werker and Curtin, 2005" startWordPosition="980" endWordPosition="983">ry procedure Here, we explore the idea that instead of learning adult-like hierarchically organized representations in a sequential fashion (phonemes, words, semantics), infants learn approximate, provisional linguistic representations in parallel. These approximate representations are subsequently used to improve each other. More precisely, we make four assumptions. First, we assume that infants start by paying attention to fine grained variation in the acoustic input, thus constructing perceptual phonetic categories that are not phonemes, but segments encoding fine grained phonetic details (Werker and Curtin, 2005; Pierrehumbert, 2003). Second, we assume that these units enable infants to segment proto-words from continuous speech and store them in this detailed format. Importantly, this proto-lexicon will not be adult-like: it will contain badly segmented word forms, and store several alternant forms for the same word. Ngon et al. (2013) have shown that 11 month old infants recognize frequent sound sequences that do not necessarily map to adult words. Third, we assume that infants can use this imperfect lexicon to acquire some semantic representation. As shown in Shukla et al. (2011), infants can simu</context>
<context position="29318" citStr="Werker and Curtin (2005)" startWordPosition="4793" endWordPosition="4796">id, this work relies on the assumption that infants start with initial perceptual categories (allophones), but we did not show how such categories could be constructed from raw speech. More work is needed to explore the robustness of the model when these units are learned in an unsupervised fashion (Lee and Glass, 2012; Huijbregts et al., 2011; Jansen and Church, 2011; Varadarajan et al., 2008). This work could be seen as a proof of principle for an iterative learning algorithm, whereby phonemes emerge from the interaction of low level perceptual categories, word forms, and the semantics (see Werker and Curtin (2005) for a similar theoretical proposition). The algorithm has yet to be implemented, but it has to address at least two major issues: First, the fact that some sound pairs are not captured by top down cues because they do not surface as minimal word forms. For instance, in English, /h/ and /N/ occur in different syllable positions and therefore, cannot appear in any minimal pair. Second, even if we have enough information about how phonetic categories are organized in the perceptual space, we still need to know how many categories are relevant in a particular language (i.e., where to stop the cat</context>
</contexts>
<marker>Werker, Curtin, 2005</marker>
<rawString>J. F. Werker and S. Curtin. 2005. PRIMIR: A developmental framework of infant speech processing. Language Learning and Development, 1(2):197–234.</rawString>
</citation>
<citation valid="true">
<authors>
<author>Janet F Werker</author>
<author>Richard C Tees</author>
</authors>
<title>Crosslanguage speech perception: Evidence for perceptual reorganization during the first year of life.</title>
<date>1984</date>
<journal>Infant Behavior and Development,</journal>
<volume>7</volume>
<issue>1</issue>
<contexts>
<context position="1548" citStr="Werker and Tees, 1984" startWordPosition="228" endWordPosition="231">sed way, an approximate lexicon and a rudimentary semantic representation. Despite the fact that all these representations are poor approximations of the ground truth, they help reorganize the fine grained categories into phoneme-like categories with a high degree of accuracy. One of the most fascinating facts about human infants is the speed at which they acquire their native language. During the first year alone, i.e., before they are able to speak, infants achieve impressive landmarks regarding three key language components. First, they tune in on the phonemic categories of their language (Werker and Tees, 1984). Second, they learn to segment the continuous speech stream into discrete units (Jusczyk and Aslin, 1995). Third, they start to recognize frequent words (Ngon et al., 2013), as well as the semantics of many of them (Bergelson and Swingley, 2012). Even though these landmarks have been documented in detail over the past 40 years of research, little is still known about the mechanisms that are operative in infant’s brain to achieve such a result. Current work in early language acquisition has proposed two competing but incomplete hypotheses that purports to account for this stunning development </context>
</contexts>
<marker>Werker, Tees, 1984</marker>
<rawString>Janet F. Werker and Richard C. Tees. 1984. Crosslanguage speech perception: Evidence for perceptual reorganization during the first year of life. Infant Behavior and Development, 7(1):49 – 63.</rawString>
</citation>
<citation valid="true">
<authors>
<author>H Yeung</author>
<author>J Werker</author>
</authors>
<title>Learning words’ sounds before learning how words sound: 9-month-olds use distinct objects as cues to categorize speech information.</title>
<date>2009</date>
<journal>Cognition,</journal>
<pages>113--234</pages>
<contexts>
<context position="4753" citStr="Yeung and Werker, 2009" startWordPosition="757" endWordPosition="760">4 months of age (Mandel et al., 1995). Vice versa, the refinement of phonemic categories does not stop at 12 months. The sensitivity to phonetic contrasts has been reported to continue at 3 years of age (Nittrouer, 1996) and beyond (Hazan and Barrett, 2000), on par with the development of the lexicon. Some researchers have therefore suggested that there might be a learning synergy which allows infants to base some of their acquisition not only on bottom up information, but also on statistics over lexical items or even on the basis of word meaning (Feldman et al., 2013a; Feldman et al., 2013b; Yeung and Werker, 2009) These experiments and computational models, however, have focused on simplified input or/and used already segmented words. It remains to be shown whether the said top-down strategies scale up when real size corpora and more realistic representations are used. There are indeed indications that, in the absence of a proper phonological representation, lexical learning becomes very difficult. For example, word segmentation algorithms that work on the basis of phoneme-like units tend to degrade quickly if phonemes are replaced by contextual allophones (Boruta et al., 2011) or with the output of ph</context>
<context position="17095" citStr="Yeung and Werker (2009)" startWordPosition="2731" endWordPosition="2734">he more the learner is prone to classify them as allophonic. We take as a measure of the semantic similarity, the cosine of the angle between word vectors of the pairs that vary on the final segment cos(Ax, Ay) or the first segment cos(xA, yA). This strategy is similar in principle to the phenomenon of acquired distinctiveness, according to which, pairing two target stimuli with distinct events enhances their perceptual differentiation, and acquired equivalence, whereby pairing two target stimuli with the same event, impairs their subsequent differentiation (Lawrence, 1949). In the same vein, Yeung and Werker (2009) tested 9 month-olds english learning infants in a task that consists in discriminating two non-native phonetic categories. They found that infants succeeded only when the categories co-occurred with two distinct visual cues. 194 Segmentation Lexicon English Japanese English Japanese Allo./phon. F P R F P R F P R F P R 2 0.61 0.57 0.65 0.45 0.44 0.47 0.29 0.42 0.22 0.23 0.54 0.15 4 0.52 0.46 0.59 0.38 0.34 0.43 0.22 0.37 0.15 0.16 0.50 0.10 10 0.51 0.45 0.59 0.34 0.30 0.38 0.21 0.34 0.16 0.16 0.41 0.10 20 0.42 0.38 0.47 0.28 0.26 0.32 0.21 0.29 0.17 0.16 0.32 0.10 Table 1 : Scores of the segme</context>
</contexts>
<marker>Yeung, Werker, 2009</marker>
<rawString>H Yeung and J Werker. 2009. Learning words’ sounds before learning how words sound: 9-month-olds use distinct objects as cues to categorize speech information. Cognition, 113:234–243.</rawString>
</citation>
<citation valid="true">
<authors>
<author>Steve J Young</author>
<author>D Kershaw</author>
<author>J Odell</author>
<author>D Ollason</author>
<author>V Valtchev</author>
<author>P Woodland</author>
</authors>
<date>2006</date>
<booktitle>The HTK Book Version 3.4.</booktitle>
<publisher>Cambridge University Press.</publisher>
<contexts>
<context position="11253" citStr="Young et al., 2006" startWordPosition="1779" endWordPosition="1782">the word /dAg/ (“dog”). The triphone models cloned from the phonemes are then retrained, but, this time, only on the relevant subset of the data, corresponding to the given triphone context. Finally, these detailed models are clustered back into inventories of various sizes (from 2 to 20 times the size of the phonemic inventory) and retrained. Clustering is done state by state using a phonetic feature-based decision tree, and results in tying together the HMM states of linguistically similar triphones so as to maximize the likelihood of the data. The HMM were built using the HMM Toolkit (HTK: Young et al., 2006). 2.3 The proto-lexicon Finding word boundaries in the continuous sequence of phones is part of the problem infants have to solve without direct supervision. We model this segmentation using a state-of-the-art unsupervised word segmentation model based on the Adaptor Grammar framework (Johnson et al., 2007). The input consists of a phonetic transcription of the corpus, with boundaries between words eliminated (we vary this transcription to correspond to different inventories with different granularity in the allophonic representation as explained above). The model tries to reconstruct the boun</context>
</contexts>
<marker>Young, Kershaw, Odell, Ollason, Valtchev, Woodland, 2006</marker>
<rawString>Steve J. Young, D. Kershaw, J. Odell, D. Ollason, V. Valtchev, and P. Woodland. 2006. The HTK Book Version 3.4. Cambridge University Press.</rawString>
</citation>
</citationList>
</algorithm>
</algorithms>
<?xml version="1.0" encoding="UTF-8"?>
<algorithms version="110505">
<algorithm name="SectLabel" version="110505">
<variant no="0" confidence="0.000273">
<title confidence="0.976894">
DCU-Lingo24 Participation in WMT 2014 Hindi-English Translation task
</title>
<author confidence="0.983585">
Xiaofeng Wu, Rejwanul Haque*, Tsuyoshi Okita
Piyush Arora, Andy Way, Qun Liu
</author>
<affiliation confidence="0.996558">
CNGL, Centre for Global Intelligent Content
School of Computing, Dublin City University
</affiliation>
<address confidence="0.861073">
Dublin 9, Ireland
</address>
<email confidence="0.974292">
{xf.wu,tokita,parora,away,qliu}@computing.dcu.ie
</email>
<address confidence="0.523733">
*Lingo24, Edinburgh, UK
</address>
<email confidence="0.988991">
rejwanul.haque@lingo24.com
</email>
<sectionHeader confidence="0.996527" genericHeader="abstract">
Abstract
</sectionHeader>
<bodyText confidence="0.992160230769231">
This paper describes the DCU-Lingo24
submission to WMT 2014 for the Hindi-
English translation task. We exploit
miscellaneous methods in our system,
including: Context-Informed PB-SMT,
OOV Word Conversion (OWC), Multi-
Alignment Combination (MAC), Oper-
ation Sequence Model (OSM), Stem-
ming Align and Normal Phrase Extraction
(SANPE), and Language Model Interpola-
tion (LMI). We also describe various pre-
processing steps we tried for Hindi in this
task.
</bodyText>
<sectionHeader confidence="0.998785" genericHeader="introduction">
1 Introduction
</sectionHeader>
<bodyText confidence="0.999948130434783">
This paper describes the DCU-Lingo24 submis-
sion to WMT 2014 for the Hindi-English transla-
tion task.
All our experiments on WMT 2014 are built
upon the Moses phrase-based model (PB-SMT)
(Koehn et al., 2007) and tuned with MERT
(Och, 2003). Starting from this baseline system,
we exploit various methods including Context-
Informed PB-SMT (CIPBSMT), zero-shot learn-
ing (Palatucci et al., 2009) using neural network-
based language modelling (Bengio et al., 2000;
Mikolov et al., 2013) for OOV word conversion,
various lexical reordering models (Axelrod et al.,
2005; Galley and Manning, 2008), various Mul-
tiple Alignment Combination (MAC) (Tu et al.,
2012), Operation Sequence Model (OSM) (Dur-
rani et al., 2011) and Language Model Interpola-
tion(LMI).
In the next section, the preprocessing steps are
explained. In Section 3 a detailed explanation of
the technique we exploit is provided. Then in Sec-
tion 4, we provide our experimental results and re-
sultant discussion.
</bodyText>
<sectionHeader confidence="0.983685" genericHeader="method">
2 Pre-processing Steps
</sectionHeader>
<bodyText confidence="0.999991">
We use all the training data provided for Hindi–
English translation. Following Bojar et al. (2010),
we apply a number of normalisation methods on
the Hindi corpus. The HindEnCorp parallel cor-
pus compiles several sources of parallel data. We
observe that the source-side (Hindi) of the TIDES
data source contains font-related noise, i.e. many
Hindi sentences are a mixture of two different en-
codings: UTF-81 and WX2 notations. We pre-
pared a WX-to-UTF-8 font conversion script for
Hindi which converts all WX encoded characters
into UTF-8, thus removing all WX encoding ap-
pearing in the TIDES data.
We also observe that a portion of the English
training corpus contained the following bracket-
like sequences of characters: -LRB-, -LSB-, -
LCB-, -RRB-, -RSB-, and -RCB-.3 For consis-
tency, those character sequences in the training
data were replaced by the corresponding brackets.
For English – both monolingual and the target
side of the bilingual data – we perform tokeniza-
tion, normalization of punctuation, and truecasing.
For parallel training data, we filter sentences pairs
containing more than 80 tokens on either side and
</bodyText>
<footnote confidence="0.99827675">
1http://en.wikipedia.org/wiki/UTF-8
2http://en.wikipedia.org/wiki/WX_notation
3The acronyms stand for (Left|Right)
(Round|Square|Curly) Bracket.
</footnote>
<page confidence="0.978148">
215
</page>
<bodyText confidence="0.66953125">
Proceedings of the Ninth Workshop on Statistical Machine Translation, pages 215–220,
Baltimore, Maryland USA, June 26–27, 2014. c�2014 Association for Computational Linguistics
sentence pairs with length difference larger than 3
times.
</bodyText>
<sectionHeader confidence="0.996046" genericHeader="method">
3 Techniques Deployed
</sectionHeader>
<subsectionHeader confidence="0.99845">
3.1 Combination of Various Lexical
Reordering Model (LRM)
</subsectionHeader>
<bodyText confidence="0.9996975">
Clearly, Hindi and English have quite different
word orders, so we adopt three lexical reordering
models to address this problem. They are word-
based LRM and phrase-based LRM, which mainly
focus on local reordering phenomena, and hierar-
chical phrase-based LRM, which mainly focuses
on longer distance reordering (Galley and Man-
ning, 2008).
</bodyText>
<subsectionHeader confidence="0.998055">
3.2 Operation Sequence Model
</subsectionHeader>
<bodyText confidence="0.999072285714286">
The Operation Sequence Model (OSM) of Dur-
rani et al. (2011) defines four translation opera-
tions: Generate(X,Y), Continue Source Concept,
Generate Source Only (X) and Generate Identical,
as well as three reordering operations: Insert Gap,
Jump Back(W) and Jump Forward.
The probability of an operation sequence O =
</bodyText>
<equation confidence="0.9939665">
(o1o2 · · · oJ) is calculated as in (1):
J
p(O) = p(oj|oj−n+1 ··· oj−1) (1)
j=1
</equation>
<bodyText confidence="0.999583333333333">
where n indicates the number of previous opera-
tions used.
We employ a 9-order OSM in our framework.
</bodyText>
<subsectionHeader confidence="0.992726">
3.3 Language Model Interpolation (LMI)
</subsectionHeader>
<bodyText confidence="0.999787266666667">
We build a large language model by including data
from the English Gigaword fifth edition, the En-
glish side of the UN corpus, the English side of the
109 French–English corpus and the English side of
the Hindi–English parallel data provided by the or-
ganisers. We interpolate language models trained
using each dataset, with the monolingual data pro-
vided split into three parts (news 2007-2013, Eu-
roparl (?) and news commentary) and the weights
tuned to minimize perplexity on the target side of
the devset.
The language models in our systems are trained
with SRILM (Stolcke, 2002). We train a 5-gram
model with Kneser-Ney discounting (Chen and
Goodman, 1996).
</bodyText>
<subsectionHeader confidence="0.964571">
3.4 Context-informed PB-SMT
</subsectionHeader>
<bodyText confidence="0.987879032258065">
Haque et al. (2011) express a context-dependent
phrase translation as a multi-class classification
problem, where a source phrase with given addi-
tional context information is classified into a dis-
tribution over possible target phrases. The size of
this distribution needs to be limited, and would
ideally omit irrelevant target phrase translations
that the standard PB-SMT (Koehn et al., 2003) ap-
proach would normally include. Following Haque
et al. (2011), we derive a context-informed feature
ˆhmbl that is expressed as the conditional probabil-
ity of the target phrase ˆek given the source phrase
ˆfk and its context information (CI), as in (2):
ˆhmbl = log P(ˆek|
Here, CI may include any feature that can pro-
vide useful information to disambiguate the given
source phrase. In our experiment, we use CCG su-
pertag (Steedman, 2000) as a contextual features.
CCG supertag expresses the specific syntactic be-
haviour of a word in terms of the arguments it
takes, and more generally the syntactic environ-
ment in which it appears.
We consider the CCG supertags of the context
words, as well as of the focus phrase itself. In our
model, the supertag of a multi-word focus phrase
is the concatenation of the supertags of the words
composing that phrase. We generate a window
of size 2l + 1 features (we set l:=2), including
the concatenated complex supertag of the focus
phrase. Accordingly, the supertag-based contex-
tual information (CIgt) is described as in (3):
</bodyText>
<equation confidence="0.9323">
ˆfk) = {st(fik−l), ..., st(fik−1), st(ˆfk), (3)
st(fjk+1), ..., st(fjk+l)}
</equation>
<bodyText confidence="0.999791">
For the Hindi-to-English translation task, we use
part-of-speech (PoS) tags4 of the source phrase
and the neighbouring words as the contextual fea-
ture, owing to the fact that supertaggers are readily
available only for English.
We use a memory-based machine learning
(MBL) classifier (TRIBL: (Daelemans, 2005))5
that is able to estimate P(ˆek |ˆfk, CI( ˆfk)) by
similarity-based reasoning over memorized
nearest-neighbour examples of source–target
phrase translations. Thus, we derive the feature
ˆhmbl defined in Equation (2). In addition to ˆhmbl,
</bodyText>
<footnote confidence="0.993789857142857">
4In order to obtain PoS tags of Hindi words,
we used the LTRC shallow parser for Hindi from
http://ltrc.iiit.ac.in/analyzer/hindi/shallow-parser-hin-
4.0.fc8.tar.gz.
5An implementation of TRIBL is freely available as part
of the TiMBL software package, which can be downloaded
from http://ilk.uvt.nl/timbl.
</footnote>
<equation confidence="0.704018">
ˆfk, CI( ˆfk)) (2)
CIgt(
</equation>
<page confidence="0.989355">
216
</page>
<bodyText confidence="0.997965">
we derive a simple two-valued feature ˆhbest,
defined in Equation (4):
</bodyText>
<equation confidence="0.9979965">
ˆhbest = � � 0 otherwise
1 if ˆek maximizes P(ˆek |ˆfk, CI( ˆfk))
</equation>
<bodyText confidence="0.960032454545454">
(4)
where ˆhbest is set to 1 when ˆek is one of the tar-
get phrases with highest probability according to
P(ˆek |ˆfk, CI(ˆfk)) for each source phrase ˆfk; oth-
erwise ˆhbest is set to 0.000001. We performed ex-
periments by integrating these two features ˆhmbl
and ˆhbest directly into the log-linear model of
Moses. Their weights are optimized using mini-
mum error-rate training (MERT)(Och, 2003) on a
held-out development set for each of the experi-
ments.
</bodyText>
<subsectionHeader confidence="0.902557">
3.5 Morphological Segmentation
</subsectionHeader>
<bodyText confidence="0.999967952380953">
Haque et al. (2012) applied a morphological suffix
separation process in a Bengali-to-English trans-
lation task and showed that suffix separation sig-
nificantly reduces data sparseness in the Bengali
corpus. They also showed an SMT model trained
on the suffix-stripped training data significantly
outperforms the state-of-the-art PB-SMT baseline.
Like Bengali, Hindi is a morphologically very rich
and highly inflected Indian language. As done
previously for Bengali-to-English (Haque et al.,
2012), we employ a suffix-stripping method for
lemmatizing inflected Hindi words in the WMT
Hindi-to-English translation task. Following Das-
gupta and Ng (2006), we developed an unsu-
pervised morphological segmentation method for
Hindi. We also used a Hindi lightweight stem-
mer (Ramanathan and Rao, 2003) in order to pre-
pare a training corpus with only Hindi stems. We
prepared Hindi-to-English SMT systems on the
both types of training data (i.e. suffix-stripped and
stemmed).6
</bodyText>
<subsectionHeader confidence="0.993424">
3.6 Multi-Alignment Combination (MAC)
</subsectionHeader>
<bodyText confidence="0.999943">
Word alignment is a critical component of MT
systems. Various methods for word alignment
have been proposed, and different models can pro-
duce signicantly different outputs. For example,
Tu et al. (2012) demonstrates that the alignment
agreement between the two best-known alignment
tools, namely Giza++(Och and Ney, 2003) and
</bodyText>
<footnote confidence="0.6488515">
6Suffixes were separated and completely removed from
the training data.
</footnote>
<bodyText confidence="0.999835">
the Berkeley aligner7 (Liang et al., 2006), is be-
low 70%. Taking into consideration the small size
of the the corpus, in order to extract more ef-
fective phrase tables, we concatenate three align-
ments: Giza++ with grow-diag-final-and, Giza++
with intersection, and that derived from the Berke-
ley aligner.
</bodyText>
<subsectionHeader confidence="0.9943355">
3.7 Stemming Alignment and Normal Phrase
Extraction (SANPE)
</subsectionHeader>
<bodyText confidence="0.9999925">
The rich morphology of Hindi will cause word
alignment sparsity, which results in poor align-
ment quality. Furthermore, word stemming on
the Hindi side usually results in too many English
words being aligned to one stemmed Hindi word,
i.e. we encounter the problem of phrase over-
extraction. Therefore, we conduct word alignment
with the stemmed version of Hindi, and then at
the phrase extraction step, we replace the stemmed
form with the original Hindi form.
</bodyText>
<subsectionHeader confidence="0.947611">
3.8 OOV Word Conversion Method
</subsectionHeader>
<bodyText confidence="0.999988944444445">
Our algorithm for OOV word conversion uses the
recently developed zero-shot learning (Palatucci
et al., 2009) using neural network language mod-
elling (Bengio et al., 2000; Mikolov et al., 2013).
The same technique is used in (Okita et al., 2014).
This method requires neither parallel nor compa-
rable corpora, but rather two monolingual corpora.
In our context, we prepare two monolingual cor-
pora on both sides, which are neither parallel nor
comparable, and a small amount of already known
correspondences between words on the source and
target sides (henceforth, we refer to this as the
‘dictionary’). Then, we train both sides with the
neural network language model, and use a contin-
uous space representation to project words to each
other on the basis of a small amount of correspon-
dences in the dictionary. The following algorithm
shows the steps involved:
</bodyText>
<listItem confidence="0.991285625">
1. Prepare the monolingual source and target
sentences.
2. Prepare the dictionary which consists of U
entries of source and target sentences com-
prising non-stop-words.
3. Train the neural network language model on
the source side and obtain the real vectors of
X dimensions for each word.
</listItem>
<footnote confidence="0.964417">
7http://code.google.com/p/berkeleyaligner/
</footnote>
<page confidence="0.996175">
217
</page>
<bodyText confidence="0.7718418125">
4. Train the neural network language model on
the target side and obtain the real vectors of
X dimensions for each word.
5. Using the real vectors obtained in the above
steps, obtain the linear mapping between the
dictionary items in two continuous spaces us-
ing canonical component analysis (CCA).
In our experiments we use U the same as the en-
tries of Wiki corpus, which is provided among
WMT14 corpora, and X as 50. The resulted pro-
jection by this algorithm can be used as the OOV
word conversion which projects from the source
language which among OOV words into the tar-
get language. The overall algorithm which uses
the projection which we build in the above step is
shown in the following.
</bodyText>
<listItem confidence="0.8521168">
1. Collect unknown words in the translation out-
puts.
2. Do Hindi named-entity recognition (NER) to
detect noun phrases.
3. If they are noun phrases, do the projection
from each unknown word in the source side
into the target words (We use the projection
prepared in the above steps). If they are not
noun phrases, run the transliteration to con-
vert each of them.
</listItem>
<bodyText confidence="0.99965825">
We perform Hindi NER by training CRF++ (Kudo
et al., 2004) using the Hindi named entity corpus,
and use the Hindi shallow parser (Begum et al.,
2008) for preprocessing of the inputs.
</bodyText>
<sectionHeader confidence="0.999556" genericHeader="evaluation">
4 Results and Discussion
</sectionHeader>
<subsectionHeader confidence="0.977686">
4.1 Data
</subsectionHeader>
<bodyText confidence="0.999914636363636">
We conduct our experiments on the standard
datasets released in the WMT14 shared translation
task. We use HindEnCorp8 (Bojar et al., 2014)
parallel corpus for MT system building. We also
used the CommonCrawl Hindi monolingual cor-
pus (Bojar et al., 2014) in order to build an addi-
tional language model for Hindi.
For the Hindi-to-English direction, we also em-
ployed monolingual English data used in the other
translation tasks for building the English language
model.
</bodyText>
<footnote confidence="0.987743">
8http://ufallab.ms.mff.cuni.cz/ bojar/hindencorp/
</footnote>
<subsectionHeader confidence="0.993233">
4.2 Moses Baseline
</subsectionHeader>
<bodyText confidence="0.9999905">
We employ a standard Moses PB-SMT model as
our baseline. The Hindi side is preprocessed but
unstemmed. We use Giza++ to perform word
alignment, the phrase table is extracted via the
grow-diag-final-and heuristic and the max-phrase-
length is set to 7.
</bodyText>
<subsectionHeader confidence="0.993522">
4.3 Automatic Evaluation
</subsectionHeader>
<figure confidence="0.51555825">
Experiments BLEU
Moses Baseline 8.7
Context-Based 9.4
Context-Based + CommonCrawl LM 11.4
</figure>
<tableCaption confidence="0.991099">
Table 1: BLEU scores of the English-to-Hindi MT
Systems on the WMT test set.
</tableCaption>
<table confidence="0.999818818181818">
Experiments BLEU
Moses Baseline 10.1
Context-Based 10.1
Suffix-Stripped 10.0
OWC 11.2
OSM 10.3
Three LRMs 10.5
MAC 10.7
SANPE 10.6
LMI 10.9
LMI+SANPE+MAC+ThreeLRMs+OSM 11.7
</table>
<tableCaption confidence="0.8810195">
Table 2: BLEU scores of the Hindi-to-English MT
Systems on the WMT test set.
</tableCaption>
<bodyText confidence="0.999964705882353">
We prepared a number of MT systems for both
English-to-Hindi and Hindi-to-English, and sub-
mitted their runs in the WMT 2014 Evaluation
Matrix. The BLEU scores of the different English-
to-Hindi MT systems (Moses Baseline, Context-
Based (CCG) MT system, and Context-Based
(CCG) MT system with an additional LM built
on the CommonCrawl Hindi monolingual corpus
(Bojar et al., 2014)) on the WMT 2014 English-
to-Hindi test set are reported in Table 1. As can
be seen from Table 1, Context-Based (CCG) MT
system produces 0.7 BLEU points improvement
(8.04% relative) over the Moses Baseline. When
we add an additional large LM built on the Com-
monCrawl data to the Context-Based (CCG) MT
system, we achieved a 2 BLEU-point improve-
ment (21.3% relative) (cf. last row in Table 1) over
</bodyText>
<page confidence="0.996398">
218
</page>
<bodyText confidence="0.9994225">
the Context-Based (CCG) MT system.9
The BLEU scores of the different Hindi-to-
English MT systems on the WMT 2014 Hindi-
to-English test set are reported in Table 2. The
first row of Table 2 shows the BLEU score for
the Baseline MT system. We note that the per-
formance of the Context-Based (PoS) MT system
obtains identical performance to the Moses base-
line (10.1 BLEU points) on the WMT 2014 Hindi-
to-English test set.
We employed a source language (Hindi) nor-
malisation technique, namely suffix separation,
but unfortunately this did not bring about any
improvement for the Hindi-to-English translation
task. The improvement gained by individually
employing OSM, three lexical reordering mod-
els, Multi-alignment Combination, Stem-align and
normal Phrase Extraction and Language Model In-
terpolation can be seen in Table 2. Our best sys-
tem is achieved by combining OSM, Three LMR,
MAC, SANPE and LMI, which results in a 1.6
BLEU point improvement over the Baseline.
</bodyText>
<sectionHeader confidence="0.998888" genericHeader="conclusions">
5 Acknowledgments
</sectionHeader>
<bodyText confidence="0.97346">
This research is supported by the Science Foun-
dation Ireland (Grant 12/CE/I2267) as part of
the CNGL Centre for Global Intelligent Content
(www.cngl.ie) at Dublin City University.
</bodyText>
<sectionHeader confidence="0.997684" genericHeader="references">
References
</sectionHeader>
<reference confidence="0.991072675675676">
Amittai Axelrod, Ra Birch Mayne, Chris Callison-
burch, Miles Osborne, and David Talbot. 2005. Ed-
inburgh system description for the 2005 iwslt speech
translation evaluation. In Proceedings of the Inter-
national Workshop on Spoken Language Translation
(IWSLT).
Rafiya Begum, Samar Husain, Arun Dhwaj,
Dipti Misra Sharma, Lakshmi Bai, and Rajeev
Sangal. 2008. Dependency annotation scheme for
indian languages. In Proceedings of The Third In-
ternational Joint Conference on Natural Language
Processing (IJCNLP).
Yoshua Bengio, Rejean Ducharme, and Pascal Vincent.
2000. A neural probabilistic language model. In
Proceedings of Neural Information Systems.
Ond Bojar, Pavel Stranak, and Daniel Zeman. 2010.
Data issues in english-to-hindi machine translation.
In LREC.
9Please note that this is an unconstrained submission.
Ondrej Bojar, V. Diatka, Rychly P., Pavel Stranak,
A. Tamchyna, and Daniel Zeman. 2014. Hindi-
english and hindi-only corpus for machine transla-
tion. In LREC.
Stanley F. Chen and Joshua Goodman. 1996. An em-
pirical study of smoothing techniques for language
modeling. In Proceedings of the 34th Annual Meet-
ing on Association for Computational Linguistics,
ACL ’96, pages 310–318, Stroudsburg, PA, USA.
Association for Computational Linguistics.
Walter Daelemans. 2005. Memory-based language
processing. Cambridge University Press.
Sajib Dasgupta and Vincent Ng. 2006. Unsupervised
morphological parsing of bengali. Language Re-
sources and Evaluation, 40(3-4):311–330.
Nadir Durrani, Helmut Schmid, and Alexander Fraser.
2011. A joint sequence translation model with in-
tegrated reordering. In Proceedings of the 49th An-
nual Meeting of the Association for Computational
Linguistics: Human Language Technologies - Vol-
ume 1, HLT ’11, pages 1045–1054, Stroudsburg, PA,
USA. Association for Computational Linguistics.
Michel Galley and Christopher D. Manning. 2008. A
simple and effective hierarchical phrase reordering
model. In Proceedings of the 2008 Conference on
Empirical Methods in Natural Language Process-
ing, pages 848–856, Honolulu, Hawaii, October. As-
sociation for Computational Linguistics.
Rejwanul Haque, Sudip Kumar Naskar, Antal van den
Bosch, and Andy Way. 2011. Integrating source-
language context into phrase-based statistical ma-
chine translation. Machine translation, 25(3):239–
285.
Rejwanul Haque, Sergio Penkale, Jie Jiang, and Andy
Way. 2012. Source-side suffix stripping for bengali-
to-english smt. In Asian Language Processing
(IALP), 2012 International Conference on, pages
193–196. IEEE.
Philipp Koehn, Franz Josef Och, and Daniel Marcu.
2003. Statistical phrase-based translation. In
Proceedings of the 2003 Conference of the North
American Chapter of the Association for Computa-
tional Linguistics on Human Language Technology-
Volume 1, pages 48–54. Association for Computa-
tional Linguistics.
Philipp Koehn, Hieu Hoang, Alexandra Birch, Chris
Callison-Burch, Marcello Federico, Nicola Bertoldi,
Brooke Cowan, Wade Shen, Christine Moran,
Richard Zens, Chris Dyer, Ondˇrej Bojar, Alexandra
Constantin, and Evan Herbst. 2007. Moses: Open
source toolkit for statistical machine translation. In
Proceedings of the 45th Annual Meeting of the ACL
on Interactive Poster and Demonstration Sessions,
ACL ’07, pages 177–180, Stroudsburg, PA, USA.
Association for Computational Linguistics.
</reference>
<page confidence="0.986833">
219
</page>
<reference confidence="0.999761095238095">
Taku Kudo, Kaoru Yamamoto, and Yuji Matsumoto.
2004. Appliying conditional random fields to
japanese morphological analysis. In Proceedings of
EMNLP.
Percy Liang, Ben Taskar, and Dan Klein. 2006. Align-
ment by agreement. In Proceedings of the main
conference on Human Language Technology Con-
ference of the North American Chapter of the As-
sociation of Computational Linguistics, pages 104–
111. Association for Computational Linguistics.
Tomas Mikolov, Quoc V. Le, and Ilya Sutskever. 2013.
Exploiting similarities among languages for ma-
chine translation. ArXiv.
Franz Josef Och and Hermann Ney. 2003. A sys-
tematic comparison of various statistical alignment
models. Computational linguistics, 29(1):19–51.
Franz Josef Och. 2003. Minimum error rate training in
statistical machine translation. In Proceedings of the
41st Annual Meeting on Association for Computa-
tional Linguistics - Volume 1, ACL ’03, pages 160–
167, Stroudsburg, PA, USA. Association for Com-
putational Linguistics.
Tsuyoshi Okita, Ali Hosseinzadeh Vahid, Andy Way,
and Qun Liu. 2014. Dcu terminology translation
system for medical query subtask at wmt14.
Mark Palatucci, Dean Pomerleau, Geoffrey Hinton,
and Tom Mitchell. 2009. Zero-shot learning with
semantic output codes. In Neural Information Pro-
cessing Systems (NIPS), December.
Ananthakrishnan Ramanathan and Durgesh D Rao.
2003. A lightweight stemmer for hindi. In the Pro-
ceedings of EACL.
Mark Steedman. 2000. The syntactic process, vol-
ume 35. MIT Press.
Andreas Stolcke. 2002. Srilm – an extensible lan-
guage modeling toolkit. In Proceedings of the Inter-
national Conference Spoken Language Processing,
pages 901–904, Denver, CO.
Zhaopeng Tu, Yang Liu, Yifan He, Josef van Genabith,
Qun Liu, and Shouxun Lin. 2012. Combining mul-
tiple alignments to improve machine translation. In
COLING (Posters), pages 1249–1260.
</reference>
<page confidence="0.997589">
220
</page>
</variant>
</algorithm>
<algorithm name="ParsHed" version="110505">
<variant no="0" confidence="0.213020">
<title confidence="0.93838">DCU-Lingo24 Participation in WMT 2014 Hindi-English Translation task</title>
<author confidence="0.8800725">Xiaofeng Wu</author>
<author confidence="0.8800725">Rejwanul Haque</author>
<author confidence="0.8800725">Tsuyoshi Piyush Arora</author>
<author confidence="0.8800725">Andy Way</author>
<author confidence="0.8800725">Qun</author>
<affiliation confidence="0.851237333333333">CNGL, Centre for Global Intelligent School of Computing, Dublin City Dublin 9,</affiliation>
<address confidence="0.983856">Lingo24, Edinburgh,</address>
<email confidence="0.99969">rejwanul.haque@lingo24.com</email>
<abstract confidence="0.828310785714286">This paper describes the to WMT 2014 for the Hindi- English translation task. We miscellaneous methods in our system, including: Context-Informed PB-SMT, OOV Word Conversion (OWC), Multi- Alignment Combination (MAC), Operation Sequence Model (OSM), Stemming Align and Normal Phrase Extraction (SANPE), and Language Model Interpolation (LMI). We also describe various preprocessing steps we tried for Hindi in this task.</abstract>
</variant>
</algorithm>
<algorithm name="ParsCit" version="110505">
<citationList>
<citation valid="true">
<authors>
<author>Amittai Axelrod</author>
<author>Ra Birch Mayne</author>
<author>Chris Callisonburch</author>
<author>Miles Osborne</author>
<author>David Talbot</author>
</authors>
<title>Edinburgh system description for the 2005 iwslt speech translation evaluation.</title>
<date>2005</date>
<booktitle>In Proceedings of the International Workshop on Spoken Language Translation (IWSLT).</booktitle>
<contexts>
<context position="1382" citStr="Axelrod et al., 2005" startWordPosition="192" endWordPosition="195">essing steps we tried for Hindi in this task. 1 Introduction This paper describes the DCU-Lingo24 submission to WMT 2014 for the Hindi-English translation task. All our experiments on WMT 2014 are built upon the Moses phrase-based model (PB-SMT) (Koehn et al., 2007) and tuned with MERT (Och, 2003). Starting from this baseline system, we exploit various methods including ContextInformed PB-SMT (CIPBSMT), zero-shot learning (Palatucci et al., 2009) using neural networkbased language modelling (Bengio et al., 2000; Mikolov et al., 2013) for OOV word conversion, various lexical reordering models (Axelrod et al., 2005; Galley and Manning, 2008), various Multiple Alignment Combination (MAC) (Tu et al., 2012), Operation Sequence Model (OSM) (Durrani et al., 2011) and Language Model Interpolation(LMI). In the next section, the preprocessing steps are explained. In Section 3 a detailed explanation of the technique we exploit is provided. Then in Section 4, we provide our experimental results and resultant discussion. 2 Pre-processing Steps We use all the training data provided for Hindi– English translation. Following Bojar et al. (2010), we apply a number of normalisation methods on the Hindi corpus. The Hind</context>
</contexts>
<marker>Axelrod, Mayne, Callisonburch, Osborne, Talbot, 2005</marker>
<rawString>Amittai Axelrod, Ra Birch Mayne, Chris Callisonburch, Miles Osborne, and David Talbot. 2005. Edinburgh system description for the 2005 iwslt speech translation evaluation. In Proceedings of the International Workshop on Spoken Language Translation (IWSLT).</rawString>
</citation>
<citation valid="true">
<authors>
<author>Rafiya Begum</author>
<author>Samar Husain</author>
<author>Arun Dhwaj</author>
<author>Dipti Misra Sharma</author>
<author>Lakshmi Bai</author>
<author>Rajeev Sangal</author>
</authors>
<title>Dependency annotation scheme for indian languages.</title>
<date>2008</date>
<booktitle>In Proceedings of The Third International Joint Conference on Natural Language Processing (IJCNLP).</booktitle>
<contexts>
<context position="12691" citStr="Begum et al., 2008" startWordPosition="1974" endWordPosition="1977">he overall algorithm which uses the projection which we build in the above step is shown in the following. 1. Collect unknown words in the translation outputs. 2. Do Hindi named-entity recognition (NER) to detect noun phrases. 3. If they are noun phrases, do the projection from each unknown word in the source side into the target words (We use the projection prepared in the above steps). If they are not noun phrases, run the transliteration to convert each of them. We perform Hindi NER by training CRF++ (Kudo et al., 2004) using the Hindi named entity corpus, and use the Hindi shallow parser (Begum et al., 2008) for preprocessing of the inputs. 4 Results and Discussion 4.1 Data We conduct our experiments on the standard datasets released in the WMT14 shared translation task. We use HindEnCorp8 (Bojar et al., 2014) parallel corpus for MT system building. We also used the CommonCrawl Hindi monolingual corpus (Bojar et al., 2014) in order to build an additional language model for Hindi. For the Hindi-to-English direction, we also employed monolingual English data used in the other translation tasks for building the English language model. 8http://ufallab.ms.mff.cuni.cz/ bojar/hindencorp/ 4.2 Moses Basel</context>
</contexts>
<marker>Begum, Husain, Dhwaj, Sharma, Bai, Sangal, 2008</marker>
<rawString>Rafiya Begum, Samar Husain, Arun Dhwaj, Dipti Misra Sharma, Lakshmi Bai, and Rajeev Sangal. 2008. Dependency annotation scheme for indian languages. In Proceedings of The Third International Joint Conference on Natural Language Processing (IJCNLP).</rawString>
</citation>
<citation valid="true">
<authors>
<author>Yoshua Bengio</author>
<author>Rejean Ducharme</author>
<author>Pascal Vincent</author>
</authors>
<title>A neural probabilistic language model.</title>
<date>2000</date>
<booktitle>In Proceedings of Neural Information Systems.</booktitle>
<contexts>
<context position="1278" citStr="Bengio et al., 2000" startWordPosition="176" endWordPosition="179">mal Phrase Extraction (SANPE), and Language Model Interpolation (LMI). We also describe various preprocessing steps we tried for Hindi in this task. 1 Introduction This paper describes the DCU-Lingo24 submission to WMT 2014 for the Hindi-English translation task. All our experiments on WMT 2014 are built upon the Moses phrase-based model (PB-SMT) (Koehn et al., 2007) and tuned with MERT (Och, 2003). Starting from this baseline system, we exploit various methods including ContextInformed PB-SMT (CIPBSMT), zero-shot learning (Palatucci et al., 2009) using neural networkbased language modelling (Bengio et al., 2000; Mikolov et al., 2013) for OOV word conversion, various lexical reordering models (Axelrod et al., 2005; Galley and Manning, 2008), various Multiple Alignment Combination (MAC) (Tu et al., 2012), Operation Sequence Model (OSM) (Durrani et al., 2011) and Language Model Interpolation(LMI). In the next section, the preprocessing steps are explained. In Section 3 a detailed explanation of the technique we exploit is provided. Then in Section 4, we provide our experimental results and resultant discussion. 2 Pre-processing Steps We use all the training data provided for Hindi– English translation.</context>
<context position="10457" citStr="Bengio et al., 2000" startWordPosition="1596" endWordPosition="1599">rd alignment sparsity, which results in poor alignment quality. Furthermore, word stemming on the Hindi side usually results in too many English words being aligned to one stemmed Hindi word, i.e. we encounter the problem of phrase overextraction. Therefore, we conduct word alignment with the stemmed version of Hindi, and then at the phrase extraction step, we replace the stemmed form with the original Hindi form. 3.8 OOV Word Conversion Method Our algorithm for OOV word conversion uses the recently developed zero-shot learning (Palatucci et al., 2009) using neural network language modelling (Bengio et al., 2000; Mikolov et al., 2013). The same technique is used in (Okita et al., 2014). This method requires neither parallel nor comparable corpora, but rather two monolingual corpora. In our context, we prepare two monolingual corpora on both sides, which are neither parallel nor comparable, and a small amount of already known correspondences between words on the source and target sides (henceforth, we refer to this as the ‘dictionary’). Then, we train both sides with the neural network language model, and use a continuous space representation to project words to each other on the basis of a small amou</context>
</contexts>
<marker>Bengio, Ducharme, Vincent, 2000</marker>
<rawString>Yoshua Bengio, Rejean Ducharme, and Pascal Vincent. 2000. A neural probabilistic language model. In Proceedings of Neural Information Systems.</rawString>
</citation>
<citation valid="true">
<authors>
<author>Ond Bojar</author>
<author>Pavel Stranak</author>
<author>Daniel Zeman</author>
</authors>
<title>Data issues in english-to-hindi machine translation. In LREC. 9Please note that this is an unconstrained submission.</title>
<date>2010</date>
<contexts>
<context position="1908" citStr="Bojar et al. (2010)" startWordPosition="275" endWordPosition="278">t al., 2013) for OOV word conversion, various lexical reordering models (Axelrod et al., 2005; Galley and Manning, 2008), various Multiple Alignment Combination (MAC) (Tu et al., 2012), Operation Sequence Model (OSM) (Durrani et al., 2011) and Language Model Interpolation(LMI). In the next section, the preprocessing steps are explained. In Section 3 a detailed explanation of the technique we exploit is provided. Then in Section 4, we provide our experimental results and resultant discussion. 2 Pre-processing Steps We use all the training data provided for Hindi– English translation. Following Bojar et al. (2010), we apply a number of normalisation methods on the Hindi corpus. The HindEnCorp parallel corpus compiles several sources of parallel data. We observe that the source-side (Hindi) of the TIDES data source contains font-related noise, i.e. many Hindi sentences are a mixture of two different encodings: UTF-81 and WX2 notations. We prepared a WX-to-UTF-8 font conversion script for Hindi which converts all WX encoded characters into UTF-8, thus removing all WX encoding appearing in the TIDES data. We also observe that a portion of the English training corpus contained the following bracketlike seq</context>
</contexts>
<marker>Bojar, Stranak, Zeman, 2010</marker>
<rawString>Ond Bojar, Pavel Stranak, and Daniel Zeman. 2010. Data issues in english-to-hindi machine translation. In LREC. 9Please note that this is an unconstrained submission.</rawString>
</citation>
<citation valid="true">
<authors>
<author>Ondrej Bojar</author>
<author>V Diatka</author>
<author>P Rychly</author>
<author>Pavel Stranak</author>
<author>A Tamchyna</author>
<author>Daniel Zeman</author>
</authors>
<title>Hindienglish and hindi-only corpus for machine translation.</title>
<date>2014</date>
<booktitle>In LREC.</booktitle>
<contexts>
<context position="12897" citStr="Bojar et al., 2014" startWordPosition="2007" endWordPosition="2010">detect noun phrases. 3. If they are noun phrases, do the projection from each unknown word in the source side into the target words (We use the projection prepared in the above steps). If they are not noun phrases, run the transliteration to convert each of them. We perform Hindi NER by training CRF++ (Kudo et al., 2004) using the Hindi named entity corpus, and use the Hindi shallow parser (Begum et al., 2008) for preprocessing of the inputs. 4 Results and Discussion 4.1 Data We conduct our experiments on the standard datasets released in the WMT14 shared translation task. We use HindEnCorp8 (Bojar et al., 2014) parallel corpus for MT system building. We also used the CommonCrawl Hindi monolingual corpus (Bojar et al., 2014) in order to build an additional language model for Hindi. For the Hindi-to-English direction, we also employed monolingual English data used in the other translation tasks for building the English language model. 8http://ufallab.ms.mff.cuni.cz/ bojar/hindencorp/ 4.2 Moses Baseline We employ a standard Moses PB-SMT model as our baseline. The Hindi side is preprocessed but unstemmed. We use Giza++ to perform word alignment, the phrase table is extracted via the grow-diag-final-and </context>
<context position="14363" citStr="Bojar et al., 2014" startWordPosition="2233" endWordPosition="2236">ents BLEU Moses Baseline 10.1 Context-Based 10.1 Suffix-Stripped 10.0 OWC 11.2 OSM 10.3 Three LRMs 10.5 MAC 10.7 SANPE 10.6 LMI 10.9 LMI+SANPE+MAC+ThreeLRMs+OSM 11.7 Table 2: BLEU scores of the Hindi-to-English MT Systems on the WMT test set. We prepared a number of MT systems for both English-to-Hindi and Hindi-to-English, and submitted their runs in the WMT 2014 Evaluation Matrix. The BLEU scores of the different Englishto-Hindi MT systems (Moses Baseline, ContextBased (CCG) MT system, and Context-Based (CCG) MT system with an additional LM built on the CommonCrawl Hindi monolingual corpus (Bojar et al., 2014)) on the WMT 2014 Englishto-Hindi test set are reported in Table 1. As can be seen from Table 1, Context-Based (CCG) MT system produces 0.7 BLEU points improvement (8.04% relative) over the Moses Baseline. When we add an additional large LM built on the CommonCrawl data to the Context-Based (CCG) MT system, we achieved a 2 BLEU-point improvement (21.3% relative) (cf. last row in Table 1) over 218 the Context-Based (CCG) MT system.9 The BLEU scores of the different Hindi-toEnglish MT systems on the WMT 2014 Hindito-English test set are reported in Table 2. The first row of Table 2 shows the BLE</context>
</contexts>
<marker>Bojar, Diatka, Rychly, Stranak, Tamchyna, Zeman, 2014</marker>
<rawString>Ondrej Bojar, V. Diatka, Rychly P., Pavel Stranak, A. Tamchyna, and Daniel Zeman. 2014. Hindienglish and hindi-only corpus for machine translation. In LREC.</rawString>
</citation>
<citation valid="true">
<authors>
<author>Stanley F Chen</author>
<author>Joshua Goodman</author>
</authors>
<title>An empirical study of smoothing techniques for language modeling.</title>
<date>1996</date>
<booktitle>In Proceedings of the 34th Annual Meeting on Association for Computational Linguistics, ACL ’96,</booktitle>
<pages>310--318</pages>
<publisher>Association</publisher>
<institution>for Computational Linguistics.</institution>
<location>Stroudsburg, PA, USA.</location>
<contexts>
<context position="4961" citStr="Chen and Goodman, 1996" startWordPosition="745" endWordPosition="748">including data from the English Gigaword fifth edition, the English side of the UN corpus, the English side of the 109 French–English corpus and the English side of the Hindi–English parallel data provided by the organisers. We interpolate language models trained using each dataset, with the monolingual data provided split into three parts (news 2007-2013, Europarl (?) and news commentary) and the weights tuned to minimize perplexity on the target side of the devset. The language models in our systems are trained with SRILM (Stolcke, 2002). We train a 5-gram model with Kneser-Ney discounting (Chen and Goodman, 1996). 3.4 Context-informed PB-SMT Haque et al. (2011) express a context-dependent phrase translation as a multi-class classification problem, where a source phrase with given additional context information is classified into a distribution over possible target phrases. The size of this distribution needs to be limited, and would ideally omit irrelevant target phrase translations that the standard PB-SMT (Koehn et al., 2003) approach would normally include. Following Haque et al. (2011), we derive a context-informed feature ˆhmbl that is expressed as the conditional probability of the target phrase</context>
</contexts>
<marker>Chen, Goodman, 1996</marker>
<rawString>Stanley F. Chen and Joshua Goodman. 1996. An empirical study of smoothing techniques for language modeling. In Proceedings of the 34th Annual Meeting on Association for Computational Linguistics, ACL ’96, pages 310–318, Stroudsburg, PA, USA. Association for Computational Linguistics.</rawString>
</citation>
<citation valid="true">
<authors>
<author>Walter Daelemans</author>
</authors>
<title>Memory-based language processing.</title>
<date>2005</date>
<publisher>Cambridge University Press.</publisher>
<contexts>
<context position="6834" citStr="Daelemans, 2005" startWordPosition="1045" endWordPosition="1046">mposing that phrase. We generate a window of size 2l + 1 features (we set l:=2), including the concatenated complex supertag of the focus phrase. Accordingly, the supertag-based contextual information (CIgt) is described as in (3): ˆfk) = {st(fik−l), ..., st(fik−1), st(ˆfk), (3) st(fjk+1), ..., st(fjk+l)} For the Hindi-to-English translation task, we use part-of-speech (PoS) tags4 of the source phrase and the neighbouring words as the contextual feature, owing to the fact that supertaggers are readily available only for English. We use a memory-based machine learning (MBL) classifier (TRIBL: (Daelemans, 2005))5 that is able to estimate P(ˆek |ˆfk, CI( ˆfk)) by similarity-based reasoning over memorized nearest-neighbour examples of source–target phrase translations. Thus, we derive the feature ˆhmbl defined in Equation (2). In addition to ˆhmbl, 4In order to obtain PoS tags of Hindi words, we used the LTRC shallow parser for Hindi from http://ltrc.iiit.ac.in/analyzer/hindi/shallow-parser-hin4.0.fc8.tar.gz. 5An implementation of TRIBL is freely available as part of the TiMBL software package, which can be downloaded from http://ilk.uvt.nl/timbl. ˆfk, CI( ˆfk)) (2) CIgt( 216 we derive a simple two-va</context>
</contexts>
<marker>Daelemans, 2005</marker>
<rawString>Walter Daelemans. 2005. Memory-based language processing. Cambridge University Press.</rawString>
</citation>
<citation valid="true">
<authors>
<author>Sajib Dasgupta</author>
<author>Vincent Ng</author>
</authors>
<title>Unsupervised morphological parsing of bengali. Language Resources and Evaluation,</title>
<date>2006</date>
<pages>40--3</pages>
<contexts>
<context position="8678" citStr="Dasgupta and Ng (2006)" startWordPosition="1319" endWordPosition="1323">a morphological suffix separation process in a Bengali-to-English translation task and showed that suffix separation significantly reduces data sparseness in the Bengali corpus. They also showed an SMT model trained on the suffix-stripped training data significantly outperforms the state-of-the-art PB-SMT baseline. Like Bengali, Hindi is a morphologically very rich and highly inflected Indian language. As done previously for Bengali-to-English (Haque et al., 2012), we employ a suffix-stripping method for lemmatizing inflected Hindi words in the WMT Hindi-to-English translation task. Following Dasgupta and Ng (2006), we developed an unsupervised morphological segmentation method for Hindi. We also used a Hindi lightweight stemmer (Ramanathan and Rao, 2003) in order to prepare a training corpus with only Hindi stems. We prepared Hindi-to-English SMT systems on the both types of training data (i.e. suffix-stripped and stemmed).6 3.6 Multi-Alignment Combination (MAC) Word alignment is a critical component of MT systems. Various methods for word alignment have been proposed, and different models can produce signicantly different outputs. For example, Tu et al. (2012) demonstrates that the alignment agreement</context>
</contexts>
<marker>Dasgupta, Ng, 2006</marker>
<rawString>Sajib Dasgupta and Vincent Ng. 2006. Unsupervised morphological parsing of bengali. Language Resources and Evaluation, 40(3-4):311–330.</rawString>
</citation>
<citation valid="true">
<authors>
<author>Nadir Durrani</author>
<author>Helmut Schmid</author>
<author>Alexander Fraser</author>
</authors>
<title>A joint sequence translation model with integrated reordering.</title>
<date>2011</date>
<booktitle>In Proceedings of the 49th Annual Meeting of the Association for Computational Linguistics: Human Language Technologies - Volume 1, HLT ’11,</booktitle>
<pages>1045--1054</pages>
<publisher>Association</publisher>
<institution>for Computational Linguistics.</institution>
<location>Stroudsburg, PA, USA.</location>
<contexts>
<context position="1528" citStr="Durrani et al., 2011" startWordPosition="214" endWordPosition="218">anslation task. All our experiments on WMT 2014 are built upon the Moses phrase-based model (PB-SMT) (Koehn et al., 2007) and tuned with MERT (Och, 2003). Starting from this baseline system, we exploit various methods including ContextInformed PB-SMT (CIPBSMT), zero-shot learning (Palatucci et al., 2009) using neural networkbased language modelling (Bengio et al., 2000; Mikolov et al., 2013) for OOV word conversion, various lexical reordering models (Axelrod et al., 2005; Galley and Manning, 2008), various Multiple Alignment Combination (MAC) (Tu et al., 2012), Operation Sequence Model (OSM) (Durrani et al., 2011) and Language Model Interpolation(LMI). In the next section, the preprocessing steps are explained. In Section 3 a detailed explanation of the technique we exploit is provided. Then in Section 4, we provide our experimental results and resultant discussion. 2 Pre-processing Steps We use all the training data provided for Hindi– English translation. Following Bojar et al. (2010), we apply a number of normalisation methods on the Hindi corpus. The HindEnCorp parallel corpus compiles several sources of parallel data. We observe that the source-side (Hindi) of the TIDES data source contains font-r</context>
<context position="3829" citStr="Durrani et al. (2011)" startWordPosition="557" endWordPosition="561">2014. c�2014 Association for Computational Linguistics sentence pairs with length difference larger than 3 times. 3 Techniques Deployed 3.1 Combination of Various Lexical Reordering Model (LRM) Clearly, Hindi and English have quite different word orders, so we adopt three lexical reordering models to address this problem. They are wordbased LRM and phrase-based LRM, which mainly focus on local reordering phenomena, and hierarchical phrase-based LRM, which mainly focuses on longer distance reordering (Galley and Manning, 2008). 3.2 Operation Sequence Model The Operation Sequence Model (OSM) of Durrani et al. (2011) defines four translation operations: Generate(X,Y), Continue Source Concept, Generate Source Only (X) and Generate Identical, as well as three reordering operations: Insert Gap, Jump Back(W) and Jump Forward. The probability of an operation sequence O = (o1o2 · · · oJ) is calculated as in (1): J p(O) = p(oj|oj−n+1 ··· oj−1) (1) j=1 where n indicates the number of previous operations used. We employ a 9-order OSM in our framework. 3.3 Language Model Interpolation (LMI) We build a large language model by including data from the English Gigaword fifth edition, the English side of the UN corpus, </context>
</contexts>
<marker>Durrani, Schmid, Fraser, 2011</marker>
<rawString>Nadir Durrani, Helmut Schmid, and Alexander Fraser. 2011. A joint sequence translation model with integrated reordering. In Proceedings of the 49th Annual Meeting of the Association for Computational Linguistics: Human Language Technologies - Volume 1, HLT ’11, pages 1045–1054, Stroudsburg, PA, USA. Association for Computational Linguistics.</rawString>
</citation>
<citation valid="true">
<authors>
<author>Michel Galley</author>
<author>Christopher D Manning</author>
</authors>
<title>A simple and effective hierarchical phrase reordering model.</title>
<date>2008</date>
<booktitle>In Proceedings of the 2008 Conference on Empirical Methods in Natural Language Processing,</booktitle>
<pages>848--856</pages>
<publisher>Association</publisher>
<institution>for Computational Linguistics.</institution>
<location>Honolulu, Hawaii,</location>
<contexts>
<context position="1409" citStr="Galley and Manning, 2008" startWordPosition="196" endWordPosition="199">for Hindi in this task. 1 Introduction This paper describes the DCU-Lingo24 submission to WMT 2014 for the Hindi-English translation task. All our experiments on WMT 2014 are built upon the Moses phrase-based model (PB-SMT) (Koehn et al., 2007) and tuned with MERT (Och, 2003). Starting from this baseline system, we exploit various methods including ContextInformed PB-SMT (CIPBSMT), zero-shot learning (Palatucci et al., 2009) using neural networkbased language modelling (Bengio et al., 2000; Mikolov et al., 2013) for OOV word conversion, various lexical reordering models (Axelrod et al., 2005; Galley and Manning, 2008), various Multiple Alignment Combination (MAC) (Tu et al., 2012), Operation Sequence Model (OSM) (Durrani et al., 2011) and Language Model Interpolation(LMI). In the next section, the preprocessing steps are explained. In Section 3 a detailed explanation of the technique we exploit is provided. Then in Section 4, we provide our experimental results and resultant discussion. 2 Pre-processing Steps We use all the training data provided for Hindi– English translation. Following Bojar et al. (2010), we apply a number of normalisation methods on the Hindi corpus. The HindEnCorp parallel corpus comp</context>
<context position="3739" citStr="Galley and Manning, 2008" startWordPosition="542" endWordPosition="546">kshop on Statistical Machine Translation, pages 215–220, Baltimore, Maryland USA, June 26–27, 2014. c�2014 Association for Computational Linguistics sentence pairs with length difference larger than 3 times. 3 Techniques Deployed 3.1 Combination of Various Lexical Reordering Model (LRM) Clearly, Hindi and English have quite different word orders, so we adopt three lexical reordering models to address this problem. They are wordbased LRM and phrase-based LRM, which mainly focus on local reordering phenomena, and hierarchical phrase-based LRM, which mainly focuses on longer distance reordering (Galley and Manning, 2008). 3.2 Operation Sequence Model The Operation Sequence Model (OSM) of Durrani et al. (2011) defines four translation operations: Generate(X,Y), Continue Source Concept, Generate Source Only (X) and Generate Identical, as well as three reordering operations: Insert Gap, Jump Back(W) and Jump Forward. The probability of an operation sequence O = (o1o2 · · · oJ) is calculated as in (1): J p(O) = p(oj|oj−n+1 ··· oj−1) (1) j=1 where n indicates the number of previous operations used. We employ a 9-order OSM in our framework. 3.3 Language Model Interpolation (LMI) We build a large language model by i</context>
</contexts>
<marker>Galley, Manning, 2008</marker>
<rawString>Michel Galley and Christopher D. Manning. 2008. A simple and effective hierarchical phrase reordering model. In Proceedings of the 2008 Conference on Empirical Methods in Natural Language Processing, pages 848–856, Honolulu, Hawaii, October. Association for Computational Linguistics.</rawString>
</citation>
<citation valid="true">
<authors>
<author>Rejwanul Haque</author>
<author>Sudip Kumar Naskar</author>
<author>Antal van den Bosch</author>
<author>Andy Way</author>
</authors>
<title>Integrating sourcelanguage context into phrase-based statistical machine translation.</title>
<date>2011</date>
<journal>Machine translation,</journal>
<volume>25</volume>
<issue>3</issue>
<pages>285</pages>
<marker>Haque, Naskar, van den Bosch, Way, 2011</marker>
<rawString>Rejwanul Haque, Sudip Kumar Naskar, Antal van den Bosch, and Andy Way. 2011. Integrating sourcelanguage context into phrase-based statistical machine translation. Machine translation, 25(3):239– 285.</rawString>
</citation>
<citation valid="true">
<authors>
<author>Rejwanul Haque</author>
<author>Sergio Penkale</author>
<author>Jie Jiang</author>
<author>Andy Way</author>
</authors>
<title>Source-side suffix stripping for bengalito-english smt.</title>
<date>2012</date>
<booktitle>In Asian Language Processing (IALP), 2012 International Conference on,</booktitle>
<pages>193--196</pages>
<publisher>IEEE.</publisher>
<contexts>
<context position="8047" citStr="Haque et al. (2012)" startWordPosition="1232" endWordPosition="1235">two-valued feature ˆhbest, defined in Equation (4): ˆhbest = � � 0 otherwise 1 if ˆek maximizes P(ˆek |ˆfk, CI( ˆfk)) (4) where ˆhbest is set to 1 when ˆek is one of the target phrases with highest probability according to P(ˆek |ˆfk, CI(ˆfk)) for each source phrase ˆfk; otherwise ˆhbest is set to 0.000001. We performed experiments by integrating these two features ˆhmbl and ˆhbest directly into the log-linear model of Moses. Their weights are optimized using minimum error-rate training (MERT)(Och, 2003) on a held-out development set for each of the experiments. 3.5 Morphological Segmentation Haque et al. (2012) applied a morphological suffix separation process in a Bengali-to-English translation task and showed that suffix separation significantly reduces data sparseness in the Bengali corpus. They also showed an SMT model trained on the suffix-stripped training data significantly outperforms the state-of-the-art PB-SMT baseline. Like Bengali, Hindi is a morphologically very rich and highly inflected Indian language. As done previously for Bengali-to-English (Haque et al., 2012), we employ a suffix-stripping method for lemmatizing inflected Hindi words in the WMT Hindi-to-English translation task. F</context>
</contexts>
<marker>Haque, Penkale, Jiang, Way, 2012</marker>
<rawString>Rejwanul Haque, Sergio Penkale, Jie Jiang, and Andy Way. 2012. Source-side suffix stripping for bengalito-english smt. In Asian Language Processing (IALP), 2012 International Conference on, pages 193–196. IEEE.</rawString>
</citation>
<citation valid="true">
<authors>
<author>Philipp Koehn</author>
<author>Franz Josef Och</author>
<author>Daniel Marcu</author>
</authors>
<title>Statistical phrase-based translation.</title>
<date>2003</date>
<booktitle>In Proceedings of the 2003 Conference of the North American Chapter of the Association for Computational Linguistics on Human Language TechnologyVolume 1,</booktitle>
<pages>48--54</pages>
<publisher>Association</publisher>
<institution>for Computational Linguistics.</institution>
<contexts>
<context position="5384" citStr="Koehn et al., 2003" startWordPosition="807" endWordPosition="810">perplexity on the target side of the devset. The language models in our systems are trained with SRILM (Stolcke, 2002). We train a 5-gram model with Kneser-Ney discounting (Chen and Goodman, 1996). 3.4 Context-informed PB-SMT Haque et al. (2011) express a context-dependent phrase translation as a multi-class classification problem, where a source phrase with given additional context information is classified into a distribution over possible target phrases. The size of this distribution needs to be limited, and would ideally omit irrelevant target phrase translations that the standard PB-SMT (Koehn et al., 2003) approach would normally include. Following Haque et al. (2011), we derive a context-informed feature ˆhmbl that is expressed as the conditional probability of the target phrase ˆek given the source phrase ˆfk and its context information (CI), as in (2): ˆhmbl = log P(ˆek| Here, CI may include any feature that can provide useful information to disambiguate the given source phrase. In our experiment, we use CCG supertag (Steedman, 2000) as a contextual features. CCG supertag expresses the specific syntactic behaviour of a word in terms of the arguments it takes, and more generally the syntactic</context>
</contexts>
<marker>Koehn, Och, Marcu, 2003</marker>
<rawString>Philipp Koehn, Franz Josef Och, and Daniel Marcu. 2003. Statistical phrase-based translation. In Proceedings of the 2003 Conference of the North American Chapter of the Association for Computational Linguistics on Human Language TechnologyVolume 1, pages 48–54. Association for Computational Linguistics.</rawString>
</citation>
<citation valid="false">
<authors>
<author>Philipp Koehn</author>
<author>Hieu Hoang</author>
<author>Alexandra Birch</author>
<author>Chris Callison-Burch</author>
<author>Marcello Federico</author>
<author>Nicola Bertoldi</author>
<author>Brooke Cowan</author>
<author>Wade Shen</author>
<author>Christine Moran</author>
<author>Richard Zens</author>
<author>Chris Dyer</author>
<author>Ondˇrej Bojar</author>
<author>Alexandra Constantin</author>
<author>Evan Herbst</author>
</authors>
<title>Moses: Open source toolkit for statistical machine translation.</title>
<date>2007</date>
<booktitle>In Proceedings of the 45th Annual Meeting of the ACL on Interactive Poster and Demonstration Sessions, ACL ’07,</booktitle>
<pages>177--180</pages>
<publisher>Association</publisher>
<institution>for Computational Linguistics.</institution>
<location>Stroudsburg, PA, USA.</location>
<contexts>
<context position="1028" citStr="Koehn et al., 2007" startWordPosition="139" endWordPosition="142">to WMT 2014 for the HindiEnglish translation task. We exploit miscellaneous methods in our system, including: Context-Informed PB-SMT, OOV Word Conversion (OWC), MultiAlignment Combination (MAC), Operation Sequence Model (OSM), Stemming Align and Normal Phrase Extraction (SANPE), and Language Model Interpolation (LMI). We also describe various preprocessing steps we tried for Hindi in this task. 1 Introduction This paper describes the DCU-Lingo24 submission to WMT 2014 for the Hindi-English translation task. All our experiments on WMT 2014 are built upon the Moses phrase-based model (PB-SMT) (Koehn et al., 2007) and tuned with MERT (Och, 2003). Starting from this baseline system, we exploit various methods including ContextInformed PB-SMT (CIPBSMT), zero-shot learning (Palatucci et al., 2009) using neural networkbased language modelling (Bengio et al., 2000; Mikolov et al., 2013) for OOV word conversion, various lexical reordering models (Axelrod et al., 2005; Galley and Manning, 2008), various Multiple Alignment Combination (MAC) (Tu et al., 2012), Operation Sequence Model (OSM) (Durrani et al., 2011) and Language Model Interpolation(LMI). In the next section, the preprocessing steps are explained. </context>
</contexts>
<marker>Koehn, Hoang, Birch, Callison-Burch, Federico, Bertoldi, Cowan, Shen, Moran, Zens, Dyer, Bojar, Constantin, Herbst, 2007</marker>
<rawString>Philipp Koehn, Hieu Hoang, Alexandra Birch, Chris Callison-Burch, Marcello Federico, Nicola Bertoldi, Brooke Cowan, Wade Shen, Christine Moran, Richard Zens, Chris Dyer, Ondˇrej Bojar, Alexandra Constantin, and Evan Herbst. 2007. Moses: Open source toolkit for statistical machine translation. In Proceedings of the 45th Annual Meeting of the ACL on Interactive Poster and Demonstration Sessions, ACL ’07, pages 177–180, Stroudsburg, PA, USA. Association for Computational Linguistics.</rawString>
</citation>
<citation valid="true">
<authors>
<author>Taku Kudo</author>
<author>Kaoru Yamamoto</author>
<author>Yuji Matsumoto</author>
</authors>
<title>Appliying conditional random fields to japanese morphological analysis.</title>
<date>2004</date>
<booktitle>In Proceedings of EMNLP.</booktitle>
<contexts>
<context position="12600" citStr="Kudo et al., 2004" startWordPosition="1958" endWordPosition="1961"> which projects from the source language which among OOV words into the target language. The overall algorithm which uses the projection which we build in the above step is shown in the following. 1. Collect unknown words in the translation outputs. 2. Do Hindi named-entity recognition (NER) to detect noun phrases. 3. If they are noun phrases, do the projection from each unknown word in the source side into the target words (We use the projection prepared in the above steps). If they are not noun phrases, run the transliteration to convert each of them. We perform Hindi NER by training CRF++ (Kudo et al., 2004) using the Hindi named entity corpus, and use the Hindi shallow parser (Begum et al., 2008) for preprocessing of the inputs. 4 Results and Discussion 4.1 Data We conduct our experiments on the standard datasets released in the WMT14 shared translation task. We use HindEnCorp8 (Bojar et al., 2014) parallel corpus for MT system building. We also used the CommonCrawl Hindi monolingual corpus (Bojar et al., 2014) in order to build an additional language model for Hindi. For the Hindi-to-English direction, we also employed monolingual English data used in the other translation tasks for building th</context>
</contexts>
<marker>Kudo, Yamamoto, Matsumoto, 2004</marker>
<rawString>Taku Kudo, Kaoru Yamamoto, and Yuji Matsumoto. 2004. Appliying conditional random fields to japanese morphological analysis. In Proceedings of EMNLP.</rawString>
</citation>
<citation valid="true">
<authors>
<author>Percy Liang</author>
<author>Ben Taskar</author>
<author>Dan Klein</author>
</authors>
<title>Alignment by agreement.</title>
<date>2006</date>
<booktitle>In Proceedings of the main conference on Human Language Technology Conference of the North American Chapter of the Association of Computational Linguistics,</booktitle>
<pages>104--111</pages>
<publisher>Association</publisher>
<institution>for Computational Linguistics.</institution>
<contexts>
<context position="9474" citStr="Liang et al., 2006" startWordPosition="1440" endWordPosition="1443">rpus with only Hindi stems. We prepared Hindi-to-English SMT systems on the both types of training data (i.e. suffix-stripped and stemmed).6 3.6 Multi-Alignment Combination (MAC) Word alignment is a critical component of MT systems. Various methods for word alignment have been proposed, and different models can produce signicantly different outputs. For example, Tu et al. (2012) demonstrates that the alignment agreement between the two best-known alignment tools, namely Giza++(Och and Ney, 2003) and 6Suffixes were separated and completely removed from the training data. the Berkeley aligner7 (Liang et al., 2006), is below 70%. Taking into consideration the small size of the the corpus, in order to extract more effective phrase tables, we concatenate three alignments: Giza++ with grow-diag-final-and, Giza++ with intersection, and that derived from the Berkeley aligner. 3.7 Stemming Alignment and Normal Phrase Extraction (SANPE) The rich morphology of Hindi will cause word alignment sparsity, which results in poor alignment quality. Furthermore, word stemming on the Hindi side usually results in too many English words being aligned to one stemmed Hindi word, i.e. we encounter the problem of phrase over</context>
</contexts>
<marker>Liang, Taskar, Klein, 2006</marker>
<rawString>Percy Liang, Ben Taskar, and Dan Klein. 2006. Alignment by agreement. In Proceedings of the main conference on Human Language Technology Conference of the North American Chapter of the Association of Computational Linguistics, pages 104– 111. Association for Computational Linguistics.</rawString>
</citation>
<citation valid="true">
<authors>
<author>Tomas Mikolov</author>
<author>Quoc V Le</author>
<author>Ilya Sutskever</author>
</authors>
<title>Exploiting similarities among languages for machine translation.</title>
<date>2013</date>
<journal>ArXiv.</journal>
<contexts>
<context position="1301" citStr="Mikolov et al., 2013" startWordPosition="180" endWordPosition="183"> (SANPE), and Language Model Interpolation (LMI). We also describe various preprocessing steps we tried for Hindi in this task. 1 Introduction This paper describes the DCU-Lingo24 submission to WMT 2014 for the Hindi-English translation task. All our experiments on WMT 2014 are built upon the Moses phrase-based model (PB-SMT) (Koehn et al., 2007) and tuned with MERT (Och, 2003). Starting from this baseline system, we exploit various methods including ContextInformed PB-SMT (CIPBSMT), zero-shot learning (Palatucci et al., 2009) using neural networkbased language modelling (Bengio et al., 2000; Mikolov et al., 2013) for OOV word conversion, various lexical reordering models (Axelrod et al., 2005; Galley and Manning, 2008), various Multiple Alignment Combination (MAC) (Tu et al., 2012), Operation Sequence Model (OSM) (Durrani et al., 2011) and Language Model Interpolation(LMI). In the next section, the preprocessing steps are explained. In Section 3 a detailed explanation of the technique we exploit is provided. Then in Section 4, we provide our experimental results and resultant discussion. 2 Pre-processing Steps We use all the training data provided for Hindi– English translation. Following Bojar et al.</context>
<context position="10480" citStr="Mikolov et al., 2013" startWordPosition="1600" endWordPosition="1603">, which results in poor alignment quality. Furthermore, word stemming on the Hindi side usually results in too many English words being aligned to one stemmed Hindi word, i.e. we encounter the problem of phrase overextraction. Therefore, we conduct word alignment with the stemmed version of Hindi, and then at the phrase extraction step, we replace the stemmed form with the original Hindi form. 3.8 OOV Word Conversion Method Our algorithm for OOV word conversion uses the recently developed zero-shot learning (Palatucci et al., 2009) using neural network language modelling (Bengio et al., 2000; Mikolov et al., 2013). The same technique is used in (Okita et al., 2014). This method requires neither parallel nor comparable corpora, but rather two monolingual corpora. In our context, we prepare two monolingual corpora on both sides, which are neither parallel nor comparable, and a small amount of already known correspondences between words on the source and target sides (henceforth, we refer to this as the ‘dictionary’). Then, we train both sides with the neural network language model, and use a continuous space representation to project words to each other on the basis of a small amount of correspondences i</context>
</contexts>
<marker>Mikolov, Le, Sutskever, 2013</marker>
<rawString>Tomas Mikolov, Quoc V. Le, and Ilya Sutskever. 2013. Exploiting similarities among languages for machine translation. ArXiv.</rawString>
</citation>
<citation valid="true">
<authors>
<author>Franz Josef Och</author>
<author>Hermann Ney</author>
</authors>
<title>A systematic comparison of various statistical alignment models.</title>
<date>2003</date>
<journal>Computational linguistics,</journal>
<pages>29--1</pages>
<contexts>
<context position="9355" citStr="Och and Ney, 2003" startWordPosition="1422" endWordPosition="1425">ethod for Hindi. We also used a Hindi lightweight stemmer (Ramanathan and Rao, 2003) in order to prepare a training corpus with only Hindi stems. We prepared Hindi-to-English SMT systems on the both types of training data (i.e. suffix-stripped and stemmed).6 3.6 Multi-Alignment Combination (MAC) Word alignment is a critical component of MT systems. Various methods for word alignment have been proposed, and different models can produce signicantly different outputs. For example, Tu et al. (2012) demonstrates that the alignment agreement between the two best-known alignment tools, namely Giza++(Och and Ney, 2003) and 6Suffixes were separated and completely removed from the training data. the Berkeley aligner7 (Liang et al., 2006), is below 70%. Taking into consideration the small size of the the corpus, in order to extract more effective phrase tables, we concatenate three alignments: Giza++ with grow-diag-final-and, Giza++ with intersection, and that derived from the Berkeley aligner. 3.7 Stemming Alignment and Normal Phrase Extraction (SANPE) The rich morphology of Hindi will cause word alignment sparsity, which results in poor alignment quality. Furthermore, word stemming on the Hindi side usually </context>
</contexts>
<marker>Och, Ney, 2003</marker>
<rawString>Franz Josef Och and Hermann Ney. 2003. A systematic comparison of various statistical alignment models. Computational linguistics, 29(1):19–51.</rawString>
</citation>
<citation valid="true">
<authors>
<author>Franz Josef Och</author>
</authors>
<title>Minimum error rate training in statistical machine translation.</title>
<date>2003</date>
<booktitle>In Proceedings of the 41st Annual Meeting on Association for Computational Linguistics - Volume 1, ACL ’03,</booktitle>
<pages>160--167</pages>
<publisher>Association</publisher>
<institution>for Computational Linguistics.</institution>
<location>Stroudsburg, PA, USA.</location>
<contexts>
<context position="1060" citStr="Och, 2003" startWordPosition="147" endWordPosition="148">ion task. We exploit miscellaneous methods in our system, including: Context-Informed PB-SMT, OOV Word Conversion (OWC), MultiAlignment Combination (MAC), Operation Sequence Model (OSM), Stemming Align and Normal Phrase Extraction (SANPE), and Language Model Interpolation (LMI). We also describe various preprocessing steps we tried for Hindi in this task. 1 Introduction This paper describes the DCU-Lingo24 submission to WMT 2014 for the Hindi-English translation task. All our experiments on WMT 2014 are built upon the Moses phrase-based model (PB-SMT) (Koehn et al., 2007) and tuned with MERT (Och, 2003). Starting from this baseline system, we exploit various methods including ContextInformed PB-SMT (CIPBSMT), zero-shot learning (Palatucci et al., 2009) using neural networkbased language modelling (Bengio et al., 2000; Mikolov et al., 2013) for OOV word conversion, various lexical reordering models (Axelrod et al., 2005; Galley and Manning, 2008), various Multiple Alignment Combination (MAC) (Tu et al., 2012), Operation Sequence Model (OSM) (Durrani et al., 2011) and Language Model Interpolation(LMI). In the next section, the preprocessing steps are explained. In Section 3 a detailed explanat</context>
<context position="7937" citStr="Och, 2003" startWordPosition="1216" endWordPosition="1217">hich can be downloaded from http://ilk.uvt.nl/timbl. ˆfk, CI( ˆfk)) (2) CIgt( 216 we derive a simple two-valued feature ˆhbest, defined in Equation (4): ˆhbest = � � 0 otherwise 1 if ˆek maximizes P(ˆek |ˆfk, CI( ˆfk)) (4) where ˆhbest is set to 1 when ˆek is one of the target phrases with highest probability according to P(ˆek |ˆfk, CI(ˆfk)) for each source phrase ˆfk; otherwise ˆhbest is set to 0.000001. We performed experiments by integrating these two features ˆhmbl and ˆhbest directly into the log-linear model of Moses. Their weights are optimized using minimum error-rate training (MERT)(Och, 2003) on a held-out development set for each of the experiments. 3.5 Morphological Segmentation Haque et al. (2012) applied a morphological suffix separation process in a Bengali-to-English translation task and showed that suffix separation significantly reduces data sparseness in the Bengali corpus. They also showed an SMT model trained on the suffix-stripped training data significantly outperforms the state-of-the-art PB-SMT baseline. Like Bengali, Hindi is a morphologically very rich and highly inflected Indian language. As done previously for Bengali-to-English (Haque et al., 2012), we employ a</context>
</contexts>
<marker>Och, 2003</marker>
<rawString>Franz Josef Och. 2003. Minimum error rate training in statistical machine translation. In Proceedings of the 41st Annual Meeting on Association for Computational Linguistics - Volume 1, ACL ’03, pages 160– 167, Stroudsburg, PA, USA. Association for Computational Linguistics.</rawString>
</citation>
<citation valid="true">
<authors>
<author>Tsuyoshi Okita</author>
<author>Ali Hosseinzadeh Vahid</author>
<author>Andy Way</author>
<author>Qun Liu</author>
</authors>
<title>Dcu terminology translation system for medical query subtask at wmt14.</title>
<date>2014</date>
<contexts>
<context position="10532" citStr="Okita et al., 2014" startWordPosition="1610" endWordPosition="1613">, word stemming on the Hindi side usually results in too many English words being aligned to one stemmed Hindi word, i.e. we encounter the problem of phrase overextraction. Therefore, we conduct word alignment with the stemmed version of Hindi, and then at the phrase extraction step, we replace the stemmed form with the original Hindi form. 3.8 OOV Word Conversion Method Our algorithm for OOV word conversion uses the recently developed zero-shot learning (Palatucci et al., 2009) using neural network language modelling (Bengio et al., 2000; Mikolov et al., 2013). The same technique is used in (Okita et al., 2014). This method requires neither parallel nor comparable corpora, but rather two monolingual corpora. In our context, we prepare two monolingual corpora on both sides, which are neither parallel nor comparable, and a small amount of already known correspondences between words on the source and target sides (henceforth, we refer to this as the ‘dictionary’). Then, we train both sides with the neural network language model, and use a continuous space representation to project words to each other on the basis of a small amount of correspondences in the dictionary. The following algorithm shows the </context>
</contexts>
<marker>Okita, Vahid, Way, Liu, 2014</marker>
<rawString>Tsuyoshi Okita, Ali Hosseinzadeh Vahid, Andy Way, and Qun Liu. 2014. Dcu terminology translation system for medical query subtask at wmt14.</rawString>
</citation>
<citation valid="true">
<authors>
<author>Mark Palatucci</author>
<author>Dean Pomerleau</author>
<author>Geoffrey Hinton</author>
<author>Tom Mitchell</author>
</authors>
<title>Zero-shot learning with semantic output codes.</title>
<date>2009</date>
<booktitle>In Neural Information Processing Systems (NIPS),</booktitle>
<contexts>
<context position="1212" citStr="Palatucci et al., 2009" startWordPosition="166" endWordPosition="169">bination (MAC), Operation Sequence Model (OSM), Stemming Align and Normal Phrase Extraction (SANPE), and Language Model Interpolation (LMI). We also describe various preprocessing steps we tried for Hindi in this task. 1 Introduction This paper describes the DCU-Lingo24 submission to WMT 2014 for the Hindi-English translation task. All our experiments on WMT 2014 are built upon the Moses phrase-based model (PB-SMT) (Koehn et al., 2007) and tuned with MERT (Och, 2003). Starting from this baseline system, we exploit various methods including ContextInformed PB-SMT (CIPBSMT), zero-shot learning (Palatucci et al., 2009) using neural networkbased language modelling (Bengio et al., 2000; Mikolov et al., 2013) for OOV word conversion, various lexical reordering models (Axelrod et al., 2005; Galley and Manning, 2008), various Multiple Alignment Combination (MAC) (Tu et al., 2012), Operation Sequence Model (OSM) (Durrani et al., 2011) and Language Model Interpolation(LMI). In the next section, the preprocessing steps are explained. In Section 3 a detailed explanation of the technique we exploit is provided. Then in Section 4, we provide our experimental results and resultant discussion. 2 Pre-processing Steps We </context>
<context position="10396" citStr="Palatucci et al., 2009" startWordPosition="1586" endWordPosition="1589">ase Extraction (SANPE) The rich morphology of Hindi will cause word alignment sparsity, which results in poor alignment quality. Furthermore, word stemming on the Hindi side usually results in too many English words being aligned to one stemmed Hindi word, i.e. we encounter the problem of phrase overextraction. Therefore, we conduct word alignment with the stemmed version of Hindi, and then at the phrase extraction step, we replace the stemmed form with the original Hindi form. 3.8 OOV Word Conversion Method Our algorithm for OOV word conversion uses the recently developed zero-shot learning (Palatucci et al., 2009) using neural network language modelling (Bengio et al., 2000; Mikolov et al., 2013). The same technique is used in (Okita et al., 2014). This method requires neither parallel nor comparable corpora, but rather two monolingual corpora. In our context, we prepare two monolingual corpora on both sides, which are neither parallel nor comparable, and a small amount of already known correspondences between words on the source and target sides (henceforth, we refer to this as the ‘dictionary’). Then, we train both sides with the neural network language model, and use a continuous space representatio</context>
</contexts>
<marker>Palatucci, Pomerleau, Hinton, Mitchell, 2009</marker>
<rawString>Mark Palatucci, Dean Pomerleau, Geoffrey Hinton, and Tom Mitchell. 2009. Zero-shot learning with semantic output codes. In Neural Information Processing Systems (NIPS), December.</rawString>
</citation>
<citation valid="true">
<authors>
<author>Ananthakrishnan Ramanathan</author>
<author>Durgesh D Rao</author>
</authors>
<title>A lightweight stemmer for hindi.</title>
<date>2003</date>
<booktitle>In the Proceedings of EACL.</booktitle>
<contexts>
<context position="8821" citStr="Ramanathan and Rao, 2003" startWordPosition="1342" endWordPosition="1345">ata sparseness in the Bengali corpus. They also showed an SMT model trained on the suffix-stripped training data significantly outperforms the state-of-the-art PB-SMT baseline. Like Bengali, Hindi is a morphologically very rich and highly inflected Indian language. As done previously for Bengali-to-English (Haque et al., 2012), we employ a suffix-stripping method for lemmatizing inflected Hindi words in the WMT Hindi-to-English translation task. Following Dasgupta and Ng (2006), we developed an unsupervised morphological segmentation method for Hindi. We also used a Hindi lightweight stemmer (Ramanathan and Rao, 2003) in order to prepare a training corpus with only Hindi stems. We prepared Hindi-to-English SMT systems on the both types of training data (i.e. suffix-stripped and stemmed).6 3.6 Multi-Alignment Combination (MAC) Word alignment is a critical component of MT systems. Various methods for word alignment have been proposed, and different models can produce signicantly different outputs. For example, Tu et al. (2012) demonstrates that the alignment agreement between the two best-known alignment tools, namely Giza++(Och and Ney, 2003) and 6Suffixes were separated and completely removed from the trai</context>
</contexts>
<marker>Ramanathan, Rao, 2003</marker>
<rawString>Ananthakrishnan Ramanathan and Durgesh D Rao. 2003. A lightweight stemmer for hindi. In the Proceedings of EACL.</rawString>
</citation>
<citation valid="true">
<authors>
<author>Mark Steedman</author>
</authors>
<title>The syntactic process, volume 35.</title>
<date>2000</date>
<publisher>MIT Press.</publisher>
<contexts>
<context position="5823" citStr="Steedman, 2000" startWordPosition="883" endWordPosition="884">le target phrases. The size of this distribution needs to be limited, and would ideally omit irrelevant target phrase translations that the standard PB-SMT (Koehn et al., 2003) approach would normally include. Following Haque et al. (2011), we derive a context-informed feature ˆhmbl that is expressed as the conditional probability of the target phrase ˆek given the source phrase ˆfk and its context information (CI), as in (2): ˆhmbl = log P(ˆek| Here, CI may include any feature that can provide useful information to disambiguate the given source phrase. In our experiment, we use CCG supertag (Steedman, 2000) as a contextual features. CCG supertag expresses the specific syntactic behaviour of a word in terms of the arguments it takes, and more generally the syntactic environment in which it appears. We consider the CCG supertags of the context words, as well as of the focus phrase itself. In our model, the supertag of a multi-word focus phrase is the concatenation of the supertags of the words composing that phrase. We generate a window of size 2l + 1 features (we set l:=2), including the concatenated complex supertag of the focus phrase. Accordingly, the supertag-based contextual information (CIg</context>
</contexts>
<marker>Steedman, 2000</marker>
<rawString>Mark Steedman. 2000. The syntactic process, volume 35. MIT Press.</rawString>
</citation>
<citation valid="true">
<authors>
<author>Andreas Stolcke</author>
</authors>
<title>Srilm – an extensible language modeling toolkit.</title>
<date>2002</date>
<booktitle>In Proceedings of the International Conference Spoken Language Processing,</booktitle>
<pages>901--904</pages>
<location>Denver, CO.</location>
<contexts>
<context position="4883" citStr="Stolcke, 2002" startWordPosition="735" endWordPosition="736">anguage Model Interpolation (LMI) We build a large language model by including data from the English Gigaword fifth edition, the English side of the UN corpus, the English side of the 109 French–English corpus and the English side of the Hindi–English parallel data provided by the organisers. We interpolate language models trained using each dataset, with the monolingual data provided split into three parts (news 2007-2013, Europarl (?) and news commentary) and the weights tuned to minimize perplexity on the target side of the devset. The language models in our systems are trained with SRILM (Stolcke, 2002). We train a 5-gram model with Kneser-Ney discounting (Chen and Goodman, 1996). 3.4 Context-informed PB-SMT Haque et al. (2011) express a context-dependent phrase translation as a multi-class classification problem, where a source phrase with given additional context information is classified into a distribution over possible target phrases. The size of this distribution needs to be limited, and would ideally omit irrelevant target phrase translations that the standard PB-SMT (Koehn et al., 2003) approach would normally include. Following Haque et al. (2011), we derive a context-informed featu</context>
</contexts>
<marker>Stolcke, 2002</marker>
<rawString>Andreas Stolcke. 2002. Srilm – an extensible language modeling toolkit. In Proceedings of the International Conference Spoken Language Processing, pages 901–904, Denver, CO.</rawString>
</citation>
<citation valid="true">
<authors>
<author>Zhaopeng Tu</author>
<author>Yang Liu</author>
<author>Yifan He</author>
<author>Josef van Genabith</author>
<author>Qun Liu</author>
<author>Shouxun Lin</author>
</authors>
<title>Combining multiple alignments to improve machine translation.</title>
<date>2012</date>
<booktitle>In COLING (Posters),</booktitle>
<pages>1249--1260</pages>
<marker>Tu, Liu, He, van Genabith, Liu, Lin, 2012</marker>
<rawString>Zhaopeng Tu, Yang Liu, Yifan He, Josef van Genabith, Qun Liu, and Shouxun Lin. 2012. Combining multiple alignments to improve machine translation. In COLING (Posters), pages 1249–1260.</rawString>
</citation>
</citationList>
</algorithm>
</algorithms>
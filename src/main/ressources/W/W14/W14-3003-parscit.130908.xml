<?xml version="1.0" encoding="UTF-8"?>
<algorithms version="110505">
<algorithm name="SectLabel" version="110505">
<variant no="0" confidence="0.087979">
<title confidence="0.834417">
Case, Constructions, FrameNet, and the Deep Lexicon
</title>
<author confidence="0.992169">
Jerry R. Hobbs
</author>
<affiliation confidence="0.965737666666667">
Information Sciences Institute
University of Southern California
Marina del Rey, California
</affiliation>
<sectionHeader confidence="0.975209" genericHeader="abstract">
Abstract
</sectionHeader>
<bodyText confidence="0.999849181818182">
Three major contributions that Charles
Fillmore made in linguistics play an im-
portant role in the enterprise of deep
lexical semantics, which is the effort to
link lexical meaning to underlying abstract
core theories. I will discuss how case re-
lates to lexical decompositions, how moti-
vated constructions span the borderline be-
tween syntax and semantics, and how the
frames of FrameNet provide an excellent
first step in deep inference.
</bodyText>
<sectionHeader confidence="0.89" genericHeader="keywords">
1 Deep Lexical Semantics
</sectionHeader>
<bodyText confidence="0.999118924242424">
Deep lexical semantics (Hobbs, 2008) is the effort
to construct formal theories of abstract phenom-
ena, such as composite entities, the figure-ground
relation, scales, change of state, and causality, and
to link the most common words in English to these
theories with axioms explicating their meanings.
This work has been deeply influenced by the work
of Charles Fillmore in at least three ways – the
insights underlying case grammar, in the impor-
tance of being able to represent constructions, and
in the development of FrameNet. In this talk I will
describe how each of these issues is dealt with in
deep lexical semantics. First I will sketch three of
the underlying core theories.
Composite Entities and the Figure-Ground Re-
lation: A composite entity is a thing made of other
things. This is intended to cover physical objects
like a telephone, mixed objects like a book, ab-
stract objects like a theory, and events like a con-
cert. It is characterized by a set of components,
a set of properties of the components, a set of re-
lations among its components (the structure), and
relations between the entity as a whole and its en-
vironment (including its function). The predicate
at relates an external entity, the figure, to a com-
ponent in a composite entity, the ground. Differ-
ent figures and different grounds give us different
meanings for at.
Spatial location: Pat is at the back of the
store.
Location on a scale: Nuance closed at
58.
Membership in an organization: Pat is
now at Google.
Location in a text: The table is at the end
of the article.
Time of an event: At that moment, Pat
stood up.
Event at event: Let’s discuss that at
lunch.
At a predication: She was at ease in his
company.
When at is specialized in this way, we tap into a
whole vocabulary for talking about the domain.
Change of State: The predication
change(e1, e2) says that state e1 changes
into state e2. Its principal properties are that
e1 and e2 should have an entity in common – a
change of state is a change of state of something.
States e1 and e2 are not the same unless there
is an intermediate state. The predicate change
is defeasibly transitive; in fact, backchaining on
the transitivity axiom is one way to refine the
granularity on processes.
Causality: We distinguish between the “causal
complex” for an effect and the concept “cause”. A
causal complex includes all the states and events
that have to happen or hold in order for the effect
to occur. We say that flipping a switch causes the
light to go on. But many other conditions must
be in the causal complex – the light bulb can’t be
burnt out, the wiring has to be intact, the power
has to be on in the city, and so on. The two key
properties of a causal complex are that when ev-
erything in the causal complex happens or holds,
so will the effect, and that everything that is in the
</bodyText>
<page confidence="0.947452">
10
</page>
<subsubsectionHeader confidence="0.687233">
Proceedings of Frame Semantics in NLP: A Workshop in Honor of Chuck Fillmore (1929–2014), pages 10–12,
</subsubsectionHeader>
<bodyText confidence="0.9818915">
Baltimore, Maryland USA, June 27, 2014. c�2014 Association for Computational Linguistics
causal complex is relevant in a sense that can be
made precise. “Causal complex” is a rigorous or
monotonic notion, but its utility in everyday life is
limited because we almost never can specify ev-
erything in it.
“Cause” by contrast is a defeasible or nonmono-
tonic notion. It selects out of a causal complex
a particular eventuality that in a sense is the “ac-
tive” part of the causal complex, the thing that isn’t
necessarily normally true. Flipping the switch, in
most contexts, is the action that causes the light to
come on. Causes are the focus of planning, predic-
tion, explanation, and interpreting discourse, but
not diagnosis, since in diagnosis, something that
normally happens or holds, doesn’t.
As illustrations, here is how two verbs are de-
fined in terms of these core theories. The transitive
sense of “move”, as in “x moves y from z to w” is
captured by the axiom
</bodyText>
<equation confidence="0.947363">
move(x, y, z, w)
≡ cause(x, e1)
∧ change&apos;(e1, e2, e3)
∧ at&apos;(e2, y, z)
∧ at&apos;(e3, y, w)
</equation>
<bodyText confidence="0.73531925">
That is, x causes a change e1 from the state e2 in
which y is at z to the state e3 in which y is at w.
The verb “let” as in “x lets e happen” means x
does not cause e not to happen. The axiom is
</bodyText>
<construct confidence="0.570623">
let(x, e)
≡ not(e1) ∧ cause&apos;(e1, x, e2)
∧ not&apos;(e2, e)
</construct>
<sectionHeader confidence="0.971974" genericHeader="introduction">
2 Case
</sectionHeader>
<bodyText confidence="0.999966090909091">
The various case roles proposed by Filllmore
(1968) and many others since then can be under-
stood in terms of the roles entities play in these ax-
iomatic decompositions. In the axiom for move,
x is the agent. An agent is an entity that is viewed
as being capable of initiating a causal chain, and
the agent of an action is the agent that initiated it.
What Fillmore originally called the object and
has since been called the patient and, more
bizarrely, the theme is the entity that undergoes the
change of state or location. In the move axiom, y
plays this role.
When the property that changes in the object is a
real or metaphorical ”at” relation, as in move, then
Z is the source and w is the goal. An instrument
is an entity that the agent causes to go through a
change of state where this change plays an inter-
mediate role in the causal chain. Other proposed
case roles can be analyzed similarly.
The more similar verbs are to “move”, the eas-
ier it is to assign case labels to their arguments.
When verbs are not very similar to “move”, e.g.,
“outnumber”, assigning case labels becomes more
problematic, a factor no doubt in Fillmore’s deci-
sion not to utilize a small fixed list in FrameNet.
Nevertheless, the abstractness of the underly-
ing core theories, particularly the theory of com-
posite entities, ensures that this understanding of
case applies to the verbal lexicon widely. Thus, al-
though case labels play no formal role in deep lexi-
cal semantics, the insights of case grammar can be
captured and inform the analyses of specific verb
meanings.
</bodyText>
<sectionHeader confidence="0.99832" genericHeader="method">
3 Constructions
</sectionHeader>
<bodyText confidence="0.999937375">
In the 1980s Fillmore and his colleagues at Berke-
ley developed the theory of Construction Gram-
mar (Fillmore et al., 1988). I take constructions
to be fragments of language that elemplify gen-
eral compositional principles, but have a conven-
tionalized meaning which is one of perhaps many
meanings licensed by the general lexical and com-
positional structure, but is the sole, or at least the
usual, interpretation normally assigned to it in dis-
course.
An example will perhaps make this clear. The
contraction “let’s” has a particular meaning, sub-
sumed by, but much more specific than, “let us”.
“Let us go.” could mean the same as “Let’s go,”
although it sounds stilted. But it could also be
something kidnap victims say to the kidnapper. By
general principles, “let’s go” could have either of
these meanings. But in fact it only has the first.
Thus, “let’s” can be viewed as a conventional-
ization of one specific interpretation of “let us”.
The source interpretation is this: “Let’s” is a con-
traction for “let us”. A rule of contraction would
tell us that when the string “let us” describes a
parameterized situation, the string “let’s” can de-
scribe the same situation. Thus, the best expla-
nation for the occurrence of “let’s” is that it is a
contraction of “let us”, “Let’s” is only used in im-
perative sentences, so the implicit subject is “you”.
The verb “to let” means, as in the axiom above, “to
not cause not”. Thus, “let us go.” means “Don’t
you cause us not to go.” So far, this supports both
meanings above. Now the set of people designated
</bodyText>
<page confidence="0.998188">
11
</page>
<bodyText confidence="0.999940538461538">
by “us” may or may not include you in general,
but in the desired interpretation it does. One way
for you to cause us not to go, provided you are a
part of us, is for you not to go yourself. The sen-
tence “Let’s go.” tells you not to cause us not to
go by not going yourself. This abductive interpre-
tation is straightforwardly represented in a proof
graph. This is the conventionalized meaning asso-
ciated with the “let’s” construction.
In normal usage we do not unpack this graph
structure, but it nevertheless provides the conven-
tional interpretation’s motivation, a term I believe
I first heard from Fillmore in a discussion group
in 1980. The conventional interpretation of “let’s
go” is not completely arbitrary. We can unpack it,
and often need to in interpreting discourse. The re-
ply could be “No, you go alone” or “No, let’s stay
here.” Each of these taps into a different aspect of
the conventional interpretation’s motivation.
Constructions are not phrases like “let’s go” or
parameterized phrases like “let’s VP” but frag-
ments of a proof graph encoding the motivated
syntactic and compositional semantic structure as
well as the conventionalized interpretation. They
are normally deployed in a block, but they can be
effortlessly unpacked when one needs to.
</bodyText>
<sectionHeader confidence="0.999303" genericHeader="method">
4 FrameNet
</sectionHeader>
<bodyText confidence="0.999922130434783">
The FrameNet frames (Baker et al., 2003) can
be viewed as providing the first level of axioms
mapping words and phrases into underlying core
theories. For example, “let” is mapped into a
frame of enablement (not-cause-not), along with
the verbs “permit” and “allow” and the parame-
terized phrase “make possible”. The frames are
not expressed in the FrameNet resource as ax-
ioms. However, FrameNet was converted into log-
ical axioms by Ovchinnikova (Ovchinnikova et al.
2013), and she and her colleagues have shown that
an abduction engine using a knowledge base de-
rived from these sources is competitive with the
best of the statistical systems in recognizing tex-
tual entailment and in semantic role labelling.
The FrameNet project, in addition, has demon-
strated that a concerted, long-term effort, when
intelligently thought out with a sensitivity to the
nature of language, can produce a highly valu-
able resource for deep, knowledge-based process-
ing of natural language. This was certainly among
Charles Fillmore’s greatest contributions to com-
putational linguistics.
</bodyText>
<sectionHeader confidence="0.996333" genericHeader="references">
References
</sectionHeader>
<reference confidence="0.99987785">
Baker, C., Charles Fillmore, and B. Cronin, 2003. The
Structure of the Framenet Database, International
Journal of Lexicography, Volume 16.3, 281-296.
Fillmore, Charles, 1968, The Case for Case, in Bach
and Harms (Eds.), Universals in Linguistic Theory,
New York: Holt, Rinehart, and Winston, 1-88.
Fillmore, Charles, Paul Kay, and Catherine O’Connor,
1988. Regularity and Idiomaticity in Grammatical
Constructions: The Case of let alone, Language, 64,
501-38.
Hobbs, Jerry R. 2008. Deep Lexical Semantics. Pro-
ceedings of 9th International Conference on Intelli-
gent Text Processing and Computational Linguistics
(CICLing-2008), Haifa, Israel.
Ovchinnikova, Ekaterina, Niloofar Montazeri, Teodor
Alexandrov, Jerry R. Hobbs, Michael C. McCord,
and Rutu Mulkar-Mehta. 2013. Abductive Reason-
ing with a Large Knowledge Base for Discourse Pro-
cessing. In H. Hunt, J. Bos, and S. Pulman, (Eds.),
Computing Meaning, 4:104-124.
</reference>
<page confidence="0.998461">
12
</page>
</variant>
</algorithm>
<algorithm name="ParsHed" version="110505">
<variant no="0" confidence="0.021338">
<title confidence="0.977455">Case, Constructions, FrameNet, and the Deep Lexicon</title>
<author confidence="0.999908">R Jerry</author>
<affiliation confidence="0.999305">Information Sciences University of Southern</affiliation>
<address confidence="0.834834">Marina del Rey, California</address>
<abstract confidence="0.998038255506608">Three major contributions that Charles Fillmore made in linguistics play an important role in the enterprise of deep lexical semantics, which is the effort to link lexical meaning to underlying abstract core theories. I will discuss how case relates to lexical decompositions, how motivated constructions span the borderline between syntax and semantics, and how the frames of FrameNet provide an excellent first step in deep inference. 1 Deep Lexical Semantics Deep lexical semantics (Hobbs, 2008) is the effort to construct formal theories of abstract phenomena, such as composite entities, the figure-ground relation, scales, change of state, and causality, and to link the most common words in English to these theories with axioms explicating their meanings. This work has been deeply influenced by the work of Charles Fillmore in at least three ways – the insights underlying case grammar, in the importance of being able to represent constructions, and in the development of FrameNet. In this talk I will describe how each of these issues is dealt with in deep lexical semantics. First I will sketch three of the underlying core theories. Composite Entities and the Figure-Ground Relation: A composite entity is a thing made of other things. This is intended to cover physical objects like a telephone, mixed objects like a book, abstract objects like a theory, and events like a concert. It is characterized by a set of components, a set of properties of the components, a set of relations among its components (the structure), and relations between the entity as a whole and its environment (including its function). The predicate an external entity, the figure, to a component in a composite entity, the ground. Different figures and different grounds give us different for Spatial location: Pat is at the back of the store. Location on a scale: Nuance closed at 58. Membership in an organization: Pat is now at Google. Location in a text: The table is at the end of the article. Time of an event: At that moment, Pat stood up. Event at event: Let’s discuss that at lunch. At a predication: She was at ease in his company. specialized in this way, we tap into a whole vocabulary for talking about the domain. Change of State: The predication that state state Its principal properties are that have an entity in common – a of state is a change of state of not the same unless there an intermediate state. The predicate is defeasibly transitive; in fact, backchaining on the transitivity axiom is one way to refine the granularity on processes. Causality: We distinguish between the “causal complex” for an effect and the concept “cause”. A causal complex includes all the states and events that have to happen or hold in order for the effect to occur. We say that flipping a switch causes the light to go on. But many other conditions must be in the causal complex – the light bulb can’t be burnt out, the wiring has to be intact, the power has to be on in the city, and so on. The two key properties of a causal complex are that when everything in the causal complex happens or holds, so will the effect, and that everything that is in the 10 of Frame Semantics in NLP: A Workshop in Honor of Chuck Fillmore pages Maryland USA, June 27, 2014. Association for Computational Linguistics causal complex is relevant in a sense that can be made precise. “Causal complex” is a rigorous or monotonic notion, but its utility in everyday life is limited because we almost never can specify everything in it. “Cause” by contrast is a defeasible or nonmonotonic notion. It selects out of a causal complex a particular eventuality that in a sense is the “active” part of the causal complex, the thing that isn’t necessarily normally true. Flipping the switch, in contexts, is the action that light to come on. Causes are the focus of planning, prediction, explanation, and interpreting discourse, but not diagnosis, since in diagnosis, something that normally happens or holds, doesn’t. As illustrations, here is how two verbs are defined in terms of these core theories. The transitive of “move”, as in is captured by the axiom y, z, y, y, is, a change the state at the state which at verb “let” as in means not cause to happen. The axiom is x, 2 Case The various case roles proposed by Filllmore (1968) and many others since then can be understood in terms of the roles entities play in these axdecompositions. In the axiom for the agent. An agent is an entity that is viewed as being capable of initiating a causal chain, and the agent of an action is the agent that initiated it. What Fillmore originally called the object and has since been called the patient and, more bizarrely, the theme is the entity that undergoes the of state or location. In the plays this role. When the property that changes in the object is a or metaphorical ”at” relation, as in then the source and the goal. An instrument is an entity that the agent causes to go through a change of state where this change plays an intermediate role in the causal chain. Other proposed case roles can be analyzed similarly. The more similar verbs are to “move”, the easier it is to assign case labels to their arguments. When verbs are not very similar to “move”, e.g., “outnumber”, assigning case labels becomes more problematic, a factor no doubt in Fillmore’s decision not to utilize a small fixed list in FrameNet. Nevertheless, the abstractness of the underlying core theories, particularly the theory of composite entities, ensures that this understanding of case applies to the verbal lexicon widely. Thus, although case labels play no formal role in deep lexical semantics, the insights of case grammar can be captured and inform the analyses of specific verb meanings. 3 Constructions In the 1980s Fillmore and his colleagues at Berkeley developed the theory of Construction Grammar (Fillmore et al., 1988). I take constructions to be fragments of language that elemplify general compositional principles, but have a conventionalized meaning which is one of perhaps many meanings licensed by the general lexical and compositional structure, but is the sole, or at least the usual, interpretation normally assigned to it in discourse. An example will perhaps make this clear. The contraction “let’s” has a particular meaning, subsumed by, but much more specific than, “let us”. “Let us go.” could mean the same as “Let’s go,” although it sounds stilted. But it could also be something kidnap victims say to the kidnapper. By general principles, “let’s go” could have either of these meanings. But in fact it only has the first. Thus, “let’s” can be viewed as a conventionalization of one specific interpretation of “let us”. The source interpretation is this: “Let’s” is a contraction for “let us”. A rule of contraction would tell us that when the string “let us” describes a parameterized situation, the string “let’s” can describe the same situation. Thus, the best explanation for the occurrence of “let’s” is that it is a contraction of “let us”, “Let’s” is only used in imperative sentences, so the implicit subject is “you”. The verb “to let” means, as in the axiom above, “to not cause not”. Thus, “let us go.” means “Don’t you cause us not to go.” So far, this supports both meanings above. Now the set of people designated 11 by “us” may or may not include you in general, but in the desired interpretation it does. One way cause to go, provided you are a part of us, is for you not to go yourself. The sentence “Let’s go.” tells you not to cause us not to go by not going yourself. This abductive interpretation is straightforwardly represented in a proof graph. This is the conventionalized meaning associated with the “let’s” construction. In normal usage we do not unpack this graph structure, but it nevertheless provides the conveninterpretation’s a term I believe I first heard from Fillmore in a discussion group in 1980. The conventional interpretation of “let’s go” is not completely arbitrary. We can unpack it, and often need to in interpreting discourse. The reply could be “No, you go alone” or “No, let’s stay here.” Each of these taps into a different aspect of the conventional interpretation’s motivation. Constructions are not phrases like “let’s go” or parameterized phrases like “let’s VP” but fragments of a proof graph encoding the motivated syntactic and compositional semantic structure as well as the conventionalized interpretation. They are normally deployed in a block, but they can be effortlessly unpacked when one needs to. 4 FrameNet The FrameNet frames (Baker et al., 2003) can be viewed as providing the first level of axioms mapping words and phrases into underlying core theories. For example, “let” is mapped into a frame of enablement (not-cause-not), along with the verbs “permit” and “allow” and the parameterized phrase “make possible”. The frames are not expressed in the FrameNet resource as axioms. However, FrameNet was converted into logical axioms by Ovchinnikova (Ovchinnikova et al. 2013), and she and her colleagues have shown that an abduction engine using a knowledge base derived from these sources is competitive with the best of the statistical systems in recognizing textual entailment and in semantic role labelling. The FrameNet project, in addition, has demonstrated that a concerted, long-term effort, when intelligently thought out with a sensitivity to the nature of language, can produce a highly valuable resource for deep, knowledge-based processing of natural language. This was certainly among Charles Fillmore’s greatest contributions to computational linguistics.</abstract>
<note confidence="0.961027545454545">References Baker, C., Charles Fillmore, and B. Cronin, 2003. The of the Framenet Database, of Volume 16.3, 281-296. Fillmore, Charles, 1968, The Case for Case, in Bach Harms (Eds.), in Linguistic New York: Holt, Rinehart, and Winston, 1-88. Fillmore, Charles, Paul Kay, and Catherine O’Connor, 1988. Regularity and Idiomaticity in Grammatical The Case of 64, 501-38.</note>
<author confidence="0.542816">Pro-</author>
<affiliation confidence="0.692296">ceedings of 9th International Conference on Intelligent Text Processing and Computational Linguistics</affiliation>
<address confidence="0.994595">Haifa, Israel.</address>
<email confidence="0.454547">Ovchinnikova,Ekaterina,NiloofarMontazeri,Teodor</email>
<author confidence="0.6999305">Abductive Reason-</author>
<note confidence="0.7922405">ing with a Large Knowledge Base for Discourse Processing. In H. Hunt, J. Bos, and S. Pulman, (Eds.), 4:104-124. 12</note>
</variant>
</algorithm>
<algorithm name="ParsCit" version="110505">
<citationList>
<citation valid="true">
<authors>
<author>C Baker</author>
<author>Charles Fillmore</author>
<author>B Cronin</author>
</authors>
<title>The Structure of the Framenet Database,</title>
<date>2003</date>
<journal>International Journal of Lexicography, Volume</journal>
<volume>16</volume>
<pages>281--296</pages>
<contexts>
<context position="9312" citStr="Baker et al., 2003" startWordPosition="1633" endWordPosition="1636">etely arbitrary. We can unpack it, and often need to in interpreting discourse. The reply could be “No, you go alone” or “No, let’s stay here.” Each of these taps into a different aspect of the conventional interpretation’s motivation. Constructions are not phrases like “let’s go” or parameterized phrases like “let’s VP” but fragments of a proof graph encoding the motivated syntactic and compositional semantic structure as well as the conventionalized interpretation. They are normally deployed in a block, but they can be effortlessly unpacked when one needs to. 4 FrameNet The FrameNet frames (Baker et al., 2003) can be viewed as providing the first level of axioms mapping words and phrases into underlying core theories. For example, “let” is mapped into a frame of enablement (not-cause-not), along with the verbs “permit” and “allow” and the parameterized phrase “make possible”. The frames are not expressed in the FrameNet resource as axioms. However, FrameNet was converted into logical axioms by Ovchinnikova (Ovchinnikova et al. 2013), and she and her colleagues have shown that an abduction engine using a knowledge base derived from these sources is competitive with the best of the statistical system</context>
</contexts>
<marker>Baker, Fillmore, Cronin, 2003</marker>
<rawString>Baker, C., Charles Fillmore, and B. Cronin, 2003. The Structure of the Framenet Database, International Journal of Lexicography, Volume 16.3, 281-296.</rawString>
</citation>
<citation valid="true">
<authors>
<author>Charles Fillmore</author>
</authors>
<title>The Case for Case,</title>
<date>1968</date>
<booktitle>in Bach and Harms (Eds.), Universals in Linguistic Theory,</booktitle>
<pages>1--88</pages>
<location>New York: Holt, Rinehart, and Winston,</location>
<marker>Fillmore, 1968</marker>
<rawString>Fillmore, Charles, 1968, The Case for Case, in Bach and Harms (Eds.), Universals in Linguistic Theory, New York: Holt, Rinehart, and Winston, 1-88.</rawString>
</citation>
<citation valid="true">
<authors>
<author>Charles Fillmore</author>
<author>Paul Kay</author>
<author>Catherine O’Connor</author>
</authors>
<title>Regularity and Idiomaticity in Grammatical Constructions: The Case of let alone,</title>
<date>1988</date>
<journal>Language,</journal>
<volume>64</volume>
<pages>501--38</pages>
<marker>Fillmore, Kay, O’Connor, 1988</marker>
<rawString>Fillmore, Charles, Paul Kay, and Catherine O’Connor, 1988. Regularity and Idiomaticity in Grammatical Constructions: The Case of let alone, Language, 64, 501-38.</rawString>
</citation>
<citation valid="true">
<authors>
<author>Jerry R Hobbs</author>
</authors>
<title>Deep Lexical Semantics.</title>
<date>2008</date>
<booktitle>Proceedings of 9th International Conference on Intelligent Text Processing and Computational Linguistics (CICLing-2008),</booktitle>
<location>Haifa,</location>
<contexts>
<context position="666" citStr="Hobbs, 2008" startWordPosition="99" endWordPosition="100">rry R. Hobbs Information Sciences Institute University of Southern California Marina del Rey, California Abstract Three major contributions that Charles Fillmore made in linguistics play an important role in the enterprise of deep lexical semantics, which is the effort to link lexical meaning to underlying abstract core theories. I will discuss how case relates to lexical decompositions, how motivated constructions span the borderline between syntax and semantics, and how the frames of FrameNet provide an excellent first step in deep inference. 1 Deep Lexical Semantics Deep lexical semantics (Hobbs, 2008) is the effort to construct formal theories of abstract phenomena, such as composite entities, the figure-ground relation, scales, change of state, and causality, and to link the most common words in English to these theories with axioms explicating their meanings. This work has been deeply influenced by the work of Charles Fillmore in at least three ways – the insights underlying case grammar, in the importance of being able to represent constructions, and in the development of FrameNet. In this talk I will describe how each of these issues is dealt with in deep lexical semantics. First I wil</context>
</contexts>
<marker>Hobbs, 2008</marker>
<rawString>Hobbs, Jerry R. 2008. Deep Lexical Semantics. Proceedings of 9th International Conference on Intelligent Text Processing and Computational Linguistics (CICLing-2008), Haifa, Israel.</rawString>
</citation>
<citation valid="true">
<authors>
<author>Ekaterina Ovchinnikova</author>
<author>Niloofar Montazeri</author>
<author>Teodor Alexandrov</author>
<author>Jerry R Hobbs</author>
<author>Michael C McCord</author>
<author>Rutu Mulkar-Mehta</author>
</authors>
<title>Abductive Reasoning with a Large Knowledge Base for Discourse Processing. In</title>
<date>2013</date>
<journal>(Eds.), Computing Meaning,</journal>
<pages>4--104</pages>
<contexts>
<context position="9743" citStr="Ovchinnikova et al. 2013" startWordPosition="1702" endWordPosition="1705">s well as the conventionalized interpretation. They are normally deployed in a block, but they can be effortlessly unpacked when one needs to. 4 FrameNet The FrameNet frames (Baker et al., 2003) can be viewed as providing the first level of axioms mapping words and phrases into underlying core theories. For example, “let” is mapped into a frame of enablement (not-cause-not), along with the verbs “permit” and “allow” and the parameterized phrase “make possible”. The frames are not expressed in the FrameNet resource as axioms. However, FrameNet was converted into logical axioms by Ovchinnikova (Ovchinnikova et al. 2013), and she and her colleagues have shown that an abduction engine using a knowledge base derived from these sources is competitive with the best of the statistical systems in recognizing textual entailment and in semantic role labelling. The FrameNet project, in addition, has demonstrated that a concerted, long-term effort, when intelligently thought out with a sensitivity to the nature of language, can produce a highly valuable resource for deep, knowledge-based processing of natural language. This was certainly among Charles Fillmore’s greatest contributions to computational linguistics. Refe</context>
</contexts>
<marker>Ovchinnikova, Montazeri, Alexandrov, Hobbs, McCord, Mulkar-Mehta, 2013</marker>
<rawString>Ovchinnikova, Ekaterina, Niloofar Montazeri, Teodor Alexandrov, Jerry R. Hobbs, Michael C. McCord, and Rutu Mulkar-Mehta. 2013. Abductive Reasoning with a Large Knowledge Base for Discourse Processing. In H. Hunt, J. Bos, and S. Pulman, (Eds.), Computing Meaning, 4:104-124.</rawString>
</citation>
</citationList>
</algorithm>
</algorithms>
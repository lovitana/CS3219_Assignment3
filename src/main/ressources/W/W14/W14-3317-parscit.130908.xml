<?xml version="1.0" encoding="UTF-8"?>
<algorithms version="110505">
<algorithm name="SectLabel" version="110505">
<variant no="0" confidence="0.004116">
<title confidence="0.7643325">
The RWTH Aachen German-English Machine Translation System for
WMT 2014
</title>
<author confidence="0.990144">
Stephan Peitz, Joern Wuebker, Markus Freitag and Hermann Ney
</author>
<affiliation confidence="0.991503333333333">
Human Language Technology and Pattern Recognition Group
Computer Science Department
RWTH Aachen University
</affiliation>
<address confidence="0.609147">
D-52056 Aachen, Germany
</address>
<email confidence="0.99742">
&lt;surname&gt;@cs.rwth-aachen.de
</email>
<sectionHeader confidence="0.997366" genericHeader="abstract">
Abstract
</sectionHeader>
<bodyText confidence="0.998893230769231">
This paper describes the statistical ma-
chine translation (SMT) systems devel-
oped at RWTH Aachen University for the
German—*English translation task of the
ACL 2014 Eighth Workshop on Statisti-
cal Machine Translation (WMT 2014).
Both hierarchical and phrase-based SMT
systems are applied employing hierarchi-
cal phrase reordering and word class lan-
guage models. For the phrase-based sys-
tem, we run discriminative phrase training.
In addition, we describe our preprocessing
pipeline for German—*English.
</bodyText>
<sectionHeader confidence="0.999392" genericHeader="keywords">
1 Introduction
</sectionHeader>
<bodyText confidence="0.999943461538462">
For the WMT 2014 shared translation task1
RWTH utilized state-of-the-art phrase-based and
hierarchical translation systems. First, we describe
our preprocessing pipeline for the language pair
German—*English in Section 2. Furthermore, we
utilize morpho-syntactic analysis to preprocess the
data (Section 2.3). In Section 3, we give a survey
of the employed systems and the basic methods
they implement. More details are given about the
discriminative phrase training (Section 3.4) and
the hierarchical reordering model for hierarchical
machine translation (Section 3.5). Experimental
results are discussed in Section 4.
</bodyText>
<sectionHeader confidence="0.99346" genericHeader="introduction">
2 Preprocessing
</sectionHeader>
<bodyText confidence="0.994836333333333">
In this section we will describe the modification of
our preprocessing pipeline compared to our 2013
WMT German—*English setup.
</bodyText>
<subsectionHeader confidence="0.904867">
2.1 Categorization
</subsectionHeader>
<bodyText confidence="0.9997465">
We put some effort in building better categories for
digits and written numbers. All written numbers
</bodyText>
<footnote confidence="0.8957545">
1http://www.statmt.org/wmt14/
translation-task.html
</footnote>
<bodyText confidence="0.999521111111111">
were categorized. In 2013 they were just handled
as normal words which leads to a higher number of
out-of-vocabulary words. For German—*English,
in most cases for numbers like ’3,000’ or ’2.34’
the decimal mark ’,’ and the thousands separator
’.’ has to be inverted. As the training data and also
the test sets contain several errors for numbers in
the source as well as in the target part, we put more
effort into producing correct English numbers.
</bodyText>
<subsectionHeader confidence="0.989456">
2.2 Remove Foreign Languages
</subsectionHeader>
<bodyText confidence="0.999828233333333">
The WMT German—*English corpus contains
some bilingual sentence pairs with non-German
source or/and non-English target sentences. For
this WMT translation task, we filtered all non-
matching language pairs (in terms of source lan-
guage German and target language English) from
our bilingual training set.
First, we filtered languages which contain non-
ascii characters. For example Chinese, Arabic or
Russian can be easily filtered when deleting sen-
tences which contain more than 70 percent non-
ascii words. The first examples of Table 1 was
filtered due to the fact, that the source sentence
contains too many non-ascii characters.
In a second step, we filtered European lan-
guages containing ascii characters. We used the
WMT monolingual corpora in Czech, French,
Spanish, English and German to filter these lan-
guages from our bilingual data. We could both
delete a sentence pair if it contains a wrong source
language or a wrong target language. That is the
reason why we even search for English sentences
in the source part and for German sentences in
the target part. For each language, we built a
word count of all words in the monolingual data
for each language separately. We removed punc-
tuation which are no indicator of a language. In
our experiments, we only considered words with
frequency higher than 20 (e.g. to ignore names).
Given the word frequency, we removed a bilingual
</bodyText>
<page confidence="0.976215">
157
</page>
<affiliation confidence="0.3463775">
Proceedings of the Ninth Workshop on Statistical Machine Translation, pages 157–162,
Baltimore, Maryland USA, June 26–27, 2014. c�2014 Association for Computational Linguistics
</affiliation>
<tableCaption confidence="0.998803">
Table 1: Examples of sentences removed in preprocessing.
</tableCaption>
<bodyText confidence="0.98693948">
Example
remove non-ascii symbols •Î¿ð)¨Fƒð .
zum Bericht A˜noveros Trias de Bes
remove wrong languages from target Honni soit qui mal y pense !
as you yourself have said: travailler plus pour gagner plus
remove wrong languages from source je d´eclare interrompue la session du Parlement europ´een .
Quelle der Tabelle : “ what Does the European Union do ? ”
sentence pair from our training data if more than
70 percent of the words had a higher count in a
different language then the one we expected. In
Table 1 some example sentences, which were re-
moved, are illustrated.
In Table 2 the amount of sentences and the cor-
responding vocabulary sizes of partial and totally
cleaned data sets are given. Further we provide the
number of out-of-vocabulary words (OOVs) for
newstest2012. The vocabulary size could be re-
duced by ∼130k words for both source and target
side of our bilingual training data while the OOV
rate kept the same. Our experiments showed, that
the translation quality is the same with or with-
out removing wrong sentences. Nevertheless, we
reduced the training data size and also the vocabu-
lary size without any degradation in terms of trans-
lation quality.
</bodyText>
<subsectionHeader confidence="0.998797">
2.3 Morpho-syntactic Analysis
</subsectionHeader>
<bodyText confidence="0.9999255">
In order to reduce the source vocabulary size for
the German→English translation further, the Ger-
man text is preprocessed by splitting German com-
pound words with the frequency-based method de-
scribed in (Koehn and Knight, 2003). To reduce
translation complexity, we employ the long-range
part-of-speech based reordering rules proposed by
Popovi´c and Ney (2006).
</bodyText>
<sectionHeader confidence="0.971136" genericHeader="method">
3 Translation Systems
</sectionHeader>
<bodyText confidence="0.9998367">
In this evaluation, we employ phrase-based trans-
lation and hierarchical phrase-based translation.
Both approaches are implemented in Jane (Vilar et
al., 2012; Wuebker et al., 2012), a statistical ma-
chine translation toolkit which has been developed
at RWTH Aachen University and is freely avail-
able for non-commercial use.2 In the newest inter-
nal version, we use the KenLM Language Model
Interface provided by (Heafield, 2011) for both de-
coders.
</bodyText>
<footnote confidence="0.899816">
2http://www.hltpr.rwth-aachen.de/jane/
</footnote>
<subsectionHeader confidence="0.998873">
3.1 Phrase-based System
</subsectionHeader>
<bodyText confidence="0.999897857142857">
In the phrase-based decoder (source cardinality
synchronous search, SCSS, Wuebker et al. (2012)),
we use the standard set of models with phrase
translation probabilities and lexical smoothing in
both directions, word and phrase penalty, distance-
based distortion model, an n-gram target language
model and three binary count features. Additional
models used in this evaluation are the hierarchical
reordering model (HRM) (Galley and Manning,
2008) and a word class language model (wcLM)
(Wuebker et al., 2013). The parameter weights
are optimized with minimum error rate training
(MERT) (Och, 2003). The optimization criterion
is BLEU (Papineni et al., 2002).
</bodyText>
<subsectionHeader confidence="0.99998">
3.2 Hierarchical Phrase-based System
</subsectionHeader>
<bodyText confidence="0.999857055555555">
In hierarchical phrase-based translation (Chiang,
2007), a weighted synchronous context-free gram-
mar is induced from parallel text. In addition to
contiguous lexical phrases, hierarchical phrases
with up to two gaps are extracted. The search is
carried out with a parsing-based procedure. The
standard models integrated into our Jane hierar-
chical systems (Vilar et al., 2010; Huck et al.,
2012) are: Phrase translation probabilities and lex-
ical smoothing probabilities in both translation di-
rections, word and phrase penalty, binary features
marking hierarchical phrases, glue rule, and rules
with non-terminals at the boundaries, three binary
count features, and an n-gram language model.
We utilize the cube pruning algorithm for decod-
ing (Huck et al., 2013a) and optimize the model
weights with MERT. The optimization criterion is
BLEU.
</bodyText>
<subsectionHeader confidence="0.999858">
3.3 Other Tools and Techniques
</subsectionHeader>
<bodyText confidence="0.99995175">
We employ GIZA++ (Och and Ney, 2003) to train
word alignments. The two trained alignments
are heuristically merged to obtain a symmetrized
word alignment for phrase extraction. All lan-
</bodyText>
<page confidence="0.998926">
158
</page>
<tableCaption confidence="0.998959">
Table 2: Corpus statistics after each filtering step and compound splitting.
</tableCaption>
<table confidence="0.997694875">
Sentences Vocabulary English OOVs
German newstest2012
Preprocessing 2013 4.19M 1.43M 784K 1019
Preprocessing 2014 4.19M 1.42M 773K 1018
+ remove non-ascii symbols 4.17M 1.36M 713K 1021
+ remove wrong languages from target 4.15M 1.34M 675K 1027
+ remove wrong languages from source 4.08M 1.30M 655K 1039
+ compound splitting 4.08M 652K 655K 441
</table>
<bodyText confidence="0.995684142857143">
guage models (LMs) are created with the SRILM
toolkit (Stolcke, 2002) or with the KenLM lan-
guage model toolkit (Heafield et al., 2013) and are
standard 4-gram LMs with interpolated modified
Kneser-Ney smoothing (Kneser and Ney, 1995;
Chen and Goodman, 1998). We evaluate in true-
case with BLEU and TER (Snover et al., 2006).
</bodyText>
<subsectionHeader confidence="0.733035">
3.4 Discriminative Phrase Training
</subsectionHeader>
<bodyText confidence="0.999075">
In our baseline translation systems the phrase ta-
bles are created by a heuristic extraction from
word alignments and the probabilities are esti-
mated as relative frequencies, which is still the
state-of-the-art for many standard SMT systems.
Here, we applied a more sophisticated discrimi-
native phrase training method for the WMT 2014
German→English task. Similar to (He and Deng,
2012), a gradient-based method is used to opti-
mize a maximum expected BLEU objective, for
which we define BLEU on the sentence level with
smoothed 3-gram and 4-gram precisions. To that
end, the training data is decoded to generate 100-
best lists. We apply a leave-one-out heuristic
(Wuebker et al., 2010) to make better use of the
training data. Using these n-best lists, we itera-
tively perform updates on the phrasal translation
scores of the phrase table. After each iteration,
we run MERT, evaluate on the development set
and select the best performing iteration. In this
work, we perform two rounds of discriminative
training on two separate data sets. In the first
round, training is performed on the concatenation
of newstest2008 through newstest2010 and an au-
tomatic selection from the News-commentary, Eu-
roparl and Common Crawl corpora. The selec-
tion is based on cross-entropy difference of lan-
guage models and IBM-1 models as described by
Mansour et al. (2011) and contains 258K sentence
pairs. The training took 4.5 hours for 30 iterations.
On top of the final phrase-based systems, a second
round of discriminative training is run on the full
news-commentary corpus concatenated with new-
stest2008 through newstest2010.
</bodyText>
<sectionHeader confidence="0.6016585" genericHeader="method">
3.5 A Phrase Orientation Model for
Hierarchical Machine Translation
</sectionHeader>
<bodyText confidence="0.999926142857143">
In Huck et al. (2013b) a lexicalized reorder-
ing model for hierarchical phrase-based machine
translation was introduced. The model scores
monotone, swap, and discontinuous phrase ori-
entations in the manner of the one presented by
(Tillmann, 2004). Since improvements were re-
ported on a Chinese→English translation task, we
investigate the impact of this model on a European
language pair. As in German the word order is
more flexible compared with the target language
English, we expect that an additional reordering
model could improve the translation quality. In
our experiments we use the same settings which
worked best in (Huck et al., 2013b).
</bodyText>
<sectionHeader confidence="0.999063" genericHeader="method">
4 Setup
</sectionHeader>
<bodyText confidence="0.999989">
We trained the phrase-based and the hierarchical
translation system on all available bilingual train-
ing data. Corpus statistics can be found in the
last row of Table 2. The language model are
4-grams trained on the respective target side of
the bilingual data, 21 of the Shuffled News Crawl
corpus, 4 of the 109 French-English corpus and
</bodyText>
<equation confidence="0.428906">
1
2 of the LDC Gigaword Fifth Edition corpus.
1
</equation>
<bodyText confidence="0.999555625">
The monolingual data selection is based on cross-
entropy difference as described in (Moore and
Lewis, 2010). For the baseline language model,
we trained separate models for each corpus, which
were then interpolated. For our final experiments,
we also trained a single unpruned language model
on the concatenation of all monolingual data with
KenLM.
</bodyText>
<page confidence="0.999565">
159
</page>
<tableCaption confidence="0.782545">
Table 3: Results (truecase) for the German→English translation task. BLEU and TER are given in
percentage. All HPBT setups are tuned on the concatenation of newstest2012 and newstest2013. The
very first SCSS setups are optimized on newstest2012 only.
</tableCaption>
<table confidence="0.999941416666667">
newstest2011 TER newstest2012 TER newstest2013 TER
BLEU BLEU BLEU
SCSS +HRM 22.4 60.1 23.7 59.0 25.9 55.7
+wcLM 22.8 59.6 24.0 58.6 26.3 55.4
+1st round discr. 23.0 59.5 24.2 58.2 26.8 55.1
+tune11+12. 23.4 59.5 24.2 58.6 26.8 55.2
+unprunedLM 23.6 59.5 24.2 58.6 27.1 55.0
+2nd round discr. 23.7 59.5 24.4 58.5 27.2 55.0
HPBT baseline 23.3 59.9 24.2 58.9 26.7 55.6
+wcLM 23.4 59.8 24.1 58.9 26.8 55.6
+HRM 23.3 60.0 24.2 58.9 26.9 55.5
+HRM +wcLM 23.3 59.9 24.1 59.1 26.7 55.9
</table>
<subsectionHeader confidence="0.982827">
4.1 Experimental Results
</subsectionHeader>
<bodyText confidence="0.999985333333333">
The results of the phrase-based system (SCSS)
as well as the hierarchical phrase-based system
(HPBT) are summarized in Table 3.
The phrase-based baseline system, which in-
cludes the hierarchical reordering model by (Gal-
ley and Manning, 2008) and is tuned on new-
stest2012, reaches a performance of 25.9% BLEU
on newstest2013. Adding the word class language
model improves performance by 0.4% BLEU ab-
solute and the first round of discriminative phrase
training by 0.5% BLEU absolute. Next, we
switched to tuning on a concatenation of new-
stest2011 and newstest2012, which we expect to
be more reliable with respect to unseen data. Al-
though the BLEU score does not improve and TER
goes up slightly, we kept this tuning set in the sub-
sequent setups, as it yielded longer translations,
which in our experience will usually be preferred
by human evaluators. Switching from the inter-
polated language model to the unpruned language
model trained with KenLM on the full concate-
nated monolingual training data in a single pass
gained us another 0.3% BLEU. For the final sys-
tem, we ran a second round of discriminative train-
ing on different training data (cf. Section 3.4),
which increased performance by 0.1% BLEU to
the final score 27.2.
For the phrase-based system, we also exper-
imented with weighted phrase extraction (Man-
sour and Ney, 2012), but did not observe improve-
ments.
The hierarchical phrase-based baseline without
any additional model is on the same level as the
phrase-based system including the word class lan-
guage model, hierarchical reordering model and
discriminative phrase training in terms of BLEU.
However, extending the system with a word class
language model or the additional reordering mod-
els does not seem to help. Even the combination
of both models does not improve the translation
quality. Note, that the hierarchical system was
tuned on the concatenation newstest2011 and new-
stest2012. The final system employs both word
class language model and hierarchical reordering
model.
Both phrase-based and hierarchical phrase-
based final systems are used in the EU-Bridge sys-
tem combination (Freitag et al., 2014).
</bodyText>
<sectionHeader confidence="0.995192" genericHeader="conclusions">
5 Conclusion
</sectionHeader>
<bodyText confidence="0.999893071428571">
For the participation in the WMT 2014 shared
translation task, RWTH experimented with both
phrase-based and hierarchical translation systems.
For both approaches, we applied a hierarchical
phrase reordering model and a word class lan-
guage model. For the phrase-based system we em-
ployed discriminative phrase training. Addition-
ally, improvements of our preprocessing pipeline
compared to our WMT 2013 setup were described.
New introduced categories lead to a lower amount
of out-of-vocabulary words. Filtering the corpus
for wrong languages gives us lower vocabulary
sizes for source and target without loosing any per-
formance.
</bodyText>
<page confidence="0.996863">
160
</page>
<sectionHeader confidence="0.995209" genericHeader="acknowledgments">
Acknowledgments
</sectionHeader>
<bodyText confidence="0.999957454545455">
The research leading to these results has partially
received funding from the European Union Sev-
enth Framework Programme (FP7/2007-2013) un-
der grant agreement no 287658.
Furthermore, this material is partially based
upon work supported by the DARPA BOLT
project under Contract No. HR0011- 12-C-0015.
Any opinions, findings and conclusions or recom-
mendations expressed in this material are those of
the authors and do not necessarily reflect the views
of DARPA.
</bodyText>
<sectionHeader confidence="0.996555" genericHeader="references">
References
</sectionHeader>
<reference confidence="0.999688255319149">
Stanley F. Chen and Joshua Goodman. 1998. An
Empirical Study of Smoothing Techniques for Lan-
guage Modeling. Technical Report TR-10-98, Com-
puter Science Group, Harvard University, Cam-
bridge, Massachusetts, USA, August.
David Chiang. 2007. Hierarchical Phrase-Based
Translation. Computational Linguistics, 33(2):201–
228.
Markus Freitag, Stephan Peitz, Joern Wuebker, Her-
mann Ney, Matthias Huck, Rico Sennrich, Nadir
Durrani, Maria Nadejde, Philip Williams, Philipp
Koehn, Teresa Herrmann, Eunah Cho, and Alex
Waibel. 2014. EU-BRIDGE MT: Combined Ma-
chine Translation. In Proceedings of the ACL 2014
Ninth Workshop on Statistical Machine Translation,
Baltimore, MD, USA, June.
Michel Galley and Christopher D. Manning. 2008. A
Simple and Effective Hierarchical Phrase Reorder-
ing Model. In Proceedings of the 2008 Conference
on Empirical Methods in Natural Language Pro-
cessing, pages 847–855, Honolulu, Hawaii, USA,
October.
Xiaodong He and Li Deng. 2012. Maximum Expected
BLEU Training of Phrase and Lexicon Translation
Models. In Proceedings of the 50th Annual Meet-
ing of the Association for Computational Linguistics
(ACL), pages 292–301, Jeju, Republic of Korea, Jul.
Kenneth Heafield, Ivan Pouzyrevsky, Jonathan H.
Clark, and Philipp Koehn. 2013. Scalable modi-
fied Kneser-Ney language model estimation. In Pro-
ceedings of the 51st Annual Meeting of the Associa-
tion for Computational Linguistics, pages 690–696,
Sofia, Bulgaria, August.
Kenneth Heafield. 2011. KenLM: faster and smaller
language model queries. In Proceedings of the
EMNLP 2011 Sixth Workshop on Statistical Ma-
chine Translation, pages 187–197, Edinburgh, Scot-
land, United Kingdom, July.
Matthias Huck, Jan-Thorsten Peter, Markus Freitag,
Stephan Peitz, and Hermann Ney. 2012. Hierar-
chical Phrase-Based Translation with Jane 2. The
Prague Bulletin of Mathematical Linguistics, 98:37–
50, October.
Matthias Huck, David Vilar, Markus Freitag, and Her-
mann Ney. 2013a. A Performance Study of
Cube Pruning for Large-Scale Hierarchical Machine
Translation. In Proceedings of the NAACL 7th Work-
shop on Syntax, Semantics and Structure in Statis-
tical Translation, pages 29–38, Atlanta, Georgia,
USA, June.
Matthias Huck, Joern Wuebker, Felix Rietig, and Her-
mann Ney. 2013b. A phrase orientation model
for hierarchical machine translation. In ACL 2013
Eighth Workshop on Statistical Machine Transla-
tion, pages 452–463, Sofia, Bulgaria, August.
Reinhard Kneser and Hermann Ney. 1995. Im-
proved Backing-Off for M-gram Language Model-
ing. In Proceedings of the International Conference
on Acoustics, Speech, and Signal Processing, vol-
ume 1, pages 181–184, May.
Philipp Koehn and Kevin Knight. 2003. Empirical
Methods for Compound Splitting. In Proceedings of
European Chapter of the ACL (EACL 2009), pages
187–194.
Saab Mansour and Hermann Ney. 2012. A Simple and
Effective Weighted Phrase Extraction for Machine
Translation Adaptation. In Proceedings of the Inter-
national Workshop on Spoken Language Translation
(IWSLT), pages 193–200, Hong Kong, December.
Saab Mansour, Joern Wuebker, and Hermann Ney.
2011. Combining Translation and Language Model
Scoring for Domain-Specific Data Filtering. In Pro-
ceedings of the International Workshop on Spoken
Language Translation (IWSLT), pages 222–229, San
Francisco, California, USA, December.
Robert C. Moore and William Lewis. 2010. Intelligent
Selection of Language Model Training Data. In ACL
(Short Papers), pages 220–224, Uppsala, Sweden,
July.
Franz Josef Och and Hermann Ney. 2003. A System-
atic Comparison of Various Statistical Alignment
Models. Computational Linguistics, 29(1):19–51,
March.
Franz Josef Och. 2003. Minimum Error Rate Training
in Statistical Machine Translation. In Proc. of the
41th Annual Meeting of the Association for Compu-
tational Linguistics (ACL), pages 160–167, Sapporo,
Japan, July.
Kishore Papineni, Salim Roukos, Todd Ward, and Wei-
Jing Zhu. 2002. Bleu: a Method for Automatic
Evaluation of Machine Translation. In Proceed-
ings of the 41st Annual Meeting of the Associa-
tion for Computational Linguistics, pages 311–318,
Philadelphia, Pennsylvania, USA, July.
</reference>
<page confidence="0.976547">
161
</page>
<reference confidence="0.999695775510204">
Maja Popovi´c and Hermann Ney. 2006. POS-based
Word Reorderings for Statistical Machine Transla-
tion. In International Conference on Language Re-
sources and Evaluation, pages 1278–1283, Genoa,
Italy, May.
Matthew Snover, Bonnie Dorr, Richard Schwartz, Lin-
nea Micciulla, and John Makhoul. 2006. A Study of
Translation Edit Rate with Targeted Human Annota-
tion. In Proceedings of the 7th Conference of the As-
sociation for Machine Translation in the Americas,
pages 223–231, Cambridge, Massachusetts, USA,
August.
Andreas Stolcke. 2002. SRILM – An Extensible Lan-
guage Modeling Toolkit. In Proc. of the Int. Conf.
on Speech and Language Processing (ICSLP), vol-
ume 2, pages 901–904, Denver, Colorado, USA,
September.
Christoph Tillmann. 2004. A Unigram Orientation
Model for Statistical Machine Translation. In Pro-
ceedings of HLT-NAACL 2004: Short Papers, HLT-
NAACL-Short ’04, pages 101–104, Boston, MA,
USA.
David Vilar, Daniel Stein, Matthias Huck, and Her-
mann Ney. 2010. Jane: Open Source Hierarchi-
cal Translation, Extended with Reordering and Lex-
icon Models. In ACL 2010 Joint Fifth Workshop on
Statistical Machine Translation and Metrics MATR,
pages 262–270, Uppsala, Sweden, July.
David Vilar, Daniel Stein, Matthias Huck, and Her-
mann Ney. 2012. Jane: an advanced freely avail-
able hierarchical machine translation toolkit. Ma-
chine Translation, 26(3):197–216, September.
Joern Wuebker, Arne Mauser, and Hermann Ney.
2010. Training phrase translation models with
leaving-one-out. In Proceedings of the 48th Annual
Meeting of the Assoc. for Computational Linguistics,
pages 475–484, Uppsala, Sweden, July.
Joern Wuebker, Matthias Huck, Stephan Peitz, Malte
Nuhn, Markus Freitag, Jan-Thorsten Peter, Saab
Mansour, and Hermann Ney. 2012. Jane 2:
Open Source Phrase-based and Hierarchical Statis-
tical Machine Translation. In International Confer-
ence on Computational Linguistics, pages 483–491,
Mumbai, India, December.
Joern Wuebker, Stephan Peitz, Felix Rietig, and Her-
mann Ney. 2013. Improving statistical machine
translation with word class models. In Conference
on Empirical Methods in Natural Language Pro-
cessing, pages 1377–1381, Seattle, USA, October.
</reference>
<page confidence="0.997782">
162
</page>
</variant>
</algorithm>
<algorithm name="ParsHed" version="110505">
<variant no="0" confidence="0.400851">
<title confidence="0.929064">The RWTH Aachen German-English Machine Translation System for WMT 2014</title>
<author confidence="0.776172">Stephan Peitz</author>
<author confidence="0.776172">Joern Wuebker</author>
<author confidence="0.776172">Markus Freitag</author>
<author confidence="0.776172">Hermann</author>
<affiliation confidence="0.859550333333333">Human Language Technology and Pattern Recognition Computer Science RWTH Aachen</affiliation>
<address confidence="0.996403">D-52056 Aachen,</address>
<email confidence="0.998659">&lt;surname&gt;@cs.rwth-aachen.de</email>
<abstract confidence="0.996625384615385">This paper describes the statistical machine translation (SMT) systems developed at RWTH Aachen University for the translation task of the ACL 2014 Eighth Workshop on Statisti- Machine Translation 2014). Both hierarchical and phrase-based SMT systems are applied employing hierarchical phrase reordering and word class language models. For the phrase-based system, we run discriminative phrase training. In addition, we describe our preprocessing</abstract>
<intro confidence="0.815325">for</intro>
</variant>
</algorithm>
<algorithm name="ParsCit" version="110505">
<citationList>
<citation valid="true">
<authors>
<author>Stanley F Chen</author>
<author>Joshua Goodman</author>
</authors>
<title>An Empirical Study of Smoothing Techniques for Language Modeling.</title>
<date>1998</date>
<tech>Technical Report TR-10-98,</tech>
<institution>Computer Science Group, Harvard University,</institution>
<location>Cambridge, Massachusetts, USA,</location>
<contexts>
<context position="8379" citStr="Chen and Goodman, 1998" startWordPosition="1272" endWordPosition="1275">d compound splitting. Sentences Vocabulary English OOVs German newstest2012 Preprocessing 2013 4.19M 1.43M 784K 1019 Preprocessing 2014 4.19M 1.42M 773K 1018 + remove non-ascii symbols 4.17M 1.36M 713K 1021 + remove wrong languages from target 4.15M 1.34M 675K 1027 + remove wrong languages from source 4.08M 1.30M 655K 1039 + compound splitting 4.08M 652K 655K 441 guage models (LMs) are created with the SRILM toolkit (Stolcke, 2002) or with the KenLM language model toolkit (Heafield et al., 2013) and are standard 4-gram LMs with interpolated modified Kneser-Ney smoothing (Kneser and Ney, 1995; Chen and Goodman, 1998). We evaluate in truecase with BLEU and TER (Snover et al., 2006). 3.4 Discriminative Phrase Training In our baseline translation systems the phrase tables are created by a heuristic extraction from word alignments and the probabilities are estimated as relative frequencies, which is still the state-of-the-art for many standard SMT systems. Here, we applied a more sophisticated discriminative phrase training method for the WMT 2014 German→English task. Similar to (He and Deng, 2012), a gradient-based method is used to optimize a maximum expected BLEU objective, for which we define BLEU on the </context>
</contexts>
<marker>Chen, Goodman, 1998</marker>
<rawString>Stanley F. Chen and Joshua Goodman. 1998. An Empirical Study of Smoothing Techniques for Language Modeling. Technical Report TR-10-98, Computer Science Group, Harvard University, Cambridge, Massachusetts, USA, August.</rawString>
</citation>
<citation valid="true">
<authors>
<author>David Chiang</author>
</authors>
<title>Hierarchical Phrase-Based Translation.</title>
<date>2007</date>
<journal>Computational Linguistics,</journal>
<volume>33</volume>
<issue>2</issue>
<pages>228</pages>
<contexts>
<context position="6697" citStr="Chiang, 2007" startWordPosition="1015" endWordPosition="1016">hrase translation probabilities and lexical smoothing in both directions, word and phrase penalty, distancebased distortion model, an n-gram target language model and three binary count features. Additional models used in this evaluation are the hierarchical reordering model (HRM) (Galley and Manning, 2008) and a word class language model (wcLM) (Wuebker et al., 2013). The parameter weights are optimized with minimum error rate training (MERT) (Och, 2003). The optimization criterion is BLEU (Papineni et al., 2002). 3.2 Hierarchical Phrase-based System In hierarchical phrase-based translation (Chiang, 2007), a weighted synchronous context-free grammar is induced from parallel text. In addition to contiguous lexical phrases, hierarchical phrases with up to two gaps are extracted. The search is carried out with a parsing-based procedure. The standard models integrated into our Jane hierarchical systems (Vilar et al., 2010; Huck et al., 2012) are: Phrase translation probabilities and lexical smoothing probabilities in both translation directions, word and phrase penalty, binary features marking hierarchical phrases, glue rule, and rules with non-terminals at the boundaries, three binary count featu</context>
</contexts>
<marker>Chiang, 2007</marker>
<rawString>David Chiang. 2007. Hierarchical Phrase-Based Translation. Computational Linguistics, 33(2):201– 228.</rawString>
</citation>
<citation valid="true">
<authors>
<author>Markus Freitag</author>
<author>Stephan Peitz</author>
<author>Joern Wuebker</author>
<author>Hermann Ney</author>
<author>Matthias Huck</author>
<author>Rico Sennrich</author>
<author>Nadir Durrani</author>
</authors>
<title>EU-BRIDGE MT: Combined Machine Translation.</title>
<date>2014</date>
<booktitle>In Proceedings of the ACL 2014 Ninth Workshop on Statistical Machine Translation,</booktitle>
<location>Maria Nadejde, Philip Williams, Philipp Koehn, Teresa</location>
<contexts>
<context position="14432" citStr="Freitag et al., 2014" startWordPosition="2250" endWordPosition="2253">class language model, hierarchical reordering model and discriminative phrase training in terms of BLEU. However, extending the system with a word class language model or the additional reordering models does not seem to help. Even the combination of both models does not improve the translation quality. Note, that the hierarchical system was tuned on the concatenation newstest2011 and newstest2012. The final system employs both word class language model and hierarchical reordering model. Both phrase-based and hierarchical phrasebased final systems are used in the EU-Bridge system combination (Freitag et al., 2014). 5 Conclusion For the participation in the WMT 2014 shared translation task, RWTH experimented with both phrase-based and hierarchical translation systems. For both approaches, we applied a hierarchical phrase reordering model and a word class language model. For the phrase-based system we employed discriminative phrase training. Additionally, improvements of our preprocessing pipeline compared to our WMT 2013 setup were described. New introduced categories lead to a lower amount of out-of-vocabulary words. Filtering the corpus for wrong languages gives us lower vocabulary sizes for source an</context>
</contexts>
<marker>Freitag, Peitz, Wuebker, Ney, Huck, Sennrich, Durrani, 2014</marker>
<rawString>Markus Freitag, Stephan Peitz, Joern Wuebker, Hermann Ney, Matthias Huck, Rico Sennrich, Nadir Durrani, Maria Nadejde, Philip Williams, Philipp Koehn, Teresa Herrmann, Eunah Cho, and Alex Waibel. 2014. EU-BRIDGE MT: Combined Machine Translation. In Proceedings of the ACL 2014 Ninth Workshop on Statistical Machine Translation, Baltimore, MD, USA, June.</rawString>
</citation>
<citation valid="true">
<authors>
<author>Michel Galley</author>
<author>Christopher D Manning</author>
</authors>
<title>A Simple and Effective Hierarchical Phrase Reordering Model.</title>
<date>2008</date>
<booktitle>In Proceedings of the 2008 Conference on Empirical Methods in Natural Language Processing,</booktitle>
<pages>847--855</pages>
<location>Honolulu, Hawaii, USA,</location>
<contexts>
<context position="6392" citStr="Galley and Manning, 2008" startWordPosition="970" endWordPosition="973"> newest internal version, we use the KenLM Language Model Interface provided by (Heafield, 2011) for both decoders. 2http://www.hltpr.rwth-aachen.de/jane/ 3.1 Phrase-based System In the phrase-based decoder (source cardinality synchronous search, SCSS, Wuebker et al. (2012)), we use the standard set of models with phrase translation probabilities and lexical smoothing in both directions, word and phrase penalty, distancebased distortion model, an n-gram target language model and three binary count features. Additional models used in this evaluation are the hierarchical reordering model (HRM) (Galley and Manning, 2008) and a word class language model (wcLM) (Wuebker et al., 2013). The parameter weights are optimized with minimum error rate training (MERT) (Och, 2003). The optimization criterion is BLEU (Papineni et al., 2002). 3.2 Hierarchical Phrase-based System In hierarchical phrase-based translation (Chiang, 2007), a weighted synchronous context-free grammar is induced from parallel text. In addition to contiguous lexical phrases, hierarchical phrases with up to two gaps are extracted. The search is carried out with a parsing-based procedure. The standard models integrated into our Jane hierarchical sys</context>
<context position="12548" citStr="Galley and Manning, 2008" startWordPosition="1944" endWordPosition="1948">9.6 24.0 58.6 26.3 55.4 +1st round discr. 23.0 59.5 24.2 58.2 26.8 55.1 +tune11+12. 23.4 59.5 24.2 58.6 26.8 55.2 +unprunedLM 23.6 59.5 24.2 58.6 27.1 55.0 +2nd round discr. 23.7 59.5 24.4 58.5 27.2 55.0 HPBT baseline 23.3 59.9 24.2 58.9 26.7 55.6 +wcLM 23.4 59.8 24.1 58.9 26.8 55.6 +HRM 23.3 60.0 24.2 58.9 26.9 55.5 +HRM +wcLM 23.3 59.9 24.1 59.1 26.7 55.9 4.1 Experimental Results The results of the phrase-based system (SCSS) as well as the hierarchical phrase-based system (HPBT) are summarized in Table 3. The phrase-based baseline system, which includes the hierarchical reordering model by (Galley and Manning, 2008) and is tuned on newstest2012, reaches a performance of 25.9% BLEU on newstest2013. Adding the word class language model improves performance by 0.4% BLEU absolute and the first round of discriminative phrase training by 0.5% BLEU absolute. Next, we switched to tuning on a concatenation of newstest2011 and newstest2012, which we expect to be more reliable with respect to unseen data. Although the BLEU score does not improve and TER goes up slightly, we kept this tuning set in the subsequent setups, as it yielded longer translations, which in our experience will usually be preferred by human ev</context>
</contexts>
<marker>Galley, Manning, 2008</marker>
<rawString>Michel Galley and Christopher D. Manning. 2008. A Simple and Effective Hierarchical Phrase Reordering Model. In Proceedings of the 2008 Conference on Empirical Methods in Natural Language Processing, pages 847–855, Honolulu, Hawaii, USA, October.</rawString>
</citation>
<citation valid="true">
<authors>
<author>Xiaodong He</author>
<author>Li Deng</author>
</authors>
<title>Maximum Expected BLEU Training of Phrase and Lexicon Translation Models.</title>
<date>2012</date>
<booktitle>In Proceedings of the 50th Annual Meeting of the Association for Computational Linguistics (ACL),</booktitle>
<pages>292--301</pages>
<location>Jeju, Republic of</location>
<contexts>
<context position="8866" citStr="He and Deng, 2012" startWordPosition="1349" endWordPosition="1352">., 2013) and are standard 4-gram LMs with interpolated modified Kneser-Ney smoothing (Kneser and Ney, 1995; Chen and Goodman, 1998). We evaluate in truecase with BLEU and TER (Snover et al., 2006). 3.4 Discriminative Phrase Training In our baseline translation systems the phrase tables are created by a heuristic extraction from word alignments and the probabilities are estimated as relative frequencies, which is still the state-of-the-art for many standard SMT systems. Here, we applied a more sophisticated discriminative phrase training method for the WMT 2014 German→English task. Similar to (He and Deng, 2012), a gradient-based method is used to optimize a maximum expected BLEU objective, for which we define BLEU on the sentence level with smoothed 3-gram and 4-gram precisions. To that end, the training data is decoded to generate 100- best lists. We apply a leave-one-out heuristic (Wuebker et al., 2010) to make better use of the training data. Using these n-best lists, we iteratively perform updates on the phrasal translation scores of the phrase table. After each iteration, we run MERT, evaluate on the development set and select the best performing iteration. In this work, we perform two rounds o</context>
</contexts>
<marker>He, Deng, 2012</marker>
<rawString>Xiaodong He and Li Deng. 2012. Maximum Expected BLEU Training of Phrase and Lexicon Translation Models. In Proceedings of the 50th Annual Meeting of the Association for Computational Linguistics (ACL), pages 292–301, Jeju, Republic of Korea, Jul.</rawString>
</citation>
<citation valid="true">
<authors>
<author>Kenneth Heafield</author>
<author>Ivan Pouzyrevsky</author>
<author>Jonathan H Clark</author>
<author>Philipp Koehn</author>
</authors>
<title>Scalable modified Kneser-Ney language model estimation.</title>
<date>2013</date>
<booktitle>In Proceedings of the 51st Annual Meeting of the Association for Computational Linguistics,</booktitle>
<pages>690--696</pages>
<location>Sofia, Bulgaria,</location>
<contexts>
<context position="8256" citStr="Heafield et al., 2013" startWordPosition="1254" endWordPosition="1257">ain a symmetrized word alignment for phrase extraction. All lan158 Table 2: Corpus statistics after each filtering step and compound splitting. Sentences Vocabulary English OOVs German newstest2012 Preprocessing 2013 4.19M 1.43M 784K 1019 Preprocessing 2014 4.19M 1.42M 773K 1018 + remove non-ascii symbols 4.17M 1.36M 713K 1021 + remove wrong languages from target 4.15M 1.34M 675K 1027 + remove wrong languages from source 4.08M 1.30M 655K 1039 + compound splitting 4.08M 652K 655K 441 guage models (LMs) are created with the SRILM toolkit (Stolcke, 2002) or with the KenLM language model toolkit (Heafield et al., 2013) and are standard 4-gram LMs with interpolated modified Kneser-Ney smoothing (Kneser and Ney, 1995; Chen and Goodman, 1998). We evaluate in truecase with BLEU and TER (Snover et al., 2006). 3.4 Discriminative Phrase Training In our baseline translation systems the phrase tables are created by a heuristic extraction from word alignments and the probabilities are estimated as relative frequencies, which is still the state-of-the-art for many standard SMT systems. Here, we applied a more sophisticated discriminative phrase training method for the WMT 2014 German→English task. Similar to (He and D</context>
</contexts>
<marker>Heafield, Pouzyrevsky, Clark, Koehn, 2013</marker>
<rawString>Kenneth Heafield, Ivan Pouzyrevsky, Jonathan H. Clark, and Philipp Koehn. 2013. Scalable modified Kneser-Ney language model estimation. In Proceedings of the 51st Annual Meeting of the Association for Computational Linguistics, pages 690–696, Sofia, Bulgaria, August.</rawString>
</citation>
<citation valid="true">
<authors>
<author>Kenneth Heafield</author>
</authors>
<title>KenLM: faster and smaller language model queries.</title>
<date>2011</date>
<booktitle>In Proceedings of the EMNLP 2011 Sixth Workshop on Statistical Machine Translation,</booktitle>
<pages>187--197</pages>
<location>Edinburgh, Scotland, United Kingdom,</location>
<contexts>
<context position="5863" citStr="Heafield, 2011" startWordPosition="900" endWordPosition="901">bed in (Koehn and Knight, 2003). To reduce translation complexity, we employ the long-range part-of-speech based reordering rules proposed by Popovi´c and Ney (2006). 3 Translation Systems In this evaluation, we employ phrase-based translation and hierarchical phrase-based translation. Both approaches are implemented in Jane (Vilar et al., 2012; Wuebker et al., 2012), a statistical machine translation toolkit which has been developed at RWTH Aachen University and is freely available for non-commercial use.2 In the newest internal version, we use the KenLM Language Model Interface provided by (Heafield, 2011) for both decoders. 2http://www.hltpr.rwth-aachen.de/jane/ 3.1 Phrase-based System In the phrase-based decoder (source cardinality synchronous search, SCSS, Wuebker et al. (2012)), we use the standard set of models with phrase translation probabilities and lexical smoothing in both directions, word and phrase penalty, distancebased distortion model, an n-gram target language model and three binary count features. Additional models used in this evaluation are the hierarchical reordering model (HRM) (Galley and Manning, 2008) and a word class language model (wcLM) (Wuebker et al., 2013). The par</context>
</contexts>
<marker>Heafield, 2011</marker>
<rawString>Kenneth Heafield. 2011. KenLM: faster and smaller language model queries. In Proceedings of the EMNLP 2011 Sixth Workshop on Statistical Machine Translation, pages 187–197, Edinburgh, Scotland, United Kingdom, July.</rawString>
</citation>
<citation valid="true">
<authors>
<author>Matthias Huck</author>
<author>Jan-Thorsten Peter</author>
<author>Markus Freitag</author>
<author>Stephan Peitz</author>
<author>Hermann Ney</author>
</authors>
<date>2012</date>
<booktitle>Hierarchical Phrase-Based Translation with Jane 2. The Prague Bulletin of Mathematical Linguistics, 98:37– 50,</booktitle>
<contexts>
<context position="7036" citStr="Huck et al., 2012" startWordPosition="1066" endWordPosition="1069"> model (wcLM) (Wuebker et al., 2013). The parameter weights are optimized with minimum error rate training (MERT) (Och, 2003). The optimization criterion is BLEU (Papineni et al., 2002). 3.2 Hierarchical Phrase-based System In hierarchical phrase-based translation (Chiang, 2007), a weighted synchronous context-free grammar is induced from parallel text. In addition to contiguous lexical phrases, hierarchical phrases with up to two gaps are extracted. The search is carried out with a parsing-based procedure. The standard models integrated into our Jane hierarchical systems (Vilar et al., 2010; Huck et al., 2012) are: Phrase translation probabilities and lexical smoothing probabilities in both translation directions, word and phrase penalty, binary features marking hierarchical phrases, glue rule, and rules with non-terminals at the boundaries, three binary count features, and an n-gram language model. We utilize the cube pruning algorithm for decoding (Huck et al., 2013a) and optimize the model weights with MERT. The optimization criterion is BLEU. 3.3 Other Tools and Techniques We employ GIZA++ (Och and Ney, 2003) to train word alignments. The two trained alignments are heuristically merged to obtai</context>
</contexts>
<marker>Huck, Peter, Freitag, Peitz, Ney, 2012</marker>
<rawString>Matthias Huck, Jan-Thorsten Peter, Markus Freitag, Stephan Peitz, and Hermann Ney. 2012. Hierarchical Phrase-Based Translation with Jane 2. The Prague Bulletin of Mathematical Linguistics, 98:37– 50, October.</rawString>
</citation>
<citation valid="true">
<authors>
<author>Matthias Huck</author>
<author>David Vilar</author>
<author>Markus Freitag</author>
<author>Hermann Ney</author>
</authors>
<title>A Performance Study of Cube Pruning for Large-Scale Hierarchical Machine Translation.</title>
<date>2013</date>
<booktitle>In Proceedings of the NAACL 7th Workshop on Syntax, Semantics and Structure in Statistical Translation,</booktitle>
<pages>29--38</pages>
<location>Atlanta, Georgia, USA,</location>
<contexts>
<context position="7401" citStr="Huck et al., 2013" startWordPosition="1120" endWordPosition="1123">n to contiguous lexical phrases, hierarchical phrases with up to two gaps are extracted. The search is carried out with a parsing-based procedure. The standard models integrated into our Jane hierarchical systems (Vilar et al., 2010; Huck et al., 2012) are: Phrase translation probabilities and lexical smoothing probabilities in both translation directions, word and phrase penalty, binary features marking hierarchical phrases, glue rule, and rules with non-terminals at the boundaries, three binary count features, and an n-gram language model. We utilize the cube pruning algorithm for decoding (Huck et al., 2013a) and optimize the model weights with MERT. The optimization criterion is BLEU. 3.3 Other Tools and Techniques We employ GIZA++ (Och and Ney, 2003) to train word alignments. The two trained alignments are heuristically merged to obtain a symmetrized word alignment for phrase extraction. All lan158 Table 2: Corpus statistics after each filtering step and compound splitting. Sentences Vocabulary English OOVs German newstest2012 Preprocessing 2013 4.19M 1.43M 784K 1019 Preprocessing 2014 4.19M 1.42M 773K 1018 + remove non-ascii symbols 4.17M 1.36M 713K 1021 + remove wrong languages from target 4</context>
<context position="10180" citStr="Huck et al. (2013" startWordPosition="1560" endWordPosition="1563"> the concatenation of newstest2008 through newstest2010 and an automatic selection from the News-commentary, Europarl and Common Crawl corpora. The selection is based on cross-entropy difference of language models and IBM-1 models as described by Mansour et al. (2011) and contains 258K sentence pairs. The training took 4.5 hours for 30 iterations. On top of the final phrase-based systems, a second round of discriminative training is run on the full news-commentary corpus concatenated with newstest2008 through newstest2010. 3.5 A Phrase Orientation Model for Hierarchical Machine Translation In Huck et al. (2013b) a lexicalized reordering model for hierarchical phrase-based machine translation was introduced. The model scores monotone, swap, and discontinuous phrase orientations in the manner of the one presented by (Tillmann, 2004). Since improvements were reported on a Chinese→English translation task, we investigate the impact of this model on a European language pair. As in German the word order is more flexible compared with the target language English, we expect that an additional reordering model could improve the translation quality. In our experiments we use the same settings which worked be</context>
</contexts>
<marker>Huck, Vilar, Freitag, Ney, 2013</marker>
<rawString>Matthias Huck, David Vilar, Markus Freitag, and Hermann Ney. 2013a. A Performance Study of Cube Pruning for Large-Scale Hierarchical Machine Translation. In Proceedings of the NAACL 7th Workshop on Syntax, Semantics and Structure in Statistical Translation, pages 29–38, Atlanta, Georgia, USA, June.</rawString>
</citation>
<citation valid="true">
<authors>
<author>Matthias Huck</author>
<author>Joern Wuebker</author>
<author>Felix Rietig</author>
<author>Hermann Ney</author>
</authors>
<title>A phrase orientation model for hierarchical machine translation.</title>
<date>2013</date>
<booktitle>In ACL 2013 Eighth Workshop on Statistical Machine Translation,</booktitle>
<pages>452--463</pages>
<location>Sofia, Bulgaria,</location>
<contexts>
<context position="7401" citStr="Huck et al., 2013" startWordPosition="1120" endWordPosition="1123">n to contiguous lexical phrases, hierarchical phrases with up to two gaps are extracted. The search is carried out with a parsing-based procedure. The standard models integrated into our Jane hierarchical systems (Vilar et al., 2010; Huck et al., 2012) are: Phrase translation probabilities and lexical smoothing probabilities in both translation directions, word and phrase penalty, binary features marking hierarchical phrases, glue rule, and rules with non-terminals at the boundaries, three binary count features, and an n-gram language model. We utilize the cube pruning algorithm for decoding (Huck et al., 2013a) and optimize the model weights with MERT. The optimization criterion is BLEU. 3.3 Other Tools and Techniques We employ GIZA++ (Och and Ney, 2003) to train word alignments. The two trained alignments are heuristically merged to obtain a symmetrized word alignment for phrase extraction. All lan158 Table 2: Corpus statistics after each filtering step and compound splitting. Sentences Vocabulary English OOVs German newstest2012 Preprocessing 2013 4.19M 1.43M 784K 1019 Preprocessing 2014 4.19M 1.42M 773K 1018 + remove non-ascii symbols 4.17M 1.36M 713K 1021 + remove wrong languages from target 4</context>
<context position="10180" citStr="Huck et al. (2013" startWordPosition="1560" endWordPosition="1563"> the concatenation of newstest2008 through newstest2010 and an automatic selection from the News-commentary, Europarl and Common Crawl corpora. The selection is based on cross-entropy difference of language models and IBM-1 models as described by Mansour et al. (2011) and contains 258K sentence pairs. The training took 4.5 hours for 30 iterations. On top of the final phrase-based systems, a second round of discriminative training is run on the full news-commentary corpus concatenated with newstest2008 through newstest2010. 3.5 A Phrase Orientation Model for Hierarchical Machine Translation In Huck et al. (2013b) a lexicalized reordering model for hierarchical phrase-based machine translation was introduced. The model scores monotone, swap, and discontinuous phrase orientations in the manner of the one presented by (Tillmann, 2004). Since improvements were reported on a Chinese→English translation task, we investigate the impact of this model on a European language pair. As in German the word order is more flexible compared with the target language English, we expect that an additional reordering model could improve the translation quality. In our experiments we use the same settings which worked be</context>
</contexts>
<marker>Huck, Wuebker, Rietig, Ney, 2013</marker>
<rawString>Matthias Huck, Joern Wuebker, Felix Rietig, and Hermann Ney. 2013b. A phrase orientation model for hierarchical machine translation. In ACL 2013 Eighth Workshop on Statistical Machine Translation, pages 452–463, Sofia, Bulgaria, August.</rawString>
</citation>
<citation valid="true">
<authors>
<author>Reinhard Kneser</author>
<author>Hermann Ney</author>
</authors>
<title>Improved Backing-Off for M-gram Language Modeling.</title>
<date>1995</date>
<booktitle>In Proceedings of the International Conference on Acoustics, Speech, and Signal Processing,</booktitle>
<volume>1</volume>
<pages>181--184</pages>
<contexts>
<context position="8354" citStr="Kneser and Ney, 1995" startWordPosition="1268" endWordPosition="1271">each filtering step and compound splitting. Sentences Vocabulary English OOVs German newstest2012 Preprocessing 2013 4.19M 1.43M 784K 1019 Preprocessing 2014 4.19M 1.42M 773K 1018 + remove non-ascii symbols 4.17M 1.36M 713K 1021 + remove wrong languages from target 4.15M 1.34M 675K 1027 + remove wrong languages from source 4.08M 1.30M 655K 1039 + compound splitting 4.08M 652K 655K 441 guage models (LMs) are created with the SRILM toolkit (Stolcke, 2002) or with the KenLM language model toolkit (Heafield et al., 2013) and are standard 4-gram LMs with interpolated modified Kneser-Ney smoothing (Kneser and Ney, 1995; Chen and Goodman, 1998). We evaluate in truecase with BLEU and TER (Snover et al., 2006). 3.4 Discriminative Phrase Training In our baseline translation systems the phrase tables are created by a heuristic extraction from word alignments and the probabilities are estimated as relative frequencies, which is still the state-of-the-art for many standard SMT systems. Here, we applied a more sophisticated discriminative phrase training method for the WMT 2014 German→English task. Similar to (He and Deng, 2012), a gradient-based method is used to optimize a maximum expected BLEU objective, for whi</context>
</contexts>
<marker>Kneser, Ney, 1995</marker>
<rawString>Reinhard Kneser and Hermann Ney. 1995. Improved Backing-Off for M-gram Language Modeling. In Proceedings of the International Conference on Acoustics, Speech, and Signal Processing, volume 1, pages 181–184, May.</rawString>
</citation>
<citation valid="true">
<authors>
<author>Philipp Koehn</author>
<author>Kevin Knight</author>
</authors>
<title>Empirical Methods for Compound Splitting.</title>
<date>2003</date>
<booktitle>In Proceedings of European Chapter of the ACL (EACL</booktitle>
<pages>187--194</pages>
<contexts>
<context position="5279" citStr="Koehn and Knight, 2003" startWordPosition="812" endWordPosition="815">ld be reduced by ∼130k words for both source and target side of our bilingual training data while the OOV rate kept the same. Our experiments showed, that the translation quality is the same with or without removing wrong sentences. Nevertheless, we reduced the training data size and also the vocabulary size without any degradation in terms of translation quality. 2.3 Morpho-syntactic Analysis In order to reduce the source vocabulary size for the German→English translation further, the German text is preprocessed by splitting German compound words with the frequency-based method described in (Koehn and Knight, 2003). To reduce translation complexity, we employ the long-range part-of-speech based reordering rules proposed by Popovi´c and Ney (2006). 3 Translation Systems In this evaluation, we employ phrase-based translation and hierarchical phrase-based translation. Both approaches are implemented in Jane (Vilar et al., 2012; Wuebker et al., 2012), a statistical machine translation toolkit which has been developed at RWTH Aachen University and is freely available for non-commercial use.2 In the newest internal version, we use the KenLM Language Model Interface provided by (Heafield, 2011) for both decode</context>
</contexts>
<marker>Koehn, Knight, 2003</marker>
<rawString>Philipp Koehn and Kevin Knight. 2003. Empirical Methods for Compound Splitting. In Proceedings of European Chapter of the ACL (EACL 2009), pages 187–194.</rawString>
</citation>
<citation valid="true">
<authors>
<author>Saab Mansour</author>
<author>Hermann Ney</author>
</authors>
<title>A Simple and Effective Weighted Phrase Extraction for Machine Translation Adaptation.</title>
<date>2012</date>
<booktitle>In Proceedings of the International Workshop on Spoken Language Translation (IWSLT),</booktitle>
<pages>193--200</pages>
<location>Hong Kong,</location>
<contexts>
<context position="13640" citStr="Mansour and Ney, 2012" startWordPosition="2128" endWordPosition="2132">ing set in the subsequent setups, as it yielded longer translations, which in our experience will usually be preferred by human evaluators. Switching from the interpolated language model to the unpruned language model trained with KenLM on the full concatenated monolingual training data in a single pass gained us another 0.3% BLEU. For the final system, we ran a second round of discriminative training on different training data (cf. Section 3.4), which increased performance by 0.1% BLEU to the final score 27.2. For the phrase-based system, we also experimented with weighted phrase extraction (Mansour and Ney, 2012), but did not observe improvements. The hierarchical phrase-based baseline without any additional model is on the same level as the phrase-based system including the word class language model, hierarchical reordering model and discriminative phrase training in terms of BLEU. However, extending the system with a word class language model or the additional reordering models does not seem to help. Even the combination of both models does not improve the translation quality. Note, that the hierarchical system was tuned on the concatenation newstest2011 and newstest2012. The final system employs bo</context>
</contexts>
<marker>Mansour, Ney, 2012</marker>
<rawString>Saab Mansour and Hermann Ney. 2012. A Simple and Effective Weighted Phrase Extraction for Machine Translation Adaptation. In Proceedings of the International Workshop on Spoken Language Translation (IWSLT), pages 193–200, Hong Kong, December.</rawString>
</citation>
<citation valid="true">
<authors>
<author>Saab Mansour</author>
<author>Joern Wuebker</author>
<author>Hermann Ney</author>
</authors>
<title>Combining Translation and Language Model Scoring for Domain-Specific Data Filtering.</title>
<date>2011</date>
<booktitle>In Proceedings of the International Workshop on Spoken Language Translation (IWSLT),</booktitle>
<pages>222--229</pages>
<location>San Francisco, California, USA,</location>
<contexts>
<context position="9831" citStr="Mansour et al. (2011)" startWordPosition="1507" endWordPosition="1510">ing these n-best lists, we iteratively perform updates on the phrasal translation scores of the phrase table. After each iteration, we run MERT, evaluate on the development set and select the best performing iteration. In this work, we perform two rounds of discriminative training on two separate data sets. In the first round, training is performed on the concatenation of newstest2008 through newstest2010 and an automatic selection from the News-commentary, Europarl and Common Crawl corpora. The selection is based on cross-entropy difference of language models and IBM-1 models as described by Mansour et al. (2011) and contains 258K sentence pairs. The training took 4.5 hours for 30 iterations. On top of the final phrase-based systems, a second round of discriminative training is run on the full news-commentary corpus concatenated with newstest2008 through newstest2010. 3.5 A Phrase Orientation Model for Hierarchical Machine Translation In Huck et al. (2013b) a lexicalized reordering model for hierarchical phrase-based machine translation was introduced. The model scores monotone, swap, and discontinuous phrase orientations in the manner of the one presented by (Tillmann, 2004). Since improvements were </context>
</contexts>
<marker>Mansour, Wuebker, Ney, 2011</marker>
<rawString>Saab Mansour, Joern Wuebker, and Hermann Ney. 2011. Combining Translation and Language Model Scoring for Domain-Specific Data Filtering. In Proceedings of the International Workshop on Spoken Language Translation (IWSLT), pages 222–229, San Francisco, California, USA, December.</rawString>
</citation>
<citation valid="true">
<authors>
<author>Robert C Moore</author>
<author>William Lewis</author>
</authors>
<title>Intelligent Selection of Language Model Training Data.</title>
<date>2010</date>
<booktitle>In ACL (Short Papers),</booktitle>
<pages>220--224</pages>
<location>Uppsala, Sweden,</location>
<contexts>
<context position="11308" citStr="Moore and Lewis, 2010" startWordPosition="1743" endWordPosition="1746">d improve the translation quality. In our experiments we use the same settings which worked best in (Huck et al., 2013b). 4 Setup We trained the phrase-based and the hierarchical translation system on all available bilingual training data. Corpus statistics can be found in the last row of Table 2. The language model are 4-grams trained on the respective target side of the bilingual data, 21 of the Shuffled News Crawl corpus, 4 of the 109 French-English corpus and 1 2 of the LDC Gigaword Fifth Edition corpus. 1 The monolingual data selection is based on crossentropy difference as described in (Moore and Lewis, 2010). For the baseline language model, we trained separate models for each corpus, which were then interpolated. For our final experiments, we also trained a single unpruned language model on the concatenation of all monolingual data with KenLM. 159 Table 3: Results (truecase) for the German→English translation task. BLEU and TER are given in percentage. All HPBT setups are tuned on the concatenation of newstest2012 and newstest2013. The very first SCSS setups are optimized on newstest2012 only. newstest2011 TER newstest2012 TER newstest2013 TER BLEU BLEU BLEU SCSS +HRM 22.4 60.1 23.7 59.0 25.9 55</context>
</contexts>
<marker>Moore, Lewis, 2010</marker>
<rawString>Robert C. Moore and William Lewis. 2010. Intelligent Selection of Language Model Training Data. In ACL (Short Papers), pages 220–224, Uppsala, Sweden, July.</rawString>
</citation>
<citation valid="true">
<authors>
<author>Franz Josef Och</author>
<author>Hermann Ney</author>
</authors>
<title>A Systematic Comparison of Various Statistical Alignment Models.</title>
<date>2003</date>
<journal>Computational Linguistics,</journal>
<volume>29</volume>
<issue>1</issue>
<contexts>
<context position="7549" citStr="Och and Ney, 2003" startWordPosition="1144" endWordPosition="1147">The standard models integrated into our Jane hierarchical systems (Vilar et al., 2010; Huck et al., 2012) are: Phrase translation probabilities and lexical smoothing probabilities in both translation directions, word and phrase penalty, binary features marking hierarchical phrases, glue rule, and rules with non-terminals at the boundaries, three binary count features, and an n-gram language model. We utilize the cube pruning algorithm for decoding (Huck et al., 2013a) and optimize the model weights with MERT. The optimization criterion is BLEU. 3.3 Other Tools and Techniques We employ GIZA++ (Och and Ney, 2003) to train word alignments. The two trained alignments are heuristically merged to obtain a symmetrized word alignment for phrase extraction. All lan158 Table 2: Corpus statistics after each filtering step and compound splitting. Sentences Vocabulary English OOVs German newstest2012 Preprocessing 2013 4.19M 1.43M 784K 1019 Preprocessing 2014 4.19M 1.42M 773K 1018 + remove non-ascii symbols 4.17M 1.36M 713K 1021 + remove wrong languages from target 4.15M 1.34M 675K 1027 + remove wrong languages from source 4.08M 1.30M 655K 1039 + compound splitting 4.08M 652K 655K 441 guage models (LMs) are crea</context>
</contexts>
<marker>Och, Ney, 2003</marker>
<rawString>Franz Josef Och and Hermann Ney. 2003. A Systematic Comparison of Various Statistical Alignment Models. Computational Linguistics, 29(1):19–51, March.</rawString>
</citation>
<citation valid="true">
<authors>
<author>Franz Josef Och</author>
</authors>
<title>Minimum Error Rate Training in Statistical Machine Translation.</title>
<date>2003</date>
<booktitle>In Proc. of the 41th Annual Meeting of the Association for Computational Linguistics (ACL),</booktitle>
<pages>160--167</pages>
<location>Sapporo, Japan,</location>
<contexts>
<context position="6543" citStr="Och, 2003" startWordPosition="996" endWordPosition="997">based System In the phrase-based decoder (source cardinality synchronous search, SCSS, Wuebker et al. (2012)), we use the standard set of models with phrase translation probabilities and lexical smoothing in both directions, word and phrase penalty, distancebased distortion model, an n-gram target language model and three binary count features. Additional models used in this evaluation are the hierarchical reordering model (HRM) (Galley and Manning, 2008) and a word class language model (wcLM) (Wuebker et al., 2013). The parameter weights are optimized with minimum error rate training (MERT) (Och, 2003). The optimization criterion is BLEU (Papineni et al., 2002). 3.2 Hierarchical Phrase-based System In hierarchical phrase-based translation (Chiang, 2007), a weighted synchronous context-free grammar is induced from parallel text. In addition to contiguous lexical phrases, hierarchical phrases with up to two gaps are extracted. The search is carried out with a parsing-based procedure. The standard models integrated into our Jane hierarchical systems (Vilar et al., 2010; Huck et al., 2012) are: Phrase translation probabilities and lexical smoothing probabilities in both translation directions, </context>
</contexts>
<marker>Och, 2003</marker>
<rawString>Franz Josef Och. 2003. Minimum Error Rate Training in Statistical Machine Translation. In Proc. of the 41th Annual Meeting of the Association for Computational Linguistics (ACL), pages 160–167, Sapporo, Japan, July.</rawString>
</citation>
<citation valid="true">
<authors>
<author>Kishore Papineni</author>
<author>Salim Roukos</author>
<author>Todd Ward</author>
<author>WeiJing Zhu</author>
</authors>
<title>Bleu: a Method for Automatic Evaluation of Machine Translation.</title>
<date>2002</date>
<booktitle>In Proceedings of the 41st Annual Meeting of the Association for Computational Linguistics,</booktitle>
<pages>311--318</pages>
<location>Philadelphia, Pennsylvania, USA,</location>
<contexts>
<context position="6603" citStr="Papineni et al., 2002" startWordPosition="1003" endWordPosition="1006"> cardinality synchronous search, SCSS, Wuebker et al. (2012)), we use the standard set of models with phrase translation probabilities and lexical smoothing in both directions, word and phrase penalty, distancebased distortion model, an n-gram target language model and three binary count features. Additional models used in this evaluation are the hierarchical reordering model (HRM) (Galley and Manning, 2008) and a word class language model (wcLM) (Wuebker et al., 2013). The parameter weights are optimized with minimum error rate training (MERT) (Och, 2003). The optimization criterion is BLEU (Papineni et al., 2002). 3.2 Hierarchical Phrase-based System In hierarchical phrase-based translation (Chiang, 2007), a weighted synchronous context-free grammar is induced from parallel text. In addition to contiguous lexical phrases, hierarchical phrases with up to two gaps are extracted. The search is carried out with a parsing-based procedure. The standard models integrated into our Jane hierarchical systems (Vilar et al., 2010; Huck et al., 2012) are: Phrase translation probabilities and lexical smoothing probabilities in both translation directions, word and phrase penalty, binary features marking hierarchica</context>
</contexts>
<marker>Papineni, Roukos, Ward, Zhu, 2002</marker>
<rawString>Kishore Papineni, Salim Roukos, Todd Ward, and WeiJing Zhu. 2002. Bleu: a Method for Automatic Evaluation of Machine Translation. In Proceedings of the 41st Annual Meeting of the Association for Computational Linguistics, pages 311–318, Philadelphia, Pennsylvania, USA, July.</rawString>
</citation>
<citation valid="true">
<authors>
<author>Maja Popovi´c</author>
<author>Hermann Ney</author>
</authors>
<title>POS-based Word Reorderings for Statistical Machine Translation.</title>
<date>2006</date>
<booktitle>In International Conference on Language Resources and Evaluation,</booktitle>
<pages>1278--1283</pages>
<location>Genoa, Italy,</location>
<marker>Popovi´c, Ney, 2006</marker>
<rawString>Maja Popovi´c and Hermann Ney. 2006. POS-based Word Reorderings for Statistical Machine Translation. In International Conference on Language Resources and Evaluation, pages 1278–1283, Genoa, Italy, May.</rawString>
</citation>
<citation valid="true">
<authors>
<author>Matthew Snover</author>
<author>Bonnie Dorr</author>
<author>Richard Schwartz</author>
<author>Linnea Micciulla</author>
<author>John Makhoul</author>
</authors>
<title>A Study of Translation Edit Rate with Targeted Human Annotation.</title>
<date>2006</date>
<booktitle>In Proceedings of the 7th Conference of the Association for Machine Translation in the Americas,</booktitle>
<pages>223--231</pages>
<location>Cambridge, Massachusetts, USA,</location>
<contexts>
<context position="8444" citStr="Snover et al., 2006" startWordPosition="1285" endWordPosition="1288">est2012 Preprocessing 2013 4.19M 1.43M 784K 1019 Preprocessing 2014 4.19M 1.42M 773K 1018 + remove non-ascii symbols 4.17M 1.36M 713K 1021 + remove wrong languages from target 4.15M 1.34M 675K 1027 + remove wrong languages from source 4.08M 1.30M 655K 1039 + compound splitting 4.08M 652K 655K 441 guage models (LMs) are created with the SRILM toolkit (Stolcke, 2002) or with the KenLM language model toolkit (Heafield et al., 2013) and are standard 4-gram LMs with interpolated modified Kneser-Ney smoothing (Kneser and Ney, 1995; Chen and Goodman, 1998). We evaluate in truecase with BLEU and TER (Snover et al., 2006). 3.4 Discriminative Phrase Training In our baseline translation systems the phrase tables are created by a heuristic extraction from word alignments and the probabilities are estimated as relative frequencies, which is still the state-of-the-art for many standard SMT systems. Here, we applied a more sophisticated discriminative phrase training method for the WMT 2014 German→English task. Similar to (He and Deng, 2012), a gradient-based method is used to optimize a maximum expected BLEU objective, for which we define BLEU on the sentence level with smoothed 3-gram and 4-gram precisions. To tha</context>
</contexts>
<marker>Snover, Dorr, Schwartz, Micciulla, Makhoul, 2006</marker>
<rawString>Matthew Snover, Bonnie Dorr, Richard Schwartz, Linnea Micciulla, and John Makhoul. 2006. A Study of Translation Edit Rate with Targeted Human Annotation. In Proceedings of the 7th Conference of the Association for Machine Translation in the Americas, pages 223–231, Cambridge, Massachusetts, USA, August.</rawString>
</citation>
<citation valid="true">
<authors>
<author>Andreas Stolcke</author>
</authors>
<title>SRILM – An Extensible Language Modeling Toolkit.</title>
<date>2002</date>
<booktitle>In Proc. of the Int. Conf. on Speech and Language Processing (ICSLP),</booktitle>
<volume>2</volume>
<pages>901--904</pages>
<location>Denver, Colorado, USA,</location>
<contexts>
<context position="8191" citStr="Stolcke, 2002" startWordPosition="1244" endWordPosition="1245">he two trained alignments are heuristically merged to obtain a symmetrized word alignment for phrase extraction. All lan158 Table 2: Corpus statistics after each filtering step and compound splitting. Sentences Vocabulary English OOVs German newstest2012 Preprocessing 2013 4.19M 1.43M 784K 1019 Preprocessing 2014 4.19M 1.42M 773K 1018 + remove non-ascii symbols 4.17M 1.36M 713K 1021 + remove wrong languages from target 4.15M 1.34M 675K 1027 + remove wrong languages from source 4.08M 1.30M 655K 1039 + compound splitting 4.08M 652K 655K 441 guage models (LMs) are created with the SRILM toolkit (Stolcke, 2002) or with the KenLM language model toolkit (Heafield et al., 2013) and are standard 4-gram LMs with interpolated modified Kneser-Ney smoothing (Kneser and Ney, 1995; Chen and Goodman, 1998). We evaluate in truecase with BLEU and TER (Snover et al., 2006). 3.4 Discriminative Phrase Training In our baseline translation systems the phrase tables are created by a heuristic extraction from word alignments and the probabilities are estimated as relative frequencies, which is still the state-of-the-art for many standard SMT systems. Here, we applied a more sophisticated discriminative phrase training </context>
</contexts>
<marker>Stolcke, 2002</marker>
<rawString>Andreas Stolcke. 2002. SRILM – An Extensible Language Modeling Toolkit. In Proc. of the Int. Conf. on Speech and Language Processing (ICSLP), volume 2, pages 901–904, Denver, Colorado, USA, September.</rawString>
</citation>
<citation valid="true">
<authors>
<author>Christoph Tillmann</author>
</authors>
<title>A Unigram Orientation Model for Statistical Machine Translation.</title>
<date>2004</date>
<booktitle>In Proceedings of HLT-NAACL 2004: Short Papers, HLTNAACL-Short ’04,</booktitle>
<pages>101--104</pages>
<location>Boston, MA, USA.</location>
<contexts>
<context position="10405" citStr="Tillmann, 2004" startWordPosition="1594" endWordPosition="1595">els as described by Mansour et al. (2011) and contains 258K sentence pairs. The training took 4.5 hours for 30 iterations. On top of the final phrase-based systems, a second round of discriminative training is run on the full news-commentary corpus concatenated with newstest2008 through newstest2010. 3.5 A Phrase Orientation Model for Hierarchical Machine Translation In Huck et al. (2013b) a lexicalized reordering model for hierarchical phrase-based machine translation was introduced. The model scores monotone, swap, and discontinuous phrase orientations in the manner of the one presented by (Tillmann, 2004). Since improvements were reported on a Chinese→English translation task, we investigate the impact of this model on a European language pair. As in German the word order is more flexible compared with the target language English, we expect that an additional reordering model could improve the translation quality. In our experiments we use the same settings which worked best in (Huck et al., 2013b). 4 Setup We trained the phrase-based and the hierarchical translation system on all available bilingual training data. Corpus statistics can be found in the last row of Table 2. The language model a</context>
</contexts>
<marker>Tillmann, 2004</marker>
<rawString>Christoph Tillmann. 2004. A Unigram Orientation Model for Statistical Machine Translation. In Proceedings of HLT-NAACL 2004: Short Papers, HLTNAACL-Short ’04, pages 101–104, Boston, MA, USA.</rawString>
</citation>
<citation valid="true">
<authors>
<author>David Vilar</author>
<author>Daniel Stein</author>
<author>Matthias Huck</author>
<author>Hermann Ney</author>
</authors>
<title>Jane: Open Source Hierarchical Translation, Extended with Reordering and Lexicon Models.</title>
<date>2010</date>
<booktitle>In ACL 2010 Joint Fifth Workshop on Statistical Machine Translation and Metrics MATR,</booktitle>
<pages>262--270</pages>
<location>Uppsala, Sweden,</location>
<contexts>
<context position="7016" citStr="Vilar et al., 2010" startWordPosition="1062" endWordPosition="1065"> word class language model (wcLM) (Wuebker et al., 2013). The parameter weights are optimized with minimum error rate training (MERT) (Och, 2003). The optimization criterion is BLEU (Papineni et al., 2002). 3.2 Hierarchical Phrase-based System In hierarchical phrase-based translation (Chiang, 2007), a weighted synchronous context-free grammar is induced from parallel text. In addition to contiguous lexical phrases, hierarchical phrases with up to two gaps are extracted. The search is carried out with a parsing-based procedure. The standard models integrated into our Jane hierarchical systems (Vilar et al., 2010; Huck et al., 2012) are: Phrase translation probabilities and lexical smoothing probabilities in both translation directions, word and phrase penalty, binary features marking hierarchical phrases, glue rule, and rules with non-terminals at the boundaries, three binary count features, and an n-gram language model. We utilize the cube pruning algorithm for decoding (Huck et al., 2013a) and optimize the model weights with MERT. The optimization criterion is BLEU. 3.3 Other Tools and Techniques We employ GIZA++ (Och and Ney, 2003) to train word alignments. The two trained alignments are heuristic</context>
</contexts>
<marker>Vilar, Stein, Huck, Ney, 2010</marker>
<rawString>David Vilar, Daniel Stein, Matthias Huck, and Hermann Ney. 2010. Jane: Open Source Hierarchical Translation, Extended with Reordering and Lexicon Models. In ACL 2010 Joint Fifth Workshop on Statistical Machine Translation and Metrics MATR, pages 262–270, Uppsala, Sweden, July.</rawString>
</citation>
<citation valid="true">
<authors>
<author>David Vilar</author>
<author>Daniel Stein</author>
<author>Matthias Huck</author>
<author>Hermann Ney</author>
</authors>
<title>Jane: an advanced freely available hierarchical machine translation toolkit.</title>
<date>2012</date>
<journal>Machine Translation,</journal>
<volume>26</volume>
<issue>3</issue>
<contexts>
<context position="5594" citStr="Vilar et al., 2012" startWordPosition="855" endWordPosition="858">y degradation in terms of translation quality. 2.3 Morpho-syntactic Analysis In order to reduce the source vocabulary size for the German→English translation further, the German text is preprocessed by splitting German compound words with the frequency-based method described in (Koehn and Knight, 2003). To reduce translation complexity, we employ the long-range part-of-speech based reordering rules proposed by Popovi´c and Ney (2006). 3 Translation Systems In this evaluation, we employ phrase-based translation and hierarchical phrase-based translation. Both approaches are implemented in Jane (Vilar et al., 2012; Wuebker et al., 2012), a statistical machine translation toolkit which has been developed at RWTH Aachen University and is freely available for non-commercial use.2 In the newest internal version, we use the KenLM Language Model Interface provided by (Heafield, 2011) for both decoders. 2http://www.hltpr.rwth-aachen.de/jane/ 3.1 Phrase-based System In the phrase-based decoder (source cardinality synchronous search, SCSS, Wuebker et al. (2012)), we use the standard set of models with phrase translation probabilities and lexical smoothing in both directions, word and phrase penalty, distancebas</context>
</contexts>
<marker>Vilar, Stein, Huck, Ney, 2012</marker>
<rawString>David Vilar, Daniel Stein, Matthias Huck, and Hermann Ney. 2012. Jane: an advanced freely available hierarchical machine translation toolkit. Machine Translation, 26(3):197–216, September.</rawString>
</citation>
<citation valid="true">
<authors>
<author>Joern Wuebker</author>
<author>Arne Mauser</author>
<author>Hermann Ney</author>
</authors>
<title>Training phrase translation models with leaving-one-out.</title>
<date>2010</date>
<booktitle>In Proceedings of the 48th Annual Meeting of the Assoc. for Computational Linguistics,</booktitle>
<pages>475--484</pages>
<location>Uppsala, Sweden,</location>
<contexts>
<context position="9166" citStr="Wuebker et al., 2010" startWordPosition="1399" endWordPosition="1402">d by a heuristic extraction from word alignments and the probabilities are estimated as relative frequencies, which is still the state-of-the-art for many standard SMT systems. Here, we applied a more sophisticated discriminative phrase training method for the WMT 2014 German→English task. Similar to (He and Deng, 2012), a gradient-based method is used to optimize a maximum expected BLEU objective, for which we define BLEU on the sentence level with smoothed 3-gram and 4-gram precisions. To that end, the training data is decoded to generate 100- best lists. We apply a leave-one-out heuristic (Wuebker et al., 2010) to make better use of the training data. Using these n-best lists, we iteratively perform updates on the phrasal translation scores of the phrase table. After each iteration, we run MERT, evaluate on the development set and select the best performing iteration. In this work, we perform two rounds of discriminative training on two separate data sets. In the first round, training is performed on the concatenation of newstest2008 through newstest2010 and an automatic selection from the News-commentary, Europarl and Common Crawl corpora. The selection is based on cross-entropy difference of langu</context>
</contexts>
<marker>Wuebker, Mauser, Ney, 2010</marker>
<rawString>Joern Wuebker, Arne Mauser, and Hermann Ney. 2010. Training phrase translation models with leaving-one-out. In Proceedings of the 48th Annual Meeting of the Assoc. for Computational Linguistics, pages 475–484, Uppsala, Sweden, July.</rawString>
</citation>
<citation valid="true">
<authors>
<author>Joern Wuebker</author>
<author>Matthias Huck</author>
<author>Stephan Peitz</author>
<author>Malte Nuhn</author>
<author>Markus Freitag</author>
<author>Jan-Thorsten Peter</author>
<author>Saab Mansour</author>
<author>Hermann Ney</author>
</authors>
<title>Jane 2: Open Source Phrase-based and Hierarchical Statistical Machine Translation.</title>
<date>2012</date>
<booktitle>In International Conference on Computational Linguistics,</booktitle>
<pages>483--491</pages>
<location>Mumbai, India,</location>
<contexts>
<context position="5617" citStr="Wuebker et al., 2012" startWordPosition="859" endWordPosition="862">ms of translation quality. 2.3 Morpho-syntactic Analysis In order to reduce the source vocabulary size for the German→English translation further, the German text is preprocessed by splitting German compound words with the frequency-based method described in (Koehn and Knight, 2003). To reduce translation complexity, we employ the long-range part-of-speech based reordering rules proposed by Popovi´c and Ney (2006). 3 Translation Systems In this evaluation, we employ phrase-based translation and hierarchical phrase-based translation. Both approaches are implemented in Jane (Vilar et al., 2012; Wuebker et al., 2012), a statistical machine translation toolkit which has been developed at RWTH Aachen University and is freely available for non-commercial use.2 In the newest internal version, we use the KenLM Language Model Interface provided by (Heafield, 2011) for both decoders. 2http://www.hltpr.rwth-aachen.de/jane/ 3.1 Phrase-based System In the phrase-based decoder (source cardinality synchronous search, SCSS, Wuebker et al. (2012)), we use the standard set of models with phrase translation probabilities and lexical smoothing in both directions, word and phrase penalty, distancebased distortion model, an</context>
</contexts>
<marker>Wuebker, Huck, Peitz, Nuhn, Freitag, Peter, Mansour, Ney, 2012</marker>
<rawString>Joern Wuebker, Matthias Huck, Stephan Peitz, Malte Nuhn, Markus Freitag, Jan-Thorsten Peter, Saab Mansour, and Hermann Ney. 2012. Jane 2: Open Source Phrase-based and Hierarchical Statistical Machine Translation. In International Conference on Computational Linguistics, pages 483–491, Mumbai, India, December.</rawString>
</citation>
<citation valid="true">
<authors>
<author>Joern Wuebker</author>
<author>Stephan Peitz</author>
<author>Felix Rietig</author>
<author>Hermann Ney</author>
</authors>
<title>Improving statistical machine translation with word class models.</title>
<date>2013</date>
<booktitle>In Conference on Empirical Methods in Natural Language Processing,</booktitle>
<pages>1377--1381</pages>
<location>Seattle, USA,</location>
<contexts>
<context position="6454" citStr="Wuebker et al., 2013" startWordPosition="981" endWordPosition="984">e provided by (Heafield, 2011) for both decoders. 2http://www.hltpr.rwth-aachen.de/jane/ 3.1 Phrase-based System In the phrase-based decoder (source cardinality synchronous search, SCSS, Wuebker et al. (2012)), we use the standard set of models with phrase translation probabilities and lexical smoothing in both directions, word and phrase penalty, distancebased distortion model, an n-gram target language model and three binary count features. Additional models used in this evaluation are the hierarchical reordering model (HRM) (Galley and Manning, 2008) and a word class language model (wcLM) (Wuebker et al., 2013). The parameter weights are optimized with minimum error rate training (MERT) (Och, 2003). The optimization criterion is BLEU (Papineni et al., 2002). 3.2 Hierarchical Phrase-based System In hierarchical phrase-based translation (Chiang, 2007), a weighted synchronous context-free grammar is induced from parallel text. In addition to contiguous lexical phrases, hierarchical phrases with up to two gaps are extracted. The search is carried out with a parsing-based procedure. The standard models integrated into our Jane hierarchical systems (Vilar et al., 2010; Huck et al., 2012) are: Phrase trans</context>
</contexts>
<marker>Wuebker, Peitz, Rietig, Ney, 2013</marker>
<rawString>Joern Wuebker, Stephan Peitz, Felix Rietig, and Hermann Ney. 2013. Improving statistical machine translation with word class models. In Conference on Empirical Methods in Natural Language Processing, pages 1377–1381, Seattle, USA, October.</rawString>
</citation>
</citationList>
</algorithm>
</algorithms>
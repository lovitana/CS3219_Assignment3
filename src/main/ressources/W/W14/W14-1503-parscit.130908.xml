<?xml version="1.0" encoding="UTF-8"?>
<algorithms version="110505">
<algorithm name="SectLabel" version="110505">
<variant no="0" confidence="0.000967">
<title confidence="0.998664">
A Systematic Study of Semantic Vector Space Model Parameters
</title>
<author confidence="0.994017">
Douwe Kiela Stephen Clark
</author>
<affiliation confidence="0.998375">
University of Cambridge University of Cambridge
Computer Laboratory Computer Laboratory
</affiliation>
<email confidence="0.992252">
douwe.kiela@cl.cam.ac.uk sc609@cam.ac.uk
</email>
<sectionHeader confidence="0.993715" genericHeader="abstract">
Abstract
</sectionHeader>
<bodyText confidence="0.9995874">
We present a systematic study of parame-
ters used in the construction of semantic
vector space models. Evaluation is car-
ried out on a variety of similarity tasks, in-
cluding a compositionality dataset, using
several source corpora. In addition to rec-
ommendations for optimal parameters, we
present some novel findings, including a
similarity metric that outperforms the al-
ternatives on all tasks considered.
</bodyText>
<sectionHeader confidence="0.998795" genericHeader="keywords">
1 Introduction
</sectionHeader>
<bodyText confidence="0.999808285714286">
Vector space models (VSMs) represent the mean-
ings of lexical items as vectors in a “semantic
space”. The benefit of VSMs is that they can eas-
ily be manipulated using linear algebra, allowing
a degree of similarity between vectors to be com-
puted. They rely on the distributional hypothesis
(Harris, 1954): the idea that “words that occur in
similar contexts tend to have similar meanings”
(Turney and Pantel, 2010; Erk, 2012). The con-
struction of a suitable VSM for a particular task is
highly parameterised, and there appears to be little
consensus over which parameter settings to use.
This paper presents a systematic study of the
following parameters:
</bodyText>
<listItem confidence="0.999934428571428">
• vector size;
• window size;
• window-based or dependency-based context;
• feature granularity;
• similarity metric;
• weighting scheme;
• stopwords and high frequency cut-off.
</listItem>
<bodyText confidence="0.975302142857143">
A representative set of semantic similarity
datasets has been selected from the literature, in-
cluding a phrasal similarity dataset for evaluating
compositionality. The choice of source corpus is
likely to influence the quality of the VSM, and so
we use a selection of source corpora. Hence there
are two additional “superparameters”:
</bodyText>
<listItem confidence="0.9995955">
• dataset for evaluation;
• source corpus.
</listItem>
<bodyText confidence="0.999824096774193">
Previous studies have been limited to investigat-
ing only a small number of parameters, and us-
ing a limited set of source corpora and tasks for
evaluation (Curran and Moens, 2002a; Curran and
Moens, 2002b; Curran, 2004; Grefenstette, 1994;
Pado and Lapata, 2007; Sahlgren, 2006; Turney
and Pantel, 2010; Schulte im Walde et al., 2013).
Rohde et al. (2006) considered several weighting
schemes for a large variety of tasks, while Weeds
et al. (2004) did the same for similarity metrics.
Stone et al. (2008) investigated the effectiveness
of sub-spacing corpora, where a larger corpus is
queried in order to construct a smaller sub-spaced
corpus (Zelikovitz and Kogan, 2006). Blacoe and
Lapata (2012) compare several types of vector rep-
resentations for semantic composition tasks. The
most comprehensive existing studies of VSM pa-
rameters — encompassing window sizes, feature
granularity, stopwords and dimensionality reduc-
tion — are by Bullinaria and Levy (2007; 2012)
and Lapesa and Evert (2013).
Section 2 introduces the various parameters of
vector space model construction. We then attempt,
in Section 3, to answer some of the fundamen-
tal questions for building VSMs through a number
of experiments that consider each of the selected
parameters. In Section 4 we examine how these
findings relate to the recent development of dis-
tributional compositional semantics (Baroni et al.,
2013; Clark, 2014), where vectors for words are
combined into vectors for phrases.
</bodyText>
<sectionHeader confidence="0.638127" genericHeader="introduction">
2 Data and Parameters
</sectionHeader>
<bodyText confidence="0.409377666666667">
Two datasets have dominated the literature with
respect to VSM parameters: WordSim353 (Finkel-
stein et al., 2002) and the TOEFL synonym dataset
</bodyText>
<page confidence="0.989716">
21
</page>
<note confidence="0.930954222222222">
Proceedings of the 2nd Workshop on Continuous Vector Space Models and their Compositionality (CVSC) @ EACL 2014, pages 21–30,
Gothenburg, Sweden, April 26-30 2014. c�2014 Association for Computational Linguistics
Dataset Pairings Words
RG 65 48
MC 30 39
W353 353 437
MEN 3000 751
TOEFL 80 400
M&amp;L10 324 314
</note>
<tableCaption confidence="0.976094">
Table 1: Datasets for evaluation
</tableCaption>
<bodyText confidence="0.999932181818182">
(Landauer and Dumais, 1997). There is a risk
that semantic similarity studies have been overfit-
ting to their idiosyncracies, so in this study we
evaluate on a variety of datasets: in addition to
WordSim353 (W353) and TOEFL, we also use
the Rubenstein &amp; Goodenough (RG) (1965) and
Miller &amp; Charles (MC) (1991) data, as well as
a much larger set of similarity ratings: the MEN
dataset (Bruni et al., 2012). All these datasets con-
sist of human similarity ratings for word pairings,
except TOEFL, which consists of multiple choice
questions where the task is to select the correct
synonym for a target word. In Section 4 we ex-
amine our parameters in the context of distribu-
tional compositional semantics, using the evalua-
tion dataset from Mitchell and Lapata (2010). Ta-
ble 1 gives statistics for the number of words and
word pairings in each of the datasets.
As well as using a variety of datasets, we also
consider three different corpora from which to
build the vectors, varying in size and domain.
These include the BNC (Burnard, 2007) (106
word types, 108 tokens) and the larger ukWaC
(Baroni et al., 2009) (107 types, 109 tokens).
We also include a sub-spaced Wikipedia corpus
(Stone et al., 2008): for all words in the eval-
uation datasets, we build a subcorpus by query-
ing the top 10-ranked Wikipedia documents using
the words as search terms, resulting in a corpus
with 106 word types and 107 tokens. For examin-
ing the dependency-based contexts, we include the
Google Syntactic N-gram corpus (Goldberg and
Orwant, 2013), with 107 types and 1011 tokens.
</bodyText>
<subsectionHeader confidence="0.97667">
2.1 Parameters
</subsectionHeader>
<bodyText confidence="0.998616309090909">
We selected the following set of parameters for in-
vestigation, all of which are fundamental to vector
space model construction1.
1Another obvious parameter would be dimensionality re-
duction, which we chose not to include because it does not
represent a fundamental aspect of VSM construction: di-
mensionality reduction relies on some original non-reduced
model, and directly depends on its quality.
Vector size Each component of a vector repre-
sents a context (or perhaps more accurately a “con-
textual element”, such as second word to the left
of the target word).2 The number of components
varies hugely in the literature, but a typical value
is in the low thousands. Here we consider vec-
tor sizes ranging from 50,000 to 500,000, to see
whether larger vectors lead to better performance.
Context There are two main approaches to mod-
elling context: window-based and dependency-
based. For window-based methods, contexts are
determined by word co-occurrences within a win-
dow of a given size, where the window simply
spans a number of words occurring around in-
stances of a target word. For dependency-based
methods, the contexts are determined by word
co-occurrences in a particular syntactic relation
with a target word (e.g. target word dog is the
subject of run, where run subj is the context).
We consider different window sizes and compare
window-based and dependency-based methods.
Feature granularity Context words, or “fea-
tures”, are often stemmed or lemmatised. We in-
vestigate the effect of stemming and lemmatisa-
tion, in particular to see whether the effect varies
with corpus size. We also consider more fine-
grained features in which each context word is
paired with a POS tag or a lexical category from
CCG (Steedman, 2000).
Similarity metric A variety of metrics can be
used to calculate the similarity between two vec-
tors. We consider the similarity metrics in Table 2.
Weighting Weighting schemes increase the im-
portance of contexts that are more indicative of the
meaning of the target word: the fact that cat co-
occurs with purr is much more informative than
its co-occurrence with the. Table 3 gives defini-
tions of the weighting schemes considered.
Stopwords, high frequency cut-off Function
words and stopwords are often considered too un-
informative to be suitable context words. Ignor-
ing them not only leads to a reduction in model
size and computational effort, but also to a more
informative distributional vector. Hence we fol-
lowed standard practice and did not use stopwords
as context words (using the stoplist in NLTK (Bird
et al., 2009)). The question we investigated is
</bodyText>
<footnote confidence="0.9849495">
2We will use the term “feature” or “context” or “context
word” to refer to contextual elements.
</footnote>
<page confidence="0.999549">
22
</page>
<tableCaption confidence="0.7452595">
Table 2: Similarity measures between vectors v
and u, where vi is the ith component of v
</tableCaption>
<figure confidence="0.970549625">
Measure
Definition
Jensen-Shannon Div
α-skew
2 (D(u ||u+v
1 2 )+D(v ||u+v
2 ))
√2 log 2
1 − D(u||αv+(1−α)u)
√2 log 2
1
Euclidean
Cityblock
Chebyshev
Cosine
Correlation
Dice
Jaccard
Jaccard2
Lin
Tanimoto
1
1+√Pn i=1(ui−vi)2
1
1+Pni=1 |ui−vi|
1
1+maxi |ui−vi|
u·v
|u||v|
(u−µu)·(v−µv)
|u||v|
2 Pn
Pni=0 min(ui,vi)
i=0 ui+vi
u·v
Pni=0 ui+vi
Pn
Pn i=0 min(ui,vi)
i=0 max(ui,vi)
Pni=0 ui+vi
|u|+|v|
u·v
|u|+|v|−u·v
Scheme
Definition
None
TF-IDF
TF-ICF
</figure>
<equation confidence="0.981978076923077">
wij = fij
wij = log(fij) × log(njN)
wij = log(fij) × log(fjN)
Okapi BM25
fij
wij = fj
0.5+1.5× fj
N−nj+0.5
log fij+0.5
rPN i=1[(0.5+0.5× fij
maxf ) log( nj N )]2
(log(fij)+1.0) log( nj N )
0.8+0.2×fj×fj j
P (tij|cj)
wij = log P(tij)P(cj)
max(0, MI)
P(tij|cj)−P(tij)P(cj)
√ P(tij)P(cj )
see (Curran, 2004, p. 83)
fij×f
wij = fi×fj
wij = −1 × log nj N
wij =
wij =
log fij+1
log nj +1
</equation>
<figure confidence="0.8034616">
+fij
j
(0.5+0.5× fij
maxf ) log( nj N )
wij =
wij =
ATC
LTU
MI
PosMI
T-Test
x2
Lin98a
Lin98b
Gref94
</figure>
<bodyText confidence="0.9692295">
whether removing more context words, based on
a frequency cut-off, can improve performance.
</bodyText>
<sectionHeader confidence="0.997696" genericHeader="method">
3 Experiments
</sectionHeader>
<bodyText confidence="0.997766735294118">
The parameter space is too large to analyse ex-
haustively, and so we adopted a strategy for how
to navigate through it, selecting certain parame-
ters to investigate first, which then get fixed or
“clamped” in the remaining experiments. Unless
specified otherwise, vectors are generated with the
following restrictions and transformations on fea-
tures: stopwords are removed, numbers mapped
to ‘NUM’, and only strings consisting of alphanu-
meric characters are allowed. In all experiments,
the features consist of the frequency-ranked first n
words in the given source corpus.
Four of the five similarity datasets (RG, MC,
W353, MEN) contain continuous scales of sim-
ilarity ratings for word pairs; hence we follow
standard practice in using a Spearman correlation
coefficient ps for evaluation. The fifth dataset
(TOEFL) is a set of multiple-choice questions,
for which an accuracy measure is appropriate.
Calculating an aggregate score over all datasets
is non-trivial, since taking the mean of correla-
tion scores leads to an under-estimation of per-
formance; hence for the aggregate score we use
the Fisher-transformed z-variable of the correla-
Table 3: Term weighting schemes. fij denotes the
target word frequency in a particular context, fi
the total target word frequency, fj the total context
frequency, N the total of all frequencies, nj the
number of non-zero contexts. P(tij|cj) is defined
fij
as fj and P(tij) as fij N .
tion datasets, and take the weighted average of
its inverse over the correlation datasets and the
TOEFL accuracy score (Silver and Dunlap, 1987).
</bodyText>
<subsectionHeader confidence="0.998837">
3.1 Vector size
</subsectionHeader>
<bodyText confidence="0.999860866666667">
The first parameter we investigate is vector size,
measured by the number of features. Vectors are
constructed from the BNC using a window-based
method, with a window size of 5 (2 words either
side of the target word). We experiment with vec-
tor sizes up to 0.5M features, which is close to the
total number of context words present in the en-
tire BNC according to our preprocessing scheme.
Features are added according to frequency in the
BNC, with increasingly more rare features being
added. For weighting we consider both Positive
Mutual Information and T-Test, which have been
found to work best in previous research (Bullinaria
and Levy, 2012; Curran, 2004). Similarity is com-
puted using Cosine.
</bodyText>
<page confidence="0.9977">
23
</page>
<figureCaption confidence="0.9824125">
Figure 1: Impact of vector size on performance
across different datasets
</figureCaption>
<bodyText confidence="0.9999252">
The results in Figure 1 show a clear trend: for
both weighting schemes, performance no longer
improves after around 50,000 features; in fact, for
T-test weighting, and some of the datasets, perfor-
mance initially declines with an increase in fea-
tures. Hence we conclude that continuing to add
more rare features is detrimental to performance,
and that 50,000 features or less will give good per-
formance. An added benefit of smaller vectors is
the reduction in computational cost.
</bodyText>
<subsectionHeader confidence="0.99954">
3.2 Window size
</subsectionHeader>
<bodyText confidence="0.98049768">
Recent studies have found that the best window
size depends on the task at hand. For example,
Hill et al. (2013) found that smaller windows work
best for measuring similarity of concrete nouns,
whereas larger window sizes work better for ab-
stract nouns. Schulte im Walde et al. (2013) found
that a large window size worked best for a com-
positionality dataset of German noun-noun com-
pounds. Similar relations between window size
and performance have been found for similar ver-
sus related words, as well as for similar versus as-
sociated words (Turney and Pantel, 2010).
We experiment with window sizes of 3, 5, 7, 9
and a full sentence. (A window size of n implies
n−1
2 words either side of the target word.) We
use Positive Mutual Information weighting, Co-
sine similarity, and vectors of size 50,000 (based
on the results from Section 3.1). Figure 2 shows
the results for all the similarity datasets, with the
aggregated score at the bottom right.
Performance was evaluated on three corpora,
in order to answer three questions: Does win-
dow size affect performance? Does corpus size
interact with window size? Does corpus sub-
</bodyText>
<figureCaption confidence="0.937746">
Figure 2: Impact of window size across three cor-
pora
</figureCaption>
<bodyText confidence="0.999865333333333">
spacing interact with window size? Figure 2
clearly shows the answer to all three questions is
“yes”. First, ukWaC consistently outperforms the
BNC, across all window sizes, indicating that a
larger source corpus leads to better performance.
Second, we see that the larger ukWaC performs
better with smaller window sizes compared to the
BNC, with the best ukWaC performance typically
being found with a window size of only 3. For
the BNC, it appears that a larger window is able to
offset the smaller size of corpus to some extent.
We also evaluated on a sub-spaced Wikipedia
source corpus similar to Stone et al. (2008), which
performs much better with larger window sizes
than the BNC or ukWaC. Our explanation for this
result is that sub-spacing, resulting from search-
ing for Wikipedia pages with the appropriate tar-
get terms, provides a focused, less noisy corpus in
which context words some distance from the target
word are still relevant to its meaning.
In summary, the highest score is typically
achieved with the largest source corpora and
smallest window size, with the exception of the
much smaller sub-spaced Wikipedia corpus.
</bodyText>
<subsectionHeader confidence="0.995243">
3.3 Context
</subsectionHeader>
<bodyText confidence="0.987585666666667">
The notion of context plays a key role in VSMs.
Pado and Lapata (2007) present a comparison of
window-based versus dependency-based methods
and conclude that dependency-based contexts give
better results. We also compare window-based and
dependency-based models.
Dependency-parsed versions of the BNC and
ukWaC were used to construct syntactically-
informed vectors, with a single, labelled arc be-
</bodyText>
<page confidence="0.997842">
24
</page>
<figureCaption confidence="0.997687">
Figure 3: Window versus dependency contexts
</figureCaption>
<bodyText confidence="0.999889323529412">
tween the target word and context word.3 Since
this effectively provides a window size of 3, we
also use a window size of 3 for the window-based
method (which provided the best results in Sec-
tion 3.2 with the ukWaC corpus). As well as
the ukWaC and BNC source corpora, we use the
Google syntactic N-gram corpus (Goldberg and
Orwant, 2013), which is one of the largest cor-
pora to date, and which consists of syntactic n-
grams as opposed to window-based n-grams. We
use vectors of size 50,000 with Positive Mutual In-
formation weighting and Cosine similarity. Due
to its size and associated computational cost, we
used only 10,000 contexts for the vectors gener-
ated from the syntactic N-gram corpus. The re-
sults are shown in Figure 3.
In contrast to the idea that dependency-based
methods outperform window-based methods, we
find that the window-based models outperform
dependency-based models when they are con-
structed from the same corpus using the small
window size. However, Google’s syntactic N-
gram corpus does indeed outperform window-
based methods, even though smaller vectors were
used for the Google models (10,000 vs. 50,000
features). We observe large variations across
datasets, with window-based methods performing
particularly well on some, but not all. In partic-
ular, window-based methods clearly outperform
dependency-based methods on the RG dataset (for
the same source corpus), whereas the opposite
trend is observed for the TOEFL synonym dataset.
The summary is that the model built from the syn-
tactic N-grams is the overall winner, but when we
</bodyText>
<footnote confidence="0.9838565">
3The Clark and Curran (2007) parser was used to provide
the dependencies.
</footnote>
<bodyText confidence="0.972859666666667">
compare both methods on the same corpus, the
window-based method on a large corpus appears
to work best (given the small window size).
</bodyText>
<subsectionHeader confidence="0.988246">
3.4 Feature granularity
</subsectionHeader>
<bodyText confidence="0.999992689655172">
Stemming and lemmatisation are standard tech-
niques in NLP and IR to reduce data sparsity.
However, with large enough corpora it may be
that the loss of information through generalisa-
tion hurts performance. In fact, it may be that in-
creased granularity – through the use of grammat-
ical tags – can lead to improved performance. We
test these hypotheses by comparing four types of
processed context words: lemmatised, stemmed,
POS-tagged, and tagged with CCG lexical cate-
gories (which can be thought of as fine-grained
POS tags (Clark and Curran, 2007)).4 The source
corpora are BNC and ukWaC, using a window-
based method with windows of size 5, Positive
Mutual Information weighting, vectors of size
50,000 and Cosine similarity. The results are re-
ported in Figure 4.
The ukWaC-generated vectors outperform the
BNC-generated ones on all but a single instance
for each of the granularities. Stemming yields
the best overall performance, and increasing the
granularity does not lead to better results. Even
with a very large corpus like ukWaC, stemming
yields signficantly better results than not reduc-
ing the feature granularity at all. Conversely, apart
from the results on the TOEFL synonym dataset,
increasing the feature granularity of contexts by
including POS tags or CCG categories does not
yield any improvement.
</bodyText>
<subsectionHeader confidence="0.974706">
3.5 Similarity-weighting combination
</subsectionHeader>
<bodyText confidence="0.999936571428571">
There is contrasting evidence in the literature re-
garding which combination of similarity metric
and weighting scheme works best. Here we inves-
tigate this question using vectors of size 50,000,
no processing of the context features (i.e., “nor-
mal” feature granularity), and a window-based
method with a window size of 5. Aggregated
scores across the datasets are reported in Tables
4 and 5 for the BNC and ukWaC, respectively.
There are some clear messages to be taken from
these large tables of results. First, two weighting
schemes perform better than the others: Positive
Mutual Information (PosMI) and T-Test. On the
BNC, the former yields the best results. There are
</bodyText>
<footnote confidence="0.969682">
4Using NLTK’s Porter stemmer and WordNet lemmatiser.
</footnote>
<page confidence="0.998343">
25
</page>
<figureCaption confidence="0.990305">
Figure 4: Feature granularity: stemmed (S), lem-
matised (L), normal (N), POS-tagged (T) and
CCG-tagged (C)
</figureCaption>
<table confidence="0.9997726">
RG MC W353 MEN TOEFL
P+COS 0.74 0.64 0.50 0.66 0.76
P+COR 0.74 0.65 0.58 0.71 0.83
T+COS 0.78 0.69 0.54 0.68 0.78
T+COR 0.78 0.71 0.54 0.68 0.78
</table>
<tableCaption confidence="0.969124">
Table 6: Similarity scores on individual datasets
</tableCaption>
<bodyText confidence="0.991362608695652">
for positive mutual information (P) and T-test
(T) weighting, with cosine (COS) and correlation
(COR) similarity
three similarity metrics that perform particularly
well: Cosine, Correlation and the Tanimoto coef-
ficient (the latter also being similar to Cosine; see
Table 2). The Correlation similarity metric has the
most consistent performance across the different
weighting schemes, and yields the highest score
for both corpora. The most consistent weighting
scheme across the two source corpora and similar-
ity metrics appears to be PosMI.
The highest combined aggregate score is that of
PosMI with the Correlation metric, in line with
the conclusion of Bullinaria and Levy (2012) that
PosMI is the best weighting scheme5. However,
for the large ukWaC corpus, T-Test achieves sim-
ilarly high aggregate scores, in line with the work
of Curran (2004). When we look at these two
weighting schemes in more detail, we see that T-
Test works best for the RG and MC datasets, while
PosMI works best for the others; see Table 6. Cor-
relation is the best similarity metric in all cases.
</bodyText>
<footnote confidence="0.996382">
5In some cases, the combination of weighting scheme and
similarity metric results in a division by zero or leads to tak-
ing the logarithm of a negative number, in which cases we
report the aggregate scores as nan (not-a-number).
</footnote>
<figureCaption confidence="0.990623">
Figure 5: Finding the optimal “contiguous subvec-
tor” of size 10,000
</figureCaption>
<subsectionHeader confidence="0.994719">
3.6 Optimal subvector
</subsectionHeader>
<bodyText confidence="0.9999275">
Stopwords are typically removed from vectors and
not used as features. However, Bullinaria and
Levy (2012) find that removing stopwords has no
effect on performance. A possible explanation
is that, since they are using a weighting scheme,
the weights of stopwords are low enough that
they have effectively been removed anyhow. This
raises the question: are we removing stopwords
because they contribute little towards the meaning
of the target word, or are we removing them be-
cause they have high frequency?
The experiment used ukWaC, with a window-
based method and window size of 5, normal fea-
ture granularity, Cosine similarity and a sliding
vector of size 10,000. Having a sliding vector im-
plies that we throw away up to the first 40,000 con-
texts as we slide across to the 50,000 mark (replac-
ing the higher frequency contexts with lower fre-
quency ones). In effect, we are trying to find the
cut-off point where the 10,000-component “con-
tiguous subvector” of the target word vector is
optimal (where the features are ordered by fre-
quency). Results are given for PosMI, T-Test and
no weighting at all.
The results are shown in Figure 5. T-test outper-
forms PosMI at the higher frequency ranges (to the
left of the plots) but PosMI gives better results for
some of the datasets further to the right. For both
weighting schemes the performance decreases as
high frequency contexts are replaced with lower
frequency contexts.
A different picture emerges when no weight-
ing is used, however. Here the performance can
increase as high-frequency contexts are replaced
</bodyText>
<page confidence="0.994643">
26
</page>
<table confidence="0.997402714285714">
British National Corpus
COS COR DIC JC1 JC2 TAN LIN EUC CIB CHS JSD ASK
none 0.49 0.50 0.34 0.35 0.27 0.22 0.30 0.09 0.11 0.08 0.45 0.36
tfidf 0.43 0.44 0.33 0.34 0.22 0.16 0.27 0.13 0.12 0.16 0.38 0.32
tficf 0.47 0.48 0.34 0.36 0.23 0.16 0.27 0.13 0.12 0.15 0.40 0.33
okapi 0.40 0.42 0.37 0.42 0.22 0.23 0.26 0.25 0.15 0.14 0.37 0.26
atc 0.40 0.43 0.25 0.24 0.16 0.34 0.30 0.10 0.13 0.08 0.33 0.23
ltu 0.44 0.45 0.35 0.36 0.22 0.23 0.26 0.22 0.13 0.21 0.37 0.27
mi 0.58 0.61 0.31 0.56 0.29 -0.07 0.45 0.15 0.10 0.09 0.16 -0.04
posmi 0.63 0.66 0.52 0.58 0.35 -0.08 0.45 0.15 0.11 0.06 0.54 0.46
ttest 0.63 0.62 0.11 0.34 0.08 0.63 0.17 0.18 0.14 0.11 nan nan
chisquared 0.50 0.50 0.46 0.42 0.42 0.42 nan 0.06 0.07 0.08 0.57 0.52
lin98b 0.47 0.52 0.35 0.40 0.21 -0.10 0.29 0.10 0.11 nan 0.38 0.29
gref94 0.46 0.49 0.35 0.37 0.23 0.06 0.28 0.12 0.11 0.09 0.41 0.30
</table>
<tableCaption confidence="0.914919">
Table 4: Aggregated scores for combinations of weighting schemes and similarity metrics using the BNC.
The similarity metrics are Cosine (COS), Correlation (COR), Dice (DIC), Jaccard (JC1), Jaccard2 (JC2),
Tanimoto (TAN), Lin (LIN), Euclidean (EUC), CityBlock (CIB), Chebyshev (CHS), Jensen-Shannon
Divergence (JSD) and α-skew (ASK)
</tableCaption>
<table confidence="0.993287571428571">
ukWaC
COS COR DIC JC1 JC2 TAN LIN EUC CIB CHS JSD ASK
none 0.55 0.55 0.28 0.35 0.24 0.41 0.31 0.06 0.09 0.08 0.56 0.49
tfidf 0.45 0.47 0.26 0.30 0.20 0.28 0.22 0.14 0.12 0.16 0.37 0.27
tficf 0.45 0.49 0.27 0.33 0.20 0.29 0.24 0.13 0.11 0.09 0.37 0.28
okapi 0.37 0.42 0.33 0.37 0.18 0.27 0.26 0.26 0.17 0.12 0.34 0.20
atc 0.34 0.42 0.13 0.13 0.08 0.15 0.28 0.10 0.09 0.07 0.28 0.15
ltu 0.43 0.48 0.30 0.34 0.19 0.26 0.25 0.26 0.16 0.24 0.36 0.23
mi 0.51 0.53 0.18 0.51 0.16 0.28 0.37 0.18 0.10 0.09 0.12 nan
posmi 0.67 0.70 0.56 0.62 0.42 0.59 0.52 0.23 0.15 0.06 0.60 0.49
ttest 0.70 0.70 0.16 0.48 0.10 0.70 0.22 0.16 0.11 0.15 nan nan
chisquared 0.57 0.58 0.52 0.56 0.44 0.52 nan 0.08 0.06 0.10 0.63 0.60
lin98b 0.43 0.63 0.31 0.37 0.20 0.23 0.26 0.09 0.10 nan 0.34 0.24
gref94 0.48 0.54 0.27 0.33 0.20 0.17 0.23 0.13 0.11 0.09 0.38 0.25
</table>
<tableCaption confidence="0.999703">
Table 5: Aggregated scores for combinations of weighting schemes and similarity metrics using ukWaC
</tableCaption>
<bodyText confidence="0.999934615384615">
with lower-frequency ones, with optimal perfor-
mance comparable to when weighting is used.
There are some scenarios where it may be ad-
vantageous not to use weighting, for example in
an online setting where the total set of vectors is
not fixed; in situations where use of a dimension-
ality reduction technique does not directly allow
for weighting, such as random indexing (Sahlgren,
2006); as well as in settings where calculating
weights is too expensive. Where to stop the slid-
ing window varies with the datasets, however, and
so our conclusion is that the default scheme should
be weighting plus high frequency contexts.
</bodyText>
<sectionHeader confidence="0.995131" genericHeader="method">
4 Compositionality
</sectionHeader>
<bodyText confidence="0.988024266666667">
In order to examine whether optimal parame-
ters carry over to vectors that are combined into
phrasal vectors using a composition operator, we
perform a subset of our experiments on the canoni-
cal compositionality dataset from Mitchell and La-
pata (2010), using vector addition and pointwise
multiplication (the best performing operators in
the original study).
We evaluate using two source corpora (the BNC
and ukWaC) and two window sizes (small, with
a window size of 3; and big, where the full sen-
tence is the window). In addition to the weight-
ing schemes from the previous experiment, we in-
clude Mitchell &amp; Lapata’s own weighting scheme,
which (in our notation) is defined as wi j - fz, ×N
</bodyText>
<equation confidence="0.876495">
z� - fz×f, .
</equation>
<bodyText confidence="0.999833928571428">
While all weighting schemes and similarity met-
rics were tested, we report only the best perform-
ing ones: correlations below 0.5 were ommitted
for the sake of brevity. Table 7 shows the results.
We find that many of our findings continue to
hold. PosMI and T-Test are the best performing
weighting schemes, together with Mitchell &amp; La-
pata’s own weighting scheme. We find that ad-
dition outperforms multiplication (contrary to the
original study) and that small window sizes work
best, except in the VO case. Performance across
corpora is comparable. The best performing simi-
larity metrics are Cosine and Correlation, with the
latter having a slight edge over the former.
</bodyText>
<page confidence="0.997109">
27
</page>
<table confidence="0.9984517">
BNC - Small window
AN NN VO ALL
add-posmi-cosine 0.57 0.56 0.52 0.55
add-posmi-correlation 0.66 0.60 0.53 0.60
add-ttest-cosine 0.59 0.54 0.53 0.56
add-ttest-correlation 0.60 0.54 0.53 0.56
add-mila-correlation 0.64 0.38 0.51 0.51
ukWaC - Small window
AN NN VO ALL
add-posmi-correlation 0.64 0.59 0.56 0.59
add-ttest-cosine 0.61 0.55 0.53 0.56
add-ttest-correlation 0.61 0.55 0.53 0.56
add-mila-correlation 0.64 0.48 0.57 0.56
mult-mila-correlation 0.52 0.44 0.63 0.53
BNC - Large window
AN NN VO ALL
add-posmi-correlation 0.47 0.49 0.57 0.51
add-ttest-cosine 0.50 0.53 0.60 0.54
add-ttest-correlation 0.50 0.53 0.60 0.54
add-mila-correlation 0.51 0.49 0.61 0.54
mult-posmi-correlation 0.48 0.48 0.66 0.54
mult-mila-correlation 0.53 0.51 0.67 0.57
ukWaC - Large window
AN NN VO ALL
add-posmi-correlation 0.46 0.44 0.60 0.50
add-ttest-cosine 0.46 0.46 0.59 0.50
add-ttest-correlation 0.47 0.46 0.60 0.51
add-mila-correlation 0.47 0.46 0.64 0.52
mult-posmi-correlation 0.44 0.46 0.65 0.52
mult-mila-correlation 0.56 0.49 0.70 0.58
</table>
<tableCaption confidence="0.818336">
Table 7: Selected Spearman ρ scores on the
Mitchell &amp; Lapata 2010 compositionality dataset
</tableCaption>
<sectionHeader confidence="0.996458" genericHeader="conclusions">
5 Conclusion
</sectionHeader>
<bodyText confidence="0.999935761904762">
Our experiments were designed to investigate a
wide range of VSM parameters, using a variety
of evaluation tasks and several source corpora.
Across each of the experiments, results are com-
petitive with the state of the art. Some important
messages can be taken away from this study:
Experiment 1 Larger vectors do not always lead
to better performance. As vector size increases,
performance stabilises, and a vector size of around
50,000 appears to be optimal.
Experiment 2 The size of the window has a
clear impact on performance: a large corpus with
a small window size performs best, but high per-
formance can be achieved on a small subspaced
corpus, if the window size is large.
Experiment 3 The size of the source corpus
is more important than whether the model is
window- or dependency-based. Window-based
methods with a window size of 3 yield better re-
sults than dependency-based methods with a win-
dow of 3 (i.e. having a single arc). The Google
Syntactic N-gram corpus yields very good perfor-
mance, but it is unclear whether this is due to being
dependency-based or being very large.
Experiment 4 The granularity of the context
words has a relatively low impact on performance,
but stemming yields the best results.
Experiment 5 The optimal combination of
weighting scheme and similarity metric is Posi-
tive Mutual Information with a mean-adjusted ver-
sion of Cosine that we have called Correlation.
Another high-performing weighting scheme is T-
Test, which works better for smaller vector sizes.
The Correlation similarity metric consistently out-
performs Cosine, and we recommend its use.
Experiment 6 Use of a weighting scheme ob-
viates the need for removing high-frequency fea-
tures. Without weighting, many of the high-
frequency features should be removed. However,
if weighting is an option we recommend its use.
Compositionality The best parameters for
individual vectors generally carry over to a com-
positional similarity task where phrasal similarity
is evaluated by combining vectors into phrasal
vectors.
Furthermore, we observe that in general perfor-
mance increases as source corpus size increases,
so we recommend using a corpus such as ukWaC
over smaller corpora like the BNC. Likewise,
since the MEN dataset is the largest similarity
dataset available and mirrors our aggregate score
the best across the various experiments, we rec-
ommend evaluating on that similarity task if only
a single dataset is used for evaluation.
Obvious extensions include an analysis of the
performance of the various dimensionality reduc-
tion techniques, examining the importance of win-
dow size and feature granularity for dependency-
based methods, and further exploring the relation
between the size and frequency distribution of a
corpus together with the optimal characteristics
(such as the high-frequency cut-off point) of vec-
tors generated from that source.
</bodyText>
<sectionHeader confidence="0.998296" genericHeader="acknowledgments">
Acknowledgments
</sectionHeader>
<bodyText confidence="0.99764575">
This work has been supported by EPSRC grant
EP/I037512/1. We would like to thank Laura
Rimell, Tamara Polajnar and Felix Hill for help-
ful comments and suggestions.
</bodyText>
<page confidence="0.997649">
28
</page>
<sectionHeader confidence="0.985424" genericHeader="references">
References
</sectionHeader>
<reference confidence="0.999341198113207">
Marco Baroni, Silvia Bernardini, Adriano Ferraresi,
and Eros Zanchetta. 2009. The WaCky Wide Web:
A collection of very large linguistically processed
Web-crawled corpora. Language Resources and
Evaluation, 43(3):209–226.
Marco Baroni, Raffaella Bernardi, and Roberto Zam-
parelli. 2013. Frege in Space: A program for com-
positional distributional semantics. Linguistic Is-
sues in Language Technologies (LiLT).
Steven Bird, Ewan Klein, and Edward Loper.
2009. Natural Language Processing with Python.
O’Reilly Media.
William Blacoe and Mirella Lapata. 2012. A Com-
parison of Vector-based Representations for Seman-
tic Composition. In Proceedings of the 2012 Joint
Conference on Empirical Methods in Natural Lan-
guage Processing and Computational Natural Lan-
guage Learning, pages 546–556, Jeju Island, Korea,
July. Association for Computational Linguistics.
Elia Bruni, Gemma Boleda, Marco Baroni, and N. K.
Tran. 2012. Distributional Semantics in Techni-
color. In Proceedings of the ACL 2012.
John A. Bullinaria and Joseph P. Levy. 2007. Ex-
tracting Semantic Representations from Word Co-
occurrence Statistics: A computational study. Be-
havior Research Methods, 39:510–526.
John A. Bullinaria and Joseph P. Levy. 2012. Ex-
tracting Semantic Representations from Word Co-
occurrence Statistics: Stop-lists, Stemming and
SVD. Behavior Research Methods, 44:890–907.
L. Burnard. 2007. Reference Guide
for the British National Corpus.
http://www.natcorp.ox.ac.uk/docs/URG/.
Stephen Clark and James R. Curran. 2007. Wide-
Coverage Efficient Statistical Parsing with CCG and
Log-Linear Models. Computational Linguistics,
33(4):493—552.
Stephen Clark. 2014. Vector Space Models of Lexical
Meaning (to appear). In Shalom Lappin and Chris
Fox, editors, Handbook of Contemporary Semantics.
Wiley-Blackwell, Oxford.
James R. Curran and Marc Moens. 2002a. Improve-
ments in Automatic Thesaurus Extraction. In Pro-
ceedings of the ACL-02 workshop on Unsupervised
lexical acquisition-Volume 9, pages 59–66. Associa-
tion for Computational Linguistics.
James R. Curran and Marc Moens. 2002b. Scaling
Context Space. In Proceedings of the 40th Annual
Meeting on Association for Computational Linguis-
tics, pages 231–238. Association for Computational
Linguistics.
James R. Curran. 2004. From Distributional to Seman-
tic Similarity. Ph.D. thesis, University of Edinburgh.
Katrin Erk. 2012. Vector Space Models of Word
Meaning and Phrase Meaning: A Survey. Language
and Linguistics Compass, 6(10):635–653.
Lev Finkelstein, Evgeniy Gabrilovich, Yossi Matias,
Ehud Rivlin, Zach Solan, Gadi Wolfman, and Ey-
tan Ruppin. 2002. Placing Search in Context: The
Concept Revisited. ACM Transactions on Informa-
tion Systems, 20(1):116—131.
Yoav Goldberg and Jon Orwant. 2013. A Dataset
of Syntactic-Ngrams over Time from a Very Large
Corpus of English Books. In Second Joint Con-
ference on Lexical and Computational Semantics
(*SEM), Volume 1: Proceedings of the Main Con-
ference and the Shared Task: Semantic Textual Simi-
larity, pages 241–247, Atlanta, Georgia, USA, June.
Association for Computational Linguistics.
Gregory Grefenstette. 1994. Explorations in Auto-
matic Thesaurus Discovery. Kluwer Academic Pub-
lishers, Norwell, MA, USA.
Z. Harris. 1954. Distributional Structure. Word,
10(23):146—162.
F. Hill, D. Kiela, and A. Korhonen. 2013. Con-
creteness and Corpora: A Theoretical and Practical
Analysis. In Proceedings of ACL 2013, Workshop
on Cognitive Modelling and Computational Linguis-
tics, Sofia, Bulgaria.
Thomas K. Landauer and Susan T. Dumais. 1997.
A solution to Platos problem: The latent semantic
analysis theory of acquisition, induction, and rep-
resentation of knowledge. Psychological Review,
104(2):211–240.
Gabriella Lapesa and Stefan Evert. 2013. Evaluat-
ing neighbor rank and distance measures as predic-
tors of semantic priming. In In Proceedings of the
ACL Workshop on Cognitive Modeling and Compu-
tational Linguistics (CMCL 2013), Sofia, Bulgaria.
G.A. Miller and W.G. Charles. 1991. Contextual Cor-
relates of Semantic Similarity. Language and Cog-
nitive Processes, 6(1):1—28.
Jeff Mitchell and Mirella Lapata. 2010. Composition
in Distributional Models of Semantics. Cognitive
Science, 34(8):1388–1429.
Sebastian Pado and Mirella Lapata. 2007.
Dependency-based Construction of Semantic
Space Models. Computational Linguistics,
33(2):161–199.
Douglas L. T. Rohde, Laura M. Gonnerman, and
David C. Plaut. 2006. An Improved Model of Se-
mantic Similarity based on Lexical Co-occurence.
Communciations of the ACM, 8:627–633.
Herbert Rubenstein and John B. Goodenough. 1965.
Contextual Correlates of Synonymy. Commun.
ACM, 8(10):627–633, October.
</reference>
<page confidence="0.973968">
29
</page>
<reference confidence="0.999202527777778">
Magnus Sahlgren. 2006. The Word-Space Model: Us-
ing distributional analysis to represent syntagmatic
and paradigmatic relations between words in high-
dimensional vector spaces. Ph.D. thesis, Depart-
ment of Linguistics, Stockholm University.
Sabine Schulte im Walde, Stefan M¨uller, and Stephen
Roller. 2013. Exploring Vector Space Models to
Predict the Compositionality of German Noun-Noun
Compounds. In Proceedings of the 2nd Joint Con-
ference on Lexical and Computational Semantics,
pages 255–265, Atlanta, GA.
N. Clayton Silver and William P. Dunlap. 1987. Av-
eraging Correlation Coefficients: Should Fisher’s z
Transformation Be Used? Journal of Applied Psy-
chology, 72(1):146–148, February.
Mark Steedman. 2000. The Syntactic Process. MIT
Press, Cambridge, MA, USA.
Benjamin P. Stone, Simon J. Dennis, and Peter J.
Kwantes. 2008. A Systematic Comparison of Se-
mantic Models on Human Similarity Rating Data:
The Effectiveness of Subspacing. In The Proceed-
ings of the Thirtieth Conference of the Cognitive Sci-
ence Society.
Peter D. Turney and Patrick Pantel. 2010. From Fre-
quency to Meaning: vector space models of seman-
tics. J. Artif. Int. Res., 37(1):141–188, January.
Julie Weeds, David Weir, and Diana McCarthy. 2004.
Characterising Measures of Lexical Distributional
Similarity. In Proceedings of Coling 2004, pages
1015–1021, Geneva, Switzerland, Aug 23–Aug 27.
COLING.
S. Zelikovitz and M. Kogan. 2006. Using Web
Searches on Important Words to create Background
Sets for LSI Classification. In In Proceedings of
the 19th International FLAIRS Conference, pages
598—603, Menlo Park, CA. AAAI Press.
</reference>
<page confidence="0.998812">
30
</page>
</variant>
</algorithm>
<algorithm name="ParsHed" version="110505">
<variant no="0" confidence="0.605041">
<title confidence="0.999875">A Systematic Study of Semantic Vector Space Model Parameters</title>
<author confidence="0.996911">Douwe Kiela Stephen Clark</author>
<affiliation confidence="0.999764">University of Cambridge University of Cambridge Computer Laboratory Computer Laboratory</affiliation>
<email confidence="0.662076">douwe.kiela@cl.cam.ac.uksc609@cam.ac.uk</email>
<abstract confidence="0.992457636363636">We present a systematic study of parameters used in the construction of semantic vector space models. Evaluation is carried out on a variety of similarity tasks, including a compositionality dataset, using several source corpora. In addition to recommendations for optimal parameters, we present some novel findings, including a similarity metric that outperforms the alternatives on all tasks considered.</abstract>
</variant>
</algorithm>
<algorithm name="ParsCit" version="110505">
<citationList>
<citation valid="true">
<authors>
<author>Marco Baroni</author>
<author>Silvia Bernardini</author>
<author>Adriano Ferraresi</author>
<author>Eros Zanchetta</author>
</authors>
<title>The WaCky Wide Web: A collection of very large linguistically processed Web-crawled corpora. Language Resources and Evaluation,</title>
<date>2009</date>
<pages>43--3</pages>
<contexts>
<context position="4935" citStr="Baroni et al., 2009" startWordPosition="782" endWordPosition="785">TOEFL, which consists of multiple choice questions where the task is to select the correct synonym for a target word. In Section 4 we examine our parameters in the context of distributional compositional semantics, using the evaluation dataset from Mitchell and Lapata (2010). Table 1 gives statistics for the number of words and word pairings in each of the datasets. As well as using a variety of datasets, we also consider three different corpora from which to build the vectors, varying in size and domain. These include the BNC (Burnard, 2007) (106 word types, 108 tokens) and the larger ukWaC (Baroni et al., 2009) (107 types, 109 tokens). We also include a sub-spaced Wikipedia corpus (Stone et al., 2008): for all words in the evaluation datasets, we build a subcorpus by querying the top 10-ranked Wikipedia documents using the words as search terms, resulting in a corpus with 106 word types and 107 tokens. For examining the dependency-based contexts, we include the Google Syntactic N-gram corpus (Goldberg and Orwant, 2013), with 107 types and 1011 tokens. 2.1 Parameters We selected the following set of parameters for investigation, all of which are fundamental to vector space model construction1. 1Anoth</context>
</contexts>
<marker>Baroni, Bernardini, Ferraresi, Zanchetta, 2009</marker>
<rawString>Marco Baroni, Silvia Bernardini, Adriano Ferraresi, and Eros Zanchetta. 2009. The WaCky Wide Web: A collection of very large linguistically processed Web-crawled corpora. Language Resources and Evaluation, 43(3):209–226.</rawString>
</citation>
<citation valid="true">
<authors>
<author>Marco Baroni</author>
<author>Raffaella Bernardi</author>
<author>Roberto Zamparelli</author>
</authors>
<title>Frege in Space: A program for compositional distributional semantics. Linguistic Issues in Language Technologies (LiLT).</title>
<date>2013</date>
<contexts>
<context position="3242" citStr="Baroni et al., 2013" startWordPosition="497" endWordPosition="500">ion tasks. The most comprehensive existing studies of VSM parameters — encompassing window sizes, feature granularity, stopwords and dimensionality reduction — are by Bullinaria and Levy (2007; 2012) and Lapesa and Evert (2013). Section 2 introduces the various parameters of vector space model construction. We then attempt, in Section 3, to answer some of the fundamental questions for building VSMs through a number of experiments that consider each of the selected parameters. In Section 4 we examine how these findings relate to the recent development of distributional compositional semantics (Baroni et al., 2013; Clark, 2014), where vectors for words are combined into vectors for phrases. 2 Data and Parameters Two datasets have dominated the literature with respect to VSM parameters: WordSim353 (Finkelstein et al., 2002) and the TOEFL synonym dataset 21 Proceedings of the 2nd Workshop on Continuous Vector Space Models and their Compositionality (CVSC) @ EACL 2014, pages 21–30, Gothenburg, Sweden, April 26-30 2014. c�2014 Association for Computational Linguistics Dataset Pairings Words RG 65 48 MC 30 39 W353 353 437 MEN 3000 751 TOEFL 80 400 M&amp;L10 324 314 Table 1: Datasets for evaluation (Landauer and</context>
</contexts>
<marker>Baroni, Bernardi, Zamparelli, 2013</marker>
<rawString>Marco Baroni, Raffaella Bernardi, and Roberto Zamparelli. 2013. Frege in Space: A program for compositional distributional semantics. Linguistic Issues in Language Technologies (LiLT).</rawString>
</citation>
<citation valid="true">
<authors>
<author>Steven Bird</author>
<author>Ewan Klein</author>
<author>Edward Loper</author>
</authors>
<date>2009</date>
<booktitle>Natural Language Processing with Python. O’Reilly Media.</booktitle>
<contexts>
<context position="7956" citStr="Bird et al., 2009" startWordPosition="1276" endWordPosition="1279">xts that are more indicative of the meaning of the target word: the fact that cat cooccurs with purr is much more informative than its co-occurrence with the. Table 3 gives definitions of the weighting schemes considered. Stopwords, high frequency cut-off Function words and stopwords are often considered too uninformative to be suitable context words. Ignoring them not only leads to a reduction in model size and computational effort, but also to a more informative distributional vector. Hence we followed standard practice and did not use stopwords as context words (using the stoplist in NLTK (Bird et al., 2009)). The question we investigated is 2We will use the term “feature” or “context” or “context word” to refer to contextual elements. 22 Table 2: Similarity measures between vectors v and u, where vi is the ith component of v Measure Definition Jensen-Shannon Div α-skew 2 (D(u ||u+v 1 2 )+D(v ||u+v 2 )) √2 log 2 1 − D(u||αv+(1−α)u) √2 log 2 1 Euclidean Cityblock Chebyshev Cosine Correlation Dice Jaccard Jaccard2 Lin Tanimoto 1 1+√Pn i=1(ui−vi)2 1 1+Pni=1 |ui−vi| 1 1+maxi |ui−vi| u·v |u||v| (u−µu)·(v−µv) |u||v| 2 Pn Pni=0 min(ui,vi) i=0 ui+vi u·v Pni=0 ui+vi Pn Pn i=0 min(ui,vi) i=0 max(ui,vi) Pni</context>
</contexts>
<marker>Bird, Klein, Loper, 2009</marker>
<rawString>Steven Bird, Ewan Klein, and Edward Loper. 2009. Natural Language Processing with Python. O’Reilly Media.</rawString>
</citation>
<citation valid="true">
<authors>
<author>William Blacoe</author>
<author>Mirella Lapata</author>
</authors>
<title>A Comparison of Vector-based Representations for Semantic Composition.</title>
<date>2012</date>
<booktitle>In Proceedings of the 2012 Joint Conference on Empirical Methods in Natural Language Processing and Computational Natural Language Learning,</booktitle>
<pages>546--556</pages>
<institution>Jeju Island, Korea, July. Association for Computational Linguistics.</institution>
<contexts>
<context position="2553" citStr="Blacoe and Lapata (2012)" startWordPosition="391" endWordPosition="394">f parameters, and using a limited set of source corpora and tasks for evaluation (Curran and Moens, 2002a; Curran and Moens, 2002b; Curran, 2004; Grefenstette, 1994; Pado and Lapata, 2007; Sahlgren, 2006; Turney and Pantel, 2010; Schulte im Walde et al., 2013). Rohde et al. (2006) considered several weighting schemes for a large variety of tasks, while Weeds et al. (2004) did the same for similarity metrics. Stone et al. (2008) investigated the effectiveness of sub-spacing corpora, where a larger corpus is queried in order to construct a smaller sub-spaced corpus (Zelikovitz and Kogan, 2006). Blacoe and Lapata (2012) compare several types of vector representations for semantic composition tasks. The most comprehensive existing studies of VSM parameters — encompassing window sizes, feature granularity, stopwords and dimensionality reduction — are by Bullinaria and Levy (2007; 2012) and Lapesa and Evert (2013). Section 2 introduces the various parameters of vector space model construction. We then attempt, in Section 3, to answer some of the fundamental questions for building VSMs through a number of experiments that consider each of the selected parameters. In Section 4 we examine how these findings relate</context>
</contexts>
<marker>Blacoe, Lapata, 2012</marker>
<rawString>William Blacoe and Mirella Lapata. 2012. A Comparison of Vector-based Representations for Semantic Composition. In Proceedings of the 2012 Joint Conference on Empirical Methods in Natural Language Processing and Computational Natural Language Learning, pages 546–556, Jeju Island, Korea, July. Association for Computational Linguistics.</rawString>
</citation>
<citation valid="true">
<authors>
<author>Elia Bruni</author>
<author>Gemma Boleda</author>
<author>Marco Baroni</author>
<author>N K Tran</author>
</authors>
<title>Distributional Semantics in Technicolor.</title>
<date>2012</date>
<booktitle>In Proceedings of the ACL</booktitle>
<contexts>
<context position="4232" citStr="Bruni et al., 2012" startWordPosition="661" endWordPosition="664">, Gothenburg, Sweden, April 26-30 2014. c�2014 Association for Computational Linguistics Dataset Pairings Words RG 65 48 MC 30 39 W353 353 437 MEN 3000 751 TOEFL 80 400 M&amp;L10 324 314 Table 1: Datasets for evaluation (Landauer and Dumais, 1997). There is a risk that semantic similarity studies have been overfitting to their idiosyncracies, so in this study we evaluate on a variety of datasets: in addition to WordSim353 (W353) and TOEFL, we also use the Rubenstein &amp; Goodenough (RG) (1965) and Miller &amp; Charles (MC) (1991) data, as well as a much larger set of similarity ratings: the MEN dataset (Bruni et al., 2012). All these datasets consist of human similarity ratings for word pairings, except TOEFL, which consists of multiple choice questions where the task is to select the correct synonym for a target word. In Section 4 we examine our parameters in the context of distributional compositional semantics, using the evaluation dataset from Mitchell and Lapata (2010). Table 1 gives statistics for the number of words and word pairings in each of the datasets. As well as using a variety of datasets, we also consider three different corpora from which to build the vectors, varying in size and domain. These </context>
</contexts>
<marker>Bruni, Boleda, Baroni, Tran, 2012</marker>
<rawString>Elia Bruni, Gemma Boleda, Marco Baroni, and N. K. Tran. 2012. Distributional Semantics in Technicolor. In Proceedings of the ACL 2012.</rawString>
</citation>
<citation valid="true">
<authors>
<author>John A Bullinaria</author>
<author>Joseph P Levy</author>
</authors>
<title>Extracting Semantic Representations from Word Cooccurrence Statistics: A computational study.</title>
<date>2007</date>
<journal>Behavior Research Methods,</journal>
<pages>39--510</pages>
<contexts>
<context position="2815" citStr="Bullinaria and Levy (2007" startWordPosition="429" endWordPosition="432"> Rohde et al. (2006) considered several weighting schemes for a large variety of tasks, while Weeds et al. (2004) did the same for similarity metrics. Stone et al. (2008) investigated the effectiveness of sub-spacing corpora, where a larger corpus is queried in order to construct a smaller sub-spaced corpus (Zelikovitz and Kogan, 2006). Blacoe and Lapata (2012) compare several types of vector representations for semantic composition tasks. The most comprehensive existing studies of VSM parameters — encompassing window sizes, feature granularity, stopwords and dimensionality reduction — are by Bullinaria and Levy (2007; 2012) and Lapesa and Evert (2013). Section 2 introduces the various parameters of vector space model construction. We then attempt, in Section 3, to answer some of the fundamental questions for building VSMs through a number of experiments that consider each of the selected parameters. In Section 4 we examine how these findings relate to the recent development of distributional compositional semantics (Baroni et al., 2013; Clark, 2014), where vectors for words are combined into vectors for phrases. 2 Data and Parameters Two datasets have dominated the literature with respect to VSM parameter</context>
</contexts>
<marker>Bullinaria, Levy, 2007</marker>
<rawString>John A. Bullinaria and Joseph P. Levy. 2007. Extracting Semantic Representations from Word Cooccurrence Statistics: A computational study. Behavior Research Methods, 39:510–526.</rawString>
</citation>
<citation valid="true">
<authors>
<author>John A Bullinaria</author>
<author>Joseph P Levy</author>
</authors>
<title>Extracting Semantic Representations from Word Cooccurrence Statistics: Stop-lists, Stemming and SVD. Behavior Research Methods,</title>
<date>2012</date>
<pages>44--890</pages>
<contexts>
<context position="11447" citStr="Bullinaria and Levy, 2012" startWordPosition="1863" endWordPosition="1866">e investigate is vector size, measured by the number of features. Vectors are constructed from the BNC using a window-based method, with a window size of 5 (2 words either side of the target word). We experiment with vector sizes up to 0.5M features, which is close to the total number of context words present in the entire BNC according to our preprocessing scheme. Features are added according to frequency in the BNC, with increasingly more rare features being added. For weighting we consider both Positive Mutual Information and T-Test, which have been found to work best in previous research (Bullinaria and Levy, 2012; Curran, 2004). Similarity is computed using Cosine. 23 Figure 1: Impact of vector size on performance across different datasets The results in Figure 1 show a clear trend: for both weighting schemes, performance no longer improves after around 50,000 features; in fact, for T-test weighting, and some of the datasets, performance initially declines with an increase in features. Hence we conclude that continuing to add more rare features is detrimental to performance, and that 50,000 features or less will give good performance. An added benefit of smaller vectors is the reduction in computation</context>
<context position="19700" citStr="Bullinaria and Levy (2012)" startWordPosition="3198" endWordPosition="3201">ing, with cosine (COS) and correlation (COR) similarity three similarity metrics that perform particularly well: Cosine, Correlation and the Tanimoto coefficient (the latter also being similar to Cosine; see Table 2). The Correlation similarity metric has the most consistent performance across the different weighting schemes, and yields the highest score for both corpora. The most consistent weighting scheme across the two source corpora and similarity metrics appears to be PosMI. The highest combined aggregate score is that of PosMI with the Correlation metric, in line with the conclusion of Bullinaria and Levy (2012) that PosMI is the best weighting scheme5. However, for the large ukWaC corpus, T-Test achieves similarly high aggregate scores, in line with the work of Curran (2004). When we look at these two weighting schemes in more detail, we see that TTest works best for the RG and MC datasets, while PosMI works best for the others; see Table 6. Correlation is the best similarity metric in all cases. 5In some cases, the combination of weighting scheme and similarity metric results in a division by zero or leads to taking the logarithm of a negative number, in which cases we report the aggregate scores a</context>
</contexts>
<marker>Bullinaria, Levy, 2012</marker>
<rawString>John A. Bullinaria and Joseph P. Levy. 2012. Extracting Semantic Representations from Word Cooccurrence Statistics: Stop-lists, Stemming and SVD. Behavior Research Methods, 44:890–907.</rawString>
</citation>
<citation valid="true">
<authors>
<author>L Burnard</author>
</authors>
<title>Reference Guide for the British National Corpus.</title>
<date>2007</date>
<note>http://www.natcorp.ox.ac.uk/docs/URG/.</note>
<contexts>
<context position="4863" citStr="Burnard, 2007" startWordPosition="771" endWordPosition="772">ets consist of human similarity ratings for word pairings, except TOEFL, which consists of multiple choice questions where the task is to select the correct synonym for a target word. In Section 4 we examine our parameters in the context of distributional compositional semantics, using the evaluation dataset from Mitchell and Lapata (2010). Table 1 gives statistics for the number of words and word pairings in each of the datasets. As well as using a variety of datasets, we also consider three different corpora from which to build the vectors, varying in size and domain. These include the BNC (Burnard, 2007) (106 word types, 108 tokens) and the larger ukWaC (Baroni et al., 2009) (107 types, 109 tokens). We also include a sub-spaced Wikipedia corpus (Stone et al., 2008): for all words in the evaluation datasets, we build a subcorpus by querying the top 10-ranked Wikipedia documents using the words as search terms, resulting in a corpus with 106 word types and 107 tokens. For examining the dependency-based contexts, we include the Google Syntactic N-gram corpus (Goldberg and Orwant, 2013), with 107 types and 1011 tokens. 2.1 Parameters We selected the following set of parameters for investigation, </context>
</contexts>
<marker>Burnard, 2007</marker>
<rawString>L. Burnard. 2007. Reference Guide for the British National Corpus. http://www.natcorp.ox.ac.uk/docs/URG/.</rawString>
</citation>
<citation valid="true">
<authors>
<author>Stephen Clark</author>
<author>James R Curran</author>
</authors>
<title>WideCoverage Efficient Statistical Parsing with CCG and Log-Linear Models.</title>
<date>2007</date>
<journal>Computational Linguistics,</journal>
<volume>33</volume>
<issue>4</issue>
<contexts>
<context position="16429" citStr="Clark and Curran (2007)" startWordPosition="2679" endWordPosition="2682">. However, Google’s syntactic Ngram corpus does indeed outperform windowbased methods, even though smaller vectors were used for the Google models (10,000 vs. 50,000 features). We observe large variations across datasets, with window-based methods performing particularly well on some, but not all. In particular, window-based methods clearly outperform dependency-based methods on the RG dataset (for the same source corpus), whereas the opposite trend is observed for the TOEFL synonym dataset. The summary is that the model built from the syntactic N-grams is the overall winner, but when we 3The Clark and Curran (2007) parser was used to provide the dependencies. compare both methods on the same corpus, the window-based method on a large corpus appears to work best (given the small window size). 3.4 Feature granularity Stemming and lemmatisation are standard techniques in NLP and IR to reduce data sparsity. However, with large enough corpora it may be that the loss of information through generalisation hurts performance. In fact, it may be that increased granularity – through the use of grammatical tags – can lead to improved performance. We test these hypotheses by comparing four types of processed context</context>
</contexts>
<marker>Clark, Curran, 2007</marker>
<rawString>Stephen Clark and James R. Curran. 2007. WideCoverage Efficient Statistical Parsing with CCG and Log-Linear Models. Computational Linguistics, 33(4):493—552.</rawString>
</citation>
<citation valid="true">
<authors>
<author>Stephen Clark</author>
</authors>
<title>Vector Space Models of Lexical Meaning (to appear).</title>
<date>2014</date>
<booktitle>Handbook of Contemporary Semantics.</booktitle>
<editor>In Shalom Lappin and Chris Fox, editors,</editor>
<publisher>Wiley-Blackwell,</publisher>
<location>Oxford.</location>
<contexts>
<context position="3256" citStr="Clark, 2014" startWordPosition="501" endWordPosition="502">omprehensive existing studies of VSM parameters — encompassing window sizes, feature granularity, stopwords and dimensionality reduction — are by Bullinaria and Levy (2007; 2012) and Lapesa and Evert (2013). Section 2 introduces the various parameters of vector space model construction. We then attempt, in Section 3, to answer some of the fundamental questions for building VSMs through a number of experiments that consider each of the selected parameters. In Section 4 we examine how these findings relate to the recent development of distributional compositional semantics (Baroni et al., 2013; Clark, 2014), where vectors for words are combined into vectors for phrases. 2 Data and Parameters Two datasets have dominated the literature with respect to VSM parameters: WordSim353 (Finkelstein et al., 2002) and the TOEFL synonym dataset 21 Proceedings of the 2nd Workshop on Continuous Vector Space Models and their Compositionality (CVSC) @ EACL 2014, pages 21–30, Gothenburg, Sweden, April 26-30 2014. c�2014 Association for Computational Linguistics Dataset Pairings Words RG 65 48 MC 30 39 W353 353 437 MEN 3000 751 TOEFL 80 400 M&amp;L10 324 314 Table 1: Datasets for evaluation (Landauer and Dumais, 1997)</context>
</contexts>
<marker>Clark, 2014</marker>
<rawString>Stephen Clark. 2014. Vector Space Models of Lexical Meaning (to appear). In Shalom Lappin and Chris Fox, editors, Handbook of Contemporary Semantics. Wiley-Blackwell, Oxford.</rawString>
</citation>
<citation valid="true">
<authors>
<author>James R Curran</author>
<author>Marc Moens</author>
</authors>
<title>Improvements in Automatic Thesaurus Extraction.</title>
<date>2002</date>
<booktitle>In Proceedings of the ACL-02 workshop on Unsupervised lexical acquisition-Volume 9,</booktitle>
<pages>59--66</pages>
<publisher>Association</publisher>
<institution>for Computational Linguistics.</institution>
<contexts>
<context position="2033" citStr="Curran and Moens, 2002" startWordPosition="310" endWordPosition="313">metric; • weighting scheme; • stopwords and high frequency cut-off. A representative set of semantic similarity datasets has been selected from the literature, including a phrasal similarity dataset for evaluating compositionality. The choice of source corpus is likely to influence the quality of the VSM, and so we use a selection of source corpora. Hence there are two additional “superparameters”: • dataset for evaluation; • source corpus. Previous studies have been limited to investigating only a small number of parameters, and using a limited set of source corpora and tasks for evaluation (Curran and Moens, 2002a; Curran and Moens, 2002b; Curran, 2004; Grefenstette, 1994; Pado and Lapata, 2007; Sahlgren, 2006; Turney and Pantel, 2010; Schulte im Walde et al., 2013). Rohde et al. (2006) considered several weighting schemes for a large variety of tasks, while Weeds et al. (2004) did the same for similarity metrics. Stone et al. (2008) investigated the effectiveness of sub-spacing corpora, where a larger corpus is queried in order to construct a smaller sub-spaced corpus (Zelikovitz and Kogan, 2006). Blacoe and Lapata (2012) compare several types of vector representations for semantic composition tasks.</context>
</contexts>
<marker>Curran, Moens, 2002</marker>
<rawString>James R. Curran and Marc Moens. 2002a. Improvements in Automatic Thesaurus Extraction. In Proceedings of the ACL-02 workshop on Unsupervised lexical acquisition-Volume 9, pages 59–66. Association for Computational Linguistics.</rawString>
</citation>
<citation valid="true">
<authors>
<author>James R Curran</author>
<author>Marc Moens</author>
</authors>
<title>Scaling Context Space.</title>
<date>2002</date>
<booktitle>In Proceedings of the 40th Annual Meeting on Association for Computational Linguistics,</booktitle>
<pages>231--238</pages>
<publisher>Association</publisher>
<institution>for Computational Linguistics.</institution>
<contexts>
<context position="2033" citStr="Curran and Moens, 2002" startWordPosition="310" endWordPosition="313">metric; • weighting scheme; • stopwords and high frequency cut-off. A representative set of semantic similarity datasets has been selected from the literature, including a phrasal similarity dataset for evaluating compositionality. The choice of source corpus is likely to influence the quality of the VSM, and so we use a selection of source corpora. Hence there are two additional “superparameters”: • dataset for evaluation; • source corpus. Previous studies have been limited to investigating only a small number of parameters, and using a limited set of source corpora and tasks for evaluation (Curran and Moens, 2002a; Curran and Moens, 2002b; Curran, 2004; Grefenstette, 1994; Pado and Lapata, 2007; Sahlgren, 2006; Turney and Pantel, 2010; Schulte im Walde et al., 2013). Rohde et al. (2006) considered several weighting schemes for a large variety of tasks, while Weeds et al. (2004) did the same for similarity metrics. Stone et al. (2008) investigated the effectiveness of sub-spacing corpora, where a larger corpus is queried in order to construct a smaller sub-spaced corpus (Zelikovitz and Kogan, 2006). Blacoe and Lapata (2012) compare several types of vector representations for semantic composition tasks.</context>
</contexts>
<marker>Curran, Moens, 2002</marker>
<rawString>James R. Curran and Marc Moens. 2002b. Scaling Context Space. In Proceedings of the 40th Annual Meeting on Association for Computational Linguistics, pages 231–238. Association for Computational Linguistics.</rawString>
</citation>
<citation valid="true">
<authors>
<author>James R Curran</author>
</authors>
<title>From Distributional to Semantic Similarity.</title>
<date>2004</date>
<tech>Ph.D. thesis,</tech>
<institution>University of Edinburgh.</institution>
<contexts>
<context position="2073" citStr="Curran, 2004" startWordPosition="318" endWordPosition="319">requency cut-off. A representative set of semantic similarity datasets has been selected from the literature, including a phrasal similarity dataset for evaluating compositionality. The choice of source corpus is likely to influence the quality of the VSM, and so we use a selection of source corpora. Hence there are two additional “superparameters”: • dataset for evaluation; • source corpus. Previous studies have been limited to investigating only a small number of parameters, and using a limited set of source corpora and tasks for evaluation (Curran and Moens, 2002a; Curran and Moens, 2002b; Curran, 2004; Grefenstette, 1994; Pado and Lapata, 2007; Sahlgren, 2006; Turney and Pantel, 2010; Schulte im Walde et al., 2013). Rohde et al. (2006) considered several weighting schemes for a large variety of tasks, while Weeds et al. (2004) did the same for similarity metrics. Stone et al. (2008) investigated the effectiveness of sub-spacing corpora, where a larger corpus is queried in order to construct a smaller sub-spaced corpus (Zelikovitz and Kogan, 2006). Blacoe and Lapata (2012) compare several types of vector representations for semantic composition tasks. The most comprehensive existing studies</context>
<context position="8929" citStr="Curran, 2004" startWordPosition="1444" endWordPosition="1445"> Chebyshev Cosine Correlation Dice Jaccard Jaccard2 Lin Tanimoto 1 1+√Pn i=1(ui−vi)2 1 1+Pni=1 |ui−vi| 1 1+maxi |ui−vi| u·v |u||v| (u−µu)·(v−µv) |u||v| 2 Pn Pni=0 min(ui,vi) i=0 ui+vi u·v Pni=0 ui+vi Pn Pn i=0 min(ui,vi) i=0 max(ui,vi) Pni=0 ui+vi |u|+|v| u·v |u|+|v|−u·v Scheme Definition None TF-IDF TF-ICF wij = fij wij = log(fij) × log(njN) wij = log(fij) × log(fjN) Okapi BM25 fij wij = fj 0.5+1.5× fj N−nj+0.5 log fij+0.5 rPN i=1[(0.5+0.5× fij maxf ) log( nj N )]2 (log(fij)+1.0) log( nj N ) 0.8+0.2×fj×fj j P (tij|cj) wij = log P(tij)P(cj) max(0, MI) P(tij|cj)−P(tij)P(cj) √ P(tij)P(cj ) see (Curran, 2004, p. 83) fij×f wij = fi×fj wij = −1 × log nj N wij = wij = log fij+1 log nj +1 +fij j (0.5+0.5× fij maxf ) log( nj N ) wij = wij = ATC LTU MI PosMI T-Test x2 Lin98a Lin98b Gref94 whether removing more context words, based on a frequency cut-off, can improve performance. 3 Experiments The parameter space is too large to analyse exhaustively, and so we adopted a strategy for how to navigate through it, selecting certain parameters to investigate first, which then get fixed or “clamped” in the remaining experiments. Unless specified otherwise, vectors are generated with the following restrictions</context>
<context position="11462" citStr="Curran, 2004" startWordPosition="1867" endWordPosition="1868">e, measured by the number of features. Vectors are constructed from the BNC using a window-based method, with a window size of 5 (2 words either side of the target word). We experiment with vector sizes up to 0.5M features, which is close to the total number of context words present in the entire BNC according to our preprocessing scheme. Features are added according to frequency in the BNC, with increasingly more rare features being added. For weighting we consider both Positive Mutual Information and T-Test, which have been found to work best in previous research (Bullinaria and Levy, 2012; Curran, 2004). Similarity is computed using Cosine. 23 Figure 1: Impact of vector size on performance across different datasets The results in Figure 1 show a clear trend: for both weighting schemes, performance no longer improves after around 50,000 features; in fact, for T-test weighting, and some of the datasets, performance initially declines with an increase in features. Hence we conclude that continuing to add more rare features is detrimental to performance, and that 50,000 features or less will give good performance. An added benefit of smaller vectors is the reduction in computational cost. 3.2 Wi</context>
<context position="19867" citStr="Curran (2004)" startWordPosition="3228" endWordPosition="3229">eing similar to Cosine; see Table 2). The Correlation similarity metric has the most consistent performance across the different weighting schemes, and yields the highest score for both corpora. The most consistent weighting scheme across the two source corpora and similarity metrics appears to be PosMI. The highest combined aggregate score is that of PosMI with the Correlation metric, in line with the conclusion of Bullinaria and Levy (2012) that PosMI is the best weighting scheme5. However, for the large ukWaC corpus, T-Test achieves similarly high aggregate scores, in line with the work of Curran (2004). When we look at these two weighting schemes in more detail, we see that TTest works best for the RG and MC datasets, while PosMI works best for the others; see Table 6. Correlation is the best similarity metric in all cases. 5In some cases, the combination of weighting scheme and similarity metric results in a division by zero or leads to taking the logarithm of a negative number, in which cases we report the aggregate scores as nan (not-a-number). Figure 5: Finding the optimal “contiguous subvector” of size 10,000 3.6 Optimal subvector Stopwords are typically removed from vectors and not us</context>
</contexts>
<marker>Curran, 2004</marker>
<rawString>James R. Curran. 2004. From Distributional to Semantic Similarity. Ph.D. thesis, University of Edinburgh.</rawString>
</citation>
<citation valid="true">
<authors>
<author>Katrin Erk</author>
</authors>
<title>Vector Space Models of Word Meaning and Phrase Meaning: A Survey. Language and Linguistics Compass,</title>
<date>2012</date>
<pages>6--10</pages>
<contexts>
<context position="1070" citStr="Erk, 2012" startWordPosition="160" endWordPosition="161">dition to recommendations for optimal parameters, we present some novel findings, including a similarity metric that outperforms the alternatives on all tasks considered. 1 Introduction Vector space models (VSMs) represent the meanings of lexical items as vectors in a “semantic space”. The benefit of VSMs is that they can easily be manipulated using linear algebra, allowing a degree of similarity between vectors to be computed. They rely on the distributional hypothesis (Harris, 1954): the idea that “words that occur in similar contexts tend to have similar meanings” (Turney and Pantel, 2010; Erk, 2012). The construction of a suitable VSM for a particular task is highly parameterised, and there appears to be little consensus over which parameter settings to use. This paper presents a systematic study of the following parameters: • vector size; • window size; • window-based or dependency-based context; • feature granularity; • similarity metric; • weighting scheme; • stopwords and high frequency cut-off. A representative set of semantic similarity datasets has been selected from the literature, including a phrasal similarity dataset for evaluating compositionality. The choice of source corpus</context>
</contexts>
<marker>Erk, 2012</marker>
<rawString>Katrin Erk. 2012. Vector Space Models of Word Meaning and Phrase Meaning: A Survey. Language and Linguistics Compass, 6(10):635–653.</rawString>
</citation>
<citation valid="true">
<authors>
<author>Lev Finkelstein</author>
<author>Evgeniy Gabrilovich</author>
<author>Yossi Matias</author>
<author>Ehud Rivlin</author>
<author>Zach Solan</author>
<author>Gadi Wolfman</author>
<author>Eytan Ruppin</author>
</authors>
<title>Placing Search in Context: The Concept Revisited.</title>
<date>2002</date>
<journal>ACM Transactions on Information Systems,</journal>
<volume>20</volume>
<issue>1</issue>
<contexts>
<context position="3455" citStr="Finkelstein et al., 2002" startWordPosition="529" endWordPosition="533">esa and Evert (2013). Section 2 introduces the various parameters of vector space model construction. We then attempt, in Section 3, to answer some of the fundamental questions for building VSMs through a number of experiments that consider each of the selected parameters. In Section 4 we examine how these findings relate to the recent development of distributional compositional semantics (Baroni et al., 2013; Clark, 2014), where vectors for words are combined into vectors for phrases. 2 Data and Parameters Two datasets have dominated the literature with respect to VSM parameters: WordSim353 (Finkelstein et al., 2002) and the TOEFL synonym dataset 21 Proceedings of the 2nd Workshop on Continuous Vector Space Models and their Compositionality (CVSC) @ EACL 2014, pages 21–30, Gothenburg, Sweden, April 26-30 2014. c�2014 Association for Computational Linguistics Dataset Pairings Words RG 65 48 MC 30 39 W353 353 437 MEN 3000 751 TOEFL 80 400 M&amp;L10 324 314 Table 1: Datasets for evaluation (Landauer and Dumais, 1997). There is a risk that semantic similarity studies have been overfitting to their idiosyncracies, so in this study we evaluate on a variety of datasets: in addition to WordSim353 (W353) and TOEFL, we</context>
</contexts>
<marker>Finkelstein, Gabrilovich, Matias, Rivlin, Solan, Wolfman, Ruppin, 2002</marker>
<rawString>Lev Finkelstein, Evgeniy Gabrilovich, Yossi Matias, Ehud Rivlin, Zach Solan, Gadi Wolfman, and Eytan Ruppin. 2002. Placing Search in Context: The Concept Revisited. ACM Transactions on Information Systems, 20(1):116—131.</rawString>
</citation>
<citation valid="true">
<authors>
<author>Yoav Goldberg</author>
<author>Jon Orwant</author>
</authors>
<title>A Dataset of Syntactic-Ngrams over Time from a Very Large Corpus of English Books.</title>
<date>2013</date>
<booktitle>In Second Joint Conference on Lexical and Computational Semantics (*SEM), Volume 1: Proceedings of the Main Conference and the Shared Task: Semantic Textual Similarity,</booktitle>
<pages>241--247</pages>
<publisher>Association</publisher>
<institution>for Computational Linguistics.</institution>
<location>Atlanta, Georgia, USA,</location>
<contexts>
<context position="5351" citStr="Goldberg and Orwant, 2013" startWordPosition="851" endWordPosition="854">e also consider three different corpora from which to build the vectors, varying in size and domain. These include the BNC (Burnard, 2007) (106 word types, 108 tokens) and the larger ukWaC (Baroni et al., 2009) (107 types, 109 tokens). We also include a sub-spaced Wikipedia corpus (Stone et al., 2008): for all words in the evaluation datasets, we build a subcorpus by querying the top 10-ranked Wikipedia documents using the words as search terms, resulting in a corpus with 106 word types and 107 tokens. For examining the dependency-based contexts, we include the Google Syntactic N-gram corpus (Goldberg and Orwant, 2013), with 107 types and 1011 tokens. 2.1 Parameters We selected the following set of parameters for investigation, all of which are fundamental to vector space model construction1. 1Another obvious parameter would be dimensionality reduction, which we chose not to include because it does not represent a fundamental aspect of VSM construction: dimensionality reduction relies on some original non-reduced model, and directly depends on its quality. Vector size Each component of a vector represents a context (or perhaps more accurately a “contextual element”, such as second word to the left of the ta</context>
<context position="15180" citStr="Goldberg and Orwant, 2013" startWordPosition="2481" endWordPosition="2484">hat dependency-based contexts give better results. We also compare window-based and dependency-based models. Dependency-parsed versions of the BNC and ukWaC were used to construct syntacticallyinformed vectors, with a single, labelled arc be24 Figure 3: Window versus dependency contexts tween the target word and context word.3 Since this effectively provides a window size of 3, we also use a window size of 3 for the window-based method (which provided the best results in Section 3.2 with the ukWaC corpus). As well as the ukWaC and BNC source corpora, we use the Google syntactic N-gram corpus (Goldberg and Orwant, 2013), which is one of the largest corpora to date, and which consists of syntactic ngrams as opposed to window-based n-grams. We use vectors of size 50,000 with Positive Mutual Information weighting and Cosine similarity. Due to its size and associated computational cost, we used only 10,000 contexts for the vectors generated from the syntactic N-gram corpus. The results are shown in Figure 3. In contrast to the idea that dependency-based methods outperform window-based methods, we find that the window-based models outperform dependency-based models when they are constructed from the same corpus u</context>
</contexts>
<marker>Goldberg, Orwant, 2013</marker>
<rawString>Yoav Goldberg and Jon Orwant. 2013. A Dataset of Syntactic-Ngrams over Time from a Very Large Corpus of English Books. In Second Joint Conference on Lexical and Computational Semantics (*SEM), Volume 1: Proceedings of the Main Conference and the Shared Task: Semantic Textual Similarity, pages 241–247, Atlanta, Georgia, USA, June. Association for Computational Linguistics.</rawString>
</citation>
<citation valid="true">
<authors>
<author>Gregory Grefenstette</author>
</authors>
<title>Explorations in Automatic Thesaurus Discovery.</title>
<date>1994</date>
<publisher>Kluwer Academic Publishers,</publisher>
<location>Norwell, MA, USA.</location>
<contexts>
<context position="2093" citStr="Grefenstette, 1994" startWordPosition="320" endWordPosition="321">ff. A representative set of semantic similarity datasets has been selected from the literature, including a phrasal similarity dataset for evaluating compositionality. The choice of source corpus is likely to influence the quality of the VSM, and so we use a selection of source corpora. Hence there are two additional “superparameters”: • dataset for evaluation; • source corpus. Previous studies have been limited to investigating only a small number of parameters, and using a limited set of source corpora and tasks for evaluation (Curran and Moens, 2002a; Curran and Moens, 2002b; Curran, 2004; Grefenstette, 1994; Pado and Lapata, 2007; Sahlgren, 2006; Turney and Pantel, 2010; Schulte im Walde et al., 2013). Rohde et al. (2006) considered several weighting schemes for a large variety of tasks, while Weeds et al. (2004) did the same for similarity metrics. Stone et al. (2008) investigated the effectiveness of sub-spacing corpora, where a larger corpus is queried in order to construct a smaller sub-spaced corpus (Zelikovitz and Kogan, 2006). Blacoe and Lapata (2012) compare several types of vector representations for semantic composition tasks. The most comprehensive existing studies of VSM parameters —</context>
</contexts>
<marker>Grefenstette, 1994</marker>
<rawString>Gregory Grefenstette. 1994. Explorations in Automatic Thesaurus Discovery. Kluwer Academic Publishers, Norwell, MA, USA.</rawString>
</citation>
<citation valid="true">
<authors>
<author>Z Harris</author>
</authors>
<date>1954</date>
<journal>Distributional Structure. Word,</journal>
<volume>10</volume>
<issue>23</issue>
<contexts>
<context position="949" citStr="Harris, 1954" startWordPosition="140" endWordPosition="141">n is carried out on a variety of similarity tasks, including a compositionality dataset, using several source corpora. In addition to recommendations for optimal parameters, we present some novel findings, including a similarity metric that outperforms the alternatives on all tasks considered. 1 Introduction Vector space models (VSMs) represent the meanings of lexical items as vectors in a “semantic space”. The benefit of VSMs is that they can easily be manipulated using linear algebra, allowing a degree of similarity between vectors to be computed. They rely on the distributional hypothesis (Harris, 1954): the idea that “words that occur in similar contexts tend to have similar meanings” (Turney and Pantel, 2010; Erk, 2012). The construction of a suitable VSM for a particular task is highly parameterised, and there appears to be little consensus over which parameter settings to use. This paper presents a systematic study of the following parameters: • vector size; • window size; • window-based or dependency-based context; • feature granularity; • similarity metric; • weighting scheme; • stopwords and high frequency cut-off. A representative set of semantic similarity datasets has been selected</context>
</contexts>
<marker>Harris, 1954</marker>
<rawString>Z. Harris. 1954. Distributional Structure. Word, 10(23):146—162.</rawString>
</citation>
<citation valid="true">
<authors>
<author>F Hill</author>
<author>D Kiela</author>
<author>A Korhonen</author>
</authors>
<title>Concreteness and Corpora: A Theoretical and Practical Analysis.</title>
<date>2013</date>
<booktitle>In Proceedings of ACL 2013, Workshop on Cognitive Modelling and Computational Linguistics,</booktitle>
<location>Sofia, Bulgaria.</location>
<contexts>
<context position="12184" citStr="Hill et al. (2013)" startWordPosition="1985" endWordPosition="1988">t datasets The results in Figure 1 show a clear trend: for both weighting schemes, performance no longer improves after around 50,000 features; in fact, for T-test weighting, and some of the datasets, performance initially declines with an increase in features. Hence we conclude that continuing to add more rare features is detrimental to performance, and that 50,000 features or less will give good performance. An added benefit of smaller vectors is the reduction in computational cost. 3.2 Window size Recent studies have found that the best window size depends on the task at hand. For example, Hill et al. (2013) found that smaller windows work best for measuring similarity of concrete nouns, whereas larger window sizes work better for abstract nouns. Schulte im Walde et al. (2013) found that a large window size worked best for a compositionality dataset of German noun-noun compounds. Similar relations between window size and performance have been found for similar versus related words, as well as for similar versus associated words (Turney and Pantel, 2010). We experiment with window sizes of 3, 5, 7, 9 and a full sentence. (A window size of n implies n−1 2 words either side of the target word.) We u</context>
</contexts>
<marker>Hill, Kiela, Korhonen, 2013</marker>
<rawString>F. Hill, D. Kiela, and A. Korhonen. 2013. Concreteness and Corpora: A Theoretical and Practical Analysis. In Proceedings of ACL 2013, Workshop on Cognitive Modelling and Computational Linguistics, Sofia, Bulgaria.</rawString>
</citation>
<citation valid="true">
<authors>
<author>Thomas K Landauer</author>
<author>Susan T Dumais</author>
</authors>
<title>A solution to Platos problem: The latent semantic analysis theory of acquisition, induction, and representation of knowledge.</title>
<date>1997</date>
<journal>Psychological Review,</journal>
<volume>104</volume>
<issue>2</issue>
<contexts>
<context position="3856" citStr="Landauer and Dumais, 1997" startWordPosition="595" endWordPosition="598">et al., 2013; Clark, 2014), where vectors for words are combined into vectors for phrases. 2 Data and Parameters Two datasets have dominated the literature with respect to VSM parameters: WordSim353 (Finkelstein et al., 2002) and the TOEFL synonym dataset 21 Proceedings of the 2nd Workshop on Continuous Vector Space Models and their Compositionality (CVSC) @ EACL 2014, pages 21–30, Gothenburg, Sweden, April 26-30 2014. c�2014 Association for Computational Linguistics Dataset Pairings Words RG 65 48 MC 30 39 W353 353 437 MEN 3000 751 TOEFL 80 400 M&amp;L10 324 314 Table 1: Datasets for evaluation (Landauer and Dumais, 1997). There is a risk that semantic similarity studies have been overfitting to their idiosyncracies, so in this study we evaluate on a variety of datasets: in addition to WordSim353 (W353) and TOEFL, we also use the Rubenstein &amp; Goodenough (RG) (1965) and Miller &amp; Charles (MC) (1991) data, as well as a much larger set of similarity ratings: the MEN dataset (Bruni et al., 2012). All these datasets consist of human similarity ratings for word pairings, except TOEFL, which consists of multiple choice questions where the task is to select the correct synonym for a target word. In Section 4 we examine</context>
</contexts>
<marker>Landauer, Dumais, 1997</marker>
<rawString>Thomas K. Landauer and Susan T. Dumais. 1997. A solution to Platos problem: The latent semantic analysis theory of acquisition, induction, and representation of knowledge. Psychological Review, 104(2):211–240.</rawString>
</citation>
<citation valid="true">
<authors>
<author>Gabriella Lapesa</author>
<author>Stefan Evert</author>
</authors>
<title>Evaluating neighbor rank and distance measures as predictors of semantic priming. In</title>
<date>2013</date>
<booktitle>In Proceedings of the ACL Workshop on Cognitive Modeling and Computational Linguistics (CMCL 2013),</booktitle>
<location>Sofia, Bulgaria.</location>
<contexts>
<context position="2850" citStr="Lapesa and Evert (2013)" startWordPosition="435" endWordPosition="438">al weighting schemes for a large variety of tasks, while Weeds et al. (2004) did the same for similarity metrics. Stone et al. (2008) investigated the effectiveness of sub-spacing corpora, where a larger corpus is queried in order to construct a smaller sub-spaced corpus (Zelikovitz and Kogan, 2006). Blacoe and Lapata (2012) compare several types of vector representations for semantic composition tasks. The most comprehensive existing studies of VSM parameters — encompassing window sizes, feature granularity, stopwords and dimensionality reduction — are by Bullinaria and Levy (2007; 2012) and Lapesa and Evert (2013). Section 2 introduces the various parameters of vector space model construction. We then attempt, in Section 3, to answer some of the fundamental questions for building VSMs through a number of experiments that consider each of the selected parameters. In Section 4 we examine how these findings relate to the recent development of distributional compositional semantics (Baroni et al., 2013; Clark, 2014), where vectors for words are combined into vectors for phrases. 2 Data and Parameters Two datasets have dominated the literature with respect to VSM parameters: WordSim353 (Finkelstein et al., </context>
</contexts>
<marker>Lapesa, Evert, 2013</marker>
<rawString>Gabriella Lapesa and Stefan Evert. 2013. Evaluating neighbor rank and distance measures as predictors of semantic priming. In In Proceedings of the ACL Workshop on Cognitive Modeling and Computational Linguistics (CMCL 2013), Sofia, Bulgaria.</rawString>
</citation>
<citation valid="true">
<authors>
<author>G A Miller</author>
<author>W G Charles</author>
</authors>
<date>1991</date>
<booktitle>Contextual Correlates of Semantic Similarity. Language and Cognitive Processes,</booktitle>
<pages>6--1</pages>
<marker>Miller, Charles, 1991</marker>
<rawString>G.A. Miller and W.G. Charles. 1991. Contextual Correlates of Semantic Similarity. Language and Cognitive Processes, 6(1):1—28.</rawString>
</citation>
<citation valid="true">
<authors>
<author>Jeff Mitchell</author>
<author>Mirella Lapata</author>
</authors>
<title>Composition in Distributional Models of Semantics.</title>
<date>2010</date>
<journal>Cognitive Science,</journal>
<volume>34</volume>
<issue>8</issue>
<contexts>
<context position="4590" citStr="Mitchell and Lapata (2010)" startWordPosition="720" endWordPosition="723"> study we evaluate on a variety of datasets: in addition to WordSim353 (W353) and TOEFL, we also use the Rubenstein &amp; Goodenough (RG) (1965) and Miller &amp; Charles (MC) (1991) data, as well as a much larger set of similarity ratings: the MEN dataset (Bruni et al., 2012). All these datasets consist of human similarity ratings for word pairings, except TOEFL, which consists of multiple choice questions where the task is to select the correct synonym for a target word. In Section 4 we examine our parameters in the context of distributional compositional semantics, using the evaluation dataset from Mitchell and Lapata (2010). Table 1 gives statistics for the number of words and word pairings in each of the datasets. As well as using a variety of datasets, we also consider three different corpora from which to build the vectors, varying in size and domain. These include the BNC (Burnard, 2007) (106 word types, 108 tokens) and the larger ukWaC (Baroni et al., 2009) (107 types, 109 tokens). We also include a sub-spaced Wikipedia corpus (Stone et al., 2008): for all words in the evaluation datasets, we build a subcorpus by querying the top 10-ranked Wikipedia documents using the words as search terms, resulting in a </context>
<context position="25003" citStr="Mitchell and Lapata (2010)" startWordPosition="4128" endWordPosition="4132">e of a dimensionality reduction technique does not directly allow for weighting, such as random indexing (Sahlgren, 2006); as well as in settings where calculating weights is too expensive. Where to stop the sliding window varies with the datasets, however, and so our conclusion is that the default scheme should be weighting plus high frequency contexts. 4 Compositionality In order to examine whether optimal parameters carry over to vectors that are combined into phrasal vectors using a composition operator, we perform a subset of our experiments on the canonical compositionality dataset from Mitchell and Lapata (2010), using vector addition and pointwise multiplication (the best performing operators in the original study). We evaluate using two source corpora (the BNC and ukWaC) and two window sizes (small, with a window size of 3; and big, where the full sentence is the window). In addition to the weighting schemes from the previous experiment, we include Mitchell &amp; Lapata’s own weighting scheme, which (in our notation) is defined as wi j - fz, ×N z� - fz×f, . While all weighting schemes and similarity metrics were tested, we report only the best performing ones: correlations below 0.5 were ommitted for t</context>
<context position="27222" citStr="Mitchell &amp; Lapata 2010" startWordPosition="4475" endWordPosition="4478"> - Large window AN NN VO ALL add-posmi-correlation 0.47 0.49 0.57 0.51 add-ttest-cosine 0.50 0.53 0.60 0.54 add-ttest-correlation 0.50 0.53 0.60 0.54 add-mila-correlation 0.51 0.49 0.61 0.54 mult-posmi-correlation 0.48 0.48 0.66 0.54 mult-mila-correlation 0.53 0.51 0.67 0.57 ukWaC - Large window AN NN VO ALL add-posmi-correlation 0.46 0.44 0.60 0.50 add-ttest-cosine 0.46 0.46 0.59 0.50 add-ttest-correlation 0.47 0.46 0.60 0.51 add-mila-correlation 0.47 0.46 0.64 0.52 mult-posmi-correlation 0.44 0.46 0.65 0.52 mult-mila-correlation 0.56 0.49 0.70 0.58 Table 7: Selected Spearman ρ scores on the Mitchell &amp; Lapata 2010 compositionality dataset 5 Conclusion Our experiments were designed to investigate a wide range of VSM parameters, using a variety of evaluation tasks and several source corpora. Across each of the experiments, results are competitive with the state of the art. Some important messages can be taken away from this study: Experiment 1 Larger vectors do not always lead to better performance. As vector size increases, performance stabilises, and a vector size of around 50,000 appears to be optimal. Experiment 2 The size of the window has a clear impact on performance: a large corpus with a small w</context>
</contexts>
<marker>Mitchell, Lapata, 2010</marker>
<rawString>Jeff Mitchell and Mirella Lapata. 2010. Composition in Distributional Models of Semantics. Cognitive Science, 34(8):1388–1429.</rawString>
</citation>
<citation valid="true">
<authors>
<author>Sebastian Pado</author>
<author>Mirella Lapata</author>
</authors>
<title>Dependency-based Construction of Semantic Space Models.</title>
<date>2007</date>
<journal>Computational Linguistics,</journal>
<volume>33</volume>
<issue>2</issue>
<contexts>
<context position="2116" citStr="Pado and Lapata, 2007" startWordPosition="322" endWordPosition="325"> set of semantic similarity datasets has been selected from the literature, including a phrasal similarity dataset for evaluating compositionality. The choice of source corpus is likely to influence the quality of the VSM, and so we use a selection of source corpora. Hence there are two additional “superparameters”: • dataset for evaluation; • source corpus. Previous studies have been limited to investigating only a small number of parameters, and using a limited set of source corpora and tasks for evaluation (Curran and Moens, 2002a; Curran and Moens, 2002b; Curran, 2004; Grefenstette, 1994; Pado and Lapata, 2007; Sahlgren, 2006; Turney and Pantel, 2010; Schulte im Walde et al., 2013). Rohde et al. (2006) considered several weighting schemes for a large variety of tasks, while Weeds et al. (2004) did the same for similarity metrics. Stone et al. (2008) investigated the effectiveness of sub-spacing corpora, where a larger corpus is queried in order to construct a smaller sub-spaced corpus (Zelikovitz and Kogan, 2006). Blacoe and Lapata (2012) compare several types of vector representations for semantic composition tasks. The most comprehensive existing studies of VSM parameters — encompassing window si</context>
<context position="14470" citStr="Pado and Lapata (2007)" startWordPosition="2370" endWordPosition="2373">o Stone et al. (2008), which performs much better with larger window sizes than the BNC or ukWaC. Our explanation for this result is that sub-spacing, resulting from searching for Wikipedia pages with the appropriate target terms, provides a focused, less noisy corpus in which context words some distance from the target word are still relevant to its meaning. In summary, the highest score is typically achieved with the largest source corpora and smallest window size, with the exception of the much smaller sub-spaced Wikipedia corpus. 3.3 Context The notion of context plays a key role in VSMs. Pado and Lapata (2007) present a comparison of window-based versus dependency-based methods and conclude that dependency-based contexts give better results. We also compare window-based and dependency-based models. Dependency-parsed versions of the BNC and ukWaC were used to construct syntacticallyinformed vectors, with a single, labelled arc be24 Figure 3: Window versus dependency contexts tween the target word and context word.3 Since this effectively provides a window size of 3, we also use a window size of 3 for the window-based method (which provided the best results in Section 3.2 with the ukWaC corpus). As w</context>
</contexts>
<marker>Pado, Lapata, 2007</marker>
<rawString>Sebastian Pado and Mirella Lapata. 2007. Dependency-based Construction of Semantic Space Models. Computational Linguistics, 33(2):161–199.</rawString>
</citation>
<citation valid="true">
<authors>
<author>Douglas L T Rohde</author>
<author>Laura M Gonnerman</author>
<author>David C Plaut</author>
</authors>
<title>An Improved Model of Semantic Similarity based on Lexical Co-occurence.</title>
<date>2006</date>
<journal>Communciations of the ACM,</journal>
<pages>8--627</pages>
<contexts>
<context position="2210" citStr="Rohde et al. (2006)" startWordPosition="338" endWordPosition="341">similarity dataset for evaluating compositionality. The choice of source corpus is likely to influence the quality of the VSM, and so we use a selection of source corpora. Hence there are two additional “superparameters”: • dataset for evaluation; • source corpus. Previous studies have been limited to investigating only a small number of parameters, and using a limited set of source corpora and tasks for evaluation (Curran and Moens, 2002a; Curran and Moens, 2002b; Curran, 2004; Grefenstette, 1994; Pado and Lapata, 2007; Sahlgren, 2006; Turney and Pantel, 2010; Schulte im Walde et al., 2013). Rohde et al. (2006) considered several weighting schemes for a large variety of tasks, while Weeds et al. (2004) did the same for similarity metrics. Stone et al. (2008) investigated the effectiveness of sub-spacing corpora, where a larger corpus is queried in order to construct a smaller sub-spaced corpus (Zelikovitz and Kogan, 2006). Blacoe and Lapata (2012) compare several types of vector representations for semantic composition tasks. The most comprehensive existing studies of VSM parameters — encompassing window sizes, feature granularity, stopwords and dimensionality reduction — are by Bullinaria and Levy </context>
</contexts>
<marker>Rohde, Gonnerman, Plaut, 2006</marker>
<rawString>Douglas L. T. Rohde, Laura M. Gonnerman, and David C. Plaut. 2006. An Improved Model of Semantic Similarity based on Lexical Co-occurence. Communciations of the ACM, 8:627–633.</rawString>
</citation>
<citation valid="true">
<authors>
<author>Herbert Rubenstein</author>
<author>John B Goodenough</author>
</authors>
<date>1965</date>
<journal>Contextual Correlates of Synonymy. Commun. ACM,</journal>
<volume>8</volume>
<issue>10</issue>
<marker>Rubenstein, Goodenough, 1965</marker>
<rawString>Herbert Rubenstein and John B. Goodenough. 1965. Contextual Correlates of Synonymy. Commun. ACM, 8(10):627–633, October.</rawString>
</citation>
<citation valid="true">
<authors>
<author>Magnus Sahlgren</author>
</authors>
<title>The Word-Space Model: Using distributional analysis to represent syntagmatic and paradigmatic relations between words in highdimensional vector spaces.</title>
<date>2006</date>
<tech>Ph.D. thesis,</tech>
<institution>Department of Linguistics, Stockholm University.</institution>
<contexts>
<context position="2132" citStr="Sahlgren, 2006" startWordPosition="326" endWordPosition="327">rity datasets has been selected from the literature, including a phrasal similarity dataset for evaluating compositionality. The choice of source corpus is likely to influence the quality of the VSM, and so we use a selection of source corpora. Hence there are two additional “superparameters”: • dataset for evaluation; • source corpus. Previous studies have been limited to investigating only a small number of parameters, and using a limited set of source corpora and tasks for evaluation (Curran and Moens, 2002a; Curran and Moens, 2002b; Curran, 2004; Grefenstette, 1994; Pado and Lapata, 2007; Sahlgren, 2006; Turney and Pantel, 2010; Schulte im Walde et al., 2013). Rohde et al. (2006) considered several weighting schemes for a large variety of tasks, while Weeds et al. (2004) did the same for similarity metrics. Stone et al. (2008) investigated the effectiveness of sub-spacing corpora, where a larger corpus is queried in order to construct a smaller sub-spaced corpus (Zelikovitz and Kogan, 2006). Blacoe and Lapata (2012) compare several types of vector representations for semantic composition tasks. The most comprehensive existing studies of VSM parameters — encompassing window sizes, feature gra</context>
<context position="24498" citStr="Sahlgren, 2006" startWordPosition="4049" endWordPosition="4050">b 0.43 0.63 0.31 0.37 0.20 0.23 0.26 0.09 0.10 nan 0.34 0.24 gref94 0.48 0.54 0.27 0.33 0.20 0.17 0.23 0.13 0.11 0.09 0.38 0.25 Table 5: Aggregated scores for combinations of weighting schemes and similarity metrics using ukWaC with lower-frequency ones, with optimal performance comparable to when weighting is used. There are some scenarios where it may be advantageous not to use weighting, for example in an online setting where the total set of vectors is not fixed; in situations where use of a dimensionality reduction technique does not directly allow for weighting, such as random indexing (Sahlgren, 2006); as well as in settings where calculating weights is too expensive. Where to stop the sliding window varies with the datasets, however, and so our conclusion is that the default scheme should be weighting plus high frequency contexts. 4 Compositionality In order to examine whether optimal parameters carry over to vectors that are combined into phrasal vectors using a composition operator, we perform a subset of our experiments on the canonical compositionality dataset from Mitchell and Lapata (2010), using vector addition and pointwise multiplication (the best performing operators in the orig</context>
</contexts>
<marker>Sahlgren, 2006</marker>
<rawString>Magnus Sahlgren. 2006. The Word-Space Model: Using distributional analysis to represent syntagmatic and paradigmatic relations between words in highdimensional vector spaces. Ph.D. thesis, Department of Linguistics, Stockholm University.</rawString>
</citation>
<citation valid="true">
<authors>
<author>Sabine Schulte im Walde</author>
<author>Stefan M¨uller</author>
<author>Stephen Roller</author>
</authors>
<title>Exploring Vector Space Models to Predict the Compositionality of German Noun-Noun Compounds.</title>
<date>2013</date>
<booktitle>In Proceedings of the 2nd Joint Conference on Lexical and Computational Semantics,</booktitle>
<pages>255--265</pages>
<location>Atlanta, GA.</location>
<marker>Walde, M¨uller, Roller, 2013</marker>
<rawString>Sabine Schulte im Walde, Stefan M¨uller, and Stephen Roller. 2013. Exploring Vector Space Models to Predict the Compositionality of German Noun-Noun Compounds. In Proceedings of the 2nd Joint Conference on Lexical and Computational Semantics, pages 255–265, Atlanta, GA.</rawString>
</citation>
<citation valid="true">
<authors>
<author>N Clayton Silver</author>
<author>William P Dunlap</author>
</authors>
<title>Averaging Correlation Coefficients: Should Fisher’s z Transformation Be Used?</title>
<date>1987</date>
<journal>Journal of Applied Psychology,</journal>
<volume>72</volume>
<issue>1</issue>
<contexts>
<context position="10783" citStr="Silver and Dunlap, 1987" startWordPosition="1751" endWordPosition="1754">sets is non-trivial, since taking the mean of correlation scores leads to an under-estimation of performance; hence for the aggregate score we use the Fisher-transformed z-variable of the correlaTable 3: Term weighting schemes. fij denotes the target word frequency in a particular context, fi the total target word frequency, fj the total context frequency, N the total of all frequencies, nj the number of non-zero contexts. P(tij|cj) is defined fij as fj and P(tij) as fij N . tion datasets, and take the weighted average of its inverse over the correlation datasets and the TOEFL accuracy score (Silver and Dunlap, 1987). 3.1 Vector size The first parameter we investigate is vector size, measured by the number of features. Vectors are constructed from the BNC using a window-based method, with a window size of 5 (2 words either side of the target word). We experiment with vector sizes up to 0.5M features, which is close to the total number of context words present in the entire BNC according to our preprocessing scheme. Features are added according to frequency in the BNC, with increasingly more rare features being added. For weighting we consider both Positive Mutual Information and T-Test, which have been fo</context>
</contexts>
<marker>Silver, Dunlap, 1987</marker>
<rawString>N. Clayton Silver and William P. Dunlap. 1987. Averaging Correlation Coefficients: Should Fisher’s z Transformation Be Used? Journal of Applied Psychology, 72(1):146–148, February.</rawString>
</citation>
<citation valid="true">
<authors>
<author>Mark Steedman</author>
</authors>
<title>The Syntactic Process.</title>
<date>2000</date>
<publisher>MIT Press,</publisher>
<location>Cambridge, MA, USA.</location>
<contexts>
<context position="7129" citStr="Steedman, 2000" startWordPosition="1141" endWordPosition="1142">contexts are determined by word co-occurrences in a particular syntactic relation with a target word (e.g. target word dog is the subject of run, where run subj is the context). We consider different window sizes and compare window-based and dependency-based methods. Feature granularity Context words, or “features”, are often stemmed or lemmatised. We investigate the effect of stemming and lemmatisation, in particular to see whether the effect varies with corpus size. We also consider more finegrained features in which each context word is paired with a POS tag or a lexical category from CCG (Steedman, 2000). Similarity metric A variety of metrics can be used to calculate the similarity between two vectors. We consider the similarity metrics in Table 2. Weighting Weighting schemes increase the importance of contexts that are more indicative of the meaning of the target word: the fact that cat cooccurs with purr is much more informative than its co-occurrence with the. Table 3 gives definitions of the weighting schemes considered. Stopwords, high frequency cut-off Function words and stopwords are often considered too uninformative to be suitable context words. Ignoring them not only leads to a red</context>
</contexts>
<marker>Steedman, 2000</marker>
<rawString>Mark Steedman. 2000. The Syntactic Process. MIT Press, Cambridge, MA, USA.</rawString>
</citation>
<citation valid="true">
<authors>
<author>Benjamin P Stone</author>
<author>Simon J Dennis</author>
<author>Peter J Kwantes</author>
</authors>
<title>A Systematic Comparison of Semantic Models on Human Similarity Rating Data: The Effectiveness of Subspacing.</title>
<date>2008</date>
<booktitle>In The Proceedings of the Thirtieth Conference of the Cognitive Science Society.</booktitle>
<contexts>
<context position="2360" citStr="Stone et al. (2008)" startWordPosition="363" endWordPosition="366">on of source corpora. Hence there are two additional “superparameters”: • dataset for evaluation; • source corpus. Previous studies have been limited to investigating only a small number of parameters, and using a limited set of source corpora and tasks for evaluation (Curran and Moens, 2002a; Curran and Moens, 2002b; Curran, 2004; Grefenstette, 1994; Pado and Lapata, 2007; Sahlgren, 2006; Turney and Pantel, 2010; Schulte im Walde et al., 2013). Rohde et al. (2006) considered several weighting schemes for a large variety of tasks, while Weeds et al. (2004) did the same for similarity metrics. Stone et al. (2008) investigated the effectiveness of sub-spacing corpora, where a larger corpus is queried in order to construct a smaller sub-spaced corpus (Zelikovitz and Kogan, 2006). Blacoe and Lapata (2012) compare several types of vector representations for semantic composition tasks. The most comprehensive existing studies of VSM parameters — encompassing window sizes, feature granularity, stopwords and dimensionality reduction — are by Bullinaria and Levy (2007; 2012) and Lapesa and Evert (2013). Section 2 introduces the various parameters of vector space model construction. We then attempt, in Section </context>
<context position="5027" citStr="Stone et al., 2008" startWordPosition="797" endWordPosition="800">nonym for a target word. In Section 4 we examine our parameters in the context of distributional compositional semantics, using the evaluation dataset from Mitchell and Lapata (2010). Table 1 gives statistics for the number of words and word pairings in each of the datasets. As well as using a variety of datasets, we also consider three different corpora from which to build the vectors, varying in size and domain. These include the BNC (Burnard, 2007) (106 word types, 108 tokens) and the larger ukWaC (Baroni et al., 2009) (107 types, 109 tokens). We also include a sub-spaced Wikipedia corpus (Stone et al., 2008): for all words in the evaluation datasets, we build a subcorpus by querying the top 10-ranked Wikipedia documents using the words as search terms, resulting in a corpus with 106 word types and 107 tokens. For examining the dependency-based contexts, we include the Google Syntactic N-gram corpus (Goldberg and Orwant, 2013), with 107 types and 1011 tokens. 2.1 Parameters We selected the following set of parameters for investigation, all of which are fundamental to vector space model construction1. 1Another obvious parameter would be dimensionality reduction, which we chose not to include becaus</context>
<context position="13869" citStr="Stone et al. (2008)" startWordPosition="2271" endWordPosition="2274">pacing interact with window size? Figure 2 clearly shows the answer to all three questions is “yes”. First, ukWaC consistently outperforms the BNC, across all window sizes, indicating that a larger source corpus leads to better performance. Second, we see that the larger ukWaC performs better with smaller window sizes compared to the BNC, with the best ukWaC performance typically being found with a window size of only 3. For the BNC, it appears that a larger window is able to offset the smaller size of corpus to some extent. We also evaluated on a sub-spaced Wikipedia source corpus similar to Stone et al. (2008), which performs much better with larger window sizes than the BNC or ukWaC. Our explanation for this result is that sub-spacing, resulting from searching for Wikipedia pages with the appropriate target terms, provides a focused, less noisy corpus in which context words some distance from the target word are still relevant to its meaning. In summary, the highest score is typically achieved with the largest source corpora and smallest window size, with the exception of the much smaller sub-spaced Wikipedia corpus. 3.3 Context The notion of context plays a key role in VSMs. Pado and Lapata (2007</context>
</contexts>
<marker>Stone, Dennis, Kwantes, 2008</marker>
<rawString>Benjamin P. Stone, Simon J. Dennis, and Peter J. Kwantes. 2008. A Systematic Comparison of Semantic Models on Human Similarity Rating Data: The Effectiveness of Subspacing. In The Proceedings of the Thirtieth Conference of the Cognitive Science Society.</rawString>
</citation>
<citation valid="true">
<authors>
<author>Peter D Turney</author>
<author>Patrick Pantel</author>
</authors>
<title>From Frequency to Meaning: vector space models of semantics.</title>
<date>2010</date>
<journal>J. Artif. Int. Res.,</journal>
<volume>37</volume>
<issue>1</issue>
<contexts>
<context position="1058" citStr="Turney and Pantel, 2010" startWordPosition="156" endWordPosition="159">ral source corpora. In addition to recommendations for optimal parameters, we present some novel findings, including a similarity metric that outperforms the alternatives on all tasks considered. 1 Introduction Vector space models (VSMs) represent the meanings of lexical items as vectors in a “semantic space”. The benefit of VSMs is that they can easily be manipulated using linear algebra, allowing a degree of similarity between vectors to be computed. They rely on the distributional hypothesis (Harris, 1954): the idea that “words that occur in similar contexts tend to have similar meanings” (Turney and Pantel, 2010; Erk, 2012). The construction of a suitable VSM for a particular task is highly parameterised, and there appears to be little consensus over which parameter settings to use. This paper presents a systematic study of the following parameters: • vector size; • window size; • window-based or dependency-based context; • feature granularity; • similarity metric; • weighting scheme; • stopwords and high frequency cut-off. A representative set of semantic similarity datasets has been selected from the literature, including a phrasal similarity dataset for evaluating compositionality. The choice of s</context>
<context position="12638" citStr="Turney and Pantel, 2010" startWordPosition="2060" endWordPosition="2063">s is the reduction in computational cost. 3.2 Window size Recent studies have found that the best window size depends on the task at hand. For example, Hill et al. (2013) found that smaller windows work best for measuring similarity of concrete nouns, whereas larger window sizes work better for abstract nouns. Schulte im Walde et al. (2013) found that a large window size worked best for a compositionality dataset of German noun-noun compounds. Similar relations between window size and performance have been found for similar versus related words, as well as for similar versus associated words (Turney and Pantel, 2010). We experiment with window sizes of 3, 5, 7, 9 and a full sentence. (A window size of n implies n−1 2 words either side of the target word.) We use Positive Mutual Information weighting, Cosine similarity, and vectors of size 50,000 (based on the results from Section 3.1). Figure 2 shows the results for all the similarity datasets, with the aggregated score at the bottom right. Performance was evaluated on three corpora, in order to answer three questions: Does window size affect performance? Does corpus size interact with window size? Does corpus subFigure 2: Impact of window size across thr</context>
</contexts>
<marker>Turney, Pantel, 2010</marker>
<rawString>Peter D. Turney and Patrick Pantel. 2010. From Frequency to Meaning: vector space models of semantics. J. Artif. Int. Res., 37(1):141–188, January.</rawString>
</citation>
<citation valid="true">
<authors>
<author>Julie Weeds</author>
<author>David Weir</author>
<author>Diana McCarthy</author>
</authors>
<title>Characterising Measures of Lexical Distributional Similarity.</title>
<date>2004</date>
<booktitle>In Proceedings of Coling</booktitle>
<pages>1015--1021</pages>
<publisher>COLING.</publisher>
<location>Geneva, Switzerland,</location>
<contexts>
<context position="2303" citStr="Weeds et al. (2004)" startWordPosition="353" endWordPosition="356">influence the quality of the VSM, and so we use a selection of source corpora. Hence there are two additional “superparameters”: • dataset for evaluation; • source corpus. Previous studies have been limited to investigating only a small number of parameters, and using a limited set of source corpora and tasks for evaluation (Curran and Moens, 2002a; Curran and Moens, 2002b; Curran, 2004; Grefenstette, 1994; Pado and Lapata, 2007; Sahlgren, 2006; Turney and Pantel, 2010; Schulte im Walde et al., 2013). Rohde et al. (2006) considered several weighting schemes for a large variety of tasks, while Weeds et al. (2004) did the same for similarity metrics. Stone et al. (2008) investigated the effectiveness of sub-spacing corpora, where a larger corpus is queried in order to construct a smaller sub-spaced corpus (Zelikovitz and Kogan, 2006). Blacoe and Lapata (2012) compare several types of vector representations for semantic composition tasks. The most comprehensive existing studies of VSM parameters — encompassing window sizes, feature granularity, stopwords and dimensionality reduction — are by Bullinaria and Levy (2007; 2012) and Lapesa and Evert (2013). Section 2 introduces the various parameters of vect</context>
</contexts>
<marker>Weeds, Weir, McCarthy, 2004</marker>
<rawString>Julie Weeds, David Weir, and Diana McCarthy. 2004. Characterising Measures of Lexical Distributional Similarity. In Proceedings of Coling 2004, pages 1015–1021, Geneva, Switzerland, Aug 23–Aug 27. COLING.</rawString>
</citation>
<citation valid="true">
<authors>
<author>S Zelikovitz</author>
<author>M Kogan</author>
</authors>
<title>Using Web Searches on Important Words to create Background Sets for LSI Classification. In</title>
<date>2006</date>
<booktitle>In Proceedings of the 19th International FLAIRS Conference,</booktitle>
<pages>598--603</pages>
<publisher>AAAI Press.</publisher>
<location>Menlo Park, CA.</location>
<contexts>
<context position="2527" citStr="Zelikovitz and Kogan, 2006" startWordPosition="387" endWordPosition="390">igating only a small number of parameters, and using a limited set of source corpora and tasks for evaluation (Curran and Moens, 2002a; Curran and Moens, 2002b; Curran, 2004; Grefenstette, 1994; Pado and Lapata, 2007; Sahlgren, 2006; Turney and Pantel, 2010; Schulte im Walde et al., 2013). Rohde et al. (2006) considered several weighting schemes for a large variety of tasks, while Weeds et al. (2004) did the same for similarity metrics. Stone et al. (2008) investigated the effectiveness of sub-spacing corpora, where a larger corpus is queried in order to construct a smaller sub-spaced corpus (Zelikovitz and Kogan, 2006). Blacoe and Lapata (2012) compare several types of vector representations for semantic composition tasks. The most comprehensive existing studies of VSM parameters — encompassing window sizes, feature granularity, stopwords and dimensionality reduction — are by Bullinaria and Levy (2007; 2012) and Lapesa and Evert (2013). Section 2 introduces the various parameters of vector space model construction. We then attempt, in Section 3, to answer some of the fundamental questions for building VSMs through a number of experiments that consider each of the selected parameters. In Section 4 we examine</context>
</contexts>
<marker>Zelikovitz, Kogan, 2006</marker>
<rawString>S. Zelikovitz and M. Kogan. 2006. Using Web Searches on Important Words to create Background Sets for LSI Classification. In In Proceedings of the 19th International FLAIRS Conference, pages 598—603, Menlo Park, CA. AAAI Press.</rawString>
</citation>
</citationList>
</algorithm>
</algorithms>
<?xml version="1.0" encoding="UTF-8"?>
<algorithms version="110505">
<algorithm name="SectLabel" version="110505">
<variant no="0" confidence="0.006163">
<title confidence="0.985015">
Comparing Models of Phonotactics for Word Segmentation
</title>
<author confidence="0.997704">
Natalie M. Schrimpf Gaja Jarosz
</author>
<affiliation confidence="0.999203">
Department of Linguistics Department of Linguistics
Yale University Yale University
</affiliation>
<email confidence="0.999322">
natalie.schrimpf@yale.edu gaja.jarosz@yale.edu
</email>
<sectionHeader confidence="0.993902" genericHeader="abstract">
Abstract
</sectionHeader>
<bodyText confidence="0.999616956521739">
Developmental research indicates that infants
use low-level statistical regularities, or pho-
notactics, to segment words from continuous
speech. In this paper, we present a segmenta-
tion framework that enables the direct com-
parison of different phonotactic models for
segmentation. We compare a model using
phoneme transitional probabilities, which
have been widely used in computational
models, to syllable-based bigram models,
which have played a prominent role in the
developmental literature. We also introduce a
novel estimation method, and compare it to
other strategies for estimating the parameters
of the phonotactic models from unsegmented
data. The results show that syllable-based
models outperform the phoneme models,
specifically in the context of improved unsu-
pervised parameter estimation. The syllable-
based transitional probability model achieves
a word token f-score of nearly 80%, the high-
est reported performance for a phonotactic
segmentation model with no lexicon.
</bodyText>
<sectionHeader confidence="0.999134" genericHeader="introduction">
1 Introduction
</sectionHeader>
<bodyText confidence="0.999969456140351">
One of the first language learning tasks infants
must solve is the segmentation of fluent speech
into words. Extensive experimental work has
demonstrated that infants are able to use phono-
tactic restrictions (Jusczyk &amp; Luce, 1994; Mattys
et al., 1999; Mattys &amp; Jusczyk, 2001) and other
low-level statistical regularities (Saffran et al.,
1996; Thiessen &amp; Saffran, 2003; Pelucchi et al.,
2009) to extract words from fluent speech before
the age of one. This work has shown that infants
utilize these low-level statistical regularities to
segment speech during the second half of the
first year of life before they have developed ex-
tensive vocabularies that could provide top-down
lexical information to guide segmentation. De-
velopmental research indicates that on average
infants know fewer than 100 word types during
this period (Dale &amp; Fenson, 1996; Daland &amp;
Pierrehumbert, 2011).
One statistical cue that has received a great
deal of support in experimental work on infant
speech segmentation is transitional probability
calculated over syllables. In foundational work,
Saffran et al. (1996) found that infants are able to
segment words from continuous speech using
statistical regularities between syllables. Numer-
ous subsequent studies have confirmed that in-
fants can track transitional probabilities and use
them to segment speech (Aslin et al., 1998;
Thiessen &amp; Saffran, 2003; Pelucchi et al., 2009).
Despite the extensive experimental literature
demonstrating infants’ sensitivity to transitional
probability in an artificial language learning set-
ting, the utility of these statistical cues in a natu-
ral language learning context is disputed. Yang
(2004) shows that a segmentation strategy rely-
ing on transitional probabilities over syllables
achieves very poor results on English child-
directed speech, even when the input is perfectly
syllabified. Yang implements the local minimum
segmentation strategy proposed by Saffran et al.
(1996) wherein word boundaries are posited at
syllable transitions whenever the transitional
probabilities at these positions are lower than at
the neighboring transitions. He reports that this
strategy discovers a mere 23% of target words
and posits incorrect words nearly 60% of the
time. Swingley (2005) argues that statistical cues
calculated over syllables can provide sufficient
information for infants to begin building an ini-
tial lexicon. However, the learning strategy ex-
plored by Swingley is highly conservative, relia-
bly detecting only a small proportion of target
words in the input. Overall, these results raise
questions about whether syllable-based statistics
can be reliably used to identify word boundaries
in natural language data.
</bodyText>
<page confidence="0.992164">
19
</page>
<note confidence="0.787546">
Proceedings of the 2014 Joint Meeting of SIGMORPHON and SIGFSM, pages 19–28,
Baltimore, Maryland USA, June 27 2014. c�2014 Association for Computational Linguistics
</note>
<bodyText confidence="0.999922218181819">
While the experimental work emphasizes syl-
lable-level transitional probability, recent com-
putational modeling work and corpus analyses
have primarily focused on the utility of pho-
neme-level statistics. A number of phonotactical-
ly-based segmentation models, focusing on the
discovery of word boundaries based on pho-
neme-level statistics, have achieved more prom-
ising results (Adriaans &amp; Kager, 2010; Daland &amp;
Pierrehumbert, 2011; see also Brent, 1999). For
example, Brent (1999) showed that a local mini-
mum strategy relying on phoneme bigrams cor-
rectly extracts about 50% of word tokens in Eng-
lish child-directed speech. Corpus analyses of
child-directed speech have also highlighted the
information content of phoneme-level statistics
(Hockema, 2006; Jarosz &amp; Johnson, 2013). Re-
lated work has shown that phonotactic infor-
mation can improve the performance of state-of-
the-art segmentation models whose primary ob-
jective is to discover the lexicon that underlies
the regularities in the continuous speech signal.
Again, this work has largely emphasized pho-
neme-level statistical cues (Blanchard &amp; Heinz
2008, 2010), and those models that do rely on
syllable structure (Johnson, 2008a; Johnson &amp;
Goldwater, 2009), do not directly encode sequen-
tial statistics between adjacent syllables of the
sort investigated in the infant literature. Finally,
some models assume computations are per-
formed over syllables and that all word bounda-
ries in the input are aligned with syllable bounda-
ries, but provide no mechanism by which such
language-specific syllabification principles could
be learned (Yang, 2004; Swingley, 2005; Lignos
&amp; Yang, 2010).
Overall, the existing evidence clearly shows
that there are phonotactic cues to word bounda-
ries in spontaneous, child-directed speech. How-
ever, there are remaining questions regarding the
exact nature of these cues, their reliability, and
how they relate to the statistical cues explored in
the infant word segmentation literature. In this
paper, we investigate the computational mecha-
nisms underlying infants’ early speech segmenta-
tion abilities relying on low-level statistical regu-
larities, or phonotactics. We present a computa-
tional framework that permits the direct compari-
son of segmentation predictions for alternative
models of phonotactics. In particular, we com-
pare a standard phonotactic model relying on
phoneme-level bigrams to two syllable-based
phonotactic models relying on transitional prob-
abilities. Unlike previous models relying on syl-
labified data (Yang, 2004; Swingley, 2005; Lig-
nos &amp; Yang, 2010), we do not assume that word
boundaries align with syllable boundaries in the
input. Rather, we present a simple syllabification
method that can be used to model phonotactic
probability for arbitrary strings using statistics
estimated from unsyllabified, unsegmented utter-
ances. We also compare the local minimum seg-
mentation strategy (Saffran et al., 1996; Yang,
2004) to alternatives designed to deal with the
challenges of unsupervised estimation of transi-
tional probabilities from unsegmented input.
Our focus on the early phonotactic segmenta-
tion stage differentiates our approach from many
computational models emphasizing the discovery
of the lexicon and higher-level language struc-
ture (Brent, 1999; Venkataraman, 2001; Swin-
gley, 2005; Johnson, 2008a; Goldwater et al.,
2009; Johnson &amp; Goldwater, 2009; Blanchard &amp;
Heinz 2008, 2010; Lignos &amp; Yang, 2010). It
complements that of recent work investigating
the use of phoneme-level statistical regularities
for segmentation (Adriaans &amp; Kager, 2010; Da-
land &amp; Pierrehumbert, 2011). Our work differs
from these latter approaches, however, in com-
paring several phonotactic models, including
ones relying on the syllable-based transitional
probability statistics investigated in infant re-
search. Our work also contributes to existing
segmentation work that assumes a syllabified
input (Yang, 2004; Swingley, 2005; Lignos &amp;
Yang, 2010) by showing how many aspects of
syllable structure can be inferred.
Our results reveal an interaction between es-
timation strategy and the choice of phonotactic
model. The local minimum segmentation strate-
gy works poorly in general for all models con-
sidered, but the lowest performance is achieved
by the syllable-based models. However, when
the same cues are used in the context of a simple,
generative probability model with improved un-
supervised parameter estimation, the syllable-
based models substantially outperform the pho-
neme-based models. Indeed, the syllable-based
transitional probability phonotactic model
achieves a word token segmentation f-score of
nearly 80%, which is the highest reported per-
formance among purely phonotactically-based
segmentation models (Adriaans &amp; Kager, 2010;
Daland &amp; Pierrehumbert, 2011). Indeed, this per-
formance compares favorably with state-of-the-
art segmentation models that involve learning of
higher level regularities, such as the lexicon and
collocations (Brent, 1999; Venkataraman, 2001;
Johnson, 2008a; Goldwater et al., 2009; Johnson
&amp; Goldwater, 2009), and demonstrates that good
</bodyText>
<page confidence="0.990092">
20
</page>
<bodyText confidence="0.989961">
segmentation performance can be achieved by
exploiting simple syllable-level phonotactic cues.
</bodyText>
<sectionHeader confidence="0.985701" genericHeader="method">
2 Segmentation Model
</sectionHeader>
<bodyText confidence="0.99998975">
The proposed segmentation model defines the
probability of an utterance in terms of an abstract
phonotactic probability component that assigns
word well-formedness probabilities to phoneme
strings. The segmentation algorithm uses those
probabilities to determine the maximum likeli-
hood segmentation as defined by a simple gener-
ative model. Since the phonotactics and segmen-
tation components are separate, they can be in-
dependently modified. This framework makes it
possible to compare models of phonotactics
while using the same segmentation strategy.
</bodyText>
<subsectionHeader confidence="0.895947">
2.1 Probability Model
</subsectionHeader>
<bodyText confidence="0.999645705882353">
The segmentation probability model relies on the
phonotactic component to assign probabilities to
potential words. The probability of a segmenta-
tion w is defined in terms of a simple unigram
model by multiplying the probabilities of the
words ws...n posited in that segmentation.
P wi is the probability assigned by the phono-
tactic models, which will be defined in the next
section. The various phonotactic models change
how exactly P wi is defined, but the segmenta-
tion probability always depends directly on the
word probabilities given by a particular phono-
tactic model. For example, for the utterance
[lukwtmi] ‘lookatme’, the segmentation model
compares different segmentations, such as
[luk#w#tmi] and [luk#wt#mi] based on the pho-
notactic well-formedness of the posited words.
</bodyText>
<subsectionHeader confidence="0.999532">
2.2 Segmentation Algorithm
</subsectionHeader>
<bodyText confidence="0.99999">
The segmentation algorithm computes and out-
puts the segmentation with the highest likeli-
hood: argmaxwP w . The optimal segmenta-
tion is found using dynamic programming, as in
several previous proposals (Brent, 1999; Venka-
taraman, 2001). Given an input utterance, the
model considers placing word boundaries at dif-
ferent positions within the utterance without re-
gard to phonotactics or syllable structure. The
phonotactic probability of each posited word is
calculated independently as it is considered and
used to update the probability of segmentations
utilizing that word. In this way, the segmentation
component remains entirely divorced from the
details of the phonotactic models. Crucially, this
means the full space of possible segmentations is
considered by the segmentation model regardless
of the phonotactic model, with no a priori re-
strictions imposed by phonotactic or syllable
constraints as to where boundaries are permitted.
</bodyText>
<sectionHeader confidence="0.995076" genericHeader="method">
3 Phonotactic Models
</sectionHeader>
<bodyText confidence="0.999279083333333">
We implement and compare several models of
phonotactics that are utilized by the segmentation
component described above. While all models
rely on transitional probabilities, or bigrams, as
defined in (2), the unit of analysis varies between
the models. One model uses phonemes and pho-
neme transitions, and two models incorporate
syllable information: we use x to denote a gener-
ic unit. The model determines the probability of
a word, w = xo...n+1 where xo and xn+1 are the
word boundary symbol #, by multiplying the
probabilities of all bigrams in the word.
</bodyText>
<equation confidence="0.778547">
2) ! ! = o !(!!+! ! !!)
</equation>
<bodyText confidence="0.99146">
The transitional probability for the sequence
xixi+l can be calculated using relative frequency
estimates based on counts C in the corpus.
</bodyText>
<equation confidence="0.965937666666667">
!(!!!!!!)
3) ! !! !!-1 =
C(XL-1)
</equation>
<bodyText confidence="0.9998232">
Section 4 describes strategies that we consider
for estimating these parameters in an unsuper-
vised way from unsegmented data where the on-
ly word boundaries are those that coincide with
utterance boundaries.
</bodyText>
<subsectionHeader confidence="0.984178">
3.1 Phoneme Model
</subsectionHeader>
<bodyText confidence="0.999506142857143">
The first phonotactic model is a standard pho-
neme bigram model that determines the probabil-
ity of a word by multiplying the phoneme bi-
grams in the word (Jurafsky &amp; Martin, 2008).
For example, to calculate the phonotactic proba-
bility of the sequence [bot] as a word, this model
multiplies together P(bj#)P(ojb)P(tjo)P(#jt).
</bodyText>
<subsectionHeader confidence="0.989687">
3.2 Syllable-Based Models
</subsectionHeader>
<bodyText confidence="0.99994625">
The other two phonotactic models use syllables
rather than phonemes. One model relies on tran-
sitional probabilities over syllables, and the other
uses onsets and rhymes as the unit of analysis.
</bodyText>
<subsectionHeader confidence="0.674606">
3.2.1 Unsupervised Syllabification
</subsectionHeader>
<bodyText confidence="0.9995275">
The syllabification method relies on the language
universal principle of onset maximization to-
</bodyText>
<equation confidence="0.7826315">
1) P w = P Ws..., = !
! !(!!)
</equation>
<page confidence="0.988355">
21
</page>
<bodyText confidence="0.999970477272727">
gether with an inventory of syllable onsets de-
rived from the beginnings of utterances. When
syllabifying an intervocalic sequence of conso-
nants, this method finds the longest legal onset
aligned with the right edge and places any re-
maining consonants in the coda of the previous
syllable. Thus, a sequence like [wtmi] would be
syllabified as [wt.mi] in English since [m] but
not [tm] occurs utterance-initially. The only lan-
guage-particular information required for this
approach is knowledge of which phonemes are
vowels (syllabic) and which are consonants, a
limited type of information also assumed by oth-
er syllable inference models for segmentation
(Johnson, 2008a; Johnson &amp; Goldwater, 2009).
As the segmentation component posits poten-
tial words, they are passed to the phonotactic
component for syllabification and phonotactic
probability calculation. This differs crucially
from previous work assuming a fixed syllabifica-
tion of the input corpus in which word bounda-
ries always align with syllable boundaries (Yang,
2004; Swingley, 2005; Lignos &amp; Yang, 2010). In
a setting in which syllabification must be in-
ferred from unsegmented utterances, the learner
must be capable of assigning syllabification more
flexibly since word boundaries do not always
align with the syllable boundaries that would be
posited for the utterance as a whole. For exam-
ple, the universal onset maximization principle
always parses singleton consonants VCV as the
onsets V.CV rather than codas VC.V. Therefore,
without prior knowledge of word boundaries, the
utterance [l!kwtmi] (‘look at me’) would be syl-
labified as [l!.kwt.mi], and if the segmentation
algorithm never considered words that misa-
ligned with these syllable boundaries, it would
never extract any vowel-initial words like ‘at’.
Thus, a crucial feature of the current model is
that syllabification takes place on a word-by-
word basis as potential words are posited. The
resulting syllabification for the potential word is
used by the syllable-based models to assign pho-
notactic probability as discussed below.
</bodyText>
<subsectionHeader confidence="0.459824">
3.2.2 Syllable Model
</subsectionHeader>
<bodyText confidence="0.999976304347826">
The first syllable-based model is one in which
bigram transitional probabilities are calculated
over syllables. These transitional probabilities
are precisely those discussed earlier as having
played a prominent role in the infant segmenta-
tion literature. The phonotactic probability of a
posited word is calculated by multiplying the
transitional probabilities of all syllable bigrams
in the word, including an assumed initial and
final #. For example, if the segmentation compo-
nent posits a potential word such as [l!kwtmi]
‘lookatme’, this sequence is first syllabified us-
ing the procedure described earlier as
[l!.kwt.mi]. Then the phonotactic probability of
this potential word is calculated by multiplying
together the syllable-based bigram probabilities:
P(l!|#)P(kwt|l!)P(mi|kwt)P(#|mi). As before, rel-
ative frequency estimates calculated from un-
segmented input data (automatically syllabified
using the unsupervised syllabification method
described earlier) provide a starting point for pa-
rameter estimation. Estimation strategies are dis-
cussed in depth in Section 4.
</bodyText>
<subsectionHeader confidence="0.491309">
3.2.3 Onset Rhyme Model
</subsectionHeader>
<bodyText confidence="0.999392884615384">
In addition to the phoneme level and syllable
level bigram models, we consider an intermedi-
ate model that makes use of the main subconstit-
uents of syllables: onsets and rhymes. Recall that
the syllabification procedure relies on identifying
maximal onsets, whereas rhymes are composed
of the remaining material in the syllable. So the-
se constituents are already available during the
syllabification procedure, and this phonotactic
model operates over these smaller constituents,
rather than over entire syllables. The syllable-
based model operates over indivisible syllable
units, while this models treats syllables as com-
binations of smaller subconstituents.
Once a sequence is syllabified (separating on-
sets and rhymes), this model uses bigrams over
these units to determine word probabilities. Con-
sider again the potential word [l!kwtmi]
‘lookatme’. This sequence is first syllabified into
onsets and rhymes as [l.!.k.wt.m.i]. Then its
phonotactic probability is calculated by multiply-
ing together the bigram probabilities:
P(l|#)P(!|l)P(k|!)P(wt|k)P(m|wt)P(i|m)P(#|i). As
before, relative frequency estimates are calculat-
ed from an (automatically syllabified) unseg-
mented version of the input corpus.
</bodyText>
<sectionHeader confidence="0.999161" genericHeader="method">
4 Estimation
</sectionHeader>
<bodyText confidence="0.9999505">
Inferring the parameters of these models in an
unsupervised way from unsegmented utterances
presents a number of challenges. First, a genera-
tive model relying on these parameters must be
able to accommodate elements and sequences of
elements that have not previously been encoun-
</bodyText>
<page confidence="0.982731">
22
</page>
<bodyText confidence="0.999839333333333">
tered. This includes unseen phonemes, onsets,
rhymes, syllables, and unseen sequences of these
units. A second difficulty for the generative
model arises specifically in the context of seg-
mentation due to the number of boundaries en-
countered in the input data. In an unsegmented
corpus there are no boundaries within an utter-
ance. The only evidence for word boundaries
comes from boundaries at the beginnings and
ends of utterances. The effect is that the total
number of boundaries is lower than the number
that must be inferred by the learner, and the
overall probability of boundaries is underrepre-
sented in the input data. We considered several
estimation methods to overcome these effects.
</bodyText>
<subsectionHeader confidence="0.966423">
4.1 Local Minimum Strategy
</subsectionHeader>
<bodyText confidence="0.999886875">
In previous research (Saffran et al., 1996) it has
been suggested that word boundaries are placed
at troughs in transitional probability so that a
boundary is inserted between two elements when
the transitional probability of those elements is
lower than the probability of the neighboring
transitions. This strategy captures the fact that
word boundaries are more likely to occur be-
tween elements that have a low probability of
occurring together. Since this strategy does not
incorporate transitional probabilities into a gen-
erative segmentation model, it provides a simple
way around the estimation challenges discussed
above. We include it for comparison to previous
results relying on syllable-based transitional
probabilities (Yang, 2004).
</bodyText>
<subsectionHeader confidence="0.978347">
4.2 Adjusted Boundary Count Strategy
</subsectionHeader>
<bodyText confidence="0.99188908">
We also introduce a novel, simple method for
adjusting the estimates of transitional probabili-
ties based on input data that underrepresents
word boundaries. This method directly adjusts
the parameter estimates in order to increase the
overall likelihood of word boundaries. The main
insight behind this estimation strategy is that ob-
served bigram counts (of co-occurring pho-
nemes, syllables, or onsets and rhymes) in the
input data are overestimated since a proportion
of them are in reality separated by word bounda-
ries in the desired segmentation. For a given pro-
portion p# (a parameter of this estimation meth-
od), the bigram counts of co-occurring elements
(phonemes, syllables, or onsets/rhymes) are sys-
tematically decreased by a factor of (1- p#) and
for each context c, are reallocated to the transi-
tional probability of P(#  |c). The formula below
illustrates how this adjustment works for arbi-
trary contexts c and proportion p#. The probabil-
ity of each possible element ei that can follow c
is decreased by a factor of p# as shown in (4).
The total probability taken away from all contin-
uations of c is used to increase the probability of
P(#  |c) as shown in (5).
</bodyText>
<table confidence="0.5827182">
!!!!!)
! !! ! =
!!!! (1 — P#)
! # ! = !!!!! !!!!!
!!!! + !#(1 — !!!! )
</table>
<bodyText confidence="0.9988874">
Consider an example for the context x, with three
bigrams observed in the input: c(xy) = 10, c(xz) =
6, and c(x#) = 4. The relative frequency estimates
for these transitional probabilities are 0.5, 0.3,
and 0.2 respectively. The adjusted count method
takes away p# of the xy and xz counts and reallo-
cates them to x#. For p# = 0.5, for example, the
new estimates would be 0.25, 0.15, and 0.6. The
adjustment works analogously for every context
for each of the units of analysis.
</bodyText>
<subsectionHeader confidence="0.998616">
4.3 Smoothing
</subsectionHeader>
<bodyText confidence="0.99997">
We also utilized rudimentary smoothing tech-
niques to allow the generative model to deal with
unknown sequences. We chose a simple method
that allocated non-zero probability to unseen se-
quences while minimally disrupting the estimates
computed using the adjusted boundary count
strategy, since our primary concern was in ex-
ploring the effects of this novel re-estimation
strategy. For all models, add-lambda smoothing
(Jurafsky &amp; Martin, 2008) with a value of 0.001
was used. For the syllable-based models this total
value was allocated to all unseen bigrams in or-
der to avoid over-allocation of probability to the
numerous combinations of unseen syllabic units.
</bodyText>
<subsectionHeader confidence="0.942726">
4.4 Iterative Re-estimation
</subsectionHeader>
<bodyText confidence="0.999832133333333">
After estimating the transitional probabilities
from the unsegmented corpus, the above strate-
gies can be used to compute the optimal segmen-
tation of the input corpus in a single pass. In ad-
dition to the above strategies, we also investigat-
ed a greedy, iterative re-estimation strategy that
makes multiple passes through the corpus. This
estimation method takes the output of the above
methods and uses it to re-estimate (smoothed and
adjusted) parameters for the phonotactic models.
It then recomputes the optimal segmentation of
the corpus based on the new parameters and re-
peats until convergence. This method is moti-
vated by previous segmentation work highlight-
ing the effectiveness of greedy re-estimation
</bodyText>
<page confidence="0.99572">
23
</page>
<bodyText confidence="0.998105833333333">
techniques (Brent, 1999; Venkataraman, 2001;
Goldwater et al., 2009; Johnson &amp; Goldwater,
2009). As noted in previous work, such greedy
re-estimation has the potential to infer additional
word boundaries based on commitments made to
word boundaries on earlier passes.
</bodyText>
<sectionHeader confidence="0.998655" genericHeader="evaluation">
5 Experiments
</sectionHeader>
<subsectionHeader confidence="0.960427">
5.1 Corpus
</subsectionHeader>
<bodyText confidence="0.999907555555556">
The experiments for all the models were run on
the Brent (1999) version of the Bernstein-Ratner
(1987) corpus of English child-directed speech
consisting of phonetically transcribed utterances.
This corpus has been widely used for evaluating
segmentation models. Other models evaluated on
this corpus include those of Brent (1999), Venka-
taraman (2001), Blanchard and Heinz (2008),
and Johnson and Goldwater (2009).
</bodyText>
<subsectionHeader confidence="0.975055">
5.2 Evaluation
</subsectionHeader>
<bodyText confidence="0.999978833333333">
Precision, recall, and f-scores of both word to-
kens and boundaries were used to evaluate per-
formance. For the models with iterative re-
estimation, the reported performance scores are
taken from the iteration after convergence. This
typically happened after 5-10 iterations.
</bodyText>
<subsectionHeader confidence="0.867226">
5.3 Results and Discussion
</subsectionHeader>
<bodyText confidence="0.999981013157895">
Table 1 summarizes the word boundary and
word token f-scores for all models, while Table 2
presents the precision and recall scores for the
best-performing adjusted count models and the
local minimum models.
Focusing first on the local minimum estima-
teion strategy, there are several noteworthy ef-
fects. First, our results with local minima for the
syllable-level transitional probabilities achieves
very similar word token precision and recall to
that reported by Yang (2004), who examined a
different corpus of child-directed English. The
word token precision and recall of our model is
40.2% and 23.7%, respectively, while Yang re-
ported 41.6% and 23.3%, respectively, for his
experiments. This corroborates Yang’s finding
that the local minima estimation strategy for syl-
lable-level transitional probabilities works very
poorly, this time showing that this level of per-
formance can be achieved with simultaneous in-
ference of syllabification. As Table 2 shows, the
poor performance can be attributed to poor re-
call, which the low boundary recall and high pre-
cision illustrate most clearly. As Yang discusses,
the fatal flaw for this approach is that it categori-
cally fails to segment monosyllabic words, which
account for an overwhelming majority of words
in child-directed speech. This is because local
minima must, by definition, be separated by at
least one transition with a higher bigram proba-
bility, which is not treated as a boundary. Indeed,
the proportion of monosyllables is so high that a
baseline strategy that simply posits word bounda-
ries at all syllable boundaries achieves a word
token f-score of 58.0% using the minimally-
supervised syllabification procedure described
here1. The high performance of the monosyllabic
baseline highlights the ineffectiveness of the lo-
cal minimum strategy but also indicates that syl-
lable structure provides a significant amount of
information about word boundaries in English,
even if this syllable structure is automatically
inferred from unsegmented input using minimal
prior knowledge.
Furthermore, our results with the phoneme
bigram local minimum strategy (47.1% word
token f-score) corroborate Brent’s (1999) finding
that this method achieves a roughly 50% word
token f-score (Brent did not provide exact num-
bers). The improvement in performance is not
surprising given the above discussion about the
prevalence of monosyllabic words: local minima
defined over the smaller phoneme units do not
automatically rule out the possibility of segment-
ing short words. We also demonstrate that the
onset-rhyme model achieves performance similar
to that of the syllable bigram model using the
local minima strategy. Finally, the results with
iterative re-estimation show that further refine-
ment of the posited word boundaries can lead to
some improvement, but none of the local mini-
mum models surpass 53% word token f-score,
and the syllable-based models perform substan-
tially worse. Overall, these partial results are
consistent with the trend suggested by previous
work that the syllable-level bigrams examined in
the infant studies provide little information about
word boundaries in natural language data when
the local minimum strategy is used.
However, a different picture emerges when
the performance of the adjusted count strategy is
considered. The fact that the local minimum
strategy is ineffectual is already clear from the
comparison with the monosyllabic baseline;
however, the results for the adjusted counts esti-
mation strategy reveal that it is possible to ex-
</bodyText>
<footnote confidence="0.8483744">
1 In contrast, Lignos &amp; Yang (2010) report a word token f-
score of 78.9% for this baseline for already syllabified in-
put. The difference between these baselines highlights how
much more difficult the segmentation task is when the syl-
labification must be inferred from unsegmented input.
</footnote>
<page confidence="0.99678">
24
</page>
<table confidence="0.998891375">
p# = 0 p# = 0.35 p# = 0.5 p# = 0.6 p# = 0.75 p# = 0.99 LM
WF BF WF BF WF BF WF BF WF BF WF BF WF BF
P 13.0 10.2 34.7 51.9 40.3 60.6 49.9 69.2 45.9 68.8 13.9 50.1 47.1 64.5
OR 15.4 17.9 28.7 43.3 37.1 55.8 42.2 62.0 58.4 76.0 52.3 71.4 27.9 44.1
S 10.7 3.1 12.7 8.6 14.2 12.4 15.9 16.3 20.7 26.1 74.1 84.1 29.8 51.0
P-IR 13.0 10.2 34.7 51.9 40.3 60.6 50.7 69.6 46.9 69.6 9.9 47.0 52.9 70.5
OR-IR 19.8 29.1 36.8 54.7 47.7 67.7 53.4 72.8 63.8 79.8 37.1 62.1 42.3 62.3
S-IR 10.9 3.8 13.3 10.5 15.2 15.0 16.8 18.7 23.1 31.4 79.8 88.0 27.2 43.9
</table>
<tableCaption confidence="0.984583">
Table 1: Word token (WF) and boundary (BF) f-scores for all models. The columns in the first section
of the table represent different settings of the p# parameter, with highest performance for each adjusted
count model shown in bold. p# values were selected to show a representative range of performance. P
= phoneme model; OR = onset-rhyme model; S = syllable model; IR = iterative re-estimation; LM =
local minimum strategy. The best performing local minimum model is shaded.
</tableCaption>
<table confidence="0.967753230769231">
Adjusted Count Estimation
WP WR BP BR
50.3 51.1 68.8 70.4
63.8 63.8 79.9 79.8
85.2 75.0 97.0 80.6
Local Minimum Estimation
WP WR BP BR
53.4 52.4 71.5 69.5
44.2 40.5 66.5 58.6
40.4 20.5 94.0 28.6
P-IR
OR-IR
S-IR
</table>
<tableCaption confidence="0.980303333333333">
Table 2: Word precision (WP), word recall (WR), boundary precision (BP), and boundary recall (BR)
scores for selected models. For the adjusted count estimation models, the results for the best perform-
ing parameter value are shown (P-IR: 0.6; OR-IR: 0.75; S-IR: 0.99).
</tableCaption>
<bodyText confidence="0.999295827586207">
extract substantially more information about
word boundaries from syllable-based models
when these cues are used in the context of a gen-
erative model and better methods are used for
unsupervised estimation of these parameters. In
fact, using the adjusted counts estimation method
with the optimal parameter settings, the reverse
trend is observed, wherein the phoneme-level
bigrams perform worse than the syllable-based
models, and syllable-level bigrams perform best
of all, reaching word token f-scores of nearly
80%. Crucially, both the onset-rhyme and the
syllable bigram models achieve levels of perfor-
mance that surpass the monosyllabic baseline. In
the case of the syllable bigram, the improvement
in word token f-score is more than 20% when
iterative re-estimation is used and more than
15% when segmentation is performed in only a
single pass through the corpus.
The phoneme-based models perform about as
well whether adjusted counts or local minimum
estimation is used. However, compensation for
the underrepresentation of word boundaries in
the input is crucial to the syllable-based models.
These models surpass the local minimum estima-
tion models only when the p# parameter compen-
sates sufficiently for the input bias against word
boundaries. As shown in Table 1, without any
compensation (p# = 0), all models perform terri-
bly. This is because utterance boundaries provide
very little evidence of word boundaries, and the
models estimated directly from such input mas-
sively undersegment. It is only at higher settings
of the parameter that performance improves. As
expected, the optimal parameter value increases
with the granularity of the unit over which bi-
grams are computed. This makes sense since
boundaries are more likely to fall between larger
units than between smaller units.
Less expected is the fact that the optimal pa-
rameter values are high compared to the empiri-
cal rates of word boundaries in the true segmen-
tation of the input corpus. For example, the true
rate of utterance-internal word boundaries is
around 30% at the phoneme level, yet the opti-
mal p# value for phoneme bigrams is around
60%. The reason for this is that our generative
model, like that of a number of previous models
discussed in the literature (Brent, 1999; Venkata-
raman, 2001; Goldwater et al., 2009), has an in-
herent undersegmentation bias. Due to the way
the phonotactic models are defined, there is a
cost for every additional word boundary posited
in the segmentation. This is because positing a
boundary corresponds to the generation of an
additional symbol, #, which otherwise does not
have to be generated. Since generating a # is
never done with 100% probability, doing so al-
</bodyText>
<page confidence="0.992841">
25
</page>
<bodyText confidence="0.999909142857143">
ways incurs a cost relative to a segmentation
where no such # has to be generated. The high
optimal settings of the p# parameter reflect this
inherent bias and enable the estimation procedure
to compensate not only for the underrepresenta-
tion of word boundaries in the input but also for
this bias in the generative model.
</bodyText>
<sectionHeader confidence="0.999462" genericHeader="conclusions">
6 Conclusions
</sectionHeader>
<bodyText confidence="0.99998514">
We compared segmentation models that rely on
phoneme transitions to models that make use of
syllable structure. The results indicate that sylla-
ble-based statistics are valuable for segmenta-
tion. We also showed that it is possible to utilize
this structure successfully with limited prior
knowledge of the target language by using a
simple syllabification strategy inferred from un-
segmented utterances. The performance of the
syllable-based models also demonstrates that it is
possible to achieve good segmentation results
without the use of a lexicon. Another contribu-
tion of this work is a novel estimation procedure
that addresses some challenges of unsupervised
segmentation. We showed that adjusting parame-
ter estimates inferred from unsegmented input is
essential for achieving good performance.
The strong performance of the syllable level
bigram phonotactic model has a number of im-
plications. First, it demonstrates that the kind of
statistical regularities that infants have been con-
sistently shown to be sensitive to in artificial ex-
perimental stimuli do provide a substantial
amount of information about word boundaries in
natural language data, at least in English. This
lends significant credibility to the claim that sen-
sitivity to such statistical regularities plays a cru-
cial role in infants’ early language development
(contra Yang 2004). This result also highlights
the role that sensitivity to richer phonological
information, beyond the level of phonemes, plays
in language learning, a result that is echoed in
much recent work on the modeling of phonotac-
tic well-formedness of isolated words (Hayes &amp;
Wilson, 2008; Albright, 2009; Daland et al.,
2011). A consistent finding of this work has been
that access to abstract structure and robust gener-
alization mechanisms is crucial to the modeling
of human phonotactic knowledge. While our re-
sults are compatible with these conclusions, our
results cannot confirm that it is syllable structure
per se that improves segmentation since the syl-
lable-based models have several co-occurring
advantages. In addition to abstract structure, they
can track longer and more complex dependen-
cies. Nonetheless, these results motivate further
investigation into the role that richer models of
phonotactics may play in word segmentation and
into the precise mechanisms responsible for im-
proved segmentation using syllable structure.
Particularly critical is exploration of phonotacti-
cally-based segmentation models for languages
besides English, for which phonotactic cues hold
significant promise (Jarosz &amp; Johnson, 2013)
given the relatively low performance of state-of-
the-art lexicon-building models (Johnson 2008b).
Another important direction for future work is
investigating how early, phonotactically-based
segmentation interacts with subsequent learning
of higher-level structure, including the lexicon.
Johnson (2008a) and Johnson &amp; Goldwater
(2009) have already demonstrated that syllable
structure provides valuable information in this
context; however, their models relied on very
different syllable regularities than those investi-
gated here, and the consequences of these differ-
ences should be explored in future work.
Goldwater et al. (2009) showed that a number
of proposed segmentation models have an under-
segmentation bias that can be avoided by simul-
taneously modeling statistical dependencies be-
tween words. They proposed a Bayesian prior to
favor a smaller lexicon and showed that other-
wise unigram models introduce a severe under-
segmentation bias due to the possibility of
matching empirical probabilities by memorizing
utterances as words. Note that the same is not
true of syllable-based models since the hypothe-
sis space does not permit memorization of utter-
ances, and the size of the syllable inventory, un-
like a lexicon, remains relatively stable under
different segmentations. Thus, the syllable-based
models are not subject to the same kind of under-
segmentation bias. Interestingly, the syllable bi-
gram model surpasses the performance of the
word bigram model proposed by Goldwater et al.
(word token f-score 72.3) given sufficient com-
pensation for its undersegmentation bias. How-
ever, this level of performance requires adjust-
ment of the p# parameter to compensate for the
cost of generating additional boundaries. Alt-
hough parameters are common in computational
models (for example, Goldwater et al. used a p#
parameter to modulate the prior distributions in
their Bayesian models), they do not provide a
particularly satisfying explanation for why in-
fants are compelled to break up the speech
stream into smaller units (words). Further work
is needed to determine how undersegmentation
biases are ultimately overcome by children.
</bodyText>
<page confidence="0.994149">
26
</page>
<sectionHeader confidence="0.990336" genericHeader="references">
References
</sectionHeader>
<reference confidence="0.999872980769231">
Adriaans, Frans and Kager, René. 2010. Adding gen-
eralization to statistical learning: The induction of
phonotactics from continuous speech. Journal of
Memory and Language 62(3): 311-331.
Albright, Adam. 2009. Feature-based generalisation
as a source of gradient acceptability. Phonology,
26(1): 9-41.
Aslin, Richard N., Saffran, Jenny R., &amp; Newport,
Elissa L. 1998. Computation of conditional proba-
bility statistics by 8-month-old infants. Psychologi-
cal Science, 9, 321-324.
Bernstein-Ratner, Nan. 1987. The phonology of par-
ent child speech. Children’s Language, 6: 159-174.
Blanchard, Daniel and Heinz, Jeffrey. 2008. Improv-
ing word segmentation by simultaneously learning
phonotactics. In Conll ’08: Proceedings of the 12th
Conference on Computational Natural Language
Learning. Stroudsburg, PA: Association for Com-
putational Linguistics.
Blanchard, Daniel, Heinz, Jeffrey and Golinkoff,
Roberta. 2010. Modeling the contribution of pho-
notactic cues to the problem of word segmentation.
Journal of Child Language, 37(3): 487-511.
Brent, Michael R. 1999. An efficient, probabilistically
sound algorithm for segmentation and word dis-
covery. Machine Learning, 34(1-3): 71-105.
Daland, Robert and Pierrehumbert, Janet B. 2011.
Learning Diphone-Based Segmentation. Cognitive
science, 35(1). Wiley Online Library. 119–155.
Daland, Robert, Hayes, Bruce, White, James,
Garellek, Marc, Davis, Andrea and Norrmann, In-
grid. 2011. Explaining sonority projection effects.
Phonology, 28(2): 197-234.
Dale, P. S., &amp; Fenson, L. 1996. Lexical development
norms for young children. Behavior Research
Methods, Instruments, &amp; Computers, 28, 125-127.
Goldwater, Sharon, Griffiths, Thomas L. and Johnson,
Mark. 2009. A Bayesian framework for word seg-
mentation: Exploring the effects of context. Cogni-
tion, 112(1): 21-54.
Hayes, Bruce &amp; Wilson, Colin. 2008. A maximum
entropy model of phonotactics and phonotactic
learning. Linguistic Inquiry, 39(3): 379-440.
Hockema, Stephen A. 2006. Finding Words in
Speech: An Investigation of American English.
Language Learning and Development, 2(2). Psy-
chology Press. 119-146.
Jarosz, Gaja and Johnson, J. Alex. 2013. The Rich-
ness of Distributional Cues to Word Boundaries in
Speech to Young Children. Language Learning
and Development, 9(2): 175-210.
Johnson, Mark. 2008a. Using adaptor grammars to
identify synergies in the unsupervised acquisition
of linguistic structure. Proceedings of the 46th An-
nual Meeting of the Association for Computational
Linguistics. Association for Computational Lin-
guistics.
Johnson, Mark. 2008b. Unsupervised Word Segmen-
tation for Sesotho Using Adaptor Grammars. Pro-
ceedings of the 10th Meeting of ACL SIGMOR-
PHON. Columbus, OH: Association of Computa-
tional Linguistics.
Johnson, Mark &amp; Goldwater, Sharon. 2009. Improv-
ing nonparametric Bayesian inference: Experi-
ments on unsupervised word segmentation with
adaptor grammars. In NAACL ’09: Proceedings of
the Annual Conference of the North American
Chapter of the Association for Computational Lin-
guistics. Boulder, CO: Association for Computa-
tional Linguistics.
Jurafsky, Daniel &amp; Martin, James H. 2008. Speech
and language processing, 2nd edition. Upper Sad-
dle River, NJ: Prentice-Hall.
Jusczyk, Peter W. &amp; Luce, Paul A. 1994. Infants’
Sensitivity to Phonotactic Patterns in the Native
Language. Journal of Memory and Language,
33(5): 630-645.
Lignos, Constantine and Yang, Charles. 2010. Reces-
sion Segmentation: Simpler Online Word Segmen-
tation Using Limited Resources. Proceedings of
the Fourteenth Conference on Computational Nat-
ural Language Learning. (CoNLL &apos;10). Associa-
tion for Computational Linguistics. .
Mattys, Sven L. and Jusczyk, Peter W. 2000. Phono-
tactic cues for segmentation of fluent speech by in-
fants. Cognition, 78(2): 91-121.
Mattys, Sven L., Jusczyk, Peter W., Luce, Paul A.,
and Morgan, James L. 1999. Phonotactic and pro-
sodic effects on word segmentation in infants.
Cognitive psychology, 38(4): 465-494.
Newport, Elissa L. and Aslin, Richard N. 2004.
Learning at a distance I. Statistical learning of non-
adjacent dependencies. Cognitive psychology,
48(2): 127-162.
Pelucchi, Bruna, Hay, Jessica F., and Saffran, Jenny
R. 2009. Learning in reverse: Eight-month-old in-
fants track backward transitional probabilities.
Cognition, 113(2): 244-247.
Saffran, Jenny R., Aslin, Richard N., and Newport,
Elissa L. 1996. Statistical learning by 8-month-old
infants. Science, 274(5294): 1926-1928.
Swingley, Daniel. 2005. Statistical clustering and the
contents of the infant vocabulary. Cognitive Psy-
chology, 50(1): 86-32.
</reference>
<page confidence="0.969322">
27
</page>
<reference confidence="0.9562623">
Thiessen, Erik D. and Saffran, Jenny R. 2003. When
cues collide: use of stress and statistical cues to
word boundaries by 7-to 9-month-old infants. De-
velopmental psychology, 39(4): 706.
Venkataraman, Anand 2001. A statistical model for
word discovery in transcribed speech. Computa-
tional Linguistics, 27(3): 352-372.
Yang, Charles D. 2004. Universal Grammar, statistics
or both? Trends in Cognitive Sciences, 8(10): 451-
456.
</reference>
<page confidence="0.999069">
28
</page>
</variant>
</algorithm>
<algorithm name="ParsHed" version="110505">
<variant no="0" confidence="0.968176">
<title confidence="0.99993">Comparing Models of Phonotactics for Word Segmentation</title>
<author confidence="0.999965">Natalie M Schrimpf Gaja Jarosz</author>
<affiliation confidence="0.9986445">Department of Linguistics Department of Linguistics Yale University Yale University</affiliation>
<email confidence="0.998862">natalie.schrimpf@yale.edugaja.jarosz@yale.edu</email>
<abstract confidence="0.998820666666667">Developmental research indicates that infants use low-level statistical regularities, or phonotactics, to segment words from continuous speech. In this paper, we present a segmentation framework that enables the direct comparison of different phonotactic models for segmentation. We compare a model using phoneme transitional probabilities, which have been widely used in computational models, to syllable-based bigram models, which have played a prominent role in the developmental literature. We also introduce a novel estimation method, and compare it to other strategies for estimating the parameters of the phonotactic models from unsegmented data. The results show that syllable-based models outperform the phoneme models, specifically in the context of improved unsupervised parameter estimation. The syllablebased transitional probability model achieves a word token f-score of nearly 80%, the highest reported performance for a phonotactic segmentation model with no lexicon.</abstract>
</variant>
</algorithm>
<algorithm name="ParsCit" version="110505">
<citationList>
<citation valid="true">
<authors>
<author>Frans Adriaans</author>
<author>René Kager</author>
</authors>
<title>Adding generalization to statistical learning: The induction of phonotactics from continuous speech.</title>
<date>2010</date>
<journal>Journal of Memory and Language</journal>
<volume>62</volume>
<issue>3</issue>
<pages>311--331</pages>
<contexts>
<context position="4481" citStr="Adriaans &amp; Kager, 2010" startWordPosition="644" endWordPosition="647">ed to identify word boundaries in natural language data. 19 Proceedings of the 2014 Joint Meeting of SIGMORPHON and SIGFSM, pages 19–28, Baltimore, Maryland USA, June 27 2014. c�2014 Association for Computational Linguistics While the experimental work emphasizes syllable-level transitional probability, recent computational modeling work and corpus analyses have primarily focused on the utility of phoneme-level statistics. A number of phonotactically-based segmentation models, focusing on the discovery of word boundaries based on phoneme-level statistics, have achieved more promising results (Adriaans &amp; Kager, 2010; Daland &amp; Pierrehumbert, 2011; see also Brent, 1999). For example, Brent (1999) showed that a local minimum strategy relying on phoneme bigrams correctly extracts about 50% of word tokens in English child-directed speech. Corpus analyses of child-directed speech have also highlighted the information content of phoneme-level statistics (Hockema, 2006; Jarosz &amp; Johnson, 2013). Related work has shown that phonotactic information can improve the performance of state-ofthe-art segmentation models whose primary objective is to discover the lexicon that underlies the regularities in the continuous s</context>
<context position="7641" citStr="Adriaans &amp; Kager, 2010" startWordPosition="1108" endWordPosition="1111">tives designed to deal with the challenges of unsupervised estimation of transitional probabilities from unsegmented input. Our focus on the early phonotactic segmentation stage differentiates our approach from many computational models emphasizing the discovery of the lexicon and higher-level language structure (Brent, 1999; Venkataraman, 2001; Swingley, 2005; Johnson, 2008a; Goldwater et al., 2009; Johnson &amp; Goldwater, 2009; Blanchard &amp; Heinz 2008, 2010; Lignos &amp; Yang, 2010). It complements that of recent work investigating the use of phoneme-level statistical regularities for segmentation (Adriaans &amp; Kager, 2010; Daland &amp; Pierrehumbert, 2011). Our work differs from these latter approaches, however, in comparing several phonotactic models, including ones relying on the syllable-based transitional probability statistics investigated in infant research. Our work also contributes to existing segmentation work that assumes a syllabified input (Yang, 2004; Swingley, 2005; Lignos &amp; Yang, 2010) by showing how many aspects of syllable structure can be inferred. Our results reveal an interaction between estimation strategy and the choice of phonotactic model. The local minimum segmentation strategy works poorl</context>
</contexts>
<marker>Adriaans, Kager, 2010</marker>
<rawString>Adriaans, Frans and Kager, René. 2010. Adding generalization to statistical learning: The induction of phonotactics from continuous speech. Journal of Memory and Language 62(3): 311-331.</rawString>
</citation>
<citation valid="true">
<authors>
<author>Adam Albright</author>
</authors>
<title>Feature-based generalisation as a source of gradient acceptability.</title>
<date>2009</date>
<journal>Phonology,</journal>
<volume>26</volume>
<issue>1</issue>
<pages>9--41</pages>
<contexts>
<context position="33625" citStr="Albright, 2009" startWordPosition="5160" endWordPosition="5161">cial experimental stimuli do provide a substantial amount of information about word boundaries in natural language data, at least in English. This lends significant credibility to the claim that sensitivity to such statistical regularities plays a crucial role in infants’ early language development (contra Yang 2004). This result also highlights the role that sensitivity to richer phonological information, beyond the level of phonemes, plays in language learning, a result that is echoed in much recent work on the modeling of phonotactic well-formedness of isolated words (Hayes &amp; Wilson, 2008; Albright, 2009; Daland et al., 2011). A consistent finding of this work has been that access to abstract structure and robust generalization mechanisms is crucial to the modeling of human phonotactic knowledge. While our results are compatible with these conclusions, our results cannot confirm that it is syllable structure per se that improves segmentation since the syllable-based models have several co-occurring advantages. In addition to abstract structure, they can track longer and more complex dependencies. Nonetheless, these results motivate further investigation into the role that richer models of pho</context>
</contexts>
<marker>Albright, 2009</marker>
<rawString>Albright, Adam. 2009. Feature-based generalisation as a source of gradient acceptability. Phonology, 26(1): 9-41.</rawString>
</citation>
<citation valid="true">
<authors>
<author>Richard N Aslin</author>
<author>Jenny R Saffran</author>
<author>Elissa L Newport</author>
</authors>
<title>Computation of conditional probability statistics by 8-month-old infants.</title>
<date>1998</date>
<journal>Psychological Science,</journal>
<volume>9</volume>
<pages>321--324</pages>
<contexts>
<context position="2582" citStr="Aslin et al., 1998" startWordPosition="370" endWordPosition="373">esearch indicates that on average infants know fewer than 100 word types during this period (Dale &amp; Fenson, 1996; Daland &amp; Pierrehumbert, 2011). One statistical cue that has received a great deal of support in experimental work on infant speech segmentation is transitional probability calculated over syllables. In foundational work, Saffran et al. (1996) found that infants are able to segment words from continuous speech using statistical regularities between syllables. Numerous subsequent studies have confirmed that infants can track transitional probabilities and use them to segment speech (Aslin et al., 1998; Thiessen &amp; Saffran, 2003; Pelucchi et al., 2009). Despite the extensive experimental literature demonstrating infants’ sensitivity to transitional probability in an artificial language learning setting, the utility of these statistical cues in a natural language learning context is disputed. Yang (2004) shows that a segmentation strategy relying on transitional probabilities over syllables achieves very poor results on English childdirected speech, even when the input is perfectly syllabified. Yang implements the local minimum segmentation strategy proposed by Saffran et al. (1996) wherein w</context>
</contexts>
<marker>Aslin, Saffran, Newport, 1998</marker>
<rawString>Aslin, Richard N., Saffran, Jenny R., &amp; Newport, Elissa L. 1998. Computation of conditional probability statistics by 8-month-old infants. Psychological Science, 9, 321-324.</rawString>
</citation>
<citation valid="true">
<authors>
<author>Nan Bernstein-Ratner</author>
</authors>
<title>The phonology of parent child speech.</title>
<date>1987</date>
<journal>Children’s Language,</journal>
<volume>6</volume>
<pages>159--174</pages>
<contexts>
<context position="23015" citStr="Bernstein-Ratner (1987)" startWordPosition="3460" endWordPosition="3461"> then recomputes the optimal segmentation of the corpus based on the new parameters and repeats until convergence. This method is motivated by previous segmentation work highlighting the effectiveness of greedy re-estimation 23 techniques (Brent, 1999; Venkataraman, 2001; Goldwater et al., 2009; Johnson &amp; Goldwater, 2009). As noted in previous work, such greedy re-estimation has the potential to infer additional word boundaries based on commitments made to word boundaries on earlier passes. 5 Experiments 5.1 Corpus The experiments for all the models were run on the Brent (1999) version of the Bernstein-Ratner (1987) corpus of English child-directed speech consisting of phonetically transcribed utterances. This corpus has been widely used for evaluating segmentation models. Other models evaluated on this corpus include those of Brent (1999), Venkataraman (2001), Blanchard and Heinz (2008), and Johnson and Goldwater (2009). 5.2 Evaluation Precision, recall, and f-scores of both word tokens and boundaries were used to evaluate performance. For the models with iterative reestimation, the reported performance scores are taken from the iteration after convergence. This typically happened after 5-10 iterations.</context>
</contexts>
<marker>Bernstein-Ratner, 1987</marker>
<rawString>Bernstein-Ratner, Nan. 1987. The phonology of parent child speech. Children’s Language, 6: 159-174.</rawString>
</citation>
<citation valid="true">
<authors>
<author>Daniel Blanchard</author>
<author>Jeffrey Heinz</author>
</authors>
<title>Improving word segmentation by simultaneously learning phonotactics.</title>
<date>2008</date>
<booktitle>In Conll ’08: Proceedings of the 12th Conference on Computational Natural Language Learning.</booktitle>
<publisher>Association</publisher>
<institution>for Computational Linguistics.</institution>
<location>Stroudsburg, PA:</location>
<contexts>
<context position="23292" citStr="Blanchard and Heinz (2008)" startWordPosition="3496" endWordPosition="3499">oldwater et al., 2009; Johnson &amp; Goldwater, 2009). As noted in previous work, such greedy re-estimation has the potential to infer additional word boundaries based on commitments made to word boundaries on earlier passes. 5 Experiments 5.1 Corpus The experiments for all the models were run on the Brent (1999) version of the Bernstein-Ratner (1987) corpus of English child-directed speech consisting of phonetically transcribed utterances. This corpus has been widely used for evaluating segmentation models. Other models evaluated on this corpus include those of Brent (1999), Venkataraman (2001), Blanchard and Heinz (2008), and Johnson and Goldwater (2009). 5.2 Evaluation Precision, recall, and f-scores of both word tokens and boundaries were used to evaluate performance. For the models with iterative reestimation, the reported performance scores are taken from the iteration after convergence. This typically happened after 5-10 iterations. 5.3 Results and Discussion Table 1 summarizes the word boundary and word token f-scores for all models, while Table 2 presents the precision and recall scores for the best-performing adjusted count models and the local minimum models. Focusing first on the local minimum estim</context>
<context position="5189" citStr="Blanchard &amp; Heinz 2008" startWordPosition="750" endWordPosition="753">d that a local minimum strategy relying on phoneme bigrams correctly extracts about 50% of word tokens in English child-directed speech. Corpus analyses of child-directed speech have also highlighted the information content of phoneme-level statistics (Hockema, 2006; Jarosz &amp; Johnson, 2013). Related work has shown that phonotactic information can improve the performance of state-ofthe-art segmentation models whose primary objective is to discover the lexicon that underlies the regularities in the continuous speech signal. Again, this work has largely emphasized phoneme-level statistical cues (Blanchard &amp; Heinz 2008, 2010), and those models that do rely on syllable structure (Johnson, 2008a; Johnson &amp; Goldwater, 2009), do not directly encode sequential statistics between adjacent syllables of the sort investigated in the infant literature. Finally, some models assume computations are performed over syllables and that all word boundaries in the input are aligned with syllable boundaries, but provide no mechanism by which such language-specific syllabification principles could be learned (Yang, 2004; Swingley, 2005; Lignos &amp; Yang, 2010). Overall, the existing evidence clearly shows that there are phonotact</context>
<context position="7472" citStr="Blanchard &amp; Heinz 2008" startWordPosition="1084" endWordPosition="1087">ng statistics estimated from unsyllabified, unsegmented utterances. We also compare the local minimum segmentation strategy (Saffran et al., 1996; Yang, 2004) to alternatives designed to deal with the challenges of unsupervised estimation of transitional probabilities from unsegmented input. Our focus on the early phonotactic segmentation stage differentiates our approach from many computational models emphasizing the discovery of the lexicon and higher-level language structure (Brent, 1999; Venkataraman, 2001; Swingley, 2005; Johnson, 2008a; Goldwater et al., 2009; Johnson &amp; Goldwater, 2009; Blanchard &amp; Heinz 2008, 2010; Lignos &amp; Yang, 2010). It complements that of recent work investigating the use of phoneme-level statistical regularities for segmentation (Adriaans &amp; Kager, 2010; Daland &amp; Pierrehumbert, 2011). Our work differs from these latter approaches, however, in comparing several phonotactic models, including ones relying on the syllable-based transitional probability statistics investigated in infant research. Our work also contributes to existing segmentation work that assumes a syllabified input (Yang, 2004; Swingley, 2005; Lignos &amp; Yang, 2010) by showing how many aspects of syllable structur</context>
</contexts>
<marker>Blanchard, Heinz, 2008</marker>
<rawString>Blanchard, Daniel and Heinz, Jeffrey. 2008. Improving word segmentation by simultaneously learning phonotactics. In Conll ’08: Proceedings of the 12th Conference on Computational Natural Language Learning. Stroudsburg, PA: Association for Computational Linguistics.</rawString>
</citation>
<citation valid="true">
<authors>
<author>Daniel Blanchard</author>
<author>Jeffrey Heinz</author>
<author>Roberta Golinkoff</author>
</authors>
<title>Modeling the contribution of phonotactic cues to the problem of word segmentation.</title>
<date>2010</date>
<journal>Journal of Child Language,</journal>
<volume>37</volume>
<issue>3</issue>
<pages>487--511</pages>
<marker>Blanchard, Heinz, Golinkoff, 2010</marker>
<rawString>Blanchard, Daniel, Heinz, Jeffrey and Golinkoff, Roberta. 2010. Modeling the contribution of phonotactic cues to the problem of word segmentation. Journal of Child Language, 37(3): 487-511.</rawString>
</citation>
<citation valid="true">
<authors>
<author>Michael R Brent</author>
</authors>
<title>An efficient, probabilistically sound algorithm for segmentation and word discovery.</title>
<date>1999</date>
<booktitle>Machine Learning,</booktitle>
<pages>34--1</pages>
<contexts>
<context position="4534" citStr="Brent, 1999" startWordPosition="654" endWordPosition="655">ceedings of the 2014 Joint Meeting of SIGMORPHON and SIGFSM, pages 19–28, Baltimore, Maryland USA, June 27 2014. c�2014 Association for Computational Linguistics While the experimental work emphasizes syllable-level transitional probability, recent computational modeling work and corpus analyses have primarily focused on the utility of phoneme-level statistics. A number of phonotactically-based segmentation models, focusing on the discovery of word boundaries based on phoneme-level statistics, have achieved more promising results (Adriaans &amp; Kager, 2010; Daland &amp; Pierrehumbert, 2011; see also Brent, 1999). For example, Brent (1999) showed that a local minimum strategy relying on phoneme bigrams correctly extracts about 50% of word tokens in English child-directed speech. Corpus analyses of child-directed speech have also highlighted the information content of phoneme-level statistics (Hockema, 2006; Jarosz &amp; Johnson, 2013). Related work has shown that phonotactic information can improve the performance of state-ofthe-art segmentation models whose primary objective is to discover the lexicon that underlies the regularities in the continuous speech signal. Again, this work has largely emphasized</context>
<context position="7345" citStr="Brent, 1999" startWordPosition="1067" endWordPosition="1068"> present a simple syllabification method that can be used to model phonotactic probability for arbitrary strings using statistics estimated from unsyllabified, unsegmented utterances. We also compare the local minimum segmentation strategy (Saffran et al., 1996; Yang, 2004) to alternatives designed to deal with the challenges of unsupervised estimation of transitional probabilities from unsegmented input. Our focus on the early phonotactic segmentation stage differentiates our approach from many computational models emphasizing the discovery of the lexicon and higher-level language structure (Brent, 1999; Venkataraman, 2001; Swingley, 2005; Johnson, 2008a; Goldwater et al., 2009; Johnson &amp; Goldwater, 2009; Blanchard &amp; Heinz 2008, 2010; Lignos &amp; Yang, 2010). It complements that of recent work investigating the use of phoneme-level statistical regularities for segmentation (Adriaans &amp; Kager, 2010; Daland &amp; Pierrehumbert, 2011). Our work differs from these latter approaches, however, in comparing several phonotactic models, including ones relying on the syllable-based transitional probability statistics investigated in infant research. Our work also contributes to existing segmentation work that</context>
<context position="9036" citStr="Brent, 1999" startWordPosition="1309" endWordPosition="1310"> probability model with improved unsupervised parameter estimation, the syllablebased models substantially outperform the phoneme-based models. Indeed, the syllable-based transitional probability phonotactic model achieves a word token segmentation f-score of nearly 80%, which is the highest reported performance among purely phonotactically-based segmentation models (Adriaans &amp; Kager, 2010; Daland &amp; Pierrehumbert, 2011). Indeed, this performance compares favorably with state-of-theart segmentation models that involve learning of higher level regularities, such as the lexicon and collocations (Brent, 1999; Venkataraman, 2001; Johnson, 2008a; Goldwater et al., 2009; Johnson &amp; Goldwater, 2009), and demonstrates that good 20 segmentation performance can be achieved by exploiting simple syllable-level phonotactic cues. 2 Segmentation Model The proposed segmentation model defines the probability of an utterance in terms of an abstract phonotactic probability component that assigns word well-formedness probabilities to phoneme strings. The segmentation algorithm uses those probabilities to determine the maximum likelihood segmentation as defined by a simple generative model. Since the phonotactics a</context>
<context position="10869" citStr="Brent, 1999" startWordPosition="1577" endWordPosition="1578">s change how exactly P wi is defined, but the segmentation probability always depends directly on the word probabilities given by a particular phonotactic model. For example, for the utterance [lukwtmi] ‘lookatme’, the segmentation model compares different segmentations, such as [luk#w#tmi] and [luk#wt#mi] based on the phonotactic well-formedness of the posited words. 2.2 Segmentation Algorithm The segmentation algorithm computes and outputs the segmentation with the highest likelihood: argmaxwP w . The optimal segmentation is found using dynamic programming, as in several previous proposals (Brent, 1999; Venkataraman, 2001). Given an input utterance, the model considers placing word boundaries at different positions within the utterance without regard to phonotactics or syllable structure. The phonotactic probability of each posited word is calculated independently as it is considered and used to update the probability of segmentations utilizing that word. In this way, the segmentation component remains entirely divorced from the details of the phonotactic models. Crucially, this means the full space of possible segmentations is considered by the segmentation model regardless of the phonotac</context>
<context position="22643" citStr="Brent, 1999" startWordPosition="3403" endWordPosition="3404">optimal segmentation of the input corpus in a single pass. In addition to the above strategies, we also investigated a greedy, iterative re-estimation strategy that makes multiple passes through the corpus. This estimation method takes the output of the above methods and uses it to re-estimate (smoothed and adjusted) parameters for the phonotactic models. It then recomputes the optimal segmentation of the corpus based on the new parameters and repeats until convergence. This method is motivated by previous segmentation work highlighting the effectiveness of greedy re-estimation 23 techniques (Brent, 1999; Venkataraman, 2001; Goldwater et al., 2009; Johnson &amp; Goldwater, 2009). As noted in previous work, such greedy re-estimation has the potential to infer additional word boundaries based on commitments made to word boundaries on earlier passes. 5 Experiments 5.1 Corpus The experiments for all the models were run on the Brent (1999) version of the Bernstein-Ratner (1987) corpus of English child-directed speech consisting of phonetically transcribed utterances. This corpus has been widely used for evaluating segmentation models. Other models evaluated on this corpus include those of Brent (1999)</context>
<context position="31207" citStr="Brent, 1999" startWordPosition="4780" endWordPosition="4781"> of the unit over which bigrams are computed. This makes sense since boundaries are more likely to fall between larger units than between smaller units. Less expected is the fact that the optimal parameter values are high compared to the empirical rates of word boundaries in the true segmentation of the input corpus. For example, the true rate of utterance-internal word boundaries is around 30% at the phoneme level, yet the optimal p# value for phoneme bigrams is around 60%. The reason for this is that our generative model, like that of a number of previous models discussed in the literature (Brent, 1999; Venkataraman, 2001; Goldwater et al., 2009), has an inherent undersegmentation bias. Due to the way the phonotactic models are defined, there is a cost for every additional word boundary posited in the segmentation. This is because positing a boundary corresponds to the generation of an additional symbol, #, which otherwise does not have to be generated. Since generating a # is never done with 100% probability, doing so al25 ways incurs a cost relative to a segmentation where no such # has to be generated. The high optimal settings of the p# parameter reflect this inherent bias and enable th</context>
</contexts>
<marker>Brent, 1999</marker>
<rawString>Brent, Michael R. 1999. An efficient, probabilistically sound algorithm for segmentation and word discovery. Machine Learning, 34(1-3): 71-105.</rawString>
</citation>
<citation valid="true">
<authors>
<author>Robert Daland</author>
<author>Janet B Pierrehumbert</author>
</authors>
<title>Learning Diphone-Based Segmentation.</title>
<date>2011</date>
<journal>Cognitive science,</journal>
<volume>35</volume>
<issue>1</issue>
<pages>119--155</pages>
<publisher>Wiley Online Library.</publisher>
<contexts>
<context position="2107" citStr="Daland &amp; Pierrehumbert, 2011" startWordPosition="300" endWordPosition="303"> Mattys &amp; Jusczyk, 2001) and other low-level statistical regularities (Saffran et al., 1996; Thiessen &amp; Saffran, 2003; Pelucchi et al., 2009) to extract words from fluent speech before the age of one. This work has shown that infants utilize these low-level statistical regularities to segment speech during the second half of the first year of life before they have developed extensive vocabularies that could provide top-down lexical information to guide segmentation. Developmental research indicates that on average infants know fewer than 100 word types during this period (Dale &amp; Fenson, 1996; Daland &amp; Pierrehumbert, 2011). One statistical cue that has received a great deal of support in experimental work on infant speech segmentation is transitional probability calculated over syllables. In foundational work, Saffran et al. (1996) found that infants are able to segment words from continuous speech using statistical regularities between syllables. Numerous subsequent studies have confirmed that infants can track transitional probabilities and use them to segment speech (Aslin et al., 1998; Thiessen &amp; Saffran, 2003; Pelucchi et al., 2009). Despite the extensive experimental literature demonstrating infants’ sens</context>
<context position="4511" citStr="Daland &amp; Pierrehumbert, 2011" startWordPosition="648" endWordPosition="651">daries in natural language data. 19 Proceedings of the 2014 Joint Meeting of SIGMORPHON and SIGFSM, pages 19–28, Baltimore, Maryland USA, June 27 2014. c�2014 Association for Computational Linguistics While the experimental work emphasizes syllable-level transitional probability, recent computational modeling work and corpus analyses have primarily focused on the utility of phoneme-level statistics. A number of phonotactically-based segmentation models, focusing on the discovery of word boundaries based on phoneme-level statistics, have achieved more promising results (Adriaans &amp; Kager, 2010; Daland &amp; Pierrehumbert, 2011; see also Brent, 1999). For example, Brent (1999) showed that a local minimum strategy relying on phoneme bigrams correctly extracts about 50% of word tokens in English child-directed speech. Corpus analyses of child-directed speech have also highlighted the information content of phoneme-level statistics (Hockema, 2006; Jarosz &amp; Johnson, 2013). Related work has shown that phonotactic information can improve the performance of state-ofthe-art segmentation models whose primary objective is to discover the lexicon that underlies the regularities in the continuous speech signal. Again, this work</context>
<context position="7672" citStr="Daland &amp; Pierrehumbert, 2011" startWordPosition="1112" endWordPosition="1116">ith the challenges of unsupervised estimation of transitional probabilities from unsegmented input. Our focus on the early phonotactic segmentation stage differentiates our approach from many computational models emphasizing the discovery of the lexicon and higher-level language structure (Brent, 1999; Venkataraman, 2001; Swingley, 2005; Johnson, 2008a; Goldwater et al., 2009; Johnson &amp; Goldwater, 2009; Blanchard &amp; Heinz 2008, 2010; Lignos &amp; Yang, 2010). It complements that of recent work investigating the use of phoneme-level statistical regularities for segmentation (Adriaans &amp; Kager, 2010; Daland &amp; Pierrehumbert, 2011). Our work differs from these latter approaches, however, in comparing several phonotactic models, including ones relying on the syllable-based transitional probability statistics investigated in infant research. Our work also contributes to existing segmentation work that assumes a syllabified input (Yang, 2004; Swingley, 2005; Lignos &amp; Yang, 2010) by showing how many aspects of syllable structure can be inferred. Our results reveal an interaction between estimation strategy and the choice of phonotactic model. The local minimum segmentation strategy works poorly in general for all models con</context>
</contexts>
<marker>Daland, Pierrehumbert, 2011</marker>
<rawString>Daland, Robert and Pierrehumbert, Janet B. 2011. Learning Diphone-Based Segmentation. Cognitive science, 35(1). Wiley Online Library. 119–155.</rawString>
</citation>
<citation valid="true">
<authors>
<author>Robert Daland</author>
<author>Bruce Hayes</author>
<author>James White</author>
<author>Marc Garellek</author>
<author>Andrea Davis</author>
<author>Ingrid Norrmann</author>
</authors>
<title>Explaining sonority projection effects.</title>
<date>2011</date>
<journal>Phonology,</journal>
<volume>28</volume>
<issue>2</issue>
<pages>197--234</pages>
<contexts>
<context position="33647" citStr="Daland et al., 2011" startWordPosition="5162" endWordPosition="5165">l stimuli do provide a substantial amount of information about word boundaries in natural language data, at least in English. This lends significant credibility to the claim that sensitivity to such statistical regularities plays a crucial role in infants’ early language development (contra Yang 2004). This result also highlights the role that sensitivity to richer phonological information, beyond the level of phonemes, plays in language learning, a result that is echoed in much recent work on the modeling of phonotactic well-formedness of isolated words (Hayes &amp; Wilson, 2008; Albright, 2009; Daland et al., 2011). A consistent finding of this work has been that access to abstract structure and robust generalization mechanisms is crucial to the modeling of human phonotactic knowledge. While our results are compatible with these conclusions, our results cannot confirm that it is syllable structure per se that improves segmentation since the syllable-based models have several co-occurring advantages. In addition to abstract structure, they can track longer and more complex dependencies. Nonetheless, these results motivate further investigation into the role that richer models of phonotactics may play in </context>
</contexts>
<marker>Daland, Hayes, White, Garellek, Davis, Norrmann, 2011</marker>
<rawString>Daland, Robert, Hayes, Bruce, White, James, Garellek, Marc, Davis, Andrea and Norrmann, Ingrid. 2011. Explaining sonority projection effects. Phonology, 28(2): 197-234.</rawString>
</citation>
<citation valid="true">
<authors>
<author>P S Dale</author>
<author>L Fenson</author>
</authors>
<title>Lexical development norms for young children.</title>
<date>1996</date>
<journal>Behavior Research Methods, Instruments, &amp; Computers,</journal>
<volume>28</volume>
<pages>125--127</pages>
<contexts>
<context position="2076" citStr="Dale &amp; Fenson, 1996" startWordPosition="296" endWordPosition="299"> Mattys et al., 1999; Mattys &amp; Jusczyk, 2001) and other low-level statistical regularities (Saffran et al., 1996; Thiessen &amp; Saffran, 2003; Pelucchi et al., 2009) to extract words from fluent speech before the age of one. This work has shown that infants utilize these low-level statistical regularities to segment speech during the second half of the first year of life before they have developed extensive vocabularies that could provide top-down lexical information to guide segmentation. Developmental research indicates that on average infants know fewer than 100 word types during this period (Dale &amp; Fenson, 1996; Daland &amp; Pierrehumbert, 2011). One statistical cue that has received a great deal of support in experimental work on infant speech segmentation is transitional probability calculated over syllables. In foundational work, Saffran et al. (1996) found that infants are able to segment words from continuous speech using statistical regularities between syllables. Numerous subsequent studies have confirmed that infants can track transitional probabilities and use them to segment speech (Aslin et al., 1998; Thiessen &amp; Saffran, 2003; Pelucchi et al., 2009). Despite the extensive experimental literat</context>
</contexts>
<marker>Dale, Fenson, 1996</marker>
<rawString>Dale, P. S., &amp; Fenson, L. 1996. Lexical development norms for young children. Behavior Research Methods, Instruments, &amp; Computers, 28, 125-127.</rawString>
</citation>
<citation valid="true">
<authors>
<author>Sharon Goldwater</author>
<author>Thomas L Griffiths</author>
<author>Mark Johnson</author>
</authors>
<title>A Bayesian framework for word segmentation: Exploring the effects of context.</title>
<date>2009</date>
<journal>Cognition,</journal>
<volume>112</volume>
<issue>1</issue>
<pages>21--54</pages>
<contexts>
<context position="7421" citStr="Goldwater et al., 2009" startWordPosition="1076" endWordPosition="1079">l phonotactic probability for arbitrary strings using statistics estimated from unsyllabified, unsegmented utterances. We also compare the local minimum segmentation strategy (Saffran et al., 1996; Yang, 2004) to alternatives designed to deal with the challenges of unsupervised estimation of transitional probabilities from unsegmented input. Our focus on the early phonotactic segmentation stage differentiates our approach from many computational models emphasizing the discovery of the lexicon and higher-level language structure (Brent, 1999; Venkataraman, 2001; Swingley, 2005; Johnson, 2008a; Goldwater et al., 2009; Johnson &amp; Goldwater, 2009; Blanchard &amp; Heinz 2008, 2010; Lignos &amp; Yang, 2010). It complements that of recent work investigating the use of phoneme-level statistical regularities for segmentation (Adriaans &amp; Kager, 2010; Daland &amp; Pierrehumbert, 2011). Our work differs from these latter approaches, however, in comparing several phonotactic models, including ones relying on the syllable-based transitional probability statistics investigated in infant research. Our work also contributes to existing segmentation work that assumes a syllabified input (Yang, 2004; Swingley, 2005; Lignos &amp; Yang, 201</context>
<context position="9096" citStr="Goldwater et al., 2009" startWordPosition="1315" endWordPosition="1318">ameter estimation, the syllablebased models substantially outperform the phoneme-based models. Indeed, the syllable-based transitional probability phonotactic model achieves a word token segmentation f-score of nearly 80%, which is the highest reported performance among purely phonotactically-based segmentation models (Adriaans &amp; Kager, 2010; Daland &amp; Pierrehumbert, 2011). Indeed, this performance compares favorably with state-of-theart segmentation models that involve learning of higher level regularities, such as the lexicon and collocations (Brent, 1999; Venkataraman, 2001; Johnson, 2008a; Goldwater et al., 2009; Johnson &amp; Goldwater, 2009), and demonstrates that good 20 segmentation performance can be achieved by exploiting simple syllable-level phonotactic cues. 2 Segmentation Model The proposed segmentation model defines the probability of an utterance in terms of an abstract phonotactic probability component that assigns word well-formedness probabilities to phoneme strings. The segmentation algorithm uses those probabilities to determine the maximum likelihood segmentation as defined by a simple generative model. Since the phonotactics and segmentation components are separate, they can be indepen</context>
<context position="22687" citStr="Goldwater et al., 2009" startWordPosition="3407" endWordPosition="3410"> corpus in a single pass. In addition to the above strategies, we also investigated a greedy, iterative re-estimation strategy that makes multiple passes through the corpus. This estimation method takes the output of the above methods and uses it to re-estimate (smoothed and adjusted) parameters for the phonotactic models. It then recomputes the optimal segmentation of the corpus based on the new parameters and repeats until convergence. This method is motivated by previous segmentation work highlighting the effectiveness of greedy re-estimation 23 techniques (Brent, 1999; Venkataraman, 2001; Goldwater et al., 2009; Johnson &amp; Goldwater, 2009). As noted in previous work, such greedy re-estimation has the potential to infer additional word boundaries based on commitments made to word boundaries on earlier passes. 5 Experiments 5.1 Corpus The experiments for all the models were run on the Brent (1999) version of the Bernstein-Ratner (1987) corpus of English child-directed speech consisting of phonetically transcribed utterances. This corpus has been widely used for evaluating segmentation models. Other models evaluated on this corpus include those of Brent (1999), Venkataraman (2001), Blanchard and Heinz (</context>
<context position="31252" citStr="Goldwater et al., 2009" startWordPosition="4785" endWordPosition="4788">re computed. This makes sense since boundaries are more likely to fall between larger units than between smaller units. Less expected is the fact that the optimal parameter values are high compared to the empirical rates of word boundaries in the true segmentation of the input corpus. For example, the true rate of utterance-internal word boundaries is around 30% at the phoneme level, yet the optimal p# value for phoneme bigrams is around 60%. The reason for this is that our generative model, like that of a number of previous models discussed in the literature (Brent, 1999; Venkataraman, 2001; Goldwater et al., 2009), has an inherent undersegmentation bias. Due to the way the phonotactic models are defined, there is a cost for every additional word boundary posited in the segmentation. This is because positing a boundary corresponds to the generation of an additional symbol, #, which otherwise does not have to be generated. Since generating a # is never done with 100% probability, doing so al25 ways incurs a cost relative to a segmentation where no such # has to be generated. The high optimal settings of the p# parameter reflect this inherent bias and enable the estimation procedure to compensate not only</context>
<context position="35183" citStr="Goldwater et al. (2009)" startWordPosition="5375" endWordPosition="5378"> relatively low performance of state-ofthe-art lexicon-building models (Johnson 2008b). Another important direction for future work is investigating how early, phonotactically-based segmentation interacts with subsequent learning of higher-level structure, including the lexicon. Johnson (2008a) and Johnson &amp; Goldwater (2009) have already demonstrated that syllable structure provides valuable information in this context; however, their models relied on very different syllable regularities than those investigated here, and the consequences of these differences should be explored in future work. Goldwater et al. (2009) showed that a number of proposed segmentation models have an undersegmentation bias that can be avoided by simultaneously modeling statistical dependencies between words. They proposed a Bayesian prior to favor a smaller lexicon and showed that otherwise unigram models introduce a severe undersegmentation bias due to the possibility of matching empirical probabilities by memorizing utterances as words. Note that the same is not true of syllable-based models since the hypothesis space does not permit memorization of utterances, and the size of the syllable inventory, unlike a lexicon, remains </context>
</contexts>
<marker>Goldwater, Griffiths, Johnson, 2009</marker>
<rawString>Goldwater, Sharon, Griffiths, Thomas L. and Johnson, Mark. 2009. A Bayesian framework for word segmentation: Exploring the effects of context. Cognition, 112(1): 21-54.</rawString>
</citation>
<citation valid="true">
<authors>
<author>Bruce Hayes</author>
<author>Colin Wilson</author>
</authors>
<title>A maximum entropy model of phonotactics and phonotactic learning.</title>
<date>2008</date>
<journal>Linguistic Inquiry,</journal>
<volume>39</volume>
<issue>3</issue>
<pages>379--440</pages>
<contexts>
<context position="33609" citStr="Hayes &amp; Wilson, 2008" startWordPosition="5156" endWordPosition="5159">sensitive to in artificial experimental stimuli do provide a substantial amount of information about word boundaries in natural language data, at least in English. This lends significant credibility to the claim that sensitivity to such statistical regularities plays a crucial role in infants’ early language development (contra Yang 2004). This result also highlights the role that sensitivity to richer phonological information, beyond the level of phonemes, plays in language learning, a result that is echoed in much recent work on the modeling of phonotactic well-formedness of isolated words (Hayes &amp; Wilson, 2008; Albright, 2009; Daland et al., 2011). A consistent finding of this work has been that access to abstract structure and robust generalization mechanisms is crucial to the modeling of human phonotactic knowledge. While our results are compatible with these conclusions, our results cannot confirm that it is syllable structure per se that improves segmentation since the syllable-based models have several co-occurring advantages. In addition to abstract structure, they can track longer and more complex dependencies. Nonetheless, these results motivate further investigation into the role that rich</context>
</contexts>
<marker>Hayes, Wilson, 2008</marker>
<rawString>Hayes, Bruce &amp; Wilson, Colin. 2008. A maximum entropy model of phonotactics and phonotactic learning. Linguistic Inquiry, 39(3): 379-440.</rawString>
</citation>
<citation valid="true">
<authors>
<author>Stephen A Hockema</author>
</authors>
<title>Finding Words in Speech: An</title>
<date>2006</date>
<journal>Investigation of American English. Language Learning and Development,</journal>
<volume>2</volume>
<issue>2</issue>
<pages>119--146</pages>
<publisher>Psychology Press.</publisher>
<contexts>
<context position="4833" citStr="Hockema, 2006" startWordPosition="698" endWordPosition="699">ave primarily focused on the utility of phoneme-level statistics. A number of phonotactically-based segmentation models, focusing on the discovery of word boundaries based on phoneme-level statistics, have achieved more promising results (Adriaans &amp; Kager, 2010; Daland &amp; Pierrehumbert, 2011; see also Brent, 1999). For example, Brent (1999) showed that a local minimum strategy relying on phoneme bigrams correctly extracts about 50% of word tokens in English child-directed speech. Corpus analyses of child-directed speech have also highlighted the information content of phoneme-level statistics (Hockema, 2006; Jarosz &amp; Johnson, 2013). Related work has shown that phonotactic information can improve the performance of state-ofthe-art segmentation models whose primary objective is to discover the lexicon that underlies the regularities in the continuous speech signal. Again, this work has largely emphasized phoneme-level statistical cues (Blanchard &amp; Heinz 2008, 2010), and those models that do rely on syllable structure (Johnson, 2008a; Johnson &amp; Goldwater, 2009), do not directly encode sequential statistics between adjacent syllables of the sort investigated in the infant literature. Finally, some m</context>
</contexts>
<marker>Hockema, 2006</marker>
<rawString>Hockema, Stephen A. 2006. Finding Words in Speech: An Investigation of American English. Language Learning and Development, 2(2). Psychology Press. 119-146.</rawString>
</citation>
<citation valid="true">
<authors>
<author>Gaja Jarosz</author>
<author>J Alex Johnson</author>
</authors>
<title>The Richness of Distributional Cues to Word Boundaries in Speech to Young Children. Language Learning and Development,</title>
<date>2013</date>
<volume>9</volume>
<issue>2</issue>
<pages>175--210</pages>
<contexts>
<context position="4858" citStr="Jarosz &amp; Johnson, 2013" startWordPosition="700" endWordPosition="703">ocused on the utility of phoneme-level statistics. A number of phonotactically-based segmentation models, focusing on the discovery of word boundaries based on phoneme-level statistics, have achieved more promising results (Adriaans &amp; Kager, 2010; Daland &amp; Pierrehumbert, 2011; see also Brent, 1999). For example, Brent (1999) showed that a local minimum strategy relying on phoneme bigrams correctly extracts about 50% of word tokens in English child-directed speech. Corpus analyses of child-directed speech have also highlighted the information content of phoneme-level statistics (Hockema, 2006; Jarosz &amp; Johnson, 2013). Related work has shown that phonotactic information can improve the performance of state-ofthe-art segmentation models whose primary objective is to discover the lexicon that underlies the regularities in the continuous speech signal. Again, this work has largely emphasized phoneme-level statistical cues (Blanchard &amp; Heinz 2008, 2010), and those models that do rely on syllable structure (Johnson, 2008a; Johnson &amp; Goldwater, 2009), do not directly encode sequential statistics between adjacent syllables of the sort investigated in the infant literature. Finally, some models assume computations</context>
<context position="34550" citStr="Jarosz &amp; Johnson, 2013" startWordPosition="5291" endWordPosition="5294">tructure per se that improves segmentation since the syllable-based models have several co-occurring advantages. In addition to abstract structure, they can track longer and more complex dependencies. Nonetheless, these results motivate further investigation into the role that richer models of phonotactics may play in word segmentation and into the precise mechanisms responsible for improved segmentation using syllable structure. Particularly critical is exploration of phonotactically-based segmentation models for languages besides English, for which phonotactic cues hold significant promise (Jarosz &amp; Johnson, 2013) given the relatively low performance of state-ofthe-art lexicon-building models (Johnson 2008b). Another important direction for future work is investigating how early, phonotactically-based segmentation interacts with subsequent learning of higher-level structure, including the lexicon. Johnson (2008a) and Johnson &amp; Goldwater (2009) have already demonstrated that syllable structure provides valuable information in this context; however, their models relied on very different syllable regularities than those investigated here, and the consequences of these differences should be explored in fut</context>
</contexts>
<marker>Jarosz, Johnson, 2013</marker>
<rawString>Jarosz, Gaja and Johnson, J. Alex. 2013. The Richness of Distributional Cues to Word Boundaries in Speech to Young Children. Language Learning and Development, 9(2): 175-210.</rawString>
</citation>
<citation valid="true">
<authors>
<author>Mark Johnson</author>
</authors>
<title>Using adaptor grammars to identify synergies in the unsupervised acquisition of linguistic structure.</title>
<date>2008</date>
<booktitle>Proceedings of the 46th Annual Meeting of the Association for Computational Linguistics. Association for Computational Linguistics.</booktitle>
<contexts>
<context position="5264" citStr="Johnson, 2008" startWordPosition="764" endWordPosition="765">50% of word tokens in English child-directed speech. Corpus analyses of child-directed speech have also highlighted the information content of phoneme-level statistics (Hockema, 2006; Jarosz &amp; Johnson, 2013). Related work has shown that phonotactic information can improve the performance of state-ofthe-art segmentation models whose primary objective is to discover the lexicon that underlies the regularities in the continuous speech signal. Again, this work has largely emphasized phoneme-level statistical cues (Blanchard &amp; Heinz 2008, 2010), and those models that do rely on syllable structure (Johnson, 2008a; Johnson &amp; Goldwater, 2009), do not directly encode sequential statistics between adjacent syllables of the sort investigated in the infant literature. Finally, some models assume computations are performed over syllables and that all word boundaries in the input are aligned with syllable boundaries, but provide no mechanism by which such language-specific syllabification principles could be learned (Yang, 2004; Swingley, 2005; Lignos &amp; Yang, 2010). Overall, the existing evidence clearly shows that there are phonotactic cues to word boundaries in spontaneous, child-directed speech. However, </context>
<context position="7396" citStr="Johnson, 2008" startWordPosition="1074" endWordPosition="1075"> be used to model phonotactic probability for arbitrary strings using statistics estimated from unsyllabified, unsegmented utterances. We also compare the local minimum segmentation strategy (Saffran et al., 1996; Yang, 2004) to alternatives designed to deal with the challenges of unsupervised estimation of transitional probabilities from unsegmented input. Our focus on the early phonotactic segmentation stage differentiates our approach from many computational models emphasizing the discovery of the lexicon and higher-level language structure (Brent, 1999; Venkataraman, 2001; Swingley, 2005; Johnson, 2008a; Goldwater et al., 2009; Johnson &amp; Goldwater, 2009; Blanchard &amp; Heinz 2008, 2010; Lignos &amp; Yang, 2010). It complements that of recent work investigating the use of phoneme-level statistical regularities for segmentation (Adriaans &amp; Kager, 2010; Daland &amp; Pierrehumbert, 2011). Our work differs from these latter approaches, however, in comparing several phonotactic models, including ones relying on the syllable-based transitional probability statistics investigated in infant research. Our work also contributes to existing segmentation work that assumes a syllabified input (Yang, 2004; Swingley,</context>
<context position="9071" citStr="Johnson, 2008" startWordPosition="1313" endWordPosition="1314">unsupervised parameter estimation, the syllablebased models substantially outperform the phoneme-based models. Indeed, the syllable-based transitional probability phonotactic model achieves a word token segmentation f-score of nearly 80%, which is the highest reported performance among purely phonotactically-based segmentation models (Adriaans &amp; Kager, 2010; Daland &amp; Pierrehumbert, 2011). Indeed, this performance compares favorably with state-of-theart segmentation models that involve learning of higher level regularities, such as the lexicon and collocations (Brent, 1999; Venkataraman, 2001; Johnson, 2008a; Goldwater et al., 2009; Johnson &amp; Goldwater, 2009), and demonstrates that good 20 segmentation performance can be achieved by exploiting simple syllable-level phonotactic cues. 2 Segmentation Model The proposed segmentation model defines the probability of an utterance in terms of an abstract phonotactic probability component that assigns word well-formedness probabilities to phoneme strings. The segmentation algorithm uses those probabilities to determine the maximum likelihood segmentation as defined by a simple generative model. Since the phonotactics and segmentation components are sepa</context>
<context position="13968" citStr="Johnson, 2008" startWordPosition="2067" endWordPosition="2068">m the beginnings of utterances. When syllabifying an intervocalic sequence of consonants, this method finds the longest legal onset aligned with the right edge and places any remaining consonants in the coda of the previous syllable. Thus, a sequence like [wtmi] would be syllabified as [wt.mi] in English since [m] but not [tm] occurs utterance-initially. The only language-particular information required for this approach is knowledge of which phonemes are vowels (syllabic) and which are consonants, a limited type of information also assumed by other syllable inference models for segmentation (Johnson, 2008a; Johnson &amp; Goldwater, 2009). As the segmentation component posits potential words, they are passed to the phonotactic component for syllabification and phonotactic probability calculation. This differs crucially from previous work assuming a fixed syllabification of the input corpus in which word boundaries always align with syllable boundaries (Yang, 2004; Swingley, 2005; Lignos &amp; Yang, 2010). In a setting in which syllabification must be inferred from unsegmented utterances, the learner must be capable of assigning syllabification more flexibly since word boundaries do not always align wit</context>
<context position="34644" citStr="Johnson 2008" startWordPosition="5305" endWordPosition="5306">ntages. In addition to abstract structure, they can track longer and more complex dependencies. Nonetheless, these results motivate further investigation into the role that richer models of phonotactics may play in word segmentation and into the precise mechanisms responsible for improved segmentation using syllable structure. Particularly critical is exploration of phonotactically-based segmentation models for languages besides English, for which phonotactic cues hold significant promise (Jarosz &amp; Johnson, 2013) given the relatively low performance of state-ofthe-art lexicon-building models (Johnson 2008b). Another important direction for future work is investigating how early, phonotactically-based segmentation interacts with subsequent learning of higher-level structure, including the lexicon. Johnson (2008a) and Johnson &amp; Goldwater (2009) have already demonstrated that syllable structure provides valuable information in this context; however, their models relied on very different syllable regularities than those investigated here, and the consequences of these differences should be explored in future work. Goldwater et al. (2009) showed that a number of proposed segmentation models have an</context>
</contexts>
<marker>Johnson, 2008</marker>
<rawString>Johnson, Mark. 2008a. Using adaptor grammars to identify synergies in the unsupervised acquisition of linguistic structure. Proceedings of the 46th Annual Meeting of the Association for Computational Linguistics. Association for Computational Linguistics.</rawString>
</citation>
<citation valid="true">
<authors>
<author>Mark Johnson</author>
</authors>
<title>Unsupervised Word Segmentation for Sesotho Using Adaptor Grammars.</title>
<date>2008</date>
<booktitle>Proceedings of the 10th Meeting of ACL SIGMORPHON. Columbus, OH: Association of Computational Linguistics.</booktitle>
<contexts>
<context position="5264" citStr="Johnson, 2008" startWordPosition="764" endWordPosition="765">50% of word tokens in English child-directed speech. Corpus analyses of child-directed speech have also highlighted the information content of phoneme-level statistics (Hockema, 2006; Jarosz &amp; Johnson, 2013). Related work has shown that phonotactic information can improve the performance of state-ofthe-art segmentation models whose primary objective is to discover the lexicon that underlies the regularities in the continuous speech signal. Again, this work has largely emphasized phoneme-level statistical cues (Blanchard &amp; Heinz 2008, 2010), and those models that do rely on syllable structure (Johnson, 2008a; Johnson &amp; Goldwater, 2009), do not directly encode sequential statistics between adjacent syllables of the sort investigated in the infant literature. Finally, some models assume computations are performed over syllables and that all word boundaries in the input are aligned with syllable boundaries, but provide no mechanism by which such language-specific syllabification principles could be learned (Yang, 2004; Swingley, 2005; Lignos &amp; Yang, 2010). Overall, the existing evidence clearly shows that there are phonotactic cues to word boundaries in spontaneous, child-directed speech. However, </context>
<context position="7396" citStr="Johnson, 2008" startWordPosition="1074" endWordPosition="1075"> be used to model phonotactic probability for arbitrary strings using statistics estimated from unsyllabified, unsegmented utterances. We also compare the local minimum segmentation strategy (Saffran et al., 1996; Yang, 2004) to alternatives designed to deal with the challenges of unsupervised estimation of transitional probabilities from unsegmented input. Our focus on the early phonotactic segmentation stage differentiates our approach from many computational models emphasizing the discovery of the lexicon and higher-level language structure (Brent, 1999; Venkataraman, 2001; Swingley, 2005; Johnson, 2008a; Goldwater et al., 2009; Johnson &amp; Goldwater, 2009; Blanchard &amp; Heinz 2008, 2010; Lignos &amp; Yang, 2010). It complements that of recent work investigating the use of phoneme-level statistical regularities for segmentation (Adriaans &amp; Kager, 2010; Daland &amp; Pierrehumbert, 2011). Our work differs from these latter approaches, however, in comparing several phonotactic models, including ones relying on the syllable-based transitional probability statistics investigated in infant research. Our work also contributes to existing segmentation work that assumes a syllabified input (Yang, 2004; Swingley,</context>
<context position="9071" citStr="Johnson, 2008" startWordPosition="1313" endWordPosition="1314">unsupervised parameter estimation, the syllablebased models substantially outperform the phoneme-based models. Indeed, the syllable-based transitional probability phonotactic model achieves a word token segmentation f-score of nearly 80%, which is the highest reported performance among purely phonotactically-based segmentation models (Adriaans &amp; Kager, 2010; Daland &amp; Pierrehumbert, 2011). Indeed, this performance compares favorably with state-of-theart segmentation models that involve learning of higher level regularities, such as the lexicon and collocations (Brent, 1999; Venkataraman, 2001; Johnson, 2008a; Goldwater et al., 2009; Johnson &amp; Goldwater, 2009), and demonstrates that good 20 segmentation performance can be achieved by exploiting simple syllable-level phonotactic cues. 2 Segmentation Model The proposed segmentation model defines the probability of an utterance in terms of an abstract phonotactic probability component that assigns word well-formedness probabilities to phoneme strings. The segmentation algorithm uses those probabilities to determine the maximum likelihood segmentation as defined by a simple generative model. Since the phonotactics and segmentation components are sepa</context>
<context position="13968" citStr="Johnson, 2008" startWordPosition="2067" endWordPosition="2068">m the beginnings of utterances. When syllabifying an intervocalic sequence of consonants, this method finds the longest legal onset aligned with the right edge and places any remaining consonants in the coda of the previous syllable. Thus, a sequence like [wtmi] would be syllabified as [wt.mi] in English since [m] but not [tm] occurs utterance-initially. The only language-particular information required for this approach is knowledge of which phonemes are vowels (syllabic) and which are consonants, a limited type of information also assumed by other syllable inference models for segmentation (Johnson, 2008a; Johnson &amp; Goldwater, 2009). As the segmentation component posits potential words, they are passed to the phonotactic component for syllabification and phonotactic probability calculation. This differs crucially from previous work assuming a fixed syllabification of the input corpus in which word boundaries always align with syllable boundaries (Yang, 2004; Swingley, 2005; Lignos &amp; Yang, 2010). In a setting in which syllabification must be inferred from unsegmented utterances, the learner must be capable of assigning syllabification more flexibly since word boundaries do not always align wit</context>
<context position="34644" citStr="Johnson 2008" startWordPosition="5305" endWordPosition="5306">ntages. In addition to abstract structure, they can track longer and more complex dependencies. Nonetheless, these results motivate further investigation into the role that richer models of phonotactics may play in word segmentation and into the precise mechanisms responsible for improved segmentation using syllable structure. Particularly critical is exploration of phonotactically-based segmentation models for languages besides English, for which phonotactic cues hold significant promise (Jarosz &amp; Johnson, 2013) given the relatively low performance of state-ofthe-art lexicon-building models (Johnson 2008b). Another important direction for future work is investigating how early, phonotactically-based segmentation interacts with subsequent learning of higher-level structure, including the lexicon. Johnson (2008a) and Johnson &amp; Goldwater (2009) have already demonstrated that syllable structure provides valuable information in this context; however, their models relied on very different syllable regularities than those investigated here, and the consequences of these differences should be explored in future work. Goldwater et al. (2009) showed that a number of proposed segmentation models have an</context>
</contexts>
<marker>Johnson, 2008</marker>
<rawString>Johnson, Mark. 2008b. Unsupervised Word Segmentation for Sesotho Using Adaptor Grammars. Proceedings of the 10th Meeting of ACL SIGMORPHON. Columbus, OH: Association of Computational Linguistics.</rawString>
</citation>
<citation valid="true">
<authors>
<author>Mark Johnson</author>
<author>Sharon Goldwater</author>
</authors>
<title>Improving nonparametric Bayesian inference: Experiments on unsupervised word segmentation with adaptor grammars.</title>
<date>2009</date>
<booktitle>In NAACL ’09: Proceedings of the Annual Conference of the North American Chapter of the Association for Computational Linguistics.</booktitle>
<publisher>Association</publisher>
<institution>for Computational Linguistics.</institution>
<location>Boulder, CO:</location>
<contexts>
<context position="23326" citStr="Johnson and Goldwater (2009)" startWordPosition="3501" endWordPosition="3504"> Goldwater, 2009). As noted in previous work, such greedy re-estimation has the potential to infer additional word boundaries based on commitments made to word boundaries on earlier passes. 5 Experiments 5.1 Corpus The experiments for all the models were run on the Brent (1999) version of the Bernstein-Ratner (1987) corpus of English child-directed speech consisting of phonetically transcribed utterances. This corpus has been widely used for evaluating segmentation models. Other models evaluated on this corpus include those of Brent (1999), Venkataraman (2001), Blanchard and Heinz (2008), and Johnson and Goldwater (2009). 5.2 Evaluation Precision, recall, and f-scores of both word tokens and boundaries were used to evaluate performance. For the models with iterative reestimation, the reported performance scores are taken from the iteration after convergence. This typically happened after 5-10 iterations. 5.3 Results and Discussion Table 1 summarizes the word boundary and word token f-scores for all models, while Table 2 presents the precision and recall scores for the best-performing adjusted count models and the local minimum models. Focusing first on the local minimum estimateion strategy, there are several</context>
<context position="5293" citStr="Johnson &amp; Goldwater, 2009" startWordPosition="766" endWordPosition="769">ns in English child-directed speech. Corpus analyses of child-directed speech have also highlighted the information content of phoneme-level statistics (Hockema, 2006; Jarosz &amp; Johnson, 2013). Related work has shown that phonotactic information can improve the performance of state-ofthe-art segmentation models whose primary objective is to discover the lexicon that underlies the regularities in the continuous speech signal. Again, this work has largely emphasized phoneme-level statistical cues (Blanchard &amp; Heinz 2008, 2010), and those models that do rely on syllable structure (Johnson, 2008a; Johnson &amp; Goldwater, 2009), do not directly encode sequential statistics between adjacent syllables of the sort investigated in the infant literature. Finally, some models assume computations are performed over syllables and that all word boundaries in the input are aligned with syllable boundaries, but provide no mechanism by which such language-specific syllabification principles could be learned (Yang, 2004; Swingley, 2005; Lignos &amp; Yang, 2010). Overall, the existing evidence clearly shows that there are phonotactic cues to word boundaries in spontaneous, child-directed speech. However, there are remaining questions</context>
<context position="7448" citStr="Johnson &amp; Goldwater, 2009" startWordPosition="1080" endWordPosition="1083">y for arbitrary strings using statistics estimated from unsyllabified, unsegmented utterances. We also compare the local minimum segmentation strategy (Saffran et al., 1996; Yang, 2004) to alternatives designed to deal with the challenges of unsupervised estimation of transitional probabilities from unsegmented input. Our focus on the early phonotactic segmentation stage differentiates our approach from many computational models emphasizing the discovery of the lexicon and higher-level language structure (Brent, 1999; Venkataraman, 2001; Swingley, 2005; Johnson, 2008a; Goldwater et al., 2009; Johnson &amp; Goldwater, 2009; Blanchard &amp; Heinz 2008, 2010; Lignos &amp; Yang, 2010). It complements that of recent work investigating the use of phoneme-level statistical regularities for segmentation (Adriaans &amp; Kager, 2010; Daland &amp; Pierrehumbert, 2011). Our work differs from these latter approaches, however, in comparing several phonotactic models, including ones relying on the syllable-based transitional probability statistics investigated in infant research. Our work also contributes to existing segmentation work that assumes a syllabified input (Yang, 2004; Swingley, 2005; Lignos &amp; Yang, 2010) by showing how many aspe</context>
<context position="9124" citStr="Johnson &amp; Goldwater, 2009" startWordPosition="1319" endWordPosition="1322">yllablebased models substantially outperform the phoneme-based models. Indeed, the syllable-based transitional probability phonotactic model achieves a word token segmentation f-score of nearly 80%, which is the highest reported performance among purely phonotactically-based segmentation models (Adriaans &amp; Kager, 2010; Daland &amp; Pierrehumbert, 2011). Indeed, this performance compares favorably with state-of-theart segmentation models that involve learning of higher level regularities, such as the lexicon and collocations (Brent, 1999; Venkataraman, 2001; Johnson, 2008a; Goldwater et al., 2009; Johnson &amp; Goldwater, 2009), and demonstrates that good 20 segmentation performance can be achieved by exploiting simple syllable-level phonotactic cues. 2 Segmentation Model The proposed segmentation model defines the probability of an utterance in terms of an abstract phonotactic probability component that assigns word well-formedness probabilities to phoneme strings. The segmentation algorithm uses those probabilities to determine the maximum likelihood segmentation as defined by a simple generative model. Since the phonotactics and segmentation components are separate, they can be independently modified. This framew</context>
<context position="13997" citStr="Johnson &amp; Goldwater, 2009" startWordPosition="2069" endWordPosition="2072"> of utterances. When syllabifying an intervocalic sequence of consonants, this method finds the longest legal onset aligned with the right edge and places any remaining consonants in the coda of the previous syllable. Thus, a sequence like [wtmi] would be syllabified as [wt.mi] in English since [m] but not [tm] occurs utterance-initially. The only language-particular information required for this approach is knowledge of which phonemes are vowels (syllabic) and which are consonants, a limited type of information also assumed by other syllable inference models for segmentation (Johnson, 2008a; Johnson &amp; Goldwater, 2009). As the segmentation component posits potential words, they are passed to the phonotactic component for syllabification and phonotactic probability calculation. This differs crucially from previous work assuming a fixed syllabification of the input corpus in which word boundaries always align with syllable boundaries (Yang, 2004; Swingley, 2005; Lignos &amp; Yang, 2010). In a setting in which syllabification must be inferred from unsegmented utterances, the learner must be capable of assigning syllabification more flexibly since word boundaries do not always align with the syllable boundaries tha</context>
<context position="22715" citStr="Johnson &amp; Goldwater, 2009" startWordPosition="3411" endWordPosition="3414">. In addition to the above strategies, we also investigated a greedy, iterative re-estimation strategy that makes multiple passes through the corpus. This estimation method takes the output of the above methods and uses it to re-estimate (smoothed and adjusted) parameters for the phonotactic models. It then recomputes the optimal segmentation of the corpus based on the new parameters and repeats until convergence. This method is motivated by previous segmentation work highlighting the effectiveness of greedy re-estimation 23 techniques (Brent, 1999; Venkataraman, 2001; Goldwater et al., 2009; Johnson &amp; Goldwater, 2009). As noted in previous work, such greedy re-estimation has the potential to infer additional word boundaries based on commitments made to word boundaries on earlier passes. 5 Experiments 5.1 Corpus The experiments for all the models were run on the Brent (1999) version of the Bernstein-Ratner (1987) corpus of English child-directed speech consisting of phonetically transcribed utterances. This corpus has been widely used for evaluating segmentation models. Other models evaluated on this corpus include those of Brent (1999), Venkataraman (2001), Blanchard and Heinz (2008), and Johnson and Goldw</context>
<context position="34886" citStr="Johnson &amp; Goldwater (2009)" startWordPosition="5332" endWordPosition="5335">tion and into the precise mechanisms responsible for improved segmentation using syllable structure. Particularly critical is exploration of phonotactically-based segmentation models for languages besides English, for which phonotactic cues hold significant promise (Jarosz &amp; Johnson, 2013) given the relatively low performance of state-ofthe-art lexicon-building models (Johnson 2008b). Another important direction for future work is investigating how early, phonotactically-based segmentation interacts with subsequent learning of higher-level structure, including the lexicon. Johnson (2008a) and Johnson &amp; Goldwater (2009) have already demonstrated that syllable structure provides valuable information in this context; however, their models relied on very different syllable regularities than those investigated here, and the consequences of these differences should be explored in future work. Goldwater et al. (2009) showed that a number of proposed segmentation models have an undersegmentation bias that can be avoided by simultaneously modeling statistical dependencies between words. They proposed a Bayesian prior to favor a smaller lexicon and showed that otherwise unigram models introduce a severe undersegmenta</context>
</contexts>
<marker>Johnson, Goldwater, 2009</marker>
<rawString>Johnson, Mark &amp; Goldwater, Sharon. 2009. Improving nonparametric Bayesian inference: Experiments on unsupervised word segmentation with adaptor grammars. In NAACL ’09: Proceedings of the Annual Conference of the North American Chapter of the Association for Computational Linguistics. Boulder, CO: Association for Computational Linguistics.</rawString>
</citation>
<citation valid="true">
<authors>
<author>Daniel Jurafsky</author>
<author>James H Martin</author>
</authors>
<title>Speech and language processing, 2nd edition. Upper Saddle River,</title>
<date>2008</date>
<publisher>Prentice-Hall.</publisher>
<location>NJ:</location>
<contexts>
<context position="12772" citStr="Jurafsky &amp; Martin, 2008" startWordPosition="1880" endWordPosition="1883"> all bigrams in the word. 2) ! ! = o !(!!+! ! !!) The transitional probability for the sequence xixi+l can be calculated using relative frequency estimates based on counts C in the corpus. !(!!!!!!) 3) ! !! !!-1 = C(XL-1) Section 4 describes strategies that we consider for estimating these parameters in an unsupervised way from unsegmented data where the only word boundaries are those that coincide with utterance boundaries. 3.1 Phoneme Model The first phonotactic model is a standard phoneme bigram model that determines the probability of a word by multiplying the phoneme bigrams in the word (Jurafsky &amp; Martin, 2008). For example, to calculate the phonotactic probability of the sequence [bot] as a word, this model multiplies together P(bj#)P(ojb)P(tjo)P(#jt). 3.2 Syllable-Based Models The other two phonotactic models use syllables rather than phonemes. One model relies on transitional probabilities over syllables, and the other uses onsets and rhymes as the unit of analysis. 3.2.1 Unsupervised Syllabification The syllabification method relies on the language universal principle of onset maximization to1) P w = P Ws..., = ! ! !(!!) 21 gether with an inventory of syllable onsets derived from the beginnings </context>
<context position="21659" citStr="Jurafsky &amp; Martin, 2008" startWordPosition="3247" endWordPosition="3250">to x#. For p# = 0.5, for example, the new estimates would be 0.25, 0.15, and 0.6. The adjustment works analogously for every context for each of the units of analysis. 4.3 Smoothing We also utilized rudimentary smoothing techniques to allow the generative model to deal with unknown sequences. We chose a simple method that allocated non-zero probability to unseen sequences while minimally disrupting the estimates computed using the adjusted boundary count strategy, since our primary concern was in exploring the effects of this novel re-estimation strategy. For all models, add-lambda smoothing (Jurafsky &amp; Martin, 2008) with a value of 0.001 was used. For the syllable-based models this total value was allocated to all unseen bigrams in order to avoid over-allocation of probability to the numerous combinations of unseen syllabic units. 4.4 Iterative Re-estimation After estimating the transitional probabilities from the unsegmented corpus, the above strategies can be used to compute the optimal segmentation of the input corpus in a single pass. In addition to the above strategies, we also investigated a greedy, iterative re-estimation strategy that makes multiple passes through the corpus. This estimation meth</context>
</contexts>
<marker>Jurafsky, Martin, 2008</marker>
<rawString>Jurafsky, Daniel &amp; Martin, James H. 2008. Speech and language processing, 2nd edition. Upper Saddle River, NJ: Prentice-Hall.</rawString>
</citation>
<citation valid="true">
<authors>
<author>Peter W Jusczyk</author>
<author>Paul A Luce</author>
</authors>
<title>Infants’ Sensitivity to Phonotactic Patterns in the Native Language.</title>
<date>1994</date>
<journal>Journal of Memory and Language,</journal>
<volume>33</volume>
<issue>5</issue>
<pages>630--645</pages>
<contexts>
<context position="1456" citStr="Jusczyk &amp; Luce, 1994" startWordPosition="199" endWordPosition="202">he phonotactic models from unsegmented data. The results show that syllable-based models outperform the phoneme models, specifically in the context of improved unsupervised parameter estimation. The syllablebased transitional probability model achieves a word token f-score of nearly 80%, the highest reported performance for a phonotactic segmentation model with no lexicon. 1 Introduction One of the first language learning tasks infants must solve is the segmentation of fluent speech into words. Extensive experimental work has demonstrated that infants are able to use phonotactic restrictions (Jusczyk &amp; Luce, 1994; Mattys et al., 1999; Mattys &amp; Jusczyk, 2001) and other low-level statistical regularities (Saffran et al., 1996; Thiessen &amp; Saffran, 2003; Pelucchi et al., 2009) to extract words from fluent speech before the age of one. This work has shown that infants utilize these low-level statistical regularities to segment speech during the second half of the first year of life before they have developed extensive vocabularies that could provide top-down lexical information to guide segmentation. Developmental research indicates that on average infants know fewer than 100 word types during this period </context>
</contexts>
<marker>Jusczyk, Luce, 1994</marker>
<rawString>Jusczyk, Peter W. &amp; Luce, Paul A. 1994. Infants’ Sensitivity to Phonotactic Patterns in the Native Language. Journal of Memory and Language, 33(5): 630-645.</rawString>
</citation>
<citation valid="true">
<authors>
<author>Constantine Lignos</author>
<author>Charles Yang</author>
</authors>
<title>Recession Segmentation: Simpler Online Word Segmentation Using Limited Resources.</title>
<date>2010</date>
<booktitle>Proceedings of the Fourteenth Conference on Computational Natural Language Learning. (CoNLL &apos;10). Association for Computational Linguistics. .</booktitle>
<contexts>
<context position="5718" citStr="Lignos &amp; Yang, 2010" startWordPosition="830" endWordPosition="833">in, this work has largely emphasized phoneme-level statistical cues (Blanchard &amp; Heinz 2008, 2010), and those models that do rely on syllable structure (Johnson, 2008a; Johnson &amp; Goldwater, 2009), do not directly encode sequential statistics between adjacent syllables of the sort investigated in the infant literature. Finally, some models assume computations are performed over syllables and that all word boundaries in the input are aligned with syllable boundaries, but provide no mechanism by which such language-specific syllabification principles could be learned (Yang, 2004; Swingley, 2005; Lignos &amp; Yang, 2010). Overall, the existing evidence clearly shows that there are phonotactic cues to word boundaries in spontaneous, child-directed speech. However, there are remaining questions regarding the exact nature of these cues, their reliability, and how they relate to the statistical cues explored in the infant word segmentation literature. In this paper, we investigate the computational mechanisms underlying infants’ early speech segmentation abilities relying on low-level statistical regularities, or phonotactics. We present a computational framework that permits the direct comparison of segmentation</context>
<context position="7500" citStr="Lignos &amp; Yang, 2010" startWordPosition="1089" endWordPosition="1092">nsyllabified, unsegmented utterances. We also compare the local minimum segmentation strategy (Saffran et al., 1996; Yang, 2004) to alternatives designed to deal with the challenges of unsupervised estimation of transitional probabilities from unsegmented input. Our focus on the early phonotactic segmentation stage differentiates our approach from many computational models emphasizing the discovery of the lexicon and higher-level language structure (Brent, 1999; Venkataraman, 2001; Swingley, 2005; Johnson, 2008a; Goldwater et al., 2009; Johnson &amp; Goldwater, 2009; Blanchard &amp; Heinz 2008, 2010; Lignos &amp; Yang, 2010). It complements that of recent work investigating the use of phoneme-level statistical regularities for segmentation (Adriaans &amp; Kager, 2010; Daland &amp; Pierrehumbert, 2011). Our work differs from these latter approaches, however, in comparing several phonotactic models, including ones relying on the syllable-based transitional probability statistics investigated in infant research. Our work also contributes to existing segmentation work that assumes a syllabified input (Yang, 2004; Swingley, 2005; Lignos &amp; Yang, 2010) by showing how many aspects of syllable structure can be inferred. Our resul</context>
<context position="14366" citStr="Lignos &amp; Yang, 2010" startWordPosition="2123" endWordPosition="2126">mation required for this approach is knowledge of which phonemes are vowels (syllabic) and which are consonants, a limited type of information also assumed by other syllable inference models for segmentation (Johnson, 2008a; Johnson &amp; Goldwater, 2009). As the segmentation component posits potential words, they are passed to the phonotactic component for syllabification and phonotactic probability calculation. This differs crucially from previous work assuming a fixed syllabification of the input corpus in which word boundaries always align with syllable boundaries (Yang, 2004; Swingley, 2005; Lignos &amp; Yang, 2010). In a setting in which syllabification must be inferred from unsegmented utterances, the learner must be capable of assigning syllabification more flexibly since word boundaries do not always align with the syllable boundaries that would be posited for the utterance as a whole. For example, the universal onset maximization principle always parses singleton consonants VCV as the onsets V.CV rather than codas VC.V. Therefore, without prior knowledge of word boundaries, the utterance [l!kwtmi] (‘look at me’) would be syllabified as [l!.kwt.mi], and if the segmentation algorithm never considered </context>
<context position="27200" citStr="Lignos &amp; Yang (2010)" startWordPosition="4093" endWordPosition="4096">, these partial results are consistent with the trend suggested by previous work that the syllable-level bigrams examined in the infant studies provide little information about word boundaries in natural language data when the local minimum strategy is used. However, a different picture emerges when the performance of the adjusted count strategy is considered. The fact that the local minimum strategy is ineffectual is already clear from the comparison with the monosyllabic baseline; however, the results for the adjusted counts estimation strategy reveal that it is possible to ex1 In contrast, Lignos &amp; Yang (2010) report a word token fscore of 78.9% for this baseline for already syllabified input. The difference between these baselines highlights how much more difficult the segmentation task is when the syllabification must be inferred from unsegmented input. 24 p# = 0 p# = 0.35 p# = 0.5 p# = 0.6 p# = 0.75 p# = 0.99 LM WF BF WF BF WF BF WF BF WF BF WF BF WF BF P 13.0 10.2 34.7 51.9 40.3 60.6 49.9 69.2 45.9 68.8 13.9 50.1 47.1 64.5 OR 15.4 17.9 28.7 43.3 37.1 55.8 42.2 62.0 58.4 76.0 52.3 71.4 27.9 44.1 S 10.7 3.1 12.7 8.6 14.2 12.4 15.9 16.3 20.7 26.1 74.1 84.1 29.8 51.0 P-IR 13.0 10.2 34.7 51.9 40.3 6</context>
</contexts>
<marker>Lignos, Yang, 2010</marker>
<rawString>Lignos, Constantine and Yang, Charles. 2010. Recession Segmentation: Simpler Online Word Segmentation Using Limited Resources. Proceedings of the Fourteenth Conference on Computational Natural Language Learning. (CoNLL &apos;10). Association for Computational Linguistics. .</rawString>
</citation>
<citation valid="true">
<authors>
<author>Sven L Mattys</author>
<author>Peter W Jusczyk</author>
</authors>
<title>Phonotactic cues for segmentation of fluent speech by infants.</title>
<date>2000</date>
<journal>Cognition,</journal>
<volume>78</volume>
<issue>2</issue>
<pages>91--121</pages>
<marker>Mattys, Jusczyk, 2000</marker>
<rawString>Mattys, Sven L. and Jusczyk, Peter W. 2000. Phonotactic cues for segmentation of fluent speech by infants. Cognition, 78(2): 91-121.</rawString>
</citation>
<citation valid="true">
<authors>
<author>Sven L Mattys</author>
<author>Peter W Jusczyk</author>
<author>Paul A Luce</author>
<author>James L Morgan</author>
</authors>
<title>Phonotactic and prosodic effects on word segmentation in infants.</title>
<date>1999</date>
<journal>Cognitive psychology,</journal>
<volume>38</volume>
<issue>4</issue>
<pages>465--494</pages>
<contexts>
<context position="1477" citStr="Mattys et al., 1999" startWordPosition="203" endWordPosition="206">from unsegmented data. The results show that syllable-based models outperform the phoneme models, specifically in the context of improved unsupervised parameter estimation. The syllablebased transitional probability model achieves a word token f-score of nearly 80%, the highest reported performance for a phonotactic segmentation model with no lexicon. 1 Introduction One of the first language learning tasks infants must solve is the segmentation of fluent speech into words. Extensive experimental work has demonstrated that infants are able to use phonotactic restrictions (Jusczyk &amp; Luce, 1994; Mattys et al., 1999; Mattys &amp; Jusczyk, 2001) and other low-level statistical regularities (Saffran et al., 1996; Thiessen &amp; Saffran, 2003; Pelucchi et al., 2009) to extract words from fluent speech before the age of one. This work has shown that infants utilize these low-level statistical regularities to segment speech during the second half of the first year of life before they have developed extensive vocabularies that could provide top-down lexical information to guide segmentation. Developmental research indicates that on average infants know fewer than 100 word types during this period (Dale &amp; Fenson, 1996;</context>
</contexts>
<marker>Mattys, Jusczyk, Luce, Morgan, 1999</marker>
<rawString>Mattys, Sven L., Jusczyk, Peter W., Luce, Paul A., and Morgan, James L. 1999. Phonotactic and prosodic effects on word segmentation in infants. Cognitive psychology, 38(4): 465-494.</rawString>
</citation>
<citation valid="true">
<authors>
<author>Elissa L Newport</author>
<author>Richard N Aslin</author>
</authors>
<title>Learning at a distance I. Statistical learning of nonadjacent dependencies.</title>
<date>2004</date>
<journal>Cognitive psychology,</journal>
<volume>48</volume>
<issue>2</issue>
<pages>127--162</pages>
<marker>Newport, Aslin, 2004</marker>
<rawString>Newport, Elissa L. and Aslin, Richard N. 2004. Learning at a distance I. Statistical learning of nonadjacent dependencies. Cognitive psychology, 48(2): 127-162.</rawString>
</citation>
<citation valid="true">
<authors>
<author>Bruna Pelucchi</author>
<author>Jessica F Hay</author>
<author>Jenny R Saffran</author>
</authors>
<title>Learning in reverse: Eight-month-old infants track backward transitional probabilities.</title>
<date>2009</date>
<journal>Cognition,</journal>
<volume>113</volume>
<issue>2</issue>
<pages>244--247</pages>
<contexts>
<context position="1619" citStr="Pelucchi et al., 2009" startWordPosition="224" endWordPosition="227">nsupervised parameter estimation. The syllablebased transitional probability model achieves a word token f-score of nearly 80%, the highest reported performance for a phonotactic segmentation model with no lexicon. 1 Introduction One of the first language learning tasks infants must solve is the segmentation of fluent speech into words. Extensive experimental work has demonstrated that infants are able to use phonotactic restrictions (Jusczyk &amp; Luce, 1994; Mattys et al., 1999; Mattys &amp; Jusczyk, 2001) and other low-level statistical regularities (Saffran et al., 1996; Thiessen &amp; Saffran, 2003; Pelucchi et al., 2009) to extract words from fluent speech before the age of one. This work has shown that infants utilize these low-level statistical regularities to segment speech during the second half of the first year of life before they have developed extensive vocabularies that could provide top-down lexical information to guide segmentation. Developmental research indicates that on average infants know fewer than 100 word types during this period (Dale &amp; Fenson, 1996; Daland &amp; Pierrehumbert, 2011). One statistical cue that has received a great deal of support in experimental work on infant speech segmentati</context>
</contexts>
<marker>Pelucchi, Hay, Saffran, 2009</marker>
<rawString>Pelucchi, Bruna, Hay, Jessica F., and Saffran, Jenny R. 2009. Learning in reverse: Eight-month-old infants track backward transitional probabilities. Cognition, 113(2): 244-247.</rawString>
</citation>
<citation valid="true">
<authors>
<author>Jenny R Saffran</author>
<author>Richard N Aslin</author>
<author>Elissa L Newport</author>
</authors>
<title>Statistical learning by 8-month-old infants.</title>
<date>1996</date>
<journal>Science,</journal>
<volume>274</volume>
<issue>5294</issue>
<pages>1926--1928</pages>
<contexts>
<context position="1569" citStr="Saffran et al., 1996" startWordPosition="216" endWordPosition="219">odels, specifically in the context of improved unsupervised parameter estimation. The syllablebased transitional probability model achieves a word token f-score of nearly 80%, the highest reported performance for a phonotactic segmentation model with no lexicon. 1 Introduction One of the first language learning tasks infants must solve is the segmentation of fluent speech into words. Extensive experimental work has demonstrated that infants are able to use phonotactic restrictions (Jusczyk &amp; Luce, 1994; Mattys et al., 1999; Mattys &amp; Jusczyk, 2001) and other low-level statistical regularities (Saffran et al., 1996; Thiessen &amp; Saffran, 2003; Pelucchi et al., 2009) to extract words from fluent speech before the age of one. This work has shown that infants utilize these low-level statistical regularities to segment speech during the second half of the first year of life before they have developed extensive vocabularies that could provide top-down lexical information to guide segmentation. Developmental research indicates that on average infants know fewer than 100 word types during this period (Dale &amp; Fenson, 1996; Daland &amp; Pierrehumbert, 2011). One statistical cue that has received a great deal of suppor</context>
<context position="3172" citStr="Saffran et al. (1996)" startWordPosition="454" endWordPosition="457">ment speech (Aslin et al., 1998; Thiessen &amp; Saffran, 2003; Pelucchi et al., 2009). Despite the extensive experimental literature demonstrating infants’ sensitivity to transitional probability in an artificial language learning setting, the utility of these statistical cues in a natural language learning context is disputed. Yang (2004) shows that a segmentation strategy relying on transitional probabilities over syllables achieves very poor results on English childdirected speech, even when the input is perfectly syllabified. Yang implements the local minimum segmentation strategy proposed by Saffran et al. (1996) wherein word boundaries are posited at syllable transitions whenever the transitional probabilities at these positions are lower than at the neighboring transitions. He reports that this strategy discovers a mere 23% of target words and posits incorrect words nearly 60% of the time. Swingley (2005) argues that statistical cues calculated over syllables can provide sufficient information for infants to begin building an initial lexicon. However, the learning strategy explored by Swingley is highly conservative, reliably detecting only a small proportion of target words in the input. Overall, t</context>
<context position="6995" citStr="Saffran et al., 1996" startWordPosition="1016" endWordPosition="1019">articular, we compare a standard phonotactic model relying on phoneme-level bigrams to two syllable-based phonotactic models relying on transitional probabilities. Unlike previous models relying on syllabified data (Yang, 2004; Swingley, 2005; Lignos &amp; Yang, 2010), we do not assume that word boundaries align with syllable boundaries in the input. Rather, we present a simple syllabification method that can be used to model phonotactic probability for arbitrary strings using statistics estimated from unsyllabified, unsegmented utterances. We also compare the local minimum segmentation strategy (Saffran et al., 1996; Yang, 2004) to alternatives designed to deal with the challenges of unsupervised estimation of transitional probabilities from unsegmented input. Our focus on the early phonotactic segmentation stage differentiates our approach from many computational models emphasizing the discovery of the lexicon and higher-level language structure (Brent, 1999; Venkataraman, 2001; Swingley, 2005; Johnson, 2008a; Goldwater et al., 2009; Johnson &amp; Goldwater, 2009; Blanchard &amp; Heinz 2008, 2010; Lignos &amp; Yang, 2010). It complements that of recent work investigating the use of phoneme-level statistical regular</context>
<context position="18743" citStr="Saffran et al., 1996" startWordPosition="2768" endWordPosition="2771">el arises specifically in the context of segmentation due to the number of boundaries encountered in the input data. In an unsegmented corpus there are no boundaries within an utterance. The only evidence for word boundaries comes from boundaries at the beginnings and ends of utterances. The effect is that the total number of boundaries is lower than the number that must be inferred by the learner, and the overall probability of boundaries is underrepresented in the input data. We considered several estimation methods to overcome these effects. 4.1 Local Minimum Strategy In previous research (Saffran et al., 1996) it has been suggested that word boundaries are placed at troughs in transitional probability so that a boundary is inserted between two elements when the transitional probability of those elements is lower than the probability of the neighboring transitions. This strategy captures the fact that word boundaries are more likely to occur between elements that have a low probability of occurring together. Since this strategy does not incorporate transitional probabilities into a generative segmentation model, it provides a simple way around the estimation challenges discussed above. We include it</context>
</contexts>
<marker>Saffran, Aslin, Newport, 1996</marker>
<rawString>Saffran, Jenny R., Aslin, Richard N., and Newport, Elissa L. 1996. Statistical learning by 8-month-old infants. Science, 274(5294): 1926-1928.</rawString>
</citation>
<citation valid="true">
<authors>
<author>Daniel Swingley</author>
</authors>
<title>Statistical clustering and the contents of the infant vocabulary.</title>
<date>2005</date>
<journal>Cognitive Psychology,</journal>
<volume>50</volume>
<issue>1</issue>
<pages>86--32</pages>
<contexts>
<context position="3472" citStr="Swingley (2005)" startWordPosition="501" endWordPosition="502">ontext is disputed. Yang (2004) shows that a segmentation strategy relying on transitional probabilities over syllables achieves very poor results on English childdirected speech, even when the input is perfectly syllabified. Yang implements the local minimum segmentation strategy proposed by Saffran et al. (1996) wherein word boundaries are posited at syllable transitions whenever the transitional probabilities at these positions are lower than at the neighboring transitions. He reports that this strategy discovers a mere 23% of target words and posits incorrect words nearly 60% of the time. Swingley (2005) argues that statistical cues calculated over syllables can provide sufficient information for infants to begin building an initial lexicon. However, the learning strategy explored by Swingley is highly conservative, reliably detecting only a small proportion of target words in the input. Overall, these results raise questions about whether syllable-based statistics can be reliably used to identify word boundaries in natural language data. 19 Proceedings of the 2014 Joint Meeting of SIGMORPHON and SIGFSM, pages 19–28, Baltimore, Maryland USA, June 27 2014. c�2014 Association for Computational </context>
<context position="5696" citStr="Swingley, 2005" startWordPosition="828" endWordPosition="829">eech signal. Again, this work has largely emphasized phoneme-level statistical cues (Blanchard &amp; Heinz 2008, 2010), and those models that do rely on syllable structure (Johnson, 2008a; Johnson &amp; Goldwater, 2009), do not directly encode sequential statistics between adjacent syllables of the sort investigated in the infant literature. Finally, some models assume computations are performed over syllables and that all word boundaries in the input are aligned with syllable boundaries, but provide no mechanism by which such language-specific syllabification principles could be learned (Yang, 2004; Swingley, 2005; Lignos &amp; Yang, 2010). Overall, the existing evidence clearly shows that there are phonotactic cues to word boundaries in spontaneous, child-directed speech. However, there are remaining questions regarding the exact nature of these cues, their reliability, and how they relate to the statistical cues explored in the infant word segmentation literature. In this paper, we investigate the computational mechanisms underlying infants’ early speech segmentation abilities relying on low-level statistical regularities, or phonotactics. We present a computational framework that permits the direct comp</context>
<context position="7381" citStr="Swingley, 2005" startWordPosition="1071" endWordPosition="1073"> method that can be used to model phonotactic probability for arbitrary strings using statistics estimated from unsyllabified, unsegmented utterances. We also compare the local minimum segmentation strategy (Saffran et al., 1996; Yang, 2004) to alternatives designed to deal with the challenges of unsupervised estimation of transitional probabilities from unsegmented input. Our focus on the early phonotactic segmentation stage differentiates our approach from many computational models emphasizing the discovery of the lexicon and higher-level language structure (Brent, 1999; Venkataraman, 2001; Swingley, 2005; Johnson, 2008a; Goldwater et al., 2009; Johnson &amp; Goldwater, 2009; Blanchard &amp; Heinz 2008, 2010; Lignos &amp; Yang, 2010). It complements that of recent work investigating the use of phoneme-level statistical regularities for segmentation (Adriaans &amp; Kager, 2010; Daland &amp; Pierrehumbert, 2011). Our work differs from these latter approaches, however, in comparing several phonotactic models, including ones relying on the syllable-based transitional probability statistics investigated in infant research. Our work also contributes to existing segmentation work that assumes a syllabified input (Yang, </context>
<context position="14344" citStr="Swingley, 2005" startWordPosition="2121" endWordPosition="2122">particular information required for this approach is knowledge of which phonemes are vowels (syllabic) and which are consonants, a limited type of information also assumed by other syllable inference models for segmentation (Johnson, 2008a; Johnson &amp; Goldwater, 2009). As the segmentation component posits potential words, they are passed to the phonotactic component for syllabification and phonotactic probability calculation. This differs crucially from previous work assuming a fixed syllabification of the input corpus in which word boundaries always align with syllable boundaries (Yang, 2004; Swingley, 2005; Lignos &amp; Yang, 2010). In a setting in which syllabification must be inferred from unsegmented utterances, the learner must be capable of assigning syllabification more flexibly since word boundaries do not always align with the syllable boundaries that would be posited for the utterance as a whole. For example, the universal onset maximization principle always parses singleton consonants VCV as the onsets V.CV rather than codas VC.V. Therefore, without prior knowledge of word boundaries, the utterance [l!kwtmi] (‘look at me’) would be syllabified as [l!.kwt.mi], and if the segmentation algor</context>
</contexts>
<marker>Swingley, 2005</marker>
<rawString>Swingley, Daniel. 2005. Statistical clustering and the contents of the infant vocabulary. Cognitive Psychology, 50(1): 86-32.</rawString>
</citation>
<citation valid="true">
<authors>
<author>Erik D Thiessen</author>
<author>Jenny R Saffran</author>
</authors>
<title>When cues collide: use of stress and statistical cues to word boundaries by 7-to 9-month-old infants.</title>
<date>2003</date>
<journal>Developmental psychology,</journal>
<volume>39</volume>
<issue>4</issue>
<pages>706</pages>
<contexts>
<context position="1595" citStr="Thiessen &amp; Saffran, 2003" startWordPosition="220" endWordPosition="223"> the context of improved unsupervised parameter estimation. The syllablebased transitional probability model achieves a word token f-score of nearly 80%, the highest reported performance for a phonotactic segmentation model with no lexicon. 1 Introduction One of the first language learning tasks infants must solve is the segmentation of fluent speech into words. Extensive experimental work has demonstrated that infants are able to use phonotactic restrictions (Jusczyk &amp; Luce, 1994; Mattys et al., 1999; Mattys &amp; Jusczyk, 2001) and other low-level statistical regularities (Saffran et al., 1996; Thiessen &amp; Saffran, 2003; Pelucchi et al., 2009) to extract words from fluent speech before the age of one. This work has shown that infants utilize these low-level statistical regularities to segment speech during the second half of the first year of life before they have developed extensive vocabularies that could provide top-down lexical information to guide segmentation. Developmental research indicates that on average infants know fewer than 100 word types during this period (Dale &amp; Fenson, 1996; Daland &amp; Pierrehumbert, 2011). One statistical cue that has received a great deal of support in experimental work on </context>
</contexts>
<marker>Thiessen, Saffran, 2003</marker>
<rawString>Thiessen, Erik D. and Saffran, Jenny R. 2003. When cues collide: use of stress and statistical cues to word boundaries by 7-to 9-month-old infants. Developmental psychology, 39(4): 706.</rawString>
</citation>
<citation valid="true">
<authors>
<author>Anand Venkataraman</author>
</authors>
<title>A statistical model for word discovery in transcribed speech.</title>
<date>2001</date>
<journal>Computational Linguistics,</journal>
<volume>27</volume>
<issue>3</issue>
<pages>352--372</pages>
<contexts>
<context position="7365" citStr="Venkataraman, 2001" startWordPosition="1069" endWordPosition="1070">mple syllabification method that can be used to model phonotactic probability for arbitrary strings using statistics estimated from unsyllabified, unsegmented utterances. We also compare the local minimum segmentation strategy (Saffran et al., 1996; Yang, 2004) to alternatives designed to deal with the challenges of unsupervised estimation of transitional probabilities from unsegmented input. Our focus on the early phonotactic segmentation stage differentiates our approach from many computational models emphasizing the discovery of the lexicon and higher-level language structure (Brent, 1999; Venkataraman, 2001; Swingley, 2005; Johnson, 2008a; Goldwater et al., 2009; Johnson &amp; Goldwater, 2009; Blanchard &amp; Heinz 2008, 2010; Lignos &amp; Yang, 2010). It complements that of recent work investigating the use of phoneme-level statistical regularities for segmentation (Adriaans &amp; Kager, 2010; Daland &amp; Pierrehumbert, 2011). Our work differs from these latter approaches, however, in comparing several phonotactic models, including ones relying on the syllable-based transitional probability statistics investigated in infant research. Our work also contributes to existing segmentation work that assumes a syllabifi</context>
<context position="9056" citStr="Venkataraman, 2001" startWordPosition="1311" endWordPosition="1312">model with improved unsupervised parameter estimation, the syllablebased models substantially outperform the phoneme-based models. Indeed, the syllable-based transitional probability phonotactic model achieves a word token segmentation f-score of nearly 80%, which is the highest reported performance among purely phonotactically-based segmentation models (Adriaans &amp; Kager, 2010; Daland &amp; Pierrehumbert, 2011). Indeed, this performance compares favorably with state-of-theart segmentation models that involve learning of higher level regularities, such as the lexicon and collocations (Brent, 1999; Venkataraman, 2001; Johnson, 2008a; Goldwater et al., 2009; Johnson &amp; Goldwater, 2009), and demonstrates that good 20 segmentation performance can be achieved by exploiting simple syllable-level phonotactic cues. 2 Segmentation Model The proposed segmentation model defines the probability of an utterance in terms of an abstract phonotactic probability component that assigns word well-formedness probabilities to phoneme strings. The segmentation algorithm uses those probabilities to determine the maximum likelihood segmentation as defined by a simple generative model. Since the phonotactics and segmentation comp</context>
<context position="10890" citStr="Venkataraman, 2001" startWordPosition="1579" endWordPosition="1581">exactly P wi is defined, but the segmentation probability always depends directly on the word probabilities given by a particular phonotactic model. For example, for the utterance [lukwtmi] ‘lookatme’, the segmentation model compares different segmentations, such as [luk#w#tmi] and [luk#wt#mi] based on the phonotactic well-formedness of the posited words. 2.2 Segmentation Algorithm The segmentation algorithm computes and outputs the segmentation with the highest likelihood: argmaxwP w . The optimal segmentation is found using dynamic programming, as in several previous proposals (Brent, 1999; Venkataraman, 2001). Given an input utterance, the model considers placing word boundaries at different positions within the utterance without regard to phonotactics or syllable structure. The phonotactic probability of each posited word is calculated independently as it is considered and used to update the probability of segmentations utilizing that word. In this way, the segmentation component remains entirely divorced from the details of the phonotactic models. Crucially, this means the full space of possible segmentations is considered by the segmentation model regardless of the phonotactic model, with no a </context>
<context position="22663" citStr="Venkataraman, 2001" startWordPosition="3405" endWordPosition="3406">ntation of the input corpus in a single pass. In addition to the above strategies, we also investigated a greedy, iterative re-estimation strategy that makes multiple passes through the corpus. This estimation method takes the output of the above methods and uses it to re-estimate (smoothed and adjusted) parameters for the phonotactic models. It then recomputes the optimal segmentation of the corpus based on the new parameters and repeats until convergence. This method is motivated by previous segmentation work highlighting the effectiveness of greedy re-estimation 23 techniques (Brent, 1999; Venkataraman, 2001; Goldwater et al., 2009; Johnson &amp; Goldwater, 2009). As noted in previous work, such greedy re-estimation has the potential to infer additional word boundaries based on commitments made to word boundaries on earlier passes. 5 Experiments 5.1 Corpus The experiments for all the models were run on the Brent (1999) version of the Bernstein-Ratner (1987) corpus of English child-directed speech consisting of phonetically transcribed utterances. This corpus has been widely used for evaluating segmentation models. Other models evaluated on this corpus include those of Brent (1999), Venkataraman (2001</context>
<context position="31227" citStr="Venkataraman, 2001" startWordPosition="4782" endWordPosition="4784">over which bigrams are computed. This makes sense since boundaries are more likely to fall between larger units than between smaller units. Less expected is the fact that the optimal parameter values are high compared to the empirical rates of word boundaries in the true segmentation of the input corpus. For example, the true rate of utterance-internal word boundaries is around 30% at the phoneme level, yet the optimal p# value for phoneme bigrams is around 60%. The reason for this is that our generative model, like that of a number of previous models discussed in the literature (Brent, 1999; Venkataraman, 2001; Goldwater et al., 2009), has an inherent undersegmentation bias. Due to the way the phonotactic models are defined, there is a cost for every additional word boundary posited in the segmentation. This is because positing a boundary corresponds to the generation of an additional symbol, #, which otherwise does not have to be generated. Since generating a # is never done with 100% probability, doing so al25 ways incurs a cost relative to a segmentation where no such # has to be generated. The high optimal settings of the p# parameter reflect this inherent bias and enable the estimation procedu</context>
</contexts>
<marker>Venkataraman, 2001</marker>
<rawString>Venkataraman, Anand 2001. A statistical model for word discovery in transcribed speech. Computational Linguistics, 27(3): 352-372.</rawString>
</citation>
<citation valid="true">
<authors>
<author>Charles D Yang</author>
</authors>
<title>Universal Grammar, statistics or both?</title>
<date>2004</date>
<journal>Trends in Cognitive Sciences,</journal>
<volume>8</volume>
<issue>10</issue>
<pages>451--456</pages>
<contexts>
<context position="2888" citStr="Yang (2004)" startWordPosition="415" endWordPosition="416">In foundational work, Saffran et al. (1996) found that infants are able to segment words from continuous speech using statistical regularities between syllables. Numerous subsequent studies have confirmed that infants can track transitional probabilities and use them to segment speech (Aslin et al., 1998; Thiessen &amp; Saffran, 2003; Pelucchi et al., 2009). Despite the extensive experimental literature demonstrating infants’ sensitivity to transitional probability in an artificial language learning setting, the utility of these statistical cues in a natural language learning context is disputed. Yang (2004) shows that a segmentation strategy relying on transitional probabilities over syllables achieves very poor results on English childdirected speech, even when the input is perfectly syllabified. Yang implements the local minimum segmentation strategy proposed by Saffran et al. (1996) wherein word boundaries are posited at syllable transitions whenever the transitional probabilities at these positions are lower than at the neighboring transitions. He reports that this strategy discovers a mere 23% of target words and posits incorrect words nearly 60% of the time. Swingley (2005) argues that sta</context>
<context position="5680" citStr="Yang, 2004" startWordPosition="826" endWordPosition="827">ontinuous speech signal. Again, this work has largely emphasized phoneme-level statistical cues (Blanchard &amp; Heinz 2008, 2010), and those models that do rely on syllable structure (Johnson, 2008a; Johnson &amp; Goldwater, 2009), do not directly encode sequential statistics between adjacent syllables of the sort investigated in the infant literature. Finally, some models assume computations are performed over syllables and that all word boundaries in the input are aligned with syllable boundaries, but provide no mechanism by which such language-specific syllabification principles could be learned (Yang, 2004; Swingley, 2005; Lignos &amp; Yang, 2010). Overall, the existing evidence clearly shows that there are phonotactic cues to word boundaries in spontaneous, child-directed speech. However, there are remaining questions regarding the exact nature of these cues, their reliability, and how they relate to the statistical cues explored in the infant word segmentation literature. In this paper, we investigate the computational mechanisms underlying infants’ early speech segmentation abilities relying on low-level statistical regularities, or phonotactics. We present a computational framework that permits</context>
<context position="7008" citStr="Yang, 2004" startWordPosition="1020" endWordPosition="1021">a standard phonotactic model relying on phoneme-level bigrams to two syllable-based phonotactic models relying on transitional probabilities. Unlike previous models relying on syllabified data (Yang, 2004; Swingley, 2005; Lignos &amp; Yang, 2010), we do not assume that word boundaries align with syllable boundaries in the input. Rather, we present a simple syllabification method that can be used to model phonotactic probability for arbitrary strings using statistics estimated from unsyllabified, unsegmented utterances. We also compare the local minimum segmentation strategy (Saffran et al., 1996; Yang, 2004) to alternatives designed to deal with the challenges of unsupervised estimation of transitional probabilities from unsegmented input. Our focus on the early phonotactic segmentation stage differentiates our approach from many computational models emphasizing the discovery of the lexicon and higher-level language structure (Brent, 1999; Venkataraman, 2001; Swingley, 2005; Johnson, 2008a; Goldwater et al., 2009; Johnson &amp; Goldwater, 2009; Blanchard &amp; Heinz 2008, 2010; Lignos &amp; Yang, 2010). It complements that of recent work investigating the use of phoneme-level statistical regularities for seg</context>
<context position="14328" citStr="Yang, 2004" startWordPosition="2119" endWordPosition="2120">ly language-particular information required for this approach is knowledge of which phonemes are vowels (syllabic) and which are consonants, a limited type of information also assumed by other syllable inference models for segmentation (Johnson, 2008a; Johnson &amp; Goldwater, 2009). As the segmentation component posits potential words, they are passed to the phonotactic component for syllabification and phonotactic probability calculation. This differs crucially from previous work assuming a fixed syllabification of the input corpus in which word boundaries always align with syllable boundaries (Yang, 2004; Swingley, 2005; Lignos &amp; Yang, 2010). In a setting in which syllabification must be inferred from unsegmented utterances, the learner must be capable of assigning syllabification more flexibly since word boundaries do not always align with the syllable boundaries that would be posited for the utterance as a whole. For example, the universal onset maximization principle always parses singleton consonants VCV as the onsets V.CV rather than codas VC.V. Therefore, without prior knowledge of word boundaries, the utterance [l!kwtmi] (‘look at me’) would be syllabified as [l!.kwt.mi], and if the se</context>
<context position="19444" citStr="Yang, 2004" startWordPosition="2873" endWordPosition="2874">ity so that a boundary is inserted between two elements when the transitional probability of those elements is lower than the probability of the neighboring transitions. This strategy captures the fact that word boundaries are more likely to occur between elements that have a low probability of occurring together. Since this strategy does not incorporate transitional probabilities into a generative segmentation model, it provides a simple way around the estimation challenges discussed above. We include it for comparison to previous results relying on syllable-based transitional probabilities (Yang, 2004). 4.2 Adjusted Boundary Count Strategy We also introduce a novel, simple method for adjusting the estimates of transitional probabilities based on input data that underrepresents word boundaries. This method directly adjusts the parameter estimates in order to increase the overall likelihood of word boundaries. The main insight behind this estimation strategy is that observed bigram counts (of co-occurring phonemes, syllables, or onsets and rhymes) in the input data are overestimated since a proportion of them are in reality separated by word boundaries in the desired segmentation. For a given</context>
<context position="24119" citStr="Yang (2004)" startWordPosition="3623" endWordPosition="3624">mance scores are taken from the iteration after convergence. This typically happened after 5-10 iterations. 5.3 Results and Discussion Table 1 summarizes the word boundary and word token f-scores for all models, while Table 2 presents the precision and recall scores for the best-performing adjusted count models and the local minimum models. Focusing first on the local minimum estimateion strategy, there are several noteworthy effects. First, our results with local minima for the syllable-level transitional probabilities achieves very similar word token precision and recall to that reported by Yang (2004), who examined a different corpus of child-directed English. The word token precision and recall of our model is 40.2% and 23.7%, respectively, while Yang reported 41.6% and 23.3%, respectively, for his experiments. This corroborates Yang’s finding that the local minima estimation strategy for syllable-level transitional probabilities works very poorly, this time showing that this level of performance can be achieved with simultaneous inference of syllabification. As Table 2 shows, the poor performance can be attributed to poor recall, which the low boundary recall and high precision illustrat</context>
<context position="33329" citStr="Yang 2004" startWordPosition="5114" endWordPosition="5115">ented input is essential for achieving good performance. The strong performance of the syllable level bigram phonotactic model has a number of implications. First, it demonstrates that the kind of statistical regularities that infants have been consistently shown to be sensitive to in artificial experimental stimuli do provide a substantial amount of information about word boundaries in natural language data, at least in English. This lends significant credibility to the claim that sensitivity to such statistical regularities plays a crucial role in infants’ early language development (contra Yang 2004). This result also highlights the role that sensitivity to richer phonological information, beyond the level of phonemes, plays in language learning, a result that is echoed in much recent work on the modeling of phonotactic well-formedness of isolated words (Hayes &amp; Wilson, 2008; Albright, 2009; Daland et al., 2011). A consistent finding of this work has been that access to abstract structure and robust generalization mechanisms is crucial to the modeling of human phonotactic knowledge. While our results are compatible with these conclusions, our results cannot confirm that it is syllable str</context>
</contexts>
<marker>Yang, 2004</marker>
<rawString>Yang, Charles D. 2004. Universal Grammar, statistics or both? Trends in Cognitive Sciences, 8(10): 451-456.</rawString>
</citation>
</citationList>
</algorithm>
</algorithms>